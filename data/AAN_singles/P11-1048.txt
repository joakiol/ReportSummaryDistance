Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 470?480,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Comparison of Loopy Belief Propagation and Dual Decomposition forIntegrated CCG Supertagging and ParsingMichael AuliSchool of InformaticsUniversity of Edinburghm.auli@sms.ed.ac.ukAdam LopezHLTCOEJohns Hopkins Universityalopez@cs.jhu.eduAbstractVia an oracle experiment, we show that theupper bound on accuracy of a CCG parseris significantly lowered when its search spaceis pruned using a supertagger, though the su-pertagger also prunes many bad parses.
In-spired by this analysis, we design a singlemodel with both supertagging and parsing fea-tures, rather than separating them into dis-tinct models chained together in a pipeline.To overcome the resulting increase in com-plexity, we experiment with both belief prop-agation and dual decomposition approaches toinference, the first empirical comparison ofthese algorithms that we are aware of on astructured natural language processing prob-lem.
On CCGbank we achieve a labelled de-pendency F-measure of 88.8% on gold POStags, and 86.7% on automatic part-of-speeochtags, the best reported results for this task.1 IntroductionAccurate and efficient parsing of Combinatorial Cat-egorial Grammar (CCG; Steedman, 2000) is a long-standing problem in computational linguistics, dueto the complexities associated its mild context sen-sitivity.
Even for practical CCG that are stronglycontext-free (Fowler and Penn, 2010), parsing ismuch harder than with Penn Treebank-style context-free grammars, with vast numbers of nonterminalcategories leading to increased grammar constants.Where a typical Penn Treebank grammar may havefewer than 100 nonterminals (Hockenmaier andSteedman, 2002), we found that a CCG grammarderived from CCGbank contained over 1500.
Thesame grammar assigns an average of 22 lexical cate-gories per word (Clark and Curran, 2004a), resultingin an enormous space of possible derivations.The most successful approach to CCG parsing isbased on a pipeline strategy (?2).
First, we tag (ormultitag) each word of the sentence with a lexicalcategory using a supertagger, a sequence model overthese categories (Bangalore and Joshi, 1999; Clark,2002).
Second, we parse the sentence under therequirement that the lexical categories are fixed tothose preferred by the supertagger.
Variations onthis approach drive the widely-used, broad coverageC&C parser (Clark and Curran, 2004a; Clark andCurran, 2007; Kummerfeld et al, 2010).
However,it fails when the supertagger makes errors.
We showexperimentally that this pipeline significantly lowersthe upper bound on parsing accuracy (?3).The same experiment shows that the supertag-ger prunes many bad parses.
So, while we want toavoid the error propagation inherent to a pipeline,ideally we still want to benefit from the key insightof supertagging: that a sequence model over lexi-cal categories can be quite accurate.
Our solutionis to combine the features of both the supertaggerand the parser into a single, less aggressively prunedmodel.
The challenge with this model is its pro-hibitive complexity, which we address with approx-imate methods: dual decomposition and belief prop-agation (?4).
We present the first side-by-side com-parison of these algorithms on an NLP task of thiscomplexity, measuring accuracy, convergence be-havior, and runtime.
In both cases our model signifi-cantly outperforms the pipeline approach, leading tothe best published results in CCG parsing (?5).4702 CCG and SupertaggingCCG is a lexicalized grammar formalism encodingfor each word lexical categories that are either ba-sic (eg.
NN, JJ) or complex.
Complex lexical cat-egories specify the number and directionality of ar-guments.
For example, one lexical category for theverb like is (S\NP )/NP , specifying the first argu-ment as an NP to the right and the second as an NPto the left; there are over 100 lexical categories forlike in our lexicon.
In parsing, adjacent spans arecombined using a small number of binary combina-tory rules like forward application or composition(Steedman, 2000; Fowler and Penn, 2010).
In thefirst derivation below, (S\NP )/NP and NP com-bine to form the spanning category S\NP , whichonly requires an NP to its left to form a completesentence-spanning S. The second derivation usestype-raising to change the category type of I.I like teaNP (S\NP)/NP NP>S\NP<SI like teaNP (S\NP)/NP NP>TS/(S\NP)>BS/NP>SAs can be inferred from even this small example,a key difficulty in parsing CCG is that the numberof categories quickly becomes extremely large, andthere are typically many ways to analyze every spanof a sentence.Supertagging (Bangalore and Joshi, 1999; Clark,2002) treats the assignment of lexical categories (orsupertags) as a sequence tagging problem.
Becausethey do this with high accuracy, they are often ex-ploited to prune the parser?s search space: the parseronly considers lexical categories with high posteriorprobability (or other figure of merit) under the su-pertagging model (Clark and Curran, 2004a).
Theposterior probabilities are then discarded; it is theextensive pruning of lexical categories that leads tosubstantially faster parsing times.Pruning the categories in advance this way has aspecific failure mode: sometimes it is not possibleto produce a sentence-spanning derivation from thetag sequences preferred by the supertagger, since itdoes not enforce grammaticality.
A workaround forthis problem is the adaptive supertagging (AST) ap-proach of Clark and Curran (2004a).
It is based ona step function over supertagger beam widths, re-laxing the pruning threshold for lexical categoriesonly if the parser fails to find an analysis.
The pro-cess either succeeds and returns a parse after someiteration or gives up after a predefined number of it-erations.
As Clark and Curran (2004a) show, mostsentences can be parsed with a very small number ofsupertags per word.
However, the technique is inher-ently approximate: it will return a lower probabilityparse under the parsing model if a higher probabil-ity parse can only be constructed from a supertagsequence returned by a subsequent iteration.
In thisway it prioritizes speed over exactness, although thetradeoff can be modified by adjusting the beam stepfunction.
Regardless, the effect of the approxima-tion is unbounded.We will also explore reverse adaptive supertag-ging, a much less aggressive pruning method thatseeks only to make sentences parseable when theyotherwise would not be due to an impractically largesearch space.
Reverse AST starts with a wide beam,narrowing it at each iteration only if a maximumchart size is exceeded.
In this way it prioritizes ex-actness over speed.3 Oracle ParsingWhat is the effect of these approximations?
Toanswer this question we computed oracle best andworst values for labelled dependency F-score usingthe algorithm of Huang (2008) on the hybrid modelof Clark and Curran (2007), the best model of theirC&C parser.
We computed the oracle on our devel-opment data, Section 00 of CCGbank (Hockenmaierand Steedman, 2007), using both AST and ReverseAST beams settings shown in Table 1.The results (Table 2) show that the oracle bestaccuracy for reverse AST is more than 3% higherthan the aggressive AST pruning.1 In fact, it is al-most as high as the upper bound oracle accuracy of97.73% obtained using perfect supertags?in otherwords, the search space for reverse AST is theoreti-cally near-optimal.2 We also observe that the oracle1The numbers reported here and in later sections differ slightlyfrom those in a previously circulated draft of this paper, fortwo reasons: we evaluate only on sentences for which a parsewas returned instead of all parses, to enable direct comparisonwith Clark and Curran (2007); and we use their hybrid modelinstead of their normal-form model, except where noted.
De-spite these changes our main findings remained unchanged.2This idealized oracle reproduces a result from Clark and Cur-471Condition Parameter Iteration 1 2 3 4 5AST?
(beam width) 0.075 0.03 0.01 0.005 0.001k (dictionary cutoff) 20 20 20 20 150Reverse?
0.001 0.005 0.01 0.03 0.075k 150 20 20 20 20Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.Parameter ?
is a beam threshold while k bounds the use of a part-of-speech tag dictionary, which is used for wordsseen less than k times.Viterbi F-score Oracle Max F-score Oracle Min F-scoreLF LP LR LF LP LR LF LP LR cat/wordAST 87.38 87.83 86.93 94.35 95.24 93.49 54.31 54.81 53.83 1.3-3.6Reverse 87.36 87.55 87.17 97.65 98.21 97.09 18.09 17.75 18.43 3.6-1.3Table 2: Comparison of adaptive supertagging (AST) and a less restrictive setting (Reverse) with Viterbi and oracleF-scores on CCGbank Section 00.
The table shows the labelled F-score (LF), precision (LP) and recall (LR) and thethe number of lexical categories per word used (from first to last parsing attempt).88.2?88.4?88.6?88.8?89.0?89.2?89.4?89.6?89.8?85600?85800?86000?86200?86400?86600?86800?87000?87200?87400?0.075?
0.03?0.01?0.005?0.001?0.0005?0.0001?0.00005?0.00001?Labelleld?F-??score?Model?score?Supertagger?beam?Model?score?
F-??measure?93.5?94.0?94.5?95.0?95.5?96.0?96.5?97.0?97.5?98.0?98.5?82500?83000?83500?84000?84500?85000?0.075?
0.03?0.01?0.005?0.001?0.0005?0.0001?0.00005?0.00001?Labelleld?F-??score?Model?score?Supertagger?beam?Model?score?
F-?
?measure?Figure 1: Comparison between model score and Viterbi F-score (left); and between model score and oracle F-score(right) for different supertagger beams on a subset of CCGbank Section 00.worst accuracy is much lower in the reverse setting.It is clear that the supertagger pipeline has two ef-fects: while it beneficially prunes many bad parses,it harmfully prunes some very good parses.
We canalso see from the scores of the Viterbi parses thatwhile the reverse condition has access to much betterparses, the model doesn?t actually find them.
Thismirrors the result of Clark and Curran (2007) thatthey use to justify AST.Digging deeper, we compared parser model scoreagainst Viterbi F-score and oracle F-score at a va-ran (2004b).
The reason that using the gold-standard supertagsdoesn?t result in 100% oracle parsing accuracy is that someof the development set parses cannot be constructed by thelearned grammar.riety of fixed beam settings (Figure 1), consideringonly the subset of our development set which couldbe parsed with all beam settings.
The inverse re-lationship between model score and F-score showsthat the supertagger restricts the parser to mostlygood parses (under F-measure) that the model wouldotherwise disprefer.
Exactly this effect is exploitedin the pipeline model.
However, when the supertag-ger makes a mistake, the parser cannot recover.4 Integrated Supertagging and ParsingThe supertagger obviously has good but not perfectpredictive features.
An obvious way to exploit thiswithout being bound by its decisions is to incorpo-rate these features directly into the parsing model.472In our case both the parser and the supertagger arefeature-based models, so from the perspective of asingle parse tree, the change is simple: the tree issimply scored by the weights corresponding to allof its active features.
However, since the features ofthe supertagger are all Markov features on adjacentsupertags, the change has serious implications forsearch.
If we think of the supertagger as defining aweighted regular language consisting of all supertagsequences, and the parser as defining a weightedmildly context-sensitive language consisting of onlya subset of these sequences, then the search prob-lem is equivalent to finding the optimal derivationin the weighted intersection of a regular and mildlycontext-sensitive language.
Even allowing for theobservation of Fowler and Penn (2010) that our prac-tical CCG is context-free, this problem still reducesto the construction of Bar-Hillel et al (1964), mak-ing search very expensive.
Therefore we need ap-proximations.Fortunately, recent literature has introduced tworelevant approximations to the NLP community:loopy belief propagation (Pearl, 1988), applied todependency parsing by Smith and Eisner (2008);and dual decomposition (Dantzig and Wolfe, 1960;Komodakis et al, 2007; Sontag et al, 2010, interalia), applied to dependency parsing by Koo et al(2010) and lexicalized CFG parsing by Rush et al(2010).
We apply both techniques to our integratedsupertagging and parsing model.4.1 Loopy Belief PropagationBelief propagation (BP) is an algorithm for com-puting marginals (i.e.
expectations) on structuredmodels.
These marginals can be used for decoding(parsing) in a minimum-risk framework (Smith andEisner, 2008); or for training using a variety of al-gorithms (Sutton and McCallum, 2010).
We experi-ment with both uses in ?5.
Many researchers in NLPare familiar with two special cases of belief prop-agation: the forward-backward and inside-outsidealgorithms, used for computing expectations in se-quence models and context-free grammars, respec-tively.3 Our use of belief propagation builds directlyon these two familiar algorithms.3Forward-backward and inside-outside are formally shown tobe special cases of belief propagation by Smyth et al (1997)and Sato (2007), respectively.f(T1)f(T2)b(T0) b(T1)t1T0T1t2T2e0e1e2Figure 2: Supertagging factor graph with messages.
Cir-cles are variables and filled squares are factors.BP is usually understood as an algorithm on bi-partite factor graphs, which structure a global func-tion into local functions over subsets of variables(Kschischang et al, 1998).
Variables maintain a be-lief (expectation) over a distribution of values andBP passes messages about these beliefs betweenvariables and factors.
The idea is to iteratively up-date each variable?s beliefs based on the beliefs ofneighboring variables (through a shared factor), us-ing the sum-product rule.This results in the following equation for a mes-sage mx?f (x) from a variable x to a factor fmx?f (x) =?h?n(x)\fmh?x(x) (1)where n(x) is the set of all neighbours of x. Themessage mf?x from a factor to a variable ismf?x(x) =??
{x}f(X)?y?n(f)\xmy?f (y) (2)where ?
{x} represents all variables other than x,X = n(f) and f(X) is the set of arguments of thefactor function f .Making this concrete, our supertagger defines adistribution over tags T0...TI , based on emissionfactors e0...eI and transition factors t1...tI (Fig-ure 2).
The message fi a variable Ti receives from itsneighbor to the left corresponds to the forward prob-ability, while messages from the right correspond tobackward probability bi.fi(Ti) =?Ti?1fi?1(Ti?1)ei?1(Ti?1)ti(Ti?1, Ti) (3)bi(Ti) =?Ti+1bi+1(Ti+1)ei+1(Ti+1)ti+1(Ti, Ti+1) (4)473span(0,2)span(1,3)span(0,3)TREEn(T0) o(T2)o(T0)n(T2)T0e0e1e2T1T2f(T1)f(T2)b(T0)b(T1)t1t2Figure 3: Factor graph for the combined parsing and su-pertagging model.The current belief Bx(x) for variable x can be com-puted by taking the normalized product of all its in-coming messages.Bx(x) =1Z?h?n(x)mh?x(x) (5)In the supertagger model, this is just:p(Ti) =1Zfi(Ti)bi(Ti)ei(Ti) (6)Our parsing model is also a distribution over vari-ables Ti, along with an additional quadratic numberof span(i, j) variables.
Though difficult to representpictorially, a distribution over parses is captured byan extension to graphical models called case-factordiagrams (McAllester et al, 2008).
We add thiscomplex distribution to our model as a single fac-tor (Figure 3).
This is a natural extension to the useof complex factors described by Smith and Eisner(2008) and Dreyer and Eisner (2009).When a factor graph is a tree as in Figure 2, BPconverges in a single iteration to the exact marginals.However, when the model contains cycles, as in Fig-ure 3, we can iterate message passing.
Under certainassumptions this loopy BP it will converge to ap-proximate marginals that are bounded under an in-terpretation from statistical physics (Yedidia et al,2001; Sutton and McCallum, 2010).The TREE factor exchanges inside ni and outsideoi messages with the tag and span variables, tak-ing into account beliefs from the sequence model.We will omit the unchanged outside recursion forbrevity, but inside messages n(Ci,j) for categoryCi,j in span(i, j) are computed using rule probabil-ities r as follows:n(Ci,j) =????
?fi(Ci,j)bi(Ci,j)ei(Ci,j) if j=i+1?k,X,Yn(Xi,k)n(Yk,j)r(Ci,j , Xi,k, Yk,j)(7)Note that the only difference from the classic in-side algorithm is that the recursive base case of a cat-egory spanning a single word has been replaced bya message from the supertag that contains both for-ward and backward factors, along with a unary emis-sion factor, which doubles as a unary rule factor andthus contains the only shared features of the originalmodels.
This difference is also mirrored in the for-ward and backward messages, which are identical toEquations 3 and 4, except that they also incorporateoutside messages from the tree factor.Once all forward-backward and inside-outsideprobabilities have been calculated the belief of su-pertag Ti can be computed as the product of all in-coming messages.
The only difference from Equa-tion 6 is the addition of the outside message.p(Ti) =1Zfi(Ti)bi(Ti)ei(Ti)oi(Ti) (8)The algorithm repeatedly runs forward-backwardand inside-outside, passing their messages back andforth, until these quantities converge.4.2 Dual DecompositionDual decomposition (Rush et al, 2010; Koo et al,2010) is a decoding (i.e.
search) algorithm for prob-lems that can be decomposed into exactly solvablesubproblems: in our case, supertagging and parsing.Formally, given Y as the set of valid parses, Z as theset of valid supertag sequences, and T as the set ofsupertags, we want to solve the following optimiza-tion for parser f(y) and supertagger g(z).argmaxy?Y,z?Zf(y) + g(z) (9)such that y(i, t) = z(i, t) for all (i, t) ?
I (10)Here y(i, t) is a binary function indicating whetherword i is assigned supertag t by the parser, for the474set I = {(i, t) : i ?
1 .
.
.
n, t ?
T} denotingthe set of permitted supertags for each word; sim-ilarly z(i, t) for the supertagger.
To enforce the con-straint that the parser and supertagger agree on atag sequence we introduce Lagrangian multipliersu = {u(i, t) : (i, t) ?
I} and construct a dual ob-jective over variables u(i, t).L(u) = maxy?Y(f(y)?
?i,tu(i, t)y(i, t)) (11)+maxz?Z(f(z) +?i,tu(i, t)z(i, t))This objective is an upper bound that we want tomake as tight as possible by solving for minu L(u).We optimize the values of the u(i, t) variables usingthe same algorithm as Rush et al (2010) for theirtagging and parsing problem (essentially a percep-tron update).4 An advantages of DD is that, on con-vergence, it recovers exact solutions to the combinedproblem.
However, if it does not converge or we stopearly, an approximation must be returned: followingRush et al (2010) we used the highest scoring outputof the parsing submodel over all iterations.5 ExperimentsParser.
We use the C&C parser (Clark and Curran,2007) and its supertagger (Clark, 2002).
Our base-line is the hybrid model of Clark and Curran (2007);our integrated model simply adds the supertaggerfeatures to this model.
The parser relies solely on thesupertagger for pruning, using CKY for search overthe pruned space.
Training requires repeated calcu-lation of feature expectations over packed charts ofderivations.
For training, we limited the number ofitems in this chart to 0.3 million, and for testing, 1million.
We also used a more permissive trainingsupertagger beam (Table 3) than in previous work(Clark and Curran, 2007).
Models were trained withthe parser?s L-BFGS trainer.Evaluation.
We evaluated on CCGbank (Hocken-maier and Steedman, 2007), a right-most normal-form CCG version of the Penn Treebank.
Weuse sections 02-21 (39603 sentences) for training,4The u terms can be interpreted as the messages from factorsto variables (Sontag et al, 2010) and the resulting messagepassing algorithms are similar to the max-product algorithm, asister algorithm to BP.section 00 (1913 sentences) for development andsection 23 (2407 sentences) for testing.
We sup-ply gold-standard part-of-speech tags to the parsers.Evaluation is based on labelled and unlabelled pred-icate argument structure recovery and supertag ac-curacy.
We only evaluate on sentences for which ananalysis was returned; the coverage for all parsers is99.22% on section 00, and 99.63% on section 23.Model combination.
We combine the parser andthe supertagger over the search space defined by theset of supertags within the supertagger beam (see Ta-ble 1); this avoids having to perform inference overthe prohibitively large set of parses spanned by allsupertags.
Hence at each beam setting, the modeloperates over the same search space as the baseline;the difference is that we search with our integratedmodel.5.1 Parsing AccuracyWe first experiment with the separately trained su-pertagger and parser, which are then combined us-ing belief propagation (BP) and dual decomposition(DD).
We run the algorithms for many iterations,and irrespective of convergence, for BP we computethe minimum risk parse from the current marginals,and for DD we choose the highest-scoring parseseen over all iterations.
We measured the evolvingaccuracy of the models on the development set (Fig-ure 4).
In line with our oracle experiment, these re-sults demonstrate that we can coax more accurateparses from the larger search space provided by thereverse setting; the influence of the supertagger fea-tures allow us to exploit this advantage.One behavior we observe in the graph is that theDD results tend to incrementally improve in accu-racy while the BP results quickly stabilize, mirroringthe result of Smith and Eisner (2008).
This occursbecause DD continues to find higher scoring parsesat each iteration, and hence the results change.
How-ever for BP, even if the marginals have not con-verged, the minimum risk solution turns out to befairly stable across successive iterations.We next compare the algorithms against the base-line on our test set (Table 4).
We find that the earlystability of BP?s performance generalises to the testset as does DD?s improvement over several itera-tions.
More importantly, we find that the applying475Parameter Iteration 1 2 3 4 5 6 7Training ?
0.001 0.001 0.0045 0.0055 0.01 0.05 0.1k 150 20 20 20 20 20 20Table 3: Beam step function used for training (cf.
Table 1).section 00 (dev) section 23 (test)AST Reverse AST ReverseLF UF ST LF UF ST LF UF ST LF UF STBaseline 87.38 93.08 94.21 87.36 93.13 93.99 87.73 93.09 94.33 87.65 93.06 94.01C&C ?07 87.24 93.00 94.16 - - - 87.64 93.00 94.32 - - -BPk=1 87.70 93.28 94.44 88.35 93.69 94.73 88.20 93.28 94.60 88.78 93.66 94.81BPk=25 87.70 93.31 94.44 88.33 93.72 94.71 88.19 93.27 94.59 88.80 93.68 94.81DDk=1 87.40 93.09 94.23 87.38 93.15 94.03 87.74 93.10 94.33 87.67 93.07 94.02DDk=25 87.71 93.32 94.44 88.29 93.71 94.67 88.14 93.24 94.59 88.80 93.68 94.82Table 4: Results for individually-trained submodels combined using dual decomposition (DD) or belief propagation(BP) for k iterations, evaluated by labelled and unlabelled F-score (LF/UF) and supertag accuracy (ST).
We compareagainst the previous best result of Clark and Curran (2007); our baseline is their model with wider training beams (cf.Table 3).87.2?87.4?87.6?87.8?88.0?88.2?88.4?1?
6?
11?
16?
21?
26?
31?
36?
41?
46?Labelled?F-??score?Itera?ons?BL??AST?
BL?Rev?
BP?AST?BP?Rev?
DD?AST?
DD?Rev?Figure 4: Labelled F-score of baseline (BL), belief prop-agation (BP), and dual decomposition (DD) on section00.our combined model using either algorithm consis-tently outperforms the baseline after only a few iter-ations.
Overall, we improve the labelled F-measureby almost 1.1% and unlabelled F-measure by 0.6%over the baseline.
To the best of our knowledge,the results obtained with BP and DD are the bestreported results on this task using gold POS tags.Next, we evaluate performance when using au-tomatic part-of-speech tags as input to our parserand supertagger (Table 5).
This enables us to com-pare against the results of Fowler and Penn (2010),who trained the Petrov parser (Petrov et al, 2006)on CCGbank.
We outperform them on all criteria.Hence our combined model represents the best CCGparsing results under any setting.Finally, we revisit the oracle experiment of ?3 us-ing our combined models (Figure 5).
Both show animproved relationship between model score and F-measure.5.2 Algorithmic ConvergenceFigure 4 shows that parse accuracy converges af-ter a few iterations.
Do the algorithms converge?BP converges when the marginals do not change be-tween iterations, and DD converges when both sub-models agree on all supertags.
We measured theconvergence of each algorithm under these criteriaover 1000 iterations (Figure 6).
DD converges muchfaster, while BP in the reverse condition convergesquite slowly.
This is interesting when contrastedwith its behavior on parse accuracy?its rate of con-vergence after one iteration is 1.5%, but its accu-racy is already the highest at this point.
Over theentire 1000 iterations, most sentences converge: allbut 3 for BP (both in AST and reverse) and all but476section 00 (dev) section 23 (test)LF LP LR UF UP UR LF LP LR UF UP URBaseline 85.53 85.73 85.33 91.99 92.20 91.77 85.74 85.90 85.58 91.92 92.09 91.75Petrov I-5 85.79 86.09 85.50 92.44 92.76 92.13 86.01 86.29 85.74 92.34 92.64 92.04BPk=1 86.44 86.74 86.14 92.54 92.86 92.23 86.73 86.95 86.50 92.45 92.69 92.21DDk=25 86.35 86.65 86.05 92.52 92.85 92.20 86.68 86.90 86.46 92.44 92.67 92.21Table 5: Results on automatically assigned POS tags.
Petrov I-5 is based on the parser output of Fowler and Penn(2010); we evaluate on sentences for which all parsers returned an analysis (2323 sentences for section 23 and 1834sentences for section 00).89.4?89.5?89.6?89.7?89.8?89.9?90.0?60000?80000?100000?120000?140000?160000?180000?200000?0.075?
0.03?0.01?0.005?0.001?0.0005?0.0001?0.00005?0.00001?Labelleld?F-??score?Model?score?Supertagger?beam?Model?score?
F-??measure?89.2?89.3?89.4?89.5?89.6?89.7?89.8?89.9?85200?85400?85600?85800?86000?86200?86400?0.075?
0.03?0.01?0.005?0.001?0.0005?0.0001?0.00005?0.00001?Labelleld?F-??score?Model?score?Supertagger?beam?Model?score?
F-?
?measure?Figure 5: Comparison between model score and Viterbi F-score for the integrated model using belief propagation (left)and dual decomposition (right); the results are based on the same data as Figure 1..!"#!"$!"%!"&!"'!"(!")!"*!"+!"#!!
"#" #!"
#!!"
#!!!"!"#$%&'%#(%)&*+%),-.
)/+%&*0"#1),-"./0",-"1232452"66"./0"66"1232452"Figure 6: Rate of convergence for belief propagation (BP)and dual decomposition (DD) with maximum k = 1000.41 (2.6%) for DD in reverse (6 in AST).5.3 Parsing SpeedBecause the C&C parser with AST is very fast, wewondered about the effect on speed for our model.We measured the runtime of the algorithms underthe condition that we stopped at a particular iteration(Table 6).
Although our models improve substan-tially over C&C, there is a significant cost in speedfor the best result.5.4 Training the Integrated ModelIn the experiments reported so far, the parsing andsupertagging models were trained separately, andonly combined at test time.
Although the outcomeof these experiments was successful, we wonderedif we could obtain further improvements by trainingthe model parameters together.Since the gradients produced by (loopy) BPare approximate, for these experiments we used astochastic gradient descent (SGD) trainer (Bottou,2003).
We found that the SGD parameters describedby Finkel et al (2008) worked equally well for ourmodels, and, on the baseline, produced similar re-sults to L-BFGS.
Curiously, however, we found thatthe combined model does not perform as well when477AST Reversesent/sec LF sent/sec LFBaseline 65.8 87.38 5.9 87.36BPk=1 60.8 87.70 5.8 88.35BPk=5 46.7 87.70 4.7 88.34BPk=25 35.3 87.70 3.5 88.33DDk=1 64.6 87.40 5.9 87.38DDk=5 41.9 87.65 3.1 88.09DDk=25 32.5 87.71 1.9 88.29Table 6: Parsing time in seconds per sentence (vs. F-measure) on section 00.AST ReverseLF UF ST LF UF STBaseline 86.7 92.7 94.0 86.7 92.7 93.9BP inf 86.8 92.8 94.1 87.2 93.1 94.2BP train 86.3 92.5 93.8 85.6 92.1 93.2Table 7: Results of training with SGD on approximategradients from LPB on section 00.
We test LBP in bothinference and training (train) as well as in inference only(inf); a maximum number of 10 iterations is used.the parameters are trained together (Table 7).
A pos-sible reason for this is that we used a stricter su-pertagger beam setting during training (Clark andCurran, 2007) to make training on a single machinepractical.
This leads to lower performance, particu-larly in the Reverse condition.
Training a model us-ing DD would require a different optimization algo-rithm based on Viterbi results (e.g.
the perceptron)which we will pursue in future work.6 Conclusion and Future WorkOur approach of combining models to avoid thepipeline problem (Felzenszwalb and McAllester,2007) is very much in line with much recent workin NLP.
Such diverse topics as machine transla-tion (Dyer et al, 2008; Dyer and Resnik, 2010;Mi et al, 2008), part-of-speech tagging (Jiang etal., 2008), named entity recognition (Finkel andManning, 2009) semantic role labelling (Sutton andMcCallum, 2005; Finkel et al, 2006), and oth-ers have also been improved by combined models.Our empirical comparison of BP and DD also com-plements the theoretically-oriented comparison ofmarginal- and margin-based variational approxima-tions for parsing described by Martins et al (2010).We have shown that the aggressive pruning usedin adaptive supertagging significantly harms the or-acle performance of the parser, though it mostlyprunes bad parses.
Based on these findings, we com-bined parser and supertagger features into a singlemodel.
Using belief propagation and dual decom-position, we obtained more principled?and moreaccurate?approximations than a pipeline.
Mod-els combined using belief propagation achieve verygood performance immediately, despite an initialconvergence rate just over 1%, while dual decompo-sition produces comparable results after several iter-ations, and algorithmically converges more quickly.Our best result of 88.8% represents the state-of-theart in CCG parsing accuracy.In future work we plan to integrate the POS tag-ger, which is crucial to parsing accuracy (Clark andCurran, 2004b).
We also plan to revisit the ideaof combined training.
Though we have focused onCCG in this work we expect these methods to beequally useful for other linguistically motivated butcomputationally complex formalisms such as lexi-calized tree adjoining grammar.AcknowledgementsWe would like to thank Phil Blunsom, PrachyaBoonkwan, Christos Christodoulopoulos, StephenClark, Michael Collins, Chris Dyer, TimothyFowler, Mark Granroth-Wilding, Philipp Koehn,Terry Koo, Tom Kwiatkowski, Andre?
Martins, MattPost, David Smith, David Sontag, Mark Steed-man, and Charles Sutton for helpful discussion re-lated to this work and comments on previous drafts,and the anonymous reviewers for helpful comments.We also acknowledge funding from EPSRC grantEP/P504171/1 (Auli); the EuroMatrixPlus projectfunded by the European Commission, 7th Frame-work Programme (Lopez); and the resources pro-vided by the Edinburgh Compute and Data Fa-cility (http://www.ecdf.ed.ac.uk).
TheECDF is partially supported by the eDIKT initiative(http://www.edikt.org.uk).ReferencesS.
Bangalore and A. K. Joshi.
1999.
Supertagging: AnApproach to Almost Parsing.
Computational Linguis-478tics, 25(2):238?265, June.Y.
Bar-Hillel, M. Perles, and E. Shamir.
1964.
On formalproperties of simple phrase structure grammars.
InLanguage and Information: Selected Essays on theirTheory and Application, pages 116?150.L.
Bottou.
2003.
Stochastic learning.
In Advanced Lec-tures in Machine Learning, pages 146?168.S.
Clark and J. R. Curran.
2004a.
The importance of su-pertagging for wide-coverage CCG parsing.
In COL-ING, Morristown, NJ, USA.S.
Clark and J. R. Curran.
2004b.
Parsing the WSJ usingCCG and log-linear models.
In Proc.
of ACL, pages104?111, Barcelona, Spain.S.
Clark and J. R. Curran.
2007.
Wide-Coverage Ef-ficient Statistical Parsing with CCG and Log-LinearModels.
Computational Linguistics, 33(4):493?552.S.
Clark.
2002.
Supertagging for Combinatory Catego-rial Grammar.
In TAG+6.G.
B. Dantzig and P. Wolfe.
1960.
Decompositionprinciple for linear programs.
Operations Research,8(1):101?111.M.
Dreyer and J. Eisner.
2009.
Graphical models overmultiple strings.
In Proc.
of EMNLP.C.
Dyer and P. Resnik.
2010.
Context-free reordering,finite-state translation.
In Proc.
of HLT-NAACL.C.
J. Dyer, S. Muresan, and P. Resnik.
2008.
Generaliz-ing word lattice translation.
In Proc.
of ACL.P.
F. Felzenszwalb and D. McAllester.
2007.
The Gener-alized A* Architecture.
In Journal of Artificial Intelli-gence Research, volume 29, pages 153?190.J.
R. Finkel and C. D. Manning.
2009.
Joint parsing andnamed entity recognition.
In Proc.
of NAACL.
Associ-ation for Computational Linguistics.J.
R. Finkel, C. D. Manning, and A. Y. Ng.
2006.
Solv-ing the problem of cascading errors: ApproximateBayesian inference for linguistic annotation pipelines.In Proc.
of EMNLP.J.
R. Finkel, A. Kleeman, and C. D. Manning.
2008.Feature-based, conditional random field parsing.
InProceedings of ACL-HLT.T.
A. D. Fowler and G. Penn.
2010.
Accurate context-free parsing with combinatory categorial grammar.
InProc.
of ACL.J.
Hockenmaier and M. Steedman.
2002.
Generativemodels for statistical parsing with Combinatory Cat-egorial Grammar.
In Proc.
of ACL.J.
Hockenmaier and M. Steedman.
2007.
CCGbank:A corpus of CCG derivations and dependency struc-tures extracted from the Penn Treebank.
Computa-tional Linguistics, 33(3):355?396.L.
Huang.
2008.
Forest Reranking: Discriminative pars-ing with Non-Local Features.
In Proceedings of ACL-08: HLT.W.
Jiang, L. Huang, Q. Liu, and Y. Lu?.
2008.
A cas-caded linear model for joint Chinese word segmen-tation and part-of-speech tagging.
In Proceedings ofACL-08: HLT.N.
Komodakis, N. Paragios, and G. Tziritas.
2007.MRF optimization via dual decomposition: Message-passing revisited.
In Proc.
of Int.
Conf.
on ComputerVision (ICCV).T.
Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-tag.
2010.
Dual Decomposition for Parsing with Non-Projective Head Automata.
In In Proc.
EMNLP.F.
R. Kschischang, B. J. Frey, and H.-A.
Loeliger.
1998.Factor graphs and the sum-product algorithm.
IEEETransactions on Information Theory, 47:498?519.J.
K. Kummerfeld, J. Rosener, T. Dawborn, J. Haggerty,J.
R. Curran, and S. Clark.
2010.
Faster parsing bysupertagger adaptation.
In Proc.
of ACL.A.
F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar,and M. A. T. Figueiredo.
2010.
Turbo parsers: Depen-dency parsing by approximate variational inference.In Proc.
of EMNLP.D.
McAllester, M. Collins, and F. Pereira.
2008.
Case-factor diagrams for structured probabilistic modeling.Journal of Computer and System Sciences, 74(1):84?96.H.
Mi, L. Huang, and Q. Liu.
2008.
Forest-based trans-lation.
In Proc.
of ACL-HLT.J.
Pearl.
1988.
Probabilistic Reasoning in IntelligentSystems: Networks of Plausible Inference.
MorganKaufmann.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In Proc.
of ACL.A.
M. Rush, D. Sontag, M. Collins, and T. Jaakkola.2010.
On dual decomposition and linear program-ming relaxations for natural language processing.
InIn Proc.
EMNLP.T.
Sato.
2007.
Inside-outside probability computationfor belief propagation.
In Proc.
of IJCAI.D.
A. Smith and J. Eisner.
2008.
Dependency parsing bybelief propagation.
In Proc.
of EMNLP.P.
Smyth, D. Heckerman, and M. Jordan.
1997.
Prob-abilistic independence networks for hidden Markovprobability models.
Neural computation, 9(2):227?269.D.
Sontag, A. Globerson, and T. Jaakkola.
2010.
Intro-duction to dual decomposition.
In S. Sra, S. Nowozin,and S. J. Wright, editors, Optimization for MachineLearning.
MIT Press.M.
Steedman.
2000.
The syntactic process.
MIT Press,Cambridge, MA.C.
Sutton and A. McCallum.
2005.
Joint parsing andsemantic role labelling.
In Proc.
of CoNLL.479C.
Sutton and A. McCallum.
2010.
An introduction toconditional random fields.
arXiv:stat.ML/1011.4088.J.
Yedidia, W. Freeman, and Y. Weiss.
2001.
Generalizedbelief propagation.
In Proc.
of NIPS.480
