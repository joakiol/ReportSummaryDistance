IIIIIIIIIIIIIIIIIIIIIIReconciliation of Unsupervised Clustering, Segmentation and CohesionDavid M.  W. PowersDepartment of Computer ScienceThe Flinders University of South Australiapowers@acm.orgAbstractThis extended abstract examines the progress of a projecton unsupervised language learning, and focuses on twodifferent approaches to segmentation, as well as howcohesion may be generalized from it definitive morpho-syntactic instantiation.
It is intended as a discussionpaper, and outlines the specific hypotheses currenltybeing tested.1.
Introduction and Mot ivat ionThis extended abstract summarize recent and currentwork being carried out by my group in relation tounsupervised learning of language.The work described is unsupervised in the sense that?
there is no human preproeessing of the raw corpora;?
there is no preconceived target grammar;?
there is a minimum of imposed formalism;?
there is no tuning for particular languages/datasets.The methodology used has been inspired by linguistic,psycholinguistie and cognitivist research in language andvision.
Whilst a connectionist framework is not ingeneral used, the approach is intended to beneurologically plausbile, although there is more directinfluence from probability theory and informationtheory, and some experiments have used self-organizingneural nets (Powers, 1989; Schifferdecker, 1994).Our earliest work focussed on learning syntax byfinding statistical correlations or using self-organizingtime-delay networks (Powers, 1989).
Hierarchicalgrammars were produced by introducing newly foundrelationships as new candidates for correlation.Reasonable grammars were produced only for trainingdate of consistent short length phrases or sentences, butthe experiment lead to significant insights: the closedclass elements (function words in the initial experiments)were learned first, and these acted like seeds whichexpanded into larger and larger grammaticallymeaningful units.This research was subsequently generalized to a binaryclustering approach inspired by Pike's Phonemic (1949)and Tagmemic methodologies (1977).
Tokens thatfunction similarly in some sense (phonological,morphological, syntactic or semantic) but representsystematic rather than free variation, will formComplementary Distributions or classes.
However, freedistributions and distributions based on correlations withinformation which is unavailable at the current level ofanalysis will not be distinguishable, and thuscomplementarity will not be established, and may onlybe assumed under the hypothesis that there is no suchthing as free variation or arbitrariness, and that apparentfree variation always has causal roots.
Thecomplementary method of Pike is Contrast in Identical orAnalogous Environments (CIE/CAE) and assumes thatthere is extraneous information about whether or not theunits belong to the same emic unit or not.
I f  we do notmake any use of extra-sentential information, and ratherassume that all units with distinct forms at the currentlevel convey distinct information, but where they occurin the same set of contexts (coset) they are treated assimilar and belong to the same class and convey somecommon information.
The set of units which earl occur ina contextual coset are Contextual Distributions and havethe same basic character as ComplementaryDistributions, and are thus treated as such.2.
Speech and PhonologyWhilst the outlawing of free variation looks like heresyat the speech and phonetic levels, it simply means thatinformation that is not relevant to a phonemictranscription is discarded as useless in models that allowfree variation, while in deeper models, explanationsshould be available.
One of our current projects isfocussing precisely on this supraphonemic information,both from the perspective of capturing supralinguisticinformation (speaker atuibutes/mood etc.)
for its ownsake and with a goal of tracking speakers againstcomplex auditory backgrounds.Schifferdecker (1994) has successfully used thetechnique to produce phonemes from raw speech data aswell as from raw phonetic transcriptions, although thiswork did not explore the hierarchical aspects of thetechnique (except as a consequence of dendriticrepresentation f the classification space).3.
Semantics and SynonymyAt the semantic level, it supports our denial of thePowers 307 Unsupervised Clustering, Segmentation a d CohesionDavid M. W. Powers (1998) Reconciliation of Unsupervised Clustering, Segmentation a d Cohesion.
In D.M.W.
Powers(ed.)
NeMLaP3/CoNLL 98 Workshop on Paradigms and Grounding inLanguage Learning, ACL, pp 307-310.existence of pure synonymy.
Thus words like 'too' and'also' which are apparent synonyms have quite differentsyntactic constraints, while words like "small' and 'little'which are apparent synonyms and appear to occupy thesame same syntactic role, actually have quite differentconnotations.
Thus 'a small boy' is small for his age,whilst 'a little boy' is a young child, and 'a small littleboy' combines these implications; the tendency for'little' to prefer and be preferred when a metaphoricalinterpretation is appropriate is confirmed by idioms like'a little while' and 'a little bit" whereas 'small' tends tohave more direct connection to the underlying spatialinterpretation, and when used in a metaphorical ortemporal context i thus tends to reinforce the metaphorand supply additional emphasis - -  contrast 'except forone little detail' and 'except for one small detail'.
Ofcourse, any examples of this sort are highly influenced bythe specific language, dialect and idiolect of the speakerand may vary at each level.Many researchers have used clustering techniques toinduce semantic lasses (e.g.
Finch, 1993), althoughthese have tended again to be non-hierarchical except tothe extent that a pairwise clustering technique induces adendritic structure on the semantic space (although Finchdid perform two levels of analysis in one experiment).4.
Morphology and SyntaxAt the syntactic level the assumption of absence of freevariation is not so controversial, nd although generativegrammarians have tended to treat some choices asarbitrary, e.g.
the choice between active and passive,which is probably more a function of their ignoringpragmatics to focus on grammar.
At the level ofmorphology, what may appear to be free variationsynchronically usually has a diachronic explanation, andinvariably involves clear complementarity in terms of thedistribution of allomorph according to the syntactic roleof the embedding word.The experiments ofPowers (1992) demonstrated bothlearning of classes and hierarchical rules from thecharacter level up to the level of simple noun phrases andsimple clauses.
As is the case with subjaeency, nounphrases and clauses tend to act similarly, and indeed wepropose that they themselves form a complementarydistribution (involving their multiple forms, includingnominalized clauses and verbs: 'he wanted the girl tocome', 'the girl must come', 'he decided that the girlshould come', 'he decided the girl could come') andsuggests a generalization of the finiteness feature ofverbs should apply to both nouns and verbs and theirdominated structures ('to' and 'that' axe both optionalmarkers of the infinite form; the finite form would appearto be the default role of a verb and the unmarked form).5.
Segmentation and GrammarBoth Powers (1989) and Powers (1994) depend for theirhierarchical organization on a fuzzy approach tosegments.
At the word level, Powers (I 989) allowed fourhypotheses: a word should group with the word to the leftor the word to the right, or with a phrase to the left of aphrase to the fight, where a phrase has previously beenrecognized as a candidate group.
Hypotheses were ratedaccording to their usage, and those involved in the mosthighly rated overall parse were reinforced.
Powers(1992) allowed one or two (or in some xperiments three)given or induced units to operate as a putative unit for thepurposes of distributional analysis.
Apart fromthresholding (to eliminate noise, and to make it amenableto the small computer available), frequency informationwas ignored and each context was associated with a cosetof(one to three) units on either side.
Classes were formedby a technique which tunas out to be clustering using aHamming distance of 2 (or 3 in some experiments), inwhich classes can be merged (union) and the eornmoncoset determined (intersection).The size and coverage of the individual left and fightcosets and their union and intersection gave eightmeasures of the strength of a class, and in all easesidentified the vowels as the strongest class for theoriginal dictionary corpus, and for most other corporatried, with right context appearing more useful than left,eoset size being more accurate than coset coverage,union size being more reliable than intersection size.Note that Powers (1997a) generalizes the approach andconsiders a multitude of different clustering metrics andmethods, introducing a pair of goodness measures whichallow a more principled approach to closing andevaluating clusters (rather than closing at a specificcluster, you close when the goodness measure r aches itsfirst local maximum).In the Powers (1992) experiments, classes were addedas new units and the process was repeated.
The fuzzyvariable size candidate units for the next level meant hathyperelasses of context-free rules were learned.However the grammar led to high levels of ambiguityusing non-deterministic parsing, and the presentedhierarchy is based arbitrarily on a simple greedyapproach, but (for this reason) performance as arecognizer/parser was not evaluated.Though in this work phonologically, morphologicallyand grammatically meaningful classes and structure wereformed, up to phrase/clause level, no interpretation f thestructures or classes was offered, and no attempt wasmade to discover or propose cohesive constraints orsemantic relationships.
At the same time however,Entwisle and Groves (1994), Powers (1997"o) andEntwisle and Posers (1997) have produced a constraintPowers 308 Unsupervised Clustering, Segmentation a d CohesionIIIlIIII|IIIIIIIIIIIIIIIIIIIIiIIparser which uses precisely the kind of morphologicaland grammatical c asses which are thro:wn, up by the self-organizing and clustering experiments, and have startedto address how one develop meaningful statistics for atrue grammar learning system without any preconceivednotions of what the correct parse/phrase tructure is (ifany).
In particular Powers (1997b) performedexperiments in the context of grammat checkingapplication, using automatic segmentation techniquesbased on those of Harris (I 960) and similar to those usedby Brent (1997), but combined with context-conditionedprobabilities which were used to decide betweenconfusable words.
The same technique has been appliedin a Loebner Prize entry by Bastin and Cordier (1997).This gives us two competing approaches tosegmentation.
I  the first, segmentation is a side effect ofthe fuzzification of input units during classification (thesegments chosen are those which give the bestclassification according to some metric).
Incidentally,Powers (1992) also reports work in which hyphenationpoints were marked, thus introducing an element ofsupervision, but it did not improve performance (whichagained suffered from ambiguity and thus didn't producedefinite results, being non-probabilistie, although agreedy algorithm performed quite reasonably).
Thesecond (Harris) approach examines the conditionalinformation or perplexity for each possible prefix/suffixto determine likely segmentation points - -  which isexpected to show a local maximum in the perplexity.6.
Reconciling the MethodsThe Harris (1960) approach works on the insight thatwithin a unit, particularly a closed class functional unitsuch as an affix, there is less freedom of choice than at theboundary of units.
This depends trongly on the fact thatthe number of affixes is much lower than individualcharacters, whilst their frequency is so much higher thanthe morphs they collate with.
Viz.
they define largecosets.The Powers (1992) approach works by finding thegroups of segments which have the largest cosets, andthus have high frequency and low information, theirinformation content tending to be more syntactic thansemantic.
The segmentation and classification occursimultaneously, and it seems there is no advantage todoing perplexity-based segmentation before doing theclassification, although this has not yet been investigated.The segmentation process may however be repeated,finding the subsequent perplexity or informationmaxima.
In addition, even the initial functional segmentsfound may be used directly to learn or check a grammar(Entwisle and Groves, 1994; Powers, 1997b), althoughthis already makes use of the known word segmentationand the assumption, which is for English is an excellentfirst approximation, that affixes are either word initial orword final, and that it is this prefixes and suffixes whichdetermine the syntactic roles of the words.7.
Augmenting the MethodsThe approach used by Entwisle and Groves (1994) isonly semi-automatic, and wasn't originally conceived asa learning system.
When a sentence fails to parse, itmeans that a constraint must be relaxed, and thisconstraint is identified manually--being a system whichinvolves no statistics, which is being trained on textwhich may contain errors (e.g.
one error was discoveredin the first chapter of the Alice Corpus, Carroll, 1865),and where the relaxation may involve the supplying ofnew roles or the removal of a' constraint at any one of anumber of possible points.The approach used by Powers (1997b) is only intendedto identify typing errors and substitution errors (e.g.
'there' for 'their') and builds and stores a differentialgrammar only when the word can be disambiguated fromits closed-class context, but already constraints based onthe closed class words and functional affixes suffices toperform better than commercial grammar checkers.The segmentation and classification methods on theirown do not attempt to cheek cohesive constraints, uch asagreement, but doing so could be expected toreduce theambiguity which is so rife.
Powers (1992) reports oneword with around 5000 different 'parses'.The specific approach we are using in our current workis to extend the structure determined by a version of theapproach of Powers (1992 and 1997a) which producesbinary grammar rules.
The extended structure augmentsa higher level unit with features constructed from orinherited from the lower level units.
This construction isbeing carded out virtually at present, while we examinethe best way to propogate information, and weinvestigate and seek to differentiate the specifichypotheses that (a) the more frequent, or Co) the higherperplexity, segments play the morpho-syntactic cohesiveroles, whilst their binary siblings hold the primarycontent to be retained and passed on.Whilst his strategy isthe one suggested by the primarymorphological cohesion, and could straightforwardly beapplied after a single segmentation pass, using thehierarchical classification approach produces a farstronger hypothesis, predicting that vowels in English,where they are strongest under both conditions (a) andCo), play a primarily structural or phonological role, andthat affixes, prepositions, articles, relatives, conjunctionsand the like act as the heads of their superordinatestructures.An additional aim of the present project is to seek toPowers 309 Unsupervised Clustering, Segmentation a d Cohesiontease apart homonyms and their manifestations at theother levels, including the dual role of the letter 'y'(sometimes clearly vowel as in 'xylophone', sometimesambiguously consonantal s in 'play, playing, played'),the suffix '-s' and the word 'to'.
In Powers (1997a) both'y' and space were identified as vowels using certainclustering techniques and methods (and the issues arediscussed in that paper).
We are generalizing theapproach of identifying a class, such as the vowels, andthen identifying those units, such as 'y', which atypicallyhave a larger coset han the class which has been selectedas having maximal coverage (resolving the Powers(1992) dilemma in favour of coverage as the preferredmetric).8.
Discussion and ConclusionThis extended abstract documents work in progress,contrasting existing approaches in recent publicationsand setting out the direction we are following.Preliminary results hould be available at the workshop,but the paper is mainly intended to provoke discussion ofthe pro's and con's of the two approaches tosegmentation.9.
ReferencesBastin, V. and Cordier, D. (1998).
Methods and tricks usedin an attempt to pas the Turing Test, NeMLaP3/CoNLL98Workshop on Human Computer Conversation.Brent, M.R.
(1997).
aUnified Model of Lexical Acquisitionand Lexical Access.
Journal of PsycholinguisticResearch 26:363-375.Carroll, L. (1865).
Alice's Adventures in Wonderland.
TheMillennium Fulcrum Edition 2.9, Gutenberg Project.Entwisle, J. and Grovers, M. (1994).
A Method of ParsingEnglish Based on Sentence Form, NeMLaP, 116-122.Entwisle, J. and Powers, D.M.W.
(1998).
The Present Useof Statistics in the Evaluation of NLP Parsers.NeMLaP3/CoNLL98 Joint Conference.Finch, S. (1993) Finding Structure in Language.
Ph.DDissertation, University of EdinburghHarris, Z.
(1960) Structural Linguistics.
University ofChicago PressPike.
K. (1949) Phonemics, University of Michigan PressPike, K. and Pike, E. (1977) Grammatical Analysis, SILPowers, D.M.W.
and Turk, C.C.R.
(1989).
MachineLearning of Natural Language.
Springer- VedagPowers, D.M.W.
(1992).
On the significance of ClosedClasses and Boundary Conditions: Experiments inLExical and Syntactic Learning,.
SHOE 245-266.
ITK,University of Tilburg, NLPowers.
D.M.W.
(1997a).
Unsupervised Learning ofLinguistic Structure: An Empirical Evaluation, Journalof Corpus LinguisticsPowers, D.M.W.
(1997b).
Learning and Application ofDifferential Grammars.
CoNLL97, ACL, Madrid, SpainRissanen, J.
(1989).
Stochastic Complexity in StatisticalInquiry.
Singapore:World ScientificShannon, C.E.
and Weaver, W. (1949) The MathematicalTheory of Communication.
Urbana: U. Illinois PressSchifferdecker, G. (1994) Finding Structure inLanguage.Diplom Thesis, University of Karlsruhe.Zipf, G.K. (1949) Human Behaviour and the Principle ofLeast Effort: An Introduction toHuman Ecology.
AWPowers 310 Unsupervised Clustering, Segmentation a d CohesionII11II1lIIl11lIIIIII
