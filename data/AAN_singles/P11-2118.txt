Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 670?675,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Hierarchical Model of Web SummariesYves Petinot and Kathleen McKeown and Kapil ThadaniDepartment of Computer ScienceColumbia UniversityNew York, NY 10027{ypetinot|kathy|kapil}@cs.columbia.eduAbstractWe investigate the relevance of hierarchicaltopic models to represent the content of Webgists.
We focus our attention on DMOZ,a popular Web directory, and propose twoalgorithms to infer such a model from itsmanually-curated hierarchy of categories.
Ourfirst approach, based on information-theoreticgrounds, uses an algorithm similar to recur-sive feature selection.
Our second approachis fully Bayesian and derived from the moregeneral model, hierarchical LDA.
We evalu-ate the performance of both models against aflat 1-gram baseline and show improvementsin terms of perplexity over held-out data.1 IntroductionThe work presented in this paper is aimed at lever-aging a manually created document ontology tomodel the content of an underlying document col-lection.
While the primary usage of ontologies isas a means of organizing and navigating documentcollections, they can also help in inferring a signif-icant amount of information about the documentsattached to them, including path-level, statistical,representations of content, and fine-grained viewson the level of specificity of the language used inthose documents.
Our study focuses on the ontologyunderlying DMOZ1, a popular Web directory.
Wepropose two methods for crystalizing a hierarchicaltopic model against its hierarchy and show that theresulting models outperform a flat unigram model inits predictive power over held-out data.1http://www.dmoz.orgTo construct our hierarchical topic models, weadopt the mixed membership formalism (Hofmann,1999; Blei et al, 2010), where a document is rep-resented as a mixture over a set of word multi-nomials.
We consider the document hierarchy H(e.g.
the DMOZ hierarchy) as a tree where internalnodes (category nodes) and leaf nodes (documents),as well as the edges connecting them, are known apriori.
Each node Ni in H is mapped to a multi-nomial word distribution MultNi , and each path cdto a leaf node D is associated with a mixture overthe multinonials (MultC0 .
.
.MultCk ,MultD) ap-pearing along this path.
The mixture componentsare combined using a mixing proportion vector(?C0 .
.
.
?Ck), so that the likelihood of string w be-ing produced by path cd is:p(w|cd) =|w|?i=0|cd|?j=0?jp(wi|cd,j) (1)where:|cd|?j=0?j = 1,?d (2)In the following, we propose two models that fitin this framework.
We describe how they allow thederivation of both p(wi|cd,j) and ?
and present earlyexperimental results showing that explicit hierarchi-cal information of content can indeed be used as abasis for content modeling purposes.2 Related WorkWhile several efforts have focused on the DMOZcorpus, often as a reference for Web summarization670tasks (Berger and Mittal, 2000; Delort et al, 2003)or Web clustering tasks (Ramage et al, 2009b), verylittle research has attempted to make use of its hier-archy as is.
The work by Sun et al (2005), wherethe DMOZ hierarchy is used as a basis for a hierar-chical lexicon, is closest to ours although their con-tribution is not a full-fledged content model, but aselection of highly salient vocabulary for every cat-egory of the hierarchy.
The problem considered inthis paper is connected to the area of Topic Modeling(Blei and Lafferty, 2009) where the goal is to reducethe surface complexity of text documents by mod-eling them as mixtures over a finite set of topics2.While the inferred models are usually flat, in thatno explicit relationship exists among topics, morecomplex, non-parametric, representations have beenproposed to elicit the hierarchical structure of vari-ous datasets (Hofmann, 1999; Blei et al, 2010; Liet al, 2007).
Our purpose here is more specializedand similar to that of Labeled LDA (Ramage et al,2009a) or Fixed hLDA (Reisinger and Pas?ca, 2009)where the set of topics associated with a document isknown a priori.
In both cases, document labels aremapped to constraints on the set of topics on whichthe - otherwise unaltered - topic inference algorithmis to be applied.
Lastly, while most recent develop-ments have been based on unsupervised data, it isalso worth mentioning earlier approaches like TopicSignatures (Lin and Hovy, 2000) where words (orphrases) characteristic of a topic are identified usinga statistical test of dependence.
Our first model ex-tends this approach to the hierarchical setting, build-ing actual topic models based on the selected vocab-ulary.3 Information-Theoretic ApproachThe assumption that topics are known a-priori al-lows us to extend the concept of Topic Signatures toa hierarchical setting.
Lin and Hovy (2000) describea Topic Signature as a list of words highly correlatedwith a target concept, and use a ?2 estimator overlabeled data to decide as to the allocation of a wordto a topic.
Here, the sub-categories of a node corre-spond to the topics.
However, since the hierarchy isnaturally organized in a generic-to-specific fashion,2Here we use the term topic to describe a normalized distri-bution over a fixed vocabulary V .for each node we select words that have the least dis-criminative power between the node?s children.
Therationale is that, if a word can discriminate well be-tween one child and all others, then it belongs in thatchild?s node.3.1 Word AssignmentThe algorithm proceeds in two phases.
In the firstphase, the hierarchy tree is traversed in a bottom-upfashion to compile word frequency information un-der each node.
In the second phase, the hierarchyis traversed top-down and, at each step, words getassigned to the current node based on whether theycan discriminate between the current node?s chil-dren.
Once a word has been assigned on a givenpath, it can no longer be assigned to any other nodeon this path.
Thus, within a path, a word alwaystakes on the meaning of the one topic to which it hasbeen assigned.The discriminative power of a term with respectto node N is formalized based on one of the follow-ing measures:Entropy of the a posteriori children category dis-tribution for a given w.Ent(w) = ?
?C?Sub(N)p(C|w) log(p(C|w) (3)Cross-Entropy between the a priori children cat-egory distribution and the a posteriori children cate-gories distribution conditioned on the appearance ofw.CrossEnt(w) = ?
?C?Sub(N)p(C) log(p(C|w)) (4)?2 score, similar to Lin and Hovy (2000) but ap-plied to classification tasks that can involve an ar-bitrary number of (sub-)categories.
The number ofdegrees of freedom of the ?2 distribution is a func-tion of the number of children.
?2(w) =?i?{w,w}?C?Sub(N)(nC(i)?
p(C)p(i))2p(C)p(i)(5)To identify words exhibiting an unusually low dis-criminative power between the children categories,we assume a gaussian distribution of the score usedand select those whose score is at least ?
= 2 stan-dard deviations away from the population mean3.3Although this makes the decision process less arbitrary671Algorithm 1 Generative process for hLLDA?
For each topic t ?
H?
Draw ?t = (?t,1, .
.
.
, ?t,V )T ?
Dir(?|?)?
For each document, d ?
{1, 2 .
.
.K}?
Draw a random path assignment cd ?
H?
Draw a distribution over levels along cd, ?d ?Dir(?|?)?
Draw a document length n ?
?H?
For each word wd,i ?
{wd,1, wd,2, .
.
.
wd,n},?
Draw level zd,i ?Mult(?d)?
Draw word wd,i ?Mult(?cd [zd,i])3.2 Topic Definition & Mixing ProportionsBased on the final word assignments, we estimatethe probability of word wi in topic Tk, as:P (wi|Tk) =nCk(wi)nCk(6)with nCk(wi) the total number of occurrence of wiin documents under Ck, and nCk the total number ofwords in documents under Ck.Given the individual word assignments we eval-uate the mixing proportions using corpus-level esti-mates, which are computed by averaging the mixingproportions of all the training documents.4 Hierarchical Bayesian ApproachThe previous approach, while attractive in its sim-plicity, makes a strong claim that a word can beemitted by at most one node on any given path.
Amore interesting model might stem from allowingsoft word-topic assignments, where any topic on thedocument?s path may emit any word in the vocabu-lary space.We consider a modified version of hierarchicalLDA (Blei et al, 2010), where the underlying treestructure is known a priori and does not have tobe inferred from data.
The generative story for thismodel, which we designate as hierarchical Labeled-LDA (hLLDA), is shown in Algorithm 1.
Just aswith Fixed Structure LDA4 (Reisinger and Pas?ca,than with a hand-selected threshold, this raises the issue of iden-tifying the true distribution for the estimator used.4Our implementation of hLLDA was partiallybased on the UTML toolkit which is available athttps://github.com/joeraii/2009), the topics used for inference are, for eachdocument, those found on the path from the hierar-chy root to the document itself.
Once the target pathcd ?
H is known, the model reduces to LDA overthe set of topics comprising cd.
Given that the jointdistribution p(?, z, w|cd) is intractable (Blei et al,2003), we use collapsed Gibbs-sampling (Griffithsand Steyvers, 2004) to obtain individual word-levelassignments.
The probability of assigning wi, theith word in document d, to the jth topic on path cd,conditioned on all other word assignments, is givenby:p(zi = j|z?i,w, cd) ?nd?i,j + ?|cd|(?+ 1)?nwi?i,j + ?V (?
+ 1)(7)where nd?i,j is the frequency of words from docu-ment d assigned to topic j, nwi?i,j is the frequencyof word wi in topic j, ?
and ?
are Dirichlet con-centration parameters for the path-topic and topic-word multinomials respectively, and V is the vocab-ulary size.
Equation 7 can be understood as defin-ing the unormalized posterior word-level assignmentdistribution as the product of the current level mix-ing proportion ?i and of the current estimate of theword-topic conditional probability p(wi|zi).
By re-peatedly resampling from this distribution we ob-tain individual word assignments which in turn al-low us to estimate the topic multinomials and theper-document mixing proportions.
Specifically, thetopic multinomials are estimated as:?cd[j],i = p(wi|zcd[j]) =nwizcd[j] + ?
?n?zcd[j] + V ?
(8)while the per-document mixing proportions ?d canbe estimated as:?d,j ?nd?,j + ?nd + |cd|?,?j ?
1, .
.
.
, cd (9)Although we experimented with hyper-parameterlearning (Dirichlet concentration parameter ?
), do-ing so did not significantly impact the final model.The results we report are therefore based on stan-dard values for the hyper-parameters (?
= 1 and?
= 0.1).5 Experimental ResultsWe compared the predictive power of our model tothat of several language models.
In every case, we672compute the perplexity of the model over the held-out dataW = {w1 .
.
.wn} given the modelM andthe observed (training) data, namely:perplM(W) = exp(?1nn?i=11|wi||wi|?j=1log pM(wi,j))(10)5.1 Data PreprocessingOur experiments focused on the English portion ofthe DMOZ dataset5 (about 2.1 million entries).
Theraw dataset was randomized and divided accordingto a 98% training (31M words), 1% development(320k words), 1% testing (320k words) split.
Gistswere tokenized using simple tokenization rules, withno stemming, and were case-normalized.
Akin toBerger and Mittal (2000) we mapped numerical to-kens to the NUM placeholder and selected the V =65535 most frequent words as our vocabulary.
Anytoken outside of this set was mapped to the OOV to-ken.
We did not perform any stop-word filtering.5.2 Reference ModelsOur reference models consists of several n-gram(n ?
[1, 3]) language models, none of which makesuse of the hierarchical information available fromthe corpus.
Under these models, the probability ofa given string is given by:p(w) =|s|?i=1p(wi|wi?1, .
.
.
,wi?
(n?1)) (11)We used the SRILM toolkit (Stolcke, 2002), en-abling Kneser-Ney smoothing with default param-eters.Note that an interesting model to include herewould have been one that jointly infers a hierarchyof topics as well as the topics that comprise it, muchlike the regular hierarchical LDA algorithm (Blei etal., 2010).
While we did not perform this experimentas part of this work, this is definitely an avenue forfuture work.
We are especially interested in seeingwhether an automatically inferred hierarchy of top-ics would fundamentally differ from the manually-curated hierarchy used by DMOZ.5We discarded the Top/World portion of the hierarchy.5.3 Experimental ResultsThe perplexities obtained for the hierarchical and n-gram models are reported in Table 1.reg all# documents 1153000 2083949avg.
gist length 15.47 15.361-gram 1644.10 1414.982-gram 352.10 287.093-gram 239.08 179.71entropy 812.91 1037.70cross-entropy 1167.07 1869.90?2 1639.29 1693.76hLLDA 941.16 983.77Table 1: Perplexity of the hierarchical models and thereference n-gram models over the entire DMOZ dataset(all), and the non-Regional portion of the dataset (reg).When taken on the entire hierarchy (all), the per-formance of the Bayesian and entropy-based mod-els significantly exceeds that of the 1-gram model(significant under paired t-test, both with p-value <2.2 ?
10?16) while remaining well below that of ei-ther the 2 or 3 gram models.
This suggests that, al-though the hierarchy plays a key role in the appear-ance of content in DMOZ gists, word context is alsoa key factor that needs to be taken into account: thetwo families of models we propose are based on thebag-of-word assumption and, by design, assume thatwords are drawn i.i.d.
from an underlying distribu-tion.
While it is not clear how one could extend theinformation-theoretic models to include such con-text, we are currently investigating enhancements tothe hLLDA model along the lines of the approachproposed in Wallach (2006).A second area of analysis is to compare the per-formance of the various models on the entire hier-archy versus on the non-Regional portion of the tree(reg).
We can see that the perplexity of the proposedmodels decreases while that of the flat n-grams mod-els increase.
Since the non-Regional portion of theDMOZ hierarchy is organized more consistently ina semantic fashion6, we believe this reflects the abil-ity of the hierarchical models to take advantage of6The specificity of the Regional sub-tree has also been dis-cussed by previous work (Ramage et al, 2009b), justifying aspecial treatment for that part of the DMOZ dataset.673Figure 1: Perplexity of the proposed algorithms against the 1-gram baseline for each of the 14 top level DMOZ cate-gories: Arts, Business, Computer, Games, Health, Home, News, Recreation, Reference, Regional, Science, Shopping,Society, Sports.the corpus structure to represent the content of thesummaries.
On the other hand, the Regional por-tion of the dataset seems to contribute a significantamount of noise to the hierarchy, leading to a loss inperformance for those models.We can observe that while hLLDA outperformsall information-theoretical models when applied tothe entire DMOZ corpus, it falls behind the entropy-based model when restricted to the non-regionalsection of the corpus.
Also if the reduction inperplexity remains limited for the entropy, ?2 andhLLDA models, the cross-entropy based model in-curs a more significant boost in performance whenapplied to the more semantically-organized portionof the corpus.
The reason behind such disparity inbehavior is not clear and we plan on investigatingthis issue as part of our future work.Further analyzing the impact of the respectiveDMOZ sub-sections, we show in Figure 1 re-sults for the hierarchical and 1-gram models whentrained and tested over the 14 main sub-trees ofthe hierarchy.
Our intuition is that differencesin the organization of those sub-trees might af-fect the predictive power of the various mod-els.
Looking at sub-trees we can see that thetrend is the same for most of them, with the bestlevel of perplexity being achieved by the hierar-chical Bayesian model, closely followed by theinformation-theoretical model using entropy as itsselection criterion.6 ConclusionIn this paper we have demonstrated the creation of atopic-model of Web summaries using the hierarchyof a popular Web directory.
This hierarchy providesa backbone around which we crystalize hierarchicaltopic models.
Individual topics exhibit increasingspecificity as one goes down a path in the tree.
Whilewe focused on Web summaries, this model can bereadily adapted to any Web-related content that canbe seen as a mixture of the component topics appear-ing along a paths in the hierarchy.
Such model canbecome a key resource for the fine-grained distinc-tion between generic and specific elements of lan-guage in a large, heterogenous corpus.AcknowledgmentsThis material is based on research supported in partby the U.S. National Science Foundation (NSF) un-der IIS-05-34871.
Any opinions, findings and con-clusions or recommendations expressed in this ma-terial are those of the authors and do not necessarilyreflect the views of the NSF.674ReferencesA.
Berger and V. Mittal.
2000.
Ocelot: a system forsummarizing web pages.
In Proceedings of the 23rdAnnual International ACM SIGIR Conference on Re-search and Development in Information Retrieval (SI-GIR?00), pages 144?151.David M. Blei and J. Lafferty.
2009.
Topic models.
In A.Srivastava and M. Sahami, editors, Text Mining: The-ory and Applications.
Taylor and Francis.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent dirichlet alocation.
JMLR, 3:993?1022.David M. Blei, Thomas L. Griffiths, and Micheal I. Jor-dan.
2010.
The nested chinese restaurant process andbayesian nonparametric inference of topic hierarchies.In Journal of ACM, volume 57.Jean-Yves Delort, Bernadette Bouchon-Meunier, andMaria Rifqi.
2003.
Enhanced web document sum-marization using hyperlinks.
In Hypertext 2003, pages208?215.Thomas L. Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
PNAS, 101(suppl.
1):5228?5235.Thomas Hofmann.
1999.
The cluster-abstraction model:Unsupervised learning of topic hierarchies from textdata.
In Proceedings of IJCAI?99.Wei Li, David Blei, and Andrew McCallum.
2007.
Non-parametric bayes pachinko allocation.
In Proceedingsof the Proceedings of the Twenty-Third Conference An-nual Conference on Uncertainty in Artificial Intelli-gence (UAI-07), pages 243?250, Corvallis, Oregon.AUAI Press.C.-Y.
Lin and E. Hovy.
2000.
The automated acqui-sition of topic signatures for text summarization.
InProceedings of the 18th conference on Computationallinguistics, pages 495?501.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009a.
Labeled lda: Asupervised topic model for credit attribution in multi-labeled corpora.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP 2009), Singapore, pages 248?256.Daniel Ramage, Paul Heymann, Christopher D. Man-ning, and Hector Garcia-Molina.
2009b.
Clusteringthe tagged web.
In Proceedings of the Second ACM In-ternational Conference on Web Search and Data Min-ing, WSDM ?09, pages 54?63, New York, NY, USA.ACM.Joseph Reisinger and Marius Pas?ca.
2009.
Latent vari-able models of concept-attribute attachment.
In ACL-IJCNLP ?09: Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural Language Pro-cessing of the AFNLP: Volume 2, pages 620?628, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Proc.
Intl.
Conf.
on Spoken Lan-guage Processing, vol.
2, pages 901?904, September.Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,Yuchang Lu, and Zheng Chen.
2005.
Web-page sum-marization using clickthrough data.
In SIGIR 2005,pages 194?201.Hanna M. Wallach.
2006.
Topic modeling: Beyond bag-of-words.
In Proceedings of the 23rd InternationalConference on Machine Learning, Pittsburgh, Penn-sylvania, U.S., pages 977?984.675
