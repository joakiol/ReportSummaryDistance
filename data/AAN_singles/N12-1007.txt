2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 60?69,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsEntity Clustering Across LanguagesSpence Green*, Nicholas Andrews?, Matthew R. Gormley?,Mark Dredze?, and Christopher D. Manning**Computer Science Department, Stanford University{spenceg,manning}@stanford.edu?Human Language Technology Center of Excellence, Johns Hopkins University{noa,mrg,mdredze}@cs.jhu.eduAbstractStandard entity clustering systems commonlyrely on mention (string) matching, syntacticfeatures, and linguistic resources like EnglishWordNet.
When co-referent text mentions ap-pear in different languages, these techniquescannot be easily applied.
Consequently, wedevelop new methods for clustering text men-tions across documents and languages simulta-neously, producing cross-lingual entity clusters.Our approach extends standard clustering algo-rithms with cross-lingual mention and contextsimilarity measures.
Crucially, we do not as-sume a pre-existing entity list (knowledge base),so entity characteristics are unknown.
On anArabic-English corpus that contains seven dif-ferent text genres, our best model yields a 24.3%F1 gain over the baseline.1 IntroductionThis paper introduces techniques for clustering co-referent text mentions across documents and lan-guages.
On the web today, a breaking news itemmay instantly result in mentions to a real-world entityin multiple text formats: news articles, blog posts,tweets, etc.
Much NLP work has focused on modeladaptation to these diverse text genres.
However, thediversity of languages in which the mentions appearis a more significant challenge.
This was particularlyevident during the 2011 popular uprisings in the Arabworld, in which electronic media played a prominentrole.
A key issue for the outside world was the aggre-gation of information that appeared simultaneouslyin English, French, and various Arabic dialects.To our knowledge, we are the first to consider clus-tering entity mentions across languages without a pri-ori knowledge of the quantity or types of real-worldentities (a knowledge base).
The cross-lingual set-ting introduces several challenges.
First, we cannotassume a prototypical name format.
For example,the Anglo-centric first/middle/last prototype used inprevious name modeling work (cf.
(Charniak, 2001))does not apply to Arabic names like Abdullah ibnAbd Al-Aziz Al-Saud or Chinese names like Hu Jin-tao (referred to as Mr. Hu, not Mr. Jintao).
Sec-ond, organization names often require both translit-eration and translation.
For example, the ArabicPP?K??
?Qg.??Q??
?General Motors Corp?
containstransliterations of 	PP?K??
?Qg.
?General Motors?,but a translation of??Q??
?Corporation?.Our models are organized as a pipeline.
First, foreach document, we perform standard mention detec-tion and coreference resolution.
Then, we use pair-wise cross-lingual similarity models to measure bothmention and context similarity.
Finally, we clusterthe mentions based on similarity.Our work makes the following contributions: (1)introduction of the task, (2) novel models for cross-lingual entity clustering of person and organization en-tities, (3) cross-lingual annotation of the NIST Auto-matic Content Extraction (ACE) 2008 Arabic-Englishevaluation set, and (4) experimental results using bothgold and automatic within-document processing.
Wewill release our software and annotations to supportfuture research.1.1 Task Description via a Simple ExampleConsider the toy corpus in Fig.
1.
The English docu-ments contain mentions of two people: Steven PaulJobs and Mark Elliot Zuckerberg.
Of course, the sur-face realization of Mr. Jobs?
last name in English isalso an ordinary nominal, hence the ambiguous men-tion string (absent context) in the second document.The Arabic document introduces an organization en-tity (Apple Inc.) along with proper and pronominalreferences to Mr. Jobs.
Finally, the French documentrefers to Mr. Jobs by the honorific ?Monsieur,?
and to60Jobs program details delayedSteve Jobs admired Mark ZuckerbergM.
Jobs, le fondateur d'Apple, est mort	?=?
E1E2E3==doc1:doc2:doc3:doc4:Figure 1: Clustering entity mentions across languages and documents.
The toy corpus contains English (doc1 anddoc2), Arabic (doc3), and French (doc4).
Together, the documents make reference to three real-world entities, theidentification of which is the primary objective of this work.
We use a separately-trained system for within-documentmention detection and coreference (indicated by the text boxes and intra-document links, respectively).
Our experimentalresults are for Arabic-English only.Apple without its corporate designation.Our goal is to automatically produce the cross-lingual entity clusters E1 (Mark Elliot Zuckerberg),E2 (Apple Inc.), and E3 (Steven Paul Jobs).
Both thetrue number and characteristics of these entities areunobserved.
Our models require two pre-processingsteps: mention detection and within-document coref-erence/anaphora resolution, shown in Fig.
1 by thetext boxes and intra-document links, respectively.
Forexample, in doc3, a within-document coreference sys-tem would pre-link 	QK.
?k.
joobz ?Jobs?
with the mascu-line pronoun ?
h ?his?.
In addition, the mention detec-tor determines that the surface form ?Jobs?
in doc2is not an entity reference.
For this within-documentpre-processing we use Serif (Ramshaw et al, 2011).1Our models measure cross-lingual similarity of thecoreference chains to make clustering decisions (?in Fig.
1).
The similarity models (indicated by the= and 6= operators in Fig.
1) consider both mentionstring and context similarity (?2).
We use the men-tion similarities as hard constraints, and the contextsimilarities as soft constraints.
In this work, we inves-tigate two standard constrained clustering algorithms(?3).
Our methods can be used to extend existing sys-tems for mono-lingual entity clustering (also knownas ?cross-document coreference resolution?)
to thecross-lingual setting.1Serif is a commercial system that assumes each documentcontains only one language.
Currently, there are no publicly avail-able within-document coreference systems for Arabic and manyother languages.
To remedy this problem, the CoNNL-2012shared task aims to develop multilingual coreference systems.2 Mention and Context SimilarityOur goal is to create cross-lingual sets of co-referentmentions to real-world entities (people, places, orga-nizations, etc.).
In this paper, we adopt the followingnotation.
LetM be a set of distinct text mentions in acollection of documents;C is a partitioning ofM intodocument-level sets of co-referent mentions (calledcoreference chains); E is a partitioning of C into setsof co-referent chains (called entities).
Let i, j be non-negative integers less than or equal to |M | and a, b benon-negative integers less than or equal to |C|.
Ourexperiments use a separate within-document corefer-ence system to createC, which is fixed.
We will learnE, which has size no greater than |C| since the set ofmono-lingual chains is the largest valid partitioning.We define accessor functions to access propertiesof mentions and chains.
For any mentionmi, definethe following functions: lang(mi) is the language;doc(mi) is the document containingmi; type(mi) isthe semantic type, which is assigned by the within-document coreference system.
We also extract a setof mention contexts S, which are the sentences con-taining each mention (i.e., |S| = |M |).We learn the partition E by considering mentionand context similarity, which are measured with sep-arate component models.2.1 Mention SimilarityWe use separate methods for within- and cross-language mention similarity.
The pairwise similarity61Arabic RulesH.
?
bH?
t H?
th h. ?
jh?
h p?
kh X?
dX?
thP?
r 	P?
z ??
s ??
sh??
s 	??
d ??
t 	??
th??
a??
g??
f??
q??
k ??
l ??
m 	??
n??
h @?
a ??
w ??
a??
ah ??
?
Z?
?English Rulesk?
c p?
b x?
ks e,i,o,u?
?Table 1: English-Arabic mapping rules to a common or-thographic representation.
???
indicates a null mapping.For English, we also lowercase and remove determinersand punctuation.
For Arabic, we remove the determiner?
@ Al ?the?
and the elongation character tatwil ??.of any two mentionsmi andmj is:sim(mi,mj) ={jaro-winkler(mi,mj) if lang(mi) = lang(mj)maxent(mi,mj) otherwiseJaro-Winkler Distance (within-language) Iflang(mi) = lang(mj), we use the Jaro-Winkler editdistance (Porter and Winkler, 1997).
Jaro-Winklerrewards matching prefixes, the empirical justificationbeing that less variation typically occurs at thebeginning of names.2 The metric produces a score inthe range [0,1], where 0 indicates equality.Maxent model (cross-language) When lang(mi)6= lang(mj), then the two mentions might be in dif-ferent writing systems.
Edit distance calculationsno longer apply directly.
One solution would befull-blown transliteration (Knight and Graehl, 1998),followed by application of Jaro-Winkler.
However,transliteration systems are complex and require sig-nificant training resources.
We find that a simpler,low-resource approach works well in practice.First, we deterministically map both languages to acommon phonetic representation (Tbl.
1).3 Next, wealign the mention pairs with the Hungarian algorithm,2For multi-token names, we sort the tokens prior to computingthe score, as suggested by Christen (2006).3This idea is reminiscent of Soundex, which Freeman et al(2006) used for cross-lingual name matching.Overlap Active for each bigram incbigrams(mi,u)?cbigrams(mj,v)Bigram-Diff-mi Active for each bigram incbigrams(mi)?
cbigrams(mj)Bigram-Diff-mj Active for each bigram incbigrams(mj)?
cbigrams(mi)Bigram-Len-Diff Value of abs(size(cbigrams(mi)?cbigrams(mj)))Big-Edit-Dist Count of token pairs withLev(mi,u,mj,v) > 3.0Total-Edit-Dist Sum of aligned token edit distancesLength Active for one of:len(mi) > len(mj) orlen(mi) < len(mj) orlen(mi) = len(mj)Length-Diff abs(len(mi)?
len(mj))Singleton Active if len(mi) = 1Singleton-Pair Active if len(mi) = len(mj) = 1Table 2: Cross-language Maxent feature templates for awhitespace-tokenized mention pair ?mi,mj?
with align-ment Ami,mj .
Let (u, v) ?
Ami,mj indicate aligned to-ken indices.
Define the following functions for strings:cbigrams(?)
returns the set of character bigrams; len(?)
isthe token length; Lev(?, ?)
is the Levenshtein edit distancebetween two strings.
Prior to feature extraction, we addunique start and end symbols to the mention strings.which produces a word-to-word alignment Ami,mj .4Finally, we build a simple binary Maxent classifierp(y|mi,mj ;?)
that extracts features from the alignedmentions (Tbl.
2).
We learn the parameters ?
using aquasi-Newton procedure with L1 (lasso) regulariza-tion (Andrew and Gao, 2007).2.2 Context Mapping and SimilarityMention strings alone are not always sufficient fordisambiguation.
Consider again the simple exam-ple in Fig.
1.
Both doc3 and doc4 reference ?SteveJobs?
and ?Apple?
in the same contexts.
Context co-occurence and/or similarity can thus disambiguatethese two entities from other entities with similar ref-erences (e.g., ?Steve Jones?
or ?Apple Corps?).
Aswith the mention strings, the contexts may originatein different writing systems.
We consider both high-and low-resource approaches for mapping contexts toa common representation.4The Hungarian algorithm finds an optimal minimum-costalignment.
For pairwise costs between tokens, we used the Lev-enshtein edit distance62Machine Translation (MT) For the high-resourcesetting, if lang(mi) 6=English, then we translate bothmi and its context si to English with an MT system.We use Phrasal (Cer et al, 2010), a phrase-basedsystem which, like most public MT systems, lacks atransliteration module.
We believe that this approachyields the most accurate context mapping for high-resource language pairs (like English-Arabic).Polylingual Topic Model (PLTM) The polylin-gual topic model (PLTM) (Mimno et al, 2009) isa generative process in which document tuples?groups of topically-similar documents?share a topicdistribution.
The tuples need not be sentence-aligned,so training data is easier to obtain.
For example, onedocument tuple might be the set of Wikipedia articles(in all languages) for Steve Jobs.Let D be a set of document tuples, wherethere is one document in each tuple for eachof L languages.
Each language has vocabu-lary Vl and each document dlt has Nlt tokens.We specify a fixed-size set of topics K. ThePLTM generates the document tuples as follows:Polylingual Topic Model?t ?
Dir(?K) [cross-lingual tuple-topic prior]?lk ?
Dir(?Vl) [word-topic prior]for each token wlt,n with n = {1, .
.
.
, Nlt}:zt,n ?
Mult(?t)wlt,n ?
Mult(?lzt,n)For cross-lingual context mapping, we infer the 1-best topic assignments for each token in all S mentioncontexts.
This technique reduces Vl = k for all l.Moreover, all languages have a common vocabulary:the set of K topic indices.
Since the PLTM is nota contribution of this paper, we refer the interestedreader to (Mimno et al, 2009) for more details.After mapping each mention context to a commonrepresentation, we measure context similarity basedon the choice of clustering algorithm.3 Clustering AlgorithmsWe incorporate the mention and context similaritymeasures into a clustering framework.
We considertwo algorithms.
The first is hierarchical agglomera-tive clustering (HAC), with which we assume basicfamiliarity (Manning et al, 2008).
A shortcoming ofHAC is that a stop threshold must be tuned.
To avoidthis requirement, we also consider non-parametricprobabilistic clustering in the form of a Dirichlet pro-cess mixture model (DPMM) (Antoniak, 1974) .Both clustering algorithms can be modified to ac-commodate pairwise constraints.
We have observedbetter results by encoding mention similarity as ahard constraint.
Context similarity is thus the clusterdistance measure.5To turn the Jaro-Winkler distance into a hardboolean constraint, we tuned a threshold ?
on held-outdata, i.e., jaro-winkler(mi,mj) ?
?
?
mi = mj .Likewise, the Maxent model is a binary classifier, sop(y = 1|mi,mj ;?)
> 0.5?
mi = mj .In both clustering algorithms, any two chains Caand Cb cannot share the same cluster assignment if:1.
Document origin: doc(Ca) = doc(Cb)2.
Semantic type: type(Ca) 6= type(Cb)3.
Mention Match: sim(mi,mj) = false,wheremi = repr(Ca) andmj = repr(Cb).The deterministic accessor function repr(Ca) returnsthe representative mention of a chain.
The heuristicwe used was ?first mention?
: the function returns theearliest mention that appears in the associated docu-ment.
In many languages, the first mention is typi-cally more complete than later mentions.
This heuris-tic also makes our system less sensitive to within-document coreference errors.6 The representativemention only has special status for mention similar-ity: context similarity considers all mention contexts.3.1 Constrained Hierarchical ClusteringHAC iteratively merges the ?nearest?
clusters accord-ing to context similarity.
In our system, each clustercontext is a bag of wordsW formed from the contextsof all coreference chains in that cluster.
For each wordinW we estimate a unigram Entity Language Model(ELM) (Raghavan et al, 2004):P (w) =countW (w) + ?PV (w)?w?
countW (w?)
+ ?PV (w) is the unigram probability in all contexts inthe corpus7 and ?
is a smoothing parameter.
For any5Specification of a combined similarity measure is an inter-esting direction for future work.6These constraints are similar to the pair-filters of Mayfieldet al (2009).7Recall that after context mapping, all languages have a com-mon vocabulary V .63two entity clusters Ea and Eb, the distance betweenPEa and PEb is given by a metric based on the Jensen-Shannon Divergence (JSD) (Endres and Schindelin,2003):dist(PEa , PEb) =?2 ?
JSD(PEa ||PEb)=?KL(PEa ||M) +KL(M ||PEb)where KL(PEa ||M) is the Kullback-Leibler diver-gence andM = 12(PEa + PEb).We initialize HAC to E = C, i.e., the initial clus-tering solution is just the set of all coreference chains.Thenwe remove all links in the HAC proximitymatrixthat violate pairwise cannot-link constraints.
Duringclustering, we do not merge Ea and Eb if any pair ofchains violates a cannot-link constraint.
This proce-dure propagates the cannot-link constraints (Klein etal., 2002).
To output E, we stop clustering when theminimum JSD exceeds a stop threshold ?, which istuned on a development set.3.2 Constrained Dirichlet Process MixtureModel (DPMM)Instead of tuning a parameter like ?, it would be prefer-able to let the data dictate the number of entity clus-ters.
We thus consider a non-parametric Bayesianmixture model where the mixtures are multinomialdistributions over the entity contexts S. Specifically,we consider a DPMM, which automatically infersthe number of mixtures.
Each Ca has an associatedmixture ?a:Ca|?a ?
Mult(?a)?a|G ?
GG|?,G0 ?
DP(?,G0)?
?
Gamma(1, 1)where ?
is the concentration parameter of the DPprior and G0 is the base distribution with support V .For our experiments, we set G0 = Dir(pi1, .
.
.
, piV ),where pii = PV (wi).For inference, we use the Gibbs sampler of Vla-chos et al (2009), which can incorporate pairwiseconstraints.
The sampler is identical to a standard col-lapsed, token-based sampler, except the conditionalprobability p(Ea = E|E?a, Ca) = 0 if Ca cannotbe merged with the chains in clusterE.
This propertymakes the model non-exchangeable, but in practicenon-exchangeable models are sometimes useful (Bleiand Frazier, 2010).
During sampling, we also learn ?using the auxiliary variable procedure of West (1995),so the only fixed parameters are those of the vagueGamma prior.
However, we found that these hyper-parameters were not sensitive.4 Training Data and ProceduresWe trained our system for Arabic-English cross-lingual entity clustering.8Maxent Mention Similarity The Maxent mentionsimilarity model requires a parallel name list for train-ing.
Name pair lists can be obtained from the LDC(e.g., LDC2005T34 contains nearly 450,000 parallelChinese-English names) or Wikipedia (Irvine et al,2010).
We extracted 12,860 name pairs from the par-allel Arabic-English translation treebanks,9 althoughour experiments show that the model achieves highaccuracy with significantly fewer training examples.We generated a uniform distribution of training ex-amples by running a Bernoulli trial for each alignedname pair in the corpus.
If the coin was heads, wereplaced the English name with another English namechosen randomly from the corpus.MT Context Mapping For the MT context map-ping method, we trained Phrasal with all data permit-ted under the NIST OpenMT Ar-En 2009 constrainedtrack evaluation.
We built a 5-gram language modelfrom the Xinhua and AFP sections of the Gigawordcorpus (LDC2007T07), in addition to all of the targetside training data.
In addition to the baseline Phrasalfeature set, we used the lexicalized re-ordering modelof Galley and Manning (2008).PLTM Context Mapping For PLTM training, weformed a corpus of 19,139 English-Arabic topically-aligned Wikipedia articles.
Cross-lingual links inWikipedia are abundant: as of February 2010, therewere 77.07M cross-lingual links among Wikipedia?s272 language editions (de Melo and Weikum, 2010).To increase vocabulary coverage for our ACE2008evaluation corpus, we added 20,000 document sin-gletons from the ACE2008 training corpus.
The8We tokenized all English documents with packages fromthe Stanford parser (Klein and Manning, 2003).
For Arabicdocuments, we used Mada (Habash and Rambow, 2005) fororthographic normalization and clitic segmentation.9LDC Catalog numbers LDC2009E82 and LDC2009E88.64topically-aligned tuples served as ?glue?
to share top-ics between languages, while the ACE documentsdistribute those topics over in-domain vocabulary.10We used the PLTM implementation in Mallet (Mc-Callum, 2002).
We ran the sampler for 10,000 itera-tions and set the number of topicsK = 512.5 Task Evaluation FrameworkOur experimental design is a cross-lingual extensionof the standard cross-document coreference resolu-tion task, which appeared in ACE2008 (Strassel etal., 2008; NIST, 2008).
We evaluate name (NAM)mentions for cross-lingual person (PER) and organi-zation (ORG) entities.
Neither the number nor theattributes of the entities are known (i.e., the task doesnot include a knowledge base).
We report results forboth gold and automatic within-document mentiondetection and coreference resolution.Evaluation Metrics We use entity-level evaluationmetrics, i.e., we evaluate the E entity clusters ratherthan the mentions.
For the gold setting, we report:?
B3 (Bagga and Baldwin, 1998a): Precision andrecall are computed from the intersection of thehypothesis and reference clusters.?
CEAF (Luo, 2005): Precision and recall arecomputed from a maximum bipartite matchingbetween hypothesis and reference clusters.?
NVI (Reichart and Rappoport, 2009):Information-theoretic measure that uti-lizes the entropy of the clusters and their mutualinformation.
Unlike the commonly-used Varia-tion of Information (VI) metric, normalized VI(NVI) is not sensitive to the size of the data set.For the automatic setting, we must apply a differentmetric since the number of system chains may differfrom the reference.
We use B3sys (Cai and Strube,2010), a variant of B3 that was shown to penalizeboth twinless reference chains and spurious systemchains more fairly.Evaluation Corpus The automatic evaluation ofcross-lingual coreference systems requires annotated10Mimno et al (2009) showed that so long as the proportionof topically-aligned to non-aligned documents exceeded 0.25,the topic distributions (as measured by mean Jensen-ShannonDivergence between distributions) did not degrade significantly.Docs Tokens Entities Chains MentionsArabic 412 178,269 2,594 4,216 9,222English 414 246,309 2,278 3,950 9,140Table 3: ACE2008 evaluation corpus PER and ORG entitystatistics.
Singleton chains account for 51.4% of the Arabicdata and 46.2% of the English data.
Just 216 entities appearin both languages.multilingual corpora.
Cross-document annotationis expensive (Strassel et al, 2008), so we chose theACE2008 Arabic-English evaluation corpus as a start-ing point for cross-lingual annotation.
The corpusconsists of seven genres sampled from independentsources over the course of a decade (Tbl.
3).
Thecorpus provides gold mono-lingual cross-documentcoreference annotations for both PER and ORG enti-ties.
Using these annotations as a starting point, wefound and annotated 216 cross-lingual entities.11Because a similar corpus did not exist for develop-ment, we split the evaluation corpus into developmentand test sections.
However, the usual method of split-ting by document would not confine all mentions ofeach entity to one side of the split.
We thus split thecorpus by global entity id.
We assigned one-third ofthe entities to development, and the remaining two-thirds to test.6 Comparison to Related Tasks and WorkOur modeling techniques and task formulation can beviewed as cross-lingual extensions to cross-documentcoreference resolution.
The classic work on this taskwas by Bagga and Baldwin (1998b), who adaptedthe Vector Space Model (VSM) (Salton et al, 1975).Gooi and Allan (2004) found effective algorithmicextensions like agglomerative clustering.
Successfulfeature extensions to the VSM for cross-documentcoreference have included biographical information(Mann and Yarowsky, 2003) and syntactic context(Chen and Martin, 2007).
However, neither of thesefeature sets generalize easily to the cross-lingual set-ting with multiple entity types.
Fleischman and Hovy(2004) added a discriminative pairwise mention clas-sifier to a VSM-like model, much as we do.
More11The annotators were the first author and another fluentspeaker of Arabic.
The annotations, corrections, and corpussplit are available at http://www.spencegreen.com/research/.65recent work has considered new models for web-scalecorpora (Rao et al, 2010; Singh et al, 2011).Cross-document work on languages other than En-glish is scarce.
Wang (2005) used a combination ofthe VSM and heuristic feature selection strategies tocluster transliterated Chinese personal names.
ForArabic, Magdy et al (2007) started with the output ofthe mention detection and within-document corefer-ence system of Florian et al (2004).
They clusteredthe entities incrementally using a binary classifier.Baron and Freedman (2008) used complete-link ag-glomerative clustering, wheremerging decisions werebased on a variety of features such as document topicand name uniqueness.
Finally, Sayeed et al (2009)translated Arabic name mentions to English and thenformed clusters greedily using pairwise matching.To our knowledge, the cross-lingual entity cluster-ing task is novel.
However, there is significant priorwork on similar tasks:?
Multilingual coreference resolution: AdaptEnglish within-document coreference models toother languages (Harabagiu andMaiorano, 2000;Florian et al, 2004; Luo and Zitouni, 2005).?
Named entity translation: For a non-Englishdocument, produce an inventory of entities inEnglish.
An ACE2007 pilot task (Song andStrassel, 2008).?
Named entity clustering: Assign semantictypes to text mentions (Collins and Singer, 1999;Elsner et al, 2009).?
Cross-language name search / entity linking:Match a single query name against a list ofknown multilingual names (knowledge base).
Atrack in the 2011NIST Text Analysis Conference(TAC-KBP) evaluation (Aktolga et al, 2008;McCarley, 2009; Udupa and Khapra, 2010; Mc-Namee et al, 2011).Our work incorporates elements of the first three tasks.Most importantly, we avoid the key element of entitylinking: a knowledge base.7 ExperimentsWe performed intrinsic evaluations for both mentionand context similarity.
For context similarity, weanalyzed mono-lingual entity clustering, which alsofacilitated comparison to prior work on the ACE2008Genre #Train #Test Accuracy(%)wb 125 16 87.5bn 2,720 340 95.6nw 7,443 930 96.6all 10,288 1,286 97.1 (+7.55)Table 4: Cross-lingual mention matching accuracy [%].The training data contains names from three genres: broad-cast news (bn), newswire (nw), and weblog (wb).
We usedthe full training corpus (all) for the cross-lingual clusteringexperiments, but the model achieved high accuracy withsignificantly fewer training examples (e.g., bn).CEAF?
NVI?
B3 ?#hyp P R F1Mono-lingual Arabic (#gold=1,721)HAC 87.2 0.052 1,669 89.8 89.8 89.8Mono-lingual English (#gold=1,529)HAC 88.5 0.042 1,536 93.7 89.0 91.4Table 5: Mono-lingual entity clustering evaluation (testset, gold within-document processing).
Higher scores (?
)are better for CEAF and B3, whereas lower (?)
is betterfor NVI.
#gold indicates the number of reference entities,whereas #hyp is the size of E.evaluation set.
Our main results are for the new task:cross-lingual entity clustering.7.1 Intrinsic EvaluationsCross-lingual Mention Matching We created arandom 80/10/10 (train, development, test) split ofthe Maxent training corpus and evaluated binary clas-sification accuracy (Tbl.
4).
Of the mis-classifiedexamples, we observed three major error types.
First,the model learns that high edit distance is predictiveof a mismatch.
However, singleton strings that do notmatch often have a lower edit distance than longerstrings that do match.
As a result, singletons oftencause false positives.
Second, names that originate ina third language tend to violate the phonemic corre-spondences.
For example, the model gives a false neg-ative for a German football team: 	?QK??
?P 	Q?
????
@(phonetic mapping: af s kazrslawtrn) versus ?FCKaiserslautern.?
Finally, names that require trans-lation are problematic.
For example, the classifierproduces a false negative for ?God, gd?
?= ?
?<?
@, allh?.66#gold = 3,057 CEAF?
NVI?
B3 ?
B3target ?
(#gold = 146)#hyp P R F1 #hyp P R F1Singleton 64.9 0.165 5,453 100.0 56.1 71.8 1,587 100.0 9.20 16.9No-context 57.4 0.136 2,216 65.6 75.2 70.1 517 78.3 41.8 54.5HAC+MT 79.8 0.070 2,783 84.4 86.4 85.4 310 91.7 69.1 78.8DPMM+MT 74.3 0.122 3,649 89.3 64.1 74.6 634 93.3 24.3 38.6HAC+PLTM 72.1 0.110 2,746 76.9 77.6 77.3 506 84.4 44.6 58.4DPMM+PLTM 57.2 0.180 2,609 64.0 62.8 63.4 715 73.9 22.2 34.1Table 6: Cross-lingual entity clustering (test set, gold within-document processing).
B3target is the standard B3 metricapplied to the subset of target cross-lingual entities in the test set.
For CEAF and B3, Singleton is the stronger baselinedue to the high proportion of singleton entities in the corpus.
Of course, cross-lingual entities have at least two chains,so No-context is a better baseline for cross-lingual clustering.Mono-lingual Entity Clustering For comparison,we also evaluated our system on a standard mono-lingual cross-document coreference task (Arabic andEnglish) (Tbl.
5).
We configured the system withHAC clustering and Jaro-Winkler (within-language)mention similarity.
We built mono-lingual ELMs forcontext similarity.We used two baselines:?
Singleton: E = C, i.e., the cross-lingual clus-tering solution is just the set of mono-lingualcoreference chains.
This is a common baselinefor mono-lingual entity clustering (Baron andFreedman, 2008).?
No-context: We run HAC with ?
=?.
There-fore, E is the set of fully-connected componentsin C subject to the pairwise constraints.For HAC, we manually tuned the stop threshold ?,the Jaro-Winkler threshold ?, and the ELM smoothingparameter ?
on the development set.
For the DPMM,no development tuning was necessary, and we evalu-ated a single sample of E taken after 3,000 iterations.To our knowledge, Baron and Freedman (2008)reported the only previous results on the ACE2008data set.
However, they only gave gold results forEnglish, and clustered the entire evaluation corpus(test+development).
To control for the effect ofwithin-document errors, we considered their gold in-put (mention detection and within-document coref-erence resolution) results.
They reported B3 for thetwo entity types separately: ORG (91.5% F1) andPER (94.3% F1).
The different experimental designspreclude a precise comparison, but the accuracy of#gold = 3,057 B3sys ?#hyp P R F1Singleton 7,655 100.0 57.1 72.7No-context 2,918 63.3 71.1 67.0HAC+MT 3,804 75.6 77.8 76.7DPMM+MT 4,491 77.1 62.5 69.0HAC+PLTM 6,353 94.1 62.8 75.3DPMM+PLTM 3,522 64.6 62.0 63.3Table 7: Cross-lingual entity clustering (test set, automatic(Serif) within-document processing).
For HAC, we usedthe same parameters as the gold setting.the two systems are at least in the same range.7.2 Cross-lingual Entity ClusteringWe evaluated four system configurations on the newtask: HAC+MT, HAC+PLTM, DPMM+MT, andDPMM+PLTM.
First, we established an upper boundby assuming gold within-document mention detectionand coreference resolution (Tbl.
6).
This setting iso-lated the new cross-lingual clustering methods fromwithin-document processing errors.
Then we evalu-ated with Serif (automatic) within-document process-ing (Tbl.
7).
This second experiment replicated anapplication setting.
We used the same baselines andtuning procedures as in the mono-lingual clusteringexperiment.Results In the gold setting, HAC+MTproduces thebest results, as expected.
The dimensionality reduc-tion of the vocabulary imposed by PLTM significantlyreduces accuracy, but HAC+PLTM still exceeds the67baseline.
We tried increasing the number of PLTMtopics k, but did not observe an improvement in taskaccuracy.
For both context-mapping methods, theDPMM suffers from low-recall.
Upon inspection, theclustering solution of DPMM+MT contains a highproportion of singleton hypotheses, suggesting thatthe model finds lower similarity in the presence of alarger vocabulary.
When the context vocabulary con-sists of PLTM topics, larger clusters are discovered(DPMM+PLTM).The effect of dimensionality reduction is also appar-ent in the clustering solutions of the PLTM models.For example, for the Serif output, DPMM+PLTMproduces a cluster consisting of ?White House?, ?Sen-ate?, ?House of Representatives?, and ?Parliament?.Arabic mentions of the latter three entities pass thepairwise mention similarity constraints due to theword ??m.?
?council?, which appears in text mentionsfor all three legislative bodies.
A cross-languagematching error resulted in the linking of ?WhiteHouse?, and the reduced granularity of the contextsprecluded further disambiguation.
Of course, theseentities probably appear in similar contexts.The caveat with the Serif results in Tbl.
7 is that3,251 of the 7,655 automatic coreference chains arenot in the reference.
Consequently, the evaluation isdominated by the penalty for spurious system coref-erence chains.
Nonetheless, all models except forDPMM+PLTM exceed the baselines, and the rela-tionships between models depicted in the gold exper-iments hold for the this setting.8 ConclusionCross-lingual entity clustering is a natural step to-ward more robust natural language understanding.We proposed pipeline models that make clusteringdecisions based on cross-lingual similarity.
We inves-tigated two methods for mapping documents in differ-ent languages to a common representation: MT andthe PLTM.
Although MT may achieve more accurateresults for some language pairs, the PLTM trainingresources (e.g., Wikipedia) are readily available formany languages.
As for the clustering algorithms,HAC appears to perform better than the DPMM onour dataset, but this may be due to the small corpussize.
The instance-level constraints represent tenden-cies that could be learned from larger amounts of data.With more data, we might be able to relax the con-straints and use an exchangeable DPMM,whichmightbe more effective.
Finally, we have shown that sig-nificant quantities of within-document errors cascadeinto the cross-lingual clustering phase.
As a result,we plan a model that clusters the mentions directly,thus removing the dependence on within-documentcoreference resolution.In this paper, we have set baselines and proposedmodels that significantly exceeded those baselines.The best model improved upon the cross-lingual en-tity baseline by 24.3% F1.
This result was achievedwithout a knowledge base, which is required by previ-ous approaches to cross-lingual entity linking.
Moreimportantly, our techniques can be used to extendexisting cross-document entity clustering systems forthe increasingly multilingual web.AcknowledgmentsWe thank Jason Eisner, David Mimno,Scott Miller, Jim Mayfield, and Paul McNamee for helpfuldiscussions.
This work was started during the SCALE2010 summer workshop at Johns Hopkins.
The first authoris supported by a National Science Foundation GraduateFellowship.ReferencesE.
Aktolga, M. Cartright, and J. Allan.
2008.
Cross-documentcross-lingual coreference retrieval.
In CIKM.G.
Andrew and J. Gao.
2007.
Scalable training of L1-regularizedlog-linear models.
In ICML.C.
E. Antoniak.
1974.
Mixtures of Dirichlet processes withapplications to Bayesian nonparametric problems.
The Annalsof Statistics, 2(6):1152?1174.A.
Bagga and B. Baldwin.
1998a.
Algorithms for scoring coref-erence chains.
In LREC.A.
Bagga and B. Baldwin.
1998b.
Entity-based cross-documentcoreferencing using the vector space model.
In COLING-ACL.A.
Baron and M. Freedman.
2008. Who is Who and Whatis What: Experiments in cross-document co-reference.
InEMNLP.D.
Blei and P. Frazier.
2010.
Distance dependent Chinese restau-rant processes.
In ICML.J.
Cai and M. Strube.
2010.
Evaluation metrics for end-to-end coreference resolution systems.
In Proceedings of theSIGDIAL 2010 Conference.D.
Cer, M. Galley, D. Jurafsky, and C. D.Manning.
2010.
Phrasal:A statistical machine translation toolkit for exploring newmodel features.
In HLT-NAACL, Demonstration Session.E.
Charniak.
2001.
Unsupervised learning of name structurefrom coreference data.
In NAACL.Y.
Chen and J. Martin.
2007.
Towards robust unsupervisedpersonal name disambiguation.
In EMNLP-CoNLL.68P.
Christen.
2006.
A comparison of personal name matching:Techniques and practical issues.
Technical Report TR-CS-06-02, Australian National University.M.
Collins and Y.
Singer.
1999.
Unsupervised models for namedentity classification.
In EMNLP.G.
de Melo and G. Weikum.
2010.
Untangling the cross-linguallink structure of Wikipedia.
In ACL.M.
Elsner, E. Charniak, and M. Johnson.
2009.
Structuredgenerative models for unsupervised named-entity clustering.In HLT-NAACL.D.
M. Endres and J. E. Schindelin.
2003.
A new metric forprobability distributions.
IEEE Transactions on InformationTheory, 49(7):1858 ?
1860.M.
Fleischman and E. Hovy.
2004.
Multi-document person nameresolution.
In ACL Workshop on Reference Resolution and itsApplications.R.
Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, et al2004.
A statistical model for multilingual entity detection andtracking.
In HLT-NAACL.A.
T. Freeman, S. L. Condon, and C. M. Ackerman.
2006.
Crosslinguistic name matching in English and Arabic: a one tomany mapping extension of the Levenshtein edit distancealgorithm.
In HLT-NAACL.M.
Galley and C. D. Manning.
2008.
A simple and effectivehierarchical phrase reordering model.
In EMNLP.C.
H. Gooi and J. Allan.
2004.
Cross-document coreference ona large scale corpus.
In HLT-NAACL.N.
Habash and O. Rambow.
2005.
Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fellswoop.
In ACL.S.
M. Harabagiu and S. J. Maiorano.
2000.
Multilingual corefer-ence resolution.
In ANLP.A.
Irvine, C. Callison-Burch, and A. Klementiev.
2010.
Translit-erating from all languages.
In AMTA.D.
Klein and C. D. Manning.
2003.
Accurate unlexicalizedparsing.
In ACL.D.
Klein, S. D. Kamvar, and C. D.Manning.
2002.
From instance-level constraints to space-level constraints: Making the mostof prior knowledge in data clustering.
In ICML.K.
Knight and J. Graehl.
1998.
Machine transliteration.
Compu-tational Linguistics, 24:599?612.X.
Luo and I. Zitouni.
2005.
Multi-lingual coreference resolutionwith syntactic features.
In HLT-EMNLP.X.
Luo.
2005.
On coreference resolution performance metrics.In HLT-EMNLP.W.
Magdy, K. Darwish, O. Emam, and H. Hassan.
2007.
Arabiccross-document person name normalization.
In Workshop onComputational Approaches to Semitic Languages.G.
S. Mann and D. Yarowsky.
2003.
Unsupervised personalname disambiguation.
In NAACL.C.
D. Manning, P. Raghavan, and H. Sch?tze.
2008.
Introductionto Information Retrieval.
Cambridge University Press.J.
Mayfield, D. Alexander, B. Dorr, J. Eisner, T. Elsayed, et al2009.
Cross-document coreference resolution: A key technol-ogy for learning by reading.
In AAAI Spring Symposium onLearning by Reading and Learning to Read.A.
K. McCallum.
2002.
MALLET: A machine learning forlanguage toolkit.
http://mallet.cs.umass.edu.J.
S. McCarley.
2009.
Cross language name matching.
In SIGIR.P.
McNamee, J. Mayfield, D. Lawrie, D.W. Oard, and D. Doer-mann.
2011.
Cross-language entity linking.
In IJCNLP.D.
Mimno, H. M. Wallach, J. Naradowsky, D. A. Smith, andA.
McCallum.
2009.
Polylingual topic models.
In EMNLP.NIST.
2008.
Automatic Content Extraction 2008 evaluationplan (ACE2008): Assessment of detection and recognitionof entities and relations within and across documents.
Tech-nical Report rev.
1.2d, National Institute of Standards andTechnology (NIST), 8 August.E.
H. Porter and W. E. Winkler, 1997.
Approximate String Com-parison and its Effect on an Advanced Record Linkage System,chapter 6, pages 190?199.
U.S. Bureau of the Census.H.
Raghavan, J. Allan, and A. McCallum.
2004.
An explo-ration of entity models, collective classification and relationdescription.
In KDD Workshop on Link Analysis and GroupDetection.L.
Ramshaw, E. Boschee, M. Freedman, J. MacBride,R.
Weischedel, and A. Zamanian.
2011.
SERIF languageprocessing?effective trainable language understanding.
InJ.
Olive et al, editors,Handbook of Natural Language Process-ing and Machine Translation: DARPA Global AutonomousLanguage Exploitation, pages 636?644.
Springer.D.
Rao, P. McNamee, and M. Dredze.
2010.
Streaming crossdocument entity coreference resolution.
In COLING.R.
Reichart and A. Rappoport.
2009.
The NVI clustering evalu-ation measure.
In CoNLL.G.
Salton, A. Wong, and C. S. Yang.
1975.
A vector space modelfor automatic indexing.
CACM, 18:613?620, November.A.
Sayeed, T. Elsayed, N. Garera, D. Alexander, T. Xu, et al2009.
Arabic cross-document coreference detection.
In ACL-IJCNLP, Short Papers.S.
Singh, A. Subramanya, F. Pereira, and A. McCallum.
2011.Large-scale cross-document coreference using distributed in-ference and hierarchical models.
In ACL.Z.
Song and S. Strassel.
2008.
Entity translation and alignmentin the ACE-07 ET task.
In LREC.S.
Strassel, M. Przybocki, K. Peterson, Z.
Song, and K. Maeda.2008.
Linguistic resources and evaluation techniques forevaluation of cross-document automatic content extraction.In LREC.R.
Udupa and M. M. Khapra.
2010.
Improving the multilin-gual user experience of Wikipedia using cross-language namesearch.
In HLT-NAACL.A.
Vlachos, A. Korhonen, and Z. Ghahramani.
2009.
Unsuper-vised and constrained Dirichlet process mixture models forverb clustering.
In Proc.
of the Workshop on GeometricalModels of Natural Language Semantics.H.
Wang.
2005.
Cross-document transliterated personal namecoreference resolution.
In L. Wang and Y. Jin, editors, FuzzySystems and Knowledge Discovery, volume 3614 of LectureNotes in Computer Science, pages 11?20.
Springer.M.
West.
1995.
Hyperparameter estimation in Dirichlet processmixture models.
Technical report, Duke University.69
