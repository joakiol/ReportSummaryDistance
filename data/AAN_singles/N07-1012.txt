Proceedings of NAACL HLT 2007, pages 89?96,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsInformation Retrieval On Empty FieldsVictor Lavrenko, Xing Yi and James AllanCenter for Intelligent Information RetrievalDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003-4610, USA{lavrenko,yixing,allan}@cs.umass.eduAbstractWe explore the problem of retrievingsemi-structured documents from a real-world collection using a structured query.We formally develop Structured Rele-vance Models (SRM), a retrieval modelthat is based on the idea that plausiblevalues for a given field could be inferredfrom the context provided by the otherfields in the record.
We then carry out aset of experiments using a snapshot of theNational Science Digital Library (NSDL)repository, and queries that only mentionfields missing from the test data.
For suchqueries, typical field matching would re-trieve no documents at all.
In contrast, theSRM approach achieves a mean averageprecision of over twenty percent.1 IntroductionThis study investigates information retrieval onsemi-structured information, where documents con-sist of several textual fields that can be queried in-dependently.
If documents contained subject andauthor fields, for example, we would expect to seequeries looking for documents about theory of rela-tivity by the author Einstein.This setting suggests exploring the issue of inex-act match?is special theory of relativity relevant?
?that has been explored elsewhere (Cohen, 2000).Our interest is in an extreme case of that problem,where the content of a field is not corrupted or in-correct, but is actually absent.
We wish to find rele-vant information in response to a query such as theone above even if a relevant document is completelymissing the subject and author fields.Our research is motivated by the challenges weencountered in working with the National ScienceDigital Library (NSDL) collection.1 Each item inthe collection is a scientific resource, such as a re-search paper, an educational video, or perhaps anentire website.
In addition to its main content, eachresource is annotated with metadata, which providesinformation such as the author or creator of the re-source, its subject area, format (text/image/video)and intended audience ?
in all over 90 distinct fields(though some are very related).
Making use of suchextensive metadata in a digital library paves the wayfor constructing highly-focused models of the user?sinformation need.
These models have the potentialto dramatically improve the user experience in tar-geted applications, such as the NSDL portals.
Toillustrate this point, suppose that we are runningan educational portal targeted at elementary schoolteachers, and some user requests teaching aids foran introductory class on gravity.
An intelligentsearch system would be able to translate the requestinto a structured query that might look somethinglike: subject=?gravity?
AND audience=?grades 1-4?AND format=?image,video?
AND rights=?free-for-academic-use?.
Such a query can be efficiently an-swered by a relational database system.Unfortunately, using a relational engine to query asemi-structured collection similar to NSDL will runinto a number of obstacles.
The simplest problem is1http://www.nsdl.org89that natural language fields are filled inconsistently:e.g., the audience field contains values such as K-4, K-6, second grade, and learner, all of which areclearly semantically related.A larger problem, and the one we focus on in thisstudy, is that of missing fields.
For example 24%of the items in the NSDL collection have no sub-ject field, 30% are missing the author information,and over 96% mention no target audience (readinglevel).
This means that a relational query for ele-mentary school material will consider at most 4% ofall potentially relevant resources in the NSDL col-lection.2The goal of our work is to introduce a retrievalmodel that will be capable of answering complexstructured queries over a semi-structured collectionwith corrupt and missing field values.
This studyfocuses on the latter problem, an extreme versionof the former.
Our approach is to use a generativemodel to compute how plausible a word would ap-pear in a record?s empty field given the context pro-vided by the other fields in the record.The remainder of this paper is organized as fol-lows.
We survey previous attempts at handling semi-structured data in section 2.
Section 3 will providethe details of our approach, starting with a high-levelview, then providing a mathematical framework, andconcluding with implementation details.
Section 4will present an extensive evaluation of our model onthe large set of queries over the NSDL collection.We will summarize our results and suggest direc-tions for future research in Section 5.2 Related workThe issue of missing field values is addressed in anumber of recent publications straddling the areas ofrelational databases and machine learning.
In mostcases, researchers introduce a statistical model forpredicting the value of a missing attribute or relation,based on observed values.
Friedman et al(1999) in-troduce a technique called Probabilistic RelationalModels (PRM) for automatically learning the struc-ture of dependencies in a relational database.
Taskar2Some of the NSDL metadata fields overlap substantially inmeaning, so it might be argued that the overlapping fields willcover the collection better.
Under the broadest possible inter-pretation of field meanings, more than 7% of the documentsstill contain no subject and 95% still contain no audience field.et al(2001) demonstrate how PRM can be used topredict the category of a given research paper andshow that categorization accuracy can be substan-tially improved by leveraging the relational structureof the data.
Heckerman et al(2004) introduce theProbabilistic Entity Relationship (PER) model as anextension of PRM that treats relations between enti-ties as objects.
Neville at al (2003) discuss predict-ing binary labels in relational data using RelationalProbabilistic Trees (RPT).
Using this method theysuccessfully predict whether a movie was a box of-fice hit based on other movies that share some ofthe properties (actors, directors, producers) with themovie in question.Our work differs from most of these approaches inthat we work with free-text fields, whereas databaseresearchers typically deal with closed-vocabularyvalues, which exhibit neither the synonymy nor thepolysemy inherent in natural language expressions.In addition, the goal of our work is different: we aimfor accurate ranking of records by their relevance tothe user?s query, whereas database research has typ-ically focused on predicting the missing value.Our work is related to a number of existing ap-proaches to semi-structured text search.
Desai etal (1987) followed by Macleod (1991) proposed us-ing the standard relational approach to searchingunstructured texts.
The lack of an explicit rank-ing function in their approaches was partially ad-dressed by Blair (1988).
Fuhr (1993) proposed theuse of Probabilistic Relational Algebra (PRA) overthe weights of individual term matches.
Vasan-thukumar et al(1996) developed a relational imple-mentation of the inference network retrieval model.A similar approach was taken by de Vries andWilschut (1999), who managed to improve the ef-ficiency of the approach.
De Fazio et al(1995) in-tegrated IR and RDBMS technology using an ap-proached called cooperative indexing.
Cohen (2000)describes WHIRL ?
a language that allows efficientinexact matching of textual fields within SQL state-ments.
A number of relevant works are also pub-lished in the proceedings of the INEX workshop.3The main difference between these endeavors andour work is that we are explicitly focusing on thecases where parts of the structured data are missing3http://inex.is.informatik.uni-duisburg.de/index.html90or mis-labeled.3 Structured Relevance ModelIn this section we will provide a detailed descriptionof our approach to searching semi-structured data.Before diving into the details of our model, we wantto clearly state the challenge we intend to addresswith our system.3.1 Task: finding relevant recordsThe aim of our system is to identify a set ofrecords relevant to a structured query provided bythe user.
We assume the query specifies a set ofkeywords for each field of interest to the user, forexample Q: subject=?physics,gravity?
AND audi-ence=?grades 1-4?4.
Each record in the database isa set of natural-language descriptions for each field.A record is considered relevant if it could plausiblybe annotated with the query fields.
For example, arecord clearly aimed at elementary school studentswould be considered relevant to Q even if it does notcontain ?grades 1-4?
in its description of the targetaudience.
In fact, our experiments will specificallyfocus on finding relevant records that contain no di-rect match to the specified query fields, explicitlytargeting the problem of missing data and inconsis-tent schemata.This task is not a typical IR task because thefielded structure of the query is a critical aspect ofthe processing, not one that is largely ignored in fa-vor of pure content based retrieval.
On the otherhand, the approach used is different from most DBwork because cross-field dependencies are a keycomponent of the technique.
In addition, the taskis unusual for both communities because it consid-ers an unusual case where the fields in the query donot occur at all in the documents being searched.3.2 Overview of the approachOur approach is based on the idea that plausible val-ues for a given field could be inferred from the con-text provided by the other fields in the record.
Forinstance, a resource titled ?Transductive SVMs?
andcontaining highly technical language in its descrip-tion is unlikely to be aimed at elementary-school stu-4For this paper we will focus on simple conjunctive queries.Extending our model to more complex queries is reserved forfuture research.dents.
In the following section we will describe astatistical model that will allow us to guess the val-ues of un-observed fields.
At the intuitive level, themodel takes advantage of the fact that records sim-ilar in one respect will often be similar in others.For example, if two resources share the same authorand have similar titles, they are likely to be aimed atthe same audience.
Formally, our model is based onthe generative paradigm.
We will describe a proba-bilistic process that could be viewed, hypothetically,as the source of every record in our collection.
Wewill assume that the query provided by our user isalso a sample from this generative process, albeit avery short one.
We will use the observed query fields(e.g.
audience and subject) to estimate the likely val-ues for other fields, which would be plausible in thecontext of the observed subject and audience.
Thedistributions over plausible values will be called rel-evance models, since they are intended to mimic thekind of record that might be relevant to the observedquery.
Finally, all records in the database will beranked by their information-theoretic similarity tothese relevance models.3.3 DefinitionsWe start with a set of definitions that will be usedthrough the remainder of this paper.
Let C be acollection of semi-structured records.
Each recordw consists of a set of fields w1.
.
.wm.
Eachfield wi is a sequence of discrete variables (words)wi,1.
.
.wi,ni , taking values in the field vocabularyVi.5 When a record contains no information for thei?th field, we assume ni=0 for that record.
A user?squery q takes the same representation as a recordin the database: q={qi,j?Vi : i=1..m, j = 1..ni}.We will use pi to denote a language model over Vi,i.e.
a set of probabilities pi(v)?
[0, 1], one for eachword v, obeying the constraint ?vpi(v) = 1.
Theset of all possible language models over Vi will bedenoted as the probability simplex IPi.
We definepi : IP1??
?
??IPm?
[0, 1] to be a discrete measurefunction that assigns a probability mass pi(p1.
.
.pm)to a set of m language models, one for each of them fields present in our collection.5We allow each field to have its own vocabulary Vi, since wegenerally do not expect author names to occur in the audiencefield, etc.
We also allow Vi to share same words.913.4 Generative ModelWe will now present a generative process that will beviewed as a hypothetical source that produced ev-ery record in the collection C. We stress that thisprocess is purely hypothetical; its only purpose is tomodel the kinds of dependencies that are necessaryto achieve effective ranking of records in response tothe user?s query.
We assume that each record w inthe database is generated in the following manner:1.
Pick m distributions p1.
.
.pm according to pi2.
For each field i = 1. .
.m:(a) Pick the length ni of the i?th field of w(b) Draw i.i.d.
words wi,1.
.
.wi,ni from piUnder this process, the probability of observing arecord {wi,j : i=1..m, j=1..ni} is given by the fol-lowing expression:?IP1...IPm[ m?i=1ni?j=1pi(wi,j)]pi(p1.
.
.pm)dp1.
.
.dpm (1)3.4.1 A generative measure functionThe generative measure function pi plays a criticalpart in equation (1): it specifies the likelihood of us-ing different combinations of language models in theprocess of generating w. We use a non-parametricestimate for pi, which relies directly on the combi-nations of language models that are observed in thetraining part of the collection.
Each training recordw1.
.
.wm corresponds to a unique combination oflanguage models pw1 .
.
.pwm defined by the followingequation:pwi (v) = #(v,wi) + ?icvni + ?i (2)Here #(v,wi) represents the number of times theword v was observed in the i?th field of w, niis the length of the i?th field, and cv is the rela-tive frequency of v in the entire collection.
Meta-parameters ?i allow us to control the amount ofsmoothing applied to language models of differentfields; their values are set empirically on a held-outportion of the data.We define pi(p1.
.
.pm) to have mass 1N whenits argument p1.
.
.pm corresponds to one of the Nrecords w in the training part Ct of our collection,and zero otherwise:pi(p1.
.
.pm) = 1N?w?Ctm?i=11pi=pwi (3)Here pwi is the language model associated with thetraining record w (equation 2), and 1x is the Booleanindicator function that returns 1 when its predicate xis true and zero when it is false.3.4.2 Assumptions and limitations of the modelThe generative model described in the previoussection treats each field in the record as a bag ofwords with no particular order.
This representationis often associated with the assumption of word in-dependence.
We would like to stress that our modeldoes not assume word independence, on the con-trary, it allows for strong un-ordered dependenciesamong the words ?
both within a field, and acrossdifferent fields within a record.
To illustrate thispoint, suppose we let ?i?0 in equation (2) to re-duce the effects of smoothing.
Now consider theprobability of observing the word ?elementary?
inthe audience field together with the word ?differen-tial?
in the title (equation 1).
It is easy to verify thatthe probability will be non-zero only if some train-ing record w actually contained these words in theirrespective fields ?
an unlikely event.
On the otherhand, the probability of ?elementary?
and ?differen-tial?
co-occurring in the same title might be consid-erably higher.While our model does not assume word indepen-dence, it does ignore the relative ordering of thewords in each field.
Consequently, the model willfail whenever the order of words, or their proximitywithin a field carries a semantic meaning.
Finally,our generative model does not capture dependenciesacross different records in the collection, each recordis drawn independently according to equation (1).3.5 Using the model for retrievalIn this section we will describe how the generativemodel described above can be used to find databaserecords relevant to the structured query provided bythe user.
We are given a structured query q, anda collection of records, partitioned into the trainingportion Ct and the testing portion Ce.
We will usethe training records to estimate a set of relevance92records average uniquecovered length wordstitle 655,673 (99%) 7 102,772description 514,092 (78%) 38 189,136subject 504,054 (77%) 12 37,385content 91,779 (14%) 743 575,958audience 22,963 (3.5%) 4 119Table 1: Summary statistics for the five NSDL fieldsused in our retrieval experiments.models R1.
.
.Rm, intended to reflect the user?s in-formation need.
We will then rank testing records bytheir divergence from these relevance models.
A rel-evance Ri(v) specifies how plausible it is that wordv would occur in the i?th field of a record, giventhat the record contains a perfect match to the queryfields q1.
.
.qm:Ri(v) = P (q1.
.
.v?qi.
.
.qm)P (q1.
.
.qi.
.
.qm) (4)We use v?qi to denote appending word v to thestring qi.
Both the numerator and the denomina-tor are computed using equation (1).
Once we havecomputed relevance models Ri for each of the mfields, we can rank testing records w?
by their sim-ilarity to these relevance models.
As a similaritymeasure we use weighted cross-entropy, which is anextension of the ranking formula originally proposedby (Lafferty and Zhai, 2001):H(R1..m;w1..m) =m?i=1?i?v?ViRi(v) logpwi (v) (5)The outer summation goes over every field of inter-est, while the inner extends over all the words in thevocabulary of the i?th field.
Ri are computed accord-ing to equation (4), while pwi are estimated fromequation (2).
Meta-parameters ?i allow us to varythe importance of different fields in the final rank-ing; the values are selected on a held-out portion ofthe data.4 Experiments4.1 Dataset and queriesWe tested the performance of our model on a Jan-uary 2005 snapshot of the National Science Digi-tal Library repository.
The snapshot contains a to-tal of 656,992 records, spanning 92 distinct (thoughsometimes related) fields.
6Only 7 of these fieldsare present in every record, and half the fields arepresent in less than 1% of the records.
An averagerecord contains only 17 of the 92 fields.
Our experi-ments focus on a subset of 5 fields (title, description,subject, content and audience).
These fields wereselected for two reasons: (i) they occur frequentlyenough to allow a meaningful evaluation and (ii)they seem plausible to be included in a potentialquery.7 Of these fields, title represents the title of theresource, description is a very brief abstract, contentis a more detailed description (but not the full con-tent) of the resource, subject is a library-like clas-sification of the topic covered by the resource, andaudience reflects the target reading level (e.g.
ele-mentary school or post-graduate).
Summary statis-tics for these fields are provided in Table 1.The dataset was randomly split into three sub-sets: the training set, which comprised 50% of therecords and was used for estimating the relevancemodels as described in section 3.5; the held-out set,which comprised 25% of the data and was used totune the smoothing parameters ?i and the bandwidthparameters ?i; and the evaluation set, which con-tained 25% of the records and was used to evaluatethe performance of the tuned model8.Our experiments are based on a set of 127 auto-matically generated queries.
We randomly split thequeries into two groups, 64 for training and 63 forevaluation.
The queries were constructed by com-bining two randomly picked subject words with twoaudience words, and then discarding any combi-nation that had less than 10 exact matches in anyof the three subsets of our collection.
This proce-dure yields queries such as Q91={subject:?artificialintelligence?
AND audience=?researchers?
}, orQ101={subject:?philosophy?
AND audience=?highschool?
}.4.2 Evaluation paradigmWe evaluate our model by its ability to find ?rele-vant?
records in the face of missing values.
We de-6As of May 2006, the NSDL contains over 1.5 million doc-uments.7The most frequent NSDL fields (id, icon, url, link and 4brand fields) seem unlikely to be used in user queries.8In real use, typical pseudo relevance feedback scheme canbe followed: retrieve top-k documents to build relevance mod-els then perform IR again on the same whole collection93fine a record w to be relevant to the user?s query qif every keyword in q is found in the correspondingfield of w. For example, in order to be relevant toQ101 a record must contain the word ?philosophy?
inthe subject field and words ?high?
and ?school?
in theaudience field.
If either of the keywords is missing,the record is considered non-relevant.9When the testing records are fully observable,achieving perfect retrieval accuracy is trivial: wesimply return all records that match all query key-words in the subject and audience fields.
As westated earlier, our main interest concerns the sce-nario when parts of the testing data are missing.
Weare going to simulate this scenario in a rather ex-treme manner by completely removing the subjectand audience fields from all testing records.
Thismeans that a straightforward approach ?
matchingquery fields against record fields ?
will yield no rel-evant results.
Our approach will rank testing recordsby comparing their title, description and contentfields against the query-based relevance models, asdiscussed in section 3.5.We will use the standard rank-based evaluationmetrics: precision and recall.
Let NR be the totalnumber of records relevant to a given query, sup-pose that the first K records in our ranking containNK relevant ones.
Precision at rank K is definedas NKK and recall is defined asNKNR .
Average preci-sion is defined as the mean precision over all rankswhere relevant items occur.
R-precision is definedas precision at rank K=NR.4.3 Baseline systemsOur experiments will compare the ranking perfor-mance of the following retrieval systems:cLM is a cheating version of un-structured textsearch using a state-of-the-art language-modelingapproach (Ponte and Croft, 1998).
We disregardthe structure, take all query keywords and run themagainst a concatenation of all fields in the testingrecords.
This is a ?cheating?
baseline, since the con-9This definition of relevance is unduly conservative by thestandards of Information Retrieval researchers.
Many recordsthat might be considered relevant by a human annotator will betreated as non-relevant, artificially decreasing the accuracy ofany retrieval algorithm.
However, our approach has the advan-tage of being fully automatic: it allows us to test our model ona scale that would be prohibitively expensive with manual rele-vance judgments.catenation includes the audience and subject fields,which are supposed to be missing from the testingrecords.
We use Dirichlet smoothing (Lafferty andZhai, 2001), with parameters optimized on the train-ing data.
This baseline mimics the core search capa-bility currently available on the NSDL website.bLM is a combination of SQL-like structuredmatching and unstructured search with query ex-pansion.
We take all training records that containan exact match to our query and select 10 highly-weighted words from the title, description, and con-tent fields of these records.
We run the resulting 30words as a language modeling query against the con-catenation of title, description, and content fields inthe testing records.
This is a non-cheating baseline.bMatch is a structured extension of bLM.
As inbLM, we pick training records that contain an ex-act match to the query fields.
Then we match 10highly-weighted title words, against the title field oftesting records, do the same for the description andcontent fields, and merge the three resulting rankedlists.
This is a non-cheating baseline that is similarto our model (SRM).
The main difference is that thisapproach uses exact matching to select the trainingrecords, whereas SRM leverages a best-match lan-guage modeling algorithm.SRM is the Structured Relevance Model, as de-scribed in section 3.5.
For reasons of both effec-tiveness and efficiency, we firstly run the originalquery to retrieve top-500 records, then use theserecords to build SRMs.
When calculating the crossentropy(equ.
5), for each field we only include thetop-100 words which will appear in that field withthe largest probabilities.Note that our baselines do not include a standardSQL approach directly on testing records.
Suchan approach would have perfect performance in a?cheating?
scenario with observable subject and au-dience fields, but would not match any records whenthe fields are removed.4.4 Experimental resultsTable 2 shows the performance of our model (SRM)against the three baselines.
The model parameterswere tuned using the 64 training queries on the train-ing and held-out sets.
The results are for the 63 testqueries run against the evaluation corpus.
(Similarresults occur if the 64 training queries are run against94cLM bMatch bLM SRM %change improvedRel-ret: 949 582 914 861 -5.80 26/50Interpolated Recall - Precision:at 0.00 0.3852 0.3730 0.4153 0.5448 31.2 33/49at 0.10 0.3014 0.3020 0.3314 0.4783 44.3 42/56at 0.20 0.2307 0.2256 0.2660 0.3641 36.9 40/59at 0.30 0.2105 0.1471 0.2126 0.2971 39.8 36/58at 0.40 0.1880 0.1130 0.1783 0.2352 31.9 36/58at 0.50 0.1803 0.0679 0.1591 0.1911 20.1 32/57at 0.60 0.1637 0.0371 0.1242 0.1439 15.8 27/51at 0.70 0.1513 0.0161 0.1001 0.1089 8.7 21/42at 0.80 0.1432 0.0095 0.0901 0.0747 -17.0 18/36at 0.90 0.1292 0.0055 0.0675 0.0518 -23.2 12/27at 1.00 0.1154 0.0043 0.0593 0.0420 -29.2 9/23Avg.Prec.
0.1790 0.1050 0.1668 0.2156 29.25 43/63Precision at:5 docs 0.1651 0.2159 0.2413 0.3556 47.4 32/4310 docs 0.1571 0.1651 0.2063 0.2889 40.0 34/4815 docs 0.1577 0.1471 0.1841 0.2360 28.2 32/4920 docs 0.1540 0.1349 0.1722 0.2024 17.5 28/4730 docs 0.1450 0.1101 0.1492 0.1677 12.4 29/50100 docs 0.0913 0.0465 0.0849 0.0871 2.6 37/57200 docs 0.0552 0.0279 0.0539 0.0506 -6.2 33/53500 docs 0.0264 0.0163 0.0255 0.0243 -4.5 26/481000 docs 0.0151 0.0092 0.0145 0.0137 -5.8 26/50R-Prec.
0.1587 0.1204 0.1681 0.2344 39.44 31/49Table 2: Performance of the 63 test queries retrieving 1000 documents on the evaluation data.
Bold figuresshow statistically significant differences.
Across all 63 queries, there are 1253 relevant documents.the evalution corpus.
)The upper half of Table 2 shows precision atfixed recall levels; the lower half shows precisionat different ranks.
The %change column shows rel-ative difference between our model and the base-line bLM.
The improved column shows the num-ber of queries where SRM exceeded bLM vs. thenumber of queries where performance was different.For example, 33/49 means that SRM out-performedbLM on 33 queries out of 63, underperformed on49?33=16 queries, and had exactly the same per-formance on 63?49=14 queries.
Bold figures in-dicate statistically significant differences (accordingto the sign test with p < 0.05).The results show that SRM outperforms threebaselines in the high-precision region, beatingbLM?s mean average precision by 29%.
User-oriented metrics, such as R-precision and precisionat 10 documents, are improved by 39.4% and 44.3%respectively.
The absolute performance figures arealso very encouraging.
Precision of 28% at rank 10means that on average almost 3 out of the top 10records in the ranked list are relevant, despite the re-quested fields not being available to the model.We note that SRM continues to outperform bLMuntil very high recall and until the 100-documentcutoff.
After that, SRM degrades rapidly with re-spect to bLM.
We feel the drop in effectiveness is ofmarginal interest because precision is already wellbelow 10% and few users will be continuing to thatdepth in the list.It is encouraging to see that SRM outperformsboth cLM, the cheating baseline that takes advantageof the field values that are supposed to be ?miss-ing?, and bMatch, suggesting that best-match re-trieval provides a superior strategy for selecting a setof appropriate training records.5 ConclusionsWe have developed and empirically validated a newretrieval model for semi-structured text.
The modelis based on the idea that missing or corrupted val-ues for one field can be inferred from values in otherfields of the record.
The cross-field inference makesit possible to find documents in response to a struc-tured query when those query fields do not exist inthe relevant documents at all.We validated the SRM approach on a large95archive of the NSDL repository.
We developed alarge set of structured Boolean queries that had rel-evant documents in the test portion of collection.We then indexed the documents without the fieldsused in the queries.
As a result, using standard fieldmatching approaches, not a single document wouldbe returned in response to the queries?in particular,no relevant documents would be found.We showed that standard information retrievaltechniques and structured field matching could becombined to address this problem, but that the SRMapproach outperforms them.
We note that SRMbrought two relevant documents into the top five?again, querying on missing fields?and achieved anaverage precision of 23%, a more than 35% im-provement over a state-of-the-art relevance modelapproach combining the standard field matching.Our work is continuing by exploring methodsfor handling fields with incorrect or corrupted val-ues.
The challenge becomes more than just inferringwhat values might be there; it requires combininglikely missing values with confidence in the valuesalready present: if an audience field contains ?under-graduate?, it should be unlikely that ?K-6?
would bea plausible value, too.In addition to using SRMs for retrieval, we arecurrently extending the ideas to provide field valida-tion and suggestions for data entry and validation:the same ideas used to find documents with miss-ing field values can also be used to suggest potentialvalues for a field and to identify values that seeminappropriate.
We have also begun explorations to-ward using inferred values to help a user browsewhen starting from some structured information?e.g., given values for two fields, what values areprobable for other fields.AcknowledgmentsThis work was supported in part by the Centerfor Intelligent Information Retrieval and in part bythe Defense Advanced Research Projects Agency(DARPA) under contract number HR0011-06-C-0023.
Any opinions, findings and conclusions orrecommendations expressed in this material are theauthors?
and do not necessarily reflect those of thesponsor.ReferencesD.C.
Blair.
1988.
An extended relational document re-trieval model.
Inf.
Process.
Manage., 24(3):349?371.W.W.
Cohen.
2000.
WHIRL: A word-based informa-tion representation language.
Artificial Intelligence,118(1?2):163?196.S.
DeFazio, A. Daoud, L. A. Smith, and J. Srinivasan.1995.
Integrating IR and RDBMS Using CooperativeIndexing.
In Proceedings of SIGIR, pages 84?92.B.
C. Desai, P. Goyal, and F. Sadri.
1987.
Non-first nor-mal form universal relations: an application to infor-mation retrieval systems.
Inf.
Syst., 12(1):49?55.N.
Friedman, L. Getoor, D. Koller, and A. Pfeffer.
1999.Learning probabilistic relational models.
In IJCAI,pages 1300?1309.N.
Fuhr.
1993.
A probabilistic relational model for theintegration of IR and databases.
In Proceedings of SI-GIR, pages 309?317.D.
Heckerman, C. Meek, and D. Koller.
2004.
Proba-bilistic models for relational data.
Technical ReportMSR-TR-2004-30, Microsoft Research.J.
Lafferty and C. Zhai.
2001.
Document language mod-els, query models, and risk minimization for informa-tion retrieval.
In Proceedings of SIGIR, pages 111?119.I.
Macleod.
1991.
Text retrieval and the relational model.Journal of the American Society for Information Sci-ence, 42(3):155?165.J.
Neville, D. Jensen, L. Friedland, and M. Hay.
2003.Learning relational probability trees.
In Proceedingsof ACM KDD, pages 625?630, New York, NY, USA.J.
M. Ponte and W. B. Croft.
1998.
A language modelingapproach to information retrieval.
In Proceedings ofSIGIR, pages 275?281.B.
Taskar, E. Segal, and D. Koller.
2001.
Probabilisticclassification and clustering in relational data.
In Pro-ceedings of IJCAI, pages 870?876.S.
R. Vasanthakumar, J.P. Callan, and W.B.
Croft.
1996.Integrating INQUERY with an RDBMS to support textretrieval.
IEEE Data Eng.
Bull., 19(1):24?33.A.D.
Vries and A. Wilschut.
1999.
On the integration ofIR and databases.
In Proceedings of IFIP 2.6 WorkingConf.
on Data Semantics, Rotorua, New Zealand.96
