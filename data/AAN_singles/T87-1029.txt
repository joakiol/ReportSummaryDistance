They say it's a new sort of engine:but the SUMP's still thereKaren Sparck JonesComputer Laboratory, University of CambridgeCorn Exchange Street, Cambridge CB2 3QG, Englandsparckjones%cl.cam.ac.uk@cs.ucl.ac.ukI shall lump the specific semantic formalisms currently touted together asmanifestations of logicism, because the issue is whether the logicist approach tolanguage processing is the right one, not whether one particular formalism is betterthan another.The \[ogicist model of language processing is essentially as follows.
We use aphrase structure grammar, heavily laced with features, for syntactic parsing; inanalysis yntactic processing drives semantic interpretation strictly compositionally,to build a logical form representing the literal meaning of a sentence.
This logicalform is further processed, both in discourse operations of a larger scale linguisticcharacter, as in (some) pronoun resolution, and, more importantly, in inference onglobal and local world knowledge, to obtain a filled-out utterance interpretation.Logical form plays a key role, motivated as much by the need for reasoning tocomplete interpretation as by the need to supply an appropriate input to furtherreactive problem solving.
The use of a logical form to represent the meaning of aninput naturally fits the use of a logical formalism to characterise general and specificknowledge of the world to which a discourse refers and in which it occurs.Whether the logicist position is psychologically plausible is not an issue here: itcan be adopted as a base for language processing without a commitment to, say,FOPC as a vehicle for human thought, and indeed can be adopted in apsychologically implausible form, for example with complete syntactic processingfollowed by semantic processing.
The fact that a good deal of development work isbeing done using a case frame approach is no problem for the logicist either: caseframes are either an alternative notation or can only work for the discourse of veryrestricted applications.The logicists' problems are those of getting an expressive and tractable enoughlogic: what sort of logic is powerful enough to capture linguistic constructs,characterise the world, and support common-sense r asoning; and is this logiccomputationally practical?
Determining and representing the knowledge required fora particular system application is not necessarily more of a problem for the logicistthan for anyone else.
The threat to the logicist's moral, or at least mental, purity isWhether a logic which will do the job is really a logic at all.
If the world isheterogeneous, our thinking sloppy, and our language uncertain, whatever closelyreflects these may be barely worthy of the label "logic".This is not to rake up the old semantic nets versus predicate logic controversy, orits analogues.
We may have a formalism with axioms, rules of inference, and so forthwhich is quite kosher as far as the manifest criteria for logics go, but which is a logiconly in the letter, not the spirit.
This is because, to do its job, it has got to absorb140the ad hoc miscellaneity that makes language only approximately systematic.Broadly speaking, it can do this in two ways.
It can achieve its results through someproliferation of rules, weakening the idea of inference.
Or it can achieve themessentially by following the expert system path, retaining a single rule of inference atthe cost of very many specific, individual axioms.
It is at least arguable that if thestock of initial propositions is a vast heap of particulars defining idiosyncratic localrelationships, the fact that one is technically applying some plain rule of inference tofollow a chain of argument is not that impressive: conciseness and generality, whichat least some expect a logic to have, are not much in evidence.
Precision and claritymay be equally unattainable, at any rate in practice.I believe the second possibility is already with us, masquerading in the respectableguise of meaning postulates, and that whatever precise view is taken of meaningpostulates, they sell the logicists' pass.This is well illustrated by following through the implications of the processordesign adumbrated in a recent SR\[ report.
(As I am one of the authors of thisreport, I should make it clear that \[ am using this design as a vehicle for discussion,and not in a particularist critical spirit.)
The report proposal adopts the logicistapproach outlined earlier, for the purpose of building language processing interfacesto, for example, advisor systems.
The design is for two processors.
The first, thelinguistic processor proper, is a general-purpose, application-independent componentfor syntactic analysis and the correlated construction of logical forms.
The output ofthe linguistic processor is then fully interpreted (progressing from a representation ofa sentence to that of an utterance) in relation to a discourse and domain context.The semantic operations of the linguistic processor proper deal, respectively, withthe logical correlates of linguistic terms and expressions, and with the application ofselection restrictions.
The logical structures for linguistic expressions ~re determinedby the domain-independent properties of items like articles and moda\[s and ofsyntactic onstructs like verb phrases, and by the formal characterisation f domainlexical items primarily as predicates of so many arguments.
The lexical informationabout sorts, which supports the selection restrictions, is functionally distinct, as itsrole is simply to eliminate interpretations.In this scheme of things the semantic information given for lexical items, andespecially for 'content words', in the processor's output sentence representation, isfairly minimal.
It is sparse, abstract, and opaque.
The assumption is that predicatescorresponding to substantive words are primarily given meaning by the domaindescription, and hence by the world which models this.
Within the linguisticprocessor one sense of "supply", call it 'supply1', just means SUPPLY, whereSUPPLY is an undefined predicate label.
The sortal information bearing onpredicate arguments which is exploited via selection restrictions does not appear inthe linguistic processor's output meaning representation.The domain description gives meaning to the predicates through the \[ink providedby meaning postulates.
These establish relations between predicates of the domaindescription language.
But they are in a material sense part of the domaindescription, since these names are also used in the description of the properties ofthe domain world.
Broadly speaking, the meaning postulates form part of theaxiomatic apparatus of the domain description.
Thus from a conventional point ofview the lexicon says rather little about meaning: it merely points into a store ofinformation about the world about which the system reasons, both to understand141what is being said and to react to this both in task appropriate actions and morespecifically in linguistic response.
The system structure is thus a particularmanifestation of AI's emphasis on world knowledge and inference on this.
The factthat meaning postulates are also the source of the sortal information applied throughselection restrictions underlines the somewhat ambiguous character that meaningpostulates have; but as noted, this sortal information does not figure as part of theinformation supplied in the representation f input text items in the output of thestrictly linguistic processor.
However the predicate labels of the meaning postulatesmay be word sense names so, e.g.
'supplyl' is directly mapped onto 'provide3': thissuggests that the boundary between semantic information in some narrow linguisticsense which refers to the content of the lexicon that is transmitted by the firstprocessor, and semantic or conceptual information in the broader sense of theknowledge about the world that is incorporated in the non-linguistic domaindescription, has no theoretical but only operational status.The immediate motivation for the system design just outlined is a very practicalone, that of maximising system portability.
Given our current inability to handlemore than a very small universe of discourse computationally, we have to allow forthe fact that some of the particular domain information appropriate to one specificapplication may be unhelpful or even confusing for another, and that the systemdesign should therefore clearly separate the body of information which is general tolanguage use and so should be transportable from that which is not.
In the schemepresented the domain dependent information is confined to the lexical entries for theapplication vocabulary, and to the domain description.But logicists also appear to advocate this form of processor as a matter ofprinciple.
Setting aside the question of whether the control structure of the processoris psychologically plausible (because it would be perfectly possible to apply syntacticand semantic, and linguistic and non-linguistic, operations concurrently), there isstill the question of whether a viable general-purpose computational languageprocessing system can be built with a strategy that treats meaning in the way thedesign described oes, with so little information about it in the lexicon and so muchin the knowledge base.
The strategy implies both that there are no particularprocessing problems which would stem from the need to include both common andspecialised knowledge, and perhaps several areas of specialised knowledge, in theknowledge base and, more importantly that none follow from the comparative lack ofsemantic information of the conventional kind found in ordinary dictionarydefinitions in the lexicon used for the purely linguistic processes, i.e.
that there isonly sortal information for selection restriction purposes.
The first problem is notunique to the logicist position: any attempt o use information about the world, asall systems must, has to tackle the problem of arbitrarily related subworlds.
Thesecond problem seems to be more narrowly one for the logicist.
The point here is notso much that, in staged processing, the attempt to avoid duplicating informationmeans that information is unhelpfully withheld from earlier processes in favour oflater ones.
The point is rather whether, even in a situation where concurrentprocessing is done, providing much of the information germane to word meanings viadomain descriptions i the right way to do semantics.
It may be a mistake to regardlinguistic meaning and world reference as the same; it is possible that someinformation about meaning has to be supplied, for representational use, in a formexclusively designed for strictly linguistic processing.142But all this is speculation.
What is clear, on the other hand, is that the meaningpostulates trategy, even if it does not involve the problems just mentioned, will,when applied to a non-trivial universe of discourse, imply vast amounts of verymiscellaneous tuff.
If language is intrinsically complex, simplicity at one pointsimply pushes all the complexity (or mess) somewhere lse.
In more exclusivelylinguistic approaches to processing this tends to take the form of putting all thedetail in the lexicon: then the grammar can be nice and straightforward.
Maybe onecan get away with a simple syntactic analyser and semantic interpreter; but only bysupplying all the specialised matter they need to work effectively on the varioustexts they will encounter, through lexical entries.
And if simplicity in one place isfound at the expense of complexity in another, it does not obviously follow that thesystem as a whole has the elegance of its simpler part.
In the same way, in thelogicist approach, the set of meaning postulates required may turn out to be such ahuge heterogenous mass as to suggest, to the disinterested observer, that the puritythe use of logic might imply has been compromised.
From this point of view, indeed,whether the kind of information captured by meaning postulates is deemed to bepart of the domain description, or is deemed part of the lexicon and is evenexpressed in the representation delivered by the linguistic processor, is irrelevant.Either way, the logicist approach suffers from the Same UnManageable Problem ofmiscellaneous linguistically-relevant detail as every other approach to languageprocessing.This is without considering the proposition that there is a much larger problemfor which the logicists have so far offered us no real solutions: how to capturelanguage use as this is a matter of salience, plausibility, metaphor, and the like.
Butwhether or not the logicists can solve this, the real problem, we should not assumethat they have got more mundane, literal matters of language taped.H.
Alshawi, R.C.
Moore, S.G. Pulman and K. Sparck Jones, Feasibility study fora research programme in natural-language processing, Final Report Project ECC-1437, SR\[ International, Cambridge, August 1986.J.R.
Hobbs and R.C.
Moore (edsi, Formal theories of the commonsense world,Norwood, N J: Ablex, 1985.143
