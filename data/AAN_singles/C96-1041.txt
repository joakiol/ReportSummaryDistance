Marker  random field basedEngl ish Part -Of -Speech tagging sys temSung-Young Jung , Young C. Park, Key-Sun Choi and Youngwhan Kim*Comput ;er  Sc ience \ ] )e l )a.rtmentKorea  Advanced \[nstitul, e of Sc ience and q%chnologyTae jon,  Korea*Mu l t imed ia  Re.search Ibaborator iesKorea  ~\[>lecom{ chopin,ycpark,kschoi}(ci!csone.kaist.ac.krAbstractProbabilistic models have been widelyused for natural language processing.Part-of-speech tagging, which assignsthe most likely tag to each word in agiven sentence, is one.
of tire problemswhich can be solved by statisticM ap-proach.
Many researchers haw~ triedto solve the problem by hidden Markermodel (HMM), which is well known asone of the statistical models.
But ithas many difficulties: integrating hetero-geneous information, coping with datasparseness prohlem, and adapting to newenvironments.
In this paper, we pro-pose a Markov radom field (MRF) modelbased approach to the tagging problem.The MRF provides the base frame tocombine various statistical informationwith maximum entropy (ME) method.As Gibbs distribution can be used todescribe a posteriori probability of tag-ging, we use it in ma.ximum a posteri-ori (MAP) estimation of optimizing pro-cess.
Besides, several tagging models aredeveloped to show the effect of addinginformation.
Experimental results showthat the performance of the tagger getsimproved as we add more statistical in-formation, and that Mt{F-based taggingmodel is better than ttMM based taggingmodel in data sparseness problem.1 Int roduct ionPart-of-speech tagging is to assign the correct tagto each word in the context of the sentence.
'\['hereare three main approaches in tagging problem:rule-based approach (Klein and Simmons 1%3;Brodda 1982; Paulussen and Martin 1992; Brillet al 1990), statistical approach (Church :1988;Merialdo 1994; Foster 1991; Weischedel et al1993; Kupiec 1992) and connectionist approach(Benello et al 1989; Nakanmra et al 1989).
Inthese approaches, tatistical approach as the fol-lowing advantages :?
a theoretical framework is provided?
automatic learning facility is provided?
the probabilities provide a straightforwardway to disambiguateMany information sources must be combined tosolve tagging problem with statistical approach.It is a significant assumption that tire correct tagcan generally be chosen from I.he local context.Not only local sequences of words and tags areneeded to solve tagging problem, but syntax, se-mantic, and morphological level information is alsorequired in general.
Usually information sourcessuch as t)igram, trigram and migra.m are used inthe tagging systems which are based on statisticalmethod.
Traditionally, linear interpolation an(t itsvariants have been used to combine the informa-tion sources, })tit these are shown to be seriouslydeficient.ME (Maximum Entropy) estimation methodprovides the facility to combine several informa-tion sources.
Each inR)rmation source gives riseto a set of constraints, to be imposed on the con>bined estimate.
The function with the highest en-tropy within the constraints is the ME solution.Given consistent statistical evidence, a unique M Esolution is guaranteed to exist and an iteratiw~" al-gorithm is provided.MRF (Marker random field) model is based onME method and it; has the facility to combinemany inlbrmation sources through feature flmc-tions.
MRF model has the following adwmtages:robustness, adaptability, parallelism and the facil-ity of combining informatiort sources.
M RF-basedtagging model inherits these advantages.In this paper, we will present one of the statis-tical models, namely MRF-based tagging systern.We will show that several information sources in-cluding unigram, bigram and trigram, can be com-bined in MRF-based tagging model.
Experimen-tal results show that the MRF-based tagger hasvery good performance specially when trainingdata size is small.Section 2 describes the tagging problem , Sec-tion "l describes tatistical model already known236a.nd ,,;ect.iou 4 t ip  rcso{~rch for contl)it~ing st.at.ist, i-cal in\['orutal.ion.
Sect.ion 5 provides Ml{F- I)asedtagging model  attd secl.ion 9 sho,.w's the expcr i -t:twnl;al rcsult.s.
Sc.ct.iou 10 (:Opal)arcs M\[{I :  wilhI \ ]MM.
Fit(ally we conclude in scct, iott l.2 The Prob lem of  Taggi l+gWhen scnt.ct~c(!
i,'V = wt, w2, .,., u,,,~ is given, t.lwrcexist ( 'orresl>onding (.ags 7' = / i , *2 , .
.
.
, t ,  of thesame hmgl.h.
W{> call l he pair  (W,T)  an al ign-lltOtll;.
~'Vo ::-;a.y that.
wor(l iv: has l)('('tt ass igned l.}wl.ag t i ill t.his al iguutcnl..  V'v'o suppose l.hat, a sel.,:)f I,ags is giv~'n.
'l'aggittg is assigniltg ('()rr('('1.
lag:s(!quetlCO "1'-- It ,  t2, ..., tn I'or given word sc:qtlcnceH / ~- lt; t , 11:2, ..., IU++.3 Probab i l i s t i cFormulat ion  ( I IMM)I:% us assunm t,hal, we want  i.o I,:ttov,, the n~,::)st;likc'ly tag seqtt<mcc ~b(PV), given a par l ic t t lar  wot:dSe(lUtmcc I'V.
T im l.agging prol)h-nt is dol im'd asl iuding t.lw tttosl, likely t.?tg s("tltlClt('(!
"1'q~(w) :- .,':z ,t~,tx/'('/'llI,:) (t)z'(Wl',")/'('/') = ..,'q max (2)",' P (w)= +,rff nF3xI'(VVI'l')l'(73 (:It /wh.cre P(7') is 1,he a priori iwohal)ilit.y of a tagscquom:o~ 'r, t , (WI ' I  ') is t,h.c cot~dil.ioual I, 'olmhilil,y of word sc,,tu(m(-c. H/, given I.It,:~ .
'-;cqucuco o1" tags7', at,d / ' (W)  is l,ho tmcotMitiotled i)roba.I)ilit,y ot:word scqmmcc W. 'l'hc t>rol>abilit.y I ' (W)  in (2) isrc'tnovc'd I)ccause it lt~ts no ell'cot, on 0(W).
(:on-scquent.ly, it.
is sui l ic icnt i.o find the tag SC(ltmnce7' which sal.isiies (3).Wc can rcwvit,(> the prolmldl i ly of0ach scqttcnc('as a. prodltct, of 1,he cotMil.iona.l prot>altilit.i<~s ofeach word or fag given all of the lm'viotts t, ags.t,(wv/') v(~r), ,  { :'(.,,,:It+, .... ~,, ,,,_ ~, ..., ,,t) }= l l i= l  ?l ' ( l i \ [ l i _  I, .
.
.
, /1)Typica l ly ,  otto nmkcs t.wo s i l l lp l i fy ing assmttp(.ions I.o (:llt.
dowu o ,  l, lt<: nttnd>er of F, rc, l>al:,ilil.ies1.,.)
Ive <+st.inml.ed.
I,'irst., rat.her I.h;tn ;tssuttling lt ' idclwnds on all IH'cviotm words and all i)rcviottsta.gs, ono ;-tsstttnes w+ d,:'F, cn,:Is (:,nly , :ml i .
Sc,::oml,rath(,r i:hau aSS(lining the tag ti deltends on thet'ull s(>quc:ncc of l )rcvious l, ags, w(' can assume l.hal.local COltl;oxl, is sullicicnt.
This locality assumed isrcfercd t,o as ;t Mm'kov imlcl )endence rlSStlllll)liioIl.Using 1.lwse ass(trutH,ion, w(> al)lwoxhtml,c l,}touqua.l.ion l,o l, ttc R)llowingz'(WlV') ~ II'A~z'(.,~le,) (4)/,(~/') ~ tl}%,z,(,~lz+_t) (:,)Accordingly, O(i'V) is (h'rived I) 3, applying (,I), .td (,~) to (:~)., / , (w) : .
.
.
,,+p+.
: tI 'L,; '( , .+lz;)z'(z,: lt ,_.)
((i)We can gel each l)robabil it,  y va.hte front thet.aggcd corItus which is i , rq+arcd for l .raining byusiu:4 (7) aml (8).z'(.,zlt,) - ( : (+<,t , )  (T)cT(t~)/'(/,:1t~_, ) c'(z~) (stwhere (7 ( t : ) ,C ( t i ,  Ig) is tit(" \['reqttency obt.aincdfronl l ra inhlg dal.a.\:it;orl>i a lgor i lhnt  ( l :orncy73) is the one goner-al ly used to liw.l t.he t,a.g SO<luencc which safislies(6) aim I.ttis algori l .hnt gttaranl.ccs the opl, itttal so-hit ion to I,he I)r(+bhmt.This  model  has several prot>l(+tns.
First,, so(nowot'ds or \[,ag~ s<Xltl(~ll(W','-; 1/13.y itot, O(HHIt' ill l.ra.ili-htg dal.a or Hlay occur with very low \[reqttetlcy;ii('vt'rlh('lcs,% t,llc words or lag  soqtt(~ltC(~s c;/tt ;+\])-l)ear ill t.agging l>roccss, lit this case, it, usual lycauses V(!l'y bad result, t.o COllll)ttt.c (6), becausethe lwol)al)ility has zero wdue or very low value.'
l 'his problont is c.alh'd data s lmr,sc .css  i>rol)h+tn.To avoid thi+q l~roldetn, sm,)ot, hing of itd'ortttat.i,:mtlJtts/, I:,c ttscd.
Sntool, hing proc('ss is ahnost  ('s-scntia,\] in t lMM tmcattse I IMM has sevet'c d:at,;tsparseness prol>hmt.4 combin ing  in fo rmat ion  sources4.1 l inear  in l ; ( ' , \ ] r lmlat iouVarious Mttds o\[' inlol'ntal, ion sf:.ttr(;(~s and differ-out knowledge sources must, he Colnl)incd l.O S()IV("the l,a.gging prol>l(+m.
The gemwal method usedit( I lMM is l inear iul,erl~olatiot L which is l,hew(gght, ed sutnmal, ion of all prol)alfi l i l:y infornlat, ion,%o11 r c(:s.kP ....... ,,.,,,,(,,'7,) -: ~ A~/'~I,,,I/,) (!
))/=1wlt~t'c 0 < Ai N I mid }3:  Ai = 1.This  h ie(hod cnn I)e used I)ol, h as a way of con>Itining Mtowh:dg(" sources and snloot, hing infornm-t, iotI sou t'.
::cs.I1MM based l.agging modd times un igram,  t>i-gl'atll a, lld t.rigt'altt in\[ortn;d.iott.
These in for(mr(ionsources are l inearly cotnl>ined by weighl.cd Slllliliift-l.ion.237P(zg It~-~, tg_2) = A1 P(ti Iti_l, li-2) + A2P(ti Iti- 1)(lo)where A1 + A2 = 1.
Tire parameter l land A2can be estimated by forward-backward algorithm(Deroua86+) (Charniak93+)(tIUANG90+).Linear interpolation is so advantageous becauseit reconciles the different information sources in astraightforward and simple-minded way.
But suchsimpliticy is also the source of its weaknesses:?
Linearly interpolated information is generallyinconsistent with their information sourcesbecause information sources are heteroge-neous for each other in general.?
Linear interpolation does not make optimalcombination of information sources.?
Linear interpolation has over-estimationproblem because it adjusts the model on thetraining data only and has no policy for un-trained data.
This problem occur seriouslywhen the size of the training data is not largeenough.4.2 ME(max lmum ent ropy)  pr inc ip leThere is very powerful estimation methodwhich combines information sonrces objectively.ME(maximum entropy) principle (,laynes57) pro-vides the method to combine information sourcesconsistently and the ability to overcome over-estimation problem by maximizing entropy of thedomain with which the training data do not pro-vide information.Let us describe ME principle briefly.
For givenx, the quantity x is capable of assuming the dis-crete wdues xi, (i = 1, 2, ..., n).
We are not giventhe corresponding probabilities pi; all we know istire expectation value of the function f,.
(x), (r =1, 2, ..., m):;qE\[fr(x)\] = Ep i (x i ) f , .
(x i )  (1l)i=1On the basis of this information, how can wedetermine the probability value of the functionpi(x)?
At first glance, the problem seems insol-uble because the given information is insufficientto determine the probabilities pi(x).We call the function f,.
(xi) a constraint functionor fealure.
Given consistent constraints, a uniqueME soluton is guaranteed to exist and to be of theform:where the Ar's are some nnknown constants tobe found.
This formula is derived by maximizingthe entropy of the probability distribution Pi assatisfying all the constraint given, qb search thel,.
's that make pi(x) satisfy all tile constraints, anexternalobservation:OOO W\[_2 Wi 1 Wi Wi+l Wi+2 oooMRF:  .co { ~ ~  eee <L.V <?L~/ ',,IL/ <d J  <L.vFigure 1: MRF T is defined for the neighborhoodsystem with distance 2iterative algorithm, "Generalized Iterative Scal-ing" (GIS), exists, which is guaranteed to convergeto the solution (l)arroch72+).
(12) is similar to Gibbs distribution, whichis the primary probability distribution of M\[{Fmodel.
MRF model uses ME principle in combin-ing information sources and parameter estimation.We will describe MRFF model and its parameterestimation method later.5 MRF-based  tagg ing  model5.1 MRF in taggingNeighborhood of given random variable is definedby the set of random variables that directly atfectthe given random variable.
Let N(i) denote a setof random variables which are neighbors of ithrandom variable.
Let's define the neighborhoodsystem with distance L in tagging fbr words W =wl, ..., w,~, and tags T = h ,  ..., t,~.N(i) = { i -  L , .
.
.
, i -  1, i+ l , .
.
.
, i+ L} (13)This neighborhood system has on(; dimensionalrelation and describes the one dimenstional struc-ture of sentence.
Fig.
1 showes MP~F T which isdefined for the neighborhood system with distance2.
The arrows represent that the random variableti is affected by the neighbors ti- 2, ti- 1, ti+ t, ~j+'~.It also showes that t i , t i - t  and ti,ti+l have theneighborhood relation connected by bigram, andthat t i , t i - l , t i -2  and ti,ti+l,ti+2 have ttm neigh-borhood relation connected by trigram.A clique is defined as the set of random vari-ables that all of the pairs of random variables areneighborhood in it.
Let's define the clique as thetag sequence with size L in tagging problem.G = {ti-L,ti-(t,-1), ...,ti} (14)A clique concept is used to define clique fimctionthat evaluates current state of random variables inclique.The definition of MRF is presented as following.Definil~ion of MI{F: Random 'variable T isMarkov random field if T' salisfies the follow-ing two properties.238Posit iv i ty :t)('F) > O, W' (15)Locality :S'(t~I%,Vj, j ?
i) = P(t~I%,Vj, j ~ iV(j))We assume tha.t every prob;d>lity value e l  tag se-(luenee is larger l, hml zero bee;rose ungraluiuat, ic;dSellt, ellCeS (;fill ,tl)pem" in htlllHill l~tligll&ge liS~ge,including meaningless sequence of characters.
St)the posit ivity of' MRt!'
is satislied.
This :+tSStllnp-tion results in the robustness mid ada.ptability ofthe inodel, eveli though unti:a~ined events ocolir.The locality of MRF is consistent with the as-Sll i l iptioi i  O\[ I;a.ggiilg t)roblein in that  the tag ofgiven word ca, it be deterinined by the local con-text.
(Tonsequenl, ly, the random variable 7' isMRF for neighborhood systenl N(i) its 7' satis-ties the positivity and the locality.5.2 A Poster io r i  Prot iat i i l l tyA posteriori probat)ility is needed to sea.rcb forthe Jrlost, likely tag sequence.
M II, F provides thei;heoretical bi~cliground about the probal)ility ofthe system (Bes~tg74) ((leiJfla, ii84+).H~mniersley-(\]liflbrd thcorein: 7'he probabilitydish'ib'ulio'n I'(7') is (Tibbs dish'ibulion if andonly if 'random wzriable 7' is ,,llarkov randomfield for givcn ncigborhood syslc'/n N(i) ,e. ",'~,, uCr)P(7 ' )  - Z (17)Z = Ee-~"~'~l(m) (:18)where "\['HI is l;elillJel'i~tllre~  is nor lna l i z ing  COIl-Sl;~tllt, cal led par t i t ion  ftlllCLioil aAld U ('\[') iS etlergyfimct;ion.
The a priori probal)ilit;y P(7') of tag se-quence 7 ~ is Gibbs distribution because the ran-dora variable 7' of tagging is MRF.It can be proved that a posteriori probabilityi)(TiW) for given word sequen(;e W is also Gil)bsdistribution (Chun93).
(7onsequent/y, a I)osterioriprobability of 7' for giwm W ist , u(.rlw) 1'('VlW) = - - /~-  ~ ( i ,q  ZWe llSe ( |9 )  i,O ('.m'ry Ollt MAP estiui;dion in thetagging model .
The energy function U(TJW) isof this form.u@'lW) = ~ w~(;t'lW) (20)cwhere V,, is clique function wii;h the propertythat Vc depends only oil those randoui variable, inclique e. This lllelLllS t;hat ellergy funcl, ioli (Urill beobliained \[rOlll each clique funtion which splits l,\[ieset of ralldOll l  viu' iables to slibscLs.6 Cl ique  funct ion  des ignThe more state of random variables are near toLlie solution, the niore the system becomes table,and energy function has lower vahie.
Energy flmc-i, ion repre, sents the degree of unstability of currentstntc of raiidoni vl.triables in M RF.
It is similar tothe I)ehaviour o\[' molecular particles in the rcMworld.
('~lique function is proportional to energy fun(:-tion, and it represents the unstability of currentstate of randoni varia.bles in clique or it has highvalue when the state of MRF is bad, low valuewhen the st;~te of MI{F is nero: to solution.
Cliquefimction contributes to reduce the comi)utation ofevahmtion function of entire MRF by clique con-cept that separates random v~triables to the sub-sets.
(llique function V/(TJ W) is described by the.
few.tures that represent the constraint or informationsources of givcu prol)h;m domain.~5(:z'lvv) := ~ a,.j;~.
('clm) ('2J)F6.1 MRF Mode l  1 (Basic mode l )The basic information sources which arc used mstatistical tagging model are unigram, l)igrani nndtrigrain.
M I{I" nlodel I lises unigrmn, higranri andt, rigraln.
We write the \[ea?ure furiction o\[ unigraln;iSj\~,,.:,,.
.... = (t - ~'(t~I,<)) (~)and the feature f'illtCtioll O\[ II-gralll, inchidingbigram, trigram ~sf li, - :, ?
..... =where(t - Z'(t~ I J)) (m3)ioN(i)t'(ti lt i_j,t~_j+x,...,t~_t), i I ' i  > jP(ti IJ) = t"(6: Iti+l, h+~, ..., zi+j), it" i < jThe clique filnction of the model 1 is ttt~de asfollows./01 ' lw)  -- A, ?/;,,,~,.,,,~ + x~.
f , , - : , , .
,m (~4)6.2 Mode l  2 (Morpho log ica l  in fo rntat ionindnded)Morphok)gical evel information helps tagger todetermine the tag of the word, more.
especiallyof the unknown word.
The suffix of a word givesvery useful information about the tag of the wordin F, nglish.
The (:li(ltte function of model 2 is de-lined asf.~,\]:i~, = ( \ [ -  t'(gi\].suffix(wi))) (25)We used the statistical distribution of the sixtysll\[l ixes thztt are IlK)st f requent ly  used ill Engl ish.239We can expand the clique flnlction of the model1 easily by just adding Stlficix inforui~-ttion to theclique function of the ntodel 2.'~,~.
(7' IW) = A~ J;, , ,o, '  .....
+Ae.f,+_<,.a,,,+Aa.f.~ff~.
(2 5)6.3 Mode l  3 (er ror  correct ion)There exist error prone words in every ta.gging sys-tern.
We adjust er ror  prone words 1)y collectingthe.
error results and adding more inforniation ofthe words.
The feature function of Model 3 is foradjusting errors in word level.= (1 - (2r )f#vo,.2 = (;1 - P(lil'wi_2,ti_l)) (28)YVe used the probat)ility d istribu tion of five hun-tired error proiie words ill Model 2 in oMer to re-duce the tltllllber 0t' paF31ileters.7 Opt imizat ionThe process of selecting the best tag sequenceis called ms optimization process.
We use MAP(Maximum A l)osteriori) estiniation method.
Thetag sequence 7' is selected to niaximize the a pos-teriori probM)ility of tagging (19) by MAP.Simulated annealing is used to searcti the op-timal tag sequence as Gibbs distribution providessimulated anneMing facility with teliiperatur(+ arideileFgy ('OllCept.
}go change the tag candidate ofone word selected to tninilnize the energy t"iinctionin k-th step froni T (k) to j,(k+i) , a.n(l l'(+'t)e;/t thisprocess until there is tlO change.
The t(?llll)(?l?a-ture 7'm is started in high vahle and lower to zeroas tile above process is doing.
Then the final tagseqtlellce is the solution.
Sininlat,ed annealing isUS0flil in the prol)leni which has very hugo searchsl)ace, and it is the approxiniation of MAP est.i-fllatioll ((\]elll&iq 84 -t--).There is another algorithm called Viterbi algo-rithtn to lind ol)timal solution.
Viterbi algoritl lmguarantees optinial sohltion \]tilt, it canilot bc usedin the probleln which has very huge search space.SO it iS /iscd in the l)rol)leni which has Slliall sea, rchspace 3,11(1 Ilsed ill I1M M. M RF model Call ilSe bothViterbi algoril, hni and siinulated anealing, but itis not \](nowtl IO IlSe sinitllated allne, aling ill fIMM.8 parameter  es t imat ionThe weighting parameter A in tile clique \['unction(19) Call be estiinated frOlil training data by MIgprinciple (.\] ayiles57).Let tlS descrit)e ME princil)le and IIS algorithnibriefly.
For given x = (X l , .
.
.
, ; F r t ) ,  the corr(?-Sl)onding probal)ilities t)i(xi) is nod klloWll.
Allwe know is the expectation value of the flmctionJ;+(x), (r = 1,2, .. .
,m):?
t\];;\[J;.
(,)\] = pg(.<)J; .
( .+:) (2.<))i=1(riven consist(mr constraints, we can find theprot)ability distribution p~ that niakes the entrol)y-- ~ Pi In t)i wlhle lllaxillllllil \])y llSillg Largl:angiallniultipliers in the nsual way, and obtai u the result:pi(a?i ) = cXt)(-- ~ J,.J;.
(xi)) (30)7"This forniula is alniost siinilar 1.o Gibbs distri-bution (17), also J\].
correspoilds to the feature ofclique function in M I{,F (20) (2l).
Using this fact,we can use M 1!
; in paranieter est imation in M Ii.F+We can derive (31) to be used in pgLralneter es-t iniation fi'om training data.0- o-A-t,+z -- (31)7z _- (32)i r'l?o solve the solution of it, a numerical analysismt;thod (-~IS ((\]enerlaized Iterative Scaling) wassuggested ( l )arroch72+).
Pietra used his owu al-gor i thm IIS (hnt)roved lterative Scaling) based onG IS to induce the features and parameters  of ran-dom field automatical ly (l)ietra95) .
Following isIIS algorithnll lS(In)provcd lterative Scaling)t Initial dataA rof'ol'ellce distribution 1), all initialmodel q0 and fl), f l  .... , fn .?
Outputq.
alld A by MI'\] estiiiiationA lgori t h m(0) Set qC0) = qs)(1) Per each i (ihd Xi, t,il(; llnique sohlth)nel"q(X.
)fi(7,)c~,lk) ~,.
f,.
(T) =- ~ IS(T));(7/, )T 7'(33)(2 )  k +-- k+l ,  set q~+l with new Ai(3) l\[' qt~O tins converged, set q, = q(~')and tertilhiate.
Otherwise go to step(i)where q(k) is the distribution of the iriodel in k-th step, alld it, corresponds to the posteriori pro}>ability of the tagging model (\[.(J).
A, tile sohltion()f' (:/:t) (:all be ol)i,ained 1)y Newton niei.hod ((,'tlr-tis89+), Olie Of lilllll(~rica\] analysis nietilod.The I'ef?TellC(" distribution \]~ is the l)rol)abilitydistril)ution which is obtaiued directly frOlll ill'aiD-illg data.
\]) corresponds to tile posterior (listri-button t'(TIW ) ill the IA/g~illg iItod(ti.
~?Vo tlS( t the240Model Tagging accuracy11MM 96.11 1 M H,F(l) !
)6.2 M I{,F(2) 9(i.5 M I{F(3) .97.
ITable 1: Measuring l, hc.
a('cur;~cy of I IMM audM R,F nioch;Is.posterior t)rol)alf i l i ty of l:,hc words sequence o\[ win-{low size n (CSl>ecially 3 in this lliodel) I)y col i l l t ingthe entry Oil t ra ining data.
Tra i l i i i lg  data l l lewistagged (:orl)us tmrc.9 ExperimentsThe Ili;,I, ill ol>jcctivc of' this experinicnts is to ('Olii-l)a, re ttio M I{i!'
l,a,gghig nlodcl  w i th  l lic I IM M tag-ging nl()(hJ.
\?C consl, lulci(xl a, ~,'11{1" /.aggcr mid aII MM tagger usiilg sa.lne inl'orlll;tli ion on t.hc sailic(?llVil:onlllelll;.li, is lle(:esSa, l:y t,o do smooth ing  tiroccss for datasl)arseiicss l>roblelit which is scw;l:c i\[l \ ]1MM, whileMRF has tll(' \ [aci l i ty of sniooi,hing in it,self likeneura, l-nel, .
IvVc ilSCd line;tr inl,erl>olat, ion ine/,hod(l),~rot,a.S6+) (jclin&Sg) and assigning &C,lUel,cy1 for ui lknowll word (Weisctig:/+) for s l i iooth i l igin II M M.!t"V'(?
llS(?
(I l,\[lC \] lrowII  (;orlillS ill l icnl iTl 'ce Bank,dcscril)cd in (Ma.rc,s93+) with ,l~ dilli'rent, tags.A set of t~00,000 words is colhx'tod for ('ach parl,Of l i rowlt  ('Orl)llS ali(\[ llSe(I ;t,"; t.l'aiilillg (h~l;a, whichis used to 1)uihl 1,tw niodels.
And a set of 30,000words (',()l'l)llS is used as i,c'st da.C~, which is usedto t,esl, the qua.lil;y of Ltic models.'
l 'alJe 1 shows the 3~CClll;&(;y o\[" each L~ggillgniodcl.
'1't,~ average a(:(:ura.
(:y of tll(' I IMM-hascdl,a,gg(;r iS Sii n ilar i,o t Ila, t of M I{F( 1 ) l,a, gger I)c(:aiiS(~l,hey iis( 1 \[,he SalilO hil'ornial, h)n.ld<% 7 sliows dial, l,he error tale as \[,iic sizeo\[ \],r;IJnhig dnla, is illcreased.
M I{F( I )  has lowererror rate Lha, n that of i lMM when l, lie size oftraining data is slnMl.
'\]'hc crrol: l:~tl, e of M t{,F(2)is decreased CSlmcially wllell t lw size of the tr{dn-ing dab~t is StlllJl, l)c:(;a.llSC luorphologicvd ia\['ornia.-don helps I,t,~ process of l l l lk i iowi i  words.
Fi l ia l ly,MI{ l" (3)  show itnl iroveinent as the size of l;rain-ing (hfl,;~ grows I)ul; COllVel'g(is l;o l, ile \ ] in l i t  Oll sOl\]l(?poinl,s.These e?pcrilnen~s how thai, M tt, F has I)el, l,cra,d(lal)l;abilil, y with snlMI I, ra i l l i l ig data than II MMdoes, aud l,h;fl: M I{F  tagger has bss data sl)arseIleSS probhmi  than I IMM l:aggor.<;04OHt4i,l{e20 MRF ( L I~I}{!."
\[ 2 )i0do0 41 .
.
.
.
L000 /<: .
.
.
.
.
.
13 .
.
.
.
.
.
.
.
.
; .
.
.
.
.
.
.
% .
.
.
.
t .....Ji <~t tLa I I l l \ [ I  I WL~\[ Il " igurc :7: Error  ral,c of  each model  \[or g iwm sizeof t, raining word10 Comparison of MRF withHMMWe (:~\[ii derive l, llc si inldif ied cquatioi l  of I IMMonly wilh bigra.m :l>(v'tw)=(35) is consid(m~d as l, he Inull, i lfl ied probal>ilticso\[ a the h)cal cwml,s.
The iioarer the probabi l i lyvahm of local cv(mt is to zero , t, hc ,~or(' it, a\[Ii;cl,sI,h(' l)robahili l:y of the (ml, ire evenl,.
This prol)ertystr ict ly reflccl;s on the cwmt.
'-; which does not occurin l,rainhig (lat,~L \[:Jui, lid prohibits even the cvcul.that does l lot OCCllr in l;rainiug datl h althougtl theCVellt is legal.M I{I" can he sinil) l i f icd t)y lhe sunnlml:ion ofcl ique \['un(;tion as (3{\]).I - ~%:{ v ,+v,+.
.
+<,,} (3(J) /'('/'lW)-= 2!
'vl I{,1" uses rvalual, io,  funclion I)y suunuali<).,while I IMM do('s I)y umlt i l ) l ic~tion.
F, ven i f  acliqm~ flmcl,ion wd.e  is very bad, o(,hcr cliqncfunction ca, n conll)ensate dequa~ely lmca,use theclique functions are coime('l,ed by summatiou.
'l'here is no crit ical point of postcriori l)robal)lil,yin MRI  i', while I IMM has cril, ical poi,1, in zerovalue.
This property results in the rolmstness midthe ada, ptab i l i ty  o\[ l;tl(~ ll iodol aud niakcs M t{Fuiodcl stronger in data Sl)arscncss probhml.11 Conc lus ionWe prol)oscd a Ml:\[l!
'-based tagging lnodel.
In-formation sourccs for tagging aro combiimd hyM F, 1)rincil)le which is ,sod i ,  M I{F as theoreticalbackground.
A I1 I)ara~liiclxu's used in the iiio(\]e\] areesl Jmated from tra, ining data a.ul,omatica,lly.
As;t result,, our MRl!
'-l)ascd tagging nlodcl has bet,-ter tmrfornla, ncc Lha, n t \ [MM tagging nio(h'l, CSl)Ccial ly when the size of" the training dal,a is Sliiall.Vv'e haw~ sooii l, hat tim i)or\['oriilali(:o Of l,he M HI"-I)ascd tagging nio(hJ can be' i inprow'd by addinginl'orina, i ion I,o tim niodct.241ReferencesBesag, J.
"Spatial interaction and the statisticalanMysis of lattice systems( with discusstion ),"J. Royal Statist.
,%c., series B, vol.
36, pp.
192-326, 1974.Besag, J.
"On the Statistical Analysis of DirtyPictures", J. Royal Statist.
Soc., vol.
B48, 1986.Brill, E. "A Simple Rule-Based Part of SpeechTagger", In Proceedings of the 3rd Co@ on Ap-plied Natural Language Processing, pages 153-155, April, 1992.Charniak, E., C. ltendrickson, N. Jacobson andM.
Perkowitz, "Equations for Part-of Speech'Fagging," Proc.
of Nat'l Conf.
on Artificial\[ntelligence(AAAI-86), pp.784-789, 11993.In, G. Chun, "Range hnage Segmentation UsingMultiple Markov Random Fiehls", Ph.D. thesis,KAIST, KOt{EA, 1993.Church, K. W., "A Stochastic PAI~;rS Pro-gram and Noun Phrase Parser for UnrestrictedText,", Proceedings of Applied Natural Lan-guage Processing, Austin, Texas, pp.
136-143,1988.Darroch, J. N. and D. Ratcliff, "Generalized It-erative Scaling for Long-Linear Models..", TheAnnals of Mathematical Statistics, Volume 43,pages 1470-1480, 1972.Derouault, A. M. and B. Merialdo, "NaturalLanguage Modeling for Phoneme-to-Text Tran-scription,", \[EEE Tr.
on Pattern Anaysis andMachine Intelligence, vol.
PAMI-8, no.6, Nov.1986.Curtis, F. G. and Patric O. Wheatley, "AppliedNumerical Analysis", forth edition, ADDISONWESLEY, 1989.Forney, G. D., "The Viterbi Algorithm", Proc.
ofthe IEEE, vo\[.
61, pp.
268-278, Mar.
1973.Gamble, E. B., Geiger D. and Possio T., "in-tegration of Vision Modules and labeling ofSurface Discontinuities", IEEE Transactions onsystems, man and cybernetics, vol.
19, no.
6,November/deeemver 1989.Geman, S. and Geman D., "Stochastic Relax-ation, Gibbs Distributions, and the BayesianRestoration of Images", IEEE transactionson pattern analysis and machine intelligence,VoI.PAMI-6, NO.
6, NOVEMBER 1984.Geiger, D. and Girosi F., "ParalM and Determin-istic Algorithms from MRF's: Surface Recon-struction", \[EEE Transactions on pattern anal-ysis and machine intelligence, VOL 13, NO.
5,MAY 1991.IIUANF, X.D., Y. AtHKI and M.A.
JACK, "Hid-den Markov Models for Speech Recognition",1990.Jaynes, E. T., Information Theory and StatisticMMechanics, Physics Reviewsl06, pages 620-630,1957.Jelinek, F. , "Self-Organized Language Model-ing for Speech Recognition."
, in Readings inSpeech Recognition, Alex Waibel and Kai-l,'uLee(Editors).
Morgan Kaufinann, 1989.Kupiec, ,l., Robust Part-of-Speeh Tagging Usinga llidden Markov Model, Computer Speech andLanguage, 1992.Marcus, M. P., Beatrice Santorini and Mary AnnMarcinkiewiez, "Building a large annotated cor-pus of English: the Penn Treebank", Computa-tional Linguistics, Vol.
19, No.
2, pp 313-330,June, 1993.Merialdo, B., "Tagging English Text with a Prob-abilistic Model" ,  Computational Linguistics,Volume 20, no 2, June 1994.Nakamura, M., K. Maruyama, T. Kawanata andK.
Shikano, "Neural Net-work Approach of Word Category Predictionfor English rl~XtS," Int'l Conf on ComputationalLinguislics(Coling-90), pp.
213-218, 1990.Pietra, S. D., V. D. Pietra and J. Lafferty, "Induc-ing features of random fields", Carnegie MellonUniversity, Technical report CMU-CS-95-144,MAY, 1995.Rosenfeld, R., "Adaptive Statistical languageModeling: A Maximum Entropy Approach", Carnegie Mellon Uniw~rsity, technical reportCMU-CS-94-138, April 19, 1994.Weischedel, 1{., R. Scewartz, a. Ralmucci, M.Meteer, and L. P~awshaw.
"Coping with Am-biguity and Unknown Words through Prob-abilistic Models", Computational Linguistics,19(2):359-382, 1993.Zhang, J. and J.W.
Modestino, "A Markov Ran-dora Field model-based approach to image in-terpretation", Visual Communications and im-age Processing IV, Vol 1199, 1989.242
