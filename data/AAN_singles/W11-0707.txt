Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 48?57,Portland, Oregon, 23 June 2011. c?2011 Association for Computational LinguisticsAnnotating Social Acts: Authority Claims and Alignment Movesin Wikipedia Talk PagesEmily M.
Bender?, Jonathan T.
Morgan?, Meghan Oxley?, Mark Zachry?,Brian Hutchinson?, Alex Marin?, Bin Zhang?, Mari Ostendorf?
?Department of Linguistics, ?Department of Human Centered Design and Engineering?Department of Electrical EngineeringUniversity of Washington{ebender,jmo25,what,zachry}@uw.edu, {brianhutchinson,iskander,binz,mo}@ee.washington.eduAbstractWe present the AAWD corpus, a collectionof 365 discussions drawn from Wikipedia talkpages and annotated with labels capturing twokinds of social acts: alignment moves and au-thority claims.
We describe these social actsand our annotation process, and analyze theresulting data set for interactions between par-ticipant status and social acts and between thesocial acts themselves.1 IntroductionThis paper presents a new annotated resource: theAuthority and Alignment in Wikipedia Discussions(AAWD) corpus (available from http://ssli.ee.washington.edu/projects/SCIL.html).The AAWD corpus contains discussions fromEnglish-language Wikipedia talk pages extractedfrom the 2008 Wikipedia data dump and annotatedfor two types of social acts: authority claims andpositive/negative alignment moves.
In brief, anauthority claim is a statement made by a discussionparticipant aimed at bolstering their credibility inthe discussion.
An alignment move is a statementby a participant which explicitly positions them asagreeing or disagreeing with another participant orparticipants regarding a particular topic.These annotations are intended to make acces-sible for automated processing two interesting andcharacteristic aspects of interaction in online discus-sion forums.
As a dataset for computational andsociolinguistic analysis, the discussion pages withinWikipedia are valuable for several reasons.
First, theinteraction among the participants is nearly entirelycaptured within the dataset, and all of the ?identity-work?
(Bucholtz and Hall, 2010) done by Wikipediadiscussion participants needs to be done directly inthe text of their comments.
Furthermore, the discus-sions tend to be task-driven, focused on the sharedgoal of improving the associated article.
This leadsthe data to be a particularly rich source of linguisticexpressions of authority and alignment.Our annotations represent a kind of informationwhich is rather different from that involved in NLPtasks such as POS tagging, morphological analysis,parsing and semantic role labeling.
Such tasks in-volve recognizing information that is implicit in thelinguistic signal but nonetheless part of its struc-ture.
Tasks such as named-entity recognition andword sense disambiguation are also close to the lin-guistic structure of the signal.
Authority claims andalignment moves, on the other hand, are examplesof communicative moves aimed at social position-ing of a discussant within a group of participants,which may be specialized dialog acts but are referredto here as ?social acts.?
We distinguish social actsfrom ?social events?
as described in (Agarwal andRambow, 2010): social events correspond to typesof interactions among people, whereas a social actis associated with a fine-grained social goal and re-flected in the specific choices of words and ortho-graphic or prosodic cues at the level of a turn.The primary value of this new data set is in facil-itating computational modeling of a new task type,i.e.
the identification of fine-grained social acts inlinguistic interaction.
While there has been someprior work on detecting agreements and disagree-48ments in multiparty discussions (Hillard et al, 2003;Galley et al, 2004), which is related to detectingpositive/negative alignment moves, most previouswork on authority bids has involved descriptive stud-ies, e.g.
(Galegher et al, 1998).
Computationalmodeling of these phenomena and automatic detec-tion will help with understanding effective argumen-tation strategies in online discussions and automaticidentification of divisive or controversial discussionsand online trolls.
We believe that these tasks alsoprovide an interesting arena in which to study lin-guistic feature engineering and feature selection.
Aswith tasks such as sentiment analysis, a simple ?bag-of-words?
model with word or even n-gram-basedfeatures is not sufficiently powerful to detect manyinstances of these social acts, where combinations ofpositive and negative words must be interpreted incontext, e.g.
absolutely is positive alone but ampli-fies a negative in absolutely not, and yeah in yeah,I want to correct something John said of coursedoesn?t necessarily indicate agreement.
The typicalscenario where hand-annotated training data is lim-ited presents a challenge for learning phrase patternsthat discriminate social acts.In the remainder of this paper, we further describethe social acts and annotation schemata (Section 2),provide details of the AAWD corpus (Section 3),and analyze the distribution of the social acts (Sec-tion 4).
This analysis describes the distribution ofthe social acts and tests hypotheses about their inter-actions with each other and with user status.2 Annotation Schemata2.1 Authority ClaimsThe ability to persuade others to believe in one?sstatements or the soundness of one?s judgments isa necessary component of human social interac-tion.
In order to establish the necessary credibil-ity to secure the belief or assent of others, commu-nicators will often couch their statements in somebroadly-recognized basis for authority.
These ?ar-guments from authority?
have been recognized asan important component of informal logic by manylanguage philosophers (Liu, 1997), including JohnLocke (1959 [1690]).
In recent decades the self-presentation of authority has been studied in a va-riety of spoken and written contexts by scholarsfrom disciplines such as communication, rhetoric,health studies, sociolinguistics, linguistic pragmat-ics and political science in order to understand thestrategies that communicators operating in differ-ent genres and media employ to establish them-selves as credible discursive participants.
Studiesof online product reviews (Mackiewicz, 2010), on-line political deliberation (Jensen, 2003), scientificpublications (Thompson, 1993), online forum posts(Galegher et al, 1998; Richardson, 2003) and radiotalk-shows (Thornborrow, 2001) have revealed thatconsiderations of genre, medium and social contextall shape the ways interactants attempt to claim theauthority to be listened to and taken seriously.From the perspective of discourse analysis, au-thority claims provide an interesting lens throughwhich to view a text, as the overall frequency ofclaims can reflect the nature or purpose of the dis-course (e.g.
task-oriented collaboration vs. undi-rected conversation) and the distribution of claimtypes can reveal features of the social context inwhich they are made, such as shared norms, prac-tices and community values.
For example, since cer-tain bases for authority may be seen as more credi-ble than others in certain contexts (such as citationof peer-reviewed publications in academic scholar-ship, or references to personal experience in onlinesupport groups), the prevalence and distribution ofdifferent types of claims in a written text or a con-versation transcript can illuminate the shared valuesof speakers and audiences in a given genre (Galegheret al, 1998).
Although the linguistic construction ofauthority claims can vary greatly according to thegenre of the communication, within a single genrethere is often great regularity in the ways claimsare made, such as the common I?m a long-timelistener introduction used by radio talk-show call-in guests.
Even across genres, recognizable typesemerge: references to personal credentials (such aseducation or profession) are found to be importantin newsgroup messages (Richardson, 2003), productreviews (Mackiewicz, 2010) and online scientific ar-ticle comments (Shanahan, 2010).Our taxonomy of authority claims was itera-tively developed based on our empirical analysisof conversational interaction in two different gen-res: political talk shows and Wikipedia discus-sion pages (Oxley et al, 2010), with reference to49the literature cited above.
Our codebook (avail-able from http://ssli.ee.washington.edu/projects/SCIL.html) includes detailed defini-tions as well as positive and negative examples foreach claim type.We classify authority claims into the followingtypes (examples are drawn from our data):Credentials: Credentials claims involve refer-ence to education, training, or a history of work inan area.
(Ex: Speaking as a native born Midwest-erner who is also a professional writer.
.
.
)Experiential: Experiential claims are based onan individual?s involvement in or witnessing of anevent.
(Ex: If I recall correctly, God is mentioned incivil ceremonies in Snohomish County, Washington,the only place I?ve witnessed one.
)Institutional: Institutional claims are based onan individual?s position within an organization struc-ture that governs the current discussion forum or haspower to affect the topic or direction of the discus-sion.
(Not attested in our corpus.
)Forum: Forum claims are based on policy,norms, or contextual rules of behavior in the in-teraction.
(Ex: Do any of these meet wikipedia?s[[WP:RS | Reliable Sources ]] criteria?
)External: External claims are based on an out-side authority or source of expertise, such as a book,magazine article, website, written law, press release,or court decision.
(Ex: The treaty of internationallaw which states that wars have to begin with adeclaration is the Hague Convention relative to theOpening of Hostilities from 1907.
)Social Expectations: Social Expectations claimsare based on the intentions or expectations (whatthey think, feel or believe) of groups or communitiesthat exist beyond the current conversational context.
(Ex: I think in the minds of most people, includingthe government, the word ?war?
and a formal dec-laration of war have come apart.
)2.2 Alignment MovesIn multiparty discourse, relationships among par-ticipants manifest themselves in social moves thatparticipants make to demonstrate alignment with oragainst other participants.
Expressing alignmentwith another participant functions as a means ofenhancing solidarity with that participant while ex-pressing alignment against another participant main-tains social distance between conversational partic-ipants, particularly in situations where participantsmay be previously unacquainted with each other(Svennevig, 1999).
Changes in the alignment of par-ticipants toward one another or ?shifts in footing?may reflect changes in interpersonal relationships ormay be more transitory, demonstrating minor con-cessions and critiques embedded within larger, morestable patterns of participant agreement and dis-agreement (Goffman, 1981; Wine, 2008).As Wikipedia editors negotiate about article con-tent, they make statements that support or opposepropositions suggested by other editors and therebypublicly align either with or against other editors inthe discussion.
Although ways of expressing agree-ment and disagreement vary according to power re-lations between participants, participant goals, andconversational context (Rees-Miller, 2000), pre-vious research has suggested that expressions ofagreement and disagreement in written language aremore explicit than oral expressions of agreement anddisagreement (Mulkay, 1985; Mulkay, 1986) andthat statements of agreement are particularly explicitin online discussions (Baym, 1996).We classify alignment moves into positive andnegative types, according to whether the participantis agreeing or disagreeing with the target:Positive alignment moves express agreementwith the opinions of another participant.
Positivealignment is annotated in cases of explicit agree-ment, praise/thanking, positive reference to anotherparticipant?s point (e.g.
As Joe pointed out.
.
.
), orwhere other clear indicators of positive alignmentare present.Negative alignment moves express disagreementwith the opinions of another participant.
Negativealignment is annotated in cases of explicit disagree-ment, doubting, sarcastic praise, criticism/insult,dismissing, or where other clear indicators of neg-ative alignment (such as typographical cues) arepresent.Based on our experience using the types of au-thority claims to diagnose and correct sources ofinter-annotator disagreement (see ?3.3 below), wedeveloped subtypes of positive and negative align-ment.
While these do not have the same theoreticalgrounding as the types of authority claims, they didserve the same purpose of improving our annotation50over time.We annotate a target for each alignment move,which may be one or more specific other parties inthe conversation, the group as the whole, or some-one outside the conversation.
In addition, we in-clude a category labeled ?unclear?
for cases wherethere is an alignment move, but the annotators arenot able to discern its target.
Again, the codebookincludes example subtypes as part of detailed defini-tions as well as positive and negative examples foreach alignment type.3 The Corpus3.1 Source DataWikipedia talk pages (also called discussion pages)are editable pages on which editors can take part inthreaded, asynchronous discussions about the con-tent of other pages.
All editors potentially interestedin a given article can join the conversation on thatarticle?s talk page.
Sometimes these conversationstake the form of a deliberative exchange or even aheated argument as editors advocate different ideasabout such things as the content or form of an ar-ticle.
Each edit to the talk pages is recorded as aunique revision in the system and thus becomes partof the permanent record of system activity.Wikipedia constitutes a particularly valuable nat-ural laboratory for studies such as this one, forseveral reasons.
First, the interaction among theparticipants is almost entirely captured within theWikipedia database: while some Wikipedians mightinteract with each other in person or in other onlinefora (such as IRC or mailing lists), this is the excep-tion rather than the rule.
Furthermore, while partici-pants often maintain persistent identities (usernamesfor registered users; IP addresses for unregisteredones) there are no cues to social identities availableto the participants beyond what is captured in thedigital record.
Therefore all of the effort that partic-ipants put into constructing their online identities isin the record for analysis.
Second, the discussionson Wikipedia talk pages tend to be goal-oriented, asthe discussion topic is the Wikipedia article that theparticipants are collaboratively editing.
This goal-orientation motivates participants to explicitly alignwith each other in the course of discussions and but-tress their arguments with authority claims.
Finally,the Wikipedia dataset contains rich metadata, suchas the date and time of each edit (identified by re-vision id) to every article or talk page; the editorresponsible for the edit (identified by username orIP address, depending on registration status); andmarkup such as hyperlinks and formatting used inthe textual content of each edit.
These metadata al-low for sophisticated data analysis at the editor level(e.g.
how many edits made by one editor in a givenspan of time) and the page level (e.g.
how many ed-itors have participated in a talk page discussion).The Wikimedia Foundation frequently releasesthe database dump of the Wikipedia pages in theform of XML (available at http://download.wikimedia.org).
The database dumps are cate-gorized into languages, and for each language, thereare XML files corresponding to different levels ofdetail in terms of the information they contain.
Toget the information on all revisions, we used thelargest database dump, which contains all Wikipediapages and complete edit history.
The XML file wasparsed and a database created locally with all therevision information for both main pages and talkpages.
We then constructed queries to retrieve themain pages and corresponding talk pages based ona list of topics for which extensive discussions arelikely to occur.Our data is drawn from a set of 365 discussionsfrom 47 talk pages.
The discussions were selectedto contain at least 5 turns and at least 4 human par-ticipants.1 The earliest edit in our data set is fromJanuary 29, 2002 and the latest is from January 6,2008.
A total of 1,509 editors collectively make6,066 turns in this data.
Of the 365 discussions,185 were annotated for both alignment moves andauthority claims.
An additional 26 were annotatedfor alignment only and an additional 154 were an-notated for authority only.
The numbers of editorsand turns in these sets are shown in Table 1.3.2 Annotation UnitsA Wikipedia talk page is in itself a wiki-style docu-ment.
Thus, each modification to a talk page by aneditor can modify multiple sections of the page.
Wedefine a ?turn?
as a contiguous body of text on the1Wikipedia discussions may also include contributions byautomated ?bots?.51Annotated forauthority alignment bothpages 47 36 36discussions 339 211 185editors 1,417 988 896turns 5,636 3,390 2,960Table 1: Pages, discussions, editors and turns in anno-tated datacorresponding page that was modified as part of asingle revision.
Thus, a single revision may result inmultiple turns being added.
Each turn may includeone or more paragraphs of text, either existing butmodified, or new additions.
We annotated authorityclaims at the paragraph level and alignment moves atthe turn level.
The larger unit is used for alignmentmoves because the phenomenon as defined can spana larger section of text.The annotation tool (a modified version of LDC?sXTrans (Glenn et al, 2009)) allowed annotators toindicate the presence and type of claims or moves ineach annotation unit, in addition to selecting spansof text corresponding to each social act.
For align-ment moves, within a turn, alignment of the sametype (positive or negative) with the same target wasannotated as a single alignment move, even acrossmultiple sentences.
Where the type or target dif-fered, we annotated up to three separate alignmentmoves per annotation unit.
For authority claims, wealso annotated up to three claims per annotation unit,with each claim identified by a single span of text.Claims in separate sentences of an annotation unitcounted as separate even if they were of the sametype.
Figure 1 gives an example from our codebookof a turn with multiple alignment moves.3.3 Annotation ProcessEach discussion thread was annotated independentlyby two or more annotators.
Inter-annotator agree-ment was calculated at weekly intervals to assessannotation progress and identify areas of disagree-ment.
Adjudicators also performed ?spot checks?
ofannotated data weekly and provided feedback whenthere were disagreements among annotators or whencodes seemed to be inconsistently or erroneously ap-plied.
The codebooks for authority claims and align-ment moves were also iteratively refined with the ad-dition of positive and negative examples and specificlinguistic cues commonly associated with particularmove or claim types based on spot-check results andannotator feedback.Two strategies that proved useful in maintainingconsistency in the frequency and reliability of cod-ing across annotators were the computation of av-erage agreement and comparison of overall countsof each codable unit on a weekly basis.
Comput-ing average agreement allowed adjudicators to iden-tify particular categories that were proving espe-cially difficult to code consistently, and to better fo-cus their efforts on re-training annotators and up-dating the relevant sections of the annotation guide-lines.
Comparing counts of the number of times twoannotators had coded a particular category over thesame number of discussions also proved useful foridentifying potential problems with under- or over-coding of a category by a particular annotator.3.4 ReconciliationThe manual annotation process was completed in-dependently by each annotator, resulting in multiplesets of labels.
To create a single copy of the data thatcan be used in learning experiments, an algorithmwas designed to merge the annotations into a single,?master?
version.
The algorithm balances annota-tion consistency and simplicity of the merging pro-cess.
We treat the annotations for each unit in a fileas a set with respect to type: Multiple labels of thesame type are treated as a single label for purposesof reconciliation, with only one label of each typeallowed for each annotation unit.We mark each social act which had been identi-fied by at least two annotators as having ?high con-fidence.?
If a social act was identified by only oneannotator in that annotation unit, it is marked as hav-ing ?low confidence.?
This procedure yields two setsof social act types found in each annotation unit, oneconsisting of the high confidence labels, and anotherof the low confidence labels.
The labels from eachset are kept distinct, i.e.
for each label in the highconfidence set, the corresponding label in the lowconfidence set has the suffix ?
single?
appended tothe high confidence label.Aggregated social act labels are propagated tothe sentence level by using a dynamic program-ming algorithm to match sentences (determined byautomatic segmentation) with the keyword spans52speaker turn transcript alignment1 alignment2 alignment3S1 3 <k1>S2, I think you?re right</k1>.
<k2>S3?s ideais way off base </k2>, but <k1> you seem to have agood solution</k1>.
<k3>But I disagree with yourname for the section</k3> ?
Iraq War is used in theUnited States media and should be used here as well.positive:S2::explicitagreementnegative:S3::explicitdisagreementnegative:S2::explicitdisagreementFigure 1: Example from alignment codebookbased on overlap.
A sentence could have multi-ple positive labels if one or more annotators la-beled it for different types in the high or low con-fidence set.
Sentences in turns with a markedsocial act but not aligned to text spans are la-beled as ?unused?
due to the ambiguity associatedwith a limit on the number of social acts anno-tated per unit.
All sentences in an annotation unitfor which no annotator found any positive labelsare labeled with the negative class.
The data dis-tributed at http://ssli.ee.washington.edu/projects/SCIL.html include both the underly-ing per-annotator files as well as the files output bythe reconciliation process.3.5 Annotation QualityIn complicated annotation tasks, such as those con-ducted in this work, establishing reliable groundtruth is a fundamental challenge.
The most popularapproach to measuring annotation quality is via thesurrogate of annotation consistency.
This assumesthat when annotators working independently arriveat the same decisions they have correctly carried outthe task specified by the annotation guidelines.
Sev-eral quantitative measures of annotator consistencyhave been proposed and debated over the years (Art-stein and Poesio, 2008).
We use the well-knownCohen?s kappa coefficient ?, which accounts for un-even class priors, so one may obtain a low agreementscore even when a high percentage of tokens havethe same label.
We also report the percentage of in-stances on which the annotators agreed, A, whichincludes agreement on the absence of a particularlabel.
When a set of instances have been labeled bymore than two annotators, we compute the averageof pairwise agreement.Scores for authority claim and alignment moveagreement are presented in Tables 2 and 3.2 For2Institutional claims are exceedingly rare in our data, ap-pearing in only three labels.
This is not sufficient for proper ?Claim Type N ?
Aforum 451 0.52 0.92external 715 0.63 0.91experiential 185 0.33 0.96social expectations 78 0.13 0.98credentials 6 0.57 0.99Overall 1157 0.59 0.86Table 2: Agreement summary for authority claims.
Ndenotes the number of turns of the given type that at leastone annotator marked.Move Type N ?
Aexplicit agreement 379 0.62 0.94praise/thanking 117 0.60 0.98positive reference 86 0.20 0.98explicit disagreement 453 0.29 0.92doubting 198 0.23 0.96sarcastic praise 38 0.30 0.99criticism/insult 556 0.32 0.91dismissing 396 0.16 0.91All positive 509 0.66 0.94All negative 1092 0.45 0.85Overall 1378 0.50 0.80Table 3: Agreement summary for alignment moves.
Ndenotes the number of turns of the given type that at leastone annotator marked.authority, the most common types of claims, forumand external, are also two of the most reliably identi-fied.
For alignment, the positive type has much bet-ter agreement scores than the negative type.
Inter-estingly, it appears that the fine distinctions betweenthe types of negative alignment move are a large fac-tor in the low agreement scores.
When all of thenegative categories are merged, agreement is higher,although still less than for positive alignment moves.Our ?
values generally fall within the range thatLandis and Koch (1977) deem ?moderate agree-ment?, but below the .8 cut-off tentatively suggestedcomputation, and so we do not include them in Table 3.53by Artstein and Poesio (2008).3 One possible rea-son is that the negative class is not as discrete as itmight be in other tasks: both alignment moves andauthority claims can be more or less subtle or ex-plicit.
We have designed our annotation guidelinesto emphasize the more explicit variants of each, butthe same guidelines can sometimes lead annotatorsto pick up more subtle examples that other annota-tors might not feel meet the strict definitions in theguidelines.
Thus we expect our ?high-confidence?labels to correspond to the more blatant examplesand the ?low-confidence?
labels, while sometimesbeing genuine noise, to pick out more subtle exam-ples.4 AnalysisWhile the main goal of this paper is to documentthe AAWD corpus, we also performed several sta-tistical analyses of authority and alignment, in or-der to demonstrate the relevance of these social actsas markers of user identity and social dynamicswithin our corpus.
In this section we present theoverall distribution of authority claims and align-ment moves, compare the prevalence of authorityclaims across user types, and show how a partici-pant?s claim-making behavior may affect how otherssubsequently align with them.
In doing so, we con-sider only high-confidence labels from files whichwere annotated by at least two annotators.
This sub-set includes 186 discussions annotated for alignmentmoves and 200 discussions annotated for authorityclaims.
Of those, 149 discussions were annotatedfor both types of social acts.4.1 Distribution of Social ActsWe find that 25% of the turns in our alignment datacontain alignment moves and 21% of the turns in ourauthority data contain authority claims.
In addition,35% and 32% of the editors in each set make align-ment moves and authority claims, respectively.
Thebreakdown by alignment move and authority claimtype is given in Table 4.
Note that any given turnmight contain both positive and negative alignmentmoves or multiple types of authority claims.3Artstein and Poesio also note that it may not make sense tohave only one threshold for the field.N %Alignment datatotal turns 2,890 100turns w/positive alignment 330 11.4turns w/negative alignment 467 16.2turns w/any alignment 710 24.6total editors 905 100editors w/alignment moves 315 34.8Authority datatotal turns 3,361 100turns w/external claim 459 13.7turns w/forum claim 260 7.7turns w/experiential claim 77 2.3turns w/soc.
exp.
claim 21 0.6turns w/credentials claim 3 0.1turns w/institutional claim 0 0turns w/any claim 703 20.9total editors 930 100editors w/authority claims 297 31.9Table 4: Summary of high-confidence alignment movesand authority claims4.2 Authority Claim Types by User StatusWikipedia distinguishes three different statuses: un-registered users (able to perform most editing activ-ities, identified only by IP address), registered users(able to perform more editing activities, edits at-tributed to a consistent user name) and administra-tors (registered users with additional ?sysop?
privi-leges).
Participants of different statuses tend to dodifferent kinds of work on Wikipedia, with admin-istrators in particular being more likely to take onmoderator work (Burke and Kraut, 2008), such asmediating and diffusing disputes among editors.
Be-cause conflict mediation requires a different kind ofcredibility than collaborative writing work, and be-cause unregistered users are likely to be newer andtherefore less likely to be incorporating referencesto Wikipedia-specific rules and norms into their pro-jected identities (and, therefore, their conversation),we hypothesized that editors of different statuseswould use different kinds of authority claims.Indeed, this is borne out.
While no user groupwas significantly more or less likely than any otherto include authority claims overall in their posts(chi square test for independence, n=3164, df=2,?2=2.367, p=.306) users of different statuses did usesignificantly different proportions of each type ofclaim (chi square test for independence, n=973, df=854Participant # % % % claim-type users forum external bearingturnsadmin 44 47.1 45.1 19.6reg 192 29.1 63.6 22.3unreg 55 18.3 70.6 19.8all 291 29.8 62.5 21.6Table 5: Percentage of authority claims of forum andexternal types, and percentage of total turns which con-tained claims, across user statuses?2=38.301, p<.001).
As illustrated in Table 5, ad-ministrators are more likely than the other groups tomake forum claims and less likely to make exter-nal claims, unregistered users make more externalclaims and fewer forum claims, and registered usersexhibit a claim distribution that more closely reflectsthe overall distribution of claim types.4.3 Authority Claim Prevalence by V-IndexGiven the few visible markers of status on Wikipediaand the fact that editors are constantly interact-ing with new collaborators, Wikipedians performauthority by adopting insider language and normsof interaction.
Supporting arguments with specificreferences is one such norm.
Thus we hypothe-sized that as editors become more integrated intoWikipedia, they will make more authority claims.In order to test this hypothesis, we developed ?v-index?
as a proxy measure of degree of integration or?veteran status?
within the community.
Inspired byBall?s (2005) ?h-index?
of scholarly productivity, v-index balances frequency of interaction with lengthof interaction.
Specifically, an editor?s v-index at thetime of a particular revision is the greatest v suchthat the editor has made at least v edits within thepast v months (28-day periods).We measured the v-index for each revision inour dataset, using all edits to Wikipedia in orderto calculate v (not just edits to the discussions wehave annotated).
The v-index values for edits withinour dataset range from 1 to 46.4 We measured theproportion of turns with authority claims (of anytype) for each v-index.
The proportion of turnswith authority claims is in fact positively correlated4The data becomes very sparse for v-indices above 29, withevery v-index in this range represented by < 10 turns, so thev-indices of 30-46 were not included in this analysis.Initial turn Alignment in next 10 turnsno auth.
claim 0.52any auth.
claim 0.63Table 6: Average prevalence of alignment moves targetedat participant in 10 following turnswith v-index, confirming our hypothesis (one-sidedPearson?s correlation coefficient, n=29 v-indices,r=0.371, p=0.024).4.4 Interaction of Social PhenomenaThus far, we have been addressing our social actsindependently, but of course no social act occurs ina vacuum.
Alignment moves and authority claimsare only two types of social acts; many other typesof social acts are present (and could be annotated) inthis same data set.
Even with only these two types(and their subtypes), however, we find interactions.We hypothesized that authority claims would belikely to provoke alignment moves.
That is, al-though participants may make alignment moveswhenever someone else has expressed an opinion ortaken action (e.g.
edited the article attached to thediscussion), we hypothesized that by making an au-thority claim, a participant becomes more likely tobecome a focal point in the debate.
To test this,we calculated, for every turn, the number of align-ment moves targeted at the author of that turn withinthe next 10 turns.
We then divided the turns intothose that contained authority claims and those thatdid not.
Making an authority claim in a given turnmade the participant significantly more likely to bethe target of an alignment move within the subse-quent 10 turns compared to turns that did not containany claims (t=-2.086, df=772, p=.037; Table 6))Furthermore, we find that different types of au-thority claims elicit different numbers of subsequentalignment moves.
Specifically, turns that contain ei-ther external claims or forum claims (the two mostprevalent claim types in our sample) interact dif-ferently with alignment.
External claims elicitedmore alignment overall (t=3.189, df=411, p=.002)and more negative alignment moves than did forumclaims (t=3.839, df=415, p<.001).
However, ex-ternal claims did not elicit significantly more pos-itive alignment moves than forum claims (t=0.695,df=309, p=.488).
This is illustrated in Table 7.55Initial turn Alignment in next 10 turnspositive negative overallexternal claim 0.26 0.49 0.74forum claim 0.22 0.20 0.42Table 7: Average prevalence of alignment moves targetedat participant in 10 following turns5 ConclusionWe have presented the Authority and Alignmentin Wikipedia Discussions (AAWD) corpus, a col-lection of 365 discussions drawn from Wikipediatalk pages and annotated for two broad types of so-cial acts: authority claims and alignment moves.These annotations make explicit important discur-sive strategies that discussion participants use toconstruct their identities in this online forum.
That?identity work?
is being done with these social actsis confirmed by the correlations we find betweenproportions of turns with authority claims and ex-ternal variables such as user status and v-index, onthe one hand, and the interaction between authorityclaims and alignment moves on the other.As an example of a social medium, Wikipedia ischaracterized by its task-orientation and by the factthat all of the interactants?
?identity work?
with re-spect to their identity in the medium is captured inthe database.
This, in turn, causes the data set to berich in the type of social acts we are investigating.The dataset was used for research in automatic de-tection of forum claims, as presented in a compan-ion paper (Marin et al, 2011).
That work focusedon using lexical features, filtered through word listsobtained from domain experts and through data-driven methods, and extended with parse tree infor-mation.
Automatic detection of other types of au-thority claims and of alignment moves is left for fu-ture research.We believe that, as social acts, authority claimsand alignment moves are broadly recognized com-munication behaviors that play an important role inhuman interaction across a variety of contexts.
How-ever, because Wikipedia discussions are shaped bya set of well-defined, local communication normswhich are closely tied to the task of distributed,collaborative writing, we expect authority claimsand alignment moves will manifest differently inother genres.
Future work could explore the rangeof variation among the linguistic cues associatedwith authority and alignment categories across gen-res, cultures and communication media, as wellas the possible role of additional categories or so-cial acts not discussed here.
We believe that thecommunicative ecology of Wikipedia discussions,combined with the rich metadata of the Wikipediadatabase, presents a highly valuable natural labora-tory in which to explore social scientific analyses ofcommunication behaviors as well as a resource forthe development of NLP systems which can auto-matically identify these social acts, in Wikipedia andbeyond.AcknowledgmentsThis research was funded by the Office of the Di-rector of National Intelligence (ODNI), IntelligenceAdvanced Research Projects Activity (IARPA).
Allstatements of fact, opinion or conclusions containedherein are those of the authors and should not beconstrued as representing the official views or poli-cies of IARPA, the ODNI or the U.S. Government.The original Wikipedia discussion page data forthis study was made available from a researchproject supported by NSF award IIS-0811210.
Wethank Travis Kriplean for his initial assistance withscripts to process this data dump.We also gratefully acknowledge the contributionof the annotators: Wendy Kempsell, Kelley Kilan-ski, Robert Sykes and Lisa Tittle.ReferencesApoorv Agarwal and Owen Rambow.
2010.
Automaticdetection and classification of social events.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, pages 1024?1034, Cambridge, MA.
Association for ComputationalLinguistics.Ron Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Philip Ball.
2005.
Index aims for fair ranking of scien-tists.
Nature, 436:900?900.Nancy Baym.
1996.
Agreements and disagreements ina computer-mediated discussion.
Research on Lan-guage and Social Interaction, 29:315?345.Mary Bucholtz and Kira Hall.
2010.
Locating identityin language.
In C. Llamas and D. Watt, editors, Lan-56guage and Identities.
Edinburgh University Press, Ed-inburgh.Moira Burke and Robert Kraut.
2008.
Mopping up:Modeling wikipedia promotion decisions.
In Proceed-ings of the 2008 ACM Conference on Computer Sup-ported Cooperative Work, pages 27?36.
Association ofComputing Machinery.Jolene Galegher, Lee Sproull, and Sara Kiesler.
1998.Legitimacy, authority, and community in electronicsupport groups.
Written Communication, 15(4):493?530.Michel Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agree-ment and disagreement in conversational speech: Useof Bayesian networks to model pragmatic dependen-cies.
In Proceedings of the 42nd Meeting of the Asso-ciation for Computational Linguistics (ACL?04), MainVolume, pages 669?676, Barcelona, Spain.Meghan Lammie Glenn, Stephanie M. Strassel, and Hae-joong Lee.
2009.
XTrans: A speech annotation andtranscription tool.
In INTERSPEECH-2009, pages2855?2858.Erving Goffman.
1981.
Forms of Talk.
University ofPennsylvania Press, Philadelphia.Dustin Hillard, Mari Ostendorf, and Elizabeth Shriberg.2003.
Detection of agreement vs. disagreement inmeetings: training with unlabeled data.
In Proceed-ings of the 2003 Human Language Technology Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics (HLT/NAACL),pages 34?36.Jakob L. Jensen.
2003.
Public spheres on the inter-net: Anarchic or government sponsored; a compari-son.
Scandinavian Political Studies, 26:349?374.J.
Richard Landis and Gary G. Koch.
1977.
Measure-ment of observer agreement for categorical data.
Bio-metrics, 33:159?174.Yameng Liu.
1997.
Authority, presumption and inven-tion.
Philosophy and Rhetoric, 30(4):413?427.John Locke.
1959 [1690].
An Essay Concerning HumanUnderstanding.
Dover Publications, New York.Jo Mackiewicz.
2010.
Assertions of expertise in onlineproduct reviews.
Journal of Business and TechnicalCommunication, 24(1):3?28.Alex Marin, Bin Zhang, and Mari Ostendorf.
2011.
De-tecting forum authority claims in online discussions.Proceedings of the Workshop on Language in SocialMedia (LSM 2011).Michael Mulkay.
1985.
Agreement and disagreement inconversations and letters.
Text, 5(3):201?227.Michael Mulkay.
1986.
Conversations and texts.
HumanStudies, 9(2-3):303?321.Meghan Oxley, Jonathan T. Morgan, Mark Zachry, andBrian Hutchinson.
2010.
?What I know is...?
: Estab-lishing credibility on Wikipedia talk pages.
In Pro-ceedings of the 6th International Symposium on Wikisand Open Collaboration, Gdansk, Poland.
Associationfor Computing Machinery.Janie Rees-Miller.
2000.
Power, severity, and context indisagreeement.
Journal of Pragmatics, 32(8):1087?1111.Kay Richardson.
2003.
Health risks on the internet: Es-tablishing credibility on line.
Health, Risk and Society,5(2):171?184.Marie-Claire Shanahan.
2010.
Changing the meaningof peer-to-peer?
Exploring online comment spaces assites of negotiated expertise.
Journal of Science Com-munication, 9(1):1?13.Jan Svennevig.
1999.
Getting Acquainted in Conversa-tion: A Study of Initial Interactions.
John BenjaminsPublishing Company, Amsterdam.Dorothea K. Thompson.
1993.
Arguing for experimental?facts?
in science.
Written Communication, 10:106.Joanna Thornborrow.
2001.
Authenticating talk: Build-ing public identities in audience participation broad-casting.
Discourse Studies, 3(4):459?479.Linda Wine.
2008.
Towards a deeper understanding offraming, footing, and alignment.
Teachers College,Columbia University Working Papers in TESOL andApplied Linguistics, 8(3):1?3.57
