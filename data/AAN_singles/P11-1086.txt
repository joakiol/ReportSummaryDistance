Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 856?864,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsRule Markov Models for Fast Tree-to-String TranslationAshish VaswaniInformation Sciences InstituteUniversity of Southern Californiaavaswani@isi.eduHaitao MiInstitute of Computing TechnologyChinese Academy of Scienceshtmi@ict.ac.cnLiang Huang and David ChiangInformation Sciences InstituteUniversity of Southern California{lhuang,chiang}@isi.eduAbstractMost statistical machine translation systemsrely on composed rules (rules that can beformed out of smaller rules in the grammar).Though this practice improves translation byweakening independence assumptions in thetranslation model, it nevertheless results inhuge, redundant grammars, making both train-ing and decoding inefficient.
Here, we take theopposite approach, where we only use min-imal rules (those that cannot be formed outof other rules), and instead rely on a ruleMarkov model of the derivation history tocapture dependencies between minimal rules.Large-scale experiments on a state-of-the-arttree-to-string translation system show that ourapproach leads to a slimmer model, a fasterdecoder, yet the same translation quality (mea-sured using Bleu) as composed rules.1 IntroductionStatistical machine translation systems typicallymodel the translation process as a sequence of trans-lation steps, each of which uses a translation rule,for example, a phrase pair in phrase-based transla-tion or a tree-to-string rule in tree-to-string transla-tion.
These rules are usually applied independentlyof each other, which violates the conventional wis-dom that translation should be done in context.To alleviate this problem, most state-of-the-art sys-tems rely on composed rules, which are larger rulesthat can be formed out of smaller rules (includ-ing larger phrase pairs that can be formerd out ofsmaller phrase pairs), as opposed to minimal rules,which are rules that cannot be formed out of otherrules.
Although this approach does improve trans-lation quality dramatically by weakening the inde-pendence assumptions in the translation model, theysuffer from two main problems.
First, compositioncan cause a combinatorial explosion in the numberof rules.
To avoid this, ad-hoc limits are placed dur-ing composition, like upper bounds on the numberof nodes in the composed rule, or the height of therule.
Under such limits, the grammar size is man-ageable, but still much larger than the minimal-rulegrammar.
Second, due to large grammars, the de-coder has to consider many more hypothesis transla-tions, which slows it down.
Nevertheless, the advan-tages outweigh the disadvantages, and to our knowl-edge, all top-performing systems, both phrase-basedand syntax-based, use composed rules.
For exam-ple, Galley et al (2004) initially built a syntax-basedsystem using only minimal rules, and subsequentlyreported (Galley et al, 2006) that composing rulesimproves Bleu by 3.6 points, while increasing gram-mar size 60-fold and decoding time 15-fold.The alternative we propose is to replace composedrules with a rule Markov model that generates rulesconditioned on their context.
In this work, we re-strict a rule?s context to the vertical chain of ances-tors of the rule.
This ancestral context would playthe same role as the context formerly provided byrule composition.
The dependency treelet model de-veloped by Quirk and Menezes (2006) takes suchan approach within the framework of dependencytranslation.
However, their study leaves unansweredwhether a rule Markov model can take the placeof composed rules.
In this work, we investigate theuse of rule Markov models in the context of tree-856to-string translation (Liu et al, 2006; Huang et al,2006).
We make three new contributions.First, we carry out a detailed comparison of ruleMarkov models with composed rules.
Our experi-ments show that, using trigram rule Markov mod-els, we achieve an improvement of 2.2 Bleu overa baseline of minimal rules.
When we compareagainst vertically composed rules, we find that ourrule Markov model has the same accuracy, but ourmodel is much smaller and decoding with our modelis 30% faster.
When we compare against full com-posed rules, we find that our rule Markov model stilloften reaches the same level of accuracy, again withsavings in space and time.Second, we investigate methods for pruning ruleMarkov models, finding that even very simple prun-ing criteria actually improve the accuracy of themodel, while of course decreasing its size.Third, we present a very fast decoder for tree-to-string grammars with rule Markov models.
Huangand Mi (2010) have recently introduced an efficientincremental decoding algorithm for tree-to-stringtranslation, which operates top-down and maintainsa derivation history of translation rules encountered.This history is exactly the vertical chain of ancestorscorresponding to the contexts in our rule Markovmodel, which makes it an ideal decoder for ourmodel.We start by describing our rule Markov model(Section 2) and then how to decode using the ruleMarkov model (Section 3).2 Rule Markov modelsOur model which conditions the generation of a ruleon the vertical chain of its ancestors, which allows itto capture interactions between rules.Consider the example Chinese-English tree-to-string grammar in Figure 1 and the example deriva-tion in Figure 2.
Each row is a derivation step; thetree on the left is the derivation tree (in which eachnode is a rule and its children are the rules that sub-stitute into it) and the tree pair on the right is thesource and target derived tree.
For any derivationnode r, let anc1(r) be the parent of r (or  if it has noparent), anc2(r) be the grandparent of node r (or  ifit has no grandparent), and so on.
Let ancn1(r) be thechain of ancestors anc1(r) ?
?
?
ancn(r).The derivation tree is generated as follows.
Withprobability P(r1 | ), we generate the rule at the rootnode, r1.
We then generate rule r2 with probabilityP(r2 | r1), and so on, always taking the leftmost opensubstitution site on the English derived tree, and gen-erating a rule ri conditioned on its chain of ancestorswith probability P(ri | ancn1(ri)).
We carry on untilno more children can be generated.
Thus the proba-bility of a derivation tree T isP(T ) =?r?TP(r | ancn1(r)) (1)For the minimal rule derivation tree in Figure 2, theprobability is:P(T ) = P(r1 | ) ?
P(r2 | r1) ?
P(r3 | r1)?
P(r4 | r1, r3) ?
P(r6 | r1, r3, r4)?
P(r7 | r1, r3, r4) ?
P(r5 | r1, r3) (2)Training We run the algorithm of Galley et al(2004) on word-aligned parallel text to obtain a sin-gle derivation of minimal rules for each sentencepair.
(Unaligned words are handled by attachingthem to the highest node possible in the parse tree.
)The rule Markov modelcan then be trained on the path set of these deriva-tion trees.Smoothing We use interpolation with absolutediscounting (Ney et al, 1994):Pabs(r | ancn1(r)) =max{c(r | ancn1(r)) ?
Dn, 0}?r?
c(r?
| ancn1(r?
))+ (1 ?
?n)Pabs(r | ancn?11 (r)), (3)where c(r | ancn1(r)) is the number of times we haveseen rule r after the vertical context ancn1(r), Dn isthe discount for a context of length n, and (1 ?
?n) isset to the value that makes the smoothed probabilitydistribution sum to one.We experiment with bigram and trigram ruleMarkov models.
For each, we try different values ofD1 and D2, the discount for bigrams and trigrams,respectively.
Ney et al (1994) suggest using the fol-lowing value for the discount Dn:Dn =n1n1 + n2(4)857rule id translation ruler1 IP(x1:NP x2:VP) ?
x1 x2r2 NP(Bu`sh??)
?
Bushr3 VP(x1:PP x2:VP) ?
x2 x1r4 PP(x1:P x2:NP) ?
x1 x2r5 VP(VV(ju?x?
?ng) AS(le) NPB(hu?`ta?n)) ?
held talksr6 P(yu?)
?
withr?6 P(yu?)
?
andr7 NP(Sha?lo?ng) ?
SharonFigure 1: Example tree-to-string grammar.derivation tree derived tree pair IP@ : IP@r1IP@NP@1 VP@2 : NP@1 VP@2r1r2 r3IP@NP@1Bu`sh?
?VP@2PP@2.1 VP@2.2: Bush VP@2.2 PP@2.1r1r2 r3r4 r5IP@NP@1Bu`sh?
?VP@2PP@2.1P@2.1.1 NP@2.1.2VP@2.2VVju?x?
?ngASleNPhu?`ta?n: Bush held talks P@2.1.1 NP@2.1.2r1r2 r3r4r6 r7r5IP@NP@1Bu`sh??VP@2PP@2.1P@2.1.1yu?NP@2.1.2Sha?lo?ngVP@2.2VVju?x?
?ngASleNPhu?`ta?n: Bush held talks with SharonFigure 2: Example tree-to-string derivation.
Each row shows a rewriting step; at each step, the leftmost nonterminalsymbol is rewritten using one of the rules in Figure 1.858Here, n1 and n2 are the total number of n-grams withexactly one and two counts, respectively.
For ourcorpus, D1 = 0.871 and D2 = 0.902.
Additionally,we experiment with 0.4 and 0.5 for Dn.Pruning In addition to full n-gram Markov mod-els, we experiment with three approaches to buildsmaller models to investigate if pruning helps.
Ourresults will show that smaller models indeed give ahigher Bleu score than the full bigram and trigrammodels.
The approaches we use are:?
RM-A: We keep only those contexts in whichmore than P unique rules were observed.
Byoptimizing on the development set, we set P =12.?
RM-B: We keep only those contexts that wereobserved more than P times.
Note that this is asuperset of RM-A.
Again, by optimizing on thedevelopment set, we set P = 12.?
RM-C: We try a more principled approachfor learning variable-length Markov models in-spired by that of Bejerano and Yona (1999),who learn a Prediction Suffix Tree (PST).
Theygrow the PST in an iterative manner by start-ing from the root node (no context), and thenadd contexts to the tree.
A context is added ifthe KL divergence between its predictive distri-bution and that of its parent is above a certainthreshold and the probability of observing thecontext is above another threshold.3 Tree-to-string decoding with ruleMarkov modelsIn this paper, we use our rule Markov model frame-work in the context of tree-to-string translation.Tree-to-string translation systems (Liu et al, 2006;Huang et al, 2006) have gained popularity in recentyears due to their speed and simplicity.
The input tothe translation system is a source parse tree and theoutput is the target string.
Huang and Mi (2010) haverecently introduced an efficient incremental decod-ing algorithm for tree-to-string translation.
The de-coder operates top-down and maintains a derivationhistory of translation rules encountered.
The historyis exactly the vertical chain of ancestors correspond-ing to the contexts in our rule Markov model.
ThisIP@NP@1Bu`sh??VP@2PP@2.1P@2.1.1yu?NP@2.1.2Sha?lo?ngVP@2.2VV@2.2.1ju?x?
?ngAS@2.2.2leNP@2.2.3hu?`ta?nFigure 3: Example input parse tree with tree addresses.makes incremental decoding a natural fit with ourgenerative story.
In this section, we describe howto integrate our rule Markov model into this in-cremental decoding algorithm.
Note that it is alsopossible to integrate our rule Markov model withother decoding algorithms, for example, the morecommon non-incremental top-down/bottom-up ap-proach (Huang et al, 2006), but it would involvea non-trivial change to the decoding algorithms tokeep track of the vertical derivation history, whichwould result in significant overhead.Algorithm Given the input parse tree in Figure 3,Figure 4 illustrates the search process of the incre-mental decoder with the grammar of Figure 1.
Wewrite X@?
for a tree node with label X at tree address?
(Shieber et al, 1995).
The root node has address ,and the ith child of node ?
has address ?.i.
At eachstep, the decoder maintains a stack of active rules,which are rules that have not been completed yet,and the rightmost (n ?
1) English words translatedthus far (the hypothesis), where n is the order of theword language model (in Figure 4, n = 2).
The stacktogether with the translated English words comprisea state of the decoder.
The last column in the fig-ure shows the rule Markov model probabilities withthe conditioning context.
In this example, we use atrigram rule Markov model.After initialization, the process starts at step 1,where we predict rule r1 (the shaded rule) with prob-ability P(r1 | ) and push its English side onto thestack, with variables replaced by the correspond-ing tree nodes: x1 becomes NP@1 and x2 becomesVP@2.
This gives us the following stack:s = [ NP@1 VP@2]The dot () indicates the next symbol to process in859stack hyp.
MR prob.0 [<s>  IP@ </s>] <s>1 [<s>  IP@ </s>] [ NP@1 VP@2] <s> P(r1 | )2 [<s>  IP@ </s>] [ NP@1 VP@2] [ Bush] <s> P(r2 | r1)3 [<s>  IP@ </s>] [ NP@1 VP@2] [Bush  ] .
.
.
Bush4 [<s>  IP@ </s>] [NP@1  VP@2] .
.
.
Bush5 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] .
.
.
Bush P(r3 | r1)6 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held talks] .
.
.
Bush P(r5 | r1, r3)7 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held  talks] .
.
.
held8 [<s>  IP@ </s>] [NP@1  VP@2] [ VP@2.2 PP@2.1] [ held talks  ] .
.
.
talks9 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] .
.
.
talks10 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] .
.
.
talks P(r4 | r1, r3)11 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [ with] .
.
.
with P(r6 | r3, r4)12 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [with  ] .
.
.
with13 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] .
.
.
with14 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [ Sharon] .
.
.
with P(r7 | r3, r4)11?
[<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [ and] .
.
.
and P(r?6 | r3, r4)12?
[<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [ P@2.1.1 NP@2.1.2] [and  ] .
.
.
and13?
[<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] .
.
.
and14?
[<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [ Sharon] .
.
.
and P(r7 | r3, r4)15 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1  NP@2.1.2] [Sharon  ] .
.
.
Sharon16 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2  PP@2.1] [P@2.1.1 NP@2.1.2  ] .
.
.
Sharon17 [<s>  IP@ </s>] [NP@1  VP@2] [VP@2.2 PP@2.1  ] .
.
.
Sharon18 [<s>  IP@ </s>] [NP@1 VP@2  ] .
.
.
Sharon19 [<s> IP@  </s>] .
.
.
Sharon20 [<s> IP@ </s>  ] .
.
.
</s>Figure 4: Simulation of incremental decoding with rule Markov model.
The solid arrows indicate one path and thedashed arrows indicate an alternate path.860VP@2VP@2.2 PP@2.1P@2.1.1yu?NP@2.1.2Figure 5: Vertical context r3 r4 which allows the modelto correctly translate yu?
as with.the English word order.
We expand node NP@1 firstwith English word order.
We then predict lexical ruler2 with probability P(r2 | r1) and push rule r2 ontothe stack:[ NP@1 VP@2 ] [ Bush]In step 3, we perform a scan operation, in whichwe append the English word just after the dot to thecurrent hypothesis and move the dot after the word.Since the dot is at the end of the top rule in the stack,we perform a complete operation in step 4 where wepop the finished rule at the top of the stack.
In thescan and complete steps, we don?t need to computerule probabilities.An interesting branch occurs after step 10 withtwo competing lexical rules, r6 and r?6.
The Chineseword yu?
can be translated as either a preposition with(leading to step 11) or a conjunction and (leadingto step 11?).
The word n-gram model does not haveenough information to make the correct choice, with.As a result, good translations might be pruned be-cause of the beam.
However, our rule Markov modelhas the correct preference because of the condition-ing ancestral sequence (r3, r4), shown in Figure 5.Since VP@2.2 has a preference for yu?
translating towith, our corpus statistics will give a higher proba-bility to P(r6 | r3, r4) than P(r?6 | r3, r4).
This helpsthe decoder to score the correct translation higher.Complexity analysis With the incremental decod-ing algorithm, adding rule Markov models does notchange the time complexity, which is O(nc|V |g?1),where n is the sentence length, c is the maximumnumber of incoming hyperedges for each node in thetranslation forest, V is the target-language vocabu-lary, and g is the order of the n-gram language model(Huang and Mi, 2010).
However, if one were to userule Markov models with a conventional CKY-stylebottom-up decoder (Liu et al, 2006), the complexitywould increase to O(nCm?1|V |4(g?1)), where C is themaximum number of outgoing hyperedges for eachnode in the translation forest, and m is the order ofthe rule Markov model.4 Experiments and results4.1 SetupThe training corpus consists of 1.5M sentence pairswith 38M/32M words of Chinese/English, respec-tively.
Our development set is the newswire portionof the 2006 NIST MT Evaluation test set (616 sen-tences), and our test set is the newswire portion ofthe 2008 NIST MT Evaluation test set (691 sen-tences).We word-aligned the training data using GIZA++followed by link deletion (Fossum et al, 2008),and then parsed the Chinese sentences using theBerkeley parser (Petrov and Klein, 2007).
To extracttree-to-string translation rules, we applied the algo-rithm of Galley et al (2004).
We trained our ruleMarkov model on derivations of minimal rules asdescribed above.
Our trigram word language modelwas trained on the target side of the training cor-pus using the SRILM toolkit (Stolcke, 2002) withmodified Kneser-Ney smoothing.
The base featureset for all systems is similar to the set used in Mi etal.
(2008).
The features are combined into a standardlog-linear model, which we trained using minimumerror-rate training (Och, 2003) to maximize the Bleuscore on the development set.At decoding time, we again parse the inputsentences using the Berkeley parser, and convertthem into translation forests using rule pattern-matching (Mi et al, 2008).
We evaluate translationquality using case-insensitive IBM Bleu-4, calcu-lated by the script mteval-v13a.pl.4.2 ResultsTable 1 presents the main results of our paper.
Weused grammars of minimal rules and composed rulesof maximum height 3 as our baselines.
For decod-ing, we used a beam size of 50.
Using the bestbigram rule Markov models and the minimal rulegrammar gives us an improvement of 1.5 Bleu overthe minimal rule baseline.
Using the best trigramrule Markov model brings our gain up to 2.3 Bleu.861grammar rule Markov max parameters (?106) Bleu timemodel rule height full dev+test test (sec/sent)minimal None 3 4.9 0.3 24.2 1.2RM-B bigram 3 4.9+4.7 0.3+0.5 25.7 1.8RM-A trigram 3 4.9+7.6 0.3+0.6 26.5 2.0vertical composed None 7 176.8 1.3 26.5 2.9composed None 3 17.5 1.6 26.4 2.2None 7 448.7 3.3 27.5 6.8RM-A trigram 7 448.7+7.6 3.3+1.0 28.0 9.2Table 1: Main results.
Our trigram rule Markov model strongly outperforms minimal rules, and performs at the samelevel as composed and vertically composed rules, but is smaller and faster.
The number of parameters is shown forboth the full model and the model filtered for the concatenation of the development and test sets (dev+test).These gains are statistically significant with p <0.01, using bootstrap resampling with 1000 samples(Koehn, 2004).
We find that by just using bigramcontext, we are able to get at least 1 Bleu pointhigher than the minimal rule grammar.
It is interest-ing to see that using just bigram rule interactions cangive us a reasonable boost.
We get our highest gainsfrom using trigram context where our best perform-ing rule Markov model gives us 2.3 Bleu points overminimal rules.
This suggests that using longer con-texts helps the decoder to find better translations.We also compared rule Markov models againstcomposed rules.
Since our models are currently lim-ited to conditioning on vertical context, the closestcomparison is against vertically composed rules.
Wefind that our approach performs equally well usingmuch less time and space.Comparing against full composed rules, we findthat our system matches the score of the base-line composed rule grammar of maximum height 3,while using many fewer parameters.
(It should benoted that a parameter in the rule Markov model isjust a floating-point number, whereas a parameter inthe composed-rule system is an entire rule; there-fore the difference in memory usage would be evengreater.)
Decoding with our model is 0.2 secondsfaster per sentence than with composed rules.These experiments clearly show that rule Markovmodels with minimal rules increase translation qual-ity significantly and with lower memory require-ments than composed rules.
One might wonder ifthe best performance can be obtained by combin-ing composed rules with a rule Markov model.
Thisrule Markov D1Bleu timemodel dev (sec/sent)RM-A 0.871 29.2 1.8RM-B 0.4 29.9 1.8RM-C 0.871 29.8 1.8RM-Full 0.4 29.7 1.9Table 2: For rule bigrams, RM-B with D1 = 0.4 gives thebest results on the development set.rule Markov D1 D2Bleu timemodel dev (sec/sent)RM-A 0.5 0.5 30.3 2.0RM-B 0.5 0.5 29.9 2.0RM-C 0.5 0.5 30.1 2.0RM-Full 0.4 0.5 30.1 2.2Table 3: For rule bigrams, RM-A with D1, D2 = 0.5 givesthe best results on the development set.is straightforward to implement: the rule Markovmodel is still defined over derivations of minimalrules, but in the decoder?s prediction step, the ruleMarkov model?s value on a composed rule is cal-culated by decomposing it into minimal rules andcomputing the product of their probabilities.
We findthat using our best trigram rule Markov model withcomposed rules gives us a 0.5 Bleu gain on top ofthe composed rule grammar, statistically significantwith p < 0.05, achieving our highest score of 28.0.14.3 AnalysisTables 2 and 3 show how the various types of ruleMarkov models compare, for bigrams and trigrams,1For this experiment, a beam size of 100 was used.862parameters (?106) Bleu dev/test time (sec/sent)dev/test without RMM with RMM without/with RMM2.6 31.0/27.0 31.1/27.4 4.5/7.02.9 31.5/27.7 31.4/27.3 5.6/8.13.3 31.4/27.5 31.4/28.0 6.8/9.2Table 6: Adding rule Markov models to composed-rule grammars improves their translation performance.D2D10.4 0.5 0.8710.4 30.0 30.00.5 29.3 30.30.902 30.0Table 4: RM-A is robust to different settings of Dn on thedevelopment set.parameters (?106) Bleu timedev+test dev test (sec/sent)1.2 30.2 26.1 2.81.3 30.1 26.5 2.91.3 30.1 26.2 3.2Table 5: Comparison of vertically composed rules usingvarious settings (maximum rule height 7).respectively.
It is interesting that the full bigram andtrigram rule Markov models do not give our high-est Bleu scores; pruning the models not only savesspace but improves their performance.
We think thatthis is probably due to overfitting.Table 4 shows that the RM-A trigram model doesfairly well under all the settings of Dn we tried.
Ta-ble 5 shows the performance of vertically composedrules at various settings.
Here we have chosen thesetting that gives the best performance on the testset for inclusion in Table 1.Table 6 shows the performance of fully composedrules and fully composed rules with a rule MarkovModel at various settings.2 In the second line (2.9million rules), the drop in Bleu score resulting fromadding the rule Markov model is not statistically sig-nificant.5 Related WorkBesides the Quirk and Menezes (2006) work dis-cussed in Section 1, there are two other previous2For these experiments, a beam size of 100 was used.efforts both using a rule bigram model in machinetranslation, that is, the probability of the current ruleonly depends on the immediate previous rule in thevertical context, whereas our rule Markov modelcan condition on longer and sparser derivation his-tories.
Among them, Ding and Palmer (2005) alsouse a dependency treelet model similar to Quirk andMenezes (2006), and Liu and Gildea (2008) use atree-to-string model more like ours.
Neither com-pared to the scenario with composed rules.Outside of machine translation, the idea of weak-ening independence assumptions by modeling thederivation history is also found in parsing (Johnson,1998), where rule probabilities are conditioned onparent and grand-parent nonterminals.
However, be-sides the difference between parsing and translation,there are still two major differences.
First, our workconditions rule probabilities on parent and grandpar-ent rules, not just nonterminals.
Second, we com-pare against a composed-rule system, which is anal-ogous to the Data Oriented Parsing (DOP) approachin parsing (Bod, 2003).
To our knowledge, there hasbeen no direct comparison between a history-basedPCFG approach and DOP approach in the parsingliterature.6 ConclusionIn this paper, we have investigated whether we caneliminate composed rules without any loss in trans-lation quality.
We have developed a rule Markovmodel that captures vertical bigrams and trigrams ofminimal rules, and tested it in the framework of tree-to-string translation.
We draw three main conclu-sions from our experiments.
First, our rule Markovmodels dramatically improve a grammar of minimalrules, giving an improvement of 2.3 Bleu.
Second,when we compare against vertically composed ruleswe are able to get about the same Bleu score, butour model is much smaller and decoding with our863model is faster.
Finally, when we compare againstfull composed rules, we find that we can reach thesame level of performance under some conditions,but in order to do so consistently, we believe weneed to extend our model to condition on horizon-tal context in addition to vertical context.
We hopethat by modeling context in both axes, we will beable to completely replace composed-rule grammarswith smaller minimal-rule grammars.AcknowledgmentsWe would like to thank Fernando Pereira, YoavGoldberg, Michael Pust, Steve DeNeefe, DanielMarcu and Kevin Knight for their comments.Mi?s contribution was made while he was vis-iting USC/ISI.
This work was supported in partby DARPA under contracts HR0011-06-C-0022(subcontract to BBN Technologies), HR0011-09-1-0028, and DOI-NBC N10AP20031, by a GoogleFaculty Research Award to Huang, and by the Na-tional Natural Science Foundation of China undercontracts 60736014 and 90920004.ReferencesGill Bejerano and Golan Yona.
1999.
Modeling pro-tein families using probabilistic suffix trees.
In Proc.RECOMB, pages 15?24.
ACM Press.Rens Bod.
2003.
An efficient implementation of a newDOP model.
In Proceedings of EACL, pages 19?26.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probablisitic synchronous dependency in-sertion grammars.
In Proceedings of ACL, pages 541?548.Victoria Fossum, Kevin Knight, and Steve Abney.
2008.Using syntax to improve word alignment precision forsyntax-based machine translation.
In Proceedings ofthe Workshop on Statistical Machine Translation.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of HLT-NAACL, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of COLING-ACL, pages 961?968.Liang Huang and Haitao Mi.
2010.
Efficient incrementaldecoding for tree-to-string translation.
In Proceedingsof EMNLP, pages 273?283.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA, pages66?73.Mark Johnson.
1998.
PCFG models of linguistic treerepresentations.
Computational Linguistics, 24:613?632.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP, pages 388?395.Ding Liu and Daniel Gildea.
2008.
Improved tree-to-string transducer for machine translation.
In Proceed-ings of the Workshop on Statistical Machine Transla-tion, pages 62?69.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of COLING-ACL, pages 609?616.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL: HLT, pages192?199.H.
Ney, U. Essen, and R. Kneser.
1994.
On structur-ing probabilistic dependencies in stochastic languagemodelling.
Computer Speech and Language, 8:1?38.Franz Joseph Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL,pages 160?167.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HLT-NAACL, pages 404?411.Chris Quirk and Arul Menezes.
2006.
Do we needphrases?
Challenging the conventional wisdom in sta-tistical machine translation.
In Proceedings of NAACLHLT, pages 9?16.Stuart Shieber, Yves Schabes, and Fernando Pereira.1995.
Principles and implementation of deductiveparsing.
Journal of Logic Programming, 24:3?36.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP, vol-ume 30, pages 901?904.864
