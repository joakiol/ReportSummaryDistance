Terminological variants for document selection andquestion/answer matchingOlivier Ferret Brigitte Grau Martine Hurault-PlantetGabriel Illouz Christian JacqueminLIMSI-CNRSBat.508 Universit?
ParisXI91403 Orsay, France{ferret, grau, mhp, gabrieli, jacquemin}@limsi.frAbstractAnswering precise questions requiresapplying Natural Language techniquesin order to locate the answers insideretrieved documents.
The QALCsystem, presented in this paper,participated to the Question Answeringtrack of the TREC8 and TREC9evaluations.
QALC exploits an analysisof documents based on the search formulti-word terms and their variations.These indexes are used to select aminimal number of documents to beprocessed and to give indices whencomparing question and sentencerepresentations.
This comparison alsotakes advantage of a question analysismodule and recognition of numeric andnamed entities in the documents.1 IntroductionThe Question Answering (QA) track at TREC8and TREC9 is due to the recent need for moresophisticated paradigms in InformationRetrieval (IR).
Question answering generallyrefers to encyclopedic or factual questions thatrequire concise answers.
But current IRtechniques do not yet enable a system to giveprecise answers to precise questions.
Questionanswering is thus an area of IR that calls forNatural Language Processing (NLP) techniquesthat can provide rich linguistic features asoutput.
Such NLP modules should be deeplyintegrated in search and matching componentsso that answer selection can be performed onsuch linguistic features and take advantage ofthem.
In addition, IR and NLP techniques haveto collaborate in the resulting system in order tocope with large-scale and broad coverage textdatabases while deriving benefit from addedknowledge.We developed a system for questionanswering, QALC, evaluated in the frameworkof the QA tracks at TREC8 and TREC9.
TheQALC system comprises NLP modules formulti-word term and named entity extractionwith a specific concern for term conflationthrough variant recognition.
Since named entityrecognition has already been describedextensively in other publications (Baluja 1999),we present the contribution of terminologicalvariants to adding knowledge to our system.The two main activities involvingterminology in NLP are term acquisition andterm recognition.
Basically, terms can be viewedas a particular type of lexical data.
Termvariation may involve structural, morphological,and semantic transformations of single or multi-words terms (Fabre and Jacquemin, 2000).In this paper, we describe how QALC useshigh level indexes, made of terms and variants,to select among documents the most relevantones with regard to a question, and then tomatch candidate answers with this question.
Inthe selection process, the documents firstretrieved by a search engine, are thenpostfiltered and ranked through a weightingscheme based on high level indexes, in order toretain the top ranked ones.
Similarly, all systemsthat participated in TREC9 have a search enginecomponent that firstly selects a subset of theprovided database of about one milliondocuments.
Since a search engine produces aranked list of relevant documents, systems thenhave to define the highest number of documentsto retain.
Indeed, having too many documentsleads to a question processing time that is toolong, but conversely, having too few documentsreduces the possibility of obtaining the correctanswer.
For reducing the amount of text toprocess, one approach consists of keeping one ormore relevant text paragraphs from eachdocument retrieved.
Kwok et al(2000), forinstance use an IR engine that retrieves the top300 sub-documents of about 300-550 words and,on the other hand, the FALCON system(Harabagiu et al 2000) performs a paragraphretrieval stage after the application of a booleanretrieval engine.
These systems work on thewhole database and apply a bag-of-wordstechnique to select passages whereas QALC firstretains a large subset of documents, amongwhich it then selects relevant documents byapplying richer criteria based on the use of thelinguistic structures of the words.QALC indexes, used for document selection,are made of single and multi-word termsretrieved by a 2-step procedure: (1)?automaticterm extraction from questions through part-of-speech tagging and pattern matching and(2)?automatic document indexing through termrecognition and variant conflation.
As a result,linguistic variation is explicitly addressedthrough the exploitation of word paradigms,contrarily to other approaches like the one takenin COPSY (Schwarz 1988) where anapproximate matching technique between thequery and the documents implicitly takes it intoaccount.
Finally, terms acquired at step?
(1) andindexes from step?
(2) are also used by thematching procedure between a question and therelevant document sentences.In the next section, we describe thearchitecture of the QALC system.
Then, wepresent the question processing for termextraction.
We continue with the description ofFASTR, a transformational shallow parser thatrecognizes and marks the extracted terms as wellas their linguistic variants within the documents.The two following sections present the modulesof the QALC system where terms and variantsare used, namely the document selection andquestion/answer matching modules.
Finally, wepresent the results obtained by the QALCsystem as well as an evaluation of thecontribution of this NLP technique to the QAtask through the use of the reference collectionsfor the QA track.
In conclusion, suggestions formore ambitious, but still realistic, developmentsusing NLP are outlined.2 System OverviewNatural Language Processing components in theQALC system (see Figure 1) enrich the selecteddocuments with terminological indexes in orderto go beyond reasoning about single words.
Richlinguistic features are also used to deduce what aquestion is about.Tagged Questions:Named entity tagsVocabulary &frequenciesNamed entityrecognitionCandidatetermsRetrieveddocumentsTagged sentences: named entitytags and term indexationOrdered sequences of 250 and50 charactersQuestion analysis Search engineQuestionsSubset of ranked documentsCorpusRe-indexing and selection ofdocuments (FASTR)Question/Sentence pairingFigure 1.
The QALC systemThe analysis of a question relies on a shallowparser which spots discriminating patterns andassigns categories to the question.
Thecategories correspond to the types of entities thatare likely to constitute the answer to thequestion.In order to select the best documents fromthe results given by the search engine and tolocate the answers inside them, we work withterms and their variants, i.e.
morphologic,syntactic and semantic equivalent expressions.A term extractor has been developed, based onsyntactic patterns which describe complexnominal phrases and their subparts.
These termsare used by FASTR (Jacquemin 1999), ashallow transformational natural languageanalyzer that recognizes their occurrences andtheir variants.
Each occurrence or variantconstitutes an index that is subsequently used inthe processes of document ranking andquestion/document matching.Documents are ordered according to a weightcomputed thanks to the number and the qualityof the terms and variants they contain.
Forexample, original terms with proper names areconsidered more reliable than semantic variants.An analysis of the weight graph enables thesystem to select a relevant subpart of thedocuments, whose size varies along thequestions.
This selection takes all its importancewhen applying the last processes which consistof recognizing named-entities and analyzingeach sentence to decide whether it is a possibleanswer or not.
As such processes are timeconsuming we attempt to limit their applicationto a minimal number of documents.Named entities are recognized in thedocuments and used to measure the similaritybetween the document sentences and a question.Named entities receive one of the followingtypes: person, organization, location (city orplace), number (a time expression or a numberexpression).
They are defined in a way similar tothe MUC task and recognized through acombination of lexico-syntactic patterns andsignificantly large lexical data.Finally, the question/answer matchingmodule uses all the data extracted from thequestions and the documents by the precedingmodules.
We developed a similarity measurethat attributes weights to each characteristic, i.e.named entity tags and terms and variants, andmakes a combination of them.
The QALCsystem proposes long and short answers.Concerning the short ones, the system focuseson parts of sentences that contain the expectednamed entity tags, when they are known, or onthe largest subpart without any terms of thequestion.3 Terms and Variants3.1 Term extractionFor automatic acquisition of terms fromquestions, we use a simple technique of filteringthrough patterns of part-of-speech categories.No statistical ranking is possible because of thesmall size of the questions from which terms areextracted.
First, questions are tagged with thehelp of the TreeTagger (Schmid 1999).
Patternsof syntactic categories are then used to extractterms from the tagged questions.
They are veryclose to those described by Justeson andKatz?
(1995), but we do not include post-posedprepositional phrases.
The pattern used forextracting terms is:(((((JJ | NN | NP | VBG)) ?
(JJ | NN | NP | VBG) (NP| NN))) | (VBD) | (NN) | (NP) | (CD))where NN are common nouns, NP proper nouns,JJ adjectives, VBG gerunds, VBD pastparticiples and CD numeral determiners.The longest string is acquired first andsubstrings can only be acquired if they do notbegin at the same word as the superstring.
Forinstance, from the sequence nameNN ofIN theDTUSNP helicopterNN pilotNN shotVBD downRP,the following four terms are acquired: U Shelicopter pilot, helicopter pilot, pilot, andshoot.The mode of acquisition chosen for termsamounts to considering only the substructuresthat correspond to an attachment of modifiers tothe leftmost constituents (the closest one).
Forinstance, the decomposition of US helicopterpilot into helicopter pilot and pilot is equivalentto extracting the subconstituents of the structure[US [helicopter [pilot]]].3.2 Variant recognition through FASTRThe automatic indexing of documents isperformed by FASTR (Jacquemin 1999), atransformational shallow parser for therecognition of term occurrences and variants.Terms are transformed into grammar rules andthe single words building these terms areextracted and linked to their morphological andsemantic families.The morphological family of a single word wis the set M(w) of terms in the CELEX database(CELEX 1998) which have the same rootmorpheme as w. For instance, the morphologicalfamily of the noun maker is made of the nounsmaker, make and remake, and the verbs to makeand to remake.The semantic family of a single word w is theunion S (w ) of the synsets  of WordNet1.6(Fellbaum 1998) to which w belongs.
A synset isa set of words that are synonymous for at leastone of their meanings.
Thus, the semantic familyof a word w is the set of the words w' such thatw' is considered as a synonym of one of themeanings of w. The semantic family of maker,obtained from WordNet1.6, is composed ofthree nouns: maker, manufacturer, shaper andthe semantic family of c a r is car, auto,automobile, machine, motorcar.Variant patterns that rely on morphologicaland semantic families are generated throughmetarules.
They are used to extract terms andvariants from the document sentences in theTREC corpus.
For instance, the followingpattern, named NtoSemArg, extracts theoccurrence making many automobiles as avariant of the term car maker:VM('maker') RP?
PREP?
(ART (NN|NP)?
PREP)?ART?
(JJ?|?NN?|?NP |?VBD?|?VBG)[0-3] NS('car')where RP are particles, PREP prepositions, ARTarticles, and VBD, VBG verbs.
VM('maker') isany verb in the morphological family of thenoun maker and NS('car') is any noun in thesemantic family of car.Relying on the above morphological andsemantic families, auto maker, auto partsmaker , car manufacturer, make autos, andmaking many automobiles are extracted ascorrect variants of the original term car makerthrough the set of metarules used for the QAtrack experiment.
Unfortunately, some incorrectvariants are extracted as well, such as makethose cuts in auto produced by the precedingmetarule.3.3 Document selectionThe output of NLP-based indexing is a list ofterm occurrences composed of a documentidentifier d, a term identifier?a pair t(q,i)composed of a question number q and a uniqueindex i?, a text sequence, and a variationidentifier v (a metarule).
For instance, thefollowing index :LA092690-0038 t(131,1)making many automobiles NtoVSemArgmeans that the occurrence making manyautomobiles from document d=LA092690-0038is obtained as a variant of term i=1 in questionq=131 (car maker) through the variationNtoVSemArg given in Section 3.2.Each document d selected for a question q isassociated with a weight.
The weighting schemerelies on a measure of quality of the differentfamilies of variations described byJacquemin?
(1999): non-variant occurrences areweighted 3.0, morphological and morpho-syntactic variants are weighted 2.0, andsemantic and morpho-syntactico-semanticvariants are weighted 1.0.Since proper names are more reliable indicesthan common names, each term t(q,i) receives aweight P(t(q , i )) between 0 and 1.0corresponding to its proportion of proper names.For instance, President Cleveland's wife isweighted 2/3=0.66.
Since another factor ofreliability is the length of terms, a factor |t(q,i)|in the weighting formula denotes the number ofwords in term t(q,i).
The weight Wq(d) of aquery q  in a document d  is given by thefollowing formula (1).
The products of theweightings of each term extracted by the indexerare summed over the indices I(d) extracted fromdocument d and normalized according to thenumber of terms |T(q)| in query q.W (d)( ) ( ( ( , ))) ( , )( )q( ( , ), ) ( )=?
+ ???
w v P t q i t q iT qt q i v I d1 2(1)Mainly two types of weighting curves areobserved for the retrieved documents: curveswith a plateau and a sharp slope at a giventhreshold (Figure 2.a) and curves with a slightlydecreasing weight (Figure 2.b).The edge of a plateau is detected by examiningsimultaneously the relative decrease of the slopewith respect to the preceding one, and therelative decrease of the value with respect to thepreceding one.
When a threshold is detected, weonly select documents before this threshold,otherwise a fixed cutoff threshold is used.
In ourexperiments, for each query q, the 200 bestranked documents retrieved by the searchengine1 were subsequently processed by the re-indexing module.
Our studies (Ferret et al 2000)show that 200 is a minimum number such asalmost all the relevant documents are kept.When no threshold was detected, we fixed thevalue of the threshold to 100.001010202030304040505060607070808090901001000011223344556678910rank of the documentweightQuestion #87rank of the documentTruncation of the ranked listQuestion #86weight(a)(b)Figure 2.
Two types of weighting curve.Through this method, the cutoff threshold is8 for question #87 (Who followed Willy Brandtas chancellor of the Federal Republic ofGermany?, Figure 2(a))2 and 100 for question#86 (Who won two gold medals in skiing in theOlympic Games in Calgary?, Figure 2(b)).
Asindicated by Figure?
?2(a), there is an importantdifference of weight between documents #8 and#9.
The weight of document #8 is 9.57 while the1 We used in particular Indexal (Loupy et al1998), a searchengine provided by Bertin Technologie.2 Questions come from the TREC8 data.weight of document #9 is 7.29 because the termFederal Republic only exists in document #8.This term has a high weight because it iscomposed of two proper names.4 Question-Answer Matching4.1 Question type categorizationQuestion type categorization is performed inorder to assign features to questions and usethese features for the similarity measurementbetween a question and potential answersentences.
Basically, question categorizationallows the prediction of the kind(s) of answer,called target (for instance, NUMBER).Sentences inside the retrieved documents arelabeled with the same tags as questions.
Duringthe similarity measurement, the more thequestion and a sentence share the same tags, themore they are considered as involved in aquestion-answer relation.
For example:Question:How many people live in the Falklands?
?> target = NUMBERAnswer:F a l k l a n d s  p o p u l a t i o n  o f  <bnumexTYPE=NUMBER> 2,100 <enumex> isconcentrated.We established 17 types of answer.
Somesystems define more categories.
For instancePrager et al (2000) identify about 50 types ofanswer.4.2 Answer SelectionIn the QALC system, we have taken thesentence as a basic unit because it is largeenough to contain the answer to questions aboutsimple facts and to give a context that permitsthe user to judge if the suggested answer isactually correct.
The module associates eachquestion with the Na most similar sentences (Nais equal to 5 for the QA task at TREC).The overall principle of the selection processis the following: each sentence from thedocuments selected for a question is comparedwith this question.
To perform this comparison,sentences and questions are turned into vectorsthat contain three kinds of elements: contentwords, term identifiers and named entity tags.
Aspecific weight (between 0 and 1.0) is associatedwith each of these elements in order to expresstheir relative importance.The content words are the lemmatized formsof mainly adjectives, verbs and nouns such asthey are given by the TreeTagger.
Each contentword in a vector is weighted according to itsdegree of specificity in relation to the corpus inwhich answers are searched through the tf.idfweighting scheme.
For questions, the termidentifiers refer to the terms extracted by theterm extractor described in Section?3.1 andreceive a fixed weight.
In sentence vectors, termidentifiers are associated with the normalizedscore from the ranking module (see Section 3.3).The named entity tags correspond to the possibletypes of answers, provided by the questionanalysis module.
In each sentence these tagsdelimit the named entities that were recognizedby the corresponding module of the QALCsystem and specify their type.
Unlike termidentifiers, named entity tags are given the samefixed weight in both sentence and questionvectors because the matching module uses thetypes of the named entities and not their values.In our experiments, the linguistic features(terms and named entities) are used to favorappropriate sentences when they have notenough content words in common with thequestion or when the question only contains afew content words.
Thus, the weights of termidentifiers or named entity tags are reduced byapplying a coefficient in order to be globallylower than the weights of the content words.Finally, the comparison between a sentencevector Vd and a question vector Vq is achievedby computing the following similarity measure:?
?=j ji idqwqwdVVsim ),( (2)where wqj is the weight of an element in thequestion vector and wdi is the weight of anelement in a sentence vector that is also in thequestion vector.
This measure evaluates theproportion and the importance of the elements inthe question vector that are found in thesentence vector with regards to all the elementsof the question vector.
Moreover, when thesimilarity value is nearly the same for twosentences, we favor the one in which the contentwords of the question are the least scattered.The next part gives an example of thematching operations for the TREC8 questionQ16 What two US biochemists won the NobelPrize in medicine in 1992?
This question isturned into the following vector:two (1.0) US (1.0) biochemist (0.9)nobel (1.0) prize (0,6) medicine (0,5)win (0,3) 1992 (1.0) <PERSON> (0.5)16.01 (0.5) 16.04 (0.5)where <PERSON> is the expected type of theanswer, 16.01 is the identifier of the U Sbiochemist term and 16.04 is the identifier of theNobel Prize term.The same kind of vector is built for thesentence <NUMBER> Two </NUMBER> USbiochemists, <PERSON> Edwin Krebs</PERSON> and <CITY> Edmond </CITY>Fischer, jointly won the <NUMBER> 1992</NUMBER> Nobel Medicine Prize for workthat could advance the search for an anti-cancerdrug, coming from the document FT924-14045that was selected for the question Q163 :two (1.0) US (1.0) biochemist (0.9)nobel (1.0) prize (0,6) medicine (0,5)win (0,3) 1992 (1.0) Edwin (0.0)Krebs (0.0) Edmond (0.0) Fischer (0.0)work (0.0) advance (0.0) search (0.0)anti-cancer (0.0) jointly (0.0) drug (0.0)<PERSON> (0.5) <NUMBER> (0.0) <CITY>(0.0)16.01 (0.5) 16.04 (0.3)where the weight 0.0 is given to the elementsthat are not part of the question vector.
The termUS biochemist is found with no variation andNobel Prize appears as a syntactic variant.Finally, according to (2), the similarity measurebetween theses two vectors is equal to 0.974.5 Results and EvaluationWe sent to TREC9 three runs whose variationsconcern the searched engine used and the lengthof the answer (250 or 50 characters).
Amongthose runs, the best one obtained a score of0.407 with 375 correct answers among 682questions, for answers of 250 characters length.The score computed by NIST is the reciprocalmean of the rank, from 1 to 5, of the correct3 This sentence is taken from the output of the named entityrecognizer.answer.
With this score, the QALC system wasranked 6th among 25 participants at TREC 9QA task.Document selection relies on a quantitativemeasure, i.e.
the document weight, whosecomputation is based on syntactic and semanticindices, i.e.
the terms and the terminologicalvariants.
Those indices allow the system to takeinto account words as well as group of wordsand their internal relations within thedocuments.
Following examples, that we havegot from selected documents for TREC9 QAtask, show what kind of indices are added to thequestion words.For the question 252 When was the first flushtoilet invented?
, one multi-word extracted termis flush toilet.
This term is marked by FASTRwhen recognized in a document, but it is alsomarked when a variant is found, as for instancelow-flush toilet in the following documentsentence where low-flush is recognized asequivalent to flush:Santa Barbara , Calif. , is giving $ 80 toanyone who converts to a low-flush toilet.252.01   flush toilet[JJ][NN]low-flush[flush][JJ] toilet[toilet][NN]1.00In the given examples, after the identificationnumber of the term, appears the reference term,made of the lemmatized form of the words andtheir syntactic category, followed by the variantfound in the sentence, with each word, itslemmatized form and its category, and finally itsweight.In the example above, the term found in thesentence is equivalent to the reference term, andthus its weight is 1.00.The second example shows a semanticvariant.
Salary and average salary are termsextracted from the question 337, What's theaverage salary of a professional baseball player?.
The semantic variant pay, got from WordNet,was recognized in the following sentence?
:Did the NBA union opt for the courtroombecause its members, whose average pay tops$500000 a year, wouldn't stand still for astrike over free agency ?337.01    salary[NN] pay[pay][NN] 0.25337.00    average [JJ]salary[NN]average[average][JJ] pay[pay][NN]0.40In order to evaluate the efficiency of theselection process, we proceeded to severalmeasures.
We apply our system on the materialgiven for the TREC8 evaluation, one time withthe selection process, and another time withoutthis process.
At each time, 200 documents werereturned by the search engine for each of the 200questions.
When selection was applied, at most100 documents were selected and subsequentlyprocessed by the matching module.
Otherwise,the 200 documents were processed.
The systemwas scored by 0.463 in the first case, and by0.452 in the second case.
These results showthat the score increases when processing lessdocuments above all because it is just therelevant documents that are selected.The benefit from performing such a selectionis also illustrated by the results given in Table 1,computed on the TREC9 results.Number of documents selectedby ranking100 <<100Distribution among thequestions342(50%)340(50%)Number of correct answers 175(51%)200(59%)Number of correct answer atrank 188(50%)128(64%)Table 1.
Evaluation of the ranking processWe see that the selection process discards alot of documents for 50% of the questions (340questions are processed from less than 100documents).
The document set retrieved forthose questions had a weighting curve with asharp slope and a plateau as in Figure 2(a).QALC finds more often the correct answer andin a better position for these 340 questions thanfor the 342 remaining ones.
The average numberof documents selected, when there are less than100, is 37.
These results are very interestingwhen applying such time-consuming processesas  named ent i ty  recogni t ion andquestion/sentence matching.
Document selectionwill also enable us to apply later on syntacticand semantic sentence analysis.6 ConclusionThe goal of a question-answering system is tofind an answer to a precise question, with aresponse time short enough to satisfy the user.As the answer is searched within a great amountof documents, it seems relevant to apply mainlynumerical methods because they are fast.
But, aswe said in the introduction, precise answerscannot be obtained without adding NLP tools toIR techniques.
In this paper, we proposed aquestion answering system which usesterminological variants first to reduce thenumber of documents to process whileincreasing the system performance, and then toimprove the matching between a question and itspotential answers.
Furthermore, reducing theamount of text to process will afterwards allowus to apply more complex methods such assemantic analysis.
Indeed, TREC organizersforesee a number of possible improvements forthe future?
: real-time answering, evaluation andjustification of the answer, completeness of theanswer which could result from answersdistributed along multiple documents, andfinally interactive question answering so that theuser could specify her/his intention.
All thoseimprovements require more data sources as wellas advanced reasoning about pragmatic andsemantic knowledge.Thus, the improvements that we now want tobring to our system will essentially pertain to asemantic and pragmatic approach.
For instance,WordNet that we already use to get the semanticvariants of a word, will be exploited to refineour set of question types.
We also plan to use ashallow syntactico-semantic parser in order toconstruct a semantic representation of both thepotential answer and the question.
Thisrepresentation will allow QALC to select theanswer not only from the terms and variants butalso from the syntactic and semantic links thatterms share with each other.ReferencesBaluja, S., Vibhu O. M., Sukthankar, R. 1999Applying machine learning for high performancenamed-entity extraction.
P r o c e e d i n g sPACLING'99 Waterloo, CA.
365-378.CELEX.
1998.http://www.ldc.upenn.edu/readme_files/celex.readme.html.
Consortium for Lexical Resources,UPenns, Eds.Fabre C., Jacquemin C, 2000.
Boosting variantrecognition with light semantics.
ProceedingsCOLING?2000, pp.
264-270, Luxemburg.Fellbaum, C. 1998.
WordNet: An Electronic LexicalDatabase.
Cambridge, MA, MIT Press.Ferret O., Grau B., Hurault-Plantet M., Illouz G.,Jacquemin C. (2000), QALC ?
the Question-Answering system of LIMSI-CNRS, pre-proceedings of TREC9, NIST, Gaithersburg, CA.Harabagiu S., Pasca M., Maiorano J.
2000.Experiments with Open-Domain Textual QuestionAnswering.
Proceedings of  Coling'2000,Saarbrucken, Germany.Jacquemin C. 1999.
Syntagmatic and paradigmaticrepresentations of term variation.
Proceedings ofACL'99.
341-348.Justeson J., Katz S. 1995.
Technical terminology:some linguistic properties and an algorithm foridentification in texte.
Natural LanguageEngineering.
1: 9-27.Kwok K.L., Grunfeld L., Dinstl N., Chan M. 2000.TREC9 Cross Language, Web and Question-Answering Track experiments using PIRCS.
Pre-proceedings of TREC9, Gaithersburg, MD, NISTEds.
26-35.Loupy C. , Bellot P., El-B?ze M., Marteau P.-F..Query Expansion and Classification of RetrievedDocuments, TREC (1998), 382-389.Prager J., Brown, E., Radev, D., Czuba, K. (2000),One Search Engine or two for Question-Answering, NISTs, Eds., Proceedings of TREC9,Gaithersburg, MD.
250-254.Schmid H. 1999.
Improvments in Part-of-SpeechTagging with an Application To German.Natural?Language Processing Using Very LargeCorpora, Dordrecht, S. Armstrong, K. W. Chuch,P.
Isabelle, E. Tzoukermann,  D. Yarowski, Eds.,Kluwer Academic Publisher.Schwarz C. 1988.
The TINA Project: text contentanalysis at the Corporate Research Laboratories atSiemens.
Proceedings of Intelligent MultimediaInformation Retrieval Systems and Management(RIAO?88) Cambridge, MA.
361-368.
