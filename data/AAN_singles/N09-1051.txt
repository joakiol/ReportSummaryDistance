Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 450?458,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsPerformance Prediction for Exponential Language ModelsStanley F. ChenIBM T.J. Watson Research CenterP.O.
Box 218, Yorktown Heights, NY 10598stanchen@watson.ibm.comAbstractWe investigate the task of performance pre-diction for language models belonging to theexponential family.
First, we attempt to em-pirically discover a formula for predicting testset cross-entropy for n-gram language mod-els.
We build models over varying domains,data set sizes, and n-gram orders, and performlinear regression to see whether we can modeltest set performance as a simple function oftraining set performance and various modelstatistics.
Remarkably, we find a simple rela-tionship that predicts test set performance witha correlation of 0.9997.
We analyze why thisrelationship holds and show that it holds forother exponential language models as well, in-cluding class-based models and minimum dis-crimination information models.
Finally, wediscuss how this relationship can be applied toimprove language model performance.1 IntroductionIn this paper, we investigate the following questionfor language models belonging to the exponentialfamily: given some training data and test data drawnfrom the same distribution, can we accurately pre-dict the test set performance of a model estimatedfrom the training data?
This problem is known asperformance prediction and is relevant for model se-lection, the task of selecting the best model from aset of candidate models given data.1Let us first define some notation.
Events have theform (x, y), where we attempt to predict the cur-rent word y given previous words x.
We denote thetraining data as D = (x1, y1), .
.
.
, (xD, yD) and de-fine p?
(x, y) = countD(x, y)/D to be the empiricaldistribution of the training data.
Similarly, we have1A long version of this paper can be found at (Chen, 2008).a test set D?
and an associated empirical distribu-tion p?
(x, y).
We take the performance of a condi-tional language model p(y|x) to be the cross-entropyH(p?, p) between the empirical test distribution p?and the model p(y|x):H(p?, p) = ??x,yp?
(x, y) log p(y|x) (1)This is equivalent to the negative mean log-likelihood per event, as well as to log perplexity.We only consider models in the exponential fam-ily.
An exponential model p?
(y|x) is a model witha set of features {f1(x, y), .
.
.
, fF (x, y)} and equalnumber of parameters ?
= {?1, .
.
.
, ?F } wherep?
(y|x) = exp(?Fi=1 ?ifi(x, y))Z?
(x) (2)and where Z?
(x) is a normalization factor.One of the seminal methods for performance pre-diction is the Akaike Information Criterion (AIC)(Akaike, 1973).
For a model, let ??
be the maxi-mum likelihood estimate of ?
on some training data.Akaike derived the following estimate for the ex-pected value of the test set cross-entropy H(p?, p??
):H(p?, p??)
?
H(p?, p??)
+FD (3)H(p?, p??)
is the cross-entropy of the training set, Fis the number of parameters in the model, and D isthe number of events in the training data.
However,maximum likelihood estimates for language mod-els typically yield infinite cross-entropy on test data,and thus AIC behaves poorly for these domains.In this work, instead of deriving a performanceprediction relationship theoretically, we attempt toempirically discover a formula for predicting testperformance.
Initially, we consider only n-gram lan-guage models, and build models over varying do-mains, data set sizes, and n-gram orders.
We per-form linear regression to discover whether we can450model test set cross-entropy as a simple function oftraining set cross-entropy and other model statistics.For the 200+ n-gram models we evaluate, we findthat the empirical relationshipH(p?, p??)
?
H(p?, p??)
+?DF?i=1|?
?i| (4)holds with a correlation of 0.9997 where ?
is a con-stant and where ??
= {?
?i} are regularized parameterestimates; i.e., rather than estimating performancefor maximum likelihood models as in AIC, we dothis for regularized models.
In other words, test setcross-entropy can be approximated by the sum of thetraining set cross-entropy and the scaled sum of themagnitudes of the model parameters.To maximize the correlation achieved by eq.
(4),we find that it is necessary to use the same regular-ization method and regularization hyperparametersacross models and that the optimal value of ?
de-pends on the values of the hyperparameters.
Con-sequently, we first evaluate several types of regu-larization and find which of these (and which hy-perparameter values) work best across all domains,and use these values in all subsequent experiments.While `22 regularization gives the best performancereported in the literature for n-gram models, we findhere that `1 + `22 regularization works even better.The organization of this paper is as follows: InSection 2, we evaluate various regularization tech-niques for n-gram models and select the method andhyperparameter values that give the best overall per-formance.
In Section 3, we discuss experiments tofind a formula for predicting n-gram model perfor-mance, and provide an explanation for why eq.
(4)works so well.
In Section 4, we evaluate how welleq.
(4) holds for several class-based language mod-els and minimum discrimination information mod-els.
Finally, in Sections 5 and 6 we discuss relatedwork and conclusions.2 Selecting Regularization SettingsIn this section, we address the issue of how to per-form regularization in our later experiments.
Fol-lowing the terminology of Dud?
?k and Schapire(2006), the most widely-used and effective methodsfor regularizing exponential models are `1 regular-ization (Tibshirani, 1994; Kazama and Tsujii, 2003;data token range training voc.source type of n sents.
sizeA RH letter 2?7 100?75k 27B WSJ POS 2?7 100?30k 45C WSJ word 2?5 100?100k 300D WSJ word 2?5 100?100k 3kE WSJ word 2?5 100?100k 21kF BN word 2?5 100?100k 84kG SWB word 2?5 100?100k 19kTable 1: Statistics of data sets.
RH = Random Housedictionary; WSJ = Wall Street Journal; BN = BroadcastNews; SWB = Switchboard.Goodman, 2004) and `22 regularization (Lau, 1994;Chen and Rosenfeld, 2000; Lebanon and Lafferty,2001).
While not as popular, another regularizationscheme that has been shown to be effective is 2-norminequality regularization (Kazama and Tsujii, 2003)which is an instance of `1+`22 regularization as notedby Dud?
?k and Schapire (2006).
Under `1 + `22 regu-larization, the regularized parameter estimates ??
arechosen to optimize the objective functionO`1+`22(?)
= H(p?, p?)
+?DF?i=1|?i|+ 12?2DF?i=1?2i(5)Note that `1 regularization can be considered a spe-cial case of this (by taking ?
= ?)
as can `22 regu-larization (by taking ?
= 0).Here, we evaluate `1, `22, and `1 + `22 regulariza-tion for exponential n-gram models.
An exponen-tial n-gram model contains a binary feature f?
foreach n?-gram ?
occurring in the training data forn?
?
n, where f?
(x, y) = 1 iff xy ends in ?.
Wewould like to find the regularization method and as-sociated hyperparameters that work best across dif-ferent domains, training set sizes, and n-gram or-ders.
As it is computationally expensive to evalu-ate a large number of hyperparameter settings overa large collection of models, we divide this searchinto two phases.
First, we evaluate a large set of hy-perparameters on a limited set of models to come upwith a short list of candidate hyperparameters.
Wethen evaluate these candidates on our full model setto find the best one.We build n-gram models over data from five dif-ferent sources and consider three different vocabu-lary sizes for one source, giving us seven ?domains?451in total.
We refer to these domains by the letters A?G; summary statistics for each domain are given inTable 1.
The domains C?G consist of regular worddata, while domains A and B consist of letter andpart-of-speech (POS) sequences, respectively.
Do-mains C?E differ only in vocabulary.For each domain, we first randomize the order ofsentences in that data.
We partition off two devel-opment sets and an evaluation set (5000 ?sentences?each in domain A and 2500 sentences elsewhere) anduse the remaining data as training data.
In this way,we assure that our training and test data are drawnfrom the same distribution as is assumed in our laterexperiments.
Training set sizes in sentences are 100,300, 1000, 3000, etc., up to the maximums given inTable 1.
Building models for each training set sizeand n-gram order in Table 1 gives us a total of 218models over the seven domains.In the first phase of hyperparameter search, wechoose a subset of these models (57 total) and evalu-ate many different values for (?, ?2) with `1+`22 reg-ularization on each.
We perform a grid search, tryingeach value ?
?
{0.0, 0.1, 0.2, .
.
.
, 1.2} with eachvalue ?2 ?
{1, 1.2, 1.5, 2, 2.5, 3, 4, 5, 6, 7, 8, 10,?
}where ?
= ?
corresponds to `1 regularization and?
= 0 corresponds to `22 regularization.
We usea variant of iterative scaling for parameter estima-tion.
For each model and each (?, ?2), we denotethe cross-entropy of the development data as Hm?,?for the mth model, m ?
{1, .
.
.
, 57}.
Then, for eachm and (?, ?2), we can compute how much worse thesettings (?, ?2) perform with model m as comparedto the best hyperparameter settings for that model:H?m?,?
= Hm?,?
?
min??,??
Hm??,??
(6)We would like to select (?, ?2) for which H?m?,?
tendsto be small; in particular, we choose (?, ?2) thatminimizes the root mean squared (RMS) errorH?RMS?,?
=????
15757?m=1(H?m?,?
)2 (7)For each of `1, `22, and `1 + `22 regularization, we re-tain the 6?8 best hyperparameter settings.
To choosethe best single hyperparameter setting from withinthis candidate set, we repeat the same analysis ex-cept over the full set of 218 models.statistic RMSE coeff.1D?Fi=1 |?
?i| 0.043 0.9381D?i:?
?i>0 ?
?i 0.044 0.9391D?Fi=1 ?
?i 0.047 0.9401D?Fi=1 |?
?i|43 0.162 0.7551D?Fi=1 |?
?i|32 0.234 0.6691D?Fi=1 ?
?2i 0.429 0.443F 6=0D 0.709 1.289F 6=0 logDD 0.783 0.129FD 0.910 1.109F logDD 0.952 0.1121 1.487 1.698FD?F?1 2.232 -0.028F 6=0D?F 6=0?1 2.236 -0.023Table 2: Root mean squared error (RMSE) in nats whenpredicting difference in development set and training setcross-entropy as linear function of a single statistic.
Thelast column is the optimal coefficient for that statistic.On the development sets, the (?, ?2) value withthe lowest squared error is (0.5, 6), and these arethe hyperparameter settings we use in all later ex-periments unless otherwise noted.
The RMS error,mean error, and maximum error for these hyperpa-rameters on the evaluation sets are 0.011, 0.007, and0.033 nats, respectively.2 An error of 0.011 nats cor-responds to a 1.1% difference in perplexity whichis generally considered insignificant.
Thus, we canachieve good performance across domains, data setsizes, and n-gram orders using a single set of hyper-parameters as compared to optimizing hyperparam-eters separately for each model.3 N -Gram Model Performance PredictionNow that we have established which regularizationmethod and hyperparameters to use, we attempt toempirically discover a simple formula for predict-ing the test set cross-entropy of regularized n-grammodels.
The basic strategy is as follows: We firstbuild a large number of n-gram models over differ-ent domains, training set sizes, and n-gram orders.Then, we come up with a set of candidate statistics,e.g., training set cross-entropy, number of features,etc., and do linear regression to try to best model test2All cross-entropy values are reported in nats, or naturalbits, equivalent to log2 e regular bits.
This will let us directlycompare ?
values with average discounts in Section 3.1.45201234567  01234567test cross-entropy - training cross-entropy (nats)?
|?i| / DFigure 1: Graph of optimism on evaluation data vs.1D?Fi=1 |?
?i| for various n-gram models under `1 + `22regularization, ?
= 0.5 and ?2 = 6.
The line repre-sents the predicted optimism according to eq.
(9) with?
= 0.938.set cross-entropy as a linear function of these statis-tics.
We assume that training and test data comefrom the same distribution; otherwise, it would bedifficult to predict test performance.We use the same 218 n-gram models as in Sec-tion 2.
For each model, we compute training setcross-entropy H(p?, p??)
as well as all of the statis-tics listed on the left in Table 2.
The statistics FD ,FD?F?1 , andF logDD are motivated by AIC, AICc(Hurvich and Tsai, 1989), and the Bayesian Infor-mation Criterion (Schwarz, 1978), respectively.
Asfeatures fi with ?
?i = 0 have no effect, instead ofF we also consider using F 6=0, the number of fea-tures fi with ?
?i 6= 0.
The statistics 1D?Fi=1 |?
?i| and1D?Fi=1 ?
?2i are motivated by eq.
(5).
The statisticswith fractional exponents are suggested by Figure 2.The value 1 is present to handle constant offsets.After some initial investigation, it became clearthat training set cross-entropy is a very good (par-tial) predictor of test set cross-entropy with coeffi-cient 1.
As there is ample theoretical support forthis, instead of fitting test set performance directly,we chose to model the difference between test andtraining performance as a function of the remainingstatistics.
This difference is sometimes referred to asthe optimism of a model:optimism(p??)
?
H(p?, p??
)?H(p?, p??)
(8)First, we attempt to model optimism as a lin-ear function of a single statistic.
For each statis-tic listed previously, we perform linear regressionto minimize root mean squared error when predict-ing development set optimism.
In Table 2, we dis-play the RMSE and best coefficient for each statis-tic.
We see that three statistics have by far the lowesterror: 1D?Fi=1 |?
?i|, 1D?i:?
?i>0 ?
?i, and1D?Fi=1 ?
?i.In practice, most ?
?i in n-gram models are positive,so these statistics tend to have similar values.
Wechoose the best ranked of these, 1D?Fi=1 |?
?i|, andshow in Section 3.1 why this statistic is more appeal-ing than the others.
Next, we investigate modelingoptimism as a linear function of a pair of statistics.We find that the best RMSE for two variables (0.042)is only slightly lower than that for one (0.043), so itis doubtful that a second variable helps.Thus, our analysis suggests that among our candi-dates, the best predictor of optimism is simplyoptimism ?
?DF?i=1|?
?i| (9)where ?
= 0.938, with this value being independentof domain, training set size, and n-gram order.
Inother words, the difference between test and train-ing cross-entropy is a linear function of the sum ofparameter magnitudes scaled per event.
Substitutinginto eq.
(8) and rearranging, we get eq.
(4).To assess the accuracy of eq.
(4), we compute var-ious statistics on our evaluation sets using the best?
from our development data, i.e., ?
= 0.938.
InFigure 1, we graph optimism for the evaluation dataagainst 1D?Fi=1 |?
?i| for each of our models; we seethat the linear correlation is very good.
The correla-tion between the actual and predicted cross-entropyon the evaluation data is 0.9997; the mean absoluteprediction error is 0.030 nats; the RMSE is 0.043nats; and the maximum absolute error is 0.166 nats.Thus, on average we can predict test performance towithin 3% in perplexity, though in the worst case wemay be off by as much as 18%.33The sampling variation in our test set selection limits themeasured accuracy of our performance prediction.
To givesome idea of the size of this effect, we randomly selected 100test sets in domain D of 2500 sentences each (as in our otherexperiments).
We evaluated their cross-entropies using mod-els trained on 100, 1k, 10k, and 100k sentences.
The empiri-453-1.5-1-0.500.511.522.5-1  0  1  2  3  4discount?Figure 2: Smoothed graph of discount versus ?
?i for allfeatures in ten different models built on domains A andE.
Each smoothed point represents the average of at least512 raw data points.If we compute the prediction error of eq.
(4) overthe same models except using `1 or `22 regulariza-tion (with the best corresponding hyperparametervalues found in Section 2), the prediction RMSE is0.054 and 0.139 nats, respectively.
Thus, we findthat choosing hyperparameters carefully in Section 2was important in doing well in performance predic-tion.
While hyperparameters were chosen to opti-mize test performance rather than prediction accu-racy, we find that the chosen hyperparameters arefavorable for the latter task as well.3.1 Why Does Prediction Work So Well?The correlation in Figure 1 is remarkably high, andthus it begs for an explanation.
First, let us expressthe difference in test and training cross-entropy fora model in terms of its parameters ?.
Substitutingeq.
(2) into eq.
(1), we getH(p?, p?)
= ?F?i=1?iEp?
[fi] +?xp?
(x) logZ?
(x)(10)where Ep?
[fi] = ?x,y p?
(x, y)fi(x, y).
Then, wecan express the difference in test and training per-formance asH(p?, p?
)?H(p?, p?)
=?Fi=1 ?i(Ep?[fi]?
Ep?
[fi])+?x(p?(x)?
p?
(x)) logZ?
(x) (11)cal standard deviation across test sets was found to be 0.0123,0.0144, 0.0167, and 0.0174 nats, respectively.
This effect canbe mitigated by simply using larger test sets.Ignoring the last term on the right, we see that opti-mism for exponential models is a linear function ofthe ?i?s with coefficients Ep?[fi]?
Ep?
[fi].Then, we can ask what Ep?
[fi] ?
Ep?
[fi] valueswould let us satisfy eq.
(4).
Consider the relationship(Ep?[fi]?
Ep?
[fi])?D ?
?
sgn ?
?i (12)If we substitute this into eq.
(11) and ignore the lastterm on the right again, this gives us exactly eq.
(4).We refer to the value (Ep?[fi]?
Ep?
[fi])?D as thediscount of a feature.
It can be thought of as rep-resenting how many times less the feature occurs inthe test data as opposed to the training data, if thetest data were normalized to be the same size as thetraining data.
Discounts for n-grams have been stud-ied extensively, e.g., (Good, 1953; Church and Gale,1991; Chen and Goodman, 1998), and tend not tovary much across training set sizes.We can check how well eq.
(12) holds for actualregularized n-gram models.
We construct a total often n-gram models on domains A and E. We buildfour letter 5-gram models on domain A on trainingsets ranging in size from 100 words to 30k words,and six models (either trigram or 5-gram) on do-main E on training sets ranging from 100 sentencesto 30k sentences.
We create large development testsets (45k words for domain A and 70k sentences fordomain E) to better estimate Ep?
[fi].Because graphs of discounts as a function of ?
?iare very noisy, we smooth the data before plotting.We partition data points into buckets containing atleast 512 points.
We average all of the points ineach bucket to get a ?smoothed?
data point, and plotthis single point for each bucket.
In Figure 2, weplot smoothed discounts as a function of ?
?i over therange ?
?i ?
[?1, 4] for all ten models.We see that eq.
(12) holds at a very rough levelover the ?
?i range displayed.
If we examine howmuch different ranges of ?
?i contribute to the over-all value of?Fi=1 ??i(Ep?[fi]?Ep?
[fi]), we find thatthe great majority of the mass (90?95%+) is concen-trated in the range ?
?i ?
[0, 4] for all ten models un-der consideration.
Thus, to a first approximation, thereason that eq.
(4) holds with ?
= 0.938 is becauseon average, feature expectations have a discount ofabout this value for ?
?i in this range.44This analysis provides some insight as to when eq.
(4)45400.511.522.533.544.550  1  2  3  4  5test cross-entropy- trainingcross-entropy (nats)?
|?i| / DFigure 3: Graph of optimism on evaluation data vs.1D?Fi=1 |?
?i| for various models.
The ?+?
marks corre-spond to models S, M, and L over different training setsizes, n-gram orders, and numbers of classes.
The ??
?marks correspond to MDI models over different n-gramorders and in-domain training set sizes.
The line andsmall points are taken from Figure 1.Due to space considerations, we only summarizeour other findings; a longer discussion is providedin (Chen, 2008).
We find that the absolute error incross-entropy tends to be quite small across modelsfor several reasons.
For non-sparse models, thereis significant variation in average discounts, but be-cause 1D?Fi=1 |?
?i| is low, the overall error is low.In contrast, sparse models are dominated by single-count n-grams with features whose average discountis quite close to ?
= 0.938.
Finally, the last term onthe right in eq.
(11) also plays a small but significantrole in keeping the prediction error low.4 Other Exponential Language ModelsIn (Chen, 2009), we show how eq.
(4) can be usedto motivate a novel class-based language model anda regularized version of minimum discrimination in-formation (MDI) models (Della Pietra et al, 1992).In this section, we analyze whether in addition toword n-gram models, eq.
(4) holds for these otherexponential language models as well.won?t hold.
For example, if a feature function fi is doubled, itsexpectations and discount will also double.
Thus, eq.
(4) won?thold in general for models with continuous feature values, asaverage discounts may vary widely.4.1 Class-Based Language ModelsWe assume a word w is always mapped to the sameclass c(w).
For a sentence w1 ?
?
?wl, we havep(w1 ?
?
?wl) =?l+1j=1 p(cj |c1 ?
?
?
cj?1, w1 ?
?
?wj?1)?
?lj=1 p(wj |c1 ?
?
?
cj , w1 ?
?
?wj?1) (13)where cj = c(wj) and where cl+1 is an end-of-sentence token.
We use the notation png(y|?)
todenote an exponential n-gram model as defined inSection 2, where we have features for each suffix ofeach ?y occurring in the training set.
We use thenotation png(y|?1, ?2) to denote a model containingall features in the models png(y|?1) and png(y|?2).We consider three class models, models S, M, andL, defined aspS(cj |c1???cj?1,w1??
?wj?1)=png(cj |cj?2cj?1)pS(wj |c1??
?cj ,w1??
?wj?1)=png(wj |cj)pM (cj |c1???cj?1,w1??
?wj?1)=png(cj |cj?2cj?1,wj?2wj?1)pM (wj |c1??
?cj ,w1??
?wj?1)=png(wj |wj?2wj?1cj)pL(cj |c1???cj?1,w1??
?wj?1)=png(cj |wj?2cj?2wj?1cj?1)pL(wj |c1??
?cj ,w1??
?wj?1)=png(wj |wj?2cj?2wj?1cj?1cj)Model S is an exponential version of the class-basedn-gram model from (Brown et al, 1992); model Mis a novel model introduced in (Chen, 2009); andmodel L is an exponential version of the model ind-expredict from (Goodman, 2001).To evaluate whether eq.
(4) can accurately pre-dict test performance for these class-based models,we use the WSJ data and vocabulary from domainE and consider training set sizes of 1k, 10k, 100k,and 900k sentences.
We create three different wordclassings containing 50, 150, and 500 classes usingthe algorithm of Brown et al (1992) on the largesttraining set.
For each training set and number ofclasses, we build both 3-gram and 4-gram versionsof each of our three class models.In Figure 3, we plot optimism (i.e., test minustraining cross-entropy) versus 1D?Fi=1 |?
?i| for thesemodels (66 in total) on our WSJ evaluation set.
The?+?
marks correspond to our class n-gram models,while the small points replicate the points for wordn-gram models from Figure 1.
Remarkably, eq.
(4)appears to accurately predict performance for our455class n-gram models using the same ?
= 0.938value found for word n-gram models.
The mean ab-solute prediction error is 0.029 nats, comparable tothat found for word n-gram models.It is interesting that eq.
(4) works for class-basedmodels despite their being composed of two sub-models, one for word prediction and one for classprediction.
However, taking the log of eq.
(13), wenote that the cross-entropy of text can be expressedas the sum of the cross-entropy of its word tokensand the cross-entropy of its class tokens.
It wouldnot be surprising if eq.
(4) holds separately for theclass prediction model predicting class data and theword prediction model predicting word data, sinceall of these component models are basically n-grammodels.
Summing, this explains why eq.
(4) holdsfor the whole class model.4.2 Models with Prior DistributionsMinimum discrimination information models (DellaPietra et al, 1992) are exponential models with aprior distribution q(y|x):p?
(y|x) = q(y|x)exp(?Fi=1 ?ifi(x, y))Z?
(x) (14)The central issue in performance prediction for MDImodels is whether q(y|x) needs to be accounted for.That is, if we assume q is an exponential model,should its parameters ?qi be included in the sum ineq.
(4)?
From eq.
(11), we note that if Ep?
[fi] ?Ep?
[fi] = 0 for a feature fi, then the feature doesnot affect the difference between test and trainingcross-entropy (ignoring its impact on the last term).By assumption, the training and test set for p comefrom the same distribution while q is derived froman independent data set.
It follows that we expectEp?
[f qi ]?Ep?
[f qi ] to be zero for features in q, and weshould ignore q when applying eq.
(4).To evaluate whether eq.
(4) holds for MDI mod-els, we use the same WSJ training and evaluationsets from domain E as in Section 4.1.
We considerthree different training set sizes: 1k, 10k, and 100ksentences.
To train q, we use the 100k sentence BNtraining set from domain F. We build both trigramand 4-gram versions of each model.In Figure 3, we plot test minus training cross-entropy versus 1D?Fi=1 |?
?i| for these models on ourWSJ evaluation set; the ???
marks correspond tothe MDI models.
As expected, eq.
(4) appears towork quite well for MDI models using the same?
= 0.938 value as before; the mean absolute pre-diction error is 0.077 nats.5 Related WorkWe group existing performance prediction methodsinto two categories: non-data-splitting methods anddata-splitting methods.
In non-data-splitting meth-ods, test performance is directly estimated fromtraining set performance and/or other statistics of amodel.
Data-splitting methods involve partitioningtraining data into a truncated training set and a surro-gate test set and using surrogate test set performanceto estimate true performance.The most popular non-data-splitting methods forpredicting test set cross-entropy (or likelihood) areAIC and variants such as AICc, quasi-AIC (QAIC),and QAICc (Akaike, 1973; Hurvich and Tsai, 1989;Lebreton et al, 1992).
In Section 3, we consid-ered performance prediction formulae with the sameform as AIC and AICc (except using regularized pa-rameter estimates), and neither performed as well aseq.
(4); e.g., see Table 2.There are many techniques for bounding testset classification error including the Occam?s Ra-zor bound (Blumer et al, 1987; McAllester, 1999),PAC-Bayes bound (McAllester, 1999), and the sam-ple compression bound (Littlestone and Warmuth,1986; Floyd and Warmuth, 1995).
These methodsderive theoretical guarantees that the true error rateof a classifier will be below (or above) some valuewith a certain probability.
Langford (2005) evalu-ates these techniques over many data sets; while thebounds can sometimes be fairly tight, in many datasets the bounds are quite loose.When learning an element from a set of targetclassifiers, the Vapnik-Chervonenkis (VC) dimen-sion of the set can be used to bound the true error raterelative to the training error rate with some probabil-ity (Vapnik, 1998); this technique has been used tocompute error bounds for many types of classifiers.For example, Bartlett (1998) shows that for a neuralnetwork with small weights and small training setsquared error, the true error depends on the size ofits weights rather than the number of weights; thisfinding is similar in spirit to eq.
(4).456In practice, the most accurate methods for perfor-mance prediction in many contexts are data-splittingmethods (Guyon et al, 2006).
These techniques in-clude the hold-out method; leave-one-out and k-foldcross-validation; and bootstrapping (Allen, 1974;Stone, 1974; Geisser, 1975; Craven and Wahba,1979; Efron, 1983).
However, unlike non-data-splitting methods, these methods do not lend them-selves well to providing insight into model design asdiscussed in Section 6.6 DiscussionWe show that for several types of exponential lan-guage models, it is possible to accurately predict thecross-entropy of test data using the simple relation-ship given in eq.
(4).
When using `1 + `22 regulariza-tion with (?
= 0.5, ?2 = 6), the value ?
= 0.938works well across varying model types, domains,vocabulary sizes, training set sizes, and n-gram or-ders, yielding a mean absolute error of about 0.03nats (3% in perplexity).
We evaluate ?300 languagemodels in total, including word and class n-grammodels and n-gram models with prior distributions.While there has been a great deal of work inperformance prediction, the vast majority of workon non-data-splitting methods has focused on find-ing theoretically-motivated approximations or prob-abilistic bounds on test performance.
In contrast, wedeveloped eq.
(4) on a purely empirical basis, andthere has been little, if any, existing work that hasshown comparable performance prediction accuracyover such a large number of models and data sets.
Inaddition, there has been little, if any, previous workon performance prediction for language modeling.5While eq.
(4) performs well as compared to othernon-data-splitting methods for performance predic-tion, the prediction error can be several percent inperplexity, which means we cannot reliably rankmodels that are close in quality.
In addition, inspeech recognition and many other applications, anexternal test set is typically provided, which meanswe can measure test set performance directly.
Thus,in practice, eq.
(4) is not terribly useful for the task5Here, we refer to predicting test set performance fromtraining set performance and other model statistics.
However,there has been a good deal of work on predicting speech recog-nition word-error rate from test set perplexity and other statis-tics, e.g., (Klakow and Peters, 2002).of model selection; instead, what eq.
(4) gives us isinsight into model design.
That is, instead of select-ing between candidate models once they have beenbuilt as in model selection, it is desirable to be ableto select between models at the model design stage.Being able to intelligently compare models (with-out implementation) requires that we know whichaspects of a model impact test performance, and thisis exactly what eq.
(4) tells us.Intuitively, simpler models should perform betteron test data given equivalent training performance,and model structure (as opposed to parameter val-ues) is an important aspect of the complexity of amodel.
Accordingly, there have been many meth-ods for model selection that measure the size of amodel in terms of the number of features or param-eters in the model, e.g., (Akaike, 1973; Rissanen,1978; Schwarz, 1978).
Surprisingly, for exponentiallanguage models, the number of model parametersseems to matter not at all; all that matters are themagnitudes of the parameter values.
Consequently,one can improve such models by adding features (ora prior model) that reduce parameter values whilemaintaining training performance.In (Chen, 2009), we show how these ideas can beused to motivate heuristics for improving the perfor-mance of existing language models, and use theseheuristics to develop a novel class-based model anda regularized version of MDI models that outper-form comparable methods in both perplexity andspeech recognition word-error rate on WSJ data.
Inaddition, we show how the tradeoff between train-ing set performance and model size impacts aspectsof language modeling as diverse as backoff n-gramfeatures, class-based models, and domain adapta-tion.
In sum, eq.
(4) provides a new and valuableframework for characterizing, analyzing, and de-signing statistical models.AcknowledgementsWe thank Bhuvana Ramabhadran and the anony-mous reviewers for their comments on this and ear-lier versions of the paper.ReferencesHirotugu Akaike.
1973.
Information theory and an ex-tension of the maximum likelihood principle.
In Sec-457ond Intl.
Symp.
on Information Theory, pp.
267?281.David M. Allen.
1974.
The relationship between vari-able selection and data augmentation and a method forprediction.
Technometrics, 16(1):125?127.Peter L. Bartlett.
1998.
The sample complexity of pat-tern classification with neural networks: the size of theweights is more important than the size of the network.IEEE Trans.
on Information Theory, 44(2):525?536.Alselm Blumer, Andrzej Ehrenfeucht, David Haussler,and Manfred K. Warmuth.
1987.
Occam?s razor.
In-formation Processing Letters, 24(6):377?380.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,Jennifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479, December.Stanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
Tech.
Report TR-10-98, Harvard Univ.Stanley F. Chen and Ronald Rosenfeld.
2000.
A surveyof smoothing techniques for maximum entropy mod-els.
IEEE Trans.
Speech and Aud.
Proc., 8(1):37?50.Stanley F. Chen.
2008.
Performance prediction for ex-ponential language models.
Tech.
Report RC 24671,IBM Research Division, October.Stanley F. Chen.
2009.
Shrinking exponential languagemodels.
In Proc.
of HLT-NAACL.Kenneth W. Church and William A. Gale.
1991.
A com-parison of the enhanced Good-Turing and deleted esti-mation methods for estimating probabilities of Englishbigrams.
Computer Speech and Language, 5:19?54.Peter Craven and Grace Wahba.
1979.
Smoothing noisydata with spline functions: estimating the correct de-gree of smoothing by the method of generalized cross-validation.
Numerische Mathematik, 31:377?403.Stephen Della Pietra, Vincent Della Pietra, Robert L.Mercer, and Salim Roukos.
1992.
Adaptive languagemodeling using minimum discriminant estimation.
InProc.
Speech and Natural Lang.
DARPA Workshop.Miroslav Dud?
?k and Robert E. Schapire.
2006.
Maxi-mum entropy distribution estimation with generalizedregularization.
In Proc.
of COLT.Bradley Efron.
1983.
Estimating the error rate of a pre-diction rule: Improvement on cross-validation.
J. ofthe American Statistical Assoc., 78(382):316?331.Sally Floyd and Manfred Warmuth.
1995.
Sample com-pression, learnability, and the Vapnik-Chervonenkisdimension.
Machine Learning, 21(3):269?304.Seymour Geisser.
1975.
The predictive sample reusemethod with applications.
J. of the American Statisti-cal Assoc., 70:320?328.I.J.
Good.
1953.
The population frequencies ofspecies and the estimation of population parameters.Biometrika, 40(3 and 4):237?264.Joshua T. Goodman.
2001.
A bit of progress in languagemodeling.
MSR-TR-2001-72, Microsoft Research.Joshua Goodman.
2004.
Exponential priors for maxi-mum entropy models.
In Proc.
of NAACL.Isabelle Guyon, Amir Saffari, Gideon Dror, and JoachimBuhmann.
2006.
Performance prediction challenge.In Proc.
of Intl.
Conference on Neural Networks(IJCNN06), IEEE World Congress on ComputationalIntelligence (WCCI06), pp.
2958?2965, July.Clifford M. Hurvich and Chih-Ling Tsai.
1989.
Regres-sion and time series model selection in small samples.Biometrika, 76(2):297?307, June.Jun?ichi Kazama and Jun?ichi Tsujii.
2003.
Evaluationand extension of maximum entropy models with in-equality constraints.
In Proc.
of EMNLP, pp.
137?144.Dietrich Klakow and Jochen Peters.
2002.
Testing thecorrelation of word error rate and perplexity.
SpeechCommunications, 38(1):19?28.John Langford.
2005.
Tutorial on practical predictiontheory for classification.
J. of Machine Learning Re-search, 6:273?306.Raymond Lau.
1994.
Adaptive statistical language mod-elling.
Master?s thesis, Department of Electrical En-gineering and Computer Science, Massachusetts Insti-tute of Technology, Cambridge, MA.Guy Lebanon and John Lafferty.
2001.
Boosting andmaximum likelihood for exponential models.
Tech.Report CMU-CS-01-144, Carnegie Mellon Univ.Jean-Dominique Lebreton, Kenneth P. Burnham, JeanClobert, and David R. Anderson.
1992.
Modeling sur-vival and testing biological hypotheses using markedanimals: a unified approach with case studies.
Eco-logical Monographs, 62:67?118.Nick Littlestone and Manfred K. Warmuth.
1986.
Re-lating data compression and learnability.
Tech.
report,Univ.
of California, Santa Cruz.David A. McAllester.
1999.
PAC-Bayesian model aver-aging.
In Proc.
of COLT, pp.
164?170.Jorma Rissanen.
1978.
Modeling by the shortest datadescription.
Automatica, 14:465?471.Gideon Schwarz.
1978.
Estimating the dimension of amodel.
Annals of Statistics, 6(2):461?464.M.
Stone.
1974.
Cross-validatory choice and assessmentof statistical predictions.
J. of the Royal Statistical So-ciety B, 36:111?147.Robert Tibshirani.
1994.
Regression shrinkage and se-lection via the lasso.
Tech.
report, Univ.
of Toronto.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.John Wiley, New York.458
