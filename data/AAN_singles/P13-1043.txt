Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434?443,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsFast and Accurate Shift-Reduce Constituent ParsingMuhua Zhu?, Yue Zhang?, Wenliang Chen?, Min Zhang?
and Jingbo Zhu?
?Natural Language Processing Lab., Northeastern University, China?Singapore University of Technology and Design, Singapore?
Soochow University, China and Institute for Infocomm Research, Singaporezhumuhua@gmail.com yue zhang@sutd.edu.sgchenwenliang@gmail.com mzhang@i2r.a-star.edu.sgzhujingbo@mail.neu.edu.cnAbstractShift-reduce dependency parsers givecomparable accuracies to their chart-based counterparts, yet the best shift-reduce constituent parsers still lag behindthe state-of-the-art.
One important reasonis the existence of unary nodes in phrasestructure trees, which leads to differentnumbers of shift-reduce actions betweendifferent outputs for the same input.
Thisturns out to have a large empirical impacton the framework of global training andbeam search.
We propose a simple yeteffective extension to the shift-reduceprocess, which eliminates size differencesbetween action sequences in beam-search.Our parser gives comparable accuraciesto the state-of-the-art chart parsers.
Withlinear run-time complexity, our parser isover an order of magnitude faster than thefastest chart parser.1 IntroductionTransition-based parsers employ a set of shift-reduce actions and perform parsing using a se-quence of state transitions.
The pioneering mod-els rely on a classifier to make local decisions, andsearch greedily for a transition sequence to build aparse tree.
Greedy, classifier-based parsers havebeen developed for both dependency grammars(Yamada and Matsumoto, 2003; Nivre et al, 2006)and phrase-structure grammars (Sagae and Lavie,2005).
With linear run-time complexity, they werecommonly regarded as a faster but less accuratealternative to graph-based chart parsers (Collins,1997; Charniak, 2000; McDonald et al, 2005).Various methods have been proposed to addressthe disadvantages of greedy local parsing, amongwhich a framework of beam-search and globaldiscriminative training have been shown effectivefor dependency parsing (Zhang and Clark, 2008;Huang and Sagae, 2010).
While beam-searchreduces error propagation compared with greedysearch, a discriminative model that is globally op-timized for whole sequences of transition actionscan avoid local score biases (Lafferty et al, 2001).This framework preserves the most important ad-vantage of greedy local parsers, including linearrun-time complexity and the freedom to define ar-bitrary features.
With the use of rich non-local fea-tures, transition-based dependency parsers achievestate-of-the-art accuracies that are comparable tothe best-graph-based parsers (Zhang and Nivre,2011; Bohnet and Nivre, 2012).
In addition, pro-cessing tens of sentences per second (Zhang andNivre, 2011), these transition-based parsers can bea favorable choice for dependency parsing.The above global-learning and beam-searchframework can be applied to transition-basedphrase-structure (constituent) parsing also (Zhangand Clark, 2009), maintaining all the afore-mentioned benefits.
However, the effects werenot as significant as for transition-based depen-dency parsing.
The best reported accuracies oftransition-based constituent parsers still lag behindthe state-of-the-art (Sagae and Lavie, 2006; Zhangand Clark, 2009).
One difference between phrase-structure parsing and dependency parsing is thatfor the former, parse trees with different numbersof unary rules require different numbers of actionsto build.
Hence the scoring model needs to disam-biguate between transitions sequences with differ-ent sizes.
For the same sentence, the largest outputcan take twice as many as actions to build as the434smallest one.
This turns out to have a significantempirical impact on parsing with beam-search.We propose an extension to the shift-reduce pro-cess to address this problem, which gives signifi-cant improvements to the parsing accuracies.
Ourmethod is conceptually simple, requiring only oneadditional transition action to eliminate size dif-ferences between different candidate outputs.
Onstandard evaluations using both the Penn Tree-bank and the Penn Chinese Treebank, our parsergave higher accuracies than the Berkeley parser(Petrov and Klein, 2007), a state-of-the-art chartparser.
In addition, our parser runs with over 89sentences per second, which is 14 times faster thanthe Berkeley parser, and is the fastest that we areaware of for phrase-structure parsing.
An opensource release of our parser (version 0.6) is freelyavailable on the Web.
1In addition to the above contributions, we applya variety of semi-supervised learning techniques toour transition-based parser.
These techniques havebeen shown useful to improve chart-based pars-ing (Koo et al, 2008; Chen et al, 2012), but littlework has been done for transition-based parsers.We therefore fill a gap in the literature by report-ing empirical results using these methods.
Experi-mental results show that semi-supervised methodsgive a further improvement of 0.9% in F-score onthe English data and 2.4% on the Chinese data.Our Chinese results are the best that we are awareof on the standard CTB data.2 Baseline parserWe adopt the parser of Zhang and Clark (2009) forour baseline, which is based on the shift-reduceprocess of Sagae and Lavie (2005), and employsglobal perceptron training and beam search.2.1 Vanilla Shift-ReduceShift-reduce parsing is based on a left-to-rightscan of the input sentence.
At each step, a tran-sition action is applied to consume an input wordor construct a new phrase-structure.
A stackis used to maintain partially constructed phrase-structures, while the input words are stored in abuffer.
The set of transition actions are?
SHIFT: pop the front word from the buffer,and push it onto the stack.1http://sourceforge.net/projects/zpar/Axioms [?, 0, false,0]Goal [S, n, true, C]Inference Rules:[S, i, false, c]SHIFT [S|w, i + 1, false, c + cs][S|s1s0, i, false, c]REDUCE-L/R-X [S|X, i, false, c + cr][S|s0, i, false, c]UNARY-X [S|X, i, false, c + cu][S, n, false, c]FINISH [S, n, true, c + cf ]Figure 1: Deduction system of the baseline shift-reduce parsing process.?
REDUCE-L/R-X: pop the top two con-stituents off the stack, combine them into anew constituent with label X, and push thenew constituent onto the stack.?
UNARY-X: pop the top constituent off thestack, raise it to a new constituent with la-bel X, and push the new constituent onto thestack.?
FINISH: pop the root node off the stack andends parsing.The deduction system for the process is shownin Figure 1, where the item is formed as ?stack,buffer front index, completion mark, score?, andcs, cr , and cu represent the incremental score ofthe SHIFT, REDUCE, and UNARY parsing steps,respectively; these scores are calculated accordingto the context features of the parser state item.
nis the number of words in the input.2.2 Global Discriminative Training andBeam-SearchFor a given input sentence, the initial state has anempty stack and a buffer that contains all the inputwords.
An agenda is used to keep the k best stateitems at each step.
At initialization, the agendacontains only the initial state.
At each step, everystate item in the agenda is popped and expandedby applying a valid transition action, and the topk from the newly constructed state items are putback onto the agenda.
The process repeats untilthe agenda is empty, and the best completed stateitem (recorded as candidate output) is taken for435Description Templatesunigrams s0tc, s0wc, s1tc, s1wc, s2tcs2wc, s3tc, s3wc, q0wt, q1wtq2wt, q3wt, s0lwc, s0rwcs0uwc, s1lwc, s1rwc, s1uwcbigrams s0ws1w, s0ws1c, s0cs1w, s0cs1c,s0wq0w, s0wq0t, s0cq0w, s0cq0t,q0wq1w, q0wq1t, q0tq1w, q0tq1t,s1wq0w, s1wq0t, s1cq0w, s1cq0ttrigrams s0cs1cs2c, s0ws1cs2c, s0cs1wq0ts0cs1cs2w, s0cs1cq0t, s0ws1cq0ts0cs1wq0t, s0cs1cq0wTable 1: A summary of baseline feature templates,where si represents the ith item on the stack S andqi denotes the ith item in the queue Q. w refers tothe head lexicon, t refers to the head POS, and crefers to the constituent label.the output.The score of a state item is the total score of thetransition actions that have been applied to buildthe item:C(?)
=N?i=1?
(ai) ?
~?Here ?
(ai) represents the feature vector for the ithaction ai in state item ?.
It is computed by apply-ing the feature templates in Table 1 to the contextof ?.
N is the total number of actions in ?.The model parameter ~?
is trained with the aver-aged perceptron algorithm, applied to state items(sequence of actions) globally.
We apply the earlyupdate strategy (Collins and Roark, 2004), stop-ping parsing for parameter updates when the gold-standard state item falls off the agenda.2.3 Baseline FeaturesOur baseline features are adopted from Zhang andClark (2009), and are shown in Table 1 Here sirepresents the ith item on the top of the stack Sand qi denotes the ith item in the front end of thequeue Q.
The symbol w denotes the lexical headof an item; the symbol c denotes the constituentlabel of an item; the symbol t is the POS of a lex-ical head.
These features are adapted from Zhangand Clark (2009).
We remove Chinese specificfeatures and make the baseline parser language-independent.3 Improved hypotheses comparisonUnlike dependency parsing, constituent parsetrees for the same sentence can have differentnumbers of nodes, mainly due to the existenceof unary nodes.
As a result, completed stateNPNNaddressNNSissuesVPVBaddressNPNNSissuesFigure 2: Example parse trees of the same sen-tence with different numbers of actions.items for the same sentence can have differentnumbers of unary actions.
Take the phrase ?ad-dress issues?
for example, two possible parsesare shown in Figure 2 (a) and (b), respectively.The first parse corresponds to the action sequence[SHIFT, SHIFT, REDUCE-R-NP, FINISH], whilethe second parse corresponds to the action se-quence [SHIFT, SHIFT, UNARY-NP, REDUCE-L-VP, FINISH], which consists of one more actionthan the first case.
In practice, variances betweenstate items can be much larger than the chosen ex-ample.
In the extreme case where a state item doesnot contain any unary action, the number of ac-tions is 2n, where n is the number of words inthe sentence.
On the other hand, if the maximumnumber of consequent unary actions is 2 (Sagaeand Lavie, 2005; Zhang and Clark, 2009), then themaximum number of actions a state item can haveis 4n.The significant variance in the number of ac-tions N can have an impact on the linear sepa-rability of state items, for which the feature vec-tors are?Ni=1 ?
(ai).
This turns out to have a sig-nificant empirical influence on perceptron trainingwith early-update, where the training of the modelinteracts with search (Daume III, 2006).One way of improving the comparability ofstate items is to reduce the differences in theirsizes, and we use a padding method to achievethis.
The idea is to extend the set of actions byadding an IDLE action, so that completed stateitems can be further expanded using the IDLE ac-tion.
The action does not change the state itself,but simply adds to the number of actions in thesequence.
A feature vector is extracted for theIDLE action according to the final state context,in the same way as other actions.
Using the IDLEaction, the transition sequence for the two parsesin Figure 2 can be [SHIFT, SHIFT, REDUCE-NP, FINISH, IDLE] and [SHIFT, SHIFT, UNARY-NP, REDUCE-L-VP, FINISH], respectively.
Their436Axioms [?, 0, false, 0, 0]Goal [S, n, true, m : 2n ?
m ?
4n, C]Inference Rules:[S, i, false, k,c]SHIFT [S|w, i + 1, false, k + 1, c + cs][S|s1s0, i, false, k, c]REDUCE-L/R-X [S|X, i, false, k + 1, c + cr][S|s0, i, false, k, c]UNARY-X [S|X, i, false, k + 1, c + cu][S, n, false, k, c]FINISH [S, n, true, k + 1, c + cf ][S,n, true, k, c]IDLE [S, n, true, k + 1, c + ci]Figure 3: Deductive system of the extended tran-sition system.corresponding feature vectors have about the samesizes, and are more linearly separable.
Note thatthere can be more than one action that are paddedto a sequence of actions, and the number of IDLEactions depends on the size difference between thecurrent action sequence and the largest action se-quence without IDLE actions.Given this extension, the deduction system isshown in Figure 3.
We add the number of actionsk to an item.
The initial item (Axioms) has k = 0,while the goal item has 2n ?
k ?
4n.
Given thisprocess, beam-search decoding can be made sim-pler than that of Zhang and Clark (2009).
Whilethey used a candidate output to record the bestcompleted state item, and finish decoding whenthe agenda contains no more items, we can sim-ply finish decoding when all items in the agendaare completed, and output the best state item inthe agenda.
With this new transition process, weexperimented with several extended features,andfound that the templates in Table 2 are useful toimprove the accuracies further.
Here sill denotesthe left child of si?s left child.
Other notations canbe explained in a similar way.4 Semi-supervised Parsing with LargeDataThis section discusses how to extract informa-tion from unlabeled data or auto-parsed data tofurther improve shift-reduce parsing accuracies.We consider three types of information, includings0llwc, s0lrwc, s0luwcs0rlwc, s0rrwc, s0ruwcs0ulwc, s0urwc, s0uuwcs1llwc, s1lrwc, s1luwcs1rlwc, s1rrwc, s1ruwcTable 2: New features for the extended parser.paradigmatic relations, dependency relations, andstructural relations.
These relations are capturedby word clustering, lexical dependencies, and adependency language model, respectively.
Basedon the information, we propose a set of novel fea-tures specifically designed for shift-reduce con-stituent parsing.4.1 Paradigmatic Relations: WordClusteringWord clusters are regarded as lexical intermedi-aries for dependency parsing (Koo et al, 2008)and POS tagging (Sun and Uszkoreit, 2012).
Weemploy the Brown clustering algorithm (Liang,2005) on unannotated data (word segmentation isperformed if necessary).
In the initial state of clus-tering, each word in the input corpus is regardedas a cluster, then the algorithm repeatedly mergespairs of clusters that cause the least decrease inthe likelihood of the input corpus.
The clusteringresults are a binary tree with words appearing asleaves.
Each cluster is represented as a bit-stringfrom the root to the tree node that represents thecluster.
We define a function CLU(w) to return thecluster ID (a bit string) of an input word w.4.2 Dependency Relations: LexicalDependenciesLexical dependencies represent linguistic relationsbetween words: whether a word modifies anotherword.
The idea of exploiting lexical dependencyinformation from auto-parsed data has been ex-plored before for dependency parsing (Chen et al,2009) and constituent parsing (Zhu et al, 2012).To extract lexical dependencies, we first run thebaseline parser on unlabeled data.
To simplifythe extraction process, we can convert auto-parsedconstituency trees into dependency trees by usingPenn2Malt.
2 From the dependency trees, we ex-tract bigram lexical dependencies ?w1, w2, L/R?where the symbol L (R) means that w1 (w2) is thehead of w2 (w1).
We also extract trigram lexical2http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html437dependencies ?w1, w2, w3, L/R?, where L meansthat w1 is the head of w2 and w3, meanwhile w2and w3 are required to be siblings.Following the strategy of Chen et al (2009),we assign categories to bigram and trigram itemsseparately according to their frequency counts.Specifically, top-10% most frequent items are as-signed to the category of High Frequency (HF);otherwise if an item is among top 20%, we assignit to the category of Middle Frequency (MF); oth-erwise the category of Low Frequency (LF).
Here-after, we refer to the bigram and trigram lexicaldependency lists as BLD and TLD, respectively.4.3 Structural Relations: DependencyLanguage ModelThe dependency language model is proposed byShen et al (2008) and is used as additional in-formation for graph-based dependency parsing inChen et al (2012).
Formally, given a depen-dency tree y of an input sentence x, we candenote by H(y) the set of words that have atleast one dependent.
For each xh ?
H(y), wehave a corresponding dependency structure Dh =(xLk, .
.
.
xL1, xh, xR1, .
.
.
, xRm).
The probabilityP (Dh) is defined to beP (Dh) = PL(Dh) ?
PR(Dh)where PL(Dh) can be in turn defined as:PL(Dh) ?
P (xL1|xh)?P (xL2|xL1, xh)?
.
.
.
?P (xLk|xLk?1, .
.
.
, xLk?N+1, xh)PR(Dh) can be defined in a similar way.We build dependency language models on auto-parsed data.
Again, we convert constituency treesinto dependency trees for the purpose of simplic-ity.
From the dependency trees, we build a bigramand a trigram language model, which are denotedby BLM and TLM, respectively.
The followingare the templates of the records of the dependencylanguage models.
(1) ?xLi, xh, P (xLi|xh)?
(2) ?xRi, xh, P (xRi|xh)?
(3) ?xLi, xLi?1, xh, P (xLi|xLi?1, xh)?
(4) ?xRi, xRi?1, xh, P (xRi|xRi?1, xh)?Here the templates (1) and (2) belong to BLMand the templates (3) and (4) belong to TLM.
ToStat Train Dev Test UnlabeledEN # sent 39.8k 1.7k 2.4k 3,139.1k# word 950.0k 40.1k 56.7k 76,041.4kCH # sent 18.1k 350 348 11,810.7k# word 493.8k 8.0k 6.8k 269,057.2kTable 4: Statistics on sentence and word numbersof the experimental data.use the dependency language models, we employa map function ?
(r) to assign a category to eachrecord r according to its probability, as in Chen etal.
(2012).
The following is the map function.?
(r) =????
?HP if P (r) ?
top?10%MP else if P (r) ?
top?30%LP otherwise4.4 Semi-supervised FeaturesWe design a set of features based on the infor-mation extracted from auto-parsed data or unan-notated data.
The features are summarized in Ta-ble 3.
Here CLU returns a cluster ID for a word.The functions BLDl/r(?
), TLDl/r(?
), BLMl/r(?
),and TLMl/r(?)
check whether a given word com-bination can be found in the corresponding lists.For example, BLDl(s1w, s0w) returns a categorytag (HF, MF, or LF) if ?s1w, s0w,L?
exits in thelist BLD, else it returns NONE.5 Experiments5.1 Set-upLabeled English data employed in this paper werederived from the Wall Street Journal (WSJ) corpusof the Penn Treebank (Marcus et al, 1993).
Weused sections 2-21 as labeled training data, section24 for system development, and section 23 for fi-nal performance evaluation.
For labeled Chinesedata, we used the version 5.1 of the Penn ChineseTreebank (CTB) (Xue et al, 2005).
Articles 001-270 and 440-1151 were used for training, articles301-325 were used as development data, and arti-cles 271-300 were used for evaluation.For both English and Chinese data, we used ten-fold jackknifing (Collins, 2000) to automaticallyassign POS tags to the training data.
We found thatthis simple technique could achieve an improve-ment of 0.4% on English and an improvement of2.0% on Chinese.
For English POS tagging, weadopted SVMTool, 3 and for Chinese POS tagging3http://www.lsi.upc.edu/?nlp/SVMTool/438Word Cluster FeaturesCLU(s1w) CLU(s0w) CLU(q0w)CLU(s1w)s1t CLU(s0w)s0t CLU(q0w)q0wLexical Dependency FeaturesBLDl(s1w, s0w) BLDl(s1w, s0w)?s1t?s0t BLDr(s1w, s0w)BLDr(s1w, s0w)?s1t?s0t BLDl(s1w, q0w)?s1t?q0t BLDl(s1w, q0w)BLDr(s1w, q0w) BLDr(s1w, q0w)?s1t?q0t BLDl(s0w, q0w)BLDl(s0w, q0w)?s0t?q0t BLDr(s0w, q0w)?s0t?q0t BLDr(s0w, q0w)TLDl(s1w, s1rdw, s0w) TLDl(s1w, s1rdw, s0w)?s1t?s0t TLDr(s1w, s0ldw, s0w)TLDr(s1w, s0ldw, s0w)?s1t?s0t TLDl(s0w, s0rdw, q0w)?s0t?q0t TLDl(s0w, s0rdw, q0w)TLDr(s0w,NONE, q0w) TLDr(s0w,NONE, q0w)?s0t?q0tDependency Language Model FeaturesBLMl(s1w, s0w) BLMl(s1w, s0w)?s1t?s0t BLMr(s1w, s0w)BLMr(s1w, s0w)?s1t?s0t BLMl(s0w, q0w) BLMl(s0w, q0w)?s0t?q0tBLMr(s0w, q0w)?s0t?q0t BLMr(s0w, q0w) TLMl(s1w, s1rdw, s0w)TLMl(s1w, s1rdw, s0w)?s1t?s0t TLMr(s1w, s0ldw, s0w) TLMr(s1w, s0ldw, s0w)?s1t?s0tTable 3: Semi-supervised features designed on the base of word clusters, lexical dependencies, anddependency language models.
Here the symbol si denotes a stack item, qi denotes a queue item, wrepresents a word, and t represents a POS tag.Lan.
System LR LP F1ENG Baseline 88.4 88.7 88.6+padding 88.8 89.5 89.1+features 89.0 89.7 89.3CHN Baseline 85.6 86.3 86.0+padding 85.5 87.2 86.4+features 85.5 87.6 86.5Table 5: Experimental results on the English andChinese development sets with the padding tech-nique and new supervised features added incre-mentally.we employed the Stanford POS tagger.
4We took the WSJ articles from the TIPSTERcorpus (LDC93T3A) as unlabeled English data.
Inaddition, we removed from the unlabeled Englishdata the sentences that appear in the WSJ corpusof the Penn Treebank.
For unlabeled Chinese data,we used Chinese Gigaword (LDC2003T09), onwhich we conducted Chinese word segmentationby using a CRF-based segmenter.
Table 4 summa-rizes data statistics on sentence and word numbersof the data sets listed above.We used EVALB to evaluate parser perfor-mances, including labeled precision (LP), labeledrecall (LR), and bracketing F1.
5 For significancetests, we employed the randomized permutation-based tool provided by Daniel Bikel.
6In both training and decoding, we set the beamsize to 16, which achieves a good tradeoff be-tween efficiency and accuracy.
The optimal iter-ation number of perceptron learning is determined4http://nlp.stanford.edu/software/tagger.shtml5http://nlp.cs.nyu.edu/evalb6http://www.cis.upenn.edu/?dbikel/software.html#comparatorLan.
Features LR LP F1ENG +word cluster 89.3 90.0 89.7+lexical dependencies 89.7 90.3 90.0+dependency LM 90.0 90.6 90.3CHN +word cluster 85.7 87.5 86.6+lexical dependencies 87.2 88.6 87.9+dependency LM 87.2 88.7 88.0Table 6: Experimental results on the English andChinese development sets with different types ofsemi-supervised features added incrementally tothe extended parser.on the development sets.
For word clustering, weset the cluster number to 50 for both the Englishand Chinese experiments.5.2 Results on Development SetsTable 5 reports the results of the extended parser(baseline + padding + supervised features) on theEnglish and Chinese development sets.
We inte-grated the padding method into the baseline parser,based on which we further incorporated the super-vised features in Table 2.
From the results we findthat the padding method improves the parser accu-racies by 0.5% and 0.4% on English and Chinese,respectively.
Incorporating the supervised featuresin Table 2 gives further improvements of 0.2% onEnglish and 0.1% on Chinese.Based on the extended parser, we experimenteddifferent types of semi-supervised features byadding the features incrementally.
The results areshown in Table 6.
By comparing the results in Ta-ble 5 and the results in Table 6 we can see that thesemi-supervised features achieve an overall im-provement of 1.0% on the English data and an im-439Type Parser LR LP F1SIRatnaparkhi (1997) 86.3 87.5 86.9Collins (1999) 88.1 88.3 88.2Charniak (2000) 89.5 89.9 89.5Sagae & Lavie (2005)?
86.1 86.0 86.0Sagae & Lavie (2006)?
87.8 88.1 87.9Baseline 90.0 89.9 89.9Petrov & Klein (2007) 90.1 90.2 90.1Baseline+Padding 90.2 90.7 90.4Carreras et al (2008) 90.7 91.4 91.1RE Charniak & Johnson (2005) 91.2 91.8 91.5Huang (2008) 92.2 91.2 91.7SEZhu et al (2012)?
90.4 90.5 90.4Baseline+Padding+Semi 91.1 91.5 91.3Huang & Harper (2009) 91.1 91.6 91.3Huang et al (2010)?
91.4 91.8 91.6McClosky et al (2006) 92.1 92.5 92.3Table 7: Comparison of our parsers and relatedwork on the English test set.
?
Shift-reduceparsers.
?
The results of self-training with a sin-gle latent annotation grammar.Type Parser LR LP F1SICharniak (2000)?
79.6 82.1 80.8Bikel (2004)?
79.3 82.0 80.6Baseline 82.1 83.1 82.6Baseline+Padding 82.1 84.3 83.2Petrov & Klein (2007) 81.9 84.8 83.3RE Charniak & Johnson (2005)?
80.8 83.8 82.3SE Zhu et al (2012) 80.6 81.9 81.2Baseline+Padding+Semi 84.4 86.8 85.6Table 8: Comparison of our parsers and relatedwork on the test set of CTB5.1.?
Huang (2009)adapted the parsers to Chinese parsing on CTB5.1.?
We run the parser on CTB5.1 to get the results.provement of 1.5% on the Chinese data.5.3 Final ResultsHere we report the final results on the English andChinese test sets.
We compared the final resultswith a large body of related work.
We grouped theparsers into three categories: single parsers (SI),discriminative reranking parsers (RE), and semi-supervised parsers (SE).
Table 7 shows the com-parative results on the English test set and Table 8reports the comparison on the Chinese test set.From the results we can see that our extendedparser (baseline + padding + supervised features)outperforms the Berkeley parser by 0.3% on En-glish, and is comparable with the Berkeley parseron Chinese (?0.1% less).
Here +padding meansthe padding technique and the features in Table 2.After integrating semi-supervised features, theparsing accuracy on English is improved to 91.3%.We note that the performance is on the same levelParser #Sent/SecondRatnaparkhi (1997) UnkCollins (1999) 3.5Charniak (2000) 5.7Sagae & Lavie (2005)?
3.7?Sagae & Lavie (2006)?
2.2?Petrov & Klein (2007) 6.2Carreras et al (2008) UnkThis PaperBaseline 100.7Baseline+Padding 89.5Baseline+Padding+Semi 46.8Table 9: Comparison of running times on the En-glish test set, where the time for loading modelsis excluded.
?
The results of SVM-based shift-reduce parsing with greedy search.
?
The results ofMaxEnt-based shift-reduce parser with best-firstsearch.
?
Times reported by authors running ondifferent hardware.as the performance of self-trained parsers, exceptfor McClosky et al (2006), which is based on thecombination of reranking and self-training.
OnChinese, the final parsing accuracy is 85.6%.
Toour knowledge, this is by far the best reported per-formance on this data set.The padding technique, supervised features,and semi-supervised features achieve an overallimprovement of 1.4% over the baseline on En-glish, which is significant on the level of p <10?5.
The overall improvement on Chinese is3.0%, which is also significant on the level ofp < 10?5.5.4 Comparison of Running TimeWe also compared the running times of our parserswith the related single parsers.
We ran timing testson an Intel 2.3GHz processor with 8GB mem-ory.
The comparison is shown in Table 9.
Fromthe table, we can see that incorporating semi-supervised features decreases parsing speed, butthe semi-supervised parser still has the advantageof efficiency over other parsers.
Specifically, thesemi-supervised parser is 7 times faster than theBerkeley parser.
Note that Sagae & Lavie (2005)and Sagae & Lavie (2006) are also shift-reduceparsers, and their running times were evaluated ondifferent hardwares.
In practice, the running timesof the shift-reduce parsers should be much shorterthan the reported times in the table.5.5 Error AnalysisWe conducted error analysis for the three sys-tems: the baseline parser, the extended parser with44086889092941 2 3 4 5 6 7 8FScoreSpan LengthBaselineExtendedSemi-supervisedFigure 5: Comparison of parsing accuracies ofthe baseline, extended parser, and semi-supervisedparsers on spans of different lengths.the padding technique, and the semi-supervisedparser, focusing on the English test set.
The analy-sis was performed in four dimensions: parsing ac-curacies on different phrase types, on constituentsof different span lengths, on different sentencelengths, and on sentences with different numbersof unknown words.5.5.1 Different Phrase TypesTable 10 shows the parsing accuracies of the base-line, extended parser, and semi-supervised parseron different phrase types.
Here we only considerthe nine most frequent phrase types in the Englishtest set.
In the table, the phrase types are orderedfrom left to right in the descending order of theirfrequencies.
We also show the improvements ofthe semi-supervised parser over the baseline parser(the last row in the table).
As the results show, theextended parser achieves improvements on mostof the phrase types with two exceptions: Preposi-tion Prase (PP) and Quantifier Phrase (QP).
Semi-supervised features further improve parsing accu-racies over the extended parser (QP is an excep-tion).
From the last row, we can see that improve-ments of the semi-supervised parser over the base-line on VP, S, SBAR, ADVP, and ADJP are abovethe average improvement (1.4%).5.5.2 Different Span LengthsFigure 5 shows a comparison of the three parserson spans of different lengths.
Here we considerspan lengths up to 8.
As the results show, boththe padding extension and semi-supervised fea-tures are more helpful on relatively large spans:the performance gaps between the three parsersare enlarged with increasing span lengths.8284868890929410 20 30 40 50 60 70FScoreSentence LengthBaselineExtendedSemi-supervisedFigure 6: Comparison of parsing accuracies ofthe baseline, extended parser, and semi-supervisedparser on sentences of different lengths.5.5.3 Different Sentence LengthsFigure 6 shows a comparison of parsing accura-cies of the three parsers on sentences of differentlengths.
Each number on the horizontal axis repre-sents the sentences whose lengths are between thenumber and its previous number.
For example, thenumber 30 refers to the sentences whose lengthsare between 20 and 30.
From the results we cansee that semi-supervised features improve parsingaccuracy on both short and long sentences.
Thepoints at 70 are exceptions.
In fact, sentences withlengths between 60 and 70 have only 8 instances,and the statistics on such a small number of sen-tences are not reliable.5.5.4 Different Numbers of Unknown WordsFigure 4 shows a comparison of parsing accura-cies of the baseline, extended parser, and semi-supervised parser on sentences with different num-bers of unknown words.
As the results show,the padding method is not very helpful on sen-tences with large numbers of unknown words,while semi-supervised features help significantlyon this aspect.
This conforms to the intuition thatsemi-supervised methods reduce data sparsenessand improve the performance on unknown words.6 ConclusionIn this paper, we addressed the problem of dif-ferent action-sequence lengths for shift-reducephrase-structure parsing, and designed a set ofnovel non-local features to further improve pars-ing.
The resulting supervised parser outperformsthe Berkeley parser, a state-of-the-art chart parser,in both accuracies and speeds.
In addition, we in-corporated a set of semi-supervised features.
The441System NP VP S PP SBAR ADVP ADJP WHNP QPBaseline 91.9 90.1 89.8 88.1 85.7 84.6 72.1 94.8 89.3Extended 92.1 90.7 90.2 87.9 86.6 84.5 73.6 95.5 88.6Semi-supervised 93.2 92.0 91.5 89.3 88.2 86.8 75.1 95.7 89.1Improvements +1.3 +1.9 +1.7 +1.2 +2.5 +2.2 +3.0 +0.9 -0.2Table 10: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parserson different phrase types.0 1 2 3 4 5 6 770809010091.9889.7388.8787.9685.9583.781.4282.7492.1790.5389.5187.9988.6687.3383.8980.4992.8891.2690.4389.8890.3586.39 90.6890.24F-score(%)Baseline Extended Semi-supervisedFigure 4: Comparison of parsing accuracies of the baseline, extended parser, and semi-supervised parseron sentences of different unknown words.final parser reaches an accuracy of 91.3% on En-glish and 85.6% on Chinese, by far the best re-ported accuracies on the CTB data.AcknowledgementsWe thank the anonymous reviewers for their valu-able comments.
Yue Zhang and Muhua Zhuwere supported partially by SRG-ISTD-2012-038from Singapore University of Technology and De-sign.
Muhua Zhu and Jingbo Zhu were fundedin part by the National Science Foundation ofChina (61073140; 61272376), Specialized Re-search Fund for the Doctoral Program of HigherEducation (20100042110031), and the Fundamen-tal Research Funds for the Central Universities(N100204002).
Wenliang Chen was funded par-tially by the National Science Foundation of China(61203314).ReferencesDaniel M. Bikel.
2004.
On the parameter spaceof generative lexicalized statistical parsing models.Ph.D.
thesis, University of Pennsylvania.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Pro-ceedings of EMNLP, pages 12?14, Jeju Island, Ko-rea.Xavier Carreras, Michael Collins, and Terry Koo.2008.
Tag, dynamic programming, and the percep-tron for efficient, feature-rich parsing.
In Proceed-ings of CoNLL, pages 9?16, Manchester, England.Eugune Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of ACL, pages 173?180.Eugune Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of NAACL, pages132?139, Seattle, Washington, USA.Wenliang Chen, Junichi Kazama, Kiyotaka Uchimoto,and Kentaro Torisawa.
2009.
Improving depen-dency parsing with subtrees from auto-parsed data.In Proceedings of EMNLP, pages 570?579, Singa-pore.Wenliang Chen, Min Zhang, and Haizhou Li.
2012.Utilizing dependency language models for graph-based dependency.
In Proceedings of ACL, pages213?222, Jeju, Republic of Korea.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of ACL, Stroudsburg, PA, USA.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings ofACL, Madrid, Spain.Michael Collins.
1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Michael Collins.
2000.
Discriminative rerankingfor natural language processing.
In Proceedings ofICML, pages 175?182, Stanford, CA, USA.Hal Daume III.
2006.
Practical Structured Learn-ing for Natural Language Processing.
Ph.D. thesis,USC.Zhongqiang Huang and Mary Harper.
2009.
Self-training PCFG grammars with latent annotations442across languages.
In Proceedings of EMNLP, pages832?841, Singapore.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProceedings of ACL, pages 1077?1086, Uppsala,Sweden.Zhongqiang Huang, Mary Harper, and Slav Petrov.2010.
Self-training with products of latent variablegrammars.
In Proceedings of EMNLP, pages 12?22,Massachusetts, USA.Liang Huang.
2008.
Forest reranking: discriminativeparsing with non-local features.
In Proceedings ofACL, pages 586?594, Ohio, USA.Liang-Ya Huang.
2009.
Improve Chinese parsing withMax-Ent reranking parser.
In Master Project Re-port, Brown University.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of ACL.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceed-ings of ICML, pages 282?289, Massachusetts, USA,June.Percy Liang.
2005.
Semi-supervised learning for nat-ural language.
Master?s thesis, Massachusetts Insti-tute of Technology.Mitchell P. Marcus, Beatrice Santorini, and Mary A.Marcinkiewiz.
1993.
Building a large anno-tated corpus of English.
Computational Linguistics,19(2):313?330.David McClosky, Eugene Charniak, and Mark John-son.
2006.
Effective self-training for parsing.
InProceedings of the HLT/NAACL, Main Conference,pages 152?159, New York City, USA, June.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of ACL, pages 91?98, Ann Arbor, Michigan, June.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.Maltparser: a data-driven parser-generator for de-pendency parsing.
In Proceedings of LREC, pages2216?2219.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Proceedings ofHLT/NAACL, pages 404?411, Rochester, New York,April.Adwait Ratnaparkhi.
1997.
A linear observed time sta-tistical parser based on maximum entropy models.In Proceedings of EMNLP, Rhode Island, USA.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of IWPT, pages 125?132, Vancouver, Canada.Kenji Sagae and Alon Lavie.
2006.
Parser combina-tion by reparsing.
In Proceedings of HLT/NAACL,Companion Volume: Short Papers, pages 129?132,New York, USA.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL, pages 577?585, Ohio, USA.Weiwei Sun and Hans Uszkoreit.
2012.
Capturingparadigmatic and syntagmatic lexical relations: to-wards accurate Chinese part-of-speech tagging.
InProceedings of ACL, Jeju, Republic of Korea.Nianwen Xue, Fei Xia, Fu dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese Treebank: phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of IWPT, pages 195?206,Nancy, France.Yue Zhang and Stephen Clark.
2008.
Joint word seg-mentation and POS tagging using a single percep-tron.
In Proceedings of ACL/HLT, pages 888?896,Columbus, Ohio.Yue Zhang and Stephen Clark.
2009.
Transition-basedparsing of the Chinese Treebank using a global dis-criminative model.
In Proceedings of IWPT, Paris,France, October.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of ACL, pages 188?193, Portland, Ore-gon, USA.Muhua Zhu, Jingbo Zhu, and Huizhen Wang.
2012.Exploiting lexical dependencies from large-scaledata for better shift-reduce constituency parsing.
InProceedings of COLING, pages 3171?3186, Mum-bai, India.443
