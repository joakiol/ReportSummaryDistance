A FORMAL LEX ICON IN THE MEANING-TEXT THEORY(OR HOW TO DO LEX ICA WITH WORDS)Igor A. Mel'6~ukD~partement de linguistiqueUniversit6 de Montr6alC.P.
6128, Succ.
"A"Montreal, P.Q.
H3C 3J7 CanadaAlain Polgu~reOdyssey Research AssociatesPlace Outremont, 1290 Van HorneMontreal, P.Q.
H2V 1K6 CanadaThe goal of this paper is to present a particular type of lexicon, elaborated within a formal theory ofnatural language called Meaning-Text Theory (MTT).
This theory puts strong emphasis on thedevelopment of highly structured lexica.
Computational linguistics does of course recognize theimportance of the lexicon in language processing.
However, MTT probably goes further in this directionthan various well-known approaches within computational linguistics; it assigns to the lexicon a centralplace, so that the rest of linguistic description is supposed to pivot around the lexicon.
It is in this spiritthat MTT views the model of natural language: the Meaning-Text Model, or MTM.
It is believed thata very rich lexicon presenting individual information about iexemes in a consistent and detailed wayfacilitates the general task of computational linguistics by dividing it into two more or less autonomoussubtasks: a linguistic and a computational one.
The MTM lexicon, embodying a vast amount of linguisticinformation, can be used in different computational applications.We will present here a short outline of the lexicon in question as well as of its interaction with othercomponents of the MTM, with special attention to computational implications of the Meaning-TextTheory.1.
LEVELS OF UTTERANCE REPRESENTATION INMEANING-TEXT THEORY AND THE MEANING-TEXTMODEL OF NATURAL LANGUAGE.The goal of the present paper is two-fold:1) To present a specific viewpoint on the role oflexica in "intell igent" systems designed to process textsin natural anguage and based on access to meaning.2) To present a specific format for such a lexicon - -so-called Explanatory Combinatorial Dictionary (ECD).We believe that a rich enough lexicon, which couldenable us to solve the major problem of computationallinguistics - -  that of presenting all necessary informa-tion about natural anguage in compact form, should beanchored in a formal and comprehensive theory oflanguage.
The lexicon to be discussed, that is ECD, hasbeen conceived and developed within the framework ofa particular linguistic theory - -  more specifically, Mean-ing-Text Theory or MTT (Mel'6uk 1974, 1981, 1988:43-101).
Note that this is by no means a theory of howlinguistic knowledge could or should be applied in thecontext of any computational task.
The MTT is a theoryof how to describe and formally present linguisticknowledge, a theory of linguistic description; therefore,its contribution to computational linguistics is only apartial one: to take care exclusively of the linguistic partof the general endeavor.We cannot present here the Meaning-Text Theory indetail, so we will limit ourselves to a brief characteri-zation of the following two aspects, which are ofCopyright 1987 by the Association for Computational Linguistics.
Permission tocopy without fee all or part of this material isgranted providedthat the copies are not made for direct commercial dvantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, or to republish, requires afee and/or specific permission.0362-613X/87/030261-275503.00Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 261Igor Mel'~uk and A~ain Polgu~re A Formal Lexicon in Meaning-Text Theoryparticular elevance to this paper: the system of linguis-tic representations the theory makes use of, and thelinguistic Meaning-Text model which it presupposes.1.1.
UTTERANCE REPRESENTATIONS IN THE MTT.We have to warn our reader that limitations of spaceforce us to have recourse to drastic simplifications,perhaps even too drastic sometimes.
Thus, an MTTutterance representation is in fact a set of formal objectscalled structures, their vocation being the characteriza-tion of separate aspects of the phenomena to be de-scribed.
But in this paper we use the term representa-tion to refer to the main one of the structures whichcompose a representation (since we disregard the otherstructures).In Meaning-Text Theory, an utterance~ is repre-sented at seven levels:1) The Sem(antic) R(epresentation) of utterance U is,roughly speaking, a network which depicts the linguisticmeaning of U without taking into consideration the waythis meaning is expressed in U (distribution of meaningbetween words and constructions, and the like).
Thus aSemR represents in fact the meaning of the whole familyof utterances ynonymous with each other.
2 The nodesof a SemR network are labeled with semantic units, asemantic unit being a specific sense of a lexeme in thelanguage in question.
The arcs of the network arelabeled with distinctive numbers which identify dif-ferent arguments of a predicate.
Thus,a 1 P 2 bO4 O ~0is equivalent to the more familiar notation P( a ,  b ).
Asthe reader can see, our SemR is based on predicate-argument relations (although we do not use the linearnotation of predicate calculus nor predicate calculus assuch).2) The D(eep-)Synt(actic) R(epresentation) of U is,roughly speaking, a dependency tree whose nodes arenot linearly ordered (because linear order is taken to bea means of expressing syntactic structure rather thanbeing part of it).
The nodes of a DSyntR are labeled withmeaningful exemes of U, which are supplied withThe vague term utterance is used on purpose.
In the present paperwe take the sentence as our basic unit, but the MTT is not restrictedto sentences.
In principle, it can deal with sequences of sentences,although up to now, within the MTT, the way to represent suchsequences and the rules to process them have not yet been developed(in contrast with such works as, e.g., McKeown (1985)).2 The term meaning is to be construed here in the narrowest sense--as referring to strictly linguistic meaning, i.e.
the meaning (of utter-ances) which is given to any native speaker just by the mastery of hislanguage.
We say this to avoid a misunderstanding: ourmeaning hasnothing to do with "actual" meaning, which is aimed at by suchquestions as 'What do you mean by that?'
or 'What is the meaning ofthis paper?'
This restricted character of meaning in our interpretationwill become clear when we discuss semantic representation in theM'rT (see below).meaning-bearing morphological values (such as numberin nouns or tense in verbs; in our example below, weindicate such values for the top node only).
Thebranches of a DSyntR carry the names of universalDSynt-relations, which are few in number (less thanten).3) The S(urface-)Synt(actic) R(epresentation) of U isalso a dependency tree of the same formal type but, itsnodes are labeled with all actual exemic occurrences ofU (including all structural words), and branches of itcarry the names of a few dozen specific SSynt-relations,which correspond to the actual syntactic onstructionsof a particular language.The distinction between Deep- and Surface- suble-vels is related to the fact that some syntactic phenom-ena (basically, cooccurrence restrictions) are morelinked to meaning, while other syntactic phenomena(word order, agreement, and the like) are more relevantfrom the viewpoint of actual text.
Phenomena of thefirst type are captured in the DSyntR, which is geared tomeaning, and those of the second type, in the SSyntR,geared to text.In the same vein and with the same purpose, MTTintroduces two sublevels - -  deep vs. surface - -  inmorphology and phonology:4) D(eep-)Morph(ological) R(epresentation).5) S(urface-)Morph(ological) R(epresentation).6) D(eep-)Phon(etic) R(epresentation),or phonological representation.7) S(urface-)Phon(etic) R(epresentation),or phonetic representation proper.We will not justify the Deep- vs. Surface- dichotomyor, more generally, the composit ion and organization ofour set of representation levels.
Instead of this, we willtry to link our somewhat abstract statements to aspecific example.
Namely,  we will quote a Frenchsentence along with its representations on the first threelevels.
We will not consider here the morphological ndphonological representations of this sentence, sincethey are not relevant o our goal.
Note that throughoutthis paper we will use examples borrowed from French(since we had the corresponding information availableonly in that language, while to work out the Englishexamples would require special research, which we arein no position to undertake); but to facilitate the readingof all the examples, we will supply approximate Englishglosses for all French lexical items.Let us consider French sentence (1):(1) Fr.
Alcide (X) a aide Mordecai" (Y) d passer  sonbac (Z) par ses conseils avertis (W)'Alcide helped Mordecai pass his \[high school leav-ing\] exams with his judicious advice'.At the Sem-level, sentence (1) appears as Figure 1:262 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Igor Md'~uk and Alain Polgu~re A Formal Lexicon in Meaning-Text Theory'aider 2' 'avant'\[ he\]4 p 2 \] o \[ before \]1 j .o__  l "~"",-......~y !
oa iiii'conseils avertis' 'concerner'\[ judicious advice \] \[ concern \]SemR of Sentence (1)Figure 1In this figure, II is a dummy to indicate an unspecifiedmeaning (it is not specified what exactly the advice fromAlcide is).
In principle, every lexemic sense must beidentified by a number; for the sake of simplicity we dothis here for one node only: AIDER 2, which will bediscussed in 2.1.3.
To avoid unnecessary complications,we have grouped certain semantic elements together('passer le bac' and 'conseils avertis').
The same short-cut is used in the next two figures.At the DSynt-level, sentence (1) appears as follows:The subscript "present perfect" on AIDER 2 corre-sponds, in the SemR, to the subnetwork 'before now':this is roughly the meaning of the French presentperfect (called "pass6 compos6").
During the transitionto the SSynt-level, this subscript riggers a DSynt-rulethat introduces the appropriate auxiliary verb (in ourcase, AVOIR 'have') along with the auxiliary SSynt-relation, linking it to the lexical verb (i.e., AIDER 2) inthe participial form: see Fig.
3.
Note that the firstDSynt-dependent of the verb AIDER 2 in the DSyntR(ALCIDE) must be linked as the grammatical subject othe AVOIR node in the SSyntR, while all other depen-dents of AIDER 2 remain dependents of its participialform.At the SSynt-level, we get Figure 3:Now we will move to the characterization of theformal device, a set of rules, which ensures the transi-tion between the representations of the above levels:the Meaning-Text Model (MTM).1.2.
THE MEANING-TEXT MODELThe MTM is nothing else but what is currently calledgrammar (we avoid this usage because we would like todistinguish and even contrast grammar vs. lexicon, bothbeing parts of a linguistic model).
More specifically, it isALDER Z\[ help 2 \] present perfect0o w'" o ~ \ SES CO'E lLS  AYERTISALCI DE MORDECA'I" ~ ,  \[ his judicious advice \]0PASSER SON BAC\[ pass his exams \]DSyntR of Sentence (I)Figure 2Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 263Igor Mei'~uk and Ahdn Polgu~re A Formal Lexicon in Meaning-Text TheoryAVOIR \[ have \] presentopred~ ~aux i l i a rWo ~""~o AIDER 2ALCIDE direct ~ / ~ \ [  help 2 \]?bjectivet J 1 eo=J""  indirect / NNNx~bl'quobjective 'NHORDECA'I" !
k NX~o PAR/prepositional /oPASSER SON BACprepositionaloSES CONSEIES AVERTIS\[ his judicious advice \]\[ pass his exams \]SSyntR of Sentence (1)Figure 3a set of formal rules, with complex internal organiza-tion, which, so to speak, translate the initial SemR intoa final SPhonR (or into a written text) and vice versa.
Ofcourse, in doing so, the rules pass through all interme-diate representations.
Therefore the MTM is subdividedinto components uch that each one deals with thecorrespondence b tween two adjacent levels n and n +1; given seven levels of representation, there are sixcomponents:1) The Semantic Component ( ransition between SemRand DSyntR);2) The Deep-Syntactic Component ( ransition betweenDSyntR and SSyntR);3) The Surface-Syntactic Component (transition be-tween SSyntR and DMorphR);4) The Deep-Morphological Component ( ransition be-tween DMorphR and SMorphR);5) The Surface-Morphological Component (transitionbetween SMorphR and DPhonR);6) The Deep-Phonetic Component (transition betweenDPhonR and SPhonR).Each component has a roughly identical internal struc-ture; namely, it contains three types of rules:- -  well-formedness rules for representations of thesource level;- -  well-formedness rules for representations of thetarget level;- -  transition rules proper.The well-formedness rules serve simultaneously both tocheck the correctness of the representation in questionand to contol the application of the transition rules.
Letit be emphasized that until now the MTM (and the MTTin general) does not contain the data necessary toeffectively carry out the said control; thus the develop-ment and organization of these data constitutes, from acomputational viewpoint, the main problem in the ap-plication of the MTT.To sum up, the synthesis of a sentence appears in theMeaning-Text framework as a series of subsequenttransitions, or translations, from one representation tothe next one, beginning with SemR; the analysis takesof course the opposite direction, starting with theSPhonR or with the written text.
This is, however, onlya logical description of what happens.
In a real compu-tational implementation, it is often necessary, in orderto take a decision concerning a particular level ofrepresentation, to consider several other levels simulta-neously.As the reader has probably realized, the MTM be-longs to so-called stratificational models of language,launched about a quarter of a century ago (Lamb 1966,Sgall 1967, Mel'ruk 1974); note that at present we cansee a clear tendency to introduce certain ideas of thestratificational pproach into theoretical and computa-tional linguistics (compare, e.g., the distinction of sev-264 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Igor Mel'~uk and Alain Polgu~re A Formal Lexicon in Meaning-Text Theoryeral levels of linguistic representation i Bresnan'sLexical Functional Grammar).Since our goal in the paper is to discuss the role of aformal lexicon in a linguistic model, and, as we will tryto show, our formal lexicon is a part of the semanticcomponent (of the MTM), we will concentrate only onthis portion of the model.2.
EXPLANATORY COMBINATORIAL DICTIONARY.A lexicographic unit in the ECD 3, i.e.
a dictionaryentry, covers one lexical item - -  a word or a set phrase- -  taken in one well-specified sense.
All such itemscalled lexemes (respectively, phrasemes) are describedin a rigorous and uniform way, so that a dictionary entryis divided into three major zones: the semantic zone, thesyntactic zone, and the lexical cooccurrence zone.
(Weleave out of consideration all other zones, as irrelevantto the main purpose of this paper.
)2.1 .
SEMANTIC ZONE.2.1.1.
THE LEXICOGRAPHIC DEFINITION AS THE BASIS OFAN ECD ENTRY.An ECD entry is centered around the definition of thehead word, i.e.
the representation f its meaning, or itsSemR.
All particularities of the ECD, as far as thesemantic domain is concerned, follow from the fact thatmeaning is taken to be the first and foremost motivationfor everything else in the dictionary: relevant propertiesof lexical items are discovered and described contingentupon their definitions.
In order to make this importantproperty more obvious, let us compare the MTT ap-proach to the lexicon with certain current approachesknown in computational linguistics.
The approaches wemean here are rather syntax-motivated, so that theyview the lexicon as an appendix to the grammar, thelatter being equated with the formulation of syntacticwell-formedness.
In MTT, it is exactly the opposite: thegrammar is considered to be an appendix to the lexicon,an appendix which, on the one hand, expresses usefulgeneralizations over the lexicon, and on the other hand,embodies all procedures necessary for manipulatinglexical data.
As we have said, the lexicon and thegrammar taken together constitute the MT model, withthe lexicon at its foundations.ECD definitions possess, among other things, thefollowing two properties, which distinguish them from3 Let it be emphasized that the abstract concept of an ECD aspresented below is by no means the same thing as an actual ECDimplemented in a specific form, such as, e.g., Mel'~uk et al (1984) andMel'~uk and Zholkovsky (1984).
The particular concept of ECDwhich we propose here is no more than a logical constraint on aninfinity of possible ways to build concrete ECDs.
On the one hand, weinsist on the concept of ECD only, without touching upon the methodsof its realization; on the other hand, the actual ECDs of Russian andFrench, mentioned above, are not well adapted, under their presentform, to computational treatment.the definitions of many lexica used in computationallinguistics:a) An ECD definition must be adequate in the sensethat all possible 4 correct usages of the lexeme definedare covered by it and all incorrect usages are excluded.In other terms, all the components of an ECD definitionare necessary and the set of these is sufficient for thetask just stated.b) In various computational or formal approaches,lexicographic definitions are written in terms of a smallset of prefabricated elements.
It looks as if the resear-cher's goal were to make his definitional language asdifferent as possible from his object language (cf.
con-ceptual dependencies in Schank (1972), logical repre-sentations in Dowty (1979), and the like).
One majordrawback of this type of approach is that it does notprovide for a direct and explicit expression of lexicalcohesion in the language under analysis: possible rela-tionships among lexical items have to be inferred fromtheir definitions in a very indirect way.
In sharp contrastto this, the ECD defines a lexeme L of language i_ interms of other lexemes LI, L 2 .
.
.
.
.
L n of t. in such away that the meaning of an Li is simpler than that of L,which precludes circularity (for further discussion, see2.1.2.a).
As a result, in the ECD, semantic relationshipsamong lexemes are established and explicitly stateddirectly via their definitions.2.1.2.
FORMAL CHARACTER OF AN ECD DEFINITION.An ECD definition is a decomposition of the meaning ofthe corresponding lexeme.
It is a semantic networkwhose nodes are labeled either with semantic units(actually, lexemes) of I.., or with variables, and whosearcs are labeled with distinctive numbers which identifydifferent arguments of a predicate.
A lexical label rep-resents the definition (the meaning) of the correspond-ing lexeme, rather than the lexeme itself.
Therefore,each node of a definitional network stands, in its turn,for another network, whose nodes are replaceable bytheir corresponding networks, and so forth, until thebottom level primitives are reached.
For practical rea-sons, though, one can take as primitives the lexemes ofthe level at which the researcher decides to stop theprocess of decomposing.
This approach is directly re-lated to the pioneering work of Wierzbicka: see, e.g.,Wierzbicka (1980).The elaboration of ECD definitions must satisfy anumber of principles, of which we will mention here thefollowing two:a) The Decomposition Principle requires that a lex-eme be defined in terms of lexemes which are semanti-cally simpler than it; as mentioned above, this precludescircularity.
This principle, if applied consistently, will4 By possible we mean 'possible in any imaginable context' -- withthe obvious exception of contexts involving either the phonetic form(as, e.g., in poetry) or metalinguistic use of lexical items.Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 265Igor Mel'~uk and Aiain Polgu~re A Formal Lexicon in Meaning-Text Theory'emplower'\[ use \]o. , / 2\ 3 ~ 'secour i r  1j \  "~'o \[ succ?ur \]Y' ressources '\[ resources  \]<=>'secourir 2'01 2 3?
Y ZDefinition of the French verb SECOURIR 2What ~?e mean by a (lexicographic) definition of a lexicalmeaning is an equivalence between this lexical meaningitself taken together ~?ith its Sem-actants (the righthandpart of the figure) and its semantic decomposition observingthe Maximal Block Principle (the lefthand part).In English, the ne%cork in the lefthand part can be read as :'X uses Z ~?hich is X's resources  in order to SECOURIR 1 Y'Figure 4lead to a set of semantic primitives.
As is easily seen,we do not begin by postulating a set of semanticprimitives: we hope to have them discovered by a longand painstaking process of semantic decompositionapplied to thousands of actual lexical items.
But let it beunderscored that within the MTT framework it is notvital to have semantic primitives at hand in order to beable to proceed successfully with linguistic description.b) The Maximal  Block Principle requires that, withinthe definition of lexeme L, any subnetwork which is thedefinition of L' be replaced by L'; this ensures thegraduality of decomposition, which contributes to mak-ing explicit interlexical links in the language.
In fact thisprinciple forbids writing a definition in terms of seman-tic primitives if semantic units of a higher level areavailable; this makes our definitions immediately grasp-able and more workable.item that covers exactly the meaning of this Frenchverb.
However we believe that this is a good examplebecause it shows how different the lexicalizations of thesame meaning in two different languages can be.
Wehope that our semantic description of SECOURIR willeventually make its meaning clear to the reader.)
SE-COURIR corresponds to two lexemes the meaning ofwhich can be illustrated by the following examples:(2) SECOURIR 1Ce vaccin a secouru de nombreux enfants'This vaccine helped/saved many children'.SECOURIR 2Le mEdecin a secouru de nombreux enfants avec cevaccin'The doctor helped/saved many children with thisvaccine'.2.1.3.
AN EXAMPLE OF THE INTERACTION AMONG ECDDEFINITIONS.To illustrate the way the vocabulary of L is semanticallyhierarchized using ECD definitions, we will considersome definitions involving semantically related lexemes.Let us take the French verb SECOURIR, roughly'succour'.
(Although this English gloss is not a veryfamiliar word in Modern English, we cannot find abetter equivalent, since there is no single English lexicalThere exists an obvious semantic relation between thetwo lexemes: a causative one; thus SECOURIR 2means 'to use something for it to SECOURIR 1 some-one', that is, very roughly, 'to cause something toSECOURIR 1 someone' (the concept of causation isimplicit in 'use').
In a more formal way, we can repre-sent the meaning of SECOURIR 2 by a semanticnetwork which includes SECOURIR 1 (Figure 4).SECOURIR 1, in its turn, can be defined in terms ofAIDER 1, roughly 'help' (as in La lumidre aide les266 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Igor Mel'~uk and Main Polgu~re A Formal Lexicon in Meaning-Text Theory'aider I'\[ help 1 \]0xo-Y 'survivre'\[ survive \]<=>'secourir 1'o1X YDefinition of the French verb SECOURIR 1In English, the network can be read as :' X helps 1 Y to survive, this help1 beingnecessary for Y's survival 'Figure 5plantes dans leur croissance, lit.
'Light helps plants intheir growth'), plus the following two important seman-tic components: 'survive' and 'necessary'.
More specif-ically, X SECOURT 1 Y means 'X AIDE 1 \[=helps 1\] Yto survive, X's helping 1 Y being necessary for Y'ssurvival'.
The node 'secourir 1' in the network of Fig.
4can be replaced by its own definition, i.e.
by thenetwork of Fig.
5, giving Fig.
6.
It is obvious that,generally speaking, the inverse substitution is also pos-sible: a network representing a definition of a lexicalmeaning can be replaced by a single node representingthis meaning.
This type of network manipulation mustbe carried out automatically by substitution proceduresbased on lexico-semantic rules of the form:SemR I <=> MEANING OF LEXEME L I'employer'\[ use \]ol p l  \ ] /'ressources'\[ resources \]'n~cessaire'\[necessary \]2Y 'survivre'\[ survive \]Deeper semantic decomposition for 'X SECOURT 2 Y'(with the definition of SECOURIR 1 substitutedfor the latter)Figure 6Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 267Igor Mel'(:uk and Alain Polgu~re A Formal Lexicon in Meaning-Text TheorySuch a rule is nothing less than the central element of  alexical entry; more precisely, the core of  its semanticzone.
sNote that the same causative relation that existsbetween SECOURIR  1 and SECOURIR  2 exists alsobetween A IDER 1 and A IDER 2 (which appeared in ourfirst example, sentence (1)), as can be illustrated by thedefinition of  A IDER 2 in Fig.
7:2.1.4.
ON INTERPRETING THE SEMR.As we have already said, the SemR in the MTT is aimedat representing the l inguistic meaning, i.e., the commoncore of  all synonymous  utterances; it serves to reducethe enormous synonymy of  natural language to a stan-dard and easily manageable form - -  but it has no directlink with the real world.
Therefore,  the full-fledgeddescription of  linguistic behavior  should include another'moment'\[ t ime \] 'moment'o 1 o \[ time \] 2 - -  1 ~o~'effectuer' = /  T ,~2 'ressources\[ make \] ,oxj~.~.
\[ resources \] \~ ~ d e r 1 ~  ~ / oX /?w~ \[hel 1 \] ~ .
\  1v p i \ ]  /'employer'\[ use \]<->'aider 2'ooY4o' oX Y ZDefinition of the French verb ALDER 2In English, the network can be read as follows:'Y carrying out Z, X uses his resources W in orderfor W to help 1 Y to carry out Z; the use of resourcesby X and the carrying out of Z by Y are simultaneous'.Figure 7To sum up the semantic inclusion relations among thefour lexemes discussed, let us present these in thefollowing obvious form:'secourir  2' includes 'secourir  1', which includes'aider 1', which is included in 'aider 2';at the same time, there is an approximate proportionality:'secour ir  2' / 'secourir  1' - 'aider 2' / 'aider 1'.s In our framework, a (lexicographic) definition is a semantic rulewhich is part of the semantic omponent ofthe MTM.
Semantic rulesof this type serve to reduce aSemR network to a more compact form(if applied from left to right) and/or to expand a SemR network (ifapplied from right to left).
A second type of semantic rule are rulesmatching a lexeme meaning with the corresponding lexeme, i.e.
rulesthat carry out the transition between a SemR and a DSyntR (or viceversa).
Generally speaking, all the rules of the MTM are subdividedinto these two types: (a) rules establishing correspondences betweenunits of the same linguistic level, or equivalence rules (semantic rulesof the type illustrated in this paper in Figs.
4, 5, 7; paraphrasing rules- -  see (9) below); (b) rules establishing correspondences betweenunits of two adjacent linguistic levels, or manifestation rules (e.g.,Fig.
9).
We are in no position to go into further details, but we wouldlike to stake out this important distinction.representation - -  the representation of the state of  affairsin the real world, and another model - -  the Reality-Meaning Model.
The latter ensures the transition betweenthe results of  perception etc.
and the SemR, which is thestarting/end point of  linguistic activity proper.Our exclusion of  real world knowledge from theSemR and therefore f rom the MTM opposes our ap-proach to others, such as, e.g., Montague Grammar,which claims that the rules associating semantic repre-sentations with sentences and the rules interpreting thesame representations in terms of  extralinguistic reality(set-theoretical interpretation) are of  the same natureand can be integrated within the same model.
We insiston distinguishing the SemR proper f rom the real world/knowledge representation because we feel that thedescription of  linguistic activity in the strict sense of  theterm is very different f rom the descript ion of  perceptualor logical activity.To illustrate the relation between a SemR and thecorresponding: real-world representation, we can takethe example of  machine translation.
We think that theonly common denominator  of  two texts (in different268 Computational Linguistics, "Volume 13, Numbers 3-4, July-December 1987Igor Mei'6uk and Alain Poigu~re A Formal Lexicon in Meaning-Text Theorylanguages) equivalent under translation is the represen-tation of the state of affairs referred to, not a SemR ofeither text.
The SemR is not an ideal interlingua - -although it can be effectively used to bring closer thesource and the target languages - -  because a SemR, bydefinition, reflects the idiomatic characteristics of thelanguage in question.2.2.
SYNTACTIC ZONE.This zone stores the data on the syntactic behavior ofthe head lexeme - -  more specifically, on its capacity toparticipate in various syntactic configurations.
Alongwith the part of speech (= syntactic category), thesyntactic zone presents two major types of information:- -  Syntactic features.- -  The government pattern.These two types are distinguished by their bearing onthe semantic actants (= arguments) of the head lexeme.2.2.1.
SYNTACTIC FEATURES.A syntactic feature of lexeme L specifies particularsyntactic structures which accept L but which are notdirectly related to the semantic actants appearing in itsdefinition (though this does not preclude a syntacticfeature from being linked to the meaning of L in adifferent way).
For instance, the feature "geogr"  sin-gles out English common nouns capable of appearing inthe construction2 Detde f + Nlgeogr + of + N proper,having the meaning 'the N 1 called N 2'Such nouns are, e.g., city, island, republic (but notmountain, peninsula, river .
.
.
.
):(3) a. the city of Londonthe island of Borneothe Republic of Polandb.
*the Mountain of Montblanc <the MontblancMountain>*the Peninsula of Kamtchatka <the KamtchatkaPeninsula>*the River of Saint-Lawrence <the Saint-Law-rence River>.Note that "geogr"  will block such an expression as*Mountain of London, but not because London is not amountain; this will happen because a mountain does notsyntact i ca l l y  admit its name in the form of of X. TheLondon Mountain will not be precluded by this featuresince it is syntactically perfectly correct.Syntactic features, which do not presuppose strictlydisjoint sets, provide for a more flexible and multifacet-ted subclassification f lexemes than do parts of speech,which induce a strict partition of the lexical stock.2.2.2.
GOVERNMENT PATTERN.The government pattern of lexeme L specifies thecorrespondence b tween L's semantic actants and theirrealization at the DSynt-level, SSynt-level and DMorph-level.
It is a rectangular matrix with three rows:- -  the upper one contains semantic actants of L(X,Y,Z,...);- -  the middle one indicates the DSynt-roles(I,II,III,...) played by the manifestations of the Sem-actants on the DSynt-level with respect o L;- -  the lower one indicates structural words andmorphological forms necessary for the manifesta-tions of the same Sem-actants on the SSynt- andDMorph-levels contingent on L.The number of columns in this matrix is equal to thenumber of Sem-actants of L. Each column specifies thecorrespondence b tween a Sem-actant and its realiza-tions on closer-to-surface levels.To illustrate, we will use the government pattern ofthe French lexeme AIDER 2, the definition of which hasbeen presented in Fig.
7.
This definition involves foursemantic actants (X, Y, Z and W), all of which areimplemented in sentence (1), see the end of Section 1.1.The government pattern of AIDER 2 has the formshown in Fig.
8.Let us now discuss the way the government patternof a lexeme is used to ensure the appropriate corre-spondence between the representations of adjacentlevels (within the framework of a transition from theSemR toward the sentence).
We will focus on a frag-ment of sentence (1), more specifically, on the cor-respondence between the semantic actant Z of AIDER2 and its syntactic correlates on both syntactic lev-els.In the transition between the SemR of (1) and itsDSyntR (see Figs.
1 and 2), the government pattern ofAIDER 2 establishes that Z is DSynt-actant III of thislexeme.
The correspondence "Z <=> I I I"  is specifiedin the 3rd column of the government pattern (Fig.
8),which will be our working column.The transition between the DSyntR and the SSyntRof (1), Figs.
2 and 3, may be thought of as being carriedout in three steps:1) Since the dependent of the DSynt-relation III,headed by AIDER 2, is a verb, 6 namely PASSER'pass', we select lines 2 (d V) and 5 (pour V) of columnIII (only these two lines imply a verb occurrence).2) We make our choice between lines 2 and 5.
(In thisspecific case, the choice is made according to therestrictions imposed on the government pattern in ques-tion, but not given in this paper.
More specifically, thepreposition pour seems to imply the participation of X,agent of AIDER 2, in activity Z; since, however,6 The indication of the part of speech of a given lexeme, as well as itsgovernment pattern, its syntactic features etc., do not appear in therepresentation itself but are stored in the dictionary entry of thelexeme, which must be made asily available to all computationalprocedures.Computat iona l  L inguist ics ,  Vo lume 13, Numbers  3-4, Ju ly -December  1987 269Igor Mel'~uk and Aiain Polgu~re A Formal Lexicon in Meaning-Text TheoryX Y Z VI II I I I  IVI .N  I .N  1 .
_~N2.~V3.
dans IN4.
pour IN5.
pour V1.
deN2.
avec N3.
par N4.
AdvGovernment pattern of the French verb ALDER 2X, Y, Z and ~/are the semantic actants of ALDER 2 : X is the person whohelps, Y the person who receives help, Z the activity of Y in which heneeds help, and ~# the resources by which X helps Y.I, II, III and IV are the Deep-Syntactic aotants of AIDER 2: I refers tothe noun phrase that expresses X etc.Figure 8PASSER SON BAC 'pass one's exams' is a purelyindividual action, one has to choose the preposition d,line 2.
)3) The selection of the preposition having been made,a DSynt-rule (cf.
Figure 9) introduces (into the SSyntR)the lexical node A, with the concomitant SSynt-relations: "indirect objective" ~ from AIDER 2 to A,and "prepositional" - -  from A to PASSER (SONBAC).This rule provides a general frame for the expressionof DSynt- relations, but it is the government pattern ofa specific lexeme, in this case A = AIDER 2, thatsupplies the specific preposition, in this case PREP = A.Thus a particular government pattern serves to instan-tiate in an appropriate way the variables in certainDSynt-rules.In general, a government pattern has associated withit a number of restrictions concerning the cooccurrenceand the realization of actants:an actant cannot appear together with/withoutanother actant;a given surface form of an actant determines thesurface form of another actant;a given realization of an actant is possible onlyunder given conditions, semantic or otherwise (cf.
therestriction mentioned above, step 2, concerning thechoice between ti and pour);etc.These restrictions function as filters screening possi-ble forms and combinations of actants on the DSynt-, aswell as on the SSynt-level.2.2.3.
A FEW REMARKS ON THE GOVERNMENTPATTERN VS.
THE FEATURE APPROACH.The notion of government pattern, as an element oflexical description, is not in itself revolutionary and ispresent in a number of linguistic theories.
Thus what wecall the government pattern is related to the concept ofsubcategorization i generative grammars.
For in-stance, Generalized Phrase Structure Grammar (Gazdaret al 1985) utilizes in its rules what are called syntacticfeatures, 7 subcategorizing lexical items according to, inour terminology, their government pattern.
GPSG pos-tulates the existence of subcategories, for instanceV\[32\], i.e.
a verb of type 32, etc., which constrain theapplication of grammar ules by selecting the lexicalitems they can deal with.
It seems obvious that it ispossible to make generalizations by grouping certainelements of a single syntactic ategory on the basis ofcertain similarities in their syntactic behavior; we do notbelieve, however, that one has to establish those group-ings according to general syntactic rules rather thanaccording to individual lexicographic descriptions.Tackling the problem from the viewpoint of the lexiconpresents at least two advantages:1) One can describe the syntactic behavior of lexemeL vis h vis its syntactic actants in L's lexical entrywithout having to examine a rather complex system ofsyntactic rules dealing with syntactic features attributedto L. Thus, syntactic features are a code that needsinterpretation i terms of pre-existing syntactic rules; in7 Let it be strongly emphasized that the GPSG concept of syntacticfeature does not correspond to syntactic features as used in the MTT(see 2.2.1 above).270 Computational Linguistics, 'Volume 13, Numbers 3-4, July-December 1987Igor Mel'~uk and Alain Polgu~re A Formal Lexicon in Meaning-Text TheoryIIIA(111 \[PREP\])0< >oBAo (III \[PREP\])l indirect objectivePREPprepositionalo BDSgntR SSyntRA D$ynt-rule for the realization of the DSynt-relation IIIFigure 9contrast to that, a government pattern is a "speaking"expression which explicitly specifies the syntactic mi-cro-structure typical of L - -  independently of anysyntactic rules that might use it.2) One avoids postulating disjoint lexemic classesaccording to syntactic behavior (note that as a rule onehas no means of evaluating, a priori, how many suchclasses there are and what their characteristics wouldbe).We could mention also that a very detailed escrip-tion of lexemes in terms of government patterns allowsone to consider nearly as many distinct syntactic behav-iors (i.e., subclasses) as there are lexical items.
ForFrench, this seems to be what can be inferred from verblists constructed by M. Gross and his associates (Gross1975).2.3.
LEXICAL COMBINATORICS ZONE.2.3.1.
WHAT IS RESTRICTED LEXICAL COOCCURRENCE?The main novelty of the ECD is a systematic descrip-tion of the restricted lexical cooccurrence ofevery headlexeme.
Restricted lexical cooccurrence can be of twoquite different types.The first type is illustrated by the impossible cooc-currence observed in a sentence such as (4):(4) The telephone was drinking the sexy integrals.In (4), certain lexemes cannot cooccur only because oftheir meanings and of our knowledge of the world.
Theexact translation of (4) will be deviant in any language,and this means that the restrictions violated in (4) are ofa non-linguistic nature.
They should not be covered bya linguistic description.In sharp contrast o (4), the phrases in (5) demon-strate the second type of restricted lexical cooccurr-ence, i.e.
what are considered by MTT to be trulylinguistic restrictions on lexical cooccurrence:(5) a.Eng.
(to) ASK a questionFr.
POSER (litt.
'put') une questionSp.
HACER ('make') una preguntaRuss.
ZADAT' ('give') voprosb.Eng.
(to) LET OUT a cryFr.
POUSSER ('push') un criSp.
DAR ('give') un gritoRuss.
ISPUSTIT' ('let out') krikThe selection of the appropriate verb to go with a givennoun cannot be done according to the meaning of thelatter; there is nothing in the meaning of 'question' toexplain why in English you ask it while in Spanish youmake it, in French you put it and in Russian you give it.This type of restricted lexical cooccurrence is the primetarget of lexicographic description, which uses for thispurpose so-called lexical functions (see below).However, we think that our lexicographic descrip-tion has no place for selectional restrictions formulatedin terms of semantic features.
Let us consider anexample of selectional restrictions as used in someapproaches in computational linguistics: McCord's SlotGrammar (McCord 1982), which has the advantage ofgiving, in a sample lexicon for a database questioningsystem, fully explicit information concerning the gov-ernment pattern of each lexical entry.
Thus we have forComputational Linguistics, Volume 13, Numbers 3-4, July-December 1987 271Igor Mel'(mk and Alain Polgu~re A Formal Lexicon in Meaning-Text Theorythe lexeme COURSE (as in programming course, takea course etc.
):(6) noun(course, crs(X,Y .... ), nil, X:crs,\[npobj(in):Y: subject\] ).In this Prolog expression, the last argument of thefive-place predicate 'noun' indicates that the comple-ment (= NP object) of the lexeme COURSE introducedby the preposition IN (course in ancient history) mustbe labeled with the semantic feature "subject" .
We,however, think that such indications should by nomeans appear in the description of lexemes as cooccurr-ence restrictions.
Actually, one can give a course onanything at all (flowerpots, sexual habits of bedbugs,etc.).
Just giving a course on something makes thissomething a subject.
Therefore, we must indicate thatthe Y argument of the predicate 'course' is calledsubject but we cannot say that in order to be able to fillin the argument Y of COURSE a noun must be seman-tically labeled "subject" .
82.3.2.
THE CONCEPT OF LEXICAL FUNCTION.A lexical function f is a dependency that associateswith a lexeme L, called the argument of f, anotherlexeme (or a set of (quasi-)synonymous lexemes) L'which expresses, with respect to L, a very abstractmeaning (which can even be zero) and plays a specificsyntactic role.
For instance, for a noun N denoting anaction, the lexical function Operl specifies a verb (se-mantically empty - -  or at least emptied) which takes asits grammatical subject the name of the agent of saidaction and as its direct object, the lexeme N itself.
Thusthe phrases in (5) are described as follows:(7) a.Engl.
Oper I (QUESTION) = ASKFr.
Oper 1 (QUESTION) = POSERSp.
Operl (PREGUNTA) = HACERRuss.
Oper 1 (VOPROS) = ZADAT'b.Engl.
Oper I (CRY) = LET  OUTFr.
Oper I CRI) = POUSSERSp.
Oper I (GRITO) = DARRuss.
Oper~ (KRIK) = ISPUSTIT'There are about 60 lexical functions of the Operl type,called standard elementary LFs.
They and their combi-nations allow one to describe exhaustively and in ahighly systematic way almost the whole of restrictedlexical cooccurrence in natural languages.
Given the8 Selectional restrictions ofthe type just indicated are needed in mostcases for possible applications ofa grammar, e.g., in order to facilitatethe resolution of syntactic homonymy.
We, on the other hand, do notwant to mix the theoretical description of a grammar with toolsappropriate for its applications.
Note, however, that we use selectio-nal restrictions a  well, but only if they are necessary for the choice ofthe correct linguistic expression: for instance, with a given verb, onepreposition is chosen with human ouns while the other one takesonly abstract nouns, etc.
(cf.
the choice between pour vs. a in 2.2.2).importance of lexical functions for "intell igent" linguis-tic systems, we will offer an abridged list thereof in theappendix (see also Mel'6uk 1982).In the MTM LFs play a double role:1) During: the production of the text from a givenSemR, LFs control the proper choice of lexical itemslinked to the lexeme in question by regular semanticrelations.
For instance, if we need to express in Frenchthe meaning 'badly wounded' \['badly' ~ Fr.
mal,'wounded' :9 blessd, but 'badly wounded' cannot betranslated by *mal bless~\], we take the following steps:(8) a.
'wounded' ~ Fr.
bless~;b.
'badly' with respect o 'wounded' is the LF Magn\[meaning 'very' ,  'intensely'\]: Magn ( wounded )badly ;c. therefore, 'badly wounded' ~ Magn ( blessd ) +blessO;d. Magn (blessO ) = gridvement ;e. finally, 'badly wounded' ~ gridvement blessd.During the analysis of a text, LFs help to resolvesyntactic homonymy, since they indicate which word hasthe greater likelihood of going with which other word.2) In text production, LFs are used to describesentence synonymy, or, more precisely, the derivationof a set of synonymous sentences from the same DSynt-structure.
This is done by formulating, in terms of LFs,a number of equivalences of the following type:(9) Co <v) <=>Oper l  ( So ( Co ) ) II > So ( Co ) ,where C o stands for the head lexeme and So for theaction nominal.This equivalence can be illustrated by (10)"(10) Alex received \[C o\] me quite well <=> Alex gave\[Operl ( So ( Co ) )\] me quite a good reception \[S O ( C o )\].The operation carried out by this type of rule is calledparaphrasing.
About sixty paraphrasing rules of theform (9) are needed to cover all systematic paraphrasesin any language (moreover, there must be about thirtysyntactic rules which describe transformations of treesand "serve"  the rules of the above type).
A powerfulparaphrasing system is necessary, not only because it isinteresting in itself, but mainly because without such asystem it seems impossible to produce texts of goodquality for a given SemR: indeed when one is blockedduring a derivation by linguistic restrictions, one canbypass the obstacle by recourse to paraphrases.During text analysis, a powerful paraphrasing systemhelps to reduce the vast synonymy of natural anguageto a standard and therefore more manageable r presen-tation.3.
THE ECD IN THE COMPUTATIONALIMPLEMENTATION OF THE MEANING-TEXT MODEL.Within the framework of the global task of automaticnatural anguage processing (which is involved in, e.g.,natural language understanding, machine translation,272 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Igor Mei'~uk and Main Polgu~re A Formal Lexicon in Meaning-Text Theory"natural anguage interfaces, etc.)
the MTM addressesone rather restricted but precise problem: the produc-tion of the highest possible number of synonymousutterances for a given meaning, presented in the stan-dard form of a SemR, or, inversely, the reduction of awealth of synonymous utterances to their standardsemantic invariant - -  their SemR.
All the other compo-nents of a natural language system could then be gearedto a representation f meaning, thus avoiding all thecapricious phenomena of natural anguage not directlyrelated to meaning.
We would like to discuss below twoissues relevant o this aspect of natural anguage proc-essing, an aspect which we consider to be strictlylinguistic, and specifically relevant to the ECD: 1) usingthe lexicon in the transition between levels of linguisticrepresentation a d 2) structuring the lexicon.3.1.
USING THE LEXICON IN THE TRANSITION BETWEENLEVELS OF REPRESENTATION.To see how the ECD is applied by the MTM in theutilization of the latter, let us assume that the MTM hasto deal with a pre-existent SemR which corresponds toa sentence (we disregard the problem of cutting anyinitial SemR, which can represent the global meaning ofa whole text, into "smaller" SemRs, representing themeanings of separate sentences).
We presuppose thatthe model is able to associate with such a SemR allDSyntRs of the sentences which a speaker of thelanguage in question could produce for the given mean-ing.
In other words, the model carries out the mapping{SemRi} <==> {DSyntRk}.
This mapping is imple-mented via the following two operations which arelogically distinct, even though we can conceive of themas constantly interacting and applying simultaneously:1) The first operation groups semantic elements ofthe initial SemR into clusters each of which correspondsto a lexical unit of the language.
Thus this operationselects the lexicai stock for the target DSyntR and can becalled lexicalization.
(As the careful reader may havenoticed, we have already discussed lexicalization with-out naming it at the end of Sec.
2.1.3.
)2) The second operation organizes the lexical unitssupplied by the first operation into a dependency treeunder the control, on the one hand, of the informationfound under the dictionary entries for these units (var-ious constraints on cooccurrence), and, on the otherhand, of semantic links between their sources in theSemR.
Thus this operation constructs the syntacticstructure for the target DSyntR and can be calledsyntacticization.It is obvious that the two operations are not com-pletely independent of each other; but at present we donot know how they interact.
There is an even moreserious problem: the MTM does not offer proceduraltools ensuring different groupings of semantic networkelements, in order to join them into lexicalizable clus-ters.
In other words, the MTM in its current state lacksmechanisms and devices for semantic network analysis.Nevertheless, whatever these mechanisms may be, theymust rely in an essential way on the ECD, from whichthey have to draw all necessary semantic and syntacticinformation.
After all, semantic units in a network, i.e.in a SemR, are nothing else but lexical meanings in thelanguage under consideration.Each DSyntR obtained in the way just describedundergoes a set of paraphrasing rules, which involvelexical functions; an example of such a rule is rule (9).These rules derive from the initial DSyntR the set ofDSyntRs which are synonymous to it, thus providingfor necessary synonymic flexibility: as we have justsaid, under synthesis, the model displays the multitudeof variants, so that the selection of an appropriate onebecomes easier; under analysis, it reduces the host ofvariants to a standard representation, which facilitatesits subsequent processing.
The paraphrasing rules of theMTM have already been studied from the computa-tional viewpoint (Boyer and Lapalme 1985).
Note thattheoretically these rules are reversible: although theyhave been implemented in the synthesis direction, theymust function as well under analysis.As for the operations and the corresponding compo-nents of the MTM which have the task of carrying outthe transition between closer-to-surface l vels (fromDSyntR to SSyntR, from SSyntR to DMorphR, etc.
),we will not consider them here for lack of space.3.2.
THE OVERALL STRUCTURING APPROACH TO THELEXICON AS A TYPICAL TRAIT OF THE ECD.Computational linguistics today knows many lan-guage analysis and synthesis systems relying on anexplicit formalized lexicon.
Traditionally, however,these systems use a "flat" declarative representation flexical information.
More specifically, a lexical entry ina classical computational lexicon is an independentpropos i t ion -  i.e., a structure fully autonomous withrespect to all other entries; such an entry contains a setof lexical data (concerning the head word) which isalmost always isolated from other similar sets of lexicaldata.
This boils down to the following principle: gener-ally speaking, a lexeme is not described as a function ofother lexemes of the language, so that the informationsupplied by the lexicon does not relate it to the rest ofthe vocabulary.
The structure of typical computationallexica is not relational: they are basically sequences ofentries, each of which is a set of features associatedwith the head lexeme.
See, for instance, the entry ofMcCord's Slot Grammar lexicon cited in (6) above, orthe lexicon of Pereira's well-known Chat-80 (Pereira1983).This approach can be loosely called non-structuring.
9(Note that this state of affairs recalls what has beenobserved in the development of modern semantics:9 To be sure, this approach presupposes some degree of structuring;we, however, allow ourselves to use the term to emphasize theinsufficiency ofthis structuring.Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 273Igor Mel'~uk and Alain Polgu~re A Formal Lexicon in Meaning-Text Theoryinitially, linguists believed that one could do with un-structured sets of semantic features; later, they em-barked on a research path with a heavy emphasis onhierarchization of semantic representations, uch as"logical form";  finally, we see the advent of highlysophisticated semantic networks.
One might expect asimilar evolution in the organization of the lexicon.
)The non-structuring approach has certain advan-tages, the main one being the possibility of isolating, inthe course of actual text processing, a mini-fragment ofthe whole lexicon, sufficient for a specific task and mucheasier to manipulate.
In order to analyse or synthesize atext, a system using a non-structured lexicon will haveto deal only with the lexemes actually present in thistext, without being forced to reach out to other entriesin the lexicon.
Computationally, this is very practicalbut then the system is restricted to scanty lexicalinformation.
The result is that a non- structured lexiconprevents the system from performing high-level linguis-tic jobs, which involves looking for new words and setphrases, making subtle choices, controlling style, andthe like.In sharp contrast to this, the ECD is consistentlystructured; this means that, with respect o its meaningand its cooccurrence, a lexeme is specified in terms ofother lexemes.
The necessity of such structuring mani-fests itself in many tasks of which we will consider thefollowing two, considered previously: lexicalization andparaphrasing.The lexicalization of a SemR, i.e.
of a semanticnetwork, is carried out due to the fact that a semanticunit labeling one of the network's nodes is actually thesense of a word n a lexeme, which is fully determinedin the lexicon by a hierarchical set of properties.
Morespecifically, one of these properties is the participationof the lexeme in question in the definitions of otherlexemes, as well as the presence of certain lexemes inits own definition.
The screening of semantic nodes withtheir properties will hopefully allow the model to carryout successfully the analysis of SemRs in a givenlanguage m in view of their lexicalization.
We think thatit will be possible to generalize this particular way ofexploiting a structured lexicon to other levels of linguis-tic computation.The paraphrasing of a DSyntR, i.e.
of a DSynt-tree,which is so important o ensure the transition betweenthe Sem- and the DSynt-levels (see 2.3.2), is of coursepossible only through Lexical Functions, which obvi-ously represent another aspect of lexical structuring.To conclude, we would like to mention that thewhole of the information presented in a lexicon of theECD format may sometimes eem too detailed withregard to certain kinds of computational pplications.However,  we do not see why, in computational s wellas in more traditional linguistics, a lexical descriptioncould not be as complete, as consistent and as detailedas possible, so that it could be employed (even ifpartially modified) in all imaginable contexts of researchand/or application.
Such a description ideally containsall the lexical information that could be necessary forany task; for a specific task, one can extract from thissource as much information as is deemed sufficient.
Adescription of a linguistic phenomenon formulated in atheoretically consistent and exhaustive way is valuablefor computational linguistics applications because it is"reusable":  it can be utilized in many different applica-tions.
From this point of view, the rigorous descriptionof even one \[exeme in a precise and formal frameworkappears as an improvement, in itself, on what has beenachieved in this domain.ACKNOWLEDGMENTSThanks to Geoffrey Hird for his comments on an earlierdraft of this paper and to Kathleen Connors and LidijaIordanskaja tbr their remarks and suggestions concern-ing the final draft.
We are also grateful to two anony-mous reviewers of Computat ional  Linguistics, whosewell-directed criticisms helped us considerably in re-shaping our paper.REFERENCESBoyer, Michel and Lapalme, Guy.
1985 Generating Paraphrases fromMeaning- Text Semantic Networks.
Computational Intelligence1(3-4): 103-117.Dowty, David R. 1979 Word Meaning and Montague Grammar.
D.Reidel, Dordrecht, Holland.Gazdar, Gerald; Klein, Ewan; Pullum, Geoffrey and Sag, Ivan.
1985Generalized Phrase Structure Grammar.
Harvard UniversityPress, Cambridge, Massachusetts.Gross, Maurice.
1975 M~thodes en Syntaxe.
Hermann, Paris, France.Lamb, Sydney.
1966 Outline of Stratificational Grammar.
George-town University Press, Washington.McCord, Michael C. 1982 Using Slots and Modifiers in Logic Gram-mars for Natural Language.
Artificial Intelligence 18: 327-367.McKeown, Kathleen R. 1985 Text Generation.
Cambridge UniversityPress, Cambridge, England.Mel'~uk, Igor A.
1974 Opyt Teorii Lingvisti(eskix Modelej "Smysl-Tekst".
Nauka, Moscow, USSR.Mel'~uk, Igor A.
1981 Meaning-Text Models: A Recent Trend inSoviet Linguistics.
Annual Review of Anthropology 10: 27-62.Mel'~uk, Igor A.
1982 Lexical Functions in Lexicographic Descrip-tion.
In: Proceedings of the Vlllth Annual Meeting of the BerkeleyLinguistic Society, Berkeley: UCB, 427-444.Mel'~uk, Igor A.
1988.
Dependency Syntax: Theory and Practice.SUNY Press, New York.Mel'~uk, Igor A. et al 1984 Dictionnaire Explicatif et Combinatoiredu Francis Contemporain.
Recherches Lexico-S~mantiques 1.Presses de rUniversitd e Montrdal, Montrdal, Canada.Mel'~uk, Igor A. and ~holkovsky, Alexander K. 1984 ExplanatoryCombinatorial Dictionary of Modern Russian.
\[in Russian\] WienerSlawistischer Almanach, Vienna, Austria.Pereira, Fernando C.N.
1983 Logic for Natural Language Analysis.Technical Note 275, SRI International, Menlo Park, California.Schank, Roger C. 1972 Conceptual Dependency: A Theory of NaturalLanguage Understanding.
Cognitive Psychology 3: 552-631.Sgall, Peter.
1967 Generativini Popis Javyka a Ceskd Declinace.Academia, Prague, Czechoslovakia.Wierzbicka, Anna.
1980 Lingua Mentalis.
Academic Press, NewYork, New York.274 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987Igor Mel'~uk and Alain Poigu~re A Formal Lexicon in Meaning-Text TheoryAPPENDIXAn illustrative list of lexical functionsThe argument of a LF  is called its key word.AoDerived adjective with the same meaning as the keyword:A o ( city ) = urbanA1, A2, A3, ?
?
.Typical qualifier for the first, second, third .
.
.
.
actantof the key word:A 1 ( surprise ) = surprisedA 2 ( surprise ) = surprisingContMeans 'continue':ContOperl( contact ) = stay, remain, keep \[ incontact with \](Here is a typical example of the way LFs can be"composed" to describe more complex new relationsbetween lexical items.
)ContrContrastive term:Contr ( top ) = bottomContr ( night ) = dayConV(iklConversive, or a lexeme denoting a relation that is theconverse of the one expressed by the argument of theLF.
The indices i, j, k, l indicate the type of argumentpermutation which is effected:ConY21 ( more ) = lessTheo is MORE religious than Oeht <=> Oeht isLESS  religious than TheoC0nv3214 ( sell ) = buyIvan SOLD his soul to the Devil  fo r  three bucks < = >The Devil  BOUGHT Ivan's  soul f rom him for  threebucksGenerGeneric word:Gener ( anger ) = feel ing \[ o f  anger \]Gener ( pain ) = sensation, feel ing \[ o f  pain \]LaborqSemantically empty word which takes the actants i andj as its subject and direct object, respectively, and thekey word as its indirect object:Labor12 ( esteem ) = hold \[ someone in (high) esteem\]LiquMeans 'liquidate', 'eliminate':Liqu ( meeting ) = adjournMultStandard word for a collectivity:Mult ( ship ) = f leetOperl, Oper2, ?
.
.See 2.3.2 above.
Semantically empty verb which takesthe first, second .
.
.
.
actant of the key word as itssubject and the key word as its direct object:Operl ( attention ) = payOper 2 ( attention ) = attractMagnMeans ' very', 'intense', 'intensely':Magn ( escape ) = narrowMagn ( bleed ) = profuselySoDerived noun with the same meaning as the key word:So ( honest ) = honestySl, 52, S 3 .
.
.
.Typical noun for the first, second, third .
.
.
.
actant ofthe key word:S 1 ( sell ) = vendorS 2 ( sell ) = merchandiseS 3 ( sell ) = buyerS 4 ( sell ) = priceSyn, Sync, Syn-~, Syn nSynonymous and quasi-synonymous ( "C"  means 'nar-rower'; "D"  means 'broader'; "A"  means'intersecting'):Syn ( calling ) = vocationSyn c ( respect ) = venerationSyn~ ( keen ) = interestedSyn n ( escape ) = break out \[ of\] ,  run away \ [ f rom \]Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 275
