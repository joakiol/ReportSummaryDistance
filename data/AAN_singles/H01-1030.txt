First Story Detection using a Composite DocumentRepresentation.Nicola Stokes, Joe Carthy,Department of Computer Science,University College Dublin,Ireland.
{nicola.stokes,joe.carthy}@ucd.ieABSTRACTIn this paper, we explore the effects of data fusion on First StoryDetection [1] in a broadcast news domain.
The data fusionelement of this experiment involves the combination of evidencederived from two distinct representations of document content in asingle cluster run.
Our composite document representationconsists of a concept representation (based on the lexical chainsderived from a text) and free text representation (using traditionalkeyword index terms).
Using the TDT1 evaluation methodologywe evaluate a number of document representation strategies andpropose reasons why our data fusion experiment showsperformance improvements in the TDT domain.KeywordsLexical Chaining, Data Fusion, First Story Detection.1.
INTRODUCTIONThe goal of TDT is to monitor and reorganize a stream ofbroadcast news stories in such a way as to help a userrecognize and explore different news events that haveoccurred in the data set.
First story detection (or online newevent detection [1]) is one aspect of the detection problemwhich constitutes one of the three technical tasks definedby the TDT initiative (the other two being segmentationand tracking).
Given a stream of news stories arriving inchronological order, a detection system must group orcluster articles that discuss distinct news events in the datastream.
The TDT initiative has further clarified the notionof topic detection by differentiating between classificationin a retrospective (Event Clustering) and an onlineenvironment (First Story Detection).
In FSD the systemmust identify all stories in the data stream that discussnovel news events.
This classification decision is made byconsidering only those documents that have arrived prior tothe current document being evaluated, forcing the system toadhere to the temporal constraints of a real-time newsstream.In other words the system must make an irrevocableclassification decision (i.e.
either the document discusses anew event or previously detected event) as soon as thedocument arrives on the input stream.
The goal of eventclustering on the other hand is to partition the data streaminto clusters of related documents that discuss distinctevents.
This decision can be made after the system hasconsidered all the stories in the input stream.In addition to defining three research problemsassociated with broadcast news, the TDT initiative alsoattempted to formally define an event with respect to how itdiffers from the traditional IR notion of a subject or a topicas defined by the TREC community.
An event is defined as?something that happens at some specific time and place(e.g.
an assassination attempt, or a volcanic eruption inGreece)?.
A topic on the other hand is a ?seminal event oractivity along with all directly related events and activities(e.g.
an investigation or a political campaign)?
[1].
InitialTDT research into event tracking and detection focused ondeveloping a classification algorithm to address this subtledistinction between an event and a topic.
For examplesuccessful attempts were made to address the temporalnature of news stories1 by exploiting the time betweenstories when determining their similarity in the detectionprocess [1].
However current research is now focusing onthe use of NLP techniques such as language modeling [2,3], or other forms of feature selection like the identificationof events based on the domain dependencies betweenwords [4], or the extraction of certain word classes fromstories i.e.
noun phrases, noun phrases heads [5].
All thesetechniques offer a means of determining the mostinformative features about an event as opposed toclassifying documents based on all the words in thedocument.
The aim of our research is also based on thisnotion of feature selection.
In this paper we investigate ifthe use of lexical chains to classify documents can betterencapsulate this notion of an event.
In particular we look atthe effect on FSD when a composite documentrepresentation (using a lexical chain representation and freetext representation) is used to represent events in the TDTdomain.1Stories closer together on the input stream are more likely todiscuss the same event than stories further apart on this stream.In sections 2 and 3 we describe the first component of ourcomposite document representation derived from lexicalchains, with a subsequent description of FSD classificationbased on our data fusion strategy in Section 4.
Theremaining sections of this paper give a detailed account ofour experimental results, concluding with a discussion oftheir significance in terms of two general criteria forsuccessful data fusion.2.
LEXICAL CHAININGA lexical chain is a set of semantically related words in atext.
For example in a document concerning cars a typicalchain might consist of the following words {vehicle,engine, wheel, car, automobile, steering wheel}, whereeach word in the chain is directly or indirectly related toanother word by a semantic relationship such as holonymy,hyponymy, meronymy and hypernymy.When reading any text it is obvious that it is notmerely made up of a set of unrelated sentences, but thatthese sentences are in fact connected to each other in one oftwo ways cohesion and coherence.
As Morris and Hirst[6] point out cohesion relates to the fact that the elementsof a text ?tend to hang together?.
Whilst coherence refersto the fact that ?there is sense in the text?.
Obviouslycoherence is a semantic relationship and needscomputationally expensive processing for identification,however cohesion is a surface relationship and is hencemore accessible.
As indicated by Halliday and Hasan [7]cohesion can be roughly classified into three distinctclasses, reference, conjunction and lexical cohesion.Conjunction is the only class, which explicitly shows therelationship between two sentences, ?I have a cat and hisname is Felix?.
Reference and lexical cohesion on the otherhand indicate sentence relationships in terms of twosemantically same or related words.
In the case ofreference, pronouns are the most likely means of conveyingreferential meaning.
For example in the followingsentences, ?
?Get inside now!?
shouted the teacher.
Whennobody moved, he was furious?.
In order for the reader tounderstand that ?the teacher?
is being referred to by thepronoun ?he?
in the second sentence, they must refer backto the first sentence.
Lexical cohesion on the other handarises from the selection of vocabulary items and thesemantic relationships between them.
For example,  ?Iparked outside the library, and then went inside thebuilding to return my books?, where cohesion is representedby the semantic relationship between the lexical items?library?, ?building?
and ?books?.
For automaticidentification of these relationships it is far easier to workwith lexical cohesion than reference because lessunderlying implicit information is needed to discover therelationship between the above pronoun and the word itreferences.
Hence lexical cohesion is used as a linguisticdevice for investigating the discourse structure of texts andlexical chains have been found to be an adequate means ofexposing this discourse structure.
These lexical chainshave many practical applications in IR and computationallinguistics such as hypertext construction [8], automaticdocument summarization [9], the detection ofmalapropisms within text [10], as a term weightingtechnique capturing the lexical cohesion in a text [11], as ameans of segmenting text into distinct blocks of selfcontained text [12].
For the purpose of this project weexploit three such applications:1.
We use lexical chains as a means of exploring andpresenting the most prevalent topics discussed in newsstories.2.
A valuable side effect of lexical chain creation is thatthe words of a text are automatically disambiguated.3.
Because lexical chains disambiguate words based onthe context in which they occur, lexical chains alsoaddress two linguistic problems synonymy andpolysemy, which hinder the effectiveness of traditionalIR systems such as the vector space model.3.
CHAIN FORMATION ALGORITHMIn general the first task of an IR system is to execute a setof text operations (e.g.
stemming, removal of stopwords) toreduce the complexity of a full text representation of adocument into a more manageable set of index terms.Although these index terms are a subset of the originalrepresentation, their purpose is to adequately represent thesemantic content of the original document in a moreconcise manner.
This is a difficult NLP task, as naturallanguage frequently does not obey the principle ofcompositionality where the meaning of the whole can bestrictly determined from its parts.
So in order to derive thecorrect representation of a text, we need to determine theinterpretation of a word or phase in the context in which itoccurs i.e.
before the original text is manipulated into a setof index terms.
The creation of lexical chains which isdescribed below, aims to capture this additional textualinformation while still maintaining a manageablerepresentation size.Firstly each term contained in a particular documentis dealt with in chronological order.
Then each subsequentword is added to an existing lexical chain or becomes theseed of a new chain, in much the same manner as theclustering of documents.
A stronger criterion than simplesemantic similarity is imposed on the addition of a term toa chain, where terms must be added to the most recentlyupdated (semantically related) chain.
This favors thecreation of lexical chains containing words that are in closeproximity within the text, prompting the correctdisambiguation of a word based on the context in which itwas used.
We use WordNet to determine the semanticrelatedness between a candidate word and the words of achain.
If we view WordNet as a large semantic network ofnodes (meanings) inter-related by semantic relations(meronymy, hyponymy, etc.
), then finding a relationshipbetween two words in the chaining process involvesactivating the network of one node and observing theactivity of the other in this activated network.Figure 1: Shows expanded document terms ?car?
and ?trunk?and their semantic relatedness.So far we have talked abstractly about how to determine ifa word is semantically related to a chain.
To explain thisfully it is first necessary to discuss the structure of theWordNet thesaurus, which is used to determine thissemantic connection or closeness between words in a text.In WordNet, nouns, verbs, adjectives, and adverbs arearranged into synsets (group of synonymous words e.g.
cat,feline, tabby), which are further organized into a set oflexical source files by syntactic category.
In our case weare only interested in the noun index and data files, becausethe verb file in WordNet has no relation with the threeother files (noun, adverb and adjective files), and theadverb file has only unidirectional relations with theadjective file.
So each word in a particular document issearched for in the noun index file, if it is not found thenwe make the assumption that this word is not a noun andhence will play no further part in the chaining process.
Ifthe word is found then it will be represented by a unique setof synset numbers, where each synset number represents aparticular sense associated with that word.
Each synsetnumber points to the position in the noun data file wherewords related to this sense of the word are stored with agloss, and sample sentence using this word.
Words relatedto a particular sense are associated with it by severaldifferent semantic relations, such as hyponymy (kind-of,lorry/vehicle), hypernymy (is-a, vehicle/car), holonymy(has-part, tree/branch) and meronymy (part-of, engine/car).As shown in Figure 1, each sense associated with a word isexpanded using WordNet (in reality these senses and sensesrelated to them are represented by synset numbers).
Thisexample of the chain formation process shows us that theword ?car?
is related to the word ?trunk?
by the fact that ?cartrunk?, one of the senses of ?trunk?, is a meronymy of?automobile?
which is a possible sense of  ?car?.
In this wayboth words have been successfully disambiguated so allredundant senses belonging to each word are eliminatedand ?car?
is added to the chain containing ?trunk?.
Thischain may also contain other semantically related wordspertaining to the topic of an automobile e.g.
{car, trunk,engine, vehicle?}.
The chain formation process iscontinued in this way until all the words in a particulardocument (in our case nouns) have been chained.
Anywords that remain unchained or ambiguous after thischaining process are eliminated from our chain wordrepresentation based on the following hypothesis:?The occurrence of words in a text which fail to participatein the overall cohesive structure of a text (i.e.
remainunchained) is purely coincidental.
Consequently thesewords are considered irrelevant in describing the generaltopic of a document.
?This implies that our lexical chaining strategy also providesus with an automatic means of selecting the most salientfeatures of a particular news story.
So when all redundantwords have been removed in this manner, all remainingchains are then merged into a single chain containing all thesynset numbers from each individual chain involved in thisprocess.
This representation is a semantic representation asopposed to a syntactic representation (in the case of a ?bagof words?
representation) because it contains concepts (i.e.synset numbers) rather than simple terms to represent thecontent of a document.SENSE OF WORDKIND-OF (HYPONYMY)HAS PART (HOLONYMY)PART OF (MERONYMY)TRUNKCAR TRUNKAUTOMOBILETREETRUNKELEPHANTTRUNKBARKVEHICLECARTRAIN CAR AUTOMOBILECABLE CARThe final stage of our combined documentrepresentation strategy involves collecting all free textwords for each document and storing them in a set of indexfiles.
So effectively our composite document representationused in the detection process (described in the next section)consists of two weighted vectors, a chain vector and anordinary term vector, where both chain words and free textwords are weighted simply in terms of the frequency inwhich they occur in a document.4.
DETECTION ALGORITHM USING THEFUSION METHODOnline Detection or First Story Detection is in essence aclassification problem where documents arriving inchronological order on the input stream are tagged with a?YES?
flag if they discuss a previously unseen news event,or a ?NO?
flag when they discuss an old news topic.However unlike detection in a retrospective environment astory must be identified as novel before subsequent storiescan be considered.
The single-pass clustering algorithmbases its clustering methodology on the same assumption,the general structure of which is summarised as follows.1.
Convert the current document into a weighted chainword vector and a weighted free text vector.2.
The first document on the input stream will become thefirst cluster.3.
All subsequent incoming documents are comparedwith all previously created clusters up to the currentpoint in time.
A comparison strategy is used here todetermine the extent of the similarity between adocument and a cluster.
In our IR model we use sub-vectors to describe our two distinct documentrepresentations.
This involves calculating the closenessor similarity between the chain word vectors and freetext vectors for each document/cluster comparisonusing the standard cosine similarity measure (used inthis variation of the vector space model to compute thecosine of the angle between two weighted vectors).The data fusion element of this experiment involvesthe combination of two distinct representations ofdocument content in a single cluster run i.e.
j equals 2in equation (1).
So the overall similarity between adocument D and a cluster C is a linear combination ofthe similarities for each sub-vector formally defined as:where Sim(X, Y) is the cosine similarity measure fortwo vectors X and Y,  and w is a coefficient that biasesthe weight of evidence each document representation j,contributes to the similarity measure.4.
When the most similar cluster is found a thresholdingstrategy [13] is used to discover if this similaritymeasure is high enough to warrant the addition of thatdocument to the cluster and the classification of thecurrent document as an old event.
If this documentdoes not satisfy the similarity condition set out by thethresholding methodology then the document isdeclared as discussing a new event, and this documentwill form the seed of a new cluster.5.
This clustering process will continue until alldocuments in the input stream have been classified.5.
EXPERIMENTAL RESULTSA number of experiments were conducted on the TDT-1broadcast news collection [1].
The results of theseexperiments were used to observe the effects on first storydetection when lexical chains are used in conjunction withfree text as a combined document classifier.
The main aimof the experiments was to determine if lexical chains are asuitable document representation when classifying newsstories in the TDT domain.
The official TDT evaluationrequires that the system output is a declaration (a YES orNO flag) for each story processed.
These declarations arethen used to calculate two system errors percentage missesand false alarms.
Misses occur when the system fails todetect the first story discussing a new event and falsealarms occur when a document discussing a previouslydetected event is classified as a new event.5.1 System DescriptionsThree distinct detection systems TRAD, CHAIN andLexDetect are examined in the following set ofexperiments.
The TRAD system [13], our benchmarksystem in these experiments is a basic FSD system thatclassifies news stories based on the syntactic similaritybetween documents and clusters.
The design of this systemis based on a traditional vector space model whichrepresents documents as a vector, each component of whichcorresponds to a particular word and who?s value reflectsthe frequency of that word in the document.
Classificationof a new event occurs in a similar manner to that describedin Section 4, the most important difference between the twomethods is that a single free text representation is used toexpress document content, rather than a combinedrepresentation.
A Time Window [13] of length 30 isemployed in the TRAD, CHAIN and LexDetect systems.The design of our second system LexDetect hasbeen described in detail in sections 3 and 4.
Thedimensionality of LexDetect (80 words) remains staticthrough out these experiments.
Using the current method oflexical chain creation, just under 72% of documentscontained greater than or equal to 30 chained words.
Wetherefore normalized the length of chain wordrepresentations by imposing a chain dimensionality value?=?=kjjjj )CDSimwCDSim1)1(,(),(of 30 on all LexDetect schemes2.
In theory it is possible tovary the length of the free text representation in ourcombined representation however in these experiments allschemes contain free text representations of length 50,since optimal performance is achieved for TRAD whendimensionality 50 is used.
The final system parameter to bevaried in these experiments is the weighting coefficient wjused inequation (1).
The design of our third systemCHAIN like TRAD, involves the use of a singulardocument representation.
However this documentrepresentation contains chain words only rather than freetext terms, and so the dimensionality of the system must be30.5.2 The Data Fusion ExperimentFrom the results shown in Figure 2 (a DetectionError Tradeoff Graph where points closer to the originindicate better overall performance), we deduce that amarginal increase in system effectiveness can be achievedwhen lexical chain representations are used in conjunctionwith free text representations in the detection process.
Inparticular, we see that the miss rate of our FSD systemLexDetect decreased with little or no impact to the falsealarm rate of the system.DET graph showing %Misses and %False Alarms forTRAD_50, LexDetect and CHAIN systems01020304050607080901000 20 40 60 80 100%Misses%FalseAlarmsLex_DetectTRAD_50CHAINFigure 2: The effect on performance when a weightedcombined document representation is used.2An IR ?system?
and an IR ?scheme?
are used in this context todescribe two different concepts.
An IR system refers to thephysical implementation of an IR algorithm, which can havevarious operational modes or various parameter settings.
Thesame IR system may be used to execute different IR schemes byadjusting these parameters [20].Optimal performance for the LexDetect system (asshown in Figure 2) was found when a weightedcombination of evidence was used.
This involved treatingour free text representation as weaker evidence during thedetection process.
Results shown in Figure 3 contrast theeffect on LexDetect performance when both the chain andfree text representations are given equal weight (Lex) andwhen the weight of the free text representation is halved(LexDetect).
This is an interesting result as similarexperiments using composite document representations toimprove search system performance based on ranking, onlyexperienced optimal effectiveness when they allowed freetext evidence to bias the retrieval process [14, 15].
Thisprompted us to question the necessity of the free textcomponent of our composite representation, howeverresults show that system performance degrades when thiselement of document content is excluded.
This is due to theinability of WordNet to correlate the relationship betweenproper nouns and other semantically related concepts i.e.
{Bill Clinton, US president}, which are often crucial inrepresenting journalistic event identity because they reflectthe ?who, what, where, when and how?
of a news story.Our final experiment involves plotting TRAD_80against LexDetect shown in Figure 4.
The aim of thisexperiment is to prove that the increase in systemeffectiveness observed when a composite documentrepresentation is used can be attributed solely to thecombination of evidence derived from our free text andchain representations rather than as a consequence ofincreasing the dimensionality of the system to 80 features.As the DET graph in Figure 4 shows, our LexDetect systemstill outperforms our TRAD system under conditions ofequal dimensionality.DET graph showing % Misses and %False Alarms forLexDetect and Lex01020304050607080901000 20 40 60 80 100% Misses%FalseAlarmsLexLexDetectFigure 3: The effect on performance when equal weight isgiven to both representations (Lex) in contrast to a weightedcombined document representation (LexDetect).DET graph showing % Misses and %False Alarms for LexDetectand TRAD_8001020304050607080901000 20 40 60 80 100% Misses%FalseAlarmsTRAD_80LexDetectFigure 4: The effect on performance when equaldimensionality of 80 is given to both the LexDetect and TRADsystems.6.
CRITERIA FOR SUCCESSFUL DATAFUSIONIn the previous section our results showed that when achain word representation is used in conjunction with a freetext representation of a document, improvements in FSDeffectiveness are observed.
However these results fail toprovide any concrete reasoning as to why data fusion underthese particular conditions work.
There are many papers inthe data fusion literature, which attempt to explain whycertain data fusion experiments succeed where others havefailed.
Many of these papers look at the effects ofcombining specific sources of evidence such as thecombination of rank retrieval lists, multiple searches ormultiple queries.
However Ng and Kantor [16] have tried toformulate some general preconditions for successful datafusion involving non-specific sources of evidence.The first of these criteria is based on thedissimilarity between two sources of evidence.1.
Dissimilarity: Data fusion between operationally verysimilar IR systems may not give better performance.To calculate the level of dissimilarity between our FSDsystems described in Section 5, we now define two ratiosbased on the number of common relevant and commonnon-relevant tagged documents between two distinctsystems.
The number of relevant tagged documents,|r1?r2| is defined as the number of documents that werecorrectly classified (as a new or old event) by both systems.The total number of relevant documents, r1+r2 is the sumof the number of correctly classified documents for eachsystem.
|n1?n2| and n1+n2 are similarly defined in termsof  the number of incorrectly classified documents returnedby both systems (i.e.
missed events or wrongly detectednew events) as shown in equation 3.)3(2)2(221212121nnnnNrrrrRoverlapoverlap+?+?
?=?=The results for this experiment are shown in tables 1and 2 below.
We can see that in general the relevantdocument overlap Roverlap between the pair-wise similaritiesof all four systems is between 85% and 92%, the mostsimilar systems being not surprisingly our two TRADschema which differ only in the length of their classifiers.The pair-wise similarities Noverlap of all four systemsregarding non-relevant document classifications exhibit asimilar trend of high similarity between the TRAD andLexDetect systems.
However the most important point tobe taken from these sets of results regards the fact that ourCHAIN and TRAD systems exhibit the lowest relevant andnon-relevant document overlap of all our pair-wisecomparisons.
This is an important and encouraging resultas it shows that our chain word representations (used inCHAIN) is sufficiently dissimilar to our simple ?bag ofwords?
representation (used in TRAD) to contributeadditional evidence to a combination experiment involvingboth these representations.
In particular this satisfaction ofNg and Kantor?s dissimilarity criteria explains whymarginal improvements in system performance wereobserved in our data fusion experiment.Table 1: Relevant document overlap between FSD systems.ROVERLAP LexDetect TRAD_50 TRAD_80  CHAINLexDetect 1TRAD_50 0.85 1TRAD_80 0.85 0.92 1CHAIN 0.56 0.52 0.53 1Table 2: Non-relevant document overlap between FSDsystems.NOVERLAP LexDetect TRAD_50 TRAD_80  CHAINLexDetect 1TRAD_50 0.67 1TRAD_80 0.68 0.82 1CHAIN 0.58 0.51 0.53 1The second criteria defined for successful datafusion regards efficacy or the quality of the individualsources of evidence before they are combined in the datafusion process.2.
Efficacy: Data fusion between a capable IR system anda very incapable IR system may not give betterperformance.In our data fusion experiment in Section 5 we observed thatour CHAIN system was our worst performing FSD system.So as the efficacy criteria suggests a better performingchain word representation is needed before furtherimprovements are observed in our combination systemLexDetect.7.
FUTURE WORKThere are many factors which can affect the final chainword representation of a document, ranging from thegreedy nature of the chaining algorithm, to the effectscaused when varying degrees of freedom are used in thisalgorithm (i.e.
system parameters such as the amount ofactivation used in WordNet).
However the single biggestinfluence on the quality of the resultant lexical chains is theknowledge source used to create them.
In other words thequality of our lexical chain formation is directly dependenton the comprehensiveness/complexity of the thesaurus usedto create them.
In the case of WordNet, there are a numberof structural inadequacies that degrade the effectiveness ofour chain representation:1.
Missing semantic links between related words.2.
Inconsistent semantic distances between differentconcepts.3.
Overloaded synsets such as ?being?
which areconnected to a large number of synsets.
These types ofsynsets cause spurious chaining, where an unrelatedword is added to a chain based on a weak yetsemantically close relationship with one of theseoverloaded synsets  (a special case of 2.).4.
No means of correlating the relationship betweenproper nouns and other noun phrases (see Section 5.2).5.
The level of sense granularity used to define wordmeanings in WordNet is often too fine for the chainformation process.All of these factors play a part in reducing theeffectiveness of the disambiguation process and thecomprehensiveness and accuracy of the final chainrepresentation.
A number of these weaknesses arediscussed in previous work on lexical chaining [8, 12].However the last two cases are particularly important whenconsidering the similarity between documents and clustersin the detection process.
As explained in Section 6.2lexical chains are an incomplete means of representingevents in a topic detection application since they fail tocontain information on the proper nouns involved in thediscourse structure of the text.The last case is more a comment on the unsuitability ofWordNet as a knowledge source in this application ratherthan as a reference to any specific weakness in its design.For example consider two distinct documents which bothcontain the word ?city?
in their respective chainrepresentations.
WordNet defines three distinct meaningsor senses of this word:?
An incorporated administrative district establish by astate charter.?
A large densely populated municipality.?
An urban center.When disambiguating a word like ?city?
in the chainformation process this level of sense distinction isunnecessary.
In fact if our aforementioned documents havechosen two different yet closely related definitions of thisword (i.e.
different synset numbers) then these documentswill be considered less related than they actually are.
Otherresearch efforts in the lexical chaining area have suggested?cleaning?
WordNet [8] of rare senses or using someadditional knowledge source in the chaining process thatcould biases the suitability of certain senses in particularcontexts3.
In future work we hope to address this problemby considering the use of collocation information like nounpairs such as ?physician/hospital?
or ?Gates/Microsoft?
inthe chain formation process.
Using such information willhelp to smooth out the discrepancies in semantic distancesbetween concepts and help detect missing semanticrelationships between these concepts.
This occurrenceinformation could also reduce the sensitivity of thedetection process to fine levels of sense granularity if suchinformation was used when determining the similaritybetween two document representations.
So effectively thistechnique would eliminate the need for a compositerepresentation in the identification of novel events in anews stream.
Instead the data fusion element of our systemwould involve supplementing our knowledge sourceWordNet with word co-occurrence information in the chainformation process.8.
CONCLUSIONSA variety of techniques for data fusion have been proposedin IR literature.
Results from data fusion research havesuggested that significant improvements in systemeffectiveness can be obtained by combining multiplesources of evidence of relevancy such as documentrepresentations, query formulations and search strategies.3Recent editions of WordNet now contain information on theprobability of use of a word based on polysemy.
WordNetresearchers noted the direct relationship between the increase inthe frequency of occurrence of a word and the number ofdistinct meanings it has.
This frequency value could also beused in the ?cleaning?
process.In this paper we investigated the impact on FSDperformance when a composite document representation isused in this TDT task.
Our results showed that a marginalincrease in system effectiveness could be achieved whenlexical chain representations were used in conjunction withfree text representations.
In particular, we saw that the missrate of our FSD system LexDetect, decreased with little orno impact to the false alarm rate of the system.
When aweighted combination of evidence was used on the samesystem this improvement was even more apparent.
Fromthese results we deduced that using our chain wordrepresentation as stronger evidence in the classificationprocess could lead to improved performance.
Based on Ngand Kantor?s dissimilarity criteria for successful data fusionwe attributed the success of our composite documentrepresentation to the fact that a chain word classifier issufficiently dissimilar to a simple ?bag of words?
classifierto contribute additional evidence to a combinationexperiment involving both these representations.
In futureexperiments, we expect an even greater improvement inFSD effectiveness as we continue to refine our lexicalchain representation.9.
ACKNOWLEDGMENTThis project is funded by an Enterprise Ireland researchgrant [SC/1999/083].10.
REFERENCES[1] R. Papka, J. Allan, Topic Detection and Tracking: EventClustering as a basis for first story detection, KluwerAcademic Publishers, pp.
97-126, 2000.
[2] Y. Yang, T. Ault, T. Pierce, Combining multiple learningstrategies for effective cross validation, the Proceedings ofthe 17th International Conference on Machine Learning(ICML), pp.
1167-1182, 2000.
[3] F. Walls, H. Jin, S.Sista, R. Schwartz, Topic Detection inbroadcast news, In the proceedings of the DARPA BroadcastNews Workshop, pp.
193-198, San Francisco, CA: MorganKaufman Publishers Inc, 1999.
[4] F. Fukumoto, Y. Suzuki, Event Tracing based on DomainDependency, In the proceedings of the 23rd ACM SIGIRConference, Athens, pp.
57-63, 2000.
[5] V. Hatzivassiloglou, L. Gravano, A. Maganti, AnInvestigation of Linguistic Features and ClusteringAlgorithms for Topical Document Clustering, In theproceedings of the 23rd ACM SIGIR Conference, Athens, pp.224-231, 2000.
[6] J. Morris, G. Hirst, Lexical Cohesion by Thesaural Relationsas an Indicator of the Structure of Text, ComputationalLinguistics 17(1), March 1991.
[7] M. Halliday, R. Hasan, Cohesion in English, Longman:1976.
[8] S. J.
Green, Automatically Generating Hypertext ByComparing Semantic Similarity, University of Toronto,Technical Report number 366, October 1997.
[9] R. Barzilay, M. Elhadad, Using Lexical Chains for TextSummarization, In Proceedings of the Intelligent ScalableText Summarization Workshop (ISTS?97), ACL, Madrid,1997.
[10] D. St-Onge, Detection and Correcting Malapropisms withLexical Chains, Dept.
of Computer Science, University ofToronto, M.Sc Thesis, March 1995.
[11] M. A. Stairmand, W. J.
Black, Conceptual and ContextualIndexing using WordNet-derived Lexical Chains, In theProceedings of BCS IRSG Colloquium, pp.
47-65, 1997.
[12] M. Okumura, T. Honda, Word sense disambiguation and textsegmentation based on lexical cohesion, In Proceedings ofthe Fifteen Conference on Computational Linguistics(COLING-94), volume 2, pp.
755-761, 1994.
[13] N. Stokes, P. Hatch, J. Carthy, Topic Detection, a newapplication for lexical chaining?, In the Proceedings of the22nd BCS IRSG Colloquium on Information Retrieval, pp.94-103, 2000.
[14] E. Fox, G. Nunn, W. Lee, Coefficients for combining conceptclasses in a collection, In the proceedings of the 11th ACMSIGIR Conference, pp.
291-308, 1988.
[15] J. Katzer, M. McGill, J. Tessier, W. Frakes, P. DasGupta, Astudy of the overlap among document representations,Information Technology: Research and Development,1(4):261-274, 1982.
[16] K. Ng, P. Kantor, An Investigation of the preconditions foreffective data fusion in IR: A pilot study, In the Proceedingsof the 61th Annual Meeting of the American Society forInformation Science 1998.
