Automatic diacritization of Arabic for Acoustic Modeling inSpeech RecognitionDimitra VergyriSpeech Technology and Research Lab.,SRI International,Menlo Park, CA 94025, USAdverg@speech.sri.comKatrin KirchhoffDepartment of Electrical Engineering,University of Washington,Seattle, WA 98195, USAkatrin@ee.washington.eduAbstractAutomatic recognition of Arabic dialectal speech isa challenging task because Arabic dialects are es-sentially spoken varieties.
Only few dialectal re-sources are available to date; moreover, most avail-able acoustic data collections are transcribed with-out diacritics.
Such a transcription omits essen-tial pronunciation information about a word, suchas short vowels.
In this paper we investigate var-ious procedures that enable us to use such train-ing data by automatically inserting the missing dia-critics into the transcription.
These procedures useacoustic information in combination with differentlevels of morphological and contextual constraints.We evaluate their performance against manually dia-critized transcriptions.
In addition, we demonstratethe effect of their accuracy on the recognition perfor-mance of acoustic models trained on automaticallydiacritized training data.1 IntroductionLarge-vocabulary automatic speech recognition(ASR) for conversational Arabic poses severalchallenges for the speech research community.The most difficult problems in developing highlyaccurate speech recognition systems for Arabicare the predominance of non-diacritized textmaterial, the enormous dialectal variety, andthe morphological complexity.Most available acoustic training material forArabic ASR is transcribed in the Arabic scriptform, which does not include short vowels andother diacritics that reflect differences in pro-nunciation, such as the shadda, tanween, etc.
Inparticular, almost all additional text data thatcan easily be obtained (e.g.
broadcast news cor-pora) is represented in standard script form.
Toour knowledge, the only available corpus thatdoes include detailed phonetic information isthe CallHome (CH) Egyptian Colloquial Ara-bic (ECA) corpus distributed by the Linguis-tic Data Consortium (LDC).
This corpus hasbeen transcribed in both the script form anda so-called romanized form, which is an ASCIIrepresentation that includes short vowels andother diacritic information and thus has com-plete pronunciation information.
It is quitechallenging to create such a transcription: na-tive speakers of Arabic are not used to writingtheir language in a ?romanized?
form, or even infully diacritized script form.
Consequently, thistask is considered almost as difficult as phonetictranscription.
Transcribing a sufficiently largeamount of training data in this way is there-fore labor-intensive and costly since it involves(re)-training native speakers for this purpose.The constraint of having mostly non-diacritized texts as recognizer training materialleads to problems for both acoustic and lan-guage modeling.
First, it is difficult to trainaccurate acoustic models for short vowels iftheir identity and location in the signal is notknown.
Second, the absence of diacritics leadsto a larger set of linguistic contexts for a givenword form; language models trained on non-diacritized material may therefore be less pre-dictive than those trained on diacritized texts.Both of these factors may lead to a loss inrecognition accuracy.
Previous work (Kirchhoffet al, 2002; Lamel, 2003) has shown that ig-noring available vowel information does indeedlead to a significant increase in both languagemodel perplexity and word error rate.
There-fore, we are interested in automatically deriv-ing a diacritized transcription from the Arabicscript representation when a manual diacritiza-tion is not available.
Some software companies(Sakhr, Apptek, RDI) have developed commer-cial products for the automatic diacritization ofArabic.
However, these products use only text-based information, such as the syntactic contextand possible morphological analyses of words, topredict diacritics.
In the context of diacritiza-tion for speech recognition, by contrast, acous-tic data is available that can be used as an ad-ditional knowledge source.
Moreover, commer-cial products concentrate exclusively on ModernStandard Arabic (MSA), whereas a common ob-jective of Arabic ASR is conversational speechrecognition, which is usually dialectal.
For thisreason, a more flexible set of tools is requiredin order to diacritize dialectal Arabic prior tospeech recognizer training.In this work we investigate the relative ben-efits of a variety of knowledge sources (acous-tic, morphological, and contextual) to automat-ically diacritize MSA transcriptions.
We eval-uate the different approaches in two differentways: (a) by comparing the automatic outputagainst a manual reference diacritization andcomputing the diacritization error rate, and (b)by using automatically diacritized training datain a cross-dialectal speech recognition applica-tion.The remainder of this paper is structured asfollows: Section 2 gives a detailed description ofthe motivation as well as prior work.
Section 3describes the corpora used for the experimentsreported in this paper.
The automatic diacriti-zation procedures and results are explained inSection 4.
The speech recognition experimentsand results are reported in Section 5.
Section 6presents our conclusions.2 Motivation and Prior WorkWe first describe the Arabic writing systemand its inherent problems for speech recognizertraining, and then discuss previous attempts atautomatic diacritization.2.1 The Arabic Writing SystemThe Arabic alphabet consists of twenty-eightletters, twenty-five of which represent conso-nants and three of which represent the longvowels (/i:/,/a:/,/u:/).
A distinguishing fea-ture of Arabic-script based writing systems isthat short vowels are not represented by theletters of the alphabet.
Instead, they aremarked by so-called diacritics, short strokesplaced either above or below the preceding con-sonant.
Several other pronunciation phenom-ena are marked by diacritics, such as consonantdoubling (phonemic in Arabic), which is indi-cated by the ?shadda?
sign, and the ?tanween?,i.e.
word-final adverbial markers that add /n/ tothe pronunciation of the word.
These diacriticsare listed in Table 1.
Arabic texts are almostnever fully diacritized; normally, diacritics areused sparingly and only to prevent misunder-standings.
Exceptions are important religiousand/or political texts or beginners?
texts forMSA Symbol Name Meaning@ fatHa /a/@kasra /i/@ Damma /u/P shadda consonant doubling?PX sukuun vowel absence@ tanween al-fatHa /an/@tanween al-kasr /in/@ tanween aD-Damm /un/Table 1: Arabic diacriticsstudents of Arabic.
The lack of diacritics maylead to considerable lexical ambiguity that mustbe resolved by contextual information, whichin turn presupposes knowledge of the language.It was observed in (Debili et al, 2002) thata non-diacritized dictionary word form has 2.9possible diacritized forms on average and thatan Arabic text containing 23,000 word formsshowed an average ratio of 1:11.6.
The formI.J?, for instance, has 21 possible diacritiza-tions.
The correspondence between graphemesand phonemes is relatively transparent com-pared to other languages like English or French:apart from certain special graphemes (e.g.
laamalif), the relationship is one to one.
Finally,it is worth noting that the writing system de-scribed above is that of MSA.
Arabic dialectsare primarily oral varieties in that they do nothave generally agreed-upon writing standards.Whenever there is the need to write down di-alectal speech, speakers will try to approximatethe standard system as far as possible and use aphonetic spelling for non-MSA or foreign words.The lack of diacritics in standard Arabic textsmakes it difficult to use non-diacritized text fortraining since the location and identity of shortvowels and other phonetic segments are un-known.
One possible approach is to use acous-tic models for long vowels and consonants only,where the acoustic signal portions correspond-ing to unwritten segments are implicitly incor-porated into the acoustic models for consonants(Billa et al 2002).
However, this leads to lessdiscriminative acoustic and language models.Previous work (Kirchhoff et al, 2002; Lamel,2003) has compared the word error rates oftwo CH ECA recognizers: one trained on scripttranscriptions and another trained on roman-ized transcriptions.
It was shown that the lossin information due to training on script formsresults in significantly worse performance: a rel-ative increase in word error rate of almost 10%was observed.It seems clear that diacritized data should beused for training Arabic ASR systems wheneverpossible.
As explained above, however, it is veryexpensive to obtain manually transcribed datain a diacritized form.
Therefore, the corporathat do include detailed transcriptions are fairlysmall and any dialectal data that might becomeavailable in the future will also very likely beof limited size.
By contrast, it is much easierto collect publicly available data (e.g.
broadcastnews data) and to transcribe it in script form.In order to be able to take advantage of suchresources, we need to restore short vowels andother missing diacritics in the transcription.2.2 Prior WorkVarious software companies have developedautomatic diacritization products for Arabic.However, all of these are targeted towards MSA;to our knowledge, there are no products for di-alectal Arabic.
In a previous study (Kirchhoffet al, 2002) one of these products was testedon three different texts, two MSA texts and oneECA text.
It was found that the diacritizationerror rate (percentage of missing and wronglyidentified or inserted diacritics) on MSA rangedbetween 9% and 28%, depending on whether ornot case vowel endings were counted.
However,on the ECA text, the diacritization software ob-tained an error rate of 48%.A fully automatic approach to diacritizationwas presented in (Gal, 2002), where an HMM-based bigram model was used for decodingdiacritized sentences from non-diacritized sen-tences.
The technique was applied to the Quranand achieved 14% word error (incorrectly dia-critized words).A first attempt at developing an automaticdiacritizer for dialectal speech was reported in(Kirchhoff et al, 2002).
The basic approachwas to use a small set of parallel script and dia-critized data (obtained from the ECA CallHomecorpus) and to derive diacritization rules in anexample-based way.
This entirely knowledge-free approach achieved a 16.6% word error rate.Other studies (El-Imam, 2003) have ad-dressed problems of grapheme-to-phoneme con-version in Arabic, e.g.
for the purpose of speechsynthesis, but have assumed that a fully dia-critized version of the text is already available.Several knowledge sources are available fordetermining the most appropriate diacritizationof a script form: analysis of the morphologicalstructure of the word (including segmentationinto stems, prefixes, roots and patterns), con-sideration of the syntactic context in which theword form appears, and, in the context of speechrecognition, the acoustic data that accompaniesthe transcription.
Specific dictionary informa-tion could in principle be added (such as infor-mation about proper names), but this knowl-edge source is ignored for the purpose of thisstudy.
All of the approaches described abovemake use of text-based information only and donot attempt to use acoustic information.3 DataFor the present study we used two different cor-pora, the FBIS corpus of MSA speech and theLDC CallHome ECA corpus.The FBIS corpus is a collection of radio news-casts from various radio stations in the Ara-bic speaking world (Cairo, Damascus, Bagh-dad) totaling approximately 40 hours of speech(roughly 240K words).
The transcription of theFBIS corpus was done in Arabic script onlyand does not contain any diacritic information.There were a total of 54K different script forms,with an average of 2.5 different diacritizationsper word.The CallHome corpus, made available byLDC, consists of informal telephone conversa-tions between native speakers (friends and fam-ily members) of Egyptian Arabic, mostly fromthe Cairene dialect region.
The corpus con-sists of about 20 hours of training data (roughly160K words) and 6 hours of test data.
It is tran-scribed in two different ways: (a) using stan-dard Arabic script, and (b) using a romaniza-tion scheme developed at LDC and distributedwith the corpus.
The romanized transcriptioncontains short vowels and phonetic segmentscorresponding to other diacritics.
It is not en-tirely equivalent to a diacritized Arabic scriptrepresentation since it includes additional in-formation.
For instance, symbols particular toEgyptian Arabic were used (e.g.
?g?
for /g/,the ECA pronunciation of the MSA letter `),whereas the script transcriptions contain MSAletters only.
In general, the romanized tran-scription provides more information about ac-tual pronunciation and is thus closer to a broadphonetic transcription.4 Automatic DiacritizationWe describe three techniques for the automaticdiacritization of Arabic text data.
The firstcombines acoustic, morphological and contex-tual information to predict the correct form, thesecond ignores contextual information, and thethird is fully acoustics based.
The latter tech-nique uses no morphological or syntactic con-straints, and allows for all possible items to beinserted at every possible position.4.1 Combination of Acoustic,Morphological and ContextualInformationMost Arabic script forms can have a numberof possible morphological interpretations, whichoften correspond to different diacritized forms.Our goal is to combine morphological knowledgewith contextual information in order to identifypossible diacritizations and assign probabilitiesto them.
Our procedure is as follows:1.
Generate all possible diacritized variantsfor each word, along with their morphologicalanalyses (tags).2.
Train an unsupervised tagger to assignprobabilities to sequences of these morpholog-ical tags.3.
Use the trained tagger to assign proba-bilities to all possible diacritizations for a givenutterance.For the first step we used the Buckwalterstemmer, which is an Arabic morphologicalanalysis tool available from the LDC.
The stem-mer produces all possible morphological anal-yses of a given Arabic script form; as a by-product it also outputs the concomitant dia-critized word forms.
An example of the outputis shown in Figure 1.
The next step was to trainan unsupervised tagger on the output to obtaintag n-gram probabilities.
The number of differ-ent morphological tags generated by applyingthe stemmer to the FBIS text was 763.
In or-der to obtain a smaller tag set and to be ableto estimate probabilities for tag sequences morerobustly, this initial tag needed to be conflatedto a smaller set.
We adopted the set used inthe LDC Arabic TreeBank project, which wasalso developed based on the Buckwalter mor-phological analysis scheme.
The FBIS tags weremapped to TreeBank tags using longest com-mon substring matching; this resulted in 392tags.
Further possible reductions of the tagset were investigated but it was found that toomuch clustering (e.g.
of verb subclasses into aLOOK-UP WORD: ?J.?
(qbl)SOLUTION 1: (qabola) qabola/PREP(GLOSS): + before +SOLUTION 2: (qaboli) qaboli/PREP(GLOSS): + before +SOLUTION 3: (qabolu) qabolu/ADV(GLOSS): + before/prior +SOLUTION 4:(qibal) qibal/NOUN(GLOSS): + (on the) part of +SOLUTION 5:(qabila)qabil/VERB PERFECT+a/PVSUFF SUBJ:3MS(GLOSS): + accept/receive/approve + he/it <verb>SOLUTION 6: (qab?ala)qab al/VERB PERFECT+a/PVSUFF SUBJ:3MS(GLOSS): + kiss + he/it <verb>Figure 1: Sample output of Buckwalter stem-mer showing the possible diacritizations andmorphological analyses of the script form ?J.?(qbl).
Lower-case o stands for sukuun (lack ofvowel).single verb class) could result in the loss of im-portant information.
For instance, the tenseand voice features of verbs are strong predictorsof the short vowel patterns and should thereforebe preserved in the tagset.We adopted a standard statistical trigramtagging model:P (t0, .
.
.
, tn|w0, .
.
.
, wn) =n?i=0P (wi|ti)P (ti|ti?1, ti?2) (1)where t is a tag, w is a word, and n is the to-tal number of words in the sentence.
In thismodel, words (i.e.
non-diacritized script forms)and morphological tags are treated as observedrandom variables during training.
Training isdone in an unsupervised way, i.e.
the correctmorphological tag assignment for each word isnot known.
Instead, all possible assignmentsare initially considered and the Expectation-Maximization (EM) training procedure itera-tively trains the probability distributions in theabove model (the probability of word giventag, P (wi|ti), and the tag sequence probabil-ity, P (ti|ti?1, ti?2)) until convergence.
Duringtesting, only the word sequence is known andthe best tag assignment is found by maximiz-ing the probability in Equation 1.
We used thegraphical modeling toolkit GMTK (Bilmes andZweig, 2002) to train the tagger.
The trainedtagger was then used to assign probabilities toall possible sequences of three successive mor-phological tags and their associated diacritiza-tions to all utterances in the FBIS corpus.Using the resulting possible diacritizationsfor each utterance we constructed a word-pronunciation network with the probabilityscores assigned by the tagger acting as transi-tion weights.
These word networks were usedas constraining recognition networks with theacoustic models trained on the CallHome cor-pus to find the most likely word sequence (aprocess called alignment).
We performed thisprocedure with different weights on the taggerprobabilities to see how much this informationshould be weighted compared to the acousticscores.
Results for weights 1 and 5 are reportedbelow.Since the Buckwalter stemmer does not pro-duce case endings, the word forms obtainedby adding case endings were included as vari-ants in the pronunciation dictionary used by thealigner.
Additional variants listed in the dictio-nary are the taa marbuta alternations /a/ and/at/.
In some cases (approximately 1.5% of allwords) the Buckwalter stemmer was not able toproduce an analysis of the word form due to mis-spellings or novel words.
These were mapped toa generic reject model.4.2 Combination of Acoustic andMorphological ConstraintsWe were interested in separately evaluating theusefulness of the probabilistic contextual knowl-edge provided by the tagger, and the morpho-logical knowledge contributed by the Buckwal-ter tool.
To that end we used the word networksproduced by the method described above butstripped the tagger probabilities, thus assigninguniform probability to all diacritized forms pro-duced by the morphological analyzer.
We usedthe same acoustic models to find the most likelyalignment from the word networks.4.3 Using only Acoustic InformationSimilarly, we wanted to evaluate the importanceof using morphological information versus onlyacoustic information to constrain the possiblediacritizations.
This is particularly interestingsince, as new dialectal speech data become avail-able, the acoustics may be the only informa-tion source.
As explained above, existing mor-phological analysis tools such as the Buckwalterstemmer have been developed for MSA only.For that purpose, we generated word net-works that include all possible short vowels ateach allowed position in the word and allowedall possible case endings.
This means that af-ter every consonant there are at least 5 dif-ferent choices: no vowel (corresponding to thesukuun diacritic), /i/, /a/, /u/, or consonantdoubling caused by a shadda sign.
Combina-tions of shadda and a short vowel are also pos-sible.
Since we do not use acoustic models fordoubled consonants in our speech recognizer, weignore the variants involving shadda and allowonly four possibilities after every word-medialconsonant: the three short vowels or absence ofa vowel.
Finally, we include the three tanweenendings in addition to these four possibilities inword-final position.
As before, the taa marbutavariants are also included.In this way, many more possible ?pronuncia-tions?
are generated for a script form than couldever occur.
The number of possible variants in-creases exponentially with the number of pos-sible vowel slots in the word.
For instance, fora longer word with 7 possible positions, morethan 16K diacritized forms are possible, noteven counting the possible word endings.
As be-fore, we use these large pronunciation networksto constrain our alignment with acoustic modelstrained on CallHome data and choose the mostlikely path as the output diacritization.In principle it would also be possible to deter-mine diacritization performance in the absenceof acoustic information, using only morphologi-cal and contextual knowledge.
This can be doneby selecting the best path from the weightedword transition networks without rescoring thenetwork with acoustic models.
However, thiswould not lead to a valid comparison in our casebecause case endings are only represented in thepronunciation dictionary used by the acousticaligner; they are not present in the weightedtransition network and thus cannot be hypoth-esized unless the acoustic aligner is used.4.4 Autodiacritization Error RatesWe measured the performance of all three meth-ods by comparing the output against hand tran-scribed references on a 500 word subset of theFBIS corpus.
These references were fully dia-critized script transcriptions created by a na-tive speaker of Arabic who was trained in or-thographic transcription but not in phonetictranscription.
The diacritization error rate wasmeasured as the percentage of wrong diacritiza-tion decisions out of all possible decisions.
Inparticular, an error occurs when:?
a vowel is inserted although the referencetranscription shows either sukuun or no dia-critic mark at the corresponding position (in-sertion).?
no vowel is produced by the automatic pro-cedure but the reference contains a vowel markat the corresponding position (deletion).?
the short vowel inserted does not match thevowel at the corresponding position (substitu-tion).?
in the case of tanween and taa marbuta end-ings, either the required consonants or vowelsare missing or wrongly inserted.
Thus, in thecase of a taa marbuwta ending with a followingcase vowel /i/, for instance, both the /t/ andthe /i/ need to be present.
If either is missing,one error is assigned; if both are missing, twoerrors are assigned.Results are listed in Table 2.
The first columnreports the error rate at the word level, i.e.
thepercentage of words that contained at least onediacritization mistake.
The second column liststhe diacritization error computed as explainedabove.
The first three methods have a very sim-ilar performance with respect to diacritizationerror rate.
The use of contextual information(the tagger probabilities) gives a slight advan-tage, although the difference is not statisticallysignificant.
Despite these small differences, theword error rate is the same for all three meth-ods; this is because a word that contains at leastone mistake is counted as a word error, regard-less of the total number of mistakes in the word,which may vary from system to system.
Usingonly acoustic information doubles the diacriti-zation error rate and increases the word errorrate to 50%.
Errors result mostly from incorrectinsertions of vowels (e.g.
X@Y?K.?
X@Y?K.).
Manyof these insertions may stem from acoustic ef-fects created by neighbouring consonants, thatgive a vowel-like quality to transitions betweenconsonants.
The main benefit of using morpho-logical knowledge lies in the prevention of suchspurious vowel insertions, since only those inser-tions are permitted which result in valid words.Even without the use of morphological infor-mation, the vast majority of the missing vowelsare still identified correctly.
Thus, this methodmight be of use when diacritizing a variety ofArabic for which morphological analysis toolsare not available.
Note that the results obtainedhere are not directly comparable to any of theworks described in Section 2.2 since we used adata set with a much larger vocabulary size.Word CharacterInformation used level levelacoustic + morphological+ contextual 27.3 13.24(tagger prob.
weight=5)acoustic + morphological+ contextual 27.3 11.54(tagger prob.
weight=1)acoustic + morphological(tagger prob.
weight=0) 27.3 11.94acoustic only 50.0 23.08Table 2: Automatic diacritization error rates(%).5 ASR ExperimentsOur overall goal is to use large amounts of MSAacoustic data to enrich training material for aspeech recognizer for conversational EgyptianArabic.
The ECA recognizer was trained on theromanized transcription of the CallHome cor-pus described above and uses short vowel mod-els.
In order to be able to use the phoneticallydeficient MSA transcriptions, we first need toconvert them to a diacritized form.
In additionto measuring autodiacritization error rates, asabove, we would like to evaluate the differentdiacritization procedures by investigating howacoustic models trained on the different outputsaffect ASR performance.One motivation for using cross-dialectal datais the assumption that infrequent triphones inthe CallHome corpus might have more trainingsamples in the larger MSA corpus.
In (Kirch-hoff and Vergyri, 2004) we demonstrated thatit is possible to get a small improvement in thistask by combining the scores of models trainedstrictly on CallHome (CH) with models trainedon the combined FBIS+CH data, where theFBIS data was diacritized using the method de-scribed in Section 4.1.
Here we compare that ex-periment with the experiments where the meth-ods described in Sections 4.2 and 4.3 were usedfor diacritizing the FBIS corpus.5.1 Baseline SystemThe baseline system was trained with onlyCallHome data (CH-only).
For these exper-iments we used a single front-end (13 mel-frequency cepstral coefficients with first andsecond differences).
Mean and variance aswell as Vocal Tract Length (VTL) normaliza-tion were performed per conversation side forCH and per speaker cluster (obtained auto-matically) for FBIS.
We trained non-crossword,System dev96 eval03simple CH-only 56.1 42.7RT-2003 CH-only 52.6 39.7Table 3: CH-only baseline WER (%)continuous-density, genonic hidden Markovmodels (HMMs) (Digalakis and Murveit, 1994),with 128 gaussians per genone and 250 genones.Recognition was done by SRI?s DECIPHERTMengine in a multipass approach: in the firstpass, phone-loop adaptation with two Max-imum Likelihood Linear Regression (MLLR)transforms was applied.
A recognition lexiconwith 18K words and a bigram language modelwere used to generate the first pass recogni-tion hypothesis.
In the second pass the acousticmodels were adapted using constrained MLLR(with 6 transformations) based on the previ-ous hypothesis.
Bigram lattices were generatedand then expanded using a trigram languagemodel.
Finally, N-best lists were generated us-ing the adapted models and the trigram lattices.The final best hypothesis was obtained using N-best ROVER (?).
This system is simpler thanour best current recognition system (submittedfor the NIST RT-2003 benchmark evaluations)(Stolcke et al, 2003) since we used a single frontend (instead of a combination of systems basedon different front ends) and did not includeHLDA, cross-word triphones, MMIE trainingor a more complex language model.
The lackof these features resulted in a higher error ratebut our goal here was to explore exclusively theeffect of the additional MSA training data us-ing different diacritization approaches.
Table 3shows the word error rates of the system usedfor these experiments and the full system usedfor the NIST RT-03 evaluations.
Our full sys-tem was about 2% absolute worse than the bestsystem submitted for that task.
This shows thateven though the system is simpler we are notoperating far from the state-of-the-art perfor-mance for this task.5.2 ASR Systems Using FBIS DataIn order to investigate the effect of additionalMSA training data, we trained a system similarto the baseline but used training data pooledfrom both corpora (CH+FBIS).
After perform-ing alignment of the FBIS data with the net-works described in Section 4.1, 10% of the datawas discarded since no alignments could befound.
This could be due to segmentation prob-lems or noise in the acoustic files.
The remain-ing 90% were used for our experiments.
In or-der to account for the fact that we had muchmore data, and also more dissimilar data, weincreased the model size to 300 genones.For training the CH+FBIS acoustic models,we first used the whole data set with weight2 for CH utterances and 1 for FBIS utterances.Models were then MAP adapted on the CH-onlydata (Digalakis et al, 1995).
Since training in-volves several EM iterations, we did not wantto keep the diacritization fixed from the firstpass, which used CH-only models.
At every it-eration, we obtain better acoustic models whichcan be used to re-align the data.
Thus, for thefirst two approaches, where the size of the pro-nunciation networks is limited due to the useof morphological information, the EM forward-backward counts were collected using the wholediacritization network and the best diacritiza-tion path was allowed to change at every iter-ation.
In the last case, where only acoustic in-formation was used, the pronunciation networkswere too large to be run efficiently.
For this rea-son, we updated the diacritized references onceduring training by realigning the networks withthe newer models after the first training iter-ation.
As reported in (Kirchhoff and Vergyri,2004) the CH+FBIS trained system by itself didnot improve much over the baseline (we onlyfound a small improvement on the eval03 test-set) but it provided sufficiently different infor-mation, so that ROVER combination (Fiscus,1997) with the baseline yielded an improvement.As we can see in Table 4, all diacritization pro-cedures performed practically the same: therewas no significant difference in the word errorrates obtained after the combination with theCH-only baseline.
This suggests that we maybe able to obtain improvements with automat-ically diacritized data even when using inaccu-rate diacritization, produced without the use ofmorphological constraints.6 ConclusionsIn this study we have investigated different op-tions for automatically diacritizing Arabic textfor use in acoustic model training for ASR.
Acomparison of the different approaches showedthat more linguistic information (morphologyand syntactic context) in combination withthe acoustics provides lower diacritization er-ror rates.
However, there is no significant dif-ference among the word error rates of ASR sys-dev96 eval03System alone Rover with CH-only alone Rover with CH-onlyCH-only 56.1 42.7CH+FBIS1(weight 1) 56.3 55.3 42.2 41.6CH+FBIS1(weight 5) 56.1 55.2 42.2 41.8CH+FBIS2 56.2 55.3 42.4 41.6CH+FBIS3 56.6 55.7 42.1 41.6Table 4: Word error rates (%) obtained after the final recognition pass and with ROVER combina-tion with the baseline system.
FBIS1, FBIS2 and FBIS3 correspond to the diacritization proceduresdescribed in Sections 4.1, 4.2 and 4.3 respectively.
For the first approach we report results usingthe tagger probabilities with weights 1 and 5.tems trained on data resulting from the differentmethods.
This result suggests that it is pos-sible to use automatically diacritized trainingdata for acoustic modeling, even if the data hasa comparatively high diacritization error rate(23% in our case).
Note, however, that onereason for this may be that the acoustic mod-els are finally adapted to the accurately tran-scribed CH-only data.
In the future, we plan toapply knowledge-poor diacritization proceduresto other dialects of Arabic, for which morpho-logical analyzers do not exist.7 AcknowledgmentsThis work was funded by DARPA under con-tract No.
MDA972-02-C-0038.
We are gratefulto Kathleen Egan for making the FBIS corpusavailable to us, and to Andreas Stolcke and JingZheng for valuable advice on several aspects ofthis work.ReferencesJ.
Billa et al 2002.
Audio indexing of Broad-cast News.
In Proceedings of ICASSP.J.
Bilmes and G. Zweig.
2002.
The GraphicalModels Toolkit: An open source software sys-tem for speech and time-series processing.
InProceedings of ICASSP.F.
Debili, H. Achour, and E Souissi.
2002.
Del?e?tiquetage grammatical a` la voyellation au-tomatique de l?arabe.
Technical report, Cor-respondances de l?Institut de Recherche surle Maghreb Contemporain.V.
Digalakis and H. Murveit.
1994.GENONES: Optimizing the degree ofmixture tying in a large vocabulary hiddenmarkov model based speech recognizer.
InProceeding of ICASSP, pages I?537?540.V.V.
Digalakis, D. Rtischev, and L. G.Neumeyer.
1995.
Speaker adaptation usingconstrained estimation of gaussian mixtures.IEEE Transactions SAP, 3:357?366.Yousif A. El-Imam.
2003.
Phonetization ofArabic: rules and algorithms.
Computer,Speech and Language, in press, preprint avail-able online at www.sciencedirect.com.J.
G. Fiscus.
1997.
A post-processing systemto yield reduced word error rates: Recognizeroutput voting error reduction (ROVER).
InProceedings IEEE Automatic Speech Recog-nition and Understanding Workshop, pages347?352, Santa Barbara, CA.Ya?akov Gal.
2002.
An HMM approach to vowelrestoration in Arabic and Hebrew.
In Pro-ceedings of the Workshop on ComputationalApproaches to Semitic Languages, pages 27?33, Philadelphia, July.
Association for Com-putational Linguistics.K.
Kirchhoff and D. Vergyri.
2004.
Cross-dialectal acoustic data sharing for Ara-bic speech recognition.
In Proceedings ofICASSP.K.
Kirchhoff, J. Bilmes, J. Henderson,R.
Schwartz, M. Noamany, P. Schone, G. Ji,S.
Das, M. Egan, F. He, D. Vergyri, D. Liu,and N. Duta.
2002.
Novel approaches to Ara-bic speech recognition - final report from theJHU summer workshop 2002.
Technical re-port, Johns Hopkins University.L.
Lamel.
2003.
Personal communication.A.
Stolcke, Y. Konig, and M. Weintraub.
1997.Explicit word error minimization in N-bestlist rescoring.
In Proceedings of Eurospeech,volume 1, pages 163?166.A.
Stolcke et al 2003.
Speech-to-text re-search at sri-icsi-uw.
Technical report, NISTRT-03 Spring Workshop.
availble onlinehttp://www.nist.gov/speech/tests/rt/rt2003/spring/presentations/sri+-rt03-stt.pdf.
