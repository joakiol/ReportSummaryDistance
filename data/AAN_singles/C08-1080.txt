Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 633?640Manchester, August 2008Computer aided correction and extension of a syntactic wide-coveragelexiconLionel NICOLAS?, Beno?
?t SAGOT?, Miguel A. MOLINERO?,Jacques FARR?E?,?Eric DE LA CLERGERIE?
?Team RL, Laboratory I3S - UNSA + CNRS, 06903 Sophia Antipolis, France{lnicolas, jf}@i3s.unice.fr?Project ALPAGE, INRIA Rocquencourt + Paris 7, 78153 Le Chesnay, France{benoit.sagot, Eric.De La Clergerie}@inria.fr?Grupo LYS, Univ.
de A Coru?na, 15001 A Coru?na, Espa?nammolinero@udc.esAbstractThe effectiveness of parsers based on man-ually created resources, namely a grammarand a lexicon, rely mostly on the qualityof these resources.
Thus, increasing theparser coverage and precision usually im-plies improving these two resources.
Theirmanual improvement is a time consumingand complex task : identifying which re-source is the true culprit for a given mis-take is not always obvious, as well as find-ing the mistake and correcting it.Some techniques, like van Noord (2004)or Sagot and Villemonte de La Clergerie(2006), bring a convenient way to automat-ically identify forms having potentially er-roneous entries in a lexicon.
We have in-tegrated and extended such techniques in awider process which, thanks to the gram-mar ability to tell how these forms couldbe used as part of correct parses, is able topropose lexical corrections for the identi-fied entries.We present in this paper an implementa-tion of this process and discuss the main re-sults we have obtained on a syntactic wide-coverage French lexicon.1 IntroductionIncreasing the coverage and precision of non-trained parsers based on manually created gram-c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.mar and lexicon, relies mostly on the improvementof these two resources.The manual improvement of wide coverage lin-guistic resources is a labour-extensive, complexand error prone task, requiring an important humanexpert work.In order to minimize human intervention, sim-plify the process and increase its relevance, auto-matic or semi-automatic tools can be used.
Wepresent one such tool, using raw inputs, which de-tects shortcomings of a lexicon and helps correctthem by proposing relevant corrections.Detecting forms erroneously or incompletelydescribed in the lexicon is achieved by applyingtwo techniques which exhibit suspicious forms andassociate them with a set of non parsable sen-tences.Proposing relevant corrections relies on the fol-lowing assumption: when studying the expecta-tions of a grammar for a suspicious form in variousnon-parsable sentences, we can observe expecteduse patterns for this form.
Those patterns can beregarded as possible corrections for the lexicon.
Ina metaphorical way, we believe the problem to bedue to the lexicon1, and we ask the grammar to ex-press possible corrections for the lexicon.The set of techniques we present here is fullysystem and language independent: it can be eas-ily applied to most existing lexica and unification-grammar based parsers.
The only condition is toprovide lexically and grammatically valid inputs,such as law texts or newspapers, in order to ensurethat the rejection of a sentence is only due to errors1We will discuss later lexical forms incorrectly suspectedbecause of errors in other components of the parsing system,notably the grammar.633in some components on which the parser relies on.This paper is organized as follows.
We start bygiving a global view of the whole process (Sect.
2)that we later detail step by step (Sect.
3, 4, 5, 6and 7).
We then compare our work with previ-ously publicated ones (Sect.
8) right before expos-ing the practical context and the results obtained(Sect.
9).
Finally, we outline the planned improve-ments (Sect.
10) and conclude (Sect.
11).2 Global viewA lexical form is generally described in a lexiconwith one or more entries including different kindsof information: the POS (part of speech), morpho-logic features, syntactic features and sometimessemantic features.A form will cause a parsing failure if its descrip-tion in the lexicon leads to a conflict with the gram-mar expectations for this form, i.e., if grammar andlexicon do not agree on a particular instance of theform in a given sentence.For practical reasons, we make a difference be-tween conflicts concerning the POS, that we nowcall POS defect, and conflicts concerning the fea-tures, that we now call overspecification.
POS de-fect generally happen with homonyms, i.e., withforms related to frequent lemmas while a seldomused one is missing in the lexicon.
Overspecifica-tion is usually caused by the difficulty of describ-ing exhaustively all the subcategorization framesof a lemma (optional arguments, polysemy, etc.
).Therefore, if for a given lemma the most restrictiveframes are also the most frequent, some entries canbe overspecified and induce such conflicts.We generate lexical corrections according to thefollowing process.1.
We first detect suspicious forms and associatethem with a set of non-parsable sentences inwhich the form is suspected to be responsibleof the sentences?
parsing failures.2.
We get as close as possible to the set of parsesthat the grammar would have allowed with anerror-free lexicon.
We achieve this goal byunderspecifying the suspicious form, i.e., weincrease the set of its possible POS (that is,by virtually adding new entries to the lexi-con) and/or underspecify the morphologicaland syntactic informations of a given exist-ing entry.
A full underspecification can besimulated in the following way: during theparsing process, each time a lexical informa-tion is checked about the suspicious form, thelexicon is bypassed and all the constraints areconsidered as satisfied.
We actually achievedthis operation by replacing the suspiciousform in the associated sentences with specialforms called wildcards.3.
If the suspicious form has been correctly de-tected, such underspecification increases theparsing rate (except for sentences for whichthe form was not the only problem).
In thenewly successful parses, the form becamewhatever the grammar wanted it to be, i.e., ithas matched any morphological, syntactic orsemantic pattern required.
Those patterns arethe data we use to generate the corrections.We thus extract the instances of the wildcardin the newly producted parses, and after rank-ing, we propose them as corrections.4.
Finally, we manually validate and apply thecorrections.We will now explain with details how each stepis achieved, starting with the detection of suspi-cious forms.3 Detection of suspicious formsIn order to detect erroneous entries in a lexicon, wehave developed and implemented two techniques :a shallow technique to identify POS defects and anextended version of an existing parser-based tech-nique to (mostly) identify overspecification.
Bothprovide for the form a suspicion rate and a set ofassociated non-parsable sentences.3.1 Tagger-based detection of POS defectsThis technique is based on a stochastic tagger.
Theunderlying idea is to generate new POS for formsin the input corpus by using an especially con-figured stochastic tagger (Molinero et al, 2007).Such a tagger considers every form belonging toopen POS (adjectives, common nouns, adverbs,verbs and proper nouns) as unknown.
CandidatePOS for unknown forms are then proposed by thetagger?s guesser and the most likely to be correctare selected by the tagging process itself.
Thus,new POS arise for some forms present in the in-put.To obtain such a tagger, we have used two train-ing sets.
One is a training corpus composed of634manually tagged sentences (330k words) extractedfrom the French Paris 7 Treebank (Abeill?e, 2003),and the other one is composed of a small list oflemmas belonging to closed POS (prepositions,determiners, pronouns and punctuation marks).The tagger was modified so that only lemmaspresent in the second set are considered as known.After applying the tagger on the input corpus,we extracted the produced pairs of form/POS andchecked their presence in the lexicon.
Every nonpresent pair has been proposed as POS defect can-didate.
The emergence of false positives has beensmoothed by sorting the POS defect candidates ac-cording to the following measure:(nwt/nw) ?
log(nwt),where nwtis the number of occurrences of theform w tagged as t and nwis the total number ofoccurrences of the form w.3.2 Parsing-based suspicious forms detectionThe technique described hereafter extends theideas exposed in (Sagot and Villemonte de LaClergerie, 2006), in which suspicious forms are de-tected through a statistical analysis of the parsingsuccess and errors produced by a parser.This error mining technique relies on the follow-ing idea.?
When the parsing of a sentence known tobe lexically and grammatically correct fails,there is no automatic and unquestionable wayto decide if this rejection is caused by an errorin the lexicon or by a flaw in another compo-nent of the parsing system (grammar, etc.).?
Given the parsing results of a large corpusof reliable sentences, the more often a lex-ical form appears in non-parsable sentencesand not in parsable ones, the more likely itis that its lexical entries are erroneous.
Thissuspicion is reinforced if it appears in non-parsable sentences together with forms thatappear mostly in parsable ones.The statistical computation establishes a rele-vant list of lexical forms that are likely to be in-correctly or incompletely described in the lexicon.As such, the main drawback of this approachis the dependence to the quality of the grammarused.
Indeed, if a specific form is naturally tiedwith some syntactic construction non-handled bythe grammar, this form will always be found inrejected sentences and will thus be unfairly sus-pected.
Nevertheless, we limited this drawback byapplying two extensions.The first, already described in (Sagot and Ville-monte de La Clergerie, 2006), mixes the detectionresults obtained from various parsers with differentgrammars, hence with different shortcomings.The second extension detects short-range rep-resentative syntactic patterns non-handled by thegrammar and filters the non-parsable sentenceswhere they appear.
To do so, we reduce every sen-tence to a single POS sequence through the useof a tagger and train a maximum entropy clas-sifier (Daum?e III, 2004) with the different possi-ble trigrams and the corresponding parse failureor success.
Even if non-perfect (the tagger or themaximum entropy classifier might be mistaken atsome point), this pre-filtering has proved to notice-ably increase the quality of the suspicious formsprovided.We will now explain how we manage to permitthe parsing process of the associated non-parsablesentences in order to extract afterwise the correc-tions hypotheses.4 Parsing originally non-parsablesentences with wildcardsAs explained in Sect.
2, in order to generate lexicalcorrections, we first need to get as close as possibleto the set of parses that the grammar would have al-lowed with an error-free lexicon.
We achieve thisgoal by replacing in the associated sentences ev-ery suspicious forms with special underspecifiedforms called wildcards.The simplest way would be to use totally un-derspecified wildcards.
Indeed, this would havethe benefit to cover all kinds of conflicts and thus,it would notably increase the parsing coverage.However, as observed by (Fouvry, 2003), it in-troduces an unnecessary ambiguity which usuallyleads to a severe overgeneration of parses or to noparses at all because of time or memory shortage.In a metaphorical way, we said that we wantedthe grammar to tell us what lexical information itwould have accepted for the suspicious form.
Well,by introducing a totally underspecified wildcard,either the grammar has so many things to say thatit is hard to know what to listen to, or it has somany things to think about that it stutters and doesnot say anything at all.Therefore, we refined the wildcard by introduc-635ing some data.
For technical, linguistic and read-ability reasons, we added POS information.When facing a POS defect, we need the parserto explore other grammar rules than those alreadyvisited during the failed parses.
We thus generatewildcards with different POS than those alreadypresent in the lexicon for the suspicious form.When facing an overspecification, we need theparser to explore the same grammar rules withoutbeing stopped by unification failures.
We thus gen-erate wildcards with the same POS than those al-ready present in the lexicon, but with no feature-level constraints.When suspicious forms were correctly detected,such exchanges usually increases the parsing rateof the associated sentences.
Those parses placethe wildcards in grammatical contexts/patternswhich clearly express what lexical informationsthe grammar would have accepted for the suspi-cious forms.We will now explain how we extract the cor-rection hypotheses from the newly obtained parsesand how we rank them.5 Extracting corrections hypothesesThe extraction directly depends on how one planesto use the correction hypotheses.
In a previouswork (Nicolas et al, 2007), we extracted the cor-rections proposals in the parser?s output format.Such a way to process had three important draw-backs :?
one needed to understand the parser?s outputformat before being able to study the correc-tions;?
merging results produced by various parserswas difficult, although it is an efficient solu-tion to tackle most limitations of the process(see Sect.
6.2);?
some parts of the correction proposals wereusing representations that are not easy to re-late with the format used by the lexicon (spe-cific tagsets, under- or overspecified informa-tion w.r.t.
the lexicon, etc.
).We therefore developed for each of the parsersused a conversion module in order to extract froma given parse the instantiated lexical entry of eachwildcard in the format used by the lexicon.6 Ranking corrections hypothesesNatural languages are ambiguous, and so need tobe the grammars that model them.
For example, inmany romance languages, an adjective can be usedas a noun and a noun as an adjective.Consequently, an inadequate wildcard may per-fectly lead to new parses and provide irrelevantcorrections.
We thus separate the correction hy-potheses according to their corresponding wild-card before ranking them.
Afterwards, the parsingrate induced by each type of wildcard and the as-sociated parsed sentences allows to easily identifywhich wildcard is the correct one.When only one parser is used to generate cor-rection hypotheses, ranking correction hypothesesproves straightforward, but, as we will explain, theresults heavily depend on the quality of the gram-mar.
We thus put together correction hypothesesobtained thanks to different parsers in order to rankthem in a more sophisticated way.6.1 Baseline ranking: single parser modeThe correction hypotheses obtained after introduc-ing a wildcard are generally irrelevant, i.e., mostof them are parasitic hypotheses resulting from theambiguity brought by the wildcards.
Nevertheless,among all these hypotheses, some are valid, or atleast close to valid.
In the scope of only one sen-tence, there is no reliable way to determine whichcorrections are the valid ones.
But, if we considersimultaneously various sentences that contain thesame suspicious form embedded in different syn-tactic structures, we usually observe a strong vari-ability of the noisy correction hypotheses.
On theopposite, if some stable correction hypothesis isproposed for various sentences, it is likely to bevalid, i.e, to represent the correct sense of the formaccording to the grammar.
We thus simply rankcorrection hypotheses according to the number ofsentences that have produced them.6.2 Reducing grammar influence:multi-parser modeUsing various parsers not only improves the suspi-cious forms detection (Sect.
3.2), it also allows tomerge correction hypotheses in order to minimizethe influence of the shortcomings of the grammars.When some form is naturally related to syntacticconstructions that are not correctly handled by thegrammar, this form is always found in rejected sen-tences, and therefore is always suspected.
Replac-636ing it by wildcards will only produce incorrect cor-rections or no correction at all because the problemis not related to the lexicon.Having various sets of non-parsable sentencesfor a given suspicious form f , and various sets ofcorrection hypotheses for f , one can discard (orconsider less relevant) correction hypotheses ac-cording to the following three statements:?
If any form in a sentence is actually incor-rectly described in the lexicon, then this sen-tence should be non-parsable for both parsers.Correction hypotheses produced from sen-tences that are non-parsable for only oneparser should be discarded.?
For the same reason, correction hypothesesproduced with sentences in which only oneparser made f identified as a suspicious formshould be avoided.?
Finally, correction hypotheses proposed byonly one of both parsers (or proposed muchmore often by one parser than by the otherone) might just be the consequence of the am-biguity of one grammar.
Afterall, both gram-mar describe the same language, they shouldfind an agreement about the uses of a form.In our experiments, we decided to apply the fol-lowing ranking scheme: for a given suspiciousform, we only keep the corrections hypothesesthat are obtained from sentences that were orig-inally non-parsable and parsable after a wildcardintroduction for both parsers.
Afterwards, we sep-arately rank the correction hypotheses for eachparser and merge the results.We will now explain how we manually validatethe ranked correction hypotheses.7 Manual validation of the correctionsWhen studying the ranked corrections for a givenwildcard, there might be three cases:1.
There are no corrections at all: the form wasunfairly suspected or the generation of wild-cards was inadequate.
It also happens whenthe erroneous entries of the suspicious formare not the only reasons for all the parsingfailures.2.
There are relevant corrections: the form wascorrectly detected, the generation of wild-cards was adequate and the form was the onlyreason for various parsing failures.3.
There are irrelevant corrections: the ambi-guity introduced by the relevant or irrelevantwildcards opened the path to irrelevant parsesproviding irrelevant corrections.It is truly important to note that an incorrectlysuspected form may perfectly lead to irrelevantcorrections brought by the ambiguity introduced.Consequently, unless the grammar used, the de-tection of suspicious form and the generation ofwildcards are perfect, such a correcting processshould always be semi-automatic (manually vali-dated) and not automatic.Now that the whole system has been explainedwith details, we will expose the similarities anddifferences of our methods with previously pub-licated ones.8 Related worksSince efficient and linguistically relevant lexicaland grammatical formalisms have been developed,the acquisition/extension/correction of linguisticressources has been an active research field, espe-cially during the last decade.The idea to infer lexical data from the grammat-ical context first appeared in 1990 (Erbach, 1990).The combination with error mining/detection tech-nique, such as van Noord (2004), begun in 2006(van de Cruys, 2006; Yi and Kordoni, 2006).
Ex-cept in our previous work (2007), nobody has com-bined it with the technique described in Sagot andVillemonte de La Clergerie (2006).
The idea ofprefiltering the sentences (Sec.
3.2) to improve theerror mining performance has never been appliedso far.The wildcards generation started to be refinedwith Barg and Walther (1998).
Since then,the wildcards are partially underspecified and re-strained to open class POS.
In Yi and Kordoni(2006), the authors use an elegant technique basedon an entropy classifier to select the most adequatewildcards.The way to rank the corrections is usually basedon a trained tool (van de Cruys, 2006; Yi and Kor-doni, 2006), such as an entropy classifier.
Surpris-ingly, the evaluation of hypotheses on various sen-tences for a same suspicious form in order to dis-criminate the irrelevant ones has never been con-sidered so far.Finally, all the previous works were achievedwith HPSG parsers and no results has been ex-posed until 2005. van de Cruys (2006) expose its637results for each POS and one can clearly observe,for complex lemmas like verbs, the impossibilityto apply such set of techniques in an automatic waywithout harming the quality of the lexicon.
The re-sults would be even worse if applied to corpus withsentences non-covered by the grammar because norelevant corrections could be generated but irrele-vant ones might perfectly be.9 ResultsWe now expose the results of our experiments bydescribing the practical context, giving some cor-rection examples and discussing the effectivenessof the correction process through the parsing rateincreases we have obtained.9.1 Practical contextThe lexicon we are improving is called the Lefff.2This wide-coverage morphological and syntacticFrench lexicon has been built partly automati-cally (Sagot et al, 2006) and is under constant de-velopment.
At the time these lines are written, itcontains more than 520 000 entries.
The less dataan entry has, the more specified it is.We used two parsers based on two differentgrammars in order to improve the quality of ourcorrections.?
The FRMG (French Meta-Grammar) gram-mar is generated in an hybrid TAG/TIG formfrom a more abstract meta-grammar withhighly factorized trees (Thomasset and Ville-monte de La Clergerie, 2005).?
The SXLFG-FR grammar (Boullier andSagot, 2006), is an efficient deep non-probabilistic LFG grammar.The corpus used is extracted from the Frenchpolitics newspaper Le monde diplomatique.
Thiscorpora is composed with around 280 000 sen-tences of 25 or less elements and 4,3 million ofwords in total.9.2 Examples of corrections9.2.1 POS correctionsMost of the POS corrections performed wereabout missing forms or about adjectives that couldbe used as noun and vice versa.
Here are someexamples :2Lexique des formes fl?echies du franc?ais/Lexicon of in-flected forms of French.?
isra?elien (israeli) as an adjective,?
portugais (portuguese) as an adjective,?
politiques (politic) as a common noun,?
parabolique (parabolic) as an adjective,?
pittoresque (picturesque) as an adjective,?
minutieux (meticulous) as an adjective.9.2.2 Features correctionsAs one can expect, most of the features correc-tions performed were about lemmas with complexsubcategorization frames / features, i.e., essentiallyverbs.?
?revenir?
(to come back) did not handle con-structions like to come back from or to comeback in?
?se partager?
(to share) did not handle con-structions like to share something between.?
?aimer?
(to love) was described as always ex-pecting a direct object and an attribute.?
?livrer?
(to deliver) did not handle construc-tions like to deliver to somebody.9.3 Correction process relevanceAs explained earlier (Sect.
7), this process mightgenerate erroneous corrections, especially if gen-eral corpora with sentences non-covered by thegrammar are used and various correction sessionsare made.
Globally, the accuracy of the correc-tions goes decreasing after each session.
Indeed,there are less and less lexical mistakes to correctafter each session.
Anyway, we are more inter-ested in improving efficiently our lexicon.
We thusprove the relevance of the whole process by show-ing the gains of parsing rate obtained during ourexperiments.
One must keep in mind that the cor-rections are manually validated, i.e, the noticeableincreases of parsing coverage (Figure 1) are mostlydue to the improvement of the quality of the lexi-con.Table 1 lists the number of lexical forms updatedat each session.Except for the second session, all correctionsessions have been achieved with the error miningand the hypothesis generation modules.
The sec-ond session has been achieved with the POS defectmining module only (Sect.
3.1).
We planned to6381500001510001520001530001540001550001560001570001580000 1 2 3Successful parsesSession numberFrmgSxlfgFigure 1: Number of sentences successfully parsedafter each session.Session 1 2 3 totalnc 30 99 1 130adj 66 694 27 787verbs 1183 0 385 1568adv 1 7 0 8total 1280 800 413 2493Table 1: Lexical forms updated at each sessioninterface it with the hypothesis generation modulebut we could not finish it on time.
Nevertheless,the suspicious form list provided was good andsimple enough (mostly proper nouns, adjectivesand common nouns) to be reviewed without thehelp of the hypothesis generation module.As expected, we were quickly limited by thequality of the grammars and by the corpus used.Indeed, the lexicon and the grammars have beendeveloped together for the last few years, usingthis same corpus for test.
Thus, the error miningtechnique came, after few corrections sessions, toprovide us irrelevant suspicious forms.
The tagger-based detection of POS defects can only be usedonce on each corpus.
Further correction and ex-tension sessions make sense only after grammarimprovements or obtention of new corpora.Nevertheless, we have already detected and cor-rected 254 lemmas corresponding to 2493 forms.The coverage rate (percentage of sentences forwhich a full parse is found) has undergone an ab-solute increase of 3,41% (5141 sentences) for theFRMG parser and 1,73% (2677 sentences) for theSXLFG parser.
Thoses results were achieved inonly few hours of manual work on the lexicon !9.4 DiscussionThis set of techniques has two major qualities.The first one, as one can observe through ourresults, it allows to improve significantly a lexiconin a short amount of time.The second one is related to the main drawbackof our approach: the dependence to the grammarsused.
If in a non-parsable sentence, none of thesuspicious forms is a true culprit (there are no rel-evant correction), then this sentence can be consid-ered as lexically correct w.r.t.
the current state ofthe grammar.
It thus exhibits shortcomings of thegrammar and can be used to improve it.A cycling process which alternatively and incre-mentally improves both the lexicon and the gram-mar can then be elaborated.
This data is even moreimportant considering the fact that nowadays, largescale French TreeBank are rare.10 Future developmentsThe whole system has globally proved to be ma-ture.
Nevertheless, we are planning the followingimprovements to continue our investigation.?
We need to interface the POS defect miningmodule with the hypothesis generation one.?
The tagger-based detection of POS defects isstill young and can be improved.?
We will refine the wildcard generation ina similar way as done in (Yi and Kordoni,2006).?
In order to pursue the corrections of the lexi-con, we will improve our grammars accord-ing to the corpus of failed sentences.
It isnow globally representative of shortcomingsof the grammars, thus we are thinking aboutdeveloping some detection techniques in or-der to emphasize cases of error for the gram-mar.
The entropy model built by the maxi-mum entropy classifier should be a good start-ing point.11 ConclusionThe path taken, highlighted by the dependence onthe grammar, seems to be a promising one.
It willallow to develop a cycling process which alterna-tively and incrementally improves both the lexiconand the grammar.639The correction process of lexicon presented hereis now globally mature and has proved to be rele-vant and effective in practice.
Indeed, noticeableimprovements of the lexicon could be achieved ina few amount of manual work.The time spend to validate the corrections gener-ated has also confirmed our doubts about evolvingsuch process to an automatic one.We will definitively continue the correction ses-sions after upgrading some components.Acknowledgement We thank the COLE team(University of Vigo, Spain) for granting us accessto their computers.We would like to thanks Sylvain Schmitz andthe reviewers for their valuable comments.The POS mining technique could be achievedpartially thanks to the support of Ministerio deEducaci?on y Ciencia of Spain (HUM2007-66607-C04-02) and the Xunta de Galicia (?Galician Net-work for Language Processing and InformationRetrieval?
2006-2009).ReferencesAbeill?e, Anne.
2003.
Annotationmorpho-syntaxique.
Paper available athttp://www.llf.cnrs.fr/Gens/Abeille/guide-morpho-synt.02.pdf, January.Barg, Petra and Markus Walther.
1998.
Processing un-konwn words in hpsg.
In Proceedings of the 36thConference of the ACL and the 17th InternationalConference on Computational Linguistics.Boullier, Pierre and Beno?
?t Sagot.
2006.
Efficient pars-ing of large corpora with a deep LFG parser.
In Pro-ceedings of LREC?06.Daum?e III, Hal.
2004.
Notes on CG and LM-BFGSoptimization of logistic regression.
Paper availableat http://pub.hal3.name/daume04cg-bfgs, implemen-tation available at http://hal3.name/megam/, August.Erbach, Gregor.
1990.
Syntactic processing of un-known words.
In IWBS Report 131.Fouvry, Frederik.
2003.
Lexicon acquisition with alarge coverage unification-based grammar.
In Com-panion to the 10th of EACL.Molinero, Miguel A., Fco.
Mario Barcala, Juan Otero,and Jorge Gra?na.
2007.
Practical application of one-pass viterbi algorithm in tokenization and pos tag-ging.
Recent Advances in Natural Language Pro-cessing (RANLP).
Proceedings, pp.
35-40.Nicolas, Lionel, Jacques Farr?e, and?Eric Villemonte deLa Clergerie.
2007.
Correction mining in parsingresults.
In Proceedings of LTC?07.Sagot, Beno?
?t and?Eric Villemonte de La Clergerie.2006.
Error mining in parsing results.
In Proceed-ings of ACL/COLING?06, pages 329?336.
Associa-tion for Computational Linguistics.Sagot, Beno?
?t, Lionel Cl?ement,?Eric Villemonte de LaClergerie, and Pierre Boullier.
2006.
The Lefff 2syntactic lexicon for french: architecture, acquisi-tion, use.
In Proceedings of LREC?06.Thomasset, Franc?ois and?Eric Villemonte de La Clerg-erie.
2005.
Comment obtenir plus des m?eta-grammaires.
In Proceedings of TALN?05.van de Cruys, Tim.
2006.
Automatically extending thelexicon for parsing.
In Proceedings of the eleventhESSLLI student session.van Noord, Gertjan.
2004.
Error mining for wide-coverage grammar engineering.
In Proceedings ofACL 2004.Yi, Zhang and Valia Kordoni.
2006.
Automated deeplexical acquisition for robust open texts processing.In Proceedings of LREC-2006.640
