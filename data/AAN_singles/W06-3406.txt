Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 35?41,New York City, New York, June 2006. c?2006 Association for Computational LinguisticsImproving ?Email Speech Acts?
Analysis via N-gram SelectionVitor R. CarvalhoLanguage Technologies InstituteCarnegie Mellon University5000 Forbes Avenue, Pittsburgh PAvitor@cs.cmu.eduWilliam W. CohenMachine Learning DepartmentCarnegie Mellon University5000 Forbes Avenue, Pittsburgh PAwcohen@cs.cmu.eduAbstractIn email conversational analysis, it is of-ten useful to trace the the intents behindeach message exchange.
In this paper,we consider classification of email mes-sages as to whether or not they containcertain intents or email-acts, such as ?pro-pose a meeting?
or ?commit to a task?.We demonstrate that exploiting the con-textual information in the messages cannoticeably improve email-act classifica-tion.
More specifically, we describe acombination of n-gram sequence featureswith careful message preprocessing that ishighly effective for this task.
Comparedto a previous study (Cohen et al, 2004),this representation reduces the classifica-tion error rates by 26.4% on average.
Fi-nally, we introduce Ciranda: a new opensource toolkit for email speech act predic-tion.1 IntroductionOne important use of work-related email is negoti-ating and delegating shared tasks and subtasks.
Toprovide intelligent email automated assistance, it isdesirable to be able to automatically detect the intentof an email message?for example, to determine ifthe email contains a request, a commitment by thesender to perform some task, or an amendment to anearlier proposal.
Successfully adding such a seman-tic layer to email communication is still a challengeto current email clients.In a previous work, Cohen et al (2004) used textclassification methods to detect ?email speech acts?.Based on the ideas from Speech Act Theory (Searle,1975) and guided by analysis of several email cor-pora, they defined a set of ?email acts?
(e.g., Re-quest, Deliver, Propose, Commit) and then classifiedemails as containing or not a specific act.
Cohen etal.
(2004) showed that machine learning algorithmscan learn the proposed email-act categories reason-ably well.
It was also shown that there is an accept-able level of human agreement over the categories.A method for accurate classification of email intosuch categories would have many potential appli-cations.
For instance, it could be used to helpusers track the status of ongoing joint activities, im-proving task delegation and coordination.
Emailspeech acts could also be used to iteratively learnuser?s tasks in a desktop environment (Khoussainovand Kushmerick, 2005).
Email acts classificationcould also be applied to predict hierarchy positionsin structured organizations or email-centered teams(Leusky, 2004); predicting leadership positions canbe useful to analyze behavior in teams without anexplicitly assigned leader.By using only single words as features, Cohen etal.
(2004) disregarded a very important linguistic as-pect of the speech act inference task: the textualcontext.
For instance, the specific sequence of to-kens ?Can you give me?
can be more informative todetect a Request act than the words ?can?, ?you?,?give?
and ?me?
separately.
Similarly, the word se-quence ?I will call you?
may be a much stronger in-dication of a Commit act than the four words sep-arately.
More generally, because so many specific35sequence of words (or n-grams) are inherently as-sociated with the intent of an email message, onewould expect that exploiting this linguistic aspectof the messages would improve email-act classifi-cation.In the current work we exploit the linguistic as-pects of the problem by a careful combination of n-gram feature extraction and message preprocessing.After preprocessing the messages to detect entities,punctuation, pronouns, dates and times, we gener-ate a new feature set by extracting all possible termsequences with a length of 1, 2, 3, 4 or 5 tokens.Using this n-gram based representation in classi-fication experiments, we obtained a relative averagedrop of 26.4% in error rate when compared to theoriginal Cohen et al (2004) paper.
Also, rankingthe most ?meaningful?
n-grams based on Informa-tion Gain score (Yang and Pedersen, 1997) revealedan impressive agreement with the linguistic intuitionbehind the email speech acts.We finalize this work introducing Ciranda: anopen source package for Email Speech Act predic-tion.
Among other features, Ciranda provides aneasy interface for feature extraction and feature se-lection, outputs the prediction confidence, and al-lows retraining using several learning algorithms.2 ?Email-Acts?
Taxonomy andApplicationsA taxonomy of speech acts applied to email com-munication (email-acts) is described and motivatedin (Cohen et al, 2004).
The taxonomy was dividedinto verbs and nouns, and each email message is rep-resented by one or more verb-noun pairs.
For exam-ple, an email proposing a meeting and also request-ing a project report would have the labels Propose-Meeting and Request-Data.The relevant part of the taxonomy is shown in Fig-ure 1.
Very briefly, a Request asks the recipient toperform some activity; a Propose message proposesa joint activity (i.e., asks the recipient to performsome activity and commits the sender); a Commitmessage commits the sender to some future courseof action; Data is information, or a pointer to infor-mation, delivered to the recipient; and a Meeting is ajoint activity that is constrained in time and (usually)space.Several possible verbs/nouns were not consideredhere (such as Refuse, Greet, and Remind), either be-cause they occurred very infrequently in the corpus,or because they did not appear to be important fortask-tracking.
The most common verbs found in thelabeled datasets were Deliver, Request, Commit, andPropose, and the most common nouns were Meet-ing and deliveredData (abbreviated as dData hence-forth).In our modeling, a single email message may havemultiple verbs-nouns pairs.Figure 1: Taxonomy of email-acts used in experi-ments.
Shaded nodes are the ones for which a clas-sifier was constructed.Cohen et al (2004) showed that machine learn-ing algorithms can learn the proposed email-act cat-egories reasonably well.
It was also shown thatthere is an acceptable level of human agreementover the categories.
In experiments using differenthuman annotators, Kappa values between 0.72 and0.85 were obtained.
The Kappa statistic (Carletta,1996) is typically used to measure the human inter-rater agreement.
Its values ranges from -1 (com-plete disagreement) to +1 (perfect agreement) andit is defined as (A-R)/(1-R), where A is the empiri-cal probability of agreement on a category, and R isthe probability of agreement for two annotators that36label documents at random (with the empirically ob-served frequency of each label).3 The CorpusThe CSpace email corpus used in this paper con-tains approximately 15,000 email messages col-lected from a management course at Carnegie Mel-lon University.
This corpus originated from work-ing groups who signed agreements to make certainparts of their email accessible to researchers.
In thiscourse, 277 MBA students, organized in approxi-mately 50 teams of four to six members, ran sim-ulated companies in different market scenarios overa 14-week period (Kraut et al, ).
The email tends tobe very task-oriented, with many instances of taskdelegation and negotiation.Messages were mostly exchanged with membersof the same team.
Accordingly, we partitioned thecorpus into subsets according to the teams.
The 1F3team dataset has 351 messages total, while the 2F2,3F2, 4F4 and 11F1 teams have, respectively, 341,443, 403 and 176 messages.
All 1716 messageswere labeled according to the taxonomy in Figure1.4 N-gram FeaturesIn this section we detail the preprocessing step andthe feature selection applied to all email acts.4.1 PreprocessingBefore extracting the n-grams features, a sequenceof preprocessing steps was applied to all email mes-sages in order to emphasize the linguistic aspects ofthe problem.
Unless otherwise mentioned, all pre-processing procedures were applied to all acts.Initially, forwarded messages quoted inside emailmessages were deleted.
Also, signature files andquoted text from previous messages were removedfrom all messages using a technique described else-where (Carvalho and Cohen, 2004).
A similar clean-ing procedure was executed by Cohen et al (2004).Some types of punctuation marks (?,;:.)(][?)
wereremoved, as were extra spaces and extra pagebreaks.
We then perform some basic substitutionssuch as: from ??m?
to ?
am?, from ??re?
to ?
are?,from ??ll?
to ?
will?, from ?won?t?
to ?will not?,from ?doesn?t?
to ?does not?
and from ??d?
to ?would?.Any sequence of one or more numbers was re-placed by the symbol ?[number]?.
The pattern?[number]:[number]?
was replaced with ?
[hour]?.The expressions ?pm or am?
were replaced by?[pm]?.
?[wwhh]?
denoted the words ?why, where,who, what or when?.
The words ?I, we, you, he,she or they?
were replaced by ?[person]?.
Daysof the week (?Monday, Tuesday, ..., Sunday?)
andtheir short versions (i.e., ?Mon, Tue, Wed, ..., Sun?
)were replaced by ?[day]?.
The words ?after, beforeor during?
were replaced by ?[aaafter]?.
The pro-nouns ?me, her, him, us or them?
were substituted by?[me]?.
The typical filename types ?.doc, .xls, .txt,.pdf, .rtf and .ppt?
were replaced by ?.[filetype]?.
Alist with some of these substitutions is illustrated inTable 1.Symbol Pattern[number] any sequence of numbers[hour] [number]:[number][wwhh] ?why, where, who, what, or when?
[day] the strings ?Monday, Tuesday, ..., or Sunday?
[day] the strings ?Mon, Tue, Wed, ..., or Sun?
[pm] the strings ?P.M., PM, A.M. or AM?
[me] the pronouns ?me, her, him, us or them?
[person] the pronouns ?I, we, you, he, she or they?
[aaafter] the strings ?after, before or during?
[filetype] the strings ?.doc, .pdf, .ppt, .txt, or .xls?Table 1: Some PreProcessing Substitution PatternsFor the Commit act only, references to the firstperson were removed from the symbol [person] ?i.e., [person] was used to replace ?he, she or they?.The rationale is that n-grams containing the pronoun?I?
are typically among the most meaningful for thisact (as shall be detailed in Section 4.2).4.2 Most Meaningful N-gramsAfter preprocessing the 1716 email messages, n-gram sequence features were extracted.
In this pa-per, n-gram features are all possible sequences oflength 1 (unigrams or 1-gram), 2 (bigram or 2-gram), 3 (trigram or 3-gram), 4 (4-gram) and 5 (5-gram) terms.
After extracting all n-grams, the newdataset had more than 347500 different features.
Itwould be interesting to know which of these n-gramsare the ?most meaningful?
for each one of emailspeech acts.371-gram 2-gram 3-gram 4-gram 5-gram?
do [person] [person] need to [wwhh] do [person] think [wwhh] do [person] think ?please ?
[person] [wwhh] do [person] do [person] need to let [me] know [wwhh] [person][wwhh] could [person] let [me] know and let [me] know a call [number]-[number]could [person] please would [person] call [number]-[number] give [me] a call [number]do ?
thanks do [person] think would be able to please give give [me] a callcan are [person] are [person] meeting [person] think [person] need [person] would be able toof can [person] could [person] please let [me] know [wwhh] take a look at it[me] need to do [person] need do [person] think ?
[person] think [person] need toTable 2: Request Act:Top eight N-grams Selected by Information Gain.One possible way to accomplish this is usingsome feature selection method.
By computing theInformation Gain score (Forman, 2003; Yang andPedersen, 1997) of each feature, we were able torank the most ?meaningful?
n-gram sequence foreach speech act.
The final rankings are illustratedin Tables 2 and 3.Table 2 shows the most meaningful n-grams forthe Request act.
The top features clearly agree withthe linguistic intuition behind the idea of a Requestemail act.
This agreement is present not only inthe frequent 1g features, but also in the 2-grams,3-grams, 4-grams and 5-grams.
For instance, sen-tences such as ?What do you think ??
or ?let meknow what you ...?
can be instantiations of the toptwo 5-grams, and are typically used indicating a re-quest in email communication.Table 3 illustrates the top fifteen 4-grams for allemail speech acts selected by Information Gain.
TheCommit act reflects the general idea of agreeing todo some task, or to participate in some meeting.
Aswe can see, the list with the top 4-grams reflects theintuition of commitment very well.
When acceptingor committing to a task, it is usual to write emailsusing ?Tomorrow is good for me?
or ?I will put thedocument under your door?
or ?I think I can finishthis task by 7?
or even ?I will try to bring this to-morrow?.
The list even has some other interesting4-grams that can be easily associated to very specificcommitment situations, such as ?I will bring copies?and ?I will be there?.Another act in Table 3 that visibly agrees withits linguistic intuition is Meeting.
The 4-gramslisted are usual constructions associated with ei-ther negotiating a meeting time/location (?
[day] at[hour][pm]?
), agreeing to meet (?is good for [me]?
)or describing the goals of the meeting (?to go overthe?
).The top features associated with the dData act inTable 3 are also closely related to its general intu-ition.
Here the idea is delivering or requesting somedata: a table inside the message, an attachment, adocument, a report, a link to a file, a url, etc.
Andindeed, it seems to be exactly the case in Table 3:some of the top 4-grams indicate the presence of anattachment (e.g., ?forwarded message begins here?
),some features suggest the address or link where a filecan be found (e.g., ?in my public directory?
or ?inthe etc directory?
), some features request an actionto access/read the data (e.g., ?please take a look?
)and some features indicate the presence of data in-side the email message, possibly formatted as a table(e.g., ?
[date] [hour] [number] [number]?
or ?
[date][day] [number] [day]?
).From Table 3, the Propose act seems closely re-lated to the Meeting act.
In fact, by checking thelabeled dataset, most of the Proposals were associ-ated with Meetings.
Some of the features that are notnecessarily associated with Meeting are ?
[person]would like to?, ?please let me know?
and ?was hop-ing [person] could?.The Deliver email speech act is associated withtwo large sets of actions: delivery of data and deliv-ery of information in general.
Because of this gener-ality, is not straightforward to list the most meaning-ful n-grams associated with this act.
Table 3 showsa variety of features that can be associated with aDeliver act.
As we shall see in Section 5, the De-liver act has the highest error rate in the classifica-tion task.In summary, selecting the top n-gram featuresvia Information Gain revealed an impressive agree-ment with the linguistic intuition behind the differ-ent email speech acts.38Request Commit Meeting[wwhh] do [person] think is good for [me] [day] at [hour] [pm]do [person] need to is fine with [me] on [day] at [hour]and let [me] know i will see [person] [person] can meet atcall [number]-[number] i think i can [person] meet at [hour]would be able to i will put the will be in the[person] think [person] need i will try to is good for [me]let [me] know [wwhh] i will be there to meet at [hour]do [person] think ?
will look for [person] at [hour] in the[person] need to get $[number] per person [person] will see [person]?
[person] need to am done with the meet at [hour] ina copy of our at [hour] i will [number] at [hour] [pm]do [person] have any [day] is fine with to go over the[person] get a chance each of us will [person] will be in[me] know [wwhh] i will bring copies let?s plan to meetthat would be great i will do the meet at [hour] [pm]dData Propose Deliver?
forwarded message begins [person] would like to forwarded message begins hereforwarded message begins here would like to meet [number] [number] [number] [number]is in my public please let [me] know is good for [me]in my public directory to meet with [person] if [person] have any[person] have placed the [person] meet at [hour] if fine with meplease take a look would [person] like to in my public directory[day] [hour] [number] [number] [person] can meet tomorrow [person] will try to[number] [day] [number] [hour] an hour or so is in my public[date] [day] [number] [day] meet at [hour] in will be able toin our game directory like to get together just wanted to letin the etc directory [hour] [pm] in the [pm] in the lobbythe file name is [after] [hour] or [after] [person] will be ableis in our game [person] will be available please take a lookfyi ?
forwarded message think [person] can meet can meet in thejust put the file was hoping [person] could [day] at [hour] ismy public directory under do [person] want to in the commons atTable 3: Top 4-grams Selected by Information Gain5 ExperimentsHere we describe how the classification experimentson the email speech acts dataset were carried out.Using all n-gram features, we performed 5-foldcrossvalidation tests over the 1716 email messages.Linear SVM1 was used as classifier.
Results are il-lustrated in Figure 2.Figure 2 shows the test error rate of four dif-ferent experiments (bars) for all email acts.
Thefirst bar denotes the error rate obtained by Cohenet al (2004) in a 5-fold crossvalidation experiment,also using linear SVM.
Their dataset had 1354 emailmessages, and only 1-gram features were extracted.The second bar illustrates the error rate obtainedusing only 1-gram features with additional data.
Inthis case, we used 1716 email messages.
The thirdbar represents the the same as the second bar (1-1We used the LIBSVM implementation (Chang and Lin,2001) with default parameters.gram features with 1716 messages), with the differ-ence that the emails went through the preprocessingprocedure previously described.The fourth bar shows the error rate when all 1-gram, 2-gram and 3-gram features are used and the1716 messages go through the preprocessing proce-dure.
The last bar illustrates the error rate when alln-gram features (i.e., 1g+2g+3g+4g+5g) are used inaddition to preprocessing in all 1716 messages.In all acts, a consistent improvement in 1-gramperformance is observed when more data is added,i.e., a drop in error rate from the first to the sec-ond bar.
Therefore, we can conclude that Cohen etal.
(2004) could have obtained better results if theyhad used more labeled data.A comparison between the second and third barsreveals the extent to which preprocessing seems tohelp classification based on 1-grams only.
As wecan see, no significant performance difference canbe observed: for most acts the relative difference is39Figure 2: Error Rate 5-fold Crossvalidation Experimentvery small, and in one or maybe two acts some smallimprovement can be noticed.A much larger performance improvement can beseen between the fourth and third bars.
This reflectsthe power of the contextual features: using all 1-grams, 2-grams and 3-grams is considerably morepowerful than using only 1-gram features.
Thissignificant difference can be observed in all acts.Compared to the original values from (Cohen etal., 2004), we observed a relative error rate drop of24.7% in the Request act, 33.3% in the Commit act,23.7% for the Deliver act, 38.3% for the Proposeact, 9.2% for Meeting and 29.1% in the dData act.In average, a relative improvement of 26.4% in errorrate.We also considered adding the 4-gram and 5-gramfeatures to the best system.
As pictured in the lastbar of Figure 2, this addition did not seem to im-prove the performance and, in some cases, even asmall increase in error rate was observed.
We be-lieve this was caused by the insufficient amount oflabeled data in these tests; and the 4-gram and 5-gram features are likely to improve the performanceof this system if more labeled data becomes avail-able.Precision versus recall curves of the Request actclassification task are illustrated in Figure 3.
Thecurve on the top shows the Request act performancewhen the preprocessing step cues and n-grams pro-posed in Section 4 are applied.
For the bottom curve,only 1g features were used.
These two curves corre-spond to the second bar (bottom curve) and forth bar(top curve) in Figure 2.
Figure 3 clearly shows thatboth recall and precision are improved by using thecontextual features.To summarize, these results confirm the intuitionthat contextual information (n-grams) can be veryeffective in the task of email speech act classifica-tion.40Figure 3: Precision versus Recall of the Request ActClassification6 The Ciranda PackageCiranda is an open source package for Email SpeechAct prediction built on the top of the Minorthirdpackage (Cohen, 2004).
Among other features,Ciranda allows customized feature engineering, ex-traction and selection.
Email Speech Act classi-fiers can be easily retrained using any learning al-gorithm from the Minorthird package.
Ciranda iscurrently available from http://www.cs.cmu.edu/?vitor.7 ConclusionsIn this work we considered the problem of automat-ically detecting the intents behind email messagesusing a shallow semantic taxonomy called ?emailspeech acts?
(Cohen et al, 2004).
We were in-terested in the task of classifying whether or notan email message contains acts such as ?propose ameeting?
or ?deliver data?.By exploiting contextual information in emailssuch as n-gram sequences, we were able to notice-ably improve the classification performance on thistask.
Compared to the original study (Cohen et al,2004), this representation reduced the classificationerror rates by 26.4% on average.
Improvements ofmore than 30% were observed for some acts (Pro-pose and Commit).We also showed that the selection of the top n-gram features via Information Gain revealed an im-pressive agreement with the linguistic intuition be-hind the different email speech acts.References[Carletta1996] Jean Carletta.
1996.
Assessing agreementon classification tasks: The kappa statistic.
Computa-tional Linguistics, 22(2):249?254.
[Carvalho and Cohen2004] Vitor R. Carvalho andWilliam W. Cohen.
2004.
Learning to extract signa-ture and reply lines from email.
In Proceedings of theConference on Email and Anti-Spam, Palo Alto, CA.
[Chang and Lin2001] Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: a library for sup-port vector machines.
Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.
[Cohen et al2004] William W. Cohen, Vitor R. Carvalho,and Tom M. Mitchell.
2004.
Learning to classifyemail into ?speech acts?.
In Proceedings of Empiri-cal Methods in Natural Language Processing, pages309?316, Barcelona, Spain, July.
[Cohen2004] William W. Cohen, 2004.
Minorthird:Methods for Identifying Names and Ontological Re-lations in Text using Heuristics for Inducing Reg-ularities from Data.
http://minorthird.sourceforge.net.
[Forman2003] George Forman.
2003.
An extensive em-pirical study of feature selection metrics for text classi-fication.
The Journal of Machine Learning Research,3:1289?1305.
[Khoussainov and Kushmerick2005] Rinat Khoussainovand Nicholas Kushmerick.
2005.
Email task man-agement: An iterative relational learning approach.
InConference on Email and Anti-Spam (CEAS?2005).
[Kraut et al] R.E.
Kraut, S.R.
Fussell, F.J. Lerch, andA.
Espinosa.
Coordination in teams: Evidence froma simulated management game.
To appear in the Jour-nal of Organizational Behavior.
[Leusky2004] Anton Leusky.
2004.
Email is a stage:Discovering people roles from email archives.
In ACMConference on Research and Development in Informa-tion Retrieval (SIGIR).
[Searle1975] J. R. Searle.
1975.
A taxonomy of illo-cutionary acts.
In In K. Gunderson (Ed.
), Language,Mind and Knowledge., pages 344?369, Minneapolis,MN.
University of Minnesota Press.
[Yang and Pedersen1997] Yiming Yang and Jan O. Peder-sen. 1997.
A comparative study on feature selection intext categorization.
In Proceedings of ICML-97, 14thInternational Conference on Machine Learning, pages412?420.41
