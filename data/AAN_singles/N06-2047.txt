Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 185?188,New York, June 2006. c?2006 Association for Computational LinguisticsA Maximum Entropy Framework that Integrates Word Dependencies andGrammatical Relations for Reading ComprehensionKui Xu1,2 and Helen Meng11Human-Computer Communications LaboratoryDept.
of Systems Engineering andEngineering ManagementThe Chinese University of Hong KongHong Kong SAR, China{kxu, hmmeng}@se.cuhk.edu.hkFuliang Weng22Research and Technology CenterRobert Bosch Corp.Palo Alto, CA 94304, USAFuliang.weng@rtc.bosch.comAbstractAutomatic reading comprehension (RC)systems can analyze a given passage andgenerate/extract answers in response toquestions about the passage.
The RCpassages are often constrained in theirlengths and the target answer sentenceusually occurs very few times.
In orderto generate/extract a specific precise an-swer, this paper proposes the integrationof two types of ?deep?
linguistic features,namely word dependencies and grammati-cal relations, in a maximum entropy (ME)framework to handle the RC task.
Theproposed approach achieves 44.7% and73.2% HumSent accuracy on the Reme-dia and ChungHwa corpora respectively.This result is competitive with other re-sults reported thus far.1 IntroductionAutomatic reading comprehension (RC) systemscan analyze a given passage and generate/extractanswers in response to questions about the pas-sage.
The RC passages are often constrained intheir lengths and the target answer sentence usu-ally occurs only once (or very few times).
Thisdifferentiates the RC task from other tasks such asopen-domain question answering (QA) in the TextRetrieval Conference (Light et al, 2001).
In orderto generate/extract a specific precise answer to agiven question from a short passage, ?deep?
linguis-tic analysis of sentences in a passage is needed.Previous efforts in RC often use the bag-of-words(BOW) approach as the baseline, which is furtheraugmented with techniques such as shallow syn-tactic analysis, the use of named entities (NE) andpronoun references.
For example, Hirschman etal.
(1999) have augmented the BOW approachwith stemming, NE recognition, NE filtering, se-mantic class identification and pronoun resolutionto achieve 36% HumSent1 accuracy in the Reme-dia test set.
Based on these technologies, Riloffand Thelen (2000) improved the HumSent accuracyto 40% by applying a set of heuristic rules that as-sign handcrafted weights to matching words and NE.Charniak et al (2000) used additional strategies fordifferent question types to achieve 41%.
An exam-ple strategy for why questions is that if the first wordof the matching sentence is ?this,?
?that,?
?these?
or?those,?
the system should select the previous sen-tence as an answer.
Light et al (2001) also intro-duced an approach to estimate the performance up-per bound of the BOW approach.
When we applythe same approach to the Remedia test set, we ob-tained the upper bound of 48.3% HumSent accuracy.The state-of-art performance reached 42% with an-swer patterns derived from web (Du et al, 2005).This paper investigates the possibility of enhanc-ing RC performance by applying ?deep?
linguisticanalysis for every sentence in the passage.
Werefer to the use of two types of features, namelyword dependencies and grammatical relations, that1If the system?s answer sentence is identical to the corre-sponding human marked answer sentence, the question scoresone point.
Otherwise, the question scores no point.
HumSentaccuracy is the average score across all questions.185are integrated in a maximum entropy framework.Word dependencies refer to the headword depen-dencies in lexicalized syntactic parse trees, togetherwith part-of-speech (POS) information.
Grammat-ical relations (GR) refer to linkages such as sub-ject, object, modifier, etc.
The ME frameworkhas shown its effectiveness in solving QA tasks (It-tycheriah et al, 1994).
In comparison with previ-ous approaches mentioned earlier, the current ap-proach involves richer syntactic information thatcover longer-distance relationships.2 CorporaWe used the Remedia corpus (Hirschman et al,1999) and ChungHwa corpus (Xu and Meng, 2005)in our experiments.
The Remedia corpus contains55 training stories and 60 testing stories (about 20Kwords).
Each story contains 20 sentences on aver-age and is accompanied by five types of questions:who, what, when, where and why.
The ChungHwacorpus contains 50 training stories and 50 test stories(about 18K words).
Each story contains 9 sentencesand is accompanied by four questions on average.Both the Remedia and ChungHwa corpora containthe annotation of NE, anaphor referents and answersentences.3 The Maximum Entropy FrameworkSuppose a story S contains n sentences, C0, .
.
.
, Cn,the objective of an RC system can be described as:A = arg maxCi?S P (Ci answers Q|Q).
(1)Let ?x?
be the question (Q) and ?y?
be the answersentence Ci that answers ?x?.
Equation 1 can becomputed by the ME method (Zhou et al, 2003):p(y|x) = 1Z(x) exp?j ?jfj(x,y), (2)where Z(x) = ?y exp?j?jfj(x,y) is a normalizationfactor, fj(x, y) is the indicator function for featurefj; fj occurs in the context x, ?j is the weight offj .
For a given question Q, the Ci with the highestprobability is selected.
If multiple sentences havethe maximum probability, the one that occursthe earliest in the passage is returned.
We usedthe selective gain computation (SGC) algorithm(Zhou et al, 2003) to select features and estimateparameters for its fast performance.Question: Who wrote the "Pledge of Allegiance"Answer sentence: The pledge was written by Frances Bellamy.PP(by)by/INFrances/NNP Bellamy/NNPwas/VBDNPB(Bellamy)PP(of)NP(Pledge)VP(wrote)Who/WP of/INWHNP(Who)SBARQ(wrote)wrote/VBD NP(Allegiance)Allegiance/NNP "/?
?NP(Pledge)the/DT "/??
Pledge/NNThe/DTNPB(pledge)written/VBNVP(written)S(written)VP(written)pledge/NNFigure 1.
The lexicalized syntactic parse trees of aquestion and a candidate answer sentence.4 Features Used in the ?Deep?
LinguisticAnalysisA feature in the ME approach typically has binaryvalues: fj(x, y) = 1 if the feature j occurs; other-wise fj(x, y) = 0.
This section describes two typesof ?deep?
linguistic features to be integrated in theME framework in two subsections.4.1 POS Tags of Matching Words andDependenciesConsider the following question Q and sentence C ,Q: Who wrote the ?Pledge of Allegiance?C: The pledge was written by Frances Bellamy.The set of words and POS tags2 are:Q: {write/VB, pledge/NN, allegiance/NNP}C: {write/VB, pledge/NN, by/IN, Frances/NNP,Bellamy/NNP}.Two matching words between Q and C (i.e.
?write?and ?pledge?)
activate two POS tag features:fV B(x, y)=1 and fNN (x, y)=1.We extracted dependencies from lexicalizedsyntactic parse trees, which can be obtained accord-ing to the head-rules in (Collins, 1999) (e.g.
seeFigure 1).
In a lexicalized syntactic parse tree, adependency can be defined as:< hc ?
hp > or < hr ?
TOP >,where hc is the headword of the child node, hpis the headword of the parent node (hc 6= hp),hr is the headword of the root node.
Sample2We used the MXPOST toolkit downloaded fromftp://ftp.cis.upenn.edu/pub/adwait/jmx/ to generate POStags.
Stop words including who, what, when, where, why,be, the, a, an, and of are removed in all questions and storysentences.
All plural noun POS tags are replaced by theirsingle forms (e.g.
NNS?NN); all verb POS tags are replacedby their base forms (e.g.
VBN?VB) due to stemming.186modbewrite/VsubjQuestion: Who wrote the "Pledge of Allegiance"the/Det be/beby/Preppcomp?nFrances Bellamy/Npledge/Nobjdetwrite/V subjAnswer sentence: The pledge was written by Frances Bellamy.Who/N the/DetPledge/Ndetpunc"/U of/PrepAllegiance/Npunc"/UmodobjFigure 2.
The dependency trees produced by MINI-PAR for a question and a candidate answer sentence.dependencies in C (see Figure 1) are:<write?TOP> and <pledge?write>.The dependency features are represented by thecombined POS tags of the modifiers and headwordsof (identical) matching dependencies3 .
A matchingdependency between Q and C , <pledge?write>activates a dependency feature: fNN?V B(x, y)=1.In total, we obtained 169 and 180 word dependencyfeatures from the Remedia and ChungHwa trainingsets respectively.4.2 Matching Grammatical Relationships (GR)We extracted grammatical relationships from the de-pendency trees produced by MINIPAR (Lin, 1998),which covers 79% of the dependency relationshipsin the SUSANNE corpus with 89% precision4 .
INa MINIPAR dependency relationship:(word1 CATE1:RELATION:CATE2 word2),CATE1 and CATE2 represent such grammatical cat-egories as nouns, verbs, adjectives, etc.
; RELA-TION represents the grammatical relationships suchas subject, objects, modifiers, etc.5 Figure 2 showsdependency trees of Q and C produced by MINI-PAR.
Sample grammatical relationships in C arepledge N:det:Det the, and write V:by-subj:Prep by.GR features are extracted from identical matchingrelationships between questions and candidate sen-tences.
The only identical matching relationship be-tween Q and C , ?write V:obj:N pledge?
activates agrammatical relationship feature: fobj(x, y)=1.
Intotal, we extracted 44 and 45 GR features from theRemedia and ChungHwa training sets respectively.3We extracted dependencies from parse trees generated byCollins?
parser (Collins, 1999).4MINIPAR outputs GR directly, while Collins?
parser givesbetter result for dependencies.5Refer to the readme file of MINIPAR downloaded fromhttp://www.cs.ualberta.ca/ lindek/minipar.htm5 Experimental ResultsWe selected the features used in Quarc (Riloff andThelen, 2000) to establish the reference performancelevel.
In our experiments, the 24 rules in Quarc aretransferred6 to ME features:?If contains(Q,{start, begin}) and contains(S,{start,begin, since, year}) Then Score(S)+=20?
?fj(x, y) = 1 (0< j <25) if Q is a when question thatcontains ?start?
or ?begin?
and C contains ?start,??begin,?
?since?
or ?year?
; fj(x, y) = 0 otherwise.In addition to the Quarc features, we resolved fivepronouns (he, him, his, she and her) in the storiesbased on the annotation in the corpora.
The resultof using Quarc features in the ME framework is38.3% HumSent accuracy on the Remedia test set.This is lower than the result (40%) obtained by ourre-implementation of Quarc that uses handcraftedscores.
A possible explanation is that handcraftedscores are more reliable than ME, since humanscan generalize the score even for sparse data.Therefore, we refined our reference performancelevel by combining the ME models (MEM) andhandcrafted models (HCM).
Suppose the score of aquestion-answer pair is score(Q,Ci), the conditionalprobability that Ci answers Q in HCM is:HCM(Q,Ci) = P (Ci answers Q|Q) = score(Q,Ci)?j?nscore(Q,Cj) .We combined the probabilities from MEM andHCM in the following manner:score?
(Q, Ci) = ?MEM(Q, Ci) + (1 ?
?
)HCM(Q, Ci).To obtain the optimal ?, we partitioned the trainingset into four bins.
The ME models are trained onthree different bins; the optimal ?
is determinedon the other bins.
By trying different bins com-binations and different ?
such that 0 < ?
< 1with interval 0.1, we obtained the average optimal?
= 0.15 and 0.9 from the Remedia and ChungHwatraining sets respectively7 .
Our baseline used thecombined ME models and handcrafted models toachieve 40.3% and 70.6% HumSent accuracy in theRemedia and ChungHwa test sets respectively.We set up our experiments such that the linguisticfeatures are applied incrementally - (i) First , we useonly POS tags of matching words among questions6The features in (Charniak et al, 2000) and (Du et al, 2005)could have been included similarly if they were available.7HCM are tuned by hand on Remedia, thus a bigger weight,0.85 represents their reliability.
For ChungHwa, a weight, 0.1means that HCM are less reliable.187and candidate answer sentences.
(ii) Then we addPOS tags of the matching dependencies.
(iii) We ap-ply only GR features from MINIPAR.
(iv) All fea-tures are used.
These four feature sets are denotedas ?+wp,?
?+wp+dp,?
?+mini?
and ?+wp+dp+mini?respectively.
The results are shown in Figure 3 forthe Remedia and ChungHwa test sets.With the significance level 0.05, the pairwise t-test (for every question) to the statistical significanceof the improvements shows that the p-value is 0.009and 0.025 for the Remedia and ChungHwa test setsrespectively.
The ?deep?
syntactic features signif-icantly improve the performance over the baselinesystem on the Remedia and ChungHwa test sets8.Baseline +wp +wp+dp +mini +wp+dp+miniCombinations of different featuresHumSentAccuracy(%)30405060708090RemediaChungHwa40.3 41.743.3 43 44.770.6 71.1 72.7 72.273.2Figure 3.
Baseline and proposed feature results onthe Remedia and ChungHwa test sets.6 ConclusionsThis paper proposes the integration of two types of?deep?
linguistic features, namely word dependen-cies and grammatical relations, in a ME frameworkto handle the RC task.
Our system leverageslinguistic information such as POS, word depen-dencies and grammatical relationships in order toextract the appropriate answer sentence for a givenquestion from all available sentences in the passage.Our system achieves 44.7% and 73.2% HumSentaccuracy on the Remedia and ChungHwa test setsrespectively.
This shows a statistically significantimprovement over the reference performance levels,40.3% and 70.6% on the same test sets.AcknowledgementsThis work is done during the first author?s internship8Our previous work about developing the ChungHwa corpus(Xu and Meng, 2005) shows that most errors can only be solvedby reasoning with domain ontologies and world knowledge.at RTC Bosch Corp.
The work is also affiliated withthe CUHK Shun Hing Institute of Advanced Engi-neering and partially supported by CUHK4237/03Efrom RGC of HKSAR Government.ReferencesDekang Lin.
1998.
Dependency-based Evaluation ofMINIPAR.
Workshop on the Evaluation of ParsingSystems 1998.Ellen Riloff and Michael Thelen.
2000.
A Rule-basedQuestion Answering System for Reading Comprehen-sion Test.
ANLP/NAACL-2000 Workshop on Read-ing Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems.Eugene Charniak, Yasemin Altun, Rofrigo D. Braz, Ben-jamin Garrett, Margaret Kosmala, Tomer Moscovich,Lixin Pang, Changhee Pyo, Ye Sun, Wei Wy, ZhongfaYang, Shawn Zeller, and Lisa Zorn.
2000.
ReadingComprehension Programs In a Statistical-Language-Processing Class.
ANLP-NAACL 2000 Work-shop: Reading Comprehension Tests as Evaluation forComputer-Based Language Understanding Systems.Kui Xu and Helen Meng.
2005.
Design and Develop-ment of a Bilingual Reading Comprehension Corpus.International Journal of Computational Linguistics &Chinese Language Processing, Vol.
10, No.
2.Lynette Hirschman, Marc Light, Eric Breck, and John D.Burger.
1999.
Deep Read: A Reading ComprehensionSystem.
Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics.Marc Light, Gideon S. Mann, Ellen Riloff, and EricBreck.
2001.
Analyses for Elucidating Current Ques-tion Answering Technology.
Journal of Natural Lan-guage Engineering, No.
4 Vol.
7.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
PhD thesis, Universityof Pennsylvania.Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu andAdwait Ratnaparkhi.
2001.
Question Answering Us-ing Maximum-Entropy Components.
Proceedings ofNAACL 2001.Yaqian Zhou, Fuliang Weng, Lide Wu, Hauke Schmidt.2003.
A Fast Algorithm for Feature Selection in Con-ditional Maximum Entropy Modeling.
Proceedings ofEMNLP 2003.Yongping Du, Helen Meng, Xuanjing Huang, LideWu.
2005.
The Use of Metadata, Web-derived An-swer Patterns and Passage Context to Improve Read-ing Comprehension Performance.
Proceedings ofHLT/EMNLP 2005.188
