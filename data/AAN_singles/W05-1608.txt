Incremental Generation of Multimodal Deixis Referring to ObjectsAlfred Kranstedt, Ipke WachsmuthUniversity of BielefeldArtificial Intelligence Group, Faculty of TechnologyD-33594 Bielefeld{akranste, ipke}@techfak.uni-bielefeld.deAbstractThis paper describes an approach for the generationof multimodal deixis to be uttered by an anthro-pomorphic agent in virtual reality.
The proposedalgorithm integrates pointing and definite descrip-tion.
Doing so, the context-dependent discrimina-tory power of the gesture determines the content-selection for the verbal constituent.
The conceptof a pointing cone is used to model the region sin-gled out by a pointing gesture and to distinguishtwo referential functions called object-pointing andregion-pointing.1 IntroductionDeixis anchors utterances in their spatio-temporal context andcan therefore be seen as a central part of the aboutness oflanguage.
In face-to-face interaction deixis is typically ex-pressed using several modalities.
In this paper we describean approach for the generation of multimodal deixis referringto objects.
These expressions integrate two different kindsof referring to objects, indicating the location of an objectby pointing or describing its properties by a definite descrip-tion.
Following McNeill [McNeill, 1992], we distinguishbetween abstract pointings and pointings into concrete do-mains.
Here, we focus on pointings into concrete domainsco-occurring with verbal expressions, typically definite nounphrases.
As we will see further on, the interrelation betweengesture and verbal expression is of a complex nature.
Bothare often under-specified; only together they identify the ref-erent unambiguously.In the growing number of applications which are char-acterised by an anthropomorphic human-computer interfacethere is an increasing need for robust mechanisms when re-ferring to objects by speech and gesture.
Emphasising theimportance of deixis in the interaction with humanoid agents,[Lester et al, 1999] introduced the expression deictic believ-ability.
In contrast, the generation of multimodal referenceis an open issue until now, while the generation of referringexpressions, which identify objects by description, is well in-vestigated (several computational models have been proposedover the last years).The approach proposed for the generation of multimodaldeictic expressions is based on the incremental algorithm by[Dale and Reiter, 1995].
This algorithm for the generationof verbal referring expressions was adapted in that the spatialproperty location, which can be expressed either absolutelyby pointing or relationally by verbal expressions (e.g.
?theleft object?
), is evaluated besides other object properties incontent-selection.
Taking account of the inherent imprecise-ness of pointing gestures, two referential functions of point-ing are distinguished, object-pointing and region-pointing.While object-pointing refers on its own, region-pointing isused to narrow down the set of objects from which the refer-ent has to be distinguished by a definite description.Figure 1: The interaction scenarioThe described research is undertaken in the course of thedevelopment of human computer interfaces for natural inter-action in Virtual Reality (VR).
Conducting empirical inves-tigations and developing computational models we focus ondialogues in a construction task domain, where a kit consist-ing of generic parts is used to construct models of mechanicalobjects such as a toy airplane.
A typical setting consists of ahuman instructor and an anthropomorphic virtual agent inter-acting in face-to-face manner in VR realised in a three-sideCave-like installation.
Our human-sized virtual agent calledMax is able to interpret simple multimodal (speech and ges-ture) input from the human instructor on the one hand and toproduce synchronised output involving synthetic speech, fa-cial display and gesture [Kopp and Wachsmuth, 2004] on theother hand.
As illustrated in Fig.
1, Max and the human dia-logue partner are located at a virtual table with toy parts andcommunicate about how to assemble them.
Speech and ges-ture are used by both interlocutors to specify tasks and selectrelevant objects.On the way towards dialogue generation a setting we calldemonstration games has been established to get at the un-derstanding and generation of complex deictic expressions.These demonstration games which reduce interaction to twoturns are based on the minimal dialogue games proposed by[Mann, 1988].
The setting consists of two interlocutors lo-cated at a table with some objects lying on it.
One inter-locutor has to indicate an object by speech and gesture, andthe other interlocutor has to give feedback on which objectwas referred to.
In a human-human realisation this settingis used to conduct empirical studies to investigate the refer-ring behaviour of subjects [Ku?hnlein and Stegmann, 2003;Lu?cking et al, 2004].
An annotated corpus was acquiredwhich comprises 65 multimodal demonstrations uttered byseveral subjects.
In a human-machine realisation the settingis used as a testbed for the developed communicative abili-ties of our agent concerning deictic reference.
This enablesus to directly link and compare the results of speech-gestureprocessing with empirically recorded data in a comparablesetting [Kranstedt et al, 2004].In the section to follow, the role of pointing in multimodalreferring expressions is analysed in more detail.
The conceptof a pointing cone and two referential functions of pointing,object-pointing and region-pointing, are introduced.
In Sec.
3a short overview on related work concerning the generation ofreferring expression is given.
The incremental algorithm pro-posed by [Dale and Reiter, 1995] underlying our approach isoutlined in Sec.
3.1.
In Sec.
4 the content-selection algorithmproposed for multimodal deictic expressions is described indetail.
Sec.
5 illustrates its functionality giving an example.Sec.
6 describes the embedding of the algorithm in a gener-ation framework and the current potentials and limitations ofthe approach.
The paper concludes with a short discussion ofthe proposed approach.2 Pointing in Multimodal Deictic ExpressionsThere is little doubt in the literature that pointing is tied upwith reference as the following quotation from [Lyons, 1977,p.
654] shows:When we identify an object by pointing to it (and this no-tion, as we have seen, underlies the term ?deixis?
and Peirce?sterm ?index?
: cf.
15.1), we do so by drawing the attentionof the addressee to some spatio-temporal region in which theobject is located.Pointing, then, is related to objects indicated and regionsoccupied.
Lyons also emphasises that certain kinds of ex-pressions, especially definite descriptions, are closely linkedto pointing or demonstration (op.
cit., p. 657):[.
.
. ]
definite referring noun-phrases, as they have beenanalysed in this section, always contain a deictic element.
Itfollows that reference by means of definite descriptions de-pends ultimately upon deixis, just as much as does referenceby means of demonstratives and (as we saw in the previoussection) personal pronouns.Pointing and definite descriptions therefore represent onthe one hand different kinds of referring to objects (indicat-ing their location or describing their properties).
On the otherhand they appear to be intimately connected.
Lyons does notdiscuss how exactly pointing and verbal expression are re-lated.
Following [Rieser, 2004], we pursue a line of thoughtassociated with Peirce, who maintains the idea of gestures be-ing part of more complex signs [Peirce, 1965].
Transferringthat to deixis we call such complex signs, which are com-posed of a pointing gesture and a definite description, com-plex demonstrations.
In other words, complex demonstra-tions are definite descriptions to which pointings add content,either by specifying an object independently of the definitedescription (Lyons?
attention being drawn to some object) orby narrowing down the description?s restrictor (Lyons?
spatio-temporal region).
Below, we refer to these two possibilities asthe respective functions of demonstration, see [Rieser, 2004]for discussion.
If a pointing gesture uniquely singles out anobject, it is said to have object-pointing function.
If the ges-ture draws the attention of the addressee to a region makingthe objects inside it salient it is ascribed a region-pointingfunction.The distinction between object-pointing and region-pointing is closely connected with the observation that point-ing gestures are inherently ambiguous, varying with the dis-tance between pointing agent and referent.
In the empiricaldata collected in our demonstration games we found object-pointing only in demonstrations to objects near to the demon-strating subject, while pointings to objects farther are accom-panied by definite descriptions [Lu?cking et al, 2004].
Twophenomena can be recognised (even though they are blurredby over-specification which we observe very often in com-plex demonstrations).
First, pointing saves words; definitedescriptions accompanied by a pointing gesture are shorterand less complex than definite descriptions without gesture.Secondly, length and complexity of the definite descriptionin complex demonstrations depend on the distance betweendemonstrating subject and referent pointed to.
Similar resultscan be found in literature, e.g.
[Beun and Cremers, 2001;van der Sluis and Krahmer, 2004].These results indicate that the discriminative power ofpointing gestures influences the construction of definite de-scriptions and that in order to determine the set of entitiesdelimited by a pointing gesture the distance to the referenthas to be accounted for.
As a first approximation we modelthe topology of the region singled out by a pointing gestureas a cone anchored at the index finger tip and directed alongthe vector defined by the stretched index finger.It has to be stressed, however, that a cone is an idealisationof the pointing region.
There are a lot of influencing param-eters, which we can divide in perceivable parameters on theone hand (like spatial configuration of demonstrating agent,addressee, and referents as well as the clustering of the en-tities under demonstration) and dialogue parameters on theother hand.
Determining the pointing cone in more detail isthe issue of further empirical investigations currently under-taken.
The concept of pointing cone we use is based on a setof parameters which guarantees that the cone?s form and sizecan be adjusted as further findings become available.Observations we made in our corpus suggest that we haveto acknowledge that each of the two referential functionsof pointing, i.e.
object-pointing and region-pointing, comeswith a cone on its own.
Therefore, the concept of pointingcone can be divided into two topologically different types forobject- and for region-pointing respectively, with the formerhaving a narrower angle than the latter.
The cone of object-pointing represents the resolution of a pointing gesture visu-ally perceivable to the dialogue participants, and therefore,defines the borderline up to which object-pointing can beconducted successfully.
Preliminary findings [Ku?hnlein andStegmann, 2003] indicate an apex angle of this cone of about12 to 24 degrees.
In contrast, region-pointing draws the at-tention of the addressee to a wider region making the objectsinside this region salient.
The cone representing this regionhas to be modelled with a wider apex angle than the cone forobject-pointing to ensure robust reference and to fit empiricalfindings concerning over-specification.3 Related WorkWhile much work concerning the generation of referring ex-pressions has been published over the last 15 years, work onthe generation of multi-modal referring expressions is rare.Most of the approaches which can be found in this field useidealised pointing in addition or instead of referring expres-sions.
[Claassen, 1992] and [Reithinger, 1992] highlight thereferent in two-dimensional settings by an idealised pointinggesture represented by an arrow or a schematic hand.
[Nomaand Badler, 1997] and [Andre?
et al, 1999] introduce virtualagents in presentation tasks able to produce simple point-ing gestures.
[Lester et al, 1999] and [Rickel and Johnson,1999] generate pointing gestures expressed by an agent whichmoves to the referent, and therefore, achieve unambiguouspointing.
Only [Krahmer and van der Sluis, 2003] integratepointing and definite descriptions in a more natural way andaccount for vague pointing.
They distinguish three types ofpreciseness, i.e.
precise, imprecise, or very imprecise point-ing, and integrate pointing into the graph-based algorithmproposed by [Krahmer et al, 2003].Examining the generation of referring expressions realisedas definite descriptions one has to mention, first of all, that theproblem of selecting the minimal set (in the sense of Grice?squantity maxim) of object properties needed for an unam-biguous description of the referent has exponential computa-tional complexity [Reiter, 1990].
Each combination of prop-erties has to be tested whether it is true only for the referent,and the shortest one of these combinations has to be chosen.Especially for real-time applications in domains with high ob-ject density and objects with a high number of properties thiscomputation is intractable with brute-force methods.
Severalapproaches have been proposed to deal with this problem,namely [Dale, 1992; Krahmer et al, 2003; Horacek, 1997;Gardent, 2002].
[Dale and Reiter, 1995] proposed an incre-mental algorithm which violates the quantity maxim in thestrict sense, but achieves linear compute time and fits wellwith empirical findings.3.1 The Incremental Algorithm by Dale and ReiterTo achieve linear compute time, [Dale and Reiter, 1995] pro-pose a fixed sequence of property evaluation and avoid back-tracking.
This approach leads to over-specification, but theycan show that the generation results fit well with empiricalfindings if the sequence of properties is chosen accuratelyw.r.t.
the specific domain.
Therefore, the content-selectionalgorithm (see Alg.
3.1) gets, in addition to the referent r andthe context set C, also a sorted list of properties P as an input.The functionality of this algorithm can be described inshort as follows.
In the ordering of P each property Ai in Pis evaluated concerning its discriminatory power, that meansit is checked if there is at least one object in C which has an-other value for A than the referent r has.
These objects areruled out.
If the contrast set C is empty the algorithm termi-nates and returns a list with the discriminating properties L.W.r.t.
observations in their corpora Dale and Reiter add theproperty type everytime.
The task of FINDBESTVALUE is thesearch for the most specific value of an attribute that (1), dis-criminates the referent r from more elements in D than thenext general one does, and (2), is known by the addressee.We chose this algorithm as a starting point for our workand adapted it for multimodal expressions because of its ap-propriateness w.r.t.
empirical data and its efficient computetime .Algorithm 3.1: MAKEREFERRINGEXPRESSION(r, C, P )L?
{}for each member Ai of list P doV = FINDBESTVALUE(r, Ai, BASICLEVELVALUE(r, Ai))if RULESOUT((Ai, V )) 6= nilthen L?
L ?
{(Ai, V )}C ?
C \ RULESOUT((Ai, V ))if C = {} thenif (type,X) ?
L for some Xthen return (L)else return (L ?
{(type, BASICLEVELVALUE(r, type))})return (failure)procedure FINDBESTVALUE(r, A, initial-value)if USERKNOWS(r, (A, initial-value)) = truethen value?
initial-valueelse value?
no-valueif (more-specific-value?
MORESPECIFICVALUE(r, A, value)) 6= nil ?(new-value?
FINDBESTVALUE(A,more-specific-value)) 6= nil ?
(|RULESOUT((A,new-value))| > |RULESOUT((A, value))|)then value?
new-valuereturn (value)procedure RULESOUT((A, V ))if V = no-valuethen return (nil)else return (x : x ?
C ?
USERKNOWS(x, (A, V )) = false)4 Incremental Multimodal Content SelectionWe integrate in the incremental algorithm by Dale and Re-iter an evaluation of the spatial property location, either tobe uttered absolutely by a pointing gesture or to be expressedverbally in relation to other objects in speaker-intrinsic coor-dinates.Before presenting the algorithm we first have to clarify theterminology used.
Analogous to [Dale and Reiter, 1995], wedefine the context set C to be the set of entities (physical ob-jects in our scenario) that the hearer is currently assumed tobe attending to.
We also define the set of distractors D to bethe set of entities from which the referent r has to be distin-guished further on.
At the beginning of the content selectionprocess the distractor set D will be the context set C exceptthe referent r; at the end D will be empty if content selectionwas successful.
R represents the set of restricting propertiesfound, each composed of an attribute-value pair.P represents the ordered list of properties which the algo-rithm gets as additional input.
Based on observations in ourdata we assume that referring to objects by pointing is thefirst choice in face-to-face dialogues, while expressing rela-tive location is only used after basic properties like type orcolour.
Therefore, we get absolut location, type, colour, size,and relative location to be the list of properties which have tobe evaluated concerning their discriminatory power.Algorithm 4.1: CONTENTSELECTRE(r, P, C)R?
{}D ?
C??
objectPointingConeApexAngle?
?
regionPointingConeApexAngleif REACHABLE?
(r) (i)then?????R?
{(location,?
)}(~h, ~r)?
GENERATEPOINTINGRAY(r)if GETPOINTINGMAP((~h, ~r), C, ?)
= {r}then return (R ?
{type, GETVALUE(r, type)})else D ?
GETPOINTINGMAP((~h, ~r), C, ?
)for each p ?
P (ii)do????????????????
?if RELATIONALPROPERTY?
(p)then v ?
GETRELATIVEVALUE(r, p,D)else v ?
GETVALUE(r, p)if v 6= null and RULESOUT(p, v,D) 6= {}then{R?
R ?
{(p, v)}D ?
D \ RULESOUT(p, v,D)if D = {}then{if (type, x) ?
R for some xthen return (R)else return (R ?
{type, GETVALUE(r, type)})return (failure)procedure RULESOUT(p, v,D)return ({x | x ?
D ?
GETVALUE(x, p) 6= v})The incremental content-selection in our algorithm (seeAlg.
4.1) is organised in two main steps: First, see part(i), disambiguation of the referent by pointing is checked ifthe referent is visible for both participants.
The decision,which kind of pointing, object-pointing or region-pointing,is appropriate is based on an evaluation of their discrimina-tory power.
Object-pointing can only be used if the gestureis able to indicate the referent in an unambiguous manner.This is tested by generating a pointing cone with an apex an-gle of 12 degrees anchored in an approximated hand-position(covered in the functions GENERATEPOINTINGRAY(r) andGETPOINTINGMAP((~h,~r), C, ?)
with the apex angle ?).
Ifonly the intended referent r is found inside this cone, thealgorithm terminates and referring can be done by object-pointing.
Otherwise, region-pointing is evaluated using thesame functions to narrow down the distractor set D to theobjects found in the cone, now with the wider apex angle ?.For determining additional discriminating properties (seepart (ii)) we use an adapted version of the incremental algo-rithm of Dale and Reiter described above.
Each property p inP is evaluated concerning its discriminatory power.
If it rulesout some objects in D, these objects are deleted in D and pand its value v are added to R.On the one hand we extend the original algorithm account-ing for properties which are expressed in relation to other ob-jects in the scene.
On the other hand our algorithm is sim-plified in as much as in our prototypical implementation theFINDBESTVALUE function defined by Dale and Reiter is re-placed by the cheaper function GETVALUE.
We realise thesearch for the appropriate value on a specialisation hierarchyonly for the special case type (?screw?
instead of ?pan headslotted screw?
is used).
If an appropriate value for type doesnot exist (this is the case for some aggregates under construc-tion in our domain), type is uttered in an unspecific mannerlike ?this part?, the value v for the property type is then setto object, the most general value in the specialisation hierar-chy.
Analogous to [Dale and Reiter, 1995], type is added toR even if it has no discriminatory power.
This complies withthe most frequent kind of over-specification found in our em-pirical data.For the other properties like colour we do not need such asophisticated search on a specialisation hierarchy in our do-main.
We operate in a highly simplified domain with objectscharacterised by properties having only a few and well dis-tinguished values perceivable by both dialogue participants.For the property colour, e.g., only the values red, green, blue,yellow, purple, orange, and brown exist.In the following we describe the realisation of the essentialmodifications proposed in our approach in greater detail, theevaluation of the discriminating power of pointing and theconsideration of relational properties.4.1 Considering the Spatial Context:Object-pointing vs. Region-pointingIf we assume that the spatial context of the interaction de-termines the discriminatory power of pointing as describedin Section 2 we have to anchor multimodal content-selectioninto this context.
The central concept for this task is the point-ing cone.
It models the region which is indicated by the point-ing gesture.
The objects inside the cone can not be distin-guished without further information.In the course of our multimodal content-selection algo-rithm the generation of the pointing cone and the identifica-tion of the objects lying inside it is realised using the follow-ing functions:?
REACHABLE?
(r): Tests if the referent r is visually avail-able to both dialogue participants.?
GENERATEPOINTINGRAY(r): This function gets thereferent r and computes a pointing ray which is repre-sented by two vectors, its origin ~h located in the demon-strating hand and its direction ~r determined by the refer-ent r.?
GETPOINTINGMAP((~h,~r), C, ?
): This function (for de-tails see Alg.
4.2) gets the pointing ray (~h,~r), a set of ob-jects C, and an apex angle ?
and returns a sorted list ofobjects located inside the cone defined by (~h,~r) and ?.The decision criterion is the apex angle ?.
If the vectororiginated in~h directed to o ?
C spans with the pointingray an angle less than ?
o is said to be located inside thecone, otherwise not.?
GETPOSITION(o,~h): Computes the position of object ow.r.t.
the position represented by ~h, in this case the handposition.?
GETANGLE(~x, ~y): Computes the angle between the vec-tors ~x and ~y.?
INSERT(o,M,?
): Inserts the object o in the map M inincreasing order w.r.t.
the angle ?.Algorithm 4.2: GETPOINTINGMAP((~h, ~r), C, ?
)M ?
{}for each o ?
Cdo{~x?
GETPOSITION((o,~h))?
?
GETANGLE(~x, ~r)if ?
?
?then INSERT(o,M, ?
)return (M)In the course of evaluating pointing, it is tested firstwhether the referent is reachable by both participants.
In ourapplication domain this implies whether r is a visible objectlying on the table, the construction area.
If this is the case,pointing in general is appropriate, the property location withthe value ?
indicating a pointing gesture is added to the listof restricting properties R.To decide whether object-pointing or region-pointing is ap-propriate, the pointing cones for these two kinds of point-ing have to be generated.
This is achieved by generatingthe pointing ray first using the function GENERATEPOINT-INGRAY.
To determine the origin of the pointing ray withoutsynthesising a pointing gesture at this early point of time anapproximated hand position is computed located in a typicaldistance in front of the body on a straight line between a pointin-between the shoulders of the demonstrating agent and thereferent r.The pointing ray is used as an input for the function GET-POINTINGMAP which stores all objects inside the cone in asorted map.
First, this is done for a cone with the apex an-gle ?, the cone for object-pointing.
If this map contains atleast one object besides the referent r, disambiguation basedonly on a pointing gesture is not possible.
Region-pointingis then chosen to narrow down the set of distractors.
Againthe function GETPOINTINGMAP is used to determine the setof objects which are indicated by pointing, now by region-pointing.
The wider apex angle ?
for the pointing cone ofregion-pointing is used to ensure robust reference.4.2 Relational Object PropertiesIn our corpus we often found properties which are typicallyexpressed in relation to other objects.
The most frequent ex-amples concern the properties size and location leading todescriptions like ?the big object?
respectively ?the left ob-ject?.
The function RELATIONALPROPERTY?
(p) tests foreach property p if it is a property which can be expressedrelationally.
To evaluate these properties we use the functionGETRELATIVEVALUE.
This function (see Alg.
4.3) com-pares the absolute value of the referent?s property p with thecorresponding values of the objects in D. If the referent rholds the maximum or minimum of the values the functionreturns the according max or min value, e.g., big or small ifthe property is size.
To do so, GETRELATIVEVALUE needs apartial order for each property.
In our system this is imple-mented for size and relative location.In the case of size we relate the property to the shape ofthe objects under discussion.
Shape is a property often usedon its own if the type of an object is unknown but it is diffi-cult to handle in generation because the description of shape,especially for complex shapes, is highly ambiguous and sub-jective.
However, in our corpus data aspects of shape can beoften found as part of descriptions of size.
This can be foundif the shape of an object is characterised by one or two des-ignated dimensions.
For these objects size is substituted by,e.g., length respectively thickness (?long screw?
is used in-stead of ?big screw?
).In the case of relative location we use a similar kind ofsubstitution.
The relative location is evaluated along the axesdefining the subjective coordinate systems of the dialogueparticipants (left-right, ahead-behind, and top-down).
E.g.,GETRELATIVEVALUE returns left if the referent r is the left-most located object in D ?
{r}.The function GETVALUE(o, p) returns the absolute value vof the property p of the object o fetched from the knowledge-base.
The search for an appropriate value on a specialisationhierarchy for the property type, as described above, is realisedwithin this function.Algorithm 4.3: GETRELATIVEVALUE(r, p,D)vr ?
GETVALUE(r, p)if min{v | v = GETVALUE(x, p) ?
x ?
(D ?
{r})} = vrthen{vmin ?
typically used minV alue(p)return (vmin)if max{v | v = GETVALUE(x, p) ?
x ?
(D ?
{r})} = vrthen{vmax ?
typically used maxV alue(p)return (vmax)return (null)5 ExampleThe following example illustrates the process of content-selection as it is realised by the described algorithm (Fig.
2):The starting point is a query concerning the reference to aspecific object with the technical name five-hole-bar-0 (Fig.2a).
This object lying on the table is visible to both dialogueparticipants, therefore pointing is appropriate and the prop-erty location with the value ?
indicating a pointing gesture isadded to R. Now it has to be decided which kind of pointingis appropriate (Alg.
4.1, part (i)), that means whether pointingalone (object-pointing) yields the referent in an unambigu-ous manner.
To do so, the pointing cone for object-pointingis generated.
In this example the object density is high andmore than one object is found inside this cone.
Therefore,pointing alone does not yield the referent and region-pointingis evaluated next.
This is illustrated in Fig.
2b) schematically:The two ellipses mark the intersection of the pointing coneswith the table, the smaller ellipse w.r.t.
object-pointing, thebigger one w.r.t.
region-pointing.
The smaller ellipse coverstwo objects, that means pointing alone can not distinguish be-tween these two objects, an additional definite description isneeded.
Region-pointing is used to narrow down the set ofdistractors C for the construction of the definite description.To make the multimodal reference consisting of pointing anddefinite description more robust (in analogy to the empiricalfindings) now a wider apex angle is used resulting in the big-ger ellipse.
The objects inside this bigger ellipse, the two barsfive-hole-bar-0 and three-hole-bar-0, a block, a screw, and adisc constitute the distractor set.The second part of the algorithm determines the proper-ties needed for the definite description.
It starts with test-ing the property type.
The type five-hole-bar is too spe-cific, so the super-type bar is chosen.
This property rulesout all objects except the two bars (now C = {five-hole-bar-0, three-hole-bar-0}) and type with the value bar is addedto R. The property colour is tested next; it has no discrim-inatory power concerning the two bars.
But the followingproperty size discriminates the two objects.
The shape ofbars is characterised by one designated dimension.
There-fore, size is substituted by length.
In our case the referentr has the maximum length of all objects in C, the propertylength with the value long is added to R. Now C containsonly r, the algorithm terminates and returns R = {(location,?
), (type, bar), (length, long)} (Fig.
2b).Based on R, a pointing gesture directed to r is specified,the noun phrase ?die lange Leiste?
(the long bar) is gener-ated, and both are inserted into an utterance template (see Fig.2c)).
The complete utterance is synthesised and uttered by theagent Max (Fig.
2d).6 Application in the context ofHuman-Computer Interaction in VRAs explained in the introduction, the described approach wasdeveloped in the context of research on interfaces for natu-ral interaction with an anthropomorphic agent in VR.
Theembodied agent Max should be enabled to produce believ-able deictic references to virtual objects in real-time inter-action.
Following [Dale and Reiter, 2000], the generationof natural language can be divided into three main steps,namely, macroplanning (document planning), microplanning,and surface realisation.
Extending this, we add synthesis asa fourth step, including motorplanning and visualisation forgestural and a text-to-speech synthesis for verbal utterances.Content-selection for complex demonstrations is part of mi-croplanning.
The starting point is a logical representation ofthe performative of a planned utterance (as illustrated in theexample above, see Fig.
2a)), which will be provided as resultof the reasoning processes of the agent in future work.The results of the content selection as represented by a listof attribute-value-pairs are fed into a surface realisation mod-ule generating a syntactically correct noun phrase.
This nounphrase is combined with a gesture specification and both areinserted into a template of a multi-modal utterance fetchedfrom a database and described in MURML [Kranstedt et al,a) query(five-hole-bar-0)b)R = {(location,?
), (type, bar), (length, long)}c) <definition><parameter name=?NP?/><parameter name=?Object?/><utterance><specification>Meinst du <time id=?t1?/> $NP?
<time id=?t2?/></specification><behaviorspec id=?gesture 0?><gesture><affiliate onset=?t1?
end=?t2?/><function name=?refer to loc?><argument name=?refloc?
value=?$Object?/><argument name=?frame of reference?
value=?world?/></function></gesture></behaviorspec></utterance></definition>d)?Meinst du die lange Leiste??
(Do you mean the long bar?
)Figure 2: Example of the generation of a complex demon-stration in four steps: a) A query concerning the object five-hole-bar-o constitutes the starting point; b) pointing conesfor object-pointing and region-pointing are generated, the lat-ter one specifies the distractor set for further property evalua-tion; c) the pointing gesture and the noun phrase are insertedin an utterance description template described in MURML;d) an appropriate animation (German speech, here with thevisualised pointing cone) is synthesised.2002] (see Fig.
2c) for illustration).
MURML enables thespecification of arbitrary co-verbal gestures.
Cross-modalsynchrony is established appending the gesture stroke to theaffiliated word or sub-phrase in the co-expressive speech.Based on these descriptions, an utterance generator synthe-sises continuous speech and gesture in a synchronised manner(for details see [Kopp and Wachsmuth, 2004]).The VR environment in which the interaction takes placeis realised using the framework Avango [Tramberend, 1999]which is based on the common scenegraph representation ofvirtual worlds.
With PrOSA (Patterns On Sequences of At-tributes, [Latoschik, 2001] this framework was extended forinteracting in immersive virtual reality by means of speechand gesture.
The scenegraph is not only used to model theenvironment, it also builds the agent?s knowledgebase of itsenvironment.
Each object represented in the scenegraph canbe correlated with a so-called semantic entity [Latoschik andSchilling, 2003], which provides arbitrary semantic proper-ties associated with this entity.
During content-selection, theproperty values of the objects under discussion are fetchedfrom these semantic entities.The vocabulary used is geared to the ontology of the toy-kit, called Baufix, we use in our setting.
It consists of a smallnumber of generic parts like bars, screws, blocks, discs etc.
(twelve different types, some of them in different size andcolour).
All the parts and the values of their properties canbe named.
Therefore, all possible descriptions in this smalldomain can be generated.
Currently, deictic expressions aspart of different types of speech acts can be generated, espe-cially query, request, and inform.
Only a small number ofverb phrases can be used.
In sum, the vocabulary currentlyavailable is very small.
However, the focus of this work is notto generate a huge amount of speech output but to investigatethe correlation between speech and gesture in the generationof multimodal reference.Up to now we can generate in the course of deictic expres-sions pointing gestures synchronised with speech for all ob-jects reachable for the agent without moving.
In most casesmoving will not be necessary, respectively more costly thangenerating a definite description.
But we know that this is notadequate in all cases.
The integration of moving in the courseof content-selection will be an issue of future work.7 ConclusionIn this paper an approach was presented which enables thegeneration of multimodal deictic expressions consisting of apointing gesture indicating the location of an object and a def-inite noun phrase describing the object using its properties.Taking account of the inherent impreciseness of pointing ges-tures two referential functions of pointing are distinguished,object-pointing and region-pointing.
With the increasing dis-tance between demonstrating agent and referent the discrim-inatory power of the gesture decreases and more additionalproperties are needed to identify the referent.
A pointingcone for each referring function of pointing gestures was de-fined to model the distance dependency of pointing.
An algo-rithm was presented that integrates pointing and definite de-scriptions by using the objects highlighted by the gesture asdistractor set for the construction of the definite description.Drawing the attention to a spatial region and the objects lyinginside this region region-pointing ensures that these objectsare in the focus of attention of the addressee ([Dale and Re-iter, 1995] speak in this context about a navigational functionof the expression).Dale and Reiter emphasise that their content-selection al-gorithm is defined domain independently while the prop-erty list P and the functions MORESPECIFICVALUE, BA-SICLEVELVALUE, and USERKNOWS define the interfaceto the domain of application, especially to the knowledgeabout this domain shared by the interlocutors.
Analogously,the functions REACHABLE?, GENERATEPOINTINGRAY, andGETPOINTINGMAP in our approach can be seen as a link be-tween the content-selection algorithm and the spatial contextin which the interaction takes place.
Implementing the con-cept of the pointing cone they provide an interface betweenthe geometrical aspects of pointing gestures and their refer-ential semantics.The quality of the generation results using the describedapproach depends on the precision of the topology of thepointing cones and the knowledge about the parameters in-fluencing this topology.
We have started to conduct empiricalstudies using tracking technology to collect analytical dataconcerning the pointing behaviour of human subjects in vary-ing pointing domains [Kranstedt et al, 2005].Up to now, we do not have a comprehensive evaluationof our approach.
But if we compare the generation resultswith the empirical data collected in the demonstration gamesmentioned in Sec.
1 and with other corpora about instructor-constructor dialogues in the Baufix-world [Sagerer et al,1994] we notice a good correspondence with the empiricalfindings.
A critical point we found in these comparisons isthat the perceivable resolution of pointing in real world is notexactly the same as in VR.
In the latter it depends massivelyon kind and quality of the display technology used.
There-fore, mechanisms which adapt the pointing cone?s size andform to the constraints of the interaction environment seemto be useful.AcknowledgmentThis research is partially supported by the DeutscheForschungsgemeinschaft (DFG) in the Collaborative Re-search Centre SFB 360.References[Andre?
et al, 1999] E.
Andre?, T. Rist, and J. Mu?ller.
Em-ploying AI Methods to Control the Behavior of AnimatedInterface Agents.
Applied Artificial Intelligence, 13:415?448, 1999.
[Beun and Cremers, 2001] R.-J.
Beun and A. Cremers.
Mul-timodal Reference to Objects: An Empirical Approach.
InProceedings of Cooperative Multimodal Communication,Second International Conference, pages 64?86, 2001.
[Claassen, 1992] W. Claassen.
Generating Referring Expres-sions in a Multimodal Environment.
In R. Dale et al, edi-tors, Aspects of Automated Natural Language Generation.Springer, Berlin, 1992.
[Dale and Reiter, 1995] R. Dale and E. Reiter.
Computa-tional Interpretations of the Gricean Maxims in the Gen-eration of Referring Expressions.
Cognitive Science,18:233?263, 1995.
[Dale and Reiter, 2000] R. Dale and E. Reiter.
Building Nat-ural Language Generation Systems.
Cambridge UniversityPress, Cambridge, UK, 2000.
[Dale, 1992] R. Dale.
Generating Referring Expressions:Constructing Descriptions in a Domain of Objects andProcesses.
MIT Press, Cambridge, MA, 1992.
[Gardent, 2002] C. Gardent.
Generating Minimal DefiniteDescriptions.
In Proceedings of the 40th Annual Meetingof the ACL, 2002.
[Horacek, 1997] H. Horacek.
An Algorithm for GeneratingReferential Descriptions with Flexible Interfaces.
In Pro-ceedings of the 35th Annual Meeting of the ACL, 1997.
[Kopp and Wachsmuth, 2004] S. Kopp and I. Wachsmuth.Synthesizing Multimodal Utterances for ConversationalAgents.
Comp.
Anim.
Virtual Worlds, 15:39?52, 2004.
[Krahmer and van der Sluis, 2003] E. Krahmer andI.
van der Sluis.
A New Model for the Generationof Multimodal Referring Expressions.
In Proceedings ofENLG 2003, 2003.
[Krahmer et al, 2003] E. Krahmer, S. van Erk, and A. Ver-leg.
Graphbased Generation of Referring Expressions.Computational Linguistics, 29(1):53?72, 2003.
[Kranstedt et al, 2002] A. Kranstedt, S. Kopp, andI.
Wachsmuth.
MURML: A Multimodal UtteranceRepresentation Markup Language for ConversationalAgents.
In Proceedings of the Workshop EmbodiedConversational Agents ?
let?s specify and evaluate them,AAMAS 2002, 2002.
[Kranstedt et al, 2004] A. Kranstedt, P. Ku?hnlein, andI.
Wachsmuth.
Deixis in Multimodal Human Computer In-teraction: An Interdisciplinary Approach.
In A. Camurriand G. Volpe, editors, Gesture-based Communication inHuman-Computer Interaction, pages 112?123.
Springer(LNAI 2915), Berlin, 2004.
[Kranstedt et al, 2005] A. Kranstedt, A. Lu?cking, T. Pfeif-fer, H. Rieser, and I. Wachsmuth.
Deixis: How to Deter-mine Demonstrated Objects.
Presented at Gesture Work-shop 2005, Ile de Berder, France, 2005.
[Ku?hnlein and Stegmann, 2003] P. Ku?hnlein andJ.
Stegmann.
Empirical Issues in Deictic Gesture:Referring to Objects in Simple Identification Tasks.
Tech-nical Report 2003/3, SFB 360, University of Bielefeld,2003.
[Latoschik and Schilling, 2003] M. E. Latoschik andM.
Schilling.
Incorporating VR Databases into AIKnowledge Representations: A Framework for IntelligentGraphics Applications.
In Proceedings of the SixthIASTED International Conference on Computer Graphicsand Imaging, pages 79?84, 2003.
[Latoschik, 2001] M. E. Latoschik.
A General Frameworkfor Multimodal Interaction in Virtual Reality Systems:PrOSA.
In W. Broll and L. Scha?fer, editors, The Futureof VR and AR Interfaces - Multimodal, Humanoid, Adap-tive and Intelligent.
Proceedings of the Workshop at IEEEVirtual Reality 2001, pages 21?25, 2001.
[Lester et al, 1999] J. Lester, J. Voerman, S. Towns, andC.
Callaway.
Deictic Believability: Coordinating Gesture,Locomotion, and Speech in Lifelike Pedagogical Agents.Applied Artificial Intelligence, 13(4-5):383?414, 1999.
[Lu?cking et al, 2004] A. Lu?cking, H. Rieser, andJ.
Stegmann.
Statistical Support for the Study ofStructures in Multimodal Dialogue: Inter-rater Agreementand Synchronisation.
In Proceedings of the 8th Workshopon the Semantics and Pragmatics of Dialogue (Catalog04), pages 56?64, 2004.
[Lyons, 1977] J. Lyons.
Semantics (2 Vols.).
Cambridge Uni-versity Press, Cambridge, UK, 1977.
[Mann, 1988] W. C. Mann.
Dialogue Games: Conventionsof Human Interaction.
Argumentation, 2:512?532, 1988.
[McNeill, 1992] D. McNeill.
Hand and Mind.
University ofChicago Press, Chicago, Illinois, 1992.
[Noma and Badler, 1997] T. Noma and N. Badler.
A Vir-tual Human Presenter.
In Workshop on Animated InterfaceAgents: Making them Intelligent, IJCAI?97, pages 45?51,1997.
[Peirce, 1965] C. S. Peirce.
Collected Papers of CharlesSanders Peirce.
Cambridge University Press, Cambridge,UK, 1965.
[Reiter, 1990] E. Reiter.
The Computational Complexity ofAvoiding Conversational Implicatures.
In Proceedings ofthe 28th Annual Meeting of the ACL, pages 97?104, 1990.
[Reithinger, 1992] N. Reithinger.
The Performance of an In-cremental Generation Component for Multi-modal DialogContributions.
In R. Dale et al, editors, Aspects of Au-tomated Natural Language Generation.
Springer, Berlin,1992.
[Rickel and Johnson, 1999] J. Rickel and W. L. Johnson.Animated Agents for Procedural Training in Virtual Re-ality: Perception, Cognition, and Motor Control.
AppliedArtificial Intelligence, 13:343?382, 1999.
[Rieser, 2004] H. Rieser.
Pointing in Dialogue.
In Proceed-ings of the 8th Workshop on the Semantics and Pragmaticsof Dialogue (Catalog 04), pages 93?101, 2004.
[Sagerer et al, 1994] G. Sagerer, H. Eikmeyer, and G. Rick-heit.
?Dies ist ein runder Gegenstand?.
Technical report,University of Bielefeld, SFB 360, 1994.
[Tramberend, 1999] H. Tramberend.
Avocado: A Dis-tributed Virtual Reality Framework.
In Proceedings ofIEEE Virtual Reality 1999, pages 14?21, 1999.
[van der Sluis and Krahmer, 2004] I. van der Sluis andE.
Krahmer.
The Influence of Target Size and Distanceon the Production of Speech and Gesture in MultimodalReferring Expressions.
In Proceedings of the 8the Interna-tional Conference on Spoken Language Processing, 2004.
