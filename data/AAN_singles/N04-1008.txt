Automatic Question Answering: Beyond the FactoidRadu SoricutInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292, USAradu@isi.eduEric BrillMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USAbrill@microsoft.comAbstractIn this paper we describe and evaluate a Ques-tion Answering system that goes beyond an-swering factoid questions.
We focus on FAQ-like questions and answers, and build our sys-tem around a noisy-channel architecture whichexploits both a language model for answersand a transformation model for an-swer/question terms, trained on a corpus of 1million question/answer pairs collected fromthe Web.1 IntroductionThe Question Answering (QA) task has received a greatdeal of attention from the Computational Linguisticsresearch community in the last few years (e.g., Text RE-trieval Conference TREC 2001-2003).
The definition ofthe task, however, is generally restricted to answeringfactoid questions: questions for which a complete answercan be given in 50 bytes or less, which is roughly a fewwords.
Even with this limitation in place, factoid ques-tion answering is by no means an easy task.
The chal-lenges posed by answering factoid question have beenaddressed using a large variety of techniques, such asquestion parsing (Hovy et al, 2001; Moldovan et al,2002), question-type determination (Brill et al, 2001;Ittycheraih and Roukos, 2002;   Hovy et al, 2001;Moldovan et al, 2002), WordNet exploitation (Hovy etal., 2001; Pasca and Harabagiu, 2001; Prager et al,2001), Web exploitation (Brill et al, 2001; Kwok et al,2001), noisy-channel transformations (Echihabi andMarcu, 2003), semantic analysis (Xu et al, 2002; Hovyet al, 2001; Moldovan et al, 2002), and inferencing(Moldovan et al, 2002).The obvious limitation of any factoid QA system isthat many questions that people want answers for are notfactoid questions.
It is also frequently the case that non-factoid questions are the ones for which answers cannotas readily be found by simply using a good search en-gine.
It follows that there is a good economic incentivein moving the QA task to a more general level: it islikely that a system able to answer complex questions ofthe type people generally and/or frequently ask hasgreater potential impact than one restricted to answeringonly factoid questions.
A natural move is to recast thequestion answering task to handling questions peoplefrequently ask or want answers for, as seen in FrequentlyAsked Questions (FAQ) lists.
These questions are some-times factoid questions (such as, ?What is Scotland'snational costume??
), but in general are more complexquestions (such as, ?How does a film qualify for anAcademy Award?
?, which requires an answer along thefollowing lines: ?A feature film must screen in a LosAngeles County theater in 35 or 70mm or in a 24-frameprogressive scan digital format suitable for exhibiting inexisting commercial digital cinema sites for paid admis-sion for seven consecutive days.
The seven day run mustbegin before midnight, December 31, of the qualifyingyear.
[?]?
).In this paper, we make a first attempt towards solv-ing a QA problem more generic than factoid QA, forwhich there are no restrictions on the type of questionsthat are handled, and there is no assumption that the an-swers to be provided are factoids.
In our solution to thisproblem we employ learning mechanisms for question-answer transformations (Agichtein et al, 2001; Radev etal., 2001), and also exploit large document collectionssuch as the Web for finding answers (Brill et al, 2001;Kwok et al, 2001).
We build our QA system around anoisy-channel architecture which exploits both a lan-guage model for answers and a transformation model foranswer/question terms, trained on a corpus of 1 millionquestion/answer pairs collected from the Web.
Ourevaluations show that our system achieves reasonableperformance in terms of answer accuracy for a large va-riety of complex, non-factoid questions.2 Beyond Factoid Question AnsweringOne of the first challenges to be faced in automatic ques-tion answering is the lexical and stylistic gap betweenthe question string and the answer string.
For factoidquestions, these gaps are usually bridged by questionreformulations, from simple rewrites (Brill et al, 2001),to more sophisticated paraphrases (Hermjakob et al,2001), to question-to-answer translations (Radev et al,2001).
We ran several preliminary trials using variousquestion reformulation techniques.
We found out that ingeneral, when complex questions are involved, reformu-lating the question (using either simple rewrites or ques-tion-answer term translations) more often hurts theperformance than improves on it.Another widely used technique in factoid QA issentence parsing, along with question-type determina-tion.
As mentioned by Hovy et al (2001), their hierar-chical QA typology contains 79 nodes, which in manycases can be even further differentiated.
While we ac-knowledge that QA typologies and hierarchical questiontypes have the potential to be extremely useful beyondfactoid QA, the volume of work involved is likely toexceed by orders of magnitude the one involved in theexisting factoid QA typologies.
We postpone such workfor future endeavors.The techniques we propose for handling our ex-tended QA task are less linguistically motivated andmore statistically driven.
In order to have access to theright statistics, we first build a question-answer pairtraining corpus by mining FAQ pages from the Web, asdescribed in Section 3.
Instead of sentence parsing, wedevise a statistical chunker that is used to transform aquestion into a phrase-based query (see Section 4).
Aftera search engine uses the formulated query to return the Nmost relevant documents from the Web, an answer to thegiven question is found by computing an answer lan-guage model probability (indicating how similar the pro-posed answer is to answers seen in the training corpus),and an answer/question translation model probability(indicating how similar the proposed answer/questionpair is to pairs seen in the training corpus).
In Section 5we describe the evaluations we performed in order toassess our system?s performance, while in Section 6 weanalyze some of the issues that negatively affected oursystem?s performance.3 A Question-Answer Corpus for FAQsIn order to employ the learning mechanisms described inthe previous section, we first need to build a large train-ing corpus consisting of question-answer pairs of a broadlexical coverage.
Previous work using FAQs as a sourcefor finding an appropriate answer (Burke et al, 1996) orfor learning lexical correlations (Berger et al, 2000)focused on using the publicly available Usenet FAQcollection and other non-public FAQ collections, andreportedly worked with an order of thousands of ques-tion-answer pairs.Our approach to question/answer pair collectiontakes a different path.
If one poses the simple query?FAQ?
to an existing search engine, one can observe thatroughly 85% of the returned URL strings correspondingto genuine FAQ pages contain the substring ?faq?, whilevirtually all of the URLs that contain the substring ?faq?are genuine FAQ pages.
It follows that, if one has accessto a large collection of the Web?s existent URLs, a sim-ple pattern-matching for ?faq?
on these URLs will havea recall close to 85% and precision close to 100% onreturning FAQ URLs from those available in the collec-tion.
Our URL collection contains approximately 1 bil-lion URLs, and using this technique we extractedroughly 2.7 million URLs containing the (uncased)string ?faq?, which amounts to roughly 2.3 million FAQURLs to be used for collecting question/answer pairs.The collected FAQ pages displayed a variety of for-mats and presentations.
It seems that the variety of waysquestions and answers are usually listed in FAQ pagesdoes not allow for a simple high-precision high-recallsolution for extracting question/answer pairs: if oneassumes that only certain templates are used whenpresenting FAQ lists, one can obtain clean ques-tion/answer pairs at the cost of losing many other suchpairs (which happen to be presented in different tem-plates); on the other hand, assuming very loose con-straints on the way information is presented on suchpages, one can obtain a bountiful set of question/answerpairs, plus other pairs that do not qualify as such.
Wesettled for a two-step approach: a first recall-orientedpass based on universal indicators such as punctuationand lexical cues allowed us to retrieve most of the ques-tion/answer pairs, along with other noise data; a secondprecision-oriented pass used several filters, such as lan-guage identification, length constrains, and lexical cuesto reduce the level of noise of the question/answer paircorpus.
Using this method, we were able to collect a totalof roughly 1 million question/answer pairs, exceeding byorders of magnitude the amount of data previously usedfor learning question/answer statistics.4 A QA System ArchitectureThe architecure of our QA system is presented in Figure1.
There are 4 separate modules that handle variousstages in the system?s pipeline: the first module is calledQuestion2Query, in which questions posed in naturallanguage are transformed into phrase-based queries be-fore being handed down to the SearchEngine module.The second module is an Information Retrieval enginewhich takes a query as input and returns a list of docu-ments deemed to be relevant to the query in a sortedmanner.
A third module, called Filter, is in charge offiltering out the returned list of documents, in order toprovide acceptable input to the next module.
The forthmodule, AnswerExtraction, analyzes the content pre-sented and chooses the text fragment deemed to be thebest answer to the posed question.Figure 1: The QA system architectureThis architecture allows us to flexibly test for vari-ous changes in the pipeline and evaluate their overalleffect.
We present next detailed descriptions of how eachmodule works, and outline several choices that presentthemselves as acceptable options to be evaluated.4.1 The Question2Query ModuleA query is defined to be a keyword-based string thatusers are expected to feed as input to a search engine.Such a string is often thought of as a representation for auser?s ?information need?, and being proficient in ex-pressing one?s ?need?
in such terms is one of the keypoints in successfully using a search engine.
A naturallanguage-posed question can be thought of as such aquery.
It has the advantage that it forces the user to paymore attention to formulating the ?information need?
(and not typing the first keywords that come to mind).
Ithas the disadvantage that it contains not only the key-words a search engine normally expects, but also a lot ofextraneous ?details?
as part of its syntactic and discourseconstraints, plus an inherently underspecified unit-segmentation problem, which can all confuse the searchengine.To counterbalance some of these disadvantages, webuild a statistical chunker that uses a dynamic program-ming algorithm to chunk the question intochunks/phrases.
The chunker is trained on the answerside of the Training corpus in order to learn 2 and 3-word collocations, defined using the likelihood ratio ofDunning (1993).
Note that we are chunking the questionusing answer-side statistics, precisely as a measure forbridging the stylistic gap between questions and answers.Our chunker uses the extracted collocation statisticsto make an optimal chunking using a Dijkstra-style dy-namic programming algorithm.
In Figure 2 we presentan example of the results returned by our statisticalchunker.
Important cues such as ?differ from?
and?herbal medications?
are presented as phrases to thesearch engine, therefore increasing the recall of thesearch.
Note that, unlike a segmentation offered by aparser (Hermjakob et al, 2001), our phrases are not nec-essarily syntactic constituents.
A statistics-based chunkeralso has the advantage that it can be used ?as-is?
forquestion segmentation in languages other than English,provided training data (i.e., plain written text) is avail-able.Figure 2: Question segmentation into query using astatistical chunker4.2 The SearchEngine ModuleThis module consists of a configurable interface withavailable off-the-shelf search engines.
It currently sup-ports MSNSearch and Google.
Switching from onesearch engine to another allowed us to measure the im-pact of the IR engine on the QA task.4.3 The Filter ModuleThis module is in charge of providing the AnswerExtrac-tion module with the content of the pages returned by thesearch engine, after certain filtering steps.
One first stepis to reduce the volume of pages returned to only a man-ageable amount.
We implement this step as choosing toreturn the first N hits provided by the search engine.Other filtering steps performed by the Filter Moduleinclude tokenization and segmentation of text into sen-tences.One more filtering step was needed for evaluationpurposes only: because both our training and test datawere collected from the Web (using the procedure de-scribed in Section 3), there was a good chance that ask-ing a question previously collected returned its alreadyavailable answer, thus optimistically biasing our evalua-tion.
The Filter Module therefore had access to the refer-ence answers for the test questions as well, and ensuredthat, if the reference answer matched a string in someretrieved page, that page was discarded.
Moreover, wefound that slight variations of the same answer coulddefeat the purpose of the string-matching check.
For thepurpose of our evaluation, we considered that if thequestion/reference answer pair had a string of 10 wordsor more identical with a string in some retrieved page,that page was discarded as well.
Note that, outside theQuestion2QueryModuleQ Search EngineModuleFilterModuleAnswer ExtractionModuleAQueryDocumentsAnswerListTrainingCorpusWebQueryHow do herbal medications differ fromconventional drugs?
"How do" "herbal medications" "differ from""conventional" "drugs"evaluation procedure, the string-matching filtering stepis not needed, and our system?s performance can onlyincrease by removing it.4.4 The AnswerExtraction ModuleAuthors of previous work on statistical approaches toanswer finding (Berger et al, 2000) emphasized the needto ?bridge the lexical chasm?
between the question termsand the answer terms.
Berger et al showed that tech-niques that did not bridge the lexical chasm were likelyto perform worse than techniques that did.For comparison purposes, we consider two differentalgorithms for our AnswerExtraction module: one thatdoes not bridge the lexical chasm, based on N-gram co-occurrences between the question terms and the answerterms; and one that attempts to bridge the lexical chasmusing Statistical Machine Translation inspired techniques(Brown et al, 1993) in order to find the best answer for agiven question.For both algorithms, each 3 consecutive sentencesfrom the documents provided by the Filter module forma potential answer.
The choice of 3 sentences comesfrom the average number of sentences in the answersfrom our training corpus.
The choice of consecutivenesscomes from the empirical observation that answers builtup from consecutive sentences tend to be more coherentand contain more non-redundant information than an-swers built up from non-consecutive sentences.4.4.1 N-gram Co-Occurrence Statistics for AnswerExtractionN-gram co-occurrence statistics have been successfullyused in automatic evaluation (Papineni et al 2002, Linand Hovy 2003), and more recently as training criteria instatistical machine translation (Och 2003).We implemented an answer extraction algorithmusing the BLEU score of Papineni et al (2002) as ameans of assessing the overlap between the question andthe proposed answers.
For each potential answer, theoverlap with the question was assessed with BLEU (withthe brevity penalty set to penalize answers shorter than 3times the length of the question).
The best scoring poten-tial answer was presented by the AnswerExtractionModule as the answer to the question.4.4.2 Statistical Translation for Answer ExtractionAs proposed by Berger et al (2000), the lexical gap be-tween questions and answers can be bridged by a statis-tical translation model between answer terms andquestion terms.
Their model, however, uses only an An-swer/Question translation model (see Figure 3) as ameans to find the answer.A more complete model for answer extraction canbe formulated in terms of a noisy channel, along thelines of Berger and Lafferty (2000) for the InformationRetrieval task, as illustrated in Figure 3: an answer gen-eration model proposes an answer A according to an an-swer generation probability distribution; answer A isfurther transformed into question Q by an an-swer/question translation model according to a question-given-answer conditional probability distribution.
Thetask of the AnswerExtraction algorithm is to take thegiven question q and find an answer a in the potentialanswer list that is most likely both an appropriate andwell-formed answer.Figure 3: A noisy-channel model for answerextractionThe AnswerExtraction procedure employed dependson the task T we want it to accomplish.
Let the task T bedefined as ?find a 3-sentence answer for a given ques-tion?.
Then we can formulate the algorithm as findingthe a-posteriori most likely answer given question andtask, and write it as p(a|q,T).
We can use Bayes?
law towrite this as:)|()|(),|(),|(TqpTapTaqpTqap ?=  (1)Because the denominator is fixed given question andtask, we can ignore it and find the answer that maxi-mizes the probability of being both a well-formed and anappropriate answer as:4342143421dependentquestiontindependenquestionaTaqpTapa??
?= ),|()|(maxarg  (2)The decomposition of the formula into a question-independent term and a question-dependent term allowsus to separately model the quality of a proposed answera with respect to task T, and to determine the appropri-ateness of the proposed answer a with respect to ques-tion q to be answered in the context of task T.Because task T fits the characteristics of the ques-tion-answer pair corpus described in Section 3, we canuse the answer side of this corpus to compute the priorprobability p(a|T).
The role of the prior is to help down-grading those answers that are too long or too short, orare otherwise not well-formed.
We use a standard tri-gram language model to compute the probability distri-bution p(?|T).The mapping of answer terms to question terms ismodeled using Black et al?s (1993) simplest model,called IBM Model 1.
For this reason, we call our modelAnswerGenerationModelA QAnswer ExtractionAlgorithmq aAnswer/QuestionTranslationModelModel 1 as well.
Under this model, a question is gener-ated from an answer a of length n according to the fol-lowing steps: first, a length m is chosen for the question,according to the distribution ?
(m|n) (we assume thisdistribution is uniform); then, for each position j in q, aposition i in a is chosen from which qj is generated, ac-cording to the distribution t(?| ai ).
The answer is as-sumed to include a NULL word, whose purpose is togenerate the content-free words in the question (such asin ?Can you please tell me???).
The correspondencebetween the answer terms and the question terms iscalled an alignment, and the probability p(q|a) is com-puted as the sum over all possible alignments.
We ex-press this probability using the following formula:))|(11))|()|((1()|()|(11NULLqtnaacaqtnnnmaqpjiinijmj+++?+= ??
==?
(3)where t(qj| ai ) are the probabilities of ?translating?
an-swer terms into question terms, and c(ai|a) are the rela-tive counts of the answer terms.
Our parallel corpus ofquestions and answers can be used to compute the trans-lation table t(qj| ai ) using the EM algorithm, as describedby Brown et al (1993).
Note that, similarly with thestatistical machine translation framework, we deal herewith ?inverse?
probabilities, i.e.
the probability of aquestion term given an answer, and not the more intui-tive probability of answer term given question.Following Berger and Lafferty (2000), an even sim-pler model than Model 1 can be devised by skewing thetranslation distribution t(?| ai ) such that all the probabil-ity mass goes to the term ai.
This simpler model is calledModel 0.
In Section 5 we evaluate the proficiency ofboth Model 1 and Model 0 in the answer extraction task.5 Evaluations and DiscussionsWe evaluated our QA system systematically for eachmodule, in order to assess the impact of various algo-rithms on the overall performance of the system.
Theevaluation was done by a human judge on a set of 115Test questions, which contained a large variety of non-factoid questions.
Each answer was rated as either cor-rect(C), somehow related(S), wrong(W), or cannottell(N).
The somehow related option allowed the judgeto indicate the fact that the answer was only partiallycorrect (for example, because of missing information, orbecause the answer was more general/specific than re-quired by the question, etc.).
The cannot tell option wasused in those cases when the validity of the answer couldnot be assessed.
Note that the judge did not have accessto any reference answers in order to asses the quality of aproposed answer.
Only general knowledge and humanjudgment were involved when assessing the validity ofthe proposed answers.
Also note that, mainly becauseour system?s answers were restricted to a maximum of 3sentences, the evaluation guidelines stated that answersthat contained the right information plus other extrane-ous information were to be rated correct.For the given set of Test questions, we estimated theperformance of the system using the formula(|C|+.5|S|)/(|C|+|S|+|W|).
This formula gives a score of 1if the questions that are not ?N?
rated are all consideredcorrect, and a score of 0 if they are all considered wrong.A score of 0.5 means that, in average, 1 out of 2 ques-tions is answered correctly.5.1 Question2Query Module EvaluationWe evaluated the Question2Query module while keepingfixed the configuration of the other modules(MSNSearch as the search engine, the top 10 hits in theFilter module), except for the AnswerExtraction module,for which we tested both the N-gram co-occurrencebased algorithm (NG-AE) and a Model 1 based algo-rithm (M1e-AE, see Section 5.4).The evaluation assessed the impact of the statisticalchunker used to transform questions into queries, againstthe baseline strategy of submitting the question as-is tothe search engine.
As illustrated in Figure 4, the overallperformance of the QA system significantly increasedwhen the question was segmented before being submit-ted to the SearchEngine module, for both AnswerExtrac-tion algorithms.
The score increased from 0.18 to 0.23when using the NG-AE algorithm, and from 0.34 to 0.38when using the M1e-AE algorithm.00.10.20.30.4NG-AE M1e-AEAs-isSegmentedFigure 4: Evaluation of the Question2Querymodule5.2 SearchEngine Module EvaluationThe evaluation of the SearchEngine module assessed theimpact of different search engines on the overall systemperformance.
We fixed the configurations of the othermodules (segmented question for the Question2Querymodule, top 10 hits in the Filter module), except for theAnswerExtraction module, for which we tested the per-formance while using for answer extraction the NG-AE,M1e-AE, and ONG-AE algorithms.
The later algorithmworks exactly like NG-AE, with the exception that thepotential answers are compared with a reference answeravailable to an Oracle, rather than against the question.The performance obtained using the ONG-AE algorithmcan be thought of as indicative of the ceiling in the per-formance that can be achieved by an AE algorithm giventhe potential answers available.As illustrated in Figure 5, both the MSNSearch andGoogle search engines achieved comparable perform-ance accuracy.
The scores were 0.23 and 0.24 when us-ing the NG-AE algorithm, 0.38 and 0.37 when using theM1e-AE algorithm, and 0.46 and 0.46 when using theONG-AE algorithm, for MSNSearch and Google, re-spectively.
As a side note, it is worth mentioning thatonly 5% of the URLs returned by the two search enginesfor the entire Test set of questions overlapped.
There-fore, the comparable performance accuracy was not dueto the fact that the AnswerExtraction module had accessto the same set of potential answers, but rather to the factthat the 10 best hits of both search engines provide simi-lar answering options.00.10.20.30.40.5NG-AE M1e-AE ONG-AEMSNSearchGoogleFigure 5: MSNSearch and Google give similarperformance both in terms of realistic AEalgorithms and oracle-based AE algorithms5.3 Filter Module EvaluationAs mentioned in Section 4, the Filter module filters outthe low score documents returned by the search engineand provides a set of potential answers extracted fromthe N-best list of documents.
The evaluation of the Filtermodule therefore assessed the trade-off between compu-tation time and accuracy of the overall system: the sizeof the set of potential answers directly influences theaccuracy of the system while increasing the computationtime of the AnswerExtraction module.
The ONG-AEalgorithm gives an accurate estimate of the performanceceiling induced by the set of potential answers availableto the AnswerExtraction Module.As illustrated in Figure 6, there is a significant per-formance ceiling increase from considering only thedocument returned as the first hit (0.36) to consideringthe first 10 hits (0.46).
There is only a slight increase inperformance ceiling, however, from considering the first10 hits to considering the first 50 hits (0.46 to 0.49).00.10.20.30.40.5First Hit First 10HitsFirst 50HitsONG-AEFigure 6: The scores obtained using the ONG-AEanswer extraction algorithm for various N-best lists5.4 AnswerExtraction Module EvaluationThe Answer-Extraction module was evaluated whilefixing all the other module configurations (segmentedquestion for the Question2Query module, MSNSearch asthe search engine, and top 10 hits in the Filter module).The algorithm based on the BLEU score, NG-AE,and its Oracle-informed variant ONG-AE, do not dependon the amount of training data available, and thereforethey performed uniformly at 0.23 and 0.46, respectively(Figure 7).
The score of 0.46 can be interpreted as a per-formance ceiling of the AE algorithms given the avail-able set of potential answers.The algorithms based on the noisy-channel architec-ture displayed increased performance with the increasein the amount of available training data, reaching as highas 0.38.
An interesting observation is that the extractionalgorithm using Model 1 (M1-AE) performed poorerthan the extraction algorithm using Model 0 (M0-AE),for the available training data.
Our explanation is thatthe probability distribution of question terms given an-swer terms learnt by Model 1 is well informed (manymappings are allowed) but badly distributed, whereas theprobability distribution learnt by Model 0 is poorly in-formed (indeed, only one mapping is allowed), but betterdistributed.
Note the steep learning curve of Model 1,whose performance gets increasingly better as the distri-bution probabilities of various answer terms (includingthe NULL word) become more informed (more map-pings are learnt), compared to the gentle learning curveof Model 0, whose performance increases slightly onlyas more words become known as self-translations to thesystem (and the distribution of the NULL word gets bet-ter approximated).From the above analysis, it follows that a modelwhose probability distribution of question terms givenanswer terms is both well informed and well distributedis likely to outperform both M1-AE and M0-AE.
Such amodel was obtained when Model 1 was trained on boththe question/answer parallel corpus from Section 3 andan artificially created parallel corpus in which each ques-tion had itself as its ?translation?.
This training regimeallowed the model to assign high probabilities to identitymappings (and therefore be better distributed), while alsodistributing some probability mass to other question-answer term pairs (and therefore be well informed).
Wecall the extraction algorithm that uses this model M1e-AE, and the top score of 0.38 was obtained by M1e-AEwhen trained on 1 million question/answer pairs.
Notethat the learning curve of algorithm M1e-AE in Figure 7indeed indicates that this answer extraction procedure iswell informed about the distribution probabilities of vari-ous answer terms (it has the same steepness in thelearning curve as for M1-AE), while at the same timeuses a better distribution of the probability mass for eachanswer term compared to M1-AE (it outperforms M1-AE by roughly a constant amount for each training setsize in the evaluation).104 105 1060.20.250.30.350.40.450.5Training size (QA pairs)AccuracyONG?AEM0?AEM1e?AENG?AEM1?AEFigure 7: The performance of our QA system withvarious answer extraction algorithms and differenttraining set sizes6 Performance issuesIn building our system, we have demonstrated that astatistical model can capitalize on large amounts of read-ily available training data to achieve reasonable per-formance on answering non-factoid questions.
Oursystem was able to successfully answer a large variety ofcomplex, non-factoid questions, such as:Q: Can I travel with the Ameripass in Mexico?A: The Ameripass includes all of USA excluding Alaska.In Canada, it is valid for travel to Montreal, Quebec, To-ronto, Ontario, Vancouver BC, and New Westminster BC.In Mexico it is valid for travel to Matamoros, NuevoLaredo, Tamaulipas and Tijuana Baja California.Q: What is a PIC Freeze?A: How to Prevent Slamming: Institute a PIC Freeze Thebest way to keep from being slammed, or having a long dis-tance company change your service without your permis-sion, is to request a "PIC freeze" for your line.
PIC, or pre-subscribed interexchange carrier, is the telecom industry 'sterm for a long distance company.For those questions which were not answered cor-rectly, we identified some of the most frequent causeswhich led to erroneous answers:?
answer was not in the retrieved pages (see the 46%performance ceiling given by the Oracle)?
answer was of the wrong ?type?
(e.g., an answer for?how-to?
instead of ?what-is?
):Q: What are best graduate schools for AI?A: If you are applying for grad school in AI, and you didsome research for an AI person, even if it was long agoand you don't consider it to be relevant, get a recommen-dation if you think that the person likes you at all.
[?]?
it pointed to where an answer might be instead ofanswering the question:Q: What do research studies say about massage therapy?A: It supports research on the efficacy of therapeuticmassage through the public, charitable AMTA Founda-tion.
Additional information about massage therapy andabout AMTA is available via the Web atwww.amtamassage.org.?
the translation model overweighed the answer lan-guage model (too good a "translation", too bad ananswer)Q: What are private and public keys?A: Private and public keys Private and public keys Algo-rithms can use two types of keys: private and public.?
did not pick up the key content word (in the exam-ple below, eggs)Q: What makes eggs have thin, brittle shells?A: The soft-shelled clams, such as steamer, razor, andgeoduck clams, have thin brittle shells that can't com-pletely close.
Cod - A popular lean, firm, white meatfish from the Pacific and the North Atlantic.It is worth pointing out that most of these errors do notarise from within a single module, but rather they are theresult of various interactions between modules that misson some relevant information.7 ConclusionsPrevious work on question answering has focused almostexclusively on building systems for handling factoidquestions.
These systems have recently achieved impres-sive performance (Moldovan et al, 2002).
The worldbeyond the factoid questions, however, is largely unex-plored, with few notable exceptions (Berger et al, 2001;Agichtein et al, 2002; Girju 2003).
The present paperattempts to explore the portion related to answeringFAQ-like questions, without restricting the domain ortype of the questions to be handled, or restricting thetype of answers to be provided.
While we still have along way to go in order to achieve robust non-factoidQA, this work is a step in a direction that goes beyondrestricted questions and answers.We consider the present QA system as a baseline onwhich more finely tuned QA architectures can be built.Learning from the experience of factoid question an-swering, one of the most important features to be addedis a question typology for the FAQ domain.
Efforts to-wards handling specific question types, such as causalquestions, are already under way (Girju 2003).
A care-fully devised typology, correlated with a systematic ap-proach to fine tuning, seem to be the lessons for successin answering both factoid and beyond factoid questions.ReferencesEugene Agichten, Steve Lawrence, and Luis Gravano.2002.
Learning to Find Answers to Questions on theWeb.
ACM Transactions on Internet Technology.Adam L. Berger, John D. Lafferty.
1999.
InformationRetrieval as Statistical Translation.
Proceedings ofthe SIGIR 1999, Berkeley, CA.Adam Berger, Rich Caruana, David Cohn, DayneFreitag, Vibhu Mittal.
2000.
Bridging the LexicalChasm: Statistical Approaches to Answer-Finding.Research and Development in Information Retrieval,pages 192--199.Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais,Andrew Ng.
2001.
Data-Intensive Question Answer-ing.
Proceedings of the TREC-2001Conference, NIST.Gaithersburg, MD.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263--312.Robin Burke, Kristian Hammond, Vladimir Kulyukin,Steven Lytinen, Noriko Tomuro, and Scott Schoen-berg.
1997.
Question Answering from Frequently-Asked-Question Files: Experiences with the FAQFinder System.
Tech.
Rep. TR-97-05, Dept.
of Com-puter Science, University of Chicago.Ted Dunning.
1993.
Accurate Methods for the Statisticsof Surprise and Coincidence.
Computational Linguis-tics, Vol.
19, No.
1.Abdessamad Echihabi and Daniel Marcu.
2003.
A Noisy-Channel Approach to Question Answering.
Proceed-ings of the ACL 2003.
Sapporo, Japan.Roxana Garju.
2003.
Automatic Detection of CausalRelations for Question Answering.
Proceedings of theACL 2003, Workshop on "Multilingual Summariza-tion and Question Answering - Machine Learning andBeyond", Sapporo, Japan.Ulf Hermjakob, Abdessamad Echihabi, and DanielMarcu.
2002.
Natural Language Based ReformulationResource and Web Exploitation for Question Answer-ing.
Proceedings of the TREC-2002 Conference,NIST.
Gaithersburg, MD.Abraham Ittycheriah and Salim Roukos.
2002.
IBM'sStatistical Question Answering System-TREC 11.
Pro-ceedings of the TREC-2002 Conference, NIST.Gaithersburg, MD.Cody C. T. Kwok, Oren Etzioni, Daniel S. Weld.
ScalingQuestion Answering to the Web.
2001.
WWW10.Hong Kong.Chin-Yew Lin and E.H. Hovy.
2003.
Automatic Evalua-tion of Summaries Using N-gram Co-occurrence Sta-tistics.
Proceedings of the HLT/NAACL 2003.Edmonton, Canada.Dan Moldovan, Sanda Harabagiu, Roxana Girju, PaulMorarescu, Finley Lacatusu, Adrian Novischi, Adri-ana Badulescu, Orest Bolohan.
2002.
LCC Tools forQuestion Answering.
Proceedings of the TREC-2002Conference, NIST.
Gaithersburg, MD.Franz Joseph Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
Proceedings of theACL 2003.
Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, Wei-JingZhu.
2002.
Bleu: a Method for Automatic Evaluationof Machine Translation.
Proceedings of the ACL2002.
Philadephia, PA.Marius Pasca, Sanda Harabagiu, 2001.
The InformativeRole of WordNet in Open-Domain Question Answer-ing.
Proceedings of the NAACL 2001 Workshop onWordNet and Other Lexical Resources, CarnegieMellon University.
Pittsburgh, PA.John M. Prager, Jennifer Chu-Carroll, Krysztof Czuba.2001.
Use of WordNet Hypernyms for AnsweringWhat-Is Questions.
Proceedings of the TREC-2002Conference, NIST.
Gaithersburg, MD.Dragomir Radev, Hong Qi, Zhiping Zheng, Sasha Blair-Goldensohn, Zhu Zhang, Weiguo Fan, and JohnPrager.
2001.
Mining the Web for Answers to NaturalLanguage Questions.
Tenth International ConferenceonInformation and Knowledge Management.
Atlanta,GA.Jinxi Xu, Ana Licuanan, Jonathan May, Scott Miller,Ralph Weischedel.
2002.
TREC 2002 QA at BBN:Answer Selection and Confidence Estimation.
Pro-ceedings of the TREC-2002 Conference, NIST.Gaithersburg, MD.
