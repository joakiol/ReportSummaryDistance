Decomposable Modeling in NaturalLanguage ProcessingRebecca  F. Bruce*University of North Carolinaat AshevilleJanyce M. Wiebe  tNew Mexico State UniversityIn this paper, we describe aframework for developing probabilistic lassifiers in natural anguageprocessing.
Our focus is on formulating models that capture the most important interdependenciesamong features, to avoid overfitting the data while also characterizing the data well.
The classof probability models and the associated inference techniques described here were developed inmathematical statistics, and are widely used in artificial intelligence and applied statistics.
Ourgoal is to make this model selection framework accessible to researchers in NLP, and providepointers to available software and important references.
In addition, we describe how the qualityof the three determinants of classifier performance (the features, the form of the model, andthe parameter estimates) can be separately evaluated.
We also demonstrate he classificationperformance of these models in a large-scale xperiment involving the disambiguation of34words taken from the HECTOR word sense corpus (Hanks 1996).
In lO-fold cross-validations,the model search procedure performs ignificantly better than naive Bayes on 6 of the wordswithout being significantly worse on any of them.1.
IntroductionThis paper describes a framework for developing probabilistic lassifiers in naturallanguage processing (NLP).
1 A probabilistic classifier assigns the most probable class toan object, based on a probability model of the interdependencies among the class and aset of input features.
This paper focuses on formulating a model that captures the mostimportant interdependencies, to avoid overfitting the data while also characterizingthe data well.
The goal is to achieve a balance between feasibility and expressivepower, which is needed in an area as complex as NLP.The class of probability models and the associated inference techniques describedhere were developed in mathematical statistics, and are widely used in artificial intel-ligence and applied statistics.
However, these techniques have not been widely usedin NLP, although the software required to implement these procedures i freely avail-able.
Within this framework, we can unify many of the metrics and types of modelscurrently used in NLP.
The class of models, decomposable models, is large and ex-pressive, yet there are computationally feasible model search procedures defined forthem.
They can include any kind of discrete variable, and the formality of the methodsupports evaluation.In this paper, our goal is to make this model selection framework accessible to re-searchers in NLP, by providing a concise xplanation of the underlying theor~ pointing* Department of Computer Science, Asheville, NC 28804-3299.Department of Computer Science, Las Cruces, NM 88003.1 This framework was originally introduced into NLP in Bruce and Wiebe (1994).
(~) 1999 Association for Computational LinguisticsComputational Linguistics Volume 25, Number 2out relationships to existing NLP research, and providing pointers to available soft-ware and important references.
In addition, we describe how the quality of the threedeterminants of classifier performance (the features, the form of the model and theparameter estimates) can be separately evaluated.We also demonstrate he classification performance of these models in a large-scale experiment involving the disambiguation f 34 words taken from the HECTORword sense corpus (Atkins 1993; Hanks 1996).
We compare the performance of classi-fiers based on models elected by this algorithm with the performance ofnaive Bayesclassifiers (classifiers based on the naive Bayes model).
Naive Bayes classifiers havebeen found to be remarkably successful in many applications, including word sensedisambiguation (Mooney 1996).
In 10-fold cross-validations, the model search proce-dure achieves an overall 1.4 percentage point improvement over naive Bayes, and issignificantly better on 6 of the words without being significantly worse on any ofthem.2.
Probabilistic ModelingWe will use word sense disambiguation f the word interest as a concrete xample inthis section.
For simplicity, we will use only two contextual features, the part of speechof the word to the left and the part of speech of the word to the right.
Assume thatthere are 8 senses of interest and 20 part of speech tags.
We will map the features tofeature variables and the sense tag to the classification variable, yielding a discrete,finite random vector X -- (FV1 .
.
.
.
.
Fl,~w, CV) (where w here is 2).Suppose that there are N occurrences of interest in the training sample.
The trainingsample is viewed as being composed of a set of N independent and identical trialsdrawn from a three-variable population distribution.
The outcome of each trial isa particular combination of the values of the three variables, i.e., one of the 3,200(8 x 20 x 20) possible configurations of the variables in X.
Let y~ be the frequency andPi the probability of the i th configuration of the variables in X.
Then (/:1 .
.
.
.
.
f3,200) hasa multinomial distribution with parameters (N, P1,..., P3,200), where N is fixed.
Theparameters P1.
.
.
.
.
P3,200 define the joint probability distribution of the variables in X.These parameters could be estimated irectly from counts in the training data; thatis, we could use the unrestricted maximum-likelihood estimate of Pi (Mood, Graybill,and Boes 1974):=~.If there is not enough training data for the estimation task at hand, then there aremany configurations of the variables that seldom or never occur in the training data.For these, the unrestricted maximum-likelihood estimates are unreliable.An alternative is to hypothesize conditional independence assumptions of theform: variables FVi and FVj are conditionally independent of one another, given thevalues of the remaining variables.
Then, we need only count the configurations of thesets of variables that are still interdependent.A simple example will show why (Whittaker 1990).
Recall that X is a vector ofthree binary variables, X = (FV1, FV2, CV).
There are 3,200 parameters tobe estimated,namely:P(FV1 = 0, FV2 = O, CV = 0), P(FV1 = O, FV2 = O, CV = 1) .
.
.
.
.P(FV1 -- 19, FV2 = 19, CV -- 6), P(FV1 = 19, FV2 = 19, CV = 7).196Bruce and Wiebe Decomposable Modeling in Natural Language ProcessingThe joint distribution can be expressed as follows, according to a basic axiom of prob-ability theory (where fVl, fv2, and cv represent particular values of FV1, FV2, and CV,respectively):P(fvl,fv2, cv) = P(fVl \] y~02, cv)P(fv2 I cv)P(cv) (1)But if fV 1 and fv2 are conditionally independent given the value of cv, then the jointdistribution can be expressed as follows:P(fPl,fv2, cv) = P(fu I \] ?v)P(fv 2 \]cv)P(cv)P0COl, cv) P(fv2, cv) n, ,- -  P (cv)= P0rvl, cv)P(fv2, cv) (2)P(cv)The parameters of the model expressed in (2) are the terms on the right-handside of the equation.
They describe the marginal distributions of just the interdepen-dent variables.
Thus, we see that the conditional-independence constraint allows usto express the joint distribution in terms of these smaller marginal distributions.The marginal distribution of FV1 and CV is the full joint distribution "collapsed"over FV2.
For example, the estimate for P(FV1 = O, CV = 0) is the sum of the relativef requenc ies  of (FV1 = O, CV = O, FV2 = 0), (FV1 = O, CV  = 0,FV2 = 1) , .
.
.
, (FV1  =0, CV = 0, FV2 = 19), i.e., the relative frequency of configurations for which FV1 -- 0and CV = 0, whatever the value of FV2.
Maximum likelihood estimates of the parame-ters of marginal distributions are more reliable than those of the full joint distribution,because, in a given sample of training data, the frequency of each combination ofvalues of the variables in a marginal distribution is always as large or larger than thefrequency of each combination of the values of the variables in the full distribution.There are many possible sets of noninteraction assumptions that could be maderegarding a set of variables.
The various possibilities can be formalized as probabilitymodels.
A probability model (more specifical134 its parametric form) expresses the rela-tionships among the variables of the model, and specifies a family of distributions--alldistributions in which those relationships hold.
For example, the model in which FV1is conditionally independent of FV2 given the value of CV is the family of all distribu-tions for vector X in which this constraint holds.
The differences among the membersof this family result from differences in the values of the parameters.A probabilistic model (a parametric form complete with parameter estimates)forms the basis of a probabilistic lassifier.
The classifier assigns to each ambigu-ous object he category or tag that has the highest probability of occurring, given theobserved values of the feature variables:P ( C V, j~ol, fv2, fv3 .
.
.
.
.
fVn )P(CW If'Ol,fOa,dO 3. .
.
.
.
f?dn) ~- p(fv l , fv2, fv 3. .
.
.
.
yon ) (3)Since the denominator is the same for all classes, the numerator, i.e., the joint distri-bution defined by the model, determines which class is assigned.3.
The Class of ModelsRecall that the parametric form of a model expresses a set of noninteraction assump-tions regarding the relationships among the variables.
Different model classes allowfor different ypes of noninteraction assumptions.197Computational Linguistics Volume 25, Number 2The class of log-linear models is the most widely used class of probability modelsfor analyzing discrete data.
It supports awide range of noninteraction assumptions andthe use of maximum likelihood parameter estimates (Bishop, Fienberg, and Holland1975).
Graphical models are the subset of log-linear models in which the only kindof noninteraction is conditional independence (Whittaker 1990).The interdependencies among the variables in a graphical model can be expressedgraphically, in a dependency graph.
A dependency graph is formed by mapping eachvariable in the model to a node in the graph and drawing an edge between the nodescorresponding to interdependent variables.
All variables that are not directly con-nected are conditionally independent given the values of the variables mapping tothe connecting nodes.
Therefore, the maximal sets of interdependent variables corre-spond exactly to the cliques of the graph (where a clique is a maximal fully connectedcomponent).As shown by Darroch, Lauritzen, and Speed (1980), each graphical model describesa Markov random field.
The fundamental property of a Markov random field is thatthe conditional probability of a variable given the values of the others is the sameas the conditional probability of that variable given only the values of the variablescorresponding to adjacent nodes.
Thus:P(Xi = xi I Xj = xj;j # i) = P(Xi -- xi I Xk = Xk, .
.
.
,  Xm = Xm)where Xk through Xm are the adjacent variables.
It is this property of conditionalindependence that was used to formulate quation (2) from (1).The framework described in this paper uses decomposable models, a subclass ofgraphical models (Whittaker 1990; Darroch, Lauritzen, and Speed 1980), because theyoffer many computational advantages while retaining agreat deal of expressive power.There are a number of different ways to define the class of decomposable models,one of which is the following: The class of decomposable models is composed of allgraphical models that have triangulated dependency graphs, i.e., all cycles of length _>four in the dependency graph contain achord.
A chord is an edge between onadjacentnodes in the cycle.Another definition of decomposable models is the following: They are those graph-ical models that express the joint distribution of a set of variables as the product ofmarginal distributions of those variables, where the new expression is a full factor-ization (Whittaker 1990) of the joint distribution.
A product of marginal distributionsis a full factorization of a joint distribution if the former is derived from the latter byfactorization steps such as that between equations (1) and (2), and "an independencestatement corresponding to every pair of non-adjacent vertices in the dependencygraph of X is applied exactly once to factorize the joint distribution into the productof marginal distributions" (Whittaker 1990, 393).Consider a set of five random variables, X = (A, B, C, D, E) (E, say, might be theclassification variable, and the others the feature variables).
We will consider the modelin which:CI1: A is conditionally independent of B given the values of C, D, and E;CI2: A is conditionally independent of C given the values of B, D, and E; andCI3: B is conditionally independent of C given the values of A, D, and E.This model is a decomposable model.
We will derive the full factorization of the198Bruce and Wiebe Decomposable Modeling in Natural Language ProcessingA AD C D Ca.
b.Figure 1Decomposable models.A ABD C D Cc.
d.joint distribution by applying an independence statement corresponding to each of(CI1)-(CI3) in turn.
2As in equation (1), the joint distribution of the variables can be expressed as:P(a, b, c, d, e) = P(a \] b, c, d, e)P(b \[ c, d, e)P(c \] d, e)P(d \] e)P(e) (4)The dependency graph of the model corresponding to this equation is shown inFigure l(a).
Applying (Ch) to (4), the following factorization can be performed, by thedefinition of conditional independence.P(a \] b, c, d, e) = P(a \[ c, d, e) (5)The resulting model is:P(a, b, c, d, e) = P(a \] c, d, e)P(b \] c, d, e)P(c \] d, e)P(d \] e)P(e)P(a, c, d, e) P(b, c, d, e) P(c, d, e) P(d, e) pP(c,d,e) P(c,d,e) P(d,e) ~(e)  (e)P(a, c, d, e)P(b, c, d, e)P(c, d, e) (6)The dependency graph of the model containing (CIt) is shown in Figure l(b).Factorization (5) can be understood in terms of this dependency graph by noting thatthe neighbors of A in this graph are {C, D, E} (and not {B, C, D, E}).Applying (CI2) to (6):P(a I c, d, e) = P(a I d, e) (7)The resulting model is:P(a, b, c, d, e) = P(a I d, e)P(b I c, d, e)P(c I d, e)P(d I e)P(e)P(a, d, e)P(b, c, d, e)P(d,e) (8)The dependency graph of the model containing (CI1)-(CI2) is shown in Figure 1(c).
Tosee that (7) can be performed, note that the neighbors of A in Figure 1(c) are {E, D}, so2 Such a factorization exists for any decomposable model, but the independence statements must  beapplied in an appropriate order to achieve the factorization; see Whittaker (1990).199Computational Linguistics Volume 25, Number 2that A is conditionally independent of {B, C} given the values of {E, D}, and it followsfrom a basic axiom of probability that A is conditionally independent of {C} given thevalues of {E, D}.Finally, applying (C/3) to (8):P(b I c, d, e) = P(b I d, e) (9)The final model incorporating all factorizations i :n(a, b, c, el, e) = P(a I c l, e)P(b \[ d, e)P(c l d, e)P(d l e)P(e )= P(a, d, e)P(b, d, e)P(c, d, e) (10)P(d,e)P(d,e)The dependency graph of the model containing all three conditional independenciesis shown in Figure l(d).Thus, a decomposable model expresses the joint distribution of a set of variablesas a product of the marginal distributions of the maximal sets of interdependent vari-ables (the cliques in the dependency graph) scaled by the marginal distributions ofthe variables common to two or more of these maximal sets.
The fact that this kind ofclosed-form expression of the joint distribution exists provides one of the key advan-tages in using decomposable models.
The parameters of the marginal distributions canbe estimated irectly from the counts in the data.
The joint distribution is expressed interms of these, as in equations (6), (8), and (10).
Thus, we can estimate the parametersfrom the data without the need for an iterative fitting procedure (as used in NLP max-imum entropy modeling \[Berger, Della Pietra, and Della Pietra 1996\]).
This propertyis unique to decomposable models (Pearl 1988; Whittaker 1990).4.
Mode l  Select ionWe showed above how conditional independence assumptions can be used to simplifythe expression of the joint distribution.
Given a particular set of variables, there areoften very many different conditional independence assumptions that could be made.The generation and testing of different sets of assumptions can be computationallyrealized as a search through a space of probability models, in our case decomposablemodels.
Removing an edge from a dependency graph of a decomposable model isequivalent to adding a conditional independency to the model.
Another way to viewthe derivation of equations (6), (8), and (10) is as the process of beginning with the fullyconnected model, in which all variables are interdependent, and successively removingedges, corresponding to adding conditional independencies (CI1)-(CI3).
This is howbackward search is done.
In forward search, we start with the fully disconnected model,in which all variables are independent, and successively add edges, corresponding toadding interdependencies.
The space of decomposable models is very large, so greedysearch is typically done.
In a backward search, at each step, all edges in the currentmodel are evaluated, and one is removed; in forward search, at each step, all edgesthat could be added are evaluated, and one is added.
(Note that decomposable modelsare not closed under the operations of adding and deleting edges, so a test must beperformed to assure that all the models considered are decomposable.
)As in decision tree induction, feature selection is also performed as a result ofmodel search (Pedersen, Bruce, and Wiebe 1997).
If a feature is not connected to theclassification variable in a model, then that feature cannot affect which class is assignedby a classifier based on that model.200Bruce and Wiebe Decomposable Modeling in Natural Language ProcessingThe goal of the search process is to find a model with the fewest interdependenciesthat fits the data well.
The fit of the model is how closely the counts observed in atraining sample correspond to those that would be expected if the model being testedwere the true population model.
This is measured using a goodness-of-fit s atistic.Read and Cressie (1988) have shown that most measures used to evaluate model fitare instances of the power divergence statistic, where different measures are generatedby changing a single parameter.
These include Pearson's X 2, the Kullback-Leiblerinformation divergence D, which is also known as cross entropy; and the log-likelihoodratio statistic, G 2.
The two most commonly used measures in NLP, D and G 2, aretrivially expressed in terms of each other.
In the general case, D is used to evaluatethe difference between any two density functions gy and fy for the same random vectorY.
When D is used to evaluate model fit, gy is the distribution of Y in the data sample,fy is the distribution of Y predicted by the model, and G 2 is 2N x D(gy;fy).In the model search described above, models are modified an edge at a time.
Inevaluating an edge, we are testing the model of conditional independence betweenthe two variables connected by that edge.
The information divergence applied in thiscase is the same as conditional mutual information, another widely used measure inNLP.Using decomposable models affords an important advantage in assessing modelfit: the test for conditional independence of two nodes as described above is simplified.Rather than assessing the conditional independence of the two nodes conditioned onall of the other variables, we need only consider the other nodes in the same clique inthe dependency graph.In general, a goodness-of-fit s atistic an be thought of as a cost function, where alower value represents a better model fit.
Model selection can be based directly on thevalue of a goodness-of-fit s atistic, or it can be based on a cost function that combinesa goodness-of-fit statistic with a penalty for model complexity, such as the Akaikeinformation criterion (AIC) (Akaike 1974) or the Bayesian information criterion (BIC)(Schwarz 1978).The final model selected can be based on a predefined cutoff value.
In the caseof measures uch as AIC and BIC, a cutoff on the value of the measure itself can bedefined.
In the case of statistics uch as G 2, the appropriate cutoff is a predeterminedthreshold efining statistical significance.
Alternatively, all the models generated ur-ing search can be considered, and the one with the highest accuracy on a held-outportion of the training data can be selected as the final model (Kayaalp, Pedersen, andBruce 1997; Wiebe, Bruce, and Duan 1997).
3The freely available software package CoCo performs forward and backwardsearch using all of the measures described above (Badsberg 1995).
4 Pedersen, Bruce,and Wiebe (1997) present he results of experiments covarying these measures andthe direction of search.
In addition to these methods, Buntine (1996) describes othersearch strategies and measures, such as minimum description length, that can be usedfor model selection.There are a number of other ways to utilize the results of a model search procedurethat are extensions to the basic framework.
In model switching (Kayaalp, Pedersen,and Bruce 1997) and the naive mix (Pedersen and Bruce 1997), more than one ofthe models generated uring search is used to perform classification.
In Boutilier et al3 One could also consider applying this kind of test to evaluate ach edge, replacing the goodness-of-fitor cost metric.
However, this would be more computationally expensive, and would not directlymeasure conditional independence.4 CoCo is available at http: / / web.math.auc.dk/jhb  CoCo /others.html201Computational Linguistics Volume 25, Number 2(1996), context-sensitive models are formulated.
These models include independenciesthat hold only in certain contexts, that is, they hold only given specific assignmentsof values to variables.5.
Diagnostic AnalysisAs seen above, the model selection framework provides many choices.
Because theapproach is a formal approach to probabilistic modeling, we can analyze the quality ofthe three determinants of classifier performance: the features, the form of the modeland the parameter stimates.
In the paragraphs below, we describe how to isolate thecontribution that each of them makes to classification error.
This analysis can provideinsight into which choices are most appropriate for a particular data set.
5Features.
For diagnostic purposes, it is revealing to train and test the model on the samedata.
6First, consider training and testing the fully connected model on the same data.Since the fully connected model contains no conditional independence assumptions,and the model parameters are not estimated on a separate training set, the modeldescribes the exact joint distribution of the data.
Because of this, classification errorscan only be due to a lack of discriminatory power of the features.
That is, there mustbe combinations of feature values that occur with more than one class.Form of the model.
Consider training and testing other models on the same data.
As forthe fully connected model, the parameter stimates are optimal for that data.
However,we have added approximations to the model in the form of conditional independenceassumptions.
Thus, for the same data and feature set, variations in the performance ofdifferent models are due only to the different conditional independence assumptionsmade in those models.Parameter estimates.
Consider a comparison i  which the features, test set, and modelform are fixed, but in one case, the parameters are estimated on a separate trainingset, and in the other case, the parameters are estimated from the test set, as above.Differences in the performance of two such models can only be due to the parameterestimates.As more conditional independence assumptions are made, the parameter esti-mates become more reliable, in the sense that they are based on the same or greaterfrequencies ( ee Section 2).
Even so, if important interdependencies areremoved fromthe form of the model, model performance may actually degrade.
Thus, by evaluatingthe contribution that each of the above factors makes to model performance, we canassess how well the model search procedure is balancing model expressiveness andthe reliability of the parameter estimates.6.
ShortcomingsWe have described a very general and expressive framework, but of course there aresome shortcomings.
The approach is a supervised learning approach, and therefore re-quires manually tagged training data.
In fact, to take full advantage ofhigh-complexitymodels, a large amount of data may be required.
However, by generating models of5 The material inthis section was originally published inBruce, Wiebe, and Pedersen (1996).6 Held-out portions of the training data can be used.202Bruce and Wiebe Decomposable Modeling in Natural Language Processingvarying complexity, the model search procedure can adjust the complexity of the finalmodel to the amount of data that is available.Another point of concern is the computational complexity of the search procedure.Because it is greedy, the search procedure itself is not inefficient: the number of edgesevaluated uring the search is polynomial in the number of variables.
However, themeasures used to evaluate dges during the search procedure are inefficient.
Section 4mentions a number of these measures, which can all be expressed as a function of G 2.The complexity of calculating G 2 is a function of the number of configurations of thevariables, which is exponential in the number of variables.
Therefore, the worst-casetime complexity of any search procedure that uses a function of G 2 is exponential inthe number of variables.
In practice, the method is feasible for a reasonable numberof variables (certainly on the order of 100 in the final model), and, once the model isdeveloped uring training, the process does not need to be repeated.7.
Relationships to Other Classes of ModelsIt is common in NLP to simply assume a particular model form rather than searchingfor one that is appropriate for the data.
Two kinds of statistical models widely used inNLP are the n-gram and naive Bayes models.
These models are decomposable models.In an n-gram model the variables are the class assigned to the current object and theclasses assigned to the previous N-  1 objects, and there are edges between all pairsof variables.
A naive Bayes model includes edges between the classification variableand each feature variable (and contains no other edges).
Because n-gram and naiveBayes models are decomposable, they are possible candidates during model selection.However, they would be selected only if they appear to be the most appropriatemodels for the particular data.In maximum entropy modeling as applied to NLP (Berger, Della Pietra, and DellaPietra 1996; Ratnaparkhi 1997), feature selection and model search are typically com-bined, but the procedure differs from that described here.
It is important o note thatdecomposable models are a subset of maximum entropy models.
Even so, no effort ismade to select for decomposable models (and take advantage of their benefits), or todemonstrate he need for a broader class of models.Bayesian etworks are extensively used in artificial intelligence.
They are popularbecause of their graphical representations and because there are probability propa-gation algorithms for computing the joint and conditional distributions of the vari-ables.
Decomposable models can be represented as Bayesian etworks.
In fact, in thewidely used probability propagation algorithm described by Lauritzen and Spiegel-halter (1988) and Pearl (1988), a Bayesian network is ultimately transformed into adecomposable model, to take advantage of the computational benefits of that class ofmodels (see the triangulation step described in Pearl \[1988\]).Although decision trees are not formal probability models, there are similaritiesbetween decision tree induction (Breiman et al 1984) and the model selection frame-work presented here.
Both search procedures perform feature selection and reduce theinterdependencies between features to avoid overfitting the data.
For a further dis-cussion of the relationships between graphical models and decision trees, see Buntineand Roy (1995).8.
Word Sense Disambiguation ResultsIn a recent collection of experiments, we applied the basic method to word sensedisambiguation of 34 words from the HECTOR corpus (Atkins 1993; Hanks 1996).
The203?
Computational Linguistics Volume 25, Number 2words were not chosen by the authors, but were randomly selected from a set of 38words included in the training set for the SENSEVAL evaluation project (Kilgarriff1998).
The data set for each word consisted of all sentences containing that wordin the corpus.
The results are presented in Figure 2.
Tenfold cross validation wasperformed for each word, for a total of 340 experiments.
On each fold, a forwardsearch with G 2 as the goodness-of-fit test was performed.
In addition, we ensuredthat naive Bayes was included as a competitor in each fold.
For each fold, evaluationon a single held-out portion of the training data was performed to choose the finalmodel.
The results of applying this model to the actual test set, averaged over folds,are shown in the column labeled Model Selection.
The results of applying naive Bayesexclusively (averaged over folds) are shown in the column labeled Naive Bayes.
Thecolumn labeled Best Model shows the highest results on the actual test set obtainedby any of the models generated during search (again, averaged over folds).
The sametypes of features were used in each model: the part-of-speech tags one place to theleft and right of the ambiguous word; the part-of-speech tags two places to the leftand right of the word; the part-of-speech tag of the word; and a collocation variablefor each sense of the word whose representation is per-class-binary as presented inWiebe, Bruce, and Duan (1997).
7Naive Bayes has been shown to be competitive with state-of-the-art classifiers, andhas proven remarkably successful on many AI and NLP applications ( ee, for exam-ple, Leacock, Towell, and Voorhees \[1993\]; Friedman, Geiger, and Goldszmidt \[1997\];Mooney \[1996\]; Langley, Iba, and Thompson \[1992\]).
As can be seen by comparingcolumns 5 and 6, the model selection procedure achieves an overall average accuracythat is 1.4 percentage points higher than exclusively using the naive Bayes classifier.Evaluating the results on a per-word basis more clearly shows the benefits of per-forming model selection in these experiments.
There are more words for which modelselection isbetter than there are words for which model selection is worse.
Further, weassessed the statistical significance of the differences in accuracy presented, in Figure 2between the two methods for the individual words, using a paired t-test (describedin Cohen \[1995\]) with 0.05 as the significance l vel.
For six of the words, the modelselection performance is significantly better than the performance ofexclusively usingnaive Bayes.
Further, the model selection procedure is not significantly worse thannaive Bayes for any of the words.In addition, on average, the set of words for which model selection is superior aremore difficult han the ones on which naive Bayes is superior: for the former set, theaverage number of senses is 10 and the average ntropy is 2.2; for the latter set, theaverage number of senses is 7 and the average ntropy is 1.7 (see columns 2 and 4 inFigure 2).
We can also see that, on average, there is less annotated ata available forthe words on which model selection does better, a total of 524 tagged instances (seecolumn 3 in Figure 2), than for those on which it does worse, a total of 645 taggedinstances.
8 This supports the idea that model selection tailors the complexity of themodel to the amount of data that is available.As shown in column 8, models are generated uring model search that providehigh accuracy.
In fact, the accuracy of the best model generated is consistently higherthan that of both naive Bayes and the final model actually selected uring the search.This illustrates that there are further potential gains to be exploited by investigatingalternative methods for selecting the best model for each fold.7 The variable for each sense Sis binary, corresponding to the absence orpresence ofany word in a setspecifically chosen for S. A word W is chosen for S if P(S I W) > 0.5.8 In 10-fold cross validation, 90% of the data is used for training on each fold.204Bruce and Wiebe Decomposable Model ing in Natural  Language ProcessingData Set Naive ModelWord Number (10% used as a test Entropy Bayes Selection MS - NB Majority Bestof Senses set on each fold) (NB) (MS) Classifier ModelTagged WordInstances Countsick 14 659 15066 2.969 56.8 65.1 +8.3 30.8 67.4storm 18 752 20806 2.895 63.4 71.6 +8.2 39.6 73.6drift 17 520 13484 2.889 56.0 63.3 +7.3 31.7 66.0curious 3 459 12950 0.833 83.0 87.8 +4.8 76.9 89.1beam 17 328 8824 2.950 61.1 65.8 +4.8 35.4 70.4drain 16 595 15033 3.253 57.3 60.9 +3.6 19.3 64.2brick 15 547 16530 2.289 68.1 71.7 +3.6 47.9 74.1raider 6 174 4481 2.216 79.6 82.8 +3.3 36.2 89.6dawn 8 494 14558 2.328 74.3 77.3 +3.0 47.0 81.4sugar 7 841 20580 1.786 82.5 84.9 +2.4 52.9 88.8creamy 3 100 2556 1.012 72.3 74.5 +2.3 68.0 82.7bake 11 349 10871 2.691 79.1 80.9 +1.8 23.8 84.3impress 4 637 16751 0.758 89.3 90.8 +1.6 85.6 92.3govern 7 585 18584 2.139 67.1 68.7 +1.5 43.4 74.0layer 10 605 14139 1.806 80.3 81.6 +1.4 44.6 85.9boil 14 664 13831 2.443 68.7 70.1 +1.4 42.9 75.8collective 9 550 14729 2.347 64.3 65.4 +1.1 39.5 73.2civilian 3 581 16043 1.504 88.2 88.4 +0.2 48.7 92.2provincial 4 331 11202 0.293 96.5 96.5 0.0 95.8 97.3overlook 5 435 11765 1.597 86.1 86.1 0.0 41.6 90.9impressive 1 709 19790 0200 100 100.
0.0 100.
100.bucket 10 176 4873 1.974 71.4 71.4 0.0 56.8 80.3complain 4 1109 29170 0.701 89.7 89.6 -0.1 87.5 90.5spite 4 577 17865 0.404 96.5 96.4 -0.2 94.3 97.6lemon 10 245 5549 2.398 71.2 70.6 -0.6 36.3 76.2literary 5 678 20510 1.661 66.5 65.7 -0.9 48.7 70.6connect 12 351 15029 2.283 56.8 55.8 -0.9 52.7 64.3attribute 5 360 10871 1.949 7610 75.0 -1.0 46.9 82.2confine 6 583 16743 1.392 83.9 82.8 -1.1 74.1 87.3comic 7 516 14300 2.033 74.9 73.8 -1.1 52.9 77.5cell 9 689 18683 2.099 74.6 73.5 -1.1 49.2 77.6cook 11 1523 48038 2.386 77.7 76.4 -1.3 46.3 80.3intensify 3 232 6872 1.316 72.8 71.2 -1.5 51.7 79.9expression 10 873 26279 2.137 64.0 61.1 -2.9 36.4 69.9average 18 .4711553.7115435.6  I 1.874 I 75.0 I 76.4 I +1.4 l 52.5 I 80.8 IFigure 2Compar ison of accuracy.9.
SummaryThis  paper  has  descr ibed  an  impor tant  c lass of  p robab i l i ty  mode ls  and  procedures  formode l  se lec t ion  that  have  not  been  w ide ly  used  in NLP.
The  procedures  complementthe  set  of  ava i lab le  methods  for  ba lanc ing  express iveness  and  feasibi l i ty.
The  f rame-work  is unders tandab le ,  power fu l ,  and  computat iona l ly  feas ib le.
Its e f fec t iveness  foran  NLP  prob lem is demonst ra ted  here  in  a la rge-sca le  word  sense  d i sambiguat ionexper iment .AcknowledgmentsThis research was supported in part  by theOffice of Naval Research under  grant numberN00014-95-1-0776.
The authors thank OUP foruse of the HECTOR data for the SENSEVALproject.
We gratefullyacknowledge the contributions to thiswork by Tom O'Hara, Ted Pedersen,Kenneth McKeever, and Gerald Rogers.The authors also wholeheartedly  thankthe anonymous  reviewers for theircomments  and suggestions.205Computational Linguistics Volume 25, Number 2ReferencesAkaike, Hirotugu.
1974.
A new look at thestatistical model identification.
IEEETransactions on Automatic Control,AC-19(6):716-723.Atkins, Sue.
1993.
Tools for computer-aidedlexicography: The Hector project.
InPapers in Computational Lexicography:COMPLEX "93, Budapest.Badsberg, Jens Henik.
1995.
An Environmentfor Graphical Models.
Ph.D. thesis, AalborgUniversity.Berger, Adam, Stephen Della Pietra, andVincent Della Pietra.
1996.
A maximumentropy approach to natural anguageprocessing.
Computational Linguistics,22(1):39-71.Bishop, Yvonne, Stephen Fienberg` and PaulHolland.
1975.
Discrete MultivariateAnalysis: Theory and Practice.
MIT Press,Cambridge, MA.Boutilier, Craig, Nir Friedman, MoisesGoldszmidt, and Daphne Koller.
1996.Context-specific independence inBayesian etworks.
In Proceedings oftheTwelfth Conference on Uncertainty in ArtificialIntelligence (UAI-96), Portland, OR.Breiman, Leo, Jerome Friedman, RichardOlshen, and Charles Stone.
1984.Classification and Regression Trees.Wadsworth & Brooks/Cole AdvancedBooks & Software, Monterey, CA.Bruce, Rebecca nd Janyce Wiebe.
1994.Word-sense disambiguation usingdecomposable models.
In Proceedings ofthe32nd Annual Meeting, pages 139-146.Association for ComputationalLinguistics.Bruce, Rebecca, Janyce Wiebe, and TedPedersen.
1996.
The measure of a model.In Proceedings ofthe Conference on EmpiricalMethods in Natural Language Processing(EMNLP-96), pages 101-112, Philadelphia,PA, May.
Association for ComputationalLinguistics SIGDAT.Buntine, Wray.
1996.
A guide to theliterature on learning probabilisticnetworks from data.
IEEE Transactions onKnowledge and Data Engineering, 8(3).Buntine, Wray and H. Scott Roy.
1995.Software for data analysis with graphicalmodels: Basic tools.
In Fifth InternationalArtificial Intelligence and Statistics Workshop,Ft.
Lauderdale, FL.Cohen, Paul.
1995.
Empirical Methods forArtificial Intelligence.
MIT Press,Cambridge, MA.Darroch, John, Steffen Lauritzen, and TerrySpeed.
1980.
Markov fields and log-linearinteraction models for contingency tables.Annals of Statistics, 8(3):522-539.Friedman, Nir, Dan Geiger, and MoisesGoldszmidt.
1997.
Bayesian etworkclassifiers.
Machine Learning, 29:131-163.Hanks, Patrick.
1996.
Contextualdependency and lexical sets.
InternationalJournal of Corpus Linguistics, 1(1):75-98.Kayaalp, Mehmet, Ted Pedersen, andRebecca Bruce.
1997.
Statistical decisionmaking method: A case study onprepositional phrase attachment.
InProceedings ofComputational NaturalLanguage Learning (CoNLL-97), Madrid,Spain.Kilgarriff, Adam.
1998.
SENSEVAL: Anexercise in evaluating word sensedisambiguation programs.
In Proceedingsof the First International Conference onLanguage Resources and Evaluation,pages 581-588, Granada, Spain, May.Langley, Pat, Wayne Iba, and KevinThompson.
1992.
An analysis of bayesianclassifiers.
In Proceedings ofthe lOthNational Conference on Artificial Intelligence(AAAI-92), pages 223-228.Lauritzen, Steffen and David Spiegelhalter.1988.
Local computations withprobabilities on graphical structures andtheir application to expert systems.Journal of the Royal Statistical Society B,50(2):157-224.Leacock, Claudia, Geoffrey Towell, andEllen Voorhees.
1993.
Corpus-basedstatistical sense resolution.
In Proceedingsof the ARPA Workshop on Human LanguageTechnology, Princeton, NJ.Mood, Alexander, Franklin Graybill, andDuane Boes.
1974.
Introduction to the Theoryof Statistics.
McGraw-Hill, New York, NY.Mooney, Ray.
1996.
Comparativeexperiments on disambiguating wordsenses: An illustration of the role of biasin machine learning.
In Proceedings oftheConference on Empirical Methods in NaturalLanguage Processing, pages 82-91.Pearl, Judea.
1988.
Probabilistic Reasoning InIntelligent Systems: Networks of PlausibleInference.
Morgan Kaufmann, San Mateo,CA.Pedersen, Ted and Rebecca Bruce.
1997.
Anew supervised learning algorithm forword sense disambiguation.
I  Proceedingsof the 14th National Conference on ArtificialIntelligence (AAAI-97), Providence, RI.Pedersen, Ted, Rebecca Bruce, and JanyceWiebe.
1997.
Sequential model selectionfor word sense disambiguation.
IProceedings ofthe Fifth Conference on Applied206Bruce and Wiebe Decomposable Modeling in Natural Language ProcessingNatural Language Processing (ANLP-97),Washington, DC.Ratnaparkhi, Adwait.
1997.
A linearobserved time statistical parser based onmaximum entropy.
In Proceedings of theSecond Conference on Empirical Methods inNatural Language Processing, Providence,RI.Read, Timothy and Noel Cressie.
1988.Goodness-of-Jit Statistics for DiscreteMultivariate Data.
Springer-Verlag Inc.,New York NY.Schwarz, Gideon.
1978.
Estimating thedimension of a model.
The Annals ofStatistics, 6(2):461-464.Whittaker, Joe.
1990.
Graphical Models InApplied Multivariate Statistics.
John Wiley& Sons, New York NY.Wiebe, Janyce, Rebecca Bruce, and LeiDuan.
1997.
Probabilistic eventcategorization.
I  Proceedings of the Second?
International Conference on Recent Advancesin NLP (RANLP-97), Tzigov Chark,Bulgaria.207
