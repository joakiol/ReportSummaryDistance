Proceedings of NAACL HLT 2009: Short Papers, pages 221?224,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsOn the Importance of Pivot Language Selectionfor Statistical Machine TranslationMichael Paul?
?, Hirofumi Yamamoto?
?, Eiichiro Sumita?
and Satoshi Nakamura??
NICT, Hikaridai 2-2-2, Keihanna Science City, 619-0288 Kyoto, Japan?
Kinki University School of Science and Engineering, Higashi-Osaka City, 577-8502, JapanMichael.Paul@nict.go.jpAbstractRecent research on multilingual statistical machinetranslation focuses on the usage of pivot languagesin order to overcome resource limitations for certainlanguage pairs.
Due to the richness of available lan-guage resources, English is in general the pivot lan-guage of choice.
In this paper, we investigate theappropriateness of languages other than English aspivot languages.
Experimental results using state-of-the-art statistical machine translation techniques totranslate between twelve languages revealed that thetranslation quality of 61 out of 110 language pairsimproved when a non-English pivot language waschosen.1 IntroductionThe translation quality of state-of-the-art, phrase-basedstatistical machine translation (SMT) approaches heavilydepends on the amount of bilingual language resourcesavailable to train the statistical models.
For frequentlyused language pairs like French-English or Chinese-English, large-sized text data sets are readily available.There exist several data collection initiatives like the Lin-guistic Data Consortium1, the European Language Re-source Association2, or the GSK3, amassing and distribut-ing large amounts of textual data.
However, for less fre-quently used language pairs, e.g., most of the Asian lan-guages, only a limited amount of bilingual resources areavailable, if at all.In order to overcome such language resource limita-tions, recent research on multilingual SMT focuses on theusage of pivot languages.
Instead of a direct translationbetween two languages where only a limited amount ofbilingual resources is available, the pivot translation ap-proach makes use of a third language that is more appro-priate due to the availability of more bilingual corporaand/or its relatedness towards either the source or the tar-get language.
Several pivot translation techniques likecascading, phrase-table combination, or pseudo corpusgeneration have already been proposed (cf.
Section 2).However, for most recent research efforts, English isthe pivot language of choice due to the richness of avail-1LDC: http://www.ldc.upenn.edu2ELRA: http://www.elra.info3GSK: http://www.gsk.or.jp/catalog.htmlable language resources.
For example, the Europarl cor-pus is exploited in (Utiyama and Isahara, 2007) for com-paring pivot translation approaches between French, Ger-man and Spanish via English.
Other research efforts triedto exploit the closeness between specific language pairsto generate high-quality translation hypotheses in the firststep to minimize the pivot detoriation effects, e.g., forCatalan-to-English translations via Spanish (Gispert andMarino, 2006).This paper investigates the appropriateness of lan-guages other than English as pivot languages to supportfuture research on machine translation between under-resourced language pairs.
Pivot translation experimentsusing state-of-the-art SMT techniques are carried out totranslate between twelve of the major world languagescovering Indo-European as well as Asian languages andthe effects of selecting a non-English language as thepivot language are discussed in Section 3.2 Pivot TranslationPivot translation is a translation from a source language(SRC) to a target language (TRG) through an intermedi-ate pivot (or bridging) language (PVT).
Within the SMTframework, the following coupling strategies have al-ready been investigated:1. cascading of two translation systems where the firstMT engine translates the source language input intothe pivot language and the second MT engine takesthe obtained pivot language output as its input andtranslates it into the target language.2.
pseudo corpus approach that (i) creates a ?noisy?SRC-TRG parallel corpus by translating the pivotlanguage parts of the SRC-PVT and PVT-TRG train-ing resources into the target language using an SMTengine trained on the PVT-TRG and PVT-SRC lan-guage resources, respectively, and (ii) directly trans-lates the source language input into the target lan-guage using a single SMT engine that is trained onthe obtained SRC-TRG language resources (Gispertand Marino, 2006).3. phrase-table composition in which the translationmodels of the SRC-PVT and PVT-TRG translation en-gines are combined to a new SRC-TRG phrase-tableby merging SRC-PVT and PVT-TRG phrase-table en-tries with identical pivot language phrases and mul-221tiplying posterior probabilities (Utiyama and Isa-hara, 2007; Wu and Wang, 2007).4. bridging at translation time where the coupling isintegrated into the SMT decoding process by model-ing the pivot text as a hidden variable and assumingindependence between source and target sentences(Bertoldi et al, 2008).3 Pivot Language SelectionThe effects of using different pivot languages are inves-tigated using the multilingual Basic Travel ExpressionsCorpus (BTEC), which is a collection of sentences thatbilingual travel experts consider useful for people goingto or coming from another country.
For the pivot transla-tion experiments, we selected twelve of the major worldlanguages covered by BTEC, favoring languages that areactively being researched on, i.e.,Chinese (zh), English(en), French (fr), German (de), Hindi (hi), Indonesian(id), Japanese (ja), Korean (ko), Malay (ms), Spanish(es), Thai (th), and Vietnamese (vi).
These languagesdiffer largely in word order (SVO, SOV), segmentationunit (phrase, word, none), and degree of inflection (high,moderate, light).
All data sets were case-sensitive withpunctuation marks preserved.However, in a real-world application, identical lan-guage resources covering three or more languages are notnecessarily to be expected.
In order to avoid a trilingualscenario for the pivot translation experiments describedin this paper, the 160k sentence-aligned BTEC corpuswas randomly split into two subsets of 80k sentenceseach, whereby the first set of sentence pairs was used totrain the source-to-pivot translation models (80ksp) andthe second subset of sentence pairs was used to train thepivot-to-target translation models (80kpt).
Table 1 sum-marizes the characteristics of the BTEC corpus data setsused for the training (train) of the SMT models, the tun-ing of model weights (dev), and the evaluation of transla-tion quality (eval).
Besides the number of sentences (sen)and the vocabulary (voc), the sentence length (len) is alsogiven, as the average number of words per sentence.For the training of the SMT models, standard wordalignment (Och and Ney, 2003) and language modeling(Stolcke, 2002) tools were used.
Minimum error ratetraining (MERT) was used to tune the decoder?s param-eters, and performed on the dev set using the techniqueproposed in (Och and Ney, 2003).
For the translation,an in-house multi-stack phrase-based decoder compara-ble to MOSES was used.
For the evaluation of trans-lation quality, we applied standard automatic evaluationmetrics, i.e., BLEU (Papineni et al, 2002) and METEOR(Banerjee and Lavie, 2005).
For the experimental resultsin this paper, the given scores are calculated as the aver-age of the respective BLEU and METEOR scores obtainedfor each system output and are listed as percent figures.Table 1: Language ResourcesBTEC train dev evalCorpus 80ksp 80kpt set set# of sen 80,000 80,000 1,000 1,000en voc 12,264 11,047 1,262 1,292len 7.8 7.2 7.1 7.2de voc 19,593 17,324 1,486 1,491len 7.4 6.8 6.7 6.8es voc 16,317 14,807 1,486 1,511len 7.6 7.1 7.0 7.2fr voc 15,319 13,663 1,455 1,466len 7.8 7.3 7.1 7.3hi voc 26,096 19,906 1,558 1,588len 8.1 7.6 7.4 7.5id voc 14,585 13,224 1,433 1,394len 7.0 6.5 6.3 6.4ja voc 13,868 12,517 1,407 1,408len 8.8 8.2 8.1 8.2ko voc 13,546 12,281 1,366 1,365len 8.3 7.8 7.7 7.8ms voc 15,113 13,616 1,459 1,438len 7.1 6.6 6.4 6.5th voc 6,103 5,603 1,081 1,053len 8.1 7.4 7.3 7.4vi voc 7,980 7,335 1,245 1,267len 9.4 8.7 8.5 8.6zh voc 11,084 10,159 1,312 1,301len 7.1 6.6 6.4 6.5In order to get an idea of how difficult the translationtask for the different languages is supposed to be, theautomatic evaluation scores for the direct translation ap-proach using the 80ksp language resources are summa-rized in Section 3.1.
The effects of the pivot language se-lection are discussed in Section 3.2 using the pivot trans-lation method of cascading two SMT systems.
In addition,the dependency between selecting the optimal pivot lan-guage for a given language pair and the amount of avail-able training resources are described in Section 3.3.3.1 Direct Translation ResultsThe automatic evaluation scores for all source and targetlanguage pair combinations of the direct translation ap-proach are given in Table 2.
For each target language, thehighest evaluation scores are marked in boldface and thelowest scores are marked in typewriter mode.The highest translation quality was achieved for theJapanese?Korean, Indonesian?Malay, and Span-ish?English translation tasks.
In addition, relativelyhigh evaluation scores were achieved for Japanese?Chinese and for translations from English into Ger-man, French, Hindi, Thai, and Vietnamese.
On the otherhand, the most difficult translation tasks were those hav-ing Korean or Chinese as the source language.3.2 Pivot Translation ResultsThe automatic evaluation scores for all pivot translationcombinations are summarized in Table 3 whereby foreach source-target language pair, the results of the exper-iments using (i) English (en) and (ii) the best performinglanguage (best) as the pivot language are listed.Comparing the results of the pivot translation experi-ments towards the direct translation results, we can see222Table 2: Translation Quality of Direct Translation ApproachSRC de en es fr hi id ja ko ms th vi zhde ?
74.24 56.22 49.78 63.25 69.31 54.09 50.88 69.33 66.83 67.17 51.59en 63.31 ?
64.30 56.10 66.43 73.46 55.64 54.15 73.66 70.57 72.64 53.18es 58.98 76.43 ?
53.53 63.60 70.46 55.37 51.41 70.46 67.69 69.15 52.03fr 55.45 72.24 57.25 ?
61.70 68.58 55.17 52.15 68.72 65.03 65.97 52.83hi 52.89 67.82 50.69 45.53 ?
68.65 52.94 50.93 68.14 66.44 66.88 51.31id 52.75 67.58 52.06 46.00 62.43 ?
55.52 52.90 88.69 67.20 68.01 52.77ja 35.43 51.65 37.82 32.70 46.94 52.90 ?
78.73 53.26 54.14 51.45 67.83ko 32.65 50.12 36.97 31.62 44.67 53.51 78.88 ?
51.75 52.35 51.34 63.19ms 53.16 68.17 53.06 45.30 63.36 91.12 54.88 52.18 ?
67.79 67.93 53.23th 49.66 64.53 50.16 42.70 59.40 66.58 53.82 50.81 65.76 ?
65.90 52.22vi 52.59 69.16 53.17 45.60 61.19 68.39 52.95 50.68 69.44 67.64 ?
51.29zh 34.18 49.79 37.13 31.16 44.33 52.72 65.64 62.23 52.46 51.88 51.09 ?Table 4: Pivot Language SelectionPVT usage (%) PVT usage (%)en 49 (44.5) ko 12 (10.9)ms 16 (14.5) zh 2 ( 1.8)id 16 (14.5) es 1 ( 0.9)ja 14 (12.7)that in general the pivot translation approach performsworse than the direct translation approach due to the ef-fect of error chaining, i.e., translation errors of the SRC-PVT engine cause a degradation in translation qualityof the PVT-TRG system output.
However, for languagepairs like Korean?German, Japanese?
Indonesianand German/Spanish?Korean, the best pivot transla-tion system outperforms the direct translation approachslightly.
This phenomenon is caused mainly by the highSRC-PVT (PVT-TRG) translation quality in combinationwith a better PVT-TRG (SRC-PVT) performance comparedto the direct SRC-TRG system output results.Besides the automatic evaluation scores, Table 3 listsalso the optimal pivot language for each source-targetlanguage pair in boldface.
The experimental results showthat English is indeed the best pivot language when trans-lating between languages, like German, Spanish, French,Hindi, Thai, and Vietnamese, whose direct translationperformance from/into English is high.
For these sixlanguages, all language pair combinations achieved thehighest scores using the English pivot translation ap-proach.
In contrast, English is the pivot language ofchoice for only 16.2% (11 out of 68) of the language pairswhen translating from/into Japanese, Korean, Indone-sian, or Malay.
In the remaining cases, the language withthe highest direct translation scores is in general selectedas the optimal pivot language, i.e., Japanese for Korean,Malay for Indonesian and vice versa.
For Chinese, thechoice of the optimal pivot language varies largely de-pending on the language direction.
However, the selec-tion of the optimal pivot language is not symmetric for34.5% of the language pairs, i.e., a different optimal pivotlanguage was obtained for the SRC-TRG compared to theTRG-SRC translation task.
This indicates that the choiceof the optimal pivot language depends on the relatednessof the SRC and PVT languages as well as the relatednessof the PVT and TRG languages.The distribution of the optimal pivot language selectionTable 5: Pivot Selection Shifts for 10k vs. 80k Training Data10k 80k pivot translation 10k 80k pivot translationPVT PVT language pair PVT PVT language pairko en ja-fr, ja-de, ja-vi ko ms ja-idko zh-fr, zh-es, zh-hi en vi-zhja ko-vi, zh-vi, zh-th es fr-zhms id-fr en ja fr-ko, hi-ko, vi-koja es ko-hi id zh-msja id ko-ms,th-zh ms ko id-jaen es-ms,hi-zh,hi-ja es fr-jaen de-ja,es-jafor all language pairs is given in Table 4.
The figuresshow that the English pivot approach still achieves thehighest scores for the majority of the examined languagepairs.
However, in 55.5% (61 out of 110) of the cases, anon-English pivot language, mainly Malay, Indonesian,Japanese, or Korean, is to be preferred.3.3 Training Data Size DependencyIn order to investigate the dependency between selectingthe optimal pivot language for a given language pair andthe amount of available training resources, we repeatedthe pivot translation experiments described in Section 3.2for statistical models trained on subsets of 10k sentencesrandomly extracted from the 80ksp and the 80kpt cor-pora, respectively.The results showed that 75.5% of the pivot languageselections are identical for small (10k) and large (80k)training data sets.
For the remaining 27 out of 110 trans-lation tasks, Table 5 lists how the optimal pivot languageselection changed.
In the case of small training data sets,the pivot language is closely related (in terms of highdirect translation quality) to the source language.
How-ever, for larger training data sets, the focus shifts towardsclosely related target languages.
Therefore, the higherthe translation quality of the pivot translation task is, themore dependend the selection of the optimal pivot lan-guage is on the system performance of the PVT-TRG task.4 ConclusionIn this paper, the effects of using non-English pivot lan-guages for translations between twelve major world lan-guages were compared to the standard English pivottranslation approach.
The experimental results revealedthat English was indeed more frequently (45.5% out of223Table 3: Translation Quality of Pivot Translation ApproachSRC PVT de es fr hi id ja ko ms th vi zhde en ?
54.69 47.01 60.48 66.42 52.53 51.10 66.47 65.06 66.08 50.46best ?
(en) 54.69 (en) 47.01 (en) 60.48 (ms) 66.92 (ko) 52.67 (en) 51.10 (en) 66.47 (en) 65.06 (en) 66.08 (en) 50.46es en 55.37 ?
48.75 60.24 68.10 52.68 51.80 67.54 65.59 66.99 51.08best (en) 55.37 ?
(en) 48.75 (en) 60.24 (ms) 69.29 (ko) 53.10 (en) 51.80 (id) 68.37 (en) 65.59 (en) 66.99 (en) 51.08fr en 52.03 53.88 ?
58.27 65.59 52.51 51.19 65.43 62.47 64.34 50.12best (en) 52.03 (en) 53.88 ?
(en) 58.27 (ms) 67.25 (ko) 53.06 (ja) 51.81 (en) 65.43 (en) 62.47 (en) 64.34 (ms) 50.35hi en 48.56 48.69 41.71 ?
63.01 50.21 48.96 63.13 62.08 62.48 48.12best (en) 48.56 (en) 48.69 (en) 41.71 ?
(ms) 65.43 (id) 51.09 (ja) 49.06 (id) 65.54 (en) 62.08 (en) 62.48 (id) 48.71id en 48.97 49.48 42.56 57.41 ?
51.30 50.19 72.94 62.40 64.60 49.45best (ms) 49.19 (ms) 50.16 (en) 42.56 (ms) 60.30 ?
(ko) 54.12 (ja) 51.54 (en) 72.94 (ms) 64.51 (ms) 65.51 (ms) 51.82ja en 33.43 36.61 31.20 44.27 52.31 ?
56.34 51.34 52.57 50.97 52.85best (en) 33.43 (ko) 36.88 (en) 31.20 (ko) 44.96 (ms) 53.13 ?
(zh) 60.99 (ko) 51.37 (ko) 52.65 (en) 50.97 (ko) 62.65ko en 31.52 34.50 29.01 43.23 50.70 54.43 ?
49.83 50.74 49.97 51.66best (ja) 33.23 (ja) 36.18 (ja) 31.20 (es) 44.24 (ja) 52.21 (zh) 60.10 ?
(id) 51.79 (ja) 51.98 (en) 49.97 (ja) 62.74ms en 49.64 49.71 42.39 57.85 73.25 51.01 49.52 ?
62.64 64.09 49.22best (id) 51.14 (id) 50.95 (id) 43.87 (id) 60.76 (en) 73.25 (id) 54.56 (ja) 50.94 ?
(id) 65.99 (id) 66.97 (id) 52.46th en 46.57 46.61 39.83 55.41 61.88 50.54 48.75 61.09 ?
61.50 47.75best (en) 46.57 (en) 46.61 (en) 39.83 (en) 55.41 (ms) 63.22 (ko) 51.37 (ja) 50.39 (id) 62.36 ?
(en) 61.50 (id) 48.72vi en 49.87 50.17 43.04 57.42 64.94 50.68 49.45 64.60 62.50 ?
48.12best (en) 49.87 (en) 50.17 (en) 43.04 (en) 57.42 (ms) 67.14 (ko) 51.86 (ja) 49.48 (id) 66.57 (en) 62.50 ?
(ms) 48.86zh en 32.26 35.29 28.35 43.20 50.11 53.27 52.53 49.20 51.54 49.92 ?best (en) 32.26 (en) 35.29 (en) 28.35 (en) 43.20 (ms) 52.18 (ko) 61.96 (ja) 60.64 (ja) 49.71 (en) 51.54 (en) 49.92 ?110 language pairs) selected as the best pivot languageover any other examined language.
However, its usage islimited to translations between Indo-European languagesand some Asian languages like Thai or Vietnamese.
Oth-erwise, the English pivot approach is largely outper-formed by using Asian languages as the pivot languages,especially Japanese, Malay, Indonesian, or Korean.The analysis of the results revealed that the selection ofthe optimal pivot language largely depends on the SRC-PVT and PVT-TRG translation performance, i.e., for smalltraining corpora, the relationship between source/pivotlanguages seems to be more important, whereas the se-lection criteria moves towards the relationship betweenpivot/target languages for larger amounts of training dataand thus for MT engines of higher translation quality.In order to explore the question of pivot selection fur-ther and arrive at firmer conclusions, future work willhave to investigate in detail what kind of features are im-portant in selecting a pivot language for a given languagepair.
Besides the translation quality of SMT engines, au-tomatic metrics to measure the relatedness of a languagepair should also be taken into account to find optimalpivot languages.
For example, (Birch et al, 2008) pro-poses features like amount of reordering, the morpholog-ical complexity of the target language, and historical re-latedness of the two languages as strong predictors forthe variability of SMT system performance.In addition, concerning the question of how the pivotlanguage selection criteria depends on the choice of thepivot translation method, future work will also have toinvestigate the effects of pivot language selection for theother pivot translation approaches described in Section 2.Based on these findings, we plan to determine the con-tribution of different language characteristics on the sys-tem performance automatically to obtain useful indica-tors that could be used to train statistical classificationmodels to predict the best pivot language for a new lan-guage pair and improve the usability of machine transla-tion between under-resourced languages further.AcknowledgmentThis work is partly supported by the Grant-in-Aid for Sci-entific Research (C) Number 19500137 and ?Construc-tion of speech translation foundation aiming to overcomethe barrier between Asian languages?, the Special Coor-dination Funds for Promoting Science and Technology ofthe Ministry of Education, Culture, Sports, Science andTechnology, Japan.ReferencesS.
Banerjee and A. Lavie.
2005.
METEOR: An AutomaticMetric for MT Evaluation.
In Proc.
of the ACL, pages 65?72, Ann Arbor, US.N.
Bertoldi, M. Barbaiani, M. Federico, and R. Cattoni.
2008.Phrase-Based SMT with Pivot Languages.
In Proc.
of theIWSLT, pages 143?149, Hawaii, US.A.
Birch, M. Osborne, and P. Koehn.
2008.
Predicting Successin MT.
In Proc.
of the EMNLP, pages 744?753, Hawaii, US.A.
Gispert and J. Marino.
2006.
Catalan-English SMT withoutParallel Corpus: Bridging through Spanish.
In Proc.
of 5thLREC, pages 65?68, Genoa, Italy.F.
Och and H. Ney.
2003.
A Systematic Comparison ofStatistical Alignment Models.
Computational Linguistics,29(1):19?51.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.
BLEU:a Method for Automatic Evaluation of Machine Translation.In Proc.
of the 40th ACL, pages 311?318, Philadelphia, US.A.
Stolcke.
2002.
SRILM an extensible language modelingtoolkit.
In Proc.
of ICSLP, pages 901?904, Denver.M.
Utiyama and H. Isahara.
2007.
A comparison of pivot meth-ods for phrase-based SMT.
In Proc.
of HLT, pages 484?491,New York, US.H.
Wu and H. Wang.
2007.
Pivot Language Approachfor Phrase-Based SMT.
In Proc.
of ACL, pages 856?863,Prague, Czech Republic.224
