Non-Dictionary-Based Thai Word Segmentation UsingDecision TreesThanaruk Theeramunkong1Information Technology ProgramSirindhorn International Institute of TechnologyThammasat University, Pathumthani 12121, Thailand+66-2-986-9103(-8) Ext.
2004ping@siit.tu.ac.thSasiporn UsanavasinInformation Technology ProgramSinrindhorn International Institute of TechnologyThammasat University, Pathumthani 12121, Thailand+66-2986-9103(-8) Ext.
2002sasiporn@kind.siit.tu.ac.thABSTRACTFor languages without word boundary delimiters, dictionaries areneeded for segmenting running texts.
This figure makessegmentation accuracy depend significantly on the quality of thedictionary used for analysis.
If the dictionary is not sufficientlygood, it will lead to a great number of unknown or unrecognizedwords.
These unrecognized words certainly reduce segmentationaccuracy.
To solve such problem, we propose a method based ondecision tree models.
Without use of a dictionary, specificinformation, called syntactic attribute, is applied to identify thestructure of Thai words.
C4.5 is used as a tool for this purpose.Using a Thai corpus, experiment results show that our methodoutperforms some well-known dictionary-dependent techniques,maximum and longest matching methods, in case of no dictionary.KeywordsDecision trees, Word segmentation without a dictionary1.
INTRODUCTIONWord segmentation is a crucial topic in analysis of languageswithout word boundary markers.
Many researchers have beentrying to develop and implement in order to gain higher accuracy.Unlike in English, word segmentation in Thai, as well as in manyother Asian languages, is more complex because the languagedoes not have any explicit word boundary delimiters, such as aspace, to separate between each word.
It is even more complicatedto precisely segment and identify the word boundary in Thailanguage because there are several levels and several roles in Thaicharacters that may lead to ambiguity in segmenting the words.
Inthe past, most researchers had implemented Thai wordsegmentation systems based on using a dictionary ([2], [3], [4],[6], [7]).
When using a dictionary, word segmentation has to copewith an unknown word problem.
Up to present, it is clear thatmost researches on Thai word segmentation with a dictionarysuffer from this problem and then introduce some particularprocess to handle such problem.
In our preliminary experiment,we extracted words from a pre-segmented corpus to form adictionary, randomly deleted some words from the dictionary andused the modified dictionary in segmentation process based twowell-known techniques; Maximum and Longest Matchingmethods.
The result is shown in Figure 1.
The percentages ofaccuracy with different percentages of unknown words areexplored.
We found out that in case of no unknown words, theaccuracy is around 97% in both maximum matching and longestmatching but the accuracy drops to 54% and 48% respectively, incase that 50% of words are unknown words.
As the percentage ofunknown words rises, the percentage of accuracy dropscontinuously.
This result reflects seriousness of unknown wordproblem in word segmentation.
1Accuracy (%)Unknownword (%) Maximum Matching Longest Matching0 97.24 97.035 95.92 95.6310 93.12 92.2315 89.99 87.9720 86.21 82.6025 78.40 74.4130 68.07 64.5235 69.23 62.2140 61.53 57.2145 57.33 54.8450 54.01 48.67Figure 1.
The accuracy of two dictionary-based systems vs.percentage of unknown wordsIn this paper, to take care of both known and unknown words, wepropose the implementation of a non-dictionary-based systemwith the knowledge based on the decision tree model ([5]).
Thismodel attempts to identify word boundaries of a Thai text.
To do1 National Electronics and Computer Technology Center(NECTEC), 539/2 Sriyudhya Rd., Rajthevi Bangkok 10400,Thailandthis, the specific information about the structure of Thai words isneeded.
We called such information in our method as syntacticattributes of Thai words.
As the learning stage, a training corpus isutilized to construct a decision tree based on C4.5 algorithm.
Inthe segmentation process, a Thai text is segmented according tothe rules produced by the obtained decision tree.
The rest showsthe proposed method, experimental results, discussion andconclusion.2.
PREVIOUS APPROACHES2.1 Longest MatchingMost of Thai early works in Thai word segmentation are based onlongest matching method ([4]).
The method scans an inputsentence from left to right, and select the longest match with adictionary entry at each point.
In case that the selected matchcannot lead the algorithm to find the rest of the words in thesentence, the algorithm will backtrack to find the next longest oneand continue finding the rest and so on.
It is obvious that thisalgorithm will fail to find the correct the segmentation in manycases because of its greedy characteristic.
For example:?????????
(go to see the queen) will be incorrectly segmented as: ??
(go)   ???
(carry)  ??
(deviate)   ?
?
(color), while the correct one that cannot befound by the algorithm is: ??
(go)  ??
(see)  ????
?
(Queen).2.2 Maximum MatchingThe maximum matching algorithm was proposed to solve theproblem of the longest matching algorithm describes above ([7]).This algorithm first generates all possible segmentations for asentence and then select the one that contain the fewest words,which can be done efficiently by using dynamic programmingtechnique.
Because the algorithm actually finds real maximummatching instead of using local greedy heuristics to guess, italways outperforms the longest matching method.
Nevertheless,when the alternatives have the same number of words, thealgorithm cannot determine the best candidate and some otherheuristics have to be applied.
The heuristic often used is again thegreedy one: to prefer the longest matching at each point.
For theexample, ???
(expose) ??
(wind) is preferred to ??
(eye) ???
(round).2.3 Feature-based ApproachA number of feature-based methods have been developed in([3]) for solving ambiguity in word segmentation.
In thisapproach, the system generates multiple possible segmentation fora string, which has segmentation ambiguity.
The problem is thathow to select the best segmentation from the set of candidates.
Atthis point, this research applies and compares two learningtechniques, called RIPPER and Winnow.
RIPPER algorithm is apropositional learning algorithm that constructs a set of ruleswhile Winnow algorithm is a weighted-majority learningalgorithm that learns a network, where each node in the network iscalled a specialist.
Each specialist looks at a particular value of anattribute of the target concept, and will vote for a value of thetarget concept based on its specialty; i.e., based on a value of theattribute it examines.
The global algorithm combines the votesfrom all specialists and makes decision.
This approach is adictionary-based approach.
It can acquire up to 91-99% of thenumber of correct segmented sentences to the total number ofsentences.2.4 Thai Character ChusterIn Thai language, some contiguous characters tend to be aninseparable unit, called Thai character cluster (TCC).
Unlike wordsegmentation that is a very difficult task, segmenting a text intoTCCs is easily realized by applying a set of rules.
The method tosegment a text into TCCs was proposed in ([8]).
This methodneeds no dictionary and can always correctly segment a text atevery word boundaries.3.
WORD SEGMENTATION WITHDECISION TREE MODELSIn this paper, we propose a word segmentation method that (1)uses a set of rules to combine contiguous characters to aninseparable unit (syllable-like unit) and (2) then applies a learneddecision tree to combine these contiguous units to words.
Thissection briefly shows the concept of TCC and the proposedmethod based on decision trees.3.1 Segmenting a Text into TCCsIn Thai language, some contiguous characters tend to be aninseparable unit, called Thai character cluster (TCC).
Unlike wordsegmentation that is a very difficult task, segmenting a text intoTCCs is easily recognized by applying a set of rules (in oursystem, 42 BNF rules).
The method to segment a text into TCCswas proposed in [8].
This method needs no dictionary and canalways correctly segment a text at every word boundaries.
As thefirst step of our word segmentation approach, a set of rules isapplied to group contiguous characters in a text together to formTCCs.
The accuracy of this process is 100% in the sense that thereis no possibility that these units are divided to two or more units,which are substrings in two or more different words.
This processcan be implemented without a dictionary, but uses a set of simplelinguistic rules based on the types of characters.
Figure 2 displaysthe types of Thai characters.
As an example rule, a front voweland its next consonant must exist in the same unit.
Figure 3 showsa fragment of a text segmented into TCCs by the proposed methodand its correct word segmentation.
Here, a character ?|?
indicatesa segmentation point.
The corpus where characters are groupedinto TCCs is called a TCC corpus.Types of ThaiCharacters MembersConsonant ?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?Upper vowel     ?
?
?
?
?
?
?Lower vowel  ?
?Front vowel ?
?
?
?
?Rear vowel ?
?
?
?
?
?
??
?Figure 2.
Types of Thai charactersTCCs ?
?|?| ????|??|?|??|??|??|?|?
?|?|CORRECT ??
?| ????|???|???????|???|??
?|Figure 3.
An example of TCCs vs. correct3.2 Learning a Decision Tree for WSegmentationTo learn a decision tree for this task, some attrifor classifying whether two contiguous TCCs areunit or not.
In this paper, eight types of attributeproposed to identify possible word boundariesanswers (or classes) in the decision tree for thitypes: combine and not combine.
Moreover, totwo contiguous TCCs should be combined orfront of the current two TCCs and the TCC behiinto account.
That is, there are four sets of attrtwo for current two TCCs and two for TCCsbehind the current TCCs.
Therefore, the total nuis 32 (that is, 8x4) and there is one dependent vwhether the current two contiguous TCCs shoulnot.Attribute Name Attribute DFront_vowel 0(don?t have), 1(don?t h2(may be followed by reFront_consonant 0(don?t have), 1(don?t lor oang), 2(lead with hoMiddle_vowel 0(don?t have), 1(upper v2(lower vowel)Middle_consonant 0(don?t have), 1 (have)Rear_vowel 0(don?t have), 1 (sara_a2 (sara_aa, sara_am)Rear_consonant0-9 are (don?t have), (ko(kod_tone), (kong_tone(kob_tone), (kon_tone),(wowaen_tone), (yoyakLength Length of the block(the number of characteSpace & Enter 0 (don?t have), 1 (have)Figure 4.
Types of TCC Attribu??|?|????|?
?|    ?|??|????
|?|????|??|?|?
?|??|?
|????|??|?|?
?|TCCFrom above we get th1!
0,1,0,0,2,0,2,0,0,2!
0,1,0,0,0,0,1,0,1,3!
1,1,1,0,0,5,4,0,0,???|?|??|??|?|?????
?|segmentationordbutes are definedcombined to ones (in Figure 4 arein the text.
Thes task are of twodecide whethernot, the TCC innd them are takenibutes concerned:in front of andmber of attributesariable indicatingd be combined oretailave rear vowel),ar vowel)ead with hohiphip or oang)owel),),k_tone),), (kom_tone),_tone), (others)rs)tesFigure 5 illustrates an example of the process to extract attributesfrom the TCC corpus and use them as a training corpus.
Theprocess is done by investigating the current TCCs in the bufferand recording their attribute values.
The dependent variable is setby comparing the combination of the second and the third blocksof characters in the buffer to the same string in the correct word-segmented corpus, the corpus that is segmented by human.
Theresult of this comparison will output whether the second and thethird blocks in the buffer should be merged to each other or not.This output is then kept as a training set with the dependentvariable, ?Combine (1)?
or ?NotCombine (0)?.
Repetitively,  thestart of the buffer is shifted by one block.
This process executesuntil the buffer reaches the end of the corpus.
The obtainedtraining set then is used as the input to the C4.5 application ([5])for learning a decision tree.The C4.5 program will examine and construct the decision treeusing the statistical values calculated from the events occurred.After the decision tree is created, the certainty factor is calculatedand assigned to each leaf as a final decision-making factor.
Thiscertainty factor is the number that identifies how certain theanswer at each terminal node is.
It is calculated according to thenumber of terminal class answers at each leaf of the tree.
Forexample, at leaf node i, if there are ten terminal class answers; sixof them are ?Combine?
and the rest are ?Not Combine?.
Theanswer at this node would be ?Combine?
with the certainty factorequals to 0.6 (6/10).
On the other hand, leaf node j has 5elements; two are ?Combine?
and three are ?Not Combine?, thenthe answer at this node would be ?Not Combine?
with thecertainty factor equals to 0.6 (3/5).
The general formula for thecertainty factor (CF) is shown as follow:CFi = Total number of the answer elements at leaf node iTotal number of all elements at leaf node iWe also calculate the recall, precision, and accuracy as definedbelow:Precision   =   number of  correct ?|?s  in the system answernumber of ?|?s in the system answerRecall        =   number of  correct ?|?s in the system answernumber of ?|?s in the correct answerAccuracy  =  number of correct segmented units in system answertotal number of segmented units in correct answer|??|?|??|?|???|?|??|?
?|?|     !1??|??|?|??|?|???|?|??|?
?|?|  !2??|??|?|??|?|???|?|??|?
?|?|  !3Buffer = 4 blockse following sets of attributes for the three points.1,0,0,0,0,1,0,1,1,1,0,0,5,4,0,0,1,0,0,2,0,2,0,01,1,0,0,5,4,0,0,1,0,0,2,0,2,0,0,1,1,0,0,0,2,0,01,0,0,2,0,2,0,0,1,1,0,0,0,2,0,0,1,0,0,0,0,1,0,1Not CombineCombineFigure 5.
Attributes taken from the corpus4.
EXPERIMENT RESULTSIn our experiments, the TCC corpus is divided into five sets, fourfor training and one for testing.
Based on this, five times crossvalidation are performed.
To test the accuracy, we trained thedecision trees and tested them several times for six different levelsof merging permission according to certainty factor(CF).
Eachlevel is the starting level of merging permission of the strings inthe second and the third blocks in the buffer.
Recall, precision,and accuracy where the certainty factor ranges between 50% and100% are shown in Figure 6.From the result, we observed that our msatisfactory in the percentage of accuracy anand recall compared to those numbers ofperformance.
The TCC corpus has 100% reprecision, and 44.93% accuracy.
Using the dfrom a Thai corpus, the precision improvesand the accuracy increases up to 85.51-87recall drops to 63.72-94.52%.
For a high CFdrops a little because there are few cases to mprecision and accuracy improve dominantly torespectively.
For a lower CF, say 50%, recabut precision and accuracy dramatically imp85.51% respectively.However, from 50 to 100% CF, at approximaccuracy had declined.
The reason to this declvery high level of merging permission, there aremoving ?|?
because of the %CF at those lethis permission level.
Therefore, there are moword segmentation, which lead to decconclusion, the appropriate level of mergingused in order to achieve high accuracy.
Frombest permission level is approximately equalthe recall equals to 96.13%, precision equals to 91.92% and theaccuracy equals to 87.41%.5.
DISCUSSION AND CONCLUSIONDue to the problem of the unknown words that most of theexisting Thai word segmentation systems have to cope with, thispaper has introduced an alternative method for avoiding suchproblem.
Our approach is based on using the decision tree as thedecision support model with no need of dictionary at all.
Theexperimental results clearly show that our method gives somehieving high accuracy when suitable andng permission factor is used.
In our experiments,60.00065.00070.00075.00080.00085.00090.00095.000100.00050% 60% 70% 80% 90% 100%Decision tree certainty factorPercentageRecall (%) Precision (%) Accuracy (%)Figure 4.
Recall, precision, and accuracyethod presented thepromises on acappropriate mergid both in precisionthe original TCCcall but has 52.12%ecision tree learnedup to 94.11-99.85%.41%.
However, the, say 100 %, recallerge two TCCs but63.72% and 62.97,ll drops dominantlyrove to 94.52% andately 80% CF, theination is that with are a few chances foraves are lower thanre chances for wrongrease accuracy.
Inpermission has to beour experiment, theto 70%, which givesthe best level of permission that leads to the highest accuracy isapproximately equals to 70%, which gives the accuracy equal to87.41%, as shown in Figure 6.The dictionary-based method so-called the feature-based systemwith context independence gives the highest accuracy equals to99.74% and with context dependence, which has the highestaccuracy equals to 95.33% ([3]).
In [1], the Japanese wordsegmentation is explored based on decision tree.
However, itfocuses on the part-of-speech for word segmentation.
Another twowell known dictionary-based methods, Maximum and LongestMatching methods, have the accuracy equal to 86.21% and82.60% respectively when there are 20% of unknown words,which are lower than our system accuracy, and their accuracydrops as percentage of unknown words increases.
By comparingthese percentages of accuracy, we can conclude that our methodcan achieve satisfied accuracy even without dictionary.
Therefore,our method is useful for solving an unknown word problem and itwill be even more useful to apply our method to the dictionary-based system in order to improve the system accuracy.
Inaddition, our results seem to suggest that our method is efficientnot only for Thai texts but also for any language when suitableand appropriate syntactic attributes are used.Our plan for further research is to apply our method to thedictionary based system in order to take care of the unknownword parts.
This would improve the accuracy of the systemregardless of the level of the unknown words found in the context.6.
ACKNOWLEDGEMENTThis work has been supported by National Electronics andComputer Technology Center (NECTEC) under the projectnumber NT-B-06-4F-13-311.7.
REFERENCES[1] Kasioka, H., Eubank, S. G., and Black, E. W., Decision-TreeMorphological Analysis without a Dictionary for Japanese,Proceedings of the Natural Language Processing Pacific RimSymposium, pp.
541-544, Phuket, Thailand, 1997.
[2] Kawtrakul, A., Thumkanon, C., Poovorawan, Y., Varasrai, P.and Suktarachan, M., Automatic Thai Unknown WordRecognition, Proceedings of the Natural LanguageProcessing Pacific Rim Symposium, pp.
341-348, Phuket,Thailand, 1997.
[3] Mekanavin, S., Charenpornsawat, P., and Kijsirikul, B.,Feature-based Thai Words Segmentation, Proceedings of theNatural Language Processing Pacific Rim Symposium, pp.41-48, Phuket, Thailand, 1997.
[4] Poowarawan, Y., Dictionary-based Thai Syllable Separation,Proceedings of the Ninth Electronics EngineeringConference, 1986.
[5] Quinlan, J.R., Induction of Decision Trees, MachineLearning, 1, pp.
81-106, 1986.
[6] Rarunrom, S. Dictionary-based Thai Word Separation,Thesis, Thailand.
[7] Sornlertlamvanich, V., Word Segmentation for Thai in aMachine Translation system (in Thai), Papers on NaturalLanguage processing, NECTEC, Thailand, 1995.
[8] Theeramunkong, T., Sornlertlamvanich, V., Tanhermhong,T., Chinnan, W., Character-Cluster Based Thai InformationRetrieval, Proceedings of the Fifth International Workshopon Information Retrieval with Asian Languages, September30 - October 20, 2000, Hong Kong, pp.75-80.
