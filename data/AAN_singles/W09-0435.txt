Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 206?214,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsA POS-Based Model for Long-Range Reorderings in SMTJan Niehues and Muntsin KolssUniversita?t KarlsruheKarlsruhe, Germany{jniehues,kolss}@ira.uka.deAbstractIn this paper we describe a new approachto model long-range word reorderings instatistical machine translation (SMT).
Un-til now, most SMT approaches are onlyable to model local reorderings.
But eventhe word order of related languages likeGerman and English can be very different.In recent years approaches that reorder thesource sentence in a preprocessing stepto better match target sentences accordingto POS(Part-of-Speech)-based rules havebeen applied successfully.
We enhancethis approach to model long-range reorder-ings by introducing discontinuous rules.We tested this new approach on a German-English translation task and could signifi-cantly improve the translation quality, byup to 0.8 BLEU points, compared to a sys-tem which already uses continuous POS-based rules to model short-range reorder-ings.1 IntroductionStatistical machine translation (SMT) is currentlythe most promising approach to machine transla-tion of large vocabulary tasks.
The approach wasfirst presented by Brown et al (1993) and has sincebeen used in many translation systems (Wang andWaibel, 1998), (Och and Ney, 2000), (Yamadaand Knight, 2000), (Vogel et al, 2003).
State-of-the-art SMT systems often use translation mod-els based on phrases to describe translation corre-spondences and word reordering between two lan-guages.
The reordering of words is one of the maindifficulties in machine translation.Phrase-based translation models by themselveshave only limited capability to model differentword orders in the source and target language, bycapturing local reorderings within phrase pairs.
Inaddition, the decoder can reorder phrases, subjectto constraints such as confining reorderings to arelatively small window.
In combination with adistance-based distortion model, some short-rangereorderings can be handled.
But for many lan-guage pairs this is not sufficient, and several au-thors have proposed additional reordering mod-els as described in Section 2.
In this work wepresent a new method that explicitly handles long-range word reorderings by applying discontinu-ous, POS-based reordering rules.The paper is structured as follows: In the nextsection we present related work that was carriedout in this area.
Afterwards, we describe the prob-lem of long-range reordering.
In Section 4 theexisting framework for reordering will be intro-duced.
Section 5 describes the extraction of rulesmodeling long-range reorderings, and in the fol-lowing section the integration into the frameworkwill be explained.
Finally, the model will be eval-uated in Section 7, and a conclusion is given inSection 8.2 Related WorkSeveral approaches have been proposed to ad-dress the problem of word reordering in SMT.
Wu(1996) and Berger et al (1996), for example, re-strict the possible reorderings either during decod-ing time or during the alignment, but do not useany additional linguistic knowledge.
A compari-son of both methods can be found in Zens and Ney(2003).Furthermore, techniques to use additional lin-guistic knowledge to improve the word order havebeen developed.
Shen et al (2004) and Och et al(2004) presented approaches to re-rank the outputof the decoder using syntactic information.
Fur-thermore, lexical block-oriented reordering mod-els have been developed in Tillmann and Zhang(2005) and Koehn et al (2005).
These models de-cide during decoding time for a given phrase, if206the next phrase should be aligned to the left or tothe right.In recent years several approaches using re-ordering rules on the source side have been appliedsuccessfully in different systems.
These rules canbe used in rescoring as in Chen et al (2006) or canbe used in a preprocessing step.
The aim of thisstep is to monotonize the source and target sen-tence.
In Collins et al (2005) and Popovic?
andNey (2006) hand-made rules were used to reorderthe source side depending on information from asyntax tree or based on POS information.
Theserules had to be created manually, but only a fewrules were needed and they were able to modellong-range reorderings.
Consequently, for everylanguage pair these rules have to be created anew.In contrast, other authors propose data-drivenmethods.
In Costa-jussa` and Fonollosa (2006)the source sentence is first translated into an aux-iliary sentence, whose word order is similar tothe one of the target sentences.
Thereby statisti-cal word classes were used.
Rottmann and Vogel(2007),Zhang et al (2007) and Crego and Habash(2008) used rules to reorder the source side andstore different possible reorderings in a word lat-tice.
They use POS tags and in the latter two casesalso chunk tags to generalize the rules.
The dif-ferent reorderings are assigned weights dependingon their relative frequencies (Rottmann and Vo-gel, 2007) or depending on a source side languagemodel (Zhang et al, 2007).In the presented work we will use discontinuousrules in addition to the rules used in Rottmann andVogel (2007).
This enables us to model long-rangereorderings although we only need POS informa-tion and no chunk tags.3 Long-Range ReorderingsOne of the main problems when translating fromGerman to English is the different word orderin both languages.
Although both languages areclosely related, the word order is very differentin some cases.
Especially when translating theverb long-range reorderings have to be performed,since the position of the German verb is differ-ent from the one in the English sentence in manycases.The finite verbs in the English language are al-ways located at the second position, in the mainclauses as well as in subordinate clauses.
In Ger-man this is only true for the main clause.
In con-trast to that, in German subordinate clauses theverb (glauben) is at the final position as shown inExample 1.Example 1: ..., die an den Markt und an dieGleichbehandlung aller glauben.... who believe in markets and equal treatmentfor all.Example 2: Das wird mit derart unter-schiedlichen Mitgliedern unmo?glich sein .That will be impossible with such disparatemembers.A second difference in both languages is the po-sition of the infinitive verb (sein/be) as shown inExample 2.
In contrast to the English language,where it directly follows the finite verb, it is at thefinal position of the sentence in the German lan-guage.The two examples show that in order to be ableto handle the reorderings between German andEnglish, the model has to allow some words tobe shifted across the whole sentence.
If this isnot handled correctly, phrase-based systems some-times generate translations that omit words, as willbe shown in Section 7.
This is especially problem-atic in the German-English case because the verbmay be omitted, which carries the most importantinformation of the sentence.4 POS-Based ReorderingWe will first briefly introduce the framework pre-sented in Rottmann and Vogel (2007) since we ex-tended it to also use discontinuous rules.In this framework, the first step is to extract re-ordering rules.
Therefore, an aligned parallel cor-pus and the POS tags of the source side are needed.For every sequence of source words where the tar-get words are in a different order, a rule is ex-tracted that describes how the source side has to bereordered to match the target side.
A rule may forexample look like this: VVIMP VMFIN PPER ?PPER VMFIN VVIMP.
The framework can handlerules that only depend on POS tags as well as rulesthat depend on POS tags and words.
We will referto these rules as short-range reordering rules.The next step is to calculate the relative frequen-cies which are used as a score in the word lattice.The relative frequencies are calculated as the num-ber of times the source side is reordered this waydivided by the number of times the source side oc-curred in the corpus.In a preprocessing step to the actual decoding,207different reorderings of the source sentences areencoded in a word lattice.
For all reordering rulesthat can be applied to the sentence, the resultingedge is added to the lattice if the score is betterthan a given threshold.
If a reordering is generatedby different rules, only the path of the reorderingwith the highest score is added to the lattice.
Then,decoding is performed on the resulting word lat-tice.5 Rule ExtractionTo be able to handle long-range reorderings, weextract discontinuous reordering rules in additionto the continuous ones.
The extracted rules shouldlook, for example, like this: VAFIN * VVPP ?VAFIN VVPP *, where the placeholder ?*?
repre-sents one or more arbitrary POS tags.Compared to the continuous, short-range re-ordering rules described in the previous section,extracting such discontinuous rules presents an ad-ditional difficulty.
Not only do we need to findreorderings and extract the corresponding rules,but we also have to decide which parts of the ruleshould be replaced by the placeholder.
Since itis not always clear what is the best part to be re-placed, we extract four different types of discon-tinuous rules.
Then we decide during decodingwhich type of rules to use.In a first step the reordering rule has to be found.Since this is done in a different way than for thecontinuous one, we will first describe it in detail.Like the continuous rules, the discontinuous onesare extracted from a word aligned corpus, whosesource side is annotated with POS tags.
Then thesource side is scanned for reorderings.
This isdone by comparing the alignment points ai andai+1 of two consecutive words.
We found a re-ordering if the target words aligned to fi and fi+1are in a different order than the source words.
Inour case the target word eai+1 has to precede thetarget word eai .
More formally said, we check thefollowing condition:ai > ai+1 (1)In Figure 1 an example with an automaticallygenerated alignment is given.
There, for example,a reordering can be found at the position of theword ?Kenntnis?.Since we only check the links of consecutivewords, we may miss some reorderings where thereis an unaligned word between the words with aFigure 1: Example training sentence used to ex-tract reordering rulescrossing link.
However, in this case it is not clearwhere to place the unaligned word, so we do notextract rules from such a reordering.So now we have found a reordering and alsothe border between the left and right part of thereordering.
To be able to extract a rule for thisreordering we need to find the beginning of theleft and the end of the right part.
This is doneby searching for the last word before and the firstword after the reordering.
In the given example,the left part is ?ihre Bereitschaft zur Kenntnis?
andthe right part would be ?genommen?.
As shown inthe figure, the words of the first part have to bealigned to target words that follow the target wordaligned to the first word of the right part.
Oth-erwise, they would not be part of the reordering.Consequently, to find the first word that is not partof the reordering, we search for the first word be-fore the word fi+1 that is aligned to the word eai+1or to a target word before this word.
More for-mally, we search for the word fj that satisfies thefollowing condition:j = argmaxl<i al ?
ai+1 (2)The first word after the reordering is found in thesame way.
Formally, we search for the word fksatisfying the condition:k = argmaxl>i+1 al ?
ai (3)In our example, we now can extract the fol-lowing reordering rule: ihre Bereitschaft zurKenntnis genommen ?
genommen ihre Bere-itschaft zur Kenntnis.
In general, we willextract the rule: fj+1 .
.
.
fifi+1 .
.
.
fk?1 ?fi+1 .
.
.
fk?1fj+1 .
.
.
fi208An additional problem are unaligned words af-ter fj and before fk.
For these words it is not clearif they are part of the reordering or not.
There-fore, we will include or exclude them dependingon the type of rule we extract.
To be able to writethe rules in a easier way let fj?
be the first wordfollowing fj that is aligned and fk?
the last wordbefore fk.After extracting the reordering rule, we need toreplace some parts of the rule by a placeholder toobtain more general rules.
As described before, itis not directly clear which part of the rule shouldbe replaced and therefore, we extract four differenttypes of rules.In the reordering, there is always a left part, inour example ihre Bereitschaft zur Kenntnis, anda right part (genommen).
So we can either re-place the left or the right part of the reordering bya placeholder.
One could argue that always thelonger sequence should be replaced, since that ismore intuitive, but to lose no information we justextract both types of rules.
Later we will see thatdepending on the language pair, one or the othertype will generalize better.
In the evaluation partthe different types will be referred to as Left andRight rules.Furthermore, not the whole part has to be re-placed.
It can be argued that the first or last wordof the part is important to characterize the reorder-ing and should therefore not be replaced.
For eachof the types described before, we extract two dif-ferent sub-types of rules, which leads altogether tofour different types of rules.Let us first have a look at the types where wereplace the left part.
If we replace the whole part,in the example we would get the following rule: *VVPP ?
VVPP *.
This would lead to problemsduring rule application.
Since the rule begins witha placeholder, it is not clear where the matchingshould start.
Therefore, we also include the lastword before the reordering into the rule and cannow extract the following rule from the sentence:VAFIN * VVPP ?
VAFIN VVPP *.
In general, weextract the following rule to which we will refer asLeft All:fj ?
fi+1 .
.
.
fk?
?
fjfi+1 .
.
.
fk?
?As mentioned in the beginning, we extracted asecond sub-type of rule.
This time, the first wordof the left part is not replaced.
The reason can beseen by looking at the reordered sequence.
There,the second part of the reordering is moved betweenthe last word before the reordering (fj) and thefirst word of the first part (fj+1).
In our examplethis results in the following rule: VAFIN PPOSAT* VVPP ?
VAFIN VVPP PPOSAT * and in gen-eral, we extract the rule (Left Part):fjfj+1 ?
fi+1 .
.
.
fk?
?
fjfi+1 .
.
.
fk?fj+1 ?If we replace the right part by a star, we sim-ilarly get the following rule (Right All): PPOSATNN APPART NN * ?
* PPOSAT NN APPART NN.The other rule (Right Part) can not be extractedfrom this example, since the right part has lengthone.
But in general we get the two rules:fj?
.
.
.
fi ?
fk?1fk ?
?fk?1fj+1 .
.
.
fifkfj?
.
.
.
fi ?
fk ?
?fj?
.
.
.
fifkHere we already see that the rules where thefirst part is replaced result in typical reordering be-tween the German and English language.
The sec-ond part of the verb is at the end of the sentencein German, but in an English sentence it directlyfollows the first part.6 Rule ApplicationDuring the training of the system all reorderingrules are extracted from the parallel corpus in theway described in the last section.
The rules areonly used if they occur more often than a giventhreshold value.
In the experiments a threshold of5 is used.The rules are scored in the same way as the con-tinuous rules were.
The relative frequencies arecalculated as the number of times the rule was ex-tracted divided by the number of times both partsoccur in one sentence.Then, in the preprocessing step, continuousrules as described in Section 4 and discontinuousrules are applied to the source sentence.
As in theframework presented before, the rules are appliedonly to the source sentence and not to the lattice.Thus the rules cannot be applied recursively.
Forthe discontinuous rules the ?*?
could match anysequence of POS tags, but it has to consist of atleast one tag.
If more than one rule can be ap-plied to a sequence of POS tags and they generatedifferent output, all edges are added to the lattice.If they generate the same sequence, only the rulewith the highest probability is applied.209In initial experiments we observed that somerules can be applied very often to a sentence andtherefore the lattice gets quite big.
Therefore, wefirst check how often a rule can be applied to asentence.
If this exceeds a given threshold, we donot use this rule for this sentence.
In these cases,the rule will most likely not find a good reorder-ing, but randomly shuffle the words.
In the experi-ments we use 5 as threshold, since this reduces thelattices to a decent size.These restrictions limit the number of reorder-ings that have to be tested during decoding.
Butif all reorderings that can be generated by the re-maining rules would be inserted into the lattice,the size of the lattice would still be too big tobe able to do efficient decoding.
Therefore, onlyrules with a probability greater than a given thresh-old are used to reorder the source sentence.
Sincethe probabilities of the long-range reorderings arequite small compared to those of the short-rangereorderings, we used two different thresholds.7 EvaluationWe performed the experiments on the translationtask of the WMT?08 evaluation.
Most of the ex-periments were done on the German-English task,but in the end also some results on German-Frenchand English-German are shown.
The systemswere trained on the European Parliament Proceed-ings (EPPS) and the News Commentary corpus.For the German-French task we used the inter-section of the parallel corpora from the German-English and English-French task.
The data waspreprocessed and we applied compound splittingto the German corpus for the tasks translating fromGerman.
Afterwards, the word alignment wasgenerated with the GIZA++-Toolkit and the align-ments of the two directions were combined us-ing the grow-diag-final-and heuristic.
Then thephrase tables were created where we performedadditional smoothing of the relative frequencies(Foster et al, 2006).
Furthermore, the phrase ta-ble applied in the news task was adapted to thisdomain.
In addition, a 4-gram language modelwas trained on both corpora.
The rules were ex-tracted using the POS tags generated by the Tree-Tagger (Schmid, 1994).
In the end a beam-searchdecoder as described in Vogel (2003) was usedto optimize the weights using the MER-trainingon the development sets provided for the differenttask by the workshop.
The systems were testedTable 1: Evaluation of different Lattice sizesgenerated by changing the short-range threshold?short and long-range threshold ?long?short ?long #Edges Dev Test0.2 1 112K 24.57 27.250.1 1 203K 24.71 27.480.2 0.2 113K 24.70 27.510.2 0.1 121K 24.97 27.560.2 0.05 152K 25.28 27.800.1 0.1 212K 24.97 27.490.1 0.05 243K 25.12 27.81on the test2007 set for the EPPS task and on thenc-test2007 testset for the news task.
For test settranslations the statistical significance of the re-sults was tested using the bootstrap technique asdescribed in Zhang and Vogel (2004).7.1 Lattice CreationIn a first group of experiments we analyzed the in-fluence of the two thresholds that determine theminimal probability of a rule that is used to insertthe reordering into the lattice.
The experimentswere performed on the news task and used only thelong-range rules generated by the Part All rules.The results are shown in Table 1 where ?shortis the threshold for the short-range reorderingsand ?long for the long-range reorderings.
Con-sequently, only paths were added that are gener-ated by a short-range reordering rule that has aprobability greater than ?short or paths generatedby a long-range reordering rule with a minimumprobability of ?long.
We used different thresholdsfor both groups of rules since the probabilities oflong-range reorderings are in general lower.The first two systems use no long-range reorder-ings.
Adding the long-range reorderings does im-prove the translation quality and it makes sense toadd even all edges generated by rules with a prob-ability of at least 0.05.
Using this system, lessshort-range reorderings are needed.
The systemusing the thresholds of 0.2 and 0.05 has a perfor-mance nearly as good as the one using the thresh-olds 0.1 and 0.05, but it needs fewer edges.
Iflong-range reordering is applied, fewer edges areneeded than in the case of using only short-rangereordering even though the translation quality isbetter.
Therefore, we used the thresholds 0.2 and0.05 in the following experiments.210Figure 2: Most common long-range reordering rules of type Left PartNN ADV * VAFIN ?
NN VAFIN ADV *VAFIN ART * VVPP ?
VAFIN VVPP ART *?
ADV * PPER ?
?
PPER ADV *$, ART * VVINF PTKZU ?
$, VVINF PTKZU ART *PRELS ART * VVFIN ?
PRELS VVFIN ART *Figure 3: Most common long-range reordering rules of type Left AllPRELS * VAFIN ?
PRELS VAFIN *PRELS * VAFIN VVPP ?
PRELS VAFIN VVPP *PPER * VMFIN ?
PPER VMFIN *PRELS * VMFIN ?
PRELS VMFIN *VMFIN * VAINF ?
VMFIN VAINF *Table 2: Number of long-range reordering rules ofdifferent types used to create the latticesType Left RightPart 8079 1127All 2470 509Both 9223 14057.2 Rule UsageWe analyzed which long-range reordering ruleswere used to build the lattices.
First, we comparedthe usage of the different types of rules.
There-fore, we counted the number of rules that were ap-plied to the development set of 2000 sentences ifthe thresholds 0.2 and 0.05 were used.
The result-ing numbers are shown in Table 2.As it can be seen, the Left rules are more of-ten used than the Right ones.
This is what weexpected, since when translating from German toEnglish, the most important rules move the verbto the left.
And these rules should be more gen-eral and therefore have a higher probability thanthe rules that move the words preceding the verbto the end of the sentence.Next we analyzed which rules of the Left Partones are used most frequently.
The five most fre-quent rules are shown in Figure 2.
The first, fourthand fifth rule moves the verb more to the front,as is often needed in English subordinate clauses.The second one moves both parts of the verb to-gether.The third most frequent rule moves personal pro-nouns to the front.
In the English language theTable 3: Translation results for the German-English task using different rule types (BLEU)Type EPPS NEWSDev Test Dev TestLeft Part 26.99 29.16 25.12 27.88Right Part 26.69 28.73 24.76 27.28Right/Left Part 26.99 28.96 25.06 27.69Left All 26.77 28.76 24.37 26.56Left Part/All 26.99 29.32 25.38 27.86All 27.02 29.14 25.20 27.63subject has to be always at the front.
In contrast,in German the word order is not that strict and thesubject can appear later.We have done the same for the Left All rules.The rules are shown in Figure 3.
In this type ofrule the five most frequent rules all try to move theverb more to the front of the sentence.
In the lastcase both parts of the verb are put together.7.3 Rule TypesIn a next group of experiments we evaluated theperformance of the different rule types.
In Table 3the translation performance of systems using dif-ferent rule types is shown.
The experiments werecarried out on the EPPS task as well as on theNEWS task.First it can be seen that the Left rules performbetter than the Right rules.
This is not surpris-ing, since they better describe how to reorder fromGerman to English and because they are more of-ten used in the lattice.
If both types are used this211Table 4: Summary of translation results for theGerman-English tasks (BLEU)System EPPS NEWSDev Test Dev TestBaseline 25.47 27.24 23.40 25.90Short 26.77 28.54 24.73 27.48Long 26.99 29.32 25.38 27.86lowers the performance a little.
So if it is clearwhich type explains the reordering better, only thistype should be used, but if that is not possible us-ing both types can still help.If both types of rules are compared, it can beseen that Part rules seem to have a more positiveinfluence than All ones.
The reason for this may bethat the Part rules can also be applied more oftenthan the rules of the other type.
Using the com-bination of both types of rules, the performance isbetter on one task and equally good on the othertask.
Consequently, we used the combination ofboth types in the remaining experiments.7.4 German-EnglishThe results on the German-English task are sum-marized in Table 4.
The long-range reorderingscould improve the performance by 0.8 and 0.4BLEU points on the different tasks compared toa system applying only short-range reorderings.These improvements are significant at a level of5%.We also analyzed the influence of tagging er-rors.
Therefore, we tagged every word of the testsentence with the tag that this word is mostly as-signed to in the training corpus.
If the word doesnot occur in the training corpus, it was tagged as anoun.
This results in different tags for 5% of thewords and a BLEU score of 27.68 on the NEWStest set using long-range reorderings.
So the trans-lation quality drops by about 0.2 BLEU points, butit is still better than the system using only short-range reorderings.In Figure 4 example translations of the baselinesystem, the system modeling only short-range re-orderings and the system using also long-range re-orderings rules are shown.
The part of the sen-tences that needs long-range reorderings is alwaysunderlined.In the first two examples the verbal phrase con-sists of two parts and the German one is splitted.In these cases, it was impossible for the short-Table 5: Translation results for the German-French translation task (BLEU)System EPPS NEWSDev Test Dev TestBaseline 25.86 27.05 17.90 18.52Short 27.02 28.06 18.59 19.99Long 27.27 28.61 19.10 20.11range reordering model to move the second part ofthe verb to the front so that it could be translatedcorrectly.
In one case this leads to a selection of aphrase pair that removes the verb from the transla-tion.
Thus it is hard to understand the meaning ofthe sentence.In the other two examples the verb of the subor-dinate clause has to be moved from the last posi-tion in the German sentence to the second positionin the English one.
This is again only possible us-ing the long-range reordering rules.
Furthermore,if these rules are not used, it is possible that theverb will be not translated at all as in the last ex-ample.7.5 German-FrenchWe also performed similar experiments on theGerman-French task.
Since the type of reorderingneeded for this language pair is similar to the oneused in the German-English task, we used also theLeft rules in the long-range reorderings.
As it canbe seen in Table 5, the long-range reordering rulescould also help to improve the translation perfor-mance for this language pair.
The improvement onthe EPPS task is significant at a level of 5%.7.6 English-GermanIn a last group of experiments we applied the sameapproach also to the English-German translationtask.
In this case the verb has to be moved tothe right, so that we used the Right rules for thelong-range reorderings.
Looking at the rule us-age of the different type of rules, the picture wasquite promising.
This time the Right rules couldbe applied more often and the Left ones only a fewtimes.
But if we look at the results as shown in Ta-ble 6, the long-range reorderings do not improvethe performance.
We will investigate the reasonsfor this in future work.212Figure 4: Example translation from German to English using different type of rulesSource: Diese Ma?nahmen werden als eine Art Wiedergutmachung fu?r fru?her begangenesUnrecht angesehen .Baseline: these measures will as a kind of compensation for once injustice done .Short: these measures will as a kind of compensation for once injustice done .Long: these measures will be seen as a kind of compensation for once injustice done .Source: Das wird mit derart unterschiedlichen Mitgliedern unmo?glich sein .Baseline: this will with such different impossible .Short: this will with such different impossible .Long: this will be impossible to such different members .Source: Er braucht die Unterstu?tzung derer , die an den Markt und an die Gleichbehandlungaller glauben .Baseline: he needs the support of those who market and the equal treatment of all believe .Short: it needs the support of those who in the market and the equal treatment of all believe .Long: it needs the support of those who believe in the market and the equal treatment of all .Source: .., da?
sie das Einwanderungsproblem als politischen Hebel benutzen .Baseline: .. that they the immigration problem as a political lever .Short: .. that the problem of immigration as a political lever .Long: .. that they use the immigration problem as a political lever .Table 6: Translation results for the English-German translation task (BLEU)System EPPS NEWSDev Test Dev TestBaseline 18.93 2072 16.31 17.91Short 19.49 21.56 17.13 18.31Long 19.56 21.33 16.93 18.158 ConclusionWe have presented a new method to model long-range reorderings in statistical machine transla-tion.
This method extends a framework basedon extracting POS-based reordering rules from analigned parallel corpus by adding discontinuousreordering rules.
Allowing rules with gaps cap-tures very long-range reorderings while avoidingthe data sparseness problem of very long continu-ous reordering rules.The extracted rules are used to generate a wordlattice with different possible reorderings of thesource sentence in a preprocessing step prior to de-coding.
Placing various restrictions on the appli-cation of the rules keeps the lattice small enoughfor efficient decoding.
Compared to a baselinesystem that only uses continuous reordering rules,applying additional discontinuous rules improvedthe translation performance on a German-Englishtranslation task significantly by up to 0.8 BLEUpoints.In contrast to approaches like Collins et al(2005) and Popovic?
and Ney (2006), the rules arecreated in a data-driven way and not manually.
Itwas therefore easily possible to transfer this ap-proach to the German-French translation task, andwe showed that we could improve the translationquality for this language pair as well.
Further-more, this approach needs only the POS informa-tion and no syntax tree.
Thus, if we use the ap-proximation for the tags as described before, theapproach could also easily be integrated into areal-time translation system.An unsolved problem is still why this ap-proach does not improve the results of the English-German translation task.
An explanation might bethat here the reordering problem is even more dif-ficult, since the German word order is very free.AcknowledgmentsThis work was partly supported by Quaero Pro-gramme, funded by OSEO, French State agencyfor innovation.ReferencesAdam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A Maximum Entropy Ap-213proach to Natural Language Processing.
Compua-tional Linguistics, 22(1):39?71.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation: Pa-rameter Estimation.
Computational Linguistics,19(2):263?311.Boxing Chen, Mauro Cettolo, and Marcello Federico.2006.
Reordering Rules for Phrase-based Statisti-cal Machine Translation.
In International Workshopon Spoken Language Translation (IWSLT 2006), Ky-oto, Japan.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause Restructuring for Statistical MachineTranslation.
In Proc.
of the 43rd Annual Meeting onAssociation for Computational Linguistics (ACL),pages 531?540.Marta R. Costa-jussa` and Jose?
A. R. Fonollosa.
2006.Statistical Machine Reordering.
In Conference onEmpirical Methods on Natural Language Process-ing (EMNLP 2006), Sydney, Australia.Nizar Crego and Nizar Habash.
2008.
Using Shal-low Syntax Information to Improve Word Align-ment and Reordering for SMT.
In 46th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies (ACL-08:HLT), Columbus, Ohio, USA.George Foster, Roland Kuhn, and Howard Johnson.2006.
Phrasetable Smoothing for Statistical Ma-chine Translation.
In Conference on EmpiricalMethods in Natural Language Processing (EMNLP2006), Sydney, Australia.Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,Chris Callison-Burch, Miles Osborne, and DavidTalbot.
2005.
Edinburgh System Description forthe 2005 IWSLT Speech Translation Evaluation.
InIWSLT, Pittsburgh, PA, USA.Franz Josef Och and Herman Ney.
2000.
ImprovedStatistical Alignment Models.
In 38th Annual Meet-ing of the Association for Computational Linguistics(ACL 2000), Hong Kong.Franz J. Och, Daniel Gildea, Sanjeev P. Khudan-pur, Anoop Sarkar, Kenji Yamada, AlexanderFraser, Shankar Kumar, Libin Shen, David A.Smith, Katherine Eng, Viren Jain, Zhen Jin, andDragomir R. Radev.
2004.
A Smorgasboard of Fea-tures for Statistical Machine Translation.
In HumanLanguage Technology Conference and the 5th Meet-ing of the North American Association for Com-putational Linguistics (HLT-NAACL 2004), Boston,USA.Maja Popovic?
and Hermann Ney.
2006.
POS-basedWord Reorderings for Statistical Machine Transla-tion.
In International Conference on Language Re-sources and Evaluation (LREC 2006), Genoa, Italy.Kay Rottmann and Stephan Vogel.
2007.
Word Re-ordering in Statistical Machine Translation with aPOS-Based Distortion Model.
In TMI, Sko?vde,Sweden.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In InternationalConference on New Methods in Language Process-ing, Manchester, UK.Libin Shen, Anoop Sarkar, and Franz Och.
2004.Discriminative Reranking for Machine Translation.In Human Language Technology Conference andthe 5th Meeting of the North American Associationfor Computational Linguistics (HLT-NAACL 2004),Boston, USA.Christoph Tillmann and Tong Zhang.
2005.
A Local-ized Prediction Model for Statistical Machine Trans-lation.
In 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL 2005), Ann Ar-bor, Michigan, USA.Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble,Ashish Venogupal, Bing Zhao, and Alex Waibel.2003.
The CMU Statistical Translation System.
InMT Summit IX, New Orleans, LA, USA.Stephan Vogel.
2003.
SMT Decoder Dissected: WordReordering.
In Int.
Conf.
on Natural LanguageProcessing and Knowledge Engineering, Beijing,China.Yeyi Wang and Alex Waibel.
1998.
Fast Decodingfor Statistical Machine Translation.
In ICSLP?98,Sydney, Australia.Dekai Wu.
1996.
A Polynomial-time Algorithm forStatistical Machine Translation.
In ACL-96: 34thAnnual Meeting of the Assoc.
for ComputationalLinguistics, Santa Cruz, CA, USA, June.Kenji Yamada and Kevin Knight.
2000.
A Syntax-based Statistical Translation Model.
In 38th AnnualMeeting of the Association for Computational Lin-guistics (ACL 2000), Hong Kong.Richard Zens and Hermann Ney.
2003.
A Compar-ative Study on Reordering Constraints in StatisticalMachine Translation.
In 41st Annual Meeting of theAssociation for Computational Linguistics (ACL),pages 192?202, Sapporo, Japan.Ying Zhang and Stephan Vogel.
2004.
MeasuringConfidence Intervals for mt Evaluation Metrics.
InTMI 2004, Baltimore, MD, USA.Yuqi Zhang, Richard Zens, and Hermann Ney.
2007.Chunk-Level Reordering of Source Language Sen-tences with Automatically Learned Rules for Sta-tistical Machine Translation.
In HLT-NAACL Work-shop on Syntax and Structure in Statistical Transla-tion, Rochester, NY, USA.214
