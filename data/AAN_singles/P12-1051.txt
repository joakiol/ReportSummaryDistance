Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 488?496,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsSemantic Parsing with Bayesian Tree TransducersBevan Keeley Jones?
?b.k.jones@sms.ed.ac.ukMark Johnson?Mark.Johnson@mq.edu.auSharon Goldwater?sgwater@inf.ed.ac.uk?
School of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UK?
Department of ComputingMacquarie UniversitySydney, NSW 2109, AustraliaAbstractMany semantic parsing models use tree trans-formations to map between natural languageand meaning representation.
However, whiletree transformations are central to severalstate-of-the-art approaches, little use has beenmade of the rich literature on tree automata.This paper makes the connection concretewith a tree transducer based semantic parsingmodel and suggests that other models can beinterpreted in a similar framework, increasingthe generality of their contributions.
In par-ticular, this paper further introduces a varia-tional Bayesian inference algorithm that is ap-plicable to a wide class of tree transducers,producing state-of-the-art semantic parsing re-sults while remaining applicable to any do-main employing probabilistic tree transducers.1 IntroductionSemantic parsing is the task of mapping natural lan-guage sentences to a formal representation of mean-ing.
Typically, a system is trained on pairs of naturallanguage sentences (NLs) and their meaning repre-sentation expressions (MRs), as in figure 1(a), andthe system must generalize to novel sentences.Most semantic parsing models rely on an assump-tion of structural similarity between MR and NL.Since strict isomorphism is overly restrictive, thisassumption is often relaxed by applying transforma-tions.
Several approaches assume a tree structure tothe NL, MR, or both (Ge and Mooney, 2005; Kateand Mooney, 2006; Wong and Mooney, 2006; Luet al, 2008; Bo?rschinger et al, 2011), and often in-Figure 1: (a) An example sentence/meaning pair, (b) atree transformation based mapping, and (c) a tree trans-ducer that performs the mapping.volve tree transformations either between two treesor a tree and a string.The tree transducer, a formalism from automatatheory which has seen interest in machine transla-tion (Yamada and Knight, 2001; Graehl et al, 2008)and has potential applications in many other areas,is well suited to formalizing such tree transforma-tion based models.
Yet, while many semantic pars-ing systems resemble the formalism, each was pro-posed as an independent model requiring custom al-gorithms, leaving it unclear how developments inone line of inquiry relate to others.
We argue for aunifying theory of tree transformation based seman-tic parsing by presenting a tree transducer model anddrawing connections to other similar systems.We make a further contribution by bringing totree transducers the benefits of the Bayesian frame-work for principled handling of data sparsity and488prior knowledge.
Graehl et al (2008) present an EMtraining procedure for top down tree transducers, butwhile there are Bayesian approaches to string trans-ducers (Chiang et al, 2010) and PCFGs (Kuriharaand Sato, 2006), there has yet to be a proposal forBayesian inference in tree transducers.
Our vari-ational algorithm produces better semantic parsesthan EM while remaining general to a broad classof transducers appropriate for other domains.In short, our contributions are three-fold: wepresent a new state-of-the-art semantic parsingmodel, propose a broader theory for tree transforma-tion based semantic parsing, and present a generalinference algorithm for the tree transducer frame-work.
We recommend the last of these as just onebenefit of working within a general theory: contri-butions are more broadly applicable.2 Meaning representations and regulartree grammarsIn semantic parsing, an MR is typically an expres-sion from a machine interpretable language (e.g., adatabase query language or a logical language likeProlog).
In this paper we assume MRs can be rep-resented as trees, either by pre-parsing or becausethey are already trees (often the case for functionallanguages like LISP).1 More specifically, we assumethe MR language is a regular tree language.A regular tree grammar (RTG) closely resemblesa context free grammar (CFG), and is a way of de-scribing a language of trees.
Formally, define T?
asthe set of trees with symbols from alphabet ?, andT?
(A) as the set of all trees in T?
?A where symbolsfrom A only occur at the leaves.
Then an RTG is atuple (Q,?, qstart,R), where Q is a set of states, ?is an alphabet, qstart ?
Q is the initial state, and Ris a set of grammar rules of the form q ?
t, where qis a state from Q and t is a tree from T?
(Q).A rule typically consists of a parent state (left) andits child states and output symbol (right).
We indi-cate states using all capital letters:NUM ?
population(PLACE).Intuitively, an RTG is a CFG where the yield ofevery parse is itself a tree.
In fact, for any CFG G, it1See Liang et al (2011) for work in representing lambdacalculus expressions with trees.is straightforward to produce a corresponding RTGthat generates the set of parses of G. Consequently,while we assume we have an RTG for the MR lan-guage, there is no loss of generality if the MR lan-guage is actually context free.3 Weighted root-to-frontier, linear,non-deleting tree-to-string transducersTree transducers (Rounds, 1970; Thatcher, 1970) aregeneralizations of finite state machines that operateon trees.
Mirroring the branching nature of its in-put, the transducer may simultaneously transition toseveral successor states, assigning a separate state toeach subtree.There are many classes of transducer with dif-ferent formal properties (Knight and Greahl, 2005;Maletti et al, 2009).
Figure 1(c) is an example ofa root-to-frontier, linear, non-deleting tree-to-stringtransducer.
It is defined using rules where the lefthand side identifies a state of the transducer and afragment of the input tree, and the right hand sidedescribes a portion of the output string.
Variablesxi stand for entire sub-trees, and state-variable pairsqj .xi stand for strings produced by applying thetransducer starting at state qj to subtree xi.
Fig-ure 1(b) illustrates an application of the transducer,taking the tree on the left as input and outputting thestring on the right.Formally, a weighted root-to-frontier, tree-to-string transducer is a 5-tuple (Q,?,?, qstart,R).
Qis a finite set of states, ?
and ?
are the input and out-put alphabets, qstart is the start state, and R is theset of rules.
Denote a pair of symbols, a and b bya.b, the cross product of two sets A and B by A.B,and let X be the set of variables {x0, x1, ...}.
Then,each rule r ?
R is of the form [q.t ?
u].v, wherev ?
?
?0 is the rule weight, q ?
Q, t ?
T?
(X ), andu is a string in (?
?
Q.X )?
such that every x ?
Xin u also occurs in t.We say q.t is the left hand side of rule r and u itsright hand side.
The transducer is linear iff no vari-able appears more than once on the right hand side.It is non-deleting iff all variables on the left handside also occur on the right hand side.
In this paperwe assume that every tree t on the left hand side is ei-ther a single variable x0 or of the form ?
(x0, ...xn),where ?
?
?
(i.e., it is a tree of depth ?
1).489A weighted tree transducer may define a probabil-ity distribution, either a joint distribution over inputand output pairs or a conditional distribution of theoutput given the input.
Here, we will use joint dis-tributions, which can be defined by ensuring that theweights of all rules with the same state on the left-hand side sum to one.
In this case, it can be help-ful to view the transducer as simultaneously gener-ating both the input and output, rather than the usualview of mapping input trees into output strings.
Ajoint distribution allows us to model with a singlemachine both the input and output languages, whichis important during decoding when we want to inferthe input given the output.4 A generative model of semantic parsingLike the hybrid tree semantic parser (Lu et al, 2008)and the synchronous grammar based WASP (Wongand Mooney, 2006), our model simultaneously gen-erates the input MR tree and the output NL string.The MR tree is built up according to the providedMR grammar, one grammar rule at a time.
Coupledwith the application of the MR rule, similar CFG-like productions are applied to the NL side, repeateduntil both the MR and NL are fully generated.
Ineach step, we select an MR rule and then build theNL by first choosing a pattern with which to expandit and then filling out that pattern with words drawnfrom a unigram distribution.This kind of coupled generative process canbe naturally formalized with tree transducer rules,where the input tree fragment on the left side of eachrule describes the derivation of the MR and the rightdescribes the corresponding NL derivation.For a simple example of a tree-to-string trans-ducer rule considerq.population(x1) ?
?population of?
q.x1 (1)which simultaneously generates tree fragmentpopulation(x1) on the left and sub-string ?popula-tion of q.x1?
on the right.
Variable x1 stands foran MR subtree under population, and, on the right,state-variable pair q.x1 stands for the NL substringgenerated while processing subtree x1 starting fromq.
While this rule can serve as a single step ofan MR-to-NL map such as the example transducershown in Figure 1(c), such rules do not model theNUM ?
population(PLACE) (m)PLACE ?
cityid(CITY, STATE) (r)CITY ?
portland (u)STATE ?
maine (v)qMRm,1.x1 ?
qNLr .x1 (2)qMRr,1 .x1 ?
qNLu .x1qMRr,2 .x1 ?
qNLv .x1qNLm .population(w1, x1, w2) ?qWm .w1 qMRm,1.x1 qEND.w2 (3)qNLr .cityid(w1, x1, w2, x2, w3) ?qEND.w1 qMRr,2 .x2 qWr .w2 qMRr,1 .x1 qEND.w3 (4)qWm .w1 ?
?population?
qWm .w1 (5)qWm .w1 ?
?of?
qWm .w1qWm .w1 ?
... qWm .w1qWm .w1 ?
?of?
qEND.w1 (6)qWm .w1 ?
... qEND.w1qEND.W ?
?
(7)Figure 2: Examples of transducer rules (bottom) that gen-erate MR and NL associated with MR rules m-v (top).Transducer rule 2 selects MR rule r from the MR gram-mar.
Rule 3 simultaneously writes the MR associatedwith rule m and chooses an NL pattern (as does 4 forr).
Rules 5-7 generate the words associated with m ac-cording to a unigram distribution specific to m.grammaticality of the MR and lack flexibility sincesub-strings corresponding to a given tree fragmentmust be completely pre-specified.
Instead, we breaktransductions down into a three stage process ofchoosing the (i) MR grammar rule, (ii) NL expan-sion pattern, and (iii) individual words according toa unigram distribution.
Such a decomposition in-corporates independence assumptions that improvegeneralizability.
See Figure 2 for example rulesfrom our transducer and Figure 3 for a derivation.To ensure that only grammatical MRs are gener-ated, each state of our transducer encodes the iden-tity of exactly one MR grammar rule.
Transitionsbetween qMR and qNL states implicitly select the em-bedded rule.
For instance, rule 2 in Figure 2 selects490MR grammar rule r to expand the ith child of theparent produced by rule m. Aside from ensuringthe grammaticality of the generated MR, rules ofthis type also model the probability of the MR, con-ditioning the probability of a rule both on the par-ent rule and the index of the child being expanded.Thus, parent state qMRm,1 encodes not only the identityof rule m, but also the child index, 1 in this case.Once the MR rule is selected, qNL states are ap-plied to select among rules such as 3 and 4 to gen-erate the MR entity and choose the NL expansionpattern.
These rules determine the word order of thelanguage by deciding (i) whether or not to generatewords in a given location and (ii) where to insert theresult of processing each MR subtree.
Decision (i) ismade by either transitioning to state qWr to generatewords or to qEND to generate the empty string.
De-cision (ii) is made with the order of xi?s on the righthand side.
Rule 4 illustrates the case where port-land and maine in cityid(portland, maine) would berealized in reverse order as ?maine ... portland?.The particular set of patterns that appear on theright of rules such as 3 embodies the binary word at-tachment decisions and the particular permutation ofxi in the NL.
We allow words to be generated at thebeginning and end of each pattern and between thexis.
Thus, rule 4 is just one of 16 such possible pat-terns (3 binary decisions and 2 permutations), whilerule 3 is one of 4.
We instantiate all such rules andallow the system to learn weights for them accordingto the language of the training data.Finally, the NL is filled out with words chosen ac-cording to a unigram distribution, implemented in aPCFG-like fashion, using a different rule for eachword which recursively chooses the next word un-til a string termination rule is reached.2 Generatingword sequence ?population of?
entails first choosingrule 5 in Figure 2.
State qWr is then recursively ap-plied to choose rule 6, generating ?of?
at the sametime as deciding to terminate the string by transi-tioning to a new state qEND which deterministicallyconcludes by writing the empty string ?.On the MR side, rules 5-7 do very little: the treeon the left side of rules 5 and 6 consists entirely of a2There are roughly 25,000 rules in the transducers in ourexperiments, and the majority of these implement the unigramword distributions since every entity in the MR may potentiallyproduce any of the words it is paired with in training.subtree variable w1, indicating that nothing is gener-ated in the MR. Rule 7 subsequently generates thesesubtrees as W symbols, marking corresponding lo-cations where words might be produced in the NL,which are later removed during post processing.3Figure 3(b) illustrates the coupled generative pro-cess.
At each step of the derivation, an MR rule ischosen to expand a node of the MR tree, and then acorresponding part of the NL is expanded.
Step 1.1of the example chooses MR rule m, NUM ?population(PLACE).
Transducer rule 3 then gener-ates population in the MR (shown in the left column)at the same time as choosing an NL expansion pat-tern (Step 1.2) which is subsequently filled out withspecific words ?population?
(1.3) and ?of?
(1.4).This coupled derivation can be represented by atree, shown in Figure 3(c), which explicitly repre-sents the dependency structure of the coupled MRand NL (a simplified version is shown in (d) for clar-ity).
In our transducer, which defines a joint distri-bution over both the MR and NL, the probability ofa rule is conditioned on the parent state.
Since eachstate encodes an MR rule, MR rule specific distribu-tions are learned for both the words and their order.5 Relation to existing modelsThe tree transducer model can be viewed either asa generative procedure for building up two separatestructures or as a transformative machine that takesone as input and produces another as output.
Dif-ferent semantic parsing approaches have taken oneor the other view, and both can be captured in thissingle framework.WASP (Wong and Mooney, 2006) is an exam-ple of the former perspective, coupling the genera-tion of the MR and NL with a synchronous gram-mar, a formalism closely related to tree transducers.The most significant difference from our approachis that they use machine translation techniques forautomatically extracting rules from parallel corpora;similar techniques can be applied to tree transduc-ers (Galley et al, 2004).
In fact, synchronous gram-mars and tree transducers can be seen as instances ofthe same more general class of automata (Shieber,3The addition of W symbols is a convenience; it is easier todesign transducer rules where every substring on the right sidecorresponds to a subtree on the left.491Figure 3: Coupled derivation of an (MR, NL) pair.
At each step an MR grammar rule is chosen to expand the MR andthe corresponding portion of the NL is then generated.
Symbols W stand for locations in the tree corresponding tosubstrings of the output and are removed in a post-processing step.
(a) The (MR, NL) pair.
(b) Step by step derivation.
(c) The same derivation shown in tree form.
(d) The underlying dependency structure of the derivation.2004).
Rather than argue for one or the other, wesuggest that other approaches could also be inter-preted in terms of general model classes, groundingthem in a broader base of theory.The hybrid tree model (Lu et al, 2008) takesa transformative perspective that is in some waysmore similar to our model.
In fact, there is a one-to-one relationship between the multinomial param-eters of the two models.
However, they represent theMR and NL with a single tree and apply tree walk-ing algorithms to extract them.
Furthermore, theyimplement a custom training procedure for search-ing over the potential MR transformations.
The treetransducer, on the other hand, naturally captures thesame probabilistic dependencies while maintainingthe separation between MR and NL, and further al-lows us to build upon a larger body of theory.KRISP (Kate and Mooney, 2006) uses string clas-sifiers to label substrings of the NL with entitiesfrom the MR. To focus search, they impose an or-dering constraint based on the structure of the MRtree, which they relax by allowing the re-orderingof sibling nodes and devise a procedure for recover-ing the MR from the permuted tree.
This procedurecorresponds to backward-application in tree trans-ducers, identifying the most likely input tree given a492particular output string.SCISSOR (Ge and Mooney, 2005) takes syntacticparses rather than NL strings and attempts to trans-late them into MR expressions.
While few seman-tic parsers attempt to exploit syntactic information,there are techniques from machine translation forusing tree transducers to map between parsed par-allel corpora, and these techniques could likely beapplied to semantic parsing.Bo?rschinger et al (2011) argue for the PCFG asan alternative model class, permitting conventionalgrammar induction techniques, and tree transducersare similar enough that many techniques are applica-ble to both.
However, the PCFG is less amenable toconceptualizing correspondences between parallelstructures, and their model is more restrictive, onlyapplicable to domains with finite MR languages,since their non-terminals encode entire MRs. Thetree transducer framework, on the other hand, allowsus to condition on individual MR rules.6 Variational Bayes for tree transducersAs seen in the example in Figure 3(c), tree trans-ducers not only operate on trees, their derivationsare themselves trees, making them amenable to dy-namic programming and an EM training procedureresembling inside-outside (Graehl et al, 2008).
EMassigns zero probability to events not seen in thetraining data, however, limiting the ability to gen-eralize to novel items.
The Bayesian framework of-fers an elegant solution to this problem, introducinga prior over rule weights which simultaneously en-sures that all rules receive non-zero probability andallows the incorporation of prior knowledge and in-tuitions.
Unfortunately, the introduction of a priormakes exact inference intractable, so we use an ap-proximate method, variational Bayesian inference(Bishop, 2006), deriving an algorithm similar to thatfor PCFGs (Kurihara and Sato, 2006).The tree transducer defines a joint distributionover the input y, output w, and their derivation xas the product of the weights of the rules appearingin x.
That is,p(y, x, w|?)
=?r?R?
(r)cr(x)where ?
is the set of multinomial parameters, r is atransducer rule, ?
(r) is its weight, and cr(x) is thenumber of times r appears in x.
In EM, we are in-terested in the point estimate for ?
that maximizesp(Y,W|?
), where Y and W are the N input-outputpairs in the training data.
In the Bayesian setting,however, we place a symmetric Dirichlet prior over?
and estimate a posterior distribution over both Xand ?.p(?,X|Y,W) = p(Y,X ,W, ?
)p(Y,W)= p(?
)?Ni=1 p(yi, xi, wi|?)?p(?
)?Ni=1?x?Xi p(yi, x, wi|?
)d?Since the integral in the denominator is in-tractable, we look for an appropriate approximationq(?,X ) ?
p(?,X|Y,W).
In particular, we assumethe rule weights and the derivations are independent,i.e., q(?,X ) = q(?
)q(X ).
The basic idea is then todefine a lower bound F ?
ln p(Y,W) in terms of qand then apply the calculus of variations to find a qthat maximizes F .ln p(Y,W|?)
= lnEq[p(Y,X ,W|?
)q(?,X ) ]?
Eq[lnp(Y,X ,W|?
)q(?,X ) ] = F ,Applying our independence assumption, we arrive atthe following expression for F , where ?t is the par-ticular parameter vector corresponding to the ruleswith parent state t:F =?t?Q(Eq(?t)[ln p(?t|?t)]?
Eq(?t)[ln q(?t)])+N?i=1(Eq[ln p(wi, xi, yi|?)]?
Eq(xi)[ln q(xi)]).We find the q(?t) and q(xi) that maximize F bytaking derivatives of the Lagrangian, setting them tozero, and solving, which yields:q(?t) = Dirichlet(?t|?
?t)q(xi) =?r?R ??
(r)cr(xi)?x?Xi?r?R ??(r)cr(x)where??
(r) = ?
(r) +?iEq(xi)[cr(xi)]??
(r) = exp???(??(r))??(?r:s(r)=t??(r))??
.493The parameters of q(?t) are defined with respectto q(xi) and the parameters of q(xi) with respectto the parameters of q(?t).
q(xi) can be computedefficiently using inside-outside.
Thus, we can per-form an EM-like alternation between calculating ?
?and ?
?.4It is also possible to estimate the hyper-parameters ?
from data, a practice known as em-pirical Bayes, by optimizing F .
We explore learn-ing separate hyper-parameters ?t for each ?t, us-ing a fixed point update described by Minka (2000),where kt is the number of rules with parent state t:?
?t =(1?t+ 1kt?2t(?2F?
?2t)?1( ?F?
?t))?17 Training and decodingWe implement our VB training algorithm inside thetree transducer package Tiburon (May and Knight,2006), and experiment with both manually set andautomatically estimated priors.
For our manuallyset priors, we explore different hyper-parameter set-tings for three different priors, one for each of themain decision types: MR rule, NL pattern, and wordgeneration.
For the automatic priors, we estimateseparate hyper-parameters for each multinomial (ofwhich there are hundreds).
As is standard, we ini-tialize the word distributions using a variant of IBMmodel 1, and make use of NP lists (a manually cre-ated list of the constants in the MR language pairedwith the words that refer to them in the corpus).At test time, since finding the most probable MRfor a sentence involves summing over all possiblederivations, we instead find the MR associated withthe most probable derivation.8 Experimental setup and evaluationWe evaluate the system on GeoQuery (Wong andMooney, 2006), a parallel corpus of 880 Englishquestions and database queries about United Statesgeography, 250 of which were translated into Span-ish, Japanese, and Turkish.
We present here ad-ditional translations of the full 880 sentences into4Because of the resemblance to EM, this procedure has beencalled VBEM.
Unlike EM, however, this procedure alternatesbetween two estimation steps and has no maximization step.German, Greek, and Thai.
For evaluation, follow-ing from Kwiatkowski et al (2010), we reserve 280sentences for test and train on the remaining 600.During development, we use cross-validation on the600 sentence training set.
At test, we run once on theremaining 280 and perform 10 fold cross-validationon the 250 sentence sets.To judge correctness, we follow standard prac-tice and submit each parse as a GeoQuery databasequery, and say the parse is correct only if the answermatches the gold standard.
We report raw accuracy(the percentage of sentences with correct answers),as well as F1: the harmonic mean of precision (theproportion of correct answers out of sentences witha parse) and recall (the proportion of correct answersout of all sentences).5We run three other state-of-the-art systems forcomparison.
WASP (Wong and Mooney, 2006) andthe hybrid tree (Lu et al, 2008) are chosen to rep-resent tree transformation based approaches, and,while this comparison is our primary focus, we alsoreport UBL-S (Kwiatkowski et al, 2010) as a non-tree based top-performing system.6 The hybrid treeis notable as the only other system based on a gen-erative model, and uni-hybrid, a version that uses aunigram distribution over words, is very similar toour own model.
We also report the best performingversion, re-hybrid, which incorporates a discrimina-tive re-ranking step.We report transducer performance under three dif-ferent training conditions: tsEM using EM, tsVB-auto using VB with empirical Bayes, and tsVB-handusing hyper-parameters manually tuned on the Ger-man training data (?
of 0.3, 0.8, and 0.25 for MRrule, NL pattern, and word choices, respectively).Table 1 shows results for 10 fold cross-validationon the training set.
The results highlight the benefitof the Dirichlet prior, whether manually or automat-ically set.
VB improves over EM considerably, mostlikely because (1) the handling of unknown wordsand MR entities allows it to return an analysis for allsentences, and (2) the sparse Dirichlet prior favorsfewer rules, reasonable in this setting where only afew words are likely to share the same meaning.5Note that accuracy and f-score reduce to the same formulaif there are no parse failures.6UBL-S is based on CCG, which can be viewed as a map-ping between graphs more general than trees.494DEV geo600 - 10 fold cross-valGerman GreekAcc F1 Acc F1UBL-S 76.7 76.9 76.2 76.5WASP 66.3 75.0 71.2 79.7uni-hybrid 61.7 66.1 71.0 75.4re-hybrid 62.3 69.5 70.2 76.8tsEM 61.7 67.9 67.3 73.2tsVB-auto 74.0 74.0 ?79.8 ?79.8tsVB-hand ?78.0 ?78.0 79.0 79.0English ThaiUBL-S 85.3 85.4 74.0 74.1WASP 73.5 79.4 69.8 73.9uni-hybrid 76.3 79.0 71.3 73.7re-hybrid 77.0 82.2 71.7 76.0tsEM 73.5 78.1 69.8 72.9tsVB-auto 81.2 81.2 74.7 74.7tsVB-hand ?83.7 ?83.7 ?76.7 ?76.7Table 1: Accuracy and F1 score comparisons on thegeo600 training set.
Highest scores are in bold, whilethe highest among the tree based models are marked witha bullet.
The dotted line separates the tree based fromnon-tree based models.On the test set (Table 2), we only run the modelvariants that perform best on the training set.
Test setaccuracy is consistently higher for the VB trainedtree transducer than the other tree transformationbased models (and often highest overall), while f-score remains competitive.79 ConclusionWe have argued that tree transformation based se-mantic parsing can benefit from the literature on for-mal language theory and tree automata, and havetaken a step in this direction by presenting a treetransducer based semantic parser.
Drawing this con-nection facilitates a greater flow of ideas in theresearch community, allowing semantic parsing toleverage ideas from other work with tree automata,while making clearer how seemingly isolated ef-forts might relate to one another.
We demonstratethis by both building on previous work in train-ing tree transducers using EM (Graehl et al, 2008),7Numbers differ slightly here from previously published re-sults due to the fact that we have standardized the inputs to thedifferent systems.TEST geo880 - 600 train/280 testGerman GreekAcc F1 Acc F1UBL-S 75.0 75.0 73.6 73.7WASP 65.7 ?
74.9 70.7 ?
78.6re-hybrid 62.1 68.5 69.3 74.6tsVB-hand ?
74.6 74.6 ?75.4 75.4English ThaiUBL-S 82.1 82.1 66.4 66.4WASP 71.1 77.7 71.4 75.0re-hybrid 76.8 ?
81.0 73.6 76.7tsVB-hand ?
79.3 79.3 ?
78.2 ?
78.2geo250 - 10 fold cross-valEnglish SpanishUBL-S 80.4 80.6 79.7 80.1WASP 70.0 80.8 72.4 81.0re-hybrid 74.8 82.6 78.8 ?
86.2tsVB-hand ?
83.2 ?
83.2 ?
80.0 80.0Japanese TurkishUBL-S 80.5 80.6 74.2 74.9WASP 74.4 ?
82.9 62.4 75.9re-hybrid 76.8 82.4 66.8 ?
77.5tsVB-hand ?
78.0 78.0 ?
75.6 75.6Table 2: Accuracy and F1 score comparisons on thegeo880 and geo250 test sets.
Highest scores are inbold, while the highest among the tree based models aremarked with a bullet.
The dotted line separates the treebased from non-tree based models.7and describing a general purpose variational infer-ence algorithm for adapting tree transducers to theBayesian framework.
The new VB algorithm re-sults in an overall performance improvement for thetransducer over EM training, and the general effec-tiveness of the approach is further demonstrated bythe Bayesian transducer achieving highest accuracyamong other tree transformation based approaches.AcknowledgmentsWe thank Joel Lang, Michael Auli, Stella Frank,Prachya Boonkwan, Christos Christodoulopoulos,Ioannis Konstas, and Tom Kwiatkowski for provid-ing the new translations of GeoQuery.
This researchwas supported in part under the Australian Re-search Council?s Discovery Projects funding scheme(project number DP110102506).495ReferencesChristopher M. Bishop.
Pattern Recognition and Ma-chine Learning.
Springer, 2006.Benjamin Bo?rschinger, Bevan K. Jones, and Mark John-son.
Reducing grounded learning tasks to grammati-cal inference.
In Proc.
of the Conference on EmpiricalMethods in Natural Language Processing, 2011.David Chiang, Jonathan Graehl, Kevin Knight, AdamPauls, and Sujith Ravi.
Bayesian inference for finite-state transducers.
In Proc.
of the annual meeting ofthe North American Association for Computational Lin-guistics, 2010.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
What?s in a translation rule?
In Proc.
of theannual meeting of the North American Association forComputational Linguistics, 2004.Ruifang Ge and Raymond J. Mooney.
A statistical se-mantic parser that integrates syntax and semantics.
InProceedings of the Conference on Computational Natu-ral Language Learning, 2005.Jonathon Graehl, Kevin Knight, and Jon May.
Trainingtree transducers.
Computational Linguistics, 34:391?427, 2008.Rohit J. Kate and Raymond J. Mooney.
Using string-kernels for learning semantic parsers.
In Proc.
of theInternational Conference on Computational Linguisticsand the annual meeting of the Association for Compu-tational Linguistics, 2006.Kevin Knight and Jonathon Greahl.
An overview of prob-abilistic tree transducers for natural language process-ing.
In Proc.
of the 6th International Conference onIntelligent Text Processing and Computational Linguis-tics, 2005.Kenichi Kurihara and Taisuke Sato.
Variational Bayesiangrammar induction for natural language.
In Proc.
ofthe 8th International Colloquium on Grammatical In-ference, 2006.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
Inducing probabilistic CCGgrammars from logical form with higher-order unifica-tion.
In Proc.
of the Conference on Empirical Methodsin Natural Language Processing, 2010.Percy Liang, Michael I. Jordan, and Dan Klein.
Learningdependency-based compositional semantics.
In Proc.of the annual meeting of the Association for Computa-tional Linguistics, 2011.Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-moyer.
A generative model for parsing natural languageto meaning representations.
In Proc.
of the Conferenceon Empirical Methods in Natural Language Processing,2008.Andreas Maletti, Jonathan Graehl, Mark Hopkins, andKevin Knight.
The power of extended top-down treetransducers.
SIAM J.
Comput., 39:410?430, June 2009.Jon May and Kevin Knight.
Tiburon: A weighted tree au-tomata toolkit.
In Proc.
of the International Conferenceon Implementation and Application of Automata, 2006.Tom Minka.
Estimating a Dirichlet distribution.
Techni-cal report, M.I.T., 2000.W.C.
Rounds.
Mappings and grammars on trees.
Mathe-matical Systems Theory 4, pages 257?287, 1970.Stuart M. Shieber.
Synchronous grammars as tree trans-ducers.
In Proc.
of the Seventh International Workshopon Tree Adjoining Grammar and Related Formalisms,2004.J.W.
Thatcher.
Generalized sequential machine maps.
J.Comput.
System Sci.
4, pages 339?367, 1970.Yuk Wah Wong and Raymond J. Mooney.
Learning forsemantic parsing with statistical machine translation.
InProc.
of Human Language Technology Conference andthe annual meeting of the North American Chapter ofthe Association for Computational Linguistics, 2006.Kenji Yamada and Kevin Knight.
A syntax-based statis-tical translation model.
In Proc.
of the annual meetingof the Association for Computational Linguistics, 2001.496
