Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 343?350,New York, June 2006. c?2006 Association for Computational LinguisticsEstimation of ConsistentProbabilistic Context-free GrammarsMark-Jan NederhofMax Planck Institutefor PsycholinguisticsP.O.
Box 310NL-6500 AH NijmegenThe NetherlandsMarkJan.Nederhof@mpi.nlGiorgio SattaDept.
of Information EngineeringUniversity of Paduavia Gradenigo, 6/AI-35131 PadovaItalysatta@dei.unipd.itAbstractWe consider several empirical estimatorsfor probabilistic context-free grammars,and show that the estimated grammarshave the so-called consistency property,under the most general conditions.
Ourestimators include the widely applied ex-pectation maximization method, used toestimate probabilistic context-free gram-mars on the basis of unannotated corpora.This solves a problem left open in the lit-erature, since for this method the consis-tency property has been shown only underrestrictive assumptions on the rules of thesource grammar.1 IntroductionProbabilistic context-free grammars are one of themost widely used formalisms in current work in sta-tistical natural language parsing and stochastic lan-guage modeling.
An important property for a proba-bilistic context-free grammar is that it be consistent,that is, the grammar should assign probability of oneto the set of all finite strings or parse trees that itgenerates.
In other words, the grammar should notlose probability mass with strings or trees of infinitelength.Several methods for the empirical estimation ofprobabilistic context-free grammars have been pro-posed in the literature, based on the optimization ofsome function on the probabilities of the observeddata, such as the maximization of the likelihood ofa tree bank or a corpus of unannotated sentences.
Ithas been conjectured in (Wetherell, 1980) that thesemethods always provide probabilistic context-freegrammars with the consistency property.
A first re-sult in this direction was presented in (Chaudhuri etal., 1983), by showing that a probabilistic context-free grammar estimated by maximizing the likeli-hood of a sample of parse trees is always consistent.In later work by (Sa?nchez and Bened?
?, 1997)and (Chi and Geman, 1998), the result was in-dependently extended to expectation maximization,which is an unsupervised method exploited to es-timate probabilistic context-free grammars by find-ing local maxima of the likelihood of a sample ofunannotated sentences.
The proof in (Sa?nchez andBened?
?, 1997) makes use of spectral analysis of ex-pectation matrices, while the proof in (Chi and Ge-man, 1998) is based on a simpler counting argument.Both these proofs assume restrictions on the un-derlying context-free grammars.
More specifically,in (Chi and Geman, 1998) empty rules and unaryrules are not allowed, thus excluding infinite ambi-guity, that is, the possibility that some string in theinput sample has an infinite number of derivations inthe grammar.
The treatment of general form context-free grammars has been an open problem so far.In this paper we consider several estimation meth-ods for probabilistic context-free grammars, and weshow that the resulting grammars have the consis-tency property.
Our proofs are applicable underthe most general conditions, and our results alsoinclude the expectation maximization method, thussolving the open problem discussed above.
We usean alternative proof technique with respect to pre-343vious work, based on an already known renormal-ization construction for probabilistic context-freegrammars, which has been used in the context oflanguage modeling.The structure of this paper is as follows.
We pro-vide some preliminary definitions in Section 2, fol-lowed in Section 3 by a brief overview of the esti-mation methods we investigate in this paper.
In Sec-tion 4 we prove some properties of a renormaliza-tion technique for probabilistic context-free gram-mars, and use this property to show our main resultsin Section 5.
Section 6 closes with some concludingremarks.2 PreliminariesIn this paper we use mostly standard notation, as forinstance in (Hopcroft and Ullman, 1979) and (Boothand Thompson, 1973), which we summarize below.A context-free grammar (CFG) is a 4-tupleG =(N,?, S,R) where N and ?
are finite disjoint setsof nonterminal and terminal symbols, respectively,S ?
N is the start symbol and R is a finite set ofrules.
Each rule has the form A ?
?, where A ?
Nand ?
?
(?
?N)?.
We write V for set ?
?N .Each CFG G is associated with a left-most de-rive relation ?, defined on triples consisting of twostrings ?, ?
?
V ?
and a rule pi ?
R. We write ?
pi?
?if and only if ?
= uA??
and ?
= u??
?, for someu ?
?
?, ??
?
V ?, and pi = (A ?
?).
A left-most derivation for G is a string d = pi1 ?
?
?pim,m ?
0, such that ?0 pi1?
?1 pi2?
?
?
?
pim?
?m, forsome ?0, .
.
.
, ?m ?
V ?
; d = ?
(where ?
denotesthe empty string) is also a left-most derivation.
Inthe remainder of this paper, we will let the termderivation always refer to left-most derivation.
If?0pi1?
?
?
?
pim?
?m for some ?0, .
.
.
, ?m ?
V ?, thenwe say that d = pi1 ?
?
?pim derives ?m from ?0 andwe write ?0 d?
?m; d = ?
derives any ?0 ?
V ?
fromitself.A (left-most) derivation d such that S d?
w,w ?
?
?, is called a complete derivation.
If d isa complete derivation, we write y(d) to denote the(unique) string w ?
??
such that S d?
w. Wedefine D(G) to be the set of all complete deriva-tions for G. The language generated by G is the setof all strings derived by complete derivations, i.e.,L(G) = {y(d) | d ?
D(G)}.
It is well-known thatthere is a one-to-one correspondence between com-plete derivations and parse trees for strings in L(G).For X ?
V and ?
?
V ?, we write f(X,?)
todenote the number of occurrences of X in ?.
For(A ?
?)
?
R and a derivation d, f(A ?
?, d)denotes the number of occurrences of A ?
?
in d.We let f(A, d) =??
f(A ?
?, d).A probabilistic CFG (PCFG) is a pair G =(G, pG), where G is a CFG and pG is a functionfrom R to real numbers in the interval [0, 1].
Wesay that G is proper if, for every A ?
N , we have?A?
?pG(A ?
?)
= 1.
(1)Function pG can be used to assign probabilities toderivations of the underlying CFG G, in the follow-ing way.
For d = pi1 ?
?
?pim ?
R?,m ?
0, we definepG(d) =m?i=1pG(pii).
(2)Note that pG(?)
= 1.
The probability of a stringw ?
??
is defined aspG(w) =?y(d)=wpG(d).
(3)A PCFG is consistent if?wpG(w) = 1.
(4)Consistency implies that the PCFG defines a proba-bility distribution over both sets D(G) and L(G).If a PCFG is proper, then consistency means thatno probability mass is lost in derivations of infinitelength.
All PCFGs in this paper are implicitly as-sumed to be proper, unless otherwise stated.3 Estimation of PCFGsIn this section we give a brief overview of some esti-mation methods for PCFGs.
These methods will belater investigated to show that they always provideconsistent PCFGs.In natural language processing applications, esti-mation of a PCFG is usually carried out on the ba-sis of a tree bank, which in this paper we assume tobe a sample, that is, a finite multiset, of completederivations.
Let D be such a sample, and let D be344the underlying set of derivations.
For d ?
D, welet f(d,D) be the multiplicity of d in D, that is, thenumber of occurrences of d in D. We definef(A ?
?,D) =?d?Df(d,D) ?
f(A ?
?, d), (5)and let f(A,D) =??
f(A ?
?,D).Consider a CFG G = (N,?,R, S) defined byall and only the nonterminals, terminals and rulesobserved in D. The criterion of maximum likeli-hood estimation (MLE) prescribes the constructionof a PCFG G = (G, pG) such that pG maximizes thelikelihood of D, defined aspG(D) =?d?DpG(d)f(d,D), (6)subject to the properness conditions??
pG(A ??)
= 1 for eachA ?
N .
The maximization problemabove has a unique solution, provided by the estima-tor (see for instance (Chi and Geman, 1998))pG(A ?
?)
=f(A ?
?,D)f(A,D) .
(7)We refer to this as the supervised MLE method.In applications in which a tree bank is not avail-able, one might still use the MLE criterion to traina PCFG in an unsupervised way, on the basis of asample of unannotated sentences, also called a cor-pus.
Let us call C such a sample and C the underly-ing set of sentences.
For w ?
C, we let f(w, C) bethe multiplicity of w in C.Assume a CFG G = (N,?,R, S) that is ableto generate all of the sentences in C, and possiblymore.
The MLE criterion prescribes the construc-tion of a PCFG G = (G, pG) such that pG maxi-mizes the likelihood of C, defined aspG(C) =?w?CpG(w)f(w,C), (8)subject to the properness conditions as in the super-vised case above.
The above maximization prob-lem provides a system of |R| nonlinear equations(see (Chi and Geman, 1998))pG(A ?
?)
=?w?C f(w, C) ?
EpG(d |w) f(A ?
?, d)?w?C f(w, C) ?
EpG(d |w) f(A, d), (9)where Ep denotes an expectation computed underdistribution p, and pG(d |w) is the probability ofderivation d conditioned by sentence w (so thatpG(d |w) > 0 only if y(d) = w).
The solution tothe above system is not unique, because of the non-linearity.
Furthermore, each solution of (9) identi-fies a point where the curve in (8) has partial deriva-tives of zero, but this does not necessarily corre-spond to a local maximum, let alne an absolutemaximum.
(A point with partial derivatives of zerothat is not a local maximum could be a local min-imum or even a so-called saddle point.)
In prac-tice, this system is typically solved by means of aniterative algorithm called inside/outside (Charniak,1993), which implements the expectation maximiza-tion (EM) method (Dempster et al, 1977).
Startingwith an initial function pG that probabilistically ex-tends G, a so-called growth transformation is com-puted, defined aspG(A ?
?)
=?w?C f(w, C)?
?y(d)=wpG(d)pG(w) ?f(A ?
?, d)?w?C f(w, C)?
?y(d)=wpG(d)pG(w) ?f(A, d).
(10)Following (Baum, 1972), one can show thatpG(C) ?
pG(C).
Thus, by iterating the growth trans-formation above, we are guaranteed to reach a localmaximum for (8), or possibly a saddle point.
Werefer to this as the unsupervised MLE method.We now discuss a third estimation method forPCFGs, which was proposed in (Corazza and Satta,2006).
This method can be viewed as a general-ization of the supervised MLE method to probabil-ity distributions defined over infinite sets of com-plete derivations.
Let D be an infinite set of com-plete derivations using nonterminal symbols in N ,start symbol S ?
N and terminal symbols in ?.We assume that the set of rules that are observedin D is drawn from some finite set R. Let pD bea probability distribution defined over D, that is,a function from set D to interval [0, 1] such that?d?D pD(d) = 1.Consider the CFG G = (N,?,R, S).
Note thatD ?
D(G).
We wish to extend G to some PCFGG = (G, pG) in such a way that pD is approxi-mated by pG (viewed as a distribution over completederivations) as well as possible according to somecriterion.
One possible criterion is minimization of345the cross-entropy between pD and pG, defined asthe expectation, under distribution pD, of the infor-mation of the derivations in D computed under dis-tribution pG, that isH(pD || pG) = EpD log1pG(d)= ?
?d?DpD(d) ?
log pG(d).
(11)We thus want to assign to the parameters pG(A ??
), A ?
?
?
R, the values that minimize (11), sub-ject to the conditions??
pG(A ?
?)
= 1 for eachA ?
N .
Note that minimization of the cross-entropyabove is equivalent to minimization of the Kullback-Leibler distance between pD and pG.
Also note thatthe likelihood of an infinite set of derivations wouldalways be zero and therefore cannot be consideredhere.The solution to the above minimization problemprovides the estimatorpG(A ?
?)
=EpD f(A ?
?, d)EpD f(A, d).
(12)A proof of this result appears in (Corazza and Satta,2006), and is briefly summarized in Appendix A,in order to make this paper self-contained.
We callthe above estimator the cross-entropy minimizationmethod.The cross-entropy minimization method can beviewed as a generalization of the supervised MLEmethod in (7), as shown in what follows.
Let D andD be defined as for the supervisedMLEmethod.
Wedefine a distribution over D aspD(d) =f(d,D)|D| .
(13)Distribution pD is usually called the empirical dis-tribution associated withD.
Applying the estimatorin (12) to pD, we obtainpG(A ?
?)
==?d?D pD(d) ?
f(A ?
?, d)?d?D pD(d) ?
f(A, d)=?d?Df(d,D)|D| ?
f(A ?
?, d)?d?Df(d,D)|D| ?
f(A, d)=?d?D f(d,D) ?
f(A ?
?, d)?d?D f(d,D) ?
f(A, d).
(14)This is the supervised MLE estimator in (7).
This re-minds us of the well-known fact that maximizing thelikelihood of a (finite) sample through a PCFG dis-tribution amounts to minimizing the cross-entropybetween the empirical distribution of the sample andthe PCFG distribution itself.4 RenormalizationIn this section we recall a renormalization techniquefor PCFGs that was used before in (Abney et al,1999), (Chi, 1999) and (Nederhof and Satta, 2003)for different purposes, and is exploited in the nextsection to prove our main results.
In the remainderof this section, we assume a fixed, not necessarilyproper PCFG G = (G, pG), with G = (N,?, S,R).We define the renormalization of G as the PCFGR(G) = (G, pR) with pR specified bypR(A ?
?)
=pG(A ?
?)
?
?d,w pG(?d?
w)?d,w pG(Ad?
w).
(15)It is not difficult to see that R(G) is a proper PCFG.We now show an important property of R(G), dis-cussed before in (Nederhof and Satta, 2003) in thecontext of so-called weighted context-free gram-mars.Lemma 1 For each derivation d with A d?
w, A ?N and w ?
?
?, we havepR(A d?
w) =pG(A d?
w)?d?,w?
pG(Ad??
w?).
(16)Proof.
The proof is by induction on the length of d,written |d|.
If |d| = 1 we must have d = (A ?
w),and thus pR(d) = pR(A ?
w).
In this case, thestatement of the lemma directly follows from (15).Assume now |d| > 1 and let pi = (A ?
?
)be the first rule used in d. Note that there mustbe at least one nonterminal symbol in ?.
We canthen write ?
as u0A1u1A2 ?
?
?uq?1Aquq, for q ?
1,Ai ?
N , 1 ?
i ?
q, and uj ?
?
?, 0 ?j ?
q.
In words, A1, .
.
.
, Aq are all of the occur-rences of nonterminals in ?, as they appear fromleft to right.
Consequently, we can write d in theform d = pi ?
d1 ?
?
?
dq for some derivations di,1 ?
i ?
q, with Ai di?
wi, |di| ?
1 and with346w = u0w1u1w2 ?
?
?uq?1wquq.
Below we use thefact that pR(uj ??
uj) = pG(uj ??
uj) = 1 foreach j with 0 ?
j ?
q, and further using the def-inition of pR and the inductive hypothesis, we canwritepR(A d?
w) == pR(A ?
?)
?q?i=1pR(Ai di?
wi)= pG(A ?
?)
??d?,w?
pG(?d??
w?)?d?,w?
pG(Ad??
w?)?
?q?i=1pR(Ai di?
wi)= pG(A ?
?)
??d?,w?
pG(?d??
w?)?d?,w?
pG(Ad??
w?)?
?q?i=1pG(Ai di?
wi)?d?,w?
pG(Aid??
w?
)= pG(A ?
?)
??d?,w?
pG(?d??
w?)?d?,w?
pG(Ad??
w?)??
?qi=1 pG(Aidi?
wi)?qi=1?d?,w?
pG(Aid??
w?
)= pG(A ?
?)
??d?,w?
pG(?d??
w?)?d?,w?
pG(Ad??
w?)??
?qi=1 pG(Aidi?
wi)?d?,w?
pG(?d??
w?
)= pG(A ?
?)
?
?qi=1 pG(Aidi?
wi)?d?,w?
pG(Ad??
w?
)?= pG(Ad?
w)?d?,w?
pG(Ad??
w?).
(17)As an easy corollary of Lemma 1, we have thatR(G) is a consistent PCFG, as we can write?d,wpR(S d?
w) ==?d,wpG(S d?
w)?d?,w?
pG(Sd??
w?
)=?d,w pG(Sd?
w)?d?,w?
pG(Sd??
w?
)= 1.
(18)5 ConsistencyIn this section we prove the main results of thispaper, namely that all of the estimation methodsdiscussed in Section 3 always provide consistentPCFGs.
We start with a technical lemma, centralto our results, showing that a PCFG that minimizesthe cross-entropy with a distribution over any set ofderivations must be consistent.Lemma 2 Let G = (G, pG) be a proper PCFGand let pD be a probability distribution defined oversome set D ?
D(G).
If G minimizes functionH(pD || pG), then G is consistent.Proof.
LetG = (N,?, S,R), and assume that G isnot consistent.
We establish a contradiction.
Since Gis not consistent, we must have?d,w pG(Sd?
w) <1.
Let then R(G) = (G, pR) be the renormalizationof G, defined as in (15).
For any derivation S d?
w,w ?
?
?, with d in D, we can use Lemma 1 andwritepR(S d?
w) == 1?d?,w?
pG(Sd??
w?)?
pG(S d?
w)> pG(S d?
w).
(19)In words, every complete derivation d in D has aprobability in R(G) that is strictly greater than inG.
But this means H(pD || pR) < H(pD || pG),against our hypothesis.
Therefore, G is consistentand pG is a probability distribution over set D(G).Thus function H(pD || pG) can be interpreted as thecross-entropy.
(Observe that in the statement of thelemma we have avoided the term ?cross-entropy?,since cross-entropies are only defined for probabilitydistributions.
)Lemma 2 directly implies that the cross-entropyminimization method in (12) always provides a con-sistent PCFG, since it minimizes cross-entropy for adistribution defined over a subset of D(G).
We havealready seen in Section 3 that the supervised MLEmethod is a special case of the cross-entropy min-imization method.
Thus we can also conclude thata PCFG trained with the supervised MLE method is347always consistent.
This provides an alternative proofof a property that was first shown in (Chaudhuri etal., 1983), as discussed in Section 1.We now prove the same result for the unsuper-vised MLE method, without any restrictive assump-tion on the rules of our CFGs.
This solves a problemthat was left open in the literature (Chi and Geman,1998); see again Section 1 for discussion.
Let C andC be defined as in Section 3.
We define the empiri-cal distribution of C aspC(w) =f(w, C)|C| .
(20)Let G = (N,?, S,R) be a CFG such that C ?L(G).
Let D(C) be the set of all complete deriva-tions for G that generate sentences in C, that is,D(C) = {d | d ?
D(G), y(d) ?
C}.Further, assume some probabilistic extension G =(G, pG) of G, such that pG(d) > 0 for every d ?D(C).
We define a distribution over D(C) bypD(C)(d) = pC(y(d)) ?pG(d)pG(y(d)).
(21)It is not difficult to verify that?d?D(C)pD(C)(d) = 1.
(22)We now apply to G the estimator in (12), in orderto obtain a new PCFG G?
= (G, p?G) that minimizesthe cross-entropy between pD(C) and p?G.
Accordingto Lemma 2, we have that G?
is a consistent PCFG.Distribution p?G is specified byp?G(A ?
?)
==?d?D(C) pD(C)(d)?f(A ?
?, d)?d?D(C) pD(C)(d)?f(A, d)=?d?D(C)f(y(d),C)|C| ?pG(d)pG(y(d)) ?f(A ?
?, d)?d?D(C)f(y(d),C)|C| ?pG(d)pG(y(d)) ?f(A, d)=?w?C f(w, C)?
?y(d)=wpG(d)pG(w) ?f(A ?
?, d)?w?C f(w, C)?
?y(d)=wpG(d)pG(w) ?f(A, d)=?w?C f(w, C)?EpG(d |w)f(A ?
?, d)?w?C f(w, C)?EpG(d |w)f(A, d).
(23)Since distribution pG was arbitrarily chosen, sub-ject to the only restriction that pG(d) > 0 for ev-ery d ?
D(C), we have that (23) is the growthestimator (10) already discussed in Section 3.
Infact, for each w ?
L(G) and d ?
D(G), we havepG(d |w) = pG(d)pG(w) .
We conclude with the desiredresult, namely that a general form PCFG obtained atany iteration of the EM method for the unsupervisedMLE is always consistent.6 ConclusionsIn this paper we have investigated a number ofmethods for the empirical estimation of probabilis-tic context-free grammars, and have shown that theresulting grammars have the so-called consistencyproperty.
This property guarantees that all the prob-ability mass of the grammar is used for the finitestrings it derives.
Thus if the grammar is used incombination with other probabilistic models, as forinstance in a speech processing system, consistencyallows us to combine or compare scores from differ-ent modules in a sound way.To obtain our results, we have used a novel prooftechnique that exploits an already known construc-tion for the renormalization of probabilistic context-free grammars.
Our proof technique seems moreintuitive than arguments previously used in the lit-erature to prove the consistency property, based oncounting arguments or on spectral analysis.
It isnot difficult to see that our proof technique canalso be used with probabilistic rewriting formalismswhose underlying derivations can be characterizedby means of context-free rewriting.
This is forinstance the case with probabilistic tree-adjoininggrammars (Schabes, 1992; Sarkar, 1998), for whichconsistency results have not yet been shown in theliterature.A Cross-entropy minimizationIn order to make this paper self-contained, we sketcha proof of the claim in Section 3 that the estimatorin (12) minimizes the cross entropy in (11).
A fullproof appears in (Corazza and Satta, 2006).Let D, pD and G = (N,?,R, S) be defined asin Section 3.
We want to find a proper PCFG G =(G, pG) such that the cross-entropy H(pD || pG) isminimal.
We use Lagrange multipliers ?A for eachA ?
N and define the form?
=?A?N?A ?
(?
?pG(A ?
?)?
1) +348?
?d?DpD(d) ?
log pG(d).
(24)We now consider all the partial derivatives of?.
Foreach A ?
N we have????A=?
?pG(A ?
?)?
1.
(25)For each (A ?
?)
?
R we have??
?pG(A ?
?
)== ?A ??
?pG(A ?
?
)?d?DpD(d) ?
log pG(d)= ?A ?
?d?DpD(d) ??
?pG(A ?
?
)log pG(d)= ?A ?
?d?DpD(d) ??
?pG(A ?
?)log?(B??
)?RpG(B ?
?)f(B?
?,d)= ?A ?
?d?DpD(d) ??
?pG(A ?
?)?(B??
)?Rf(B ?
?, d) ?
log pG(B ?
?
)= ?A ?
?d?DpD(d) ??(B??
)?Rf(B ?
?, d) ??
?pG(A ?
?
)log pG(B ?
?
)= ?A ?
?d?DpD(d) ?
f(A ?
?, d) ??
1ln(2) ?1pG(A ?
?
)= ?A ?1ln(2) ?1pG(A ?
?)??
?d?DpD(d) ?
f(A ?
?, d)= ?A ?1ln(2) ?1pG(A ?
?)??
EpD f(A ?
?, d).
(26)By setting to zero all of the above partial derivatives,we obtain a system of |N |+|R| equations, which wemust solve.
From ???pG(A??)
= 0 we obtain?A ?
ln(2) ?
pG(A ?
?)
=EpDf(A ?
?, d).
(27)We sum over all strings ?
such that (A ?
?)
?
R,deriving?A ?
ln(2) ??
?pG(A ?
?)
==?
?EpD f(A ?
?, d)=??
?d?DpD(d) ?
f(A ?
?, d)=?d?DpD(d) ??
?f(A ?
?, d)=?d?DpD(d) ?
f(A, d)= EpD f(A, d).
(28)From each equation ???
?A = 0 we obtain??
pG(A ?
?)
= 1 for each A ?
N (our originalconstraints).
Combining this with (28) we obtain?A ?
ln(2) = EpD f(A, d).
(29)Replacing (29) into (27) we obtain, for every rule(A ?
?)
?
R,pG(A ?
?)
=EpD f(A ?
?, d)EpD f(A, d).
(30)This is the estimator introduced in Section 3.ReferencesS.
Abney, D. McAllester, and F. Pereira.
1999.
Relatingprobabilistic grammars and automata.
In 37th AnnualMeeting of the Association for Computational Linguis-tics, Proceedings of the Conference, pages 542?549,Maryland, USA, June.L.
E. Baum.
1972.
An inequality and associated max-imization technique in statistical estimations of prob-abilistic functions of Markov processes.
Inequalities,3:1?8.T.L.
Booth and R.A. Thompson.
1973.
Applying prob-abilistic measures to abstract languages.
IEEE Trans-actions on Computers, C-22(5):442?450, May.E.
Charniak.
1993.
Statistical Language Learning.
MITPress.R.
Chaudhuri, S. Pham, and O. N. Garcia.
1983.
Solutionof an open problem on probabilistic grammars.
IEEETransactions on Computers, 32(8):748?750.Z.
Chi and S. Geman.
1998.
Estimation of probabilis-tic context-free grammars.
Computational Linguistics,24(2):299?305.349Z.
Chi.
1999.
Statistical properties of probabilisticcontext-free grammars.
Computational Linguistics,25(1):131?160.A.
Corazza and G. Satta.
2006.
Cross-entropy and es-timation of probabilistic context-free grammars.
InProc.
of HLT/NAACL 2006 Conference (this volume),New York.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society, B,39:1?38.J.E.
Hopcroft and J.D.
Ullman.
1979.
Introductionto Automata Theory, Languages, and Computation.Addison-Wesley.M.-J.
Nederhof and G. Satta.
2003.
Probabilistic pars-ing as intersection.
In 8th International Workshop onParsing Technologies, pages 137?148, LORIA, Nancy,France, April.J.-A.
Sa?nchez and J.-M.
Bened??.
1997.
Consistencyof stochastic context-free grammars from probabilis-tic estimation based on growth transformations.
IEEETransactions on Pattern Analysis and Machine Intelli-gence, 19(9):1052?1055, September.A.
Sarkar.
1998.
Conditions on consistency of proba-bilistic tree adjoining grammars.
In Proc.
of the 36thACL, pages 1164?1170, Montreal, Canada.Y.
Schabes.
1992.
Stochastic lexicalized tree-adjoininggrammars.
In Proc.
of the 14th COLING, pages 426?432, Nantes, France.C.
S. Wetherell.
1980.
Probabilistic languages: A re-view and some open questions.
Computing Surveys,12(4):361?379.350
