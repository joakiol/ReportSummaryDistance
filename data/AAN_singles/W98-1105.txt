Semantic Tagging using a Probabilistic Context Free Grammar *Michae l  Co l l insDept .
o f  Computer  and  In format ion  Sc ienceUn ivers i ty  o f  Pennsy lvan ia200 South  33rd  Street ,  Ph i lade lph ia ,  PA 19104mcol  l ins@grad ient ,  cis.
upenn,  eduScot t  M i l le rBBN Techno log ies70  Fawcet t  S t reetCambr idge ,  MA 02138s zmi l le r@bbn,  comAbst ractThis paper describes a statistical model for extraction ofevents at the sentence level, or "semantic tagging", typi-cally the first level of processing in Information Extrac-tion systems.
We illustrate the approach using a manage-ment succession task, tagging sentences with three slotsinvolved in each succession event: the post, person com-ing into the post, and person leaving the post.
The ap-proach requires very limited resources: a part-of-speechtagger; a morphological nalyzer; and a set of trainingexamples that have been labeled with the three slots andthe indicator (verb or noun) used to express the event.Training on 560 sentences, and testing on 356 sentences,shows the accuracy of the approach is 77.5% (if partialslot matches are deemed incorrect) or 87.8% (if partialslot matches are deemed correct).1 I n t roduct ionStatistical models have been used quite successfully innatural language processing for recovery of hidden struc-ture such as part-of-speech tags, or syntactic structure.This paper considers emantic tagging of text withinthe context of information extraction, as in the SixthMessage Understanding Conference (MUC-6).
MUC-6looked at extraction of events concerning managementsuccessions in newspaper texts: recovering the post,company, person entering and person leaving the post.We will concentrate on the initial stage of processing,extraction of events at the sentence level.
For example,given the sentenceLast week Hensley West,  59 years o ld ,  wasnamed as president, a surprising development.the desired output from the system would be{IN = Hensley West, POST = president, IND = named }POST is a slot designating the title of the position, INis the person coming in to fill the post, and IND is an"indicator" - usually a verb or a noun - used to expressthe event.
Table 1 gives some more examples.The traditional approach to this problem, as exempli-fied in SRI's FASTUS system (Appelt et al 93), has been" The work repotted here was supported in part by the Defense Ad-vanced Research Projects Agency.
Technical gents for part of thiswork were Fort Huachucha under contract number DABT63-94-C-0062.
The views and conclusions contained in this document are thoseof the authors and should not be interpreted asnecessarily represent-ing the official policies, either expressed or implied, of the DefenseAdvanced Research Projects Agency or the United States Government.
(IN Jack Bradley) was (IND named) (POST acting president and chiefexecutive officer) of this computer-network manufacturer.
(IN He) (IND succeeds) as(POST president) (OUT Edward Marinaro)?
56, who is retiring.
(IN Mr. Stanley) 's(IND appointment) comes as AK Steel's Mid-dletown Works is under investigation byOSHA because of its safetyrecord, which includes one accident that killed four men in April 1994~Fhe unexpected (IND departure) of(OUT Citicorp's highly regardedhead of retail banking) and the appointment of a tobacco executive tofill his shoes has caught most Citicorp employees off-guard and con"founded many analysts.On Friday, the bank said (OUT Pei-yuan Chia), head of retail banking?
will (IND retire) this year.Table 1 : Some example sentences.to use hand-coded rules.
These are typically encoded in aseries of finite-state transducers that progressively buildinformation in a bottom-up fashion.
We are interested indeveloping a machine-learning approach to this problemfor two reasons: First, developing hand-coded rules is alengthy task which requires a fairly considerable amountof expertise - and a new set of rules must be developedfor each new domain.
Annotating training text examplessuch as those in table 1 can conceivably be done by anon-expert.
Second, writing accurate rules is difficult, asthere are many complex interactions between the rules,and there are many details to be covered.
This task be-comes even more complex when the interaction betweenthe sentence-level rules and the later stages of process-ing (co-reference and merging, see section 1.1) is con-sidered.
Machine learning techniques have been shownto be highly effective at managing this kind of complex-ity in applications such as speech recognition, part-of-speech tagging and parsing.Not surprisingly this problem can be approached us-ing finite-state tagging methods, which have previouslybeen applied to part of speech tagging (Church 88) andnamed-entity identification (Bikel et al 97).
We initiallyconsider this approach, but argue that the Markov ap-proximation gives an extremely bad parameterization fthe problem.
Instead, the method uses a ProbabilisticContext Free Grammar (PCFG), which has the advan-tages of being flexible enough to allow a good parameter-ization of the problem, while having an efficient decod-ing algorithm, a variant of the CKY dynamic program-ming algorithm for parsing with context-free grammars.The PCFG does not  encode linguistic phrase structure;38rather, it is semantically motivated, modeling choicessuch as the choice Of indicator, number of slots, fillersfor the slots, and generation of other "noise" words inthe sentence.While this paper largely concentrates on the man-agement succession domain, and motivates many of thechoices regarding representation with examples from it,the principles hould be general enough to also work forother domains - -  in fact, the method was originally de:veloped for an IE task involving company acquisitions(identifying the buyer, seller and item being bought), andthen moved to the management succession domain withno domain-specific tuning.1.1 The Complete Information Extraction TaskWhile a semantic tagging model may be useful for manytasks, our primary motivation is to use it within an in-formation extraction (IE) system.
Information extrac-tion tasks involve processing an input text to fill slotsin an output template, the DARPA-sponsored MessageUnderstanding Conferences (MUCs) have evaluated IEsystems from several research sites.
Some previous tasksattempted at MUC involved extraction of informationabout terrorist attacks (MUC-4); joint ventures (MUC-5); and most recently, at MUC-6, management succes-sions.
In this paper we concentrate on the managementsuccession task, figure 1 gives an example input-outputpair from the domain 1.
(a) Who's News: Restor Industries Inc.RESTOR INDUSTRIES Inc. (Orlando, Fla.) --Hensley E. West, 50 years old, was namedpresident of this telecommunicat ions-productconcern.
Mr. West, who most recently was agroup vice president for DSC CommunicationsCorp.
in Dallas, fills a vacancy created bythe retirement last September of JohnBradley, 63.
(b)EventNumberiSlot , FillerINOUTPOSTCOMPANYOUTPOSTCOMPANYHensley E. WestJohn BradleypresidentRESTOR INDUSTRIES Inc.Hensley E. Westgroup vice presidentDSC Communications Corp.Figure h (a) A sample text from WSJ, involving man-agement successions.
(b) The two succession events inthe text.Most systems described in the MUC-6 proceedingsfollowed the following three stages of processing inmap-ping an input text to a set of output emplates:I We've just shown the most important, "core" slots for tie task -the MUC-6 specification i cludes additional information such as thereason for the change, the title of the people involved etc.391) Pattern matching at the sentence level.
This is thetask that is approached in this paper.
In the text in fig-ure 1, "Hensley E. West, 50 years old, was named presi-dent of this telecommunications-product concern" wouldbe processed to give { IN = "Hensley E. West", POST= "'president", COMPANY = "this telecommunications-product concern", VERB = "named" } and ".... the re-tirement last September of John Bradley .... " would give{ OUT = "John Bradley", NOUN = "retirement", IND= "resignation"}2) Coreference.
Pronouns and definite NPs arcresolved to their antecedents.
For example, "thistelecommunications-product concern" would be re-solved to "RESTOR INDUSTRIES Inc.".
This stage isimportant because pronouns and definite noun-phraseslike "this telecommunications-product concern" are notinformative slot-fillers.3) Merging.
The information in a template may bespread across everal sentences.
In the merging stage theinformation from multiple mentions of the same event ismerged into a single template.
In the example, the infor-mation centered around "named" and "retirement" wouldbe identified as referring to the same event, and wouldbe combined to give { IN = "Hensley E. West", OUT= "John Bradley", POST = "president", COMPANY ="RESTOR INDUSTRIES Inc." }This paper's work attacks problem (1) alone, and isrestricted to recovery of the IN, OUT and POST slots.1.2 Previous WorkThe majority of systems at MUC-6, including SRI's sys-tem FASTUS (Appelt et al 93), and the best-performingsy.stem, from NYU (Grishman 95), used cascaded finite-state transducers, which were built by hand.
The domainindependent transducers tokenize the text, recognise per-son and company names, "chunk" noun and verb groups,and finally build some higher level, complete clauses.The domain specific rules then extract he slots from thesentence, using patterns uch as the example on page 244of (Appelt et al 93): "Company hires or recruits personfrom company as position".There have been a number of machine learning ap-proaches to the sentence-level stage of information ex-traction.
The AutoSiog system (Riloff 93; Riloff 96)automatically earned "concept node" definitions for useon the MUC-4 terrorist events domain.
A concept nodespecifies a trigger word, usually a verb, and maps syn-tactic roles with respect o this trigger to semantic slots- for example, a concept node might specify/f  trigger= "destroyed" and syntax = direct-object then concept= Damaged-Object (Damaged-object is the name of theslot in this case).
A concept node may also specify hardor soft constraints on the slot-fillers.
The system usesthe CIRCUS parser (Lehnert et al 93) to find the syn-tactic roles in relation to the trigger.
AutoSlog learnsconcept nodes given input-output pairs like those in fig-ure 1, so the indicator words do not need to be specified.Experiments showed that running AutoSlog followed by5 hours of filtering the rules by hand gave a system thatperformed as well as a hand-crafted system.The CRYSTAL system (Soderland et al 9.
'i) alsolearns rules that map syntactic frames to semantic roles.The triggers can be more complicated than those in Au-toSlog, in that they can specify whole sequences ofwords, or restrict patterns by specifying words or classesin the surrounding context.
CRYSTAL learns patternsby initially specifying a maximally detailed pattern foreach training example, then progressively simplifyingand merging patterns until some error bound is exceeded.CRYSTAL uses the BADGER sentence analyzer to givesyntactic information.
(Califf and Mooney 97) describe a system for extrac-tion of information about job posfings from a newsgroup.Relational learning is used to learn rule-based patternsthat specify: 1) a pre-filler pattern that matches the textbefore the slot; 2) a pattern that must match the actualslot filler; and 3) a post-filler pattern that matches the textafter the slot.
The patterns can involve parts of speech,semantic lasses of words, or the words themselves.
Anexample pattern from (Califf and Mooney 97) for identi-fying locations is pre-filler = in, filler = 2 or fewer wordsall proper nouns, post-filler = wordl is ",", word2 is astate.
This matches phrases like "in Kansas City, Mis-souri" or "in Atlanta, Georgia".
The learning algorithmstarts with the most specific rule for each training exam-ple, then generalizes by merging similar ules.A major difference between the approaches describedin (Riloff 93; Riloff 96; Soderland et al 95) and the ap-proach in this paper is that (Riloff 93; Riloff 96; Soder-land et al 95) rely on a syntactic parser to produce at leasta shallow syntactic analysis.
The approach described inthis paper builds a system from a set of training exam-ples, with only a part-of-speech tagger and a morpho-logical analyzer as additional resources.
The system in(Califf and Mooney 97) does not require a parser, but thepatterns it uses are quite local (the pre-filler and post-filler patterns are adjacent to the slot).
It isn't clear thismethod would work well for the management succes-sions domain where there are often many "noise" wordsbetween the slots and the indicator.
Another major dif-ference between the methods is that the PCFG basedmethod is probabilisfic.
This may be an advantage whenthe sentence-level stage of processing is combined withthe later merging and coreference stages, as it gives aprincipled way of combining evidence from the differ-ent stages of processing: an uncertainty at the sentencelevel may, for example, be resolved at the merging stage- -  in this case it is useful for the sentence level systemto be capable of giving a list of candidate analyses withassociated probabilities.2 Background2.1 The ProblemWe assume the following definitions:401.
A sentence, W, consists of n words, 'tU1, LU2, ...W n.2.
A template, T, is a non-empty set of slots, whereeach slot is a label together with a tuple giving thestart and end point of the slot in the sentence.
Forexample, T = {IN = (3, 4), OUT = (5, 6)} meansthere is an IN slot spanning words 3 to 4 inclusive,and an OUT slot spanning words 5 to 6 inclusive.In the management succession domain there arethree possible slots, IN, OUT and POST (abbrevi-ated to I, O and P respectively).
IN is the stringdenoting the person who is filling the post, OUT isthe person who is leaving the post, and POST is thename of the post.3.
In addition, we assume that each template containsan additional indicator slot, which is the verb ornoun used to express the template.For example, a (W, T) pair might beW = Last week Hensley West, 59 years old, joinedthe company as president, a surprising development.T = {IN = (3,4),POST = (14, 14), IND = (I0, I0)}As alternative notation in this paper we either list thestrings in the template, for example T = {IN = "Hens-ley West", POST = "president", IND = "joined"}, or weshow the (W, T) pair as a bracketed sentence:Last week (IN Hensley West), 59 years old,(INDjoined) the company as (POST president), a surprising development.Table 1 shows more examples from the management suc-cession domain.
The machine learning task is to learn afunction that maps an arbitrary sentence W to a templateT, given a training set of N pairs (Wi, Ti) 1 < i < N.A test set of (W, T) pairs is used to evaluate the model.In addition to a training set, we assume the followingresources:1.
A part of speech (POS) tagger.
The POS tagger de-scribed in (Ratnaparkhi 96) was used to tag bothtraining and test data.2.
A lexicon which maps each indicator word in train-ing data to a class, for example the morphologi-cal variants "join", "joins", "joined" and "joining"could all be mapped to the JOIN class.
This can bedone automatically by a morphological nalyzer asin (Karp et al 94), or by hand.
(This resource isnot strictly necessary, but will help to reduce sparsedata problems).A probabilistic approach defines a conditional proba-bility P(T  I W)  or a joint probability P(T ,  W)  for everycandidate template for a sentence.
The most likely tem-plate for a sentence W is thenTb,a = argmTaxe(T \]W) = argrr~axe(T,W) (1)The major part of this paper will be concerned with for-malizing a stochastic model that defines P(T, W).3 A Probabilistic Model3.1 A Naive Approach m Finite State TaggingIt is useful to note that a (W, T) pair can be representedas a tagged sentence wl/t l ,  w2/t2, ...w,/tn where T =tl, t2...tn is the sequence of tags denoting the semantictype for each word in the sentence.
For example, the tagscould be I, O, P, IND, for the 3 slots and the indicator,and N for other (noise) words, as inLast/N week/N Hensley/I West/I ,/N 59/Nyears/N old/N ,/N joined/IND the/N com-pany/N as/N president/P ,/N a/N surprising/Ndevelopment/N ./NAs a straw man we consider using a standard bigramtagging model to tag test-set sentences.
(Church 88) usedthis to recover part-of-speech tags, a related approach de-scribed in (Bikel et al 97) gives a useful decompositionof P(T, W) into two terms:P(T,W) = P(L1,L2,...Lm) ?
H P(WilLi) (2)i= l...rn{L1, L~, ...L,~ } is the underlying sequence of tags, in theabove example m = 7 and the sequence is {N, I, N, lIND,N ,  P, N}.
Wi is the string of words under label Li, forexample W1 = {Last, week}, W2 = {Hensley, West}.The two terms are then simplified, using bigram Markovindependence assumptions, to beP(L1,L2,...Lm) = P(L1\]Start)P(End l nm)x r I  P(L i IL i -1)  (3)i=2.. .rnand (if label Li covers words wsi...wei)P(Wi \ [L i )  = P (ws i ,  wsi+l ,  ...wei\[Li) =P(wsi I Start, Li)P(End I w~i, Li)X H P(wj I Wj-l, Li) (4)j=si+l...eiThis finite state approach as been highly effective forpart of speech tagging (Church 88) and name finding(Bikel et al 97).
However, the next section considersthe characteristics of the task in more detail, and arguesthat a finite-state tagger is a poor model for the task.3.2 More about the taskIn developing an intuition for the task, and motivatingthe choices made in modeling it, it is useful to considerthe types of information that may be useful to a system.Consider the following 5 points:I.
There are 7 possible templates corresponding to the7 non-empty subsets of {I,O,P}.
The distributionover these alternatives i by no means uniform - seetable 2 for the distribution.412.
The different slots tend to contain quite differentlexical items or strings - for example, the IN andOUT slots are most likely to contain a proper nameor a personal pronoun, whereas the POST slot con-tains strings such as "president", chairman" etc.3.
The choice of indicator word depends greatly on thechoice of template.
For example "name" is verylikely to be used to express an event involving a{I, P} template; "succeed" is very likely to expressan {I, O, P} or {I, O} template.
See the final columnof table 2 for more examples.4.
The relative order of the slots and indicator in thetext varies considerably depending on the choice ofindicator.
For example, given the template {I} andthe verb "join" the order is most likely to be { IIndicator } (e.g.
IN joined the company); whereasgiven the verb "hire" the order is usually {IndicatorI} (e.g.
the company hired IN).5.
In addition to the central indicator, there are oftensecondary indicators - mainly prepositions - whichare strong signals of particular slots.
For example,given the verb is "named" or "succeeded", the postis very likely to be preceded by the preposition "as"(e.g., the company named her as president, he suc-ceeds Jim Smith as president).By considering points 1-5 we can see that the finite-state tagging approach is deficient for the semantic tag-ging task.
The lexical probabilities in equation (4) areprobably sufficient o capture the lexical differences be-tween different states (the preference of the IN slot togenerate proper names, of the POST slot to generatewords like "president" and so on).
But the Markov ap-proximation in equation (3) is deficient in many ways: itfails to capture the non-uniform distribution over the 7possible templates, worse still it is deficient in that it canlabel more than one substring with the same slot label;it fails to capture the dependence of the slot order on theindicator word, or the dependence between the templateand indicator.3.3 A Probabilistic Context-Free GrammarOur proposal is to replace the Markov assumption in(3) with a probabilistic ontext-free grammar, that is weassume that the label sequence has been generated bythe application of r context-free rules LHSj =~ RHSj1 _< j _< r (LHS stands for left hand side, RHS standsfor fight hand side), and thatP(L1L2....Lm) = H P(RHSi l LHSi).
(5)i= l .
.
rEach LHS is a single non-terminal, and RHS is a stringof one or more non-terminals.
So for each non-terminalLHS in the grammar there is a distribution over possibleRHSs which sums to one.
Counts of context-free rulescan be extracted from a training set of context free trees,Template %age \[ Typical example{I,P} ,~5.6 IN was named as POST{O} 18.6 OUT retired{I,O} 16.6 i IN succeeds OUT{I} 7.3 IN joined the company{O,P} 7.1 OUT resigned as POST{I,O,P} 3.5 IN succeeded OUT as POST{P} 1.3 the company hired POSTMost frequent indicator/percentagename/67%retire/26%succeed/98%j oin/31%resign/46%succeed/100%Table 2: The distribution over the possible templates (the 7 non-empty subsets of {I,O,P}), and the most commonindicator for each template.
For example, 45.6% of the templates are {I,P}, and in these 45.6% of the cases "name" ischosen 67% of the time.and used to estimate the parameters P(RHSi I LHSi).Given a test data sentence, the most likely tree (and hencethe most likely template) can be recovered efficiently us-ing a variant of the CKY algorithm.4 The  GrammarThis section describes the underlying context-freestructure 2 that we assume has generated the labels, andmotivates it in terms of the observations in section 3.2.The context-free structure (the tree topology, and thechoice of non-terminal labels within the tree), is deter-ministically derived from the initial labeling of the sen-tences - -  so given a set of labeled sentences, the context-free structures can be recovered and the parameters canbe estimated.4.1 The Leaf CategoriesThe tagging model as applied in the above example as-sumed five tags - for the IN, OUT, and POST slots, theindicator, and for noise (other words).
In fact, we usedrather more categories, which are listed in table 3.
Theselabels can still be deterministically recovered from the la-beled sentence though, given the additional informationof a mapping from indicator words to their morphologi-cal stem (for example, the mapping "joined" ~ JOIN).The example sentence would have the following under-lying leaf labels:\[PREN Last week \] \[I Hensl~IK West \]\[NOISE+, 59 years old, \] \[IND(JOIN)joined\] \[NOISE- the company \]\[P.Prep-(JOIN) as \]\[P president \]\[POSTN, a surprising develop-ment.
\]4.2 The Context-Free Component -  a Brief SketchThe PCFG model assumes the pre-terminal 3 abel se-quence {Li, L2...Lm} has been generated by a stochas-tic process with the following steps:2The structures in this paper are non-recursive, and could, there-fore, be equivalently handled by a hierarchy of finite-state transduc-ers, or even a single equivalent non-deterministic finite-state automa-ton.
However, it is quite possible that extensions to the models couldrequire recursive structures.SBy pre-terminal, we mean a non-terminal that dominates wordsrather than other non-terminals.1.
Decide whether to have noise words (PREN) beforethe template TEMR2.
Decide whether to have noise words (POSTN) afterthe template TEMP.3.
Decide which slots to have (one of the 7 subsets of{I, O, P}).4.
Decide the class of indicator words.5.
Decide the order of the slots and indicator word.6.
For each slot, choose whether to have noise betweenit and the indicator (NOISE+ or NOISE-).7.
For each slot, choose whether to have a prepositiondirectly preceding or following it.Figure 2 gives an example tree and describes thecontext-free rules within it.
The next section describesthe grammar in more detail, showing how these 7 typesof decision can be encoded as context-free rules.4.3 The Context-Free Component in DetailThis section describes the top-down derivation of a se-quence of leaves within a PCFG framework.Choosing noise at the start/end of the sentenceThis level of the model chooses whether to have noisepreceding or following the text which expresses the suc-cession information.TOP -> PREN TEMPI  #there  is no ise  at  s ta r t-> TEMPI  #or there  i sn ' tTEMPi  -> TEMP POSTN #there  is no ise  fo l low ing-> TEMP #or there  i sn ' tThe TEMP non-terminal covers the span of the succes-sion information, in the above xample "Hensley ... pres-ident".
P(PREN TEMPI I TOP) can be interpreted asthe probability of having noise at the beginning of thesentence, P(TEMP POSTN \[ TEMPI) is the probabilityof having post-noise.P(Slots)TEMP first re-writes in one of seven ways, correspond-ing to the 7 possible templates.
The T. non-terminalencodes the slots that will be generated below it, for ex-ample T. IO would generate an IN and an OUT slot be-low it.
So P(RHS \[ TEMP) will mirror the distributionin column 2 of table 2.42Leaf labelI ,O,PPRENPOSTNNOISE-DescriptionThe IN, OUT and POST non-terminals"Noise" words before the template"Noise" words after the template"Noise" between slots and the indicator, which comes before the indicator"Noise" between slots and the indicator, which comes after the indicatorlND(class) Leaf dominating the indicator, class can be any one of the morphological stems een in training data.For example, IND(join) could dominate "join", "joins", "joined" or "joining".l.Prep-(class) Prepositions for the I, O and P slots, for a particular class, and which follow the indicator.
For example,O.Prep-(class) P.Prep-(join) would be a preposition for the Post slot, with an indicator in the join class, and wouldP.Prep-(class) most likely be "as"I.Prep+(class) Prepositions for the 1, O and P slots, for a particular class, and which precede the indicator.O.Prep+(class)P.Prep+(class)Table 3: The pre-terminal labels that are used in the system.TopLu ,  - - .~k  H .
.
.~ .y  w. .
,  .
~9  y .
.
.
.
o ld .
j o* .~ ~ .
comp.~.y  ~ F .
.~ .
i~ .
,  .
- .~ .~.~*~-a  do~lo~.
.
, .Rule InterpretationTOP ~ PREN TEMPI Choose to have pre-noise (PREN)TEMP 1 ~ TEMP POSTN Choose to have post-noise (POSTN)TEMP ~ T.IP Choose to have IN and POST slotsT.IP ~ IND.IP(JO1N) Choose to use a member of the JOIN class of indicatorsIND.1P(JOIN) ~ IND.I(JOIN) P2-(JOIN) Generate the POST slot to the fight of the indicatorIND.I(JOIN) =~- 12+(JOIN) IND(JOIN) Generate the IN slot to the left of the indicator12+(JOIN) ~ I I +(JOIN) NOISE+ Have noise between the IN slot and the indicatorII+(JOIN) ~ I Choose not to have a preposition for the IN slotP2-(JO1N) ~ NOISE- PI-(JOIN) Have noise between the POST slot and the indicatorPI-(JOIN) ~ P.Prep-(JOIN) P Choose to have a preposition attached to the POST slotFigure 2: An example context-free tree.
The table shows the interpretation f each of the rules in the tree.TEMP -> T .
IOP  TEMP -> T .
IO  TEMP -> T. ITEMP -> T .
IP  TEMP -> T .OTEMP -> T .OP  TFEMP-> T.PP(ClasslSlots )The next step is to choose the Class of indicator thatis used to express the transaction.
Each Class is a setof words with the same morphological stem, for exam-ple the JOIN class would include join, joins, joined andjoining.
P(ClasslSlots ) is implemented in the CFGfragment shown below.
The IND non-terminal encodeswhich slots need to be generated, and the Class used toexpress the transaction.
Each T. rule can re-write in Nways, where N is the number of classes.T .
IOP  -> IND.
IOP\ [C lass \ ]  T .
I  -> IND.
I \ [C lass \ ]T .
IO  -> IND.
IO\ [C lass \ ]  T .O  -> IND.O\ [C lass \ ]T .
IP  -> IND.IPICiassi :  T .P  -> IND.P \ [C lass \ ]T .OP  -> IND.
OP !Class\]P( OrderlClass , Slots)Having chosen the Slots to be generated, and the Classused to express the event, there are many possible ordersin which the slots and class can appear.
In the above ex-ample (Slots = {I, P}, Class = JOIN) there are 6 permuta-tions ( {JOIN I P}, {I JOIN P}, {I P JOIN } and so on).
Itis necessary to estimate a distribution over these alterna-tives.
The order is parameterized using a binary branch-ing, context-free fragment: part of this (all rules withLHS = IND.
IP \ [C lass \ ]  ) is shown below.
Thefullgrammar specifies imilar ules for all IND.
X \ [C lass  \]where X is any one of the non-empty subsets of {I, O,P}.IND.
IP IC iass \ ]  -> IND.
I \ [C lass \ ]  P2 - \ [C lass \ ]IND.
IP \ [C lass l  -> P2+lClass\ ]  IND.
I \ [C lass \ ]IND.
IP\[Class\]  -> IND.P\ [C lass \ ]  I2 - lC lass \ ]IND.
IP \ [C iass \ ]  -> 12+\[Class\ ]  IND.P \ [C lass \ ]43The notation is:?
IND keeps tracks of which slots still need to begenerated.
For example IND.IP\[Class\] meansthat the IN and POST slots need to be generated.?
The I2, 02, and P2 non-terminals will eventuallygenerate the IN, OUT and POST leaves.
The "2"stands for level 2 - more in the next section on whythis is necessary.
"+" means the slot appears be-fore the head-word, "-" means it appears after.
TheClass is propagated to the I2, 02  and P2 non-terminals.
Propagation of the Class and direction(+ or -) is important because the identity of anyprepositions i conditioned on this information.Each binary rule expresses a choice of which of theremaining slots to generate next, and which direction togenerate it in.
So IND.IP\[Class\] can re-write in 4 ways:either the IN or POST slot can be generated either to theleft or right of the head-word itself.Choosing to generate noise between the slotsNoise can appear after any slot preceding the indicator,or before any slot following the indicator.
The CFG rulesbelow encode the decision to have noise in a gap or not,for an IN slot generated before or after the indicator.
Therules for OUT and POST are similar.I2+\[Class\]  -> I i+\[Class\]  NOISE+I2?\[Class\]  -> I I+\[Class\]I2- \ [Class\]  -> NOISE-  I f - \ [C lass\ ]I2- \ [Class\]  -> I I - \ [Class\]Choosing to generate a preposition (or otherindicator) linked to a slotAny of the slots can have an adjacent "indicator", usu-ally a preposition.
The rules below encode the binarydecision of whether to include an indicator for an IN slot- the OUT and POST cases are similar.I I+\[Class\]  -> I I .P rep+\[C lass \ ]I I+\[Class\]  -> II i - \ [C lass\]  -> I .Prep- \ [C lass \ ]  II i - \ [Class\]  -> IAgain, for each I1, O1 or P1 non-terminal there aretwo possible re-writes, one binary, one unary, encodingwhether or not to generate a preposition.
The I.Prep,O.Prep and P.Prep non-terminals then generate the in-dicator with a bigram model.
The non-terminal encodeswhether the slot appears before or after the head-wordC+" or 'v ' ) ,  and the Class of the head-word.5 Train ing the ModelThere are two steps to training the model: first, recov-ering the underlying tree structure from the training datalabels; second, deriving counts of the CF rule applica-tions and bigram sequences and using these to estimatethe parameters of the model.445.1 Deriving the Tree Structures in Training DataWhile the tree structure described in section 4.3 mayseem complex, it is important o realise that it can bedeterministically derived from an annotator's labeling ofthe Slots and Indicator.
This section describes how thestructure is derived in a bottom up fashion using the fol-lowing annotated sentence as example input to the pro-cess:Last week \[I Hensley West \] , 59 years old ,\[IND joined \] the company as \[P president \] ,  asurprising development.The 6 stages are as follows:1.
Identify the class of the indicator, and add this infor-mation to the IND label.
Mark any prepositions ad-jacent to the slots.
Label "noise" words with eitherPREN, POSTN, NOISE+ or NOISE-.
The outputfrom this stage would be:\[PREN Last week \] \[I Hensley West \] \[NOISE+, 59years old, \] \[IND(JOIN) joined \] \[NOISE- the com-pany \] \[P.Prep-(JOIN) as \] \[P president \]\[POSTN,a surprising development.
\]2.
Build level I of the slots, by including attachedprepositions, or just building a unary ruleII+l)OlN) PI-OOl/q) '!| ~FrcpqJO~ pXe'n~Jey wm I IFre~.ident3.
Build level 2 of the slots, by attaching NOISE+ orNOISE- leaves to the slots,12.H JOIN) P2~JOIN)| | NOISE- PI.4JOIN)\[ .59 ycarJ old.
lI d~e compla~y P.Pr~JOIN) PI.knta~ w~a l Iu pr~itlcm4.
Build the binary-branching context-free structurethat defines the order of the slots and indicator.fiqD(IOIN)JPI2+OOIN) II41;~JOIN).PIN~/OI/~ P2~JOIN)5.
Add the top level of the treeTOP,"flEMP ~)$TNI IT.IP .
I susFmill I deve~opmlml.In'/DOOINklP5.2 Context Free Rule ProbabilitiesOnce the training data is processed to have full context-free trees, the grammar can be automatically read fromthese trees, and event counts can be extracted and usedto estimate the parameters of the model.
The maximumlikelihood estimate for a CF rule LHS -> RHS isP(RHSiLHS) = C(RHS, LHS)C(LHS)where C(z) is the number of times event z has been seenin training data.
This estimate can be unreliable, partic-ularly for low values of C(LHS).
So we smooth thisestimate with a "backed off" estimate PbP(RHSILHS) = A C(RHS, LHS) C(LHS) + (i - A)Pbwhere 0 < A < 1.
The backed off estimate Pb =C(RHS,L~FSb) is based on a subset of the context and the C(LHSb)estimate is more robust but is less detailed.
For example,Pb for P(T.IOP --+ IND.IOP\[Class\]IT.IOP) might beP (T  ---> IND \[Class\]IT), i.e.
an estimate that ignoresthe slots when choosing the class of indicators.
Thismethod borrows heavily from smoothing techniques inlanguage modeling for speech recognition m (Jelinek90) describes methods for estimating A.5.3 Bigram ProbabilitiesThe bigram model is used at the leaves of the treeto generate the words themselves, for example to es-timate P ( the  pres ident  I P)-  The most obvi-ous way to estimate this is as P(theISTART, P)* P(presidentlthe , P) ?
P( E N DIpresident, P) withsmoothing being implemented by interpolation betweenP(wlw-x, State) --r P(wlState) ~ ~ where V is thevocabulary size.
Unfortunately we do not have space to?
go into the full details of the smoothing here (in the fi-nal implementation part-of-speech information was alsoused to smooth the estimates).6 Exper imentsThis section describes experiments on the managementsuccessions domain.
Before giving the results, we dis-cuss how to deal with sentences that have more than oneindicator.6.1 Dealing with Sentences that have more thanone IndicatorThus far the model has assumed that there is only oneindicator per sentence.
However, training data frequentlyhas more than one indicator, as inMr.
Smith was named president of the com-pany, succeeding Fred Jones.There are two events in this sentence, one centeredaround named, the other centered around succeeding.The solution is to transform sentences in both trainingand test data to give one sentence per indicator, in thiscase the sentence would be expanded to give two sen-tences:Mr. Smith was *named* president of the com-pany, succeeding Fred Jones.Mr.
Smith was named president of the com-pany, *succeeding* Fred Jones.45The first sentence is for the named event, the second isfor succeeding.
The indicator is replaced with *indica-tor* to show that it is under interest - -  when decodingtest data the model either ecognises *named* as a poten-tial indicator, but ignores succeeding, or ignores namedand recognises *succeeding*.
If the sentence appeared intraining data it would be transformed to give two train-ing data trees.
We should stress that this process is com-pletely automatic once the indicators have been identifiedin the text.6.2 ResultsThe model was trained on 563 sentences, and tested onanother 356 sentences.
(That is, 563/356 sentences af-ter producing one sentence per indicator as described insection 6.1).
The sentences were taken from the "Who'snews" section of Wall Street Journal, which is almost ex-clusively about management successions.
The trainingsentences were taken from 219 Who's News articles inthe 1996 section, the test sentences were taken from 131articles in the 1995 section.
The sentence level annota-tion was part of an annotation effort for the full extractiontask, which therefore also marked the relevant corefer-ence relationships and the complete output template asin figure 1.The test data sentences always contain an event, andhave all indicators marked as *indicator*-- only thoseindicators that have 1 or more slots attached to them aremarked.
This is an idealization, in that we avoid prob-lems of false positives, cases where a potential indicatoris not used to express an event.
See section 6.4 for sug-gestions about how to extend the model to deal with falsep.ositives.The results are shown in table 4.
We define precisionand recall when comparing to the annotated test set an-swers (gold standard) asNumber of correct slotsPrecision =Number of slots proposedNumber of correct slotsRecall =Number of slots in the gold standardIn addition we report he standard "F-Measure", which isa combination of precision and recall2 x Precision ?
RecallF-Measure =Precision + RecallThe results are quoted for the IN, OUT and POST slots(the IND slot is not scored, as it is marked in test data andwould score 100% recall/precision, i flating the scores).The number of "correct" slots varies depending on howpartial matches are scored - a partial match is where anoutput slot does not match a gold standard slot exactly,but does partially overlap.
For example, inBill Smith was elected vice president, humanresources .Score forpartia!
Precision Recall F-Measure0 80.6% 74.6% 77.5%0.5 85.9% 79.6% 82.6%1.0 91.3% 84.5% 87.8%JTable 4: Results on 356 test data sentences, training on563 sentencesthe gold standard might designate the slot as "vice pres-ident, human resources", whereas the program outputmight just mark "vice president".
We present hree: preci-sion/recall scores - -  where a partial match scores 0, 0.5or 1.0, andNumber of correct slots = Number of exact matches+Score for a partial match x Number of partial matches6.3 Analysis of the resultsIn this section we look at the errors the system makesin more detail.
There are two categories of error: preci-sion errors (incorrect slots); and recall errors (slots thesystem failed to propose).
For these tests we ran ex-periments on the training data, jack-knifing (i.e.
usingcross-validation) it into 4 sections, in each case trainingon three-quarters of the training set and testing on theother quarter.
Tables 5 and 6 show the results on thisdata set.Gold Proposed Correct I Partial Correcti +Partial986 944 769 71 840 ITable 5: Results on the jack-knifed training set (Counts)Score for partial Precision0 81.5%0.5 85.2%1.0 89.0%Recall F-Measure78.0% 79.7%81.6% 83.4%85.2% 87.0%Table 6: Results on the jack-knifed training set (Percent-ages)6.3,1 Precision errorsTable 7 shows the 104 precision errors categorized byhand into four categories.
These four categories were:1) Semantically Plausible.
Here the model has se-lected a slot-filler that looks good semantically, but isruled out for other reasons (usually syntactic).
For ex-ample,The appointment puts (IN Mr. Zwirn) , 41years o ld,  in line to succeed the unit 's  pres-ident, Frank R. Bakos, 58, who is (IND retir-ing) at year end.Here "Mr. Zwirn" is semantically a good filler for "retir-ing", but syntactically this is almost impossible.46Error type %ageSemantically 37.1%Plausible"Correct" 25.7%Bad lexical 8.6%information iOthers 28 .6%Error sub-type %ageRelative clauses 18.1%Subject 10.5%Other 8.6%Good alternative 17.1%> 1 reference 8.6%Table 7: The percentage of errors in each error categoryWe sub-divided this class into 3 sub-categories: prob-lems with relative clauses, as in the example above; prob-lems with non-relativized subjects, for example "Bran-don Sweitzer, 53, succeeds (IN Mr. Wakefield) as pres-ident of Guy Carpenter and also (IND becomes) (POSTthe unit's CEO), succeeding Richard Blum, 56 Y; andproblems that fell outside these categories.2) "Correct".
These slots were not seen in the gold-standard, but were deemed pretty much correct, in thatthey would not hurt (and might even help) the score of afull system.
They fall into two sub-categories- "good al-ternative", where the model's output is different from thegold standard but still looks reasonable, either becausethe sentence has more than one reasonable answer, or thegold standard is simply wrong; "> 1 reference", wherethere is more than one reference to the slot filler in thesentence, and the model has chosen a different one fromthe gold standard.
For example,(OUT Mr. Johnson) , 52 , said he resigned(POST his positions as chief executive officer)Here the model marked "Mr. Johnson" as OUT, the an-notator marked "he", and both are in some sense correct.3) Bad Lexieal Information.
In these cases the modelselected aslot filler that is clearly bad for lexical reasons,for exampleMr.
Broeksmit is the (OUT latest) in a stringof employees to (IND leave) the firm ...4) Other.
Miscellaneous errors which do not fall intothe above three categories.6.3.2 Recall ProblemsOf the 356 test-set sentences, 330 (92.7%) were pro-cessed by the system to give some output - -  no outputwas produced for 26 cases.
This accounts for the recallfigures in table 4 being lower than the precision figures(for example, with a score of 0 for partials, precision =80.6%, recall = 74.6%, and 92.7%*80.6% =74.7%).
Ofthese 26 cases, 24 involved an indicator word that hadnever been seen in training data.
The other 2 cases in-volved an unusual usage of "succeed", which had neverbeen seen in training data and was peculiar enough forthe system to fail to get an analysis (we set a probabilitythreshold such that the machine gives up if it fails to findan analysis above this probability).6.4 Dealing with False PositivesThis work has made a simplifying assumption, that testsentences were marked with indicators that had one ormore slots.
This section considers how this process couldbe automated.A first step would be to identify in test data morpho-logical variants of words that had been seen as indicatorsin training data.
However this would inevitably lead tofalse positives - -  that is, potential indicators appearingin cases where they don't indicate an event.
We could seetwo potential approaches for filtering out these spuriouscases: first, word-sense disambiguation methods imilarto those in (Yarowsky 95); second, we could extend themodel to have an eighth, empty, template as a possibilitythe model should then learn how often null templatesoccur, and what kind of lexical items tend to producethem.We leave this to future work.
At least in this dataset(Who's News articles) we believe that the false positiveproblem will not be severe, as the articles contain infor-mation almost exclusively on management successions,and most of the indicators are unambiguous within thissub-domain.The models have also made the assumption that an in-dicator is used to express each event.
This may not bethe case in all information extraction tasks, in some theremay not be clear indicator words; again, we leave dealingwith this limitation to future work.7 Future  WorkWe anticipate two directions for future work: first, re-fining the current model to improve its performance, andsecond, extending the current model to encompass thecomplete information extraction task.7.1 Refining the ModelWhen deciding on the direction of future work, it is use-ful to consider the error analysis in table 7.
The majorityof errors (the "semantically plausible" class) were caseswhere the model picked a slot that was semantically plau-sible, but syntactically impossible.
It is unlikely that thisproblem can be solved with the approach described here,even with vastly increased amounts of training data.
Ourfeeling is that a full syntactic parser as a first stage couldradically improve performance.
An improved approachmight be to fully integrate the recovery of syntactic struc-ture and semantic labelings, in a similar way to the ap-proach used in BBN's SIFT system (Miller et al 98).7.2 Extending the ModelAs discussed in section 1.1, the standard approach toinformation extraction involves three stages of process-ing: sentence level pattern matching, coreference, andtemplate merging.
Of these stages, our current work ad-dresses only sentence level pattern matching.
However,we believe that the generative statistical framework de-scribed in this paper could be extended advantageouslyto the complete information extraction problem.
In ex-tending the framework, we envision that the informationextraction task would be performed using an inverted"information production" model.We can think of this model as approximating, tosome?
degree, the process by which text is produced by an au-thor.
Specifically, we assume that each message is pro-duced according to a four stage process:1) First, the author decides what facts to express.
Forexample, the text in figure 1 can be thought of as express-ing two succession events: IN = "Hensley E. West", OUT= "John Bradley", POST = "president", COMPANY ="RESTOR INDUSTRIES Inc.", and OUT = "HensleyE.
West", POST = "group vice president", COMPANY= "DSC Communications Corp.".
This process can bemodeled as a prior probability distribution over sets oftemplates.
In this example, the model would give theprior probability of a message containing exactly twosuccession templates: one containing slots IN, OUT,POST, COMPANY and the other containing slots OUT,POST, COMPANY.2) After deciding what facts to express, the authormust decompose them into one or more componentevents.
For example, the succession event IN = "HensleyE.
West", OUT = "John Bradley", POST = "president",COMPANY = "RESTOR INDUSTRIES Inc." is decom-posed into two smaller events: IN = "Hensley E. West",POST = "president", COMPANY = "RESTOR INDUS-TRIES Inc." and IN = "Hensley E. West", OUT = "JohnBradley".
This process can be modeled as a probabilitydistribution over "template splitting operations", condi-tioned on the full template being expressed.
Templatesplitting operations are thus the generative analogue ofthe merging operations used in most information extrac-tion systems.3) Next, each component event must be expressed as alinguistic pattern.
For example, the event IN = "HensleyE.
West", POST = "president", COMPANY = "RESTORINDUSTRIES Inc." is expressed as the linguistic pattern"IN ... was named POST of COMPANY", and the eventIN = "Hensley E. West", OUT = "John Bradley" is ex-pressed as the linguistic pattern "IN ... fills a vacancycreated by the retirement ... of OUT".
This process canbe modeled as a probability distribution over linguisticpatterns, conditioned on the partial template being ex-pressed.
Modeling this distribution is the subject of themain body of this paper.4) Finally, the entities involved in events must berealized as word strings within patterns.
For exam-ple, "'RESTOR INDUSTRIES Inc." is realized as "thistelecommunications-product concern", and "Hensley E.West" is realized as "Mr. West".
This process canbe modeled as a probability distribution over "descrip-tor generating operations", conditioned on the entity be-ing expressed and other features of the text.
For exam-47pie, given that the author intends to express "HensleyE.
West", and given that the full name appears earlierin the text, the model would assign a certain probabilityto generating the word string "Mr. West".
In this case,the descriptor generating operation would be \[title + lastname\].Clearly, there are many details that would need to beresolved before a complete generative model of !informa-tion extraction could be implemented.
In this paper, wehave described a model containing two of the necessarycomponents: a prior model over templates, and a modelof linguistic patterns conditioned on those templates.
Acomplete generative model for IE would offer two po-tentially powerful advantages.
First, the model wouldprovide pnncipled probability estimates for selecting themost likely set of templates given an input message:T = set of templates (the final output)M= the message, C = componentsP = linguistic patterns, S = slot fillers in TD = descriptions used to express the slotsP(TIM) = ~ P(T, C, PIM)C,PwhereP(T, C, PIM) = P(T) x P(CIT) x P(PIC) x P(DIS)P(M)The second potential advantage derives from the gen-erative aspect of the proposed model.
While there is ananalogue in conventional IE systems for each of stages 2through 4 described above, there is no conventional na-logue to stage 1: the prior model.
We can think of thisprior model as encoding domain-specific world knowl-edge about he plausibility of proposed sets of relations.8 ConclusionsWe have shown that a simple statistical model can iden-tify semantic slot-fillers in a management succession taskwith 83% accuracy (F-measure with a score of 0.5 forpartial matches).
The system was trained on only 560sentences, with the additional requirements of only apart-of-speech tagger and a morphological nalyser.
Weinitially considered a finite-state approach similar to thatused for POS tagging (Church 88), or named-entity iden-tification (Bikel et al 97), but argued that the Markovapproximation gives a poor model for this task.
The al-ternative, which has a PCFG component to define theprobability of the underlying sequence of labels, allowsa good parameterization f the problem, and can be de-coded efficiently using the CKY algorithm.
Finally, webelieve that the framework presented in this paper can beextended to model the complete information extractionprocess.48AcknowledgementsWe would like to thank Richard Schwartz and RalphWeischedel for many helpful discussions and sugges-tions concerning this work.
We would also like to thankthe anonymous reviewers for several useful comments.ReferencesD.
Appelt, J. Hobbs, J.
Bear, D. J. Israel, and M. Tyson.
1993.
FASTUS:a finite-state processor for information extraction from real-worldtext.
In Proceedings of IJCAI-93, (Chamber'y, France), September1993.D.
M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997.
Nymble:a High-Performance L arning Name-finder.
In Proceedings of theFifth Conference on Applied Natural Language Processing, pages194-201.M.
E. Caiiff and R. J. Mooney.
1997.
Relational Learning of Pattern-Match Rules for Information Extraction.
In Proceedings of the ACLWorkshop on Natural Language Learning, Madrid, Spain.K.
Church.
1988.
A Stochastic Parts Program and Noun Phrase Parserfor Unrestricted Text.
Second Conference on Applied Natural Lan-guage Processing, ACILR.
Gfishman.
1995.
The NYU System for MUC-6 or Where's the Syn-tax?
In proceedings of the Sixth Message Understanding Confer-ence, Morgan Kaufmann.E Jelinek.
1990.
Self-organized Language Modeling for Speech Recog-nition.
In Readings in Speech Recognition.
Edited by Waibel andLee.
Morgan Kaufmann Publishers.Daniel Karp, Yves Schabes, Martin Zaidel and Dania Egedi.
A FreelyAvailable Wide Coverage Morphological Analyzer for English.
InProceedings of the 15th International Conference on ComputationalLinguistics, 1994.W.
Lehnert, J. McCarthy, S. Soderland, E. Riloff, C. Cardie, J. Peter-son, E Feng, C. Dolan, and S. Goldman.
1993.
University of Mas-sachussets/Hughes: Description of the CIRCUS system as used forMUC-5.
In Proceedings of the Fifth Message Understanding Con-ference (MUC-5), pages 277-290.M.
Marcus, B. Santofini and M. Marcinkiewicz.
1993.
Building a LargeAnnotated Corpus of English: the Penn Treebank.
Computational- Linguistics, 19(2):313-330.S.
Miller, M. Crystal, H. Fox, L. Ramshaw, R.
Schwartz., R. Stone,R.
Weischedel and the Annotation Group.
1998.
Algorithms thatLearn to Extract Information.
BBN: Description of the SIFT Sys-tem as used for MUC-7.
In Proceedings of the Seventh MessageUnderstanding Conference.Proceedings of the Third, Fourth, Fifth and Sixth Message Understand-ing Conferences (MUC-3, MUC-4, MUC-S and MUC-6).
MorganKaufmann, San Mateo, CA.A.
Ratnaparkhi.
1996.
A Maximum Entropy Model for Part-Of-SpeechTagging.
Conference on Empirical Methods in Natural LanguageProcessing, May 1996.E.
Riloff.
1993.
Automatically Constructing a Dictionary for Informa-tion Exlraction Tasks.
In Proceedings of the Eleventh National Con-ference on Artificial Intelligence, Washington,'DC.
AAAI Press IMIT Press.
811-816.E.
Riloff.
1996.
Automatically Generating Extraction Patterns fromUntagged Text.
In Proceedings of the Thirteenth National Confer-ence on Artificial Intelligence, Portland, OR.
AAAI Press / MITPress.
1044-1049.S.
Soderland, D. Fisher, J. Aseltine and W. Lehnert.
1995.
CRYSTAL:Inducing aConceptual Dictionary.
In Proceedings t~fthe ~mrteenthInternational Conference on Artificial Intelligence.
AAAI Press IMIT Press.
1314-1319.Yarowsky, D. 1995.
Decision Lists for Lexical Ambiguity Resolution:Application to Accent Restoration i  Spanish and French.
In Pro-ceedings of the 32nd Annual Meeting of the Association for Compu-tational Linguistics.
Las Cruces, NM, pp.
88-95, 1994.
