Using Corpus-derived Name Lists for Named Entity RecognitionMark  Stevenson  and Rober t  Ga izauskasDepar tment  of  Computer  Science,Un ivers i ty  of  Sheff ieldRegent  Cour t ,  211 Por tobe l lo  S t reet ,Sheff ieldS1 4DP Un i ted  K ingdom{marks, robertg}~dcs, shef.
ac.ukAbst ractThis paper describes experiments to establish theperformance of a named entity recognition systemwhich builds categorized lists of names from manu-ally annotated training data.
Names in text are thenidentified using only these lists.
This approach doesnot perform as well as state-of-the-art named en-tity recognition systems.
However, we then showthat by using simple filtering techniques for improv-ing the automatically acquired lists, substantial per-formance benefits can be achieved, with resulting F-measure scores of 87% on a standard test set.
Theseresults provide a baseline against which the con-tribution of more sophisticated supervised learningtechniques for NE recognition should be measured.1 In t roduct ionNamed entity (NE) recognition is the process ofidentifying and categorising names in text.
Systemswhich have attempted the NE task have, in general,made use of lists of common ames to provide clues.Name lists provide an extremely efficient way of re-cognising names, as the only processing required isto match the name pattern in the list against thetext and no expensive advanced processing such asfull text parsing is required.
However, name lists area naive method for recognising names.
McDonald(1996) defines internal and external evidence in theNE task.
The first is found within the name stringitself, while the second is gathered from its context.For example, in the sentence "President Washingtonchopped the tree" the word "President" is clear ex-ternal evidence that "Washington" denotes a person.In this case internal evidence from the name cannotconclusively tell us whether "Washington" is a per-son or a location ("Washington, DC").
A NE sys-tem based solely on lists of names makes use of onlyinternal evidence and examples uch as this demon-strate the limitations of this knowledge source.Despite these limitations, many NE systems useextensive lists of names.
Krupke and Hausman(1998) made extensive use of name lists in their sys-tem.
They found that reducing their size by morethan 90% had little effect on performance, converselyadding just 42 entries led to improved results.
Thisimplies that the quality of list entries is a more im-portant factor in their effectiveness than the totalnumber of entries.
Mikheev et al (1999) experi-mented with different ypes of lists in an NE systementered for MUC7 (MUC, 1998).
They concludedthat small lists of carefully selected names are aseffective as more complete lists, a result consistentwith Krupke and Hausman.
However, both studiesaltered name lists within a larger NE system and itis difficult to tell whether the consistency of perform-ance is due to the changes in lists or extra, external,evidence being used to balance against the loss ofinternal evidence.In this paper a NE system which uses only the in-ternal evidence contained in lists of names is presen-ted.
Section 3 explains how such lists can be auto-matically generated from annotated text.
Sections4 and 5 describe xperiments in which these corpus-generated lists are applied and their performancecompared against hand-crafted lists.
In the next sec-tion the NE task is described in further detail.2 NE background2.1 NE Recognition of Broadcast NewsThe NE task itself was first introduced as part ofthe MUC6 (MUC, 1995) evaluation exercise and wascontinued in MUC7 (MUC, 1998).
This formulationof the NE task defines seven types of NE: PERSON,ORGANIZATION, LOCATION, DATE, TIME, MONEY andPERCENT.
Figure 1 shows a short text marked upin SGML with NEs in the MUC style.The task was duplicated for the DARPA/N ISTHUB4 evaluation exercise (Chinchor et al, 1998)but this time the corpus to be processed consistedof single case transcribed speech, rather than mixedcase newswire text.
Participants were asked to carryout NE recognition on North American broadcastnews stories recorded from radio and television andprocessed by automatic speech recognition (ASR)software.
The participants were provided with atraining corpus consisting of around 32,000 wordsof transcribed broadcast news stories from 1997 an-notated with NEs.
Participants used these text to290"It's a chance to think about first-level questions," said Ms. <enamextype="PERS0N">Cohn<enamex>, a partner in the <enamex type="0RGANIZATION">McGlashanSarrail<enamex> firm in <enamex type="L0CATION">San Mateo<enamex>, <enamextype="L0CATION">Calif.<enamex>Figure 1: Text with MUC-style NE's markeddevelop their systems and were then provided withnew, unannotated texts, consisting of transcribedbroadcast news from 1998 which they were given ashort time to annotate using their systems and re-turn.
Participants are not given access to the eval-uation data while developing their systems.After the evaluation, BBN, one of the parti-cipants, released a corpus of 1 million words whichthey had manually annotated to provide their sys-tem with more training data.
Through the re-mainder of this paper we refer to the HUB4 trainingdata provided by DARPA/NIST as the SNORT_TRAINcorpus and the union of this with the BBN data asthe LONG_TRAIN corpus.
The data used for the 1998HUB4 evaluation was kept blind, we did not exam-ine the text themselves, and shall be referred to asthe TEST corpus.The systems were evaluated in terms of the com-plementary precision (P) and recall (R) metrics.Briefly, precision is the proportion of names pro-posed by a system which are true names while recallis the proportion of the true names which are actu-ally identified.
These metrics are often combinedusing a weighted harmonic called the F-measure(F) calculated according to formula 1 where fl is aweighting constant often set to 1.
A full explana-tion of these metrics is provided by van Rijsbergen(1979).F= ( f~+l)  xPxR(fl ?
P) + R (1)The best performing system in the MUC7 exercisewas produced by the Language Technology Group ofEdinburgh University (Mikheev et al, 1999).
Thisachieved an F-measure of 93.39% (broken down asa precision of 95% and 92% recall).
In HUB4 BBN(Miller et al, 1999) produced the best scoring sys-tem which achieved an F-measure of 90.56% (preci-sion 91%, recall 90%) on the manually transcribedtest data.2.2 A Full NE  sys temThe NE system used in this paper is based on Shef-field's LaSIE system (Wakao et al, 1996), versionsof which have participated in MUC and HUB4 eval-uation exercises (Renals et al, 1999).
The systemidentifies names using a process consisting of fourmain modules:List Lookup This module consults several ists oflikely names and name cues, marking each oc-currence in the input text.
The name lists in-clude lists of organisations, locations and per-son first names and the name cue lists of titles(eg.
"Mister", "Lord"), which are likely to pre-cede person names, and company designators(eg.
"Limited" or "Incorporated"), which arelikely to follow company names.Par t  of speech tagger  The text is the part ofspeech tagged using the Brill tagger (Brill,1992).
This tags some tokens as "proper name"but does not attempt o assign them to a NEclass (eg.
PERSON, LOCATION).Name pars ing  Next the text is parsed using a col-lection of specialised NE grammars.
The gram-mar rules identify sequences of part of speechtags as added by the List Lookup and Par tof  speech tagger  modules.
For example, thereis a rule which says that a phrase consistingof a person first name followed by a word partof speech tagged as a proper noun is a personname.Namematch ing  The names identified so far in thetext are compared against all unidentified se-quences of proper nouns produced by the part ofspeech tagger.
Such sequences form candidateNEs and a set of heuristics is used to determ-ine whether any such candidate names matchany of those already identified.
For example onesuch heuristics ays that if a person is identifiedwith a title (eg.
"President Clinton") then anyoccurrences without the title are also likely tobe person names '(so "Clinton" on it own wouldalso be tagged as a person name).For the experiments described in this paper a re-stricted version of the system which used only theList Lookup module was constructed.
The listlookup mechanism arks all words contained in anyof the name lists and each is proposed as a NE.
Anystring occurring in more than one list is assigned thecategory form the first list in which it was found, al-though this did not occur in any of the sets of listsused in the experiments described here.3 L i s t  Generat ionThe List Lookup module uses a set of hand-crafted lists originally created for the MUC6 eval-uation.
They consisted of lists of names from thegazetteers provided for that competition, supple-mented by manually added entries.
These lists291evolved for the MUC7 competition with new entriesand lists being added.
For HUB4 we used a se-lection of these lists, again manually supplementingthem where necessary.
These lists included lists ofcompanies, organisations (such as government de-partments), countries and continents, cities, regions(such as US states) and person first names as well ascompany designators and person titles.
We specu-late that this ad hoc, evolutionary, approach to cre-ating name lists is quite common amongst systemswhich perform the NE task.In order to compare this approach against a simplesystem which gathers together all the names occur-ring in NE annotated training text, a program wasimplemented to analyse text annotated in the MUCSGML style (see Figure 1) and create lists for eachNE type found.
For example, given the NE <enamextype="LOCATION">SAN MATE0<enamex> an entrySAN MATE0 would be added a list of locations.This simple approach is certainly acceptable forthe LOCATION, ORGANIZATION and, to a more lim-ited extent, PERSON classes.
It is less applicable tothe remaining classes of names (DATE, TIME, MONEYand PERCENT) because these are most easily recog-nised by their grammatical structure.
For example,there is a rule in the NE grammar which says a num-ber followed by a currency unit is as instance of theMONEY name class- eg.
FIFTY THREE DOLLARS, FIVEMILLION ECU.
According to Przbocki et al (1999)88% of names occurring in broadcast news text fallinto one of the LOCATION, ORGANIZATION and PERSONcategories.Two sets of lists were derived, one fromthe SHORT_TRAIN corpus and a second from theLONG_TRAIN texts.
The lengths of the lists producedare shown in Table 1.CorpusCategory SHORT_TRAIN LONG_TRAINORGANIZATION 245 2,157PERSON 252 3,947LOCATION 230 1,489Table 1: Lengths of lists derived from SHORT_TRAINand LONG_TRAIN corpora4 L i s t  App l i ca t ionThe SHORT_TRAIN and LONG_TRAIN lists were eachapplied in two ways, alone and appended to the ori-ginal, manually-created, lists.
In addition, we com-puted the performance obtained using only the ori-ginal lists for comparison.
Although both sets of listswere derived using the SHORT_TRAIN data (since theLONG_TRAIN corpus includes SHORT_TRAIN), we stillcompute the performance of the SHORT_TRAIN listson that corpus since this provides some insight intothe best possible performance which can be expectedfrom NE recognition using a simple list lookup mech-anism.
No scores were computed for the LONG_TRAINlists against he SHORT_TRAIN corpus since this is un-likely to provide more information.Table 2 shows the results obtained when theSHORT_TRAIN lists were applied to that corpus.
Thisfirst experiment was designed to determine howwell the list lookup approach would perform givenlists compiled directly from the corpus to whichthey are being applied.
Only PERSON, LOCATIONand ORGANIZATION name classes are considered sincethey form the majority of names occurring in theHUB4 text.
As was mentioned previously, the re-maining categories of name are more easily recog-nised using the NE parser.
For each configuration oflists the precision, recall and F-measure are calcu-lated for the each name class both individually andtogether.We can see that the original ists performed reas-onably well, scoring an F-measure of 79% overall.However, the corpus-based lists performed far bet-ter achieving high precision and perfect recall.
Wewould expect he system to recognise very name inthe text, since they are all in the lists, but perfectprecision is unlikely as this would require that noword appeared as both a name and non-name or inmore than one name class.
Even bearing this in mindthe calculated precision for the ORGANIZATION classof names is quite low.
Analysis of the output showedthat several words occurred as names a few times inthe text but also as non-names more frequently.
Forexample, "police" appeared 35 times but only onceas an organisation; similarly "finance" and "repub-lican" occur frequently but only as a name a fewtimes.
In fact, these three list entries account for 61spuriously generated names, from a total of 86 forthe ORGANIZATION class.
The original lists do notinclude words which are likely to generate spuriousentries and names like "police" would only be recog-nised when there was further evidence.The SHORT_TRAIN lists contain all the names oc-curring in that text.
When these lists are combinedwith the original system lists the observed recall re-mains 100% while the precision drops.
The originalsystem lists introduce more spurious entries, leadingto a drop of 3% F-measure.The results of applying the corpus-derived lists tothe texts from which they were obtained show that,even under these circumstances, perfect results can-not be obtained.
Table 3 shows a more meaningfulevaluation; the SHORT_TRAIN lists are applied to theTEST corpus, an unseen text.
The original systemlists achieve an F-measure of 83% on this text andthe corpus-derived lists perform 8% worse.
However,the configuration of lists which performs best is theunion of the original ists with those derived from the292Lists Original SHORT_TRAIN CombinationName Type P R F P R F P R FALL 86 73 79 94 100 97 88 100 94ORGANIZATION 84 49 62 83 100 90 79 100 88PERSON 78 71 74 99 100 99 88 100 94LOCATION 92 88 90 98 100 99 95 100 97Table 2: SHORT_TRAIN lists applied to SHORT_TRAIN corpuscorpus.
This out-performs each set of lists taken inisolation both overall and for each name category in-dividually.
This is clear evidence that the lists usedby the system described could be improved with theaddition of lists derived from annotated text.It is worth commenting on some of the results forindividual classes of names in this experiment.
Wecan see that the performance for the ORGANIZATIONclass actually increases when the corpus-based listsare used.
This is partially because names which aremade up from initials (eg.
"C. N. N." and "B.
B. C. ")are not generally recognised by the list lookup mech-anism in our system, but are captured by theparser and so were not included in the original lists.However, it is also likely that the organisation list islacking, at least to some level.
More interestingly,there is a very noticeable drop in the performancefor the PERSON class.
The SHORT_TRAIN lists achievedan F-measure of 99% on that text but only 48% onthe TEST text.
In Section 2.1 we mentioned that theHUB4 training data consists of news stories from1997, while the test data contains tories from 1998.We therefore suggest hat the decrease in perform-ance for the PERSON category demonstrates a generalproperty of broadcast news: many person namesmentioned are specific to a particular time period(eg.
"Monica Lewinksi" and "Rodney King").
Incontrast, the locations and organisations mentionedare more stable over time.Table 4 shows the performance obtained when thelists derived from LONG_TRAIN were applied to theTEST corpus.
The corpus-derived lists perform sig-nificantly worse than the original system lists, show-ing a large drop in precision.
This is to be expec-ted since the lists derived from LONG_TRAIN containall the names occurring in a large body of text andtherefore contain many words and phrases which arenot names in this text, but spuriously match non-names.
Although the F-measure result is worse thanwhen the SHORT_TRAIN lists were used, the recallis higher showing that a higher proportion of thetrue names can be found by analysing a larger bodyof text.
Combining the original and corpus-derivedlists leads to a 1% improvement.
Recall is noticeablyimproved compared with the original lists, howeverprecision is lowered and this shows that the corpus-derived lists introduce a large number of spuriousnames.From this first set of experiments it can be seenthat perfect results will not be obtained even usinglists contain all and only the names in a particulartext, thus demonstrating the limitations of this na-ive approach to named entity recognition.
We havealso demonstrated that it is possible for the addi-tion of corpus-derived lists to improve the perform-ance of a NE recognition system based on gazetteers.However, this is not guaranteed and it appears thatadding too many names without any restriction mayactually lead to poorer results, as happened whenthe LONG_TRAIN lists were applied.5 F i l te r ing  L is tsThe results from our first set of experiments led us toquestion whether it is possible to restrict the entriesbeing added to the lists in order to avoid those likelyto generate spurious names.
We now go on to de-scribe some methods which can be used to identifyand remove list entries which may generate spuriousnames.Method 1: Dic t ionary  F i l te r ing  The derivedlists can be improved by removing items in thelist which also occur as entries in a dictionary.We began by taking the Longman Dictionary ofContemporary Englisb (LDOCE) (Procter, 1978) andextracting a list of words it contained including allderived forms, for example pluralisation of nounsand different verb forms.
This produced a list of52,576 tokens which could be used to filter namelists.Method 2: Probability F i l te r ing  The lists canbe improved by removing names which occurmore frequently in the corpus as non-namesthan names.Another method for filtering lists was imple-mented, this time using the relative frequencies ofphrases occurring as names and non-names.
We canextract the probability that a phrase occurs as aname in the training corpus by dividing the num-ber of times it occurs as a name by the total numberof corpus occurrences.
If this probability estimate isan accurate reflection of the name's behaviour in a293Lists Original SHORT_TI~IN CombinationName Type P R F P R F P R FALL 86 79 83 90 65 75 83 86 84ORGANIZATION 82 57 67 76 66 71 79 81 80PERSON 77 80 78 93 32 48 79 83 81LOCATION 93 89 91 97 81 88 92 94 93Table 3: SHORT_TRAIN \]ists applied to TEST corpusLists Original LONG_TRAIN CombinationName Type P R F P R F P R FALL 86 79 83 64 86 73 62 91 74ORGANIZATION 82 57 67 44 85 58 43 88 58PERSON 77 80 78 55 75 63 53 86 66LOCATION 93 89 91 87 92 89 84 94 89Table 4: LONG_TRAIN lists applied to TEST corpusnew text we can use it to estimate the accuracy ofadding that name to the list.
Adding a name to alist will lead to a recall score of 1 for that name anda precision of Pr  (where Pr  is the probability valueestimated from the training corpus) which implies anF-measure of ~.2Pr 1 Therefore the probabilities canbe used to filter out candidate list items which implylow F-measure scores.
We chose names whose cor-pus probabilities produced an F-measure lower thanthe overall score for the list.
The LONG_TRAIN listsscored an F-measure of 73% on the unseen, TEST,data (see Table 4).
Hence a filtering probability of73% was used for these lists, with the corpus stat-istics gathered from LONG_TRAIN.Method  3: Combin ing  F i l ters  These filteringstrategies can be improved by combining them.We also combined these two filtering strategies intwo ways.
Firstly, all names which appeared in thelexicon or whose corpus probability is below the fil-tering probability are removed from the lists.
This isdubbed the "or combination".
The second combin-ation strategy removes any names which appear inthe lexicon and occur with a corpus frequency belowthe filtering probability are removed.
This secondstrategy is called the "and combination".These filtering strategies were applied to theLONG_TRAIN lists.
The lengths of the lists producedare shown in Table 5.The strategies were evaluated by applying thefiltered LONG_TRAIN lists to the TEST corpus, the res-ults of which are shown in Table 6.
There is an1Analysis of the behaviour of the function f (P r )  -- 2P~ l+Prshows that it does not deviate too far from the value of Pr  (ie.. f (P r )  ~ Pr )  and so there is an argument for simply filteringthe lists using the raw probabilities.improvement in performance of 4% F-measure whenlists filtered using the "and" combination are usedcompared to the original, hand-crafted, lists.
Al-though this approach removes only 108 items fromall the lists there is a 14% F-measure improvementover the un-filtered lists.
Each filtering strategy usedindividually demonstrates a lower level of improve-ment: the dictionary filtered lists 12% and the prob-ability filtered 10%.The "and" combination is more successful be-cause filtering lists using the dictionary alone re-moves many names we would like to keep (eg.
coun-try names are listed in LDOCE) but many of theseare retained since both filters must agree.
Theseexperiments demonstrate hat appropriately filteredcorpus-derived lists can be more effective for NE re-cognition than hand-crafted lists.
The differencebetween the observed performance of our simplemethod and those reported for the best-performingHUB4 system is perhaps lower that one may ex-pect.
The BBN system achieved 90.56% overall,and about 92% when only the PERSON, LOCATIONand ORGANIZATION name classes are considered, 5%more than the method reported here.
This differenceis perhaps lower than we might expect given thatname lists use only internal evidence (in the senseof Section 1).
This indicates that simple applicationof the information contained in manually annotatedNE training data can contribute massively to theoverall performance of a system.
They also providea baseline against which the contribution of moresophisticated supervised learning techniques for NErecognition should be measured.294NECategoryORGANIZATIONPERSONLOCATIONUn-Filtered Dictionary ProbabilityList Filtered Filtered2,157 1,978 2,0003,947 3,769 3,2351,489 1,412 1,364OrCombined1,9643,5221,382AndCombined2,0493,8091,449Table 5: Lengths of corpus-derived listsOriginal t Un-Filtered Dictionary I Probability Or AndLists Lists Filtered Filtered Combination CombinationName Type P R F P R F P R F P R F P R F P R FALLORGANIZATIONPERSONLOCATION86 79 8382 57 6777 80 7893 89 9164 86 7344 85 5855 75 6387 92 8995 79 8586 72 7896 66 7898 89 9396 73 8385 74 7996 40 5697 90 9395 73 8384 60 70100 49 6698 90 9493 81 8784 76 8094 66 7897 92 94Table 6: Filtered and un-filtered LONG_TRAIN lists applied to TEST corpus6 ConclusionThis paper explored the role of lists of names inNE recognition, comparing hand-crafted and corpus-derived lists.
It was shown that, under certain condi-tions, corpus-derived lists outperform hand-craftedones.
Also, supplementing hand-crafted lists withcorpus-based ones often improves their performance.The reported method was more effective for theORGANIZATION and LOCATION classes of names thanfor PERSON, which was attributed to the fact thatreportage of these names does not change as muchover time in broadcast news.The method reported here achieves 87% F-measure, 5% less than the best performing systemin the HUB4 evaluation.
However, it should be re-membered that this technique uses only a simple ap-plication of internal evidence.ReferencesE.
Brill.
1992.
A simple rule-based part of speechtagger.
In Proceeding of the Third Conference onApplied Natural Language Processing (ANLP-92),pages 152-155, Trento, Italy.N.
Chinchor, P. Robinson, and E. Brown.1998.
Hub-4 named entity task defini-tion (version 4.8).
Technical report, SAIC.http ://www.
nist.
gov/speech/hub4_98.G.
Krupke and K. Hausman.
1998.
Isoquest Inc:description of the NetOwl(TM) extractor systemas used for MUC-7.
In Message UnderstandingConference Proceedings: MUC 7.
Available fromhttp ://www.muc.
saic.
com.D.
McDonald.
1996.
Internal and external evid-ence in the identification and semantic ategor-ization of proper names.
In B. Boguraev andJ.
Pustejovsky, editors, Corpus Processing forLexical Aquisition, chapter 2, pages 21-39.
MITPress, Cambridge, MA.A.
Mikheev, M. Moens, and C. Grovel 1999.Named entity recognition without gazeteers.
InProceedings of the Ninth Conference of theEuropean Chapter of the Association for Compu-tational Linguistics, pages 1-8, Bergen, Norway.D.
Miller, R. Schwartz, R. Weischedel, and R. Stone.1999.
Named entity extraction from broadcastnews.
In Proceedings of the DARPA BroadcastNews Workshop, ages 37-40, I-Ierndon, Virginia.MUC.
1995.
Proceedings of the Sixth Message Un-derstanding Conference (MUC-6}, San Mateo,CA.
Morgan Kaufmann.1998.
Message Understanding Conference Proceed-ings: MUC7.
http ://www.muc.
sale com.P.
Procter, editor.
1978.
Longman Dictionary ofContemporary English.
Longman Group, Essex,UK.M.
Przbocki, J. Fiscus, J. Garofolo, and D. Pallett.1999.
1998 HUB4 Information Extraction Eval-uation.
In Proceedings of the DARPA BroadcastNews Workshop, ages 13-18, Herndon, Virginia.S.
Renals, Y. Gotoh, R. Gaizausaks, and M. Steven-son.
1999.
Baseline IE-NE Experimants Using theSPRACH/LASIE System.
In Proceedings of theDAPRA Broadcast News Workshop, ages 47-50,Herndon, Virginia.C.
van Rijsbergen.
1979.
Information Retrieval.Butterworths, London.T.
Wakao, R. Gaizauskas, and K. Humphreys.
1996.Evaluation of an algorithm for the recognition andclassification of proper names.
In Proceedings ofthe 16th International Conference on Computa-tional Linguistics (COLING-96), pages 418-423,Copenhagen, Denmark.295
