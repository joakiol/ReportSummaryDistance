English-to-Korean Transliteration using Multiple UnboundedOverlapping Phoneme ChunksIn-Ho Kang and Gi lChang KimDepartment  of Computer  ScienceKorea Advanced Inst i tute of Science and TechnologyAbst ractWe present in this paper the method ofEnglish-to-Korean(E-K) transliteration andback-transliteration.
In Korean technicaldocuments, many English words are translit-erated into Korean words in w~rious tbrms indiverse ways.
As English words and Koreantransliterations are usually technical terms andproper nouns, it; is hard to find a transliterationand its variations in a dictionary.
Theretbrean automatic transliteration system is neededto find the transliterations of English wordswithout manual intervention.3?.o explain E-K transliteration t)henomena,we use phoneme chunks that do not have alength limit.
By aI)plying phoneme chunks,we combine different length intbrmation witheasy.
The E-K transliteration method h~mthree stet)s. In the first, we make a t)honemenetwork that shows all possible transliterationsof the given word.
In the second step, we applyt)honeme chunks, extracted fl'om training data,to calculate the reliability of each possibletransliteration.
Then we obtain probabletransliterations of the given English word.1 Introduct ionIn Korean technical documents, many Englishwords are used in their original forms.
Butsometimes they are transliterated into Koreanin different forms.
Ex.
1, 2 show the examplesof w~rious transliterations in KTSET 2.0(Parket al, 1996).
(1) data(a) l:~\] o\] >\]-(teyitha) \[1,033\] 1(b) r~l o\] >\] (teyithe) \[527\]1the frequency in KTSET(2) digital(a) ~\] x\] ~-(ticithul) \[254\](b) qN~-(t ichithM)\[7\](c) ~1 z\] ~ (ticithel) \[6\]These various transliterations are not negligi-ble tbr natural angnage processing, especially illinformation retrieval.
Because same words aretreated as different ones, the calculation based(m tile frequency of word would produce mis-leading results.
An experiment shows that theeffectiveness of infbrmation retrieval increaseswhen various tbrms including English words aretreated eqnivMently(Jeong et al, 1997).We may use a dictionary, to tind a correcttransliteratkm and its variations.
But it is notfhasible because transliterated words are usuallytechnical terms and proper nouns that have richproductivity.
Therefore an automatic translit-eration system is needed to find transliterationswithout manual intervention.There have been some studies on E-Ktransliteration.
They tried to explain translit-eration as tflloneme-lmr-phoneme or alphabet-per-phonenm classification problem.
They re-stricted the information length to two or threeunits beibre and behind an input unit.
In tact,ninny linguistic phenomena involved in the E-Ktransliteration are expressed in terms of unitsthat exceed a phoneme and an alphabet.
Forexample, 'a' in 'ace' is transliterated into %11?l (ey0" lint in 'acetic', "ot (eft and ill 'acetone',"O}(a)".
If we restrict the information lengthto two alphabets, then we cmmot explain thesephenomena.
Three words ge~ the same result~()r ~ a'.
(3) ace otl o\] ~(eyisu)(4) acetic cq.q v I (esithik)418(5) acetone ol-J~ll ~-(aseython)In this t)at)er, we t)rot)ose /;he E-K transliter-al;ion model t)ased on l)honeme chunks thatdo not have a length limit and can explaintransliter;~tion l)henolnem, in SOllle degree ofreliability.
Not a alphal)et-per-all)habet but achunk-i)er-chunk classification 1)roblem.This paper is organized as tbllows.
In section2, we survey an E-K transliteration.
\]111 section3, we propose, phonenm chunks 1)asexl translit-eration and back-transliteration.
In Seel;ion 4,the lesults of ext)erilnents are presented.
Fi-nally, the con(:hlsion follows in section 5.2 Eng l i sh - to -Korean  t rans l i te ra t ionE-K transliteration models are (:lassitied in twomethods: the l)ivot method and the directmethod.
In the pivot method, transliterationis done in two steps: (:onverting English wordsill|;() pronunciation symbols and then (:onvertingthese symbols into Kore~m wor(ts by using theKorean stm~(tard conversion rule.
In the directmethod, English words are directly converted toKorean words without interlnediate stct)s. Anexl)eriment shows that the direct method is bet-ter than the pivot method in tin(ling wtriationsof a transliteration(Lee and (~hoi, 1998).
Statis-ti(:al information, neural network and de(:isiontree were used to imt)lelneld; the direct method.2.1 S ta t i s t iea l  T rans l i te ra t ion  methodAn English word is divided into phoneme se-quence or alphal)et sequence as (21~(22~... ~e n.Then a corresponding Korean word is rel)-resented as kl, k2,.
.
.
, t~:n. If n correspond-ing Korean character (hi) does not exist, wefill the blank with '-'.
For example, an En-glish word "dressing" and a Korean word ">N\] zg (tuleysing)" are represented as Fig.
1.
Theut)per one in Fig.
1 is divided into an Englishphoneme refit and the lower one is divided intoan alphabet mlit.dressh, g :---~ ~1~d/~+r /a  +e/  4l + ss /  x + i /  I +n-g/ ol d/=---+r/e +e/41 +s/~.+s/ -+ i /  I +n/o  +g/ -Figure 1: An E-K transliteration exault)leThe t)roblem in statistical transliterationreel;hod is to lind out the.
lllOSt probable translit-eration fbr a given word.
Let p(K) be the 1)tel)-ability of a Korean word K, then, for a givenEnglish word E, the transliteration probal)ilityof a word K Call be written as P(KIE).
By usingthe Bayes' theorem, we can rewrite the translit-eration i)rol)lem as follows:,a'.q maz p(K IE )  = a,..q ma:,: p(K)p(~IK)  (1)K KWith the Markov Independent Assulni)tion ,we apl)roximate p(K) and p(EIK) as tbllows:7~i=2i=1As we do not know the t)rommciation of agiven word, we consider all possible tfllonelnesequences, l?or exanlple, 'data' has tbllowingpossible t)holmme sequences, 'd-a-t-a, d-at-a,da-ta, .
.
. '
.As the history length is lengthened, we.
canget more discrimination.
But long history in-fornlation c~mses a data sl)arseness prol)lenl.
Inorder to solve, a Sl)arseness t)rol)len~, Ma.ximmnEntropy Model, Back-off, and Linear intert)ola-tion methods are used.
They combine differentst~tistical estimators.
(Tae-il Kim, 2000) use u t)to five phonemes in feature finlction(Berger eta,l., 1996).
Nine %ature flmctions are combinedwith Maximum Entrot)y Method.2.2 Neura l  Network  and  Dec is ion  TreeMethods based 011 neural network and decisiontree detenninistically decide a Korean charac-ter for a given English input.
These methodstake two or three alphabets or t)honemes asan input and generate a Korean alphabetor phoneme as an output.
(Jung-.\]ae Kim,1.999) proposed a neural network method thatuses two surrom~ding t)holmmes as an intmt.
(Kang, 1999) t)roposed a decision tree methodthat uses six surrounding alphabets.
If allinl)ut does not cover the phenomena of prol)ertransliterations, we cammt gel; a correct answer.419Even though we use combining methods tosolve the data sparseness problem, the increaseof an intbrmation length would double thecomplexity and the time cost of a problem.
Itis not easy to increase the intbrmation length.To avoid these difficulties, previous studiesdoes not use previous outputs(ki_z).
But itloses good information of target language.Our proposed method is based on the directmethod to extract the transliteration and itsvariations.
Unlike other methods that deter-mine a certain input unit's output with historyinformation, we increase the reliability of a cer-tain transliteration, with known E-K transliter-ation t)henonmna (phoneme chunks).3 Trans l i te ra t ion  us ing  Mul t ip leunbounded over lapp ing  phonemechunksFor unknown data, we can estimate a Koreantransliteration ti'onl hand-written rules.
Wecan also predict a Korean transliteration withexperimental intbrmation.
With known Englishand Korean transliteration pairs, we can as-sume possible transliterations without linguisticknowledge.
For example, 'scalar" has commonpart with 'scalc:~sqlN (suhhcyil)', ' casinoJ\[xl  (t:hacino)', 't:oala:   e-l-&:hoalla)', and'car:~l-(kh.a)' (Fig.
2).
We can assume possibletransliteration with these words and theirtransliterations.
From 'scale' and its transliter-ation l'-~\] ~ (sukheyil), the 'sc' in 'scalar' can betransliterated as '~:-J(sukh)'.
From a 'casino'example, the 'c' has nlore evidence that can betransliterated as 'v  (kh)'.
We assume that wecan get a correct Korean transliteration, if weget useful experinlental information and theirproper weight that represents reliability.3.1 The a l ignment  of  an Engl ish wordwith  a Korean  wordWe can align an English word with its translit-eration in alphabet unit or in phoneme unit.Korean vowels are usually aligned with Englishvowels and Korean consonants are aligned withEnglish consonants.
For example, a Koreanconsonant, '1~ (p)' can be aligned with Englishconsonants 'b', 'p', and 'v'.
With this heuristicwe can align an English word with its translit-eration in an alphabet unit and a t)honeIne unitwith the accuracy of 99.4%(Kang, 1999).s c a 1 a rs c a 1 ek o a1 n oI L ?C a rFFigure 2: the transliteration of 'scalar : ~~\]-(sukhalla)'3.2 Ext ract ion  o f  Phoneme ChunksFrom aligned training data, we extract phonemeclumks.
We emmw.rate all possible subsets ofthe given English-Korean aligned pair.
Duringenumerating subsets, we add start and end posi-tion infbrmation.
From an aligned data "dress-ing" and "~etl N (tuleysing)", we can get subsetsas Table 12.Table 1: The extraction of phoneme chunksContext Outputdr.
#?,_)dr'c d/=--(d)+r/~- (r')+e/ql (ey)The context stands tbr a given English al-phabets, and the output stands for its translit-eration.
We assign a proper weight to eachphoneme chunk with Equation 4.C ( output )wei.qh, t(contcxt : output) - C(contcxt) (4)C(x) means tile frequency of z in training data.Equation 4 shows that the ambiguous phe-nomenon gets the less evidence.
The clnmkweight is transmitted to each phoneme symbol.To compensate for the length of phoneme, wemultiply the length of phoneme to the weight ofthe phoneme chunk(Fig.
3).2@ means the start and end position of a word420weight(surfing: s/Z- + ur/4 + i f=  + i/l + r ig/o) = (Z4, 4, 4, 4, 4,o~ 2a a a 2o~\]?igure 3: The weight of a clmnk and a t)honemeThis chunk weight does not mean the.
relia-t)ility of a given transliteration i)henomenon.We know real reliM)itity, after all overlappingphonenm chunks are applied.
The chunk thathas some common part with other chunksgives a context information to them.
Thereforea chunk is not only an int)ut unit but alsoa means to (-Mculate the reliability of otherdmnks.We also e, xl;ra(:t the  connection information.From Migned training (b:~ta, we obtain M1 pos-sible combinations of Korem~ characters andEnglish chara(:ters.
With this commction in-tbrmation, we exclude iml)ossit)h; connectionsof Korean characters ~md English phon(;nte se-quences.
We can gel; t;he following (:ommctioninformation from "dressing" examph'.
(~12fl)le 2).2?fl)le 2: Conne(:tion InformationEnffli.sh, Kore.a',.
1\]lql't\[righ, t II Z(?lt l ,.,:.,1,,t /a ,.
( ,9, ('.
093.3 A Trans l i te ra t ion  NetworkFor a given word, we get al t)ossil)h~ t)honemesand make a Korean transliteration etwork.Each node in a net;work has an English t)honent(;and a ('orrcspondillg Korean character.
Nodesare comm(:ted with sequence order.
For exam-ple, 'scalar' has the Kore, an transliteration et-work as Fig.
4.
In this network, we dis('ommctsome no(les with extracted (:onne('tion infornla-tion.After drawing the Korean tr~msliteration net-work, we apply all possible phone, me, chunksto the.
network.
Each node increases its ownweight with the weight of t)honeme symbol in aphoneme chunks (Fig.
5).
By overlapping theweight, nodes in the longer clmnks get; more ev-idence.
Then we get the best t)ath that has theFigure 4: Korean Transliteration Network for'scalar'highest sum of weights, with the Viterbi algo-ril, hm.
The Tree.-Trcllis Mgorithm is used to gel;the variations(Soong and Huang, 1991).Figure 5: Weight aptflication examt)le4 E -K  back - t rans l i te ra t ionE-K back transliteration is a more difficult prot)-lem thtnt F,-K trmlsliteration.
During the E-Ktrm~slit;cra|;ion~ (lifli'xent alphabets are treatedcquiw~h'.ntly.
\],~)r exmnph'., ~f, t / mM ~v~ b'spectively and the long sound and the shortstrand are also treated equivalently.
Therefim',the number of possible English phone, rues pera Korean character is bigger than the numberof Korean characters per an English phoneme.The ambiguity is increased.
In  E-K back-transliteration, Korean 1)honemes and Englishphoneme, s switch their roles.
Just switching theposition.
A Korean word ix Migned with anEnglish word in a phoneme unit or a characterrefit (Fig.
6).\[ ~---~l~ : dressing\]F /d+- - / -+  ~/ r+ 41/e+~-/ss+ I / i+o /n  9 ,~Figure 6: E-K back-transliteration examt)le4215 Exper imentsExperiments were done in two points of view:the accuracy test and the variation coveragetest.5.1 Test SetsWe use two data sets for an accuracy test.
TestSet I is consists of 1.,650 English and Koreanword pairs that aligned in a phoneme unit.
Itwas made by (Lee and Choi, 1998) and tested bymany methods.
To compare our method withother methods, we use this data set.
We usesame training data (1,500 words) and test data(150 words).
Test Set I I  is consists of 7,185English and Korean word paii's.
We use TestSet H to show the relation between the size oftraining data and the accuracy.
We use 90%of total size as training data and 10% as testdata.
For a variation coverage test, we use TestSet I I I  that is extracted from KTSET 2.0.
TestSet HI  is consists of 2,391 English words andtheir transliterations.
An English word has 1.14various transliterations in average.5.2 Eva luat ion funct ionsAccuracy was measured by the percentage ofthe number of correct transliterations dividedby the number of generated transliterations.
We(:all it as word accuracy(W.A.).
We use onemore measure, called character accuracy(C.A.
)that measures the character edit distance be-tween a correct word and a generated word.no.
of  correct wordsW.A.
= no.
o.f .qenerated words (5)C.A.
= L (6)where L is the length of the original string, andi, d, mid s are the number of insertion, deletionand substitution respectively.
If the dividend isnegative (when L < (i + d + s)), we consider itas zero(Hall and Dowling, 1980).For the real usage test, we used variation cov-erage (V.C.)
that considers various usages.
Weevaluated both tbr the term frequency (tf) anddocument frequency (d J), where tfis the numberof term appearance in the documents and df isthe number of documents that contain the term.If we set the usage tf (or d./) of the translitera-tions to 1 tbr each transliteration, we can calcu-late the transliteration coverage tbr the uniqueword types, single .frequency(.sf).V.C.
= {if ,  df, s f}  of  found  words (7) {t.f, 4f, <f} of  ,sed   o,'ds5.3 Accuracy  testsWe compare our result \[PCa, PUp\] a with thesimple statistical intbrmation based model(Leeand Choi, 1998) \[ST\], the Maxinmm Entropybased model(Tae-il Kim, 2000) \[MEM\], theNeural Network model(Jung-Jae Kim, 1999)INN\] and the Decision %'ee based model(Kang,1999)\[DT\].
Table 3 shows the result of E-K transliteration and back-transliteration testwith Test ,get LTable 3: C.A.
and W.A.
with Test Set IE-K trans.method C.A.
I W.A.ST 69.3% 40.7% 4MEM 72.3% 43.3%NN 79.0% 35.1%DT 78.1% 37.6%Pep 86.5% 55.3%PCa 85.3% 46.7%E-K back trans.C.A.
\[ W.A.60.5%77.1% 31.0%81.4% 34.7%79.3% 32.6%95857565554535Fig.
7, 8 show the results of our proposedmethod with the size of training data, Test SetII.
We compare our result with the decision treebased method.~ - - - - ~  I~C 'A 'PC?
Ii - -~- w.A DT I+ W.A.
POp~ W,A.
BCaJ J J ~ J i1000 2000 3000 4000 5000 6000Figure 7: E-K transliteration results with TestSet HaPC stands for phoneme chunks based method anda and b stands for aligned by an alphabet unit and a1)honeme unit respectively4with 20 higher rank results4229080-706O5O4030 i20 t1000 2000 3000 4000 5000 6000"--?-- C.A, DT---U-- C.A.
PCpC.A.
PCa I!--x- ~A.
Dr iI .
_ _  144A.
POp\]I .~ l~ W,A.
POaFigure 8: E-K back-transliteration results withTest Set HWith 7'c,s't Sc, t H~ we (:m~ get t;15(; fi)llowingresult (Table, 4).Table d: C.A.
and W.A.
with the Test Set HE-K tr~ms.
E-K back tr~ms.method C.A.
14LA.
C.A.
I W.A.PUp \[~9.5% 57.2% 84.9% 40.9%PCa \[19o.6% 58.3% s4.8% 4(/.8%5.4 Var iat ion coverage testsTo (:oml)~re our result(PCp) with (Lee and()hoi, 1998), we tr~fincd our lnethods with thetraining data of Test Set L In ST, (Lee midChoi, 1998) use 20 high rank results, but wej t l s t  l lSe  5 results.
TM)le 5 shows the (:overageof ore: i)rol)osed me.thod.Table 5: w~riation eover~ge with Tc.~t Set II Imethod tf d.f ,~fST 76.0% 73.9% 47.1%PCp 84.0% 84.0% 64.0%Fig.
9 shows the increase of (:overage with thenumber of outputs.5.5 DiscussionWe summarize the, information length ~md thekind of infonnation(Tnble 6).
The results ofexperimenLs and information usage show theftMEM combines w~rious informal;ion betterthan DT and NN.
ST does not list & previousinlmt (el- l)  but use ~ previous output(t,:i_~) tocalculate the current outlml?s probability like95908580706560555O45---,I!
-m-e l f, , , ~ sf1 2 3 4 5 6 7 8 9 10Figure 9: The 17.
C. resultq~,l)le 6: Intbrmation Usageprevious outputST  2 0 YMEM 2 2 NNN \] 1 NDT 3 3 NPC YPart-ofSt)eeeh rl'~gging probleln.
But ST getsthe lowest aecm'acy.
It means that surrmmdingalphal)ei;s give more informed;ion than t)reviousoutlmL.
In other words, E-K trmlslii;e.ration isnot the all)h~bet-per-alphabet or phonenle-per-t)honeme (:lassific~tion problem.
A previousoutI)ut does not give, enough information forcllrrent ltnit's dismnbiguat;ion.
An input millmid an OUtlmt unit shouht be exl:ende(t. E-Ktransliteration is a (:hunk-l)er-chunk classifica-tion prot)lenLWe restri(:t the length of infiwm~tion, to seethe influence of' phoneme-chunk size.
Pig.
10shows the results.i 9oi ~ ~ ~ _ ~ " , ~F 70 6oi5040 / ~ #  .~-- -- C.A.
7bst Sot f 30 / / I-~- c.A.
z~.~ so,, I20 t~ / I~WA To,.t Set / iI o -?-  ~i L;, L,, !x ~01 2 3 4 5 6 7Figure 10: the result of ~ length limit test423With the same length of information, weget the higher C.A.
and W.A.
than othermethods.
It means previous outputs give goodinformation and our chunk-based nmthod isa good combining method.
It also suggeststhat we can restrict he max size of chunk in apermissible size.PCa gets a higher accuracy than PCp.
Itis clue to the number of possible phoneme se-quences.
A transliteration network that con-sists of phoneme nnit has more nodes than atransliteration network that consists of alpha-bet unit.
With small training data, despite ofthe loss due to the phoneme sequences ambi-guity a phoneme gives more intbrmation thanan alphabet.
When the infbrmation is enough,PCa outpertbrms Pep.6 Conc lus ionsWe propose the method of English-to-Koreantransliteration and back-transliteration withmultiple mfl)ounded overlapping phonemechunks.
We showed that E-K transliterationand back-transliteration are not a t)honeme-per-phoneme and alphabet-per-alphabetclassification problem.
So we use phonemechunks that do not have a length limit andcan explain E-K transliteration phenomena.We get the reliability of a given transliter-ation phenomenon by applying overlapt)ingphoneme chunks.
Our method is simple anddoes not need a complex combining methodtbr w, rious length of information.
The changeof an intbrmation length does not affect theinternal representation f the problem.
Ourchunk-based method can be used to otherclassification problems and can give a simplecombining method.ReferencesTae-il Kim.
2000.
English to Korean translit-eration model using maxinmm entropy modelfor cross language information retrieval.
Mas-ter's thesis, Seogang University (in Korean).Kil Soon aeong, Sllng Hyun Myaeng, Jae SungLee, and Key-Sun Choi.
1999.
Automaticidentification and back-transliteration of for-eign words tbr information retrieval, b~:for-mation Processing and Management.Key-Sun Choi Jung-Jae Kim, Jae Sung Lee.1999.
Pronunciation unit based automaticEnglish-Korean transliteration model usingneural network.
In Pwceedings of Korea Cog-nitive Science Association(in Korean).Byung-Ju Kang.
1999.
Automatic Korean-English back-transliteration.
I  Pwecedingsof th, c 11th, Conference on Iiangul and Ko-rean Language Information Prvcessing( in Ko-Fean).Jae Sung Lee and Key-Sun Choi.
1998.
Englishto Korean statistical transliteration for in-formation retrieval.
Computer PTvcessin9 ofOriental Languages.K.
Jeong, Y. Kwon, and S. H. Myaeng.
1997.The effect of a proper handling of foreign andEnglish words in retrieving Korean text.
InProceedings of the 2nd Irdernational Work-shop on lrtforrnation Retrieval with AsianLanguages.K.
Knight and J. Graehl.
1997.
Machinetransliteration.
In Proceedings o.f the 35thAnnual Meeting of the Association J'or Com-putational Linguistics.Adam L. Berger, Stephen A. Della Pietra, andVincent J. Della Pietra.
1996.
A maximumentroI)y approach to natural language pro-cessing.
Computational Linguistics.Y.
C. Park, K. Choi, J. Kim, and Y. Kim.1996.
Development of the data collection ver.2.0ktset2.0 tbr Korean intbrmation retrievalstudies.
In Artificial Intelligence Spring Con-ference.
Korea Intbrmation Science Society(in Korean).Frank K. Soong and Eng-Fong Huang.
1991.A tree-trellis based Nst search for tindingthe n best sentence hypotheses in eontimmusspeech recognition.
In IEEE InternationalConference on Acoustic Speech and SignalPwcessing, pages 546-549.P.
Hall and G. Dowling.
1980.
Approximatestring matching.
Computing Surveys.424
