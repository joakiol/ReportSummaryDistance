Identifying Repair Targets in Action Control DialogueKotaro Funakoshi and Takenobu TokunagaDepartment of Computer Science,Tokyo Institute of Technology2-12-1 Oookayama Meguro, Tokyo, JAPAN{koh,take}@cl.cs.titech.ac.jpAbstractThis paper proposes a method for deal-ing with repairs in action control dialogueto resolve participants?
misunderstanding.The proposed method identifies the re-pair target based on common groundingrather than surface expressions.
We extendTraum?s grounding act model by introduc-ing degree of groundedness, and partialand mid-discourse unit grounding.
Thispaper contributes to achieving more natu-ral human-machine dialogue and instanta-neous and flexible control of agents.1 IntroductionIn natural language dialogue, misunderstandingand its resolution is inevitable for the naturalcourse of dialogue.
The past research dealingwith misunderstanding has been focused on the di-alogue involving only utterances.
In this paper,we discuss misunderstanding problem in the di-alogue involving participant?s actions as well asutterances.
In particular, we focus on misunder-standing in action control dialogue.Action control dialogue is a kind of task-oriented dialogue in which a commander con-trols the actions1 of other agents called followersthrough verbal interaction.This paper deals with disagreement repair ini-tiation utterances2 (DRIUs) which are used bycommanders to resolve followers?
misunderstand-ings3, or to correct commanders?
previous erro-neous utterances.
These are so called third-turn1We use the term ?action?
for the physical behavior ofagents except for speaking.2This denomination is lengthy and may be still controver-sial.
However we think this is most descriptively adequate forthe moment.3Misunderstanding is a state where miscommunicationhas occurred but participants are not aware of this, at leastinitially (Hirst et al, 1994).repair (Schegloff, 1992).
Unlike in ordinary dia-logue consisting of only utterances, in action con-trol dialogue, followers?
misunderstanding couldbe manifested as their inappropriate actions in re-sponse to a given command.Let us look at a sample dialogue (1.1 ?
1.3).
Ut-terance (1.3) is a DRIU for repairing V?s mis-understanding of command (1.1) which is mani-fested by his action performed after saying ?OK?in (1.2).
(1.1) U: Put the red book on the shelf to the right.
(1.2) V: OK. <V performs the action>(1.3) U: Not that.It is not easy for machine agents to under-stand DRIUs because they can sometimes be soelliptical and context-dependent that it is difficultto apply traditional interpretation methodology toDRIUs.In the rest of this paper, we describe the dif-ficulty of understanding DRIUs and propose amethod to identify repair targets.
The identifica-tion of repair targets plays a key role in under-standing DRIUs and this paper is intensively fo-cused on this issue.2 Difficulty of Understanding DRIUsUnderstanding a DRIU consists of repair tar-get identification and repair content interpretation.Repair target identification identifies a target to berepaired by the speaker?s utterance.
Repair con-tent interpretation recovers the speaker?s intentionby replacing the identified repair target with thecorrect one.One of the major source of difficulties in un-derstanding DRIUs is that they are often elliptical.Repair content interpretation depends heavily onrepair targets but the information to identify re-pair targets is not always mentioned explicitly inDRIUs.Let us look at dialogue (1.1 ?
1.3) again.
TheDRIU (1.3) indicates that V failed to identify U?sintended object in utterance (1.1).
However, (1.3)does not explicitly mention the repair target, i.e.,either book or shelf in this case.The interpretation of (1.3) changes dependingon when it is uttered.
More specifically, the inter-pretation depends on the local context and the sit-uation when the DRIU is uttered.
If (1.3) is utteredwhen V is reaching for a book, it would be natu-ral to consider that (1.3) is aimed at repairing V?sinterpretation of ?the book?.
On the other hand,if (1.3) is uttered when V is putting the book on ashelf, it would be natural to consider that (1.3) isaimed at repairing V?s interpretation of ?the shelfto the right?.Assume that U uttered (1.3) when V was puttinga book in his hand on a shelf, how can V identifythe repair target as shelf instead of book?
This pa-per explains this problem on the basis of commongrounding (Traum, 1994; Clark, 1996).
Commongrounding or shortly grounding is the process ofbuilding mutual belief among a speaker and hear-ers through dialogue.
Note that in action controldialogue, we need to take into account not onlyutterances but also followers?
actions.
To identifyrepair targets, we keep track of states of groundingby treating followers?
actions as grounding acts(see Section 3).
Suppose V is placing a book inhis hand on a shelf.
At this moment, V?s inter-pretation of ?the book?
in (1.1) has been alreadygrounded, since U did not utter any DRIU whenV was taking the book.
This leads to the interpre-tation that the repair target of (1.1) is shelf ratherthan already grounded book.3 GroundingThis section briefly reviews the grounding actsmodel (Traum, 1994) which we adopted in ourframework.
We will extend the grounding actmodel by introducing degree of groundedness thathave a quaternary distinction instead of the orig-inal binary distinction.
The notions of partialgrounding and mid-discourse unit grounding arealso introduced for dealing with action control di-alogue.3.1 Grounding Acts ModelThe grounding acts model is a finite state transi-tion model to dynamically compute the state ofgrounding in a dialogue from the viewpoint ofeach participant.This theory models the process of groundingwith a theoretical construct, namely the discourseunit (DU).
A DU is a sequence of utterance units(UUs) assigned grounding acts (GAs).
Each UUin a dialogue has at least one GA, except fillers orseveral cue phrases, which are considered usefulfor turn taking but not for grounding.
Each DUhas an initiator (I) who opened it, and other par-ticipants of that DU are called responders (R).Each DU is in one of seven states listed in Ta-ble 1 at a time.
Given one of GAs shown in Table 2as an input, the state of DU changes according tothe current state and the input.
A DU starts witha transition from initial state S to state 1, and fin-ishes at state F or D. DUs in state F are regardedas grounded.Analysis of the grounding process for a sam-ple dialogue is illustrated in Figure 1.
Speaker Bcan not understand the first utterance by speakerA and requests a repair (ReqRep-R) with his ut-terance.
Responding to this request, A makes arepair (Repair-I).
Finally, B acknowledges toshow he has understood the first utterance and thediscourse unit reaches the final state, i.e., state F.State DescriptionS Initial state1 Ongoing2 Requested a repair by a responder3 Repaired by a responder4 Requested a repair by the initiatorF FinishedD CanceledTable 1: DU statesGrounding act DescriptionInitiate Begin a new DUContinue Add related contentAck Present evidences of understandingRepair Correct misunderstandingReqRepair Request a repair actReqAck Request an acknowledge actCancel Abandon the DUTable 2: Grounding actsUU DU1A : Can I speak to Jim Johnstoneplease?Init-I 1B : Senior?
ReqRep-R 2A : Yes Repair-I 1B : Yes Ack-R FFigure 1: An example of grounding (Ishizaki andDen, 2001)1783.2 Degree of Groundedness and EvidenceIntensityAs Traum admitted, the binary distinction betweengrounded and ungrounded in the grounding actsmodel is an oversimplification (Traum, 1999).
Re-pair target identification requires more finely de-fined degree of groundedness.
The reason for thiswill be elucidated in Section 5.Here, we will define the four levels of evidenceintensity and equate these with degrees of ground-edness, i.e., if an utterance is grounded with evi-dence of level N intensity, the degree of ground-edness of the utterance is regarded as level N .
(2) Levels of evidence intensityLevel 0: No evidence (i.e., not grounded).Level 1: The evidence shows that the re-sponder thinks he understood the utter-ance.
However, it does not necessar-ily mean that the responder understoodit correctly.
E.g., the acknowledgment?OK?
in response to the request ?turn tothe right.
?Level 2: The evidence shows that the re-sponder (partially) succeeded in trans-ferring surface level information.
It doesnot yet ensure that the interpretation ofthe surface information is correct.
E.g.,the repetition ?to the right?
in responseto the request ?turn to the right.
?Level 3: The evidence shows that the re-sponder succeeded in interpretation.E.g., turning to the right as the speakerintended in response to the request ?turnto the right.
?3.3 Partial and mid-DU GroundingIn Traum?s grounding model, the content of a DUis uniformly grounded.
However, things in thesame DU should be more finely grounded at var-ious levels individually.
For example, if one ac-knowledged by saying ?to the right?
in responseto the command ?put the red chair to the right ofthe table?, to the right of should be regarded asgrounded at Level 2 even though other parts of therequest are grounded at Level 1.In addition, in Traum?s model, the content of aDU is grounded all at once when the DU reachesthe final state, F. However, some elements in a DUcan be grounded even though the DU has not yetreached state F. For example, if one requested arepair as ?to the right of what??
in response tothe command ?put the red chair to the right ofthe table?, to the right of should be regarded asgrounded at level 2 even though table has not yetbeen grounded.Although Traum admitted these problems ex-isted in his model, he retained it for the sake ofsimplicity.
However, such partial and mid-DUgrounding is necessary to identify repair targets.We will describe the usage of these devices toidentify repair targets in Section 5.
In brief, whena level 3 evidence is presented by the follower andnegative feedback (i.e., DRIUs) is not provided bythe commander, only propositions supported bythe evidence are considered to be grounded eventhough the DU has not yet reached state F.4 Treatment of Actions in DialogueIn general, past work on discourse has targeted di-alogue consisting of only utterances, or has con-sidered actions as subsidiary elements.
In contrast,this paper targets action control dialogue, whereactions are considered to be primary elements ofdialogue as well as utterances.Two issues have to be mentioned for handlingaction control dialogue in the conventional se-quential representation as in Figure 1.
We will in-troduce assumptions (3) and (4) as shown below.Overlap between utterances and actionsActions in dialogue do not generally obey turnallocation rules as Clark pointed out (Clark, 1996).In human-human action control dialogue, follow-ers often start actions in the middle of a comman-der?s utterance.
This makes it difficult to analyzediscourse in sequential representation.
Given thisfact, we impose the three assumptions on follow-ers as shown in (3) so that followers?
actions willnot overlap the utterances of commanders.
Theserequirements are not unreasonable as long as fol-lowers are machine agents.
(3) Assumptions on follower?s actions(a) The follower will not commence actionuntil turn taking is allowed.
(b) The follower immediately stops the ac-tion when the commander interruptshim.
(c) The follower will not make action as pri-mary elements while speaking.
44We regard gestures such as pointing as secondary ele-179Hierarchy of actionsAn action can be composed of several sub-actions, thus has a hierarchical structure.
For ex-ample, making tea is composed of boiling the wa-ter, preparing the tea pot, putting tea leaves in thepot, and pouring the boiled water into it, and soon.
To analyze actions in dialogue as well as ut-terances in the traditional way, a unit of analysisshould be determined.
We assume that there is acertain granularity of action that human can recog-nize as primitive.
These actions would correspondto basic verbs common to humans such as ?walk?,?grasp?, ?look?, etc.We call these actions funda-mental actions and consider them as UUs in actioncontrol dialogue.
(4) Assumptions on fundamental actionsIn the hierarchy of actions, there is a cer-tain level consisting of fundamental actionsthat human can commonly recognize as prim-itives.
Fundamental actions can be treated asunits of primary presentations in an analogywith utterance units .5 Repair Target IdentificationIn this section, we will discuss how to identify therepair target of a DRIU based on the notion ofgrounding.
The following discussion is from theviewpoint of the follower.Let us look at a sample dialogue (5.1 ?
5.5),where U is the commander and V is the fol-lower.
The annotation Ack1-R:F in (5.2) meansthat (5.2) has grounding act Ack by the respon-der (R) for DU1 and the grounding act made DU1enter state F. The angle bracketed descriptions in(5.3) and (5.4) indicate the fundamental actions byV.Note that thanks to assumption (4) in Section 4,a fundamental action itself can be considered as aUU even though the action is performed withoutany utterances.
(5.1) U: Put the red ball on the left box.
(Init1-I:1)(5.2) V: Sure.
(Ack1-R:F)(5.3) V: <V grasps the ball> (Init2-I:1)(5.4) V: <V moves the ball> (Cont2-I:1)(5.5) U: Not that.
(Repair1-R:3)The semantic content of (5.1) can be repre-sented as a set of propositions as shown in (6).ments when they are presented in parallel with speech.
There-fore, this constraint does not apply to them.
(6) ?
= Request(U, V, Put(#Agt1, #Obj1, #Dst1))(a) speechActType(?
)=Request(b) presenter(?
)=U(c) addressee(?
)=V(d) actionType(content(?
))=Put(e) agent(content(?
))=#Agt1,referent(#Agt1)=V(f) object(content(?
))=#Obj1,referent(#Obj1)=Ball1(g) destination(content(?))=#Dst1,referent(#Dst1)=Box1?
represents the entire content of (5.1).
Sym-bols beginning with a lower case letter are func-tion symbols.
For example, (6a) means the speechact type for ?
is ?Request?.
Symbols beginningwith an upper case letter are constants.
?Request?is the name of a speech act type and ?Move?
isthat of fundamental action respectively.
U and Vrepresents dialogue participants and ?Ball1?
rep-resents an entity in the world.
Symbols beginningwith # are notional entities introduced in the dis-course and are called discourse referents.
A dis-course referent represents something referred tolinguistically.
During a dialogue, we need to con-nect discourse referents to entities in the world, butin the middle of the dialogue, some discourse ref-erents might be left unconnected.
As a result wecan talk about entities that we do not know.
How-ever, when one takes some actions on a discoursereferent, he must identify the entity in the world(e.g., an object or a location) corresponding to thediscourse referent.
Many problems in action con-trol dialogue are caused by misidentifying entitiesin the world.Follower V interprets (5.1) to obtain (6), andprepares an action plan (7) to achieve ?Put(#Agt1,#Obj1, #Dst1)?.
Plan (7) is executed downwardfrom the top.
(7) Plan for Put(#Agt1, #Obj1, #Dst1)Grasp(#Agt1, #Obj1),Move(#Agt1, #Obj1, #Dst1),Release(#Agt1, #Obj1)Here, (5.1 ?
5.5) are reformulated as in (8.1 ?8.5).
?Perform?
represents performing the action.
(8.1) U: Request(U, V, Put(#Agt1, #Obj1, #Dst1))(8.2) V: Accept(V, U, ?
)(8.3) V: Perform(V, U, Grasp(#Agt1, #Obj1))180(8.4) V: Perform(V, U,Move(#Agt1, #Obj1, #Dst1))(8.5) U: Inform(U, V, incorrect(X))To understand DRIU (5.5), i.e., (8.5), followerV has to identify repair target X in (8.5) referredto as ?that?
in (5.5).
In this case, the repair targetof (5.5) X is ?the left box?, i.e., #Dst1.5 However,the pronoun ?that?
cannot be resolved by anaphoraresolution only using textual information.We treat propositions, or bindings of variablesand values, such as (6a ?
6g), as the minimumgranularity of grounding because the identificationof repair targets requires that granularity.
We thenmake the following assumptions concerning repairtarget identification.
(9) Assumptions on repair target identification(a) Locality of elliptical DRIUs: The targetof an elliptical DRIU that interrupted thefollower?s action is a proposition that isgiven an evidence of understanding bythe interrupted action.
(b) Instancy of error detection: A dialogueparticipant observes his dialogue con-stantly and actions presenting strong ev-idence (Level 3).
Thus, when there is anerror, the commander detects it immedi-ately once an action related to that erroroccurs.
(c) Instancy of repairs: If an error isfound, the commander immediately in-terrupts the dialogue and initiates a re-pair against it.
(d) Lack of negative evidence as positiveevidence: The follower can determinethat his interpretation is correct if thecommander does not initiates a repairagainst the follower?s action related tothe interpretation.
(e) Priority of repair targets: If there areseveral possible repair targets, the leastgrounded one is chosen.
(9a) assumes that a DRIU can only be ellipti-cal when it presupposes the use of local context toidentify its target.
It also predicts that if the targetof a repair is neither local nor accessible withinlocal information, the DRIU will not be ellipticaldepending on local context but contain explicit and5We assume that there is a sufficiently long interval be-tween the initiations of (5.4) and (5.5).sufficient information to identify the target.
(9b)and (9c) enable (9a).Nakano et al (2003) experimentally confirmedthat we observe negative responses as well as pos-itive responses in the process of grounding.
Ac-cording to their observations, speakers continuedialogues if negative responses are not found evenwhen positive responses are not found.
This evi-dence supports (9d).An intuitive rationale for (9e) is that an issuewith less proof would more probably be wrongthan one with more proof.Now let us go through (8.2) to (8.5) again ac-cording to the assumptions in (9).
First, ?
isgrounded at intensity level 1 by (8.2).
Second, Vexecutes Grasp(#Agt1, #Obj1) at (8.3).
BecauseV does not observe any negative response from Ueven after this action is completed, V considersthat the interpretations of #Agt1 and #Obj1 havebeen confirmed and grounded at intensity level 3according to (9d) (this is the partial and mid-DUgrounding mentioned in Section 3.3).
After initiat-ing Move(#Agt1, #Obj1, #Dst1), V is interruptedby commander U with (8.5) in the middle of theaction.V interprets elliptical DRIU (5.5) as ?Inform(S,T, incorrect(X))?, but he cannot identify repair tar-get X.
He tries to identify this from the discoursestate or context.
According to (9a), V assumes thatthe repair target is a proposition that its interpre-tation is demonstrated by interrupted action (8.4).Due to the nature of the word ?that?, V knows thatpossible candidates are not types of action or thespeech act but discourse referents #Agt1, #Obj1and #Dst16.
Here, #Agt1 and #Obj1 have beengrounded at intensity level 3 by the completion of(8.3).
Now, (9e) tells V that the repair target is#Dst1, which has only been grounded at intensitylevel 1 7.
(10) below summarizes the method of repair tar-get identification based on the assumptions in (9).
(10) Repair target identification6We have consistently assumed Japanese dialogues in thispaper although examples have been translated into English.?That?
is originally the pronoun ?sotti?
in Japanese, whichcan only refer to objects, locations, or directions, but cannotrefer to actions.7There are two propositions concerned with #Dst1:destination(content(?))
= #Dst1 and referent(#Dst1) = Box1.However if dest(content(?))
= #Dst1 is not correct, thismeans that V grammatically misinterpreted (8.1).
It seemshard to imagine for participants speaking in their mothertongue and thus one can exclude dest(content(?))
= #Dst1from the candidates of the repair target.181(a) Specify the possible types of the repairtarget from the linguistic expression.
(b) List the candidates matching the typesdetermined in (10a) from the latest pre-sented content.
(c) Rank candidates based on groundednessaccording to (9e) and choose the topranking one.Dependencies between ParametersThe follower prepares an action plan to achievethe commander?s command as in plan (7).
Here,the planned actions can contain parameters not di-rectly corresponding to the propositions given bythe commander.
Sometimes a selected parameterby using (10) is not the true target but the depen-dent of the target.
Agents must retrieve the truetarget by recognizing dependencies of parameters.For example, assume a situation where objectsare not within the follower?s reach as shown inFigure 2.
Then, the commander issues command(6) to the follower (Agent1 in Figure 2) and heprepares an action plan (11).
(11) Agent1?s plan (partial) for (6) in Figure 2.Walk(#Agt1, #Dst1),Grasp(#Agt1, #Obj1),.
.
.The first Walk is a prerequisite action for Graspand #Dst1 depends on #Obj1.
In this case, if refer-ent(#Obj1) is Object1 then referent(#Dst1) is Po-sition1, or if referent(#Obj1) is Object2 then ref-erent(#Dst1) is Position2.
Now, assume that thecommander intends referent(#Obj1) to be Object2with (6), but the follower interprets this as refer-ent(#Obj1) = Object1 (i.e., referent(#Dst1) = Po-sition1) and performs Walk(#Agt1, #Dst1).
Thecommander then observes the follower moving to-ward a direction different from his expectation andinfers the follower has misunderstood the targetobject.
He, then, interrupts the follower with theutterance ?not that?
at the timing illustrated in Fig-ure 3.
Because (10c) chooses #Dst2 as the repairtarget, the follower must be aware of the depen-dencies between parameters #Dst1 and #Obj1 tonotice his misidentification of #Obj1.6 Implementation and Some ProblemsWe implemented the repair target identificationmethod described in Section 5 into our prototypePosition1?Agent1 Object1 (wrong)Object2 (correct)?Position2Figure 2: Situation with dependent parametersTimeWalk(#Agt1, #Dst1) Grasp(#Agt1, #Obj1)" Not that "Figure 3: Dependency between parametersdialogue system (Figure 4).
The dialogue systemhas animated humanoid agents in its visualized 3Dvirtual world.
Users can command the agent byspeech to move around and relocate objects.Figure 4: Snapshot of the dialogue systemBecause our domain is rather small, current pos-sible repair targets are agents, objects and goalsof actions.
According to the qualitative evalua-tion of the system through interaction with sev-eral subjects, most of the repair targets were cor-rectly identified by the proposed method describedin Section 5.
However, through the evaluation, wefound several important problems to be solved asbelow.6.1 Feedback DelayIn a dialogue where participants are paying atten-tion to each other, the lack of negative feedbackcan be considered as positive evidence (see (9d)).However, it is not clear how long the system needsto wait to consider the lack of negative feedback aspositive evidence.
In some cases, it will be not ap-propriate to consider the lack of negative feedback182as positive evidence immediately after an actionhas been completed.
Non-linguistic informationsuch as nodding and gazing should be taken intoconsideration to resolve this problem as (Nakanoet al, 2003) proposed.Positive feedback is also affected by delay.When one receives feedback shortly after an actionis completed and begins the next action, it may bedifficult to determine whether the feedback is di-rected to the completed action or to the just startedaction.6.2 Visibility of ActionsThe visibility of followers?
actions must be con-sidered.
If the commander cannot observe the fol-lower?s action due to environmental conditions,the lack of negative feedback cannot be positiveevidence for grounding.For example, assume the command ?bring mea big red cup from the next room?
is given andassume that the commander cannot see the insideof the next room.
Because the follower?s funda-mental action of taking a cup in the next room isinvisible to the commander, it cannot be groundedat that time.
They have to wait for the return of thefollower with a cup.6.3 Time-dependency of GroundingUtterances are generally regarded as points on thetime-line in dialogue processing.
However, thisapproximation cannot be applied to actions.
Oneaction can present evidences for multiple propo-sitions but it will present these evidences at con-siderably different time.
This affects repair targetidentification.Let us look at an action Walk(#Agt, #Dst),where agent #Agt walks to destination #Dst.
Thisaction will present evidence for ?who is the in-tended agent (#Agt)?
at the beginning.
However,the evidence for ?where is the intended position(#Dst)?
will require the action to be completed.However, if the position intended by the followeris in a completely different direction from the oneintended by the commander, his misunderstandingwill be evident at a fairly early stage of the action.6.4 Differences in Evidence Intensitiesbetween ActionsEvidence intensities vary depending on the char-acteristics of actions.
Although the symbolic de-scription of actions such as (12) and (13) does notexplicitly represent differences in intensity, thereis a significant difference between (12) where#Agent looks at #Object at a distance, and (13)where #Agent directly contacts #Object.
Agentsmust recognize these differences to conform withhuman recognition and share the same state ofgrounding with participants.
(12) LookAt(#Agent, #Object)(13) Grasp(#Agent, #Object)6.5 Other Factors of Confidence inUnderstandingPerforming action can provide strong evidence ofunderstanding and such evidence enables partic-ipants to have strong confidence in understand-ing.
However, other factors such as linguistic con-straints (not limited to surface information) andplan/goal inference can provide confidence in un-derstanding without grounding.
Such factors ofconfidence also must be incorporated to explainsome repairs.Let us see a sample dialogue below, and assumethat follower V missed the word red in (14.3).
(14.1) U: Get the white ball in front of the table.
(14.2) V: OK. <V takes a white ball>(14.3) U: Put it on the (red) table.
(14.4) V: Sure.
<V puts the white ball holding inhis hand on a non-red table>(14.5) U: I said red.When commander U repairs V?s misunder-standing by (14.5), V cannot correctly decide thatthe repair target is not ?it?
but ?the (red) table?
in(14.3) by using the proposed method, because thereferent of ?it?
had already been in V?s hand andno explicit action choosing a ball was performedafter (14.3).
However, in such a situation we seemto readily doubt misunderstanding of ?the table?because of strong confidence in understanding of?it?
that comes from outside of grounding process.Hence, we need a unified model of confidence inunderstanding that can map different sources ofconfidence into one dimension.
Such a model isalso useful for clarification management of dia-logue systems.7 Discussion7.1 Advantage of Proposed MethodThe method of repair target identification pro-posed in this paper less relies on surface infor-mation to identify targets.
This is advantageous183against some sort of misrecognitions by automaticspeech recognizers and contributes to the robust-ness of spoken dialogue systems.Only surface information is generally insuffi-cient to identify repair targets.
For example, as-sume that there is an agent acting in response to(15) and his commander interrupts him with (16).
(15) Put the red ball on the table(16) Sorry, I meant blueIf one tries to identify the repair target with sur-face information, the most likely candidate willbe ?the red ball?
because of the lexical similar-ity.
Such methods easily break down.
They can-not deal with (16) after (17).
If, however, one paysattention to the state of grounding as our proposedmethod, he can decide which one is likely to be re-paired ?the red ball?
or ?the green table?
depend-ing on the timing of the DRIU.
(17) Put the red ball on the green table7.2 Related WorkMcRoy and Hirst (1995) addressed the detectionand resolution of misunderstandings on speechacts using abduction.
Their model only dealt withspeech acts and did not achieve our goals.Ardissono et al (1998) also addressed the sameproblem but with a different approach.
Theirmodel could also handle misunderstanding regard-ing domain level actions.
However, we think thattheir model using coherence to detect and resolvemisunderstandings cannot handle DRIUs such as(8.5), since both possible repairs for #Obj1 and#Dst1 have the same degree of coherence in theirmodel.Although we did not adopt this, the notion ofQUD (questions under discussion) proposed byGinzburg (Ginzburg, 1996) would be another pos-sible approach to explaining the problems ad-dressed in this paper.
It is not yet clear whetherQUD would be better or not.8 ConclusionIdentifying repair targets is a prerequisite to un-derstand disagreement repair initiation utterances(DRIUs).
This paper proposed a method to iden-tify the target of a DRIU for conversational agentsin action control dialogue.
We explained how a re-pair target is identified by using the notion of com-mon grounding.
The proposed method has beenimplemented in our prototype system and eval-uated qualitatively.
We described the problemsfound in the evaluation and looked at the futuredirections to solve these problems.AcknowledgmentThis work was supported in part by the Ministry ofEducation, Science, Sports and Culture of Japan asthe Grant-in-Aid for Creative Basic Research No.13NP0301.ReferencesL.
Ardissono, G. Boella, and R. Damiano.
1998.
Aplan based model of misunderstandings in cooper-ative dialogue.
International Journal of Human-Computer Studies, 48:649?679.Herbert H. Clark.
1996.
Using Language.
CambridgeUniversity Press.Jonathan Ginzburg.
1996.
Interrogatives: ques-tions, facts and dialogue.
In Shalom Lappin, editor,The Handbook of Contemporary Semantic Theory.Blackwell, Oxford.G.
Hirst, S. McRoy, P. Heeman, P. Edmonds, andD.
Horton.
1994.
Repairing conversational misun-derstandings and non-understandings.
Speech Com-munication, 15:213?230.Masato Ishizaki and Yasuharu Den.
2001.
Danwato taiwa (Discourse and Dialogue).
University ofTokyo Press.
(In Japanese).Susan Weber McRoy and Graeme Hirst.
1995.
The re-pair of speech act misunderstandings by abductiveinference.
Computational Linguistics, 21(4):435?478.Yukiko Nakano, Gabe Reinstein, Tom Stocky, and Jus-tine Cassell.
2003.
Towards a model of face-to-facegrounding.
In Erhard Hinrichs and Dan Roth, edi-tors, Proceedings of the 41st Annual Meeting of theAssociation for Computational Linguistics, pages553?561.E.
A Schegloff.
1992.
Repair after next turn: Thelast structurally provided defense of intersubjectiv-ity in conversation.
American Journal of Sociology,97(5):1295?1345.David R. Traum.
1994.
Toward a ComputationalTheory of Grounding.
Ph.D. thesis, University ofRochester.David R. Traum.
1999.
Computational models ofgrounding in collaborative systems.
In WorkingPapers of AAAI Fall Symbosium on PsychologicalModels of Communication in Collaborative Systems,pages 137?140.184
