Web Search Intent Induction via Automatic Query ReformulationHal Daume?
IIIInformation Sciences Institute4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292hdaume@isi.eduEric BrillMicrosoft ResearchOne Microsoft WaySeattle, WA 98052brill@microsoft.comAbstractWe present a computationally efficient methodfor automatic grouping of web search resultsbased on reformulating the original query to al-ternative queries the user may have intended.The method requires no data other than querylogs and the standard inverted indices used bymost search engines.
Our method outperformsstandard web search in the task of enablingusers to quickly find relevant documents for in-formational queries.1 Introduction and MotivationIn a study of web search query logs, Broder (2002) ob-served that most queries fall into one of three basic cate-gories: navigational, informational and transactional.
Anavigational query is one where the user has a particu-lar URL they are attempting to find.
An informationalquery is one where the user has a particular informationneed to satisfy.
A transactional query is one in which theuser seeks to perform some sort of web-mediated activity(such as purchasing a product).In that paper, Broder (2002) also confirms that mostqueries are very short: on the order of two words.
For in-formational queries this is often inadequate.
The brevityof queries is often due to the fact that the user does notknow exactly what he is looking for.
This makes it dif-ficult for him to formulate enough, or even correct, key-words.
These types of queries make up anywhere be-tween 39 and 48 percent of all web queries, according toBroder, making them a prime target for research.2 Prior WorkOur interest is in informational queries.
The general ap-proach we explore to assist users find what they want isto present structured results.
Dumais et.
al.
(2001) haveshown that displaying structured results improves a user?sability to find relevant documents quickly.There are three general techniques for presenting websearch results in a structured manner, ranging from to-tally supervised methods to totally unsupervised meth-ods.
The first approach, manual classication, is typi-fied by a system like Yahoo!, where humans have cre-ated a hierarchical structure describing the web and man-ually classify web pages into this hierarchy.
The secondapproach, automatic classication (see, for instance, theclassification system reported by Dumais (2000)) buildson the hierarchies constructed for manual classificationsystems, but web pages are categorized by a (machine-learned) text classification system.
The third approach,typified by systems such as Vivisimo and the system ofZamir et al (1999), look at the text of the returned docu-ments and perform document clustering.A related unsupervised approach to this problem isfrom Beeferman and Berger (2000).
Their approachleverages click-through data to cluster related queries.The intuition behind their method is that if two differ-ent queries lead to users clicking on the same URL, thenthese queries are related (and vice-versa).
They per-form agglomerative clustering to group queries, based onclick-through data.Our approach is most closely related to this agglom-erative clustering approach, but does not require click-through data.
Moreover, the use of click-through datacan result in query clusters with low user utility (see Sec-tion 3.2).
Furthermore, our approach does not suffer fromthe computation cost of document clustering by text andproduces structured results with meaningful names with-out the economic cost of building hierarchies.3 MethodologyOur goal is to provide a range of possible needs to auser whose query is underspecified.
Suppose a naiveuser John enters a query for ?fly fishing.?
This querywill retrieve a large set of documents.
We assume thatJohn?s search need (information about flies for catchingtrout) is somewhere in or near this set, but we do notknow exactly where.
However, we can attempt to iden-tify other queries, made by other people, that are relevantto John?s need.
We refer to this process as Query DrivenSearch Expansion and henceforth refer to our system asthe QDSE system.3.1 Formal SpecificationFormally, if Q is the set of queries to our search engineand D is the set of indexed documents, let R be a binaryrelation on Q?D where qRd if and only if d is in the re-turn set for the query q.
It is likely that the set of relatedqueries is quite large for a given q (in practice the sizeis on the order of ten thousand; for our dataset, ?fly fish-ing?
has 29, 698 related queries).
However, some of thesequeries will be only tangentially related to q. Moreover,some of them will be very similar to each other.
In orderto measure these similarities, we define a distance met-ric between two queries q and q?
based on their returneddocument sets, ignoring the text of the query:?q, q??
= 1 ?
|R[q] ?
R[q?
]||R[q] ?
R[q?
]| (1)One could then sort the set of related queries accordingto ?q, q??
and present the top few to the user.
Unfortu-nately, this is insufficient: the top few are often too sim-ilar to each other to provide any new useful information.To get around this problem, we use the maximal marginalrelevance (MMR) scheme originally introduced by Car-bonell et.
al.
(1998).
In doing so, we order alternativequereies according to:argminq?[?
?q, q??
?
(1 ?
?)
minq??
?q?, q???
](2)where q?s are drawn from unreturned query expansionsand q?
?s are drawn from the previously returned set.13.2 Alternative Distance MetricsOne particular thing to note in Equation 1 is that we donot take relative rankings into account in calculating dis-tance.
One could define a distance metric weighted byeach document?s position in the return list.We ran experiments using PageRank to weight the dis-tance (calculated based on a recent full web crawl).
Sys-tem output was observed to be significantly inferior to thestandard ranking.
We attribute this degradation to the fol-lowing: if two queries agree only on their top documents,they are too similar to be worth presenting to the user asalternatives.
This is the same weakness as is found in theBeeferman and Berger (2000) approach.4 SystemThe system described above functions in a completelyautomatic fashion and responds in real-time to usersqueries.
Across the top of the return results, the query1Queries that appear to be URLs, and strings with a verysmall edit distance to the original are discarded.is listed, as are the top ranked alternative queries.
Eachof these query suggestions is a link to a heading, whichare shown below.
Below this list are the top five searchresult links from MSN Search under the original query2.After the top five results from MSN Search, we displayeach header with a +/- toggle to expand or collapse it.Under each expanded query we list its top 4 results.5 Evaluation SetupEvaluating the results of search engine algorithms with-out embedding these algorithms in an on-line system isa challenge.
We evaluate our system against a standardweb search algorithm (in our case, MSN Search).
Ide-ally, since our system is focused on informational queries,we would like a corpus of ?query, intent?
pairs, where thequery is underspecified.
One approach would be to createthis corpus ourselves.
However, doing so would bias theresults.
An alternative would be to use query logs; unfor-tunately, these do not include intents.
In the next section,we explain how we create such pairs.5.1 Deriving Query/Intent PairsWe have a small collection of click-through data, basedon experiments run at Microsoft Research over the pastyear.
Given this data, for a particular user and query,we look for the last URL they clicked on and viewedfor at least two minutes3.
We consider all of these doc-uments to be satisfactory solutions for the user?s searchneed.
We further discard pairs that were in the top fivebecause we intend to use these pairs to evaluate our sys-tem against vanilla MSN Search.
Since the first five re-sults our system returns are identical to the first five re-sults MSN Search returns, it is not worthwhile annotatingthese data-points (this resulted in a removal of about 20%of the data, most of which were navigational queries).These ?query, URL?
pairs give us a hint at how to get tothe desired ?query, intent?
pairs.
For each ?query, URL?pair, we looked at the query itself and the web page at theURL.
Given the query, the relevant URL and the top fiveMSN Search results, we attempted to create a reasonablesearch intent that was (a) consistent with the query andthe URL, but (b) not satisfied by any of the top five re-sults.
There were a handful of cases (approximately anadditional 5%) where we could not think of a reasonableintent for which (b) held ?
in these cases, we discardedthat pair.4 In all, we created 52 such pairs; four randomly2The top five queries originally returned by MSN Search areincluded because there is a chance the user knew what he wasdoing and actually entered a good query.3It may be the case that the users found an earlier URL alsoto be relevant.
This does not concern us, as we do not actuallyuse these URLs for evaluation purposes ?
we simply use themto gain insight into intents.4We make no claim that the intents we derive were neces-sarily the original intent in the mind of the user.
We only gothrough this process to get a sense of the sorts of informationchosen ?query, URL, intent?
triples are shown in Table 1.Once the intents have been derived, the original URLs arethrown away: they are not used in any of our experiments.5.2 Relevance AnnotationOur evaluation now consists of giving human annotators?query, intent?
pairs and having them mark the first rele-vant URL in the return set (if there is one).
However, inorder to draw an unbiased comparison between our sys-tem and vanilla MSN Search, we need to present the out-put from both as a simple ordered list.
This requires firstconverting our system?s output to a list.5.2.1 Linearization of QDSE OutputWe wish to linearize our results in such a way thatthe position of the first relevant URL enables us to drawmeaningful inferences.
In vanilla MSN search, we canascribe a cost of 1 to reading each URL in the list: havinga relevant URL as the 8th position results in a cost of 8.Similarly, we wish to ascribe a cost to each item in ourresults.
We do this by making the assumption that theuser is able to guess (with 100% accuracy) which sub-category a relevant URL will be in (we will evaluate thisassumption later).
Given this assumption, we say that thecost of a link in the top 5 vanilla MSN links is simply itsposition on the page.
Further down, we assume there is acost for reading each of the MSN links, as well as a costfor reading each header until you get to the one you want.Finally, there is a cost for reading down the list of linksunder that header.
Given this cost model, we can linearizeour results by simply sorting them by cost (in this model,several links will have the same cost ?
in this case, we fallback to the original ordering).5.2.2 AnnotationWe divided the 52 ?query, intent?
pairs into two sets of32 (12 common pairs).
Each set of 32 was then scrambledand half were assigned to class System 1 and half wereassigned to class System 2.
It was ensured that the 12overlapping pairs were evenly distributed.Four annotators were selected.
The first two were pre-sented with the first 32 pairs and the second two werepresented with the second 32 pairs, but with the sys-tems swapped.5 Annotators were given a query, the in-tent, and the top 100 documents returned from the searchaccording to the corresponding system (in the case ofQDSE, enough alternate queries were selected so thatthere would be exactly 100 total documents listed).
Theannotator selected the first link which answered the in-tent.
If there was no relevant link, they recorded that.people really are looking for, so that we need not invent queriesoff the tops of our heads.5The interface used for evaluation converted the QDSE re-sults into a linear list using our linearization technique so thatthe interface was consistent for both systems.5.3 Predictivity AnnotationOur cost function for the linearization of the hierarchicalresults (see Section 5.2.1) assumes that users are able topredict which category will contain a relevant link.
In or-der to evaluate this assumption, we took our 52 queriesand the automatically generated category names for eachusing the QDSE system.
We then presented four new an-notators with the queries, intents and categories.
They se-lected the first category which they thought would containa relevant link.
They also were able to select a ?None?category if they did not think any would contain relevantlinks.
Each of the four annotators performed exactly thesame annotation ?
it was done four times so agreementcould be calculated.6 Results and AnalysisOur results are calculated on two metrics: relevance andpredictivity, as described in the previous section.6.1 Relevance ResultsThe results of the evaluation are summarized in Table 2.The table reports four statistics for each of the systemscompared.
In the table, MSN is vanilla MSN search andQDSE is the system described in this paper.The first row is probability of success using this sys-tem (number of successful searches divided by the num-ber of total searches).
The second line is the probabilityof success, given that you are only allowed to read thefirst 20 results.
Next, Avg.
Success Cost, is the averagecost of the relevant URL for that system.
This cost aver-ages only over the successes (queries for which a relevantURL was found).
The next statistic, Avg.
Cost, is the av-erage cost including failures, where the cost of a failureis, in the case of vanilla MSN, the number of returnedresults and, in the case of QDSE, the cost of reading thetop five results, all the labels and one category expansion6The last statistic, Avg.
Mutual Cost, is the average costfor all pairs where both systems found a relevant docu-ment.
The last line reports inter-annotator agreement ascalculated over the 12 pairs, which is low due partly tothe small sample size and partly to the fact that the in-tents themselves were still somewhat underspecified.76.2 Predictivity ResultsWe performed two calculations on the results of the pre-dictivity annotations.
In the first calculation, we considerthe relevance judgments on the QDSE system to be thegold standard.
We calculated accuracy of choosing thecorrect first category.
This measures the extent to which6The user may have been able to determine his search hadfailed having only read the categories, yielding a lower cost.7We intend to run timed user studies in our future work;however, it has been observed (Dumais et al, 2001) that pre-senting users with structured results enables them to find rel-evant documents more quickly; to do timed studies in the lin-earization is an unrealistic scenario, since one would never de-ploy the system in this configuration.Query: Soldering iron URL: www.siliconsolar.com/accessories.htmIntent: looking for accessories for soldering irons (but not soldering irons themselves)Query: Whole Foods URL: www.wholefoodsmarket.com/company/communitygiving.htmlIntent: looking for the Whole Foods Market?s community giving policyQuery: final fantasy URL: www.playonline.com/ff11/home/Intent: looking for a webforum for final fantasy gamesQuery: online computer course URL: www.microsoft.com/traincert/Intent: looking for information on Microsoft Certified Technical Education centersTable 1: Four random ?query, URL, intent?
triplesMSN QDSEProb.
Success 88.0% 67.7%Prob.
Success 20 68.7% 62.6%Avg.
Success Cost 12.4 4.7Avg.
Cost 22.9 9.0Avg.
Mutual Cost 23.0 9.0kappa 0.57 0.45Table 2: Results of the evaluationthe oracle system is correct.
On this task, accuracy was0.54.
The second calculation we made was to determinewhether a user can predict, looking at the headers only,whether their search has been successful.
In the taskof simply identifying failed searches, accuracy was 0.70.Inter-annotator agreement for predictivity was somewhatlow, with a kappa value of only 0.49.6.3 AnalysisAs can be seen from Table 2, a user is less likely to find arelevant query in the top 100 documents using the QDSEsystem than using the MSN system.
However, this is anartificial task: very few users will actually read throughthe top 100 returned documents before giving up.
At acutoff of 20 documents, the user is still more likely to suc-ceed using MSN, but the difference is not nearly so large(note, however, that by cutting off at 20 in the QDSE lin-earization, the user will typically see only one result fromeach alternate query, thus heavily relying on the under-lying search engine to do a good job).
The rest of thenumbers (not included for brevity) are consistent at 20.Moreover, as seen in the evaluation of the predictiv-ity results, users can decide, with 70% accuracy, whethertheir search has failed having read only the category la-bels.
This is in stark contrast to the vanilla MSN searchwhere they could not know without reading all the resultswhether their search had succeeded.If one does not wish to give up on recall at all, we couldsimply list all the MSN search results immediately afterthe QDSE results.
By doing this, we ensure that the prob-ability of success is at least as high for the QDSE system.We can upper-bound the additional cost this would incurto the QDSE system by 4.15, yielding an upper bound of13.2, still superior to vanilla MSN.If one is optimistic and is willing to assume that a userwill know based only on the category labels whether ornot their search has succeeded, then the relevant com-parison from Table 2 is between Avg.
Success Cost forQDSE and Avg.
Cost for MSN.
In this case, our costof 4.7 is a factor of 5 better than the MSN cost.
If, onthe other hand, one is pessimistic and believes that a userwill not be able to identify based on the category nameswhether or not their search has succeeded in the QDSEsystem, then the interesting comparison is between theAvg.
Costs for MSN and QDSE.
Both favor QDSE.Lastly, the reciprocal rank statistic at 20 results confirmthat the QDSE system is more able to direct the user torelevant documents than vanilla MSN search.7 ConclusionWe have presented a method for providing useful sug-gested queries for underspecified informational queries.We evaluated our system using an unbiased metricagainst a standard web search system and found thatour system enables users to more quickly find relevantpages.
This conclusion is based on an ?oracle?
assump-tion, which we also evaluate.
Based on these evaluations,we can show that even under a pessimistic view point, oursystem outperforms the vanilla search engine.There is still room for improvement, especially in thepredictivity results.
We would like users to be able tomore readily identify the class into which a relevant doc-ument (if one exists) would be found.
We are investi-gating multi-document summarization techniques whichmight allow users to better pinpoint the category in whicha relevant document might be found.ReferencesD.
Beeferman and A. Berger.
2000.
Agglomerative clus-tering of a search engine query log.
In KDD.S.
Brin and L Page.
1998.
The anatomy of a large-scalehypertextual Web search engine.
Computer Networksand ISDN Systems.A.
Broder.
2002.
A taxonomy of web search.
In SIGIR.J.
Carbonell and J. Goldstein.
1998.
The use of MMR,diversity-based reranking for reordering documentsand producing summaries.
In Research and Develop-ment in Information Retrieval.S.
Dumais and H. Chen.
2000.
Hierarchical classifica-tion of Web content.
In Proc.
of SIGIR-00.S.
Dumais, E. Cutrell, and H. Chen.
2001.
Optimizingsearch by showing results in context.
In CHI.O.
Zamir and O. Etzioni.
1999.
Grouper: a dynamicclustering interface to Web search results.
In Com-puter Networks.
