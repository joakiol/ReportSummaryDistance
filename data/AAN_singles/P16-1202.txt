Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2144?2153,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsLearning To Use Formulas To Solve Simple Arithmetic ProblemsArindam MitraArizona State Universityamitra7@asu.eduChitta BaralArizona State Universitychitta@asu.eduAbstractSolving simple arithmetic word problemsis one of the challenges in Natural Lan-guage Understanding.
This paper presentsa novel method to learn to use formulasto solve simple arithmetic word problems.Our system, analyzes each of the sen-tences to identify the variables and theirattributes; and automatically maps this in-formation into a higher level representa-tion.
It then uses that representation torecognize the presence of a formula alongwith its associated variables.
An equa-tion is then generated from the formal de-scription of the formula.
In the trainingphase, it learns to score the <formula,variables> pair from the systematicallygenerated higher level representation.
It isable to solve 86.07% of the problems ina corpus of standard primary school testquestions and beats the state-of-the-art bya margin of 8.07%.1 IntroductionDeveloping algorithms to solve math word prob-lems (Table 1) has been an interest of NLP re-searchers for a long time (Feigenbaum and Feld-man, 1963).
It is an interesting topic of study fromthe point of view of natural language understand-ing and reasoning for several reasons.
First, it in-corporates rigorous standards of accurate compre-hension.
Second, we know of a good representa-tion to solve the word problems, namely algebraicequations.
Finally, the evaluation is straightfor-ward and the problems can be collected easily.In the recent years several challenges havebeen proposed for natural language understanding.This includes the Winograd Schema challengefor commonsense reasoning (Levesque, 2011),Story Comprehension Challenge (Richardson etal., 2013), Facebook bAbl task (Weston et al,2015), Semantic Textual Similarity (Agirre et al,2012) and Textual Entailment (Bowman et al,2015; Dagan et al, 2010).
The study of word mathproblems is also an important problem as quantita-tive reasoning is inextricably related to human life.Clark & Etzioni (Clark, 2015; Clark and Etzioni,2016) discuss various properties of math word(and science) problems emphasizing elementaryschool science and math tests as a driver for AI.Researchers at Allen AI Institute have publishedtwo standard datasets as part of the Project Euclid1for future endeavors in this regard.
One of themcontains simple addition-subtraction arithmeticproblems (Hosseini et al, 2014) and the othercontains general arithmetic problems (Koncel-Kedziorski et al, 2015).
In this research, we focuson the former one, namely the AddSub dataset.Dan grew 42 turnips and 38 cantelopes .
Jes-sica grew 47 turnips .
How many turnips didthey grow in total ?Formula Associated variablespart-whole whole: x, parts: {42, 47}Equation x = 42 + 47Table 1: Solving a word problem using part-wholeBroadly speaking, common to the existing ap-proaches (Kushman et al, 2014; Hosseini et al,2014; Zhou et al, 2015; Shi et al, 2015; Roy andRoth, 2015) is the task of grounding, that takes asinput a word problem in the natural language andrepresents it in a formal language, such as, a sys-tem of equations, expression trees or states (Hos-seini et al, 2014), from which the answer can beeasily computed.
In this work, we divide this taskof grounding into two parts as follows:1http://allenai.org/euclid.html2144In the first step, the system learns to connect theassertions in a word problem to abstract mathe-matical concepts or formulas.
In the second step,it maps that formula into an algebraic equation.Examples of such formulas in the arithmetic do-main includes part whole which says, ?the wholeis equal to the sum of its parts?, or the UnitaryMethod that is used to solve problems like ?A manwalks seven miles in two hours.
What is his aver-age speed?
?.Consider the problem in Table 1.
If the systemcan determine it is a ?part whole?
problem wherethe unknown quantity X plays the role of wholeand its parts are 42 and 47, it can easily expressthe relation as X = 42 + 47.
The translation ofa formula to an equation requires only the knowl-edge of the formula and can be formally encoded.Thus, we are interested in the question, ?how canan agent learn to apply the formulas for the wordproblems??
Solving a word problem in general,requires several such applications in series or par-allel, generating multiple equations.
However, inthis research, we restrict the problems to be of asingle equation which requires only one applica-tion.Our system currently considers three mathemat-ical concepts: 1) the concept of part whole, 2) theconcept of change and 3) the concept of compar-ison.
These concepts are sufficient to solve thearithmetic word problems in AddSub.
Table 2 il-lustrates each of these three concepts with exam-ples.
The part whole problems deal with the partwhole relationships and ask for either the part orthe whole.
The change problems make use of therelationship between the new value of a quantityand its original value after the occurrence of a se-ries of increase or decrease.
The question thenasks for either the initial value of the quantity orthe final value of the quantity or the change.
Incase of comparison problems, the equation can bevisualized as a comparison between two quanti-ties and the question typically looks for either thelarger quantity or the smaller quantity or the dif-ference.
While the equations are simple, the prob-lems describe a wide variety of scenarios and thesystem needs to make sense of multiple sentenceswithout a priori restrictions on the syntax or thevocabulary to solve the problem.Training has been done in a supervised fash-ion.
For each example problem, we specify theformula that should be applied to generate the ap-ChangeRESULT UNKNOWNMary had 18 baseball cards , and 8 were torn .Fred gave Mary 26 new baseball cards .
Marybought 40 baseball cards .
How many baseballcards does Mary have now ?CHANGE UNKNOWNThere were 28 bales of hay in the barn .
Timstacked bales in the barn today .
There are now54 bales of hay in the barn .
How many balesdid he store in the barn ?START UNKNOWNSam ?s dog had puppies and 8 had spots .
Hegave 2 to his friends .
He now has 6 puppies .How many puppies did he have to start with?Part WholeTOTAL SET UNKNOWNTom went to 4 hockey games this year , butmissed 7 .
He went to 9 games last year .
Howmany hockey games did Tom go to in all ?PART UNKNOWNSara ?s high school played 12 basketball gamesthis year .
The team won most of their games.
They were defeated during 4 games .
Howmany games did they win ?ComparisionDIFFERENCE UNKNOWNLast year , egg producers in Douglas Countyproduced 1416 eggs .
This year , those samefarms produced 4636 eggs .
How many moreeggs did the farms produce this year ?LARGE QUANTITY UNKNOWNBill has 9 marbles.
Jim has 7 more marbles thanBill.
How many marbles does Jim have?SMALL QUANTITY UNKNOWNBill has 9 marbles.
He has 7 more marbles thanJim.
How many marbles does Jim have?Table 2: Examples of Add-Sub Word Problemspropriate equation and the relevant variables.
Thesystem then learns to apply the formulas for newproblems.
It achieves an accuracy of 86.07% onthe AddSub corpus containing 395 word arithmeticproblems with a margin of 8.07% with the currentstate-of-the-art (Roy and Roth, 2015).Our contributions are three-fold: (a) We modelthe application of a formula and present a novelmethod to learn to apply a formula; (b) We anno-tate the publicly available AddSub corpus with the2145correct formula and its associated variables; and(c) We make the code publicly available.2The rest of the paper is organized as follows.
Insection 2, we formally define the problem and de-scribe our learning algorithm.
In section 3, we de-fine our feature function.
In section 4, we discussrelated works.
Section 5 provides a detailed de-scription of the experimental evaluation.
Finally,we conclude the paper in section 6.2 Problem FormulationA single equation word arithmetic problem P isa sequence of k words ?w1, ..., wk?
and containsa set of variables VP= {v0, v1, ..., vn?1, x}where v0, v1, ..., vn?1are numbers in P and x isthe unknown whose value is the answer we seek(Koncel-Kedziorski et al, 2015).
Let Paddsubbethe set of all such problems, where each prob-lem P ?
Paddsubcan be solved by a evaluatinga valid mathematical equation E formed by com-bining the elements ofVPand the binary operatorsfrom O = {+,?
}.We assume that each target equation E ofP ?
Paddsubis generated by applying oneof the possible mathematical formulas fromC = {Cpartwhole, Cchange, Ccomparision}.
LetP1addsub?
Paddsubbe the set of all problemswhere the target equation E can be generated by asingle application of one of the possible formulasfrom C. The goal is then to find the correct appli-cation of a formula for the problem P ?
P1addsub.2.1 Modelling Formulas And theirApplicationsWe model each formula as a template that has pre-defined slots and can be mapped to an equationwhen the slots are filled with variables.
Applica-tion of a formula C ?
C to the problem P , is thendefined as the instantiation of the template by asubset of VPthat contains the unknown.Part Whole The concept of part whole hastwo slots, one for the whole that accepts a singlevariable and the other for its parts that accepts aset of variables of size at least two.
If the valueof the whole is w and the value of the parts arep1, p2, ..., pm, then that application is mapped tothe equation, w = p1+ p2+ ... + pm, denotingthat whole is equal to the sum of its parts.2The code and data is publicly available athttps://github.com/ari9dam/MathStudent.Change The change concept has four slots,namely start, end, gains, losses which respectivelydenote the original value of a variable, the finalvalue of that variable, and the set of incrementsand decrements that happen to the original valueof the variable.
The start slot can be empty; inthat case it is assumed to be 0.
For example, con-sider the problem, ?Joan found 70 seashells on thebeach .
she gave Sam some of her seashells.
Shehas 27 seashell .
How many seashells did she giveto Sam??.
In this case, our assumption is that be-fore finding the 70 seashells Joan had an emptyhand.
Given an instantiation of change conceptthe equation is generated as follows:valstart+?g?gainsvalg=?l?lossesvall+ valendComparision The comparision concept hasthree slots namely the large quantity, the smallquantity and their difference.
An instantiation ofthe comparision concept is mapped to the follow-ing equation: large = small + difference.2.2 The Space of Possible ApplicationsConsider the problem in Table 1.
Even though thecorrect application is an instance of part wholeformula with whole = x and the parts being{42, 47}, there are many other possible applica-tions, such as, partWhole(whole=47, parts=x,42),change(start=47, losses={x}, gains={}, end= 42), comparison(large=47, small=x, differ-ence=42).
Note that, comparison(large=47,small=38, difference=42) is not a valid applica-tion since none of the associated variables is anunknown.
Let APbe the set of all possible appli-cations to the problem P .
The following lemmacharacterizes the size of APas a function of thenumber of variables in P .Lemma 2.2.1.
Let P ?
P1addsubbe an arithmeticword problem with n variables (|VP| = n), thenthe following are true:1.
The number of possible applications of partwhole formula to the problem P , Npartwholeis (n+ 1)2n?2+ 1.2.
The number of possible applications ofchange formula to the problem P , Nchangeis 3n?3(2n2+ 6n+ 1)?
2n+ 1.3.
The number of possible applications ofcomparison formula to the problem P ,Ncomparisonis 3(n?
1)(n?
2).21464.
The number of all possible applications tothe problem P is Npartwhole+ Nchange+Ncomparison.Proof of lemma 2.2.1 is provided in the Ap-pendix.
The total number of applications for prob-lems having 3, 6, 7, 8 number of variables are 47,3, 105, 11, 755, 43, 699 respectively.
Addition-Subtraction arithmetic problems hardly containmore than 6 variables.
So, the number of possi-ble applications is not intractable in practice.The total number of applications increasesrapidly mainly due to the change concept.
Since,the template involves two sets, there is a 3n?3fac-tor present in the formula of Nchange.
However,any application of change concept with gains andlosses slots containing a collection of variables canbe broken down into multiple instances of changeconcept where the gains and losses slots acceptsonly a single variable by introducing more inter-mediate unknown variables.
Since, for any for-mula that does not have a slot that accepts a set,the number of applications is polynomial in thenumber of variables, there is a possibility to re-duce the application space.
We plan to explorethis possibility in our future work.
For the partwhole concept, even though there is a exponen-tial term involved, it is practically tractable (forn = 10, Npartwhole= 2, 817 ).
In practice, webelieve that there will hardly be any part wholeapplication involving more than 10 variables.
Forformulas that are used for other categories of wordmath problems (algebraic or arithmetic), such asthe unitary method, formulas for ratio, percentage,time-distance and rate of interest, none of themhave any slot that accepts sets of variables.
Thus,further increase in the space of possible applica-tions will be polynomial.2.3 Probabilistic ModelFor each problem P there are different possibleapplications y ?
AP, however not all of them aremeaningful.
To capture the semantics of the wordproblem to discriminate between competing appli-cations we use the log-linear model, which has afeature function ?
and parameter vector ?
?
Rd.The feature function ?
: H ?
Rdtakes as in-put a problem P and a possible application y andmaps it to a d-dimensional real vector (featurevector) that aims to capture the important infor-mation required to discriminate between compet-ing applications.
Here, the set H is defined as{(P, y) : P ?
P1addsub?
y ?
AP}, to accommo-date the dependency of the possible applicationson the problem instance.
Given the definition ofthe feature function ?
and the parameter vector ?,the probability of an application y given a problemP is defined as,p(y|P ; ?)
=e?.?(P,y)?y??APe?.?(P,y?
)Here, .
denotes dot product.
Section 3 definesthe feature function.
Assuming that the parame-ter ?
is known, the function f that computes thecorrect application is defined as,f(P ) = arg maxy?APp(y|P ; ?
)2.4 Parameter EstimationTo learn the function f , we need to estimate theparameter vector ?.
For that, we assume access ton training examples, {Pi, y?i: i = 1 .
.
.
n}, eachcontaining a word problem Piand the correct ap-plication y?ifor the problem Pi.
We estimate ?by minimizing the negative of the conditional log-likelihood of the data:O(?)
= ?n?i=1log p(y?i|Pi; ?
)= ?n?i=1[?.?
(Pi, y?i)?
log?y?APie?.?
(Pi,y)]We use stochastic gradient descent to optimizethe parameters.
The gradient of the objective func-tion is given by:?O?
?= ?n?i=1[?
(Pi, y?i)?
?y?APip(y|Pi; ?)?
?
(Pi, y)](1)Note that, even though the space of possible ap-plications vary with the problem Pi, the gradientfor the example containing the problem Pican beeasily computed.3 Feature Function ?A formula captures the relationship between vari-ables in a compact way which is sufficient to gen-erate an appropriate equation.
In a word prob-lem, those relations are hidden in the assertions2147of the story.
The goal of the feature function isthus to gather enough information from the storyso that underlying mathematical relation betweenthe variables can be discovered.
The feature func-tion thus needs to be aware of the mathemati-cal relations so that it knows what information itneeds to find.
It should also be ?familiar?
withthe word problem language so that it can extractthe information from the text.
In this research,the feature function has access to machine read-able dictionaries such as WordNet (Miller, 1995),ConceptNet (Liu and Singh, 2004) which capturesinter word relationships such as hypernymy, syn-onymy, antonymy etc, and syntactic and depen-dency parsers that help to extract the subject, verb,object, preposition and temporal information fromthe sentences in the text.
Given these resources,the feature function first computes a list of at-tributes for each variable.
Then, for each applica-tion y it uses that information, to compute if someaspects of the expected relationship described in yis satisfied by the variables in y.Let the first b dimensions of the feature vectorcontain part whole related features, the next c di-mensions are for change related features and theremaining d features are for comparison concept.Then the feature vector for a problem P and anapplication of a formula y is computed in the fol-lowing way:Data: A word problem P , an application yResult: d-dimensional feature vector, fvInitialize fv := 0if y is instance of part whole thencompute fv[1 : b]endif y is instance of change thencompute fv[b+ 1 : b+ c]endif y is instance of comparision thencompute fv[b+ c+ 1 : b+ c+ d]endAlgorithm 1: Skeleton of the feature function ?The rest of the section is organized as follows.We first describe the attributes of the variables thatare computed from the text.
Then, we define a listof boolean variables which computes semantic re-lations between the attributes of each pair of vari-ables.
Finally, we present the complete definitionof the feature function using the description of theattributes and the boolean variables.3.1 Attributes of VariablesFor each occurrence of a number in the text a vari-able is created with the attribute value referringto that numeric value.
An unknown variable iscreated corresponding to the question.
A specialattribute type denotes the kind of object the vari-able refers to.
Table 3 shows several examplesof the type attribute.
It plays an important rolein identifying irrelevant numbers while answeringthe question.Text TypeJohn had 70 seashells seashells70 seashells and 8 were broken seashells61 male and 78 female salmon male, salmon35 pears and 27 apples pearTable 3: Example of type for highlighted variables.The other attributes of a variable captures itslinguistic context to surrogate the meaning of thevariable.
This includes the verb attribute i.e.the verb attached to the variable, and attributescorresponding to Stanford dependency relations(De Marneffe and Manning, 2008), such as nsubj,tmod, prep in, that spans from either the words inassociated verb or words in the type.
These at-tributes were computed using Stanford Core NLP(Manning et al, 2014).
For the sentence, ?Johnfound 70 seashells on the beach.?
the attributes ofthe variable are the following: { value : {70},verb : {found} , nsubj : {John}, prep on :{beach }}.3.2 Cross Attribute RelationsOnce the variables are created and their attributesare extracted, our system computes a set ofboolean variables, each denoting whether the at-tribute a1of the variable v1has the same valueas the attribute a2of the variable v2.
The valueof each attribute is a set of words, consequentlyset equality is used to calculate attribute equality.Two words are considered equal if their lemmamatches.Four more boolean variables are computed foreach pair of variables based on the attribute typeand they are defined as follows:subType: Variable v1is a subType of vari-able v2if v2.type ?
v1.type or their type consistsof a single word and there exists the IsA relationbetween them in ConceptNet (Speer and Havasi,2013; Liu and Singh, 2004).2148disjointType is true if v1.type?v2.type = ?intersectingType is true if v1is neither asubType of v2nor is disjointType nor equal.We further compute some more variables by uti-lizing several relations that exist between words:antonym: For every pair of variables v1andv2, we compute an antonym variable that is true ifthere exists a pair of word in (v1.verb?v1.adj)?
(v2.verb?v2.adj) that are antonym to each otherin WordNet irrespective of their part of speech tag.relatedVerbs: The verbs of two variables arerelated if there exists a RelatedTo relations in Con-ceptNet between them.subjConsume: The nsubj of v1consumes thensubj of v2if the formers refers to a group and thelatter is a part of that group.
For example, in theproblem, ?Joan grew 29 carrots and 14 watermel-ons .
Jessica grew 11 carrots .
How many carrotsdid they grow in all ?
?, the nsubj of the unknownvariable consumes others.
This is computed usingStanford co-reference resolution.
For the situationwhere there is a variable with nsubj as ?they?
andit does not refer to any entity, the subjConsumevariable is assumed to be implicitly true for anyvariable having a nsubj of type person.3.3 Features: Part WholeThe part whole features look for some combina-tions of the boolean variables and the presenceof some cue words (e.g.
?all?)
in the attributelist.
These features capture the underlying reason-ings that can affect the decision of applying a partwhole concept.
We describe the conditions whichwhen satisfied activate the features.
If active, thevalue of a feature is the number of variables asso-ciated with the application y and 0 otherwise.
Thisis also true for change and comparision featuresalso.
Part whole features are computed only whenthe y is an instance of the formula part whole.
Thesame applies for change and comparision features.Generic Word Cue This feature is activatedif y.whole has a word in its attributes that belongsto the ?total words set?
containing the followingswords ?all?, ?total?, ?overall?, ?altogether?, ?to-gether?
and ?combine?
; and none of the variablesin parts are marked with these words.ISA Type Cue is active if all the part variablesare subType of the whole.Type-Verb Cue is active if the type and verbattributes of vwholematches that of all the variablesin the part slot of y.Type-Individual Group Cue is active if thevariable vwholesubjConsume each part variable vpin y and their type matches.Type-Verb-Tmod Cue is active if the vari-able in the slot whole is the unknown and for eachpart variable vptheir verb, type and tmod (timemodifier of the verb) attributes match.Type-SubType-Verb Cue is active if the vari-able in the slot whole is either the unknown ormarked with a word in ?total words set?
and forall parts vp, their verb matches and one of the typeor subType boolean variable is true.Type-SubType-Related Verb Cue is similarto Type-SubType-Verb Cue however relaxes theverb match conditions to related verb match.
Thisis helpful in problems like ?Mary went to the mall.She spent $ 13.04 on a shirt and $ 12.27 on ajacket .
She went to 2 shops .
In total , how muchmoney did Mary spend on clothing ?
?.Type-Loose Verb Cue ConceptNet does notcontain all relations between verbs.
For example,according to ConceptNet ?buy?
and ?spend?
are re-lated however there is no relation in ConceptNetbetween ?purchase?
and ?spend?.
To handle thesesituations, we use this feature which is similar tothe previous one.
The difference is that it assumesthat the verbs of part-whole variable pairs are re-lated if all verbs associated with the parts are same,even though there is no relation in ConceptNet.Type-Verb-Prep Cue is active if type andverb matches.
The whole does not have a ?prepo-sition?
but parts have and they are different.Other Cues There are also features that addnsubj match criteria to the above ones.
The priorfeature for part whole is that the whole if not un-known, is smaller than the sum of the parts.
Thereis one more feature that is active if the two partvariables are antonym to each other; one of typeor subType should be true.3.4 Features: ChangeThe change features are computed from a set of 10simple indicator variables, which are computed inthe following way:2149Start Cue is active if the verb associated withthe variable in start slot has one of the followingpossessive verbs : {?call for?, ?be?, ?contain?, ?re-main?, ?want?, ?has?, ?have?, ?hold?, ...}; the typeand nsubj of start variable match with the end vari-able and the tense of the end does not precede thestart.
The list of ?possessive verbs?
is automati-cally constructed by adding all the verbs associ-ated with the start and the end slot variables inannotated corpus.Start Explicit Cue is active if one of follow-ing words, ?started with?, ?initially?, ?begining?,?originally?
appear in the context of the start vari-able and the type of start and end variables match.Start prior is active if the verb associatedwith the variable in start slot is a member of theset ?possessive verbs?
and the variable appears infirst sentence.Start Default Cue is active if the start vari-able has a ?possessive verb?
with past tense.End Cue is active if the verb associated withthe variable in slot end has a possessive verb withthe tense of the verb not preceding the tense ofthe start, in case the start is not missing.
The typeand nsubj should match with either the start or thegains in case the start is missing.End Prior is true if vendhas a possessive verband an unknown quantity and at least one of vendor vstartdoes not have a nsubj attribute.Gain Cue is active if for all variables in thegains slot, the type matches with either vendorvstartand one of the following is true: 1) the nsubjof the variable matches with vendor vstartand theverb implies gain (such as ?find?)
and 2) the nsubjof the variable does not match with vendor vstartand the verb implies losing (e.g.
spend).
The setof gain and loss verbs are collected from the anno-tated corpus by following the above procedure.Gain Prior is true if the problem containsonly three variables, with vstart< vendand theonly variable in the gain slot, associated with non-possessive verb is the unknown.Loss Cue & Loss prior are designed in afashion similar to the Gain cue and Gain Prior.Let us say badgainsdenotes that none of the gainprior or gain cue is active even though the gain slotis not empty.
badlossesis defined similarly and letbad = badgains?
badlosses.
Then the change fea-tures are computed from these boolean indicatorsusing logical operators and, or, not.
Table4 showssome of the change features.
!bad ?
gaincue?
startdefault?
endcue!bad?!gaincue?
losscue?startdefault?endcue!bad ?
(gaincue?
losscue) ?startcue?!startdefault?
endcue!bad ?
(gaincue?
losscue) ?startexplicit?!startdefault?
endcue!bad ?
(gaincue?
losscue) ?
startprior?
(endcue||endprior)!bad ?
(gaincue?
losscue) ?
(startprior?startcue)?!startdefault?
endpriorTable 4: Activation criteria of some change relatedfeatures.3.5 Features: ComparisonThe features for the ?compare?
concept are rela-tively straight forward.Difference Unknown Que If the applicationy states that the unknown quantity is the differ-ence between the larger and smaller quantity, it isnatural to see if the variable in the difference slot ismarked with a comparative adjective or compara-tive adverb.
The prior is that the value of the largerquantity must be bigger than the small one.
An-other two features add the type and subject match-ing criteria along with the previous ones.Large & Small Unknown Que These fea-tures can be active only when the variable in thelarge or small slot is unknown.
To detect if the ref-erent is bigger or smaller, it is important to knowthe meaning of the comparative words such as?less?
and ?longer?.
Since, the corpus contains only33 comparison problems we collect these compar-ative words from web which are then divided intotwo categories.
With these categories, the featuresare designed in a fashion similar to change fea-tures that looks for type, subject matches.3.6 Handling Arbitrary Number of VariablesThis approach can handle arbitrary number ofvariables.
To see that consider the problem, ?Sallyfound 9 seashells , Tom found 7 seashells , andJessica found 5 seashells on the beach .
Howmany seashells did they find together ??.
Let ussay that feature vector contains only the ?Type-Individual Group Cue?
feature and the weight2150of that feature is 1.
Consider the two follow-ing applications: y1= partWhole(x,{9,7}) andy2= partWhole(x,{9,7, 5}).
For both y1and y2the ?Type-Individual Group Cue?
feature is activesince the subject of the unknown x refers to agroup that contains the subject of all part variablesin y1and y2and their types match.
However, asmentioned in section 3.3, when active, the valueof a feature is the number of variables associatedwith the application.
Thusp(y2;P,?)p(y1;P,?
)=e4e3= e.Thus, y2is more probable than y1.4 Related WorksResearchers in early years have studied math wordproblems in a constrained domain by either lim-iting the input sentences to a fixed set of pat-terns (Bobrow, 1964b; Bobrow, 1964a; Hinsley etal., 1977) or by directly operating on a proposi-tional representation instead of a natural languagetext (Kintsch and Greeno, 1985; Fletcher, 1985).Mukherjee and Garain (2008) survey these works.Among the recent algorithms, the most generalones are the work in (Kushman et al, 2014; Zhouet al, 2015) .
Both algorithms try to map a wordmath problem to a ?system template?
that containsa set of ?equation templates?
such as ax + by =c.
These ?system templates?
are collected fromthe training data.
They implicitly assume thatthese templates will reoccur in the new exampleswhich is a major drawback of these algorithms.Also, Koncel-Kedziorski et al (2015) show thatthe work of Kushman et al (2014) heavily re-lies on the overlap between train and test data andwhen this overlap is reduced the system performspoorly.Work of (Koncel-Kedziorski et al, 2015; Royand Roth, 2015) on the other hand try to map themath word problem to an expression tree.
Eventhough, these algorithms can handle all the fourarithmetic operators they cannot solve problemsthat require more than one equation.
Moreover,experiments show that our system is much morerobust to diversity in the problem types betweentraining and test data for the problems it handles.The system ARIS in (Hosseini et al, 2014)solves the addition-subtraction problems by cat-egorizing the verbs into seven categories such as?positive transfer?, ?loss?
etc.
It represents the in-formation in a problem as a state and then updatesthe state according to the category of a verb as thestory progresses.
Both ARIS and our system sharethe property that they give some explanation be-hind the equation they create.
However, the verbcategorization approach of ARIS can only solve asubset of addition-subtraction problems (see erroranalysis in (Hosseini et al, 2014)); whereas the us-age of formulas to model the word problem world,gives our system the ability to accommodate othermath word problems as well.5 Experimental Evaluation5.1 DatasetThe AddSub dataset consist of a total of 395addition-subtraction arithmetic problems for third,fourth, and fifth graders.
The dataset is dividedinto three diverse set MA1, MA2, IXL containing134, 140 and 121 problems respectively.
As men-tioned in (Hosseini et al, 2014), the problems inMA2 have more irrelevant information comparedto the other two datasets, and IXL includes moreinformation gaps.5.2 ResultHosseini et al (2014) evaluate their system using3-fold cross validation.
We follow that same pro-cedure.
Table 5 shows the accuracy of our sys-tem on each dataset (when trained on the othertwo datasets).
Table 6 shows the distribution ofthe part whole, change, comparison problems andthe accuracy on recognizing the correct formula.MA1 IXL MA2 AvgARIS 83.6 75.0 74.4 77.7KAZB 89.6 51.1 51.2 64.0ALGES - - - 77.0Roy & Roth - - - 78.0Majority 45.5 71.4 23.7 48.9Our System 96.27 82.14 79.33 86.07Table 5: Comparision with ARIS, KAZB (Kush-man et al, 2014), ALGES (Koncel-Kedziorski etal., 2015) and the state of the art Roy & Roth onthe accuracy of solving arithmetic problems.As we can see in Table 6 only IXL containsproblems of type ?comparison?.
So, to study theaccuracy in detecting the compare formula weuniformly distribute the 33 examples over the 3datasets.
Doing that results in only two errors inthe recognition of a compare formula and also in-creases the overall accuracy of solving arithmeticproblems to 90.38%.21515.3 Error AnalysisAn equation that can be generated from a changeor comparision formula can also be generated bya part whole formula.
Four such errors happenedfor the change problems and out of the 33 com-pare problems, 18 were solved by part whole.Also, there are 3 problems that require two appli-cations.
One example of such problem is, ?Thereare 48 erasers in the drawer and 30 erasers on thedesk.
Alyssa placed 39 erasers and 45 rulers onthe desk.
How many erasers are now there in to-tal ??.
To solve this we need to first combine thetwo numbers 48 and 30 to find the total number oferasers she initially had.
This requires the knowl-edge of ?part-whole?.
Now, that sum of 48 and30, 39 and x can be connected together using the?change?
formula.
With respect to ?solving?
arith-metic problems, we find the following categoriesas the major source of errors:Problem Representation: Solving problemsin this category requires involved representation.Consider the problem, ?Sally paid $ 12.32 total forpeaches , after a ?3 dollar?
coupon , and $ 11.54for cherries .
In total , how much money did Sallyspend??.
Since the associated verb for the variable3 dollar is ?pay?, our system incorrectly thinks thatSally did spend it.Information Gap: Often, information that iscritical to solve a problem is not present in the text.E.g.
Last year , 90171 people were born in a coun-try , and 16320 people immigrated to it .
Howmany new people began living in the country lastyear ?.
To correctly solve this problem, it is impor-tant to know that both the event ?born?
and ?immi-gration?
imply the ?began living?
event, howeverthat information is missing in the text.
Anotherexample is the problem, ?Keith spent $6.51 on arabbit toy , $5.79 on pet food , and a cage costhim $12.51 .
He found a dollar bill on the ground.What was the total cost of Keith ?s purchases?
?.
Itis important to know here that if a cage cost Keith$12.51 then Keith has spent $12.51 for cage.Modals: Consider the question ?Jason went to11 football games this month .
He went to 17games last month , and plans to go to 16 gamesnext month .
How many games will he attend inall??
To solve this question one needs to under-stand the meanings of the verb ?plan?
and ?will?.If we replace ?will?
in the question by ?did?
theanswer will be different.
Currently our algorithmType MA1 IXL MA2part wholeTotal 59 89 51correct 59 81 40changeTotal 74 18 68correct 70 15 56compareTotal 0 33 0correct 0 0 0Table 6: Accuracy on recognizing the correct ap-plication.
None of the MA1 and MA2 dataset con-tains ?compare?
problems so the cross validationaccuracy on ?IXL?
for ?compare?
problems is 0.cannot solve this problem and we need to eitheruse a better representation or a more powerfullearning algorithm to be able to answer correctly.Another interesting example of this kind is thefollowing: ?For his car , Mike spent $118.54 onspeakers and $106.33 on new tires .
Mike wanted3 CD ?s for $4.58 but decided not to .
In total ,how much did Mike spend on car parts?
?Incomplete IsA Knowledge: For the prob-lem ?Tom bought a skateboard for $ 9.46 , andspent $ 9.56 on marbles .
Tom also spent $ 14.50on shorts .
In total , how much did Tom spendon toys ?
?, it is important to know that ?skate-board?
and ?marbles?
are toys but ?shorts?
are not.However, such knowledge is not always present inConceptNet which results in error.Parser Issue: Error in dependency parsing isanother source of error.
Since the attribute valuesare computed from the dependency parse tree, awrong assignment (mostly for verbs) often makesthe entity irrelevant to the computation.6 ConclusionSolving math word problems often requires ex-plicit modeling of the word.
In this research, weuse well-known math formulas to model the wordproblem and develop an algorithm that learns tomap the assertions in the story to the correct for-mula.
Our future plan is to apply this model togeneral arithmetic problems which require multi-ple applications of formulas.7 AcknowledgementWe thank NSF for the DataNet Federation Consor-tium grant OCI-0940841 and ONR for their grantN00014-13-1-0334 for partially supporting this re-search.2152ReferencesEneko Agirre, Mona Diab, Daniel Cer, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In Proceedingsof the First Joint Conference on Lexical and Com-putational Semantics-Volume 1: Proceedings of themain conference and the shared task, and Volume2: Proceedings of the Sixth International Workshopon Semantic Evaluation, pages 385?393.
Associa-tion for Computational Linguistics.Daniel G Bobrow.
1964a.
Natural language input for acomputer problem solving system.Daniel G. Bobrow.
1964b.
A question-answeringsystem for high school algebra word problems.
InProceedings of the October 27-29, 1964, Fall JointComputer Conference, Part I, AFIPS ?64 (Fall, partI), pages 591?614, New York, NY, USA.
ACM.Samuel R Bowman, Gabor Angeli, Christopher Potts,and Christopher D Manning.
2015.
A large anno-tated corpus for learning natural language inference.arXiv preprint arXiv:1508.05326.Peter Clark and Oren Etzioni.
2016.
My computeris an honor student but how intelligent is it?
stan-dardized tests as a measure of ai.
AI Magazine.
(Toappear).Peter Clark.
2015.
Elementary school science andmath tests as a driver for ai: Take the aristo chal-lenge!
In AAAI, pages 4019?4021.Ido Dagan, Bill Dolan, Bernardo Magnini, and DanRoth.
2010.
Recognizing textual entailment: Ra-tional, evaluation and approaches?erratum.
NaturalLanguage Engineering, 16(01):105?105.Marie-Catherine De Marneffe and Christopher D Man-ning.
2008.
Stanford typed dependencies manual.Technical report, Technical report, Stanford Univer-sity.Edward A Feigenbaum and Julian Feldman.
1963.Computers and thought.Charles R Fletcher.
1985.
Understanding and solvingarithmetic word problems: A computer simulation.Behavior Research Methods, Instruments, & Com-puters, 17(5):565?571.Dan A Hinsley, John R Hayes, and Herbert A Simon.1977.
From words to equations: Meaning and repre-sentation in algebra word problems.
Cognitive pro-cesses in comprehension, 329.Mohammad Javad Hosseini, Hannaneh Hajishirzi,Oren Etzioni, and Nate Kushman.
2014.
Learningto solve arithmetic word problems with verb catego-rization.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 523?533.Walter Kintsch and James G Greeno.
1985.
Un-derstanding and solving word arithmetic problems.Psychological review, 92(1):109.Rik Koncel-Kedziorski, Hannaneh Hajishirzi, AshishSabharwal, Oren Etzioni, and Siena Dumas Ang.2015.
Parsing algebraic word problems into equa-tions.
Transactions of the Association for Computa-tional Linguistics, 3:585?597.Nate Kushman, Yoav Artzi, Luke Zettlemoyer, andRegina Barzilay.
2014.
Learning to automaticallysolve algebra word problems.
Association for Com-putational Linguistics.Hector J Levesque.
2011.
The winograd schema chal-lenge.Hugo Liu and Push Singh.
2004.
Conceptneta practi-cal commonsense reasoning tool-kit.
BT technologyjournal, 22(4):211?226.Christopher D Manning, Mihai Surdeanu, John Bauer,Jenny Rose Finkel, Steven Bethard, and David Mc-Closky.
2014.
The stanford corenlp natural lan-guage processing toolkit.
In ACL (System Demon-strations), pages 55?60.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Anirban Mukherjee and Utpal Garain.
2008.
A reviewof methods for automatic understanding of naturallanguage mathematical problems.
Artificial Intelli-gence Review, 29(2):93?122.Matthew Richardson, Christopher JC Burges, and ErinRenshaw.
2013.
Mctest: A challenge dataset forthe open-domain machine comprehension of text.
InEMNLP, volume 1, page 2.Subhro Roy and Dan Roth.
2015.
Solving generalarithmetic word problems.
EMNLP.Shuming Shi, Yuehui Wang, Chin-Yew Lin, XiaojiangLiu, and Yong Rui.
2015.
Automatically solv-ing number word problems by semantic parsing andreasoning.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), Lisbon, Portugal.Robert Speer and Catherine Havasi.
2013.
Conceptnet5: A large semantic network for relational knowl-edge.
In The Peoples Web Meets NLP, pages 161?176.
Springer.Jason Weston, Antoine Bordes, Sumit Chopra, andTomas Mikolov.
2015.
Towards ai-complete ques-tion answering: A set of prerequisite toy tasks.arXiv preprint arXiv:1502.05698.Lipu Zhou, Shuaixiang Dai, and Liwei Chen.
2015.Learn to solve algebra word problems usingquadratic programming.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, pages 817?822.2153
