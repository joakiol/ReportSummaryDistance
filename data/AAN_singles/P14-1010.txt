Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 100?110,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLattice Desegmentation for Statistical Machine TranslationMohammad Salameh?Colin Cherry?Grzegorz Kondrak?
?Department of Computing Science?National Research Council CanadaUniversity of Alberta 1200 Montreal RoadEdmonton, AB, T6G 2E8, Canada Ottawa, ON, K1A 0R6, Canada{msalameh,gkondrak}@ualberta.ca Colin.Cherry@nrc-cnrc.gc.caAbstractMorphological segmentation is an effec-tive sparsity reduction strategy for statis-tical machine translation (SMT) involv-ing morphologically complex languages.When translating into a segmented lan-guage, an extra step is required to deseg-ment the output; previous studies have de-segmented the 1-best output from the de-coder.
In this paper, we expand our trans-lation options by desegmenting n-best listsor lattices.
Our novel lattice desegmenta-tion algorithm effectively combines bothsegmented and desegmented views of thetarget language for a large subspace ofpossible translation outputs, which allowsfor inclusion of features related to the de-segmentation process, as well as an un-segmented language model (LM).
We in-vestigate this technique in the context ofEnglish-to-Arabic and English-to-Finnishtranslation, showing significant improve-ments in translation quality over deseg-mentation of 1-best decoder outputs.1 IntroductionMorphological segmentation is considered to beindispensable when translating between Englishand morphologically complex languages such asArabic.
Morphological complexity leads to muchhigher type to token ratios than English, whichcan create sparsity problems during translationmodel estimation.
Morphological segmentationaddresses this issue by splitting surface forms intomeaningful morphemes, while also performing or-thographic transformations to further reduce spar-sity.
For example, the Arabic noun ??Y??
lldwl?to the countries?
is segmented as l+ ?to?
Aldwl?the countries?.
When translating from Arabic,this segmentation process is performed as inputpreprocessing and is otherwise transparent to thetranslation system.
However, when translatinginto Arabic, the decoder produces segmented out-put, which must be desegmented to produce read-able text.
For example, l+ Aldwl must be con-verted to lldwl.Desegmentation is typically performed as apost-processing step that is independent from thedecoding process.
While this division of labor isuseful, the pipeline approach may prevent the de-segmenter from recovering from errors made bythe decoder.
Despite the efforts of the decoder?svarious component models, the system may pro-duce mismatching segments, such as s+ hzymp,which pairs the future particle s+ ?will?
with anoun hzymp ?defeat?, instead of a verb.
In this sce-nario, there is no right desegmentation; the post-processor has been dealt a losing hand.In this work, we show that it is possible tomaintain the sparsity-reducing benefit of segmen-tation while translating directly into unsegmentedtext.
We desegment a large set of possible de-coder outputs by processing n-best lists or lat-tices, which allows us to consider both the seg-mented and desegmented output before locking inthe decoder?s decision.
We demonstrate that sig-nificant improvements in translation quality can beachieved by training a linear model to re-rank thistransformed translation space.2 Related WorkTranslating into morphologically complex lan-guages is a challenging and interesting task thathas received much recent attention.
Most tech-niques approach the problem by transforming thetarget language in some manner before training thetranslation model.
They differ in what transforma-tions are performed and at what stage they are re-versed.
The transformation might take the form ofa morphological analysis or a morphological seg-mentation.1002.1 Morphological AnalysisMany languages have access to morphological an-alyzers, which annotate surface forms with theirlemmas and morphological features.
Bojar (2007)incorporates such analyses into a factored model,to either include a language model over target mor-phological tags, or model the generation of mor-phological features.
Other approaches train anSMT system to predict lemmas instead of surfaceforms, and then inflect the SMT output as a post-processing step (Minkov et al, 2007; Clifton andSarkar, 2011; Fraser et al, 2012; El Kholy andHabash, 2012b).
Alternatively, one can reparame-terize existing phrase tables as exponential mod-els, so that translation probabilities account forsource context and morphological features (Jeonget al, 2010; Subotin, 2011).
Of these approaches,ours is most similar to the translate-then-inflect ap-proach, except we translate and then desegment.In particular, Toutanova et al (2008) inflect andre-rank n-best lists in a similar manner to how wedesegment and re-rank n-best lists or lattices.2.2 Morphological SegmentationInstead of producing an abstract feature layer,morphological segmentation transforms the tar-get sentence by segmenting relevant morphemes,which are then handled as regular tokens duringalignment and translation.
This is done to reducesparsity and to improve correspondence with thesource language (usually English).
Such a seg-mentation can be produced as a byproduct of anal-ysis (Oflazer and Durgar El-Kahlout, 2007; Badret al, 2008; El Kholy and Habash, 2012a), or maybe produced using an unsupervised morphologicalsegmenter such as Morfessor (Luong et al, 2010;Clifton and Sarkar, 2011).
Work on target lan-guage morphological segmentation for SMT canbe divided into three subproblems: segmentation,desegmentation and integration.
Our work is con-cerned primarily with the integration problem, butwe will discuss each subproblem in turn.The usefulness of a target segmentation de-pends on its correspondence to the source lan-guage.
If a morphological feature does not man-ifest itself as a separate token in the source, thenit may be best to leave its corresponding segmentattached to the stem.
A number of studies havelooked into what granularity of segmentation isbest suited for a particular language pair (Oflazerand Durgar El-Kahlout, 2007; Badr et al, 2008;Clifton and Sarkar, 2011; El Kholy and Habash,2012a).
Since our focus here is on integrating seg-mentation into the decoding process, we simplyadopt the segmentation strategies recommendedby previous work: the Penn Arabic Treebankscheme for English-Arabic (El Kholy and Habash,2012a), and an unsupervised scheme for English-Finnish (Clifton and Sarkar, 2011).Desegmentation is the process of convertingsegmented words into their original surface form.For many segmentations, especially unsupervisedones, this amounts to simple concatenation.
How-ever, more complex segmentations, such as theArabic tokenization provided by MADA (Habashet al, 2009), require further orthographic adjust-ments to reverse normalizations performed dur-ing segmentation.
Badr et al (2008) presenttwo Arabic desegmentation schemes: table-basedand rule-based.
El Kholy and Habash (2012a)provide an extensive study on the influence ofsegmentation and desegmentation on English-to-Arabic SMT.
They introduce an additional deseg-mentation technique that augments the table-basedapproach with an unsegmented language model.Salameh et al (2013) replace rule-based deseg-mentation with a discriminatively-trained char-acter transducer.
In this work, we adopt theTable+Rules approach of El Kholy and Habash(2012a) for English-Arabic, while concatenationis sufficient for English-Finnish.Work on integration attempts to improve SMTperformance for morphologically complex targetlanguages by going beyond simple pre- and post-processing.
Oflazer and Durgar El-Kahlout (2007)desegment 1000-best lists for English-to-Turkishtranslation to enable scoring with an unsegmentedlanguage model.
Unlike our work, they replacethe segmented language model with the unseg-mented one, allowing them to tune the linearmodel parameters by hand.
We use both seg-mented and unsegmented language models, andtune automatically to optimize BLEU.Like us, Luong et al (2010) tune on un-segmented references,1and translate with bothsegmented and unsegmented language modelsfor English-to-Finnish translation.
However,they adopt a scheme of word-boundary-aware1Tuning on unsegmented references does not require sub-stantial modifications to the standard SMT pipeline.
For ex-ample, Badr et al (2008) also tune on unsegmented refer-ences by simply desegmenting SMT output before MERTcollects sufficient statistics for BLEU.101morpheme-level phrase extraction, meaning thattarget phrases include only complete words,though those words are segmented into mor-phemes.
This enables full decoder integration,where we do n-best and lattice re-ranking.
Butit also comes at a substantial cost: when targetphrases include only complete words, the systemcan only generate word forms that were seen dur-ing training.
In this setting, the sparsity reduc-tion from segmentation helps word alignment andtarget language modeling, but it does not resultin a more expressive translation model.
Further-more, it becomes substantially more difficult tohave non-adjacent source tokens contribute mor-phemes to a single target word.
For example,when translating ?with his blue car?
into the Ara-bic ZAP?Q?
@ ?KPAJ?.bsyArth AlzrqA?, the target wordbsyArth is composed of three tokens: b+ ?with?,syArp ?car?
and +h ?his?.
With word-boundary-aware phrase extraction, a phrase pair containingall of ?with his blue car?
must have been seen inthe parallel data to translate the phrase correctly attest time.
With lattice desegmentation, we needonly to have seen AlzrqA?
?blue?
and the threemorphological pieces of bsyArth for the decoderand desegmenter to assemble the phrase.3 MethodsOur goal in this work is to benefit fromthe sparsity-reducing properties of morphologicalsegmentation while simultaneously allowing thesystem to reason about the final surface forms ofthe target language.
We approach this problem byaugmenting an SMT system built over target seg-ments with features that reflect the desegmentedtarget words.
In this section, we describe our vari-ous strategies for desegmenting the SMT system?soutput space, along with the features that we addto take advantage of this desegmented view.3.1 BaselinesThe two obvious baseline approaches each decodeusing one view of the target language.
The un-segmented approach translates without segment-ing the target.
This trivially allows for an unseg-mented language model and never makes deseg-mentation errors.
However, it suffers from datasparsity and poor token-to-token correspondencewith the source language.The one-best desegmentation approach seg-ments the target language at training time andthen desegments the one-best output in post-processing.
This resolves the sparsity issue, butdoes not allow the decoder to take into accountfeatures of the desegmented target.
To the best ofour knowledge, we are the first group to go beyondone-best desegmentation for English-to-Arabictranslation.
In English-to-Finnish, although alter-native integration strategies have seen some suc-cess (Luong et al, 2010), the current state-of-the-art performs one-best-desegmentation (Cliftonand Sarkar, 2011).3.2 n-best DesegmentationThe one-best approach can be extended easily bydesegmenting n-best lists of segmented decoderoutput.
Doing so enables the inclusion of anunsegmented target language model, and with asmall amount of bookkeeping, it also allows theinclusion of features related to the operations per-formed during desegmentation (see Section 3.4).With new features reflecting the desegmented out-put, we can re-tune our enhanced linear model ona development set.
Following previous work, wewill desegment 1000-best lists (Oflazer and Dur-gar El-Kahlout, 2007).Once n-best lists have been desegmented, wecan tune on unsegmented references as a side-benefit.
This could improve translation quality,as it brings our training scenario closer to our testscenario (test BLEU is always measured on unseg-mented references).
In particular, it could addressissues with translation length mismatch.
Previouswork that has tuned on unsegmented referenceshas reported mixed results (Badr et al, 2008; Lu-ong et al, 2010).3.3 Lattice DesegmentationAn n-best list reflects a tiny portion of a decoder?ssearch space, typically fixed at 1000 hypotheses.Lattices2can represent an exponential number ofhypotheses in a compact structure.
In this section,we discuss how a lattice from a multi-stack phrase-based decoder such as Moses (Koehn et al, 2007)can be desegmented to enable word-level features.Finite State AnalogyA phrase-based decoder produces its output fromleft to right, with each operation appendingthe translation of a source phrase to a grow-ing target hypothesis.
Translation continues un-2Or forests for hierarchical and syntactic decoders.1020 1b+ 2lEbp5+hm4+hA3AlTfl(a)?(b)?
(c)?1AlTfl:AlTfl0b+:<epsilon>2lEbp:<epsilon><epsilon>:blEbp+hA:blEbthA+hm:blEbthm05blEbthm4blEbthA2blEbp3AlTflTransduces?into?Figure 1: The finite state pipeline for a lattice translating the English fragment ?with the child?s game?.The input morpheme lattice (a) is desegmented by composing it with the desegmenting transducer (b) toproduce the word lattice (c).
The tokens in (a) are: b+ ?with?, lEbp ?game?, +hm ?their?, +hA ?her?,and AlTfl ?the child?.til each source word has been covered exactlyonce (Koehn et al, 2003).The search graph of a phrase-based decoder canbe interpreted as a lattice, which can be interpretedas a finite state acceptor over target strings.
In itsmost natural form, such an acceptor emits targetphrases on each edge, but it can easily be trans-formed into a form with one edge per token, asshown in Figure 1a.
This is sometimes referred toas a word graph (Ueffing et al, 2002), although inour case the segmented phrase table also producestokens that correspond to morphemes.Our goal is to desegment the decoder?s outputlattice, and in doing so, gain access to a compact,desegmented view of a large portion of the trans-lation search space.
This can be accomplished bycomposing the lattice with a desegmenting trans-ducer that consumes morphemes and outputs de-segmented words.
This transducer must be ableto consume every word in our lattice?s output vo-cabulary.
We define a word using the followingregular expression:[prefix]* [stem] [suffix]* | [prefix]+ [suffix]+(1)where [prefix], [stem] and [suffix] are non-overlapping sets of morphemes, whose membersare easily determined using the segmenter?s seg-ment boundary markers.3The second disjunct ofEquation 1 covers words that have no clear stem,such as the Arabic ??
lh ?for him?, segmented as l+?for?
+h ?him?.
Equation 1 may need to be modi-fied for other languages or segmentation schemes,but our techniques generalize to any definition thatcan be written as a regular expression.A desegmenting transducer can be constructedby first encoding our desegmenter as a table thatmaps morpheme sequences to words.
Regardlessof whether the original desegmenter was basedon concatenation, rules or table-lookup, it can beencoded as a lattice-specific table by applying itto an enumeration of all words found in the lat-tice.
We can then transform that table into a fi-nite state transducer with one path per table en-try.
Finally, we take the closure of this trans-ducer, so that the resulting machine can transduceany sequence of words.
The desegmenting trans-3Throughout this paper, we use ?+?
to mark morphemesas prefixes or suffixes, as in w+ or +h.
In Equation 1 only,we overload ?+?
as the Kleene cross: X+ == XX?.103ducer for our running example is shown in Fig-ure 1b.
Note that tokens requiring no desegmen-tation simply emit themselves.
The lattice (Fig-ure 1a) can then be desegmented by composing itwith the transducer (1b), producing a desegmentedlattice (1c).
This is a natural place to introducefeatures that describe the desegmentation process,such as scores provided by a desegmentation table,which can be incorporated into the desegmentingtransducer?s edge weights.We now have a desegmented lattice, but it hasnot been annotated with an unsegmented (word-level) language model.
In order to annotate latticeedges with an n-gram LM, every path coming intoa node must end with the same sequence of (n?1)tokens.
If this property does not hold, then nodesmust be split until it does.4This property is main-tained by the decoder?s recombination rules for thesegmented LM, but it is not guaranteed for the de-segmented LM.
Indeed, the expanded word-levelcontext is one of the main benefits of incorporatinga word-level LM.
Fortunately, LM annotation aswell as any necessary lattice modifications can beperformed simultaneously by composing the de-segmented lattice with a finite state acceptor en-coding the LM (Roark et al, 2011).In summary, we are given a segmented lattice,which encodes the decoder?s translation space asan acceptor over morphemes.
We compose thisacceptor with a desegmenting transducer, and thenwith an unsegmented LM acceptor, producing afully annotated, desegmented lattice.
Instead ofusing a tool kit such as OpenFst (Allauzen etal., 2007), we implement both the desegmentingtransducer and the LM acceptor programmatically.This eliminates the need to construct intermediatemachines, such as the lattice-specific desegmenterin Figure 1b, and facilitates working with edgesannotated with feature vectors as opposed to sin-gle weights.Programmatic DesegmentationLattice desegmentation is a non-local lattice trans-formation.
That is, the morphemes forming a wordmight span several edges, making desegmentationnon-trivial.
Luong et al (2010) address this prob-lem by forcing the decoder?s phrase table to re-spect word boundaries, guaranteeing that each de-segmentable token sequence is local to an edge.4Or the LM composition can be done dynamically, ef-fectively decoding the lattice with a beam or cube-prunedsearch (Huang and Chiang, 2007).Inspired by the use of non-local features in forestdecoding (Huang, 2008), we present an algorithmto find chains of edges that correspond to deseg-mentable token sequences, allowing lattice deseg-mentation with no phrase-table restrictions.
Thisalgorithm can be seen as implicitly constructing acustomized desegmenting transducer and compos-ing it with the input lattice on the fly.Before describing the algorithm, we definesome notation.
An input morpheme lattice is atriple ?ns,N , E?, where N is a set of nodes, E isa set of edges, and ns?
N is the start node thatbegins each path through the lattice.
Each edgee ?
E is a 4-tuple ?from, to, lex , w?, where from ,to ?
N are head and tail nodes, lex is a singletoken accepted by this edge, and w is the (po-tentially vector-valued) edge weight.
Tokens aredrawn from one of three non-overlapping morpho-syntactic sets: lex ?
Prefix ?
Stem ?
Suffix ,where tokens that do not require desegmentation,such as complete words, punctuation and num-bers, are considered to be in Stem .
It is also usefulto consider the set of all outgoing edges for a noden.out = {e ?
E|e.from = n}.With this notation in place, we can define achain c to be a sequence of edges [e1.
.
.
el] suchthat for 1 ?
i < l : ei.to = ei+1.from .
Wedenote singleton chains with [e], and when unam-biguous, we abbreviate longer chains with theirstart and end node [e1.from ?
el.to].
A chainis valid if it emits the beginning of a word as de-fined by the regular expression in Equation 1.
Avalid chain is complete if its edges form an entireword, and if it is part of a path through the lat-tice that consists only of words.
In Figure 1a, thecomplete chains are [0 ?
2], [0 ?
4], [0 ?
5],and [2 ?
3].
The path restriction on completechains forces words to be bounded by other wordsin order to be complete.5For example, if we re-moved the edge 2 ?
3 (AlTfl) from Figure 1a,then [0?
2] ([b+ lEbp]) would cease to be a com-plete chain, but it would still be a valid chain.
Notethat in the finite-state analogy, the path restrictionis implicit in the composition operation.Algorithm 1 desegments a lattice by finding allcomplete chains and replacing each one with a sin-gle edge.
It maintains a work list of nodes thatlie on the boundary between words, and for eachnode on this list, it launches a depth first search5Sentence-initial suffix morphemes and sentence-finalprefix morphemes represent a special case that we omit forthe sake of brevity.
Lacking stems, they are left segmented.104Algorithm 1 Desegment a lattice ?ns,N , E?
{Initialize output lattice and work list WL}n?s= ns, N?= ?, E?= ?, WL = [ns]while n = WL.pop() do{Work on each node only once}if n ?
N?then continueN?= N??
{n}{Initialize the chain stack C}C = ?for e ?
n.out doif [e] is valid then C.push([e]){Depth-first search for complete chains}while [e1, .
.
.
, el] = C.pop() do{Attempt to extend chain}for e ?
el.to.out doif [e1.
.
.
el, e] is valid thenC.push([e1, .
.
.
, el, e])elseMark [e1, .
.
.
, el] as complete{Desegment complete chains}if [e1, .
.
.
, el] is complete thenWL.push(el.to)E?= E??
{deseg([e1, .
.
.
, el])}return ?n?s,N?, E?
?to find all complete chains extending from it.
Thesearch recognizes the valid chain c to be completeby finding an edge e such that c+ e forms a chain,but not a valid one.
By inspection of Equation 1,this can only happen when a prefix or stem fol-lows a stem or suffix, which always marks a wordboundary.
The chains found by this search are de-segmented and then added to the output lattice asedges.
The nodes at end points of these chains areadded to the work list, as they lie at word bound-aries by definition.
Note that although this algo-rithm creates completely new edges, the resultingnode set N?will be a subset of the input node setN .
The complementN ?N?will consist of nodesthat are word-internal in all paths through the inputlattice, such as node 1 in Figure 1a.Programmatic LM IntegrationProgrammatic composition of a lattice with ann-gram LM acceptor is a well understood prob-lem.
We use a dynamic program to enumerate all(n ?
1)-word contexts leading into a node, andthen split the node into multiple copies, one foreach context.
With each node corresponding to asingle LM context, annotation of outgoing edgeswith n-gram LM scores is straightforward.3.4 Desegmentation FeaturesOur re-ranker has access to all of the features usedby the decoder, in addition to a number of featuresenabled by desegmentation.Desegmentation Score We use a table-baseddesegmentation method for Arabic, which is basedon segmenting an Arabic training corpus andmemorizing the observed transformations to re-verse them later.
Finnish does not require a ta-ble, as all words can be desegmented with sim-ple concatenation.
The Arabic table consists ofX ?
Y entries, where X is a target morphemesequence and Y is a desegmented surface form.Several entries may share the same X , resultingin multiple desegmentation options.
For the sakeof symmetry with the unambiguous Finnish case,we augment Arabic n-best lists or lattices withonly the most frequent desegmentation Y .6Weprovide the desegmentation score log p(Y |X)=log(count of X ?
Ycount of X)as a feature, to indicate the en-try?s ambiguity in the training data.7When an X ismissing from the table, we fall back on a set of de-segmentation rules (El Kholy and Habash, 2012a)and this feature is set to 0.
This feature is always0 for English-Finnish.Contiguity One advantage of our approach isthat it allows discontiguous source words to trans-late into a single target word.
In order to maintainsome control over this powerful capability, we cre-ate three binary features that indicate the contigu-ity of a desegmentation.
The first feature indicatesthat the desegmented morphemes were translatedfrom contiguous source words.
The second indi-cates that the source words contained a single dis-contiguity, as in a word-by-word translation of the?with his blue car?
example from Section 2.2.
Thethird indicates two or more discontiguities.Unsegmented LM A 5-gram LM trained on un-segmented target text is used to assess the fluencyof the desegmented word sequence.4 Experimental SetupWe train our English-to-Arabic system using 1.49million sentence pairs drawn from the NIST 2012training set, excluding the UN data.
This trainingset contains about 40 million Arabic tokens before6Allowing the re-ranker to choose between multiple Y s isa natural avenue for future work.7We also experimented on log p(X|Y ) as an additionalfeature, but observed no improvement in translation quality.105segmentation, and 47 million after segmentation.We tune on the NIST 2004 evaluation set (1353sentences) and evaluate on NIST 2005 (1056 sen-tences).
As these evaluation sets are intended forArabic-to-English translation, we select the firstEnglish reference to use as our source text.Our English-to-Finnish system is trained on thesame Europarl corpus as Luong et al (2010) andClifton and Sarkar (2011), which has roughly onemillion sentence pairs.
We also use their develop-ment and test sets (2000 sentences each).4.1 SegmentationFor Arabic, morphological segmentation is per-formed by MADA 3.2 (Habash et al, 2009), usingthe Penn Arabic Treebank (PATB) segmentationscheme as recommended by El Kholy and Habash(2012a).
For both segmented and unsegmentedArabic, we further normalize the script by convert-ing different forms of Alif @@@ @ and Ya ?
?tobare Alif @ and dotless Ya ?.
To generate the de-segmentation table, we analyze the segmentationsfrom the Arabic side of the parallel training datato collect mappings from morpheme sequences tosurface forms.For Finnish, we adopt the Unsup L-match seg-mentation technique of Clifton and Sarkar (2011),which uses Morfessor (Creutz and Lagus, 2005)to analyze the 5,000 most frequent Finnish words.The analysis is then applied to the Finnish side ofthe parallel text, and a list of segmented suffixesis collected.
To improve coverage, words are fur-ther segmented according to their longest match-ing suffix from the list.
As Morfessor does notperform any orthographic normalizations, it can bedesegmented with simple concatenation.4.2 SystemsWe align the parallel data with GIZA++ (Och etal., 2003) and decode using Moses (Koehn et al,2007).
The decoder?s log-linear model includes astandard feature set.
Four translation model fea-tures encode phrase translation probabilities andlexical scores in both directions.
Seven distor-tion features encode a standard distortion penaltyas well as a bidirectional lexicalized reorderingmodel.
A KN-smoothed 5-gram language modelis trained on the target side of the parallel data withSRILM (Stolcke, 2002).
Finally, we include wordand phrase penalties.
The decoder uses the defaultparameters for English-to-Arabic, except that themaximum phrase length is set to 8.
For English-to-Finnish, we follow Clifton and Sarkar (2011) insetting the hypothesis stack size to 100, distortionlimit to 6, and maximum phrase length to 20.The decoder?s log-linear model is tuned withMERT (Och, 2003).
Re-ranking models are tunedusing a batch variant of hope-fear MIRA (Chi-ang et al, 2008; Cherry and Foster, 2012), us-ing the n-best variant for n-best desegmentation,and the lattice variant for lattice desegmentation.MIRA was selected over MERT because we havean in-house implementation that can tune on lat-tices very quickly.
During development, we con-firmed that MERT and MIRA perform similarly,as is expected with fewer than 20 features.
Boththe decoder?s log-linear model and the re-rankingmodels are trained on the same development set.Historically, we have not seen improvements fromusing different tuning sets for decoding and re-ranking.
Lattices are pruned to a density of 50edges per word before re-ranking.We test four different systems.
Our first base-line is Unsegmented, where we train on unseg-mented target text, requiring no desegmentationstep.
Our second baseline is 1-best Deseg, wherewe train on segmented target text and desegmentthe decoder?s 1-best output.
Starting from the sys-tem that produced 1-best Deseg, we then output ei-ther 1000-best lists or lattices to create our two ex-perimental systems.
The 1000-best Deseg systemdesegments, augments and re-ranks the decoder?s1000-best list, while Lattice Deseg does the samein the lattice.
We augment n-best lists and latticesusing the features described in Section 3.4.8We evaluate our system using BLEU (Papineniet al, 2002) and TER (Snover et al, 2006).
Fol-lowing Clark et al (2011), we report averagescores over five random tuning replications to ac-count for optimizer instability.
For the baselines,this means 5 runs of decoder tuning.
For the de-segmenting re-rankers, this means 5 runs of re-ranker tuning, each working on n-best lists or lat-tices produced by the same (representative) de-coder weights.
We measure statistical significanceusing MultEval (Clark et al, 2011), which imple-ments a stratified approximate randomization testto account for multiple tuning replications.8Development experiments on a small-data English-to-Arabic scenario indicated that the Desegmentation Score wasnot particularly useful, so we exclude it from the main com-parison, but include it in the ablation experiments.1065 ResultsTables 1 and 2 report results averaged over 5 tun-ing replications on English-to-Arabic and English-to-Finnish, respectively.
In all scenarios, both1000-best Deseg and Lattice Deseg significantlyoutperform the 1-best Deseg baseline (p < 0.01).For English-to-Arabic, 1-best desegmentationresults in a 0.7 BLEU point improvement overtraining on unsegmented Arabic.
Moving to lat-tice desegmentation more than doubles that im-provement, resulting in a BLEU score of 34.4 andan improvement of 1.0 BLEU point over 1-bestdesegmentation.
1000-best desegmentation alsoworks well, resulting in a 0.6 BLEU point im-provement over 1-best.
Lattice desegmentation issignificantly better (p < 0.01) than 1000-best de-segmentation.For English-to-Finnish, the Unsup L-match seg-mentation with 1-best desegmentation does notimprove over the unsegmented baseline.
The seg-mentation may be addressing issues with modelsparsity, but it is also introducing errors that wouldhave been impossible had words been left un-segmented.
In fact, even with our lattice deseg-menter providing a boost, we are unable to seea significant improvement over the unsegmentedmodel.
As we attempted to replicate the approachof Clifton and Sarkar (2011) exactly by workingwith their segmented data, this difference is likelydue to changes in Moses since the publication oftheir result.
Nonetheless, the 1000-best and latticedesegmenters both produce significant improve-ments over the 1-best desegmentation baseline,with Lattice Deseg achieving a 1-point improve-ment in TER.
These results match the establishedstate-of-the-art on this data set, but also indicatethat there is still room for improvement in identi-fying the best segmentation strategy for English-to-Finnish translation.We also tried a similar Morfessor-based seg-mentation for Arabic, which has an unsegmentedtest set BLEU of 32.7.
As in Finnish, the 1-bestdesegmentation using Morfessor did not surpassthe unsegmented baseline, producing a test BLEUof only 31.4 (not shown in Table 1).
Lattice deseg-mentation was able to boost this to 32.9, slightlyabove 1-best desegmentation, but well below ourbest MADA desegmentation result of 34.4.
Thereappears to be a large advantage to using MADA?ssupervised segmentation in this scenario.Model Dev TestBLEU BLEU TERUnsegmented 24.4 32.7 49.41-best Deseg 24.4 33.4 48.61000-best Deseg 25.0 34.0 48.0Lattice Deseg 25.2 34.4 47.7Table 1: Results for English-to-Arabic translationusing MADA?s PATB segmentation.Model Dev TestBLEU BLEU TERUnsegmented 15.4 15.1 70.81-best Deseg 15.3 14.8 71.91000-best Deseg 15.4 15.1 71.5Lattice Deseg 15.5 15.1 70.9Table 2: Results for English-to-Finnish translationusing unsupervised segmentation.5.1 AblationWe conducted an ablation experiment on English-to-Arabic to measure the impact of the various fea-tures described in Section 3.4.
Table 3 comparesdifferent combinations of features using lattice de-segmentation.
The unsegmented LM alone yieldsa 0.4 point improvement over the 1-best deseg-mentation score.
Adding contiguity indicators ontop of the unsegmented LM results in another 0.6point improvement.
As anticipated, the tuner as-signs negative weights to discontiguous cases, en-couraging the re-ranker to select a safer transla-tion path when possible.
Judging from the out-put on the NIST 2005 test set, the system usesthese discontiguous desegmentations very rarely:only 5% of desegmented tokens align to discon-tiguous source phrases.
Adding the desegmenta-tion score to these two feature groups does not im-prove performance, confirming the results we ob-served during development.
The desegmentationscore would likely be useful in a scenario wherewe provide multiple desegmentation options to there-ranker; for now, it indicates only the ambiguityof a fixed choice, and is likely redundant with in-formation provided by the language model.5.2 Error AnalysisIn order to better understand the source of ourimprovements in the English-to-Arabic scenario,we conducted an extensive manual analysis ofthe differences between 1-best and lattice deseg-107Features dev test1-best Deseg 24.5 33.4+ Unsegmented LM 24.9 33.8+ Contiguity 25.2 34.4+ Desegmentation Score 25.2 34.3Table 3: The effect of feature ablation on BLEUscore for English-to-Arabic translation with latticedesegmentation.mentation on our test set.
We compared theoutput of the two systems using the Unix toolwdiff , which transforms a solution to the longest-common-subsequence problem into a sequenceof multi-word insertions and deletions (Hunt andMcIlroy, 1976).
We considered adjacent insertion-deletion pairs to be (potentially phrasal) substitu-tions, and collected them into a file, omitting anyunpaired insertions or deletions.
We then sampled650 cases where the two sides of the substitutionwere deemed to be related, and divided these casesinto categories based on how the lattice desegmen-tation differs from the one-best desegmentation.We consider a phrase to be correct only if it canbe found in the reference.Table 4 breaks down per-phrase accuracy ac-cording to four manually-assigned categories: (1)clitical ?
the two systems agree on a stem, but atleast one clitic, often a prefix denoting a prepo-sition or determiner, was dropped, added or re-placed; (2) lexical ?
a word was changed to a mor-phologically unrelated word with a similar mean-ing; (3) inflectional ?
the words have the samestem, but different inflection due to a change ingender, number or verb tense; (4) part-of-speech?
the two systems agree on the lemma, but haveselected different parts of speech.For each case covering a single phrasal differ-ence, we compare the phrases from each systemto the reference.
We report the number of in-stances where each system matched the reference,as well as cases where they were both incorrect.The majority of differences correspond to clitics,whose correction appears to be a major source ofthe improvements obtained by lattice desegmen-tation.
This category is challenging for the de-coder because English prepositions tend to corre-spond to multiple possible forms when translatedinto Arabic.
It also includes the frequent casesinvolving the nominal determiner prefix Al ?the?
(left unsegmented by the PATB scheme), and theLatticeCorrect1-bestCorrectBothIncorrectClitical 157 71 79Lexical 61 39 80Inflectional 37 32 47Part-of-speech 19 17 11Table 4: Error analysis for English-to-Arabictranslation based on 650 sampled instances.sentence-initial conjunction w+ ?and?.
The sec-ond most common category is lexical, where theunsegmented LM has drastically altered the choiceof translation.
The remaining categories show nomajor advantage for either system.6 ConclusionWe have explored deeper integration of morpho-logical desegmentation into the statistical machinetranslation pipeline.
We have presented a novel,finite-state-inspired approach to lattice desegmen-tation, which allows the system to account for adesegmented view of many possible translations,without any modification to the decoder or anyrestrictions on phrase extraction.
When appliedto English-to-Arabic translation, lattice desegmen-tation results in a 1.0 BLEU point improvementover one-best desegmentation, and a 1.7 BLEUpoint improvement over unsegmented translation.We have also applied our approach to English-to-Finnish translation, and although segmentation ingeneral does not currently help, we are able toshow significant improvements over a 1-best de-segmentation baseline.In the future, we plan to explore introducingmultiple segmentation options into the lattice, andthe application of our method to a full morpho-logical analysis (as opposed to segmentation) ofthe target language.
Eventually, we would liketo replace the functionality of factored transla-tion models (Koehn and Hoang, 2007) with latticetransformation and augmentation.AcknowledgmentsThanks to Ann Clifton for generously provid-ing the data and segmentation for our English-to-Finnish experiments, and to Marine Carpuat andRoland Kuhn for their helpful comments on anearlier draft.
This research was supported by theNatural Sciences and Engineering Research Coun-cil of Canada.108ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In Proceedings of the Ninth InternationalConference on Implementation and Application ofAutomata, (CIAA 2007), volume 4783 of LectureNotes in Computer Science, pages 11?23.
Springer.http://www.openfst.org.Ibrahim Badr, Rabih Zbib, and James Glass.
2008.Segmentation for English-to-Arabic statistical ma-chine translation.
In Proceedings of ACL, pages153?156.Ond?rej Bojar.
2007.
English-to-Czech factored ma-chine translation.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages232?239, Prague, Czech Republic, June.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of HLT-NAACL, Montreal, Canada, June.David Chiang, Yuval Marton, and Philip Resnik.2008.
Online large-margin training of syntactic andstructural translation features.
In Proceedings ofEMNLP, pages 224?233.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testingfor statistical machine translation: Controlling foroptimizer instability.
In Proceedings of ACL, pages176?181.Ann Clifton and Anoop Sarkar.
2011.
Combin-ing morpheme-based machine translation with post-processing morpheme prediction.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 32?42, Portland, Oregon, USA, June.Mathias Creutz and Krista Lagus.
2005.
Induc-ing the morphological lexicon of a natural languagefrom unannotated text.
In In Proceedings of theInternational and Interdisciplinary Conference onAdaptive Knowledge Representation and Reasoning(AKRR05, pages 106?113.Ahmed El Kholy and Nizar Habash.
2012a.
Ortho-graphic and morphological processing for English?Arabic statistical machine translation.
MachineTranslation, 26(1-2):25?45, March.Ahmed El Kholy and Nizar Habash.
2012b.
Trans-late, predict or generate: Modeling rich morphologyin statistical machine translation.
Proceeding of theMeeting of the European Association for MachineTranslation.Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-bienne Cap.
2012.
Modeling inflection and word-formation in SMT.
In Proceedings of the 13th Con-ference of the European Chapter of the Associationfor Computational Linguistics, pages 664?674, Avi-gnon, France, April.
Association for ComputationalLinguistics.Nizar Habash, Owen Rambow, and Ryan Roth.
2009.Mada+tokan: A toolkit for Arabic tokenization,diacritization, morphological disambiguation, POStagging, stemming and lemmatization.
In KhalidChoukri and Bente Maegaard, editors, Proceedingsof the Second International Conference on ArabicLanguage Resources and Tools, Cairo, Egypt, April.The MEDAR Consortium.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages144?151, Prague, Czech Republic, June.Liang Huang.
2008.
Forest reranking: Discrimina-tive parsing with non-local features.
In Proceedingsof ACL-08: HLT, pages 586?594, Columbus, Ohio,June.James W. Hunt and M. Douglas McIlroy.
1976.
Analgorithm for differential file comparison.
Technicalreport, Bell Laboratories, June.Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,and Chris Quirk.
2010.
A discriminative lexiconmodel for complex morphology.
In The Ninth Con-ference of the Association for Machine Translationin the Americas.Philipp Koehn and Hieu Hoang.
2007.
Factoredtranslation models.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 868?876, Prague, Czech Republic, June.
Association forComputational Linguistics.Philipp Koehn, Franz Joesef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT-NAACL, pages 127?133.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 177?180, Prague, Czech Republic,June.
Association for Computational Linguistics.Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan.2010.
A hybrid morpheme-word representation formachine translation of morphologically rich lan-guages.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Process-ing, pages 148?157, Cambridge, MA, October.109Einat Minkov, Kristina Toutanova, and Hisami Suzuki.2007.
Generating complex morphology for machinetranslation.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 128?135, Prague, Czech Republic, June.Franz Josef Och, Hermann Ney, Franz Josef, andOch Hermann Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29.Franz Joseph Och.
2003.
Minimum error rate trainingfor statistical machine translation.
In Proceedings ofACL, pages 160?167.Kemal Oflazer and Ilknur Durgar El-Kahlout.
2007.Exploring different representational units inEnglish-to-Turkish statistical machine translation.In Proceedings of the Second Workshop on Statis-tical Machine Translation, pages 25?32, Prague,Czech Republic, June.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318.Brian Roark, Richard Sproat, and Izhak Shafran.
2011.Lexicographic semirings for exact automata encod-ing of sequence models.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 1?5, Portland, Oregon, USA, June.Mohammad Salameh, Colin Cherry, and GrzegorzKondrak.
2013.
Reversing morphological tokeniza-tion in English-to-Arabic SMT.
In Proceedings ofthe 2013 NAACL HLT Student Research Workshop,pages 47?53, Atlanta, Georgia, June.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In Intl.
Conf.
Spoken LanguageProcessing, pages 901?904.Michael Subotin.
2011.
An exponential translationmodel for target language morphology.
In Pro-ceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 230?238, Portland, Ore-gon, USA, June.Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.2008.
Applying morphology generation models tomachine translation.
In Proceedings of ACL-08:HLT, pages 514?522, Columbus, Ohio, June.Nicola Ueffing, Franz J. Och, and Hermann Ney.
2002.Generation of word graphs in statistical machinetranslation.
In Proceedings of EMNLP, pages 156?163, Philadelphia, PA, July.110
