Proceedings of the 6th Workshop on Statistical Machine Translation, pages 533?541,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsBayesian Extraction of Minimal SCFG Rules forHierarchical Phrase-based TranslationBaskaran SankaranSimon Fraser UniversityBurnaby BC, Canadabaskaran@cs.sfu.caGholamreza HaffariMonash UniversityMelbourne, Australiareza@monash.eduAnoop SarkarSimon Fraser UniversityBurnaby BC, Canadaanoop@cs.sfu.caAbstractWe present a novel approach for extractinga minimal synchronous context-free grammar(SCFG) for Hiero-style statistical machinetranslation using a non-parametric Bayesianframework.
Our approach is designed to ex-tract rules that are licensed by the word align-ments and heuristically extracted phrase pairs.Our Bayesian model limits the number ofSCFG rules extracted, by sampling from thespace of all possible hierarchical rules; addi-tionally our informed prior based on the lex-ical alignment probabilities biases the gram-mar to extract high quality rules leading to im-proved generalization and the automatic iden-tification of commonly re-used rules.
Weshow that our Bayesian model is able to ex-tract minimal set of hierarchical phrase ruleswithout impacting the translation quality asmeasured by the BLEU score.1 IntroductionHierarchical phrase-based (Hiero) machine transla-tion (Chiang, 2007) has attracted significant interestwithin the Machine Translation community.
It ex-tends phrase-based translation by automatically in-ferring a synchronous grammar from an aligned bi-text.
The synchronous context-free grammar linksnon-terminals in source and target languages.
De-coding in such systems employ a modified CKY-parser that is integrated with a language model.The primary advantage of Hiero-style systems liein their unsupervised model of syntax for transla-tion: allowing long-distance reordering and cap-turing certain syntactic constructions, particularlythose that involve discontiguous phrases.
It hasbeen demonstrated to be a successful frameworkwith comparable performance with other statisti-cal frameworks and suitable for large-scale cor-pora (Zollmann et al, 2008).
However, one of themajor difficulties in Hiero-style systems has been onlearning a concise and general synchronous gram-mar from the bitext.While most of the research in Hiero-style sys-tems is focused on the improving the decoder, andin particular the link to the language model, compar-atively few papers have considered the inference ofthe probabilistic SCFG from the word alignments.A majority of the systems employ the classic rule-extraction algorithm (Chiang, 2007) which extractsrules by replacing possible sub-spans (permitted bythe word alignments) with a non-terminal and thenusing relative frequencies to estimate the probabilis-tic synchronous context-free grammar.
One of theissues in building Hiero-style systems is in manag-ing the size of the synchronous grammar.
The origi-nal approach extracts a larger number of rules whencompared to a phrase-based system on the same dataleading to practical issues in terms of memory re-quirements and decoding speed.Extremely large Hiero phrase tables may also leadto statistical issues, where the probability mass hasto be shared by more rules: the probability p(e|f)has to be shared by all the rules having the samesource side string f , leading to fragmentation andresulting in many rules having very poor probability.Approaches to improve the inference (the induc-tion of the SCFG rules from the bitext) typicallyfollows two streams.
One focusses on filtering theextracted hierarchical rules either by removing re-dundancy (He et al, 2009) or by filtering rulesbased on certain patterns (Iglesias et al, 2009),while the other stream is concerned about alterna-tive approaches for learning the synchronous gram-mar (Blunsom et al, 2008; Blunsom et al, 2009; deGispert et al, 2010).
This paper falls under the lat-ter category and we use a non-parametric Bayesianapproach for rule extraction for Hiero-style systems.Our objective in this paper is to provide a principled533rule extraction method using a Bayesian frameworkthat can extract the minimal SCFG rules without re-ducing the BLEU score.2 Motivation and Related WorkThe large number of rules in Hiero-style systemsleads to slow decoding and increased memory re-quirements.
The heuristic rule extraction algo-rithm (Chiang, 2007) introduces redundant mono-tone composed rules (He et al, 2009) in the SCFGgrammar.
The research on Hiero rule extraction fallsinto two broad categories: i) rule reduction by elim-inating a subset of rules extracted by the heuristicapproach and ii) alternate approaches for rule extrac-tion.There have been approaches to reduce the size ofHiero phrase table, without significantly affectingthe translation quality.
He et.
al.
(2009) proposed theidea of discarding monotone composed rules fromthe phrase table that can instead be obtained dynami-cally by combining the minimal rules in the same or-der.
They achieve up to 70% reduction in the phrasetable by discarding these redundant rules, withoutappreciable reduction in the performance as mea-sured by BLEU.
Empirically analyzing the effective-ness of specific rule patterns, (Iglesias et al, 2009)show that some patterns having over 95% of the to-tal SCFG rules can be safely eliminated without anyreduction in the BLEU score.Along a different track, some prior works haveemployed alternate rule extraction approaches usinga Bayesian framework (DeNero et al, 2008; Blun-som et al, 2008; Blunsom et al, 2009).
(DeNeroet al, 2008) use a Maximum likelihood model oflearning phrase pairs (Marcu and Wong, 2002), butuse sampling to compute the expected counts of thephrase pairs for the E-step.
Other recent approachesuse Gibbs sampler for learning the SCFG by explor-ing a fixed grammar having pre-defined rule tem-plates (Blunsom et al, 2008) or by reasoning overthe space of derivations (Blunsom et al, 2009).We differ from earlier Bayesian approaches in thatour model is guided by the word alignments to rea-son over the space of the SCFG rules and this re-stricts the search space of our model.
We believethe word alignments to encode information, usefulfor identifying the good phrase-pairs.
For example,several attempts have been made to learn a phrasaltranslation model directly from the bitext withoutthe word alignments (Marcu and Wong, 2002; DeN-ero et al, 2008; Blunsom et al, 2008), but withoutany clear breakthrough that can scale to larger cor-pora.Our model exploits the word alignment informa-tion in the form of lexical alignment probability inorder to construct an informative prior over SCFGrules and it moves away from a heuristic framework,instead using a Bayesian non-parametric model toinfer a minimal, high-quality grammar from thedata.3 ModelOur model is based on similar assumptions as theoriginal Hiero system.
We assume that the bitext hasbeen word aligned, and that we can use that wordalignment to extract phrase pairs.Given the word alignments and the heuristicallyextracted phrase pairs Rp, our goal is to extract theminimal set of hierarchical rules Rg that would bestexplain Rp.
This is achieved by inferring a distribu-tion over the derivations for each phrase pair, wherethe set of derivations collectively specify the gram-mar.
In the following, we denote the sequence ofderivations for the set of phrase pairs by r, which iscomposed of grammar rules r. We will essentiallyread off our learned grammar from the sequence ofderivations r.Our non-parametric model reasons over the spaceof the (hierarchical and terminal) rules and sam-ples a set of rules by employing a prior based onthe alignment probability of the words in the phrasepairs.
We hypothesize that the resulting grammarwill be compact and also will explain the phrasepairs better (the SCFG rules will maximize the like-lihood of producing the entire set of observed phrasepairs).Using Bayes?
rule, the posterior over the deriva-tions r given the phrase pairs Rp can be written as:P (r|Rp) ?
P (Rp|r)P (r) (1)where P (Rp|r) is equal to one when the sequenceof rules r and phrase-pairs Rp are consistent, i.e.
rcan be partitioned into derivations to compose theset of phrase-pairs such that the derivations respect534the given word alignments; otherwise P (Rp|r) iszero.
The overall structure of the model is analo-gous to the Bayesian model for inducing Tree Sub-stitution Grammars proposed by Cohn et al (2009).Note that, our model extracts hierarchical rules forthe word-aligned phrase pairs and not for the sen-tences.Similar to the other Hiero-style systems, we usetwo types of rules: terminal and hierarchical rules.For each phrase-pair, our model either generates aterminal rule by not segmenting the phrase-pair, ordecides to segment the phrase-pair and extract somerules.Though it is possible to segment phrase-pairs bytwo (or more) non-overlapping spans, we proposea simpler model in this paper and restrict the hierar-chical rules to contain only one non-terminal (unlikethe case of classic Hiero-style grammars containingtwo non-terminals).
This simpler model, samplesthe space of derivations and identifies a sub-spanfor introducing the non-terminal, which can be ex-pressed as terminal rules (it is not decomposed fur-ther).
Figure 1 shows an example phrase-pair withthe Viterbi-best word alignment and Figure 2 showstwo possible derivations for the same phrase-pairwith the non-terminals introduced at different sub-spans.
It can be seen that the sub-phrase correspond-ing to the non-terminal spanX1 is directly written asa terminal rule and is not decomposed further.While the resulting model is slightly weaker thanthe original Hiero grammar, it should be noted oursimpler model does allow reordering and discontigu-ous alignments.
For example our model includesrules such as, X ?
(?X1?, ???
?X1), which cancapture phrases like (not X1, ne X1 pas) in the caseof English-French translation.
In terms of the re-ordering, our model lies in between the hierarchi-cal phrase-based and phrase-based models.
To sum-marize, the segmentation of each phrase-pair in ourmodel results in two rules: a hierarchical rule withone nonterminal as well as a terminal rule.More specifically, the generative process for gen-erating a phrase pair x from the grammar rulesmay have two steps as follows.
In the first step,the model decides on the type of the rule tx ?
{TERMINAL,HIERARCHICAL} used to generate thephrase-pair based on a Bernoulli distribution, havinga prior ?
coming from a Beta distribution:tx ?
Bernoulli(?)?
?
Beta(lx, 0.5)The lexical alignment probability lx controls thetendency for extracting hierarchical rules from thephrase-pair x.
For a given phrase-pair, lx is com-puted by taking the (geometric or arithmetic) aver-age of the reverse and forward alignment probabil-ities, which we explain later in this section.
Inte-grating out ?
gives us the conditional probabilitiesof choosing the rule type tx as:p(tterm|x) ?
nxterm + lx (2)p(thier|x) ?
nxhier + 0.5 (3)where nxterm and nxhier denote the number of termi-nal or hierarchical rules, among the rules extractedso far from the phrase-pair x during the sampling.In the second step, if the rule type tx =HIERARCHICAL, the model generates the phrase-pair by sampling from the hierarchical and terminalrules.
We use a Dirichlet Process (DP) to model thegeneration of hierarchical rules r:G ?
DP (?h, P0(r))r ?
GIntegrating out the grammar G, the predictive dis-tribution of a hierarchical rule rx for generating thecurrent phrase-pair (conditioned on the rules fromthe rest of the phrase-pairs) is:p(rx|r?x, ?h, P0) ?
n?xrx + ?hP0(rx) (4)where n?xrx is the count of the rule rx in the rest ofthe phrase-pairs that is represented by r?x, P0 is thebase measure, and ?h is the concentration parametercontrolling the model?s preference towards using anexisting hierarchical rule from the cache or to createa new rule sanctioned by the base distribution.
Weuse the lexical alignment probabilities of the compo-nent rules as our base measure P0:P0(r) =[( ?
(k,l)?ap(el|fk)) 1|a|( ?
(k,l)?ap(fk|el)) 1|a|] 12(5)535octavo y noveno Fondos Europeos de Desarrollo para el ejercicioEighth and Ninth European Development Funds for the financial yearFigure 1: An example phrase-pair with Viterbi alignmentsX ?
(Eighth and Ninth X1 for the financial year, octavo y noveno X1 para el ejercicio)X ?
(European Development Funds, Fondos Europeos de Desarrollo)X ?
(Eighth and Ninth X1, octavo y noveno X1)X ?
(European Development Funds for the financial year,Fondos Europeos de Desarrollo para el ejercicio)Figure 2: Two possible derivations of the phrase-pair in Figure 1where a is the set of alignments in the given sub-span; if the sub-span has multiple Viterbi alignmentsfrom different phrase-pairs, we consider the union ofall such alignments.
DeNero et al (2008) use a sim-ilar prior- geometric mean of the forward and reverseIBM-1 alignments.
However, we use the product ofgeometric means of the forward and reverse align-ment scores.
We also experimented with the arith-metic mean of the lexical alignment probabilities.The lexical prior lx in the first step can be definedsimilarly.
We found the particular combination of,?arithmetic mean?
for the lexical prior lx (in the firststep) and ?geometric mean?
for the base distributionP0 (in the second step) to work better, as we discusslater in Section 5.Assuming the heuristically extracted phrase pairsto be the input to our inference algorithm, ourapproach samples the space of rules to find thebest possible segmentation for the sentences as de-fined by the cache and base distribution.
We ex-plore a subset of the space of rules being consid-ered by (Blunsom et al, 2009) ?
i.e., only thoserules satisfying the word alignments and heuristi-cally grown phrase alignments.4 InferenceWe train our model by using a Gibbs sampler ?
aMarkov Chain Monte Carlo (MCMC) method forsampling one variable in the model, conditional tothe other variables.
The sampling procedure is re-peated for what is called a long Gibbs chain span-ning several iterations, while the counts are collectedat fixed thin intervals in the chain.
As is common inthe MCMC procedures, we ignore samples from afixed number of initial burn-in iterations, allowingthe model to move away from the initial bias.
Therules in the final sampler state at the end of the Gibbschain along with their counts averaged by the num-ber of thin iterations become our translation model.In our model, a sample for a given phrase paircorresponds either to its terminal derivation or tworules in a hierarchical derivation.
The model sam-ples a derivation from the space of derivations thatare consistent with the word alignments.
In orderto achieve this, we need an efficient way to enumer-ate the derivations for a phrase pair such that theyare consistent with the alignments.
We use the lin-ear time algorithm to maximally decompose a word-aligned phrase pair, so as to encode it as a compactalignment tree (Zhang et al, 2008).f0 f1 f2 f3 f4e0 e1 e2 e3 e4 e5Figure 3: Example phrase pair with alignments.536For a phrase-pair with a given alignment as shownin Figure 3, Zhang et al (2008) generalize theO(n+K) time algorithm for computing all K common in-tervals of two different permutations of length n.The contiguous blocks of the alignment are cap-tured as the nodes in the alignment tree and the treestructure for the example phrase pair in Figure 3 isshown in Figure 4.
The italicized nodes form a left-branching chain in the alignment tree and the sub-spans of this chain also lead to alignment nodes thatare not explicitly captured in the tree (Please referto Zhang et al (2008) for details).
In our work, eachnode in the tree (and also each sub-span in the left-branching chain) corresponds to an aligned source-target sub-span within the phrase-pair, and is a po-tential site for introducing the non-terminal X togenerate hierarchical rules.Given this alignment tree for a phrase pair, aderivation can be obtained by introducing a non-terminal at some node nd in the tree and re-writingthe span rooted at nd as a separate rule.
As men-tioned earlier, we compute the derivation probabilityas a product of the probabilities of the componentrules, which are computed using the Equation 4.We initialize the sampler by using our lexicalalignment prior and sampling from the distributionof derivations as suggested by the priors.
We foundthis to perform better in practice, than a naive sam-pler without an initializer.At each iteration, the Gibbs sampler processes thephrase pairs in random order.
For each phrase pairRp, it visits the nodes in the corresponding align-ment tree and computes the posterior probability ofthe derivations and samples from this posterior dis-tribution.
To speedup the sampling, we store thepre-computed alignment tree for the phrase pairs andjust recompute the derivation probabilities based onthe sampler state at every iteration.
While the sam-pler state is updated with the counts at each iteration,we accumulate the counts only at fixed intervals inthe Gibbs chain.
In applying the model for decoding,we use the grammar from the final sampler state.Since our model includes only one hyperparam-eter ?h, we tune its value manually by empiricallyexperimenting on a small set of initial phrase pairs.We keep for future work the task of automaticallytuning for hyper-parameter values by sampling.
([0,5],[0,4])([0,2],[0,2])([0,1],[0,1])([0,0],[0,0]) ([1,1],[1,1])([2,2],[2,2])([4,5],[3,4])Figure 4: Decomposed alignment tree for the examplealignment in Fig.
3.5 ExperimentsWe use the English-Spanish data from WMT-10shared task for the experiments to evaluate the effec-tiveness of our Bayesian rule extraction approach.We used the entire shared task training set exceptthe UN data for training translation model and thelanguage model was trained with the same set andan additional 2 million sentences from the UN data,using SRILM toolkit with Knesser-Ney discounting.We tuned the feature weights on the WMT-10 dev-set using MERT (Och, 2003) and evaluate on thetest set by computing lower-cased BLEU score (Pa-pineni et al, 2002) using the WMT-10 standard eval-uation script.We use Kriya ?
an in-house implementation of hi-erarchical phrase-based translation written predom-inantly in Python.
Kriya supports the entire transla-tion pipeline of SCFG rule extraction and decodingwith cube pruning (Huang and Chiang, 2007) andLM integration (Chiang, 2007).
We use the 7 fea-tures (4 translation model features, extracted rulespenalty, word penalty and language model) as is typ-ical in Hiero-style systems.
For tuning the featureweights, we have adapted the MERT implementa-tion in Moses1 for use with Kriya as the decoder.We started by training and evaluating the twobaseline systems using i) two non-terminals andii) one non-terminal, which were trained using theconventional heuristic extraction approach.
For thebaseline with one non-terminal, we modified theheuristic rule extraction algorithm appropriately2.1www.statmt.org/moses/2Given an initial phrase pair, the algorithm would introducea non-terminal for each sub-span consistent with the alignmentsand extract rules corresponding to each sub-span.
The con-537Experiment# of rules filteredfor devset(in millions)BLEUBaseline (w/ 2 non-terminals) 52.36 27.45Baseline (w/ 1 non-terminal) 22.09 26.71Pattern-based filtering?
18.78 24.611 non-terminal; monotone & non-monotone 10.36 24.171 non-terminal; non-monotone 3.62 23.99Table 1: Kriya: Baseline and Filtering experiments.
?
: This is the initial rule set used in Iglesias et al (2009) obtainedby greedy filtering.
Rows 4 and 5 represents the filtering that uses single non-terminal rules with row 4 allowingmonotone rules in addition to the non-monotone (reordering) rules.As part of the baseline methods to be applied to min-imize the number of SCFG rules, We also wanted toassess the effect of a simpler rule filtering, wherethe idea is to filter the heuristically extracted rulesbased on certain patterns.
Our first baseline filteringstrategy uses the heuristic methods in Iglesias et al(2009) in order to minimize the number of rules3.For the other baseline filtering experiments, we re-tained only one non-terminal rules and then furtherlimited it by retaining only non-monotone one non-terminal rules; in both cases the terminal rules wereretained.Table 1 shows the results for baseline and the rulefiltering experiments.
Restricting rule extraction tojust one non-terminal doesn?t affect the BLEU scoresignificantly and this justifies the simpler modelused in this paper.
Secondly, we find significant re-duction in the BLEU for the pattern-based filteringstrategy and this is because we only use the initialrule set obtained by greedy filtering without aug-menting it with other specific patterns.
The othertwo filtering methods reduced the BLEU further butnot significantly.
The second column in the tablegives the number of SCFG rules filtered for the dev-set, which is typically much less than the full set ofrules.
We later use this to put in perspective theeffective reduction in the model size achieved byour Bayesian model.
We can ideally compare ourBayesian rule extraction using Gibbs sampling withstraints relating to two non-terminals (such as, no adjacent non-terminals in source side) does not apply for the one non-terminalcase.3It should be noted that we didn?t use the augmentations tothe initial rule set (Iglesias et al, 2009) and our objective is tofind the impact of the filtering approaches.the baselines and the filtering approaches.
However,running our Gibbs sampler on the full set of phrasepairs demand sampling to be distributed, possiblywith approximation (?
; ?
), which we reserve for ourfuture work.In this work, we focus on evaluating our Gibbssampler on reasonable sized set of phrase pairs withcorresponding baselines.
We filter the initial phrasepairs based on their frequency using three differentthresholds, viz.
20, 10 and 3- resulting in smallersets of initial phrase pairs because we throw out in-frequent phrase pairs (the threshold-20 case is thesmallest initial set of phrase pairs).
This allows usto run our sampler as a stand-alone instance for thethree sets, obviating the need for distributed sam-pling.Table 2 shows the number of unique phrase pairsin each set.
While, the filtering reduces the numberof phrase pairs to a small fraction of the total phrasepairs, it also increases the unknown words (OOV)in the test set by a factor between 1.8 and 3.
In or-der to address this issue due to the OOV words, weadditionally added non-decomposable phrase pairshaving just one word at either source or target side,Phrase-pairs set# of Uniquephrase-pairsTestsetOOVAll phrase-pairs 110782174 1136Threshold-20 292336 3735Threshold-10 606590 3056Threshold-3 2689855 2067Table 2: Phrase-pair statistics for different frequencythreshold538Experiment Threshold-20 Threshold-10 Threshold-3Baseline (w/ 2 non-terminals) 24.30 25.96 26.34Baseline (w/ 1 non-terminal) 24.00 25.90 26.83Bayesian rule extraction 23.39 24.30 25.22Table 3: BLEU scores: Heuristic vs Bayesian rule extractionExperiment Rules Extracted (in millions) ReductionHeuristic (1 nt) BayesianThreshold-20 1.93 (0.117) 1.86 (0.07) 3.57 (38.34)Threshold-10 2.91 (1.09) 2.10 (0.28) 27.7 (73.95)Threshold-3 7.46 (5.64) 2.45 (0.71) 67.17 (87.28)Table 4: Model compression: Heuristic vs Bayesian rule extractionPriors ?h BLEUArith + Arith means 0.5 22.46Arith + Geom means 0.5 23.39Geom + Arith means 0.5 22.96Arith + Geom means 0.5 22.83Arith + Geom means 0.1 22.88Arith + Geom means 0.2 22.97Arith + Geom means 0.3 22.98Arith + Geom means 0.4 22.69Arith + Geom means 0.5 23.39Arith + Geom means 0.6 22.89Arith + Geom means 0.7 22.82Arith + Geom means 0.8 22.82Arith + Geom means 0.9 22.67Table 5: Effect of different priors and ?h on Threshold-20 set.
The two priors correspond to the lexical prior lxin the first step and the base distribution P0 in the secondstep.as coverage rules.
The coverage rules (about 1.8million) were added separately to the SCFG rulesinduced by both heuristic algorithm and Gibbs sam-pler.
This is justified because we only add the rulesthat can not be decomposed further by both rule ex-traction approaches.
However, note that both ap-proaches can independently induce rules that over-lap with the coverage rules set and in such cases wesimply add the original corpus count to the countsreturned by the respective rule extraction method.The Gibbs sampler considers the phrase pairs inrandom order at each iteration and induces SCFGrules by sampling a derivation for each phrase pair.Given a phrase pair x with raw corpus frequency fx,we simply scale the count for its sampled deriva-tion r by its frequency fx.
Alternately, we also ex-perimented with independently sampling for eachinstance of the phrase pair and found their perfor-mances to be comparable.
Sampling phrase pairsonce and then scaling the sampled derivation, helpus to speed up the sampling process.
In our experi-ments, we ran the Gibbs sampler for 2000 iterationswith a burn-in period of 200, collecting counts every50 iterations.
We set the concentration parameter ?hto be 0.5 based on our experiments detailed later inthis section.The BLEU scores for the SCFG learned from theGibbs sampler are shown in Table 3.
We first notethat, the threshold-20 set has lower baseline BLEUthan threshold-10 and threshold-3 sets, as can be ex-pected because threshold-20 set uses a much smallersubset of the full set of phrase pairs to extract hier-archical rules.
The Bayesian approach results in amaximum BLEU score reduction of 1.6 for the setsusing thresholds 10 and 3, compared to the one non-terminal baseline.
The two non-terminal baseline isalso provided to place our results in perspective.Table 4 shows the model size, including the cov-erage rules for the two rule extraction approaches.The number of extracted rules, excluding the cov-erage rules are shown within the parenthesis.
Thelast column shows the reduction in the model sizefor both with and without the coverage rules; yield-ing a maximum absolute reduction of 67.17% for the539threshold-3 phrase pairs set.
It can be seen that thenumber of rules are far fewer than the rules extractedusing the baseline heuristic methods for filtering de-tailed in Table 1.
Interestingly, we obtain a smallermodel size, even as we decrease the threshold to in-clude more initial phrase pairs used as input to theinference procedure, e.g.
a 67.17% reduction overthe rules extracted from the threshold-3 phrase pairsv.s.
a 27.7% reduction for threshold-10.These results show that our model is capable ofextracting high-value Hiero-style SCFG rules, albeitwith a reduction in the BLEU score.
However, ourcurrent approach offers scope for improvement inseveral avenues, for example we can use annealingto perturb the initial sampling iterations to encour-age the Gibbs sampler to explore several derivationsfor each phrase pair.
Though this might result inslightly large models than the current ones, we stillexpect substantial reduction than the original Hierorule extraction.
In future, we also plan to sample thehyperparameter ?h, instead of using a fixed value.Table 5 shows the effect of different values ofthe concentration parameter ?h and the priors usedin the model.
The order of priors in each settingcorrespond to the prior used in deciding the rule-type and identifying the non-terminal span for sam-pling a derivation.
We found the geometric mean towork better in both cases.
We further found that theconcentration parameter ?h value 0.5 gives the bestBLEU score.6 Conclusion and Future WorkWe proposed a novel method for extracting mini-mal set of hierarchical rules using non-parametricBayesian framework.
We demonstrated substantialreduction in the size of extracted grammar with thebest case reduction of 67.17%, as compared to theheuristic approach, albeit with a slight reduction inthe BLEU scores.We plan to extend our model to handle two non-terminals to allow for better reordering.
We alsoplan to run our sampler on the full set of phrasepairs using distributed sampling and our prelimi-nary results in this direction are encouraging.
Fi-nally, we would like to directly sample from theViterbi aligned sentence pairs instead of relying onthe heuristically extracted phrase pairs.
This canbe accomplished by using a model that is closerto the Tree Substitution Grammar induction modelin (Cohn et al, 2009) but in our case the modelwould infer a Hiero-style SCFG from word-alignedsentence pairs.ReferencesPhil Blunsom, Trevor Cohn, and Miles Osborne.
2008.Bayesian synchronous grammar induction.
In Pro-ceedings of Neural Information Processing Systems-08.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A gibbs sampler for phrasal synchronousgrammar induction.
In Proceedings of Association ofComputational Linguistics-09, pages 782?790.
Asso-ciation for Computational Linguistics.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33.Trevor Cohn, Sharon Goldwater, and Phil Blunsom.2009.
Inducing compact but accurate tree-substitutiongrammars.
In Proceedings of Human Language Tech-nologies: North American Chapter of the Associationfor Computational Linguistics-09, pages 548?556.
As-sociation for Computational Linguistics.Adria` de Gispert, Juan Pino, and William Byrne.
2010.Hierarchical phrase-based translation grammars ex-tracted from alignment posterior probabilities.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, pages 545?554.Association for Computational Linguistics.John DeNero, Alexandre Bouchard-Cote, and Klein Dan.2008.
Sampling alignment structure under a bayesiantranslation model.
In In Proceedings of EmpiricalMethods in Natural Language Processing-08, pages314?323.
Association for Computational Linguistics.Zhongjun He, Yao Meng, and Hao Yu.
2009.
Discardingmonotone composed rule for hierarchical phrase-basedstatistical machine translation.
In Proceedings of the3rd International Universal Communication Sympo-sium, pages 25?29.
ACM.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 144?151.Association for Computational Linguistics.Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,and William Byrne.
2009.
Rule filtering by pattern forefficient hierarchical translation.
In Proceedings of the12th Conference of the European Chapter of the ACL(EACL 2009), pages 380?388.
Association for Com-putational Linguistics.540Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In In Proceedings of Empirical Methods in Natu-ral Language Processing-02, pages 133?139.
Associ-ation for Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 160?167.
Association forComputational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wie-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In In Proceedings ofAssociation of Computational Linguistics, pages 311?318.
Association for Computational Linguistics.Hao Zhang, Daniel Gildea, and David Chiang.
2008.
Ex-tracting synchronous grammar rules from word-levelalignments in linear time.
In In Proceedings of the22nd International Conference on Computational Lin-guistics (COLING) - Volume 1, pages 1081?1088.
As-sociation for Computational Linguistics.Andreas Zollmann, Ashish Venugopal, Franz Och, andJay Ponte.
2008.
A systematic comparison of phrase-based, hierarchical and syntax-augmented statisticalmt.
In Proceedings of the 22nd International Confer-ence on Computational Linguistics (COLING) - Vol-ume 1, pages 1145?1152.
Association for Computa-tional Linguistics.541
