Towards Terascale Knowledge AcquisitionPatrick Pantel, Deepak Ravichandran and Eduard HovyInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA  90292{pantel,ravichan,hovy}@isi.eduAbstractAlthough vast amounts of textual data are freelyavailable, many NLP algorithms exploit only aminute percentage of it.
In this paper, we study thechallenges of working at the terascale.
We presentan algorithm, designed for the terascale, for miningis-a relations that achieves similar performance to astate-of-the-art linguistically-rich method.
We fo-cus on the accuracy of these two systems as a func-tion of processing time and corpus size.1 IntroductionThe Natural Language Processing (NLP) com-munity has recently seen a growth in corpus-basedmethods.
Algorithms light in linguistic theories butrich in available training data have been success-fully applied to several applications such as ma-chine translation (Och and Ney 2002), informationextraction (Etzioni et al 2004), and question an-swering (Brill et al 2001).In the last decade, we have seen an explosion inthe amount of available digital text resources.
It isestimated that the Internet contains hundreds ofterabytes of text data, most of which is in anunstructured format.
Yet, many NLP algorithmstap into only megabytes or gigabytes of thisinformation.In this paper, we make a step towards acquiringsemantic knowledge from terabytes of data.
Wepresent an algorithm for extracting is-a relations,designed for the terascale, and compare it to a stateof the art method that employs deep analysis oftext (Pantel and Ravichandran 2004).
We showthat by simply utilizing more data on this task, wecan achieve similar performance to a linguistically-rich approach.
The current state of the art co-occurrence model requires an estimated 10 yearsjust to parse a 1TB corpus (see Table 1).
Instead ofusing a syntactically motivated co-occurrence ap-proach as above, our system uses lexico-syntacticrules.
In particular, it finds lexico-POS patterns bymaking modifications to the basic edit distancealgorithm.
Once these patterns have been learnt,the algorithm for finding new is-a relations runs inO(n), where n is the number of sentences.In semantic hierarchies such as WordNet (Miller1990), an is-a relation between two words x and yrepresents a subordinate relationship (i.e.
x is morespecific than y).
Many algorithms have recentlybeen proposed to automatically mine is-a (hypo-nym/hypernym) relations between words.
Here, wefocus on is-a relations that are characterized by thequestions ?What/Who is X??
For example, Table 2shows a sample of 10 is-a relations discovered bythe algorithms presented in this paper.
In this table,we call azalea, tiramisu, and Winona Ryder in-stances of the respective concepts flower, dessertand actress.
These kinds of is-a relations would beuseful for various purposes such as ontology con-struction, semantic information retrieval, questionanswering, etc.The main contribution of this paper is a compari-son of the quality of our pattern-based and co-occurrence models as a function of processing timeand corpus size.
Also, the paper lays a foundationfor terascale acquisition of knowledge.
We willshow that, for very small or very large corpora orfor situations where recall is valued over precision,the pattern-based approach is best.2 Relevant WorkPrevious approaches to extracting is-a relationsfall under two categories: pattern-based and co-occurrence-based approaches.2.1 Pattern-based approachesMarti Hearst (1992) was the first to use a pat-tern-based approach to extract hyponym relationsfrom a raw corpus.
She used an iterative process tosemi-automatically learn patterns.
However, acorpus of 20MB words yielded only 400 examples.Our pattern-based algorithm is very similar to theone used by Hearst.
She uses seed examples tomanually discover her patterns whearas we use aminimal edit distance algorithm to automaticallydiscover the patterns.771Riloff and Shepherd (1997) used a semi-automatic method for discovering similar wordsusing a few seed examples by using pattern-basedtechniques and human supervision.
Berland andCharniak (1999) used similar pattern-based tech-niques and other heuristics to extract meronymy(part-whole) relations.
They reported an accuracyof about 55% precision on a corpus of 100,000words.
Girju et al (2003) improved upon Berlandand Charniak's work using a machine learningfilter.
Mann (2002) and Fleischman et al (2003)used part of speech patterns to extract a subset ofhyponym relations involving proper nouns.Our pattern-based algorithm differs from theseapproaches in two ways.
We learn lexico-POSpatterns in an automatic way.
Also, the patterns arelearned with the specific goal of scaling to theterascale (see Table 2).2.2 Co-occurrence-based approachesThe second class of algorithms uses co-occurrence statistics (Hindle 1990, Lin 1998).These systems mostly employ clustering algo-rithms to group words according to their meaningsin text.
Assuming the distributional hypothesis(Harris 1985), words that occur in similar gram-matical contexts are similar in meaning.
Curranand Moens (2002) experimented with corpus sizeand complexity of proximity features in buildingautomatic thesauri.
CBC (Clustering by Commit-tee) proposed by Pantel and Lin (2002) achieveshigh recall and precision in generating similaritylists of words discriminated by their meaning andsenses.
However, such clustering algorithms fail toname their classes.Caraballo (1999) was the first to use clusteringfor labeling is-a relations using conjunction andapposition features to build noun clusters.
Re-cently, Pantel and Ravichandran (2004) extendedthis approach by making use of all syntactic de-pendency features for each noun.3 Syntactical co-occurrence approachMuch of the research discussed above takes asimilar approach of searching text for simple sur-face or lexico-syntactic patterns in a bottom-upapproach.
Our co-occurrence model (Pantel andRavichandran 2004) makes use of semantic classeslike those generated by CBC.
Hyponyms are gen-erated in a top-down approach by naming eachgroup of words and assigning that name as a hypo-nym of each word in the group (i.e., one hyponymper instance/group label pair).The input to the extraction algorithm is a list ofsemantic classes, in the form of clusters of words,which may be generated from any source.
For ex-ample, following are two semantic classes discov-ered by CBC:(A) peach, pear, pineapple, apricot,mango, raspberry, lemon, cherry,strawberry, melon, blueberry, fig, apple,plum, nectarine, avocado, grapefruit,papaya, banana, cantaloupe, cranberry,blackberry, lime, orange, tangerine, ...(B) Phil Donahue, Pat Sajak, ArsenioHall, Geraldo Rivera, Don Imus, Larry King,David Letterman, Conan O'Brien, RosieO'Donnell, Jenny Jones, Sally Jessy Raph-ael, Oprah Winfrey, Jerry Springer, HowardStern, Jay Leno, Johnny Carson, ...The extraction algorithm first labels concepts(A) and (B) with fruit and host respectively.
Then,is-a relationships are extracted, such as: apple is afruit, pear is a fruit, and David Letterman is a host.An instance such as pear is assigned a hypernymfruit not because it necessarily occurs in any par-ticular syntactic relationship with the word fruit,but because it belongs to the class of instances thatdoes.
The labeling of semantic classes is performedin three phases, as outlined below.3.1 Phase IIn the first phase of the algorithm, feature vec-tors are extracted for each word that occurs in asemantic class.
Each feature corresponds to agrammatical context in which the word occurs.
Forexample, ?catch __?
is a verb-object context.
If theword wave occurred in this context, then the con-text is a feature of wave.We then construct a mutual information vectorMI(e) = (mie1, mie2, ?, miem) for each word e,where mief is the pointwise mutual informationbetween word e and context f, which is defined as:NcNcNcef mjejniifefmi?
?== ?=11logTable 2.
Sample of 10 is-a relationships discovered byour co-occurrence and pattern-based systems.CO-OCCURRENCE SYSTEM PATTERN-BASED SYSTEMWord Hypernym Word Hypernymazalea flower American airlinebipolar disorder disease Bobby Bonds coachBordeaux wine radiation therapy cancertreatmentFlintstones television show tiramisu dessertsalmon fish Winona Ryder actressTable 1.
Approximate processing time on a singlePentium-4 2.5 GHz machine.TOOL 15 GB ORPUS 1 TB CORPUSPOS Tagger 2 days 125 daysNP Chunker 3 days 214 daysDependency Parser 56 days 10.2 yearsSyntactic Parser 5.8 years 388.4 years772where n is the number of elements to be clustered,cef is the frequency count of word e in grammaticalcontext f, and N is the total frequency count of allfeatures of all words.3.2 Phase IIFollowing (Pantel and Lin 2002), a committeefor each semantic class is constructed.
A commit-tee is a set of representative elements that unambi-guously describe the members of a possible class.For example, in one of our experiments, the com-mittees for semantic classes (A) and (B) from Sec-tion 3 were:A) peach, pear, pineapple, apricot, mango,raspberry, lemon, blueberryB) Phil Donahue, Pat Sajak, Arsenio Hall,Geraldo Rivera, Don Imus, Larry King,David Letterman3.3 Phase IIIBy averaging the feature vectors of the commit-tee members of a particular semantic class, weobtain a grammatical template, or signature, forthat class.
For example, Figure 1 shows an excerptof the grammatical signature for semantic class(B).
The vector is obtained by averaging the fea-ture vectors of the words in the committee of thisclass.
The ?V:subj:N:joke?
feature indicates a sub-ject-verb relationship between the class and theverb joke while ?N:appo:N:host?
indicates an ap-position relationship between the class and thenoun host.
The two columns of numbers indicatethe frequency and mutual information scores.To name a class, we search its signature for cer-tain relationships known to identify class labels.These relationships, automatically learned in(Pantel and Ravichandran 2004), include apposi-tions, nominal subjects, such as relationships, andlike relationships.
We sum up the mutual informa-tion scores for each term that occurs in these rela-tionships with a committee of a class.
The highestscoring term is the name of the class.The syntactical co-occurrence approach hasworst-case time complexity O(n2k), where n is thenumber of words in the corpus and k is the feature-space (Pantel and Ravichandran 2004).
Just toparse a 1 TB corpus, this approach requires ap-proximately 10.2 years (see Table 2).4 Scalable pattern-based approachWe propose an algorithm for learning highlyscalable lexico-POS patterns.
Given two sentenceswith their surface form and part of speech tags, thealgorithm finds the optimal lexico-POS alignment.For example, consider the following 2 sentences:1) Platinum is a precious metal.2) Molybdenum is a metal.Applying a POS tagger (Brill 1995) gives thefollowing output:Surface Platinum is a precious metal .POS NNP VBZ DT JJ NN .Surface Molybdenum is a metal .POS NNP VBZ DT NN .A very good pattern to generalize from thealignment of these two strings would beSurface  is a  metal .POS NNP     .We use the following notation to denote thisalignment: ?_NNP is a (*s*) metal.
?, where?_NNP represents the POS tag NNP?.To perform such alignments we introduce twowildcard operators, skip (*s*) and wildcard (*g*).The skip operator represents 0 or 1 instance of anyword (similar to the \w* pattern in Perl), while thewildcard operator represents exactly 1 instance ofany word (similar to the \w+ pattern in Perl).4.1 AlgorithmWe present an algorithm for learning patterns atmultiple levels.
Multilevel representation is de-fined as the different levels of a sentence such asthe lexical level and POS level.
Consider twostrings a(1, n) and b(1, m) of lengths n and m re-spectively.
Let a1(1, n) and a2(1, n) be the level 1(lexical level) and level 2 (POS level) representa-tions for the string a(1, n).
Similarly, let b1(1, m)and b2(1, m) be the level 1 and level 2 representa-tions for the string b(1, m).
The algorithm consistsof two parts: calculation of the minimal edit dis-tance and retrieval of an optimal pattern.
Theminimal edit distance algorithm calculates thenumber of edit operations (insertions, deletions andreplacements) required to change one string toanother string.
The optimal pattern is retrieved by{Phil Donahue,Pat Sajak,Arsenio Hall}N:gen:Ntalk show 93 11.77television show 24 11.30TV show 25 10.45show 255 9.98audience 23 7.80joke 5 7.37V:subj:Njoke 39 7.11tape 10 7.09poke 15 6.87host 40 6.47co-host 4 6.14banter 3 6.00interview 20 5.89N:appo:Nhost 127 12.46comedian 12 11.02King 13 9.49star 6 7.47Figure 1.
Excerpt of the grammatical signature for thetelevision host class.773keeping track of the edit operations (which is thesecond part of the algorithm).Algorithm for calculating the minimal edit distancebetween two stringsD[0,0]=0for i = 1 to n do  D[i,0] = D[i-1,0] + cost(insertion)for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion)for i = 1 to n dofor j = 1 to m doD[i,j] = min( D[i-1,j-1] + cost(substitution),D[i-1,j] + cost(insertion),D[i,j-1] + cost(deletion))Print (D[n,m])Algorithm for optimal pattern retrievali = n, j = m;while i ?
0 and j ?
0if D[i,j] = D[i-1,j] + cost(insertion)print (*s*), i = i-1else if D[i,j] = D[i,j-1] + cost(deletion)print(*s*), j = j-1else if a1i = b1jprint (a1i), i = i -1, j = j =1else if a2i = b2jprint (a2i), i = i -1, j = j =1elseprint (*g*), i = i -1, j = j =1We experimentally set (by trial and error):cost(insertion)  = 3cost(deletion)  = 3cost(substitution) = 0 if a1i=b1j= 1 if a1i?b1j, a2i=b2j= 2 if a1i?b1j, a2i?b2j4.2 Implementation and filteringThe above algorithm takes O(y2) time for everypair of strings of length at most y.
Hence, if thereare x strings in the collection, each string having atmost length y, the algorithm has time complexityO(x2y2) to extract all the patterns in the collection.Applying the above algorithm on a corpus of3GB  with 50 is-a relationship seeds, we obtain aset of 600 lexico-POS.
Following are two of them:1) X_JJ#NN|JJ#NN#NN|NN _CC Y_JJ#JJ#NN|JJ|NNS|NN|JJ#NNS|NN#NN|JJ#NN|JJ#NN#NNe.g.
?caldera or lava lake?2) X_NNP#NNP|NNP#NNP#NNP#NNP#NNP#CC#NNP|NNP|VBN|NN#NN|VBG#NN|NN ,_, _DTY_NN#IN#NN|JJ#JJ#NN|JJ|NN|NN#IN#NNP|NNP#NNP|NN#NN|JJ#NN|JJ#NN#NNe.g.
?leukemia, the cancer of ...Note that we store different POS variations ofthe anchors X and Y.
As shown in example 1, thePOS variations of the anchor X are (JJ NN, JJ NNNN, NN).
The variations for anchor Y are (JJ JJNN, JJ, etc.).
The reason is quite straightforward:we need to determine the boundary of the anchorsX and Y and a reasonable way to delimit themwould be to use POS information.
All the patternsproduced by the multi-level pattern learning algo-rithm were generated from positive examples.From amongst these patterns, we need to find themost important ones.
This is a critical step becausefrequently occurring patterns have low precisionwhereas rarely occurring patterns have high preci-sion.
From the Information Extraction point ofview neither of these patterns is very useful.
Weneed to find patterns with relatively high occur-rence and high precision.
We apply the log likeli-hood principle (Dunning 1993) to compute thisscore.
The top 15 patterns according to this metricare listed in Table 3 (we omit the POS variationsfor visibility).
Some of these patterns are similar tothe ones discovered by Hearst (1992) while otherpatterns are similar to the ones used by Fleischmanet al (2003).4.3 Time complexityTo extract hyponym relations, we use a fixednumber of patterns across a corpus.
Since we treateach sentences independently from others, thealgorithm runs in linear time O(n) over the corpussize, where n is number of sentences in the corpus.5 Experimental ResultsIn this section, we empirically compare the pat-tern-based and co-occurrence-based models pre-sented in Section 3 and Section 4.
The focus is onthe precision and recall of the systems as a func-tion of the corpus size.5.1 Experimental SetupWe use a 15GB newspaper corpus consisting ofTREC9, TREC 2002, Yahoo!
News ~0.5GB, APnewswire ~2GB, New York Times ~2GB, Reuters~0.8GB, Wall Street Journal ~1.2GB, and variousonline news website ~1.5GB.
For our experiments,we extract from this corpus six data sets of differ-ent sizes: 1.5MB, 15 MB, 150 MB, 1.5GB, 6GBand 15GB.For the co-occurrence model, we used Minipar(Lin 1994), a broad coverage parser, to parse eachdata set.
We collected the frequency counts of thegrammatical relationships (contexts) output byMinipar and used them to compute the pointwisemutual information vectors described in Section3.1.
For the pattern-based approach, we use Brill?sPOS tagger (1995) to tag each data set.5.2 PrecisionWe performed a manual evaluation to estimatethe precision of both systems on each dataset.
Foreach dataset, both systems extracted a set of is-aTable 3.
Top 15 lexico-syntactic patterns discoveredby our system.X, or Y X, _DT Y _(WDT|IN) Y like X andX, (a|an) Y X, _RB known as Y _NN, X and other YX, Y X ( Y ) Y, including X,Y, or X Y such as X Y, such as XX is a Y X, _RB called Y Y, especially X774relationships.
Six sets were extracted for the pat-tern-based approach and five sets for the co-occurrence approach (the 15GB corpus was toolarge to process using the co-occurrence model ?see dependency parsing time estimates in Table 2).From each resulting set, we then randomly se-lected 50 words along with their top 3 highestranking is-a relationships.
For example, Table 4shows three randomly selected names for the pat-tern-based system on the 15GB dataset.
For eachword, we added to the list of hypernyms a humangenerated hypernym (obtained from an annotatorlooking at the word without any system or Word-Net hyponym).
We also appended the WordNethypernyms for each word (only for the top 3senses).
Each of the 11 random samples containeda maximum of 350 is-a relationships to manuallyevaluate (50 random words with top 3 system, top3 WordNet, and human generated relationship).We presented each of the 11 random samples totwo human judges.
The 50 randomly selectedwords, together with the system, human, andWordNet generated is-a relationships, were ran-domly ordered.
That way, there was no way for ajudge to know the source of a relationship nor eachsystem?s ranking of the relationships.
For eachrelationship, we asked the judges to assign a scoreof correct, partially correct, or incorrect.
We thencomputed the average precision of the system,human, and WordNet on each dataset.
We alsocomputed the percentage of times a correct rela-tionship was found in the top 3 is-a relationships ofa word and the mean reciprocal rank (MRR).
Foreach word, a system receives an MRR score of 1 /M, where M is the rank of the first name judgedcorrect.
Table 5 shows the results comparing thetwo automatic systems.
Table 6 shows similarresults for a more lenient evaluation where bothcorrect and partially correct are judged correct.For small datasets (below 150MB), the pattern-based method achieves higher precision since theco-occurrence method requires a certain criticalmass of statistics before it can extract useful classsignatures (see Section 3).
On the other hand, thepattern-based approach has relatively constantprecision since most of the is-a relationships se-lected by it are fired by a single pattern.
Once theco-occurrence system reaches its critical mass (at150MB), it generates much more precise hypo-nyms.
The Kappa statistics for our experimentswere all in the range 0.78 ?
0.85.Table 7 and Table 8 compare the precision of thepattern-based and co-occurrence-based methodswith the human and WordNet hyponyms.
Thevariation between the human and WordNet scoresacross both systems is mostly due to the relativecleanliness of the tokens in the co-occurrence-based system (due to the parser used in the ap-proach).
WordNet consistently generated higherprecision relationships although both algorithmsapproach WordNet quality on 6GB (the pattern-based algorithm even surpasses WordNet precisionon 15GB).
Furthermore, WordNet only generated ahyponym 40% of the time.
This is mostly due tothe lack of proper noun coverage in WordNet.On the 6 GB corpus, the co-occurrence approachtook approximately 47 single Pentium-4 2.5 GHzprocessor days to complete, whereas it took thepattern-based approach only four days to completeon 6 GB and 10 days on 15 GB.5.3 RecallThe co-occurrence model has higher precisionthan the pattern-based algorithm on most datasets.Table 4.
Is-a relationships assigned to three randomly selected words (using pattern-based system on 15GB dataset).RANDOM WORD HUMAN WORDNET PATTERN-BASED SYSTEM (RANKED)Sanwa Bank bank none subsidiary / lender / bankMCI Worldcom Inc. telecommunications company none phone company / competitor / companycappuccino beverage none item / food / beverageTable 5.
Average precision, top-3 precision, and MRRfor both systems on each dataset.PATTERN SYSTEM CO-OCCURRENCE SYSTEMPrec Top-3 MRR Prec Top-3 MRR1.5MB38.7% 41.0% 41.0% 4.3% 8.0% 7.3%15MB 39.1% 43.0% 41.5% 14.6% 32.0% 24.3%150MB 40.6% 46.0% 45.5% 51.1% 73.0% 67.0%1.5GB 40.4% 39.0% 39.0% 56.7% 88.0% 77.7%6GB 46.3% 52.0% 49.7% 64.9% 90.0% 78.8%15GB 55.9% 54.0% 52.0% Too large to processTable 6.
Lenient average precision, top-3 precision,and MRR for both systems on each dataset.PATTERN SYSTEM CO-OCCURRENCE SYSTEMPrec Top-3 MRR Prec Top-3 MRR1.5MB56.6% 60.0% 60.0% 12.4% 20.0% 15.2%15MB 57.3% 63.0% 61.0% 23.2% 50.0% 37.3%150MB 50.7% 56.0% 55.0% 60.6% 78.0% 73.2%1.5GB 52.6% 51.0% 51.0% 69.7% 93.0% 85.8%6GB 61.8% 69.0% 67.5% 78.7% 92.0% 86.2%15GB 67.8% 67.0% 65.0% Too large to process775However, Figure 2 shows that the pattern-basedapproach extracts many more relationships.Semantic extraction tasks are notoriously diffi-cult to evaluate for recall.
To approximate recall,we defined a relative recall measure and conducteda question answering (QA) task of answering defi-nition questions.5.3.1 Relative recallAlthough it is impossible to know the number ofis-a relationships in any non-trivial corpus, it ispossible to compute the recall of a system relativeto another system?s recall.
The recall of a systemA, RA, is given by the following formula:CCR AA =where CA is the number of correct is-a relation-ships extracted by A and C is the total number ofcorrect is-a relationships in the corpus.
We definerelative recall of system A given system B, RA,B, as:BABABA CCRRR ==,Using the precision estimates, PA, from the pre-vious section, we can estimate CA ?
PA ?
|A|, whereA is the total number of is-a relationships discov-ered by system A.
Hence,BPAPRBABA?
?=,Figure 3 shows the relative recall of A = pattern-based approach relative to B = co-occurrencemodel.
Because of sparse data, the pattern-basedapproach has much higher precision and recall (sixtimes) than the co-occurrence approach on thesmall 15MB dataset.
In fact, only on the 150MBdataset did the co-occurrence system have higherrecall.
With datasets larger than 150MB, the co-occurrence algorithm reduces its running time byfiltering out grammatical relationships for wordsthat occurred fewer than k = 40 times and hencerecall is affected (in contrast, the pattern-basedapproach may generate a hyponym for a word thatit only sees once).5.3.2 Definition questionsFollowing Fleischman et al (2003), we selectthe 50 definition questions from the TREC2003(Voorhees 2003) question set.
These questions areof the form ?Who is X??
and ?What is X??
Foreach question (e.g., ?Who is Niels Bohr?
?, ?Whatis feng shui??)
we extract its respective instance(e.g., ?Neils Bohr?
and ?feng shui?
), look up theircorresponding hyponyms from our is-a table, andpresent the corresponding hyponym as the answer.We compare the results of both our systems withWordNet.
We extract at most the top 5 hyponymsprovided by each system.
We manually evaluatethe three systems and assign 3 classes ?Correct(C)?, ?Partially Correct (P)?
or ?Incorrect (I)?
toeach answer.This evaluation is different from the evaluationperformed by the TREC organizers for definitionquestions.
However, by being consistent across allTotal Number of Is-A Relationships vs. Dataset02000004000006000008000001000000120000014000001.5MB 15MB 150MB 1.5GB 6GB 15GBDatasetsTotal Is-ARelationshipssPattern-based SystemCo-occurrence-based SystemFigure 2.
Number of is-a relationships extracted bythe pattern-based and co-occurrence-based approaches.Table 7.
Average precision of the pattern-based sys-tem vs. WordNet and human hyponyms.PRECISION MRRPat.
WNet Human Pat.
WNet Human1.5MB38.7% 45.8% 83.0% 41.0% 84.4% 83.0%15MB 39.1% 52.4% 81.0% 41.5% 95.0% 91.0%150MB 40.6% 49.4% 84.0% 45.5% 88.9% 94.0%1.5GB 40.4% 43.4% 79.0% 39.0% 93.3% 89.0%6GB 46.3% 46.5% 76.0% 49.7% 75.0% 76.0%15GB 55.9% 45.6% 79.0% 52.0% 78.0% 79.0%Table 8.
Average precision of the co-occurrence-based system vs. WordNet and human hyponyms.PRECISION MRRCo-occ WNet Human Co-occ WNet Human1.5MB4.3% 42.7% 52.7% 7.3% 87.7% 95.0%15MB 14.6% 38.1% 48.7% 24.3% 86.6% 95.0%150MB 51.1% 57.5% 65.8% 67.0% 85.1% 98.0%1.5GB 56.7% 62.8% 70.3% 77.7% 93.0% 98.0%6GB 64.9% 68.9% 75.2% 78.8% 94.3% 98.0%Relative Recall (Pattern-based vs. Co-occurrence-based)0.001.002.003.004.005.006.007.001.5MB 15MB 150MB 1.5GB 6GB 15GB(projected)DatesetsRelativeRecallFigure 3.
Relative recall of the pattern-based approachrelative to the co-occurrence approach.776systems during the process, these evaluations givean indication of the recall of the knowledge base.We measure the performance on the top 1 and thetop 5 answers returned by each system.
Table 9and Table 10 show the results.The corresponding scores for WordNet are 38%accuracy in both the top-1 and top-5 categories (forboth strict and lenient).
As seen in this experiment,the results for both the pattern-based and co-occurrence-based systems report very poor per-formance for data sets up to 150 MB.
However,there is an increase in performance for both sys-tems on the 1.5 GB and larger datasets.
The per-formance of the system in the top 5 category ismuch better than that of WordNet (38%).
There ispromise for increasing our system accuracy by re-ranking the outputs of the top-5 hypernyms.6 ConclusionsThere is a long standing need for higher qualityperformance in NLP systems.
It is possible thatsemantic resources richer than WordNet will en-able them to break the current quality ceilings.Both statistical and symbolic NLP systems canmake use of such semantic knowledge.
With theincreased size of the Web, more and more trainingdata is becoming available, and as Banko and Brill(2001) showed, even rather simple learning algo-rithms can perform well when given enough data.In this light, we see an interesting need to de-velop fast, robust, and scalable methods to minesemantic information from the Web.
This papercompares and contrasts two methods for extractingis-a relations from corpora.
We presented a novelpattern-based algorithm, scalable to the terascale,which outperforms its more informed syntacticalco-occurrence counterpart on very small and verylarge data.Albeit possible to successfully apply linguisti-cally-light but data-rich approaches to some NLPapplications, merely reporting these results oftenfails to yield insights into the underlying theoriesof language at play.
Our biggest challenge as weventure to the terascale is to use our new foundwealth not only to build better systems, but to im-prove our understanding of language.ReferencesBanko, M. and Brill, E. 2001.
Mitigating the paucity of data problem.In Proceedings of HLT-2001.
San Diego, CA.Berland, M. and E. Charniak, 1999.
Finding parts in very largecorpora.
In ACL-1999.
pp.
57?64.
College Park, MD.Brill, E., 1995.
Transformation-based error-driven learning andnatural language processing: A case study in part of speechtagging.
Computational Linguistics, 21(4):543?566.Brill, E.; Lin, J.; Banko, M.; Dumais, S.; and Ng, A.
2001.
Data-intensive question answering.
In Proceedings of the TREC-10Conference, pp 183?189.
Gaithersburg, MD.Caraballo, S. 1999.
Automatic acquisition of a hypernym-labelednoun hierarchy from text.
In Proceedings of ACL-99.
pp 120?126,Baltimore, MD.Curran, J. and Moens, M. 2002.
Scaling context space.
In Proceedingsof ACL-02.
pp 231?238, Philadelphia, PA.Dunning, T. 1993.
Accurate methods for the statistics of surprise andcoincidence.
Computational Linguistics 191 (1993), 61?74.Etzioni, O.; Cafarella, M.; Downey, D.; Kok, S.; Popescu, A.M.;Shaked, T.; Soderland, S.; Weld, D. S.; and Yates, A.
2004.
Web-scale information extraction in Know-It All (Preliminary Results).To appear in the Conference on WWW.Fleischman, M.; Hovy, E.; and Echihabi, A.
2003.
Offline strategiesfor online question answering: Answering questions before they areasked.
In Proceedings of ACL-03.
pp.
1?7.
Sapporo, Japan.Girju, R.; Badulescu, A.; and Moldovan, D. 2003.
Learning semanticconstraints for the automatic discovery of part-whole relations.
InProceedings of HLT/NAACL-03.
pp.
80?87.
Edmonton, Canada.Harris, Z.
1985.
Distributional structure.
In: Katz, J. J.
(ed.)
ThePhilosophy of Linguistics.
New York: Oxford University Press.
pp.26?47.Hearst, M. 1992.
Automatic acquisition of hyponyms from large textcorpora.
In COLING-92.
pp.
539?545.
Nantes, France.Hindle, D. 1990.
Noun classification from predicate-argumentstructures.
In Proceedings of ACL-90.
pp.
268?275.
Pittsburgh, PA.Lin, D. 1994.
Principar - an efficient, broad-coverage, principle-basedparser.
Proceedings of COLING-94.
pp.
42?48.
Kyoto, Japan.Lin, D. 1998.
Automatic retrieval and  clustering of similar words.
InProceedings of COLING/ACL-98.
pp.
768?774.
Montreal, Canada.Mann, G. S. 2002.
Fine-Grained Proper Noun Ontologies for QuestionAnswering.
SemaNet?
02: Building and Using Semantic Networks,Taipei, Taiwan.Miller, G. 1990.
WordNet: An online lexical database.
InternationalJournal of Lexicography, 3(4).Och, F.J. and Ney, H. 2002.
Discriminative training and maximumentropy models for statistical machine translation.
In Proceedingsof ACL.
pp.
295?302.
Philadelphia, PA.Pantel, P. and Lin, D. 2002.
Discovering Word Senses from Text.
InProceedings of SIGKDD-02.
pp.
613?619.
Edmonton, Canada.Pantel, P. and Ravichandran, D. 2004.
Automatically labeling seman-tic classes.
In Proceedings of HLT/NAACL-04.
pp.
321?328.
Bos-ton, MA.Riloff, E. and Shepherd, J.
1997.
A corpus-based approach forbuilding semantic lexicons.
In Proceedings of EMNLP-1997.Voorhees, E. 2003.
Overview of the question answering track.
InProceedings of TREC-12 Conference.
NIST, Gaithersburg, MD.Table 9.
QA definitional evaluations for pattern-basedsystem.TOP-1 TOP5Strict Lenient Strict Lenient1.5MB0% 0% 0% 0%15MB 0% 0% 0% 0%150MB 2.0% 2.0% 2.0% 2.0%1.5GB 16.0% 22.0% 20.0% 22.0%6GB 38.0% 52.0% 56.0% 62.0%15GB 38.0% 52.0% 70.0% 74.0%Table 10.
QA definitional evaluations for co-occurrence-based system.TOP-1 TOP5Strict Lenient Strict Lenient1.5MB0% 0% 0% 0%15MB 0% 0% 0% 0%150MB 0% 0% 0% 0%1.5GB 6.0% 8.0% 6.0% 8.0%6GB 36.0% 44.0% 60.0% 62.0%777
