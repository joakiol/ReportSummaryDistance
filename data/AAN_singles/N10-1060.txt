Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 381?384,Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics?cba to check the spelling?Investigating Parser Performance on Discussion Forum PostsJennifer FosterNational Centre for Language TechnologySchool of ComputingDublin City Universityjfoster@computing.dcu.ieAbstractWe evaluate the Berkeley parser on text froman online discussion forum.
We evaluate theparser output with and without gold tokensand spellings (using Sparseval and Parseval),and we compile a list of problematic phenom-ena for this domain.
The Parseval f-score for asmall development set is 77.56.
This increasesto 80.27 when we apply a set of simple trans-formations to the input sentences and to theWall Street Journal (WSJ) training sections.1 IntroductionParsing techniques have recently become efficientenough for parsers to be used as part of a pipeline ina variety of tasks.
Another recent development is therise of user-generated content in the form of blogs,wikis and discussion forums.
Thus, it is both inter-esting and necessary to investigate the performanceof NLP tools trained on edited text when applied tounedited Web 2.0 text.
McClosky et al (2006) re-port a Parseval f-score decrease of 5% when a WSJ-trained parser is applied to Brown corpus sentences.In this paper, we move even further from the WSJ byinvestigating the performance of the Berkeley parser(Petrov et al, 2006) on user-generated content.We create gold standard phrase structure trees forthe posts on two threads of the same online dis-cussion forum.
We then parse the sentences inone thread, our development set, with the Berke-ley parser under three conditions: 1) when it per-forms its own tokenisation, 2) when it is providedwith gold tokens and 3) when misspellings in the in-put have been corrected.
A qualitative evaluation isthen carried out on parser output under the third con-dition.
Based on this evaluation, we identify some?low-hanging fruit?
which we attempt to handle ei-ther by transforming the input sentence or by trans-forming the WSJ training material.
The success ofthese transformations is evaluated on our develop-ment and test sets, with encouraging results.2 Parser Evaluation ExperimentsData Our data consists of sentences that occur onthe BBC Sport 606 Football discussion forum.
Theposts on this forum are quite varied, ranging fromthrowaway comments to more considered essay-likecontributions.
The development set consists of 42posts (185 sentences) on a thread discussing a con-troversial refereeing decision in a soccer match.1The test set is made up of 40 posts (170 sentences)on a thread discussing a player?s behaviour in thesame match.2 The average sentence length in thedevelopment set is 18 words and the test set 15words.
Tokenisation and spelling correction werecarried out by hand on the sentences in both sets.3They were then parsed using Bikel?s parser (Bikel,2004) and corrected by hand using the Penn Tree-bank Bracketing Guidelines (Bies et al, 1995).Parser The Berkeley parser is an unlexicalisedphrase structure parser which learns a latent vari-able PCFG by iteratively splitting the treebank non-1http://www.bbc.co.uk/dna/606/F15264075?thread=7065503&show=502http://www.bbc.co.uk/dna/606/F15265997?thread=7066196&show=503Note that abbreviated forms such as cos which are typicalof computer-mediated communication are not corrected.381terminals, estimating rule probabilities for the newgrammar using EM and merging the less usefulsplits.
We train a PCFG from WSJ2-21 by carryingout five cycles of the split-merge process (SM5).Tokenisation and Spelling Effects In the first ex-periment, the parser is given the original devel-opment set sentences which contain spelling mis-takes and which have not been tokenised.
We askthe parser to perform its own tokenisation.
In thesecond experiment, the parser is given the hand-tokenised sentences which still contain spelling mis-takes.
These are corrected for the third experiment.Since the yields of the parser output and gold treesare not guaranteed to match exactly, we cannot usethe evalb implementation of the Parseval evalua-tion metrics.
Instead we use Sparseval (Roark et al,2006), which was designed to be used to evaluate theparsing of spoken data and can handle this situation.An unaligned dependency evaluation is carried out:head-finding rules are used to convert a phrase struc-ture tree into a dependency graph.
Precision and re-call are calculated over the dependenciesThe Sparseval results are shown in Table 1.
Forthe purposes of comparison, the WSJ23 perfor-mance is displayed in the top row.
We can see thatperformance suffers when the parser performs itsown tokenisation.
A reason for this is the under-useof apostrophes in the forum data, with the result thatwords such as didnt and im remain untokenised andare tagged by the parser as common nouns:(NP (NP (DT the) (NNS refs)) (SBAR (S (NP (NN didnt))(VP want to make it to obvious))))To properly see the effect of the 39 spelling errorson parsing accuracy, we factor out the mismatchesbetween the correctly spelled words in the referenceset and their incorrectly spelled equivalents.
We dothis by evaluating against a version of the gold stan-dard which contains the original misspellings (thirdrow).
We can see that the effect of spelling errorsis quite small.
The Berkeley parser?s mechanismfor handling unknown words makes use of suffix in-formation and it is able to ignore many of the con-tent word spelling errors.
It is the errors in functionwords that appear to cause a greater problem:(NP (DT the) (JJ zealous) (NNS fans) (NN whpo) (NNcare) (JJR more) )Test Set R P FWSJ 23 88.66 88.66 88.66Football 68.49 70.74 69.60Football Gold Tokens 71.54 73.25 72.39Ft Gold Tok (misspelled gold) 73.49 75.25 74.36Football Gold Tokens+Spell 73.94 75.59 74.76Table 1: Sparseval scores for Berkeley SM5Test Set R P FWSJ 23 88.88 89.46 89.17Football Gold Tokens+Spell 78.15 76.97 77.56Table 2: Parseval scores for Berkeley SM5Gold Tokens and Spelling Leaving aside theproblems of automatic tokenisation and spelling cor-rection, we focus on the results of the third experi-ment.
The Parseval results are given in Table 2.
Notethat the performance degradation is quite large, morethan has been reported for the Charniak parser onthe Brown corpus.
We examine the parser output foreach sentence in the development set.
The phenom-ena which lead the parser astray are listed in Table 3.One problem is coordination which is difficult forparsers on in-domain data but which is exacerbatedhere by the omission of conjunctions, the use of acomma as a conjunction and the tendency towardsunlike constituent coordination.Parser Comparison We test the lexicalised Char-niak parser plus reranker (Charniak and Johnson,2005) on the development set sentences.
We alsotest the Berkeley parser with an SM6 grammar.
Thef-scores are shown in Table 4.
The parser achiev-ing the highest score on WSJ23, namely, the C&Jreranking parser, also achieves the highest score onour development set.
The difference between thetwo Berkeley grammars supports the claim that anSM6 grammar overfits to the WSJ (Petrov and Klein,2007).
However, the differences between the fourparser/grammar configurations are small.Parser WSJ23 FootballBerkeley SM5 89.17 77.56Berkeley SM6 89.56 77.01Charniak First-Stage 89.13 77.13C & J Reranking 91.33 78.33Table 4: Cross-parser and cross-grammar comparison382Problematic Phenomena ExamplesIdioms/Fixed Expressions Spot on(S (VP (VB Spot) (PP (IN on))) (.
.
))Acronymslmao(S (NP (PRP you))(VP (VBZ have) (RB n?t) (VP (VBN done)(NP (ADVP (RB that) (RB once)) (DT this) (NN season))(NP (NN lmao)))))Missing subjectDoes n?t change the result though(SQ (VBZ Does) (RB n?t) (NP (NN change))(NP (DT the) (NN result)) (ADVP (RB though)) (.
!
))Lowercase proper nouns paul scholes(NP (JJ paul) (NNS scholes))CoordinationVery even game and it?s sad that...(S (ADVP (RB Very))(NP (NP (JJ even) (NN game)) (CC and) (NP (PRP it)))(VP (VBZ ?s) (ADJP (JJ sad)) (SBAR (IN that)...Adverb/Adjective Confusionwhen playing bad(SBAR (WHADVP (WRB when))(S (VP (VBG playing) (ADJP (JJ bad)))))CAPS LOCK IS ONYOU GOT BEATEN BY THE BETTER TEAM(S (NP (PRP YOU)) (VP (VBP GOT) (NP (NNP BEATEN)(NNP BY) (NNP THE) (NNP BETTER) (NNP TEAM))))cos instead of becauseor it was cos you lost(VP (VBD was) (ADJP (NN cos)(SBAR (S (NP (PRP you)) (VP (VBD lost))))))Table 3: Phenomena which lead the parser astray.
The output of the parser is given for each example.3 Initial ImprovementsParsing performance on noisy data can be improvedby transforming the input data so that it resemblesthe parser?s training data (Aw et al, 2006), trans-forming the training data so that it resembles the in-put data (van der Plas et al, 2009), applying semi-supervised techniques such as the self-training pro-tocol used by McClosky et al (2006), and changingthe parser internals, e.g.
adapting the parser?s un-known word model to take into account variation incapitalisation and function word misspelling.4We focus on the first two approaches and attemptto transform both the input data and the WSJ trainingmaterial.
The transformations that we experimentwith are shown in Table 5.
The treebank transfor-mations are performed in such a way that their fre-quency distribution mirrors their distribution in thedevelopment data.
We remove discourse-markingacronyms such as lol5 from the input sentence, but4Even when spelling errors have been corrected, unknownwords are still an issue: 8.5% of the words in the football devel-opment set do not occur in WSJ2-21, compared to 3.6% of thewords in WSJ23.5In a study of teenage instant messaging, Tagliamonte andDennis (2008) found that forms such as lol are not as ubiquitousas is commonly perceived.
Although only occurring a couple ofdo not attempt to handle acronyms which are inte-grated into the sentence.6We examine the effect of each transformation ondevelopment set parsing performance and discardthose which do not improve performance.
We keepall the input sentence transformations and those tree-bank transformations which affect lexical rules, i.e.changing the endings on adverbs and changing thefirst character of proper nouns.
The treebank trans-formations which delete subject pronouns and co-ordinating conjunctions are not as effective.
Theywork in individual cases, e.g.
the original analysisof the sentence Will be here all day is(S (NP (NNP Will)) (VP be here all day) (.
.
))After applying the treebank transformation, it is(S (VP (MD Will) (VP be here all day)) (.
.
))Their overall effect is, however, negative.
It is likelythat, for complex phenomena such as coordinationand subject ellipsis, the development set is still toosmall to inform how much of and in what way theoriginal treebank should be transformed.
The resultsof applying the effective transformations to the de-velopment set and the test set are shown in Table 6.times in our data, they are problematic for the parser.6An example is: your loss to Wigan would be more scrutu-nized (cba to check spelling) than it has been this year383Input Sentencecos ?
becauseSentences consisting of all uppercase characters converted to standard capitalisationDEAL WITH IT ?
Deal with itRemove certain acronymslol?
TreebankDelete subject noun phrases when the subject is a pronoun(S (NP (PRP It)) (VP (VBD arrived)...
??
(S (VP (VBD arrived)...Delete or replace conjunctions with a comma (for sentence coordination)(S ...) (CC and) (S ...) ??
(S ...) (, ,) (S ...) OR (S ...) (CC and) (S ...) ??
(S ...) (S ...)Delete ly from adverbs(VP (VBD arrived) (ADVP (RB quickly))) ??
(VP (VBD arrived) (ADVP (RB quick)))Replace uppercase first character in proper nouns(NP (NP (NNP Warner) (POS ?s)) (NN price)) ??
(NP (NP (NNP warner) (POS ?s)) (NN price))Table 5: Input Sentence and Treebank TransformationsConfiguration Recall Precision F-ScoreBaseline Dev 78.15 76.97 77.56Transformed Dev 80.83 79.73 80.27Baseline Test 77.61 79.14 78.37Transformed Test 80.10 79.77 79.93Table 6: Effect of transformations on dev and test setThe recall and precision improvements on the devel-opment set are statistically significant (p < 0.02), asis the recall improvement on the test set (p < 0.05).4 ConclusionOngoing research on the problem of parsingunedited informal text has been presented.
At themoment, because of the small size of the data setsand the variety of writing styles in the developmentset, only tentative conclusions can be drawn.
How-ever, even this small data set reveals clear problemsfor WSJ-trained parsers: the handling of long co-ordinated sentences (particularly in the presence oferratic punctuation usage), domain-specific fixed ex-pressions and unknown words.
We have presentedsome preliminary experimental results using simpletransformations to both the input sentence and theparser?s training material.
Treebank transformationsneed to be more thoroughly explored with use madeof the Switchboard corpus as well as the WSJ.AcknowledgmentsThanks to the reviewers and to Emmet ?O Briain,Deirdre Hogan, Adam Bermingham, Joel Tetreault.ReferencesAiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
Aphrase-based statistical model for SMS text normali-sation.
In Proceedings of the 21st COLING/44th ACL.Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-Intyre.
1995.
Bracketing guidelines for Treebank IIstyle, Penn Treebank Project.
Technical Report TechReport MS-CIS-95-06, University of Pennsylvania.Daniel Bikel.
2004.
Intricacies of Collins Parsing Model.Computational Linguistics, 30(4):479?511.Eugene Charniak and Mark Johnson.
2005.
Course-to-fine n-best-parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd ACL.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Reranking and self-training for parser adapta-tion.
In Proceedings of the 21st COLING/44th ACL.Slav Petrov and Dan Klein.
2007.
Improved infer-ence for unlexicalized parsing.
In Proceedings of HLTNAACL 2007.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact and inter-pretable tree annotation.
In Proceedings of the 21stCOLING and the 44th ACL.Brian Roark, Mary Harper, Eugene Charniak, BonnieDorr, Mark Johnson, Jeremy G. Kahn, Yang Liu, MariOstendorf, John Hale, Anna Krasnyanskaya, MatthewLease, Izhak Shafran, Matthew Snover, Robin Stewart,and Lisa Yung.
2006.
SParseval: Evaluation metricsfor parsing speech.
In Proceedings of LREC.Sali A. Tagliamonte and Derek Dennis.
2008.
Linguis-tic ruin?
LOL!
Instant messaging and teen language.American Speech, 83(1).Lonneke van der Plas, James Henderson, and PaolaMerlo.
2009.
Domain adaptation with artificial datafor semantic parsing of speech.
In Proceedings of HLTNAACL 2009, Companion Volume: Short Papers.384
