Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 772?778,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsSentence Level Dialect Identificationfor Machine Translation System SelectionWael Salloum, Heba Elfardy, Linda Alamir-Salloum, Nizar Habash and Mona Diab?Center for Computational Learning Systems, Columbia University, New York, USA{wael,heba,habash}@ccls.columbia.edu?Department of Computer Science, The George Washington University, Washington DC, USA?mtdiab@email.gwu.eduAbstractIn this paper we study the use of sentence-level dialect identification in optimizingmachine translation system selection whentranslating mixed dialect input.
We testour approach on Arabic, a prototypicaldiglossic language; and we optimize thecombination of four different machinetranslation systems.
Our best result im-proves over the best single MT systembaseline by 1.0% BLEU and over a strongsystem selection baseline by 0.6% BLEUon a blind test set.1 IntroductionA language can be described as a set of dialects,among which one "standard variety" has a spe-cial representative status.1Despite being increas-ingly ubiquitous in informal written genres suchas social media, most non-standard dialects areresource-poor compared to their standard variety.For statistical machine translation (MT), which re-lies on the existence of parallel data, translatingfrom non-standard dialects is a challenge.
In thispaper we study the use of sentence-level dialectidentification together with various linguistic fea-tures in optimizing the selection of outputs of fourdifferent MT systems on input text that includes amix of dialects.We test our approach on Arabic, a prototypi-cal diglossic language (Ferguson, 1959) where thestandard form of the language, Modern StandardArabic (MSA) and the regional dialects (DA) liveside-by-side and are closely related.
MSA is thelanguage used in education, scripted speech andofficial settings while DA is the primarily spoken1This paper presents work supported by the Defense Ad-vanced Research Projects Agency (DARPA) contract No.HR0011-12-C-0014.
Any opinions, findings and conclusionsor recommendations expressed in this paper are those of theauthors and do not necessarily reflect the views of DARPA.native vernacular.
We consider two DAs: Egyp-tian and Levantine Arabic in addition to MSA.
Ourbest system selection approach improves over ourbest baseline single MT system by 1.0% absoluteBLEU point on a blind test set.2 Related WorkArabic Dialect Machine Translation.
Two ap-proaches have emerged to alleviate the problemof DA-English parallel data scarcity: using MSAas a bridge language (Sawaf, 2010; Salloum andHabash, 2011; Salloum and Habash, 2013; Sajjadet al, 2013), and using crowd sourcing to acquireparallel data (Zbib et al, 2012).
Sawaf (2010)and Salloum and Habash (2013) used hybrid so-lutions that combine rule-based algorithms and re-sources such as lexicons and morphological ana-lyzers with statistical models to map DA to MSAbefore using MSA-to-English MT systems.
Zbibet al (2012) obtained a 1.5M word parallel corpusof DA-English using crowd sourcing.
Applied ona DA test set, a system trained on their 1.5M wordcorpus outperformed a system that added 150Mwords of MSA-English data, as well as outper-forming a system with oracle DA-to-MSA pivot.In this paper we use four MT systems that trans-late from DA to English in different ways.
Similarto Zbib et al (2012), we use DA-English, MSA-English and DA+MSA-English systems.
Our DA-English data includes the 1.5M words created byZbib et al (2012).
Our fourth MT system usesELISSA, the DA-to-MSA MT tool by Salloum andHabash (2013), to produce an MSA pivot.Dialect Identification.
There has been a num-ber of efforts on dialect identification (Biadsy etal., 2009; Zaidan and Callison-Burch, 2011; Ak-bacak et al, 2011; Elfardy et al, 2013; Elfardyand Diab, 2013).
Elfardy et al (2013) performedtoken-level dialect ID by casting the problem asa code-switching problem and treating MSA andEgyptian as two different languages.
They later772used features from their token-level system to traina classifier that performs sentence-level dialect ID(Elfardy and Diab, 2013).
In this paper, we useAIDA, the system of Elfardy and Diab (2013), toprovide a variety of dialect ID features to trainclassifiers that select, for a given sentence, the MTsystem that produces the best translation.System Selection and Combination in MachineTranslation.
The most popular approach to MTsystem combination involves building confusionnetworks from the outputs of different MT sys-tems and decoding them to generate new transla-tions (Rosti et al, 2007; Karakos et al, 2008; Heet al, 2008; Xu et al, 2011).
Other researchersexplored the idea of re-ranking the n-best outputof MT systems using different types of syntacticmodels (Och et al, 2004; Hasan et al, 2006; Maand McKeown, 2013).
While most researchersuse target language features in training their re-rankers, others considered source language fea-tures (Ma and McKeown, 2013).Most MT system combination work uses MTsystems employing different techniques to train onthe same data.
However, in this paper, we use thesame MT algorithms for training, tuning, and test-ing, but vary the training data, specifically in termsof the degree of source language dialectness.
Ourapproach runs a classifier trained only on sourcelanguage features to decide which system shouldtranslate each sentence in the test set, which meansthat each sentence goes through one MT systemonly.
Since we do not combine the output of theMT systems on the phrase level, we call our ap-proach "system selection" to avoid confusion.3 Machine Translation ExperimentsIn this section, we present our MT experimentalsetup and the four baseline systems we built, andwe evaluate their performance and the potential oftheir combination.
In the next section we presentand evaluate the system selection approach.MT Tools and Settings.
We use the open-sourceMoses toolkit (Koehn et al, 2007) to build fourArabic-English phrase-based statistical machinetranslation systems (SMT).
Our systems use astandard phrase-based architecture.
The parallelcorpora are word-aligned using GIZA++ (Och andNey, 2003).
The language model for our systemsis trained on English Gigaword (Graff and Cieri,2003).
We use SRILM Toolkit (Stolcke, 2002)to build a 5-gram language model with modifiedKneser-Ney smoothing.
Feature weights are tunedto maximize BLEU on tuning sets using Mini-mum Error Rate Training (Och, 2003).
Resultsare presented in terms of BLEU (Papineni et al,2002).
All evaluation results are case insensi-tive.
The English data is tokenized using simplepunctuation-based rules.
The MSA portion of theArabic side is segmented according to the ArabicTreebank (ATB) tokenization scheme (Maamouriet al, 2004; Sadat and Habash, 2006) using theMADA+TOKAN morphological analyzer and tok-enizer v3.1 (Roth et al, 2008), while the DA por-tion is ATB-tokenized with MADA-ARZ (Habashet al, 2013).
The Arabic text is also Alif/Ya nor-malized.
For more details on processing Arabic,see (Habash, 2010).MT Train/Tune/Test Data.
We have two par-allel corpora.
The first is a DA-English corpusof 5M tokenized words of Egyptian (?3.5M)and Levantine (?1.5M).
This corpus is part ofBOLT data.
The second is an MSA-English cor-pus of 57M tokenized words obtained from sev-eral LDC corpora (10 times the size of the DA-English data).
We work with eight standard MTtest sets: three MSA sets from NIST MTEval withfour references (MT06, MT08, and MT09), fourEgyptian sets from LDC BOLT data with two ref-erences (EgyDevV1, EgyDevV2, EgyDevV3, andEgyTestV2), and one Levantine set from BBN(Zbib et al, 2012) with one reference which wesplit into LevDev and LevTest.
We used MT08and EgyDevV3 to tune SMT systems while we di-vided the remaining sets among classifier trainingdata (5,562 sentences), dev (1,802 sentences) andblind test (1,804 sentences) sets to ensure each ofthese new sets has a variety of dialects and genres(weblog and newswire).MT Systems.
We build four MT systems.
(1) DA-Only.
This system is trained on the DA-English data and tuned on EgyDevV3.
(2) MSA-Only.
This system is trained on theMSA-English data and tuned on MT08.
(3) DA+MSA.
This system is trained on thecombination of both corpora (resulting in 62M to-kenized2words on the Arabic side) and tuned on2Since the DA+MSA system is intended for DA data andDA morphology, as far as tokenization is concerned, is morecomplex, we tokenized the training data with dialect aware-ness (DA with MADA-ARZ and MSA with MADA) sinceMADA-ARZ does a lot better than MADA on DA (Habashet al, 2013).
Tuning and Test data, however, are tokenizedby MADA-ARZ since we do not assume any knowledge ofthe dialect of a test sentence.773EgyDevV3.
(4) MSA-Pivot.
This MSA-pivoting systemuses Salloum and Habash (2013)?s DA-MSA MTsystem followed by an Arabic-English SMT sys-tem which is trained on both corpora augmentedwith the DA-English where the DA side is prepro-cessed with the same DA-MSA MT system thentokenized with MADA-ARZ.
The result is 67Mtokenized words on the Arabic side.
EgyDevV3was similarly preprocessed with the DA-MSA MTsystem and MADA-ARZ and used for tuning thesystem parameters.
Test sets are similarly prepro-cessed before decoding with the SMT system.Baseline MT System Results.
We report the re-sults of our dev set on the four MT systems webuilt in Table 1.
The MSA-Pivot system producesthe best singleton result among all systems.
Alldifferences in BLEU scores between the four sys-tems are statistically significant above the 95%level.
Statistical significance is computed usingpaired bootstrap re-sampling (Koehn, 2004).System Training Data (TD) BLEUName DA-En MSA-En DAT-En TD Size1.
DA-Only 5M 5M 26.62.
MSA-Only 57M 57M 32.73.
DA+MSA 5M 57M 62M 33.64.
MSA-Pivot 5M 57M 5M 67M 33.9Oracle System Selection 39.3Table 1: Results from the baseline MT systems and their or-acle system selection.
The training data west used in differentMT systems are also indicated.
DAT(in the fourth column)is the DA part of the 5M word DA-En parallel data processedwith the DA-MSA MT system.Oracle System Selection.
We also report in Ta-ble 1 an oracle system selection where we pick, foreach sentence, the English translation that yieldsthe best BLEU score.
This oracle indicates thatthe upper bound for improvement achievable fromsystem selection is 5.4% BLEU.
Excluding dif-ferent systems from the combination lowered theoverall score between 0.9% and 1.8%, suggestingthe systems are indeed complementary.4 MT System SelectionThe approach we take in this paper benefits fromthe techniques and conclusions of previous papersin that we build different MT systems similar tothose discussed above but instead of trying to findwhich one is the best, we try to leverage the useof all of them by automatically deciding what sen-tences should go to which system.
Our hypothesisis that these systems complement each other in in-teresting ways where the combination of their se-lections could lead to better overall performancestipulating that our approach could benefit fromthe strengths while avoiding the weaknesses ofeach individual system.4.1 Dialect ID Binary ClassificationFor baseline system selection, we use the clas-sification decision of Elfardy and Diab (2013)?ssentence-level dialect identification system to de-cide on the target MT system.
Since the deci-sion is binary (DA or MSA) and we have four MTsystems, we considered all possible configurationsand determined empirically that the best configu-ration is to select MSA-Only for the MSA tag andMSA-Pivot for the DA tag.
We do not report otherconfiguration results due to space restrictions.4.2 Feature-based Four-Class ClassificationFor our main approach, we train a four-class clas-sifier to predict the target MT system to selectfor each sentence using only source-language fea-tures.
We experimented with different classifiersin the Weka Data Mining Tool (Hall et al, 2009)for training and testing our system selection ap-proach.
The best performing classifier was NaiveBayes (with Weka?s default settings).Training Data Class Labels.
We run the5,562 sentences of the classification trainingdata through our four MT systems and producesentence-level BLEU scores (with length penalty).We pick the name of the MT system with the high-est BLEU score as the class label for that sen-tence.
When there is a tie in BLEU scores, we pickthe system label that yields better overall BLEUscores from the systems tied.Training Data Source-Language Features.We use two sources of features extracted fromuntokenized sentences to train our four-classclassifiers: basic and extended features.A.
Basic FeaturesThese are the same set of features that were usedby the dialect ID tool together with the class labelgenerated by this tool.i.
Token-Level Features.
These features rely onlanguage models, MSA and Egyptian morphologi-cal analyzers and a Highly Dialectal Egyptian lex-icon to decide whether each word is MSA, Egyp-tian, Both, or Out of Vocabulary.ii.
Perplexity Features.
These are two featuresthat measure the perplexity of a sentence against774two language models: MSA and Egyptian.iii.
Meta Features.
Features that do not di-rectly relate to the dialectalness of words in thegiven sentence but rather estimate how informalthe sentence is and include: percentage of to-kens, punctuation, and Latin words, number of to-kens, average word length, whether the sentencehas any words that have word-lengthening effectsor not, whether the sentence has any diacritizedwords or not, whether the sentence has emoticonsor not, whether the sentence has consecutive re-peated punctuation or not, whether the sentencehas a question mark or not, and whether the sen-tence has an exclamation mark or not.iv.
The Dialect-Class Feature.
We run the sen-tence through the Dialect ID binary classifier andwe use the predicted class label (DA or MSA) as afeature in our system.
Since the Dialect ID systemwas trained on a different data set, we think its de-cision may provide additional information to ourclassifiers.B.
Extended FeaturesWe add features extracted from two sources.i.
MSA-Pivoting Features.
Salloum and Habash(2013) DA-MSA MT system produces interme-diate files used for diagnosis or debugging pur-poses.
We exploit one file in which the sys-tem identifies (or, "selects") dialectal words andphrases that need to be translated to MSA.
We ex-tract confidence indicating features.
These fea-tures are: sentence length (in words), percent-age of selected words and phrases, number of se-lected words, number of selected phrases, num-ber of words morphologically selected as dialec-tal by a mainly Levantine morphological analyzer,number of words selected as dialectal by the tool?sDA-MSA lexicons, number of OOV words againstthe MSA-Pivot system training data, number ofwords in the sentences that appeared less than 5times in the training data, number of words in thesentences that appeared between 5 and 10 timesin the training data, number of words in the sen-tences that appeared between 10 and 15 timesin the training data, number of words that havespelling errors and corrected by this tool (e.g.,word-lengthening), number of punctuation marks,and number of words that are written in Latinscript.ii.
MT Training Data Source-Side LM Perplex-ity Features.
The second set of features uses per-plexity against language models built from thesource-side of the training data of each of the fourbaseline systems.
These four features may tell theclassifier which system is more suitable to trans-late a given sentence.4.3 System Selection EvaluationDevelopment Set.
The first part of Table 2 re-peats the best baseline system and the four-systemoracle combination from Table 1 for convenience.The third row shows the result of running our sys-tem selection baseline that uses the Dialect ID bi-nary decision on the Dev set sentences to decideon the target MT system.
It improves over the bestsingle system baseline (MSA-Pivot) by a statisti-cally significant 0.5% BLEU.
Crucially, we shouldnote that this is a deterministic process.System BLEU Diff.Best Single MT System Baseline 33.9 0.0Oracle 39.3 5.4Dialect ID Binary Selection Baseline 34.4 0.5Four-Class ClassificationBasic Features 35.1 1.2Extended Features 34.8 0.9Basic + Extended Features 35.2 1.3Table 2: Results of baselines and system selection systemson the Dev set in terms of BLEU.
The best single MT systembaseline is MSA-Pivot.The second part of Table 2 shows the results ofour four-class Naive Bayes classifiers trained onthe classification training data we created.
Thefirst column shows the source of sentence levelfeatures employed.
As mentioned earlier, we usethe Basic features alone, the Extended featuresalone, and then their combination.
The classifierthat uses both feature sources simultaneously asfeature vectors is our best performer.
It improvesover our best baseline single MT system by 1.3%BLEU and over the Dialect ID Binary Classifica-tion system selection baseline by 0.8% BLEU.
Im-provements are statistically significant.System BLEU Diff.DA-Only 26.6MSA-Only 30.7DA+MSA 32.4MSA-Pivot 32.5Four-System Oracle Combination 38.0 5.5Best Dialect ID Binary Classifier 32.9 0.4Best Classifier: Basic + Extended Features 33.5 1.0Table 3: Results of baselines and system selection systemson the Blind test set in terms of BLEU.Blind Test Set.
Table 3 shows the results on ourBlind Test set.
The first part of the table showsthe results of our four baseline MT systems.
Thesystems have the same rank as on the Dev set and775System All Dialect MSADA-Only 26.6 19.3 33.2MSA-Only 32.7 14.7 50.0DA+MSA 33.6 19.4 46.3MSA-Pivot 33.9 19.6 46.4Four-System Oracle Combination 39.3 24.4 52.1Best Performing Classifier 35.2 19.8 50.0Table 4: Dialect breakdown of performance on the Dev setfor our best performing classifier against our four baselinesand their oracle combination.
Our classifier does not knowof these subsets, it runs on the set as a whole; therefore, werepeat its results in the second column for convenience.MSA-Pivot is also the best performer.
The differ-ences in BLEU are statistically significant.
Thesecond part shows the four-system oracle combi-nation which shows a 5.5% BLEU upper boundon improvements.
The third part shows the re-sults of the Dialect ID Binary Classification whichimproves by 0.4% BLEU.
The last row showsthe four-class classifier results which improves by1.0% BLEU over the best single MT system base-line and by 0.6% BLEU over the Dialect ID Bi-nary Classification.
Results on the Blind Test setare consistent with the Dev set results.5 Discussion and Error AnalysisDA versus MSA Performance.
In Table 4, col-umn All illustrates the results over the entire Devset, while columns DA and MSA show systemperformance on the DA and MSA subsets of theDev set, respectively.
The best single baseline MTsystem for DA is MSA-Pivot has a large room forimprovement given the oracle upper bound (4.8%BLEU absolute).
However, our best system selec-tion approach improves over MSA-Pivot by a smallmargin of 0.2% BLEU absolute only, albeit a sta-tistically significant improvement.
The MSA col-umn oracle shows a smaller improvement of 2.1%BLEU absolute over the best single MSA-Only MTsystem.
Furthermore, when translating MSA withour best system selection performer we get thesame results as the best baseline MT system forMSA even though our system does not know thedialect of the sentences a priori.
If we consider thebreakdown of the performance in our best overall(33.9% BLEU) single baseline MT system (MSA-Pivot), we observe that the performance on MSAis about 3.6% absolute BLEU points below ourbest results; this suggests that most of the systemselection gain over the best single baseline is onMSA selection.Manual Error Analysis.
We performed manualerror analysis on a Dev set sample of 250 sen-tences distributed among the different dialects andgenres.
Our best performing classifier selected thebest system in 48% of the DA cases and 52% ofthe MSA cases.
We did a detailed manual erroranalysis for the cases where the classifier failed topredict the best MT system.
The sources of errorswe found cover 89% of the cases.
In 21% of theerror cases, our classifier predicted a better trans-lation than the one considered gold by BLEU dueto BLEU bias, e.g., severe sentence-level lengthpenalty due to an extra punctuation in a short sen-tence.
Also, 3% of errors are due to bad refer-ences, e.g., a dialectal sentence in an MSA set thatthe human translators did not understand.A group of error sources resulted from MSAsentences classified correctly as MSA-Only; how-ever, one of the other three systems produced bet-ter translations for two reasons.
First, since theMSA training data is from an older time span thanthe DA data, 10% of errors are due to MSA sen-tences that use recent terminology (e.g., Egyp-tian revolution 2011: places, politicians, etc.
)that appear in the DA training data.
Also, webwriting styles in MSA sentences such as blogstyle (e.g., rhetorical questions), blog punctuationmarks (e.g., "..", "???!!
"), and formal MSA forumgreetings resulted in 23%, 16%, and 6% of thecases, respectively.Finally, in 10% of the cases our classifier is con-fused by a code-switched sentence, e.g., a dialec-tal proverb in an MSA sentence or a weak MSAliteral translation of dialectal words and phrases.Some of these cases may be solved by addingmore features to our classifier, e.g., blog style writ-ing features, while others need a radical change toour technique such as word and phrase level di-alect identification for MT system combination ofcode-switched sentences.6 Conclusion and Future WorkWe presented a sentence-level classification ap-proach for MT system selection for diglossic lan-guages.
We got a 1.0% BLEU improvement overthe best baseline single MT system.
In the futurewe plan to add more training data to see the effecton the accuracy of system selection.
We plan togive different weights to different training exam-ples based on the drop in BLEU score the exam-ple can cause if classified incorrectly.
We also planto explore confusion-network combination and re-ranking techniques based on target language fea-tures.776ReferencesMurat Akbacak, Dimitra Vergyri, Andreas Stolcke,Nicolas Scheffer, and Arindam Mandal.
2011.Effective arabic dialect classification using diversephonotactic models.
In INTERSPEECH, volume 11,pages 737?740.Fadi Biadsy, Julia Hirschberg, and Nizar Habash.2009.
Spoken arabic dialect identification usingphonotactic modeling.
In Proceedings of the Work-shop on Computational Approaches to Semitic Lan-guages at the meeting of the European Associa-tion for Computational Linguistics (EACL), Athens,Greece.Heba Elfardy and Mona Diab.
2013.
Sentence LevelDialect Identification in Arabic.
In Proceedings ofthe 51th Annual Meeting of the Association for Com-putational Linguistics (ACL-13), Sofia, Bulgaria.Heba Elfardy, Mohamed Al-Badrashiny, and MonaDiab.
2013.
Code switch point detection in arabic.In Proceedings of the 18th International Conferenceon Application of Natural Language to InformationSystems (NLDB2013), MediaCity, UK.Charles F Ferguson.
1959.
Diglossia.
Word,15(2):325?340.David Graff and Christopher Cieri.
2003.
English Gi-gaword, LDC Catalog No.
: LDC2003T05.
Linguis-tic Data Consortium, University of Pennsylvania.Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-kander, and Nadi Tomeh.
2013.
MorphologicalAnalysis and Disambiguation for Dialectal Arabic.In Proceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT), Atlanta, GA.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool Publish-ers.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Explorations, 11(1):10?18.S.
Hasan, O. Bender, and H. Ney.
2006.
Rerank-ing translation hypotheses using structural proper-ties.
In EACL?06 Workshop on Learning StructuredInformation in Natural Language Applications.Xiaodong He, Mei Yang, Jianfeng Gao, PatrickNguyen, and Robert Moore.
2008.
Indirect-hmm-based hypothesis alignment for combining outputsfrom machine translation systems.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing, pages 98?107.
Associa-tion for Computational Linguistics.Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,and Markus Dreyer.
2008.
Machine translationsystem combination using itg-based alignments.
InProceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics on HumanLanguage Technologies: Short Papers, pages 81?84.Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-pher Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Christopher Dyer, Ondrej Bo-jar, Alexandra Constantin, and Evan Herbst.
2007.Moses: open source toolkit for statistical machinetranslation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational LinguisticsCompanion Volume Proceedings of the Demo andPoster Sessions, pages 177?180, Prague, Czech Re-public.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP 2004, pages 388?395, Barcelona, Spain,July.
Association for Computational Linguistics.Wei-Yun Ma and Kathleen McKeown.
2013.
Usinga supertagged dependency language model to selecta good translation in system combination.
In Pro-ceedings of NAACL-HLT, pages 433?438.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The Penn Arabic Treebank:Building a Large-Scale Annotated Arabic Corpus.In NEMLAR Conference on Arabic Language Re-sources and Tools, pages 102?109, Cairo, Egypt.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Franz Josef Och.
2004.A smorgasbord of features for statistical machinetranslation.
In Meeting of the North American chap-ter of the Association for Computational Linguistics.Franz Josef Och.
2003.
Minimum Error Rate Train-ing for Statistical Machine Translation.
In Proceed-ings of the 41st Annual Conference of the Associa-tion for Computational Linguistics, pages 160?167,Sapporo, Japan.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceed-ings of the 40th Annual Meeting of the Associa-tion for Computational Linguistics, pages 311?318,Philadelphia, PA.Antti-Veikko Rosti, Spyros Matsoukas, and RichardSchwartz.
2007.
Improved word-level system com-bination for machine translation.
In Proceedingsof the 45th Annual Meeting of the Association ofComputational Linguistics, pages 312?319, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,and Cynthia Rudin.
2008.
Arabic MorphologicalTagging, Diacritization, and Lemmatization UsingLexeme Models and Feature Ranking.
In Proceed-ings of ACL-08: HLT, Short Papers, pages 117?120,Columbus, Ohio.Fatiha Sadat and Nizar Habash.
2006.
Combinationof Arabic preprocessing schemes for statistical ma-777chine translation.
In Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 1?8, Sydney, Aus-tralia, July.
Association for Computational Linguis-tics.Hassan Sajjad, Kareem Darwish, and Yonatan Be-linkov.
2013.
Translating dialectal arabic to en-glish.
In The 51st Annual Meeting of the Associationfor Computational Linguistics - Short Papers (ACLShort Papers 2013), Sofia, Bulgaria.Wael Salloum and Nizar Habash.
2011.
Dialectal toStandard Arabic Paraphrasing to Improve Arabic-English Statistical Machine Translation.
In Pro-ceedings of the First Workshop on Algorithms andResources for Modelling of Dialects and LanguageVarieties, pages 10?21, Edinburgh, Scotland.Wael Salloum and Nizar Habash.
2013.
DialectalArabic to English Machine Translation: Pivotingthrough Modern Standard Arabic.
In Proceedings ofthe 2013 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies (NAACL-HLT), At-lanta, GA.Hassan Sawaf.
2010.
Arabic dialect handling in hybridmachine translation.
In Proceedings of the Confer-ence of the Association for Machine Translation inthe Americas (AMTA), Denver, Colorado.Andreas Stolcke.
2002.
SRILM an Extensible Lan-guage Modeling Toolkit.
In Proceedings of the In-ternational Conference on Spoken Language Pro-cessing.Daguang Xu, Yuan Cao, and Damianos Karakos.
2011.Description of the jhu system combination schemefor wmt 2011.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, pages 171?176.Association for Computational Linguistics.Omar F Zaidan and Chris Callison-Burch.
2011.
Thearabic online commentary dataset: an annotateddataset of informal arabic with high dialectal con-tent.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics(ACL-11), pages 37?41.Rabih Zbib, Erika Malchiodi, Jacob Devlin, DavidStallard, Spyros Matsoukas, Richard Schwartz, JohnMakhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012.
Machine Translation of Arabic Di-alects.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 49?59, Montr?al, Canada, June.
As-sociation for Computational Linguistics.778
