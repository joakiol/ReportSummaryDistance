Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 12?21,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsAutomatic Assessment of the Speech of Young English LearnersJian Cheng1, Yuan Zhao D?Antilio1, Xin Chen1, Jared Bernstein21Knowledge Technologies, Pearson, Menlo Park, California, USA2Tasso Partners LLC, Palo Alto, California, USAjian.cheng@pearson.comAbstractThis paper introduces some of the researchbehind automatic scoring of the speak-ing part of the Arizona English LanguageLearner Assessment, a large-scale test nowoperational for students in Arizona.
Ap-proximately 70% of the students tested arein the range 4-11 years old.
We cover themethods used to assess spoken responsesautomatically, considering both what thestudent says and the way in which the stu-dent speaks.
We also provide evidencefor the validity of machine scores.
Theassessments include 10 open-ended itemtypes.
For 9 of the 10 open item types,machine scoring performed at a similarlevel or better than human scoring at theitem-type level.
At the participant level,correlation coefficients between machineoverall scores and average human overallscores were: Kindergarten: 0.88; Grades1-2: 0.90; Grades 3-5: 0.94; Grades 6-8:0.95; Grades 9-12: 0.93.
The average cor-relation coefficient was 0.92.
We includea note on implementing a detector to catchproblematic test performances.1 IntroductionArizona English Language Learner Assessment(AZELLA) (Arizona Department of Education,2014) is a test administered in the state of Arizonato all students from kindergarten up to grade 12(K-12) who had been previously identified as En-glish learners (ELs).
AZELLA is used to place ELstudents into an appropriate level of instructionaland to reassess EL students on an annual basis tomonitor their progress.
AZELLA was originallya fully human-delivered paper-pencil test cover-ing four domains: listening, speaking, reading andwriting.
The Arizona Department of Educationchose to automate the delivery and scoring of thespeaking parts of the test, and further decided thattest delivery via speakerphone would be the mostefficient and universally accessible mode of ad-ministration.
During the first field test (Nov. 7 -Dec. 2, 2011) over 31,000 tests were administeredto 1st to 12th graders on speakerphones in Arizonaschools.
A second field test in April 2012 deliv-ered over 13,000 AZELLA tests to kindergartenstudents.
This paper reports research results basedon analysis of data sets from the 44,000 studentstested in these two administrations.2 AZELLA speaking testsAZELLA speaking tests are published in fivestages (Table 1), one for each of five grade rangesor student levels.
Each stage has four fixed testforms.
Table 1 presents the total number of fieldtests delivered for each stage, or level.Table 1: Stages, grades, and number of field testsStage I II III IV VGrade K 1-2 3-5 6-8 9-12N 13184 10646 9369 6439 5231Fourteen different speaking exercises (item-types) were included in the various level-specificforms of the test.
Some item-types were accom-panied by images; some only had audio prompts.Note, however, that before the change to automaticadministration and scoring, test forms had only in-cluded speaking item-types from a set of thirteendifferent types, of which ten were not designed toconstrain the spoken responses.
On the contrary,these ten item-types were designed to elicit rela-tively open-ended displays of speaking ability, andmost test forms included one or two items of mosttypes.
A Repeat Sentence item type was addedto the test designs (10 Repeat items per test format every level), yielding test forms with around1227 items total, including Repeats.
Table 2 listsall the speaking item types that are presented inone AZELLA test form for Stage III (Grades 3-5).
Some items such as Questions on Image, Sim-ilarities & Differences, Ask Qs about a Statement,and Detailed Response to Topic are presented as asequence of two related questions and the two re-sponses are human-rated together to produce oneholistic score.Table 2: Stage III (Grades 3-5) items.Descriptions items/test Score-PointsRepeat Sentence 10 0-4Read-by-Syllables 3 0-1Read-Three-Words 3 0-1Questions on Image 3 0-4Similarities & Differences 2 0-4Give Directions from Map 1 0-4Ask Qs about a Statement 1 0-4Give Instructions 1 0-4Open Question on Topic 1 0-4Detailed Response to Topic 1 0-4Table 3: Item types used in AZELLA speakingfield tests.Description (restriction) Score-PointsNaming (Stage I) 0-1Short Response (Stage I) 0-2Open Question (Stage I) 0-2Read-by-Syllables 0-1Read-Three-Words 0-1 or 0-3Repeat Sentence 0-4Questions on Image 0-4Similarities & Differences (III) 0-4Give Directions from Map 0-4Ask Qs about a Thing (II) 0-2Ask Qs about a Statement (III) 0-4Give Instructions 0-4Open Questions on Topic 0-4Detailed Response to Topic 0-4All the speaking item-types used at any levelin the AZELLA field tests are listed in Table3.
Item-types used at only one stage (level) arenoted.
From Table 3 we can see that, except forNaming, Repeat Sentence, Read-by-Syllables, andRead-Three-Words, all the items are fairly uncon-strained questions.
Engineering considerations didnot guide the design of these items to make thembe more suitable for machine learning and auto-matic scoring, and they were, indeed, a challengeto score.By tradition and by design, human scoring ofAZELLA responses is limited to a single holisticscore, guided by sets of Score-Point rubrics defin-ing scores at 2, 3, 4, or 5 levels.
The column Score-Points specifies the number of categories used inholistic scoring.
One set of five abbreviated holis-tic rubrics for assigning points by human rating ispresented below in Table 4.
For the Repeat Sen-tence items only, separate human ratings were col-lected under a pronunciation rubric and a fluencyrubric.Table 4: Example AZELLA abbreviated holisticrubric (5 Score-Points).Points Descriptors4 Correct understandable English us-ing two or more sentences.
1.
Com-plete declarative or interrogative sen-tences.
2.
Grammar (or syntax) er-rors are not evident and do not im-pede communication.
3.
Clear andcorrect pronunciation.
4.
Correctsyntax.3 Understandable English using two ormore sentences.
1.
Complete declar-ative or interrogative sentences.
2.Minor grammatical (or syntax) er-rors.
3.
Clear and correct pronuncia-tion.2 An intelligible English response.
1.Less than two complete declarativeor interrogative sentences.
2.
Errorsin grammar (or syntax).
3.
Attemptto respond with clear and correct pro-nunciation.1 Erroneous responses.
1.
Not com-plete declarative or interrogative sen-tences.
2.
Significant errors in gram-mar (or syntax).
3.
Not clear and cor-rect pronunciation.0 Non-English or silence.3 Development and validation dataFrom the data in the first field test (Stages II, III,IV, V), for each AZELLA Stage, we randomlysampled 300 tests (75 tests/form x 4 forms) as avalidation set and 1, 200 tests as a developmentset.
For the data in the second field test (StageI), we randomly sampled 167 tests from the fourforms as the validation set and 1, 200 tests as the13development set.
No validation data was used formodel training.3.1 Human transcriptions and scoringIn the development sets, we needed from 100 to300 responses per item to be transcribed, depend-ing on the complexity of the item type.
In the val-idation sets, all responses were fully transcribed.Depending on the item type, we got single or dou-ble transcriptions, as necessary.All responses from the tests were scored bytrained professional raters according to predefinedrubrics (Arizona Department of Education, 2012),such as those in Table 4.
Departing from usualpractice in production settings, we used the aver-age score from different raters as the final scoreduring machine learning.
The responses in eachvalidation set were double rated (producing twofinal scores) for use in validation.
Note that five ofthe 1,367 tests in the validation sets had no humantranscriptions and ratings, and so were excludedfrom the final validation results.4 Machine scoring methodsPrevious research on automatic assessment ofspoken responses can be found in Bernstein etal.
(2000; 2010), Cheng (2011) and Higginset al.
(2011).
Past work on automatic assess-ment of children?s oral reading fluency has beenreported at the passage-level (Cheng and Shen,2010; Downey et al., 2011) and at the word-level(Tepperman et al., 2007).
A comprehensive reviewof spoken language technologies for education canbe found in Eskanazi (2009).
The following sub-sections summarize the methods we have used forscoring AZELLA tests.
Those methods with cita-tions have been previously discussed in researchpapers.
Other methods described are novel modi-fications or extensions of known methods.Both the linguistic content and the manner ofspeaking are scored.
Our machine scoring meth-ods include a combination of automatic speechrecognition (ASR), speech processing, statisticalmodeling, linguistics, word vectors, and machinelearning.
The speech processing technology wasbuilt to handle the different rhythms and variedpronunciations used by a range of natives andlearners.
In addition to recognizing the wordsspoken, the system also aligns the speech signal,i.e., it locates the part of the signal containingrelevant segments, syllables, and words, allowingthe system to assign independent scores based onthe content of what is spoken and the manner inwhich it is said.
Thus, we derive scores based onthe words used, as well as the pace, fluency, andpronunciation of those words in phrases and sen-tences.
For each response, base measures are thenderived from the linguistic units (segments, sylla-bles, words), with reference to statistical modelsbuilt from the spoken performances of natives andlearners.
Except for the Repeat items, the systemproduces only one holistic score per item from acombination of base measures.4.1 Acoustic modelsWe tried various sets of recorded responses to trainGMM-HMM acoustic models as implemented inHTK (Young et al., 2000).
Performance im-proved by training acoustic models on larger setsof recordings, including material from studentsout of the age range being tested.
For exam-ple, training acoustic models using only the StageII transcriptions to recognize other Stage II re-sponses was significantly improved by using moredata from outside the Stage II data set, such asother AZELLA field data.
We observed that themore child speech data, the better the automaticscoring.
The final acoustic models used for recog-nition were trained on all transcribed AZELLAfield data, except the data in the validation sets,plus data from an unrelated set of children?s oralreading of passages (Cheng and Shen, 2010), andthe data collected during the construction of theVersant Junior English tests for use by young chil-dren in Asia (Bernstein and Cheng, 2007).
Thus,the acoustic models were built using any and allrelevant data available: totaling about 380 hoursof data (or around 176, 000 responses).
The worderror rate (WER) over all the validation sets usingthe final acoustic models is around 35%.For machine scoring (after recognition andalignment), native acoustic models are used tocompute native likelihoods of producing the ob-served base measures.
Human listeners classifiedstudent recordings from Stage II (grades 1-2) asnative or non-native.
For example, in Stage IIdata, 287 subjects were identified as native and therecordings from these 287 subjects plus the nativerecordings from the Versant Junior English testswere used to build native acoustic models for grad-ing.
(approximately 66 hours of speech data, or39, 000 responses).144.2 Language modelsItem-specific bigram language models were builtusing the human transcription of the development-set as described in Section 3.1.4.3 Content modeling"Content" refers to the linguistic material (words,phrases, and semantic elements) in the spoken re-sponse.
Appropriate response content reflects thespeaker?s productive control of English vocabu-lary and also indicates how well the test-taker un-derstood the prompt.
Previous work on scoringlinguistic content in the speech domain includesBernstein et al.
(2010) and Xie et al.
(2012).Except for the four relatively closed-response-form items (Naming, Repeat, Read-by-Syllablesand Read-Three-Words), we produced aword_vector score for each response (Bern-stein et al., 2010).
The value of the word_vectorscore is calculated by scaling the weighted sum ofthe occurrence of a large set of expected wordsand word sequences available in an item-specificresponse scoring model.
An automatic processassigned weights to the expected words and wordsequences according to their semantic relation toknown good responses using a method similar tolatent semantic analysis (Landauer et al., 1998).The word_vector score is generally the mostpowerful feature used to predict the final humanscores.Note that a recent competition to develop accu-rate scoring algorithms for student-written short-answer responses (Kaggle, 2012) focused on asimilar problem to the content scoring task forAZELLA open-ended responses.
We assume thatthe methods used by the prize-winning teams,for example Tandalla (2012) and Zbontar (2012),should work well for the AZELLA open-endedmaterial too, although we did not try these meth-ods.For the responses to Naming, Read-by-Syllables, and Read-Three-Words items, the ma-chine scoring makes binary decisions based on theoccurrence of a correct sequence of syllables orwords (keywords).
In Stage II forms, for firstand second grade students, the responses to Read-Three-Words items were human-rated in four cat-egories.
For this stage, the machine counted thenumber of words read correctly.For the responses to Repeat items, the recog-nized string is compared to the word string re-cited in the prompt, and the number of word er-rors (word_errors) is calculated as the minimumnumber of substitutions, deletions, and/or inser-tions required to find a best string match in theresponse.
This matching algorithm ignores hes-itations and filled or unfilled pauses, as well asany leading or trailing material in the response(Bernstein et al., 2010).
A verbatim repetitionwould have zero word errors.
For Repeat re-sponses, the percentage of words repeated cor-rectly (percent_correct) was used as an addi-tional feature.4.4 Duration modelingPhone-level duration statistics contribute to ma-chine scores of test-takers?
pronunciation and flu-ency.
Native-speakers segment duration statis-tics from Versant Junior English tests (Bernsteinand Cheng, 2007) were used to compute thelog-likelihood of phone durations produced bytest-takers.
No data from AZELLA tests con-tributed to the duration models.
We calculated thephoneme duration log-likelihood: log_seg_proband the inter-word silence duration log-likelihood:iw_log_seg_prob (Cheng, 2011).Assume in a recognized response that the se-quence of phonemes and their corresponding du-rations are piand Di, i = 1...N , then thelog likelihood segmental probability for phonemes(log_seg_prob) was computed as:log_seg_prob =1N ?
2N?1?i=2log(Pr(Di)), (1)where Pr(Di) was the probability that a nativewould produce phoneme piwith the observed du-ration Diin the context found.
The first and lastphonemes in the response were not used for thecalculation of the log_seg_prob because durationsof these phonemes as determined by the ASR weremore likely to be incorrect.
The log likelihoodsegmental probability for inter-word silence dura-tions, iw_log_seg_prob, was calculated the sameway (Cheng, 2011).4.5 Spectral modelingTo construct scoring models for pronunciationand fluency, we computed several spectral likeli-hood features with reference to native and learnersegment-specific models applied to the recogni-tion alignment, computing the phone-level poste-rior probabilities given the acoustic observation X15that is recognized as pi:P (pi|X) =P (X|pi)P (pi)?mk=1P (X|pk)P (pk)(2)where k runs over all the potential phonemes.
Ina real-world ASR system, it is extremely difficultto estimate?mk=1P (X|pk)P (pk) precisely.
Soapproximations are used, such as substituting amaximum for the summation, etc.
Formula 2 isthe general framework for pronunciation diagno-sis (Witt and Young, 1997; Franco et al., 1999;Witt and Young, 2000) and pronunciation assess-ment (Witt and Young, 2000; Franco et al., 1997;Neumeyer et al., 1999; Bernstein et al., 2010).Various authors use different approximations tosuit the particulars of their data and their applica-tions.In the AZELLA spectral scoring, we approx-imated Formula 2 with the following procedure.After the learner acoustic models produce a recog-nition result, we force-align the utterance on therecognized word string, but using the native mono-phone acoustic models, producing acoustic log-likelihood, duration and time boundaries for ev-ery phone.
For each such phone, again using thenative monophone time alignment, we performan all-phone recognition using the native mono-phone acoustic models.
The recognizer calculatesa log-likelihood for every phone and picks thebest match from all possible phones over that timeframe.
For each phone-of-interest in a response,we calculated the average spectral score differenceas:spectral_1 =1NN?i=1lpfai?
lpapidi(3)where the variables are:?
lpfaiis the log-likelihood corresponding tothe i-th phoneme by using the forced align-ment method;?
lpapiis the log-likelihood by using the all-phone recognition method;?
diis its duration;?
N is the number of phonemes of interest in aresponse.In calculating spectral_1, all possiblephonemes are included.
We define anothervariable, spectral_2, that only accumulatesthe log-likelihood for a target set of phonemesthat learners often have difficulty with.
We callthe percentage of phones from the all-phonerecognition that match the phones from the forcedalignment the percent phone match, or ppm.We take Formula 3 as the average log of theapproximate posterior probabilities that phoneswere produced by a native.4.6 Confidence modelingAfter finishing speech recognition, we can assignspeech confidence scores to words and phonemes(Cheng and Shen, 2011).
Then for every response,we can compute the average confidence, the per-centage of words or phonemes whose confidencesare lower than a threshold value as features to pre-dict test-takers?
performance.4.7 Final modelsAZELLA holistic score rubrics (Arizona Depart-ment of Education, 2012), such as those shownin Table 4, consider both the answer content andthe manner of speaking used in the response.
Theautomatic scoring should consider both too.
Fea-tures word_vector, keywords, word_errors,percent_correct can represent content scoresbased on what is spoken.
Features log_seg_prob,iw_log_seg_prob, spectral_1, spectral_2, ppmcan represent both the rhythmic and segmental as-pects of the performance as native likelihoods ofproducing the observed base measures.
By feed-ing these features to models, we can effectivelypredict human holistic scores, as well as humanpronunciation and fluency ratings, although we didnot model grammar errors in the way they arespecifically described in the rubrics, e.g.
in Table4.For each item, a specific combination of basescores was selected.
So, on an item-by-item basis,we tried two methods of combination: (i) multiplelinear regression and (ii) neural networks with onehidden layer trained by back propagation.
Thenwe selected the one that was more accurate forthat item.
For almost all items, the neural networkmodel worked better.4.8 Unscorable test detectionMany factors can render a test unscorable: poorsound quality (recording noise, mouth too closeto the microphone, too soft, etc.
), gibberish (non-sense words, noise, or a foreign language), off-topic (off topic, but intelligible English), unintelli-gible English (e.g.
a good-faith attempt to respond16in English, but is so unintelligible and/or disfluentthat it cannot be understood confidently).There have been several approaches to dealingwith this issue (Cheng and Shen, 2011; Chen andMostow, 2011; Yoon et al., 2011).
Some un-scorable tests can be identified easily by a hu-man listener, and we reported research on a speci-fied unscorable category (off-topic) before (Chengand Shen, 2011).
Dealing with a specified cat-egory could be significantly easier than dealingwith wide-open items as in AZELLA.
Also, be-cause we did not collect human ?unscorable" rat-ings for this data, we worked on predicting the ab-solute overall difference between human and ma-chine scores; which is like predicting outliers.
Ifthe difference is expected to exceed a threshold,the test should be sent for human grading.Many problems were due to low volume record-ings made by shy kids, so we identified features todeal with low-volume tests.
These included max-imum energy, the number of frames with funda-mental frequency, etc., using many features men-tioned in Cheng and Shen (2011).
The methodused to detect off-topic responses did not workwell here, but features based on lattice confidenceseemed to work fairly well.
If we define an un-scorable test as one with an overall difference be-tween human and machine scores greater than orequal to 3 (within the score range 0-14), our finalunscorable test detector achieves an equal-errorrate of 16.5% in validation sets; or when fixing thefalse rejection rate at 6%, the false acceptance rateis 44%.
We are actively investigating better meth-ods to achieve acceptable performance for use inreal tests.5 Experimental resultsAll results presented in this section used the vali-dation data sets, while the recognition and scoringmodels were built from completely separate mate-rial.
The participant-level speaking scores weredesigned not to consider the scores from Read-by-Syllables and Read-Three-Words.
For eachtest, the system produced holistic scores for Re-peat items and for non-Repeat items.
For everyRepeat item, the machine generated pronuncia-tion, fluency and accuracy scores mapped into the0 to 4 score-point range.
Both human and machineholistic scores for a Repeat response are equalto: 50% ?
Accuracy + 25% ?
Pronunciation +25% ?
Fluency.
Accuracy scores were scaledas percent_correct times four.
Human accuracyscores were based on human transcriptions insteadof ASR transcriptions.
Holistic scores for Repeatitems at the participant level were the simple aver-age of the corresponding item-level scores.For every non-Repeat item, we generated oneholistic score that considered pronunciation, flu-ency and content together.
The non-Repeat holis-tic scores at the participant level were the sim-ple average of the corresponding item level scoresafter normalizing them to the same scale.
Thefinal generated holistic scores for Repeats werescaled to a 0 ?
4 range and non-Repeat holis-tic scores were scaled to a 0 ?
10 range to sat-isfy an AZELLA design requirement that Repeatitems count for 4 points and non-Repeats countfor 10 points.
The overall participant level scoresare the sum of the Repeat holistic scores and thenon-Repeat holistic scores (maximum 14).
Allmachine-generated scores are continuous values.In the following tables, H-H r stands for thehuman-human correlation and M-H r stands forthe correlation between machine-generated scoresand average human scores.Table 5: Human rating reliabilities and Machine-human correlations by item type.
Third columngives mean and standard deviation of words perresponse.S Item typesWords/responseH-H r M-H r??
?I Naming 2.5?
2.5 0.83 0.67I Short Response 5.7?
3.8 0.71 0.73I Open Question 8.7?
7.9 0.70 0.76I Repeat Sentence 5.0?
2.5 0.91 0.83II Questions on Image 14.0?
10.8 0.87 0.86II Give Directions from Map 10.9?
9.7 0.82 0.84II Ask Qs about a Thing 6.8?
5.9 0.83 0.64II Open Question on Topic 11.6?
10.6 0.75 0.72II Give Instructions 11.5?
10.0 0.83 0.80II Repeat Sentence 6.1?
2.9 0.95 0.85III Questions on Image 14.5?
10.2 0.87 0.77III Similarities & Differences 19.5?
11.6 0.75 0.75III Give Directions from Map 16.3?
11.2 0.74 0.85III Ask Qs about a Statement 16.7?
13.4 0.79 0.82III Give Instructions 17.0?
12.8 0.77 0.81III Open Question on Topic 13.9?
11.1 0.85 0.85III Detailed Response to Topic 13.8?
10.5 0.81 0.80III Repeat Sentence 6.4?
3.2 0.97 0.88IV Questions on Image 13.9?
11.8 0.84 0.84IV Give Directions from Map 13.7?
13.3 0.84 0.90IV Open Question on Topic 17.2?
15.2 0.82 0.82IV Detailed Response to Topic 13.9?
11.4 0.85 0.87IV Give Instructions 16.5?
15.7 0.87 0.90IV Repeat Sentence 6.9?
3.2 0.96 0.89V Questions on Image 17.3?
12.0 0.80 0.76V Open Question on Topic 18.7?
14.9 0.84 0.82V Detailed Response to Topic 17.7?
15.2 0.88 0.87V Give Instructions 17.2?
16.6 0.90 0.90V Give Directions from Map 22.4?
16.8 0.86 0.85V Repeat Sentence 6.4?
3.5 0.95 0.8917We summarize the psychometric properties ofdifferent item types that contribute to the finalscores in Table 5.
For each item-type and eachstage, the third column in Table 5 presents themean and standard deviation of the words-per-response produced by students, showing that olderstudents generally produce more spoken material.We found that the number of words spoken is abetter measure than speech signal duration to rep-resent the amount of material produced, becauseyoung English learners often emit long silenceswhile speaking.
The difference between the twomeasures in columns 4 and 5 is statistically signif-icant (two-tailed, p < 0.05) for item types Nam-ing (Stage I), Ask Qs about a Thing (Stage II),Questions on Image (Stage III), and Repeat Sen-tence (all Stages), in which machine scoring doesnot match human; and for item types Give Direc-tions from Map (Stage III, IV), in which machineis better than a single human score.
For almostall open-ended items, machine scoring is similarto or better than human scoring.
We noticed thatmachine scoring of one open-ended item type, AskQs about a Thing used in Stage II test forms, wassignificantly worse than human scoring, leading usto identify problems specific to the item type itself,both in the human rating rubric and in the machinegrading approach.
Arizona is not using this itemtype in operational tests.Figures 1, 2, 3, 4, 5 present scatter plots of over-all scores at the participant level comparing hu-man and machine scores for test in each AZELLAstage.
Figure 6 shows the averaged human holisticscore distribution for participants in the validationset for Stage V. The human holistic score distribu-tions for participants in other AZELLA stages aresimilar to those in Figure 6, except the means shiftsomewhat.We identified several participants for whom thedifference between human and machine scores isbigger than 4 in Figures 1, 2, 3, 4, 5.
Listen-ing to the recordings of these tests, we concludedthat the most important factor was low Signal-to-Noise Ratio (SNR).
Either the background noisewas very high (in 6 of 1,362 tests in the validationset), or speech volume was low (in 3 of 1,362 testsin the validation set).
Either condition can makerecognition difficult.
With very low voice ampli-tude and high background noise levels, the SNR ofsome outlier response recordings is so low that hu-man raters refuse to affirm that they understand theFigure 1: Overall human vs. machine scores at theparticipant level for Stage I (Grade K).
Mean andstandard deviation for human scores: (8.74, 3.1).Figure 2: Overall human vs. machine scores atthe participant level for Stage II (Grades 1-2).Mean and standard deviation for human scores:(7.1, 2.5).Figure 3: Overall human vs. machine scores atthe participant level for Stage III (Grades 3-5).Mean and standard deviation for human scores:(9.6, 2.3).18Figure 4: Overall human vs. machine scores atthe participant level for Stage IV (Grades 6-8).Mean and standard deviation for human scores:(8.3, 2.9).Figure 5: Overall human vs. machine scores atthe participant level for Stage V (Grades 9-12).Mean and standard deviation for human scores:(8.9, 2.9).Figure 6: Distribution of average human holisticscore for participants in the validation set for StageV (Grades 9-12).content of the response or rate its pronunciation.Since many young children in kindergarten andearly elementary school speak softly, the youngestchildren?s speech is substantially harder to recog-nize (Li and Russell, 2002; Lee et al., 1999).
Thisprobably contributes to the lower reliabilities inStage I and II.
When setting the total rejection rateat 6%, our unscorable test detector identifies only7 of the 13 outlier tests.Table 6: Reliability of human scores and Human-Machine correlations of overall test scores bystage.Stage H-H r M-H rI 0.91 0.88II 0.96 0.90III 0.97 0.94IV 0.98 0.95V 0.98 0.93Average 0.96 0.92Table 6 summarizes the reliabilities of the testsin different stages.
At the participant level, the av-erage inter-rater reliability coefficient across thefive stages was 0.96, suggesting that the well-trained human raters agree with each other withhigh consistency when ratings are combined overall the material in all the responses in a wholetest; the average correlation coefficient betweenmachine-generated overall scores and average hu-man overall scores was 0.92.
This suggests thatthe machine grading may be sufficiently reliablefor most purposes.Table 7: Test reliability by stage, separating non-Repeat holistic scores and Repeat holistic scores.StageH-H r M-H r H-H r M-H rNonRptH NonRptH RptH RptHI 0.85 0.83 0.99 0.94II 0.93 0.89 0.99 0.90III 0.95 0.92 0.99 0.92IV 0.96 0.95 0.99 0.94V 0.96 0.91 0.99 0.93Average 0.93 0.90 0.99 0.93Table 7 summarizes the reliabilities of testscores in the different stages considering the non-Repeat holistic scores and Repeat holistic scoresseparately to check the effect of adding the Re-peat items.
Repeat items improve the machine re-19liability in Stage I significantly, but not so muchfor other stages.
This difference may relate to thedifficulty in eliciting sufficient speech samples innon-Repeat items from the young EL students inStage I. Eliciting spoken materials in Repeat itemsis more straightforward.
Consideration of Table7 suggests that using only open-ended item-typescan also achieve sufficiently reliable results.6 Discussion and future workWe believe that we can improve this system fur-ther by scoring Repeat items using a partial creditRasch model (Masters, 1982) instead of the av-erage of percent_correct, which should improvethe reliability of the Repeat item type.
We mayalso be able to train a better native acoustic modelby using a larger sample of native data fromAZELLA, if we are given access to the test-takerdemographic information.The original item selection and assignment ofitems to forms was quite simple and had room forimprovement.
Currently in the AZELLA testingprogram, test forms go through a post-pilot re-vision, so that the operational tests only includegood items in the final test forms.
This post-pilotselection and arrangement of items into formsshould improve human-machine correlations be-yond the values reported here.
If we effectivelyaddress the problem of shy-kids-talking-softly, thescoring performance will definitely improve evenmore.
Getting young students to talk louder isprobably something that can be best done at thetesting site (by instruction or by example); andit may solve several problems.
We are happyto report that the first operational AZELLA testwith automatic speech scoring took place betweenJanuary 14 and February 26, 2013, with approxi-mately 140, 700 tests delivered.Recent progress in machine learning has ap-plied deep neural networks (DNNs) to manylong-standing pattern recognition and classifica-tion problems.
Many groups have now appliedDNNs to the task of building better acoustic mod-els for speech recognition (Hinton et al., 2012).DNNs have repeatedly been shown to work betterthan Gaussian mixture models (GMMs) for ASRacoustic modeling (Hinton et al., 2012; Dahl et al.,2012).
We are actively exploring the use of DNNsfor use in recognition of children?s speech.
Weexpect that DNN acoustic models can overcomesome of the recognition difficulties mentioned inthis paper (e.g.
low SNR in responses and shortresponse item types like Naming) and boost the fi-nal assessment accuracy significantly.7 ConclusionsWe have reported an evaluation of the automaticmethods that are currently used to assess spo-ken responses to test tasks that occur in Ari-zona?s AZELLA test for young English learners.The methods score both the content of the re-sponses and the quality of the speech producedin the responses.
Although most of the speak-ing item types in the AZELLA tests are uncon-strained and open-ended, machine scoring accu-racy is similar to or better than human scoring formost item types.
We presented basic validity evi-dence for machine-generated scores, including anaverage correlation coefficient between machine-generated overall scores and human overall scoresderived from subscores that are based on multi-ple human ratings.
Further, we described the de-sign, implementation and evaluation of a detec-tor to catch problematic, unscorable tests.
We be-lieve that near-term re-optimization of some scor-ing process elements may further improve ma-chine scoring accuracy.ReferencesArizona Department of Education.
2012.AZELLA update.
http://www.azed.gov/standards-development-assessment/files/2012/12/12-12-12-update-v5.pdf.
[Accessed 19-March-2014].Arizona Department of Education.
2014.
ArizonaEnglish Language Learner Assessment (AZELLA).http://www.azed.gov/standards-development-assessment/arizona-english-language-learner-assessment-azella.
[Accessed 19-March-2014].J.
Bernstein and J. Cheng.
2007.
Logic and valida-tion of a fully automatic spoken English test.
InV.
M. Holland and F. P. Fisher, editors, The Pathof Speech Technologies in Computer Assisted Lan-guage Learning, pages 174?194.
Routledge, NewYork.J.
Bernstein, J.
De Jong, D. Pisoni, and B. Townshend.2000.
Two experiments on automatic scoring of spo-ken language proficiency.
In Proc.
of STIL (Integrat-ing Speech Technology in Learning), pages 57?61.J.
Bernstein, A.
Van Moere, and J. Cheng.
2010.
Vali-dating automated speaking tests.
Language Testing,27(3):355?377.20W.
Chen and J. Mostow.
2011.
A tale of two tasks: De-tecting children?s off-task speech in a reading tutor.In Interspeech 2011, pages 1621?1624.J.
Cheng and J. Shen.
2010.
Towards accurate recogni-tion for children?s oral reading fluency.
In IEEE-SLT2010, pages 91?96.J.
Cheng and J. Shen.
2011.
Off-topic detection inautomated speech assessment applications.
In Inter-speech 2011, pages 1597?1600.J.
Cheng.
2011.
Automatic assessment of prosodyin high-stakes English tests.
In Interspeech 2011,pages 1589?1592.G.
Dahl, D. Yu, L. Deng, and A. Acero.
2012.Context-dependent pretrained deep neural networksfor large vocabulary speech recognition.
IEEETransactions on Speech and Audio Processing, Spe-cial Issue on Deep Learning for Speech and Lang.Processing, 20(1):30?42.R.
Downey, D. Rubin, J. Cheng, and J. Bernstein.2011.
Performance of automated scoring for chil-dren?s oral reading.
In Proceedings of the SixthWorkshop on Innovative Use of NLP for BuildingEducational Applications, pages 46?55.M.
Eskanazi.
2009.
An overview of spoken languagetechnology for education.
Speech Communication,51:832?844.H.
Franco, L. Neumeyer, Y. Kim, and O. Ronen.
1997.Automatic pronunciation scoring for language in-struction.
In ICASSP 1997, pages 1471?1474.H.
Franco, L. Neumeyer, M. Ramos, and H. Bratt.1999.
Automatic detection of phone-level mispro-nunciation for language learning.
In Eurospeech1999, pages 851?854.D.
Higgins, X. Xi, K. Zechner, and D. Williamson.2011.
A three-stage approach to the automated scor-ing of spontaneous spoken responses.
ComputerSpeech and Language, 25:282?306.G.
Hinton, L. Deng, Y. Dong, G. Dahl, A. Mohamed,N.
Jaitly, A.
Senior, V. Vanhoucke, P. Nguyen,T.
Sainath, and B. Kingsbury.
2012.
Deep neu-ral networks for acoustic modeling in speech recog-nition: The shared views of four research groups.IEEE Signal Processing Magazine, 29(6):82?97.Kaggle.
2012.
The Hewlett Foundation: Shortanswer scoring.
http://www.kaggle.com/c/asap-sas;http://www.kaggle.com/c/asap-sas/details/winners.
[Accessed20-April-2014].T.
K. Landauer, P. W. Foltz, and D. Laham.
1998.Introduction to latent semantic analysis.
DiscourseProcesses, 25:259?284.S.
Lee, A. Potamianos, and S. Narayanan.
1999.Acoustics of children?s speech: developmentalchanges of temporal and spectral parameters.
Jour-nal of Acoustics Society of American, 105:1455?1468.Q.
Li and M. Russell.
2002.
An analysis of the causesof increased error rates in children?s speech recogni-tion.
In ICSLP 2002, pages 2337?2340.G.
N. Masters.
1982.
A Rasch model for partial creditscoring.
Psychometrika, 47(2):149?174.L.
Neumeyer, H. Franco, V. Digalakis, and M. Wein-traub.
1999.
Automatic scoring of pronunciationquality.
Speech Communication, 30:83?93.L.
Tandalla.
2012.
ASAP Short AnswerScoring Competition System Description:Scoring short answer essays.
https://kaggle2.blob.core.windows.net/competitions/kaggle/2959/media/TechnicalMethodsPaper.pdf.
[Accessed20-April-2014].J.
Tepperman, M. Black, P. Price, S. Lee,A.
Kazemzadeh, M. Gerosa, M. Heritage, A. Al-wan, and S. Narayanan.
2007.
A Bayesian networkclassifier for word-level reading assessment.
InInterspeech 2007, pages 2185?2188.S.
M. Witt and S. J.
Young.
1997.
Language learn-ing based on non-native speech recognition.
In Eu-rospeech 1997, pages 633?636.S.
M. Witt and S. J.
Young.
2000.
Phone-level pro-nunciation scoring and assessment for interactivelanguage learning.
Speech Communication, 30:95?108.S.
Xie, K. Evanini, and K. Zechner.
2012.
Explor-ing content features for automated speech scoring.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 103?111.S.-Y.
Yoon, K. Evanini, and K. Zechner.
2011.
Non-scorable response detection for automated speakingproficiency assessment.
In Proceedings of the SixthWorkshop on Innovative Use of NLP for BuildingEducational Applications, pages 152?160.S.
J.
Young, D. Kershaw, J. Odell, D. Ollason,V.
Valtchev, and P. Woodland.
2000.
The HTKBook Version 3.0.
Cambridge University, Cam-bridge, England.J.
Zbontar.
2012.
ASAP Short Answer Scoring Com-petition System Description: Short answer scoringby stacking.
https://kaggle2.blob.core.windows.net/competitions/kaggle/2959/media/jzbontar.pdf.
[Accessed20-April-2014].21
