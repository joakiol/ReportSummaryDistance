Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 352?357,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsAutomation and Evaluation of the Keyword Methodfor Second Language LearningG?ozde?OzbalTrento RISETrento, Italygozbalde@gmail.comDaniele PighinGoogleZ?urich, Switzerlandbiondo@google.comCarlo StrapparavaFBK-irstTrento, Italystrappa@fbk.euAbstractIn this paper, we combine existingNLP techniques with minimal supervi-sion to build memory tips according tothe keyword method, a well establishedmnemonic device for second languagelearning.
We present what we believe to bethe first extrinsic evaluation of a creativesentence generator on a vocabulary learn-ing task.
The results demonstrate that NLPtechniques can effectively support the de-velopment of resources for second lan-guage learning.1 IntroductionThe keyword method is a mnemonic device (Co-hen, 1987; Thompson, 1987) that is especiallysuitable for vocabulary acquisition in second lan-guage learning (Mizumoto and Kansai, 2009;Hummel, 2010; Shen, 2010; Tavakoli and Gerami,2013).
In this method, a target word in a foreignlanguage L2 can be learned by a native speaker ofanother language L1 in two main steps: 1) one ormore L1 words, possibly referring to a concreteentity, are chosen based on orthographic or pho-netic similarity with the target word; 2) an L1 sen-tence is constructed in which an association be-tween the translation of the target word and thekeyword(s) is established, so that the learner, whenseeing or hearing the word, immediately recallsthe keyword(s).
To illustrate, for teaching the Ital-ian word cuore which means heart in English, thelearner might be asked to imagine ?a lonely heartwith a hard core?.The keyword method has already been provento be a valuable teaching device.
However, thepreparation of the memorization tips for each newword is an activity that requires considerable time,linguistic competence and creativity.
To the bestof our knowledge, there is only one study whichattempts to automate the mechanism of the key-word method.
In (?Ozbal and Strapparava, 2011),we proposed to automate the keyword method byretrieving sentences from the Web.
However, wedid not provide any evaluation to demonstrate theeffectiveness of our approach in a real life sce-nario.
In addition, we observed that retrieval posessevere limitations in terms of recall and sentencequality, and it might incur copyright violations.In this paper, we overcome these limitations byintroducing a semi-automatic system implement-ing the keyword method that builds upon the key-word selection mechanism of?Ozbal and Strappar-ava (2011) and combines it with a state-of-the-artcreative sentence generation framework (?Ozbal etal., 2013).
We set up an experiment to simulatethe situation in which a teacher needs to preparematerial for a vocabulary teaching resource.
Ac-cording to our scenario, the teacher relies on au-tomatic techniques to generate relatively few, highquality mnemonics in English to teach Italian vo-cabulary.
She only applies a very light supervi-sion in the last step of the process, in which themost suitable among the generated sentences areselected before being presented to the learners.
Inthis stage, the teacher may want to consider factorswhich are not yet in reach of automatic linguisticprocessors, such as the evocativeness or the mem-orability of a sentence.
We show that the automat-ically generated sentences help learners to estab-lish memorable connections which augment theirability to assimilate new vocabulary.
To the best ofour knowledge, this work is the first documentedextrinsic evaluation of a creative sentence genera-tor on a real-world application.2 Related workThe effectiveness of the keyword method (KM)is a well-established fact (Sar?c?oban and Bas??bek,2012).
Sommer and Gruneberg (2002) found thatusing KM to teach French made learning easierand faster than conventional methods.
Sagarraand Alba (2006) compared the effectiveness of352three learning methods including the semanticmapping, rote memorization (i.e., memorizationby pure repetition, with no mnemonic aid) andkeyword on beginner learners of a second lan-guage.
Their results show that using KM leadsto better learning of second language vocabularyfor beginners.
Similar results have been reportedby Sar?c?oban and Bas?
?bek (2012) and Tavakoliand Gerami (2013).
Besides all the experimentalresults demonstrating the effectiveness of KM, itis worthwhile to mention about the computationalefforts to automate the mechanism.
In (?Ozbal andStrapparava, 2011) we proposed an automatic vo-cabulary teaching system which combines NLPand IR techniques to automatically generate mem-ory tips for vocabulary acquisition.
The systemexploits orthographic and phonetic similarity met-rics to find the best L2 keywords for each target L1word.
Sentences containing the keywords and thetranslation of the target word are retrieved fromthe Web, but we did not carry out an evaluationof the quality or the coverage of the retrieved sen-tences.
In?Ozbal et al (2013) we proposed an ex-tensible framework for the generation of creativesentences in which users are able to force sev-eral words to appear in the sentences.
While wehad discussed the potentiality of creative sentencegeneration as a useful teaching device, we had notvalidated our claim experimentally yet.
As a previ-ous attempt at using NLP for education, Manurunget al (2008) employ a riddle generator to createa language playground for children with complexcommunication needs.3 Memory tip generationPreparing memory tips based on KM includes twomain ingredients: one or more keywords which areorthographically or phonetically similar to the L2word to be learned; and a sentence in which thekeywords and the translation of the target L2 wordare combined in a meaningful way.
In this section,we detail the process that we employed to generatesuch memory tips semi-automatically.3.1 Target word selection and keywordgenerationWe started by compiling a collection of Ital-ian nouns consisting of three syllables from var-ious resources for vocabulary teaching includ-ing http://didattica.org/italiano.htm and http://ielanguages.com, andproduced a list of 185 target L2 words.
To gen-erate the L1 keywords for each target word, weadopted a similar strategy to?Ozbal and Strappa-rava (2011).
For each L2 target word t, the key-word selection module generates a list of possi-ble keyword pairs, K. A keyword pair k ?
Kcan either consist of two non-empty strings, i.e.,k = [w0, w1], or of one non-empty and one emptystring, i.e., w1= .
Each keyword pair has theproperty that the concatenation of its elements iseither orthographically or phonetically similar tothe target word t. Orthographic and phonetic sim-ilarity are evaluated by means of the Levenshteindistance (Levenshtein, 1966).
For orthographicsimilarity, the distance is calculated over the char-acters in the words, while for phonetic similarityit is calculated over the phonetic representationsof t and w0+ w1.
We use the CMU pronuncia-tion dictionary1to retrieve the phonetic represen-tation of English words.
For Italian words, instead,their phonetic representation is obtained from anunpublished phonetic lexicon developed at FBK-irst.3.2 Keyword filtering and rankingUnlike in (?Ozbal and Strapparava, 2011), wherewe did not enforce any constraints for selectingthe keywords, in this case we applied a more so-phisticated filtering and ranking strategy.
We re-quire at least one keyword in each pair to be acontent word; then, we require that at least onekeyword has length ?
3; finally, we discard pairscontaining at least one proper noun.
We allowedthe keyword generation module to consider all theentries in the CMU dictionary, and rank the key-word pairs based on the following criteria in de-creasing order of precedence: 1) Keywords witha smaller orthographic/phonetic distance are pre-ferred; 2) Keywords consisting of a single wordare preferred over two words (e.g., for the targetword lavagna, which means blackboard, lasagnatakes precedence over love and onion); 3) Key-words that do not contain stop words are preferred(e.g., for the target word pettine, which meanscomb, the keyword pair pet and inn is rankedhigher than pet and in, since in is a stop word); 4)Keyword pairs obtained with orthographic similar-ity are preferred over those obtained with phoneticsimilarity, as learners might be unfamiliar with thephonetic rules of the target language.
For example,for the target word forbice, which means scissors,1http://www.speech.cs.cmu.edu/cgi-bin/cmudict353Group Target SentenceA1 campagna a company runs the countryA1 isola an island of remote isolated communitiesA1 fabbrica a fabric worker in a factoryA1 bagnino lifeguards carry no bagA1 inverno the inferno started, winter leftA1 cielo the sky has no ceilingA1 marrone blood and marrow in a brown waterA1 cuore the lonely heart has hard coreA1 coperta a piece of copper in the corner of a blanketA1 locanda an inn oak door with lock and keyA2 piazza a square building serves a free pizzaA2 calzino big bloke with sock in the casinoA2 scatola a cardboard box sat in a scuttle of a houseA2 ragazzo boys also have rag dollsA2 angolo a corner kick came at an angleA2 cestino a teen movie uses basket to play the chessA2 carbone the coal is the form of carbonA2 cassetto a blank cassette tape is in a drawerA2 farfalla the butterflies are far in the fallA2 tovaglia a damp cloth towelB1 duomo the old cathedral has a domeB1 aceto a vinegar sauce contains the acidB1 nuvola the sophisticated novel depicts the cloudB1 chiesa the Catholic church has Swiss cheeseB1 bacino the explosion in the back broke the pelvisB1 maiale a pork meat comes in the mailB1 minestra Chinese ministries have soupB1 estate this estate is for summerB1 bozzolo a buzz comes wrapped in the cocoonB1 arnese harness a technology to develop a toolB2 asino an Asian elephant is riding a donkeyB2 miele do not make honey to walk a mileB2 polmone crowded pullmans stop the lungsB2 fagiolo a topical facial bean creamB2 fiore a fire in a flower marketB2 compressa the clay tablet is in the compressed formB2 cavallo horse running fast in cavalryB2 fiume the muddy river has smoke and fumesB2 pittore a famous painter has precious picturesB2 manico manic people have broken necksTable 1: Sentences used in the vocabulary acqui-sition experiment.the keyword pair for and bid is preferred to for andbeach.We selected up to three of the highest rankedkeyword pairs for each target word, obtaining 407keyword combinations for the initial 185 Italianwords, which we used as the input for the sentencegenerator.3.3 Sentence generationIn this step, our goal was to generate, for each Ital-ian word, sentences containing its L1 translationand the set of orthographically (or phonetically)similar keywords that we previously selected.
Foreach keyword combination, starting from the top-ranked ones, we generated up to 10 sentences byallowing any known part-of-speech for the key-words.
The sentences were produced by the stateof the art sentence generator of?Ozbal et al (2013).The system relies on two corpora of automaticparses as a repository of sentence templates andlexical statistics.
As for the former, we combinedtwo resources: a corpus of 16,000 proverbs (Mi-halcea and Strapparava, 2006) and a collection of5,000 image captions2collected by Rashtchian etal.
(2010).
We chose these two collections sincethey offer a combination of catchy or simple sen-tences that we expect to be especially suitablefor second language learning.
As for the sec-ond corpus, we used LDC?s English GigaWord 5thEdition3.
Of the 12 feature functions describedin (?Ozbal et al, 2013), we only implemented thefollowing scorers: Variety (to prevent duplicatewords from appearing in the sentences); Seman-tic Cohesion (to enforce the generation of sentenceas lexically related to the target words as possi-ble); Alliteration, Rhyme and Plosive (to intro-duce hooks to echoic memory in the output); De-pendency Operator andN -gram (to enforce outputgrammaticality).We observed that the sentence generation mod-ule was not able to generate a sentence for 24%of the input configurations.
For comparison, whenwe attempted to retrieve sentences from the Webas suggested in?Ozbal and Strapparava (2011), wecould collect an output for less than 10% of the in-put configurations.
Besides, many of the retrievedsentences were exceedingly long and complex tobe used in a second language learning experiment.3.4 Sentence selectionFor each L1 keyword pair obtained for each L2target word, we allowed the system to output up to10 sentences.
We manually assessed the quality ofthe generated sentences in terms of meaningful-ness, evocativeness and grammaticality to selectthe most appropriate sentences to be used for thetask.
In addition, for keyword pairs not containingthe empty string, we prioritized the sentences inwhich the keywords were closer to each other.
Forexample, let us assume that we have the keywordscall and in for the target word collina.
Amongthe sentences ?The girl received a call in the bath-room?
and ?Call the blond girl in case you need?,the first one is preferred, since the keywords arecloser to each other.
Furthermore, we gave pri-ority to the sentences that included the keywords2http://vision.cs.uiuc.edu/pascal-sentences/3http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2011T07354in the right order.
To illustrate, for the same key-words and the target words, we would prefer thesentence ?I called him in the morning yesterday?over ?You talk a lot in a call?.Accordingly, for each target word in random or-der, we sequentially scanned the outputs generatedfor each keyword pair.
As soon as a sentence ofadequate quality was found, we added it to ourevaluation data and moved on to the next keyword.We continued this process until we selected a sen-tence for 40 distinct target words, which we setas the target size of the experiment.
We had toinspect the outputs generated for 48 target wordsbefore we were able to select 40 good examples,meaning that for 17% of the target words the sen-tence generator could not produce a sentence ofacceptable quality.4 Experiment setupFor our experiment, we drew inspiration fromSagarra and Alba (2006).
We compared the re-tention error rate of learners who tried to memo-rize new words with or without the aid of the auto-matically generated sentences.
Through academicchannels, we recruited 20 native English speakerswith no prior knowledge of Italian.4After obtaining the sentences as explained inSection 3, we shuffled and then divided the wholeset including 40 target words together with theirtranslation, the generated keywords and sentencesinto 2 batches (A, B) and further divided eachbatch into 2 groups consisting of 10 elements (A1,A2, B1 and B2).
The set of sentences assignedto each group is listed in Table 1: Column ?Tar-get?
reports the Italian target word being taught;Column ?Sentence?
shows the automatically gen-erated sentence, where the translation of the tar-get word is shown in bold and the keyword(s) initalic.
For the experiments, we randomly assignedeach subject to one of the batches (A or B).
Then,each subject was asked to memorize all the wordpairs in a batch, but they would see the memorytips only for one of the two groups, which wasagain randomly assigned.
This approach resultedin 4 different memorization exercises, namely 1)A1 with tips and A2 without, 2) A2 with tips andA1 without, 3) B1 with tips and B2 without, 4) B2with tips and B1 without.4We preferred to select the experiment subjects in personas opposed to crowdsourcing the evaluation to be able to ver-ify the proficiency of the subjects in the two languages and toensure the reliability of the outcome of the evaluation.Error rate (%) ReductionGroup Rote KW ?e%eA1 4.08 3.39 0.69 16.95A2 12.07 10.42 1.65 13.69B1 12.77 10.00 2.77 21.67B2 22.50 12.50 10.00 44.44Macro-average 12.85 9.08 3.78 29.39Micro-average 11.27 8.25 3.02 26.76Table 2: Per-group and overall retention error ratewhen using rote or keyword-aided (KW) memo-rization.When memorizing the translations without theaid of memory tips, the subjects were instructedto focus only on the Italian word and its Englishtranslation and to repeat them over and over intheir mind.
Conversely, when relying on the au-tomatic memory tips the subjects were shown theword, its translation and the generated sentence in-cluding the keywords.
In this case, the subjectswere instructed to read the sentence over and overtrying to visualize it.After going through each set of slides, we dis-tracted the subjects with a short video in order toreset their short term memory.
After that, their re-tention was tested.
For each Italian word in the ex-ercise, they were asked to select the English trans-lation among 5 alternatives, including the correcttranslation and 4 other words randomly selectedfrom the same group.
In this way, the subjectswould always have to choose among the wordsthat they encountered during the exercise.5Wealso added an extra option ?I already knew thisword?
that the subjects were instructed to selectin case they already knew the Italian word prior totaking part in the experiment.5 Experiment resultsTable 2 summarizes the outcome of the experi-ment.
The contribution of the automatically gen-erated sentences to the learning task is assessedin terms of error rate-reduction, which we mea-sure both within each group (rows 1-4) and on thewhole evaluation set (rows 5-6).
Due to the pres-ence of the ?I already knew this word?
option inthe learning-assessment questionnaire, the numberof the actual answers provided by each subject canbe slightly different, hence the difference betweenmacro- and micro-average.5Otherwise, they could easily filter out the wrong answersjust because they were not exposed to them recently.355The error rate for each memorization techniquet (where t = R for ?Rote memorization?
andt = K for ?keyword-aided memorization?)
is cal-culated as: et=itct+it, where ctand itare thenumber of correct and incorrect answers providedby the subjects, respectively.
The absolute errorrate reduction ?e is calculated as the absolute dif-ference in error rate between rote and keyword-aided memorization, i.e.
: ?e= eR?
eK.
Finally,the relative error rate reduction %eis calculated asthe the ratio between the absolute error rate reduc-tion ?e and the error rate of rote memorization eR,i.e.,: %e=?eeR=eR?eKeR.The overall results (rows 5 and 6 in Table 2)show that vocabulary learning noticeably im-proves when supported by the generated sen-tences, with error rates dropping by almost 30%in terms of macro-average (almost 27% for micro-average).
The breakdown of the error rate acrossthe 4 groups shows a clear pattern.
The resultsclearly indicate that one group (A1) by chancecontained easier words to memorize as shown bythe low error rate (between 3% and 4%) obtainedwith both methods.
Similarly, groups A2 and B1are of average difficulty, whereas group B2 ap-pears to be the most difficult, with an error ratehigher than 22% when using only rote memoriza-tion.
Interestingly, there is a strong correlation(Pearson?s r = 0.85) between the difficulty ofthe words in each group (measured as the errorrate on rote memorization) and the positive contri-bution of the generated sentences to the learningprocess.
In fact, we can see how the relative er-ror rate reduction %eincreases from?17% (groupA1) to almost 45% (group B2).
Based on the re-sults obtained by Sagarra and Alba (2006), whoshowed that the keyword method results in bet-ter long-term word retention than rote memoriza-tion, we would expect the error rate reduction to beeven higher in a delayed post-test.
All in all, thesefindings clearly support the claim that a state-of-the-art sentence generator can be successfully em-ployed to support keyword-based second languagelearning.
After completing their exercise, the sub-jects were asked to provide feedback about theirexperience as learners.
We set up a 4-items Lik-ert scale (Likert, 1932) where each item consistedof a statement and a 5-point scale of values rang-ing from (1) [I strongly disagree] to (5) [I stronglyagree].
The distribution of the answers to the ques-tions is shown in Table 3.
60% of the subjects ac-knowledged that the memory tips helped them inRating (%)Question 1 2 3 4 5Sentences helped 5 20 15 35 25Sentences are grammatical - 25 30 35 10Sentences are catchy - 25 10 50 15Sentences are witty - 25 25 50 -Table 3: Evaluation of the generated sentences ona 5-point Likert scale.the memorization process; 45% found that the sen-tences were overall correct; 65% confirmed thatthe sentences were catchy and easy to remember;and 50% found the sentences to be overall wittyalthough the sentence generator does not include amechanism to generate humor.
Finally, it is worthmentioning that none of the subjects noticed thatthe sentences were machine generated, which weregard as a very positive assessment of the qual-ity of the sentence generation framework.
Fromtheir comments, it emerges that the subjects ac-tually believed that they were just comparing twomemorization techniques.6 Conclusion and Future WorkIn this paper, we have presented a semi-automaticsystem for the automation of the keyword methodand used it to teach 40 Italian words to 20 En-glish native speakers.
We let the system selectappropriate keywords and generate sentences au-tomatically.
For each Italian word, we selected themost suitable among the 10 highest ranked sug-gestions and used it for the evaluation.
The sig-nificant reduction in retention error rate (between17% and 45% on different word groups) for thewords learned with the aid of the automaticallygenerated sentences shows that they are a viablelow-effort alternative to human-constructed exam-ples for vocabulary teaching.As future work, it would be interesting to in-volve learners in an interactive evaluation to un-derstand the extent to which learners can bene-fit from ad-hoc personalization.
Furthermore, itshould be possible to use frameworks similar tothe one that we presented to automate other teach-ing devices based on sentences conforming to spe-cific requirements (Dehn, 2011), such as verbalchaining and acrostic.AcknowledgementsThis work was partially supported by the PerTeproject (Trento RISE).356ReferencesAndrew D. Cohen.
1987.
The use of verbal andimagery mnemonics in second-language vocabularylearning.
Studies in Second Language Acquisition,9:43?61, 2.M.J.
Dehn.
2011.
Working Memory and AcademicLearning: Assessment and Intervention.
Wiley.K.
M. Hummel.
2010.
Translation and short-term L2vocabulary retention: Hindrance or help?
LanguageTeaching Research, 14(1):61?74.V.
Levenshtein.
1966.
Binary codes capable of cor-recting deletions, insertions, and reversals.
SovietPhysics Doklady, 10:707?710.R.
Likert.
1932.
A technique for the measurement ofattitudes.
Archives of Psychology, 22(140):1?55.Ruli Manurung, Graeme Ritchie, Helen Pain, AnnaluWaller, Dave O?Mara, and Rolf Black.
2008.
TheConstruction of a Pun Generator for Language SkillDevelopment.
Appl.
Artif.
Intell., 22(9):841?869,October.R.
Mihalcea and C. Strapparava.
2006.
Learning tolaugh (automatically): Computational models forhumor recognition.
Journal of Computational In-telligence, 22(2):126?142, May.A.
Mizumoto and O. T. Kansai.
2009.
Examiningthe effectiveness of explicit instruction of vocabu-lary learning strategies with Japanese EFL universitystudents.
Language Teaching Research 13, 4.G?ozde?Ozbal and Carlo Strapparava.
2011.
MEANS:Moving Effective Assonances for Novice Students.In Proceedings of the 16th International Confer-ence on Intelligent User Interfaces (IUI 2011), pages449?450, New York, NY, USA.
ACM.G?ozde?Ozbal, Daniele Pighin, and Carlo Strapparava.2013.
BRAINSUP: Brainstorming Support for Cre-ative Sentence Generation.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (ACL 2013), pages 1446?1455,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Cyrus Rashtchian, Peter Young, Micah Hodosh, andJulia Hockenmaier.
2010.
Collecting image annota-tions using amazon?s mechanical turk.
In Proceed-ings of the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechan-ical Turk, CSLDAMT ?10, pages 139?147, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.N.
Sagarra and M. Alba.
2006.
The key is in thekeyword: L2 vocabulary learning methods with be-ginning learners of spanish.
The Modern LanguageJournal, 90(2):228?243.A.
Sar?c?oban and N.
Bas??bek.
2012.
Mnemonics tech-nique versus context method in teaching vocabularyat upper-intermediate level.
Journal of Educationand Science, 37(164):251?266.Helen H. Shen.
2010.
Imagery and verbal coding ap-proaches in Chinese vocabulary instruction.
Lan-guage Teaching Research, 14(4):485?499.Steffen Sommer and Michael Gruneberg.
2002.
Theuse of linkword language computer courses in aclassroom situation: a case study at rugby school.
?Language Learning Journal, 26(1):48?53.M.
Tavakoli and E. Gerami.
2013.
The effect of key-word and pictorial methods on EFL learners?
vocab-ulary learning and retention.
PORTA LINGUARUM,19:299?316.G.
Thompson.
1987.
Using bilingual dictionar-ies.
ELT Journal, 41(4):282?286.
cited By (since1996)6.357
