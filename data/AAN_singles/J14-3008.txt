Pushdown Automata in StatisticalMachine TranslationCyril Allauzen?Google ResearchBill Byrne?
?University of CambridgeAdria` de Gispert?
?University of CambridgeGonzalo Iglesias?
?University of CambridgeMichael Riley?Google ResearchThis article describes the use of pushdown automata (PDA) in the context of statistical machinetranslation and alignment under a synchronous context-free grammar.
We use PDAs to com-pactly represent the space of candidate translations generated by the grammar when applied to aninput sentence.
General-purpose PDA algorithms for replacement, composition, shortest path,and expansion are presented.
We describe HiPDT, a hierarchical phrase-based decoder using thePDA representation and these algorithms.
We contrast the complexity of this decoder with a de-coder based on a finite state automata representation, showing that PDAs provide a more suitableframework to achieve exact decoding for larger synchronous context-free grammars and smallerlanguage models.
We assess this experimentally on a large-scale Chinese-to-English alignmentand translation task.
In translation, we propose a two-pass decoding strategy involving a weakerlanguage model in the first-pass to address the results of PDA complexity analysis.
We studyin depth the experimental conditions and tradeoffs in which HiPDT can achieve state-of-the-artperformance for large-scale SMT.?
Google Research, 76 Ninth Avenue, New York, NY 10011.
E-mail: {allauzen,riley}@google.com.??
University of Cambridge, Department of Engineering.
CB2 1PZ Cambridge, U.K. and SDL Research,Cambridge U.K. E-mail: {wjb31,ad465,gi212}@eng.cam.ac.uk.Submission received: 6 August 2012; revised version received: 20 February 2013; accepted for publication:2 December 2013.doi:10.1162/COLI a 00197?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 31.
IntroductionSynchronous context-free grammars (SCFGs) are nowwidely used in statistical machinetranslation, with Hiero as the preeminent example (Chiang 2007).
Given an SCFG andan n-gram language model, the challenge is to decode with them, that is, to apply themto source text to generate a target translation.Decoding is complex in practice, but it can be described simply and exactly interms of the formal languages and relations involved.
We will use this descriptionto introduce and analyze pushdown automata (PDAs) for machine translation.
Thisformal description will allow close comparison of PDAs to existing decoders which arebased on other forms of automata.
Decoding can be described in terms of the followingsteps:1.
Translation: T = ?2({s}?G)The first step is to compose the finite language {s}, which represents thesource sentence to be translated, with the algebraic relation G for thetranslation grammar G. The result of this composition projected on theoutput side is T , a weighted context-free grammar that contains all possibletranslations of s under G. Following the usual definition of Hiero grammars,we assume that G does not allow unbounded insertions so that T is aregular language.2.
Language Model Application: L=T ?MThe next step is to compose the result of Step 1 with the weighted regulargrammarM defined by the n-gram language model, M. The result of thiscomposition is L, whose paths are weighted by the combined languagemodel and translation scores.3.
Search: l?=argmaxl?LLThe final step is to find the path through L that has the best combinedtranslation and language model score.The composition {s} ?
G in Step 1 that generates T can be performed by a modifiedCYK algorithm (Chiang 2007).
Our interest is in the different types of automata that canbe used to represent T as it is produced by this composition.
We focus on three typesof representations: hypergraphs (Chiang 2007), weighted finite state automata (Iglesiaset al.
2009a; de Gispert et al.
2010), and PDAs.
We will give a formal definition of PDAsin Section 2, but we will first illustrate and compare these different representations bya simple example.Consider translating a source sentence ?s1 s2 s3?
with a simple Hiero grammar G :X?
?s1, t2 t3?S?
?X s2 s3, t1 t2 X t4 t7?S?
?X s2 s3, t1 t3 X t6 t7?Step 1 yields the translations T = {?t1 t2 t2 t3 t4 t7?
, ?t1 t3 t2 t3 t6 t7?
}, and Figure 1 givesexamples of the different representations of these translations.We summarize the salientfeatures of these representations as they are used in decoding.Hypergraphs.
As described by Chiang (2007), a Hiero decoder can generate translationsin the form of a hypergraph, as in Figure 1a.
As the figure shows, there is a1:1 correspondence between each production in the CFG and each hyperedge inthe hypergraph.688Allauzen et al.
Pushdown Automata in Statistical Machine Translation(a) Hypergraph01t16t12t27t33X 4t4 5t78X 9t6 10t7 0 1t2 2t3S X(b) RTN01t12t13t24t35eps6eps7t28t29t310t311eps12eps13t414t615t716t7(c) FSA01t16t12t27t311(12t23 4t4 5t7[8 9t610t713t3)](d) PDAFigure 1Alternative representations of the regular language of possible translation candidates.
Validpaths through the PDA must have balanced parentheses.Decoding proceeds by intersecting the translation hypergraph with a languagemodel, represented as a finite automaton, yielding L as a hypergraph.
Step 3yields a translation by finding the shortest path through the hypergraphL (Huang2008).Weighted Finite State Automata (WFSAs).
Because T is a regular language and M isrepresented by a finite automaton, it follows that T and L can themselvesbe represented as finite automata.
Consequently, Steps 2 and 3 can be solved689Computational Linguistics Volume 40, Number 3using weighted finite-state intersection and single-source shortest path algo-rithms, respectively (Mohri 2009).
This is the general approach adopted in theHiFST decoder (Iglesias et al.
2009a; de Gispert et al.
2010), which first representsT as a Recursive Transition Network (RTN) and then performs expansion togenerate a WFSA.Figure 1b shows the space of translations for this example represented as an RTN.Like the hypergraph, it also has a 1:1 correspondence between each productionin the CFG and paths in the RTN components.
The recursive RTN itself can beexpanded into a single WFSA, as shown in Figure 1c.
Intersection and shortestpath algorithms are available for both of these WFSAs.Pushdown Automata.
Like WFSAs, PDAs are easily generated from RTNs, as will bedescribed later, and Figure 1d gives the PDA representation for this example.
ThePDA represents the same language as the FSA, but with fewer states.
Proceduresto carry out Steps 2 and 3 in decoding will be described in subsequent sections.We will show that PDAs provide a general framework to describe key aspectsof several existing and novel translation algorithms.
We note that PDAs have longbeen used to describe parsing algorithms (Aho and Ullman 1972; Lang 1974), and it iswell known that pushdown transducers, the extended version of PDA with input andoutput labels in each transition, do not have the expressive power needed to generatesynchronous context-free languages.
For this reason, we do not use PDAs to implementStep 1 in decoding: throughout this article a CYK-like parsing algorithm is always usedfor Step 1.
However, we do use PDAs to represent the regular languages produced inStep 1 and in the intersection and shortest distance operations needed for Steps 2 and 3.1.1 HiPDT: Hierarchical Phrase-Based Translation with PDAsWe introduce HiPDT, a hierarchical phrase-based decoder that uses a PDA representa-tion for the target language.
The architecture of the system is shown in Figure 2, whereCYK parse s with GBuild RTNExpand RTN to FSAIntersect FSA with LMFSAShortestPathFSAPruningLattice1-BestHypothesisRTN to PDA ReplacementIntersect PDA with LMPDA(Pruned)ExpansionPDAShortestPathHiPDTHiFSTFigure 2HiPDT versus HiFST: General flow and high-level operations.690Allauzen et al.
Pushdown Automata in Statistical Machine Translationwe contrast it with HiFST (de Gispert et al.
2010).
Both decoders parse the sentence witha grammar G using a modified version of the CYK algorithm to generate the translationsearch space as an RTN.
Each decoder then follows a different path: HiFST expandsthe RTN into an FSA, intersects it with the language model, and then prunes the result;HiPDT performs the following steps:1.
Convert the RTN into PDA using the replacement algorithm.
The PDArepresentation for the example grammar in Section 1 is shown in Figure 1.The algorithm will be described in Section 3.2.2.
Apply the language model scores to the PDA by composition.
This operationis described in Section 3.3.3.
Perform either one of the following operations:(a) Shortest path through the PDA to get the exact best translation underthe model.
Shortest distance/path algorithm is described in Section 3.4.
(b) Pruned expansion to an FSA.
This expansion uses admissible pruningand outputs a lattice.
We do this for posterior rescoring steps.
Thealgorithm will be presented in detail in Sections 3.5 and 3.5.2.The principal difference between the two decoders is the point at which finite-stateexpansion is performed.
In HiFST, the RTN representation is immediately expanded toan FSA.
In HiPDT, the PDA pruned expansion or shortest path computation is doneafter the language model is applied, so that all computation is done with respect to boththe translation and language model scores.The use of RTNs as an initial translation representation is somewhat influenced bythe development history of our FST and SMT systems.
RTN algorithms were availablein OpenFST at the time HiFST was developed.
HiPDT was developed as an extension toHiFST using PDA algorithms, and these have subsequently been included in OpenFST.A possible alternative approach could be to produce a PDA directly by traversing theCYK grid.
WFSAs could then be generated by PDA expansion, with a computationalcomplexity in speed and memory usage similar to the RTN-based approach.We presentRTNs as the initial translation representation because the generation of RTNs duringparsing is straightforward and has been previously presented (de Gispert et al.
2010).We note, however, that RTN composition is algorithmically more complex than PDA(and FSA) composition, so that RTNs themselves are not ideal representations of T if alanguage model is to be applied.
Composition of PDAs with FSAs will be discussed inSection 3.3.Figure 3 continues the simple translation example from earlier, showing howHiPDT andHiFST both benefit from the compactness offeredbyWFSA epsilon removal,determinization, andminimization operations.
When applied to PDAs, these operationstreat parentheses as regular symbols.
Compact representations of RTNs are shared byboth approaches.
Figure 4 illustrates the PDA representation of the translation spaceunder a slightly more complex grammar that includes rules with alternative orderingsof nonterminals.
The rule S?
?X1 s2 X2, t1 X1 X2?
produces the sequence ?t1 t3 t4 t5 t6?,and S?
?X1 s2 X2, t2 X2 X1?
produces ?t2 t5 t6 t3 t4?.
The PDA efficiently represents thealternative orderings of the phrases ?t3 t4?
and ?t5 t6?
allowed under this grammar.In addition to translation, this architecture can also be used directly to carry outsource-to-target alignment, or synchronous parsing, under the SCFG in a two-stepcomposition rather than one synchronous parsing stage.
For example, by using M as theautomata that accepts ?t1 t2 t3 t6 t7?, Step 2 will yield all derivations that yield this string691Computational Linguistics Volume 40, Number 30 1t12t23t34X5X6t4t6 7t70 1t2 2t3S X(a) Optimized RTN0 1t12t23t34t25t26t37t38t4t6 9t7(b) Optimized FSA0 1t12t23t3 8([ 9t246t47t75t610t3)](c) Optimized PDAFigure 3Optimized representations of the regular language of possible translation candidates.as a translation of the source string.
This is the approach taken in Iglesias et al.
(2009a)and de Gispert et al.
(2010) for the RTN/FSA and in Dyer (2010b) for hypergraphs.
InSection 4 we analyze how PDAs can be used for alignment.1.2 GoalsWe summarize here the aims of this article.We will show how PDAs can be used as compact representations of the space Tof candidate translations generated by a hierarchical phrase-based SCFG whenapplied to an input sentence s and intersected with a language model M.We have described the architecture of HiPDT, a hierarchical phrase-based de-coder based on PDAs, and have identified the general-purpose algorithms needed01t12t23(4[5t36t57t4 8t69)10])11]([X?
?s1, t3 t4?X?
?s3, t5 t6?S?
?X1 s2 X2, t1 X1 X2?S?
?X1 s2 X2, t2 X2 X1?Figure 4Example of translation grammar with reordered nonterminals and the PDA representing theresult of applying the grammar to input sentence s1 s2 s3.692Allauzen et al.
Pushdown Automata in Statistical Machine Translationto perform translation and alignment; in doing so we have highlighted thesimilarities and differences relative to translation with FSAs (Section 1.1).
Wewill provide a formal description of PDAs (Section 2) and present in detailthe associated PDA algorithms required to carry out Steps 2 and 3, includingRTN replacement, composition, shortest path, expansion, and pruned expansion(Section 3).We will show both theoretically and experimentally that the PDA representation iswell suited for exact decoding under a large SCFG and a small languagemodel.An analysis of decoder complexity in terms of the automata used in the repre-sentation is presented (Section 3).
One important aspect of the translation taskis whether the search for the best translation is admissible (or exact) under thetranslation and language models.
Stated differently, we wish to know whether adecoder produces the actual shortest path found or whether some form of pruningmight have introduced search errors.
In our formulation, we can exclude inadmis-sible pruning from the shortest-path algorithms, and doing so makes it straight-forward to compare the computational complexity of a full translation pipelineusing different representations of T (Section 4).
We empirically demonstrate thata PDA representation is superior to an FSA representation in the ability to performexact decoding both in an inversion transduction grammar?style word alignmenttask and in a translation task with a small language model (Section 4).
In theseexperiments we take HiFST as a contrastive system for HiPDT, but we do notpresent experimental results with hypergraph representations.
Hypergraphs arewidely used by the SMT community, and discussions and contrastive experimentsbetween HiFST and cube pruning decoders are available in the literature (Iglesiaset al.
2009a; de Gispert et al.
2010).We will propose a two-pass translation decoding strategy for HiPDT based onentropy-pruned first-pass language models.Our complexity analysis prompts us to investigate decoding strategies based onlarge translation grammars and small language models.
We describe, implement,and evaluate a two-pass decoding strategy for a large-scale translation task usingHiPDT (Section 5).
We show that entropy-pruned languagemodels can be used infirst-pass translation, followed by admissible beam pruning of the output latticeand subsequent rescoring with a full language model.
We analyze the searcherrors that might be introduced by a two-pass translation approach and showthat these can be negligible if pruning thresholds are set appropriately (Sec-tion 5.2).
Finally, we detail the experimental conditions and speed/performancetradeoffs that allow HiPDT to achieve state-of-the-art performance for large-scale SMT under a large grammar (Section 5.3), including lattice rescoring stepsunder a vast 5-gram language model and lattice minimum Bayes risk decoding(Section 5.4).With this translation strategyHiPDT can yield very good translation performance.For comparison, the performance of this Chinese-to-English SMT described inSection 5.4 is equivalent to that of the University of Cambridge submission to theNIST OpenMT 2012 Evaluation.11 For details see http://www.nist.gov/itl/iad/mig/openmt12.cfm.693Computational Linguistics Volume 40, Number 32.
Pushdown AutomataInformally, pushdown transducers are finite-state transducers that have been aug-mented with a stack.
Typically this is done by adding a stack alphabet and labelingeach transition with a stack operation (a stack symbol to be pushed onto, popped, orread from the stack) in addition to the usual input and output labels (Aho and Ullman1972; Berstel 1979) and weight (Kuich and Salomaa 1986; Petre and Salomaa 2009).
Ourequivalent representation allows a transition to be labeled by a stack operation or aregular input/output symbol, but not both.
Stack operations are represented by pairsof open and close parentheses (pushing a symbol on and popping it from the stack).The advantage of this representation is that it is identical to the finite automaton repre-sentation except that certain symbols (the parentheses) have special semantics.
As such,several finite-state algorithms either immediately generalize to this PDA representationor do so with minimal changes.
In this section we formally define pushdown automataand transducers.2.1 DefinitionsA (restricted) Dyck language consist of ?well-formed?
or ?balanced?
strings over afinite number of pairs of parentheses.
Thus the string ( [ ( ) ( ) ] { } [ ] ) ( ) is in theDyck language over three pairs of parentheses (see Berstel 1979 for a more detailedpresentation).More formally, let A and A be two finite alphabets such that there exists a bijectionf from A to A.
Intuitively, f maps an open parenthesis to its corresponding closeparenthesis.
Let a?
denote f (a) if a?A and f?1(a) if a?A.
The Dyck language DAover the alphabet A?=A ?
A is then the language defined by the following context-freegrammar: S?
?, S?
SS and S?
aSa?
for all a?A.
We define the mapping cA : A??
?
A?
?as follows.
cA(x) is the string obtained by iteratively deleting from x all factors of theform aa?
with a ?
A.
Observe that DA=c?1A (?).
Finally, for a subset B ?
A, we define themapping rB : A?
?
B?
by rB(x1 .
.
.
xn)=y1 .
.
.
yn with yi=xi if xi?B and yi=?
otherwise.A semiring (K,?,?, 0, 1) is a ring that may lack negation.
It is specified by a setof values K, two binary operations ?
and ?, and two designated values 0 and 1.The operation ?
is associative, commutative, and has 0 as identity.
The operation ?is associative, has identity 1, distributes with respect to ?, and has 0 as annihilator:for all a ?
K, a?
0 = 0?
a = 0.
If ?
is also commutative, we say that the semiring iscommutative.The probability semiring (R+,+,?, 0, 1) is used when the weights represent prob-abilities.
The log semiring (R ?
{?
},?log,+,?, 0), isomorphic to the probability semi-ring via the negative-log mapping, is often used in practice for numerical stability.
Thetropical semiring (R ?
{?
}, min,+,?, 0) is derived from the log semiring using theViterbi approximation.
These three semirings are commutative.A weighted pushdown automaton (PDA) T over a semiring (K,?,?, 0, 1) is an8-tuple (?,?,?,Q,E, I, F, ?)
where ?
is the finite input alphabet, ?
and ?
are the finiteopen and close parenthesis alphabets, Q is a finite set of states, I?Q the initial state,F ?
Q the set of final states, E ?
Q?
(?
?
??
?
{?
})?K?Q a finite set of transitions,and ?
: F?
K the final weight function.
Let e= (p[e], i[e],w[e], n[e]) denote a transitionin E; for simplicity, (p[e], i[e], n[e]) denotes an unweighted transition (i.e., a transitionwith weight 1?
).694Allauzen et al.
Pushdown Automata in Statistical Machine TranslationA path ?
is a sequence of transitions ?=e1 .
.
.
en such that n[ei]=p[ei+1] for 1 ?
i <n.
We then define p[?
]=p[e1], n[?
]=n[en], i[?
]= i[e1] ?
?
?
i[en], and w[?]=w[e1]?
.
.
.?w[en].
A path ?
is accepting if p[?
]= I and n[?]?F.
A path ?
is balanced if r??(i[?
])?D?.A balanced path ?
accepts the string x???
if it is a balanced accepting path such thatr?(i[?
])=x.The weight associated by T to a string x???
isT(x)=???P(x)w[?]??(n[?])
(1)where P(x) denotes the set of balanced paths accepting x.
A weighted language isrecognizable by a weighted pushdown automaton iff it is context-free.
We define thesize of T as |T|= |Q|+|E|.A PDA T has a bounded stack if there exists K ?
N such that for any path ?
from Isuch that c?(r??(i[?]))
?
??:|c?(r??(i[?
]))| ?
K (2)In other words, the number of open parentheses that are not closed along ?
is bounded.If T has a bounded stack, then it represents a regular language.
Figure 5 shows non-regular, regular, and bounded-stack PDAs.
A weighted finite automaton (FSA) can beviewed as a PDA where the open and close parentheses alphabets are empty (see Mohri2009 for a stand-alone definition).Finally, a weighted pushdown transducer (PDT) T over a semiring (K,?,?, 0, 1)is a 9-tuple (?,?,?,?,Q,E, I, F, ?)
where ?
is the finite input alphabet, ?
is the finiteoutput alphabet, ?
and ?
are the finite open and close parenthesis alphabets, Q is afinite set of states, I?Q the initial state, F ?
Q the set of final states, E ?
Q?
(?
?
??
?01a2?(3)b01a2?
(?3)?b01(3?2a4()5b)(a) (b) (c)0,?1,(?3,??2,(a4,(??5,(b?01a:c/12?:?(:(/13):)b:c/120?
:?1a:c/13S:  /1?b:c/1TS(d) (e) (f)Figure 5PDA Examples: (a) Non-regular PDA accepting {anbn|n ?
N}.
(b) Regular (but notbounded-stack) PDA accepting a?b?.
(c) Bounded-stack PDA accepting a?b?
and (d) itsexpansion as an FSA.
(e) Weighted PDT T1 over the tropical semiring representing theweighted transduction (anbn, c2n) 7?
3n and (f) equivalent RTN ({S},{a, b}, {c}, {TS},S).695Computational Linguistics Volume 40, Number 3{?
})?K?Q a finite set of transitions, and ?
: F?
K the final weight function.
Lete= (p[e], i[e], o[e],w[e], n[e]) denote a transition in E. Note that a PDA can be seen asa particular case of a PDT where i[e] = o[e] for all its transitions.
For simplicity, ourfollowing presentation focuses on acceptors, rather than the more general case of trans-ducers.
This is adequate for the translation applications we describe, with the exceptionof the treatment of alignment in Section 4.3, for which the intersection algorithm forPDTs and FSTs is given in Appendix A.3.
PDT OperationsIn this section we describe in detail the following PDA algorithms: Replacement, Com-position, Shortest Path, and (Pruned) Expansion.
Although these are needed to implementHiPDT, these are general purpose algorithms, and suitable for many other applicationsoutside the focus of this article.
The algorithms described in this section have beenimplemented in the PDT extension (Allauzen and Riley 2011) of the OpenFst library(Allauzen et al.
2007).
In this section, in order to simplify the presentation we will onlyconsider machines over the tropical semiring (R+ ?
{?
}, min,+,?, 0).
However, foreach operation, we will specify in which semirings it can be applied.3.1 Recursive Transition NetworksWe briefly give formal definitions for RTNs that will be needed to present the RTNexpansion operation.
Examples are shown earlier in Figures 1(b) and 3(a).
Informally,an RTN is an automaton where some labels, nonterminals, are recursively replacedby other automata.
We give the formal definition for acceptors; the extension to RTNtransducers is straightforward.An RTN R over the tropical semiring (R+ ?
{?
}, min,+,?, 0) is a 4-tuple(N,?, (T?)?
?N, S) where N is the alphabet of nonterminals, ?
is the input alpha-bet, (T?)?
?N is a family of FSTs with input alphabet ?
?N, and S ?
N is the rootnonterminal.A sequence x ?
??
is accepted by (R,?)
if there exists an accepting path ?
in T?
suchthat ?
= ?1e1 .
.
.
?nen?n+1 with i[?k] ?
?
?, i[ek] ?
N and such that there exists sequencesxk such that xk is accepted by (R, i[ek]) and x = i[?1]x1 .
.
.
i[?n]xni[?n+1].
We say that x isaccepted by R when it is accepted by (R, S).
The weight associated by (R,?)
(and by R)to x can be defined in the same recursive manner.As an example of testing whether an RTN accepts a sequence, consider the RTN Rof Figure 6 and the sequence x = a a b.
The path in the automata TS can be written as?
= ?1 e1 ?2, with i[?1] = a, i[e1] = X1, and i[?2] = b.
In addition, the machine (R, i[e1])accepts x1 = a.
Because x = i[?1] x1 i[?2], it follows that x is accepted by (R, S).3.2 ReplacementThis algorithm converts an RTN into a PDA.
As explained in Section 1.1, this PDToperation is applied by the HiPDT decoder in Step 1, and examples are given in earliersections (e.g., in figures 1 and 3).Replacement acts on every transition of the RTN that is associated with a non-terminal.
The source and destination states of these transitions are used to define thematched opening and closing parentheses, respectively, in the new PDA.
Each RTNnonterminal transition is deleted and replaced by two new transitions that lead to and696Allauzen et al.
Pushdown Automata in Statistical Machine Translationfrom the automaton indicated by the nonterminal.
These new transitions have matchedparentheses, taken from the source and destination states of the RTN transition theyreplace.
Figure 6 gives a simple example.Formally, given an RTN R, defined as (N,?, (T?)?
?N, S), its replacement is the PDAT equivalent to R defined by the 8-tuple (?,?,?,Q,E, I, F, ?)
with Q = ?
=??
?N Q?,I = IS, F = FS, ?
= ?S, and E =???N?e?E?
Ee where Ee = {e} if i[e] 6?
N andEe={(p[e], n[e], ?,w[e], I?
), (f, n[e], ?, ??
(f ), n[e])|f ?F?}
(3)with ?
= i[e] ?
N otherwise.The complexity of the construction is in O(|T|).
If |F?| = 1 for all ?
?
N, then|T| = O(??
?N |T?|) = O(|R|).
Creating a superfinal state for each T?
would lead to a Twhose size is always linear in the size of R. In this article, we assume this optimizationis always performed.
We note here that RTNs can be defined and the replacementoperation can be applied in any semiring.3.3 CompositionOnce we have created the PDA with translation scores, Step 2 in Section 1.1 applies thelanguage model scores to the translation space.
This is done by composition with anFSA containing the relevant language model weights.The class of weighted pushdown transducers is closed under composition withweighted finite-state transducers (Bar-Hillel, Perles, and Shamir 1964; Nederhof andSatta 2003).
OpenFST supports composition between automata T1 and T2, where T1is a weighted pushdown transducer and T2 is a weighted finite-state transducer.
Ifboth T1 and T2 are acceptors, rather than transducers, the composition of a PDA andan FSA produces a PDA containing their intersection, and so no separate intersectionalgorithm is required for these automata.
Given this, we describe only the simpler,special case of intersection between a PDA and an FSA, as this is sufficient for mostof the translation applications described in this article.
The alignment experiments ofRTN R1 2 3 4a X1 bTS5 6X2a 7 8bTX1 TX2R accepts a a b and a b b.PDT T1 2a5 63 47 8abb63 3?6?T accepts a 3 a 3?
b and a 3 6 b 6?
3?
b.Figure 6Conversion of an RTN R to a PDA T by the replacement operation of Section 3.2.
Using thenotation of Section 2.1, in this example ?
= {3, 5} and ?
= {3?, 5?
}, with f (3) = 3?
and f (5) = 5?.The unweighted transition (2,X1, 3) in R is deleted and replaced by two new transitions (2, 3, 5)and (6, 3?, 3); similarly, (5,X2, 6) is replaced by (5, 6, 7) and (8, 6?, 6).
After application of the r?mapping, the strings accepted by R and by T are the same.697Computational Linguistics Volume 40, Number 30 1ab 2ab 3ab 4abT201a2?
(3)b T10,01,1a2,0?0,1(3,0)1,2a2,1?b0,2(3,1)1,3a2,2?b0,3(3,2)1,4a2,3?b0,4(3,3)2,4?bTFigure 7Composition example: Composition of a PDA T1 accepting {an, bn} with an FSA T2 accepting{a, b}4 to produce a PDA T = T1 ?
T2 .
T has only one balanced path, and this path acceptsa(a(?)b)b.
Composition is performed by the PDA-FSA intersection described in Section 3.3.Section 4.3 do require composition of transducers; the algorithm for composition oftransducers is given in Appendix A.An example of composition by intersection is given in Figure 7.
The states of T arecreated as the product of all the states in T1 and T2.
Transitions are added as illustratedin Figure 8.
These correspond to all paths through T1 and T2 that can be taken bya synchronized reading of strings from {a, b}?.
The algorithm is very similar to thecomposition algorithm for finite-state transducers, the difference being the handlingof the parentheses.
The parenthesis-labeled transitions are treated similarly to epsilontransitions, but the parenthesis labels are preserved in the result.
This adds manyunbalanced paths to T. In this example, T has five paths but only one balanced path,so that T accepts the string a a b b.Formally, given a PDA T1 = (?,?,?,Q1,E1, I1, F1, ?1) and an FSA T2 =(?,Q2,E2, I2, F2, ?2), intersection constructs a new PDA T = (?,?,?,Q,E, I, F, ?
),where T = T1 ?
T2 as follows:1.
The new state space is in the product of the input state spaces: Q ?
Q1 ?Q2.2.
The new initial and final states are I = (I1, I2), and F = {(q1, q2) : q1 ?
F1, q2 ?
F2}.3.
Weights are assigned to final states (q1, q2) ?
Q as ?
(q1, q2) = ?
(q1)+ ?(q2).4.
For pairs of transitions (q1, a1,w1, q?1) ?
E1 and (q2, a2,w2, q?2) ?
E2, a transitionis added between states (q1, q2) and (q?1, q?2) as specified in Figure 8.PDT T1 FSA T2 PDT T = T1 ?
T2 Input Symbolsq1 q?1a1/w1q2 q?2a2/w2q1, q2 q?1, q?2a1/w1 + w2a1 ?
?
and a1 = a2q1, q2 q?1, q2a1/w1a1 ?
?
??
or a1 = ?Transitions are added to T if and only if the conditions on the input symbols are satisfied.Figure 8PDA?FSA intersection under the tropical semiring.
The PDA T is created by the intersection ofthe PDA T1 and the FSA T2, i.e., T = T1 ?
T2.698Allauzen et al.
Pushdown Automata in Statistical Machine TranslationThe intersection algorithm given here assumes that T2 has no input-?
transitions.When T2 has input-?
transitions, an epsilon filter (Mohri 2009; Allauzen, Riley, andSchalkwyk 2011) generalized to handle parentheses can be used.
Note that Steps 1 and 2do not require the construction of all possible pairs of states; only those states reachablefrom the initial state and needed in Step 4 are actually generated.
The complexity ofthe algorithm is in O(|T1| |T2|) in the worst case, as will be discussed in Section 4.Composition requires the semiring to be commutative.3.4 Shortest Distance and Path AlgorithmsWith a PDA including both translation and language model weights, HiPDT can ex-tract the best translation (Step 3a in Section 1.1).
To this end, a general PDA shortestdistance/path algorithm is needed.A shortest path in a PDA T is a balanced accepting path with minimal weightand the shortest distance in T is the weight of such a path.
We show that whenT has a bounded stack, shortest distance and shortest path can be computed inO(|T|3 log |T|) time (assuming T has no negative weights) and O(|T|2) space.
Figure 9gives a pseudo-code description of the shortest-distance algorithm, which we nowdiscuss.SHORTESTDISTANCE(T)1 for each q ?
Q and a ?
?
do2 B[q, a]?
?3 for each q ?
Q do4 d[q, q]?
?5 GETDISTANCE(T, I) ?
I is the unique initial state6 return d[I, f ] ?
f is the unique final stateRELAX(s,q,w,S )1 if d[s, q] > w then ?
if w is a better estimate of the distance from s to q2 d[s, q]?
w ?
update d[s, q]3 if q 6?
S then ?
enqueue q in S if needed4 ENQUEUE(S, q)GETDISTANCE(T,s)1 for each q ?
Q do2 d[s, q]?
?3 d[s, s]?
04 Ss ?
{s}5 while Ss 6=?
do6 q?
HEAD(Ss )7 DEQUEUE(Ss )8 for each e ?
E[q] do ?
E(q) is the set of transitions leaving state q9 if i[e] ?
?
?
{?}
then ?
i[e] is a regular symbol10 RELAX(s,n[e], d[s, q]+w[e],Ss )11 elseif i[e] ?
?
then ?
i[e] is a close parenthesis12 B[s, i[e]]?
B[s, i[e]] ?
{e}13 elseif i[e] ?
?
then ?
i[e] is an open parenthesis14 if d[n[e], n[e]]=?
then ?
n[e] is the destination state of transition e15 GETDISTANCE(T,n[e])16 for each e?
?
B[n[e], i[e]] do17 w?
d[s, q]+w[e]+ d[n[e], p[e?
]]+ w[e?
]18 RELAX(s,n[e?
],w,Ss )Figure 9PDT shortest distance algorithm.699Computational Linguistics Volume 40, Number 3Given a PDA T = (?,?,?,Q,E, I, F, ?
), the GETDISTANCE(T) algorithm computesthe shortest distance from the start state I to the final state2 f ?
F. The algorithmrecursively calculatesd[q, q?]
?
K ?
the shortest distance from state q to state q?
along a balanced pathAt termination, the algorithm returns d[I, f ] as the cost of the shortest path through T.The core of the shortest distance algorithm is the procedure GETDISTANCE(T, s)which calculates the distances d[s, q] for all states q that can be reached from s. For anFSA, this procedure is called once, as GETDISTANCE(T, I), to calculate d[I, q] ?q.For a PDA, the situation is more complicated.
Given a state s in T with at leastone incoming open parenthesis transition, we denote by Cs the set of states that can bereached by a balanced path starting from s. If s has several incoming open parenthesistransitions, a naive implementation might lead to the states in Cs to be visited exponen-tially many times.
This is avoided by memoizing the shortest distance from s to states inCs.
To do this, GETDISTANCE(T, s) calculates d[s, s?]
for all s?
?
Cs, and it also constructssets of transitionsB[s, a] = {e ?
E : p[e] ?
Cs and i[e] = a} ?a ?
?
(4)These are the transitions with label a leaving states in Cs.Consider any incoming transition to s, (q, a,w, s), with a ?
?.
For every transitione?
= (s?, a,w?, q?
), e?
?
B[s, a] the following holds3d[q, q?]
= w+ d[s, s?
]+ w?
(5)If d[s, s?]
is available, the shortest distance from q to q?
along any balanced path throughs can be computed trivially by Equation (5).
For any state s with incoming open paren-thesis transitions, only a single call to GETDISTANCE(T, s) is needed to precompute thenecessary values.Figure 10 gives an example.
When transition (2, (1, 0, 5) is processed,GETDISTANCE(T, 5) is called.
The distance d[5, 7] is computed, and following tran-sitions are logged: B[5, (1]?
{(7, )1, 0, 8)} and B[5, (2]?
{(7, )2, 0, 9)}.
Later, when thetransition (4, (2, 0, 5) is processed, its matching transition (7, )2, 0, 9) is extracted fromB[5, (2].
The distance d[4, 9] is then found by Equation (5) as d[5, 7].
This avoids redun-dant re-calculation of distances along the shortest balanced path from state 4 to state 9.We now briefly discuss the shortest distance pseudo-code given in Figure 9.
Thedescription may be easier to follow after reading the worked example in Figure 10.
Notethat the sets Cs are not computed explicitly by the algorithm.The shortest distance calculation proceeds as follows.
Self-distances, that is, d[q, q],are set initially to ?
; when GETDISTANCE(T, q) is called it sets d[q, q] = 0 to note thatq has been visited.
GETDISTANCE(T, s) starts a new instance of the shortest-distancealgorithm from s using the queue Ss, initially containing s. While the queue is not empty,a state is dequeued and its outgoing transitions examined (lines 7?11).
Transitionslabeled by non-parenthesis are treated as in Mohri (2009) (lines 7?8).
When a transitione is labeled by a close parenthesis, e is added to B[s, i[e]] to indicate that this transition2 For simplicity, we assume T has only one final state.3 This assumes all paths from q to q?
pass through s. The RELAX operation (Figure 9) handles thegeneral case.700Allauzen et al.
Pushdown Automata in Statistical Machine Translation01 25 6 78103 4 9t1/20t3/200(2t1/10t2/100(1 t2/1 t3/1)1 t4/1, 000)2t6/1, 000GETDISTANCE(T) runs1.
Initialization: d[q, q]?
?, ?q ?
Q2.
GETDISTANCE(T, 0) is calledGETDISTANCE(T, 0) runs3.
Distances are calculated from state 0:d[0, 0]?
0; d[0, 1]?
d[0, 0]+ w[0, 1]; d[0, 2]?
d[0, 1]+ w[1, 2]4.
Transition e1 = (2, (1, 0, 5) is reached.
e1 has symbol i[e1] = (1 and destination state n[e1] = 55. d[5, 5] =?
so GETDISTANCE(T, 5) is calledGETDISTANCE(T, 5) runs6.
Distances are calculated from state 5:d[5, 5]?
0; d[5, 6]?
d[5, 5]+ w[5, 6]; d[5, 7]?
d[5, 6]+ w[6, 7]7.
The transitions (7, )1, 0, 8) and (7, )2, 0, 9) are reached and memoizedB[5, (1]?
{(7, )1, 0, 8)}B[5, (2]?
{(7, )2, 0, 9)}GETDISTANCE(T, 5) endsGETDISTANCE(T, 0) resumes8.
Transition e1 = (2, (1, 0, 5) is still being processed, with p[e1] = 2, n[e1] = 5, and i[e1] = (19.
Transition e2 = (7, )1, 0, 8) matching (1 is extracted from B[n[e1], i[e1]], with p[e2] = 7and n[e2] = 810.
Distance d[0, 8] is calculated as d[0, n[e2]] :d[0, n[e2]]?
d[0, p[e1]]+ w[p[e1],n[e1]]+ d[n[e1], p[e2]]+ w[p[e2],n[e2]]10.
Processing of e1 finishes, and calculation of distances from 0 continues:d[0, 10]?
d[0, 8]+ w[8, 10]10 is a final state.
Processing continues with transition (0, t1, 20, 3)d[0, 3]?
d[0, 0]+ w[0, 3]; d[0, 4]?
d[0, 3]+ w[3, 4]13.
Transition e3 = (4, (2, 0, 5) is reachede3 has symbol i[e3] = (2, source state p[e3] = 4, and destination state n[e3] = 514.
GETDISTANCE(T, 5) is not called, since d[5, 5] = 0 indicates state 5 has been previouslyvisited15.
Transition e4 = (7, )2, 0, 9) matching (2 is extracted from B[n[e3], i[e3]], with p[e4] = 7 andn[e4] = 916.
Distance d[0, 9] is calculated as d[0, n[e4]], using cached values:d[0, n[e4]]?
d[0, p[e3]]+ w[p[e3],n[e3]]+ d[n[e3], p[e4]]+ w[p[e4],n[e4]]17. d[0, 10] is less than?
:d[0, 10]?
min(d[0, 10], d[0, 9]+ w[9, 10])18.
GETDISTANCE(T, 0) ends and returns d[0, 10]GETDISTANCE(T) endsFigure 10Step-by-step description of the shortest distance calculation for the given PDA by the algorithmof Figure 9.
For simplicity, w[q, q?]
indicates the weight of the transition connecting q and q?.balances all incoming open parentheses into s labeled by i[e] (lines 9?10).
Finally, if e hasan open parenthesis, and if its destination has not already been visited, a new instance ofGETDISTANCE is started from n[e] (lines 12?13).
The destination states of all transitionsbalancing e are then relaxed (lines 14?16).The space complexity of the algorithm is quadratic for two reasons.
First, thenumber of non-infinity d[q, s] is |Q|2.
Second, the space required for storing B is atmost in O(|E|2) because for each open parenthesis transition e, the size of |B[n[e], i[e]]|701Computational Linguistics Volume 40, Number 3is O(|E|) in the worst case.
This last observation also implies that the accumulatednumber of transitions examined at line 16 is in O(Z|Q| |E|2) in the worst case, whereZ denotes the maximal number of times a state is inserted in the queue for a givencall of GETDISTANCE.
Assuming the cost of a queue operation is ?
(n) for a queuecontaining n elements, the worst-case time complexity of the algorithm can then beexpressed as O(Z|T|3 ?(|T|)).
When T contains no negative weights, using a shortest-first queue discipline leads to a time complexity in O(|T|3 log |T|).
When all theCs?s are acyclic, using a topological order queue discipline leads to a O(|T|3) timecomplexity.As was shown in Section 3.2, when T has been obtained by converting an RTNor a hypergraph into a PDA, the polynomial dependency in |T| becomes a lineardependency both for the time and space complexities.
Indeed, for each q in T, thereexists a unique s such that d[s, q] is non-infinity.
Moreover, for each open parenthesistransition e, there exists a unique close parenthesis transition e?
such that e?
?B[n[e], i[e]].When each component of the RTN is acyclic, the complexity of the algorithm is O(|T|)in time and space.The algorithm can be modified (without changing the complexity) to compute theshortest path by keeping track of parent pointers.
The notion of shortest path requiresthe semiring (K,?,?, 0, 1) to have the path property: for all a, b in K, a?
b ?
{a, b}.
Theshortest-distance operation as presented here and the shortest-path operation can beapplied in any semiring having the path property by using the natural order defined by?
: a ?
b iff a?
b = a.
However, the shortest distance algorithm given in Figure 9 can beextended to work for k-closed semirings using the same techniques that were used byMohri (2002).The shortest distance in the intersection of a string s and a PDA T determines if Trecognizes s. PDA recognition is closely related to CFG parsing; a CFG can be repre-sented as a PDT whose input recognizes the CFG and whose output identifies the parse(Aho and Ullman 1972).
Lang (1974) showed that the cubic tabular method of Earleycan be naturally applied to PDAs; others give the weighted generalizations (Stolcke1995; Nederhof and Satta 2006).
Earley?s algorithm has its analogs in the algorithm inFigure 9: the scan step corresponds to taking a non-parenthesis transition at line 10, thepredict step to taking an open parenthesis at lines 14?15, and the complete step to takingthe closed parentheses at lines 16?18.Specialization to Translation.
Following the formalism of Section 1, we are interestedin applying shortest distance and shortest path algorithms to automata created asL = Tp ?M, where Tp, the translation representation, is a PDA derived from an RTN(via replacement) and M, the language model, is a finite automaton.For this particular case, the time complexity is O(|Tp||M|3) and the space complexityis O(|Tp||M2|).
The dependence on |Tp| is linear, rather than cubic or quadratic.
Thereasoning is as follows.
Given a state q in Tp, there exists a unique sq such that q belongsto Csq.
Given a state (q1, q2) in Tp?M, (q1, q2)?C(s1,s2 ) only if s1 = sq1 , and hence (q1, q2)belongs to at most |M| components.3.5 ExpansionAs explained in Section 1.1, HiPDT can apply Step 3b to generate translation lattices.This step is typically required for any posterior lattice rescoring strategies.
We first702Allauzen et al.
Pushdown Automata in Statistical Machine Translationdescribe the unpruned expansion.
However, in practice a pruning strategy of some sortis required to avoid state explosion.
Therefore, we also describe an implementation ofthe PDA expansion that includes admissible pruning under a likelihood beam, thuscontrolling on-the-fly the size of the output lattice.3.5.1 Full Expansion.
Given a bounded-stack PDA T, the expansion of T is the FSA T?equivalent to T. A simple example is given in Figure 11.Expansion starts from the PDA initial state.
States and transitions are added tothe FSA as the expansion proceeds along paths through the PDA.
In the new FSA,parentheses are replaced by epsilons, and as open parentheses are encountered onPDA transitions, they are ?pushed?
into the FSA state labels; in this way the stackdepth is maintained along different paths through the PDA.
Conversely, when a closingparenthesis is encountered on a PDA path, a corresponding opening parenthesis is?popped?
from the FSA state label; if this is not possible, for example, as in state (5, ?
)in Figure 11, expansion along that path halts.The resulting automata accept the same language.
The FSA topology changes,typically with more states and transitions than the original PDA, and the number ofadded states is controlled only by the maximum stack depth of the PDA.Formally, suppose the PDA T = (?,?,?,Q,E, I, F, ?)
has a maximum stack depthof K. The set of states in its FSA expansion T?
are thenQ?
= {(q, z) : q ?
Q , z ?
??
and |z| ?
K} (6)and T?
has initial state (I, ?)
and final states F?
= {(q, ?)
: q ?
F}.
The condition that Thas a bounded stack ensures that Q?
is finite.
Transitions are added to T?
as described inFigure 12.The full expansion operation can be applied to PDA over any semiring.
The com-plexity of the algorithm is linear in the size of T?.
However, the size of T?
can beexponential in the size of T, which motivates the development of pruned expansion,as discussed next.012 34 5 6[ [[a]b ]c0,?1, [ 2, [[ 3, [[ 4, [ 5, [ 6,???
a ?
b ?2, [3, [?ac4,?
5,?
?bFigure 11Full expansion of a PDA to an equivalent FSA.
The PDA maximum stack depth is 2; thereforethe FSA states belong to {0, .., 6} ?
{?, [, [[}.
Expansion can create incomplete paths in the FSA(e.g., corresponding here to the unbalanced PDA path [ a ] b ]); however these are guaranteed tobe unconnected, namely, not to lead to a final state.
Any unconnected states are removed afterexpansion.703Computational Linguistics Volume 40, Number 3Transition in PDA T New transition in FSA T?
Conditions Explanationq, z q?, za/wa ?
?
?
{?}
a is not a parenthesis; stackdepth is unchangedq q?a/wq, z q?, za?a ?
?a is an open parenthesis; anepsilon transition is added,and a is ?pushed?
into thedestination state, increas-ing the stack depthq, z?a q?, z?
?a ?
?a is a closing parenthe-sis; an epsilon transitionis added, and the match-ing open parenthesis a is?popped?
from the destina-tion state, decreasing thestack depthFigure 12PDA Expansion.
A states (q, z) and (q?, z? )
in the FSA T?
will be connected by a transition if andonly if the above conditions hold on the corresponding transition between q and q?
in the PDA T.3.5.2 Pruned Expansion.
Given a bounded-stack PDA T, the pruned expansion of T withthreshold ?
is an FST T??
obtained by deleting from T?
all states and transitions that donot belong to any accepting path ?
in T?
such that w[?]?
?[?]
?
d+ ?, where d is theshortest distance in T.A naive implementation consisting of fully expanding T and then applying theFST pruning algorithm would lead to a complexity in O(|T?| log |T?|)=O(e|T||T|).Assuming that the reverse TR of T is also bounded-stack, an algorithm whose com-plexity is in O(|T| |T?
?|+ |T|3 log |T|) can be obtained by first applying the shortestdistance algorithm from the previous section to TR and then using this to prune theexpansion as it is generated.
To simplify the presentation, we assume that F={ f} and?
( f )=0.The motivation for using reversed automaton in pruning is easily seen by lookingat FSAs.
For an FSA, the cost of the shortest path through a transition (q, x,w, q?)
canbe stated as d[I, q]+ w+ d[q?, f ].
Distances d[I, q] (i.e., distances from the start state) arecomputed by the shortest distance algorithm, as discussed in Section 3.4.
However,distances of the form d[q?, f ] are not readily available.
To compute these, a shortestdistance algorithm is run over the reversed automaton.
Reversal preserves states andtransitions, but swaps the source and destination state (see Figure 13 for a PDA ex-ample).
The start state in the reversed machine is f , so that distances are computedfrom f ; these are denoted dR[f, q] and correspond to d[q, f ] in the original FSA.
Thecost of the shortest path through an FSA transition (q, x,w, q?)
can then be computed asd[I, q]+ w+ dR[f, q?
].Calculation for PDAs is more complex.
Transitions with parentheses must be han-dled such that distances through them are calculated over balanced paths.
For example,if T in Figure 13 was an FSA, the shortest cost of any path through the transitione = (4, (2, 0, 5) could be calculated as d[0, 4]+ 0+ d[5, 10].
However, this is not correct,because d[5, 10], the shortest distance from 5 to 10, is found via a path through thetransition (7, )1, 0, 8).Correct calculation of the minimum cost of balanced paths through PDA transitionscan be done using quantities computed by the PDA shortest distance algorithm.
For a704Allauzen et al.
Pushdown Automata in Statistical Machine Translation01 25 6 78103 4 9t1/20t3/200(2t1/10t2/100(1 t2/1 t3/1)1 t4/1, 000)2t6/1, 000T01 25 6 78103 4 9t1/20t3/200(2t1/10t2/100(1 t2/1 t3/1)1 t4/1, 000)2t6/1, 000TRFigure 13PDA T and its reverse TR.
TR has start state 10, final state 0, ?R = {)1, )2}, and ?R = {(1, (2}.PDA transition e = (q, a,w, q?
), a ?
?, the cost of the shortest balanced path through ecan be found as4c(e) = d[I, q]+ w[e]+ mine?
?B[q?,a]d[q?, p[e?
]]+ w[e?
]+ dR[n[e?
], f ] (7)where B[q?, a] and d[p[e?
], q?]
are computed by the PDA shortest distance algorithm overT, and dR[n[e?
], f ] is computed by the PDA shortest distance algorithm over TR.In Figure 13, the shortest cost of paths through the transition e = (4, (2, 0, 5) is foundas follows: the shortest distance algorithm over T calculates d[0, 4] = 220 , d[5, 7] = 2,and B[5, (2] = {7, )2, 0, 9}; the shortest distance algorithm over TR calculates dR[10, 9] =1, 000 (trivially, here); the cost of the shortest path through e isd[0, 4]+ w[e]+ d[5, 7]+ w[e?
]+ dR[10, 9] = 220+ 0+ 2+ 0+ 1, 000Pruned expansion is therefore able to avoid expanding transitions that would notcontribute to any path that would survive pruning.
Prior to expansion of a PDA T to anFSA T?, the shortest distance d in T is calculated.
Transitions e = (q, a,w, q?
), a ?
?, areexpanded as transitions e = ((q, z), q,w, (q?, za)) in T?
only if c(e) ?
d+ ?, as calculatedby Equation (7).The pruned expansion algorithm implemented in OpenFST is necessarily morecomplicated than the simple description given here.
Pseudo-code describing the Open-FST implementation is given in Appendix B.The pruned expansion operation can be applied in any semiring having the pathproperty.4 Note that d[p[e?
], q?]
could be replaced by dR[q?, p[e?
]].705Computational Linguistics Volume 40, Number 34.
HiPDT Analysis and Experiments: Computational ComplexityWe now address the following questions:r What are the differences between the FSA and PDA representations asobserved in a translation/alignment task?r How do their respective decoding algorithms perform in relation to thecomplexity analysis described here?r How many times is exact decoding achievable in each case?We will discuss the complexity of both HiPDT and HiFST decoders as well as thehypergraph representation, with an emphasis on Hiero-style SCFGs.
We assess ouranalysis for FSA and PDA representations by contrasting HiFST and HiPDT with largegrammars for translation and alignment.
For convenience, we refer to the hypergraphrepresentation as Th, and to the FSA and PDA representations as Tf and Tp.We first analyze the complexity of each MT step described in the introduction:1.
SCFG Translation: Assuming that the parsing of the input is performed by aCYK parse, then the CFG, hypergraph, RTN, and PDA representations canbe generated in O(|s|3|G|) time and space (Aho and Ullman 1972).
The FSArepresentation can require an additional O(e|s|3|G|) time and space becausethe RTN expansion to FSA can be exponential.2.
Intersection: The intersection of a CFG Th with a finite automaton M can beperformed by the classical Bar-Hillel algorithm (Bar-Hillel, Perles, andShamir 1964) with time and space complexity O(|Th||M|l+1), where l is themaximum number of symbols on the right-hand side of a grammar rule inTh.
Dyer (2010a) presents a more practical intersection algorithm that avoidscreating rules that are inaccessible from the start symbol.
With deterministicM, the intersection complexity becomes O(|Th||M|lN+1), where lN is therank of the SCFG (i.e., lN is the maximum number of nonterminals on theright-hand side of a grammar rule).
With Hiero-styles rules, lN = 2 so thecomplexity is O(|Th||M|3) in that case.5 The PDA intersection algorithmfrom Section 3.3 has time and space complexity O(|Tp||M|).
Finally, the FSAintersection algorithm has time and space complexity O(|Tf ||M|) (Mohri 2009).3.
Shortest Path: The shortest path algorithm on the hypergraph, RTN, andFSA representations requires linear time and space (given the underlyingacyclicity) (Huang 2008; Mohri 2009).
As presented in Section 3.4, the PDArepresentation can require time cubic and space quadratic in |M|.Table 1 summarizes the complexity results for SCFGs of rank 2.
The PDA represen-tation is equivalent in time and superior in space complexity to the CFG/hypergraphrepresentation, in general, and it can be superior in both space and time to the FSArepresentation depending on the relative SCFG and language model (LM) sizes.
TheFSA representation favors smaller target translation grammars and larger languagemodels.5 The modified Bar-Hillel construction described by Chiang (2007) has time and space complexityO(|Th||M|4 ); the modifications were introduced presumably to benefit the subsequent pruning methodemployed (but see Huang, Zhong, & Gildea 2005).706Allauzen et al.
Pushdown Automata in Statistical Machine TranslationTable 1Translation complexity of target language representations for translation grammars of rank 2.Representation Time Complexity Space ComplexityCFG/hypergraph O(|s|3 |G| |M|3) O(|s|3 |G| |M|3 )PDA O(|s|3 |G| |M|3) O(|s|3 |G| |M|2 )FSA O(e|s|3|G| |M|) O(e|s|3|G| |M|)In practice, the PDA and FSA representations benefit greatly from the optimiza-tions mentioned previously (Figure 3 and accompanying discussion).
For the FSArepresentation, these operations can offset the exponential dependencies in the worst-case complexity analysis.
For example, in a translation of a 15-word sentence takenat random from the development sets described later, expansion of an RTN yields aWFSA with 174?
106 states.
By contrast, if the RTN is determinized and minimizedprior to expansion, the resulting WFSA has only 34?
103 states.
Size reductions of thismagnitude are typical.
In general, the original RTN, hypergraph, or CFG representationcan be exponentially larger than the RTN/PDT optimized as described.Although our interest is primarily in Hiero-style translation grammars, which haverank 2 and a relatively small number of nonterminals, this complexity analysis can beextended to other grammars.
For SCFGs of arbitrary rank lN, translation complexity intime for hypergraphs becomes O(|G||s|lN+1|M|lN+1); with FSAs the time complexity be-comes O(e|G||s|lN+1 |M|); and with PDAs the time complexity becomes O(|G||s|lN+1|M|3).For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmannand Venugopal 2006) or GHKM (Galley et al.
2004), this suggests that PDA represen-tations may offer computational advantages in the worst case relative to hypergraphrepresentations, although this must be balanced against other available strategies suchas binarization (Zhang et al.
2006; Xiao et al.
2009) or scope pruning (Hopkins andLangmead 2010).
Of course, practical translation systems introduce various pruningprocedures to achieve much better decoding efficiency than the worst cases given here.We will next describe the translation grammar and language model for our ex-periments, which will be used throughout the remainder of this article (except whenstated otherwise).
In the following sections we assess the complexity discussion with acontrast between HiFST (FSA representation) and HiPDT (PDA representation) underlarge grammars.4.1 Translation Grammars and Language ModelsTranslation grammars are extracted from a subset of the GALE 2008 evaluation par-allel text;6 this is 2.1M sentences and approximately 45M words per language.
Wereport translation results on a development set tune-nw (1,755 sentences) and a test settest-nw (1,671 sentences).
These contain translations produced by the GALE programand portions of the newswire sections of the NIST evaluation setsMT02 throughMT06.76 See http://projects.ldc.upenn.edu/gale/data/catalog.html.We excluded the UN material and theLDC2002E18, LDC2004T08, LDC2007E08, and CUDonga collections.7 See http://www.itl.nist.gov/iad/mig/tests/mt/.707Computational Linguistics Volume 40, Number 3Table 2Number of n-grams with explicit conditional probability estimates assigned by the 4-gramlanguage models M?1 after entropy pruning of M1 at threshold values ?.
Perplexities over the(concatenated) tune-nw reference translations are also reported.
The Kneser-Ney and Katz4-gram LM have 416,190 unigrams, which are not removed by pruning.?
0 7.5?
10?9 7.5?
10?8 7.5?
10?7 7.5?
10?6 7.5?
10?5 7.5?
10?4 7.5?
10?3KN2-grams 28M 10M 2.5M 442K 37K 1.3K 21 03-grams 61M 6M 969K 74K 2.7K 38 0 04-grams 117M 3M 219K 5K 44 0 0 0perplexity 98.1 122.2 171.5 290.4 605.1 1270.2 1883.6 2200.0KATZ2-grams 28M 7M 2M 391K 52K 4K 117 13-grams 64M 10M 1.5M 148K 8.4K 197 1 04-grams 117M 4.6M 398K 19K 510 1 0 0perplexity 106.7 120.4 146.9 210.5 336.6 596.5 905.0 1046.1In tuning the systems, MERT (Och 2003) iterative parameter estimation under IBMBLEU8 is performed on the development set.The parallel corpus is aligned using MTTK (Deng and Byrne 2008) in both source-to-target and target-to-source directions.
We then follow published procedures (Chiang2007; Iglesias et al.
2009b) to extract hierarchical phrases from the union of thedirectional word alignments.
We call a translation grammar (G) the set of rulesextracted from this process.
For reference, the number of rules in G that can apply to thetune-nw is 1.1M, of which 593K are standard non-hierarchical phrases and 511K arestrictly hierarchical rules.We will use two English language models in these translation experiments.
Thefirst language model, denoted M1, is a 4-gram estimated over 1.3B words takenfrom the target side of the parallel text and the AFP and Xinhua portions of theEnglish Gigaword Fourth Edition (LDC2009T13).
We use both Kneser-Ney (Kneserand Ney 1995) and Katz (Katz 1987) smoothing in estimating M1.
Where languagemodel reduction is required, we apply Stolcke entropy pruning (Stolcke 1998) to M1under the relative perplexity threshold ?.
The resulting language model is labeledas M?1 .The reduction in size in terms of component n-grams is summarized in Table 2.For aggressive enough pruning, the original 4-gram model can be effectively reducedto a trigram, bigram, or unigram model.
For both the Katz and the Kneser-Ney 4-gramlanguage models: at ?
= 7.5E?
05 the number of 4-grams in the LM is effectivelyreduced to zero; at ?
= 7.5E?
4 the number of 3-grams is effectively 0; and at?
= 7.5E?
3, only unigrams remain.
Development set perplexities increase as entropypruning becomes more aggressive, with the Katz smoothed model performing betterunder pruning (Chelba et al.
2010; Roark, Allauzen, and Riley 2013).We will also use a larger language model, denoted M2, obtained by interpolat-ing M1 with a zero-cutoff stupid-backoff 5-gram model (Brants et al.
2007) estimatedover 6.6B words of English newswire text; M2 is estimated as needed for the n-gramsrequired for the test sets.8 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl.708Allauzen et al.
Pushdown Automata in Statistical Machine TranslationTable 3Success in finding the 1-best translation under G with various M?1 under a memory size limit of10GB as measured over tune-nw (1,755 sentences).
We note which operations in translationexceeded the memory limit: either Expansion and Intersection for HiFST, or Intersection andShortest Path operation for HiPDT.Decoding with G + M?1 under a 10GB memory size limit# ?
HiFST HiPDTSuccess Failure Success FailureExpansion Intersection Intersection Shortest Path2 7.5?
10?9 12% 51% 37% 40% 8% 52%3 7.5?
10?8 16% 53% 31% 76% 1% 23%4 7.5?
10?7 18% 53% 29% 99.8% 0% 0.2%4.2 Exact Decoding with Large Grammars and Small LanguageModelsWe now compare HiFST and HiPDT in translation with our large grammar G. In thiscase we know that exact search is often not feasible for HiFST.We run both decoders over tune-nw with a restriction on memory use of 10 GB.If this limit is reached in decoding, the process is killed.9 Table 3 shows the numberof times each decoder succeeds in finding a hypothesis under the memory limit whendecoding with various entropy-pruned LMs M?1 .
With ?=7.5?
10?9 (row 2), HiFSTcan only decode 218 sentences, and HiPDT succeeds in 703 cases.
The difference insuccess rates between the decoders is more pronounced as the language model is moreaggressively pruned: for ?=7.5?
10?7 HiPDT succeeds for all but three sentences.As Table 3 shows, HiFST fails most frequently in its initial expansion from RTNto FSA; this operation depends only on the translation grammar and does not benefitfrom any reduction in the language model size.
Subsequent intersection of the FSAwith the language model can still pose a challenge, although as the language modelis reduced, this intersection fails less often.
By contrast, HiPDT intersects the translationgrammar with the language model prior to expansion and this operation nearly alwaysfinishes successfully.
The subsequent shortest path (or pruned expansion) operation isprone to failure, but the risk of this can be greatly reduced by using smaller languagemodels.In the next section we contrast both HiPDT and HiFST for alignment.4.3 Alignment with Inversion Transduction GrammarsWe continue to explore applications characterized by large translation grammars Gand small language models M. As an extreme instance of a problem involving a largetranslation grammar and a simple target language model, we consider parallel textalignment under an Inversion Transduction Grammar (ITG) (Wu 1997).
This task, orsomething like it, is often done in translation grammar induction.
The process shouldyield the set of derivations, with scores, that generate the target sentence as a translation9 We use the UNIX ulimit command.
The experiment was carried out over machines with differentconfigurations and loads, so these numbers should be considered as approximate values.709Computational Linguistics Volume 40, Number 3of the source sentence.
In alignment the target language model is extremely simple:It is simply an acceptor for the target language sentence so that |M| is linear in thelength of the target sentence.
In contrast, the search space needs now to be representedwith pushdown transducers (instead of pushdown automata) keeping track of bothtranslations and derivations, that is, indices of the rules in the grammar (Iglesias et al.2009a; de Gispert et al.
2010; Dyer 2010b).We define a word-based translation grammar GITG for the alignment problem asfollows.
First, we obtain word-to-word translation rules of the form X?
?s, t?
basedon probabilities from IBM Model 1 translation tables estimated over the parallel text,where s and t are one source and one target word, respectively (?16M rules).
Then,we allow monotonic and inversion transduction of two adjacent nonterminals in theusual ITG style (i.e., add X?
?X1 X2, X1 X2?
and X?
?X1 X2, X2 X1?).
Additionally,we allow unrestricted source word deletions (X?
?s, ??
), and restricted target wordinsertions (X?
?X1 X2, X1 t X2?).
This restriction, which is solely motivated by ef-ficiency reasons, disallows the insertion of two consecutive target words.
We makeno claims about the suitability or appropriateness of this specific grammar for eitheralignment or translation; we introduce this grammar only to define a challengingalignment task.A set of 2,500 sentence pairs of up to 50 source and 75 target words was chosenfor alignment.
These sentences come from the same Chinese-to-English parallel datadescribed in Section 4.1.
Hard limits on memory usage (10GB) and processing time(10 minutes) were imposed for processing each sentence pair.
If HiPDT or HiFST ex-ceeded either limit in aligning any sentence pair, alignment was stopped and a ?mem-ory/time failure?
was noted.
Even if the resource limits are not exceeded, alignmentmay fail due to limitations in the grammar.
This happens when either a particular wordpair rule that is not in our Model 1 table, or more than one consecutive target insertionsare needed to reach alignment.
In such cases, we record a ?grammar failure,?
as opposedto a ?memory/time failure.
?Results are reported in Table 4.
Of the 2,500 sentence pairs, HiFST successfullyaligns only 41% of the sentence pairs under these time and memory constraints.
Thereason for this low success rate is that HiFST must generate and expand all possiblederivations under the ITG for a given sentence pair.
Even if it is strictly enforcedthat the FSA in every CYK cell contains only partial derivations which produce sub-strings of the target sentence, expansion often exceeds the memory/time constraints.In contrast, HiPDT succeeds in aligning all sentence pairs that can be aligned underthe grammar (89%), because it never fails due to memory or time constraints.
In thisexperiment, if alignment is at all possible, HiPDT will find the best derivation.
Align-ment success rate (or coverage) could trivially be improved by modifying the ITG toallow more consecutive target insertions, or by increasing the number of word-to-wordTable 4Percentages of success and failure in aligning 2,500 sentence pairs under GITG with HiFST andHiPDT.
HiPDT finds an alignment whenever it is possible under the translation grammar.HiFST HiPDTSuccess Failure Success Failurememory/time grammar memory/time grammar41% 53% 6% 89% 0% 11%710Allauzen et al.
Pushdown Automata in Statistical Machine Translationrules, but that would not change the conclusion in the contrast between HiFST andHiPDT.The computational analysis from the beginning of this section applies to alignment.The language model M is replaced by an acceptor for the target sentence, and if weassume that the target sentence length is proportional to the source sentence length, itfollows that |M| ?
|s| and the worst-case complexity for HiPDT in alignment mode isO(|s|6|G|).
This is comparable to ITG alignment (Wu 1997) and the intersection algo-rithm of Dyer (2010b).Our experimental results support the complexity analysis summarized in Table 1.HiPDT is more efficient in ITG alignment and this is consistent with its linear depen-dence on the grammar size, whereas HiFST suffers from its exponential dependence.This use of PDAs in alignment does not rely on properties specific either to Hieroor to ITGs.
We expect that the approach should be applicable with other types ofSCFGs, although we note that alignment under SCFGs with an arbitrary number ofnonterminals can be NP-hard (Satta and Peserico 2005).5.
HiPDT Two-Pass Translation Architecture and ExperimentsThe previous complexity analysis suggests that PDAs should excel when used withlarge translation grammars and relatively small n-gram language models.
In hierar-chical phrase-based translation, this is a somewhat unusual scenario: It is far moretypical that translation tasks requiring a large translation grammar also require largelanguage models.
To accommodate these requirements we have developed a two-pass decoding strategy in which a weak version of a large language model is ap-plied prior to the expansion of the PDA, after which the full language model isapplied to the resulting WFSA in a rescoring pass.
An effective way of generatingweak language models is by means of entropy pruning under a threshold ?
; these arethe language models M?1 of Section 4.1.
Such a two-pass strategy is widely used inautomatic speech recognition (Ljolje, Pereira, and Riley 1999).
The steps in two-passtranslation using entropy-pruned language models are given here, and depicted inFigure 14.Step 1.
We translate with M?1 and G using the same parameters obtained by MERTfor the baseline system, with the exception that the word penalty parameteris adjusted to produce hypotheses of roughly the correct length.
This producestranslation lattices that contain hypotheses with exact scores under G and M?1 :?2({s} ?
G) ?M?1 .Step 2.
These translation lattices are pruned at beamwidth ?
: [?2({s} ?
G) ?M?1 ]?.Step 3.
We remove the M?1 scores from the pruned translation lattices, reapply the fulllanguage model M1, and restore the word penalty parameter to the baselinevalue obtained by MERT.
This gives an approximation to ?2({s} ?
G) ?M1:scores are correctly assigned underG andM1, but only hypotheses that survivedpruning at Step 2 are included.We can rescore the lattices produced by the baseline system or by the two-passsystem with the larger language model M2.
If ?=?
or if ?=0, the translation latticesobtained in Step 3 should be identical to lattices produced by the baseline system (i.e.,the rescoring step is no longer needed).
The aim is to increase ?
to shrink the languagemodel used at Step 1, but ?
will then have to increase accordingly to avoid pruningaway desirable hypotheses in Step 2.711Computational Linguistics Volume 40, Number 3CYK parses with G Build RTNRTN to PDAReplacementIntersect PDAwith WFSA M1?PDA to FSAPruned Expansion,threshold BIntersect FSA with LM M1FSAShortestPathFSAPruningLattice1-Best HypothesisRemoveLM scoresEntropy Pruning,threshold ?LM M1(as WFSA)further rescoringFigure 14Two-pass HiPDT translation with an entropy pruned language model.5.1 Efficient Removal of First-Pass Language Model Scores UsingLexicographic SemiringsThe two-pass translation procedure requires removal of the weak language modelscores used in the initial expansion of the translation search space; this is done sothat only the translation scores under G remain after pruning.
In the tropical semiring,the weak LM scores can be ?subtracted?
at the path level from the lattice, but thisinvolves a determinization of an unweighted translation lattice, which can be veryinefficient.As an alternative we can define a lexicographic semiring (Shafran et al.
2011;Roark, Sproat, and Shafran 2011) ?w1,w2?
over the tropical weights w1 and w2 with theoperations ?
and ?:?w1,w2?
?
?w3,w4?
={?w1,w2?
if w1 < w3 or (w1 = w3 and w2 < w4)?w3,w4?
otherwise (8)?w1,w2?
?
?w3,w4?
= ?w1 + w3,w2 + w4?
(9)The PDA algorithms described in Section 3 are valid under this new semiring becauseit is commutative and has the path property.
In particular, the PDA representing {s} ?
Gis constructed so that the translation grammar score appears in both w1 and w2 (i.e., it isduplicated).
In the first-pass language model, w1 has the n-gram language model scoresand the w2 are 0.
After composition, the resulting automata have the combined trans-lation grammar score and language model score in the first dimension, and the seconddimension contains the translation grammar scores alone.
Pruning can be performedunder the lexicographic semiring with a threshold set so that only the combined scoresin the first dimension are considered.
The resulting automata can easily be mapped backinto the regular tropical semiring such that only the translation scores in the second712Allauzen et al.
Pushdown Automata in Statistical Machine Translationdimension are retained (this is a linear operation done by the fstmap operation in theOpenFST library).5.2 Translation Quality and Modeling Errors in Two-Pass DecodingWe wish to analyze the degree to which the two-pass decoding strategy introduces?modeling errors?
into translation.
A modeling error occurs in two-pass decodingwhenever the decoder produces a translation whose score is less than the best attainableunder the grammar and language model (i.e., whenever the best possible translationis discarded by pruning at Step 2).
We refer to these as modeling errors, rather thansearch errors, because they are due to differences in scores assigned by the modelsM1 and M?1 .Ideally, we would compare the two-pass translation system against a baseline sys-tem that performs exact translation, without pruning in search, under the grammar Gand language model M1.
This would allow us to address the following questions:r Is a two-pass decoding procedure that uses entropy-pruned languagemodels adequate for translation?
How many modeling errors areintroduced?
Does two-pass decoding impact on translation quality?r Which smoothing/discounting technique is best suited for the first-passlanguage model in two-pass translation, and which smoothing/discounting technique is best at avoiding modeling errors?Our grammar G is not suitable for these experiments, as we do not have a systemcapable of exact decoding under both G and M1.
To create a suitable baseline we there-fore reduce G by excluding rules that have a forward translation probability p < 0.01,and refer to this reduced grammar as Gsmall.
This process reduces the number of strictlyhierarchical rules that apply to our tune-nw set from 511K to 189K, while the number ofstandard phrases is unchanged.Under Gsmall, both HiFST and HiPDT are able to exactly compose the entire space ofpossible candidate hypotheses with the language model and to extract the shortest pathhypothesis.
Because an exact decoding baseline is thus available, we can empiricallyevaluate the proposed two-pass strategy.
Any degradation in translation quality canonly be due to the modeling errors introduced by pruning under ?
with respect to theentropy-pruned M?1 .Figure 15 shows translation performance under grammar Gsmall for different valuesof entropy pruning threshold ?.
Performance is reported after first-pass decoding withM?1 (Step 1, Section 5), and after rescoring with M1 (Step 3, Section 5) the first-passlattices pruned at alternative ?
beams.
The first column reports the baseline for eitherKneser-Ney andKatz languagemodels, which are found by translation without entropypruning, that is, performed with M1.
Both yield 34.5 on test-nw.The first and main conclusion from this figure is that the two-pass strategy is ade-quate because we are always able to recover the baseline performance.
As expected, theharsher the entropy-pruning ofM1 (as we lower ?)
the greater?must be to recover fromthe significant degradation in first-pass decoding.
But even at a harsh ?
= 7.5?
10?7,when first-pass performance drops over 7 BLEU points, a relatively-low value of ?
= 15can recover the baseline performance.Although this is true independently of the LM smoothing approach, a secondconclusion from the figure is that the choice of LM smoothing does impact first-pass713Computational Linguistics Volume 40, Number 3Figure 15Results (lower case IBM BLEU scores over test-nw) under Gsmall with various M?1 as obtainedwith several values of ?.
Performance in subsequent rescoring with M1 after likelihood-basedpruning of the resulting translation lattices for various ?
is also reported.
In the pipeline, M1(and M?1 ) are estimated with either Katz or Kneser-Ney smoothing.translation performance.
For entropy pruning at ?
= 7.5?
10?7, the Katz LMs performbetter for smaller beamwidths ?.
These results are consistent with the test setperplexities of the entropy pruned LMs (Table 2), and are also in line with other studiesof Kneser-Ney smoothing and entropy pruning (Chelba et al.
2010; Roark, Allauzen,and Riley 2013).Modeling errors are reported in Table 5 at the entropy pruning threshold ?
= 7.5?10?7.
As expected, modeling errors decrease as the beamwidth ?
increases, althoughwe find that the language model with Katz smoothing has fewer modeling errors.However, modeling errors do not necessarily impact corpus level BLEU scores.
For widebeamwidths (e.g., ?
= 15 here), there are still some modeling errors, but these are eitherfew enough or subtle enough that two-pass decoding under either smoothing methodyields the same corpus level BLEU score as the exact decoding baseline.Table 5Two-pass translation modeling errors as a function of RTN expansion pruning threshold ?.
Amodeling error occurs whenever the score of a hypothesis produced by the two-pass translationdiffers from the score found by the exact baseline system.
Errors are tabulated over systemsreported in Figure 15, at ?
= 7.5?
10?7.?
Kneser-Ney Katz8 814 61912 343 21215 240 110714Allauzen et al.
Pushdown Automata in Statistical Machine Translation5.3 HIPDT Two-Pass Decoding Speed and Translation Performancer What are the speed and quality tradeoffs for HiPDT as a function offirst-pass LM size and translation grammar complexity?r How do these compare against the predicted computational complexity?In this section we turn back to the original large grammar, for which HiFST cannotperform exact decoding (see Table 3).
In contrast, HiPDT is able to do exact decodingso we study tradeoffs in speed and translation performance.
The speed of two-passdecoding can be increased by decreasing ?
and/or increasing ?, but at the risk ofdegradation in translation performance.
For grammar G and language model M1 weplot in Figure 16 the BLEU score against speed as a function of ?
for a selection of ?values.
BLEU score is measured over the entire test set test-nw but speed is calculatedonly on sentences of length up to 20 words (?500 sentences).
In computing speed wemeasure not only the PDA operations, but the entire HiPDT decoding process describedin Figure 14, including CYK parsing and the application of M1.
We note in passing thatthese unusually slow decoding speeds are a consequence of the large grammars, lan-guage models, and broad pruning thresholds chosen for these experiments; in practice,translation with either HiPDT or HiFST is much faster.In these experiments we find that the language model entropy pruning threshold?
and the likelihood beamwidth ?
work together to balance speed against translationquality.
For every entropy pruning threshold ?
value considered, there is a value of ?for which there is no degradation in translation quality.
For example, suppose we wantto attain a translation quality of 34.5 BLEU: then ?
should be set to 12 or greater.
If thegoal is to find the fastest system at this level, then we choose ?
= 7.5?
10?5.The interaction between pruning in expansion and pruning of the language modelis explained by Figure 17, where decoding and rescoring times are shown for variousFigure 16HiPDT translation quality versus speed (decoding with G, M?1 + rescoring with M1) underdifferent entropy pruning thresholds ?
and for likelihood beamwidths ?
= 15, 12, 9, 8, 7.715Computational Linguistics Volume 40, Number 3Figure 17Accumulated decoding+rescoring times for HiPDT under different entropy pruning thresholds,reaching a performance of at least 34.5 BLEU, for which ?
is set to 12.values of ?
and ?
that achieve at least the translation quality target of 34.5.
As ?increases, decoding time decreases because a smaller language model is easier to apply;however, rescoring times increase, because the larger values of ?
lead to larger WFSAsafter expansion, and these are costly to rescore.
The balance occurs at ?
= 7.5?
10?5and a translation rate of 3.0 words/sec.
In this case, entropy pruning yields a severelyshrunken bigram language model, but this may vary depending on the translationgrammar and the original, unpruned LM.5.4 Rescoring with 5-Gram Language Models and LMBR Decodingr Does the HiPDT two-pass decoding generate lattices that can be useful inrescoring?We now report on rescoring experiments using WFSAs produced by the two-passHiPDT translation system under the large translation grammar G. We demonstrate thatHiPDT can be used to generate large, compact representations of the translation spacethat are suitable for rescoring with large language models or by alternative decodingprocedures.
We investigate translation performance by applying versions of the lan-guage model M2 estimated with stupid backoff.
We also investigate minimum Bayesrisk (MBR) decoding (Kumar and Byrne 2004) as an alternative search strategy.
We areparticularly interested in lattice MBR (LMBR) (Tromble et al.
2008), which is well suitedfor the large WFSAs that the system can generate; we use the implementation describedby Blackwood, de Gispert, & Byrne (2010).
There are two parameters to be tuned: ascaling parameter to normalize the evidence scores and a word penalty applied to thehypotheses space; these are tuned jointly on the tune-nw set.
Results are reported inFigure 18.We note first that rescoring with the large language model M2, which is effectivelyinterpolated with M1, gives consistent gains over initial results obtained with M1 alone.After 5-gram rescoring there is already +0.5 BLEU improvement compared with Gsmall.With a richer translation grammar we have generated a richer lattice that allows gainsto be gotten by our lattice rescoring techniques.716Allauzen et al.
Pushdown Automata in Statistical Machine TranslationFigure 18HiPDT decoding with G. Decoding language model M?1 and first pass rescoring language modelM1 are Katz.
Results on test-nw are given for ML-Decoding under the 5-gram stupid backofflanguage model (?5gML?)
and for LMBR and for LMBR decoding.
Parameter values are?
= 15, 12, 9, 8 and ?
= 7.5?
10?7 , 7.5?
10?5, 7.5?
10?3.We also find that BLEU scores degrade smoothly as ?
decreases and the expansionpruning beamwidth narrows, and at all values of ?
LMBR gives improvement overthe MAP hypotheses.
Because LMBR relies on posterior distributions over n-grams, weconclude that HiPDT is able to generate compact representations of large search spaceswith posteriors that are robust to pruning conditions.Finally, we find that increasing ?
degrades performance quite smoothly for ?
?
9.Again, with appropriate choices of ?
and ?
we can easily reach a compromise betweendecoding speed and final performance of our HiPDT system.
For instance, with ?
=7.5?
10?7 and?
= 12, for whichwe decode at a rate of 3words/sec as seen in Figure 16,we are losing only 0.5 BLEU after LMBR compared to ?
= 7.5?
10?7 and ?
= 15.6.
RelatedWorkThere is extensive prior work on computational efficiency and algorithmic complexityin hierarchical phrase-based translation.
The challenge is to find algorithms that can bemade to work with large translation grammars and large language models.Following the original algorithms and analysis of Chiang (2007), Huang andChiang (2007) developed the cube-growing algorithm, and more recently Huang andMi (2010) developed an incremental decoding approach that exploits the left-to-rightnature of n-gram language models.Search errors in hierarchical translation, and in translation more generally, havenot been as extensively studied; this is undoubtedly due to the difficulties inherent infinding exact translations for use in comparison.
Using a relatively simple phrase-basedtranslation grammar, Iglesias et al.
(2009b) compared search via cube-pruning to anexact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruningsuffered significant search errors.
For Hiero translation, an extensive comparison ofsearch errors between the cube pruning and FSA implementation was presented byIglesias et al.
(2009a) and de Gispert et al.
(2010).
The effect of search errors has also been717Computational Linguistics Volume 40, Number 3studied in phrase-based translation by Zens andNey (2008).
Relaxation techniques havealso recently been shown to find exact solutions in parsing (Koo et al.
2010), phrase-based SMT (Chang and Collins 2011), and in tree-to-string translation under trigramlanguage models (Rush and Collins 2011); this prior work involved much smallergrammars and languages models than have been considered here.Efficiency in synchronous parsing with Hiero grammars and hypergraphs has beenstudied previously by Dyer (2010b), who showed that a single synchronous parsing al-gorithm (Wu 1997) can be significantly improved upon in practice through hypergraphcompositions.
We developed similar procedures for our HiFST decoder (Iglesias et al.2009a; de Gispert et al.
2010) via a different route, after noting that with the space oftranslations represented as WFSAs, alignment can be performed using operations overWFSTs (Kumar and Byrne 2005).Although entropy-pruned language models have been used to produce real-timetranslation systems (Prasad et al.
2007), we believe our use of entropy-pruned languagemodels in two-pass translation to be novel.
This is an approach that is widely used inautomatic speech recognition (Ljolje, Pereira, and Riley 1999) and we note that it relieson efficient representation of very large search spaces T for subsequent rescoring, as ispossible with FSAs and PDAs.7.
ConclusionIn this article, we have described a novel approach to hierarchical machine translationusing pushdown automata.
We have presented fundamental PDA algorithms includingcomposition, shortest-path, (pruned) expansion, and replacement and have shown howthese can be used in PDA-based machine translation decoding and how this relates toand compares with hypergraph and FSA-based decoding.On the basis of the experimental results presented in the previous sections, we cannow address the questions laid out in Sections 4 and 5:r A two-pass translation decoding procedure in which translation is firstperformed with a weak entropy-pruned language model and followed byadmissible likelihood-based pruning and rescoring with a full languagemodel can yield good quality translations.
Translation performance doesnot degrade significantly unless the first-pass language model is veryheavily pruned.r As predicted by the analysis of algorithmic complexity, intersection andexpansion algorithms based on the PDA representation are able toperform exact decoding with large translation and weak language models.By contrast, RTN to FSA expansion fails with large translation grammars,regardless of the size of the language model.
With large translationgrammars, language model composition prior to expansion may be moreattractive than expansion prior to language model composition.r Our experimental results suggest that for a translation grammar and alanguage model of a particular size, and given a value of language modelentropy pruning threshold ?, there is a value of the pruned expansionparameter ?
for which there is no degradation in translation quality withHiPDT.
This makes exact decoding under large translation grammarspossible.
The values of ?
and ?
will be grammar- and task-dependent.718Allauzen et al.
Pushdown Automata in Statistical Machine Translationr Although there is some interaction between parameter tuning, pruningthresholds, and language modeling strategies, the variation is notsignificant enough to indicate that a particular language model orsmoothing technique is best.
This is particularly true if minimum Bayesrisk decoding is applied to the output translation lattices.Several questions naturally arise about the decoding strategies presented here.
Oneis whether inadmissible pruning methods can be applied to the PDA-based systems thatare analogous to those used in current hypergraph-based systems such as cube-pruning(Chiang 2007).
Another is whether a hybrid PDA?FSA system, where some parts of thePDA are pre-expanded and some not, could provide benefits over full pre-expansion(FSA) or none (PDA).
We leave these questions for future work.Appendix A.
Composition of a Weighted PDT and a Weighted FSTGiven a pair (T1,T2) where T1 is a weighted pushdown transducer and the T2 is aweighted finite-state transducer, and such that T1 has input and output alphabets ?and ?
and T2 has input and output alphabets ?
and ?, then there exists a weightedpushdown transducer T1 ?
T2, which is the composition of T1 and T2, such that for all(x, y) ?
??
?
??
:T = (T1 ?
T2)(x, y) = minz???
(T1(x, z)+ T2(z, y)) (A.10)We also assume that T2 has no input-?
transitions, noting that for T2 with input-?
tran-sitions, an epsilon filter (Mohri 2009; Allauzen, Riley, and Schalkwyk 2011) generalizedto handle parentheses could be used.A state in T is a pair (q1, q2) where q1 is a state of T1 and q2 a state of T2.
Given atransition e1 = (q1, a, b,w1, q?1) in T1, transitions out of (q1, q2) in T are obtained using thefollowing rules.
If b ?
?, then e1 can be matched with a transition (q2, b, c,w2, q?2) in T2resulting in a transition ((q1, q2), a, c,w1 + w2, (q?1, q?2)) in T. If b = ?, then e1 is matchedwith staying in q2 resulting in a transition ((q1, q2), a, ?,w1, (q?1, q2)).
Finally, if b = a ?
?
?,e1 is also matched with staying in q2, resulting in a transition ((q1, q2), a, a,w1, (q?1, q2)) inT.
The initial state is (I1, I2) and a state (q1, q2) in T is final when both q1 and q2 are bothfinal.
Weight values are assigned as ?
((q1, q2)) = ?1(q1)+ ?2(q2).Appendix B. Pruned ExpansionLet dR and BR be the data structures computed by the shortest-distance algorithmapplied to TR.
For a state q in T?
(or equivalently T??
), let d[q] denote the shortest distancefrom the initial state to q, d[q] denote the shortest distance from q to the final state, ands[q] denote the destination state of the last unbalanced open-parenthesis transition on ashortest path from the initial state to q.The algorithm is based on the following property: Letting e denote a transition inT?
such that p[e] = (q, z) and z = z?a, the weight of a shortest path through e can beexpressed as:d[(q, z)]+ w[e]+ mine?
?BR[qs ,a]dR[n[e], p[e?
]]+ w[e?
]+ d[(n[e?
], z?)]
(B.11)719Computational Linguistics Volume 40, Number 3PRUNEDEXPANSION(T, ?
)1 (dR,BR)?
SHORTESTDISTANCE (TR )2 ??
dR[I, f ]+?
?
Compute the pruning threshold3 B?
REVERSE (BR ) ?
Compute the balance information in T from the one in TR4 (I?, f ?
)?
((I,?
), ( f,?))
?
I?
and f ?
are the initial and final states of the pruned expansion5 (F?,??
( f ?))?
({ f ?
}, 0)6 (d[I?
], s[I?])?
(0, I?
)7 (d[I?
], d[ f ?])?
(dR[I, f ], 0)8 (zD,D[ f ])?
(?, 0)9 S?
Q??
{I?
}10 while S 6=?
do11 (q, z)?
HEAD(S)12 DEQUEUE (S)13 if s[(q, z)]= (q, z) then14 if z 6= zD then ?
If the stack has changed, D needs to be cleared and recomputed15 CLEAR (D)16 zD?
z17 for each e ?
B[q, z|z|] do ?
For each close paren.
transition balancing the incoming z|z|-labeled open paren.
transition in q18 D[p[e]]?
min(D[p[e]],w[e]+ d[(n[e], z1 ?
?
?
z|z|?1 )])19 for each e ?
E[q] do20 if i[e] ?
??
{?}
then ?
If i[e] is a regular symbol21 if RETAINPATH (q, z,w[e],n[e]) then22 E??
E?
?
{((q, z), i[e], o[e],w[e], (n[e], z))}23 elseif i[e] ??
then ?
If i[e] is an open parenthesis24 z??
zi[e]25 r?
false26 for each e?
?
B[n[e], i[e]] do ?
For each close paren.
transition e?
that balances e27 w?
w[e]+ dR[n[e],p[e?]]+w[e?]
?
w: weight of the shortest bal.
path beginning by e and ending by e?
in T28 r?
r?
RETAINPATH (q, z,w,n[e?])
?
Does the expansion of that path belong to an accepting path below threshold?29 wF?min(wF, dR[n[e], p[e?]]+w[e?
]+ d[(n[e?
], z)])30 if r then ?
If any of the paths considered above are below threshold31 E??
E?
?
{((q, z),?,?,w[e], (n[e], z?
))}32 PROCESSSTATE ((n[e], z?
))33 s[(n[e], z?)]?
(n[e], z?
)34 d[(n[e], z?
)]?min(d[(n[e], z?
)], d[(q, z)]+w[e])35 d[(n[e], z?
)]?min(d[(n[e], z?
)],wF )36 elseif i[e] ??
and c?
(zi[e]) ?
??
then ?
If i[e] is the close parenthesis matching the top of the stack37 z??
c?
(zi[e])38 if d[(q, z)]+w[e]+ d[(n[e], z? )]
?
?
then39 E??
E?
?
{((q, z),?,?,w[e], (n[e], z?
))}40 return (?,?,?,?,Q?,E?, I?,F?,??
)RETAINPATH(q, z,w, q?
)1 ?
Returns true iff a path from (q, z) to (q?, z) with weight w belongs to an accepting path below threshold2 wI?
d[(q, z)]+w ?
Shortest distance from I to (q?, z) when taking a path from (q, z) to (q?, z) of weight w3 wF?
min{dR[q?, t]+D[t]|D[t] 6=?}?
Current estimate of s. d. from (q?, z) to f ?4 if wI < d[(q?
, z)] then ?
If wI is a better estimate of s.-d. from I?
to (q?, z), update d[(q?, z)] and s[(q?, z)]5 d[(q?
, z)]?
wI6 s[(q?, z)]?
s[(q, z)]7 if wF < d[(q?, z)] then ?
If wF is a better estimate of s. d. from (q?, z) to f ?
, update d[(q?, z)]8 d[(q?, z)]?
wF9 if ?
< wI +wF then ?
wI +wF: min.
weight of an accepting path taking a path of weight w from (q, z) to (q?, z)10 return false11 PROCESSSTATE ((q?
, z))12 return truePROCESSSTATE((q, z))1 if (q, z) 6?
Q?
then ?
If state (q, z) does not exist yet, create it and add it to the queue2 Q??
Q?
?
{(q, z)}3 ENQUEUE (S, (q, z))Figure 19PDT pruned expansion algorithm.
We assume that F={ f} and ?
( f )=0 to simplify thepresentation.720Allauzen et al.
Pushdown Automata in Statistical Machine Translationwhere (qs, z) = s[(q, z)].
This implies that assuming when (q, z) is visited, d[(n[e?
], z?)]
isknown; we then have all the required information for deciding whether e should bepruned or retained.
In order to ensure that each state is visited once, we need to ensurethat d[(q, z)] is known when (q, z) is visited so we can apply an A?
queue disciplineamong the states sharing the same stack.Both conditions can be achieved by using a queue discipline defined by a partialorder?
such thatz is a prefix of z?
?
(q, z) ?
(q?, z?)
(B.12)d[(q, z)]+ d[(q, z)] < d[(q?, z)]+ d[(q?, z)]?
(q, z) ?
(q?, z) (B.13)We also assume that all states sharing the same stack will be dequeued consecutively(z 6= z?
?
for all (q, q?
), (q, z) ?
(q?, z?)
or for all (q, q?
), (q?, z?)
?
(q, z)).
This allows us tocache some computations (the D data structure as described subsequently).The pseudo code of the algorithm is given in Figure 19.
First, the shortest distancealgorithm is applied to TR and the absolute pruning threshold is computed accordingly(lines 1?2).
The resulting balanced data information is then reversed (line 3).
The initialand final states are created (lines 4?5) and the d, d, and D data structures are initializedaccordingly (lines 6?8).
The default value in these data structures is assumed to be?.The queue is initialized containing the initial state (line 9).The state (q, z) at the head of the queue is dequeued (lines 10?12).
If (q, z) admits anincoming open-parenthesis transition, B contains the balance information for that stateand D can be updated accordingly (lines 13?18).If e is a regular transition, the resulting transition ((q, z), i[e], o[e],w[e], (n[e], z)) in T?can be pruned using the criterion derived from Equation (B.11).
If it is retained, thetransition is created as well as its destination state (n[e], z) if needed (lines 20?22).If e is an open-parenthesis transition, each balanced path starting by the resultingtransition in T?
and ending by a close-parenthesis transition is treated as a meta-transition and pruned using the same criterion as regular transitions (lines 23?29).
If anyof these meta-transitions is retained, the transition ((q, z), ?, ?,w[e], (n[e], zi[e])) resultingfrom e is created as well as its destination state (n[e], zi[e]) if needed (lines 30?35).If e is a closed-parenthesis transition, it is created if it belongs to a balanced pathbelow the threshold (lines 36?39).Finally, the resulting transducer T??
is returned (line 40).AcknowledgmentsThe research leading to these results hasreceived funding from the European UnionSeventh Framework Programme(FP7-ICT-2009-4) under grant agreementnumber 247762, and was supported in partby the GALE program of the DefenseAdvanced Research Projects Agency,contract no.
HR0011-06-C-0022, and aMay 2010 Google Faculty Research Award.ReferencesAho, Alfred V. and Jeffrey D. Ullman.
1972.The Theory of Parsing, Translation andCompiling, volume 1-2.
Prentice-Hall.Allauzen, Cyril and Michael Riley, 2011.Pushdown Transducers.
http://pdt.openfst.org.Allauzen, Cyril, Michael Riley, and JohanSchalkwyk.
2011.
Filters for efficientcomposition of weighted finite-statetransducers.
In Proceedings of CIAA,volume 6482 of LNCS, pages 28?38.
Blois.Allauzen, Cyril, Michael Riley, JohanSchalkwyk, Wojciech Skut, and MehryarMohri.
2007.
OpenFst: A general andefficient weighted finite-state transducerlibrary.
In Proceedings of CIAA,pages 11?23.
http://www.openfst.org.Bar-Hillel, Y., M. Perles, and E. Shamir.
1964.On formal properties of simple phrase721Computational Linguistics Volume 40, Number 3structure grammars.
In Y. Bar-Hillel,editor, Language and Information: SelectedEssays on their Theory and Application.Addison-Wesley, pages 116?150.Berstel, Jean.
1979.
Transductions andContext-Free Languages.
Teubner.Blackwood, Graeme, Adria` de Gispert,and William Byrne.
2010.
Efficient pathcounting transducers for minimumBayes-risk decoding of statistical machinetranslation lattices.
In Proceedings of theACL: Short Papers, pages 27?32, Uppsala.Brants, Thorsten, Ashok C. Popat, Peng Xu,Franz J. Och, and Jeffrey Dean.
2007.Large language models in machinetranslation.
In Proceedings of EMNLP-ACL,pages 858?867, Prague.Chang, Yin-Wen and Michael Collins.
2011.Exact decoding of phrase-based translationmodels through lagrangian relaxation.In Proceedings of EMNLP, pages 26?37,Edinburgh.Chelba, Ciprian, Thorsten Brants, WillNeveitt, and Peng Xu.
2010.
Studyon interaction between entropypruning and Kneser-Ney smoothing.In Proceedings of Interspeech,pages 2,242?2,245, Makuhari.Chiang, David.
2007.
Hierarchicalphrase-based translation.
ComputationalLinguistics, 33(2):201?228.de Gispert, Adria`, Gonzalo Iglesias, GraemeBlackwood, Eduardo R. Banga, andWilliam Byrne.
2010.
Hierarchicalphrase-based translation with weightedfinite state transducers and shallow-ngrammars.
Computational Linguistics,36(3):201?228.Deng, Yonggang and William Byrne.
2008.HMM word and phrase alignment forstatistical machine translation.
IEEETransactions on Audio, Speech, and LanguageProcessing, 16(3):494?507.Dyer, Chris.
2010a.
A Formal Model ofAmbiguity and its Applications in MachineTranslation.
Ph.D. thesis, University ofMaryland.Dyer, Chris.
2010b.
Two monolingual parsesare better than one (synchronous parse).
InProceedings of NAACL-HLT, pages 263?266,Los Angeles, CA.Galley, M., M. Hopkins, K. Knight, andD.
Marcu.
2004.
What?s in a translationrule.
In Proceedings of HLT-NAACL,pages 273?280, Boston, MA.Hopkins, M. and G. Langmead.
2010.
SCFGdecoding without binarization.
InProceedings of EMNLP, pages 646?655,Cambridge, MA.Huang, Liang.
2008.
Advanced dynamicprogramming in semiring and hypergraphframeworks.
In Proceedings of COLING,pages 1?18, Manchester.Huang, Liang and David Chiang.
2007.Forest rescoring: Faster decoding withintegrated language models.
In Proceedingsof ACL, pages 144?151, Prague.Huang, Liang and Haitao Mi.
2010.
Efficientincremental decoding for tree-to-stringtranslation.
In Proceedings of EMNLP,pages 273?283, Cambridge, MA.Huang, Liang, Hao Zhang, and DanielGildea.
2005.
Machine translation aslexicalized parsing with hooks.
InProceedings of the Ninth InternationalWorkshop on Parsing Technology,Parsing ?05, pages 65?73, Vancouver.Iglesias, Gonzalo, Adria` de Gispert,Eduardo R. Banga, and William Byrne.2009a.
Hierarchical phrase-basedtranslation with weighted finite statetransducers.
In Proceedings of NAACL-HLT,pages 433?441, Boulder, CO.Iglesias, Gonzalo, Adria` de Gispert,Eduardo R. Banga, and William Byrne.2009b.
Rule filtering by pattern for efficienthierarchical translation.
In Proceedings ofEACL, pages 380?388, Athens.Katz, Slava M. 1987.
Estimation ofprobabilities from sparse data for thelanguage model component of a speechrecognizer.
IEEE Transactions on Acoustics,Speech, and Signal Processing, 35(3):400?401.Kneser, Reinhard and Herman Ney.
1995.Improved backing-off for m-gramlanguage modeling.
In Proceedings ofICASSP, volume 1, pages 181?184,Detroit, MI.Koo, Terry, Alexander M. Rush, MichaelCollins, Tommi Jaakkola, and DavidSontag.
2010.
Dual decomposition forparsing with non-projective headautomata.
In Proceedings of EMNLP,pages 1,288?1,298, Cambridge, MA.Kuich, Werner and Arto Salomaa.
1986.Semirings, automata, languages.
Springer.Kumar, Shankar and William Byrne.2004.
Minimum Bayes-risk decodingfor statistical machine translation.
InProceedings of HLT-NAACL, pages 169?176,Boston, MA.Kumar, Shankar and William Byrne.2005.
Local phrase reorderingmodelsfor statistical machine translation.In Proceedings of EMNLP-HLT,pages 161?168, Rochester, NY.Kumar, Shankar, Yonggang Deng, andWilliam Byrne.
2006.
A weighted finite722Allauzen et al.
Pushdown Automata in Statistical Machine Translationstate transducer translation templatemodel for statistical machine translation.Natural Language Engineering, 12(1):35?75.Lang, Bernard.
1974.
Deterministictechniques for efficient non-deterministicparsers.
In Proceedings of ICALP,pages 255?269, Saarbru?cken.Ljolje, Andrej, Fernando Pereira, andMichael Riley.
1999.
Efficient general latticegeneration and rescoring.
In Proceedings ofEurospeech, pages 1,251?1,254, Budapest.Mohri, Mehryar.
2002.
Semiring frameworksand algorithms for shortest-distanceproblems.
Journal of Automata, Languagesand Combinatorics, 7:321?350.Mohri, Mehryar.
2009.
Weighted automataalgorithms.
In M. Drosde, W. Kuick,and H. Vogler, editors, Handbook ofWeighted Automata.
Springer, chapter 6,pages 213?254.Nederhof, Mark-Jan and Giorgio Satta.
2003.Probabilistic parsing as intersection.
InProceedings of 8th International Workshop onParsing Technologies, pages 137?148, Nancy.Nederhof, Mark-Jan and Giorgio Satta.
2006.Probabilistic parsing strategies.
Journal ofthe ACM, 53(3):406?436.Och, Franz J.
2003.
Minimum error ratetraining in statistical machine translation.In Proceedings of ACL, pages 160?167,Sapporo.Petre, Ion and Arto Salomaa.
2009.Algebraic systems and pushdownautomata.
In M. Drosde, W. Kuick,and H. Vogler, editors, Handbook ofWeighted Automata.
Springer, chapter 7,pages 257?289.Prasad, R., K. Krstovski, F. Choi, S. Saleem,P.
Natarajan, M. Decerbo, and D. Stallard.2007.
Real-time speech-to-speechtranslation for PDAs.
In Proceedingsof IEEE International Conference onPortable Information Devices, pages 1?5,Orlando, FL.Roark, Brian, Cyril Allauzen, andMichael Riley.
2013.
Smoothed marginaldistribution constraints for languagemodeling.
In Proceedings of ACL,pages 43?52, Sofia.Roark, Brian, Richard Sproat, and IzhakShafran.
2011.
Lexicographic semirings forexact automata encoding of sequencemodels.
In Proceedings of ACL-HLT,pages 1?5, Portland, OR.Rush, Alexander M. and Michael Collins.2011.
Exact decoding of syntactictranslation models through lagrangianrelaxation.
In Proceedings of ACL-HLT,pages 72?82, Portland, OR.Satta, Giorgio and Enoch Peserico.
2005.Some computational complexity resultsfor synchronous context-free grammars.In Proceedings of HLT-EMNLP,pages 803?810, Vancouver.Shafran, Izhak, Richard Sproat, MahsaYarmohammadi, and Brian Roark.2011.
Efficient determinization oftagged word lattices using categorialand lexicographic semirings.
InProceedings of ASRU, pages 283?288,Honolulu, HI.Stolcke, Andreas.
1995.
An efficientprobabilistic context-free parsingalgorithm that computes prefixprobabilities.
Computational Linguistics,21(2):165?201.Stolcke, Andreas.
1998.
Entropy-basedpruning of backoff language models.In Proceedings of DARPA BroadcastNews Transcription and UnderstandingWorkshop, pages 270?274, Landsdowne,VA.Tromble, Roy, Shankar Kumar, Franz J. Och,and Wolfgang Macherey.
2008.
Latticeminimum Bayes-risk decoding forstatistical machine translation.
InProceedings of EMNLP, pages 620?629,Edinburgh.Wu, Dekai.
1997.
Stochastic inversiontransduction grammars and bilingualparsing of parallel corpora.
ComputationalLinguistics, 23:377?403.Xiao, Tong, Mu Li, Dongdong Zhang,Jingbo Zhu, and Ming Zhou.
2009.
Bettersynchronous binarization for machinetranslation.
In Proceedings of EMNLP,pages 362?370, Singapore.Zens, Richard and Hermann Ney.
2008.Improvements in dynamic programmingbeam search for phrase-based statisticalmachine translation.
In Proceedings ofIWSLT, pages 195?205, Honolulu, HI.Zhang, Hao, Liang Huang, Daniel Gildea,and Kevin Knight.
2006.
Synchronousbinarization for machine translation.
InProceedings of HLT-NAACL, pages 256?263,New York, NY.Zollmann, Andreas and Ashish Venugopal.2006.
Syntax augmented machinetranslation via chart parsing.
In Proceedingsof NAACL Workshop on Statistical MachineTranslation, pages 138?141, New York, NY.723
