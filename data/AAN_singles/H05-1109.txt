Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 867?874, Vancouver, October 2005. c?2005 Association for Computational LinguisticsOCR Post-Processing for Low Density LanguagesOkan KolakComputer Science and UMIACSUniversity of MarylandCollege Park, MD 20742okan@umiacs.umd.eduPhilip ResnikLinguistics and UMIACSUniversity of MarylandCollege Park, MD 20742resnik@umiacs.umd.eduAbstractWe present a lexicon-free post-processingmethod for optical character recognition(OCR), implemented using weighted fi-nite state machines.
We evaluate thetechnique in a number of scenarios rele-vant for natural language processing, in-cluding creation of new OCR capabilitiesfor low density languages, improvementof OCR performance for a native com-mercial system, acquisition of knowledgefrom a foreign-language dictionary, cre-ation of a parallel text, and machine trans-lation from OCR output.1 IntroductionThe importance of rapidly retargeting existing natu-ral language processing (NLP) technologies to newlanguages is widely accepted (Oard, 2003).
Statisti-cal NLP models have a distinct advantage over rulebased approaches to achieve this goal, as they re-quire far less manual labor; however, training statis-tical NLP methods requires on-line text, which canbe hard to find for so-called ?low density?
languages?
that is, languages where few on-line resources ex-ist.
In addition, for many languages of interest inputdata are available mostly in printed form, and mustbe converted to electronic form prior to processing.Optical character recognition (OCR) is often theonly feasible method to perform this conversion,owing to its speed and cost-effectiveness.
Unfor-tunately, the performance of OCR systems is farfrom perfect and recognition errors significantly de-grade the performance of NLP applications.
This istrue both in resource acquisition, such as automatedbilingual lexicon generation (Kolak et al, 2003),and for end-user applications such as rapid machinetranslation (MT) in the battlefield for document fil-tering (Voss and Ess-Dykema, 2000).
Moreover, forlow density languages, there simply may not be anOCR system available.In this paper, we demonstrate that via statisticalpost-processing of existing systems, it is possibleto achieve reasonable recognition accuracy for lowdensity languages altogether lacking an OCR sys-tem, to significantly improve on the performance ofa trainable commercial OCR system, and even toimprove significantly on a native commercial OCRsystem.1 By taking a post-processing approach, werequire minimal assumptions about the OCR systemused as a starting point.The proper role of our post-processing approachdepends on the language.
For languages with littlecommercial potential for OCR, it may well providethe most practical path for language-specific OCRdevelopment, given the expensive and time consum-ing nature of OCR development for new languagesand the ?black box?
nature of virtually all state-of-the-art OCR systems.
For languages where nativeOCR development may take place, it is a fast, prac-tical method that allows entry into a new languageuntil native OCR development catches up.
For these,and also for languages where native systems exist,1Currently we assume the availability of an OCR system thatsupports the script of the language-of-interest, or which is scriptindependent (Natarajan et al, 2001).867we show that post-processing can yield improve-ments in performance.Sections 2 and 3 describe the method and its im-plementation.
In Section 4 we cover a variety of rel-evant NLP scenarios: Creating OCR capabilities forIgbo, performing OCR on a dictionary for Cebuano,using OCR to acquire the Arabic side of a commonparallel text, and evaluating the value of OCR post-processing for machine translation of Arabic andSpanish.
In Sections 5 and 6 we discuss related workand summarize our findings.2 Post-Processing SystemWe use the noisy channel framework to formulatethe correction problem, revising our previous model(Kolak et al, 2003).
That model takes the formP (O, b, a, C,W ) =P (O, b|a,C,W )P (a|C,W )P (C|W )P (W )whose components are a word-level source modelP (W ), a word-to-character model P (C|W ), a seg-mentation model P (a|C,W ), and a model for char-acter sequence transformation, P (O, b|a,C,W ).
Wis the correct word sequence and C is the corre-sponding character sequence, which is recognizedas O by the OCR system.
a and b are segmentationvectors for C and O.The original model requires a lexicon that coversall words in the processed text ?
a strong assump-tion, especially for low density languages.
We con-verted the model into a character-based one, remov-ing the need for a lexicon.
Generation of W is re-placed by generation of C, which renders P (C|W )irrelevant, and the model becomesP (O, b, a, C) = P (O, b|a,C)P (a|C)P (C)Although word-based models generally perform bet-ter, moving from words to characters is a necessarycompromise because word-based models are uselessin the absence of a lexicon, which is the case formany low-density languages.In addition to eliminating the need for a lexicon,we developed a novel method for handling wordmerge/split errors.2 Rather than modeling these er-2A merge error occurs when two or more adjacent items arerecognized as one, and a split error occurs when an item is rec-ognized as two or more items.
These errors can happen both atword level and character level.rors explicitly using a segmentation model, we sim-ply treat them as character deletion/insertion errorsinvolving the space character, allowing us to handlethem within the error model.
The segmentation stepis absorbed into the character transformation step,so a and b are no longer necessary, hence the finalequation becomesP (O,C) = P (O|C)P (C)which is a direct application of the noisy chan-nel model.
We can describe the new generativeprocess as follows: First, a sequence of charac-ters C are generated, with probability P (C), andthe OCR system converts it into O with probabilityP (O|C).
For example, if the actual input was a carand it was recognized as ajar, P (ajar, a car) =P (ajar|a car)P (a car).
Using the channel modelto address word merge/split errors without actuallyusing a word level model is, to our knowledge, anovel contribution of our approach.3 ImplementationWe implemented our post-processing system usingthe framework of weighted finite state machines(WFSM), which provides a strong theoretical foun-dation and reduces implementation time, thanks tofreely available toolkits, such as the AT&T FSMToolkit (Mohri et al, 1998).
It also allows easyintegration of our post-processor with numerousNLP applications that are implemented using FSMs(e.g.
(Knight and Graehl, 1997; Kumar and Byrne,2003)).3.1 Source ModelThe source model assigns probability P (C) to orig-inal character sequences, C. We use character leveln-gram language models as the source model, sincen-gram models are simple, easy to train, and usuallyachieve good performance.
More complicated mod-els that make use of constraints imposed by a par-ticular language, such as vowel harmony, can be uti-lized if desired.
We used the CMU-Cambridge Lan-guage Modeling Toolkit v2 (Clarkson and Rosen-feld, 1997) for training, using Witten-Bell smooth-ing and vocabulary type 1; all other parameters wereleft at their default values.8683.2 Channel ModelThe channel model assigns a probability to O giventhat it was generated from C. We experimented withtwo probabilistic string edit distance models for im-plementing the channel model.
The first, followingour earlier model (2003), permits single-charactersubstitutions, insertions, and deletions, with associ-ated probabilities.
For example, P (ajar|a car) ?P (a7?a)P ( 7??
)P (c7?j)P (a7?a)P (r 7?r).
Notethat we are only considering the most likely edit se-quence here, as opposed to summing over all pos-sible ways to convert a car to ajar.
The secondis a slightly modified version of the spelling correc-tion model of Brill and Moore (2000).3 This modelallows many-to-many edit operations, which makesP (liter|litre) ?
P (l 7?l)P (i7?i)P (tre7?ter) pos-sible.
We will refer to the these as the single-character (SC) and multi-character (MC) error mod-els, respectively.We train both error models over a set ofcorresponding ground truth and OCR sequences,?C,O?.
Training is performed using expectation-maximization: We first find the most likely editsequence for each training pair to update the editcounts, and then use the updated counts to re-estimate edit probabilities.
For MC, after finding themost likely edit sequence, extended versions of eachnon-copy operation that include neighboring charac-ters are also considered, which allows learning anycommon multi-character mappings.
Following Brilland Moore, MC training performs only one iterationof expectation-maximization.In order to reduce the time and space require-ments of the search at correction time, we impose alimit on number of errors per token.
Note that this isnot a parameter of the model, but a limit required byits computational complexity.
A lower limit will al-most always result in lower correction performance,so the highest possible limit allowed by time andmemory constraints should be used.
It is possible tocorrect more errors per token by iterating the correc-tion process.
However, iterative correction cannotguarantee that the result is optimal under the model.3We ignore the location of the error within the word, since itis not as important for OCR as it is for spelling.3.3 ChunkingSince we do not require a lexicon, we work onlines of text rather than words.
Unfortunately thesearch space for correcting a complete line is pro-hibitively large and we need a way to break it downto smaller, independent chunks.
The chunking stepis not part of the model, but rather a pre-processingstep: chunks are identified, each chunk is correctedindependently using the model, and the correctedchunks are put back together to generate the output.Spaces provide a natural break point for chunks.However, split errors complicate the process: if partsof a split word are placed in different chunks, the er-ror cannot be corrected.
For example, in Figure 1,chunking (b) allows the model to produce the de-sired output, but chunking (a) simply does not allowcombining ?sam?
and ?ple?
into ?sample?, as eachchunk is corrected independently.Figure 1: Example of a bad and a good chunkingWe address this by using the probabilities as-signed to spaces by the source model for chunking.We break the line into two chunks using the spacewith the highest probability and repeat the processrecursively until all chunks are reduced to a rea-sonable size, as defined by time and memory lim-itations.
Crucially, spurious spaces that cause spliterrors are expected to have a low probability, andtherefore breaking the line using high probabilityspaces reduces the likelihood of placing parts of asplit word in different chunks.If a lexicon does happen to be available, we canuse it to achieve more reliable chunking, as follows.The tokens of the input line that are present in thelexicon are assumed to be correct.
We identify runsof out-of-lexicon tokens and attempt to correct themtogether, allowing us to handle split errors.
Notethat in this case the lexicon is used only to improvechunking, not for correction.
Consequently, cover-age of the lexicon is far less important.Our lexicon-free chunking algorithm placed anerroneous boundary at 11.3% of word split pointsfor Arabic test data (Section 4.3).
However, cor-rection performance was identical to that of error-869Figure 2: A small excerpt from Aku.ko.free chunking.4 Incorrect decisions did not hurt be-cause the correction method was not able to fix thoseparticular split errors, regardless.
The errors of thechunking and correction models coincided as theyboth rely on the same language model.
Therefore,chunking errors are unlikely to reduce the correctionperformance.3.4 CorrectionCorrection is performed by estimating the mostprobable source character sequence C?
for a givenobserved character sequence O, using the formula:C?
= argmaxC{P (O|C)P (C)}We first encode O as an FSA and compose it withthe inverted error model FST.5 The resulting FST isthen composed with the language model FSA.
Thefinal result is a lattice that encodes all feasible se-quences C, along with their probabilities, that couldhave generated O.
We take the sequence associatedwith the most likely path through the lattice as C?.4 EvaluationWe evaluate our work on OCR post-processing ina number of scenarios relevant for NLP, includingcreation of new OCR capabilities for low densitylanguages, improvement of OCR performance fora native commercial system, acquisition of knowl-edge from a foreign-language dictionary, creation ofa parallel text, and machine translation from OCRoutput.
The languages studied include Igbo, Ce-buano, Arabic, and Spanish.For intrinsic evaluation, we use the conventionalWord Error Rate (WER) metric, which is defined asWER(C,O) =WordEditDistance(C,O)WordCount(C)4Ignoring errors that result in valid words, lexicon-basedchunking is always error-free.5Inversion reverses the direction of the error model, map-ping observed sequences to possible ground truth sequences.We do not use the Character Error Rate (CER) met-ric, since for almost all NLP applications the unit ofinformation is the words.
For extrinsic evaluation ofmachine translation, we use the BLEU metric (Pap-ineni et al, 2002).4.1 Igbo: Creating an OCR SystemIgbo is an African language spoken mainly in Nige-ria by an estimated 10 to 18 million people, writtenin Latin script.
Although some Igbo texts use dia-critics to mark tones, they are not part of the officialorthography and they are absent in most printed ma-terials.
Other than grammar books, texts for Igbo,even hardcopy, are extremely difficult to obtain.
Toour knowledge, the work reported here creates thefirst OCR system for this language.For the Igbo experiments, we used two sources.The first is a small excerpt containing 6727 wordsfrom the novel ?Juo Obinna?
(Ubesie, 1993).
Thesecond is a small collection of short stories named?Aku.ko.Ife Nke Ndi.Igbo?
(Green and Onwua-maegbu, 1970) containing 3544 words.
We will re-fer to the former as ?Juo?
and the latter as ?Aku.ko.?hereafter.
We generated the OCR data using a com-mercial English OCR system.6 Juo image files weregenerated by scanning 600dpi laser printer output at300dpi resolution.
Aku.ko.image files were gener-ated by scanning photocopies from the bound hard-copy at 300dpi.
Figure 2 provides a small excerptfrom the actual Aku.ko.page images used for recog-nition.
For both texts, we used the first two thirds fortraining and the remaining third for testing.We trained error and language models (EMs andLMs) using the training sets for Juo and Aku.ko.sep-arately, and performed corrections of English OCRoutput using different combinations of these mod-els on both test sets.
Table 1 shows the results forthe Juo test set while Table 2 presents the resultsfor Aku.ko..
The relative error reduction ranges from30% to almost 80%.
The SC error model performsbetter than the MC error model under all conditions.6Abby Fine Reader Professional Edition Version 7.0870Conditions ResultsLM Data EM Data EM type WER (%) Red.
(%)Juo Juo MC 8.66 74.18Juo Aku.ko.MC 15.23 54.59Aku.ko.Juo MC 13.25 60.49Aku.ko.Aku.ko.MC 19.08 43.11Juo Juo SC 7.11 78.80Juo Aku.ko.SC 11.49 65.74Aku.ko.Juo SC 13.42 59.99Aku.ko.Aku.ko.SC 18.92 43.59Original OCR Output 35.44 -Table 1: Post-correction WER for English OCR on JuoConditions ResultsLM Data EM Data EM type WER (%) Red.
(%)Juo Juo MC 21.42 36.33Juo Aku.ko.MC 18.08 46.25Aku.ko.Juo MC 21.51 36.06Aku.ko.Aku.ko.MC 18.16 46.02Juo Juo SC 19.92 40.78Juo Aku.ko.SC 16.49 50.98Aku.ko.Juo SC 19.92 40.78Aku.ko.Aku.ko.SC 16.40 51.25Original OCR Output 33.64 -Table 2: Post-correction WER for English OCR on Aku.ko.This is due to the fact that MC requires more train-ing data than SC.
Furthermore, most of the errors inthe data did not require many-to-many operations.Results in Tables 1 and 2 are for 6-gram languagemodel and error limit of 5; corresponding 3-gramerror rates were 1% to 2% (absolute) higher.The best correction performance is achieved whenboth the EM and LM training data come from thesame source as the test data, almost doubling theperformance achieved when they were from a dif-ferent source.7 Note that the amount of training datais small, four to eight pages, so optimizing perfor-mance via manual entry of document-specific train-ing text is not unrealistic for scenarios involvinglong documents such as books.4.1.1 Using a Trainable OCR SystemIn an additional experiment with Igbo, we found thatpost-processing can improve performance substan-tially even when an OCR system trained on Igbocharacters is the starting point.
In particular, thecommercial OCR system used for Igbo experimentssupports user-trained character shape models.
Us-7There was no overlap between training and test data underany circumstance.Conditions ResultsLM Data EM Data WER (%) Red.
(%)Juo Juo 3.69 50.34Juo Aku.ko.5.24 29.48Aku.ko.Juo 5.08 31.63Aku.ko.Aku.ko.7.38 0.67Original OCR Output 7.43 -Table 3: Post-correction WER for trained OCR system on Juoing Juo as the source, we trained the commercialOCR system manually on Igbo characters, result-ing in a 7.43% WER on Juo without postprocess-ing.8 Note that this is slightly higher than the 7.11%WER achieved using an English OCR system to-gether with our post-processing model.
We used a6-gram LM, and a SC EM with error limit of 5.
Ta-ble 3 shows that by post-processing the Igbo-trainedOCR system, we reduce the word error rate by 50%.4.2 Cebuano: Acquiring a DictionaryCebuano is a language spoken by about 15 millionpeople in the Philippines, written in Latin script.The scenario for this experiment is converting aCebuano hardcopy dictionary into electronic form,as in DARPA?s Surprise Language Dry Run (Oard,2003).
The dictionary that we used had diacritics,probably to aid in pronunciation.
The starting-pointOCR data was generated using a commercial OCRsystem.9 The fact that the tokens to be correctedcome from a dictionary means (1) there is little con-text available and (2) word usage frequencies are notreflected.
Character-based models may be affectedby these considerations, but probably not to the ex-tent that word-based models would be.Table 4 shows WER for Cebuano after post-processing.
The size column represents the numberof dictionary entries used for training, where eachentry consists of one or more Cebuano words.
Ascan be seen from the table, our model reduces WERsubstantially for all cases, ranging from 20% to 50%relative reduction.
As expected, the correction per-formance increases with the amount of training data;note, however, that we achieve reasonable correctionperformance even using only 500 dictionary entriesfor training.8The system trains by attempting OCR on a document andasking for the correct character whenever it is not confident.9ScanSoft Developer?s Kit 2000, which has no built-in sup-port for Cebuano.871Conditions ResultsSize LM EM WER (%) Red.
(%)500 3-gram SC 5.37 33.04500 3-gram MC 5.05 37.03500 6-gram SC 6.41 20.07500 6-gram MC 5.33 33.541000 3-gram SC 5.33 33.541000 3-gram MC 4.63 42.271000 6-gram SC 5.58 30.421000 6-gram MC 4.67 41.7727363 3-gram SC 4.34 45.8927363 3-gram MC 4.14 48.3827363 6-gram SC 4.55 43.2727363 6-gram MC 3.97 50.50Original OCR Output 8.02 -Table 4: Post-correction WER for CebuanoContrary to the Igbo results, the MC error modelperforms better than the SC error model.
And, inter-estingly, the 3-gram language model performs betterthan the 6-gram model, except for the largest train-ing data and MC error model combination.
Both dif-ferences are most likely caused by the implicationsof using a dictionary as discussed above.4.3 Arabic: Acquiring Parallel TextWe used Arabic to illustrate conversion from hard-copy to electronic text for a widely available paral-lel text, the Bible (Resnik et al, 1999; Kanungo etal., 2005; Oard, 2003).
We divided the Bible intoten equal size segments, using the first segment fortraining the error model, the first nine segments forthe language model, and the first 500 verses fromthe last segment for testing.
Since diacritics are onlyused in religious text, we removed all diacritics.
TheOCR data was generated using a commercial Ara-bic OCR system.10 Note that this evaluation differsfrom Igbo and Cebuano, as the experiments wereperformed using an existing native OCR system.
Italso allowed us to evaluate chunking, as Arabic datahas far more word merge/split errors compared toIgbo and Cebuano.Table 5 shows the correction performance forArabic under various conditions.
The Limit col-umn lists the maximum number of errors per to-ken allowed and the M/S column indicates whethercorrection of word merge/split errors was allowed.We achieve significant reductions in WER for Ara-bic.
The first two rows show that the 6-gram lan-10Sakhr Automatic Reader Version 6.0Conditions ResultsM/S LM Limit WER (%) Red.
(%)no 3-gram 2 22.14 10.33no 6-gram 2 17.99 27.14yes 3-gram 2 18.26 26.04yes 3-gram 4 17.74 28.15yes 5-gram 2 20.74 16.00Original OCR Output 24.69 -Table 5: Post-correction WER for Arabicguage model performs much better than the 3-grammodel.
Interestingly, higher order n-grams performworse when we allow word merge/split errors.
Notethat for handling word merge/split errors we need tolearn the character distributions within lines, ratherthan within words as we normally do.
Consequently,more training data is required for reliable parameterestimation.
Handling word merge/split errors im-prove the performance, which is expected.
Allow-ing fewer errors per token reduces the performance,since it is not possible to correct words that havemore character errors than the limit.
Unfortunately,increasing the error limit increases the search spaceexponentially, making it impossible to use high lim-its.
As mentioned in Section 3.2, iterative correctionis a way to address this problem.4.4 Extrinsic Evaluation: MTWhile our post-processing methods reduce WER,our main interest is their impact on NLP applica-tions.
We have performed machine translation ex-periments to measure the effects of OCR errors andthe post-processing approach on NLP applicationperformance.For Arabic, we trained a statistical MT system us-ing the first nine sections of the Bible data.
The lan-guage model is trained using the CMU-Cambridgetoolkit and the translation model using the GIZA++toolkit (Och and Ney, 2000).
We used the ReWritedecoder (Germann, 2003) for translation.BLEU scores for OCR, corrected, and clean textwere 0.0116, 0.0141, and 0.0154, respectively.
Thisestablishes that OCR errors degrade the performanceof the MT system, and we are able to bring the per-formance much closer to the level of performanceon clean text by using post-processing.
Clearly theBLEU scores are quite low; we are planning to per-form experiments on Arabic using a more advancedtranslation system, such as Hiero (Chiang, 2005).872MT System Input Text BLEU ScoreSystran OCR 0.2000Systran Corrected 0.2606Systran Clean 0.3188ReWrite OCR 0.1792ReWrite Corrected 0.2234ReWrite Clean 0.2590Table 6: Spanish-English translation resultsIn order to test in a scenario with better trans-lation performance, we performed MT evaluationsusing Spanish.
We used a commercial translationsystem, Systran, in addition to statistical translation.More resources being available for this language,corrected text for Spanish experiments was obtainedusing our original model that takes advantage of alexicon (2003).
Table 6 shows that scores are muchhigher compared to Arabic, but the pattern of im-provements using post-processing is the same.5 Related WorkThere has been considerable research on automaticerror correction in text.
Kukich (1992) provides ageneral survey of the research in the area.
Unfor-tunately, there is no standard evaluation benchmarkfor OCR correction, and implementations are usu-ally not publicly available, making a direct compar-ison difficult.Most correction methods are not suitable forlow density languages as they rely on lexicons.Goshtasby and Ehrich (1988) present a lexicon-freemethod based on probabilistic relaxation labeling.However, they use the probabilities assigned to in-dividual characters by the OCR system, which isnot always available.
Perez-Cortes et al (2000) de-scribe a method which does not have this limitation.They use a stochastic FSM that accepts the smallestk-testable language consistent with a representativesample.
While the method can handle words not inits lexicon in theory, it was evaluated using a largek to restrict corrections to the lexicon.
They reportreducing error rate from 33% to below 2% on OCRoutput of hand-written Spanish names.In addition to providing alternatives, the litera-ture provides complementary methods.
Guyon andPereira (1995) present a linguistic post-processorbased on variable memory length Markov modelsthat is designed to be used as the language modelcomponent of character recognizers.
Their modelcan be used as the source model for our method.Since it is a variable length model, it can allow usto handle higher order n-grams.A script-independent OCR system is presented byNatarajan et al (2001).
The system is evaluatedon Arabic, Chinese, and English, achieving 0.5% to5% CER under various conditions.
Since our post-processing method can be used to reduce the errorrate of a trained OCR system, the two methods canbe combined to better adapt to new languages.Voss and Ess-Dykema (2000) evaluated the ef-fects of OCR errors on MT in the context of theFALCon project, which combines off-the-shelf OCRand MT components to provide crude translationsfor filtering.
They report significant degradation intranslation performance as a result of OCR errors.For instance, for the Spanish system, OCR processreduced the number of words that can be recognizedby the translation module by more than 60%.6 ConclusionsWe have presented a statistical post-processingmethod for OCR error correction that requires mini-mal resources, aimed particularly at low density lan-guages and NLP scenarios.
The technique gainsleverage from existing OCR systems, enabling bothminimal-labor adaptation of systems to new lowdensity languages and improvements in native OCRperformance.We rigorously evaluated our approach using realOCR data, and have shown that we can achieverecognition accuracy lower than that achieved by atrainable OCR system for a new language.
For Igbo,a very low density language, adapting English OCRachieved relative error reductions as high as 78%, re-sulting in 7.11% WER.
We also showed that the er-ror rate of a trainable OCR system after training canbe further reduced up to 50% using post-processing,achieving a WER as low as 3.7%.
Post-processingexperiments using Cebuano validate our approach ina dictionary-acquisition scenario, with a 50.5% rel-ative reduction in error rate from 8.02% to 3.97%.Evaluation on Arabic demonstrated that the errorrate for a native commercial OCR system can be re-duced by nearly 30%.
In addition, we measured theimpact of post-processing on machine translation,873quantifying OCR degradation of MT performanceand showing that our technique moves the perfor-mance of MT on OCR data significantly closer toperformance on clean input.
See Kolak (forthcom-ing) for more details and discussion.One limitation of our approach is its reliance onan existing OCR system that supports the script ofthe language of interest.
Trainable OCR systemsare the only option if there is no OCR system thatsupports the script of interest; however, training anOCR system from scratch is usually a tedious andtime consuming task.
Post-processing can be usedto reduce the training time and improve recognitionaccuracy by aiding generation of more training dataonce basic recognition capability is in place.AcknowledgmentsThis research was supported in part by Departmentof Defense contract RD-02-5700 and DARPA/ITOCooperative Agreement N660010028910.
We aregrateful to Mohri et al for the AT&T FSMToolkit and Clarkson and Rosenfeld for the CMU-Cambridge Toolkit.
We thank David Doermannand his students for the Cebuano text; ChineduUchechukwu for the Igbo text and useful informa-tion on the language.
We also thank Mustafa MuratT?k?r for his help in generating Igbo ground truth forAku.ko., and for useful discussion.ReferencesEric Brill and Robert C. Moore.
2000.
An improved modelfor noisy channel spelling correction.
In Proceedings of theACL-00, Hong Kong, China, October.David Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proceedings of the ACL-05, pages 263?270, Ann Arbor, Michigan, USA, June.Philip Clarkson and Ronald Rosenfeld.
1997.
Statistical lan-guage modeling using the CMU-Cambridge toolkit.
In Pro-ceedings of the ESCA Eurospeech, Rhodes, Greece.Ulrich Germann.
2003.
Greedy decoding for statistical ma-chine translation in almost linear time.
In Proceedings of theHLT-NAACL-03, Edmonton, Alberta, Canada, May.Ardeshir Goshtasby and Roger W. Ehrich.
1988.
Contex-tual word recognition using probabilistic relaxation labeling.Pattern Recognition, 21(5):455?462.M.
M. Green and M. O. Onwuamaegbu, editors.
1970.
Aku.ko.Ife Nke Ndi.Igbo.
Oxford University Press, Ibadan, Nigeria.Isabelle Guyon and Fernando Pereira.
1995.
Design of a lin-guistic postprocessor using variable memory length Markovmodels.
In Proceedings of the ICDAR-95, volume 1, Mon-treal, Quebec, Canada, August.Tapas Kanungo, Philip Resnik, Song Mao, Doe wan Kim, andQigong Zheng.
2005.
The Bible and multilingual opti-cal character recognition.
Communications of the ACM,48(6):124?130.Kevin Knight and Jonathan Graehl.
1997.
Machine translitera-tion.
In Proceedings of the ACL-97, Madrid, Spain, July.Okan Kolak, William Byrne, and Philip Resnik.
2003.
Agenerative probabilistic OCR model for NLP applications.In Proceedings of the HLT-NAACL-03, Edmonton, Alberta,Canada, May.Okan Kolak.
forthcoming.
Cross-Lingual Utilization of NLPResources for New Languages.
Ph.D. thesis, University ofMaryland, College Park, Maryland, USA.Karen Kukich.
1992.
Techniques for automatically correct-ing words in text.
ACM Computing Surveys, 24(4):377?439,December.Shankar Kumar and William Byrne.
2003.
A weighted finitestate transducer implementation of the alignment templatemodel for statistical machine translation.
In Proceedings ofthe HLT-NAACL-03, Edmonton, Alberta, Canada, May.Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley.1998.
A rational design for a weighted finite-state transducerlibrary.
Lecture Notes in Computer Science, 1436.Premkumar Natarajan, Zhidong Lu, Richard Schwartz, IssamBazzi, and John Makhoul.
2001.
Multilingual machineprinted ocr.
International Journal of Pattern Recognitionand Artificial Intelligence, 15(1):43?63.Douglas W. Oard.
2003.
The surprise language exercises.ACM Transactions on Asian Language Information Process-ing (TALIP), 2(2):79?84, June.Franz.
J. Och and Hermann Ney.
2000.
Improved statisticalalignment models.
In Proceedings of the ACL-00, pages440?447, Hongkong, China, October.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
Bleu: a method for automatic evaluation of ma-chine translation.
In Proceedings of the ACL-02, Philedel-phia, Pennsylvania, USA, July.Juan Carlos Perez-Cortes, Juan-Carlos Amengual, Joaquim Ar-landis, and Rafael Llobet.
2000.
Stochastic error-correctingparsing for OCR post-processing.
In Proceedings of theICPR-00, Barcelona, Spain, September.Philip Resnik, Mari Broman Olsen, and Mona Diab.
1999.
TheBible as a parallel corpus: Annotating the ?Book of 2000Tongues?.
Computers and the Humanities, 33(1-2):129?153.Tony Uchenna Ubesie.
1993.
Juo Obinna.
University PressPLC, Ibadan, Nigeria.
ISBN: 19575395X.Clare R. Voss and Carol Van Ess-Dykema.
2000.
When is anembedded MT system ?good enough?
for filtering?
In Pro-ceedings of the Workshop on Embedded MT Systems, ANLP-NAACL-00, Seattle, Washington, USA, May.874
