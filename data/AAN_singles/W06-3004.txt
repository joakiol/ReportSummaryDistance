Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 25?32,New York City, NY, USA.
June 2006. c?2006 Association for Computational LinguisticsEnhanced Interactive Question-Answering with Conditional Random FieldsAndrew Hickl and Sanda HarabagiuLanguage Computer CorporationRichardson, Texas 75080andy@languagecomputer.comAbstractThis paper describes a new methodologyfor enhancing the quality and relevance ofsuggestions provided to users of interac-tive Q/A systems.
We show that by usingConditional Random Fields to combinerelevance feedback gathered from usersalong with information derived from dis-course structure and coherence, we canaccurately identify irrelevant suggestionswith nearly 90% F-measure.1 IntroductionToday?s interactive question-answering (Q/A) sys-tems enable users to pose questions in the contextof extended dialogues in order to obtain informationrelevant to complex research scenarios.
When work-ing with an interactive Q/A system, users formulatesequences of questions which they believe will re-turn answers that will let them reach certain infor-mation goals.Users need more than answers, however: whilethey might be cognizant of many of the differenttypes of information that they need, few ?
if any ?users are capable of identifying all of the questionsthat must be asked and answered for a particular sce-nario.
In order to take full advantage of the Q/Acapabilities of current systems, users need access tosources of domain-specific knowledge that will ex-pose them to new concepts and ideas and will allowthem to ask better questions.In previous work (Hickl et al, 2004; Harabagiu etal., 2005a), we have argued that interactive question-answering systems should be based on a predictivedialogue architecture which can be used to provideusers with both precise answers to their questions aswell as suggestions of relevant research topics thatcould be explored throughout the course of an inter-active Q/A dialogue.Typically, the quality of interactive Q/A dialogueshas been measured in three ways: (1) efficiency, de-fined as the number of questions that the user mustpose to find particular information, (2) effectiveness,defined by the relevance of the answer returned, and(3) user satisfaction (Scholtz and Morse, 2003).In our experiments with an interactive Q/A sys-tem, (known as FERRET), we found that perfor-mance in each of these areas improves as users areprovided with suggestions that are relevant to theirdomain of interest.
In FERRET, suggestions aremade to users in the form of predictive question-answer pairs (known as QUABs) which are eithergenerated automatically from the set of documentsreturned for a query (using techniques first describedin (Harabagiu et al, 2005a)), or are selected from alarge database of questions-answer pairs created off-line (prior to a dialogue) by human annotators.Figure 1 presents an example of ten QUABsthat were returned by FERRET in response to thequestion ?How are EU countries responding to theworldwide increase of job outsourcing to India?
?.While FERRET?s QUABs are intended to provideusers with relevant information about a domain ofinterest, we can see from Figure 1 that users do notalways agree on which QUAB suggestions are rel-evant.
For example, while someone unfamiliar tothe notion of ?job outsourcing?
could benefit from25Relevant?QUAB QuestionUser1 User2NO YES QUAB1: What EU countries are outsourcing jobs to India?YES YES QUAB2: What EU countries have made public statementsagainst outsourcing jobs to India?NO YES QUAB3: What is job outsourcing?YES YES QUAB4: Why are EU companies outsourcing jobs to India?NO NO QUAB5: What measures has the U.S. Congress taken to stemthe tide of job outsourcing to India?YES NO QUAB6: How could the anti-globalization movements in EUcountries impact the likelihood that the EU Parliament willtake steps to prevent job outsourcing to India?YES YES QUAB7: Which sectors of the EU economy could be mostaffected by job outsourcing?YES YES QUAB8: How has public opinion changed in the EU on joboutsourcing issues over the past 10 years?YES YES QUAB9: What statements has French President JacquesChirac made about job outsourcing?YES YES QUAB10: How has the EU been affected by anti-job outsourc-ing sentiments in the U.S.?Figure 1: Examples of QUABs.a QUAB like QUAB3: ?What is job outsourcing?
?,we expect that a more experienced researcher wouldfind this definition to be uninformative and poten-tially irrelevant to his or her particular informationneeds.
In contrast, a complex QUAB like QUAB6:?How could the anti-globalization movements in EUcountries impact the likelihood that the EU Parlia-ment will take steps to prevent job outsourcing toIndia??
could provide a domain expert with rel-evant information, but would not provide enoughbackground information to satisfy a novice user whomight not be able to interpret this information in theappropriate context.In this paper, we present results of a new set ofexperiments that seek to combine feedback gatheredfrom users with a relevance classifier based on con-ditional random fields (CRF) in order to provide sug-gestions to users that are not only related to the topicof their interactive Q/A dialogue, but provide themwith the new types of information they need to know.Section 2 presents the functionality of severalof FERRET?s modules and describes the NLP tech-niques for processing questions as well as the frame-work for acquiring domain knowledge.
In Section 3we present two case studies that highlight the impactof user background.
Section 4 describes a new classof user interaction models for interactive Q/A andpresents details of our CRF-based classifier.
Section5 presents results from experiments which demon-strate that user modeling can enhance the qualityof suggestions provided to both expert and noviceusers.
Section 6 summarizes the conclusions.2 The FERRET InteractiveQuestion-Answering SystemWe believe that the quality of interactions producedby an interactive Q/A system can be enhanced bypredicting the range of questions that a user mightask while researching a particular topic.
By provid-ing suggestions from a large database of question-answer pairs related to a user?s particular area ofinterest, interactive Q/A systems can help usersgather the information they need most ?
without theneed for complex, mixed-initiative clarification dia-logues.FERRET uses a large collection of QUABquestion-answer pairs in order to provide users withsuggestions of new research topics that could be ex-plored over the course of a dialogue.
For example,when a user asks a question likeWhat is the result ofthe European debate on outsourcing to India?
(as il-lustrated in (Q1) in Table 1), FERRET returns a set ofanswers (including (A1) and proposes the questionsin (Q2), (Q3), and (Q4) as suggestions of possiblecontinuations of the dialogue.
Users then have thefreedom to choose how the dialogue should be con-tinued, either by (1) ignoring the suggestions madeby the system, (2) selecting one of the proposedQUAB questions and examining its associated an-swer, or (3) resubmitting the text of the QUAB ques-tion to FERRET?s automatic Q/A system in order toretrieve a brand-new set of answers.
(Q1) What is the result of the European debate on outsourcing to India?
(A1) Supporters of economic openness understand how outsourcing canstrengthen the competitiveness of European companies, as well as benefi tjobs and growth in India.
(Q2) Has the number of customer service jobs outsourced to India in-creased since 1990?
(Q3) How many telecom jobs were outsourced to India from EU-basedcompanies in the last 10 years?
(Q4) Which European Union countries have experienced the most joblosses due to outsourcing over the past 10 years?Table 1: Sample Q/A Dialogue.FERRET was designed to evaluate how databasesof topic-relevant suggestions could be used to en-hance the overall quality of Q/A dialogues.
Fig-ure 2 illustrates the architecture of the FERRET sys-tem.
Questions submitted to FERRET are initiallyprocessed by a dialogue shell which (1) decomposescomplex questions into sets of simpler questions (us-ing techniques first described in (Harabagiu et al,2005a)), (2) establishes discourse-level relations be-tween the current question and the set of questions26PreviousDialogue ContextManagementManagementDialogue ActDecompositionQuestionTopic Partitioningand RepresentationAnswerFusionDatabase (QUAB)Question?AnswerInformationExtractionQuestionSimilarityConversationScenarioPredictiveQuestionsAnswerFusionOnline Question Answering Off?line Question AnsweringAnswerQuestionDocumentCollectionDialogue Shell Predictive Dialogue(PQN)NetworkPredictiveQuestionFigure 2: FERRET - A Predictive Interactive Question-Answering Architecture.already entered into the discourse, and (3) identifiesa set of basic dialogue acts that are used to managethe overall course of the interaction with a user.Output from FERRET?s dialogue shell is sent toan automatic question-answering system which isused to find answers to the user?s question(s).
FER-RET uses a version of LCC?s PALANTIR question-answering system (Harabagiu et al, 2005b) in or-der to provide answers to questions in documents.Before being returned to users, answer passages aresubmitted to an answer fusion module, which filtersredundant answers and combines answers with com-patible information content into single coherent an-swers.Questions and relational information extracted bythe dialogue shell are also sent to a predictive dia-logue module, which identifies the QUABs that bestmeet the user?s expected information requirements.At the core of the FERRET?s predictive dialoguemodule is the Predictive Dialogue Network (PQN), alarge database of QUABs that were either generatedoff-line by human annotators or created on-line byFERRET (either during the current dialogue or dur-ing some previous dialogue)1.
In order to generateQUABs automatically, documents identified fromFERRET?s automatic Q/A system are first submit-ted to a Topic Representation module, which com-putes both topic signatures (Lin and Hovy, 2000)and enhanced topic signatures (Harabagiu, 2004) inorder to identify a set of topic-relevant passages.Passages are then submitted to an Information Ex-traction module, which annotates texts with a wide1Techniques used by human annotators for creating QUABswere fi rst described in (Hickl et al, 2004); full details of FER-RET?s automatic QUAB generation components are provided in(Harabagiu et al, 2005a).range of lexical, semantic, and syntactic informa-tion, including (1) morphological information, (2)named entity information from LCC?s CICEROLITEnamed entity recognition system, (3) semantic de-pendencies extracted from LCC?s PropBank-stylesemantic parser, and (4) syntactic parse informa-tion.
Passages are then transformed into natural lan-guage questions using a set of question formationheuristics; the resultant QUABs are then stored inthe PQN.
Since we believe that the same set of re-lations that hold between questions in a dialogueshould also hold between pairs of individual ques-tions taken in isolation, discourse relations are dis-covered between each newly-generated QUAB andthe set of QUABs stored in the PQN.
FERRET?sQuestion Similarity module then uses the similar-ity function described in (Harabagiu et al, 2005a) ?along with relational information stored in the PQN?
in order to identify the QUABs that represent themost informative possible continuations of the dia-logue.
QUABs are then ranked in terms of their rel-evance to the user?s submitted question and returnedto the user.3 Two Types of Users of Interactive Q/ASystemsIn order to return answers that are responsive tousers?
information needs, interactive Q/A systemsneed to be sensitive to the different questioningstrategies that users employ over the course of a di-alogue.
Since users gathering information on thesame topic can have significantly different informa-tion needs, interactive Q/A systems need to be ableto accommodate a wide range of question types inorder to help users find the specific information that27SQ ?
How are European Union countries responding to the worldwide increase in job outsourcing to countries like India?EQ2 ?
What impact has public opposition to globalization inEU countries had on companies to relocate EU jobs to India?EQ4 ?
What economic advantages could EUcountries realize by outsourcing jobs to India?NQ1 ?
What countries in the European Union are outsourcing jobs to India?NQ2 ?
How many jobs have been outsourced to India?NQ3 ?
What industries have been mostactive in outsourcing jobs to India?NQ4 ?
Are the companies that are outsourcingjobs to India based in EU countries?NQ5 ?
What could European Countries do torespond to increases in job outsourcing to India?NQ6 ?
Do European Union Countries view joboutsourcing to countries like India as a problem?EQ1 ?
Is the European Union likely to implement protectionistpolicies to keep EU companies from outsourcing jobs to India?EQ3 ?
What economic ties has the EU maintained historically with India?EQ5 ?
Will the EU adopt either any of the the U.S.?s orJapan?s anti?outsourcing policies in the near future?ease tensions over immiggration in many EU countries?EQ6 ?
Could the increasing outsourcing of jobs to IndiaFigure 3: Expert User Interactions Versus Novice User Interactions with a Q/A System.they are looking for.In past experiments with users of interactive Q/Asystems (Hickl et al, 2004), we have found that auser?s access to sources of domain-specific knowl-edge significantly affects the types of questions thata user is likely to submit to a Q/A system.
Users par-ticipate in information-seeking dialogues with Q/Asystems in order to learn ?new?
things ?
that is, toacquire information that they do not currently pos-sess.
Users initiate a set of speech acts which allowthem to maximize the amount of new informationthey obtain from the system while simultaneouslyminimizing the amount of redundant (or previously-acquired) information they encounter.
Our experi-ments have shown that Q/A systems need to be sen-sitive to two kinds of users: (1) expert users, whointeract with a system based on a working knowl-edge of the conceptual structure of a domain, and(2) novice users, who are presumed to have lim-ited to no foreknowledge of the concepts associ-ated with the domain.
We have found that noviceusers that possess little or no familiarity with a do-main employ markedly different questioning strate-gies than expert users who possess extensive knowl-edge of a domain: while novices focus their atten-tion in queries that will allow them to discover ba-sic domain concepts, experts spend their time ask-ing questions that enable them to evaluate their hy-potheses in the context of a the currently availableinformation.
The experts tend to ask questions thatrefer to the more abstract domain concepts or thecomplex relations between concepts.
In a similarfashion, we have discovered that users who have ac-cess to structured sources of domain-specific knowl-edge (e.g.
knowledge bases, conceptual networksor ontologies, or mixed-initiative dialogues) can endup employing more ?expert-like?
questioning strate-gies, despite the amount of domain-specific knowl-edge they possess.In real-world settings, the knowledge that expertusers possess enables them to formulate a set of hy-potheses ?
or belief states ?
that correspond to eachof their perceived information needs at a given mo-ment in the dialogue context.
As can be seen in thedialogues presented in Figure 3, expert users gener-ally formulate questions which seek to validate thesebelief states in the context of a document collection.Given the global information need in S1, it seemsreasonable to presume that questions like EQ1 andEQ2 are motivated by a user?s expectation that pro-tectionist policies or public opposition to globaliza-tion could impact a European Union country?s will-ingness to take steps to stem job outsourcing to In-dia.
Likewise, questions like EQ5 are designed toprovide the user with information that can decide be-tween two competing belief states: in this case, theuser wants to know whether the European Union ismore likely to model the United States or Japan in itspolicies towards job outsourcing.
In contrast, with-out a pre-existing body of domain-specific knowl-edge to derive reasonable hypotheses from, noviceusers ask questions that enable them to discoverthe concepts (and the relations between concepts)needed to formulate new, more specific hypothesesand questions.
Returning again to Figure 3, we cansee that questions like NQ1 and NQ3 are designedto discover new knowledge that the user does notcurrently possess, while questions like NQ6 try to28establish whether or not the user?s hypothesis (i.e.namely, that EU countries view job outsourcing toIndia as an problem) is valid and deserves furtherconsideration.4 User Interaction Models for RelevanceEstimationUnlike systems that utilize mixed initiative dia-logues in order to determine a user?s informationneeds (Small and Strzalkowski, 2004), systems (likeFERRET) which rely on interactions based on pre-dictive questioning have traditionally not incorpo-rated techniques that allow them to gather relevancefeedback from users.
In this section, we describehow we have used a new set of user interaction mod-els (UIM) in conjunction with a relevance classifierbased on conditional random fields (CRF) (McCal-lum, 2003; Sha and Pereira, 2003) in order to im-prove the relevance of the QUAB suggestions thatFERRET returns in response to a user?s query.We believe that systems based on predictive ques-tioning can derive feedback from users in threeways.
First, systems can learn which suggestionsor answers are relevant to a user?s domain of inter-est by tracking which elements users select through-out the course of a dialogue.
With FERRET, eachanswer or suggestion presented to a user is associ-ated with a hyperlink that links to the original textthat the answer or QUAB was derived from.
Whileusers do not always follow links associated with pas-sages they deem to be relevant to their query, weexpect that the set of selected elements are gener-ally more likely to be relevant to the user?s intereststhan unselected elements.
Second, since interactiveQ/A systems are often used to gather informationfor inclusion in written reports, systems can identifyrelevant content by tracking the text passages thatusers copy to other applications, such as text editorsor word processors.
Finally, predictive Q/A systemscan gather explicit feedback from users through thegraphical user interface itself.
In a recent version ofFERRET, we experimented with adding a ?relevancecheckbox?
to each answer or QUAB element pre-sented to a user; users were then asked to providefeedback to the system by selecting the checkboxesassociated with answers that they deemed to be par-ticularly relevant to the topic they were researching.4.1 User Interaction ModelsWe have experimented with three models that wehave used to gather feedback from users of FERRET.The models are illustrated in Figure 4.UIM1: Under this model, the set of QUABs that users copied from were selectedas relevant; all QUABs not copied from were annotated as irrelevant.UIM2: Under this model, QUABs that users viewed were considered to be rele-vant; QUABs that remained unviewed were annotated as irrelevant.UIM3: Under this model, QUABs that were either viewed or copied from weremarked as relevant; all other QUABs were annotated as irrelevant.Figure 4: User Interaction Models.With FERRET, users are presented with as manyas ten QUABs for every question they submit to thesystem.
QUABs ?
whether they be generated auto-matically by FERRET?s QUAB generation module,or selected from FERRET?s knowledge base of over10,000 manually-generated question/answer pairs ?are presented in terms of their conceptual similarityto the original question.
Conceptual similarity (asfirst described in (Harabagiu et al, 2005a)) is calcu-lated using the version of the cosine similarity for-mula presented in Figure 5.Conceptual Similarity weights content terms in Q1 and Q2 using tfidf(wi = w(ti) = (1 + log(tfi))log Ndfi), where N is the number ofquestions in the QUAB collection, while dfi is equal to the number ofquestions containing ti and tfi is the number of times ti appears in Q1and Q2.
The questions Q1 and Q2 can be transformed into two vectors,vq = ?wq1 , wq2 , ..., wqm ?
and vu = ?wu1 , wu2 , ..., wun ?
; The sim-ilarity between Q1 and Q2 is measured as the cosine measure between theircorresponding vectors:cos(vq, vu) = (?iwqiwui )/((?iw2qi )12 ?
(?iw2ui )12 )Figure 5: Conceptual Similarity.In the three models from Figure 4, we allowedusers to perform research as they normally would.Instead of requiring users to provide explicit formsof feedback, features were derived from the set ofhyper-links that users selected and the text passagesthat users copied to the system clipboard.Following (Kristjansson et al, 2004) we analyzedthe performance of each of these three models usinga new metric derived from the number of relevantQUABs that were predicted to be returned for eachmodel.
We calculated this metric ?
which we referto as the Expected Number of Irrelevant QUABs ?using the formula:p0(n) =10?k=1kp0(k) (1)p1(n) = (1 ?
p0(0)) +10?k=1kp1(k) (2)29where pm(n) is equal to the probability of findingn irrelevant QUABs in a set of 10 suggestions re-turned to the user given m rounds of interaction.p0(n) (equation 1) is equal to the probability that allQUABs are relevant initially, while p1(n) (equation2) is equal to the probability of finding an irrelevantQUAB after the set of QUABs has been interactedwith by a user.
For the purposes of this paper, weassumed that all of the QUABs initially returned byFERRET were relevant, and that p0(0) = 1.0.
Thisenabled us to calculate p1(n) for each of the threemodels provided in Figure 4.4.2 Relevance Estimation using ConditionalRandom FieldsFollowing work done by (Kristjansson et al, 2004),we used the feedback gathered in Section 4.1 to es-timate the probability that a QUAB selected fromFERRET?S PQN is, in fact, relevant to a user?s orig-inal query.
We assume that humans gauge the rel-evance of QUAB suggestions returned by the sys-tem by evaluating the informativeness of the QUABwith regards to the set of queries and suggestionsthat have occurred previously in the discourse.
AQUAB, then, is deemed relevant when it conveyscontent that is sufficiently informative to the user,given what the user knows (i.e.
the user?s level ofexpertise) and what the user expects to receive asanswers from the system.Our approach treats a QUAB suggestionas a single node in a sequence of questions?Qn?1, Qn, QUAB?
and classifies the QUAB asrelevant or irrelevant based on features from theentire sequence.We have performed relevance estimation us-ing Conditional Random Fields (CRF).
Given arandom variable x (corresponding to data points{x1, .
.
.
, xn}) and another random variable y (cor-responding to a set of labels {y1, .
.
.
, yn}), CRFscan be used to calculate the conditional probabilityp(y|x).
Given a sequence {x1, .
.
.
, xn} and set oflabels {y1, .
.
.
, yn}, p(y|x) can be defined as:p(y|x) =1z0exp(N?n=1?k?kfk(yi?1, yi, x, n))(3)where z0 is a normalization factor and ?k is a weightlearned for each feature vector fk(yi?1, yi, x, n).We trained our CRF model in the following way.If we assume that ?
is a set of feature weights(?0, .
.
.
,?k), then we expect that we can use maxi-mum likelihood to estimate values for ?
given a setof training data pairs (x, y).Training is accomplished by maximizing the log-likelihood of each labeled data point as in the fol-lowing equation:w?
=N?i=1log(p?
(xi|yi)) (4)Again, following (Kristjansson et al, 2004), weused the CRF Viterbi algorithm to find the mostlikely sequence of data points assigned to each la-bel category using the formula:y?
= arg maxyp?
(y|x) (5)Motivated by the types of discourse relations thatappear to exist between states in an interactive Q/Adialogue, we introduced a large number of featuresto estimate relevance for each QUAB suggestion.The features we used are presented in Figure 6(a) Rank of QUAB: the rank (1, ..., 10) of the QUAB in question.
(b) Similarity: similarity of QUAB, Qn and QUAB, Qn?1.
(c) Relation likelihood: equal to the likelihood of each predicate-argumentstructure included in QUAB given all QUABs contained in FERRET?s QUAB;calculated for Arg-0, Arg-1, and ArgM-TMP for each predicate found inQUAB suggestions.
(Predicate-argument relations were identifi ed using a se-mantic parser trained on PropBank (Palmer et al, 2005) annotations.
)(d) Conditional Expected Answer Type likelihood: equal to the joint probabil-ity p(EATQUAB |EATquestion) calculated from a corpus of dialoguescollected from human users of FERRET.
(e) Terms in common: real-valued feature equal to the number of terms incommon between the QUAB and both Qn and Qn?1.
(f) Named Entities in common: same as terms in common, but calculated fornamed entities detected by LCC?s CIEROLITE named entity recognition sys-tem.Figure 6: Relevance Features.In the next section, we describe how we utilizedthe user interaction model described in Subsection4.1 in conjunction with this subsection in order toimprove the relevance of QUAB suggestions re-turned to users.5 Experimental ResultsIn this section, we describe results from two experi-ments that were conducted using data collected fromhuman interactions with FERRET.In order to evaluate the effectiveness of our rel-evance classifier, we gathered a total of 1000 ques-tions from human dialogues with FERRET.
500 of30these came from interactions (41 dialogues) wherethe user was a self-described ?expert?
on the topic;another selection of 500 questions came from a to-tal of 23 dialogues resulting from interactions withusers who described themselves as ?novice?
or wereotherwise unfamiliar with a topic.
In order tovalidate the user?s self-assessment, we selected 5QUABs at random from the set of manually createdQUABs assembled for each topic.
Users were askedto provide written answers to those questions.
Usersthat were judged to have correctly answered threeout of five questions were considered ?experts?
forthe purpose of our experiments.
Table 2 presents thebreakdown of questions across these two conditions.User Type Unique Topics # Dialogues Avg # of Qs/dialogue Total QsExpert 12 41 12.20 500Novice 8 23 21.74 500Total 12 64 15.63 1000Table 2: Question Breakdown.Each of these experiments were run using a ver-sion of FERRET that returned the top 10 most similarQUABs from a database that combined manually-created QUABs with the automatically-generatedQUABs created for the user?s question.
While a to-tal of 10,000 QUABs were returned to users duringthese experiments, only 3,998 of these QUABs wereunique (39.98%).We conducted two kinds of experiments withusers.
In the first set of experiments, users wereasked to mark all of the relevant QUABs that FER-RET returned in response to questions submitted byusers.
After performing research on a particularscenario, expert and novice users were then sup-plied with as many as 65 questions (and associ-ated QUABs) taken from previously-completed di-alogues on the same scenario; users were then askedto select checkboxes associated with QUABs thatwere relevant.
In addition, we also had 2 linguists(who were familiar with all of the research sce-narios but did not research any of them) performthe same task for all of the collected questions andQUABs.
Results from these three sets of annotationsare found in Table 3.User Type Users # Qs # QUABs # rel.
QUABs % relevant ENIQ(P1)Expert 6 250 2500 699 27.96% 5.88Novice 4 250 2500 953 38.12% 3.73Linguists 2 500 5000 2240 44.80% 3.53Table 3: User Comparison.As expected, experts believed QUABs to be sig-nificantly (p < 0.05) less relevant than novices, whofound approximately 38.12% of QUABs to be rel-evant to the original question submitted by a user.In contrast, the two linguists found 44.8% of theQUABs to be relevant.
This number may be arti-ficially high: since the linguists did not engage inactual Q/A dialogues for each of the scenarios theywere annotating, they may not have been appropri-ately prepared to make a relevance assessment.In the second set of experiments, we used the UIMin Figure 4 to train CRF-based relevance classifiers.We obtained training data for UIM1 (?copy-and-paste?-based), UIM2 (?click?-based), and UIM3(?hybrid?)
from 16 different dialogue histories col-lected from 8 different novice users.
During thesedialogues, users were asked to perform research asthey normally would; no special instructions weregiven to users to provide additional relevance feed-back to the system.
After the dialogues were com-pleted, QUABs that were copied from or clickedwere annotated as ?relevant?
examples (accordingto each UIM); the remaining QUABs were anno-tated as ?irrelevant?.
Once features (as describedin Table 3) were extracted and the classifiers weretrained, they were evaluated on a set of 1000 QUABs(500 ?relevant?, 500 ?irrelevant?)
selected at ran-dom from the annotations performed in the first ex-periment.
Table 4 presents results from these twoclassifiers.UIM1 P R F (?
= 1)Irrelevant 0.9523 0.9448 0.9485Relevant 0.3137 0.3478 0.3299UIM2 P R F (?
= 1)Irrelevant 0.8520 0.8442 0.8788Relevant 0.3214 0.4285 0.3673UIM3 P R F (?
= 1)Irrelevant 0.9384 0.9114 0.9247Relevant 0.3751 0.3961 0.3853Table 4: Experimental Results from 3 User Models.Our results suggest that feedback gathered from auser?s ?normal?
interactions with FERRET could beused to provide valuable input to a relevance classi-fier for QUABsWhen ?copy-and-paste?
events wereused to train the classifier, the system detected in-stances of irrelevant QUABs with over 80% F.Whenthe much more frequent ?clicking?
events were usedto train the classifier, irrelevant QUABs were de-tected at over 90%F for both UIM2 and UIM3.
Ineach of these three cases, however, detection of rel-31evant QUABs lagged behind significantly: relevantQUABs were detected at 42% F in UIM1 at nearly33% F under UIM2 and at 39% under UIM3.We feel that these results suggest that the detec-tion of relevant QUABs (or the filtering of irrelevantQUABs) may be feasible, even without requiringusers to provide additional forms of explicit feed-back to the system.
While we acknowledge thattraining models on these types of events may not al-ways provide reliable sources of training data ?
es-pecially as users copy or click on QUAB passagesthat may not be relevant to their interests in the re-search scenario, we believe the initial performanceof these suggests that accurate forms of relevancefeedback can be gathered without the use of mixed-initiative clarification dialogues.6 ConclusionsIn this paper, we have presented a methodology thatcombines feedback that was gathered from users inconjunction with a CRF-based classifier in order toenhance the quality of suggestions returned to usersof interactive Q/A systems.
We have shown thatthe irrelevant QUAB suggestions can be identified atover 90% when systems combine information froma user?s interaction with semantic and pragmatic fea-tures derived from the structure and coherence of aninteractive Q/A dialogue.7 AcknowledgmentsThis material is based upon work funded in wholeor in part by the U.S. Government and any opin-ions, findings, conclusions, or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of the U.S. Gov-ernment.ReferencesSanda Harabagiu, Andrew Hickl, John Lehmann, andDan Moldovan.
2005a.
Experiments with Interac-tive Question-Answering.
In Proceedings of the 43rdAnnual Meeting of the Association for ComputationalLinguistics (ACL?05).S.
Harabagiu, D. Moldovan, C. Clark, M. Bowden, A.Hickl, and P. Wang.
2005b.
Employing Two QuestionAnswering Systems in TREC 2005.
In Proceedings ofthe Fourteenth Text REtrieval Conference.Sanda Harabagiu.
2004.
Incremental Topic Represen-tations.
In Proceedings of the 20th COLING Confer-ence.Andrew Hickl, John Lehmann, John Williams, and SandaHarabagiu.
2004.
Experiments with InteractiveQuestion-Answering in Complex Scenarios.
In Pro-ceedings of the Workshop on the Pragmatics of Ques-tion Answering at HLT-NAACL 2004.T.
Kristjansson, A. Culotta, P. Viola, and A. McCallum.2004.
Interactive information extraction with con-strained conditional random fi elds.
In Proceedings ofAAAI-2004.Chin-Yew Lin and Eduard Hovy.
2000.
The AutomatedAcquisition of Topic Signatures for Text Summariza-tion.
In Proceedings of the 18th COLING Conference.A.
McCallum.
2003.
Effi ciently inducing features ofconditional random fi elds.
In Proceedings of the Nine-teenth Conference on Uncertainty in Artificial Intelli-gence (UAI03).M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An Annotated Corpus of SemanticRoles.
In Computational Linguistics, 31(1):71?106.Jean Scholtz and Emile Morse.
2003.
Using consumerdemands to bridge the gap between software engineer-ing and usability engineering.
In Software Process:Improvement and Practice, 8(2):89?98.F.
Sha and F. Pereira.
2003.
Shallow parsing with condi-tional random fields.
In Proceedings of HLT-NAACL-2003.Sharon Small and Tomek Strzalkowski.
2004.
HITIQA:Towards analytical question answering.
In Proceed-ings of Coling 2004.32
