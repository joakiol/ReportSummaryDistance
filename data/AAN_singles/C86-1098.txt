STORING TEXT USING INTEGER CODESRaja Noor AinonComputer CentreUniversity of Malaya59100 Kuala Lumpur, Malaysia.AbstractTraditionally, text is stored on computers asa stremn of characters.
The goal of this research isto store text in a form that facilitates word manipu-lation whilst reducing storage space.
A word listwith syntactic linear ordering is stored and words ina text are given two-byte integer codes that point totheir respective positions in this list.
The imple-mentation of the encoding scheme is described and theperfomnance statistics of ~lis encoding scheme ispresented.1.0 IntroductionThis research aims at storing text in a formthat facilitates word manipulation whilst savingstorage space.
Although there are many text compre-ssion algorithnts currently in use, the word manipula-tion capability has yet to be incorporated.
Harris\[2\], in his research, compiled a 40,000 word listwith syntactic linear ordering and suggested thatwords in a text be given two-byte integer codes thatpoint to their respective positions in this list.
Inthis way the coded text has inherent syntacticinformation, thereby making it useful for manyapplications including statistical linguistics andquestion-answering systems.
In this paper, we showhow such a scheme can achieve optimal compressionresults and present its efficient implementation.Three text compression techniques that havebeen tested on English texts include (i) the Huffmanvariable-length encoding schemes \[3\] which achievetight packing of data by giving variable-length bitcode for each character, with more frequently-occurring words having shorter codes, (2) theAdaptive pattern substitution algorithm, also knownas the Lempel-Ziv or LZW algorithm (see \[7\], \[8\], \[9\])which converts variable-length strings of inputsymbols into fixed length codes by first looking forcon~non patterns of two or more bytes occurringfrequently, and then substituting an unused byte forthe common long one, and (3) another technique, dueto Hahn \[i\] encodes non-blank characters in groups ofa fixed size as unique fixed point ntunbers.For measuring text compression two definitionsare used in this paper.
One is the compression ratiodefined as the ratio of size of the original text tothat of the coded text.
~he other is the compressionpercentage, defined as(Size of original text - Size of coded text)%(Size of original text)of its size, is treated a~ a unit and is representedin computers in the form of a computer word (usually4 bytes) or part of a word.
It is addressable,hence it does not require a delii~ter like a spaceto distinguish it from a neighbouring number.For this encoding scheme, we endeavour tostore text as a stream of fixed length computer wordswhich is distinguishable by the computer.
This canbe achieved by keeping an external list of all wordsin the dictionary includ\]mg derived ones, andassigning a unique integer code for each entry.Instead of words separated by delimiters the codedtext represents words as numbers, thereby dispensingthe need for representing delimiters.In using two bytes to represent an integerit is possible to have 216 - 1 = 65536 distinctcodes.
However, since it is impossible to havecodes for all the words in the English Lan~lage,it is necessary to include a mechanism that allowsfor the representation of words without codes bytheir individual characters.
Keeping one bit forthat purpose \].eaves 215 - i = 32767 possible numberof combinations.
Two adjoining character codes(ASCII or EBDIC) always have zero as the first bitand is therefore read as a positive integer.
Thefirst bit, being a sicrn bit, can be used to indicatewhether the two bytes represent a code (negativeinteger) or two characters, as follows: -IXXXXXXX XXXXXXXX a codeOXXXXXXX XXXXXXXX two charactersIt is also necessary to show that compressioncan indeed be achieved for this encoding scheme.
Inseveral studies it has been shown (see Kucera \[4\],for example)that  the word frequency distributionin natural language analysis is highly-skewed.Assu~ing a skewed distribution it may be seen thatthe 32000 most-frequently occurring words from theCobuild* corpus constituting 60% of the corpus,account for 99% of the total word usage.
Includingthese 32000 words in the list will imply that wordswithout codes (not included in the list) makes up1% of the text.
Assuming that the average size ofan English word \[4\] consists of 4.7 characters wewould expect thst an average occurring word wouldoccupy, taking one more byte for a trailing space,5.7 bytes.
Thus the compression ratio is\[(2/5.7)* 0.99 + 0.01\] -\] : 2.8, meaning that acoded text is 35.7% of the original text.2.0 The Two-Byte-Word Encoding SchemeIn most computers, text is stored as a streamof characters - made up of alphabtes, spaces, digitsand punctuation ma~s.
Fmch word is separated fromneigh~\]uring words by delimiters such as spaces,~peciaL characters or punctuation nmrks.
On the other3and, a number (integer or floating point) regardless*~he corpus used in the Cobuild Project, a projectin the Engl ishDepartment,  University of Birmingham,is made of 6 mill ion words of written text and 1.3mill ion words of transcribed speech.418The encoding scheme was implemented on aHoneywell computer using MULTICS operating system.Since MULTICS represents characters using ASCIIwith the nine bits per character, two bits are notbeing utilized.
For our implementation, one bit isused to indicate words beginning with upper-casecharacters (proper names, etc) and the otheravailable bit is kept for future develo~nent.3.0 She Word ListI~Lrris \[2\] has constructed a word listwith linear ordering in a sense that all words ina group are derived forms of a baseword (the firstword in the group), and its relative position ~npliesits syntactic information (see figure 1).
Becausethe n~nber of derived forms is not regular, the sizeof a group varies.
Even though the positionalnotation cannot be used in a strict sense as innumbers because of the length variability of thegroups, the relative position of a member in agroup can still provide its syntactic infomnation.For a comprehensive and consistent wordlist, some words which are not in the top 32000 havehad to be included, resulting in a larger word list.As the size of the integer codes cannot exceed32768, the whole word list r~%y have to be reducedby excluding s~ne words in the top 32000.In using the above word list, it is foundthat two problems may arise due to (1) the occur-rence of homographs (words identical in spellingbut different in pronunciation and meaning) andhomologs (words identical in spelling and pronun-~i=~tion but different in meaning~ and (2)theoccurrence of words having the same meaning butdifferent syntax.
If different codes are given tothe duplicate words the encoding process would needan intelligent parser to be able to differentiatebetween the two.
Such a parser, though notimpossible to implement, is not cost-justifiablefor this study; hence the only alternative is toassign the code for both words and to allow forambiguity when the coded text is used for analysis.Set A (4 verb forms)arch arches arching archedbid bids bidding bidround rounds rounding roundedSet B (4 verb fomns + 1 noun)abnndonacclaimconsentabmdons abandoningacclaims acclaimingconsents consentingabandoned abandonmentaccla~ned acclamationconsented consentVig.
14.0 ImplementationFor encoding, a code table c~nprising morethan 32000 distinct words in the word list indicatingtheir codes is stored in an indexed file using thewords as keys.
For faster encoding, the top 200words from dle Cobuild corpus are stored in a hashtable.
During encoding this hash table is searchedbefore searching the code table, therefore savingexecution time when enccx\]ing con~non words.For word mani~:~lation of the encoded textthe word list needs to k~ structured in order thatthe syntactic informaticm is captured.
That is, thegroup and the set (set-~Type)to which the wordbelongs and the relative position of the word inthe group needs robe  ir~icated.
For our ~llplemen-tation a l~ed list is employed to store a wordwhich has liJ{s to the ~mse word (the first wordin the group) and the next word in the group andinformation containing its set-type.
Each node inthe linked list is stored as a record in an indexedfile using the codes as keys.
In this way each wordin a group can be retrieved individually and thegroup can be synthesized from tracing the links tothe next word.
There is further gain in using thecodes as the search key in that the same file canbe used for ~le decoding process.5.0 Some Sample StatisticsTable 1 gives the perfomnance statisticsof the two-byte-word encoding scheme on four Englishtexts.
Comparison of compression ratios with othertechniques is sheik) in Table 2.Table 1 Perfoz~kqnce statistics of the two-byte-wordencoding scheme on several English textsText Text I a Text 2 b Text 3 c Text 4 dI.
Size ofinput text 2,927,745 2,542,977 3,605,931 2,840,193(in bits)2.
No.
ofTokens3.
No.
ofuncodedwords4.5.6.7.8.9.6005 5789 7844 7\].481120 181 "7 2860 2069CompressionRatio 2.08 2.02 2.00 1.90CompressionPercentage 51.93% 50.47 % 49.87 % 47.42 %Percentageof Textthat iscoded91% 78% 77% 8\]%EncodingTime (inSecs)56 58 78 64Word Fre-quency Countof Coded Text(in Secs)37 39 52 45Word Fre-quency Count 187 \[\].78 239 notof Uncoded availableText (in Secs)a.
"S~nall is Beautiful" by E.F. Scht~nacherb.
"Baby and Child Care" by Dr. B. Speckc.
"She Third World War" by Sir John Hacketd.
"The ArLlericans" by A. Cooke419Table 2 Comparison of the Two-Byte-WordEncoding Scheme with other AlgorithmsTechniquePechura \[5\] 1.5Welch \[7\] 1.8Huf fman \[ 3 \] i.
9Two-byte-word 2.0Hahn \[ 1 \] 2.4Rubin \[6\] 2.4From Table 1 it is observed that on theaverage the compression percentage is 82%.
Althoughshort of the 99% mentioned in section 2.0, this isexpected,since not all of the top 32000 words havebeen included in the word list.
The compressionratio as shown in the fourth row has a mean of 2.0.Comparing the last row with the sl~m of the 7th and8th row it is seen that the speed for word frequencycount for the coded texts is much faster than thatof the coded texts.6.0 Some Practical ApplicationsFor purposes of storing compressed text,the two-byte-word encoding scheme can be usedindependent of the word list and some results areshow\]\] in the previous section.
It maybe seen thatthe performance of the scheme is comparable withother well-known techniques (see Table 2).
Withthe word list the scheme is capable of wordnmniDulation and therefore it can be used for moreintelligent applications.
For example, the schemehas been used for obtaining the le~natized wordcount of several large texts - an almost impossibletask when done manually.
No comparison results isshown s~nplybecause no similar system currentlyexists.Because the words in the coded text arerepresented by integers, word comparison - a commontask in linguistic research involving comparison ofcharacter by character - becomes comparison of twonumbers which is a c~lick and simple operation ondigital computers.
This is reflected quitedramatically on obtaining the word frequencycount of the coded text.Agknowled~ementsI wish to thank Professor John Sinclairof the English Department, University of Birminghamfor the use of research facilities in the Cobuildproject.
I a~l also grateful to the University ofMalaya for providing the funds for my stay inBritain.Referencesi.
Hahn, B., "A new technique for compression andstorage of data", CAC~4 Aug. 1974, Vol.
17, No.8,pp.
434-436.2.
Harris, S., research report, English LanguageDepartment, University of Birmingham.3.
Huffman, D.A., "A method for the constructionfor minimum redundancy codes" Proc.
IRE40,40, 9 (Sept. 1952), 1098-1101.4.
Kucera, H. and Francis, W.N., "Computationalanalysis of present-day American English",Brown Univ.
Press 1967.5.
Pechura, M.," File archival techniques usingdata compression", CACM, Vol.
25, No.
9, Sept.1982, pp.
605-609.6.
Rubin, F., "Exper~nents in text file compression"CACM, Vol.
19, No.
ii, Nov. 1976, pp.
617-623.7.
Welch, T.A., "A technique for high-performancedata compression", IEEE Computer, June 1984,pp.
8 - 19.8.
Ziv, J. and Lempel, A., "A Universal algorit\[mlfor sequential data compression" IEEE Trans.Information Theory, Vol.
IT-23, No.
3, May 1977,pp.
337-343.9.
Ziv, J .
and Lempel, A., "Compression of individualsequences via variable-rate coding", IEEE Trans.Information TheoLy, Vol.
IT-24, No.
5, Sept.1978, pp.
5306.420
