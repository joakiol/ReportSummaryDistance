Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 800?807,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsOn the role of context and prosody in the interpretation of ?okay?Agust?
?n Gravano, Stefan Benus, He?ctor Cha?vez, Julia Hirschberg, Lauren WilcoxDepartment of Computer ScienceColumbia University, New York, NY, USA{agus,sbenus,hrc2009,julia,lgw23}@cs.columbia.eduAbstractWe examine the effect of contextual andacoustic cues in the disambiguation of threediscourse-pragmatic functions of the wordokay.
Results of a perception study showthat contextual cues are stronger predictorsof discourse function than acoustic cues.However, acoustic features capturing thepitch excursion at the right edge of okay fea-ture prominently in disambiguation, whetherother contextual cues are present or not.1 IntroductionCUE PHRASES (also known as DISCOURSE MARK-ERS) are linguistic expressions that can be used toconvey explicit information about the structure ofa discourse or to convey a semantic contribution(Grosz and Sidner, 1986; Reichman, 1985; Cohen,1984).
For example, the word okay can be used toconvey a ?satisfactory?
evaluation of some entity inthe discourse (the movie was okay); as a backchan-nel in a dialogue to indicate that one interlocutoris still attending to another; to convey acknowledg-ment or agreement; or, in its ?cue?
use, to start or fin-ish a discourse segment (Jefferson, 1972; Schegloffand Sacks, 1973; Kowtko, 1997; Ward and Tsuka-hara, 2000).
A major question is how speakers indi-cate and listeners interpret such variation in mean-ing.
From a practical perspective, understandinghow speakers and listeners disambiguate cue phrasesis important to spoken dialogue systems, so that sys-tems can convey potentially ambiguous terms withtheir intended meaning and can interpret user inputcorrectly.There is considerable evidence that the differentuses of individual cue phrases can be distinguishedby variation in the prosody with which they are re-alized.
For example, (Hirschberg and Litman, 1993)found that cue phrases in general could be disam-biguated between their ?semantic?
and their ?dis-course marker?
uses in terms of the type of pitchaccent borne by the cue phrase, the position of thephrase in the intonational phrase, and the amountof additional information in the phrase.
Despite thefrequence of the word okay in natural dialogues,relatively little attention has been paid to the rela-tionship between its use and its prosodic realization.
(Hockey, 1993) did find that okay differs in terms ofthe pitch contour speakers use in uttering it, suggest-ing that a final rising pitch contour ?categoricallymarks a turn change,?
while a downstepped fallingpitch contour usually indicates a discourse segmentboundary.
However, it is not clear which, if any, ofthe prosodic differences identified in this study areactually used by listeners in interpreting these po-tentially ambiguous items.In this study, we address the question of how hear-ers disambiguate the interpretation of okay.
Our goalis to identify the acoustic, prosodic and phonetic fea-tures of okay tokens for which listeners assign differ-ent meanings.
Additionally, we want to determinethe role that discourse context plays in this classi-fication: i.e., can subjects classify okay tokens reli-ably from the word alone or do they require addi-tional context?Below we describe a perception study in whichlisteners were presented with a number of spokenproductions of okay, taken from a corpus of dia-logues between subjects playing a computer game.The tokens were presented both in isolation and incontext.
Users were asked to select the meaning800of each token from three of the meanings that okaycan take on: ACKNOWLEDGEMENT/AGREEMENT,BACKCHANNEL, and CUE OF AN INITIAL DIS-COURSE SEGMENT.
Subsequently, we examined theacoustic, prosodic and phonetic correlates of theseclassifications to try to infer what cues listeners usedto interpret the tokens, and how these varied by con-text condition.
Section 2 describes our corpus.
Sec-tion 3 describes the perception experiment.
In Sec-tion 4 we analyze inter-subject agreement, introducea novel representation of subject judgments, and ex-amine the acoustic, prosodic, phonetic and contex-tual correlates of subject classification of okays.
InSection 5 we discuss our results and future work.2 CorpusThe materials for our perception study were selectedfrom a portion of the Columbia Games Corpus, acollection of 12 spontaneous task-oriented dyadicconversations elicited from speakers of StandardAmerican English.
The corpus was collected andannotated jointly by the Spoken Language Groupat Columbia University and the Department of Lin-guistics at Northwestern University.Subjects were paid to play two series of com-puter games (the CARDS GAMES and the OBJECTSGAMES), requiring collaboration between partnersto achieve a common goal.
Participants sat in frontof laptops in a soundproof booth with a curtain be-tween them, so that all communication would be ver-bal.
Each player played with two different partnersin two different sessions.
On average, each sessiontook 45m 39s, totalling 9h 8m of dialogue for thewhole corpus.
All interactions were recorded, digi-tized, and downsampled to 16K.The recordings were orthographically transcribedand words were aligned by hand by trained annota-tors in a ToBI (Beckman and Hirschberg, 1994) or-thographic tier using Praat (Boersma and Weenink,2001) to manipulate waveforms.
The corpus con-tains 2239 unique words, with 73,831 words in total.Nearly all of the Objects Games part of the corpushas been intonationally transcribed, using the ToBIconventions.
Pitch, energy and duration informationhas been extracted for the entire corpus automati-cally, using Praat.In the Objects Games portion of the corpus eachplayer?s laptop displayed a gameboard containing 5?7 objects (Figure 1).
In each segment of the game,both players saw the same set of objects at the sameposition on each screen, except for one object (theTARGET).
For one player (the DESCRIBER), this tar-get appeared in a random location among other ob-jects on the screen.
For the other player (the FOL-LOWER), the target object appeared at the bottom ofthe screen.
The describer was instructed to describethe position of the target object on their screen sothat the follower could move their representation ofthe target to the same location on their own screen.After the players had negotiated what they deter-mined to be the best location, they were awardedup to 100 points based on the actual match of thetarget location on the two screens.
The game pro-ceeded in this way through 14 tasks, with describerand follower alternating roles.
On average, the Ob-jects Games portion of each session took 21m 36s,resulting in 4h 19m of dialogue for the twelve ses-sions in the corpus.
There are 1484 unique words inthis portion of the corpus, and 36,503 words in total.Figure 1: Sample screen of the Objects Games.Throughout the Objects Games, we noted thatsubjects made frequent use of affirmative cue words,such as okay, yeah, alright, which appeared to varyin meaning.
To investigate the discourse functionsof such words, we first asked three labelers to inde-pendently classify all occurrences of alright, gotcha,huh, mmhm, okay, right, uhhuh, yeah, yep, yes, yupin the entire Games Corpus into one of ten cate-gories, including acknowledgment/agreement, cuebeginning or ending discourse segment, backchan-nel, and literal modifier.
Labelers were asked to801choose the most appropriate category for each to-ken, or indicate with ???
if they could not make adecision.
They were allowed to read the transcriptsand listen to the speech as they labeled.For our perception experiment we chose materialsfrom the tokens of the most frequent of our labeledaffirmative words, okay, from the Objects Games,which contained most of these tokens.
Altogether,there are 1151 instances of okay in this part of thecorpus; it is the third most frequent word, follow-ing the, with 4565 instances, and of, with 1534.At least two labelers agreed on the functional cat-egory of 902 (78%) okay tokens.
Of those tokens,286 (32%) were classified as BACKCHANNEL, 255(28%) as ACKNOWLEDGEMENT/AGREEMENT, 141(16%) as CUE BEGINNING, 116 (13%) as PIVOTBEGINNING (a function that combines Acknowl-edgement/agreement and Cue beginning), and 104(11%) as one of the other functions.
We sampledfrom tokens the annotators had labeled as Cue be-ginning discourse segment, Backchannel, and Ac-knowledgement/agreement, the most frequent cate-gories in the corpus; we will refer to these belowsimply as ?C?, ?B?, and ?A?
classes, respectively.3 ExperimentWe next designed a perception experiment to ex-amine naive subjects?
perception of these tokens ofokay.
To obtain good coverage both of the (labeled)A, B, and C classes, as well as the degrees of po-tential ambiguity among these classes, we identified9 categories of okay tokens to include in the experi-ment: 3 classes (A, B, C) ?
3 levels of labeler agree-ment (UNANIMOUS, MAJORITY, NO-AGREEMENT).?Unanimous?
refers to tokens assigned to a particu-lar class label by all 3 labelers, ?majority?
to tokensassigned to this class by 2 of the 3 labelers, and ?no-agreement?
to tokens assigned to this class by only1 labeler.
To decrease variability in the stimuli, weselected tokens only from speakers who produced atleast one token for each of the 9 conditions.
Therewere 6 such speakers (3 female, 3 male), which gaveus a total of 54 tokens.To see whether subjects?
classifications of okaywere dependent upon contextual information or not,we prepared two versions of each token.
The iso-lated versions consisted of only the word okay ex-tracted from the waveform.
For the contextualizedversions, we extracted two full speaker turns foreach okay including the full turn1 containing the tar-get okay plus the full turn of the previous speaker.
Inthe following three sample contexts, pauses are indi-cated with ?#?, and the target okays are underlined:Speaker A: yeah # um there?s like there?s some space there?sSpeaker B: okay # I think I got itSpeaker A: but it?s gonna be below the onionSpeaker B: okaySpeaker A: okay # alright # I?ll try it # okaySpeaker B: okay the owl is blinkingThe isolated okay tokens were single channel au-dio files; the contextualized okay tokens were for-matted so that each speaker was presented to sub-jects on a different channel, with the speaker utteringthe target okay consistently on the same channel.The perception study was divided into two parts.In the first part, each subject was presented withthe 54 isolated okay tokens, in a different ran-dom ordering for each subject.
They were givena forced choice task to classify them as A, B, orC, with the corresponding labels (Acknowledge-ment/agreement, Backchannel, and Cue beginning)also presented in a random order for each token.
Inthe second part, the same subject was given 54 con-textualized tokens, presented in a different randomorder, and asked to make the same choice.We recruited 20 (paid) subjects for the study, 10female, and 10 male, all between the ages of 20 and60.
All subjects were native speakers of StandardAmerican English, except for one subject who wasborn in Jamaica but a native speaker of English.
Allsubjects reported no hearing problems.
Subjects per-formed the study in a quiet lab using headphones tolisten to the tokens and indicating their classificationdecisions in a GUI interface on a lab workstation.They were given instructions on how to use the in-terface before each of the two sections of the study.For the study itself, for each token in the isolatedcondition, subjects were shown a screen with thethree randomly ordered classes and a link to the to-ken?s sound file.
They could listen to the sound filesas many times as they wished but were instructednot to be concerned with answering the questions1We define a TURN as a maximal sequence of words spokenby the same speaker during which the speaker holds the floor.802?correctly?, but to answer with their immediate re-sponse if possible.
However, they were allowed tochange their selection as many times as they likedbefore moving to the next screen.
In the contex-tualized condition, they were also shown an ortho-graphic transcription of part of the contextualized to-ken, to help them identify the target okay.
The meanduration of the first part of the study was 25 minutes,and of the second part, 27 minutes.4 Results4.1 Subject ratingsThe distribution of class labels in each experimentalcondition is shown in Table 1.
While this distribu-tion roughly mirrors our selection of equal numbersof tokens from each previously-labeled class, in bothparts of the study more tokens were labeled as A(acknowledgment/agreement) than as B (backchan-nel) or C (cue to topic beginning).
This supportsthe hypothesis that acknowledgment/agreement mayfunction as the default interpretation of okay.Isolated ContextualizedA 426 (39%) 452 (42%)B 324 (30%) 306 (28%)C 330 (31%) 322 (30%)Total 1080 (100%) 1080 (100%)Table 1: Distribution of label classes in eachstudy condition.We examined inter-subject agreement usingFleiss?
?
measure of inter-rater agreement for mul-tiple raters (Fleiss, 1971).2 Table 2 shows Fleiss?
?calculated for each individual label vs. the other twolabels and for all three labels, in both study condi-tions.
From this table we see that, while there is verylittle overall agreement among subjects about howto classify tokens in the isolated condition, agree-ment is higher in the contextualized condition, witha moderate agreement for class C (?
score of .497).This suggests that context helps distinguish the cuebeginning discourse segment function more than theother two functions of okay.2 This measure of agreement above chance is interpreted asfollows: 0 = None, 0 - 0.2 = Small, 0.2 - 0.4 = Fair, 0.4 - 0.6 =Moderate, 0.6 - 0.8 = Substantial, 0.8 - 1 = Almost perfect.Isolated ContextualizedA vs. rest .089 .227B vs. rest .118 .164C vs. rest .157 .497all .120 .293Table 2: Fleiss?
?
for each label classin each study condition.Recall from Section 3 that the okay tokens werechosen in equal numbers from three classes accord-ing to the level of agreement of our three originallabelers (unanimous, majority, and no-agreement),who had the full dialogue context to use in makingtheir decisions.
Table 3 shows Fleiss?
?
measurenow grouped by amount of agreement of the orig-inal labelers, again presented for each context con-dition.
We see here that the inter-subject agreementIsolated Context.
OLno-agreement .085 .104 -majority .092 .299 -unanimous .158 .452 -all .120 .293 .312Table 3: Fleiss?
?
in each study condition, groupedby agreement of the three original labelers (?OL?
).also mirrors the agreement of the three original la-belers.
In both study conditions, tokens which theoriginal labelers agreed on also had the highest ?scores, followed by tokens in the majority and no-agreement classes, in that order.
In all cases, tokenswhich subjects heard in context showed more agree-ment than those they heard in isolation.The overall ?
is small at .120 for the isolated con-dition, and fair at .293 for the contextualized con-dition.
The three original labelers also achieved fairagreement at .312.3 The similarity between the lat-ter two ?
scores suggests that the full context avail-able to the original labelers and the limited contextpresented to the experiment subjets offer compara-ble amounts of information to disambiguate betweenthe three functions, although lack of any contextclearly affected subjects?
decisions.
We conclude3 For the calculation of this ?, we considered four labelclasses: A, B, C, and a fourth class ?other?
that comprises theremaining 7 word functions mentioned in Section 2.
In conse-quence, these ?
scores should be compared with caution.803from these results that context is of considerable im-portance in the interpretation of the word okay, al-though even a very limited context appears to suf-fice.4.2 Representing subject judgmentsIn this section, we present a graphical representa-tion of subject decisions, useful for interpreting, vi-sualizing, and comparing the way our subjects in-terpreted the different tokens of okay.
For each in-dividual okay in the study, we define an associatedthree-dimensional VOTE VECTOR, whose compo-nents are the proportions of subjects that classifiedthe token as A, B or C. For example, if a particu-lar okay was labeled as A by 5 subjects, as B by 3,and as C by 12, then its associated vote vector is( 520 ,320 ,1220)= (0.25, 0.15, 0.6).
Following this def-inition, the vectors A = (1, 0, 0), B = (0, 1, 0) andC = (0, 0, 1) correspond to the ideal situations inwhich all 20 subjects agreed on the label.
We callthese vectors the UNANIMOUS-VOTE VECTORS.Figure 2.i shows a two-dimensional representa-tion that illustrates these definitions.
The black dotFigure 2: 2D representation of a vote vector (i)and of the cluster centroids (ii).represents the vote vector for our example okay,the vertices of the triangle correspond to the threeunanimous-vote vectors (A, B and C), and the crossin the center of the triangle represents the vote vectorof a three-way tie between the labelers (13 , 13 , 13).We are thus able to calculate the Euclidean dis-tance of a vote vector to each of the unanimous-votevectors.
The shortest of these distances correspondsto the label assigned by the plurality4 of subjects.Also, the smaller that distance, the higher the inter-subject agreement for that particular token.
For our4Plurality is also known as simple majority: the candidatewho gets more votes than any other candidate is the winner.example okay, the distances to A, B and C are 0.972,1.070 and 0.495, respectively; its plurality label is C.In our experiment, each okay has two associatedvote vectors, one for each context condition.
Toillustrate the relationship between decisions in theisolated and the contextualized conditions, we firstgrouped each condition?s 54 vote vectors into threeclusters, according to their plurality label.
Figure2.ii shows the cluster centroids in a two-dimensionalrepresentation of vote vectors.
The filled dots corre-spond to the cluster centroids of the isolated condi-tion, and the empty dots, to the centroids of the con-textualized condition.
Table 4 shows the distancesin each condition from the cluster centroids (denotedAc, Bc, Cc) to the respective unanimous-vote vec-tors (A, B, C), and also the distance between eachpair of cluster centroids.Isolated Contextualizedd(Ac,A) .54 .44 (?18%)d(Bc,B) .57 .52 (?10%)d(Cc, C) .52 .28 (?47%)d(Ac, Bc) .41 .48 (+17%)d(Ac, Cc) .49 .86 (+75%)d(Bc, Cc) .54 .91 (+69%)Table 4: Distances from the cluster centroids (Ac,Bc, Cc) to the unanimous-vote vectors (A, B, C)and between cluster centroids, in each condition.In the isolated condition, the three cluster cen-troids are approximately equidistant from each other?that is, the three word functions appear to beequally confusable.
In the contextualized condi-tion, while Cc is further apart from the other twocentroids, the distance between Ac and Bc remainspractically the same.
This suggests that, with somecontext available, A and B tokens are still fairly con-fusable, while both are more easily distinguishedfrom C tokens.
We posit two possible explanationsfor this: First, C is the only function for whichthe speaker uttering the okay necessarily continuesspeaking; thus the role of context in disambiguat-ing seems quite clear.
Second, both A and B have acommon element of ?acknowledgement?
that mightaffect inter-subject agreement.8044.3 Features of the okay tokensIn this section, we describe a set of acoustic,prosodic, phonetic and contextual features whichmay help to explain why subjects interpret okay dif-ferently.
Acoustic features were extracted automat-ically using Praat.
Phonetic and prosodic featureswere hand-labeled by expert annotators.
Contextualfeatures were considered only in the analysis of thecontextualized condition, since they were not avail-able to subjects in the isolated condition.We examined a number of phonetic features to de-termine whether these correlated with subject clas-sifications.
We first looked at the production of thethree phonemes in the target okay (/oU/, /k/, /eI/),noting the following possible variations:?
/oU/: [], [A], [5], [O], [OU], [m], [N], [@], [@U].?
/k/: [G], [k], [kx], [q], [x].?
/eI/: [e], [eI], [E], [e@].We also calculated the duration of each phone andof the velar closure.
Whether the target okay was atleast partially whispered or not, and whether therewas glottalization in the target okay were also noted.For each target okay, we also examined its du-ration and its maximum, mean and minimum pitchand intensity, as well as the speaker-normalized ver-sions of these values.5 We considered its pitch slope,intensity slope, and stylized pitch slope, calculatedover the whole target okay, its last 50, 80 and 100milliseconds, its second half, its second syllable, andthe second half of its second syllable, as well.We used the ToBI labeling scheme (Pitrelli et al,1994) to label the prosody of the target okays andtheir surrounding context.?
Pitch accent, if any, of the target okay (e.g., H*,H+!H*, L*).?
Break index after the target okay (0-4).?
Phrase accent and boundary tone, if any, fol-lowing the target okay (e.g., L-L%, !H-H%).For contextualized tokens, we included several fea-tures related to the exchange between the speakeruttering the target okay (Speaker B) and the otherspeaker (Speaker A).5Speaker-normalized features were normalized by comput-ing z-scores (z = (X ?mean)/st.dev) for the feature, wheremean and st.dev were calculated from all okays uttered by thespeaker in the session.?
Number of words uttered by Speaker A in thecontext, before and after the target okay.
Samefor Speaker B.?
Latency of Speaker A before Speaker B?s turn.?
Duration of silence of Speaker B before and af-ter the target okay.?
Duration of speech by Speaker B immediatelybefore and after the target okay and up to a si-lence.4.4 Cues to interpretationWe conducted a series of Pearson?s tests to look forcorrelations between the proportion of subjects thatchose each label and the numeric features describedin Section 4.3, together with two-sided t-tests to findwhether such correlations differed significantly fromzero.
Tables 5 and 6 show the significant results(two-sided t-tests, p < 0.05) for the isolated andcontextualized conditions, respectively.Acknowledgement/agreement rduration of realization of /k/ ?0.299Backchannel rstylized pitch slope over 2nd half 2nd syl.
0.752pitch slope over 2nd half of 2nd syllable 0.409speaker-normalized maximum intensity ?0.372pitch slope over last 80 ms 0.349speaker-normalized mean intensity ?0.327duration of realization of /eI/ 0.278word duration 0.277Cue to discourse segment beginning rstylized pitch slope over the whole word ?0.380pitch slope over the whole word ?0.342pitch slope over 2nd half of 2nd syllable ?0.319Table 5: Features correlated to the proportion ofvotes for each label.
Isolated condition.Table 5 shows that in the isolated condition, sub-jects tended to classify tokens of okay as Acknowl-edgment/agreement (A) which had a longer realiza-tion of the /k/ phoneme.
They tended to classifytokens as Backchannels (B) which had a lower in-tensity, a longer duration, a longer realization of the/eI/ phoneme, and a final rising pitch.
They tendedto classify tokens as C (cue to topic beginning) thatended with falling pitch.805Acknowledgement/agreement rlatency of Spkr A before Spkr B?s turn ?0.528duration of silence by Spkr B before okay ?0.404number of words by Spkr B after okay ?0.277Backchannel rpitch slope over 2nd half of 2nd syllable 0.520pitch slope over last 80 ms 0.455number of words by Spkr A before okay 0.451number of words by Spkr B after okay ?0.433duration of speech by Spkr B after okay ?0.413latency of Spkr A before Spkr B?s turn ?0.385duration of silence by Spkr B before okay 0.295intensity slope over 2nd syllable ?0.279Cue to discourse segment beginning rlatency of Spkr A before Spkr B?s turn 0.645number of words by Spkr B after okay 0.481number of words by Spkr A before okay ?0.426pitch slope over 2nd half of 2nd syllable ?0.385pitch slope over last 80 ms ?0.377duration of speech by Spkr B after okay 0.338Table 6: Features correlated to the proportion ofvotes for each label.
Contextualized condition.In the contextualized condition, we find very dif-ferent correlations.
Table 6 shows that nearly all ofthe strong correlations in this condition involve con-textual features, such as the latency before SpeakerB?s turn, or the number of words by each speaker be-fore and after the target okay.
Notably, only one ofthe features that show strong correlations in the iso-lated condition shows the same strong correlation inthe contextualized condition: the pitch slope at theend of the word.
In both conditions subjects tendedto label tokens with a final rising pitch contour asB, and tokens with a final falling pitch contour as C.This supports (Hockey, 1993)?s findings on the roleof pitch contour in disambiguating okay.We next conducted a series of two-sided Fisher?sexact tests to find correlations between subjects?
la-belings of okay and the nominal features describedin Section 4.3.
We found significant associations be-tween the realization of the /oU/ phoneme and theokay function in the isolated condition (p < 0.005).Table 7 shows that, in particular, [m] seems to be thepreferred realization for B okays, while [@] seems tobe the preferred one for A okays, and [OU] and [O]for A and C okays.?
[A] [5] [OU] [O] [N] [@U] [@] [] [m]A 0 0 5 6 4 0 0 8 0 0B 2 0 4 1 0 1 0 1 1 5C 1 1 2 3 4 0 1 3 0 0Table 7: Realization of the /oU/ phoneme, groupedby subject plurality label.
Isolated condition only.Notably, we did not find such significant asso-ciations in the contextualized condition.
We didfind significant correlations in both conditions, how-ever, between okay classifications and the type ofphrase accent and boundary tone following the target(Fisher?s Exact Test, p < 0.05 for the isolated con-dition, p < 0.005 for the contextualized condition).Table 8 shows that L-L% tends to be associated withA and C classes, H-H% with B classes, and L-H%with A and B classes.
In this case, such correlationsare present in the isolated condition, and sustainedor enhanced in the contextualized condition.H-H% H-L% L-H% L-L% otherIsolatedA 0 2 4 8 9B 3 3 1 5 3C 1 1 0 8 5Context.A 0 2 3 10 10B 4 3 2 1 2C 0 1 0 10 5Table 8: Phrase accent and boundary tone, groupedby subject plurality label.Summing up, when subjects listened to the okaytokens in isolation, with only their acoustic, prosodicand phonetic properties available, a few featuresseem to strongly correlate with the perception ofword function; for example, maximum intensity,word duration, and realizing the /oU/ phoneme as[m] tend to be associated with backchannel, whilethe duration of the realization of the /k/ phoneme,and realizing the /oU/ phoneme as [@] tend to be as-sociated with acknowledgment/agreement.In the second part of the study, when subjectslistened to contextualized versions of the same to-kens of okay, most of the strong correlations of wordfunction with acoustic, prosodic and phonetic fea-tures were replaced by correlations with contextualfeatures, like latency and turn duration.
In otherwords, these results suggest that contextual features806might override the effect of most acoustic, prosodicand phonetic features of okay.
There is nonethe-less one notable exception: word final intonation ?captured by the pitch slope and the ToBI labels forphrase accent and boundary tone ?
seems to play acentral role in the interpretation of both isolated andcontextualized okays.5 Conclusion and future workIn this study, we have presented evidence of differ-ences in the interpretation of the function of isolatedand contextualized okays.
We have shown that wordfinal intonation strongly correlates with the subjects?classification of okays in both conditions.
Addition-ally, the higher degree of inter-subject agreement inthe contextualized condition, along with the strongcorrelations found for contextualized features, sug-gests that context, when available, plays a centralrole in the disambiguation of okay.
(Note, how-ever, that further research is needed in order to assesswhether these features are indeed, in fact, perceptu-ally important, both individually and combined.
)We have also presented results suggesting that ac-knowledgment/agreement acts as a default functionfor both isolated an contextualized okays.
Further-more, while that function remains confusable withbackchannel in both conditions, the availability ofsome context helps in distinguishing those two func-tions from cue to topic beginning.These results are relevant to spoken dialogue sys-tems in suggesting how systems can convey the cueword okay with the intended meaning and can inter-pret users?
productions of okay correctly.
How theseresults extend to other cue words and to other wordfunctions remains an open question.As future work, we will extend this study to in-clude the over 5800 occurrences of alright, gotcha,huh, mmhm, okay, right, uhhuh, yeah, yep, yes, yupin the entire Games Corpus, and all 10 discoursefunctions mentioned in Section 2, as annotated byour three original labelers.
Since we have observedconsiderable differences in conversation style in thetwo parts of the corpus (the Objects Games elicitedmore ?dynamic?
conversations, with more overlapsand interruptions than the Cards Games), we willcompare cue phrase usage in these two settings.
Fi-nally, we are also interested in examining speakerentrainment in cue phrase usage, or how subjectsadapt their choice and production of cue phrases totheir conversation partner?s.AcknowledgmentsThis work was funded in part by NSF IIS-0307905.We thank Gregory Ward, Elisa Sneed, and MichaelMulley for their valuable help in collecting and la-beling the data, and the anonymous reviewers forhelpful comments and suggestions.ReferencesMary E. Beckman and Julia Hirschberg.
1994.
The ToBIannotation conventions.
Ohio State University.Paul Boersma and David Weenink.
2001.
Praat: Doingphonetics by computer.
http://www.praat.org.Robin Cohen.
1984.
A computational theory of the func-tion of clue words in argument understanding.
22ndConference of the ACL, pages 251?258.Joseph L. Fleiss.
1971.
Measuring nominal scale agree-ment among many raters.
Psychological Bulletin,76(5):378?382.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, Intentions, and the Structure of Discourse.
Com-putational Linguistics, 12(3):175?204.Julia Hirschberg and Diane Litman.
1993.
EmpiricalStudies on the Disambiguation of Cue Phrases.
Com-putational Linguistics, 19(3):501?530.Beth Ann Hockey.
1993.
Prosody and the role of okayand uh-huh in discourse.
Proceedings of the EasternStates Conference on Linguistics, pages 128?136.Gail Jefferson.
1972.
Side sequences.
Studies in socialinteraction, 294:338.Jacqueline C. Kowtko.
1997.
The function of intonationin task-oriented dialogue.
Ph.D. thesis, University ofEdinburgh.John Pitrelli, Mary Beckman, and Julia Hirschberg.1994.
Evaluation of prosodic transcription labelingreliability in the ToBI framework.
In ICSLP94, vol-ume 2, pages 123?126, Yokohama, Japan.Rachel Reichman.
1985.
Getting Computers to Talk LikeYou and Me: Discourse Context, Focus, and Seman-tics: (an ATN Model).
MIT Press.Emanuel A. Schegloff and Harvey Sacks.
1973.
Openingup closings.
Semiotica, 8(4):289?327.Nigel Ward and Wataru Tsukahara.
2000.
Prosodic fea-tures which cue back-channel responses in English andJapanese.
Journal of Pragmatics, 23:1177?1207.807
