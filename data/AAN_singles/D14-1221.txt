Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2070?2081,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsRecall Error Analysis for Coreference ResolutionSebastian Martschat and Michael StrubeHeidelberg Institute for Theoretical Studies gGmbHSchloss-Wolfsbrunnenweg 35, 69118 Heidelberg, Germany(sebastian.martschat|michael.strube)@h-its.orgAbstractWe present a novel method for coreferenceresolution error analysis which we applyto perform a recall error analysis of fourstate-of-the-art English coreference reso-lution systems.
Our analysis highlightsdifferences between the systems and iden-tifies that the majority of recall errors fornouns and names are shared by all sys-tems.
We characterize this set of com-mon challenging errors in terms of a broadrange of lexical and semantic properties.1 IntroductionCoreference resolution is the task of determiningwhich mentions in a text refer to the same entity.State-of-the-art approaches include both learning-based (Fernandes et al., 2012; Bj?orkelund andFarkas, 2012; Durrett and Klein, 2013) and de-terministic models (Lee et al., 2013; Martschat,2013).
These approaches achieve state-of-the-artperformance mainly relying on morphosyntacticand lexical factors.
However, consider the follow-ing example.In order to improving the added valueof oil products, the second phase projectof the Qinghai Petroleum Bureau?sGe?ermu oil refinery has been put intoproduction.
This will further improvethe factory?s oil products structure.Due to the lack of any string overlap, moststate-of-the-art systems will miss the link betweenthe factory and the Qinghai Petroleum Bureau?sGe?ermu oil refinery.
The information that factoryis a hypernym of refinery, however, may be usefulto resolve such links.The aim of this paper is to quantify and char-acterize such recall errors made by state-of-the-art coreference resolution systems.
By doing so,we provide a solid foundation for work on em-ploying knowledge sources for improving recallfor coreference resolution (Ponzetto and Strube,2006; Rahman and Ng, 2011; Ratinov and Roth,2012; Bansal and Klein, 2012, inter alia).
In par-ticular, we make the following contributions:We present a novel framework for coreferenceresolution error analysis.
This yields a formalfoundation for previous work on link-based erroranalysis (Uryupina, 2008; Martschat, 2013) andcomplements work on transformation-based erroranalysis (Kummerfeld and Klein, 2013).We apply the method proposed in this paper toperform a recall error analysis of four state-of-the-art systems, encompassing deterministic andlearning-based approaches.
In particular, we iden-tify and characterize a set of challenging errorscommon to all systems, and discuss strengths andweaknesses of each system regarding specific er-ror types.
We also present a brief precision erroranalysis.A toolkit which implements the framework pro-posed in this paper is available for download.12 A Link-Based Analysis FrameworkIn this section we discuss challenges in corefer-ence resolution error analysis and devise an erroranalysis framework to overcome these challenges.2.1 MotivationSuppose a document contains the entity BARACKOBAMA, which is referenced by four mentionsin the following order: Obama, he, the presidentand his.
A typical output of a current system notequipped with world knowledge will consist oftwo entities: {Obama, he} and {the president, his}Obviously, the system made a recall error.
But,due to the complex nature of the coreference reso-lution task, it is not clear how to represent the re-1http://smartschat.de/software2070(a)m1Obamam2hem3the presidentm4his(b)m1m2m3the presidentm4n1n2n3(c)m1m2m3the presidentm4(d)m1m2m3the presidentm4Figure 1: (a) a reference entity r, represented as a complete one-directional graph, (b) a set S of threesystem entities, (c) the partition rS, (d) a spanning tree for r.call error: is it missing the link between the pres-ident and Obama?
Can the error be attributed todeficiencies in pronoun resolution?Linguistically motivated error representationswould facilitate both understanding of currentchallenges and make system development fasterand easier.
The aim of this section is to devisesuch representations.2.2 Formalizing Coreference ResolutionTo start with, we give a formal description of thecoreference resolution task following the termi-nology used for the ACE (Mitchell et al., 2004)and OntoNotes (Weischedel et al., 2013) projects.A mention is a linguistic realization of a referenceto an entity.
Two mentions corefer if they refer tothe same entity.
Hence, coreference is reflexive,symmetric and transitive, and therefore an equiva-lence relation.
The task of coreference resolutionis to predict equivalence classes of mentions in adocument according to the coreference relation.In order to extract errors, we need to comparethe reference equivalence classes, given by theannotation, with the system equivalence classesobtained from system output.
The key questionnow is how we represent these equivalence classesof mentions.
Adapting common terminology, wealso refer to the equivalence classes as entities.2.3 Representing EntitiesThe most straightforward entity representation ig-nores any structure and models an entity as a setof mentions.
This representation was utilized forerror analysis by Kummerfeld and Klein (2013),who extract errors by transforming reference intosystem entities.
In this set-based representation,we can only extract whether two mentions coreferat all.
More fine-grained information, for exampleabout antecedent information, is not accessible.We therefore propose to employ a structured en-tity representation, which explicitly models linksestablished by the coreference relation betweenmentions.
This leads to a link-based error repre-sentation which formalizes the methods presentedin Uryupina (2008) and Martschat (2013).We employ for representation a complete one-directional graph.
That is, we represent an en-tity e over mentions {m1, .
.
.
,mn} as a graphe = (N,A), where N = {m1, .
.
.
,mn} andA = {(mk,mj) | k > j}.
The indices respectthe mention ordering.
Mentions earlier in the texthave a lower index.
An example graph for an en-tity over four mentions m1, .
.
.
,m4(such as theBARACK OBAMA entity) is depicted in Figure 1a.In this graph, we express all coreference relationsbetween all pairs of mentions.2Using this representation, we can represent a setof entities as a set of graphs.
In particular, given adocument we consider the set of reference entitiesR given by the annotation, and the set of systementities S, given by the system output.
In order toextract errors, we compare the graphs in R withthe graphs in S.In the following, we discuss how to compute re-call errors for a reference entity r ?
R with respectto the system entities S. For computing precisionerrors, we just switch the roles of R and S.2.4 Comparing Reference and SystemEntitiesAs we represent entities as sets of links betweenmentions, errors can be quantified as differences inthe links.
For example, if an edge (representing alink) from some reference entity r ?
R is missing2We could also use an undirected instead of a one-directional graph, but using a one-directional graph conve-niently models sequential information, which simplifies no-tation and the algorithms we will present.2071in all system entities in S, this is a recall error.In order to formalize this, we employ the notionof a partition of an entity.
Let r ?
R be some ref-erence entity, and let S be a set of system entities.The partition of r by S, written rS, is obtainedby taking all edges in r that also appear in S. rSconsists of all connected components of r (we willrefer to these as subentities) that are also in S. Alledges in r that are not in rSare candidates for re-call errors, as these were not in any entity in S.Figure 1b shows a set S of three system entities:two consist of two mentions, one of three men-tions.
In our running example, this correspondsto the system output {Obama, he} and {the pres-ident, his} plus some spurious mentions, whichare colored gray.
The graph rSfor our exampleis shown in Figure 1c.
The two edges correspondto the correctly recognized links (he, Obama) and(his, the president).
All edges in r (Figure 1a)missing from this graph are candidates for errors.2.5 Spanning TreesHowever, taking all edges in r missing in rSas er-rors leads to unintuitive results.
In the BARACKOBAMA example, this would lead to four errorsbeing extracted: (the president, Obama), (his,Obama), (the president, he) and (his, he).
But,in order to correctly predict the BARACK OBAMAentity, a coreference resolution system only needsto predict three correct links, i.e.
it has to provide aspanning tree of the entity?s graph representation.Therefore, to extract errors, we compute a span-ning tree Trof r, and take all edges in Trthat donot appear in rSas errors.
Figure 1d shows an ex-ample spanning tree for the running example en-tity r. The dashed edge, which corresponds to thelink (the president, Obama), does not appear in rSand is therefore extracted as an error.The strategies for computing a spanning treemay differ for recall and precision errors.
Hence,our extraction algorithm is parametrized by twoprocedures STrec(e, P ) and STprec(e, P ) which,given an entity e and a set of entities P , outputa spanning tree Teof e. The whole algorithm forerror extraction is summarized in Algorithm 1.3 Spanning Tree AlgorithmsIn the last section we presented a framework forlink-based error analysis, which extracts errors bycomparing entity spanning trees to entity parti-tions.
Therefore we can accommodate differentAlgorithm 1 Error Extraction from a CorpusInput: A corpus C, algorithms STrec, STprecforcomputing spanning trees.function ERRORS(C, STrec, STprec)recall errors = [ ]precision errors = [ ]for d ?
C doLet Rdbe the reference entities and Sdbe the system entities of document d.for r ?
RddoAdd all edges in STrec(r, Sd) not inrSdto recall errors.for s ?
SddoAdd all edges in STprec(s,Rd) not insRdto precision errors.Output: recall errors, precision errorsnotions of errors by varying the algorithm for com-puting spanning trees.
We now present some span-ning tree algorithms for extracting recall and pre-cision errors.3.1 Recall ErrorsWe first observe that for error extraction, the struc-ture of the spanning trees of the subentities appear-ing in rSdoes not play a role.
Edges present in rSare not candidates for errors, since they appear inboth the reference entity r and the system outputS.
Therefore, it does not matter which edges fromthe subentities are in the spanning tree.Hence, to build the spanning tree, we firstchoose arbitrary spanning trees for the subentitiesin the partition.
We choose the remaining edgesaccording to the spanning tree algorithm.Having settled on this, we only have to decidewhich edges to choose that connect the trees rep-resenting the subentities.
There are many possiblechoices for this.
For example, the graph in Fig-ure 1c has four candidate edges which connect thetrees for the subentities.We can reduce the number of candidate edgesby only considering the first mention (with respectto textual order) in a subentity as the source ofan edge to be added.
This makes sense since allother mentions in that subentity were correctly re-solved to be coreferent with some preceding men-tion.
We still have to decide on the target of theedge.
In Figure 1c, we have two choices for edges:(m3,m1) and (m3,m2).
We now present twomethods for choosing edges.2072Choosing Edges by Distance.
The moststraight-forward way to decide on an edge is totake the edge with smallest mention distancebetween source and target.
This is the approachtaken by Martschat (2013).Choosing Edges by Accessibility.
However, thedistance-based approach may lead to unintuitiveresults.
Let us consider again the BARACKOBAMA example from Figure 1.
When choosingedges by distance, we would extract the error (thepresident, he).
However, such links with a non-pronominal anaphor and a pronominal antecedentare difficult to process and considered unreliable(Ng and Cardie, 2002; Bengtson and Roth, 2008).On the other hand, the missed link (the president,Obama) constitutes a well-defined hyponymy re-lation which can be found in knowledge bases andis easily interpretable by humans.Uryupina (Uryupina, 2007; Uryupina, 2008)presents a recall error analysis where she takesthe ?intuitively easiest?
missing link to analyze(Uryupina, 2007, p. 196).
How can we formal-ize such an intuition?
We will employ a no-tion grounded in accessibility theory (Ariel, 1988).Names and nouns refer to less accessible entitiesthan pronouns do.
For such anaphors, we preferdescriptive (name/nominal) antecedents.
Inspiredby Ariel?s degrees of accessibility, we choose atarget for a given anaphor mias follows:?
If miis a pronoun, choose the closest preced-ing mention.?
If miis not a pronoun, choose the closestpreceding proper name.
If no such mentionexists, choose the closest preceding commonnoun.
If no such mention exists, choose theclosest preceding mention.Applied to the example from Figure 1, this algo-rithm extracts the error (the president, Obama).33.2 Precision ErrorsVirtually all approaches to coreference resolu-tion obtain entities by outputting pairs of anaphorand antecedent, subject to the constraint that oneanaphor has at most one antecedent.We use this information to build spanning treesfor system entities: these spanning trees con-sist of exactly the edges which correspond toanaphor/antecedent pairs in the system output.3A similar procedure was used by Ng and Cardie (2002)to extract meaningful antecedents when training a corefer-ence resolution system.4 Data and SystemsWe now discuss data and coreference resolutionsystems which we will employ for our analysis.4.1 DataWe analyze the errors of the systems on the En-glish development data of the CoNLL?12 sharedtask on multilingual coreference resolution (Prad-han et al., 2012).
This corpus contains 343 docu-ments, spanning seven genres: bible texts, broad-cast conversation, broadcast news, magazine texts,news wire, telephone conversations and web logs.4.2 SystemsState-of-the-art approaches to coreference resolu-tion encompass various paradigms, ranging fromdeterministic pairwise systems to learning-basedstructured prediction models.
Hence, we want toconduct our analysis on a representative sample ofthe state of the art, which should be publicly avail-able.
Therefore, we decided on two deterministicand two learning-based systems:?
StanfordSieve4(Lee et al., 2013) was thewinning system of the CoNLL?11 sharedtask.
It employs a multi-sieve approach bymaking more confident decisions first.?
Multigraph5(Martschat, 2013) is a deter-ministic pairwise system which is based onMartschat et al.
(2012), the second-rankingsystem in the English track of the CoNLL?12shared task.
It uses a subset of features ashard constraints and chooses an antecedentfor a mention by summing up the remainingboolean features.?
IMSCoref6(Bj?orkelund and Farkas, 2012)ranked second overall in the CoNLL?12shared task (third for English).
It stacks mul-tiple decoders and relies on a combination ofstandard pairwise and lexicalized features.?
BerkeleyCoref7(Durrett and Klein, 2013) isa state-of-the-art system that uses mainly lex-icalized features and a latent antecedent rank-ing architecture.
It outperforms Stanford-Sieve and IMSCoref on the CoNLL?11 data.4Part of Stanford CoreNLP, available at http://nlp.stanford.edu/software/corenlp.shtml.
Weuse version 3.4.5http://smartschat.de/software6http://www.ims.uni-stuttgart.de/forschung/ressourcen/werkzeuge/IMSCoref.en.html .
We use the CoNLL 2012 system.7http://nlp.cs.berkeley.edu/berkeleycoref.shtml2073System MUC B3CEAFeAverageFernandes et al.
69.46 57.83 54.43 60.57Martschat 66.22 55.47 51.90 57.86StanfordSieve 64.96 54.49 51.24 56.90Multigraph 69.13 58.61 56.06 61.28IMSCoref 67.15 55.19 50.94 57.76BerkeleyCoref 70.27 59.29 56.11 61.89Table 1: Comparison of the systems with Fernan-des et al.
(2012) and with Martschat (2013) onCoNLL?12 English development data.For Multigraph, we modified the system describedin Martschat (2013) slightly to allow for the in-corporation of distance (similar to Cai and Strube(2010)).
Inspired by Lappin and Leass (1994), weadd salience weights for subjects and objects tothe model to improve third-person pronoun reso-lution.
We also extended the feature set by a sub-string feature.
Furthermore, motivated by Chenand Ng (2012), we added a lexicalized feature fornon-pronominal mentions that were coreferent inat least 50% of the cases in the training data.StanfordSieve was run with its standard CoNLLshared task settings.
The learning-based sys-tems were trained on the CoNLL?12 training data.We trained IMSCoref with its standard settings,and trained BerkeleyCoref with the final featureset from Durrett and Klein (2013) for twenty it-erations.
We evaluate the systems on EnglishCoNLL?12 development data and compare it withthe winning system of the CoNLL?12 shared task(Fernandes et al., 2012) and with Martschat (2013)in Table 1, using the reference implementation v7of the CoNLL scorer (Pradhan et al., 2014).BerkeleyCoref performs best according to allmetrics, followed by Multigraph.
StanfordSieveis the worst performing system: the gap to Berke-leyCoref is five points in average score.4.3 DiscussionAlthough we analyze recent systems on a recentlypublished coreference data set, we believe that theresults of our analysis will have implications forcoreference in general.
The data set is the largestand most genre-diverse coreference corpus so far.The systems we investigate represent major di-rections in coreference resolution model research,and make use of large and diverse feature sets pro-posed in the literature (Ng, 2010).5 A Comparative AnalysisThe coreference resolution systems presented inthe previous section are a representative sample ofthe state of the art.
Therefore, by analyzing theerrors they make, we can learn about remainingchallenges in coreference resolution and analyzethe qualitative differences between the systems.The results of such an analysis will deepen ourunderstanding of coreference resolution and willsuggest promising directions for further research.5.1 Experimental SettingsPrevious studies identified the presence of recallerrors as a main bottleneck for improving per-formance (Raghunathan et al., 2010; Durrett andKlein, 2013; Kummerfeld and Klein, 2013).
Thisis also evidenced by the CoNLL shared tasks oncoreference resolution (Pradhan et al., 2011; Prad-han et al., 2012), where most competitive systemshad higher precision than recall.
This indicatesthat an analysis of recall errors helps to understandand improve the state of the art.
Hence, we focuson analyzing recall errors, and complement this bya brief analysis of precision errors.We analyze errors of the four systems presentedin the previous section on the CoNLL?12 Englishdevelopment data.
To extract recall errors we em-ploy the spanning tree algorithm which choosesedges by accessibility.
We obtain precision errorsfrom the pairwise output of the systems.5.2 A Recall Error Analysis of StanfordSieveSince StanfordSieve is currently the most-widelyused coreference resolution system, it serves as agood starting point for our analysis.
Rememberthat we represent each error as a pair of anaphorand antecedent.
For an initial analysis, we cate-gorize each error by mention type, distinguishingbetween proper name, common noun, pronoun,demonstrative pronoun and verb.8StanfordSieve makes 5245 recall errors.
To putthis number into context, we compare it with themaximum number of recall errors a system canmake.
This count is obtained by extracting recallerrors from the output of a system that puts eachmention in its own entity, which yields 14609 er-rors.
In Table 2 we present a detailed analysis.For each pair of mention type of anaphor and an-8We obtain the type from the part-of-speech tag of themention?s head.
Furthermore, we treat every mention whosehead has a NER label in the data as a proper name.2074Name Noun Pron.
Dem.
VerbNameErrors 1006 181 43 0 0Maximum 3578 206 56 2 0NounErrors 517 1127 46 14 91Maximum 742 2063 51 14 91Pron.Errors 483 761 543 45 53Maximum 1166 1535 4596 92 53Dem.Errors 23 86 41 31 117Maximum 27 93 43 46 117VerbErrors 1 20 2 4 10Maximum 1 20 2 4 11Table 2: Number of StanfordSieve?s recall er-rors according to mention type, compared to themaximum possible number of errors.
Rows areanaphors, columns antecedents.tecedent, the table displays the number of recallerrors and of maximum errors possible.StanfordSieve gets almost none of the links in-volving verbal or demonstrative mentions correct.This is due to the system not attempting to handleevent coreference, and performing very poorly fordemonstratives.
On the other hand, recall for pro-noun resolution is quite good, at least when con-sidering non-verbal antecedents.
While Stanford-Sieve makes 1885 recall errors when the anaphoris a pronoun, it successfully resolves most of suchlinks present in the corpus.
Finally, let us considerthe links involving only proper names and com-mon nouns.
In total, these amount to 6589 linksin the corpus (around 45% of all links).
Stanford-Sieve misses 2831 of these links.
Pairs of propernames seem to be easier to resolve than pairs ofcommon nouns.
Links between a common nounand a proper name are less frequent, but muchmore difficult: most of the links are missing.5.3 Analysis of the Other SystemsIn the previous section we identified various char-acteristics of the errors made by StanfordSieve:only (comparatively) few errors are made for pro-noun resolution and name coreference, while othertypes of nominal anaphora and coreference ofdemonstrative/verbal mentions pose a challengefor the system.
Do the other systems in our studyalso have these characteristics?
In order to answerProportionSystem Total Anaphor Pron.
Name/NounStanfordSieve 5245 36% 54%Multigraph 4630 32% 56%IMSCoref 5220 32% 58%BerkeleyCoref 4635 32% 56%Table 3: Recall error numbers for all systems.this question, we repeated the analysis for the threeother systems described in Section 4.
We summa-rize the results in Table 3.
We only report num-bers for pronoun resolution and name/noun coref-erence, as all systems do not resolve verbal men-tions and perform poorly for demonstratives.StanfordSieve makes the most recall errors,closely followed by IMSCoref.
Multigraphand BerkeleyCoref make around 600 errors less.While the total number of errors differs betweenthe systems, the distributions are similar.
In par-ticular, around 55% of recall errors made involveonly proper names and common nouns.
The num-ber is a bit higher for IMSCoref.
We concludethat, despite variations in performance, both de-terministic and learning-based state-of-the-art sys-tems have similar weaknesses regarding recall.The results displayed in Table 3 suggest vari-ous opportunities for future research.
In this pa-per, we will focus on analyzing name/noun recallerrors, as these constitute a large fraction of all re-call errors.
Future work should address the pro-noun resolution errors and a characterization of theverbal/demonstrative errors.5.4 Analysis of the Name/Noun Recall ErrorsWe now turn towards a fine-grained analysis of thename/noun recall errors.Table 4 displays the number of such recall errorsmade by each system, according to the mentiontypes of anaphor and antecedent.
We are interestedin errors common to all systems, and in qualitativedifferences of errors between the systems.5.4.1 Common ErrorsLet us first analyze the errors common to all sys-tems.
Our analysis is driven by the question howthese can be characterized, and which knowledgeis missing to resolve such links.
We discuss theerrors depending on the mention types of anaphorand antecedent.
The lower part of Table 4 displaysthe number of common errors for each category.2075Number of Recall Errors (Anaphor-Antecedent)Description Name-Name Noun-Name Name-Noun Noun-NounStanfordSieve 1006 517 181 1127Multigraph 753 501 189 1152IMSCoref 1082 500 188 1264BerkeleyCoref 910 456 171 1072Common errors 475 371 147 835Correct boundaries idenfified 257 273 108 563excl.
IMSCoref 156 222 97 475Table 4: Name/Noun recall errors for all systems.Common AllType % Type %ORG 25% PERSON 26%PERSON 19% GPE 26%GPE 16% ORG 20%DATE 14% NONE 14%NONE 9% DATE 6%Table 5: Distribution of top five named entitytypes of common name-name recall errors and allpossible name-name recall errors.Furthermore, in order to assess the impact ofmention detection, the table shows the number ofcommon errors where boundaries for both men-tions were identified correctly by some system.We can see that boundary identification is a diffi-cult problem, especially for proper name pairs: for48% of such errors, no system found the correctboundaries of both mentions participating in theerror.
The number of errors where correct bound-aries could be found drops significantly after ex-cluding IMSCoref.
This is due to the mention ex-traction strategy of IMSCoref: the other systemsin our study discard the shorter mention when twomentions have the same head, IMSCoref keepsboth mentions.
Hence, the system is able to cor-rectly identify some mentions even in the presenceof parsing or preprocessing errors.
However, asa result, IMSCoref has to process many spuriousmentions, which makes learning more difficult.We conclude that mention detection still consti-tutes a challenge.
We now proceed to a detailedanalysis of errors common to all systems.
In pass-ing we will discuss difficulties in mention detec-tion with regard to specific error types.Errors between Pairs of Proper Names.
Thesystems share 475 recall errors between pairs ofproper names.
In Table 5, we compare the distri-bution of gold named entity types of these errorswith the distribution of gold named entity types ofall possible errors (obtained via a singleton sys-tem).
We see that especially difficult classes oflinks are pairs with type ORG or DATE.Let us now consider lexical features of the er-rors.9In 154 errors, the strings match completely,but the correct resolution was mostly prevented byannotation inconsistencies (e.g.
China instead ofChina?s) or propagated parsing and NER errors,which lead to deficiencies in mention extraction.For 217 errors, at least one token appears in bothmention strings, as in the ?Cole?
and the ?USSCole?.
This shows the insufficiency of the featureswhich hint to alias relations, may it be heuristics orlearned lexical similarities (for 109 of the 217 er-rors, both mention boundaries were identified cor-rectly by at least one system).
Disambiguationwith respect to knowledge bases could provide aprincipled way to identify name variations.We classified the remaining 104 errors manu-ally, see Table 6.
For a couple of categories suchas identifying acronyms, spelling variations andaliases, disambiguation could also help.
Many er-rors happen for date mentions, which suggests theuse of temporal tagging features.Errors for Noun-Name Pairs.
We now inves-tigate the errors where the anaphor is a commonnoun and the antecedent is a proper name.
371 er-rors are common to all systems.
The high fractionof common errors shows that this is an especiallychallenging category.
We again start by investigat-ing how the distribution of the named entity type9When computing these, we ignored case and ignored alltokens with part-of-speech tag DT or POS.2076Description Occ.
ExampleAcronyms 20 National Ice Hockey League andNHLAlias 24 Florida and the Sunshine StateAnnotation 2 Annotation errors (pronoun asname)Context 2 Paula Coccoz and juror number tenDate 29 1989 and last year?sMetonymy 12 South Afria and PretoriaRoles 8 Al Gore and the Vice PresidentSpelling 7 Hsiachuotzu and HsiachuotsuTable 6: Classification of common name-name re-call errors without common tokens.Common AllType % Type %ORG 28% ORG 27%PERSON 22% GPE 22%GPE 19% PERSON 18%NONE 7% NONE 11%DATE 5% DATE 5%Table 7: Distribution of top five named entitytypes of common noun-name recall errors and allpossible noun-name recall errors.of the antecedent differs when we compare com-mon errors to all possible errors.
The results areshown in Table 7.
Links with a proper name an-tecedent of type PERSON are especially difficult.They constitute 22% of the common errors, butonly 18% of all possible errors.Most mentions are in a hyponymy relation, likethe prime minister and Mr. Papandreou.
This con-firms that harnessing such relations could improvecoreference resolution (Rahman and Ng, 2011;Uryupina et al., 2011).
For 65 of the errors (18%)there is lexical overlap: the head of the anaphor iscontained in the proper name antecedent, as in theentire park and the Ocean Park.When categorizing all common errors accord-ing to the head of the anaphor, we observe 204 dif-ferent heads.
142 heads appear only once, but thetop ten heads make up 88 of the 371 errors.
Themost frequent heads are company (15), group (12),government, country and nation (each 9).
Thissuggests that even with few reliable hyponymy re-lations recall could be significantly improved.We observe similar trends when the anaphor isa proper name and the antecedent is a noun.Reference SystemSystem Stanford MG IMS BerkeleyStanford - 51 47 61MG 17 - 42 60IMS 26 54 - 54Berkeley 12 42 25 -Table 8: Comparison of noun-name recall errors.Entries are errors made by the system in the row,while the participating mentions are coreferent ac-cording to the the system in the column.Errors between Pairs of Common Nouns.
835errors between pairs of common nouns are sharedby all systems.
For 174 of these, the anaphor isan indefinite noun phase, which makes resolutiona lot harder, since most coreference resolution sys-tems classify these as non-anaphoric and thereforedo not attempt resolution.For further analysis, we split all 835 errors intwo categories, distinguishing whether the headmatches between the mentions or not.
In 341 casesthe heads match.
For many of these cases, parsingerrors propagate and prevent the systems from rec-ognizing the correct mention boundaries.In order to get a better understanding of the er-rors for nouns with different heads, we randomlyextracted 50 of the 494 pairs and investigated therelation that holds between the heads.
In 23 cases,the heads were related via hyponymy.
In 10 casesthey were synonyms.
The remaining 17 casesinvolve many different phenomena, for examplemeronymy.
This confirms findings from previousresearch (Vieira and Poesio, 2000).Hence, looking up lexical relations, especiallyhyponymy, might be helpful to solve these cases.5.4.2 Differences between the SystemsIn order to analyze differences between the sys-tems, we compare the recall errors they make.The information how recall errors differ betweensystems will enable us to understand individualstrengths and weaknesses.Exemplarily, we will have a look at the differ-ences in the errors when the anaphor is a commonnoun and the antecedent is a proper name.
By sys-tem design and by the total error numbers (Table4) we expect the learning-based systems to have aslight advantage over the deterministic systems.In Table 8 we compare noun-name recall errorsmade by each system.
Entries are errors made by2077Number and Proportion of Precision Errors (Anaphor-Antecedent)Description Name-Name Noun-Name Name-Noun Noun-NounStanfordSieve 1038 31% 64 59% 65 72% 944 48%Multigraph 1131 30% 76 51% 24 56% 743 42%IMSCoref 834 26% 74 59% 46 64% 1050 54%BerkeleyCoref 810 24% 191 67% 60 62% 1015 48%Common errors 158 1 2 167Table 9: Name/Noun precision errors for all systems.
The percentages are the proportion of precisionerrors with respect to all decision of the system in that category.the system in the row, while the participating men-tions are coreferent according to the the system inthe column.
The numbers confirm our hypothesis,but also show that the deterministic systems areable to recover a few links missed by the learning-based systems.For example, BerkeleyCoref recovers 60 linksthat could not be found by Multigraph, including34 links without any common token, such as theairline and Pan Am.
Multigraph recovers only 42links not found by BerkeleyCoref, 21 without anycommon token.
Qualitatively, StanfordSieve andMultigraph are able to resolve a few links thanksto their engineered substring match, such as thejudge and Dallas District Judge Jack Hampton.We also conducted similar investigations forcommon noun and proper name pairs.
For com-mon nouns, the trends are similar: the learning-based systems have an advantage over the deter-ministic systems.
However, only few relations be-tween nouns with different heads are learned ?compared to StanfordSieve, BerkeleyCoref recov-ers only 11 such pairs, such as the man and anexpert in the law.
Recall of the deterministic sys-tems is further hampered by their strict checks formodifier agreement, which they employ to keepprecision high.
Both systems miss for example thelink from the anaphor the Milosevic regime to theregime, since the nominal modifier of the anaphordoes not appear in the antecedent.For proper names, Multigraph employs so-phisticated alias heuristics which help to resolvematches such as Marshall Ye Ting?s and his grand-father Ye Ting.
This explains the correspondinglow number in Table 4.
The lexicalized featuresof Multigraph, IMSCoref and BerkeleyCoref helpto learn aliases when there is no string match, es-pecially for the bible part of the corpus (resolvinglinks such as Jesus and the Son of Man).5.5 Precision ErrorsIn the above analysis we identified commonname/noun recall errors and discussed strengthsand weaknesses of each system.
Let us comple-ment this analysis by a brief discussion of corre-sponding precision errors.Table 9 gives an overview.
It displays the num-ber of precision errors for each category, and theproportion of these errors compared to all deci-sions in that category.
We can see some generaltrends from this table: first, more decisions lead toa higher proportion of errors.
This shows the dif-ficulty of balancing recall and precision.
Second,proper name coreference seems much easier thancommon noun coreference.
Coreference involvingdifferent mention types is a lot harder ?
the sys-tems only attempt few decisions, most of them arewrong.
This confirms findings from our recall er-ror analysis.
Third, the fraction of common errorsis very low, which indicates that precisions errorsstem from various sources, which are handled dif-ferently by each system.6 Related WorkWe now discuss related work in coreference res-olution error analysis and in the related field ofcoreference resolution evaluation metrics.Error Analysis.
While many papers on coref-erence resolution briefly discuss errors made andresolved by the system under consideration, onlyfew concentrate on error analysis.
Uryupina(2008) presents a manual error analysis on thesmall MUC-7 test set; Martschat (2013) performsan automatic coarse-grained error classification onCoNLL data.
By extending and formalizing theapproach of Martschat (2013), we are able to per-form a large-scale investigation of recall errorsmade by state-of-the-art systems.2078Kummerfeld and Klein (2013) devise a methodto extract errors from transformations of referenceto system entities.
They apply this method to avariety of systems and aggregate errors over thesesystems.
By aggregating, they are not able to ana-lyze differences.
They furthermore focus on de-scribing many different error classes, instead ofclosely investigating particular phenomena.Evaluation Metrics.
We extract recall and pre-cision errors.
How does our error analysis frame-work relate to coreference resolution evaluationmetrics, which quantify recall and precision er-rors?
We first observe a fundamental difference:evaluation metrics deal with scoring coreferencechains, they provide no means of extracting recallor precision errors.
Therefore our analysis com-plements insights obtained via evaluation metrics.We follow Chen and Ng (2013) and distinguishbetween linguistically agnostic metrics, which donot employ linguistic information during scoring,and linguistically informed metrics, which employlinguistic information similar as we do when com-puting spanning trees.We limit the discussion of linguistically ag-nostic metrics to the three most popular evalua-tion metrics whose average constitutes the officialscore in the CoNLL shared tasks on coreferenceresolution: MUC (Vilain et al., 1995), B3(Baggaand Baldwin, 1998) and CEAFe(Luo, 2005).10Our framework bears most similarities to theMUC metric, as both are based on the same link-based entity representation.
In particular, when wedivide the number of errors extracted from an en-tity by the size of a spanning tree for that entity, weobtain a score linearly related11to the MUC scorefor that entity (recall for reference entities, preci-sion for system entities).
B3and CEAFeare notfounded on a link-based structure.
B3computesrecall by computing the relative overlap of refer-ence and system entity for each reference mention,and then normalizes by the number of mentions.CEAFecomputes an optimal entity alignment withrespect to the relative overlap, and then normalizesby the number of entities.
As the metrics are notlink-based, they do not provide means to extractlink-based errors.
We leave determining whetherthe framework of these metrics exhibits a usefulnotion of errors to future work.10These are linguistically agnostic since they do not differbetween different mention or entity types when evaluating.11via the transformation x 7?
1?
xRecent work considered devising evaluationmetrics which take linguistic information intoaccount.
Chen and Ng (2013) inject linguis-tic knowledge into existing evaluation metrics byweighting links in an entity representation graph.Tuggener (2014) devises scoring algorithms tai-lored for particular applications by redefining thenotion of a correct link.
While both of these worksfocus on scoring, they weight or explicitly definelinks in the reference and system entities, therebythey in principle allow error extraction.
However,the authors do not attempt this and it is not clearwhether the errors extracted that way are useful foranalysis and system development.7 ConclusionsWe presented a novel link-based framework forcoreference resolution error analysis, which ex-tends and complements previous work.
We ap-plied the framework to analyze recall errors of fourstate-of-the-art systems on a large English bench-mark dataset.
Concentrating on errors involvingonly proper names and common nouns, we identi-fied a core set of challenging errors common to allsystems in our study.We characterized the common errors among abroad range of properties.
In particular, our anal-ysis highlights and quantifies the usefulness ofworld knowledge.
Furthermore, by comparing therecall errors made by each system, we identifiedindividual strengths and weaknesses.
A brief pre-cision error analysis highlighted the hardness ofresolving noun-name and noun-noun links.The presented method and findings help to iden-tify challenges in coreference resolution and to in-vestigate ways to overcome these challenges.AcknowledgmentsThis work has been funded by the Klaus TschiraFoundation, Heidelberg, Germany.
The first au-thor has been supported by a HITS Ph.D. scholar-ship.ReferencesMira Ariel.
1988.
Referring and accessibility.
Journalof Linguistics, 24(1):65?87.Amit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In Proceedingsof the 1st International Conference on LanguageResources and Evaluation, Granada, Spain, 28?30May 1998, pages 563?566.2079Mohit Bansal and Dan Klein.
2012.
Coreference se-mantics from web features.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), JejuIsland, Korea, 8?14 July 2012, pages 389?398.Eric Bengtson and Dan Roth.
2008.
Understandingthe value of features for coreference resolution.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, Waikiki,Honolulu, Hawaii, 25?27 October 2008, pages 294?303.Anders Bj?orkelund and Rich?ard Farkas.
2012.
Data-driven multilingual coreference resolution using re-solver stacking.
In Proceedings of the Shared Taskof the 16th Conference on Computational NaturalLanguage Learning, Jeju Island, Korea, 12?14 July2012, pages 49?55.Jie Cai and Michael Strube.
2010.
End-to-end coref-erence resolution via hypergraph partitioning.
InProceedings of the 23rd International Conferenceon Computational Linguistics, Beijing, China, 23?27 August 2010, pages 143?151.Chen Chen and Vincent Ng.
2012.
Combining thebest of two worlds: A hybrid approach to multilin-gual coreference resolution.
In Proceedings of theShared Task of the 16th Conference on Computa-tional Natural Language Learning, Jeju Island, Ko-rea, 12?14 July 2012, pages 56?63.Chen Chen and Vincent Ng.
2013.
Linguisticallyaware coreference evaluation metrics.
In Proceed-ings of the 6th International Joint Conference onNatural Language Processing, Nagoya, Japan, 14?18 October 2013, pages 1366?1374.Greg Durrett and Dan Klein.
2013.
Easy victoriesand uphill battles in coreference resolution.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, Seattle, Wash.,18?21 October 2013, pages 1971?1982.Eraldo Fernandes, C?
?cero dos Santos, and Ruy Milidi?u.2012.
Latent structure perceptron with feature in-duction for unrestricted coreference resolution.
InProceedings of the Shared Task of the 16th Confer-ence on Computational Natural Language Learning,Jeju Island, Korea, 12?14 July 2012, pages 41?48.Jonathan K. Kummerfeld and Dan Klein.
2013.
Error-driven analysis of challenges in coreference reso-lution.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, Seattle, Wash., 18?21 October 2013, pages 265?277.Shalom Lappin and Herbert J. Leass.
1994.
An algo-rithm for pronominal anaphora resolution.
Compu-tational Linguistics, 20(4):535?561.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolu-tion based on entity-centric, precision-ranked rules.Computational Linguistics, 39(4):885?916.Xiaoqiang Luo.
2005.
On coreference resolutionperformance metrics.
In Proceedings of the Hu-man Language Technology Conference and the 2005Conference on Empirical Methods in Natural Lan-guage Processing, Vancouver, B.C., Canada, 6?8October 2005, pages 25?32.Sebastian Martschat, Jie Cai, Samuel Broscheit,?EvaM?ujdricza-Maydt, and Michael Strube.
2012.
Amultigraph model for coreference resolution.
InProceedings of the Shared Task of the 16th Confer-ence on Computational Natural Language Learning,Jeju Island, Korea, 12?14 July 2012, pages 100?106.Sebastian Martschat.
2013.
Multigraph clustering forunsupervised coreference resolution.
In 51st AnnualMeeting of the Association for Computational Lin-guistics: Proceedings of the Student Research Work-shop, Sofia, Bulgaria, 5?7 August 2013, pages 81?88.Alexis Mitchell, Stephanie Strassel, Shudong Huang,and Ramez Zakhary.
2004.
ACE 2004 multilingualtraining corpus.
LDC2005T09, Philadelphia, Penn.
:Linguistic Data Consortium.Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics, Philadel-phia, Penn., 7?12 July 2002, pages 104?111.Vincent Ng.
2010.
Supervised noun phrase corefer-ence research: The first fifteen years.
In Proceed-ings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, Uppsala, Swe-den, 11?16 July 2010, pages 1396?1411.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Proceed-ings of the Human Language Technology Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, New York, N.Y.,4?9 June 2006, pages 192?199.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and NianwenXue.
2011.
CoNLL-2011 Shared Task: Modelingunrestricted coreference in OntoNotes.
In Proceed-ings of the Shared Task of the 15th Conference onComputational Natural Language Learning, Port-land, Oreg., 23?24 June 2011, pages 1?27.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 Shared Task: Modeling multilingual unre-stricted coreference in OntoNotes.
In Proceedings2080of the Shared Task of the 16th Conference on Com-putational Natural Language Learning, Jeju Island,Korea, 12?14 July 2012, pages 1?40.Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-uard Hovy, Vincent Ng, and Michael Strube.
2014.Scoring coreference partitions of predicted men-tions: A reference implementation.
In Proceed-ings of the 52nd Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), Baltimore, Md., 22?27 June 2014, pages 30?35.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, Cambridge, Mass.,9?11 October 2010, pages 492?501.Altaf Rahman and Vincent Ng.
2011.
Coreference res-olution with world knowledge.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers),Portland, Oreg., 19?24 June 2011, pages 814?824.Lev Ratinov and Dan Roth.
2012.
Learning-basedmulti-sieve co-reference resolution with knowledge.In Proceedings of the 2012 Conference on EmpiricalMethods in Natural Language Processing and Nat-ural Language Learning, Jeju Island, Korea, 12?14July 2012, pages 1234?1244.Don Tuggener.
2014.
Coreference resolution evalua-tion for higher level applications.
In Proceedings ofthe 14th Conference of the European Chapter of theAssociation for Computational Linguistics, Gothen-burg, Sweden, 26?30 April 2014, pages 231?235.Olga Uryupina, Massimo Poesio, Claudio Giuliano,and Kateryna Tymoshenko.
2011.
Disambigua-tion and filtering methods in using web knowledgefor coreference resolution.
In Proceedings of the24th International Florida Artificial Intelligence Re-search Society Conference, Palm Beach, USA, 18?20 May 2011, pages 317?322.Olga Uryupina.
2007.
Knowledge acquisition forcoreference resolution.
Ph.D. thesis, Saarland Uni-versity, Saarbr?ucken, Germany.Olga Uryupina.
2008.
Error analysis for learning-based coreference resolution.
In Proceedings ofthe 6th International Conference on Language Re-sources and Evaluation, Marrakech, Morocco, 26May ?
1 June 2008, pages 1914?1919.Renata Vieira and Massimo Poesio.
2000.
Anempirically-based system for processing definite de-scriptions.
Computational Linguistics, 26(4):539?593.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of the 6th Message Understanding Conference(MUC-6), pages 45?52, San Mateo, Cal.
MorganKaufmann.Ralph Weischedel, Martha Palmer, Mitchell Marcus,Eduard Hovy, Sameer Pradhan, Lance Ramshaw,Nianwen Xue, Ann Taylor, Jeff Kaufman, MichelleFranchini, Mohammed El-Bachouti, Robert Belvin,and Ann Houston.
2013.
OntoNotes release 5.0.LDC2013T19, Philadelphia, Penn.
: Linguistic DataConsortium.2081
