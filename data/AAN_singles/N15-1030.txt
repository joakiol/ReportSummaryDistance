Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 263?271,Denver, Colorado, May 31 ?
June 5, 2015. c?2015 Association for Computational LinguisticsEmpty Category Detection With Joint Context-Label EmbeddingsXun Wang, Katsuhito Sudoh and Masaaki NagataNTT Communication Science LaboratoriesKyoto 619-0237, Japanwang.xun,sudoh.katsuhito,nagata.masaaki@lab.ntt.co.jpAbstractThis paper presents a novel technique forempty category (EC) detection using dis-tributed word representations.
A jointmodel is learned from the labeled data tomap both the distributed representationsof the contexts of ECs and EC types toa low dimensional space.
In the testingphase, the context of possible EC positionswill be projected into the same space forempty category detection.
Experiments onChinese Treebank prove the effectivenessof the proposed method.
We improve theprecision by about 6 points on a subset ofChinese Treebank, which is a new state-of-the-art performance on CTB.1 IntroductionThe empty category (EC) is an important con-cept in linguistic theories.
It is used to de-scribe nominal words that do not have ex-plicit phonological forms (they are also called?covert nouns?).
This kind of grammatical phe-nomenons is usually caused by the omission ordislocation of nouns or pronouns.
Empty cat-egories are the ?hidden?
parts of text and areessential for syntactic parsing (Gabbard et al,2006; Yang and Xue, 2010).
As a basic prob-lem in NLP, the resolution of ECs also has ahuge impact on lots of downstream tasks, suchas co-reference resolution (Ponzetto and Strube,2006; Kong and Ng, 2013), long distance de-pendency relation analysis (Marcus et al, 1993;Xue et al, 2005).
Research also uncovers theimportant role of ECs in machine translation.Some recent work (Chung and Gildea, 2010; Xi-ang et al, 2013) demonstrates the improvementsthey manage to obtain through EC detection inChinese-English translation.To resolve ECs, we need to decide 1) the po-sition and type of the EC and 2) the contentof the EC (to which element the EC is linkedto if plausible).
Existing research mainly fo-cuses on the first problem which is referred toas EC detection (Cai et al, 2011; Yang andXue, 2010), and so is this paper.
As ECs arewords or phrases inferable from their context,previous work mainly designs features miningthe contexts of ECs and then trains classifica-tion models or parsers using these features (Xueand Yang, 2013; Johnson, 2002; Gabbard et al,2006; Kong and Zhou, 2010).
One problem withthese human-developed features are that theyare not fully capable of representing the seman-tics and syntax of contexts.
Besides, the featureengineering is also time consuming and labor in-tensive.Recently neural network models have proventheir superiority in capturing features using lowdense vector compared with traditional manu-ally designed features in dozens of NLP tasks(Bengio et al, 2006; Collobert and Weston,2008; Socher et al, 2010; Collobert et al, 2011;Li and Hovy, 2014; Li et al, 2014).This paper demonstrates the advantages ofdistributed representations and neural networksin predicting the locations and types of ECs.We formulate the EC detection as an annotation263task, to assign predefined labels (EC types) togiven contexts.
Recently, Weston et al (2011)proposed a system taking advantages of the hid-den representations of neural networks for imageannotation which is to annotate images with aset of textual words.
Following the work, we de-sign a novel method for EC detection.
We rep-resent possible EC positions using the word em-beddings of their contexts and then map themto a low dimension space for EC detection.Experiments on Chinese Treebank show thatthe proposed model obtains significant improve-ments over the previous state-of-the-art meth-ods based on strict evaluation metrics.
We alsoidentify the dependency relations between ECsand their heads, which is not reported in pre-vious work.
The dependency relations can helpus with the resolution of ECs and benefit othertasks, such as full parsing and machine transla-tion in practice.2 Proposed MethodWe represent each EC as a vector by concate-nating the word embeddings of its contexts.
Asis shown in Fig.
1, we learn a map MAPAfromthe annotated data, to project the ECs?
featurevectors to a low dimension space K. Meanwhile,we also obtain the distributed representations ofEC types in the same low dimension space K. Inthe testing phase, for each possible EC position,we use MAPAto project its context feature tothe same space and further compare it with therepresentations of EC types for EC detection.Figure 1: System ArchitectureDistributed representations are good at cap-turing the semantics and syntax of contexts.
Forexample, with word embeddings we are able totell that ??/eat?
and ??/drink?
have a closerrelationship than ??/eat?
and ??/walk?
or??/drink?
and ??/walk?.
Thus the knowledgewe learn from: ?EC(?/You)-?/have-EC(?
?/supper)-?/past tense marker-?/questionmarker?
could help us to detect ECs in sentencessuch as ?EC(?/You)-?
?/beverage-?/drink-?/past tense marker-?/question marker?,which are similar, though different from theoriginal sentence.Below is a list of EC types contained in theChinese Treebank, which are also the types ofEC we are to identity in this work.?
pro: small pro, refer to dropped pronouns.?
PRO: big PRO, refer to shared elementsin control structures or elements that havegeneric references.?
OP: null operator, refer to empty relativepronouns.?
T: trace left by A?-movement, e.g., topical-ization, relativization.?
RNR: used in right nodes rising.?
*: trace left by passivization, raising.?
Others: other ECs.According to the reason that one EC iscaused, we are able to assign it one of the abovecategories.We can formulate EC detection as a combi-nation of a two-class classification problem (isthere an EC or not) and a seven-class classifi-cation problem (what type the EC is if there isone) following the two-pass method.
For one-pass method, EC detection can be formulatedas an eight-class (seven EC types listed aboveplus a dummy ?No?
type) classification prob-lem.
Previous research shows there is no sig-nificant differences between their performances(Xue and Yang, 2013).
Here we adopt the one-pass method for simplicity.2642.1 System OverviewThe proposed system consists of two maps.MAPAis from the feature vector of an ECposition to a low dimensional space.MAPA: Rn?
Rk, k ?
nfA(X) ?
WAX(1)MAPAis a linear transformation, and WAis ak ?
n matrix.The other one is from labels to the same lowdimensional space.MAPB: {Label1, Label2, ...} ?
R ?
RkfB(Labeli) ?
WiB(2)MAPBis also a linear transformation.
W iBis a k dimensional vector and it is also the dis-tributed representation of Labeliin the low di-mensional space.The two maps are learned from the trainingdata simultaneously.
In the testing phase, forany possible EC position to be classified, we ex-tract the corresponding feature vector X, andthen map it to the low dimensional space usingfA(X) = WAX.
Then we have gi(X) for eachLabelias follows:gi(X) = (fA(X))TWiB(3)For each possible label Labeli, gi(X) is the scorethat the example having a Labeliand the labelpredicted for the example is the i that maximizesgi(X).Following the method of Weston et al (2011),we try to minimize a weighted pairwise loss,learned using stochastic gradient descent:?X?i?=cL(rankc(X))max(0, (gi(X)?
gc(X)))(4)Here c is the correct label for example X, andrankc(X) is the rank of Label c among all pos-sible labels for X. L is a function which reflectsour attitude towards errors.
A constant func-tion L = C implies we aim to optimize the fullranking list.
Here we adopt L(?)
= ?
?i=11/i,which aims to optimize the top 1 in the rank-ing list, as stated in (Usunier et al, 2009).
Thelearning rate and some other parameters of thestochastic gradient descent algorithm are to beoptimized using the development set.An alternative method is to train a neuralnetwork model for multi-class classification di-rectly.
It is plausible when the number of classesis not large.
One of the advantages of represent-ing ECs and labels in a hidden space is that ECdetection usually serves as an intermediate task.Usually we want to know more about the ECssuch as their roles and explicit content.
Rep-resenting labels and ECs as dense vectors willgreatly benefit other work such as EC resolutionor full parsing.
Besides, such a joint embeddingframework can scale up to the large set of la-bels as is shown in the image annotation task(Weston et al, 2011), which makes the identifi-cation of dependency types of ECs (which is alarge set) possible.2.2 Context Features Construction2.2.1 Defining LocationsIn a piece of text, possible EC positions can bedescribed with references to tokens, e.g., beforethe nth token (Yang and Xue, 2010).
One prob-lem with such methods is that if there are morethan one ECs preceding the nth token, they willoccupy the same position and can not be distin-guished.
One solution is to decide the numberof ECs for each position, which complicates theproblem.
But if we do nothing, some ECs willbe ignored.A compromised solution is to describe posi-tions using parse trees (Xue and Yang, 2013).Adjacent ECs before a certain token usuallyhave different head words, which means they areattached to different nodes (head words) in aparse tree.
Therefore it is possible to define po-sitions using ?head word, following word?
pairs.Thus the problem of EC detection can be formu-lated as a classification problem: for each ?headword, following word?
pair, what is the type ofthe EC?
An example is shown in figure 2, inwhich there are 2 possible EC positions, (?,?
)and (?, ?
)1.1Note that there are still problems with the tree basedmethod.
As is shown in Fig.
3, the pro and T are at-tached to the same head word (??)
and share the same265ROOT?
?Position-2?Position-1Figure 2: Possible EC Positions in a DependencyTreeBesides, we keep punctuations in the parsetree so that we can describe all the possible po-sitions using the ?head node, following word?pairs, as no elements will appear after a full stopin a sentence.2.2.2 Feature ExtractionThe feature vector is constructed by concate-nating the word embeddings of context wordsthat are expected to contribute to the detectionof ECs.1.
The head word (except the dummy rootnode).
Suppose words are represented us-ing d dimension vectors, we need d elementsto represent this feature.
The distributedrepresentations of the head word would beplaced at the corresponding positions.2.
The following word in the text.
This featureis extracted using the same method withhead words.3.
?Nephews?, the sons of the following word.We choose the leftmost two.4.
Words in dependency paths.
ECs usu-ally have long distance dependencies withwords which cannot be fully captured bythe above categories.
We need a new fea-ture to describe such long distance seman-tic relations: Dependency Paths.
From thetraining data, we collect all the paths fromroot nodes to ECs (ECs excluded) togetherwith dependency types.
Below we give anexample to illustrate the extraction of thiskind of features using a complex sentencefollowing word (??).
But such phenomenas are rare, sohere we still adopt the tree based method.with a multi-layer hierarchical dependencytree as in Fig.
3.
If we have m kinds ofsuch paths with different path types or de-pendency types, we need md elements torepresent this kind of features.
The dis-tributed representations of the words wouldbe placed at the corresponding positions inthe feature vector and the remaining are setto 0.Previous work usually involves lots of syntac-tic and semantic features.
In the work of (Xueand Yang, 2013), 6 kinds of features are used,including those derived from constituency parsetrees, dependency parse trees, semantic rolesand others.
Here we use only the dependencyparse trees for the feature extraction.
The wordsin dependency paths we use have proven theirpotential in representing the meanings of textin frame identification (Hermann et al, 2014).Take the OP in the sentence shown in Fig.
3for example.
For the OP, its head word is ??
?,its following word is ????
and its nephews are?NULL?
and ?NULL?
(ECs are invisible).The dependency path from root to OP is:RootROOT?????
?
?/hold COMP??????
??/ceremonyRELC?????
?/DE COMP??????
OPFor such a path, we have the followingsubpaths:RootROOT?????
.COMP??????
.RELC?????
XRootROOT?????
.COMP??????
XRootROOT?????
XFor the position of the OP in the given exam-ple, the words with corresponding dependencypaths are ??
?, ????
and ????.
Similarly,we collects all the paths from other ECs in thetraining examples to build the feature template.In the testing phase, for each possible EC po-sition, we place the distributed representationsof the right words at the corresponding positionsof its feature vector.266???
??
31 ?
??
?
OP pro T ??
??
?
??
??
?ROOTSUBJTMP PRTCOMPUNKNMODRELCAMODCOMPCOMPSBJADV COMP??
?/Russian ?
?/troops 31 ?/31rd ?
?/hold ?/past-tense-marker ?
?/farewell ?
?/Germany ?/DE ?
?/final ?
?/ceremony ?Figure 3: ECs in a Dependency TreeTrain Dev TestFile 81-325, 400-454 41-80 1-40500-554, 590-596 901-931600-885, 900#pro 1023 166 297#PRO 1089 210 298#OP 2099 301 575#T 1981 287 527#RNR 91 15 32#* 22 0 19#Others 0 0 0Total 6305 979 1748Table 1: Data Division and EC Distribution3 Experiments on CTB3.1 DataThe proposed method can be applied to variouskinds of languages as long as annotated corpusare available.
In our experiments, we use a sub-set of Chinese Treebank V7.0.We split the data set into three parts, train-ing, development and test data.
Following theprevious research, we use File 1-40 and 901-931as the test data, File 41-80 as the developmentdata.
The training data includes File {81-325,400-454, 500-554, 590-596, 6000-885, 900}.
Thedevelopment data is used to tune parametersand the final results are reported on the testdata.
CTB trees are transferred to dependencytrees for feature extraction with ECs preserved(Xue, 2007).The distributed word representation we useis learned using the word2vec toolkit (Mikolovet al, 2013).
We train the model on a largeChinese news copora provided by Sogou2, whichcontains about 1 billion words after necessarypreprocessing.
The text is segmented into wordsusing ICTCLAS(Zhang et al, 2003)3.3.2 Experiment SettingsInitialization WAis initialized according touniform[?24din+dhidden,24din+dhidden].And WBis initialized usinguniform[?24dhidden+dout,24dhidden+dout].Here din, dhiddenand doutare the dimensions ofthe input layer, the hidden space and the labelspace.Parameter Tuning To optimize the param-eters, firstly, we set the dimension of word vec-tors to be 80, the dimension of hidden space tobe 50.
We search for the suitable learning ratein {10?1, 10?2, 10?4}.
Then we deal with thedimension of word vectors {80, 100, 200}.
Fi-nally we tune the dimension of hidden space in{50, 200, 500} against the F-1 scores.
.
Thoseunderlined figures are the value of the param-eters after optimization.
We use the stochas-tic gradient descent algorithm to optimize themodel.
The details can be checked here (Westonet al, 2011).
The maximum iteration numberwe used is 10K.
In the following experiments,2http://www.sogou.com/labs/dl/cs.html3The word segment standards used by CTB and ICT-CLAS are roughly the same with minor differences.267we set the parameters to be learning rate=10?1,word vector dimension=80 and hidden layer di-mension=500.From the experiments for parameter tuning,we find that for the word embeddings in theproposed model, low dimension vectors are bet-ter than high dimensions one for low dimensionvectors are better in sharing meanings.
For thehidden space which represents inputs as uninter-preted vectors, high dimensional vectors are bet-ter than low dimensional vectors.
The learningrates also have an impact on the performance.If the learning rate is too small, we need moreiterations to achieve convergence.
If we stop it-erations too early, we will suffer under-fitting.3.3 Results3.3.1 Metrics and EvaluationPrevious work reports results based on dif-ferent evaluation metrics.
Some work uses lin-ear positions to describe ECs.
ECs are judgedon a ?whether there is an EC of type A beforea certain token in the text?
basis (Cai et al,2011).
Collapsing ECs before the same token toone, Cai et al (2011) has 1352 ECs in the testdata.
Xue and Yang (2013) has stated that someECs that share adjacent positions have differentheads in the parse tree.
They judge ECs on a?whether there is an EC of type A with a certainhead word and a certain following token in thetext?
basis.
Using this kind of metric, they gets1765 ECs.Here we use the same evaluation metric withXue and Yang (2013).
Note that we still cannotdescribe all the 1838 ECs in the corpora, for onsome occasions ECs preceding the same tokenshare the same head word.
We also omit someECs which cause cycles in dependency trees asdescribed in the previous sections.
We have 1748ECs, 95% of all the ECs in the test data, veryclose to 1765 used by Xue and Yang (2013).
Thetotal number of ECs has an impact on the re-call.
In Table 3, we include results based oneach method?s own EC count (1748, 1765, 1352for Ours, Xue?s and Cai?s respectively) and thereal total EC count 1838 (figures in brackets).Yang and Xue (2010) report an experimentresult based on a classification model in a unifiedType PRO pro T OP RNR * Others Total297 298 575 527 32 19 0 1748Xue 305 298 584 527 32 19 0 1765Cai 299 290 578 134 32 19 0 1352Table 2: EC Distribution in the Test Dataclass correct p r F1PRO 162 .479 .545 .510pro 161 .564 .540 .552OP 409 .707 .776 .740T 506 .939 .88 .908RNR 23 .767 .719 .742* 0 0 0 0Overall 1261 .712 .721 .717(.686) (.699)(Xue) 903 .653 .512 .574(.491) (.561)(Cai) 737 .660 .545 .586(.401) (.499)Table 3: Performance on the CTB Test Dataparsing frame.
We do not include it for it usesdifferent and relativelyThe distributions of ECsin the test data are shown in Table 2.The results are shown in Table 3.
We presentthe results for each kind of EC and compare ourresults with two previous state-of-the-art meth-ods(Cai et al, 2011; Xue and Yang, 2013).The proposed method yields the newest state-of-the-art performances on CTB as far as weknow.
We also identify the dependency typesbetween ECs and their heads.
Some ECs, suchas pro and PRO, are latent subjects of sen-tences.
They usually serve as SBJ with veryfew exceptions.
While the others may play var-ious roles.
There are 31 possible (EC,Dep)pairs.
Using the same model, the overall resultis p = 0.701, r = 0.703, f = 0.702.3.3.2 AnalysisWe compare the effectiveness of different fea-tures by eliminating each kind of features de-scribed in the previous section.
As Table 4shows, the most important kind is the depen-dency paths, which cause a huge drop in per-formance if eliminated.
Dependency paths en-code words and path pattern information whichis proved essential for the detection of ECs.
Be-sides, headwords are also useful.
While for the268-dep -head -following -nephewsF1 .501 .604 .703 .716(-.216) (-.103) (-.014) (-.001)Table 4: Effectiveness of Featuresothers, we cannot easily make the conclusionthat they are of little usage in the identificationof ECs.
They are not fully explored in the pro-posed model, but may be vital for EC detectionin reality.Worth to mention is that of the several kindsof ECs, the proposed method shows the bestperformance on ECs of type T, which repre-sents ECs that are the trace of ?A?
?-movement,which moves a word to a position where nofixed grammatical function is assigned.
Here wegive an example:?
[ ] ??
?/seem A ?
?/like B.?
?A ??
?/seem (EC) ?
?/like B.?A is moved to the head of the sentence as thetopic (topicalization) and left a trace which isthe EC.
To detect this EC, we need informationabout the action ??
?/like?, the link verb ????/seem?
and the arguments ?A?
and ?B?.
ECsof type T are very common in Chinese, sinceChinese is a topic-prominent language.
Usingdistributed representations, it is easy to encodethe context information in our feature vectorsfor EC detection.We also have satisfying results and significantimprovements for the other types except * (traceof A-movement), which make up about 1% of allthe ECs in the test data.
Partly because thereare too few * examples in the training data.
Weneed to further improve our models to detectsuch ECs.4 DiscussionThe proposed method is capable of handlinglarge set of labels.
Hence it is possible to detectEC types and dependency types simultaneously.Besides, some other NLP tasks can also be for-mulated as annotation tasks, and therefore canbe resolved using the same scheme, such as theframe identification for verbs (Hermann et al,2014).This work together with some previous workthat uses classification methods (Cai et al, 2011;Xue and Yang, 2013; Xue, 2007), regards ECsin a sentence as independent to each other andeven independent to words that do not appear inthe feature vectors.
Such an assumption makesit easier to design models and features but doesnot reflect the grammatic constraints of lan-guages.
For example, simple sentences in Chi-nese contain one and only one subject, whetherit is an EC or not.
If it is decided there is an ECas a subject in a certain place, there should be nomore ECs as subjects in the same sentence.
Butsuch an important property is not reflected inthese classification models.
Methods that adoptparsing techniques take the whole parse tree asinput and output a parse tree with EC anchored.So we can view the sentence as a whole and dealwith ECs with regarding to all the words in thesentence.
Iida and Poesio (2011) also take thegrammar constraints into consideration by for-mulating EC detection as an ILP problem.
Butthey usually yield poor performances comparedwith classification methods partly because themethods they use can not fully explore the syn-tactic and semantic features.5 Related WorkEmpty category is a complex problem (Li andHovy, 2015).
Existing methods for EC detec-tion mainly explores syntactic and semantic fea-tures using classification models or parsing tech-niques.Johnson (2002) proposes a simple patternbased algorithm to recover ECs, both the posi-tions and their antecedents in phrase structuretrees.
Gabbard et al (2006) presents a two stageparser that uses syntactical features to recoverPenn Treebank style syntactic analyses, includ-ing the ECs.
The first stage, sentences are parseas usual without ECs, and in the second stage,ECs are detected using a learned model with richtext features in the tree structures.
Kong andZhou (2010) reports a tree kernel-based modelwhich takes as input parse trees for EC detec-tion.
They also deal with EC resolution, to269link ECs to text pieces if possible.
They re-ports their results on Chinese Treebank.
Yangand Xue (2010) try to restore ECs from parsetrees using a Maximum Entropy model.
Iidaand Poesio (2011) propose an cross-lingual ILP-based model for zero anaphora detection.
Cai etal.
(2011) reports a classification model for ECdetection.
Their method is based on ?is therean EC before a certain token?.Recently Xue and Yang (2013) further de-velop the method of Yang and Xue (2010) andexplore rich syntactical and semantical features,including paths in parse trees and semanticroles, to train an ME classification model forEC detection and yield the best performance re-ported using a strict evaluation metric on Chi-nese Treebank as far as we know.As we have stated, the traditional featuresused by above methods are not good at cap-turing the meanings of contexts.
Currently thedistributed representations together with deepneural networks have proven their ability notonly in representing meaning of words, inferringwords from the context, but also in represent-ing structures of text (Socher et al, 2010; Liet al, 2015).
Deep neural networks are capableof learning features from corpus, therefore savesthe labor of feature engineering and have proventheir ability in lots of NLP task (Collobert et al,2011; Bengio et al, 2006).The most relevant work to this paper are thatof Weston et al (2011) and that of Hermannet al (2014).
Weston et al (2011) propose adeep neural network scheme exploring the hid-den space for image annotation.
They map boththe images and labels to the same hidden spaceand annotate new images according to their rep-resentations in the hidden space.
Hermann etal.
(2014) extend the scheme to frame identifi-cation, for which they obtain satisfying results.This paper further uses it for empty categorydetection with features designed for EC detec-tion.Compared with previous research, the pro-posed model simplifies the feature engineeringgreatly and produces distributed representationsfor both ECs and EC types which will benefitother tasks.6 ConclusionIn this paper, we propose a new empty categorydetection method using distributed word repre-sentations.
Using the word embeddings of thecontexts of ECs as features enables us to employrich information in the context without muchfeature engineering.
Experiments on CTB haveverified the advantages of the proposed method.We successfully beat the existing state-of-the-art methods based on a strict evaluation metric.The proposed method can be further applied toother languages such as Japanese.
We will fur-ther explore the feasibility of using neural net-works to resolve empty categories: to link ECsto their antecedents.ReferencesYoshua Bengio, Holger Schwenk, Jean-S?bastienSen?cal, Fr?deric Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
InInnovations in Machine Learning, pages 137?186.Springer.Shu Cai, David Chiang, and Yoav Goldberg.
2011.Language-independent parsing with empty ele-ments.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Lin-guistics: Human Language Technologies: shortpapers-Volume 2, pages 212?216.
Association forComputational Linguistics.Tagyoung Chung and Daniel Gildea.
2010.
Effectsof empty categories on machine translation.
InProceedings of EMNLP, pages 636?645.
ACL.Ronan Collobert and Jason Weston.
2008.
A uni-fied architecture for natural language processing:Deep neural networks with multitask learning.
InProceedings of the 25th international conferenceon Machine learning, pages 160?167.
ACM.Ronan Collobert, Jason Weston, L?on Bottou,Michael Karlen, Koray Kavukcuoglu, and PavelKuksa.
2011.
Natural language processing (al-most) from scratch.
The Journal of MachineLearning Research, 12:2493?2537.Ryan Gabbard, Mitchell Marcus, and Seth Kulick.2006.
Fully parsing the penn treebank.
In Pro-ceedings of the main conference on human lan-guage technology conference of the North Amer-ican chapter of the association of computationallinguistics, pages 184?191.
Association for Com-putational Linguistics.Karl Moritz Hermann, Dipanjan Das, Jason Weston,and Kuzman Ganchev.
2014.
Semantic frame270identification with distributed word representa-tions.
In Proceedings of ACL.Ryu Iida and Massimo Poesio.
2011.
A cross-lingualilp solution to zero anaphora resolution.
In Pro-ceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: HumanLanguage Technologies-Volume 1, pages 804?813.Association for Computational Linguistics.Mark Johnson.
2002.
A simple pattern-matchingalgorithm for recovering empty nodes and theirantecedents.
In Proceedings of the 40th AnnualMeeting on Association for Computational Lin-guistics, pages 136?143.
ACL.Fang Kong and Hwee Tou Ng.
2013.
Exploitingzero pronouns to improve chinese coreference res-olution.
In EMNLP, pages 278?288.Fang Kong and Guodong Zhou.
2010.
A tree kernel-based unified framework for chinese zero anaphoraresolution.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 882?891.
Association for Computa-tional Linguistics.Jiwei Li and Eduard Hovy.
2014.
A model of coher-ence based on distributed sentence representation.In Proceedings of the 2014 Conference on Empiri-cal Methods in Natural Language Processing, pages2061?2069.Jiwei Li and Eduard Hovy.
2015.
The nlp engine: Auniversal turing machine for nlp.
arXiv preprintarXiv:1503.00168.Jiwei Li, Rumeng Li, and Eduard Hovy.
2014.
Re-cursive deep models for discourse parsing.
InProceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing, pages2061?2069.Jiwei Li, Dan Jurafsky, and Eudard Hovy.
2015.When are tree structures necessary for deeplearning of representations?
arXiv preprintarXiv:1503.00185.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Com-putational linguistics, 19(2):313?330.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg SCorrado, and Jeff Dean.
2013.
Distributed rep-resentations of words and phrases and their com-positionality.
In Proceedings of NIPS2013, pages3111?3119.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting semantic role labeling, wordnet andwikipedia for coreference resolution.
In Proceed-ings of the main conference on Human LanguageTechnology Conference of the North AmericanChapter of the Association of Computational Lin-guistics, pages 192?199.
Association for Computa-tional Linguistics.Richard Socher, Christopher D Manning, and An-drew Y Ng.
2010.
Learning continuous phraserepresentations and syntactic parsing with recur-sive neural networks.
In Proceedings of the NIPS-2010 Deep Learning and Unsupervised FeatureLearning Workshop, pages 1?9.Nicolas Usunier, David Buffoni, and Patrick Galli-nari.
2009.
Ranking with ordered weighted pair-wise classification.
In Proceedings of ICML2009,pages 1057?1064.
ACM.Jason Weston, Samy Bengio, and Nicolas Usunier.2011.
Wsabie: Scaling up to large vocabulary im-age annotation.
In Proceedings of IJCAI2011, vol-ume 11, pages 2764?2770.Bing Xiang, Xiaoqiang Luo, and Bowen Zhou.
2013.Enlisting the ghost: Modeling empty categoriesfor machine translation.
In ACL (1), pages 822?831.
Citeseer.Nianwen Xue and Yaqin Yang.
2013.
Dependency-based empty category detection via phrase struc-ture trees.
In HLT-NAACL, pages 1051?1060.Naiwen Xue, Fei Xia, Fu-Dong Chiou, and MartaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
Naturallanguage engineering, 11(02):207?238.Nianwen Xue.
2007.
Tapping the implicit informa-tion for the ps to ds conversion of the chinese tree-bank.
In Proceedings of the Sixth InternationalWorkshop on Treebanks and Linguistics Theories.Yaqin Yang and Nianwen Xue.
2010.
Chasing theghost: recovering empty categories in the chinesetreebank.
In Proceedings of the 23rd Interna-tional Conference on Computational Linguistics:Posters, pages 1382?1390.
ACL.Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, andQun Liu.
2003.
Hhmm-based chinese lexi-cal analyzer ictclas.
In Proceedings of the sec-ond SIGHAN workshop on Chinese languageprocessing-Volume 17, pages 184?187.
Associationfor Computational Linguistics.271
