Modeling with Structures in Statistical Machine TranslationYe-Y i  Wang and A lex  Waibe lSchool of Computer  ScienceCarnegie Mellon University5000 Forbes AvenuePi t tsburgh,  PA 15213, USA{yyw, waibel}?cs, cmu.
eduAbst ractMost statistical machine translation systemsemploy a word-based alignment model.
In thispaper we demonstrate that word-based align-ment is a major cause of translation errors.
Wepropose a new alignment model based on shal-low phrase structures, and the structures canbe automatically acquired from parallel corpus.This new model achieved over 10% error reduc-tion for our spoken language translation task.1 In t roduct ionMost (if not all) statistical machine translationsystems employ a word-based alignment model(Brown et al, 1993; Vogel, Ney, and Tillman,1996; Wang and Waibel, 1997), which treatswords in a sentence as independent entities andignores the structural relationship among them.While this independence assumption works wellin speech recognition, it poses a major problemin our experiments with spoken language trans-lation between a language pair with very dif-ferent word orders.
In this paper we propose atranslation model that employs shallow phrasestructures.
It has the following advantages overword-based alignment:?
Since the translation model can directly de-pict phrase reordering in translation, it ismore accurate for translation between lan-guages with different word (phrase) orders.?
The decoder of the translation system canuse the phrase information and extendhypothesis by phrases (multiple words),therefore it can speed up decoding.The paper is organized as follows.
In sec-tion 2, the problems of word-based alignmentmodels are discussed.
To alienate these prob-lems, a new alignment model based on shal-low phrase structures is introduced in section3.
In section 4, a grammar inference algorithmis presented that can automatically acquire thephrase structures used in the new model.
Trans-lation performance is then evaluated in sec-tion 5, and conclusions are presented in sec-tion 6.2 Word-based  A l ignment  Mode lIn a word-based alignment translation model,the transformation from a sentence at the sourceend of a communication channel to a sentenceat the target end can be described with the fol-lowing random process:1.
Pick a length for the sentence at the targetend.2.
For each word position in the target sen-tence, align it with a source word.3.
Produce a word at each target word po-sition according to the source word withwhich the target word position has beenaligned.IBM Alignment Model 2 is a typical exampleof word-based alignment.
Assuming a sentences = S l , .
.
.
, s t  at the source of a channel, themodel picks a length m of the target sentencet according to the distribution P(m I s) = e,where e is a small, fixed number.
Then for eachposition i (0 < i _< m) in t, it finds its corre-sponding position ai in s according to an align-ment distribution P(ai  l i, a~ -1, m, s) = a(ai li, re, l).
Finally, it generates a word ti at theposition i of t from the source word s~, at thealigned position ai, according to a translationz 1 m distribution P(t i  \] t~- , a 1 , s) -- t(ti I s~,).1357waere denn Montag der sech und zwanzigste Juli moeglichit 's going to difficulty to find meeting time i think is Monday the twenty sixth of July possiblewaere denn Montag der sech und zwanzigste Juli moeglichit 's going to difficulty to find meeting time I think is Monday the twenty sixth of July possibleFigure 1: Word Alignment with deletion in translation: the top alignment is the one made by IBMAlignment Model 2, the bottom one is the 'ideal' alignment.fiter der zweiten Terrain im Mai koennte ich den Mittwoch den fuenf und zwanzigsten anbieten1 could offer ~ou Wednesday the twenty fifth for the second date in Mayfuer der zweiten Termin im Mai koennte ich den Mittwoch den fuenf und zwanzigsten anbietenI could offer you Wednesday the twenty fifth for the second date in MayFigure 2: Word Alignment of translation with different phrase order: the top alignment is the onemade by IBM Alignment Model 2, the bottom one is the 'ideal' alignment.fuer der zweiten Termin im Mai koennte ich den Mittwoch den fuenf und zwanzigsten anbieten!
could offer you Wednesday the twenty fifth for the second date in MayFigure 3: Word Alignment with Model 1 for one of the previous examples.
Because no alignmentprobability penalizes the long distance phrase reordering, it is much closer to the 'ideal' alignment.1358Therefore, P ( t \ ] s )  is the sum of the proba-bilities of generating t from s over all possiblealignments A, in which the position i in t isaligned with the position ai in s:P(t Is)l l me y~ ... ~ l"It(tjls~J)a(ajlj, l m)a l~0 am=Oj=lm le 1-I Y~ t(t j ls i)a(i l j ,  l,m) (1)j= l i=OA word-based model may have severe prob-lems when there are deletions in translation(this may be a result of erroneous entencealignment) or the two languages have differentword orders, like English and German.
Figure 1and Figure 2 show some problematic alignmentsbetween English/German sentences made byIBM Model 2, together with the 'ideal' align-ments for the sentences.
Here the alignmentparameters penalize the alignment of Englishwords with their German translation equiva-lents because the translation equivalents are faraway from the words.An experiment reveals how often this kindof "skewed" alignment happens in our En-glish/German scheduling conversation parallelcorpus (Wang and Waibel, 1997).
The ex-periment was based on the following obser-vation: IBM translation Model 1 (where thealignment distribution is uniform) and Model2 found similar Viterbi alignments when therewere no movements or deletions, and they pre-dicted very different Viterbi alignments whenthe skewness was severe ill a sentence pair, sincethe alignment parameters in Model 2 penalizethe long distance alignment.
Figure 3 shows theViterbi alignment discovered by Model 1 for thesame sentences in Figure 21 .We measured the distance of a Model 1alignment a 1 and a Model 2 alignment a z~--,Igl la ~ _ a2\].
To estimate the skew- aS A.-,i= 1ness of the corpus, we collected the statisticsabout the percentage of sentence pairs (with at~The better alignment on a given pair of sentencesdoes not mean Model 1 is a better model.
Non-uniformalignment distribution is desirable.
Otherwise, languagemodel would be the only factor that determines thesource sentence word order in decoding.e~3025201510500 0.5 1 1.5 2 2.5Alignment distance > x * target sentence lengthFigure 4: Skewness of Translationsleast five words in a sentence) with Model 1and Model 2 alignment distance greater than1 /4 ,2 /4 ,3 /4 , .
.
.
,  10/4 of the target sentencelength.
By checking the Viterbi alignmentsmade by both models, it is almost certain thatwhenever the distance is greater that 3/4 of thetarget sentence length, there is either a move-ment or a deletion in the sentence pair.
Fig-ure 4 plots this statistic - -  around 30% of thesentence pairs in our training data have somedegree of skewness in alignments.3 S t ructure -based  A l ignment  Mode lTo solve the problems with the word-basedalignment models, we present a structure-basedalignment model here.
The idea is to di-rectly model the phrase movement with a roughalignment, and then model the word alignmentwithin phrases with a detailed alignment.Given an English sentence = ele2.. .et ,  itsGerman translation g = 9192"" "gin can be gen-erated by the following process:1.
Parse e into a sequence of phrases, soZ ---- (e11 ,  e12 ,  ?
?
?
, e l / l )  (e21 ,  e22 ,  ?
?
.
,  e212)  ?
?
?
(enl, enz,.--, e~l.
)= EoEIE2.. .En,where E0 is a null phrase.2.
With the probability P(q \] e,E) ,  deter-mine q < n + 1, the number of phrases ing.
Let Gi ' "Gq denote these q phrases.Each source phrase can be aligned with atmost one target phrase.
Unlike Englishphrases, words in a German phrase do not1359have to form a consecutive sequence.
Sog may be expressed with something likeg = gllg12g21g13g22"", where gij repre-sents the j - th word in the i-th phrase.3.
For each German phrase Gi, 0 <_ i < q, withthe probability P( r i l i ,  r~ -1, E, e), align itwith an English phrase E~.4.
For each German phrase Gi, 0 <_ i < q, de-termine its beginning position bi in g withthe distribution P(bi l " 1.i-1 _q e,  E ) .
~, u 0 ~ r0~5.
Now it is time to generate the individualwords in the German phrases through de-tailed alignment.
It works like IBM Model4.
For each word eij in the phrase Ei,its fertility ?ij has the distribution P(?i j  I. .
j -1?i0-1 E).
~ 3, ?
i l  , , bo, ro, e,6.
For each word eij in the phrase Ei, it gen-erates a tablet rij = {Tij l ,Ti j2, ' ' 'Ti j?i j}by generating each of the words in rijin turn with the probability P(ri jk Ir~.li,rJ~ -1 - , rio-l, l%, bo,qr~,e,E) fo r thek- thword in the tablet.7.
For each element risk in the tabletvii, the permutation 7rij k determinesits position in the target sentence ac-cording to the distribution P(rrij k I7rk_ 1 "- .
i j l  , 7r~l 1, 7r;-1, TO/, (~/, b(~, r~, e, E) .We made the following independence assump-tions:1.
The number of target sentence phrases de-pends only on the number of phrases in thesource sentence:P (q le ,  E) - -pn(q \ [n )2.
P(r i  l i, r~- l ,E ,e )= a ( r i l i )  x 1-I0_<j<i(1 - 5(ri, rj))where 5(x,y) = 1 when x = y ,  and5(x, y) = 0 otherwise.This assumption states that P(ri  Ii, rio-X,E,e) depends on i and ri.
It also1 depends on r~- with the factor YI0<j<i(1-(f(ri, rj)) to ensure that each EnglisI~ phraseis aligned with at most one German phrase.3.
The beginning position of a target phrasedepends on its distance from the beginningposition of its preceding phrase, as well as..the length of the source phrase aligned withthe preceding phrase:P(bi l i, bio-l ,r~,e,E)= I = o (Ai I lEr,_,l)The fertility and translation tablet of asource word depend on the word only:P(?i j  l i,J, ?ilj-1 , wo'~i-1 , ~o,hq rq, e, E)= n(?i j  lP(Tijk I Tkl 1,7":i 1 -  "- , rg -1, ?0,t bo ,q r~,e,E)= lev i )The leftmost position of the translations ofa source word depends on its distance fromthe beginning of the target phrase alignedwith the source phrase that contains thatsource word.
It also depends on the iden-tity of the phrase, and the position of thesource word in the source phrase.j -1 i - i  t E)= dl (Trijl - -  bil El, j )For a target word rijk other than the left-most Tij 1 in the translation tablet of thesource eij, its position depends on its dis-tance from the position of another tabletword 7"ij(k_l) closest to its left, the class ofthe target word Tijk, and the fertility of thesource word eij.p(  jkl  l 1, i-1 i l - rCil ,Tr o ,rO,?o,b~,r~,e,E)= d2(rcijk - lrij(k_l) I 6(ri jk), ?ij)here G(g) is the equivalent class for g.3.1 Parameter EstimationEM algorithm was used to estimate the seventypes of parameters: Pn, a, a, ?, r, dl andd2.
We used a subset of probable alignmentsin the EM learning, since the total number ofalignments i exponential to the target sentencelength.
The subset was the neighboring align-ments (Brown et al, 1993) of the Viterbi align-ments discovered by Model 1 and Model 2.
Wechose to include the Model 1 Viterbi alignmenthere because the Model 1 alignment is closerto the "ideal" when strong skewness exists in asentence pair.4 F inding the StructuresIt is of little interest for the structure-basedalignment model if we have to manually find1360the language structures and write a grammarfor them, since the primary merit of statisticalmachine translation is to reduce human labor.In this section we introduce a grammar infer-ence technique that finds the phrases used in thestructure-based alignment model.
It is based onthe work in (Ries, Bu?, and Wang, 1995), wherethe following two operators are used:..Clustering: Clustering words/phraseswith similar meanings/grammatical func-tions into equivalent classes.
The mutualinformation clustering algorithm(Brown etal., 1992) were used for this.Phrasing: The equivalent class sequenceCl, c2,...c k forms a phrase ifP(cl, c2,'" "ck) log P(cI, c2,'" "ck) > 8,P(c,)P(c2)" "P(ck)where ~ is a threshold.
By changing thethreshold, we obtain a different number ofphrases.The two operators are iteratively applied tothe training corpus in alternative steps.
Thisresults in hierarchical phrases in the form of se-quences of equivalent classes of words/phrases.Since the algorithm only uses a monolin-gual corpus, it often introduces ome language-specific structures resulting from biased usagesof a specific language.
In machine transla-tion we are more interested in cross-linguisticstructures, similar to the case of using interlin-gua to represent cross-linguistic information inknowledge-based MT.To obtain structures that are common in bothlanguages, a bilingual mutual information clus-tering algorithm (Wang, Lafferty, and Waibel,1996) was used as the clustering operator.
Ittakes constraints from parallel corpus.
We alsointroduced an additional constraint in cluster-ing, which requires that words in the same classmust have at least one common potential part-of-speech.Bilingual constraints are also imposed on thephrasing operator.
We used bilingual heuris-tics to filter out the sequences acquired by thephrasing operator that may not be common inmultiple languages.
The heuristics include:..Average Translation Span: Given aphrase candidate, its average translationspan is the distance between the leftmostand the rightmost arget positions alignedwith the words inside the candidate, av-eraged over all Model 1 Viterbi alignmentsof sample sentences.
A candidate is filteredout if its average translation span is greaterthan the length of the candidate multipliedby a threshold.
This criterion states thatthe words in the translation of a phrasehave to be close enough to form a phrasein another language.Ambigu i ty  Reduct ion :  A word occur-ring in a phrase should be less ambiguousthan in other random context.
Thereforea phrase should reduce the ambiguity (un-certainty) of the words inside it.
For eachsource language word class c, its translationentropy is defined as )-'\]~g t(g \[ c)log(g \[ c).The average per source class entropy re-duction induced by the introduction of aphrase P is therefore1\[p\[ ~--~\[~-'~ t(g Iv ) logt (g \ [c )cEP g- ~_t (g lc ,  P) logt(glc, P)\]gA threshold was set up for minimum en-tropy reduction.By applying the clustering operator followedwith the phrasing operator, we obtained shallowphrase structures partly shown in Figure 5.Given a set of phrases, we can deterministi-cally parse a sentence into a sequence of phrasesby replacing the leftmost unparsed substringwith the longest matching phrase in the set.5 Eva luat ion  and D iscuss ionWe used the Janus English/German schedul-ing corpus (Suhm et al, 1995) to train ourphrase-based alignment model.
Around 30,000parallel sentences (400,000 words altogether forboth languages) were used for training.
Thesame data were used to train Simplified Model2 (Wang and Waibel, 1997) and IBM Model3 for performance comparison.
A larger En-glish monolingual corpus with around 0.5 mil-lion words was used for the training of a bigram1361\[Sunday Monday..\]\[Sunday Monday..\]\[Sunday Monday.
.\]\[Sunday Monday..\]\[Sunday Monday..\]\[Sunday Monday..\]\[January February.\[January February.\[afternoon morning...\]\[at by...\] \[one two...\]\[the every each...\] \[first second third...\]\[the every each...\] \[twenty depending remaining3\[the every each...\] \[eleventh thirteenth...\]\[in within...\] \[January February...\].\] \[first second third...\] \[at by...\].\] \[first second third...\]\[January February...\] \[the every each...\] \[first second third...\]\[I he she itself\] \[have propose remember hate...\]\[eleventh thirteenth...\] \[after before around\] \[one two three...\]Figure 5: Example of Acquired Phrases.
Words in a bracket form a cluster, phrases are clustersequences.
Ellipses indicate that a cluster has more words than those shown here.Model Correct OK Incorrect AccuracyModel 2 284 87 176 59.9%Model 3 98 45 57 60.3%S.
Model 303 96 148 64.2%Table h Translation Accuracy: a correct trans-lation gets one credit, an okay translation gets1/2 credit, an incorrect one gets 0 credit.
Sincethe IBM Model 3 decoder is too slow, its per-formance was not measured on the entire testset.ity mass is more scattered in the structure-basedmodel, reflecting the fact that English and Ger-man have different phrase orders.
On the otherhand, the word based model tends to align atarget word with the source words at similar po-sitions, which resulted in many incorrect align-ments, hence made the word translation proba-bility t distributed over many unrelated targetwords, as to be shown in the next subsection.5.3 Mode l  Complex i tylanguage model.
A preprocessor splited Ger-man compound nouns.
Words that occurredonly once were taken as unknown words.
Thisresulted in a lexicon of 1372 English and 2202German words.
The English/German lexiconswere classified into 250 classes in each languageand 560 English phrases were constructed uponthese classes with the grammar inference algo-rithm described earlier.We limited the maximum sentence length tobe 20 words/15 phrases long, the maximum fer-tility for non-null words to be 3.5.1 Trans la t ion  AccuracyTable 1 shows the end-to-end translation perfor-mance.
The structure-based model achieved anerror reduction of around 12.5% over the word-based alignment models.5.2 Word  Order  and Phrase  A l ignmentTable 2 shows the alignment distribution for thefirst German word/phrase in Simplified Model2 and the structure-based model.
The probabil-The structure-based model has 3,081,617 freeparameters, an increase of about 2% over the3,022,373 free parameters of Simplified Model 2.This small increase does not cause over-fitting,as the performance on the test data suggests.On the other hand, the structure-based modelis more accurate.
This can be illustrated withan example of the translation probability distri-bution of the English word 'T'.
Table 3 showsthe possible translations of 'T' with probabilitygreater than 0.01.
It is clear that the structure-based model "focuses" better on the correcttranslations.
It is interesting to note that theGerman translations in Simplified Model 2 of-ten appear at the beginning of a sentence, theposition where 'T' often appears in English sen-tences.
It is the biased word-based alignmentsthat pull the unrelated words together and in-crease the translation uncertainty.We define the average translation entropy asm nF_.
P(ei) F_, -t(gs Iei)logt(gs l ei).i=O j= l1362j 0 1 2 3 4 5 6 7aM2(jl 1) 0.04 0.86 0.054 0.025 0.008 0.005 0.004 0.002asM(jl 1) 0.003 0.29 0.25 0.15 0.07 0.11 0.05 0.048 93.3x I0 -4 2.9xi0 -40.02 0.01Table 2: The alignment distribution for the first German word/phrase in Simplified Model 2 andin the structure-based model.
The second distribution reflects the higher possibility of phrasereordering in translation.tM2(*l I) tSM(*l I)ich 0.708da 0.104am 0.024das 0.022dann 0.022also 0.019es 0.011ich 0.988mich 0.010Table 3: The translation distribution of "I'.
Itis more uncertain in the word-based alignmentmodel because the biased alignment distribu-tion forced the associations between unrelatedEnglish/German words.
(m, n are English and German lexicon size.
)It is a direct measurement of word transla-tion uncertainty.
The average translation en-tropy is 3.01 bits per source word in Sim-plified Model 2, 2.68 in Model 3, and 2.50in the structured-based model.
Thereforeinformation-theoretically thecomplexity of theword-based alignment models is higher thanthat of the structure-based model.6 ConclusionsThe structure-based alignment directly modelsthe word order difference between English andGerman, makes the word translation distribu-tion focus on the correct ones, hence improvestranslation performance.7 AcknowledgementsWe would like to thank the anonymous COL-ING/ACL reviewers for valuable comments.This research was partly supported by ATR andthe Verbmobil Project.
The views and conclu-sions in this document are those of the authors.ematics of Statistical Machine Translation:Parameter Estimation.
Computational Lin-guistics, 19 (2) :263-311.Brown, P. F., V. J. Della-Pietra, P. V. deSouza,J.
C. Lai, and R. L. Mercer.
1992.
Class-Based N-gram Models of Natural Language.Computational Linguistics, 18 (4) :467-479.Ries, Klaus, Finn Dag Bu?, and Ye-Yi Wang.
1995.
Improved LanguageModelling by Unsupervised Acquisi-tion of Structure.
In ICASSP '95.IEEE.
corrected version available viahttp ://www.
cs.
cmu.
edu/~ies/icassp_gs, html.Suhm, B., P.Geutner, T. Kemp, A. Lavie,L.
Mayfield, A. McNair, I. Rogina, T. Schultz,T.
Sloboda, W. Ward, M. Woszczyna, andA.
Waibel.
1995.
JANUS: Towards multilin-gual spoken language translation.
In Proceed-ings of the ARPA Speech Spoken LanguageTechnology Workshop, Austin, TX, 1995.Vogel, S., H. Ney, and C. Tillman.
1996.HMM-Based Word Alignment in StatisticalTranslation.
In Proceedings of the Seven-teenth International Conference on Compu-tational Linguistics: COLING-g6, pages 836-841, Copenhagen, Denmark.Wang, Y., J. Lafferty, and A. Waibel.
1996.Word Clustering with Parallel Spoken Lan-guage Corpora.
In Proceedings of the 4th In-ternational Conference on Spoken LanguageProcessing (ICSLP'96), Philadelphia, USA.Wang, Y. and A. Waibel.
1997.
Decoding Al-gorithm in Statistical Machine Translation.In Proceedings of the 35th Annual Meetingof the Association for Computational Lin-guistics and 8th Conference of the EuropeanChapter of the Association for ComputationalLinguistics (A CL/EA CL '97), pages 366-372,Madrid, Spain.ReferencesBrown, P. F., S. A. Della-Pietra, V. J Della-Pietra, and R. L. Mercer.
1993.
The Math-1363
