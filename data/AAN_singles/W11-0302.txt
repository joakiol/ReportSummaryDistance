Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 10?18,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsThe Effect of Automatic Tokenization, Vocalization, Stemming, and POSTagging on Arabic Dependency ParsingEmad MohamedSuez Canal UniversitySuez, Egyptemohamed@umail.iu.eduAbstractWe use an automatic pipeline of wordtokenization, stemming, POS tagging, andvocalization to perform real-world Arabicdependency parsing.
In spite of the highaccuracy on the modules, the very few errors intokenization, which reaches an accuracy of99.34%, lead to a drop of more than 10% inparsing, indicating that no high qualitydependency parsing of Arabic, and possiblyother morphologically rich languages, can bereached without (semi-)perfect tokenization.
Theother module components, stemming,vocalization, and part of speech tagging, do nothave the same profound effect on thedependency parsing process.1.
IntroductionArabic is a morphologically rich language in whichwords may be composed of several tokens andhold several syntactic relations.
We define word tobe a whitespace delimited unit and token to be(part of) a word that has a syntactic function.
Forexample, the word wsytzwjhA  (?????????
)(English:And he will marry her)  consists of 4 tokens: aconjunction w, a future marker s, a verb inflectedfor the singular masculine in the perfective formytzwj, and a feminine singular 3rd person objectpronoun.
Parsing such a word requirestokenization, and performing dependency parsingin the tradition of the CoNLL-X (Buchholz andMarsi, 2006) and CoNLL 2007 shared task (Nivreet al 2007) also requires part of speech tagging,lemmatization, linguistic features, andvocalization, all of which were in the humanannotated gold standard form in the shared task.The current study aims at measuring theeffect of a pipeline of non gold standardtokenization, lemmatization, vocalization,linguistic features and POS tagging on the qualityof Arabic dependency parsing.
We only assumethat we have gold standard sentence boundariessince we do not agree with the sentence boundariesin the data, and introducing our own will have acomplicating effect on evaluation.
The CoNLLshared tasks of 2006 and 2007 used gold standardcomponents in all fields, which is not realistic forArabic, or for any other language.
For Arabic andother morphologically rich languages, it may bemore unrealistic than it is for English, for example,since the CoNLL 2007 Arabic dataset has tokens,rather than white space delimited words, as entries.A single word may have more than onesyntactically functional token.
Dependency parsinghas been selected in belief that it is more suitablefor Arabic than constituent-based parsing.
Allgrammatical relations in Arabic are binaryasymmetrical relations that exist between thetokens of a sentence.
According to JonathanOwens (1997: 52): ?In general the Arabic notion ofdependency and that defined in certain modernversions e.g.
Tesniere (1959) rest on commonprinciples?.With a tokenization accuracy of 99.34%, aPOS tagging accuracy of 96.39%, and with theabsence of linguistic features and the use of wordstems instead of lemmas, the Labeled AttachmentScore drops from 74.75% in the gold standardexperiment to 63.10% in the completely automaticexperiment.
Most errors are a direct result oftokenization errors, which indicates that despite thehigh accuracy on tokenization, it is still not enoughto produce satisfactory parsing numbers.2.
Related StudiesThe bulk of literature on Arabic DependencyParsing stems from the two CoNLL shared tasks of2006 and 2007.
In CoNLL-X (Buchholz and10Marsi, 2006), the average Labeled AttachmentScore on Arabic across all results presented by the19 participating teams was 59.9% with a standarddeviation of 6.5.
The best results were obtained byMcDonald et al(2006) with a score of 66.9%followed by Nivre et al(2006) with 66.7%.The best results on Arabic in the CoNLL 2007shared task were obtained by Hall et al(2007) asthey obtained a Labeled Attachment Score of76.52%, 9.6 percentage points above the highestscore of the 2006 shared task.
Hall et alused anensemble system, based on the MaltParserdependency parser that extrapolates from a singleMaltParser system.
The settings with the SingleMaltParser led to a Labeled Accuracy Score of74.75% on Arabic.
The Single MaltParser is theone used in the current paper.
All the papers inboth shared tasks used gold standard tokenization,vocalization, lemmatization, POS tags, andlinguistic features.A more recent study is that by Marton et al(2010).
Although Marton et alvaried the POSdistribution and linguistic features, they still usedgold standard tokenization.
They also used theColumbia Arabic Treebank, which makes both themethods and data different from those presentedhere.3.
Data, Methods, and Evaluation3.1.DataThe data used for the current study is the same dataset used for the CoNLL (2007) shared task, withthe same division into training set, and test set.This design helps in comparing results in a waythat enables us to measure the effect of automaticpre-processing on parsing accuracy.
The data is inthe CoNLL column format.
In this format, eachtoken is represented through columns each ofwhich has some specific information.
The firstcolumn is the ID, the second the token, the thirdthe lemma, the fourth the coarse-grained POS tag,the fifth the POS tag, and the sixth column is a listof linguistic features.
The last two columns of thevector include the head of the token and thedependency relation between the token and itshead.
Linguistic features are an unordered set ofsyntactic and/or morphological features, separatedby a vertical bar (|), or an underscore if notavailable.
The features in the CoNLL 2007 Arabicdataset represent case, mood, definiteness, voice,number, gender and person.The data used for training thestemmer/tokenizer is taken from the ArabicTreebank (Maamouri and Bies, 2004).
Care hasbeen taken not to use the parts of the ATB that arealso used in the Prague Arabic DependencyTreebank (Haijc et al2004) since the PADT andthe ATB share material.3.2.
MethodsWe implement a pipeline as follows(1) We build a memory-based word segmenterusing TIMBL (Daelemans et al 2007)which treats segmentation as a per letterclassification in which each word segmentis delimited by a + sign whether it issyntactic or inflectional.
A set of hand-written rules then produces tokens andstems based on this.
Tokens aresyntactically functional units, and stemsare the tokens without the inflectionalsegments, For example, the wordwsytzwjhA above is segmented asw+s+y+tzwj+hA.
The tokenizer splits thisinto four tokens w, s, ytzwj, and hA, andthe stemmer strips the inflectional prefixfrom ytzwj to produce tzwj.
In thesegmentation experiments, the best resultswere obtained with the IB1 algorithm withsimilarity computed as weighted overlap,relevance weights computed with gainratio, and the number of k nearest distancesequal to 1.
(2)  The tokens are passed to the part ofspeech tagger.
We use the Memory-basedTagger, MBT, (Daelemans et al 2007).The MBT features for known wordsinclude the two context words to the leftalong with their disambiguated POS tags,the focus word itself, and one word to theright along with its ambitag (the set of allpossible tags it can take).
For unknownwords, the features include the first fiveletters and the last three letters of the word,the, the left context tag, the right context11ambitag, one word to the left, the focusword itself, one ambitag to the right, andone word to the right.
(3) The column containing the linguisticfeatures in the real world dependencyexperiment will have to remain vacant dueto the fact that it is hard to produce thesefeatures automatically given only naturallyoccurring text.
(4)  The dependency parser (MaltParser 1.3.1)takes all the information above andproduces the data with head anddependency annotations.Although the purpose of this experiment is toperform dependency parsing of Arabic without anyassumptions, one assumption we cannot avoid isthat the input text should be divided into sentences.For this purpose, we use the gold standard divisionof text into sentences without trying to detect thesentence boundaries, although this would benecessary in actual real-world use of dependencyparsing.
The reason for this is that it is not clearhow sentence boundaries are marked in the data asthere are sentences whose length exceeds 300tokens.
If we detected the boundariesautomatically, then we would face the problem ofaligning our sentences with those of the test set forevaluation, and many of the dependencies wouldnot still hold.In the parsing experiments below, we willuse the dependency parser MaltParser (Nivre et al,2006).
We will use Single MaltParser, as used byHall et al(2007), with the same settings for Arabicthat were used in the CoNLL 2007 shared task onthe same data to be as close as possible to theoriginal results in order to be able to compare theeffect of non gold standard elements in the parsingprocess.3.3.EvaluationThe official evaluation metric in the CoNLL 2007shared task on dependency parsing was the labeledattachment score (LAS), i.e., the percentage oftokens for which a system has predicted the correctHEAD and DEPREL, but results reported alsoincluded unlabeled attachment score (UAS), i.e.,the percentage of tokens with correct HEAD, andthe label accuracy (LA), i.e., the percentage oftokens with correct DEPREL.
We will use thesame metrics here.One major difference between the parsingexperiments which were performed in the 2007shared task and the ones performed here isvocalization.
The data set which was used in theshared task was completely vocalized with bothword-internal short vowels and case markings.Since vocalization in such a perfect form is almostimpossible to produce automatically, we havedecided to primarily use unvocalized data instead.We have removed the word internal short vowelsas well as the case markings from both the trainingset and the test set.
This has the advantage ofrepresenting naturally occurring Arabic moreclosely, and the disadvantage of losing informationthat is only available through vocalization.
Wewill, however, report on the effect of vocalizationon dependency parsing in the discussion.To give an estimate of the effectsvocalization has on dependency parsing, we havereplicated the original task with the vocalized data,and then re-run the experiment with theunvocalized version.
Table 1 presents the results:Vocalized UnvocalizedLAS 74.77% 74.16%UAS 84.09% 83.53%LA 85.68% 85.44%Table 1: Vocalized versus unvocalized dependencyparsingThe results of the experiment indicate thatvocalization has a positive effect on the quality ofthe parsing output, which may be due to the factthat ambiguity decreases with vocalization.Labeled attachment score drops from 74.77% onthe vocalized data to 74.16% on unvocalized data.Unlabeled attachment score drops from 84.09% to83.53% and labeled accuracy score from 85.68%to 85.44%.
The difference is minimal, and isexpected to be even smaller with automaticvocalization4.
Results and discussion4.1.TokenizationWe obtain an accuracy of 99.34%.
Out of the 4550words which the test set comprises, there are only30 errors affecting 21 out of the 132 sentences inthe test set.
17 of the errors can be characterized asover-tokenization while the other 13 are under-12tokenization.
13 of the over- tokenization cases aredifferent tokens of the word blywn (Eng.
billion) asthe initial b in the words was treated as apreposition while it is an original part of the word.A closer examination of the errors in thetokenization process reveals that most of the wordswhich are incorrectly tokenized do not occur in thetraining set, or occur there only in the formproduced by the tokenizer.
For example, the wordblywn does not occur in the training set, but theform b+lywn+p occurs in the training set, and thisis the reason the word is tokenized erroneously.Another example is the word bAsm, which isambiguous between a one-token word bAsm (Eng.smiling), and a two-token word, b+Asm (Eng.
inthe name of).
Although the word should betokenized as b+Asm, the word occurs in thetraining set as bAsm, which is a personal name.In fact, only five words in the 30 mis-tokenized words are available in the training set,which means that the tokenizer has a very highaccuracy on known words.
There are yet twoexamples that are worthy of discussion.
The firstone involves suboptimal orthography.
The wordr>smAl (Eng.
capital in the financial sense) is inthe training set but is nonetheless incorrectlytokenized in our experiments because it is writtenas brAsmAl (with the preposition b) but with an alifinstead of the hamza.
The word was thus nottokenized correctly.
The other example involves anerror in the tokenization in the Prague ArabicDependency Treebank.
The word >wjh (Eng.
Igive/address) has been tokenized in the PragueArabic dependency treebank as >wj+h (Eng.
itsutmost/prime), which is not the correcttokenization in this context as the h is part of theword and is not a different token.
The classifier didnonetheless tokenize it correctly but it was countedas wrong in the evaluation since it does not agreewith the PADT gold standard.4.2.StemmingSince stemming involves removing all theinflectional prefixes and suffixes from the words,and since inflectional affixes are not demarcated inthe PADT data set used in the CoNLL sharedtasks, there is no way to know the exact accuracyof the stemming process in that specificexperiment, but since stemming is a by-product ofsegmentation, and since segmentation in generalreaches an accuracy in excess of 98%, stemmingshould be trusted as an accurate process.4.3.Part of speech taggingThe performance of the tagger on gold standarddata with gold standard tokenization is shown intable 2.
The experiment yields an accuracy of96.39% on all tokens.
Known tokens reach anaccuracy of 97.49% while unknown tokens reachan accuracy of 81.48%.
These numbers constitutethe ceiling for accuracy since the real-worldexperiment makes use of automatic tokenization,which definitely leads to lower numbers.Unknown Known Total81.48% 97.49% 96.39%Table 2: Part of speech tagging on gold standardtokenizationWhen we run the experiment usingautomatic tokenization we obtain an accuracy of95.70% which is less than 1% lower than the goldstandard accuracy.
This indicates that part ofspeech tagging has been affected by tokenizationquality.
The drop in quality in part of speechtagging is almost identical to the drop in quality intokenization.While some of the errors made by the partof speech tagger are due to the fact that nouns,adjectives, and proper nouns cannot bedistinguished by any formal features, a largenumber of the nominal class annotation in the goldstandard data can hardly be justified.
For example,the expression  ???????
????????
(Eng.
the EuropeanUnion) is annotated once in the training data asproper noun and adjective, and another time as anoun and adjective.
A similar confusion holds forthe names of the months and the weekdays, whichare sometimes tagged as nouns and sometimes asproper nouns.4.4.
Dependency parsingNow that we have stems, tokens, and part ofspeech tags, we can proceed with the parsingexperiment, the final step and the ultimate goal ofthe preprocessing modules we have introduced sofar.
In order to prepare the training data, we havereplaced the lemmas in the training and testing setswith the stems since we do not have access tolemmas in real-world experiments.
While this13introduces an automatic element in the training set,it guarantees the similarity between the features inthe training set and those in the test set.In order to discover whether the fine-grained POS tagset is necessary, we have run twoparsing experiments using gold standard parts ofspeech with stems instead of lemmas, but withoutany of the linguistic features included in the goldstandard: the first experiment has the two distinctpart of speech tags and the other one has only thecoarse-grained part of speech tags.
Table 3 outlinesthe results.LAS UAS LACPOS+POS 72.54% 82.92% 84.04%CPOS 73.11% 83.31% 84.39%CoNLL2007 74.75% 84.21% 84.21%Table 3: effect of fine-grained POSAs can be seen from table 3, using two partof speech tagsets harms the performance of thedependency parser.
While the one-tag dependencyparser obtains a Labeled Accuracy Score of73.11%, the number goes down to 72.54% whenwe used the fine-grained part of speech set.
InUnlabeled Attachment Score, the one tag parserachieves an accuracy of 83.31% compared to82.92% on two tag parser.
The same is also true forLabel Accuracy Score as the numbers go downfrom 84.39% when using only one tagset comparedto 84.04% when using two tagsets.
This means thatthe fine-grained tagset is not needed to performreal world parsing.
We have thus decided to usethe coarse-grained tagset in the two positions of thepart of speech tags.
We can also see that thissetting produces results that are 1.64% lower thanthose of the Single MaltParser results reported inthe CoNLL 2007 shared task in terms of LabeledAccuracy Score.
The difference can be attributedto the lack of linguistic features, vocalization, andthe use of stems instead of lemmas.
The LAS of73.11% now constitutes the upper bound for realworld experiments where also parts of speech andtokens have to be obtained automatically (sincevocalization has been removed, linguistic featureshave been removed, and lemmas have beenreplaced with automatic stems).
It should be notedthat our experiments, with the complete set of goldstandard features, achieve higher results than thosereported in the CoNLL 2007 shared task: a LAS of74.77 (here) versus a LAS of 74.75 (CoNLL,2007).
This may be attributed to the change of theparser since we use the 1.3.1 version whereas theparser used in the 2007 shared task was the 0.4version.Using the settings above, we have run anexperiment to parse the test set, which is nowautomatic in terms of tokenization, lemmatization,and part of speech tags, and in the absence of thelinguistic features that enrich the gold standardtraining and test sets.
Table 4 presents the resultsof this experiment.Automatic Gold StandardLAS 63.10% 73.11%UAS 72.19% 83.31%LA 82.61% 84.39%Table 4: Automatic dependency parsing experimentThe LAS drops more than 10 percentagepoints from 73.11 to 63.10.
This considerable dropin accuracy is expected since there is a mismatchin the tokenization which leads to mismatch in thesentences.
The 30 errors in tokenization affect 21sentences out of a total of 129 in the test set.
Whenwe evaluate the dependency parsing output on thecorrectly tokenized sentences only, we obtainmuch better results (shown in Table 5).
LabeledAttachment Score on correctly tokenized sentencesis 71.56%, Unlabeled Attachment Score 81.91%,and Label Accuracy Score is 83.22%.
Thisindicates that no good quality parsing can beobtained if there are problems in the tokenization.A drop of a half percent in the quality oftokenization causes a drop of ten percentage pointsin the quality of parsing, whereas automatic POStags and stemming, and the lack of linguisticfeatures do not cause the same negative effect.Correctly-tokenizedSentencesIncorrectly-TokenizedSentencesLAS 71.56% 33.60%UAS 81.91% 38.32%LA 83.22% 80.49%Table 5: Dependency parsing Evaluation on Correctlyvs.
Incorrectly Tokenized Sentences14While correctly tokenized sentences yieldresults that are not extremely different from thoseusing gold standard information, and the drop inaccuracy in them can be attributed to thedifferences introduced through stemming andautomatic parts of speech as well as the absence ofthe linguistic features, incorrectly tokenizedsentences show a completely different picture asthe Labeled Attachment Score now plummets to33.6%, which is 37.96 percentage points belowthat on correctly tokenized sentences.
TheUnlabeled Attachment Score also drops from81.91% in correctly tokenized sentences to 38.32%on incorrectly tokenized sentences with adifference of 43.59 percentage points.Error AnalysisConsidering the total number of errors, out of the5124 tokens in the test set, there are 1425 headerrors (28%), and 891 dependency errors (17%).
Inaddition, there are 8% of the tokens in which boththe dependency and the head are incorrectlyassigned by the parser.
The POS tag with thelargest percentage of head errors is the Adverb (D)with an error rate of 57%, followed by Preposition(P) at 34%, and Conjunctions at 34%.
Thepreposition and conjunction errors are commonamong all experiments: those with gold standardand those with automatic information.
Theseresults also show that assigning the correct head ismore difficult than assigning the correctdependency.
This is reasonable since some tokenswill have specific dependency types.
Also, whilethere are a limited number of dependency relations,the number of potential heads is much larger.If we look at the lexicon and examine thetokens in which most errors occur, we can see oneconjunction and five prepositions.
The conjunctionw (Eng.
and) tops the list, followed by thepreposition l (Eng.
for, to), followed by thepreposition fy (Eng.
in), then the preposition b(Eng.
with), then the preposition ElY (Eng.
on),and finally the preposition mn (Eng.
from, of).
Weconclude this section by examining a very shortsentence in which we can see the effect oftokenization on dependency parsing.
Table 6 is asentence that has an instance of incorrecttokenization.Arabic  ?????????
?????????
???????????
????
?????
?????
???
???
?English The American exceptional aid to Egypt is a billion  dollarsuntil March.Buckwalter (Gold StandardTokenization)AlmsAEdAt Al>mrykyp AlAstvnA}yp l mSr blywn dwlArHtY |*ArBuckwalter (Automatic Tokenization) AlmsAEdAt Al>mrykyp AlAstvnA}yp l mSr b lywn dwlArHtY |*ArTable 6: A sentence showing the effect of tokenizationThe sentence has 8 words one of whichcomprises two tokens.
The word lmSr comprises apreposition l, and the proper noun mSr (Eng.Egypt).
The tokenizer succeeds in splitting theword into two tokens, but it fails on the one-tokenword blywn (Eng.
billion) and splits it into twotokens b and lywn.
The word is ambiguousbetween blywn (Eng.
one billion) and b+lywn(Eng.
in the city of Lyon), and since the secondsolution is much more frequent in the training set,it is the one incorrectly selected by the tokenizer.This tokenization decision leads to an ill-alignment between the gold standard sentence andthe automatic one as the gold standard has 8 tokenswhile the automatically produced one has 9.
Thisthus affects the POS tagging decisions as blywn,which in the gold standard is a NOUN, has beennow tagged as b/PREPOSITION andlywn/PROPER_NOUN.
This has also affected theassignment of heads and dependency relations.While blywn is a predicate dependent on the rootof the sentence, it has been annotated as twotokens: b is a preposition dependent on the subject,and lywn is an attribute dependent on b.Using the Penn TagsSo far, we have used only the POS tags of thePADT, and have not discussed the possibility ofusing the Penn Arabic Treebank.
The difference isthat the PADT tags are basic while the ATB oneshave detailed representations of inflections.
While15the word AlmtHdp is given the tag ADJ in thePADT, it is tagged asDET+ADJ+FEMININE_SINGULAR_MARKERin the ATB.
Table 7 shows the effect of using thePenn tagset with the gold standard full-featureddataset in three different experiments as comparedwith the PADT tagset:(1) The original Unvocalized Experimentwith the full set of features and goldstandard components.
The Penn tagset isnot used in this experiment, and it isprovided for reference purposes only.
(2) Unvocalized experiment with Penn tags asCPOS tags.
In this experiment, the Penntagset is used instead of the coarse grainedPOS tagset, while the fine-grained postagset remains unchanged.
(3) Using Penn tags as fine grained POS tags,while the CPOS tags remain unchanged.
(4) Using the Penn POS tags in bothpositions.In the four experiments, the only featuresthat change are the POS and CPOS features.Experiment LAS UASUnvocalized Original 74.16% 83.53%Using Penn Tags as CPOStags74.12% 83.43%Using Penn tags as POS 72.40% 81.79%Using Penn tags in bothpositions69.63% 79.33%Table 7: Using the ATB tagset with the PADT datasetAs can be seen from Table 7, in all threecases the Penn tagset produces lower results thanthe PADT tagset.
The reason for this may be thatthe tagset is automatic in both cases, and theperfect accuracy of the PADT tags helps theclassifier embedded in the MaltParser parser tochoose the correct label and head.
The results alsoshow that when we use the Penn tagset as theCPOS tagset, the results are almost no differentfrom the gold standard PADT tagset (74.12% vs.74.16%).
The fact that the Penn tagset does notharm the results encourages the inclusion of thePenn tags as CPOS tags in the automaticexperiments that have been used throughout thischapter.
The worst results are those obtained byusing the Penn tags in both positions (POS andCPOS).Using the Penn tagset with the reducedexperiments, those without the linguistic features,gives a different picture from that in the fullstandard experiments, as detailed in table 8.Experiment LAS UASReduced with both PADTtags72.54% 82.92%Reduced with Penn tags asCPOS73.09% 83.16%Reduced with Penn tags asCPOS and automatictokenization63.11% 72.38%Table 8: Including the Penn full tagset in the reducedexperimentsWhile the Penn tagset does not helpimprove parsing accuracy with the full-featuredparsing experiments, it helps with the reducedexperiments.
While the experiment without thePenn tags score an LAS of 72.54%, replacing theCPOS tags in this experiment with the Penn tagsetraises the accuracy to 73.09%, with an increase of0.55%.
This may be due to the fact that the fulltagset gives more information that helps the parser.The increase is not as noticeable in the automatictokenization experiment where the accuracyminimally changes from 63.10% to 63.11%.Effect of VocalizationWe have stated in the methodology section that weuse unvocalized data since naturally occurringArabic is hardly vocalized.
While this is areasonable approach, it is worth checking the effectof vocalization on dependency parsing.
Table 9presents the results of vocalization effect in threeexperiments: (a) All the gold standard featureswith vocalization.
This is the experiment reportedin the literature on Arabic dependency parsing inCoNLL (2007), (b) All the gold standard featureswithout the vocalization, (c) All gold standardfeatures except for vocalization which isautomatic, and (d) the automatic experiment withautomatic vocalization.
The vocalizer in the latter 216experiments is trained on the PADT.
The TIMBLmemory-based learner is used in the experiment.The best results are obtained with the IB1algorithm with similarity computed as weightedoverlap,.
Relevance weights are computed withgain ratio, and the number of k nearest neighbors isset to 1.
The vocalizer has an accuracy of 93.8% onthe PADT test set.Experiment LAS UASFully Gold StandardVocalized74.77% 84.09%Fully Gold StandardUnvocalized74.16% 83.53%Full-featured with automaticvocalization74.43% 83.88%Completely automatic (withautomatic vocalization)63.11% 72.19%Completely automaticwithout vocalization63.11% 72.38%Table 9: Vocalization Effect on DependencyParsingAs can be seen from Table 9, goldstandard vocalization with gold standard featuresproduces the best results (LAS: 74.77%) followedby the same settings, but with automaticvocalization with a LAS of 74.43%, thenunvocalized gold standard with a LAS of 74.16%.The fact that even automatic vocalization producesbetter results than unvocalized text given the sameconditions, in spite of a token error rate of 6.2%,may be attributed to the ability of vocalization todisambiguate text even when it is not perfect.
Wecan also notice that the LAS for the Automaticexperiment is the same whether or not vocalizationis used.
This indicates that vocalization, in spite ofits imperfections, does  not harm performance,although it also does not help the parser.Tokenization sets a ceiling for parsing accuracy.5.
ConclusionWe have presented an experiment in real worlddependency parsing of Arabic using the same data,algorithm and settings used in the CoNLL (2007)shared task on dependency parsing.
The real worldexperiment included performing tokenization,stemming, and part of speech tagging of the databefore it was passed to MaltParser.Tokenization was performed using thememory-based segmenter/tokenizer/stemmer and itreached an accuracy of 99.34% on the CoNLL2007 test set.
We performed stemming rather thanlemmatization due to the many problems anddifficulties involved in obtaining the lemmas.Part of speech tagging scored 96.39% onall tokens on gold standard tokenization, but theaccuracy dropped to 95.70% on automatic tokens.We also found that using the coarse grained POStagset alne yielded better results than using it incombination with the fine-grained POS tagset.The tokens, stems, and CPOS tags werethen fed into the dependency parser, but thelinguistic features were not since it was notfeasible to obtain these automatically.
The parseryielded a Labeled Accuracy Score of 63.10%,more than 10% below the accuracy obtained onwhen all the components are gold standard.
Themain reason behind the accuracy drop is thetokenization module, since tokenization isresponsible for creating the nodes that carrysyntactic functions.
Since this process was notperfect, many nodes were wrong, and the rightheads were missing.
When we evaluated the parseron correctly tokenized sentences, we obtained aLabeled Accuracy Score of 71.56%.
On incorrectlytokenized sentences, however, the LAS score dropsto 33.60%.We have also found that the full tagset ofthe Penn Arabic Treebank improves parsing resultsminimally in the automatic experiments, but not inthe gold standard experiments.Vocalization does not help in the realworld experiment unlike in the gold standard one.These results show that tokenization is themajor hindrance to obtaining high quality parsingin Arabic.
Arabic computational linguistics shouldthus focus on ways to perfect tokenization, or try tofind ways to parsing without having to performtokenization.AcknowledgmentWe would like to thank Joakim Nivre for hishelpful answers to our questions concerningMaltParser.17ReferencesBuchholz, Sabine and Marsi, Erwin (2006).
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of the 10th Conference on ComputationalNatural Language Learning (CoNLL), pages 149?164.Daelemans, Walter;  Zavrel, Jakub; van der Sloot, Koand van den Bosch, Antal (2007).
TiMBL: Tilburgmemory based learner ?
version 6.1 ?
reference guide.Technical Report ILK 07-09, Induction of LinguisticKnowledge, Computational Linguistics, TilburgUniversity.Daelemans, Walter,; Zavrel, Jakub; an den Bosch,Antal, and van der Sloot, Ko (2007).
MBT: Memory-Based Tagger- Version 3.1.
Reference Guide.
TechnicalReport ILK 07-09, Induction of Linguistic Knowledge,Computational Linguistics, Tilburg University.Haji?, Jan; Smr?, Otakar; Zem?nek, Petr; ?naidauf, Jan,and Be?ka, Emanuel (2004).
Prague ArabicDependency Treebank: Development in Data and Tools.In Proceedings of the EMLAR International Conferenceon Arabic Language Resources and Tools, pages 110-117, Cairo, Egypt, September 2004.Hall, Johan; Nilsson, Jens; Nivre, Joakim; Eryigit,G?lsen; Megyesi, Be?ta; Nilsson, Mattias and Saers,Markus (2007).
Single Malt or Blended?
A Study inMultilingual Parser Optimization.
In Proceedings of theCoNLL Shared Task Session of EMNLP-CoNLL 2007,933-939.Maamouri, Mohamed and Bies, Ann (2004) Developingan Arabic Treebank: Methods, Guidelines, Procedures,and Tools.
In Proceedings of the Workshop onComputational Approaches to Arabic Script-basedLanguages, COLING 2004, Geneva, August 28, 2004.Marton, Yuval; Habash, Nizar; and Rambow, Owen(2010).
Improving Arabic Dependency Parsing withLexical and Inflectional Morphological Features.
InProceddings of The FirstWorkshop on StatisticalParsing of Morphologically Rich Languages (SPMRL2010), LA, California.McDonald, Ryan; Lerman, Kevin and Pereira, Fernando(2006).
Multilingual dependency analysis with a two-stage discriminative parser.
CoNLLX shared task onmultilingual dependency parsing.
In Proceedings of the10th Conference on Computational Natural LanguageLearningNivre, Joakim; Hall, Jonathan; Nilsson, Jens; Eryigit,G?lsen and Marinov, Svetsolav (2006).
LabeledPseudo-Projective Dependency Parsing with SupportVector Machines.
In Proceedings of the TenthConference on Computational Natural LanguageLearning (CoNLL)Nivre, Joakim; Hall, Johan; K?bler, Sandra; McDonald,Ryan; Nilsson, Jens; Riedel, Sebastian, and Yuret,Deniz.
(2007).
The CoNLL 2007 shared task ondependency parsing.
In Proceedings of the CoNLLShared Task of EMNLP-CoNLL 2007, pages 915?932Owen, Jonathan.
The Arabic Grammatical Tradition.
InHetzron, Robert (ed.)
(1997).
The Semitic Languages.Routledge, London.18
