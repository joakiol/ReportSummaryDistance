Proceedings of the Fifth Law Workshop (LAW V), pages 82?91,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsA scaleable automated quality assurance technique for semanticrepresentations and proposition banksK.
Bretonnel CohenComputational Bioscience ProgramU.
of Colorado School of MedicineDepartment of LinguisticsUniversity of Colorado at Boulderkevin.cohen@gmail.comLawrence E. HunterComputational Bioscience ProgramU.
of Colorado School of Medicinelarry.hunter@ucdenver.eduMartha PalmerDepartment of LinguisticsUniversity of Colorado at Bouldermartha.palmer@colorado.eduAbstractThis paper presents an evaluation of an auto-mated quality assurance technique for a typeof semantic representation known as a pred-icate argument structure.
These representa-tions are crucial to the development of an im-portant class of corpus known as a proposi-tion bank.
Previous work (Cohen and Hunter,2006) proposed and tested an analytical tech-nique based on a simple discovery proce-dure inspired by classic structural linguisticmethodology.
Cohen and Hunter applied thetechnique manually to a small set of repre-sentations.
Here we test the feasibility of au-tomating the technique, as well as the abilityof the technique to scale to a set of seman-tic representations and to a corpus many timeslarger than that used by Cohen and Hunter.We conclude that the technique is completelyautomatable, uncovers missing sense distinc-tions and other bad semantic representations,and does scale well, performing at an accu-racy of 69% for identifying bad representa-tions.
We also report on the implications ofour findings for the correctness of the seman-tic representations in PropBank.1 IntroductionIt has recently been suggested that in addition tomore, bigger, and better resources, we need a sci-ence of creating them (Palmer et al, Download dateDecember 17 2010).The corpus linguistics community has arguablybeen developing at least a nascent science of anno-tation for years, represented by publications such as(Leech, 1993; Ide and Brew, 2000; Wynne, 2005;Cohen et al, 2005a; Cohen et al, 2005b) that ad-dress architectural, sampling, and procedural issues,as well as publications such as (Hripcsak and Roth-schild, 2005; Artstein and Poesio, 2008) that addressissues in inter-annotator agreement.
However, thereis not yet a significant body of work on the subjectof quality assurance for corpora, or for that matter,for many other types of linguistic resources.
(Mey-ers et al, 2004) describe three error-checking mea-sures used in the construction of NomBank, and theuse of inter-annotator agreement as a quality controlmeasure for corpus construction is discussed at somelength in (Marcus et al, 1993; Palmer et al, 2005).However, discussion of quality control for corpora isotherwise limited or nonexistent.With the exception of the inter-annotator-agreement-oriented work mentioned above, none ofthis work is quantitative.
This is a problem if ourgoal is the development of a true science of annota-tion.Work on quality assurance for computational lex-ical resources other than ontologies is especiallylacking.
However, the body of work on quality as-surance for ontologies (Kohler et al, 2006; Ceusterset al, 2004; Cimino et al, 2003; Cimino, 1998;Cimino, 2001; Ogren et al, 2004) is worth consider-ing in the context of this paper.
One common themein that work is that even manually curated lexical re-sources contain some percentage of errors.The small size of the numbers of errors uncoveredin some of these studies should not be taken as asignificance-reducing factor for the development ofquality assurance measures for lexical resources?82rather, the opposite: as lexical resources becomelarger, it becomes correspondingly more difficult tolocate errors in them.
Finding problems in a veryerrorful resource is easy; finding them in a mostlycorrect resource is an entirely different challenge.We present here an evaluation of a methodol-ogy for quality assurance for a particular type oflexical resource: the class of semantic representa-tion known as a predicate argument structure (PAS).Predicate argument structures are important in thecontext of resource development in part becausethey are the fundamental annotation target of theclass of corpus known as a proposition bank.
Muchof the significance claim for this work comes fromthe significance of proposition banks themselves inrecent research on natural language processing andcomputational lexical semantics.
The impact ofproposition banks on work in these fields is sug-gested by the large number of citations of just thethree publications (Kingsbury and Palmer, 2002;Kingsbury et al, 2002; Palmer et al, 2005)?at thetime of writing, 290, 220, and 567, respectively.
Ad-ditional indications of the impact of PropBank onthe field of natural language processing include itsuse as the data source for two shared tasks ((Car-reras and Ma`rquez, 2005)).The methodology consists of looking for argu-ments that never coo?ccur with each other.
In struc-tural linguistics, this property of non-coo?ccurrenceis known as complementary distribution.
Comple-mentary distribution occurs when two linguistic el-ements never occur in the same environment.
Inthis case, the environment is defined as any sen-tence containing a given predicate.
Earlier workshowed a proof-of-concept application to a small setof rolesets (defined below) representing the potentialPAS of 34 biomedical predicates (Cohen and Hunter2006).
The only inputs to the method are a set ofrolesets and a corpus annotated with respect to thoserolesets.
Here, we evaluate the ability of the tech-nique to scale to a set of semantic representations137 times larger (4,654 in PropBank versus 34 inCohen and Hunter?s pilot project) and to a corpusabout 1500 times larger (1M words in PropBank ver-sus about 680 in Cohen and Hunter?s pilot project)than that considered in previous work.
We also usea set of independent judges to assess the technique,where in the earlier work, the results were only as-sessed by one of the authors.Novel aspects of the current study include:?
Investigating the feasibility of automating thepreviously manual process?
Scaling up the size of the set of semantic repre-sentations evaluated?
Scaling up the size of the corpus against whichthe representations are evaluated?
Using independent judges to assess the predic-tions of the method1.1 DefinitionsFor clarity, we define the terms roleset, frame file,and predicate here.
A roleset is a 2-tuple of a sensefor a predicate, identified by a combination of alemma and a number?e.g., love.01?and a set of in-dividual thematic roles for that predicate?e.g., Arg0lover and Arg1 loved.
A frame file is the set of allrolesets for a single lemma?e.g., for love, the role-sets are love.01 (the sense whose antonym is hate)and love.02, the ?semi-modal?
sense in whether itbe melancholy or gay, I love to recall it (Austen,1811).
Finally, we refer to sense-labelled predicates(e.g.
love.01) as predicates in the remainder of thepaper.PropBank rolesets contain two sorts of thematicroles: (core) arguments and (non-core) adjuncts.
Ar-guments are considered central to the semantics ofthe predicate, e.g.
the Arg0 lover of love.01.
Ad-juncts are not central to the semantics and can occurwith many predicates; examples of adjuncts includenegation, temporal expressions, and locations.In this paper, the arity of a roleset is determinedby its count of arguments, disregarding adjuncts.1.2 The relationship between observedargument distributions and variouscharacteristics of the corpusThis work is predicated on the hypothesis that argu-ment distributions are affected by goodness of the fitbetween the argument set and the actual semanticsof the predicate.
However, the argument distribu-tions that are observed in a specific data set can beaffected by other factors, as well.
These include atleast:?
Inflectional and derivational forms attested inthe corpus83?
Sublanguage characteristics?
Incidence of the predicate in the corpusA likely cause of derivational effects on observeddistributions is nominalization processes.
Nomi-nalization is well known for being associated withthe omission of agentive arguments (Koptjevskaja-Tamm, 1993).
A genre in which nominalization isfrequent might therefore show fewer coo?ccurrencesof Arg0s with other arguments.
Since PropBankdoes not include annotations of nominalizations, thisphenomenon had no effect on this particular study.Sublanguage characteristics might also affect ob-served distributions.
The sublanguage of recipeshas been noted to exhibit rampant deletions of def-inite object noun phrases both in French and in En-glish, as has the sublanguage of technical manualsin English.
(Neither of these sublanguages havebeen noted to occur in the PropBank corpus.
Thesublanguage of stock reports, however, presumablydoes occur in the corpus; this sublanguage has beennoted to exhibit distributional subtleties of predi-cates and their arguments that might be relevant tothe accuracy of the semantic representations in Prop-Bank, but the distributional facts do not seem to in-clude variability in argument coo?ccurrence so muchas patterns of argument/predicate coo?ccurrence (Kit-tredge, 1982).
)Finally, incidence of the predicate in the corpuscould affect the observed distribution, and in partic-ular, the range of argument coo?ccurrences that areattested: the lower the number of observations of apredicate, the lower the chance of observing any twoarguments together, and as the number of argumentsin a roleset increases, the higher the chance of failingto see any pair together.
That is, for a roleset withan arity of three and an incidence of n occurrencesin a corpus, the likelihood of never seeing any twoof the three arguments together is much lower thanfor a roleset with an arity of six and an incidence ofn occurrences in the corpus.
The number of obser-vations required in order to be able to draw conclu-sions about the observed argument distributions withsome degree of confidence is an empirical question;prior work (Cohen and Hunter 2006) suggests thatas few as ten tokens can be sufficient to uncover er-roneous representations for rolesets with an arity offour or less, although that number of observationsof one roleset with an arity of four showed multiplenon-coo?ccurring arguments that were not obviouslyindicative of problems with the representation (i.e.,a false positive finding).Besides the effects of these aspects of the corpuscontents on the observed distributions, there are alsoa number of theoretical and practical issues in thedesign and construction of the corpus (as distinctfrom the rolesets, or the distributional characteris-tics of the contents) which have nontrivial implica-tions for the methodology being evaluated here.
Inparticular, the implications of the argument/adjunctdistinction, of the choice of syntactic representation,and of annotation errors are all discussed in Sec-tion 4.
Note that we are aware that corpus-basedstudies generally yield new lexical items and us-ages any time a new corpus is introduced, so wedo not make the naive assumption that PropBankwill give complete coverage of all coo?ccurring argu-ments, and in fact our evaluation procedure took thisinto account explicitly, as described in Section 2.3.2 Materials and Methods2.1 MaterialsWe used Rev.
1.0 of the PropBank I corpus, and theassociated framesets in the frames directory.2.2 Methods2.2.1 Determining the distribution ofarguments for a rolesetIn determining the possible coo?ccurring argumentpairs for a roleset, we considered only arguments,not adjuncts.
As we discuss in Section 4.1, thisis a non-trivial decision with potential implicationsfor the ability of the algorithm to detect problem-atic representations in general, and with implicationsfor PropBank in particular.
The rationale behind thechoice to consider only arguments is that our goalis to evaluate the representation of the semantics ofthe predicates, and that by definition, the PropBankarguments are essential to defining that semantics,while by definition, the adjuncts are not.In the first processing step, for each roleset, weused the corresponding framefile as input and gen-erated a look-up table of the possible argumentpairs for that predicate.
For example, the predi-cate post.01 has the three arguments Arg0, Arg1, and84Arg2; we generated the set {<Arg0, Arg1>, <Arg0,Arg2>, <Arg1, Arg2>} for it.In the second processing step, we iterated over allannotations in the PropBank corpus, and for each to-ken of each predicate, we extracted the complete setof arguments that occurred in association with thattoken.
We then constructed the set of coo?ccurring ar-guments for that annotation, and used it to incrementthe counts of each potential argument pair for thepredicate in question.
For example, the PropBankannotation for Oils and fats also did well, postinga 5.3% sales increase (wsj/06/wsj 0663.mrg)contains an Arg0 and an Arg1, so we incrementedthe count for that argument pair by 1; it contains noother argument pairs, so we did not increment thecounts for <Arg0, Arg2> or <Arg1, Arg2>.The output of this step was a table with the countof occurrence of every potential pair of argumentsfor every roleset; members of pairs whose count waszero were then output as arguments in complemen-tary distribution.
For example, for post.01, the pairs<Arg0, Arg2> and <Arg1, Arg2> never occurred,even as traces, so the arguments Arg0 and Arg2 arein complementary distribution for this predicate, asare the arguments Arg1 and Arg2.To manipulate the data, we used Scott Cotton?sJava API, with some extensions, which we docu-mented in the API?s Javadoc.2.3 Determining the goodness of rolesetsexhibiting complementary distributionIn (Cohen and Hunter, 2006), determinations of thegoodness of rolesets were made by pointing out thedistributional data to the corpus creators, showingthem the corresponding data, and reaching consen-sus with them about the appropriate fixes to the rep-resentations.
For this larger-scale project, one of thegoals was to obtain goodness judgements from com-pletely independent third parties.Towards that end, two judges with experience inworking with PropBank were assigned to judge thepredictions of the algorithm.
Judge 1 had two yearsof experience, and Judge 2 had four years of expe-rience.
The judges were then given a typology ofclassification to assign to the predicates: good, bad,and conditionally bad.
The definitions of these cate-gories, with the topology of the typology, were:?
Good: This label is assigned to predicates thatthe algorithm predicted to have bad representa-tions, but that are actually good.
They are falsepositives for the method.?
Not good: (This label was not actually as-signed, but rather was used to group the fol-lowing two categories.)?
Bad: This label is assigned to predicatesthat the algorithm predicted to have badrepresentations and that the judges agreedwere bad.
They are true positives for themethod.?
Conditionally bad: This label is assignedto predicates that the algorithm predictedto have bad representations and that thejudges agreed were bad based on the ev-idence available in PropBank, but that thejudges thought might be good based onnative speaker intiution or other evidence.In all of these cases, the judges did suggestchanges to the representations, and theywere counted as not good, per the typol-ogy, and are also true positives.Judges were also asked to indicate whether badrepresentations should be fixed by splitting predi-cates into more word senses, or by eliminating ormerging one or more arguments.We then took the lists of all predicted bad predi-cates that appeared at least 50, 100, or 200 times inthe PropBank corpus.
These were combined into asingle list of 107 predicates and randomized.
Thejudges then split the list into halves, and each judgeexamined half of the list.
Additionally, 31 predi-cates, or 29% of the data set, were randomly selectedfor double annotation by both judges to assess inter-judge agreement.
Judges were shown both the predi-cates themselves and the sets of non-coo?ccurring ar-guments for each predicate.3 Results3.1 AccuracyThe overall results were that out of 107 predicates,33 were judged GOOD, i.e.
were false positives.44 were judged BAD and 30 were judged CONDI-TIONAL, i.e.
were true positives.
This yields a ratioof 2.24 of true positives to false positives: the pro-85Table 1: Ratios of BAD plus CONDITIONAL to GOODfor the pooled judgements as broken down by arityArity Ratio3 1.294 1.475 4.06 8.07 None foundcedure returns about two true positives for every onefalse positive.
Expressed in terms of accuracy, thiscorresponds to 69% for correctly labelling true pos-itives.We broke down the data by (1) arity of the role-set, and (2) minimum number of observations of arole set.
This allowed us to test whether predictivepower decreased as arity increased, and to test thedependency of the algorithm on the minimum num-ber of observations; we suspected that it might beless accurate the fewer the number of observations.Table 1 shows the ratios of true positives to falsepositives, broken down by arity.
The data confirmsthat the algorithm is effective at finding bad repre-sentations, with the number of true positives out-numbering the number of false positives at everyarity.
This data is also important because it allowsus to test a hypothesis: is it the case that predictivepower becomes worse as arity increases?
As the ta-ble shows, the ratio of true positives to false posi-tives actually increases as the arity of the predicateincreases.
Therefore, the data is consistent with thehypothesis that not only does the predictive power ofthe algorithm not lessen as arity increases, but ratherit actually becomes greater.Table 2 shows the ratios of true positives to falsepositives again, this time broken down by minimumnumber of occurrences of the predicates.
Again, thedata confirms that the algorithm is effective at find-ing bad representations?it returns more bad repre-sentations than good representations at every level ofminimum number of observations.
This data is alsoimportant because it allows us to test the hypothe-sis of whether or not predictive power of the algo-rithm decreases with the minimum number of obser-vations.
As we hypothesized, it does show that thepredictive power decreases as the minimum numberTable 2: Ratios of BAD plus CONDITIONAL to GOODfor the pooled judgements as broken down by minimumnumber of observationsratioMinimum 50 1.88Minimum 100 2.63Minimum 200 2.63of observations decreases, with the ratio of true pos-itives to false positives dropping from 2.63 with aminimum of 200 or 100 observations to 1.88 with aminimum of 50 observations.
However, the ratio oftrue positives to false positives remains close to 2:1at every level.3.2 Suggested fixes to the representationsOf the 74 true positives, the judges felt that 17 ofthe bad representations should be fixed by splittingthe predicate into multiple senses.
For the 57 re-maining true positives, the judges felt that an argu-ment should be removed from the representation orconverted to an adjunct.
This demonstrates that themethod is applicable both to the problem of reveal-ing missing sense distinctions and to the problem ofidentifying bad arguments.3.3 ScalabilityThe running time was less than one and a half min-utes for all 4,654 rolesets on the 1-million-word cor-pus.3.4 Inter-judge agreementA subset of 31 predicates was double-annotated bythe two judges to examine inter-judge agreement.The judges then examined the cases on which theyinitially disagreed, and came to a consensus wherepossible.
Initially, the judges agreed in 63.3% of thecases, which is above chance but not the 80% agree-ment that we would like to see.
The judges then wentthrough a reconciliation process.
They were able tocome to a consensus in all cases.3.5 Putting the results in contextTo help put these results in context, we give here thedistribution of arities in the PropBank rolesets andthe minimum number of observations of each in thePropBank corpus.86Table 3: Distribution of arities by percentage and bycount in the 4,654 PropBank rolesets.Arity percentage (count)0 0.28% (13)1 (Arg0) 1551 (Arg1) 1461 (all) 6.5% (301)2 45.14% (2,101)3 37.02% (1,723)4 7.05% (328)5 3.5% (163)6 0.5% (24)7 0.0002% (1)Total 100% (4,654)Table 3 shows the distribution of arities in thePropBank rolesets.
It distinguishes between non-ergatives and ergatives (although for the purposeof calculating percentages, they are combined intoone single-arity group).
The mode is an arity of 2:45.14% of all rolesets (2,101/4,654) have an arity of2.
3 is a close second, with 37.02% (1,723/4,654).
(The single roleset with an arity of seven is notch.02,with a gloss of ?move incrementally.?
)Table 4 gives summary statistics for the occur-rence of complementary distribution, showing thedistribution of rolesets in which there were at leastone argument pair in complementary distributionand of the total number of argument pairs in comple-mentary distribution.
Since (as noted in Section 1.2)the incidence of a predicate has a potential effecton the incidence of argument pairs in apparent com-plementary distribution, we display the counts sepa-rately for four cut-offs for the minimum number ofobservations of the predicate: 200, 100, 50, and 10.To further explicate the operation of the discoveryprocedure, we give here some examples of rolesetsthat were found to have arguments in complemen-tary distribution.3.5.1 accept.01Accept.01 is the only roleset for the lemma ac-cept.
Its sense is take willingly.
It has four argu-ments:?
Arg0 acceptorTable 4: Summary statistics: counts of predicates withat least one argument pair in complementary distributionand of total argument pairs in complementary distributionfor four different minimum numbers of observations ofthe predicates.Minimum observations Predicates Argument pairs200 29 69100 58 12550 107 26810 328 882?
Arg1 thing accepted?
Arg2 accepted-from?
Arg3 attributeThe predicate occurs 149 times in the corpus.
Thealgorithm found Arg2 and Arg3 to be in complemen-tary distribution.Manual investigation showed the following distri-butional characteristics for the predicate and its ar-guments:?
(Arg0 or Arg1) and Arg2: 5 tokens?
(Arg0 or Arg1) and Arg3: 8 tokens?
Arg2 with neither Arg0 nor Arg1: 0 tokens?
Arg3 with neither Arg0 nor Arg1: 0 tokens?
Arg0 or Arg1 with neither Arg2 nor Arg 3: 136tokensExamination of the 5 tokens in which Arg2coo?ccurred with Arg0 or Arg1 and the 8 tokensin which Arg3 coo?ccurred with Arg0 or Arg1 sug-gested an explanation for the complementary distri-bution of arguments Arg2 and Arg3.
When Arg2appeared, the sense of the verb seemed to be oneof physical transfer: Arg2 coo?ccurred with Arg1slike substantial gifts (wsj 0051.mrg) and a $3million payment (wsj 2071.mrg).
In contrast,when Arg3 appeared, the sense was not one ofphysical transfer, but of some more metaphoricalsense?Arg3 coo?ccurred with Arg1s like the war(wsj 0946.mrg) and Friday?s dizzying 190-pointplunge (wsj 2276.mrg).
There is no accept.02;creating one with a 3-argument roleset including thecurrent Arg3 seems warranted.
Keeping the Arg3for accept.01 might be warranted, as well, but prob-ably as an adjunct (to account for usages like Johnaccepted it as a gift.
)873.5.2 affect.01Affect.01 is one of two senses for the lemma af-fect.
Its sense is have an effect on.
It has three argu-ments:?
Arg0 thing affecting?
Arg1 thing affected?
Arg2 instrumentThe predicate occurs 149 times in the corpus.
Thealgorithm found Arg0 and Arg2, as well as Arg1 andArg2, to be in complementary distribution.Manual investigation revealed that in fact, Arg2never appears in the corpus at all.
Presumably, ei-ther Arg0 and Arg2 should be merged, or?morelikely?Arg2 should not be an argument, but ratheran adjunct.3.6 Incidental findings3.6.1 Mistakes uncovered in frame filesIn the process of calculating the set of possibleargument pairs for each predicate in the PropBankframe files, we found a roleset that erroneously hadtwo Arg1s.
The predicate in question was pro-scribe.01.
The roles in the frame file were:?
Arg0 causer?
Arg1 thing proscribed?
Arg1 proscribed fromIt was clear from the annotations in the exam-ple sentence that the ?second?
Arg1 was intended tobe an Arg2: [The First AmendmentArg0] proscribes[the governmentArg1] from [passing laws abridgingthe right to free speechArg2].3.6.2 Unlicensed arguments used in the corpusWe found eighteen tokens in the corpus that wereannotated with argument structures that were not li-censed by the roleset for the corresponding predi-cate.
For example, the predicate zip.01 has onlya single argument in its semantic representation?Arg0, described as entity in motion.
However, thecorpus contains a token of zip.01 that is annotatedwith an Arg0 and an Arg1.4 Discussion/Conclusions4.1 The effect of the argument/adjunctdistinctionThe validity and usefulness of the distinction be-tween arguments and adjuncts is an ongoing con-troversy in biomedical computational lexical se-mantics.
The BioProp project (Chou et al, 2006;Tsai et al, 2006) makes considerable use of ad-juncts, essentially identically to PropBank; however,most biomedical PAS-oriented projects have rela-tively larger numbers of arguments and lesser useof adjuncts (Wattarujeekrit et al, 2004; Kogan et al,2005; Shah et al, 2005) than PropBank.
Overall,one would predict fewer non-coo?ccurring argumentswith a set of representations that made a strongerdistinction between arguments and adjuncts; over-all arity of rolesets would be smaller (see above forthe effect of arity on the number of observations re-quired for a predicate), and the arguments for such arepresentation might be more ?core?
to the seman-tics of the predicate, and might therefore be lesslikely to not occur overall, and therefore less likelyto not coo?ccur.4.2 The effect of syntactic representation onobserved argument distributionsThe original work by Cohen and Hunter assumed avery simple, and very surface, syntactic representa-tion.
In particular, there was no representation oftraces.
In contrast, PropBank is built on TreebankII, which does include representation of traces, andarguments can, in fact, be filled by traces.
This couldbe expected to reduce the number of tokens of appar-ently absent arguments, and thereby the number ofnon-coo?occurring arguments.
This doesn?t seem tohave had a strong enough effect to interfere with theability of the method to uncover errors.4.3 The effect of arityThe mode for distribution of arities in the Prop-Bank framefiles was 2 (see Table 3).
In contrast, themodes for distribution of rolesets with at least oneargument pair in complementary distribution acrossarities and for distribution of argument pairs in com-plementary distribution across arities was 4 or 5for the full range of minimum observations of thepredicates from 200 to 10 (data omitted for space).88This supports the initial assumption that higher-aritypredicates are more likely to have argument pairs incomplementary distribution?see Section 1.2 above.One aspect of a granular analysis of the data isworth pointing out with respect to the effects of ar-ity: as a validation check, note that for all arities,the number of predicates and the number of argu-ment pairs rises as the minimum required number oftokens of the predicate in the corpus goes down.4.4 ConclusionsThe goals of this study were to investigate the au-tomatability and scalability of a technique for PASquality assurance that had previously only beenshown to work for a small lexical resource anda small corpus, and to use it to characterize thequality of the shallow semantic representations inthe PropBank framefiles.
The evaluation procedurewas found to be automatable: the process of find-ing argument pairs in complementary distribution isachievable by running a single Java application.
Inaddition, the use of a common representation for ar-gument sets in a framefile and argument sets in aPropBank annotation enabled the fortuitous discov-ery of a number of problems in the framefiles and inthe corpus (see Section 3.6) as a side-effect of appli-cation of the technique.The process was also found to scale well, witha running time of less than one and a half minutesfor a set of 4,654 rolesets and a 1-million-word cor-pus on a moderately priced laptop; additionally, theresource maintainer?s efforts can easily be focussedtowards the most likely and the most prevalent errorsources by adjusting the minimum number of obser-vations required before reporting a case of comple-mentary distribution.
The process was also found tobe able to identify missing sense distinctions and toidentify bad arguments.In addition to our findings regarding the qualityassurance technique, a granular breakdown of theerrors found by the algorithm by arity and mini-mum number of observations (data not shown due tospace) allows us to estimate the number of errors inthe PropBank framefiles.
A reasonable upper-boundestimate for the number of errorful rolesets is thenumber of predicates that were observed at least 10times and were found to have at least one pair of ar-guments in complementary distribution (the bottomrow of Table 4), adjusted by the accuracy of the tech-nique that we reported in Section 3.1, i.e.
0.69.
Thisyields a worst-case scenario of (0.69*328)/4,654rolesets, or 4.9% of the rolesets in PropBank, be-ing in need of revision.
The best-case scenariowould assume that we can only draw conclusionsabout the predicates with high numbers of observa-tions and high arity, again adjusted downward forthe accuracy of the technique; taking 5 or more argu-ments as high arity, this yields a best-case scenarioof (0.69*17)/4,654 rolesets, or 0.3% of the rolesetsin PropBank, being in need of revision.
A differentsort of worst-case scenario assumes that the majorproblem in maintaining a proposition bank is not fix-ing inadequate representations, but finding them.
Onthis assumption, the problematic representations arethe ones with small numbers of tokens and low ar-ity.
Taking 3 or fewer arguments as low arity yields aworst-case scenario of 99/4,654 rolesets (no adjust-ment for accuracy required), or 2.13% of the rolesetsin PropBank, being essentially uncharacterizable asto the goodness of their semantic representation1.Besides its obvious role in quality assurance forproposition banks, there may be other uses for thistechnique, as well.
The output of the technique mayalso be useful in sense grouping and splitting and indetecting metaphorical uses of verbs (e.g.
the acceptexample).
As the PropBank model is extended to anincreasingly large set of languages (currently Ara-bic, Basque, Catalan, Chinese, Hindi, Korean, andRussian), the need for a quality assurance mecha-nism for proposition banks?both to ensure the qual-ity of their contents, and to assure funding agenciesthat they are evaluatable?will only grow larger.ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596.Jane Austen.
1811.
Sense and Sensibility.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introductionto the CoNLL-2005 shared task: semantic role label-1The situation is arguably actually somewhat worse thanthis, since it does not take into account predicates which occurfewer than ten times in the corpus; however, there is a reason-able counter-argument that those predicates are too rare for anyindividual roleset to have a large impact on the overall goodnessof the resource.89ing.
In Proceedings of the 9th conference on computa-tional natural language learning, pages 152?164.Werner Ceusters, Barry Smith, Anand Kumar, andChristoffel Dhaen.
2004.
Mistakes in medical on-tologies: where do they come from and how can theybe detected?
In D.M.
Pisanelli, editor, Ontologies inmedicine: proceedings of the workshop on medical on-tologies, pages 145?163.
IOS Press.Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su,Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu.
2006.A semi-automatic method for annotating a biomedi-cal proposition bank.
In Proceedings of the workshopon frontiers in linguistically annotated corpora 2006,pages 5?12.
Association for Computational Linguis-tics.J.J.
Cimino, H. Min, and Y. Perl.
2003.
Consistencyacross the hierarchies of the UMLS Semantic Networkand Metathesaurus.
Journal of Biomedical Informat-ics, 36:450?461.James J. Cimino.
1998.
Auditing the Unified MedicalLanguage System with semantic methods.
Journal ofthe American Medical Informatics Association, 5:41?51.James J. Cimino.
2001.
Battling Scylla and Charybdis:the search for redundancy and ambiguity in the 2001UMLS Metathesaurus.
In Proc.
AMIA annual sympo-sium, pages 120?124.K.
Bretonnel Cohen and Lawrence Hunter.
2006.
Acritical revew of PASBio?s argument structures forbiomedical verbs.
BMC Bioinformatics, 7(Suppl.
3).K.
B. Cohen, Lynne Fox, Philip V. Ogren, and LawrenceHunter.
2005a.
Corpus design for biomedical naturallanguage processing.
In Proceedings of the ACL-ISMBworkshop on linking biological literature, ontologiesand databases, pages 38?45.
Association for Compu-tational Linguistics.K.
Bretonnel Cohen, Lynne Fox, Philip V. Ogren, andLawrence Hunter.
2005b.
Empirical data on corpusdesign and usage in biomedical natural language pro-cessing.
In AMIA 2005 symposium proceedings, pages156?160.George Hripcsak and Adam S. Rothschild.
2005.
Agree-ment, the F-measure, and reliability in information re-trieval.
Journal of the American Medical InformaticsAssociation, 12(3):296?298.Nancy Ide and Chris Brew.
2000.
Requirements, tools,and architectures for annotated corpora.
In Proc.
dataarchitectures and software support for large corpora,pages 1?5.Paul Kingsbury and Martha Palmer.
2002.
From Tree-Bank to PropBank.
In Proceedings of the LREC.Paul Kingsbury, Martha Palmer, and Mitch Marcus.2002.
Adding semantic annotation to the Penn Tree-Bank.
In Proceedings of the Human Language Tech-nology Conference.Richard Kittredge.
1982.
Variation and homogene-ity of sublanguages.
In Richard Kittredge and JohnLehrberger, editors, Sublanguage: studies of languagein restricted semantic domains, pages 107?137.Yacov Kogan, Nigel Collier, Serguei Pakhomov, andMichael Krauthammer.
2005.
Towards semantic rolelabeling & IE in the medical literature.
In AMIA 2005Symposium Proceedings, pages 410?414.Jacob Kohler, Katherine Munn, Alexander Ruegg, An-dre Skusa, and Barry Smith.
2006.
Quality controlfor terms and definitions in ontologies and taxonomies.BMC Bioinformatics, 7(1).Maria Koptjevskaja-Tamm.
1993.
Nominalizations.Routledge.Geoffrey Leech.
1993.
Corpus annotation schemes.
Lit-erary and linguistic computing, pages 275?281.Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Adam Meyers, Ruth Reeves, Catherine Macleod, RachelSzekely, Veronika Zielinska, Brian Young, and RalphGrishman.
2004.
Annotating noun argument structurefor NomBank.
In Proceedings of Language Resourcesand Evaluation, LREC.Philip V. Ogren, K. Bretonnel Cohen, George K.Acquaah-Mensah, Jens Eberlein, and LawrenceHunter.
2004.
The compositional structure of GeneOntology terms.
Pacific Symposium on Biocomputing,pages 214?225.Martha Palmer, Paul Kingsbury, and Daniel Gildea.2005.
The Proposition Bank: an annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106.Martha Palmer, Stephanie Strassel, and Randee Tangi.Download date December 17, 2010.
Historical devel-opment and future directions in data resource develop-ment.
In MINDS 2006?2007.Parantu K. Shah, Lars J. Jensen, Ste?phanie Boue?, andPeer Bork.
2005.
Extraction of transcript diversityfrom scientific literature.
PLoS Computational Biol-ogy, 1(1):67?73.Richard Tzong-Han Tsai, Wen-Chi Chou, Yu-Chun Lin,Cheng-Lung Sung, Wei Ku, Ying-Shan Su, Ting-YiSung, and Wen-Lian Hsu.
2006.
BIOSMILE: adapt-ing semantic role labeling for biomedical verbs: anexponential model coupled with automatically gener-ated template features.
In Proceedings of the BioNLPWorkshop on Linking Natural Language Processingand Biology, pages 57?64.
Association for Computa-tional Linguistics.90Tuangthong Wattarujeekrit, Parantu K. Shah, and NigelCollier.
2004.
PASBio: predicate-argument structuresfor event extraction in molecular biology.
BMC Bioin-formatics, 5(155).Martin Wynne, editor.
2005.
Developing linguistic cor-pora: a guide to good practice.
David Brown BookCompany.91
