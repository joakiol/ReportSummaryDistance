Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 937?944,Sydney, July 2006. c?2006 Association for Computational LinguisticsStochastic Discourse Modeling in Spoken Dialogue SystemsUsing Semantic Dependency GraphsJui-Feng Yeh, Chung-Hsien Wu and Mao-Zhu YangDepartment of Computer Science and Information EngineeringNational Cheng Kung UniversityNo.
1, Ta-Hsueh Road, Tainan, Taiwan, R.O.C.
{jfyeh, chwu, mzyang}@csie.ncku.edu.twAbstractThis investigation proposes an approachto modeling the discourse of spoken dia-logue using semantic dependency graphs.By characterizing the discourse as a se-quence of speech acts, discourse modelingbecomes the identification of the speechact sequence.
A statistical approach isadopted to model the relations betweenwords in the user?s utterance using thesemantic dependency graphs.
Dependencyrelation between the headword and otherwords in a sentence is detected using thesemantic dependency grammar.
In orderto evaluate the proposed method, a dia-logue system for medical service is devel-oped.
Experimental results show that therates for speech act detection and task-completion are 95.6% and 85.24%, re-spectively, and the average number ofturns of each dialogue is 8.3.
Comparedwith the Bayes?
classifier and the Partial-Pattern Tree based approaches, we obtain14.9% and 12.47% improvements in ac-curacy for speech act identification, re-spectively.1 IntroductionIt is a very tremendous vision of the computertechnology to communicate with the machine us-ing spoken language (Huang et al, 2001; Allen atal., 2001).
Understanding of spontaneous languageis arguably the core technology of the spoken dia-logue systems, since the more accurate informationobtained by the machine (Higashinaka et al, 2004),the more possibility to finish the dialogue task.Practical use of speech act theories in spoken lan-guage processing (Stolcke et al 2000; Walker  andPassonneau 2001; Wu et al, 2004) have given bothinsight and deeper understanding of verbal com-munication.
Therefore, when considering thewhole discourse, the relationship between thespeech acts of the dialogue turns becomes ex-tremely important.
In the last decade, several prac-ticable dialogue systems (McTEAR, 2002), such asair travel information service system, weatherforecast system, automatic banking system, auto-matic train timetable information system, and theCircuit-Fix-it shop system, have been developed toextract the user?s semantic entities using the se-mantic frames/slots and conceptual graphs.
Thedialogue management in these systems is able tohandle the dialogue flow efficaciously.
However, itis not applicable to the more complex applicationssuch as ?Type 5: the natural language conversa-tional applications?
defined by IBM (Rajesh andLinda, 2004).
In Type 5 dialog systems, it is possi-ble for the users to switch directly from one ongo-ing task to another.
In the traditional approaches,the absence of precise speech act identificationwithout discourse analysis will result in the failurein task switching.
The capability for identifying thespeech act and extracting the semantic objects byreasoning plays a more important role for the dia-log systems.
This research proposes a semanticdependency-based discourse model to capture andshare the semantic objects among tasks that switchduring a dialog for semantic resolution.
Besides937acoustic speech recognition, natural language un-derstanding is one of the most important researchissues, since understanding and application restric-tion on the small scope is related to the data struc-tures that are used to capture and store themeaningful items.
Wang et al (Wang et al, 2003)applied the object-oriented concept to provide anew semantic representation including semanticclass and the learning algorithm for the combina-tion of context free grammar and N-gram.Among these approaches, there are two essentialissues about dialogue management in natural lan-guage processing.
The first one is how to obtainthe semantic object from the user?s utterances.
Thesecond is a more effective speech act identificationapproach for semantic understanding is needed.Since speech act plays an important role in the de-velopment of dialogue management for dealingwith complex applications, speech act identifica-tion with semantic interpretation will be the mostimportant topic with respect to the methods used tocontrol the dialogue with the users.
This paperproposes an approach integrating semantic de-pendency graph and history/discourse informationto model the dialogue discourse (Kudo and Ma-tsumoto, 2000; Hacioglu et al, 2003; Gao and Su-zuki, 2003).
Three major components, such assemantic relation, semantic class and semantic roleare adopted in the semantic dependency graph(Gildea and Jurasfky, 2002; Hacioglu and Ward,2003).
The semantic relations constrain the wordsense and provide the method for disambiguation.Semantic roles are assigned when the relation es-tablished among semantic objects.
Both semanticrelations and roles are defined in many knowledgeresources or ontologies, such as FrameNet (Bakeret al, 2004) and HowNet  with 65,000 concepts inChinese and close to 75,000 English equivalents, isa bilingual knowledge-base describing relationsbetween concepts and relations between the attrib-utes of concepts with ontological view (Dong andDong 2006).
Generally speaking, semantic class isdefined as a set with the elements that are usuallythe words with the same semantic interpretation.Hypernyms that are superordinate concepts of thewords are usually used as the semantic classes justlike the Hypernyms of synsets in WordNet(http://www.cogsci.princeton.edu/~wn/) or defini-tions of words?
primary features in HowNet.
Be-sides, the approach for understanding tries to findthe implicit semantic dependency between the con-cepts and the dependency structure between con-cepts in the utterance are also taken intoconsideration.
Instead of semantic frame/slot, se-mantic dependency graph can keep more informa-tion for dialogue understanding.2 Semantic Dependency GraphSince speech act theory is developed to extract thefunctional meaning of an utterance in the dialogue(Searle, 1979), discourse or history can be definedas a sequence of speech acts,1 2 1{ , ,.... , }t t tH SA SA SA SA?= , and accordingly thespeech act theory can be adopted for discoursemodeling.
Based on this definition, the discourseanalysis in semantics using the dependency graphstries to identify the speech act sequence of the dis-course.
Therefore, discourse modeling by means ofspeech act identification considering the history isshown in Equation (1).
By introducing the hiddenvariable Di, representing the i-th possible depend-ency graph derived from the word sequence W.The dependency relation, rk , between word wk andheadword wkh is extracted using HowNet and de-noted as  ( , )k kh kDR w w r?
.
The dependency graphwhich is composed of a set of dependency relationsin the word sequence W is defined as1 1 1 2 2 2 1 1 ( 1)( ) { ( , ), ( , ),..., ( , )}i i ii h h m m m hD W DR w w DR w w DR w w?
?
?= .The probability of hypothesis SAt given word se-quence W and history Ht-1 can be described inEquation (1).
According to the Bayes?
rule, thespeech act identification model can be decomposedinto two components, ( )1| , ,t tiP SA D W H ?
and( )1| , tiP D W H ?
, described in the following.
( )( )( ) ( )* 111 1arg ax | ,arg ax , | ,arg ax | , , | ,ttitit tSAt tiSA Dt t ti iSA DSA m P SA W Hm P SA D W Hm P SA D W H P D W H???
?=== ??
?where SA* and SAt are the most probable speechact and the potential speech act at the t-th dialogueturn, respectively.
W={w1,w2,w3,?,wm} denotes theword sequence extracted from the user?s utteancewithout considering the stop words.
Ht-1 is the his-tory representing the previous t-1 turns.
(1)9382.1 Speech act identification using semanticdependency with discourse analysisIn this analysis, we apply the semantic dependency,word sequence, and discourse analysis to the iden-tification of speech act.
Since Di is the i-th possibledependency graph derived from word sequence W,speech act identification with semantic dependencycan be simplified as Equation (2).
( ) ( )1 1| , , | ,t t t ti iP SA D W H P SA D H?
??
(2)According to Bayes?
rule, the probability ( )1| ,t tiP SA D H ?
can be rewritten as:( ) ( ) ( )( ) ( )111, || ,, |lt t tit titi l lSAP D H SA P SAP SA D HP D H SA P SA???=?
(3)As the history is defined as the speech act se-quence, the joint probability of Di and Ht-1 giventhe speech act SAt can be expressed as Equation (4).For the problem of data sparseness in the trainingcorpus, the probability, ( )1 2 1, , ,..., |t tiP D SA SA SA SA?
, is hard to obtain andthe speech act bi-gram model is adopted for ap-proximation.
( )( )( )11 2 11, |, , ,..., |, |t tit tit tiP D H SAP D SA SA SA SAP D SA SA???=?
(4)For the combination of the semantic and syntacticstructures, the relations defined in HowNet areemployed as the dependency relations, and the hy-pernym is adopted as the semantic concept accord-ing to the primary features of the words defined inHowNet.
The headwords are decided by the algo-rithm based on the part of speech (POS) proposedby Academia Sinica in Taiwan.
The probabilitiesof the headwords are estimated according to theprobabilistic context free grammar (PCFG) trainedon the Treebank developed by Sinica (Chen et al,2001).
That is to say, the headwords are extractedaccording to the syntactic structure and the de-pendency graphs are constructed by the semanticrelations defined in HowNet.
According to previ-ous definition with independent assumption andthe bigram smoothing of the speech act model us-ing the back-off procedure, we can rewrite Equa-tion (4) into Equation (5).
( )111111, |( ( , ), | )(1 ) ( ( , ) | )t timi t tk k khkmi tk k khkP D SA SAP DR w w SA SAP DR w w SA????
?= ?== +???
(5)where  ?
is the mixture factor for normalization.According to the conceptual representation of theword, the transformation function, ( )f ?
, trans-forms the word into its hypernym defined as thesemantic class using HowNet.
The dependencyrelation between the semantic classes of two wordswill be mapped to the conceptual space.
Also thesemantic roles among the dependency relations areobtained.
On condition that tSA , 1tSA ?
and the re-lations are independent, the equation becomes111( ( , ), | )( ( ( ), ( )), | )( ( ( ), ( )) | ) ( | )i t tk k khi t tk k khi t t tk k khP DR w w SA SAP DR f w f w SA SAP DR f w f w SA P SA SA???
?=(6)The conditional probability,( ( ( ), ( )) | )i tk k khP DR f w f w SA  and 1( | )t tP SA SA?
, areestimated according to Equations (7) and (8), re-spectively.
( ( ( ), ( )) | )( ( ), ( ), , )( )i tk k khtk kh ktP DR f w f w SAC f w f w r SAC SA=(7)11 ( , )( | )( )t tt ttC SA SAP SA SAC SA??
=                 (8)where ( )C ?
represents the number of events in thetraining corpus.
According to the definitions inEquations (7) and (8), Equation (6) becomes prac-ticable.9392.2 Semantic dependency analysis usingword sequence and discourseAlthough the discourse can be expressed as thespeech act sequence 1 2 1{ , ,.... , }t t tH SA SA SA SA?= ,the dependency graph iD  is determined mainly byW, but not 1tH ?
.
The probability that defines se-mantic dependency analysis using the words se-quence and discourse can be rewritten in thefollowing:( )11 2 1| ,( | , , ,..., )( | )tit tiiP D W HP D W SA SA SAP D W??
?=?
(9)and( , )( | )( )iiP D WP D WP W= `                          (10)Seeing that several dependency graphs can be gen-erated from the word sequence W, by introducingthe hidden factor Di, the probability ( )P W  can bethe sum of the probabilities ( , )iP D W as Equation(11).
: ( )( ) ( , )i iiD yield D WP W P D W== ?
(11)Because Di is generated from W, Di is the suffi-cient to represent W in semantics.
We can estimatethe joint probability ( , )iP D W  only from the de-pendency relations Di.
Further, the dependencyrelations are assumed to be independent with eachother and therefore simplified as11( , ) ( ( , ))mii k k khkP D W P DR w w?==?
(12)The probability of the dependency relation be-tween words is defined as that between the con-cepts defined as the hypernyms of the words, andthen the dependency rules are introduced.
Theprobability ( | ( ), ( ))k k khP r f w f w  is estimated fromEquation (13).
( ( , ))( ( ( ), ( )))( | ( ), ( ))( , ( ), ( ))( ( ), ( ))ik k khik k khk k khk k khk khP DR w wP DR f w f wP r f w f wC r f w f wC f w f w?==(13)According to Equations (11), (12) and (13), Equa-tion (10) is rewritten as the following equation.111: ( ) 1111: ( ) 1( ( , ))( | )( ( , ))( , ( ), ( ))( ( ), ( ))( , ( ), ( ))( ( ), ( ))i ii imik k khki mik k khD yield D W kmk k khk k khmk k khD yield D W k k khP DR w wP D WP DR w wC r f w f wC f w f wC r f w f wC f w f w?=?= =?=?= ===??
???
?
(14)where function, ( )f ?
, denotes the transformationfrom the words to the corresponding semanticclasses.Figure 1.
Speech acts corresponding to multiple services in the medical domain9403 ExperimentsIn order to evaluate the proposed method, a spokendialogue system for medical domain with multipleservices was investigated.
Three main services:registration information service, clinic informationservice, and FAQ information service are used.This system mainly provides the function of on-line registration.
For this goal, the health educationdocuments are provided as the FAQ files.
And theinference engine about the clinic information ac-cording to the patients?
syndromes is constructedaccording to a medical encyclopedia.
An exampleis illustrated as figure 2:Figure 2 An example of dialog12 Speech acts are defined and shown in Figure 1.Every service corresponds to the 12 speech actswith different probabilities.The acoustic speech recognition engine embed-ded in dialog system based on Hidden MarkovModels (HMMs) was constructed.
The feature vec-tor is parameterized on 26 MFCC coefficients.
Thedecoding strategy is based on a classical Viterbialgorithm.
The evaluation results by the charactererror rate (CER) for a Chinese speech recognitionsystem is 18.3 percent and the vocabulary size ofthe language is 25,132.3.1 Analysis of corpusThe training corpus was collected using the on-linerecording from National Cheng Kung UniversityHospital in the first phase and the Wizard-of-Ozmethod in the second phase.
Totally, there are1,862 dialogues with 13,986 sentences in the cor-pus.
The frequencies of the speech acts used in thesystem are shown in Figure 3.4.102.709.1113.9612.8113.4611.5610.719.764.704.352.750 2 4 6 8 10 12 14OthersRegistration revisionDr.
's inforamtionTimeGreetingClinic informationRegistrationFAQDr.
and ClinicConfirmatin (others)Confirmation (clinic)Cancel registrationFigure 3 Frequencies for each speech actThe number of dialogue turns is also important tothe success of the dialogue task.
According to theobservation of the corpus, we can find that the dia-logues with more than 15 turns usually failed tocomplete the dialogue, that is to say, the commonground cannot be achieved.
These failed dialogueswere filtered out from the training corpus beforeconducting the following experiments.
The distri-bution of the number of turns per dialogue isshown in Figure 4.0501001502002503003501 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16Length (Turns)FrequencyFigure 4.
The distribution of the number of turnsper dialogue3.2 Precision of speech act identification re-lated to the corpus sizeSystem:????????
?Good Afternoon.
May I help you?User:????????
?Yes, I should be diagnosed by doctor.System:????????
?Which department would you like to make an appoint-ment for?User:????
?I do not know.System:????????
?What symptoms are with you?User:??????????????
?I am with a sore throat and a fever.
I ached all over, too.System:??????????
?Well, how long have you had these symptoms?User:????
?For several days.System:????????
?My suggestion is that you may visit the Internal Medi-cine.User:?????????
?What?s the visiting schedule of the Internal Medicine?System:?????????????????????????
?You can check the Services Schedule and decide a con-venient time for you.
The Available time for now is....941The size of the training corpus is crucial to thepracticability of the proposed method.
In this ex-periment, we analyze the effect of the number ofsentences according to the precision rate of thespeech act using the semantic dependency graphswith and without the discourse information.
Fromthe results, the precision rates for speech act identi-fication achieved 95.6 and 92.4 percentages for thetraining corpus containing 10,036 and 7,012 sen-tences using semantic dependency graphs with andwithout history, respectively.
This means that se-mantic dependency graph with discourse outper-forms that without discourse, but more trainingdata are needed to include the discourse for speechact identification.
Fig.
5 shows the relationshipbetween the speech act identification rate and thesize of the training corpus.
From this figure, wecan find that more training sentences for the se-mantic dependency graph with discourse analysisare needed than that without discourse.
This im-plies discourse analysis plays an important role inthe identification of the speech act.3.3 Performance analysis of semantic depend-ency graphTo evaluate the performance, two systems weredeveloped for comparison.
One is based on theBayes?
classifier (Walker et al, 1997), and theother is the use of the partial pattern tree (Wu et al,2004) to identify the speech act of the user?s utter-ances.
Since the dialogue discourse is defined as asequence of speech acts.
The prediction of speechact of the new input utterance becomes the coreissue for discourse modeling.
The accuracy forspeech act identification is shown in Table 1.According to the observation of the results, se-mantic dependency graphs obtain obvious5062.57587.51001 2 3 4 5 6 7 8 9 10 11 12 13 14Size of corpus(the number of sentence, in thousands)Speechact identificationrate(%)semantic dependency graph withdiscourse analysissemantic dependency graphwithout discourse analysisFigure 5.
The relation between the speech act iden-tification rate and the size of training corpusimprovement compared to other approaches.
Thereason is that not only the meanings of the wordsor concepts but also the structural information andthe implicit semantic relation defined in the knowl-edge base are needed to identify the speechact.Besides, taking the discourse into considerationwill improve the prediction about the speech act ofthe new or next utterance.
This means the dis-course model can improve the accuracy of thespeech act identification, that is to say, discoursemodeling can help understand the user?s desiredintension especially when the answer is very short.Semantic dependency graph    Speech actWith discourse analysis Without discourse analysisPPT Bayes?ClassifierClinic information(26 sentences)100(26)96.1(25)88(23)92(24)Dr.?s information(42 sentences)97(41)92.8(39)66.6(28)92.8(39)Confirmation(others)(42 sentences)95(40)95(40)95(40)95(40)Others(14 sentences)57.1(8)50(7)43(6)38(5)FAQ(13 sentences)70(9)53.8(7)61.5(8)46(6)Clinic information(135 sentences)98.5(133)96.2(130)91.1(123)93.3(126)Time(38)94.7(36)89.4(34)97.3(37)92.1(35)Registration(75)100(75)100(75)86.6(65)86.6(65)Cancel registration(10)90(9)80(8)60(6)80(8)Average Precision 95.6 92.4 85 88.1Table 1 The accuracy for speech act identification942For example, the user may only say ?yes?
or ?no?for confirmation.
The misclassification in speechact will happen due to the limited information.However, it can obtain better interpretation byintroducing the semantic dependency relations aswell as the discourse information.To obtain the single measurement, the averageaccuracy for speech act identification is shown inTable 1.
The best approach is the semantic de-pendency graphs with the discourse.
This meansthe information of the discourse can help speechact identification.
And the semantic dependencygraph outperforms the traditional approach due tothe semantic analysis of words with their corre-sponding relations.The success of the dialog lies on the achievementof the common ground between users and ma-chine which is the most important issue in dia-logue management.
To compare the semanticdependency graph with previous approaches, 150individuals who were not involved in the devel-opment of this project were asked to use the dia-logue system to measure the task success rate.
Tofilter out the incomplete tasks, 131 dialogs wereemployed as the analysis data in this experiment.The results are listed in Table 2.SDG1 SDG2 PPT Bayes?Taskcompletionrate87.285.579.480.2Number ofturns onaverage8.38.710.410.5SDG1 :With discourse analysis, SDG2 :Without discourseTable 2 Comparisons on the Task completion rateand the number of dialogue turns between differ-ent approachesWe found that the dialogue completion rate andthe average length of the dialogs using the de-pendency graph are better than those using theBayes?
classifier and partial pattern tree approach.Two main reasons are concluded: First, depend-ency graph can keep the most important informa-tion in the user?s utterance, while in semanticslot/frame approach, the semantic objects notmatching the semantic slot/frame are generallyfiltered out.
This approach is able to skip the repetition or similar utterances to fill the same infor-mation in different semantic slots.
Second, thedependency graph-based approach can provide theinference to help the interpretation of the user?sintension.For semantic understanding, correct interpretationof the information from the user?s utterances be-comes inevitable.
Correct speech act identificationand correct extraction of the semantic objects areboth important issues for semantic understandingin the spoken dialogue systems.
Five main catego-ries about medical application, clinic information,Dr.
?s information, confirmation for the clinic in-formation, registration time and clinic inference,are analyzed in this experiment.SDG PPT Bayes?Clinic infor-mation 95.0 89.5 90.3Dr.
?s infor-mation 94.3 71.7 92.4Confirmation(Clinic) 98.0 98.0 98.0Clinic97.3 74.6 78.6Time97.6 97.8 95.5SDG:With discourse analysisTable 3 Correction rates for semantic object ex-tractionAccording to the results shown in Table 3, theworst condition happened in the query for theDr.
?s information using the partial pattern tree.The mis-identification of speech act results in theun-matched semantic slots/frames.
This conditionwill not happen in semantic dependency graph,since the semantic dependency graph alwayskeeps the most important semantic objects accord-ing to the dependency relations in the semanticdependency graph instead of the semantic slots.Rather than filtering out the unmatched semanticobjects, the semantic dependency graph is con-structed to keep the semantic relations in the ut-terance.
This means that the system can preservemost of the user?s information via the semanticdependency graphs.
We can observe the identifi-cation rate of the speech act is higher for the se-mantic dependency graph than that for the partialpattern tree and Bayes?
classifier as shown in Ta-ble 3.9434 ConclusionThis paper has presented a semantic depend-ency graph that robustly and effectively deals witha variety of conversational discourse informationin the spoken dialogue systems.
By modeling thedialogue discourse as the speech act sequence, thepredictive method for speech act identification isproposed based on discourse analysis instead ofkeywords only.
According to the corpus analysis,we can find the model proposed in this paper ispracticable and effective.
The results of the ex-periments show the semantic dependency graphoutperforms those based on the Bayes?
rule andpartial pattern trees.
By integrating discourseanalysis this result also shows the improvementobtained not only in the identification rate ofspeech act but also in the performance for seman-tic object extraction.AcknowledgementsThe authors would like to thank the NationalScience Council, Republic of China, for its finan-cial support of this work, under Contract No.
NSC94-2213-E-006-018.ReferencesJ.
F. Allen, D. K. Byron, D. M. Ferguson, L. Galescu,and A. Stent.
2001.
Towards Conversational Hu-man-Computer Interaction.
AI Magazine.C.
F. Baker, C. J. Fillmore, and J.
B. Lowe.
1998.
TheBerkeley FrameNet Project.
In Proceedings ofCOLING/ACL.
86-90K.
J. Chen, C. R. Huang, F.Y.
Chen, C. C. Luo, M. C.Chang, and C.J.
Chen.
2001.
Sinica Treebank: De-sign Criteria, representational issues and immple-mentation.
In Anne Abeille, editor, Building andUsing Syntactically Annotated Corpora.
Kluwer.
29-37Z.
Dong and Q. Dong.
2006.
HowNet and the computa-tion of meaning.
World Scientific Publishing Co Inc.J.
Gao, and H. Suzuki.
2003.
Unsupervised learning ofdependency structure for language modeling.
InProceedings of ACL 2003, 521-528.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistics, 28(3).245?288.K.
Hacioglu, S. Pradhan, W. Ward, J. Martin, and D.Jurafsky.
2003.
Shallow semantic parsing usingsupport vector machines.
Technical Report TR-CSLR-2003-1, Center for Spoken Language Re-search, Boulder, Colorado.K.
Hacioglu and W. Ward.
2003.
Target word detectionand semantic role chunking using support vectormachines.
In HLT-03.R.
Higashinaka, N. Miyazaki, M. Nakano, and K. Ai-kawa.
2004.
Evaluating Discourse Understanding inSpoken Dialogue Systems.
ACM Transactions onSpeech and Language Processing (TSLP), Volume 1,1-20.X.
Huang, A. Acero, and H.-W. Hon.
2001.
SpokenLanguage Proceeding.
Prentice-Hall,Inc.T.
Kudo and Y. Matsumoto.
2000.
Japanese Depend-ency Structure Analysis Based on Support VectorMachines.
In Proceedings of the EMLNP.
18?25M.
F. McTEAR.
2002.
Spoken Dialogue Technology:Enabling the Conversational User Interface.
ACMComputer Surveys, Vol 34, No.
1,  90-169..B. Rajesh, and B. Linda.
2004.
Taxonomy of speech-enabled applications (http://www106.ibm.com/de-veloperworks/wireless/library/wi-tax/)J. Searle.
1979.
Expression and Meaning: Studies in theTheory of Speech Acts.
New York, Cambridge Uni-versity Press.A.
Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,D.
Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema, and M. Meteer.
2000.
Dialogue act model-ing for automatic tagging and recognition of conver-sational speech.
Computational Linguistics 26(3),339--373.M.
A. Walker, D. Litman, C. Kamm, and A. Abella,1997.
PARADISE: a general framework for evaluat-ing spoken dialogue agents.
In Proceedings of theACL, 271?280M.
Walker  and R. Passonneau.
2001.
DATE: a dia-logue act tagging scheme for evaluation of spokendialogue systems.
In Proceedings of the first inter-national conference on Human language technologyresearch.
1-8.Y.-Y.
Wang and A. Acero.
2003.
Combination of CFGand N-gram Modeling in Semantic Grammar Learn-ing, In Proceedings of the Eurospeech Conference.Geneva, Switzerland.
September 2003.C.-H. Wu, J.-F. Yeh, and M.-J.
Chen.
2004.
SpeechAct Identification using an Ontology-Based PartialPattern Tree.
in Proceedings of ICSLP 2004, Jeju,Korea, 2004.944
