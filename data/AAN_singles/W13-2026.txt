Proceedings of the BioNLP Shared Task 2013 Workshop, pages 178?187,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsExtracting Gene Regulation Networks UsingLinear-Chain Conditional Random Fields and RulesSlavko Z?itnik??
Marinka Z?itnik?
Blaz?
Zupan?
Marko Bajec?
?Faculty of Computer and Information ScienceUniversity of LjubljanaTrz?as?ka cesta 25SI-1000 Ljubljana{name.surname}@fri.uni-lj.si?Optilab d.o.o.Dunajska cesta 152SI-1000 LjubljanaAbstractPublished literature in molecular geneticsmay collectively provide much informa-tion on gene regulation networks.
Ded-icated computational approaches are re-quired to sip through large volumes of textand infer gene interactions.
We propose anovel sieve-based relation extraction sys-tem that uses linear-chain conditional ran-dom fields and rules.
Also, we intro-duce a new skip-mention data represen-tation to enable distant relation extractionusing first-order models.
To account for avariety of relation types, multiple modelsare inferred.
The system was applied to theBioNLP 2013 Gene Regulation NetworkShared Task.
Our approach was rankedfirst of five, with a slot error rate of 0.73.1 IntroductionIn recent years we have witnessed an increas-ing number of studies that use comprehensivePubMed literature as an additional source of in-formation.
Millions of biomedical abstracts andthousands of phenotype and gene descriptions re-side in online article databases.
These representan enormous amount of knowledge that can bemined with dedicated natural language process-ing techniques.
However, extensive biologicalinsight is often required to develop text miningtechniques that can be readily used by biomedi-cal experts.
Profiling biomedical research litera-ture was among the first approaches in disease-gene prediction and is now becoming invaluableto researchers (Piro and Di Cunto, 2012; Moreauand Tranchevent, 2012).
Information from pub-lication repositories was often merged with otherdatabases.
Successful examples of such integra-tion include an OMIM database on human genesand genetic phenotypes (Amberger et al 2011),GeneRIF function annotation database (Osborneet al 2006), Gene Ontology (Ashburner et al2000) and clinical information about drugs in theDailyMed database (Polen et al 2008).
Biomed-ical literature mining is a powerful way to iden-tify promising candidate genes for which abundantknowledge might already be available.Relation extraction (Sarawagi, 2008) can iden-tify semantic relationships between entities fromtext and is one of the key information extrac-tion tasks.
Because of the abundance of publica-tions in molecular biology computational methodsare required to convert text into structured data.Early relation extraction systems typically usedhand-crafted rules to extract a small set of rela-tion types (Brin, 1999).
Later, machine learningmethods were adapted to support the task and weretrained over a set of predefined relation types.
Incases where no tagged data is available, some un-supervised techniques offer the extraction of rela-tion descriptors based on syntactic text properties(Bach and Badaskar, 2007).
Current state-of-the-art systems achieve best results by combining bothmachine learning and rule-based approaches (Xuet al 2012).Information on gene interactions are scatteredin data resources such as PubMed.
The reconstruc-tion of gene regulatory networks is a longstandingbut fundamental challenge that can improve ourunderstanding of cellular processes and molecularinteractions (Sauka-Spengler and Bronner-Fraser,2008).
In this study we aimed at extracting a generegulatory network of the popular model organismthe Bacillus subtilis.
Specifically, we focused onthe sporulation function, a type of cellular differ-entiation and a well-studied cellular function in B.subtilis.We describe the method that we used for ourparticipation in the BioNLP 2013 Gene Regula-tion Network (GRN) Shared Task (Bossy et al2013).
The goal of the task was to retrieve the178genic interactions.
The participants were providedwith manually annotated sentences from researchliterature that contain entities, events and genicinteractions.
Entities are sequences of text thatidentify objects, such as genes, proteins and reg-ulons.
Events and relations are described by type,two associated entities and direction between thetwo entities.
The participants were asked to pre-dict relations of interaction type in the test dataset.
The submitted network of interactions wascompared to the reference network and evaluatedwith Slot Error Rate (SER) (Makhoul et al 1999)SER = (S + I + D)/N that measures the frac-tion of incorrect predictions as the sum of relationsubstitutions (S), insertions (I) and deletions (D)relative to the number of reference relations (N).We begin with a description of related work andthe background of relation extraction.
We thenpresent our extension of linear-chain conditionalrandom fields (CRF) with skip-mentions (Sec.
3).Then we explain our sieve-based system archi-tecture (Sec.
4), which is the complete pipelineof data processing that includes data preparation,linear-chain CRF and rule based relation detectionand data cleaning.
Finally, we describe the resultsat BioNLP 2013 GRN Shared Task (Sec.
6).2 Related WorkThe majority of work on relation extraction fo-cuses on binary relations between two entities.Most often, the proposed systems are evaluatedagainst social relations in ACE benchmark datasets (Bunescu and Mooney, 2005; Wang et al2006).
There the task is to identify pairs of enti-ties and assign them a relation type.
A number ofmachine learning techniques have been used forrelation extraction, such as sequence classifiers,including HMM (Freitag and McCallum, 2000),CRF (Lafferty et al 2001) and MEMM (Kamb-hatla, 2004), and binary classifiers.
The latter mostoftem employ SVM (Van Landeghem et al 2012).The ACE 2004 data set (Mitchell et al 2005)contains two-tier hierarchical relation types.
Thus,a relation can have another relation as an attributeand second level relation must have only atomicattributes.
Therefore, two-tier relation hierarchieshave the maximum height of two.
Wang et al(2006) employed a one-against-one SVM classi-fier to predict relations in ACE 2004 data set usingsemantic features from WordNet (Miller, 1995).The BioNLP 2013 GRN Shared Task aims to de-tect three-tier hierarchical relations.
These rela-tions describe interactions that can have events orother interactions as attributes.
In contrast to pair-wise approach of Wang et al(2006), we extractrelations with sequence classifiers and rules.The same relation in text can be expressedin many forms.
Machine-learning approachescan resolve this heterogeneity by training mod-els on large data sets using a large number offeature functions.
Text-based features can beconstructed through application of feature func-tions.
An approach to overcome low cover-age of different relation forms was proposed byGarcia and Gamallo (2011).
They introduceda lexico-syntactic pattern-based feature functionsthat identify dependency heads and extracts rela-tions.
Their approach was evaluated over two re-lation types in two languages and achieved goodresults.
In our study we use rules to account forthe heterogeneity of relation representation.Generally, when trying to solve a rela-tion extraction task, data sets are tagged us-ing the IOB (inside-outside-beginning) nota-tion (Ramshaw and Marcus, 1995), such that thefirst word of the relation is tagged as B-REL, otherconsecutive words within it as I-REL and all othersas O.
The segment of text that best describes a pre-defined relation between two entities is called a re-lation descriptor.
Li et al(2011) trained a linear-chain CRF to uncover these descriptors.
They alsotransformed subject and object mentions of the re-lations into dedicated values that enabled them tocorrectly predict relation direction.
Additionally,they represented the whole relation descriptor asa single word to use long-range features with afirst-order model.
We use a similar model but pro-pose a new way of token sequence transformationwhich discovers the exact relation and not only thedescriptor.
Banko and Etzioni (2008) used linearmodels for the extraction of open relations (i.e.extraction of general relation descriptors withoutany knowledge about specific target relation type).They first characterized the type of relation ap-pearance in the text according to lexical and syn-tactic patterns and then trained a CRF using thesedata along with synonym detection (Yates and Et-zioni, 2007).
Their method is useful when a fewrelations in a massive corpus are unknown.
How-ever, if higher levels of recall are desired, tradi-tional relation extraction is a better fit.
In thisstudy we therefore propose a completely super-179vised relation extraction method.Methods for biomedical relation extraction havebeen tested within several large evaluation initia-tives.
The Learning language in logic (LLL) chal-lenge on genic interaction extraction (Ne?dellec,2005) is similar to the BioNLP 2013 GRN SharedTask, which contains a subset of the LLL dataset enriched with additional annotations.
Giu-liano et al(2006) solved the task using an SVMclassifier with a specialized local and global con-text kernel.
The local kernel uses only mention-related features such as word, lemma and part-of-speech tag, while the global context kernel com-pares words that appear on the left, between andon the right of two candidate mentions.
To de-tect relations, they select only documents contain-ing at least two mentions and generate(nk)train-ing examples, where n is the number of all men-tions in a document and k is number of mentionsthat form a relation (i.e.
two).
They then predictthree class values according to direction (subject-object, object-subject, no relation).
Our approachalso uses context features and syntactic featuresof neighbouring tokens.
The direction of relationspredicted in our model is arbitrary and it is furtherdetermined using rules.The BioNLP 2011 REL Supporting Shared Taskaddressed the extraction of entity relations.
Thewinning TESS system (Van Landeghem et al2012) used SVMs in a pipeline to detect entitynodes, predict relations and perform some post-processing steps.
They predict relations among ev-ery two mention pairs in a sentence.
Their studyconcluded that the term detection module has astrong impact on the relation extraction module.In our case, protein and entity mentions (i.e.
men-tions representing genes) had already been identi-fied, and we therefore focused mainly on extrac-tion of events, relations and event modificationmentions.3 Conditional Random Fields withSkip-MentionsConditional random fields (CRF) (Lafferty et al2001) is a discriminative model that estimatesjoint distribution p(y|x) over the target sequencey, conditioned on the observed sequence x. Thefollowing example shows an observed sequence xwhere mentions are printed in bold:?Transcription of cheV initiates from asigma D-dependent promoter elementboth in vivo and in vitro, and expressionof a cheV-lacZ fusion is completely de-pendent on sigD.?
1Corresponding sequences xPOS , xPARSE ,xLEMMA contain part-of-speech tags, parse treetokens and lemmas for each word, respectively.Different feature functions fj (Fig.
2), employedby CRF, use these sequences in order to modelthe target sequence y, which also correspondsto tokens in x.
Feature function modelling is anessential part when training CRF.
Selection offeature functions contributes the most to an in-crease of precision and recall when training CRFclassifiers.
Usually these are given as templatesand the final features are generated by scanningthe entire training data set.
The feature functionsused in our model are described in Sec.
3.1.CRF training finds a weight vector w that pre-dicts the best possible (i.e.
the most probable) se-quence y?
given x. Hence,y?
= argmaxyp(y|x,w), (1)where the conditional distribution equalsp(y|x,w) =exp(?mj=1 wj?ni=1 fj(y, x, i))C(x,w).
(2)Here, n is the length of the observed sequence x,m is the number of feature functions and C(x,w)is a normalization constant computed over all pos-sible y.
We do not consider the normalization con-stant because we are not interested in exact targetsequence probabilities.
We select only the targetsequence that is ranked first.y1x1ynxny2x2y3x3Figure 1: The structure of a linear-chain CRFmodel.
It shows an observable sequence x and tar-get sequence y containing n tokens.The structure of a linear-chain CRF (LCRF)model or any other more general graphical modelis defined by references to the target sequence la-bels within the feature functions.
Fig.
1 shows the1The sentence is taken from BioNLP 2013 GRN trainingdata set, article PMID-8169223-S5.180function f(y, x, i):if (yi?1 == O andyi == GENE andxi?1 == transcribes) thenreturn 1elsereturn 0Figure 2: An example of a feature function.
Itchecks if the previous label was Other, the currentis Gene and the previous word was ?transcribes?,returns 1, otherwise 0.structure of the LCRF.
Note that the i-th factorcan depend only on the current and the previoussequence labels yi and yi?1.
LCRF can be effi-ciently trained, whereas exact inference of weightsin CRF with arbitrary structure is intractable dueto an exponential number of partial sequences.Thus, approximate approaches must be adopted.3.1 Data RepresentationThe goal of our task is to identify relations be-tween two selected mentions.
If we process theinput sequences as is, we cannot model the de-pendencies between two consecutive mentions be-cause there can be many other tokens in between.From an excerpt of the example in the previoussection, ?cheV initiates from a sigmaD?, we canobserve the limitation of modelling just two con-secutive tokens.
With this type of labelling it ishard to extract the relationships using a first-ordermodel.
Also, we are not interested in identify-ing relation descriptors (i.e.
segments of text thatbest describe a pre-defined relation); therefore, wegenerate new sequences containing only mentions.Mentions are also the only tokens that can be anattribute of a relation.
In Fig.
3 we show the trans-formation of our example into a mention sequence.The observable sequence x contains sorted en-tity mentions that are annotated.
These annota-tions were part of the training corpus.
The targetsequence y is tagged with the none symbol (i.e.O) or the name of the relationship (e.g.
Interac-tion.Requirement).
Each relationship target tokenrepresents a relationship between the current andthe previous observable mention.The mention sequence as demonstrated in Fig.
3does not model the relationships that exist be-tween distant mentions.
For example, the men-tions cheV and promoter are related by a PromoterOcheVInteraction.Transcriptionsigma DMaster ofpromoterpromoterOcheVInteraction.RequirementsigDPromoter ofFigure 3: A mention sequence with zero skip-mentions.
This continues our example fromSec.
3.of relation, which cannot be identified using onlyLCRF.
Linear model can only detect dependen-cies between two consecutive mentions.
To modelsuch relationships on different distances we gen-erate appropriate skip-mention sequences.
Thenotion of skip-mention stands for the number ofother mentions between two consecutive mentionswhich are not included in a specific skip-mentionsequence.
Thus, to model relationships betweenevery second mention, we generate two one skip-mention sequences for each sentence.
A one skip-mention sequence identifies the Promoter of rela-tion, shown in Fig.
4.OcheVPromoter ofpromoterOsigDFigure 4: A mention sequence with one skip-mention.
This is one out of two generated men-tion sequences with one skip-mention.
The otherconsists of tokens sigmaD and cheV.For every s skip-mention number, we gen-erate s + 1 mention sequences of length dns e.After these sequences are generated, we trainone LCRF model per each skip-mention number.Model training and inference of predictions canbe done in parallel due to the sequence indepen-dence.
Analogously, we generate model-specificskip-mention sequences for inference and get tar-get labellings as a result.
We extract the identifiedrelations between the two mentions and representthem as an undirected graph.Fig.
5 shows the distribution of distances be-1810 1 2 3 4 5 6 7 8 9 11 13 15 17 19BioNLP 2013 GRN datasetrelation Mention distance distributionMention distance between relation argumentsNumberofrelationships020406080462869 73453114 147 4 3 6 2 2 1 2 1 2 1 1Figure 5: Distribution of distances between twomentions connected with a relation.tween the relation mention attributes (i.e.
agentsand targets) in the BioNLP 2013 GRN training anddevelopment data set.
The attribute mention dataconsists of all entity mentions and events.
We ob-serve that most of relations connect attributes ondistances of two and three mentions.To get our final predictions we train CRF mod-els on zero to ten skip-mention sequences.
We usethe same unigram and bigram feature function setfor all models.
These include the following:?
target label distribution,?
mention type (e.g.
Gene, Protein) and ob-servable values (e.g., sigma D) of mentiondistance 4 around current mention,?
context features using bag-of-words match-ing on the left, between and on the right sideof mentions,?
hearst concurrence features (Bansal andKlein, 2012),?
token distance between mentions,?
parse tree depth and path between mentions,?
previous and next lemmas and part-of-speechtags.4 Data Analysis PipelineWe propose a pipeline system combining multi-ple processing sieves.
Each sieve is an indepen-dent data processing component.
The system con-sists of eight sieves, where the first two sievesprepare data for relation extraction, main sievesconsist of linear-chain CRF and rule-based rela-tion detection, and the last sieve cleans the out-put data.
Full implementation is publicly available(https://bitbucket.org/szitnik/iobie).
We use CRF-Suite (http://www.chokkan.org/software/crfsuite)for faster CRF training and inference.First, we transform the input data into a formatappropriate for our processing and enrich the datawith lemmas, parse trees and part-of-speech tags.We then identify additional action mentions whichact as event attributes (see Sec.
4.3).
Next, we em-ploy the CRF models to detect events.
We treatevents as a relation type.
The main relation pro-cessing sieves detect relations.
We designed sev-eral processing sieves, which support different re-lation attribute types and hierarchies.
We also em-ploy rules at each step to properly set the agentand target attributes.
In the last relation processingsieve, we perform rule-based relation extraction todetect high precision relations and boost the recall.In the last step we clean the extracted results andexport the data.The proposed system sieves are executed in thefollowing order:i Preprocessing Sieveii Mention Processing Sieveiii Event Processing Sieveiv Mention Relations Processing Sievev Event Relations Processing Sievevi Gene Relations Processing Sievevii Rule-Based Relations Processing Sieveviii Data Cleaning SieveIn the description of the sieves in the follow-ing sections, we use general relation terms, nam-ing the relation attributes as subject and object, asshown in Fig.
6.subject objectrelationFigure 6: General relation representation.1824.1 Preprocessing SieveThe preprocessing sieve includes data import, sen-tence detection and text tokenization.
Addition-ally, we enrich the data using part-of-speech tags,parse trees (http://opennlp.apache.org) and lem-mas (Jurs?ic et al 2010).4.2 Mention Processing SieveThe entity mentions consist of Protein, Gene-Family, ProteinFamily, ProteinComplex, Poly-meraseComplex, Gene, Operon, mRNA, Site, Reg-ulon and Promoter types.
Action mentions (e.g.inhibits, co-transcribes) are automatically de-tected as they are needed as event attributes forthe event extraction.
We therefore select all lem-mas of the action mentions from the training dataand detect new mentions from the test data set bycomparing lemma values.4.3 Event Processing SieveThe general definition of an event is described asa change on the state of a bio-molecule or bio-molecules (e.g.
?expression of a cheV-lacZ fusionis completely dependent on sigD?).
We representevents as a special case of relationship and namethem ?EVENT?.
In the training data, the event sub-ject types are Protein, GeneFamily, PolymeraseC-omplex, Gene, Operon, mRNA, Site, Regulon andPromoter types, while the objects are always ofthe action type (e.g.
?expression?
), which we dis-cover in the previous sieve.
After identifying eventrelations using the linear-chain CRF approach, weapply a rule that sets the action mention as an ob-ject and the gene as a subject attribute for everyextracted event.4.4 Relations Processing SievesAccording to the task relation properties (i.e.
dif-ferent subject and object types), we extract rela-tions in three phases (iv, v, vi).
This enables us toextract hierarchical relations (i.e.
relation containsanother relation as subject or object) and achievehigher precision.
All sieves use the proposedlinear-chain CRF-based extraction.
The process-ing sieves use specific relation properties and areexecuted as follows:(iv) First, we extract relations that contain onlyentity mentions as attributes (e.g.
?Transcrip-tion of cheV initiates from a sigmaD?
re-solves into the relation sigmaD ?
Interac-tion.Transcription?
cheV).
(v) In the second stage, we extract relations thatcontain at least one event as their attribute.Prior to execution we transform events intotheir mention form.
Mentions generated fromevents consist of two tokens.
They are takenfrom the event attributes and the new eventmention is included into the list of existingmentions.
Its order within the list is deter-mined by the index of the lowest mention to-ken.
Next, relations are identified followingthe same principle as in the first step.
(vi) According to an evaluation peculiarity of thechallenge, the goal is to extract possible inter-actions between genes.
Thus, when a relationbetween a gene G1 and an event E shouldbe extracted, the GRN network is the sameas if the method identifies a relation betweena gene G1 and gene G2, if G2 is the objectof event E. We exploit this notion by gen-erating training data to learn relation extrac-tion only between B. subtilis genes.
Duringthis step we use an external resource of allknown genes of the bacteria retrieved fromthe NCBI2.The training and development data sets includeseven relation instances that have a relation as anattribute.
We omitted this type of hierarchy extrac-tion due to the small number of data instances andexecution of relation extraction between genes.There are also four negative relation instances.The BioNLP task focuses on positive relations, sothere would be no increase in performance if neg-ative relations were extracted.
Therefore, we ex-tract only positive relations.
According to the dataset, we could simply add a separate sieve whichwould extract negations by using manually definedrules.
Words that explicitly define these negationsare not, whereas, neither and nor.4.5 Rule-Based Relations Processing SieveThe last step of relation processing uses rules thatextract relations with high precision.
General rulesconsist of the following four methods:?
The method that checks all consequent men-tion triplets that contain exactly one actionmention.
As input we set the index of the ac-tion mention within the triplet, its matchingregular expression and target relation.2http://www.ncbi.nlm.nih.gov/nuccore/AL009126183?
The method that processes every two con-sequent B. subtilis entity mentions.
It takesa regular expression, which must match thetext between the mentions, and a target rela-tion.?
The third method is a modification of the pre-vious method that supports having a list ofentity mentions on the left or the right side.For example, this method extracts two rela-tions in the following example: ?rsfA is underthe control of both sigma(F) and sigma(G)?.?
The last method is a variation of the sec-ond method, which removes subsentencesbetween the two mentions prior to relationextraction.
For example, the method is ableto extract distant relation from the followingexample: ?sigma(F) factor turns on about 48genes, including the gene for RsfA, and thegene for sigma(G)?.
This is sigma(F) ?
In-teraction.Activation?
sigma(G).We extract the Interaction relations using regu-lar expression and specific keywords for the tran-scription types (e.g.
keywords transcrib, directstranscription, under control of), inhibition (key-words repress, inactivate, inhibits, negatively reg-ulated by), activation (e.g.
keywords governedby, activated by, essential to activation, turns on),requirement (e.g.
keyword require) and binding(e.g.
keywords binds to, -binding).
Notice that inbiomedical literature, a multitude of expressionsare often used to describe the same type of geneticinteraction.
For instance, researchers might preferusing the expression to repress over to inactivateor to inhibit.
Thus, we exploit these synsets to im-prove the predictive accuracy of the model.4.6 Data Cleaning SieveThe last sieve involves data cleaning.
This consistsof removing relation loops and eliminating redun-dancy.A relation is considered a loop if its attributementions represent the same entity (i.e.
men-tions corefer).
For instance, sentence ?...
sigmaD element, while cheV-lacZ depends on sigD ...?contains mentions sigma D and sigD, which can-not form a relationship because they represent thesame gene.
By removing loops we reduce thenumber of insertions.
Removal of redundant re-lations does not affect the final score.5 Data in BioNLP 2013 GRN ChallengeTable 1 shows statistics of data sets used in ourstudy.
For the test data set we do not have taggeddata and therefore cannot show the detailed eval-uation analysis for each sieve.
Each data setconsists of sentences extracted from PubMed ab-stracts on the topic of the gene regulation networkof the sporulation of B. subtilis.
The sentences inboth the training and the development data sets aremanually annotated with entity mentions, eventsand relations.
Real mentions in Table 1 are thementions that refer to genes or other structures,while action mentions refer to event attributes (e.g.transcription).
Our task is to extract Interactionrelations of the types regulation, inhibition, acti-vation, requirement, binding and transcription forwhich the extraction algorithm is also evaluated.The extraction task in GRN Challenge is two-fold: given annotated mentions, a participantneeds to identify a relation and then determine therole of relation attributes (i.e.
subject or object)within the previously identified relation.
Only pre-dictions that match the reference relations by bothrelation type and its attributes are considered as amatch.6 Results and DiscussionWe tested our system on the data from BioNLP2013 GRN Shared Task using the leave one outcross validation on the training data and achieveda SER of 0.756, with 4 substitutions, 81 dele-tions, 14 insertions and 46 matches, given 131 ref-erence relations.
The relatively high number ofdeletions in these results might be due to ambigu-ities in the data.
We identified the following num-ber of extracted relations in the relation extractionsieves (Sec.
4): (iii) 91 events, (iv) 130 relationsbetween mentions only, (v) 27 relations betweenan event and a mention, (vi) 39 relations betweenentity mentions, and (vii) 44 relations using onlyrules.
Our approach consists of multiple submod-ules, each designed for a specific relation attributetype (e.g.
either both attributes are mentions, or anevent and a mention, or both are genes).
Also, thetotal sum of extracted relations exceeds the num-ber of final predicted relations, which is a conse-quence of their extraction in multiple sieves.
Du-plicates and loops were removed in the data clean-ing sieve.The challenge test data set contains 290 men-tions across 67 sentences.
To detect relations184Data set Documents Tokens RealmentionsActionmentionsEvents Relations Interactionrelationsdev 48 1321 205 55 72 105 71train 86 2380 422 102 157 254 159test 67 1874 290 86 / / /Table 1: BioNLP 2013 GRN Shared Task development (dev), training (train) and test data set properties.in the test data, we trained our models on thejoint development and training data.
At the timeof submission we did not use the gene relationsprocessing sieve (see Sec.
4) because it had notyet been implemented.
The results of the par-ticipants in the challenge are shown in Table 2.According to the official SER measure, our sys-tem (U. of Ljubljana) was ranked first.
Theother four competing systems were K. U. Leuven(Provoost and Moens, 2013), TEES-2.1 (Bjo?rneand Salakoski, 2013), IRISA-TexMex (Claveau,2013) and EVEX (Hakala et al 2013).
Partici-Participant S D I M SERU.
of Ljubljana 8 50 6 30 0.73K.
U. Leuven 15 53 5 20 0.83TEES-2.1 9 59 8 20 0.86IRISA-TexMex 27 25 28 36 0.91EVEX 10 67 4 11 0.92Table 2: BioNLP 2013 GRN Shared Task results.The table shows the number of substitutions (S),deletions (D), insertions (I), matches (M) and sloterror rate (SER) metric.pants aimed at a low number of substitutions, dele-tions and insertions, while increasing the numberof matches.
We got the least number of substi-tutions and fairly good results in the other threeindicators, which gave the best final score.
Fig.
7shows the predicted gene regulation network withthe relations that our system extracted from testdata.
This network does not exactly match oursubmission due to minor algorithm modificationsafter the submission deadline.7 ConclusionWe have proposed a sieve-based system for re-lation extraction from text.
The system is basedon linear-chain conditional random fields (LCRF)and domain-specific rules.
In order to support theextraction of relations between distant mentions,we propose an approach called skip-mention lin-ear chain CRF, which extends LCRF by varying,QWHUDFWLRQ$FWLYDWLRQ,QWHUDFWLRQ%LQGLQJ,QWHUDFWLRQ,QKLELWLRQ,QWHUDFWLRQ5HJXODWLRQ,QWHUDFWLRQ5HTXLUHPHQW,QWHUDFWLRQ7UDQVFULSWLRQFigure 7: The predicted gene regulation networkby our system at the BioNLP 2013 GRN SharedTask.185the number of skipped mentions to form mentionsequences.
In contrast to common relation extrac-tion approaches, we inferred a separate model foreach relation type.We applied the proposed system to the BioNLP2013 Gene Regulation Network Shared Task.
Thetask was to reconstruct the gene regulation net-work of sporulation in the model organism B. sub-tilis.
Our approach scored best among this year?ssubmissions.AcknowledgmentsThe work has been supported by the Slovene Re-search Agency ARRS within the research programP2-0359 and in part financed by the EuropeanUnion, European Social Fund.ReferencesJoanna Amberger, Carol Bocchini, and Ada Hamosh.2011.
A new face and new challenges for onlineMendelian inheritance in man (OMIM).
HumanMutation, 32(5):564?567.Michael Ashburner, Catherine A.
Ball, Judith A. Blake,David Botstein, Heather Butler, Michael J. Cherry,Allan P. Davis, Kara Dolinski, Selina S. Dwight,Janan T. Eppig, Midori A. Harris, David P. Hill, Lau-rie Issel-Tarver, Andrew Kasarskis, Suzanna Lewis,John C. Matese, Joel E. Richardson, Martin Ring-wald, Gerald M. Rubin, and Gavin Sherlock.
2000.Gene Ontology: Tool for the unification of biology.Nature Genetics, 25(1):25?29.Nguyen Bach and Sameer Badaskar.
2007.
A reviewof relation extraction.
Literature Review for Lan-guage and Statistics II, pages 1?15.Michele Banko and Oren Etzioni.
2008.
The trade-offs between open and traditional relation extraction.Proceedings of ACL-08: HLT, page 28?36.Mohit Bansal and Dan Klein.
2012.
Coreference se-mantics from web features.
In Proceedings of the50th Annual Meeting of the Association for Com-putational Linguistics: Long Papers-Volume 1, page389?398.Jari Bjo?rne and Tapio Salakoski.
2013.
TEES 2.1: Au-tomated annotation scheme learning in the bioNLP2013 shared task.
In Proceedings of BioNLP SharedTask 2013 Workshop, Sofia, Bulgaria, August.
Asso-ciation for Computational Linguistics.Robert Bossy, Philippe Bessir`es, and Claire Ne?dellec.2013.
BioNLP shared task 2013 - an overview ofthe genic regulation network task.
In Proceedingsof BioNLP Shared Task 2013 Workshop, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.Sergey Brin.
1999.
Extracting patterns and relationsfrom the world wide web.
In The World Wide Weband Databases, page 172?183.
Springer.Razvan C. Bunescu and Raymond J. Mooney.
2005.A shortest path dependency kernel for relation ex-traction.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, page 724?731.Vincent Claveau.
2013.
IRISA participation tobioNLP-ST13: lazy-learning and information re-trieval for information extraction tasks.
In Pro-ceedings of BioNLP Shared Task 2013 Workshop,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Dayne Freitag and Andrew McCallum.
2000.
In-formation extraction with HMM structures learnedby stochastic optimization.
In Proceedings of theNational Conference on Artificial Intelligence, page584?589.Marcos Garcia and Pablo Gamallo.
2011.Dependency-based text compression for semanticrelation extraction.
Information Extraction andKnowledge Acquisition, page 21.Claudio Giuliano, Alberto Lavelli, and Lorenza Ro-mano.
2006.
Exploiting shallow linguistic infor-mation for relation extraction from biomedical liter-ature.
In Proceedings of the Eleventh Conference ofthe European Chapter of the Association for Com-putational Linguistics (EACL-2006), page 401?408.Kai Hakala, Sofie Van Landeghem, Tapio Salakoski,Yves Van de Peer, and Filip Ginter.
2013.
EVEXin ST?13: Application of a large-scale text miningresource to event extraction and network construc-tion.
In Proceedings of BioNLP Shared Task 2013Workshop, Sofia, Bulgaria, August.
Association forComputational Linguistics.Matjaz?
Jurs?ic, Igor Mozetic?, Tomaz?
Erjavec, and NadaLavrac?.
2010.
LemmaGen: multilingual lemmati-sation with induced ripple-down rules.
Journal ofUniversal Computer Science, 16(9):1190?1214.Nanda Kambhatla.
2004.
Combining lexical, syntac-tic, and semantic features with maximum entropymodels for extracting relations.
In Proceedings ofthe ACL 2004 on Interactive poster and demonstra-tion sessions, page 22.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth In-ternational Conference on Machine Learning, pages282?289.Yaliang Li, Jing Jiang, Hai L. Chieu, and Kian M.A.Chai.
2011.
Extracting relation descriptors withconditional random fields.
In Proceedings of the5th International Joint Conference on Natural Lan-guage Processing, pages 392?400, Thailand.
AsianFederation of Natural Language Processing.186John Makhoul, Francis Kubala, Richard Schwartz, andRalph Weischedel.
1999.
Performance measures forinformation extraction.
In Proceedings of DARPABroadcast News Workshop, page 249?252.George A. Miller.
1995.
WordNet: a lexical databasefor English.
Commun.
ACM, 38(11):39?41.Alexis Mitchell, Stephanie Strassel, Shudong Huang,and Ramez Zakhary.
2005.
ACE 2004 multilin-gual training corpus.
Linguistic Data Consortium,Philadelphia.Yves Moreau and Le?on-Charles Tranchevent.
2012.Computational tools for prioritizing candidategenes: boosting disease gene discovery.
Nature Re-views Genetics, 13(8):523?536.Claire Ne?dellec.
2005.
Learning language in logic-genic interaction extraction challenge.
In Proceed-ings of the 4th Learning Language in Logic Work-shop (LLL05), volume 7, pages 1?7.John D. Osborne, Simon Lin, Warren A. Kibbe, Li-hua J. Zhu, Maria I. Danila, and Rex L. Chisholm.2006.
GeneRIF is a more comprehensive, cur-rent and computationally tractable source of gene-disease relationships than OMIM.
Technical report,Northwestern University.Rosario M Piro and Ferdinando Di Cunto.
2012.Computational approaches to disease-gene predic-tion: rationale, classification and successes.
TheFEBS Journal, 279(5):678?96.Hyla Polen, Antonia Zapantis, Kevin Clauson, JenniferJebrock, and Mark Paris.
2008.
Ability of onlinedrug databases to assist in clinical decision-makingwith infectious disease therapies.
BMC InfectiousDiseases, 8(1):153.Thomas Provoost and Marie-Francine Moens.
2013.Detecting relations in the gene regulation network.In Proceedings of BioNLP Shared Task 2013 Work-shop, Sofia, Bulgaria, August.
Association for Com-putational Linguistics.Lance A. Ramshaw and Mitchell P. Marcus.
1995.Text chunking using transformation-based learning.In Proceedings of the Third ACL Workshop on VeryLarge Corpora, page 82?94.Sunita Sarawagi.
2008.
Information extraction.
Foun-dations and Trends in Databases, 1(3):261?377.Tatjana Sauka-Spengler and Marianne Bronner-Fraser.2008.
A gene regulatory network orchestrates neu-ral crest formation.
Nature reviews Molecular cellbiology, 9(7):557?568.Sofie Van Landeghem, Jari Bjo?rne, Thomas Abeel,Bernard De Baets, Tapio Salakoski, and Yves Van dePeer.
2012.
Semantically linking molecular enti-ties in literature through entity relationships.
BMCBioinformatics, 13(Suppl 11):S6.Ting Wang, Yaoyong Li, Kalina Bontcheva, HamishCunningham, and Ji Wang.
2006.
Automatic ex-traction of hierarchical relations from text.
TheSemantic Web: Research and Applications, page215?229.Yan Xu, Kai Hong, Junichi Tsujii, I Eric, and ChaoChang.
2012.
Feature engineering combined withmachine learning and rule-based methods for struc-tured information extraction from narrative clinicaldischarge summaries.
Journal of the American Med-ical Informatics Association, 19(5):824?832.Alexander Yates and Oren Etzioni.
2007.
Unsuper-vised resolution of objects and relations on the web.In Proceedings of NAACL HLT, page 121?130.187
