Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 227?238,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsComparing Triggering Policies for Social BehaviorsRohit Kumar, Carolyn P. Ros?Language Technologies Institute, Carnegie Mellon UniversityGates Hillman Center, 5000 Forbes Avenue,Pittsburgh, PA, USA 15213rohitk , cprose @ cs.cmu.eduAbstractInstructional efficacy of automated Con-versational Agents designed to help smallgroups of students achieve higher learningoutcomes can be improved by the use ofsocial interaction strategies.
These strate-gies help the tutor agent manage the atten-tion of the students while delivering usefulinstructional content.
Two technical chal-lenges involving the use of social interac-tion strategies include determining theappropriate policy for triggering thesestrategies and regulating the amount of so-cial behavior performed by the tutor.
In thispaper, a comparison of six different trigger-ing policies is presented.
We find that atriggering policy learnt from human beha-vior in combination with a filter that keepsthe amount of social behavior comparableto that performed by human tutors offersthe most effective solution to the thesechallenges.1 IntroductionWhile Conversational Agents have been shown tobe an effective technology for delivering instruc-tional content to students in a variety of learningdomains and situations (Grasser et.
al., 2005; Ku-mar et.
al., 2007; Arnott et.
al., 2008), it has beenobserved that students are more likely to ignoreand abuse the tutor in a collaborative learning set-ting (with 2 or more students) compared to the caseof one-on-one tutoring (Bhatt et.
al., 2004; Kumaret.
al., 2007).
In our prior work (Kumar et.
al.,2010a), we have addressed this problem by em-ploying agents that are capable of performing bothinstructional behavior as well as social behavior.
Inour initial implementation, the social behavior per-formed by these agents was composed of elevensocial interaction strategies that were triggered bya set of hand crafted rules (Kumar and Ros?,2010b).
Section 2 provides additional details aboutthese strategies.Comparison between the social behavior trig-gered by our hand crafted rules and that triggeredby a human tutor revealed significant perceptionbenefits (more likeable, higher task satisfaction,etc.)
for the human triggering policy.
Also, the stu-dents in a wizard-of-oz condition who interactedwith the tutors whose social behaviors were trig-gered by humans had better learning outcomes(0.93?)
with respect to a No social behavior base-line.
The condition where students interacted withthe rule-based automated tutors was also signifi-cantly better (0.71?)
than the No social behaviorbaseline in terms of learning outcomes.
While thelearning outcomes of the rule-based tutors was notsignificantly worse than the human tutor, in com-bination with the perception outcomes, we see thepotential for further improvement of conversation-al agents by employing a better triggering policy.Building on these prior results, in this paper weexplore a way to improve the effectiveness of so-cially capable tutor agents that uses a triggeringpolicy learnt from a corpus of human behavior.The underlying hypothesis of this approach is thata human-like triggering policy would lead to im-provements in the agent?s performance and percep-227tion ratings compared to a rule-based triggeringpolicy.
As a first step towards verifying this hypo-thesis, we learnt a collection of triggering policiesfrom a corpus of human behavior.
While the focusof this paper is to evaluate the most human-liketriggering policy learnt from data in terms of itsperception benefits and learning outcomes, Section4 summarizes our efforts on learning triggeringpolicies.Before we discuss the details of the evaluationwe conducted, Section 3 presents an analysis ofmediating factors that provides insights into thereasons behind the effectiveness of social behavior.The design and procedure of the user study weconducted to evaluate the learnt triggering policiesis described in Section 5.
Finally, Section 6 dis-cusses the results of this evaluation.2 Social Interaction StrategiesIn our prior work (Kumar et.
al., 2010; Ai et.
al.,2010; Kumar et.
al., 2011), we have developed andevaluated automated tutors for two different educa-tional domains equipped with eleven social interac-tion strategies.
These strategies, listed in Table 1,correspond to three positive socio-emotional inte-raction categories identified by Bales (1950):Showing Solidarity, Showing Tension Release andAgreeing.Appendix A shows excerpts of an interactionbetween three students and a tutor during a collegefreshmen mechanical engineering learning activity.The shaded turns demonstrate realizations of someof the eleven social interaction strategies.Turns 7-12 shows the tutor initiating and partic-ipating in group formation using Strategy 1a (DoIntroductions) by greeting the students and askingfor their names.
In turn 53, the tutor is employingStrategy 3b (Show Comprehension / Approval) inresponse to a student opinion expressed in turn 52.When one of the students becomes inactive in theinteraction, the tutor uses strategy 1e (Encourage)realized as a targeted prompt shown in turn 122 toelicit a response from the inactive student.
Turn148 demonstrates Strategy 1d (Complement /Praise) to appreciate student participation in a con-ceptual tutoring episode that concluded at turn 147.Finally, turn 152 shows a realization of Strategy 2c(Express Enthusiasm, Elation, Satisfaction) whichis tied to either the start or the end of lengthy prob-lem solving steps in the learning activity such ascalculating the outcome of certain design choicesmade by the students during the learning activity.1.
Showing SolidarityRaises other's status, gives help, reward1a.
Do IntroductionsIntroduce and ask names of all participants1b.
Be Protective & NurturingDiscourage teasing1c.
Give Re-assuranceWhen student is discontent, asking for help1d.
Complement / PraiseTo acknowledge student contributions1e.
EncourageWhen group or members are inactive1f.
Conclude Socially2.
Showing Tension ReleaseJokes, laughs, shows satisfaction2a.
Expression of feeling betterAfter periods of tension, work pressure2b.
Be cheerful2c.
Express Enthusiasm, Elation, SatisfactionOn completing significant steps of the task3.
AgreeingShows passive acceptance, understands,concurs, complies3a.
Show AttentionTo student ideas as encouragement3b.
Show Comprehension / ApprovalTo student opinions and orientationsTable 1.
Social Interaction Strategies for threesocial-emotional interaction categories3 Effect of Social BehaviorsWe have observed the the use of social behavior incollaborative learning settings can help in improv-ing learning outcomes (Kumar et.
al., 2010a).
Thisobservation is not necessarily consistent with intui-tion.
On the one hand, we could attribute this to theability to manage the user?s (student) attention to-wards the agent (tutor) by displaying the socialbehavior.
At the same time, we could argue againstthe use of social behavior by considering it as apotential distraction from the task.
In an earlierexperiment (Kumar et.
al., 2010c), we found that itis important to keep the amount of social behaviorat an optimal level in order to observe the learning228benefits of the social behavior and to avoid its dis-traction costs.1 Tutor Are you paying attention to this?2 S006 of course3 S001 yes4 TutorWhat would happen if the stress inour Wrench is too high?5 S006 fail6 S001 it'd fail7 S014 it would break.8 Tutor That's right.
The wrench can break!9 TutorA wrench that can break in normaluse is not safe!10 S006 :(11 TutorIn general, to keep a wrench safe,we want to keep the stress in thewrench high or low?12 S006 lowwww13 S001 low14 Tutor Right, stress should be kept low.15 TutorWhich material property deter-mines when the stress is too high?16 S001 yield strength?Table 2.
Excerpt of a Conceptual Tutoring EpisodeIn this section, we present a corpus analysis todiscover an underlying model of how the socialbehavior affects learning outcomes.
We used acorpus of collaborative learning interactions be-tween teams of three or more students and a tutor.The interactions involve the teams performing alearning activity which lasts approximately 40 mi-nutes.
During this time, the tutors provide direc-tions, hints and conceptual tutoring.
Each teamundergoes up to four conceptual tutoring episodes.An episode is a system-initiated conversation dur-ing which the tutor leads the students through adirected line of reasoning to help them reflect upona concept related to the learning activity.
An ex-cerpt of a tutoring episode discussing the relation-ship between stress and safety is shown in Table 2.3.1 Coding Tutoring EpisodesEach turn in all the tutoring episodes of the 32 inte-ractions between a team of students and an auto-mated tutor were annotated using a coding schemedescribed here.
The tutor turns were categorized aseither Respondable (TR) if the students were ex-pected to the respond to that tutor turn or Not Res-pondable (TU) otherwise.
In Table 2, all theshaded turns are labeled as Respondable.Figure 1.
Venn Diagram of Episode Turn AnnotationsStudent Turns are categorized into one of threecategories.
Good turns (SG) identifies turns wherethe students are showing attention to a respondabletutor turn (e.g.
Turn 2 & 3 in Table 2) or the stu-dents are giving a correct or an incorrect responseto a direct question by the tutor (e.g.
Turns 5, 6, 7,12, 13 & 16).
Counterproductive (Bad) studentturns (SB) include students abusing the tutor orignoring the tutor (e.g.
talking to another studentwhen the students are expected to respond to a tu-tor turn).
Student turns that are not categorized asGood or Bad are labeled as Other (SO).
Turn 10 isan example of SO because it is a response to a tu-tor turn (9) where no student response is expected.Figure 1 shows a Venn diagram of the differentannotations.
All five categories are mutually exclu-sive.3.2 Structural Equation ModelingIn order to discover an underlying model of howthe use of social behavior affects student learning,we used a structural equation modeling (SEM)technique (Scheines et.
al., 1994).Data: To measure learning outcomes, our datacomprised of scores from pre-test and post-testadministered to 88 students who were part of the32 teams whose data was annotated for this analy-sis.
We normalized the number of Good (SG) andBad (SB) student turns by the number of Respond-able (TR) tutor turns and included normalized SG(nSG) and normalized SB (nSB) as measures ofinteraction characteristics of each student in ourdataset.
Total number of social turns performed bythe tutor in each interaction was included as a cha-racteristic of social behavior displayed by the tutor.Finally, the total amount of time (in seconds) that229the students spent on the tutoring episodes wasincluded as a characteristic of the interaction quali-ty during the tutoring episodes.Prior Knowledge: The only prior knowledgeinput to the model stated that the pre-test occursbefore the post-test.Discovered Models: We used Tetrad IV to dis-cover a structural equation model in the data com-prising of 6 fields (PreTest, PostTest, nSG, nSB,SocialTurns, EpisodeDuration) for each of the 88students.
Figure 2 shows the structural equationmodel discovered by Tetrad using the dataset de-scribed above.
p-Value of 0.46 for this model con-firms the hypothesis used by Tetrad for itsstatistical analysis i.e.
the model was not discov-ered randomly.
Note that unlike other statisticaltests, SEM models built using Tetrad are evaluatedas significant if the p-Value is greater than 0.05.The numbers on the arrows are correlation coeffi-cients and the numbers on the boxes indicate meanvalues for each variable.Figure 2.
SEM discovered using all 6 variables in ourdatasetBesides the obvious causal effect of PreTestscore on PostTest score, we find that as the dura-tion of the tutoring episodes (EpisodeDuration)increases, the learning outcomes deteriorate.
Wenotice that an increase in the normalized number ofBad student turns increases EpisodeDuration indi-cating that students who abuse or ignore the tutorare likely to not pay attention to the learning con-tent presented during the tutoring episodes, henceprolonging the tutoring episode as the tutor tries toget the students through the instructional content.Furthermore, we observe that social behavior helpsin counteracting the negative learning effect of Badinteraction behaviors of the students.
Tutors thatperform social behavior are capable of managingthe student?s attention and get the students throughthe tutoring episode faster.3.3 DiscussionThe SEM analysis discussed in the previous sec-tion helps us better understand the relationship be-tween the use of social behavior and studentlearning in a collaborative learning setting.
Let?sconsider the duration of the tutoring episodes as anindicator of the students?
attention to the tutor(higher duration lower attention).
We see thatsocial behavior helps in managing the students?attention, which may be affected negatively bycounterproductive/bad interaction behavior fromthe students.Besides suggesting that social behavior could bea useful strategy for directing student attention, italso suggests that social behavior may not servethis function where counterproductive student be-havior is not present or where it does not occurenough to negatively impact task behavior.
This isbecause a minimum amount of time needs to bespent on each tutoring episode to deliver the in-structional of the concept being discussed.
In theabsence of counterproductive student behavior,episode duration may be close to that minimum.Also, in an earlier analysis (Hua et.
al., 2010) ina different learning domain where the social beha-viors described in Section 2 were employed, wehave observed that the number of abusive/negativecomment made by the students about the tutor dur-ing the interaction were significantly higher in acondition where the tutors performed a highamount of social behavior.
This suggests that therelationship between the SocialTurns and Episo-deDuration variables may not be linear in extremecases and emphasizes the importance of perform-ing an optimal amount of social behavior.4 Triggering Social BehaviorAside from designing, implementing and regulat-ing the amount of social behavior performed byautomated tutors, one of the challenges involved inthe appropriate use of social interaction strategiesis that of triggering these strategies only at themost appropriate moments during the interaction.Our initial implementation of these strategies(Kumar & Ros?, 2010b) achieved this using a set230of hand crafted rules that used features such as re-cent student turns, state of the tutoring plan, etc.Here we will summarize our efforts on buildinga better triggering policy using a data-driven ap-proach that models the behavior of human tutors attriggering the social interaction strategies listed inTable 1.
Using a corpus of 10 interactions betweena group of students and partially automated tutorswhose social behaviors were triggered by humantutors, we attempt to learn a triggering policy thatpredicts when the human tutors will trigger a socialstrategies.
Currently, we focus on only learning atriggering policy that determines if a social beha-vior should be performed.
The choice of whichbehavior is performed when triggered by the policyis still based on the rules used in our earlier im-plementation as discussed in Section 5.3.In order to compare the triggers generated by apolicy, we use a binary sequence comparison me-tric called kKappa (Neikrasz & Moore, 2010) de-veloped for evaluating discourse segmentationapproaches.
The metric allows a soft penalty formisplacing a trigger (or a segment boundary) with-in a window of k turns.We developed a large margin learning algo-rithm following McDonald et.
al.
(2005) that itera-tively learns the coefficients of a linear function inthe feature space that separates turns where humantutors decided to trigger a social behavior from therest of the turns.
Instead of using an instance-basedobjective function (like square-loss), our algorithmmaximizes the kKappa metric over a providedtraining set.
The function learnt this way can beused as a triggering policy by using it at every turnduring an interaction to predict if a human tutorwould trigger a social behavior.
We used a collec-tion of automatically extractable features thatrepresent the lexical and semantic content of recentstudent and tutor turns, current discourse state andactivity levels of the students.While details of the objective evaluation of thevarious learnt triggering policies is beyond thescope of this paper, we found that the best per-forming strategy (k-?
= 0.13) was significantly bet-ter than a random baseline (k-?
= 0.01) as well asthe rule based triggering policy (k-?
= -0.09) usedin our initial implementation.
Also, the policylearnt by our algorithm outperformed policieslearnt by algorithms such as Linear Regression (k-?
= 0.00) and Logistic Regression (k-?
= 0.05) thatuse instance-based loss metrics (Hall et.
al., 2009).5 User StudyHere we will present an experiment we conductedto evaluate the effectiveness of various ways totrigger social behavior discussed in Section 4.
Thisexperiment is a step towards verifying the hypo-thesis that a human-like triggering policy couldoutperform a rule-based triggering policy that wasused in our earlier experiments (Kumar et.
al.,2010a).
We use the same interactive situation forthe experiment presented here as in our earlierwork.
Freshmen mechanical engineering studentsenrolled at an American university participate in acomputer-aided engineering lab that is divided intothree parts, i.e., Computer-Aided Design (CAD),Computer-Aided Analysis (CAA) and Computer-Aided Manufacturing (CAM).
Students practicethe use of various engineering software packagesfor all three parts as they design, analyze and man-ufacture an Aluminum wrench.
Our experiment isconducted during the second part (CAA) of the lab.5.1 Procedure & MaterialsThe Computer-Aided Analysis lab comprises oftwo activities.
The first activity involves analyzinga wrench design given to the students by specify-ing certain loading conditions and simulating thestresses and deformations in the wrench.
Studentsare led by a teaching assistant during this activity.They spend approximately 25 minutes performingthis activity.
At the end of the analysis activity, thestudents see a simulation of the stress distributionin the body of the wrench.After the analysis activity, a pre-test is adminis-tered.
Each student spends 10 minutes working onthe pre-test individually.
The pre-test comprises of11 questions, 8 of which are multiple-choice ques-tions and the other 3 are short essay type questions.The second activity of the CAA lab is a colla-borative design activity.
During this activity, stu-dents work in teams of three.
Student in the sameteam are seated in separate parts of the lab and canonly communicate using a text-based chatroomapplication (M?hlpfordt and Wessner, 2005).
Thechatroom application also provides a shared work-space in the form of a whiteboard.After the pre-test, students are given written in-structions describing the collaborative design ac-tivity.
The instructions ask the students to design abetter wrench in terms of ease of use, cost of mate-rials and safety compared to the wrench they ana-231lyzed earlier.
The students are expected to come upwith three new designs in 40 minutes by varyingparameters like dimensions and materials of thewrench.
The instructions also include various for-mulae and data that the students might need to usefor their designs.
Besides course credit, the instruc-tions mention an additional giftcard for the teamthat comes up with the best design ($10 for eachmember of the winning team).Students are asked to log in to their respectiveteam?s chatroom.
They spend the next 40 minutesworking on the collaborative design activity.
Be-sides the three students, the chatroom for eachteam includes an automated tutor.
The tutor guidesthe students through the first two designs suggest-ing potential choices for dimension and materialsfor each design.
As the design activity progresses,the tutor initiates four conceptual tutoring episodesto help the students reflect upon underlying me-chanical engineering concepts like stress, force,moment, safety, etc., that are relevant to the designactivity.Our experimental manipulation happens duringthis 40 minute segment.
The tutor in each team?schatroom is configured to perform social behaviorusing different triggering policies as specified bythe condition assigned to the team.
The conditionsare discussed in the next section.
Irrespective ofthe condition, each team receives the 4 conceptualtutoring episodes.
Every student performs all thesteps of this procedure like all other students.At the end of the collaborative design activity, apost-test and a survey are administered.
Studentsare asked to spend 15 minutes to first complete thetest and then the survey.
The post-test is the sametest used for pre-test.
The survey comprises of 15items shown in Appendix B.
The students areasked to rate each item on a 7-point Likert scaleranging from Strongly Disagree (1) to StronglyAgree (7).
The 15 items on the survey include 11items eliciting perception of the tutor.
9 of the 11items state positive aspects of the tutor (e.g.
?tutorwas friendly?).
The other 2 items stated negativeaspects about the tutor (e.g.
?tutor?s responses gotin the way?).
Besides the items about the tutor, 2items elicited the student?s rating about the colla-borative design activity.
The last 2 items wereabout the student?s satisfaction with their perfor-mance on the design task.In total, both the activities that are part of theCAA lab take approximately 1 hour 40 minutes.5.2 Experimental DesignThe teams participating in the experiment de-scribed here were divided into six conditions.These conditions determined the triggering policyand the amount of social behavior performed bythe automated tutors.
Tutors in the None conditiondid not perform any social behavior.
Tutors in theRules condition used the same hand crafted rule-based triggering policy employed in our earlierexperiment (Kumar et.
al., 2010a).
Following theresults from another experiment (Kumar & Ros?,2010c), the automated tutors in the Rules conditionperformed a moderate amount of social behavior(atmost 20% of all tutor turns).
On average, theRules policy triggered 25 social turns per interac-tion.The RandomLow and RandomHigh condi-tions used a random triggering policy with a socialratio filter to regulate the amount of social beha-vior.
In both the random conditions, the tutorwould trigger social behavior using a randomnumber generator to generate the confidence oftriggering a social behavior after every turn (by astudent or a tutor).
In the RandomLow condition, abehavior would be triggered if the confidence wasabove 0.91.
In the RandomHigh condition, a beha-vior would be triggered if the confidence wasabove 0.85.
On average, the RandomLow condi-tion had 23 behaviors triggered per interaction.About 37 behaviors were triggered in the Ran-domHigh condition.The LearntLow and LearntHigh conditionsused the best triggering policy learnt from a corpusof human triggering of social behavior as discussedin Section 4.
The same social ratio filter used in therandom conditions was used in these two condi-tions also.
As in the case with RandomLow andRandomHigh, different values of a confidence pa-rameter were used for the LearntLow and Learn-tHigh conditions to control the number of socialbehaviors triggered.
On average, the LearntLowcondition had 22 triggers and the LearntHigh con-dition had 28 triggers.5.3 Generating BehaviorsThe various triggering policies described above foreach of our experimental conditions only deter-mine when a tutor agent will perform a social be-havior.
In order to perform the social behavior inactual use, the agent must not only determine when232a behavior should be triggered, but also determinewhich behavior should be performed when a trig-ger is received.
Our implementation of the tutoragent used in this experiment provides a conti-nuous stream of scores for each of the eleven so-cial interaction strategies that the tutor canperform.
The scores are computed using hand-crafted functions that use the same features used inour rule-based triggering policy (Kumar et.
al.,2010b).
When a social behavior is triggered, a rou-lette wheel selection is used to determine the strat-egy to be performed.
The circumference of thewheel assigned to each strategy is proportional tothe score of each strategy.
If the score of all thestrategies is zero, a generic social prompt is per-formed.6 Results126 students enrolled in an introductory mechani-cal engineering course at an American universityparticipated in the experiment described in thispaper.
The experiment was conducted on two sepa-rate days separated by one week.
On each day, foursessions of the Computer-Aided Analysis lab wereconducted, and students attended only one as-signed session.
Session assignment was madebased on an alphabetic split.
The 126 students weredivided into 42 teams.
20 teams participated on thefirst day of the experiment.
They were evenly splitinto four conditions (None, Rules, RandomHigh &LearntHigh).
The remaining 22 teams participatedon the second day.
Out of these, 5 teams each wereassigned to the None and RandomLow condition.
6teams each were assigned to the Rules andLearntLow conditions.The rest of this section presents detailed resultsand analysis of this experiment.
To summarize, wefound that out of the six evaluated policies only theLearntLow policy that uses a triggering modellearnt from human triggering data and generates amoderate amount of social behavior is consistentlybetter than the other policies in terms of both per-formance as well as perception outcomes.
Also, theLearntLow policy is found to be most efficient atdelivering the instructional content as indicated bythe smallest EpisodeDuration in Table 5.6.1 Learning OutcomesThe learning outcomes analysis presented hereshows the advantage of using a triggering policylearnt from a corpus of human triggering behavioralong with a filtering technique that regulates theamount of social behavior as shown in Table 3.We first verified that there was no significantdifference between the six conditions on the pre-test scores.
As in the case of previous experimentsusing this learning activity, we saw that the learn-ing activity was pedagogically beneficial to thestudents irrespective of the condition.
There was asignificant improvement in test scores betweenpre-test and post-test { p < 0.0001, F(1,250) =26.01, effect-size = 0.58?
}.There was no significant effect of the conditionassigned to each team on the total test scores.However, there was a significant effect on the testscores of short-essay type questions using the pre-test score as a covariate and the condition as a fac-tor { p < 0.05, F(5, 119) = 2.88 }.
The adjustedpost test scores for the short essay type questionsand their standard deviations are shown in Table 3.Post-hoc analysis showed that the LearntLow con-dition was significantly better than LearntHighcondition { effect-size = 0.65?
}.
Also, Random-Low condition was marginally better than Learn-tHigh condition { p < 0.07, effect-size = 0.62?
}.Mean St.Dev.LearntLow 5.12 0.54RandomLow 5.06 0.67None 4.75 1.13RandomHigh 4.59 1.09Rules 4.38 0.89LearntHigh 3.98 1.74Table 3.
Mean and Standard Deviation of Adjusted PostTest Scores for Short Essay Type QuestionsThis result further supports the observationfrom our earlier experiment (Kumar & Ros?,2010c) which demonstrated that importance of per-forming the right amount of social behavior.
BothRandomLow and LearntLow conditions employthe non-linear social ratio filter which keeps theamount of allowed social behavior at a level com-parable to the amount of social behavior performedby human tutors.Since the primary objective of the experimentdescribed here was to evaluate a learnt triggeringpolicy with respect to a rule-based triggering poli-cy, we repeated the ANCOVA for the short essaytype question using data from only the Rules,233LearntLow and LearntHigh conditions.
We found asignificant effect of condition on the post-test scoreusing pre-test score as a covariate { p = 0.01,F(2,62) = 4.98 }.
A post-hoc analysis showed thatthe LearntLow condition was significantly betterthan the LearntHigh condition as above and  theLearntLow condition was marginally better thanthe Rules condition { p ?
0.08, effect-size = 0.84?}.
We observe that a triggering policy learnt fromhuman triggering behavior can achieve a marginalimprovement on learning outcomes compared toour existing rule-based triggering policy.
This isconsistent with our hypothesis.6.2 Perception RatingsWe averaged the student?s rating for the 11 itemsabout the tutor into a single tutor rating measureused here.
Rating on the two negative statementsabout the tutor were inverted (7?1, 6?2, and soon) for this calculation.Mean St.Dev.Rules 4.74 1.45LearntLow 4.56 1.58None 4.42 1.49RandomHigh 3.74 1.63LearntHigh 3.55 1.26RandomLow 3.18 0.91Table 4.
Mean and Standard Deviation of Tutor RatingsWe found a significant effect of condition onthe tutor ratings { p < 0.01, F(5,120) = 3.83 }.
Ta-ble 4 shows the mean and standard deviations oftutor ratings for each condition.
Post-hoc analysisshowed that only the Rules condition was signifi-cantly better than the RandomLow condition.
Also,we found that Rules was marginally better thanLearntHigh condition { p < 0.08 } and both Learnt-Low and None conditions was marginally betterthan RandomLow condition { p < 0.08 }.While we did not see a significant improvementin perception due the use of a learnt triggering pol-icy when compared to a rule-based triggering poli-cy, we find an advantage over using a randomtriggering policy (RandomLow) which was asgood as a learnt policy on the learning outcomes.The results from the tutor?s perception ratings fur-ther support the importance of timing and regulat-ing the amount of social behavior.We did not find any significant effect of condi-tion on the ratings about the design activity or stu-dent?s task satisfaction.6.3 Analysis of Tutoring EpisodesIn order to understand the results from the experi-ment presented in this paper, we applied the struc-tural equation model discussed earlier (Figure 2) tothe data collected from our current experiment.Figure 3 shows the model for our current experi-ment (p=0.4492).
Only four variables were usedbecause the annotations of good and bad studentbehavior are not available at this time.Figure 3.
SEM applied to data from this experimentMean St.Dev.RandomHigh 540.80 49.50LearntHigh 534.80 61.00None 523.88 41.54Rules 519.80 102.70RandomLow 519.20 74.40LearntLow 484.00 69.80Table 5.
Mean and Standard Deviation of Durationof Tutoring EpisodesWe see that most of the model parameters (p-Value, means & correlations) are similar to para-meters for the model shown in Figure 2.
However,the correlation between SocialTurns and Episode-Duration is much smaller.
Also, note that the meanof EpisodeDuration is smaller compared to that inFigure 2 which indicates that lesser counterproduc-tive behavior was displayed by the students in thisexperiment.
The conceptual tutoring episodes areoperating closer to the minimum episode durationwhich leaves a smaller room for improvement by234the use of social interaction strategies.
As dis-cussed in Section 3.3, this explains the smaller cor-relation between SocialTurns and EpisodeDurationin Figure 3.Table 5 shows the mean and standard deviationsof the duration of tutoring episodes for each condi-tion.
Even though the differences are not signifi-cant, the LearntLow policy has the lowest durationindicating higher student attention than the otherconditions.7 DiscussionPrior work in the field of human-human interactionand human-machine interaction in the form of di-alog systems has emphasized the importance oftiming the display of behavior to achieve naturaland/or productive interactions.
In general, timingof interactive behaviors (verbal as well as non-verbal) has been studied in the context of joint ac-tivities being performed by the participants.
Beha-viors are timed to achieve and maintaincoordination between the participants (Clark,2005).
Specifically, among other topics, timing oflow-level (signal) interaction like turn-taking hasbeen the subject of several investigations (Raux &Eskenazi, 2008; Takeuchi et.
al., 2004).On the other hand, the use of social behavior byconversational agents to support students has beenproposed (Veletsianos et.
al., 2009; Gulz et.
al.,2010).
Work in the area of affective computing andits application to tutorial dialog has focused onidentification of student?s emotional states and us-ing those to improve choice of behavior performedby tutors (D?Mello et.
al., 2005).
Our prior work(Kumar et.
al., 2010; Kumar et.
al., 2007) hasshown that social behavior motivated from empiri-cal research in small group communication (Bales,1950) can help in effectively supporting students incollaborative learning settings.
Use of social inte-raction in other applications of conversationalagents besides education has been investigated(Bickmore et.
al., 2009; Dybala et.
al., 2009; Doh-saka et.
al., 2009).The experiments presented here bridges thesetwo tracks of research specifically proposing a so-lution to the challenge of timing social behavior inthe context of a supporting collaborative learning.Compared to the work on timing signal-level jointactivities like turn-taking, this work focuses on thetiming of joint activities at the conversation level.The success of our algorithm at learning a modelof timing conversational behaviors in the contextof an interactive task could potentially offer a gen-eral approach for realizing such behaviors in otherconversational agents.8 ConclusionIn this paper, we presented an experiment thatcompared the effectiveness of several social beha-vior triggering policies.
Specifically, we compareda triggering policy learnt from a corpus of humantriggering behavior to a rule-based policy whichhas previously been shown to be successful at trig-gering effective social behavior in a collaborativelearning activity.The presented experiment provides further evi-dence in support of the intuition that timing of so-cial behavior and regulating the amount of socialbehavior are critical to improving performance andperception outcomes.
A triggering policy based onhuman-like timing in combination with a filter thatattempts to keep amount of social behavior at thesame level as human tutors was shown to be mar-ginally better than the rule-based policy on learn-ing outcomes.
Also, on perception measures, wefound that the human-like policy is marginally bet-ter than a random triggering policy which uses thesame filter to control the amount of social beha-vior.
Only the learned model provides a win bothon learning and on perception measures.In order to better understand the effect of use ofsocial behavior by automated tutors on student?slearning outcomes, we presented a structured mod-el which suggests that social behavior helps inachieving higher learning outcomes by allowingthe tutor to better manage the student?s attention.Following this model, we saw that a human-liketriggering policy is able to achieve higher studentattention as indicated by the smaller duration oftutoring episodes.We found a significant negative correlation {coefficient = -0.20, p < 0.05 } between the tutor?sperception rating and number of social behaviorstriggered when none of the social interaction strat-egies were applicable.
As next steps, our best trig-gering policy could be potentially further refinedby achieving a closer integration of the triggeringmodel with the social behavior generation mechan-ism to prevent triggering when none of the elevenstrategies could be generated.235ReferencesHua Ai, Rohit Kumar, Dong Nguyen, Amrut Nagasund-er and Carolyn P.
Ros?, 2010, Exploring the Effec-tiveness of Social Capabilities and Goal Alignment inComputer Supported Collaborative Learning, Intelli-gent Tutoring Systems, Pittsburgh, PAElizabeth Arnott, Peter Hastings and David Allbritton,2008, Research Methods Tutor: Evaluation of a di-alogue-based tutoring system in the classroom, Be-havior Research Methods, 40 (3), 694-698Robert F. Bales, 1950, Interaction process analysis: Amethod for the study of small groups, Addison-Wesley, Cambridge, MAKhelan Bhatt, Martha Evens, Shlomo Argamon, 2004,Hedged responses and expressions of affect in hu-man/human and human/computer tutorial interac-tions, CogSci, Chicago, ILTimothy Bickmore, Daniel Schulman and LangxuanYin, 2009, Engagement vs. Deceit: Virtual Humanswith Human Autobiographies, Proc.
of IntelligentVirtual Agents, Amsterdam, NetherlandsHerbert H. Clark, 2005, Coordinating with each other ina material world, Discourse Studies, 7 (4-5), 507-525Sidney K. D?Mello, Scotty D. Craig, Barry Gholson,Stan Frankin, Rosalind Picard, Arthur C. Graesser,2005, Integrating Affect Sensors in an Intelligent Tu-toring System, Wksp on Affective Interactions: TheComputer in the Affective Loop, IUI, San Diego, CAPawel Dybala, Michal Ptaszynski, Rafal Rzepka andKenji Araki, 2009, Humoroids: ConversationalAgents that induce positive emotions with humor,AAMAS, Budapest, HungaryKohji Dohsaka, Ryoto Asai, Ryichiro Higashinaka, Ya-suhiro Minami and Eisaku Maeda, 2009, Effects ofConversational Agents on Human Communication inThough Evoking Multi-Party dialogues, SIGDial2009, London, UKAgneta Gulz, Annika Silvervarg and Bj?rn Sj?d?n,2010, Design for off-task interaction - Rethinkingpedagogy in technology enhanced learning, Intl.Conf.
on Advanced Learning Technologies, TunisiaArthur C. Graesser, Patrick Chipman, Brian C. Haynes,and Andrew Olney, 2005, AutoTutor: An IntelligentTutoring System with Mixed-initiative Dialogue,IEEE Transactions in Education, 48, 612-618Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann and Ian H. Witten,2009, The WEKA Data Mining Software: An Up-date; SIGKDD Explorations, Volume 11, Issue 1Rohit Kumar, Jack L. Beuth and Carolyn P.
Ros?, 2011,Conversational Strategies that Support Idea Genera-tion Productivity in Groups, 9th Intl.
Conf.
on Com-puter Supported Collaborative Learning, Hong KongRohit Kumar, Hua Ai, Jack Beuth and Carolyn P. Ros?,2010a, Socially-capable Conversational Tutors canbe Effective in Collaborative-Learning situations, In-telligent Tutoring Systems, Pittsburgh, PARohit Kumar and Carolyn P.
Ros?, 2010b, Engaginglearning groups using Social Interaction Strategies,NAACL-HLT, Los Angeles, CARohit Kumar and Carolyn P.
Ros?, 2010c, Conversa-tional Tutors with Rich Interactive Behaviors thatsupport Collaborative Learning, Workshop on Op-portunities for intelligent and adaptive behavior incollaborative learning systems, ITS 2010, Pittsburgh,PARohit Kumar, Gahgene Gweon, Mahesh Joshi, Yue Cuiand Carolyn Ros?, Supporting students working to-gether on Math with Social Dialogue, Workshop onSpeech and Language Technology in Education,Farmington, PA, 2007Ryan McDonald, Koby Crammer and Fernando Pereira,2005a, Online large-margin training of dependencyparsers, Proc.
of ACL, 91-98, Ann Arbor, MIMartin M?hlpfordt and Martin Wessner, 2005, Explicitreferencing in chat supports collaborative learning,Proc.
Computer Support for Collaborative Learning(CSCL)John Niekrasz and Johanna D. Moore, 2010, UnbiasedDiscourse Segmentation Evaluation, Proc.
of SLT,Berkeley, CAAntoine Raux and Maxine Eskenazi, 2008, OptimizingEndpointing Thresholds using Dialogue Features in aSpoken Dialogue System, SIGDial 2008, Columbus,OHRichard Scheines, Peter Spirtes, Clark Glymour, andChristopher Meek, 1994, TETRAD II: Tools for Dis-covery, Lawrence Erlbaum Associates, Hillsdale, NJMasashi Takeuchi, Norihide Kitaoka and Seiichi Naka-gawaM, 2004, Timing detection for realtime dialogsystems using prosodic and linguistic information,Intl Conf.
on Speech Prosody 2004, Nara, JapanTetrad IV,http://www.phil.cmu.edu/projects/tetrad/tetrad4.htmlGeorge Veletsianos, Charles Miller and Aaron Doering,Veletsianos, 2009, EnALI: A Research and DesignFramework for Virtual Characters and PedagogicalAgents, Journal of Educational Computing Research,41(2), 171-194236Appendix A. Excerpts of an interaction between three students and an automated tutor demonstratingsome of the social interaction strategies listed in Table 1.7 Tutor Hi, I am your tutor for today's lab.8 Tutor Lets introduce ourselves briefly.
I am Avis.9 S083 Hey Avis!
I'm _name1_10 S073 Im _name2_11 S089 i'm _name3_12 Tutor Its nice to meet you all.
:)13 Tutor Let's get started by reviewing the base design in your worksheet.Strategy 1a: Do Introductions48 S083 R we using the same material?49 S073 I assume so50 S073 just changing the length51 S089 yeah we have multiple design steps52 S089 so probably for now53 Tutor cool :)54 S083 O okStrategy 3b: Show Comprehension / Approval119 Tutor Is this a safe wrench?120 S073 and then is the same for design 2121 S073 so yes the wrench is safe122 Tutor _name1_ ... any thoughts you'd like to contribute?123 S083 its a safer wrench if its in steelStrategy 1e: Encourage (inactive members)143 S073 high144 Tutor Right, higher yield stress is better.145 S089 so steel or titanium146 S089 but have fun paying for that...147 Tutor All other things being equal, you want to choose a stronger material.148 Tutor It's good to have your contributions to the discussion :-)149 S073 yayStrategy 1d: Complement / Praise150 S073 :)151 Tutor Let's improve design 1 by using Steel for our 2nd _truncated_152 Tutor I am happy to be working with our team153 S083 thanks :-)154 Tutor You can start calculating the fields in the worksheet _truncated_155 S089 woo...Strategy 2c: Express Enthusiasm, Elation, Satisfaction237Appendix B.
Survey administered to the participants at the end of the Collaborative Design ActivityUsing the following scale, Indicate to what extent you agree with each of the following items.1 2 3 4 5 6 7StronglyDisagreeMostlyDisagreeSomewhatDisagreeNeutral SomewhatAgreeMostlyAgreeStronglyAgreeThe tutor was part of my team.
1 2 3 4 5 6 7The tutor provided good ideas for the discussion.
1 2 3 4 5 6 7The tutor received my contributions positively.
1 2 3 4 5 6 7The tutor was friendly during the discussion.
1 2 3 4 5 6 7The tutor responded to my contributions.
1 2 3 4 5 6 7The tutor helped in lowering the tension in my group.
1 2 3 4 5 6 7The tutor was paying attention to our conversation.
1 2 3 4 5 6 7Overall, I liked the tutor very much.
1 2 3 4 5 6 7I think the tutor was as good as a human tutor.
1 2 3 4 5 6 7I often ignored what the tutor was saying.
1 2 3 4 5 6 7The tutor's responses got in the way of our conversation.
1 2 3 4 5 6 7The design challenge was exciting.
1 2 3 4 5 6 7I did my best to come up with good designs.
1 2 3 4 5 6 7I am happy with the discussion I had with my group.
1 2 3 4 5 6 7Overall, we were successful at meeting our goals during the design challenge.
1 2 3 4 5 6 7238
