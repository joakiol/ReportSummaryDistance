Integrating Multiple Knowledge Sources toDisambiguate Word Sense: An Exemplar-Based ApproachHwee Tou NgDefence  Sc ience Organ isat ion20 Sc ience Park  Dr iveS ingapore  118230nhweet ou?trantor, dso.
gov.
sgHian Beng LeeDefence  Sc ience Organ isat ion20 Sc ience Park  Dr iveS ingapore  118230lhianben@trant or.
dso .
gov.
sgAbst ractIn this paper, we present a new approachfor word sense disambiguation (WSD) us-ing an exemplar-based learning algorithm.This approach integrates a diverse set ofknowledge sources to disambiguate wordsense, including part of speech of neigh-boring words, morphological form, the un-ordered set of surrounding words, localcollocations, and verb-object syntactic re-lation.
We tested our WSD program,named LEXAS, on both a common dataset used in previous work, as well as ona large sense-tagged corpus that we sep-arately constructed.
LEXAS achieves ahigher accuracy on the common data set,and performs better than the most frequentheuristic on the highly ambiguous wordsin the large corpus tagged with the refinedsenses of WoRDNET.1 IntroductionOne important problem of Natural Language Pro-cessing (NLP) is figuring out what a word meanswhen it is used in a particular context.
The differentmeanings of a word are listed as its various senses ina dictionary.
The task of Word Sense Disambigua-tion (WSD) is to identify the correct sense of a wordin context.
Improvement in the accuracy of iden-tifying the correct word sense will result in bettermachine translation systems, information retrievalsystems, etc.
For example, in machine translation,knowing the correct word sense helps to select theappropriate target words to use in order to translateinto a target language.In this paper, we present a new approach forWSD using an exemplar-based learning algorithm.This approach integrates a diverse set of knowledgesources to disambiguate word sense, including partof speech (POS) of neighboring words, morphologi-cal form, the unordered set of surrounding words,local collocations, and verb-object syntactic rela-tion.
To evaluate our WSD program, named LEXAS(LEXical Ambiguity-resolving _System), we tested iton a common data set involving the noun "interest"used by Bruce and Wiebe (Bruce and Wiebe, 1994).LEXAS achieves a mean accuracy of 87.4% on thisdata set, which is higher than the accuracy of 78%reported in (Bruce and Wiebe, 1994).Moreover, to test the scalability of LEXAS, we haveacquired a corpus in which 192,800 word occurrenceshave been manually tagged with senses from WORD-NET, which is a public domain lexical database con-taining about 95,000 word forms and 70,000 lexicalconcepts (Miller, 1990).
These sense tagged wordoccurrences consist of 191 most frequently occur-ring and most ambiguous nouns and verbs.
Whentested on this large data set, LEXAS performs betterthan the default strategy of picking the most fre-quent sense.
To our knowledge, this is the first timethat a WSD program has been tested on such a largescale, and yielding results better than the most fre-quent heuristic on highly ambiguous words with therefined sense distinctions of WOttDNET.2 Task  DescriptionThe input to a WSD program consists of unre-stricted, real-world English sentences.
In the out-put, each word occurrence w is tagged with its cor-rect sense (according to the context) in the form ofa sense number i, where i corresponds to the i-thsense definition of w as given in some dictionary.The choice of which sense definitions to use (andaccording to which dictionary) is agreed upon in ad-vance.For our work, we use the sense definitions as givenin WORDNET, which is comparable to a good desk-top printed dictionary in its coverage and sense dis-tinction.
Since WODNET only provides sense def-initions for content words, (i.e., words in the partsof speech (POS) noun, verb, adjective, and adverb),LEXAS is only concerned with disambiguating thesense of content words.
However, almost all existingwork in WSD deals only with disambiguating con-tent words too.LEXAS assumes that each word in an input sen-40tence has been pre-tagged with its correct POS, sothat the possible senses to consider for a contentword w are only those associated with the particu-lar POS of w in the sentence.
For instance, giventhe sentence "A reduction of principal and interestis one way the problem may be solved.
", since theword "interest" appears as a noun in this sentence,LEXAS will only consider the noun senses of "inter-est" but not its verb senses.
That  is, LEXAS is onlyconcerned with disambiguating senses of a word ina given POS.
Making such an assumption is reason-able since POS taggers that can achieve accuracyof 96% are readily available to assign POS to un-restricted English sentences (Brill, 1992; Cutting etal., 1992).In addition, sense definitions are only available forroot words in a dictionary.
These are words thatare not morphologically inflected, such as "interest"(as opposed to the plural form "interests"), "fall"(as opposed to the other inflected forms like "fell","fallen", "falling", "falls"), etc.
The sense of a mor-phologically inflected content word is the sense of itsuninflected form.
LEXAS follows this convention byfirst converting each word in an input sentence intoits morphological root using the morphological na-lyzer of WORD NET, before assigning the appropriateword sense to the root form.3 A lgor i thmLEXAS performs WSD by first learning from a train-ing corpus of sentences in which words have beenpre-tagged with their correct senses.
That is, it usessupervised learning, in particular exemplar-basedlearning, to achieve WSD.
Our approach has beenfully implemented in the program LExAs.
Part ofthe implementation uses PEBLS (Cost and Salzberg,1993; Rachlin and Salzberg, 1993), a public domainexemplar-based learning system.LEXAS builds one exemplar-based classifier foreach content word w. It operates in two phases:training phase and test phase.
In the training phase,LEXAS is given a set S of sentences in the trainingcorpus in which sense-tagged occurrences of w ap-pear.
For each training sentence with an occurrenceof w, LEXAS extracts the parts of speech (POS) ofwords surrounding w, the morphological form of w,the words that frequently co-occur with w in thesame sentence, and the local collocations containingw.
For disambiguating a noun w, the verb whichtakes the current noun w as the object is also iden-tified.
This set of values form the features of an ex-ample, with one training sentence contributing onetraining example.Subsequently, in the test phase, LEXAS is givennew, previously unseen sentences.
For a new sen-tence containing the word w, LI~XAS extracts fromthe new sentence the values for the same set of fea-tures, including parts of speech of words surround-41ing w, the morphological form of w, the frequentlyco-occurring words surrounding w, the local colloca-tions containing w, and the verb that takes w as anobject (for the case when w is a noun).
These valuesform the features of a test example.This test example is then compared to every train-ing example.
The sense of word w in the test exam-ple is the sense of w in the closest matching train-ing example, where there is a precise, computationaldefinition of "closest match" as explained later.3.1 Feature  Ext rac t ionThe first step of the algorithm is to extract a set Fof features uch that each sentence containing an oc-currence of w will form a training example supplyingthe necessary values for the set F of features.Specifically, LEXAS uses the following set of fea-tures to form a training example:L3, L2, LI, 1~i, R2, R3, M, KI, .
.
.
, Kin, el,..., 69, V3.1.1 Part of Speech and MorphologicalFormThe value of feature Li is the part of speech (POS)of the word i-th position to the left of w. The valueof Ri is the POS of the word i-th position to the rightof w. Feature M denotes the morphological form ofw in the sentence s. For a noun, the value for thisfeature is either singular or plural; for a verb, thevalue is one of infinitive (as in the uninflected formof a verb like "fall"), present-third-person-singular(as in "falls"), past (as in "fell"), present-participle(as in "falling") or past-participle (as in "fallen").3.1.2 Unordered  Set of  Sur round ing  WordsKt ,  ?
?., Km are features corresponding to a set ofkeywords that frequently co-occur with word w inthe same sentence.
For a sentence s, the value offeature Ki is one if the keyword It'~ appears ome-where in sentence s, else the value of Ki is zero.The set of keywords K1 , .
.
.
,  Km are determinedbased on conditional probability.
All the word to-kens other than the word occurrence w in a sen-tence s are candidates for consideration as keywords.These tokens are converted to lower case form beforebeing considered as candidates for keywords.Let cp(ilk ) denotes the conditional probability ofsense i of w given keyword k, whereNi,kcp(ilk) = N~Nk is the number of sentences in which keyword k co-occurs with w, and Ni,k is the number of sentencesin which keyword k co-occurs with w where w hassense i.For a keyword k to be selected as a feature, itmust satisfy the following criteria:1. cp(ilk ) >_ Mi for some sense i, where M1 is somepredefined minimum probability.2.
The keyword k must occur at least M2 timesin some sense i, where /1//2 is some predefinedminimum value.3.
Select at most M3 number of keywords for agiven sense i if the number of keywords atisfy-ing the first two criteria for a given sense i ex-ceeds M3.
In this case, keywords that co-occurmore frequently (in terms of absolute frequency)with sense i of word w are selected over thoseco-occurring less frequently.Condition 1 ensures that a selected keyword is in-dicative of some sense i of w since cp(ilk) is at leastsome minimum probability M1.
Condition 2 reducesthe possibility of selecting a keyword based on spu-rious occurrence.
Condition 3 prefers keywords thatco-occur more frequently if there is a large numberof eligible keywords.For example, M1 = 0.8, Ms = 5, M3 = 5 whenLEXAS was tested on the common data set reportedin Section 4.1.To illustrate, when disambiguating the noun "in-terest", some of the selected keywords are: ex-pressed, acquiring, great, attracted, expressions,pursue, best, conflict, served, short, minority, rates,rate, bonds, lower, payments.3.1.3 Local  Co l locat ionsLocal collocations are common expressions con-taining the word to be disambiguated.
For our pur-pose, the term collocation does not imply idiomaticusage, just words that are frequently adjacent o theword to be disambiguated.
Examples of local collo-cations of the noun "interest" include "in the interestof", "principal and interest", etc.
When a word tobe disambiguated occurs as part of a collocation, itssense can be frequently determined very reliably.
Forexample, the collocation "in the interest of" alwaysimplies the "advantage, advancement, favor" senseof the noun "interest".
Note that the method forextraction of keywords that we described earlier willfail to find the words "in", "the", "of" as keywords,since these words will appear in many different po-sitions in a sentence for many senses of the noun"interest".
It is only when these words appear inthe exact order "in the interest of" around the noun"interest" that strongly implies the "advantage, ad-vancement, favor" sense.There are nine features related to collocations inan example.
Table 1 lists the nine features and somecollocation examples for the noun "interest".
For ex-ample, the feature with left offset = -2 and right off-set = 1 refers to the possible collocations beginningat the word two positions to the left of "interest"and ending at the word one position to the right of"interest".
An example of such a collocation is "inthe interest of".The method for extraction of local collocations issimilar to that for extraction of keywords.
For each42Left Offset Right Offset Collocation Example-1 -1 accrued interest1 1 interest rate-2 -1 principal and interest-1 1 national interest in1 2 interest and dividends-3 -1 sale of an interest-2 in the interest of-1 2 an interest in a1 3 interest on the bondsTable 1: Features for Collocationsof the nine collocation features, LEXAS concatenatesthe words between the left and right offset positions.Using similar conditional probability criteria for theselection of keywords, collocations that are predic-tive of a certain sense are selected to form the pos-sible values for a collocation feature.3.1.4 Verb -Ob ject  Syntact i c  Re la t ionLEXAS also makes use of the verb-object syntacticrelation as one feature V for the disambiguation ofnouns.
If a noun to be disambiguated is the head ofa noun group, as indicated by its last position in anoun group bracketing, and if the word immediatelypreceding the opening noun group bracketing is averb, LEXAS takes such a verb-noun pair to be in averb-object syntactic relation.
Again, using similarconditional probability criteria for the selection ofkeywords, verbs that are predictive of a certain senseof the noun to be disambiguated are selected to formthe possible values for this verb-object feature V.Since our training and test sentences come withnoun group bracketing, determining verb-object re-lation using the above heuristic can be readily done.In future work, we plan to incorporate more syntac-tic relations including subject-verb, and adjective-headnoun relations.
We also plan to use verb-object and subject-verb relations to disambiguateverb senses.3.2 Training and TestingThe heart of exemplar-based learning is a measureof the similarity, or distance, between two examples.If the distance between two examples is small, thenthe two examples are similar.
We use the followingdefinition of distance between two symbolic valuesvl and v2 of a feature f:e(vl, v2) = I c1' cl c2, c. Ii=1Cl,i is the number of training examples with valuevl for feature f that is classified as sense i in thetraining corpus, and C1 is the number of trainingexamples with value vl for feature f in any sense.C2,i and C2 denote similar quantities for value v2 offeature f .
n is the total number of senses for a wordW.This metric for measuring distance is adoptedfrom (Cost and Salzberg, 1993), which in turn isadapted from the value difference metric of the ear-lier work of (Stanfill and Waltz, 1986).
The distancebetween two examples is the sum of the distancesbetween the values of all the features of the two ex-amples.During the training phase, the appropriate set offeatures is extracted based on the method describedin Section 3.1.
From the training examples formed,the distance between any two values for a feature fis computed based on the above formula.During the test phase, a test example is comparedagainst allthe training examples.
LEXAS then deter-mines the closest matching training example as theone with the minimum distance to the test example.The sense of w in the test example is the sense of win this closest matching training example.If there is a tie among several training exampleswith the same minimum distance to the test exam-ple, LEXAS randomly selects one of these trainingexamples as the closet matching training example inorder to break the tie.4 Eva luat ionTo evaluate the performance of LEXAS, we con-ducted two tests, one on a common data set used in(Bruce and Wiebe, 1994), and another on a largerdata set that we separately collected.4.1 Eva luat ion  on a Common Data  SetTo our knowledge, very few of the existing work onWSD has been tested and compared on a commondata set.
This is in contrast o established practicein the machine learning community.
This is partlybecause there are not many common data sets pub-licly available for testing WSD programs.One exception is the sense-tagged data set usedin (Bruce and Wiebe, 1994), which has been madeavailable in the public domain by Bruce and Wiebe.This data set consists of 2369 sentences each con-taining an occurrence of the noun "interest" (or itsplural form "interests") with its correct sense man-ually tagged.
The noun "interest" occurs in six dif-ferent senses in this data set.
Table 2 shows thedistribution of sense tags from the data set that weobtained.
Note that the sense definitions used in thisdata set are those from Longman Dictionary of Con-temporary English (LDOCE) (Procter, 1978).
Thisdoes not pose any problem for LEXAS, since LEXASonly requires that there be a division of senses intodifferent classes, regardless of how the sense classesare defined or numbered.POS of words are given in the data set, as wellas the bracketings of noun groups.
These are usedto determine the POS of neighboring words and theLDOCE sense Frequency Percent1: readiness to give 361 15%attention2: quality of causing 11 <1%attention to be given3: activity, subject, etc.
67 3%which one gives time andattention to178 4: advantage,advancement, or favor5: a share (in a company,business, etc.
)4996: money paid for the use 1253of money8%21%53%Table 2: Distribution of Sense Tagsverb-object syntactic relation to form the features ofexamples.In the results reported in (Bruce and Wiebe,1994), they used a test set of 600 randomly selectedsentences from the 2369 sentences.
Unfortunately,in the data set made available in the public domain,there is no indication of which sentences are used astest sentences.
As such, we conducted 100 randomtrials, and in each trial, 600 sentences were randomlyselected to form the test set.
LEXAS is trained onthe remaining 1769 sentences, and then tested on aseparate test set of sentences in each trial.Note that in Bruce and Wiebe's test run, the pro-portion of sentences in each sense in the test set isapproximately equal to their proportion in the wholedata set.
Since we use random selection of test sen-tences, the proportion of each sense in our test set isalso approximately equal to their proportion in thewhole data set in our random trials.The average accuracy of LEXAS over 100 randomtrials is 87.4%, and the standard deviation is 1.37%.In each of our 100 random trials, the accuracy ofLEXAS is always higher than the accuracy of 78%reported in (Bruce and Wiebe, 1994).Bruce and Wiebe also performed a separate testby using a subset of the "interest" data set with only4 senses (sense 1, 4, 5, and 6), so as to compare theirresults with previous work on WSD (Black, 1988;Zernik, 1990; Yarowsky, 1992), which were testedon 4 senses of the noun "interest".
However, thework of (Black, 1988; Zernik, 1990; Yarowsky, 1992)were not based on the present set of sentences, sothe comparison is only suggestive.
We reproducedin Table 3 the results of past work as well as the clas-sification accuracy of LEXAS, which is 89.9% with astandard deviation of 1.09% over 100 random trials.In summary, when tested on the noun "interest",LEXAS gives higher classification accuracy than pre-vious work on WSD.In order to evaluate the relative contribution ofthe knowledge sources, including (1) POS and mor-43WSD research AccuracyBlack (1988) 72%Zernik (1990) 70%Yarowsky (1992) 72%Bruce & Wiebe (1994) 79%LEXhS (1996) 89%Table 3: Comparison with previous resultsKnowledge SourcePOS & morphosurrounding wordscollocationsverb-objectMean Accuracy77.2%62.0%80.2%43.5%Std Dev1.44%1.82%1.55%1.79%Table 4: Relative Contribution of KnowledgeSourcesphological form; (2) unordered set of surroundingwords; (3) local collocations; and (4) verb to the left(verb-object syntactic relation), we conducted 4 sep-arate runs of 100 random trials each.
In each run,we utilized only one knowledge source and computethe average classification accuracy and the standarddeviation.
The results are given in Table 4.Local collocation knowledge yields the highest ac-curacy, followed by POS and morphological form.Surrounding words give lower accuracy, perhaps be-cause in our work, only the current sentence formsthe surrounding context, which averages about 20words.
Previous work on using the unordered set ofsurrounding words have used a much larger window,such as the 100-word window of (Yarowsky, 1992),and the 2-sentence context of (Leacock et al, 1993).Verb-object syntactic relation is the weakest knowl-edge source.Our experimental finding, that local collocationsare the most predictive, agrees with past observa-tion that humans need a narrow window of only afew words to perform WSD (Choueka and Lusignan,1985).The processing speed of LEXAS is satisfactory.Running on an SGI Unix workstation, LEXAS canprocess about 15 examples per second when testedon the "interest" data set.4.2 Eva luat ion  on  a Large  Data  SetPrevious research on WSD tend to be tested onlyon a dozen number of words, where each word fre-quently has either two or a few senses.
To test thescalability of LEXAS, we have gathered a corpus inwhich 192,800 word occurrences have been manuallytagged with senses from WoRDNET 1.5.
This dataset is almost two orders of magnitude larger in sizethan the above "interest" data set.
Manual taggingwas done by university undergraduates majoring inLinguistics, and approximately one man-year of ef-forts were expended in tagging our data set.These 192,800 word occurrences consist of 121nouns and 70 verbs which are the most frequently oc-curring and most ambiguous words of English.
The121 nouns are:action activity age air area art boardbody book business car case center cen-tury change child church city class collegecommunity company condition cost coun-try course day death development differ-ence door effect effort end example xperi-ence face fact family field figure foot forceform girl government ground head historyhome hour house information interest jobland law level life light line man mate-rial matter member mind moment moneymonth name nation need number orderpart party picture place plan point pol-icy position power pressure problem pro-cess program public purpose question rea-son result right room school section senseservice side society stage state step studentstudy surface system table term thing timetown type use value voice water way wordwork worldThe 70 verbs are:add appear ask become believe bring buildcall carry change come consider continuedetermine develop draw expect fall givego grow happen help hold indicate involvekeep know lead leave lie like live look losemean meet move need open pay raise readreceive remember equire return rise runsee seem send set show sit speak stand startstop strike take talk tell think turn waitwalk want work writeFor this set of nouns and verbs, the average num-ber of senses per noun is 7.8, while the average num-ber of senses per verb is 12.0.
We draw our sen-tences containing the occurrences of the 191 wordslisted above from the combined corpus of the 1 mil-lion word Brown corpus and the 2.5 million wordWall Street Journal (WSJ) corpus.
For every wordin the two lists, up to 1,500 sentences each con-taining an occurrence of the word are extractedfrom the combined corpus.
In all, there are about113,000 noun occurrences and about 79,800 verb oc-currences.
This set of 121 nouns accounts for about20% of all occurrences of nouns that one expects toencounter in any unrestricted English text.
Simi-larly, about 20% of all verb occurrences in any unre-stricted text come from the set of 70 verbs chosen.We estimate that there are 10-20% errors in oursense-tagged data set.
To get an idea of how thesense assignments of our data set compare withthose provided by WoRDNET linguists in SEMCOR,the sense-tagged subset of Brown corpus preparedby Miller et al (Miller et al, 1994), we compare44Test setBC50WSJ6Sense 140.5%44.8%Most Frequent LEXAS47.1% 54.0%63.7% 68.6%Table 5: Evaluation on a Large Data Seta subset of the occurrences that overlap.
Out of5,317 occurrences that overlap, about 57% of thesense assignments in our data set agree with thosein SEMCOR.
This should not be too surprising, asit is widely believed that sense tagging using thefull set of refined senses found in a large dictionarylike WORDNET involve making subtle human judg-ments (Wilks et al, 1990; Bruce and Wiebe, 1994),such that there are many genuine cases where twohumans will not agree fully on the best sense assign-ments.We evaluated LEXAS on this larger set of noisy,sense-tagged data.
We first set aside two subsets fortesting.
The first test set, named BC50, consists of7,119 occurrences of the 191 content words that oc-cur in 50 text files of the Brown corpus.
The secondtest set, named WSJ6, consists of 14,139 occurrencesof the 191 content words that occur in 6 text files ofthe WSJ corpus.We compared the classification accuracy of LEXASagainst he default strategy of picking the most fre-quent sense.
This default strategy has been advo-cated as the baseline performance l vel for compar-ison with WSD programs (Gale et al, 1992).
Thereare two instantiations of this strategy in our currentevaluation.
Since WORDNET orders its senses suchthat sense 1 is the most frequent sense, one pos-sibility is to always pick sense 1 as the best senseassignment.
This assignment method does not evenneed to look at the training sentences.
We call thismethod "Sense 1" in Table 5.
Another assignmentmethod is to determine the most frequently occur-ring sense in the training sentences, and to assignthis sense to all test sentences.
We call this method"Most Frequent" in Table 5.
The accuracy of LEXASon these two test sets is given in Table 5.Our results indicate that exemplar-based classi-fication of word senses scales up quite well whentested on a large set of words.
The classificationaccuracy of LEXAS is always better than the defaultstrategy of picking the most frequent sense.
We be-lieve that our result is significant, especially whenthe training data is noisy, and the words are highlyambiguous with a large number of refined sense dis-tinctions per word.The accuracy on Brown corpus test files is lowerthan that achieved on the Wall Street Journal testfiles, primarily because the Brown corpus consistsof texts from a wide variety of genres, includingnewspaper reports, newspaper editorial, biblical pas-sages, science and mathematics articles, general fic-tion, romance story, humor, etc.
It is harder to dis-45ambiguate words coming from such a wide variety oftexts.5 Re la ted  WorkThere is now a large body of past work on WSD.Early work on WSD, such as (Kelly and Stone, 1975;Hirst, 1987) used hand-coding of knowledge to per-form WSD.
The knowledge acquisition process is la-borious.
In contrast, LEXAS learns from tagged sen-tences, without human engineering of complex rules.The recent emphasis on corpus based NLP has re-sulted in much work on WSD of unconstrained real-world texts.
One line of research focuses on the useof the knowledge contained in a machine-readabledictionary to perform WSD, such as (Wilks et al,1990; Luk, 1995).
In contrast, LEXAS uses super-vised learning from tagged sentences, which is alsothe approach taken by most recent work on WSD, in-cluding (Bruce and Wiebe, 1994; Miller et al, 1994;Leacock et al, 1993; Yarowsky, 1994; Yarowsky,1993; Yarowsky, 1992).The work of (Miller et al, 1994; Leacock et al,1993; Yarowsky, 1992) used only the unordered set ofsurrounding words to perform WSD, and they usedstatistical classifiers, neural networks, or IR-basedtechniques.
The work of (Bruce and Wiebe, 1994)used parts of speech (POS) and morphological form,in addition to surrounding words.
However, the POSused are abbreviated POS, and only in a window of-b2 words.
No local collocation knowledge is used.
Aprobabilistic lassifier is used in (Bruce and Wiebe,1994).That local collocation knowledge provides impor-tant clues to WSD is pointed out in (Yarowsky,1993), although it was demonstrated only on per-forming binary (or very coarse) sense disambigua-tion.
The work of (Yarowsky, 1994) is perhaps themost similar to our present work.
However, his workused decision list to perform classification, in whichonly the single best disambiguating evidence thatmatched a target context is used.
In contrast, weused exemplar-based learning, where the contribu-tions of all features are summed up and taken intoaccount in coming up with a classification.
We alsoinclude verb-object syntactic relation as a feature,which is not used in (Yarowsky, 1994).
Although thework of (Yarowsky, i994) can be applied to WSD,the results reported in (Yarowsky, 1994) only dealtwith accent restoration, which is a much simplerproblem.
It is unclear how Yarowsky's method willfare on WSD of a common test data set like the onewe used, nor has his method been tested on a largedata set with highly ambiguous words tagged withthe refined senses of WORDNET.The work of (Miller et al, 1994) is the only priorwork we know of which attempted to evaluate WSDon a large data set and using the refined sense dis-tinction of WORDNET.
However, their results showno improvement (in fact a slight degradation i  per-formance) when using surrounding words to performWSD as compared to the most frequent heuristic.They attributed this to insufficient training data inSEMCOm In contrast, we adopt a different strategyof collecting the training data set.
Instead of taggingevery word in a running text, as is done in SEMCOR,we only concentrate on the set of 191 most frequentlyoccurring and most ambiguous words, and collectedlarge enough training data for these words only.
Thisstrategy ields better esults, as indicated by a bet-ter performance of LEXAS compared with the mostfrequent heuristic on this set of words.Most recently, Yarowsky used an unsupervisedlearning procedure to perform WSD (Yarowsky,1995), although this is only tested on disambiguat-ing words into binary, coarse sense distinction.
Theeffectiveness of unsupervised learning on disam-biguating words into the refined sense distinction ofWoRBNET needs to be further investigated.
Thework of (McRoy, 1992) pointed out that a diverseset of knowledge sources are important o achieveWSD, but no quantitative evaluation was given onthe relative importance of each knowledge source.No previous work has reported any such evaluationeither.
The work of (Cardie, 1993) used a case-basedapproach that simultaneously learns part of speech,word sense, and concept activation knowledge, al-though the method is only tested on domain-specifictexts with domain-specific word senses.6 ConclusionIn this paper, we have presented a new approach forWSD using an exemplar based learning algorithm.This approach integrates a diverse set of knowledgesources to disambiguate word sense.
When tested ona common data set, our WSD program gives higherclassification accuracy than previous work on WSD.When tested on a large, separately collected ataset, our program performs better than the defaultstrategy of picking the most frequent sense.
To ourknowledge, this is the first time that a WSD programhas been tested on such a large scale, and yieldingresults better than the most frequent heuristic onhighly ambiguous words with the refined senses ofWoRDNET.7 AcknowledgementsWe would like to thank: Dr Paul Wu for sharingthe Brown Corpus and Wall Street Journal Corpus;Dr Christopher Ting for downloading and installingWoRDNET and SEMCOR, and for reformatting thecorpora; the 12 undergraduates from the Linguis-tics Program of the National University of Singa-pore for preparing the sense-tagged corpus; and ProfK.
P. Mohanan for his support of the sense-taggingproject.ReferencesEzra Black.
1988.
An experiment in computationaldiscrimination ofEnglish word senses.
IBM Jour-nal of Research and Development, 32(2):185-194.Eric Brill.
1992.
A simple rule-based part of speechtagger.
In Proceedings of the Third Conference onApplied Natural Language Processing, pages 152-155.Rebecca Bruce and Janyce Wiebe.
1994.
Word-sense disambiguation using decomposable mod-els.
In Proceedings of the 32nd Annual Meetingof the Association for Computational Linguistics,Las Cruces, New Mexico.Claire Cardie.
1993.
A case-based approach toknowledge acquisition for domain-specific sen-tence analysis.
In Proceedings of the Eleventh Na-tional Conference on Artificial Intelligence, pages798-803, Washington, DC.Y.
Choueka nd S. Lusignan.
1985.
Disambiguationby short contexts.
Computers and the Humani-ties, 19:147-157.Scott Cost and Steven Salzberg.
1993.
A weightednearest neighbor algorithm for learning with sym-bolic features.
Machine Learning, 10(1):57-78.Doug Cutting, Julian Kupiec, Jan Pedersen, andPenelope Sibun.
1992.
A practical part-of-speechtagger.
In Proceedings of the Third Conference onApplied Natural Language Processing, pages 133-140.William Gale, Kenneth Ward Church, and DavidYarowsky.
1992.
Estimating upper and lowerbounds on the performance of word-sense disam-biguation programs.
In Proceedings of the 30thAnnual Meeting of the Association for Computa-tional Linguistics, Newark, Delaware.Graeme Hirst.
1987.
Semantic Interpretation andthe Resolution of Ambiguity.
Cambridge Univer-sity Press, Cambridge.Edward Kelly and Phillip Stone.
1975.
Com-puter Recognition of English Word Senses.
North-Holland, Amsterdam.Claudia Leacock, Geoffrey Towell, and EllenVoorhees.
1993.
Corpus-based statistical senseresolution.
In Proceedings of the ARPA HumanLanguage Technology Workshop.Alpha K. Luk.
1995.
Statistical sense disambigua-tion with relatively small corpora using dictio-nary definitions.
In Proceedings of the 33rd An-nual Meeting of the Association for Computa-tional Linguistics, Cambridge, Massachusetts.Susan W. McRoy 1992.
Using multiple knowledgesources for word sense discrimination.
Computa-tional Linguistics, 18(1):1-30.46George A. Miller, Ed.
1990.
WordNet: An on-linelexical database.
International Journal of Lezi-cography, 3(4):235-312.George A. Miller, Martin Chodorow, Shari Landes,Claudia Leacock, and Robert G. Thomas.
1994.Using a semantic oncordance for sense identifi-cation.
In Proceedings of the ARPA Human Lan-guage Technology Workshop.Paul Procter et al 1978.
Longman Dictionary ofContemporary English.John Rachlin and Steven Salzberg.
1993.
PEBLS3.0 User's Guide.C Stanfill and David Waltz.
1986.
Toward memory-based reasoning.
Communications of the ACM,29(12):1213-1228.Yorick Wilks, Dan Fass, Cheng-Ming Guo, James E.McDonald, Tony Plate, and Brian M. Slator.1990.
Providing machine tractable dictionarytools.
Machine Translation, 5(2):99-154.David Yarowsky.
1992.
Word-sense disambigua-tion using statistical models of Roger's categoriestrained on large corpora.
In Proceedings of theFifteenth International Conference on Computa-tional Linguistics, pages 454-460, Nantes, France.David Yarowsky.
1993.
One sense per colloca-tion.
In Proceedings of the ARPA Human Lan-guage Technology Workshop.David Yarowsky.
1994.
Decision lists for lexical am-biguity resolution: Application to accent restora-tion in Spanish and French.
In Proceedings of the32nd Annual Meeting of the Association for Com-putational Linguistics, Las Cruces, New Mexico.David Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics, Cam-bridge, Massachusetts.Uri Zernik.
1990.
Tagging word senses in corpus:the needle in the haystack revisited.
TechnicalReport 90CRD198, GE R&D Center.47
