Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 79?84,Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational LinguisticsKyotoEBMT: An Example-Based Dependency-to-DependencyTranslation FrameworkJohn Richardson?
Fabien Cromi?res?
Toshiaki Nakazawa?
Sadao Kurohashi?
?Graduate School of Informatics, Kyoto University, Kyoto 606-8501?Japan Science and Technology Agency, Kawaguchi-shi, Saitama 332-0012john@nlp.ist.i.kyoto-u.ac.jp, {fabien, nakazawa}@pa.jst.jp,kuro@i.kyoto-u.ac.jpAbstractThis paper introduces the Ky-otoEBMT Example-Based MachineTranslation framework.
Our systemuses a tree-to-tree approach, employingsyntactic dependency analysis forboth source and target languagesin an attempt to preserve non-localstructure.
The effectiveness of oursystem is maximized with online ex-ample matching and a flexible decoder.Evaluation demonstrates BLEU scorescompetitive with state-of-the-art SMTsystems such as Moses.
The currentimplementation is intended to bereleased as open-source in the nearfuture.1 IntroductionCorpus-based approaches have become a ma-jor focus of Machine Translation research.We present here a fully-fledged Example-Based Machine Translation (EBMT) plat-form making use of both source-languageand target-language dependency structure.This paradigm has been explored compar-atively less, as studies on Syntactic-basedSMT/EBMT tend to focus on constituenttrees rather than dependency trees, andon tree-to-string rather than tree-to-tree ap-proaches.
Furthermore, we employ separatedependency parsers for each language ratherthan projecting the dependencies from one lan-guage to another, as in (Quirk et.
al, 2005).The dependency structure information isused end-to-end: for improving the qualityof the alignment of the translation examples,for constraining the translation rule extractionand for guiding the decoding.
We believe thatdependency structure, which considers morethan just local context, is important in orderto generate fluent and accurate translationsof complex sentences across distant languagepairs.Our experiments focus on technical do-main translation for Japanese-Chinese andJapanese-English, however our implementa-tion is applicable to any domain and languagepair for which there exist translation examplesand dependency parsers.A further unique characteristic of our sys-tem is that, again contrary to the majority ofsimilar systems, it does not rely on precompu-tation of translation rules.
Instead it matcheseach input sentence to the full database oftranslation examples before extracting trans-lation rules online.
This has the merit of max-imizing the information available when creat-ing and combining translation rules, while re-taining the ability to produce excellent trans-lations for input sentences similar to an exist-ing translation example.The system is mostly developed in C++ andincorporates a web-based translation interfacefor ease of use.
The web interface (see Fig-ure 1) also displays information useful for erroranalysis such as the list of translation exam-ples used.
Experiments are facilitated throughthe inclusion of a curses-based graphical in-terface for performing tuning and evaluation.The decoder supports multiple threads.We are currently making preparations forthe project to be released with an open-source license.
The code will be available athttp://nlp.ist.i.kyoto-u.ac.jp/kyotoebmt/.2 System OverviewFigure 2 shows the basic structure of the pro-posed translation pipeline.The training process begins with parsingand aligning parallel sentences from the train-79Figure 1: A screenshot of the web interfaceshowing a Japanese-English translation.
Theinterface provides the source and target sidedependency tree, as well as the list of exam-ples used with their alignments.
The web in-terface facilitates easy and intuitive error anal-ysis, and can be used as a tool for computer-aided translation.ing corpus.
Alignment uses a Bayesian sub-tree alignment model based on dependencytrees.
This contains a tree-based reorder-ing model and can capture non-local reorder-ings, which sequential word-based models of-ten cannot handle effectively.
The alignmentsare then used to build an example database(?translation memory?)
containing ?examples?or ?treelets?
that form the hypotheses to becombined during decoding.Translation is performed by first parsingan input sentence then searching for treeletsmatching entries in the example database.The retrieved treelets are combined by a de-coder that optimizes a log linear model score.The example retrieval and decoding steps areexplained in more detail in sections 3 and 4respectively.
The choice of features and thetuning of the log linear model is described insection 5.Figure 3 shows the process of combining ex-amples matching the input tree to create anoutput sentence.Figure 2: Translation pipeline.
An exampledatabase is first trained from a parallel cor-pus.
Translation is performed by the decoder,which combines initial hypotheses generatedby the example retrieval module.
Weights canbe improved with batch tuning.3 Example retrieval and translationhypothesis constructionAn important characteristic of our system isthat we do not extract and store translationrules in advance: the alignment of translationexamples is performed offline.
However, for agiven input sentence i, the steps for findingexamples partially matching i and extractingtheir translation hypotheses is an online pro-cess.
This approach could be considered to bemore faithful to the original EBMT approachadvocated by Nagao (1984).
It has alreadybeen proposed for phrase-based (Callison-Burch et al., 2005), hierarchical (Lopez, 2007),and syntax-based (Cromi?res and Kurohashi,2011) systems.
It does not however, seem tobe very commonly integrated in syntax-basedMT.This approach has several benefits.
The firstis that we are not required to impose a limiton the size of translation hypotheses.
Systemsextracting rules in advance typically restrictthe size and number of extracted rules for fearof becoming unmanageable.
In particular, ifan input sentence is the same or very similarto one of our translation examples, we will beable to retrieve a perfect translation.
A secondadvantage is that we can make use of the fullcontext of the example to assign features andscores to each translation hypothesis.The main drawback of our approach is thatit can be computationally more expensive toretrieve arbitrarily large matchings in the ex-80!"#$%&"!""#$%#&''("')"#*!)+,!-')".
!/01)$"'()234"*+2546,-.274"!284"/2946 02:4"#$%&2;4"*$!<"!
"=0$6 #$%#&''("')"#*!)+,!-')".!/01)$">)?
@#6A*$$6 B@#?@#6A*$$6C%!.?,$6D!#!&!+$6,-.
"'()"*6 1"/6 06*$!<"!
"E$6 &''(62F54" 2F74"2F54" 2F74"23" ?!
?$*+"2F84" 2F84"2F84"*+6 =0$62GBBA4"2F54"2F74"2F84"Figure 3: The process of translation.
The source sentence is parsed and matching subtrees fromthe example database are retrieved.
From the examples, we extract translation hypotheses thancan contain optional target words and several position for each non-terminals.
For example thetranslation hypothesis containing ?textbook?
has three possible position for the non-terminal X3(as a left-child before ?a?, as a left-child after ?a?
or as a right-child).
The translation hypothesesare then combined during decoding.
Choice of optional words and final Non-Terminal positionsis also done during decoding.ample database online than it is to match pre-computed rules.
We use the techniques de-scribed in (Cromi?res and Kurohashi, 2011)to perform this step as efficiently as possible.Once we have found an example translation(s, t) for which s partially matches i, we pro-ceed to extract a translation hypothesis fromit.
A translation hypothesis is defined as ageneric translation rule for a part p of the in-put sentence that is represented as a target-language treelet, with non-terminals repre-senting the insertion positions for the transla-tions of other parts of the sentence.
A trans-lation hypothesis is created from a translationexample as follows:1.
We project the part of s that is matchedinto the target side t using the alignmentof s and t. This is trivial if each word ofs and t is aligned, but this is not typi-cally the case.
Therefore our translationhypotheses will often have some targetwords/nodes marked as optionals: thismeans that we will decide if they shouldbe added to the final translation only atthe moment of combination.2.
We insert the non-terminals as childnodes of the projected subtree.
This issimple if i, s and t have the same struc-ture and are perfectly aligned, but againthis is not typically the case.
A conse-quence is that we will sometimes have sev-eral possible insertion positions for eachnon-terminal.
The choice of insertion po-sition is again made during combination.4 DecodingAfter having extracted translation hypothesesfor as many parts of the input tree as possible,we need to decide how to select and combinethem.
Our approach here is similar to what81Figure 4: A translation hypothesis endodedas a lattice.
This representation allows us tohandle efficiently the ambiguities of our trans-lation rules.
Note that each path in this lat-tice corresponds to different choices of inser-tion position for X2, morphological forms of?be?, and the optional insertion of ?at?.has been proposed for Corpus-Based MachineTranslation.
We first choose a number of fea-tures and create a linear model scoring eachpossible combination of hypotheses (see Sec-tion 5).
We then attempt to find the combi-nation that maximizes this model score.The combination of rules is constrained bythe structure of the input dependency tree.
Ifwe only consider local features1, then a simplebottom-up dynamic programming approachcan efficiently find the optimal combinationwith linear O(|H|) complexity2.
However,non-local features (such as language models)will force us to prune the search space.
Thispruning is done efficiently through a varia-tion of cube-pruning (Chiang, 2007).
Weuse KenLM3 (Heafield, 2011) for computingthe target language model score.
Decodingis made more efficient by using some of themore advanced features of KenLM such asstate-reduction ((Li and Khudanpur, 2008),(Heafield et al., 2011)) and rest-cost estima-tions(Heafield et al., 2012).Compared with the original cube-pruningalgorithm, our decoder is designed to handlean arbitrary number of non-terminals.
In ad-dition, as we have seen in Section 3, the trans-lation hypotheses we initially extract from ex-amples are ambiguous in term of which targetword is going to be used and which will be thefinal position of each non-terminal.
In order tohandle such ambiguities, we use a lattice-basedinternal representation that can encode themefficiently (see Figure 4).
This lattice represen-tation also allows the decoder to make choicesbetween various morphological variations of a1The score of a combination will be the sum of thelocal scores of each translation hypothesis.2H = set of translation hypotheses3http://kheafield.com/code/kenlm/word (e.g.
be/is/are).5 Features and TuningDuring decoding we use a linear model to scoreeach possible combination of hypotheses.
Thislinear model is based on a linear combinationof both local features (local to each translationhypothesis) and non-local features (such as a5-gram language model score of the final trans-lation).
The decoder considers in total a com-bination of 34 features, a selection of which aregiven below.?
Example penalty and example size?
Translation probability?
Language model score?
Optional words added/removedThe optimal weights for each feature areestimated using the Pairwise Ranking Op-timization (PRO) algorithm (Hopkins andMay, 2011) and parameter optimization withMegaM4.
We use the implementation of PROthat is provided with the Moses SMT systemand the default settings of MegaM.6 ExperimentsIn order to evaluate our system, we conductedtranslation experiments on four languagepairs: Japanese-English (JA?EN), English-Japanese (EN?JA), Japanese-Chinese (JA?ZH) and Chinese-Japanese (ZH?JA).For Japanese-English, we evaluated on theNTCIR-10 PatentMT task data (patents)(Goto et al., 2013) and compared our systemwith the official baseline scores.
For Japanese-Chinese, we used parallel scientific paper ex-cerpts from the ASPEC5 corpus and com-pared against the same baseline system as forJapanese-English.
The corpora contain 3Mparallel sentences for Japanese-English and670K for Japanese-Chinese.The two baseline systems are based on theopen-source GIZA++/Moses pipeline.
Thebaseline labeled ?Moses?
uses the classicphrase-based engine, while ?Moses-Hiero?
usesthe Hierarchical Phrase-Based decoder.
These4http://www.umiacs.umd.edu/~hal/megam/5http://orchid.kuee.kyoto-u.ac.jp/ASPEC/82System JA?EN EN?JA JA?ZH ZH?JAMoses 28.86 33.61 32.90 42.79Moses-Hiero 28.56 32.98 ?
?Proposed 29.00 32.15 32.99 37.64Table 1: ScoresSystem BLEU TranslationMoses 31.09 Further, the expansion stroke, the sectional area of the inner tube 12,and the oil is supplied to the lower oil chamber S2 from the oil reservoirchamber R ?
stroke.Moses-Hiero21.49 Also, the expansion stroke, the cross-sectional area of the inner tube12 ?
stroke of oil supplied from the oil reservoir chamber R lower oilchamber S2.Proposed 44.99 Further in this expansion stroke, the oil at an amount obtained by mul-tiplying cross sectional area of the inner tube 12 from the oil reservoirchamber R is resupplied to the lower oil chamber S2.Reference 100.00 In this expansion stroke, oil in an amount obtained by multiplying thecross sectional area of the inner tube 12 by the stroke is resupplied fromthe upper oil reservoir chamber R to the lower oil chamber S2.Table 2: Example of JA?EN translation with better translation quality than baselines.correspond to the highest performing officialbaselines for the NTCIR-10 PatentMT task.As it appeared Moses was giving similarand slightly higher BLEU scores than Moses-Hiero for Japanese-English, we restricted eval-uation to the standard settings for Moses forour Japanese-Chinese experiments.The following dependency parsers wereused.
The scores in parentheses are the ap-proximate parsing accuracies (micro-average),which were evaluated by hand on a randomsubset of sentences from the test data.
Theparsers were trained on domains different tothose used in the experiments.?
English: NLParser6 (92%) (Charniak andJohnson, 2005)?
Japanese: KNP (96%) (Kawahara andKurohashi, 2006)?
Chinese: SKP (88%) (Shen et al., 2012)6.1 ResultsThe results shown are for evaluation on thetest set after tuning.
Tuning was conductedover 50 iterations on the development set usingan n-best list of length 500.Table 2 shows an example sentence showingsignificant improvement over the baseline.
In6Converted to dependency parses with in-housetool.particular, non-local structure has been pre-served by the proposed system, such as themodification of ?oil?
by the ?in an amount... bythe stroke?
phrase.
Another example is the in-correct location of ??
stroke?
in the Moses out-put.
The proposed system produces a muchmore fluent output than the hierarchical-basedbaseline Moses-Hiero.The proposed system also outperforms thebaseline for JA?ZH, however falls short forZH?JA.
We believe this is due to the low qual-ity of parsing for Chinese input.The decoder requires on average 0.94 sec-onds per sentence when loading from precom-piled hypothesis files.
As a comparison, Moses(default settings) takes 1.78 seconds per sen-tence, loading from a binarized and filteredphrase table.7 ConclusionThis paper introduces an example-basedtranslation system exploiting both source andtarget dependency analysis and online exam-ple retrieving, allowing the availability of fulltranslation examples at translation time.We believe that the use of dependency pars-ing is important for accurate translation acrossdistant language pairs, especially in settingssuch as ours with many long sentences.
Wehave designed a complete translation frame-83work around this idea, using dependency-parsed trees at each step from alignment toexample retrieval to example combination.The current performance (BLEU) of oursystem is similar to (or even slightly bet-ter than) state-of-the-art open-source SMTsystems.
As we have been able to obtainsteady performance improvements during de-velopment, we are hopeful that this trend willcontinue and we will shortly obtain even bet-ter results.
Future plans include enrichingthe feature set, adding a tree-based languagemodel and considering forest input for multi-ple parses to provide robustness against pars-ing errors.
When the code base is sufficientlystable, we intend to release the entire systemas open-source, in the hope of providing amore syntactically-focused alternative to ex-isting open-source SMT engines.AcknowledgementsThis work was partially supported by theJapan Science and Technology Agency.
Thefirst author is supported by a Japanese Gov-ernment (MEXT) research scholarship.
Wewould like to thank the anonymous reviewers.ReferencesEugene Charniak and Mark Johnson.
2005.Coarse-to-Fine n-Best Parsing and MaxEnt Dis-criminative Reranking.
In Proceedings of the43rd Annual Meeting of the Association forComputational Linguistics, ACL 2005.Fabien Cromi?res and Sadao Kurohashi.
2011.
Ef-ficient retrieval of tree translation examples forsyntax-based machine translation.
In Proceed-ings of the 2011 Conference on Empirical Meth-ods in Natural Language Processing.Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumitaand Benjamin Tsou.
2013.
Overview ofthe Patent Machine Translation Task at theNTCIR-10 Workshop.
In Proceedings of the 10thNTCIR Workshop Meeting on Evaluation of In-formation Access Technologies (NTCIR-10).Mark Hopkins and Jonathan May.
2011.
Tuningas Ranking.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural LanguageProcessing.Daisuke Kawahara and Sadao Kurohashi.
2006.A Fully-Lexicalized Probabilistic Model forJapanese Syntactic and Case Structure Anal-ysis.
In Proceedings of the Human LanguageTechnology Conference of the NAACL.Makoto Nagao.
1984.
A framework of a mechan-ical translation between Japanese and Englishby analogy principle.
In A. Elithorn and R.Banerji.
Artificial and Human Intelligence.Toshiaki Nakazawa and Sadao Kurohashi.
2012.Alignment by bilingual generation and mono-lingual derivation.
In Proceedings of COLING2012.Mo Shen, Daisuke Kawahara and Sadao Kuro-hashi.
2012.
A Reranking Approach for De-pendency Parsing with Variable-sized SubtreeFeatures.
In Proceedings of 26th Pacific AsiaConference on Language Information and Com-puting.Chris Callison-Burch, Colin Bannard, and JoshSchroeder.
Scaling phrase-based statistical ma-chine translation to larger corpora and longerphrases.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Lin-guistics, pages 255?262.
Association for Compu-tational Linguistics, 2005.David Chiang.
2007.
Hierarchical phrase-basedtranslation.
In Computational Linguistics.Kenneth Heafield.
2011.
KenLM: faster andsmaller language model queries.
In Proceedingsof the EMNLP 2011 Sixth Workshop on Statis-tical Machine Translation, 2011.Kenneth Heafield, Hieu Hoang, Philipp Koehn,Tetsuo Kiso, and Marcello Federico.
2011.Left language model state for syntactic ma-chine translation.
In Proceedings of the Inter-national Workshop on Spoken Language Trans-lation, 2011.Kenneth Heafield, Philipp Koehn, and Alon Lavie.2012.
Language model rest costs and space-efficient storage.
In Proceedings of the JointConference on Empirical Methods in NaturalLanguage Processing and Computational Natu-ral Language Learning, 2012.Zhifei Li and Sanjeev Khudanpur.
2008.
A scal-able decoder for parsing-based machine transla-tion with equivalent language model state main-tenance.
In Proceedings of the Second Workshopon Syntax and Structure in Statistical Transla-tion.
Association for Computational Linguistics,2008.Adam Lopez.
2007.
Hierarchical phrase-basedtranslation with suffix arrays.
In EMNLP-CoNLL 2007.Chris Quirk, Arul Menezes, and Colin Cherry.2005.
Dependency Treelet Translation: Syn-tactically Informed Phrasal SMT.
In Proceed-ings of the 43rd Annual Meeting on Associationfor Computational Linguistics.
Association forComputational Linguistics, 2005.84
