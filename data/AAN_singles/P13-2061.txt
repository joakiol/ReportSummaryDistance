Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 340?345,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsBilingual Data Cleaning for SMT using Graph-based Random Walk?Lei Cui?, Dongdong Zhang?, Shujie Liu?, Mu Li?, and Ming Zhou?
?School of Computer Science and TechnologyHarbin Institute of Technology, Harbin, Chinaleicui@hit.edu.cn?Microsoft Research Asia, Beijing, China{dozhang,shujliu,muli,mingzhou}@microsoft.comAbstractThe quality of bilingual data is a key factorin Statistical Machine Translation (SMT).Low-quality bilingual data tends to pro-duce incorrect translation knowledge andalso degrades translation modeling per-formance.
Previous work often used su-pervised learning methods to filter low-quality data, but a fair amount of humanlabeled examples are needed which arenot easy to obtain.
To reduce the re-liance on labeled examples, we proposean unsupervised method to clean bilin-gual data.
The method leverages the mu-tual reinforcement between the sentencepairs and the extracted phrase pairs, basedon the observation that better sentencepairs often lead to better phrase extractionand vice versa.
End-to-end experimentsshow that the proposed method substan-tially improves the performance in large-scale Chinese-to-English translation tasks.1 IntroductionStatistical machine translation (SMT) depends onthe amount of bilingual data and its quality.
Inreal-world SMT systems, bilingual data is oftenmined from the web where low-quality data is in-evitable.
The low-quality bilingual data degradesthe quality of word alignment and leads to the in-correct phrase pairs, which will hurt the transla-tion performance of phrase-based SMT systems(Koehn et al, 2003; Och and Ney, 2004).
There-fore, it is very important to exploit data quality in-formation to improve the translation modeling.Previous work on bilingual data cleaning ofteninvolves some supervised learning methods.
Sev-eral bilingual data mining systems (Resnik and?This work has been done while the first author was visit-ing Microsoft Research Asia.Smith, 2003; Shi et al, 2006; Munteanu andMarcu, 2005; Jiang et al, 2009) have a post-processing step for data cleaning.
Maximum en-tropy or SVM based classifiers are built to filtersome non-parallel data or partial-parallel data.
Al-though these methods can filter some low-qualitybilingual data, they need sufficient human labeledtraining instances to build the model, which maynot be easy to acquire.To this end, we propose an unsupervised ap-proach to clean the bilingual data.
It is intuitivethat high-quality parallel data tends to producebetter phrase pairs than low-quality data.
Mean-while, it is also observed that the phrase pairs thatappear frequently in the bilingual corpus are morereliable than less frequent ones because they aremore reusable, hence most good sentence pairs areprone to contain more frequent phrase pairs (Fos-ter et al, 2006; Wuebker et al, 2010).
This kind ofmutual reinforcement fits well into the frameworkof graph-based random walk.
When a phrase pairp is extracted from a sentence pair s, s is consid-ered casting a vote for p. The higher the numberof votes a phrase pair has, the more reliable of thephrase pair.
Similarly, the quality of the sentencepair s is determined by the number of votes castedby the extracted phrase pairs from s.In this paper, a PageRank-style random walk al-gorithm (Brin and Page, 1998; Mihalcea and Ta-rau, 2004; Wan et al, 2007) is conducted to itera-tively compute the importance score of each sen-tence pair that indicates its quality: the higher thebetter.
Unlike other data filtering methods, ourproposed method utilizes the importance scoresof sentence pairs as fractional counts to calculatethe phrase translation probabilities based on Maxi-mum Likelihood Estimation (MLE), thereby noneof the bilingual data is filtered out.
Experimen-tal results show that our proposed approach sub-stantially improves the performance in large-scaleChinese-to-English translation tasks.3402 The Proposed Approach2.1 Graph-based random walkGraph-based random walk is a general algorithmto approximate the importance of a vertex withinthe graph in a global view.
In our method, the ver-tices denote the sentence pairs and phrase pairs.The importance of each vertex is propagated toother vertices along the edges.
Depending on dif-ferent scenarios, the graph can take directed orundirected, weighted or un-weighted forms.
Start-ing from the initial scores assigned in the graph,the algorithm is applied to recursively compute theimportance scores of vertices until it converges, orthe difference between two consecutive iterationsfalls below a pre-defined threshold.2.2 Graph constructionGiven the sentence pairs that are word-alignedautomatically, an undirected, weighted bipartitegraph is constructed which maps the sentencepairs and the extracted phrase pairs to the ver-tices.
An edge between a sentence pair vertex anda phrase pair vertex is added if the phrase pair canbe extracted from the sentence pair.
Mutual re-inforcement scores are defined on edges, throughwhich the importance scores are propagated be-tween vertices.
Figure 1 illustrates the graph struc-ture.
Formally, the bipartite graph is defined as:G = (V,E)where V = S ?
P is the vertex set, S = {si|1 ?i ?
n} is the set of all sentence pairs.
P ={pj |1 ?
j ?
m} is the set of all phrase pairswhich are extracted from S based on the wordalignment.
E is the edge set in which the edgesare between S and P , thereby E = {?si, pj?|si ?S, pj ?
P, ?
(si, pj) = 1}.?
(si, pj) ={1 if pj can be extracted from si0 otherwise2.3 Graph parametersFor sentence-phrase mutual reinforcement, a non-negative score r(si, pj) is defined using the stan-dard TF-IDF formula:r(si, pj) ={ PF (si,pj)?IPF (pj)?p??{p|?
(si,p)=1} PF (si,p?
)?IPF (p?)
if ?
(si, pj) = 10 otherwiseSentence Pair VerticesPhrase Pair Verticess1s2s3p1p3p4p5p6p2Figure 1: The circular nodes stand for S andsquare nodes stand for P .
The lines capture thesentence-phrase mutual reinforcement.where PF (si, pj) is the phrase pair frequency ina sentence pair and IPF (pj) is the inverse phrasepair frequency of pj in the whole bilingual corpus.r(si, pj) is abbreviated as rij .Inspired by (Brin and Page, 1998; Mihalceaand Tarau, 2004; Wan et al, 2007), we com-pute the importance scores of sentence pairs andphrase pairs using a PageRank-style algorithm.The weights rij are leveraged to reflect the rela-tionships between two types of vertices.
Let u(si)and v(pj) denote the scores of a sentence pair ver-tex and a phrase pair vertex.
They are computediteratively by:u(si) = (1?d)+d?
?j?N(si)rij?k?M(pj) rkjv(pj)v(pj) = (1?d) +d?
?j?M(pj)rij?k?N(si) riku(si)where d is empirically set to the default value 0.85that is same as the original PageRank, N(si) ={j|?si, pj?
?
E}, M(pj) = {i|?si, pj?
?
E}.The detailed process is illustrated in Algorithm 1.Algorithm 1 iteratively updates the scores of sen-tence pairs and phrase pairs (lines 10-26).
Thecomputation ends when difference between twoconsecutive iterations is lower than a pre-definedthreshold ?
(10?12 in this study).2.4 ParallelizationWhen the random walk runs on some large bilin-gual corpora, even filtering phrase pairs that ap-pear only once would still require several days ofCPU time for a number of iterations.
To over-come this problem, we use a distributed algorithm341Algorithm 1 Modified Random Walk1: for all i ?
{0 .
.
.
|S| ?
1} do2: u(si)(0) ?
13: end for4: for all j ?
{0 .
.
.
|P | ?
1} do5: v(pj)(0) ?
16: end for7: ?
?
Infinity8: ?
threshold9: n?
110: while ?
>  do11: for all i ?
{0 .
.
.
|S| ?
1} do12: F (si)?
013: for all j ?
N(si) do14: F (si)?
F (si) + rij?k?M(pj) rkj?
v(pj)(n?1)15: end for16: u(si)(n) ?
(1?
d) + d ?
F (si)17: end for18: for all j ?
{0 .
.
.
|P | ?
1} do19: G(pj)?
020: for all i ?M(pj) do21: G(pj)?
G(pj) + rij?k?N(si) rik?
u(si)(n?1)22: end for23: v(pj)(n) ?
(1?
d) + d ?G(pj)24: end for25: ?
?
max(4u(si)||S|?1i=1 ,4v(pj)||P |?1j=1 )26: n?
n+ 127: end while28: return u(si)(n)||S|?1i=0based on the iterative computation in the Sec-tion 2.3.
Before the iterative computation starts,the sum of the outlink weights for each vertexis computed first.
The edges are randomly par-titioned into sets of roughly equal size.
Eachedge ?si, pj?
can generate two key-value pairsin the format ?si, rij?
and ?pj , rij?.
The pairswith the same key are summed locally and ac-cumulated across different machines.
Then, ineach iteration, the score of each vertex is up-dated according to the sum of the normalizedinlink weights.
The key-value pairs are gener-ated in the format ?si, rij?k?M(pj) rkj?
v(pj)?
and?pj , rij?k?N(si) rik?
u(si)?.
These key-value pairsare also randomly partitioned and summed acrossdifferent machines.
Since long sentence pairs usu-ally extract more phrase pairs, we need to normal-ize the importance scores based on the sentencelength.
The algorithm fits well into the MapRe-duce programming model (Dean and Ghemawat,2008) and we use it as our implementation.2.5 Integration into translation modelingAfter sufficient number of iterations, the impor-tance scores of sentence pairs (i.e., u(si)) are ob-tained.
Instead of simple filtering, we use thescores of sentence pairs as the fractional counts tore-estimate the translation probabilities of phrasepairs.
Given a phrase pair p = ?f?
, e?
?, A(f?)
andB(e?)
indicate the sets of sentences that f?
and e?appear.
Then the translation probability is definedas:PCW(f?
|e?)
=?i?A(f?)?B(e?)
u(si)?
ci(f?
, e?)?j?B(e?)
u(sj)?
cj(e?
)where ci(?)
denotes the count of the phrase orphrase pair in si.
PCW(f?
|e?)
and PCW(e?|f?)
arenamed as Corpus Weighting (CW) based transla-tion probability, which are integrated into the log-linear model in addition to the conventional phrasetranslation probabilities (Koehn et al, 2003).3 Experiments3.1 SetupWe evaluated our bilingual data cleaning ap-proach on large-scale Chinese-to-English machinetranslation tasks.
The bilingual data we usedwas mainly mined from the web (Jiang et al,2009)1, as well as the United Nations parallel cor-pus released by LDC and the parallel corpus re-leased by China Workshop on Machine Transla-tion (CWMT), which contain around 30 millionsentence pairs in total after removing duplicatedones.
The development data and testing data isshown in Table 1.Data Set #Sentences SourceNIST 2003 (dev) 919 open testNIST 2005 (test) 1,082 open testNIST 2006 (test) 1,664 open testNIST 2008 (test) 1,357 open testCWMT 2008 (test) 1,006 open testIn-house dataset 1 (test) 1,002 web dataIn-house dataset 2 (test) 5,000 web dataIn-house dataset 3 (test) 2,999 web dataTable 1: Development and testing data used in theexperiments.A phrase-based decoder was implementedbased on inversion transduction grammar (Wu,1997).
The performance of this decoder is simi-lar to the state-of-the-art phrase-based decoder inMoses, but the implementation is more straight-forward.
We use the following feature functionsin the log-linear model:1Although supervised data cleaning has been done in thepost-processing, the corpus still contains a fair amount ofnoisy data based on our random sampling.342dev NIST 2005 NIST 2006 NIST 2008 CWMT 2008 IH 1 IH 2 IH 3baseline 41.24 37.34 35.20 29.38 31.14 24.29 22.61 24.19(Wuebker et al, 2010) 41.20 37.48 35.30 29.33 31.10 24.33 22.52 24.18-0.25M 41.28 37.62 35.31 29.70 31.40 24.52 22.69 24.64-0.5M 41.45 37.71 35.52 29.76 31.77 24.64 22.68 24.69-1M 41.28 37.41 35.28 29.65 31.73 24.23 23.06 24.20+CW 41.75 38.08 35.84 30.03 31.82 25.23 23.18 24.80Table 2: BLEU(%) of Chinese-to-English translation tasks on multiple testing datasets (p < 0.05), where?-numberM?
denotes we simply filter number million low scored sentence pairs from the bilingual dataand use others to extract the phrase table.
?CW?
means the corpus weighting feature, which incorporatessentence scores from random walk as fractional counts to re-estimate the phrase translation probabilities.?
phrase translation probabilities and lexicalweights in both directions (4 features);?
5-gram language model with Kneser-Neysmoothing (1 feature);?
lexicalized reordering model (1 feature);?
phrase count and word count (2 features).The translation model was trained over theword-aligned bilingual corpus conducted byGIZA++ (Och and Ney, 2003) in both directions,and the diag-grow-final heuristic was used to re-fine the symmetric word alignment.
The languagemodel was trained on the LDC English GigawordVersion 4.0 plus the English part of the bilingualcorpus.
The lexicalized reordering model (Xionget al, 2006) was trained over the 40% randomlysampled sentence pairs from our parallel data.Case-insensitive BLEU4 (Papineni et al, 2002)was used as the evaluation metric.
The parame-ters of the log-linear model are tuned by optimiz-ing BLEU on the development data using MERT(Och, 2003).
Statistical significance test was per-formed using the bootstrap re-sampling methodproposed by Koehn (2004).3.2 BaselineThe experimental results are shown in Table 2.
Inthe baseline system, the phrase pairs that appearonly once in the bilingual data are simply dis-carded because most of them are noisy.
In ad-dition, the fix-discount method in (Foster et al,2006) for phrase table smoothing is also used.This implementation makes the baseline systemperform much better and the model size is muchsmaller.
In fact, the basic idea of our ?one count?cutoff is very similar to the idea of ?leaving-one-out?
in (Wuebker et al, 2010).
The results show??
??
?
?
?
?uncharted waters??
??
?
?
?
?unexplored new areasweijing tansuo de xin lingyuFigure 2: The left one is the non-literal translationin our bilingual corpus.
The right one is the literaltranslation made by human for comparison.that the ?leaving-one-out?
method performs al-most the same as our baseline, thereby cannotbring other benefits to the system.3.3 ResultsWe evaluate the proposed bilingual data clean-ing method by incorporating sentence scores intotranslation modeling.
In addition, we also com-pare with several settings that filtering low-qualitysentence pairs from the bilingual data based onthe importance scores.
The last N = { 0.25M,0.5M, 1M } sentence pairs are filtered before themodeling process.
Although the simple bilin-gual data filtering can improve the performance onsome datasets, it is difficult to determine the bor-der line and translation performance is fluctuated.One main reason is in the proposed random walkapproach, the bilingual sentence pairs with non-literal translations may get lower scores becausethey appear less frequently compared with thoseliteral translations.
Crudely filtering out these datamay degrade the translation performance.
For ex-ample, we have a sentence pair in the bilingualcorpus shown in the left part of Figure 2.
Althoughthe translation is correct in this situation, translat-ing the Chinese word ?lingyu?
to ?waters?
appearsvery few times since the common translations are?areas?
or ?fields?.
However, simply filtering outthis kind of sentence pairs may lead to some lossof native English expressions, thereby the trans-343lation performance is unstable since both non-parallel sentence pairs and non-literal but parallelsentence pairs are filtered.
Therefore, we use theimportance score of each sentence pair to estimatethe phrase translation probabilities.
It consistentlybrings substantial improvements compared to thebaseline, which demonstrates graph-based randomwalk indeed improves the translation modelingperformance for our SMT system.3.4 DiscussionIn (Goutte et al, 2012), they evaluated phrase-based SMT systems trained on parallel data withdifferent proportions of synthetic noisy data.
Theysuggested that when collecting larger, noisy par-allel data for training phrase-based SMT, clean-ing up by trying to detect and remove incor-rect alignments can actually degrade performance.Our experimental results confirm their findingson some datasets.
Based on our method, some-times filtering noisy data leads to unexpected re-sults.
The reason is two-fold: on the one hand,the non-literal parallel data makes false positive innoisy data detection; on the other hand, large-scaleSMT systems is relatively robust and tolerant tonoisy data, especially when we remove frequency-1 phrase pairs.
Therefore, we propose to integratethe importance scores when re-estimating phrasepair probabilities in this paper.
The importancescores can be considered as a kind of contributionconstraint, thereby high-quality parallel data con-tributes more while noisy parallel data contributesless.4 Conclusion and Future WorkIn this paper, we develop an effective approachto clean the bilingual data using graph-based ran-dom walk.
Significant improvements on severaldatasets are achieved in our experiments.
Forfuture work, we will extend our method to ex-plore the relationships of sentence-to-sentence andphrase-to-phrase, which is beyond the existingsentence-to-phrase mutual reinforcement.AcknowledgmentsWe are especially grateful to Yajuan Duan, HongSun, Nan Yang and Xilun Chen for the helpful dis-cussions.
We also thank the anonymous reviewersfor their insightful comments.ReferencesSergey Brin and Lawrence Page.
1998.
The anatomyof a large-scale hypertextual web search engine.Computer networks and ISDN systems, 30(1):107?117.Jeffrey Dean and Sanjay Ghemawat.
2008.
Mapre-duce: simplified data processing on large clusters.Communications of the ACM, 51(1):107?113.George Foster, Roland Kuhn, and Howard Johnson.2006.
Phrasetable smoothing for statistical machinetranslation.
In Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 53?61, Sydney, Australia, July.
As-sociation for Computational Linguistics.Cyril Goutte, Marine Carpuat, and George Foster.2012.
The impact of sentence alignment errors onphrase-based machine translation performance.
InProceedings of AMTA 2012, San Diego, California,October.
Association for Machine Translation in theAmericas.Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu,and Qingsheng Zhu.
2009.
Mining bilingual datafrom the web with adaptively learnt patterns.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP, pages 870?878, Suntec, Singapore, August.Association for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT-NAACL 2003 Main Papers, pages48?54, Edmonton, May-June.
Association for Com-putational Linguistics.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Rada Mihalcea and Paul Tarau.
2004.
Textrank:Bringing order into texts.
In Dekang Lin and DekaiWu, editors, Proceedings of EMNLP 2004, pages404?411, Barcelona, Spain, July.
Association forComputational Linguistics.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguis-tics, 31(4):477?504.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4):417?449.344Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 160?167, Sap-poro, Japan, July.
Association for ComputationalLinguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.
Association for Computa-tional Linguistics.Philip Resnik and Noah A Smith.
2003.
The webas a parallel corpus.
Computational Linguistics,29(3):349?380.Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.2006.
A dom tree alignment model for mining paral-lel data from the web.
In Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 489?496, Sydney,Australia, July.
Association for Computational Lin-guistics.Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007.Towards an iterative reinforcement approach for si-multaneous document summarization and keywordextraction.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 552?559, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Joern Wuebker, Arne Mauser, and Hermann Ney.2010.
Training phrase translation models withleaving-one-out.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 475?484, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Max-imum entropy based phrase reordering model forstatistical machine translation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 521?528,Sydney, Australia, July.
Association for Computa-tional Linguistics.345
