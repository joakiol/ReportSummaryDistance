Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 196?204,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsRanking the annotators: An agreement study on argumentation structureAndreas PeldszusApplied Computational LinguisticsUniversity of Potsdampeldszus@uni-potsdam.deManfred StedeApplied Computational LinguisticsUniversity of Potsdamstede@uni-potsdam.deAbstractWe investigate methods for evaluatingagreement among a relatively large groupof annotators who have not received exten-sive training and differ in terms of abilityand motivation.
We show that it is possi-ble to isolate a reliable subgroup of anno-tators, so that aspects of the difficulty ofthe underlying task can be studied.
Ourtask is to annotate the argumentative struc-ture of short texts.1 IntroductionScenarios for evaluating annotation experimentsdiffer in terms of the difficulty of the task, thenumber of annotators, and the amount of trainingthat annotators receive.
For simple tasks, crowd-sourcing involving very many annotators has re-cently attracted attention.1 For more difficulttasks, the standard setting still is to work withtwo or a few more annotators, train them well,and compute agreement, usually in terms of thekappa measure.
In this paper, we study a dif-ferent scenario, which may be called ?classroomannotation?
: The group of annotators is bigger(in our example, 26), and there are no extensivetraining sessions: Students receive detailed writ-ten guidelines, there is a brief QA period, and an-notation starts.
In such a setting, one has to expectsome agreement problems that are due to differentabilities and different motivation of the students.Our goal is to develop methods for systematicallystudying the annotation results in such groups, toidentify more or less competent subgroups, yet atthe same time also learn about the difficulty of var-ious aspects of the underlying annotation task.
Tothis end, we investigate ways of ranking and clus-tering annotators.1See, for instance, Snow et al(2008) or Bhardwaj et al(2010) for strategies to analyse and cope with diverging per-formance of annotators in that scenario.Our task is the annotation of argumentation inshort texts, which is somewhat similar to mark-ing the rhetorical structure, e.g.
in terms of RST(Mann and Thompson, 1988; Carlson et al 2003).Thus we are dealing with a relatively difficult taskinvolving text interpretation.
We devised an an-notation scheme (which is more fully describedelsewhere), and in order to study the feasibility,first ran experiments with short hand-crafted textsthat collectively cover all the relevant phenom-ena.
This is the setting we report in this paper.
Aseparate step for future work is guideline revisionon the basis of the results, and then applying thescheme to authentic argumentative text (e.g., usergenerated content on various websites).2 A theory of argumentation structureFollowing up on Toulmin?s (1958) influential anal-ysis of argument, Freeman (1991; 2011) workedon integrating those ideas into the argument dia-graming techniques of the informal logic tradition.Freeman?s central idea is to model argumentationas a hypothetical dialectical exchange between aproponent, who presents and defends claims, anda challenger (the ?opponent?
), who critically ques-tions them in a regimented fashion.
Every movein such a basic dialectical situation correspondsto a structural element in the argument diagram.The analysis of an argumentative text is thus con-ceived as finding the corresponding critical ques-tion of the challenger that is answered by a partic-ular segment of the text.Since the focus of this paper is on the evalu-ation methodology, we provide here only a briefsketch of the scheme; for a detailed descriptionwith many examples, see Peldszus and Stede (toappear).
Premises and conclusions are proposi-tions expressed in the text segments.
We cangraphically present an argument as an argumentdiagram, with propositions as nodes and the vari-ous relations as arrows linking either two nodes or196Figure 1: Example of an argumentation structureannotation for a short texta node and a link2.
See figure 1 for an example.Notice that segments in favor of the proponent?sposition are drawn in circles, whereas the chal-lenger?s perspective is given in boxes.
The rootof an argument tree is the central statement madein the text.
In the example, it is expressed both insegment 1 and in segment 8; the = indicates thatthe annotator judges the contributions of the twosegments as equivalent, which can happen for anynode in the tree.
Segments 2, 4, and 9 providesupport to the central statement, which is the mostsimple configuration.
(1) [We should tear the building down.
]1 [It is fullof asbestos.
]2Support can be serial (transitive), when a support-ing statement in turn receives support from an-other one.
E.g., example (1) could be continuedwith .
.
.
[The report of the commission made thatvery clear.
]3.If an argument involves multiple premises thatsupport the conclusion only if they are taken to-gether, we have a linked structure in Freeman?s ter-minology.
On its own none of the linked premiseswould be able to support the conclusion.
In thebasic dialectical situation, a linked structure is in-duced by the challenger?s question as to why apremise is relevant to the claim.
The proponentthen answers by presenting another premise expli-cating the connection.
Building linked structure isthus to be conceived as completing an argument.As an example, consider the following continu-ation of example (1) .
.
.
[All buildings with haz-ardous materials should be demolished.
]3 .
Linkedsupport is shown in the diagram by connecting thepremises before they link to the conclusion.Two more configurations, which turn up in Fig-ure 1, are the attacking relations (all with a cir-cled arrowhead): undercut and rebuttal.
The for-2When an artificial node is introduced in such places, astandard tree representation results.mer (segment 5) denies the relevance of a statedrelation, here: the support that 4 lends to 1=8.
Theopponent does not dispute the truth of 4 itself butchallenges the idea that it can in fact lend supportto 1=8.
We draw it as an attack arrow pointingat the relation in question.
In contrast, a rebut-tal directly challenges the truth of a statement.
Inthe example, the annotator first decided that seg-ments 6 and 7 play a joint role for the argumen-tation (this is the step of merging two segments)and then marked them as the proponent?s rebuttalof the challenger?s statement 5.3 Annotation Experiment3.1 GuidelinesWe developed annotation guidelines based on thetheory presented in Section 2.
The guidelines(6 pages) contain text examples and the cor-responding graphs for all basic structures, andthey present different combinations of attack andcounter-attack.
The annotation process is dividedinto three steps: First, one segment is identified asthe central claim of the text.
The annotator thenchooses the dialectical role (proponent or oppo-nent) for all remaining segments.
Finally, the argu-mentative function of each segment (is it support-ing or attacking) and the corresponding subtypeshave to be determined, as well as the targeted seg-ment.3.2 DataApplying the scheme demands a detailed, deep un-derstanding of the text, which is why we chooseto first evaluate this task on short and controlledinstances of argumentation.
For this purpose webuilt a set of 23 constructed German texts, whereeach text consists of only five discourse segments.While argumentative moves in authentic texts areoften surrounded by material that is not directlyrelevant to the argumentation, such as factualbackground information, elaborations or rhetori-cal decoration, in the constructed texts all seg-ments are clearly argumentative, i.e.
they eitherpresents the central claim, a reason, an objectionor a counter-attack.
Merging segments and identi-fying restatements is thus not necessary.
The textscover several combinations of the basic constructsin different linearisations, typically one centralclaim, two (simple, combined or exemplifying)premises, one objection (rebutting a premise, re-butting the conclusion or undercutting the link be-197tween them) and a possible reaction (rebutting orundercutting counter-attacks, or a new reason thatrenders the objection uncountered).
A (translated)example of a micro text is given in (2).
In thequestionaire the order of the texts has been ran-domized.
(2) [Energy-saving light bulbs contain a con-siderable amount of toxic substances.
]1 [Acustomary lamp can for instance containup to five milligrams of quicksilver.
]2 [Forthis reason, they should be taken off themarket,]3 [unless they are virtually unbreak-able.
]4 [This, however, is simply not case.
]53.3 ProcedureThe annotation experiment was carried out in thecontext of an undergraduate university course with26 students, participation was obligatory.
The an-notators only received minimal training: A shortintroduction (5 min.)
was given to set the topic.After studying the guidelines (?30 min.)
and avery brief question-answering, the subjects anno-tated the 23 texts (?45 min.
), writing their analysisas an argumentative graph in designated areas ofthe questionaire.4 Evaluation4.1 PreparationsSince the annotators were asked to assign one andonly one function to each segment, every node inthe argumentative graph has exactly one out-goingarc.
The graph can thus be reinterpreted as a listof segment labels.Every segment is labeled on different levels:The ?role?-level specifies the dialectical role (pro-ponent or opponent).
The ?typegen?-level specifiesthe general type, i.e.
whether the segment presentsthe central claim (thesis) of the text, supports orattacks another segment.
The ?type?-level addi-tionally specifies the kind of support (normal orexample) and the kind of attack (rebutter or un-dercutter).
Whether a segment?s function holdsonly in combination with that of another segment(combined) or not (simple) is represented on the?combined?-level.3 The target is finally specifiedby the segment identifier (1 .
.
.
5) or relation iden-tifier (a .
.
.
d) on the ?target?-level.The labels of each separate level can be mergedto form a complex tagset.
We interpret the result3This is roughly equivalent to Freeman?s ?linkedpremises?.as a hierarchical tagset as it is presented in Fig-ure 2.4 The label ?PSNC(3)?
for example standsfor a proponent?s segment, giving normal supportto segment 3 in combination with another seg-ment, while ?OAUS(b)?
represents an opponent?ssegment, undercutting a relation b, not combined.Due to space and readability constraints, we fo-cus the detailed discussion of the experiment?s re-sult on the ?role+type?-level.
Still, general resultswill be reported for all levels.Another question that arises before evaluation,especially in our setting, is how to deal with miss-ing annotations, since measuring inter-annotatoragreement with a ?-like coefficient requires a deci-sion of every annotator (or at least the same num-ber of annotators) on each item.
One way to copewith this is to exclude annotators with missing an-notations, another to exclude items that have notbeen annotated by every subject.
In our exper-iment only 11 of the 26 subjects annotated ev-ery segment.
Another 10 annotated at least 90%of the segments, five annotated less.
Excludingsome annotators would be possible in our setting,but keeping only 11 of 26 is unacceptable.
Ex-cluding items is also inconvenient given the smalldataset.
We thus chose to mark segments withmissing annotations as such in the data, augment-ing the tagset with the label ???
for missing anno-tations.
We are aware of the undesired possibilitythat two annotators ?agree?
on not assigning a cat-egory to a segment.
Still, we can decide to onlyexclude those annotators who omitted many deci-sions, and to measure agreement for the remainingones, thereby reducing the risk of false agreement.4.2 IAA over all annotatorsThe agreement in terms of Fleiss?s ?
(Fleiss,1971)5 of all annotators on the different levels isshown in Table 1.
For the complex levels we ad-ditionally report Krippendorff?s ?
(Krippendorff,1980) as a weighted measure of agreement.
Weuse the distance between two tags in the tag hier-archy to weigh the confusion (similar to Geertzenand Bunt (2006)), in order to capture the intuitionthat confusing, e.g., PSNC with PSNS is less se-vere than confusing it with OAUS.According to the scale of Krippendorff (1980),4Notice that this hierarchy is implicit in the annotationprocess, yet the annotators were neither confronted with adecision-tree version nor the labels of this tag hierarchy.5A generalisation of Scott?s pi (Scott, 1955) for more thantwo annotators, as Artstein and Poesio (2008) pointed out.198Figure 2: The hierarchy of segment labels.level #cats ?
AO AE ?
DO DErole 2 0.521 0.78 0.55typegen 3 0.579 0.72 0.33type 5 0.469 0.61 0.26comb 2 0.458 0.73 0.50target (9) 0.490 0.58 0.17role+typegen 5 0.541 0.66 0.25 0.534 0.28 0.60role+type 9 0.450 0.56 0.20 0.500 0.33 0.67role+type+comb 15 0.392 0.49 0.16 0.469 0.38 0.71role+type+comb+target (71) 0.384 0.44 0.08 0.425 0.45 0.79Table 1: Agreement for all 26 annotators on 115 items for the different levels.
The number of categorieson each level (without ???)
is shown in the second column (possible target categories depend on textlength).
We report Fleiss?s ?
with the associated observed (AO) and expected agreement (AE).
Weightedscores were calculated using Krippendorff?s ?, with observed (DO) and expected disagreement (DE).the annotators in our experiment did neitherachieve reliable (?
?
0.8) nor marginally reli-able (0.67 ?
?
< 0.8) agreement.
On the scaleof Landis and Koch (1977), most results can beinterpreted to show moderate correlation (0.4 <?
?
0.6), only the two most complex levels fallout.
Considering weighted scores for those com-plex levels, all fall into the window of moderatecorrelation.While typical results in discourse structure tag-ging usually reach or exceed the 0.7 threshold6 ,we expected lower results for three reasons: firstthe minimal training of the naive annotators onlybased on the guidelines, second the varying com-mitment to the task of the annotators in the con-strained setting and finally the nature of the task,which requires a precise specification of the anno-tators interpretation of the texts.When it comes to investigation of the reasonsof disagreement, the informativeness of a singleinter-annotator agreement value is limited.
Wewant to identify sources of disagreement in boththe set of annotators as well as the categories.
To6Agreement of professional annotators on 16 rhetoricalrelations was ?=0.64 in the beginning and 0.82 after extensivetraining (Carlson et al 2003).
Agreement on ?argumentativezones?
is reported ?=0.71 for trained annotators with detailedguidelines, another study for untrained annotators with onlyminimalistic guidelines reported values varying between 0.35and 0.72 (depending on the text), see Teufel (2010).cat.
??
n AO AEPT +0.265 572 0.91 0.69PSE +0.128 112 0.97 0.93PSN +0.082 1075 0.79 0.54OAR ?0.027 430 0.86 0.75PAR ?0.148 173 0.92 0.89OSN ?0.198 153 0.93 0.90OAU ?0.229 172 0.92 0.89PAU ?0.240 138 0.93 0.91OSE ?0.451 2 0.99 0.99Table 3: Krippendorff?s category definition diag-nostic for the level ?role+type?, base ?=0.45.this end, contingency tables (confusion matrices)are studied, which show the number of categoryagreements and confusions for a pair of annota-tors.
However, the high number of annotators inour study makes this strategy infeasible, as thereare 325 different pairs of annotators.
One solutionto still get an overview of typical category con-fusions, is to build an aggregated confusion ma-trix, which sums up the values of category pairsacross all 325 normal confusion matrices.
As pro-posed in Cinkova?
et al(2012), we derive a confu-sion probability matrix from this aggregated ma-trix, which is shown in Table 2.
It specifies theconditional probability that one annotator will an-notate an item with categorycolumn , given that an-other has chosen categoryrow , so the rows sum upto 1.
The diagonal cells display the probability ofagreement for each category.199PT PSN PSE PAR PAU OSN OSE OAR OAU ?PT 0.625 0.243 0.005 0.003 0.002 0.006 0.000 0.030 0.007 0.078PSN 0.123 0.539 0.052 0.034 0.046 0.055 0.001 0.052 0.021 0.078PSE 0.024 0.462 0.422 0.007 0.008 0.000 0.000 0.015 0.001 0.061PAR 0.007 0.164 0.004 0.207 0.245 0.074 0.000 0.156 0.072 0.071PAU 0.007 0.264 0.005 0.290 0.141 0.049 0.000 0.117 0.075 0.052OSN 0.016 0.292 0.000 0.081 0.046 0.170 0.004 0.251 0.075 0.065OSE 0.000 0.260 0.000 0.000 0.000 0.260 0.000 0.240 0.140 0.100OAR 0.033 0.114 0.004 0.070 0.044 0.102 0.001 0.339 0.218 0.076OAU 0.017 0.101 0.000 0.069 0.061 0.066 0.002 0.469 0.153 0.063?
0.179 0.351 0.031 0.066 0.041 0.055 0.001 0.157 0.061 0.057Table 2: Confusion probability matrix over all 26 annotators for the level ?role+type?.category pair ??
AO AEOAR+OAU +0.048 0.61 0.22PAR+PAU +0.026 0.59 0.21OAR+OSN +0.018 0.58 0.22PSN+PSE +0.012 0.59 0.23OAR+PAR +0.007 0.58 0.22PSN+OSN +0.007 0.59 0.24PAR+OSN +0.005 0.57 0.21Table 4: Krippendorff?s category distinction diag-nostic for the level ?role+type?, base ?=0.45.Krippendorff (1980) proposed another way toinvestigate category confusions by systematicallycomparing the agreement on the original categoryset with the agreement on a reduced category set.There are two different methods to collapse cat-egories: The first is the category definition test,where all but the one category of interest are col-lapsed together, yielding a binary category distinc-tion.
When measuring the agreement with this bi-nary distinction only confusions between the cat-egory of interest and the rest count, but no confu-sions between the collapsed categories.
If agree-ment increases for the reduced set compared to theoriginal set, that category of interest is better dis-tinguished than the rest of the categories.
As Ta-ble 3 shows, the highest distinguishability is foundfor PT, PSN and PSE.
Rebutters are better distin-guished for the opponent role than for the propo-nent role.
Undercutters seem equally problematicfor both roles.
The extreme value for OSE is notsurprising, given that this category was not sup-posed to be found in the dataset and was only usedtwice.
It shows, though, that the results of this testhave to be interpreted with caution for rare cate-gories, since in these cases the collapsed rest al-ways leads to a very high chance agreement.The other of Krippendorff?s diagnostics is thecategory distinction test, where two categories arecollapsed in order to measure the impact of con-fusions between them on the overall agreementvalue.
The higher the difference, the greater theconfusion between the two collapsed categories.Table 4 shows the result for some category pairs.The highest gain is found between rebutting andundercutting attacks on the opponents side: Giventhe base ?=0.45, the +0.048 increase means a po-tential improvement of 10% if these confusionscould be reduced.
However, distingishing rebut-ters and undercutters often depends on interpreta-tion and we consider it unlikely to reach perfectagreement on that decision.4.3 Comparison with gold dataWe now compare the result of the annotation ex-periment with the gold annotation.
For each an-notator and for each level of annotation, we cal-culated the F1 score, macro-averaged over the cat-egories of that level.
Figure 3 shows the distri-bution of those values as boxplots.
We observevarying degrees of difficulty on the basic levels:While the scores on the ?role?
and ?typegen?
arerelatively dense between 0.8 and 0.9, the distribu-tion is much wider and also generally lower for?type?, ?comb?
and ?target?.
Especially remarkableis the drop of the median when comparing ?type-gen?
with ?type?
: For the simpler level, all valuesof the better half of annotators lie above 0.85, butfor the more complex level, which also requiresthe distinction between rebutters and undercutters,the median drops to 0.67.
The figure also showsthe pure F1 score for identifying the central claim(PT).
While the larger part of the annotators per-forms well in this task, there are still some be-low 0.7.
This is remarkable, since identifying onesegment as the central claim of a five-segment textdoes not appear to be a challenging task.4.4 Ranking and clustering the annotatorsUntil now we have mainly investigated the tagsetas a factor in measuring agreement.
Thewidespread distribution of annotator scores in thecomparison with gold data however showed that200roletypegentypecombtargetrole+typegenrole+typero+ty+coro+ty+co+tacentral-claim0.00.10.20.30.40.50.60.70.80.91.0Figure 3: Comparison with gold annotation: Foreach level we show a boxplot of the F1 scoresof all annotators (each score macro-averaged overcategories of that level).
Also, we present the F1score for the recognition of the central claim.their performance differs greatly.
As described inSection 3.3, participation in the study was obliga-tory for our subjects (students in class).
We thuswant to make sure that the differences in perfor-mance are a result of the annotator?s varying com-mitment to the task, rather than a result of pos-sible ambiguities or flaws of the guidelines.
Theinter-annotator agreement values presented in Ta-ble 1 are not so helpful for answering this ques-tion, as they only provide us with an average mea-sure, but not with an upper and lower bound ofwhat is achievable with our annotators.
Conse-quently, the goal of this section is to give structureto the set of annotators, to impose a (partial) or-der on it or even divide it into different groups andinvestigate their characteristic confusions.Central claim: During the conversion of thewritten graphs into segment label squences, it be-came obvious that certain annotators nearly al-ways chose the first segment of the text as thecentral claim, even in cases where it was fol-lowed by a consecutive clause with a discoursemarker.
Therefore, our first heuristic was to im-pose an order on the set of annotators accordingto their F1 score in identifying the central claim.This not only identifies those outliers but can ad-ditionally serve as a rough indicator of text un-derstanding.
Although this ordering requires golddata, producing gold data for the central claim of atext is relatively simple and using them only givesminimal bias in the evaluation (in contrast to e.g.5 10 15 20 250.30.40.50.60.70.80.91.0role+type+comb+targetrole+type+combtargettypegenrolerole+typecombtyperole+typegenFigure 4: Agreement in ?
on the different levelsfor the n-best annotators ordered by their F1 scorein identifying the central claim.?role+type?
F1 score as a sorting criterion).
Withthis ordering we can then calculate agreement ondifferent subsets of the annotators, e.g.
only forthe two best annotators, for the ten best or for all.Figure 4 shows ?
on the different levels for all n-best groups of annotators: From the two best to thesix best annotators the results are quite stable.
Thesix best annotators achieve an encouraging ?=0.74on the ?role+type?
level and likewise satisfactory?=0.69 for the full task, i.e.
on the maximallycomplex ?role+type+comb+target?
level.
For in-creasingly larger n-best groups, the agreement de-creases steadily with only minor fluctuations.
Al-though the central claim F1 score proves to be auseful sorting criterion here, it might not work aswell for authentic texts, due to the possibility ofrestated, or even implicit central claims.Category distributions: Investigating the an-notator bias is also a promising way to imposestructure onto the group of annotators.
A lookon the individual distribution of categories per an-notator quickly reveals that there are some devia-tions.
Table 5 shows the individual distributionsfor the ?role+type?-level, as well as the averageannotator distribution and that found in the golddata.
We focus on three peculiarities here.
First,both annotators A18 and A21 refrain from classi-fying segments as attacking.
Although they makethe distinction between the roles, they give onlysupporting segments.
Checking the annotationsshows that they must have mixed the concepts ofdialectical role and argumentative function.
An-other example is the group of A04, A20 and A23,who refrain from using proponent attacks.
Al-201anno PT PSN PSE PAR PAU OSN OSE OAR OAU ?
?gold ?
?A01 23 40 5 13 0 6 0 24 0 4 17 15.6A02 22 33 7 8 11 3 0 23 1 7 17 16.9A03 23 40 6 4 12 5 0 16 9 0 7 11.8A04 21 52 6 1 0 0 0 14 11 10 25 20.5A05 23 42 5 15 2 5 0 20 3 0 10 14.2A06 24 39 6 6 9 7 0 15 9 0 7 10.9A07 22 41 1 12 8 5 0 13 8 5 13 9.4A08 23 35 6 6 14 6 1 17 7 0 9 13.3A09 23 43 2 6 7 7 0 15 12 0 9 10.8A10 23 51 3 3 4 8 0 8 15 0 21 21.2A11 21 41 3 2 1 1 0 22 9 15 21 16.6A12 23 42 6 15 5 3 0 13 4 4 13 11.7A13 23 40 4 16 0 7 0 17 8 0 14 13.3A14 19 33 6 10 4 4 0 11 8 20 26 20.2A15 19 37 2 6 7 3 0 18 3 20 20 16.9A16 20 31 4 7 10 7 0 14 5 17 22 16.9A17 22 53 2 4 3 0 0 20 6 5 17 15.1A18 23 51 5 0 0 34 1 0 1 0 39 40.4A19 24 41 7 13 2 5 0 20 3 0 10 14.5A20 21 41 4 0 1 2 0 31 5 10 22 18.2A21 16 40 0 1 0 20 0 0 1 37 52 44.8A22 22 34 7 5 10 6 0 17 9 5 12 10.3A23 23 52 0 1 0 0 0 32 6 1 24 27.1A24 23 41 6 6 9 5 0 22 3 0 4 11.8A25 23 38 4 5 15 0 0 7 23 0 24 27.1A26 23 44 5 8 4 4 0 21 3 3 9 10.2?
22.0 41.3 4.3 6.7 5.3 5.9 0.1 16.5 6.6 6.3gold 23 42 6 6 8 5 0 19 6 0Table 5: Distribution of categories for each annotator in absolute numbers for the ?role+type?
level.The last two rows display gold and average annotator distribution for comparison.
The two right-most columns specify for each annotator the total difference to gold or average distribution ?gold/?
=12?c?gold/?c .though they make the distinction between the ar-gumentative functions of supporting and attack-ing, they do not systematically attribute counter-attacks to the proponent.
Finally, as pointed outbefore, there are several annotators with a differentamount of missing annotations.
Note, that missingannotations must not necessarily signal an unmo-tivated annotator (who skips an item if deciding onit is too tedious).
It could very well also be a dili-gent but slow annotator.
Still, missing annotationslead to lower agreement in most cases, so filteringout the severe cases might be a good idea.
Mostof the annotators showing deviations in categorydistribution could be identified, if annotators aresorted by deviation from average distribution ?
?,which is shown in the last column of Table 5.
Fil-tering out the 7 worst annotators in terms of ?
?,the resulting ?
increases from 0.45 to 0.54 on the?role+type?-level, which is nearly equal to the 0.53achieved when using the same size of annotator setin the central claim ordering.
Although this order-ing suffices to detect outliers in the set of annota-tors without relying on gold data, it still has twodrawbacks: It only maximizes to the average andwill thus not garantuee best agreement scores forthe smaller n-best sets.
Furthermore a more gen-eral critique on total orders of annotators: Thereare various ways in which a group agrees or dis-A21A20A04A18A25A10A09A11A15A16A07A23A14A22A17A01A13A26A06A02A08A24A03A12A05A190.90.80.70.60.50.4Figure 5: Clustering of the annotators (on the x-axis) for the ?role+type?
level.
The y-axis speci-fies the distance between the clusters, i.e.
the ?reached by the annotators of both clusters.agrees simultaneously that might not be linearizedthis way.
Luckily, a better solution is at hand.Agglomerative hierarchical clustering: Weapply hierarchical clustering in order to investi-gate the structure of agreement in the set of an-notators.
The clusters are initialized as singletonsfor each annotator.
Then agreement is calculatedfor all possible pairs of those clusters.
The pair ofclusters with highest agreement is merged.
Thisprocedure is iterated until there is only one clusterleft.
In contrast to normal clustering, the linkage202criterion does not determine the distance betweencomplex clusters indirectly as function of the dis-tance between singleton clusters, but directly mea-sures agreement for the unified set of annotators ofboth clusters.
Figure 5 shows the clustering on the?role+type?-level.
It not only gives an impressionof the possible range of agreement, but also allowsus to check for ambiguities in the guidelines: Ifthere were stable alternative readings in the guide-lines, we would expect multiple larger clusters thatcan only be merged at a lower level of ?.
As theFigure shows, the clustering grows steadily, maxi-mally incorporating clusters of two annotators, sowe do not see the threat of ambiguity in the guide-lines.
Furthermore, the clustering conforms withcentral claim ordering in picking out the same setof six reliable and good annotators (with an aver-age F1 of 0.76 for ?role+type?
and of 0.67 for thefull task compared to gold) and it conforms withboth orderings in picking out similar sets of worstannotators.With this clustering we now have the possibilityto investigate the agreement for subgroups of an-notators.
Since the growth of the clusters is ratherlinear, we choose to track the confusion over thebest path of growing clusters, i.e.
starting fromthe best scoring {A24,A03} cluster to the maximalcluster.
It would be interesting to see the change inKrippendorff?s category distinction diagnostic forselected confusion pairs.
However, this value notonly depends on the amount of confusion but alsoon the frequency of that categories7, which cannotbe assume to be identical for different sets of an-notators.
We thus investigate the confusion rateconfc1,c2 , i.e.
the ratio of confusing assigmentspairs |c1 ?
c2| in the total set of agreeing and con-fusing assignments pairs for these two categories:confc1,c2 =|c1 ?
c2||c1 ?
c1|+ |c1 ?
c2|+ |c2 ?
c2|Figure 6 shows the confusion rate for selectedcategory pairs over the path from the best scoringto the maximal cluster.
The confusion between re-butters and undercutters is already at a high levelfor the best six best annotators, but increases whenworse annotators enter the cluster.
A constantand relatively low confusion rate has PSN+PAU,which means that distinguishing counter-attacksfrom new premises is equally ?hard?
for all annota-tors.
Distinguishing normal and example support,720% confusion of frequent categories have a larger im-pact on agreement than that of less frequent categories.2 3 6 7 8 9 11 12 13 14 15 16 18 19 20 21 22 23 25 260.000.050.100.150.200.250.300.350.400.450.50PAR+PAUOAR+OAUPT+PSNPSN+PAUPSN+PSEOAU+OSNFigure 6: Confusion rate for selected categorypairs in the growing clusters, with the numbers ofannotators in the cluster on the x axis.as well as central claims and supporting segmentsis not a problem for the six best annotators.
It be-comes slightly more confusing for more annota-tors, yet ends at a relatively low level around 0.08and 0.13 respectively.
Confusing undercutters andsupport on the opponents side is only a problemof the low-agreeing annotators, the confusion rateis nearly 0 for the first 21 annotators on the clus-ter path.
Finally note, that there is no confusiontypical for the high-agreeing annotators only.5 ConclusionsWe presented methods to systematically study theagreement in a larger group of annotators.
Tothis end, we evaluated an annotation study, where26 untrained annotators marked the argumentationstructure of small texts.
While the overall agree-ment showed only moderate correlation (as onecould expect from naive annotators in a text in-terpretation task) we could identify a subgroup ofannotators reaching a reliable level of agreementand good F1 scores in comparison with gold databy different ranking and clustering approaches andinvestigated which category confusions were char-acteristic for the different subgroups.AcknowledgmentsWe thank the anonymous reviewers for their help-ful comments.
The first author was supported by agrant from Cusanuswerk and the second author byDeutsche Forschungsgemeinschaft (SFB 632).203ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-coderagreement for computational linguistics.
Computa-tional Linguistics, 34(4):555?596, December.Vikas Bhardwaj, Rebecca J. Passonneau, Ansaf Salleb-Aouissi, and Nancy Ide.
2010.
Anveshan: a frame-work for analysis of multiple annotators?
labelingbehavior.
In Proceedings of the Fourth LinguisticAnnotation Workshop, LAW IV ?10, pages 47?55,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Lynn Carlson, Daniel Marcu, and Mary EllenOkurowski.
2003.
Building a discourse-tagged cor-pus in the framework of Rhetorical Structure The-ory.
In Jan van Kuppevelt and Ronnie Smith, edi-tors, Current Directions in Discourse and Dialogue.Kluwer, Dordrecht.Silvie Cinkova?, Martin Holub, and Vincent Kr??z?.
2012.Managing uncertainty in semantic tagging.
In Pro-ceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, EACL ?12, pages 840?850, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Joseph L. Fleiss.
1971.
Measuring nominal scaleagreement among many raters.
Psychological Bul-letin, 76(5):378?382.James B. Freeman.
1991.
Dialectics and theMacrostructure of Argument.
Foris, Berlin.James B. Freeman.
2011.
Argument Structure: Repre-sentation and Theory.
Argumentation Library (18).Springer.Jeroen Geertzen and Harry Bunt.
2006.
Measuringannotator agreement in a complex hierarchical di-alogue act annotation scheme.
In Proceedings ofthe 7th SIGdial Workshop on Discourse and Dia-logue, SigDIAL ?06, pages 126?133, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Klaus Krippendorff.
1980.
Content Analysis: An In-troduction to its Methodology.
Sage Publications,Beverly Hills, CA.J Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorial data.Biometrics, 33(1):159?174, March.William Mann and Sandra Thompson.
1988.
Rhetori-cal structure theory: Towards a functional theory oftext organization.
TEXT, 8:243?281.Andreas Peldszus and Manfred Stede.
to appear.
Fromargument diagrams to automatic argument mining:A survey.
International Journal of Cognitive Infor-matics and Natural Intelligence, 7(1).William A. Scott.
1955.
Reliability of content analy-sis: The case of nominal scale coding.
Public Opin-ion Quarterly, 19(3):321?325.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
Evaluating non-expert annotations for naturallanguage tasks.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?08, pages 254?263, Stroudsburg, PA,USA.
Association for Computational Linguistics.Simone Teufel.
2010.
The Structure of Scientific Arti-cles: Applications to Citation Indexing and Summa-rization.
CSLI Studies in Computational Linguis-tics.
CSLI Publications.Stephen Toulmin.
1958.
The Uses of Argument.
Cam-bridge University Press, Cambridge.204
