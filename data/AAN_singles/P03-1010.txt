Reliable Measures for Aligning Japanese-English News Articles andSentencesMasao Utiyama and Hitoshi IsaharaCommunications Research Laboratory3-5 Hikari-dai, Seika-cho, Souraku-gun, Kyoto 619-0289 Japanmutiyama@crl.go.jp and isahara@crl.go.jpAbstractWe have aligned Japanese and Englishnews articles and sentences to make alarge parallel corpus.
We first used amethod based on cross-language informa-tion retrieval (CLIR) to align the Japaneseand English articles and then used amethod based on dynamic programming(DP) matching to align the Japanese andEnglish sentences in these articles.
How-ever, the results included many incorrectalignments.
To remove these, we pro-pose two measures (scores) that evaluatethe validity of alignments.
The measurefor article alignment uses similarities insentences aligned by DP matching andthat for sentence alignment uses similar-ities in articles aligned by CLIR.
Theyenhance each other to improve the accu-racy of alignment.
Using these measures,we have successfully constructed a large-scale article and sentence alignment cor-pus available to the public.1 IntroductionA large-scale Japanese-English parallel corpus is aninvaluable resource in the study of natural languageprocessing (NLP) such as machine translation andcross-language information retrieval (CLIR).
It isalso valuable for language education.
However, nosuch corpus has been available to the public.We recently have obtained a noisy parallel cor-pus of Japanese and English newspapers consistingof issues published over more than a decade andhave tried to align their articles and sentences.
Wefirst aligned the articles using a method based onCLIR (Collier et al, 1998; Matsumoto and Tanaka,2002) and then aligned the sentences in these articlesby using a method based on dynamic programming(DP) matching (Gale and Church, 1993; Utsuro etal., 1994).
However, the results included many in-correct alignments due to noise in the corpus.To remove these, we propose two measures(scores) that evaluate the validity of article and sen-tence alignments.
Using these, we can selectivelyextract valid alignments.In this paper, we first discuss the basic statisticson the Japanese and English newspapers.
We nextexplain methods and measures used for alignment.We then evaluate the effectiveness of the proposedmeasures.
Finally, we show that our aligned corpushas attracted people both inside and outside the NLPcommunity.2 Newspapers AlignedThe Japanese and English newspapers used assource data were the Yomiuri Shimbun and the DailyYomiuri.
They cover the period from September1989 to December 2001.
The number of Japanesearticles per year ranges from 100,000 to 350,000,while English articles ranges from 4,000 to 13,000.The total number of Japanese articles is about2,000,000 and the total number of English articles isabout 110,000.
The number of English articles rep-resents less than 6 percent that of Japanese articles.Therefore, we decided to search for the Japanese ar-ticles corresponding to each of the English articles.The English articles as of mid-July 1996 have tagsindicating whether they are translated from Japanesearticles or not, though they don?t have explicit linksto the original Japanese articles.
Consequently, weonly used the translated English articles for the arti-cle alignment.
The number of English articles usedwas 35,318, which is 68 percent of all of the arti-cles.
On the other hand, the English articles beforemid-July 1996 do not have such tags.
So we used allthe articles for the period.
The number of them was59,086.
We call the set of articles before mid-July1996 ?1989-1996?
and call the set of articles aftermid-July 1996 ?1996-2001.
?If an English article is a translation of a Japanesearticle, then the publication date of the Japanese ar-ticle will be near that of the English article.
So wesearched for the original Japanese articles within 2days before and after the publication of each Englisharticle, i.e., the corresponding article of an Englisharticle was searched for from the Japanese articles of5 days?
issues.
The average number of English arti-cles per day was 24 and that of Japanese articles per5 days was 1,532 for 1989-1996.
For 1996-2001, theaverage number of English articles was 18 and thatof Japanese articles was 2,885.
As there are manycandidates for alignment with English articles, weneed a reliable measure to estimate the validity ofarticle alignments to search for appropriate Japanesearticles from these ambiguous matches.Correct article alignment does not guarantee theexistence of one-to-one correspondence betweenEnglish and Japanese sentences in article alignmentbecause literal translations are exceptional.
OriginalJapanese articles may be restructured to conform tothe style of English newspapers, additional descrip-tions may be added to fill cultural gaps, and detaileddescriptions may be omitted.
A typical example of arestructured English and Japanese article pair is:Part of an English article: ?e1?
Two bullet holes were found atthe home of Kengo Tanaka, 65, president of Bungei Shunju, in Ak-abane, Tokyo, by his wife Kimiko, 64, at around 9 a.m.
Monday.?/e1?
?e2?
Police suspect right-wing activists, who have mountedcriticism against articles about the Imperial family appearing inthe Shukan Bunshun, the publisher?s weekly magazine, were re-sponsible for the shooting.
?/e2?
?e3?
Police received an anony-mous phone call shortly after 1 a.m. Monday by a caller whoreported hearing gunfire near Tanaka?s residence.
?/e3?
?e4?
Po-lice found nothing after investigating the report, but later found abullet in the Tanakas?
bedroom, where they were sleeping at thetime of the shooting.
?/e4?Part of a literal translation of a Japanese article: ?j1?
At about8:55 a.m. on the 29th, Kimiko Tanaka, 64, the wife of BungeiShunju?s president Kengo Tanaka, 65, found bullet holes on theeastern wall of their two-story house at 4 Akabane Nishi, Kita-ku, Tokyo.?/j1?
?j2?
As a result of an investigation, the officers ofthe Akabane police station found two holes on the exterior wall ofthe bedroom and a bullet in the bedroom.?/j2?
?j3?
After receiv-ing an anonymous phone call shortly after 1 a.m. saying that twoor three gunshots were heard near Tanaka?s residence, police offi-cers hurried to the scene for investigation, but no bullet holes werefound.?/j3?
?j4?When gunshots were heard, Mr. and Mrs. Tanakawere sleeping in the bedroom.?/j4?
?j5?
Since Shukan Bunshun, aweekly magazine published by Bungei Shunju, recently ran an ar-ticle criticizing the Imperial family, Akabane police suspect right-wing activists who have mounted criticism against the recent arti-cle to be responsible for the shooting and have been investigatingthe incident.
?/j5?where there is a three-to-four correspondence be-tween {e1, e3, e4} and {j1, j2, j3, j4}, together witha one-to-one correspondence between e2 and j5.Such sentence matches are of particular interestto researchers studying human translations and/orstylistic differences between English and Japanesenewspapers.
However, their usefulness as resourcesfor NLP such as machine translation is limited forthe time being.
It is therefore important to extractsentence alignments that are as literal as possible.To achieve this, a reliable measure of the validity ofsentence alignments is necessary.3 Basic Alignment MethodsWe adopt a standard strategy to align articles andsentences.
First, we use a method based on CLIRto align Japanese and English articles (Collier etal., 1998; Matsumoto and Tanaka, 2002) and thena method based on DP matching to align Japaneseand English sentences (Gale and Church, 1993; Ut-suro et al, 1994) in these articles.
As each of thesemethods uses existing NLP techniques, we describethem briefly focusing on basic similarity measures,which we will compare with our proposed measuresin Section 5.3.1 Article alignmentTranslation of wordsWe first convert each of the Japanese articles intoa set of English words.
We use ChaSen1 to seg-ment each of the Japanese articles into words.
Wenext extract content words, which are then translatedinto English words by looking them up in the EDRJapanese-English bilingual dictionary,2 EDICT, andENAMDICT,3 which have about 230,000, 100,000,1http://chasen.aist-nara.ac.jp/2http://www.iijnet.or.jp/edr/3http://www.csse.monash.edu.au/?jwb/edict.htmland 180,000 entries, respectively.
We select two En-glish words for each of the Japanese words usingsimple heuristic rules based on the frequencies ofEnglish words.Article retrievalWe use each of the English articles as a query andsearch for the Japanese article that is most similarto the query article.
The similarity between an En-glish article and a (word-based English translationof) Japanese article is measured by BM25 (Robert-son and Walker, 1994).
BM25 and its variants havebeen proven to be quite efficient in information re-trieval.
Readers are referred to papers by the TextREtrieval Conference (TREC)4, for example.The definition of BM25 is:BM25(J,E) =?T?Ew(1) (k1 + 1)tfK + tf(k3 + 1)qtfk3 + qtfwhereJ is the set of translated English words of aJapanese article and E is the set of words of anEnglish article.
The words are stemmed and stopwords are removed.T is a word contained in E.w(1) is the weight of T , w(1) = log (N?n+0.5)(n+0.5) .N is the number of Japanese articles to be searched.n is the number of articles containing T .K is k1((1 ?
b) + b dlavdl ).
k1, b and k3 are pa-rameters set to 1, 1, and 1000, respectively.
dl isthe document length of J and avdl is the averagedocument length in words.tf is the frequency of occurrence of T in J .
qtf isthe frequency of T in E.To summarize, we first translate each of theJapanese articles into a set of English words.
Wethen use each of the English articles as a query andsearch for the most similar Japanese article in termsof BM25 and assume that it corresponds to the En-glish article.3.2 Sentence alignmentThe sentences5 in the aligned Japanese and Englisharticles are aligned by a method based on DP match-ing (Gale and Church, 1993; Utsuro et al, 1994).4http://trec.nist.gov/5We split the Japanese articles into sentences by using sim-ple heuristics and split the English articles into sentences byusing MXTERMINATOR (Reynar and Ratnaparkhi, 1997).We allow 1-to-n or n-to-1 (1 ?
n ?
6) alignmentswhen aligning the sentences.
Readers are referredto Utsuro et al (1994) for a concise description ofthe algorithm.
Here, we only discuss the similaritiesbetween Japanese and English sentences for align-ment.
Let Ji and Ei be the words of Japanese andEnglish sentences for i-th alignment.
The similar-ity6 between Ji and Ei is:SIM(Ji, Ei) = co(Ji ?Ei) + 1l(Ji) + l(Ei)?
2co(Ji ?
Ei) + 2wherel(X) =?x?X f(x)f(x) is the frequency of x in the sentences.co(Ji ?
Ei) =?
(j,e)?Ji?Ei min(f(j), f(e))Ji ?
Ei = {(j, e)|j ?
Ji, e ?
Ei} and Ji ?
Ei isa one-to-one correspondence between Japanese andEnglish words.Ji and Ei are obtained as follows.
We use ChaSen tomorphologically analyze the Japanese sentences andextract content words, which consists of Ji.
We useBrill?s tagger (Brill, 1992) to POS-tag the Englishsentences, extract content words, and use Word-Net?s library7 to obtain lemmas of the words, whichconsists of Ei.
We use simple heuristics to obtainJi ?
Ei, i.e., a one-to-one correspondence betweenthe words in Ji and Ei, by looking up Japanese-English and English-Japanese dictionaries made upby combining entries in the EDR Japanese-Englishbilingual dictionary and the EDR English-Japanesebilingual dictionary.
Each of the constructed dictio-naries has over 300,000 entries.We evaluated the implemented program against acorpus consisting of manually aligned Japanese andEnglish sentences.
The source texts were Japanesewhite papers (JEIDA, 2000).
The style of translationwas generally literal reflecting the nature of govern-ment documents.
We used 12 pairs of texts for eval-uation.
The average number of Japanese sentencesper text was 413 and that of English sentences was495.The recall, R, and precision, P , of the programagainst this corpus were R = 0.982 and P = 0.986,respectively, where6SIM(Ji, Ei) is different from the similarity function usedin Utsuro et al (1994).
We use SIM because it performed wellin a preliminary experiment.7http://www.cogsci.princeton.edu/?wn/R = number of correctly aligned sentence pairstotal number of sentence pairs aligned in corpusP = number of correctly aligned sentence pairstotal number of sentence pairs proposed by programThe number of pairs in a one-to-n alignment is n.For example, if sentences {J1} and {E1, E2, E3}are aligned, then three pairs ?J1, E1?, ?J1, E2?, and?J1, E3?
are obtained.This recall and precision are quite good consid-ering the relatively large differences in the languagestructures between Japanese and English.4 Reliable MeasuresWe use BM25 and SIM to evaluate the similarityin articles and sentences, respectively.
These mea-sures, however, cannot be used to reliably discrim-inate between correct and incorrect alignments aswill be discussed in Section 5.
This motivated us todevise more reliable measures based on basic simi-larities.BM25 measures the similarity between two bagsof words.
It is not sensitive to differences in theorder of sentences between two articles.
To rem-edy this, we define a measure that uses the similari-ties in sentence alignments in the article alignment.We define AVSIM(J,E) as the similarity betweenJapanese article, J , and English article, E:AVSIM(J,E) =?mk=1 SIM(Jk, Ek)mwhere (J1, E1), (J2, E2), .
.
.
(Jm, Em) are the sen-tence alignments obtained by the method describedin Section 3.2.
The sentence alignments in a cor-rectly aligned article alignment should have moresimilarity than the ones in an incorrectly aligned ar-ticle alignment.
Consequently, article alignmentswith high AVSIM are likely to be correct.Our sentence alignment program aligns sentencesaccurately if the English sentences are literal trans-lations of the Japanese as discussed in Section 3.2.However, the relation between English news sen-tences and Japanese news sentences are not literaltranslations.
Thus, the results for sentence align-ments include many incorrect alignments.
To dis-criminate between correct and incorrect alignments,we take advantage of the similarity in article align-ments containing sentence alignments so that thesentence alignments in a similar article alignmentwill have a high value.
We defineSntScore(Ji, Ei) = AVSIM(J,E)?
SIM(Ji, Ei)SntScore(Ji, Ei) is the similarity in the i-th align-ment, (Ji, Ei), in article alignment J and E. Whenwe compare the validity of two sentence alignmentsin the same article alignment, the rank order of sen-tence alignments obtained by applying SntScore isthe same as that of SIM because they share a com-mon AVSIM.
However, when we compare the va-lidity of two sentence alignments in different articlealignments, SntScore prefers the sentence alignmentwith the more similar (high AVSIM) article align-ment even if their SIM has the same value, whileSIM cannot discriminate between the validity of twosentence alignments if their SIM has the same value.Therefore, SntScore is more appropriate than SIM ifwe want to compare sentence alignments in differentarticle alignments, because, in general, a sentencealignment in a reliable article alignment is more re-liable than one in an unreliable article alignment.The next section compares the effectiveness ofAVSIM to that of BM25, and that of SntScore tothat of SIM.5 Evaluation of AlignmentHere, we discuss the results of evaluating article andsentence alignments.5.1 Evaluation of article alignmentWe first estimate the precision of article alignmentsby using randomly sampled alignments.
Next, wesort them in descending order of BM25 and AVSIMto see whether these measures can be used to providecorrect alignments with a high ranking.
Finally, weshow that the absolute values of AVSIM correspondwell with human judgment.Randomly sampled article alignmentsEach English article was aligned with a Japanesearticle with the highest BM25.
We sampled 100 ar-ticle alignments from each of 1996-2001 and 1989-1996.
We then classified the samples into four cate-gories: ?A?, ?B?, ?C?, and ?D?.
?A?
means that therewas more than 50% to 60% overlap in the content ofarticles.
?B?
means more than 20% to 30% and lessthan 50% to 60% overlap.
?D?
means that there wasno overlap at all.
?C?
means that alignment was notincluded in ?A?,?B?
or ?D?.
We regard alignmentsthat were judged to be A or B to be suitable for NLPbecause of their relatively large overlap.1996-2001 1989-1996type lower ratio upper lower ratio upperA 0.49 0.59 0.69 0.20 0.29 0.38B 0.06 0.12 0.18 0.08 0.15 0.22C 0.03 0.08 0.13 0.03 0.08 0.13D 0.13 0.21 0.29 0.38 0.48 0.58Table 1: Ratio of article alignmentsThe results of evaluations are in Table 1.8 Here,?ratio?
means the ratio of the number of articlesjudged to correspond to the respective categoryagainst the total number of articles.
For example,0.59 in line ?A?
of 1996-2001 means that 59 out of100 samples were evaluated as A.
?Lower?
and ?up-per?
mean the lower and upper bounds of the 95%confidence interval for ratio.The table shows that the precision (= sum of theratios of A and B) for 1996-2001 was higher thanthat for 1989-1996.
They were 0.71 for 1996-2001and 0.44 for 1989-1996.
This is because the En-glish articles from 1996-2001 were translations ofJapanese articles, while those from 1989-1996 werenot necessarily translations as explained in Section2.
Although the precision for 1996-2001 was higherthan that for 1989-1996, it is still too low to use themas NLP resources.
In other words, the article align-ments included many incorrect alignments.We want to extract alignments which will be eval-uated as A or B from these noisy alignments.
Todo this, we have to sort all alignments according tosome measures that determine their validity and ex-tract highly ranked ones.
To achieve this, AVSIM ismore reliable than BM25 as is explained below.8The evaluations were done by the authors.
We doublechecked the sample articles from 1996-2001.
Our secondchecks are presented in Table 1.
The ratio of categories in thefirst check were A=0.62, B=0.09, C=0.09, and D=0.20.
Com-paring these figures with those in Table 1, we concluded thatfirst and second evaluations were consistent.Sorted alignments: AVSIM vs. BM25We sorted the same alignments in Table 1 in de-creasing order of AVSIM and BM25.
Alignmentsjudged to be A or B were regarded as correct.
Thenumber, N, of correct alignments and precision, P,up to each rank are shown in Table 2.1996-2001 1989-1996AVSIM BM25 AVSIM BM25rankN P N P N P N P5 5 1.00 5 1.00 5 1.00 2 0.4010 10 1.00 8 0.80 10 1.00 4 0.4020 20 1.00 16 0.80 19 0.95 9 0.4530 30 1.00 25 0.83 28 0.93 16 0.5340 40 1.00 34 0.85 34 0.85 24 0.6050 50 1.00 39 0.78 37 0.74 28 0.5660 60 1.00 47 0.78 42 0.70 30 0.5070 66 0.94 55 0.79 42 0.60 35 0.5080 70 0.88 62 0.78 43 0.54 38 0.4790 71 0.79 68 0.76 43 0.48 40 0.44100 71 0.71 71 0.71 44 0.44 44 0.44Table 2: Rank vs. precisionFrom the table, we can conclude that AVSIMranks correct alignments higher than BM25.
Itsgreater accuracy indicates that it is important totake similarities in sentence alignments into accountwhen estimating the validity of article alignments.AVSIM and human judgmentTable 2 shows that AVSIM is reliable in rankingcorrect and incorrect alignments.
This section re-veals that not only rank order but also absolute val-ues of AVSIM are reliable for discriminating be-tween correct and incorrect alignments.
That is,they correspond well with human evaluations.
Thismeans that a threshold value is set for each of 1996-2001 and 1989-1996 so that valid alignments can beextracted by selecting alignments whose AVSIM islarger than the threshold.We used the same data in Table 1 to calculatestatistics on AVSIM.
They are shown in Tables 3and 4 for 1996-2001 and 1989-1996, respectively.type N lower av.
upper th.
sig.A 59 0.176 0.193 0.209 0.168 **B 12 0.122 0.151 0.179 0.111 **C 8 0.077 0.094 0.110 0.085 *D 21 0.065 0.075 0.086Table 3: Statistics on AVSIM (1996-2001)In these tables, ?N?
means the number of align-ments against the corresponding human judgment.type N lower av.
upper th.
sig.A 29 0.153 0.175 0.197 0.157 *B 15 0.113 0.141 0.169 0.131C 8 0.092 0.123 0.154 0.097 **D 48 0.076 0.082 0.088Table 4: Statistics on AVSIM (1989-1996)?Av.?
means the average value of AVSIM.
?Lower?and ?upper?
mean the lower and upper bounds ofthe 95% confidence interval for the average.
?Th.
?means the threshold for AVSIM that can be used todiscriminate between the alignments estimated to bethe corresponding evaluations.
For example, in Ta-ble 3, evaluations A and B are separated by 0.168.These thresholds were identified through linear dis-criminant analysis.
The asterisks ?**?
and ?*?
in the?sig.?
column mean that the difference in averagesfor AVSIM is statistically significant at 1% and 5%based on a one-sided Welch test.In these tables, except for the differences in theaverages for B and C in Table 4, all differencesin averages are statistically significant.
This indi-cates that AVSIM can discriminate between differ-ences in judgment.
In other words, the AVSIM val-ues correspond well with human judgment.
We thentried to determine why B and C in Table 4 were notseparated by inspecting the article alignments andfound that alignments evaluated as C in Table 4 hadrelatively large overlaps compared with alignmentsjudged as C in Table 3.
It was more difficult to dis-tinguish B or C in Table 4 than in Table 3.We next classified all article alignments in 1996-2001 and 1989-1996 based on the thresholds in Ta-bles 3 and 4.
The numbers of alignments are in Table5.
It shows that the number of alignments estimatedto be A or B was 46738 (= 31495 + 15243).
Weregard about 47,000 article alignments to be suffi-ciently large to be useful as a resource for NLP suchas bilingual lexicon acquisition and for language ed-ucation.1996-2001 1989-1996 totalA 15491 16004 31495B 9244 5999 15243C 4944 10258 15202D 5639 26825 32464total 35318 59086 94404Table 5: Number of articles per evaluationIn summary, AVSIM is more reliable than BM25and corresponds well with human judgment.
By us-ing thresholds, we can extract about 47,000 articlealignments which are estimated to be A or B evalu-ations.5.2 Evaluation of sentence alignmentSentence alignments in article alignments havemany errors even if they have been obtained fromcorrect article alignments due to free translation asdiscussed in Section 2.
To extract only correctalignments, we sorted whole sentence alignmentsin whole article alignments in decreasing order ofSntScore and selected only the higher ranked sen-tence alignments so that the selected alignmentswould be sufficiently precise to be useful as NLPresources.The number of whole sentence alignments wasabout 1,300,000.
The most important category forsentence alignment is one-to-one.
Thus, we wantto discard as many errors in this category as pos-sible.
In the first step, we classified whole one-to-one alignments into two classes: the first con-sisted of alignments whose Japanese and Englishsentences ended with periods, question marks, ex-clamation marks, or other readily identifiable char-acteristics.
We call this class ?one-to-one?.
Thesecond class consisted of the one-to-one alignmentsnot belonging to the first class.
The alignmentsin this class, together with the whole one-to-nalignments, are called ?one-to-many?.
One-to-onehad about 640,000 alignments and one-to-many hadabout 660,000 alignments.We first evaluated the precision of one-to-onealignments by sorting them in decreasing order ofSntScore.
We randomly extracted 100 samples fromeach of 10 blocks ranked at the top-300,000 align-ments.
(A block had 30,000 alignments.)
We clas-sified these 1000 samples into two classes: Thefirst was ?match?
(A), the second was ?not match?(D).
We judged a sample as ?A?
if the Japanese andEnglish sentences of the sample shared a commonevent (approximately a clause).
?D?
consisted of thesamples not belonging to ?A?.
The results of evalua-tion are in Table 6.99Evaluations were done by the authors.
We double checkedall samples.
In the 100 samples, there were a maximum of twoor three where the first and second evaluations were different.range # of A?s # of D?s1 - 100 030001 - 99 160001 - 99 190001 - 97 3120001 - 96 4150001 - 92 8180001 - 82 18210001 - 74 26240001 - 47 53270001 - 30 70Table 6: One-to-one: Rank vs. judgmentThis table shows that the number of A?s decreasesrapidly as the rank increases.
This means thatSntScore ranks appropriate one-to-one alignmentshighly.
The table indicates that the top-150,000 one-to-one alignments are sufficiently reliable.10 The ra-tio of A?s in these alignments was 0.982.We then evaluated precision for one-to-manyalignments by sorting them in decreasing order ofSntScore.
We classified one-to-many into three cat-egories: ?1-90000?, ?90001-180000?, and ?180001-270000?, each of which was covered by the range ofSntScore of one-to-one that was presented in Table6.
We randomly sampled 100 one-to-many align-ments from these categories and judged them to be Aor D (see Table 7).
Table 7 indicates that the 38,090alignments in the range from ?1-90000?
are suffi-ciently reliable.range # of one-to-many # of A?s # of D?s1 - 38090 98 290001 - 59228 87 13180001 - 71711 61 39Table 7: One-to-many: Rank vs. judgmentTables 6 and 7 show that we can extract validalignments by sorting alignments according toSntScore and by selecting only higher ranked sen-tence alignments.Overall, evaluations between the first and second check wereconsistent.10The notion of ?appropriate (correct) sentence alignment?depends on applications.
Machine translation, for example,may require more precise (literal) alignment.
To get literalalignments beyond a sharing of a common event, we will selecta set of alignments from the top of the sorted alignments thatsatisfies the required literalness.
This is because, in general,higher ranked alignments are more literal translations, becausethose alignments tend to have many one-to-one correspondingwords and to be contained in highly similar article alignments.Comparison with SIMWe compared SntScore with SIM and found thatSntScore is more reliable than SIM in discriminatingbetween correct and incorrect alignments.We first sorted the one-to-one alignments in de-creasing order of SIM and randomly sampled 100alignments from the top-150,000 alignments.
Weclassified the samples into A or D. The number ofA?s was 93, and that of D?s was 7.
The precision was0.93.
However, in Table 6, the number of A?s was491 and D?s was 9, for the 500 samples extractedfrom the top-150,000 alignments.
The precision was0.982.
Thus, the precision of SntScore was higherthan that of SIM and this difference is statisticallysignificant at 1% based on a one-sided proportionaltest.We then sorted the one-to-many alignments bySIM and sampled 100 alignments from the top38,090 and judged them.
There were 89 A?s and11 D?s.
The precision was 0.89.
However, in Ta-ble 7, there were 98 A?s and 2 D?s for samples fromthe top 38,090 alignments.
The precision was 0.98.This difference is also significant at 1% based on aone-sided proportional test.Thus, SntScore is more reliable than SIM.
Thishigh precision in SntScore indicates that it is im-portant to take the similarities of article alignmentsinto account when estimating the validity of sen-tence alignments.6 Related WorkMuch work has been done on article alignment.
Col-lier et al (1998) compared the use of machine trans-lation (MT) with the use of bilingual dictionary termlookup (DTL) for news article alignment in Japaneseand English.
They revealed that DTL is superior toMT at high-recall levels.
That is, if we want to ob-tain many article alignments, then DTL is more ap-propriate than MT.
In a preliminary experiment, wealso compared MT and DTL for the data in Table1 and found that DTL was superior to MT.11 These11We translated the English articles into Japanese with an MTsystem.
We then used the translated English articles as queriesand searched the database consisting of Japanese articles.
Thedirection of translation was opposite to the one described inSection 3.1.
Therefore this comparison is not as objective asit could be.
However, it gives us some idea into a comparisonof MT and DTL.experimental results indicate that DTL is more ap-propriate than MT in article alignment.Matsumoto and Tanaka (2002) attempted to alignJapanese and English news articles in the Nikkei In-dustrial Daily.
Their method achieved a 97% preci-sion in aligning articles, which is quite high.
Theyalso applied their method to NHK broadcast news.However, they obtained a lower precision of 69.8%for the NHK corpus.
Thus, the precision of theirmethod depends on the corpora.
Therefore, it is notclear whether their method would have achieved ahigh accuracy in the Yomiuri corpus treated in thispaper.There are two significant differences between ourwork and previous works.
(1) We have proposed AVSIM, which uses sim-ilarities in sentences aligned by DP matching, asa reliable measure for article alignment.
Previousworks, on the other hand, have used measures basedon bag-of-words.
(2) A more important difference is that we haveactually obtained not only article alignments but alsosentence alignments on a large scale.
In addition tothat, we are distributing the alignment data for re-search and educational purposes.
This is the firstattempt at a Japanese-English bilingual corpus.7 AvailabilityAs of late-October 2002, we have been distributingthe alignment data discussed in this paper for re-search and educational purposes.12 All the informa-tion on the article and sentence alignments are nu-merically encoded so that users who have the Yomi-uri data can recover the results of alignments.
Thedata also contains the top-150,000 one-to-one sen-tence alignments and the top-30,000 one-to-manysentence alignments as raw sentences.
The YomiuriShimbun generously allowed us to distribute themfor research and educational purposes.We have sent over 30 data sets to organizationson their request.
About half of these were NLP-related.
The other half were linguistics-related.
Afew requests were from high-school and junior-high-school teachers of English.
A psycho-linguist wasalso included.
It is obvious that people from both in-side and outside the NLP community are interested12http://www.crl.go.jp/jt/a132/members/mutiyama/jea/index.htmlin this Japanese-English alignment data.8 ConclusionWe have proposed two measures for extracting validarticle and sentence alignments.
The measure for ar-ticle alignment uses similarities in sentences alignedby DP matching and that for sentence alignmentuses similarities in articles aligned by CLIR.
Theyenhance each other and allow valid article and sen-tence alignments to be reliably extracted from an ex-tremely noisy Japanese-English parallel corpus.We are distributing the alignment data discussedin this paper so that it can be used for research andeducational purposes.
It has attracted the attention ofpeople both inside and outside the NLP community.We have applied our measures to a Japanese andEnglish bilingual corpus and these are language in-dependent.
It is therefore reasonable to expect thatthey can be applied to any language pair and still re-tain good performance, particularly since their effec-tiveness has been demonstrated in such a disparatelanguage pair as Japanese and English.ReferencesEric Brill.
1992.
A simple rule-based part of speech tagger.
InANLP-92, pages 152?155.Nigel Collier, Hideki Hirakawa, and Akira Kumano.
1998.
Ma-chine translation vs. dictionary term translation ?
a com-parison for English-Japanese news article alignment.
InCOLING-ACL?98, pages 263?267.William A. Gale and Kenneth W. Church.
1993.
A programfor aligning sentences in bilingual corpora.
ComputationalLinguistics, 19(1):75?102.Japan Electronic Industry Development Association JEIDA.2000.
Sizen Gengo Syori-ni Kan-suru Tyousa Houkoku-syo(Report on natural language processing systems).Kenji Matsumoto and Hideki Tanaka.
2002.
Automatic align-ment of Japanese and English newspaper articles using anMT system and a bilingual company name dictionary.
InLREC-2002, pages 480?484.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
A maxi-mum entropy approach to identifying sentence boundaries.In ANLP-97.S.
E. Robertson and S. Walker.
1994.
Some simple effec-tive approximations to the 2-Poisson model for probabilisticweighted retrieval.
In SIGIR?94, pages 232?241.Takehito Utsuro, Hiroshi Ikeda, Masaya Yamane, Yuji Mat-sumoto, and Makoto Nagao.
1994.
Bilingual text match-ing using bilingual dictionary and statistics.
In COLING?94,pages 1076?1082.
