Vocabulary Usage in Newswire SummariesTerry COPECKSchool of IT and EngineeringUniversity of OttawaOttawa, Ontario Canadaterry@site.uottawa.caStan SZPAKOWICZSchool of IT and EngineeringUniversity of OttawaOttawa, Ontario Canadaszpak@site.uottawa.caAbstractAnalysis of 9000 manually written summaries ofnewswire stories used in four Document Under-standing Conferences indicates that approximately40% of their lexical items do not occur in the sourcedocument.
A further comparison of different sum-maries of the same document shows agreement on28% of their vocabulary.
It can be argued that theserelationships establish a performance ceiling forautomated summarization systems which do notperform syntactic and semantic analysis on thesource document.1 IntroductionAutomatic summarization systems rely on manuallyprepared summaries for training data, heuristics andevaluation.
Generic summaries are notoriously hardto standardize; biased summaries, even in a mostrestricted task or application, also tend to vary be-tween authors.
It is unrealistic to expect one perfectmodel summary, and the presence of many, poten-tially quite diverse, models introduces considerableuncertainty into the summarization process.
In addi-tion, many summarization systems tacitly assumethat model summaries are somehow close to thesource documents.We investigate this assumption, and study the va-riability of manually produced summaries.
We firstdescribe the collection of documents withsummaries which has been accumulated over sev-eral years of participation in the Document Under-standing Conference (DUC) evaluation exercisessponsored by the National Institute of Science andTechnology (NIST).
We then present our methodol-ogy, discuss the rather pessimistic results, and fi-nally draw a few simple conclusions.2 The Corpus2.1 General OrganizationThe authors have assembled a corpus of manuallywritten summaries of texts from their archive of ma-terials provided to participants in the DUC confer-ences, held annually since 2001.
It is available at theDUC Web site to readers who are qualified to ac-cess the DUC document sets on application toNIST.
To help interested parties assess it for theirpurposes we provide more detail than usual on itsorganization and contents.Most summaries in the corpus are abstracts, writ-ten by human readers of the source document to bestexpress its content without restriction in any mannersave length (words or characters).
One method ofperforming automatic summarization is to constructthe desired amount of output by concatenating rep-resentative sentences from the source document,which reduces the task to one of determining mostadequately what ?representative?
means.
Such sum-maries are called extracts.
In 2002, recognizing thatmany participants summarize by extraction, NISTproduced versions of documents divided into indi-vidual sentences and asked its author volunteers tocompose their summaries similarly.
Because we usea sentence-extraction technique in our summariza-tion system, this data is of particular interest to us.
Itis not included in the corpus being treated here andwill be discussed in a separate paper.The DUC corpus contains 11,867 files organizedin a three-level hierarchy of directories totalling62MB.
The top level identifies the source year andexists simply to avoid the name collision which oc-curs when different years use same-named subdirec-tories.
The middle 291 directories identify thedocument clusters; DUC reuses collections ofnewswire stories assembled for the TREC and TDTresearch in itiatives which report on a common topicor theme.
Directories on the lowest level containSGML-tagged and untagged versions of 2,781 indi-vidual source documents, and between one and fivesummaries of each, 9,086 summaries in total.
Inmost cases the document involved is just that: a sin-gle news report originally published in a newspaper.552 directories, approximately 20% of the corpus,represent multi-document summaries?ones whichthe author has based on all the files in a cluster ofrelated documents.
For these summaries a sourcedocument against which to compare them has beenconstructed by concatenating the individual docu-ments in a cluster into one file.
Concatenation isdone in directory order, though the order of docu-ments does not matter here.2.2 The Corpus in DetailThe Document Understanding Conference hasevolved over the four years represented in our cor-pus, and this is reflected in the materials which areavailable for our purposes.
Table 1 classifies thesefiles by year and by target size of summary; therightmost column indicates the ratio of summariesto source documents, that is, the average number ofsummaries per document.
Totals appear in bold.
Thefollowing factors of interest can be identified in itsdata:?
Size .
Initially DUC targeted summaries of 50,100 and 200 words.
The following year 10-wordsummaries were added, and in 2003 only 10-and 100-word summaries were produced;?
Growth.
Despite the high cost of producingmanual summaries, the number of documentsunder consideration has doubled over the fouryears under study while the number of summa-ries has tripled;?
Ratio.
On average, three manual summaries areavailable for each source document;?
Formation.
While longer summaries are rou-tinely composed of well-formed sentences, sub-sentential constructs such as headlines are ac-ceptable 10-word summaries, as are lists of keywords and phrases.?
Author.
Although the 2004 DUC source docu-ments include machine translations of foreignlanguage news stories, in each case a parallelhuman translation was available.
Only sourcedocuments written or translated by human be-ings appear in the corpus.3 The Evaluation ModelFigure 1 shows the typical contents of a third-levelsource document directory.
Relations we wish toinvestigate are marked by arrows.
There are two: therelationship between the vocabulary used in thesource document and summaries of it, and thatamong the vocabulary used in summaries them-selves.
The first is marked by white arrows, the sec-ond by grey.The number of document-summary relations inthe corpus is determined by the larger cardinality setinvolved, which here is the number of summaries:thus 9,086 instances.
For every document with Nsummaries, we consider all C(N, 2) pairs of summa-ries.
In total there are 11,441 summary-summaryrelationships.We ask two questions: to what degree do summa-ries use words appearing in the source document?and, to what degree do different summaries use thesame vocabulary?3.1 MeasuresTo answer our two questions we decided to computestatistics on two types of elements of each pair oftest documents: their phrases, and ultimately, theirDOCUMENTS   SUMMARIES  D : S10 50 100 200 ?
10 50 100 200 ?2001  28 316 56 400   84 946 168 1198 1 : 32002 59 59 626 59 803  116 116 1228 116 1576 1 : 22003 624  90  714  2496  360  2856 1 : 42004 740  124  864  2960  496  3455 1 : 4?
1423 87 1156 115 2781  5572 200 3030 284 9086 1 : 3Table 1: Number of documents and summaries by size and by year, and ratiosSourceTextDOCUMENT ?
SUMMARY SUMMARY ?
SUMMARYB????
?
?CAFigure 1: Files and relationships investigatedindividual tokens.
Phrases were extracted by apply-ing a 987-item stop list developed by the authors(Copeck and Szpakowicz 2003) to the test docu-ments.
Each collocation separated by stop words istaken as a phrase1.
Test documents were tokenizedby breaking the text on white space and trimmingoff punctuation external to the token.
Instances ofeach sort of item were recorded in a hash table andwritten to file.Tokens are an obvious and unambiguous baselinefor lexical agreement, one used by such summaryevaluation systems as ROUGE (Lin and Hovy,2003).
On the other hand, it is important to explainwhat we mean by units we call phrases; they shouldnot be confused with syntactically correct constitu-ents such as noun phrases or verb phrases.
Our unitsoften are not syntactically well-formed.
Adjacentconstituents not separated by a stop word are uni-fied, single constituents are divided on any embed-ded stop word, and those composed entirely of stopwords are simply missed.Our phrases, however, are not n-grams.
A 10-word summary has precisely 9 bigrams but, in thisstudy, only 3.4 phrases on average (Table 2).
On thecontinuum of grammaticality these units can thus beseen as lying somewhere between generated blindlyn-grams and syntactically well-formed phrasal con-stituents.
We judge them to be weakly syntacticallymotivated2 and only roughly analogous to the fac-toids identified by van Halteren and Teufel (2003)in the sense that they also express semantic con-structs.
Where van Halteren and Teufel identifiedfactoids in 50 summaries, we sacrificed accuracy forautomation in order to process 9000.We then assessed the degree to which a pair ofdocuments for comparison shared vocabulary interms of these units.
This was done by countingmatches between the phrases.
Six different kinds ofmatch were identified and are listed here in what wedeem to be decreasing order of stringency.
Whilethe match types are labelled and described in termsof summary and source document for clarity, theyapply equally to summary pairs.
Candidate phrasesare underlined and matching elements tinted in theexamples; headings used in the results table (Table2) appear in SMALL CAPS.1 When analysis of a summary indicated that it was alist of comma- or semicolon-delimited phrases, the phras-ing provided by the summary author was adopted, includ-ing any stopwords present.
Turkey attacks Kurds in Iraq,warns Syria, accusations fuel tensions, Mubarak inter-cedes is thus split into four phrases with the first retainingthe stopword in.
There are 453 such summaries.2 While the lexical units in question might be more ac-curately labelled syntactically motivated ngrams, for sim-plicity we use phrase in the discussion.?
Exact match.
The most demanding, requirescandidates agree in all respects.
EXACTafter Mayo Clinic stay ?Mayo Clinic group?
Case-insensitive exact match relaxes the re-quirement for agreement in case.
EXACT CIconcerning bilateral relations ?Bilateral relations with?
Head of summary phrase in document re-quires only that the head of the candidate appearin the source document phrase.
The head is therightmost word in a phrase.
HEAD DOCcalls Sharon disaster ?deemed tantamount to disaster?
Head of document phrase in summary is theprevious test in reverse.
HEAD SUM?
Summary phrase is substring of documentphrase.
True if the summary phrase appearsanywhere in the document phrase.
SUB DOChas identified Iraqi agent as ?the Iraqi agent defection?
Document phrase is substring of summaryphrase reverses the previous test.
SUB SUMTests for matches between the tokens of twodocuments are more limited because only singlelexical items are involved.
Exact match can be sup-plemented by case insensitivity and by stemming toidentify any common root shared by two tokens.The Porter stemmer was used.The objective of all these tests is to capture anysort of meaningful resemblance between the vo-cabularies employed in two texts.
Without question,additional measures can and should be identifiedand implemented to correct, expand, and refine theanalysis.3.2 MethodologyThe study was carried out in three stages.
A pre-study determined the ?lie of the land?
?what thegeneral character of results was likely to be, themost appropriate methodology to realize them, andso on.
In particular this initial investigation alertedus to the fact that so few phrases in any two textsunder study matched exactly as to provide little use-ful data, leading us to add more relaxed measures oflexical agreement.
This initial investigation made itclear that there was no point in attempting to find asubset of vocabulary used in a number of summa-ries?it would be vanishingly small?and we there-fore confined ourselves in the main study topairwise comparisons.
The pre-study also suggestedthat summary size would be a significant factor inlexical agreement while source document sizewould be less so, indications which were not en-tirely borne out by the strength of the results ult i-mately observed.The main study proceeded in two phases.
Afterthe corpus had been organized as described in Sec-tion 2 and untagged versions of the source docu-ments produced for the analysis program to workon, that process traversed the directory tree, decom-posing each text file into its phrases and tokens.These were stored in hash tables and written to fileto provide an audit point on the process.
The hashtables were then used to test each pair of test docu-ments for matches?the source document to eachsummary, and all combinations of summaries.
Theresulting counts for all comparisons together withother data were then written to a file with results,one line per source document in a comma-delimitedformat suitable for importation to a spreadsheet pro-gram.The second phase of the main study involved or-ganizing the spreadsheet data into a format permit-ting the calculation of statistics on various cate-gorizations of documents they describe.
Because thesource document record was variable-length in itselfand also contained a varying number of variable-length sub-records of document pair comparisons,this was a fairly time-consuming clerical task.
It didhowever provide the counts and averages presentedin Table 2 and subsequently allowed the user to re-categorize the data fairly easily.A post-study was then conducted to validate thecomputation of measures by reporting these to theuser for individual document sets, and applied to aAFA19981230.1000.0058:  X <> W  exact: 2, exactCI: 2, partSum2: 2, partSum1 2, token-Match: 6X: Jordanian King Hussein to meet with Clinton concerning bilateral relationsW: King Hussein to meet with Clinton after visiting Mayo Clinic2 exact:  meet,Clinton2 exactCI:  meet,clinton2 headSum1:  clinton,meet2 headSum2:  meet,clinton6 tokMatch:  hussein,meet,clinton,to,king,withFigure 2: Text and matches for two summaries of AFA19981230.1000.0058DOCUMENT  - SUMMARYSUMMARY  PHRASES  TOKENSCOUNT TOKENS PHRASES  EXACT EXACT CIHEADDOCHEADSUMSUBDOCSUBSUMEXACT STEM CI10 5572 10.0 3.4  0.8 1.0 1.4 0.9 2.3 2.7  5.4 6.350 200 47.4 15.5  5.5 5.7 8.8 4.9 11.8 12.0  30.6 32.6100 3030 95.6 30.5  12.1 12.5 14.9 10.1 22.3 20.5  52.7 54.8200 284 157.5 48.6  19.7 20.4 28.3 17.1 38.4 35.3  82.9 85.8ALL 9086 44.0 14.1  5.2 5.5 6.9 8.4 10.3 28.2  24.2 25.510     22% 29% 43% 27% 69% 79%  55% 63%50     35% 37% 57% 31% 76% 77%  65% 69%100     39% 41% 49% 34% 78% 74%  55% 58%200     40% 42% 56% 35% 79% 73%  51% 53%ALL     37% 39% 49% 33% 73% 70%  55% 58%SUMMARY  -  SUMMARY10 8241 10.0 3.4  0.17 0.21 0.24 0.24    2.82 3.1350 141 47.4 15.5  0.71 0.84 1.09 1.06    10.89 11.77100 2834 95.6 30.5  4.21 4.39 4.76 4.82    28.16 29.66200 225 157.5 48.6  4.26 4.52 6.24 5.93    35.16 37.14ALL 11441 44.0 14.1  1.26 1.34 1.5 1.5    9.8 10.4810     5% 6% 7% 7%    28% 31%50     5% 5% 7% 7%    23% 25%100     14% 14% 16% 16%    29% 31%200     9% 9% 13% 12%    22% 24%ALL     9% 10% 11% 11%    22% 24%Table 2: Counts and percentages of vocabulary agreement, by size and totalsmall random sample of text pairs.
Figure 2 showsthe comparison of two summaries of source docu-ment AFA19981230.1000.0058.
A secondaryobjective of the post-study was to inspect the ac-tual data.
Were there factors in play in the datathat had escaped us?
None were made evident be-yond the all-too-familiar demonstration of thewide variety of language use in play.
The log fileof document phrase hash tables provided an addi-tional snapshot of the kind of materials withwhich the automated computation had been work-ing.4  Results4.1 Data AveragesTable 2 illustrates the degree to which summariesin the DUC corpus employ the same vocabularyas the source documents on which they are basedand the degree to which they resemble each otherin wording.
The table, actually a stack of four ta-bles which share common headings, presents dataon the document-summary relationship followedby inter-summary data, giving counts and thenpercentages for each relationship.
Statistics on thegiven relationship appear in the first three col-umns on the left; counts and averages are classi-fied by summary size.
The central group of sixcolumns presents from left to right, in decreasingorder of strictness, the average number of phrasematches found for the size category.
The final twocolumns on the right present parallel match datafor tokens.
Thus for example the column entitledSTEM CI shows the average number of stemmed,case-insensitive token matches in a pair of testdocuments of the size category indicated.
Eachtable in the stack ends with a boldface row thataverages statistics across all size categories.Inspection of the results in Table 2 leads tothese general observations:?
With the exception of 200-word summariesfalling somewhat short (157 words), each cate-gory approaches its target size quite closely;?
Phrases average three tokens in length regard-less of summary size;?
The objective of relaxing match criteria in themain study was achieved.
With few exceptions,each less strict match type produces more hitsthan its more stringent neighbors;?
The much smaller size of the now discontinued50- and 200-word categories argues against in-vesting much confidence in their data;?
Finally, while no effect was found for sourcedocument size (and results for that categorizationare therefore not presented), the percentage tablessuggest summary size has some limited impact onvocabulary agreement.
This effect occurs solelyon the phrasal level, most strongly on its strictestmeasures; token values are effectively flat.We are uncertain why this last situation is so.Consider only the well-populated 10-word and100-word summary classes.
The effect cannot beaccounted for a preponderance of multiple -document summaries in either class which mightprovide more opportunities for matches.
Despitemany more of these being among the 100-wordsummaries than the 10-word (1974 single : 1056multi, versus 116 single : 5456 multi), the per-centage of exact phrasal matches is essentially thesame in each subcategorization of these classes.We speculate that authors may compose thesentences in 100-word summaries in terms ofphrases from the source document, while 10-wordsummaries, which more closely resemble terseheadlines, cannot be composed by direct reuse ofsource document phrases.
50- and 200-wordsummaries are also composed of sentences.
Theirexact match percentages approach those of 100-word summaries, lending support to this interpre-tation.Figure 3: Percentages of summary vocabularyagreement for all source documents, by measure4.2 Data VarianceWhether count or percentage, exclusively averagedata is presented in Table 2.
While measures ofcentral tendency are an important dimension ofany population, a full statistical description alsorequires some indication of measures of variance.These appear in Figure 3 which shows, for each ofthe six phrasal and two token measures, what per-centage of the total number of summaries fallsinto each tenth of the range of possible values.
Forexample, a summary in which 40% of the phraseswere exactly matched in the source documentwould be represented in the figure by the verticalposition of the frontmost band over the extent ofthe decade labeled ?4??24%.
The figure?s three-dimensional aspect allows the viewer to trackwhich decades have the greatest number of in-stances as measures move from more strict tomore relaxed, front to back.However, the most striking message communi-cated by Figure 3 is that large numbers of summa-ries have zero values for the stricter measures,EXACT, EXACT CI and PART SUM in particular andPART DOC to a lesser degree.
These same meas-ures have their most frequent values around the50% decade, with troughs both before and after.To understand why this is so requires some expla-nation.
Suppose a summary contains two phrases.If none are matched in the source its score is 0%.If one is matched its score is 50%; if  both, 100%.A summary with three phrases has four possiblepercentage values: 0%, 33%, 66% and 100%.
The'hump' of partial matching is thus around the fiftypercent level because most summaries are tenwords, and have only 1 or 2 candidates to bematched.
The ranges involved in the strictermeasures are not large.That acknowledged, we can see that the modalor most frequent decade does indeed tend in anirregular way to move from left to right, from zeroto 100 percent, as measures become less strict.
Inmaking this observation, note that the two back-most bands represent measures on tokens, a dif-ferent syntactic element than the phrase.
Theinformation about the distribution of summarymeasures shown in this figure is not unexpected.4.3 Key FindingsThe central fact that these data communicate quiteclearly is that summaries do not employ many ofthe same phrases their source documents do, andeven fewer than do other summaries.
In particular,on average only 37% of summary phrases appearin the source document, while summaries shareonly 9% of their phrases.
This becomes more un-derstandable when we note that on average only55% of the individual words used in summaries,both common vocabulary terms and proper names,appear in the source document; and betweensummaries, on average only 22% are found inboth.It may be argued that the lower counts for inter-summary vocabulary agreement can be explainedthus: since a summary is so much smaller than itssource document, lower counts should result.
Onereply to that argument is that, while acknowledg-ing that synonymy, generalization and specializa-tion would augment the values found, the essenceof a generic summary is to report the pith, the gist,the central points, of a document and that thesekey elements should not vary so widely from onesummary to the next.5 Pertinent ResearchPrevious research addressing summary vocabularyis limited, and most has been undertaken in con-nection with another issue: either with the prob-lem of evaluating summary quality (Mani, 2001;Lin and Hovy, 2002) or to assess sentence elementsuitability for use in a summary (Jing and McKe-own, 1999).
In such a case results arise as a by-product of the main line of research and conclu-sions about vocabulary must be inferred fromother findings.Mani (2001) reports that ?previous studies, mostof which have focused on extracts, have shownevidence of low agreement among humans as towhich sentences are good summary sentences.
?Lin and Hovy?s (2002) discovery of low inter-rater agreement in single (~40%) and multiple(~29%) summary evaluation may also pertain toour findings.
It stands to reason that individua lswho disagree on sentence pertinence or do not ratethe same summary highly are not likely to use thesame words to write the summary.
In the veryovert rating situation they describe, Lin and Hovywere also able to identify human error and quan-tify it as a significant factor in rater performance.This reality may introduce variance as a conse-quence of suboptimal performance: a writer maysimply fail to use the mot juste .In contrast, Jing, McKeown, Barzilay and Elha-dad (1998) found human summarizers to be ?quiteconsistent?
as to what should be included, a resultthey acknowledge to be ?surprisingly high?.
Jinget al note that agreement drops off with summarylength, that their experience is somewhat at vari-ance with that of other researchers, and that thismay be accounted for in part by regularity in thestructure of the documents summarized.Observing that ?expert summarizers often reusethe text in the original document to produce asummary?
Jing and McKeown (1999) analyzed300 human written summaries of news articlesand found that ?a significant portion (78%) ofsummary sentences produced by humans are ba-sed on cut-and-paste?, where ?cut-and-paste?
indi-cates vocabulary agreement.
This suggests that22% of summary sentences are not produced inthis way; and the authors report that 315 (19%)sentences do not match any sentence in the docu-ment.In their 2002 paper, Lin and Hovy examine theuse of multiple gold standard summaries forsummarization evaluation, and conclude ?we needmore than one model summary although we can-not estimate how many model summaries are re-quired to achieve reliable automated summaryevaluation?.Attempting to answer that question, van Hal-teren and Teufel (2003) conclude that 30 to 40manual summaries should be sufficient to estab-lish a stable consensus model summary.
Their re-search, which directly explores the differences andsimilarities between various human summaries toestablish a basis for such an estimate, finds greatvariation in summary content as reflected in fac-toids3.
This variation does not fall off with thenumber of summaries and accordingly no twosummaries correlate highly.
Although factoidmeasures did not correlate highly with those ofunigrams (tokens), the former did clearly demon-strate an importance hierarchy which is an essen-tial condition if a consensus model summary is tobe constructed.
Our work can thus be seen as con-firming that, in large measure, van Halteren andTeufel?s findings apply to the DUC corpus ofmanual summaries.6 DiscussionWe began this study to test two hypotheses.
Thefirst is this: automatic summarization is made dif-ficult to the degree that manually-written summa-ries do not limit themselves to the vocabulary ofthe source document.
For a summarization system3 A factoid is an atomic semantic unit correspondingto an expression in first-order predicate logic.
As al-ready noted we approximate phrases to factoids.to incorporate words which do not appear in thesource document requires at a minimum that it hasa capacity to substitute a synonym of some wordin the text, and some justification for doing so.More likely it would involve constructing a repre-sentation of the text?s meaning and reasoning(generalization, inferencing) on the content of thatrepresentation.
The latter are extremely hard tasks.Our second hypothesis is that automatic sum-marization is made difficult to the degree thatmanually written summaries do not agree amongthemselves.
While the variety of possible dis-agreements are multifarious, the use of differentvocabulary is a fundamental measure of semanticheterogeneity.
Authors cannot easily talk of thesame things if they do not use words in common.Unfortunately, our study of the DUC manualsummaries and their source documents providessubstantial evidence that summarization of thesedocuments remains difficult indeed.7 ConclusionPrevious research on the degree of agreement be-tween documents and summaries, and betweensummaries, has generally indicated that there aresignificant differences in the vocabulary used byauthors of summaries and the source document.Our study extends the investigation to a corpuscurrently popular in the text summarization re-search community and finds the majority opinionto be borne out there.
In addition, our data sug-gests that summaries resemble the source docu-ment more closely than they do each other.
Thelimited number of summaries available for anyindividual source document prevents us fromlearning any characteristics of the population ofpossible summaries.
Would more summaries dis-tribute themselves evenly throughout the semanticspace defined by the source document?s vocabu-lary?
Would clumps and clusters show them-selves, or a single cluster as van Halteren andTeufel suggest?
If the latter, such a groupingwould have a good claim to call itself a consensussummary of the document and a true gold stan-dard would be revealed.ReferencesCopeck, Terry and Stan Szpakowicz.
2003.
Pick-ing phrases, picking sentences.
In DUC Work-shop at HLT/NAACL-2003 Workshop onAutomatic Summarization.Jing, Hongyan, Regina Barzilay, Kathleen McKe-own and Michael Elhadad.
1998.
Summariza-tion evaluation methods: Experiments andanalysis.
In 1998 AAAI Spring Symposium onIntelligent Text Summarization, AAAI TechnicalReport SS-98-06.Jing, Hongyan.
and Kathleen McKeown.
1999.The decomposition of human-written summarysentences.
Proceedings of the 22nd Interna-tional Conference on Research and Develop-ment in Information Retrieval (SIGIR?99).Lin, Chin-Yew and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
Proceedings of 2003 Lan-guage Technology Conference (HLT-NAACL2003).Lin, Chin-Yew and Eduard Hovy.
2002.
Manualand automatic evaluation of summaries.
Pro-ceedings of Workshop on Automatic Summari-zation, 2002 ACL (WAS/ACL-02).Mani, Inderjeet.
2001.
Summarization evaluation:An overview.
Proceedings of the Second NTCIRWorkshop on Research in Chinese & JapaneseText Retrieval and Text Summarization.Van Halteren, Hans, and Simone Teufel.
2003.Examining the consensus between human sum-maries: initial experiments with factoid analysis.Proceedings of Workshop on Automatic Sum-marization, 2003 Language Technology Confer-ence (WAS/HLT-NAACL-2003).
