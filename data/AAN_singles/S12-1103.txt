First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689?695,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsJU_CSE_NLP: Language Independent Cross-lingualTextual Entailment SystemSnehasis Neogi1, Partha Pakray2, Sivaji Bandyopadhyay1,Alexander Gelbukh31Computer Science & Engineering DepartmentJadavpur University, Kolkata, India2Computer Science & Engineering DepartmentJadavpur University, Kolkata, IndiaIntern at Xerox Research Centre EuropeGrenoble, France3Center for Computing ResearchNational Polytechnic InstituteMexico City, Mexico{snehasis1981,parthapakray}@gmail.comsbandyopadhyay@cse.jdvu.ac.ingelbukh@gelbukh.comAbstractThis article presents the experiments car-ried out at Jadavpur University as part ofthe participation in Cross-lingual TextualEntailment for Content Synchronization(CLTE) of task 8 @ Semantic EvaluationExercises (SemEval-2012).
The work ex-plores cross-lingual textual entailment as arelation between two texts in different lan-guages and proposes different measuresfor entailment decision in a four way clas-sification tasks (forward, backward, bidi-rectional and no-entailment).
We set updifferent heuristics and measures for eva-luating the entailment between two textsbased on lexical relations.
Experimentshave been carried out with both the textand hypothesis converted to the same lan-guage using the Microsoft Bing translationsystem.
The entailment system considersNamed Entity, Noun Chunks, Part ofspeech, N-Gram and some text similaritymeasures of the text pair to decide the en-tailment judgments.
Rules have been de-veloped to encounter the multi wayentailment issue.
Our system decides onthe entailment judgment after comparingthe entailment scores for the text pairs.Four different rules have been developedfor the four different classes of entailment.The best run is submitted for Italian ?English language with accuracy 0.326.1 IntroductionTextual Entailment (TE) (Dagan and Glick-man, 2004) is one of the recent challenges ofNatural Language Processing (NLP).
The Task8 of SemEval-20121 [1] defines a textual en-tailment system that specifies two major as-pects: the task is based on cross-lingualcorpora and the entailment decision must befour ways.
Given a pair of topically related textfragments (T1 and T2) in different languages,the CLTE task consists of automatically anno-tating it with one of the following entailmentjudgments:i. Bidirectional (T1 ->T2 & T1 <- T2): the twofragments entail each other (semantic equiva-lence)ii.
Forward (T1 -> T2 & T1!<- T2): unidirec-tional  entailment from T1 to T2 .iii.
Backward (T1!
-> T2 & T1 <- T2): unidirec-tional entailment from T2 to T1.iv.
No Entailment (T1!
-> T2 & T1!
<- T2):there is no entailment between T1 and T2.CLTE (Cross Lingual Textual Entailment) taskconsists of 1,000 CLTE dataset pairs (500 for1http://www.cs.york.ac.uk/semeval2012/index.php?id=tasks689training and 500 for test) available for the fol-lowing language combinations:- Spanish/English (spa-eng)- German/English (deu-eng).- Italian/English (ita-eng)- French/English (fra-eng)Seven Recognizing Textual Entailment (RTE)evaluation tracks have already been held: RTE-1in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009,RTE-6 [7] in 2010 and RTE-7 [8] in 2011.
RTEtask produces a generic framework for entail-ment task across NLP applications.
The RTEchallenges have moved from 2 ?
way entailmenttask (YES, NO) to 3 ?
way task (YES, NO,UNKNOWN).
EVALITA/IRTE [9] task is simi-lar to the RTE challenge for the Italian language.So far, TE has been applied only in a monolin-gual setting.
Cross-lingual Textual Entailment(CLTE) has been proposed ([10], [11], [12]) asan extension of Textual Entailment.
In 2010,Parser Training and Evaluation using TextualEntailment [13] was organized by SemEval-2.Recognizing Inference in Text (RITE)2 orga-nized by NTCIR-9 in 2011 is the first to expandTE as a 5-way entailment task (forward, back-ward, bi-directional, contradiction and indepen-dent) in a monolingual scenario [14].We have participated in RTE-5 [15], RTE-6[16], RTE-7 [17], SemEval-2 Parser Trainingand Evaluation using Textual Entailment Taskand RITE [18].Section 2 describes our Cross-lingual TextualEntailment system.
The various experimentscarried out on the development and test data setsare described in Section 3 along with the results.The conclusions are drawn in Section 4.2 System ArchitectureOur system for CLTE task is based on a set ofheuristics that assigns entailment scores to a textpair based on lexical relations.
The text and thehypothesis in a text pair are translated to thesame language using the Microsoft Bing ma-chine translation system.
The system separatesthe text pairs (T1 and T2) available in differentlanguages and preprocesses them.
After prepro-2 http://artigas.lti.cs.cmu.edu/rite/Main_Pagecessing we have used several techniques such asWord Overlaps, Named Entity matching, Chunkmatching, POS matching to evaluate the sepa-rated text pairs.
These modules return a set ofscore statistics, which helps the system to go formulti-class entailment decision based on thepredefined rules.
We have submitted 3 runs foreach language pair for the CLTE task and thereare some minor differences in the architecturesthat constitute the 3 runs.
The three system ar-chitectures are described in section 2.1, section2.2 and section 2.3.2.1 System Architecture 1: CLTE Taskwith  Translated English TextThe system architecture of Cross-lingual textualentailment consists of various components suchas Preprocessing Module, Lexical SimilarityModule, Text Similarity Module.
Lexical Simi-larity module again is divided into subsequentmodules like POS matching, Chunk matchingand Named Entity matching.
Our system calcu-lates these measures twice once considering T1as text and T2 as hypothesis and once T2 as textand T1 as hypothesis.
The mapping is done inboth directions T1-to-T2 and T2-to-T1 to arriveat the appropriate four way entailment decisionusing a set of rules.
Each of these modules isnow being described in subsequent subsections.Figure 1 shows our system architecture wherethe text sentence is translated to English.Figure 1: System ArchitectureCLTE Task Data(T1, T2)T1.txtT2.txtTranslated inEng.
Using BingTranslatorPreprocessing(Stop word removal,Co referencing)N-Gram ModulePreprocessing(Stop word removal,Co referencing)Chunking ModuleText Similarity Module Named Entity POS Module?
Lexical Score (S1)S1?
Lexical Score (S2)S1If (S1>S2) Then Entailment = ?forward?If (S1<S2) Then Entailment = ?backward?If (S1=S2) or (abs (S1-S2) <Threshold) Then Entailment = ?bidirectional?
(fra, ita, deu,spa language)T1- TextT2- HypothesisT1 ?
HypothesisT2 - TextIf (S1=S2 and (S1=S2) <Threshold) Then Entailment = ?no_entailment?
(Englishlanguage)6902.1.1 Preprocessing ModuleThe system separates the T1 and T2 pair fromthe CLTE task data.
T1 sentences are in differ-ent languages (In French, Italian, German andSpanish) where as T2 sentences are in English.Microsoft Bing translator3 API for Bing transla-tor (microsoft-translator-java-api-0.4-jar-with-dependencies.jar) is being used to translate theT1 text sentences into English.
The translatedT1 and T2 sentences are passed through the twosub modules.i.
Stop word Removal: Stop words are removedfrom the T1 and T2 sentences.ii.
Co-reference: Co?reference chains are eva-luated for the datasets before passing them to theTE module.
The objective is to increase the en-tailment score after substituting the anaphorswith their antecedents.
A word or phrase in thesentence is used to refer to an entity introducedearlier or later in the discourse and both havingsame things then they have the same referent orco-reference.
When the reader must look back tothe previous context, co-reference is called"Anaphoric Reference".
When the reader mustlook forward, it is termed "Cataphoric Refer-ence".
To address this problem we used a toolcalled JavaRAP4 (A java based implementationof Anaphora Procedure (RAP) - an algorithm byLappin and Leass (1994)).
It has been observedthat the presence of co ?
referential expressionsare very small in sentence based paradigm.2.1.2 Lexical Based Textual Entailment(TE) ModuleT1 - T2 pairs are the inputs to the system.
TheTE module is executed once by considering T1as text and T2 as hypothesis and again by consi-dering T2 as text and T1 as hypothesis.
Theoverall TE module is a collection of several lex-ical based sub modules.i.
N-Gram Match module: The N-Gram matchbasically measures the percentage match of theunigram, bigram and trigram of hypothesispresent in the corresponding text.
These scoresare simply combined to get an overall N ?
Grammatching score for a particular pair.
By running3 http://code.google.com/p/microsoft-translator-java-api/4 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.htmlthe module we get two scores, one for T1-T2pair and another for T2-T1 pair.ii.
Chunk Similarity module: In this sub mod-ule our system evaluates the key NP-chunks ofboth text and hypothesis identified using NPChunker v1.15.
Then our system checks thepresence of NP-Chunks of hypothesis in the cor-responding text.
System calculates the overallvalue for the chunk matching, i.e., number oftext NP-chunks that match with hypothesis NP-chunks.
If the chunks are not similar in their sur-face form then our system goes for WordNetmatching for the words and if they match inWordNet synsets information, the chunks areconsidered as similar.WordNet [19] is one of most important resourcefor lexical analysis.
The WordNet 2.0 has beenused for WordNet based chunk matching.
TheAPI for WordNet Searching (JAWS)6 is an APIthat provides Java applications with the abilityto retrieve data from the WordNet database.
Letus consider the following example taken fromtraining data:T1: Due/JJ to/TO [an/DT error/NN of/IN com-munication/NN] between/IN [the/DT police/NN]?T2: On/IN [Tuesday/NNP] [a/DT failed/VBNcommunication/NN] between/IN?The chunk in T1 [error communication] matcheswith T2 [failed communication] via WordNetbased synsets information.
A weight is assignedto the score depending upon the nature of chunkmatching.M[i] = Wm[i] * ?
/ Wc[i]Where N= Total number of chunk containinghypothesis.M[i] = Match Score of the ith  Chunk.Wm[i] = Number of words matched in the ithchunk.Wc[i] = Total words in the ith chunk.1 if surface word matches.and ?
=?
if matche via WordNet5 http://www.dcs.shef.ac.uk/~mark/phd/software/6 http://lyle.smu.edu/~tspell/jaws/index.html691System takes into consideration several text si-milarity measures calculated over the T1-T2pair.
These text similarity measures are summedup to produce a total score for a particular textpair.
Similar to the Lexical module, text simi-larity module is also executed for both T1-T2and T2-T1 pairs.iii.
Text Distance Module: The following majortext similarity measures have been consideredby our system.
The text similarity measurescores are added to generate the final text dis-tance score.?
Cosine Similarity?
Levenshtein Distance?
Euclidean Distance?
MongeElkan Distance?
NeedlemanWunch Distance?
SmithWaterman Distance?
Block Distance?
Jaro Similarity?
MatchingCoefficient Similarity?
Dice Similarity?
OverlapCoefficient?
QGrams Distanceiv.
Named Entity Matching: It is based on thedetection and matching of Named Entities in theT1-T2 pair.
Stanford Named Entity Recognizer7(NER) is used to tag the Named Entities in bothT1 and T2.
System simply matches the numberof hypothesis NEs present in the text.
A score isallocated for the matching.NE_match = (Number of common NEs in Textand Hypothesis)/(Number of NEs in Hypothe-sis).v.
Part-of-Speech (POS) Matching: This mod-ule basically deals with matching the commonPOS tags between T1 and T2 pair.
Stanford POStagger8 is used to tag the part of speech in bothT1 and T2.
System matches the verb and nounPOS words in the hypothesis that match in thetext.
A score is allocated based on the number ofPOS matching.POS_match = (Number of verb and nounPOS in Text and Hypothesis)/(Total number ofverb and noun POS in hypothesis).7 http://nlp.stanford.edu/software/CRF-NER.shtml8 http://nlp.stanford.edu/software/tagger.shtmlSystem adds all the lexical matching scores toevaluate the total score for a particular T1- T2pair, i.e.,Pair1:  (T1 ?
Text and T2 ?
Hypothesis)Pair2:   (T1 ?
Hypothesis and T2 - Text).Total lexical score for each pair can be mathe-matically represented by:where S1 represents the score for the pair withT1 as text and T2 as hypothesis while S2represents the score from T1 to T2.
The figure 2shows the sample output values of the TE mod-ule.Figure 2: output values of this moduleThe system finally compares the above two val-ues S1 and S2 as obtained from the lexical mod-ule to go for four-class entailment decision.
Ifscore S1, i.e., the mapping score with T1 as textand T2 as hypothesis is greater than the scoreS2, i.e., mapping score with T2 as text and T1 ashypothesis, then the entailment class will be?forward?.
Similarly if S1 is less than S2, i.e.,T2 now acts as the text and T1 acts as the hypo-thesis then the entailment class will be ?back-ward?.
Similarly if both the scores S1 and S2 areequal the entailment class will be ?bidirectional?
(entails in both directions).
Measuring ?bidirec-tional?
entailment is much more difficult thanany other entailment decision due to combina-tions of different lexical scores.
As our systemproduces a final score (S1 and S2) that is basi-cally the sum over different similarity measures,692the tendency of identical S1 ?
S2 will be quitesmall.
As a result we establish another heuristicfor ?bidirectional?
class.
If the absolute valuedifference between S1 and S2 is below the thre-shold value, our system recognizes the pair as?bidirectional?
(abs (S1 ?
S2) < threshold).
Thisthreshold has been set as 5 based on observationfrom the training file.
If the individual scores S1and S2 are below a certain threshold, again setbased on the observation in the training file, thensystem concludes the entailment class as?no_entailment?.
This threshold has been set as20 based on observation from the training file.2.2 System Architecture 2: CLTE Taskwith translated hypothesisSystem Architecture 2 is based on lexical match-ing between the text pairs (T1, T2) and basicallymeasures the same attributes as in the architec-ture 1.
In this architecture, the English hypothe-sis sentences are translated to the language ofthe text sentence (French, Italian, Spanish andGerman) using the Microsoft Bing Translator.The CLTE dataset is preprocessed after separat-ing the (T1, T2) pairs.
Preprocessing moduleincludes stop word removal and co-referencing.After preprocessing, the system executes the TEmodule for lexical matching between the textpairs.
This module comprises N-Gram matching,Text Similarity, Named Entity Matching, POSmatching and Chunking.
The TE module is ex-ecuted once with T1 as text and T2 as hypothe-sis and again with T1 as hypothesis and T2 astext.
But in this architecture N-Gram matchingand text similarity modules differ from the pre-vious architecture.
In system architecture 1, theN-Gram matching and text similarity values arecalculated on the English text translated from T1(i.e., Text in Spanish, German, French and Ital-ian languages).
In system architecture 2, the Mi-crosoft Bing translator is used to translate T2texts (in English) to different languages (i.e.
inSpanish, German, French and Italian) and calcu-late N ?
Gram matching and Text Similarityvalues on these (T1 ?
newly translated T2) pairs.Other lexical sub modules are executed as be-fore.
These lexical matching scores are storedand compared according to the heuristic definedin section 2.1.2.3 System Architecture 3: CLTE taskusing VotingThe system considers the output of the previoustwo systems (Run 1 from System architecture 1and Run 2 from System architecture 2) as input.If the entailment decision of both the runs agreesthen this is output as the final entailment label.Otherwise, if they do not agree, the final entail-ment label will be ?no_entailment?.
The votingrule can be defined as the ANDing rule wherelogical AND operation of the two inputs areconsidered to arrive at the final evaluation class.3 Experiments on Datasets and ResultsThree runs (Run 1, Run 2 and Run 3) for eachlanguage were submitted for the SemEval-3Task 8.
The descriptions of submissions for theCLTE task are as follows:?
Run1: Lexical matching between text pairs(Based on system Architecture ?
1).?
Run2: Lexical matching between text pairs(Based on System Architecture ?
2).?
Run3: ANDing Module between Run1 andRun2.
(Based on System Architecture ?3).The CLTE dataset consists of 500 trainingCLTE pairs and 500 test CLTE pairs.
The re-sults for Run 1, Run 2 and Run 3 for each lan-guage on CLTE Development set are shown inTable 1.Run Name AccuracyJU-CSE-NLP_deu-eng_run1 0.284JU-CSE-NLP_deu-eng_run2 0.268JU-CSE-NLP_deu-eng_run3 0.270JU-CSE-NLP_fra-eng_run1 0.290JU-CSE-NLP_fra-eng_run2 0.320JU-CSE-NLP_fra-eng_run3 0.278JU-CSE-NLP_ita-eng_run1 0.302JU-CSE-NLP_ita-eng_run2 0.298JU-CSE-NLP_ita-eng_run3 0.298JU-CSE-NLP_spa-eng_run1 0.270JU-CSE-NLP_spa-eng_run2 0.262JU-CSE-NLP_spa-eng_run3 0.262Table 1: Results on Development set693The comparison of the runs for different lan-guages shows that in case of deu-eng languagepair system architecture ?
1 is useful for devel-opment data whereas system architecture ?
2 ismore accurate for test data.
For fra-eng languagepair, system architecture - 2 is more accurate fordevelopment data whereas voting helps to getmore accurate results for test data.
Similar to thedeu-eng language pair, ita-eng language pairshows same trends, i.e., system architecture ?
1is more helpful for development data and systemarchitecture ?
2 is more accurate for test data.
Incase of spa-eng language pair system architec-ture ?
1 is helpful for both development and testdata.The results for Run 1, Run 2 and Run 3 for eachlanguage on CLTE Test set are shown in Table2.Run Name AccuracyJU-CSE-NLP_deu-eng_run1 0.262JU-CSE-NLP_deu-eng_run2 0.296JU-CSE-NLP_deu-eng_run3 0.264JU-CSE-NLP_fra-eng_run1 0.288JU-CSE-NLP_fra-eng_run2 0.294JU-CSE-NLP_fra-eng_run3 0.296JU-CSE-NLP_ita-eng_run1 0.316JU-CSE-NLP_ita-eng_run2 0.326JU-CSE-NLP_ita-eng_run3 0.314JU-CSE-NLP_spa-eng_run1 0.274JU-CSE-NLP_spa-eng_run2 0.266JU-CSE-NLP_spa-eng_run3 0.272Table 2: Results on Test Set4 Conclusions and Future WorksWe have participated in Task 8 of Semeval-2012named Cross Lingual Textual Entailment mainlybased on lexical matching and translation of textand hypothesis sentences in the cross lingualcorpora.
Both lexical matching and translationhave their limitations.
Lexical matching is usefulfor simple sentences but fails to retain high ac-curacy for complex sentences with number ofclauses.
Semantic graph matching or conceptualgraph is a good substitution to overcome theselimitations.
Machine learning technique isanother important tool for multi-class entailmenttask.
Features can be trained by some machinelearning tools (such as SVM, Na?ve Bayes orDecision tree etc.)
with multi-way entailment(forward, backward, bi-directional, no-entailment) as its class.
Works have been startedin these directions.AcknowledgmentsThe work was carried out under partial supportof the DST India-CONACYT Mexico project?Answer Validation through Textual Entail-ment?
funded by DST, Government of India andpartial support of the project CLIA Phase II(Cross Lingual Information Access) funded byDIT, Government of India.References[1] Negri, M., Marchetti, A., Mehdad, Y., Bentivogli,L., and Giampiccolo, D.: Semeval-2012 Task 8:Cross-lingual Textual Entailment for Content Syn-chronization.
In Proceedings of the 6th InternationalWorkshop on Semantic Evaluation (SemEval 2012).
[2]  Dagan, I., Glickman, O., Magnini, B.: ThePASCAL Recognising Textual Entailment Chal-lenge.
Proceedings of the First PASCAL Recog-nizing Textual Entailment Workshop.
(2005).
[3] Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L.,Giampiccolo, D., Magnini, B., Szpektor, I.: The-Seond PASCAL Recognising Textual EntailmentChallenge.
Proceedings of the Second PASCALChallenges Workshop on Recognising Textual En-tailment, Venice, Italy (2006).
[4] Giampiccolo, D., Magnini, B., Dagan, I., Dolan,B.
: The Third PASCAL Recognizing Textual En-tailment Challenge.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment andParaphrasing, Prague, Czech Republic.
(2007).
[5] Giampiccolo, D., Dang, H. T., Magnini, B., Da-gan, I., Cabrio, E.: The Fourth PASCAL Recogniz-ing Textual Entailment Challenge.
In TAC 2008Proceedings.
(2008)[6] Bentivogli, L., Dagan, I., Dang.
H.T., Giampicco-lo, D., Magnini, B.: The Fifth PASCAL Recogniz-ing Textual Entailment Challenge.
In TAC 2009Workshop, National Institute of Standards andTechnology Gaithersburg, Maryland USA.
(2009).
[7] Luisa Bentivogli, Peter Clark, Ido Dagan, HoaTrang Dang,Danilo Giampiccolo: The SixthPASCAL Recognizing Textual Entailment Chal-694lenge.
In TAC 2010 Notebook Proceedings.
(2010)[8] Bentivogli, L., Clark, P., Dagan, I., Dang, H.,Giampiccolo, D.: The Seventh PASCAL Recogniz-ing Textual Entailment Challenge.
In TAC 2011Notebook Proceedings.
(2011)[9] Bos, Johan, Fabio Massimo Zanzotto, and MarcoPennacchiotti.
2009.
Textual Entailment atEVALITA 2009: In Proceedings of EVALITA2009.
[10] Mehdad, Yashar, Matteo Negri, and MarcelloFederico.2010.
Towards Cross-Lingual Textualentailment.
In Proceedings of the 11th AnnualConference of the North American Chapter of theAssociation for Computational Linguistics,NAACL-HLT 2010.
LA, USA.
[11] Negri, Matteo, and Yashar Mehdad.
2010.Creating a Bilingual Entailment Corpus throughTranslations with Mechanical Turk: $100 for a10-day Rush.
In Proceedings of the NAACL-HLT2010, Creating Speech and Text Language DataWith Amazon's Mechanical Turk Workshop.
LA,USA.
[12] Mehdad, Yashar, Matteo Negri, Marcello Fede-rico.
2011.
Using Bilingual Parallel Corpora forCross-Lingual Textual Entailment.
In Proceedingsof ACL 2011.
[13] Yuret, D., Han, A., Turgut, Z.: SemEval-2010Task 12: Parser Evaluation using Textual Entail-ments.
Proceedings of the SemEval-2010 Evalua-tion Exercises on Semantic Evaluation.
(2010).
[14] H. Shima, H. Kanayama, C.-W. Lee, C.-J.
Lin,T.Mitamura, S. S. Y. Miyao, and K. Takeda.
Over-view of ntcir-9 rite: Recognizing inference in text.In NTCIR-9 Proceedings,2011.
[15]  Pakray, P., Bandyopadhyay, S., Gelbukh, A.:Lexical based two-way RTE System at RTE-5.
Sys-tem Report, TAC RTE Notebook.
(2009)[16] Pakray, P., Pal, S., Poria, S., Bandyopadhyay, S.,, Gelbukh, A.: JU_CSE_TAC: Textual EntailmentRecognition System at TAC RTE-6.
System Re-port, Text Analysis Conference Recognizing Tex-tual Entailment Track (TAC RTE) Notebook.
(2010)[17] Pakray, P., Neogi, S., Bhaskar, P., Poria, S.,Bandyopadhyay, S., Gelbukh, A.: A Textual En-tailment System using Anaphora Resolution.
Sys-tem Report.
Text Analysis ConferenceRecognizing Textual Entailment Track Notebook,November 14-15.
(2011)[18] Pakray, P., Neogi, S., Bandyopadhyay, S., Gel-bukh, A.: A Textual Entailment System using Webbased Machine Translation System.
NTCIR-9, Na-tional Center of Sciences, Tokyo, Japan.
Decem-ber 6-9, 2011.
(2011)[19]  Fellbaum, C.: WordNet: An Electronic LexicalDatabase.
MIT Press (1998).695
