Enriching the Knowledge Sources Used in a Maximum EntropyPart-of-Speech TaggerKristina ToutanovaDept of Computer ScienceGates Bldg 4A, 353 Serra MallStanford, CA 94305-9040, USAkristina @cs.stanford.eduChristopher D. ManningDepts of Computer Science and LinguisticsGates Bldg 4A, 353 Serra MallStanford, CA 94305-9040, USAmanning @cs.stanford.eduAbstractThis paper presents results for a maximum-entropy-based part of speech tagger, whichachieves superior performance principallyby enriching the information sources usedfor tagging.
In particular, we get improvedresults by incorporating these features:(i) more extensive treatment of capitaliza-tion for unknown words; (ii) features for thedisambiguation f the tense forms of verbs;(iii) features for disambiguating particlesfrom prepositions and adverbs.
The bestresulting accuracy for the tagger on thePenn Treebank is 96.86% overall, and86.91% on previously unseen words.Introduction IThere are now numerous systems for automaticassignment of parts of speech ("tagging"),employing many different machine learningmethods.
Among recent op performing methodsare Hidden Markov Models (Brants 2000),maximum entropy approaches (Ratnaparkhi1996), and transformation-based learning (Brill1994).
An overview of these and otherapproaches can be found in Manning andSchiitze (1999, ch.
10).
However, all thesemethods use largely the same informationsources for tagging, and often almost he samefeatures as well, and as a consequence they alsooffer very similar levels of performance.
Thisstands in contrast to the (manually-built) EngCGtagger, which achieves better performance byusing lexical and contextual information sourcesand generalizations beyond those available tosuch statistical taggers, as Samuelsson andVoutilainen (1997) demonstrate.i We thank Dan Klein and Michael Saunders foruseful discussions, and the anonymous reviewers formany helpful comments.This paper explores the notion that automat-ically built tagger performance can be furtherimproved by expanding the knowledge sourcesavailable to the tagger.
We pay special attentionto unknown words, because the markedly loweraccuracy on unknown word tagging means thatthis is an area where significant performancegains seem possible.We adopt a maximum entropy approachbecause it allows the inclusion of diversesources of information without causing frag-mentation and without necessarily assumingindependence b tween the predictors.
A maxi-mum entropy approach as been applied to part-of-speech tagging before (Ratnaparkhi 1996),but the approach's ability to incorporate non-local and non-HMM-tagger-type evidence hasnot been fully explored.
This paper describes themodels that we developed and the experimentswe performed to evaluate them.1 The Baseline Maximum Entropy ModelWe started with a maximum entropy basedtagger that uses features very similar to the onesproposed in Ratnaparkhi (1996).
The taggerlearns a loglinear conditional probability modelfrom tagged text, using a maximum entropymethod.The model assigns a probability for everytag t in the set T of possible tags given a wordand its context h, which is usually def'med as thesequence of several words and tags precedingthe word.
This model can be used for estimatingthe probability of a tag sequence h...tn given asentence w~.
.
.wn:n np(t,...t n I wl.. .w,) = I~I p(t, \[t~.. 2,_,,w~...w,) =I I  p(ti I h~)iffil i=!As usual, tagging is the process of assigning themaximum likelihood tag sequence to a string ofwords.The idea of maximum entr?py modeling isto choose the probability distribution p that hasthe highest entropy out of those distributions63that satisfy a certain set of constraints.
Theconstraints restrict the model to behave inaccordance with a set of statistics collected fromthe training data.
The statistics are expressed asthe expected values of appropriate functionsdefined on the contexts h and tags t. In particu-lar, the constraints demand that the expectationsof the features for the model match theempirical expectations of the ?features over thetraining data.For example, if we want to constrain themodel to tag make as a verb or noun with thesame frequency as the empirical model inducedby the training data, we define the features:f l (h , t )= l  iff w i=makeandt=NNf2 (h , t )= l  iff w i=makeandt=VBSome commonly used statistics for part ofspeech tagging are: how often a certain wordwas tagged in a certain way; how often two tagsappeared in sequence or how often three tagsappeared in sequence.
These look a lot like thestatistics a Markov Model would use.
However,in the maximum entropy framework it ispossible to easily define and incorporate muchmore complex statistics, not restricted to n-gramsequences.The constraints in our model are that theexpectations of these features according to thejoint distribution p are equal to the expectationsof the features in the empirical (training data)distribution ~ : Ep~h.,)fi (h, t) = E~h,,) ~ (h, t).Having defined a set of constraints that ourmodel should accord with, we proceed to findthe model satisfying the constraints that maxi-mizes the conditional entropy of p .
The intu-ition is that such a model assumes nothing apartfrom that it should satisfy the given constraints.Following Berger et al (1996), we approxi-mate p(h,t), the joint distribution of contextsand tags, by the product of ~(h) ,  the empiricaldistribution of histories h, and the conditionaldistribution p(t l h): p(h,t) = ~(h).
p(t lh).Then for the example above, our constraintswould be the following, for j E {1,2}:~(h, t)f  i (h, t) = ~ ,~(h)p(t \[h)f  i (h, t)hEH.tET hsH, t~TThis approximation is used to enableefficient computation.
The expectation for a fea-ture f is:E f= ~(h)p( t lh ) f (h , t  )h~ H ,tE Twhere H is the space of possible contexts hwhen predicting a part of speech tag t. Since thecontexts contain sequences of words and tagsand other information, the space H is huge.
Butusing this approximation, we can instead sumjust over the smaller space of observed contextsX in the training sample, because the empiricalprior ~(h) is zero for unseen contexts h:E f = 2~(h)p( t lh ) f (h , t )  (1)h~ X, t~TThe model that is a solution to thisconstrained optimization task is an exponential(or equivalently, loglinear) model with the para-metric form:p(t\[h) = j=L...Kn ea//"h")t~T j=I...,Kwhere the denominator is a normalizing term(sometimes referred to as the partition function).The parameters X: correspond to weights for thefeatures 3T-We will not discuss in detail the characteris-tics of the model or the parameter estimationprocedure used - Improved Iterative Scaling.For a more extensive discussion of maximumentropy methods, see Berger et al (1996) andJelinek (1997).
However, we note that our pa-rameter estimation algorithm directly uses equa-tion (1).
Ratnaparkhi (1996: 134) suggests useof an approximation summing over the trainingdata, which does not sum over possible tags:" h E f j = 2 P( ~)p(ti lhi)f j(hi,ti)i=1However, we believe this passage is in error:such an estimate is ineffective in the iterativescaling algorithm.
Further, we note that expecta-tions of the form (1) appear in Ratnaparkhi(1998: 12).1.1 Features in the Baseline ModelIn our baseline model, the context availablewhen predicting the part of speech tag of a wordwi in a sentence of words {wl... wn} with tags{tl... t~} is {ti.l tin wi wi+l}.
The features thatdefine the constraints on the model are obtainedby instantiation of feature templates as inRatnaparkhi (1996).
Special feature templatesexist for rare words in the training data, toincrease the model's predictioff-capacity forunknown words.64The actual feature templates for this modelare shown in the next table.
They are a subset ofthe features used in Ratnaparkhi (1996).No.
Feature Type Template1.
General wi=X & ti =T2.
General b.l=Tl & ti=T3.
General tia=T\] & ti.2=T2 & ti=T4.
General Wi+I=X & ti =T5.
Rare Suffix of wi =S,IS1<5 ,~ t,=T6.
Rare Prefix of w~=P, l<IPI<5& ti=T7.
Rare w~ contains anumber& t~=T8.
Rare wi contains an uppercasecharacter & t~=T9.
Rare w~ contains ahyphen& ti=TTable 1 Baseline Model FeaturesGeneral feature templates can be instantiated byarbitrary contexts, whereas rare feature tem-plates are instantiated only by histories wherethe current word wi is rare.
Rare words aredefined to be words that appear less than acertain number of times in the training data(here, the value 7 was used).In order to be able to throw out features thatwould give misleading statistics due to sparse-ness or noise in the data, we use two differentcutoff values for general and rare featuretemplates (in this implementation, 5 and 45respectively).
As seen in Table 1 the features areconjunctions of a boolean function on thehistory h and a boolean function on the tag t.Features whose first conjuncts are true for morethan the corresponding threshold number ofhistories in the training data are included in themodel.The feature templates in Ratnaparkhi (1996)that were left out were the ones that look at theprevious word, the word two positions beforethe current, and the word two positions after thecurrent.
These features are of the same form astemplate 4 in Table 1, but they look at words indifferent positions.Our motivation for leaving these featuresout was the results from some experiments onsuccessively adding feature templates.
Addingtemplate 4 to a model that incorporated thegeneral feature templates 1 to 3 only and therare feature templates 5-8 significantlyincreased the accuracy on the development set -from 96.0% to 96.52%.
The addition of afeature template that looked at the precedingword and the current ag to the resulting modelslightly reduced the accuracy.1.2 Testing and PerformanceThe model was trained and tested on the part-of-speech tagged WSJ section of the PennTreebank.
The data was divided into contiguousparts: sections 0-20 were used for training,sections 21-22 as a development test set, andsections 23-24 as a final test set.
The data setsizes are shown below together with numbers ofunknown words.Data Set Tokens UnknownTraining 1,061,768Development 116,206 3271 (2.81%)Test 111,221 2879 (2.59%)Table 2 Data SizesThe testing procedure uses a beam search tofind the tag sequence with maximal probabilitygiven a sentence.
In our experiments we used abeam of size 5.
Increasing the beam size did notresult in improved accuracy.The preceding tags for the word at thebeginning of the sentence are regarded ashaving the pseudo-tag NA.
In this way, theinformation that a word is the first word in asentence is available to the tagger.
We do nothave a special end-of-sentence symbol.We used a tag dictionary for known wordsin testing.
This was built from tags found in thetraining data but augmented so as to capture afew basic systematic tag ambiguities that arefound in English.
Namely, for regular verbs the-ed form can be either a VBD or a VBN andsimilarly the stem form can be either a VBP orVB.
Hence for words that had occurred withonly one of these tags in the training data theother was also included as possible forassignment.The results on the test set for the Baselinemodel are shown in Table 3.Model Overall Unknown WordAccuracy AccuracyBaseline , 96.72% 84.5%JRatnaparkhi 96.63% 85.56%(1996)Table 3 Baseline model performanceThis table also shows the results reported inRatnaparkhi (1996: 142)for COnvenience.
Theaccuracy figure for our model is higher overall65but lower for unknown words.
This may stemfrom the differences between the two models'feature templates, thresholds, and approxi-mations of the expected values for the features,as discussed in the beginning of the section, ormay just reflect differences in the choice oftraining and test sets (which are not preciselyspecified in Ratnaparkhi (1996)).The differences are not great enough tojustify any definite statement about the differentuse of feature templates or other particularitiesof the model estimation.
One conclusion that wecan draw is that at present he additional wordfeatures used in Ratnaparkhi (1996) - looking atwords more than one position away from thecurrent - do not appear to be helping the overallperformance of the models.1.3 Discussion of Problematic CasesA large number of words, including many of themost common words, can have more than onesyntactic category.
This introduces a lot ofambiguities that the tagger has to resolve.
Someof the ambiguities are easier for taggers toresolve and others are harder.Some of the most significant confusions thatthe Baseline model made on the test set can beseen in Table 5.
The row labels in Table 5signify the correct ags, and the column labelssignify the assigned tags.
For example, the num-ber 244 in the (NN, JJ) position is the number ofwords that were NNs but were incorrectlyassigned the JJ category.
These particular confu-sions, shown in the table, account for a largepercentage of the total error (2652/3651 =72.64%).
Table 6 shows part of the Baselinemodel's confusion matrix for just unknownwords.Table 4 shows the Baseline model's overallassignment accuracies for different parts ofspeech.
For example, the accuracy on nouns isgreater than the accuracy on adjectives.
Theaccuracy on NNPS (plural proper nouns) is asurprisingly low 41.1%.Tag Accuracy TagIN 97.3% JJNN 96.5% RBNNP 96.2% VBNVBD 95.2% RPVB 94.0% NNPSVBP 93.4%Accuracy93.0%92.2%90.4%41.5%41.1%Table 4 Accuracy of assignments for different partsof speech for the Baseline model.Tagger errors are of various types.
Some are theresult of inconsistency in labeling in the trainingdata (Ratnaparkhi 1996), which usually reflectsa lack of linguistic clarity or determination ofthe correct part of speech in context.
Forinstance, the status of various noun premodifiers(whether chief or maximum is NN or JJ, orwhether a word in -ing is acting as a JJ or VBG)is of this type.
Some, such as errors betweenNN/NNP/NNPS/NNS largely reflect difficultieswith unknown words.
But other cases, such asVBN/VBD and VB/VBP/NN, represent syste-matic tag ambiguity patterns in English, forwhich the fight answer is invariably clear incontext, and for which there are in general goodstructural contextual clues that one should beable to use to disarnbiguate.
Finally, in anotherclass of cases, of which the most prominent isprobably the RP/IN/RB ambiguity of words likeup, out, and on, the linguistic distinctions, whilehaving a sound empirical basis (e.g., see Baker(1995: 198-201), are quite subtle, and oftenrequire semantic intuitions.
There are not goodsyntactic ues for the correct ag (and further-more, human taggers not infrequently makeerrors).
Within this classification, the greatesthopes for tagging improvement appear to comefrom minimizing errors in the second and thirdclasses of this classification.In the following sections we discuss how weinclude additional knowledge sources to help inthe assignment of tags to forms of verbs,capitalized unknown words, particle words, andin the overall accuracy of part of speechassignments.2 Improving the Unknown Words ModelThe accuracy of the baseline model is markedlylower for unknown words than for previouslyseen ones.
This is also the case for all othertaggers, and reflects the importance of lexicalinformation to taggers: in the best accuracyfigures punished for corpus-based taggers,known word accuracy is around 97%, whereasunknown word accuracy is around 85%.In following experiments, we examinedways of using additional features to improve theaccuracy of tagging unknown words.
As previ-ously discussed in Mikbeev (1999), it is possibleto improve the accuracy on capitalized wordsthat might be proper nouns or the first word in asentence, etc.. r .66JJ NN NNP NNPS R.B RP IN VB VBD VBN VBP TotalJJ 0 177 56 0 61 2 5 10 15 108 0 488NN 244 0 103 0 12 1 1 29 5 6 19 525NNP 107/ 106 0 132 5 0 7 5 I 2 0 427NNPS 1 0 110 0 0 0 0 0 0 0 0 142RB 72 21 7 0 0 16 138 1 0 0 0 295RP 0 0 0 0 39 0 65 0 0 0 0 104IN 11 0 1 0 169 103 0 1 0 0 0 323VB 17 64 9 0 2 0 1 0 4 7 85 189VBD 10 5 3 0 0 0 0 3 0 143 2 166VBN 101 3 3 0 0 0 0 3 108 0 1 221VBP 5 34 3 1 1 0 2 49 6 3 0 104Total 626 536 348 144 317 122 279 102 140 269 108 3651Table 5 Confusion matrix of the Baseline model showing top confusion pairs overallJJ NN NNP NNS NNPS VBN TotalJJ 0 55 25 1 0 10 107NN 55 0 26 5 0 2 98NNP 20 41 0 5 4 0 87NNPS 0 0 10 11 0 0 23NNS 1 3 6 0 1 0 15VBN 12 1 1 0 0 0 20Total 109 121 98 33 7 19 448Table 6 Confusion matrix of the Baseline model for unknown words showing top confusion pairsAccuracy Test SetUnknown Words Accuracy Test SetAccuracy Development SetUnknown Words Accuracy Development SetBaseline Model 1 Model 2 Model 3Capitalization Verb forms Particles96.72% 96.76% 96.83% 96.86%84.50% 86.76% 86.87% 86.91%96.53% 96.55% 96.58% 96.62%85.48% 86.03% 86.03% 86.06%Table 7 Accuracies of all models on the test and development setsBaseline Model 1 Model 2 Model 3Capitalization Verb Forms Particles1.
Current word 15,832 15,832 15,837 15,9272.
Previous tag 1,424 1,424 1,424 1,4243.
Previous two tags 16,124 16,124 16,124 16,1244.
Next word 80,075 80,075 80,075 80,0755.
Suffixes 3,361 3,361 3,361 3,3876.
Prefixes 5,311 0 0 07.
Contains uppercase character 34 34 34 348.
Contains number 7 7 7 79.
Contains hyphen 20 20 20 2010.
Capitalized and mid.
sentence 0 33 33 3311.
All letters uppercase 0 30 30 3012.
VBPIVB feature 0 0 2 213.
VBDIVBN feature 0 0 3 314.
Particles, type 1 0 0 0 915.
Particles, type 2 0 0 0 2,178Total 122,188 116,940 116,960 118,944Table 8 Number of features of different ypes67For example, the error on the proper nouncategory (NNP) accounts :for a significantlylarger percent of the total error for unknownwords than for known words.
In the Baselinemodel, of the unknown word error 41.3% is dueto words being NNP and assigned to some othercategory, or being of other category and assignedNNP.
The percentage of the same type of errorfor known words is 16.2%.The incorporation of the following twofeature schemas greatly improved NNP accuracy:(1) A feature that looks at whether all the lettersof a word are uppercase.
The feature thatlooked at capitalization before (cf.
Table 1,feature No.
8) is activated when the wordcontains an uppercase character.
This turnsout to be a notable distinction because, forexample, in titles in the WSJ data all wordsare in all uppercase, and the distribution oftags for these words is different from theoverall distribution for words that contain anuppercase character.
(2) A feature that is activated when the wordcontains an uppercase character and it is notat the start of a sentence.
These word tokensalso have a different ag distribution from thedistribution for all tokens that contain anuppercase character.Conversely, empirically it was found that theprefix features for rare words were having a netnegative ffect on accuracy.
We do not at presenthave a good explanation for this phenomenon.The addition of the features (1) and (2) andthe removal of the prefix features considerablyimproved the accuracy on unknown words andthe overall accuracy.
The results on the test setafter adding these features are shown below:Overall Accuracy Unknown Word Accuracy \[96.76% 86.76% ITable 9 Accuracy when adding capitalization featuresand removing prefix features.Unknown word error is reduced by 15% ascompared to the Baseline model.It is important to note that (2) is composed ofinformation already 'known' to the tagger insome sense.
This feature can be viewed as theconjunction of two features, one of which isalready in the baseline model, and the other ofwhich is the negation of a feature existing in thebaseline model - since for words at the beginningof a sentence, the preceding tag is the pseudo-tagNA, and there is a feature looking at thepreceding tag.
Even though-our maximumentropy model does not require independenceamong the predictors, it provides for free only asimple combination of feature weights, andadditional 'interaction terms' are needed to modelnon-additive interactions (in log-space terms)between features.3 Features for Disambiguating Verb FormsTwo of the most significant sources of classifiererrors are the VBN/VBD ambiguity and theVBP/VB ambiguity.
As seen in Table 5,VBN/VBD confusions account for 6.9% of thetotal word error.
The VBP/VB confusions are asmaller 3.7% of the errors.
In many cases it iseasy for people (and for taggers) to determine thecorrect form.
For example, if there is a toinfinitive or a modal directly preceding theVB/VBP ambiguous word, the form is certainlynon-finite.
But often the modal can be severalpositions away from the current position - stillobvious to a human, but out of sight for thebaseline model.To help resolve a VB/VBP ambiguity in suchcases, we can add a feature that looks at thepreceding several words (we have chosen 8 as athreshold), but not across another verb, andactivates if there is a to there, a modal verb, or aform of do, let, make, or help (verbs thatfrequently take a bare infinitive complement).Rather than having a separate feature look ateach preceding position, we define one featurethat looks at the chosen number of positions tothe left.
This both increases the scope of theavailable history for the tagger and provides abetter statistic because it avoids fragmentation.We added a similar feature for resolvingVBD/VBN confusions.
It activates if there is ahave or be auxiliary form in the preceding severalpositions (again the value 8 is used in theimplementation).The form of these two feature templates wasmotivated by the structural rules of English andnot induced from the training data, but it shouldbe possible to look for "predictors" for certainparts of speech in the preceding words in thesentence by, for example, computing associationstrengths.The addition of the two feature schemashelped reduce the VB/VBP and VBD/VBN con-fusions.
Below is the performance on the test setof the resulting model when features for disam-biguating verb forms are added to the model ofSection 2.
The number of VB/VBP confusions68was reduced by 23.1% as compared to the base-line.
The number of VBD/VBN confusions wasreduced by 12.3%.Overall Accuracy Unknown Word Accuracy96.83% 86.87%Table 10 Accuracy of the extended model4 Features for Particle DisambiguationAs discussed in section 1.3 above, the task ofdetermining RB/RP/IN tags for words like down,out, up is difficult and in particular examples,there are often no good local syntactic indicators.For instance, in (2), we find the exact samesequence of parts of speech, but (2a) is a particleuse of on, while (2b) is a prepositional use.Consequently, the accuracy on the rarer RP(particles) category is as low as 41.5% for theBaseline model (cf.
Table 4).
(2) a. Kim took on the monster.b.
Kim sat on the monster.We tried to improve the tagger's capability toresolve these ambiguities through adding infor-mation on verbs' preferences to take specificwords as particles, or adverbs, or prepositions.There are verbs that take particles more thanothers, and particular words like out are muchmore likely to be used as a particle in the contextof some verb than other words ambiguousbetween these tags.We added two different feature templates tocapture this information, consisting as usual of apredicate on the history h, and a condition on thetag t. The first predicate is true if the current wordis often used as a particle, and if there is a verb atmost 3 positions to the left, which is "known" tohave a good chance of taking the current word asa particle.
The verb-particle pairs that are knownby the system to be very common were collectedthrough analysis of the training data in apreprocessing stage.The second feature template has the form:The last verb is v and the current word is w and whas been tagged as a particle and the current agis t. The last verb is the pseudo-symbol NA ifthere is no verb in the previous three positions.These features were some help in reducingthe RB/IN/RP confusions.
The accuracy on theRP category rose to 44.3%.
Although the overallconfusions in this class were reduced, some of theerrors were increased, for example, the number ofINs classified as RBs rose slightly.
There seemsto be still considerable room to improve theseresults, though the attainable accuracy is limitedby the accuracy with which these distinctions aremarked in the Penn Treebank (on a quickinformal study, this accuracy seems to be around85%).
The next table shows the final performanceon the test set.OverallAccuracy Unknown Word Accuracy \[96.86% 86.91%Table 11 Accuracy of the final modelFor ease of comparison, the accuracies of allmodels on the test and development sets areshown in Table 7.
We note that accuracy is loweron the development set.
This presumably corre-sponds with Charniak's (2000: 136) observationthat Section 23 of the Penn Treebank is easierthan some others.
Table 8 shows the differentnumber of feature templates of each kind thathave been instantiated for the different models aswell as the total number of features each modelhas.
It can be seen that the features which helpdisambiguate verb forms, which look at capital-ization and the first of the feature templates forparticles are a very small number as compared tothe features of the other kinds.
The improvementin classification accuracy therefore comes at theprice of adding very few parameters to themaximum entropy model and does not result inincreased model complexity.ConclusionEven when the accuracy figures for corpus-basedpart-of-speech taggers start to look extremelysimilar, it is still possible to move performancelevels up.
The work presented in this paperexplored just a few information sources inaddition to the ones usually used for tagging.While progress is slow, because ach new featureapplies only to a limited range of cases,nevertheless the improvement in accuracy ascompared to previous results is noticeable,particularly for the individual decisions on whichwe focused.The potential of maximum entropy methodshas not previously been fully exploited for thetask of assignment of parts of speech.
We incor-porated into a maximum entropy-based taggermore linguistically sophisticated features, whichare non-local and do not look just at particularpositions in the text.
We also added features thatmodel the interactions of previously employed69predictors.
All of these changes led to modestincreases in tagging accuracy.This paper has thus presented some initialexperiments in improving tagger accuracythrough using additional information sources.
Inthe future we hope to explore automaticallydiscovering information sources that can beprofitably incorporated into maximum entropypart-of-speech prediction.ReferencesBaker, C. L. 1995.
English Syntax.
Cambridge, MA:MIT Press, 2 nd edition.nerger, Adam L., Della Pietra, Stephen A., and DellaPietra, Vincent J.
1996.
A Maximum EntropyApproach to Natural Language Processing.
Compu-tational Linguistics 22:39-71.Brants, Thorsten.
2000.
TnT - A Statistical Part-of-Speech Tagger.
In Proceedings of the Sixth AppliedNatural Language Processing Conference (ANLP2000), Seattle, WA, pp.
224-23 I.Brill, Eric.
1994.
Some Advances in Transformation-Based Part of Speech Tagging.
Proceedings ofAAAL Vol.
1, pp.
722-727.Charniak, Eugene.
2000.
A Maximum-Entropy-Inspired Parser.
Proceedings of the I st Meeting ofthe North American Chapter of the Association forComputational Linguistics, pp.
132-139.Jelinek, Frederick.
1997.
Statistical Methods forSpeech Recognition.
Cambridge, MA: MIT Press.Manning, Christopher D. and Hinrich Schiitze.
1999.Foundations of Statistical Natural LanguageProcessing.
Cambridge, MA: MIT Press.Mikheev, Andrei.
1999.
Periods, Capitalized Words,etc.
Ms., University of Edinburgh.
Available at:http:llwww.ltg.ed.ac.ukl-mikheevlpapers.htmlRatnaparkhi, Adwait.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, University of Pennsylvania,pp.
133--142.Ratnaparkhi, Adwait.
1998.
Maximum EntropyModels for Natural Language Ambiguity Resolu-tion.
PhD Thesis, University of Pennsylvania.Samuelsson, Christer and Atro Voutilainen.
1997.Comparing a Linguistic and a Stochastic Tagger.
InProceedings of the 25 th Annual Meeting of theAssociation for Computational Linguistics, pp.
246-253.70
