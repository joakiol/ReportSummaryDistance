Boosting Applied to Tagging and PP AttachmentSteven Abney{abney,Robert E. SchapireAT&T Labs - Research180 Park AvenueFlorham Park, NJ 07932schapire,Yoram Singersinger}@research.att.comAbstractBoosting is a machine learning algorithm that is notwell known in computational linguistics.
We ap-ply it to part-of-speech tagging and prepositionalphrase attachment.
Performance is very encourag-ing.
We also show how to improve data quality byusing boosting to identify annotation errors.1 IntroductionBoosting is a machine learning algorithm that hasbeen applied successfully to a variety of problems,but is almost unknown in computational linguis-tics.
We describe experiments in which we applyboosting to part-of-speech tagging and prepositionalphrase attachment.
Results on both PP-attachmentand tagging are within sampling error of the bestprevious results.The current best technique for PP-attachment(backed-off density estimation) does not performwell for tagging, and the current best technique fortagging (maxent) is below state-of-the-art on PP-attachment.
Boosting achieves tate-of-the-art per-formance on both tasks simultaneously.The idea of boosting is to combine many sim-ple "rules of thumb," such as "the current word isa noun if the previous word is the."
Such rules of-ten give incorrect classifications.
The main idea ofboosting is to combine many such rules in a prin-cipled manner to produce a single highly accurateclassification rule.There are similarities between boosting andtransformation-based l arning (Brill, 1993): bothbuild classifiers by combining simple rules, andboth are noted for their resistance to overfitting.But boosting, unlike transformation-based l arning,rests on firm theoretical foundations; and it outper-forms transformation-based l arning in our experi-ments.There are also superficial similarities betweenboosting and maxent.
In both, the parameters areweights in a log-linear function.
But in maxent, thelog-linear function defines a probability, and the ob-jective is to maximize likelihood, which may notminimize classification error.
In boosting, the log-linear function defines a hyperplane dividing exam-ples into (binary) classes, and boosting minimizesclassification error directly.
Hence boosting is usu-ally more appropriate when the objective is classifi-cation rather than density estimation.A notable property of boosting is that it maintainsan explicit measure of how difficult it finds partic-ular training examples to be.
The most difficult ex-amples are very often mislabelled examples.
Hence,boosting can contribute to improving data quality byidentifying annotation errors.2 The boosting algorithm AdaBoostIn this section, we describe the boosting algo-rithm AdaBoost that we used in our experiments.AdaBoost was first introduced by Freund andSchapire (1997); the version described here is a(slightly simplified) version of the one given bySchapire and Singer (1998).
A formal descrip-tion of AdaBoost is shown in Figure 1.
AdaBoosttakes as input a training set of m labeled exam-ples ( (x l ,y l ) , .
.
.
,  (Xrn, Ym)) where xi is an exam-ple (say, as described by a vector of attribute val-ues), and Yi E {-1, -l--l} is the label associated withxi.
For now, we focus on the binary case, in whichonly two labels (positive or negative) are possible.Multiclass problems are discussed later.Formally, the rules of thumb mentioned in theintroduction are called weak hypotheses.
Boost-ing assumes access to an algorithm or subroutinefor generating weak hypotheses called the weaklearner.
Boosting can be combined with any suit-able weak learner; the one that we used will be de-scribed below.AdaBoost calls the weak learner repeatedly in aseries of rounds.
On round t, AdaBoost provides theweak learner with a set of importance weights overthe training set.
In response, the weak learner com-38Given: (xl, y l ) , .
.
.
,  (Xm, Ym)where xi E X,  Yi E {-1,  +1}Initialize Di (i) =: 1/m.Fort = 1 , .
.
.
,T :?
Train weak learner using distribution Dt.?
Get weak hypothesis ht : X -4 ~.?
Update:Dt+i(i) = Dt( i) exp(-yiht(xi)  )Ztwhere Zt is a normalization factor (chosen sothat Dt+l will be a distribution).Output he final hypothesis:Figure 1: The boosting algorithm AdaBoost.putes a weak hypothesis ht that maps each examplex to a real number ht(x).
The sign of this num-ber is interpreted as the predicted class ( -1  or +1)of example z, while the magnitude \]ht(z)\] is inter-preted as the level of confidence in the prediction,with larger values corresponding to more confidentpredictions.The importance weights are maintained formallyas a distribution over the training set.
We writeDr(i) to denote the weight of the ith training ex-ample (xi, Yi) on the tth round of boosting.
Ini-tially, the distribution is uniform.
Having obtained ahypothesis ht from the weak learner, AdaBoost up-dates the weights by multiplying the weight of eachexample i by I e -ylht(xi).
If ht incorrectly classifiedexample i so that ht (xi) and Yi disagree in sign, thenthis has the effect of increasing the weight on thisexample, and conversely the weights of correctlyclassified examples are decreased.
Moreover, thegreater the confidence of the prediction (that is, thegreater the magnitude of ht(xi) ), the more drasticwill be the effect of the update.
The weights are thenrenormalized, resulting in the update rule shown inthe figure.In our experiments, we used cross validation tochoose the number of rounds T. After T rounds,JSchapire and Singer (1998) multiply instead byexp(-yioetht(xi)) where at  E ~ is a parameter that needs tobe set.
In the description presented here, we fold a t  into ht.AdaBoost outputs a final hypothesis which makespredictions using a simple vote of the weak hy-potheses' predictions, taking into account he vary-ing confidences of the different predictions.
A newexample x is classified usingT=t= lwhere the label predicted for x is sign(ff(x)).2.1 Finding weak hypothesesIn this section, we describe the weak learner usedin our experiments.
Since we now focus on whathappens on a single round of boosting, we will dropt subscripts where possible.Schapire and Singer (1998) prove that the train-ing error of the final hypothesis i  at most yItr=l Zt.This suggests that the training error can be greedilydriven down by designing a weak learner which, onround t of boosting, attempts to find a weak hypoth-esis h that minimizesmZ = ~ D(i)exp(-yih(xi)) .i=1This is the principle behind the weak learner used inour experiments.In all our experiments, we use very simple weakhypotheses that test the value of a Boolean predi-cate and make a prediction based on that value.
Thepredicates used are of the form "a = v", for a anattribute and v a value; for example, "PreviousWord= the".
In the PP-attachment experiments, we alsoconsidered conjunctions of such predicates.
If, on agiven example x, the predicate holds, the weak hy-pothesis outputs prediction Pl, otherwise P0, wherePl and P0 are determined by the training data in away we describe shortly.
In this setting, weak hy-potheses can be identified with predicates, which inturn can be thought of as features of the examples;thus, in this setting, boosting can be viewed as afeature-selection method.Let ?
(z) E {0, 1} denote the value of the pred-icate ?
on the example z, and for b E {0, 1}, letPb E IR be the prediction of the weak hypothe-sis when ?
(x) = b.
Then we can write simplyh(x) = PC(z).
Given a predicate ?, we choose P0and Pl to minimize Z. Schapire and Singer (1998)show that Z is minimized when we letpb= ?
In ) (1)39MF tag O 7.66Markov 1-gram B 6.74Markov 3-gram W 3.7Markov 3-gram B 3.64Decision tree M 3.5Transformation B 3.39Maxent R 3.37Maxent O 3.11 ~.07Multi-tagger Voting B 2.84 :t=.03Table 1: TB-WSJ testing error previously reportedin the literature.
B = (Brill and Wu, 1998); M= (Magerman, 1995); O = our data; R = (Ratna-parkhi, 1996); W = (Weischedel and others, 1993).forb  E {0,1} where Ws b is thesum of D(i) forexamples i such that yi = s and ?
(xi) = b. Thischoice of p# implies thatZ:  2 Z CW+blW-bl" (2)bE(O,1}This expression can now be minimized over allchoices of ?.Thus, our weak learner works by searching forthe predicate ?
that minimizes Z of Eq.
(2), andthe resulting weak hypothesis h(x) predicts Pc(z)of Eq.
(1) on example x.In practice, very large values of p0 and pl cancause numerical problems and may also lead tooverfitting.
Therefore, we usually "smooth" thesevalues using the following alternate choice of Pbgiven by Schapire and Singer (1998):(W+ba t-q'-~) pb = ?
In \~-~ (3)where e is a small positive number.2.2 Multiclass problemsSo far, we have only discussed binary classificationproblems.
In the multiclass case (in which morethan two labels are possible), there are many pos-sible extensions of AdaBoost (Freund and Schapire,1997; Schapire, 1997; Schapire and Singer, 1998).Our default approach to multiclass problems is touse Schapire and Singer's (1998) AdaBoost.MH al-gorithm.
The main idea of this algorithm is to re-gard each example with its multiclass label as sev-eral binary-labeled examples.More precisely, suppose that the possible classesare 1 , .
.
.
, k .
We map each original example x40with label y to k binary labeled derived examples(x, 1) , .
.
.
,  (x, k) where example (x, c) is labeled+1 if c = y and -1  otherwise.
We then essen-tially apply binary AdaBoost to this derived prob-lem.
We maintain a distribution over pairs (x, c),treating each such as a separate xample.
Weak hy-potheses are identified with predicates over (x, c)pairs, though they now ignore c, so that we cancontinue to use the same space of predicates asbefore.
The prediction weights c c P0, Pl, however,are chosen separately for each class c; we haveht(x, c) = P~,(z)" Given a new example x, the finalhypothesis makes confidence-weighted predictionsf (x, c) = }2tr=l ht(x, c) for each of the discrimina-tion questions (c = 1?
c = 2?
etc.
); the class is pre-dicted to be the value of c that maximizes f (x ,  c).For more detail, see the original paper (Schapire andSinger, 1998).When memory limitations prevent he use of Ad-aBoost.MH, an alternative we have pursued is touse binary AdaBoost to train separate discrimina-tors (binary classifiers) for each class, and com-bine their output by choosing the class c that max-imizes re(x), where fc(x) is the final confidence-weighted prediction of the discriminator for classc.
Let us call this algorithm AdaBoost.MI (multi-class, independent discriminators).
It differs fromAdaBoost.MH in that predicates are selected inde-pendently for each class; we do not require thatthe weak hypothesis at round t be the same for allclasses.
The number of rounds may also differ fromdiscriminator to discriminator.3 Tagging3.1 CorpusTo facilitate comparison with previous results, weused the UPenn Treebank corpus (Marcus et al,1993).
The corpus uses 80 labels, which comprise45 parts of speech properly so-called, and 35 inde-terminate tags, representing annotator uncertainty.We introduce an 81 st label, ##, for paragraph sepa-rators.An example of an indeterminate tag is NNIO'd,which indicates that the annotator could not decidebetween NN and ,30.
The "right" thing to do with in-determinate tags would either be to eliminate themor to count the tagger's output as correct if it agreeswith any of the alternatives.
Previous work appearsto treat hem as separate tags, however, and we havefollowed that precedent.We partitioned the corpus into three samples: atest sample consisting of 1000 randomly selectedambigunambigunknowntotaln errors percent contrib28,557 (52.7%) 1396 4.89 2.5824,533 (45.3%) 167 0.68 0.311104 (2.0%) 213 19.29 0.3954,194 1776 3.28 +0.08Table 2: Performance of the multi-discriminator approach.paragraphs (54,194 tokens), a development sam-ple, also of 1000 paragraphs (52,087 tokens), anda training sample' (1,207,870 tokens).Some previously reported results on the Treebankcorpus are summarized in Table 1.
These results areall based on the Treebank corpus, but it appears thatthey do not all use the same training-test split, northe same preprocessing, hence there may be differ-ences in details of examples and labels.
The "MFtag" method simply uses the most-frequent tag fromtraining as the predicted label.
The voting schemecombines the outputs of four other taggers.3.2 Applying Boosting to TaggingThe straightforward way of applying boosting totagging is to use AdaBoost.MH.
Each word tokenrepresents an example, and the classes are the 81part-of-speech tags.
Weak hypotheses are identi-fied with "attribute=value" predicates.
We use arather spare attribute set, encoding less context hanis usual.
The attributes we use are:?
Lexical attributes: The current word as adowncased string (S); its capitalization (C);and its most-frequent tag in training (T).
T isunknown for unknown words.?
Contextual attributes: the string (LS), capi-talization (LC), and most-frequent tag (LT) ofthe preceding word; and similarly for the fol-lowing word (RS, RC, RT).?
Morphological attributes: the inflectionalsuffix (I) of the current word, as provided byan automatic stemmer; also the last two ($2)and last three ($3) letters of the current word.We note in passing that the single attribute T is agood predictor of the correct label; using T as thepredicted label gives a 7.7% error rate (see Table 1).Experiment 1.
Because of memory limitations,we could not apply AdaBoost.MH to the entiretraining sample.
We examined several approxima-tions.
The simplest approximation (experiment 1)is to run AdaBoost.MH on 400K training examples,41Exp.
1 400K training 3.68 + .08Exp.
2 4?300K 3.32+.08Exp.
3 Unambiguous & definite 3.59 ?
.08Exp.
4 AdaBoost.MI 3.28 4- .08Table 3: Performance on experiments 1-4.instead of the full training set.
Doing so yields a testerror of 3.68%, which is actually as good as usingMarkov 3-grams (Table 1).Experiment 2.
In experiment 2, we divided thetraining data into four quarters, trained a classifierusing AdaBoost.MH on each quarter, and combinedthe four classifiers using (loosely speaking) a finalround of boosting.
This improved test error signif-icantly, to 3.32%.
In fact, this tagger performs aswell as any single tagger in Table 1 except he Max-ent tagger.Experiment 3.
In experiment 3, we reduced thetraining sample by eliminating unambiguous words(multiple tags attested in training) and indefinitetags.
We examined all indefinite-tagged xamplesand made a forced choice among the alternatives.The result is not strictly comparable to results onthe larger tagset, but since only 5 out of 54K testexamples are affected, the difference is negligible.This yielded a multiclass problem with 648K exam-ples and 39 classes.
We constructed a separate clas-sifier for unknown words, using AdaBoost.MH.
Weused hapax legomena (words appearing once) fromour training sample to train it.
The error rate on un-known words was 19.1%.
The overall test error ratewas 3.59%, intermediate between the error rates inthe two previous experiments.Experiment 4.
One obvious way of reducing thetraining data would be to train a separate classifierfor each word.
However, that approach would re-sult in extreme data fragmentation.
An alternativeis to cut the data in the other direction, and build aseparate discriminator for each part of speech, and0.40.350.30.250.20.150.i0.050.
, ,  .
.
,  .
.
,Train~ .
~ Test ....." ColiinS & Brooks ....... .
, J  , , i .
.
, i  .
~ .i0 I00 i000 I0000Number of roundsFigure 2: Training and test error as afunction of the number of rounds ofboosting for the PP-attachment problem.combine them by choosing the part of speech whosediscriminator predicts 'Yes' with the most confi-dence (or 'No' with the least confidence).
We tookthis approach--algorithm AdaBoost.MI--in exper-iment 4.
To choose the appropriate number ofrounds for each discriminator, we did an initial run,and chose the point at which error on the devel-opment sample flattened out.
To handle unknownwords, we used the same unknown-word classifieras in experiment 3.The result was the best for any of our experi-ments: a test error rate of 3.28%, slightly better thanexperiment 2.
The 3.28% error rate is not signifi-cantly different (at p = 0.05) from the error rate ofthe best-known single tagger, Ratnaparkhi's Maxenttagger, which achieves 3.11% error on our data.Our results are not as good as those achieved byBrill and Wu's voting scheme.
The experiments wedescribe here use very simple features, like thoseused in the Maxent or transformation-based taggers;hence the results are not comparable to the multiple-tagger voting scheme.
We are optimistic that boost-ing would do well with tagger predictions as inputfeatures, but those experiments remain to be done.Table 2 breaks out the error sources for experi-ment 4.
Table 3 sums up the results of all four ex-periments.Experiment 5 (Sequential model).
To this point,tagging decisions are made based on local contextonly.
One would expect performance to improve ifwe consider a Viterbi-style optimization to choosea globally best sequence of labels.
Using decisionsequences also permits one to use true tags, rather42than most-frequent tags, on context tokens.
Wedid a fixed 500 rounds of boosting, testing againstthe development sample.
Surprisingly, the sequen-tial model performed much less well than the local-decision models.
The results are summarized in Ta-ble 4.4 Prepositional phrase attachmentIn this section, we discuss the use of boosting forprepositional phrase (PP) attachment.
The casesof PP-attachment that we address define a binaryclassification problem.
For example, the sentenceCongress accused the president of peccadillos isclassified according to the attachment site of theprepositional phrase:attachment toN:accused \[the president of peccadillos\]attachment to V: (4)accused \[the president\] \[of peccadillos\]The UPenn Treebank-II Parsed Wall Street Jour-nal corpus includes PP-attachment information, andPP-attachment classifiers based on this data havebeen previously described in Ratnaparkhi, Reynar,Roukos (1994), Brill and Resnik (1994), and Collinsand Brooks (1995).
We consider how to applyboosting to this classification task.We used the same training and test data as Collinsand Brooks (1995).
The instances of PP-attachmentconsidered are those involving a verb immediatelyfollowed by a simple noun phrase (the direct ob-ject) and a prepositional phrase (whose attachmentis at issue).
Each PP-attachment example is repre-sented by its value for four attributes: the main verb(V), the head word of the direct object (N1), thepreposition (P), and the head word of the objectof the preposition (N2).
For instance, in example4 above, V= accused, N1 = president, P = ofand N2 = peccadillos.
Examples have binary la-bels: positive represents attachment to noun, andnegative represents attachment to verb.
The train-ing set comprises 20,801 examples and the test setcontains 3,097 examples; there is also a separatedevelopment set of 4,039 examples.The weak hypotheses we used correspond to "at-tribute=value" predicates and conjunctions thereof.That is, there are 16 predicates that are consid-ered for each example.
For example 4, three ofthese 16 predicates are (V = accused A N1 =president A N2 = peccadillos), (P = with), and(V = accused A p = oJ).
As described in section2.1, a weak hypothesis produces one of two real-valued predictions P0, Pl, depending on the value oferrors percentLocal decisions, LT/RT = most-frequent tagLocal decisions, LT/RT = true tagSequential decisions1489/52,087 3.181418/52,087 3.042083/52,087 4.00Table 4: Performance of the sequential model on the development sample.RoundTable 5: The first five weakTest(P = of)(P = to)(N2 = NUMBER)(N1 = it)(P = at)Prediction+2.393-0.729-0.772-2.273-0.669hypotheses chosen for the PP-attachment classifier.its predicate.
We found that little information wasconveyed by knowing that a predicate is false.
Wetherefore forced each weak hypothesis to abstain ifits predicate is not satisfied--that is, we set P0 to 0for all weak hypotheses.Two free parameters in boosting are the num-ber of rounds T and the smoothing parameter e forthe confidence values (see Eq.
(3)).
Although thereare theoretical analyses of the number of roundsneeded for boosting (Freund and Schapire, 1997;Schapire et al, 1997) and for smoothing (Schapireand Singer, 1998), these tend not to give practicalanswers.
We therefore used the development sam-ple to set these parameters, and chose T = 20,000and c = 0.001.On each round of boosting, we consider everypredicate relevant o any example, and choose theone that minimizes Z as given by Eq.
(2).
In Ta-ble 5 we list the weak hypotheses chosen on the firstfive rounds of boosting, together with their assignedconfidence Pl.
Recall that a positive value meansthat noun attachment is predicted.
Note that all theweak hypotheses chosen on the first rounds test thevalue of a single attribute: boosting starts with gen-eral tendencies and moves toward less widely ap-plicable but higher-precision tests as it proceeds.In 20,000 rounds of boosting, single-attribute stswere chosen 4,615 times, two-attribute tests werechosen 4,146 ,times, three-attribute sts were cho-sen 2,779 times, and four-attribute t sts were cho-sen 8,460 times.
It is possible for the same predi-cate to be chosen in multiple rounds; in fact, pred-icates were chosen about twice on average.
The fi-nal hypothesis considers 9,677 distinct predicates.We can define the total weight of a predicate to bethe sum of Pl'S over the rounds in which it is cho-sen; this represents how big a vote the predicate hason examples it applies to.
We expect more-specifichypotheses to have more weight--otherwise theywould not be able to overrule more-general hy-potheses, and there would be no point in havingthem.
This is confirmed by examining the predi-cates with the greatest weight (in absolute value) af-ter 20,000 rounds of boosting, as shown in Table 6.After 20,000 rounds of boosting the test errorwas down to 14.6 ?
0.6%.
This is indistinguish-able from the best known results for this problem,namely, 14.5?0.6%, reported by Collins and Brookon exactly the same data.
In Figure 2, we show thetraining and test error as a function of the numberof rounds of boosting.
The boosted classifier hasthe advantage of being much more compact than thelarge decision list built by Collins and Brooks usinga back-off method.
We also did not take into ac-count the linguistic knowledge used by Collins andBrooks who, for instance, disallowed tests that ig-nore the preposition.Compared to maximum entropy methods (Ratna-parkhi et al, 1994), although the methods hare asimilar structure, the boosted classifier achieves anerror rate which is significantly lower.5 Using boosting to improve data qualityThe importance weights that boosting assigns totraining examples are very useful for improving dataquality.
Mislabelled examples resulting from anno-tator errors tend to be hard examples to classify cor-rectly; hence they tend to have large weight in the43prev word tagged word next word(V  = was, N1 = decision, P ~- of, N2 = People)( V = put, N1 = them, P = on, N2 = streets)(V  = making, N1 = it, t 9 = in, N2 = terms)(V  = prompted, N1 = speculation, 19 = in, N2 = market)(V  = is, N1 = director, 19 =- at, N2 = Bank)Prediction+25.41-23 .08-22 .89+25.76+23.83Table 6: The five weak hypotheses with the highest (absolute) weight after 20,000 rounds.beBigonlyatofsome" Towith the" the- andfor mostWe have- and- -  aby A<P> But- andI weren't makehave thoughtwill havethe firstbe involvedA 'sincluding asHalfI werein both, saidto oneto one" theto long-termhave calledhave calledwith thewas hishave thought3O %of havenew'Sinwhatoutthebycorpus labelNNJJNNJJJJVBNcorrect labelTODTDTCCJJSVBPtobiginmuchtheoutgoldto'S'SonlyJJ CCIN DTNNP DTIN CCNN CCVB VBPVBP VBVBD VBNTestVBP VBRB JJJJ VBNNNP POSJJ RBDT PDTVB VBPCC (DT)VBNNN PRPNN PRPNN PRPforforBigbeforebymoreandNNVBDVBDJJPRPVBDJJJJ(RB)VBNVBNDTPRP$VBNNNTable 7: Training examples from experiment 4 with greatest weight.final distribution DT+i  (i).
I f  we rank examples bytheir weight in the final distribution, mislabelled ex-amples tend to cluster near the top.Table 7 shows the training examples with thegreatest weight in tagging experiment 4.
All buttwo represent annotator errors, and one of the two44non-errors is a highly unusual construction ("a lotof have and have-not markets").
Table 8 similarlyillustrates the highest-weight examples from the PP-attachment data.
Many of these are errors, thoughothers are genuinely difficult to judge.v N~ Prose NUMBER todropped NUMBER toadded NUMBER togained NUMBER togained NUMBER tojumped NUMBER toreported earnings ofhad sales oflost NUMBER tolost NUMBER tolost NUMBER toearned million onoutnumbered NUMBER tohad change inhad change inposted drop inyielding PERCENT toposted loss forraise billion inis reporter inyield PERCENT inyield PERCENT inhave impact onposted drop inregistered NUMBER onauction million infollowing decline inreported earnings forsigned agreement withhave impact onreport earnings forfell NUMBER tobuy stake inreport loss formake ,payments ontook charge inis writer inearned million onearned million onreached agreement inreached agreement instarted venture withresolve disputes withbecome shareholder inreach agreement withTable 8: High-weight examplesattachment data.
The last columnthat appears in the corpus.N2NUMBER NNUMBER NNUMBER NNUMBER NNUMBER NNUMBER Nmillion Vmillion VNUMBER NNUMBER NNUMBER Nrevenue NNUMBER Vearnings Vearnings Vprofit Vassumption Nquarter Vcash Vbureau" VNUMBER NNUMBER Nmarket Vearnings Vscale Nmaturity VAugust Vquarter VInc.
Vresults Nquarter Npoint NAirlines Vquarter Ndebt Vquarter NYork Vsales Nsales Nprinciple Vprinciple VCo.
Ncompany Vbank Vregulators Vfrom the PP-gives the labelEric Brill and Jun Wu.
1998.
Classifier combination forimproved lexical disambiguation.
In Proceedings ofCOLING-A CL.Eric Brill.
1993.
Transformation-Based Learning.Ph.D.
thesis, Univ.
of Pennsylvania.Michael Collins and James Brooks.
1995.
Prepositionalphrase attachment through a backed-off model.
InProceedings of the Third Workshop on Very LargeCorpora.Yoav Freund and Robert E. Schapire.
1997.
A decision-theoretic generalization f on-line learning and an ap-plication to boosting.
Journal of Computer and Sys-tem Sciences, 55(1): 119-139, August.David Magerman.
1995.
Statistical decision-tree modelsfor parsing.
In Proc.
ACL-95.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313-330.A.
Ratnaparkhi, J. Renyar, and S. Roukos.
1994.
A max-imum entropy model for prepostional phrase attache-ment.
In Proceedings of the ARPA Workshop on Hu-man Language Technology.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the EmpiricalMethods in Natural Language Processing Conference.Robert E. Schapire and Yoram Singer.
1998.
Improvedboosting algorithms using confidence-rated predic-tions.
In Proceedings of the Eleventh Annual Confer-ence on Computational Learning Theory, pages 80-91.Robert E. Schapire, Yoav Freund, Peter Bartlett, andWee Sun Lee.
1997.
Boosting the margin: A newexplanation for the effectiveness of voting methods.In Machine Learning: Proceedings of the FourteenthInternational Conference.Robert E. Schapire.
1997.
Using output codes to boostmulticlass learning problems.
In Machine Learning:Proceedings of the Fourteenth International Confer-ence.Ralph Weischedel et al 1993.
Coping with ambigu-ity and unknown words through probabilistic models.Computational Linguistics, 19(2):359-382.ReferencesE.
Brill and E Resnik.
1994.
A rule-baed appraoch toprepositional phrase attachment disambiguation.
InProceedings of the fifteenth international conferenceon computational linguistics (COLING).45
