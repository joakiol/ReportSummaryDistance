Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 26?37,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsExact Decoding of Phrase-Based Translation Modelsthrough Lagrangian RelaxationYin-Wen ChangMIT CSAILCambridge, MA 02139, USAyinwen@csail.mit.eduMichael CollinsDepartment of Computer Science,Columbia University,New York, NY 10027, USAmcollins@cs.columbia.eduAbstractThis paper describes an algorithm for exactdecoding of phrase-based translation models,based on Lagrangian relaxation.
The methodrecovers exact solutions, with certificates ofoptimality, on over 99% of test examples.The method is much more efficient than ap-proaches based on linear programming (LP)or integer linear programming (ILP) solvers:these methods are not feasible for anythingother than short sentences.
We compare ourmethod to MOSES (Koehn et al, 2007), andgive precise estimates of the number and mag-nitude of search errors that MOSES makes.1 IntroductionPhrase-based models (Och et al, 1999; Koehn etal., 2003; Koehn et al, 2007) are a widely-usedapproach for statistical machine translation.
Thedecoding problem for phrase-based models is NP-hard1; because of this, previous work has generallyfocused on approximate search methods, for exam-ple variants of beam search, for decoding.This paper describes an algorithm for exactdecoding of phrase-based models, based on La-grangian relaxation (Lemare?chal, 2001).
The coreof the algorithm is a dynamic program for phrase-based translation which is efficient, but which allowssome ill-formed translations.
More specifically, thedynamic program searches over the space of transla-tions where exactly N words are translated (N isthe number of words in the source-language sen-tence), but where some source-language words maybe translated zero times, or some source-languagewords may be translated more than once.
La-grangian relaxation is used to enforce the constraint1We refer here to the phrase-based models of (Koehn et al,2003; Koehn et al, 2007), considered in this paper.
Other vari-ants of phrase-based models, which allow polynomial time de-coding, have been proposed, see the related work section.that each source-language word should be translatedexactly once.
A subgradient algorithm is used to op-timize the dual problem arising from the relaxation.The first technical contribution of this paper is thebasic Lagrangian relaxation algorithm.
By the usualguarantees for Lagrangian relaxation, if this algo-rithm converges to a solution where all constraintsare satisfied (i.e., where each word is translated ex-actly once), then the solution is guaranteed to beoptimal.
For some source-language sentences how-ever, the underlying relaxation is loose, and the algo-rithm will not converge.
The second technical con-tribution of this paper is a method that incrementallyadds constraints to the underlying dynamic program,thereby tightening the relaxation until an exact solu-tion is recovered.We describe experiments on translation from Ger-man to English, using phrase-based models trainedby MOSES (Koehn et al, 2007).
The methodrecovers exact solutions, with certificates of opti-mality, on over 99% of test examples.
On over78% of examples, the method converges with zeroadded constraints (i.e., using the basic algorithm);99.67% of all examples converge with 9 or fewerconstraints.
We compare to a linear programming(LP)/integer linear programming (ILP) based de-coder.
Our method is much more efficient: LP orILP decoding is not feasible for anything other thanshort sentences,2 whereas the average decoding timefor our method (for sentences of length 1-50 words)is 121 seconds per sentence.
We also compare ourmethod to MOSES, and give precise estimates of thenumber and magnitude of search errors that MOSESmakes.
Even with large beam sizes, MOSES makesa significant number of search errors.
As far as weare aware, previous work has not successfully re-2For example ILP decoding for sentences of lengths 11-15words takes on average 2707.8 seconds.26covered exact solutions for the type of phrase-basedmodels used in MOSES.2 Related WorkLagrangian relaxation is a classical technique forsolving combinatorial optimization problems (Korteand Vygen, 2008; Lemare?chal, 2001).
Dual decom-position, a special case of Lagrangian relaxation, hasbeen applied to inference problems in NLP (Koo etal., 2010; Rush et al, 2010), and also to Markov ran-dom fields (Wainwright et al, 2005; Komodakis etal., 2007; Sontag et al, 2008).
Earlier work on be-lief propagation (Smith and Eisner, 2008) is closelyrelated to dual decomposition.
Recently, Rush andCollins (2011) describe a Lagrangian relaxation al-gorithm for decoding for syntactic translation; thealgorithmic construction described in the current pa-per is, however, very different in nature to this work.Beam search stack decoders (Koehn et al, 2003)are the most commonly used decoding algorithmfor phrase-based models.
Dynamic-programming-based beam search algorithms are discussed for bothword-based and phrase-based models by Tillmannand Ney (2003) and Tillmann (2006).Several works attempt exact decoding, but effi-ciency remains an issue.
Exact decoding via integerlinear programming (ILP) for IBM model 4 (Brownet al, 1993) has been studied by Germann et al(2001), with experiments using a bigram languagemodel for sentences up to eight words in length.Riedel and Clarke (2009) have improved the effi-ciency of this work by using a cutting-plane algo-rithm, and experimented with sentence lengths upto 30 words (again with a bigram LM).
Zaslavskiyet al (2009) formulate the phrase-based decodingproblem as a traveling salesman problem (TSP), andtake advantage of existing exact and approximateapproaches designed for TSP.
Their translation ex-periment uses a bigram language model and appliesan approximate algorithm for TSP.
Och et al (2001)propose an A* search algorithm for IBM model 4,and test on sentence lengths up to 14 words.
Otherwork (Kumar and Byrne, 2005; Blackwood et al,2009) has considered variants of phrase-based mod-els with restrictions on reordering that allow exact,polynomial time decoding, using finite-state trans-ducers.The idea of incrementally adding constraints totighten a relaxation until it is exact is a core idea incombinatorial optimization.
Previous work on thistopic in NLP or machine learning includes work oninference in Markov random fields (Sontag et al,2008); work that encodes constraints using finite-state machines (Tromble and Eisner, 2006); andwork on non-projective dependency parsing (Riedeland Clarke, 2006).3 The Phrase-based Translation ModelThis section establishes notation for phrase-basedtranslation models, and gives a definition of the de-coding problem.
The phrase-based model we use isthe same as that described by Koehn et al (2003), asimplemented in MOSES (Koehn et al, 2007).The input to a phrase-based translation sys-tem is a source-language sentence with N words,x1x2 .
.
.
xN .
A phrase table is used to define theset of possible phrases for the sentence: each phraseis a tuple p = (s, t, e), where (s, t) are indices rep-resenting a contiguous span in the source-languagesentence (we have s ?
t), and e is a target-languagestring consisting of a sequence of target-languagewords.
For example, the phrase p = (2, 5, the dog)would specify that words x2 .
.
.
x5 have a translationin the phrase table as ?the dog?.
Each phrase p hasa score g(p) = g(s, t, e): this score will typicallybe calculated as a log-linear combination of features(e.g., see Koehn et al (2003)).We use s(p), t(p) and e(p) to refer to the threecomponents (s, t, e) of a phrase p.The output from a phrase-based model is asequence of phrases y = ?p1p2 .
.
.
pL?.
Wewill often refer to an output y as a derivation.The derivation y defines a target-language transla-tion e(y), which is formed by concatenating thestrings e(p1), e(p2), .
.
.
, e(pL).
For two consecutivephrases pk = (s, t, e) and pk+1 = (s?, t?, e?
), the dis-tortion distance is defined as ?
(t, s?)
= |t+ 1?
s?|.The score for a translation is then defined asf(y) = h(e(y))+L?k=1g(pk)+L?1?k=1???
(t(pk), s(pk+1))where ?
?
R is often referred to as the distortionpenalty, and typically takes a negative value.
Thefunction h(e(y)) is the score of the string e(y) under27a language model.3The decoding problem is to findargmaxy?Yf(y)where Y is the set of valid derivations.
The set Y canbe defined as follows.
First, for any derivation y =?p1p2 .
.
.
pL?, define y(i) to be the number of timesthat the source-language word xi has been translatedin y: that is, y(i) = ?Lk=1[[s(pk) ?
i ?
t(pk)]],where [[pi]] = 1 if pi is true, and 0 otherwise.
ThenY is defined as the set of finite length sequences?p1p2 .
.
.
pL?
such that:1.
Each word in the input is translated exactlyonce: that is, y(i) = 1 for i = 1 .
.
.
N .2.
For each pair of consecutive phrasespk, pk+1 for k = 1 .
.
.
L ?
1, we have?
(t(pk), s(pk+1)) ?
d, where d is thedistortion limit.An exact dynamic programming algorithm forthis problem uses states (w1, w2, b, r), where(w1, w2) is a target-language bigram that the par-tial translation ended with, b is a bit-string denotingwhich source-language words have been translated,and r is the end position of the previous phrase (e.g.,see Koehn et al (2003)).
The bigram (w1, w2) isneeded for calculation of trigram language modelscores; r is needed to enforce the distortion limit,and to calculate distortion costs.
The bit-string bis needed to ensure that each word is translated ex-actly once.
Since the number of possible bit-stringsis exponential in the length of sentence, exhaustivedynamic programming is in general intractable.
In-stead, people commonly use heuristic search meth-ods such as beam search for decoding.
However,these methods have no guarantee of returning thehighest scoring translation.4 A Decoding Algorithm based onLagrangian RelaxationWe now describe a decoding algorithm for phrase-based translation, based on Lagrangian relaxation.3The language model score usually includes a word inser-tion score that controls the length of translations.
The relativeweights of the g(p) and h(e(y)) terms, and the value for ?, aretypically chosen using MERT training (Och, 2003).We first describe a dynamic program for decodingwhich is efficient, but which relaxes the y(i) = 1constraints described in the previous section.
Wethen describe the Lagrangian relaxation algorithm,which introduces Lagrange multipliers for each con-straint of the form y(i) = 1, and uses a subgradientalgorithm to minimize the dual arising from the re-laxation.
We conclude with theorems describing for-mal properties of the algorithm, and with an examplerun of the algorithm.4.1 An Efficient Dynamic ProgramAs described in the previous section, our goal is tofind the optimal translation y?
= argmaxy?Y f(y).We will approach this problem by defining a set Y ?such that Y ?
Y ?, and such thatargmaxy?Y ?f(y)can be found efficiently using dynamic program-ming.
The set Y ?
omits some constraints?specifically, the constraints that each source-language word is translated once, i.e., that y(i) = 1for i = 1 .
.
.
N?that are enforced for membersof Y .
In the next section we describe how to re-introduce these constraints using Lagrangian relax-ation.
The set Y ?
does, however, include a looserconstraint, namely that ?Ni=1 y(i) = N , which re-quires that exactly N words are translated.We now give the dynamic program that definesY ?.
The main idea will be to replace bit-strings (asdescribed in the previous section) by a much smallernumber of dynamic programming states.
Specifi-cally, the states of the new dynamic program willbe tuples (w1, w2, n, l,m, r).
The pair (w1, w2) isagain a target-language bigram corresponding to thelast two words in the partial translation, and the inte-ger r is again the end position of the previous phrase.The integer n is the number of words that have beentranslated thus far in the dynamic programming al-gorithm.
The integers l and m specify a contiguousspan xl .
.
.
xm in the source-language sentence; thisspan is the last contiguous span of words that havebeen translated thus far.The dynamic program can be viewed as ashortest-path problem in a directed graph, withnodes in the graph corresponding to states(w1, w2, n, l,m, r).
The transitions in the28graph are defined as follows.
For each state(w1, w2, n, l,m, r), we consider any phrasep = (s, t, e) with e = (e0 .
.
.
eM?1eM ) such that:1) ?
(r, s) ?
d; and 2) t < l or s > m. The formercondition states that the phrase should satisfy thedistortion limit.
The latter condition requires thatthere is no overlap of the new phrase?s span (s, t)with the span (l,m).
For any such phrase, we createa transition(w1, w2, n, l,m, r)p=(s,t,e)??????
(w?1, w?2, n?, l?,m?, r?)where?
(w?1, w?2) ={(eM?1, eM ) if M ?
2(w2, e1) if M = 1?
n?
= n+ t?
s+ 1?
(l?,m?)
=???
(l, t ) if s = m+ 1(s,m) if t = l ?
1(s, t ) otherwise?
r?
= tThe new target-language bigram (w?1, w?2) is the lasttwo words of the partial translation after includingphrase p. It comes from either the last two wordsof e, or, if e consists of a single word, the last wordof the previous bigram, w2, and the first and onlyword, e1, in e.
(l?,m?)
is expanded from (l,m) ifthe spans (l,m) and (s, t) are adjacent.
Otherwise,(l?,m?)
will be the same as (s, t).The score of the transition is given by a sumof the phrase translation score g(p), the languagemodel score, and the distortion cost ??
?
(r, s).
Thetrigram language model score is h(e1|w1, w2) +h(e2|w2, e1) +?M?2i=1 h(ei+2|ei, ei+1), whereh(w3|w1, w2) is a trigram score (typically a logprobability plus a word insertion score).We also include start and end states in the directedgraph.
The start state is (<s>,<s>, 0, 0, 0, 0) where<s> is the start symbol in the language model.
Foreach state (w1, w2, n, l,m, r), such that n = N , wecreate a transition to the end state.
This transitiontakes the form(w1, w2, N, l,m, r)(N,N+1,</s>)????????????
ENDFor this transition, we define the score as score =h(</s>|w1, w2); thus this transition incorporatesthe end symbol </s> in the language model.The states and transitions we have described forma directed graph, where each path from the start stateto the end state corresponds to a sequence of phrasesp1p2 .
.
.
pL.
We define Y ?
to be the full set of suchsequences.
We can use the Viterbi algorithm to solveargmaxy?Y ?
f(y) by simply searching for the high-est scoring path from the start state to the end state.The set Y ?
clearly includes derivations that are ill-formed, in that they may include words that havebeen translated 0 times, or more than 1 time.
Thefirst line of Figure 2 shows one such derivation (cor-responding to the translation the quality and also theand the quality and also .).
For each phrase we showthe English string (e.g., the quality) together with thespan of the phrase (e.g., 3, 6).
The values for y(i) arealso shown.
It can be verified that this derivation is avalid member of Y ?.
However, y(i) 6= 1 for severalvalues of i: for example, words 1 and 2 are trans-lated 0 times, while word 3 is translated twice.Other dynamic programs, and definitions of Y ?,are possible: for example an alternative would beto use a dynamic program with states (w1, w2, n, r).However, including the previous contiguous span(l,m) makes the set Y ?
a closer approximation toY .
In experiments we have found that including theprevious span (l,m) in the dynamic program leadsto faster convergence of the subgradient algorithmdescribed in the next section, and in general to morestable results.
This is in spite of the dynamic pro-gram being larger; it is no doubt due to Y ?
being abetter approximation of Y .4.2 The Lagrangian Relaxation AlgorithmWe now describe the Lagrangian relaxation decod-ing algorithm for the phrase-based model.
Recallthat in the previous section, we defined a set Y ?
thatallowed efficient dynamic programming, and suchthat Y ?
Y ?.
It is easy to see that Y = {y : y ?Y ?, and ?i, y(i) = 1}.
The original decodingproblem can therefore be stated as:argmaxy?Y ?f(y) such that ?i, y(i) = 1We use Lagrangian relaxation (Korte and Vygen,2008) to deal with the y(i) = 1 constraints.
Weintroduce Lagrange multipliers u(i) for each suchconstraint.
The Lagrange multipliers u(i) can takeany positive or negative value.
The Lagrangian isL(u, y) = f(y) +?iu(i)(y(i)?
1)29Initialization: u0(i)?
0 for i = 1 .
.
.
Nfor t = 1 .
.
.
Tyt = argmaxy?Y?
L(ut?1, y)if yt(i) = 1 for i = 1 .
.
.
Nreturn ytelsefor i = 1 .
.
.
Nut(i) = ut?1(i)?
?t (yt(i)?
1)Figure 1: The decoding algorithm.
?t > 0 is the step sizeat the t?th iteration.The dual objective is thenL(u) = maxy?Y ?L(u, y).and the dual problem is to solveminuL(u).The next section gives a number of formal results de-scribing how solving the dual problem will be usefulin solving the original optimization problem.We now describe an algorithm that solves the dualproblem.
By standard results for Lagrangian re-laxation (Korte and Vygen, 2008), L(u) is a con-vex function; it can be minimized by a subgradientmethod.
If we defineyu = argmaxy?Y ?
L(u, y)and ?u(i) = yu(i) ?
1 for i = 1 .
.
.
N , then ?u isa subgradient of L(u) at u.
A subgradient methodis an iterative method for minimizing L(u), whichperfoms updates ut ?
ut?1?
?t?ut?1 where ?t > 0is the step size for the t?th subgradient step.Figure 1 depicts the resulting algorithm.
At eachiteration, we solveargmaxy?Y ?
(f(y) +?iu(i)(y(i)?
1))=argmaxy?Y ?
(f(y) +?iu(i)y(i))by the dynamic program described in the previoussection.
Incorporating the ?i u(i)y(i) terms in thedynamic program is straightforward: we simply re-define the phrase scores asg?
(s, t, e) = g(s, t, e) +t?i=su(i)Intuitively, each Lagrange multiplier u(i) penal-izes or rewards phrases that translate word i; the al-gorithm attempts to adjust the Lagrange multipliersin such a way that each word is translated exactlyonce.
The updates ut(i) = ut?1(i) ?
?t(yt(i) ?
1)will decrease the value for u(i) if yt(i) > 1, in-crease the value for u(i) if yt(i) = 0, and leave u(i)unchanged if yt(i) = 1.4.3 PropertiesWe now give some theorems stating formal proper-ties of the Lagrangian relaxation algorithm.
Theseresults for Lagrangian relaxation are well known:for completeness, we state them here.
First, definey?
to be the optimal solution for our original prob-lem:Definition 1. y?
= argmaxy?Y f(y)Our first theorem states that the dual function pro-vides an upper bound on the score for the optimaltranslation, f(y?
):Theorem 1.
For any value of u ?
RN , L(u) ?f(y?
).Proof.L(u) = maxy?Y ?f(y) +?iu(i)(y(i)?
1)?
maxy?Yf(y) +?iu(i)(y(i)?
1)= maxy?Yf(y)The first inequality follows because Y ?
Y ?.
Thefinal equality is true since any y ?
Y has y(i) =1 for all i, implying that?i u(i)(y(i)?1) = 0.The second theorem states that under an appropri-ate choice of the step sizes ?t, the method convergesto the minimum ofL(u).
Hence we will successfullyfind the tightest possible upper bound defined by thedual L(u).Theorem 2.
For any sequence ?1, ?2, .
.
.
If 1)limt??
?t ?
0; 2) ?
?t=1 ?t = ?, thenlimt??
L(ut) = minu L(u)Proof.
See Korte and Vygen (2008).30Input German: dadurch ko?nnen die qualita?t und die regelma?
?ige postzustellung auch weiterhin sichergestellt werden .t L(ut?1) yt(i) derivation yt1 -10.0988 0 0 2 2 3 3 0 0 2 0 0 0 1???
?3, 6the quality and???
?9, 9also???
?6, 6the???
?5, 5and???
?3, 3the???
?4, 6quality and???
?9, 9also???
?13, 13.???
?2 -11.1597 0 0 1 0 0 0 1 0 0 4 1 5 1???
?3, 3the???
?7, 7regular???
?12, 12will???
?10, 10continue to???
?12, 12be???
?10, 10continue to???
?12, 12be???
?10, 10continue to???
?12, 12be???
?10, 10continue to???
?11, 13be guaranteed .???
?3 -12.3742 3 3 1 2 2 0 0 0 1 0 0 0 1???
?1, 2in that way ,???
?5, 5and???
?2, 2can???
?1, 1thus???
?4, 4quality???
?1, 2in that way ,???
?3, 5the quality and???
?9, 9also???
?13, 13.???
?4 -11.8623 0 1 0 0 0 1 1 3 3 0 3 0 1???
?2, 2can???
?6, 7the regular???
?8, 8distribution should???
?9, 9also???
?11, 11ensure???
?8, 8distribution should???
?9, 9also???
?11, 11ensure???
?8, 8distribution should???
?9, 9also???
?11, 11ensure???
?13, 13.???
?5 -13.9916 0 0 1 1 3 2 4 0 0 0 1 0 1???
?3, 3the???
?7, 7regular???
?5, 5and???
?7, 7regular???
?5, 5and???
?7, 7regular???
?6, 6the???
?4, 4quality???
?5, 7and the regular???
?11, 11ensured???
?13, 13.???
?6 -15.6558 1 1 1 2 0 2 0 1 1 1 1 1 1???
?1, 2in that way ,???
?3, 4the quality of???
?6, 6the???
?4, 4quality of???
?6, 6the???
?8, 8distribution should???
?9, 10continue to???
?11, 13be guaranteed .???
?7 -16.1022 1 1 1 1 1 1 1 1 1 1 1 1 1???
?1, 2in that way ,???
?3, 4the quality???
?5, 7and the regular???
?8, 8distribution should???
?9, 10continue to???
?11, 13be guaranteed .???
?Figure 2: An example run of the algorithm in Figure 1.
For each value of t we show the dual value L(ut?1), thederivation yt, and the number of times each word is translated, yt(i) for i = 1 .
.
.
N .
For each phrase in a derivationwe show the English string e, together with the span (s, t): for example, the first phrase in the first derivation hasEnglish string the quality and, and span (3, 6).
At iteration 7 we have yt(i) = 1 for i = 1 .
.
.
N , and the translation isreturned, with a guarantee that it is optimal.Our final theorem states that if at any iteration thealgorithm finds a solution yt such that yt(i) = 1 fori = 1 .
.
.
N , then this is guaranteed to be the optimalsolution to our original problem.
First, defineDefinition 2. yu = argmaxy?Y ?
L(u, y).We then have the theoremTheorem 3.
If ?
u, s.t.
yu(i) = 1 for i = 1 .
.
.
N ,then f(yu) = f(y?
), i.e.
yu is optimal.Proof.
We haveL(u) = maxy?Y ?f(y) +?iu(i)(y(i)?
1)= f(yu) +?iu(i)(yu(i)?
1)= f(yu)The second equality is true because of the defini-tion of yu.
The third equality follows because byassumption yu(i) = 1 for i = 1 .
.
.
N .
BecauseL(u) = f(yu) and L(u) ?
f(y?)
for all u, we havef(yu) ?
f(y?).
But y?
= argmaxy?Y f(y), andyu ?
Y , hence we must also have f(yu) ?
f(y?).
Itfollows that f(yu) = f(y?
).In some cases, however, the algorithm in Figure 1may not return a solution yt such that yt(i) = 1for all i.
There could be two reasons for this.
Inthe first case, we may not have run the algorithmfor enough iterations T to see convergence.
In thesecond case, the underlying relaxation may not betight, in that there may not be any settings u for theLagrange multipliers such that yu(i) = 1 for all i.Section 5 describes a method for tighteningthe underlying relaxation by introducing hard con-straints (of the form y(i) = 1 for selected values ofi).
We will see that this method is highly effectivein tightening the relaxation until the algorithm con-verges to an optimal solution.4.4 An Example of the AlgorithmFigure 2 shows an example of how the algorithmworks when translating a German sentence into anEnglish sentence.
After the first iteration, there arewords that have been translated two or three times,and words that have not been translated.
At eachiteration, the Lagrangian multipliers are updated toencourage each word to be translated once.
Onthis example, the algorithm converges to a solutionwhere all words are translated exactly once, and thesolution is guaranteed to be optimal.5 Tightening the RelaxationIn some cases the algorithm in Figure 1 will notconverge to y(i) = 1 for i = 1 .
.
.
N becausethe underlying relaxation is not tight.
We now de-scribe a method that incrementally tightens the La-grangian relaxation algorithm until it provides an ex-act answer.
In cases that do not converge, we in-troduce hard constraints to force certain words to betranslated exactly once in the dynamic programmingsolver.
In experiments we show that typically only a31Optimize(C, u)while (dual value still improving)y?
= argmaxy?Y?C L(u, y)if y?
(i) = 1 for i = 1 .
.
.
N return y?else for i = 1 .
.
.
Nu(i) = u(i)?
?
(y?(i)?
1)count(i) = 0 for i = 1 .
.
.
Nfor k = 1 .
.
.Ky?
= argmaxy?Y?C L(u, y)if y?
(i) = 1 for i = 1 .
.
.
N return y?else for i = 1 .
.
.
Nu(i) = u(i)?
?
(y?(i)?
1)count(i) = count(i) + [[y?
(i) 6= 1]]Let C?
= set of G i?s that have the largest value forcount(i), that are not in C, and that are not adjacent toeach otherreturn Optimize(C ?
C?, u)Figure 3: A decoding algorithm with incremental addi-tion of constraints.
The function Optimize(C, u) is a re-cursive function, which takes as input a set of constraintsC, and a vector of Lagrange multipliers, u.
The initialcall to the algorithm is with C = ?, and u = 0. ?
> 0 isthe step size.
In our experiments, the step size decreaseseach time the dual value increases from one iteration tothe next; see Appendix A.few constraints are necessary.Given a set C ?
{1, 2, .
.
.
, N}, we defineY ?C = {y : y ?
Y ?, and ?
i ?
C, y(i) = 1}Thus Y ?C is a subset of Y ?, formed by adding hardconstraints of the form y(i) = 1 to Y ?.
Note that Y ?Cremains as a superset of Y , which enforces y(i) =1 for all i.
Finding argmaxy?Y ?C f(y) can againbe achieved using dynamic programming, with thenumber of dynamic programming states increasedby a factor of 2|C|: dynamic programming states ofthe form (w1, w2, n, l,m, r) are replaced by states(w1, w2, n, l,m, r, bC) where bC is a bit-string oflength |C|, which records which words in the set Chave or haven?t been translated in a hypothesis (par-tial derivation).
Note that if C = {1 .
.
.
N}, we haveY ?C = Y , and the dynamic program will correspondto exhaustive dynamic programming.We can again run a Lagrangian relaxation algo-rithm, using the set Y ?C in place of Y ?.
We will useLagrange multipliers u(i) to enforce the constraintsy(i) = 1 for i /?
C. Our goal will be to find asmall set of constraints C, such that Lagrangian re-laxation will successfully recover an optimal solu-tion.
We will do this by incrementally adding el-ements to C; that is, by incrementally adding con-straints that tighten the relaxation.The intuition behind our approach is as follows.Say we run the original algorithm, with the set Y ?,for several iterations, so that L(u) is close to con-vergence (i.e., L(u) is close to its minimal value).However, assume that we have not yet generated asolution yt such that yt(i) = 1 for all i.
In this casewe have some evidence that the relaxation may notbe tight, and that we need to add some constraints.The question is, which constraints to add?
To an-swer this question, we run the subgradient algorithmfor K more iterations (e.g., K = 10), and at each it-eration track which constraints of the form y(i) = 1are violated.
We then choose C to be the G con-straints (e.g., G = 3) that are violated most oftenduring the K additional iterations, and are not ad-jacent to each other.
We recursively call the algo-rithm, replacing Y ?
by Y ?C ; the recursive call maythen return an exact solution, or alternatively againadd more constraints and make a recursive call.4Figure 3 depicts the resulting algorithm.
We ini-tially make a call to the algorithm Optimize(C, u)with C equal to the empty set (i.e., no hard con-straints), and with u(i) = 0 for all i.
In an initialphase the algorithm runs subgradient steps, whilethe dual is still improving.
In a second step, if a so-lution has not been found, the algorithm runs for Kmore iterations, thereby choosing G additional con-straints, then recursing.If at any stage the algorithm finds a solution y?such that y?
(i) = 1 for all i, then this is the so-lution to our original problem, argmaxy?Y f(y).This follows because for any C ?
{1 .
.
.
N} wehave Y ?
Y ?C ; hence the theorems in section 4.3 gothrough for Y ?C in place of Y ?, with trivial modifica-tions.
Note also that the algorithm is guaranteed toeventually find the optimal solution, because even-tually C = {1 .
.
.
N}, and Y = Y ?C .4Formal justification for the method comes from the rela-tionship between Lagrangian relaxation and linear program-ming relaxations.
In cases where the relaxation is not tight,the subgradient method will essentially move between solu-tions whose convex combination form a fractional solution toan underlying LP relaxation (Nedic?
and Ozdaglar, 2009).
Ourmethod eliminates the fractional solution through the introduc-tion of hard constraints.32# iter.
1-10 words 11-20 words 21-30 words 31-40 words 41-50 words All sentences0-7 166 (89.7 %) 219 (39.2 %) 34 ( 6.0 %) 2 ( 0.6 %) 0 ( 0.0 %) 421 (23.1 %) 23.1 %8-15 17 ( 9.2 %) 187 (33.5 %) 161 (28.4 %) 30 ( 8.6 %) 3 ( 1.8 %) 398 (21.8 %) 44.9 %16-30 1 ( 0.5 %) 93 (16.7 %) 208 (36.7 %) 112 (32.3 %) 22 ( 13.1 %) 436 (23.9 %) 68.8 %31-60 1 ( 0.5 %) 52 ( 9.3 %) 105 (18.6 %) 99 (28.5 %) 62 ( 36.9 %) 319 (17.5 %) 86.3 %61-120 0 ( 0.0 %) 7 ( 1.3 %) 54 ( 9.5 %) 89 (25.6 %) 45 ( 26.8 %) 195 (10.7 %) 97.0 %121-250 0 ( 0.0 %) 0 ( 0.0 %) 4 ( 0.7 %) 14 ( 4.0 %) 31 ( 18.5 %) 49 ( 2.7 %) 99.7 %x 0 ( 0.0 %) 0 ( 0.0 %) 0 ( 0.0 %) 1 ( 0.3 %) 5 ( 3.0 %) 6 ( 0.3 %) 100.0 %Table 1: Table showing the number of iterations taken for the algorithm to converge.
x indicates sentences that fail toconverge after 250 iterations.
97% of the examples converge within 120 iterations.# cons.
1-10 words 11-20 words 21-30 words 31-40 words 41-50 words All sentences0-0 183 (98.9 %) 511 (91.6 %) 438 (77.4 %) 222 (64.0 %) 82 ( 48.8 %) 1,436 (78.7 %) 78.7 %1-3 2 ( 1.1 %) 45 ( 8.1 %) 94 (16.6 %) 87 (25.1 %) 50 ( 29.8 %) 278 (15.2 %) 94.0 %4-6 0 ( 0.0 %) 2 ( 0.4 %) 27 ( 4.8 %) 24 ( 6.9 %) 19 ( 11.3 %) 72 ( 3.9 %) 97.9 %7-9 0 ( 0.0 %) 0 ( 0.0 %) 7 ( 1.2 %) 13 ( 3.7 %) 12 ( 7.1 %) 32 ( 1.8 %) 99.7 %x 0 ( 0.0 %) 0 ( 0.0 %) 0 ( 0.0 %) 1 ( 0.3 %) 5 ( 3.0 %) 6 ( 0.3 %) 100.0 %Table 2: Table showing the number of constraints added before convergence of the algorithm in Figure 3, broken down by sentencelength.
Note that a maximum of 3 constraints are added at each recursive call, but that fewer than 3 constraints are added in caseswhere fewer than 3 constraints have count(i) > 0. x indicates the sentences that fail to converge after 250 iterations.
78.7% of theexamples converge without adding any constraints.The remaining question concerns the ?dual stillimproving?
condition; i.e., how to determine that thefirst phase of the algorithm should terminate.
We dothis by recording the first and second best dual val-ues L(u?)
and L(u??)
in the sequence of Lagrangemultipliers u1, u2, .
.
.
generated by the algorithm.Suppose that L(u??)
first occurs at iteration t??.
IfL(u?)?L(u??)t?t??
< , we say that the dual value does notdecrease enough.
The value for  is a parameter ofthe approach: in experiments we used  = 0.002.See the supplementary material for this submis-sion for an example run of the algorithm.When C 6= ?, A* search can be used for de-coding, with the dynamic program for Y ?
provid-ing admissible estimates for the dynamic programfor Y ?C .
Experiments show that A* gives significantimprovements in efficiency.
The supplementary ma-terial contains a full description of the A* algorithm.6 ExperimentsIn this section, we present experimental results todemonstrate the efficiency of the decoding algo-rithm.
We compare to MOSES (Koehn et al, 2007),a phrase-based decoder using beam search, and toa general purpose integer linear programming (ILP)solver, which solves the problem exactly.The experiments focus on translation from Ger-man to English, using the Europarl data (Koehn,2005).
We tested on 1,824 sentences of length atmost 50 words.
The experiments use the algorithmshown in Figure 3.
We limit the algorithm to a max-imum of 250 iterations and a maximum of 9 hardconstraints.
The distortion limit d is set to be four,and we prune the phrase translation table to have 10English phrases per German phrase.Our method finds exact solutions on 1,818 outof 1,824 sentences (99.67%).
(6 examples do notconverge within 250 iterations.)
Table 1 shows thenumber of iterations required for convergence, andTable 2 shows the number of constraints requiredfor convergence, broken down by sentence length.In 1,436/1,818 (78.7%) sentences, the method con-verges without adding hard constraints to tighten therelaxation.
For sentences with 1-10 words, the vastmajority (183 out of 185 examples) converge with0 constraints added.
As sentences get longer, moreconstraints are often required.
However most exam-ples converge with 9 or fewer constraints.Table 3 shows the average times for decoding,broken down by sentence length, and by the numberof constraints that are added.
As expected, decod-ing times increase as the length of sentences, andthe number of constraints required, increase.
Theaverage run time across all sentences is 120.9 sec-onds.
Table 3 also shows the run time of the methodwithout the A* algorithm for decoding.
The A* al-gorithm gives significant reductions in runtime.33# cons.
1-10 words 11-20 words 21-30 words 31-40 words 41-50 words All sentencesA* w/o A* w/o A* w/o A* w/o A* w/o A* w/o0-0 0.8 0.8 9.7 10.7 47.0 53.7 153.6 178.6 402.6 492.4 64.6 76.11-3 2.4 2.9 23.2 28.0 80.9 102.3 277.4 360.8 686.0 877.7 241.3 309.74-6 0.0 0.0 28.2 38.8 111.7 163.7 309.5 575.2 1,552.8 1,709.2 555.6 699.57-9 0.0 0.0 0.0 0.0 166.1 500.4 361.0 1,467.6 1,167.2 3,222.4 620.7 1,914.1mean 0.8 0.9 10.9 12.3 57.2 72.6 203.4 299.2 679.9 953.4 120.9 168.9median 0.7 0.7 8.9 9.9 48.3 54.6 169.7 202.6 484.0 606.5 35.2 40.0Table 3: The average time (in seconds) for decoding using the algorithm in Figure 3, with and without A* algorithm, broken downby sentence length and the number of constraints that are added.
A* indicates speeding up using A* search; w/o denotes withoutusing A*.method ILP LPset length mean median mean median % frac.Y ??
1-10 275.2 132.9 10.9 4.4 12.4 %11-15 2,707.8 1,138.5 177.4 66.1 40.8 %16-20 20,583.1 3,692.6 1,374.6 637.0 59.7 %Y ?
1-10 257.2 157.7 18.4 8.9 1.1 %11-15 3607.3 1838.7 476.8 161.1 3.0 %Table 4: Average and median time of the LP/ILP solver (inseconds).
% frac.
indicates how often the LP gives a fractionalanswer.
Y ?
indicates the dynamic program using set Y ?
as de-fined in Section 4.1, and Y ??
indicates the dynamic program us-ing states (w1, w2, n, r).
The statistics for ILP for length 16-20are based on 50 sentences.6.1 Comparison to an LP/ILP solverTo compare to a linear programming (LP) or inte-ger linear programming (ILP) solver, we can im-plement the dynamic program (search over the setY ?)
through linear constraints, with a linear ob-jective.
The y(i) = 1 constraints are also lin-ear.
Hence we can encode our relaxation within anLP or ILP.
Having done this, we tested the result-ing LP or ILP using Gurobi, a high-performancecommercial grade solver.
We also compare toan LP or ILP where the dynamic program makesuse of states (w1, w2, n, r)?i.e., the span (l,m) isdropped, making the dynamic program smaller.
Ta-ble 4 shows the average time taken by the LP/ILPsolver.
Both the LP and the ILP require very longrunning times on these shorter sentences, and run-ning times on longer sentences are prohibitive.
Ouralgorithm is more efficient because it leverages thestructure of the problem, by directly using a combi-natorial algorithm (dynamic programming).6.2 Comparison to MOSESWe now describe comparisons to the phrase-baseddecoder implemented in MOSES.
MOSES usesbeam search to find approximate solutions.The distortion limit described in section 3 is thesame as that in Koehn et al (2003), and is the sameas that described in the user manual for MOSES(Koehn et al, 2007).
However, a complicating fac-tor for our comparisons is that MOSES uses an ad-ditional distortion constraint, not documented in themanual, which we describe here.5 We call this con-straint the gap constraint.
We will show in experi-ments that without the gap constraint, MOSES failsto produce translations on many examples.
In ourexperiments we will compare to MOSES both withand without the gap constraint (in the latter case, wediscard examples where MOSES fails).We now describe the gap constraint.
For a se-quence of phrases p1, .
.
.
, pk define ?
(p1 .
.
.
pk) tobe the index of the left-most source-language wordnot translated in this sequence.
For example, ifthe bit-string for p1 .
.
.
pk is 111001101000, then?
(p1 .
.
.
pk) = 4.
A sequence of phrases p1 .
.
.
pLsatisfies the gap constraint if and only if for k =2 .
.
.
L, |t(pk) + 1 ?
?
(p1 .
.
.
pk)| ?
d, where d isthe distortion limit.
We will call MOSES withoutthis restriction MOSES-nogc, and MOSES with thisrestriction MOSES-gc.Results for MOSES-nogc Table 5 shows thenumber of examples where MOSES-nogc fails togive a translation, and the number of search errorsfor those cases where it does give a translation, fora range of beam sizes.
A search error is defined as acase where our algorithm produces an exact solutionthat has higher score than the output from MOSES-nogc.
The number of search errors is significant,even for large beam sizes.5Personal communication from Philipp Koehn; see also thesoftware for MOSES.34Beam size Fails # search errors percentage100 650/1,818 214/1,168 18.32 %200 531/1,818 207/1,287 16.08 %1000 342/1,818 115/1,476 7.79 %10000 169/1,818 68/1,649 4.12 %Table 5: Table showing the number of examples whereMOSES-nogc fails to give a translation, and the num-ber/percentage of search errors for cases where it does give atranslation.Diff.
MOSES-gc MOSES-gc MOSES-nogcs =100 s =200 s=10000.000 ?
0.125 66 (24.26%) 65 (24.07%) 32 ( 27.83%)0.125 ?
0.250 59 (21.69%) 58 (21.48%) 25 ( 21.74%)0.250 ?
0.500 65 (23.90%) 65 (24.07%) 25 ( 21.74%)0.500 ?
1.000 49 (18.01%) 49 (18.15%) 23 ( 20.00%)1.000 ?
2.000 31 (11.40%) 31 (11.48%) 5 ( 4.35%)2.000 ?
4.000 2 ( 0.74%) 2 ( 0.74%) 3 ( 2.61%)4.000 ?13.000 0 ( 0.00%) 0 ( 0.00%) 2 ( 1.74%)Table 6: Table showing statistics for the difference between thetranslation score from MOSES, and from the optimal deriva-tion, for those sentences where a search error is made.
ForMOSES-gc we include cases where the translation produced byour system is not reachable by MOSES-gc.
The average scoreof the optimal derivations is -23.4.Results for MOSES-gc MOSES-gc uses the gapconstraint, and thus in some cases our decoder willproduce derivations which MOSES-gc cannot reach.Among the 1,818 sentences where we produce a so-lution, there are 270 such derivations.
For the re-maining 1,548 sentences, MOSES-gc makes searcherrors on 2 sentences (0.13%) when the beam size is100, and no search errors when the beam size is 200,1,000, or 10,000.Table 6 shows statistics for the magnitude ofthe search errors that MOSES-gc and MOSES-nogcmake.BLEU Scores Finally, table 7 gives BLEU scores(Papineni et al, 2002) for decoding using MOSESand our method.
The BLEU scores under the twodecoders are almost identical; hence while MOSESmakes a significant proportion of search errors, thesesearch errors appear to be benign in terms of theirimpact on BLEU scores, at least for this particulartranslation model.
Future work should investigatewhy this is the case, and whether this applies to othermodels and language pairs.7 ConclusionsWe have described an exact decoding algorithm forphrase-based translation models, using Lagrangiantype of Moses beam size # sents Moses our methodMOSES-gc100 1,818 24.4773 24.5395200 1,818 24.4765 24.53951,000 1,818 24.4765 24.539510,000 1,818 24.4765 24.5395MOSES-nogc100 1,168 27.3546 27.3249200 1,287 27.0591 26.99071,000 1,476 26.5734 26.612810,000 1,649 25.6531 25.6620Table 7: BLEU score comparisons.
We consider onlythose sentences where both decoders produce a transla-tion.relaxation.
The algorithmic construction we havedescribed may also be useful in other areas of NLP,for example natural language generation.
Possi-ble extensions to the approach include methods thatincorporate the Lagrangian relaxation formulationwithin learning algorithms for statistical MT: we seethis as an interesting avenue for future research.A Step SizeSimilar to Koo et al (2010), we set the step size atthe t?th iteration to be ?t = 1/(1 + ?t), where ?t isthe number of times that L(u(t?))
> L(u(t?
?1)) forall t?
?
t. Thus the step size decreases each time thedual value increases from one iteration to the next.Acknowledgments Yin-Wen Chang and MichaelCollins were supported under the GALE programof the Defense Advanced Research Projects Agency,Contract No.
HR0011-06-C-0022.
Michael Collinswas also supported by NSF grant IIS-0915176.ReferencesGraeme Blackwood, Adria` de Gispert, Jamie Brunning,and William Byrne.
2009.
Large-scale statisticalmachine translation with weighted finite state trans-ducers.
In Proceeding of the 2009 conference onFinite-State Methods and Natural Language Process-ing: Post-proceedings of the 7th International Work-shop FSMNLP 2008, pages 39?49, Amsterdam, TheNetherlands, The Netherlands.
IOS Press.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19:263?311, June.Ulrich Germann, Michael Jahr, Kevin Knight, DanielMarcu, and Kenji Yamada.
2001.
Fast decoding andoptimal decoding for machine translation.
In Proceed-35ings of the 39th Annual Meeting on Association forComputational Linguistics, ACL ?01, pages 228?235.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Conference of the North AmericanChapter of the Association for Computational Linguis-tics on Human Language Technology, NAACL ?03,pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, ACL ?07,pages 177?180.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of theMT Summit.Nikos Komodakis, Nikos Paragios, and Georgios Tziri-tas.
2007.
MRF optimization via dual decomposition:Message-passing revisited.
In Proceedings of the 11thInternational Conference on Computer Vision.Terry Koo, Alexander M. Rush, Michael Collins, TommiJaakkola, and David Sontag.
2010.
Dual decompo-sition for parsing with non-projective head automata.In Proceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages1288?1298, Cambridge, MA, October.
Association forComputational Linguistics.Bernhard Korte and Jens Vygen.
2008.
CombinatorialOptimization: Theory and Application.
Springer Ver-lag.Shankar Kumar and William Byrne.
2005.
Local phrasereordering models for statistical machine translation.In Proceedings of the conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing, HLT ?05, pages 161?168.Claude Lemare?chal.
2001.
Lagrangian Relaxation.In Computational Combinatorial Optimization, Op-timal or Provably Near-Optimal Solutions [basedon a Spring School], pages 112?156, London, UK.Springer-Verlag.Angelia Nedic?
and Asuman Ozdaglar.
2009.
Approxi-mate primal solutions and rate analysis for dual sub-gradient methods.
SIAM Journal on Optimization,19(4):1757?1780.Franz Josef Och, Christoph Tillmann, Hermann Ney, andLehrstuhl Fiir Informatik.
1999.
Improved alignmentmodels for statistical machine translation.
In Pro-ceedings of the Joint SIGDAT Conference on Empiri-cal Methods in Natural Language Processing and VeryLarge Corpora, pages 20?28.Franz Josef Och, Nicola Ueffing, and Hermann Ney.2001.
An efficient A* search algorithm for statisti-cal machine translation.
In Proceedings of the work-shop on Data-driven methods in machine translation -Volume 14, DMMT ?01, pages 1?8, Stroudsburg, PA,USA.
Association for Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Computa-tional Linguistics, ACL ?03, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, andWei Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedings ofACL 2002.Sebastian Riedel and James Clarke.
2006.
Incrementalinteger linear programming for non-projective depen-dency parsing.
In Proceedings of the 2006 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP ?06, pages 129?137, Stroudsburg, PA,USA.
Association for Computational Linguistics.Sebastian Riedel and James Clarke.
2009.
Revisitingoptimal decoding for machine translation IBM model4.
In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, Companion Volume: Short Papers, NAACL-Short ?09, pages 5?8, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Alexander M. Rush and Michael Collins.
2011.
Exactdecoding of syntactic translation models through La-grangian relaxation.
In Proceedings of ACL.Alexander M Rush, David Sontag, Michael Collins, andTommi Jaakkola.
2010.
On dual decomposition andlinear programming relaxations for natural languageprocessing.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 1?11, Cambridge, MA, October.
Association forComputational Linguistics.David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?08, pages 145?156.David Sontag, Talya Meltzer, Amir Globerson, TommiJaakkola, and Yair Weiss.
2008.
Tightening LP relax-ations for MAP using message passing.
In Proceed-ings of the 24th Conference on Uncertainty in Artifi-cial Intelligence, pages 503?510.Christoph Tillmann and Hermann Ney.
2003.
Word re-ordering and a dynamic programming beam search al-gorithm for statistical machine translation.
Computa-tional Linguistics, 29:97?133, March.Christoph Tillmann.
2006.
Efficient dynamic pro-gramming search algorithms for phrase-based SMT.36In Proceedings of the Workshop on ComputationallyHard Problems and Joint Inference in Speech and Lan-guage Processing, CHSLP ?06, pages 9?16.Roy W. Tromble and Jason Eisner.
2006.
A fastfinite-state relaxation method for enforcing global con-straints on sequence decoding.
In Proceedings ofthe main conference on Human Language TechnologyConference of the North American Chapter of the As-sociation of Computational Linguistics, HLT-NAACL?06, pages 423?430, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Martin Wainwright, Tommi Jaakkola, and Alan Will-sky.
2005.
MAP estimation via agreement ontrees: Message-passing and linear programming.
InIEEE Transactions on Information Theory, volume 51,pages 3697?3717.Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-cedda.
2009.
Phrase-based statistical machine transla-tion as a traveling salesman problem.
In Proceedingsof the Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Volume1 - Volume 1, ACL ?09, pages 333?341, Stroudsburg,PA, USA.
Association for Computational Linguistics.37
