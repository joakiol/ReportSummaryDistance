An Evaluation of Strategies for Selective Utterance Verif icationfor Spoken Natural  Language DialogRonnie W. SmithDepartment of MathematicsComputer Science SubareaEast Carolina UniversityGreenville, NC 27858, USArws?cs ,  ecu .
eduAbstractAs with human-human i teraction, spokenhuman-computer dialog will contain situa-tions where there is miscommunication.
Iexperimental trials consisting of eight dif-ferent users, 141 problem-solving dialogs,and 2840 user utterances, the Circuit Fix-ItShop natural anguage dialog system misin-terpreted 18.5% of user utterances.
Thesemiscommunications created various prob-lems for the dialog interaction, rangingfrom repetitive dialog to experimenter in-tervention to occasional failure of the dia-log.
One natural strategy for reducing theimpact of miscommunication is selectiveverification of the user's utterances.
Thispaper reports on both context-independentand context-dependent strategies for utter-ance verification that show that the use ofdialog context is crucial for intelligent se-lection of which utterances to verify.1 Building Robust Spoken NaturalLanguage InterfacesRecent advances in speech recognition technologyhave raised expectations about the development ofpractical spoken natural language (NL) interfaces.Such interfaces can provide user flexibility as well asallow users to devote their hands and eyes to the taskat hand.
In particular, the ability to obtain comput-erized telephone assistance via a robust NL interfacecould provide ready access to information that cur-rently requires direct human interaction.
However,if such interfaces are to be effective with the gen-eral populous, they must be capable of dealing withmiscommunication.
Miscommunication can arise atseveral different levels, ranging from discourse struc-ture and speech act misunderstanding (McRoy and41Hirst, 1995) to misinterpretation due to misrecog-nition of a speaker's words.
We report on a studythat focuses on this latter type of miscommunica-tion.
While speech recognizer performance in con-trolled environments has improved dramatically inthe past decade, recognition errors still occur.
Fur-thermore, current speech recognizers cannot performoptimally in uncontrolled environments such as tele-phone interactions.We examine the strategy of verification subdialogsfor resolving miscommunications due to misrecog-nition.
We first review how verification subdialogscan increase the rate of correct interpretation from81.5% to 97.4% but at a cost of unnecessary verifica-tions approximately once every five user utterances.However, by adopting a context-dependent strategyfor deciding when to use a verification subdialog, wecan maintain an understanding rate of 95.3% whilereducing the number of unnecessary verifications byover one half.After describing the technique of selective utter-ance verification, this paper gives an overview of thedialog system environment that provides the dataused in testing various strategies for selective ut-terance verification, the Circuit Fix-It Shop.
Thepaper concludes with a description of both context-independent and context-dependent strategies forselective utterance verification and reports on thecomparative r sults of dialog simulations using thesestrategies.
The results show the importance of ex-ploiting dialog context for intelligent selection ofwhich utterances to verify.2 Se lec t ive  Ver i f i ca t ion  ofQuest ionab le  User  InputsEvery system that uses natural language under-standing will sometimes misunderstand its input.Misunderstandings can arise from speech recognitionerrors or inadequacies in the language grammar, orthey may result from an input that is ungrammati-Spoken: i want to fix this circuitRecognized: power a six a circuitSpoken: the one is flashing for a longer period of timeRecognized: one is flashing forth longer in a timeSpoken: there is no wire on connector one zero fourRecognized: stays know wire i connector one zero forFigure 1: Sample Utterances with Word Misrecognitioncal or ambiguous.
Here we focus on misunderstand-ings caused by speech recognition errors.
Exam-ples of misrecognized inputs from interacting withthe Circuit Fix-It Shop are given in figure 1.
Onemethod for reducing the number of misunderstand-ings is to require the user to verify each utteranceby either speaking every utterance twice, or confirm-ing a word-by-word read back of every utterance(e.g., (Baber and Hone, 1993)).
Such verificationis effective at reducing errors that result from wordmisrecognitions, but does nothing to reduce misun-derstandings that result from other causes.
Further-more, verification of all utterances can be needlesslywearisome to the user, especially if the system isworking well.A better approach is to have the system verify itsinterpretation of an input only under circumstanceswhere the accuracy of its interpretation is seriouslyin doubt, or correct understanding is essential tothe success of the dialog.
The verification is accom-plished through the use of a verification subdialog---ashort sequence of utterances intended to confirm orreject the hypothesized interpretation.
The follow-ing example of a verification subdialog illustrates theidea.Computer: What is the switch position whenthe LED is off7User: Up.Computer: Did you mean to say that theswitch is up?User : Yes.Notable features of such verification subdialogs in-clude the following.?
Verification is selective.
A verification subdialogis initiated only if it is believed that the overallperformance and accuracy of the dialog systemwill be improved.
In this way, the dialog systemresponds much as a person would.?
Verification is tunable.
The propensity of thesystem to verify can be adjusted so as to pro-42vide any required level of speech understandingaccuracy.?
Verification operates at the semantic level.
Thesystem verifies an utterance's meaning, not itssyntax.
This helps overcome misunderstandingsthat result from inadequacies in the languagemodel, or ungrammatical or ambiguous inputs.Two important definitions concerning selectiveverification are the following.
An under-verificationis defined as the event where the system generatesa meaning that is incorrect but not verified.
Anover-verification occurs when a correct meaning isverified.
The example just given is an example of anover-verification.
The goal of any algorithm for se-lective utterance verification is to minimize the rateof under-verifications while also holding the rate ofover-verifications to as low a value as possible.
Thatis, the goal is to only verify utterances that needverifying, and to verify as many of these as possi-ble.
In section 4 we report on the results of testsof various strategies for deciding when to engage inverification subdialogs within a specific dialog envi-ronment, the Circuit Fix-It Shop.
In order to un-derstand the strategies used, an overview of this en-vironment must first be presented.3 Dialog Environment: The CircuitFix-It Shop3.1 Genera l  Character i s t i csThe data used in this study were collected in ex-perimental trials conducted with "The Circuit Fix-It Shop," a spoken NL dialog system constructedin order to test the effectiveness of an integrateddialog processing model that permits variable ini-tiative behavior as described in (Smith and Hipp,1994) and (Smith, Hipp, and Biermann, 1995).
Theimplemented ialog system assists users in repair-ing a Radio Shack 160 in One Electronic ProjectKit.
The particular circuit being used causes theLight-Emitting Diode (LED) to alternately display aone and seven.
The system can detect errors causedby missing wires as well as a dead battery.
Speechrecognition is performed by a Verbex 6000 runningon an IBM PC.
To improve speech recognition per-formance we restrict the vocabulary to 125 words.A DECtalk DTCO1 text-to-speech onverter is usedto provide spoken output by the computer.After testing system prototypes with a few vol-unteers, eight subjects used the system during theformal experimental phase.
After a warmup ses-sion where the subject trained on the speech recog-nizer and practiced using the system, each subjectparticipated in two sessions where up to ten prob-lems were attempted.
Subjects attempted a totalof 141 dialogs of which 118 or 84% were completedsuccessfully.
1 The average speech rate by subjectswas 2.9 sentences per minute; the average task com-pletion times for successful dialogs were 6.5 minutes.An excerpt from an actual interaction with thesystem is given in figure 2.
2 The words in paren-theses represent he actual sequence of words thatthe speech recognizer sent to the dialog system foranalysis.
As can be seen from the example, the sys-tem usually understood the user utterance (but notalways).
We next describe two features of the sys-tem that were useful in the interpretation process:(1) error-correcting parsing; and (2) dialog expec-tation.
In section 4 we will see how these featuresassist in deciding when to engage the user in a veri-fication subdialog.3.2 Overcoming  Mis recogn i t ion  byEr ror -Cor rect ing  Pars ingThe system was able to find the correct meaning for81.5% of the more than 2800 input utterances eventhough only 50% of these inputs were correctly rec-ognized word for word by use of an error-correctingparser that uses a dynamic programming approachsimilar to (Ney, 1991) to compute the best n parsesfor the input.
What constitutes "best" is determinedby a cost matrix for the possible words in the vocab-ulary and the given grammar.
The cost matrix de-fines the cost for inserting or deleting words as wellas the cost for a word substitution when such sub-stitutions are allowed.
The intent is to permit sub-stitutions for words which sound very similar, suchas "do" and "two/to/too," words that are likely to1 Of the 23 dialogs which were not completed, misun-derstandings due to misrecognition were the cause in 13of these failures.
There were a variety of causes for thefailure in the other 10 dialogs, ranging from inadequategrammar coverage to subject error in connecting wires.2C denotes utterances spoken by the computer whileO denotes utterances spoken by the user.43be confused by the speech recognizer.
The parserperforms insertions, deletions, and substitutions inorder to transform the input into a grammatical  ut-terance.
With each "grammatical" utterance is as-sociated a parse cost (PC), which is the sum of thecosts of each insertion, deletion, and substitution re-quired for the transformation.
For each of the bestn parses, an expectation cost (EC) is also producedaccording to how likely the input is to occur accord-ing to the expectations.
The total cost of a parse isa weighted sum of PC and EC.
The values for theseweights and costs have been assigned heuristically.Figure 3 shows some of the utterances uccessfullyhandled by the implemented system during the ex-periment.
For instance, example 1 is transformedinto a grammatical utterance by substituting thephonetically similar word "six" for "fix," and "and"for "can."
This example would have a relatively lowparse cost indicating the system has high confidencein its interpretation.
On the other hand, example 3requires a large number of costly insertions and dele-tions, indicating a lack of confidence in the quality ofthe interpretation.
As will be seen in section 4, thisdifference in scores is an important component indeciding when to engage in verification subdialogs.3.3 Expectation in DialogAn important phenomenon that is exploited in com-putational models of language is that of expectation(Allen, 1995).
Based on the current context, thereis an expectation of what is to come next.
These ex-pectations can be for phonemes, morphemes, words,or meanings.
For example, if we hear the word "the"we expect the next word to be either an adjective ornoun.
When the computer asks, "Is the knob po-sition at 10T', we have greatest expectation for aresponse of either "yes" or "no", lesser expectationfor a sentence answer such as "The knob positionis five.
", and still less expectation for a clarifica-tion question or comment such as "Which knob?
","Where is the knob?
", or "I do not see it.
"Consider the following dialog sample.i.
C: Is there a wire between connector84 and connector  99?2.
U: It is not there.3.
C: Add a wire between connector  84and connector  99.4.
U: I need help.5.
C: Obtain a wire.6.
U: Okay.The system must use the established ialog contextin order to properly interpret every user utteranceas follows.I.
C: This is the Circuit Fix-It Shop.
How may I help you?2.
U: I want to fix this circuit.
(power a six a circuit)3.
C: I do not understand.
Please repeat what you said.4.
U: I want to fix this circuit (i want a fix a circuit)5.
C: What is the ID of the circuit?6.
U: RSl11.
(rs111)7.
C: I am famil iar with that circuit.
The LED is supposed to be displayingalternately f lashing one and seven.8.
U: The LED is displayin E nothing.
(be led a displaying nothing)9.
C: What is the switch at when the LED is off?10.
U: The switch is down.
(the switch yes down)11.
C: Put the switch up.12.
U: Done.
(done)13.
C: Put the knob to one zero.14.
U: The knob is at one zero.
(the knob and add one zero)15.
C: What is the LED displaying?16.
U: The LED is displaying a one and seven at the same time(be led a display?ng one an seven at the same time)17.
C: Put the knob to zero.18.
U: Done.
(done)19.
C: What is the LED displaying?20.
U: The same thing.
(the same same)21.
C: Did you mean to say the LED is displaying the same thing?22.
U: Yes.
(yes)Figure 2: Sample Dialog Interaction with the Circuit Fix-It Shop?
Utterance 2: to determine what "it" refers to(i.e., the wire from 84 to 99).?
Utterance 4: to determine what the user needshelp with (i.e., adding the wire).?
Utterance 6: to determine whether "okay" de-notes confirmation or comprehension (i.e., con-firmation that the wire has been obtained).Effective use of expectation is necessary for con-straining the search for interpretations and achievingefficient processing of NL inputs.
This is particularlycrucial in spoken NL diMog, where speakers expectfast response times (Oviatt and Cohen, 1989).The system model of expectations i similar tothat of (Young et al, 1989) in that we predict themeanings of possible user responses based on the cur-rent dialog goal.
The details of the system model canbe found in (Smith and Hipp, 1994).
Here we re-view the key aspects that are exploited in a context-dependent strategy for verification.
We define ex-pectations based on an abstract representation ofthe current task goal.
For example,goal(user, ach(prop( Obj, Prop Narne, PropValue) ) 33This notation is an abbreviated form of the actual44denotes the goal that the user achieve thevalue (PropValue) for a particular property(PropName), of an object (Obj).
The specific val-ues for Obj, PropName, and PropValue are filledin according to the current goal.
For example, thegoal of setting the switch position to up may be rep-resented asgoal(user, ach (prop( switch, position, up)))while the goal of observing the knob's color wouldbegoal(user, obs(prop(knob, color, PropYalue) ) )where PropValue is an uninstantiated variablewhose value should be specified in the userinput.
General expectations for the mean-ing of user responses to a goal of the formgoal(user, ach(prop(...))) include the following:?
A question about the location of Obj.?
A question about how to do the action.?
An assertion that Obj now has the valuePropValue for property PropName.representation used in the system as described in (Smithand Hipp, 1994).Example iComputer: There is supposed to be a wire between connector 68 and connector 87.User: Wire connecting six eight and eight seven.Recognized: Wire connecting fix eight can eight seven.Example 2Computer: Putting the knob to one zero is desirable.User: The knob is at one zero.Recognized: Seven knob use that one zero.Example 3Computer: Is anything else on the LED on?User: LED is displaying a not flashing seven.Recognized: Be down it be yes displaying be knob flashing seven then.Figure 3: Sample Misrecognitions Correctly Parsed?
An acknowledgment that the action has beencompleted.Even when using error-correcting parsing and dia-log expectations, the Circuit Fix-It Shop misunder-stood 18.5% of user utterances during the experi-mental testing.
We now turn our attention to anempirical study of strategies for selective utteranceverification that attempt o select for verification asmany of the misunderstood utterances as possiblewhile minimizing the selection of utterances thatwere understood correctly.
These strategies makeuse of information obtainable from dialog expecta-tion and the error-correcting parsing process.4 Eva luat ing  Ver i f i ca t ion  S t ra teg ies4.1 S t ra tegy  1: Us ing  Parse  Cost  On lyAn enhancement to the Circuit Fix-It Shop de-scribed in (Smith and Hipp, 1994) allows for a verifi-cation subdialog only when the hypothesized mean-ing is in doubt or when accuracy is critical for thesuccess of the dialog.
The decision of whether ornot a particular input should be verified is madeby computing for each meaning a parser confidencescore (a measure of how plausible the parser's out-put is--this measure is proportional to the inverseof the total cost (section 3.2) normalized for utter-ance length) and a verification threshold (a measureof how important he meaning is toward the suc-cess of the dialog--greater importance is denoted bya higher threshold).
The decision rule for decidingwhen to initiate a verification subdialog is specified45as follows:IF the Parser Confidence Score > theVerif ication Threshold THEN DO NOTengage in a verif ication subdialogELSEengage in a verif ication subdialogThis basic capability for verification subdialogswas not available during the 141 dialog experiment.However, simulations run on the collected dataraised the percentage of utterances that are correctlyunderstood from 81.5% to 97.4%.
4 Unfortunately,besides improving understanding through verifica-tion of utterances initially misinterpreted, the sys-tem also verified 19.2% of the utterances initiallyinterpreted correctly.
An example would be ask-ing, "Did you mean to say the switch is up?
", whenthat is what the user originally said.
These over-verifications result in extraneous dialog, and if ex-cessive, will limit usability.4.2 S t ra tegy  2" Us ing  Context  On lyThe previous decision rule for utterance verificationfocused exclusively on the local information aboutparsing cost and ignores dialog context.
In that sit-uation the over-verification rate was 19.2% while the4Consequently, the under-verification rate is 2.6%.We say that an utterance is correctly understood if itis either correctly interpreted initially, or is an utterancefor which the system will engage the user in a verificationsubdialog.
It is of course possible that the verificationsubdialog may not succeed, but we have not yet assessedthe likelihood of that and thus do not consider this pos-sibility during the evaluation of the various strategies.?
obs(prop(Obj, PropName, PropValue)) (PropValue unspecified)--observing a property.Example: a wh-question (e.g., "What is the switch position?
")Main Expectation: direct answer (e.g., "The switch is up.").?
obs(prop(Obj, PropName, PropValue)) (PropValue specified)Example: a yes-no question {e.g., "Is the switch up?
")Main Expectation: (1) yes/no response and (2) a direct answer as in the above case.?
obs(meas(Des, Val))--observing a measurement described by Des where Val is the value.Example: a wh-question (e.g., "What is the voltage between connectors 121 and 34?
")Main Expectation: direct answer (e.g., "Seven" or "The voltage is seven").?
obs(behav(Obs, Cond))--observing a behavior where the result of the observation (Obs), depends on theunderlying physical conditions present (Cond) when the observation was made.Example: a wh-question (e.g., "What is the LED displaying when the switch is up?
")Main Expectation: a direct answer (e.g., "The LED is displaying only a not flashing seven.").?
ach(prop(Obj, PropName, PropValue))--achieving a property.Example: a command (e.g., "Put the switch up.
")Main Expectation: (1) completion acknowledgement (e.g., "Okay"desired property now exists (e.g., "The switch is up.
").or "Done") and (2) assertion that the?
learn(Fact)--learning a fact.
The fact could concern a piece of state information (e.g., that the switchis located in the lower left portion of the circuit), that an action needs completing (e.g., "Putting theswitch up is desirable,"), or that a certain property should or should not be true (e.g., there should bea wire between connectors 34 and 80).
In all cases, one main expectation is an acknowledgment thatthe Fact is understood.
In the case of an action completion or a property status, there is also a mainexpectation for either that the user completed the action (e.g., "Done" or "The switch is up"), or thatthe property status is verified (e.g., "Wire connecting 34 and 80").Figure 4: Summary of Main Expectations for Major Goalsunder-verification rate was 2.6%.
What about usinga rule solely based on context?
For each abstracttask goal, we define a subset of the expectations asthe main expectation.
This subset consists of theexpected meanings that denote a normal continua-tion of the task.
Figure 4 lists these expectations forthe major task goals of the model.
For cooperativetask-assistance dialog, making the assumption thatthe meaning of the user's utterance will belong toa very small subset of the expectations for each ab-stract goal allows us to define the following context-dependent decision rule for utterance verification.IF utterance in the Main Expectation THENDO NOT engage in a verificationsubdialogELSEengage in a verification suhdialogUsing this decision rule, the over-verification raterises to 31.8% while the under-verification rate fallsto 1.4%.
Although it significantly reduces the under-verification rate, this strategy clearly leads to an ex-46cessive number of over-verifications.
We next con-sider combination strategies that look at both parsecost and context.4.3 S t ra tegy  3: Parse  Cost /ContextCombinationThe Strategy 1 decision rule for utterance verifica-tion says to engage in a verification subdialog if theparser confidence value falls below the verificationthreshold.
With context-dependent verification weadditionally require that the utterance meaning can-not be part of the main expectation.
Thus, the de-cision rule for verification may be revised as follows:IF the Parser Confidence Score > theVerification Threshold THEN DO NOTengage in a verification subdialogELSE IF utterance meaning in the MainExpectation THEN DO NOT engage in averification subdialogELSEengage in a verification subdialogUsing this decision rule and comparing it to Strat-egy 1, the over-verification rate drops from 19.2%to 7.6% while the under-verification rate rises from2.6% to 4.7% (i.e., the percentage of utterances cor-rectly understood falls from 97.4% to 95.3%).
Thiscorresponds to a reduction in over-verifications fromonce every 5.2 user utterances to once every 13.2user utterances while under-verifications (i.e., unde-tected misunderstandings) rises from once every 38.5user utterances to once every 21.3 user utterances.It should be noted that on average, users spoke 20utterances per dialog.
We now examine a context-dependent strategy that takes into account specificdomain information.4.4 S t ra tegy  4: Domaln -DependentExcept ionsAs previously noted, correctly interpreting certainutterances is crucial for efficient continuation of thedialog.
In the Circuit Fix-It Shop, the crucial condi-tion was correct determination of the LED display.Several utterances in each dialog concerned a discus-sion of the LED display.
Consequently, assertionsabout the LED display were often part of the mainexpectation.However, due to the myriad of possible LED dis-plays and the frequent misrecognition of key wordsand phrases in these descriptions, an effective di-alog system would want to be careful to ascertaincorrectness in interpreting these descriptions.
Con-sequently, we modify the verification decision rule asfollows:IF the Parser Confidence Score > theVerification Threshold THEN DO N0Tengage in verification subdialogELSE IF the utterance meaning is anassertion about the LED display THENengage in a verification subdialogELSE IF the utterance meaning is in theMain Expectation THEN DO NOT engage ina verification subdialogELSEengage in a verification subdialogAs a result, the decision rule for verifying utter-ances concerning the LED focuses solely on the localinformation about parsing cost and does not con-sider dialog context information about expectation.
5Such a modification might also be appropriate in5 In actuality, a small component of the total parsingcost is the expectation cost based on dialog context, butthat weighting is negligible compared to the weightingof the parse cost, the predominant factor in computingtotal cost.47other domains for information deemed essential tocontinuing progress.For this final decision rule the the over-verificationrate is 9.8% while the under-verification rate is 3.7%.4.5 S t ra tegy  Compars ionTable 1 summarizes the results of the four strate-gies for a fixed Verification Threshold.
We con-clude that the combination of considering both thelocal information of the parsing cost and the dia-log context information about expectation providesthe best strategy.
We also note that inclusion ofdomain-dependent information does not show anynotable improvement in the over-verification/under-verification tradeoff as compared with the context-dependent but domain-independent Strategy 3.
6 Webelieve the results show that for task-oriented o-mains where there are fairly strong expectations forutterances that relate directly to task goals such asthose described in figure 4, a context-dependent ver-ification strategy is effective at reducing the over-verification rate to a reasonable amount while keep-ing the number of under-verifications to a near min-imum.
Further study is needed to determine thepractical usefulness of this strategy in an actual ex-perimental situation and it is an open question asto whether or not such strategies are feasible for lesstask-specific domains such as advisory dialogs anddatabase query environments.4.6 Improv ing  AccuracyObtaining a higher accuracy requires reducing theunder-verification rate.
For Strategy 1 we exploredthe impact of raising and lowering the thresholdon the over- and under-verification rates.
Not sur-prisingly, there was a tradeoff.
As the thresholdwas raised, more utterances are verified, resulting infewer under-verifications but more over-verifications.Lowering the threshold had the opposite impact.
Infact, using just the strategy of lowering the thresholdto reduce the over-verification rate to 9.3% causesthe under-verification rate to rise to 8.0%.
In con-trast, the new context-dependent strategy, Strat-egy 3, achieves an over-verification rate of 7.6%, butthe under-verification rate is only 4.7%.
Clearly, theuse of dialog context in the verification subdialog de-cision rule improves ystem performance.
Neverthe-less, a small set of under-verifications remains.
Arethere any possibilities for further reductions in theunder-verifications without a substantial increase inthe over-verification rate?~This of course, does not preclude the possibility thatdomain-dependent i eraction may be more useful inother domains.Strategy1.
Parse Cost Only2.
Context Only3.
Parse Cost/Context Combination4.
Domain-Dependent ExceptionsUnder-verification Rate2.6%1.4%4.7%3.7%Over-verification Rate19.2%31.8%7.6%9.8%Table 1: Comparative Performance ofVerification Subdialog Decision StrategiesAn analysis of the 133 under-verifications that oc-cur with the new strategy indicates that while someof the under-verifications are due to deficiencies inthe grammar, there is a a core group of under-verifications where misrecognition of the speaker'swords is impossible to overcome.
Incorrect recogni-tion of digits, lost content words, and misrecognizedcontent words can cause the system to have highconfidence in an incorrect interpretation.
One ap-proach that may prove helpful with this problem isthe use of speech recognition systems that providealternate hypotheses for the speech signal along withscoring information.
Another possibility is word byword verification of the speaker input (see (Baberand Hone, 1993)), but such a strategy is too time-consuming and tedious for general spoken naturallanguage dialog, especially when the user does nothave access to a visual display of what the systemhypothesizes was spoken.
In general, experimentaltrials to observe subject reaction to verification sub-dialogs are needed.In conclusion, while useful, there appear to belimits to the effectiveness of verification subdi-alogs.
Consequently, strategies for delayed etectionand resolution of miscommunication (e.g.
(McRoyand Hirst, 1995), (Brennan and Hulteen, 1995),and (Lambert and Carberry, 1992)) become nec-essary and remain an area of continued investiga-tion.
These include both computer-initiated as wellas user-initiated strategies.5 AcknowledgmentsThe author expresses his appreciation to D. RichardHipp for his work on the error-correcting parser andfor his initial work on context-independent verifica-tion.
The author also wishes to express his thanks toSteven A. Gordon and Robert D. Hoggard for theirsuggestions concerning this work and an earlier draftof this paper.
Other researchers who contributed tothe development of the experimental system includeAlan W. Biermann, Robert D. Rodman, Ruth S.Day, Dania Egedi, and Robin Gambill.
This researchhas been supported by National Science FoundationGrant IRI-9501571.ReferencesAllen, J.F.
1995.
Natural Language Understand-ing.
The Benjamin/Cummings Publishing Com-pany, Inc., Menlo Park, California, 2nd edition.Baber, C. and K.S.
Hone.
1993.
Modelling error re-covery and repair in automatic speech recognition.Intl.
J. Man-Machine Studies, 39:495-515.Brennan, S.E.
and E.A.
Hulteen.
1995.
Interac-tion and feedback in a spoken language system:a theoretical framework.
Knowledge-Based Sys-tems, 8:143-151.Lambert, L. and S. Carberry.
1992.
Modeling ne-gotiation subdialogues.
In Proceedings of the 30thAnnual Meeting of the Association for Computa-tional Linguistics, pages 193-200.McRoy, S. and G. Hirst.
1995.
The repair ofspeech act misunderstandings by abductive infer-ence.
Computational Linguistics, pages 435-478.Ney, H. 1991.
Dynamic programming parsing forcontext-free grammars incontinuous speech recog-nition.
IEEE Transactions on Signal Processing,39(2):336-340.Oviatt, S.L.
and P.R.
Cohen.
1989.
The effects ofinteraction on spoken discourse.
In Proceedingsof the 27th Annual Meeting of the Association forComputational Linguistics, pages 126-134.Smith, R.W.
and D.R.
Hipp.
1994.
Spoken NaturalLanguage Dialog Systems: A Practical Approach.Oxford University Press, New York.Smith, R.W., D.R.
Hipp, and A.W.
Biermann.
1995.An architecture for voice dialog systems basedon Prolog-style theorem-proving.
ComputationalLinguistics, pages 281-320.Young, S.R., A.G. Hauptmann, W.H.
Ward, E.T.Smith, and P. Werner.
1989.
High level knowl-edge sources in usable speech recognition sys-tems.
Communications of the ACM, pages 183-194, February.48
