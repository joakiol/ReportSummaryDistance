Coling 2008: Advanced Dynamic Programming in Computational Linguistics ?
Tutorial notesManchester, August 2008Advanced Dynamic Programming inSemiring and Hypergraph Frameworks?Liang HuangDepartment of Computer and Information ScienceUniversity of Pennsylvanialhuang3@cis.upenn.eduJuly 15, 2008AbstractDynamic Programming (DP) is an important class of algorithmswidely used in many areas of speech and language processing.
Recentlythere have been a series of work trying to formalize many instances ofDP algorithms under algebraic and graph-theoretic frameworks.
Thistutorial surveys two such frameworks, namely semirings and directedhypergraphs, and draws connections between them.
We formalize twoparticular types of DP algorithms under each of these frameworks: theViterbi-style topological algorithms and the Dijkstra-style best-firstalgorithms.
Wherever relevant, we also discuss typical applications ofthese algorithms in Natural Language Processing.1 IntroductionMany algorithms in speech and language processing can be viewed as in-stances of dynamic programming (DP) (Bellman, 1957).
The basic idea ofDP is to solve a bigger problem by divide-and-conquer, but also reuses thesolutions of overlapping subproblems to avoid recalculation.
The simplestsuch example is a Fibonacci series, where each F (n) is used twice (if cached).The correctness of a DP algorithm is ensured by the optimal substructureproperty, which informally says that an optimal solution must contain op-timal subsolutions for subproblems.
We will formalize this property as analgebraic concept of monotonicity in Section 2.?Survey paper to accompany the COLING 2008 tutorial on dynamic programming.
Thematerial presented here is based on the author?s candidacy exam report at the Universityof Pennsylvania.
I would like to thank Fernando Pereira for detailed comments on anearlier version of this survey.
This work was supported by NSF ITR EIA-0205456.11search space \ ordering topological-order best-firstgraph + semirings (2) Viterbi (3.1) Dijkstra/A* (3.2)hypergraph + weight functions (4) Gen. Viterbi (5.1) Knuth/A* (5.2)Table 1: The structure of this paper: a two dimensional classification of dy-namic programming algorithms, based on search space (rows) and propoga-tion ordering (columns).
Corresponding section numbers are in parentheses.This report surveys a two-dimensional classification of DP algorithms(see Table 1): we first study two types of search spaces (rows): the semir-ing framework (Mohri, 2002) when the underlying representation is a di-rected graph as in finite-state machines, and the hypergraph framework(Gallo et al, 1993) when the search space is hierarchically branching as incontext-free grammars; then, under each of these frameworks, we study twoimportant types of DP algorithms (columns) with contrasting order of vis-iting nodes: the Viterbi style topological-order algorithms (Viterbi, 1967),and the Dijkstra-Knuth style best-first algorithms (Dijkstra, 1959; Knuth,1977).
This survey focuses on optimization problems where one aims to findthe best solution of a problem (e.g.
shortest path or highest probabilityderivation) but other problems will also be discussed.2 SemiringsThe definitions in this section follow Kuich and Salomaa (1986) and Mohri(2002).Definition 1.
A monoid is a triple (A,?, 1) where ?
is a closed associativebinary operator on the set A, and 1 is the identity element for ?, i.e., for alla ?
A, a ?
1 = 1 ?
a = a.
A monoid is commutative if ?
is commutative.Definition 2.
A semiring is a 5-tuple R = (A,?,?, 0, 1) such that1.
(A,?, 0) is a commutative monoid.2.
(A,?, 1) is a monoid.3.
?
distributes over ?
: for all a, b, c in A,(a ?
b) ?
c = (a ?
c) ?
(b ?
c),c ?
(a ?
b) = (c ?
a) ?
(c ?
b).4.
0 is an annihilator for ?
: for all a in A, 0 ?
a = a ?
0 = 0.22Semiring Set ?
?
0 1 intuition/applicationBoolean {0, 1} ?
?
0 1 logical deduction, recognitionViterbi [0, 1] max ?
0 1 prob.
of the best derivationInside R+?
{+?}
+ ?
0 1 prob.
of a stringReal R ?
{+?}
min + +?
0 shortest-distanceTropical R+?
{+?}
min + +?
0 with non-negative weightsCounting N + ?
0 1 number of pathsTable 2: Examples of semiringsTable 2 shows some widely used examples of semirings and their appli-cations.Definition 3.
A semiring (A,?,?, 0, 1) is commutative if its multiplicativeoperator ?
is commutative.For example, all the semirings in Table 2 are commutative.Definition 4.
A semiring (A,?,?, 0, 1) is idempotent if for all a in A,a ?
a = a.Idempotence leads to a comparison between elements of the semiring.Lemma 1.
Let (A,?,?, 0, 1) be an idempotent semiring, then the relation?
defined by(a ?
b) ?
(a ?
b = a)is a partial ordering over A, called the natural order over A.However, for optimization problems, a partial order is often not enoughsince we need to compare arbitrary pair of values, which requires a totalordering over A.Definition 5.
An idempotent semiring (A,?,?, 0, 1) is totally-ordered if itsnatural order is a total ordering.An important property of semirings when dealing with optimizationproblems is monotonicity, which justifies the optimal subproblem propertyin dynamic programming (Cormen et al, 2001) that the computation canbe factored (into smaller problems).Definition 6.
Let K = (A,?,?, 0, 1) be a semiring, and ?
a partial order-ing over A.
We say K is monotonic if for all a, b, c ?
A(a ?
b) ?
(a ?
c ?
b ?
c)(a ?
b) ?
(c ?
a ?
c ?
b)33Lemma 2.
Let (A,?,?, 0, 1) be an idempotent semiring, then its naturalorder is monotonic.In the following section, we mainly focus on totally-ordered semirings(whose natural order is monotonic).Another (optional) property is superiority which corresponds to the non-negative weights restriction in shortest-path problems.
When superiorityholds, we can explore the vertices in a best-first order as in the Dijkstraalgorithm (see Section 3.2).Definition 7.
Let K = (A,?,?, 0, 1) be a semiring, and ?
a partial order-ing over A.
We say K is superior if for all a, b ?
Aa ?
a ?
b, b ?
a ?
b.Intuitively speaking, superiority means the combination of two elementsalways gets worse (than each of the two inputs).
In shortest-path problems,if you traverse an edge, you always get worse cost (longer path).
In Table 2,the Boolean, Viterbi, and Tropical semirings are superior while the Realsemiring is not.Lemma 3.
Let (A,?,?, 0, 1) be a superior semiring with a partial order ?over A, then for all a ?
A1 ?
a ?
0.Proof.
For all a ?
A, we have 1 ?
1 ?
a = a by superiority and 1 being theidentity of ?
; on the other hand, we have a ?
0 ?
a = 0 by superiority and0 being the annihilator of ?.This property, called negative boundedness in (Mohri, 2002), intuitivelyillustrates the direction of optimization from 0, the initial value, towards asclose as possible to 1, the best possible value.3 Dynamic Programming on GraphsFollowing Mohri (2002), we next identify the common part shared betweenthese two algorithms as the generic shortest-path problem in graphs.Definition 8.
A (directed) graph is a pair G = (V, E) where V is the setof vertices and E the set of edges.
A weighted (directed) graph is a graphG = (V, E) with a mapping w : E 7?
A that assigns each edge a weight fromthe semiring (A,?,?, 0, 1).Definition 9.
The backward-star BS (v) of a vertex v is the set of incomingedges and the forward-star FS (v) the set of outgoing edges.44Definition 10.
A path ?
in a graph G is a sequence of consecutive edges,i.e.
?
= e1e2?
?
?
ekwhere eiand ei+1are connected with a vertex.
We definethe weight (or cost) of path ?
to bew(?)
=k?i=1w(ei) (1)We denote P (v) to be the set of all paths from a given source vertex sto vertex v. In the remainder of the section we only consider single-sourceshortest-path problems.Definition 11.
The best weight ?
(v) of a vertex v is the weight of the bestpath from the source s to v:1?
(v) ={1 v = s??
?P (v)w(?)
v 6= s(2)For each vertex v, the current estimate of the best weight is denotedby d(v), which is initialized in the following procedure:procedure Initialize(G, s)for each vertex v 6= s dod(v) ?
0d(s) ?
1The goal of a shortest-path algorithm is to repeatedly update d(v) foreach vertex v to some better value (based on the comparison ?)
so thateventually d(v) will converge to ?
(v), a state we call fixed.
For example, thegeneric update along an incoming edge e = (u, v) for vertex v is2d(v) ?
= d(u) ?
w(e) (3)Notice that we are using the current estimate of u to update v, so iflater on d(u) is updated we have to update d(v) as well.
This introduces theproblem of cyclic updates, which might cause great inefficiency.
To alleviatethis problem, in the algorithms presented below, we will not trigger theupdate until u is fixed, so that the u ?
v update happens at most once.3.1 Viterbi Algorithm for DAGsIn many NLP applications, the underlying graph exhibits some special struc-tural properties which lead to faster algorithms.
Perhaps the most common1By convention, if P (v) = ?, we have ?
(v) = 0.2Here we adopt the C notation where a ?
= b means the assignment a ?
a ?
b.55of such properties is acyclicity, as in Hidden Markov Models (HMMs).
Foracyclic graphs, we can use the Viterbi (1967) Algorithm3which simplyconsists of two steps:1. topological sort2.
visit each vertex in the topological ordering and do updatesThe pseudo-code of the Viterbi algorithm is presented in Algorithm 1.Algorithm 1 Viterbi Algorithm.1: procedure Viterbi(G, w, s)2: topologically sort the vertices of G3: Initialize(G, s)4: for each vertex v in topological order do5: for each edge e = (u, v) in BS (v) do6: d(v)?
= d(u) ?
w(e)The correctness of this algorithm (that d(v) = ?
(v) for all v after ex-ecution) can be easily proved by an induction on the topologically sortedsequence of vertices.
Basically, at the end of the outer-loop, d(v) is fixed tobe ?
(v).This algorithm is widely used in the literature and there have been somealternative implementions.Variant 1.
If we replace the backward-star BS (v) in line 5 by the forward-star FS (v) and modify the update accordingly, this procedure still works(see Algorithm 2 for pseudo-code).
We refer to this variant the forward-update version of Algorithm 1.4The correctness can be proved by a similarinduction (that at the beginning of the outer-loop, d(v) is fixed to be ?
(v)).Algorithm 2 Forward update version of Algorithm 1.1: procedure Viterbi-Forward(G, w, s)2: topologically sort the vertices of G3: Initialize(G, s)4: for each vertex v in topological order do5: for each edge e = (v, u) in FS (v) do6: d(u)?
= d(v) ?
w(e)3Also known as the Lawler (1976) algorithm in the theory community, but he considersit as part of the folklore.4This is not to be confused with the forward-backward algorithm (Baum, 1972).
Infact both forward and backward updates here are instances of the forward phase of aforward-backward algorithm.66Variant 2.
Another popular implemention is memoized recursion (Cormenet al, 2001), which starts from a target vertex t and invokes recursion onsub-problems in a top-down fashion.
Solved sub-problems are memoized toavoid duplicate calculation.The running time of the Viterbi algorithm, regardless of which imple-mention, is O(V + E) because each edge is visited exactly once.It is important to notice that this algorithm works for all semirings aslong as the graph is a DAG, although for non-total-order semirings thesemantics of ?
(v) is no longer ?best?
weight since there is no comparison.See Mohri (2002) for details.Example 1 (Counting).
Count the number of paths between the sourcevertex s and the target vertex t in a DAG.Solution Use the counting semiring (Table 2).Example 2 (Longest Path).
Compute the longest (worst cost) paths fromthe source vertex s in a DAG.Solution Use the semiring (R ?
{??
}, max, +,?
?, 0).Example 3 (HMM Tagging).
See Manning and Schu?tze (1999, Chap.
10).3.2 Dijkstra AlgorithmThe well-known Dijkstra (1959) algorithm can also be viewed as dynamicprogramming, since it is based on optimal substructure property, and alsoutilizes the overlapping of sub-problems.
Unlike Viterbi, this algorithm doesnot require the structural property of acyclicity; instead, it requires thealgebraic property of superiority of the semiring to ensure the correctness ofbest-first exploration.Algorithm 3 Dijkstra Algorithm.1: procedure Dijkstra(G, w, s)2: Initialize(G, s)3: Q ?
V [G]  prioritized by d-values4: while Q 6= ?
do5: v ?
Extract-Min(Q)6: for each edge e = (v, u) in FS (v) do7: d(u)?
= d(v) ?
w(e)8: Decrease-Key(Q, u)The time complexity of Dijkstra Algorithm is O((E + V ) log V ) with abinary heap, or O(E+V log V ) with a Fibonacci heap (Cormen et al, 2001).77Since Fibonacci heap has an excessively high constant overhead, it is rarelyused in real applications and we will focus on the more popular binary heapcase below.For problems that satisfy both acyclicity and superiority, which includemany applications in NLP such as HMM tagging, both Dijkstra and Viterbican apply (Nederhof, 2003).
So which one is better in this case?From the above analysis, the complexity O((V + E) log V ) of Dijkstralook inferior to Viterbi?s O(V +E) (due to the overhead for maintaining thepriority queue), but keep in mind that we can quit as long as the solutionfor the target vertex t is found, at which time we can ensure the currentsolution for the target vertex is already optimal.
So the real running time ofDijkstra depends on how early the target vertex is popped from the queue,or how good is the solution of the target vertex compared to those of othervertices, and whether this early termination is worthwhile with respect tothe priority queue overhead.
More formally, suppose the complete solutionis ranked rth among V vertices, and we prefer Dijkstra to be faster, i.e.,rV(V + E) log r < (V + E),then we haver log r < V (4)as the condition to favor Dijkstra to Viterbi.
However, in many real-worldapplications (especially AI search, NLP parsing, etc.
), often times the com-plete solution (a full parse tree, or a source-sink path) ranks very low amongall vertices (Eq.
4 does not hold), so normally the direct use of Dijkstra doesnot bring speed up as opposed to Viterbi.
To alleviate this problem, thereis a popular technique named A* (Hart et al, 1968) described below.3.2.1 A* Algorithm for State-Space SearchWe prioritize the queue using a combinationd(v) ?
?h(v)of the known cost d(v) from the source vertex, and an estimate?h(v) of the(future) cost from v to the target t:h(v) ={1 v = t??
?P (v,t)w(?)
v 6= t(5)where P (v, t) is the set of paths from v to t. In case where the estimate?h(v)is admissible, namely, no worse than the true future cost h(v),?h(v) ?
h(v) for all v,88we can prove that the optimality of d(t) when t is extracted still holds.
Ourhope is thatd(t) ?
?h(t) = d(t) ?
1 = d(t)ranks higher among d(v) ?
?h(v) and can be popped sooner.
The DijkstraAlgorithm is a special case of the A* Algorithm where?h(v) = 1 for all v.4 HypergraphsHypergraphs, as a generalization of graphs, have been extensively stud-ied since 1970s as a powerful tool for modeling many problems in DiscreteMathematics.
In this report, we use directed hypergraphs (Gallo et al, 1993)to abstract a hierarchically branching search space for dynamic program-ming, where we solve a big problem by dividing it into (more than one)sub-problems.
Classical examples of these problems include matrix-chainmultiplication, optimal polygon triangulation, and optimal binary searchtree (Cormen et al, 2001).Definition 12.
A (directed) hypergraph is a pair H = ?V, E?
with a setR, where V is the set of vertices, E is the set of hyperedges, and R is theset of weights.
Each hyperedge e ?
E is a triple e = ?T (e), h(e), fe?, whereh(e) ?
V is its head vertex and T (e) ?
V?is an ordered list of tail vertices.feis a weight function from R|T (e)|to R.Note that our definition differs slightly from the classical definitions ofGallo et al (1993) and Nielsen et al (2005) where the tails are sets ratherthan ordered lists.
In other words, we allow multiple occurrences of the samevertex in a tail and there is an ordering among the components.
We alsoallow the head vertex to appear in the tail creating a self-loop which is ruledout in (Nielsen et al, 2005).Definition 13.
We denote |e| = |T (e)| to be the arity of the hyperedge5.If |e| = 0, then fe() ?
R is a constant (feis a nullary function) and wecall h(e) a source vertex.
We define the arity of a hypergraph to be themaximum arity of its hyperedges.A hyperedge of arity one degenerates into an edge, and a hypergraph ofarity one is standard graph.Similar to the case of graphs, in many applications presented below,there is also a distinguished vertex t ?
V called target vertex.We can adapt the notions of backward- and forward-star to hypergraphs.5The arity of e is different from its cardinality defined in (Gallo et al, 1993; Nielsen etal., 2005) which is |T (e)| + 1.99Definition 14.
The backward-star BS (v) of a vertex v is the set of incominghyperedges {e ?
E | h(e) = v}.
The in-degree of v is |BS(v)|.
The forward-star FS (v) of a vertex v is the set of outgoing hyperedges {e ?
E | v ?
T (e)}.The out-degree of v is |FS(v)|.Definition 15.
The graph projection of a hypergraph H = ?V, E, t,R?
is adirected graph G = ?V, E??
whereE?= {(u, v) | ?e ?
BS(v), s.t.
u ?
T (e)}.A hypergraph H is acyclic if its graph projection G is acyclic; then a topo-logical ordering of H is an ordering of V that is a topological ordering in G.4.1 Weight Functions and SemiringsWe also extend the concepts of monotonicity and superiority from semiringsto hypergraphs.Definition 16.
A function f : Rm7?
R is monotonic with regarding to ,if for all i ?
1..m(ai a?i) ?
f(a1, ?
?
?
, ai, ?
?
?
, am)  f(a1, ?
?
?
, a?i, ?
?
?
, am).Definition 17.
A hypergraph H is monotonic if there is a total ordering on R such that every weight function f in H is monotonic with regardingto .
We can borrow the additive operator ?
from semiring to define acomparison operatora ?
b ={a a  b,b otherwise.In this paper we will assume this monotonicity, which corresponds tothe optimal substructure property in dynamic programming (Cormen et al,2001).Definition 18.
A function f : Rm7?
R is superior if the result of functionapplication is worse than each of its argument:?i ?
1..m, ai f(a1, ?
?
?
, ai, ?
?
?
, am).A hypergraph H is superior if every weight function f in H is superior.10104.2 DerivationsTo do optimization we need to extend the notion of paths in graphs to hy-pergraphs.
This is, however, not straightforward due to the assymmetry ofthe head and the tail in a hyperedge and there have been multiple propos-als in the literature.
Here we follow the recursive definition of derivationsin (Huang and Chiang, 2005).
See Section 6 for the alternative notion ofhyperpaths.Definition 19.
A derivation D of a vertex v in a hypergraph H, its size|D| and its weight w(D) are recursively defined as follows:?
If e ?
BS(v) with |e| = 0, then D = ?e, ?
is a derivation of v, its size|D| = 1, and its weight w(D) = fe().?
If e ?
BS(v) where |e| > 0 and Diis a derivation of Ti(e) for 1 ?i ?
|e|, then D = ?e, D1?
?
?D|e|?
is a derivation of v, its size |D| =1 +?|e|i=1|Di| and its weight w(D) = fe(w(D1), .
.
.
, w(D|e|)).The ordering on weights in R induces an ordering on derivations: D  D?iff w(D)  w(D?
).We denote D(v) to be the set of derivations of v and extend the bestweight in definition 11 to hypergraph:Definition 20.
The best weight ?
(v) of a vertex v is the weight of the bestderivation of v:?
(v) ={1 v is a source vertex?D?D(v)w(D) otherwise(6)4.3 Related FormalismsHypergraphs are closely related to other formalisms like AND/OR graphs,context-free grammars, and deductive systems (Shieber et al, 1995; Neder-hof, 2003).In an AND/OR graph, the OR-nodes correspond to vertices in a hy-pergraph and the AND-nodes, which links several OR-nodes to anotherOR-node, correspond to a hyperedge.
Similarly, in context-free grammars,nonterminals are vertices and productions are hyperedges; in deductive sys-tems, items are vertices and instantied deductions are hyperedges.
Table 3summarizes these correspondences.
Obviously one can construct a corre-sponding hypergraph for any given AND/OR graph, context-free grammar,or deductive system.
However, the hypergraph formulation provides greater1111hypergraph AND/OR graph context-free grammar deductive systemvertex OR-node symbol itemsource-vertex leaf OR-node terminal axiomtarget-vertex root OR-node start symbol goal itemhyperedge AND-node production instantiated deduction({u1, u2}, v, f) vf?
u1u2u1: a u2: bv : f(a, b)Table 3: Correspondence between hypergraphs and related formalisms.modeling flexibility than the weighted deductive systems of Nederhof (2003):in the former we can have a separate weight function for each hyperedge,where as in the latter, the weight function is defined for a deductive (tem-plate) rule which corresponds to many hyperedges.5 Dynamic Programming on HypergraphsSince hypergraphs with weight functions are generalizations of graphs withsemirings, we can extend the algorithms in Section 3 to the hypergraph case.5.1 Generalized Viterbi AlgorithmThe Viterbi Algorithm (Section 3.1) can be adapted to acyclic hypergraphsalmost without modification (see Algorithm 4 for pseudo-code).Algorithm 4 Generalized Viterbi Algorithm.1: procedure General-Viterbi(H)2: topologically sort the vertices of H3: Initialize(H)4: for each vertex v in topological order do5: for each hyperedge e in BS (v) do6: e is ({u1, u2, ?
?
?
, u|e|}, v, fe)7: d(v)?
= fe(d(u1), d(u2), ?
?
?
, d(u|e|))The correctness of this algorithm can be proved by a similar induction.Its time complexity is O(V + E) since every hyperedge is visited exactlyonce (assuming the arity of the hypergraph is a constant).The forward-update version of this algorithm, however, is not as trivialas the graph case.
This is because the tail of a hyperedge now containsseveral vertices and thus the forward- and backward-stars are no longersymmetric.
The naive adaption would end up visiting a hyperedge many1212times.
To ensure that a hyperedge e is fired only when all of its tail verticeshave been fixed to their best weights, we maintain a counter r[e] of theremaining vertices yet to be fixed (line 5) and fires the update rule for ewhen r[e] = 0 (line 9).
This method is also used in the Knuth algorithm(Section 5.2).Algorithm 5 Forward update version of Algorithm 4.1: procedure General-Viterbi-Forward(H)2: topologically sort the vertices of H3: Initialize(H)4: for each hyperedge e do5: r[e] ?
|e|  counter of remaining tails to be fixed6: for each vertex v in topological order do7: for each hyperedge e in FS (v) do8: r[e] ?
r[e] ?
19: if r[e] == 0 then  all tails have been fixed10: e is ({u1, u2, ?
?
?
, u|e|}, h(e), fe)11: d(h(e))?
= fe(d(u1), d(u2), ?
?
?
, d(u|e|))5.1.1 CKY AlgorithmThe most widely used algorithm for parsing in NLP, the CKY algorithm(Kasami, 1965), is a specific instance of the Viterbi algorithm for hyper-graphs.
The CKY algorithm takes a context-free grammar G in ChomskyNormal Form (CNF) and essentially intersects G with a DFA D representingthe input sentence to be parsed.
The resulting search space by this intersec-tion is an acyclic hypergraph whose vertices are items like (X, i, j) and whosehyperedges are instantiated deductive steps like ({(Y, i, k)(Z, k, j)}, (X, i, j), f)for all i < k < j if there is a production X ?
Y Z.
The weight function f issimplyf(a, b) = a ?
b ?
w(X ?
Y Z).The Chomsky Normal Form ensures acyclicity of the hypergraph butthere are multiple topological orderings which result in different variants ofthe CKY algorithm, e.g., bottom-up CKY, left-to-right CKY, and right-to-left CKY, etc.5.2 Knuth AlgorithmKnuth (1977) generalizes the Dijkstra algorithm to what he calls the gram-mar problem, which essentially corresponds to the search problem in a mono-tonic superior hypergraph (see Table 3).
However, he does not provide1313an efficient implementation nor analysis of complexity.
Graehl and Knight(2004) present an implementation that runs in time O(V log V + E) usingthe method described in Algorithm 5 to ensure that every hyperedge is vis-ited only once (assuming the priority queue is implemented as a Fibonaaciheap; for binary heap, it runs in O((V + E) log V )).Algorithm 6 Knuth Algorithm.1: procedure Knuth(H)2: Initialize(H)3: Q ?
V [H]  prioritized by d-values4: for each hyperedge e do5: r[e] ?
|e|6: while Q 6= ?
do7: v ?
Extract-Min(Q)8: for each edge e in FS (v) do9: e is ({u1, u2, ?
?
?
, u|e|}, h(e), fe)10: r[e] ?
r[e] ?
111: if r[e] == 0 then12: d(h(e))?
= fe(d(u1), d(u2), ?
?
?
, d(u|e|))13: Decrease-Key(Q, h(e))5.2.1 A* Algorithm on HypergraphsWe can also extend the A* idea to hypergraphs to speed up the KnuthAlgorithm.
A specific case of this algorithm is the A* parsing of Kleinand Manning (2003) where they achieve significant speed up using carefullydesigned heuristic functions.
More formally, we first need to extend theconcept of (exact) outside cost from Eq.
5:?
(v) ={1 v = t?D?D(v,t)w(D) v 6= t(7)where D(v, t) is the set of (partial) derivations using v as a leaf node.This outside cost can be computed from top-down following the inversetopological order: for each vertex v, for each incoming hyperedge e =({u1, .
.
.
, u|e|}, v, fe) ?
BS (v), we update?
(ui) ?
= fe(d(u1) .
.
.
d(ui?1), ?
(v), d(ui+1) .
.
.
d(u|e|)) for each i.Basically we replace d(ui) by ?
(v) for each i.
In case weight functions arecomposed of semiring operations, as in shortest paths (+) or probabilistic1414grammars (?
), this definition makes sense, but for general weight functionsthere should be some formal requirements to make the definition sound.However, this topic is beyond the scope of this paper.6 Extensions and DiscussionsIn most of the above we focus on optimization problems where one aimsto find the best solution.
Here we consider two extensions of this scheme:non-optimization problems where the goal is often to compute the summa-tion or closure, and k-best problems where one also searches for the 2nd,3rd, through kth-best solutions.
Both extensions have many applicationsin NLP.
For the former, algorithms based on the Inside semiring (Table 1),including the forward-backward algorithm (Baum, 1972) and Inside-Outsidealgorithm (Baker, 1979; Lari and Young, 1990) are widely used for unsu-pervised training with the EM algorithm (Dempster et al, 1977).
For thelatter, since NLP is often a pipeline of several modules, where the 1-bestsolution from one module might not be the best input for the next module,and one prefers to postpone disambiguation by propogating a k-best list ofcandidates (Collins, 2000; Gildea and Jurafsky, 2002; Charniak and John-son, 2005; Huang and Chiang, 2005).
The k-best list is also frequently usedin discriminative learning to approximate the whole set of candidates whichis usually exponentially large (Och, 2003; McDonald et al, 2005).6.1 Beyond Optimization ProblemsWe know that in optimization problems, the criteria for using dynamic pro-gramming is monotonicity (definitions 6 and 16).
But in non-optimizationproblems, since there is no comparison, this criteria is no longer applica-ble.
Then when can we apply dynamic programming to a non-optimizationproblem?Cormen et al (1990) develop a more general criteria of closed semir-ing where ?
is idempotent and infinite sums are well-defined and present amore sophisticated algorithm that can be proved to work for all closed semir-ings.
This definition is still not general enough since many non-optimizationsemirings including the Inside semiring are not even idempotent.
Mohri(2002) solves this problem by a slightly different definition of closednesswhich does not assume idempotence.
His generic single-source algorithmsubsumes many classical algorithms like Dijkstra, Bellman-Ford (Bellman,1958), and Viterbi as specific instances.It remains an open problem how to extend the closedness definition tothe case of weight functions in hypergraphs.15156.2 k-best ExtensionsThe straightforward extension from 1-best to k-best is to simply replace theold semiring (A,?,?, 0, 1) by its k-best version (Ak,?k,?k, 0k, 1k) whereeach element is now a vector of length k, with the ith component representthe ith-best value.
Since ?
is a comparison, we can define ?kto be thetop-k elements of the 2k elements from the two vectors, and ?kthe top-kelements of the k2elements from the cross-product of two vectors:a ?kb = ?
?k({ai| 1 ?
i ?
k} ?
{bj| 1 ?
j ?
k})a ?kb = ??k{ai?
bj| 1 ?
i, j ?
k}where ?
?kreturns the ordered list of the top-k elements in a set.
A similarconstruction is obvious for the weight functions in hypergraphs.Now we can re-use the 1-best Viterbi Algorithm to solve the k-bestproblem in a generic way, as is done in (Mohri, 2002).
However, some moresophisticated techniques that breaks the modularity of semirings results inmuch faster k-best algorithms.
For example, the Recursive Enumeration Al-gorithm (REA) (Jime?nez and Marzal, 1999) uses a lazy computation methodon top of the Viterbi algorithm to efficiently compute the ith-best solutionbased on the 1st, 2nd, ..., (i ?
1)th solutions.
A simple k-best Dijkstraalgorithm is described in (Mohri and Riley, 2002).For the hypergraph case, the REA algorithm has been adapted for k-bestderivations (Jime?nez and Marzal, 2000; Huang and Chiang, 2005).
Applica-tions of this algorithm include k-best parsing (McDonald et al, 2005; Mohriand Roark, 2006) and machine translation (Chiang, 2007).
It is also imple-mented as part of Dyna (Eisner et al, 2005), a generic langauge for dynamicprogramming.
The k-best extension of the Knuth Algorithm is studied byHuang (2005).
A separate problem, k-shortest hyperpaths, has been studiedby Nielsen et al (2005).Eppstein (2001) compiles an annotated bibliography for k-shortest-pathand other related k-best problems.7 ConclusionThis report surveys two frameworks for formalizing dynamic programmingand presents two important classes of DP algorithms under these frame-works.
We focused on 1-best optimization problems but also discussed otherscenarios like non-optimization problems and k-best solutions.
We believethat a better understanding of the theoretical foundations of DP is benefitialfor NLP researchers.1616ReferencesBaker, James K. 1979.
Trainable grammars for speech recognition.
In Proceedingsof the Spring Conference of the Acoustical Society of America, pages 547?550.Baum, L. E. 1972.
An inequality and associated maximization technique instatistical estimation of probabilistic functions of a markov process.Inequalities, (3).Bellman, Richard.
1957.
Dynamic Programming.
Princeton University Press.Bellman, Richard.
1958.
On a routing problem.
Quarterly of AppliedMathematics, (16).Charniak, Eugene and Mark Johnson.
2005.
Coarse-to-fine-grained n-best parsingand discriminative reranking.
In Proceedings of the 43rd ACL.Chiang, David.
2007.
Hierarchical phrase-based translation.
In ComputationalLinguistics.
To appear.Collins, Michael.
2000.
Discriminative reranking for natural language parsing.
InProceedings of ICML, pages 175?182.Cormen, Thomas H., Charles E. Leiserson, and Ronald L. Rivest.
1990.Introduction to Algorithms.
MIT Press, first edition.Cormen, Thomas H., Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.2001.
Introduction to Algorithms.
MIT Press, second edition.Dempster, A. P., N. M. Laird, and D. B. Rubin.
1977.
Maximum likelihood fromincomplete data via the EM algorithm.
Journal of the Royal Statistical Society,Series B, 39:1?38.Dijkstra, Edsger W. 1959.
A note on two problems in connexion with graphs.Numerische Mathematik, (1):267?271.Eisner, Jason, Eric Goldlust, and Noah A. Smith.
2005.
Compiling comp ling:Weighted dynamic programming and the dyna language.
In Proceedings ofHLT-EMNLP.Eppstein, David.
2001.
Bibliography on k shortest paths and other ?k bestsolutions?
problems.
http://www.ics.uci.edu/?eppstein/bibs/kpath.bib.Gallo, Giorgio, Giustino Longo, and Stefano Pallottino.
1993.
Directedhypergraphs and applications.
Discrete Applied Mathematics, 42(2):177?201.Gildea, Daniel and Daniel Jurafsky.
2002.
Automatic labeling of semantic roles.Computational Linguistics, 28(3):245?288.Graehl, Jonathan and Kevin Knight.
2004.
Training tree transducers.
InHLT-NAACL, pages 105?112.Hart, P. E., N. J. Nilsson, and B. Raphael.
1968.
A formal basis for the heuristicdetermination of minimum cost paths.
IEEE Transactions on Systems Scienceand Cybernetics, 4(2):100?107.Huang, Liang.
2005. k-best Knuth algorithm and k-best A* parsing.
Unpublishedmanuscript.Huang, Liang and David Chiang.
2005.
Better k-best Parsing.
In Proceedings ofthe Ninth International Workshop on Parsing Technologies (IWPT-2005).1717Jime?nez, V?
?ctor and Andre?s Marzal.
1999.
Computing the k shortest paths: Anew algorithm and an experimental comparison.
In Algorithm Engineering,pages 15?29.Jime?nez, V?
?ctor M. and Andre?s Marzal.
2000.
Computation of the n best parsetrees for weighted and stochastic context-free grammars.
In Proc.
of the JointIAPR International Workshops on Advances in Pattern Recognition.Kasami, T. 1965.
An efficient recognition and syntax analysis algorithm forcontext-free languages.
Technical Report AFCRL-65-758, Air Force CambridgeResearch Laboratory, Bedford, MA?.Klein, Dan and Chris Manning.
2003.
A* parsing: Fast exact Viterbi parseselection.
In Proceedings of HLT-NAACL.Knuth, Donald.
1977.
A generalization of Dijkstra?s algorithm.
InformationProcessing Letters, 6(1).Kuich, W. and A. Salomaa.
1986.
Semirings, Automata, Languages.
Number 5 inEATCS Monographs on Theoretical Computer Science.
Springer-Verlag, Berlin,Germany.Lari, K. and S. J.
Young.
1990.
The estimation of stochastic context-freegrammars using the inside-outside algorithm.
Computer Speech and Language,4:35?56.Lawler, E. L. 1976.
Combinatorial Optimization: Networks and Matroids.
Holt,Rinehart, and Winston.Manning, Chris and Hinrich Schu?tze.
1999.
Foundations of Statistical NaturalLanguage Processing.
MIT Press.McDonald, Ryan, Koby Crammer, and Fernando Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In Proceedings of the 43rd ACL.Mohri, Mehryar.
2002.
Semiring frameworks and algorithms for shortest-distanceproblems.
Journal of Automata, Languages and Combinatorics, 7(3):321?350.Mohri, Mehryar and Michael Riley.
2002.
An efficient algorithm for then-best-strings problem.
In Proceedings of the International Conference onSpoken Language Processing 2002 (ICSLP ?02), Denver, Colorado, September.Mohri, Mehryar and Brian Roark.
2006.
Probabilistic context-free grammarinduction based on structural zeros.
In Proceedings of HLT-NAACL.Nederhof, Mark-Jan. 2003.
Weighted deductive parsing and Knuth?s algorithm.29(1):135?143.Nielsen, Lars Relund, Kim Allan Andersen, and Daniele Pretolani.
2005.
Findingthe k shortest hyperpaths.
Computers and Operations Research.Och, Franz Joseph.
2003.
Minimum error rate training in statistical machinetranslation.
In Proceedings of ACL, pages 160?167.Shieber, Stuart, Yves Schabes, and Fernando Pereira.
1995.
Principles andimplementation of deductive parsing.
Journal of Logic Programming, 24:3?36.Viterbi, Andrew J.
1967.
Error bounds for convolutional codes and anasymptotically optimum decoding algorithm.
IEEE Transactions onInformation Theory, IT-13(2):260?269, April.1818
