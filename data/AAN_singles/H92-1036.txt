MAP Estimation of Continuous Density HMM : Theory and ApplicationsJean-Luc Gauvain t and Chin-Hui LeeSpeech  Research  Depar tmentAT&T Bel l  Laborator iesMurray  Hi l l ,  N J  07974ABSTRACTWe discuss maximum a posteriori estimation of continuous density hid-den Markov models (CDHMM).
The classical MLE reestimation algorithms,namely the forward-backward algorithm and the segmental k-means algo-rithm, are expanded and reestimation formulas are given for HMM withGaussian mixture observation densities.
Because of its adaptive nature,Bayesian learning serves as a unified approach for the following four speechrecognition applications, namely parameter smoothing, speaker adaptation,speaker group modeling and corrective ~aining.
New experimental resultson all four applications are provided to show the effectiveness of the MAPestimation approach.INTRODUCTIONEstimation of hidden Marknv model (HMM) is usually obtainedby the method of maximum likelihood (ML) \[1, 10, 6\] assumingthat the size of the training data is large enough to provide robustestimates.
This paper investigates maximum a posteriori (MAP)estimate of continuous density hidden Markov models (CDHMM).The MAP estimate can be seen as a Bayes estimate of the vector pa-rameter when the loss function is not specified \[2\].
This estimationtechnique provides away of incorporatimg prior information i  thetraining process, which is particularly useful to deal with problemsposed by sparse training data for which the ML approach givesinaccurate estimates.
This approach can be applied to two classesof estimation problems, namely, parameter smoothing and modeladaptation, both related to the problem of sparse training data.In the following the sample x = (zl, ...,z,~) is a given set ofn observations, where zl, ..., z n are either independent and identi-cally distributed (i.i.d.
), or are drawn from a probabilistic functionof a Markov chain.The difference between MAP and ML estimation lies in theassumption of an appropriate prior disliibution of the parameters tobe estimated.
If 0, assumed to be a random vector taking valuesin the space O, is the parameter vector to be estimated from thesample x with probability density function (p.d.f.)
f(.lO), and if gis the prior p.d.f, of 0, then the MAP estimate, 0~p,  is defined asthe mode of the posterior p.d.f, of 0, i.e.Oma, = argmoax f(xlO)g(O) (I)If 9 is assumed to be fixed but unknown, then there is no knowl-edge about 8, which is equivalent to assuming a non-informativeimproper prior, i,e.
g(8) ----constant.
Equation (1) then reduces tothe familiar ML formulation.Given the MAP formulation two problems remain: the choice ofthe prior distribution family and the evaluation of the maximum a~This work was done while Jean-Luc Gauvain was on leave from theSpeech Communication Group at LIMSI/CNRS, Orsay, France.posteriori.
These two problems are closely related, since the appro-pilate choice of the prior distribution can greatly simplify the MAPestimation.
Like for ML estimation, MAP estimation is relativelyeasy if the famay ofp.d.f.
's {f(-10), 0 ~ O} possesses a sufficientstatistic of fixed dimension t(x).
In this case, the natural solution isto choose the prior density in a conjugate family, {k(.ko), ~o E ~},which includes the kernel density of f( .
lO), i.e.
Vx  t(x) e ~b \[4, 2\].The MAP estimation is then reduced to the evaluation of the modeof k(Ol~o' ) = k(Oko)k(Olt(x)), a problem almost identical to theML  estimation problem.
However, among the families of interest,only exponential families have a sufficient statistic of fixed dimen-sion \[7\].
When there is no sufficient statistic of fixed dimension,MAP estimation, like ML  estimation, is a much more difficult prob-lem because the posterior density is not expressible in terms of afixed number of parameters and cannot be maximized easily.
Forboth finite mixture density and hidden Markov model, the lack of asufficient statistic of fixed dimension is due to the underlying hid-den process, i.e.
a multinomial model for the mixture and a Markovchain for an HMM.
In these cases ML  estimates are usually obtainedby using the expectation-maximization (EM) algorithm \[3, I, 13\].This algorithm exploits the fact that the complete-data likelihoodcan be simpler to maximize than the likelihood of the incompletedata, as in the case where the complete-data model has sufficientstatistics of fixed dimension.
As noted by Dempster et al \[3\],the EM algorithm can also be applied to MAP estimation.
In thenext two sections the formulations of this algorithm for MAP esti-mation of Gaussian mixture and CDHMM with Gaussian mixtureobservation densities are derived.MAP EST IMATES FOR GAUSSIAN MIXTURESuppose that x = ( z l , .
.
.
, x , )  is a sample of n i.i.d.observations drawn from a mixture of K p-dimensionalmultivariate normal densities.
The joint p.d.f, is speci-fied by f(x\[0) = \]-\[:=l ~-~f=t~kA/'(Zt\[mk,rk) where 0 =(wl, ..., wK, ml ,..., inK, rl,  ..., rK) is the parameter vector and ~kdenotes the mixture gain for the k-th mixture component with theK constraint ~kf t  Wk = 1.
A/'(Zlmk, rk) is the k-th normal densityfunction where mk is the p-dimensional mean vector and rk is thep ?
p precision matrix.
As stated in the introduction, for the parame-ter vector 0 no joint conjugate prior density exists.
However afinitemixture density can he interpreted as a density associated with a sta-tistical population which is a mixture of K component populationswith mixing proportions (wl .
.
.
.
, wK).
In other words, f(x\[0) canbe seen as a marginal p.d.f, of the product of a multinomial density(for the sizes of the component populations ) and normal densities(for the component densities).
A practical candidate to model the185prior knowledge about he mixture gain parameter vector is there-fore a Dirichlet density which is the conjugate prior density for themultinomial distributionKg(~,, ..., WK) OC H ~' -1  (2)k : lwhere vk > 0.
For the vector parameter (ink, rk) of the individualGaussian mixture component, the joint conjugate prior density is anormal-Wishart density \[2\] of the formg(mk, rk) oc I,'kl (? '
-") / :  exp\[--?tr(nkrk)\] xrk exp\[--T(mk -- Zk)%k(mk -- Zk)\] (3)where (rk,/zk, t~k, Uk) are the prior density parameters such thatak > p -- 1, rk > 0,/~k is a vector of dimension p and uk is a p ?
ppositive definite matrix.Assuming independence b tween the parameters of the mixturecomponents and the mixture weights, the joint prior density g(0)is taken to be a product of the prior p.d.f.
's defined in equations?
K ?
(2) and (3), Le.
g(0) = g(w~, .
.
.
,~K)FL: ,  z(m~,,- , ) .
As willbe shown later, this choice for the prior density family can also bejustified by noting that he EM algorithm can be applied to the MAPestimation problem if the prior density is in the conjuguate familyof the complete-data density.The EM algorithm is an iterative procedure for approximat-ing maximum-likelihood estimates in an incomplete-data contextsuch as mixture density and hidden Markov model estimationproblems \[1, 3, 13\].
This procedure consists of maximizing ateach iteration the auxilliary function Q(O, ~) defined as the ex-pectation of the complete-data log-likelihood log h(y\[0 ) giventhe incomplete data x = (~,  .
.
.
,x , )  and the current fit 0, i.e.Q(0, ~) = E\[log h(y l0) lx ,  ~.
For a mixture density, the complete-data likelihood is the joint likelihood of x and ?
= (?t, ..., ?n ) the un-observed labels referring to the mixture components, i.e.
y = (x, ?
).The EM procedure derives from the fact that log f (x l0  ) =Q(O, 0) - H(O, 0) where H(O, 0) = E(log h (y lx  , 0)Ix , 0) andH(O, 0) _< H(O, ~), and whenever a value 0 satisfies Q(O, O) >Q(0, 0) then f (x \ [0)  > f (x l0) .
It foUows that the same iterativeprocedure can be used to estimate the mode of the posterior densityby maximizing the anxilliary function R( O ,0) = Q( O ,0) + log 9(0)at each iteration instead of Q(O, 0) \[3\].For a mixture of K densities {f(.10~)}~=L...,g with mixtureweights {wk } k= ~,...,K, the auxilliary function Q takes the followingform \[13\]:Q(O, #)= ~ ~ ~"'~f(zt!O~)log~f(ztlo~) (4),=, ~=, /(z,lO)Let tP(0, 0) = exp R(O, 0) be the function to be maximized anddefine the following notations cat &~f(xtl#k) ck = ~=~ ckt,= f (z , l~)  '?k = ~=~ c~txt/ck and S~ = ~=~ c~t(xt - ?k)(Xt -- ?k) ~.
Itfollows from the definition of f(x\[O) and equation (4) thatgv(0 ,  ~) o~ z(0) IX '~;~ I~l?~/~xk=lCk exp\ [ -T (m~ - ~)%~(m~ - e~) - ?tr (s~) \ ]  (5)From (2), (3) and (5) it can easily be verified that~( .
,0)  belongs to the same family as g, and has parametersO,L ' ' ' ' rk,/~k, t~k, uk}k:l,...,K satisfying the following conditions:IVk : Vk+Ck (6)l rk = rk + ck (7)a~ = ak + ek (8)#~ _ rk#k + Ok.Ok (9)rk -+ckTkCk r a~ : uk -I- Sk + ~-~-~'tZk -- ?k)(#k -- ?k) T (I0)The considered family of distributions i therefore a conjugate fam-ily for the complete-data density.The mode of ~P(., 0), denoted J i , obtained (wk, ink, rk), may befrom the modes of the Dirichlet and normal-Wishart densities: w~ =(.~ - 1 ) /~c : t ( .~  _ 1), m~ = p~, and r~ = (~ - p)u~-'.Thus, the EM iteration is as follows:, Vk - -  1 + ~=!
Ok, (11)(M k =mk= (12), - ,  ,,~ + ~'k(#~ - mg) (~,~ - rag)  ~ +T k =~L,  ~k,(~, -- ~) (~,  -- rag) ~(13)If i t  is assumed &k > 0, then ckl, ck2, ...,ck, is a sequence ofn i.i.d, random variables with a non-degenerate distribution andl imsupn_o ~=.
ckt = co with probability one.
It follows thatw~ converges to ~=l  Ckt/n with probability one when n ~ oo.Applying the same reasoning to m~ and r~, it can be seen that theEM reestimation formulas for the MAP and ML approaches areasymptotically similar.
Thus as long as the initial estimates areidentical, the EM algorithm will provide identical estimates withprobability one when n ~ c?.MAP EST IMATES FOR CDHMMThe results obtained for a mixture of normal densities can beextended to the case of HMM with Gaussian mixture state ob-servation densities, assuming that the observation p.d.f.
's of allthe states have the same number of mixture components.
Weconsider an N-state HMM with parameter vector A = (x, A ,  0),where r is the initial probability vector, A is the transition ma-trix, and 0 is the p.d.f, parameter vector composed of the mixtureparameters 0i = {Wik,mik,rik}kfl,...,K for each state i.
For asample x = (2~1, .
.
.
,  zn ) ,  the complete data is y = (x, s ,Q  wheres = (so,..., s , )  is the unobserved state sequence, and l = (?h ..., l,~)are the unobserved mixture component labels, si E \[1, N\] andli E \[1, K\].
The joint p.d.f, h(.lX) of x, s, and?
is defined as \[1\]rlh(x,  s , l lA  ) = a'.
o Hao,_ , , ,w, , t , f (xt lO, , t , )  (14)t=|where 7ri is the initial probabilty of state i, ai j  is the transitionprobability from state i to state j, and Oik =(mik ,  rik) is theparameter vector of the k-th normal p.d.f, associated to state i. Itfollows that the likelihood of x has the form186n/(xl~,)= ~ ~,o ~I :.,_,., f(=,lO.,) (15)8 t : lwhere f(x,lOi ) K = ~k=t  w~kA/'(x*lralk, rik), and the summationis over all possible state sequences.In the general case where MAP estimation is to be applied notonly to the observation density parameters but also to the initialand transition probabilities, a Dirichlet density can also be used forthe initial probability vector ~r and for each row of the transitionprobability matrix A.
This choice directly follows the results ofthe previous ection: since the complete-data likelihood satisfiesh(x, s,tlA ) = h(s, A)h(x, t ls  , A) where h(s, A) is the productof N + 1 multinomial densities with parameters {n, a't, ..., ~N}and { n, air .
.
.
.
.
a i N } i f  l , .
.
.
,N  .
The prior density for all the HMMparameters is thusG(A) oc ~rT'-Ig(Oi) H a?
;J-I (16)i :1  j= l  JIn the following subsections we examine two ways of ap-proximating AMAp by local maximization of f(xl~)G(~) andf(x, sI~)G(A).
These two solutions are the MAP versions of theB aura-Welch algorithm \[1 \] and of the segmental k-means algorithm\[12\], algorithms which were developed for ML estimation.Forward-Backward  MAP Est imateFrom (14) it is straightforward to show that the auxilliaryfunction of the EM algorithm applied to MLE of A, Q(A, ~) =E\[log h(Yi~)lx, ?\], can be decomposed into a sum of three aux-illiary functions: Q,~(a', X), Q~(A,  X) and Qo(O, ~) \[6\].
Thesefunctions which can be independently maximized take the follow-ing forms:NQ'rOr' ~) = E ~io log ~ri (17)i=1QA(A, ~) = fist log aij (18)i=1 t= l  j=lNQo(o,?)
= ~ Qo,(od?)
(19)i--1with~ ~ ~ikf(zt\[~ik) Qo,(Oi,X)= 7" logoJikf(xtlOik) (20),=, k=t f(z,l@,)where ~i/t = Pr(st-t  = i ,  st = j lx ,  ~) and 3'.
=Pr(st =/Ix ,  ~) canbe computed at each EM iteration by using the Forward-Backwardalgorithm \[I\].
As for the mixture Gaussian case discussed in theprevious section, to estimate the mode of the posterior densitythe anxilliary function R(A, ~) = Q(A, ~) + log G(A) must bemaximized.
The form chosen for G(A) in (16) permits indepen-dent maximization of each of the following 2N + I parametersets: {Trl .... ,a'N}, {ail,...,aiN}i=t,...,g and {0i}i=l,...,N. TheMAP auxiUiary function R(A, A) can thus be written as the sumR.
( a', ~) + ~ i  R., ( a, , ~) + ~,  Ro, ( O,, ~ ), where ach term rep-resents the MAP anxilliary function associated with the indexedparameter set.We can recognize in (20) the same form as seen for Q(0\[~) in (4)for the mixture Ganssian case.
It follows that if the Ckt are replacedby the cikt defined as,~,kX(xtl,h,~, ,k ) (21)eikt = 7,t f (xt \ [~i)then the reestimation formulas (11-13) can be used to maximizeRo~ (01, ~).
It is straightforward to find the reesfimations formulasfor ~r and A by applying the same derivations used for the mixtureweights:, t/i - -  1 + 7m ~ri = (22), .q  - 1 + ~=,  6 , .
(23)aq = EjN= 1 "'J _ N -F Eju_t E~:a  ~q'For multiple independent observation sequences { xo } q= l,...,Q,t~(q) ~(q)~ with Xq = x't  .... , ~., ,, we maximize G(A) lq?
:l f(xqlA)'where f(.\[A) is defined by (15).
The EM auxilliary function isthen R(A, X) = logG(A) + ~qQ=t E\[l?gh(Yql~)lxq, X\], whereh(.lA) is defined by equation (14).
It follows that the reestima-tion formulas for A and 0 still hold if the summations over t are~(q) and - (q) replaced by summations over q and t. The values ",~jt 7.are then obtained by applying the forward-backward algorithm foreach observation sequence.
The reestimation formula for the initialprobabilities becomes, T/, - 1 + Eq%l,~, = (24) N Q (q) E i : ,  ' ,  - Iv + E .
: ,  ,,oAs for the mixture Gaussian case, it can be shown that as Q ~ co,the MAP reestimation formulas approach the ML ones, exhibitingthe asymptotic similarity of the two estimates.These reestimation equations give estimates ofthe HMM param-eters which correspond toa local maximum of the posterior density.The choice of the initial estimates i therefore ssential to findinga solution close to a global maximum and to minimize the numberof EM iterations needed to attain the local maximum.
When usingan informative prior, one natural choice for the initial estimates ithe mode of the prior density, which represents all the availableinformation about he parameters when no data has been observed.The corresponding values are simply obtained by applying the rees-timation formulas with n equal to 0.
When using a non-informativeprior, i.e.
for ML estimation, while for discrete HMMs it is possibleto use uniform initial estimates, there is no trivial solution for thecontinuous density case.Segmental  MAP Est imateBy analogy with the segmental k-means algorithm \[12\], a differ-ent optimization criterion can be considered.
Instead of maximizingG(AIx), the joint posterior density of A and s, G(A, s lx ), is maxi-mized.
The estimation procedure becomes= argmax max G(A, s ix ) (25)), s= argm~x m~x f (x ,  s\[A)G(A) (26)and A is called the segmental MAP estimate of A.
As for thesegmental k-means algorithm, it is straightforward to prove thatstarting with any estimate A (m), alternate maximization over s and187A gives a sequence of estimates with non decreasing values ofG(A, slx), i.e.
G(A (m+'), s(m+')\]x) > G(A(m), s(m)lx) withs (m) ---- argm~x f (x ,  slA (m)) (27)A (rn+l) = argmxax f (x ,  s(m)IA)G(A) (28)The most likely state sequence s (m) is decoded by the Viterbialgorithm.
In fact, maximization over A can be replaced byany hill climbing procedure which replaces A ('~) by A ('~+1)subject o the constraint that f (x ,  s(m)\[A(m+D)G(A (re+D) _>f (x ,  s (m) \[A (m))G(A(m)).
The EM algorithm is once again a goodcandidate to perform this maximization using A (m) as an initial es-timate.
The EM anxilliary function is then R(A, ~) = log G(A) +E\[log h(ylA)lx, s ~), X\] where h(.IA) is defined by equation (14).It is straightforward to show that the forward-backward reestima-tion equations still hold with f i j t=  6ts('n)~ t - t  - i)6(s~ m) - J) and"fit = ~(s~ '~) -- i), where ~ denotes the Kronecker delta function.PRIOR DENSITY ESTIMATIONIn the previous ections it was assumed that the prior densityG(A) is a member of a preassigned family of prior distributions de-fined by (16).
In a strictly Bayesian approach the vector parameterof this family ofp.d.f.
's {G(.\[~), ~ E ~b} is also assumed knownbased on common or subjective knowledge about he stochastic pro-cess.
Another solution is to adopt an empirical Bayesian approach\[14\] where the prior parameters are estimated directly from data.The estimation is then based on the marginal disttrbution ofthe datagiven the prior parameters.Adopting the empirical Bayes approach, it is assumed that thesequence of observations, X, is composed of multiple independentsequences associated with different unknown values of the HMMparameters.
Letting (X,A)  = \[(xt, Ai), (x2, A2) .... \] be such amultiple sequence of observations, where each pair is independentof the others and the Aq have a common prior distribution G(.\[~).Since the Aq are not directly observed, the prior parameter stimatesmust be obtained from the marginal density f (X\ [~) ,f(Xl~) -- ~ f(XIA)G(A\[~) dA (29)where f (X IA  ) = I~Iq f(xqlAq) and G(AIg~ ) = I~q G(AqI~)"However, maximum likelihood estimation based on f (X l~ ) ap-pears rather difficult.
To simplify this problem, we can choose asim-pler optimization criterion by maximizing the joint p.d.f, f (X ,  A I~)over A and ~ instead of the marginal p.d.f, of X given ~.
Startingwith an initial estimate of ~o, we obtain a hill climbing procedure byalternate maximization over A and ~o, i.e.A (m) = argmAax f (X ,  AIr <m)) (30)(m+D = argmaxG(A(m)\[~) (31)~pSuch a procedure provides a sequence of estimates with non-decreasing values of f(X, Al~(m)).
The solution of (30) is theMAP estimate of A based on the current prior parameter ~(m).
Itcan therefore be obtained by applying the forward-backward MAPreestimation formulas to each observation sequence Xq.
The solu-tion of (31) is simply the maximum likelihood estimate of ~ basedon the current values of the HMM parameters.Finding this estimate poses two problems.
First, due to theWishart and Dirichlet components, ML estimation for the densitydefined by (16) is not trivial.
Second, since more parameters areneeded for the prior density than for the HMM itself, there canbe a problem of overparametrization when the number of pairs(xq, Aq) is small.
One way to simplify the estimation problem isto use moment estimates to approximate he ML estimates.
Forthe overparametrization problem, it is possible to reduce the size ofthe prior family by adding constraints on the prior parameters.
Forexample, the prior family can be limited to the family of the kerneldensity of the complete-data likelihood, i.e.
the posterior densityfamily of the complete.data model when no prior information isavailable.
Doing so, it can be verified that he following constraintsholdv~k = r~k (32)aik = r ik-t-p (33)Parameter tying can also be used to further educe the size of theprior family.We use this approach for approach for two types of applica-tions: parameter smoothing and adaptation learning.
For parameter"smoothing", the goal is to estimate {Al, A2, ...}.
The previousalgorithm offers a direct solution to "smooth" these different esti-mates by assuming a common prior density for all the models.
Foradaptative l arning, we observe anew sequence of observations Xqassociated with the unobserved vector parameter value Aq.
TheMAP estimate of A, can be obtained by using for prior parametersa point estimate ~ obtained with the previous algorithm.
Such atraining process can be seen as an adaptation of an a priori model= argmaxx G(A\[~) (when no training data is available) to morespecific onditions corresponding to the new observation sequenceXq.In the applications presented in this paper, the prior density pa-rameters were estimated along with the estimation of the SI modelparameters u ing the segmental k-means algorithm.
Informationabout he variability to be modeled with the prior densities was as-sociated with each frame of the SI training data.
This informationwas simply represented bya class number which can be the speakerID, the speaker sex, or the phonetic ontext.
The HMM parametersfor each class given the mixture component were then computed,and moment estimates were obtained for the tied prior parametersalso subject to conditions (32-33) \[5\].EXPERIMENTAL SETUPThe experiments presented in this paper used various sets ofcontext-independent (CI) and context-dependent (CD) phone mod-els.
Each model is a left-to-right HMM with Gaussian mixturestate observation densities.
Diagonal covariance matrices are usedand the transition probabilities are assumed fixed and known.
Asdescribed in \[8\], a 3g-dimensional feature vector composed ofLPC-derived cepstrum coefficients, and first and second order timederivatives.
Results are reported for the RM task with the standardword pair grammar and for the TI/NIST connected igits.
Bothcorpora were down-sampled to telephone bandwidth.MODEL SMOOTHING AND ADAPTAT IONLast year we reported results for CD model smoothing, speakeradaptation, and sex-dependentmodeling \[5\].CD model smoothingwas found to reduce the word error ate by 10%.
Speaker adaptation188Training 0 mitt 2 rain 5 rain 30 rainSD - -  31.5 12.1 3.5SA (SI) 13.9 8.7 6.9 3.4SA (M/F) 11.5 7.5 6.0 3.5Table 1: Summary of SD, SA (SI), and SA (M/F) results on FEB91-SDtest.
Results are given as word error rate (%).was tested on the JUN90 data with 1 minute and 2 minutes ofspeaker-specific adaptation data.
A 16% and 31% reduction in worderror were obtained compared to the SI results \[5\].
On the FEB91test, using Bayesian learning for CD model smoothing combinedwith sex-dependent modeling, a 21% word error reduction wasobtained compared to the baseline results \[5\].In order to compare speaker adaption to ML training of SDmodels, an experiment has been carded out on the FEB91-SD testmaterial including data from 12 speakers (7m/5f), using a set of 47CI phone models.
Two, five and thirty minutes of the SD trainingdata were used for training and adaptation.
The SD, SA (SI) worderror rates are given in the two first rows of Table 1.The SD word error rate for 2 min of training data was 31.5%.The SI word error rate (0 minutes of adaptation data) was 13.9%,somewhat comparable to the SD results with 5 min of SD trainingdata.
The SA models are seen to perform better than SD modelswhen relatively small amounts of data were used for training oradaptation.
When all the available training data was used, theSA and SD results were comparable, consistent with the Bayesianformulation that the MAP estimate converges to the MLE.
Relativeto the SI results, the word error reduction was 37% with 2 rainof adaptation data, an improvement similar to that observed on theJUN90 test data with CD models \[5\].
As in the previous experiment,a larger improvement was observed for the female speakers (51%)than for the male speakers (22%).Speaker adaptation was also performed starting with sex-dependent models (third row of Table 1).
The word error rate withno speaker adaptation is 11.5%.
The error rate is reduced to 7.5%with 2 rain, and 6.0% with 5 rain, of adaptation data.
Comparingthe last 2 rows of the table it can be seen that SA is more effectivewhen sex-dependent seed models are used.
The error reductionwith 2 rain of training data is 35% compared to the sex-dependentmodel results and 46% compared to the SI model results.P .D.F .
SMOOTHINGWe have shown that Bayesian learning can be used for CD modelsmoothing \[5\].
This approach can be seen either as a way to addextra constraints to the model parameters so as to reduce the effectof insufficient training data, or it can be seen as an "interpolation"between two sets of parameter estimates: one corresponding tothe desired model and the other to a smaller model which canbe trained using MLE on the same data.
Instead of defining areduced parameter set by removing the context dependency, we canalternatively reduce the mixture size of the observation densitiesand use a single Ganssian per state in the smaller model.
Cast in theBayesian learning framework, this implies that the same marginalprior density is used for all the components of a given mixture.Variance clipping can also be viewed as a MAP estimation techniquewith a uniform prior density constrained by a maximum (positive)value for the precision parameters \[9\].
However, this does not havethe appealing interpolation capability of the conjugate priors.We experimented with this p.d.f, smoothing approach on the TIWACC SACC (Strings Correct)MLE 99.6 98.7 (8464)MLE+VC 99.6 98.8 (8477)MAP 99.7 99.1 (8502)Table 2: TI test results for p.d.t smoothing (213 inter-word CD-32 models)FEB89 OCT89 JUN90 FEB91MLE 93.3 92.5 92.1 92.9MLE+VC 95.0 95.0 94.8 95.9MAP(SI) 95.0 95.5 95.0 96.2MAP(M/F) 95.2 96.2 95.2 96.7Table 3: RM test results for p.d.f, smoothing (2421 inter-word CD-16modelsdigit and RM databases.
A set of 213 CD phone models with 32mixture components (213 CD-32) for the TI digits and a set of 2421CD phone models with 16 mixture components (2421 CD-16) forRM were used for evaluation.
Results are given for MLE training,MLE with variance clipping (MLE+VC), and MAP estimation withp.d.f, smoothing in Tables 2 and 3.
In Table 2, word accuracy(WACC) and suing accuracy (SACC) are given for the 8578 testdigit strings of the TI digit corpora.
Compared to the varianceclipping scheme, the MAP estimate reduces the number of stringerrors by 25%.
Using p.d.f, smoothing, the suing accuracy of99.1%is the best result reported on this task.For the RM tests summarized in Table 3, a consistent improve-ment over the variance clipping scheme (MLE+VC) is observedwhen p.d.f, smoothing is applied.
Combined with sex-dependentmodeling, the MAP(M/F) scheme gives an average word accuracyof about 95.8%.CORRECTIVE  TRAIN INGBayesian learning provides ascheme for model adaptation whichcan also be used for corrective training.
Corrective training maxi-mizes the recognition rate on the training data hoping that that willalso improve performance on the test data.
One simple way todo corrective training is to use the training sentences which wereincorrectly recognized as new data.
In order to do so, the statesegmentation step of the segmental MAP algorithm was modifiedto obtain not only the frame/state association for the sentence modelstates but also for the states corresponding to the model of all thepossible sentences (general model).
In the reestimation formulas,the values cikt for each state si are evaluated using (21), such that7it is equal to 1 in the sentence model and to -1 in the generalmodel.
While convergence is not guaranteed, in practice it wasfound that by using large values for r ik(_  ~ 200), the number oftraining sentence rrors decreased after each iteration until conver-gence.
If we use the forward-backward MAP algorithm we obtaina corrective training algorithm for CDHMM's very similar to therecently proposed corrective MMIE training algorithm \[11 \].Corrective training was evaluated on both the TI/NIST SI con-nected digit and the RM tasks.
Only the Ganssian mean vectorsand the mixture weights were corrected.
For the TI digits a set of21 phonetic HMMs were ~ained on the 8565 digit strings.
Resultsare given in Table 4 using 16 and 32 mixture components for theobservation p.d.L's, with and without corrective training for bothtest and training data.
The CT-16 results were obtained with 8 iter-189Training.
'onditionsMLE-16CT-16MLE-32CT-32Training Teststring word string word1.6 (134) 0.5 2.0 (168) 0.70.2 (18) 0.1 1.4 (122) 0.50.8 (67) 0.2 1.5 (126) 0.50.3 (29) 0.1 1.3 (111) 0.4Table 4: Corrective training results in siring and word error rates (%) onthe TI-digits for 21 CI models with 16 and 32 mixture components per stale.String error counts are given in parenthesis.Test Set MLE-32TRAIN 7.7FEB89 11.9OCT89 11.5JUN90 10.2FEB91 11.4FEB91-SD 13.9Overall Test 11.8CT-32 ICT-321.8 3.110.2 8.99.8 8.98.8 8.110.3 10.211.3 11.010.1 9.4Table S: Corrective aining results on the RM task (47 CI models with 32mixture components per state)ations of corrective training while the CT-32 results were based ononly 3 iterations, where one full iteration of conective training isimplemented as one recognition run which produces aset of "new"training strings (i.e.
errors and/or barely correct strings) followed by10 iterations of Bayesian adaptation using the data of these strings.String error rates of 1.4% and 1.3% were obtained with 16 and 32mixture components per state respectively, compared to 2.0% and1.5% without corrective training.
These represent suing error re-ductions of 27% and 12%.
We note that corrective training helpsmore with smaller models, as the ratio of adaptation data to thenumber of parameters i  larger.The corrective training procedure is also effective for continuoussentence recognition of the RM task.
Table 5 gives results for theRM task, using 47 SI-CI models with 32 mixture components.
TheCT-32 corrective training assumes a fixed beam width.
Since thenumber of string errors was small in the training set, the amountof data for corrective training was rather limited.
To increase theamount, a smaller beam width was used to recognize the trainingdata.
It was observed that this improved corrective training (ICT-32) procedure not only reduced the error rate in training but alsoincreased the separation between the conect string and the othercompeting strings.
The number of training errors also increased aspredicted.
The regular and the improved corrective training gavean average word error rate reduction of 15% and 20% respectivelyon the test data.SUMMARYThe theoretical framework for MAP estimation of multivariateGaussian mixt~e density and HMM with mixture Gaussian stateobservation densities was presented.
Two MAP training algorithms,the forward-baclovard MAP estimation and the segmental MAP es-timation, were formulated.
Bayesian learning serves as a unifiedapproach for speaker adaptation, speaker group modeling, parame-ter smoothing and corrective training.Tested on the RM task, encouraging results have been obtainedfor all four applications.
For speaker adaptation, a 37% word er-ror reduction over the SI results was obtained on the FEB91-SDtest with 2 minutes of speaker-specific training data.
It was alsofound that speaker adaptation is more effective when based onsex-dependent models than with an SI seed.
Compared to speaker-dependent training, speaker adaptation achieved a better perfor-mance with the same amount of training/adaptation data.
Correc-tive training appfied to CI models reduced word errors by 15-20%.The best SI results on RM tests were obtained with p.d.L smoothingand sex-dependent modeling, an average word accuracy of about95.8% on four test sets.Only corrective training and p.d.L smoothing were applied to theTI/NIST connected igit task.
It was found that corrective trainingis effective.for improving CI models, reducing the number of stringerrors by up to 27%.
Corrective training was found to be moreeffective for models having smaller numbers of parameters.
Thisimplies that we can reduce computational requierements by usingcorrective training on a smaller model and achieve performancecomparable to that of a larger model.
Using 213 CD models,p.d.L smoothing provided a robust model that gave a 99.1% stringaccuracy on the test data, the best performance reported on thiscorpus.REFERENCES\[1\] L. E. Baum, "An inequality and associated maximization techniquein statistical estimation for probabilisties functions of Markov pro-cesses," Inequalities, vol.
3, pp.
1-8, 1972.\[2\] M. DeGroot, Optimal StatisticalDecisions, McGraw-Hill, 1970.\[3\] A. Dempster, N. Laird, D. Rubin, "Maximum Likelihood from Incom-plete Data via the EM algorithm", ./.
Roy.
Statist.
Soc.
Set.
B, 39, pp.1-38, 1977.\[4\] R.O.
Duda and P. E. Hart, Pattern Classification and Scene Analysis,John Wiley & Sons, New York, 1973.\[5\] J.-L. Ganvain and C.-H. Lee, "Bayesian Learning of Ganssian MixtureDensities for Hidden Markov Models," Prec.
DARPA Speech andNatural Language Workshop, Pacific Grove, Feb. 1991.\[6\] B. H. Juang, "Maximum-Likelihood Estimation for Mixture Multi-variate Stochastic Observations of Marker Chains", AT&T TechnicalJournal, Vol.
64, No.
6, July-August 1985.\[7\] B. O. Koopman, "On distributions admitting a sufficient statistic",Trans.
Ar,~ Math.
See., vol.
39, pp.
399-409, 1936.\[8\] C.-H. Lee, E. Giachin, L. R. Rabiner, R. I'ieraccini and A. E. Rosen-berg, "Improved Acoustic Modeling for Continuous Speech Recogni-tion", Prec.
DARPA Speech and Natural _zmguage Workshop, HiddenValley, June 1990.\[9\] C.-H. Lee, C.-H. Lin and B.-H. Juang, "A Study on Speaker Adaptationof the Parameters of Continuous Density Hidden Markov Models",IEEE Trans.
on ASSP, April 1991.\[10\] L. R. Liporace, "Maximum Likelihood Estimation for MultivariateObservations of Markov Sources," IEEE Trans.
lnforr~ Theory, Vol.IT-28, no.
5, pp.
729-734, September 1982.\[l \] Y. Normandin and D. Morgera, "An Improved MMIE Training Algo-rithm for Speaker-Independent Small Vocabulary, Continuous SpeechRecognition", Prec.
ICASSPgl, pp.
537-540, May 1991.\[12\] L.R.
Rabiner, J. G. Wilpon, and B. H. Juang, "A segmental K-meanstraining procedure for connected word recognition," AT&T Tech.
Y.,voL 64, no.
3, pp.
21-40, May 1986.\[13\] R.A. Redner and H. E Walker, "Mixture Densities, Maximum Like-lihood and the EM Algorithm," SIAM Review, Vol.
26, No.
2, pp.195-239, April 1984.\[14\] H. Robbins, "The Empirical Bayes Approach to Statistical DecisionProblems," Ann.
Math.
Statist., Vol.
35, pp.
1-20, 1964.190
