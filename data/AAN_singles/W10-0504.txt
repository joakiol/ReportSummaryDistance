Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 7?8,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAn Information-Retrieval Approach to Language Modeling:Applications to Social DataJuan M. HuertaIBM T. J.Watson Research Center1101 Kitchawan RoadYorktown Heights, NY 10598, USAhuerta@us.ibm.comAbstractIn  this  paper  we  propose  the  IR-LM(Information  Retrieval  Language  Model)which is an approach to carrying out languagemodeling  based  on  large  volumes  ofconstantly  changing  data  as  is  the  case  ofsocial  media  data.
Our  approach  addressesspecific  characteristics  of  social  data:   largevolume of constantly generated content as wellas  the  need  to  frequently  integrating  andremoving data from the model.1 IntroductionWe  describe  the  Information  RetrievalLanguage  Model  (IR-LM)  which  is  a  novelapproach  to  language  modeling  motivated  bydomains with constantly changing large volumesof  linguistic  data.
Our  approach  is  based  oninformation  retrieval  methods  and  constitutes  adeparture  from  the  traditional  statistical  n-gramlanguage modeling (SLM) approach.
We believethe IR-LM is more adequate than SLM when: (a)language models  need  to  be  updated  constantly,(b) very large volumes of data are constantly beinggenerated and (c) it is possible and likely that thesentence we are trying to score has been observedin the data (albeit with small possible variations).These three characteristics  are inherent  of  socialdomains such as blogging and micro-blogging.2 N-gram SLM and IR-LMStatistical  language  models  are  widely  used  inmain computational  linguistics  tasks  to  computethe probability of a string of words: )...( 1 iwwpTo  facilitate  its  computation,  this  probability  isexpressed as:)...|(...)|()()...( 111211 ?????
iii wwwPwwPwPwwpAssuming  that  only  the  most  immediate  wordhistory affects the probability of any given word,and focusing on a trigram language model:)|()...|( 1211 ???
?
iiiii wwwPwwwPThis leads to:????
?ikkkki wwwpwwP..1211 )|()...(Language  models  are  typically  applied  in  ASR,MT and other tasks in which multiple hypothesesneed to be rescored according to their  likelihood(i.e.,  ranked).
In a  smoothed backoff SLM (e.g.,Goodman (2001)), all the n-grams up to order n arecomputed and smoothed and backoff probabilitiesare  calculated.
If  new  data  is  introduced  orremoved from the  corpus,  the  whole  model,  thecounts and weights would need to be recalculated.Levenberg  and  Osborne  (2009)  proposed  anapproach for incorporating new data as it is seen inthe stream.
Language models have been used tosupport  IR  as  a  method  to  extend  queries(Lavrenko  et al 2001); in this paper we focus onusing IR to carry out  language modeling.2.1 The IR Language ModelThe IR-LM approach consists of two steps: thefirst is the identification of a set of matches from acorpus given a query sentence, and second is theestimation of a likelihood-like value for the query.In the first step, given a corpus  C and a querysentence  S, we  identify  the  k-closest  matchingsentences  in  the  corpus  through  an  informationretrieval  approach.
We  propose  the  use  of  amodified String Edit Distance as score in the IRprocess.
To efficiently carry out the search of theclosest sentences in the corpus we propose the useof  an  inverted  index  with  word  position7information  and  a  stack  based  search  approachdescribed in Huerta (2010).
A modification of theSED  allows  queries  to  match  portions  of  longsentences  (considering  local  insertion  deletionsand substitutions) without penalizing for  missingthe non-local portion of the matching sentence.In the second step, in general, we would like tocompute  a  likelihood-like  value  of  S through  afunction  of  the  distances  (or  alternatively,similarity  scores)  of  the  query  S to  the  top  k-hypotheses.
However, for now we will focus onthe  more particular  problem of  ranking multiplesentences  in  order  of  matching  scores,  which,while not directly producing likelihood estimates itwill  allow  us  to  implement  n-best  rescoring.Specifically, our ranking is based on the level ofmatching between each sentence to be ranked andits best matching hypothesis in the corpus.
In thiscase,  integrating  and  removing  data  from  themodel  simply  involve  adding  to  or  pruning  theindex which generally are simpler than n-gram re-estimation.There  is  an  important  fundamental  differencebetween the classic n-gram SLM approach and ourapproach.
The  n-gram  approach  says  that  asentence S1 is more likely than another sentence S2given a  model  if  the  n-grams that  constitute  S1have been observed more times than the n-gramsof S2.
Our approach, on the other hand, says that asentence  S1 is  more  likely than  S2 if  the  closestmatch to S1 in C resembles S1 better than the closesmatch of  S2  resembles  S2 regardless of how manytimes these sentences have been observed.3 ExperimentsWe  carried  out  experiments  using  the  blogcorpus provided by Spinn3r (Burton et al(2009)).It consists of 44 million blog posts that originatedduring August and September 2008 from which weselected,  cleaned,  normalized  and  segmented  2million English language blogs.
We reserved thesegments originating from blogs dated September30 for testing.We took 1000 segments from the test subset andfor  each  of  these  segments  we  built  a  16-hypothesis cohort (by creating 16 overlapping sub-segments of the constant length from the segment).We  built  a  5-gram  SLM  using  a  20k  worddictionary and Knesser-Ney smoothing using theSRILM toolkit (Stolcke (2002)).
We then rankedeach of  the  1000 test  cohorts  using each  of  themodel's n-gram  levels (unigram, bigram, etc.).
Ourgoal is to determine to what extent our approachcorrelates with an n-gram SLM-based rescoring.For testing purposes we re-ranked each of  thetest  cohorts using the IR-LM approach.
We thencompared the rankings produced by n-grams andby IR-LM for every n-gram order and several IRconfigurations.
For  this,  we  computed  theSpearman  rank  correlation  coefficient  (SRCC).SRCC averages for each configuration are shownin table 1.
Row 1 shows the SRCC for the  bestoverall  IR  configuration  and  row  2  shows  theSRCC for the IR configuration producing the bestresults for each particular n-gram model.
We cansee that albeit simple, IR-LM can produce resultsconsistent  with  a  language  model  based  onfundamentally different assumptions.n=1 n=2 n=3 n=4 n=5overall 0.53 0.42 0.40 0.40 0.38individual 0.68 0.47 0.40 0.40 0.39Table 1.
Spearman rank correlation coefficient forseveral n-gram IR configurations4 ConclusionThe IR-LM can be beneficial when the languagemodel  needs  to  be  updated  with  added  andremoved  data.
This  is  particularly  important  insocial  data  where  new  content  is  constantlygenerated.
Our  approach  also  introduces  adifferent interpretation of the concept of likelihoodof a sentence: instead of assuming the frequentistassumption underlying n-gram models, it is basedon  sentence  feasibility  based  on  the  closestsegment  similarity.
Future  work  will  look  into:integrating  information  from the  top  k-matches,likelihood regression, as well as leveraging otherapproaches to information retrieval.ReferencesBurton K., Java A., and Soboroff I.
(2009) The ICWSM2009 Spinn3r Dataset.
Proc.
ICWSM 2009Goodman  J.
(2001)  A  Bit  of  Progress  in  LanguageModeling, MS Res.
Tech.
Rpt.
MSR-TR-2001-72.Huerta  J.
(2010)  A  Stack  Decoder  Approach  toApproximate String Matching, Proc.
of  SIGIR 2010Lavrenko V. and Croft W. B.
(2001) Relevance basedlanguage models.
Proc.
of SIGIR 2001Levenberg  A.  and  Osborne  M.  (2009),  Stream-basedRandomised Lang.
Models for SMT, EMNLP 2009Stolcke A.
(2002)s SRILM -- An Extensible LanguageModeling Toolkit.
Proc.
ICSLP 20028
