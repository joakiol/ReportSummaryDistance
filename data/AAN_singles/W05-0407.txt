Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 48?56,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsEngineering of Syntactic Features for Shallow Semantic ParsingAlessandro Moschitti??
DISP - University of Rome ?Tor Vergata?, Rome, Italy{moschitti, pighin, basili}@info.uniroma2.it?
ITC-Irst, ?
DIT - University of Trento, Povo-Trento, Italycoppolab@itc.itBonaventura Coppola??
Daniele Pighin?
Roberto Basili?AbstractRecent natural language learning researchhas shown that structural kernels can beeffectively used to induce accurate modelsof linguistic phenomena.In this paper, we show that the above prop-erties hold on a novel task related to predi-cate argument classification.
A tree kernelfor selecting the subtrees which encodesargument structures is applied.
Experi-ments with Support Vector Machines onlarge data sets (i.e.
the PropBank collec-tion) show that such kernel improves therecognition of argument boundaries.1 IntroductionThe design of features for natural language process-ing tasks is, in general, a critical problem.
The inher-ent complexity of linguistic phenomena, often char-acterized by structured data, makes difficult to findeffective linear feature representations for the targetlearning models.In many cases, the traditional feature selectiontechniques (Kohavi and Sommerfield, 1995) are notso useful since the critical problem relates to featuregeneration rather than selection.
For example, thedesign of features for a natural language syntacticparse-tree re-ranking problem (Collins, 2000) can-not be carried out without a deep knowledge aboutautomatic syntactic parsing.
The modeling of syn-tactic/semantic based features should take into ac-count linguistic aspects to detect the interesting con-text, e.g.
the ancestor nodes or the semantic depen-dencies (Toutanova et al, 2004).A viable alternative has been proposed in (Collinsand Duffy, 2002), where convolution kernels wereused to implicitly define a tree substructure space.The selection of the relevant structural features wasleft to the voted perceptron learning algorithm.
An-other interesting model for parsing re-ranking basedon tree kernel is presented in (Taskar et al, 2004).The good results show that tree kernels are verypromising for automatic feature engineering, espe-cially when the available knowledge about the phe-nomenon is limited.Along the same line, automatic learning tasks thatrely on syntactic information may take advantage ofa tree kernel approach.
One of such tasks is the au-tomatic boundary detection of predicate argumentsof the kind defined in PropBank (Kingsbury andPalmer, 2002).
For this purpose, given a predicate pin a sentence s, we can define the notion of predicateargument spanning trees (PAST s) as those syntac-tic subtrees of s which exactly cover all and onlythe p?s arguments (see Section 4.1).
The set of non-spanning trees can be then associated with all theremaining subtrees of s.An automatic classifier which recognizes thespanning trees can potentially be used to detect thepredicate argument boundaries.
Unfortunately, theapplication of such classifier to all possible sen-tence subtrees would require an exponential execu-tion time.
As a consequence, we can use it only todecide for a reduced set of subtrees associated witha corresponding set of candidate boundaries.
Noticehow these can be detected by previous approaches48(e.g.
(Pradhan et al, 2004)) in which a traditionalboundary classifier (tbc) labels the parse-tree nodesas potential arguments (PA).
Such classifiers, gen-erally, are not sensitive to the overall argument struc-ture.
On the contrary, a PAST classifier (pastc) canconsider the overall argument structure encoded inthe associated subtree.
This is induced by the PAsubsets.The feature design for the PAST representationis not simple.
Tree kernels are a viable alternativethat allows the learning algorithm to measure thesimilarity between two PAST s in term of all pos-sible tree substructures.In this paper, we designed and experimented aboundary classifier for predicate argument labelingbased on two phases: (1) a first annotation of po-tential arguments by using a high recall tbc and(2) a PAST classification step aiming to select thecorrect substructures associated with potential argu-ments.
Both classifiers are based on Support VectorMachines learning.
The pastc uses the tree kernelfunction defined in (Collins and Duffy, 2002).
Theresults show that the PAST classification can belearned with high accuracy (the f-measure is about89%) and the impact on the overall boundary detec-tion accuracy is good.In the remainder of this paper, Section 2 intro-duces the Semantic Role Labeling problem alongwith the boundary detection subtask.
Section 3 de-fines the SVMs using the linear kernel and the parsetree kernel for boundary detection.
Section 4 de-scribes our boundary detection algorithm.
Section 5shows the preliminary comparative results betweenthe traditional and the two-step boundary detection.Finally, Section 7 summarizes the conclusions.2 Automated Semantic Role LabelingOne of the largest resources of manually annotatedpredicate argument structures has been developed inthe PropBank (PB) project.
The PB corpus contains300,000 words annotated with predicative informa-tion on top of the Penn Treebank 2 Wall Street Jour-nal texts.
For any given predicate, the expected ar-guments are labeled sequentially from Arg0 to Arg9,ArgA and ArgM.
Figure 1 shows an example ofthe PB predicate annotation of the sentence: Johnrented a room in Boston.Predicates in PB are only embodied by verbswhereas most of the times Arg0 is the subject, Arg1is the direct object and ArgM indicates locations, asin our example.PredicateArg.
0Arg.
MSNNPD NVPV JohninrentedaroomPPIN NBostonArg.
1Figure 1: A predicate argument structure in a parse-tree rep-resentation.Several machine learning approaches for auto-matic predicate argument extraction have been de-veloped, e.g.
(Gildea and Jurasfky, 2002; Gildea andPalmer, 2002; Gildea and Hockenmaier, 2003; Prad-han et al, 2004).
Their common characteristic isthe adoption of feature spaces that model predicate-argument structures in a flat feature representation.In the next section, we present the common parsetree-based approach to this problem.2.1 Predicate Argument ExtractionGiven a sentence in natural language, all the predi-cates associated with the verbs have to be identifiedalong with their arguments.
This problem is usuallydivided in two subtasks: (a) the detection of the tar-get argument boundaries, i.e.
the span of its wordsin the sentence, and (b) the classification of the argu-ment type, e.g.
Arg0 or ArgM in PropBank or Agentand Goal in FrameNet.The standard approach to learn both the detectionand the classification of predicate arguments is sum-marized by the following steps:1.
Given a sentence from the training-set, gener-ate a full syntactic parse-tree;2. let P and A be the set of predicates and theset of parse-tree nodes (i.e.
the potential argu-ments), respectively;3. for each pair < p, a >?
P ?A:?
extract the feature representation set, Fp,a;49?
if the subtree rooted in a covers exactlythe words of one argument of p, put Fp,ain T+ (positive examples), otherwise putit in T?
(negative examples).For instance, in Figure 1, for each combination ofthe predicate rent with the nodes N, S, VP, V, NP,PP, D or IN the instances Frent,a are generated.
Incase the node a exactly covers ?John?, ?a room?
or?in Boston?, it will be a positive instance otherwiseit will be a negative one, e.g.
Frent,IN .The T+ and T?
sets are used to train the bound-ary classifier.
To train the multi-class classifier T+can be reorganized as positive T+argi and negativeT?argi examples for each argument i.
In this way,an individual ONE-vs-ALL classifier for each argu-ment i can be trained.
We adopted this solution, ac-cording to (Pradhan et al, 2004), since it is simpleand effective.
In the classification phase, given anunseen sentence, all its Fp,a are generated and clas-sified by each individual classifier Ci.
The argumentassociated with the maximum among the scores pro-vided by the individual classifiers is eventually se-lected.2.2 Standard feature spaceThe discovery of relevant features is, as usual, acomplex task.
However, there is a common con-sensus on the set of basic features.
These stan-dard features, firstly proposed in (Gildea and Juras-fky, 2002), refer to unstructured information de-rived from parse trees, i.e.
Phrase Type, PredicateWord, Head Word, Governing Category, Positionand Voice.
For example, the Phrase Type indicatesthe syntactic type of the phrase labeled as a predicateargument, e.g.
NP for Arg1 in Figure 1.
The ParseTree Path contains the path in the parse tree betweenthe predicate and the argument phrase, expressed asa sequence of nonterminal labels linked by direction(up or down) symbols, e.g.
V ?
VP ?
NP for Arg1 inFigure 1.
The Predicate Word is the surface form ofthe verbal predicate, e.g.
rent for all arguments.In the next section we describe the SVM approachand the basic kernel theory for the predicate argu-ment classification.3 Learning predicate structures viaSupport Vector MachinesGiven a vector space in <n and a set of positive andnegative points, SVMs classify vectors according toa separating hyperplane, H(~x) = ~w ?
~x + b = 0,where ~w ?
<n and b ?
< are learned by applyingthe Structural Risk Minimization principle (Vapnik,1995).To apply the SVM algorithm to Predicate Argu-ment Classification, we need a function ?
: F ?
<nto map our features space F = {f1, .., f|F|} and ourpredicate/argument pair representation, Fp,a = Fz ,into <n, such that:Fz ?
?
(Fz) = (?1(Fz), .., ?n(Fz))From the kernel theory we have that:H(~x) =( ?i=1..l?i~xi)?
~x+ b =?i=1..l?i~xi ?
~x+ b =?i=1..l?i?
(Fi) ?
?
(Fz) + b.where, Fi ?i ?
{1, .., l} are the training instancesand the product K(Fi, Fz) =<?
(Fi) ??
(Fz)> is thekernel function associated with the mapping ?.The simplest mapping that we can apply is?
(Fz) = ~z = (z1, ..., zn) where zi = 1 if fi ?
Fzand zi = 0 otherwise, i.e.
the characteristic vectorof the set Fz with respect to F .
If we choose thescalar product as a kernel function we obtain the lin-ear kernel KL(Fx, Fz) = ~x ?
~z.An interesting property is that we do not need toevaluate the ?
function to compute the above vector.Only the K(~x, ~z) values are in fact required.
This al-lows us to derive efficient classifiers in a huge (pos-sible infinite) feature space, provided that the ker-nel is processed in an efficient way.
This propertyis also exploited to design convolution kernel likethose based on tree structures.3.1 The tree kernel functionThe main idea of the tree kernels is the modeling ofa KT (T1, T2) function which computes the numberof common substructures between two trees T1 andT2.Given the set of substructures (fragments){f1, f2, ..} = F extracted from all the trees of thetraining set, we define the indicator function Ii(n)50SNP VPVP VP CCVB NPtook DT NNthe bookand VB NPread PRP$ NNits titlePRPJohnSNP VPVPVB NPreadSentence Parse-TreeSNP VPVPVB NPtooktook{ARG0, ARG1}PRPJohnPRPJohnDT NNthe bookPRP$ NNits titleread{ARG0, ARG1}Figure 2: A sentence parse tree with two predicative tree structures (PAST s)which is equal 1 if the target fi is rooted at node nand 0 otherwise.
It follows that:KT (T1, T2) =?n1?NT1?n2?NT2?
(n1, n2) (1)where NT1 and NT2 are the sets of the T1?sand T2?s nodes, respectively and ?
(n1, n2) =?|F|i=1 Ii(n1)Ii(n2).
This latter is equal to the num-ber of common fragments rooted at the n1 and n2nodes.
We can compute ?
as follows:1. if the productions at n1 and n2 are differentthen ?
(n1, n2) = 0;2. if the productions at n1 and n2 are the same,and n1 and n2 have only leaf children (i.e.
theyare pre-terminals symbols) then ?
(n1, n2) =1;3. if the productions at n1 and n2 are the same,and n1 and n2 are not pre-terminals then?
(n1, n2) =nc(n1)?j=1(1 + ?
(cjn1 , cjn2)) (2)where nc(n1) is the number of the children of n1and cjn is the j-th child of the node n. Note that, asthe productions are the same, nc(n1) = nc(n2).The above kernel has the drawback of assigninghigher weights to larger structures1.
In order to over-come this problem we scale the relative importanceof the tree fragments imposing a parameter ?
in con-ditions 2 and 3 as follows: ?
(nx, nz) = ?
and?
(nx, nz) = ?
?nc(nx)j=1 (1 + ?
(cjn1 , cjn2)).1In order to approach this problem and to map similarityscores in the [0,1] range, a normalization in the kernel space,i.e.
K?T (T1, T2) = KT (T1,T2)?KT (T1,T1)?KT (T2,T2) .
is always applied4 Boundary detection via argumentspanningSection 2 has shown that traditional argumentboundary classifiers rely only on features extractedfrom the current potential argument node.
In or-der to take into account a complete argument struc-ture information, the classifier should select a set ofparse-tree nodes and consider them as potential ar-guments of the target predicate.
The number of allpossible subsets is exponential in the number of theparse-tree nodes of the sentence, thus, we need tocut the search space.
For such purpose, a traditionalboundary classifier can be applied to select the setof potential arguments PA.
The reduced number ofPA subsets can be associated with sentence subtreeswhich in turn can be classified by using tree kernelfunctions.
These measure if a subtree is compatibleor not with the subtree of a correct predicate argu-ment structure.4.1 The Predicate Argument Spanning Trees(PAST s)We consider the predicate argument structures an-notated in PropBank along with the correspondingTreeBank data as our object space.
Given the targetpredicate p in a sentence parse tree T and a subsets = {n1, .., nk} of the T?s nodes, NT , we define asthe spanning tree root r the lowest common ancestorof n1, .., nk.
The node spanning tree (NST ), ps isthe subtree rooted in r, from which the nodes thatare neither ancestors nor descendants of any ni areremoved.Since predicate arguments are associated withtree nodes, we can define the predicate argu-51SNP VPVB NPreadJohnDT NNthe titleNP PPDT NNthe bookNP INofArg.
1Arg.
0SNP VPVB NPreadJohnDT NNthe titleNP PPDT NNthe bookNP INofSNP VPVB NPreadJohnDT NNthe titleNP PPDT NNthe bookNP INofSNP-0 VPJohnPPDT NNthe bookNP INofSNP-0 VPVB NPreadJohnDT NNthe titleNP-1 PP-2DT NNthe bookINofNP(a) (b) (c)Correct PASTIncorrect  PASTCorrect PASTIncorrect  PASTDT NNthe titleNPNP-1 VBreadFigure 3: Two-step boundary classifier.ment spanning tree (PAST ) of a predicate ar-gument set, {a1, .., an}, as the NST over suchnodes, i.e.
p{a1,..,an}.
A PAST correspondsto the minimal subparse tree whose leaves areall and only the word sequence compoundingthe arguments.
For example, Figure 2 showsthe parse tree of the sentence "John took thebook and read its title".
took{ARG0,ARG1}and read{ARG0,ARG1} are two PAST structuresassociated with the two predicates took and read,respectively.
All the other NST s are not validPAST s.Notice that, labeling ps, ?s ?
NT with a PASTclassifier (pastc) corresponds to solve the boundaryproblem.
The critical points for the application ofthis strategy are: (1) how to design suitable featuresfor the PAST characterization.
This new problemrequires a careful linguistic investigation about thesignificant properties of the argument spanning treesand (2) how to deal with the exponential number ofNST s.For the first problem, the use of tree kernels overthe PAST s can be an alternative to the manual fea-tures design as the learning machine, (e.g.
SVMs)can select the most relevant features from a high di-mensional feature space.
In other words, we can useEq.
1 to estimate the similarity between two PAST savoiding to define explicit features.
The same ideahas been successfully applied to the parse-tree re-ranking task (Taskar et al, 2004; Collins and Duffy,2002) and predicate argument classification (Mos-chitti, 2004).For the second problem, i.e.
the high computa-tional complexity, we can cut the search space by us-ing a traditional boundary classifier (tbc), e.g.
(Prad-han et al, 2004), which provides a small set of po-tential argument nodes.
Let PA be the set of nodeslocated by tbc as arguments.
We may consider theset P of the NST s associated with any subset ofPA, i.e.
P = {ps : s ?
PA}.
However, alsothe classification ofP may be computationally prob-lematic since theoretically there are |P| = 2|PA|members.In order to have a very efficient procedure, weapplied pastc to only the PA sets associated withincorrect PAST s. A way to detect such incor-rect NST s is to look for a node pair <n1, n2>?PA ?
PA of overlapping nodes, i.e.
n1 is ances-tor of n2 or viceversa.
After we have detected suchnodes, we create two node sets PA1 = PA?
{n1}and PA2 = PA ?
{n2} and classify them with thepastc to select the correct set of argument bound-aries.
This procedure can be generalized to a set ofoverlapping nodes O greater than 2 as reported inAppendix 1.Note that the algorithm selects a maximal set ofnon-overlapping nodes, i.e.
the first that is gener-ated.
Additionally, the worst case is rather rare thusthe algorithm is very fast on average.The Figure 3 shows a working example of themulti-stage classifier.
In Frame (a), tbc labels aspotential arguments (gray color) three overlappingnodes (in Arg.1).
The overlap resolution algorithmproposes two solutions (Frame (b)) of which onlyone is correct.
In fact, according to the second so-lution the propositional phrase ?of the book?
wouldincorrectly be attached to the verbal predicate, i.e.in contrast with the parse tree.
The pastc, applied52to the two NST s, should detect this inconsistencyand provide the correct output.
Note that, during thelearning, we generate the non-overlapping structuresin the same way to derive the positive and negativeexamples.4.2 Engineering Tree Fragment FeaturesIn the Frame (b) of Figure 3, we show one of thepossible cases which pastc should deal with.
Thecritical problem is that the two NST s are perfectlyidentical, thus, it is not possible to discern betweenthem using only their parse-tree fragments.The solution to engineer novel features is to sim-ply add the boundary information provided by thetbc to the NST s. We mark with a progressive num-ber the phrase type corresponding to an argumentnode, starting from the leftmost argument.
For ex-ample, in the first NST of Frame (c), we markas NP-0 and NP-1 the first and second argumentnodes whereas in the second NST we have an hy-pothesis of three arguments on the NP, NP and PPnodes.
We trasform them in NP-0, NP-1 andPP-2.This simple modification enables the tree ker-nel to generate features useful to distinguish be-tween two identical parse trees associated with dif-ferent argument structures.
For example, for the firstNST the fragments [NP-1 [NP][PP]], [NP[DT][NN]] and [PP [IN][NP]] are gener-ated.
They do not match anymore with the [NP-0[NP][PP]], [NP-1 [DT][NN]] and [PP-2[IN][NP]] fragments of the second NST .In order to verify the relevance of our model, thenext section provides empirical evidence about theeffectiveness of our approach.5 The ExperimentsThe experiments were carried out withthe SVM-light-TK software available athttp://ai-nlp.info.uniroma2.it/moschitti/which encodes the tree kernels in the SVM-lightsoftware (Joachims, 1999).
For tbc, we used thelinear kernel with a regularization parameter (option-c) equal to 1 and a cost-factor (option -j) of 10 tohave a higher Recall.
For the pastc we used ?
= 0.4(see (Moschitti, 2004)).As referring dataset, we used the PropBank cor-pora available at www.cis.upenn.edu/?ace,along with the Penn TreeBank 2(www.cis.upenn.edu/?treebank) (Marcus etal., 1993).
This corpus contains about 53,700sentences and a fixed split between training andtesting which has been used in other researches, e.g.
(Pradhan et al, 2004; Gildea and Palmer, 2002).We did not include continuation and co-referringarguments in our experiments.We used sections from 02 to 07 (54,443 argu-ment nodes and 1,343,046 non-argument nodes) totrain the traditional boundary classifier (tbc).
Then,we applied it to classify the sections from 08 to21 (125,443 argument nodes vs. 3,010,673 non-argument nodes).
As results we obtained 2,988NST s containing at least an overlapping node pairout of the total 65,212 predicate structures (accord-ing to the tbc decisions).
From the 2,988 over-lapping structures we extracted 3,624 positive and4,461 negative NST s, that we used to train thepastc.The performance was evaluated with the F1 mea-sure2 over the section 23.
This contains 10,406 ar-gument nodes out of 249,879 parse tree nodes.
Byapplying the tbc classifier we derived 235 overlap-ping NSTs, from which we extracted 204 PAST sand 385 incorrect predicate argument structures.
Onsuch test data, the performance of pastc was veryhigh, i.e.
87.08% in Precision and 89.22% in Recall.Using the pastc we removed from the tbc the PAthat cause overlaps.
To measure the impact on theboundary identification performance, we comparedit with three different boundary classification base-lines:?
tbc: overlaps are ignored and no decision istaken.
This provides an upper bound for therecall as no potential argument is rejected forlater labeling.
Notice that, in presence of over-lapping nodes, the sentence cannot be anno-tated correctly.?
RND: one among the non-overlapping struc-tures with maximal number of arguments israndomly selected.2F1 assigns equal importance to Precision P and Recall R,i.e.
F1 = 2P?RP+R .53tbc tbc+RND tbc+Heu tbc+pastcP R F P R F P R F P R FAll Struct.
92.21 98.76 95.37 93.55 97.31 95.39 92.96 97.32 95.10 94.40 98.42 96.36Overl.
Struct.
98.29 65.8 78.83 74.00 72.27 73.13 68.12 75.23 71.50 89.61 92.68 91.11Table 1: Two-steps boundary classification performance using the traditional boundary classifier tbc, the random selection ofnon-overlapping structures (RND), the heuristic to select the most suitable non-overlapping node set (Heu) and the predicateargument spanning tree classifier (pastc).?
Heu (heuristic): one of the NST s which con-tain the nodes with the lowest overlappingscore is chosen.
This score counts the numberof overlapping node pairs in the NST .
For ex-ample, in Figure 3.
(a) we have a NP that over-laps with two nodes NP and PP, thus it is as-signed a score of 2.The third row of Table 1 shows the results of tbc,tbc + RND, tbc + Heu and tbc + pastc in thecolumns 2,3,4 and 5, respectively.
We note that:?
The tbc F1 is slightly higher than the result ob-tained in (Pradhan et al, 2004), i.e.
95.37%vs.
93.8% on same training/testing conditions,i.e.
(same PropBank version, same training andtesting split and same machine learning algo-rithm).
This is explained by the fact that wedid not include the continuations and the co-referring arguments that are more difficult todetect.?
Both RND and Heu do not improve the tbc re-sult.
This can be explained by observing that inthe 50% of the cases a correct node is removed.?
When, to select the correct node, the pastc isused, the F1 increases of 1.49%, i.e.
(96.86 vs.95.37).
This is a very good result consideringthat to increase the very high baseline of tbc ishard.In order to give a fairer evaluation of our approachwe tested the above classifiers on the overlappingstructures only, i.e.
we measured the pastc improve-ment on all and only the structures that required itsapplication.
Such reduced test set contains 642 ar-gument nodes and 15,408 non-argument nodes.
Thefourth row of Table 1 reports the classifier perfor-mance on such task.
We note that the pastc im-proves the other heuristics of about 20%.6 Related WorkRecently, many kernels for natural language applica-tions have been designed.
In what follows, we high-light their difference and properties.The tree kernel used in this article was proposedin (Collins and Duffy, 2002) for syntactic parsing re-ranking.
It was experimented with the Voted Percep-tron and was shown to improve the syntactic parsing.A refinement of such technique was presented in(Taskar et al, 2004).
The substructures produced bythe proposed tree kernel were bound to local prop-erties of the target parse tree and more lexical infor-mation was added to the overall kernel function.In (Zelenko et al, 2003), two kernels over syn-tactic shallow parser structures were devised forthe extraction of linguistic relations, e.g.
person-affiliation.
To measure the similarity between twonodes, the contiguous string kernel and the sparsestring kernel (Lodhi et al, 2000) were used.
Theformer can be reduced to the contiguous substringkernel whereas the latter can be transformed in thenon-contiguous string kernel.
The high running timecomplexity, caused by the general form of the frag-ments, limited the experiments on data-set of just200 news items.In (Cumby and Roth, 2003), it is proposed a de-scription language that models feature descriptorsto generate different feature type.
The descriptors,which are quantified logical prepositions, are instan-tiated by means of a concept graph which encodesthe structural data.
In the case of relation extractionthe concept graph is associated with a syntactic shal-low parse and the extracted propositional featuresexpress fragments of a such syntactic structure.
Theexperiments over the named entity class categoriza-tion show that when the description language selectsan adequate set of tree fragments the Voted Percep-tron algorithm increases its classification accuracy.In (Culotta and Sorensen, 2004) a dependency54tree kernel is used to detect the Named Entity classesin natural language texts.
The major novelty wasthe combination of the contiguous and sparse ker-nels with the word kernel.
The results show thatthe contiguous outperforms the sparse kernel and thebag-of-words.7 ConclusionsThe feature design for new natural language learn-ing tasks is difficult.
We can take advantage fromthe kernel methods to model our intuitive knowledgeabout the target linguistic phenomenon.
In this pa-per we have shown that we can exploit the propertiesof tree kernels to engineer syntactic features for thepredicate argument boundary detection task.Preliminary results on gold standard trees suggestthat (1) the information related to the whole predi-cate argument structure is important and (2) tree ker-nel can be used to generate syntactic features.In the future, we would like to use an approachsimilar to the PAST classifier on parses providedby different parsing models to detect boundary andto classify semantic role more accurately .AcknowledgementsWe wish to thank Ana-Maria Giuglea for her help inthe design and implementation of the basic Seman-tic Role Labeling system that we used in the experi-ments.ReferencesMichael Collins and Nigel Duffy.
2002.
New rankingalgorithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron.
In ACL02.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Proceedings of ICML 2000.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedings ofthe 42nd Meeting of the Association for ComputationalLinguistics (ACL?04), Main Volume, pages 423?429,Barcelona, Spain, July.Chad Cumby and Dan Roth.
2003.
Kernel methods forrelational learning.
In Proceedings of the TwentiethInternational Conference (ICML 2003), Washington,DC, USA.Daniel Gildea and Julia Hockenmaier.
2003.
Identifyingsemantic roles using combinatory categorial grammar.In Proceedings of the 2003 Conference on EmpiricalMethods in Natural Language Processing, Sapporo,Japan.Daniel Gildea and Daniel Jurasfky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistic,28(3):496?530.Daniel Gildea and Martha Palmer.
2002.
The neces-sity of parsing for predicate argument recognition.
InProceedings of the 40th Annual Conference of theAssociation for Computational Linguistics (ACL-02),Philadelphia, PA, USA.T.
Joachims.
1999.
Making large-scale SVM learningpractical.
In B. Scho?lkopf, C. Burges, and A. Smola,editors, Advances in Kernel Methods - Support VectorLearning.Paul Kingsbury and Martha Palmer.
2002.
From Tree-bank to PropBank.
In Proceedings of the 3rd Interna-tional Conference on Language Resources and Evalu-ation (LREC-2002), Las Palmas, Spain.Ron Kohavi and Dan Sommerfield.
1995.
Feature sub-set selection using the wrapper model: Overfitting anddynamic search space topology.
In The First Interna-tional Conference on Knowledge Discovery and DataMining, pages 192?197.
AAAI Press, Menlo Park,California, August.
Journal version in AIJ.Huma Lodhi, Craig Saunders, John Shawe-Taylor, NelloCristianini, and Christopher Watkins.
2000.
Text clas-sification using string kernels.
In NIPS, pages 563?569.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of en-glish: The Penn Treebank.
Computational Linguistics,19:313?330.Alessandro Moschitti.
2004.
A study on convolution ker-nels for shallow semantic parsing.
In proceedings ofthe 42th Conference on Association for ComputationalLinguistic (ACL-2004), Barcelona, Spain.Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, WayneWard, James H. Martin, and Daniel Jurafsky.
2005.Support vector learning for semantic argument classi-fication.
to appear in Machine Learning Journal.Ben Taskar, Dan Klein, Mike Collins, Daphne Koller, andChristopher Manning.
2004.
Max-margin parsing.
InDekang Lin and Dekai Wu, editors, Proceedings ofEMNLP 2004, pages 1?8, Barcelona, Spain, July.
As-sociation for Computational Linguistics.Kristina Toutanova, Penka Markova, and Christopher D.Manning.
2004.
The leaf projection path view ofparse trees: Exploring string kernels for hpsg parse se-lection.
In Proceedings of EMNLP 2004.55V.
Vapnik.
1995.
The Nature of Statistical Learning The-ory.
Springer.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Kernelmethods for relation extraction.
Journal of MachineLearning Research.Appendix 1: Generalized BoundarySelection AlgorithmLet O be the set of overlapping nodes of PA, andNO the set of non overlapping nodes of PA.Let subs(?1)(A) = {B|B ?
2A, |B| = |A| ?
1}.Let O?
= subs(?1)(O).while(true)begin1.
H = ?2.
?o ?
O?
:(a) If o does not include any overlapping nodepairthen H = H ?
{o}3.
If H 6= ?
then:(a) Let s?
=argmaxo?H pastc(pNO?o),where pNO?o represents the node span-ning tree compatible with o, and thepastc(pNO?o) is the score provided by thePAST SVM categorizer on it(b) If pastc(s?)
> 0 then RETURN( s?)4.
If O?
= {?}
then RETURN( NO )5.
Else:(a) O?
= O?
?H(b) O?
= ?o?O?
subs(?1)(o)end56
