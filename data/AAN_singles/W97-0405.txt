A Formal Basis for Spoken Language Translation by AnalogyKeiko HoriguchiD21 Laboratory6-7-35 KitashinagawaShinagawa-ku, Tokyo 141Japankeiko?pdp, crl .
sony.
co. jpAlexander FranzSony Computer Science Laboratory &D21 Laboratory6-7-35 KitashinagawaShinagawa-ku, Tokyo 141Japanamf~csl, sony.
co. jpAbstractSince spoken language is characterizedby a number of properties that defy in-terpretation and translation by purelygrammar-based techniques, recent i -terest has turned to analogical (alsoknown as case-based or example-based)approaches.
In this framework, themost important step consists of robustlymatching the recognized input expressionwith the stored examples.
This paperpresents a probabilistic formalization ofanalogical matching, and describes howthis model is applied to speech transla-tion in the framework of translation byanalogy.1 Int roduct ionThe la.,~t decade has seen growing interest in theexample-based framework for translation of writtenand spoken language (Nagao, 1984),(Jones, 1996).This approach, sometimes called analogical, case-based, or memory-based, originated with the follow-ing insight.
In the course of translating an expres-sion, a skilled human translator often recalls a sim-ilar translation that she has performed or studiedbefore, and then carries out the new translation byanalogy to the previous case, instead of applying alarge number of lexical and grammatical rules in herhead.
In an example-based translation architecture,pairs of bilingual expressions are stored in the exam-ple database.
The source language input expressionis matched against the source language portion ofeach example pair, and the best matching exampleis selected.
The system then returns the target lan-guage portion of the best example ms output.
Thisis illustrated in Figure 1.BHiagusl Example DatabaseFigure 1: Example-based Translation ArchitectureI.I Pre-translationThe example-based approach as certain advantagesover traditional rule-based approaches totranslatingspoken language.
Since an analogical system relieson a database of pre-translated xample pairs, it re-sults in high translation quality.
High translationquality requires not only that the output be gram-matically correct, but also that the output soundnatural and idiomatic.
Spoken utterances consist oflarger portions of fully-lexicalized orsemi-lexicalizedmorpheme sequences, the use of which greatly con-tributes to sounding natural and native-like, butwhose meanings are not totally predicatable fromtheir forms (Pawley and Syder, 1983).
An analogicalsystem can generate natural-sounding output moreeasily than a compositional, rule-based system, be-cause it directly uses the correspondences betweensource-language and target-language expressions.321.2 RobustnessAnother important requirement for spoken languagetranslation is that the system has to be very robust.Spoken utterances contain a lot of disfluencies, uchas pronunciation errors, word selection errors, wordfragments and repairs.
Furthermore, a speech trans-lation module also has to handle the errors intro-duced by the speech recognition component.
In ananalogical system, the process that matches the in-put expression against examples can be very robust,and can always return the best matching output ex-pressions instead of failing completely.1.3 Improv ing  Trans la t ion  Qual i tyAn additional requirement of an automatic trans-lation system is that it should be possible to im-prove the translation quality by expending addi-tional effort.
In a traditional rule-based system, asthe knowledge sources (such ms grammar ules, se-mantic disambiguation rules, transfer ules, etc.)
ex-pand in size, there comes a point at which the com-plex interrelationships between the different ypes ofinformation precludes any further improvement.
Inan analogical system, it is possible to incrementallyimprove the translation quality by adding more ex-amples to the example database, and by effectingcorresponding improvements in the matching func-tion by e.g.
refining the thesaurus or re-estimatingword similarities from an expanded bilingual corpus.1.4 The  Prob lem of  ScalabiHtyUnfortunately, the pure analogical approach lacksscalability.
The effort required to acquire and main-tain the example database, the cost of the space re-quired to store the examples, and the cost of thetime required to search the database can becomeprohibitively high, since a pure analogical systemrequires a separate xample for every linguistic vari-ation.2 A Hybrid Analogical ApproachSince language is productive, a realistic analogicalsystem needs to be able to handle linguistic con-structions that do not have an exact match in theexample database.
Therefore it is important for asystem to be able to combine fragments from morethan one example expression to cover the input ex-pression.To meet this requirement, we have designed an ar-chitecture for robust, practical translation of spokenlanguage in limited domains that integrates morpho-logical and syntactic linguistic processing with ananalogical transfer component.
The overall systemis described briefly in this section.2.1 System Arch i tec tureThe pipelined system architecture, shown in Fig-ure 2, separates peech recognition, morphological~i~\]l ~ :- ~- - -  \] : , i~ogoizer ,j\[ Morphological Analyzer \[ ii!\[ $ 'h.er I iAnalogical \] '~:.~'"~ ..... .
~,~:~:~N~i~!
: Transfer Module .
.
.
~:~ : i~iTarget Language Generator ~ ...........  ~:::ii~i~i~-- -$  .
.
.
.,~"s'p~;'"~',',.
_~.~9.~_ ~_ ',JFigure 2: Sys tem Architectureanalysis, shallow parsing, and recursive analogicaltranslation into different modules.
This separationof general linguistic, domain, and transfer knowledgeimproves portability and scalability of the system.2.2 Shal low Source Language AnalysisThe purpose of the shallow analysis component is toidentify clauses and phrases, to identify modifyingrelations as long as they are unambiguous (derivinga canonical interpretation i ambiguous cases), andto convert some surface variations into features.In our prototype implementation, anadapted ver-sion of the JUMAN 3.1 Japanese morphological n-alyzer (Kurohashi et al, 1994) is used for Part-of-Speech disambiguation, and for dictionary and the-sanrus look-up.The second step of source language analysis iscarried out using an augmented context-free gram-mar for the NLYAcc parser (Ishii, Ohta, and Saito.1994), which is an implementation of the General-ized LR parsing parsing algorithm (Tomita, 1985).The shallow analysis module returns a shallowsyntactic parse tree with various lexical and syn-tactic features.
It is robust enough to tolerate ex-tragrammaticalities, disfluencies, and the like in theinput.2.3 Analogical  TransferThe recursive analogical transfer module matchesthe input shallow syntactic tree against the sourcelanguage portions of example shallow syntactic trees.The example data is classified into different lin-guistic constituent levels, such as clause-level xam-ples, phrase-level examples, and word-level exam-pies.
The system tries to match the input againstexamples of the largest unit.Once the system finds the best matching exam-pies of the largest unit, it checks whether there areportions that differ significantly between the input33and the example.
If so.
the system performs theanalogical matching process again on the identifiedportion from'the input, using examples of the corre-sponding smaller unit.
This recursive process con-tinues until all parts have been matched.
Finally, thetarget language portions of the selected best matchesare comhined to form the complete target languageexpression.The analogical matching step is based on a prob-abilistic formalization of matching by analogy.
Thedetails of this model are described in Section 4.2.4 Target Language GenerationThe target language generation module is designedto perform a number of linguistic operations, uchms enforcing subject-verb agreement, ensuring thatrequired efiniteness information is present (such asEnglish determiners, quantifiers, or possessives), andgenerating the appropriate inflectional morphology.In our prototype implementation, weare using thePC-KIMMO system for generating English morphol-ogy (Ant.worth, 1990).
After these operations, theshallow syntactic tree is linearized to create an ex-pression in the target language.2.5 Speech SynthesisIn the final step, spoken output is generated from thetarget language xpression.
In our Japanese-Engiishprototype, this step is carried out by the DECTALKsystem (HMlalaan, 1996).3 Advantages of the HybridAnalogical ApproachThe hybrid approach combines analogical matchingand transfer with a rule-baaed component that ac-counts for one of the fundamental properties of lan-guage: Its productiveness.
This section describeswhat we perceive to be the main advantages of thehybrid analogical approach to speech translation.3.1 Modular, Natural Knowledge SourcesThe system architecture s parates general linguisticknowledge, domain knowledge, and transfer knowl-edge.
This means it is easier to port it to differentdomains, and to apply it to new languages.We also consider the knowledge sources to be"natural".
By this we mean that, from the point ofview of knowledge representation, each knowledgesource captures certain aspects of the translationprocess in its most natural form.
For example, theexample data ba.se captures translation correspon-dences in a natural way - by means of correspondingnatural anguage xpressions in the souce and tar-get language, Other, less natural means of knowl-edge representation would require significantly moreeffort to acquire and maintain.
As a result, it is eas-ier to improve the translation quality by adding andmodifying examples, and by modifying the thesaurus(if necessary).3.2 Examples vs. Syntactic or SemanticGrammarsAs described above, analogical translation relies ona database of example pairs which can encode id-iomatic translation correspondences at the lexical.phrasal, and clausal manner in a natural way.
Thisis an improvement over previous approaches whichrely on syntactic or semantic grammars.For example, the "transfer-driven'" apl)roachof (Sobashima et M., 1994) relies on essentiallysyntactically-based analysis and transfer ules thatare manually annotated with examples by provid-ing a sound formal basis for analogical matching.This requires an extensive ffort to create a body ofrules that covers all possible xpressions, and whichcan handle extra.grammatical or disfluent input.
Asan example of a semantic-grammax b sed approach.
"concept-based" translation (Mayfield et al, 1995)requires an extensive manual knowledge acquisitioneffort to create detailed, domain and task-specifictemplates and semantic grammars.
In addition, aheavily semantics-based approach such a.s this worksuffers from a lack of generality due to the absenceof linguistic processing.3.3 ;Exa,r~ples vs. InterlinguaThe framework of Interlingua-based translation restson the presupposition that there can be a universal,unaxnbiguous, language-neutral," nd practically (ifnot formally) sound knowledge representation for-realism to mediate between source and target lan-guages.
In practice, defining, maintaining, and ex-tending such a formalism for multiple, not closelyrelated languages has proved to be a major chal-lenge.
Analogical speech translation does not relyon this presupposition, and instead seeks to captureintuitive translation correspondences.3.4 Syntactic and Lexical DistanceIn the hybrid analogical pproach, the example datais categorized by linguistic constituent.
For exant-pie, there axe translation example pairs at the clauselevel, phrase level, and word level.
This yields amore efficient search procedure during the match-ing process, while only assuming non-controversialnotions of syntactic onstituency.
By treating syn-tactically similarity and semantic similarity as twoseparate aspects of the matching process, we derivean improvement over methods that combine thesetwo aspects.
For example, (Sato and Nagao, 1990}combine a measure of structural similarity with ameasure of word distance in order to obtain the over-all distance measure that is used for matching.343.5 Computat iona l  Eff iciencyAnalogical translation relies on a large database ofexample pairs.
This incurs a significant computa-tional cost for searching and matching against allthe examples, which is proportional to the numberof examples multiplied by the average size of the rep-resentations of the examples.
(In practice, this costcan be mitigated somewhat by clustering and in-dexing schemes for the example databa.se.)
Hybridanalogical translation greatly reduces the number ofrequired examples by relying on the generality oflinguistic rules.Pure statistical machine translation (Brown et al,1993) mltst in principle recover the most probablealignment out of all possible alignments between theinput and a translation.
While this approach is the-oretically intriguing, it has yet to be shown to becomputationally feasible in practice.3.6 Linguist ic Eff iciencyIn addition to computational efficiency, we also con-sider a factor that might be called "linguistic effi-ciency".
We hold that a significant body of system-atic linguistic regularities has been identified thatnlust be accounted for somehow during the processof translation.
Linguistic efficiency refers to the no-tion of how efficient the system is with regard tothese regularities.In hybrid analogical translation, the use of a mor-phological and syntactic module for shallow analysisto derive a linguistic representation with syntacticand lexical features allows us to handle phenomenasuch as inflections, transformations, and language-specific phenomena (such as the English determinersystem and certain Japanese constructions that en-code politeness information) in a linguistically effi-cient manner.3.7 Translation AdequacyIn order to be able to provide stylistically and prag-matically adequate translations of spoken language,it is not sufficient o merely ignore or tolerate xtra-grammaticalities in the input; in many cases, theinformation carried by such phenomena must bereflected in the target language output.
The hy-brid analogical approach is able to model such phe-nomena using probabilistic operators, which are ex-plained in more detail in the next section.4 A Probabilistic Model forAnalogical MatchingWhen applied to spoken language, the central step inanalogical translation is a robust matching Step thatcompares the output of the speech recognition com-ponent with the contents of the example database.This section presents the probabilistic model thatprovides a formal basis for this matching step.Figure 3: Viewing Input a.~ Distorted Exanlples4.1 NotationLet I denote the input expression, consisting of a se-quence of words along with certain features resultingfrom shallow parsing.
Thus, an input expressionsI consists of a sequence of words iwl, iw2 .
.
.
.
.
iw,~.and a set of features ifl,if2 .
.
.
.
.
ifm.
Similarly, letthe source expression E of an example pair consistof ewl, ew2,.
.
.
,  ewp and efb el2 .
.
.
.
.
efq.4,2 The Noisy Channe l  Mode lThe "noisy-channel" model from information theoryhas proven highly effective in speech recognition and,more recently, in language understanding (Epstein etal., 1996; Miller et al, 1994).
We adopt this modelfor translation by analogy in the following manner.Given an input expression, the analogical match-ing algorithm must determine the example expres-sion that is closest in meaning to the input expres-sion.
We denote the probability that an exampleexpression is appropriate for translating some inputas the conditional probability of the example, giventhe input:(1) P(ExamplelInput )Our aim is to find the example that has the high-est conditional probability of being appropriate totranslate the given input.
We denote that exam-pie with Emax, where the max function chooses theexample with the maximum conditional probability:(2) Emax = trmXE~Examples\[P(ElI)\]Our approach to determining Ernax is as fol-lows.
First, we can use Bayes' Law to obtain a re-expression of the conditional probability that needsto be maximized:(3) P(E\ [ I )= P(E)P(IIE)P(I)Since the input expression, and therefore P(I).
re-mains constant over different examples, we can dis-regard the term P(I) in the denominator.
Thus.
weneed to determine Emax which can be defined msfollows:35(4) Emax = maXEEExamples \[P(E)P(I\]E)\]The probability distribution over the examplesP(E) encodes .the prior probability of using the dif-ferent examples to translate xpressions in the do-main.
It can be used to penalize certain specializedexpressions that should be used less frequently.
Theconditional probability distribution is estimated us-ing a "'distortion" model of utterances that is de-.scribed in the next section.4.3 Viewing Input as Distorted ExamplesThe conditional probability distribution P(I\[E) ismodeled as follows.
Consider that the speakerintends to express an underlying message S, butspeech errors, certain speech properties, misrecog-nitions, and other factors interfere, resulting in theactual utterance I, which forms the input to thetranslation system (Figure 3).
This is modeled usinga number "'distortion" operators:?
echo-word(ewi).
This operator simplyechoes the ith word, ewi, from the example tothe input.?
delete-word(ewe).
This operator deletes theith word, ewi, from the example.?
add-word(iwj).
This operator adds the jthword, iwj, to the input.?
alter-word(ewi,iwi).
This operator altersthe ith word, ewi, from the example to thejth word, iwj, in the input expression.
Thealtered word is different, but usually semanti-cally somewhat similar.?
Corresponding operators for features.Given these operators, we can view the input Ias an example E to which a number of distortionoperators have been applied.
Thus, we can representan input expression I as an example plus a set ofdistortion operators:(5) I = {E, d istort l  .
.
.
.
,distortffi}This means that we can re-express the conditionalprobability distribution for an input expression I,given that the meaning expressed by example E isintended, ms follows:(6) P(IIE) = P({d is tor t l , .
.
.
,d i s to r tz} \ ]E )4.4 Operator  I ndependence  Assumpt ionTwo independence assumptions axe required to makethis model computationally feasible.
For the first as-sumption, we assume that the individual distortionoperators are conditionally independent, given theexample E:(7) P(distorttldistortl .
.
.
.
.
distortz; E)P(distort61E)This means that we can make the following sim-plification:(8) P({distortl .
.
.
.
.
distort~}IE) =f i  P(distort~lE)k=tThus, we obtain the following:(9} P(IIE ) ~ f l  P(distort t lE  )k=lConsidering the individual components of the exam-pie E, this leads us to the following.
(10) P(IIE) =zH P(dist?rt6iel '  e2 .
.
.
.
.
ep; ell, ef2 .
.
.
.
.
efq)6=14.5 Operator  Localization Assumpt ionFor the second assumption, we make the Assumptionthat the individual distortion operators only dependon the words and features that they directly involve.In effect, we stipulate that the operators only affecta strictly local portion of the input.
For example.we assume that the probability of echoing a worddepends only on the word itself, so that the followingholds:(11) P(echo-word(ew~)\[ewx ..... p; eh ..... q)P(echo-word(ew~))Similarly, we assume that the probability of e.g.deleting a feature depends only on the feature itself.so that the following holds:(12) P(delete-feature(ef~)\[ewz ..... p;efz ..... q)P(delete-  feature(el i  ))This yields the following approximation:(13) P(I\[E) ~,zH P(distort-word6 (ewi, iwj ))t= lYH P(distort- featuret (efi, ifj))1=1364.6 Computing the MatchGiven an input I and an expression E, it is straight-?
forward io determine the isrobability of the featuredistortion, since the features are indexed by name:Y(14) I I  P (d is tor t - featuret (ef i ,  i j))/=1In order to deternfine the probability of the worddistortions, we must find the most probable set ofdistortion operators.
Given an input and an exam-ple.
there are many different sets of distortion oper-ators that could relate the two.
Of course, we areinterested in the most straightforward relation be-tween the two, which corresponds to the least cost orhighest probability.
To further complicate matters,there may not be a single unique set of distortion op-erators with a unique minimum cost (correspondingto a unique maximum probability); instead, theremay be a number of distortion sets that all sharethe same minimal cost (and maximal probability).In this case, we are content o chose one of the min-imal cost sets at random.
This set is defined as fol-lows:(15) Distortmax = rnAxOi,tor t \[P(DistortiE, I)\]We solve this problem with a dynamic program-ming algorithm that finds a set of distortion oper-ators with maximal probability.
First, to obtain adistance me&sure, we take the negative logarithm ofthis expression:(16) -logP(DistortlE, I)Given that we have assumed independence be-tween individual distortion operators above, this canbe simplified as follows:no.
of operators(17) -log H P(d istortk l  E , I )k=lWe have also assumed that the distortion oper,ators are independent of the part of the sentencethat does not directly involve them.
Thus, we cansimplify further as follows:~g(18) -log H P(distort~jewi , iwi)/c=lThis can be further split into the individual dis-tortion operktors:(19) ~ -logP(distort~(ewi,iwi)k=lThis corresponds directly to the individual costs thatwe use for the dynamic programming equation.
Letthe example expression be E = et ,e2 , .
.
.
,ep  andand the input expression be I = it,J2 ..... i,~.
Then,let D(p, n) be the distance between the example andthe input.
This distance is defined by the followingrecurrence:f D (p-t,n-t),logP(echo(ewp))J D(p,n-1) -logP(add(iw~))D(p,n)=min ~ D(p-l,n) -logP(delete(ew~))I, D(p-l,n-1)-logP(alter( wp.
iw~ )}The result of this is the optimal alignment be-tween the input and the example, as well as the min-imum distance between them.
The matcher selectsthe example with the smallest distance to the input,and assembles the target language portions of the se-lected example pairs to form a complete translationin the target language.5 Probability EstimationThe method for speech translation by analogy de-scribed in this paper was designed to overcome themanual knowledge acquisition bottleneck by relyingon techniques from symbolic and statistical machinelearning, while still allowing the kind of manual tun-lag that is necessary to produce high-quality trans-lations.5.1 Prior Probability DistributionThe prior probability distribution over the exam-pie database P(Examples) is used to penalize highlyspecialized example pairs that should be used less of-ten.
After an initial distribution is estimated, theseprobabilities can be adjusted to solve translationproblems due to idiosyncratic exRmples.5.2 Alteration Probability DistributionThe two distortion operators a l te r -word  and a l ter -feature  perform the function of matching semanti-cally similar words or feature values.
If a monolin-gual or bilingual corpus from the application domainis available, these probability distributions can beestimated using iterative methods.
If neither typeof corpus is available, the probabilities can be esti-mated with the aid of a manually-constructed the-saurus .5.3 Thesaurus-based EstimationA thesaurus is a semantic IA-A hierarchy whosenodes are semantic ategories, and whose leaves arewords.
The traditional method of estimating wordsimilarity, based on counting IS-A links, presupposesthat every link encodes equal semantic distance -but in practice, this is never the case (Resnick,1995).
Thus, we adopt a new method for judgingsemantic distance between two words.
If appropri-ate distributional information for words is available.then the semantic similarity of two words could beestimated from the entropy of their lowest commondominating node lcdn:37Entropy(Root Node)(20) Similarity(ewi, iwj) ,~ Entropy(lcdn)In the absence Of distributional information, the en-tropy of a node depends only on the number of wordsthat the node dominates.5.4 Other Distort ion Probabil it iesThe probability distributions for adding and deletingwords and features can also be estimated from cor-pora.
if available.
Since there are very few Japanesespoken language corpora vailable, we are currentlyadopting a word-class based model for the remain-ing distributions that uses the categories for "strongcontent words" (nouns and verbs), "light contentwords" (adjectives and some adverbs), "grammaticalfimction words" (e.g.
particles and conjunctions),and "modifiers and adjuncts", In addition, it is pos-sible to a.~sign specific lexical penalties to individualwords.5.5 Learning Example PairsSince the contents of the database is central toachieving high-quality translations, it is usually nec-essary to adjust it manually in response to errors inthe translation.
At the same time, since the exam-ple databa,~e must be adapted for every new domain,it is important to minimize the amount of manualeffort.
For this reason, the example database was de-signed in such a way that it is possible to acquire newexamples by a semi-automatic method consisting ofan automatic extraction step from a bilingual corpus(see.
for example, (Watanabe, 1993)), followed by amanual filtering and refinement s ep.6 Conclusions and Further  WorkBa.~ed on the probabilistic model of analogicalmatching, we have implemented a prototype thattranslates from Japanese to English in a limited do-main.
The application domain for the prototype areexpressions for traveling in a foreign country, such asexpressions related to making reservations or diningin a restaurant.The initial results from our prototype are verypromising, but extension of system coverage andsubsequent large-scale valuation is needed.
Addi-tional topics for further work include the estimationof lexical probabilities from bilingual corpora; im-proving the integration between the speech recogni-tion and the translation components in order to in-crease recognition accuracy and translation robust-ness; and extending the system to additional lan-guages.ReferencesAntworth.
Evan L. 1990.
PC.KIMMO: a two-le~,elprocessor for morphological nalysis.
Number 16in Occasional Publications in Academic Cotnpltt-ing.
Summer Institute of Linguistics, Dalla.q.TX.Brown, Peter, Stephen A. Delia Pietra.
VincentJ.
Della Pietra.
and Robert L. Mercer.
1993.
Themathematics of statistical machine transalation:Parameter estimation.
Computational Linguis-tics, 19(2):263-312.Epstein, M., K. Papieni.
S. Roukos, T. Ward, andS.
Della Pietra.
1996.
Statistical natural an-guage understanding using hidden clunq)ings.
InICASSP-96, pages 176-179, Atlanta.
GA.Furuse, Osame and Hitoshi Iida.
1992.
An example-based method for transfer-driven machine trans-lation.
In Proceedings for the Fourth Interna-tional Conference on Theoretical and Method-ological Issues in Machine Translation (TMI-92).pages 139--150, Montreal, Canada, June.Furuse, Osamu and Hitoshi Iida.
1996.
Incremen-tal translation utilizing constituent boundary pat-terns.
In COLING.96, pages 412-417, Copen-hagen, Denmark.Hallo&an, W. 1996.
DECtalk software: Text-to-speech technology and implementation.
DigitalTechnical Journal, 7(4), March.Ishii, M., K. Ohta, and H. Salto.
1994.
An effi-cient parser generator for naturM language.
InCOLING-9~, pages 417-420, Kyoto, Japan.Jones, Daniel.
1996.
Analogical Natural LanguageProcessing.
UCL Press, London.Kurohashi, S., T. Na.kamura, Y. Matsumoto.
andM.
Nagao.
1994.
Improvements of Japanese mor-phological analyzer JUMAN.
In Proceedings ofthe International Workshop on Sharable NaturalLanguage Resources, pages 417-420, Nora, .lapan.Mayfield, L., M. Gavalda~ W. Ward, and A. Waibel.1995.
Concept-based speech translation.
InICASSP-95, pages 97-100, Detroit, MI.McKevitt, Paul, editor.
1994.
Workshop on Integra.tion of Natural Language and Speech Processing.Seattle, WA.
AAAI-94.Miller, S., R. Bobrow, R. Ingria, and R. Schwartz.1994.
Hidden understanding models of naturallanguage.
In A CL-3~, pages 25-32, La.s Cruces.NM.Morimoto, Tsuyoshi, Masami Suzuki, ToshiyukiTakezawa, Gen'ichiro Kikui, Massaki Nagata, andMutsuko Tomokiyo.
1992.
A spoken languagetranslation system: Sl-trans2.
In Proceedings ofthe fifteenth International Conference on Compu-tational Linguistics, volume 2, pages 1048-1052.Nantes.38Nagao, M. 1984.
A framework of a Machine Trans-lation between .lapanese and English by analogyprinciple.
In A. Elithorn and R. Banerji, editors,Artificial and Human Intelligence, pages 173-180.North-Holland.Pawley.
Andrew and Frances Hodgetts Syder.
1983.Two puzzles for linguistic theory: nativelike selec-tion and nativelike fluency.
In Jack C. Richardsand Richard W. Schmidt.
editors, Language andCommunication, pages 191-227.
Longman.Price, Patti.
1994.
Combining linguistic with sta-tistical methods in automatic speech understand-ing.
In Proceeding., off the work,ghop-The Balanc-ing Act: Combining ,symbolic and stati,~tical p-prnache., to language, pages 76-83, Las Cruces,New Mexico.R ayner, M., D. Carter, P. Price, and B. Lyberg.1994.
Estimating performance of pipelined spo-ken language translation systems.
In IC8LP-94,pages 1251-1254, Yokohama, Japan.R.esnick, P. 1995.
Using information content toevaluate semantic similarity in a taxonomy.
InIJCAI.95.Ringger, Eric K. and James F. Allen.
1996.
A fer-tility channel model for post-correction of con-tinuous speech recognition.
In \[CSLP-96, pages897-900, Philadelphia, PA.Sato, S. and IV\[.
Nagao.
1990.
Towards memory?based translation.
In COLING-90, pages 247-252, Helsinki, Finland.Shirotsuka, O. and K. Murakami.
1994.
Anexample-b&~ed approach to semantic informationextraction from Japanese spontaneous speech.
In\[CSLP-94, pages 91-94, Yokohama, Japan.Sobs.shims.
Y., O. Furuse, S. Akamine, J. Kawal,and H. Iida.
1994.
A bidirectional, transfer-driven machine translation system for spoken di-alogues.
In COLING-94, pages 64-68, Kyoto,Japan.Stemberger, Joseph P. 1982.
Syntactic errorsin speech.
Journal of Psycholingu?~tic Rsearch,11(4):313-333.Tomita, Masaru.
1985.
Efficient Parsing ffor NaturalLanguage.
Kluwer Academic Publishing, Boston,MA.Watanabe, H. 1992.
Similarity-driven transfer sys-tem.
In COLING-9~, pages 770--776.Watanabe, Hideo.
1993.
A method for extractingtranslation patterns from translation examples.In TMI-93, pages 292-301.39
