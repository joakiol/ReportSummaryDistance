Growing Semantic GrammarsMarsa l  Gava ldh  and  A lex  Waibe lIn teract ive  Systems Laborator iesCarneg ie  Mel lon  Un ivers i tyP i t t sburgh ,  PA  15213, U .S .A .marsal@cs, cmu.
eduAbstractA critical path in the development of natural languageunderstanding (NLU) modules lies in the difficulty ofdefining a mapping from words to semantics: Usually ittakes in the order of years of highly-skilled labor to de-velop a semantic mapping, e.g., in the form of a semanticgrammar, that is comprehensive enough for a given do-main.
Yet, due to the very nature of human language,such mappings invariably fail to achieve full coverage onunseen data.
Acknowledging the impossibility of stat-ing a priori all the surface forms by which a concept canbe expressed, we present GsG: an empathic computersystem for the rapid deployment of NLU front-ends andtheir dynamic customization by non-expert end-users.Given a new domain for which an NLU front-end is tobe developed, two stages are involved.
In the author-ing stage, GSQ aids the developer in the constructionof a simple domain model and a kernel analysis gram-mar.
Then, in the run-time stage, GSG provides the end-user with an interactive environment in which the kernelgrammar isdynamically extended.
Three learning meth-ods are employed in the acquisition of semantic mappingsfrom unseen data: (i) parser predictions, (ii) hidden un-derstanding model, and (iii) end-user paraphrases.
Abaseline version of GsG has been implemented and pre-llminary experiments show promising results.1 IntroductionThe mapping between words and semantics, be it inthe form of a semantic grammar, t or of a set of rulesthat transform syntax trees onto, say, a frame-slotstructure, is one of the major bottlenecks in the de-velopment of natural anguage understanding (NLU)systems.
A parser will work for any domain butthe semantic mapping is domain-dependent.
Evenafter the domain model has been established, thedaunting task of trying to come up with all thepossible surface forms by which each concept can1 Semantic grammars are grammars whose non-terminalscorrespond to semantic concepts (e.g., \[greeting\] or\ [ suggest .
t ime\ ]  ) rather than to syntactic onstituents (suchas Verb or WounPhrase).
They have the advantage that thesemant ics  of a sentence can be directly read off its parse tree,and the disadvantage that  a new grammar must be developedfor each domain.be expressed, still lies ahead.
Writing such map-pings takes in the order of years, can only be per-formed by qualified humans (usually computationallinguists) and yet the final result is often fragile andnon-adaptive.Following a radically different philosophy, we pro-pose rapid (in the order of days) deployment ofNLUmodules for new domains with on-need basis learn-ing: let the semantic grammar grow automaticallywhen and where it is needed.2 Grammar  deve lopmentIf we analyze the traditional method of developinga semantic grammar for a new domain, we find thatthe following stages are involved.1.
Data collection.
Naturally-occurring data fromthe domain at hand are collected.2.
Design of the domain model.
A hierarchicalstructuring of the relevant concepts in the do-main is built in the form of an ontology or do-main model.3.
Development of a kernel grammar.
A grammarthat covers a small subset of the collected atais constructed.4.
Expansion of grammar coverage.
Lengthy, ar-duous task of developing the grammar to extendits coverage over the collected ata and beyond.5.
Deployment.
Release of the final grammar forthe application at hand.The GsG system described in this paper aids all butthe first of these stages: For the second stage, wehave built a simple editor to design and analize theDomain Model; for the third, a semi-automated wayof constructing the Kernel Grammar; for the fourth,an interactive nvironment in which new semanticmappings are dynamically acquired.
As for the fifth(deployment), it advances one place: after the shortinitial authoring phase (stages 2 and 3 above) thefinal application can already be launched, since thesemantic grammar will be extended, at run-time, bythe non-expert end-user.3 System arch i tec tureAs depicted in Fig.
1, GsG is composed of the fol-lowing modules: the Domain Model Editor and the451authoring stagerun.~me s tage.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Figure 1: System architecture of GSG.Kernel Grammar Editor, for the authoring stage,and the SouP parser and the IDIGA environment,for the run-time stage.3.1 Author ing stageIn the authoring stage, a developer s creates the Do-main Model (DM) with the aid of the DM Editor.In our present formalism, the DM is simply a di-rected acyclic graph in which the vertices correspondto concept-labels and the edges indicate concept-subconcept relations (see Fig.
2 for an example).Once the DM is defined, the Kernel Grammar Ed-itor drives the development of the Kernel Grammarby querying the developer to instantiate into gram-mar rules the rule templates derived from the DM.For instance, in the DM in Fig.
2, given that con-cept {suggest_time} requires ubconcept \[time\],the rule template \[suggest_time\] < \[time\] isgenerated, which the developer can instantiate into,say, rule (2) in Fig.
3.The Kernel Grammar Editor follows a concrete-to-abstract ordering of the concepts obtained via atopological sort of the DM to query the developer,after which the Kernel Grammar is complete 3 and2Understood here as a qualified person (e.g., knowledgeengineer or software developer) who is familiar with the do-main at hand and has access to some sample sentences thatthe NLU front-end is supposed to understand.3We say that grammar G is complete with respect o do-main model DM if and only if for each arc from concept i toconcept j in DM there is at least one grammar ule headedby concept i that contains concept j .
This ensures that anyidea expressible in DM has a surface form, or, seen it fromanother angle, that any in-domain utterance has a paraphrase452\ [g reet ing \ ]  \ [ fa rewe l l \ ]- .
o-\[namel{suggest ion l  \[rejectionl \ [acceptance\ ]T v ~\ [ suggest_ t ime l  \ [ re jec t  eime\] {accept_t imel\[ t ime }\ [ in terva l}  ?
{s tar t _po in t}  \[end..point} ',{point}\[ day_of  week } \[ t ime_o f_day IFigure 2: Fragment of a domain model for a schedul-ing task.
A dashed edge indicates optional subconcept(default is required), a dashed angle indicates inclusivesubconcepts (default is exclusive).
(1) \[suggestion\] ~-- {suggest_time}(2) {suggest_time} ~-- how about \[time\](3) \[time\] ~ \[point\](4) \[point\] 4---- *on {day_of_week} *{time_of_day}(5) {day_of_week} ~--- Tuesday(6) {time_of_day} 6--- afternoonFigure 3: Fragment of a grammar for a scheduling task.A '*' indicates optionality.the NLU front-end is ready to be deployed.It is assumed that: (i) after the authoring stagethe DM is fixed, and (ii) the communicative goal ofthe end-user is expressible in the domain.3.2 Run- t ime stageInstead of attempting "universal coverage" we ratheraccept he fact that one can never know all the sur-face forms by which the concepts in the domain canbe expressed.
What GsG provides in the run-timestage are mechanisms that allow a non-expert end-user to "teach" the meaning of new expressions.The tight coupling between the SouP parser 4 andthe IDIGA s environment allows for a rapid and multi-faceted analysis of the input string.
If the parse, orrather, the paraphrase automatically generated byGSG 6, is deemed incorrect by the end-user, a learn-ing episode nsues.that is covered by G.4Very fast, stochastic top-down chart parser developed bythe first author incorporating heuristics to, in this order, max-imize coverage, minimize tree complexity and maximize treeprobability.5Acronym for interactive, distributed, incremental gram-mar acquisition.6In order for all the interactions with the end-user to beperformed in natural anguage only, a generation grammaris needed to transform semantic representations i to surfaceforms.
To that effect GSG is able to cleverly use the analysisgrammar in "reverse.
"By bringing to bear contextual constraints, Gsocan make predictions as to what a sequence of un-parsed words might mean, thereby exhibiting an"empathic" behavior toward the end-user.
To thisaim, three different learning methods are employed:parser predictions, hidden understanding model,and end-user paraphrases.3.2.1 LearningSimilar to Lehman (1989), learning in GsQ takesplace by the dynamic reation of grammar rules thatcapture the meaning of unseen expressions, and bythe subsequent update of the stochastic models.
Ac-quiring a new mapping from an unparsed sequenceof words onto its desired semantic representation in-volves the following steps.1.
Hypothesis formation and filtering.
Given thecontext of the sentence at hand, Gsc constructshypotheses in the form of parse trees that coverthe unparsed sequence, discards those hypothe-ses that are not approved by the DM r and ranksthe remaining by likelihood.2.
Interaction with the end-user.
The ranked hy-potheses are presented to the end-user in theform of questions about, or rephrases of, theoriginal utterance.3.
Dynamic rule creation.
If the end-user is sat-isfied with one of the options, a new grammarrule is dynamically created and becomes partof the end-user's grammar until further notice.Each new rule is annotated with the learningepisode that gave rise to it, including end-userID, time stamp, and a counter that will keeptrack of how many times the new rule fires insuccessful parses,3.2.2 Parser predict ionsAs suggested by Kiyono and Tsujii (1993), one canmake use of parse failures to acquire new knowledge,both about the nature of the unparsed words andabout he inadequacy of the existing rammar rules.GsG uses incomplete parses to predict what cancome next (i.e.
after the partially-parsed sequence7I.e., parse trees containing concept-subconcept relationsthat are inconsistent with the stipulations of the DM.SThe degree of generalization r level o.f abstraction thata new rule should exhibit is an open question but currently aPrinciple of Maximal Abstraction is followed:(a) Parse the lexical items of the new rule's right-hand-sidewith all concepts granted top-level status, i.e., able tostand at the root of a parse tree.
(b) If a word is not covered by any tree, take it as is intothe final right-hand side.
Else, take the root of the parsetree with largest span; if tie, prefer the root that rankshigher in the DM.For example, with the DM in Fig.
2 and the grammar in Fig.
3,What about Tuesdayf  is abstracted to the maximally generalwhat about \[time\] (as opposed to what about \[day_of_week\]or what about \ [point \ ] ) .453Figure 4: Example of a learning episode using parserpredictions.
Initially only the temporal expression is un-derstood...in left-to-right parsing, or before the partially-parsedsequence in right-to-left parsing).
This allows twokinds of grammar acquisition:1.
Discovery of expression equivalence.
E.g., withthe grammar in Fig.
3 and input sentence Whatabout Tuesday afternoon?
GsQ is able to askthe end-user whether the utterance means thesame as How about Tuesday afternoon?
(SeeFigs.
4, 5 and 6).
That is because in the pro-cess of parsing What about Tuesday afternoon?right-to-left, he parser has been able to matchrule (2) in Fig.
2 up to about, and thus ithypothesizes the equivalence of what and howsince that would allow the parse to complete.
92.
Discovery of an ISA relation.
Similarly, frominput sentence How about noon?
GsG is ableto predict, in left-to-right parsing, that noon isa \[time\].3.2.3 Hidden unders tand ing  modelAs another way of bringing contextual informationto bear in the process of predicting the meaning9For real-world grammars, of, say, over 1000 rules, it isnecessary to bound the number of partial parses by enforcinga maximum beam size at the left-hand side level, i.e., placinga limit on the number of subparses under each nonterminalto curb the exponential explosion.YN NO :"; - "  " "<iFigure 5: ...but a correct prediction is made...Pmdoes  .Sin~ n?~~Vhat about Tuesday aftar~ooo?What ~t  Tuesaay aftemo~?
II*-\[ su : JgosLt t l \ ]I+ - - , l s i tI?-aboutI+- \ [ t lm\ ]I+-\[polntlI?- \[ day_of_woek lI II +-ttmldayI4.-\[ t i i .
.
e l _day \ ]Il l u toml~ Ref i l la, hat i~ut  ~ue~l~ aftemoon i iok I f  8,a ---qL... Z..J ......... ;lst~a~LlJ, '~ } <- -  ",,mat about \[ume\] {IFigure 6: ...and a new rule is acquired.of unparsed words, the following stochastic models,inspired in Miller et al (1994) and Seneff (1992),and collectively referred to as hidden understandingmodel (HUM), are employed.?
Speech-act n-gram.
Top-level concepts can beseen as speech acts of the domain.
For instance,in the DM in Fig.
2 top-level concepts uchas \[greeting\], Cfarewell\] or \[suggestion\],correspond to discourse speech acts, and innormally-occurring conversation, they follow adistribution that is clearly non-uniform.
1??
Concept-subconcept HMM.
Discrete hiddenMarkov model in which the states correspondl?Needless to say, speech-act ransition distributionsare empirically estimated, but, intuitively, the sequence<\[greet ing \ ] ,  [suggest ion\]> is more likely than the se-quence < \[greeting\], \ [ fa rewel l \ ]>.to the concepts in the DM (i.e., equivalent togrammar non-terminals) and the observationsto the embedded concepts appearing as imme-diate daughters of the state in a parse tree.For example, the parse tree in Fig.
4 containsthe following set of <state, observation> pairs:{< \[time\], \[point\] >, < \[point\], \[day_of_week\] >,< \[point\], \[time_of_day\] >}.?
Concept-word HMM.
Discrete hidden Markovmodel in which the states correspond to the con-cepts in the DM and the observations tothe em-bedded lexical items (i.e., grammar terminals)appearing as immediate daughters of the statein a parse tree.
For example, the parse treein Fig.
4 contains the pairs: {<\[day_of_week\],tuesday>, < \[time_of_day\], afternoon>}.The HUM thus attempts to capture the recurringpatterns of the language used in the domain in anasynchronous mode, i.e., independent ofword order(as opposed to parser predictions that heavily de-pend on word order).
Its aim is, again, to providepredictive power at run-time: upon encountering anunparsable expression, the HUM hypothesizes possi-ble intended meanings in the form of a ranked list ofthe most likely parse trees, given the current state inthe discourse, the subparses for the expression andthe lexical items present in the expression.Its parameters can be best estimated throughtraining over a given corpus of correct parses, butin order not to compromise our established goal ofrapid deployment, we employ the following tech-niques.1.
In the absence of a training corpus, the HUMparameters are seeded from the Kernel Gram-mar itself.2.
Training is maintained at run-time through dy-namic updates of all model parameters aftereach utterance and learning episode.3.2.4 End-user paraphrasesIf the end-user is not satisfied with the hypothesespresented by the parser predictions or the HUM, athird learning method is triggered: learning froma paraphrase of the original utterance, given alsoby the end-user.
Assuming the paraphrase isunderstood, 11 GsG updates the grammar in such afashion so that the semantics of the first sentenceare equivalent to those of the paraphrase.
1211 Precisely, the requirement that the grammar be complete(see note 3} ensures the existence of a suitable paraphrase forany utterance expressible in the domain.
In practice, however,it may take too many attempts to find an appropriate para-phrase.
Currently, if the first paraphrase is not understood,no further equests are made.12Presently, the root of the paraphrase's parse tree directlybecomes the left-hand-side of the new rule.454Perfect Ok BadExpert before 55.41 17.58 27.01Expert after 75.68 10.81 13.51A +?0.
?7 --6.77 --13.50End-user1 before 58.11 18.92 22.97End-user1 after 64.86 22.97 12.17A +6.75 +.~.05 --10.80End-user2 before 41.89 16.22 41.89End-user2 after 48.64 28.38 22.98A +6.75 +1?.16 --18.91Table 1: Comparison of parse grades (in %).
Expertusing traditional method vs. non-experts using GSG.4 Preliminary resultsWe have conducted a series of preliminary exper-iments in different languages (English, German andChinese) and domains (scheduling, travel reserva-tions).
We present here the results for an experimentinvolving the comparison of expert vs. non-expertgrammar development on a spontaneous travel reser-vation task in English.
The grammar had been de-veloped over the course of three months by a full-time expert grammar writer and the experiment con-sisted in having this expert develop on an unseenset of 72 sentences using the traditional environmentand asking two non-expert users is to "teach" Gs6the meaning of the same 72 sentences through in-teractions with the system.
Table 1 compares thecorrect parses before and after development.It took the expert 15 minutes to add 8 rules andreduce bad coverage from 27.01% to 13.51%.
Asfor the non-experts, end-user1, starting with a sim-ilar grammar, reduced bad parses from 22.97% to12.17% through a 30-minute session 14 with GsG thatgave rise to 8 new rules; end-user2, starting with thesmallest possible complete grammar, reduced badparses from 41.89% to 22.98% through a 35-minutesession 14 that triggered the creation of 17 new rules.60% of the learning episodes were successful, withan average number of questions of 2.91.
The unsuc-cessful learning episodes had an average number ofquestions of 6.19 and their failure is mostly due tounsuccessful paraphrases.As for the nature of the acquired rules, they dif-fer in that the expert makes use of optional and re-peatable tokens, an expressive power not currentlyavailable to GSG.
On the other hand this lack ofgenerality can be compensated by the Principle ofMaximal Abstraction (see note 8).
As an example,to cover the new construction And your last name?,the expert chose to create the rule:\[requestmame\] ~ *and your last nametSUndergraduate s udents not majoring in computer sci-ence or linguistics.14 Including a 5-minute introduction.whereas both end-user1 and end-users induced theautomatic acquisition of the rule:\[requostmame\] ~ CONJ POSS \[last\] name.
155 DiscussionAlthough preliminary and limited in scope, theseresults are encouraging and suggest hat grammardevelopment by non-experts through GsG is indeedpossible and cost-effective.
It can take the non-expert wice as long as the expert o go through a setof sentences, but the main point is that it is possibleat all for a user with no background in computer sci-ence or linguistics to teach Gso the meaning of newexpressions without being aware of the underlyingmachinery.Potential applications of GSG are many, most no-tably a very fast development of NLU componentsfor a variety of tasks including speech recognitionand NL interfaces.
Also, the IDIGA environmentenhances the usability of any system or applicationthat incorporates it, for the end-users are able to eas-ily "teach the computer" their individual anguagepatterns and preferences.Current and future work includes further develop-ment of the learning methods and their integration,design of a rule-merging mechanism, comparisonof individual vs. collective grammars, distributedgrammar development over the World Wide Web,and integration of GSG's run-time stage into theJANUS speech recognition system (Lavie et al 1997).AcknowledgementsThe work reported in this paper was funded in part bya grant from ATR Interpreting Telecommunications Re-search Laboratories of Japan.ReferencesKiyono, Masaki and Jun-ichi Tsujii.
1993.
"Linguisticknowledge acquisition from parsing failures."
In Pro-ceedings of the 6th Conference of the European Chap-ter of the A CL.Lavie, Alon, Alex Waibel, Lori Levin, Michael Finke,Donna Gates, Marsal Gavaldh, Torsten Zeppenfeld,and Puming Zhan.
1997.
"JANus IIh speech-to-speech translation i  multiple languages."
In Proceed-ings of ICASSP-97.Lehman, Jill Fain.
1989.
Adaptive parsing: Self-extending natural anguage interfaces.
Ph.D. disserta-tion, School of Computer Science, Carnegie MellonUniversity.Miller, Scott, Robert Bobrow, Robert Ingria, andRichard Schwartz.
1994.
"Hidden understanding mod-els of natural anguage."
In Proceedings of ACL-9$.Seneff, Stephauie.
1992.
"TINA: a natural anguage sys-tem for spoken language applications."
In Computa-tional Linguistics, vol.
18, no.
1, pp.
61-83.15Uppercased nonterminals ( uch as COIJ and POSS) aremore syntactical in nature and do not depend on the DM.455ResumUn dels camins critics en el desenvolupamentde mbduls de comprensi6 del llenguatge naturalpassa per la dificultat de definir la funci6 queassigna, a una seqii~ncia de mots, la representaci6sem~ntica desitjada.
Els m~todes tradicionals perdefinir aquesta correspond~ncia requereixen l'esforqde lingiiistes computacionals, que dediquen mesos o~dhuc anys construint, per exemple, una gram~ticasem~ntica (formalisme en el qual els s~mbols no ter-minals de la gram~tica corresponen directament alsconceptes del domini de l'aplicaci6 determinada), i,tanmateix, degut precisament a la prbpia natura delllenguatge hum~, la gram~tica resultant mai no 4scapaq de cobrir tots els mots i expressions que ocor-ren naturalment al domini en qiiesti6.Reconeixent per tant la impossibilitat d'establir apriori totes les formes uperficials amb qu~ un con-cepte pot ser expressat, presentem en aquest tre-ball GsG: un sistema computacional emp~tic peral r~pid desplegament dembduls de comprensi6 delllenguatge natural i llur adaptaci6 din&mica a lesparticularitats i prefertncies d'usuaris finals inex-perts.El proc4s de construcci6 d'un mbdul de com-prensi6 del llenguatge natural per a un nou dominipot set dividit en dues parts.
Primerament, durantla fase de composici5, GsG ajuda el desenvolupadorexpert en l'estructuraci6 dels conceptes del domini(ontologia) i en l'establiment d'una gram&tica mi-nimal.
Tot seguit, durant la fase d'execuci5, Gs~forneix l'usuari final inexpert d'un medi interactiuen qu& la gram&tica 4s augmentada in&micament.Tres m~todes d'aprenentatge autom&tic s6n uti-litzats en l'adquisici6 de regles gramaticals a partirde noves frases i construccions: (i) prediccions del'analitzador (GSG empra an&lisis incompletes perconjecturar quins roots poden apar&ixer tant desprdsde l'arbre d'anMisi ncomplet, en anMisi d'esquerraa dreta, corn abans de l'arbre d'anMisi ncomplet, enanMisi de dreta a esquerra), (ii) cadenes de Markov(m~todes estochstics que modelen, independentmentde l'ordre dels mots, la distribuci6 dels conceptes illurs transicions, emprats per calcular el concepteglobal m4s probable donats un context i uns arbresd'anMisi parcials determinats), i (iii) par&frasis (em-prades per assignar llur representaci6 sem&ntica lafrase original).Hem implementat una primera versi6 de GsG i elsresultats obtinguts, per b4 que preliminars, 6n benencoratjadors car demostren que un usuari nexpertpot "ensenyar" a GsG el significat de noves expres-sions i causar una extensi6 de la gram&tica compa-rable a la d'un expert.Actualment estem treballant en la millora delsm&todes autom&tics d'aprenentatge i llur inte-graci6, en el disseny d'un mecanisme de corn-binaci6 autom~tica de regles gramaticals, enla comparaci6 de gram&tiques individuals ambgram&tiques col.lectives, en el desenvolupamentdistribu'it de gram~tiques a trav4s de la WorldWide Web, i en la integraci6 de la fased'execuci6 de GsG en el sistema de reconeixe-ment de la parla i traducci6 autom~tica JANUS.456
