INTEGRATING MULT IPLE  KNOWLEDGE SOURCES FORDETECTION AND CORRECTION OF REPAIRS  INHUMAN-COMPUTER D IALOG*John Bear, John Dowding, Elizabeth Shriberg tSRI Internat ionalMenlo Park, California 94025ABSTRACTWe have analyzed 607 sentences of sponta-neous human-computer speech data containing re-pairs, drawn from a total corpus of 10,718 sen-tences.
We present here criteria nd techniques forautomatically detecting the presence of a repair,its location, and making the appropriate correc-tion.
The criteria involve integration of knowledgefrom several sources: pattern matching, syntacticand semantic analysis, and acoustics.INTRODUCTIONSpontaneous spoken language often includesspeech that is not intended by the speaker to bepart of the content of the utterance.
This speechmust be detected and deleted in order to correctlyidentify the intended meaning.
The broad classof disfluencies encompasses a number of phenom-ena, including word fragments, interjections, filledpauses, restarts, and repairs.
We are analyzingthe repairs in a large subset (over ten thousandsentences) of spontaneous speech data collectedfor the DARPA Spoken Language Program3 Wehave categorized these disfluencies as to type andfrequency, and are investigating methods for theirautomatic detection and correction.
Here we re-port promising results on detection and correctionof repairs by combining pattern matching, syn-tactic and semantic analysis, and acoustics.
Thispaper extends work reported in an earlier paper*This research was supported by the Defense AdvancedResearch Projects Agency under Contract ONR N00014-90-C-0085 with the Office of Naval Research.
It was alsosupported by a Grant, NSF IRI-8905249, from the NationalScience Foundation.
The views and conclusions containedin this document are those of the authors and should notbe interpreted as necessarily representing the official poll-cies, either expressed or implied, of the Defense AdvancedResearch Projects Agency of the U.S. Government, or ofthe National Science Foundation.tEl izabeth Shriberg is also affiliated with the Depart-ment of Psychology at the University of California atBerkeley.1DARPA is the Defense Advanced Research ProjectsAgency of the United States Government56(Shriberg et al, 1992a).The problem of disfluent speech for languageunderstanding systems has been noted but hasreceived limited attention.
Hindle (1983) at-tempts to delimit and correct repairs in sponta-neous human-human dialog, based on transcriptscontaining an "edit signal," or external and reli-able marker at the "expunction point," or point ofinterruption.
Carbonell and Hayes (1983) brieflydescribe recovery strategies for broken-off andrestarted utterances in textual input.
Ward (1991)addresses repairs in spontaneous speech, but doesnot attempt to identify or correct hem.
Our ap-proach is most similar to that of Hindle.
It differs,however, in that we make no assumption aboutthe existence of an explicit edit signal.
As a reli-able edit signal has yet to be found, we take it asour problem to find the site of the repair automat-ically.It is the case, however, that cues to repair existover a range of syllables.
Research in speech pro-duction has shown that repairs tend to be markedprosodically (Levelt and Cutler, 1983) and thereis perceptual evidence from work using lowpass-filtered speech that human listeners can detect heoccurrence of a repair in the absence of segmentalinformation (Lickley, 1991).In the sections that follow, we describe in de-tail our corpus of spontaneous speech data andpresent an analysis of the repair phenomena ob-served.
In addition, we describe ways in whichpattern matching, syntactic and semantic analy-sis, and acoustic analysis can be helpful in detect-ing and correcting these repairs.
We use patternmatching to determine an initial set of possiblerepairs; we then apply information from syntac-tic, semantic, and acoustic analyses to distinguishactual repairs from false positives.THE CORPUSThe data we are analyzing were collectedas part of DARPA's Spoken Language Systemsproject.
The corpus contains digitized waveformsand transcriptions of a large number of sessions inwhich subjects made air travel plans using a com-puter.
In the majority of sessions, data were col-lected in a Wizard of Oz setting, in which subjectswere led to believe they were talking to a com-puter, but in which a human actually interpretedand responded to queries.
In a small portion ofthe sessions, data were collected using SRI's Spo-ken Language System (Shriberg et al, 1992b), inwhich no human intervention was involved.
Rel-evant to the current paper is the fact that al-though the speech was spontaneous, it was some-what planned (subjects pressed a button to beginspeaking to the system) and the transcribers whoproduced lexical transcriptions ofthe sessions wereinstructed to mark words they inferred were ver-bally deleted by the speaker with special symbols.For further description of the corpus, see MAD-COW (1992).NOTATIONIn order to classify these repairs, and to facil-itate communication among the authors, it wasnecessary to develop a notational system thatwould: (1) be relatively simple, (2) capture suf-ficient detail, and (3) describe the vast majorityof repairs observed.
Table 1 shows examples ofthe notation used, which is described fully in Bearet al (1992).The basic aspects of the notation includemarking the interruption point, the extent ofthe repair, and relevant correspondences betweenwords in the region.
To mark the site of a re-pair, corresponding to Hindle's "edit signal" (Hin-die, 1983), we use a vertical bar (I)- To expressthe notion that words on one side of the repaircorrespond to words on the other, we use a com-bination of a letter plus a numerical index.
Theletter M indicates that two words match exactly.R indicates that the second of the two wordswas intended by the speaker to replace the first.The two words must be similar-either of the samelexical category, o r  morphological variants of thesame base form (including contraction pairs like"I/ I 'd").
Any other word within a repair is no-tated with X.
A hyphen affixed to a symbol in-dicates a word fragment.
In addition, certain cuewords, such as "sorry" or "oops" (marked withCR) as well as filled pauses (CF) are also labeled57I want fl- flights to boston.M1 - I M1what what are the faresM1 I M,show me flights daily flightsM, \[ X M1I want a flight one way flightM1 \] X X M1I want to leave depart before .
.
.R1 \] R1what are what are the faresM, M2 \[ M1 M2.. .
fly to boston from bostonR, M1 \[ R1 M1.
.
.
fly from boston from denverM1 R1 \[ M1 R1what are are there any flightsx x \[Table 1: Examples of Notationif they occur immediately before the site of a re-pair.DISTRIBUT IONOf the 10,000 sentences in our corpus, 607 con-tained repairs.
We found that 10% of sentenceslonger than nine words contained repairs.
In con-trast, Levelt (1983) reports a repair rate of 34% forhuman-human dialog.
While the rates in this cor-pus are lower, they are still high enough to be sig-nificant.
And, as system developers move towardmore closely modeling human-human i teraction,the percentage is likely to rise.Although only 607 sentences contained dele-tions, some sentences contained more than one,for a total of 646 deletions.
Table 2 gives thebreakdown of deletions by length, where lengthis defined as the number of consecutive deletedwords or word fragments.
Most of the deletionsDeletion Length Occurrences Percentage1 376 59%2 154 24%3 52 8%4 25 4%5 23 4%6+ 16 3%Table 2: Distribution of Repairs by LengthType Pattern Freq.Length 1 RepairsFragments MI - ,  R I - ,  X -  61%Repeats M1 \[M1 16%Insertions M1 \[ X1 ... XiM1 7%Replacement R1 \[ R1 9%Other X\[X 5%Length 2 RepairsRepeats M1 M2 \[ M1 M2 28%Replace 2nd M1 R1 \[ M1 R1 27%Insertions M1 M2 \[MIX1 ... Xi M2 19%Replace 1st R1 M1 \[ R1 M1 10%Other ...\[... 17%Table 3: Distribution of Repairs by TypeMatchLength234Fill Length0 1 2 3.82 .74 .69 .28?
(39) (65) (43) (39)1.0 .83 .73 .00(10) (6) (11) (1)1.0 .80 1.0(4) (5) (2)1.0 1.0(2) (1)- -  indicates no observationsTable 4: Fill Length vs.
Match Lengthwere fairly short; deletions of one or two words ac-counted for 82% of the data.
We categorized thelength 1 and length 2 repairs according to theirtranscriptions.
The results are summarized in Ta-ble 3.
For simplicity, in this table we have countedfragments (which always occurred as the seconddeleted word) as whole words.
The overall rate offragments for the length 2 repairs was 34%.A major repair type involved matching stringsof identical words.
More than half (339 out of 436)of the nontrivial repairs (more editing necessarythan deleting fragments and filled pauses) in thecorpus were of this type.
Table 4 shows the distri-butions of these repairs with respect o two param-eters: the length in words of the matched string,and the number of words between the two matchedstrings.
Numbers in parentheses indicate the num-ber of occurrences, and probabilities represent thelikelihood that the phrase was actually a repairand not a false positive.
Two trends emerge fromthese data.
First, the longer the matched string,the more likely the phrase was a repair.
Second,the more words there were intervening between thematched strings, the less likely the phrase was arepair.S IMPLE  PATTERN MATCHINGWe analyzed a subset of 607 sentences con-taining repairs and concluded that certain sim-ple pattern-matching techniques could successfullydetect a number of them.
The pattern-matching58component reported on here looks for identical se-quences of words, and simple syntactic anomalies,such as "a the" or "to from.
"Of the 406 sentences containing nontrivial re-pairs, the program successfully found 309.
Ofthese it successfully corrected 177.
There were 97sentences that contained repairs which it did notfind.
In addition, out of the 10,517 sentence corpus(10,718 - 201 trivial), it incorrectly hypothesizedthat an additional 191 contained repairs.
Thus of10,517 sentences of varying lengths, it pulled out500 as possibly containing a repair and missed 97sentences actually containing a repair.
Of the 500that it proposed as containing a repair, 62% actu-ally did and 38% did not.
Of the 62% that had re-pairs, it made the appropriate correction for 57%.These numbers show that although patternmatching is useful in identifying possible repairs,it is less successful at making appropriate correc-tions.
This problem stems largely from the over-lap of related patterns.
Many sentences contain asubsequence of words that match not one but sev-eral patterns.
For example the phrase "FLIGHT<word> FLIGHT" matches three different pat-terns:show the flight time flight dateM1 R1 \[ M1 R1show the flight earliest flightM1 \[ X M1show the delta f l ight united f l ightR1 M1 \[ ~I~l M1Each of these sentences i a false positive forthe other two patterns.
Despite these problemsof overlap, pattern matching is useful in reducingthe set of candidate sentences to be processed forrepairs.
Rather than applying detailed and pos-sibly time-intensive analysis techniques to 10,000sentences, we can increase efficiency by limitingourselves to the 500 sentences selected by the pat-tern matcher, which has (at least on one measure)a 75% recall rate.
The repair sites hypothesizedby the pattern matcher constitute useful input forfurther processing based on other sources of infor-mation.NATURAL LANGUAGECONSTRAINTSHere we describe two sets of experiments tomeasure the effectiveness of a natural languageprocessing system in distinguishing repairs fromfalse positives.
One approach is based on parsingof whole sentences; the other is based on parsinglocalized word sequences identified as potential re-pairs.
Both of these experiments rely on the pat-tern matcher to suggest potential repairs.The syntactic and semantic omponents of theGemini natural language processing system areused for both of these experiments.
Gemini isan extensive reimplementation f the Core Lan-guage Engine (Alshawi et al, 1988).
It includesmodular syntactic and semantic omponents, inte-grated into an efficient all-paths bottom-up arser(Moore and Dowding, 1991).
Gemini was trainedon a 2,200-sentence subset of the full 10,718-sentence corpus.
Since this subset excluded theunanswerable s ntences, Gemini's coverage on thefull corpus is only an estimated 70% for syntax,and 50% for semantics.
2Global Syntax and SemanticsIn the first experiment, based on parsing com-plete sentences, Gemini was tested on a subsetof the data that the pattern matcher eturned aslikely to contain a repair.
We excluded all sen-tences that contained fragments, resulting in a2Gemlni 's  syntact ic  coverage of the 2,200-sentencedataset  it was t ra ined on (the set of annotated  and an-swerable MADCOW queries) is approximate ly  91~,  whileits semant ic  overage is approximate ly  77%.
On a recentfair test, Gemini 's  syntact ic  overage was 87~0 and  seman-tic coverage was 71%.Syntax OnlyMarked Markedas  asRepair False PositiveRepairs 68 (96%) 56 (30%)False Positives 3 (4%) 131 (70%)Syntax and SemanticsMarked Markedas  asRepair False PositiveRepairs 64 (85%) 23 (20%)False Positives 11 (15%) 90 (80%)Table 5: Syntax and Semantics Results59dataset of 335 sentences, of which 179 containedrepairs and 176 contained false positives.
The ap-proach was as follows: for each sentence, parsingwas attempted.
If parsing succeeded, the sentencewas marked as a false positive.
If parsing did notsucceed, then pattern matching was used to detectpossible repairs, and the edits associated with therepairs were made.
Parsing was then reattempted.If parsing succeeded at this point, the sentence wasmarked as a repair.
Otherwise, it was marked asno opin ion.Table 5 shows the results of these experiments.We ran them two ways: once using syntactic on-straints alone and again using both syntactic andsemantic constraints.
As can be seen, Geminiis quite accurate at detecting a repair, althoughsomewhat less accurate at detecting a false posi-tive.
Furthermore, in cases where Gemini detecteda repair, it produced the intended correction in 62out of 68 cases for syntax alone, and in 60 out of64 cases using combined syntax and semantics.
Inboth cases, a large number of sentences (29% forsyntax, 50% for semantics) received a no op in ionevaluation.
The no op in ion  cases were evenlysplit between repairs and false positives in bothtests.The main points to be noted from Table 5 arethat with syntax alone, the system is quite ac-curate in detecting repairs, and with syntax andsemantics working together, it is accurate at de-tecting false positives.
However, since the coverageof syntax and semantics will always be lower thanthe coverage of syntax alone, we cannot comparethese rates directly.Since multiple repairs and false positives canoccur in the same sentence, the pattern matchingprocess is constrained to prefer fewer repairs tomore repairs, and shorter epairs to longer repairs.This is done to favor an analysis that deletes thefewest words from a sentence.
It is often the casethat more drastic repairs would result in a syntac-tically and semantically well-formed sentence, butnot the sentence that the speaker intended.
Forinstance, the sentence "show me <flights> dailyflights to boston" could be repaired by deletingthe words "flights daily," and would then yield agrammatical sentence, but in this case the speakerintended to delete only "flights.
"Local  Syntax  and  Semant icsIn the second experiment we attempted to im-prove robustness by applying the parser to smallsubstrings of the sentence.
When analyzing longword strings, the parser is more likely to fail dueto factors unrelated to the repair.
For this ex-periment, the parser was using both syntax andsemantics.The phrases used for this experiment were thephrases found by the pattern matcher to containmatching strings of length one, with up to threeintervening words.
This set was selected because,as can be seen from Table 4, it constitutes a largesubset of the data (186 such phrases).
Further-more, pattern matching alone contains insufficientinformation for reliably correcting these sentences.The relevant substring is taken to be thephrase constituting the matched string plus in-tervening material plus the immediately precedingword.
So far we have used only phrases where thegrammatical category of the matched word was ei-ther noun or name (proper noun).
For this test wespecified a list of possible phrase types (NP, VP,PP, N, Name) that count as a successful parse.
Weintend to run other tests with other grammaticalcategories, but expect that these other categoriescould need a different heuristic for deciding whichsubstring to parse, as well as a different set of ac-ceptable phrase types.Four candidate strings were derived from theoriginal by making the three different possibleedits, and also including the original string un-changed.
Each of these strings was analyzed bythe parser.
When the original sequence did not60parse, but one of edits resulted in a sequence thatparsed, the original sequence was very unlikely tobe a false positive (right for 34 of 35 cases).
Fur-thermore, the edit that parsed was chosen to bethe repaired string.
When more than one of theedited strings parsed, the edit was chosen by pre-ferring them in the following order: (1) M1\]XM1,(2) R1MIIR1M1, (3) M1RI\[M1R1.
Of the 37 casesof repairs, the correct edit was found in 27 cases,while in 7 more an incorrect edit was found; in3 cases no op in ion  was registered.
While thesenumbers are quite promising, they may improveeven more when information from syntax and se-mantics is combined with that from acoustics.ACOUSTICSA third source of information that can be help-ful in detecting repairs is acoustics.
In this sec-tion we describe first how prosodic information canhelp in distinguishing repairs from false positivesfor patterns involving matched words.
Second, wereport promising results from a preliminary studyof cue words such as "no" and "well."
And third,we discuss how acoustic information can aid inthe detection of word fragments, which occur fre-quently and which pose difficulty for automaticspeech recognition systems.Acoustic features reported in the followinganalyses were obtained by listening to the soundfiles associated with each transcription, and byinspecting waveforms, pitch tracks, and spectro-grams produced by the Entropic Waves softwarepackage.S imple  Pat ternsWhile acoustics alone cannot tackle the prob-lem of locating repairs, since any prosodic patternsfound in repairs are likely to be found in fluentspeech, acoustic information can be quite effectivewhen combined with other sources of information,in particular with pattern matching.In studying the ways in which acoustics mighthelp distinguish repairs from false positives, webegan by examining two patterns conducive toacoustic measurement and comparison.
First, wefocused on patterns in which there was only onematched word, and in which the two occurrencesof that word were either adjacent or separated byonly one word.
Matched words allow for compar-ison of word duration; proximity helps avoid vari-ability due to global intonation contours not asso-ciated with the patterns themselves.
We presenthere analyses for the MI\[M1 ("flights for <one>one person") and M1\]XM1 ("<flight> earliestflight") repairs, and their associated false positives("u s air five one one," '% flight on flight numberfive one one," respectively).In examining the MI\[M1 repair pattern, wefound that the strongest distinguishing cue be-tween the repairs (N = 20) and the false positives(N = 20) was the interval between the offset ofthe first word and the onset of the second.
Falsepositives had a mean gap of 42 msec (s.d.
= 55.8)as opposed to 380 msec (s.d.
= 200.4) for repairs.A second difference found between the two groupswas that, in the case of repairs, there was a statis-tically reliable reduction in duration for the sec-ond occurrence of M1, with a mean difference of53.4 msec.
However because false positives howedno reliable difference for word duration, this wasa much less useful predictor than gap duration.F0 of the matched words was not helpful in sep-arating repairs from false positives; both groupsshowed a highly significant correlation for, and nosignificant difference between, the mean F0 of thematched words.A different set of features was found to be use-ful in distinguishing repairs from false positivesfor the MI\[XM1 pattern.
A set of 12 repairsand 24 false positives was examined; the set offalse positives for this analysis included only flu-ent cases (i.e., it did not include other types ofrepairs matching the pattern).
Despite the smalldata set, some suggestive trends emerge.
For ex-ample, for cases in which there was a pause (200msec or greater) on only one side of the insertedword, the pause was never after the insertion (X)for the repairs, and rarely before the X in thefalse positives.
A second distinguishing character-istic was the peak F0 value of X.
For repairs, theinserted word was nearly always higher in F0 thanthe preceding M1; for false positives, this increasein F0 was rarely observed.
Table 6 shows the re-sults of combining the acoustic onstraints just de-scribed.
As can be seen, such features in combina-tion can be quite helpful in distinguishing repairsfrom false positives of this pattern.
Future workwill investigate the use of prosody in distinguish-ing the M1 \[XM1 repair not only from false posi-tives, but also from other possible repairs havingthis pattern, i.e., M1RI\[M1R1 and R1MI\[R1M1.RepairsFalsePositivesPauses afterX (only)andFO of X lessthan FO of 1st M1.00.58Pauses beforeX (only)andF0 of X greaterthan F0 of 1st M1.92.00Table 6: Combining Acoustic Characteristics ofM1 IX M1 RepairsCue  WordsA second way in which acoustics can be helpfulgiven the output of a pattern matcher is in deter-mining whether or not potential cue words suchas "no" are used as an editing expression (Hock-ett, 1967) as in "...flights <between> <boston><and> <dallas> <no> between oakland andboston."
False positives for these cases are in-stances in which the cue word functions in someother sense ("I want to leave boston no later thanone p m.").
Hirshberg and Litman (1987) haveshown that cue words that function differently canbe distinguished perceptually by listeners on thebasis of prosody.
Thus, we sought to determinewhether acoustic analysis could help in deciding,when such words were present, whether or notthey marked the interruption point of a repair.In a preliminary study of the cue words "no"and "well," we compared 9 examples of thesewords at the site of a repair to 15 examples ofthe same words occurring in fluent speech.
Wefound that these groups were quite distinguishableon the basis of simple prosodic features.
Table 7shows the percentage of repairs versus false pos-itives characterized by a clear rise or fall in F0F0 F0 Lexical Cont.rise fall stress speechRepairs .00 1.00 .00 .00False Positives .87 .00 .87 .736\]Table 7: Acoustic Characteristics of Cue Words8000!60004000!2000.2- : i ?
. '
!
.~ .
?
\]'~ ?
:'~'.:'*~.:."
'-"!
"!~:': '..~::!'
~i~ ~ ,~..:~?
,1.4 1.6 1.8~.k~:~; i :~r  ?
:~:~ ~ifit2.2 2.4 2.6:..::.~'~.~:: ?
i.'.......'.:~i:~.:.
:-; ;.~.
,..
;., -~ ' - .~- '..
~:.:.
:~..' .
,:.
': ,~ ~,~:..'~..
'.;.-.~.?
: : " ' ~  '. "
:i':i2.8 3 3.2I would 1 i k e to <fra-> f 1 yFigure 1: A glottalized fragment(greater than 15 Hz), lexical stress (determinedperceptually), and continuity of the speech im-mediately preceding and following the editing ex-pression ("continuous" means there was no silentpause on either side of the cue word).
As can beseen, at least for this limited data set, cue wordsmarking repairs were quite distinguishable fromthose same words found in fluent strings on thebasis of simple prosodic features.FragmentsA third way in which acoustic knowledge canassist in detecting and correcting repairs is in therecognition of word fragments.
As shown earlier,fragments are exceedingly common; they occurredin 366 of our 607 repairs.
Fragments pose diffi-culty for state-of-the-art recognition systems be-cause most recognizers are constrained to producestrings of actual words, rather than allowing par-tial words as output.
Because so many repairs in-volve fragments, if fragments are not representedin the recognizer output, then information relevantto the processing of repairs is lost.We found that often when a fragment had suf-ficient acoustic energy, one of two recognition er-rors occurred.
Either the fragment was misrecog-nized as a complete word, or it caused a recog-nition error on a neighboring word.
Therefore ifrecognizers were able to flag potential word frag-ments, this information could aid subsequent pro-cessing by indicating the higher likelihood thatwords in the region might require deletion.
Frag-ments can also be useful in the detection of repairsrequiring deletion of more than just the fragment.In approximately 40% of the sentences containingfragments in our data, the fragment occurred atthe right edge of a longer repair.
In a portion of62these cases, for example,"leaving at <seven> <fif-> eight thirty,"the presence of the fragment is an especially im-portant cue because there is nothing (e.g., nomatched words) to cause the pattern matcher tohypothesize the presence of a repair.We studied 50 fragments drawn at randomfrom our total corpus of 366.
The most reliableacoustic cue over the set was the presence of asilence following the fragment.
In 49 out of 50cases, there was a silence of greater than 60 msec;the average silence was 282 msec.
Of the 50 frag-ments, 25 ended in a vowel, 13 contained a voweland ended in a consonant, and 12 contained novocalic portion.It is likely that recognition of fragments of thefirst type, in which there is abrupt cessation ofspeech during a vowel, can be aided by looking forheavy glottalization at the end of the fragment.We coded fragments as glottalized if they showedirregular pitch pulses in their associated waveform,spectrogram, and pitch tracks.
We found glottal-ization in 24 of the 25 vowel-final fragments inour data.
An example of a glottalized fragment, isshown in Figure 1.Although it is true that glottalization occursin fluent speech as well, it normally appears onunstressed, low F0 portions of a signal.
The 24glottalized fragments we examined however, werenot at the bottom of the speaker's range, andmost had considerable nergy.
Thus when com-bined with the feature of a following silence of atleast 60 msec, glottalization on syllables with sulfi-cient energy and not at tile bottom of tile speaker'srange, may prove a useful feature in recognizingfragments.CONCLUSIONIn summary, disfluencies occur at high enoughrates in human-computer dialog to merit consid-eration.
In contrast o earlier approaches, we havemade it our goal to detect and correct repairs au-tomatically, without assuming an explicit edit sig-nal.
Without such an edit signal, however, re-pairs are easily confused both with false positivesand with other repairs.
Preliminary results showthat pattern matching is effective at detecting re-pairs without excessive overgeneration.
Our syn-tactic/semantic approaches are quite accurate atdetecting repairs and correcting them.
Acousticsis a third source of information that can be tappedto provide vidence about the existence of a repair.While none of these knowledge sources by it-self is sufficient, we propose that by combiningthem, and possibly others, we can greatly enhanceour ability to detect and correct repairs.
As a nextstep, we intend to explore additional aspects of thesyntax and semantics of repairs, analyze furtheracoustic patterns, and pursue the question of howbest to integrate information from these multipleknowledge sources.ACKNOWLEDGMENTSWe would like to thank Patti Price for herhelpful comments on earlier drafts, as well as forher participation in the development of the nota-tional system used.
We would also like to thankRobin Lickley for his feedback on the acousticssection, Elizabeth Wade for assistance with thestatistics, and Mark Gawron for work on the Gem-ini grammar.REFERENCES1.
Alshawi, H, Carter, D., van Eijck, J., Moore, R.C., Moran, D. B., Pereira, F., Pulman, S., andA.
Smith (1988) Research Programme In NaturalLanguage Processing: July 1988 Annual Report,SRI International Tech Note, Cambridge, Eng-land.2.
Bear, J., Dowding, J., Price, P., and E. E.Shriberg (1992) "Labeling Conventions for No-tating Grammatical Repairs in Speech," unpub-lished manuscript, o appear as an SRI Tech Note.3.
Hirschberg, g. and D. Litman (1987) "Now Let'sTalk About Now: Identifying Cue Phrases Into-nationally," Proceedings o.f the A CL, pp.
163-171.4.
Carbonell, J. and P. Hayes, P., (1983) "Recov-ery Strategies for Parsing Extragrammatical L n-guage," American Journal of Computational Lin-guistics, Vol.
9, Numbers 3-4, pp.
123-146.5.
Hindle, D. (1983) "Deterministic Parsing of Syn-tactic Non-fluencies," Proceedings of the A CL, pp.123-128.6.
Hockett, C. (1967) "Where the Tongue Slips,There Slip I," in To Honor Roman Jakobson: Vol.~, The Hague: Mouton.7.
Levelt, W. (1983) "Monitoring and self-repair inspeech," Cognition, Vol.
14, pp.
41-104.8.
Levelt, W., and A. Cutler (1983) "Prosodic Mark-ing in Speech Repair," Journal of Semantics, Vol.2, pp.
205-217.9.
Lickley, R., R. ShiUcock, and E. Bard (1991)"Processing Disfluent Speech: How and when aredisfluencies found?"
Proceedings of the SecondEuropean Conference on Speech Communicationand Technology, Vol.
3, pp.
1499-1502.10.
MADCOW (1992) "Multi-site Data Collection fora Spoken Language Corpus," Proceedings of theDARPA Speech and Natural Language Workshop,February 23-26, 1992.11.
Moore, R. and J. Dowding (1991) "EfficientBottom-up Parsing," Proceedings ol the DARPASpeech and Natural Language Workshop, Febru-ary 19-22, 1991, pp.
200-203.12.
Shriberg, E., Bear, 3., and Dowding, J.
(1992 a)"Automatic Detection and Correction of Repairsin Human-Computer Dialog" Proceedings of theDARPA Speech and Natural Language Workshop,February 23-26, 1992.13.
Shriberg, E., Wade, E., and P. Price (1992 b)"Human-Machine Problem Solving Using SpokenLanguage Systems (SLS): Factors Affecting Per-formance and User Satisfaction," Proceedings ofthe DARPA Speech and Natural Language Work-shop, February 23-26, 1992.14.
Ward, W. (1991) "Evaluation of the CMU ATISSystem," Proceedings of the DARPA Speech andNatural Language Workshop, February 19-22,1991, pp.
101-105.63
