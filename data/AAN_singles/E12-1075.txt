Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 736?746,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsSyntax-Based Word Ordering Incorporating a Large-Scale LanguageModelYue ZhangUniversity of CambridgeComputer Laboratoryyz360@cam.ac.ukGraeme BlackwoodUniversity of CambridgeEngineering Departmentgwb24@eng.cam.ac.ukStephen ClarkUniversity of CambridgeComputer Laboratorysc609@cam.ac.ukAbstractA fundamental problem in text generationis word ordering.
Word ordering is a com-putationally difficult problem, which canbe constrained to some extent for particu-lar applications, for example by using syn-chronous grammars for statistical machinetranslation.
There have been some recentattempts at the unconstrained problem ofgenerating a sentence from a multi-set ofinput words (Wan et al 2009; Zhang andClark, 2011).
By using CCG and learn-ing guided search, Zhang and Clark re-ported the highest scores on this task.
Onelimitation of their system is the absenceof an N-gram language model, which hasbeen used by text generation systems toimprove fluency.
We take the Zhang andClark system as the baseline, and incor-porate an N-gram model by applying on-line large-margin training.
Our system sig-nificantly improved on the baseline by 3.7BLEU points.1 IntroductionOne fundamental problem in text generation isword ordering, which can be abstractly formu-lated as finding a grammatical order for a multi-set of words.
The word ordering problem can alsoinclude word choice, where only a subset of theinput words are used to produce the output.Word ordering is a difficult problem.
Findingthe best permutation for a set of words accord-ing to a bigram language model, for example, isNP-hard, which can be proved by linear reductionfrom the traveling salesman problem.
In prac-tice, exploring the whole search space of permu-tations is often prevented by adding constraints.In phrase-based machine translation (Koehn et al2003; Koehn et al 2007), a distortion limit isused to constrain the position of output phrases.In syntax-based machine translation systems suchas Wu (1997) and Chiang (2007), synchronousgrammars limit the search space so that poly-nomial time inference is feasible.
In fluencyimprovement (Blackwood et al 2010), parts oftranslation hypotheses identified as having highlocal confidence are held fixed, so that word or-dering elsewhere is strictly local.Some recent work attempts to address the fun-damental word ordering task directly, using syn-tactic models and heuristic search.
Wan et al(2009) uses a dependency grammar to solve wordordering, and Zhang and Clark (2011) uses CCG(Steedman, 2000) for word ordering and wordchoice.
The use of syntax models makes theirsearch problems harder than word permutation us-ing an N -gram language model only.
Both meth-ods apply heuristic search.
Zhang and Clark de-veloped a bottom-up best-first algorithm to buildoutput syntax trees from input words, wheresearch is guided by learning for both efficiencyand accuracy.
The framework is flexible in allow-ing a large range of constraints to be added forparticular tasks.We extend the work of Zhang and Clark (2011)(Z&C) in two ways.
First, we apply online large-margin training to guide search.
Compared to theperceptron algorithm on ?constituent level fea-tures?
by Z&C, our training algorithm is theo-retically more elegant (see Section 3) and con-verges more smoothly empirically (see Section 5).Using online large-margin training not only im-proves the output quality, but also allows the in-corporation of an N -gram language-model into736the system.
N -gram models have been used as astandard component in statistical machine trans-lation, but have not been applied to the syntac-tic model of Z&C.
Intuitively, an N -gram modelcan improve local fluency when added to a syntaxmodel.
Our experiments show that a four-grammodel trained using the English GigaWord cor-pus gave improvements when added to the syntax-based baseline system.The contributions of this paper are as follows.First, we improve on the performance of the Z&Csystem for the challenging task of the generalword ordering problem.
Second, we develop anovel method for incorporating a large-scale lan-guage model into a syntax-based generation sys-tem.
Finally, we analyse large-margin training inthe context of learning-guided best-first search,offering a novel solution to this computationallyhard problem.2 The statistical model and decodingalgorithmWe take Z&C as our baseline system.
Givena multi-set of input words, the baseline systembuilds a CCG derivation by choosing and orderingwords from the input set.
The scoring model istrained using CCGBank (Hockenmaier and Steed-man, 2007), and best-first decoding is applied.
Weapply the same decoding framework in this paper,but apply an improved training process, and incor-porate an N -gram language model into the syntaxmodel.
In this section, we describe and discussthe baseline statistical model and decoding frame-work, motivating our extensions.2.1 Combinatory Categorial GrammarCCG, and parsing with CCG, has been describedelsewhere (Clark and Curran, 2007; Hockenmaierand Steedman, 2002); here we provide only ashort description.CCG (Steedman, 2000) is a lexicalized gram-mar formalism, which associates each word in asentence with a lexical category.
There is a smallnumber of basic lexical categories, such as noun(N), noun phrase (NP), and prepositional phrase(PP).
Complex lexical categories are formed re-cursively from basic categories and slashes, whichindicate the directions of arguments.
The CCGgrammar used by our system is read off the deriva-tions in CCGbank, following Hockenmaier andSteedman (2002), meaning that the CCG combina-tory rules are encoded as rule instances, togetherwith a number of additional rules which deal withpunctuation and type-changing.
Given a sentence,its CCG derivation can be produced by first assign-ing a lexical category to each word, and then re-cursively applying CCG rules bottom-up.2.2 The decoding algorithmIn the decoding algorithm, a hypothesis is anedge, which corresponds to a sub-tree in a CCGderivation.
Edges are built bottom-up, startingfrom leaf edges, which are generated by assigningall possible lexical categories to each input word.Each leaf edge corresponds to an input word witha particular lexical category.
Two existing edgescan be combined if there exists a CCG rule whichcombines their category labels, and if they do notcontain the same input word more times than itstotal count in the input.
The resulting edge is as-signed a category label according to the combi-natory rule, and covers the concatenated surfacestrings of the two sub-edges in their order or com-bination.
New edges can also be generated by ap-plying unary rules to a single existing edge.
Start-ing from the leaf edges, the bottom-up process isrepeated until a goal edge is found, and its surfacestring is taken as the output.This derivation-building process is reminiscentof a bottom-up CCG parser in the edge combina-tion mechanism.
However, it is fundamentallydifferent from a bottom-up parser.
Since, forthe generation problem, the order of two edgesin their combination is flexible, the search prob-lem is much harder than that of a parser.
Withno input order specified, no efficient dynamic-programming algorithm is available, and less con-textual information is available for disambigua-tion due to the lack of an input string.In order to combat the large search space, best-first search is applied, where candidate hypothe-ses are ordered by their scores, and kept in anagenda, and a limited number of accepted hy-potheses are recorded in a chart.
Here the chartis essentially a set of beams, each of which con-tains the highest scored edges covering a particu-lar number of words.
Initially, all leaf edges aregenerated and scored, before they are put onto theagenda.
During each step in the decoding process,the top edge from the agenda is expanded.
If it isa goal edge, it is returned as the output, and the737Algorithm 1 The decoding algorithm.a?
INITAGENDA( )c?
INITCHART( )while not TIMEOUT( ) donew?
[]e?
POPBEST(a)if GOALTEST(e) thenreturn eend iffor e?
?
UNARY(e, grammar) doAPPEND(new, e)end forfor e?
?
c doif CANCOMBINE(e, e?)
thene??
BINARY(e, e?, grammar)APPEND(new, e?
)end ifif CANCOMBINE(e?, e) thene??
BINARY(e?, e, grammar)APPEND(new, e?
)end ifend forfor e?
?
new doADD(a, e?
)end forADD(c, e)end whiledecoding finishes.
Otherwise it is extended withunary rules, and combined with existing edges inthe chart using binary rules to produce new edges.The resulting edges are scored and put onto theagenda, while the original edge is put onto thechart.
The process repeats until a goal edge isfound, or a timeout limit is reached.
In the lattercase, a default output is produced using existingedges in the chart.Pseudocode for the decoder is shown as Algo-rithm 1.
Again it is reminiscent of a best-firstparser (Caraballo and Charniak, 1998) in the useof an agenda and a chart, but is fundamentally dif-ferent due to the fact that there is no input order.2.3 Statistical model and feature templatesThe baseline system uses a linear model to scorehypotheses.
For an edge e, its score is defined as:f(e) = ?
(e) ?
?,where ?
(e) represents the feature vector of e and?
is the parameter vector of the model.During decoding, feature vectors are computedincrementally.
When an edge is constructed, itsscore is computed from the scores of its sub-edgesand the incrementally added structure:f(e) = ?
(e) ?
?=((?es?e?
(es))+ ?(e))?
?=(?es?e?
(es) ?
?
)+ ?
(e) ?
?=(?es?ef(es))+ ?
(e) ?
?In the equation, es ?
e represents a sub-edge ofe.
Leaf edges do not have any sub-edges.
Unary-branching edges have one sub-edge, and binary-branching edges have two sub-edges.
The fea-ture vector ?
(e) represents the incremental struc-ture when e is constructed over its sub-edges.It is called the ?constituent-level feature vector?by Z&C.
For leaf edges, ?
(e) includes informa-tion about the lexical category label; for unary-branching edges, ?
(e) includes information fromthe unary rule; for binary-branching edges, ?
(e)includes information from the binary rule, and ad-ditionally the token, POS and lexical category bi-grams and trigrams that result from the surfacestring concatenation of its sub-edges.
The scoref(e) is therefore the sum of f(es) (for all es ?
e)plus ?
(e) ??.
The feature templates we use are thesame as those in the baseline system.An important aspect of the scoring model is thatedges with different sizes are compared with eachother during decoding.
Edges with different sizescan have different numbers of features, which canmake the training of a discriminative model moredifficult.
For example, a leaf edge with one wordcan be compared with an edge over the entire in-put.
One way of reducing the effect of the size dif-ference is to include the size of the edge as part offeature definitions, which can improve the compa-rability of edges of different sizes by reducing thenumber of features they have in common.
Suchfeatures are applied by Z&C, and we make use ofthem here.
Even with such features, the questionof whether edges with different sizes are linearlyseparable is an empirical one.3 TrainingThe efficiency of the decoding algorithm is de-pendent on the statistical model, since the best-738first search is guided to a solution by the model,and a good model will lead to a solution beingfound more quickly.
In the ideal situation for thebest-first decoding algorithm, the model is perfectand the score of any gold-standard edge is higherthan the score of any non-gold-standard edge.
Asa result, the top edge on the agenda is always agold-standard edge, and therefore all edges on thechart are gold-standard before the gold-standardgoal edge is found.
In this oracle procedure, theminimum number of edges is expanded, and theoutput is correct.
The best-first decoder is perfectin not only accuracy, but also speed.
In practicethis ideal situation is rarely met, but it determinesthe goal of the training algorithm: to produce theperfect model and hence decoder.If we take gold-standard edges as positive ex-amples, and non-gold-standard edges as negativeexamples, the goal of the training problem can beviewed as finding a large separating margin be-tween the scores of positive and negative exam-ples.
However, it is infeasible to generate the fullspace of negative examples, which is factorial inthe size of input.
Like Z&C, we apply onlinelearning, and generate negative examples basedon the decoding algorithm.Our training algorithm is shown as Algo-rithm 2.
The algorithm is based on the decoder,where an agenda is used as a priority queue ofedges to be expanded, and a set of accepted edgesis kept in a chart.
Similar to the decoding algo-rithm, the agenda is intialized using all possibleleaf edges.
During each step, the top of the agendae is popped.
If it is a gold-standard edge, it is ex-panded in exactly the same way as the decoder,with the newly generated edges being put ontothe agenda, and e being inserted into the chart.If e is not a gold-standard edge, we take it as anegative example e?, and take the lowest scoredgold-standard edge on the agenda e+ as a positiveexample, in order to make an udpate to the modelparameter vector ?.
Our parameter update algo-rithm is different from the baseline perceptron al-gorithm, as will be discussed later.
After updatingthe parameters, the scores of agenda edges aboveand including e?, together with all chart edges,are updated, and e?
is discarded before the startof the next processing step.
By not putting anynon-gold-standard edges onto the chart, the train-ing speed is much faster; on the other hand a widerange of negative examples is pruned.
We leaveAlgorithm 2 The training algorithm.a?
INITAGENDA( )c?
INITCHART( )while not TIMEOUT( ) donew?
[]e?
POPBEST(a)if GOLDSTANDARD(e) and GOALTEST(e)then return eend ifif not GOLDSTANDARD(e) thene??
ee+?MINGOLD(a)UPDATEPARAMETERS(e+ , e?
)RECOMPUTESCORES(a, c)continueend iffor e?
?
UNARY(e, grammar) doAPPEND(new, e)end forfor e?
?
c doif CANCOMBINE(e, e?)
thene??
BINARY(e, e?, grammar)APPEND(new, e?
)end ifif CANCOMBINE(e?, e) thene??
BINARY(e?, e, grammar)APPEND(new, e?
)end ifend forfor e?
?
new doADD(a, e?
)end forADD(c, e)end whilefor further work possible alternative methods togenerate more negative examples during training.Another way of viewing the training process isthat it pushes gold-standard edges towards the topof the agenda, and crucially pushes them abovenon-gold-standard edges.
This is the view de-scribed by Z&C.
Given a positive example e+ anda negative example e?, they use the perceptronalgorithm to penalize the score for ?(e?)
and re-ward the score of ?
(e+), but do not update pa-rameters for the sub-edges of e+ and e?.
An argu-ment for not penalizing the sub-edge scores for e?is that the sub-edges must be gold-standard edges(since the training process is constructed so thatonly gold-standard edges are expanded).
From739the perspective of correctness, it is unnecessaryto find a margin between the sub-edges of e+ andthose of e?, since both are gold-standard edges.However, since the score of an edge not onlyrepresents its correctness, but also affects its pri-ority on the agenda, promoting the sub-edge ofe+ can lead to ?easier?
edges being constructedbefore ?harder?
ones (i.e.
those that are lesslikely to be correct), and therefore improve theoutput accuracy.
This perspective has been ob-served by other works of learning-guided-search(Shen et al 2007; Shen and Joshi, 2008; Gold-berg and Elhadad, 2010).
Intuitively, the scoredifference between easy gold-standard and hardergold-standard edges should not be as great as thedifference between gold-standard and non-gold-standard edges.
The perceptron update cannotprovide such control of separation, because theamount of update is fixed to 1.As described earlier, we treat parameter updateas finding a separation between correct and incor-rect edges, in which the global feature vectors ?,rather than ?, are considered.
Given a positive ex-ample e+ and a negative example e?, we make aminimum update so that the score of e+ is higherthan that of e?
with some margin:?
?
argmin???
???
?0 ?, s.t.?(e+)????(e?)??
?
1where ?0 and ?
denote the parameter vectors be-fore and after the udpate, respectively.
The up-date is similar to the update of online large-marginlearning algorithms such as 1-best MIRA (Cram-mer et al 2006), and has a closed-form solution:?
?
?0+f(e?)?
f(e+) + 1?
?(e+)?
?(e?)
?2(?(e+)??(e?
))In this update, the global feature vectors ?
(e+)and ?(e?)
are used.
Unlike Z&C, the scoresof sub-edges of e+ and e?
are also udpated, sothat the sub-edges of e?
are less prioritized thanthose of e+.
We show empirically that this train-ing algorithm significantly outperforms the per-ceptron training of the baseline system in Sec-tion 5.
An advantage of our new training algo-rithm is that it enables the accommodation of aseparately trained N -gram model into the system.4 Incorporating an N-gram languagemodelSince the seminal work of the IBM models(Brown et al 1993), N -gram language modelshave been used as a standard component in statis-tical machine translation systems to control out-put fluency.
For the syntax-based generation sys-tem, the incorporation of an N -gram languagemodel can potentially improve the local fluencyof output sequences.
In addition, the N -gramlanguage model can be trained separately usinga large amount of data, while the syntax-basedmodel requires manual annotation for training.The standard method for the combination ofa syntax model and an N -gram model is linearinterpolation.
We incorporate fourgram, trigramand bigram scores into our syntax model, so thatthe score of an edge e becomes:F (e) = f(e) + g(e)= f(e) + ?
?
gfour(e) + ?
?
gtri(e) + ?
?
gbi(e),where f is the syntax model score, and g is theN -gram model score.
g consists of three com-ponents, gfour, gtri and gbi, representing the log-probabilities of fourgrams, trigrams and bigramsfrom the language model, respectively.
?, ?
and?
are the corresponding weights.During decoding, F (e) is computed incremen-tally.
Again, denoting the sub-edges of e as es,F (e) = f(e) + g(e)=(?es?eF (es))+ ?(e)?
+ g?
(e)Here g?
(e) = ?
?g?four(e)+?
?g?tri(e)+?
?g?bi(e)is the sum of log-probabilities of the new N -grams resulting from the construction of e. Forleaf edges and unary-branching edges, no new N -grams result from their construction (i.e.
g?
= 0).For a binary-branching edge, new N -grams resultfrom the surface-string concatenation of its sub-edges.
The sum of log-probabilities of the newfourgrams, trigrams and bigrams contribute to g?with weights ?, ?
and ?, respectively.For training, there are at least three methods totune ?, ?, ?
and ?.
One simple method is to trainthe syntax model ?
independently, and select ?,?, and ?
empirically from a range of candidatevalues according to development tests.
We callthis method test-time interpolation.
An alterna-tive is to select ?, ?
and ?
first, initializing thevector ?
as all zeroes, and then run the trainingalgorithm for ?
taking into account the N -gramlanguage model.
In this process, g is consideredwhen finding a separation between positive and740negative examples; the training algorithm finds avalue of ?
that best suits the precomputed ?, ?and ?
values, together with the N -gram languagemodel.
We call this method g-precomputed in-terpolation.
Yet another method is to initialize ?,?, ?
and ?
as all zeroes, and run the training al-gorithm taking into account the N -gram languagemodel.
We call this method g-free interpolation.The incorporation of an N -gram languagemodel into the syntax-based generation system isweakly analogous to N -gram model insertion forsyntax-based statistical machine translation sys-tems, both of which apply a score from the N -gram model component in a derivation-buildingprocess.
As discussed earlier, polynomial-timedecoding is typically feasible for syntax-basedmachine translation systems without an N -gramlanguage model, due to constraints from thegrammar.
In these cases, incorporation of N -gram language models can significantly increasethe complexity of a dynamic-programming de-coder (Bar-Hillel et al 1961).
Efficient searchhas been achieved using chart pruning (Chiang,2007) and iterative numerical approaches to con-strained optimization (Rush and Collins, 2011).In contrast, the incorporation of an N -gram lan-guage model into our decoder is more straightfor-ward, and does not add to its asymptotic complex-ity, due to the heuristic nature of the decoder.5 ExperimentsWe use sections 2?21 of CCGBank to train oursyntax model, section 00 for development andsection 23 for the final test.
Derivations fromCCGBank are transformed into inputs by turn-ing their surface strings into multi-sets of words.Following Z&C, we treat base noun phrases (i.e.NPs that do not recursively contain other NPs) asatomic units for the input.
Output sequences arecompared with the original sentences to evaluatetheir quality.
We follow previous work and usethe BLEU metric (Papineni et al 2002) to com-pare outputs with references.Z&C use two methods to construct leaf edges.The first is to assign lexical categories accordingto a dictionary.
There are 26.8 lexical categoriesfor each word on average using this method, cor-responding to 26.8 leaf edges.
The other methodis to use a pre-processing step ?
a CCG supertag-ger (Clark and Curran, 2007) ?
to prune can-didate lexical categories according to the gold-CCGBank Sentences Tokenstraining 39,604 929,552development 1,913 45,422GigaWord v4 Sentences TokensAFP 30,363,052 684,910,697XIN 15,982,098 340,666,976Table 1: Number of sentences and tokens by languagemodel source.standard sequence, assuming that for some prob-lems the ambiguities can be reduced (e.g.
whenthe input is already partly correctly ordered).Z&C use different probability cutoff levels (the?
parameter in the supertagger) to control thepruning.
Here we focus mainly on the dictionarymethod, which leaves lexical category disam-biguation entirely to the generation system.
Forcomparison, we also perform experiments withlexical category pruning.
We chose ?
= 0.0001,which leaves 5.4 leaf edges per word on average.We used the SRILM Toolkit (Stolcke, 2002)to build a true-case 4-gram language model es-timated over the CCGBank training and develop-ment data and a large additional collection of flu-ent sentences in the Agence France-Presse (AFP)and Xinhua News Agency (XIN) subsets of theEnglish GigaWord Fourth Edition (Parker et al2009), a total of over 1 billion tokens.
The Gi-gaWord data was first pre-processed to replicatethe CCGBank tokenization.
The total numberof sentences and tokens in each LM componentis shown in Table 1.
The language model vo-cabulary consists of the 46,574 words that oc-cur in the concatenation of the CCGBank train-ing, development, and test sets.
The LM proba-bilities are estimated using modified Kneser-Neysmoothing (Kneser and Ney, 1995) with interpo-lation of lower n-gram orders.5.1 Development experimentsA set of development test results without lexicalcategory pruning (i.e.
using the full dictionary) isshown in Table 2.
We train the baseline systemand our systems under various settings for 10 iter-ations, and measure the output BLEU scores aftereach iteration.
The timeout value for each sen-tence is set to 5 seconds.
The highest score (maxBLEU) and averaged score (avg.
BLEU) of eachsystem over the 10 training iterations are shownin the table.741Method max BLEU avg.
BLEUbaseline 38.47 37.36margin 41.20 39.70margin +LM (g-precomputed) 41.50 40.84margin +LM (?
= 0, ?
= 0, ?
= 0) 40.83 ?margin +LM (?
= 0.08, ?
= 0.016, ?
= 0.004) 38.99 ?margin +LM (?
= 0.4, ?
= 0.08, ?
= 0.02) 36.17 ?margin +LM (?
= 0.8, ?
= 0.16, ?
= 0.04) 34.74 ?Table 2: Development experiments without lexical category pruning.The first three rows represent the baseline sys-tem, our largin-margin training system (margin),and our system with the N -gram model incorpo-rated using g-precomputed interpolation.
For in-terpolation we manually chose ?
= 0.8, ?
= 0.16and ?
= 0.04, respectively.
These values couldbe optimized by development experiments withalternative configurations, which may lead to fur-ther improvements.
Our system with large-margintraining gives higher BLEU scores than the base-line system consistently over all iterations.
TheN -gram model led to further improvements.The last four rows in the table show resultsof our system with the N -gram model added us-ing test-time interpolation.
The syntax model istrained with the optimal number of iterations, anddifferent ?, ?, and ?
values are used to integratethe language model.
Compared with the systemusing no N -gram model (margin), test-time inter-polation did not improve the accuracies.The row with ?, ?, ?
= 0 represents our systemwith the N -gram model loaded, and the scoresgfour , gtri and gbi computed for each N -gramduring decoding, but the scores of edges are com-puted without using N -gram probabilities.
Thescoring model is the same as the syntax model(margin), but the results are lower than the row?margin?, because computing N -gram probabil-ities made the system slower, exploring less hy-potheses under the same timeout setting.1The comparison between g-precomputed inter-polation and test-time interpolation shows that thesystem gives better scores when the syntax modeltakes into consideration the N -gram model during1More decoding time could be given to the slower N -gram system, but we use 5 seconds as the timeout settingfor all the experiments, giving the methods with the N -gramlanguage model a slight disadvantage, as shown by the tworows ?margin?
and ?margin +LM (?, ?, ?
= 0).3738394041424344451  2  3  4  5  6  7  8  9  10BLEUtraining iterationbaselinemarginmargin +LMFigure 1: Development experiments with lexical cate-gory pruning (?
= 0.0001).training.
One question that arises is whether g-free interpolation will outperform g-precomputedinterpolation.
g-free interpolation offers the free-dom of ?, ?
and ?
during training, and can poten-tially reach a better combination of the parametervalues.
However, the training algorithm failed toconverge with g-free interpolation.
One possibleexplanation is that real-valued features from thelanguage model made our large-margin trainingharder.
Another possible reason is that our train-ing process with heavy pruning does not accom-modate this complex model.Figure 1 shows a set of development experi-ments with lexical category pruning (with the su-pertagger parameter ?
= 0.0001).
The scoresof the three different systems are calculated byvarying the number of training iterations.
Thelarge-margin training system (margin) gave con-sistently better scores than the baseline system,and adding a language model (margin +LM) im-proves the scores further.Table 3 shows some manually chosen examplesfor which our system gave significant improve-ments over the baseline.
For most other sentencesthe improvements are not as obvious.
For each742baseline margin margin +LMas a nonexecutive director Pierre Vinken, 61 years old , will join the board .
29Nov.61 years old , the board will join as anonexecutive director Nov. 29 , PierreVinken .as a nonexecutive director Pierre Vinken, 61 years old , will join the board Nov.29 .Lorillard nor smokers were aware of theKent cigarettes of any research on theworkers who studied the researchersof any research who studied Neither theworkers were aware of smokers on theKent cigarettes nor the researchersNeither Lorillard nor any research on theworkers who studied the Kent cigaretteswere aware of smokers of the researchers.you But 35 years ago have to recognizethat these events took place .recognize But you took place that theseevents have to 35 years ago .But you have to recognize that theseevents took place 35 years ago .investors to pour cash into money fundscontinue in Despite yields recent declinesDespite investors , yields continue topour into money funds recent declines incash .Despite investors , recent declines inyields continue to pour cash into moneyfunds .yielding The top money funds are cur-rently well over 9 % .The top money funds currently are yield-ing well over 9 % .The top money funds are yielding wellover 9 % currently .where A buffet breakfast , held in the mu-seum was food and drinks to .
everydayvisitors bannedeveryday visitors are banned to whereA buffet breakfast was held , food anddrinks in the museum .A buffet breakfast , everyday visitors arebanned to where food and drinks washeld in the museum .A Commonwealth Edison spokesmansaid an administrative nightmare wouldbe tracking down the past 3 12 years thatthe two million customers have .
whosechangedtracking A Commonwealth Edisonspokesman said that the two million cus-tomers whose addresses have changeddown during the past 3 12 years wouldbe an administrative nightmare .an administrative nightmare whose ad-dresses would be tracking down A Com-monwealth Edison spokesman said thatthe two million customers have changedduring the past 3 12 years .The $ 2.5 billion Byron 1 plant , Ill. , wascompleted .
near Rockford in 1985The $ 2.5 billion Byron 1 plant was nearcompleted in Rockford , Ill. , 1985 .The $ 2.5 billion Byron 1 plant nearRockford , Ill. , was completed in 1985 .will ( During its centennial year , TheWall Street Journal report events of thepast century that stand as milestones ofAmerican business history .
)as The Wall Street Journal ( During itscentennial year , milestones stand ofAmerican business history that will re-port events of the past century .
)During its centennial year events will re-port , The Wall Street Journal that standas milestones of American business his-tory ( of the past century ) .Table 3: Some chosen examples with significant improvements (supertagger parameter ?
= 0.0001).method, the examples are chosen from the devel-opment output with lexical category pruning, af-ter the optimal number of training iterations, withthe timeout set to 5s.
We also tried manually se-lecting examples without lexical category prun-ing, but the improvements were not as obvious,partly because the overall fluency was lower forall the three systems.Table 4 shows a set of examples chosen ran-domly from the development test outputs of oursystem with the N -gram model.
The optimalnumber of training iterations is used, and a time-out of 1 minute is used in addition to the 5s time-out for comparison.
With more time to decodeeach input, the system gave a BLEU score of44.61, higher than 41.50 with the 5s timout.While some of the outputs we examined arereasonably fluent, most are to some extent frag-mentary.2 In general, the system outputs arestill far below human fluency.
Some samples are2Part of the reason for some fragmentary outputs is thedefault output mechanism: partial derivations from the chartare greedily put together when timeout occurs before a goalhypothesis is found.syntactically grammatical, but are semanticallyanomalous.
For example, person names are oftenconfused with company names, verbs often takeunrelated subjects and objects.
The problem ismuch more severe for long sentences, which havemore ambiguities.
For specific tasks, extra infor-mation (such as the source text for machine trans-lation) can be available to reduce ambiguities.6 Final resultsThe final results of our system without lexical cat-egory pruning are shown in Table 5.
Row ?W09CLE?
and ?W09 AB?
show the results of themaximum spanning tree and assignment-based al-gorithms of Wan et al(2009); rows ?margin?and ?margin +LM?
show the results of our large-margin training system and our system with theN -gram model.
All these results are directly com-parable since we do not use any lexical categorypruning for this set of results.
For each of oursystems, we fix the number of training iterationsaccording to development test scores.
Consis-tent with the development experiments, our sys-743timeout = 5s timeout = 1mdrooled the cars and drivers , like Fortune 500 executives .
overthe raceAfter schoolboys drooled over the cars and drivers , the racelike Fortune 500 executives .One big reason : thin margins .
One big reason : thin margins .You or accountants look around ... and at an eye blinks .
pro-fessional ballplayersblinks nobody You or accountants look around ... and at an eye.
professional ballplayersmost disturbing And of it , are educators , not students , for thewrongdoing is who .And blamed for the wrongdoing , educators , not students whoare disturbing , much of it is most .defeat coaching aids the purpose of which is , He and othercritics say can to .
standardized tests learning progressgauge coaching aids learning progress can and other critics saythe purpose of which is to defeat , standardized tests .The federal government of government debt because Congresshas lifted the ceiling on U.S. savings bonds suspended salesThe federal government suspended sales of government debtbecause Congress has n?t lifted the ceiling on U.S. savingsbonds .Table 4: Some examples chosen at random from development test outputs without lexical category pruning.System BLEUW09 CLE 26.8W09 AB 33.7Z&C11 40.1margin 42.5margin +LM 43.8Table 5: Test results without lexical category pruning.System BLEUZ&C11 43.2margin 44.7margin +LM 46.1Table 6: Test results with lexical category pruning (su-pertagger parameter ?
= 0.0001).tem outperforms the baseline methods.
The acu-racies are significantly higher when the N -grammodel is incorporated.Table 6 compares our system with Z&C usinglexical category pruning (?
= 0.0001) and a 5stimeout for fair comparison.
The results are sim-ilar to Table 5: our large-margin training systemsoutperforms the baseline by 1.5 BLEU points, andadding the N -gram model gave a further 1.4 pointimprovement.
The scores could be significantlyincreased by using a larger timeout, as shown inour earlier development experiments.7 Related WorkThere is a recent line of research on text-to-text generation, which studies the linearization ofdependency structures (Barzilay and McKeown,2005; Filippova and Strube, 2007; Filippova andStrube, 2009; Bohnet et al 2010; Guo et al2011).
Unlike our system, and Wan et al(2009),input dependencies provide additional informa-tion to these systems.
Although the search spacecan be constrained by the assumption of projec-tivity, permutation of modifiers of the same headword makes exact inference for tree lineariza-tion intractable.
The above systems typically ap-ply approximate inference, such as beam-search.While syntax-based features are commonly usedby these systems for linearization, Filippova andStrube (2009) apply a trigram model to controllocal fluency within constituents.
A dependency-based N-gram model has also been shown effec-tive for the linearization task (Guo et al 2011).The best-first inference and timeout mechanismof our system is similar to that of White (2004), asurface realizer from logical forms using CCG.8 ConclusionWe studied the problem of word-ordering usinga syntactic model and allowing permutation.
Wetook the model of Zhang and Clark (2011) as thebaseline, and extended it with online large-margintraining and an N -gram language model.
Theseextentions led to improvements in the BLEU eval-uation.
Analyzing the generated sentences sug-gests that, while highly fluent outputs can be pro-duced for short sentences (?
10 words), the sys-tem fluency in general is still way below humanstandard.
Future work remains to apply the sys-tem as a component for specific text generationtasks, for example machine translation.AcknowledgementsYue Zhang and Stephen Clark are supported by the Eu-ropean Union Seventh Framework Programme (FP7-ICT-2009-4) under grant agreement no.
247762.744ReferencesYehoshua Bar-Hillel, M. Perles, and E. Shamir.
1961.On formal properties of simple phrase structuregrammars.
Zeitschrift fu?r Phonetik, Sprachwis-senschaft und Kommunikationsforschung, 14:143?172.
Reprinted in Y. Bar-Hillel.
(1964).
Languageand Information: Selected Essays on their Theoryand Application, Addison-Wesley 1964, 116?150.Regina Barzilay and Kathleen McKeown.
2005.
Sen-tence fusion for multidocument news summariza-tion.
Computational Linguistics, 31(3):297?328.Graeme Blackwood, Adria` de Gispert, and WilliamByrne.
2010.
Fluency constraints for minimumBayes-risk decoding of statistical machine trans-lation lattices.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics(Coling 2010), pages 71?79, Beijing, China, Au-gust.
Coling 2010 Organizing Committee.Bernd Bohnet, Leo Wanner, Simon Mill, and AliciaBurga.
2010.
Broad coverage multilingual deepsentence generation with a stochastic multi-level re-alizer.
In Proceedings of the 23rd InternationalConference on Computational Linguistics (Coling2010), pages 98?106, Beijing, China, August.
Col-ing 2010 Organizing Committee.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: Parameterestimation.
Computational Linguistics, 19(2):263?311.Sharon A. Caraballo and Eugene Charniak.
1998.New figures of merit for best-first probabilistic chartparsing.
Comput.
Linguist., 24:275?298, June.David Chiang.
2007.
Hierarchical Phrase-based Translation.
Computational Linguistics,33(2):201?228.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 7:551?585.Katja Filippova and Michael Strube.
2007.
Gener-ating constituent order in german clauses.
In Pro-ceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 320?327, Prague, Czech Republic, June.
Association forComputational Linguistics.Katja Filippova and Michael Strube.
2009.
Tree lin-earization in english: Improving language modelbased approaches.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, Companion Volume:Short Papers, pages 225?228, Boulder, Colorado,June.
Association for Computational Linguistics.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 742?750, Los Angeles, California,June.
Association for Computational Linguistics.Yuqing Guo, Deirdre Hogan, and Josef van Genabith.2011.
Dcu at generation challenges 2011 surfacerealisation track.
In Proceedings of the GenerationChallenges Session at the 13th European Workshopon Natural Language Generation, pages 227?229,Nancy, France, September.
Association for Compu-tational Linguistics.Julia Hockenmaier and Mark Steedman.
2002.
Gen-erative models for statistical parsing with Combi-natory Categorial Grammar.
In Proceedings of the40th Meeting of the ACL, pages 335?342, Philadel-phia, PA.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.R.
Kneser and H. Ney.
1995.
Improved backing-offfor m-gram language modeling.
In InternationalConference on Acoustics, Speech, and Signal Pro-cessing, 1995.
ICASSP-95, volume 1, pages 181?184.Philip Koehn, Franz Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof NAACL/HLT, Edmonton, Canada, May.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical ma-chine translation.
In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics Companion Volume Proceedings of theDemo and Poster Sessions, pages 177?180, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
Bleu: a method for auto-matic evaluation of machine translation.
In Pro-ceedings of 40th Annual Meeting of the Associa-tion for Computational Linguistics, pages 311?318,Philadelphia, Pennsylvania, USA, July.
Associationfor Computational Linguistics.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2009.
English Gigaword FourthEdition, Linguistic Data Consortium.Alexander M. Rush and Michael Collins.
2011.
Exactdecoding of syntactic translation models through la-745grangian relaxation.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages72?82, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Libin Shen and Aravind Joshi.
2008.
LTAG depen-dency parsing with bidirectional incremental con-struction.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 495?504, Honolulu, Hawaii, Octo-ber.
Association for Computational Linguistics.Libin Shen, Giorgio Satta, and Aravind Joshi.
2007.Guided learning for bidirectional sequence classi-fication.
In Proceedings of ACL, pages 760?767,Prague, Czech Republic, June.Mark Steedman.
2000.
The Syntactic Process.
TheMIT Press, Cambridge, Mass.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the In-ternational Conference on Spoken Language Pro-cessing, pages 901?904.Stephen Wan, Mark Dras, Robert Dale, and Ce?cileParis.
2009.
Improving grammaticality in statisti-cal sentence generation: Introducing a dependencyspanning tree algorithm with an argument satisfac-tion model.
In Proceedings of the 12th Conferenceof the European Chapter of the ACL (EACL 2009),pages 852?860, Athens, Greece, March.
Associa-tion for Computational Linguistics.Michael White.
2004.
Reining in CCG chart realiza-tion.
In Proc.
INLG-04, pages 182?191.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3).Yue Zhang and Stephen Clark.
2011.
Syntax-based grammaticality improvement using CCG andguided search.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 1147?1157, Edinburgh, Scot-land, UK., July.
Association for Computational Lin-guistics.746
