Bootstrapping Named Entity Recognitionwith Automatically Generated Gazetteer ListsZornitsa KozarevaDept.
de Lenguajes y Sistemas Informa?ticosUniversity of AlicanteAlicante, Spainzkozareva@dlsi.ua.esAbstractCurrent Named Entity Recognition sys-tems suffer from the lack of hand-taggeddata as well as degradation when mov-ing to other domain.
This paper explorestwo aspects: the automatic generation ofgazetteer lists from unlabeled data; and thebuilding of a Named Entity Recognitionsystem with labeled and unlabeled data.1 IntroductionAutomatic information extraction and informationretrieval concerning particular person, location,organization, title of movie or book, juxtaposes tothe Named Entity Recognition (NER) task.
NERconsists in detecting the most silent and informa-tive elements in a text such as names of people,company names, location, monetary currencies,dates.
Early NER systems (Fisher et al, 1997),(Black et al, 1998) etc., participating in MessageUnderstanding Conferences (MUC), used linguis-tic tools and gazetteer lists.
However these are dif-ficult to develop and domain sensitive.To surmount these obstacles, application ofmachine learning approaches to NER became aresearch subject.
Various state-of-the-art ma-chine learning algorithms such as Maximum En-tropy (Borthwick, 1999), AdaBoost(Carreras etal., 2002), Hidden Markov Models (Bikel et al, ),Memory-based Based learning (Tjong Kim Sang,2002b), have been used1.
(Klein et al, 2003),(Mayfield et al, 2003), (Wu et al, 2003),(Kozareva et al, 2005c) among others, combinedseveral classifiers to obtain better named entitycoverage rate.1For other machine learning methods, consulthttp://www.cnts.ua.ac.be/conll2002/ner/http://www.cnts.ua.ac.be/conll2003/ner/Nevertheless all these machine learning algo-rithms rely on previously hand-labeled trainingdata.
Obtaining such data is labor-intensive, timeconsuming and even might not be present for lan-guages with limited funding.
Resource limitation,directed NER research (Collins and Singer, 1999),(Carreras et al, 2003), (Kozareva et al, 2005a)toward the usage of semi-supervised techniques.These techniques are needed, as we live in a multi-lingual society and access to information from var-ious language sources is reality.
The developmentof NER systems for languages other than Englishcommenced.This paper presents the development of a Span-ish Named Recognition system based on machinelearning approach.
For it no morphologic or syn-tactic information was used.
However, we pro-pose and incorporate a very simple method forautomatic gazetteer2 construction.
Such methodcan be easily adapted to other languages and it islow-costly obtained as it relies on n-gram extrac-tion from unlabeled data.
We compare the perfor-mance of our NER system when labeled and unla-beled training data is present.The paper is organized in the following way:brief explanation about NER process is repre-sented in Section 2.
In Section 3 follows featureextraction.
The experimental evaluation for theNamed Entity detection and classification taskswith and without labeled data are in Sections 4 and5.
We conclude in Section 6.2 The NER how toA Named Entity Recognition task can be de-scribed as composition of two subtasks, entity de-2specialized lists of names for location and person names,e.g.
Madrid is in the location gazetteer, Mary is in the persongazetteer15tection and entity classification.
Entity delimita-tion consist in determining the boundaries of theentity (e.g.
the place from where it starts and theplace it finishes).
This is important for tracingentities composed of two or more words such as?Presidente de los Estados Unidos ?3, ?Universi-dad Politecnica de Catalun?a?4.
For this purpose,the BIO scheme was incorporated.
In this scheme,tag B denotes the start of an entity, tag I continuesthe entity and tag O marks words that do not formpart of an entity.
This scheme was initially intro-duced in CoNLL?s (Tjong Kim Sang, 2002a) and(Tjong Kim Sang and De Meulder, 2003) NERcompetitions, and we decided to adapt it for ourexperimental work.Once all entities in the text are detected, theyare passed for classification in a predefined set ofcategories such as location, person, organizationor miscellaneous5 names.
This task is known asentity classification.
The final NER performanceis measured considering the entity detection andclassification tasks together.Our NER approach is based on machine learn-ing.
The two algorithms we used for the experi-ments were instance-based and decision trees, im-plemented by (Daelemans et al, 2003).
They wereused with their default parameter settings.
Weselected the instance-based model, because it isknown to be useful when the amount of trainingdata is not sufficient.Important part in the NE process takes the lo-cation and person gazetteer lists which were au-tomatically extracted from unlabeled data.
Moredetailed explanation about their generation can befound in Section 3.To explore the effect of labeled and unlabeledtraining data to our NER, two types of experimentswere conducted.
For the supervised approach, thelabels in the training data were previously known.For the semi-supervised approach, the labels in thetraining data were hidden.
We used bootstrapping(Abney, 2002) which refers to a problem settingin which one is given a small set of labeled dataand a large set of unlabeled data, and the task is toinduce a classifier.?
Goals:- utilize a minimal amount of supervised ex-amples;3?President of the United States?4?Technical University of Catalun?a?5book titles, sport events, etc.- obtain learning from many unlabeled ex-amples;?
General scheme:- initial supervision seed examples for train-ing an initial model;- corpus classification with seed model;- add most confident classifications to train-ing data and iterate.In our bootstrapping, a newly labeled examplewas added into the training data L, if the two clas-sifiers C1 and C2 agreed on the class of that ex-ample.
The number n of iterations for our ex-periments is set up to 25 and when this bound isreached the bootstrapping stops.
The scheme wefollow is described below.1.
for iteration = 0 .
.
.
n do2.
pool 1000 examples from unlabeled data;3. annotate all 1000 examples with classifier C1and C2;4. for each of the 1000 examples compareclasses of C1 and C2;5. add example into L only if classes of C1 andC2 agree;6. train model with L;7. calculate result8.
end forBootstrapping was previously used by (Carreraset al, 2003), who were interested in recognizingCatalan names using Spanish resources.
(Beckeret al, 2005) employed bootstrapping in an ac-tive learning method for tagging entities in an as-tronomic domain.
(Yarowsky, 1995) and (Mi-halcea and Moldovan, 2001) utilized bootstrap-ping for word sense disambiguation.
(Collins andSinger, 1999) classified NEs through co-training,(Kozareva et al, 2005a) used self-training and co-training to detect and classify named entities innews domain, (Shen et al, 2004) conducted ex-periments with multi-criteria-based active learningfor biomedical NER.The experimental data we work with is takenfrom the CoNLL-2002 competition.
The Spanish16corpus6 comes from news domain and was previ-ously manually annotated.
The train data set con-tains 264715 words of which 18798 are entitiesand the test set has 51533 words of which 3558are entities.We decided to work with available NE anno-tated corpora in order to conduct an exhaustive andcomparative NER study when labeled and unla-beld data is present.
For our bootstrapping experi-ment, we simply ignored the presence of the labelsin the training data.
Of course this approach can beapplied to other domain or language, the only needis labeled test data to conduct correct evaluation.The evaluation is computed per NE class by thehelp of conlleval7 script.
The evaluation measuresare:Precision =number of correct answers found by the systemnumber of answers given by the system(1)Recall =number of correct answers found by the systemnumber of correct answers in the test corpus(2)F?=1 =2?
Precision?
RecallPrecision + Recall(3)3 Feature extractionRecently diverse machine learning techniques areutilized to resolve various NLP tasks.
For all ofthem crucial role plays the feature extraction andselection module, which leads to optimal classifierperformance.
This section describes the featuresused for our Named Entity Recognition task.Feature vectors ?i={f1,...,fn} are constructed.The total number of features is denoted by n, and?i corresponds to the number of examples in thedata.
In our experiment features represent contex-tual, lexical and gazetteer information.
Here wenumber each feature and its corresponding argu-ment.f1: all letters of w08 are in capitals;f2-f8: w?3, w?2, w?1, w0, w+1, w+2, w+3 ini-tiate in capitals;f9: position of w0 in the current sentence;f10: frequency of w0;f11-f17: word forms of w0 and the words in[?3,+3] window;f18: first word making up the entity;f19: second word making up the entity, ifpresent;6http://www.cnts.ua.ac.be/conll2002/ner/data/7http://www.cnts.ua.ac.be/conll2002/ner/bin/8w0 indicates the word to be classified.f20: w?1 is trigger word for location, person ororganization;f21: w+1 is trigger word for location, person ororganization;f22: w0 belongs to location gazetteer list;f23: w0 belongs to first person name gazetteerlist;f24: w0 belongs to family name gazetteer list;f25: 0 if the majority of the words in an entityare locations, 1 if the majority of the words in anentity are persons and 2 otherwise.Features f22, f23, f24 were automatically ex-tracted by a simple pattern validation method wepropose below.The corpus from where the gazetteer lists wereextracted, forms part of Efe94 and Efe95 Spanishcorpora provided for the CLEF9 competitions.
Weconducted a simple preprocessing, where all sgmldocuments were merged in a single file and onlythe content situated among the text tags was ex-tracted and considered for further processing.
Asa result, we obtained 1 Gigabyte of unlabeled data,containing 173468453 words.
The text was tok-enized and the frequency of all unigrams in thecorpus was gathered.The algorithm we propose and use to obtainlocation and person gazetteer lists is very simple.It consists in finding and validating common pat-terns, which can be constructed and utilized alsofor languages other than Spanish.The location pattern ?prepi, wj?, looks forpreposition i which indicates location in the Span-ish language and all corresponding right capital-ized context words wj for preposition i.
The de-pendency relation between prepi and wj , con-veys the semantic information on the selection re-strictions imposed by the two related words.
Ina walk through example the pattern ?en, ?
?, ex-tracts all right capitalized context words wj as{Argentina, Barcelona, Madrid, Valencia} placednext to preposition ?en?.
These words are takenas location candidates.
The selection restrictionimplies searching for words appearing after thepreposition ?en?
(e.g.
en Madrid) and not beforethe preposition (e.g.
Madrid en).The termination of the pattern extraction ?en,?
?,initiates the extraction phase for the next preposi-tions in prepi = {en, En, desde, Desde, hacia, Ha-cia}.
This processes is repeated until the completeset of words in the preposition set are validated.Table 1 represents the number of entities extracted9http://www.clef-campaign.org/17by each one of the preposition patterns.pi en En desde Desde hacia Haciawj 15567 2381 1773 320 1336 134Table 1: Extracted entitiesThe extracted capitalized words are passedthrough a filtering process.
Bigrams ?prepiCapitalized wordj?
with frequency lower than20 were automatically discarded, because wesaw that this threshold removes words that donot tend to appear very often with the lo-cation prepositions.
In this way misspelledwords as Bacelona instead of Barcelona werefiltered.
From another side, every capitalizedword composed of two or three characters, forinstance ?La, Las?
was initiated in a trigram?prepi, Capitalized wordj , Capitalized wordj+1?
val-idation pattern.
If these words were seen in com-bination with other capitalized words and their tri-gram frequency was higher then 20 they were in-cluded in the location gazetteer file.
With this tri-gram validation pattern, locations as ?Los Ange-les?, ?Las Palmas?, ?La Corun?a?
,?Nueva York?10were extracted.In total 16819 entities with no repetition wereautomatically obtained.
The words representcountries around the world, European capitals andmostly Spanish cities.
Some noisy elements foundin the file were person names, which were accom-panied by the preposition ?en?.
As person nameswere capitalized and had frequency higher than thethreshold we placed, it was impossible for thesenames to be automatically detected as erroneousand filtered.
However we left these names, sincethe gazetteer attributes we maintain are mutuallynonexclusive.
This means the name ?Jordan?
canbe seen in location gazetteer indicating the coun-try Jordan and in the same time can be seen in theperson name list indicating the person Jordan.
Ina real NE application such case is reality, but forthe determination of the right category name en-tity disambiguation is needed as in (Pedersen etal., 2005).Person gazetteer is constructed with graph ex-ploration algorithm.
The graph consists of:1. two kinds of nodes:?
First Names?
Family Names10New York2.
undirected connections between First Namesand Family Names.The graph connects Family Names with FirstNames, and vice versa.
In practice, such a graph isnot necessarily connected, as there can be unusualfirst names and surnames which have no relationwith other names in the corpus.
Though, the cor-pus is supposed to contain mostly common namesin one and the same language, names from otherlanguages might be present too.
In this case, ifthe foreign name is not connected with a Spanishname, it will never be included in the name list.Therefore, starting from some common Span-ish name will very probably place us in the largestconnected component11.
If there exist other differ-ent connected components in the graph, these willbe outliers, corresponding to names pertaining tosome other language, or combinations of both veryunusual first name and family name.
The largerthe corpus is, the smaller the presence of such ad-ditional connected components will be.The algorithm performs an uninformed breadth-first search.
As the graph is not a tree, the stopcondition occurs when no more nodes are found.Nodes and connections are found following thepattern ?First name, Family name?.
The nodefrom which we start the search can be a commonSpanish first or family name.
In our example westarted from the Spanish common first name Jose?.The notation ?i, j?
?
C refers to finding in thecorpus C the regular expression12[A-Z][a-z]* [A-Z][a-z]*This regular expression indicates a possible rela-tion between first name and family name.
Thescheme of the algorithm is the following:Let C be the corpus, F be the set of first names,and S be the set of family names.1.
F = {?Jose??}2.
?i ?
F doSnew = Snew ?
{j} ,?j | ?i, j?
?
C3.
S = S ?
Snew4.
?j ?
S doFnew = Fnew ?
{i} ,?i | ?i, j?
?
C11A connected component refers to a maximal connectedsubgraph, in graph theory.
A connected graph, is a graphcontaining only one connected component.12For Spanish some other characters have to be added tothe regular expression, such as n?
and accents.18ManoloJoseMariaGarciaMartinezFernandezJohn LennonFirstFamilyRelationsnamenodesnamenodesConnectedComponentConnectedComponentFigure 1: An example of connected components.5.
F = F ?
Fnew6.
if (Fnew 6= ?)
?
(Snew 6= ?
)then goto 2.else finish.Suppose we have a corpus containing the fol-lowing person names: {?Jose?
Garc?
?a?, ?Jose?Mart?
?nez?, ?Manolo Garc?
?a?, ?Mar?
?a Mart??nez?,?Mar?
?a Ferna?ndez?, ?John Lennon?}
?
C.Initially we have F = {?Jose??}
and S = ?.
Af-ter the 3rd step we would have S = {?Garc??a?,?Mart??nez?
}, and after the 5th step: F = {?Jose?
?,?Manolo?, ?Mar??a?}.
During the next iteration?Ferna?ndez?
would also be added to S, as ?Mar?
?a?is already present in F .
Neither ?John?, nor?Lennon?
are connected to the rest of the names,so these will never be added to the sets.
This canbe seen in Figure 1 as well.In our implementation, we filtered relations ap-pearing less than 10 times.
Thus rare combina-tions like ?Jose Madrid, Mercedes Benz?
are fil-tered.
Noise was introduced from names related toboth person and organization names.
For examplethe Spanish girl name Mercedes, lead to the nodeBenz, and as ?Mercedes Benz?
refers also to thecar producing company, noisy elements started tobe added through the node ?Benz?.
In total 13713fist names and 103008 surnames have been auto-matically extracted.We believe and prove that constructing auto-matic location and person name gazetteer listswith the pattern search and validation model wepropose is a very easy and practical task.
Withour approach thousands of names can be obtained,especially given the ample presence of unlabeleddata and the World Wide Web.The purpose of our gazetteer construction wasnot to make complete gazetteer lists, but rathergenerate in a quick and automatic way lists ofnames that can help during our feature construc-tion module.4 Experiments for delimitation processIn this section we describe the conducted exper-iments for named entity detection.
Previously(Kozareva et al, 2005b) demonstrated that in su-pervised learning only superficial features as con-text and ortografics are sufficient to identify theboundaries of a Named Entity.
In our experimentthe superficial features f1 ?
f10 were used by thesupervised and semi-supervised classifiers.
Table2 shows the obtained results for Begin and Insidetags, which actually detect the entities and the totalBIO tag performance.experiment B I BIOSupervised 94.40 85.74 91.88Bootstrapped 87.47 68.95 81.62Table 2: F-score of detected entities.On the first row are the results of the super-vised method and on the second row are the high-est results of the bootstrapping achieved in itsseventeenth iteration.
For the supervised learn-ing 91.88% of the entity boundaries were cor-rectly identified and for the bootstrapping 81.62%were correctly detected.
The lower performanceof bootstrapping is due to the noise introduced dur-ing the learning.
Some examples were learnedwith the wrong class and others didn?t introducenew information in the training data.Figure 2 presents the learning curve of the boot-strapping processes for 25 iterations.
On each it-eration 1000 examples were tagged, but only theexamples having classes that coincide by the twoclassifiers were later included in the training data.We should note that for each iteration the sameamount of B, I and O classes was included.
Thusthe balance among the three different classes in thetraining data is maintained.According to z?
statistics (Dietterich, 1998),the highest score reached by bootstrapping can-not outperform the supervised method, however ifboth methods were evaluated on small amount ofdata the results were similar.190 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25606570758085iterationsf?scoreFigure 2: Bootstrapping performance5 Experiments for classification processIn a Named Entity classification process, to thepreviously detected Named Entities a predefinedcategory of interest such as name of person, orga-nization, location or miscellaneous names shouldbe assigned.
To obtain a better idea of the perfor-mance of the classification methods, several exper-iments were conducted.
The influence of the au-tomatically extracted gazetteers was studied, and acomparison of the supervised and semi-supervisedmethods was done.experiment PER LOC ORG MISCNoGazetteerSup.
80.98 71.66 73.72 49.94GazetteerSup.
84.32 75.06 77.83 53.98Bootstrapped 62.59 51.19 50.18 33.04Table 3: F-score of classified entities.Table 3 shows the obtained results for each oneof the experimental settings.
The first row indi-cates the performance of the supervised classifierwhen no gazetteer information is present.
Theclassifier used f1, f2, f3, f4, f5, f6, f7, f8, f18,f19, f20, f21 attributes.
The performance of thesecond row concerns the same classifier, but in-cluding the gazetteer information by adding f22,f23, f24 and f25 attributes.
The third row relates tothe bootstrapping process.
The attributes used forthe supervised and semi-supervised learning werethe same.Results show that among all classes, miscella-neous is the one with the lowest performance.
Thisis related to the heterogeneous information of thecategory.
The other three categories performedabove 70%.
As expected gazetteer informationcontributed for better distinction of person and lo-cation names.
Organization names benefitted fromthe contextual information, the organization trig-ger words and the attribute validating if an entityis not a person or location then is treated as anorganization.
Bootstrapping performance was nothigh, due to the previously 81% correctly detectednamed entity boundaries and from another side tothe training examples which were incorrectly clas-sified and included into the training data.In our experiment, unlabeled data was used toconstruct in an easy and effective way person andlocation gazetteer lists.
By their help supervisedand semi-supervised classifiers improved perfor-mance.
Although one semi-supervised methodcannot reach the performance of a supervised clas-sifier, we can say that results are promising.
Wecall them promising in the aspect of constructingNE recognizer for languages with no resources oreven adapting the present Spanish Named Entitysystem to other domain.6 Conclusions and future workIn this paper we proposed and implemented apattern validation search in an unlabeled corpusthough which gazetteer lists were automaticallygenerated.
The gazetteers were used as featuresby a Named Entity Recognition system.
The per-formance of this NER system, when labeled andunlabeled training data was available, was mea-sured.
A comparative study for the informationcontributed by the gazetteers in the entity classifi-cation process was shown.In the future we intend to develop automaticgazetteers for organization and product names.
Itis also of interest to divide location gazetteers insubcategories as countries, cities, rivers, moun-tains as they are useful for Geographic Informa-tion Retrieval systems.
To explore the behaviorof named entity bootstrapping, other domains asbioinformatics will be explored.Acknowledgements Many thanks to the threeanonymous reviewers for their useful commentsand suggestions.This research has been partially funded by theSpanish Government under project CICyT numberTIC2003-0664-C02-02 and PROFIT number FIT-340100-2004-14 and by the Valencia Governmentunder project numbers GV04B-276 and GV04B-268.20ReferencesSteven P. Abney.
2002.
Bootstrapping.
In Proceedingsof Association of Computational Linguists, pages360?367.Markus Becker, Ben Hachey, Beatrice Alex, and ClaireGrover.
2005.
Optimising selective sampling forbootstrapping named entity recognition.
In Pro-ceedings of the Workshop on Learning with MultipleView, ICML, pages 5?10.
Bonn, Germany.Daniel M. Bikel, Scott Miller, Richard Schwartz, andRalph Weischedel.
Nymble: a high-performancelearning name-finder.
In Proceedings of Conferenceon Applied Natural Language Processing.William J Black, Fabio Rinaldi, and David Mowatt.1998.
Facile: Description of the ne system used formuc-7.
In Proceedings of MUC-7.Andrew Borthwick.
1999.
A Maximum Entropy Ap-proach to Named Entity Recognition.
Ph.D. thesis,New York University., September.Xavier Carreras, Llu?
?s Ma`rques, and Llu?
?s Padro?.2002.
Named entity extraction using adaboost.In Proceedings of CoNLL-2002, pages 167?170.Taipei, Taiwan.Xavier Carreras, Llu?
?s Ma`rquez, and Llu?
?s Padro?.2003.
Named entity recognition for catalan us-ing only spanish resources and unlabelled data.
InEACL, pages 43?50.Michael Collins and Yoram Singer.
1999.
Unsuper-vised models for named entity classification.
In Pro-ceedings of the Joint SIGDAT Conference on Empir-ical Methods in Natural Language Processing andVery Large Corpora.Walter Daelemans, Jakub Zavrel, Ko van der Sloot,and Antal van den Bosch.
2003.
Timbl: Tilburgmemory-based learner.
Technical Report ILK 03-10, Tilburg University, November.Thomas G. Dietterich.
1998.
Approximate statisticaltest for comparing supervised classification learningalgorithms.
Neural Computation, 10(7):1895?1923.David Fisher, Stephen Soderland, Joseph McCarthy,Fangfang Feng, and Wendy Lehnert.
1997.
De-scription of the umass system as used for muc-6.
InProceedings of MUC-6.Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-pher D. Manning.
2003.
Named entity recognitionwith character-level models.
In Walter Daelemansand Miles Osborne, editors, Proceedings of CoNLL-2003, pages 180?183.
Edmonton, Canada.Zornitsa Kozareva, Boyan Bonev, and Andres Mon-toyo.
2005a.
Self-training and co-training for span-ish named entity recognition.
In 4th Mexican Inter-national Conference on Artificial Intelligence, pages770?780.Zornitsa Kozareva, Oscar Ferra?ndez, Andres Montoyo,and Rafael Mun?oz.
2005b.
Using language re-source independent detection for spanish named en-tity recognition.
In Proceedings of the Conferenceon Recent Advances in Natural Language Process-ing (RANLP 2005), pages 279?283.Zornitsa Kozareva, Oscar Ferra?ndez, Andre?s Montoyo,Rafael Mun?oz, and Armando Sua?rez.
2005c.
Com-bining data-driven systems for improving named en-tity recognition.
In NLDB, pages 80?90.James Mayfield, Paul McNamee, and Christine Pi-atko.
2003.
Named entity recognition using hun-dreds of thousands of features.
In Walter Daelemansand Miles Osborne, editors, Proceedings of CoNLL-2003, pages 184?187.
Edmonton, Canada.Rada Mihalcea and Dan I. Moldovan.
2001.
A highlyaccurate bootstrapping algorithm for word sense dis-ambiguation.
International Journal on Artificial In-telligence Tools, 10(1-2):5?21.Ted Pedersen, Amruta Purandare, and Anagha Kulka-rni.
2005.
Name discrimination by clustering sim-ilar contexts.
In Computational Linguistics and In-telligent Text Processing, 6th International Confer-ence, CICLing 2005, Mexico City, pages 226?237.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, andChew-Lim Tan.
2004.
Multi-criteria-based activelearning for named entity recognition.
In Proceed-ings of Association of Computational Linguists.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the conll-2003 shared task:Language-independent named entity recognition.
InWalter Daelemans and Miles Osborne, editors, Pro-ceedings of CoNLL-2003, pages 142?147.
Edmon-ton, Canada.Erik F. Tjong Kim Sang.
2002a.
Introduction tothe conll-2002 shared task: Language-independentnamed entity recognition.
In Proceedings ofCoNLL-2002, pages 155?158.
Taipei, Taiwan.Erik F. Tjong Kim Sang.
2002b.
Memory-basednamed entity recognition.
In Proceedings ofCoNLL-2002, pages 203?206.
Taipei, Taiwan.Dekai Wu, Grace Ngai, and Marine Carpuat.
2003.A stacked, voted, stacked model for named entityrecognition.
In Walter Daelemans and Miles Os-borne, editors, Proceedings of CoNLL-2003, pages200?203.
Edmonton, Canada.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Meet-ing of the Association for Computational Linguis-tics, pages 189?196.21
