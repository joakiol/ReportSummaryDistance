Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1073?1080,Sydney, July 2006. c?2006 Association for Computational LinguisticsImproving QA Accuracy by Question InversionJohn PragerIBM T.J. Watson Res.
Ctr.Yorktown HeightsN.Y.
10598jprager@us.ibm.comPablo DuboueIBM T.J. Watson Res.
Ctr.Yorktown HeightsN.Y.
10598duboue@us.ibm.comJennifer Chu-CarrollIBM T.J. Watson Res.
Ctr.Yorktown HeightsN.Y.
10598jencc@us.ibm.comAbstractThis paper demonstrates a conceptually simplebut effective method of increasing the accuracyof QA systems on factoid-style questions.
Wedefine the notion of an inverted question, andshow that by requiring that the answers to theoriginal and inverted questions be mutually con-sistent, incorrect answers get demoted in confi-dence and correct ones promoted.
Additionally,we show that lack of validation can be used toassert no-answer (nil) conditions.
We demon-strate increases of performance on TREC andother question-sets, and discuss the kinds of fu-ture activities that can be particularly beneficialto approaches such as ours.1 IntroductionMost QA systems nowadays consist of the followingstandard modules:  QUESTION PROCESSING, to de-termine the bag of words for a query and the desiredanswer type (the type of the entity that will be of-fered as a candidate answer); SEARCH, which willuse the query to extract a set of documents or pas-sages from a corpus; and ANSWER SELECTION,which will analyze the returned documents or pas-sages for instances of the answer type in the mostfavorable contexts.
Each of these components im-plements a set of heuristics or hypotheses, as de-vised by their authors (cf.
Clarke et al 2001, Chu-Carroll et al 2003).When we perform failure analysis on questions in-correctly answered by our system, we find that thereare broadly speaking two kinds of failure.
There areerrors (we might call them bugs) on the implementa-tion of the said heuristics: errors in tagging, parsing,named-entity recognition; omissions in synonymlists; missing patterns, and just plain programmingerrors.
This class can be characterized by being fix-able by identifying incorrect code and fixing it, oradding more items, either explicitly or through train-ing.
The other class of errors (what we might callunlucky) are at the boundaries of the heuristics;situations were the system did not do anything?wrong,?
in the sense of bug, but circumstances con-spired against finding the correct answer.Usually when unlucky errors occur, the system gen-erates a reasonable query and an appropriate answertype, and at least one passage containing the rightanswer is returned.
However, there may be returnedpassages that have a larger number of query termsand an incorrect answer of the right type, or thequery terms might just be physically closer to theincorrect answer than to the correct one.
ANSWERSELECTION modules typically work either by tryingto prove the answer is correct (Moldovan & Rus,2001) or by giving them a weight produced bysumming a collection of heuristic features (Radev etal., 2000); in the latter case candidates having a lar-ger number of matching query terms, even if they donot exactly match the context in the question, mightgenerate a larger score than a correct passage withfewer matching terms.To be sure, unlucky errors are usually bugs whenconsidered from the standpoint of a system with amore sophisticated heuristic, but any system at anypoint in time will have limits on what it tries to do;therefore the distinction is not absolute but is rela-tive to a heuristic and system.It has been argued (Prager, 2002) that the success ofa QA system is proportional to the impedance matchbetween the question and the knowledge sourcesavailable.
We argue here similarly.
Moreover, webelieve that this is true not only in terms of the cor-rect answer, but the distracters,1 or incorrect answerstoo.
In QA, an unlucky incorrect answer is not usu-ally predictable in advance; it occurs because of acoincidence of terms and syntactic contexts thatcause it to be preferred over the correct answer.
Ithas no connection with the correct answer and isonly returned because its enclosing passage so hap-pens to exist in the same corpus as the correct an-swer context.
This would lead us to believe that if a1 We borrow the term from multiple-choice test design.1073different corpus containing the correct answer wereto be processed, while there would be no guaranteethat the correct answer would be found, it would beunlikely (i.e.
very unlucky) if the same incorrect an-swer as before were returned.We have demonstrated elsewhere (Prager et al2004b) how using multiple corpora can improve QAperformance, but in this paper we achieve similargoals without using additional corpora.
We note thatfactoid questions are usually about relations betweenentities, e.g.
?What is the capital of France?
?, whereone of the arguments of the relationship is soughtand the others given.
We can invert the question bysubstituting the candidate answer back into the ques-tion, while making one of the given entities the so-called wh-word, thus ?Of what country is Paris thecapital??
We hypothesize that asking this question(and those formed from other candidate answers)will locate a largely different set of passages in thecorpus than the first time around.
As will be ex-plained in Section 3, this can be used to decrease theconfidence in the incorrect answers, and also in-crease it for the correct answer, so that the latter be-comes the answer the system ultimately proposes.This work is part of a continuing program of demon-strating how meta-heuristics, using what might becalled ?collateral?
information, can be used to con-strain or adjust the results of the primary QA system.In the next Section we review related work.
In Sec-tion 3 we describe our algorithm in detail, and inSection 4 present evaluation results.
In Section 5 wediscuss our conclusions and future work.2 Related WorkLogic and inferencing have been a part of Question-Answering since its earliest days.
The first suchsystems were natural-language interfaces to expertsystems, e.g., SHRDLU (Winograd, 1972), or todatabases, e.g., LIFER/LADDER (Hendrix et al1977).
CHAT-80 (Warren & Pereira, 1982), for in-stance, was a DCG-based NL-query system aboutworld geography, entirely in Prolog.
In thesesystems, the NL question is transformed into a se-mantic form, which is then processed further.
Theiroverall architecture and system operation is verydifferent from today?s systems, however, primarilyin that there was no text corpus to process.Inferencing is a core requirement of systems thatparticipate in the current PASCAL RecognizingTextual Entailment (RTE) challenge (seehttp://www.pascal-network.org/Challenges/RTE and.../RTE2).
It is also used in at least two of the morevisible end-to-end QA systems of the present day.The LCC system (Moldovan & Rus, 2001) uses aLogic Prover to establish the connection between acandidate answer passage and the question.
Textterms are converted to logical forms, and the ques-tion is treated as a goal which is ?proven?, with real-world knowledge being provided by ExtendedWordNet.
The IBM system PIQUANT (Chu-Carroll et al, 2003) used Cyc (Lenat, 1995) in an-swer verification.
Cyc can in some cases confirm orreject candidate answers based on its own store ofinstance information; in other cases, primarily of anumerical nature, Cyc can confirm whether candi-dates are within a reasonable range established fortheir subtype.At a more abstract level, the use of inversions dis-cussed in this paper can be viewed as simply an ex-ample of finding support (or lack of it) for candidateanswers.
Many current systems (see, e.g.
(Clarke etal., 2001; Prager et al 2004b)) employ redundancyas a significant feature of operation:  if the same an-swer appears multiple times in an internal top-n list,whether from multiple sources or multiple algo-rithms/agents, it is given a confidence boost, whichwill affect whether and how it gets returned to theend-user.The work here is a continuation of previous workdescribed in (Prager et al 2004a,b).
In the formerwe demonstrated that for a certain kind of question,if the inverted question were given, we could im-prove the F-measure of accuracy on a question setby 75%.
In this paper, by contrast, we do not manu-ally provide the inverted question, and in the secondevaluation presented here we do not restrict thequestion type.3 Algorithm3.1 System ArchitectureA simplified block-diagram of our PIQUANT sys-tem is shown in Figure 1.
The outer block on theleft, QS1, is our basic QA system, in which theQUESTION PROCESSING (QP), SEARCH (S) andANSWER SELECTION (AS) subcomponents are indi-cated.
The outer block on the right, QS2, is anotherQA-System that is used to answer the inverted ques-tions.
In principle QS2 could be QS1 but parameter-ized differently, or even an entirely different system,but we use another instance of QS1, as-is.
Theblock in the middle is our Constraints Module CM,which is the subject of this paper.1074The Question Processing component of QS2 is notused in this context since CM simulates its output bymodifying the output of QP in QS1, as described inSection 3.3.3.2 Inverting QuestionsOur open-domain QA system employs a named-entity recognizer that identifies about a hundredtypes.
Any of these can be answer types, and thereare corresponding sets of patterns in the QUESTIONPROCESSING module to determine the answer typesought by any question.
When we wish to invert aquestion, we must find an entity in the questionwhose type we recognize; this entity then becomesthe sought answer for the inverted question.
We callthis entity the inverted or pivot term.Thus for the question:(1) ?What was the capital of Germany in 1985?
?Germany is identified as a term with a known type(COUNTRY).
Then, given the candidate answer<CANDANS>, the inverted question becomes(2) ?Of what country was < CANDANS> the capitalin 1985?
?Some questions have more than one invertible term.Consider for example:(3) ?Who was the 33rd president of the U.S.?
?This question has 3 inversion points:(4) ?What number president of the U.S.
was<CANDANS>??
(5) ?Of what country was <CANDANS> the 33rdpresident??
(6) ?<CANDANS> was the 33rd what of the U.S.?
?Having more than one possible inversion is in theorya benefit, since it gives more opportunity for enforc-ing consistency, but in our current implementationwe just pick one for simplicity.
We observe ontraining data that, in general, the smaller the numberof unique instances of an answer type, the morelikely it is that the inverted question will be correctlyanswered.
We generated a set NELIST of the mostfrequently-occurring named-entity types in ques-tions; this list is sorted in order of estimated cardi-nality.It might seem that the question inversion process canbe quite tricky and can generate possibly unnaturalphrasings, which in turn can be difficult to reparse.However, the examples given above were simplyEnglish renditions of internal inverted structures ?
aswe shall see the system does not need to use a natu-ral language representation of the inverted questions.Some questions are either not invertible, or, like?How did X die??
have an inverted form (?Who diedof cancer??)
with so many correct answers that weknow our algorithm is unlikely to benefit us.
How-ever, as it is constituted it is unlikely to hurt us ei-ther, and since it is difficult to automatically identifysuch questions, we don?t attempt to intercept them.As reported in (Prager et al 2004a), an estimated79% of the questions in TREC question sets can beinverted meaningfully.
This places an upper limiton the gains to be achieved with our algorithm, butis high enough to be worth pursuing.Figure 1.
Constraints Architecture.
QS1 and QS2 are (possibly identical) QA systems.AnswersQuestionQS1QA systemQPquestion proc.SsearchASanswer selectionQS2QA systemQPquestion proc.SsearchASanswer selectionCMconstraintsmodule10753.3 Inversion AlgorithmAs shown in the previous section, not all questionshave easily generated inverted forms (even by a hu-man).
However, we do not need to explicate theinverted form in natural language in order to processthe inverted question.In our system, a question is processed by theQUESTION PROCESSING module, which produces astructure called a QFrame, which is used by the sub-sequent SEARCH and ANSWER SELECTION modules.The QFrame contains the list of terms and phrases inthe question, along with their properties, such asPOS and NE-type (if it exists), and a list of syntacticrelationship tuples.
When we have a candidate an-swer in hand, we do not need to produce the invertedEnglish question, but merely the QFrame that wouldhave been generated from it.
Figure 1 shows thatthe CONSTRAINTS MODULE takes the QFrame as oneof its inputs, as shown by the link from QP in QS1to CM.
This inverted QFrame can be generated by aset of simple transformations, substituting the pivotterm in the bag of words with a candidate answer<CANDANS>, the original answer type with the typeof the pivot term, and in the relationships the pivotterm with its type and the original answer type with<CANDANS>.
When relationships are evaluated, atype token will match any instance of that type.
Fig-ure 2 shows a simplified view of the originalQFrame for ?What was the capital of Germany in1945?
?, and Figure 3 shows the corresponding In-verted QFrame.
COUNTRY is determined to be abetter type to invert than YEAR, so ?Germany?
be-comes the pivot.
In Figure 3, the token<CANDANS> might take in turn ?Berlin?, ?Mos-cow?, ?Prague?
etc.Figure 2.
Simplified QFrameFigure 3.
Simplified Inverted QFrame.The output of QS2 after processing the invertedQFrame is a list of answers to the inverted question,which by extension of the nomenclature we call ?in-verted answers.?
If no term in the question has anidentifiable type, inversion is not possible.3.4 Profiting From InversionsBroadly speaking, our goal is to keep or re-rank thecandidate answer hit-list on account of inversionresults.
Suppose that a question Q is invertedaround pivot term T, and for each candidate answerCi, a list of ?inverted?
answers {Cij} is generated asdescribed in the previous section.
If T is on one ofthe {Cij}, then we say that Ci is validated.
Valida-tion is not a guarantee of keeping or improving Ci?sposition or score, but it helps.
Most cases of failureto validate are called refutation; similarly, refutationof Ci is not a guarantee of lowering its score or posi-tion.It is an open question how to adjust the results of theinitial candidate answer list in light of the results ofthe inversion.
If the scores associated with candi-date answers (in both directions) were true prob-abilities, then a Bayesian approach would be easy todevelop.
However, they are not in our system.
Inaddition, there are quite a few parameters that de-scribe the inversion scenario.Suppose Q generates a list of the top-N candidates{Ci}, with scores {Si}.
If this inversion methodwere not to be used, the top candidate on this list,C1, would be the emitted answer.
The question gen-erated by inverting about T and substituting Ci isQTi.
The system is fixed to find the top 10 passagesresponsive to QTi, and generates an ordered list Cijof candidate answers found in this set.Each inverted question QTi is run through our sys-tem, generating inverted answers {Cij}, with scores{Sij}, and whether and where the pivot term T showsup on this list, represented by a list of positions {Pi},where Pi is defined as:Pi  =  j    if Cij = T, for some jPi  =  -1 otherwiseWe added to the candidate list the special answernil, representing ?no answer exists in the corpus.
?As described earlier, we had observed from trainingdata that failure to validate candidates of certaintypes (such as Person) would not necessarily be areal refutation, so we established a set of typesSOFTREFUTATION which would contain the broadestof our types.
At the other end of the spectrum, weobserved that certain narrow candidate types such asUsState would definitely be refuted if validationdidn?t occur.
These are put in set MUSTCONSTRAIN.Our goal was to develop an algorithm for recomput-ing all the original scores {Si} from some combina-tion (based on either arithmetic or decision-trees) ofKeywords: {1945, <CANDANS>, capital}AnswerType: COUNTRYRelationships: {(COUNTRY, capital), (capital,<CANDANS>), (capital, 1945)}Keywords: {1945, Germany, capital}AnswerType: CAPITALRelationships: {(Germany, capital), (capital,CAPITAL), (capital, 1945)}1076{Si} and {Sij} and membership of SOFTREFUTATIONand MUSTCONSTRAIN.
Reliably learning all thoseweights, along with set membership, was not possi-ble given only several hundred questions of trainingdata.
We therefore focused on a reduced problem.We observed that when run on TREC question sets,the frequency of the rank of our top answer fell offrapidly, except with a second mode when the tailwas accumulated in a single bucket.
Our numbersfor TRECs 11 and 12 are shown in Table 1.Top answer rank TREC11 TREC121 170 1082 35 323 23 144 7 75 14 9elsewhere 251 244% correct 34 26Table 1.
Baseline statistics for TREC11-12.We decided to focus on those questions where wegot the right answer in second place (for brevity,we?ll call these second-place questions).
Given thatTREC scoring only rewards first-place answers, itseemed that with our incremental approach wewould get most benefit there.
Also, we were keen tolimit the additional response time incurred by ourapproach.
Since evaluating the top N answers to theoriginal question with the Constraints process re-quires calling the QA system another N times perquestion, we were happy to limit N to 2.
In addition,this greatly reduced the number of parameters weneeded to learn.For the evaluation, which consisted of determining ifthe resulting top answer was right or wrong, it meantultimately deciding on one of three possible out-comes:  the original top answer, the original secondanswer, or nil.
We hoped to promote a significantnumber of second-place finishers to top place andintroduce some nils, with minimal disturbance ofthose already in first place.We used TREC11 data for training, and establisheda set of thresholds for a decision-tree approach todetermining the answer, using Weka (Witten &Frank, 2005).
We populated sets SOFTREFUTATIONand MUSTCONSTRAIN by manual inspection.The result is Algorithm A, where (i ?
{1,2}) ando The Ci are the original candidate answerso The ak are learned parameters (k ?
{1..13})o Vi means the ith answer was validatedo Pi was the rank of the validating answer to ques-tion QTio Ai was the score of the validating answer to QTi.Algorithm A.
Answer re-ranking using con-straints validation data.1.
If C1 = nil and V2,    return C22.
If V1 and A1 > a1,     return C13.
If not V1 and not V2 andtype(T) ?
MUSTCONSTRAIN,return nil4.
If  not V1 and not V2 andtype(T) ?SOFTREFUTATION,if S1 > a2,, return C1 else nil5.
If not V2,    return C16.
If not V1 and V2 andA2 > a3 and P2 < a4 andS1-S2 < a5 and S2 > a6, return C27.
If V1 and V2 and(A2 - P2/a7) > (A1 - P1/a7) andA1 < a8 and P1 > a9 andA2 < a10 and P2 > a11 andS1-S2 < a12  and (S2 - P2/a7) > a13,return C28.
else return C14 EvaluationDue to the complexity of the learned algorithm, wedecided to evaluate in stages.
We first performed anevaluation with a fixed question type, to verify thatthe purely arithmetic components of the algorithmwere performing reasonably.
We then evaluated onthe entire TREC12 factoid question set.4.1 Evaluation 1We created a fixed question set of 50 questions ofthe form ?What is the capital of X?
?, for each statein the U.S.
The inverted question ?What state is Zthe capital of??
was correctly generated in eachcase.
We evaluated against two corpora: theAQUAINT corpus, of a little over a million news-wire documents, and the CNS corpus, with about37,000 documents from the Center for Nonprolifera-tion Studies in Monterey, CA.
We expected there tobe answers to most questions in the former corpus,so we hoped there our method would be useful inconverting 2nd place answers to first place.
The lat-ter corpus is about WMDs, so we expected there tobe holes in the state capital coverage2, for which nilidentification would be useful.32 We manually determined that only 23 state capitals were at-tested to in the CNS corpus, compared with all in AQUAINT.3 We added Tbilisi to the answer key for ?What is the capi-tal of Georgia?
?, since there was nothing in the question todisambiguate Georgia.1077The baseline is our regular search-based QA-Systemwithout the Constraint process.
In this baseline sys-tem there was no special processing for nil ques-tions, other than if the search (which alwayscontained some required terms) returned no docu-ments.
Our results are shown in Table 2.AQUAINTbaselineAQUAINTw/con-straintsCNSbaselineCNSw/con-straintsFirsts(non-nil)39/50 43/50 7/23 4/23Totalnils0/0 0/0 0/27 16/27Totalfirsts39/50 43/50 7/50 20/50%correct78 86 14 40Table 2.
Evaluation on AQUAINT and CNScorpora.On the AQUAINT corpus, four out of seven 2ndplace finishers went to first place.
On the CNS cor-pus 16 out of a possible 26 correct no-answer caseswere discovered, at a cost of losing three previouslycorrect answers.
The percentage correct score in-creased by a relative 10.3% for AQUAINT and186% for CNS.
In both cases, the error rate wasreduced by about a third.4.2 Evaluation 2For the second evaluation, we processed the 414factoid questions from TREC12.
Of special interesthere are the questions initially in first and secondplaces, and in addition any questions for which nilswere found.As seen in Table 1, there were 32 questions whichoriginally evaluated in rank 2.
Of these, four ques-tions were not invertible because they had no termsthat were annotated with any of our named-entitytypes, e.g.
#2285 ?How much does it cost for gas-tric bypass surgery?
?Of the remaining 28 questions, 12 were promoted tofirst place.
In addition, two new nils were found.On the down side, four out of 108 previous firstplace answers were lost.
There was of coursemovement in the ranks two and beyond whenevernils were introduced in first place, but these do notaffect the current TREC-QA factoid correctnessmeasure, which is whether the top answer is corrector not.
These results are summarized in Table 3.While the overall percentage improvement wassmall, note that only second?place answers werecandidates for re-ranking, and 43% of these werepromoted to first place and hence judged correct.Only 3.7% of originally correct questions werecasualties.
To the extent that these percentages arestable across other collections, as long as the size ofthe set of second-place answers is at least about 1/10of the set of first-place answers, this form of theConstraint process can be applied effectively.Baseline ConstraintsFirsts (non-nil) 105 113nils 3 5Total firsts 108 118% correct 26.1 28.5Table 3.
Evaluation on TREC12 Factoids.5 DiscussionThe experiments reported here pointed out manyareas of our system which previous failure analysisof the basic QA system had not pinpointed as beingtoo problematic, but for which improvement shouldhelp the Constraints process.
In particular, this workbrought to light a matter of major significance, termequivalence, which we had not previously focusedon too much (and neither had the QA community asa whole).
We will discuss that in Section 5.4.Quantitatively, the results are very encouraging, butit must be said that the number of questions that weevaluated were rather small, as a result of the com-putational expense of the approach.From Table 1, we conclude that the most mileage isto be achieved by our QA-System as a whole by ad-dressing those questions which did not generate acorrect answer in the first one or two positions.
Wehave performed previous analyses of our system?sfailure modes, and have determined that the pas-sages that are output from the SEARCH componentcontain the correct answer 70-75% of the time.
TheANSWER SELECTION module takes these passagesand proposes a candidate answer list.
Since the CON-STRAINTS MODULE?s operation can be viewed as are-ranking of the output of ANSWER SELECTION, itcould in principle boost the system?s accuracy up tothat 70-75% level.
However, this would either re-quire a massive training set to establish all the pa-rameters and weights required for all the possible re-ranking decisions, or a new model of the answer-listdistribution.5.1 Probability-based ScoresOur ANSWER SELECTION component assigns scoresto candidate answers on the basis of the number ofterms and term-term syntactic relationships from the1078original question found in the answer passage(where the candidate answer and wh-word(s) in thequestion are identified terms).
The resulting num-bers are in the range 0-1, but are not true probabili-ties (e.g.
where answers with a score of 0.7 would becorrect 70% of the time).
While the generatedscores work well to rank candidates for a givenquestion, inter-question comparisons are not gener-ally meaningful.
This made the learning of a deci-sion tree (Algorithm A) quite difficult, and weexpect that when addressed, will give better per-formance to the Constraints process (and maybe asimpler algorithm).
This in turn will make it morefeasible to re-rank the top 10 (say) original answers,instead of the current 2.5.2 Better confidencesEven if no changes to the ranking are produced bythe Constraints process, then the mere act of valida-tion (or not) of existing answers can be used to ad-just confidence scores.
In TREC2002 (Voorhees,2003), there was an evaluation of responses accord-ing to systems?
confidences in their own answers,using the Average Precision (AP) metric.
This is animportant consideration, since it is generally betterfor a system to say ?I don?t know?
than to give awrong answer.
On the TREC12 questions set, ourAP score increased 2.1% with Constraints, using thealgorithm we presented in (Chu-Carroll et al 2002).5.3 More complete NERExcept in pure pattern-based approaches, e.g.
(Brill,2002), answer types in QA systems typically corre-spond to the types identifiable by their named-entityrecognizer (NER).
There is no agreed-upon numberof classes for an NER system, even approximately.It turns out that for best coverage by ourCONSTRAINTS MODULE, it is advantageous to have arelatively large number of types.
It was mentionedin Section 4.2 that certain questions were not invert-ible because no terms in them were of a recogniz-able type.
Even when questions did have typedterms, if the types were very high-level then creatinga meaningful inverted question was problematic.For example, for QA without Constraints it is notnecessary to know the type of ?MTV?
in ?Whenwas MTV started?
?, but if it is only known to be aName then the inverted question ?What <Name>was started in 1980??
could be too general to be ef-fective.5.4 Establishing Term EquivalenceThe somewhat surprising condition that emergedfrom this effort was the need for a much more com-plete ability than had previously been recognized forthe system to establish the equivalence of two terms.Redundancy has always played a large role in QAsystems ?
the more occurrences of a candidate an-swer in retrieved passages the higher the answer?sscore is made to be.
Consequently, at the very least,a string-matching operation is needed for checkingequivalence, but other techniques are used to vary-ing degrees.It has long been known in IR that stemming or lem-matization is required for successful term matching,and in NLP applications such as QA, resources suchas WordNet (Miller, 1995) are employed for check-ing synonym and hypernym relationships; ExtendedWordNet (Moldovan & Novischi, 2002) has beenused to establish lexical chains between terms.However, the Constraints work reported here hashighlighted the need for more extensive equivalencetesting.In direct QA, when an ANSWER SELECTION modulegenerates two (or more) equivalent correct answersto a question (e.g.
?Ferdinand Marcos?
vs. ?Presi-dent Marcos?
; ?French?
vs.
?France?
), and fails tocombine them, it is observed that as long as eitherone is in first place then the question is correct andmight not attract more attention from developers.
Itis only when neither is initially in first place, butcombining the scores of correct candidates boostsone to first place that the failure to merge them isrelevant.
However, in the context of our system, weare comparing the pivot term from the original ques-tion to the answers to the inverted questions, andfailure here will directly impact validation and hencethe usefulness of the entire approach.As a consequence, we have identified the need for acomponent whose sole purpose is to establish theequivalence, or generally the kind of relationship,between two terms.
It is clear that the processingwill be very type-dependent ?
for example, if twopopulations are being compared, then a numericaldifference of 5% (say) might not be considered adifference at all; for ?Where?
questions, there areissues of granularity and physical proximity, and soon.
More examples of this problem were given in(Prager et al 2004a).
Moriceau (2006) reports asystem that addresses part of this problem by tryingto rationalize different but ?similar?
answers to theuser, but does not extend to a general-purposeequivalence identifier.6 SummaryWe have extended earlier Constraints-based workthrough the method of question inversion.
The ap-proach uses our QA system recursively, by takingcandidate answers and attempts to validate themthrough asking the inverted questions.
The outcome1079is a re-ranking of the candidate answers, with thepossible insertion of nil (no answer in corpus) as thetop answer.While we believe the approach is general, and canwork on any question and arbitrary candidate lists,due to training limitations we focused on two re-stricted evaluations.
In the first we used a fixedquestion type, and showed that the error rate wasreduced by 36% and 30% on two very different cor-pora.
In the second evaluation we focused on ques-tions whose direct answers were correct in thesecond position.
43% of these questions were sub-sequently judged correct, at a cost of only 3.7% oforiginally correct questions.
While in the future wewould like to extend the Constraints process to theentire answer candidate list, we have shown that ap-plying it only to the top two can be beneficial aslong as the second-place answers are at least a tenthas numerous as first-place answers.
We also showedthat the application of Constraints can improve thesystem?s confidence in its answers.We have identified several areas where improve-ment to our system would make the Constraintsprocess more effective, thus getting a double benefit.In particular we feel that much more attentionshould be paid to the problem of determining if twoentities are the same (or ?close enough?
).7 AcknowledgmentsThis work was supported in part by the DisruptiveTechnology Office (DTO)?s Advanced QuestionAnswering for Intelligence (AQUAINT) Programunder contract number H98230-04-C-1577.
Wewould like to thank the anonymous reviewersfor their helpful comments.ReferencesBrill, E., Dumais, S. and Banko M. ?An analysis ofthe AskMSR question-answering system.?
In Pro-ceedings of EMNLP 2002.Chu-Carroll, J., J. Prager, C. Welty, K. Czuba andD.
Ferrucci.
?A Multi-Strategy and Multi-SourceApproach to Question Answering?, Proceedingsof the 11th TREC, 2003.Clarke, C., Cormack, G., Kisman, D. and Lynam, T.?Question answering by passage selection(Multitext experiments for TREC-9)?
in Proceed-ings of the 9th TREC, pp.
673-683, 2001.Hendrix, G., Sacerdoti, E., Sagalowicz, D., SlocumJ.
: Developing a Natural Language Interface toComplex Data.
VLDB 1977: 292Lenat, D. 1995.
"Cyc: A Large-Scale Investment inKnowledge Infrastructure."
Communications ofthe ACM 38, no.
11.Miller, G. ?WordNet: A Lexical Database for Eng-lish?, Communications of the ACM 38(11) pp.39-41, 1995.Moldovan, D. and Novischi, A, ?Lexical Chains forQuestion Answering?, COLING 2002.Moldovan, D. and Rus, V., ?Logic Form Transfor-mation of WordNet and its Applicability to Ques-tion Answering?, Proceedings of the ACL, 2001.Moriceau, V. ?Numerical Data Integration for Co-operative Question-Answering?, in EACL Work-shop on Knowledge and Reasoning for LanguageProcessing (KRAQ?06), Trento, Italy, 2006.Prager, J.M., Chu-Carroll, J. and Czuba, K. "Ques-tion Answering using Constraint Satisfaction:QA-by-Dossier-with-Constraints", Proc.
42ndACL, pp.
575-582, Barcelona, Spain, 2004(a).Prager, J.M., Chu-Carroll, J. and Czuba, K. "AMulti-Strategy, Multi-Question Approach toQuestion Answering" in New Directions in Ques-tion-Answering, Maybury, M.
(Ed.
), AAAI Press,2004(b).Prager, J., "A Curriculum-Based Approach to a QARoadmap"' LREC 2002 Workshop on QuestionAnswering: Strategy and Resources, Las Palmas,May 2002.Radev, D., Prager, J. and Samn, V. "Ranking Sus-pected Answers to Natural Language Questionsusing Predictive Annotation", Proceedings ofANLP 2000, pp.
150-157, Seattle, WA.Voorhees, E. ?Overview of the TREC 2002 Ques-tion Answering Track?, Proceedings of the 11thTREC, Gaithersburg, MD, 2003.Warren, D., and F. Pereira "An efficient easilyadaptable system for interpreting natural languagequeries," Computational Linguistics, 8:3-4, 110-122, 1982.Winograd, T. Procedures as a representation for datain a computer program for under-standing naturallanguage.
Cognitive Psychology, 3(1), 1972.Witten, I.H.
& Frank, E. Data Mining.
PracticalMachine Learning Tools and Techniques.
El-sevier Press, 2005.1080
