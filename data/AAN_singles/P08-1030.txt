Proceedings of ACL-08: HLT, pages 254?262,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsRefining Event Extraction through Cross-document InferenceHeng Ji Ralph GrishmanComputer Science DepartmentNew York UniversityNew York, NY 10003, USA(hengji, grishman)@cs.nyu.eduAbstractWe apply the hypothesis of ?One Sense PerDiscourse?
(Yarowsky, 1995) to informationextraction (IE), and extend the scope of ?dis-course?
from one single document to a clusterof topically-related documents.
We employ asimilar approach to propagate consistent eventarguments across sentences and documents.Combining global evidence from related doc-uments with local decisions, we design a sim-ple scheme to conduct cross-documentinference for improving the ACE event ex-traction task1.
Without using any additionallabeled data this new approach obtained 7.6%higher F-Measure in trigger labeling and 6%higher F-Measure in argument labeling over astate-of-the-art IE system which extractsevents independently for each sentence.1 IntroductionIdentifying events of a particular type within indi-vidual documents ?
?classical?
information extrac-tion ?
remains a difficult task.
Recognizing thedifferent forms in which an event may be ex-pressed, distinguishing events of different types,and finding the arguments of an event are all chal-lenging tasks.Fortunately, many of these events will be re-ported multiple times, in different forms, bothwithin the same document and within topically-related documents (i.e.
a collection of documentssharing participants in potential events).
We can1 http://www.nist.gov/speech/tests/ace/take advantage of these alternate descriptions toimprove event extraction in the original document,by favoring consistency of interpretation acrosssentences and documents.
Several recent studiesinvolving specific event types have stressed thebenefits of going beyond traditional single-document extraction; in particular, Yangarber(2006) has emphasized this potential in his workon medical information extraction.
In this paper wedemonstrate that appreciable improvements arepossible over the variety of event types in the ACE(Automatic Content Extraction) evaluation throughthe use of cross-sentence and cross-document evi-dence.As we shall describe below, we can make use ofconsistency at several levels: consistency of wordsense across different instances of the same wordin related documents, and consistency of argu-ments and roles across different mentions of thesame or related events.
Such methods allow us tobuild dynamic background knowledge as requiredto interpret a document and can compensate for thelimited annotated training data which can be pro-vided for each event type.2 Task and Baseline System2.1 ACE Event Extraction TaskThe event extraction task we are addressing is thatof the Automatic Content Extraction (ACE) evalu-ations2.
ACE defines the following terminology:2 In this paper we don?t consider event mention coreferenceresolution and so don?t distinguish event mentions and events.254entity: an object or a set of objects in one of thesemantic categories of interestmention: a reference to an entity (typically, anoun phrase)event trigger: the main word which most clearlyexpresses an event occurrenceevent arguments: the mentions that are in-volved in an event (participants)event mention: a phrase or sentence withinwhich an event is described, including triggerand argumentsThe 2005 ACE evaluation had 8 types of events,with 33 subtypes; for the purpose of this paper, wewill treat these simply as 33 distinct event types.For example, for a sentence:Barry Diller on Wednesday quit as chief of VivendiUniversal Entertainment.the event extractor should detect a ?Person-nel_End-Position?
event mention, with the triggerword, the position, the person who quit the posi-tion, the organization, and the time during whichthe event happened:Trigger QuitArgumentsRole = Person Barry DillerRole =OrganizationVivendi UniversalEntertainmentRole = Position ChiefRole =Time-within WednesdayTable 1.
Event Extraction ExampleWe define the following standards to determinethe correctness of an event mention:?
A trigger is correctly labeled if its event typeand offsets match a reference trigger.?
An argument is correctly identified if its eventtype and offsets match any of the reference ar-gument mentions.?
An argument is correctly identified and classi-fied if its event type, offsets, and role matchany of the reference argument mentions.2.2 A Baseline Within-Sentence Event TaggerWe use a state-of-the-art English IE system as ourbaseline (Grishman et al, 2005).
This system ex-tracts events independently for each sentence.
Itstraining and test procedures are as follows.The system combines pattern matching with sta-tistical models.
For every event mention in theACE training corpus, patterns are constructedbased on the sequences of constituent heads sepa-rating the trigger and arguments.
In addition, a setof Maximum Entropy based classifiers are trained:?
Trigger Labeling: to distinguish event men-tions from non-event-mentions, to classifyevent mentions by type;?
Argument Classifier: to distinguish argumentsfrom non-arguments;?
Role Classifier: to classify arguments by ar-gument role.?
Reportable-Event Classifier: Given a trigger,an event type, and a set of arguments, to de-termine whether there is a reportable eventmention.In the test procedure, each document is scannedfor instances of triggers from the training corpus.When an instance is found, the system tries tomatch the environment of the trigger against the setof patterns associated with that trigger.
This pat-tern-matching process, if successful, will assignsome of the mentions in the sentence as argumentsof a potential event mention.
The argument clas-sifier is applied to the remaining mentions in thesentence; for any argument passing that classifier,the role classifier is used to assign a role to it.
Fi-nally, once all arguments have been assigned, thereportable-event classifier is applied to the poten-tial event mention; if the result is successful, thisevent mention is reported.3 MotivationsIn this section we shall present our motivationsbased on error analysis for the baseline event tag-ger.3.1 One Trigger Sense Per ClusterAcross a heterogeneous document corpus, a partic-ular verb can sometimes be trigger and sometimesnot, and can represent different event types.
How-ever, for a collection of topically-related docu-ments, the distribution may be much moreconvergent.
We investigate this hypothesis by au-tomatically obtaining 25 related documents foreach test text.
The statistics of some trigger exam-ples are presented in table 2.255Candidate TriggersEvent TypePerc./Freq.
astrigger in ACEtraining corporaPerc./Freq.
astrigger in testdocumentPerc./Freq.
astrigger in test +relateddocumentsCorrectEventTriggersadvance Movement_Transport 31% of 16 50% of 2 88.9% of 27fire Personnel_End-Position 7% of 81 100% of 2 100% of 10fire Conflict_Attack 54% of 81 100% of 3 100% of 19replace Personnel_End-Position 5% of 20 100% of 1 83.3% of 6form Business_Start-Org 12% of 8 100% of 2 100% of 23talk Contact_Meet 59% of 74 100% of 4 100% of 26IncorrectEventTriggershurt Life_Injure 24% of 33 0% of 2 0% of 7execution Life_Die 12% of 8 0% of 4 4% of 24Table 2.
Examples: Percentage of a Word as Event Trigger in Different Data CollectionsAs we can see from the table, the likelihood of acandidate word being an event trigger in the testdocument is closer to its distribution in the collec-tion of related documents than the uniform trainingcorpora.
So if we can determine the sense (eventtype) of a word in the related documents, this willallow us to infer its sense in the test document.
Inthis way related documents can help recover eventmentions missed by within-sentence extraction.For example, in a document about ?the advanceinto Baghdad?
:Example 1:[Test Sentence]Most US army commanders believe it is critical topause the breakneck advance towards Baghdad to se-cure the supply lines and make sure weapons are oper-able and troops resupplied?.
[Sentences from Related Documents]British and US forces report gains in the advance onBaghdad and take control of Umm Qasr, despite afierce sandstorm which slows another flank.
?The baseline event tagger is not able to detect?advance?
as a ?Movement_Transport?
event trig-ger because there is no pattern ?advance towards[Place]?
in the ACE training corpora (?advance?by itself is too ambiguous).
The training data,however, does include the pattern ?advance on[Place]?, which allows the instance of ?advance?
inthe related documents to be successfully identifiedwith high confidence by pattern matching as anevent.
This provides us much stronger ?feedback?confidence in tagging ?advance?
in the test sen-tence as a correct trigger.On the other hand, if a word is not tagged as anevent trigger in most related documents, then it?sless likely to be correct in the test sentence despiteits high local confidence.
For example, in a docu-ment about ?assessment of Russian president Pu-tin?
:Example 2:[Test Sentence]But few at the Kremlin forum suggested that Putin'sown standing among voters will be hurt by Russia'sapparent diplomacy failures.
[Sentences from Related Documents]Putin boosted ties with the United States by throwinghis support behind its war on terrorism after the Sept.11 attacks, but the Iraq war has hurt the relationship.
?The word ?hurt?
in the test sentence is mistaken-ly identified as a ?Life_Injure?
trigger with highlocal confidence (because the within-sentence ex-tractor misanalyzes ?voters?
as the object of ?hurt?and so matches the pattern ?
[Person] be hurt?
).Based on the fact that many other instances of?hurt?
are not ?Life_Injure?
triggers in the relateddocuments, we can successfully remove this wrongevent mention in the test document.3.2 One Argument Role Per ClusterInspired by the observation about trigger distribu-tion, we propose a similar hypothesis ?
one argu-ment role per cluster for event arguments.
In otherwords, each entity plays the same argument role, orno role, for events with the same type in a collec-tion of related documents.
For example,256Example 3:[Test Sentence]Vivendi earlier this week confirmed months of pressspeculation that it planned to shed its entertainmentassets by the end of the year.
[Sentences from Related Documents]Vivendi has been trying to sell assets to pay off hugedebt, estimated at the end of last month at more than$13 billion.Under the reported plans, Blackstone Group wouldbuy Vivendi's theme park division, including UniversalStudios Hollywood, Universal Orlando in Florida...?The above test sentence doesn?t include an ex-plicit trigger word to indicate ?Vivendi?
as a ?sel-ler?
of a ?Transaction_Transfer-Ownership?
eventmention, but ?Vivendi?
is correctly identified as?seller?
in many other related sentences (by match-ing patterns ?
[Seller] sell?
and ?buy [Seller]?s?
).So we can incorporate such additional informationto enhance the confidence of ?Vivendi?
as a ?sel-ler?
in the test sentence.On the other hand, we can remove spurious ar-guments with low cross-document frequency andconfidence.
In the following example,Example 4:[Test Sentence]The Davao Medical Center, a regional governmenthospital, recorded 19 deaths with 50 wounded.
?the Davao Medical Center?
is mistakenlytagged as ?Place?
for a ?Life_Die?
event mention.But the same annotation for this mention doesn?tappear again in the related documents, so we candetermine it?s a spurious argument.4 System Approach OverviewBased on the above motivations we propose to in-corporate global evidence from a cluster of relateddocuments to refine local decisions.
This sectiongives more details about the baseline within-sentence event tagger, and the information retrievalsystem we use to obtain related documents.
In thenext section we shall focus on describing the infe-rence procedure.4.1 System PipelineFigure 1 depicts the general procedure of our ap-proach.
EMSet represents a set of event mentionswhich is gradually updated.Figure 1.
Cross-doc Inference for Event Extraction4.2 Within-Sentence Event ExtractionFor each event mention in a test document t , thebaseline Maximum Entropy based classifiers pro-duce three types of confidence values:?
LConf(trigger,etype): The probability of astring trigger indicating an event mention withtype etype; if the event mention is produced bypattern matching then assign confidence 1.?
LConf(arg, etype): The probability that a men-tion arg is an argument of some particularevent type etype.?
LConf(arg, etype, role): If arg is an argumentwith event type etype, the probability of arghaving some particular role.We apply within-sentence event extraction to getan initial set of event mentions 0tEMSet , and con-duct cross-sentence inference (details will be pre-sented in section 5) to get an updated set of eventmentions 1tEMSet .4.3 Information RetrievalWe then use the INDRI retrieval system (Strohmanet al, 2005) to obtain the top N (N=25 in this pa-Test docWithin-sentEvent ExtractionQueryConstructionCross-sentInferenceQueryUnlabeledCorporaInformationRetrievalRelateddocsWithin-sentEvent ExtractionCross-sentInference1rEMSetCross-docInference0tEMSet0rEMSet1tEMSet2tEMSet257per3) related documents.
We construct an INDRIquery from the triggers and arguments, eachweighted by local confidence and frequency in thetest document.
For each argument we also add oth-er names coreferential with or bearing some ACErelation to the argument.For each related document r returned by INDRI,we repeat the within-sentence event extraction andcross-sentence inference procedure, and get an ex-panded event mention set 1t rEMSet + .
Then we applycross-document inference to 1t rEMSet +  and get thefinal event mention output 2tEMSet .5 Global InferenceThe central idea of inference is to obtain docu-ment-wide and cluster-wide statistics about thefrequency with which triggers and arguments areassociated with particular types of events, and thenuse this information to correct event and argumentidentification and classification.For a set of event mentions we tabulate the fol-lowing document-wide and cluster-wide confi-dence-weighted frequencies:?
for each trigger string, the frequency withwhich it appears as the trigger of an event of aparticular type;?
for each event argument string and the namescoreferential with or related to the argument,the frequency of the event type;?
for each event argument string and the namescoreferential with or related to the argument,the frequency of the event type and role.Besides these frequencies, we also define thefollowing margin metric to compute the confi-dence of the best (most frequent) event type or role:Margin =(WeightedFrequency (most frequent value)?
WeightedFrequency (second most freq value))/WeightedFrequency (second most freq value)A large margin indicates greater confidence inthe most frequent value.
We summarize the fre-quency and confidence metrics in Table 3.Based on these confidence metrics, we designedthe inference rules in Table 4.
These rules are ap-plied in the order (1) to (9) based on the principleof improving ?local?
information before global3 We tested different N ?
[10, 75] on dev set; and N=25achieved best gains.propagation.
Although the rules may seem com-plex, they basically serve two functions:?
to remove triggers and arguments with low(local or cluster-wide) confidence;?
to adjust trigger and argument identificationand classification to achieve (document-wideor cluster-wide) consistency.6 Experimental Results and AnalysisIn this section we present the results of applyingthis inference method to improve ACE event ex-traction.6.1 DataWe used 10 newswire texts from ACE 2005 train-ing corpora (from March to May of 2003) as ourdevelopment set, and then conduct blind test on aseparate set of 40 ACE 2005 newswire texts.
Foreach test text we retrieved 25 related texts fromEnglish TDT5 corpus which in total consists of278,108 texts (from April to September of 2003).6.2 Confidence Metric ThresholdingWe select the thresholds (?k with k=1~13) for vari-ous confidence metrics by optimizing the F-measure score of each rule on the development set,as shown in Figure 2 and 3 as follows.Each curve in Figure 2 and 3 shows the effect onprecision and recall of varying the threshold for anindividual rule.Figure 2.
Trigger Labeling Performance withConfidence Thresholding on Dev Set258Figure 3.
Argument Labeling Performance withConfidence Thresholding on Dev SetThe labeled point on each curve shows the bestF-measure that can be obtained on the develop-ment set by adjusting the threshold for that rule.The gain obtained by applying successive rules canbe seen in the progression of successive points to-wards higher recall and, for argument labeling,precision4.6.3 Overall PerformanceTable 5 shows the overall Precision (P), Recall (R)and F-Measure (F) scores for the blind test set.
Inaddition, we also measured the performance of twohuman annotators who prepared the ACE 2005training data on 28 newswire texts (a subset of theblind test set).
The final key was produced by re-view and adjudication of the two annotations.Both cross-sentence and cross-document infe-rences provided significant improvement over thebaseline with local confidence thresholds con-trolled.We conducted the Wilcoxon Matched-PairsSigned-Ranks Test on a document basis.
The re-sults show that the improvement using cross-sentence inference is significant at a 99.9% confi-dence level for both trigger and argument labeling;adding cross-document inference is significant at a99.9% confidence level for trigger labeling and93.4% confidence level for argument labeling.4 We didn?t show the classification adjusting rules (2), (6) and(8) here because of their relatively small impact on dev set.6.4 DiscussionFrom table 5 we can see that for trigger labelingour approach dramatically enhanced recall (22.9%improvement) with some loss (7.4%) in precision.This precision loss was much larger than that forthe development set (0.3%).
This indicates that thetrigger propagation thresholds optimized on thedevelopment set were too low for the blind test setand thus more spurious triggers got propagated.The improved trigger labeling is better than onehuman annotator and only 4.7% worse than anoth-er.For argument labeling we can see that cross-sentence inference improved both identification(3.7% higher F-Measure) and classification (6.1%higher accuracy); and cross-document inferencemainly provided further gains (1.9%) in classifica-tion.
This shows that identification consistencymay be achieved within a narrower context whilethe classification task favors more global back-ground knowledge in order to solve some difficultcases.
This matches the situation of human annota-tion as well: we may decide whether a mention isinvolved in some particular event or not by readingand analyzing the target sentence itself; but in or-der to decide the argument?s role we may need tofrequently refer to wider discourse in order to inferand confirm our decision.
In fact sometimes it re-quires us to check more similar web pages or evenwikipedia databases.
This was exactly the intuitionof our approach.
We should also note that humanannotators label arguments based on perfect entitymentions, but our system used the output from theIE system.
So the gap was also partially due toworse entity detection.Error analysis on the inference procedure showsthat the propagation rules (3), (4), (7) and (9) pro-duced a few extra false alarms.
For trigger labe-ling, most of these errors appear for support verbssuch as ?take?
and ?get?
which can only representan event mention together with other verbs ornouns.
Some other errors happen on nouns andadjectives.
These are difficult tasks even for humanannotators.
As shown in table 5 the inter-annotatoragreement on trigger identification is only about40%.
Besides some obvious overlooked cases (it?sprobably difficult for a human to remember 33 dif-ferent event types during annotation), most diffi-culties were caused by judging generic verbs,nouns and adjectives.259PerformanceSystem/HumanTriggerIdentification+ClassificationArgumentIdentificationArgumentClassificationAccuracyArgumentIdentification+ClassificationP R F P R F P R FWithin-Sentence IE withRule (1) (Baseline) 67.6 53.5 59.7 47.8 38.3 42.5 86.0 41.2 32.9 36.6Cross-sentence Inference 64.3 59.4 61.8 54.6 38.5 45.1 90.2 49.2 34.7 40.7Cross-sentence+Cross-doc Inference 60.2 76.4 67.3 55.7 39.5 46.2 92.1 51.3 36.4 42.6Human Annotator1 59.2 59.4 59.3 60.0 69.4 64.4 85.8 51.6 59.5 55.3Human Annotator2 69.2 75.0 72.0 62.7 85.4 72.3 86.3 54.1 73.7 62.4Inter-Annotator Agreement 41.9 38.8 40.3 55.2 46.7 50.6 91.7 50.6 42.9 46.4Table 5.
Overall Performance on Blind Test Set (%)In fact, compared to a statistical tagger trained onthe corpus after expert adjudication, a human an-notator tends to make more mistakes in triggerclassification.
For example it?s hard to decidewhether ?named?
represents a ?Person-nel_Nominate?
or ?Personnel_Start-Position?event mention; ?hacked to death?
represents a?Life_Die?
or ?Conflict_Attack?
event mentionwithout following more specific annotation guide-lines.7 Related WorkThe trigger labeling task described in this paper isin part a task of word sense disambiguation(WSD), so we have used the idea of sense consis-tency introduced in (Yarowsky, 1995), extendingit to operate across related documents.Almost all the current event extraction systemsfocus on processing single documents and, exceptfor coreference resolution, operate a sentence at atime (Grishman et al, 2005; Ahn, 2006; Hardy etal., 2006).We share the view of using global inference toimprove event extraction with some recent re-search.
Yangarber et al (Yangarber and Jokipii,2005; Yangarber, 2006; Yangarber et al, 2007)applied cross-document inference to correct localextraction results for disease name, location andstart/end time.
Mann (2007) encoded specific infe-rence rules to improve extraction of CEO (name,start year, end year) in the MUC managementsuccession task.
In addition, Patwardhan and Ri-loff (2007) also demonstrated that selectively ap-plying event patterns to relevant regions canimprove MUC event extraction.
We expand theidea to more general event types and use informa-tion retrieval techniques to obtain wider back-ground knowledge from related documents.8 Conclusion and Future WorkOne of the initial goals for IE was to create a da-tabase of relations and events from the entire inputcorpus, and allow further logical reasoning on thedatabase.
The artificial constraint that extractionshould be done independently for each documentwas introduced in part to simplify the task and itsevaluation.
In this paper we propose a new ap-proach to break down the document boundariesfor event extraction.
We gather together event ex-traction results from a set of related documents,and then apply inference and constraints to en-hance IE performance.In the short term, the approach provides a plat-form for many byproducts.
For example, we cannaturally get an event-driven summary for the col-lection of related documents; the sentences includ-ing high-confidence events can be used asadditional training data to bootstrap the event tag-ger; from related events in different timeframeswe can derive entailment rules; the refined consis-tent events can serve better for other NLP taskssuch as template based question-answering.
Theaggregation approach described here can be easilyextended to improve relation detection and corefe-rence resolution (two argument mentions referringto the same role of related events are likely tocorefer).
Ultimately we would like to extend thesystem to perform essential, although probablylightweight, event prediction.260XSent-Trigger-Freq(trigger, etype) The weighted frequency of string trigger appearing as the trigger of an event of type etype across all sentences within a documentXDoc-Trigger-Freq (trigger, etype) The weighted frequency of string trigger appearing as the trigger of an event of type etype across all documents in a clusterXDoc-Trigger-BestFreq (trigger) Maximum over all etypes of XDoc-Trigger-Freq (trigger, etype)XDoc-Arg-Freq(arg, etype) The weighted frequency of arg appearing as an argument of an event of type etype across all documents in a clusterXDoc-Role-Freq(arg, etype, role)  The weighted frequency of arg appearing as an argument of an event of type etype with role role across all documents in a clusterXDoc-Role-BestFreq(arg)  Maximum over all etypes and roles of XDoc-Role-Freq(arg, etype, role)XSent-Trigger-Margin(trigger) The margin value of trigger in XSent-Trigger-FreqXDoc-Trigger-Margin(trigger) The margin value of trigger in XDoc-Trigger-FreqXDoc-Role-Margin(arg) The margin value of arg in XDoc-Role-FreqTable 3.
Global Frequency and Confidence MetricsRule (1): Remove Triggers and Arguments with Low Local ConfidenceIf LConf(trigger, etype) < ?1, then delete the whole event mention EM;If LConf(arg, etype) < ?2 or LConf(arg, etype, role) < ?3, then delete arg.Rule (2): Adjust Trigger Classification to Achieve Document-wide ConsistencyIf XSent-Trigger-Margin(trigger) >?4, then propagate the most frequent etype to all event mentions with  trigger inthe document; and correct roles for corresponding arguments.Rule (3): Adjust Trigger Identification to Achieve Document-wide ConsistencyIf LConf(trigger, etype) > ?5, then propagate etype to all unlabeled strings trigger in the document.Rule (4): Adjust Argument Identification to Achieve Document-wide ConsistencyIf LConf(arg, etype) > ?6, then in the document, for each sentence containing an event mention EM with etype, addany unlabeled mention in that sentence with the same head as arg as an argument of EM with role.Rule (5): Remove Triggers and Arguments with Low Cluster-wide ConfidenceIf XDoc-Trigger-Freq (trigger, etype) < ?7, then delete EM;If XDoc-Arg-Freq(arg, etype) < ?8 or XDoc-Role-Freq(arg, etype, role) < ?9, then delete arg.Rule (6): Adjust Trigger Classification to Achieve Cluster-wide ConsistencyIf XDoc-Trigger-Margin(trigger) >?10, then propagate most frequent etype to all event mentions with trigger in thecluster; and correct roles for corresponding arguments.Rule (7): Adjust Trigger Identification to Achieve Cluster-wide ConsistencyIf XDoc-Trigger-BestFreq (trigger) >?11, then propagate etype to all unlabeled strings trigger in the cluster, overridethe results of Rule (3) if conflict.Rule (8): Adjust Argument Classification to Achieve Cluster-wide ConsistencyIf XDoc-Role-Margin(arg) >?12, then propagate the most frequent etype and role to all arguments with the samehead as arg in the entire cluster.Rule (9): Adjust Argument Identification to Achieve Cluster-wide ConsistencyIf XDoc-Role-BestFreq(arg) > ?13, then in the cluster, for each sentence containing an event mention EM with etype,add any unlabeled mention in that sentence with the same head as arg as an argument of EM with role.Table 4.
Probabilistic Inference RuleAcknowledgmentsThis material is based upon work supported by theDefense Advanced Research Projects Agency un-der Contract No.
HR0011-06-C-0023, and the Na-tional Science Foundation under Grant IIS-00325657.
Any opinions, findings and conclusionsexpressed in this material are those of the authorsand do not necessarily reflect the views of the U. S.Government.261ReferencesDavid Ahn.
2006.
The stages of event extraction.
Proc.COLING/ACL 2006 Workshop on Annotating andReasoning about Time and Events.
Sydney, Aus-tralia.Ralph Grishman, David Westbrook and Adam Meyers.2005.
NYU?s English ACE 2005 System Descrip-tion.
Proc.
ACE 2005 Evaluation Workshop.
Wash-ington, US.Hilda Hardy, Vika Kanchakouskaya and Tomek Strzal-kowski.
2006.
Automatic Event Classification Us-ing Surface Text Features.
Proc.
AAAI06 Workshopon Event Extraction and Synthesis.
Boston, Massa-chusetts.
US.Gideon Mann.
2007.
Multi-document Relationship Fu-sion via Constraints on Probabilistic Databases.Proc.
HLT/NAACL 2007.
Rochester, NY, US.Siddharth Patwardhan and Ellen Riloff.
2007.
EffectiveInformation Extraction with Semantic Affinity Pat-terns and Relevant Regions.
Proc.
EMNLP 2007.Prague, Czech Republic.Trevor Strohman, Donald Metzler, Howard Turtle andW.
Bruce Croft.
2005.
Indri: A Language-modelbased Search Engine for Complex Queries (ex-tended version).
Technical Report IR-407, CIIR,Umass Amherst, US.Roman Yangarber, Clive Best, Peter von Etter, FlavioFuart, David Horby and Ralf Steinberger.
2007.Combining Information about Epidemic Threatsfrom Multiple Sources.
Proc.
RANLP 2007 work-shop on Multi-source, Multilingual Information Ex-traction and Summarization.
Borovets, Bulgaria.Roman Yangarber.
2006.
Verification of Facts acrossDocument Boundaries.
Proc.
International Work-shop on Intelligent Information Access.
Helsinki,Finland.Roman Yangarber and Lauri Jokipii.
2005.
Redundan-cy-based Correction of Automatically ExtractedFacts.
Proc.
HLT/EMNLP 2005.
Vancouver, Cana-da.David Yarowsky.
1995.
Unsupervised Word Sense Dis-ambiguation Rivaling Supervised Methods.
Proc.ACL 1995.
Cambridge, MA, US.262
