Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 164?169,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsPre-reordering for machine translation using transition-based walks ondependency parse treesAntonio Valerio Miceli-BaroneDipartimento di InformaticaLargo B. Pontecorvo, 356127 Pisa, Italymiceli@di.unipi.itGiuseppe AttardiDipartimento di InformaticaLargo B. Pontecorvo, 356127 Pisa, Italyattardi@di.unipi.itAbstractWe propose a pre-reordering scheme toimprove the quality of machine translationby permuting the words of a source sen-tence to a target-like order.
This is accom-plished as a transition-based system thatwalks on the dependency parse tree of thesentence and emits words in target-like or-der, driven by a classifier trained on a par-allel corpus.
Our system is capable of gen-erating arbitrary permutations up to flexi-ble constraints determined by the choice ofthe classifier algorithm and input features.1 IntroductionThe dominant paradigm in statistical machinetranslation consists mainly of phrase-based sys-tem such as Moses (Koehn et.al.,2007).
Differ-ent languages, however, often express the sameconcepts in different idiomatic word orders, andwhile phrase-based system can deal to some ex-tent with short-distance word swaps that are cap-tured by short segments, they typically performpoorly on long-distance (more than four or fivewords apart) reordering.
In fact, according to(Birch et.al., 2008), the amount of reordering be-tween two languages is the most predictive featureof phrase-based translation accuracy.A number of approaches to deal with long-distance reordering have been proposed.
Since anextuasive search of the permutation space is un-feasible, these approaches typically constrain thesearch space by leveraging syntactical structure ofnatural languages.In this work we consider approaches which in-volve reordering the words of a source sentencein a target-like order as a preprocessing step, be-fore feeding it to a phrase-based decoder whichhas itself been trained with a reordered trainingset.
These methods also try to leverage syntax,typically by applying hand-coded or automaticallyinduced reordering rules to a constituency or de-pendency parse of the source sentence.
(Gal-ley and Manning, 2008; Xu et.al., 2009; Genzel,2010; Isozaki et.al., 2010) or by treating reorder-ing as a global optimization problem (Tromble andEisner, 2009; Visweswariah et.al., 2011).
In or-der to keep the training and execution processestractable, these methods impose hard constrainson the class of permutations they can generate.We propose a pre-reordering method based ona walk on the dependency parse tree of the sourcesentence driven by a classifier trained on a parallelcorpus.In principle, our system is capable of generat-ing arbitrary permutations of the source sentence.Practical implementations will necessarily limitthe available permutations, but these constraintsare not intrinsic to the model, rather they dependon the specific choice of the classifier algorithm,its hyper-parameters and input features.2 Reordering as a walk on a dependencytree2.1 Dependency parse treesLet a sentence be a list of words s ?
(w1, w2, .
.
.
, wn) and its dependency parse treebe a rooted tree whose nodes are the words of thesentence.
An edge of the tree represents a syntac-tical dependency relation between a head (parent)word and a modifier (child) word.
Typical depen-dency relations include verb-subject, verb-object,noun-adjective, and so on.We assume that in addition to its head hi anddependency relation type di each word is also an-notated with a part-of-speech pi and optionally alemma li and a morphology mi (e.g.
grammaticalcase, gender, number, tense).Some definitions require dependency parsetrees to be projective, meaning that any complete164subtree must correspond to a contiguous span ofwords in the sentence, however, we don?t placesuch a requirement.
In practice, languages with asubstantially strict word ordering like English typ-ically have largely projective dependencies, whilelanguages with a more free word ordering likeCzech can have substantial non-projectivity.2.2 Reordering modelGiven a sentence s ?
S with its dependency parsetree and additional annotations, we incrementallyconstruct a reordered sentence s?
by emitting itswords in a sequence of steps.
We model the re-ordering process as a non-deterministic transitionsystem which traverses the parse tree:Let the state of the system be a tuple x ?
(i, r, a, , .
.
. )
containing at least the index of thecurrent node i (initialized at the root), the list ofemitted nodes r (initialized as empty) and the lasttransition action a (initialized as null).
Additionalinformation can be included in the state x, such asthe list of the last K nodes that have been visited,the last K actions and a visit count for each node.At each step we choose one of the following ac-tions:?
EMIT : emit the current node.
Enabled onlyif the current node hasn?t already been emit-tedi /?
r(i, r, a, , .
.
. )
EMIT?
(i, (r | i) , EMIT, , .
.
.
)?
UP : move to the parent of the current nodehi 6= null, ?j a 6= DOWNj(i, r, a, , .
.
. )
UP?
(hi, r, UP, , .
.
.
)?
DOWNj : move to the child j of the currentnode.
Enabled if the subtree of j (includingj) contains nodes that have not been emittedyet.hj = i, a 6= UP, ?k ?
subtree(i) : k /?
r(i, r, a, , .
.
. )
DOWNj?
(j, r, DOWNj , , .
.
.
)The pre-conditions on the UP and DOWN actionsprevent them from canceling each other, ensuringthat progress is made at each step.
The additionalprecondition on DOWN actions ensures that theprocess always halts at a final state where all thenodes have been emitted.Let T (s) be the set of legal traces of the transi-tion system for sentence s. Each trace ?
?
T (s)defines a permutation s?
of s as the list of emittednodes r of its final state.We define the reordering problem as finding thetrace ??
that maximizes a scoring function ???
?
arg max?
?T (s)?
(s, ?)
(1)Note that since the parse tree is connected, inprinciple any arbitrary permutation can be gen-erated for a suitable choice of ?, though themaximization problem (1) is NP-hard and APX-complete in the general case, by trivial reductionfrom the traveling salesman problem.The intuition behind this model is to leveragethe syntactical information provided by the de-pendency parse tree, as successfully done by (Xuet.al., 2009; Genzel, 2010; Isozaki et.al., 2010)without being strictly constrained by a specifictype reordering rules.2.3 Trace scoresWe wish to design a scoring function ?
that cap-tures good reorderings for machine translation andadmits an efficient optimization scheme.We chose a function that additively decomposesinto local scoring functions, each depending onlyon a single state of the trace and the following tran-sition action?
(s, ?)
?|?
|?1?t=1?
(s, x (?, t) , xa (?, t+ 1))(2)We further restrict our choice to a functionwhich is linear w.r.t.
a set of elementary local fea-ture functions {fk}?
(s, x, a) ?|F |?k=1vkfk (s, x, a) (3)where {vk} ?
R|F | is a vector of parametersderived from a training procedure.While in principle each feature function coulddepend on the whole sentence and the whole se-quence of nodes emitted so far, in practice we re-strict the dependence to a fixed neighborhood ofthe current node and the last few emitted nodes.This reduces the space of possible permutations.2.4 Classifier-driven action selectionEven when the permutation space has been re-stricted by an appropriate choice of the featurefunctions, computing an exact solution of the opti-mization problem (1) remains non-trivial, because165at each step of the reordering generation process,the set of enabled actions depends in general onnodes emitted at any previous step, and this pre-vents us from applying typical dynamic program-ming techniques.
Therefore, we need to apply anheuristic procedure.In our experiments, we apply a simple greedyprocedure: at each step we choose an action ac-cording to the output a two-stage classifier:1.
A three-class one-vs-all logistic classifierchooses an action among EMIT, UP orDOWN based on a vector of features ex-tracted from a fixed neighborhood of the cur-rent node i, the last emitted nodes and addi-tional content of the state.2.
If a DOWN action was chosen, then a one-vs-one voting scheme is used to choosewhich child to descend to: For each pair(j, j?)
: j < j?
of children of i, a binary lo-gistic classifier assigns a vote either to j orj?.
The child that receives most votes is cho-sen.
This is similar to the max-wins approachused in packages such as LIBSVM (Changand Lin, 2011) to construct a M -class clas-sifier from M (M ?
1) /2 binary classifiers,except that we use a single binary classifieracting on a vector of features extracted fromthe pair of children (j, j?)
and the node i,with their respective neighborhoods.We also experimented with different classificationschemes, but we found that this one yields the bestperformance.Note that we are not strictly maximizing aglobal linear scoring function as as defined byequations (2) and (3), although this approach isclosely related to that framework.This approach is related to transition-based de-pendency parsing such as (Nivre and Scholz,2004; Attardi, 2006) or dependency tree revi-sion(Attardi and Ciaramita, 2007).3 Training3.1 Dataset preparationFollowing (Al-Onaizan and Papineni, 2006;Tromble and Eisner, 2009; Visweswariah et.al.,2011), we generate a source-side reference re-ordering of a parallel training corpus.
For eachsentence pair, we generate a bidirectional wordalignment using GIZA++ (Och and Ney, 2000)and the ?grow-diag-final-and?
heuristic imple-mented in Moses (Koehn et.al.,2007), then we as-sign to each source-side word a integer index cor-responding to the position of the leftmost target-side word it is aligned to (attaching unalignedwords to the following aligned word) and finallywe perform a stable sort of source-side words ac-cording to this index.On language pairs where GIZA++ producessubstantially accurate alignments (generally allEuropean languages) this scheme generates atarget-like reference reordering of the corpus.In order to tune the parameters of the down-stream phrase-based translation system and to testthe overall translation accuracy, we need two addi-tional small parallel corpora.
We don?t need a ref-erence reordering for the tuning corpus since it isnot used for training the reordering system, how-ever we generate a reference reordering for the testcorpus in order to evaluate the accuracy of the re-ordering system in isolation.
We obtain an align-ment of this corpus by appending it to the train-ing corpus, and processing it with GIZA++ andthe heuristic described above.3.2 Reference traces generation and classifiertrainingFor each source sentence s in the training setand its reference reordering s?, we generate aminimum-length trace ?
of the reordering transi-tion system, and for each state and action pair in itwe generate the following training examples:?
For the first-stage classifier we generate a sin-gle training examples mapping the local fea-tures to an EMIT, UP or DOWN action label?
For the second-stage classifier, if the action isDOWNj , for each pair of children (k, k?)
:k < k?
of the current node i, we generate apositive example if j = k or a negative ex-ample if j = k?.Both classifiers are trained with the LIBLIN-EAR package (Fan et.al., 2008), using the L2-regularized logistic regression method.
The reg-ularization parameter C is chosen by two-foldcross-validation.
In practice, subsampling of thetraining set might be required in order to keepmemory usage and training time manageable.3.3 Translation system training and testingOnce the classifiers have been trained, we runthe reordering system on the source side of the166whole (non-subsampled) training corpus and thetuning corpus.
For instance, if the parallel cor-pora are German-to-English, after the reorder-ing step we obtain German?-to-English corpora,where German?
is German in an English-likeword order.
These reordered corpora are used totrain a standard phrase-based translation system.Finally, the reordering system is applied to sourceside of the test corpus, which is then translatedwith the downstream phrase-based system and theresulting translation is compared to the referencetranslation in order to obtain an accuracy measure.We also evaluate the ?monolingual?
reordering ac-curacy of upstream reordering system by compar-ing its output on the source side of the test cor-pus to the reference reordering obtained from thealignment.4 ExperimentsWe performed German-to-English and Italian-to-English reordering and translation experiments.4.1 DataThe German-to-English corpus is Europarl v7(Koehn, 2005).
We split it in a 1,881,531 sentencepairs training set, a 2,000 sentence pairs develop-ment set (used for tuning) and a 2,000 sentencepairs test set.
We also used a 3,000 sentence pairs?challenge?
set of newspaper articles provided bythe WMT 2013 translation task organizers.The Italian-to-English corpus has been assem-bled by merging Europarl v7, JRC-ACQUIS v2.2(Steinberger et.al., 2006) and bilingual newspaperarticles crawled from news websites such as Cor-riere.it and Asianews.it.
It consists of a 3,075,777sentence pairs training set, a 3,923 sentence pairsdevelopment set and a 2,000 sentence pairs testset.The source sides of these corpora have beenparsed with Desr (Attardi, 2006).
For both lan-guage pairs, we trained a baseline Moses phrase-based translation system with the default configu-ration (including lexicalized reordering).In order to keep the memory requirements andduration of classifier training manageable, we sub-sampled each training set to 40,000 sentences,while both the baseline and reordered Moses sys-tem are trained on the full training sets.4.2 FeaturesAfter various experiments with feature selection,we settled for the following configuration for bothGerman-to-English and Italian-to-English:?
First stage classifier: current node i state-ful features (emitted?, left/right subtree emit-ted?, visit count), curent node lexical andsyntactical features (surface form wi, lemmali, POS pi, morphology mi, DEPREL di, andpairwise combinations between lemma, POSand DEPREL), last two actions, last two vis-ited nodes POS, DEPREL and visit count,last two emitted nodes POS and DEPREL, bi-gram and syntactical trigram features for thelast two emitted nodes and the current node,all lexical, syntactical and stateful featuresfor the neighborhood of the current node(left, right, parent, parent-left, parent-right,grandparent, left-child, right-child) and pair-wise combination between syntactical fea-tures of these nodes.?
Second stage classifier: stateful features forthe current node i and the the children pair(j, j?
), lexical and syntactical features foreach of the children and pairwise combina-tions of these features, visit count differencesand signed distances between the two chil-dren and the current node, syntactical trigramfeatures between all combinations of the twochildren, the current node, the parent hi andthe two last emitted nodes and the two lastvisited nodes, lexical and syntactical featuresfor the two children left and right neighbors.All features are encoded as binary one-of-n indi-cator functions.4.3 ResultsFor both German-to-English and Italian-to-English experiments, we prepared the data asdescribed above and we trained the classifiers ontheir subsampled training sets.
In order to evaluatethe classifiers accuracy in isolation from the restof the system, we performed two-fold cross vali-dation on the same training sets, which revealedan high accuracy: The first stage classifier obtainsapproximately 92% accuracy on both German andItalian, while the second stage classifier obtainsapproximately 89% accuracy on German and 92%on Italian.167BLEU NISTGerman 57.35 13.2553Italian 68.78 15.3441Table 1: Monolingual reordering scoresBLEU NISTde-en baseline 33.78 7.9664de-en reordered 32.42 7.8202it-en baseline 29.17 7.1352it-en reordered 28.84 7.1443Table 2: Translation scoresWe applied the reordering preprocessing systemto the source side of the corpora and evaluated themonolingual BLEU and NIST score of the test sets(extracted from Europarl) against their referencereordering computed from the alignmentTo evaluate translation performance, we traineda Moses phrase-based system on the reorderedtraining and tuning corpora, and evaluated theBLEU and NIST of the (Europarl) test sets.
Asa baseline, we also trained and evaluated Mosessystem on the original unreordered corpora.We also applied our baseline and reorderedGerman-to-English systems to the WMT2013translation task dataset.5 DiscussionUnfortunately we were generally unable to im-prove the translation scores over the baseline, eventhough our monolingual BLEU for German-to-English reordering is higher than the score re-ported by (Tromble and Eisner, 2009) for a com-parable dataset.Accuracy on the WMT 2013 set is very low.
Weattribute this to the fact that it comes form a differ-ent domain than the training set.Since classifier training set cross-validation ac-curacy is high, we speculate that the main problemlies with the training example generation process:training examples are generated only from opti-mal reordering traces.
This means that once theclassifiers produce an error and the system straysaway from an optimal trace, it may enter in a fea-ture space that is not well-represented in the train-ing set, and thus suffer from unrecoverable per-formance degradation.
Moreover, errors occurringon nodes high in the parse tree may cause incor-rect placement of whole spans of words, yieldinga poor BLEU score (although a cursory exami-nation of the reordered sentences doesn?t revealthis problem to be prevalent).
Both these issuescould be possibly addressed by switching froma classifier-based system to a structured predic-tion system, such as averaged structured percep-tron (Collins, 2002) or MIRA (Crammer, 2003;McDonald et.al., 2005).Another possible cause of error is the purelygreedy action selection policy.
This could be ad-dressed using a search approach such as beamsearch.We reserve to investigate these approaches infuture work.ReferencesRoy Tromble and Jason Eisner.
2009.
Learning linearordering problems for better translation.
In Proceed-ings of the 2009 Conference on Empirical Methodsin Natural Language Processing: Volume 2 - Vol-ume 2 (EMNLP ?09), Vol.
2.
Association for Com-putational Linguistics, Stroudsburg, PA, USA, 1007-1016.G.
Attardi, M. Ciaramita.
2007.
Tree Revision Learn-ing for Dependency Parsing.
In Proc.
of the HumanLanguage Technology Conference 2007.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improveSMT for subject-object-verb languages.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics(NAACL ?09).
Association for Computational Lin-guistics, Stroudsburg, PA, USA, 245-253.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine trans-lation.
In Proceedings of the 23rd InternationalConference on Computational Linguistics (COL-ING ?10).
Association for Computational Linguis-tics, Stroudsburg, PA, USA, 376-384.Yaser Al-Onaizan and Kishore Papineni.
2006.
Dis-tortion models for statistical machine translation.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics (ACL-44).
Association for ComputationalLinguistics, Stroudsburg, PA, USA, 529-536.Alexandra , Miles Osborne, and Philipp Koehn.
2008.Predicting success in machine translation.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP ?08).
Asso-ciation for Computational Linguistics, Stroudsburg,PA, USA, 745-754.168BLEU BLEU (11b) BLEU-cased BLEU-cased (11b) TERde-en baseline 18.8 18.8 17.8 17.8 0.722de-en reordered 18.1 18.1 17.3 17.3 0.739Table 3: WMT2013 de-en translation scoresPhilipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions(ACL ?07).
Association for Computational Linguis-tics, Stroudsburg, PA, USA, 177-180.Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, andKevin Duh.
2010.
Head finalization: a simple re-ordering rule for SOV languages.
In Proceedingsof the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR (WMT ?10).
Asso-ciation for Computational Linguistics, Stroudsburg,PA, USA, 244-251.Michel Galley and Christopher D. Manning.
2008.A simple and effective hierarchical phrase reorder-ing model.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP ?08).
Association for Computational Lin-guistics, Stroudsburg, PA, USA, 848-856.Karthik Visweswariah, Rajakrishnan Rajkumar, AnkurGandhe, Ananthakrishnan Ramanathan, and JiriNavratil.
2011.
A word reordering model for im-proved machine translation.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP ?11).
Association forComputational Linguistics, Stroudsburg, PA, USA,486-496.Giuseppe Attardi.
2006.
Experiments with a multi-language non-projective dependency parser.
In Pro-ceedings of the Tenth Conference on ComputationalNatural Language Learning (CoNLL-X ?06).
Asso-ciation for Computational Linguistics, Stroudsburg,PA, USA, 166-170.Joakim Nivre and Mario Scholz.
2004.
Determinis-tic dependency parsing of English text.
In Proceed-ings of the 20th international conference on Compu-tational Linguistics (COLING ?04).
Association forComputational Linguistics, Stroudsburg, PA, USA, ,Article 64 .Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of the38th Annual Meeting on Association for Computa-tional Linguistics (ACL ?00).
Association for Com-putational Linguistics, Stroudsburg, PA, USA, 440-447.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTrans.
Intell.
Syst.
Technol.
2, 3, Article 27 (May2011), 27 pages.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A Library for Large Linear Classification.
J. Mach.Learn.
Res.
9 (June 2008), 1871-1874.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
MT Summit 2005.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Toma Erjavec, Dan Tufis?.
2006.
TheJRC-Acquis: A multilingual aligned parallel corpuswith 20+ languages.
In Proceedings of the 5th In-ternational Conference on Language Resources andEvaluation (LREC?2006).Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing - Volume 10 (EMNLP?02), Vol.
10.
Association for Computational Lin-guistics, Stroudsburg, PA, USA, 1-8.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics (ACL ?05).
Association for ComputationalLinguistics, Stroudsburg, PA, USA, 91-98.Koby Crammer and Yoram Singer.
2003.
Ultraconser-vative online algorithms for multiclass problems.
J.Mach.
Learn.
Res.
3 (March 2003), 951-991.169
