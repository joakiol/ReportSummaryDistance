Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1654?1664,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsRefining Word Segmentation Using a Manually Aligned Corpusfor Statistical Machine TranslationXiaolin Wang Masao Utiyama Andrew Finch Eiichiro SumitaNational Institute of Information and Communications Technology{xiaolin.wang,mutiyama,andrew.finch,eiichiro.sumita}@nict.go.jpAbstractLanguages that have no explicit word de-limiters often have to be segmented for sta-tistical machine translation (SMT).
This iscommonly performed by automated seg-menters trained on manually annotatedcorpora.
However, the word segmentation(WS) schemes of these annotated corporaare handcrafted for general usage, andmay not be suitable for SMT.
An analysiswas performed to test this hypothesis us-ing a manually annotated word alignment(WA) corpus for Chinese-English SMT.An analysis revealed that 74.60% of thesentences in the WA corpus if segmentedusing an automated segmenter trained onthe Penn Chinese Treebank (CTB) willcontain conflicts with the gold WA an-notations.
We formulated an approachbased on word splitting with reference tothe annotated WA to alleviate these con-flicts.
Experimental results show that therefined WS reduced word alignment errorrate by 6.82% and achieved the highestBLEU improvement (0.63 on average) onthe Chinese-English open machine trans-lation (OpenMT) corpora compared to re-lated work.1 IntroductionWord segmentation is a prerequisite for manynatural language processing (NLP) applicationson those languages that have no explicit spacebetween words, such as Arabic, Chinese andJapanese.
As the first processing step, WS affectsall successive steps, thus it has a large potentialimpact on the final performance.
For SMT, theunsupervised WA, building translation models andreordering models, and decoding are all based onsegmented words.Automated word segmenters built throughsupervised-learning methods, after decades of in-tensive research, have emerged as effective so-lutions to WS tasks and become widely used inmany NLP applications.
For example, the Stan-ford word segmenter (Xue et al., 2002)1 which isbased on conditional random field (CRF) is em-ployed to prepare the official corpus for NTCIR-9 Chinese-English patent translation task (Goto etal., 2011).However, one problem with applying thesesupervised-learning word segmenters to SMT isthat the WS scheme of annotating the training cor-pus may not be optimal for SMT.
(Chang et al.,2008) noticed that the words in CTB are often toolong for SMT.
For example, a full Chinese per-sonal name which consists of a family name and agiven name is always taken as a single word, butits counterpart in English is usually two words.Manually WA corpora are precious resourcesfor SMT research, but they used to be only avail-able in small volumes due to the production cost.For example, (Och and Ney, 2000) initially an-notated 447 English-French sentence pairs, whichlater became the test data set in ACL 2003 sharedtask on word alignment (Mihalcea and Pedersen,2003), and was used frequently thereafter (Lianget al., 2006; DeNero and Klein, 2007; Haghighi etal., 2009)For Chinese and English, the shortage of man-ually WA corpora has recently been relievedby the linguistic data consortium (LDC) 2GALE Chinese-English word alignment and tag-ging training corpus (the GALE WA corpus)3.The corpus is considerably large, containing 4,735documents, 18,507 sentence pairs, 620,189 Chi-nese tokens, 518,137 English words, and 421,7631http://nlp.stanford.edu/software/segmenter.shtml2http://catalog.ldc.upenn.edu3Catalog numbers: LDC2012T16, LDC2012T20,LDC2012T24 and LDC2013T05.1654alignment annotations.
The corpus carries no Chi-nese WS annotation, and the WA annotation wasperformed between Chinese characters and En-glish words.
The alignment identifies minimumtranslation units and relations 4, referred as atomicblocks and atomic edges, respectively, in this pa-per.
Figure 1 shows an example that contains sixatomic edges.Visual inspection of the segmentation of an au-tomatic segmenter with reference to a WA cor-pus revealed a number of inconsistencies.
For ex-ample, consider the word ?bao fa?
in Figure 1.Empirically we observed that this word is seg-mented as a single token by an automatic seg-menter trained on the CTB, however, this segmen-tation differs with the alignment in the WA cor-pus, since its two components are aligned to twodifferent English words.
Our hypothesis was thatthe removal of these inconsistencies would benefitmachine translation performance (this is explainedfurther in Section 2.3), and we explored this ideain this work.This paper focuses on optimizing Chinese WSfor Chinese-English SMT, but both the researchmethod and the proposed solution are language-independent.
They can be applied to other lan-guage pairs.The major contributions of this paper include,?
analyze the CTB WS scheme for Chinese-English SMT;?
propose a lexical word splitter to refine theWS;?
achieve a BLEU improvement over a baselineStanford word segmenter, and a state-of-the-art extension, on Chinese-English OpenMTcorpora.The rest of this paper is organized as follows:first, Section 2 analyzes WS using a WA corpus;next, Section 3 proposes a lexical word splitterto refine WS; then, Section 4 evaluates the pro-posed method on end-to-end SMT as well as wordsegmentation and alignment; after that, Section 5compares this work to related work; finally, Sec-tion 6 concludes this paper.4Guidelines for Chinese-English Word Align-ment(Version 4.0)2 Analysis of a General-purposeAutomatic Word SegmenterThis section first briefly describes the GALE WAcorpus, then presents an analysis of the WS arisingfrom a CTB-standard word segmenter with refer-ence to the segmentation of the atomic blocks inthe GALE WA corpus, finally the impact of thefindings on SMT is discussed.2.1 GALE WA corpusThe GALE WA corpus was developed by theLDC, and was used as training data in the DARPAGALE global autonomous language exploitationprogram 5.
The corpus incorporates linguisticknowledge into word aligned text to help improveautomatic WA and translation quality.
It em-ploys two annotation schemes: alignment and tag-ging (Li et al., 2010).
Alignment identifies min-imum translation units and translation relations;tagging adds contextual, syntactic and language-specific features to the alignment annotation.
Forexample, the sample shown in Figure 1 carries tagson both alignment edges and tokens.The GALE WA corpus contains 18,057 man-ually word aligned Chinese and English parallelsentences which are extracted from newswire andweb blogs.
Table 1 presents the statistics on thecorpus.
One third of the sentences are approxi-mately newswire text, and the remainder consistsof web blogs.2.2 Analysis of WSIn order to produce a Chinese word segmenta-tion consistent with the CTB standard we used theStanford Chinese word segmenter with a modeltrained on the CTB corpus.
We will refer to thisas the ?CTB segmenter?
in the rest of this paper.The Chinese sentences in the GALE WA cor-pus were first segmented by the CTB segmenter,and the predicted words were compared againstthe atomic blocks with respect to the granularity ofsegmentation.
The analysis falls into the followingthree categories, two of which may be potentiallyharmful to SMT:?
Fully consistent: the word locates within theblock of one atomic alignment edge.
For ex-ample, in Figure 2(a), the Chinese text has5https://catalog.ldc.upenn.edu/LDC2012T161655 	                       	  Figure 1: Example from the GALE WA corpus.
Each line arrow represents an atomic edge, and each boxrepresents an atomic block.
SEM (semantic), GIS (grammatically inferred semantic) and FUN (function)are tags of edges.
INC (not translated), TOI (to-infinitive) and DET (determiner) are tags of tokens.Genre # Files # Sentences?
# CN tokens # EN tokens # Alignment edgesNewswire 2,175 6,218 246,371 205,281 164,033Web blog 2,560 11,839 373,818 312,856 257,730Total 4,735 18,057 620,189 518,137 421,763Table 1: GALE WA corpus.
?
Sentences rejected by the annotators are excluded.four atomic blocks; the CTB segmenter pro-duces five words which all locate within theblocks, so they are all small enough.?
Alignment inconsistent: the word aligns tomore than one atomic block, but the targetexpression is contiguous, allowing for cor-rect phrase pair extraction (Zens et al., 2002).For example, in Figure 2(b), the characters inthe word ?shuang fang?, which is producedby the CTB segmenter, contains two atomicblocks, but the span of the target ?to bothside?
is continuous, therefore the phrase pair?shuang fang ||| to both sides?
can be ex-tracted.?
Alignment inconsistent and extraction hin-dered: the word aligned to more than oneatomic block, and the target expression is notcontiguous, which hinders correct phrase pairextractions.
For example, in Figure 2(c), theword ?zeng chan?
has to be split in order tomatch the target language.Table 2 shows the statistics of the three cat-egories of CTB WS on the GALE WA corpus.90.74% of the words are fully consistent, while theremaining 9.26% of the words have inconsistentalignments.
74.60% of the sentences contain thisproblem.
The category with inconsistent align-ment and extraction hindered only accounts for0.46% of the words, affecting 9.06% of the sen-tences.2.3 Impact of WS on SMTThe word alignment has a direct impact on the na-ture of both the translation model, and lexical re-ordering model in a phrase-base SMT system.
Thewords in last two categories are all longer than anatomic block, which might lead to problems in theword alignment in two ways:?
First, longer words tend to be more sparse inthe training corpus, thus the estimated distri-bution of their target phrases are less accu-rate.?
Second, the alignment from them to targetsides are one-to-many, which is much morecomplicated and requires fertilized alignmentmodels such as IBM model 4 ?
6 (Och andNey, 2000).The words in the category of ?fully consistent?can be aligned using simple models, because thealignment from them to the target side are one-to-one or many-to-one, and simple alignment modelssuch as IBM model 1, IBM model 2 and HMMmodel are sufficient (Och and Ney, 2000).3 Refining the Word SegmentationIn the last subsection, it was shown that 74.60% ofparallel sentences were affected by issues relatedto under-segmentation of the corpus.
Our hypoth-esis is that if these words are split into pieces thatmatch English words, the accuracy of the unsuper-vised WA as well as the translation quality will beimproved.
To achieve this, we adopt a splitting1656					      (a)	      		    (b) 	 	    	 	 	  (c)Figure 2: Examples of automated WS on manually WA corpus: (a) Fully consistent; (b) Alignmentinconsistent; (c) Alignment inconsistent and extraction hindered.
The Chinese words separated by whitespace are the output of the CTB segmenter.
Arrows represent the alignment of atomic blocks.
Note that?shuang fang?
and ?zeng chan?
are words produced by the CTB segmenter, but consist of two atomicblocks.Category Count Word Ratio Sentence RatioFully consistent 355,702 90.74% 25.40%?Alignment inconsistent 34,464 8.81% 65.54%Alignment inconsistent & extraction hindered 1,830 0.46% 9.06%Sum of conflict ?
36,294 9.26% 74.60%Table 2: CTB WS on GALE WA corpus: ?
All words are fully consistent; ?
Alignment inconsistent plusalignment inconsistent & extraction hinderedstrategy, based on a supervised learning approach,to re-segment the corpus.
This subsection first for-malizes the task, and then presents the approach.3.1 Word splitting taskThe word splitting task is formalized as a sequencelabeling task as follows: each word (representedby a sequence of characters x = x1.
.
.
xTwhereT is the length of sample) produced by the CTBsegmenter is a sample, and a corresponding se-quence of binary boundary labels y = y1.
.
.
yTis the learning target,yt=??
?1 if there is a split pointbetween ctand ct?1;0 otherwise.
(1)The sequence of boundary labels is derivedfrom the gold WA annotation as follows: for asequence of two atomic blocks, where the firstcharacter of the second block is xt, then the la-  Figure 3: Samples of word splitting taskbel yt= 1.
Figure 3 presents several samples ex-tracted from the examples in Figure 2.Each word sample may have no split point, onesplit point or multiple split points, depending onthe gold WA annotation.
Table 3 shows the statis-tics of the word splitting data set which is builtfrom the GALE manual WA corpus and the CTBsegmenter?s output, where 2000 randomly sam-pled sentences are taken as a held-out test set.1657Set # Sentences # Samples # Split points # Split points per sampleTrain.
16,057 348,086 32,337 0.0929Test 2,000 43,910 3,929 0.0895Table 3: Data set for learning the word splitting3.2 CRF approachThis paper employs a condition random field(CRF) to solve this sequence labeling task (Laf-ferty et al., 2001).
A linear-chain CRF defines theconditional probability of y given x as,P?
(y|x) =1Zx(T?t=1?k?kfk(yt?1, yt,x, t)),(2)where ?
= {?1, .
.
.}
are parameters, Zxis a per-input normalization that makes the probability ofall state sequences sum to one; fk(yt?1, yt,x, t) isa feature function which is often a binary-valuedsparse feature.
The training of CRF model is tomaximize the likelihood of training data togetherwith a regularization penalty to avoid over-fittingas (Peng et al., 2004; Peng and McCallum, 2006),?
?= argmax?(?ilogP?
(yi|xi) ?
?k?2k2?2k),(3)where (x,y) are training samples; the hyperparam-eter ?kcan be understood as the variance of theprior distribution of ?k.
When predicting the la-bels of test samples, the CRF decoder searches forthe optimal label sequence y?
that maximizes theconditional probability,y?= argmaxyP?(y|x).
(4)In (Chang et al., 2008) a method is proposed toselect an appropriate level of segmentation gran-ularity (in practical terms, to encourage smallersegments).
We call their method ?length tuner?.The following artificial feature is introduced intothe learned CRF model:f0(x, yt?1, yt, 1) ={1 if yt= +10 otherwise(5)The weight ?0of this feature is set by hand tobias the output of CRF model.
By way of expla-nation, a very large positive ?0will cause everycharacter to be segmented, or conversely a verylarge negative ?0will inhibit the output of segmen-tation boundaries.
In their experiments, ?0= 2was used to force a CRF segmenter to adopt an in-termediate granularity between character and theCTB WS scheme.
Compared to the length tuner,our proposed method exploits lexical knowledgeabout word splitting, and we will therefore refer toit as the ?lexical word splitter?
or ?lexical splitter?for short.3.3 Feature SetThe features fk(yt?1, yt,x, t) we used include theWS features from the Chinese Stanford word seg-menter and a set of extended features describedbelow.
The WS features are included because thetarget split points may share some common char-acteristics with the boundaries in the CTB WSscheme.The extended features consists of four types ?named entities, word frequency, word length andcharacter-level unsupervised WA.
For each type ofthe feature, the value and value concatenated withprevious or current character are taken as sparsefeatures (see Table 4 for details).
The real val-ues of word frequency, word length and character-level unsupervised WA are converted into sparsefeatures due to the routine of CRF model.The character-level unsupervised alignmentfeature is inspired by the related works of unsu-pervised bilingual WS (Xu et al., 2008; Chung andGildea, 2009; Nguyen et al., 2010; Michael et al.,2011).
The idea is that the character-level WA canapproximately capture the counterpart English ex-pression of each Chinese token, and source tokensaligned to different target expressions should besplit into different words (see Figure 4 for an illus-tration).The values of the character-level alignment fea-tures are obtained through building a dictionary.First, unsupervised WA is performed on the SMTtraining corpus where the Chinese sentences aretreated as sequences of characters; then, the Chi-nese sentences are segmented by CTB segmenterand a dictionary of segmented words are built; fi-nally, for each word in the dictionary, the relativefrequency of being split at a certain position is cal-1658Feature Definition ExampleNE NE tag of current word Geography:NENE-C?1NE concatenated with previous character Geo.-ding:NE-C?1NE-C0NE concatenated with current character Geo.-mei:NE-C0Frequency Nearest integer of negative logarithm of word frequency 5?
:FreqFreq.-C?1Frequency concatenated with previous character 5-ding:Freq-C?1Freq.-C0Frequency concatenated with current character 5-mei:Freq-C0Length Length of current word (1,2,3,4,5,6,7 or >7) 4:LenLen.-Position Length concatenated with the position 4-2:Len-PosLen.-C?1Length concatenated with previous character 4-ding:Len-C?1Len.-C0Length concatenated with current character 4-mei:Len-C0Char.
Align.
Five-level relative frequency of being split 0.4?:CAC.A.-C?1C.A.
concatenated with previous character 0.4-ding:CA-C?1C.A.-C0C.A.
concatenated with current character 0.4-mei:CA-C0Table 4: Extended features used in the CRF model for word splitting.
The example shows the featuresused in the decision whether to split the Chinese word ?la ding mei zhou?
(Latin America, the firstfour Chinese characters in Figure 4) after the second Chinese character.
?
Round(-log10(0.00019)); ?Round(0.43 ?
5 ) / 5	       	     Figure 4: Illustration of character-level unsuper-vised alignment features.
The dotted lines areword boundaries suggested by the alignment.culated as,fCA(w, i) =ninw(6)where w is a word, i is a splitting position (from1 to the length of w minus 1); niis the number oftimes the words as split at position i according tothe character-level alignment, that is, the characterbefore and after i are aligned to different Englishexpressions; nwis occurrence count of word w inthe training corpus.4 ExperimentsIn the last section we found that 9.26% of wordsproduced by the CTB segmenter have the poten-tial to cause problems for SMT, and propose alexical word splitter to address this issue throughsegmentation refinement.
This section containsexperiments designed to empirically evaluate theproposed lexical word splitter in three aspects:first, whether the WS accuracy is improved; sec-ond, whether the accuracy of the unsupervised WAduring training SMT systems is improved; third,whether the end-to-end translation quality is im-proved.This section first describes the experimentalmethodology, then presents the experimental re-sults, and finally illustrates the operation of ourproposed method using a real example.4.1 Experimental Methodology4.1.1 Experimental CorporaThe GALE manual WA corpus and the Chinese toEnglish corpus from the shared task of the NISTopen machine translation (OpenMT) 2006 evalua-tion 6 were employed as the experimental corpus(Table 5).The experimental corpus for WS was con-structed by first segmenting 2000 held out sen-tences from the GALE manual WA corpus withthe Stanford segmenter, and then refining the seg-mentation with the gold alignment annotation.
Forexample, the gold segmentation for the examplesin Figure 2 is presented in Figure 5.
Note thatthis test corpus is intended to represent an oraclesegmentation for our proposed method, and servesprimarily to gauge the improvement of our methodover the baseline Stanford segmenter, relative toan upper bound.6http://www.itl.nist.gov/iad/mig/tests/mt/2006/1659                 !Figure 5: Examples of gold WS for evaluationSet # sent.
pairs # CN tokens # EN tokensTrain.
442,967 19,755,573 13,444,927Eval02 878?
38,204 105,944Eval03 919?
40,900 113,970Eval04 1,597?
71,890 207,279Eval05 1,082?
50,461 138,952Eval06 1,664?
62,422 189,059Table 5: NIST Open machine translation 2006Corpora.
?
Number of sentence samples whichcontain one Chinese sentence and four English ref-erence sentences.The experimental corpus for unsupervised WAwas the union set of the NIST OpenMT trainingset and the 2000 test sentence pairs from GALEWA corpus.
We removed the United Nations cor-pus from the NIST OpenMT constraint training re-sources because it is out of domain.The main result of this paper is the evaluationof the end-to-end performance of an SMT sys-tem.
The experimental corpus for this task wasthe NIST OpenMT corpus.
The data set of theNIST evaluation 2002 was used as a developmentset for MERT tuning (Och, 2003), and the remain-ing data sets of the NIST evaluation from 2003 to2006 were used as test sets.
The English sentenceswere tokenized by Stanford toolkit 7 and convertedto lowercase.4.1.2 EvaluationThe performance of WS was measured by pre-cision, recall and F1of gold words (Sproat andEmerson, 2003),The performance of unsupervised WA in theSMT training procedure was measured throughalignment error rate (AER)(Och and Ney, 2000;Liang et al., 2006).
Sure alignment edges andpossible alignment edges were not distinguishedin this paper as no such tags are found in GALEmanual WA corpus.The performance of SMT was measured usingBLEU (Papineni et al., 2002).7http://nlp.stanford.edu/software/corenlp.shtml4.1.3 Baseline MethodsTwo Chinese WS methods were taken as the base-line methods in this paper.
One method was theCTB segmenter, that is, Stanford Chinese wordsegmenter with the model trained on CTB corpus.The other method was the length tuner in (Changet al., 2008), which added a constant into the con-fidence scores of a trained CRF word segmenter toencourage it to output more word boundaries (seeSection 3.2 for details).4.1.4 Implementation and Parameter settingsThe proposed lexical word splitter was imple-mented on the CRF model toolkit released withthe Stanford segmenter (Tseng et al., 2005).
Theregularity parameters ?kare set to be 3, the sameas the Stanford segmenter, because no significantperformance improvements were observed by tun-ing that parameter.To extract features for the word splitter, theStanford named entity recognizer (Finkel et al.,2005)8 was employed to obtain the tags of namedentities.
Word frequencies were caculated fromthe source side of SMT training corpus.
Thecharacter-level unsupervised alignment was con-ducted using GIZA++ (Och and Ney, 2003)9.The length tuner reused the CRF model of CTBsegmenter.
The parameter ?0was tuned throughthe grid search in (Chang et al., 2008), that is, ob-serving the BLEU score on the SMT developmentset varing from ?0= 0 to ?0= 32.
The gridsearch showed that ?0= 2 was optimal, agreeingwith the value in (Chang et al., 2008).Moses (Koehn et al., 2007)10, a state-of-the-artphrase-based SMT system, was employed to per-form end-to-end SMT experiments.
GIZA++ wasemployed to perform unsupervised WA.4.2 Experimental Results4.2.1 Word SegmentationThe WS performance of CTB segmenter, lengthtuner and the proposed lexical splitter are pre-sented in Table 6.
The proposed method achievesthe highest scores on all the criterion of F1, preci-sion and recall.
The length tuner outperforms theCTB segmenter in terms of recall, but with lowerprecision.8http://nlp.stanford.edu/software/CRF-NER.shtml9http://www.statmt.org/moses/giza/GIZA++.html10http://www.statmt.org/moses/1660WS F1Prec.
RecallCTB segmenter 0.878 0.917 0.842Length tuner 0.873 0.894 0.852Lexical splitter 0.915 0.922 0.908Table 6: Performance of WSWS AER Prec.
RecallCTB segmenter 0.425 0.622 0.534Length tuner 0.417 0.642 0.535Lexical splitter 0.396 0.674 0.547Table 7: Performance of unsupervised WA usingdifferent WS strategies4.2.2 Word AlignmentThe WA performance of the CTB segmenter,length tuner and the proposed lexical spliter is pre-sented in Table 7.
Both lexical splitter and lengthtuner outperform the CTB segmenter, indicatingthe splitting words into smaller pieces can improvethe accuracy of unsupervised WA.
This result sup-ports the finding in (Chang et al., 2008) that thesegment size from CTB WS is too large for SMT.In addition, the proposed lexical splitter signifi-cantly outperforms the length tuner.4.2.3 Machine TranslationThe end-to-end SMT performance of CTB seg-menter, length tuner and the proposed lexicalspliter are presented in Table 8.
Each experimentwas performed three times, and the average BLEUand standard derivation were calculated, becausethere is randomness in the results from MERT.The proposed lexical splitter outperformed the twobaselines on all the test sets, and achieves anaverage improvement of 0.63 BLEU percentagepoints, indicating that the proposed method caneffectively improve the translation quality.
Thelength tuner also outperforms the CTB segmenter,but the average improvement is 0.15 BLEU per-centage points, much less than the proposed meth-ods.4.3 AnalysisFigure 6 presents an example from the test cor-pus, which demonstrates how the proposed lexicalsplitter splits words more accurately than the base-line length tuner method.
Two words in the seg-mentation result of the CTB segmenter are wor-thy of attention.
The first one is ?yang nian?
(theyear of goat), the lexical splitter split this word andgot the right translation, while the length tuner didnot split it.
The second is ?rong jing?
(booming orprosperity), the length tuner split this word, whichresulted in wrong translations, while the lexicalsplitter avoided this mistake.5 Comparison to Related WorkThe most similar work in the literature to the pro-posed method is the the length tuner method pro-posed by (Chang et al., 2008).
This method alsoencourages the generation of more words duringsegmentation by using a single parameter that canbe use to control segment length.
Our method dif-fers from theirs in that it is able to acquire vocabu-lary knowledge from word alignments that can beused to more accurately split words into segmentssuitable for machine translation.There is large volume of research using bilin-gual unsupervised and semi-supervised WS to ad-dress the problem of optimizing WS for SMT (Xuet al., 2008; Chung and Gildea, 2009; Nguyen etal., 2010; Michael et al., 2011).
The main differ-ence with our approach is that they use automaticWA results, most often obtained using the sametools as are used in training SMT systems.
One ofthe main problems of using unsupervised WA isthat it is noisy, and therefore, employing iterativeoptimization methods to refine the results of unsu-pervised WA is a key issue in their research, forexample boosting (Ma and Way, 2009; Michael etal., 2011), expectation maximization (Chung andGildea, 2009), Bayesian sampling (Xu et al., 2008;Nguyen et al., 2010), or heuristic search (Zhao etal., 2013).
Nevertheless, noisy WA makes bothanalyzing WS and improving SMT quality quitehard.
In contrast, by using manual WA, we canclearly analyze the segmentation problems (Sec-tion 2), and train supervised models to solve theproblem (Section 3).As far as we are aware, among related workon WS, our method achieves the highest BLEUimprovement relative to the start-of-the-art WS ?the Stanford Chinese word segmenter ?
on theChinese-English OpenMT corpora.
The meth-ods proposed in (Ma and Way, 2009; Chungand Gildea, 2009) fail to outperform the Stan-ford Chinese word segmenter on Chinese-EnglishOpenMT corpora.
The length tuner method pro-posed in (Chang et al., 2008) is less effective toours according to the experimental results in thispaper.1661WS eval03 eval04 eval05 eval06 improveCTB segmenter 31.89 ?
0.09 32.73 ?
0.19 31.03 ?
0.16 31.38 ?
0.23Length tuner 32.06 ?
0.07 32.74 ?
0.10 31.34 ?
0.11 31.50 ?
0.11 0.15 ?
0.12Lexical splitter 32.55 ?
0.18 32.94 ?
0.11 31.87 ?
0.15 32.17 ?
0.35 0.63 ?
0.29Table 8: Performance (BLEU) of SMT            	 			 (a) 			 		  	        (b)  		        (c) 	 	 		 	      (d)Figure 6: Example of SMT from test sets.
(a) source; (b) CTB segmenter; (c) length tuner; (d) lexicalsplitter.
The four gold references are: ?ethnic chinese in asia celebrate year of goat and hope for economicprosperity in new year?, ?
asian chinese celebrate the arrival of the year of sheep and wish a prosperousnew year?, ?
asian chinese happily welcome the year of goat , expecting economic prosperity in newyear?,?asian chinese happily welcomed year of the goat , praying for prosperity in the new year?6 ConclusionThis paper is concerned with the role of wordsegmentation in Chinese-to-English SMT.
We ex-plored the use of a manually annotated word align-ment corpus to refine word segmentation for ma-chine translation.
Based on an initial finding that74.60% of running sentences in the WA corpushave segmentation inconsistent with a gold WAannotation, we proposed a supervised lexical re-segmentation model to modify the WS in order torelieve these issues.Our main experimental results show that theproposed approach is capable of improving bothalignment quality and end-to-end translation qual-ity.
The proposed method achieved the highestBLEU score relative to a number of respectablebaseline systems that included the Stanford wordsegmenter, and an improved Stanford word seg-menter that could be tuned for segment length.
Nolanguage-specific techniques other than a manu-ally aligned corpus were employed in this paper,thus the approach can applied to other SMT lan-guage pairs that require WS.In the future, we plan to explore combiningmultiple source words which are aligned to thesame target words.
This is the symmetric topicof the post word splitting which is studied in thispaper.
The effect of this word combination oper-1662ation on SMT is non-trivial.
On one hand, it canreduce the ambiguity in the source side.
On theother hand, it may cause sparseness problems.AcknowledgementsWe thank the three reviewers for their valuablecomments.
We also thank the Stanford natural lan-guage processing group for releasing the sourcecodes of their word segmenter.ReferencesPi-Chuan Chang, Michel Galley, and Christopher DManning.
2008.
Optimizing Chinese word segmen-tation for machine translation performance.
In Pro-ceedings of the 3rd Workshop on Statistical MachineTranslation, pages 224?232.
Association for Com-putational Linguistics.Tagyoung Chung and Daniel Gildea.
2009.
Unsu-pervised tokenization for machine translation.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume2-Volume 2, pages 718?726.
Association for Com-putational Linguistics.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In ACL,volume 45, page 17.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.
Association for Computational Lin-guistics.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, andBenjamin K Tsou.
2011.
Overview of the patentmachine translation task at the NTCIR-9 workshop.In Proceedings of NTCIR, volume 9, pages 559?578.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with su-pervised itg models.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP: Volume 2-Volume 2, pages 923?931.
Association for Compu-tational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.
Association for Computational Lin-guistics.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the 18th Interna-tional Conference on Machine Learning, pages 282?289.
Association for Computing Machinery.Xuansong Li, Niyu Ge, Stephen Grimes, StephanieStrassel, and Kazuaki Maeda.
2010.
Enrichingword alignment with linguistic tags.
In LREC, pages2189?2195.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the mainconference on Human Language Technology Con-ference of the North American Chapter of the As-sociation of Computational Linguistics, pages 104?111.
Association for Computational Linguistics.Yanjun Ma and Andy Way.
2009.
Bilingually moti-vated domain-adapted word segmentation for statis-tical machine translation.
In Proceedings of the 12thConference of the European Chapter of the Associa-tion for Computational Linguistics, pages 549?557.Association for Computational Linguistics.Paul Michael, Andrew Finch, and Eiichiro Sumita.2011.
Integration of multiple bilingually-trainedsegmentation schemes into statistical machine trans-lation.
IEICE transactions on information and sys-tems, 94(3):690?697.Rada Mihalcea and Ted Pedersen.
2003.
An evaluationexercise for word alignment.
In Rada Mihalcea andTed Pedersen, editors, HLT-NAACL 2003 Workshop:Building and Using Parallel Texts: Data Driven Ma-chine Translation and Beyond, pages 1?10, Edmon-ton, Alberta, Canada, May 31.
Association for Com-putational Linguistics.ThuyLinh Nguyen, Stephan Vogel, and Noah A Smith.2010.
Nonparametric word segmentation for ma-chine translation.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics,pages 815?823.
Association for Computational Lin-guistics.Franz Josef Och and Hermann Ney.
2000.
A com-parison of alignment models for statistical machinetranslation.
In Proceedings of the 18th confer-ence on Computational linguistics-Volume 2, pages1086?1090.
Association for Computational Linguis-tics.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics-Volume 1, pages 160?167.
As-sociation for Computational Linguistics.1663Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, pages 311?318.
Associationfor Computational Linguistics.Fuchun Peng and Andrew McCallum.
2006.
Infor-mation extraction from research papers using con-ditional random fields.
Information Processing &Management, 42(4):963?979.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detec-tion using conditional random fields.
Computer Sci-ence Department Faculty Publication Series.Richard Sproat and Thomas Emerson.
2003.
Thefirst international chinese word segmentation bake-off.
In Proceedings of the second SIGHAN work-shop on Chinese language processing-Volume 17,pages 133?143.
Association for Computational Lin-guistics.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A con-ditional random field word segmenter for SIGHANBakeoff 2005.
In Proceedings of the 4th SIGHANWorkshop on Chinese Language Processing, volume171.
Jeju Island, Korea.Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-mann Ney.
2008.
Bayesian semi-supervisedChinese word segmentation for statistical machinetranslation.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics-Volume 1, pages 1017?1024.
Association for Com-putational Linguistics.Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.2002.
Building a large-scale annotated chinese cor-pus.
In Proceedings of the 19th international confer-ence on Computational linguistics-Volume 1, pages1?8.
Association for Computational Linguistics.Richard Zens, Franz Josef Och, and Hermann Ney.2002.
Phrase-based statistical machine translation.In Advances in Artificial Intelligence, pages 18?32.Springer.Hai Zhao, Masao Utiyama, Eiichro Sumita, and Bao-Liang Lu.
2013.
An empirical study on word seg-mentation for chinese machine translation.
A. Gel-bukh (Ed.
): CICLing 2013, Part II, LNCS 7817,pages 248?263.1664
