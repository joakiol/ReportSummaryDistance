Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 512?520,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsStreaming for large scale NLP: Language ModelingAmit Goyal, Hal Daume?
III, and Suresh VenkatasubramanianUniversity of Utah, School of Computing{amitg,hal,suresh}@cs.utah.eduAbstractIn this paper, we explore a streaming al-gorithm paradigm to handle large amountsof data for NLP problems.
We present anefficient low-memory method for construct-ing high-order approximate n-gram frequencycounts.
The method is based on a determinis-tic streaming algorithm which efficiently com-putes approximate frequency counts over astream of data while employing a small mem-ory footprint.
We show that this method eas-ily scales to billion-word monolingual corporausing a conventional (8 GB RAM) desktopmachine.
Statistical machine translation ex-perimental results corroborate that the result-ing high-n approximate small language modelis as effective as models obtained from othercount pruning methods.1 IntroductionIn many NLP problems, we are faced with the chal-lenge of dealing with large amounts of data.
Manyproblems boil down to computing relative frequen-cies of certain items on this data.
Items can bewords, patterns, associations, n-grams, and others.Language modeling (Chen and Goodman, 1996),noun-clustering (Ravichandran et al, 2005), con-structing syntactic rules for SMT (Galley et al,2004), and finding analogies (Turney, 2008) areexamples of some of the problems where we needto compute relative frequencies.
We use languagemodeling as a canonical example of a large-scaletask that requires relative frequency estimation.Computing relative frequencies seems like aneasy problem.
However, as corpus sizes grow,it becomes a highly computational expensive task.Cutoff Size BLEU NIST METExact 367.6m 28.73 7.691 56.322 229.8m 28.23 7.613 56.033 143.6m 28.17 7.571 56.535 59.4m 28.33 7.636 56.0310 18.3m 27.91 7.546 55.64100 1.1m 28.03 7.607 55.91200 0.5m 27.62 7.550 55.67Table 1: Effect of count-based pruning on SMT per-formance using EAN corpus.
Results are according toBLEU, NIST and METEOR (MET) metrics.
Bold #s arenot statistically significant worse than exact model.Brants et al (2007) used 1500 machines for aday to compute the relative frequencies of n-grams(summed over all orders from 1 to 5) from 1.8TBof web data.
Their resulting model contained 300million unique n-grams.It is not realistic using conventional computing re-sources to use all the 300 million n-grams for ap-plications like speech recognition, spelling correc-tion, information extraction, and statistical machinetranslation (SMT).
Hence, one of the easiest way toreduce the size of this model is to use count-basedpruning which discards all n-grams whose count isless than a pre-defined threshold.
Although count-based pruning is quite simple, yet it is effective formachine translation.
As we do not have a copy ofthe web, we will use a portion of gigaword i.e.
EAN(see Section 4.1) to show the effect of count-basedpruning on performance of SMT (see Section 5.1).Table 1 shows that using a cutoff of 100 produces amodel of size 1.1 million n-grams with a Bleu scoreof 28.03.
If we compare this with an exact modelof size 367.6 million n-grams, we see an increase of0.8 points in Bleu (95% statistical significance level512?
Size BLEU NIST METExact 367.6m 28.73 7.691 56.321e-10 218.4m 28.64 7.669 56.335e-10 171.0m 28.48 7.666 56.381e-9 148.0m 28.56 7.646 56.515e-9 91.9m 28.27 7.623 56.161e-8 69.4m 28.15 7.609 56.195e-7 28.5m 28.08 7.595 55.91Table 2: Effect of entropy-based pruning on SMT perfor-mance using EAN corpus.
Results are as in Table 1is ?
0.53 Bleu).
However, we need 300 times big-ger model to get such an increase.
Unfortunately, itis not possible to integrate such a big model inside adecoder using normal computation resources.A better way of reducing the size of n-grams is touse entropy pruning (Stolcke, 1998).
Table 2 showsthe results with entropy pruning with different set-tings of ?.
We see that for three settings of ?
equal to1e-10, 5e-10 and 1e-9, we get Bleu scores compara-ble to the exact model.
However, the size of all thesemodels is not at all small.
The size of smallest modelis 25% of the exact model.
Even with this size it isstill not feasible to integrate such a big model insidea decoder.
If we take a model of size comparable tocount cutoff of 100, i.e., with ?
= 5e-7, we see bothcount-based pruning as well as entropy pruning per-forms the same.There also have been prior work on maintain-ing approximate counts for higher-order languagemodels (LMs) ((Talbot and Osborne, 2007a; Tal-bot and Osborne, 2007b; Talbot and Brants, 2008))operates under the model that the goal is to store acompressed representation of a disk-resident table ofcounts and use this compressed representation to an-swer count queries approximately.There are two difficulties with scaling all theabove approaches as the order of the LM increases.Firstly, the computation time to build the database ofcounts increases rapidly.
Secondly, the initial diskstorage required to maintain these counts, prior tobuilding the compressed representation is enormous.The method we propose solves both of these prob-lems.
We do this by making use of the streaming al-gorithm paradigm (Muthukrishnan, 2005).
Workingunder the assumption that multiple-GB models areinfeasible, our goal is to instead of estimating a largemodel and then compressing it, we directly estimatea small model.
We use a deterministic streaming al-gorithm (Manku and Motwani, 2002) that computesapproximate frequency counts of frequently occur-ring n-grams.
This scheme is considerably more ac-curate in getting the actual counts as compared toother schemes (Demaine et al, 2002; Karp et al,2003) that find the set of frequent items without car-ing about the accuracy of counts.We use these counts directly as features in anSMT system, and propose a direct way to integratethese features into an SMT decoder.
Experimentsshow that directly storing approximate counts of fre-quent 5-grams compared to using count or entropy-based pruning counts gives equivalent SMT perfor-mance, while dramatically reducing the memory us-age and getting rid of pre-computing a large model.2 Background2.1 n-gram Language ModelsLanguage modeling is based on assigning probabil-ities to sentences.
It can either compute the proba-bility of an entire sentence or predict the probabilityof the next word in a sequence.
Let wm1 denote a se-quence of words (w1, .
.
.
, wm).
The probability ofestimating word wm depends on previous n-1 wordswhere n denotes the size of n-gram.
This assump-tion that probability of predicting a current word de-pends on the previous words is called a Markov as-sumption, typically estimated by relative frequency:P (wm | wm?1m?n+1) =C(wm?1m?n+1wm)C(wm?1m?n+1)(1)Eq 1 estimates the n-gram probability by taking theratio of observed frequency of a particular sequenceand the observed frequency of the prefix.
This isprecisely the relative frequency estimate we seek.2.2 Large-scale Language modelingUsing higher order LMs to improve the accuracyof SMT is not new.
(Brants et al, 2007; Emamiet al, 2007) built 5-gram LMs over web using dis-tributed cluster of machines and queried them vianetwork requests.
Since the use of cluster of ma-chines is not always practical, (Talbot and Osborne,2007b; Talbot and Osborne, 2007a) showed a ran-domized data structure called Bloom filter, that canbe used to construct space efficient language models513for SMT.
(Talbot and Brants, 2008) presented ran-domized language model based on perfect hashingcombined with entropy pruning to achieve furthermemory reductions.
A problem mentioned in (Tal-bot and Brants, 2008) is that the algorithm that com-putes the compressed representation might need toretain the entire database in memory; in their paper,they design strategies to work around this problem.
(Federico and Bertoldi, 2006) also used single ma-chine and fewer bits to store the LM probability byusing efficient prefix trees.
(Uszkoreit and Brants, 2008) used partially class-based LMs together with word-based LMs to im-prove SMT performance despite the large size ofthe word-based models used.
(Schwenk and Koehn,2008; Zhang et al, 2006) used higher language mod-els at time of re-ranking rather than integrating di-rectly into the decoder to avoid the overhead ofkeeping LMs in the main memory since disk lookupsare simply too slow.
Now using higher order LMs attime of re-ranking looks like a good option.
How-ever, the target n-best hypothesis list is not diverseenough.
Hence if possible it is always better to inte-grate LMs directly into the decoder.2.3 StreamingConsider an algorithm that reads the input from aread-only stream from left to right, with no abilityto go back to the input that it has already processed.This algorithm has working storage that it can use tostore parts of the input or other intermediate compu-tations.
However, (and this is a critical constraint),this working storage space is significantly smallerthan the input stream length.
For typical algorithms,the storage size is of the order of logk N , where Nis the input size and k is some constant.Stream algorithms were first developed in theearly 80s, but gained in popularity in the late 90sas researchers first realized the challenges of dealingwith massive data sets.
A good survey of the modeland core challenges can be found in (Muthukrish-nan, 2005).
There has been considerable work on theproblem of identifying high-frequency items (itemswith frequency above a threshold), and a detailed re-view of these methods is beyond the scope of this ar-ticle.
A new survey by (Cormode and Hadjielefthe-riou, 2008) comprehensively reviews the literature.3 Space-Efficient Approximate FrequencyEstimationPrior work on approximate frequency estimation forlanguage models provide a ?no-false-negative?
guar-antee, ensuring that counts for n-grams in the modelare returned exactly, while working to make sure thefalse-positive rate remains small (Talbot and Os-borne, 2007a).
The notion of approximation we useis different: in our approach, it is the actual countvalues that will be approximated.
We also exploitthe fact that low-frequency n-grams, while consti-tuting the vast majority of the set of unique n-grams,are usually smoothed away and are less likely to in-fluence the language model significantly.
Discard-ing low-frequency n-grams is particularly importantin a stream setting, because it can be shown in gen-eral that any algorithm that generates approximatefrequency counts for all n-grams requires space lin-ear in the input stream (Alon et al, 1999).We employ an algorithm for approximate fre-quency counting proposed by (Manku and Motwani,2002) in the context of database management.
Fixparameters s ?
(0, 1), and ?
?
(0, 1), ?
?
s. Ourgoal is to approximately find all n-grams with fre-quency at least sN .
For an input stream of n-gramsof length N , the algorithm outputs a set of items(and frequencies) and guarantees the following:?
All items with frequencies exceeding sN areoutput (no false negatives).?
No item with frequency less than (s ?
?
)N isoutput (few false positives).?
All reported frequencies are less than the truefrequencies by at most ?N (close-to-exact fre-quencies).?
The space used by the algorithm isO(1?
log ?N).A simple example illustrates these properties.
Letus fix s = 0.01, ?
= 0.001.
Then the algorithm guar-antees that all n-grams with frequency at least 1%will be returned, no element with frequency less than0.9% will be returned, and all frequencies will be nomore than 0.1% away from the true frequencies.
Thespace used by the algorithm is O(logN), which canbe compared to the much larger (close to N ) space514needed to store the initial frequency counts.
In addi-tion, the algorithm runs in linear time by definition,requiring only one pass over the input.
Note thatthere might be 1?
elements with frequency at least?N , and so the algorithm uses optimal space (up toa logarithmic factor).3.1 The AlgorithmWe present a high-level overview of the algorithm;for more details, the reader is referred to (Mankuand Motwani, 2002).
The algorithm proceeds byconceptually dividing the stream into epochs, eachcontaining 1/?
elements.
Note that there are ?Nepochs.
Each such epoch has an ID, starting from1.
The algorithm maintains a list of tuples1 of theform (e, f,?
), where e is an n-gram, f is its re-ported frequency, and ?
is the maximum error in thefrequency estimation.
While the algorithm reads n-grams associated with the current epoch, it does oneof two things: if the new element e is contained inthe list of tuples, it merely increments the frequencycount f .
If not, it creates a new tuple of the form(e, 1, T ?1), where T is the ID of the current epoch.After each epoch, the algorithm ?cleans house?
byeliminating tuples whose maximum true frequencyis small.
Formally, if the epoch that just endedhas ID T , then the algorithm deletes all tuples sat-isfying condition f + ?
?
T .
Since T ?
?N ,this ensures that no low-frequency tuples are re-tained.
When all elements in the stream have beenprocessed, the algorithm returns all tuples (e, f,?
)where f ?
(s??
)N .
In practice, however we do notcare about s and return all tuples.
At a high level,the reason the algorithm works is that if an elementhas high frequency, it shows up more than once eachepoch, and so its frequency gets updated enough tostave off elimination.4 Intrinsic EvaluationWe conduct a set of experiments with approxi-mate n-gram counts (stream counts) produced bythe stream algorithm.
We define various metrics onwhich we evaluate the quality of stream counts com-pared with exact n-gram counts (true counts).
To1We use hash tables to store tuples; however smarter datastructures like suffix trees could also be used.Corpus Gzip-MB M-wrds PerplexityEP 63 38 1122.69afe 417 171 1829.57apw 1213 540 1872.96nyt 2104 914 1785.84xie 320 132 1885.33Table 3: Corpus Statistics and perplexity of LMs madewith each of these corpuses on development setevaluate the quality of stream counts on these met-rics, we carry out three experiments.4.1 Experimental SetupThe freely available English side of Europarl (EP)and Gigaword corpus (Graff, 2003) is used forcomputing n-gram counts.
We only use EP alongwith two sections of the Gigaword corpus: AgenceFrance Press English Service(afe) and The NewYork Times Newswire Service (nyt).
The unigramlanguage models built using these corpuses yieldbetter perplexity scores on the development set (seeSection 5.1) compared to The Xinhua News AgencyEnglish Service (xie) and Associated Press World-stream English Service (apw) as shown in Table 3.The LMs are build using the SRILM language mod-elling toolkit (Stolcke, 2002) with modified Kneser-Ney discounting and interpolation.
The evaluationof stream counts is done on EP+afe+nyt (EAN) cor-pus, consisting of 1.1 billion words.4.2 Description of the metricsTo evaluate the quality of counts produced by ourstream algorithm four different metrics are used.The accuracy metric measures the quality of top Nstream counts by taking the fraction of top N streamcounts that are contained in the top N true counts.Accuracy = Stream Counts ?
True CountsTrue CountsSpearman?s rank correlation coefficient or Spear-man?s rho(?)
computes the difference between theranks of each observation (i.e.
n-gram) on two vari-ables (that are top N stream and true counts).
Thismeasure captures how different the stream count or-dering is from the true count ordering.?
= 1?
6?
d2iN(N2 ?
1)515di is the difference between the ranks of correspond-ing elements Xi and Yi; N is the number of elementsfound in both sets; Xi and Yi in our case denote thestream and true counts.Mean square error (MSE) quantifies the amountby which a predicted value differs from the truevalue.
In our case, it estimates how different thestream counts are from the true counts.MSE = 1NN?i=1(truei ?
predictedi)2true and predicted denotes values of true and streamcounts; N denotes the number of stream counts con-tained in true counts.4.3 Varying ?
experimentsIn our first experiment, we use accuracy, ?
and MSEmetrics for evaluation.
Here, we compute 5-gramstream counts with different settings of ?
on the EANcorpus.
?
controls the number of stream counts pro-duced by the algorithm.
The results in Table 4 sup-port the theory that decreasing the value of ?
im-proves the quality of stream counts.
Also, as ex-pected, the algorithm produces more stream countswith smaller values of ?.
The evaluation of streamcounts obtained with ?
= 50e-8 and 20e-8 reveal thatthe stream counts learned with this large value aremore susceptible to errors.If we look closely at the counts for ?
= 50e-8, wesee that we get at least 30% of the stream countsfrom 245k true counts.
This number is not signifi-cantly worse than the 36% of stream counts obtainedfrom 4, 018k true counts for the smallest value of?
= 5e-8.
However, if we look at the other two met-rics, the ranking correlation ?
of stream counts com-pared with true counts on ?
= 50e-8 and 20e-8 is lowcompared to other ?
values.
For the MSE, the errorwith stream counts on these ?m values is again highcompared to other values.
As we decrease the valueof ?
we continually get better results: decreasing ?pushes the stream counts towards the true counts.However, using a smaller ?
increases the memoryusage.
Looking at the evaluation, it is therefore ad-visable to use 5-gram stream counts produced withat most ?
?
10e-7 for the EAN corpus.Since it is not possible to compute true 7-gramscounts on EAN with available computing resources,?
5-gram Acc ?
MSEproduced50e-8 245k 0.294 -3.6097 0.495420e-8 726k 0.326 -2.6517 0.115510e-8 1655k 0.352 -1.9960 0.03685e-8 4018k 0.359 -1.7835 0.0114Table 4: Evaluating quality of 5-gram stream counts fordifferent settings of ?
on EAN corpus?
7-gram Acc ?
MSEproduced50e-8 44k 0.509 0.3230 0.034120e-8 128k 0.596 0.5459 0.006310e-8 246k 0.689 0.7413 0.00185e-8 567k 0.810 0.8599 0.0004Table 5: Evaluating quality of 7-gram stream counts fordifferent settings of ?
on EP corpuswe carry out a similar experiment for 7-grams on EPto verify the results for higher order n-grams 2.
Theresults in Table 5 tell a story similar to our results for7-grams.
The size of EP corpus is much smaller thanEAN and so we see even better results on each of themetrics with decreasing the value of ?.
The overalltrend remains the same; here too, setting ?
?
10e-8 is the most effective strategy.
The fact that theseresults are consistent across two datasets of differentsizes and different n-gram sizes suggests that theywill carry over to other tasks.4.4 Varying top K experimentsIn the second experiment, we evaluate the qualityof the top K (sorted by frequency) 5-gram streamcounts.
Here again, we use accuracy, ?
and MSE forevaluation.
We fix the value of ?
to 5e-8 and com-pute 5-gram stream counts on the EAN corpus.
Wevary the value of K between 100k and 4, 018k (i.eall the n-gram counts produced by the stream algo-rithm).
The experimental results in Table 6 supportthe theory that stream count algorithm computes theexact count of most of the high frequency n-grams.Looking closer, we see that if we evaluate the algo-rithm on just the top 100k 5-grams (roughly 5% ofall 5-grams produced), we see almost perfect results.Further, if we take the top 1, 000k 5-grams (approx-imately 25% of all 5-grams) we again see excellent2Similar evaluation scores are observed for 9-gram streamcounts with different values of ?
on EP corpus.516Top K Accuracy ?
MSE100k 0.994 0.9994 0.01266500k 0.934 0.9795 0.01051000k 0.723 0.8847 0.01432000k 0.504 0.2868 0.01374018k 0.359 -1.7835 0.0114Table 6: Evaluating top K sorted 5-gram stream countsfor ?=5e-8 on EAN corpusperformance on all metrics.
The accuracy of the re-sults decrease slightly, but the ?
and MSE metricsare not decreased that much in comparison.
Perfor-mance starts to degrade as we get to 2, 000k (over50% of all 5-grams), a result that is not too surpris-ing.
However, even here we note that the MSE islow, suggesting that the frequencies of stream counts(found in top K true counts) are very close to thetrue counts.
Thus, we conclude that the quality ofthe 5-gram stream counts produced for this value of?
is quite high (in relation to the true counts).As before, we corroborate our results with higherorder n-grams.
We evaluate the quality of top K 7-gram stream counts on EP.3 Since EP is a smallercorpus, we evaluate the stream counts produced bysetting ?
to 10e-8.
Here we vary the value of K be-tween 10k and 246k (the total number produced bythe stream algorithm).
Results are shown in Table7.
As we saw earlier with 5-grams, the top 10k (i.e.approximately 5% of all 7-grams) are of very highquality.
Results, and this remains true even whenwe increase K to 100k.
There is a drop in the accu-racy and a slight drop in ?, while the MSE remainsthe same.
Taking all counts again shows a signifi-cant decrease in both accuracy and ?
scores, but thisdoes not affect MSE scores significantly.
Hence, the7-gram stream counts i.e.
246k counts produced by?
= 10e-8 are quite accurate when compared to thetop 246k true counts.4.5 Analysis of tradeoff between coverage andspaceIn our third experiment, we investigate whether alarge LM can help MT performance.
We evaluatethe coverage of stream counts built on the EAN cor-pus on the test data for SMT experiments (see Sec-3Similar evaluation scores are observed for different top Ksorted 9-gram stream counts with ?=10e-8 on EP corpus.Top K Accuracy ?
MSE10k 0.996 0.9997 0.001520k 0.989 0.9986 0.001650k 0.950 0.9876 0.0016100k 0.876 0.9493 0.0017246k 0.689 0.7413 0.0018Table 7: Evaluating top K sorted 7-gram stream countsfor ?=10e-8 on EP corpustion 5.1) with different values of ?m.
We computethe recall of each model against 3071 sentences oftest data where recall is the fraction of number ofn-grams of a dataset found in stream counts.Recall = Number of n-grams found in stream countsNumber of n-grams in datasetWe build unigram, bigram, trigram, 5-gram and7-gram with four different values of ?.
Table 8 con-tains the gzip size of the count file and the recallof various different stream count n-grams.
As ex-pected, the recall with respect to true counts is max-imum for unigrams, bigrams, trigrams and 5-grams.However the amount of space required to store alltrue counts in comparison to stream counts is ex-tremely high: we need 4.8GB of compressed spaceto store all the true counts for 5-grams.For unigram models, we see that the recall scoresare good for all values of ?.
If we compare theapproximate stream counts produced by largest ?
(which is worst) to all true counts, we see that thestream counts compressed size is 50 times smallerthan the true counts size, and is only three pointsworse in recall.
Similar trends hold for bigrams,although the loss in recall is higher.
As with uni-grams, the loss in recall is more than made up for bythe memory savings (a factor of nearly 150).
Fortrigrams, we see a 14 point loss in recall for thesmallest ?, but a memory savings of 400 times.
For5-grams, the best recall value is .020 (1.2k out of60k 5-gram stream counts are found in the test set).However, compared with the true counts we onlyloss a recall of 0.05 (4.3k out of 60k) points butmemory savings of 150 times.
In extrinsic evalua-tions, we will show that integrating 5-gram streamcounts with an SMT system performs slightly worsethan the true counts, while dramatically reducing thememory usage.517N -gram unigram bigram trigram 5-gram 7-gram?
Gzip Recall Gzip Recall Gzip Recall Gzip Recall Gzip RecallMB MB MB MB MB50e-8 .352 .785 2.3 .459 3.3 .167 1.9 .006 .864 5.6e-520e-8 .568 .788 4.5 .494 7.6 .207 5.3 .011 2.7 1.3e-410e-8 .824 .791 7.6 .518 15 .237 13 .015 9.7 4.1e-45e-8 1.3 .794 13 .536 30 .267 31 .020 43 5.9e-4all 17 .816 228 .596 1200 .406 4800 .072 NATable 8: Gzipped space required to store n-gram counts on disk and their coverage on a test set with different ?mFor 7-gram we can not compute the true n-gramcounts due to limitations of available computationalresources.
The memory requirements with smallestvalue of ?
are similar to those of 5-gram, but the re-call values are quite small.
For 7-grams, the best re-call value is 5.9e-4 which means that stream countscontains only 32 out of 54k 7-grams contained intest set.
The small recall value for 7-grams suggeststhat these counts may not be that useful in SMT.We further substantiate our findings in our extrinsicevaluations.
There we show that integrating 7-gramstream counts with an SMT system does not affectits overall performance significantly.5 Extrinsic Evaluation5.1 Experimental SetupAll the experiments conducted here make use ofpublicly available resources.
Europarl (EP) corpusFrench-English section is used as parallel data.
Thepublicly available Moses4 decoder is used for train-ing and decoding (Koehn and Hoang, 2007).
Thenews corpus released for ACL SMT workshop in2007 consisting of 1057 sentences5 is used as the de-velopment set.
Minimum error rate training (MERT)is used on this set to obtain feature weights to opti-mize translation quality.
The final SMT system per-formance is evaluated on a uncased test set of 3071sentences using the BLEU (Papineni et al, 2002),NIST (Doddington, 2002) and METEOR (Banerjeeand Lavie, 2005) scores.
The test set is the union ofthe 2007 news devtest and 2007 news test data fromACL SMT workshop 2007.64http://www.statmt.org/moses/5http://www.statmt.org/wmt07/6We found that testing on Parliamentary test data was com-pletely insensitive to large n-gram LMs, even when these LMsare exact.
This suggests that for SMT performance, more data5.2 Integrating stream counts feature intodecoderOur method only computes high-frequency n-gramcounts; it does not estimate conditional probabili-ties.
We can either turn these counts into conditionalprobabilities (by using SRILM) or use the counts di-rectly.
We observed no significant difference in per-formance between these two approaches.
However,using the counts directly consumes significantly lessmemory at run-time and is therefore preferable.
Dueto space constraints, SRILM results are omitted.The only remaining open question is: how shouldwe turn the counts into a feature that can be used inan SMT system?
We considered several alternatives;the most successful was a simple weighted countof n-gram matches of varying size, appropriatelybacked-off.
Specifically, consider an n-gram model.For every sequence of words wi, .
.
.
, wi+N?1, weobtain a feature score computed recursively accord-ing to Eq (2).f(wi) = log?C(wi)Z?
(2)f(wi, .
.
.
, wi+k) = log?C(wi, .
.
.
, wi+k)Z?+ 12f(wi+1, .
.
.
, wi+k)Here, 12 is the backoff factor and Z is the largestcount in the count set (the presence of Z is simply toensure that these values remain manageable).
In or-der to efficiently compute these features, we storethe counts in a suffix-tree.
The computation pro-ceeds by first considering wi+N?1 alone and then?expanding?
to consider the bigram, then trigramand so on.
The advantage to this order of computa-tion is that the recursive calls can cease whenever ais better only if it comes from the right domain.518n-gram(?)
BLEU NIST MET MemGB3 EP(exact) 25.57 7.300 54.48 2.75 EP(exact) 25.79 7.286 54.44 2.93 EAN(exact) 27.04 7.428 55.07 4.65 EAN(exact) 28.73 7.691 56.32 20.54(10e-8) 27.36 7.506 56.19 2.74(5e-8) 27.40 7.507 55.90 2.85(10e-8) 27.97 7.605 55.52 2.85(5e-8) 27.98 7.611 56.07 2.87(10e-8) 27.97 7.590 55.88 2.97(5e-8) 27.88 7.577 56.01 2.99(10e-8) 28.18 7.611 55.95 2.99(5e-8) 27.98 7.608 56.08 2.9Table 9: Evaluating SMT with different LMs on EAN.Results are according to BLEU, NIST and MET metrics.Bold #s are not statistically significant worse than exact.zero count is reached.
(Extending Moses to includethis required only about 100 lines of code.
)5.3 ResultsTable 9 summarizes SMT results.
We have 4 base-line LMs that are conventional LMs smoothed usingmodified Kneser-Ney smoothing.
The first two tri-gram and 5-gram LMs are built on EP corpus andthe other two are built on EAN corpus.
Table 9show that there is not much significant differencein SMT results of 5-gram and trigram LM on EP.As expected, the trigram built on the large corpusEAN gets an improvement of 1.5 Bleu Score.
How-ever, unlike the EP corpus, building a 5-gram LMon EAN (huge corpus) gets an improvement of 3.2Bleu Score.
(The 95% statistical significance bound-ary is about ?
0.53 Bleu on the test data, 0.077 Nistand 0.16 Meteor according to bootstrap resampling)We see similar gains in Nist and Meteor metrics asshown in Table 9.We use stream counts computed with two valuesof ?, 5e-8 and 10e-8 on EAN corpus.
We use allthe stream counts produced by the algorithm.
4, 5, 7and 9 order n-gram stream counts are computed withthese settings of ?.
These counts are used along witha trigram LM built on EP to improve SMT perfor-mance.
The memory usage (Mem) shown in Table9 is the full memory size required to run on the testdata (including phrase tables).Adding 4-gram and 5-gram stream counts as fea-ture helps the most.
The performance gain by using5-gram stream counts is slightly worse than com-pared to true 5-gram LM on EAN.
However, using5-gram stream counts directly is more memory ef-ficient.
Also, the gains for stream counts are ex-actly the same as we saw for same sized count-based and entropy-based pruning counts in Table 1and 2 respectively.
Moreover, unlike the pruningmethods, our algorithm directly computes a smallmodel, as opposed to compressing a pre-computedlarge model.Adding 7-gram and 9-gram does not help signifi-cantly, a fact anticipated by the low recall of 7-gram-based counts that we saw in Section 4.5.
The resultswith two different settings of ?
are largely the same.This validates our intrinsic evaluation results in Sec-tion 4.3 that stream counts learned using ?
?
10e-8are of good quality, and that the quality of the streamcounts is high.6 ConclusionWe have proposed an efficient, low-memory methodto construct high-order approximate n-gram LMs.Our method easily scales to billion-word monolin-gual corpora on conventional (8GB) desktop ma-chines.
We have demonstrated that approximate n-gram features could be used as a direct replacementfor conventional higher order LMs in SMT withsignificant reductions in memory usage.
In future,we will be looking into building streaming skip n-grams, and other variants (like cluster n-grams).In NLP community, it has been shown that havingmore data results in better performance (Ravichan-dran et al, 2005; Brants et al, 2007; Turney, 2008).At web scale, we have terabytes of data and that cancapture broader knowledge.
Streaming algorithmparadigm provides a memory and space-efficientplatform to deal with terabytes of data.
We hopethat other NLP applications (where we need to com-pute relative frequencies) like noun-clustering, con-structing syntactic rules for SMT, finding analogies,and others can also benefit from streaming methods.We also believe that stream counts can be applied toother problems involving higher order LMs such asspeech recognition, information extraction, spellingcorrection and text generation.519ReferencesNoga Alon, Yossi Matias, and Mario Szegedy.
1999.
Thespace complexity of approximating the frequency mo-ments.
J. Comput.
Syst.
Sci., 58(1).Satanjeev Banerjee and Alon Lavie.
2005.
Meteor:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL).S.
Chen and J. Goodman.
1996.
An Empirical Studyof Smoothing Techniques for Language Modeling.
InProceedings of 34th Annual Meeting of the Associ-ation for Computational Linguistics, pages 310?318,Santa Cruz, CA, June.Graham Cormode and Marios Hadjieleftheriou.
2008.Finding frequent items in data streams.
In VLDB.E.D.
Demaine, A. Lopez-Ortiz, and J.I.
Munro.
2002.Frequency estimation of internet packet streams withlimited space.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of the second interna-tional conference on Human Language TechnologyResearch.Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.2007.
Large-scale distributed language modeling.
InProceedings of the 2007 IEEE International Confer-ence on Acoustics, Speech, and Signal Processing(ICASSP), volume 4, pages 37?40.Marcello Federico and Nicola Bertoldi.
2006.
Howmany bits are needed to store probabilities for phrase-based translation?
In Proceedings on the Workshop onStatistical Machine Translation at ACL06.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of HLT/NAACL-04.D.
Graff.
2003.
English Gigaword.
Linguistic Data Con-sortium, Philadelphia, PA, January.Richard M. Karp, Christos H. Papadimitriou, and ScottShenker.
2003.
A simple algorithm for finding fre-quent elements in streams and bags.Philipp Koehn and Hieu Hoang.
2007.
Factored transla-tion models.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 868?876.G.
S. Manku and R. Motwani.
2002.
Approximate fre-quency counts over data streams.
In Proceedings ofthe 28th International Conference on Very Large DataBases.S.
Muthukrishnan.
2005.
Data streams: Algorithms andapplications.
Foundations and Trends in TheoreticalComputer Science, 1(2).K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized algorithms and nlp: using localitysensitive hash function for high speed noun clustering.In ACL ?05: Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics.Holger Schwenk and Philipp Koehn.
2008.
Large anddiverse language models for statistical machine trans-lation.
In Proceedings of The Third International JointConference on Natural Language Processing (IJCNP).Andreas Stolcke.
1998.
Entropy-based pruning of back-off language models.
In In Proc.
DARPA BroadcastNews Transcription and Understanding Workshop.A.
Stolcke.
2002.
SRILM ?
An Extensible LanguageModeling Toolkit.
In Proceedings of the 7th Inter-national Conference on Spoken Language Processing,pages 901?904, Denver, CO, September.David Talbot and Thorsten Brants.
2008.
Randomizedlanguage models via perfect hash functions.
In Pro-ceedings of ACL-08: HLT.David Talbot and Miles Osborne.
2007a.
Randomisedlanguage modelling for statistical machine translation.In Proceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics.David Talbot and Miles Osborne.
2007b.
SmoothedBloom filter language models: Tera-scale LMs on thecheap.
In Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Processingand Computational Natural Language Learning (EMNLP-CoNLL).Peter D. Turney.
2008.
A uniform approach to analogies,synonyms, antonyms, and associations.
In Proceed-ings of COLING 2008.Jakob Uszkoreit and Thorsten Brants.
2008.
Distributedword clustering for large scale class-based languagemodeling in machine translation.
In Proceedings ofACL-08: HLT.Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.2006.
Distributed language modeling for n-best listre-ranking.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Processing.520
