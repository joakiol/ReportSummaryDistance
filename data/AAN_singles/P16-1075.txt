Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 789?799,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsConstrained Multi-Task Learning for Automated Essay ScoringRonan CumminsALTA InstituteComputer LabUniversity of CambridgeMeng ZhangALTA InstituteComputer LabUniversity of Cambridge{rc635,mz342,ejb}@cl.cam.ac.ukTed BriscoeALTA InstituteComputer LabUniversity of CambridgeAbstractSupervised machine learning models forautomated essay scoring (AES) usually re-quire substantial task-specific training datain order to make accurate predictions fora particular writing task.
This limita-tion hinders their utility, and consequentlytheir deployment in real-world settings.
Inthis paper, we overcome this shortcomingusing a constrained multi-task pairwise-preference learning approach that enablesthe data from multiple tasks to be com-bined effectively.Furthermore, contrary to some recent re-search, we show that high performanceAES systems can be built with little or notask-specific training data.
We perform adetailed study of our approach on a pub-licly available dataset in scenarios wherewe have varying amounts of task-specifictraining data and in scenarios where thenumber of tasks increases.1 IntroductionAutomated essay scoring (AES) involves the pre-diction of a score (or scores) relating to the qualityof an extended piece of written text (Page, 1966).With the burden involved in manually grading stu-dent texts and the increase in the number of ESL(English as a second language) learners world-wide, research into AES is increasingly seen asplaying a viable role in assessment.
Automatingthe assessment process is not only useful for ed-ucators but also for learners, as it can provide in-stant feedback and encourage iterative refinementof their writing.The AES task has usually been addressed usingmachine learning.
Given a set of texts and associ-ated gold scores, machine learning approaches aimto build models that can generalise to unseen in-stances.
Regression (Page, 1994; Persing and Ng,2014; Phandi et al, 2015), classification (Larkey,1998; Rudner and Liang, 2002), and preference-ranking1approaches (Yannakoudakis et al, 2011)have all been applied to the task.
In general, ma-chine learning models only perform well when thetraining and test instances are from similar dis-tributions.
However, it is usually the case thatessays are written in response to prompts whichare carefully designed to elicit answers accord-ing to a number of dimensions (e.g.
register,topic, and genre).
For example, Table 1 shows ex-tracts from two prompts from a publicly availabledataset2that aim to elicit different genres of per-suasive/argumentative responses on different top-ics.Most previous work on AES has either ignoredthe differences between essays written in responseto different prompts (Yannakoudakis et al, 2011)with the aim of building general AES systems, orhas built prompt-specific models for each promptindependently (Chen and He, 2013; Persing andNg, 2014).
One of the problems hindering thewide-scale adoption and deployment of AES sys-tems is the dependence on prompt-specific train-ing data, i.e.
substantial model retraining is oftenneeded when a new prompt is released.
Therefore,systems that can adapt to new writing tasks (i.e.prompts) with relatively few new task-specifictraining examples are particularly appealing.
Forexample, a system that is trained using only re-sponses from prompt #1 in Table 1 may not gener-alise well to essays written in response to prompt#2, and vice versa.
Even more complications arisewhen the scoring scale, marking criteria, and/orgrade level (i.e.
educational stage) vary from task1also known as pairwise learning-to-rank2available at https://www.kaggle.com/c/asap-aes789#1Some experts are concerned that people are spending too much time on their computers and less time exercising,enjoying nature, and interacting with family and friends.
Write a letter to your local newspaper in which you stateyour opinion on the effects computers have on people.#2Do you believe that certain materials, such as books, music, movies, magazines, etc., should be removed from theshelves if they are found offensive?
Support your position with convincing arguments from your own experience,observations, and/or reading.Table 1: Two sample writing tasks from the ASAP (Automated Student Assessment Prize) dataset.to task.
If essays written in response to differenttasks are marked on different scoring scales, thenthe actual scores assigned to essays across tasksare not directly comparable.
This effect becomeseven more pronounced when prompts are aimed atstudents in different educational stages.In this paper, we address this problem of promptadaptation using multi-task learning.
In particular,we treat each prompt as a different task and intro-duce a constrained preference-ranking approachthat can learn from multiple tasks even when thescoring scale and marking criteria are differentacross tasks.
Our constrained preference-rankingapproach significantly increases performance overa strong baseline system when there is limitedprompt-specific training data available.
Further-more, we perform a detailed study using varyingamounts of task-specific training data and varyingnumbers of tasks.
First, we review some relatedwork.2 Related WorkA number of commercially available systems forAES, have been developed using machine learn-ing techniques.
These include PEG (Project EssayGrade) (Page, 2003), e-Rater (Attali and Burstein,2006), and Intelligent Essay Assessor (IEA) (Lan-dauer et al, 1998).
Beyond commercial systems,there has been much research into varying as-pects involved in automated assessment, includingcoherence (Higgins et al, 2004; Yannakoudakisand Briscoe, 2012), prompt-relevance (Persingand Ng, 2014; Higgins et al, 2006), argumenta-tion (Labeke et al, 2013; Somasundaran et al,2014; Persing and Ng, 2015), grammatical errordetection and correction (Rozovskaya and Roth,2011; Felice et al, 2014), and the developmentof publicly available resources (Yannakoudakis etal., 2011; Dahlmeier et al, 2013; Persing and Ng,2014; Ng et al, 2014).While most of the early commercially availablesystems use linear-regression models to map essayfeatures to a score, a number of more sophisticatedapproaches have been developed.
Preference-ranking (or pairwise learning-to-rank) has beenshown to outperform regression for the AES prob-lem (Yannakoudakis et al, 2011).
However, theydid not study prompt-specific models, as theirmodels used training data originating from dif-ferent prompts.
We also adopt a preference-ranking approach but explicitly model prompt ef-fects during learning.
Algorithms that aim to di-rectly maximise an evaluation metric have alsobeen attempted.
A listwise learning-to-rank ap-proach (Chen and He, 2013) that directly op-timises quadratic-weighted Kappa, a commonlyused evaluation measure in AES, has also shownpromising results.Using training data from natural language tasksto boost performance of related tasks, for whichthere is limited training data, has received muchattention of late (Collobert and Weston, 2008; Duhet al, 2010; Cheng et al, 2015).
However, therehave been relatively few attempts to apply transferlearning to automated assessment tasks.
Notwith-standing, Napoles and Callison-Burch (2015) usea multi-task approach to model differences inassessors, while Heilman and Madnani (2013)specifically focus on domain-adaptation for shortanswer scoring over common scales.
Most rel-evant is the work of Phandi et al (2015), whoapplied domain-adaptation to the AES task usingEasyAdapt (EA) (Daume III, 2007).
They showedthat supplementing a Bayesian linear ridge re-gression model (BLRR) with data from one othersource domain is beneficial when there is limitedtarget domain data.
However, it was shown thatsimply using the source domain data as extra train-ing data outperformed the EA domain adaptationapproach in three out of four cases.
One majorlimitation to their approach was that in many in-stances the source domain and target domain pairswere from different grade levels.
This means thatany attempt to resolve scores to a common scaleis undermined by the fact that the gold scores arenot comparable across domains, as the essays werewritten by students of different educational levels.A further limitation is that multi-domain adapta-790tion (whereby one has access to multiple sourcedomains) was not considered.The main difference between our work and pre-vious work is that our model incorporates multiplesource tasks and introduces a learning mechanismthat enables us to combine these tasks even whenthe scores across tasks are not directly compara-ble.
This has not been achieved before.
This isnon-trivial as it is difficult to see how this can beaccomplished using a standard linear-regressionapproach.
Furthermore, we perform the first com-prehensive study of multi-task learning for AESusing different training set sizes for a number ofdifferent learning scenarios.3 Preference Ranking ModelIn this section, we describe our baseline AESmodel which is somewhat similar to that devel-oped by Yannakoudakis et al (2011).3.1 Perceptron Ranking (TAPrank)We use a preference-ranking model based on a bi-nary margin-based linear classifier (the Timed Ag-gregate Perceptron or TAP) (Briscoe et al, 2010).In its simplest form this Perceptron uses batchlearning to learn a decision boundary for classi-fying an input vector xi as belonging to one oftwo categories.
A timing-variable ?
(set to 1.0by default) controls both the learning rate and thenumber of epochs during training.
A preference-ranking model is then built by learning to clas-sify pairwise difference vectors, i.e.
learning aweight vector w such that w(xi ?
xj) > ?,when essay i has a higher gold score than essay j,where ?
is the one-sided margin3(Joachims, 2002;Chapelle and Keerthi, 2010).
Therefore, insteadof directly learning to predict the gold score of anessay vector, the model learns a weight vector wthat minimizes the misclassification of differencevectors.
Given that the number of pairwise differ-ence vectors in a moderately sized dataset can beextremely large, the training set is reduced by ran-domly sampling difference vectors according to auser-defined probability (Briscoe et al, 2010).
Inall experiments in our paper we choose this proba-bility such that 5n difference vectors are sampled,where n is the number of training instances (es-says) used.
We did not tune any of the hyper-parameters of the model.3This margin is set to ?
= 2.0 by default.3.2 From Rankings to Predicted ScoresAs the weight vector w is optimized for pairwiseranking, a further step is needed to use the rank-ing model for predicting a score.
In particular,for each of the n vectors in our training set, areal-scalar value is assigned according to the dot-product of the weight vector and the training in-stance (i.e.
w ?
xi), essentially giving its distance(or margin) from the zero vector.
Then using thetraining data, we train a one-dimensional linear re-gression model ?
+  to map these assignments tothe gold score of each instance.Finally, to make a prediction y?
for a test vector,we first calculate its distance from the zero vectorusing w ?
xi and map it to the scoring scale usingthe linear regression model y?
= ?
(w ?xi)+ .
Forbrevity we denote this entire approach (a rankingand a linear regression step) to predicting the finalscore as TAP.3.3 FeaturesThe set of features used for our ranking model issimilar to those identified in previous work (Yan-nakoudakis et al, 2011; Phandi et al, 2015) and isas follows:1. word unigrams, bigrams, and trigrams2.
POS (part-of-speech) counts3.
essay length (as the number of unique words)4.
GRs (grammatical relations)5. max-word length and min-sentence length6.
the presence of cohesive devices7.
an estimated error rateEach essay is processed by the RASP system(Briscoe et al, 2006) with the standard tokeni-sation and sentence boundary detection modules.All n-grams are extracted from the tokenised sen-tences.
The grammatical relations (GRs) are ex-tracted from the top parse of each sentence in theessay.
The presence of cohesive devices are usedas features.
In particular, we use four categories(i.e.
addition, comparison, contrast and conclu-sion) which are hypothesised to measure the cohe-sion of a text.The error rate is estimated based on a languagemodel using ukWaC (Ferraresi et al, 2008) whichcontains more than 2 billion English tokens.
A tri-gram in an essay will be treated as an error if it791Details System Performance (QW-?
)Task # essays Grade Original Mean Score Human BLRR SVM TAPLevel Scale Resolved (0-60) Agreement Phandi Phandi1 1783 8 2-12 39 0.721 0.761 0.781 0.8152 1800 10 1-6 29 0.814 0.606 0.621 0.6743 1726 10 0-3 37 0.769 0.621 0.630 0.6424 1772 10 0-3 29 0.851 0.742 0.749 0.7895 1805 8 0-4 36 0.753 0.784 0.782 0.8016 1800 10 0-4 41 0.776 0.775 0.771 0.7937 1569 7 0-30 32 0.721 0.730 0.727 0.7728 723 10 0-60 37 0.629 0.617 0.534 0.688Table 2: Details of ASAP dataset and a preliminary evaluation of the performance of our TAP baselineagainst previous work (Phandi et al, 2015).
All models used only task-specific data and 5-fold cross-validation.
Best result is in bold.is not found in the language model.
Spelling er-rors are detected using a dictionary lookup, whilea rule-based error module (Andersen et al, 2013)with rules generated from the Cambridge LearnerCorpus (CLC) (Nicholls, 2003) is used to detectfurther errors.
Finally, the unigrams, bigramsand trigrams are weighted by tf-idf (Sparck Jones,1972), while all other features are weighted bytheir actual frequency in the essay.4 Data and Preliminary EvaluationIn order to compare our baseline with previ-ous work, we use the ASAP (Automated Stu-dent Assessment Prize) public dataset.
Somedetails of the essays for the eight tasks in thedataset are described in the Table 2.
The promptselicit responses of different genres and of dif-ferent lengths.
In particular, it is important tonote that the prompts have different scoring scalesand are associated with different grade levels (7-10).
Furthermore, the gold scores are distributeddifferently even if resolved to a common 0-60scale.
In order to benchmark our baseline systemagainst previously developed approaches (BLRRand SVM regression (Phandi et al, 2015)) whichuse this data, we learned task-specific models us-ing 5-fold cross-validation within each of the eightASAP sets and aim to predict the unresolved orig-inal score as per previous work.
We presentthe quadratic weighted kappa (QW-?)
of the sys-tems in Table 2.4Our baseline preference-rankingmodel (TAP) outperforms previous approaches ontask-specific data.
It is worth noting that we didnot tune either of the hyperparameters of TAP.4The results for BLRR and SVM regression are taken di-rectly from the original work and it is unlikely that we haveused the exact same fold split.
Regardless, the consistent in-creases mean that TAP represents a strong baseline systemupon which we develop our constrained multi-task approach.5 Multi-Task LearningFor multi-task learning we use EA encoding(Daume III, 2007) extended over k tasks Tj=1..kwhere each essay xiis associated with one taskxi?
Tj.
The transfer-learning algorithm takes aset of input vectors associated with the essays, andfor each vector xi?
RFmaps it via ?
(xi) to ahigher dimensional space ?
(xi) ?
R(1+k)?F.
Theencoding function ?
(xi) is as follows:?
(x) =k?j=0f(x, j) (1)where?is vector concatenation and f(x, j) is asfollows:f(x, j) =????
?x, if j = 0x, if x ?
Tj0F, otherwise(2)Essentially, the encoding makes a task-specificcopy of the original feature space of dimen-sionality F to ensure that there is one shared-representation and one task-specific representationfor each input vector (with a zero vector for allother tasks).
This approach can be seen as a re-encoding of the input vectors and can be usedwith any vector-based learning algorithm.
Fig.
1(left) shows an example of the extended featurevectors for three tasks Tjon different scoringscales.
Using only the shared-representation (inblue) as input vectors to a learning algorithm re-sults in a standard approach which does not learntask-specific characteristics.
However, using thefull representation allows the learning algorithmto capture both general and task-specific charac-teristics jointly.
This simple encoding technique iseasy to implement and has been shown to be usefulfor a number of NLP tasks (Daume III, 2007).792x1 x1 0 0x2 x2 0 0x3 x3 0 0x4 0 0 x4x5 0 0 x5x6 0 x6 0x7 0 x7 0w0 w1 w2 w301030045602010T1T2T3F0F1F2F3originalscore yisharedrepresentation task-specificrepresentations?(x4)?
(x5) ?(x5)?
?(x4)trainrankingmodelww?(x1)w?
(x2)train linearregressor ?1w?(x4)w?
(x5)train linearregressor ?2w?(x6)w?
(x7)train linearregressor ?3Figure 1: Example of the constrained multi-task learning approach for three tasks where the sharedrepresentation is in blue and the task-specific representations are in orange, red, and green.
The originalgold scores for each task Tjare on different scoring scales.
The preference-ranking weight vector w tobe learned is shown at the bottom.
A one-dimensional linear regression model is learned for each task.5.1 Constrained Preference-RankingGiven essays from multiple tasks, it is often thecase that the gold scores have different distribu-tions, are not on the same scale, and have beenmarked using different criteria.
Therefore, we in-troduce a modification to TAP (called cTAPrank)that constrains the creation of pairwise differencevectors when training the weight vector w. In par-ticular, during training we ensure that pairwise dif-ference vectors are not created from pairs of essaysoriginating from different tasks.5We ensure thatthe same number of difference vectors are sam-pled during training for both TAPrankand our con-strained version (i.e.
both models use the samenumber of training instances).
Figure 1 showsan example of the creation of a valid pairwise-difference vector in the multi-task framework.Furthermore, for cTAPrankwe train a final lin-ear regression step on each of the task-specifictraining data separately.
Therefore, we predicta score y for essay xi for task Tj as y?
=?j(w ?
xi) + j .
This is because for cTAPrank weassume that scores across tasks are not necessar-ily comparable.
Therefore, although we utilise in-formation originating from different tasks, the ap-proach never mixes or directly compares instancesoriginating from different tasks.
This approach topredicting the final score is denoted cTAP.5The same effect can be achieved in SVMrankby encod-ing the prompt/task using the query id (qid).
This constraintis analogous to the way SVMrankis used in information re-trieval where document relevance scores returned from dif-ferent queries are not comparable.6 Experimental Set-upIn this section, we outline the different learningscenarios, data folds, and evaluation metrics usedin our main experiments.6.1 Learning ApproachesWe use the same features outlined in Section 3.3to encode feature vectors for our learning ap-proaches.
In particular we study three learningapproaches denoted and summarised as follows:TAP: which uses the TAPrankalgorithm withinput vectors xi of dimensionality F .MTL-TAP: which uses the TAPrankalgorithmwith MTL extended input vectors ?
(xi).MTL-cTAP: which uses the cTAPrankalgo-rithm with MTL extended input vectors ?
(xi).6For TAP and MTL-TAP, we attempt to resolvethe essay score to a common scale (0-60) andsubsequently train and test using this resolvedscale.
We then convert the score back to theoriginal prompt-specific scale for evaluation.
Thisis the approach used by the work most similar toours (Phandi et al, 2015).
It is worth noting thatthe resolution of scores to a common scale prior totraining is necessary for both TAP and MTL-TAPwhen using data from multiple ASAP prompts.However, this step is not required for MTL-cTAPas this algorithm learns a ranking function wwithout directly comparing essays from differentsets during training.
Furthermore, the final regres-6In the standard learning scenario when only target taskdata is available, MTL-TAP and MTL-cTAP are identical.793Target Task/PromptsSystem 1 2 3 4 5 6 7 8Tgt-TAP 0.830 0.728 0.717 0.842 0.851 0.811 0.790 0.730Src-TAP 0.779 0.663 0.703 0.735 0.789 0.688 0.616 0.625Src-MTL-TAP 0.824?
0.683?
0.728?
0.771?
0.829?
0.699 0.737?
0.575Src-MTL-cTAP 0.826?
0.698???0.729?
0.773??0.827?
0.702??0.744???0.589?
?All-TAP 0.806 0.652 0.702 0.805 0.814 0.802 0.728 0.629All-MTL-TAP 0.831?
0.722?
0.728?
0.823?
0.849?
0.808 0.783?
0.680?All-MTL-cTAP 0.832?
0.731??0.729??0.840???0.852??0.810?
0.802???0.717??
?Table 3: Average Spearman ?
of systems over two-folds on the ASAP dataset.
The best approach perprompt is in bold.
?
(?)
means that ?
is statistically greater than Src-TAP (top half) and All-TAP (bottomhalf) using the Steiger test at the 0.05 level (?
means significant for both folds, ?
means for one of thefolds), while?
?means statistically greater than All-MTL-TAP on both folds (?for one fold).Target Tasks/PromptsSystem 1 2 3 4 5 6 7 8Tgt-TAP 0.813 0.667 0.626 0.779 0.789 0.763 0.758 0.665All-TAP 0.803 0.598 0.583 0.648 0.747 0.741 0.674 0.462All-MTL-TAP 0.825?
0.658?
0.643?
0.702?
0.784?
0.759?
0.778?
0.692?All-MTL-cTAP 0.816?
0.667??0.654???0.783???0.801???0.778???0.787?
?0.692?Table 4: Average QW-?
of systems over two-folds on the ASAP dataset.
The best approach per promptis in bold.
?
(?)
means that ?
is statistically (p < 0.05) greater than All-TAP using an approximaterandomisation test (Yeh, 2000) using 50,000 samples.?
?means statistically greater than All-MTL-TAPon both folds (?for one fold).sion step in cTAP only uses original target taskscores and therefore predicts scores on the correctscoring scale for the task.
We study the threedifferent learning approaches, TAP, MTL-TAP,and MTL-cTAP, in the following scenarios:All: where the approach uses data from boththe target task and the available source tasks.Tgt: where the approach uses data from thetarget task only.Src: where the approach uses data from onlythe available source tasks.6.2 Data FoldsFor our main experiments we divide the essaysassociated with each of the eight tasks into twofolds.
For all subsequent experiments, we train us-ing data in one fold (often associated with multipletasks) and test on data in the remaining fold of thespecific target task.
We report results for each taskseparately.
These splits allow us to perform stud-ies of all three learning approaches (TAP, MTL-TAP, and MTL-cTAP) using varying amounts ofsource and target task training data.6.3 Evaluation MetricsWe use both Spearman?s ?
correlation andQuadratic-weighted ?
(QW-?)
to evaluate the per-formance of all approaches.
Spearman?s ?
mea-sures the quality of the ranking of predicted scoresproduced by the system (i.e.
the output from theranking-preference model).
We calculate Spear-man?s ?
using the ordinal gold score and the real-valued prediction on the original prompt-specificscoring scale of each prompt.
Statistical signifi-cant differences between two correlations sharingone dependent variable (i.e.
the gold scores) canbe determined using Steiger?s (1980) test.QW-?
measures the chance corrected agree-ment between the predicted scores and the goldscores.
QW-?
can be viewed as a measure of ac-curacy as it is lower when the predicted scores arefurther away from the gold scores.
This metricmeasures both the quality of the ranking of scoresand the quality of the linear regression step ofour approach.
These metrics are complementaryas they measure different aspects of performance.We calculate QW-?
using the ordinal gold scoreand the real-valued prediction rounded to the near-est score on the original prompt-specific scale (seeTable 2).7940 0.10.2 0.30.4 0.50.6 0.70.8 0.94  8  16  32  64  128  256  512 1024QW-kappa# of target task training essaysTask/Prompt 1All-TAPTgt-TAPAll-MTL-TAPAll-MTL-cTAP0.15 0.20.25 0.30.35 0.40.45 0.50.55 0.60.65 0.74  8  16  32  64  128  256  512 1024QW-kappa# of target task training essaysTask/Prompt 2All-TAPTgt-TAPAll-MTL-TAPAll-MTL-cTAP0.10.20.30.40.50.60.74  8  16  32  64  128  256  512 1024QW-kappa# of target task training essaysTask/Prompt 3All-TAPTgt-TAPAll-MTL-TAPAll-MTL-cTAP00.10.20.30.40.50.60.70.84  8  16  32  64  128  256  512 1024QW-kappa# of target task training essaysTask/Prompt 4All-TAPTgt-TAPAll-MTL-TAPAll-MTL-cTAP0.10.20.30.40.50.60.70.84  8  16  32  64  128  256  512 1024QW-kappa# of target task training essaysTask/Prompt 5All-TAPTgt-TAPAll-MTL-TAPAll-MTL-cTAP00.10.20.30.40.50.60.70.84  8  16  32  64  128  256  512 1024QW-kappa# of target task training essaysTask/Prompt 6All-TAPTgt-TAPAll-MTL-TAPAll-MTL-cTAP00.10.20.30.40.50.60.70.84  8  16  32  64  128  256  512 1024QW-kappa# of target task training essaysTask/Prompt 7All-TAPTgt-TAPAll-MTL-TAPAll-MTL-cTAP0.10.20.30.40.50.60.72  4  8  16  32  64  128  256  512QW-kappa# of target task training essaysTask/Prompt 8All-TAPTgt-TAPAll-MTL-TAPAll-MTL-cTAPFigure 2: Average QW-?
over two folds for all tasks as size of target task training data increases7 Results and DiscussionTable 3 and Table 4 show the performance of anumber of models for both ?
and ?
respectively.In general, we see that the MTL versions nearlyalways outperform the baseline TAP when usingthe same training data.
This shows that multi-task learning is superior to simply using the sourcetasks as extra training data for the AES task.
Inter-estingly this has not been shown before.
Further-more, the MTL-cTAP approach tends to be sig-nificantly better than the other for many promptsunder varying scenarios for both Spearman?s ?and QW-?.
This shows that models that attemptto directly compare essays scores across certainwriting-tasks lead to poorer performance.When looking at Spearman?s ?
in Table 3 we seethat the models that do not use any target task dataduring training (Src) can achieve a performancewhich is close to the baseline that only uses all ofthe available target data (Tgt-TAP).
This indicatesthat our system can rank essays well without anytarget task data.
However, it is worth noting thatwithout any target task training data and lackingany prior information as to the distribution of goldscores for the target task, achieving a consistentlyhigh accuracy (i.e.
QW-?)
is extremely difficult(if not impossible).
Therefore, Table 4 only showsresults for models that make use of target task data.For the models trained with data from all eighttasks, we can see that All-MTL-cTAP outperformsboth All-TAP and All-MTL-TAP on most of thetasks for both evaluation metrics (?
and ?).
In-terestingly, All-MTL-cTAP also outperforms Tgt-TAP on most of the prompts for both evaluationmetrics.
This indicates that All-MTL-cTAP man-ages to successfully incorporate useful informa-tion from the source tasks even when there is am-ple target-task data.
We next look at scenarioswhen target-task training data is lacking.7.1 Study of Target-Task Training SizeIn real-world scenarios, it is often the case that welack training data for a new writing task.
We nowreport the results of an experiment that uses vary-ing amounts of target-task training data.
In partic-ular, we use all source tasks and initially a smallsample of task-specific data for each task (every128thtarget essay) and measure the performanceof Tgt-TAP and the All-* models.
We then dou-ble the amount of target-task training data used (byusing every 64thessay) and again measure perfor-mance, repeating this process until all target-taskdata is used.
Figure 2 shows the performance ofTgt-TAP and the All-* models as target-task dataincreases.In particular, Figure 2 shows that All-MTL-cTAP consistently outperforms all approaches interms of agreement (QW-?)
and is particularly su-perior when there is very little target-task trainingdata.
It is worth remembering that All-MTL-cTAPonly uses the target-task training instances for thefinal linear regression step.
These results indicatethat because the preference-ranking model per-forms so well, only a few target-task training in-stances are needed for the linear-regression step ofAll-MTL-cTAP.
On the other hand, All-MTL-TAPuses all of the training instances in its final linearregression step, and performs significantly worseon a number of prompts.
Again this shows thestrengths of the constrained multi-task approach.7.2 Study of Number of Source-tasksAll previous experiments that used source taskdata used the entire seven additional tasks.
We7950.50.550.60.650.70.750.80.851 2 3 4 5 6 7 8QW-kappasource data added cumulativelyTask/Prompt 1All-TAPAll-MTL-TAPAll-MTL-cTAP0.2 0.250.3 0.350.4 0.450.5 0.550.6 0.652 3 4 5 6 7 8 1QW-kappasource data added cumulativelyTask/Prompt 2All-TAPAll-MTL-TAPAll-MTL-cTAP0.15 0.20.25 0.30.35 0.40.45 0.50.55 0.60.65 0.73 4 5 6 7 8 1 2QW-kappasource data added cumulativelyTask/Prompt 3All-TAPAll-MTL-TAPAll-MTL-cTAP0.2 0.250.3 0.350.4 0.450.5 0.550.6 0.654 5 6 7 8 1 2 3QW-kappasource data added cumulativelyTask/Prompt 4All-TAPAll-MTL-TAPAll-MTL-cTAP0.25 0.30.35 0.40.45 0.50.55 0.60.65 0.70.75 0.85 6 7 8 1 2 3 4QW-kappasource data added cumulativelyTask/Prompt 5All-TAPAll-MTL-TAPAll-MTL-cTAP0.2 0.250.3 0.350.4 0.450.5 0.550.6 0.656 7 8 1 2 3 4 5QW-kappasource data added cumulativelyTask/Prompt 6All-TAPAll-MTL-TAPAll-MTL-cTAP0.25 0.30.35 0.40.45 0.50.55 0.60.65 0.77 8 1 2 3 4 5 6QW-kappasource data added cumulativelyTask/Prompt 7All-TAPAll-MTL-TAPAll-MTL-cTAP00.10.20.30.40.50.60.78 1 2 3 4 5 6 7QW-kappasource data added cumulativelyTask/Prompt 8All-TAPAll-MTL-TAPAll-MTL-cTAPFigure 3: Average QW-?
over two folds as number of source tasks increases (using 25 target task in-stances)now study the performance of the approaches asthe number of source tasks changes.
In particu-lar, we limit the number of target task training in-stances to 25 and cumulatively add entire sourcetask data in the order in which they occur in Ta-ble 2, starting with the source task appearing di-rectly after the target task.
We then measure per-formance at each stage.
At the end of the process,each approach has access to all source tasks andthe limited target task data.Figure 3 shows the QW-?
for each prompt as thenumber of source tasks increases.
We can see thatAll-TAP is the worst performing approach and of-ten decreases as certain tasks are added as trainingdata.
All-MTL-cTAP is the best performing ap-proach for nearly all prompts.
Furthermore, All-MTL-cTAP is more robust than other approaches,as it rarely decreases in performance as the num-ber of tasks increases.8 Qualitative AnalysisAs an indication of the type of interpretable in-formation contained in the task-specific repre-sentations of the All-MTL-cTAP model, we ex-amined the shared representation and two task-specific representations that relate to the exampletasks outlined in Table 1.
Table 5 shows the topweighted lexical features (i.e.
unigrams, bigrams,or trigrams) (and their respective weights) in dif-ferent parts of the All-MTL-cTAP model.In general, we can see that the task-specific lex-ical components of the model capture topical as-pects of the tasks and enable domain adaptation tooccur.
For example, we can see that books, materi-als, and censorship are highly discriminative lexi-cal features for ranking essays written in responseto task #2.
The shared representation containshighly weighted lexical features across all tasksand captures vocabulary items useful for rankingin general.
While this analysis gives us some in-sight into our model, it is more difficult to interpretthe weights of other feature types (e.g.
POS, GRs)across different parts of the model.
We leave fur-ther analysis of our approach to future work.9 Discussion and ConclusionUnlike previous work (Phandi et al, 2015) wehave shown, for the first time, that MTL outper-forms an approach of simply using source taskdata as extra training data.
This is because our ap-proach uses information from multiple tasks with-out directly relying on the comparability of goldscores across tasks.
Furthermore, it was concludedin previous work that at least some target-tasktraining data is necessary to build high perform-ing AES systems.
However, as seen in Table 3,high performance rankers (?)
can be built with-out any target-task data.
Nevertheless, it is worthnoting that without any target-data, accurately pre-dicting the actual score (high ?)
is extremely dif-ficult.
Therefore, although some extra informa-tion (i.e.
the expected distribution of gold scores)would need to be used to produce accurate scoreswith a high quality ranker, the ranking is still use-ful for assessment in a number of scenarios (e.g.grading on a curve where the distribution of stu-dent scores is predefined).The main approach adopted in this paper isquite similar to using SVMrank(Joachims, 2002)while encoding the prompt id as the qid.
Whencombined with a multi-task learning technique thisallows the preference-ranking algorithm to learn796Shared Task #1 Task #22.024 offensive 1.146 this 2.027 offensive1.852 hydrogen 0.985 less 1.229 books1.641 hibiscus 0.980 computers 0.764 do n?t1.602 shows 0.673 very 0.720 materials1.357 strong 0.661 would 0.680 censorship1.326 problem 0.647 could 0.679 person1.288 grateful 0.624 , and 0.676 read1.286 dirigibles 0.599 family 0.666 children1.234 books 0.599 less time 0.661 offensive .1.216 her new 0.579 spend 0.659 those... ... ... ... ... ...1.068 urban areas 0.343 benefit our society 0.480 should be able1.007 airships 0.341 believe that computers 0.475 able toTable 5: Highest weighted lexical features (i.e.
unigrams, bigrams, or trigrams) and their weights inboth shared and task-specific representations of the All-MTL-cTAP model (associated with results inTable 4) for the two example tasks referred to in Table 1.both task-specific and shared-representations in atheoretically sound manner (i.e.
without makingany speculative assumptions about the relative or-derings of essays that were graded on differentscales using different marking criteria), and is gen-eral enough to be used in many situations.Ultimately these complementary techniques(multi-task learning and constrained pairwisepreference-ranking) allow essay scoring data fromany source to be included during training.
Asshown in Section 7.2, our approach is robust toincreases in the number of tasks, meaning thatone can freely add extra data when available andexpect the approach to use this data appropri-ately.
This constrained multi-task preference-ranking approach is likely to be useful for manyapplications of multi-task learning, when the gold-scores across tasks are not directly comparable.Future work will aim to study different dimen-sions of the prompt (e.g.
genre, topic) using multi-task learning at a finer level.
We also aim to furtherstudy the characteristics of the multi-task modelin order to determine which features transfer wellacross tasks.
Another avenue of potential researchis to use multi-task learning to predict scores fordifferent aspects of text quality (e.g.
coherence,grammaticality, topicality).AcknowledgementsWe would like to thank Cambridge English Lan-guage Assessment for supporting this research,and the anonymous reviewers for their useful feed-back.
We would also like to thank EkaterinaKochmar, Helen Yannakoudakis, Marek Rei, andTamara Polajnar for feedback on early drafts ofthis paper.References?istein E Andersen, Helen Yannakoudakis, FionaBarker, and Tim Parish.
2013.
Developing and test-ing a self-assessment and tutoring system.
In Pro-ceedings of the Eighth Workshop on Innovative Useof NLP for Building Educational Applications, BEA,pages 32?41.Yigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-raterR?
v. 2.
The Journal of Technol-ogy, Learning and Assessment, 4(3).Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Pro-ceedings of the COLING/ACL 2006 Interactive Pre-sentation Sessions, pages 77?80, Sydney, Australia,July.
Association for Computational Linguistics.Ted Briscoe, Ben Medlock, and ?istein Andersen.2010.
Automated assessment of esol free text ex-aminations.
Technical Report 790, The ComputerLab, University of Cambridge, February.Olivier Chapelle and S Sathiya Keerthi.
2010.
Effi-cient algorithms for ranking with svms.
InformationRetrieval, 13(3):201?215.Hongbo Chen and Ben He.
2013.
Automated es-say scoring by maximizing human-machine agree-ment.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, pages 1741?1752, Seattle, Washington, USA,October.
Association for Computational Linguistics.Hao Cheng, Hao Fang, and Mari Ostendorf.
2015.Open-domain name error detection using a multi-task RNN.
In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Pro-cessing, EMNLP 2015, Lisbon, Portugal, September17-21, 2015, pages 737?746.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th international conference onMachine learning, pages 160?167.
ACM.797Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerenglish: The nus corpus of learner english.
InProceedings of the Eighth Workshop on InnovativeUse of NLP for Building Educational Applications,pages 22?31, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Hal Daume III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages256?263, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, HidekiIsozaki, and Masaaki Nagata, 2010.
Proceed-ings of the Joint Fifth Workshop on Statistical Ma-chine Translation and MetricsMATR, chapter N-Best Reranking by Multitask Learning, pages 375?383.
Association for Computational Linguistics.Mariano Felice, Zheng Yuan, ?istein E. Andersen, He-len Yannakoudakis, and Ekaterina Kochmar.
2014.Grammatical error correction using hybrid systemsand type filtering.
In Proceedings of the Eigh-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, CoNLL 2014, Balti-more, Maryland, USA, June 26-27, 2014, pages 15?24.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukwac, a very large web-derived corpus of english.In Proceedings of the 4th Web as Corpus Workshop(WAC-4) Can we beat Google, pages 47?54.Michael Heilman and Nitin Madnani.
2013.
Ets: do-main adaptation and stacking for short answer scor-ing.
In Proceedings of the 2nd joint conferenceon lexical and computational semantics, volume 2,pages 275?279.Derrick Higgins, Jill Burstein, Daniel Marcu, and Clau-dia Gentile.
2004.
Evaluating multiple aspects ofcoherence in student essays.
In HLT-NAACL, pages185?192.D.
Higgins, J. Burstein, and Y. Attali.
2006.
Identi-fying off-topic student essays without topic-specifictraining data.
Natural Language Engineering,12(2):145?159.Thorsten Joachims.
2002.
Optimizing search en-gines using clickthrough data.
In Proceedings of theeighth ACM SIGKDD international conference onKnowledge discovery and data mining, pages 133?142.
ACM.N Van Labeke, D Whitelock, D Field, S Pulman, andJTE Richardson.
2013.
Openessayist: extractivesummarisation and formative assessment of free-text essays.
In Proceedings of the 1st InternationalWorkshop on Discourse-Centric Learning Analytics,Leuven, Belgium, April.Thomas K Landauer, Peter W Foltz, and Darrell La-ham.
1998.
An introduction to latent semantic anal-ysis.
Discourse processes, 25(2-3):259?284.Leah S Larkey.
1998.
Automatic essay grading usingtext categorization techniques.
In Proceedings of the21st annual international ACM SIGIR conference onResearch and development in information retrieval,pages 90?95.
ACM.Courtney Napoles and Chris Callison-Burch.
2015.Automatically scoring freshman writing: A prelim-inary investigation.
In Proceedings of the TenthWorkshop on Innovative Use of NLP for BuildingEducational Applications, pages 254?263, Denver,Colorado, June.
Association for Computational Lin-guistics.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, ChristianHadiwinoto, Raymond Hendy Susanto, and Christo-pher Bryant.
2014.
The conll-2014 shared task ongrammatical error correction.
In In Proceedings ofthe Seventeenth Conference on Computational Natu-ral Language Learning: Shared Task (CoNLL-2013Shared Task).
Association for Computational Lin-guistics.Diane Nicholls.
2003.
The cambridge learner corpus:Error coding and analysis for lexicography and elt.In Proceedings of the Corpus Linguistics 2003 con-ference, volume 16, pages 572?581.Ellis B Page.
1966.
The imminence of grading essaysby computer.
Phi Delta Kappan, 47:238?243.Ellis Batten Page.
1994.
Computer grading of studentprose, using modern concepts and software.
TheJournal of experimental education, 62(2):127?142.Ellis Batten Page.
2003.
Project essay grade: Peg.Automated essay scoring: A cross-disciplinary per-spective, pages 43?54.Isaac Persing and Vincent Ng.
2014.
Modeling promptadherence in student essays.
In Proceedings of the52nd Annual Meeting of the Association for Com-putational Linguistics, pages 1534?1543, Baltimore,Maryland, June.
ACL.Isaac Persing and Vincent Ng.
2015.
Modeling ar-gument strength in student essays.
In Proceedingsof the 53rd Annual Meeting of the Association forComputational Linguistics and the 7th InternationalJoint Conference on Natural Language Processing(Volume 1: Long Papers), pages 543?552.Peter Phandi, Kian Ming A. Chai, and Hwee Tou Ng.2015.
Flexible domain adaptation for automated es-say scoring using correlated linear regression.
InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages431?439, Lisbon, Portugal, September.
Associationfor Computational Linguistics.798Alla Rozovskaya and Dan Roth.
2011.
Algorithm se-lection and model adaptation for esl correction tasks.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies - Volume 1, HLT ?11, pages924?933, Stroudsburg, PA, USA.
Association forComputational Linguistics.Lawrence M Rudner and Tahung Liang.
2002.
Au-tomated essay scoring using bayes?
theorem.
TheJournal of Technology, Learning and Assessment,1(2).Swapna Somasundaran, Jill Burstein, and MartinChodorow.
2014.
Lexical chaining for measur-ing discourse coherence quality in test-taker essays.In Proceedings of COLING 2014, the 25th Inter-national Conference on Computational Linguistics:Technical Papers, pages 950?961, Dublin, Ireland,August.
Dublin City University and Association forComputational Linguistics.Karen Sparck Jones.
1972.
A statistical interpretationof term specificity and its application in retrieval.Journal of documentation, 28(1):11?21.James H Steiger.
1980.
Tests for comparing ele-ments of a correlation matrix.
Psychological bul-letin, 87(2):245.Helen Yannakoudakis and Ted Briscoe.
2012.
Model-ing coherence in esol learner texts.
In Proceedingsof the Seventh Workshop on Building EducationalApplications Using NLP, pages 33?43, Montr?eal,Canada, June.
Association for Computational Lin-guistics.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading esol texts.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages180?189, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Pro-ceedings of the 18th Conference on ComputationalLinguistics - Volume 2, COLING ?00, pages 947?953, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.799
