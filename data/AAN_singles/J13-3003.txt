Measuring Word Meaning in ContextKatrin Erk?University of Texas at AustinDiana McCarthy?
?University of CambridgeNicholas Gaylord?University of Texas at AustinWord sense disambiguation (WSD) is an old and important task in computational linguisticsthat still remains challenging, to machines as well as to human annotators.
Recently there havebeen several proposals for representing word meaning in context that diverge from the traditionaluse of a single best sense for each occurrence.
They represent word meaning in context throughmultiple paraphrases, as points in vector space, or as distributions over latent senses.
Newmethods of evaluating and comparing these different representations are needed.In this paper we propose two novel annotation schemes that characterize word meaning incontext in a graded fashion.
In WSsim annotation, the applicability of each dictionary senseis rated on an ordinal scale.
Usim annotation directly rates the similarity of pairs of usages ofthe same lemma, again on a scale.
We find that the novel annotation schemes show good inter-annotator agreement, as well as a strong correlation with traditional single-sense annotation andwith annotation of multiple lexical paraphrases.
Annotators make use of the whole ordinal scale,and give very fine-grained judgments that ?mix and match?
senses for each individual usage.We also find that the Usim ratings obey the triangle inequality, justifying models that treat usagesimilarity as metric.There has recently been much work on grouping senses into coarse-grained groups.
Wedemonstrate that graded WSsim and Usim ratings can be used to analyze existing coarse-grainedsense groupings to identify sense groups that may not match intuitions of untrained nativespeakers.
In the course of the comparison, we also show that the WSsim ratings are not subsumedby any static sense grouping.?
Linguistics Department.
CLA Liberal Arts Building, 305 E. 23rd St. B5100, Austin, TX, USA 78712.E-mail: katrin.erk@mail.utexas.edu, nlgaylord@utexas.edu.??
Visiting Scholar, Department of Theoretical and Applied Linguistics, University of Cambridge,Sidgwick Avenue, Cambridge, CB3 9DA, UK.
E-mail: diana@dianamccarthy.co.uk.Submission received: 3 November 2011; revised version received: 30 April 2012; accepted for publication:25 June 2012.doi:10.1162/COLI a 000142?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 31.
IntroductionWord sense disambiguation (WSD) is a task that has attracted much work in computa-tional linguistics (see Agirre and Edmonds [2007] and Navigli [2009] for an overview),including a series of workshops, SENSEVAL (Kilgarriff and Palmer 2000; Preiss andYarowsky 2001; Mihalcea and Edmonds 2004) and SemEval (Agirre, Ma`rquez, andWicentowski 2007; Erk and Strapparava 2010), which were originally organizedexpressly as a forum for shared tasks in WSD.
In WSD, polysemy is typically modeledthrough a dictionary, where the senses of a word are understood to be mutually disjoint.The meaning of an occurrence of a word is then characterized through the best-fittingamong its dictionary senses.The assumption of senses that are mutually disjoint and that have clear bound-aries has been drawn into doubt by lexicographers (Kilgarriff 1997; Hanks 2000), lin-guists (Tuggy 1993; Cruse 1995), and psychologists (Kintsch 2007).
Hanks (2000) arguesthat word senses have uses where they clearly fit, and borderline uses where only afew of a sense?s identifying features apply.
This notion matches results in psychol-ogy on human concept representation: Mental categories show ?fuzzy boundaries,?and category members differ in typicality and degree of membership (Rosch 1975;Rosch and Mervis 1975; Hampton 2007).
This raises the question of annotation: Is itpossible to collect word meaning annotation that captures degrees to which a senseapplies?Recently, there have been several proposals for modeling word meaning in contextthat can represent different degrees of similarity to a word sense, as well as differentdegrees of similarity between occurrences of a word.
The SemEval Lexical Substitu-tion task (McCarthy and Navigli 2009) represents each occurrence through multipleweighted paraphrases.
Other approaches represent meaning in context through a vectorspace model (Erk and Pado 2008; Mitchell and Lapata 2008; Thater, Fu?rstenau, andPinkal 2010) or through a distribution over latent senses (Dinu and Lapata 2010).
Again,this raises the question of annotation: Can human annotators give fine-grained judg-ments about degrees of similarity between word occurrences, like these computationalmodels predict?The question that we explore in this paper is: Can word meaning be describedthrough annotation in the form of graded judgments?
We want to know whether an-notators can provide graded meaning annotation in a consistent fashion.
Also, wewant to know whether annotators will use the whole graded scale, or whetherthey will fall back on binary ratings of either ?identical?
or ?different.?
Our ques-tion, however, is not whether annotators can be trained to do this.
Rather, ouraim is to describe word meaning as language users perceive it.
We want to tap intothe annotators?
intuitive notions of word meaning.
As a consequence, we use un-trained annotators.
We view it as an important aim on its own to capture lan-guage users?
intuitions on word meaning, but it is also instrumental in answeringour first question, of whether word meaning can be described through gradedannotator judgments: Training annotators in depth on how to distinguish pre-defined hand-crafted senses could influence them to assign those senses in a binaryfashion.We introduce two novel annotation tasks in which human annotators characterizeword meaning in context.
In the first task, they rate the applicability of dictionarysenses on a graded scale.
In the second task, they rate the similarity between pairs ofusages of the same word, also on a graded scale.
In designing the annotation tasks, weutilize techniques from psycholinguistic experimentation: Annotators give ratings on a512Erk, McCarthy, and Gaylord Measuring Word Meaning in Contextscale, rather than selecting a single label; we also use multiple annotators for each item,retaining all annotator judgments.1The result of this graded annotation can then be used to evaluate computationalmodels of word meaning: either to evaluate graded models of word meaning, or toevaluate traditional WSD systems in a graded fashion.
They can also be used to ana-lyze existing word sense inventories, in particular to identify sense distinctions worthrevisiting?we say more on this latter use subsequently.Our aim is not to improve inter-annotator agreement over traditional sense an-notation.
It is highly unlikely that ratings on a scale would ever achieve higher exactagreement than binary annotation.
Our aim is also not to maximize exact agreement, aswe expect to see individual differences in perceived meaning, and want to capture thosedifferences.
Still it is desirable to have an end product of the annotation that is robustagainst such individual differences.
In order to achieve this, we average judgments overmultiple annotators after first inspecting pairwise correlations between annotators toensure that they are all doing their work diligently and with similar outcomes.Analyzing the annotation results, we find that the annotators make use of inter-mediate points on the graded scale and do not treat the task as inherently binary.
Wefind that there is good inter-annotator agreement, measured as correlation.
There isalso a highly significant correlation across tasks and with traditional WSD and lexicalsubstitution tasks.
This indicates that the annotators performed these tasks in a con-sistent fashion.
It also indicates that diverse ways of representing word meaning incontext?single best sense, weighted senses, multiple paraphrases, usage similarity?yield similar characterizations.
We find that annotators frequently give high scores tomore than one sense, in a way that is not remedied by a more coarse-grained senseinventory.
In fact, the annotations are often inconsistent with disjoint sense partitions.The work reported here is based on our earlier work reported in Erk, McCarthy, andGaylord (2009).
The current paper extends the previous work in three ways.1.
We add extensive new annotation to corroborate our findings fromthe previous, smaller study.
In this new, second round of annotation,annotators do the two graded ratings tasks as well as traditionalsingle-sense annotation and annotation with paraphrases (lexicalsubstitutes), all on the same data.
Each item is rated by eight annotatorsin parallel.
This setting, with four different types of word meaningannotation on the same data, allows us to compare annotation resultsacross tasks more directly than before.22.
We test whether the similarity ratings on pairs of usages obey the triangleinequality, and find that they do.
This point is interesting for psychologicalreasons.
Tversky and Gati (Tversky 1977; Tversky and Gati 1982) foundthat similarity ratings on words did not obey the triangle inequality?although, unlike our study, they were dealing with words out of context.The fact that usage similarity ratings obey the triangle inequality is alsoimportant for modeling and annotation purposes.1 We do not use as many raters per item as is usual in psycholinguistics, however, as our aim is to cover asizeable amount of corpus data.2 The annotation data from this second round are available at http://www.dianamccarthy.co.uk/downloads/WordMeaningAnno2012/.513Computational Linguistics Volume 39, Number 33.
We examine the extent to which our graded annotation accords with twoexisting coarse-grained sense groupings, and we demonstrate that ourgraded annotations can be used to double-check on sense groupings andfind potentially problematic groupings.2.
BackgroundIn this section, we offer an overview of previous word sense annotation efforts, and thendiscuss alternative approaches to the annotation and modeling of word meaning.2.1 Word Sense AnnotationInter-annotator agreement (also called inter-tagger agreement, or ITA) is one indicatorof the difficulty of the task of manually assigning word senses (Krishnamurthy andNicholls 2000).
With WordNet, the sense inventory currently most widely used inword sense annotation, ITA ranges from 67% to 78% (Landes, Leacock, and Tengi 1998;Mihalcea, Chklovski, and Kilgarriff 2004; Snyder and Palmer 2004), depending onfactors such as degree of polysemy and inter-relatedness of the senses.
This issue isnot specific to WordNet.
Annotation efforts based on other dictionaries have achievedsimilar ITA levels, as shown in Table 1.
The first group in that table shows two corporain which all open-class words are annotated for word sense, in both cases usingWordNet.
The second group consists of two English lexical sample corpora, in whichonly some target words are annotated.
One of them uses WordSmyth senses for verbsand WordNet for all other parts of speech, and the other uses HECTOR, with similarITA, so the choice of dictionary does not seem to make much difference in this case.3Next is SALSA, a German corpus using FrameNet frames as senses, then OntoNotes,again an English lexical sample corpus.
Inter-annotator agreement is listed in the lastcolumn of the table; agreement is in general relatively low for the first four corpora,which use fine-grained sense distinctions, and higher for SALSA and OntoNotes, whichhave more coarse-grained senses.Sense granularity has a clear impact upon levels of inter-annotator agreement(Palmer, Dang, and Fellbaum 2007).
ITA is substantially improved by using coarser-grained senses, as seen in OntoNotes (Hovy et al2006), which uses an ITA of 90% as thecriterion for constructing coarse-grained sense distinctions.
Although this strategy doesimprove ITA, it does not eliminate the issues seen with more fine-grained annotationefforts: For some lemmas, such as leave, 90% ITA is not reached even after multiplere-partitionings of the semantic space (Chen and Palmer 2009).
This suggests that themeanings of at least some words may not be separable into senses distinct enoughfor consistent annotation.4 Moreover, sense granularity does not appear to be the onlyquestion influencing ITA differences between lemmas.
Passonneau et al(2010) foundthree main factors: sense concreteness, specificity of the context in which the target wordoccurs, and similarity between senses.
It is worth noting that of these factors, only thethird can be directly addressed by a change in the dictionary.3 HECTOR senses are described in richer detail than WordNet senses and the resource is stronglycorpus-based.
We use WordNet in our work due to its high popularity and free availability.4 Examples such as this indicate that there is at times a problem with clearly defining consistentlyseparable senses of a word.
There is no clear measure of exactly how frequent such cases are, however.This is due in part to the fact that this question depends so heavily on the data being considered and thedistinctions being posited.514Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextTable 1Word sense-annotated data, with inter-annotator agreement (ITA).Corpus Dictionary Corpus reference ITASemCor WordNet Landes, Leacock, and Tengi(1998)78.6%SensEval-3 WordNet Snyder and Palmer (2004) 72.5%SensEval-1 lex.
sample HECTOR Kilgarriff and Rosenzweig(2000)66.5%SensEval-3 lex.
sample WordNet, WordSmyth Mihalcea, Chklovski, andKilgarriff (2004)67.3%SALSA FrameNet Burchardt et al(2006) 86%OntoNotes OntoNotes Hovy et al(2006) most > 90%Table 2Best word sense disambiguation performance in SensEval/SemEval English lexical sampletasks.Shared task Shared task overview Best precision BaselineSensEval-1 Kilgarriff and Rosenzweig (2000) 77% 69%SensEval-2 Senseval-2 (2001) 64% 51%SensEval-3 Mihalcea, Chklovski, and Kilgarriff (2004) 73% 55%SemEval-1 Pradhan et al(2007) 89% (not given)ITA levels in word sense annotation tasks are mirrored in the performance of WSDsystems trained on the annotated data.
Table 2 shows results for the best systems thatparticipated at four English lexical sample tasks.
With fine-grained sense inventories,the top-ranking WSD systems participating in the event achieved precision scores of 73%to 77% (Edmonds and Cotton 2001; Mihalcea, Chklovski, and Kilgarriff 2004).
Currentstate-of-the-art systems have made modest improvements on this; for example, thesystem described by Zhong and Ng (2010) achieves 65.3% on the English lexical sampleat SENSEVAL-2, though the same system obtains 72.6%, just below Mihalcea, Chklovski,and Kilgarriff (2004), on the English lexical sample at SENSEVAL-3.
Nevertheless, the pic-ture remains the same with systems getting around three out of four word occurrencescorrect.
Under a coarse-grained approach, system performance improves considerably(Palmer, Dang, and Fellbaum 2007; Pradhan et al2007), with the best participatingsystem achieving a precision close to 90%.5 The merits of a coarser-grained approachare still a matter of debate (Stokoe 2005; Ide and Wilks 2006; Navigli, Litkowski, andHargraves 2007; Brown 2010), however.Although identifying the proper level of granularity for sense repositories has im-portant implications for improving WSD, we do not focus on this question here.
Rather,we propose novel annotation tasks that allow us to probe the relatedness betweendictionary senses in a flexible fashion, and to explore word meaning in context withoutpresupposing hard boundaries between usages.
The resulting data sets can be usedto compare different inventories, coarse or otherwise.
In addition, we hope that theywill prove useful for the evaluation of alternative representations of ambiguity in word5 Zhong, Ng, and Chan (2008) report similar results (89.1%) with their state-of-the-art system whenevaluating on the OntoNotes corpus, which is larger than the SENSEVAL data sets.515Computational Linguistics Volume 39, Number 3meaning (Erk and Pado 2008; Mitchell and Lapata 2008; Reisinger and Mooney 2010;Thater, Fu?rstenau, and Pinkal 2010; Reddy et al2011; Van de Cruys, Poibeau, andKorhonen 2011).2.2 Representation of Word Meaning in Word Sense InventoriesOne possible factor contributing to the difficulty of manual and automatic word senseassignment is the design of word sense inventories themselves.
As we have seen, suchdifficulties are encountered across dictionaries, and it has been argued that there areproblems with the characterization of word meanings as sets of discrete and mutuallyexclusive senses (Tuggy 1993; Cruse 1995; Kilgarriff 1997; Hanks 2000; Kintsch 2007).2.2.1 Criticisms of Enumerative Approaches to Meaning.
Dictionaries are practical resourcesand the nature of the finished product depends upon the needs of the target audience, aswell as budgetary and related constraints (cf.
Hanks 2000).
Consequently, dictionariesdiffer in the words that they cover, and also in the word meanings that they distinguish.Dictionary senses are generalizations over the meanings that a word can take, and thesegeneralizations themselves are abstractions over collected occurrences of the word indifferent contexts (Kilgarriff 1992, 1997, 2006).
Regardless of a dictionary?s granularity,the possibility exists for some amount of detail to be lost as a result of this process.Kilgarriff (1997) calls into question the possibility of general, all-purpose senses ofa word and argues that sense distinction only makes sense with respect to a given task.For example, in machine translation, the senses to be distinguished should be thosethat lead to different translations in the target language.
It has since been demonstratedthat this is in fact the case (Carpuat and Wu 2007a, 2007b).
Hanks (2000) questions theview of senses as disjoint classes defined by necessary and sufficient conditions.
Heshows that even with a classic homonym like ?bank,?
some occurrences are more typicalexamples of a particular sense than others.
This notion of typicality is also important intheories of concept representation in psychology (Murphy 2002).
Theoretical treatmentsof word meaning such as the Generative Lexicon (Pustejovsky 1991) also draw attentionto the subtle, yet reliable, fluctuations of meaning-in-context, and work in this paradigmalso provides evidence that two senses which may appear to be quite distinct can infact be quite difficult to distinguish in certain contexts (Copestake and Briscoe 1995,page 53).2.2.2 Psychological Research on Lexical and Conceptual Knowledge.
Not all members of amental category are equal.
Some are perceived as more typical than others (Rosch 1975;Rosch and Mervis 1975; and many others), and even category membership itself isclearer in some cases than in others (Hampton 1979).
These results are about mentalconcepts, however, rather than word meanings per se, which raises the question ofthe relation between word meanings and conceptual knowledge.
Murphy (1991, 2002)argues that although not every concept is associated with a word, word meanings showmany of the same phenomena as concepts in general?word meaning is ?made up ofpieces of conceptual structure?
(Murphy 2002, page 391).
A body of work in cognitivelinguistics also discusses the relation between word meaning and conceptual structure(Coleman and Kay 1981; Taylor 2003).Psycholinguistic studies on word meaning offer insight into the question of themental representation of word senses.
Unlike homonym meanings, the senses of apolysemous word are thought to be related, suggesting that the mental representationsof these senses may overlap as well.
The psycholinguistic literature on this question516Erk, McCarthy, and Gaylord Measuring Word Meaning in Contextis not wholly clear-cut, but by and large does support the position that polysemoussenses are not entirely discrete in the mental lexicon.
Whereas Klein and Murphy (2001,2002) do provide evidence for discreteness of mental sense representations, it appearsas though these findings may be due in part to the particular senses included in theirstudies (Klepousniotou, Titone, and Romero 2008).Moreover, many psycholinguistic studies have indeed found evidence for process-ing differences between homonyms and polysemous words, using a variety of experi-mental designs, including eye movements and reading times (Frazier and Rayner 1990;Pickering and Frisson 2001) as well as response times in sensicality and lexical decisiontasks (Williams 1992; Klepousniotou 2002).
Brown (2008, 2010) takes the question ofshared vs. separate meaning representations one step further in a semantic primingstudy6 in which she shows that intuitive meaning-in-context similarity judgments havea processing correlate in on-line sentence comprehension.
Response time to the targetis a negative linear function of its similarity in meaning to the prime, and responseaccuracy is a positive linear function of this similarity.
In other words, the more similarin meaning a prime?target pair was judged to be, the faster and more accurately sub-jects responded.
This provides empirical support for a processing correlate of gradedsimilarity-in-meaning judgments.In our work reported here, we take inspiration from work in psychology and lookat ways to model word meaning more continuously.
Even though there is still somecontroversy, the majority of studies support the view that senses of polysemous wordsare linked in their mental representations.
In our work we do not make an explicitdistinction between homonymy and polysemy, but the data sets we have produced maybe useful for a future exploration of this distinction.2.3 Alternative Approaches to Word MeaningEarlier we suggested that word meaning may be better described without positingdisjoint senses.
We now describe some alternatives to word sense inventory approachesto word meaning, most of which do not rely on disjoint senses.2.3.1 Substitution-Based Approaches.
McCarthy and Navigli (2007) explore the use ofsynonym or near-synonym lexical substitutions to characterize the meaning of wordoccurrences.
In contrast to dictionary senses, substitutes are not taken to partitiona word?s meaning into distinct senses.
McCarthy and Navigli gathered their lexicalsubstitution data using multiple annotators.
Annotators were allowed to provide up tothree paraphrases for each item.
Data were gathered for 10 sentences per lemma for 210lemmas, spanning verbs, nouns, adjectives, and adverbs.
The annotation took the formof each occurrence being associated with a multiset of supplied paraphrases, weightedby the frequency with which each paraphrase was supplied.
We make extensive use ofthe LEXSUB dataset in our work reported here.
An example sentence with substitutesfrom the LEXSUB dataset (sentence 451) is given in Table 3.A related approach also characterizes meaning through equivalent terms, but termsin another language.
Resnik and Yarowsky (2000, page 10) suggest ?to restrict a wordsense inventory to distinctions that are typically lexicalized cross-linguistically?
[emphasisin original].
They argue that such an approach will avoid being too fine-grained, andthat the distinctions that are made will be independently motivated by crosslinguistic6 See McNamara (2005) for more information on priming studies.517Computational Linguistics Volume 39, Number 3Table 3An example of annotation from the lexical substitution data set: sentence 451.Sentence: My interest in Europe?s defence policy is nothing new.Annotation: original 2; recent 2; novel 2; different 1; additional 1trends.
Although substitution and translation methods are not without their own issues(Kilgarriff 1992, page 48), they constitute an approach to word meaning that avoidsmany of the drawbacks of more traditional sense distinction and annotation.
Somecross-linguistic approaches group translations into disjoint senses (Lefever and Hoste2010), whereas others do not (Mihalcea, Sinha, and McCarthy 2010).2.3.2 Distributional Approaches.
Recently there have been a growing number of distri-butional approaches to representing word meaning in context.
These models offer anopportunity to model subtle distinctions in meaning between two occurrences of a wordin different contexts.
In particular, they allow comparisons between two occurrences ofa word without having to classify them as having the same sense or different senses.Some of these approaches compute a distributional representation for a word across allits meanings, and then adapt this to a given sentence context (Landauer and Dumais1997; Erk and Pado 2008; Mitchell and Lapata 2008; Thater, Fu?rstenau, and Pinkal 2010;Van de Cruys, Poibeau, and Korhonen 2011).
Others group distributional contexts intosenses.
This can be done on the fly for a given occurrence (Erk and Pado 2010; Reddyet al2011), or beforehand (Dinu and Lapata 2010; Reisinger and Mooney 2010).
Thelatter two approaches then represent an occurrence through weights over those senses.A third group of approaches is based on language models (Deschacht and Moens 2009;Washtell 2010; Moon and Erk 2012): They infer other words that could be used in theposition of the target word.73.
Two Novel Annotation TasksIn this section we introduce two novel annotation schemes that draw on methodscommon in psycholinguistic experiments, but uncommon in corpus annotation.
Tra-ditional word sense annotation usually assumes that there is a single correct labelfor each markable.
Annotators are trained to identify the correct labels consistently,often with highly specific a priori guidelines.
Multiple annotators are often used, butdespite the frequently low ITA in word sense annotation, differences between annotatorresponses are often treated as the result of annotator error and are not retained in thefinal annotation data.In these respects, traditional word sense annotation tasks differ in design frommany psycholinguistic experiments, such as the ones discussed in the previous section.Psycholinguistic experiments frequently do not make strong assumptions about howparticipants will respond, and in fact are designed to gather data on that very ques-tion.
Participants are given general guidelines for completing the experiment but these7 Distributional models for phrases have recently received much attention, even more so than models forword meaning in context (Baroni and Zamparelli 2010; Coecke, Sadrzadeh, and Clark 2010; Mitchell andLapata 2010; Grefenstette and Sadrzadeh 2011; Socher et al2011).
They are less directly relevant to thecurrent paper, however, as we focus on eliciting judgments for individual words in sentence contexts,rather than whole phrases.518Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextTable 4Interpretation of the five-point scale given to the annotators.
This interpretation is the same forthe Usim and WSsim tasks.1 completely different2 mostly different3 similar4 very similar5 identicalguidelines generally stop short of precise procedural detail, to avoid undue influenceover participant responses.
All of the psycholinguistic studies discussed earlier usedparticipants na?
?ve as to the purpose of the experiment, and who were minimally trained.Responses are often graded in nature, involving ratings on an ordinal scale or in somecases even a continuously valued dimension (e.g., as in Magnitude Estimation).
Mul-tiple participants respond to each stimulus, but all participant responses are typicallyretained, as there are often meaningful discrepancies in participant responses that arenot ascribable to error.
All of the psycholinguistic studies discussed previously collecteddata from multiple participants (up to 80 in the case of one experiment by Williams[1992]).The annotation tasks we present subsequently draw upon these principles of exper-imental design.
We collected responses using a scale, rather than binary judgments; wedesigned the annotation tasks to be accomplishable without prior training and withminimal guidelines, and we used multiple annotators (up to eight) and retained allresponses in an effort to capture individual differences.
In the following, we describetwo different annotation tasks, one with and one without the use of dictionary senses.Graded Ratings for Dictionary Senses.
In our first annotation task, dubbed WSsim (forWord Sense Similarity), annotators rated the applicability of WordNet dictionary senses,using a five-point ordinal scale.8 Annotators rated the applicability of every singleWordNet sense for the target lemma, where a rating of 1 indicated that the sensein question did not apply at all, and a rating of 5 indicated that the sense appliedcompletely to that occurrence of the lemma.
Table 4 shows the descriptions of the fivepoints on the scale that the annotators were given.
By asking annotators to provideratings for each individual sense, we strive to eliminate all bias toward either single-sense or multiple-sense annotation.
By asking annotators to provide ratings on a scale,we allow for the fact that senses may not be perceived in a binary fashion.Graded Ratings for Usage Similarity.
In our second annotation task, dubbed Usim (forUsage Similarity), we collected annotations of word usages without recourse to dic-tionary senses, by asking annotators to judge the similarity in meaning of one usageof a lemma to other usages.
Annotators were presented with pairs of contexts thatshare a word in common, and were asked to rate how similar in meaning they perceivethose two occurrences to be.
Ratings are again on a five-point ordinal scale; a rating of1 indicated that the two occurrences of the target lemma were completely dissimilar inmeaning, and a rating of 5 indicated that the two occurrences of the target lemma wereidentical in meaning.
The descriptions of the five points on the scale, shown in Table 4,8 The use of a five-point scale is a common choice when collecting ordinal ratings, as it allows moredetailed responses than the ?yes/no/maybe?
provided by a three-point scale.519Computational Linguistics Volume 39, Number 3were identical to those used in the WSsim task.
Annotators were able to respond ?I don?tknow?
if they were unable to gauge the similarity in meaning of the two occurrences.9Annotation Procedure.
All annotation for this project was conducted over the Internetin specially designed interfaces.
In both tasks, all annotator responses were retained,without resolution of disagreement between annotators.
We do not focus on obtaininga single ?correct?
annotation, but rather view all responses as valuable sources ofinformation, even when they diverge.For each item presented, annotators additionally were provided a comment fieldshould they desire to include a more detailed response regarding the item in question.They could use this, for example, to comment on problems understanding the sentence.The annotators were able to revisit previous items in the task.
Annotators were not ableto skip forward in the task without rating the current item.
If an annotator attempted tosubmit an incomplete annotation they were prompted to provide a complete responsebefore proceeding.
They were free to log out and resume later at any point, however,and also could access the instructions whenever they wanted.Two Rounds of Annotation.
We performed two rounds of the annotation experiments,hereafter referred to as R1 and R2.10 Both annotation rounds included both a WSsim anda Usim task, labeled in the subsequent discussion as WSsim-1 and Usim-1 for R1, andWSsim-2 and Usim-2 for R2.
An important part of the data analysis is to compare thenew, graded annotation to other types of annotation.
We compare it to both traditionalword sense annotation, with a single best sense for each occurrence, and lexicalsubstitution, which characterizes each occurrence through paraphrases.
In R1, we choseannotation data that had previously been labeled with either traditional single senseannotation or with lexical substitutions.
R2 included two additional annotation tasks,one involving traditional WSD methodology (WSbest) and a lexical substitution task(SYNbest).
In the SYNbest task, annotators provided a single best lexical substitution,in contrast to the multiple substitutes annotators provided in the original LEXSUB data.11Three annotators participated in each task in the R1, and eight annotators partici-pated in R2.
In R1, separate groups of annotators participated in WSsim and Usim an-notation, whereas in R2 the same group of annotators was used for all annotation, so asto allow comparison across tasks for the same annotator as well as across annotators.
InR2, therefore, the same annotators did both traditional word sense annotation (WSbest)and the graded word sense annotation of the WSsim task.
This raises the question ofwhether their experience on one task will influence their annotation choice on the othertask.
We tested this by varying the order in which annotators did WSsim and WSbest.R2 annotators were divided into two groups of four annotators with the order of tasksas follows:group 1: Usim-2 SYNbest WSsim-2 WSbestgroup 2: Usim-2 SYNbest WSbest WSsim-2Another difference between the two rounds of annotation was that in R2 we per-mitted the annotators to see one more sentence of context on either side of the target9 The ?I don?t know?
option was present only in the Usim interface, and was not available in WSsim.10 The annotation was conducted in two separate rounds due to funding.11 Annotation guidelines for R1 are at http://www.katrinerk.com/graded-sense-and-usage-annotationand guidelines for R2 tasks are at http://www.dianamccarthy.co.uk/downloads/WordMeaningAnno2012/.520Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextTable 5Abbreviations used in the text for annotation tasks and rounds.WSsim Task: graded annotation of WordNet senses on a five-point scaleUsim Task: graded annotation of usage similarity on a five-point scaleWSbest Task: traditional single-sense annotationSYNbest Task: lexical substitutionR1 Annotation round 1R2 Annotation round 2sentence.
In R1 each item was given only one sentence as context.
We added morecontext in order to reduce the chance that the sentence would be unclear.
Table 5summarizes up the annotation tasks and annotation rounds on which we report.Data Annotated.
The data to be annotated in WSsim-1 were taken primarily fromSemcor (Miller et al1993) and the Senseval-3 English lexical sample (SE-3) (Mihalcea,Chklovski, and Kilgarriff 2004).
This experiment contained a total of 430 sentences span-ning 11 lemmas (nouns, verbs, and adjectives).
For eight of these lemmas, 50 sentenceswere included, 25 randomly sampled from Semcor and 25 randomly sampled from SE-3.The remaining three lemmas in the experiment had 10 sentences each, from the LEXSUBdata.
Each of the three annotators annotated each of the 430 items, providing a responsefor each WordNet sense for that lemma.
Usim-1 used data from LEXSUB.
Thirty-fourlemmas were manually selected, including the three lemmas also used in WSsim-1.
Weselected lemmas which exhibited a range of meanings and substitutes in the LEXSUBdata, with as few multiword substitutes as possible.
Each lemma is the target in 10LEXSUB sentences except there were only nine sentences for the lemma bar.n becauseof a part-of-speech tagging error in the LEXSUB trial data.
For each lemma, annotatorswere presented with every pairwise comparison of these 10 sentences.
We refer to eachsuch pair as an SPAIR.
There were 45 SPAIRs per lemma (36 for bar.n), adding up to 1,521comparisons per annotator in Usim-1.In R1, only 30 sentences were included in both WSsim and Usim.
Because compar-ison of annotator responses on this subset of the two tasks yielded promising results,R2 used the same set of sentences for both Usim and WSsim so as to better comparethese tasks.
All data in the second round were taken from LEXSUB, and contained 26lemmas with 10 sentences for each.
We produced the SYNbest annotation, rather thanuse the existing LEXSUB annotation, so that we could ensure the same conditions aswith the other annotation tasks, that is, using the same annotators and providing theextra sentence of context on either side of the original LEXSUB context.
We also onlyrequired that the annotators provide one substitute.
As such, there were 260 targetlemma occurrences that received graded word sense applicability ratings in WSsim-2,and 1,170 SPAIRs (pairs of occurrences) to be annotated in Usim-2.4.
Analysis of the AnnotationIn this section we present our analysis of the annotated data.
We test inter-annotatoragreement, and we test to what extent annotators make use of the added flexibilityof the graded annotation.
We also compare the outcome of our graded annotation totraditional word sense annotation and lexical substitutions for the same data.521Computational Linguistics Volume 39, Number 34.1 Evaluation MeasuresBecause both graded annotation tasks, WSsim and Usim, use ratings on five-pointscales rather than binary ratings, we measure agreement in terms of correlation.
Becauseratings were not normally distributed, we choose a non-parametric test which usesranks rather than absolute values: We use Spearmans rank correlation coefficient (rho),following Mitchell and Lapata (2008).
For assessing inter-tagger agreement on the R2WSbest task we adopt the standard WSD measure of average pairwise agreement, andfor R2 SYNbest, we use the same pairwise agreement calculation used in LEXSUB.When comparing graded ratings with single-sense or lexical substitution annota-tion, we use the mean of all annotator ratings in the WSsim or Usim annotation.
Thisis justified because the inter-annotator agreement is highly significant, with respectablerho compared with previous work (Mitchell and Lapata 2008).As the annotation schemes differ between R1 and R2 (as mentioned previously, thenumber of annotators and the amount of visible context are different, and R2 annotatorsdid traditional word sense annotation in the WSbest task in addition to the gradedtasks) we report the results of R1 and R2 separately.124.2 WSsim: Graded Ratings for WordNet SensesIn the WSsim task, annotators rated the applicability of each sense of the target word ona five-point scale.
We first do a qualitative analysis, then turn to a quantitative analysisof annotation results.4.2.1 Qualitative Analysis.
Table 6 shows an example of WSsim annotation.
The targetis the verb dismiss, which was annotated in R2.
The first column gives the WordNetsense number (sn).13 Note that in the task, the annotators were given the synonymsand full description but in this figure we only supply part of the description for thesake of space.
As can be seen, three of the annotators chose a single-sense annotationby giving a rating of 5 to one sense and ratings of 1 to all others.
Two annotators gaveratings of 1 and 2 to all but one sense.
The other three annotators gave positive ratings(ratings of at least 3 [similar], see Table 4) to at least two of the senses.
All annotatorsagree that the first sense fits the usage perfectly, and all annotators agree that senses3 and 5 do not apply.
The second sense, on the other hand, has an interestingly widedistribution of judgments, ranging from 1 to 4.
This is the judicial sense of the verb, asin ?this case is dismissed.?
Some annotators consider this sense to be completely distinctfrom sense 1, whereas others see a connection.
There is disagreement among annotators,about sense 6.
This is the sense ?dismiss, dissolve,?
as in ?the president dissolved theparliament.?
Six of the annotators consider this sense completely unrelated to ?dismissour actions as irrelevant,?
whereas two annotators view it as highly related (thoughnot completely identical).
It is noteworthy that each of the two opinions, a rating of 112 It is known that when responses are collected on an ordinal scale, the possibility exists for differentindividuals to use the scale differently.
As such, it is common practice to standardize responses using az-score, which maps a response X to z = X???
.
The calculation of z-scores makes reference to the meanand the standard deviation of an annotator?s responses.
Because responses were not normally distributedin our task, a transformation that relies on measures of central tendency is not appropriate.
So we do notuse z-scores in this paper.
We repeated all analyses with z-score transform anyway, and found the resultsto be basically the same as those we report here with the raw values.
Overall, using z-scores slightlystrengthened most findings, but there were no differences in statistical significance anywhere.13 We use WordNet 3.0 for our annotation.522Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextTable 6WSsim example, R2: Annotator judgments for the different senses of dismiss.If we see ourselves as separate from the world, it is easy to dismiss our actions as irrelevantor unlikely to make any difference.
(902)sn Description Ratings By Annotator Mean1 bar from attention or consideration 5 5 5 5 5 5 5 5 52 cease to consider 1 4 1 3 2 2 1 3 2.1253 stop associating with 1 2 1 1 1 2 1 1 1.254 terminate the employment of 1 4 1 2 1 1 1 1 1.55 cause or permit a person to leave 1 2 1 1 1 1 1 2 1.256 declare void 1 1 1 4 1 1 1 4 1.75and a rating of 4, was chosen by multiple annotators.
Because multiple annotators giveeach judgment, these data seem to reflect a genuine difference in perceived sense.
Wediscuss inter-annotator agreement, both overall and considering individual annotators,subsequently.Table 7 gives an example sentence from R1, where the annotated target is the nounpaper.
All annotators agree that sense 5, ?scholarly article,?
applies fully.
Sense 2 (?essay?
)also gets ratings of ?
3 from all annotators.
The first annotator seems also to haveperceived the ?physical object?
connotation to apply strongly to this example, and hasexpressed this quite consistently by giving high marks to sense 1 as well as 7.Table 8 shows a sample annotated sentence with an adjective target, neat, annotatedin R2.
In this case, only one annotator chose single-sense annotation by marking exclu-sively sense 4.
One annotator gave ratings ?
3 (similar) to all senses of the lemma.
Allother annotators saw at least two senses as applying (with ratings ?
3) and at least onesense as not applying at all (with a rating of 1).
Sense 4 has received positive ratings (thatis, ratings ?
3) throughout.
Senses 1, 2, and 6 have mixed ratings, and senses 3 and 5have positive ratings only from the one annotator who marked everything as applying.Interestingly, ratings for senses 1, 2, and 6 diverge sharply, with some annotators seeingthem as not applying at all, and some giving them ratings in the 3?5 range.
Note that theTable 7WSsim example, R1: Annotator judgments for the different senses of paper.This can be justified thermodynamically in this case, and this will be done in a separatepaper which is being prepared.
(br-j03, sent.
4)sn Description Ratings Mean1 a material made of cellulose pulp 4 1 1 1.32 an essay (especially one written as an assignment) 3 3 5 3.73 a daily or weekly publication on folded sheets; containsnews and articles and advertisements2 1 3 24 a medium for written communication 5 3 1 35 a scholarly article describing the results of observationsor stating hypotheses5 5 5 56 a business firm that publishes newspapers 2 1 1 1.37 the physical object that is the product of a newspaperpublisher4 1 1 1.7523Computational Linguistics Volume 39, Number 3Table 8WSsim example, R2: Annotator judgments for the different senses of neat.Over the course of the 20th century scholars have learned that such images tried to makemessy reality neater than it really is (103)sn Description Ratings By Annotator Mean1 free from clumsiness; precisely ordeftly executed1 5 1 4 5 5 5 5 3.3752 refined and tasteful in appearance orbehavior or style3 4 1 4 4 3 1 3 2.8753 having desirable or positive qualitiesespecially those suitable for athing specified1 3 1 1 1 1 1 1 1.254 marked by order and cleanliness inappearance or habits4 5 5 3 4 5 5 5 4.55 not diluted 1 4 1 1 1 1 1 1 1.3756 showing care in execution 1 4 1 3 4 1 3 3 2.5Table 9Correlation matrix for pairwise correlation agreement for WSsim-1.
The last row provides theagreement of the annotator in that column against the average from the other annotators.A B CA 1.00 0.47 0.51B 0.47 1.00 0.54C 0.51 0.54 1.00against avg 0.56 0.58 0.61annotators who give ratings of 1 are not the same for these three ratings, pointing to dif-ferent, but quite nuanced, judgments of the ?make reality neater?
usage in this sentence.4.2.2 Inter-annotator Agreement.
We now turn to a quantitative analysis, starting withinter-annotator agreement.
For the graded WSsim annotation, it does not make senseto compute the percentage of perfect agreement.
As discussed earlier, we report inter-annotator agreement in terms of correlation, using Spearman?s rho.
We calculate pair-wise agreements and report the average over all pairs.
The pairwise correlations areshown in the matrix in Table 9.
We have used capital letters to represent the individ-uals, preserving the same letter for the same person across tasks.
In the last row weshow agreement of each annotator?s judgments against the average judgment from theother annotators.
The pairwise correlations range from 0.47 to 0.54 and all pairwisecorrelations were highly significant (p  0.001), with an average of rho = 0.504.
Thisis a very reasonable result given that Mitchell and Lapata (2008) report a rho of 0.40on a graded semantic similarity task.14 The lowest correlation against the average14 Direct comparison across tasks is not appropriate, but we wish to point out that for graded semanticjudgments this level of correlation is perfectly reasonable.
The Mitchell and Lapata (2008) dataset has been used in an evaluation exercise (GEMS-2011, https://sites.google.com/site/geometricalmodels/shared-evaluation).
Mitchell and Lapata point out that Spearman?s rhotends to yield lower coefficients compared with parametric alternatives such as Pearson?s.524Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextTable 10Correlation matrix for pairwise correlation agreement for WSsim-2.
The last row provides theagreement of the annotator in that column against the average from the other annotators.A C D F G H I JA 1.00 0.55 0.58 0.60 0.61 0.63 0.61 0.59C 0.55 1.00 0.54 0.66 0.57 0.55 0.65 0.52D 0.58 0.54 1.00 0.55 0.58 0.52 0.56 0.54F 0.60 0.66 0.55 1.00 0.62 0.62 0.72 0.59G 0.61 0.57 0.58 0.62 1.00 0.63 0.62 0.62H 0.63 0.55 0.52 0.62 0.63 1.00 0.64 0.64I 0.61 0.65 0.56 0.72 0.62 0.64 1.00 0.58J 0.59 0.52 0.54 0.59 0.62 0.64 0.58 1.00against avg 0.70 0.58 0.62 0.64 0.70 0.71 0.66 0.71from the other annotators was 0.56.
We discuss the annotations of individuals in Sec-tion 4.6, including our decision to retain the judgments of all annotators for our goldstandard.From the correlation matrix in Table 10 we see that for WSsim-2, pairwise corre-lations ranged from 0.52 to 0.72.
The average value of the pairwise correlations wasrho = 0.60, and again every pair was highly significant (p  0.001).
The lowest correla-tion against the average from all the other annotators was 0.58.4.2.3 Choice of Single Sense Versus Multiple Senses.
In traditional word sense annotation,annotators can mark more than one sense as applicable, but annotation guidelines oftenencourage them to view the choice of a single sense as the norm.
In WSsim, annotatorsgave ratings for all senses of the target.
So we would expect that in WSsim, there wouldbe a higher proportion of senses selected as applicable.
Indeed we find this to be thecase: Table 11 shows the proportion of sentences where some annotator has assignedmore than one sense with a judgment of 5, the highest value.
Both WSsim-1 and WSsim-2 have a much higher proportion of sentences with multiple senses chosen than thetraditional sense-annotated data sets SemCor and SE-3.
Interestingly, we notice thatthe percentage for WSsim-1 is considerably higher than for WSsim-2.
In principle, thiscould be due to differences in the lemmas that were annotated, or differences in thesense perception of the annotators between R1 and R2.
Another potential influencingTable 11WSsim annotation: Proportion of sentences where multiple senses received a rating of 5 (highestjudgment) from the same annotator.ProportionWSsim-1 46%WSsim-2 30%WSsim-2, WSsim first 36%WSsim-2, WSbest first 23%SemCor 0.3%SE-3 8%525Computational Linguistics Volume 39, Number 3factor is the order of annotation experiments: As described earlier, half of the R2 anno-tators did WSbest annotation before doing WSsim-2, and half did the two experimentsin the opposite order.
As Table 11 shows, those doing the graded task WSsim-2 beforethe binary task WSbest had a greater proportion of multiple senses annotated withthe highest response.
This demonstrates that annotators in a word meaning task canbe influenced by factors outside of the current annotation task, in this case anotherannotation task that they have done previously.
We take this as an argument in favor ofusing as many annotators as possible in order to counteract factors that contribute noise.In our case, we counter the influence of previous annotation tasks somewhat by usingmultiple annotators and altering the order of the WSsim and WSbest tasks.
Anotheroption would have been to use different annotators for different tasks; by using thesame set of annotators for all four tasks, however, we can better control for individualvariation.4.2.4 Use of the Graded Scale.
We next ask whether annotators in WSsim made use of thewhole five-point scale, or whether they mostly chose the extreme ratings of 1 and 5.If the latter were the case, this could indicate that they viewed the task of word senseassignment as binary.
Figure 1a shows the relative frequency distribution of responsesfrom all annotators over the five scores for both R1 and R2.
Figures 2a and 3a show thesame but for each individual annotator.
In both rounds the annotators chose the ratingof 1 (?completely different,?
see Table 4) most often.
This is understandable because eachitem is a sentence and sense combination and there will typically be several irrelevantsenses for a given sentence.
The second most frequent choice was 5 (?identical?).
Bothrounds had plenty of judgments somewhere between the two poles, so the annotatorsdo not seem to view the task of assigning word sense as completely binary.
Althoughthe annotators vary, they all use the intermediate categories to some extent and certainlythe intermediate category judgments do not originate from a minority of annotators.We notice that R2 annotators tended to give more judgments of 1 (?completelydifferent?)
than the R1 annotators.
One possible reason is again that half our annota-tors did WSbest before WSsim-2.
If this were the cause for the lower judgments, wewould expect more ratings of 1 for the annotators who did the traditional word senseannotation (WSbest) first.
In Table 12 we list the relative frequency of each rating for thedifferent groups of annotators.
We certainly see an increase in the judgments of 1 whereFigure 1WSsim and Usim R1 and R2 ratings.526Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextFigure 2WSsim and Usim R1 individual ratings.Figure 3WSsim and Usim R2 individual ratings.WSbest is performed before WSsim-2.
Again, this may indicate that annotators wereleaning more towards finding a single exact match because they were influenced bythe WSbest task they had done before.
Annotators in that group were also slightly lessinclined to take the middle ground, but this was true of both groups of R2 annotatorscompared with the R1 annotators.
We think that this difference between the two roundsmay well be due to the lemmas and data.In Table 18, we show the average range15 and average variance of the judgmentsper item for each of the graded annotation tasks.
WSsim naturally has less variation15 As an example, the first two senses (1 and 2) in Table 6 have ranges of 0 and 3, respectively.527Computational Linguistics Volume 39, Number 3Table 12The relative frequency of the annotations at each judgment from all annotators.JudgmentExp 1 2 3 4 5WSsim-1 0.43 0.106 0.139 0.143 0.181WSsim-2 0.696 0.081 0.067 0.048 0.109WSsim-2, WSsim first 0.664 0.099 0.069 0.048 0.12WSsim-2, WSbest first 0.727 0.063 0.065 0.048 0.097Usim-1 0.360 0.202 0.165 0.150 0.123Usim-2 0.316 0.150 0.126 0.112 0.296compared with Usim because, for any sentence, there are inevitably many WordNetsenses which are irrelevant to the context at hand and which will obtain a judgmentof 1 from everyone.
This is particularly the case for WSsim-2 where the annotatorsgave more judgments of 1, as discussed previously.
The majority of items have a rangeof less than two for WSsim.
We discuss the Usim figures further in the followingsection.4.3 Usim: Graded Ratings for Usage SimilarityIn Usim annotation, annotators compared pairs of usages of a target word (SPAIRs) andrated their similarity on the five-point scale given in Table 4.
The annotators were alsopermitted a response of ?don?t know.?
Such responses were rare but were used whenthe annotators really could not judge usage similarity, perhaps because the meaningof one sentence was not clear.
We removed any pairs where one of the annotators hadgiven a ?don?t know?
verdict (9 in R1, 28 in R2).
For R1 this meant that we were left witha total of 1,512 SPAIRs and in R2 we had a resultant 1,142 SPAIRs.4.3.1 Qualitative Analysis.
We again start by inspecting examples of Usim annotation.Table 13 shows the annotation for an SPAIR of the verb dismiss.
The first of the twosentences talks about ?dismissing actions as irrelevant,?
the second is about dismissinga person.
Interestingly, the second usage could be argued to carry both a connotationof ?ushering out?
and a connotation of ?disregarding.?
Annotator opinions on this SPAIRvary from a 1 (completely different) to a 5 (identical), but most annotators seem to viewthe two usages as related to an intermediate degree.
This is adequately reflected in theaverage rating of 3.125.
Table 14 compares the sentence from Table 8 to another sentenceTable 13Usim example: Annotator judgments for a pair of usages of dismiss.Sentences RatingsIf we see ourselves as separate from the world, it is easy to dismissour actions as irrelevant or unlikely to make any difference.1, 2, 3, 3,3, 4, 4, 5Simply thank your Gremlin for his or her opinion, dismiss him orher, and ask your true inner voice to turn up its volume.528Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextTable 14Usim example: Annotator judgments for a pair of usages of neat.Sentences RatingsOver the course of the 20th century scholars have learned that suchimages tried to make messy reality neater than it really is.3, 3, 4, 4,4, 4, 5, 5Strong field patterns created by hedgerows give the landscape aneat, well structured appearance.Table 15Usim example: Annotator judgments for a pair of usages of account.Sentences RatingsSamba-3 permits use of multiple account data base backends.
1, 2, 3, 3,3, 4, 4, 4Within a week, Scotiabank said that it had frozen some accountslinked to Washington?s hit list.with the target neat.
The first sentence is a metaphorical use (making reality neater),the second is literal (landscape with neat appearance), but still the SPAIR gets highratings of 3?5 throughout for an average of 4.0.
Note that the WordNet senses, shownin Table 8, do not distinguish the literal and metaphorical uses of the adjective, either.Table 15 shows two uses of the noun account.
The first pertains to accounts on a softwaresystem, the second to bank accounts.
The spread of annotator ratings shows that thesetwo uses are not the same, but that some relation exists.
The average rating for thisSPAIR is 3.0.4.3.2 Inter-annotator Agreement.
We again calculate inter-annotator agreement as theaverage over pairwise Spearman?s correlations.
The pairwise correlations are shownin the matrix in Table 16.
In the last row we show agreement of each annotator?sjudgments against the average judgment from the other annotators.
For Usim-1 therange of correlation coefficients is between 0.50 and 0.64 with an average correlationof rho = 0.548.
All the pairs are highly significantly correlated (p  0.001).
The smallestcorrelation for any individual against the average is 0.55.
The correlation matrix forUsim-2 is provided in Table 17; the range of correlation coefficients is between 0.42 andTable 16Correlation matrix for pairwise correlation agreement for Usim-1.
The last row provides theagreement of the annotator in that column against the average from the other annotators.A D EA 1.00 0.50 0.64D 0.50 1.00 0.50E 0.64 0.50 1.00against avg 0.67 0.55 0.67529Computational Linguistics Volume 39, Number 3Table 17Correlation matrix for pairwise correlation agreement for Usim-2.
The last row provides theagreement of the annotator in that column against the average from the other annotators.A C D F G H I JA 1.00 0.70 0.52 0.70 0.69 0.72 0.73 0.67C 0.70 1.00 0.48 0.72 0.60 0.66 0.71 0.69D 0.52 0.48 1.00 0.48 0.49 0.51 0.50 0.42F 0.70 0.72 0.48 1.00 0.66 0.71 0.74 0.68G 0.69 0.60 0.49 0.66 1.00 0.71 0.65 0.62H 0.72 0.66 0.51 0.71 0.71 1.00 0.70 0.65I 0.73 0.71 0.50 0.74 0.65 0.70 1.00 0.72J 0.67 0.69 0.42 0.68 0.62 0.65 0.72 1.00against avg 0.82 0.78 0.58 0.80 0.76 0.80 0.81 0.760.73.
All these correlations are highly significant (p 0.001) with an average correlationof rho = 0.62.
The lowest agreement between any individual and the average judgmentof the others is 0.58.
Again, we note that these are all respectable values for tasksinvolving semantic similarity ratings.Use of the graded scale.
Figure 1b shows how annotators made use of the graded scalein Usim-1 and Usim-2.
It graphs the relative frequency of each of the judgments on thefive-point scale.
Figures 2b and 3b show the same but for each individual annotator.
Inboth annotation rounds, the rating 1 (completely different) was chosen most frequently.There are also in both annotation rounds many ratings in the middle points of thescale, indeed we see a larger proportion of mid-range scores for Usim than for WSsimin general, as shown in Table 12.
Figures 2b and 3b show that although individualsdiffer, all use the mid points to some extent and it is certainly not the case that thesemid-range judgments come from a minority of annotators.
In Usim, annotators com-pared pairs of usages, whereas in WSsim, they compared usages with sense defini-tions.
The sense definitions suggest a categorization that may bias annotators towardscategorical choices.
Comparing the two annotation rounds for Usim, we see that inUsim-2 there seem to be many more judgments at 5 than in Usim-1.
This is similarto our findings for WSsim, where we also obtained more polar judgments for R2 thanfor R1.There is a larger range on average for Usim-2 compared with the other tasks asshown earlier by Table 18.
This is understandable given that there are eight annotatorsTable 18Average range and average variance of judgments for each of the graded experiments.avg range avg varianceWSsim-1 1.78 1.44WSsim-2 1.55 0.71Usim-1 1.41 0.92Usim-2 2.50 1.12530Erk, McCarthy, and Gaylord Measuring Word Meaning in Contextfor R2 compared with R1,16 and so a greater chance of a larger range per item.
There issubstantial variation by lemma.
In Usim-2, fire.v, rough.a, and coach.n have an averagerange of 1.33, 1.76, and 1.93, respectively, whereas suffer.v, neat.a, and function.n haveaverage ranges of 3.14, 3.16, and 3.58, respectively.
The variation in range appears todepend on the lemma rather than POS.
This variation can be viewed as a gauge of howdifficult the lemma is.
Although the range is larger in Usim-2, however, the averagevariance per item (i.e., the variance considering the eight annotators) is 1.12 and lowerthan that for WSsim-1.Usim and the triangle inequality.
In Euclidean space, the lengths of two sides of a triangle,taken together, must always be greater than the length of the third side.
This is thetriangle inequality:length(longest) < length(second longest) + length(shortest)We now ask whether the triangle inequality holds for Usim ratings.
If Usim similaritiesare metric, that is, if we can view the ratings as proximity in a Euclidean ?meaningspace,?
then the triangle inequality would have to hold.
This question is interesting forwhat it says about the psychology of usage similarity judgments.
Classic results dueto Tversky and colleagues (Tversky 1977; Tversky and Gati 1982) show that humanjudgments of similarity are not always metric.
Tversky (1977), varying an exampleby William James, gives the following example, which involves words, but explicitlyignores context:Consider the similarity between countries: Jamaica is similar to Cuba (because ofgeographical proximity); Cuba is similar to Russia (because of their political affinity);but Jamaica and Russia are not similar at all.
[.
.
. ]
the perceived distance of Jamaica toRussia exceeds the perceived distance of Jamaica to Cuba, plus that of Cuba toRussia?contrary to the triangle inequality.Note, however, that Tversky was considering similarity judgments for different words,whereas we look at different usages of the same word.
The question of whether thetriangle inequality holds for Usim ratings is also interesting for modeling reasons.Several recent approaches model word meaning in context through points in vectorspace (Erk and Pado 2008; Mitchell and Lapata 2008; Dinu and Lapata 2010; Reisingerand Mooney 2010; Thater, Fu?rstenau, and Pinkal 2010; Washtell 2010; Van de Cruys,Poibeau, and Korhonen 2011).
They work on the tacit assumption that similarity ofword usages is metric?an assumption that we can directly test here.
Third, the triangleinequality question is also relevant for future annotation; we will discuss this in moredetail subsequently.To test whether Usim ratings obey the triangle inequality, we first convert thesimilarity ratings that the annotators gave to dissimilarity ratings: Let savg be the meansimilarity rating over all annotators, then we use the dissimilarity rating d = 6 ?
savg(as 5 was the highest possible similarity score).We examine the proportion of sentence triples where the triangle inequality holds(that is, we consider every triple of sentences that share the same target lemma).
In those16 A likely reason for the larger range in WSsim-1 compared with WSsim-2 is that in WSsim-2 half theannotators had performed WSbest before WSsim-2 and produced more judgments of 1 compared withWSsim-1.531Computational Linguistics Volume 39, Number 3cases where the triangle inequality is violated, we also assess the degree to which it isviolated, calculated as the average distance that is missed: Let Tmiss be the set of triplesfor which the triangle inequality does not hold, then we computem = 1|Tmiss|?t?Tmisslength(longestt) ?
(length(second longestt) + length(shortestt))This is the average amount by which the longest side is ?too long.
?For the first round of annotation, Usim-1, we found that 99.2% of the sentencetriples obey the triangle inequality.
For the triples that miss it, the average amountby which the longest side is too long is m = 0.520.
This is half a point on the five-point rating scale, a low amount.
In R2, all sentence triples obey the triangle inequality.One potential reason for this is that we have eight annotators for R2, and a largersample of annotators reduces the variation from individuals.
Another reason maybe that the annotators in R2 could view two more sentences of context than thosein R1.Tables 19 and 20 show results of the triangle inequality analysis, but by individualannotator.
Every annotator has at least 93% of sentence triples obeying the principle.
Forthe triples that miss it, they tend to miss it by between one and two points.
The resultsfor individuals accord with the triangle inequality principle, though to a lesser extentcompared with the analysis using the average, which reduces the impact of variationfrom individuals.As discussed previously, this result (that the triangle inequality holds for Usimannotation triples) is interesting because it contrasts with Tversky?s findings (Tversky1977; Tversky and Gati 1982) that similarity ratings between different words are notmetric.
And although we consider similarity ratings for usages of the same word, notdifferent words, we would argue that our findings point to the importance of consider-ing the context in which a word is used.
It would be interesting to test whether similarityratings for different words, when used in context, obey the triangle inequality.
Toreference the Tversky example, and borrowing some terminology from Cruse, evokingthe ISLAND facet of Jamaica and Cuba versus the COMMUNIST STATE facet of Cuba andRussia would account for the non-metricality of the similarity judgments as TverskyTable 19Triangle inequality analysis by annotator, Usim-1.average A D Eperc obey 93.8 97.2 97.3missed by 1.267 1.221 1.167Table 20Triangle inequality analysis by annotator, Usim-2.A C D F G H I Jperc obey 94.1 97.5 98.4 97.2 93.6 97.0 97.4 97.4missed by 1.508 1.405 1.122 1.824 1.477 1.281 1.759 1.338532Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextTable 21WSbest annotations.sense selected Proportion withno yes multiple choiceWSbest 19,599 2,401 0.13WSbest, WSsim-2 first 9,779 1,221 0.15WSbest, WSbest first 9,820 1,180 0.11points out, and moreover highlight the lack of an apt comparison between Jamaica andRussia at all.
There is some motivation for this idea in the psychological literature onstructural alignment and alignable differences (Gentner and Markman 1997; Gentnerand Gunn 2001).In addition, our finding that the triangle inequality holds for Usim annotationwill be useful for future Usim annotation.
Usage similarity annotation is costly (andsomewhat tedious) as annotators give ratings for each pair of sentences for a giventarget lemma.
Given that we can expect usage similarity to be metric, we can eliminatethe need for some of the ratings.
Once annotators have rated two usage pairs out of atriple, their ratings set an upper limit on the similarity of the third pair.
In the best case, ifusages s1 and s2 have a distance of 1 (i.e., a similarity of 5), and s1 and s3 have a distanceof 1, then the distance of s1 and s3 can be at most 2.
For all usage triples where twopairs have been judged highly similar, we can thus omit obtaining a rating for the thirdpair.
A second option for obtaining more Usim annotation is to use crowdsourcing.
Incrowdsourcing annotation, quality control is always an issue, and again we can makeuse of the triangle inequality to detect spurious annotation: Ratings that grossly violatethe triangle inequality can be safely discarded.4.4 WSbestThe WSbest task reflects the traditional methodology in word sense annotation wherewords are annotated with the best fitting sense.
The guidelines17 allow for selectingmore than one sense provided all fit the example equally well.
Table 21 shows that,as one would expect given the number of senses in WordNet, there are more unse-lected senses than selected.
We again find an influence of task order: When annota-tors did the graded annotation (WSsim-2) before WSbest, there were more multipleassignments (see the last column) and therefore more senses selected.
This differenceis statistically significant (?2 test, p = 0.02).
Regardless of the order of tasks, we no-tice that the proportion of multiple sense choice is far lower than the equivalent forWSsim (see Table 11), as is expected due to the different annotation schemes andguidelines.We calculated inter-annotator agreement using pairwise agreement, as is standardin WSD.
There are several ways to calculate pairwise agreement in cases of multipleselection, though these details are not typically given in WSD papers.
We use the sizeof the intersection of selections divided by the maximum number of selections from17 See http://www.dianamccarthy.co.uk/downloads/WordMeaningAnno2012/wsbest.html.533Computational Linguistics Volume 39, Number 3Table 22Inter-annotator agreement without one individual for WSbest and SYNbest R2.average A C D F G H I JWSbest 0.574 0.579 0.564 0.605 0.560 0.582 0.566 0.566 0.568SYNbest 0.261 0.261 0.259 0.285 0.254 0.256 0.245 0.260 0.267either annotator.
This is equivalent to 1 for agreement and 0 for disagreement in caseswhere both annotators have selected only one sense.
Formally, let i ?
I be one annotatedsentence.
Let A be the set of annotators and let PA = {{a, a?}
| a, a?
?
A} be the set ofannotator pairs.
Let ai be the set of senses that annotator a ?
A has chosen for sentencei.
Then pairwise agreement between annotators is calculated as:ITA WSbest =?i?I?{a,a?
}?PA|ai?a?i |max(|ai|,|a?i |)|PA| ?
|I|(1)The average ITA was calculated as 0.574.18 If we restrict the calculation to itemswhere each annotator only selected one sense (not multiple), the average is 0.626.For SE-3, ITA was 0.628 on the English Lexical Sample task, not including themultiword data (Mihalcea, Chklovski, and Kilgarriff 2004).
This annotation exerciseused volunteers from the Web (Mihalcea and Chklovski 2003).
Like our study, it hadtaggers without lexicography background and gave a comparable ITA to our 0.626.
Wecalculated pairwise agreement for eight annotators.
To carry out the experiment undermaximally similar conditions to previous studies, we also calculated ITA for items withonly one response and use only the four annotators who performed WSbest first.
Thisresulted in an average ITA of 0.638.We also calculated the agreement for WSbest in R2 as in Equation 1 but with eachindividual removed to see the change in agreement.
The results are in the first row ofTable 22.4.5 SYNbestThe SYNbest task is a repetition of the LEXSUB task (McCarthy and Navigli 2007, 2009)except that annotators were asked to provide one synonym at most.
As in LEXSUB,agreement between a pair of annotators was counted as the proportion of all thesentences for which the two annotators had given the same response.As in WSbest, let A be the set of annotators.
I is the set of test items, but as inLEXSUB we only include those where at least two annotators have provided at leastone substitute: If only one annotator can think of a substitute then it is likely to be aproblematic item.
As in WSbest, let ai be the set19 of responses (substitutes) for an item18 Although there is a statistical difference in the number of multiple assignments depending upon whetherWSsim-2 is completed before or after WSbest, ITA on the WSbest task does not significantly differbetween the two sets.19 Though, in fact, unlike LEXSUB and WSbest, we only collect one substitute per annotator for SYNbest.534Erk, McCarthy, and Gaylord Measuring Word Meaning in Contexti ?
I for annotator a ?
A.
Let PA again be the set of pairs of annotators from A. Pairwiseagreement between annotators is calculated as in LEXSUB as:PA =?i?I?{a,a?
}?PA|ai?a?i ||ai?a?i ||PA| ?
|I|(2)Note that in contrast to pairwise agreement for traditional word sense annotation(WSbest), the credit for each item (the intersection of annotations from the annotatorpair) is divided by the union of the responses.
For traditional WSD evaluation, it isdivided by the number of responses from either annotator, which is usually one.
Forlexical substitution this is important as the annotations are not collected over a predeter-mined inventory.
In LEXSUB, the PA figure was 0.278, whereas we obtain PA = 0.261 onSYNbest.
There were differences in the experimental set-up.
We had eight annotators,compared with five, and for SYNbest each annotator only provided one substitute.Additionally, our experiment involved only a subset of the data used in LEXSUB.
Thefigures are not directly comparable, but are reasonably in line.In our task, out of eight annotators we had at most three people who could not finda substitute for any given item, so there were always at least five substitutes per item.In LEXSUB there were 16 items excluded from testing in the full data set of 2010 becausethere was only one token substitute provided by the set of annotators.We also calculated the agreement for SYNbest as in Equation 2 but with eachindividual removed to see the change in agreement.
The results are in the second rowof Table 22.4.6 Discussion of the Annotations of IndividualsWe do not pose these annotation tasks as having ?correct responses.?
We wish insteadto obtain the annotators?
opinions, and accept the fact that the judgments will vary.Nevertheless, we would not wish to conduct our analysis using annotators who werenot taking the task seriously.
In the analyses that follow in subsequent sections, weuse the average judgment from our annotators to reduce variation from individuals.Nevertheless, before doing so, in this subsection we briefly discuss the analysis of theindividual annotations provided earlier in this section in support of our decision to useall annotators for the gold standard.Although there was variation in the profile of annotations for individuals, all of theannotators showed reasonable correlation on the graded task and at a level in excessof that achieved on other graded semantic tasks (Mitchell and Lapata 2008).
There willinevitably be one annotator that has the lowest correlation with the others on any giventask, but we found that this was not the same annotator on every task.
For example,C on WSsim-2 has the lowest correlation with the average, yet concurs with othersmuch more on Usim-2 and leaving C out would reduce agreement on WSbest and onSYNbest.
D has lower correlation with others on several tasks, though higher than C onWSsim-2.
When we redo the triangle inequality analysis in Section 4.3 individually wesee from Tables 19 and 20 that annotator D is the highest performing annotator in termsof meeting the triangle inequality principle in R2 and is a close runner-up in R1.
Theseresults indicate that although annotators may use the graded scale in different ways,their annotations tally to a reasonable extent.
We therefore used all annotators for thegold standard.535Computational Linguistics Volume 39, Number 34.7 Agreement Between Annotations in Different FrameworksIn this paper we are considering various different annotations of the same underly-ing phenomenon: word meaning as it appears in context.
In doing so, we contrasttraditional WSD methodology (SE-3, SemCor, and WSbest) with graded judgments ofsense applicability (WSsim), usage similarity (Usim), and lexical substitution as inLEXSUB and SYNbest.
In this section we compare the annotations from these differentparadigms where the annotations are performed on the same underlying data.
ForWSsim and Usim, we use average ratings as the point of comparison.4.7.1 Agreement Between WSsim and Traditional Sense Assignment.
To compare WSsimratings on a five-point scale with traditional sense assignment on the same data, weconvert the traditional word sense assignments to ratings on a five-point scale: Anysense that is assigned is given a score of 5, and any sense that is not assigned isgiven a score of 1.
If multiple senses are chosen in the gold standard, then they areall given scores of 5.
We then correlate the converted ratings of the traditional wordsense assignment with the average WSsim ratings using Spearman?s rho.As described earlier, most of the sentences annotated in WSsim-1 were taken fromeither SE-3 or SemCor.
The correlation of WSsim-1 and SE-3 is rho = 0.416, and the cor-relation of WSsim-1 with SemCor is rho = 0.425.
Both are highly significant (p  0.001).For R2 we can directly contrast WSsim with the traditional sense annotation inWSbest on the same data.
This allows a fuller comparison of traditional and gradedtagging because we have a data set annotated with both methodologies, under the sameconditions, and with the same set of annotators.
We use the mode (most common) sensetag from our eight annotators as the traditional gold standard label for WSbest andassign a rating of 5 to that sense, and a rating of 1 elsewhere.
We again used Spearman?srho to measure correlation between WSbest and WSsim and obtained rho = 0.483(p  0.001).4.7.2 Agreement Between WSsim and Usim.
WSsim and Usim provide two graded an-notations of word usage in context.
To compare the two, we convert WSsim scoresto usage similarity ratings as in Usim.
In WSsim, each sense has a rating (aver-aged over annotators), so a sentence has a vector of ratings with a ?dimension?
foreach sense.
For example, the vector of average ratings for the sentence in Table 6 is?5, 2.125, 1.25, 1.5, 1.25, 1.75?.
All sentences with the same target will have vectors in thesame space, as they share the same sense list.
Accordingly, we can compare a pair u,u?of sentences that share a target using Euclidean distance:d(u, u?)
=?
?i(ui ?
u?i)2where ui is the ith dimension of the vector u of ratings for sentence u.
Note that this givesus a dissimilarity rating for u,u?.
We can now compare these sentence pair dissimilaritiesto the similarity ratings of the Usim annotation.In R1 we found a correlation of rho = ?0.596 between WSsim and Usim ratings.20The basis of this comparison is small, at three lemmas with 10 sentences each, giving20 The negative correlation is due to the comparison of dissimilarity ratings with similarity ratings.536Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextTable 23Spearman?s correlation between lexical paraphrase overlap on the one hand, and Usimsimilarity or WSsim dissimilarity on the other hand.tasks rhoUsim-1 vs. LEXSUB 0.590Usim-2 vs. SYNbest 0.764WSsim-1 vs. LEXSUB ?0.495WSsim-2 vs. SYNbest ?0.749135 sentence pairs in total, because that is all the data available annotated in bothparadigms.
For R2 we can perform the analysis on the whole Usim-2 and WSsim-2data, which gives us 26 lemmas, with 1,142 sentence pairs.21 Correlation on R2 data isrho = ?0.816.
The degree of correlation is striking.
We conclude that there is a verystrong relationship between the annotations for Usim and WSsim.
This bodes well forusing Usim as a resource for evaluating sense inventories, an idea that we will pursuefurther in Section 6: It reflects word meaning but is not tied to any given sense inventory.4.7.3 Agreement of WSsim and Usim with Lexical Substitution.
Lexical paraphrases (sub-stitutes) have been used as a means of evaluating WSD systems in a task where theinventory is not predefined (McCarthy and Navigli 2007, 2009).
Because the R1 an-notation was done in part on data that had previously been annotated with lexicalsubstitutions, and R2 included lexical substitution annotation, we can compare para-phrase annotation with the results of WSsim and Usim.
Again, we need to transformannotations to make the comparison feasible.
We convert all annotations to a Usim-like format using sentence pair similarity or dissimilarity ratings.
For WSsim, we usethe transformation described previously, using Euclidean distance between sense ratingvectors.
We transform lexical substitution annotation using multiset intersection, asthe lexical substitution annotation of a sentence is a multiset of substitutes22 from allannotators.
If sentences s1, s2 have substitute multisets subs1 and subs2, respectively, andfreqi(w) is the frequency of substitute w in multiset subsi, then we calculate multisetintersection asINTER(s1, s2) =1max(|subs1|, |subs2|)?w?subs1?subs2min( freq1(w), freq2(w))Again, as before and in LEXSUB, we only keep sentences for which at least twoannotators could come up with a substitute.
We also did not include any items thatwere tagged with the wrong POS in LEXSUB.23Table 23 shows correlation, in terms of Spearman?s rho, of Usim and WSsimannotation with lexical substitution annotation.
The values of Usim and WSsim arebased on mean scores averaged over all annotators.
The INTER values computed for the21 This is the number of pairs remaining after we exclude any pairs where one of the annotators provided a?do not know?
response.22 The frequency of a substitute in a multiset depends on the number of annotators that picked thesubstitute for the particular data point.23 This was relevant only for the trial portion of LEXSUB, as the test portion was manually verified.537Computational Linguistics Volume 39, Number 3lexical substitution annotation yield similarity ratings for sentence pairs; accordingly,correlations of transformed lexical substitution with Usim are positive, and correlationsof transformed lexical substitution with the WSsim-based sentence dissimilarity ratingsare negative.
All correlations are highly significant (p  0.001).
We anticipated a highercorrelation of SYNbest with R2 annotation compared with that obtained using LEXSUBand R1 annotation: In R2 the set of annotators is larger, the same set of annotators doall experiments, and the SYNbest annotation focuses on obtaining one substitute perannotator (whereas LEXSUB allowed annotators to supply up to three paraphrases).
Thisturned out to be in fact the case, as a comparison of rows 1 and 2 of Table 23 shows,and likewise a comparison of rows 3 and 4.
We notice that the correlation is slightlystronger for Usim compared with WSsim, for both annotation rounds.
One possiblereason for this is that the comparison of lexical substitution data with Usim involvesonly one transformation of annotation data (the INTER calculation), whereas the com-parison with WSsim involves two (INTER and also the Euclidean distance transforma-tion).
We can expect each transformation of annotation data to be ?lossy?
in the senseof introducing additional variance.
Furthermore, WSsim relies on WordNet, whichmay add a layer of structure that does not reflect the overlap in semantic similaritybetween usages.4.7.4 Summary.
The Usim framework enables us to compare different annotationschemes for word meaning, as it is relatively straightforward to map all annotationsto sentence pair (dis-)similarity ratings.
We found strong relationships between WSsimand Usim annotation, and between both graded annotation frameworks on the onehand and traditional word sense annotation or lexical substitutions on the other hand.This provides some validation for the novel annotation frameworks.
Also, if all labelingschemes provide comparable results, that opens up opportunities for choosing the best-fitting labeling scheme for each situation.
All these tasks pursue the same endeavor,although the graded annotations and substitutions strive to capture the more subtlenuances of meaning that are not adequately represented by the winner takes all ap-proach of traditional methodology.
WSsim is closest to the traditional methodology andwould suit systems needing to output WordNet sense labels, for example because theywant to exploit the semantic relations in WordNet.
Usim is application-independent.
Itallows for evaluation of systems that relate usages, whether into clusters or simply ona continuum.
It could, for example, be used as a resource-independent gold standardfor word sense induction.
Lexical substitution tasks are particularly useful where theapplication being considered would benefit from lexical paraphrasing, for example, textsimplification, summarization, or query expansion in information retrieval.5.
Examining Sense Groupings Emerging from WSsim AnnotationRecently there has been considerable work on grouping fine-grained senses, often fromWordNet, into more coarse-grained sense groups (Palmer, Dang, and Fellbaum 2007).The use of coarse-grained sense groups has been shown to yield considerable improve-ments in inter-annotator agreement in manual annotation, as well as in the accuracy ofWSD systems (Palmer, Dang, and Fellbaum 2007; Pradhan et al2007).
In our WSsimannotation, we have used fine-grained WordNet senses, but we want to check that ourresults are not an artifact of this fine-grained inventory.
Furthermore, the annotationresults might be useful for identifying senses that could be grouped or for identifyingsenses where grouping is not straightforward.538Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextIn WSsim, annotators gave ratings for each sense of a target word.
If an annotatorperceives two senses of some target word as very similar, they will probably give themsimilar ratings, and not just for a single sentence but across all the sentences featuringthe target word in question.
So by looking for pairs of senses that tended to receivesimilar ratings across all sentences, we can identify sense descriptions that accordingto our annotators describe similar senses.
Conversely, we expect that unrelated senseswould have dissimilar ratings.
If there were many senses that the WSsim annotatorsimplicitly ?grouped?
by giving them similar ratings throughout, we would have torevise our finding that WSsim annotators often perceived more than one sense to beapplicable, as they would have perceived only what could be described as one implicitsense group.If a coarse-grained sense grouping is designed with the aim of reflecting sensedistinctions that would be intuitively plausible to an untrained speaker of the language,then senses in a common group should also be similar according to WSsim annotation.So when WSsim annotators give very different ratings to senses that are in the samecoarse-grained group, or very similar ratings to senses that are in different groups, thiscan point to problems in a coarse-grained sense group.In this section, first we describe two existing sense groupings (Hovy et al2006;Navigli, Litkowski, and Hargraves 2007).
Then we test the extent that the annotationsaccord with sense groupings by:1. comparing judgments against the existing groupings, and re-examiningthe question of how often WSsim annotators found multiple differentWordNet senses highly applicable.2.
using the WSsim data to examine the extent that the annotations could beused to induce sense groupings.5.1 Existing Sense Grouping EffortsOntoNotes.
The OntoNotes project (Hovy et al2006; Chen and Palmer 2009) annotatesword sense, along with coreference and semantic roles.
The senses that it uses for verbsare WordNet 2.1 and 2.0, manually grouped based on both syntactic and semanticcriteria.
Examples of these criteria include the causative/inchoative distinction, andsemantic features of particular argument positions, like animacy.
Once the sense groupsfor a lemma are constructed manually, they are used in trial annotation.
If an inter-annotator agreement of approximately 90% is reached, the lemma?s sense groups areused for annotation; otherwise they are revised.
Chen and Palmer report that the sensegroups used in OntoNotes have resulted in a rise in inter-annotator agreement as wellas annotator productivity.
The third column of Table 24 shows OntoNotes groups forthe noun account.5.1.1 The SemEval-2007 English All Words Task (EAW).
For the English All Words taskat SemEval-2007, WordNet 2.1 senses were grouped by mapping them to the morecoarse-grained Oxford Dictionary of English senses.
For the training data, this mappingwas done automatically; for the test data, the mapping was done by hand (Navigli,Litkowski, and Hargraves 2007).
For our analysis, we used only lemmas that wereincluded in the test data where the mapping had been produced manually.539Computational Linguistics Volume 39, Number 3Table 24WordNet 2.1 senses of the noun account, and their groups in OntoNotes (ON) and EAW.WordNet sense WordNet ON EAWsense no.
group groupbusiness relationship: ?he asked to see the executivewho handled his account?3 1.1 5report: ?by all accounts they were a happy couple?
8 1.2 2explanation: ?I expected a brief account?
4 1.2 2history, story: ?he gave an inaccurate account ofthe plot [...]?1 1.3 2report, story: ?the account of his speech [...] madethe governor furious?2 1.3 2account statement: ?they send me an accountingevery month?7 1.4 4bill: ?send me an account of what I owe?
9 1.4 4score: ?don?t do it on my account?
5 1.5 3importance: ?a person of considerable account?
6 1.6 3the quality of taking advantage: ?she turned herwriting skills to good account?10 1.7 1Column (4) of Table 24 shows EAW groups for the noun account.24 The two resourceslargely agree in the groupings for account.
But whereas EAW groups senses 1, 2, 4, and8 together, OntoNotes splits those senses into two groups.5.2 Does WSsim Annotation Conform to Existing Sense Groups?In the WSsim annotation, we have used the fine-grained senses of WordNet 3.0.
Butannotators were free to give high ratings for a sentence to more than one sense.
Soit is possible that they implicitly used more coarse-grained sense distinctions.
In thisand the following section, we will explore the question of whether, and to what extent,WSsim annotators used implicit coarse-grained sense groups.
In this section, we willfirst ask whether their annotation matched the sense groups of either OntoNotes orEAW.
OntoNotes and EAW differ in the lemmas they cover.
Also, as we saw earlier,when they both cover a lemma, they do not always agree in the sense groups that theypropose.
So we study the agreement of WSsim annotation with the two sense groupingsseparately.
We only study the lemmas which are in both the WSsim data and in eitherOntoNotes or the EAW test data, listed in Table 25.Tables 26 and 27 show the results.
Table 26 looks at the number of sentences wheretwo senses both had high ratings, but are in different groupings in either OntoNotes orEAW.
The first row shows how many sentences there were where two senses receiveda judgment of ?
3, but the two senses were not in a common OntoNotes/EAW group.The second row shows the same for judgments ?
4, and the last row for judgmentsof 5 only.
In general, the percentages are higher for EAW than for OntoNotes.
This isnot due to any difference in granularity between the two resources.
The EAW sensegroups encompass on average 2.3 fine-grained senses for the R1 lemmas and 2.6 for the24 The table shows the EAW groups of the WordNet senses, but the group numbering is our own for ease ofreference as no labels are given in EAW.540Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextTable 25Lemmas in R1 and R2 WSsim that have coarse-grained mappings in OntoNotes and SemEval2007 EAW.R1 R2lemma ON EAW ON EAWaccount.n?
?add.v?ask.v?
?call.v?
?coach.n?different.a?dismiss.v?
?fire.v?fix.v?hold.v?
?lead.n?new.a?order.v?
?paper.n?rich.a?shed.v?suffer.v?
?win.v?
?Table 26Sentences that have positive judgments for senses in different coarse groupings: percentage, andabsolute number in parentheses.
J.
= WSsim judgment, averaged over annotators.OntoNotes EAWJ.
Rd.
1 Rd.
2 Rd.
1 Rd.
2?
3 28% (42) 52% (52) 78% (157) 62% (50)?
4 13% (19) 16% (16) 41% (82) 22% (18)5 3% (5) 3% (3) 8% (17) 6% (5)Table 27Sentences that have widely different judgments for pairs of senses in the same coarse grouping:percentage, and absolute number in parentheses.
J1 = WSsim judgment for the sense with thelower rating, averaged over annotators; J2 = averaged WSsim judgment for the higher-rated ofthe two senses.OntoNotes EAWJ1 J2 Rd.
1 Rd.
2 Rd.
1 Rd.
2?
2 ?
4 35% (52) 30% (30) 20% (39) 60% (48)?
2 5 11% (16) 4% (4) 2% (4) 15% (12)541Computational Linguistics Volume 39, Number 3R2 lemmas, and for OntoNotes the mean group sizes are 2.3 (R1) and 2.4 (R2).
Morelikely it is due to the individual lemmas.
We observe that ratings of ?similar?
or higherare frequent.
In all conditions except WSsim-1/OntoNotes, we find percentages over50%.
On the other hand, there are many fewer sentences where two senses receivedjudgments of ?very similar?
or ?identical?
but were not in the same OntoNotes orEAW group, but these cases do exist.
For example, there were five sentences with thetarget dismiss.v which in WSsim received an average judgment of 4 or 5 for senses fromtwo different OntoNotes groups, 1.1 and 1.2.
As dismiss is an R2 lemma, for whichonly 10 sentences were annotated, this means that this phenomenon was found inhalf the sentences annotated.
The two sense groups are related: One is a literal, theother a metaphorical, use of the verb.
OntoNotes group 1.1 is defined as ?refuse to giveconsideration to something or someone,?
and group 1.2 is ?discharge, let go, persuadeto leave, send away.?
One such sentence was the second sentence in Table 13.Table 27 lists the number of sentences where two senses in the same OntoNotesor EAW grouping received widely different ratings in the WSsim annotation.
The firstrow shows how many sentences there were where one sense received a rating of ?
2and another sense from the same OntoNotes or EAW group had a rating of ?
4.
Thesecond row shows the same for sense pairs in the same coarse-grained group whereone received a rating of ?
2 and the other the highest possible rating of 5.
(Note thatthe table considers judgments averaged over all annotators, so this row counts onlysentences where all annotators agreed on the highest rating.)
An example of such a caseis Rich people manage their money well.
In WSsim the first sense in WordNet (possessingmaterial wealth) received an average score of 5 (i.e.
a unanimous verdict), whereas allother senses received a score of less than 2.
This included the third sense (of great worthor quality; ?a rich collection of antiques?
), which had an average of 1.625, and sense 8(suggestive of or characterized by great expense; ?a rich display?)
with an average of 1.125.Both senses 3 and 8 are in the same group as sense 1 in EAW.These are sentences where the WSsim annotation suggests a more fine-grainedanalysis than the OntoNotes and EAW groups offer.
The percentages are substantial:For the more inclusive analysis in the first row, the numbers are between 20% and 60%of all sentences, and between 2% and 15% of sentences even fall into the more restrictivecase in the second row.
There is no clear trend in whether we see more of this type ofdisagreement for OntoNotes or for EAW, or for the first or the second round of WSsimannotation.We see that there are a considerable number of sentences where either two sensesfrom the same OntoNotes or EAW group have received diverging WSsim ratings, ortwo senses from different groups have received high ratings.
In this way, the WSsimannotation can be used to scrutinize sense groupings: If one aim of the sense groupingsis to form groups that would match intuitive sense judgments by untrained subjects,then WSsim annotation would suggest that the senses of dismiss.v that correspond to?dismiss a person?
and ?dismiss an idea?
may be too close together to be placed indifferent groups.5.3 Inducing Sense Relatedness from WSsim AnnotationIn the WSsim annotation, annotators have annotated each occurrence of a target wordwith a rating for each of the WordNet senses for the target, as illustrated in Tables 6?8.This allows us, conversely, to chart the ratings that a WordNet sense received acrossall sentences.
Table 28 shows this chart for two senses of the noun account.
In thetable, ratings are averaged across all annotators.
In this case, the averaged ratings are542Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextTable 28WSsim ratings for two senses of the noun account for 10 annotated sentences (averaged overannotators).WordNet Sentencesense 1 2 3 4 5 6 7 8 9 101 1.00 2.25 1.13 4.25 1.13 1.0 1.13 1.13 1.13 4.254 1.50 3.00 1.25 2.88 1.50 1.50 1.63 1.00 1.38 3.88Figure 4Correlation between sense pairs: Distribution of rho values (Spearman?s rho).similar for the two senses: They tend to be high for the same sentences, and low for thesame sentences.
In general, senses that are closely related should tend to receive similarratings: high on the same sentences, and low on the same sentences, as illustrated forthe two senses in Table 28.This then means that we can test the correlation on the ratings for two senses to seeif the WSsim annotators perceived them to be similar.
We compute correlation for anypair of senses for a common lemma, again using Spearman?s rho.25 Figure 4 shows thedistribution of rho values obtained for all the sense pairs, as histograms for R1 (left)and R2 (right).
When two senses are strongly positively correlated, this means thatthe annotators likely viewed them as similar.
When two senses are strongly negativelycorrelated, this means they are probably so different that they tend never to be assignedhigh ratings for the same sentences.
We see that in both rounds, there were roughly asmany positive correlations as negative correlations.
In R1, the rho values seem more orless equally distributed over the range from ?1 to 1.
In R2, there were more annotatorsand the distribution is closer to a normal distribution with more rho values close to 0.25 We exclude senses that received a uniform rating of 1 on all items.
For R1 there were no such cases andfor R2 there were only 14 out of a total of 275 senses.543Computational Linguistics Volume 39, Number 3We have shown the OntoNotes and EAW sense groups for the noun account.
We cannow look at the WSsim-derived correlations for the same lemma, shown in Figure 5.
Thefirst row in each box shows the WordNet sense number, and the second row shows theOntoNotes and EAW sense groups.
All three labels are those used in Table 24.
Each edgerepresents a correlation in the WSsim annotation.
To avoid clutter, only correlationswith rho ?
0.5 are included, and a sense is only shown if it is correlated with any othersense.
Edge thickness corresponds to the value of the correlation coefficient rho betweeneach two senses; rho is also annotated on the edges.
The first thing to note is that WSsim-based correlation does not give us sense groups.
Correlations are of different strengths,and different cutoffs would result in different link graphs.
Even for the chosen cutoffof rho = 0.5, the correlations do not induce cliques (in the graph-theoretic sense).
Forexample, the sixth sense of account shows a correlation of rho ?
0.5 with the eighthsense, but not with any of the other senses to which the eighth sense is linked.
Thefigure also shows that there are some senses that are strongly correlated in their annota-tion but are not grouped in one or the other of the existing groupings.
For example,senses 3 (the executive who handles his account) and 7 (account statement) are stronglycorrelated, but are in different groups in OntoNotes as well as in EAW.
There are alsosenses that share the same group in one of the coarse grained inventories, but havea weak or even a negative correlation based on the WSsim annotation.
For example,Figure 5Sense correlation in the WSsim annotation for the noun account.
Showing all correlations withrho ?
0.5.
Upper row in each box: WordNet sense number.
Lower row: OntoNotes and EAWsense groups.
Edge thickness expresses strength of correlation.544Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextFigure 6Overall correlation versus annotation in single sentences: Number of sentences in which twosenses with an overall correlation ?
?
have both been annotated with a judgment of ?
j, forj = 3, 4, 5.
(Judgments averaged over annotators.
)for the lemma paper, senses 1 (a material made from cellulose pulp) and 4 (a medium forwritten communication) are in the same EAW group, but have a correlation in WSsim ofrho = ?0.52.In Section 5.2 we asked whether the many cases where WSsim annotators gavehigh ratings to more than a single sense could possibly be explained by them im-plicitly using more coarse-grained senses.
We answered this question by comparingthe WSsim annotation to OntoNotes and EAW sense groups, finding a considerablenumber of sentences where two senses received a high rating but were not from thesame sense group.
Now we can repeat the question, but try to answer it using theWSsim sense relations obtained from correlation: Is it possible that WSsim annotatorsimplicitly used more coarse-grained senses, but just not the OntoNotes or EAW sensegroups?We tested how often annotators gave ratings of at least similar (i.e., ratings ?
3) tosenses that were related at a level ?
rho, for rho ranging from ?1 to 1.
The questionthat we want to answer is: If annotators give high ratings to multiple senses on thesame sentence, is it always to senses that are strongly positively correlated, or do theysometimes pick multiple senses that are not strongly correlated, or even senses thatare negatively correlated?
The results are shown in Figure 6.
First, we can see thatthere is a sizeable number of sentences where two senses that are negatively correlatedhave both received a positive judgment.
For R1, the numbers for negatively correlatedsenses are 135 ( j ?
3), 29 ( j ?
4), and 2 ( j = 5).
For R2, the numbers of sentences arelower absolutely and in proportion, with 29 ( j ?
3), 7 ( j ?
4), and 0 ( j = 5).
It is alsointeresting to look at a less stringent threshold than rho ?
0; we can use the significancelevels p ?
0.05 and p ?
0.01 for this.
If we look at sense pairs that were not positivelycorrelated at p ?
0.05 (p ?
0.01), there were 185 (205) sentences in R1 and 54 (88)sentences in R2 where two such senses both received judgments of 3 or higher.
Notethat the significance levels of p ?
0.05, p ?
0.01 are here just arbitrary thresholds atwhich to inspect the data; they are not thresholds that determine the significance of545Computational Linguistics Volume 39, Number 3some hypothesis.26 This brings us back to the question asked above of whether theWSsim annotators implicitly used more coarse-grained senses.
If they had implicitlyused more coarse-grained senses, we would have expected to see very few cases whereunrelated senses got a high rating on the same sentence.
What we found instead wasthat such cases were relatively frequent, which implies that WSsim annotators in bothrounds ?mix and match?
senses specifically for each sentence that they evaluate.
Forexample, the senses 1 (she dismissed his advances) and 5 (I was dismissed after I gave myreport) of dismiss are negatively correlated (rho = ?0.61) yet have average judgments of3.25 and 4.125 on the second example in Table 13.5.3.1 Summary.
In this section we have analyzed the WSsim annotation in comparisonwith more coarse-grained sense repositories.
One aim was to find out whether anno-tators really used the fine granularity that the WSsim task offered or whether theyimplicitly used more coarse-grained senses.
Both by comparing the WSsim annotationto coarse-grained OntoNotes and EAW sense groups, and by comparing the WSsimannotation to the sense relations implied by WSsim, we find that annotators did makeuse of the ability to combine sense ratings in a way that was particular to each sentencethey annotated.
We also conclude that WSsim annotation can be used to evaluateOntoNotes and EAW groupings with respect to the level to which senses are intuitivelydistinguishable to untrained subjects.
Here, WSsim annotation can uncover senses indifferent groups that WSsim annotators often conflate, or senses in a single coarse groupthat WSsim annotators treat differently.6.
Usim and Sense GroupingsOne of the major motivations for the Usim task is that it allows us to examine themeanings of words without recourse to a predefined inventory.
We have demonstratedin this paper that the data from this task can be compared directly to paraphrasedata as well as to data annotated for word sense.
In the previous section we havefocused on using our WSsim data to examine existing sense groupings.
WSsim is usefulprecisely because it has sense annotations from an existing inventory, WordNet, so wecan use the graded annotations to see how these senses relate, and also relationshipsbetween coarser grained inventories with mappings to WordNet.
Usim does not capturethis information, nevertheless it might be useful as a resource for examining sensegroupings.
We can use it to examine the extent to which sense groupings keep usagestogether that have a high usage similarity according to Usim, and keep sentenceswith low usage similarity apart.
In this analysis, we use the data from R2 becausethis has Usim judgments for sentences alongside traditional word sense annotations(WSbest).
As WSbest annotation, we use the mode of the chosen senses27 (as in theanalysis in Section 4.7) for each sentence in R2, and map it to its coarse-grained sense in26 We are performing multiple tests on the same senses, which increases the likelihood of falsely assumingtwo senses to be significantly correlated at some significance level (Type I errors).
The significance levelsare only arbitrary thresholds in our case, however.
In addition, our analysis focuses on sense pairs thatare not significantly positively correlated.
For that reason, Type I errors actually reduce our estimateof the number of sentences in which two non-related senses both received high ratings.
Conversely,correcting for multiple testing makes our estimate less conservative: If we count sentences with positiveratings for sense pairs that are not positively correlated at p ?
0.05 with Bonferroni correction, the numberof sentences rises from 185 to 207 for judgments of 3 or higher.27 We perform this analysis only on sentences where there was one sense found as mode and where this hada coarse-grained mapping in either the EAW or OntoNotes resources.546Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextEAW and/or OntoNotes.
We then compute the average Usim similarity for all pairs ofsentences with the same coarse-grained sense, and compare it with the average Usimsimilarity for sentence pairs with different coarse-grained senses.
The results are shownin the first row of figures in Table 29.
We see that the OntoNotes and EAW sense groupsdo indeed partition the sentences such that pairs within the same group have high usagesimilarity (4 or above) and those in different groups have low usage similarity (2 orbelow).The second part of Table 29 performs the same analysis on the basis of individuallemmas.
A dash (?)
means that either there was no coarse mapping, or there were nosentence pairs in this category.
For example, there were no sentence pairs identifiedas having different OntoNotes groups or EAW groups for the lemma suffer.v.
For thelemmas call.v and dismiss.v, the two sense inventories give rise to the same groupings ofsentence pairs.In the table, we see many lemmas where the groupings concur with the Usimjudgments.
One example is account.n, where the sentence pairs in the same coarse groupget high average Usim values, whereas sentence pairs with different coarse groups havelow average Usim values.
There are, however, a few lemmas where the average Usimvalues indicate that either the coarse groupings might benefit from another inspection,or that the lemma has meanings with subtle relationships where grouping is not astraightforward exercise.
One example is new.a, which has the same high Usim valuesfor both same and different categories in EAW.
Another is shed.v, where the sentencesannotated with the same OntoNotes groups actually have a lower average Usim valuethan those with different groups.We can also use Usim judgments to analyze individual sense groups.
This couldbe useful in determining specific groups that might warrant further revision, or thatrepresent meanings which are simply difficult to distinguish.
To demonstrate this, weanalyzed all coarse-grained sense groups with at least one sentence pair in R2, that is, allgroups that had at least two R2 sentences whose WSbest mode mapped to that coarseTable 29Average Usim rating for R2 where WSbest annotations suggested the same or different coarsegrouping.OntoNotes EAWsame different same different4.0 1.9 4.1 2.0by lemmaaccount.n 4.0 1.6 4.0 1.5call.v 4.3 1.4 4.3 1.4coach.n 4.6 2.3 ?
?dismiss.v 3.8 2.6 3.8 2.6fire.v 4.6 1.2 ?
?fix.v 4.2 1.1 ?
?hold.v 4.5 2.0 3.8 1.9lead.v ?
?
2.9 1.5new.a ?
?
4.6 4.6order.v 4.3 1.7 ?
?rich.a ?
?
4.6 2.0shed.v 2.9 3.3 ?
?suffer.v 4.2 ?
4.2 ?547Computational Linguistics Volume 39, Number 3group.
(Naturally, due to the skewed nature of sense distributions and the fact thatwe only have ten sentences for each lemma, some groups do not meet this criterion.
)We find that the majority of groups that were analyzed have an average Usim ratingof over 4.
This is the case for 75% of the analyzed EAW groups and 76% of OntoNotesgroups.
There were, however, groups with very low values.
One example was group 1.1of shed.v in OntoNotes, with an average Usim rating of 2.9.
This group includes bothliteral senses (trees shed their leaves) and metaphorical senses (he shed his image as apushy boss) of the verb shed.
Another example is group 7 of lead.n in EAW, also withan average Usim of 2.9.
This group includes taking the lead as well as lead actor, so quitea diverse collection of usages.
Two example sentences annotated with these two sensesare shown here.
This pair had an average Usim value of 1.25.My students perform a wide variety of music and they can be found singing leadingroles in their high school and college musical productions, singing lead in rock andwedding bands, winning classical music competitions, singing at the summerconservatory of The Papermill Playhouse, and learning to sing so they can sing withlocal choirs.And as a result of President Bush?s initiative, which he took as part of the G-8Presidency, and also the other changes in which the US, UK has been in the lead, notleast in Afghanistan and Iraq, you can now feel the winds of change blowing throughthe Arab world.In the future we hope to obtain more Usim data.
When we have more data, we willinvestigate whether the groupings that Usim identifies as problematic tend to be thesame ones that require more iterations in inventory construction (Hovy et al2006).
Wealso plan to test whether groupings with low Usim ratings tend to have lower inter-tagger agreement on traditional WSD annotation.7.
Computational ModelingThe graded meaning annotation data from Usim and WSsim annotation can be used toevaluate computational models of word meaning.
In this section we summarize existingwork on modeling the R1 data, which has already been made publicly available.The WSsim data can be used to evaluate graded word meaning models as wellas traditional WSD systems.
Instead of evaluating only the highest-confidence sense of aWSD model, we can take a more nuanced look at a model?s predictions, and give credit ifit proposes multiple appropriate senses.
In Erk and McCarthy (2009) we take advantageof this fact to evaluate and compare two supervised models on the WSsim data: atraditional WSD model, and a distributional model that forms one prototype vector foreach sense of a given lemma.
Both are trained on traditional single-sense annotation, butthe prototype model does not see any negative data during training in order to avoidspurious negative data.
For training, each word occurrence is represented as either afirst-order or a second-order bag-of-words vector of its sentence.
In an evaluation usingweighted variants of precision and recall, we find that when the traditional WSD modelis credited for all the senses that it proposes, rather than only the single sense withthe highest confidence value, it does much better on both measures.
This shows thatthe model does propose multiple appropriate senses, such that its performance may beunderestimated in traditional evaluation.
As was to be expected, the prototype modelsthat do not see negative data during training have much higher recall at lower precision,for an overall better F-score (again using weighted variants of the evaluation measures).548Erk, McCarthy, and Gaylord Measuring Word Meaning in ContextThater, Fu?rstenau, and Pinkal (2010) address the WSsim data with an unsupervisedmodel.
It represents a word sense as the sum of the vectors for all synonyms in its synset,plus the vectors for all hypernyms scaled down by a factor of 10.
They also use a morecomplex, syntax-based model to derive occurrence representations.
Unfortunately theirresults are not directly comparable to Erk and McCarthy (2009) because they evaluateon a subset of the data (verb lemmas only).The Usim data, which directly describes the similarity of pairs of usages, can beused to evaluate distributional models of word meaning in context.
So far, only one typeof model has been evaluated on this data to the best of our knowledge: the clustering-based approach of Reisinger and Mooney (2010).
They use the Usim data to test to whatextent their clusters correspond to human intuitions on a word?s senses.
Their result isnegative, as a low correlation of human judgments and predictions suggests to themthat the induced clusters are not a good match for human senses.
The Usim data isparticularly interesting for a different way of evaluating distributional and vector spaceapproaches for word meaning in context.
These have been evaluated on the tasks oflexical substitution (Erk and Pado 2008; Dinu and Lapata 2010; Thater, Fu?rstenau, andPinkal 2010; Van de Cruys, Poibeau, and Korhonen 2011), information retrieval, andword sense disambiguation (Schu?tze 1998), but Usim, in contrast, offers a different andmore direct evaluation perspective.8.
ConclusionIn this paper we have explored the question of whether word meaning can be describedin a graded fashion.
Our aim has been to use annotation with graded ratings to captureuntrained speakers?
intuitions on word meaning.
Our motivation has been two-fold.
Onthe one hand we are drawing on current theories of cognition, which hold that mentalconcepts have ?fuzzy boundaries.?
On the other hand we wanted to give a basis tocurrent computational models for word meaning in context that predicts degrees ofsimilarity between word occurrences.
We have addressed this question through twonovel types of graded annotations of word meaning in context that draws on methodsfrom psycholinguistic experimentation.
WSsim obtains word sense annotations from agiven sense inventory but uses graded judgments for each sense.
Usim judges similarityof pairs of usages of the same lemma.The analysis of annotation results lets us answer our main question in the affir-mative.
Annotators can describe word meaning through graded ratings with goodinter-annotator agreement, measured through pairwise correlation.
Even though no in-depth training on sense distinctions was provided, the pairwise correlations were goodin every single case, indicating that all annotators did the tasks in a similar fashion.In both tasks, all annotators made use of the full graded scale, and did not treat thetask as binary.
The Usim annotation provides us with a means of comparing differentword meaning annotation paradigms.
We have used it to demonstrate that there isstrong correlation of these new annotations with both traditional WSD labels, and withoverlap of lexical paraphrases.
This is as we anticipated, as all of these annotations aredescribing the same phenomenon of word meaning in context through different means.In additional analysis of the WSsim annotation, we found a high proportion ofsentences (between 23% and 46%) in which multiple senses received high positivejudgments from the same annotators.
At the same time, annotators used the WSsimratings in a nuanced and fine-grained fashion, sometimes assigning high ratings on thesame sentence to two senses that overall patterned very differently.
Analyzing Usimannotation, we found that all annotators?
ratings obey the triangle inequality in almost549Computational Linguistics Volume 39, Number 3all cases.
This can be taken as a measure of intra-annotator consistency on the task.It also means that current distributional approaches to word meaning in context arejustified in viewing usage similarity as metric.
Triangle inequality can be used to checkthe validity of future Usim annotation.We do not propose that either one of our annotations is a panacea for evaluationof systems that represent word meaning in context, but we argue that they providedata sets that better reflect the fluid nature of word meaning and allow us to evaluatequestions related to word meaning in a new fashion.
In this paper, we have usedboth WSsim and Usim data to analyze existing coarse-grained sense inventories.
Wehave demonstrated that it is often not straightforward to group sentences into disjointsenses, depending on the lemma.
We have also shown how both WSsim and Usim stylejudgments can be used to identify problematic lemmas, as well as sense groupings thatmay warrant another inspection to check whether they match naive speakers?
intuitivejudgments.
The graded annotation can also be used to identify lemmas whose usagesare difficult to group into clear distinct senses.
This information can in the future beused to handle such lemmas differently when making sense inventories, in annotation,and in computational systems.An important next question to consider is the use of WSsim and Usim data to eval-uate computational models of word meaning.
As we have shown (Erk and McCarthy2009), WSsim data can be used to evaluate traditional WSD systems in a graded fashion.We plan to do a more large-scale evaluation to assess to what extent the performanceof current WSD systems is underestimated.
Also, fine-grained WSsim annotation can beused for a comparison of fine-grained and coarse-grained traditional WSD systems.
Wehave also shown (Erk and McCarthy 2009) that WSsim can be used to evaluate gradedword sense assignment systems.
Although we used a supervised setting, however, wetrained on traditional sense annotation.
We plan to collect more WSsim annotation inorder to be able to train word sense assignment systems on graded data, for example,using a regression model.In the same vein, we will extend the available Usim data to cover many moresentences by using crowdsourcing.
The use of Usim for supervised training of wordmeaning models is particularly interesting as all existing usage similarity models areunsupervised; given previous results in WSD, we can expect that supervision willimprove the performance of models of usage meaning.
One way of using Usim datain training is to learn a similarity metric.
Metric learning (see, e.g., Davis et al2007)induces a distance measure from given constraints stating similarity or dissimilarity ofitems.Our novel graded annotation frameworks, WSsim and Usim, are validated boththrough good agreement between those data sets themselves, as well as good agreementbetween those data sets and traditional word sense annotation and lexical substitutions.Because all labeling schemes provide comparable results, this allows different ways ofevaluating systems providing different perspectives on system output.
Furthermore,the different paradigms may suit different types of systems.
Lexical substitution tasks(McCarthy 2002; McCarthy and Navigli 2009) are particularly useful where the ap-plication being considered would benefit from lexical paraphrasing, for example, textsimplification, summarization, or query expansion in information retrieval.
WSsim isclosest to the traditional methodology (WSbest) and would suit systems needing tooutput WordNet sense labels, for example, because they want to exploit the semanticrelations in WordNet for tasks such as inferencing or producing lexical chains.
UnlikeWSbest, it avoids a winner-takes-all approach and allows for more nuanced sensetagging.
Usim is application-independent.
It allows for evaluation of systems that relate550Erk, McCarthy, and Gaylord Measuring Word Meaning in Contextusages, whether into clusters or simply on a continuum.
It could, for example, be usedas a resource-independent gold standard for word sense induction by calculating thewithin and across class similarities.
Aside from its use as an enabling technology withina natural language processing application, a system that performs well at the Usim taskmay be useful in its own right.
For example, it could be used to enable lexicographersto work on groups of examples that reflect similar meanings, or find further examplesclose to the one being scrutinized.AcknowledgmentsThe annotation was funded by a UK RoyalSociety Dorothy Hodgkin Fellowship toDiana McCarthy.
This work was supportedby National Science Foundation grantIIS-0845925 for Katrin Erk.
We are gratefulto Huw McCarthy for implementing theinterface for round 2 of the annotation.
Wethank the anonymous reviewers for manyhelpful comments and suggestions.ReferencesAgirre, Eneko and Philip Edmonds,editors.
2007.
Word Sense Disambiguation:Algorithms and Applications.
Springer,Dordrecht.Agirre, Eneko, Llu?
?s Ma`rquez, and RichardWicentowski, editors.
2007.
Proceedingsof the Fourth International Workshop onSemantic Evaluations (SemEval-2007).Prague.Baroni, Marco and Roberto Zamparelli.
2010.Nouns are vectors, adjectives are matrices:Representing adjective-noun constructionsin semantic space.
In Proceedings of the 2010Conference on Empirical Methods in NaturalLanguage Processing, pages 1,183?1,193,Cambridge, MA.Brown, Susan.
2008.
Choosing sensedistinctions for WSD: Psycholinguisticevidence.
In Proceedings of ACL-08: HLT,Short Papers (Companion Volume),pages 249?252, Columbus, OH.Brown, Susan.
2010.
Finding Meaning:Sense Inventories for Improved Word SenseDisambiguation.
Ph.D. thesis, Universityof Colorado at Boulder.Burchardt, Aljoscha, Katrin Erk, AnnetteFrank, Andrea Kowalski, Sebastian Pado,and Manfred Pinkal.
2006.
The SALSAcorpus: A German resource for lexicalsemantics.
In Proceedings of the FifthInternational Conference on LanguageResources and Evaluation (LREC 2006),pages 969?974, Genoa.Carpuat, Marine and Dekai Wu.
2007a.
Howphrase sense disambiguation outperformsword sense disambiguation for statisticalmachine translation.
In Proceedingsof the 11th Conference on Theoretical andMethodological Issues in Machine Translation(TMI 2007), pages 43?52, Skovde.Carpuat, Marine and Dekai Wu.
2007b.Improving statistical machine translationusing word sense disambiguation.
InProceedings of the 2007 Joint Conference onEmpirical Methods in Natural LanguageProcessing and Computational NaturalLanguage Learning (EMNLP-CoNLL 2007),pages 61?72, Prague.Chen, Jinying and Martha Palmer.
2009.Improving English verb sensedisambiguation performance withlinguistically motivated features and clearsense distinction boundaries.
Journal ofLanguage Resources and Evaluation (SpecialIssue on SemEval-2007), 43:181?208.Coecke, Bob, Mehrnoosh Sadrzadeh, andStephen Clark.
2010.
Mathematicalfoundations for a compositionaldistributed model of meaning.
LambekFestschrift, Linguistic Analysis, 36:345?384.Coleman, Linda and Paul Kay.
1981.Prototype semantics: The English word?lie.?
Language, 57:26?44.Copestake, Ann and Ted Briscoe.
1995.Semi-productive polysemy and senseextension.
Journal of Semantics, 12:15?67.Cruse, D. A.
1995.
Polysemy and relatedphenomena from a cognitive linguisticviewpoint.
In Philip Saint-Dizier andEvelyne Viegas, editors, ComputationalLexical Semantics.
Cambridge UniversityPress, pages 33?49.Davis, Jason, Brian Kulis, Prateek Jain,Suvrit Sra, and Inderjit Dhillon.
2007.Information-theoretic metric learning.In Proceedings of the 24th InternationalConference on Machine Learning,pages 209?216, Corvallis, OR.Deschacht, Koen and Marie-Francine Moens.2009.
Semi-supervised semantic rolelabeling using the Latent Words LanguageModel.
In Proceedings of EMNLP,pages 21?29, Singapore.Dinu, Georgiana and Mirella Lapata.
2010.Measuring distributional similarity incontext.
In Proceedings of the 2010551Computational Linguistics Volume 39, Number 3Conference on Empirical Methods in NaturalLanguage Processing, pages 1,162?1,172,Cambridge, MA.Edmonds, Philip and Scott Cotton, editors.2001.
Proceedings of the SensEval-2Workshop.
Toulouse.
See http://www.sle.sharp.co.uk/senseval.Erk, Katrin and Diana McCarthy.
2009.Graded word sense assignment.
InProceedings of the 2009 Conference onEmpirical Methods in Natural LanguageProcessing, pages 440?449, Singapore.Erk, Katrin, Diana McCarthy, and NicholasGaylord.
2009.
Investigations on wordsenses and word usages.
In Proceedingsof the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th InternationalJoint Conference on Natural LanguageProcessing of the AFNLP, pages 10?18,Suntec.Erk, Katrin and Sebastian Pado.
2008.
Astructured vector space model for wordmeaning in context.
In Proceedings ofEMNLP-08, pages 897?906, Waikiki, HI.Erk, Katrin and Sebastian Pado.
2010.Exemplar-based models for word meaningin context.
In Proceedings of the ACL 2010Conference Short Papers, pages 92?97,Uppsala.Erk, Katrin and Carlo Strapparava, editors.2010.
Proceedings of the 5th InternationalWorkshop on Semantic Evaluation(SemEval).Uppsala.Frazier, Lyn and Keith Rayner.
1990.
Takingon semantic commitments: Processingmultiple meanings vs. multiple senses.Journal of Memory and Language,29:181?200.Gentner, Dedre and Virginia Gunn.
2001.Structural alignment facilitates thenoticing of differences.
Memory andCognition, 21:565?577.Gentner, Dedre and Arthur Markman.
1997.Structural alignment in analogy andsimilarity.
American Psychologist, 52:45?56.Grefenstette, Edward and MehrnooshSadrzadeh.
2011.
Experimentalsupport for a categorical compositionaldistributional model of meaning.In Proceedings of the 2011 Conference onEmpirical Methods in Natural LanguageProcessing, pages 1,394?1,404, Edinburgh.Hampton, James A.
1979.
Polymorphousconcepts in semantic memory.
Journalof Verbal Learning and Verbal Behavior,18:441?461.Hampton, James A.
2007.
Typicality, gradedmembership, and vagueness.
CognitiveScience, 31:355?384.Hanks, Patrick.
2000.
Do word meaningsexist?
Computers and the Humanities,34(1?2):205?215.Hovy, Eduard H., Mitchell Marcus, MarthaPalmer, Lance Ramshaw, and RalphWeischedel.
2006.
OntoNotes: The 90%solution.
In Proceedings of the HumanLanguage Technology Conference of the NorthAmerican Chapter of the ACL (NAACL-2006),pages 57?60, New York.Ide, Nancy and Yorick Wilks.
2006.
Makingsense about sense.
In Eneko Agirre andPhilip Edmonds, editors, Word SenseDisambiguation, Algorithms and Applications.Springer, Dordrecht, pages 47?73.Kilgarriff, Adam.
1992.
Polysemy.
Ph.D.thesis, University of Sussex.Kilgarriff, Adam.
1997.
I don?t believe inword senses.
Computers and the Humanities,31(2):91?113.Kilgarriff, Adam.
2006.
Word senses.In Eneko Agirre and Philip Edmonds,editors, Word Sense Disambiguation:Algorithms and Applications.
Springer,Dordrecht, pages 29?46.Kilgarriff, Adam and Martha Palmer,editors.
2000.
Senseval: Special Issue of theJournal Computers and the Humanities,volume 34(1?2).
Kluwer, Dordrecht.Kilgarriff, Adam and Joseph Rosenzweig.2000.
Framework and results for EnglishSenseval.
Computers and the Humanities,34(1-2):15?48.Kintsch, Walter.
2007.
Meaning in context.
InT.
K. Landauer, D. McNamara, S. Dennis,and W. Kintsch, editors, Handbook of LatentSemantic Analysis.
Erlbaum, Mahwah, NJ,pages 89?105.Klein, Devorah and Gregory Murphy.
2001.The representation of polysemous words.Journal of Memory and Language,45:259?282.Klein, Devorah and Gregory Murphy.
2002.Paper has been my ruin: Conceptualrelations of polysemous senses.
Journal ofMemory and Language, 47:548?570.Klepousniotou, Ekaterini.
2002.
Theprocessing of lexical ambiguity:Homonymy and polysemy in the mentallexicon.
Brain and Language, 81:205?223.Klepousniotou, Ekaterini, Debra Titone, andCaroline Romero.
2008.
Making sense ofword senses: The comprehension ofpolysemy depends on sense overlap.Journal of Experimental Psychology: Learning,Memory, and Cognition, 34(6):1,534?1,543.Krishnamurthy, Ramesh and DianeNicholls.
2000.
Peeling an onion: Thelexicographers?
experience of manual552Erk, McCarthy, and Gaylord Measuring Word Meaning in Contextsense-tagging.
Computers and theHumanities, 34(1-2):85?97.Landauer, Thomas and Susan Dumais.
1997.A solution to Plato?s problem: The LatentSemantic Analysis theory of acquisition,induction, and representation ofknowledge.
Psychological Review,104:211?240.Landes, Shari, Claudia Leacock, andRandee Tengi.
1998.
Building semanticconcordances.
In Christiane Fellbaum,editor, WordNet: An Electronic LexicalDatabase.
The MIT Press, Cambridge, MA.Lefever, Els and Ve?ronique Hoste.
2010.Semeval-2010 task 3: Cross-lingual wordsense disambiguation.
In Proceedings of the5th International Workshop on SemanticEvaluation, pages 15?20, Uppsala.McCarthy, Diana.
2002.
Lexical substitutionas a task for wsd evaluation.
In Proceedingsof the ACL Workshop on Word SenseDisambiguation: Recent Successes andFuture Directions, pages 109?115,Philadelphia, PA.McCarthy, Diana and Roberto Navigli.
2007.SemEval-2007 task 10: English lexicalsubstitution task.
In Proceedings of the4th International Workshop on SemanticEvaluations (SemEval-2007), pages 48?53,Prague.McCarthy, Diana and Roberto Navigli.
2009.The English lexical substitution task.Language Resources and Evaluation SpecialIssue on Computational Semantic Analysisof Language: SemEval-2007 and Beyond,43(2):139?159.McNamara, Timothy P. 2005.
SemanticPriming: Perspectives from Memory and WordRecognition.
Psychology Press, New York.Mihalcea, Rada and Timothy Chklovski.2003.
Open Mind Word Expert: Creatinglarge annotated data collections with webusers?
help.
In Proceedings of the EACL 2003Workshop on Linguistically AnnotatedCorpora (LINC 2003), pages 53?60,Budapest.Mihalcea, Rada, Timothy Chklovski, andAdam Kilgarriff.
2004.
The SENSEVAL-3English lexical sample task.
In RadaMihalcea and Phil Edmonds, editors,Proceedings SENSEVAL-3 SecondInternational Workshop on EvaluatingWord Sense Disambiguation Systems,pages 25?28, Barcelona.Mihalcea, Rada and Phil Edmonds, editors.2004.
Proceedings SENSEVAL-3 SecondInternational Workshop on EvaluatingWord Sense Disambiguation Systems,Barcelona.Mihalcea, Rada, Ravi Sinha, and DianaMcCarthy.
2010.
Semeval-2010 task 2:Cross-lingual lexical substitution.
InProceedings of the 5th InternationalWorkshop on Semantic Evaluation,pages 9?14, Uppsala.Miller, George A., Claudia Leacock,Randee Tengi, and Ross T Bunker.1993.
A semantic concordance.
InProceedings of the ARPA Workshopon Human Language Technology,pages 303?308, Plainsboro, NJ.Mitchell, Jeff and Mirella Lapata.
2008.Vector-based models of semanticcomposition.
In Proceedings of ACL-08:HLT, pages 236?244, Columbus, OH.Mitchell, Jeff and Mirella Lapata.
2010.Composition in distributional modelsof semantics.
Cognitive Science,34(8):1388?1429.Moon, Taesun and Katrin Erk.
In press.An inference-based model of wordmeaning in context as a paraphrasedistribution.
ACM Transactions onIntelligent Systems and Technologyspecial issue on paraphrasing.Murphy, Gregory L. 1991.
Meaning andconcepts.
In Paula Schwanenflugel, editor,The Psychology of Word Meanings.
LawrenceErlbaum Associates, Mahwah, NJ,pages 11?35.Murphy, Gregory L. 2002.
The Big Book ofConcepts.
MIT Press, Cambridge, MA.Navigli, Roberto.
2009.
Word sensedisambiguation: a survey.
ACMComputing Surveys, 41(2):1?69.Navigli, Roberto, Kenneth C. Litkowski,and Orin Hargraves.
2007.
SemEval-2007task 7: Coarse-grained English all-wordstask.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations(SemEval-2007), pages 30?35, Prague.Palmer, Martha, Hoa Trang Dang, andChristiane Fellbaum.
2007.
Makingfine-grained and coarse-grained sensedistinctions, both manually andautomatically.
Natural LanguageEngineering, 13:137?163.Passonneau, Rebecca, Ansaf Salleb-Aouissi,Vikas Bhardwaj, and Nancy Ide.
2010.Word sense annotation of polysemouswords by multiple annotators.
InProceedings of LREC-7, pages 3,244?3,249,Valleta.Pickering, Martin and Steven Frisson.
2001.Processing ambiguous verbs: Evidencefrom eye movements.
Journal ofExperimental Psychology: Learning,Memory, and Cognition, 27:556?573.553Computational Linguistics Volume 39, Number 3Pradhan, Sameer, Edward Loper, DmitriyDligach, and Martha Palmer.
2007.Semeval-2007 task 17: English lexicalsample, SRL and all words.
In 4thInternational Workshop on SemanticEvaluations (SemEval-4) at ACL-2007,pages 87?92, Prague.Preiss, Judita and David Yarowsky, editors.2001.
Proceedings of Senseval-2 SecondInternational Workshop on Evaluating WordSense Disambiguation Systems, Toulouse.Pustejovsky, James.
1991.
The generativelexicon.
Computational Linguistics,17(4):409?441.Reddy, Siva, Ioannis P. Klapaftis, DianaMcCarthy, and Suresh Manandhar.
2011.Dynamic and static prototype vectors forsemantic composition.
In Proceedings of The5th International Joint Conference on NaturalLanguage Processing 2011 (IJCNLP 2011),pages 210?218, Chiang Mai.Reisinger, Joseph and Raymond J. Mooney.2010.
Multi-prototype vector-space modelsof word meaning.
In Proceedings of HumanLanguage Technologies: The 11th AnnualConference of the North American Chapter ofthe Association for Computational Linguistics,pages 109?117, Los Angeles, CA.Resnik, Philip and David Yarowsky.
2000.Distinguishing systems and distinguishingsenses: New evaluation methods for wordsense disambiguation.
Natural LanguageEngineering, 5(3):113?133.Rosch, Eleanor.
1975.
Cognitiverepresentations of semantic categories.Journal of Experimental Psychology: General,104:192?233.Rosch, Eleanor and Carolyn B. Mervis.
1975.Family resemblance: Studies in the internalstructure of categories.
CognitivePsychology, 7:573?605.Schu?tze, Hinrich.
1998.
Automatic wordsense discrimination.
ComputationalLinguistics, 24(1):97?123.Senseval-2.
2001.
Web page:http://www.sle.sharp.co.uk/senseval2.Snyder, Benjamin and Martha Palmer.2004.
The English all-words task.
In3rd International Workshop on SemanticEvaluations (SensEval-3) at ACL-2004,pages 41?43, Barcelona.Socher, Richard, Eric H. Huang, JeffreyPennin, Andrew Y. Ng, and Christopher D.Manning.
2011.
Dynamic pooling andunfolding recursive autoencoders forparaphrase detection.
In Advances inNeural Information Processing Systems 24,pages 801?809, Grenada.Stokoe, Christopher.
2005.
Differentiatinghomonymy and polysemy in informationretrieval.
In Proceedings of HLT/EMNLP-05,pages 403?410, Vancouver.Taylor, John R. 2003.
Linguistic Categorization.Oxford University Press, New York.Thater, Stefan, Hagen Fu?rstenau, andManfred Pinkal.
2010.
Contextualizingsemantic representations usingsyntactically enriched vector models.In Proceedings of the 48th Annual Meetingof the Association for ComputationalLinguistics, pages 948?957, Uppsala.Tuggy, David H. 1993.
Ambiguity, polysemyand vagueness.
Cognitive Linguistics,4(2):273?290.Tversky, Amos.
1977.
Features of similarity.Psychological Review, 84(4):327?352.Tversky, Amos and Itamar Gati.
1982.Similarity, separability, and the triangleinequality.
Psychological Review,89(2):123?154.Van de Cruys, Tim, Thierry Poibeau, andAnna Korhonen.
2011.
Latent vectorweighting for word meaning in context.In Proceedings of the 2011 Conferenceon Empirical Methods in NaturalLanguage Processing, pages 1,012?1,022,Edinburgh.Washtell, Justin.
2010.
Expectation vectors:A semiotics inspired approach togeometric lexical-semantic representation.In Proceedings of the 2010 Workshop onGEometrical Models of Natural LanguageSemantics, pages 45?50, Uppsala.Williams, John.
1992.
Processing polysemouswords in context: Evidence for interrelatedmeanings.
Journal of PsycholinguisticResearch, 21:193?218.Zhong, Zhi and Hwee Tou Ng.
2010.
Itmakes sense: A wide-coverage wordsense disambiguation system for freetext.
In Proceedings of the ACL 2010System Demonstrations, pages 78?83,Uppsala.Zhong, Zhi, Hwee Tou Ng, and Yee SengChan.
2008.
Word sense disambiguationusing OntoNotes: An empirical study.In Proceedings of the 2008 Conferenceon Empirical Methods in NaturalLanguage Processing, pages 1,002?1,010,Honolulu, HI.554
