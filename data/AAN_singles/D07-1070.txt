Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
667?677, Prague, June 2007. c?2007 Association for Computational LinguisticsBootstrapping Feature-Rich Dependency Parsers with Entropic PriorsDavid A. Smith and Jason EisnerDepartment of Computer ScienceJohns Hopkins UniversityBalitmore, MD 21218, USA{dasmith,eisner}@jhu.eduAbstractOne may need to build a statistical parser for a new language,using only a very small labeled treebank together with rawtext.
We argue that bootstrapping a parser is most promisingwhen the model uses a rich set of redundant features, as in re-cent models for scoring dependency parses (McDonald et al,2005).
Drawing on Abney?s (2004) analysis of the Yarowskyalgorithm, we perform bootstrapping by entropy regulariza-tion: we maximize a linear combination of conditional likeli-hood on labeled data and confidence (negative Re?nyi entropy)on unlabeled data.
In initial experiments, this surpassed EMfor training a simple feature-poor generative model, and alsoimproved the performance of a feature-rich, conditionally esti-mated model where EM could not easily have been applied.
Forour models and training sets, more peaked measures of con-fidence, measured by Re?nyi entropy, outperformed smootherones.
We discuss how our feature set could be extended withcross-lingual or cross-domain features, to incorporate knowl-edge from parallel or comparable corpora during bootstrapping.1 MotivationIn this paper, we address the problem of bootstrap-ping new statistical parsers for new languages, gen-res, or domains.Why is this problem important?
Many applica-tions of multilingual NLP require parsing in orderto extract information, opinions, and answers fromtext, and to produce improved translations.
Yetan adequate labeled training corpus?a large tree-bank of manually constructed parse trees of typi-cal sentences?is rarely available and would be pro-hibitively expensive to develop.We show how it is possible to train instead froma small hand-labeled treebank in the target domain,together with a large unannotated collection of in-domain sentences.
Additional resources such asparsers for other domains or languages can be in-tegrated naturally.Dependency parsing is important as a key com-ponent in leading systems for information extrac-tion (Weischedel, 2004)1 and question answering(Peng et al, 2005).
These systems rely on edgesor paths in dependency parse trees to define their ex-traction patterns and classification features.
Parsingis also key to the latest advances in machine transla-tion, which translate syntactic phrases (Galley et al,2006; Marcu et al, 2006; Cowan et al, 2006).2 Our ApproachOur approach rests on three observations:?
Recent ?feature-based?
parsing models are anexcellent fit for bootstrapping, because theparse is often overdetermined by many redun-dant features.?
The feature-based framework is flexibleenough to incorporate other sources of guid-ance during training or testing?such as theknowledge contained in a parser for anotherlanguage or domain.?
Maximizing a combination of likelihood on la-beled data and confidence on unlabeled data isa principled approach to bootstrapping.2.1 Feature-Based ParsingMcDonald et al (2005) introduced a simple, flexi-ble framework for scoring dependency parses.
Eachdirected edge e in the dependency tree is describedwith a high-dimensional feature vector f(e).
Theedge?s score is the dot product f(e) ?
?, where ?
is alearned weight vector.
The overall score of a depen-dency tree is the sum of the scores of all edges in thetree.1Ralph Weischedel (p.c.)
reports that this system?s perfor-mance degrades considerably when only phrase chunking isavailable rather than full parsing.667Given an n-word input sentence, the parser beginsby scoring each of the O(n2) possible edges, andthen seeks the highest-scoring legal dependency treeformed by any n?
1 of these edges, using an O(n3)dynamic programming algorithm (Eisner, 1996) forprojective trees.
For non-projective parsing, O(n3),or with some trickery O(n2), greedy algorithms ex-ist (Chu and Liu, 1965; Edmonds, 1967; Gabow etal., 1986).The feature function f may pay attention to manyproperties of the directed edge e. Of course, featuresmay consider the parent and child words connectedby e, and their parts of speech.2 But some featuresused by McDonald et al (2005) also consider theparts of speech of words adjacent to the parent andchild, or between the parent and child, as well as thenumber of words between the parent and child.
Ingeneral, these features are not available in a genera-tive model such as a PCFG.Although feature-based models are often trainedpurely discriminatively, we will see in ?2.6 how totrain them to model conditional probabilities.2.2 Feature-Based Parsing and BootstrappingThe above parsing model is robust, thanks to itsmany features.
On the Penn Treebank WSJ sections02?21, for example, McDonald?s parser extracts 5.5million feature types from supervised edges alone,with about 120 feature tokens firing per edge.
Thehighest-scoring parse tree represents a consensusamong all features on all prospective edges.
Even ifa prospective edge has some discouraging features(i.e., with negative or zero weights), it may still havea relatively high score thanks to its other features.Furthermore, even if the edge has a low total score,it may still appear in the consensus parse if the al-ternatives are even worse or are incompatible withother high-scoring edges.Put another way, the parser is not able to includehigh-scoring features or edges independently of oneanother.
Selecting a good feature means acceptingall other features on that edge.
It also means reject-ing various other edges, because of the global con-straints that a legal parse tree must give each wordonly one parent and must be free of cycles and, in2Note that since we are not trying to predict parts of speech,we treat the output of one or more automatic taggers as yet moreinputs to edge feature functions.the projective case, crossings.Our observation is that this situation is ideal forso-called ?bootstrapping,?
?co-training,?
or ?min-imally supervised?
learning methods (Yarowsky,1995; Blum and Mitchell, 1998; Yarowsky and Wi-centowski, 2000).
Such methods should thrive whenthe right answer is overdetermined owing to redun-dant features and/or global constraints.Concretely, suppose we start by training a super-vised parser on only 100 examples, using some reg-ularization method to prevent overfitting to this set.While many features might truly be relevant to thetask, only a few appear often enough in this smalltraining set to acquire significantly positive or nega-tive weights.Even this lightly trained parser may be quite sureof itself on some test sentences in a large unanno-tated corpus, when one parse scores far higher thanall others.
More generally, the parser may be sureabout part of a sentence: it may be certain that a par-ticular edge is present (or absent), because that edgetends to be present (or absent) in all high-scoringparses.Retraining the feature weights ?
on these high-confidence edges can learn about additional featuresthat are correlated with an edge?s success or failure.For example, it may now learn strong weights forlexically specific features that were never observedin the supervised training set.
The retrained parsermay now be able to confidently parse even more ofthe unannotated examples; so we can iterate the pro-cess.Our hope is that the model identifies new goodand bad edges at each step, and does so correctly.The more features and global constraints the modelhas,?
the more power it will have to discriminateamong edges even when ?
is insufficientlytrained.
(Some feature weights may be tooweak (i.e., too close to zero) because the initiallabeled set is small.)?
the more robust it will be against errors evenwhen ?
is incorrectly trained.
(Some featureweights may be too strong or have the wrongsign, because of overfitting or mistaken parsesduring bootstrapping.
)668In the former case, strong features lend their strengthto weak ones.
In the latter case, a conflict amongstrong features weakens the ones that depart fromthe consensus, or discounts the example sentence ifthere is no consensus.Previous work on parser bootstrapping has notbeen able to exploit this redundancy among features,because it has used PCFG-like models with far fewerfeatures (Steedman et al, 2003).2.3 Adaptation and Projection via FeaturesThe previous section assumed that we had a smallsupervised treebank in the target language and do-main (plus a large unsupervised corpus).
We nowconsider other, more dubious, knowledge sourcesthat might supplement or replace this small tree-bank.
In each case, we can use these knowledgesources to derive features that may?or may not?prove trustworthy during bootstrapping.Parses from a different domain.
One might havea treebank for a different domain or genre of the tar-get language.One could simply include these trees in the ini-tial supervised training, and hope that bootstrappingcorrects any learned weights that are inappropriateto the target domain, as discussed above.
In fact,McClosky et al (2006) found a similar technique tobe effective?though only in a model with a largefeature space (?PCFG + reranking?
), as we wouldpredict.However, another approach is to train a separateout-of-domain parser, and use this to generate addi-tional features on the supervised and unsupervisedin-domain data (Blitzer et al, 2006).
Bootstrappingnow teaches us where to trust the out-of-domainparser.
If our basic model has 100 features, we couldadd features 101 through 200, where for examplef123(e) = f23 ?
log P?r(e) and P?r(e) is the poste-rior edge probability according to the out-of-domainparser.
Learning that this feature has a high weightmeans learning to trust the out-of-domain parser?sdecision on edges where in-domain feature 23 fires.Even more sensibly, we could add features such asf201(e) =?10i=1 f?i(e) ?
?
?i, where f?
and ??
are the fea-ture and weight vectors for the out-of-domain parser.Learning that this feature has a high weight meanslearning to trust the out-of-domain parser?s featureweights for a particular class of features (those num-bered 1 through 10).
This addresses the intuition thatsome linguistic phenomena remain stable across do-mains.Parses of translations.
Suppose we have transla-tions into English of some of our supervised or unsu-pervised sentences.
Good probabilistic dependencyparsers already exist for English, so we run one overthe English translation.
We can now derive manyadditional features on candidate edges on the tar-get sentence.
For example, dependency edges in thetarget language of the form cposs??
p (this denotesa child-to-parent dependency with label possessor)might often correspond to dependency paths in theEnglish translation of the form p?prep??
ofpobj??
c?.
Todiscover whether this is so, we define a feature i byfi(cposs??
p) def= log?c?,p?
(Pr(c aligns with c?
)?Pr(p aligns with p?)?Pr(p?prep??
ofpobj??
c?
))(1)where c?, p?
range over word tokens in the Englishtranslation, ?of?
is a literal English word, and theprobabilities are posteriors provided by a probabilis-tic aligner and a probabilistic English parser.
Notethat this is a single feature (not a feature family pa-rameterized by c, p).
It scores any candidate edge onwhether it is aposs??
edge that seems to align to anEnglishprep??
ofpobj??
path.This method is inspired by Hwa et al (2005),who bootstrapped parsers for Spanish and Chineseby projecting dependencies from English transla-tions and training a new parser on the resulting noisytreebank.
They used only 1-best translations, 1-bestalignments, dependency paths of length 1, and nolabeled data in Spanish or Chinese.Hwa et al (2005) used a manually written post-processor to correct some of the many incorrect pro-jections.
By contrast, our framework uses the pro-jected dependencies only as one source of features.They may be overridden by other features in particu-lar cases, and will be given a high weight only if theytend to agree with other features during bootstrap-ping.
A similar soft projection of dependencies wasused in supervised machine translation by Smith andEisner (2006), who used a source sentence?s depen-dency paths to bias the generation of its translation.669Note that these bilingual features will only fireon those supervised or unsupervised sentences forwhich we have an English translation.
In particu-lar, they will usually be unavailable on the test set.However, we hope that they will seed and facilitatethe bootstrapping process, by helping us confidentlyparse some unsupervised sentences that we wouldnot be able to confidently parse without an Englishtranslation.Parses of comparable English sentences.
Worldknowledge can be useful in parsing.
Supposeyou see a French sentence that contains mangeonsand pommes, and you know that manger=eat andpomme=apple.
You might reasonably guess thatpommes is the direct object of mangeons, becauseyou know that apple is a plausible direct object foreat.
We can discover this last bit of world knowl-edge from comparable English text.
Translation dic-tionaries can themselves be induced from compara-ble corpora (Schafer and Yarowsky, 2002; Schafer,2006; Klementiev and Roth, 2006), or extractedfrom bitext or digitized versions of human-readabledictionaries if these are available.The above inference pattern can be captured byfeatures similar to those in equation (1).
For exam-ple, one can define a feature j byfi(cposs??
p) def= log Pr (p?prep??
ofpobj??
c?| p?
translates p, c?
translates c)(2)where each event in the event space is a pair (c?, p?
)of same-sentence tokens in comparable English text,all pairs being equally likely.
Thus, to estimatePr(?
| ?
), the denominator counts same-sentencetoken pairs (c?, p?)
in the comparable English cor-pus that translate into the types (c, p), and the nu-merator counts such pairs that are also related byaprep??
ofpobj??
path.
Since the lexical transla-tions and dependency paths are typically not labeledin the English corpus, a given pair must be countedfractionally according to its posterior probability ofsatisfying these conditions, given models of contex-tual translation and English parsing.33Similarly, Jansche (2005) imputes ?missing?
trees by usingcomparable corpora.2.4 Bootstrapping as OptimizationSection 2.2 assumed a relatively conventional kindof bootstrapping, where each iteration retrains themodel on the examples where it is currently mostconfident.
This kind of ?confidence thresholding?has been popular in previous bootstrapping work (ascited in ?2.2).
It attempts to maintain high accu-racy while gradually expanding coverage.
The as-sumption is that throughout the training procedure,the parser?s confidence is a trustworthy guide to itscorrectness.
Different bootstrapping procedures usedifferent learners, smoothing methods, confidencemeasures, and procedures for ?forgetting?
the label-ings from previous iterations.In his analysis of Yarowsky (1995), Abney (2004)formulates several variants of bootstrapping.
Theseare shown to increase either the likelihood of thetraining data, or a lower bound on that likelihood.
Inparticular, Abney defines a function K that is an up-per bound on the negative log-likelihood, and showshis bootstrapping algorithms locally minimize K.We now present a generalization of Abney?s Kfunction and relate it to another semi-supervisedlearning technique, entropy regularization (Brand,1999; Grandvalet and Bengio, 2005; Jiao et al,2006).
Our experiments will tune the feature weightvector, ?, to minimize our function.
We will do sosimply by applying a generic function minimizationmethod (stochastic gradient descent), rather than bycrafting a new Yarowsky-style or Abney-style itera-tive procedure for our specific function.Suppose we have examples xi and correspond-ing possible labelings yi,k.
We are trying to learna parametric model p?
(yi,k | xi).
If p?
(yi,k | xi) isa ?labeling distribution?
that reflects our uncertaintyabout the true labels, then our expected negative log-likelihood of the model isK def= ??i?kp?
(yi,k | xi) log p?
(yi,k | xi)=?i?kp?
(yi,k|xi) logp?(yi,k|xi)p?(yi,k|xi)p?
(yi,k|xi)=?iD(p?i?p?,i) + H(p?i) (3)where p?i(?
)def= p?(?
| xi) and p?,i(?
)def= p?(?
| xi).Note that K is a function not only of ?
but also670of the labeling distribution p?
; a learner might be al-lowed to manipulate either in order to decrease K.The summands of K in equation (3) can be di-vided into two cases, according to whether xi is la-beled or not.
For the labeled examples {xi : i ?
L},the labeling distribution p?i is a point distribution thatassigns all probability to the true, known label y?i .Then H(p?i) = 0.
The total contribution of these ex-amples to K simplifies to?i?L?
log p?
(y?i | xi),i.e., just the negative log-likelihood on the labeleddata.But what is the labeling distribution for the unla-beled examples {xi : i 6?
L}?
Abney simply usesa uniform distribution over labels (e.g., parses), toreflect that the label is unknown.
If his bootstrap-ping algorithm ?labels?
xi, then i moves into L andH(p?i) is thereby reduced from maximal to 0.
As aresult, a method that labels the most confident ex-amples may reduce K, and Abney shows that hismethod does so.Our approach is different: we will take the label-ing distribution p?i to be our actual current beliefp?,i, and manipulate it through changing ?
ratherthan L. L remains the original set of supervised ex-amples.
The total contribution of the unsupervisedexamples to K then simplifies to?i6?L H(p?,i).We have no reason to believe that these two con-tributions (supervised and unsupervised) should beweighted equally.
We thus introduce a multiplier ?to form the actual objective function that we mini-mize with respect to ?:4?
?i?Llog p?,i(y?i ) + ?N?i6?LH(p?,i) (4)One may regard ?
as a Lagrange multiplier that isused to constrain the classifier?s uncertainty H tobe low, as presented in the work on entropy regular-ization (Brand, 1999; Grandvalet and Bengio, 2005;Jiao et al, 2006).Conventional bootstrapping retrains on the mostconfident unsupervised examples, making them4This function is not necessarily convex in ?, because of theaddition of the entropy term (Jiao et al, 2006).
One might try anannealing strategy: start ?
at zero (where the function is convex)and gradually increase it, hoping to ?ride?
the global maximum.Although we could increase ?
until the entropy term dominatesthe minimizations and we approach a completely deterministicclassifier, it is preferable to use some labeled heldout data toevaluate a stopping criterion.more confident.
Gradient descent on equation (4)essentially does the same, since unsupervised exam-ples contribute to (4) only through H , and the shapeof the H function means that it is most rapidly de-creased by making the most confident unsupervisedexamples more confident.Besides favoring models that are self-confident onthe unlabeled data, the objective function (4) also ex-plicitly asks the model to continue to get the correctanswers on the initial supervised corpus.
1/?
con-trols the strength of this request.
One could obtaina similar effect in conventional bootstrapping by up-weighting the initial labeled corpus when retraining.2.5 Online LearningMinimizing equation (4) for parsing is more com-putationally intensive than in many other applica-tions of bootstrapping, such as word sense disam-biguation or document classification.
With millionsof features, our objective could take many iterationsto converge to a local optimum, if we were only toupdate our parameter vector ?
after each iterationthrough a large unsupervised corpus.For many machine learning problems over largedatasets, online learning methods such as stochas-tic gradient descent (SGD) have been empiricallyobserved to converge in fewer iterations (Bottou,2003).
In SGD, instead of taking an optimiza-tion step in the direction of the gradient calculatedover all unsupervised training examples, we parseeach example, calculate the gradient of the objectivefunction evaluated on that example alone, and thentake a small step downhill.
The update rule is thus?
(t+1) ?
?
(t) ?
?
?
?F (t)(?
(t)) (5)where ?
(t) is the parameter vector at time t, F (t)(?
)is the objective function specialized to the time-t ex-ample, and ?
> 0 is a learning rate that we choose.We check for convergence after each pass throughthe example set.2.6 Algorithms and ComplexityTo evaluate equation (4), we need a conditionalmodel of trees given a sentence xi.
We define oneby exponentiating and normalizing the tree scores:p?,i(yi,k)def= exp(?e?yi,k f(e) ?
?
)/Zi.With exponentially many parses of xi, does ourobjective function (4) now have prohibitive com-671putational complexity?
The complexity is actuallysimilar to that of the inside algorithm for parsing.In fact, the first term of (4) for projective parsingis found by running the O(n3) inside algorithm onsupervised data,5 and its gradient is found by thecorresponding O(n3) outside algorithm.
For non-projective parsing, the analogy to the inside algo-rithm is the O(n3) ?matrix-tree algorithm,?
which isdominated asymptotically by a matrix determinant(Smith and Smith, 2007; Koo et al, 2007; McDon-ald and Satta, 2007).
The gradient of a determinantmay be computed by matrix inversion, so evaluatingthe gradient again has the same O(n3) complexityas evaluating the function.The second term of (4) is the Shannon entropyof the posterior distribution over parses.
Computingthis for projective parsing takes O(n3) time, using adynamic programming algorithm that is closely re-lated to the inside algorithm (Hwa, 2000).6 For non-projective parsing, unfortunately, the runtime risesto O(n4), since it requires determinants of n distinctmatrices (each incorporating a log factor in a dif-ferent column; we omit the details).
The gradientevaluation in both cases is again about as expensiveas the function evaluation.A convenient speedup is to replace Shannon en-tropy with Re?nyi entropy.
The family of Re?nyi en-tropy measures is parameterized by ?:R?
(p) =11?
?log(?yp(y)?
)(6)In our setting, where p = p?,i, the events y are thepossible parses yi,k of xi.
Observe that under ourdefinition of p,?y p(y)?
= {?y exp[?e?y f(e) ?(??
)]}/Z?i .
We already have Zi from running theinside algorithm, and we can find the numerator byrunning the inside algorithm again with ?
scaledby ?.
Thus with Re?nyi entropy, all computationsand their gradients are O(n3)?even in the non-projective case.Re?nyi entropy is also a theoretically attractivegeneralization.
It can be shown that lim?
?1 R?
(p)5The numerator of p?,i(y?i ) (see definition above) is trivialsince y?i is a single known parse.
But the denominator Zi is anormalizing constant that sums over all parses; it is found by adependency-parsing variant of the inside algorithm, following(Eisner, 1996).6See also (Mann and McCallum, 2007) for similar results onconditional random fields.is in fact the Shannon entropy H(p) and thatlim???R?
(p) = ?
logmaxy p(y), i.e.
the nega-tive log probability of the modal or ?Viterbi?
label(Arndt, 2001; Karakos et al, 2007).
The ?
= 2case, widely used as a measure of purity in decisiontree learning, is often called the ?Gini index.?
Fi-nally, when ?
= 0, we get the log of the numberof labels, which equals the H(uniform distribution)that Abney used in equation (3).3 EvaluationFor this paper, we performed some initial bootstrap-ping experiments on small corpora, using the fea-tures from (McDonald et al, 2005).
After discussingexperimental setup (?3.1), we look at the correlationof confidence with accuracy and with oracle likeli-hood, and at the fine-grained behaviour of models?dependency edge posteriors (?3.2).
We then com-pare our confidence-maximizing bootstrapping toEM, which has been widely used in semi-supervisedlearning (?3.4).
Section 3.3 presents overall boot-strapping accuracy.3.1 Experimental DesignWe bootstrapped non-projective parsers for lan-guages assembled for the CoNLL dependency pars-ing competitions (Buchholz and Marsi, 2006).
Weselected German, Spanish, and Czech (Brants etal., 2002; Civit Torruella and Mart??
Anton?
?n, 2002;Bo?hmova?
et al, 2003).
After removing sentencesmore than 60 words long, we randomly divided eachcorpus into small seed sets of 100 and 1000 trees;development and test sets of 200 trees each; and anunlabeled training set from the rest.These treebanks contain strict dependency trees,in the sense that their only nodes are the words anda distinguished root node.
In the Czech dataset,more than one word can attach to the root; also, thetrees in German, Spanish, and Czech may be non-projective.
We use the MSTParser implementa-tion described in McDonald et al (2005) for fea-ture extraction.
Since our seed sets are so small, weextracted features from all edges in both the seedand the unlabeled parts of our training data, not justthe edges annotated as correct.
Since this producedmany more features, we pruned our features to thosewith at least 10 occurrences over all edges.672Correlation of100-tree model 1000-tree modelRe?nyi ?
Acc.
Xent.
Acc.
Xent.
(uniform, Abney) 0 -0.254 0.980 -0.180 0.937.5 -0.256 0.981 -0.203 0.955(Shannon) 1 -0.260 0.983 -0.220 0.964(Gini) 2 -0.266 0.985 -0.250 0.9775 -0.291 0.992 -0.304 0.9907 -0.301 0.993 -0.341 0.991(Viterbi) ?
-0.317 0.995 -0.326 0.992Xent.
-0.391 1.000 -0.410 1.000Table 1: Correlation, on development sentences, of Re?nyi en-tropy with model accuracy and with cross-entropy (?Xent.?
).Since these are measures of uncertainty, we see a negative cor-relation.
As ?
increases, we place more confidence in high-probability parses and correlate better with accuracy.We used stochastic gradient descent first to min-imize equation (4) on the labeled seed sets.
Thenwe continued to optimize over the labeled and unla-beled data together.
We tested for convergence usingaccuracy on development data.3.2 Empirically Evaluating EntropyBootstrapping assumes that where the parser is con-fident, it tends to be correct.
Standard bootstrappingmethods retrain directly on confident links; simi-larly, our approach tries to make the parser evenmore confident on those links.Is this assumption really true empirically?
Yes:not only does confidence on unlabeled data correlatewith cross-entropy, but both confidence and cross-entropy correlate well with accuracy.
As we willsee, some confidence measures correlate better thanothers.
In particular, measures that are more peakedaround the one-best prediction of the parser, as inViterbi re-estimation, perform well.If we train a non-projective German parser onsmall seed sets of 100 and 1000 trees, only, how welldoes its own confidence predict its performance?For 200 points?labeled development sentences?we measured the linear correlation of various Re?nyientropies (6), normalized by sentence length, withtree accuracy (Table 1).
We also measured how thesenormalized Re?nyi entropies correlate with the pos-terior log-probability the model assigns to the trueparse (the cross-entropy).Since Re?nyi entropy is a measure of uncertainty,we see a negative correlation with accuracy.
Thiscorrelation strengthens as we raise ?
to ?, so wemight expect Viterbi re-estimation, or a differen-Figure 1: Posterior probability of correct and incorrect edgesin German test data under various models.
We show the distri-bution of posterior probabilities for correct edges, known froman oracle, in black and incorrect edges in gray.
In the upperrow, learning on an initial supervised set raises the posteriorprobability of correct edges while dragging along some incor-rect edges.
In the lower row, we see that adding unlabeled datawith R2 entropy continues the pattern of the supervised learner.R?
(Viterbi) training induces a second mode in correct pos-terior probabilities near 1 although it does shift more incorrectedges closer to 1.Figure 2: Precision-recall curves for selecting edges accordingto their posterior probabilities: better bootstrapping puts morearea under the curve.tiable objective function with a very high ?, to per-form best on held-out data.
Note also that the cross-entropy, which looks at the true labels on the held-out data, does not itself correlate very much bet-ter with accuracy than the best unsupervised confi-dence measures.
Finally, we see that Re?nyi entropieswith higher ?
are more stable: when calculated for amodel trained on more data, they improve their cor-relation with accuracy.From tree confidence, we now turn to edge confi-dence: what is the posterior probability that a modelassigns to each of the n2 edges in the dependencygraph?
Figure 1 shows smoothed histograms of trueedges (black) and false edges (gray) in held-out data,according to the posterior probabilities we assign to673them.
Since there are many more false edges, thefigures are cropped to zoom in on the distribution oftrue edges.
As we start training on the labeled seedset, the posterior probabilities of true edges move to-wards one; many false edges also get greater mass,but not to the same extent.
As we add unlabeleddata, we can see the different learning strategies ofdifferent confidence measures.
R2 gradually movesa few true and many fewer false edges towards 1,while R?
(Viterbi) learning is so confident as to in-duce a bimodal distribution in the posteriors of trueedges.
Figure 2 visualizes the same data as fourprecision-recall curves, which show how noisy thehighest-confidence edges are, across a range of con-fidence thresholds.
Although the very high precisionend stays stable after 10 iterations on the seed set,the addition of unlabeled data puts more area underthe curve.
Again, R?
dominates R2.3.3 Bootstrapping ResultsWe performed bootstrapping experiments on the fullCoNLL sets for Czech, German, and Spanish us-ing the non-projective model from McDonald et al(2005).
Performance confirms the results of ouranalysis above (Table 2).
Adding unlabeled data im-proves performance over that of the seed set, withthe exception of the Czech data with R2 bootstrap-ping.
As we saw in ?3.2, bootstrapping with R?dominates bootstrapping with R2 confidence.
Forcomparison, we also show the results obtained bysupervised training on the combined seed and unla-beled sets.
Recall that we did not use the tree anno-tations to perform feature selection; models trainedwith only supported features ought to perform better.Although we see statistically significant improve-ments (at the .05 level on a paired permutation test),the quality of the parsers is still quite poor, in con-trast to other applications of bootstrapping which?rival supervised methods?
(Yarowsky, 1995).
Al-most certainly, the CoNLL datasets, comprising atmost some tens of thousands of sentences per lan-guage, are too small to afford qualitative improve-ments.
Also, at these relatively small training sizes,our preliminary attempts to leverage comparable En-glish corpora did not improve performance.What features were learned, and how dependentis performance on the seed set?
We analyzed theperformance of German bootstrapping on a develop-% accuracySeed trees ?
= 0 2 ?Czech 100 56.1 54.8 58.31000 68.1 68.2 68.271468 77.9 ?
?German 100 60.9 62.4 65.31000 74.6 74.5 75.037745 86.0 ?
?Spanish 100 63.6 64.1 64.42786 76.6 ?
?Table 2: Dependency accuracy of the McDonald model on 200test sentences.
When ?
= 0, training only occurs on the super-vised seed data.
As ?
increases, we train based on confidencein our model?s analysis of the unlabeled data.
Boldface resultsare the best in their rows in a permutation test at the .05 level.ment set (Table 3).
Using the parameters at the lastiteration of supervised training on the seed set as abaseline, we tried updating to their bootstrapped val-ues the weights of only those features that occurredin the seed set.
This achieved nearly the same ac-curacy as updating all the features.
As one wouldexpect, using only the non-seed features?
weightsperforms abysmally.
This might be the case sim-ply because the seed set is likely to contain fre-quently occurring features.
If, however, we use onlythe features occurring in an alternate training set ofthe same size (100 sentences), we get much worseperformance.
These results indicate that our boot-strapped parser is still heavily dependent on the fea-tures that happened to fire in the seed set; we havenot ?forgotten?
our initial conditions.
Similar exper-iments show that unlexicalized features contributethe most to bootstrapping performance.
Since inour log-linear models features have been trained towork together, we must not put too much weight onthese ablation results.
These experiments do, how-ever, suggest that bootstrapping improved our resultsby refining the values of known, non-lexicalized fea-tures.3.4 Comparison with EMPerhaps the most popular statistical method forlearning from incomplete data is the EM algorithm(Dempster et al, 1977).
Since we cannot try EM onMcDonald?s conditional model, we ran some pilotexperiments using the generative dependency modelwith valence (DMV) of Klein and Manning (2004).As in their experiments, and unlike the other exper-iments in the current paper, we restricted ourselves674Updated M feat.
acc.
Updated M feat.
acc.all 15.5 64.3 none 0 60.9seed 1.4 64.1 non-seed 14.1 44.7non-lexical 3.5 64.4 lexical 12.0 59.9non-bilex.
12.6 64.4 bilexical 2.9 61.0Table 3: Using all features, dependency accuracy on Germandevelopment data rose to 64.3% on bootstrapping.
We show thecontribution of different feature splits to the performance of thisfinal model.
For example, although this model was trained byupdating all 15.5M feature weights, it performs as well if wethen keep only the 1.4M features that appeared at least once inthe seed set, zeroing out the weights of the others.
We do as wellas the full feature set if we keep only the 3.5M non-lexicalizedfeatures.% accuracytrain Bulg.
German Spanishsupervised ML 74.2 80.0 71.3CL 77.5 79.3 75.0semi- EM 58.6 58.8 68.4supervised Conf.
80.0 80.5 76.7Table 4: Dependency accuracy of the DMV model (Klein andManning, 2004).
Maximizing confidence using R1 (Shannon)entropy improved performance over its own conditional like-lihood (CL) baseline and over maximum likelihood (ML).
EMdegraded its ML baseline.
Since these models were only trainedand tested on sentences of 10 words or fewer, accuracy is muchhigher than the full results in Table 2.to sentences of ten words or fewer and to part-of-speech sequences alone, without any lexical infor-mation.
Since the DMV models projective trees, weran experiments on three CoNLL corpora that hadaugmented their primary non-projective parses withalternate projective annotations: Bulgarian (Simovet al, 2005), German, and Spanish.We performed supervised maximum likelihoodand conditional likelihood estimation on a seed setof 100 sentences for each language.
These modelsrespectively initialized EM and confidence trainingon unlabeled data.
We see (Table 4) that EM de-grades the performance of its ML baseline.
Meri-aldo (1994) saw a similar degradation over small(and large) seed sets in HMM POS tagging.
Wetried fixing and not fixing the feature expectations onthe seed set during EM and show the former, betternumbers.
Confidence maximization improved overboth its own conditional likelihood initializer andalso over ML.
We selected optimal smoothing pa-rameters for all models and optimal ?
(equation (6))and ?
(equation (4)) for the confidence model on la-beled held-out data.4 Future WorkWe hypothesize that qualitatively better bootstrap-ping results will require much larger unlabeled datasets.
In scaling up bootstrapping to larger unla-beled training sets, we must carefully weight trade-offs between expanding coverage and introducingnoise from out-of-domain data.
We could also bet-ter exploit the data we have with richer models ofsyntax.
In supervised dependency parsing, second-order edge features provide improvements (McDon-ald and Pereira, 2006; Riedel and Clarke, 2006);moreover, the feature-based approach is not limitedto dependency parsing.
Similar techniques couldscore parses in other formalisms, such as CFG orTAG.
In this case, f extracts features from eachof the derivation tree?s rewrite rules (CFG) or ele-mentary trees (TAG).
In lexicalized formalisms, fwill still be able to score lexical dependencies thatare implicitly represented in the parse.
Finally, wewant to investigate whether larger training sets willprovide traction for sparser cross-lingual and cross-domain features.5 ConclusionsFeature-rich dependency models promise to helpbootstrapping by providing many redundant featuresfor the learner, and they can also cleanly incorporatecross-domain and cross-language information.We explored bootstrapping feature-rich non-projective dependency parsers for Czech, German,and Spanish.
Our bootstrapping method maximizesa linear combination of likelihood and confidence.In initial experiments on small datasets, this sur-passed EM for training a simple feature-poor gener-ative model, and also improved the performance ofa feature-rich, conditionally estimated model whereEM could not easily have been applied.
For ourmodels and training sets, more peaked measuresof confidence, measured by Re?nyi entropy, outper-formed smoother ones.AcknowledgmentsThe authors thank the anonymous reviewers, NoahA.
Smith, and Keith Hall for helpful comments, andRyan McDonald for making his parsing code pub-licly available.
This work was supported in part byNSF ITR grant IIS-0313193.675ReferencesSteven Abney.
2004.
Understanding the Yarowsky algo-rithm.
CL, 30(3):365?395.Cristoph Arndt.
2001.
Information Measures.
Springer.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP, pages 120?128.A.
Blum and Tom Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In COLT.A.
Bo?hmova?, J.
Hajic?, E.
Hajic?ova?, and B. Hladka?.
2003.The PDT: a 3-level annotation scenario.
In A. Abeille?,editor, Treebanks: Building and Using Parsed Cor-pora, volume 20 of Text, Speech and Language Tech-nology, chapter 7.
Kluwer.Le?on Bottou.
2003.
Stochastic learning.
In Ad-vanced Lectures in Machine Learning, pages 146?168.Springer.Matthew E. Brand.
1999.
Structure learning in condi-tional probability models via an entropic prior and pa-rameter extinction.
Neural Computation, 11(5):1155?1182.S.
Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.2002.
The TIGER treebank.
In TLT.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared taskon multilingual dependency parsing.
In CoNLL.Y.J.
Chu and T.H.
Liu.
1965.
On the shortest arbores-cence of a directed graph.
Science Sinica, 14:1396?1400.M.
Civit Torruella and M. A.
Mart??
Anton??n.
2002.
De-sign principles for a Spanish treebank.
In TLT.Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.2006.
A discriminative model for tree-to-tree trans-lation.
In EMNLP, pages 232?241.A.
Dempster, N. Laird, and D. Rubin.
1977.
Maximumlikelihood estimation from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society B,39:1?38.J.
Edmonds.
1967.
Optimum branchings.
Journal of Re-search of the National Bureau of Standards, 71B:233?240.Jason Eisner.
1996.
Three new probabilistic models fordependency parsing: An exploration.
In COLING,pages 340?345.H.
N. Gabow, Z. Galil, T. H. Spencer, and R. E. Tarjan.1986.
Efficient algorithms for finding minimum span-ning trees in undirected and directed graphs.
Combi-natorica, 6(2):109?122.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In ACL,pages 961?968.Yves Grandvalet and Yoshua Bengio.
2005.
Semi-supervised learning by entropy minimization.
InNIPS.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Natural Language Engineering, 11:311?325.Rebecca Hwa.
2000.
Sample selection for statisticalgrammar induction.
In EMNLP, pages 45?52.Martin Jansche.
2005.
Treebank transfer.
In IWPT.Feng Jiao, Shaojun Wang, Chi-Hoon Lee, RussellGreiner, and Dale Schuurmans.
2006.
Semi-supervised conditional random fields for improved se-quence segmentation and labeling.
In COLING/ACL,pages 209?216.Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,and Carey E. Priebe.
2007.
Cross-instance tuningof unsupervised document clustering algorithms.
InHLT-NAACL, pages 252?259.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In ACL, pages 479?486.Alexandre Klementiev and Dan Roth.
2006.
Weaklysupervised named entity transliteration and discoveryfrom multilingual comparable corpora.
In COLING-ACL, pages 817?824.T.
Koo, A. Globerson, X. Carreras, and M. Collins.
2007.Structured prediction models via the Matrix-Tree The-orem.
In EMNLP-CoNLL.Gideon S. Mann and Andrew McCallum.
2007.
Efficientcomputation of entropy gradient for semi-supervisedconditional random fields.
In HLT-NAACL.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical machinetranslation with syntactified target language phrases.In EMNLP, pages 44?52, July.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Reranking and self-training for parser adapta-tion.
In ACL, pages 337?344.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In EACL.676Ryan McDonald and Giorgio Satta.
2007.
On the com-plexity of non-projective data-driven dependency pars-ing.
In IWPT.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In ACL, pages 91?98.Bernardo Merialdo.
1994.
Tagging English text with aprobabilistic model.
CL, 20(2):155?72.Fuchun Peng, Ralph Weischedel, Ana Licuanan, andJinxi Xu.
2005.
Combining deep linguistics analy-sis and surface pattern learning: A hybrid approachto Chinese definitional question answering.
In HLT-EMNLP, pages 307?314.Sebastian Riedel and James Clarke.
2006.
Incrementalinteger linear programming for non-projective depen-dency parsing.
In EMNLP, pages 129?137.Charles Schafer and David Yarowsky.
2002.
Induc-ing translation lexicons via diverse similarity measuresand bridge languages.
In CoNLL.Charles Schafer.
2006.
Translation Discovery Using Di-verse Smilarity Measures.
Ph.D. thesis, Johns Hop-kins University.K.
Simov, P. Osenova, A. Simov, and M. Kouylekov.2005.
Design and implementation of the BulgarianHPSG-based treebank.
In Journal of Research on Lan-guage and Computation ?
Special Issue.
Kluwer.David A. Smith and Jason Eisner.
2006.
Quasi-synchronous grammars: Alignment by soft projectionof syntactic dependencies.
In Proceedings of the HLT-NAACL Workshop on Statistical Machine Translation,pages 23?30.David A. Smith and Noah A. Smith.
2007.
Proba-bilistic models of nonprojective dependency trees.
InEMNLP-CoNLL.Mark Steedman, Miles Osborne, Anoop Sarkar, StephenClark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,Steven Baker, and Jeremiah Crim.
2003.
Bootstrap-ping statistical parsers from small datasets.
In EACL.Ralph Weischedel.
2004.
Extracting dynamic evidencenetworks.
Technical Report AFRL-IF-RS-TR-2004-246, BBN Technologies, Cambridge, MA, December.David Yarowsky and Richard Wicentowski.
2000.
Min-imally supervised morphological analysis by multi-modal alignment.
In ACL, pages 207?216.David Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In ACL, pages189?196.677
