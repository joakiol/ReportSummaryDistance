More  accurate  tes ts  Ibr the  s ta t i s t i ca l  s ign i f i cance  of resu l td i f ferences  *Alexander  YehMitre Corp.202 Burli l lgl;on Rd.Bedford,  MA 01730USAasy~mit rc .o rgAbstractStatisti(:a,1 signiticance testing of (litl'erelmeS inv;~hl(`-s of metri(:s like recall, i)rccision and bat-au(:(~(l F-s(:()rc is a ne(:(`-ssary t)art of eml)iricalual;ural language 1)ro(:essing.
Unfortunately, welind in a set of (;Xl)erinlc\]d;s (;hal; many (:ore-inertly used tesl;s ofte, n underest imate t.he s ignificancc an(l so are less likely to detect differencesthat exist 1)el;ween ditl'ercnt techniques.
Thisundel'esi;imation comes from an in(let)endcn('(~a,-;SUlnl)tion that is often violated.
\~fe l)oint outsome useful l;e,%s (;hal; (lo nol; make this assuml)-lion, including computationally--intcnsive ran-d()mizat,ion 1;cs|;s.1 I n t rodu( - t ionIn Clnl)irical natural  \]al~gUag(~ l)rocessing, on(',is ot'tcal |:('~st;ing whether some new technique1)ro(lu('es im\])rove(l l'esull;s (as mcasur(xl \])y one()1' 111017(` - IIICI;I'\]CS) Oit son Ic  i;esl; (lai;~L set; \V\]l(`-ll(;Olll\])aI'e(l l i ( )sol l le (; l lrrel lt  ( l )ascl i lm) l;c(:\]lnique.\5/\]lell ,\]le lCsllll;s are better  with the new tcch-ni(lUe , a question arises as t() wh(',l;h(;r these l:(`--sult; (litl'eren(:es are due t() the new techniquea(:t;ually 1)eing l)cl;t('x or just; due 1;o (:han(:e. Un-t'ortmmtely, one usually Callll()t) directly answerthe qnesl;ion "what is the 1)robatfility that 1;11(;now l;(x:hni(luC, is t)el;lx~r givell l;he results on thet(',sl, dal;a sol;":I)(new technique is better \ [ test  set results)\]~ul; with statistics, one cml answer the follow-ing proxy question: if the new technique was a(>tually no ditt'erent han the old t(',('hnique ((;he* This paper reports on work l)erfonncd at the MITR1,;Corporation under the SUl)porl: of the MITIlJ,; ,qponsoredResearch l)rogrmn.
Warren Grcit\[, l ,ynette Il irschlnm bChristilm l)orall, John llen(lerson, Kelmeth Church, Tedl)unning, Wessel Kraaij, Milch Marcus and an anony-mous reviewer l)rovided hell)rid suggestions.
Copyright@2000 The MITRE Corl)oration.
All rights r(~s(n'vcd.null hyl)othesis), wh~tt is 1:11(; 1)robat)ility thatthe results on the test set would l)e at least thisskewed in the new technique's favor (Box eta\] .
,1978, So(:.
2.3)?
Thai; is, what isP(test  se, t results at least this skew('Ain the new techni(lue's favorI new technique is no (liffercnt than the old)If the i)robtfl)ility is small enough (5% off;on isused as the threshold), then one will rqiect themill hyi)otheMs and say that the differences in1;he results are :'sta.tisl;ically siglfilicant" aI; thatthrt,shold level.This 1)al)(n" examines some of th(`- 1)ossil)leme?hods for trying to detect statistically signif'-leant difl'el'enc(`-s in three commonly used met-l'i(:s: tel'all, 1)re('ision and balanced F-score.Many of these met;Ire(Is arc foun(t to be i)rol)lem-a.ti(" ill a, so, t; of eXl)erinw, nts that are performed.Thes(~ methods have a, tendency to ullderesti-mat(`- th(', signili(:ance, of the results, which tendst() 1hake one, 1)elieve thai; some new techni(tuc isno 1)el;l;er l;lmn the (:urrent technique even whenil; is.This mtderest imate comes fl'om these lnc|h-ells assuming l;hat; the te(:hlfi(tues being con>lmrcd produce indepen(lc, nt results when in oureXl)eriments , the techniques 1)eing COml)aredtend to 1)reduce l)ositively corr(`-lated results.To handle this problem, we, point out somest~ttistical tests, like the lnatche(t-pair t, signand Wilcoxon tests (Harnett,  1982, See.
8.7 and15.5), which do not make this assulnption.
OneCall ITS(', l;llcse tes ts  Oll I;hc recall nlel;r ic, but  l;heprecision an(l 1)alanced F-score metric have tooCOml)lex a tbrm for these tests.
For such com-1)lex lne|;ri(;s~ we llSe a colnplll;e-in|;Clisiv(~ ran-domization test (Cohen, 1995, Sec.
5.3), whichalso ~tvoids this indet)en(lence assmnption.947The next section describes many of the stan-dard tests used and their problem of assumingcertain forms of independence.
The first subsec-rio11 describes tests where this assumption ap-pears in estimating the standard deviation ofthe difference between the techniques' results.The second subsection describes using contin-gency tables and the X 2 test.
Following this is asection on methods that do not 1hake this inde-pendence assumption.
Subsections in turn de-scribe some analytical tests, how they can applyto recall but not precision or the F-score, andhow to use randomization tests to test preci-sion and F-score.
We conclude with a discussionof dependencies within a test set's instances, atopic that we have yet to deal with.2 Tests  that  assume independencebetween compared  resu l ts2.1 F ind ing  and using the variance of aresult differenceFor each metric, after determining how well anew and current technique t)efforms on solnetest set according to that metric, one takes thediflbrence between those results and asks "isthat difference significant?
"A way to test this is to expect 11o difference inthe results (the null hypothesis) and to ask, as-suming this expectation, how mmsual are theseresults?
One way to answer this question is toassulne that the diffb, rence has a normal or t dis-tribution (Box et al, 1978, Sec.
2.4).
Then onecalculates the following:(d - Z \ [4 ) / s  d = d/,~,~ (1)where d = x l -  x2 is the difference found be-tween xl and x2, the results for the new andcurrent echniques, respectively.
E\[d\] is the ex-pected difference (which is 0 under the null hy-pothesis) and Sd is an estimate of the standarddeviation of d. Standard eviation is the squareroot of the variance, a measure of how much arandom variable is expected to vary.
The resultsof equation 1are compared to tables (c.f.
in Boxet al (1978, Appendix)) to find out what thechances are of equaling or exceeding the equa-tion 1 results if the null hypothesis were true.The larger the equation 1 results, the more un-usual it would be under the null hypothesis.A complication of using equation 1 is thatone usually does not have Sd, but only st ands2, where Sl is the estimate for Xl'S standarddeviation and similarly for s2.
Ilow does oneget the former fi'om the latter?
It turns outthat (Box et al, 1978, Ch.
3)o -2 o-12 + a~ d = --  2p12a10-2where cri is the true standard eviation (insteadof the estimate si) and pl'2 is the correlationcoefficient between xl and :c2.
Analogously, itturns out that2 z S d 82 -t- 82 - -  2 r128182 (2)where r12 is an estimate for P12.
So not onlydoes cr d (and Sd) depend on the properties ofxl and x2 in isolation, it also depends on howXl and .~'2 interact, as measured by P12 (and'rr)).
When Xl and x2 are independent, p12 =0, and then (Td = ~-+ c7~ and analogously,Sd = ~ + s~.
When P~2 is positive, ;1; 1 andx2 are positively correlated: a rise in xl or x2tends to be accompanied by a rise in the otherresult.
When P12 is negative, :cl and x2 arenegatively correlated: a rise in :cl or x9 tendsto be accompmfied by a decline in the otherresult.
-1  < P12 < 1 (Larsen and Marx, 1986,Sec.
10.2).The assu lnpt ion  of' independence is often usedin fornlnlas to determine the statistical signifi-cance of the difference d = .~:1 - x2.
But howaccurate is this assumption?
One nfight expectsonic positive correlation from both results com-ing from the same test set;.
One may also expectsome positive correlation when either both tech-niques are just variations of each other 1 or bothtechniques are trained on the same set of train-ing data (and so are missing the same examplesrelative to the test set).This assumption was tested during someexperiments for finding granunatical relations(subject, object, various types of nxodifiers,etc.).
The metric used was the fraction of therelations of interest in the test set that were re-called (tbund) by some technique.
The relationsof interest were w~rious ubsets of the 748 rela-tion instances in that test set.
An example sub-set is all the modifier elations.
Another subsetis just that of all the time modifier elations.1 These  var ia t ions  are often des igned to usual ly  behavei l l  the  stone way and  on ly  differ in jus t  a few cases.948First, two difl'erent e(:hniques, one ltlelllory-t)ased and the other tl'ansti)rlnation-rule based,wei"e trained on the same training set, and thenboth teste(1 on that ticst set;.
l~.e(:all eonlt)a.risonswe, re made tbr ten subsets of tim relations andthe r12 was found for each cOral)arisen.
FromBox et al (1978, Ch.
3)" '12 = ~( I ,  J lk -- Y l ) (~2k  - -  ~2) / ( 'g l t~2(  71 - -  l . )
)kwhere Yil~ = \] if the ith technique recalls thetcl;h relation and = 0 if not.
'lz, is the nmnl)crof relations in the subset.
!\]i and si are meanand stmJ(lard de, vial, ion estimate.s (based on theYik'S), rest)ectively, fl)r the ith technique.For the ten subsets, only Clio COlnl)arison hada 'r12 (::lose to 0 (It was -0.05).
The other ninec()ml)arisons had 'r12's 1)etw(x',n 0.29 and 0.53.The ten coral)arisen inedian value was 0.38.Next;, the transformatiol>rulc t)ased t.cch-nique was rUll with difl'erent sets of start ing con-ditions and/or  different, but overlapl)ing , sub-sets of the training set.
Recall comparisons werema(le on the same test (lata.
set 1)etween l;he d i ffcrent variations.
Many of the comparisons were,of how well two wu:iations recalled a particularsubset of the relations.
A total of 40 compar-isons were made..
The 'r\]2's on all d0 were 1)osi-tire.
3 of the 'r,2's w('~re ill the 0.20-0.30 range.24 of the rj2's wore in the 0.50--0.79 range.
13of the 'r\]2's were in the 0.80-1.0() range.So in our ext)erin~ents, we were usually eom-t)aring 1)ositivcly correlated results.
How mucherror is introdu(:e(t t)y assuming independence?An easy--to-analyze case is when the stan-dard devial,ions for the results being eoml)areda:t'c the same.
?~ 'J}hen equation 2 reduces tos , , -  sV /2 ( l -  r12), where s = sl = ,s'2.
If one,assumes the re.sults m'e indcpel:dent (~/SSlllller,2 = 0), then sd :-~ .sv/22.
Call this wflue sd-i,7,g.As flu increases in value, Sd decreases:\[().38 d 0.T87(sd_i,,d) 1.27\[p.ao I 1.41\[O.80J 0.447(Sd.__i.,,.d) 2.24'l'he rightmost cohunn above indicates the mag-nitude by which erroneously assuming indepen->\[lifts is actually roughly true in the coml)arisonsnmde, and is assumed to be true in many of the standardWsts for statistical significance.
(lence (using 8d_in d ill 1)lace of sd) will increasethe standard eviation estimate.
In equation 1,sd forms the denominator of the ratio d/.s d. Soerroneously assmning independence will meanthat  the mmmrator  d, the difference between thetwo results: will nee(t to increase by that samefactor in order f()r equation 1 to have the samewtlue as without the indel)endence assmnt)tion.Since the value of that  equation indicates thestatistical significance of d, assunfing indepen-dence will mean that  e1 will have to be largerthan without the.
assumption to achieve thesame al)parent level of statistical significance.l?roln tile tal)le above, when r12 = 0.50, (1 willneed to 1)c about 41% larger.
Another way tolook at this is that  assuming indei)en(lenee willmake the same.
v~due, of d appear less statist;i-cally signifiealtt.The common tests of statistical significanceuse this assumt)tion.
The, tesl; klloWlt as the1, (Box et; al., 1978, Sec.
4.1) or two-saml)le t(Harnett,  1982, See.
8.7) test does.
This testuses equation 1 and then compares the resultingva.lue against he t; distr ibution tal)les.
This testhas a (:Oml)licated form for sd l)eeause:1. :c!
and :c2 can t)e 1)ased on (tiffering num-1)ers of saml)les.
Call these retail)ors 'n~ and'n2 r(;sl)ectivcly.2.
111l this t(;st, the z i ' s  are each an ni sam-pie average, of altother varial)le ((:all it yi).
'\['his is important  because the si's in thistest are standm'd deviation estimates torthe yi's, not the xi 's .
The relationship be-tween them is that  si for Jli is the same as( for :,:,:.3.
The test itself assumes that !11 and Y2 havethe same standard eviation (call this com-mon value s).
The denominator estimates,s using a weighte(1 average of 81 and s2.The weighting is b~sed on nl and r7,2.From Harnett (1982, Scc.
8.7), the denominatorSd ~-nl  + n2 - 2711 -b r~,2 )'i7,177,2When 'nl = 'n2 (call this common value 'n), '~1and s2 will be given equal weight, and Sd siml)li-fie.s to ~ + ,s'~)/n.
Making the substitut iondescribed above of si v/57 tbr si leads to an Sd of949s 2 the fbrm had earlier for the -t-, 2, we us ingindependence assumption.Another test that both makes this assulnt)-tion and uses a tbrm of equation 1 is a test tbrbinonlial data (Harnett, 1982, Sec.
8.1.1) whichuses the "t'aet" that binomial distributions tendto approximate normal distributions.
In thistest, the zi's being compared are the fractionof the items of interest that are recovered bythe ith technique.
In this test, the denomina-tor sd of equation 1also has a complicated fbrm,both due to the reasons mentioned for the t, testabove and to the fact that with a binomial dis-tribution, the standard eviation is a flmctionof the number of samples and the mean wflue.2.2 Using cont ingency tab les  and  X 2 totest  precis ionA test that does not use equation 1 but stillmakes an assunlption of independence l)etweena:l and a:u is that of using contingency tableswith the chi-squared 0,52) distribution (Box etal., 1978, Sec.
5.7).
When tile assmnption isvalid, this test is good for comparing differencesill the pr'ecision metric.
Precision is the fractionof the items "Ibund" 1)y some technique thatare actually of interest.
Precision = l~,/(I~, + S),where R is the number of items that are of inter-est and m'e Recalled (fbund) by tile technique,and S is the munber of items that are found bytile technique that turn out to be Spurious (notof interest).
One can test whether the precisionresults from two techniques are different by us-ing a 2 x 2 contingency table to test whether theratio R/S  is different for the two techniques.One makes tile latter test, by seeing if tile as-sumption that the ratios for the two techniquesare the same (the null hypothesis) leads to a sta-tistically significant result when using a X 2 dis-tribution with one degree of freedom.
A 2 x 2 ta-ble has 4 cells.
The top 2 cells are filled with theR and S of one technique and the bottom 2 cellsget the R and S of the other technique.
In thistest, the valuc in each cell is assumed to have aPoisson distribution.
When the cell values arenot too small, these Poisson distributions areapproximately Normal (Gaussiml).
As a result,when the cell values are independent, smnmingtlle normalized squares of the difference betweeneach cell and its expected value leads to a X 2distribution (Box el; al., 1978, Sec.
2.5-2.6).How well does this test work in our experi-ments?
Precision is a non-linear time(ion of tworandom wu'iables R and S, so we did not try toestimate the correlation coefficient \]'or precision.However, we can easily estimate the correlationcoefficients for the R's.
They are the r12's foundin section 2.1.
As that section mentions, ther12's fbund are just about always positive.
Soat least in our experiments, the R's are not ill-dependent, but are positively correlated, whichviolates the assumptions of the test.An example of how this test behaves is thefollowing comparison of the precision of two dif-ferent methods at finding the modifier elationsusing tile stone training and test set.
The corm-lation coefficient estilnate tor R is 0.35 mid thedata isMethod 17, 5' t?recision1 47 48 4!
)%2 25 14 64%Placing the l~, and S values into a 2 x 2 tableleads to a X 2 value of 2.38. a At t degree offreedom, tile X 2 tables indicate that if the nullhypothesis were true, there would 1)e a 10% to20% chance of producing a X 2 value at least thislarge.
So according to this test, this nnlch of anobserved difference in precision wouht not beunusual if no actual differ(,ncc in the precisionexists between the two nw, thods.This test assumes independence b tween the/~, wdues.
When we use a 22(I (=1048576) trialapproximate rmldomization test (section 3.3),which makes no such assumptions, then we findthat this latter test indicates that under thenull hypothesis, there is less than a 4% chanceof producing a difference in precision results aslarge as the one observed.
So this latter test in-dicates that this nmch of an observed ifferencein precision would be mmsual if no actual dif-ference ill the precision exists between the twomethods.It should be mentioned that the manner oftesting here is slightly different han the man-ner in the rest of this paper.
The X 2 test looksat the square of the difference of two results,and rejects the mill hylmthesis (the comparedtechniques are the same) when this square isa\Ve do not use Yate's adjustment to compensate lbrthe numbers in the table being integers.
1)oing so wouldlmve made the results even worse.950large, whel;he, r l;lm largeness is (:aused l)y t;henew t;eehni(lue t)l"o(lucing' a, much l)(fl;l;er result;titan l;he current, l;e(:hlfique or vice-versa.
So1,o l)e fair, we eolnl)ared l;he X 2 resull;s with al;wo-sided version of l;hc rmldon~iz~fl;ion t;esl,: es-l;inm|;e, l;he likelihood glu~l; l;he obsea'ved magni-l;u(le of t, he resull; (lifl'eren(:e would 1)c matchedor exceeded (regardless of' which l;echnique pro-duced l;he betl;er resull;) raider the mill hyl)oth-esis.
A one-sided version of the test;, which iscolnt)aral)le t;o what we use in l;he rest of the t)a -per, esl;inml;es l;he likelihood of a (tifferenl; oul;-come under t;he null hyt)oChesis: that of m~l:cll-ing or exceeding t;he (lit\['erence of how lllllchl)?,l;ter i;he new (possibly 1)ett, er) l;e(:lmi(lue's oh-s('a'ved result is than l;he currenl; l;e('hnique's o|)-serve,(1 l'esull;, ht t;he ahoy(; scenario, a one-sidedt(;sl; t)rodu(:es ~ 3(~, tigure insl;ead of s~ d:% figure.3 Tests  w i thout  that  independenceassumpt iona.1 Tests  for  matched pa i rsAt; l;his point, one may wonder it' all st;al;isl;icalt;CSt;S lllil\](e SllC\]l s/,\]l int lepealdenct~ asSllltl\])l;ioll.Th(', miswer is no, lml; t;\]l()se lesl:s l;hal; (to nol;lltC}~Slll;e, how ll l l lch {;we l;e(;\]lni(llles illl;(',ra(:l; (toneed  i;o lmtke, some assmnpl ; ioH  al)oul; t;\]m|; ill-I;(;r~t('l;ion mid l;yl)it:a.ll E l;\]ml; assuml)l;ioll is in(te-t)(~\]ldell(;e. 'Fhose I;esl;s I;ll~H; Hol;i(;c in S()lll(~ \\r;~Brhow much l;wo tc(:hniqucs hm;ra(:l; (;~1,11 lib(; ~h()seol)servations insl;ead of relying on assumt)l:ions.One w',~y t;o measure how 1;we l;e(:lmi(lucs in-i;erac(; is 1;o comtm.re \]tow similarly (;he, t;wo t;ecl>ni(tues tea.el; 1;o various l)arl;s ()f 1;he l;(;s\[; seA;.'_l.
"his is done in the mal;t:hed-lm.ir 1, I;esl; (Hm'-nctl;, 1982, Se(:.
8.7).
This l;csI; tin(ls the dith'a'-once bet;we, n how t;eclmiques 1 and 2 l)eribrmon e~t(::h l;esl; set, Saml)le.
The/ ,  dist;ri|)ul;ion anda fOI'ln of eqm~l;ion l m:e used.
The null }lyl)ol;h-esis is st;ill l;\]l~tl; ~he mtmeral;or d \]ms ~t 0 me,m,but el is now l;he stun of these difference values(divided 1)y t;he number of Smnl)les), instead ofbeing :r~ - :re.
Similm'ly, the (lenomimd;or .sd isnow esl;inml;ing l;he si;a.ndm'd (leviation of l;hesedifl'erenee wdues, instead of being a funcl;ion ofs:l and su.
'.Flfis means for example, (;hal; even ift;lm values fl'om l,eclmiques l and 2 vary on (lii-ti:rent; test; Smnl)les , Sd will now 1)(' 0 if on eachtesI; smnl)le, l;echnique \]1)reduces a. value l;lmt isthe  ssulle C()llS|;allI; tHI1OlllIi; lllOl'e t;han l;he va,\]uefl'om t, echnique 2.Two ol;h(',r tests for eomlmring how (;we tech-ni(lueS 1)ert'()rm 1) 3, comtmring how well l;heyperform on each I;est Smnl)le arc the sign midWilcoxon tests (Harnel;t;, 1!
)82, See.
15.5).
Un-like, t;\]le nl~tl;ched-tmir t: t;esI;~ neither of t, hese l;woI;CSI;5 slSSllllte t;ln~l; I hc sum of l;he (litl'crences hasa normal (Gaussian) (listribul;ion.
The i;wo testsare, so-calh~d nonl)a.rmut%ri(: l;esl;s, which (lo not;make assuml)l, ions a.1)out; how l, he rcsull;s axe dis-ln'il)ut, ed (thrnel,l,, 1982, Ch.
15).il'he sign |;est is I;he simplier of lJm I;wo.
It usesa 1)inomial dist,rilm|;ion to examine the munberof l;esl; smni)les where t;e(:hlfi(lUe \] 1)crforms \])el;-l;er t;ha.n l;e(:hnique 2 ve, rsus l;he munl)er where1;he Ol)posite occurs.
The null hyl)ol;hesis is l;h~d;1;he t;wo t;eclmiques 1)ert'orm equally well.Unlike the sign t;esl;, t;he Vfilcoxon |;esl; alsouses inlbl'nlal;ion on how large a difference xisl;s1)el;ween t, hc l;wo l;echniques' r(,,sull;s on each ofl;hc l;csl; smnpl(;s.3.2 Us ing  the  tes ts  for matched-pa i rsAll three of l;hc ma.l,(:he(1-tmir t, sign andWilcoxon t;csl;s can 1)e a.pl)lied t;o t;hc re, call met-ric, whicll is the fl'act;ion of |;he il;ems of inl;crcsl;in ~,he l:csl; sol; l;lml; a, I;e, ehniquc recalls (finds).Each il;em of inl,eresi; in |;he l;esl; (la~;a serves asa.
l;cst sainlflU.
\?e use t;he sign l;esl; b(',causc iI;11Htkcs fcwel" assumi)i;ions 1;hart i;he nml;chcd-l)air1: I;est and is simplier l;han the Wih'oxon I;esi;.
111addit;ion, the fro:i; glml; t~he sign l;e, st ignores l;hesize of 1;he result; difl'erence on eacll l;esl; Smnl)le(tocs llOI; nml;ter here.
\?iI:h I;he recall met;rio,each sa.mple of int;eresl; is either found or nol; bya.
t;eehnique.
There are no interlnedbtte values.While 1;he 1;hree l;esl;s described in sccl;ion 3.1can be used on the re(:~dl mctxic, 1;hey CallllO|; bc"" ' ' used on ell;lint t;hc precision or slamgh|fforwardly1)abmced F-score met;rics.
This is because bothprecision and F-score ~tre more coml)licated non-linem' flmci;ions of rml(lom varial)lcs than recall.In fst(:t bol;h can be l;hought of as non-linem"flm(:l;ions involving recall.
As described in Sec-tion 2.2, precision = 1~./(1~ + S), where I~ is i;henmnl)er of iWms t;lmt; are of inl:eresl; that; are '/'c'-called by a W, chnique mid S is l;he mmfl)er ofit;e, ms (fi)und 1)y s~ technique) that; are nol; ofinterest;.
The 1)~dmmed F-score = 2ab/(a + b),where a is recall and b is precision.9513.3 Using randomizat ion fbr precisionand F -scoreA class of technique that ean handke all ldnds offlmetions of random variables without the aboveproblenls is the computationally-intellsive ran-domization tests (Noreen, 1989, Ch.
2) (Cohen,1995, Sec.
5.3).
These tests have previouslyused on such flmctions during the "message un-derstanding" (MUC) evaluations (Chinchor etal., 1993).
The randomization test we use is likea randomization version of the paired sample(matched-1)air) t test (Cohen, 1995, Sec.
5.3.2).This is a type of stratified shuffling (Noreen,198!
), Sec.
2.7).
When eomt)aring two tech-niques, we gather-u I) all the responses (whetheractually of interest or not) produced by oneof the two techniques when examining the testdata, but not both techniques.
Under the 111111hyl)othesis , the two techniques are not reallydifferent, so any resl)onse produced by one ofthe teehniques eonld have just as likely comefl'om the other.
So we shuffle these responses,reassign each response to one of the two tech-niques (equally likely to either technique) andsee how likely such a shuffle 1)roduces a differ-ence (new technique lninus old technique) in themetric(s) of interest @1 our ease, precision andl?-score) that is at least; as large as the differenceobserved when using the two techniques on thetest data.
'n responses to shuttle and assign 4 leads to2 ~' difl'erent w~\ys to shuffle and assign I;hose re-sponses.
So when 'n.
is small, one can try eachof the different shuttles once and produce anexact randomization.
V~;hen n gets large, themmfl)er of different shutttes gets too large to beexhaustively evaluated.
~J?hen one performs a.uapproximate randomization where each shuffleis perfornmd with randoln assignments.For us, when n < 20 (2'" .<_.
1048576), we usean exact randomization.
For n > 20, we use anapproximate randomization with 1048576 shufties.
Because an approximate randomizationuses random nmnbers, which both lead to oc~casional unusual results and may involve usinga not-so-good pseudo-random 1111111\])(;I" genera-tol "~, we perfbrm the following cheeks:4Note that responses produced by both or neithertechniques do not need to be shulIled and ,~ssigned.5One examI)le is the RANDU routine on the IBM360(Forsythe t al., 1977, See.
10.1).?
We run the 1048576 shuttles a seeond timeand colnpare the two sets of results.?
We also use tile same shutttes to calcu-late the statistical significance for the recallmetric, and compare this significance valuewith the significance value found for recallanalytically by the sign test.An example of using randomization is to com-pare two different methods on finding modifierrelations ill the same test set,.
The results onthe test; set, are:Method ~ ~  Precision F-score ti _l_556t ' I 49.5% 47.5%Zl: 64.1% 35.2%Two questions being tested are whether the ap-parent ilnt)rovement in reca.ll and F-score f!romusing method I is significant.
Also being testedis whether the apparent imt)rovenmnt; in pl'eci-sion fl'om using method Ii is significant.In this example, there are 10"1 relations thatshould be found (are of interest).
Of these, 19are recalled by both methods, 28 are recalledby method I but not; II, and 6 are recalled byII but not I.
The correlation coeificient estilnatebetween the methods' recalls is 0.35.
In addi-tion, 5 stmrious (not of interest) relations arcfound by both methods, with method I find-ing an additional 43 Sl)uriolls relationships (notfound by method II) and me?hod II finding anadditional 9 relationships.There are a total of 28+6+43+9=86 relationsthat are found (whether of interest oi' not) byone method, but not the other.
This is toomany to t)erfornl an exact randolnizgtion, soa 1048576 trial apt)roximate randomization isperfornmd.In 96 of these trials, method I's recallis greater than method iI's recall by at,least (45.6%-24.3%).
Similarly, in 14794of the trials, the F-score difference is atleast (47.5%-35.2%).
In 25770 of the trials,method II's precision is greater than method I'sprecision by at least; (64.1%-49.5%).
N:om(Noreen, 1989, Sec.
aA.a), the significance level(probability under the null hypothesis) is atmost (.,e + 1)/(,~t + 1), where ',,.
: is the nul~lt/erof trials that meet the criterion alld 1t, t is thenumber of trials.
So fbr recall, the significancelevel is at most (96+1)/(1048576+1) =0.00009.952Similarly, for F-score, the significance level is atmost 0.
()1 d: and for l)re(:ision, the level is at lllOSt0.025.
A secon(l 1048576 trial t)ro(luces imilarresults, as does a sign test on recall.
'l'hus, wesee that all three dit\[ere.n(:es are statistically sig-lfiIica.nt.4 The  future.
: hand l ing  in ter -smnpledependenc iesAn assmnption made by all I;he methods men-tioned in this I)~tl)er is ttmt the nlenlbcrs of theLest set are all independent of one anothex.
Tlmtis, knowing how a method l)('rforms on one testsot sanlple should not give any information onhow that method \ ] )e l ' for l l ls  on  other test setsamples.
This assulnl)tJon is not always true.Church and Mercer (1993) give some exaln-ples of dependence bctwe.en test set insl;ancesill na tura l  la.llguage.
One tyt)e of  dci)endenceis that of a lexeme's part of speech on thel)m'l;s of speech of  neighl)oring lexenm,~ (th(,irsection 2.1).
Sinfilar is the concept of collo-ca, t;ion, where the prolml)ility of a lexeme's al>l)earance is influenced by the.
lexemes ai)pea.rin: ~i1~ nearby positions (their section 3).
A type of(tet)en(lence that is less local is that often, a. con--tent word's al)pe.arance in a piece of text gr(;atlyincreases the cha.n('es of th~tt s;ulle wor(1 ~q)l)ear -illg b~ter in that 1)iece of texl; (their se(:l;ion 2.;/).What ~tr('.
the effects when SOllle d{:t)endencyexists?
The expected (average) value of' the in-stallC(~ results will stay the, same.
However, the('lmnees of getting an llllllSllal resl l l t  (;a,lt c\]la.ll~re.As an eXmnl)le , take five flips of a Nit coin.When no dependen(:ies exist 1)etween the tlil)s ,the clmnces of the extreme result tha.t all theflit)s l:md on :~ particular side is faMy small((1/2) 5 -- i\[/32).
When the ttil)s are positivelycorrelated, these chmices increase.
When thefirst flip lands on that side, the chances of theother four tlil)s doing the same are now ea.chgreater tlmn 1/2.Since statistical significance testing involvesfinding the chances of getting an mmsmd(skcwe(1) result under some null hyt)othesis, oneneeds to determine those del)endencies in orderto accurately determine those dmnces, l)eter-mining the etk's:t of these dependencies is some-thing that is yet to l)e done,.5 Conc lus ionsIn elnpirical natural language processing, oneis often COml)aring differences in values of met-rics like recall, precision and balanced F-score.Many of the statistics tests commonly used tomake such comparisons assume the indepen-dence between the results being compared.
\?eran ~ set of m~tural language processing exper-iments and tbund that this assuml)tion is oftenviolated in .~uch a way as t,o understate the sta-l, istical significance of the difli;rences betweenthe results.
We point out some analyt;ica.1 statis-tics tests like lnatched-l)air t,, sign mid Wilcoxontests, which do not midge this assmnption andshow that they (;tl,ll \])e l lsed Oll a l l letr ic likerecall, l?br more complicated 1nettles like pre-cision and balanced F-score, wc use a compute--intensive randonfization test, which also avoidsthis assumption.
A next topic to address is thatof possible dependencies l)etween test set  sam-ples.Re ferencesG.
Box, W. Hunter, and J. Hmlter.
1978.,gta, iisl.ics for" <rpc.ri'm.ent, er.~.
John Wiley andS()llS.N.
Chinchor, L. Hirschman, and l).
Lewis.
\] 9!
)3.\]~vahmtillg message understanding systems:an analysis of the, third message understand-ing conferc.nce (muc-.3).
Co'll~,p'ltt(tt'io'llgl Li ~,-gui,stic.s, 1!)(3).K.
Church mid 171.. Mercer.
1993.
Introduction1;o the sl)ecial issue on computational linguis-tics using large corpora.
Cornp'u, tational Lin-guistic.s, 1!)(1.
):1 24.P.
Cohen.
1!)95.
Empirical Meth, ods for Ar'tifi-cial Intelligence.. MIT Press, MA, USA.G.
Forsythe, M. M~dcolm, and C. Moler.
1977.Com, putc'r methods for ~nathcm, atical comp'u-l.ati~m,.s.
Prentice-lI~dl~ N,J, USA.D.
Harnett.
1982.
Statistical Methods.Addison-XYesley Publishing Co., 3rd edi-tion.R.
Larsen and M. Marx.
1986.
An Introduc-tion to Ma, th, cmatical Statistics and Its Appli-cations.
Prentice-Hall, N J, USA, 2nd edition.E.
Noreen.
1989.
Computer-intensive met;hods.\[br testing h, ypoth, cscs: an int, r'od'ttction.
JolmWiley and Sons, Inc.953
