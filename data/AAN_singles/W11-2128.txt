Proceedings of the 6th Workshop on Statistical Machine Translation, pages 237?249,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsFiltering Antonymous, Trend-Contrasting, and Polarity-DissimilarDistributional Paraphrases for Improving Statistical Machine TranslationYuval MartonT.J.
Watson Research CenterIBMyymarton@us.ibm.comAhmed El Kholy and Nizar HabashCenter for Computational Learning SystemsColumbia University{akholy,habash}@ccls.columbia.eduAbstractParaphrases are useful for statistical machinetranslation (SMT) and natural language pro-cessing tasks.
Distributional paraphrase gen-eration is independent of parallel texts andsyntactic parses, and hence is suitable alsofor resource-poor languages, but tends to erro-neously rank antonyms, trend-contrasting, andpolarity-dissimilar candidates as good para-phrases.
We present here a novel methodfor improving distributional paraphrasing byfiltering out such candidates.
We evalu-ate it in simulated low and mid-resourcedSMT tasks, translating from English to twoquite different languages.
We show statisti-cally significant gains in English-to-Chinesetranslation quality, up to 1 BLEU from non-filtered paraphrase-augmented models (1.6BLEU from baseline).
We also show thatyielding gains in translation to Arabic, a mor-phologically rich language, is not straightfor-ward.1 IntroductionParaphrase recognition and generation has provenuseful for various natural language processing(NLP) tasks, including statistical machine transla-tion (SMT), information retrieval, query expansion,document summarization, and natural language gen-eration.
We concentrate here on phrase-level (asopposed to sentence-level) paraphrasing for SMT.Paraphrasing is useful for SMT as it increases trans-lation coverage ?
a persistent problem, even in large-scale systems.
Two common approaches are ?pivot?and distributional paraphrasing.
Pivot paraphrasingtranslates phrases of interest to other languages andback (Callison-Burch et al, 2006; Callison-Burch,2008).
It relies on parallel texts (or translationphrase tables) in various languages, which are typ-ically scarce, and hence limit its applicability.
Dis-tributional paraphrasing (Marton et al, 2009) gener-ates paraphrases using a distributional semantic dis-tance measure computed over a large monolingualcorpus.1 Monolingual corpora are relatively easyand inexpensive to collect, but distributional seman-tic distance measures are known to rank antonymousand polarity-dissimilar phrasal candidates high.
Wetherefore attempt to identify and filter out such ill-suited paraphrase candidates.A phrase pair may have a varying degree ofantonymy, beyond the better-known complete op-posites (hot / cold) and contradictions (did / didnot), e.g., weaker contrasts (hot / cool), contrast-ing trends (covered / reduced coverage), or senti-ment polarity (happy / sad).
Information extrac-tion, opinion mining and sentiment analysis litera-ture has been grappling with identifying such pairs(Pang and Lee, 2008), e.g., in order to distinguishpositive and negative reviews or comments, or to de-tect contradictions (Marneffe et al, 2008; Voorhees,2008).
We transfer some of the insights, data andtechniques to the area of paraphrasing and SMT.
Wedistributionally expand a small seed set of antonymsin an unsupervised manner, following Mohammadet al (2008).
We then present a method for fil-tering antonymous and polarity-dissimilar distribu-tional paraphrases using the expanded antonymouslist and a list of negators (e.g., cannot) and trend-decreasing words (reduced).
We evaluate the im-pact of our approach in a SMT setting, where non-1Other variants use a lexical resource in conjunction withthe monolingual corpus (Mirkin et al, 2009; Marton, 2010).237baseline translation models are augmented with dis-tributional paraphrases.
We show gains of up to1 BLEU relative to non-filtered models (1.6 BLEUfrom non-augmented baselines) in English-Chinesemodels trained on small and medium-large size data,but lower to no gains in English-Arabic.
The smalltraining size simulates resource-poor languages.The rest of this paper is organized as follows:We describe distributional paraphrase generation inSection 2, antonym discovery in Section 3, andparaphrase-augmented SMT in Section 4.
We thenreport experimental results in Section 5, and discussthe implications in Section 6.
We survey relatedwork in Section 7, and conclude with future workin Section 8.2 Distributional ParaphrasesOur method improves on the method presented inMarton et al (2009).
Using a non-annotated mono-lingual corpus, our method constructs distributionalprofiles (DP; a.k.a.
context vectors) of focal wordsor phrases.
Each DPphr is a vector containing log-likelihood ratios of the focal phrase phr and eachword w in the corpus.
Given a paraphrase candidatephrase cand, the semantic distance between phr andcand is calculated using the cosine of their respec-tive DPs (McDonald, 2000).
For details on DPs anddistributional measures, see Weeds et al (2004) andTurney and Pantel (2010).The search of the corpus for paraphrase candi-dates is performed in the following manner:1.
For each focal phrase phr, build distributionalprofile DPphr.2.
Gather contexts: for each occurrence of phr,keep surrounding (left and right) context L R.3.
For each such context, gather paraphrase can-didates cand which occur between L and R inother locations in the training corpus, i.e., allcand such that L cand R occur in the corpus.4.
For each candidate cand, build a profileDPcand and measure profile similarity betweenDPcand and DPphr.5.
Rank all cand according to the profile similar-ity score.6.
Filter out every candidate cand that textuallyentails phr: This is approximated by filteringcand if its words all appear in phr in the sameorder.
For example, if phr is spoken softly, thenspoken very softly would be filtered out.7.
Filter out every candidate cand that is antony-mous to phr (See Algorithm 1 below).8.
Output k-best remaining candidates above acertain similarity score threshold t.Most of the steps above are similar to, and havebeen elaborated in, Marton et al (2009).
Due tospace limitations, we concentrate on the main novelelement here, which is the antonym filtering step,detailed below.
Antonyms (largely speaking) are op-posites, terms that contrast in meaning, such as hot /cold.
Negators are terms such as not and lost, whichoften flip the meaning of the word or phrase that fol-lows or contains them, e.g., confidence / lost confi-dence.
Details on obtaining their definitions and onobtaining the antonymous pair list and the negatorlist are given in Section 3.Algorithm 1 Antonymous candidate filteringGiven an antonymous pair list, a negator list, and aphrase-paraphrase candidate (phr-cand) pair list,for all phr-cand pairs dofor all words w in phr doif w is also in cand, and there is a negator upto two words before it in either phr or cand(but not both!)
thenfilter out this pairif w, ant is an antonymous pair, and ant isin cand, and there is no negator up to twowords before w and ant, or there is such anegator before both thenfilter out this pair3 Antonyms, Trends, Sentiment PolarityNative speakers of a language are good at deter-mining whether two words are antonyms (hot?cold,ascend?descend, friend?foe) or not (penguin?clown,cold?chilly, boat?rudder) (Cruse, 1986; Lehrer andLehrer, 1982; Deese, 1965).
Strict antonyms apart,there are also many word pairs that exhibit some de-gree of contrast in meaning, for example, lukewarm?cold, ascend?slip, and fan?enemy (Mohammad etal., 2008).
Automatically identifying such con-trasting word pairs has many uses including detect-ing and generating paraphrases (The lion caughtthe gazel / The gazel could not escape the lion)238and detecting contradictions (Marneffe et al, 2008;Voorhees, 2008) (The inhabitants of Peru are welloff / the inhabitants of Peru are poor).
Of course,such ?contradictions?
may be a result of differingsentiment, new information, non-coreferent men-tions, or genuinely contradictory statements.
Iden-tifying paraphrases and contradictions are in turnuseful in effectively re-ranking target language hy-potheses in machine translation, and for re-rankingquery responses in information retrieval.
Identifyingcontrasting word pairs (or short phrase pairs) is alsouseful for detecting humor (Mihalcea and Strappar-ava, 2005), as satire and jokes tend to have contra-dictions and oxymorons.
Lastly, it is useful to knowwhich words contrast a focal word, even if only tofilter them out.
For example, in the automatic cre-ation of a thesaurus it is necessary to distinguishnear-synonyms from contrasting word pairs.
Distri-butional similarity measures typically fail to do so.Instances of strong contrast are recorded to someextent in manually created dictionaries, but hun-dreds of thousands of other contrasting pairs are not.Further, antonyms can be of many kinds such asthose described in Section 3.1 below.
We use theMohammad et al (2008) method to automaticallygenerate a large list of contrasting word pairs, whichare used to identify false paraphrases.
Their methodis briefly described in Section 3.2.3.1 Kinds of antonymsAntonyms can be classified into different kinds.A detailed description of one such classification canbe found in Cruse (1986) (Chapters 9, 10, and 11),where the author describes complementaries (open?shut, dead?alive), gradable adjective pairs (long?short, slow?fast) (further classified into polar, over-lapping, and equipollent antonyms), directional op-posites (up?down, north?south), (further classifiedinto antipodals, counterparts, and reversives), re-lational opposites (husband?wife, predator?prey),indirect converses (give?receive, buy?pay), con-gruence variants (huge?little, doctor?patient), andpseudo opposites (black?white).
It should benoted, however, that even though contrasting wordpairs and antonyms have long been studied bylinguists, lexicographers, and others, experts donot always agree on the scope of antonymy andthe kinds of contrasting word pairs.
Some lex-ical relations have also received attention at theEducational Testing Services (ETS).
They clas-sify antonyms into contradictories (alive?dead,masculine?feminine), contraries (old?young, happy-sad), reverses (attack?defend, buy?sell), direction-als (front?back, left?right), incompatibles (happy?morbid, frank?hypocritical), asymmetric contraries(hot?cool, dry?moist), pseudoantonyms (popular?shy, right?bad), and defectives (default?payment,limp?walk) (Bejar et al, 1991).As mentioned earlier, in addition to antonyms,there are other meaning-contrasting phenomena, orother ways to classify them, such as contrastingtrends and sentiment polarity.
They all may havevarying degrees of contrast in meaning.
Hereafterwe sometime broadly refer to all of these as antony-mous phrases.
The antonymous phrase pair genera-tion algorithm that we use here does not employ anyantonym-subclass-specific techniques.3.2 Detecting antonymsMohammad et al (2008) used a Roget-like the-saurus, co-occurrence statistics, and a seed set ofantonyms to identify the degree of antonymy be-tween two words, and generate a list of antony-mous words.
The thesaurus divides the vocabularyinto about a thousand coarse categories.
Each cat-egory has, on average, about a hundred closely re-lated words.
(A word with more than one sense,is listed in more than one category.)
Mohammadet al first determine pairs of thesaurus categoriesthat are contrasting in meaning.
A category pairis said to be contrasting if it has a seed antonympair.
A list of seed antonyms is compiled using 16affix patterns such as X and unX (clear?unclear)and X and disX (honest?dishonest).
Once a con-trasting category pair is identified, all the word pairsacross the two categories are considered to have con-trasting meaning.
The strength of co-occurrence(as measured by pointwise mutual information) be-tween two contrasting word pairs is taken to be thedegree of antonymy.
This is based on the distri-butional hypothesis of antonyms, which states thatantonymous pairs tend to co-occur in text more of-ten than chance.
Co-occurrence counts are madefrom the British National Corpus (BNC) (Burnard,2000).
The approach attains more than 80% accu-racy on GRE-style closest opposite questions.2393.3 Detecting negatorsThe General Inquirer (GI) (Stone et al, 1966) has11,788 words labeled with 182 categories of wordtags, such as positive and negative semantic orien-tation, pleasure, pain, and so on.2 Two of the GIcategories, NOTLW and DECREAS, contain termsthat negate the meaning of what follows (Choi andCardie, 2008; Kennedy and Inkpen, 2005).
Theseterms (with limited added inflection variation) formour list of negators.4 Paraphrase-Augmented SMTAugmenting the source side of SMT phrase tableswith paraphrases of out-of-vocabulary (OOV) itemswas introduced by Callison-Burch et al (2006),and was adopted practically ?as-is?
in consequentwork (Callison-Burch, 2008; Marton et al, 2009;Marton, 2010).
Given an OOV source-side phrasef , if the translation model has a rule ?f ?, e?
whosesource side is a paraphrase f ?
of f , then a new rule?f, e?
is added, with an extra weighted log-linearfeature, whose value for the new rule is the similar-ity score between f and f ?
(computed as a functionof the pivot translation probabilities or the distribu-tional semantic distance of the respective DPs).
Wefollow the same line here:h(e, f) =??????????????
?asim(DPf ?
, If phrase table entry (e, f)DPf ) is generated from (e, f ?
)using monolingually-derived paraphrases.1 Otherwise.
(1)where the definition of asim is repeated below.
Asnoted in that previous work, it is possible to con-struct a new translation rule from f to e via morethan one pair of source-side phrase and its para-phrase; e.g., if f1 is a paraphrase of f , and so is f2,and both f1, f2 translate to the same e, then both leadto the construction of the new rule translating f to e,but with potentially different feature scores.
In orderto leverage on these paths and resolve feature valueconflicts, an aggregated similarity measure was ap-plied: For each paraphrase f of source-side phrases2http://www.wjh.harvard.edu/?inquirerfi with similarity scores sim(fi, f),asimi = asimi?1+(1?asimi?1) sim(fi, f) (2)where asim0 = 0.
We only augment the phrasetable with a single rule from f to e, and in it are thefeature values of the phrase fi for which sim(fi, f)was the highest.5 Experiment5.1 System and ParametersWe augmented translation models with para-phrases based on distributional semantic distancemeasures, with our novel antonym-filtering, andwithout it.
We tested all models in English-to-Chinese and English-to-Arabic translation, aug-menting the models with translation rules for un-known English phrases.
We also contrasted thesemodels with non-augmented baseline models.For baseline we used the phrase-based SMT sys-tem Moses (Koehn et al, 2007), with the defaultmodel features: 1. phrase translation probability,2.
reverse phrase translation probability, 3. lexicaltranslation probability, 4. reverse lexical translationprobability, 5. word penalty, 6. phrase penalty, 7. sixlexicalized reordering features, 8. distortion cost,and 9. language model (LM) probability.
We usedGiza++ (Och and Ney, 2000) for word alignment.All features were weighted in a log-linear frame-work (Och and Ney, 2002).
Feature weights wereset with minimum error rate training (Och, 2003) ona tuning set using BLEU (Papineni et al, 2002) as theobjective function.
Test results were evaluated usingBLEU and TER (Snover et al, 2006): The higherthe BLEU score, the better the result; the lower theTER score, the better the result.
This is denotedwith BLEU?
and TER?
in Table 1.
Statistical signif-icance of model output differences was determinedusing Koehn (2004)?s test on the objective function(BLEU).The paraphrase-augmented models were createdas described in Section 4.
We used the same dataand parameter settings as in Marton (2010).3 Weused cosine distance over DPs of log-likelihood ra-tios (McDonald, 2000), built with a sliding win-3Data preprocessing and paraphrasing code slightly differfrom those used in Marton et al (2009) and Marton (2010), andhence scores are not exactly the same across these publications.240dow of size ?6, a sampling threshold of 10000 oc-currences, and a maximal paraphrase length of 6tokens.
We applied a paraphrase score thresholdt = 0.05; a dynamic context length (the short-est non-stoplisted left context L occurring less than512 times in the corpus, and similarly for R); para-phrasing of OOV unigrams; filtering paraphrase can-didates occurring less than 25 times in the corpus(inspired by McDonald, 2000); and allowing up tok = 100 best paraphrases per phrase.
We tunedthe weights of each model (non-augmented base-line, unigram-augmented, and unigram-augmented-filtered) with a separate minimum error rate training.We explored here augmenting OOV unigrams,although our paraphrasing and antonym filteringmethods can be applied to longer n-grams with nofurther modifications.
However, preliminary experi-ments showed that longer n-grams require additionalprovisions in order to yield gains.5.2 DataIn order to take advantage of the English antonymresource, we chose English as the source languagefor the translation task.
We chose Chinese asthe translation target language in order to comparewith Marton (2010), and for the same reasons it waschosen there: It is quite different from English (e.g.,in word order), and four reference translation wereavailable from NIST.
We chose Arabic as anothertarget language, because it is different from bothEnglish and Chinese, and richer morphologically,which introduces additional challenges.English-Chinese: For training we used theLDC Sinorama and FBIS tests (LDC2005T10 andLDC2003E14), and segmented the Chinese sidewith the Stanford Segmenter (Tseng et al, 2005).After tokenization and filtering, this bitext contained231,586 lines (6.4M + 5.1M tokens).
We trained atrigram language model on the Chinese side, withthe SRILM toolkit (Stolcke, 2002), using the mod-ified Kneser-Ney smoothing option.
We followedthe split in Marton (2010), and constructed the re-duced set of about 29,000 sentence pairs.
The pur-pose of creating this subset model was to simulate aresource-poor language.
We trained separate trans-lation models, using either the subset or the full-sizetraining dataset.For weight tuning we used the Chinese-EnglishNIST MT 2005 evaluation set.
In order to use it forthe reverse translation direction (English-Chinese),we arbitrarily chose the first English reference setas the tuning ?source?, and the Chinese source as asingle ?reference translation?.
For testing we usedthe English-Chinese NIST MT evaluation 2008 testset with its four reference translations.English-Arabic: We use an English-Arabic par-allel corpus of about 135k sentences (4 millionwords) and a subset of 30K sentences (one mil-lion words) for the translation models?
training data.The sentences were extracted from Arabic News(LDC2004T17), eTIRR (LDC2004E72), Englishtranslation of Arabic Treebank (LDC2005E46),and Ummah (LDC2004T18).4 For Arabic pre-processing, we follow previously reported best to-kenization scheme (TB)5 and orthographic wordnormalization condition (Reduced) when translat-ing from English to Arabic (El Kholy and Habash,2010b).
MADA (Habash and Rambow, 2005) isused to pre-process the Arabic text for the translationmodel and 5-gram language model (LM).
As a post-processing step, we jointly denormalize and deto-kenize the text to produce the final Arabic output.Following El Kholy and Habash (2010a), we usetheir best detokenization technique, T+R+LM.
Thetechnique crucially utilizes a lookup table (T), map-ping tokenized forms to detokenized forms, basedon our MADA-fied LM.
Alternatives are given con-ditional probabilities, P (detokenized|tokenized).Tokenized words absent from the tables are deto-kenized using deterministic rules (R), as a backoffstrategy.
We use a 5-gram untokenized LM andthe disambig utility in the SRILM toolkit to de-cide among different alternatives.
Word alignmentis done using GIZA++, as in English-Chinese sys-tem.
We use lemma-based alignment which consis-tently yields superior results to surface-based align-ment (El Kholy and Habash, 2010b).
For LM, weuse 200M words from the Arabic Gigaword Corpus(LDC2007T40) together with the Arabic side of ourtraining data.All experiments were conducted using Moseshere as well.
We used a maximum phrase length4All are available from the Linguistic Data Consortium(LDC) http://www.ldc.upenn.edu5TB: Penn Arabic Tree Bank tokenization scheme241of size 8 tokens.
Weight optimization was done us-ing a set of 300 sentences from the NIST MT 2004Arabic-English evaluation test set (MT04).
The tun-ing was based on tokenized Arabic without detok-enization.
Testing was done on the NIST Arabic-English MT05 and MEDAR 2010 English-Arabicfour-reference evaluation sets.
For both tuning onMT04 and testing on MT05, since we need the re-verse English-Arabic direction, we chose one En-glish reference translation as the ?source?, and theArabic as a single ?reference?.
We evaluated usingBLEU and TER here too.English paraphrases: We augmented the base-line models with paraphrases generated as describedabove, using a monolingual text of over 516M to-kens, consisting of the BNC and English Gigaworddocuments from 2004 and 2008 (LDC2009T13),pre-processed to remove punctuation and to conflatenumbers, dates, months, days of week, and alphanu-meric tokens to their respective classes.5.3 ResultsEnglish-Chinese: Results are given in Table 1.Augmenting SMT phrase tables with paraphrases ofOOV unigrams resulted in gains of 0.6-0.7 BLEUpoints for both subset and full models, but TERscores were worse (higher) for the full model.
Aug-menting same models with same paraphrases filteredfor antonyms resulted in further gains of 1.6 and 1BLEU points for both subset and full models, respec-tively, relative to the respective baselines.
The TERscores of the antonym filtered models were also asgood or better (lower) than those of the baselines.reduced size large sizemodel BLEU?
TER?
BLEU?
TER?baseline 15.8 69.2 21.8 63.8aug-1gram 16.4B 68.9 22.5B 64.4aug-1gram-ant-filt 17.4BD 68.7 22.8BD 63.7Table 1: English-Chinese scores.
B/D = statistically significantw.r.t.
(B)aseline or (D)istributional 1gram model, using Koehn(2004)?s statistical significance.English-Arabic: Results are given in columns 1-7of Table 2.
On the MT05 test set, the 135k-sentenceaug-1gram model outperformed its baseline in bothBLEU and TER scores.
The lemmatized variantsof the scores showed higher or same gains.
Sinceonly one entry was antonym-filtered here, we donot provide separate scores for aug-1gram-ant-filt.Surprisingly, for the reduced 30k models, all scores(BLEU, TER, and even their lemmatized variants) ofthe augmented 1gram model were somewhat worsethan the baseline?s, and those of the antonym-filteredmodel were the worst.
we also ran a 4-reference test(Medar) to see whether the single MT05 referencewas problematic, but results were similar.
We exam-ine possible reasons for this in the next section.6 DiscussionFiltering quality: Our filtering technique is basedon antonymous pair and negator lists that were ex-panded distributionally from seed sets.
Therefore,they are noisy.
From a small random sample (Ta-ble 3) it seems that only about 10% of filtered casesshould not have been filtered; of the rest, 50% werestrongly antonymous, 25% mildly so, and 15% weresiblings (co-hypernyms) in a natural categorical hi-erarchy or otherwise noisy paraphrases filtered dueto a noisy antonym pair.
Negators in the unigrams?paraphrase candidates were rare.English-Chinese: Our paraphrase filtering tech-nique yielded an additional 1 BLEU point gainover the non-filtered paraphrase-augmented reducedmodel (totaling 1.6 BLEU over baseline).
The re-duced and large augmented models?
phrase tablesize increased by about 27% and 4%, respectively ?and antonym filtering did not change these numbersby much (see left side of Table 4).
Therefore, the dif-ference in performance between the filtered and non-filtered systems is unlikely to be quantitative (phrasetable size).
The out of vocabulary (OOV) rate of the29k subset model is somewhat high (see Table 4),especially for the test set; but only after these exper-iments were completed did we peek at the test setfor calculating these statistics, and in any case, weshould not be guided by such information in choos-ing the test set.
At first glance it may seem surpris-ing that only 0.4% of the paraphrase candidates ofthe English OOV unigrams (248 candidates) werefiltered by our procedure, and that it accounted foras much as 1 BLEU in the reduced set.
(For English-Arabic only 0.6%, or 23 candidates, were filtered).Leaving the estimation of antonymous phrase detec-tion recall for the future, we note that these num-242BLEU Lemm.
Brev.
Ref/Sys TER Lemm.
Unigram Lemma Match Analysis?
BLEU penal.
ratio ?
TER Exact Match Lemma-only Unmatchable Total30k-sentence (1M word) training dataset modelsMT05 baseline 23.6 31.3 99.2 1.008 57.6 47.3 15614 55.4% 4055 14.4% 8550 30.3% 28219aug-1gram 23.2 30.8 99.9 1.001 58.8 48.4 15387 54.2% 4195 14.8% 8831 31.1% 28413aug-1gram-ant-filt 23.2 30.8 99.9 1.001 58.8 48.3 15387 54.2% 4195 14.8% 8831 31.1% 28413MEDAR baseline 13.6 18.7 93.6 1.066 67.6 61.3 4924 53.0% 1563 16.8% 2800 30.1% 9287aug-1gram 12.9 18.3 94.2 1.060 68.9 62.3 4894 52.0% 1710 18.2% 2815 29.9% 9419aug-1gram-ant-filt 12.9 18.3 94.2 1.060 69.0 62.3 4891 51.9% 1715 18.2% 2815 29.9% 9421135k-sentence (4M word) training dataset modelsMT05 baseline 25.8 33.5 99.2 1.008 55.7 45.3 16115 57.1% 3999 14.2% 8128 28.8% 28242aug-1gram 26.4 34.3B 99.5 1.005 55.1 44.7 16156 57.1% 4068 14.4% 8089 28.6% 28313aug-1gram-ant-filt 26.4 34.3B 99.5 1.005 55.0 44.6 16153 57.1% 4090 14.5% 8068 28.5% 28311MEDAR baseline 17.1 23.1 94.7 1.054 65.1 58.6 5483 57.7% 1577 16.6% 2438 25.7% 9498aug-1gram 17.2 23.5 95.3 1.048 65.1 58.6 5586 58.1% 1606 16.7% 2424 25.2% 9616aug-1gram-ant-filt 17.2 23.5 95.3 1.048 65.1 58.6 5586 58.1% 1606 16.7% 2424 25.2% 9616Table 2: English-Arabic translation scores and analysis for NIST MT05 and MEDAR test sets.
B = statistically significant w.r.t.
(B)aseline using Koehn (2004)?s statistical significance test.bers from English are not directly comparable to theChinese side: they relate to paraphrase candidatesand not phrase table entries; they relate to types andnot tokens; each OOV English word may translateto one or more Chinese words, each of which maycomprise of one or more characters; and last but notleast, the BLEU score we use is character-based.phrase ||| paraphrase ||| score commentsabsence ||| occupation ||| 0.06 mildabsence ||| presence ||| 0.33 goodbackwards ||| forwards ||| 0.21 goodwooden ||| plastic lawn ||| 0.12 siblingdump ||| dispose of ||| 0.41 badcooler ||| warm ||| 0.45 milddiminished ||| increased ||| 0.23 goodminor ||| serious ||| 0.42 goodrelic ||| youth activist in the ||| 0.12 harmlessdive ||| rise ||| 0.15 goodargue ||| also recognize ||| 0.05 mildbother ||| waste time ||| 0.79 baddive ||| climb ||| 0.17 goodmoonlight ||| spring ||| 0.05 harmlesssharply ||| slightly ||| 0.60 goodsubstantial ||| meager ||| 0.14 goodwarmer ||| cooler ||| 0.72 goodtough ||| delicate ||| 0.07 goodtiny ||| mostly muslim ||| 0.06 mildsoftly ||| deep ||| 0.06 mildTable 3: Random filtering examplesWhile individual unigram to 4gram scores for theaugmented models were lower than the baseline?s,filtered model?s unigram and bigram scores werelower or similar to the baseline?s, and their trigramand 4gram scores were higher than the baseline?s.We intend to further investigate the cause for thispattern, and its effect on translation quality, with thehelp of a native Chinese speaker ?
and on BLEU, to-gether with the brevity penalty ?
in the future.English-Arabic: The most striking fact is the set ofdifferences between the language pairs: In English-Chinese, we see gains with distributional paraphraseaugmentation, and further gains when antonymousand contrasting paraphrase candidates are filteredout.
But in the 30k-sentence English-Arabic models,paraphrase augmentation actually degrades perfor-mance, even in lemma scores.
It has been observedbefore that BLEU (and similarly TER) is not idealfor evaluation of contributions of this sort (Callison-Burch et al, 2006).
Therefore we conducted bothmanual and focused automatic analysis, includingOOV statistics and unigram lemma match analysis66Unigram lemma match analysis is a classification of all thewords in the translation hypothesis (against the translation ref-erence) into: (a) exact match, which is equal to simple unigramprecision, (b) lemma-only match, which counts words that canonly be matched at the lemma level, and (c) unmatchable.243between the system output and the reference trans-lation.Table 4 shows that the OOV rates for English-Arabic are lower than English-Chinese.
But if theywere negligible, we would not expect to see gains(or in fact any change) in either model size, contraryto fact.
It is interesting to point out that our trans-lation model augmentation technique handles about50% of the (non-digit, non-punctuation) OOV wordsin all models (except for only half that in the 135kmodel, which still showed gains).Another concern is that the current maximal para-phrase length (6 tokens) may be too far from theparaphrasee?s length (unigram), resulting in lowerquality.
However, a closer examination of thelength difference evident through the BLEU brevitypenalty and the reference:system-output length ra-tio (columns 4-5 of Table 2), reveals that the dif-ferences are small and inconsistent; on average, thebrevity penalty difference accounts for roughly 0.1absolute BLEU points and 0.2 absolute lemmatizedBLEU points of the respective differences.7Last, Modern Standard Arabic is a morphologi-cally rich language: It has many inflected forms formost verbs, and several inflected forms for nouns,adjectives and other parts of speech ?
and complexsyntactic agreement patterns showing these inflec-tions.
It might be the case that the inflected ArabicLM model might not serve well the augmented mod-els, since they include translation rules that are morelikely to be ?off?
inflection-wise (e.g., showing un-grammatical syntactic agreement or simply an ac-ceptable choice that differs from the reference).
Pre-sumably, the smaller the training set, the larger thisproblem, since there would be fewer rules and hencesmaller variety of inflected forms per similar coremeaning.
The unigram lemma match analysis andlemma scores?
statistics (Table 2) support this con-cern.
In the 30k model, lemma-only match seemsto even further increase, at the expense of the exactword-form match.
Possible solutions include usinga lemma-based LM, or another LM that is adjustedto this sort of inflection-wise ?off?
text.7These values are computed by subtracting the differencebetween two BLEU scores from the difference between the sametwo BLEU scores without the effect of brevity penalty (i.e., eachdivided by its brevity penalty).Error Analysis We conducted an error analysis ofour Arabic 30k system using part of the MT05 testset.
That set had 571 OOV types, out of which,we were able to augment phrases for 196 OOVtypes.
The majority of OOV words were propernouns (67.8%), with the rest being mostly nouns, ad-jectives and verbs (in the order of their frequency).Among the OOVs for which we augmented phrases,the proper noun ratio was smaller than the full set(45.4% relative).
We selected a random sample of50 OOV words, and examined their translations inthe MT05 test set.
The analysis considered all theOOV word occurrences (96 sentences).
We classi-fied each OOV translation in the augmented systemand the augmented-filtered system as follows:a1 correct (and in reference)a2 correct (morphological variation)a3 acceptable translation into a synonyma4 acceptable translation into a hypernymb1 wrong translation into a hypernymb2 co-hypernym: a sibling in a psychologicallynatural category hierarchyb3 antonymous, trend-contrasting, or polarity dis-similar meaningc1 wrong proper-noun translation (sibling)c2 wrong proper-noun translation (other)d wrong translation for other reasonsBoth the augmented and augmented-filtered systemhad 27.1% correct cases (category a).
Only one-quarter of these were exact matches with the refer-ence (category a1) that can be captured by BLEU.Incorrect proper-noun translation (category c) wasthe biggest error (augmented model: 33.3%, filteredmodel: 37.5%); within this category, sibling mis-translations (category c1), e.g., Buddhism is trans-lated as Islam, were the majority (over half in aug-mented model, and about two-thirds in the filteredmodel).
Proper nouns seem to be a much biggerproblem for translation into Arabic than into Chi-nese in our sets.
Category b mis-translations ap-peared in 20.8% of the time (equally in augmentedand filtered).
Almost half of these were sibling mis-translations (category b2), e.g., diamond translatedas gold.
Only two OOV translations in our sam-ple were antonymous (category b3).
It is possible,therefore, that our Arabic sets do not give room forour filtering method to be effective.
In one case,the verb deepen (reference translation??
?K) is mis-244translated as summit (???).
In the other case, theadjective cool (political relations), whose referencetranslation uses a figure of speech periods of tension(QK?J?
@ 	??
H@Q?
), is mistranslated as good (?YJk.
),which carries the opposite sentiment.
The rest ofcategory b involve hypernyms (b1), such as trans-lating the OOV word telecom into company (??Q???
@).Overall, the filtered model did not behave signifi-cantly differently from its augmented counterpart.Chinese-Arabic score difference: We conjecturethat another possible reason for the different scoregain patterns between the two language pairs is thefact that in Chinese, many words that are siblings-in-meaning share a character, which doesn?t necessar-ily have a stand-alone meaning; therefore, character-based BLEU was able to give credit to such para-phrases on the Chinese side, which was not case forthe word-based BLEU on the Arabic side.7 Related WorkThis paper brings together several sub-areas:SMT, paraphrase generation, distributional seman-tic distance measures, and antonym-related work.Therefore we can only briefly survey the most rel-evant work here.
Our work can be viewed as an ex-tension of the line of research that seeks to augmenttranslation tables with automatically generated para-phrases of OOV words or phrases in a fashion sim-ilar to Section 4: Callison-Burch et al (2006) usepivoting technique (translating to other languagesand back) in order to generate paraphrases, and thepivot translation probability as their similarity score;Callison-Burch (2008) filters such paraphrases usingsyntactic parsing information; Marton et al (2009)use distributional paraphrasing technique that ap-plies distributional semantic distance measure forthe paraphrase score; Marton (2010) applies a lexi-cal resource / corpus-based hybrid semantic distancemeasure for the paraphrase score instead, approxi-mating word senses; here, we apply a distributionalsemantic distance measure that is similar to Martonet al (2009), with the main difference being the fil-tering of the resulting paraphrases for antonymity.Other work on augmentating SMT: Habash andHu (2009) show, pivoting via a trilingual paralleltext, that using English as a pivot language be-tween Chinese and Arabic outperforms translationusing a direct Chinese-Arabic bilingual parallel text.Other attempts to reduce the OOV rate by augment-ing the phrase table?s source side include Habash(2009), providing an online tool for paraphrasingOOV phrases by lexical and morphological expan-sion of known phrases and dictionary terms ?
andtransliteration of proper names.Bond et al (2008) also pivot for paraphrasing.They improve SMT coverage by using a manuallycrafted monolingual HPSG grammar for generatingmeaning and grammar-preserving paraphrases.
Thisgrammar allows for certain word reordering, lexicalsubstitutions, contractions, and ?typo?
corrections.Onishi et al (2010), Du et al (2010), and others,pivot-paraphrase the input, and represent the para-phrases in a lattice format, decoding it with Moses.Work on paraphrase generation: Barzilay andMcKeown (2001) extract paraphrases from a mono-lingual parallel corpus, containing multiple transla-tions of the same source.
However, monolingualparallel corpora are extremely rare and small.
Dolanet al (2004) use edit distance for paraphrasing.Max (2009) and others take the context of the para-phrased word?s occurrence into account.
Zhao et al(2008) apply SMT-style decoding for paraphrasing,using several log linear weighted resources whileZhao et al (2009) filter out paraphrase candidatesand weight paraphrase features according to the de-sired NLP task.
Chevelu et al (2009) introducea new paraphrase generation tool based on Monte-Carlo sampling.
Mirkin et al (2009), inter alia,frame paraphrasing as a special, symmetrical case of(WordNet-based) textual entailment.
See Madnaniand Dorr (2010) for a good paraphrasing survey.Work on measuring distributional semantic dis-tance: For one survey of this rich topic, see Weedset al (2004) and Turney and Pantel (2010).
Weuse here cosine of log-likelihood ratios (McDonald,2000).
A recent paper (Kazama et al, 2010) advo-cates a Bayesian approach, making rare terms havelower strength of association, as a by-product of re-lying on their probabilistic Expectation.Work on detecting antonyms: Our work withantonyms can be thought of as an application-basedextension of the (Mohammad et al, 2008) method.Some of the earliest computational work in thisarea is by Lin et al (2003) who used patterns245model e2z:29k e2z:232k e2a:30k e2a:135kphrase table baseline vocab.
(# source-side types) 13916 34825 24371 49854phrase table entries: baseline 1996k 13045k 2606k 12344kphrase table entries: aug-1gram 2543k 127.38% 13615k 104.37% 2635k 101.09% *12373k 100.23%phrase table entries: aug-1gram-ant-filt 2542k 127.35% 13615k 104.37% 2635k 101.09% *12373k 100.23%OOV types in tune (% tune types) 1097 21.58% 451 8.87% 141 7.31% 84 4.35%OOV tokens in tune (% tune tokens) 2138 6.10% 917 2.62% 193 2.18% 115 1.30%OOV types in test (% test types) 2473 33.59% 1227 16.66% 574 12.42% 339 7.34%OOV tokens in test (% test tokens) 4844 10.40% 2075 4.46% 992 2.83% 544 1.55%tune OOV token decrease in aug-1gram/ant-filt 1343 27.73% 510 24.58% 79 7.96% 28 5.15%tune OOV type decrease in aug-1gram/ant-filt 646 58.89% 203 45.01% 60 42.55% 22 26.19%test OOV token decrease in aug-1gram /ant-filt 2776 57.31% 996 48.00% 460 46.37% 127 23.35%test OOV type decrease in aug-1gram/ant-filt 1394 56.37% 585 47.68% 246 42.86% 76 22.42%Table 4: Out-of-vocabulary (OOV) word rates and phrase table sizes for all model sizes and language pairs.
e2z = English-Chinese;e2a = English-Arabic.
The statistics marked with * in the top-right cell are identical, see ?5.3.such as ?from X to Y ?
and ?either X or Y ?
todistinguish between antonymous and similar wordpairs.
Harabagiu et al (2006) detected antonymsby determining if their WordNet synsets are con-nected by the hypernymy?hyponymy links and ex-actly one antonymy link.
Turney (2008) proposed asupervised method to solve word analogy questionsthat require identifying synonyms, antonyms, hyper-nyms, and other lexical-semantic relations betweenword pairs.8 Conclusions and Future WorkWe presented here a novel method for filtering outantonymous phrasal paraphrase candidates, adaptedfrom sentiment analysis literature, and tested in sim-ulated low- and mid-resourced SMT tasks from En-glish to two quite different languages.
We used anantonymous word pair list extracted distributionallyby extending a seed list.
Then, the extended list, to-gether with a negator list and a novel heuristic, wereused to filter out antonymous paraphrase candidates.Finally, SMT models were augmented with the fil-tered paraphrases, yielding English-Chinese transla-tion improvements of up to 1 BLEU from the corre-sponding non-filtered paraphrase-augmented model(up to 1.6 BLEU from the corresponding baselinemodel).
Our method proved effective for mod-els trained on both reduced and mid-large English-Chinese parallel texts.
The reduced models sim-ulated ?low density?
languages by limiting theamount of the training text.We also showed for the first time transla-tion gains for English-Arabic with paraphrase-augmented (non-filtered) models.
However, Ara-bic, and presumably other morphologically rich lan-guages, may require more complex models in orderto benefit from our filtering method.Our antonym detection and filtering method isdistributional and heuristic-based; hence it is noisy.We suspect that OOV terms in larger models tendto be harder to paraphrase (judging by the differ-ence from the reduced models, and the lower OOVrate), and also harder to filter paraphrase candidatesof (due to the lower paraphrase quality, which mightnot even include sufficiently distributionally similarcandidates, antonymous or otherwise).
In the future,we intend to improve our method, so that it can beused to improve also the quality of models trainedon even larger parallel texts.Last, we intend to extend our method beyond un-igrams, limit paraphrase length to the vicinity of theparaphrasee?s length, and improve our inflected Ara-bic generation technique, so it can handle this noveltype of augmented data well.AcknowledgmentsPart of this work was done while the first authorwas at Columbia University.
The second author wasfunded through a Google research award.
The au-thors wish to thank Saif Mohammad for providinghis data and for useful discussion, and also thank theanonymous reviewers for their useful feedback.246ReferencesRegina Barzilay and Kathleen McKeown.
2001.
Extract-ing paraphrases from a parallel corpus.
In Proceed-ings of the Association for Computational Linguistics(ACL).Isaac I. Bejar, Roger Chaffin, and Susan Embretson.1991.
Cognitive and Psychometric Analysis of Ana-logical Problem Solving.
Springer-Verlag, New York,NY.Francis Bond, Eric Nichols, Darren Scott Appling, andMichael Paul.
2008.
Improving statistical machinetranslation by paraphrasing the training data.
In Pro-ceedings of IWSLT, Hawai?i, USA.Lou Burnard.
2000.
Reference Guide for the BritishNational Corpus (World Edition).
Oxford UniversityComputing Services.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine translationusing paraphrases.
In Proceedings of the North Ameri-can Chapter of the Association for Computational Lin-guistics (NAACL).Chris Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP), Waikiki,Hawai?i.Jonathan Chevelu, Thomas Lavergne, Yves Lepage, andThierry Moudenc.
2009.
Introduction of a new para-phrase generation tool based on monte-carlo sampling.In Proceedings of the 47th Annual Meeting of the As-sociation for Computational Linguistics (ACL) - the4th International Joint Conference on Natural Lan-guage Processing of the Asian Federation of NaturalLanguage Processing (IJCNLP) Short Papers, pages249?252, Suntec, Singapore.Yejin Choi and Claire Cardie.
2008.
Learning withcompositional semantics as structural inference forsubsentential sentiment analysis.
In Proceedings ofEmpirical Methods in Natural Language Processing(EMNLP), Waikiki, Hawaii.David A. Cruse.
1986.
Lexical semantics.
CambridgeUniversity Press.James Deese.
1965.
The structure of associations in lan-guage and thought.
The Johns Hopkins Press.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised construction of large paraphrase corpora:exploiting massively parallel news sources.
In Pro-ceedings of the 20th Annual Meeting of the Associ-ation for Computational Linguistics (ACL), Geneva,Switzerland.Jinhua Du, Jie Jiang, and Andy Way.
2010.
Facilitatingtranslation using source language paraphrase lattices.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), pages420?429, MIT, Massachusetts, USA.Ahmed El Kholy and Nizar Habash.
2010a.
Techniquesfor Arabic Morphological Detokenization and Ortho-graphic Denormalization.
In Proceedings of the sev-enth International Conference on Language Resourcesand Evaluation (LREC), Valletta, Malta.Ahmed El Kholy and Nizar Habash.
2010b.
Ortho-graphic and Morphological Processing for English-Arabic Statistical Machine Translation.
In In Actesde Traitement Automatique des Langues Naturelles(TALN), Montreal, Canada.Nizar Habash and Jun Hu.
2009.
Improving Arabic-Chinese statistical machine translation using Englishas pivot language.
In Proceedings of the 4th EACLWorkshop on Statistical Machine Translation, pages173?181, Athens, Greece.Nizar Habash and Owen Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging and MorphologicalDisambiguation in One Fell Swoop.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 573?580, Ann Ar-bor, Michigan, June.
Association for ComputationalLinguistics.Nizar Habash.
2009.
REMOOV: A tool for online han-dling of out-of-vocabulary words in machine transla-tion.
In Proceedings of the 2nd International Con-ference on Arabic Language Resources and Tools(MEDAR), Cairo, Egypt.Sanda M. Harabagiu, Andrew Hickl, and Finley Laca-tusu.
2006.
Lacatusu: Negation, contrast and contra-diction in text processing.
In Proceedings of the 23rdNational Conference on Artificial Intelligence (AAAI),Boston, MA.Junichi Kazama, Stijn De Saeger, Kow Kuroda, MasakiMurata, and Kentaro Torisawa.
2010.
A Bayesianmethod for robust estimation of distributional similar-ities.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 247?256, Uppsala, Sweden.Alistair Kennedy and Diana Inkpen.
2005.
Sentimentclassification of movie and product reviews using con-textual valence shifters.
COMPUTATIONAL INTEL-LIGENCE, pages 110?125.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
Inthe Annual Meeting of the Association for Com-putational Linguistics (ACL) demonstration session,Prague, Czech Republic.Philipp Koehn.
2004.
Statistical significance tests for247machine translation evaluation.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP).Adrienne Lehrer and K. Lehrer.
1982.
Antonymy.
Lin-guistics and Philosophy, 5:483?501.Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou.2003.
Identifying synonyms among distributionallysimilar words.
In Proceedings of the 18th Interna-tional Joint Conference on Artificial Intelligence (IJ-CAI), pages 1492?1493, Acapulco, Mexico.Nitin Madnani and Bonnie Dorr.
2010.
Generatingphrasal and sentential paraphrases: A survey of data-driven methods.
Computational Linguistics, 36(3).Marie-Catherine de Marneffe, Anna Rafferty, andChristopher D. Manning.
2008.
Finding contradic-tions in text.
In Proceedings of the 46th Annual Meet-ing of the Association for Computational Linguistics(ACL), Columbus, OH.Yuval Marton, Chris Callison-Burch, and Philip Resnik.2009.
Improved statistical machine translation usingmonolingually-derived paraphrases.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), Singapore.Yuval Marton.
2010.
Improved statistical machine trans-lation using monolingual text and a shallow lexical re-source for hybrid phrasal paraphrase generation.
InProceedings of the Ninth Conference of the Associa-tion for Machine Translation in the Americas (AMTA),Denver, Colorado.Aurelien Max.
2009.
Sub-sentential paraphrasing bycontextual pivot translation.
In Proceedings of the47th Annual Meeting of the Association for Compu-tational Linguistics (ACL) - the 4th InternationalJoint Conference on Natural Language Processing ofthe Asian Federation of Natural Language Processing(IJCNLP) - Workshop on Applied Textual Inference,pages 18?26, Singapore.
Suntec.Scott McDonald.
2000.
Environmental determinants oflexical processing effort.
Ph.D. thesis, University ofEdinburgh.Rada Mihalcea and Carlo Strapparava.
2005.
Makingcomputers laugh: Investigations in automatic humorrecognition.
In Proceedings of the Conference on Hu-man Language Technology and Empirical Methods inNatural Language Processing, pages 531?538, Van-couver, Canada.Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido Da-gan, Marc Dymetman, and Idan Szpektor .
2009.Source-language entailment modeling for translatingunknown terms.
In Proceedings of the 47th AnnualMeeting of the Association for Computational Linguis-tics (ACL) - the 4th International Joint Conferenceon Natural Language Processing of the Asian Federa-tion of Natural Language Processing (IJCNLP), pages791?799, Suntec, Singapore.Saif Mohammad, Bonnie Dorr, and Codie Dunn.
2008.Computing word-pair antonymy.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 982?991, Waikiki,Hawaii.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 440?447.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proceedings of ACL.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the ACL, pages 160?167.Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.2010.
Paraphrase lattice for statistical machine trans-lation.
In Proceedings of the Association for Computa-tional Linguistics (ACL) Short Papers, pages 1?5, Up-psala, Sweden.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1?2):1?135.Kishore Papineni, Salim Roukos, Todd Ward, John Hen-derson, and Florence Reeder.
2002.
Corpus-basedcomprehensive and diagnostic MT evaluation: InitialArabic, Chinese, French, and Spanish results.
In Pro-ceedings of the ACL Human Language TechnologyConference, pages 124?127, San Diego, CA.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Translationin the Americas, pages 223?231, Cambridge, MA.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,volume 2, pages 901?904.Philip Stone, Dexter C. Dunphy, Marshall S. Smith,Daniel M. Ogilvie, and associates.
1966.
The GeneralInquirer: A Computer Approach to Content Analysis.The MIT Press.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A con-ditional random field word segmenter.
In FourthSIGHAN Workshop on Chinese Language Processing.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.Journal of Articial Intelligence Research, 37:141?188.Peter Turney.
2008.
A uniform approach to analo-gies, synonyms, antonyms, and associations.
InProceedings of the 22nd International Conference248on Computational Linguistics (COLING), pages 905?912, Manchester, UK.Ellen M Voorhees.
2008.
Contradictions and jus-tifications: Extensions to the textual entailment task.In Proceedings of the 46th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), Colum-bus, OH.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributional sim-ilarity.
In Proceedings of the 20th InternationalConference on Computational Linguistics (COLING),pages 1015?1021, Geneva, Switzerland.Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, andSheng Li.
2008.
Combining multiple resources toimprove smt-based paraphrasing model.
In Proceed-ings of the Association for Computational Linguis-tics (ACL)Human Language Technology (HLT), pages1021?1029, Columbus, Ohio, USA.Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li.
2009.Application-driven statistical paraphrase generation.In Proceedings of the 47th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL) - the 4thInternational Joint Conference on Natural LanguageProcessing of the Asian Federation of Natural Lan-guage Processing (IJCNLP), pages 834?842, Suntec,Singapore.249
