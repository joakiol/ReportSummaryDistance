Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 869?873,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAutomatic Domain Partitioning for Multi-Domain LearningDi WangSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAdiwang@cs.cmu.eduChenyan XiongSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAcx@cs.cmu.eduWilliam Yang WangSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAww@cmu.eduAbstractMulti-Domain learning (MDL) assumes thatthe domain labels in the dataset are known.However, when there are multiple metadata at-tributes available, it is not always straightfor-ward to select a single best attribute for do-main partition, and it is possible that combin-ing more than one metadata attributes (includ-ing continuous attributes) can lead to betterMDL performance.
In this work, we proposean automatic domain partitioning approachthat aims at providing better domain identi-ties for MDL.
We use a supervised clusteringapproach that learns the domain distance be-tween data instances , and then cluster the datainto better domains for MDL.
Our experimenton real multi-domain datasets shows that us-ing our automatically generated domain parti-tion improves over popular MDL methods.1 IntroductionInstead of assuming data are i.i.d, Multi-domainlearning (MDL) methods assumes that data comefrom several domains and make use of domain la-bels to improve modeling performance (Daume?
III,2007).
The motivation of using MDL is that datasetsfrom different domains could be different, in twoways.
First, the feature distribution p(x) could bedomain specific, meaning that the importance ofeach feature is different across domains.
Second,the distribution of label Y given X , p(y|x), of dif-ferent domains could be different.
These differ-ences could create problems for traditional machinelearning methods: models learned from one domainmight not be generalizable to other domains (Ben-David et al 2006; Ben-David et al 2010).One common assumption of MDL methods is thatthe domain identities are pre-defined.
For example,in the multi-domain Amazon product review dataset(Finkel and Manning, 2009), the product categoriesare typically used as the domain identities.
How-ever, a question raised by Joshi et al(2012) is that,in real-world data sets, there could be many ways tosplit data into domains, and it is hard to decide whichone to use.
Consider the Amazon product reviews,where we have multiple attributes attached to eachreview: for example, product category, reviewer lo-cation, price, and number of feedback.
Which at-tribute is the most informative domain label?
Or weshould use all of these meta-data and partition thedata into many small domains?In this paper, we investigate the problem of au-tomatic domain partitioning.
We propose an em-pirical domain difference testing method to exam-ine whether two groups of data are i.i.d, or gener-ated from different distributions, and how differentthey are.
Using this approach, we generate data pairsthat belong to the same distribution, and data pairsthat should be partitioned into different domains.These pairs are then used as training data for a super-vised clustering algorithm, which automatically par-titions the dataset into several domains.
In the eval-uation, we show that our automatically-partitioneddomains improve the performances of two popularMDL methods on real sentiment analysis data sets.Note that Joshi et al(2013) proposed a Multi-Attribute Multi-Domain learning (MAMD) method,which also exploited multiple dimensions of meta-869data and provided extensions to two traditional MDLmethods.
However, extensions to the MAMD set-ting may not be trivial for every MDL algorithm,while our method serves as a pre-processing step andcan be easily used for all MDL approaches.
In ad-dition to this, MAMD only works with categoricalmetadata, and can not fully utilize information in theform of continuous metadata values.2 Automatic Domain PartitioningIn this section, we introduce the Automatic DomainPartitioning (ADP) problem: given data X , meta-data M and label Y , find a function g : M 7?
I suchthat the common MDL methods perform better withdata X and domain identity I in the prediction of Y .For example, on Amazon sentiment analysis data, Xis the feature matrix extracted from reviews, Y is thepositive or negative label vector, and M is the meta-data matrix associated with reviews (e.g.
productprice and category).Our approach works as follows: in training, wefirst use an empirical domain difference testingmethod to detect whether two groups of data shouldbe considered as different domains; after that we ap-ply supervised clustering to learn the distance met-ric between two data points, i.e.
how different thayare in MDL view, from training data generated byour domain difference test method; finally, based onthe distance metric learned, we cluster our data intoseveral clusters, and train MDL models with thoseclusters as domain labels; in testing, we assign datainstance to its nearest cluster and use that cluster asits domain identity, and then apply the trained MDLmodels for prediction.2.1 Empirical Domain Difference TestThe key motivation of MDL is that a model fits forone domain may not fit well for other domains.
Fol-lowing the same motivation, we propose an empiri-cal method for domain difference test called DomainModel Loss (DML) that provides us the domain dif-ference score d(G1, G2) between two groups of dataG1 = {X1, Y1} and G2 = {X2, Y2}.DomainModel Loss If the mapping functions f1 :X1 7?
Y1 and f2 : X2 7?
Y2 are different fortwo data groups, we could directly use the disagree-ment of f1 and f2 as domain difference score.
Morespecifically, if we train two classifiers f?1 : X1 7?Y1, f?2 : X2 7?
Y2 individually on G1 and G2, wecould have the K-fold empirical loss:l?
(f1, G1) =1K?iError of f1 on i-th fold of G1,l?
(f2, G2) =1K?iError of f2 on i-th fold of G2.And we could also apply the trained model f1 onG2, and f2 on G1 to get:l?
(f1, G2) = Error of f1 on G2,l?
(f2, G1) = Error of f2 on G1.Then, if G1 and G2 are actually the same with eachother, then both models will have same empiricalloss on either data set, but if they are not, we willhave a positive DML score:DML(G1, G2) =12(L?
(f1, G2) + L?
(f2, G1)),where:L?
(f1, G2) =l?
(f1, G2)?
l?
(f1, G1)l?
(f1, G1),L?
(f2, G1) =l?
(f2, G1)?
l?
(f2, G2)l?
(f2, G2).2.2 Supervised Clustering for DomainPartitioningOur domain difference test method calculates thedistance between two partitioned data groups.
How-ever, to directly use it for domain partitioning, wemust go through all possible combinations of do-main assignments in exponential time, which is in-feasible.
Our solution is to use a polynomial-timesupervised clustering method developed by Xing etal.
(2002) to learn a distance function that calculatesthe distance between any two data points.
Formally,given a set of data pairs D, which belong to differentdomains, and a set of data pairs S, which belong tothe same domain, it learns a distance metric A by:maxAg(A) =?(i,j)?D?
(mi ?mj)TA(mi ?mj)s.t.f(A) =?
(i,j)?S(mi ?mj)TA(mi ?mj) ?
1A  0,870where mi,mj are meta data of i and j.The metadata M are preprocessed as follows: 1)Each categorical attribute was converted to severalbinary questions, one per category, and each bi-nary question was considered as one metadata di-mension in ADP method.
For example, if categor-ical attribute ?Product Type?
has two values ?Mu-sic?
and ?Electronics?, then there will be two dimen-sions of metadata corresponding to ?Product Type?in ADP.
Two metadata dimensions correspond to bi-nary questions: ?Is Product Type Music?
and ?IsProduct Type Electronics?.
2) Each continuous at-tribute was normalized by scaling between 0 and 1.The training data S,D for metric learning are gen-erated as follows:1.
For each dimension Mk of M , split data atvalue 0.5, sample two equally sized groups, ap-ply our domain difference testing method andfind the difference between these data groups.2.
Assign distance to each pair of instances by theaverage distance of all partitions that partitionsthe pair into different groups.3.
Select top n similar pairs as S and top n differ-ent pairs as D.The learned distance metric A now conveys thedomain difference information obtained from ourdomain distance test results: which meta attributesare important for domain partitioning and which arenot as important.
Following Xing et al(2002), wetransfer the instance?s metadata feature M by MBT ,where BTB = A.
Then we use a clustering methodon MBT , and the output is our domain partitioningresult.3 Experiment MethodologyDatasets To evaluate our methods, we used twosubsets of Amazon review corpus (Jindal and Liu,2008), which originally contain 5.8 million reviewswith a variety of metadata about products and users.The first subset (BOOK) contains 20,000 reviews onbooks published by eleven most popular publishers,while the second (PROD) is reviews about productswithin seven most common product categories.
Werandomly split each dataset into training and testingsets with equal size.
The task is to predict a positiveor negative label for each review.
Case insensitiveunigrams excluding stop words are used as features,and all features appear less than 500 times are re-moved for efficient experiment processing.
Reviewsof 4 or 5 stars are considered positive and 1 or 2 starsare considered negative, while 3 stars reviews are ex-cluded.
Each review has multiple metadata such asbook?s publisher, product?s type, user?s state loca-tion, product price, review year, and number of otheruser feedback.
Reviews with missing metadata arefiltered out.MDL Methods Our first MDL algorithm is theFrustratingly Easy Domain Adaptation (FEDA)(Daume?
III, 2007) which is easy to implement andachieved competitive performance on many applica-tions.
It creates an augmented feature space as theCartesian product of the input features and the orig-inal domains plus a shared domain.
Then it uses aSVM classifier over the augmented feature space toobtain classification result.
Specifically, our FEDAmethods use L2-regularized SVM with linear ker-nel by LIBLINEAR package1.
The parameters C =0.01 was selected using five-fold cross-validation ontraining set.Our second MDL algorithm is Multi-DomainRegularization (MDR) (Dredze and Crammer,2008), which is a classifier combination ap-proach based on Confidence-Weighted (CW) learn-ing (Dredze et al 2008).
The CW learning is an on-line update method that maintains probabilistic con-fidence for each parameter by keeping track of itsvariance.
In our experiments, we use the CW im-plementation provided by its authors and choose thebest performing configurations described in (Dredzeand Crammer, 2008).Domain Partition Methods We evaluated the do-main partition results provided by our ADP on thetwo MDL methods (FEDA & MDR).
For simplic-ity and efficiency, we use Naive Bayes as our baseprediction model f1 and f2 to generate the domainmodel loss score, described in section 2.1.
In train-ing data generation, we choose top 10% similar pairsas S and top 10% different pairs as D. And giventhe learnt distance metric A, we use K-means to dothe clustering.
The number of clusters is selected byfive-fold cross-validation on training set.1http://www.csie.ntu.edu.tw/?cjlin/liblinear871We compare our domain partition quality withthree other methods: 1) 1-Best chooses best per-forming categorical metadata on a validation set asdomain indicators, where the original training setwas splitted equally to train and validate the per-formance of each categorical attribute; 2) Randompartition that assigns domain identities to instancesrandomly with same number of domains as 1-Best.We run each random partition ten times and took theaverage; 3) MAMD proposed by Joshi et al(2013).However, the original version of MAMD does notsupport continuous attribute such as price.
So wemade an extension that sorts these values to ten binsand then treats them as categorical values.4 Results and DiscussionsPartition + MDL PROD BOOKADP + FEDA 82.02 ?
86.22 ?
?MAMD + FEDA 81.04 86.081-Best + FEDA 82.00 85.85Random + FEDA 79.36 84.72ADP + MDR 82.10 ] ?
?
86.62 ] ?
?MAMD + MDR 80.17 84.371-Best + MDR 79.79 83.68Random + MDR 74.65 81.16Table 1: Overall accuracies on PROD and BOOKdatasets.
ADP results that are statistically significantlybetter than MAMD are marked with ], and better than 1-Best and Random are indicated by ?
and ?
respectively,using a paired t-test, with p < 0.05.Table 1 shows the overall experimental resultsof four domain partition methods with two MDLmethods on PROD and BOOK datasets.
One couldsee that when using MDR method, ADP couldsignificantly outperform all baselines on both datasets, with relatively more than 2% gains.
ForFEDA, on PROD data, ADP performs the same withMAMD and 1-Best; on BOOK data, ADP outper-forms 1-Best significantly, but is just slightly betterthan MAMD.
One possible reason is that the bestnumbers of cluster selected by cross-validation arearound 150.
With such large number of none-perfectdomains, FEDA will generate huge dimension offeatures and perhaps require more training data toprovide better performances.
Another possible rea-son is that FEDA and the SVM underlying FEDAare very robust against bad domain partition results.This might be the reason of high FEDA baselines.In general, our ADP method helps existing MDLapproaches achieve better performance, while bad(Random) partitioning does hurt.Figure 1(a) and 1(b) shows the performances ofapplying FEDA on different domain partitioningmethods on PROD and BOOK, while Figure 1(c)and 1(d) shows experiment results with MDR.
Thex-axis is the size of the output domains (the K inour K-means clustering), and y-axis is the accuracyof models.
With our domain partitioning approach,MDR can perform consistently higher than all thethree baselines on both dataset when k > 50.
Aswe discussed for Table 1, FEDA might be less sen-sitive to domain partition results, which causes highbaseline performance and high ADP+FEDA perfor-mance with small K. Since the performance trendsto increase along with k until 50 in three figures(1(b), 1(c) and 1(d)), we believe that the ground-truth domain size is likely larger than 50.
Theseresults clearly indicate ADP does provide more de-sirable domain assignments for MDL.
The domainselected by 1-Best such as publishers has only 11domains, which limits the ability of 1-Best to com-pletely express domain information.
And our gener-ated domains integrate multiple metadata attributes,lead to more detailed domain partitions, and enhancethe ability of MDL methods to capture the differencebetween different groups of data.
Although accu-racies are growing with k in general, we also seethat there are fluctuations on curves especially whencurves are zoomed to a small range.
To get smootherresults, we can sample more data to calculate do-main similarity and repeat the K-means clusteringwith more different initializations.5 ConclusionsIn this paper, we propose an Automatic Domain Par-tition (ADP) method that provides better domainidentities for multi-domain learning methods.
Wefirst propose a new approach to identify whether twodata groups should be considered as different do-mains, by comparing the differences using DomainModel Loss.
We use a supervised clustering ap-proach to train our model with labels generated bydomain difference tests, and cluster the re-weighted872metadata as our domain partition by K-means.
Ex-periments on real world multi-domain data showthat the domain identities generated by our methodcan improve the performance of MDL models.0 50 100 150 2007979.58080.58181.58282.5Number of domains (K)Accuracy %ADP1?BestRandomMAMD(a) FEDA results on PROD0 50 100 150 20084.58585.58686.587Number of domains (K)Accuracy %ADP1?BestRandomMAMD(b) FEDA results on BOOK0 50 100 150 20072747678808284Number of domains (K)Accuracy %ADP1?BestRandomMAMD(c) MDR results on PROD0 50 100 150 20081828384858687Number of domains (K)Accuracy %ADP1?BestRandomMAMD(d) MDR results on BOOKFigure 1: Accuracies over different size of the output do-mains (K)ReferencesShai Ben-David, John Blitzer, Koby Crammer, and Fer-nando Pereira.
2006.
Analysis of representations fordomain adaptation.
In Advances in Neural Informa-tion Processing Systems (NIPS), pages 137?144.Shai Ben-David, John Blitzer, Koby Crammer, AlexKulesza, Fernando Pereira, and Jennifer WortmanVaughan.
2010.
A theory of learning from differentdomains.
Machine Learning, 79(1-2):151?175.Mark Dredze and Koby Crammer.
2008.
Online methodsfor multi-domain learning and adaptation.
In Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP), pages 689?697.Mark Dredze, Koby Crammer, and Fernando Pereira.2008.
Confidence-weighted linear classification.
InMachine Learning, Proceedings of the Twenty-FifthInternational Conference (ICML), pages 264?271.Jenny Rose Finkel and Christopher D. Manning.
2009.Hierarchical bayesian domain adaptation.
In Proceed-ings of the 2009 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies (NAACL-HLT),pages 602?610.Hal Daume?
III.
2007.
Frustratingly easy domain adapta-tion.
In Proceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics (ACL).Nitin Jindal and Bing Liu.
2008.
Opinion spam and anal-ysis.
In Proceedings of the International Conferenceon Web Search and Web Data Mining (WSDM), pages219?230.Mahesh Joshi, Mark Dredze, William W. Cohen, andCarolyn Penstein Rose?.
2012.
Multi-domain learn-ing: When do domains matter?
In Proceedings of the2012 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, (EMNLP-CoNLL), pages 1302?1312.Mahesh Joshi, Mark Dredze, William W. Cohen, andCarolyn P. Rose?.
2013.
Whats in a domain?
multi-domain learning for multi-attribute data.
In Proceed-ings of the 2013 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies (NAACL-HLT),pages 685?690, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stu-art J. Russell.
2002.
Distance metric learning withapplication to clustering with side-information.
InAdvances in Neural Information Processing Systems(NIPS), pages 505?512.873
