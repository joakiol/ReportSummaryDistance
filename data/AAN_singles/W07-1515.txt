Proceedings of the Linguistic Annotation Workshop, pages 93?100,Prague, June 2007. c?2007 Association for Computational LinguisticsAnnotating Expressions of Appraisal in EnglishJonathon Read, David Hope and John CarrollDepartment of InformaticsUniversity of SussexUnited Kingdom{j.l.read,drh21,j.a.carroll}@sussex.ac.ukAbstractThe Appraisal framework is a theory of thelanguage of evaluation, developed within thetradition of systemic functional linguistics.The framework describes a taxonomy of thetypes of language used to convey evaluationand position oneself with respect to the eval-uations of other people.
Accurate automaticrecognition of these types of language caninform an analysis of document sentiment.This paper describes the preparation of testdata for algorithms for automatic Appraisalanalysis.
The difficulty of the task is as-sessed by way of an inter-annotator agree-ment study, based on measures analogous tothose used in the MUC-7 evaluation.1 IntroductionThe Appraisal framework (Martin and White, 2005)describes a taxonomy of the language employed incommunicating evaluation, explaining how users ofEnglish convey attitude (emotion, judgement of peo-ple and appreciation of objects), engagement (as-sessment of the evaluations of other people) andhow writers may modify the strength of their atti-tude/engagement.
Accurate automatic analysis ofthese aspects of language will augment existing re-search in the fields of sentiment (Pang et al, 2002)and subjectivity analysis (Wiebe et al, 2004), but as-sessing the usefulness of analysis algorithms lever-aging the Appraisal framework will require test data.At present there are no machine-readableAppraisal-annotated texts publicly available.
Real-world instances of Appraisal in use are limitedto example extracts that demonstrate the theory,coming from a wide variety of genres as disparateas news reporting (White, 2002; Martin, 2004) andpoetry (Martin and White, 2005).
These examples,while useful in demonstrating the various aspectsof Appraisal, can only be employed in a qualitativeanalysis and would bring about inconsistenciesif analysed collectively ?
one can expect thewriting style to depend upon the genre, resulting insignificantly different syntactic constructions andlexical choices.We therefore need to examine Appraisal acrossdocuments in the same genre and investigate pat-terns within that particular register.
This paper dis-cusses the methodology of an Appraisal annotationstudy and an analysis of the inter-annotator agree-ment exhibited by two human judges.
The outputof this study has the additional benefit of bringinga set of machine-readable annotations of Appraisalinto the public domain for further research.This paper is structured as follows.
The next sec-tion offers an overview of the Appraisal framework.Section 3 discusses the methodology adopted forthe annotation study.
Section 4 discusses the mea-sures employed to assess inter-annotator agreementand reports the results of these measures.
Section5 offers an analysis of cases of systematic disagree-ment.
Other computational work utilising the Ap-praisal framework is reviewed in Section 6.
Section7 summarises the paper and outlines future work.2 The linguistic framework of AppraisalThe Appraisal framework (Martin and White, 2005)is a development of work in Systemic Functional93appraisalattitudeengagementgraduationaffectjudgementappreciationinclinationhappinesssecuritysatisfactionesteemsanctionnormalitycapacitytenacityveracityproprietyreactioncompositionvaluationimpactqualitybalancecomplexitycontractexpanddisclaimproclaimdenycounterpronounceendorseconcuraffirmconcedeentertainattribute acknowledgedistanceforcefocusquantificationintensificationnumbermassextentproximity (space)proximity (time)distribution (space)distribution (time)degreevigourFigure 1: The Appraisal framework.Linguistics (Halliday, 1994) and is concerned withinterpersonal meaning in text?the negotiation ofsocial relationships by communicating emotion,judgement and appreciation.
The taxonomy de-scribed by the Appraisal framework is depicted inFigure 1.Appraisal consists of three subsystems that oper-ate in parallel: attitude looks at how one expressesprivate state (Quirk et al, 1985) (one?s emotion andopinions); engagement considers the positioning ofoneself with respect to the opinions of others andgraduation investigates how the use of languagefunctions to amplify or diminish the attitude and en-gagement conveyed by a text.2.1 Attitude: emotion, ethics and aestheticsThe Attitude sub-system describes three areas of pri-vate state: emotion, ethics and aesthetics.
An atti-tude is further qualified by its polarity (positive ornegative).
Affect identifies feelings?author?s emo-tions as represented by their text.
Judgement dealswith authors?
attitude towards the behaviour of peo-ple; how authors applaud or reproach the actionsof others.
Appreciation considers the evaluation ofthings?both man-made and natural phenomena.2.2 Engagement: appraisals of appraisalsThrough engagement, Martin and White (2005) dealwith the linguistic constructions by which authorsconstrue their point of view and the resources usedto adopt stances towards the opinions of other peo-ple.
The theory of engagement follows Stubbs(1996) in that it assumes that all utterances conveypoint of view and Bakhtin (1981) in supposing thatall utterances occur in a miscellany of other utter-ances on the same motif, and that they carry bothimplicit and explicit responses to one another.
Inother words, all text is inherently dialogistic as it en-codes authors?
reactions to their experiences (includ-ing previous interaction with other writers).
Engage-ment can be both retrospective (that is, an author willacknowledge and agree or disagree with the stancesof others who have previously appraised a subject),and prospective (one may anticipate the responses ofan intended audience and include counter-responsesin the original text).2.3 Graduation: strength of evaluationsMartin and White (2005) consider the resources bywhich writers alter the strength of their evaluationas a system of graduation.
Graduation is a generalproperty of both attitude and engagement.
In atti-tude it enables authors to convey greater or lesserdegrees of positivity or negativity, while graduationof engagements scales authors?
conviction in theirutterance.Graduation is divided into two subsystems.
Forcealters appraisal propositions in terms of its inten-94sity, quantity or temporality, or by means of spatialmetaphor.
Focus considers the resolution of seman-tic categories, for example:They play real jazz.They play jazz, sort of.In real terms a musician either plays jazz or theydo not, but these examples demonstrate how authorsblur the lines of semantic sets and how binary rela-tionships can be turned into scalar ones.3 Annotation methodologyThe corpus used in this study consists of uneditedbook reviews.
Book reviews are good candidates forthis study as, while they are likely to contain similarlanguage by virtue of being from the same genre ofwriting, we can also expect examples of Appraisal?smany classes (for example, the emotion attributedto the characters in reviews of novels, judgementsof authors?
competence and character, appreciationof the qualities of books and engagement with thepropositions put forth by the authors under review).The articles were taken from the web sites offour British newspapers (The Guardian, The Inde-pendent, The Telegraph and The Times) on two dif-ferent dates?31 July 2006 and 11 September 2006.Each review is attributed to a unique author.
Thecorpus is comprised of 38 documents, containing atotal of 36,997 tokens in 1,245 sentences.Two human annotators, d and j, participated inthis study, assigning tags independently.
The anno-tators were well-versed in the Appraisal framework,having studied the latest literature.
The judges wereasked to annotate appraisal-bearing terms with theappraisal type presumed to be intended by the au-thor of the text.
They were asked to highlight eachexample of appraisal and specify the type of atti-tude, engagement or graduation present.
They alsoassigned a polarity (positive or negative) to attitudi-nal items and a scaling (up or down) to graduatingitems, employing a custom-developed software toolto annotate the documents.Four alternative annotation strategies were con-sidered.
One approach is to allow only a single tokenper annotation.
However, this is too simplistic foran Appraisal annotation study?a unit of Appraisalis frequently larger than a single token.
Consider thefollowing examples:(1)The design was deceptively?VERACITY simple?COMPLEXITY.
(?
)(2)The design was deceptively simple?COMPLEXITY.Example 1 demonstrates that a single-token ap-proach is inappropriate as it ascribes a judgementof someone?s honesty, whereas Example 2 indicatesthe correct analysis?the sentence is an apprecia-tion of the simplicity of the ?design?.
This exampleshows how it is necessary to annotate larger units ofappraisal-bearing language.Including more tokens, however, increases thecomplexity of the annotation task, and reduces thelikelihood of agreement between the judges, as theannotated tokens of one judge may be a subset of,or overlap with, those of another.
We therefore ex-perimented with tagging entire sentences in order toconstrain the annotators?
range of choices.
This re-sulted in its own problems as there is often more thanone appraisal in a sentence, for example:(3)The design was deceptively simple?COMPLEXITYand belied his ingenuity?CAPACITY.An alternative approach is to permit annotatorsto tag an arbitrary number of contiguous tokens.Arbitrary-length tagging is disadvantageous as thejudges will frequently tag units of differing length,but this can be compensated for by relaxing the rulesfor agreement?for example, by allowing intersect-ing annotations to match successfully (Wiebe et al,2005).
Bruce and Wiebe (1999) employ anotherapproach, creating units from every non-compoundsentence and each conjunct of every compound sen-tence.
This side-steps the problem of ambiguity inappraisal unit length, but will still fail to capture bothappraisals demonstrated in the second conjunct ofExample 4.
(4)The design was deceptively simple?COMPLEXITYand belied his remarkable?NORMALITYingenuity?CAPACITY.Ultimately in this study, we permitted judges toannotate any number of tokens in order to allowfor multiple Appraisal units of differing sizes withinsentences.
Annotation was carried out over tworounds, punctuated by an intermediary analysis of95d j d j d jInclination 1.26 3.50 Balance 2.64 1.84 Distance 0.69 0.59Happiness 2.80 2.32 Complexity 2.52 2.74 Number 0.82 2.63Security 4.31 2.22 Valuation 6.08 9.29 Mass 0.22 1.63Satisfaction 1.67 2.32 Deny 3.05 3.67 Proximity (Space) 0.09 0.14Normality 8.00 4.44 Counter 4.79 3.78 Proximity (Time) 0.03 0.55Capacity 11.46 9.63 Pronounce 3.84 1.21 Distribution (Space) 0.41 1.39Tenacity 3.72 4.44 Endorse 2.05 1.49 Distribution (Time) 0.82 2.56Veracity 3.15 2.01 Affirm 0.54 1.14 Degree 4.38 5.72Propriety 13.32 12.61 Concede 0.38 0.03 Vigour 0.60 0.45Impact 6.11 4.23 Entertain 2.27 2.43 Focus 3.02 2.29Quality 2.55 3.40 Acknowledge 2.42 3.33Table 1: The distribution of the Appraisal types selected by each annotator (%).d jDocuments 115.74 77.21Sentences 3.65 2.43Words 0.12 0.08Table 2: The density of annotations relative to thenumber of documents, sentences and words.agreement and disagreement between the two anno-tators.
The judges discussed examples of the mostcommon types of disagreement in an attempt to ac-quire a common understanding for the second round,but annotations from the first round were left unal-tered.Following the methodology described above, dmade 3,176 annotations whilst j made 2,886 anno-tations.
The distribution of the Appraisal types as-cribed is shown in Table 1, while Table 2 details thedensity of annotations in documents, sentences andwords.4 Measuring inter-annotator agreementThe study of inter-annotator agreement begins byconsidering the level of agreement exhibited by theannotators in deciding which tokens are representa-tive of Appraisal, irrespective of the type.
As dis-cussed, this is problematic as judges are liable tochoose different length token spans when markingup what is essentially the same appraisal, as demon-strated by Example 5.
(5)[d] It is tempting to point to the bombs in Lon-don and elsewhere, to the hideous mess?QUALITYin Iraq, to recent victories of the Islamists, tothe violent and polarised rhetoric?PROPRIETY andanswer yes.
[j] It is tempting to point to the bombs inLondon and elsewhere, to the hideous?QUALITYmess?BALANCE in Iraq, to recent victories of Is-lamists, to the violent?PROPRIETY and polarised?PROPRIETY rhetoric and answer yes.Wiebe et al (2005), who faced this problem whenannotating expressions of opinion under their ownframework, accept that it is necessary to consider thevalidity of all judges?
interpretations and thereforeconsider intersecting annotations (such as ?hideous?and ?hideous mess?)
to be matches.
The same relax-ation of constraints is employed in this study.Tasks with a known number of annotative unitscan be analysed with measures of agreement such asCohen?s ?
Coefficient (1960), but the judges?
free-dom in this task prohibits meaningful application ofthis measure.
For example, consider howword senseannotators are obliged to choose from a limited fixedset of senses for each token, whereas judges anno-tating Appraisal are free to select one of thirty-twoclasses for any contiguous substring of any lengthwithin each document; there are 16(n2 ?
n)pos-sible choices in a document of n tokens (approxi-mately 6.5 ?
108 possibilities in this corpus).A wide range of evaluation metrics have been em-ployed by the Message Understanding Conferences(MUCs).
The MUC-7 tasks included extraction ofnamed entities, equivalence classes, attributes, factsand events (Chinchor, 1998).
The participating sys-tems were evaluated using a variety of related mea-sures, defined in Table 3.
These tasks are similar toAppraisal annotation in that the units are formed ofan arbitrary number of contiguous tokens.In this study the agreement exhibited by an an-notator a is evaluated as a pair-wise comparisonagainst the other annotator b. Annotator b provides96COR Number correctINC Number incorrectMIS Number missingSPU Number spuriousPOS Number possible = COR + INC + MISACT Number actual = COR + INC + SPUFSC F-score = (2 ?
REC ?
PRE)/ (REC + PRE)REC Precision = COR/POSPRE Recall = COR/ACTSUB Substitution = INC/ (COR + INC)ERR Error per response = (INC + SPU + MIS)/ (COR + INC + SPU + MIS)UND Under-generation = MIS/POSOVG Over-generation = SPU/ACTTable 3: MUC-7 score definitions (Chinchor 1998).FSC REC PRE ERR UND OVGd 0.682 0.706 0.660 0.482 0.294 0.340j 0.715 0.667 0.770 0.444 0.333 0.230x?
0.698 0.686 0.711 0.462 0.312 0.274Table 4: MUC-7 test scores, evaluating the agree-ment in text anchors selected by the annotators.
x?denotes the average value, calculated using the har-monic mean.a presumed gold standard for the purposes of evalu-ating agreement.
Note, however, that in this case itdoes not necessarily follow that REC (a w.r.t.
b) =PRE (b w.r.t.
a).
Consider that a may tend to makeone-word annotations whilst b prefers to annotatephrases; the set of a?s annotations will contain mul-tiple matches for some of the phrases annotated by b(refer to Example 5, for instance).
The ?number cor-rect?
will differ for each annotator in the pair underevaluation.Table 4 lists the values for the MUC-7 measuresapplied to the text spans selected by the annota-tors.
Annotator d is inclined to identify text as Ap-praisal more frequently than annotator j.
This re-sults in higher recall for d, but with lower preci-sion.
Naturally, the opposite observation can bemade about annotator j.
Both annotators exhibit ahigh error rate at 48.2% and 44.4% for d and j re-spectively.
The substitution rate is not listed as thereare no classes to substitute when considering onlytext anchor agreement.
The second round of anno-tation achieved slightly higher agreement (the meanF-score increased by 0.033).FSC REC PRE SUB ERR0 0.698 0.686 0.711 0.000 0.4621 0.635 0.624 0.647 0.090 0.5112 0.528 0.518 0.538 0.244 0.5943 0.448 0.441 0.457 0.357 0.6554 0.396 0.388 0.403 0.433 0.6965 0.395 0.388 0.403 0.433 0.696Table 5: Harmonic means of the MUC-7 test scoresevaluating the agreement in text anchors and Ap-praisal classes selected by the annotators, at eachlevel of hierarchical abstraction.Having considered the annotators?
agreementwith respect to text anchors, we go on to analysethe agreement exhibited by the annotators with re-spect to the types of Appraisal assigned to the textanchors.
The Appraisal framework is a hierarchi-cal system?a tree with leaves corresponding to theannotation types chosen by the judges.
When in-vestigating agreement in Appraisal type, the follow-ing measures include not just the leaf nodes but alsotheir parent types, collapsing the nodes into increas-ingly abstract representations.
For example happi-ness is a kind of affect, which is a kind of attitude,which is a kind of appraisal.
These relationships aredepicted in full in Figure 2.
Note that in the follow-ing measurements of inter-annotator agreement leafnodes are included in subsequent levels (for exam-ple, focus is a leaf node at level 2, but is also consid-ered to be a member of levels 3, 4 and 5).Table 5 shows the harmonic means of the MUC-7 measures of the annotators?
agreement at each ofthe levels depicted in Figure 2.
As one might ex-pect, the agreement steadily drops as the classes be-come more concrete?classes become more specificand more numerous so the complexity of the taskincreases.Table 5 also lists the average rate of substitutionsas the annotation task?s complexity increases, show-ing that the annotators were able to fairly easilydistinguish between instances of the three subsys-tems of Appraisal (Attitude, Engagement and Grad-uation) as the substitution rate at level 1 is low (only9%).
As the number of possible classes increases an-notators are more likely to confuse appraisal types,with disagreement occurring on approximately 44%of annotations at level 5.
The second round of an-notations resulted in slightly improved agreement at97Level 0: .698Level 1: .635Level 2: .528Level 3: .448Level 4: .396Level 5: .395appraisalattitude: .701engagement: .507graduation: .479affect: .519judgement: .586appreciation: .567contract: .502expand: .445force: .420focus: .287inclination: .249happiness: .448security: .335satisfaction: .374esteem: .489sanction: .575reaction: .510composition: .432valuation: .299disclaim: .555proclaim: .336entertain: .459attribute: .427quantification: .233intensification: .513normality: .289capacity: .431tenacity: .395veracity: .519propriety: .540impact: .462quality: .336balance: .300complexity: .314deny: .451counter: .603pronounce: .195endorse: .331concur: .297acknowledge: .390distance: .415number: .191mass: .104extent: .242degree: .510vigour: .117affirm: .325concede: .000proximity (space): .000proximity (time): .000distribution (space): .110distribution (time): .352Figure 2: The Appraisal framework with hierarchical levels highlighted.
Appraisal classes and levels areaccompanied by the harmonic mean of the F-scores of the annotators for that class/level.each level of abstraction (the mean F-score increasedby 0.051 at the most abstract level).Of course, some Appraisal classes are easier toidentify than others.
Figure 2 summarises the agree-ment for each node in the Appraisal hierarchy withthe harmonic mean of the F-scores of the annotatorsfor each class.
Typically, the attitude annotations areeasiest to identify, whereas the other subsystems ofengagement and graduation tend to be more difficult.The Proximity children of Extent exhibited noagreement whatsoever.
This seems to have arisenfrom the differences in the judges?
interpretations ofproximity.
In the case of Proximity (Space), for ex-ample, one judge annotated words that function tomodify the spatial distance of other concepts (e.g.near), whereas the other selected words placing con-cepts at a specific location (e.g.
homegrown, local).This confusion between modifying words and spe-98cific locations also accounts for the low agreementin the Distribution (Space) type.The measures show that it is also difficult toachieve a consensus on what qualifies as engage-ments of the Pronounce type.
Both annotators selectexpressions that assert the irrefutability of a propo-sition (e.g.
certainly or in fact or it has to be said).Judge d, however, tends to perceive pronouncementas occurring wherever the author makes an assertion(e.g.
this is or there will be).
Judge j seems to re-quire that the assertion carry a degree of emphasis toinclude a term in the Pronounce class.The low agreement of the Mass graduations canalso be explained in this way, as both d and j se-lect strong expressions relating to size (e.g.
massiveor scant).
Annotator j found additional but weakerterms like largely or slightly.The Pronounce and Mass classes provide typicalexamples of the disagreement exhibited by the an-notators.
It is not that the judges have wildly differ-ent understandings of the system, but rather they dis-agree in the bounds of a class?one annotator mayrequire a greater degree of strength of a term to war-rant its inclusion in a class.Contingency tables (not depicted due to spaceconstraints) reveal some interesting tendencies forconfusion between the two annotators.
Approxi-mately 33% of d?s annotations of Proximity (Space)were ascribed as Capacity by j.
The high percent-age is due to the rarity of annotations of Proxim-ity (Space), but the confusion comes from differingunits of Appraisal, as shown in Example 6.
(6)[d] But at key points in this story, one getsthe feeling that the essential factors are op-erating just outside?PROXIMITY (SPACE)James?s field of vision?CAPACITY.
[j] But at key points in this story, one gets thefeeling that the essential factors are operating justoutside James?s field of vision?CAPACITY.Another interesting case of frequent confusion isthe pair of Satisfaction and Propriety.
Though notclosely related in the Attitude subsystem, j choosesPropriety for 21% of d?s annotations of Satisfaction.The confusion is typified by Example 7, where it isapparent that there is disagreement in terms of whois being appraised.
(7)[d] Like him, Vermeer ?
or so he chose to be-lieve ?
was an artist neglected?SATISFACTION andwronged?SATISFACTION by critics and who haddied an almost unknown.
[j] Like him, Vermeer ?
or so he chose to believe?
was an artist neglected and wronged?PROPRIETYby critics and who had died an almost unknown.Annotator d believes that the author is communi-cating the artist?s dissatisfaction with the way he istreated by critics, whereas j believes that the criticsare being reproached for their treatment of the artist.This highlights a problem with the coding scheme,which simplifies the task by assuming only one typeof Appraisal is conveyed by each unit.5 Related workTaboada and Grieve (2004) initiated computationalexperimentation with the Appraisal framework, as-signing adjectives into one of the three broad atti-tude classes.
The authors apply SO-PMI-IR (Turney,2002) to extract and determine the polarity of adjec-tives.
They then use a variant of SO-PMI-IR to de-termine a ?potential?
value for affect, judgement andappreciation, calculating the mutual information be-tween the adjective and three pronoun-copular pairs:I was (affect); he was (judgement) and it was (ap-preciation).
While the pairs seem compelling mark-ers of the respective attitude types, they incorrectlyassume that appraisals of affect are limited to thefirst person whilst judgements are made only of thethird person.
We can expect a high degree of overlapbetween the sets of documents retrieved by queriesformed using these pairs (e.g.
I was a happy ?X?
;he was a happy ?X?
; It was a happy ?X?
).Whitelaw et al (2005) use the Appraisal frame-work to specify frames of sentiment.
These ?Ap-praisal Groups?
are derived from aspects of Attitudeand Graduation:Attitude: affect | judgement | appreciationOrientation positive | negativeForce: low | neutral | highFocus: low | neutral | highPolarity: marked | unmarkedTheir process begins with a semi-automatically con-structed lexicon of these Appraisal groups, built us-ing example terms from Martin and White (2005) asseeds into WordNet synsets.
The frames supplementbag of words-based machine learning techniques for99sentiment analysis and they achieve minor improve-ments over unigram features.6 SummaryThis paper has discussed the methodology of an ex-ercise annotating book reviews according to the Ap-praisal framework, a functional linguistic theory ofevaluation in English.
The agreement exhibited bytwo human judges was measured by analogy withthe evaluation employed for the MUC-7 shared tasks(Chinchor, 1998).The agreement varied greatly depending on thelevel of abstraction in the Appraisal hierarchy(a mean F-score of 0.698 at the most abstractlevel through to 0.395 at the most concrete level).The agreement also depended on the type beingannotated?there was more agreement evident fortypes of attitude compared to types of engagementor graduation.The exercise is the first step in an ongoing studyof approaches for the automatic analysis of expres-sions of Appraisal.
The primary output of this workis a corpus of book reviews independently annotatedwith Appraisal types by two coders.
Agreement wasin general low, but if one assumes that the intersec-tion of both sets of annotations contains reliable ex-amples, this leaves 2,223 usable annotations.Future work will employ these annotations toevaluate algorithms for the analysis of Appraisal,and investigate the usefulness of the Appraisalframework when in the computational analysis ofdocument sentiment and subjectivity.AcknowledgmentsWe would like to thank Bill Keller for advice whendesigning the annotation methodology.
The work ofthe first author is supported by a UK EPSRC stu-dentship.ReferencesM.
M. Bakhtin.
1981.
The Dialogic Imagination.
Uni-versity of Texas Press, Austin.
Translated by C. Emer-son & M. Holquist.Rebecca Bruce and Janyce Wiebe.
1999.
Recognizingsubjectivity: a case study in manual tagging.
NaturalLanguage Engineering, 5(1):1?16.N.
Chinchor.
1998.
MUC-7 test scores introduction.In Proceedings of the Seventh Message UnderstandingConference.Jacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Measures,20:37?46.M.
A. K. Halliday.
1994.
An Introduction to FunctionalGrammar.
Edward Arnold, London.J.
R. Martin and P. R. R. White.
2005.
Language of Eval-uation: Appraisal in English.
Palgrave Macmillan.J.
R. Martin.
2004.
Mourning: how we get algned.
Dis-course & Society, 15(2-3):321?344.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing, Philadelphia, PA, USA.Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,and Jan Svartvik.
1985.
A Comprehensive Grammarof the English Language.
Longman.M.
Stubbs.
1996.
Towards a modal grammar of English:a matter of prolonged fieldwork.
In Text and CorpusAnalysis.
Blackwell, Oxford.Maite Taboada and Jack Grieve.
2004.
Analyzing Ap-praisal automatically.
In Spring Symposium on Ex-ploring Attitude and Affect in Text.
American Associa-tion for Artificial Intelligence, Stanford.
AAAI Tech-nical Report SS-04-07.Peter D. Turney.
2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics, Philadelphia, PA, USA.P.
R. R.White.
2002.
Appraisal?
the language of evalu-ation and stance.
In Jef Verschueren, Jan-Ola O?stman,Jan Blommaert, and Chris Bulcaen, editors, Handbookof Pragmatics, pages 1?27.
John Benjamins, Amster-dam.Casey Whitelaw, Navendu Garg, and Shlomo Argamon.2005.
Using appraisal groups for sentiment analysis.In Proceedings of the 14th ACM international confer-ence on Information and knowledge management.Janyce Wiebe, Theresa Wilson, Rebecca Bruce, MatthewBell, and Melanie Martin.
2004.
Learning subjectivelanguage.
Computational linguistics, 30(3):277?308.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation, 39(2-3):165?210.100
