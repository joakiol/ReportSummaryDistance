Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 654?663,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsProbabilistic Hierarchical Clustering ofMorphological ParadigmsBurcu CanDepartment of Computer ScienceUniversity of YorkHeslington, York, YO10 5GH, UKburcucan@gmail.comSuresh ManandharDepartment of Computer ScienceUniversity of YorkHeslington, York, YO10 5GH, UKsuresh@cs.york.ac.ukAbstractWe propose a novel method for learningmorphological paradigms that are struc-tured within a hierarchy.
The hierarchi-cal structuring of paradigms groups mor-phologically similar words close to eachother in a tree structure.
This allows detect-ing morphological similarities easily lead-ing to improved morphological segmen-tation.
Our evaluation using (Kurimo etal., 2011a; Kurimo et al 2011b) datasetshows that our method performs competi-tively when compared with current state-of-art systems.1 IntroductionUnsupervised morphological segmentation of atext involves learning rules for segmenting wordsinto their morphemes.
Morphemes are the small-est meaning bearing units of words.
The learn-ing process is fully unsupervised, using only rawtext as input to the learning system.
For example,the word respectively is split into morphemes re-spect, ive and ly.
Many fields, such as machinetranslation, information retrieval, speech recog-nition etc., require morphological segmentationsince new words are always created and storingall the word forms will require a massive dictio-nary.
The task is even more complex, when mor-phologically complicated languages (i.e.
agglu-tinative languages) are considered.
The sparsityproblem is more severe for more morphologicallycomplex languages.
Applying morphological seg-mentation mitigates data sparsity by tackling theissue with out-of-vocabulary (OOV) words.In this paper, we propose a paradigmatic ap-proach.
A morphological paradigm is a pair(StemList, SuffixList) such that each concatena-tion of Stem+Suffix (where Stem ?
StemList andSuffix ?
SuffixList) is a valid word form.
Thelearning of morphological paradigms is not novelas there has already been existing work in this areasuch as Goldsmith (2001), Snover et al(2002),Monson et al(2009), Can and Manandhar (2009)and Dreyer and Eisner (2011).
However, none ofthese existing approaches address learning of thehierarchical structure of paradigms.Hierarchical organisation of words help cap-ture morphological similarities between words ina compact structure by factoring these similaritiesthrough stems, suffixes or prefixes.
Our inferencealgorithm simultaneously infers latent variables(i.e.
the morphemes) along with their hierarchicalorganisation.
Most hierarchical clustering algo-rithms are single-pass, where once the hierarchi-cal structure is built, the structure does not changefurther.The paper is structured as follows: section 2gives the related work, section 3 describes theprobabilistic hierarchical clustering scheme, sec-tion 4 explains the morphological segmenta-tion model by embedding it into the clusteringscheme and describes the inference algorithmalong with how the morphological segmentationis performed, section 5 presents the experimentsettings along with the evaluation scores, and fi-nally section 6 presents a discussion with a com-parison with other systems that participated inMorpho Challenge 2009 and 2010 .2 Related WorkWe propose a Bayesian approach for learning ofparadigms in a hierarchy.
If we ignore the hierar-chical aspect of our learning algorithm, then our654walk walking talked  talks{walk}{0,ing} {talk}{ed,s} {quick}{0,ly}quick quickly{walk, talk, quick}{0,ed,ing,ly, s}{walk, talk}{0,ed,ing,s}Figure 1: A sample tree structure.method is similar to the Dirichlet Process (DP)based model of Goldwater et al(2006).
Fromthis perspective, our method can be understoodas adding a hierarchical structure learning layeron top of the DP based learning method proposedin Goldwater et al(2006).
Dreyer and Eisner(2011) propose an infinite Diriclet mixture modelfor capturing paradigms.
However, they do notaddress learning of hierarchy.The method proposed in Chan (2006) alsolearns within a hierarchical structure where La-tent Dirichlet Allocation (LDA) is used to findstem-suffix matrices.
However, their work is su-pervised, as true morphological analyses of wordsare provided to the system.
In contrast, our pro-posed method is fully unsupervised.3 Probabilistic Hierarchical ModelThe hierarchical clustering proposed in this workis different from existing hierarchical clusteringalgorithms in two aspects:?
It is not single-pass as the hierarchical struc-ture changes.?
It is probabilistic and is not dependent on adistance metric.3.1 Mathematical DefinitionIn this paper, a hierarchical structure is a binarytree in which each internal node represents a clus-ter.Let a data set be D = {x1, x2, .
.
.
, xn} andT be the entire tree, where each data point xi islocated at one of the leaf nodes (see Figure 2).Here, Dk denotes the data points in the branchTk.
Each node defines a probabilistic model forwords that the cluster acquires.
The probabilisticDiDkDjX1 X2 X3 X4Figure 2: A segment of a tree with with internal nodesDi, Dj , Dk having data points {x1, x2, x3, x4}.
Thesubtree below the internal node Di is called Ti, thesubtree below the internal node Dj is Tj , and the sub-tree below the internal node Dk is Tk.model can be denoted as p(xi|?)
where ?
denotesthe parameters of the probabilistic model.The marginal probability of data in any nodecan be calculated as:p(Dk) =?p(Dk|?)p(?|?)d?
(1)The likelihood of data under any subtree is de-fined as follows:p(Dk|Tk) = p(Dk)p(Dl|Tl)p(Dr|Tr) (2)where the probability is defined in terms of left Tland right Tr subtrees.
Equation 2 provides a re-cursive decomposition of the likelihood in termsof the likelihood of the left and the right sub-trees until the leaf nodes are reached.
We use themarginal probability (Equation 1) as prior infor-mation since the marginal probability bears theprobability of having the data from the left andright subtrees within a single cluster.4 Morphological SegmentationIn our model, data points are words to be clus-tered and each cluster represents a paradigm.
Inthe hierarchical structure, words will be organisedin such a way that morphologically similar wordswill be located close to each other to be groupedin the same paradigms.
Morphological similarityrefers to at least one common morpheme betweenwords.
However, we do not make a distinction be-tween morpheme types.
Instead, we assume thateach word is organised as a stem+suffix combina-tion.4.1 Model DefinitionLet a dataset D consist of words to be analysed,where each word wi has a latent variable which is655the split point that analyses the word into its stemsi and suffix mi:D = {w1 = s1 +m1, .
.
.
, wn = sn +mn}The marginal likelihood of words in the node kis defined such that:p(Dk) = p(Sk)p(Mk)= p(s1, s2, .
.
.
, sn)p(m1,m2, .
.
.
,mn)The words in each cluster represents aparadigm that consists of stems and suffixes.
Thehierarchical model puts words sharing the samestems or suffixes close to each other in the tree.Each word is part of all the paradigms on thepath from the leaf node having that word to theroot.
The word can share either its stem or suffixwith other words in the same paradigm.
Hence,a considerable number of words can be generatedthrough this approach that may not be seen in thecorpus.We postulate that stems and suffixes are gen-erated independently from each other.
Thus, theprobability of a word becomes:p(w = s+m) = p(s)p(m) (3)We define two Dirichlet processes to generatestems and suffixes independently:Gs|?s, Ps ?
DP (?s, Ps)Gm|?m, Pm ?
DP (?m, Pm)s|Gs ?
Gsm|Gm ?
Gmwhere DP (?s, Ps) denotes a Dirichlet processthat generates stems.
Here, ?s is the concentrationparameter, which determines the number of stemtypes generated by the Dirichlet process.
Thesmaller the value of the concentration parameter,the less likely to generate new stem types the pro-cess is.
In contrast, the larger the value of concen-tration parameter, the more likely it is to generatenew stem types, yielding a more uniform distribu-tion over stem types.
If ?s < 1, sparse stems aresupported, it yields a more skewed distribution.To support a small number of stem types in eachcluster, we chose ?s < 1.Here, Ps is the base distribution.
We use thebase distribution as a prior probability distribu-tion for morpheme lengths.
We model morpheme?s ?mPs PmGs Gmsi miwiL NnFigure 3: The plate diagram of the model, representingthe generation of a word wi from the stem si and thesuffix mi that are generated from Dirichlet processes.In the representation, solid-boxes denote that the pro-cess is repeated with the number given on the cornerof each box.lengths implicitly through the morpheme letters:Ps(si) =?ci?sip(ci) (4)where ci denotes the letters, which are distributeduniformly.
Modelling morpheme letters is a wayof modelling the morpheme length since shortermorphemes are favoured in order to have fewerfactors in Equation 4 (Creutz and Lagus, 2005b).The Dirichlet process,DP (?m, Pm), is definedfor suffixes analogously.
The graphical represen-tation of the entire model is given in Figure 3.Once the probability distributions G ={Gs, Gm} are drawn from both Dirichlet pro-cesses, words can be generated by drawing a stemfrom Gs and a suffix from Gm.
However, we donot attempt to estimate the probability distribu-tions G; instead, G is integrated out.
The jointprobability of stems is calculated by integratingout Gs:p(s1, s2, .
.
.
, sM )=?p(Gs)L?i=1p(si|Gs)dGs(5)where L denotes the number of stem tokens.
Thejoint probability distribution of stems can be tack-led as a Chinese restaurant process.
The Chi-nese restaurant process introduces dependenciesbetween stems.
Hence, the joint probability of656stems S = {s1, .
.
.
, sL} becomes:p(s1, s2, .
.
.
, sL)= p(s1)p(s2|s1) .
.
.
p(sM |s1, .
.
.
, sM?1)= ?(?s)?
(L+ ?s)?K?1sK?i=1Ps(si)K?i=1(nsi ?
1)!
(6)where K denotes the number of stem types.
Inthe equation, the second and the third factor corre-spond to the case where novel stems are generatedfor the first time; the last factor corresponds to thecase in which stems that have already been gener-ated for nsi times previously are being generatedagain.
The first factor consists of all denominatorsfrom both cases.The integration process is applied for proba-bility distributions Gm for suffixes analogously.Hence, the joint probability of suffixes M ={m1, .
.
.
,mN} becomes:p(m1,m2, .
.
.
,mN )= p(m1)p(m2|m1) .
.
.
p(mN |m1, .
.
.
,mN?1)= ?(?)?
(N + ?
)?TT?i=1Pm(mi)T?i=1(nmi ?
1)!
(7)where T denotes the number of suffix types andnmi is the number of stem types mi which havebeen already generated.Following the joint probability distribution ofstems, the conditional probability of a stem givenpreviously generated stems can be derived as:p(si|S?si , ?s, Ps)=??
?nS?sisiL?1+?s if si ?
S?si?s?Ps(si)L?1+?s otherwise(8)where nS?sisi denotes the number of stem in-stances si that have been previously generated,where S?si denotes the stem set excluding thenew instance of the stem si.The conditional probability of a suffix given theother suffixes that have been previously generatedis defined similarly:p(mi|M?mi , ?m, Pm)=??
?nM?mimiN?1+?m if mi ?M?mi?m?Pm(mi)N?1+?m otherwise(9)where nM?ikmi is the number of instances mi thathave been generated previously where M?mi isplugg+ed skew+edexclaim+edborrow+s borrow+edliken+s liken+edconsist+sconsist+edFigure 4: A portion of a sample tree.the set of suffixes, excluding the new instance ofthe suffix mi.A portion of a tree is given in Figure 4.
Ascan be seen on the figure, all words are lo-cated at leaf nodes.
Therefore, the root nodeof this subtree consists of words {plugg+ed,skew+ed, exclaim+ed, borrow+s, borrow+ed,liken+s, liken+ed, consist+s, consist+ed}.4.2 InferenceThe initial tree is constructed by randomly choos-ing a word from the corpus and adding this into arandomly chosen position in the tree.
When con-structing the initial tree, latent variables are alsoassigned randomly, i.e.
each word is split at a ran-dom position (see Algorithm 1).We use Metropolis Hastings algorithm (Hast-ings, 1970), an instance of Markov Chain MonteCarlo (MCMC) algorithms, to infer the optimalhierarchical structure along with the morphologi-cal segmentation of words (given in Algorithm 2).During each iteration i, a leaf node Di = {wi =si +mi} is drawn from the current tree structure.The drawn leaf node is removed from the tree.Next, a node Dk is drawn uniformly from the tree657Algorithm 1 Creating initial tree.1: input: data D = {w1 = s1 + m1, .
.
.
, wn =sn +mn},2: initialise: root?
D1 whereD1 = {w1 = s1 +m1}3: initialise: c?
n?
14: while c >= 1 do5: Draw a word wj from the corpus.6: Split the word randomly such that wj =sj +mj7: Create a new node Dj where Dj ={wj = sj +mj}8: Choose a sibling node Dk for Dj9: Merge Dnew ?
Dj ?Dk10: Remove wj from the corpus11: c?
c?
112: end while13: output: Initial treeto make it a sibling node to Di.
In addition to asibling node, a split point wi = s?i + m?i is drawnuniformly.
Next, the node Di = {wi = s?i + m?i}is inserted as a sibling node to Dk.
After updatingall probabilities along the path to the root, the newtree structure is either accepted or rejected by ap-plying the Metropolis-Hastings update rule.
Thelikelihood of data under the given tree structure isused as the sampling probability.We use a simulated annealing schedule to up-date PAcc:PAcc =(pnext(D|T )pcur(D|T ))1?
(10)where ?
denotes the current temperature,pnext(D|T ) denotes the marginal likelihoodof the data under the new tree structure, andpcur(D|T ) denotes the marginal likelihood ofdata under the latest accepted tree structure.
If(pnext(D|T ) > pcur(D|T )) then the update isaccepted (see line 9, Algorithm 2), otherwise, thetree structure is still accepted with a probabilityof pAcc (see line 14, Algorithm 2).
In ourexperiments (see section 5) we set ?
to 2.
Thesystem temperature is reduced in each iterationof the Metropolis Hastings algorithm:?
?
?
?
?
(11)Most tree structures are accepted in the earlierstages of the algorithm, however, as the tempera-Algorithm 2 Inference algorithm1: input: data D = {w1 = s1 + m1, .
.
.
, wn =sn + mn}, initial tree T , initial temperatureof the system ?, the target temperature of thesystem ?, temperature decrement ?2: initialise: i ?
1, w ?
wi = si + mi,pcur(D|T )?
p(D|T )3: while ?
> ?
do4: Remove the leaf node Di that has theword wi = si +mi5: Draw a split point for the word such thatwi = s?i +m?i6: Draw a sibling node Dj7: Dm ?
Di ?Dj8: Update pnext(D|T )9: if pnext(D|T ) >= pcur(D|T ) then10: Accept the new tree structure11: pcur(D|T ) ?
pnext(D|T )12: else13: random ?
Normal(0, 1)14: if random <(pnext(D|T )pcur(D|T ))1?
then15: Accept the new tree structure16: pcur(D|T ) ?
pnext(D|T )17: else18: Reject the new tree structure19: Re-insert the node Di at its pre-vious position with the previoussplit point20: end if21: end if22: w ?
wi+1 = si+1 +mi+123: ?
?
?
?
?24: end while25: output: A tree structure where each nodecorresponds to a paradigm.ture decreases only tree structures that lead lead toa considerable improvement in the marginal prob-ability p(D|T ) are accepted.An illustration of sampling a new tree structureis given in Figure 5 and 6.
Figure 5 shows thatD0 will be removed from the tree in order to sam-ple a new position on the tree, along with a newsplit point of the word.
Once the leaf node is re-moved from the tree, the parent node is removedfrom the tree, as the parent node D5 will consistof only one child.
Figure 6 shows that D8 is sam-pled to be the sibling node of D0.
Subsequently,the two nodes are merged within a new cluster that658D5D1D6D2 D3 D4D0D7D8Figure 5: D0 will be removed from the tree.D9D1D6D2 D3 D4 D0D7D8Figure 6: D8 is sampled to be the sibling of D0.introduces a new node D9.4.3 Morphological SegmentationOnce the optimal tree structure is inferred, alongwith the morphological segmentation of words,any novel word can be analysed.
For the segmen-tation of novel words, the root node is used as itcontains all stems and suffixes which are alreadyextracted from the training data.
Morphologicalsegmentation is performed in two ways: segmen-tation at a single point and segmentation at multi-ple points.4.3.1 Single Split PointIn order to find single split point for the mor-phological segmentation of a word, the split pointyielding the maximum probability given inferredstems and suffixes is chosen to be the final analy-sis of the word:argmaxjp(wi = sj +mj |Droot, ?m, Pm, ?s, Ps)(12)where Droot refers to the root of the entire tree.Here, the probability of a segmentation of agiven word given Droot is calculated as given be-low:p(wi = sj +mj |Droot, ?m, Pm, ?s, Ps) =p(sj |Sroot, ?s, Ps) p(mj |Mroot, ?m, Pm)(13)where Sroot denotes all the stems in Droot andMroot denotes all the suffixes in Droot.
Herep(sj |Sroot, ?s, Ps) is calculated as given below:p(si|Sroot, ?s, Ps) =??
?nSrootsiL+?s if si ?
Sroot?s?Ps(si)L+?s otherwise(14)Similarly, p(mj |Mroot, ?m, Pm) is calculatedas:p(mi|Mroot, ?m, Pm) =??
?nMrootmiN+?m if mi ?Mroot?m?Pm(mi)N+?m otherwise(15)4.3.2 Multiple Split PointsIn order to discover words with multiple splitpoints, we propose a hierarchical segmentationwhere each segment is split further.
The rules forgenerating multiple split points is given by the fol-lowing context free grammar:w ?
s1 m1|s2 m2 (16)s1 ?
s m|s s (17)s2 ?
s (18)m1 ?
m m (19)m2 ?
s m|m m (20)Here, s is a pre-terminal node that generates allthe stems from the root node.
And similarly, m isa pre-terminal node that generates all the suffixesfrom the root node.
First, using Equation 16, theword (e.g.
housekeeper) is split into s1 m1 (e.g.housekeep+er) or s2 m2 (house+keeper).
The firstsegment is regarded as a stem, and the secondsegment is either a stem or a suffix, consider-ing the probability of having a compound word.Equation 12 is used to decide whether the sec-ond segment is a stem or a suffix.
At the sec-ond segmentation level, each segment is split oncemore.
If the first production rule is followed inthe first segmentation level, the first segment s1can be analysed as s m (e.g.
housekeep+?)
or s s659!"#$%&%%'%(!
"#$% &%%'%(!
"#$% ) &%%' %(Figure 7: An example that depicts how the wordhousekeeper can be analysed further to find more splitpoints.(e.g.
house+keep) (Equation 17).
The decisionto choose which production rule to apply is madeusing:s1 ?
{s s if p(s|S, ?s, Ps) > p(m|M,?m, Pm)s m otherwise(21)where S and M denote all the stems and suffixesin the root node.Following the same production rule, the secondsegment m1 can only be analysed as m m (er+?
).We postulate that words cannot have more thantwo stems and suffixes always follow stems.
Wedo not allow any prefixes, circumfixes, or infixes.Therefore, the first production rule can output twodifferent analyses: s m m m and s s m m (e.g.housekeep+er and house+keep+er).On the other hand, if the word is analysed ass2 m2 (e.g.
house+keeper), then s2 cannot beanalysed further.
(e.g.
house).
The second seg-ment m2 can be analysed further, such that s m(stem+suffix) (e.g.
keep+er, keeper+?)
or m m(suffix+suffix).
The decision to choose which pro-duction rule to apply is made as follows:m2 ?
{s m if p(s|S, ?s, Ps) > p(m|M,?m, Pm)mm otherwise(22)Thus, the second production rule yields twodifferent analyses: s s m and s m m (e.g.house+keep+er or house+keeper).5 Experiments & ResultsTwo sets of experiments were performed for theevaluation of the model.
In the first set of exper-iments, each word is split at single point giving asingle stem and a single suffix.
In the second setof experiments, potentially multiple split points	       	!Figure 8: Marginal likelihood convergence for datasetsof size 16K and 22K words.are generated, by splitting each stem and suffixonce more, if it is possible to do so.Morpho Challenge (Kurimo et al 2011b) pro-vides a well established evaluation frameworkthat additionally allows comparing our model ina range of languages.
In both sets of experiments,the Morpho Challenge 2010 dataset is used (Ku-rimo et al 2011b).
Experiments are performedfor English, where the dataset consists of 878,034words.
Although the dataset provides word fre-quencies, we have not used any frequency infor-mation.
However, for training our model, we onlychose words with frequency greater than 200.In our experiments, we used dataset sizes of10K, 16K, 22K words.
However, for final eval-uation, we trained our models on 22K words.
Wewere unable to complete the experiments withlarger training datasets due to memory limita-tions.
We plan to report this in future work.
Oncethe tree is learned by the inference algorithm, thefinal tree is used for the segmentation of the entiredataset.
Several experiments are performed foreach setting where the setting varies with the treesize and the model parameters.
Model parametersare the concentration parameters ?
= {?s, ?m}of the Dirichlet processes.
The concentration pa-rameters, which are set for the experiments, are0.1, 0.2, 0.02, 0.001, 0.002.In all experiments, the initial temperature of thesystem is assigned as ?
= 2 and it is reduced tothe temperature ?
= 0.01 with decrements ?
=0.0001.
Figure 8 shows how the log likelihoods oftrees of size 16K and 22K converge in time (wherethe time axis refers to sampling iterations).Since different training sets will lead to differ-ent tree structures, each experiment is repeatedthree times keeping the experiment setting thesame.660Data Size P(%) R(%) F(%) ?s, ?m10K 81.48 33.03 47.01 0.1, 0.116K 86.48 35.13 50.02 0.002, 0.00222K 89.04 36.01 51.28 0.002, 0.002Table 1: Highest evaluation scores of single split pointexperiments obtained from the trees with 10K, 16K,and 22K words.Data Size P(%) R(%) F(%) ?s, ?m10K 62.45 57.62 59.98 0.1, 0.116K 67.80 57.72 62.36 0.002, 0.00222K 68.71 62.56 62.56 0.001 0.001Table 2: Evaluation scores of multiple split point ex-periments obtained from the trees with 10K, 16K, and22K words.5.1 Experiments with Single Split PointsIn the first set of experiments, words are split intoa single stem and suffix.
During the segmentation,Equation 12 is used to determine the split positionof each word.
Evaluation scores are given in Ta-ble 1.
The highest F-measure obtained is 51.28%with the dataset of 22K words.
The scores are no-ticeably higher with the largest training set.5.2 Experiments with Multiple Split PointsThe evaluation scores of experiments with mul-tiple split points are given in Table 2.
The high-est F-measure obtained is 62.56%with the datasetwith 22K words.
As for single split points, thescores are noticeably higher with the largest train-ing set.For both, single and multiple segmentation, thesame inferred tree has been used.5.3 Comparison with Other SystemsFor all our evaluation experiments using Mor-pho Challenge 2010 (English and Turkish) andMorpho Challenge 2009 (English), we used 22kwords for training.
For each evaluation, we ran-domly chose 22k words for training and ran ourMCMC inference procedure to learn our model.We generated 3 different models by choosing 3different randomly generated training sets eachconsisting of 22k words.
The results are the bestresults over these 3 models.
We are reporting thebest results out of the 3 models due to the small(22k word) datasets used.
Use of larger datasetswould have resulted in less variation and betterresults.System P(%) R(%) F(%)Allomorf1 68.98 56.82 62.31Morf.
Base.2 74.93 49.81 59.84PM-Union3 55.68 62.33 58.82Lignos4 83.49 45.00 58.48Prob.
Clustering (multiple) 57.08 57.58 57.33PM-mimic3 53.13 59.01 55.91MorphoNet5 65.08 47.82 55.13Rali-cof6 68.32 46.45 55.30CanMan7 58.52 44.82 50.761 Virpioja et al(2009)2 Creutz and Lagus (2002)3 Monson et al(2009)4 Lignos et al(2009)5 Bernhard (2009)6 Lavalle?e and Langlais (2009)7 Can and Manandhar (2009)Table 3: Comparison with other unsupervised systemsthat participated in Morpho Challenge 2009 for En-glish.We compare our system with the other partici-pant systems in Morpho Challenge 2010.
Resultsare given in Table 6 (Virpioja et al 2011).
Sincethe model is evaluated using the official (hidden)Morpho Challenge 2010 evaluation dataset wherewe submit our system for evaluation to the organ-isers, the scores are different from the ones thatwe presented Table 1 and Table 2.We also demonstrate experiments with MorphoChallenge 2009 English dataset.
The dataset con-sists of 384, 904 words.
Our results and the re-sults of other participant systems in Morpho Chal-lenge 2009 are given in Table 3 (Kurimo et al2009).
It should be noted that we only presentthe top systems that participated in Morpho Chal-lenge 2009.
If all the systems are considered, oursystem comes 5th out of 16 systems.The problem of morphologically rich lan-guages is not our priority within this research.Nevertheless, we provide evaluation scores onTurkish.
The Turkish dataset consists of 617,298words.
We chose words with frequency greaterthan 50 for Turkish since the Turkish dataset is notlarge enough.
The results for Turkish are given inTable 4.
Our system comes 3rd out of 7 systems.6 DiscussionThe model can easily capture common suffixessuch as -less, -s, -ed, -ment, etc.
Some sample treenodes obtained from trees are given in Table 6.661System P(%) R(%) F(%)Morf.
CatMAP 79.38 31.88 45.49Aggressive Comp.
55.51 34.36 42.45Prob.
Clustering (multiple) 72.36 25.81 38.04Iterative Comp.
68.69 21.44 32.68Nicolas 79.02 19.78 31.64Morf.
Base.
89.68 17.78 29.67Base Inference 72.81 16.11 26.38Table 4: Comparison with other unsupervised systemsthat participated in Morpho Challenge 2010 for Turk-ish.regard+less, base+less, shame+less, bound+less,harm+less, regard+ed, relent+lesssolve+d, high+-priced, lower+s, lower+-level,high+-level, lower+-income, histor+ianspre+mise, pre+face, pre+sumed, pre+, pre+gnantbase+ment, ail+ment, over+looked, predica+ment,deploy+ment, compart+ment, embodi+mentanti+-fraud, anti+-war, anti+-tank, anti+-nuclear,anti+-terrorism, switzer+, anti+gua, switzer+landsharp+ened, strength+s, tight+ened, strength+ened,black+enedinspir+e, inspir+ing, inspir+ed, inspir+es, earn+ing,ponder+ingdowngrade+s, crash+ed, crash+ing, lack+ing,blind+ing, blind+, crash+, compris+ing, com-pris+es, stifl+ing, compris+ed, lack+s, assist+ing,blind+ed, blind+er,Table 5: Sample tree nodes obtained from varioustrees.As seen from the table, morphologically similarwords are grouped together.
Morphological sim-ilarity refers to at least one common morphemebetween words.
For example, the words high-priced and lower-level are grouped in the samenode through the word high-level which sharesthe same stem with high-priced and the same end-ing with lower-level.As seen from the sample nodes, prefixescan also be identified, for example anti+fraud,anti+war, anti+tank, anti+nuclear.
This illus-trates the flexibility in the model by capturing thesimilarities through either stems, suffixes or pre-fixes.
However, as mentioned above, the modeldoes not consider any discrimination between dif-ferent types of morphological forms during train-ing.
As the prefix pre- appears at the beginning ofwords, it is identified as a stem.
However, identi-fying pre- as a stem does not yield a change in themorphological analysis of the word.System P(%) R(%) F(%)Base Inference1 80.77 53.76 64.55Iterative Comp.1 80.27 52.76 63.67Aggressive Comp.1 71.45 52.31 60.40Nicolas2 67.83 53.43 59.78Prob.
Clustering (multiple) 57.08 57.58 57.33Morf.
Baseline3 81.39 41.70 55.14Prob.
Clustering (single) 70.76 36.51 48.17Morf.
CatMAP4 86.84 30.03 44.631 Lignos (2010)2 Nicolas et al(2010)3 Creutz and Lagus (2002)4 Creutz and Lagus (2005a)Table 6: Comparison of our model with other unsuper-vised systems that participated in Morpho Challenge2010 for English.Sometimes similarities may not yield a validanalysis of words.
For example, the prefix pre-leads the words pre+mise, pre+sumed, pre+gnantto be analysed wrongly, whereas pre- is a validprefix for the word pre+face.
Another nice fea-ture about the model is that compounds are easilycaptured through common stems: e.g.
doubt+fire,bon+fire, gun+fire, clear+cut.7 Conclusion & Future WorkIn this paper, we present a novel probabilis-tic model for unsupervised morphology learn-ing.
The model adopts a hierarchical structurein which words are organised in a tree so thatmorphologically similar words are located closeto each other.In hierarchical clustering, tree-cutting would bea very useful thing to do but it is not addressedin the current paper.
We used just the root nodeas a morpheme lexicon to apply segmentation.Clearly, adding tree cutting would improve the ac-curacy of the segmentation and will help us iden-tify paradigms with higher accuracy.
However,the segmentation accuracy obtained without us-ing tree cutting provides a very useful indicatorto show whether this approach is promising.
Andexperimental results show that this is indeed thecase.In the current model, we did not use any syn-tactic information, only words.
POS tags can beutilised to group words which are both morpho-logically and syntactically similar.662ReferencesDelphine Bernhard.
2009.
Morphonet: Exploring theuse of community structure for unsupervised mor-pheme analysis.
In Working Notes for the CLEF2009 Workshop, September.Burcu Can and Suresh Manandhar.
2009.
Cluster-ing morphological paradigms using syntactic cate-gories.
In Working Notes for the CLEF 2009 Work-shop, September.Erwin Chan.
2006.
Learning probabilistic paradigmsfor morphology in a latent class model.
In Proceed-ings of the Eighth Meeting of the ACL Special Inter-est Group on Computational Phonology and Mor-phology, SIGPHON ?06, pages 69?78, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Mathias Creutz and Krista Lagus.
2002.
Unsu-pervised discovery of morphemes.
In Proceed-ings of the ACL-02 workshop on Morphologicaland phonological learning - Volume 6, MPL ?02,pages 21?30, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Mathias Creutz and Krista Lagus.
2005a.
Induc-ing the morphological lexicon of a natural languagefrom unannotated text.
In In Proceedings of theInternational and Interdisciplinary Conference onAdaptive Knowledge Representation and Reasoning(AKRR 2005, pages 106?113.Mathias Creutz and Krista Lagus.
2005b.
Unsu-pervised morpheme segmentation and morphologyinduction from text corpora using morfessor 1.0.Technical Report A81.Markus Dreyer and Jason Eisner.
2011.
Discover-ing morphological paradigms from plain text usinga dirichlet process mixture model.
In Proceedingsof the 2011 Conference on Empirical Methods inNatural Language Processing, pages 616?627, Ed-inburgh, Scotland, UK., July.
Association for Com-putational Linguistics.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
ComputationalLinguistics, 27(2):153?198.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2006.
Interpolating between types and to-kens by estimating power-law generators.
In In Ad-vances in Neural Information Processing Systems18, page 18.W.
K. Hastings.
1970.
Monte carlo sampling meth-ods using markov chains and their applications.Biometrika, 57:97?109.Mikko Kurimo, Sami Virpioja, Ville T. Turunen,Graeme W. Blackwood, and William Byrne.
2009.Overview and results of morpho challenge 2009.In Proceedings of the 10th cross-language eval-uation forum conference on Multilingual infor-mation access evaluation: text retrieval experi-ments, CLEF?09, pages 578?597, Berlin, Heidel-berg.
Springer-Verlag.Mikko Kurimo, Krista Lagus, Sami Virpioja, andVille Turunen.
2011a.
Morpho challenge2009.
http://research.ics.tkk.fi/events/morphochallenge2009/, June.Mikko Kurimo, Krista Lagus, Sami Virpioja, andVille Turunen.
2011b.
Morpho challenge2010.
http://research.ics.tkk.fi/events/morphochallenge2010/, June.Jean Franc?ois Lavalle?e and Philippe Langlais.
2009.Morphological acquisition by formal analogy.
InWorking Notes for the CLEF 2009 Workshop,September.Constantine Lignos, Erwin Chan, Mitchell P. Marcus,and Charles Yang.
2009.
A rule-based unsuper-vised morphology learning framework.
In WorkingNotes for the CLEF 2009 Workshop, September.Constantine Lignos.
2010.
Learning from unseendata.
In Mikko Kurimo, Sami Virpioja, Ville Tu-runen, and Krista Lagus, editors, Proceedings of theMorpho Challenge 2010 Workshop, pages 35?38,Aalto University, Espoo, Finland.Christian Monson, Kristy Hollingshead, and BrianRoark.
2009.
Probabilistic paramor.
In Pro-ceedings of the 10th cross-language evaluation fo-rum conference on Multilingual information accessevaluation: text retrieval experiments, CLEF?09,September.Lionel Nicolas, Jacques Farre?, and Miguel A. Mo-linero.
2010.
Unsupervised learning of concate-native morphology based on frequency-related formoccurrence.
In Mikko Kurimo, Sami Virpioja, VilleTurunen, and Krista Lagus, editors, Proceedings ofthe Morpho Challenge 2010 Workshop, pages 39?43, Aalto University, Espoo, Finland.Matthew G. Snover, Gaja E. Jarosz, and Michael R.Brent.
2002.
Unsupervised learning of morphol-ogy using a novel directed search algorithm: Takingthe first step.
In Proceedings of the ACL-02 Work-shop on Morphological and Phonological Learn-ing, pages 11?20, Morristown, NJ, USA.
ACL.Sami Virpioja, Oskar Kohonen, and Krista Lagus.2009.
Unsupervised morpheme discovery with al-lomorfessor.
In Working Notes for the CLEF 2009Workshop.
September.Sami Virpioja, Ville T. Turunen, Sebastian Spiegler,Oskar Kohonen, and Mikko Kurimo.
2011.
Em-pirical comparison of evaluation methods for unsu-pervised learning of morphology.
In Traitement Au-tomatique des Langues.663
