Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 640?649,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsGenerating Templates of Entity Summarieswith an Entity-Aspect Model and Pattern MiningPeng Li1 and Jing Jiang2 and Yinglin Wang11Department of Computer Science and Engineering, Shanghai Jiao Tong University2School of Information Systems, Singapore Management University{lipeng,ylwang}@sjtu.edu.cn jingjiang@smu.edu.sgAbstractIn this paper, we propose a novel approachto automatic generation of summary tem-plates from given collections of summaryarticles.
This kind of summary templatescan be useful in various applications.
Wefirst develop an entity-aspect LDA modelto simultaneously cluster both sentencesand words into aspects.
We then apply fre-quent subtree pattern mining on the depen-dency parse trees of the clustered and la-beled sentences to discover sentence pat-terns that well represent the aspects.
Keyfeatures of our method include automaticgrouping of semantically related sentencepatterns and automatic identification oftemplate slots that need to be filled in.
Weapply our method on five Wikipedia entitycategories and compare our method withtwo baseline methods.
Both quantitativeevaluation based on human judgment andqualitative comparison demonstrate the ef-fectiveness and advantages of our method.1 IntroductionIn this paper, we study the task of automaticallygenerating templates for entity summaries.
An en-tity summary is a short document that gives themost important facts about an entity.
In Wikipedia,for instance, most articles have an introductionsection that summarizes the subject entity beforethe table of contents and other elaborate sections.These introduction sections are examples of en-tity summaries we consider.
Summaries of enti-ties from the same category usually share somecommon structure.
For example, biographies ofphysicists usually contain facts about the national-ity, educational background, affiliation and majorcontributions of the physicist, whereas introduc-tions of companies usually list information suchas the industry, founder and headquarter of thecompany.
Our goal is to automatically constructa summary template that outlines the most salienttypes of facts for an entity category, given a col-lection of entity summaries from this category.Such kind of summary templates can be veryuseful in many applications.
First of all, theycan uncover the underlying structures of summaryarticles and help better organize the informationunits, much in the same way as infoboxes do inWikipedia.
In fact, automatic template genera-tion provides a solution to induction of infoboxstructures, which are still highly incomplete inWikipedia (Wu and Weld, 2007).
A templatecan also serve as a starting point for human edi-tors to create new summary articles.
Furthermore,with summary templates, we can potentially ap-ply information retrieval and extraction techniquesto construct summaries for new entities automati-cally on the fly, improving the user experience forsearch engine and question answering systems.Despite its usefulness, the problem has not beenwell studied.
The most relevant work is by Fila-tova et al (2006) on automatic creation of domaintemplates, where the defintion of a domain is sim-ilar to our notion of an entity category.
Filatovaet al (2006) first identify the important verbs fora domain using corpus statistics, and then find fre-quent parse tree patterns from sentences contain-ing these verbs to construct a domain template.There are two major limitations of their approach.First, the focus on verbs restricts the template pat-terns that can be found.
Second, redundant orrelated patterns using different verbs to expressthe same or similar facts cannot be grouped to-gether.
For example, ?won X award?
and ?re-ceived X prize?
are considered two different pat-terns by this approach.
We propose a method thatcan overcome these two limitations.
Automatictemplate generation is also related to a number ofother problems that have been studied before, in-640cluding unsupervised IE pattern discovery (Sudoet al, 2003; Shinyama and Sekine, 2006; Sekine,2006; Yan et al, 2009) and automatic generationof Wikipedia articles (Sauper and Barzilay, 2009).We discuss the differences of our work from exist-ing related work in Section 6.In this paper we propose a novel approach tothe task of automatically generating entity sum-mary templates.
We first develop an entity-aspectmodel that extends standard LDA to identify clus-ters of words that can represent different aspectsof facts that are salient in a given summary col-lection (Section 3).
For example, the words ?re-ceived,?
?award,?
?won?
and ?Nobel?
may beclustered together from biographies of physiciststo represent one aspect, even though they may ap-pear in different sentences from different biogra-phies.
Simultaneously, the entity-aspect modelseparates words in each sentence into backgroundwords, document words and aspect words, andsentences likely about the same aspect are natu-rally clustered together.
After this aspect identi-fication step, we mine frequent subtree patternsfrom the dependency parse trees of the clusteredsentences (Section 4).
Different from previouswork, we leverage the word labels assigned by theentity-aspect model to prune the patterns and tolocate template slots to be filled in.We evaluate our method on five entity cate-gories using Wikipedia articles (Section 5).
Be-cause the task is new and thus there is no stan-dard evaluation criteria, we conduct both quanti-tative evaluation using our own human judgmentand qualitative comparison.
Our evaluation showsthat our method can obtain better sentence patternsin terms of f1 measure compared with two baselinemethods, and it can also achieve reasonably goodquality of aspect clusters in terms of purity.
Com-pared with standard LDA and K-means sentenceclustering, the aspects identified by our method arealso more meaningful.2 The TaskGiven a collection of entity summaries from thesame entity category, our task is to automaticallyconstruct a summary template that outlines themost important information one should include ina summary for this entity category.
For example,given a collection of biographies of physicists, ide-ally the summary template should indicate that im-portant facts about a physicist include his/her ed-Aspect PatternENT received his phd from ?
university1 ENT studied ?
under ?ENT earned his ?
in physics from university of?ENT was awarded the medal in ?2 ENT won the ?
awardENT received the nobel prize in physics in ?ENT was ?
director3 ENT was the head of ?ENT worked for ?ENT made contributions to ?4 ENT is best known for work on ?ENT is noted for ?Table 1: Examples of some good template patternsand their aspects generated by our method.ucational background, affiliation, major contribu-tions, awards received, etc.However, it is not clear what is the best repre-sentation of such templates.
Should a templatecomprise a list of subtopic labels (e.g.
?educa-tion?
and ?affiliation?)
or a set of explicit ques-tions?
Here we define a template format based onthe usage of the templates as well as our obser-vations from Wikipedia entity summaries.
First,since we expect that the templates can be used byhuman editors for creating new summaries, we usesentence patterns that are human readable as basicunits of the templates.
For example, we may havea sentence pattern ?ENT graduated from ?
Uni-versity?
for the entity category ?physicist,?
whereENT is a placeholder for the entity that the sum-mary is about, and ???
is a slot to be filled in.
Sec-ond, we observe that information about entities ofthe same category can be grouped into subtopics.For example, the sentences ?Bohr is a Nobel lau-reate?
and ?Einstein received the Nobel Prize?
areparaphrases of the same type of facts, while thesentences ?Taub earned his doctorate at Prince-ton University?
and ?he graduated from MIT?
areslightly different but both describe a person?s ed-ucational background.
Therefore, it makes senseto group sentence patterns based on the subtopicsthey pertain to.
Here we call these subtopics theaspects of a summary template.Formally, we define a summary template to be aset of sentence patterns grouped into aspects.
Eachsentence pattern has a placeholder for the entity tobe summarized and possibly one or more templateslots to be filled in.
Table 1 shows some sentencepatterns our method has generated for the ?physi-cist?
category.6412.1 Overview of Our MethodOur automatic template generation method con-sists of two steps:Aspect Identification: In this step, our goal isto automatically identify the different aspects orsubtopics of the given summary collection.
We si-multaneously cluster sentences and words into as-pects, using an entity-aspect model extended fromthe standard LDA model that is widely used intext mining (Blei et al, 2003).
The output of thisstep are sentences clustered into aspects, with eachword labeled as a stop word, a background word,a document word or an aspect word.Sentence Pattern Generation: In this step, wegenerate human-readable sentence patterns to rep-resent each aspect.
We use frequent subtree pat-tern mining to find the most representative sen-tence structures for each aspect.
The fixed struc-ture of a sentence pattern consists of aspect words,background words and stop words, while docu-ment words become template slots whose valuescan vary from summary to summary.3 Aspect IdentificationAt the aspect identification step, our goal is to dis-cover the most salient aspects or subtopics con-tained in a summary collection.
Here we proposea principled method based on a modified LDAmodel to simultaneously cluster both sentencesand words to discover aspects.We first make the following observation.
In en-tity summaries such as the introduction sectionsof Wikipedia articles, most sentences are talk-ing about a single fact of the entity.
If we lookclosely, there are a few different kinds of words inthese sentences.
First of all, there are stop wordsthat occur frequently in any document collection.Second, for a given entity category, some wordsare generally used in all aspects of the collection.Third, some words are clearly associated with theaspects of the sentences they occur in.
And finally,there are also words that are document or entityspecific.
For example, in Table 2 we show twosentences related to the ?affiliation?
aspect fromthe ?physicist?
summary collection.
Stop wordssuch as ?is?
and ?the?
are labeled with ?S.?
Theword ?physics?
can be regarded as a backgroundword for this collection.
?Professor?
and ?univer-sity?
are clearly related to the ?affiliation?
aspect.Finally words such as ?Modena?
and ?Chicago?are specifically associated with the subject enti-ties being discussed, that is, they are specific tothe summary documents.To capture background words and document-specific words, Chemudugunta et al (2007)proposed to introduce a background topic anddocument-specific topics.
Here we borrow theiridea and also include a background topic as wellas document-specific topics.
To discover aspectsthat are local to one or a few adjacent sentences butmay occur in many documents, Titov and McDon-ald (2008) proposed a multi-grain topic model,which relies on word co-occurrences within shortparagraphs rather than documents in order to dis-cover aspects.
Inspired by their model, we relyon word co-occurrences within single sentences toidentify aspects.3.1 Entity-Aspect ModelWe now formally present our entity-aspect model.First, we assume that stop words can be identifiedusing a standard stop word list.
We then assumethat for a given entity category there are threekinds of unigram language models (i.e.
multino-mial word distributions).
There is a backgroundmodel ?B that generates words commonly usedin all documents and all aspects.
There are Ddocument models ?d (1 ?
d ?
D), where Dis the number of documents in the given sum-mary collection, and there are A aspect models ?a(1 ?
a ?
A), where A is the number of aspects.We assume that these word distributions have auniform Dirichlet prior with parameter ?.Since not all aspects are discussed equally fre-quently, we assume that there is a global aspectdistribution ?
that controls how often each aspectoccurs in the collection.
?
is sampled from anotherDirichlet prior with parameter ?.
There is also amultinomial distribution pi that controls in eachsentence how often we encounter a backgroundword, a document word, or an aspect word.
pi hasa Dirichlet prior with parameter ?.Let Sd denote the number of sentences in doc-ument d, Nd,s denote the number of words (afterstop word removal) in sentence s of document d,and wd,s,n denote the n?th word in this sentence.We introduce hidden variables zd,s for each sen-tence to indicate the aspect a sentence belongs to.We also introduce hidden variables yd,s,n for eachword to indicate whether a word is generated fromthe background model, the document model, orthe aspect model.
Figure 1 shows the process of642Venturi/D is/S a/S professor/A of/S physics/B at/S the/S University/A of/SModena/D ./SHe/S was/S a/S professor/A of/S physics/B at/S the/S University/A of/SChicago/D until/S 1982/D ./STable 2: Two sentences on ?affiliation?
from the ?physicist?
entity category.
S: stop word.
B: backgroundword.
A: aspect word.
D: document word.1.
Draw ?
?
Dir(?
), ?B ?
Dir(?
), pi ?
Dir(?)2.
For each aspect a = 1, .
.
.
, A,(a) draw ?a ?
Dir(?)3.
For each document d = 1, .
.
.
, D,(a) draw ?d ?
Dir(?
)(b) for each sentence s = 1, .
.
.
, Sdi.
draw zd,s ?
Multi(?)ii.
for each word n = 1, .
.
.
, Nd,sA.
draw yd,s,n ?
Multi(pi)B. draw wd,s,n ?
Multi(?B) if yd,s,n = 1,wd,s,n ?
Multi(?d) if yd,s,n = 2, orwd,s,n ?
Multi(?zd,s) if yd,s,n = 3Figure 1: The document generation process.y z?pi?
???
AdSD sdN ,B?
?wFigure 2: The entity-aspect model.generating the whole document collection.
Theplate notation of the model is shown in Figure 2.Note that the values of ?, ?
and ?
are fixed.
Thenumber of aspects A is also manually set.3.2 InferenceGiven a summary collection, i.e.
the set of allwd,s,n, our goal is to find the most likely assign-ment of zd,s and yd,s,n, that is, the assignment thatmaximizes p(z,y|w;?, ?, ?
), where z, y and w rep-resent the set of all z, y and w variables, respec-tively.
With the assignment, sentences are natu-rally clustered into aspects, and words are labeledas either a background word, a document word, oran aspect word.We approximate p(y, z|w;?, ?, ?)
byp(y,z|w; ?
?B, {?
?d}Dd=1, {?
?a}Aa=1, ?
?, p?i), where ??B,{?
?d}Dd=1, {?
?a}Aa=1, ??
and p?i are estimated usingGibbs sampling, which is commonly used forinference for LDA models (Griffiths and Steyvers,2004).
Due to space limit, we give the formulasfor the Gibbs sampler below without derivation.First, given sentence s in document d, we sam-ple a value for zd,s given the values of all other zand y variables using the following formula:p(zd,s = a|z?{d,s},y,w)?
CA(a) + ?CA(?)
+A??
?Vv=1?E(v)i=0 (Ca(v) + i+ ?)?E(?
)i=0 (Ca(?)
+ i+ V ?
).In the formula above, z?
{d,s} is the current aspectassignment of all sentences excluding the currentsentence.
CA(a) is the number of sentences assignedto aspect a, and CA(?)
is the total number of sen-tences.
V is the vocabulary size.
Ca(v) is the num-ber of times word v has been assigned to aspecta.
Ca(?)
is the total number of words assigned toaspect a.
All the counts above exclude the currentsentence.
E(v) is the number of times word v oc-curs in the current sentence and is assigned to bean aspect word, as indicated by y, and E(?)
is thetotal number of words in the current sentence thatare assigned to be an aspect word.We then sample a value for yd,s,n for each wordin the current sentence using the following formu-las:p(yd,s,n = 1|z,y?
{d,s,n}) ?Cpi(1) + ?Cpi(?)
+ 3?
?CB(wd,s,n) + ?CB(?)
+ V ?,p(yd,s,n = 2|z,y?
{d,s,n}) ?Cpi(2) + ?Cpi(?)
+ 3?
?Cd(wd,s,n) + ?Cd(?)
+ V ?,p(yd,s,n = 3|z,y?
{d,s,n}) ?Cpi(3) + ?Cpi(?)
+ 3?
?Ca(wd,s,n) + ?Ca(?)
+ V ?.In the formulas above, y?
{d,s,n} is the set of all yvariables excluding yd,s,n.
Cpi(1), Cpi(2) and Cpi(3) arethe numbers of words assigned to be a backgroundword, a document word, or an aspect word, respec-tively, and Cpi(?)
is the total number of words.
CBand Cd are counters similar to Ca but are for thebackground model and the document models.
Inall these counts, the current word is excluded.With one Gibbs sample, we can make the fol-lowing estimation:643?
?Bv =CB(v) + ?CB(?)
+ V ?, ?
?dv =Cd(v) + ?Cd(?)
+ V ?, ?
?av =Ca(v) + ?Ca(?)
+ V ?,?
?a =CA(a) + ?CA(?)
+A?, p?it =Cpi(t) + ?Cpi(?)
+ 3?
(1 ?
t ?
3).Here the counts include all sentences and allwords.In our experiments, we set ?
= 5, ?
= 0.01 and?
= 20.
We run 100 burn-in iterations through alldocuments in a collection to stabilize the distri-bution of z and y before collecting samples.
Wefound that empirically 100 burn-in iterations weresufficient for our data set.
We take 10 samples witha gap of 10 iterations between two samples, andaverage over these 10 samples to get the estima-tion for the parameters.After estimating ?
?B, {?
?d}Dd=1, {?
?a}Aa=1, ??
and p?i,we find the values of each zd,s and yd,s,n that max-imize p(y, z|w; ?
?B, {?
?d}Dd=1, {?
?a}Aa=1, ?
?, p?i).
This as-signment, together with the standard stop word listwe use, gives us sentences clustered into A as-pects, where each word is labeled as either a stopword, a background word, a document word or anaspect word.3.3 Comparison with Other ModelsA major difference of our entity-aspect modelfrom standard LDA model is that we assume eachsentence belongs to a single aspect while in LDAwords in the same sentence can be assigned todifferent topics.
Our one-aspect-per-sentence as-sumption is important because our goal is to clus-ter sentences into aspects so that we can minecommon sentence patterns for each aspect.To cluster sentences, we could have used astraightforward solution similar to document clus-tering, where sentences are represented as featurevectors using the vector space model, and a stan-dard clustering algorithm such as K-means canbe applied to group sentences together.
However,there are some potential problems with directly ap-plying this typical document clustering method.First, unlike documents, sentences are short, andthe number of words in a sentence that imply itsaspect is even smaller.
Besides, we do not knowthe aspect-related words in advance.
As a result,the cosine similarity between two sentences maynot reflect whether they are about the same aspect.We can perform heuristic term weighting, but themethod becomes less robust.
Second, after sen-tence clustering, we may still want to identify thethe aspect words in each sentence, which are use-ful in the next pattern mining step.
Directly takingthe most frequent words from each sentence clus-ter as aspect words may not work well even af-ter stop word removal, because there can be back-ground words commonly used in all aspects.4 Sentence Pattern GenerationAt the pattern generation step, we want to iden-tify human-readable sentence patterns that bestrepresent each cluster.
Following the basic ideafrom (Filatova et al, 2006), we start with the parsetrees of sentences in each cluster, and apply afrequent subtree pattern mining algorithm to findsentence structures that have occurred at least Ktimes in the cluster.
Here we use dependency parsetrees.However, different from (Filatova et al, 2006),the word labels (S, B, D and A) assigned by theentity-aspect model give us some advantages.
In-tuitively, a representative sentence pattern for anaspect should contain at least one aspect word.
Onthe other hand, document words are entity-specificand therefore should not appear in the generic tem-plate patterns; instead, they correspond to tem-plate slots that need to be filled in.
Furthermore,since we work on entity summaries, in each sen-tence there is usually a word or phrase that refersto the subject entity, and we should have a place-holder for the subject entity in each pattern.Based on the intuitions above, we have the fol-lowing sentence pattern generation process.1.
Locate subject entities: In each sentence, wewant to locate the word or phrase that refers to thesubject entity.
For example, in a biography, usu-ally a pronoun ?he?
or ?she?
is used to refer tothe subject person.
We use the following heuristicto locate the subject entities: For each summarydocument, we first find the top 3 frequent basenoun phrases that are subjects of sentences.
Forexample, in a company introduction, the phrase?the company?
is probably used frequently as asentence subject.
Then for each sentence, we firstlook for the title of the Wikipedia article.
If it oc-curs, it is tagged as the subject entity.
Otherwise,we check whether one of the top 3 subject basenoun phrases occurs, and if so, it is tagged as thesubject entity.
Otherwise, we tag the subject of thesentence as the subject entity.
Finally, for the iden-tified subject entity word or phrase, we replace thelabel assigned by the entity-aspect model with a644professor_Ais_SENT a_Sphysics_B university_A?the_Snsubjcopdetprep_ofdetprep_atprep_ofFigure 3: An example labeled dependency parsetree.new label E.2.
Generate labeled parse trees: We parse eachsentence using the Stanford Parser1.
After parsing,for each sentence we obtain a dependency parsetree where each node is a single word and eachedge is labeled with a dependency relation.
Eachword is also labeled with one of {E, S, B, D,A}.
We replace words labeled with E by a place-holder ENT, and replace words labeled with D bya question mark to indicate that these correspondto template slots.
For the other words, we attachtheir labels to the tree nodes.
Figure 3 shows anexample labeled dependency parse tree.3.
Mine frequent subtree patterns: For the setof parse trees in each cluster, we use FREQT2, asoftware that implements the frequent subtree pat-tern mining algorithm proposed in (Zaki, 2002), tofind all subtrees with a minimum support of K.4.
Prune patterns: We remove subtree patternsfound by FREQT that do not contain ENT or anyaspect word.
We also remove small patterns thatare contained in some other larger pattern in thesame cluster.5.
Covert subtree patterns to sentence patterns:The remaining patterns are still represented as sub-trees.
To covert them back to human-readable sen-tence patterns, we map each pattern back to one ofthe sentences that contain the pattern to order thetree nodes according to their original order in thesentence.In the end, for each summary collection, we ob-tain A clusters of sentence patterns, where eachcluster presumably corresponds to a single aspector subtopic.1http://nlp.stanford.edu/software/lex-parser.shtml2http://chasen.org/?taku/software/freqt/Category D S Sdmin max avgUS Actress 407 1721 1 21 4Physicist 697 4238 1 49 6US CEO 179 1040 1 24 5US Company 375 2477 1 36 6Restaurant 152 1195 1 37 7Table 3: The number of documents (D), totalnumber of sentences (S) and minimum, maximumand average numbers of sentences per document(Sd) of the data set.5 EvaluationBecause we study a non-standard task, there is noexisting annotated data set.
We therefore created asmall data set and made our own human judgmentfor quantitative evaluation purpose.5.1 DataWe downloaded five collections of Wikipedia ar-ticles from different entity categories.
We tookonly the introduction sections of each article (be-fore the tables of contents) as entity summaries.Some statistics of the data set are given in Table 3.5.2 Quantitative EvaluationTo quantitatively evaluate the summary templates,we want to check (1) whether our sentence pat-terns are meaningful and can represent the corre-sponding entity categories well, and (2) whethersemantically related sentence patterns are groupedinto the same aspect.
It is hard to evaluate bothtogether.
We therefore separate these two criteria.5.2.1 Quality of sentence patternsTo judge the quality of sentence patterns withoutlooking at aspect clusters, ideally we want to com-pute the precision and recall of our patterns, thatis, the percentage of our sentence patterns that aremeaningful, and the percentage of true meaningfulsentence patterns of each category that our methodcan capture.
The former is relatively easy to obtainbecause we can ask humans to judge the quality ofour patterns.
The latter is much harder to com-pute because we need human judges to find the setof true sentence patterns for each entity category,which can be very subjective.We adopt the following pooling strategy bor-rowed from information retrieval.
Assume wewant to compare a number of methods that eachcan generate a set of sentence patterns from a sum-mary collection.
We take the union of these sets645of patterns generated by the different methods andorder them randomly.
We then ask a human judgeto decide whether each sentence pattern is mean-ingful for the given category.
We can then treatthe set of meaningful sentence patterns found bythe human judge this way as the ground truth, andprecision and recall of each method can be com-puted.
If our goal is only to compare the differentmethods, this pooling strategy should suffice.We compare our method with the following twobaseline methods.Baseline 1: In this baseline, we use the samesubtree pattern mining algorithm to find sentencepatterns from each summary collection.
We alsolocate the subject entities and replace them withENT.
However, we do not have aspect words ordocument words in this case.
Therefore we do notprune any pattern except to merge small patternswith the large ones that contain them.
The pat-terns generated by this method do not have tem-plate slots.Baseline 2: In the second baseline, we apply averb-based pruning on the patterns generated bythe first baseline, similar to (Filatova et al, 2006).We first find the top-20 verbs using the scoringfunction below that is taken from (Filatova et al,2006), and then prune patterns that do not containany of the top-20 verbs.s(vi) = N(vi)?vj?V N(vj)?
M(vi)D ,where N(vi) is the frequency of verb vi in thecollection, V is the set of all verbs, D is the totalnumber of documents in the collection, and M(vi)is the number of documents in the collection thatcontains vi.In Table 4, we show the precision, recall and f1of the sentence patterns generated by our methodand the two baseline methods for the five cate-gories.
For our method, we set the support ofthe subtree patterns K to 2, that is, each patternhas occurred in at least two sentences in the cor-responding aspect cluster.
For the two baselinemethods, because sentences are not clustered, weuse a larger support K of 3; otherwise, we findthat there can be too many patterns.
We can seethat overall our method gives better f1 measuresthan the two baseline methods for most categories.Our method achieves a good balance between pre-cision and recall.
For BL-1, the precision is highbut recall is low.
Intuitively BL-1 should have ahigher recall than our method because our methodCategory B PurityUS Actress 4 0.626Physicist 6 0.714US CEO 4 0.674US Company 4 0.614Restaurant 3 0.587Table 5: The true numbers of aspects as judgedby the human annotator (B), and the purity of theclusters.does more pattern pruning than BL-1 using aspectwords.
Here it is not the case mainly because weused a higher frequency threshold (K = 3) to se-lect frequent patterns in BL-1, giving overall fewerpatterns than in our method.
For BL-2, the preci-sion is higher than BL-1 but recall is lower.
It isexpected because the patterns of BL-2 is a subsetof that of BL-1.There are some advantages of our method thatare not reflected in Table 4.
First, many of our pat-terns contain template slots, which make the pat-tern more meaningful.
In contrast the baseline pat-terns do not contain template slots.
Because thehuman judge did not give preference over patternswith slots, both ?ENT won the award?
and ?ENTwon the ?
award?
were judged to be meaningfulwithout any distinction, although the former onegenerated by our method is more meaningful.
Sec-ond, compared with BL-2, our method can obtainpatterns that do not contain a non-auxiliary verb,such as ?ENT was ?
director.
?5.2.2 Quality of aspect clustersWe also want to judge the quality of the aspectclusters.
To do so, we ask the human judge togroup the ground truth sentence patterns of eachcategory based on semantic relatedness.
We thencompute the purity of the automatically generatedclusters against the human judged clusters usingpurity.
The results are shown in Table 5.
In ourexperiments, we set the number of clusters A usedin the entity-aspect model to be 10.
We can seefrom Table 5 that our generated aspect clusters canachieve reasonably good performance.5.3 Qualitative evaluationWe also conducted qualitative comparison be-tween our entity-aspect model and standard LDAmodel as well as a K-means sentence clusteringmethod.
In Table 6, we show the top 5 fre-quent words of three sample aspects as found byour method, standard LDA, and K-means.
Notethat although we try to align the aspects, there is646CategoryMethod US Actress Physicist US CEO US Company RestaurantBL-1 precision 0.714 0.695 0.778 0.622 0.706recall 0.545 0.300 0.367 0.425 0.361f1 0.618 0.419 0.499 0.505 0.478BL-2 precision 0.845 0.767 0.829 0.809 1.000recall 0.260 0.096 0.127 0.167 0.188f1 0.397 0.17 0.220 0.276 0.316Ours precision 0.544 0.607 0.586 0.450 0.560recall 0.710 0.785 0.712 0.618 0.701f1 0.616 0.684 0.643 0.520 0.624Table 4: Quality of sentence patterns in terms of precision, recall and f1.Method Sample Aspects1 2 3Our university prize academyentity- received nobel sciencesaspect ph.d. physics membermodel college awarded nationaldegree medal societyStandard physics nobel physicsLDA american prize instituteprofessor physicist researchreceived awarded memberuniversity john sciencesK-means physics physicist physicsuniversity american academyinstitute physics scienceswork university universityresearch nobel newTable 6: Comparison of the top 5 words of threesample aspects using different methods.no correspondence between clusters numbered thesame but generated by different methods.We can see that our method gives very mean-ingful aspect clusters.
Standard LDA also givesmeaningful words, but background words suchas ?physics?
and ?physicist?
are mixed with as-pect words.
Entity-specific words such as ?john?also appear mixed with aspect words.
K-meansclusters are much less meaningful, with too manybackground words mixed with aspect words.6 Related WorkThe most related existing work is on domain tem-plate generation by Filatova et al (2006).
Thereare several differences between our work andtheirs.
First, their template patterns must contain anon-auxiliary verb whereas ours do not have thisrestriction.
Second, their verb-centered patternsare independent of each other, whereas we groupsemantically related patterns into aspects, givingmore meaningful templates.
Third, in their work,named entities, numbers and general nouns aretreated as template slots.
In our method, we ap-ply the entity-aspect model to automatically iden-tify words that are document-specific, and treatthese words as template slots, which can be poten-tially more robust as we do not rely on the qualityof named entity recognition.
Last but not least,their documents are event-centered while ours areentity-centered.
Therefore we can use heuristics toanchor our patterns on the subject entities.Sauper and Barzilay (2009) proposed a frame-work to learn to automatically generate Wikipediaarticles.
There is a fundamental difference be-tween their task and ours.
The articles they gen-erate are long, comprehensive documents consist-ing of several sections on different subtopics ofthe subject entity, and they focus on learning thetopical structures from complete Wikipedia arti-cles.
We focus on learning sentence patterns of theshort, concise introduction sections of Wikipediaarticles.Our entity-aspect model is related to a num-ber of previous extensions of LDA models.Chemudugunta et al (2007) proposed to intro-duce a background topic and document-specifictopics.
Our background and document languagemodels are similar to theirs.
However, they stilltreat documents as bags of words rather than setsof sentences as in our model.
Titov and McDon-ald (2008) exploited the idea that a short paragraphwithin a document is likely to be about the sameaspect.
Our one-aspect-per-sentence assumptionis a stricter than theirs, but it is required in ourmodel for the purpose of mining sentence patterns.The way we separate words into stop words, back-ground words, document words and aspect wordsbears similarity to that used in (Daume?
III andMarcu, 2006; Haghighi and Vanderwende, 2009),but their task is multi-document summarizationwhile ours is to induce summary templates.6477 Conclusions and Future WorkIn this paper, we studied the task of automati-cally generating templates for entity summaries.We proposed an entity-aspect model that can auto-matically cluster sentences and words into aspects.The model also labels words in sentences as eithera stop word, a background word, a document wordor an aspect word.
We then applied frequent sub-tree pattern mining to generate sentence patternsthat can represent the aspects.
We took advan-tage of the labels generated by the entity-aspectmodel to prune patterns and to locate templateslots.
We conducted both quantitative and qualita-tive evaluation using five collections of Wikipediaentity summaries.
We found that our method gaveoverall better template patterns than two baselinemethods, and the aspect clusters generated by ourmethod are reasonably good.There are a number of directions we plan to pur-sue in the future in order to improve our method.First, we can possibly apply linguistic knowledgeto improve the quality of sentence patterns.
Cur-rently the method may generate similar sentencepatterns that differ only slightly, e.g.
change of apreposition.
Also, the sentence patterns may notform complete, meaningful sentences.
For exam-ple, a sentence pattern may contain an adjectivebut not the noun it modifies.
We plan to studyhow to use linguistic knowledge to guide the con-struction of sentence patterns and make them moremeaningful.
Second, we have not quantitativelyevaluated the quality of the template slots, becauseour judgment is only at the whole sentence patternlevel.
We plan to get more human judges and morerigorously judge the relevance and usefulness ofboth the sentence patterns and the template slots.It is also possible to introduce certain rules or con-straints to selectively form template slots ratherthan treating all words labeled with D as templateslots.AcknowledgmentsThis work was done during Peng Li?s visit to theSingapore Management University.
This workwas partially supported by the National High-techResearch and Development Project of China (863)under the grant number 2009AA04Z106 and theNational Science Foundation of China (NSFC) un-der the grant number 60773088.
We thank theanonymous reviewers for their helpful comments.ReferencesDavid Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Chaitanya Chemudugunta, Padhraic Smyth, and MarkSteyvers.
2007.
Modeling general and specific as-pects of documents with a probabilistic topic model.In Advances in Neural Information Processing Sys-tems 19, pages 241?248.Hal Daume?
III and Daniel Marcu.
2006.
Bayesianquery-focused summarization.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 305?312.Elena Filatova, Vasileios Hatzivassiloglou, and Kath-leen McKeown.
2006.
Automatic creation of do-main templates.
In Proceedings of 21st Interna-tional Conference on Computational Linguistics andthe 44th Annual Meeting of the Association for Com-putational Linguistics, pages 207?214.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences of the United States of Amer-ica, 101(Suppl.
1):5228?5235.Aria Haghighi and Lucy Vanderwende.
2009.
Explor-ing content models for multi-document summariza-tion.
In Proceedings of the Human Language Tech-nology Conference of the North American Chapterof the Association for Computational Linguistics,pages 362?370.Christina Sauper and Regina Barzilay.
2009.
Automat-ically generating Wikipedia articles: A structure-aware approach.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 208?216.Satoshi Sekine.
2006.
On-demand information extrac-tion.
In Proceedings of 21st International Confer-ence on Computational Linguistics and the 44th An-nual Meeting of the Association for ComputationalLinguistics, pages 731?738.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted rela-tion discovery.
In Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 304?311.Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.2003.
An improved extraction pattern representa-tion model for automatic IE pattern acquisition.
InProceedings of the 41st Annual Meeting of the Asso-ciation for Computational Linguistics, pages 224?231.Ivan Titov and Ryan McDonald.
2008.
Modelingonline reviews with multi-grain topic models.
In648Proceeding of the 17th International Conference onWorld Wide Web, pages 111?120.Fei Wu and Daniel S. Weld.
2007.
Autonomously se-mantifying Wikipedia.
In Proceedings of the 16thACM Conference on Information and KnowledgeManagement, pages 41?50.Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, ZhengluYang, and Mitsuru Ishizuka.
2009.
Unsupervisedrelation extraction by mining Wikipedia texts usinginformation from the Web.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages1021?1029.Mohammed J. Zaki.
2002.
Efficiently mining fre-quent trees in a forest.
In Proceedings of the 8thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 71?80.649
