Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 29?31,Avignon, France, April 24 2012. c?2012 Association for Computational LinguisticsWeb Services for Bayesian LearningMuntsa Padr?
N?ria BelUniversitat Pompeu Fabra Universitat Pompeu FabraBarcelona, Spain Barcelona, Spainmuntsa.padro@upf.edu nuria.bel@upf.eduAbstractIn this demonstration we present our webservices to perform Bayesian learning forclassification tasks.1 IntroductionThe Bayesian framework for probabilistic infe-rence has been proposed (for instance, Griffithset al, 2008 and a survey in Chater and Manning,2006 for language related topics) as a generalapproach to understanding how problems of in-duction can be solved given only the sparse andnoisy data that humans observe.
In particular,how human acquire words if the available dataseverely limit the possibility of making infe-rences.
Bayesian framework has been proposedas way to introduce a priori knowledge to guidethe inference process.
In particular for LexicalAcquisition, Xu and Tenembaum (2007) pro-posed that given a hypothesis space (all what aword can be, according to a set of existingclasses) and one or more examples of a newword, the learner evaluates all hypotheses forcandidate word classes by computing their post-erior probabilities, proportional to the product ofprior probabilities and likelihood.
The priorprobabilities are the learner?s beliefs about whichhypotheses are more or less plausible.
The like-lihood reflects the learner?s expectations aboutwhich examples are likely to be observed given aparticular hypothesis about a word class.
And thedecision on new words is determined by averag-ing the predictions of all hypothesis weighted bytheir posterior probabilities.The hypothesis behind is that natural languagecharacteristics, such as the Zipfian distribution ofwords (Zipf, 1935) and considerations as theclassic argument on sparse data (Chomsky,1980), make it necessary to postulate that thelearning of words must be guided by the know-ledge of the lexical system itself, informationabout abstracted, not directly observable catego-ries (Goldberg, 2006; Bybee, 1998).In order to test this hypothesis we developed aseries of tools for the task of noun classificationinto lexical semantic classes (such as EVENT,HUMAN, LOCATION, etc.).
The tools performBayesian parameter estimation where priorknowledge is included into the parameters asvirtual evidence (following Griffiths et al 2008)and a Naive Bayes based classification.
Our as-sumption is that, if introducing prior knowledgeimproves the classification results, it may givesome insights about the way humans learn lexicalclasses.The developed tools have been deployed asweb services (following web-based architectureof the PANACEA project 1 ) in order to makethem easily available to the community.
Theycan be used in the task just mentioned but also inother tasks that may profit from a Bayesian ap-proach.2 Web Services for Bayesian modelingIn this demonstration, we present two web ser-vices that can be used for Bayesian inference ofparameters and classification with the aim thatthey may be useful to other researchers willing touse Bayesian methods in their research.2.1 Naive Bayes ClassifierA first web service performs a traditional NaiveBayes classification.
The input is the observeddata from a given instance encoded as cue vec-tors, this is, the number of times we have seeneach cue in the context of the studied instance.Then, the web service computes how likely isthat this instance belongs to a particular class.The input needed by the classifier is the set ofprobabilities of seeing each cue given eachclass??????|??.
Those parameters should have1http://panacea-lr.eu/29been previously induced (using Maximum Like-lihood Estimation (MLE), a Bayesian approach,etc.
).The classifier web service reads those prob-abilities from a coma separated file and the cuevectors of the instances we want to classify inWeka format (Quinlan, 1993).
In our implemen-tation, we work with binary classification, i.e.
wewant to decide whether the noun belongs or doesnot belong to a given class.
Thus, the servicereturns the most likely class for each instancegiven the parameters and a score for this classifi-cation (i.e.
how different was the probability ofbeing and not being a member of the class).2.2 Bayesian Estimation of ProbabilitiesA second web service performs parameter infer-ence for the Naive Bayes classifier using Bayes-ian methods.Bayesian methods (Griffiths et al, 2008;Mackay, 2003) are a formal framework to intro-duce prior knowledge when estimating the pa-rameters (probabilities) of a given system.
Themain difference between those methods andMLE is that the latter use only data to estimateparameters, while the former use both data andprior knowledge.An example of Bayesian learning is determin-ing the probability of a coin producing heads in ashort throw series.
A MLE approach will deter-mine this probability as ???????
?
???????
.
Thus,after observing a sequence of 5 heads in a row,MLE would assess that the probability of thecoin producing heads is 1.
Nevertheless, becauseof our knowledge, we would rather say that a tailis more than possible, and that the coin probabil-ity can still be close to 0.5.
Bayesian modelsallow us to formally introduce this knowledgewhen estimating the probabilities.In the case of Naive Bayes classification usingcue vectors, we need to estimate ??????|?
?foreach cue and k (for binary classification thiswould be k=1 for being a member of the classand k=0 for not being a member of the class).Bayesian modelling computes these parame-ters approximating them by their Maximum aPosteriori (MAP) estimator.
The canonical ap-proach introduces the prior probabilities as aBeta distribution, and leads to the followingMAP estimator (see Griffiths et al (2008) andMackay  (2003) for details):???
?
???????|??
??????
???
?
?????
????????
???
?
?????
???
?
????
???
?
????
??
?Where ?????
???
and ????
???
are the observed oc-currences in real data (?????
???
is the number oftimes we have seen cuei with class k and ????
??
?is the number of times we have not seen it, and?????
???
and ????
???
represent what is called virtualdata, this is, the data we expect to observe a pri-ori.
Thus, it can be seen from the MAP estimatorthat Bayesian inference allows us to add virtualdata to actual evidence.The web service we want to show in this dem-onstration implements the estimation of??????|??
combining the data and the priors sup-plied by the user.
The service reads labelled datain Weka format and the priors for each cue andclass and computes ??????|??.
The output of thisweb service can be directly used to classify newinstances with the first one.3 Test case: Lexical AcquisitionAs a showcase, we will show our work in cue-based noun classification.
The aim is the auto-matic acquisition of lexical semantic informationby building classifiers for a number of lexicalsemantic classes.3.1 Demonstration OutlineIn our demonstration, we will show how we canuse the web services to learn, tune and testBayesian models for different lexical classes.
Wewill compare our results with a Naive Bayesapproach, which can also be learned with oursystem, using null virtual data.First of all, we will get noun occurrences froma corpus and encode these occurrences as cuevectors applying a set of regular expressions.This will be done with another web service thatdirectly outputs a Weka file.
This Weka file willbe divided into train and test data.Secondly, the obtained training data will beused as input in the Bayesian learner web ser-vice, obtaining the values for ??????|??
for eachcue and class.
We will perform two calls: oneusing prior knowledge and one without it (MLEapproach).Finally, these two sets of parameters will beused to annotate the test data and we will com-pare the performance of the Bayesian model withthe performance of the MLE model.AcknowledgmentsThis work was funded by the EU 7FP project248064 PANACEA.30ReferencesJ.
Bybee.
1998.
The emergent lexicon.
CLS 34: Thepanels.
Chicago Linguistics Society.
421-435.N.
Chater,and C.D.
Manning.
2006.
Probabilisticmodels of language processing and acquisi-tion.Trends in Cognitive Sciences, 10, 335-344.N.
Chomsky.1980.
Rules and representations.
Oxford:Basil Blackwell.A.
E. Goldberg.
2006.
Constructions at work.
OxfordUniversity Press.T.
L. Griffiths, C. Kemp, and J.B. Tenenbaum.
2008.Bayesian models of cognition.
In Ron Sun (ed.
),Cambridge Handbook of Computational CognitiveModeling.Cambridge University Press.D.
J. C. MacKay.
2003.
Information Theory, Infe-rence, and Learning Algorithms.
Cambridge Uni-versity Press, 2003.
ISBN 0-521-64298-1R.J.
Quinlan.
1993.
C4.5: Programs for MachineLearning.
Series in Machine Learning.MorganKaufman, San Mateo, CA.Xu, F. and Tenenbaum, J.
B.
(2007).Word learning asBayesian inference.Psychological Review 114(2).G.K.
Zipf.
1935.
The Psycho-Biology of Language,Houghton Mifflin, Boston.31
