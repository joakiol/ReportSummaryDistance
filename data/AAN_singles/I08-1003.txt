A Hybrid Approach to the Induction of Underlying MorphologyMichael TepperDepartment of LinguisticsUniversity of WashingtonSeattle, WA 98195mtepper@u.washington.eduFei XiaDepartment of LinguisticsUniversity of WashingtonSeattle, WA 98195fxia@u.washington.eduAbstractWe present a technique for refining a base-line segmentation and generating a plausibleunderlying morpheme segmentation by inte-grating hand-written rewrite rules into an ex-isting state-of-the-art unsupervised morpho-logical induction procedure.
Performance onmeasures which consider surface-boundaryaccuracy and underlying morpheme consis-tency indicates this technique leads to im-provements over baseline segmentations forEnglish and Turkish word lists.1 Introduction1.1 Unsupervised Morphological InductionThe primary goal of unsupervised morphological in-duction (UMI) is the simultaneous induction of areasonable morphological lexicon as well as an op-timal segmentation of a corpus of words, given thatlexicon.
The majority of existing approaches em-ploy statistical modeling towards this goal, but dif-fer with respect to how they learn or refine the mor-phological lexicon.
While some approaches involvelexical priors, either internally motivated or moti-vated by the minimal description length (MDL) cri-terion, some utilize heuristics.
Pure maximum like-lihood (ML) approaches may refine the lexicon withheuristics in lieu of explicit priors (Creutz and La-gus, 2004), or not make categorical refinements at allconcerning which morphs are included, only proba-bilistic refinements through a hierarchical EM pro-cedure (Peng and Schuurmans, 2001).
Approachesthat optimize the lexicon with respect to priors comein several flavors.
There are basic maximum a priori(MAP) approaches that try to maximize the proba-bility of the lexicon against linguistically motivatedpriors (Deligne and Bimbot, 1997; Snover and Brent,2001; Creutz and Lagus, 2005).
An alternative toMAP, MDL approaches use their own set of pri-ors motivated by complexity theory.
These studiesattempt to minimize lexicon complexity (bit-lengthin crude MDL) while simultaneously minimizing thecomplexity (by maximizing the probability) of thecorpus given the lexicon (de Marcken, 1996; Gold-smith, 2001; Creutz and Lagus, 2002).Many of the approaches mentioned above utilize asimplistic unigram model of morphology to producethe segmentation of the corpus given the lexicon.Substrings in the lexicon are proposed as morphswithin a word based on frequency alone, indepen-dently of phrase-, word- and morph-surroundings (deMarcken, 1996; Peng and Schuurmans, 2001; Creutzand Lagus, 2002).
There are many approaches,however, which further constrain the segmentationprocedure.
The work by Creutz and Lagus (2004;2005; 2006) constrains segmentation by accountingfor morphotactics, first assigning mophotactic cate-gories (prefix, suffix, and stem) to baseline morphs,and then seeding and refining an HMM using thosecategory assignments.
Other more structured mod-els include Goldsmith?s (2001) work which, insteadof inducing morphemes, induces morphological sig-natures like {?, s, ed, ing} for English regular verbs.Some techniques constrain possible analyses by em-ploying approximations for morphological meaningor usage to prevent false derivations (like singed =sing + ed).
There is work by Schone and Juraf-sky (2000; 2001) where meaning is proxied by word-and morph-context, condensed via LSA.
Yarowskyand Wicentowski (2000) and Yarowsky et al (2001)use expectations on relative frequency of alignedinflected-word, stem pairs, as well as POS contextfeatures, both of which approximate some sort ofmeaning.1.2 Allomorphy in UMIAllomorphy, or allomorphic variation, is the processby which a morpheme varies (orthographically or17phonologically) in particular contexts, as constrainedby a grammar.1 To our knowledge, there is onlyhandful of work within UMI attempting to integrateallomorphy into morpheme discovery.
A notable ap-proach is the Wordframe model developed by Wi-centowski (2002), which performs weighted edits onroot-forms, given context, as part of a larger similar-ity alignment model for discovering <inflected-form,root-form> pairs.Morphological complexity is fixed by a template;the original was designed for inflectional morpholo-gies and thus constrained to finding an optional affixon either side of a stem.
Such a template wouldbe difficult to design for agglutinative morphologieslike Turkish or Finnish, where stems are regularly in-flected by chains of affixes.
Still, it can be extended.A notable recent extension accounts for phenomenalike infixation and reduplication in Filipino (Chengand See, 2006).In terms of allomorphy, the approach succeedsat generalizing allomorphic patterns, both stem-internally and at points of affixation.
A major draw-back is that, so far, it does not account for affix allo-morphy involving character replacement?that is, be-yond point-of-affixation epentheses or deletions.1.3 Our ApproachOur approach aims to integrate a rule-based com-ponent consisting of hand-written rewrite rules intoan otherwise unsupervised morphological inductionprocedure in order to refine the segmentations it pro-duces.1.3.1 Context-Sensitive Rewrite RulesThe major contribution of this work is a rule-based component which enables simple encoding ofcontext-sensitive rewrite rules for the analysis of in-duced morphs into plausible underlying morphemes.2A rule has the form general form:?underlying?
?surface/ ?l.
context_ ?r.
context(1)It is also known as a SPE-style rewrite rule, partof the formal apparatus to introduced by Chom-sky and Halle (1968) to account for regularities inphonology.
Here we use it to describe orthographic1In this work we focus on orthographic allomorphy.2Ordered rewrite rules, when restricted from applyingto their own output, have similar expressive capabilitiesto Koskenniemi?s two-level constraints.
Both define regu-lar relations on strings, both can be compiled into lexicaltransducers, and both have been used in finite-state ana-lyzers (Karttunen and Beesley, 2001).
We choose orderedrules because they are easier to write given our task andresources.patterns.
Mapping morphemes to underlying formswith context-sensitive rewrite rules allows us to peerthrough the fragmentation created by allomorphicvariation.
Our experiments will show that thishas the effect of allowing for more unified, consis-tent morphemes while simultaneously making sur-face boundaries more transparent.For example, take the English multipurpose inflec-tional suffix ?s, normally written as ?s, but as ?es aftersibilants (s,sh, ch, .
.
.
).
We can write the followingSPE-style rule to account for its variation.?underlying?
esurface/ [+SIB] + _s (2)This rule says, ?Insert an e (map nothing to e) fol-lowing a character marked as a sibilant (+SIB) anda morphological boundary (+), at the focus position(_), immediately preceding an s.?
In short, it en-ables the mapping of the underlying form ?s to ?esby inserting an e before s where appropriate.
Whenthis rule is reversed to produce underlying analyses,the ?es variant in such words as glasses, matches,swishes, and buzzes can be identified with the ?s vari-ant in words like plots, sits, quakes, and nips.1.3.2 Overview of ProcedureBefore the start of the procedure, there is a pre-processing step to derive an initial segmentation.This segmentation is fed to the EM Stage, the goalof which is to find the maximum probability seg-mentation of a wordlist into underlying morphemes.First, analyses of initial segments are produced byrule.
Then, their frequency is used to determine theirlikelihood as underlying morphemes.
Finally, proba-bility of a segmentation into underlying morphemesis maximized.The output segmentation feeds into the SplitStage, where heuristics are used to split large, high-frequency segments that fail to break into smallerunderlying morphemes during the EM algorithm.2 ProcedureA flowchart of the procedure is given in Figure 1.Preprocessing We use the Categories-MAP algo-rithm developed by Creutz and Lagus (2005; 2006)to produce an initial morphological segmentation.Here, a segmentation is optimized by maximum aposteriori estimate given priors on length, frequency,and usage of morphs stored in the model.
Theirprocedure begins with morphological tags indicatingbasic morphotactics (prefix, stem, suffix, noise) be-ing assigned heuristically to a baseline segmentation.That tag assignment is then used to seed an HMM.18EM STAGEPreprocessMorfessor 0.9Categories-MAPStep 1ProposeUnderlyingAnalysesStep 2EstimateHMMProbabilitiesStep 3Re-segmentWordlistRewriteRulesanalysesprobs.Orig.Word-listSPLIT STAGEStep 4Re-tagSegmentationRewriteRulesStep 7Re-segment(Split) Morphsprobs.surface segment'n,analysestags, surface segmentationsurface segmentationtags,surface segment'ntags,Step 6EstimateHMMProbabilitiesStep 5ProposeUnderlyingAnalysestags, surface segmentationsurface segment'ntags,underlying segmentationtags,Figure 1: Flowchart showing the entire procedure.Optimal segmentation of a word is simultaneouslythe best tag and morph3 sequence given that word.The contents of the model are optimized with respectto length, frequency, and usage priors during split-ting and joining phases.
The final output is a taggedsegmentation of the input word-list.2.1 EM StageThe model we train is a modified version of themorphological HMM from the work of Creutz andLagus (2004-2006), where a word w consists of asequence of morphs generated by a morphological-category tag sequence.
The difference between theirHMM and ours is that theirs emits surface morphs,while ours emits underlying morphemes.
Morphemesmay either be analyses proposed by rule or surfacemorphs acting as morphemes.
We do not modify thetags Creutz and Lagus use (prefix, stem, suffix, andnoise).We proceed by EM, initialized by the preprocessedsegmentation.
Rule-generated underlying analysesare produced (Step 1), and used to estimate the emis-sion probability P (ui|ti) and transition probabilityP (ti|ti?1) (Step 2).
In successive E-steps, Steps 1and 2 are repeated.
The M-step (Step 3) involvesfinding the maximum probability decoding of eachword according to Eq (6), i.e.
maximum probabilitytag and morpheme sequence.Step 1 - Derive Underlying Analyses In thisstep, handwritten context-sensitive rewrite rules de-rive context-relevant analyses for morphs in the pre-processed segmentation.
These analyses are pro-duced by a set of ordered rules that propose dele-3A morph is a linguistic morpheme as it occurs inproduction, i.e.
as it occurs in a surface word.tions, insertions, or substitutions when triggered bythe proper characters around a segmentation bound-ary.4 A rule applies wherever contextually triggered,from left to right, and may apply more than onceto the same word.
To prevent the runaway appli-cation of certain rules, a rule may not apply to itsown output.
The result of applying a rule is a (pos-sibly spelling-changed) segmented word, which is fedto the next rule.
This enables multi-step analyses byusing rules designed specifically to apply to the out-puts of other rules.
See Figure 2 for a small example.Step 2 - Estimate HMM Probabilities Tran-sition probabilities P (ti|ti?1) are estimated by max-imum likelihood, given a tagged input segmentation.Emission probabilities P (ui|ti) are also estimatedby maximum likelihood, but the situation is slightlymore complex; the probability of morphemes ui areestimated according to frequencies of association(coindexation) with surface morphs si and tags ti.Furthermore an underlying morpheme ui can ei-ther be identical to its associated surface morph siwhen no rules apply, or be a rule-generated analysis.For the sake of clarity, we call the former u?i and thelatter u?
?i , as defined below:ui ={u?i if ui = siu?
?i otherwiseWhen an underlying morpheme ui is associated toa surface morph s, we refer to s as an allomorph of4Some special substitution rules, like vowel harmonyin Turkish and Finnish, have a spreading effect, mov-ing from syllable to syllable within and beyond morph-boundaries.
In our formulation, these rules differ fromother rules by not being conditioned on a morph-boundary.19city  +   s glass  +   sseat  +  s citi  +  es glass  +  esSTM STMSTMSUF SUFSUFUnderlying AnalysesSurface SegmentationTags?
?e / [+VWL] + _s?
?e / [+SIB] + _sApplicable Rule(s)y?i / _ + [+ANY]Features:VWL = vowelANY = any char.SIB = sibilant{s,sh,ch,...}seat  +  sFigure 2: Underlying analyses for a segmentation are generated by passing it through context-sensitiverewrite rules.
Rules apply to some morphs (e.g., citi?
city) but not to others (e.g., glass?
glass).ui.
The probability of ui given tag ti is calculated bysumming over all allomorphs s of ui the probabilitythat ui realizes s in the context of tag ti:P (ui|ti) =?s?allom.-of(ui)P (ui, s|ti) (3)=?s?allom.-of(ui)P (ui|s, ti)P (s|ti) (4)Both Eq (3) and Eq (4) are trivial to estimatewith counting on our input from Step 1 (see Figure2).
We show (4) because it has the term P (ui|s, ti),which may be used for thresholding and discountingterms of the sum where ui is rarely associated witha particular allomorph and tag.
In the future, suchdiscounting may be useful to filter out noise gener-ated by noisy or permissive rules.
So far, this typeof discounting has not improved results.Step 3 - Resegment Word List Next we reseg-ment the word list into underlying morphemes.Searching for the best breakdown of a word w intomorpheme sequence u and tag sequence t, we maxi-mize the probability of the following formula:P (w,u, t) = P (w|u, t)P (u, t)= P (w|u, t)P (u|t)P (t) (5)To simplify, we assume that P (w|u, t) is equal toone.5 With this assumption in mind, Eq (5) reducesto P (u|t)P (t).
With independence assumptions anda local time horizon, we estimate:argmaxu,tP (u|t)P (t)?
argmaxu,t[ n?i=1P (ui|ti)P (ti|ti?1)](6)5In other words, we make the assumption that a se-quence of underlying morphemes and tags correspondsto just one word.
This assumption may need revision incases where morphemes can optionally undergo the typesof spelling changes we are trying to encode; this has notbeen the case for the languages under investigation.The search for the maximum probability tag andmorph sequence in Eq (6) is carried out by a modi-fied version of the Viterbi algorithm.
The maximumprobability segmentation for a given word may be amixture of both types of underlying morpheme, u?iand u?
?i .
Also, wherever we have a choice betweenemitting u?i, identical to the surface form, or u?
?i ,an analysis with rule-proposed changes, the highestprobability of the two is always selected.2.2 Split StageMany times, large morphs have substructure andyet are too frequent to be split when segmented bythe HMM in the EM Stage.
To overcome this, weapproximately follow the heuristic procedure6 laidout by Creutz and Lagus (2004), encouraging split-ting of larger morphs into smaller underlying mor-phemes.
This process has the danger of introducingmany false analyses, so first the segmentation mustbe re-tagged (Step 4) to identify which morphemesare noise and should not be used.
Once we re-tag, were-analyze morphs in the surface segmentation (Step5) and re-estimate HMM probabilities (Step 6).
(forSteps 5 and 6, refer to Steps 1 and 2).
Finally, weuse these HMM probabilities to split morphs (Step7).Step 4 - Re-tag the Segmentation To iden-tify noise morphemes, we estimate a distributionP (CAT |ui) for three true categories CAT (prefix,stem, or suffix) and one noise category; we then as-sign categories randomly according to this distribu-tion.
Stem probabilities are proportional to stem-length, while affix probabilities are proportional toleft- or right- perplexity.
The probability of true cat-egories are also tied to the value of sigmoid-cutoffparameters, the most important of which is b, whichthresholds the probability of both types of affix (pre-fix and suffix).The probability of the noise category is converselyrelated to the product of true category probabilities;6The main difference between our procedure andCreutz and Lagus (2004) is that we allow splitting intotwo or more morphemes (see Step 7) while they allowbinary splits only.20when true categories are less probable, noise becomesmore probable.
Thus, adjusting parameters like bcan increase or decrease the probability of noise.Step 7 - Split Morphs In this step, we exam-ine <morph, tag> pairs in the segmentation to seeif a split into sub-morphemes is warranted.
We con-strain this process by restricting splitting to stems(with the option to split affixes), and by splittinginto restricted sequences of tags, particularly avoid-ing noise.
We also use parameter b in Step 4 asa way to discourage excessive splitting by taggingmore morphemes as noise.
Stems are split into thesequence: (PRE?
STM SUF?).
Affixes (prefixes andsuffixes) are split into other affixes of the same cat-egory.
Whether to split affixes depends on typolog-ical properties of the language.
If a language hasagglutinative suffixation, for example, we hand-set aparameter to allow suffix-splitting.When examining a morph for splitting, we searchover all segmentations with at least one split, andchoose the one that is both optimal according to Eq(6) and does not violate our constraints on what cat-egory sequences are allowed for its category.
We endthis step by returning to the EM Stage, where an-other cycle of EM is performed.3 Experiments and ResultsIn this section we report and discuss development re-sults for English and Turkish.
We also report final-test results for both languages.
Results for the pre-processed segmentation are consistently used as abaseline.
In order to isolate the effect of the rewriterules, we also compare against results taken on aparallel set of experiments, run with all the same pa-rameters but without rule-generated underlying mor-phemes, i.e.
without morphemes of type u?
?i .
Butbefore we get to these results, we will describe theconditions of our experiments.
First we introducethe evaluation metrics and data used, and then de-tail any parameters set during development.3.1 Evaluation MetricsWe use two procedures for evaluation, described inthe Morpho Challenge ?05 and ?07 Competition Re-ports (Kurimo et al, 2006; Kurimo et al, 2007).Both procedures use gold-standards created withcommercially available morphological analyzers foreach language.
Each procedure is associated with itsown F-score-based measure.The first was used in Morpho Challenge ?05, andmeasures the extent to which boundaries match be-tween the surface-layer of our segmentations andgold-standard surface segmentations.The second was used in Morpho Challenge ?07and measures the extent to which morphemes matchbetween the underlying-layer of our segmentationsand gold-standard underlying analyses.
The F-scorehere is not actually on matched morphemes, but in-stead on matched morpheme-sharing word-pairs.
Apoint is given whenever a morpheme-sharing word-pair in the gold-standard segmentation also sharesmorphemes in the test segmentation (for recall), andvice-versa for precision.3.2 DataTraining Data The data-sets used for trainingwere provided by the Helsinki University of Technol-ogy in advance of the Morpho Challenge ?07 and weredownloaded by the authors from the contest web-site7.
According to the website, they were compiledfrom the University of Leipzig Wortschatz Corpora.Sentences Tokens TypesEnglish 3?
106 6.22?
107 3.85?
105Turkish 1?
106 1.29?
107 6.17?
105Table 1: Training corpus sizes vary slightly, with 3million English sentences and 1 million Turkish sen-tences.Development Data The development gold-standard for the surface metric was provided inadvance of Morpho Challenge ?05 and consists ofsurface segmentations for 532 English and 774Turkish words.The development gold-standard for the underlyingmetric was provided in advance of Morpho Challenge?07 and consists of morphological analyses for 410English and 593 Turkish words.Test Data For final testing, we use the gold-standard data reserved for final evaluation in theMorpho Challenge ?07 contest.
The gold-standardconsists of approximately 1.17 ?
105 English and3.87 ?
105 Turkish analyzed words, roughly a tenththe size of training word-lists.
Word pairs that existin both the training and gold standard are used forevaluation.3.3 ParametersThere are two sets of parameters used in this exper-iment.
First, there are parameters used to producethe initial segmentation.
They were set as suggestedin Cruetz and Lagus (2005), with parameter b tunedon development data.7http://www.cis.hut.fi/morphochallenge2007/datasets.shtml211.55 6065 7075Baseline EM SPL:b=100 SPL:b=300 SPL:b=500English: Surface Layer Eval.
(F-Score)No RulesWith Rules2.55 6065 7075Baseline EM SPL:b=100 SPL:b=300 SPL:b=500Turkish: Surface Layer Eval.
(F-Score)No RulesWith Rules3.35 40 4550 55 6065 70Baseline EM SPL:b=100 SPL:b=300 SPL:b=500English: Underlying Layer Eval.
(F-Score)No RulesWith Rules4.20 3040 5060 70Baseline EM SPL:b=100 SPL:b=300 SPL:b=500Turkish: Underlying Layer Eval.
(F-Score)No RulesWith RulesFigure 3: Development results for the preprocessed initial segmentation (Baseline), and segmentations pro-duced by our approach, first after the EM Stage (EM) and again after the Split Stage (SPL) with differentvalues of parameter b.
Rules that generate underlying analyses have either been included (With Rules), orleft out (No Rules).Then there are parameters used for the main pro-cedure.
Here we have rewrite rules, numerical pa-rameters, and one typology parameter.
Rewrite rulesand any orthographic features they use were culledfrom linguistic literature.
We currently have 6 rulesfor English and 10 for Turkish; See Appendix A.1for the full set of English rules used.
Numerical pa-rameters were set as suggested in Cruetz and Lagus(2004), and following their lead we tuned b on devel-opment data; we show development results for thefollowing values: b = 100, 300, and 500 (see Fig-ure 3).
Finally, as introduced in Section 2.2, we havea hand-set typology parameter that allows us to splitprefixes or suffixes if the language has an aggluti-native morphology.
Since Turkish has agglutinativesuffixation, we set this parameter to split suffixes forTurkish.3.4 Development ResultsDevelopment results were obtained by evaluating En-glish and Turkish segmentations at several stages,and with several values of parameter b as shown inFigure 3.Overall, our development results were very pos-itive.
For the surface-level evaluation, the largestF-score improvement was observed for English (Fig-ure 3, Chart 1), 63.75% to 68.99%, a relative F-scoregain of 8.2% over the baseline segmentation.
TheTurkish result also improves to a similar degree, butit is only achieved after the model as been refined bysplitting.
For English we observe the improvementearlier, after the EM Stage.
For the underlying-levelevaluation, the largest F-score improvement was ob-served for Turkish (Chart 4), 31.37% to 54.86%, arelative F-score gain of over 74%.In most experiments with rules to generate under-lying analyses (With Rules), the successive applica-tions of EM and splitting result in improved results.Without rule-generated forms (No Rules) the resultstend be negative compared to the baseline (see Fig-ure 3, Chart 2), or mixed (Charts 1 and 4).
Whenwe look at recall and precision numbers directly, weobserve that even without rules, the algorithm pro-duces large recall boosts (especially after splitting).However, these boosts are accompanied by precisionlosses, which result in unchanged or lower F-scores.The exception is the underlying-level evaluationof English segmentations (Figure 3, Chart 3).
Herewe observe a near-parity of F-score gains for seg-mentations produced with and without underlyingmorphemes derived by rule.
One explanation is thatthe English initial segmentation is conservative andthat coverage gains are the main reason for improvedEnglish scores.
Creutz and Lagus (2005) note thatthe Morfessor EM approach often has better cover-age than the MAP approach we use to produce the22Hybrid:After SplitMC Morf.
MC Top Baseline No Rules With RulesEnglish 47.17 60.81 47.04 57.35 59.78Turkish 37.10 29.23 32.76 31.10 54.54Table 2: Final test F-scores on the underlying morpheme measure used in Morpho Challenge ?07.
MC Morf.is Morfessor MAP, which was used as a reference method in the contest.
MC Top is the top contestant.For our hybrid approach, we show the F-score obtained with and without using rewrite rules.
The splittingparameter b was set to the best performing value seen in development evaluations (Tr.
b = 100, En.
b = 500).initial segmentation.
Also, in English, allomorphy isnot as extensive as in Turkish (see Chart 4) whereprecision losses are greater without rules, i.e.
whennot representing allomorphs by the same morpheme.3.5 Final Test ResultsFinal test results, given in Table 2, are mixed.
ForEnglish, though we improve on our baseline and onMorfessor MAP trained by Creutz and Lagus, we arebeaten by the top unsupervised Morpho Challengecontestant, entered by Delphine Bernhard (2007).Bernhard?s approach was purely unsupervised anddid not explicitly account for allomorphic phenom-ena.
There are several possible reasons why we werenot the top performer here.
Our splitting constraintfor stems, which allows them to split into stems andchains of affixes, is suited for agglutinative morpholo-gies.
It does not seem particularly well suited to En-glish morphology.
Our rewrite-rules might also beimproved.
Finally, there may be other, more press-ing barriers (besides allomorphy) to improving mor-pheme induction in English, like ambiguity betweenhomographic morphemes.For Turkish, the story is very different.
We observeour baseline segmentation going from 32.76% F-scoreto 54.54% when re-segmented using rules, a relativeimprovement of over 66%.
Compared with the topunsupervised approach, Creutz and Lagus?s Morfes-sor MAP, our F-score improvement is over 48%.
Thedistance between our hybrid approach and unsuper-vised approaches emphasizes the problem allomor-phy can be for a language like Turkish.
Turkishinflectional suffixes, for instance, regularly undergomultiple spelling-rules and can have 10 or more vari-ant forms.
Knowing that these variants are all onemorpheme makes a difference.4 ConclusionIn this work we showed that we can use a smallamount of knowledge in the form of context-sensitiverewrite rules to improve unsupervised segmentationsfor Turkish and English.
This improvement can bequite large.
On the morpheme-consistency measureused in the last Morpho Challenge, we observed animprovement of the Turkish segmentation of over66% against the baseline, and 48% against the top-of-the-line unsupervised approach.Work in progress includes error analysis of the re-sults to more closely examine the contribution ofeach rule, as well as developing rule sets for addi-tional languages.
This will help highlight various as-pects of the most beneficial rules.There has been recent work on discovering allo-morphic phenomena automatically (Dasgupta andNg, 2007; Demberg, 2007).
It is hoped that our workcan inform these approaches, if only by showing whatvariation is possible, and what is relevant to particu-lar languages.
For example, variation in inflectionalsuffixes, driven by vowel harmony and other phenom-ena, should be captured for a language like Turkish.Future work involves attempting to learn broad-coverage underlying morphology without the hand-coded element of the current work.
This might in-volve employing aspects of the most beneficial rulesas variable features in rule-templates.
It is hopedthat we can start to derive underlying morphemesthrough processes (rules, constraints, etc) suggestedby these templates, and possibly learn instantiationsof templates from seed corpora.A AppendixA.1 Rules Used For Englishe epenthesis before s suffix?
?e / ..[+V] + _s?
?e / ..[+SIB] + _slong e deletione ??
/ ..[+V][+C]_ + [+V]change y to i before suffixy ?i / ..[+C] +?
_ + [+ANY]consonant gemination?
??
[+STOP] / ..?
[+STOP]_ + [+V]?
??
[+STOP] / ..?
[+STOP]_ + [+GLI]Table 3: English Rules23A.2 Example SegmentationsBase EM SPL:b=300 SPL:b=500happen s happen s happ e n s happen shappier happier happi er happi erhappiest happiest happ i est happiesthappily happily happi ly happi lyhappiness happiness happi ness happinessTable 4: Surface segmentations after preprocessing(Base), EM Stage (EM), and Split Stage (SPL)ReferencesDelphine Bernhard.
2007.
Simple morpheme label-ing in unsupervised morpheme analysis.
In Work-ing Notes for the CLEF 2007 Workshop, Budapest,Hungary.Charibeth K. Cheng and Solomon L. See.
2006.
Therevised wordframe model for the filipino language.Journal of Research in Science, Computing andEngineering.Noam Chomsky and Morris Halle.
1968.
The SoundPattern of English.
Harper & Row, New York.Mathias Creutz and Krista Lagus.
2002.
Unsuper-vised discovery of morphemes.
In Proc.
Work-shop on Morphological and Phonological Learningof ACL?02, pages 21?30, Philadelphia.
Associationfor Computational Linguistics.Mathias Creutz and Krista Lagus.
2004.
Inductionof a simple morphology for highly inflecting lan-guages.
In Proc.
7th Meeting of the ACL SpecialInterest Group in Computational Phonology (SIG-PHON), pages 43?51, Barcelona.Mathias Creutz and Krista Lagus.
2005.
Inducingthe morphological lexicon of a natural languagefrom unannotated text.
In Proc.
International andInterdisciplinary Conference on Adaptive Knowl-edge Representation and Reasoning (AKRR?05),pages 106?113, Espoo, Finland.Mathias Creutz and Krista Lagus.
2006.
Morfessorin the morpho challenge.
In Proc.
PASCAL Chal-lenge Workshop on Unsupervised Segmentation ofWords into Morphemes, Venice, Italy.Sajib Dasgupta and Vincent Ng.
2007.
High perfor-mance, language-independent morphological seg-mentation.
In Proc.
NAACL?07.Carl G. de Marcken.
1996.
Unsupervised LanguageAcquisition.
Ph.D. thesis, Massachussetts Insti-tute of Technology, Boston.Sabine Deligne and Fr?d?ric Bimbot.
1997.
Inferenceof variable-length linguistic and acoustic units bymultigrams.
Speech Communication, 23:223?241.Vera Demberg.
2007.
A language-independent un-supervised model for morphological segmentation.In Proc.
ACL?07.John Goldsmith.
2001.
Unsupervised learning of themorphology of a natural language.
ComputationalLinguistics, 27.2:153?198.Lauri Karttunen and Kenneth R. Beesley.
2001.
Ashort history of two-level morphology.
In Proc.ESSLLI 2001.Mikko Kurimo, Mathias Creutz, Matti Varjokallio,Ebru Arisoy, and Murat Sara?lar.
2006.
Unsu-pervised segmentation of words into morphemes ?Morpho Challenge 2005, an introduction and eval-uation report.
In Proc.
PASCAL Challenge Work-shop on Unsupervised Segmentation of Words intoMorphemes, Venice, Italy.Mikko Kurimo, Mathias Creutz, and Matti Var-jokallio.
2007.
Unsupervised morpheme analysisevaluation by a comparison to a linguistic goldstandard ?
Morpho Challenge 2007.
In WorkingNotes for the CLEF 2007 Workshop, Budapest,Hungary.Fuchun Peng and Dale Schuurmans.
2001.
A hier-archical em approach to word segmentation.
InProc.
4th Intl.
Conference on Intel.
Data Analysis(IDA), pages 238?247.Patrick Schone and Daniel Jurafsky.
2000.Knowledge-free induction of morphology using la-tent semantic analysis.
In Proc.
CoNLL?00 andLLL?00, pages 67?72, Lisbon.Patrick Schone and Daniel Jurafsky.
2001.Knowledge-free induction of inflectional morpholo-gies.
In Proc.
NAACL?01, Pittsburgh.Matthew G. Snover and Michael R. Brent.
2001.A bayesian model for morpheme and paradigmidentification.
In Proc.
ACL?01, pages 482?490,Toulouse, France.Richard Wicentowski.
2002.
Modeling and Learn-ing Multilingual Inflectional Morphology in a Min-imally Supervised Framework.
Ph.D. thesis, JohnsHopkins University, Baltimore, Maryland.David Yarowsky and Richard Wicentowski.
2000.Minimally supervised morphological analysis bymultimodal alignment.
In Proc.
ACL?00.David Yarowsky, Grace Ngai, and Richard Wicen-towski.
2001.
Inducing multilingual text analysistools via robust projection accross aligned corpora.In Proc.
HLT?01, volume HLT 01, pages 161?168,San Diego.24
