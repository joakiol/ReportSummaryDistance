Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 211?214,New York, June 2006. c?2006 Association for Computational LinguisticsIncorporating Gesture and Gaze into Multimodal Models ofHuman-to-Human CommunicationLei ChenDept.
of Electrical and Computer EngineeringPurdue UniversityWest Lafayette, IN 47907chenl@ecn.purdue.eduAbstractStructural information in language is im-portant for obtaining a better understand-ing of a human communication (e.g., sen-tence segmentation, speaker turns, andtopic segmentation).
Human communica-tion involves a variety of multimodal be-haviors that signal both propositional con-tent and structure, e.g., gesture, gaze, andbody posture.
These non-verbal signalshave tight temporal and semantic links tospoken content.
In my thesis, I am work-ing on incorporating non-verbal cues intoa multimodal model to better predict thestructural events to further improve theunderstanding of human communication.Some research results are summarized inthis document and my future research planis described.1 IntroductionIn human communication, ideas tend to unfold ina structured way.
For example, for an individualspeaker, he/she organizes his/her utterances into sen-tences.
When a speaker makes errors in the dy-namic speech production process, he/she may cor-rect these errors using a speech repair scheme.
Agroup of speakers in a meeting organize their ut-terances by following a floor control scheme.
Allthese structures are helpful for building better mod-els of human communication but are not explicit inthe spontaneous speech or the corresponding tran-scription word string.
In order to utilize these struc-tures, it is necessary to first detect them, and to doso as efficiently as possible.
Utilization of variouskinds of knowledge is important; For example, lex-ical and prosodic knowledge (Liu, 2004; Liu et al,2005) have been used to detect structural events.Human communication tends to utilize not onlyspeech but also visual cues such as gesture, gaze,and so on.
Some studies (McNeill, 1992; Casselland Stone, 1999) suggest that gesture and speechstem from a single underlying mental process, andthey are related both temporally and semantically.Gestures play an important role in human commu-nication but use quite different expressive mecha-nisms than spoken language.
Gaze has been foundto be widely used in coordinating multi-party con-versations (Argyle and Cook, 1976; Novick, 2005).Given the close relationship between non-verbalcues and speech and the special expressive capac-ity of non-verbal cues, we believe that these cuesare likely to provide additional important informa-tion that can be exploited when modeling structuralevents.
Hence, in my Ph.D thesis, I have been in-vestigating the combination of lexical, prosodic, andnon-verbal cues for detection of the following struc-tural events: sentence units, speech repairs, andmeeting floor control.This paper is organized as follows: Section 1 hasdescribed the research goals of my thesis.
Section 2summarizes the efforts made related to these goals.Section 3 lays out the research work needed to com-plete my thesis.2 Completed WorksOur previous research efforts related to multimodalanalysis of human communication can be roughlygrouped to three fields: (1) multimodal corpus col-211Figure 1: VACE meeting corpus productionlection, annotation, and data processing, (2) mea-surement studies to enrich knowledge of non-verbalcues to structural events, and (3) model construc-tion using a data-driven approach.
Utilizing non-verbal cues in human communication processing isquite new and there is no standard data or off-the-shelf evaluation method.
Hence, the first part of myresearch has focused on corpus building.
Throughmeasurement investigations, we then obtain a bet-ter understanding of the non-verbal cues associatedwith structural events in order to model those struc-tural events more effectively.2.1 Multimodal Corpus CollectionUnder NSF KDI award (Quek and et al, ), we col-lected a multimodal dialogue corpus.
The corpuscontains calibrated stereo video recordings, time-aligned word transcriptions, prosodic analyses, andhand positions tracked by a video tracking algo-rithm (Quek et al, 2002).
To improve the speedof producing a corpus while maintaining its qual-ity, we have investigated factors impacting the ac-curacy of the forced alignment of transcriptions toaudio files (Chen et al, 2004a).Meetings, in which several participants commu-nicate with each other, play an important role in ourdaily life but increase the challenges to current infor-mation processing techniques.
Understanding hu-man multimodal communicative behavior, and howwitting and unwitting visual displays (e.g., gesture,head orientation, gaze) relate to spoken content iscritical to the analysis of meetings.
These multi-modal behaviors may reveal static and dynamic so-cial structure of the meeting participants, the flowof topics being discussed, the control of floor ofthe meeting, and so on.
For this purpose, we havebeen collecting a multimodal meeting corpus un-der the sponsorship of ARDA VACE II (Chen et al,2005).
In a room equipped with synchronized mul-tichannel audio,video and motion-tracking record-ing devices, participants (from 5 to 8 civilian, mil-itary, or mixed) engage in planning exercises, suchas managing rocket launch emergency, exploring aforeign weapon component, and collaborating to se-lect awardees for fellowships.
we have collected andcontinued to do multichannel time synchronized au-dio and video recordings.
Using a series of audioand video processing techniques, we obtain the wordtranscriptions and prosodic features, as well as head,torso and hand 3D tracking traces from visual track-ers and Vicon motion capture device.
Figure 1 de-picts our meeting corpus collection process.2.2 Gesture Patterns during Speech RepairsIn the dynamic speech production process, speak-ers may make errors or totally change the contentof what is being expressed.
In either of these cases,speakers need refocus or revise what they are saying212and therefore speech repairs appear in overt speech.A typical speech repair contains a reparandum, anoptional editing phrase, and a correction.
Basedon the relationship between the reparandum and thecorrection, speech repairs can be classified into threetypes: repetitions, content replacements, and falsestarts.
Since utterance content has been modifiedin last two repair types, we call them content mod-ification (CM) repairs.
We carried out a measure-ment study (Chen et al, 2002) to identify patterns ofgestures that co-occur with speech repairs that canbe exploited by a multimodal processing system tomore effectively process spontaneous speech.
Weobserved that modification gestures (MGs), whichexhibit a change in gesture state during speech re-pair, have a high correlation with content modifica-tion (CM) speech repairs, but rarely occur with con-tent repetitions.
This study does not only provide ev-idence that gesture and speech are tightly linked inproduction, but also provides evidence that gesturesprovide an important additional cue for identifyingspeech repairs and their types.2.3 Incorporating Gesture in SU DetectionA sentence unit (SU) is defined as the complete ex-pression of a speaker?s thought or idea.
It can be ei-ther a complete sentence or a semantically completesmaller unit.
We have conducted an experiment thatintegrates lexical, prosodic and gestural cues in or-der to more effectively detect sentence unit bound-aries in conversational dialog (Chen et al, 2004b).As can be seen in Figure 2, our multimodal modelcombines lexical, prosodic, and gestural knowl-edge sources, with each knowledge source imple-mented as a separate model.
A hidden event lan-guage model (LM) was trained to serve as lexicalmodel (P (W,E)).
Using a direct modeling ap-proach (Shriberg and Stolcke, 2004), prosodic fea-tures were extracted using the SRI prosodic fea-ture extraction tool1 by collaborators at ICSI andthen were used to train a CART decision tree as theprosodic model (P (E|F )).
Similarly to the prosodicmodel, we computed gesture features directly fromvisual tracking measurements (Quek et al, 1999;Bryll et al, 2001): 3D hand position, Hold (a statewhen there is no hand motion beyond some adaptive1A similar prosody feature extraction tool has been devel-oped in our lab (Huang et al, 2006) using Praat.threshold results), and Effort (analogous to the ki-netic energy of hand movement).
Using gestural fea-tures, we trained a CART tree to serve as the gestu-ral model (P (E|G)).
Finally, an HMM based modelcombination scheme was used to integrate predic-tions from individual models to obtain an overall SUprediction (argmax(E|W,F,G)).
In our investiga-tions, we found that gesture features complement theprosodic and lexical knowledge sources; by usingall of the knowledge sources, the model is able toachieve the lowest overall detection error rate.Figure 2: Data flow diagram of multimodal SUmodel using lexical, prosodic and gestural cues2.4 Floor Control Investigation on MeetingsAn underlying, auto-regulatory mechanism knownas ?floor control?, allows participants communicatewith each other coherently and smoothly.
A personcontrolling the floor bears the burden of moving thediscourse along.
By increasing our understanding offloor control in meetings, there is a potential to im-pact two active research areas: human-like conver-sational agent design and automatic meeting analy-sis.
We have recently investigated floor control inmulti-party meetings (Chen et al, 2006).
In particu-lar, we analyzed patterns of speech (e.g., the use ofdiscourse markers) and visual cues (e.g., eye gazeexchange, pointing gesture for next speaker) that areoften involved in floor control changes.
From thisanalysis, we identified some multimodal cues thatwill be helpful for predicting floor control events.Discourse markers are found to occur frequently atthe beginning of a floor.
During floor transitions, the213previous holder often gazes at the next floor holderand vice verse.
The well-known mutual gaze breakpattern in dyadic conversations is also found in somemeetings.
A special participant, an active meetingmanager, is found to play a role in floor transitions.Gesture cues are also found to play a role, especiallywith respect to floor capturing gestures.3 Research DirectionsIn the next stage of my research, I will focus on inte-grating previous efforts into a complete multimodalmodel for structural event detection.
In particular, Iwill improve current gesture feature extraction, andexpand the non-verbal features to include both eyegaze and body posture.
I will also investigate alter-native integration architectures to the HMM shownin Figure 2.
In my thesis, I hope to better understandthe role that the non-verbal cues play in assistingstructural event detection.
My research is expectedto support adding multimodal perception capabili-ties to current human communication systems thatrely mostly on speech.
I am also interested in inves-tigating mutual impacts among the structural events.For example, we will study SUs and their relation-ship to floor control structure.
Given progress instructural event detection in human communication,I also plan to utilize the detected structural eventsto further enhance meeting understanding.
A par-ticularly interesting task is to locate salient portionsof a meeting from multimodal cues (Chen, 2005) tosummarize it.ReferencesM.
Argyle and M. Cook.
1976.
Gaze and Mutual Gaze.Cambridge Univ.
Press.R.
Bryll, F. Quek, and A. Esposito.
2001.
Automatichand hold detection in natural conversation.
In IEEEWorkshop on Cues in Communication, Kauai,Hawaii,Dec.J.
Cassell and M. Stone.
1999.
Living Hand to Mouth:Psychological Theories about Speech and Gesture inInteractive Dialogue Systems.
In AAAI.L.
Chen, M. Harper, and F. Quek.
2002.
Gesture pat-terns during speech repairs.
In Proc.
of Int.
Conf.
onMultimodal Interface (ICMI), Pittsburg, PA, Oct.L.
Chen, Y. Liu, M. Harper, E. Maia, and S. McRoy.2004a.
Evaluating factors impacting the accuracy offorced alignments in a multimodal corpus.
In Proc.
ofLanguage Resource and Evaluation Conference, Lis-bon, Portugal, June.L.
Chen, Y. Liu, M. Harper, and E. Shriberg.
2004b.Multimodal model integration for sentence unit detec-tion.
In Proc.
of Int.
Conf.
on Multimodal Interface(ICMI), University Park, PA, Oct.L.
Chen, T.R.
Rose, F. Parrill, X. Han, J. Tu, Z.Q.
Huang,I.
Kimbara, H. Welji, M. Harper, F. Quek, D. McNeill,S.
Duncan, R. Tuttle, and T. Huang.
2005.
VACEmultimodal meeting corpus.
In Proceeding of MLMI2005 Workshop.L.
Chen, M. Harper, A. Franklin, T. R. Rose, I. Kimbara,Z.
Q. Huang, and F. Quek.
2006.
A multimodal anal-ysis of floor control in meetings.
In Proc.
of MLMI 06,Washington, DC, USA, May.L.
Chen.
2005.
Locating salient portions of meeting us-ing multimodal cues.
Research proposal submitted toAMI training program, Dec.Z.
Q. Huang, L. Chen, and M. Harper.
2006.
An opensource prosodic feature extraction tool.
In Proc.
ofLanguage Resource and Evaluation Conference, May2006.Y.
Liu, E. Shriberg, A. Stolcke, B. Peskin, J. Ang, HillardD., M. Ostendorf, M. Tomalin, P. Woodland, andM.
Harper.
2005.
Structural Metadata Research inthe EARS Program.
In Proc.
of ICASSP.Y.
Liu.
2004.
Structural Event Detection for Rich Tran-scription of Speech.
Ph.D. thesis, Purdue University.D.
McNeill.
1992.
Hand and Mind: What Gestures Re-veal about Thought.
Univ.
Chicago Press.D.
G. Novick.
2005.
Models of gaze in multi-party dis-course.
In Proc.
of CHI 2005 Workshop on the Virtu-ality Continuum Revisted, Portland OR, April 3.F.
Quek and et al KDI: Cross-model AnalysisSignal and Sense- Data and Computational Re-sources for Gesture, Speech and Gaze Research,http://vislab.cs.vt.edu/kdi.F.
Quek, R. Bryll, and X. F. Ma.
1999.
A parallel algo-righm for dynamic gesture tracking.
In ICCV Work-shop on RATFG-RTS, Gorfu,Greece.F.
Quek, D. McNeill, R. Bryll, S. Duncan, X. Ma, C. Kir-bas, K. E. McCullough, and R. Ansari.
2002.
Mul-timodal human discourse: gesture and speech.
ACMTrans.
Comput.-Hum.
Interact., 9(3):171?193.E.
Shriberg and A. Stolcke.
2004.
Direct modeling ofprosody: An overview of applications in automaticspeech processing.
In International Conference onSpeech Prosody.214
