Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 195?199,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsEnhancing Authorship Attribution By Utilizing Syntax Tree ProfilesMichael Tschuggnall and G?unther SpechtInstitute of Computer Science, University of InnsbruckTechnikerstra?e 21a, 6020 Innsbruck, Austria{michael.tschuggnall, guenther.specht}@uibk.ac.atAbstractThe aim of modern authorship attributionapproaches is to analyze known authorsand to assign authorships to previously un-seen and unlabeled text documents basedon various features.
In this paper wepresent a novel feature to enhance cur-rent attribution methods by analyzing thegrammar of authors.
To extract the fea-ture, a syntax tree of each sentence of adocument is calculated, which is then splitup into length-independent patterns usingpq-grams.
The mostly used pq-grams arethen used to compose sample profiles ofauthors that are compared with the pro-file of the unlabeled document by utiliz-ing various distance metrics and similarityscores.
An evaluation using three differentand independent data sets reveals promis-ing results and indicate that the grammarof authors is a significant feature to en-hance modern authorship attribution meth-ods.1 IntroductionThe increasing amount of documents availablefrom sources like publicly available literarydatabases often raises the question of verifyingdisputed authorships or assigning authors to un-labeled text fragments.
The original problemwas initiated already in the midst of the twenti-eth century by Mosteller and Wallace, who triedto find the correct authorships of The FederalistPapers (Mosteller and Wallace, 1964), nonethe-less authorship attribution is still a major researchtopic.
Especially with latest events in politics andacademia, the verification of authorships becomesincreasingly important and is used frequently inareas like juridical applications (Forensic Linguis-tics) or cybercrime detection (Nirkhi and Dha-raskar, 2013).
Similarily to works in the fieldof plagiarism detection (e.g.
(Stamatatos, 2009;Tschuggnall and Specht, 2013b)) which aim tofind text fragments not written but claimed to bewritten by an author, the problem of traditionalauthorship attribution is defined as follows: Givenseveral authors with text samples for each of them,the question is to label an unknown documentwith the correct author.
In contrast to this so-called closed-class problem, an even harder taskis addressed in the open-class problem, whereadditionally a ?none-of-them?-answer is allowed(Juola, 2006).In this paper we present a novel feature for the tra-ditional, closed-class authorship attribution task,following the assumption that different authorshave different writing styles in terms of the gram-mar structure that is used mostly unconsciously.Due to the fact that an author has many differ-ent choices of how to formulate a sentence us-ing the existing grammar rules of a natural lan-guage, the assumption is that the way of construct-ing sentences is significantly different for individ-ual authors.
For example, the famous Shakespearequote ?To be, or not to be: that is the question.?
(S1) could also be formulated as ?The question iswhether to be or not to be.?
(S2) or even ?Thequestion is whether to be or not.?
(S3) which is se-mantically equivalent but differs significantly ac-cording to the syntax (see Figure 1).
The main ideaof this approach is to quantify those differencesby calculating grammar profiles for each candidateauthor as well as for the unlabeled document, andto assign one of the candidates as the author of theunseen document by comparing the profiles.
Toquantify the differences between profiles multiplemetrics have been implemented and evaluated.The rest of this paper is organized as follows: Sec-tion 2 sketches the main idea of the algorithmwhich incorporates the distance metrics explainedin detail in Section 3.
An extensive evaluation us-195S: SVB(be)SVPVPCC(or)RB(not)VPVPTO(To)VB(be)VPTO(to)NP VPDT(that)RBZ(is)NPDT(the)NN(question)SNP VPDT(The)NN(question)VBZ(is)SBARIN(whether)SS CC(or)SVB(be)VPVPTO(to)VB(be)VPVPTO(to)RB(S1) (S2)SNP VPDT(The)NN(question)VBZ(is)SBARIN(whether)SVPVPTO(to)VB(be)NPQPCC(or)RB(not)(S3)Figure 1: Syntax Trees Resulting From Parsing Sentence (S1), (S2) and (S3).ing three different test sets is shown in Section 4,while finally Section 5 and Section 6 summarizerelated work and discuss future work, respectively.2 Syntax Tree ProfilesThe basic idea of the approach is to utilize the syn-tax that is used by authors to distinguish author-ships of text documents.
Based on our previouswork in the field of intrinsic plagiarism detection(Tschuggnall and Specht, 2013c; Tschuggnall andSpecht, 2013a) we modify and enhance the algo-rithms and apply them to be used in closed-classauthorship attribution.The number of choices an author has to for-mulate a sentence in terms of grammar is ratherhigh, and the assumption in this approach is thatthe concrete choice is made mostly intuitively andunconsciously.
Evaluations shown in Section 4 re-inforce that solely parse tree structures represent asignificant feature that can be used to distinguishbetween authors.From a global view the approach comprises thefollowing three steps: (A) Creating a grammar pro-file for each author, (B) creating a grammar profilefor the unlabeled document, and (C) calculatingthe distance between each author profile and thedocument profile and assigning the author havingthe lowest distance (or the highest similarity, de-pending on the distance metric chosen).
As thisapproach is based on profiles a key criterion is thecreation of distinguishable author profiles.
In or-der to calculate a grammar profile for an authoror a document, the following procedure is applied:(1) Concatenate all text samples for the author intoa single, large sample document, (2) split the re-sulting document into single sentences and calcu-late a syntax tree for each sentence, (3) calculatethe pq-gram index for each tree, and (4) composethe final grammar profile from the normalized fre-quencies of pq-grams.At first the concatenated document is cleaned tocontain alphanumeric characters and punctuationmarks only, and then split into single sentences1.Each sentence is then parsed2.
For example, Fig-ure 1 depicts the syntax trees resulting from sen-tences (S1), (S2) and (S3).
The labels of each treecorrespond to a Penn Treebank tag (Marcus et al.,1993), where e.g NP corresponds to a noun phraseor JJS to a superlative adjective.
In order to exam-ine solely the structure of sentences, the terminalnodes (words) are ignored.Having computed a syntax tree for every sentence,the pq-gram index (Augsten et al., 2010) of eachtree is calculated in the next step.
Pq-grams con-sist of a stem (p) and a base (q) and may be re-lated to as ?n-grams for trees?.
Thereby p defineshow much nodes are included vertically, and q de-fines the number of nodes to be considered hor-izontally.
For example, a pq-gram using p = 2and q = 3 starting from level two of tree (S1)would be [S-VP-VP-CC-RB].
In order to ob-tain all pq-grams of a tree, the base is addition-ally shifted left and right: If then less than pnodes exist horizontally, the corresponding placein the pq-gram is filled with*, indicating a miss-ing node.
Applying this idea to the previous exam-ple, also the pq-grams [S-VP-*-*-VP] (baseshifted left by two), [S-VP-*-VP-CC] (baseshifted left by one), [S-VP-RB-VP-*] (baseshifted right by one) and [S-VP-VP-*-*] (baseshifted right by two) have to be considered.
Fi-nally, the pq-gram index contains all pq-grams of1using OpenNLP, http://incubator.apache.org/opennlp,visited October 20132using the Stanford Parser (Klein and Manning, 2003)196a syntax tree, whereby multiple occurences of thesame pq-grams are also present multiple times inthe index.The remaining part for creating the author profileis to compute the pq-gram index of the wholedocument by combining all pq-gram indexes of allsentences.
In this step the number of occurencesis counted for each pq-gram and then normalizedby the total number of all appearing pq-grams.
Asan example, the three mostly used pq-grams ofa selected document together with their normal-ized frequencies are {[NP-NN-*-*-*],2.7%}, {[PP-IN-*-*-*], 2.3%}, and{[S-VP-*-*-VBD], 1.1%}.
The final pq-gram profile then consists of the complete tableof pq-grams and their occurences in the givendocument.3 Distance and Similarity MetricsWith the use of the syntax tree profiles calculatedfor each candidate author as well as for the unla-beled document, the last part is to calculate a dis-tance or similarity, respectively, for every authorprofile.
Finally, the unseen document is simply la-beled with the author of the best matching profile.To investigate on the best distance or simi-larity metric to be used for this approach, sev-eral metrics for this problem have been adaptedand evaluated3: 1.
CNG (Ke?selj et al., 2003),2.
Stamatatos-CNG (Stamatatos, 2009), 3.Stamatatos-CNG with Corpus Norm (Stamatatos,2007), 4.
Sentence-SPI.For the latter, we modified the original SPI score(Frantzeskou et al., 2006) so that each sentenceis traversed separately: Let SDbe the set of sen-tences of the document, I(s) the pq-gram-index ofsentence s and Pxthe profile of author X , then theSentence-SPI score is calculated as follows:sPx,PD=?s?SD?p?I(s){1 if p ?
Px0 else4 EvaluationThe approach described in this paper has been ex-tensively evaluated using three different Englishdata sets, whereby all sets are completely unre-lated and of different types: (1.)
CC04: the train-ing set used for the Ad-hoc-Authorship Attribution3The algorithm names are only used as a reference forthis paper, but were not originally proposed like thatCompetition workshop held in 20044- type: nov-els, authors: 4, documents: 8, samples per author:1; (2.)
FED: the (undisputed) federalist paperswritten by Hamilton, Madison and Jay in the 18thcentury - type: political essays, authors: 3, doc-uments: 61, samples per author: 3; (3.)
PAN12:from the state-of-the-art corpus, especially createdfor the use in authorship identification for the PAN2012 workshop5(Juola, 2012), all closed-classedproblems have been chosen - type: misc, authors:3-16, documents: 6-16, samples per author: 2.For the evaluation, each of the sets has been usedto optimize parameters while the remaining setshave been used for testing.
Besides examining thediscussed metrics and values for p and q (e.g.
bychoosing p = 1 and q = 0 the pq-grams of a gram-mar profile are equal to pure POS tags), two addi-tional optimization variables have been integratedfor the similarity metric Sentence-SPI:?
topPQGramCount tc: by assigning a valueto this parameter, only the correspondingamount of mostly used pq-grams of a gram-mar profile are used.?
topPQGramOffset to: based on the idea thatall authors might have a frequently used andcommon set of syntax rules that are prede-fined by a specific language, this parameterallows to ignore the given amount of mostlyused pq-grams.
For example if to= 3 in Ta-ble 1, the first pq-gram to be used would be[NP-NNP-*-*-*].The evaluation results are depicted in Table 1.
Itshows the rate of correct author attributions basedon the grammar feature presented in this paper.Generally, the algorithm worked best using theSentence-SPI score, which led to a rate of 72% byusing the PAN12 data set for optimization.
Theoptimal configuration uses p = 3 and q = 2,which is the same configuration that was used in(Augsten et al., 2010) to produce the best results.The highest scores are gained by using a limit oftop pq-grams (tc?
65) and by ignoring the firstthree pq-grams (to= 3), which indicates that it issufficient to limit the number of syntax structures4http://www.mathcs.duq.edu/?juola/authorship contest.html,visited Oct. 20135PAN is a well-known workshop on UncoveringPlagiarism, Authorship, and Social Software Misuses.http://pan.webis.de, visited Oct. 2013197metric p q Optimized With CC04 FED PAN12 OverallSentence-SPI (tc= 65, to= 3) 3 2 PAN12 57.14 86.89 (76.04) 72.02CNG 0 2 PAN12 14.29 80.33 (57.29) 47.31Stamatatos-CNG 2 2 PAN12 14.29 78.69 (60.42) 46.49Stamatatos-CNG-CN 0 2 CC04 (42.86) 52.46 18.75 35.61Table 1: Evaluation Results.and that there exists a certain number (3) of gen-eral grammar rules for English which are used byall authors.
I.e.
those rules cannot by used to inferinformation about individual authors (e.g.
everysentence starts with [S-...]).All other metrics led to worse results, whichmay also be a result of the fact that only theSentence-SPI metric makes use of the additionalparameters tcand to.
Future work should also in-vestigate on integrating these parameters also inother metrics.
Moreover, results are better usingthe PAN12 data set for optimization, which maybe because this set is the most hetergeneous one:The Federalist Papers contain only political essayswritten some time ago, and the CC04 set only usesliterary texts written by four authors.5 Related WorkSuccessful current approaches often are based onor include character n-grams (e.g.
(Hirst andFeiguina, 2007; Stamatatos, 2009)).
Several stud-ies have shown that n-grams represent a significantfeature to identify authors, whereby the major ben-efits are the language independency as well as theeasy computation.
As a variation, word n-gramsare used in (Balaguer, 2009) to detect plagiarismin text documents.Using individual features, machine learning al-gorithms are often applied to learn from au-thor profiles and to predict unlabeled documents.Among methods that are utilized in authorship at-tribution as well as the related problem classes liketext categorization or intrinsic plagiarism detec-tion are support vector machines (e.g.
(Sandersonand Guenter, 2006; Diederich et al., 2000)), neuralnetworks (e.g.
(Tweedie et al., 1996)), naive bayesclassifiers (e.g.
(McCallum and Nigam, 1998)) ordecision trees (e.g.
(?O.
Uzuner et.
al, 2005)).Another interesting approach used in authorshipattribution that tries to detect the writing style ofauthors by analyzing the occurences and varia-tions of spelling errors is proposed in (Koppel andSchler, 2003).
It is based on the assumption thatauthors tend to make similar spelling and/or gram-mar errors and therefore uses this information toattribute authors to unseen text documents.Approaches in the field of genre categorizationalso use NLP tools to analyze documents basedon syntactic annotations (Stamatatos et al., 2000).Lexicalized tree-adjoining-grammars (LTAG) arepoposed in (Joshi and Schabes, 1997) as a rulesetto construct and analyze grammar syntax by usingpartial subtrees.6 Conclusion and Future WorkIn this paper we propose a new feature to enhancemodern authorship attribution algorithms by uti-lizing the grammar syntax of authors.
To distin-guish between authors, syntax trees of sentencesare calculated which are split into parts by usingpq-grams.
The set of pq-grams is then stored in anauthor profile that is used to assign unseen docu-ments to known authors.The algorithm has been optimized and evalu-ated using three different data sets, resulting inan overall attribution rate of 72%.
As the workin this paper solely used the grammar feature andcompletely ignores information like the vocabu-lary richness or n-grams, the evaluation results arepromising.
Future work should therefore concen-trate on integrating other well-known and good-working features as well as considering commonmachine-learning techniques like support vectormachines or decision trees to predict authors basedon pq-gram features.
Furthermore, the optimiza-tion parameters currently only applied on the si-miliarity score should also be integrated with thedistance metrics as they led to the best results.
Re-search should finally also be done on the appli-cability to other languages, especially as syntac-tically more complex languages like German orFrench may lead to better results due to the higheramount of grammar rules, making the writing styleof authors more unique.198ReferencesNikolaus Augsten, Michael B?ohlen, and Johann Gam-per.
2010.
The pq-Gram Distance between OrderedLabeled Trees.
ACM Transactions on Database Sys-tems (TODS).Enrique Vall?es Balaguer.
2009.
Putting Ourselves inSME?s Shoes: Automatic Detection of Plagiarismby the WCopyFind tool.
In Proceedings of the SE-PLN?09 Workshop on Uncovering Plagiarism, Au-thorship and Social Software Misuse, pages 34?35.Joachim Diederich, J?org Kindermann, Edda Leopold,and Gerhard Paass.
2000.
Authorship attributionwith support vector machines.
APPLIED INTELLI-GENCE, 19:2003.Georgia Frantzeskou, Efstathios Stamatatos, StefanosGritzalis, and Sokratis Katsikas.
2006.
Effectiveidentification of source code authors using byte-level information.
In Proceedings of the 28th inter-national conference on Software engineering, pages893?896.
ACM.Graeme Hirst and Ol?ga Feiguina.
2007.
Bigramsof syntactic labels for authorship discrimination ofshort texts.
Literary and Linguistic Computing,22(4):405?417.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In Handbook of formal lan-guages, pages 69?123.
Springer.Patrick Juola.
2006.
Authorship attribution.
Founda-tions and Trends in Information Retrieval, 1(3):233?334.Patrick Juola.
2012.
An overview of the traditional au-thorship attribution subtask.
In CLEF (Online Work-ing Notes/Labs/Workshop).Vlado Ke?selj, Fuchun Peng, Nick Cercone, and CalvinThomas.
2003.
N-gram-based author profiles forauthorship attribution.
In Proceedings of the confer-ence pacific association for computational linguis-tics, PACLING, volume 3, pages 255?264.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430, Stroudsburg, PA, USA.Moshe Koppel and Jonathan Schler.
2003.
Exploit-ing Stylistic Idiosyncrasies for Authorship Attribu-tion.
In IJCAI?03 Workshop On Computational Ap-proaches To Style Analysis And Synthesis, pages 69?72.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: The Penn Treebank.
Com-putational Linguistics, 19:313?330, June.Andrew McCallum and Kamal Nigam.
1998.
A com-parison of event models for naive bayes text classifi-cation.F.
Mosteller and D. Wallace.
1964.
Inference and Dis-puted Authorship: The Federalist.
Addison-Wesley.Smita Nirkhi and RV Dharaskar.
2013.
Comparativestudy of authorship identification techniques for cy-ber forensics analysis.
International Journal.?O.
Uzuner et.
al.
2005.
Using Syntactic Informationto Identify Plagiarism.
In Proc.
2nd Workshop onBuilding Educational Applications using NLP.Conrad Sanderson and Simon Guenter.
2006.
Shorttext authorship attribution via sequence kernels,markov chains and author unmasking: an investiga-tion.
In Proc.
of the 2006 Conference on Empiri-cal Methods in Natural Language Processing, pages482?491, Stroudsburg, PA, USA.Efstathios Stamatatos, George Kokkinakis, and NikosFakotakis.
2000.
Automatic text categorizationin terms of genre and author.
Comput.
Linguist.,26:471?495, December.Efstathios Stamatatos.
2007.
Author identificationusing imbalanced and limited training texts.
InDatabase and Expert Systems Applications, 2007.DEXA?07.
18th International Workshop on, pages237?241.
IEEE.Efstathios Stamatatos.
2009.
Intrinsic Plagiarism De-tection Using Character n-gram Profiles.
In CLEF(Notebook Papers/Labs/Workshop).Michael Tschuggnall and G?unther Specht.
2013a.Countering Plagiarism by Exposing Irregularities inAuthors Grammars.
In EISIC, European Intelli-gence and Security Informatics Conference, Upp-sala, Sweden, pages 15?22.Michael Tschuggnall and G?unther Specht.
2013b.Detecting Plagiarism in Text Documents throughGrammar-Analysis of Authors.
In 15.
GI-Fachtagung Datenbanksysteme f?ur Business, Tech-nologie und Web, Magdeburg, Germany.Michael Tschuggnall and G?unther Specht.
2013c.
Us-ing grammar-profiles to intrinsically expose plagia-rism in text documents.
In NLDB, pages 297?302.Fiona J. Tweedie, S. Singh, and David I. Holmes.1996.
Neural network applications in stylometry:The federalist papers.
Computers and the Humani-ties, 30(1):1?10.199
