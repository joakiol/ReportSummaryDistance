Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 337?340,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPA Note on the Implementation ofHierarchical Dirichlet ProcessesPhil Blunsom?pblunsom@inf.ed.ac.ukSharon Goldwater?sgwater@inf.ed.ac.ukTrevor Cohn?tcohn@inf.ed.ac.ukMark Johnson?mark johnson@brown.edu?Department of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UK?Department of Cognitive and Linguistic SciencesBrown UniversityProvidence, RI, USAAbstractThe implementation of collapsed Gibbssamplers for non-parametric Bayesianmodels is non-trivial, requiring con-siderable book-keeping.
Goldwater etal.
(2006a) presented an approximationwhich significantly reduces the storageand computation overhead, but we showhere that their formulation was incorrectand, even after correction, is grossly inac-curate.
We present an alternative formula-tion which is exact and can be computedeasily.
However this approach does notwork for hierarchical models, for whichcase we present an efficient data structurewhich has a better space complexity thanthe naive approach.1 IntroductionUnsupervised learning of natural language is oneof the most challenging areas in NLP.
Recently,methods from nonparametric Bayesian statisticshave been gaining popularity as a way to approachunsupervised learning for a variety of tasks,including language modeling, word and mor-pheme segmentation, parsing, and machine trans-lation (Teh et al, 2006; Goldwater et al, 2006a;Goldwater et al, 2006b; Liang et al, 2007; Finkelet al, 2007; DeNero et al, 2008).
These mod-els are often based on the Dirichlet process (DP)(Ferguson, 1973) or hierarchical Dirichlet process(HDP) (Teh et al, 2006), with Gibbs samplingas a method of inference.
Exact implementationof such sampling methods requires considerablebookkeeping of various counts, which motivatedGoldwater et al (2006a) (henceforth, GGJ06) todevelop an approximation using expected counts.However, we show here that their approximationis flawed in two respects: 1) It omits an impor-tant factor in the expectation, and 2) Even aftercorrection, the approximation is poor for hierar-chical models, which are commonly used for NLPapplications.
We derive an improvedO(1) formulathat gives exact values for the expected counts innon-hierarchical models.
For hierarchical models,where our formula is not exact, we present anefficient method for sampling from the HDP (andrelated models, such as the hierarchical Pitman-Yor process) that considerably decreases the mem-ory footprint of such models as compared to thenaive implementation.As we have noted, the issues described in thispaper apply to models for various kinds of NLPtasks; for concreteness, we will focus on n-gramlanguage modeling for the remainder of the paper,closely following the presentation in GGJ06.2 The Chinese Restaurant ProcessGGJ06 present two nonparametric Bayesian lan-guage models: a DP unigram model and an HDPbigram model.
Under the DP model, words in acorpus w = w1.
.
.
wnare generated as follows:G|?0, P0?
DP(?0, P0)wi|G ?
Gwhere G is a distribution over an infinite set ofpossible words, P0(the base distribution of theDP) determines the probability that an item willbe in the support of G, and ?0(the concentrationparameter) determines the variance of G.One way of understanding the predictions thatthe DP model makes is through the Chinese restau-rant process (CRP) (Aldous, 1985).
In the CRP,customers (word tokenswi) enter a restaurant withan infinite number of tables and choose a seat.
Thetable chosen by the ith customer, zi, follows thedistribution:P (zi= k|z?i) ={nz?iki?1+?0, 0 ?
k < K(z?i)?0i?1+?0, k = K(z?i)337The1meow4cats2cats3cats5abcde fghFigure 1.
A seating assignment describing the state ofa unigram CRP.
Letters and numbers uniquely identifycustomers and tables.
Note that multiple tables mayshare a label.where z?i= z1.
.
.
zi?1are the table assignmentsof the previous customers, nz?ikis the number ofcustomers at table k in z?i, andK(z?i) is the totalnumber of occupied tables.
If we further assumethat table k is labeled with a word type `kdrawnfrom P0, then the assignment of tokens to tablesdefines a distribution over words, with wi= `zi.See Figure 1 for an example seating arrangement.Using this model, the predictive probability ofwi, conditioned on the previous words, can befound by summing over possible seating assign-ments for wi, and is given byP (wi= w|w?i) =nw?iw+ ?0P0i?
1 + ?0(1)This prediction turns out to be exactly that of theDP model after integrating out the distribution G.Note that as long as the base distribution P0isfixed, predictions do not depend on the seatingarrangement z?i, only on the count of word win the previously observed words (nw?iw).
How-ever, in many situations, we may wish to estimatethe base distribution itself, creating a hierarchicalmodel.
Since the base distribution generates tablelabels, estimates of this distribution are based onthe counts of those labels, i.e., the number of tablesassociated with each word type.An example of such a hierarchical model is theHDP bigram model of GGJ06, in which each wordtypew is associated with its own restaurant, wherecustomers in that restaurant correspond to wordsthat follow w in the corpus.
All the bigram restau-rants share a common base distribution P1overunigrams, which must be inferred.
Predictions inthis model are as follows:P2(wi|h?i) =nh?i(wi?1,wi)+ ?1P1(wi|h?i)nh?i(wi?1,?
)+ ?1P1(wi|h?i) =th?iwi+ ?0P0(wi)th?i?+ ?0(2)where h?i= (w?i, z?i), th?iwiis the number oftables labelled with wi, and th?i?is the total num-ber of occupied tables.
Of particular note for ourdiscussion is that in order to calculate these condi-tional distributions we must know the table assign-ments z?ifor each of the words in w?i.
Moreover,in the Gibbs samplers often used for inference in1         10         100         10000.1110100Meannumberoflexical entriesWord frequency (nw)ExpectationAntoniak approx.Empirical, fixed baseEmpirical, inferred baseFigure 2.
Comparison of several methods of approx-imating the number of tables occupied by words ofdifferent frequencies.
For each method, results using?
= {100, 1000, 10000, 100000} are shown (from bottomto top).
Solid lines show the expected number of tables,computed using (3) and assuming P1is a fixed uni-form distribution over a finite vocabulary (values com-puted using the Digamma formulation (7) are the same).Dashed lines show the values given by the Antoniakapproximation (4) (the line for ?
= 100 falls below thebottom of the graph).
Stars show the mean of empiricaltable counts as computed over 1000 samples from anMCMC sampler in which P1is a fixed uniform distri-bution, as in the unigram LM.
Circles show the meanof empirical table counts when P1is inferred, as in thebigram LM.
Standard errors in both cases are no largerthan the marker size.
All plots are based on the 30114-word vocabulary and frequencies found in sections 0-20of the WSJ corpus.these kinds of models, the counts are constantlychanging over multiple samples, with tables goingin and out of existence frequently.
This can createsignificant bookkeeping issues in implementation,and motivated GGJ06 to present a method of com-puting approximate table counts based on wordfrequencies only.3 Approximating Table CountsRather than explicitly tracking the number oftables twassociated with each word w in theirbigram model, GGJ06 approximate the tablecounts using the expectation E[tw].
Expectedcounts are used in place of th?iwiand th?i?in (2).The exact expectation, due to Antoniak (1974), isE[tw] = ?1P1(w)nw?i=11?1P1(w) + i?
1(3)338Antoniak also gives an approximation to thisexpectation:E[tw] ?
?1P1(w) lognw+ ?1P1(w)?1P1(w)(4)but provides no derivation.
Due to a misinterpre-tation of Antoniak (1974), GGJ06 use an approx-imation that leaves out all the P1(w) terms from(4).1Figure 2 compares the approximation tothe exact expectation when the base distributionis fixed.
The approximation is fairly good when?P1(w) > 1 (the scenario assumed by Antoniak);however, in most NLP applications, ?P1(w) <1 in order to effect a sparse prior.
(We returnto the case of non-fixed based distributions in amoment.)
As an extreme case of the paucity ofthis approximation consider ?1P1(w) = 1 andnw= 1 (i.e.
only one customer has entered therestaurant): clearly E[tw] should equal 1, but theapproximation gives log(2).We now provide a derivation for (4), which willallow us to obtain an O(1) formula for the expec-tation in (3).
First, we rewrite the summation in (3)as a difference of fractional harmonic numbers:2H(?1P1(w)+nw?1)?H(?1P1(w)?1)(5)Using the recurrence for harmonic numbers:E[tw] ?
?1P1(w)[H(?1P1(w)+nw)?1?1P1(w) + nw?H(?1P1(w)+nw)+1?1P1(w)](6)We then use the asymptotic expansion,HF?
logF + ?
+12F, omiting trailing termswhich are O(F?2) and smaller powers of F :3E[tw] ?
?1P1(w) lognw+?1P1(w)?1P1(w)+nw2(?1P1(w)+nw)Omitting the trailing term leads to theapproximation in Antoniak (1974).
However, wecan obtain an exact formula for the expecta-tion by utilising the relationship between theDigamma function and the harmonic numbers:?
(n) = Hn?1?
?.4Thus we can rewrite (5) as:5E[tw] = ?1P1(w)?[?
(?1P1(w) + nw)?
?
(?1P1(w))](7)1The authors of GGJ06 realized this error, and currentimplementations of their models no longer use these approx-imations, instead tracking table counts explicitly.2Fractional harmonic numbers between 0 and 1 are givenby HF=R101?xF1?xdx.
All harmonic numbers follow therecurrence HF= HF?1+1F.3Here, ?
is the Euler-Mascheroni constant.4AccurateO(1) approximations of the Digamma functionare readily available.5(7) can be derived from (3) using: ?(x+1)??
(x) =1x.Explicit table tracking:customer(wi)?
table(zi)na : 1, b : 1, c : 2, d : 2, e : 3, f : 4, g : 5, h : 5otable(zi)?
label(`)n1 : The, 2 : cats, 3 : cats, 4 : meow, 5 : catsoHistogram:word type?
{table occupancy?
frequency}nThe : {2 : 1}, cats : {1 : 1, 2 : 2}, meow : {1 : 1}oFigure 3.
The explicit table tracking and histogram rep-resentations for Figure 1.A significant caveat here is that the expectedtable counts given by (3) and (7) are only validwhen the base distribution is a constant.
However,in hierarchical models such as GGJ06?s bigrammodel and HDP models, the base distribution isnot constant and instead must be inferred.
As canbe seen in Figure 2, table counts can diverge con-siderably from the expectations based on fixedP1when P1is in fact not fixed.
Thus, (7) canbe viewed as an approximation in this case, butnot necessarily an accurate one.
Since knowingthe table counts is only necessary for inferencein hierarchical models, but the table counts can-not be approximated well by any of the formu-las presented here, we must conclude that the bestinference method is still to keep track of the actualtable counts.
The naive method of doing so is tostore which table each customer in the restaurantis seated at, incrementing and decrementing thesecounts as needed during the sampling process.
Inthe following section, we describe an alternativemethod that reduces the amount of memory neces-sary for implementing HDPs.
This method is alsoappropriate for hierarchical Pitman-Yor processes,for which no closed-form approximations to thetable counts have been proposed.4 Efficient Implementation of HDPsAs we do not have an efficient expected tablecount approximation for hierarchical models wecould fall back to explicitly tracking which tableeach customer that enters the restaurant sits at.However, here we describe a more compact repre-sentation for the state of the restaurant that doesn?trequire explicit table tracking.6Instead we main-tain a histogram for each dish wiof the frequencyof a table having a particular number of customers.Figure 3 depicts the histogram and explicit repre-sentations for the CRP state in Figure 1.Our alternative method of inference for hierar-chical Bayesian models takes advantage of their6Teh et al (2006) also note that the exact table assign-ments for customers are not required for prediction.339Algorithm 1 A new customer enters the restaurant1: w: word type2: Pw0: Base probability for w3: HDw: Seating Histogram for w4: procedure INCREMENT(w,Pw0,HDw)5: pshare?nw?1wnw?1w+?0.
share an existing table6: pnew??0?Pw0nw?1w+?0.
open a new table7: r ?
random(0, pshare+ pnew)8: if r < pnewor nw?1w= 0 then9: HDw[1] = HDw[1] + 110: else.
Sample from the histogram of customers at tables11: r ?
random(0, nw?1w)12: for c ?
HDwdo .
c: customer count13: r = r ?
(c?
HDw[c])14: if r ?
0 then15: HDw[c] = HDw[c] + 116: Break17: nww= nw?1w+ 1 .
Update token countAlgorithm 2 A customer leaves the restaurant1: w: word type2: HDw: Seating histogram for w3: procedure DECREMENT(w,Pw0,HDw)4: r ?
random(0, nww)5: for c ?
HDwdo .
c: customer count6: r = r ?
(c?
HDw[c])7: if r ?
0 then8: HDw[c] = HDw[c]?
19: if c > 1 then10: HDw[c?
1] = HDw[c?
1] + 111: Break12: nww= nww?
1 .
Update token countexchangeability, which makes it unnecessary toknow exactly which table each customer is seatedat.
The only important information is how manytables exist with different numbers of customers,and what their labels are.
We simply maintain ahistogram for each word type w, which stores, foreach number of customersm, the number of tableslabeled with w that have m customers.
Figure 3depicts the explicit representation and histogramfor the CRP state in Figure 1.Algorithms 1 and 2 describe the two operationsrequired to maintain the state of a CRP.7Whena customer enters the restaurant (Alogrithm 1)),we sample whether or not to open a new table.If not, we sample an old table proportional to thecounts of how many customers are seated thereand update the histogram.
When a customer leavesthe restaurant (Algorithm 2), we decrement oneof the tables at random according to the numberof customers seated there.
By exchangeability, itdoesn?t actually matter which table the customerwas ?really?
sitting at.7A C++ template class that implementsthe algorithm presented is made available at:http://homepages.inf.ed.ac.uk/tcohn/5 ConclusionWe?ve shown that the HDP approximation pre-sented in GGJ06 contained errors and inappropri-ate assumptions such that it significantly divergesfrom the true expectations for the most commonscenarios encountered in NLP.
As such we empha-sise that that formulation should not be used.Although (7) allowsE[tw] to be calculated exactlyfor constant base distributions, for hierarchicalmodels this is not valid and no accurate calculationof the expectations has been proposed.
As a rem-edy we?ve presented an algorithm that efficientlyimplements the true HDP without the need forexplicitly tracking customer to table assignments,while remaining simple to implement.AcknowledgementsThe authors would like to thank Tom Grif-fiths for providing the code used to produceFigure 2 and acknowledge the support of theEPSRC (Blunsom, grant EP/D074959/1; Cohn,grant GR/T04557/01).ReferencesD.
Aldous.
1985.
Exchangeability and related topics.
In?Ecole d?
?Et?e de Probabiliti?es de Saint-Flour XIII 1983, 1?198.
Springer.C.
E. Antoniak.
1974.
Mixtures of dirichlet processes withapplications to bayesian nonparametric problems.
TheAnnals of Statistics, 2(6):1152?1174.J.
DeNero, A. Bouchard-C?ot?e, D. Klein.
2008.
Samplingalignment structure under a Bayesian translation model.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, 314?323, Hon-olulu, Hawaii.
Association for Computational Linguistics.S.
Ferguson.
1973.
A Bayesian analysis of some nonpara-metric problems.
Annals of Statistics, 1:209?230.J.
R. Finkel, T. Grenager, C. D. Manning.
2007.
The infinitetree.
In Proc.
of the 45th Annual Meeting of the ACL(ACL-2007), Prague, Czech Republic.S.
Goldwater, T. Griffiths, M. Johnson.
2006a.
Contex-tual dependencies in unsupervised word segmentation.
InProc.
of the 44th Annual Meeting of the ACL and 21stInternational Conference on Computational Linguistics(COLING/ACL-2006), Sydney.S.
Goldwater, T. Griffiths, M. Johnson.
2006b.
Interpolatingbetween types and tokens by estimating power-law gener-ators.
In Y. Weiss, B. Sch?olkopf, J. Platt, eds., Advancesin Neural Information Processing Systems 18, 459?466.MIT Press, Cambridge, MA.P.
Liang, S. Petrov, M. Jordan, D. Klein.
2007.
The infinitePCFG using hierarchical Dirichlet processes.
In Proc.
ofthe 2007 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-2007), 688?697, Prague,Czech Republic.Y.
W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei.
2006.Hierarchical Dirichlet processes.
Journal of the AmericanStatistical Association, 101(476):1566?1581.340
