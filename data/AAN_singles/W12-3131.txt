Proceedings of the 7th Workshop on Statistical Machine Translation, pages 261?266,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsThe CMU-Avenue French-English Translation SystemMichael Denkowski Greg Hanneman Alon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{mdenkows,ghannema,alavie}@cs.cmu.eduAbstractThis paper describes the French-English trans-lation system developed by the Avenue re-search group at Carnegie Mellon Universityfor the Seventh Workshop on Statistical Ma-chine Translation (NAACL WMT12).
Wepresent a method for training data selection,a description of our hierarchical phrase-basedtranslation system, and a discussion of the im-pact of data size on best practice for systembuilding.1 IntroductionWe describe the French-English translation sys-tem constructed by the Avenue research group atCarnegie Mellon University for the shared trans-lation task in the Seventh Workshop on StatisticalMachine Translation.
The core translation systemuses the hierarchical phrase-based model describedby Chiang (2007) with sentence-level grammars ex-tracted and scored using the methods described byLopez (2008).
Improved techniques for data selec-tion and monolingual text processing significantlyimprove the performance of the baseline system.Over half of all parallel data for the French-English track is provided by the Giga-FrEn cor-pus (Callison-Burch et al, 2009).
Assembled fromcrawls of bilingual websites, this corpus is known tobe noisy, containing sentences that are either not par-allel or not natural language.
Rather than simply in-cluding or excluding the resource in its entirety, weuse a relatively simple technique inspired by work inmachine translation quality estimation to select thebest portions of the corpus for inclusion in our train-ing data.
Including around 60% of the Giga-FrEnchosen by this technique yields an improvement of0.7 BLEU.Prior to model estimation, we process all paralleland monolingual data using in-house tokenizationand normalization scripts that detect word bound-aries better than the provided WMT12 scripts.
Aftertranslation, we apply a monolingual rule-based post-processing step to correct obvious errors and makesentences more acceptable to human judges.
Thepost-processing step alone yields an improvement of0.3 BLEU to the final system.We conclude with a discussion of the impact ofdata size on important decisions for system building.Experimental results show that ?best practice?
deci-sions for smaller data sizes do not necessarily carryover to systems built with ?WMT-scale?
data, andprovide some explanation for why this is the case.2 Training DataTraining data provided for the French-English trans-lation task includes parallel corpora taken from Eu-ropean Parliamentary proceedings (Koehn, 2005),news commentary, and United Nations documents.Together, these sets total approximately 13 millionsentences.
In addition, a large, web-crawled parallelcorpus termed the ?Giga-FrEn?
(Callison-Burch etal., 2009) is made available.
While this corpus con-tains over 22 million parallel sentences, it is inher-ently noisy.
Many parallel sentences crawled fromthe web are neither parallel nor sentences.
To makeuse of this large data source, we employ data se-lection techniques discussed in the next subsection.261Corpus SentencesEuroparl 1,857,436News commentary 130,193UN doc 11,684,454Giga-FrEn 1stdev 7,535,699Giga-FrEn 2stdev 5,801,759Total 27,009,541Table 1: Parallel training dataParallel data used to build our final system totals 27million sentences.
Precise figures for the number ofsentences in each data set, including selections fromthe Giga-FrEn, are found in Table 1.2.1 Data Selection as Quality EstimationDrawing inspiration from the workshop?s featuredtask, we cast the problem of data selection as oneof quality estimation.
Specia et al (2009) reportseveral estimators of translation quality, the most ef-fective of which detect difficult-to-translate sourcesentences, ungrammatical translations, and transla-tions that align poorly to their source sentences.
Wecan easily adapt several of these predictive featuresto select good sentence pairs from noisy parallel cor-pora such as the Giga-FrEn.We first pre-process the Giga-FrEn by removinglines with invalid Unicode characters, control char-acters, and insufficient concentrations of Latin char-acters.
We then score each sentence pair in the re-maining set (roughly 90% of the original corpus)with the following features:Source language model: a 4-gram modifiedKneser-Ney smoothed language model trained onFrench Europarl, news commentary, UN doc, andnews crawl corpora.
This model assigns high scoresto grammatical source sentences and lower scores toungrammatical sentences and non-sentences such assite maps, large lists of names, and blog comments.Scores are normalized by number of n-grams scoredper sentence (length + 1).
The model is built usingthe SRILM toolkit (Stolke, 2002).Target language model: a 4-gram modifiedKneser-Ney smoothed language model trained onEnglish Europarl, news commentary, UN doc, andnews crawl corpora.
This model scores grammati-cality on the target side.Word alignment scores: source-target andtarget-source MGIZA++ (Gao and Vogel, 2008)force-alignment scores using IBM Model 4 (Ochand Ney, 2003).
Model parameters are estimatedon 2 million words of French-English Europarl andnews commentary text.
Scores are normalized bythe number of alignment links.
These features mea-sure the extent to which translations are parallel withtheir source sentences.Fraction of aligned words: source-target andtarget-source ratios of aligned words to total words.These features balance the link-normalized align-ment scores.To determine selection criteria, we use this featureset to score the news test sets from 2008 through2011 (10K parallel sentences) and calculate themean and standard deviation of each feature scoredistribution.
We then select two subsets of the Giga-FrEn, ?1stdev?
and ?2stdev?.
The 1stdev set in-cludes sentence pairs for which the score for eachfeature is above a threshold defined as the develop-ment set mean minus one standard deviation.
The2stdev set includes sentence pairs not included in1stdev that meet the per-feature threshold of meanminus two standard deviations.
Hard, per-featurethresholding is motivated by the notion that a sen-tence pair must meet al the criteria discussed aboveto constitute good translation.
For example, highsource and target language model scores are irrel-evant if the sentences are not parallel.As primarily news data is used for determiningthresholds and building language models, this ap-proach has the added advantage of preferring par-allel data in the domain we are interested in translat-ing.
Our final translation system uses data from both1stdev and 2stdev, corresponding to roughly 60% ofthe Giga-FrEn corpus.2.2 Monolingual DataMonolingual English data includes European Parlia-mentary proceedings (Koehn, 2005), news commen-tary, United Nations documents, news crawl, the En-glish side of the Giga-FrEn, and the English Giga-word Fourth Edition (Parker et al, 2009).
We use allavailable data subject to the following selection de-cisions.
We apply the initial filter to the Giga-FrEnto remove non-text sections, leaving approximately90% of the corpus.
We exclude the known prob-262Corpus WordsEuroparl 59,659,916News commentary 5,081,368UN doc 286,300,902News crawl 1,109,346,008Giga-FrEn 481,929,410Gigaword 4th edition 1,960,921,287Total 3,903,238,891Table 2: Monolingual language modeling data (uniqued)lematic New York Times section of the Gigaword.As many data sets include repeated boilerplate textsuch as copyright information or browser compat-ibility notifications, we unique sentences from theUN doc, news crawl, Giga-FrEn, and Gigaword setsby source.
Final monolingual data totals 4.7 billionwords before uniqueing and 3.9 billion after.
Wordcounts for all data sources are shown in Table 2.2.3 Text ProcessingAll monolingual and parallel system data is runthrough a series of pre-processing steps beforeconstruction of the language model or translationmodel.
We first run an in-house normalization scriptover all text in order to convert certain variably en-coded characters to a canonical form.
For example,thin spaces and non-breaking spaces are normalizedto standard ASCII space characters, various types of?curly?
and ?straight?
quotation marks are standard-ized as ASCII straight quotes, and common Frenchand English ligatures characters (e.g.
?, fi) are re-placed with standard equivalents.English text is tokenized with the Penn Treebank-style tokenizer attached to the Stanford parser (Kleinand Manning, 2003), using most of the default op-tions.
We set the tokenizer to Americanize vari-ant spellings such as color vs. colour or behaviorvs.
behaviour.
Currency-symbol normalization isavoided.For French text, we use an in-house tokenizationscript.
Aside from the standard tokenization basedon punctuation marks, this step includes French-specific rules for handling apostrophes (French eli-sion), hyphens in subject-verb inversions (includ-ing the French t euphonique), and European-stylenumbers.
When compared to the default WMT12-provided tokenization script, our custom Frenchrules more accurately identify word boundaries, par-ticularly in the case of hyphens.
Figure 1 highlightsthe differences in sample phrases.
Subject-verb in-versions are broken apart, while other hyphenatedwords are unaffected; French aujourd?hui (?today?
)is retained as a single token to match English.Parallel data is run through a further filtering stepto remove sentence pairs that, by their length char-acteristics alone, are very unlikely to be true paralleldata.
Sentence pairs that contain more than 95 to-kens on either side are globally discarded, as are sen-tence pairs where either side contains a token longerthan 25 characters.
Remaining pairs are checked forlength ratio between French and English, and sen-tences are discarded if their English translations areeither too long or too short given the French length.Allowable ratios are determined from the tokenizedtraining data and are set such that approximately themiddle 95% of the data, in terms of length ratio, iskept for each French length.3 Translation SystemOur translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase-based translation model (Chiang, 2007) that uses theKenLM library (Heafield, 2011) for language modelinference.
The system translates from cased Frenchto cased English; at no point do we lowercase data.The Parallel data is aligned in both directions us-ing the MGIZA++ (Gao and Vogel, 2008) imple-mentation of IBM Model 4 and symmetrized withthe grow-diag-final heuristic (Och and Ney,2003).
The aligned corpus is then encoded as asuffix array to facilitate sentence-level grammar ex-traction and scoring (Lopez, 2008).
Grammars areextracted using the heuristics described by Chiang(Chiang, 2007) and feature scores are calculated ac-cording to Lopez (2008).Modified Knesser-Ney smoothed (Chen andGoodman, 1996) n-gram language models are builtfrom the monolingual English data using the SRIlanguage modeling toolkit (Stolke, 2002).
We ex-periment with both 4-gram and 5-gram models.System parameters are optimized using minimumerror rate training (Och, 2003) to maximize thecorpus-level cased BLEU score (Papineni et al,263Base: Y a-t-il un colle`gue pour prendre la paroleCustom: Y a -t-il un colle`gue pour prendre la paroleBase: Peut-e?tre , a` ce sujet , puis-je dire a` M. Ribeiro i CastroCustom: Peut-e?tre , a` ce sujet , puis -je dire a` M. Ribeiro i CastroBase: le proce`s-verbal de la se?ance d?
aujourd?
huiCustom: le proce`s-verbal de la se?ance d?
aujourd?huiBase: s?
e?tablit environ a` 1,2 % du PIBCustom: s?
e?tablit environ a` 1.2 % du PIBFigure 1: Customized French tokenization rules better identify word boundaries.pre?-e?l?ectoral ?
pre-electoralmosa?
?que ?
mosaiquede?ragulation ?
deragulationFigure 2: Examples of cognate translation2002) on news-test 2008 (2051 sentences).
This de-velopment set is chosen for its known stability andreliability.Our baseline translation system uses Viterbi de-coding while our final system uses segment-levelMinimum Bayes-Risk decoding (Kumar and Byrne,2004) over 500-best lists using 1 - BLEU as the lossfunction.3.1 Post-ProcessingOur final system includes a monolingual rule-basedpost-processing step that corrects obvious transla-tion errors.
Examples of correctable errors includecapitalization, mismatched punctuation, malformednumbers, and incorrectly split compound words.
Wefinally employ a coarse cognate translation systemto handle out-of-vocabulary words.
We assume thatuncapitalized French source words passed throughto the English output are cognates of English wordsand translate them by removing accents.
This fre-quently leads to (in order of desirability) fully cor-rect translations, correct translations with foreignspellings, or correct translations with misspellings.All of the above are generally preferable to untrans-lated foreign words.
Examples of cognate transla-tions for OOV words in newstest 2011 are shown inFigure 2.11Some OOVs are caused by misspellings in the dev-testsource sentences.
In these cases we can salvage misspelled En-glish words in place of misspelled French wordsBLEU (cased) Meteor TERbase 5-gram 28.4 27.4 33.7 53.2base 4-gram 29.1 28.1 34.0 52.5+1stdev GFE 29.3 28.3 34.2 52.1+2stdev GFE 29.8 28.9 34.5 51.7+5g/1K/MBR 29.9 29.0 34.5 51.5+post-process 30.2 29.2 34.7 51.3Table 3: Newstest 2011 (dev-test) translation results4 ExperimentsBeginning with a baseline translation system, we in-crementally evaluate the contribution of additionaldata and components.
System performance is eval-uated on newstest 2011 using BLEU (uncased andcased) (Papineni et al, 2002), Meteor (Denkowskiand Lavie, 2011), and TER (Snover et al, 2006).For full consistency with WMT11, we use the NISTscoring script, TER-0.7.25, and Meteor-1.3 to eval-uate cased, detokenized translations.
Results areshown in Table 3, where each evaluation point is theresult of a full tune/test run that includes MERT forparameter optimization.The baseline translation system is built from 14million parallel sentences (Europarl, news commen-tary, and UN doc) and all monolingual data.
Gram-mars are extracted using the ?tight?
heuristic thatrequires phrase pairs to be bounded by word align-ments.
Both 4-gram and 5-gram language modelsare evaluated.
Viterbi decoding is conducted with acube pruning pop limit (Chiang, 2007) of 200.
Forthis data size, the 4-gram model is shown to signifi-cantly outperform the 5-gram.Adding the 1stdev and 2stdev sets from the Giga-FrEn increases the parallel data size to 27 million264BLEU (cased) Meteor TER587M tight 29.1 28.1 34.0 52.5587M loose 29.3 28.3 34.0 52.5745M tight 29.8 28.9 34.5 51.7745M loose 29.6 28.6 34.3 52.0Table 4: Results for extraction heuristics (dev-test)sentences and further improves performance.
Theseruns require new grammars to be extracted, butuse the same 4-gram language model and decodingmethod as the baseline system.
With large trainingdata, moving to a 5-gram language model, increas-ing the cube pruning pop limit to 1000, and usingMinimum Bayes-Risk decoding (Kumar and Byrne,2004) over 500-best lists collectively show a slightimprovement.
Monolingual post-processing yieldsfurther improvement.
This decoding/processingscheme corresponds to our final translation system.4.1 Impact of Data SizeThe WMT French-English track provides an oppor-tunity to experiment in a space of data size that isgenerally not well explored.
We examine the impactof data sizes of hundreds of millions of words ontwo significant system building decisions: grammarextraction and language model estimation.
Compar-ative results are reported on the newstest 2011 set.In the first case, we compare the ?tight?
extrac-tion heuristic that requires phrases to be boundedby word alignments to the ?loose?
heuristic that al-lows unaligned words at phrase edges.
Lopez (2008)shows that for a parallel corpus of 107 millionwords, using the loose heuristic produces muchlarger grammars and improves performance by a fullBLEU point.
However, even our baseline systemis trained on substantially more data (587 millionwords on the English side) and the addition of theGiga-FrEn sets increases data size to 745 millionwords, seven times that used in the cited work.
Foreach data size, we decode with grammars extractedusing each heuristic and a 4-gram language model.As shown in Table 4, the differences are muchsmaller and the tight heuristic actually produces thebest result for the full data scenario.
We believethis to be directly linked to word alignment quality:smaller training data results in sparser, noisier wordBLEU (cased) Meteor TER587M 4-gram 29.1 28.1 34.0 52.5587M 5-gram 28.4 27.4 33.7 53.2745M 4-gram 29.8 28.9 34.5 51.7745M 5-gram 29.8 28.9 34.4 51.7Table 5: Results for language model order (dev-test)alignments while larger data results in denser, moreaccurate alignments.
In the first case, accumulatingunaligned words can make up for shortcomings inalignment quality.
In the second, better rules are ex-tracted by trusting the stronger alignment model.We also compare 4-gram and 5-gram languagemodel performance with systems using tight gram-mars extracted from 587 million and 745 millionsentences.
As shown in Table 5, the 4-gram sig-nificantly outperforms the 5-gram with smaller datawhile the two are indistinguishable with larger data2.With modified Kneser-Ney smoothing, a lower or-der model will outperform a higher order model ifthe higher order model constantly backs off to lowerorders.
With stronger grammars learned from largerparallel data, the system is able to produce outputthat matches longer n-grams in the language model.5 SummaryWe have presented the French-English translationsystem built for the NAACL WMT12 shared transla-tion task, including descriptions of our data selectionand text processing techniques.
Experimental re-sults have shown incremental improvement for eachaddition to our baseline system.
We have finallydiscussed the impact of the availability of WMT-scale data on system building decisions and pro-vided comparative experimental results.ReferencesChris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
In Proc.of ACL WMT 2009.2We find that for the full data system, also increasing thecube pruning pop limit and using MBR decoding yields a veryslight improvement with the 5-gram model over the same de-coding scheme with the 4-gram.265Stanley F. Chen and Joshua Goodman.
1996.
An Em-pirical Study of Smoothing Techniques for LanguageModeling.
In Proc.
of ACL 1996.David Chiang.
2007.
Hierarchical Phrase-Based Trans-lation.Michael Denkowski and Alon Lavie.
2011.
Meteor 1.3:Automatic Metric for Reliable Optimization and Eval-uation of Machine Translation Systems.
In Proc.
ofthe EMNLP WMT 2011.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec:A Decoder, Alignment, and Learning Framework forFinite-State and Context-Free Translation Models.
InProc.
of ACL 2010.Qin Gao and Stephan Vogel.
2008.
Parallel Implemen-tations of Word Alignment Tool.
In Proc.
of ACLWSETQANLP 2008.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proc.
of EMNLP WMT2011.Dan Klein and Christopher D. Manning.
2003.
AccurateUnlexicalized Parsing.
In Proc.
of ACL 2003.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proc.
of MT Sum-mit 2005.Shankar Kumar and William Byrne.
2004.
MinimumBayes-Risk Decoding for Statistical Machine Transla-tion.
In Proc.
of NAACL/HLT 2004.Adam Lopez.
2008.
Tera-Scale Translation Models viaPattern Matching.
In Proc.
of COLING 2008.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29.Franz Josef Och.
2003.
Minimum Error Rate Trainingfor Statistical Machine Translation.
In Proc.
of ACL2003.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
In Proc.
of ACL 2002.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2009.
English Gigaword Fourth Edi-tion.
Linguistic Data Consortium, LDC2009T13.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Proc.
of AMTA 2006.Lucia Specia, Craig Saunders, Marco Turchi, ZhuoranWang, and John Shawe-Taylor.
2009.
Improving theConfidence of Machine Translation Quality Estimates.In Proc.
of MT Summit XII.Andreas Stolke.
2002.
SRILM - an Extensible LanguageModeling Toolkit.
In Proc.
of ICSLP 2002.266
