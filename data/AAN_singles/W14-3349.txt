Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 381?386,Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational LinguisticsApplication of Prize based on Sentence Length in Chunk-basedAutomatic Evaluation of Machine TranslationHiroshi Echizen?yaHokkai-Gakuen UniversityS26-Jo, W11-Chome, Chuo-ku,Sapporo 064-0926 Japanechi@lst.hokkai-s-u.ac.jpKenji ArakiHokkaido UniversityN 14-Jo, W 9-Chome, Kita-ku,Sapporo 060-0814 Japanaraki@ist.hokudai.ac.jpEduard HovyCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213 USAhovy@cmu.eduAbstractAs described in this paper, we pro-pose a new automatic evaluation met-ric for machine translation.
Our met-ric is based on chunking between thereference and candidate translation.Moreover, we apply a prize based onsentence-length to the metric, dissim-ilar from penalties in BLEU or NIST.We designate this metric as AutomaticEvaluation of Machine Translation inwhich the Prize is Applied to a Chunk-based metric (APAC).
Through meta-evaluation experiments and compari-son with several metrics, we confirmedthat our metric shows stable correla-tion with human judgment.1 IntroductionIn the field of machine translation, variousautomatic evaluation metrics have been pro-posed.
Among them, chunk-based metricssuch as METEOR(A. Lavie and A. Agarwal,2007), ROUGE-L(Lin and Och, 2004), andIMPACT(H. Echizen-ya and K. Araki, 2007)are effective.
In general, BLEU(K. Papineni etal., 2002), NIST(NIST, 2002), and RIBES(H.Isozaki et al., 2010) use a penalty for calcula-tion of scores because the high score is oftengiven extremely when the candidate transla-tion is short.
Therefore, the penalty is effectiveto obtain high correlation with human judg-ment.
On the other hand, almost all chunk-based metrics use the F -measure based on aprecision by candidate translation and a re-call by reference.
Moreover, they assign apenalty for the difference of chunk order be-tween the candidate translation and the refer-ence, not the penalty for the difference of sen-tence length.
Nevertheless, it is also impor-tant for chunk-based metrics to examine thesentence length.
In chunk-based metrics, eachword?s weight depends on the sentence length.For example, the weight of each word is 0.2(=1/5) when the number of words in a sen-tence is 5; it is 0.1 (=1/10) when the numberof words in a sentence is 10.
Therefore, theweight of the non-matched word in the shortsentence is large.To resolve this problem, it is effective forshort sentences to give a prize based on thesentence length in the chunk-based metrics.Therefore, we propose a new metric using aprize based on the sentence length.
We des-ignate this metric as Automatic Evaluationof Machine Translation in which the Prize isApplied to a Chunk-based metric (APAC).
Inour metric, the weight of a non-matched wordbecomes small for the short sentence by award-ing of the prize.
It is almost identical to thatfor a long sentence by awarding of the prize.Therefore, our metric does not depend heavilyon sentence length because the weight of non-matched words is constantly small.
We con-firmed the effectiveness of APAC using meta-evaluation experiments.2 Score calculation in APACThe APAC score is calculated in two phases.In the first phase, the chunk sequence isdetermined between a candidate translationand the reference.
The chunk sequence381is determined using the Longest CommonSubsequence (LCS).
Generally, several chunksequences are obtained using LCS.
In thatcase, APAC determines only one chunk se-quence using the number of words in eachchunk and the position of each chunk.For example, in between the candidatetranslation ?In this case, the system powersupply is accessory battery 86.?
and ?In thiscase, the system power supply is the accessorypower supply battery 86.?, the chunk sequenceis ?in this case, the system power supply is?,?accessory?
and ?battery 86.?, and the chunksequence is ony one in these sentences.
Onlyone chunk sequence is determined using thenumber of words in each chunk and the po-sition of each chunk when several chunk se-quences are obtained.The second phase is calculation of the scorebased on the determined chunk sequence.
TheCh score in Eq.
(3) is calculated using the de-termined chunk sequence.
In Eq.
(3), ch de-notes each chunk and ch num represents thenumber of chunks.
Moreover, length(ch) is theword number of each chunk.
?
is the weightparameter for the length of each chunk.
Forexample, in between the candidate translation?In this case, the system power supply is ac-cessory battery 86.?
and ?In this case, thesystem power supply is the accessory powersupply battery 86.?, ch num is 3 (?in thiscase, the system power supply is?, ?accessory?and ?battery 86.?).
Therefore, Ch score is 91(=92.0+ 12.0+ 32.0) when ?
is 2.0.P ={(?RN?1i=0(?i?
Ch score)m?)1?+0.5?
Prize m}/2.0 (1)R ={(?RN?1i=0(?i?
Ch score)n?)1?+0.5?
Prize n}/2.0 (2)Ch score =?ch?ch numlength(ch)?
(3)Prize m =1log(m) + 1(4)Prize n =1log(n) + 1(5)APAC score =(1 + ?2)RPR + ?2P(6)The P and R in Eqs.
(1) and (2) re-spectively denote precision by candidatetranslation and recall by reference.
Theseare calculated using the Ch score obtainedusing Eq.
(3).
Therein, m and n respectivelyrepresent the word numbers of the candidatetranslation and the reference.
Moreover,the chunk sequence determination process isrepeated recursively to all common words.The number of determination processes ofthe chunk sequence is high when the wordorder of the candidate translation differsfrom that of the reference.
The RN is thenumber of determination processes of thechunk sequence.
Here, ?
is the parameter forthe chunk order.
It is less than 1.0.
The valueof the Ch score is small when the chunk orderbetween the candidate translation and refer-ences differs because the value of length(ch)in each chunk becomes small.
For example,in between the candidate translation ?In thiscase, the system power supply is accessorybattery 86.?
and ?In this case, the systempower supply is the accessory power sup-ply battery 86.?,(?RN?1i=0(?i?Ch score)m?
)1?is 0.773 (=?91169=?
?1?1i=0(0.10?91)132.0)and(?RN?1i=0(?i?Ch score)n?
)1?is 0.596(=?91256=?
?1?1i=0(0.10?91)162.0) when ?
and ?respectively stand for 0.1 and 2.0.
Thevalue of RN is 1 because there is no morematching words after the determined chunks(?in this case, the system power supply is?,?accessory?
and ?battery 86.?)
are removedfrom the candidate translation ?In this case,the system power supply is accessory battery86.?
and ?In this case, the system powersupply is the accessory power supply battery86.
?.Moreover, Prize m and Prize n in Eqs.
(1)and (2) are calculated respectively using Eqs.382(4) and (5).
Each is less than 1.0.
For ex-ample, in the candidate translation ?In thiscase, the system power supply is accessorybattery 86.?
and ?In this case, the systempower supply is the accessory power supplybattery 86.?, Prize m and Prize n respec-tively stand for 0.473 (=11.114+1=1log(13)+1) and0.454 (=11.204+1=1log(16)+1).
These values be-come large in the short sentences.
They be-come small in the long sentences.
Therefore,the weight of each non-matched word is smallin the short sentences.
It is kept small inthe long sentences.
Finally, the score is cal-culated using Eq.
(6).
This equation showsthe f -measure based on P and R. In Eq.
(6),?
is determined as P/R(C. J. V. Rijsbergen,1979).
The APAC score is between 0.0 and1.0.
For example, in the candidate transla-tion ?In this case, the system power supply isaccessory battery 86.?
and ?In this case, thesystem power supply is the accessory powersupply battery 86.?, P and R respectivelystand for 0.505 (=0.773+0.5?0.4732.0) and 0.412(=0.596+0.5?0.4542.0).
Therefore, APAC score is0.445 (=0.5211.171=(1+1.503)?0.412?0.5050.412+1.503?0.505) and ?
is1.226 (=0.5050.412)3 Experiments3.1 Experimental ProcedureMeta-evaluation experiments are performedusing WMT2012(C. Callison-Burch et al.,2012) data and WMT2013(O. Bojar et al.,2013) data, and NTCIR-7(A. Fujii et al., 2008)data and NTCIR-9(A. Goto et al., 2011) data.All sentences by NTCIR data are Englishpatent sentences obtained through Japanese-to-English translation.
The number of refer-ences is 1.
In NTICR-7 data, the averagevalue in the evaluation results of three hu-man judgments is used as the scores of 1?5 from the perspective of adequacy and flu-ency.
In NTCIR-9 data, the evaluation resultsof one human judgment is used as the scoresof 1?5 from the view of adequacy and accep-tance.
For this meta-evaluation, we used onlyEnglish and Japanese candidate translationsbecause we can evaluate them in comparisonwith other languages correctly.We calculated the correlation between thescores by automatic evaluation and the scoresby human judgments at the system level andthe segment level, respectively.
Spearman?srank correlation coefficient is used at the sys-tem level.
The Kendall tau rank correlationcoefficient is used in the segment level.Moreover, we used BLEU (ver.
13a),NIST (ver.
13a), METEOR (ver.
1.4), andAPAC with no prize (APAC no p) as theautomatic evaluation metrics for comparisonwith APAC as shown in Eqs.
(4) and (5).In APAC no p,(?RN?1i=0(?i?Ch score)m?
)1?as Pand(?RN?1i=0(?i?Ch score)m?
)1?as R are used re-spectively in Eqs.
(1) and (2).3.2 Experimental ResultsTables 1 and 2 respectively present Spear-man?s rank correlation coefficients of system-level and Kendall tau rank correlation coef-ficients of segment-level in WMT2012 data.Tables 3 and 4 respectively show Spearman?srank correlation coefficients of the system-leveland Kendall tau rank correlation coefficients ofsegment-level in WMT2013 data.
Moreover,Tables 5 and 6 respectively present Spear-man?s rank correlation coefficients of system-level and Kendall tau rank correlation coeffi-cients of segment-level in NTCIR-7 data.
Ta-bles 7 and 8 respectively show Spearman?srank correlation coefficients of system-leveland Kendall tau rank correlation coefficientsof the segment level in NTCIR-9 data.In APAC, 0.1 and 1.2 were used as the valuesof parameters ?
and ?
by the preliminarily ex-perimentally obtained results.
In Tables 1?8,?Rank?
denotes the ranking based on ?Avg.
?The value of ?()?
denotes the number of MTsystems in Tables 1, 3, 5, and 7.
The value of?()?
represents the number of sentence pairsin Tables 2, 4, 6, and 8.
These values dependon the data.3.3 DiscussionThe results presented in Tables 1?8 indicatethat APAC can obtain the most stable corre-lation coefficients among some metrics.
Theranking of APAC is No.
1 through NTCIRdata in Tables 5?8.
In WMT data of Ta-bles 1?4, the ranking of APAC is the lowestexcept for Table 3.
However, the difference383cs-en(6) de-en(16) es-en(12) fr-en(15) Avg.
RankAPAC 0.886 0.650 0.958 0.811 0.826 5APAC no p 0.886 0.676 0.958 0.807 0.832 3METEOR 0.943 0.841 0.979 0.818 0.895 1BLEU 0.886 0.674 0.958 0.796 0.828 4NIST 0.943 0.700 0.944 0.779 0.841 2Table 1: Spearman?s rank correlation coefficient of system-level in WMT2012 data.cs-en(11,155) de-en(12,042) es-en(9,880) fr-en(11,682) Avg.
RankAPAC 0.185 0.204 0.209 0.226 0.206 3APAC no p 0.189 0.207 0.208 0.226 0.207 2METEOR 0.223 0.279 0.248 0.243 0.248 1Table 2: Kendall tau rank correlation coefficient of the segment level in WMT2012 data.between the ranking of METEOR, which isthe highest, and that of APAC is not largerin WMT data.
The correlation coefficients ofAPAC in NTCIR data of Tables 5?8 are higherthan those of METEOR.
In Tables 5 and 6,underlining in APAC signifies that the differ-ences between correlation coefficients obtainedusing APAC and METEOR are statisticallysignificant at the 5% significance level.
In Ta-ble 7, the correlation coefficients of METEOR,BLEU, and NIST are extremely low.
Only onehuman judgment was used in NTCIR-9 data.As a result, APAC is fundamentally effectivefor various languages independent of the differ-ences in the grammatical structures betweenlanguages: these experimentally obtained re-sults indicate that APAC is the most stablemetric.Moreover, in APAC, the correlation coeffi-cients of the segment level in NTCIR data wereincreased using the prize of Eqs.
(4) and (5).In WMT data, the correlation coefficients arealmost identical using the prize.
Therefore,use of the prize was fundamentally effectiveat the segment level.
The evaluation qualityof segment level is generally very low in theautomatic evaluation metrics.
Therefore, it isextremely important to improve the correla-tion coefficient of segment level.
Applicationof the prize is effective to improve the evalua-tion quality of the segment level.4 ConclusionAs described in this paper, we proposed a newchunk-based automatic evaluation metric us-ing the prize based on the sentence length.The experimentally obtained results indicatethat APAC is the most stable metric.We will improve APAC to obtain highercorrelation coefficients in future studies.Particularly, we will strive to improvethe correlation coefficients at the segmentlevel.
The APAC software will be re-leased by http://www.lst.hokkai-s-u.ac.jp/~echi/automatic_evaluation_mt.html.AcknowledgmentsThis work was done as research under theAAMT/JAPIO Special Interest Group onPatent Translation.
The Japan Patent In-formation Organization (JAPIO) and the Na-tional Institute of Information (NII) providedcorpora used in this work.
The author grate-fully acknowledges support from JAPIO andNII.ReferencesO.
Bojar, C. Buck, C. Callison-Burch, C. Feder-mann, B. Haddow, P. Koehn, C. Monz, M. Post,R.
Sortcut and L. Specia.
2013.
Findings of the2013 Workshop on Statistical Machine Transla-tion.
Proceedings of the Eighth Workshop onStatistical Machine Translation.
pp.1?44.C.
Callison-Burch, P. Koehn, C. Monz, M. Post,R.
Sortcut and L. Specia.
2012.
Findings of the2012 Workshop on Statistical Machine Transla-tion.
Proceedings of the Seventh Workshop onStatistical Machine Translation.
pp.10?51.H.
Echizen-ya and K. Araki.
2007.
AutomaticEvaluation of Machine Translation based on384cs-en(11) de-en(17) es-en(12) fr-en(13) ru-en(19) Avg.
RankAPAC 0.900 0.904 0.916 0.934 0.709 0.873 3APAC no p 0.909 0.909 0.937 0.934 0.721 0.882 2METEOR 0.982 0.946 0.923 0.967 0.889 0.941 1BLEU 0.945 0.897 0.853 0.951 0.614 0.852 4NIST 0.900 0.828 0.804 0.786 0.465 0.757 5Table 3: Spearman?s rank correlation coefficient of the system level in WMT2013 data.cs-en de-en es-en fr-en ru-enMetrics(85,469) (128,668) (67,832) (80,741) (151,422)Avg.
RankAPAC 0.144 0.163 0.169 0.139 0.121 0.147 3APAC no p 0.148 0.167 0.176 0.142 0.123 0.151 2METEOR 0.222 0.236 0.241 0.194 0.226 0.224 1Table 4: Kendall tau rank correlation coefficient of the segment level in WMT2013 data.Recursive Acquisition of an Intuitive CommonParts Continuum.
Proceedings of the EleventhMachine Translation Summit.
pp.151?158.A.
Fujii, M. Utiyama, M. Yamamoto and T. Ut-suro.
2008.
Overview of the Patent TranslationTask at the NTCIR-7 Workshop.
Proceedingsof the Seventh NTCIR Workshop Meeting onEvaluation of Information Access Technologies:Information Retrieval, Question Answering andCross-lingual Information Access.
pp.389?400.I.
Goto, B. Lu, K. P. Chow, E. Sumita and B. K.Tsou.
2011.
Overview of the Patent TranslationTask at the NTCIR-9 Workshop.
Proceedings ofthe Ninth NTCIR Workshop Meeting.
pp.559?578.H.
Isozaki, T. Hirao, K. Duh, K. Sudoh andH.
Tsukada.
2010.
Automatic Evaluation ofTranslation Quality for Distant Language Pairs.Proceedings of the 2010 Conference on Empir-ical Methods in Natural Language Processing.pp.944?952.A.
Lavie and A. Agarwal.
2007.
Meteor: An Auto-matic Metric for MT Evaluation with High Lev-els of Correlation with Human Judgments.
Pro-ceedings of the Second Workshop on StatisticalMachine Translation.Chin-Yew Lin and F. J. Och.
2004.
AutomaticEvaluation of Machine Translation Quality Us-ing the Longest Common Subsequence and Skip-Bigram Statistics.
In Proc.
of ACL?04, 606?613.NIST.
2002.
Automatic Evaluationof Machine Translation Quality Us-ing N-gram Co-Occurrence Statistics.http://www.nist.gov/speech/tests/mt/doc/ngram-study.pdf.K.
Papineni, S. Roukos, T. Ward, and Wei-JingZhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
Proceedingsof the 40th Annual Meeting of the Associationfor Computational Linguistics (ACL).
pp.311?318.C.
J.
Van Rijsbergen.
1979.
Information Retrieval(2nd ed.
), Butterworths.385Adequacy(15) Fluency(15) Avg.
RankAPAC 0.872 0.805 0.839 1APAC no p 0.872 0.805 0.839 1METEOR 0.424 0.380 0.402 5BLEU 0.582 0.586 0.584 3NIST 0.578 0.568 0.573 4Table 5: Spearman?s rank correlation coefficient of the system level in NTCIR-7 data.Adequacy (1,500) Fluency (1,500) Avg.
RankAPAC 0.494 0.489 0.491 1APAC no p 0.482 0.476 0.479 2METEOR 0.366 0.383 0.375 3Table 6: Kendall tau rank correlation coefficient of the segment level in NTCIR-7 data.Adequacy (19) Acceptance (14) Avg.
RankAPAC 0.182 0.298 0.240 1APAC no p 0.182 0.298 0.240 1METEOR -0.081 0.015 -0.033 4BLEU -0.123 0.059 -0.032 3NIST -0.344 -0.275 -0.309 5Table 7: Spearman?s rank correlation coefficient of the system level in NTCIR-9 data.Adequacy (5,700) Acceptance (5,700) Avg.
RankAPAC 0.250 0.261 0.256 1APAC no p 0.242 0.250 0.246 2METEOR 0.167 0.217 0.192 3Table 8: Kendall tau rank correlation coefficient of segment-level in NTCIR-9 data.386
