Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1489?1499, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsRevisiting the Predictability of Language:Response Completion in Social MediaBo Pang Sujith RaviYahoo!
Research4401 Great America ParkwaySanta Clara, CA 95054, USAbopang42@gmail.com sujith ravi@yahoo.comAbstractThe question ?how predictable is English?
?has long fascinated researchers.
While priorwork has focused on formal English typicallyused in news articles, we turn to texts gener-ated by users in online settings that are moreinformal in nature.
We are motivated by anovel application scenario: given the difficultyof typing on mobile devices, can we help re-duce typing effort with message completion,especially in conversational settings?
We pro-pose a method for automatic response comple-tion.
Our approach models both the languageused in responses and the specific context pro-vided by the original message.
Our experi-mental results on a large-scale dataset showthat both components help reduce typing ef-fort.
We also perform an information-theoreticstudy in this setting and examine the entropyof user-generated content, especially in con-versational scenarios, to better understand pre-dictability of user generated English.1 IntroductionHow predictable is language?
As early as 1951, longbefore large quantities of texts (or the means to pro-cess them) were easily available, Shannon had raisedthis question and proceeded to answer it with a setof clever analytical estimations.
He studied the pre-dictability of printed English, or ?how well can thenext letter of a text be predicted when the preced-ing N letters are known?
(Shannon, 1951).
Thiswas quantified as the conditional entropy, whichmeasures the amount of information conveyed fromstatistics over the preceding context.
In this paper,we discuss a novel application setting which mirrorsthe predictability study as defined by Shannon.
(a) Google (b) Amazon (c) NetflixFigure 1: Query completion as users type into the ?Searchusing Google?
box on a browser, as well as the search boxin Amazon and Netflix.Text completion for user-generated texts: Con-sider a user who is chatting with her contact or post-ing to a social media site using a mobile device.
Ifwe can predict the next word given the precedingwords that were already typed in, we can help reducethe typing cost by offering users suggestions of pos-sible completions of their partially typed messages(e.g., in a drop-down list).
If the intended word isranked reasonably high, the user can select the wordinstead of typing it.
Assuming a lower cost associ-ated with selections, this could lead to less typingeffort for the user.An interface like this would be quite familiar toWeb users today.
Providing suggestions of possi-ble completions to partially typed queries, which wewill refer to as query completion,1 is a common fea-ture of search boxes (Figure 1).
In spite of thesimilarity in the interface, the underlying technicalchallenge can be quite different.
Query completiondoes not necessarily rely on language models: can-1Note that this feature is often tagged as ?query suggestion?in the user interface; we avoid that terminology since it is oftenused to refer to query re-formulation (of a completely enteredquery) in the literature, which is a very different task.1489didate completions can be limited to popular queriesthat were previously submitted to the site or entriesin a closed database of available objects, and rank-ing can be done by overall popularity.
In contrast,our scenario requires generation of unseen texts.Given the difficulty of generating full-length text,we consider a more realistic setting, where we per-form completion on a word-by-word basis.
Eachtime, we propose candidate completions at the word-level when the user is about to start a new word,or has partially entered the first few letters; oncethis word is successfully completed, we move onto the next one.
This predict-verify-predict processexactly mirrors the human experiment described byShannon (1951), except we do this at the word-levelrather than the letter-level: having the user examineand verify predictions at the letter level would not bea practical solution for the intended application.The response completion task: In addition, ourtask has another interesting difference from Shan-non?s human experiment.
Consider the mobile-device user mentioned previously.
If the user is re-plying to a piece of text (e.g., an instant messagesent by a contact), we have an additional source ofcontextual information in the stimulus, or the textwhich triggered the response that the user is try-ing to type.
Can we learn from previously observedstimulus-response pairs (which we will refer to asexchanges)?
That is, can we take advantage of thisconversational setting and effectively use the infor-mation provided by stimulus to better predict thenext word in the response?
We refer to this task asthe response completion task.Our task is different from ?chatter-bots?
(Weizen-baum, 1966), where the goal is to generate a re-sponse to an input that would resemble a human con-versation partner.
Instead, we want to complete aresponse as the replier intends to.
Recently, Ritteret.
al (2011) experimented with automatic responsegeneration in social media.
They had a similar con-versational setting, but instead of completion basedon partial input, they attempted to generate a re-sponse in its entirety given only the stimulus.
Whilemany of the generated responses are deemed possi-ble replies to the stimulus, they have a low chanceof actually matching the real response given by theuser: they reported BLEU scores between 0 and 2for various systems.
This clearly shows the diffi-culty of the task.
While we are addressing a moremodest setting, would the problem prove to be toodifficult even in this case?In this paper, we propose a method for auto-matic response completion.
Our approach modelsthe generic language used in responses, as well asthe contextual information provided by the stim-ulus.
We construct a large-scale dataset of user-generated textual exchanges, and our experimentalresults show that both components help reduce typ-ing effort.
In addition, to better understand pre-dictability of user generated English, we performan information-theoretic study in this conversationalsetting to investigate the entropy of user-generatedcontent.2 Related workThere has been previous work in the area of human-computer interaction that examined text entry formobile devices (MacKenzie and Soukoreff, 2002).In particular, one line of work looked into predic-tive text input, which examined input effort reduc-tion by language prediction.
Previous work in pre-dictive text input had very different focus from ourstudy.
Oftentimes, the focus was to model actualtyping efforts using mobile device keypads, examinethe speed and cognitive load of different input meth-ods, and evaluate with emprical user studies in labsettings (James and Reischel, 2001; How and Kan,2005), where the underlying technique for languageprediction can be as simple as unigram frequency(James and Reischel, 2001), or restricted to narrowdomains such as grocery shopping lists (Nurmi etal., 2009).
In addition, to the best of our knowledge,no previous work in predictive text input addressedthe conversationl setting.As discussed in Section 1, the response genera-tion task (Ritter et al 2011) also considered the con-verstational setting, but the MT-based technique wasnot well-suited to produce responses as intended bythe user.
There has been extensive previous researchin language modeling (Rosenfeld, 2000).
While pre-vious work has explored Web text sources that are?better matched to a conversational speaking style?
(Bulyko et al 2003), we are not aware of much pre-vious work that has taken advantage of informationin the stimulus for word predictions in responses.1490Previous work on entropy of language stems fromthe field of information theory (Shannon, 1948),starting with Shannon (1951).
An extensive bibliog-raphy covering early related work (e.g., insights intothe structure of language via information theory, en-tropy estimates via other techniques and/or for dif-ferent languages, as well as a broad range of applica-tions of such estimates) can be found in (Cover andKing, 1978).
More recently, Brown et.
al (1992)computed an upper bound for the entropy of En-glish with a trigram model, using the Brown corpus.Some other related works on this topic include (Tea-han and Cleary, 1996; Moradi et al 1998).
Therewas also a recent study using entropy in the contextof Web search (Mei and Church, 2008).
In other set-tings, entropy has also been employed as a tool forstudying the linguistic properties of ancient scripts(e.g., Indus Script) (Rao et al 2009).
While thisseems like an interesting application of informationtheory for linguistic studies, it has also generatedsome controversies (Farmer et al 2004).In contrast, our work departs from traditional sce-narios significantly.
We perform entropy studiesover texts generated in online settings which aremore informal in nature.
Additionally, we utilize theproperties of language predictability within a novelapplication for automatically completing responsesin conversational settings.
Also, in our case we donot have to worry about issues like ?is this a lan-guage or not??
because we work with real Englishnews data which include articles written by pro-fessional editors and comments generated by usersreading those articles.3 ModelIn this section, we first state our problem more for-mally, followed by descriptions of the basicN -gramlanguage model we use, as well as two approachesthat model both stimulus and preceding words inresponse as the context for the next-word genera-tion.
Given the intended application, we hope toachieve better prediction without incurring signifi-cant increase in model size.3.1 Problem definitionConsider a stimulus-response pair, where the stim-ulus is a sequence of tokens s = (s1, s2, ..., sm),and the response is a sequence of tokens r =(r1, r2, ..., rn).
Let r1..i = (r1, r2, ..., ri), our taskis to generate and rank candidates for ri+1 given sand r1..i.Note the models described in this section do notassume any knowledge of partial input for ri+1.
Forthe setting where the first c characters of ri+1 werealso entered, we can restrict the candidate list to thesubset with the matching prefix, and use the sameranking function.3.2 Generic Response Language ModelFirst, we consider an N -gram language modeltrained on all responses in the training data as ourgeneric response language model.
Here we considerN = 3.
Normally, trigram models use back-off toboth bigrams and unigrams; in order to compare theeffectiveness of trigram models vs. bigram modelsunder comparable model size, we use back-off onlyto unigrams in both cases:trigram: P (ri+1 | r1..i) = ?1 ?
P3(ri+1 | ri, ri?1)+(1?
?1) ?
P1(ri+1)bigram: P (ri+1 | r1..i) = ?1 ?
P2(ri+1 | ri)+(1?
?1) ?
P1(ri+1)If we ignore the context provided by texts in thestimulus, we can simply generate and rank candidatewords from the dictionary according to the genericresponse LM: P (ri+1 | r1..i).As we will discuss in more detail in Section 6.2,modeling s and r1..i jointly in the prediction of ri+1would be rather expensive.
In the following sec-tions, we follow two main approaches to break thisdown into separate components: P (ri+1 | r1..i) andP (ri+1 | s), and model each one separately.3.3 Translating Stimuli to ResponsesAs mentioned in Section 1, Ritter et.
al (2011) haveconsidered a related task of generating a response inits entirety given only the text in the stimulus.
Theycast the problem as a translation task, where thestimulus is considered as the source language andthe response is considered as the target language.We can adapt this approach for our response com-pletion task.Consider the noisy channel model used in statisti-cal machine translation: P (r|s) ?
P (r)?P (s|r).
In1491order to predict ri+1 given r1..i and s, for each can-didate ri+1, in principle one can marginalize overall possible completions of r1..i+1, and rank candi-date ri+1 by that.
That is, let P(n) be the distributionof response length, let r?
be a possible completionof r1..i+1 (i.e., a response whose first i + 1 tokensmatch r1..i+1).
For each possible n > i, we need tomarginalize over all possible r?
of length n, and rankri+1 according toP (r1..i+1 | s) =?n>iP (n) ?
?|r?|=nP (r?
| s)Clearly this will be computationally expensive.
In-stead, we take a greedy approach, and choose ri+1which yields the optimal partial response (withoutlooking ahead):P (r1..i+1 | s) ?
P (r1..i+1) ?
P (s | r1..i+1)which is equivalent to ranking candidate ri+1 byP (ri+1 | r1..i) ?
P (s | r1..i+1) (1)Since the first component is our LM model, and thesecond component is a translation model, we denotethis as the LM+TM model.
We use IBM Model-1 tolearn the translation table on the training data.
Attest time, equal number of candidates are generatedby each component, and combined to be ranked byEq.
1.3.4 Mixture ModelOne potential concern over applying the translationmodel is that the response can often contain novel in-formation not implied by the stimulus.
While tech-nically this could be generated from the so-callednull token used in machine translation (added to thesource text to account for target text with no clearalignment in the source text), significant amount oftext corresponding to new information not in thesource text is not what null tokens are meant to becapturing.
In general, our problem here is a lack ofclear word-to-word or phrase-to-phrase mapping ina stimulus-response pair, at least not what one wouldexpect in clean parallel data.Alternatively, one can model the response genera-tion process with a mixture model: with probability?s, we generate a word according to a distributionover s (P (w | s)), and with probability 1 ?
?s, wegenerate a word using the response language model:P (ri+1 | s, r1..i) = ?s ?
P (ri+1 | s)+ (1?
?s) ?
P (ri+1 | r1..i)(2)We examine two concrete ways of exploiting thecontext provided by s.Model 1 ?
LM + Selection model First, we ex-amine a very simple instantiation of P (w | s) wherewe select a token in s uniformly at random.
Thisis based on the intuition that to be semantically co-herent, a reply often needs to repeat certain contentwords in the stimulus.
(Similar intuition has beenexplored in the context of text coherency (Barzilayand Lapata, 2005).)
This is particularly useful forwords that are less frequently used: they may notbe able to receive enough statistics to be promotedotherwise.
More specifically,P (ri+1 | s) =1ri+1?s|s|We can take ?s to be a constant ?select, which can beestimated in the training data as the probability of aresponse token being a repetition of a token in thecorresponding stimulus.Model 2 ?
LM + Topic model Another way toincorporate information provided in s is to use it toconstrict the topic in r. We can learn a topic modelover conversations in the training data using LatentDirchlet Allocation (LDA) (Blei et al 2003).
At testtime, we identify the most likely topic of the conver-sation based on s, and expect ri+1 to be generatedfrom this topic.
That is,P (ri+1 | s) = P (ri+1 | t?
)where t?
= argmaxtP (topic = t | s)More specifically, we first train a topic model on (s,r) pairs from the training data.
Given a new stim-ulus s, we then select the highest ranked topic asbeing representative of s. Note that alternatively wecould consider all possible topic assignments; in thatcase we would have had to sum probabilities overall topics, and that could also introduce noise.
Asimilar strategy has been previously employed for1492other topic modeling applications in information re-trieval, where documents are smoothed with theirhighest ranked topic (Yi and Allan, 2009).
We lowerthe weight ?s if P (t?
| s) is low.
That is, we use?s = ?topic ?
P (t?
| s) in Eq.
2.4 DataIn order to investigate text completion in a conver-sational setting, we need to construct a large-scaledataset with textual exchanges among users.
Anideal dataset would have been a collection of in-stant messages, but these type of datasets are diffi-cult to obtain given privacy concerns.
To the bestof our knowledge, existing SMS (short message ser-vice) datasets only contain isolated text spans and donot provide enough information to reconstruct theconversations.
There are, however, a high volumeof textual exchanges taking places in public forums.Many sites with a user comment environment allowother users to reply to existing comments, where theoriginal comment and its reply can form a (stimulus,response) pair for our purposes.To this end, we extracted (comment, reply) pairsfrom Yahoo!
News2, where under each news article,a user can post a new comment or reply to an exist-ing comment.
In fact, a popular comment can have along thread of replies where multi-party discussionstake place.
To ensure the reply is a direct response tothe original comment, we took only the first reply toa comment, and consider the resulting pair as a tex-tual exchange in the form of a (stimulus, response)pair.
We gathered data from a period of 14 weeksbetween March and May, 2011.
A random sampleyielded a total of 1,487,995 exchanges, representing237,040 unique users posting responses to stimulicomments authored by 357,811 users.
In the rawdataset (i.e., before tokenization), stimuli average at59 tokens (332 characters), and responses average at26 tokens (144 characters).We took the first 12 weeks of data as training data(1,269,732 exchanges) and the rest 2 weeks of dataas test data (218,263 exchanges).2Note that previous work has used a dataset with 1 millionTwitter conversations extracted from a scraping of the site (Rit-ter et al 2011), where a status update and its replies in Twitterform ?conversations?.
This dataset is no longer publicly avail-able.
At the time of this writing, we were not able to identify adata source to re-construct a dataset like that.5 Experiments5.1 Evaluation measuresRecall@k : Here, we follow a standard evaluationstrategy used to assess ranking quality in informa-tion retrieval applications.
For each word, we checkif the correct answer is one of the top-k tokens beingsuggested.
We then compute the recall at differentvalues of k. While this is a straight-forward mea-sure to assess the overall quality of different top-klists, it is not tailored to suit our specific task of re-sponse completion.
In particular, this measure (a)does not distinguish between (typing) savings for ashort word versus a long one, and (b) does not dis-tinguish between the correct answer being higher upin the list versus lower as long as the word is presentin the top-k list.TypRed : Our main evaluation measure is based on?reduction in typing effort for a user of the system?,which is a more informative measure for our task.We estimate the typing reduction via a hypotheticaltyping model3 in the following manner:Suppose we show top k predictions for a givensetting.
Now, there are two possible scenarios:1. if the user does not find the correct answer inthe top-k list, he/she gives up on this word andwill have to type the entire word.
The typingcost is then estimated to be the number of char-acters in the word lw;2. if the user spots the correct answer in the list,the cost for choosing the word is proportionalto the rank of the word rankw, with a fixed costratio c0.
Suppose the user scrolls down the listusing the down-arrow (?)
to reach the intendedword (instead of typing), then rankw ?
c0 re-flects the scrolling effort required, where c0 isthe relative cost of scrolling down versus typinga character.In general, pressing a fixed key can have a lowercost than typing a new one, in addition, we canimagine a virtual keyboard where navigational keysoccupy bigger real-estate, and thus incur less costto press.
As a result, it?s reasonable to assume c0value that is smaller than 1.
In all our experiments,c0 = 0.5 unless otherwise noted.
Note that if the3More accurate measures can be be developed by observinguser behavior in a lab setting.
We leave this as future work.1493System TypRed (c = 0) TypRed (c = 1) TypRed (c = 2)1.
Generic Response LM (trigram) 15.10 22.57 14.292.
Generic Response LM (trigram) + TM 9.03 17.53 11.563.
Mixture Model 1: 15.18?
23.43?
15.13?Generic Response LM (trigram) + Selection4.
Mixture Model 2: 15.10 22.57 14.33Generic Response LM (trigram) + Topic ModelTable 1: Comparison of various prediction models (in terms of TypRed score @ rank 5) on all (stimulus,response)pairs from a large test collection (218,263 exchanges) when the first c characters of each word are typed in by the user.A higher score indicates better performance.
?
indicates statistical significance (p < 0.05) over the baseline score.typing model assumes a user selects the intendedword using an interface that is similar to a mousingdevice, the cost may increase with rankw at a sub-linear rate; in that case, our measure will be over-estimating the cost.In order to have a consistent measure that al-ways improves as the ranking improves, we assumea clever user who will choose to finish the word bytyping or by selecting, depending on which cost islower.
Combining these two cases under the clever-user model, we estimate the reduction in typing costfor every word as follows:TypRed(w, rankw) = 100?
[1?min(lw, rankw ?
c0)lw]where w is the correct word, lw is the length ofw, and rankw is the rank of w in the top-k list.A higher value of TypRed implies higher savingsachieved in typing cost and thereby better predictionperformance.5.2 Experimental setupWe run experiments using the models described inSection 3 under two different settings: (1) previouswords from the response are provided, and (2) pre-vious words from response + first c characters of thecurrent word are provided.During the candidate generation phase, for everyposition in the response message we present the top1,000 candidates (as scored by the generic responselanguage model or mixture models).
We reserve asmall subset of (?1,000) exchanges as developmentdata for tuning parameters from our models.For the generic response language models, weset the interpolation weight ?1 = 0.9.
For theselection-based mixture model, we estimate the mix-ture weights on the training data and set ?select(0.09).
For the topic-based mixture model, we rana grid search with different parameter settings for?topic on the held-out development set and chose thevalue (0.01) that gave the best performance (in termsof TypRed).5.3 ResultsPrevious words from response observed: We firstpresent results for the setting where only previouswords from the response are provided as context.We use TypRed scores as our evaluation measurehere (higher TypRed implies more savings in typ-ing effort).
Even with a unigram LM we achievea small but non-negligible reduction (TypRed=2.15)in the typing cost.
But a bigram LM significantlyimproves performance (TypRed=11.91), and withtrigram LM we observe even better performance(TypRed=15.10).
Since the trigram LM yields ahigh performance, we set this as our default LM forall other models.Recall that in all experiments, we set c0, the costratio of selecting a candidate from the ranked top-k list (via scrolling) versus typing a character to avalue of 0.5.
But we also experimented with a hy-pothetical setting where c0 = 1 and noticed that thetrigram LM achieves a slightly lower but still signif-icant typing reduction (TypRed score of 9.58 versus15.10 for the earlier case).The first column of Table 1 (c = 0) compares theperformance of other models for this setting.
Wefind that adding a translation model (LM+TM) doesnot help for this task; in fact, it results in lowerscores than using the LM alone.
This suggests thata translation-based generative approach may not besuitable, if the goal is to predict text as intended bythe user.
This is consistent with previous observa-tions on a related task (Ritter et al 2011), as we1494discussed in Section 1.In contrast, the mixture models do much better.In fact, LM+Selection model produces better resultsthan trigram LM alone.
We also note that estimat-ing the mixture parameter on the training data ratherthan using a fixed value increases TypRed scores:14.02 with a fixed ?select = 0.5 versus 15.18 with?
?select = 0.09.
This comparison also holds forc > 0 ?
that is, a naive version of LM+Selectionthat selects a word from the stimulus whenever theprefix allows would not have worked well.In principle the LM+Topic model is potentiallymore powerful in that P (w | s) is not limited tothe words in s. However, in our experiments itdoes not yield any considerable improvement overthe original LM.
We postulate that this could be dueto the following reason: once the context providedby s is reduced to the topic level, it is not specificenough to provide additional information over pro-ceding words in the response.Previous words from response + first c charactersof current word observed: Table 1 also comparesthe TypRed performance of all the models under set-tings where c > 0.
We notice striking improve-ments in performance for c = 1 which is consistentacross all models.
Our best model is able to savethe user approximately 23% in terms of typing ef-fort (according to TypRed scores).
Interestingly a lotless reduction was observed for c = 2: the secondcharacter, on average, does not improve the rankingenough to justify the cost of typing this extra char-acter.Next, we pick our best model (Mixture Model 1)and perform some further analysis.
We examine theeffect of providing longer list (shown in Table 2) andnotice little further improvement beyond k = 10.We also note that the TypRed improvement achievedover the baseline (LM) model at rank 10 is more thantwice the gain achieved at rank 5.We also evaluated its performance using a stan-dard measure (Recall@k).
Figure 2 plots the recallachieved by the system at different ranks k. An in-creasing recall at even high ranks (k = 100) sug-gests that the quality of the candidate list retrievedby this model is good.
This also suggests that thereis still room for improvements, and we leave that asinteresting future work.Rank (k) TypRed score1 9.025 15.1810 16.1415 16.2820 16.2925 16.29Table 2: Comparison of typing reductions achieved overthe entire test data when top k list is provided to the user.0 10 20 30 40 50 60 70  0102030405060708090100Recall @ kRank(k)Figure 2: Recall @ rank k for Mixture Model 1 on theentire test data.05101520253035  024681012Average TypRedWordlength(# of characters)no letter provided (c=0)first letter provided(c=1)first 2lettersprovided (c=2)Figure 3: Average TypRed score versus Word length (#of characters) for Mixture Model 1 when the first c char-acters of the word is typed in by the user.Finally, in Figure 3, we plot the average TypRedscores against individual token (word) length.
Fig-ure 3 indicates that the model is able to achieve ahigher reduction for shorter words compared to verylong ones.
This demonstrates the utility of such a re-sponse completion system, especially since shorterwords are predominant in conversational settings.We also compared the average reduction achievedon messages of different lengths (number of words).Overall we observe consistent reduction for differ-ent message lengths.
This suggests our system can1495Source Top translations:) :) !
you ?
:Dlmao lol lmao u ... ifeeling feeling feel better !
youquestion .
question the , toAre I .
, are yesTable 3: Examples of a few stimulus/response transla-tions learned using IBM Model-1.OBAMA, USA, Fact, Meghan, GIVE, PRESIDENT, Canadian,Mitch, Jon, Kerry, TODAY, Justice, Liberalism, ...President, Notice, Tax, LMAO, Hmmm, Trump, people,OBAMA, common, Aren, WAIT, Bachman, mon, McCain, ...Great, Cut, Release, Ummm, Rest, Mark, isnt, YAHOO, Sad,END, RON, jesus, Ugh, TRUMP, ...Nice, Navy, Make, Interesting, Remember, Excuse, WAKE,Hooray, Birth, mon, Yeah, Dumb, Michael, geronimo, ...Table 4: Examples of top representative words for a fewtopics generated by the LDA topic model trained on newscomment data.be useful for both Tweet-like short messages as wellas more lengthy exchanges in detailed discussions.5.4 DiscussionTable 3 displays some sample translations learnedusing the TM model described in Section 3.3.
In-terestingly, emoticons and informal expressions like:) or lmao in the stimulus tend to evoke similartype of expressions in the response (as seen in Ta-ble 3).
Some translations (e.g., feeling?
better) areindicative of question/answer type of exchanges inour data.
But most of the other translations are noisyor uninformative (e.g., Are?
.).
This provides fur-ther evidence as to why a translation-based approachis not well suited for this particular task and hencedoes not perform as well as other methods.Finally, in Table 4, we provide a few sample top-ics generated by the LDA topic model (which is usedby Mixture Model 2 described in Section 3.4).
Wefind that while a few topics display some seman-tic coherence (e.g., political figures), many of themare noisy (or too generic) which further supports ourearlier observation that they are not useful enough tohelp in the prediction task.6 Entropy of user commentsWe adapt the notion of predictability of English asexamined by Shannon (1951) from letter-predictionto token-prediction, and define the predictability ofEnglish as how well can the next token be predictedwhen the precedingN tokens are known.
How muchdoes the immediate context in the response help re-duce the uncertainty?
How does user-generated con-tent compare with more formal English in this re-spect?
And how about the corresponding stimuli ?given the preceding N tokens, does the knowledgeof stimulus further reduce the uncertainty?
Thesequestions motivated a series of studies over entropyin different datasets.46.1 Comparison of N -gram entropyFollowing Shannon (1951), we consider the follow-ing function FN , which can be called the N -gramentropy, as the measure of predictability:FN = ?
?i,jp(bi, j) log2 p(j | bi)where bi is a block of N ?
1 tokens, j is an arbitrarytoken following bi, and p(j | bi) is the conditionalprobability of j given bi.
This conditional entropyreflects how much is the uncertainty of the next to-ken reduced by knowing the precedingN?1 tokens.Under this measure, is user-generated contentmore predictable or less predictable than the moreformal ?printed?
English examined by Shannon?Maybe it is more predictable, since most users ininformal settings use simpler English, which maycontain fewer variations than the complex structuresobserved in more formal English.
Or perhaps it isless predictable ?
variations among different users(who may not follow proper grammar) may leadto more uncertainty in the prediction of ?the nextword?.
Which would be the case?To answer this question empirically, we constructa reference dataset written in more formal English(Df ) to be compared against the user commentsdataset described in Section 4 (Du).
If Df coversvery different topics fromDu, then even if we do ob-serve differences in entropy, it could be due to topi-cal differences.
A standard mixed-topic dataset like4Note that our findings are not to be interpreted as predictionperformance over unseen texts.
For that, one needs to computecross-entropy between training and test corpora.
Since Section5 is already addressing this question with proper training / testsplit, in this section, we focus on the variability of languageusage in a corpus.
This also avoids having to control for ?com-parable?
training/test splits in different types of datasets.1496the Brown Corpus (Kucera and Francis, 1967) maynot be ideal in this sense (e.g., it contains fiction cat-egories such as ?Romance and Love Story?, whichmay not be represented in our Du).
Instead, we ob-tained a sample of news articles on Yahoo!
Newsduring March - May, 2011, and extracted uniquesentences from these articles.
This yields a Df withmore comparable subject matters to Du.5Next, we compare both the entropy over unigramsand N -gram entropy in three datasets: the news ar-ticle dataset described above, and comments data(Section 4) separated into stimuli and responses.
Wealso report corresponding numbers computed on theBrown Corpus as references.
Note that datasets withdifferent vocabulary size can lead to different en-tropy: the entropy of picking a word from the vo-cabulary uniformly at random would have been dif-ferent.
Thus, we sample each dataset at differentrates, and plot the (conditional) entropy in the sam-ple against the corresponding vocabulary size.As shown in Figure 4(a), the entropy of unigramsin Du (both stimuli and responses) is consistentlylower than in Df .6 On the other hand, both stimuliand responses exhibit higher uncertainty in bigramentropy (Figure 4(b)) and trigram entropy (Figure4(c)).
That is, when no contexts are provided, wordchoices (from similarly-sized vocabularies) in Df ismore evenly distributed than in Du; but once theproceeding words are given, the next word is morepredictable in Df than in Du.
We postulate thatthe difference in unigram entropy could be due to(a) more balanced topic coverage in Df vs. moreskewed topic coverage in Du, or (b) professional re-porters mastering a more balanced use of the vocab-ulary.
If (b) is the main reason, however, the lowertrigram entropy in Df would seem unexpected ?shouldn?t professional journalists also have a morebalanced use of different phrases?
Upon furthercontemplation, what we hypothesized earlier couldbe true: professional writers use the ?proper?
En-glish expected in news coverage, which could limit5We note that this does not guarantee the exact same topicdistribution as in the comment data.6For reference, Shannon (1951) estimated the entropy of En-glish to be 11.82 bits per word, due to an incorrect calculationof a 8727-word vocabulary given Zipf distribution.
The correctnumber should be 9.27 bits per word for a vocabulary size of12,366 (Yavuz, 1978).99.51010.51110000  100000  1e+06entropyvocabulary sizebrown corpusnews articlesstimulusresponse(a) Entropy of unigrams44.555.566.577.510000  100000  1e+06conditional entropyvocabulary sizebrown corpusnews articlestimulusresponse(b) Bigram entropy (F2)11.522.533.544.5510000  100000  1e+06conditional entropyvocabulary sizebrown corpusnews articlestimulusresponse(c) Trigram entropy (F3)Figure 4: Entropy of unigrams and N -gram entropy.their trigram uncertainties; on the other hand, usersare not bound by conventions (or even grammars),which could lead to higher variations.Interestingly, distributions in the stimulus datasetare closer to news articles: they have a higher uni-gram entropy than responses, but a lower trigram en-tropy at comparable vocabulary sizes.
In particular,recall from Section 4 that our comments dataset con-tains roughly 237K repliers and 357K original com-149701234567810000  100000  1e+06conditionalentropyvocabulary sizebigram (F 2 )bigram+stimulus (G 2 )trigram (F 3 )Figure 5: Predicting the next word in responses: bigramentropy vs. bigram+stimulus entropy vs. trigram entropy.menters.
If higher trigram entropy is due to varianceamong different users, the stimulus dataset shouldhave had a higher trigram entropy.
We leave an ex-planation of this interesting behavior as future work.6.2 Information in stimuliWe now examine the next question: does knowingwords in the stimulus further reduce the uncertaintyof the next word in the response?
For simplicity,we model the stimulus as a collection of unigrams.Consider the following conditional entropy:GN = ?
?i,k,jp(bi, j, sk) log2 p(j | bi, sk)where bi is a block of N ?
1 tokens in a responser, j is an arbitrary token following bi, and sk is anarbitrary token in the corresponding stimulus s for r.Note that for each bi, we consider every token in thecorresponding s. That is, a (stimulus, response) pairwithm and n tokens respectively generatesm?
(n?N +1) observations of (bi, j, sk) tuples.
We refer tothis as the N -gram+stimulus entropy.
If knowing skin addition to bi does not provide extra information,then p(j | bi, sk) = p(j | bi), and GN = FN .Figure 5 plots GN for N = 2.
Interestingly, weobserve F2 > G2 > F3 (this trend holds for largervalues ofN , omitted here for clarity).
That is, know-ing both the preceding N ?
1 tokens and tokens inthe stimulus results lowers the uncertainty over thenext token in response (bigram+stimulus entropy <bigram entropy); on the other hand, this is not as ef-fective as knowing one more token in the precedingblock (trigram entropy< bigram+stimulus entropy).Note that from the model size perspective, mod-eling p(j | bi, sk) as in GN would have been muchmore expensive than p(j | bi) in FN+1.
Take thecase of G2 vs. F3.
Let V be the vocabulary ofuser comments (ignore for now differences in re-sponses and stimuli).
While both seem to requirecomputations over V ?
V ?
V , the number ofunique observed (bi, j, sk) tuples for G2 (i.e., num-ber of unique bigrams in responses paired up withunigrams in corresponding stimuli) is 725,458,892,whereas the number of unique observed (bi, j) pairsfor F3 (i.e., number of unique trigrams) is only14,692,952.
This means modeling trigrams wouldresult in a model 2% the size of bigram+stimulus,yet it could achieve better reduction in uncertainty.Note that in order to reduce model complex-ity, the models proposed in Section 3 all brokedown P (ri+1 | s, r1..i) into independent componentsP (ri+1 | s) and P (ri+1 | r1..i), rather than model-ing the effect of s and r1..i jointly as the underlyingmodel corresponding to GN .
Indeed, it would havebeen impractical to model p(j | bi, sk) directly.
Ourstudies confirmed the validity of this choice: even ifwe look at the performance on the training data itself(i.e., igoring data sparseness issues), the smaller tri-gram model would have yielded better results thanthe significantly more expensive bigram+stimulusmodel.
Still, since GN shows a consistent improve-ment over FN , there could be more information inthe stimulus that we are not yet fully utilizing, whichcan be interesting future work.7 ConclusionsIn this paper, we examined a novel application: au-tomatic response completion in conversational set-tings.
We investigated the effectiveness of severalmodels that incorporate contextual information pro-vided by the partially typed response as well as thestimulus.
We found that the partially typed responseprovides strong signals.
In addition, using a mix-ture model which also incorporates stimulus contentyielded the best overall result.
We also performedempirical studies to examine the predictability ofuser-generated content.
Our analysis (entropy es-timates along with upper-bound numbers observedfrom experiments) suggest that there can be interest-ing future work to explore the contextual informa-tion provided by the stimulus more effectively andfurther improve the response completion task.1498ReferencesRegina Barzilay and Mirella Lapata.
2005.
Modelinglocal coherence: An entity-based approach.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, ACL?05.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
Journal of MachineLearning Research, 3:993?1022.Peter F. Brown, Vincent J. Della Pietra, Robert L. Mercer,Stephen A. Della Pietra, and Jennifer C. Lai.
1992.
Anestimate of an upper bound for the entropy of English.Comput.
Linguist., 18:31?40.Ivan Bulyko, Mari Ostendorf, and Andreas Stolcke.2003.
Getting more mileage from web text sources forconversational speech language modeling using class-dependent mixtures.
In Proceedings of the 2003 Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics on Human Lan-guage Technology: companion volume of the Proceed-ings of HLT-NAACL 2003?short papers - Volume 2,NAACL-Short ?03, pages 7?9.Thomas M. Cover and Roger C. King.
1978.
A con-vergent gambling estimate of the entropy of English.IEEE Transactions on Information Theory, 24:413?421.Steve Farmer, Richard Sproat, and Michael Witzel.
2004.The collapse of the Indus-script thesis: The myth of aliterate Harappan civilization.
Electronic Journal ofVedic Studies, 11:379?423 and 623?656.Yijue How and Min-Yen Kan. 2005.
Optimizing pre-dictive text entry for short message service on mobilephones.
In Proceedings of the Human Computer In-terfaces International (HCII).Christina L. James and Kelly M. Reischel.
2001.
Textinput for mobile devices: comparing model predictionto actual performance.
In Proceedings of the SIGCHIconference on Human factors in computing systems,CHI ?01, pages 365?371, New York, NY, USA.
ACM.Henry Kucera and W. Nelson Francis.
1967.
Com-putational analysis of present-day American English.Brown University Press.I.
Scott MacKenzie and R. William Soukoreff.
2002.Text entry for mobile computing: Models and meth-ods,theory and practice.
Human-Computer Interac-tion, 17(2-3):147?198.Qiaozhu Mei and Kenneth Church.
2008.
Entropy ofsearch logs: how hard is search?
with personalization?with backoff?
In Proceedings of the international con-ference on Web search and web data mining, WSDM?08, pages 45?54.Hamid Moradi, Jerzy W. Grzymala-busse, and James A.Roberts.
1998.
Entropy of english text: experimentswith humans and a machine learning system based onrough sets.
Information Sciences, 104:31?47.Petteri Nurmi, Andreas Forsblom, Patrik Flore?en, PeterPeltonen, and Petri Saarikko.
2009.
Predictive textinput in a mobile shopping assistant: methods and in-terface design.
In Proceedings of the 14th interna-tional conference on Intelligent user interfaces, IUI?09, pages 435?438, New York, NY, USA.
ACM.Rajesh P. N. Rao, Nisha Yadav, Mayank N. Vahia,Hrishikesh Joglekar, R. Adhikari, and Iravatham Ma-hadevan.
2009.
Entropic evidence for linguistic struc-ture in the Indus script.
Science.Alan Ritter, Colin Cherry, and William B. Dolan.
2011.Data-driven response generation in social media.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 583?593.Ronald Rosenfeld.
2000.
Two decades of statistical lan-guage modeling: Where do we go from here?
Pro-ceedings of the IEEE, 88.Claude E. Shannon.
1948.
A mathematical theoryof communication.
Bell System Technical Journal,27:379?423 and 623?656.Claude E. Shannon.
1951.
Prediction and entropyof printed English.
Bell System Technical Journal,30:50?64.W.
J. Teahan and John G. Cleary.
1996.
The entropy ofEnglish using PPM-based models.
In In Data Com-pression Conference, pages 53?62.
IEEE ComputerSociety Press.Joseph Weizenbaum.
1966.
Eliza: a computer programfor the study of natural language communication be-tween man and machine.
Commun.
ACM, 9:36?45.D.
Yavuz.
1978.
Zipf?s law and entropy (Corresp.
).IEEE Transactions on Information Theory, 20:650.Xing Yi and James Allan.
2009.
A comparative studyof utilizing topic models for information retrieval.
InProceedings of the European Conference on IR Re-search on Advances in Information Retrieval, pages29?41.1499
