Coling 2010: Poster Volume, pages 525?533,Beijing, August 2010A Comparative Study on Ranking and Selection Strategies forMulti-Document SummarizationFeng Jin, Minlie Huang, Xiaoyan ZhuState Key Laboratory of Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDept.
of Computer Science and Technology, Tsinghua Universityjinfengfeng@gmail.com,{aihuang,zxy-dcs}@tsinghua.edu.cnAbstractThis paper presents a comparative studyon two key problems existing in extrac-tive summarization: the ranking problemand the selection problem.
To this end,we presented a systematic study ofcomparing different learning-to-rank al-gorithms and comparing different selec-tion strategies.
This is the first work ofproviding systematic analysis on theseproblems.
Experimental results on twobenchmark datasets demonstrate threefindings: (1) pairwise and listwise learn-ing-to-rank algorithms outperform thebaselines significantly; (2) there is nosignificant difference among the learn-ing-to-rank algorithms; and (3) the in-teger linear programming selectionstrategy generally outperformed Maxi-mum Marginal Relevance and DiversityPenalty strategies.1 IntroductionAs the rapid development of the Internet, docu-ment summarization has become an importanttask since document collections are growinglarger and larger.
Document summarization,which aims at producing a condensed version ofthe original document(s), helps users to acquireinformation that is both important and relevantto their information need.
So far, researchershave mainly focused on extractive methodswhich choose a set of salient textual units toform a summary.
Such textual units are typical-ly sentences, sub-sentences (Gillick and Favre,2009), or excerpts (Sauper and Barzilay, 2009).Almost all extractive summarization methodsface two key problems: the first problem is howto rank textual units, and the second one is howto select a subset of those ranked units.
Theranking problem requires systems model therelevance of a textual unit to a topic or a query.In this paper, the ranking problem refers to ei-ther sentence ranking or concept ranking.
Con-cepts can be unigrams, bigrams, semantic con-tent units, etc., although in our experiment, onlybigrams are used as concepts.
The selectionproblem requires systems improve diversity orremove redundancy so that more relevant in-formation can be covered by the summary as itslength is limited.
As our paper focuses on ex-tractive summarization, the selection problemrefers to selecting sentences.
However, the se-lection framework presented here is universalfor selecting arbitrary textual units, as discussedin Section 4.There have been a variety of studies to ap-proach the ranking problem.
These include bothunsupervised sentence ranking (Luhn, 1958;Radev and Jing, 2004, Erkan and Radev, 2004),and supervised methods (Ouyang et al, 2007;Shen et al, 2007; Li et al, 2009).
Even given alist of ranked sentences, it is not trivial to selecta subset of sentences to form a good summarywhich includes diverse information within alength limit.
Three common selection strategieshave been studied to address this problem: Max-imum Marginal Relevance (MMR) (Carbonelland Goldstein, 1998), Diversity Penalty (DiP)(Wan, 2007), and integer linear programming(ILP) (McDonald, 2007; Gillick and Favre,2009).
As different methods were often eva-luated on different datasets, it is of great valueto systematically compare ranking and selectionstrategies on the same dataset.
However, to thebest of our knowledge, there is still no work tocompare different ranking strategies or comparedifferent selection strategies.In this paper, we presented a comparativestudy on the ranking problem and the selection525problem for extractive summarization.
Wecompared three genres of learning-to-rank me-thods for ranking sentences or concepts: SVR, apointwise ranking algorithm; RankNet, a pair-wise learning-to-rank algorithm; and ListNet, alistwise learning-to-rank algorithm.
We adoptedan ILP framework that is able to select sen-tences based on sentence ranking or conceptranking.
We compared it with other selectionstrategies such as MMR and Diversity Penalty.We conducted our comparative experiments onthe TAC 2008 and TAC 2009 datasets, respec-tively.
Our contributions are two-fold: First, tothe best of our knowledge, this is the first workof presenting systematic and in-depth analysison comparing ranking strategies and comparingselection strategies.
Second, this is the firstwork using pairwise and liswise learning-to-rank algorithms to perform concept (word bi-gram) ranking for extractive summarization.The rest of this paper is organized as follows.We introduce the related work in Section 2.
InSection 3, we present three ranking algorithms,SVR, RankNet, and ListNet.
We describe thesentence selection problem with an ILP frame-work described in Section 4.
We introduce fea-tures in Section 5.
Evaluation and experimentsare presented in Section 6.
Finally, we concludethis paper in Section 7.2 Related WorkA number of extractive summarization studiesused unsupervised methods with surface fea-tures, linguistic features, and statistical featuresto guide sentence ranking (Edmundson, 1969;McKeown and Radev, 1995; Radev et al, 2004;Nekova et al, 2006).
Recently, graph-basedranking methods have been proposed for sen-tence ranking and scoring, such as LexRank(Erkan and Radev, 2004) and TextRank (Mihal-cea and Tarau, 2004).There are also a variety of studies on su-pervised learning methods for sentence rankingand selection.
Kupiec et al (1995) developed anaive Bayes classifier to decide whether a sen-tence is worthy to extract.
Recently, ConditionalRandom Field (CRF) and Structural SVM havebeen employed for single document summariza-tion (Shen et al, 2007; Li et al, 2009).Besides ranking sentences directly, there aresome approaches that select sentences based onconcept ranking.
Radev et al (2004) used cen-troid words whose tf*idf scores are above athreshold.
Filatova and Hatzivassiloglou (2004)used atomic event as concept.
Moreover, sum-marization evaluation metrics such as BasicElement (Hovy et al, 2006), ROUGE (Lin andHovy, 2003) and Pyramid (Passonneau et al,2005) are all counting the concept overlap be-tween generated summaries and human-writtensummaries.Another important issue existing in extractivesummarization is to find an optimal sentencesubset which can cover diverse information.Maximal Marginal Relevance (MMR) (Carbo-nell and Goldstein, 1998) and Diversity Penalty(Wan, 2007) are most widely used approachesto reduce redundancy.
The two methods are es-sentially based on greedy search.
By contrast,ILP based approaches view summary generationas a global optimization problem.
McDonald(2007) proposed a sentence-level ILP solution.Sauper and Barzilay (2009) presented an ex-cerpt-level ILP method to generate Wikipediaarticles.
Gillick and Favre (2009) proposed aconcept-level ILP, but they used document fre-quency to score concepts (bigrams), without anylearning process.
Some recent studies (Gillickand Favre, 2009; Martins and Smith, 2009) alsomodeled sentence selection and compressionjointly using ILP.
Our ILP framework proposedhere is based on these studies.
Although variousselection strategies have been proposed, there isno work to systematically compare these strate-gies yet.Learning to rank attracts much attention inthe information retrieval community recently.Pointwise, pairwise and listwise learning-to-rank approaches have been extensively studied(Liu, 2009).
Some of those have been applied todocument summarization, such as SVR(Ouyang et al, 2007), classification SVM(Wang et al, 2007), and RankNet (Svore et al,2007).
Again, there is no work to systematicallycompare these ranking algorithms.
To the bestof our knowledge, this is the first time that alistwise learning-to-rank algorithm, ListNet(Cao et al, 2007), is adapted to document sum-marization in this paper.
Moreover, pairwiseand listwise learning-to-rank algorithms havenever been used to perform concept ranking forextractive summarization.5263 Ranking Sentences or ConceptsGiven a query and a collection of relevant doc-uments, an extractive summarization system isrequired to generate a summary consisting of aset of text units (usually sentences).
The firstproblem we need to consider is to determine theimportance of these sentences according to theinput query.
We approach this ranking problemin two ways: the first way is to score sentencesdirectly using learning-to-rank algorithms, andthus the goal of summarization is to select asubset of sentences, considering both relevanceand redundancy.
The second way is to scoreconcepts within the document collection, andthen the summarization task is to select a sen-tence subset that can cover those important con-cepts maximally.
The problem of sentence se-lection will be described in Section 4.Suppose the relevant document collection fora query q is Dq.
From this collection, we obtaina set of sentences or concepts (e.g., word bi-grams), S={s1,s2,?,sn} or C={c1,c2,?, cn}.
Be-fore training, each si or ci is associated with agold standard score, yi.
A feature vector, xj=?
(sj/cj,q,Dq), is constructed for each sentence orconcept.
The learning algorithm will learn aranking function f(xj) from a collection ofquery-document pairs {(qi,Dqi)|i= 1, 2,?,m}.We investigated three learning-to-rank me-thods to learn f(xj).
The first one is a pointwiseranking algorithm, support vector regression(SVR).
This algorithm treats sentences (or con-cepts) independently.
The second method is apairwise ranking algorithm, RankNet, whichlearns a ranking function from a list of sentence(or concept) pairs.
Each pair is labeled as 1 ifthe first sentence si (or concept ci) ranks aheadof the second sj (or cj), and 0 otherwise.The listwise ranking algorithm, ListNet,learns the ranking function f(xj) in a differentway.
A list of sentences (or concepts) is treatedas a whole.
Both RankNet and ListNet take intoaccount the dependency between sentences (orconcepts).3.1  Support Vector RegressionSupport Vector Regression (SVR), a generaliza-tion of the classical SVM formulation, attemptsto learn a regression model.
SVR has been ap-plied to summarization in (Ouyang et al, 2007;Metzler and Kanungo, 2008).
In our work, wetrain the SVR model to fit the gold standardscore of each sentence or concept.Formally, the objective of SVR is to minim-ize the following objective:2, ,1 1|| || ( ( ))2ii ixw bw C v L y f xN???
( ) =?
??
??
?+ ?
+ ??
??
??
??
??
??
(1)where L(x)=|x|-?
if x > ?
and otherwise L(x)=0;yi is the gold standard score of xi; f(x) =wTx+b,the predicted score of x; C and v are two para-meters; and N is the total number of trainingexamples.3.2 RankNetRankNet is a pairwise learning-to-rank method(Burges et al, 2005).
In this algorithm, trainingexamples are handled pair by pair.
Given a pairof feature vectors (xi, xj), the gold standardprobability ijP is set to be 1 if the label of thepair is 1, which means xi ranks ahead of xj.
Thegold standard probability is 0 if the label of thepair is 0.
Then the predicted probability Pij,which defines the probability of xi rankingahead of xj by the model, is represented as a lo-gistic function:exp( ( ) ( ))1 exp( ( ) ( ))i jiji jf x f xPf x f x?=+ ?
(2)where f(x) is the ranking function.
The objectiveof the algorithm is to minimize the cross entro-py between the gold standard probability andthe predicted probability, which is defined asfollows:( ) log (1 ) log(1 )ij ij ij ij ijC f P P P P= ?
?
?
?
(3)A three-layer neural network is used as theranking function, as follows:3 32 2 21 2 3( ) ( ( ) )n ij jk nk j ij kf x g w g w x b b= + +?
?
(4)where for weights w and bias b, the superscriptsindicate the node layer while the subscripts in-dicate the node indexes within each layer.
Andxnk is the k-th component of input feature vectorxn.
Then a gradient descent method is used tolearn the parameters.
For details, refer to(Burges et al, 2005).3.3 ListNetListNet takes a list of items as input in the learn-ing process.
More specifically, suppose we have527a list of feature vectors (x1, x2,?, xn) and eachfeature vector xi has an gold standard score yi,which has been assigned before training.
Ac-cordingly, we have a list of gold standard scores(y1, y2,?,yn).
We also have a list of scores as-signed by the algorithm during training, say,(f(x1), f(x2),?, f(xn)).
Given a score listS={s1,s2,?,sn}, the probability that xj will rankthe first place among the n items is defined asfollows:1 1( ) exp( )( )( ) exp( )j js n nk kk ks sP js s= =?= =??
?
(5)It is easy to prove that (Ps(1), Ps(2), ?, Ps(n)) isa probability distribution, as the sum of themequals to 1.
Therefore, the cross entropy can beused to define the loss between the gold stan-dard distribution Py(j) and the distribution Pf(j),as follows:1( , ) ( ) log ( )ny fjL y f P j P j== ??
(6)where y represents the gold standard score list(y1, y2,?,yn) and f=(f(x1), f(x2),?, f(xn)) is thescore list output by the ranking algorithm.The function f is defined as a linear function,as follows:( ) Tw i if x w x=                          (7)Then the gradient of loss function L(y,f) withrespect to the parameter vector w can be calcu-lated as follows:111( )( , )( )( )1exp( ( ))exp( ( ))nw jwy jjnw jw jnjw jjf xL y fw P xw wf xf xwf x===???
= = ??
??+????
(8)During training, w is updated in a gradient des-cent manner: w=w -?
?w and ?
is the learningrate.
For details, refer to (Cao et al, 2007).4 ILP-based Selection FrameworkAfter we have a way of ranking sentences orconcepts, we face a sentence selection problem:selecting an optimal subset of sentences as thefinal summary.
To integrate sentence/conceptranking, we adopted an integer linear program-ming (ILP) framework to find the optimal sen-tence subset (Filatova and Hatzivassiloglou,2004; McDonald, 2007; Gillick and Favre, 2009;Takamura and Okumura, 2009).
ILP is a globaloptimization problem whose objective and con-straints are linear in a set of integer variables.Formally, we define the problem of sentenceselection as follows:maximize:  ( )*  xi iif x z?
??
??
??
(9).
.
* | |       uj jjz u Lims t ??
* ( , ) ,          u xj ijz I i j z i?
??
( )* ( , )   ,    x xi j i jz z sim x x i j?+ < ?, {0,1},      ,x ui jz z i j?
?where:xi ?
the representation unit, such as a sentenceor a concept.
We term it representation unit be-cause the summary quality is represented by theset of included xi;f(xi) - the ranking function given by the learn-ing-to-rank algorithms;uj - the selection unit, for instance, a sentence inthis paper.
|uj| is the number of words in uj;xiz - the indicator variable which denotes thepresence or absence of xi in the summary;ujz - the indicator variable which denotes inclu-sion or exclusion of uj;I(i, j) - a  binary constant indicating that wheth-er xi appears in uj.
It is either 1 or 0;Lim - the length limit;sim(xi, xj) - a similarity measure for consideringthe redundancy;?
- the redundancy threshold.The first constraint indicates the length limit.The second constraint asserts that if a represen-tation unit xi is included in a summary, at leastone selection unit that contains xi must be se-lected.
The third constraint considers redundan-cy.
If the representation unit is sentence, thesimilarity measure is defined as tf*idf similarity,and ?/2 is the similarity threshold, which wasset to be 1 here.
For concepts, the similaritymeasure can be defined as1,( , )0,    otherwisei ji jx xsim x x=?= ??
.However, other definition is also feasible, de-pending on what has been selected as represen-tation unit.528Note that this framework is very general.
Ifthe representation unit xi is a sentence, the rank-ing function is defined on sentence.
Thus theILP framework will find a set of sentences thatcan optimize the total scores of selected sen-tences, subject to several constraints.
If the re-presentation unit is a concept, the ranking func-tion measures the importance of a concept to beincluded in a summary.
Thus the goal of ILP isto find a set of sentences by maximizing thescores of concepts covered by those selectedsentences.Dq relevant document collection in responseto query qd one single documentwi unigramwiwi+1 bigramS sentencetfd(wi) the frequency of wi occurring in ddfD(wi) the number of documents containing wiin collection DTable 1.
Notations for features.5 FeaturesTo facilitate the following description, somenotations are defined in Table 1.
In our dataset,each query has a title and narrative to preciselydefine an information need.
The following is aquery example from the TAC 2008 test dataset:<topic id = "D0801A"><title> Airbus A380 </title><narrative>Describe developments in the production andlaunch of the Airbus A380.</narrative></topic>Features for sentence ranking and conceptranking are listed in the following.
We use wordbigrams as concept here.Sentence Features(1) Cluster frequency: ( )qiD iw Stf w??
(2) Title frequency: ( )id iw Stf w??
where d is anew document that consists of all the titles ofdocuments in Dq.
(3) Query frequency: ( )id iw Stf w??
where d isa document consisting of the title and narrativefields of the current topic.
(4) Theme frequency: ( )qi iD iw S w Ttf w?
?
?
?where T is the top 10% frequent unigram wordsin Dq.
(5) Document frequency of bigrams in the sen-tence:11( )i iD i iw w Sdf w w++??
.
(6) PageRank score: as described in (Mihalceaand Tarau, 2004), each sentence in Dq is a nodein the graph and the cosine similarity between apair of sentences is used as edge weight.Concept Features(1) Cluster frequency: 1( )qD i itf w w + , the fre-quency of 1i iw w + occurring in Dq.
(2) Title frequency: 1( )d i itf w w + , where d is adocument consisting of all the titles of docu-ments in Dq.
(3) Query Frequency: the frequency of the bi-gram occurring in the topic title and narrative.
(4) Average term frequency:1( )/ | |qd i i qd Dtf w w D+??
.
|Dq| is the number ofdocuments in the set.
(5) Document frequency: the document fre-quency of this bigram.
(6) Minimal position: the minimal position ofthis bigram relative to the document length.
(7) Average position: the average position ofthis bigram in collection Dq .6 Experimental Results6.1 Data PreprocessingWe conducted experiments on the TAC 2008and TAC 2009 datasets.
The task requires pro-ducing a 100-word summary for each query (al-so termed topic sometimes).
There are 48 que-ries in TAC 2008 and 44 queries in TAC 2009.A query example has been given in Section 5.Relevant documents for these queries have beenspecified.
And four human-written summarieswere supplied as reference summaries for eachquery.We segmented the relevant documents intosentences using the LingPipe toolkit 1  andstemmed words using the Porter Stemmer.Word bigrams are used as concepts in this paper.If the two words in a bigram are both stop-words, the bigram will be discarded.
The sen-1 http://alias-i.com/lingpipe/index.html529tence features and bigram features are then cal-culated.
As our focus is on comparing differentranking strategies and selection strategies, wedid not apply any sophisticated linguistic or se-mantic processing techniques (as pre- or post-processing).
Thus we did not compare our re-sults to those submitted to the TAC conferences.We train the learning algorithms on one data-set and then evaluate the algorithms on the other.The generated summaries are evaluated usingthe ROUGE toolkit (Lin and Hovy, 2003).6.2 Preparing Training SamplesAs our work includes both sentence ranking andconcept ranking, we need to establish two typesof training data.
Fortunately, we are able to dothis based on the reference summaries and an-notation results provided by the TAC confe-rences.For the sentence ranking problem, we com-pute the average ROUGE-1 score for each sen-tence by comparing it to the four referencesummaries for each query.
This score is treatedas the gold-standard score.
In ListNet, thesescores are directly used (see formula (5)).
Whilein RankNet, the sentences for a query aregrouped into 10 bins according to theirROUGE-1 scores, and then we extract sentencesfrom different bins respectively to form a pair.We assume that a sentence in a higher scoredbin should rank ahead of those sentences inlower scored bins.As for the concept ranking problem, gold-standard scores are obtained from the humanannotated Pyramid data.
The weight of eachsemantic content unit (SCU) is the number ofreference summaries in which the SCU appears.So straightforwardly, the gold-standard score ofa bigram is the largest weight of all SCUs thatcontain the bigram.
And if a bigram does notoccur in any SCU, its score will be 0.
Thus thebigram scores belong to the set {0,1,2,3,4} asthere are four human-written summaries foreach query.
These scores are directly used inListNet (see formula (5)).
And in RankNet, bi-gram pairs are constructed according to thegold-standard scores.6.3 Learning ParametersFor SVR, the radial basis kernel function is em-ployed and the optimal values for parameters C,v and g (for the kernel) are found using the gri-dregression.py tool provided by LibSVM(Chang and Lin, 2001) with a 5-fold cross vali-dation on the training set.RankNet applies a three-layer (one hiddenlayer) neural network with only one node in theoutput layer, as described in (Burges et al,2005).
The number of hidden neurons was em-pirically set to be 10.
The learning rate was setto 0.001 for sentence ranking and 0.01 for bi-gram ranking.As for ListNet, the learning rate for sentenceranking and concept ranking are both set to be0.1 empirically.6.4 Comparing Ranking StrategiesIn this section, we compared different rankingstrategies for both sentence ranking and conceptranking.
The sentence selection strategies werefixed to the ILP selection framework as shownin Section 4.
We chose ILP as the selectionstrategy because we want to compare our sys-tem with the following two methods (as base-lines):(1) SENT_ILP: A sentence-level method pro-posed by McDonald (2007) with ILP formula-tion.
We implemented the query-focused ver-sion of the formulae as TAC 2008 and 2009required query-focused summarization.
(2) DF_ILP: A concept-level ILP method usingdocument frequency to score word bigrams(Gillick and Favre, 2009), without any learningprocess.The differences between our framework andSENT_ILP are: a) SENT_ILP used a redundan-cy factor in the objective function whereas wemodeled redundancy as constraints; b)SENT_ILP used tf*idf similarity to computerelevance scores whereas we used learning algo-rithms.The ROUGE-1 and ROUGE-2 measures foreach method are presented in Table 2 and Table3.
Note that the performance on the TAC 2008dataset was obtained from the models that weretrained on the TAC 2009 dataset.
Then, the da-tasets were interchanged for training and testing,respectively.
Different learning-to-rank strate-gies (SVR, RankNet, ListNet) do not show sig-nificant differences between one and another,but they all outperform SENT_ILP substantially(p-value < 0.0001).
And for concept ranking,RankNet and ListNet both achieve significantlybetter ROUGE-2 results (p-value < 0.005) than530DF_ILP.
This infers that considering more fea-tures will have better results than using docu-ment frequency to score concepts.
The Wilcox-on signed-rank test (Wilcoxon, 1945) is used forsignificance tests in our experiment.
A goodranking strategy for modeling relevance is im-portant for extractive summarization.
RankNetwhich used a three-layer network (non-linearfunction) as the ranking function performsslightly better than ListNet which is based on alinear ranking function.Dataset Method ROUGE-1 ROUGE-2TAC2008SVR 0.35086 0.08447RankNet 0.36025 0.09291ListNet 0.35365 0.09129SENT_ILP 0.31546 0.06500TAC2009SVR 0.36125 0.09659RankNet 0.36216 0.09778ListNet 0.35480 0.09126SENT_ILP 0.31962 0.07034Table 2.
Results of sentence ranking strategies.Dataset Method ROUGE-1 ROUGE-2TAC 2008SVR 0.36555 0.10291RankNet 0.37564 0.11213ListNet 0.36863 0.10660DF_ILP 0.36922 0.10373TAC 2009SVR 0.37126 0.10698RankNet 0.37513 0.11364ListNet 0.37499 0.11313DF_ILP 0.36347 0.10156Table 3.
Results of concept ranking strategies.It is worth noting that Pyramid annotationsmay not cover all important bigrams, partly be-cause SCUs in reference summaries have beenrephrased by human annotators.
Note that wesimply extract original sentences to form asummary, thus it is possible that a bigram whichis important in the original sentences does notappear in any rephrased SCUs at all.
Such bi-grams will have a gold-standard score of 0,which is erroneous supervision.
For example,the bigrams hurricane katrina in topic D0804Aabout Katrina pet rescue and life support inD0806A about Terri Schiavo case are not anno-tated in any SCUs, but these bigrams are bothkey terms for the topics.6.5 Comparing Selection StrategiesIn order to study the influence of different selec-tion strategies, we compare the ILP selectionstrategy (as introduced in Section 4) with otherpopular selection strategies, based on the samesentence ranking algorithm (we chose sentence-level RankNet).
The baselines to be comparedare as follows:(1) MMR: As shown in (Carbonell andGoldstein, 1998), the formula of MMR is:{ }1 2arg max ( , ) (1 ) max ( , )i ji i js R S s SMMR D q s D s s?
??
?
?= ?
?where q is the given query; R is the set of allsentences; S is the set of already included sen-tences; D1 is the normalized ranking score f(xi)of si, and D2 is the cosine similarity of the fea-ture vectors for si  and sj.
Our implementationwas similar to the MMR strategy in theMEAD2summarizer.
(2) DiP: Diversity penalty which penalizes thescore of candidate sentences according to thealready selected ones (Wan, 2007).Dataset Method ROUGE-1 ROUGE-2TAC 2008ILP 0.36025 0.09291MMR 0.35459 0.09086DiP 0.35263 0.08689TAC 2009ILP 0.36216 0.09778MMR 0.35148 0.08881DiP 0.34714 0.08672Table 4.
Comparing selection strategies.The corresponding ROUGE scores are pre-sented in Table 4.
ILP outperforms other selec-tion strategies significantly on the TAC 2009dataset (both ILP vs. MMR and ILP vs. DiP).Although improvements are observed with ILPon the TAC 2008 dataset, the difference is notsignificant (using ILP vs. using MMR).
MMR iscomparable to DiP as they are both based ongreedy search in nature.To investigate the difference between thesestrategies, we present in-depth analysis here.First, the average length of summaries generat-ed by ILP is 97.1, while that by MMR and DiPare 95.5 and 92.7, respectively.
Note that therequired summary length is 100 and that morewords can potentially cover more information.Thus, ILP can generate summaries with moreinformation.
This is because ILP is a global op-timization algorithm, subject to the length con-straint.
Second, the average rank of sentencesselected by ILP is 12.6, while that by MMR and2 http://www.summarization.com/mead/531DiP is about 5, which is substantially different.ILP can search down the ranked list while theother two methods tend to only select the verytop sentences.
Third, there are 4.1 sentences onaverage in each ILP-generated summary, whilethe number for MMR and DiP generated sum-maries are 2.7 and 2.5, respectively.
Thus ILPtend to select shorter sentences than MMR andDiP.
This may help reduce redundancy as long-er sentences may contain more topic irrelevantclauses or phrases.6.6 DiscussionsInterestingly, although the learning-to-rank al-gorithms combined with the ILP selection strat-egy perform well in summarization, the perfor-mance is still far from that of manual summari-zation.
In this study, we investigate the upperbound performance.
We used the presented ILPframework to generate summaries based on thegold-standard scores, rather than the scores giv-en by the learning algorithms.
In other words,f(xi) in formula (9) is replaced by the gold-standard scores.
The ROUGE results are shownin Table 5.
We also listed the best/worst/averageROUGE scores of human summaries in TAC bycomparing one human summary (as generatedsummary) to the other three human summaries(as reference summaries).
These results are sub-stantially better than those by the learning algo-rithms.
Sentence- and concept- level rankingproduces very close results to best human sum-maries.
Some ROUGE-2 scores are even higherthan those of human summaries.
This is reason-able as human annotators may have difficulty inorganizing content when there are many docu-ments and sentences.
The results reflect thatthere is a remarkable gap between the gold-standard scores and the learned scores.Dataset Method ROUGE-1 ROUGE-2TAC2008Sentence-level 0.44216 0.14842Concept-level 0.42222 0.16018Human Best 0.44220 0.13079Human Average 0.41417 0.11606Human Worst 0.38005 0.10736TAC2009Sentence-level 0.45500 0.15565Concept-level 0.43526 0.17118Human Best 0.45663 0.14864Human Average 0.44443 0.12680Human Worst 0.39652 0.11109Table 5.
Upper bound performance.7 Conclusion and Future WorkWe presented systematic and extensive analysison studying two key problems in extractivesummarization: the ranking problem and theselection problem.
We compared three genres oflearning-to-rank algorithms for the rankingproblem, and investigated ILP, MMR, and Di-versity Penalty strategies for the selection prob-lem.
To the best of our knowledge, this is thefirst work of presenting systematic comparisonand analysis on studying these problems.
Wealso at the first time proposed to use learning-to-rank algorithms to perform concept ranking forextractive summarization.Our future work will focus on: (1) exploitingmore features that can reflect summary quality;(2) optimizing summarization evaluation me-trics directly with new learning algorithms.AcknowledgmentsThis work was partly supported by the ChineseNatural Science Foundation under grant No.60973104 and No.
60803075, and with the aidof a grant from the International DevelopmentResearch Center, Ottawa, Canada IRCI projectfrom the International Development.ReferencesChris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,Matt Deeds, Nicole Hamilton and Greg Hullender.2005.
Learning to Rank Using Gradient Descent.In Proceedings of the 22nd International Confe-rence on Machine Learning.Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai andHang Li.
2007.
Learning to Rank: from PairwiseApproach to Listwise Approach.
In Proceedingsof ICML 2007.Jaime Carbonell and Jade Goldstein.
1998.
The Useof MMR, Diversity-Based Reranking for Reorder-ing Documents and Producing Summaries.
InProceedings of SIGIR, August 1998, pp.
335 - 336.Chih-Chung Chang and Chih-Jen Lin.
2001.LIBSVM: a Library for Support Vector Machines.Software available athttp://www.csie.ntu.edu.tw/~cjlin/libsvmH.
P. Edmundson.
1969.
New Methods in AutomaticExtracting.
Journal of the ACM (JACM) Archive,Volume 16, Issue 2 (April 1969) Pages: 264 - 285.G.
Erkan and Dragomir R. Radev.
2004.
LexPage-Rank: Prestige in Multi-Document Text Summa-532rization.
In Proceedings of EMNLP 2004, Barce-lona, Spain.Elena Filatova and Vasileios Hatzivassiloglou.
2004.Event-based Extractive Summarization.
In Pro-ceedings of ACL Workshop on Summarization,volume 111.Dan Gillick and Benoit Favre.
2009.
A ScalableGlobal Model for Summarization.
In Proceedingsof the Workshop on Integer Linear Programmingfor Natural Language Processing.Eduard Hovy, Chin-yew Lin, Liang Zhou and Juni-chi Fukumoto.
2006.
Automated SummarizationEvaluation with Basic Elements.
In Proceedingsof the Fifth Conference on Language Resourcesand Evaluation.Julian Kupiec, Jan Pedersen and Francine Chen.1995.
A Trainable Document Summarizer.
InProceedings of SIGIR'95, pages 68 - 73, NewYork, USA.Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zhaand Yong Yu.
2009.
Enhancing Diversity, Cover-age and Balance for Summarization throughStructure Learning.
In Proceedings of the 18th In-ternational Conference on World Wide Web.Chin-Yew Lin and Eduard Hovy.
2003.
AutomaticEvaluation of Summaries Using N-gram Co-Occurrence Statistics.
In Proceedings of HLT-NAACL, pages 71-78.Tie-Yan Liu.
2009.
Learning to Rank for Informa-tion Retrieval, Foundation and Trends on Infor-mation Retrieval.
Now Publishers.H.P.
Luhn.
1958.
The Automatic Creation of Litera-ture Abstracts.
In IBM Journal of Research andDevelopment, Vol.
2, No.
2, pp.
159-165, April1958.Andr?
F. T. Martins and Noah A. Smith.
2009.Summarization with a Joint Model for SentenceExtraction and Compression.
In Proceedings ofthe Workshop on Integer Linear Programming forNatural Langauge Processing\.Ryan McDonald.
2007.
A Study of Global InferenceAlgorithms in Multi-Document Summarization.
InProceedings of the 29th ECIR.Kathleen McKeown and Dragomir R. Radev.
1995.Generating Summaries of Multiple News Articles.In Proceedings of SIGIR'95, pages 74?82.Donald Metzler and Tapas Kanungo.
2008.
MachineLearned Sentence Selection Strategies for Query-Biased Summarization.
SIGIR Learning to RankWorkshop.Rada Mihalcea and Paul Tarau.
2004.
TextRank:Bringing Order into Texts.
In Proceedings ofEMNLP 2004, Barcelona, Spain, July 2004.Ani Nenkova, Lucy Vanderwende and KathleenMcKeown.
2006.
A Compositional Context Sensi-tive Multi-document Summarizer: Exploring theFactors that Influence Summarization.
In Pro-ceedings of SIGIR 2006.You Ouyang, Sujian Li, Wenjie Li.
2007.
Develop-ing Learning Strategies for Topic-based Summa-rization.
In Proceedings of the sixteenth ACMConference on Information and Knowledge Man-agement, 2007.Rebecca J. Passonneau, Ani Nenkova, KathleenMcKeown and Sergey Sigelman.
2005.
Applyingthe Pyramid Method in DUC 2005.
DUC 2005Workshop.Dragomir R. Radev, Hongyan Jing, Malgorzata Stys,and Daniel Tam.
2004.
Centroid-based Summari-zation of Multiple Documents.
InformationProcessing and Management, 40:919?938.Christina Sauper and Regina Barzilay.
2009.
Auto-matically Generating Wikipedia Articles: A Struc-ture-Aware Approach.
In Proceedings of ACL2009.Dou Shen, Jian-Tao Sun, Hua Li, QiangYang andZheng Chen.
2007.
Document Summarization Us-ing Conditional Random Fields.
In IJCAI, pages2862 - 2867, 2007.Krysta Svore, Lucy Vanderwende, and Chris Burges.2007.
Enhancing Single-Document Summariza-tion by Combining RankNet and Third-PartySources.
In Proceedings of EMNLP-CoNLL(2007), pp.
448-457..Hiroya Takamura and Manabu Okumura.
TextSummarization Model Based on Maximum Cov-erage Problem and its Variant.
In ProceedingsEACL, 2009.Xiaojun Wan and Jianguo Xiao.
2007.
Towards aUnified Approach Based on Affinity Graph toVarious Multi-document Summarizations.
ECDL2007, 297-308.Changhu Wang, Feng Jing, Lei Zhang and Hong-Jiang Zhang.
2007.
Learning Query-Biased WebPage Summarization.
In Proceedings of the six-teenth ACM Conference on Information andKnowledge Management.Frank Wilcoxon.
1945.
Individual comparisons byranking methods.
Biometrics, 1, 80-83.533
