MUC/MET Evaluation TrendsNancy A. ChinchorScience Appl icat ions International Corporat ion10260 Campus Point Dr iveSan Diego,  CA 92121ch inchor  @ gso.
saic .com(619)458-2614INTRODUCTIONDuring the course of the Tipster Program,evaluation methodology for information extractiondeveloped as the technology progressed.
Multiple tasklevels and multiple languages were successful targets ofinformation extraction.
Automated scoring andstatistical significance algorithms were developed foruse in scoring systems and for interannotator agreementmeasures.
The scoring interface allowed both systemdevelopers and annotators to analyze rrors and improvetheir work.
This software and the marked datasets arenow in the public domain.
Future projects are beingcarried out based on simplifications indicated by thedata, downstream applications, and tractability ofscoring algorithms.EVALUATION METHODOLOGYTasksThe original MUC task was to extractinformation about relevant events from newswire textsand use it to fill the slots in a scenario template.
InMUC-3 and MUC-4 the terrorism scenario was used forthe entire evaluation cycle including training, dry run,and formal run \[1\].In MUC-5, the two domains of joint venturesand microelectronics fabrication capabilities were usedin English and Japanese for the entire developmentcycle.In MUC-6, a transition to focus on portabilitylimited the time that developers could know the domainof the scenario template for the test so training and testdata were provided for labor negotiations in the dry runand for management succession i  the formal run, eachlasting one month.
Also, in MUC-6, the task was brokendown into markup of named entities and coreference aswell as template lements which were then used in thescenario template.After MUC-6, a dry run of MET, themultilingual extraction task, was done for named entityin Chinese, Japanese, and Spanish \[2\].
A full evaluationof named entity markup in Chinese and Japanese wasdone in MET-2 as part of MUC-7.The template relation task was also added inMUC-7 to allow the extraction of relationships betweentemplate lements unrelated to scenario template.
The.domain for the MUC-7 dry run of scenario template wasEvaluation\TasksMUC-3MUC-4MUC-5MUC-6MUC-7MET-1MET-2NamedEntityYESYESYESYESCoreferenceYESYESTemplateElementYESYESTemplateRelationYESScenarioTemplateYESYESYESYESYESMultilingualYESYESYES235air crashes and the formal run was launch events.The table above summarizes this developmentof tasks over the Tipster years of MUC/MET.
'The tasksare defined in more detail in the remainder of thissection.
The results of the evaluations are given in thenext section in tabular format.EntitiesOn the level of entity extraction, NamedEntities (NE) were defined as proper names andquantities of interest.
Person, organization, and locationnames were marked as well as dates, times, percentages,and monetary amounts.
The annotation was SGMLwithin the text stream.
An example from MUC-7 (NewYork Times News Service) follows.The <ENAMEX TYPE="LOCATION">U.K.</ENAMEX> satellite television broadcaster said itssubscriber base grew <NUMEX TYPE="PER-CENT"> 17.5 percent</NUMEX> during <TIMEXTYPE="DATE">the past year</TIMEX> to 5.35millionEquivalence ClassesThe task of Coreference (CO) had its origins inSemeval, an attempt after MUC-5 to define semanticresearch tasks that needed to be solved to be successfulat generating scenario templates.
In the MUCevaluations, only coreference of type identity wasmarked and scored \[3\].
The following example fromMUC-7 (New York Times News Service) illustratesidentity coreference between "its" and "The U.K.satellite television broadcaster" as well as that betweenthe function "its subscriber base" and the value "5.35million.
"*The U.K. satellite television broadcaster* said**its* subscriber base* grew 17.5 percent duringthe past year to *5.35 million*The coreference task is a bridge between theNE task and the TE task.AttributesThe attributes of entities are slot fills inTemplate Elements (TE) that consist of name, type,descriptor, and category slots.
The attributes in theTemplate Element serve to further identify the entitybeyond the name level.All aliases are put in the NAME slot.
Persons,organizations, artifacts, and locations are all TYPEs ofTemplate Elements.
All substantial descriptors used inthe text appear in the DESCRIPTOR slot.
TheCATEGORY slot contains categories dependent on theelement involved: persons can be civilian, military, orother; organizations can be government, company, orother; artifacts are limited to vehicles and can be fortraveling on land, water, or in air; locations can be city,province, country, region, body of water, airport, orunknown.An example of a Template Element fromMUC-7 follows:<ENTITY-9602040136-11 > :=ENT_NAME: "Dennis Gillespie"ENT_TYPE: PERSONENT_DESCRIPTOR: "Capt.
"/ "the commander of Carrier Air Wing 11"ENT_CATEGORY: PER_MILFactsThe Template Relations (TR) task marksrelationships between template elements and can bethought of as a task in which well-defined facts areextracted from newswire text.In MUC-7, we limited TR to relationships withorganizations: employee_of, product_of, location_of.However, the task is easily expandable to all logicalcombinations and relations between entity typesAn example of Template Relations from MUC-7 follows:<EMPLOYEE_OF-9602040136-5> :=PERSON: <ENTITY-9602040136-11>ORGANIZATION: <ENTITY-9602040136-1><ENTITY-9602040136-11> :=ENT NAME: "Dennis Gillespie"ENT TYPE: PERSONENT DESCRIPTOR: "Capt.
"/ "the commander of Carrier Air Wing 11"ENT_CATEGORY: PER_MIL<ENTITY-9602040136-1 > :=ENT_NAME: "NAVY"ENT_TYPE: ORGANIZATIONENT_CATEGORY: ORG_GOVTEventsThe Scenario Template (ST) was built aroundan event in which entities participated.
The scenarioprovided the domain of the dataset and allowed forrelevancy judgments of high accuracy by systems.236The task definition for ST required relevancyand fill rules.
The choice of the domain was dependentto some extent on the evaluation epoch.
The structure ofthe template and the task definition tended to bedependent on the author of the task, but the richness ofthe templates also served to illustrated the utility ofinformation extraction to users most effectively.The filling of the slots in the scenario templatewas generally adifficult task for systems and a relativelylarge effort was required to produce ground truth.Reasonable agreement(>80%) between annotators waspossible, but required sometimes ornate refinement ofthe task definition based on the data encountered.Task DefinitionsAs experience was gained in defining tasks forinformation extraction, certain principles becameinvaluable.
It was important for the utility of the task tobe apparent to end users.
The lower level tasks needed todovetail into the higher level tasks requiring relativelymore processing for each higher level.It was important for the task definitions toallow the achievement of an 80 - 99% threshold ininterannotator agreement depending on how wellsystems were performing.
Also, the ability to annotatetext rapidly and with ease was critical to the endproduct: guidelines and datasets of high quality forresearch and development.The process of refining the task definitionrequired concentration on several cycles of independentannotation, analysis of annotator agreement, anddetailed note-taking on which examples requiredupdates to the task definition.
The production ofconsistent datasets was always the goal.DatasetsTo be sure that systems could work onnewswire from different sources, the differentevaluations utilized material from various sources.MUC-3 and MUC-4 used articles from the ForeignBroadcast Information Service (FBIS).
MUC-5 andMUC-6 used the well-edited Wall Street Journal (WSJ).MUC-7 used articles from the New York Times NewsService (NYTNS) which contained journalism frommultiple news organizations in a uniform SGML format.Typically, there were 100 texts per dataset.
Thetexts were chosen using mixtures of keywordsassociated with the domains (terrorism, joint ventures,microelectronics, labor relations, managementsuccession, air crashes, launch events).
A pre-definedrelevancy ratio was used for each dataset, usually 65%of the texts were relevant.
Datasets in MUC-7 wereprovided for general training, dry run training and test,and formal run training and test data.In all of the MUC/MET evaluations theannotation accuracy was at least 80%, or higherwhenever system performance was closer to humanperformance.
We coordinated adjudication after theevaluation results were reported but before the finalpackage was released for research use by thecommunity at large.EVALUATION RESULTSThe evaluation results are given in the tablebelow in terms of the highest score approached by thebest system to the nearest percentage point.
Early on insome of the tasks, only recall (R) and precision (P) werecalculated, but usually the combined F-measure (F) wasused with recall and precision weighted equally.The scores for Scenario Template did not everget above the mid-60's for E The reasons for this barrierare several and were partially addressed when multipletasks were attempted at varying levels of processing.Scoring and template design issues that interfere withmeaningful measures of progress are discussed in thenext section and are being addressed in future evaluationmethods.The scores for Named Entity are in the mid-90's and are close to human performance.
The machinesand the annotators are still significantly different in theirperformance, but automation is preferable due to itsspeed.
Applications of Named Entity have beensuccessful in assisting humans in processing largeamounts of textual data.The scores for Template Element and TemplateRelations are also high enough to make the technologyreliable for use by analysts.
The Template Elementsextracted from newswire articles are indicative of thecontent of the article for most purposes.Although the corefercnce scores are lower thanthe Template Element scores, enough coreference isbeing processed to achieve reliable results in TemplateElement.The multi-lingual scores are impressive both inScenario Template and in Named Entity.
Some of thevariability is due to changes of domain between trainingand test documents in MUC-7.
The results weresurprising because many of the developers were notnative or fluent speakers of the languages on which theirsystems were evaluated.
Also, differences in style across237Eva luat ion \TasksMUC-3MUC-4MUC-5MUC-6MUC-7MultilingualMET-1MET-2Legend:NamedEnt i tyF < 97%F < 94%c F < 85%J F < 93%S F < 94%CF<91%J F < 87%R = RecallE = EnglishJV = Joint VentureCoreferenceR < 63%P < 72%F < 62%P = Precisionc = ChineseTemplateE lementF < 80%F < 87%TemplateRe la t ionF < 76%ScenarioTemplateR < 50%P < 70%F < 56%EJV F < 53%EME F < 50%F < 57%F<51%F = F-Measure with Recall and Precision Weighted EquallyJ = Japanese S = SpanishME = MicroelectronicsMul t i l ingua lJJV F < 64%JME F < 57%languages sometimes made processing easierlanguages other than English.EVALUATION ALGORITHMSinEvaluation MetricsThe evaluation metrics used for informationextraction were adapted from Information Retrievalearly in MUC-3 \[1\].
Both SGML markup in the textstream and template slot fills are scored automatically.The determination of scores for the coreferenceequivalence classes is based on a model theoreticalgorithm that counts the minimal number of links thatmust be added to make the classes in the answer key andsystem response match \[4\].Statistical Significance TestingAt the end of MUC-3, a statistical significancetesting algorithm was developed to determine thesignificance of the results of the evaluation \[3\].
Themethod is a computer intensive method calledapproximate randomization and is based on a document-by-document comparison of performance for each pairof systems in the evaluation.
The results are graphed andthe sets of systems that are not significantly differentfrom each other in performance on the test set areenclosed in the same circle.
The method does not saythat results are significant within a certain percentage,but rather looks at the characteristics of the performanceof the systems across all documents.Interannotator ScoringDuring the course of the Tipster evaluations amethod for measuring interannotator agreement wasprovided by the scoring software.
This measure and theaccompanying error reports assisted during thedevelopment of task definitions using training data andduring the development of training and test datasets.The scoring software is designed to work domain-independently so it was easy to adapt for differentscenarios and template slot designations in ST, TE, andTR and SGML markup for NE and CO.
The key2keyconfiguration option needed only be given to score ananswer key against an answer key.
Feedback is given ina strict fashion as to whether the annotators' keys agreed238on the alternatives and optional elements allowed onlyin answer keys.User InterfacesThe scoring software has formatted reportsand a GUI for viewing evaluation results in alllanguages.
These tools for visualizing systems errorsassisted evelopers in debugging their systems and inpresenting their results.
The user interfaces weredesigned based on input from the participants and thecustomers.Remaining Scoring IssuesAlignmentThe tree structure of the Scenario Templatesrequires choosing which objects to score against theobjects given in the answer key.
In order not to overpenalize systems, alignment is done to optimize the F-measure.
However, the optimization is not exhaustiveand in some cases does not converge.
Instead of scoringslot fills as both missing and spurious in less than idealmappings, the scorer tends to map and score themismatching slot fills as incorrect and the F-measure iscalculated in such a way as to minimize the negativeeffect of missing and spurious lots.In the future, the tree structure of the templatewill be greatly simplified so that the alignment problemis insignificant in understanding the results duringdevelopment and testing.Linchpin EffectAnother issue that arose in task design was theproblem of penalizing a system in multiple places forone mistake.
The inherent interdependencies ofinformation especially in event descriptions made thisaspect of task design difficult.
Clearly over the course ofTipster, the annotations and templates changed to showthe amelioration of this effect.
Future evaluations willstill need to beware of it.CURRENT WORKCurrently, the Named Entity task used toevaluate information extraction systems on newswirearticles is being adapted to evaluate such systems whenprocessing errorful data, specifically the transcriptionsdone by speech recognition systems on Broadcast News.The purpose of this form of evaluation is two-fold.Improvements in speech recognition are expected tofocus on the information-bearing elements of the signal.Improvements in information extraction are expected tomake the systems more robust in the kinds of input theycan handle.FUTURE DIRECTIONSPlans for the future of evaluations ofinformation extraction from Broadcast News are nowfocused on event extraction.
The template will be asimple set of slots for each event ype and the fills willbe unprocessed text extracted from the transcriptions.The fill rules will be simplified and the annotations willbe more closely agreed upon.
The alignment problemand the linchpin effect will be minimized.ACKNOWLEDGMENTSIn a successful program such as Tipster thereare many people to thank.
The sponsors have been manyover the years and none of these results would have beenachieved without their efforts to keep the program ontrack.
The participants in the evaluations have giventheir all in making information extraction emerge as anapplication-rich technology.
The wider computationallinguistics community responded with support and lettheir work be changed by evaluations.
We are allgrateful for this opportunity to have had an impact.REFERENCES\[1\] Chinchor, N.; Hirschman, L.; and Lewis, D. D.(1993).
"Evaluating Message UnderstandingSystems: An Analysis of the Third MessageUnderstanding Conference (MUC-3).
"Computational Linguistics, 19(3),409 - 449.\[2\] Merchant, R,; Okurowski, M.E.
; and Chinchor,N.
(1996).
"The Multilingual Entity Task (MET)Overview."
In Proceedings, Tipster Text Program(Phase II).
Morgan Kaufmann.
San Mateo, CA.\[3\] Chinchor, N. (1992).
"The StatisticalSignificance of the MUC-4 Results."
InProceedings, Fourth Message UnderstandingConference (MUC-4).
Morgan Kaufmann.
SanMateo, CA.\[4\] Vilain, M.; Burger, J.; Aberdeen, J.; Connolly, D.;and Hirschman, L. (1995).
"A Model-TheoreticCoreference Scoring Scheme."
In Proceedings,Sixth Message Understanding Conference(MUC-6).
Morgan Kaufmann.
San Mateo, CA.239
