Incremental Parsing, or Incremental Grammar?
?Matthew Purver?
and Ruth Kempson?Departments of ?Computer Science and ?Philosophy,King?s College London, Strand,London WC2R 2LS,UK{matthew.purver, ruth.kempson}@kcl.ac.ukAbstractStandard grammar formalisms are defined with-out reflection of the incremental and serial na-ture of language processing, and incremental-ity must therefore be reflected by independentlydefined parsing and/or generation techniques.We argue that this leads to a poor setup formodelling dialogue, with its rich speaker-hearerinteraction, and instead propose context-basedparsing and generation models defined in termsof an inherently incremental grammar formal-ism (Dynamic Syntax), which allow a straight-forward model of otherwise problematic dia-logue phenomena such as shared utterances, el-lipsis and alignment.1 IntroductionDespite increasing psycholinguistic evidence ofincrementality in language processing, both inparsing (see e.g.
(Crocker et al, 2000)) and inproduction (Ferreira, 1996), there is almost uni-versal agreement that this should not be re-flected in grammar formalisms which constitutethe underlying model of language (for rare ex-ceptions, see (Hausser, 1989; Kempson et al,2001)).
Constraint-based grammar formalismsare accordingly defined neutrally between eitherof these applications, with parsing/generationsystems (whether incremental or not) definedas independent architectures manipulating thesame underlying system.1 Such assumptionshowever lead to formal architectures that arerelatively poorly set up for modelling dialogue,for they provide no basis for the very rich de-gree of interaction between participants in dia-logue.
A common phenomenon in dialogue is?
Related papers from the point of view of generationrather than parsing, and from the point of view of align-ment rather than incrementality, are to be presented atINLG?04 and Catalog?04 respectively.1Authors vary as to the extent to which these archi-tectures might be defined to be reversible.
See (Neu-mann, 1994).that of shared utterances (Clark and Wilkes-Gibbs, 1986), with exchange of roles of parserand producer midway through an utterance:2(1)Daniel: Why don?t you stop mumblingandMarc: Speak proper like?Daniel: speak proper?
(2) Ruth: What did Alex .
.
.Hugh: Design?
A kaleidoscope.Such utterances clearly show the need for astrictly incremental model: however, they areparticularly problematic for any overall archi-tecture in which parsing and generation are in-dependently defined as applications of a use-neutral grammar formalism which yields as out-put the set of well-formed strings, for in thesetypes of exchange, the string uttered by one andparsed by the other need not be a wellformedstring in its own right, so will not fall withinthe set of data which the underlying formalismis set up to capture.
Yet, with the transition be-tween interlocutors seen as a shift from one sys-tem to another, each such substring will have tobe characterised independently.
Many other di-alogue phenomena also show the need for inter-action between the parsing and generation pro-cesses, among them cross-speaker ellipsis (e.g.simple bare fragment answers to wh-questions),and alignment (Pickering and Garrod, 2004), inwhich conversational participants mirror eachother?s patterns at many levels (including lexi-cal choice and syntactic structure).The challenge of being able to model thesephenomena, problematic for theorists but ex-tremely easy and natural for dialogue partici-pants themselves, has recently been put out byPickering and Garrod (2004) as a means of eval-uating both putative grammar formalisms and2Example (1) from the BNC, file KNY (sentences315?317).models of language use.
In response to this chal-lenge, we suggest that an alternative means ofevaluating parsing implementations is by eval-uation of paired parsing-generation models andthe dialogue model that results.
As an illustra-tion of this, we show how if we drop the assump-tion that grammar formalisms are defined neu-trally between parsing and production in favourof frameworks in which the serial nature of lan-guage processing is a central design feature (asin Dynamic Syntax: (Kempson et al, 2001)),then we can define a model in which incrementalsub-systems of parsing and generation are nec-essarily tightly coordinated, and can thus pro-vide a computational model of dialogue whichdirectly corresponds with currently emergingpsycholinguistic results (Branigan et al, 2000).In particular, by adding a shared model of con-text to previously defined word-by-word incre-mental parsing and generation models, we showhow the switch in speaker/hearer roles duringa shared utterance can be seen as a switch be-tween processes which are directed by differentgoals, but which share the same incrementallybuilt data structures.
We then show how thisinherent incrementality and structure/contextsharing also allows a straightforward model ofcross-speaker ellipsis and alignment.2 BackgroundDynamic Syntax (DS) (Kempson et al, 2001) isa parsing-directed grammar formalism in whicha decorated tree structure representing a se-mantic interpretation for a string is incremen-tally projected following the left-right sequenceof the words.
Importantly, this tree is not amodel of syntactic structure, but is strictly se-mantic, being a representation of the predicate-argument structure of the sentence.
In DS,grammaticality is defined as parsability (thesuccessful incremental construction of a tree-structure logical form, using all the informationgiven by the words in sequence): there is no cen-tral use-neutral grammar of the kind assumedby most approaches to parsing and/or gener-ation.
The logical forms are lambda terms ofthe epsilon calculus (see (Meyer-Viol, 1995) fora recent development), so quantification is ex-pressed through terms of type e whose complex-ity is reflected in evaluation procedures that ap-ply to propositional formulae once constructed,and not in the tree itself.
With all quantificationexpressed as type e terms, the standard groundsfor mismatch between syntactic and semanticanalysis for all NPs is removed; and, indeed, allsyntactic distributions are explained in terms ofthis incremental and monotonic growth of par-tial representations of content.
Hence the claimthat the model itself constitutes a NL grammarformalism.Parsing (Kempson et al, 2001) defines pars-ing as a process of building labelled semantictrees in a strictly left-to-right, word-by-word in-cremental fashion by using computational ac-tions and lexical actions defined (for some natu-ral language) using the modal tree logic LOFT(Blackburn and Meyer-Viol, 1994).
These ac-tions are defined as transition functions be-tween intermediate states, which monotonicallyextend tree structures and node decorations.Words are specified in the lexicon to have as-sociated lexical actions: the (possibly partial)semantic trees are monotonically extended byapplying these actions as each word is consumedfrom the input string.
Partial trees may be un-derspecified: tree node relations may be onlypartially specified; node decorations may be de-fined in terms of unfulfilled requirements andmetavariables; and trees may lack a full set ofscope constraints.
Anaphora resolution is a fa-miliar case of update: pronouns are defined toproject metavariables that are substituted fromcontext as part of the construction process.
Rel-ative to the same tree-growth dynamics, long-distance dependency effects are characterisedthrough restricted licensing of partial trees withrelation between nodes introduced with merelya constraint on some fixed extension (followingD-Tree grammar formalisms (Marcus, 1987)),an underspecification that gets resolved withinthe left-to-right construction process.3 Quanti-fying terms are also built up using determinerand noun to yield a partially specified term e.g.
(, y, Man?
(y)) with a requirement for a scopestatement.
These scope statements, of the formx < y (?the term binding x is to be evaluatedas taking scope over the term binding y?
), areadded to a locally dominating type-t-requiringnode.
Generally, they are added to an accu-mulating set following the serial order of pro-cessing in determining the scope dependency,but indefinites (freer in scope potential) are as-signed a metavariable as first argument, allow-3In this, the system is also like LFG, modelling long-distance dependency in the same terms as functional un-certainty (Kaplan and Zaenen, 1989), differing from thatconcept in the dynamics of update internal to the con-struction of a single tree.Figure 1: Parsing ?john likes mary?
.
.
.
.
.
.
and generating ?john likes mary?{}{john?}
{?}john{}{john?}
{}{like?}
{?}likes{like?(mary?)(john?),?}{john?}
{like?(mary?)}{like?}
{mary?}mary{}{john?}
{?
}FAIL FAILjohnlikes mary{}{john?}
{}{like?}
{?}FAILlikesmary{like?(mary?)(john?),?}{john?}
{like?(mary?)}{like?}
{mary?
}marying selection from any term already added, in-cluding temporally-sorted variables associatedwith tense/modality specifications.
The gen-eral mechanism is the incremental analogue ofquantifier storage; and once a propositional for-mula of type t has been derived at a node withsome collection of scope statements, these arejointly evaluated to yield fully expanded termsthat reflect all relative dependencies within therestrictor of the terms themselves.
For example,parsing A man coughed yields the pair Si < x,Cough?
(, x, Man?
(x)) (Si the index of evalua-tion), then evaluated as Man?
(a) ?
Cough?
(a)where a = (, x, Man?
(x) ?
Cough?
(x)).4Once all requirements are satisfied and allpartiality and underspecification is resolved,trees are complete, parsing is successful and theinput string is said to be grammatical.
Centralto the formalism is the incremental and mono-tonic growth of labelled partial trees: the parserstate at any point contains all the partial treeswhich have been produced by the portion of thestring so far consumed and which remain can-didates for completion.54For formal details of this approach to quantificationsee (Kempson et al, 2001) chapter 7; for an early imple-mentation see (Kibble et al, 2001).5Figure 1 assumes, simplistically, that linguisticnames correspond directly to scopeless names in the log-Generation (Otsuka and Purver, 2003;Purver and Otsuka, 2003) (hereafter O&P)give an initial method of context-independenttactical generation based on the same incre-mental parsing process, in which an outputstring is produced according to an inputsemantic tree, the goal tree.
The generatorincrementally produces a set of correspondingoutput strings and their associated partial trees(again, on a left-to-right, word-by-word basis)by following standard parsing routines andusing the goal tree as a subsumption check.At each stage, partial strings and trees aretentatively extended using some word/actionpair from the lexicon; only those candidateswhich produce trees which subsume the goaltree are kept, and the process succeeds whena complete tree identical to the goal tree isproduced.
Generation and parsing thus usethe same tree representations and tree-buildingactions throughout.3 Contextual ModelThe current proposed model (and its imple-mentation) is based on these earlier definitionsbut modifies them in several ways, most signif-icantly by the addition of a model of context:ical language that decorate the tree.while they assume some notion of context theygive no formal model or implementation.6 Thecontextual model we now assume is made up notonly of the semantic trees built by the DS pars-ing process, but also the sequences of words andassociated lexical actions that have been usedto build them.
It is the presence of (and as-sociations between) all three, together with thefact that this context is equally available to bothparsing and generation processes, that allow ourstraightforward model of dialogue phenomena.7For the purposes of the current implementa-tion, we make a simplifying assumption thatthe length of context is finite and limited to theresult of some immediately previous parse (al-though information that is independently avail-able can be represented in the DS tree format,so that, in reality, larger and only partially or-dered contexts are no doubt possible): contextat any point is therefore made up of the treesand word/action sequences obtained in parsingthe previous sentence and the current (incom-plete) sentence.Parsing in Context A parser state is there-fore defined to be a set of triples ?T, W, A?,where T is a (possibly partial) semantic tree,8W the sequence of words and A the sequenceof lexical and computational actions that havebeen used in building it.
This set will initiallycontain only a single triple ?Ta, ?, ??
(where Tais the basic axiom taken as the starting point ofthe parser, and the word and action sequencesare empty), but will expand as words are con-sumed from the input string and the corre-sponding actions produce multiple possible par-tial trees.
At any point in the parsing process,the context for a particular partial tree T in6There are other departures in the treatment of linkedstructures (for relatives and other modifiers) and quan-tification, and more relevantly to improve the incremen-tality of the generation process: we do not adopt theproposal of O&P to speed up generation by use of a re-stricted multiset of lexical entries selected on the basisof goal tree features, which prevents strictly incrementalgeneration and excludes modification of the goal tree.7In building n-tuples of trees corresponding topredicate-argument structures, the system is similar toLTAG formalisms (Joshi and Kulick, 1997).
However,unlike LTAG systems (see e.g.
(Stone and Doran, 1997)),both parsing and generation are not head-driven, butfully (word-by-word) incremental.
This has the ad-vantage of allowing fully incremental models for alllanguages, matching psycholinguistic observations (Fer-reira, 1996).8Strictly speaking, scope statements should be in-cluded in these n-tuples ?
for now we consider them aspart of the tree.this set can then be taken to consist of: (a) asimilar triple ?T0, W0, A0?
given by the previoussentence, where T0 is its semantic tree repre-sentation, W0 and A0 the sequences of wordsand actions that were used in building it; and(b) the triple ?T, W, A?
itself.
Once parsing iscomplete, the final parser state, a set of triples,will form the new starting context for the nextsentence.
In the simple case where the sentenceis unambiguous (or all ambiguity has been re-moved) this set will again have been reducedto a single triple ?T1, W1, A1?, corresponding tothe final interpretation of the string T1 with itssequence of words W1 and actions A1, and thisreplaces ?T0, W0, A0?
as the new context; in thepresence of persistent ambiguity there will sim-ply be more than one triple in the new context.9Generation in Context A generator stateis now defined as a pair (Tg, X) of a goal treeTg and a set X of pairs (S, P ), where S is acandidate partial string and P is the associatedparser state (a set of ?T, W, A?
triples).
Ini-tially, the set X will usually contain only onepair, of an empty candidate string and the stan-dard initial parser state, (?, {?Ta, ?, ??}).
How-ever, as both parsing and generation processesare strictly incremental, they can in theory startfrom any state.
The context for any partial treeT is defined exactly as for parsing: the previ-ous sentence triple ?T0, W0, A0?
; and the cur-rent triple ?T, W, A?.
Generation and parsingare thus very closely coupled, with the centralpart of both processes being a parser state: a setof tree/word-sequence/action-sequence triples.Essential to this correspondence is the lack ofconstruction of higher-level hypotheses aboutthe state of the interlocutor.
All transitionsare defined over the context for the individ-ual (parser or generator).
In principle, con-texts could be extended to include high-levelhypotheses, but these are not essential and arenot implemented in our model (see (Millikan,2004) for justification of this stance).4 Shared UtterancesOne primary evidence for this close couplingand sharing of structures and context is the easewith which shared utterances can be expressed.O&P suggest an analysis of shared utterances,9The current implementation of the formalism doesnot include any disambiguation mechanism.
We simplyassume that selection of some (minimal) context and at-tendant removal of any remaining ambiguity is possibleby inference.Figure 2: Transition from hearer to speaker: ?What did Alex .
.
.
/ .
.
.
design?
?Pt =?
{+Q}{WH} {alex?
}{?Ty(e ?
t),?
}, {what, did, alex}, {a1, a2, a3}?Gt =({+Q, design?(WH)(alex?)}{alex?}
{design(WH)}{WH}{design?},(?,?
{+Q}{WH} {alex?
}{?Ty(e ?
t),?
}, {what, did, alex}, {a1, a2, a3}?
))G1 =({+Q, design?(WH)(alex?)}{alex?}
{design?(WH)}{WH}{design?},({design},?{+Q}{WH}{alex?}
{?Ty(e ?
t)}{?}{design?
}, {.
.
.
, design}, {.
.
.
, a4}?
))and this can now be formalised given the cur-rent model.
As the parsing and generation pro-cesses are both fully incremental, they can startfrom any state (not just the basic axiom state?Ta, ?, ??).
As they share the same lexical en-tries, the same context and the same semantictree representations, a model of the switch ofroles now becomes relatively straightforward.Transition from Hearer to Speaker Nor-mally, the generation process begins withthe initial generator state as defined above:(Tg, {(?, P0)}), where P0 is the standard initial?empty?
parser state {?Ta, ?, ??}.
As long as asuitable goal tree Tg is available to guide gen-eration, the only change required to generate acontinuation from a heard partial string is toreplace P0 with the parser state (a set of triples?T, W, A?)
as produced from that partial string:we call this the transition state Pt.
The initialhearer A therefore parses as usual until transi-tion,10 then given a suitable goal tree Tg, formsa transition generator state Gt = (Tg, {(?, Pt)}),from which generation can begin directly ?
seefigure 2.11 Note that the context does notchange between processes.For generation to begin from this transitionstate, the new goal tree Tg must be subsumedby at least one of the partial trees in Pt (i.e.the proposition to be expressed must be sub-sumed by the incomplete proposition that hasbeen built so far by the parser).
Constructing10We have little to say about exactly when transitionsoccur.
Presumably speaker pauses and the availabilityto the hearer of a possible goal tree both play a part.11Figure 2 contains several simplifications to aid read-ability, both to tree structure details and by show-ing parser/generator states as single triples/pairs ratherthan sets thereof.Tg prior to the generation task will often be acomplex process involving inference and/or ab-duction over context and world/domain knowl-edge ?
Poesio and Rieser (2003) give some ideaas to how this inference might be possible ?
fornow, we make the simplifying assumption thata suitable propositional structure is available.Transition from Speaker to Hearer Attransition, the initial speaker B?s generatorstate G?t contains the pair (St, P ?t), where St isthe partial string output so far, and P ?t is thecorresponding parser state, the transition stateas far as B is concerned.12 In order for B tointerpret A?s continuation, B need only use P ?tas the initial parser state which is extended asthe string produced by A is consumed.As there will usually be multiple possible par-tial trees at the transition point, A may con-tinue in a way that does not correspond to B?sinitial intentions ?
i.e.
in a way that does notmatch B?s initial goal tree.
For B to be ableto understand such continuations, the genera-tion process must preserve all possible partialparse trees (just as the parsing process does),whether they subsume the goal tree or not, aslong as at least one tree in the current state doessubsume the goal tree.
A generator state musttherefore rule out only pairs (S, P ) for which Pcontains no trees which subsume the goal tree,rather than thinning the set P directly via thesubsumption check as proposed by O&P.It is the incrementality of the underlyinggrammar formalism that allows this simpleswitch: the parsing process can begin directly12Of course, if both A and B share the same lexicalentries and communication is perfect, Pt = P ?t , but wedo not have to assume that this is the case.from a state produced by an incomplete gener-ation process, and vice versa, as their interme-diate representations are necessarily the same.5 Cross-Speaker EllipsisThis inherent close coupling of the two incre-mental processes, together with the inclusionof tree-building actions in the model of con-text, also allows a simple analysis of many cross-speaker elliptical phenomena.Fragments Bare fragments (3) may be anal-ysed as taking a previous structure from con-text as a starting point for parsing (or genera-tion).
WH -expressions are analysed as partic-ular forms of metavariables, so parsing a wh-question yields a type-complete but open for-mula, which the term presented by a subsequentfragment can update:(3) A: What did you eat for breakfast?B: Porridge.Parsing the fragment involves constructing anunfixed node, and merging it with the contex-tually available structure, so characterising thewellformedness/interpretation of fragment an-swers to questions without any additional mech-anisms: the term (, x, porridge?
(x)) stands in alicensed growth relation from the metavariableWH provided by the lexical actions of what.Functional questions (Ginzburg and Sag,2000) with their fragment answers (4) poseno problem.
As the wh-question contains ametavariable, the scope evaluation cannot becompleted; completion of structure and evalu-ation of scope can then be effected by merg-ing in the term the answer provides, identifyingany introduced metavariable in this context (thegenitive imposes narrow scope of the introducedepsilon term):(4) A: Who did every student ignore?B: Their supervisor.
{Si < x}{(?, x, stud?
(x))} {}{WH,?}
{ignr?}?
{Si < x, x < y}{(?, x, stud?
(x))} {}{(, y, sup?(x)(y)}{ignr?
}VP Ellipsis Anaphoric devices such as pro-nouns and VP ellipsis are analysed as decoratingtree nodes with metavariables licensing updatefrom context using either established terms, or,for ellipsis, (lexical) tree-update actions.
Strictreadings of VP ellipsis result from taking a suit-able semantic formula directly from a tree nodein context: any node n ?
(T0 ?
T ) of suitabletype (e ?
t) with no outstanding requirements.Sloppy readings involve re-use of actions: anysequence of actions (a1; a2; .
.
.
; an) ?
(A0 ?
A)can be used (given the appropriate ellipticaltrigger) to extend the current tree T if this pro-vides a formula of type e ?
t.13 This latterapproach, combined with the representation ofquantified elements as terms, allows a range ofphenomena, including those which are problem-atic for other (abstraction-based) approaches(for discussion see (Dalrymple et al, 1991)):(5)A: A policeman who arrested Bill readhim his rights.B: The policeman who arrested Tomdid too.The actions associated with A?s use of readhim his rights in (5) include the projection of ametavariable associated with him, and its res-olution to the term in context associated withBill.
B?s ellipsis allows this action sequence tobe re-used, again projecting a metavariable andresolving it, this time (given the new context) tothe term provided by parsing Tom.
This leadsto a copy of Tom within the constructed predi-cate, and a sloppy reading.This analysis also applies to yield parallellismeffects in scoping (Hirschbu?hler, 1982; Shieberet al, 1996), allowing narrow scope construalfor indefinites in subject position:(6) A: A nurse interviewed every patient.B: An orderly did too.Resolution of the underspecification in thescope statement associated with an indefinitecan be performed at two points: either at theimmediate point of processing the lexical ac-tions, or at the later point of compiling the re-sulting node?s interpretation within the emer-gent tree.14 In (6), narrow scope can be as-signed to the subject in A?s utterance via thislate assignment of scope; at this late point in the13In its re-use of actions provided by context, this ap-proach to ellipsis is essentially similar to the glue lan-guage approach (see (Asudeh and Crouch, 2002) andpapers in (Dalrymple, 1999) but, given the lack of in-dependent syntax /semantics vocabularies, the need foran intermediate mapping language is removed.14This pattern parallels expletive pronouns whichequally allow a delayed update (Cann, 2003).parse process, the term constructed from the ob-ject node will have been entered into the set ofscope statements, allowing the subject node tobe dependent on the following quantified expres-sion.
The elliptical word did in B?s utterancewill then license re-use of these late actions, re-peating the procedures used in interpreting A?santecedent and so determining scope of the newsubject relative to the object.Again, these analyses are possible becauseparsing and generation processes share incre-mentally built structures and contextual pars-ing actions, with this being ensured by the in-crementality of the grammar formalism itself.6 Alignment & RoutinizationThe parsing and generation processes must bothsearch the lexicon for suitable entries at ev-ery step (i.e.
when parsing or generating eachword).
For generation in particular, this is acomputationally expensive process in principle:every possible word/action pair must be tested ?the current partial tree extended and the resultchecked for goal tree subsumption.
As proposedby O&P (though without formal definitions orimplementation) our model of context now al-lows a strategy for minimising this effort: asit includes previously used words and actions,a subset of such actions can be re-used in ex-tending the current tree, avoiding full lexicalsearch altogether.
High frequency of ellipticalconstructions is therefore expected, as ellipsislicenses such re-use; the same can be said forpronouns, as long as they (and their correspond-ing actions) are assumed to be pre-activated orotherwise readily available from the lexicon.As suggested by O&P, this can now lead di-rectly to a model of alignment phenomena, char-acterisable as follows.
For the generator, if thereis some action a ?
(A0 ?A) suitable for extend-ing the current tree, a can be re-used, generat-ing the word w which occupies the correspond-ing position in the sequence W0 or W .
This re-sults in lexical alignment ?
repeating w ratherthan choosing an alternative word from the lex-icon.
Alignment of syntactic structure (e.g.
pre-serving double-object or full PP forms in the useof a verb such as give rather than shifting tothe semantically equivalent form (Branigan etal., 2000)) also follows in virtue of the procedu-ral action-based specification of lexical content.A word such as give has two possible lexicalactions a?
and a??
despite semantic equivalenceof output, corresponding to the two alternativeforms.
A previous use will cause either a?
or a?
?to be present in (A0 ?
A); re-use of this actionwill cause the same form to be repeated.15A similar definition holds for the parser: for aword w presented as input, if w ?
(W0?W ) thenthe corresponding action a in the sequence A0 orA can be used without consulting the lexicon.Words will therefore be interpreted as havingthe same sense or reference as before, modellingthe semantic alignment described by (Garrodand Anderson, 1987).
These characterisationscan also be extended to sequences of words ?a sub-sequence (a1; a2; .
.
.
; an) ?
(A0 ?
A) canbe re-used by a generator, producing the cor-responding word sequence (w1; w2; .
.
.
; wn) ?
(W0 ?
W ); and similarly the sub-sequence ofwords (w1; w2; .
.
.
; wn) ?
(W0 ?
W ) will causethe parser to use the corresponding action se-quence (a1; a2; .
.
.
; an) ?
(A0 ?
A).
This willresult in sequences or phrases being repeatedlyassociated by both parser and generator withthe same sense or reference, leading to whatPickering and Garrod (2004) call routinization(construction and re-use of word sequences withconsistent meanings).It is notable that these various patterns ofalignment, said by Pickering and Garrod (2004)to be alignment across different levels, are ex-pressible without invoking distinct levels of syn-tactic or lexical structure, since context, contentand lexical actions are all defined in terms of thesame tree configurations.7 SummaryThe inherent left-to-right incrementality andmonotonicity of DS as a grammar formalism al-lows both parsing and generation processes tobe not only incremental but closely coupled,sharing structures and context.
This enablesshared utterances, cross-speaker elliptical phe-nomena and alignment to be modelled straight-forwardly.
A prototype system has been im-plemented in Prolog which reflects the modelgiven here, demonstrating shared utterancesand alignment phenomena in simple dialoguesequences.
The significance of this direct re-flection of psycholinguistic data is to buttressthe DS claim that the strictly serial incremen-tality of processing is not merely essential tothe modelling of natural-language parsing, but15Most frameworks would have to reflect this via pref-erences defined over syntactic rules or parallelisms withsyntactic trees in context, both problematic.to the design of the underlying grammar formal-ism itself.AcknowledgementsThis paper is an extension of joint work on theDS framework with Wilfried Meyer-Viol, on ex-pletives and on defining a context-dependentformalism with Ronnie Cann, and on DS gen-eration with Masayuki Otsuka.
Each has pro-vided ideas and input without which the cur-rent results would have differed, although anymistakes here are ours.
Thanks are also due tothe ACL reviewers.
This work was supportedby the ESRC (RES-000-22-0355) and (for thesecond author) by the Leverhulme Trust.ReferencesA.
Asudeh and R. Crouch.
2002.
Derivationalparallelism and ellipsis parallelism.
In Pro-ceedings of WCCFL 21.P.
Blackburn and W. Meyer-Viol.
1994.
Lin-guistics, logic and finite trees.
Bulletin of theIGPL, 2:3?31.H.
Branigan, M. Pickering, and A. Cleland.2000.
Syntactic co-ordination in dialogue.Cognition, 75:13?25.R.
Cann.
2003.
Semantic underspecificationand the interpretation of copular clauses inEnglish.
In Where Semantics Meets Pragmat-ics.
University of Michigan.H.
H. Clark and D. Wilkes-Gibbs.
1986.
Re-ferring as a collaborative process.
Cognition,22:1?39.M.
Crocker, M. Pickering, and C. Clifton, ed-itors.
2000.
Architectures and Mechanismsin Sentence Comprehension.
Cambridge Uni-versity Press.M.
Dalrymple, S. Shieber, and F. Pereira.
1991.Ellipsis and higher-order unification.
Linguis-tics and Philosophy, 14(4):399?452.M.
Dalrymple, editor.
1999.
Syntax and Se-mantics in Lexical Functional Grammar: TheResource-Logic Approach.
MIT Press.V.
Ferreira.
1996.
Is it better to give than todonate?
Syntactic flexibility in language pro-duction.
Journal of Memory and Language,35:724?755.S.
Garrod and A. Anderson.
1987.
Saying whatyou mean in dialogue.
Cognition, 27:181?218.J.
Ginzburg and I.
A.
Sag.
2000.
InterrogativeInvestigations.
CSLI Publications.R.
Hausser.
1989.
Computation of Language.Springer-Verlag.P.
Hirschbu?hler.
1982.
VP deletion and across-the-board quantifier scope.
In Proceedings ofNELS 12.A.
Joshi and S. Kulick.
1997.
Partial proof treesas building blocks for a categorial grammar.Linguistics and Philosophy, 20:637?667.R.
Kaplan and A. Zaenen.
1989.
Long-distance dependencies, constituent structure,and functional uncertainty.
In M. Baltin andA.
Kroch, editors, Alternative Conceptions ofPhrase Structure, pages 17?42.
University ofChicago Press.R.
Kempson, W. Meyer-Viol, and D. Gabbay.2001.
Dynamic Syntax: The Flow of Lan-guage Understanding.
Blackwell.R.
Kibble, W. Meyer-Viol, D. Gabbay, andR.
Kempson.
2001.
Epsilon terms: a la-belled deduction account.
In H. Bunt andR.
Muskens, editors, Computing Meaning.Kluwer Academic Publishers.M.
Marcus.
1987.
Deterministic parsing anddescription theory.
In P. Whitelock et al, ed-itor, Linguistic Theory and Computer Appli-cations, pages 69?112.
Academic Press.W.
Meyer-Viol.
1995.
Instantial Logic.
Ph.D.thesis, University of Utrecht.R.
Millikan.
2004.
The Varieties of Meaning.MIT Press.G.
Neumann.
1994.
A Uniform Computa-tional Model for Natural Language Parsingand Generation.
Ph.D. thesis, Universita?t desSaarlandes.M.
Otsuka and M. Purver.
2003.
Incrementalgeneration by incremental parsing.
In Pro-ceedings of the 6th CLUK Colloquium.M.
Pickering and S. Garrod.
2004.
Toward amechanistic psychology of dialogue.
Behav-ioral and Brain Sciences, forthcoming.M.
Poesio and H. Rieser.
2003.
Coordination ina PTT approach to dialogue.
In Proceedingsof the 7th Workshop on the Semantics andPragmatics of Dialogue (DiaBruck).M.
Purver and M. Otsuka.
2003.
Incrementalgeneration by incremental parsing: Tacticalgeneration in Dynamic Syntax.
In Proceed-ings of the 9th European Workshop in NaturalLanguage Generation (ENLG-2003).S.
Shieber, F. Pereira, and M. Dalrymple.
1996.Interactions of scope and ellipsis.
Linguisticsand Philosophy, 19:527?552.M.
Stone and C. Doran.
1997.
Sentence plan-ning as description using tree-adjoining gram-mar.
In Proceedings of the 35th Annual Meet-ing of the ACL, pages 198?205.
