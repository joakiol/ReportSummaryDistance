Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 53?63,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsTwitter Polarity Classification with Label Propagationover Lexical Links and the Follower GraphMichael SperiosuUniversity of Texas at Austinsperiosu@mail.utexas.eduNikita SudanUniversity of Texas at Austinnsudan@utexas.eduSid UpadhyayUniversity of Texas at Austinsid.upadhyay@utexas.eduJason BaldridgeUniversity of Texas at Austinjbaldrid@mail.utexas.eduAbstractThere is high demand for automated tools thatassign polarity to microblog content such astweets (Twitter posts), but this is challengingdue to the terseness and informality of tweetsin addition to the wide variety and rapid evolu-tion of language in Twitter.
It is thus impracti-cal to use standard supervised machine learn-ing techniques dependent on annotated train-ing examples.
We do without such annota-tions by using label propagation to incorpo-rate labels from a maximum entropy classifiertrained on noisy labels and knowledge aboutword types encoded in a lexicon, in combina-tion with the Twitter follower graph.
Resultson polarity classification for several datasetsshow that our label propagation approach ri-vals a model supervised with in-domain an-notated tweets, and it outperforms the nois-ily supervised classifier it exploits as well asa lexicon-based polarity ratio classifier.1 IntroductionTwitter is a microblogging service where users postmessages (?tweets?)
of no more than 140 charac-ters.
With around 200 million users generating 140million tweets per day, Twitter represents one of thelargest and most dynamic datasets of user generatedcontent.
Along with other social networking web-sites such as Facebook, the content on Twitter is realtime: tweets about everything from a friend?s birth-day to a devastating earthquake can be found postedduring and immediately after an event in question.This vast stream of real time data has major im-plications for any entity interested in public opin-ion and even acting on what is learned and engag-ing with the public directly.
Companies have theopportunity to examine what customers and poten-tial customers are saying about their products andservices without costly and time-consuming surveysor explicit requests for feedback.
Political organi-zations and candidates might be able to determinewhat issues the public is most interested in, as wellas where they stand on those issues.
Manual inspec-tion of tweets can be useful for many such analyses,but many applications and questions require real-time analysis of massive amounts of social mediacontent.
Computational tools that automatically ex-tract and analyze relevant information about opinionexpressed on Twitter and other social media sourcesare thus in high demand.Full sentiment analysis for a given question ortopic requires many stages, including but not lim-ited to: (1) extraction of tweets based on an ini-tial query, (2) filtering out spam and irrelevant itemsfrom those tweets, (3) identifying subjective tweets,and (4) identifying the polarity of those tweets.
Likemost work in sentiment analysis, we focus on thelast stage, polarity classification.
The simplest ap-proaches are based on the presence of words oremoticons that are indicators of positive or nega-tive polarity (e.g.
Twitter?s own API, O?Connoret al (2010)), or calculating a ratio of positive tonegative terms (Choi and Cardie, 2009).
Thoughthese are a useful first pass, the nuance of lan-guage often defeats them (Pang and Lee, 2008).Tweets provide additional challenges compared toedited text; e.g.
they are short and include infor-mal/colloquial/abbreviated language.53Standard supervised classification methods im-prove the situation somewhat (Pang et al, 2002),but these require texts labeled with polarity as in-put and they do not adapt to changes in languageuse.
One way around this is to use noisy labels (alsoreferred to as ?distant supervision?
), e.g.
by tak-ing emoticons like ?:)?
as positive and ?:(?
as neg-ative, and train a standard classifier (Read, 2005; Goet al, 2009).1 Semi-supervised methods can alsoreduce dependence on labeled texts: for example,Sindhwani and Melville (2008) use a polarity lexi-con combined with label propagation.
Several haveused label propagation starting with a small numberof hand-labeled words to induce a lexicon for usein polarity classification (Blair-Goldensohn et al,2008; Rao and Ravichandran, 2009; Brody and El-hadad, 2010).In this paper, we bring together several of theabove approaches via label propagation using modi-fied adsorption (Talukdar and Crammer, 2009).
Thisalso allows us to explore the possibility of exploit-ing the Twitter follower graph to improve polarityclassification, under the assumption that people in-fluence one another or have shared affinities abouttopics.
We construct a graph that has users, tweets,word unigrams, word bigrams, hashtags, and emoti-cons as its nodes; users are connected based on theTwitter follower graph, users are connected to thetweets they created, and tweets are connected tothe unigrams, bigrams, hashtags and emoticons theycontain.
We seed the graph using the polarity valuesin the OpinionFinder lexicon (Wilson et al, 2005),the known polarity of emoticons, and a maximumentropy classifier trained on 1.8 million tweets withautomatically assigned labels based on the presenceof positive and negative emoticons, like Read (2005)and Go et al (2009).We compare the label propagation approach tothe noisily supervised classifier itself and to a stan-dard lexicon-based method using positive/negativeratios.
Evaluation is performed on several datasetsof tweets that have been annotated for polarity: theStanford Twitter Sentiment set (Go et al, 2009),1Davidov et al (2010) use 15 emoticons and 50 Twitterhashtags as proxies for sentiment in a similar manner, but theirevaluation is indirect.
Rather than predicting gold standard sen-timent labels, they instead predict whether those same emoti-cons and hashtags would be appropriate for other tweets.tweets from the 2008 debate between Obama andMcCain (Shamma et al, 2009), and a new datasetof tweets about health care reform that we have cre-ated.
In addition to performing standard per-tweetaccuracy, we also measure per-target accuracy (forhealth care reform) and an aggregate error metricover all users in our test set that captures how simi-lar predicted positivity of each user is to their actualpositivity.
Across all datasets and measures, we findthat label propagation is consistently better than thenoisily supervised classifier, which in turn outper-forms the lexicon-based method.
Additionally, forthe health care reform dataset, the label propagationapproach?which uses no gold labeled tweets, just ahand-created lexicon?outperforms a maximum en-tropy classifier trained on gold labels.
However, wedo not find the follower graph to improve perfor-mance with our current implementation.2 DatasetsWe use several different Twitter datasets as train-ing or evaluation resources.
From the annotateddatasets, only tweets with positive or negative polar-ity are used, so neutral tweets are ignored.
While im-portant, subjectivity detection is largely a differentproblem from polarity classification.
For example,Pang and Lee (2004) use minimum cuts in graphsfor the former and machine-learned text classifica-tion for the latter.
We also do not give any specialtreatment to retweets, though doing so is a possiblefuture improvement.2.1 Emoticon-based training set (EMOTICON)Emoticons are commonly exploited as noisy in-dicators of polarity?including by Twitter?s ownadvanced search ?with positive/negative attitude.
?While imperfect, there is potential for millions oftweets containing emoticons to serve as a sourceof noisy training material for a supervised classi-fier.
We create such a training set from a sampleof the ?garden hose?2 Twitter feed, from Septemberto December, 2009.
At the time of collection, thisincluded up to 15% of all tweets worldwide.From this feed, 6,265,345 tweets containing atleast one of the emoticons listed in Table 1 are ex-tracted; 5,156,277 contain a positive emoticon and2http://dev.twitter.com/pages/streaming_api54+ :) :D =D =) :] =] :-) :-D :-] ;) ;D ;] ;-) ;-D ;-]?
:( =( :[ =[ :-( :-[ :?
( :?
[ D:Table 1: Positive and negative emoticons.+ #ff, congrats, gracias, yay, thx, smile,awesome, hello, excited, moon, loving, glad,sweet, wonderful, birthday, enjoy, goodnight,amazing, cute, bom?
nickjonas, murphy, brittany, rip, triste, sad,hurts, died, snow, huhu, headache, upset,crying, throat, poor, sucks, ugh, sakit,stomach, horribleTable 2: Top 20 most predictive common unigram fea-tures for the positive and negative classes, in order frommore predictive to less predictive.1,109,068 contain a negative emoticon.
A smallnumber of tweets contain both negative and posi-tive emoticons.
These are permitted to appear twice,once for each label.
Then, a balanced ratio ofpositive/negative labels is obtained by keeping only1,109,068 of the positive tweets.
Finally, a large pro-portion of non-English tweets are excluded by a fil-ter that requires a tweet to have at least two words(with at least two characters) from the CMU Pro-nouncing Dictionary.3 A few non-English tweetspass through this filter and some English tweetswith very unusual words or incorrect spelling aredropped, but this simple strategy works well over-all.
The final training set contains 1,839,752 tweets,still balanced for positive and negative emoticons.Table 2 shows the 20 most predictive unigramfeatures of each class in the EMOMAXENT classi-fier (described below) that are among the 1000 mostcommon unigrams in this dataset and are not them-selves emoticons.
A few non-English (but polar-ized) words (e.g.
gracias, bom, triste) make it pastour simple language filter and onto these lists, butthe majority of the most predictive words are En-glish.
Other highly predictive words are artifactsof the particular tweet sample that comprises theEMOTICON dataset, such as ?nickjonas,?
?brittany,?and ?murphy,?
the latter two explained by the abun-3The dictionary contains 133k English words, including in-flected forms and proper nouns.
http://www.speech.cs.cmu.edu/cgi-bin/cmudictDataset Use Size % PosSTS dev 183 59.0OMD dev 1898 73.1HCR-TRAIN train 488 43.2HCR-DEV dev 534 32.2HCR-TEST test 396 38.6Table 3: Basic properties of the annotated datasets usedin this paper.dance of negative tweets after actress Brittany Mur-phy?s death.
Most others are intuitively good mark-ers of positive or negative polarity.2.2 Datasets with polarity annotationsThree annotated datasets, summarized in Table 3 anddescribed below, are used for training, development,or evaluation of polarity classifiers.Stanford Twitter Sentiment (STS).
Go et al(2009) created a collection of 216 annotated tweetson various topics.4 Of these, 108 tweets are positiveand 75 are negative.Obama-McCain Debate (OMD).
Shamma et al(2009) used Amazon Mechanical Turk to annotate3,269 tweets posted during the presidential debateon September 26, 2008 between Barack Obama andJohn McCain.
Each tweet was annotated by oneor more Turkers for the categories positive, nega-tive, mixed, or other.
We filter this dataset with twoconstraints in order to ensure high inter-annotatoragreement.
First, at least three votes must havebeen provided for a tweet to be included.
Second,more than half of the votes must have been posi-tive or negative; the majority label is taken as thegold standard for that tweet.
This results in a set of1,898 tweets.
Of these, 705 had positive gold labelsand 1192 had negative gold labels, and the averageinter-annotator agreement of the Turk votes for thesetweets was 83.7%.
To our knowledge, we are thefirst to perform automatic polarity classification onthis dataset.Health Care Reform (HCR).
We create a newannotated dataset based on tweets about health carereform in the USA.
This was a strongly debated4http://twittersentiment.appspot.com/55topic that created a large number of polarized tweets,especially in the run up to the signing of the healthcare bill on March 23, 2010.
We extract tweets con-taining the health care reform hashtag ?#hcr?
fromearly 2010; a subset of these are annotated by us andcolleagues for polarity (positive, negative, neutral,irrelevant) and polarity targets (health care reform,Obama, Democrats, Republicans, Tea Party, conser-vatives, liberals, and Stupak).
These are separatedinto training, dev and test sets.
As with the otherdatasets, we restrict attention in this paper only topositive and negative tweets.52.3 The Twitter follower graphOne of the key ideas we test in this paper is whethersocial connections can be used to improve polarityclassification for individual tweets and users.
Weconstruct the Twitter follower graphs for the users inthe above datasets in stages using publicly availabledata from the Twitter API.
From the full list of eachuser?s followers, we retain only followers foundwithin the datasets; this prunes unknown users whodid not tweet about the topic and thus are unlikely toprovide useful information.
This method for graphconstruction offers nearly complete graphs, but hastwo main disadvantages.
First, many users haveraised their privacy levels over time, which hindersthe ability to view their follower graph.
In thesecases only their tweet information is known.
Sec-ondly, due to the rapid pace of growth on Twit-ter, user graphs tend to grow quickly; thus our con-structed graph is a representation of the user?s cur-rent social graph and not the exact graph that existedat the time of the tweet.3 ApproachWe compare three main approaches: using lexicon-based positive/negative ratios, maximum entropyclassification and label propagation.3.1 Lexicon-based baseline (LEXRATIO)A reasonable baseline to use in polarity classifica-tion is to count the number of positive and negativeterms in a tweet and pick the category with moreterms (O?Connor et al, 2010).
This actually uses5A public release of this data, along with our code, is avail-able at https://bitbucket.org/speriosu/updown.supervision at the level of word types.
Like mostothers, we use the OpinionFinder subjectivity lexi-con,6 which contains 2,304 words annotated as pos-itive and 4,153 words as negative.
If the number ofpositive and negative words in a tweet is equal (in-cluding zero for both), the label is chosen at random.3.2 Maximum entropy classifier (MAXENT)The OpenNLP Maximum Entropy package7 is usedto train polarity classifiers using either EMOTICONor HCR-TRAIN, henceforth referred to as EMO-MAXENT and GOLDMAXENT, respectively.
Aftertokenizing on whitespace, unigram and bigramfeatures are extracted.
All characters are lowercasedand non-alphanumeric characters are trimmed fromthe left and right sides of tokens.
However, tokensthat contain no alphanumeric characters are nottrimmed.
Stop words8 are excluded as unigramfeatures.
However, bigram features are extracted be-fore stop words are removed since many stop wordsare informative in the context of content words: e.g.,contrast shit (negative) from the shit (very positive).The beginning and end of tweets are indicated by?$?
in bigram features.
Thus, the full feature set forthe tweet I love my new iPod Touch!
:D is [love,ipod, touch, $ i, i love, love my,my ipod, ipod touch, touch :D, :D$].
The same tokenization method is used for alldatasets in this paper.3.3 Label Propagation (LPROP)Tweets are not created in isolation?each tweet islinked to other tweets by the same author, and eachauthor is influenced by the tweets of those he or shefollows.
Common vocabulary and topics of discus-sion also connect tweets to each other.
Graph-basedmethods such as label propagation (Zhu and Ghahra-mani, 2002; Baluja et al, 2008; Talukdar and Cram-mer, 2009) provide a natural means to represent andexploit such relationships in order to improve classi-fication, often while requiring less supervision thanwith standard classification.
Label propagation al-gorithms spread label distributions from a small set6http://www.cs.pitt.edu/mpqa/opinionfinderrelease/7http://incubator.apache.org/opennlp/8Taken from: http://www.ranks.nl/resources/stopwords.html56Opinion FinderN-GramsHashtagsEmoticonsU1Ahhh#ObamacareI Love#NY!
{Tweet3} {Tweetn}U2We can't passthis :( #Killthebilllovehateenjoyi lovelove nywe can't#ny#obamacare#killthebill:):( =]U3{Tweetn}Un{Tweetn}...EmoMaxent SeedsLabeled SeedsUnseeded....... .
....Figure 1: An illustration of our graph with All-edges and Noisy-seed (see text for description).of nodes seeded with some initial label information(always noisy, heuristic information rather than goldinstance labels in our case) throughout the graph.Label distributions are spread across a graph G ={V,E,W} where V is the set of n nodes, E is a setofm edges andW is an n?nmatrix of weights, withwij as the weight of edge (i, j).
We use ModifiedAdsorption (MAD) (Talukdar and Crammer, 2009)over a graph with nodes representing tweets, authorsand features, while varying the seed information andthe construction of the edge sets.
The spreading ofthe label distributions can be viewed as a controlledrandom walk with three possible actions: (i) inject-ing a seeded node with its seed label, (ii) continu-ing the walk from the current node to a neighbor-ing node, and (iii) abandoning the walk.
MAD takesthree parameters, ?1, ?2 and ?3, which control therelative importance of each of these actions, respec-tively.
We use the Junto Label Propagation Toolkit?simplementation of MAD in this paper.9Modified Adsorption requires some nodes in thegraph to have seed distributions, which can come fora variety of knowledge sources.
We consider the fol-lowing variants for seeding the graph:?
Maxent-seed: EMOMAXENT is trained on theEMOTICON dataset; every tweet node is seeded9http://code.google.com/p/junto/with its polarity predictions for the tweet.?
Lexicon-seed: Nodes are created for every wordin the OpinionFinder lexicon.
Positive words areseeded as 90% positive if they are strongly subjec-tive and 80% positive if weakly subjective; simi-larly and conversely for negative words.
Everytweet is connected by an edge to every word inthe polarity lexicon it contains, using the weight-ing scheme discussed with Feature-edges below.?
Emoticon-seed: Nodes are created for emoticonsfrom Table 1 and seeded as 90% positive or nega-tive depending on their polarity.?
Annotated-seed: The annotations in HCR-TRAIN are used to seed the tweets from thatdataset as 100% positive or negative, in accor-dance with the label.We use Noisy-seed as a collective term for all of theabove seed sets except Annotated-seed.The other main aspect of graph construction isspecifying edges and their weights.
We consider thefollowing variants:?
Follower-edges: When a user A follows anotheruser B, we add an edge from A to B with a weightof 1.0, a weight that is comparable to that of amoderately frequent word in Feature-edges below.?
Feature-edges: Nodes are added for hashtags andthe features described in ?3.2 and connected to the57tweets that contain them.
An edge connecting atweet t to a feature f has weight wtf using rel-ative frequency ratios of the feature between thedataset d in question and the EMOTICON datasetas a reference corpus r:wtf ={log Pd(f)Pr(f) if Pd(f) > Pr(f)0 o.w.
(1)We use All-edges when combining both edge sets.Figure 1 illustrates the connections for All-edgesand Noisy-seed by example.
Each user un is at-tached to anyone who follows them or who theyfollow.
Each user is also connected to the tweetsthey authored.
Words from OpinionFinder are con-nected to tweets that contain those words, and sim-ilarly for hashtags, emoticons, unigrams, and bi-grams.
Emoticons and words from OpinionFinderare seeded according to the explanation above.
Alledges other than Feature-edges are given a weight of1.0.4 Results4.1 Parameter tuningWe evaluated our models on the STS, OMD, andHCR-DEV datasets during development and keptHCR-TEST as a final held-out test set used once, af-ter all relevant parameters had been set.
For Mod-ified Adsorption, 100 iterations were used, and aseed injection parameter ?1 of .005 gave the bestbalance of allowing seed distributions to affect othernodes without overwhelming them.
The Junto de-fault value of .01 was used for both ?2 and ?3.4.2 Per-tweet accuracyTable 4 shows the per-tweet accuracy results ofthe random baseline, the LEXRATIO baseline, theEMOMAXENT classifier alone, the LPROP classifierrun only on Follower-edges with Maxent-seed, theLPROP classifier run on the full graph from Figure 1only seeded with Lexicon-seed, and the LPROP clas-sifier run on All-edges and Noisy-seed.For all datasets, LPROP with Feature-edges andNoisy-seed outperforms or matches all other meth-ods.
For STS, our best result of 84.7% accu-racy beats Go et al (2009)?s reported best resultClassifier MSERandom .167LEXRATIO .170EMOMAXENT .233LPROP (Follower-edges, Maxent-seed) .233LPROP (All-edges, Lexicon-seed) .187LPROP (Feature-edges, Noisy-seed) .148LPROP (All-edges, Noisy-seed) .148Table 5: Mean squared error (MSE) per-user on HCR-TEST, for users with at least 3 tweetsof 82.7%.
Their approach uses a Maxent classifiertrained on a noisily labeled emoticon training setsimilar to our EMOTICON dataset.
Note that theyalso remove neutral tweets from the test set.Our semi-supervised label propagation methodcompares favorably to fully supervised approaches.For example, a graph with Feature-edges seededwith gold labels from HCR-TRAIN (i.e.
Annotated-seed) obtains only 64.6% per-tweet accuracy onHCR-TEST.
A maximent entropy classifier trainedon HCR-TRAIN achieves 66.7%.
Our best labelpropagation approach surpasses both of these at71.2%.We find that in general Follower-edges are nothelpful as implemented here.
Further work is neededto explore more nuanced ways of modeling the so-cial graph, such as allowing leaders to influence fol-lowers more than vice versa.4.3 Per-user errorIn many sentiment analysis applications, it is of in-terest to know what the polarity of a given individualor the overall polarity toward a particular product is.Here we compare the positivity ratio predicted byour methods to that in the gold standard labels on aper-user basis, using the mean squared error betweenthe predicted positivity ratios ppr and the actual ra-tios apr for all users:MSE(ppr, apr) =?i(apri ?
ppri)2Where apri and ppri are the actual and predictedpositivity ratios of the ith user.Table 5 gives MSE results on HCR-TEST forusers with at least 3 tweets.
LPROP (Feature-edges,58Classifier STS OMD HCR-DEV HCR-TESTRandom 50.0 50.0 50.0 50.0LEXRATIO 72.1 59.1 54.3 58.1EMOMAXENT 83.1 61.3 58.6 62.9LPROP (Follower-edges, Maxent-seed) 83.1 61.2 57.9 62.9LPROP (All-edges, Lexicon-seed) 70.0 62.6 64.6 64.6LPROP (Feature-edges, Noisy-seed) 84.7 66.7 65.7 71.2LPROP (All-edges, Noisy-seed) 84.7 66.5 65.2 71.0Table 4: Per-tweet accuracy percentages.
The models and parameters were developed while tracking performance onSTS, OMD, and HCR-DEV, and HCR-TEST results were obtained from a single, blind run.+ pow pow, good debate, hack the, hack$ barackobama, barackobama, the vp,good job, to vote, john is, is to, obama did,they both, gergen, knowledge, voting for,for veterans, the veterans, america, will take?
language, this was, drinking, terrorists,government, china, obama i, that we, father,obama in, mc, diplomacy, wars, afghanistan,debt, simply, financial, the spin, the bottom,bottomTable 7: Top 20 most positive and most negative n-gramsin OMD after running LPROP with All-edges and Noisy-seed.
Note that ?$?
indicates the beginning or end of atweet.Noisy-seed) and LPROP (All-edges, Noisy-seed) aretied for the lowest error.4.4 Per-target accuracyTable 6 gives results on a per-target basis for thefive most common targets in the HCR-TEST dataset,in order from most common to least common: hcr,dems, obama, gop, and conservatives.
The per-centages reflect the fraction of tweets correctly la-beled for each target.
These distributions are highlyskewed: the hcr target covers about 69% of thetweets, while the conservatives target covers onlyabout 5%.
Thus performance on the hcr targettweets is most important for overall accuracy.5 DiscussionPolar language An attractive property of labelpropagation algorithms is that label distributions canbe obtained for nodes other than the tweets (and im-+ human, stupak, you do, sunday, firedvote for, yes on, $ we, vote yes, to vote,vote on, goal, nation, do it, up to, ago, votes,this #hcr, #hcr is, on #hcr?
gop, #tlot #hcr, #tcot #tlot, 12, #topprog,medicare, #tlot, #tlot $, #ocra, cbo,tea party, tea, passes, #hhrs, $ dems, #hc,#obamacare, #sgp, dems, do notTable 8: Top 20 most positive and most negative n-gramsin HCR-TEST after running LPROP with All-edges andNoisy-seed.portantly, nodes that were unseeded).
For example,all of the feature nodes?unigrams, bigrams, andhashtags?have a loading for the positive and neg-ative labels.
These could be used for various vi-sualizations of the results of the polarity classifica-tion, including terms that are the most positive andnegative and also highlighting or bolding such termswhen showing a user individual tweets.Table 7 shows the 20 unigrams and bigrams withthe highest and lowest ratio of positive label prob-ability to negative label probability after runningLPROP with All-edges and Noisy-seed.
These listsare restricted to terms that had an edge weight of atleast 1.0, i.e.
that were twice as frequent in OMDcompared to the reference corpus, that had a rawcount of at least 5 in OMD, and that didn?t al-ready appear in the OpinionFinder lexicon.
Some ofthe terms are intuitively positive and negative, e.g.good job and wars.
Others reflect more specific as-pects of the OMD dataset, such as good debate andafghanistan.Table 8 shows the top 20 for HCR-TEST.
Many59hcr dems obama gop conservativesClassifier (274) (27) (26) (22) (20)LEXRATIO 58.0 64.8 69.2 50.0 52.5EMOMAXENT 62.4 66.7 73.1 68.2 60.0LPROP (Follower-edges, Maxent-seed) 62.4 66.7 73.1 68.2 60.0LPROP (All-edges, Lexicon-seed) 60.6 85.2 73.1 86.4 60.0LPROP (Feature-edges, Noisy-seed) 69.0 81.5 80.8 86.4 70.0LPROP (All-edges, Noisy-seed) 69.0 77.8 80.8 86.4 70.0Table 6: Per-target accuracy percentages for HCR-TEST.
The number of tweets for each target is given in parentheses.terms simply reflect a rallying to either pass or defeatthe healthcare reform bill (vote for, do not).
Otherpositive words represent more abstract concepts pro-ponents of the bill may be expressing (human, goal).Conversely, opponents such as those who would at-tend a tea party are concerned about what they call#obamacare.Domain differences There are several reasonswhy performance is much lower on both the OMDand HCR datasets than on STS.
First, both theEMOTICON (noisy) training set and the STS dev setare general in topic.
Correct estimations of the posi-tivity and negativity of general words in the trainingset like yay and upset are more likely to be usefulin a broad-domain evaluation set, whereas misesti-mations of the weights of more specific words andbigrams are likely to be washed out.
In contrast,the OMD and HCR datasets contain a very differ-ent vocabulary distribution from the STS set.
Wordsand phrases referring to specific political issues likehealth care and iraq war have frequencies that areorders of magnitude higher than either the EMOTI-CON training set or the STS dev set.
Thus, misesti-mations of the positivity or negativity of these fea-tures will be amplified in evaluation.
Lastly, expres-sion of political opinions tends to be more nuancedthan the general opinions and feelings, simply dueto the complex nature of political issues.
Everyoneagrees that a sore throat is bad, while it is less ob-vious how much government involvement in healthcare is beneficial.LEXRATIO vs. EMOMAXENT LEXRATIO haslow coverage for words that tend to indicate positiveand negative sentiment in particular domains.
Forexample, STS has the tweet In montreal for a longweekend of R&R.
Much needed, with a positive goldlabel.
The only word in this tweet in the Opinion-Finder lexicon is long, which is labeled as negative.Thus, LEXRATIO incorrectly classifies the tweet asnegative.
EMOMAXENT correctly labels this tweetpositive due to features like weekend being strongindicators of the positive class.
Similarly, the tweetBooz Allen Hamilton has a bad ass homegrown so-cial collaboration platform.
Way cool!
#ttiv is la-beled negative by LEXRATIO due to the presence ofbad.
While EMOMAXENT has a negative preferencefor both bad and ass, it has a strong positive prefer-ence for bad ass, as well as both cool and way cool.EMOMAXENT vs. LPROP As seen from the per-tweet and per-user results, LPROP does consistentlybetter than MAXENT.
We now discuss one exampleof this improvement from the OMD set.
One userauthored the following four tweets:?
t1: obama +3 the conspicuousness of their pres-ence is only matched by our absence #tweetdebate?
t2: Fundamentally, if McCain fundamentally uses?fundamental?
one more time, I?m gonna go nuts.#tweetdebate?
t3: McCain likes the bears in Montana joke toomuch#tweetdebate #current?
t4: We are less respected now... Obama #current#debate08 And I give credit to McCain... NOOOThe gold label for t1 is positive and the rest are nega-tive.
All of the LPROP classifiers correctly predictedthe labels for all four tweets.
EMOMAXENT missedt2 and t3, so this primarily negative user is incor-rectly indicated as primarily positive by EMOMAX-ENT.
LPROP gets around this by propagating senti-ment polarity through unigram features in this case.60The unigram mccain has an edge weight to tweetsthat contain it of 8.6 for the OMD corpus, meaningmccain is much more frequent in this corpus thanthe reference corpus, so any sentiment associatedwith mccain is propagated strongly.
In this case, theoutput of label propagation seeded with Noisy-seedreveals that mccain has negative sentiment for thisdataset.6 Related WorkMuch work in sentiment analysis involves the useand generation of dictionaries capturing the senti-ment of words.
These methods range from manualapproaches of developing domain-dependent lexi-cons (Das and Chan, 2001) to semi-automated ap-proaches (Hu and Liu, 2004) and fully automatedapproaches (Turney, 2002).
Melville et al (2009)use a unified framework combining background lex-ical information in terms of word-class associationsand refine this information for specific domains us-ing any available training examples.
They producebetter results than using either a lexicon or training.O?Connor et al (2010) use the OpinionFindersubjectivity lexicon to label the polarity of tweetsabout Barack Obama and compare daily aggregatesentiment scores to the Gallup poll time series ofmanually gathered approval ratings of Obama.
Evenwith this simple polarity determination, they findsignificant correlation between their predicted ag-gregate sentiment per day and the Gallup poll.Using the OMD dataset, Shamma et al (2009)find that amount of Twitter activity is a good pre-dictor of topic changes during the debate, and thatthe content of concurrent tweets reflects a mix ofthe current debate topic and Twitter users?
reactionsto that topic.
Diakopoulos and Shamma (2010) usethe same dataset to develop analysis and visualiza-tion techniques to aid journalists and others in un-derstanding the relationship between the live debateevent and the timestamped tweets.Bollen et al (2010) perform aggregate sentimentanalysis on tweets over time, comparing predictedsentiment to time series such as the stock marketand crude oil prices, as well as major events suchas election day and Thanksgiving.
However, the au-thors use hand-built rules for classification based onthe Profile of Mood States (POMS) and largely eval-uate based on inspection.7 ConclusionWe have improved upon existing tweet polarity clas-sification methods by combining several knowledgesources with a noisily supervised label propagationalgorithm.
We show that a maximum entropy clas-sifier trained with distant supervision works betterthan a lexicon-based ratio predictor, improving theaccuracy for polarity classification on our held-outtest set from 58.1% to 62.9%.
By using the predic-tions of that classifier in combination with a graphthat incorporates tweets and lexical features, we ob-tain even better accuracy of 71.2%.We did not find overall gains from using the fol-lower graph as implemented here.
There is roomfor improvement in the way the follower graph isencoded in our graph, particularly with respect tousing asymmetric relationships rather than an undi-rected graph, and in how follower relationships areweighted.Another source of information that could be usedto improve results is the text in pages that havebeen linked to from a tweet.
In many cases, it isonly possible to know what the polarity is by look-ing at the page being linked to.
Our label propa-gation setup can incorporate this straightforwardlyby adding nodes for those pages plus edges betweenthem and all tweets that reference them.AcknowledgmentsThis research was supported by a grant from theMorris Memorial Trust Fund of the New York Com-munity Trust.
We thank Leif Johnson for providingthe tweets from the Twitter firehose for the EMOTI-CON and HCR datasets, Partha Talukdar for theJunto label propagation toolkit, and the UT NaturalLanguage Learning reading group for helpful feed-back.ReferencesShumeet Baluja, Rohan Seth, D. Sivakumar, Yushi Jing,Jay Yagnik, Shankar Kumar, Deepak Ravichandran,and Mohamed Aly.
Video suggestion and discoveryfor youtube: taking random walks through the viewgraph.
In WWW ?08: Proceeding of the 17th interna-tional conference on World Wide Web, pages 895?904,New York, NY, USA, 2008.
ACM.61S.
Blair-Goldensohn, K. Hannan, R. McDonald, T. Ney-lon, G. Reis, and J. Reynar.
Building a sentimentsummarizer for local service reviews.
In WWWWorkshop on NLP in the Information Explosion Era(NLPIX), 2008.
URL http://www.ryanmcd.com/papers/local_service_summ.pdf.J.
Bollen, A. Pepe, and H. Mao.
Modeling public moodand emotion: Twitter sentiment and socio-economicphenomena.
In Proceedings of the 19th InternationalWorld Wide Web Conference, 2010.Samuel Brody and Noemie Elhadad.
An unsupervisedaspect-sentiment model for online reviews.
In Hu-man Language Technologies: The 2010 Annual Con-ference of the North American Chapter of the As-sociation for Computational Linguistics, HLT ?10,pages 804?812, Stroudsburg, PA, USA, 2010.
As-sociation for Computational Linguistics.
ISBN 1-932432-65-5.
URL http://portal.acm.org/citation.cfm?id=1857999.1858121.Yejin Choi and Claire Cardie.
Adapting a polar-ity lexicon using integer linear programming fordomain-specific sentiment classification.
In Proceed-ings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing, pages 590?598.
Association for Computational Linguistics, 2009.URL http://www.aclweb.org/anthology/D/D09/D09-1062.S.
Das and M. Chan.
Extracting market sentiment fromstock message boards.
Asia Pacific Finance Associa-tion, 2001, 2001.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
En-hanced sentiment learning using twitter hashtags andsmileys.
In Proceedings of the 23rd International Con-ference on Computational Linguistics, 2010.Nicholas A. Diakopoulos and David A. Shamma.
Char-acterizing debate performance via aggregated twittersentiment.
In Proceedings of the 28th internationalconference on Human factors in computing systems,pages 1195?1198, 2010.Alec Go, Richa Bhayani, and Lei Huang.
Twitter senti-ment classification using distant supervision.
Unpub-lished manuscript.
Stanford University, 2009.Minqing Hu and Bing Liu.
Mining and summarizing cus-tomer reviews.
In KDD ?04: Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 168?177, NewYork, NY, USA, 2004.
ACM.
ISBN 1-58113-888-1.doi: http://doi.acm.org/10.1145/1014052.1014073.Prem Melville, Wojciech Gryc, and Richard D.Lawrence.
Sentiment analysis of blogs by combin-ing lexical knowledge with text classification.
In KDD?09: Proceedings of the 15th ACM SIGKDD interna-tional conference on Knowledge discovery and datamining, pages 1275?1284, New York, NY, USA, 2009.ACM.
ISBN 978-1-60558-495-9. doi: http://doi.acm.org/10.1145/1557019.1557156.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
From tweetsto polls: Linking text sentiment to public opiniontime series.
In Proceedings of the International AAAIConference on Weblogs and Social Media, 2010.B.
Pang, L. Lee, and S. Vaithyanathan.
Thumbs up?
:sentiment classification using machine learning tech-niques.
In Proceedings of the ACL-02 conference onEmpirical methods in natural language processing-Volume 10, pages 79?86, 2002.Bo Pang and Lillian Lee.
A sentimental education: senti-ment analysis using subjectivity summarization basedon minimum cuts.
In Proceedings of the 42nd An-nual Meeting on Association for Computational Lin-guistics, 2004.Bo Pang and Lillian Lee.
Opinion mining and sentimentanalysis.
Foundations and Trends in Information Re-trieval, 2(1-2):1?135, 2008.Delip Rao and Deepak Ravichandran.
Semi-supervisedpolarity lexicon induction.
In Proceedings of the12th Conference of the European Chapter of the ACL(EACL 2009), pages 675?682.
Association for Com-putational Linguistics, 2009.
URL http://www.aclweb.org/anthology/E09-1077.Jonathon Read.
Using emoticons to reduce dependencyin machine learning techniques for sentiment classifi-cation.
In Proceedings of the ACL Student ResearchWorkshop, ACLstudent ?05, pages 43?48, Strouds-burg, PA, USA, 2005.
Association for ComputationalLinguistics.
URL http://portal.acm.org/citation.cfm?id=1628960.1628969.David A. Shamma, Lyndon Kennedy, and Elizabeth F.Churchill.
Tweet the debates: understanding commu-nity annotation of uncollected sources.
In Proceedingsof the first SIGMM workshop on Social media, pages3?10, 2009.Vikas Sindhwani and Prem Melville.
Document-word co-regularization for semi-supervised sentiment analysis.In Proceedings of IEEE International Conference onData Mining (ICDM-08), 2008.Partha Talukdar and Koby Crammer.
New regularizedalgorithms for transductive learning.
In Wray Bun-tine, Marko Grobelnik, Dunja Mladenic, and JohnShawe-Taylor, editors, Machine Learning and Knowl-edge Discovery in Databases, volume 5782, pages442?457.
Springer Berlin / Heidelberg, 2009.62P.
D. Turney.
Thumbs up or thumbs down?
semantic ori-entation applied to unsupervised classification of re-views.
In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics, pages417?424, 2002.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi, ClaireCardie, Ellen Riloff, and Siddharth Patwardhan.
Opin-ionFinder: A system for subjectivity analysis.
In Proc.Human Language Technology Conference and Con-ference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP-2005) Companion Volume(software demonstration), 2005.Xiaojin Zhu and Zoubin Ghahramani.
Learning from la-beled and unlabeled data with label propagation.
Tech-nical Report CMU-CALD-02-107, Carnegie MellonUniversity, 2002.63
