Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1478?1488,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsInvestigating Language Universal and Specific Propertiesin Word EmbeddingsPeng Qian Xipeng Qiu?Xuanjing HuangShanghai Key Laboratory of Intelligent Information Processing, Fudan UniversitySchool of Computer Science, Fudan University825 Zhangheng Road, Shanghai, China{pqian11, xpqiu, xjhuang}@fudan.edu.cnAbstractRecently, many NLP tasks have benefit-ed from distributed word representation.However, it remains unknown whetherembedding models are really immune tothe typological diversity of languages,despite the language-independent archi-tecture.
Here we investigate three repre-sentative models on a large set of languagesamples by mapping dense embedding tosparse linguistic property space.
Experi-ment results reveal the language universaland specific properties encoded in variousword representation.
Additionally, strongevidence supports the utility of word form,especially for inflectional languages.1 IntroductionWord representation is a core issue in naturallanguage processing.
Context-based word rep-resentation, which is inspired by Harris (1954),has achieved huge successes in many NLP ap-plications.
Despite its popularity, character-basedapproach also comes out as an equal competitor(Santos and Zadrozny, 2014; Kim et al, 2016;Ling et al, 2015b; Ling et al, 2015a; Faruqui etal., 2016; Ballesteros et al, 2015) .
Moreover,questions arise when we consider what thesemodels could capture from linguistic cues underthe perspective of cross-language typological di-versity, as is argued by Bender (2009).Despite previous efforts in empirically inter-preting word embedding and exploring the in-trinsic/extrinsic factors in learning process (An-dreas and Klein, 2014; Lai et al, 2015; K?ohn,2015; Melamud et al, 2016), it remains unknownwhether embedding models are really immune tothe structural variance of languages.
?Corresponding author.Current research has gaps for understandingmodel behaviours towards language typologicaldiversity as well as the utility of context andform for different languages.
Thus, we selectthree representative types of models and designa series of experiments to reveal the universalsand specifics of various word representationson decoding linguistic properties.
Our workcontributes to shedding new insights into thefollowing topics:a) How do typological differences of languagestructure influence a word embedding mod-el?
Does a model behave similarly towardsphylogenetically-related languages?b) Is word form a more efficient predictor of a cer-tain grammatical function than word contextfor specific languages?c) How do the neurons of a model respondto linguistic features?
Can we explain theutility of context and form by analyzing neuronactivation pattern?2 Experiment DesignTo study the proposed questions above, we designfour series of experiments to comprehensivelycompare context-based and character-based wordrepresentations on different languages, coveringsyntactic, morphological and semantic properties.The basic paradigm is to decode interpretablelinguistic features from a target collection ofword representations.
We hypothesize that thereexists a linear/nonlinear map between a wordrepresentation x and a high-level sparse featurevector y if the word vector implicitly encodesufficient information1.
Figure1 visualizes how a1Our experiment results show that nonlinear mappingmodel significantly works better than linear map forall languages.
Only nonlinear mapping accuracies arementioned in the following sections due to the space limit.1478word embedding is mapped to different linguisticattribute vectors.
For example, the Czech wordd?etem means children in English.
Its grammaticalgender is female.
It is in the plural form andshould be used in dative case.
These are all impor-tant properties of a word.
The word embedding ofd?etem is mapped to different sparse representationof these lexical properties respectively.Listed in Table1 is the outline of the experi-ments.ID Attribute CategoryI Part-of-SpeechSyntaxII Dependency RelationIIIGender / Number / CaseMorphologyAnimacy / Definite / PersonTense / Aspect / Mood / VoicePronType / VerbFormIV Sentiment Score SemanticsTable 1: Outline of Experiment Design.For linear map, we train a matrix ?
that mapsword embedding x to a sparse feature vector ywith the least L2error.
For nonlinear map, wetrain a neural network (MLP) with 4 hidden layersvia back propagation.
Their dimensions are 50,80, 80, and 50 in order.
For each linguistic featureof each language, a mapping model is trained onthe randomly-selected 90% of the words with thetarget feature and tested over the remaining 10%.Details about the construction of the linguisticfeature vectors will be mentioned in the specificsection of a certain experiment.For syntactic and morphological features, weconstruct the corresponding feature vectors of aword from the Universal Dependencies Treebank(Joakim Nivre and Zhu, 2015) and the ChineseTreebank (CTB 7.0) (Xue et al, 2010).
For acertain word w with a certain linguistic attributea (e.g.
POS), w may be annotated with one ordifferent labels (e.g.
NOUN, VERB, etc) from thepossible label set of a in the whole treebank.
Wecalculate the normalized label frequency distribu-tion~yawfrom the manual annotation of the corpusas the representation of the linguistic attribute afor the word w in each language.For word sentiment feature, we use the manual-ly annotated data collected by Dodds et al (2015).The data contains emotion scores for a list ofwords in several languages.
In our experiment,the original score scale in Dodds et al (2015) istransformed into the interval [0, 1].0 1 0 0 1 0 0 00 00.082 -0.022  0.077   0.006  0.051     ???
-0.1370.082 -0.022 0.077 0.066 0.051 -0.137??
?1 0 0??
?d?tem (chidren.Female.Plural.Dative)Gender Case Number0.082 -0.022 0.077 0.066 0.051 -0.137??
?d?tem (chidren.Female.Plural.Dative)0 1 0 0 1 0 0 00 0 1 0 0??
?Gender Case NumberFigure 1: Visualizing experiment paradigm.
Thedense representation of a Czech word d?etem ismapped to different sparse representation of thelexical properties respectively.3 Embedding Model DescriptionFaced with three questions proposed before, weselect the following models from various candi-dates, as they are popular, representative and basedon either word context or purely word form.Type I C&W Model (referred as CW in short),which aims to estimate the joint probability of aword sequence (Collobert et al, 2011).
In thispaper, C&W word vectors are all from the releasedversion of the polyglot multilingual embeddings(Al-Rfou et al, 2013) trained on Wikipedia.Type II Skip-gram2(referred as SG in short),which aims to predict the context words based onthe target word.
We use word2vec (Mikolov etal., 2013) to train SG on multilingual Wikipedieaprovid d by (Al-Rfou et al, 2013).Type III Character-based LSTM autoencoder(referred as AE in short), which takes the charactersequence of a word as the input and reconstruct theinput character sequence.
It takes the advantageof pure word form instead of the context.
Thehidden layer vector of the model is used as arepresentation of the word.
In this way, we areable to quantify the utility of pure word form byevaluating the representation generated from thecharacter-based LSTM autoencoder on differentdecoding tasks.
We trained one-hidden layer AEwith the words covered in CW for each languageindependently.To ensure a fair comparison, all the wordvectors have the same dimension 64.
CW and SGare trained with a common 5-word window size.4 Results4.1 Part-of-SpeechIn experiment I, we decode Part-of-Speech, themost basic syntactic feature, from word embed-2SG results for some languages are missed due to the lackof the corpus data or special preprocessing.1479ISO Language |V | CW SG AEar Arabic 24967 0.712 0.658 0.648Iga Irish 3164 0.826 ?
0.697zh Chinese 30496 0.780 0.721 n/a IIfa Persian 11471 0.895 0.827 0.746IIIla Latin 6678 0.746 ?
0.707hi Hindi 12703 0.858 0.799 0.592IVta Tamil 1940 0.768 ?
0.541eu Basque 11212 0.857 ?
0.711et Estonian 2166 0.862 0.765 0.530Vfi Finnish 26086 0.910 0.818 0.715hu Hungarian 6105 0.912 0.831 0.674de German 29899 0.916 0.902 0.74 VIfr French 29445 0.905 0.889 0.759VIIpt Portuguese 17715 0.927 0.903 0.746he Hebrew 22754 0.911 ?
0.680ru Russian 55416 0.959 0.913 0.906hr Croatian 12581 0.926 0.862 0.790da Danish 10705 0.913 0.913 0.666sv Swedish 8408 0.938 0.888 0.670no Norwegian 18709 0.926 0.861 0.704sl Slovenian 19514 0.919 0.820 0.756cs Czech 55789 0.949 0.883 0.853ro Romanian 3170 0.858 0.814 0.618en English 15116 0.857 0.839 0.659id Indonesian 15635 0.852 0.819 0.801it Italian 21184 0.902 0.880 0.700VIIIes Spanish 33696 0.906 0.883 0.75el Greek 8499 0.937 0.879 0.801pl Polish 18062 0.941 0.842 0.800bg Bulgarian 17079 0.920 0.852 0.741Table 2: Model comparison on decoding POS,along with WALS word-order features.
TypeI: VS+VO+Pre+NR.
II: SV+VO+Pre+RN.
III:SV+OV+Pre+NR.
IV: SV+OV+Post+RN/Co.
V:SV+OV+Post+NR.
VI: SV+ND+Pre+NR.
VII:SV+VO+Pre+NR.
VIII: ND+VO+Pre+NR.ding.
To construct the POS vector for each word,we calculate the normalized POS-tag frequencydistribution from the manual annotation of the U-niversal Dependencies (Version 1.2) (De Marneffeet al, 2014) and Chinese Treebank (CTB 7.0)(Xue et al, 2010) for each language.We evaluate the predicted results by judgingwhether the most probable POS tag of a wordpredicted by the model equals to the most probablecorrect POS tag of the word.
Formally, for a set ofwords W in a language, the correct tag of the ithword Wiis yaWiand the predicted tag is y?aWi.
Theaccuracy is computed as:acc =1|W ||W |?i?
(y?aWi, yaWi) (1)?
(y?aWi, yaWi) ={1 y?aWi= yaWi0 otherwise(2)It is obvious that context-based representation(CW and SG) performs better than character-basedrepresentation (AE).
We, however, notice thatAE peforms nearly as well as the context-basedembedding on Russian, Czech and Indonesian.0.7 0.75 0.8 0.85 0.9 0.95IIIIIIIVVVIVIIAccuracyWordOrderTypeFigure 2: Interaction between CW performanceson decoding POS tag and WALS word orderfeatures.It turns out that these languages employ affixmarkers to indicate the POS category of a word.For example, in Indonesian, co-occurrence of theprefix ?me-?
and the suffix ?-kan?
in the word formmeans that this word is a verb.Besides, we explore the relationship betweenCW performances on decoding POS tags andthe word order typology of different languages,since CW is sensitive to word order.
We classifythe languages into 8 types, based on the basicword order features (Order of Subject and Verb;Order of Object and Verb; Order of Noun andAdposition; Order of Noun and Relative clause)from the World Atlas of Language Structures(Dryer and Haspelmath, 2013).
Figure 2 showsthat CW performs similar in this experiment forlanguages of the same word order type, indicatingan implicit interaction between typological diver-sity and model performance.4.2 Dependency RelationIn this section, we will get into the details ofExperiment II: decoding dependency relation fromword representation.
Dependency relation refersto how a word is syntactically related to otherwords in a sentence.
It is the label annotated onthe arc of the dependency tree.We compute the normalized frequency distri-bution of dependency relations for each word inthe Universal Dependency Treebank and ChineseTreebank (CTB 7.0) (Xue et al, 2010).
The distri-bution of dependency relations is the probabilisticdistribution of different arc types, such as subject,object, nmod, etc.
Evaluation is similar to that inSection 4.1.We can see from Figure 3 that the overallperformance is worse than that in Experiment I,as dependency analysis is more difficult than POSinduction.
CW achieves the best performance.
It1480zh id fasv en noda de hi frroitespteletfi bg hr sl pl0.30.40.50.6SlavicAccuracyCW SG AEFigure 3: Comparison of models on decodingDEPENDENCY RELATION.cssl pl bgnodasvel hi itespt0.70.80.91RomanceSlavicAccuracyCW SG AEFigure 4: Comparison of models on decodingGENDER.is also interesting to see that all the embeddingswork slightly better on Slavic languages.4.3 Morphological FeaturesExperiment III aims to decode morphologicalinformation from various word representation.
E-valuation is similar to that in Section 4.1.
Morpho-logical information refers to the explicit marker ofthe grammatical functions.
We consider 12 mor-phological features, as is shown in Table 1.
Theycan be split into 5 nominal features (GENDER,NUMBER, CASE, ANIMACY, DEFINITENESS)and 7 verbal features (PERSON, TENSE, ASPECT,MOOD, VOICE, PRONOUNTYPE, VERBFORM).Gender is a very special feature for westernlanguages.
It is partially based on semantics, suchas biological sex.
In most of the languages withgender features, there are agreements between thenoun and the determiners.
This could be a goodindicator for context-based model.
On the otherhand, gender is also expressed as an inflectionalfeature via declension or umlaut, especially foradjectives and verbs.
Therefore, we can see fromFigure 4 that the AE also achieves some goodresults without using context information.From a typological perspective, we found thatall the embeddings work well on decoding wordgender of Romance languages (Italian, Spanishand Portuguese) but worst on Slavic languages(e.g.
Czech, Slovenian).
This is probablyhi facspl bg hr sl dasv no enel he itesptrofiethu0.91RomanceSlavicAccuracyCW SG AEFigure 5: Model comparison on decoding NUM-BER.Language |V | C&W SG AE # CaseAnaly.Danish 372 0.947 0.946 1.000 3Swedish 5893 0.995 0.990 0.981 2Bulgarian 104 0.636 0.546 0.818 4Agglut.Finnish 21094 0.868 0.871 0.908 15Hungarian 4536 0.852 0.901 22Tamil 1144 0.896 ?
0.835 7Basque 8020 0.761 ?
0.857 15FusionalHindi 10682 0.712 0.704 0.646 7Czech 38666 0.788 0.776 0.663 7Polish 13715 0.828 0.785 0.636 7Slovenian 15150 0.796 0.768 0.617 6Croatian 9945 0.807 0.789 0.628 7Greek 5790 0.841 0.851 0.774 5Latin 4773 0.674 ?
0.636 7Table 3: Model comparison on decoding CASE.because that Romance languages employ regularrules to judge the gender of a word.
However,Slavic languages have other nonlinear fusionalmorphological features that are not easy to tackle.Number refers to the linguistic abstraction ofobjects?
quantities.
It is an inflectional feature ofnoun and other parts of speech (adjective, verb)that have agreement with noun.
The basic valuecan be singular, dual or plural.
We can see fromFigure 5 that SG, CW and AE all perform well.AE performs almost as well as CW and SG onEnglish, Spanish and Portuguese.Case is one of the most significant features.Gender and number are indexical morphemes,which means that there is a phrase in the sentencethat necessarily agrees with the target item.
Case,on the contrary, is a relational morpheme, accord-ing to (Croft, 2002).
Case reflects the semanticrole of a noun, relative to the pivot verb.
Allthe languages studied in this paper, more or less,employ word inflection to explicitly express thespecific case role.
The model performances arelisted in Table 3.We notice some important inter-language dif-ferences.
Swedish has only two cases, nominaland genitive.
The form of genitive case is verysimple.
Adding an s to the coda of a nounwill change it to genitive case.
Thus, we cansee that character-based encoding performs well1481on Swedish.
Since genitive case usually meanspossession, we also notice that context-baseddistributed representation also performs well indecoding case information from Swedish words.By classifying these languages into differentmorphological types in Table 3, we find that wordvectors of highly inflected fusional languages (e.g.Czech) performs worse than agglutinative lan-guages (e.g.
Finnish).
This is typically reflected inAE, as agglutinative languages simply concatenatethe case marker with the nominative form of anoun.
The morphological transformation of agglu-tinative languages is linear and simple.
Besides,the case system of the analytic languages hasbeen largely simplified due to historical change.Therefore, all the embeddings perform well onanalytic languages.
This evidence supports thatmorphological complexity is positively correlatedwith the quality of word embedding.Besides, for fusional languages, using dis-tributed representation and context informationwould largely increase the performance.
This, inturn, indicates that cases are a special semanticrelations distributed in the words around the targetnoun.
Although a case is not explicitly agreedwith other components in an utterance, the wordcategory might serve as a good indicator, such aspreposition and verb.Animacy is a special nominal feature in a fewlanguages, which is used to discriminate alive andhicsbg sl el itesptfi0.90.951RomanceSlavicAccuracy(a) PERSONbg dasv no eu rohu0.90.951Accuracy(b) DEFINITEcspl0.80.91(c) ANIM.Figure 6: Model comparison on decoding PER-SON, DEFINITENESS and ANIMACY.animate objects from inanimate nouns.
Generally,it is based on the lexical semantic feature.
Asis shown in Figure 6, it is easier to decodeanimacy from the context-based representationsthan character-based representation.hi hecspl bg hr sl dasv no enel fiethu itesptrofa0.80.91RomanceSlavicAccuracyCW SG AE(a) TENSEhicspl bg dano enel fi0.850.90.951 SlavicAccuracyCW SG AE(b) VOICEcspl bg sl dasv no enitesptfi0.90.951SlavicRomanceAccuracyCW SG AE(c) MOODpl bgcssleuhi0.80.91SlavicAccuracy(d) ASPECThi bg hr sl itesptfi0.60.81SlavicRomance(e) PRONTYPEcspl bg hr dano enitesptfiethu0.80.91SlavicAccuracyCWSGAE(f) VERBFORMFigure 7: Model comparison on TENSE, VOICE,MOOD, ASPECT, PRONTYPE and VERBFORM1482en ardeesfr hi idptruzh0.20.40.60.8CorrelationCWSGAEFigure 8: Model comparison on EMOTION.From Figure 6, 7, we can see that all thethree models give quite perfect performance ondecoding person, definiteness, tense, voice, mood,aspect, pronoun type and verb form.Overall, character-based representation is mosteffective for Slavic languages on decoding verbalmorphological features but not nominal features.The result is vice versa for Romance languages,which is not as morphologically complex asSlavic.
It is worth noticing that models behavedifferently on Bulgarian, an analytic language,although Bulgarian belongs to Slavic languagefrom the phylogenetic perspective.
We think thatthis is because many morphological features inBulgarian have been simplified or weakened.4.4 Emotion ScoreIn Experiment IV, we use the manually annotateddata collected by Dodds et al (2015).
Thedata contains emotion scores for a list of wordsin several languages.
In our experiment, theoriginal score scale is transformed into the interval[0, 1].
A nonlinear map is trained to regress therepresentation of a word (CW, SG, AE) to itsemotion score.To evaluate the predicted results, we measurethe Spearman correlation between the gold scoresand predicted scores.
The result in Figure 8reveals a significantly strong correlation betweenthe predicted emotion scores of SG and the realemotion scores.
CW comes the second.
For AE, itis hard to decode emotion just from the word form.5 Contrastive AnalysisAs we have mentioned before, Type I C&Wmodel utilizes ordered context information totrain the distributed word representation.
TypeII skip-gram model utilizes unordered contextinformation.
Type III character-based LSTMautoencoder model utilizes the grapheme infor-mation to represent a word.
Towards the keyquestions that we raised at the very beginningof the paper, we propose our contrastive analysisItalianSpanishPortugueseGreekBulgarianDanishSwedishNorwegianHindiPolishSlovenian??
1(a) Hierarchical tree based onmodel performancesItalianSpanishPortugueseSwedishDanishGreekBulgarianSlovenianPolishHindiNorwegian(b) WALS Genus TreeFigure 9: Comparison of the tree based onmodel performances and the WALS dendrogrammanually constructed by linguists.based on the experiment results.5.1 Typology vs. PhylogenyExperiment results have shown that word embed-ding models are influenced by the syntactic andmorphological diversity more or less.
Here wedisplay how typological similarity and phyloge-netic relation is revealed from the observed modelperformance variation.
We hierarchically clusterlanguages according to the model performanceon decoding syntactic and morphological features.The dendrogram of the languages in Figure 9vividly shows that most of the phylogenetic-related languages are clustered together.However, there is some interesting exceptions.Bulgarian does not form a primary cluster withother Slavic languages (e.g.
Slovenian).
We thinkthat this is because Bulgarian is typologicallydissimilar to Slavic language family.
Therefore,Figure 9 reflects that language typology explainsthe model variation better than language phyloge-ny.5.2 Form vs. ContextHere we discuss the effectiveness of word formand different types of word context.Regarding the correlation between context typeand language function, previous results show thatSG performs worse than CW on decoding POSand dependency relation while SG performs betterthan CW on decoding emotion score.
Since CWkeeps word order of the context, this comparisonsuggests that word order information is vital tosyntactic information, but it might also be a kindof noise for the word vectors to encode semanticinformation.1483CW SG AE0.80.850.90.95Accuracygennumcasanidef(a) Nominal featuresCW SG AE0.80.9Accuracytenpermoovoiaspprovfo(b) Verbal featuresFigure 10: Overall performances of differentmodels (averaged over languages) on decodingmorphological features.Regarding the correlation between form andlanguage function, previous results on POS, de-pendency relation and emotion scores show theeffectiveness of the word context.
However, formorphological features, results in Table 10 indi-cate that context-based word representation worksslightly better than character-based representa-tion.
Specifically, character-based embedding(AE) does outperform context-based embedding(CW, SG) on decoding verbal morphological fea-tures, even though AE does not access any contextinformation.
In other words, word form could bean explicit and discriminative cue for the model todecode the morphological feature of a word.To prove that word form could provides infor-mative and explicit cues for grammatical function-s, we train another shuffled character-based wordrepresentation, which means that the autoencoderinputs shuffled letters and outputs the shuffledletters again.
We use the hidden layer of theshuffled autoencoder as the representation foreach word.
The result in Table 4 shows thatnow the character-based model cannot perform aswell as the original character-based autoencoderrepresentation does, which again proves that theorder of the word form is necessary for learningthe grammatical function of a word.Since many languages share similar phono-graphic writing systems, we naturally want toknow whether the grapheme-phoneme knowledgefrom one language can be transferred to anotherlanguage.
We train an autoencoder purely onLan.
Raw Shuf.
Lan.
Raw Shuf.Russian 0.906 0.671 Slovenian 0.800 0.653Table 4: Comparison of original and shuffledcharacter-based word representation on decodingPOS tag.Source Language Arabic FinnishTarget Language fa ud en shuf en randBigram type overlap.
0.176 0.761 0.891 0.864 0.648Bigram token overlap.
0.689 0.881 0.999 0.993 0.650Trigram type overlap.
0.523 0.522 0.665 0.449 0.078Trigram token overlap.
0.526 0.585 0.978 0.796 0.078Reconstruction Acc.
0.586 0.689 0.95 0.83 0.22Table 5: Comparison of morpho-phonologicalknowledge transfer on different language pairs.The reconstruction accuracy is correlated withthe overlapping proportion of grapheme patternsbetween source language and target language.Finnish and directly test the trained model onmemorizing raw English words, letter-shuffledEnglish words and random letter sequences.
Re-sults in Table 5 indicate that the character autoen-coder can successfully reconstruct raw Englishwords instead of the letter-shuffled English wordsor random letter sequences.
However, if we trainan autoencoder purely on Arabic and then directlytest the trained model on memorizing Urdu (ud)words or Persian (fa) words, the reconstructionaccuracy is quite low, although Arabic, Persianand Urdu use the same Arabic writing system.To explain the behaviour of AE, we calculatethe correlation between the bigram character fre-quency in the words of the training language (e.g.Finnish) and the bigram character frequency inthe words of the testing language (e.g.
English).Table 5 reveals that phonological knowledge canbe transferred if two languages share similarbigram and trigram character frequency distribu-tion.
For example, Finnish and English are bothIndo-European language.
Their writing systemstores similar phonological structure.
Arabic isa Semitic language.
Persian is an Indo-Europeanlanguage.
Their writing system stores differentphonological structures respectively.
This againproves that character-based LSTM autoencoderdoes ?memorize?
the grapheme or phoneme clus-ters of a words.
Morpho-phonological knowledgecan be transferred among typologically-relatedlanguages.Additionally, we are surprised to find thatusing the English word representations encoded1484One ?Brain?, Many Languages:Typological universals revealed in word embeddingsPeng Qian?School of Computer ScienceFudan UniversityShanghai, PRC 2004330 20 40 600.60.864 Neurons of the ModelSelectivityCWSGAE(a) Me-kan0 20 40 600.20.40.60.8164 Neurons of the ModelSelectivityAESGCW(b) -nya0 20 40 600.40.60.8164 Neurons of the ModelSelectivityAESGCW(c) -indonesia0 20 40 600.40.60.8164 Neurons of the ModelSelectivityCWSGAE(d) country name0 20 40 600.40.60.864 Neurons of the ModelSelectivityAESGCW(e) verb?1 ?0.5 0 0.500.511.52Threshold tActivation(f) Neuron activation pattern?Student ID: 152102400861(a) m - kanOne ?Brain?, Many Languages:Typological universals revealed in word embeddingsPeng Qian?School of Computer ScienceFudan UniversityShanghai, PRC 2004330 20 40 600.60.864 Neurons of the ModelSelectivityCWSGAE(a) Me-kan0 20 40 600.20.40.60.8164 Neurons of the ModelSelectivityAESGCW(b) -nya0 20 40 600.40.60.8164 Neurons of the ModelSelectivityAESGCW(c) -indonesia0.460.8SelectivityCWAE(d) countr name0.40.60.8Selectivity(e) verb?1 ?0.5 0 0.500.511.52Threshold tActivation(f) Neuron activation pattern?Student ID: 152102400861(b) C tr namesOne ?Brain?, Many Languages:Typological universals revealed in word embeddingsPeng Qian?School of Computer ScienceFudan UniversityShanghai, PRC 2004330 20 40 600.60.864 Neurons of the ModelSelectivityCWSGAE(a) Me-kan0 20 40 600.20.40.60.8164 eurons of the o elSelectivityAESGCW(b) -ny0.40.60.8r f t lSelectivityES(c) -indonesia0 20 40 600.40.60.8164 Neurons of the ModelSelectivityCWSGAE(d) country ame0 20 40 600.40.60.864 Neurons of the odelSelectivityAESGCW(e) verb?1 ?0.5 0 0.500.511.52Threshold tActivati n(f) Neuron activation pattern?Student ID: 152102400861(c) Single uronFigure 11: Visualising the Neuron activation pattern for different word embedding modelsby AE model trained on Finnish can increase theaccuracy of English AE embedding in ExperimentI (up to .7076), compared with the originalaccuracy 0.6587.
This is probably due to theshared knowledge about the morphemes in theword form.5.3 Neuronal Activation PatternLe (2011) found out that it is empirically possibleto learn a ?Grandmother neuron?-like face detectorfrom unlabelled data.
In their experiment onunlabeled data, one neuron learns to activatespecifically towards the pictures with cat facesinstead of other pictures.
Based on this finding,we hypothesize that there should exist selectiveneuron activation towards a linguistic featuretrigger.
The feature trigger can be a specialconsonant cluster, a specific suffix or the syntacticcategory of a word.To quantitatively show the collective neuronbehaviours and the individual neuron response to-wards different linguistic trigger, we compute themaximum probability that a neuron discriminatesthe words with trigger f from the words withouttrigger f .
We defined this probability as theDegree of Selectivity p. For a given neuron n ina given model M towards linguistic trigger f , wetry to find a threshold t that maximizes pf,t,cf,t=N+f,tNf, c?f,t=N+?f,tN?f,Selectivity = pf,t=2?
cf,t?
c?f,tcf,t+ c?f,twhere N+f,tis the number of correctly discrim-inated words with linguistic feature f based onthe threshold t. Nfis the real number of wordswith linguistic feature f .
N+?f,tis the number ofcorrectly discriminated words without linguisticfeature f based on the threshold t. N?fmeansthe real number of words without linguistic featuref .
cf,t/ c?f,tis the accuracy for the neuron n ofmodel M to detect the existence / nonexistence ofthe linguistic feature f .
pf,tis the F-score of cf,tand c?f,t, indicating the degree to which a certainneuron discriminates the words with/without acertain trigger f at a certain threshold t.After calculating the selectivity of 64 neurons inan embedding model towards a linguistic triggerf , we sort the neurons according to the value ofselectivity and draw the curve in Figure 11 foreach model.
The x-axis is the rank of the modelneurons based on their selectivity towards a certainlinguistic trigger.
The y-axis is the selectivity ofthe corresponding neuron.
The curve can tell ushow many neurons selectively respond to triggerf to a certain degree.
For example, we can seefrom Figure 11 that the max selectivity of the AEneurons reaches nearly 0.9.
This means that oneneuron of the AE model is especially sensitive tothe prefix ?Me-?
and affix ?-an?.
It can detect thewords with the prefix ?Me-?
and the affix ?-an?
justfrom its activation pattern.It is also interesting to see from Figure 11that neurons of AE respond more selectively tomorphological triggers than those of the word-based model.
For example, almost 30% of theAE neurons fall in the selectivity level [0.7, 1]towards the verb marker, namely prefix ?Me-?and affix ?-an?, in Indonesian.
Context-basedmodel also shows some selectivity towards thismorphological triggers.
For SG model, the maxselectivity of the model neurons is only just above0.7.On the contrary, the context-based distributedmodels showed strong selective activation towardscountry names in Indonesian.
However, theselectivity of all the AE neurons is below 0.7towards these semantically-related words.Similar patterns are found also in other lan-guages.
We conclude that the character-basedmodel captures much morphological information1485/ syntactic marker than semantic information.
Thepopular word-based model captures both semanticinformation and syntactic information, althoughthe latter is not displayed as explicitly as theformer.6 Related worksThere have been a lot of research on interpreting orrelating word embedding with linguistic features.Yogatama et al (2014) projects word embeddinginto a sparse vector.
They found some linguisti-cally interpretable dimensions.
Faruqui and Dyer(2015) use linguistic features to build word vector.Their results show that these representation ofword meaning can also achieve good performancein the analogy and similarity tasks.
These workcan be regarded as the foreshadowing of ourexperiment paradigm that mapping dense vectorto a sparse linguistic property space.Besides, a lot of study focus on empiricalcomparison of different word embedding model.Melamud et al (2016) investigates the influenceof context type and vector dimension on wordembedding.
Their main finding is that concate-nating two different types of embeddings canstill improve performance even if the utility ofdimensionality has run out.
Andreas and Klein(2014) assess the potential syntactic informationencoded in word embeddings by directly applyword embeddings to parser and they concludedthat embeddings add redundant information towhat the conventional parser has already extract-ed.
Tsvetkov et al (2015) propose a method toevaluate word embeddings through the alignmentof distributional vectors and linguistic word vec-tors.
However, the method still lacks a direct andcomprehensive investigation of the utility of form,context and language typological diversity.
This isexactly our novelty and contribution.It is worth noticing that K?ohn (2015) evalu-ates multilingual word embedding and comparesskip-gram, language model and other competitiveembedding models.
They show that dependency-based skip-gram embedding is effective, even atlow dimension.
Although K?ohn (2015) workinvolves different languages, they focus on thesimilarity among multilingual embeddings withonly 7 languages.
Our work, however, notonly provides a comprehensive investigation withmassive language samples (30 for Experiment I)and nonlinear mapping models, but also reveal theutility of pure word form and novelly point out thecross-language differences in word representation,which have been overlooked by huge amountof monolingual/bilingual research on well-studiedlanguages.7 ConclusionIn this paper, we quantify the utility of wordform and the effect of language typological di-versity in learning word representations.
Cross-language perspective and novel analysis of neuronbehaviours provide us with new evidence aboutthe typological universal and specific revealedin word embedding.
We summarize from ourexperiments on a massive set of languages that:?
Language typological diversity, especiallythe specific word order type and morphologi-cal complexity, does influence how linguisticinformation is encoded in word embedding.?
It is plausible (and sometimes even better)to decode grammatical function just from theword form, for certain inflectional languages.?
Quantification of neuron activation patternreveals different characteristics of thecontext-based model and the character-basedcounterpart.Therefore, we think that it is necessary tomaximize both the utility of word form andthe advantage of the context for a better wordrepresentation.
It would also be a promisingdirection to incorporate the factor of languagetypological diversity when designing advancedword representation model for languages otherthan English.AcknowledgmentsWe would like to thank the anonymous review-ers for their valuable comments.
This workwas partially funded by National Natural ScienceFoundation of China (No.
61532011, 61473092,and 61472088), the National High TechnologyResearch and Development Program of China(No.
2015AA015408).ReferencesRami Al-Rfou, Bryan Perozzi, and Steven Skiena.2013.
Polyglot: Distributed word representationsfor multilingual nlp.
In Proceedings of the1486Seventeenth Conference on Computational Natu-ral Language Learning, pages 183?192, Sofia,Bulgaria, August.
Association for ComputationalLinguistics.Jacob Andreas and Dan Klein.
2014.
How muchdo word embeddings encode about syntax?
InProceedings of ACL, pages 822?827.Miguel Ballesteros, Chris Dyer, and Noah A Smith.2015.
Improved transition-based parsing bymodeling characters instead of words with LSTMs.In Proceedings of EMNLP.Emily M Bender.
2009.
Linguistically na?
?ve!=language independent: why nlp needs linguistictypology.
In Proceedings of the EACL 2009Workshop on the Interaction between Linguisticsand Computational Linguistics: Virtuous, Vicious orVacuous?, pages 26?32.
Association for Computa-tional Linguistics.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost)from scratch.
The Journal of Machine LearningResearch, 12:2493?2537.William Croft.
2002.
Typology and universals.Cambridge University Press.Marie-Catherine De Marneffe, Timothy Dozat, NataliaSilveira, Katri Haverinen, Filip Ginter, JoakimNivre, and Christopher D Manning.
2014.Universal stanford dependencies: A cross-linguistictypology.
In LREC, volume 14, pages 4585?4592.Peter Sheridan Dodds, Eric M Clark, Suma Desu,Morgan R Frank, Andrew J Reagan, Jake RylandWilliams, Lewis Mitchell, Kameron Decker Harris,Isabel M Kloumann, James P Bagrow, et al 2015.Human language reveals a universal positivity bias.Proceedings of the National Academy of Sciences,112(8):2389?2394.Matthew S. Dryer and Martin Haspelmath, editors.2013.
WALS Online.
Max Planck Institutefor Evolutionary Anthropology.
http://wals.info/.Manaal Faruqui and Chris Dyer.
2015.
Non-distributional word vector representations.
InProceedings of ACL.Manaal Faruqui, Yulia Tsvetkov, Graham Neubig,and Chris Dyer.
2016.
Morphological inflectiongeneration using character sequence to sequencelearning.
In Proceedings of NAACL.Zellig S. Harris.
1954.
Distributional structure.Synthese Language Library, 10:146?162.Maria Jesus Aranzabe Masayuki Asahara AitziberAtutxa Miguel Ballesteros John Bauer KepaBengoetxea Riyaz Ahmad Bhat Cristina BoscoSam Bowman Giuseppe G. A. Celano MiriamConnor Marie-Catherine de Marneffe Arantza Di-az de Ilarraza Kaja Dobrovoljc Timothy DozatToma?z Erjavec Rich?ard Farkas Jennifer FosterDaniel Galbraith Filip Ginter Iakes Goenaga KoldoGojenola Yoav Goldberg Berta Gonzales BrunoGuillaume Jan Haji?c Dag Haug Radu Ion ElenaIrimia Anders Johannsen Hiroshi Kanayama JennaKanerva Simon Krek Veronika Laippala AlessandroLenci Nikola Ljube?si?c Teresa Lynn ChristopherManning Ctlina Mrnduc David Mare?cek H?ectorMart?
?nez Alonso Jan Ma?sek Yuji Matsumoto RyanMcDonald Anna Missil?a Verginica Mititelu YusukeMiyao Simonetta Montemagni Shunsuke MoriHanna Nurmi Petya Osenova Lilja ?vrelid ElenaPascual Marco Passarotti Cenel-Augusto Perez SlavPetrov Jussi Piitulainen Barbara Plank Martin PopelProkopis Prokopidis Sampo Pyysalo LoganathanRamasamy Rudolf Rosa Shadi Saleh SebastianSchuster Wolfgang Seeker Mojgan Seraji NataliaSilveira Maria Simi Radu Simionescu KatalinSimk?o Kiril Simov Aaron Smith Jan?St?ep?anekAlane Suhr Zsolt Sz?ant?o Takaaki Tanaka ReutTsarfaty Sumire Uematsu Larraitz Uria Viktor VargaVeronika Vincze Zden?ek?Zabokrtsk?y Daniel ZemanJoakim Nivre,?Zeljko Agi?c and Hanzhi Zhu.
2015.Universal dependencies 1.2.
In LINDAT/CLARINdigital library at Institute of Formal and AppliedLinguistics, Charles University in Prague.Yoon Kim, Yacine Jernite, David Sontag, andAlexander M Rush.
2016.
Character-aware neurallanguage models.
In Proceedings of AAAI.Arne K?ohn.
2015.
Whats in an embedding?
analyzingword embeddings through multilingual evaluation.In Poceedings of EMNLP.Siwei Lai, Kang Liu, Liheng Xu, and Jun Zhao.
2015.How to generate a good word embedding?
arXivpreprint arXiv:1507.05523.Q.
V. Le.
2011.
Building high-level features usinglarge scale unsupervised learning.
In Acoustics,Speech and Signal Processing (ICASSP), 2013 IEEEInternational Conference on, pages 8595 ?
8598.Wang Ling, Tiago Lu?
?s, Lu?
?s Marujo, Ram?on Fernan-dez Astudillo, Silvio Amir, Chris Dyer, Alan WBlack, and Isabel Trancoso.
2015a.
Findingfunction in form: Compositional character modelsfor open vocabulary word representation.
InProceedings of EMNLP.Wang Ling, Isabel Trancoso, Chris Dyer, and Alan WBlack.
2015b.
Character-based neural machinetranslation.
arXiv preprint arXiv:1511.04586.Oren Melamud, David McClosky, Siddharth Patward-han, and Mohit Bansal.
2016.
The role ofcontext types and dimensionality in learning wordembeddings.
In Proceedings of NAACL.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of wordrepresentations in vector space.
In Proceedings ofICLR.1487Cicero D Santos and Bianca Zadrozny.
2014.
Learningcharacter-level representations for part-of-speechtagging.
In Proceedings of the 31st InternationalConference on Machine Learning (ICML-14), pages1818?1826.Yulia Tsvetkov, Manaal Faruqui, Wang Ling, Guil-laume Lample, and Chris Dyer.
2015.
Evaluation ofword vector representations by subspace alignment.In Proceedings of EMNLP.Nianwen Xue, Zixin Jiang, Xiuhong Zhong, MarthaPalmer, Fei Xia, Fu-Dong Chiou, and Meiyu Chang.2010.
Chinese treebank 7.0.
Linguistic DataConsortium, Philadelphia.Dani Yogatama, Manaal Faruqui, Chris Dyer, andNoah A Smith.
2014.
Learning word repre-sentations with hierarchical sparse coding.
InProceedings of ICML.1488
