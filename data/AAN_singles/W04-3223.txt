Incremental Feature Selection and `1 Regularizationfor Relaxed Maximum-Entropy ModelingStefan Riezler and Alexander VassermanPalo Alto Research Center3333 Coyote Hill Road, Palo Alto, CA 94304AbstractWe present an approach to bounded constraint-relaxation for entropy maximization that corre-sponds to using a double-exponential prior or `1 reg-ularizer in likelihood maximization for log-linearmodels.
We show that a combined incremental fea-ture selection and regularization method can be es-tablished for maximum entropy modeling by a nat-ural incorporation of the regularizer into gradient-based feature selection, following Perkins et al(2003).
This provides an efficient alternative to stan-dard `1 regularization on the full feature set, anda mathematical justification for thresholding tech-niques used in likelihood-based feature selection.Also, we motivate an extension to n-best featureselection for linguistic features sets with moderateredundancy, and present experimental results show-ing its advantage over `0, 1-best `1, `2 regularizationand over standard incremental feature selection forthe task of maximum-entropy parsing.11 IntroductionThe maximum-entropy (ME) principle, which pre-scribes choosing the model that maximizes the en-tropy out of all models that satisfy given featureconstraints, can be seen as a built-in regularizationmechanism that avoids overfitting the training data.However, it is only a weak regularizer that cannotavoid overfitting in situations where the number oftraining examples is significantly smaller than thenumber of features.
In such situations some fea-tures will occur zero times on the training set andreceive negative infinity weights, causing the as-signment of zero probabilities for inputs includingthose features.
Similar assignment of (negative) in-finity weights happens to features that are pseudo-minimal (or pseudo-maximal) on the training set(see Johnson et al (1999)), that is, features whosevalue on correct parses always is less (or greater)1This research has been funded in part by contractMDA904-03-C-0404 of the Advanced Research and Develop-ment Activity, Novel Intelligence from Massive Data program.than or equal to their value on all other parses.
Also,if large features sets are generated automaticallyfrom conjunctions of simple feature tests, many fea-tures will be redundant.
Besides overfitting, largefeature sets also create the problem of increasedtime and space complexity.Common techniques to deal with these problemsare regularization and feature selection.
For MEmodels, the use of an `2 regularizer, correspondingto imposing a Gaussian prior on the parameter val-ues, has been proposed by Johnson et al (1999) andChen and Rosenfeld (1999).
Feature selection forME models has commonly used simple frequency-based cut-off, or likelihood-based feature inductionas introduced by Della Pietra et al (1997).
Whereas`2 regularization produces excellent generalizationperformance and effectively avoids numerical prob-lems, parameter values almost never decrease tozero, leaving the problem of inefficient computa-tion with the full feature set.
In contrast, feature se-lection methods effectively decrease computationalcomplexity by selecting a fraction of the featureset for computation; however, generalization per-formance suffers from the ad-hoc character of hardthresholds on feature counts or likelihood gains.Tibshirani (1996) proposed a technique based on`1 regularization that embeds feature selection intoregularization such that both a precise assessment ofthe reliability of features and the decision about in-clusion or deletion of features can be done in thesame framework.
Feature sparsity is produced bythe polyhedral structure of the `1 norm which ex-hibits a gradient discontinuity at zero that tends toforce a subset of parameter values to be exactlyzero at the optimum.
Since this discontinuity makesoptimization a hard numerical problem, standardgradient-based techniques for estimation cannot beapplied directly.
Tibshirani (1996) presents a spe-cialized optimization algorithm for `1 regularizationfor linear least-squares regression called the Lassoalgorithm.
Goodman (2003) and Kazama and Tsujii(2003) employ standard iterative scaling and con-jugate gradient techniques, however, for regulariza-tion a simplified one-sided exponential prior is em-ployed which is non-zero only for non-negative pa-rameter values.
In these approaches the full fea-ture space is considered in estimation, so savingsin computational complexity are gained only in ap-plications of the resulting sparse models.
Perkinset al (2003) presented an approach that combines`1 based regularization with incremental feature se-lection.
Their basic idea is to start with a model inwhich almost all weights are zero, and iterativelydecide, by comparing regularized feature gradients,which weight should be adjusted away from zeroin order to decrease the regularized objective func-tion by the maximum amount.
The `1 regularizer isthus used directly for incremental feature selection,which on the one hand makes feature selection fast,and on the other hand avoids numerical problemsfor zero-valued weights since only non-zero weightsare included in the model.
Besides the experimentalevidence presented in these papers, recently a theo-retical account on the superior sample complexity of`1 over `2 regularization has been presented by Ng(2004), showing logarithmic versus linear growth inthe number of irrelevant features for `1 versus `2regularized logistic regression.In this paper, we apply `1 regularization to log-linear models, and motivate our approach in termsof maximum entropy estimation subject to relaxedconstraints.
We apply the gradient-based feature se-lection technique of Perkins et al (2003) to ourframework, and improve its computational com-plexity by an n-best feature inclusion technique.This extension is tailored to linguistically motivatedfeature sets where the number of irrelevant featuresis moderate.
In experiments on real-world data frommaximum-entropy parsing, we show the advantageof n-best `1 regularization over `2, `1, `0 regulariza-tion and standard incremental feature selection interms of better computational complexity and im-proved generalization performance.2 `p Regularizers for Log-Linear ModelsLet p?
(x|y) = e?ni=1 ?ifi(x,y)?x e?ni=1 ?ifi(x,y)be a conditionallog-linear model defined by feature functions f andlog-parameters ?.
For data {(xj , yj)}mj=1, the objec-tive function to be minimized in `p regularization ofthe negative log-likelihood L(?)
isC(?)
= L(?)
+ ?p(?
)= ?1mm?j=1ln p?
(xj |yj) + ???
?ppThe regularizer family ?p(?)
is defined by theMinkowski `p norm of the parameter vector ?raised to the pth power, i.e.
??
?pp =?ni=1 |?i|p.The essence of this regularizer family is to penalizeoverly large parameter values.
If p = 2, the regu-larizer corresponds to a zero-mean Gaussian priordistribution on the parameters with ?
correspondingto the inverse variance of the Gaussian.
If p = 0,the regularizer is equivalent to setting a limit on themaximum number of non-zero weights.
In our ex-periments we replace `0 regularization by the re-lated technique of frequency-based feature cutoff.`1 regularization is defined by the case wherep = 1.
Here parameters are penalized in the sumof their absolute values, which corresponds to ap-plying a zero-mean Laplacian or double exponentialprior distribution of the formp(?i) =12?e?|?i|?with ?
= 1?
being proportional to the inverse stan-dard deviation?2?
.
In contrast to the Gaussian, theLaplacian prior puts more mass near zero (and inthe tails), thus tightening the prior by decreasingthe standard deviation ?
provides stronger regular-ization against overfitting and produces more zero-valued parameter estimates.
In terms of `1-normregularization, feature sparsity can be explained bythe following observation: Since every non-zero pa-rameter weight incurs a regularizer penalty of ?|?i|,its contribution to minimizing the negative log-likelihood has to outweigh this penalty.
Thus param-eters values where the gradient at ?
= 0 is?????L(?)??i????
?
?
(1)can be kept zero without changing the optimality ofthe solution.3 Bounded Constraint Relaxation forMaximum Entropy EstimationAs shown by Lebanon and Lafferty (2001), in termsof convex duality, a regularization term for the dualproblem corresponds to a ?potential?
on the con-straint values in the primal problem.
For a dualproblem of regularized likelihood estimation forlog-linear models, the corresponding primal prob-lem is a maximum entropy problem subject to re-laxed constraints.
Let H(p) denote the entropy withrespect to probability function p, and g : IRn ?
IRbe a convex potential function, and p?[?]
and p[?]
beexpectations with respect to the empirical distribu-tion p?
(x, y) = 1m?mj=1 ?
(xj , x)?
(yj , y) and themodel distribution p(x|y)p?(y).
The primal problemcan then be stated asMaximize H(p)?
g(c) subject top[fi]?
p?
[fi] = ci, i = 1, .
.
.
, nConstraint relaxation is achieved in that equality ofthe feature expectations is not enforced, but a certainamount of overshooting or undershooting is allowedby a parameter vector c ?
IRn whose potential is de-termined by a convex function g(c) that is combinedwith the entropy term H(p).In the case of `2 regularization, the potential func-tion for the primal problem is a quadratic penaltyof the form 12?
?i c2i for ?
= 1?2i , i = 1, .
.
.
, n(Lebanon and Lafferty, 2001).
In order to recoverthe specific form of the primal problem for our case,we have to start from the given dual problem.
Fol-lowing Lebanon and Lafferty (2001), the dual func-tion for regularized estimation can be expressed interms of the dual function ?(p?,?)
for the unregu-larized case and the convex conjugate g?(?)
of thepotential function g(c).
In our case the negative of?(p?,?)
corresponds to the likelihood term L(?
),and the negative of the convex conjugate g?(?)
isthe `1 regularizer.
Thus our dual problem can bestated as??
= argmax??(p?,?)?
g?(?
)= argmin?L(?)
+ ???
?11Since for convex and closed functions, the con-jugate of the conjugate is the original function, i.e.g??
= g (Boyd and Vandenberghe, 2004), the poten-tial function g(c) for the primal problem can be re-covered by calculating the conjugate g??
of the con-jugate g?(?)
= ????11.
In our case, we getg??
(c) = g(c) ={0 ?c??
?
??
otherwise (2)where ?c??
= max{|c1|, .
.
.
, |cn|}.
A proof forthis proposition is given in the Appendix.
The re-sulting potential function g(c) is the indicator func-tion on the interval [?
?, ?].
That is, it restricts theallowable amount of constraint relaxation to at most??.
From this perspective, increasing ?
means to al-low for more slack in constraint satisfaction, whichin turn allows to fit a more uniform, less overfit-ting distribution to the data.
For features that are in-cluded in the model, the parameter values have to beadjusted away from zero to meet the constraints|p[fi]?
p?
[fi]| ?
?, i = 1, .
.
.
, n (3)Initialization: Initialize selected features S to ?, andzero-weighted features Z to the full feature set,yielding the uniform distribution p?
(0),S(0) .n-best grafting: For steps t = 1, .
.
.
, T ,(1) for all features fi in Z(t?1), calculate??????L(?
(t?1), S(t?1))??i????
?> ?,(2) S(t) := S(t?1) ?N (t) and Z(t) := Z(t?1) \N (t) where N (t) is the set of n-best featurespassing the test in (1),(3) perform conjugate gradient optimization tofind the optimal model p?
?,S(t) where ?
isinitialized at ?
(t?1), and ?
(t) := ??
=argmax?C(?, S(t)).Stopping condition: Stop if for all fi in Z(t?1):??????L(?
(t?1), S(t?1))??i??????
?Figure 1: n-best gradient feature testingFor features that meet the constraints without pa-rameter adjustment, parameter values can be kept atzero, effectively discarding the features.
Note thatequality of equations 3 and 1 connects the maxi-mum entropy problem to likelihood regularization.4 StandardizationNote that the ?p regularizer presented above penal-izes the model parameters uniformly, correspond-ing to imposing a uniform variance onto all modelparameters.
This motivates a normalization of in-put data to the same scale.
A standard techniqueto achieve this is to linearly rescale each featurecount to zero mean and standard deviation of oneover all training data.
The same rescaling has to bedone for training and application of the model to un-seen data.
As we will see in the experimental evalua-tion presented below, a standardization of input datacan also dramatically improve convergence behav-ior in unregularized optimization .
Furthermore, pa-rameter values estimated from standardized featurecounts are directly interpretable to humans.
Com-bined with feature selection, interpretable parame-ter weights are particularly useful for error analysisof the model?s feature design.5 Incremental n-best Feature SelectionThe basic idea of the ?grafting?
(for ?gradient fea-ture testing?)
algorithm presented by (Perkins et al,2003) is to assume a tendency of `1 regularizationto produce a large number of zero-valued param-eters at the function?s optimum, thus to start withall-zero weights, and incrementally add features tothe model only if adjusting their parameter weightsaway from zero sufficiently decreases the optimiza-tion criterion.
This idea allows for efficient, incre-mental feature selection, and at the same time avoidsnumerical problems caused by the discontinuity ofthe gradient in `1 regularization.
Furthermore, theregularizer is incorporated directly into a criterionfor feature selection, based on the observation madeabove: It only makes sense to add a feature to themodel if the regularizer penalty is outweighed bythe reduction in negative log-likelihood.
Thus fea-tures considered for selection have to pass the fol-lowing test:?????L(?)??i????
> ?In the grafting procedure suggested by (Perkinset al, 2003), this gradient test is applied to each fea-ture, and at each step the feature passing the testwith maximum magnitude is added to the model.Adding one feature at a time effectively discardsnoisy and irrelevant features, however, the overheadintroduced by grafting can outweigh the gain in ef-ficiency if there is a moderate number of noisy andtruly redundant features.
In such cases, it is bene-ficial to add a number of n > 1 features at eachstep, where n is adjusted by cross-validation or on aheld-out data set.
In the experiments on maximum-entropy parsing presented below, a feature set of lin-guistically motivated features is used that exhibitsonly a moderate amount of redundancy.
We will seethat for such cases, n-best feature selection consid-erably improves computational complexity, and alsoachieves slightly better generalization performance.After adding n ?
1 features to the model ina grafting step, the model is optimized with re-spect to all parameters corresponding to currentlyincluded features.
This optimization is done by call-ing a gradient-based general purpose optimizationroutine for the regularized objective function.
Weuse a conjugate gradient routine for this purpose(Minka, 2001; Malouf, 2002)2.
The gradient of ourcriterion with respect to a parameter ?i is:?C(?)??i=1mm?k=1?L(?)?
?i+ ?
sign(?i)2Note that despite gradient feature testing, the parametersfor some features can be driven to zero in conjugate gradientoptimization of the `1-regularized objective function.
Care hasto be taken to catch those features and prune them explicitly toavoid numerical instability.The sign of ?i decides if ?
is added or subtractedfrom the gradient for feature fi.
For a feature thatis newly added to the model and thus has weight?i = 0, we use the feature gradient test to determinethe sign.
If ?L(?)?
?i > ?, we know that?C(?)?
?i> 0,thus we let sign(?i) = ?1 in order to decrease C.Following the same rationale, if ?L(?)?
?i < ??
weset sign(?i) = +1.
An outline of an n-best graftingalgorithm is given in Fig.
1.6 Experiments6.1 Train and Test DataIn the experiments presented in this paper, we eval-uate `2, `1, and `0 regularization on the task ofstochastic parsing with maximum-entropy modelsFor our experiments, we used a stochastic parsingsystem for LFG that we trained on section 02-21of the UPenn Wall Street Journal treebank (Mar-cus et al, 1993) by discriminative estimation of aconditional maximum-entropy model from partiallylabeled data (see Riezler et al (2002)).
For esti-mation and best-parse searching, efficient dynamic-programming techniques over features forests areemployed (see Kaplan et al (2004)).
For the setupof discriminative estimation from partially labeleddata, we found that a restriction of the training datato sentences with a relatively low ambiguity ratewas possible at no loss in accuracy compared totraining from all sentences.
Furthermore, data wererestricted to sentences of which a discriminativelearner can possibly take advantage, i.e.
sentenceswhere the set of parses assigned to the labeled stringis a proper subset of the parses assigned to the un-labeled string.
Together with a restriction to exam-ples that could be parsed by the full grammar anddid not have to use a backoff mechanism of frag-ment parses, this resulted in a training set of 10,000examples with at most 100 parses.
Evaluation wasdone on the PARC 700 dependency bank3, whichis an LFG annotation of 700 examples randomlyextracted from section 23 of the UPenn WSJ tree-bank.
To tune regularization parameters, we split thePARC 700 into a heldout and test set of equal size.6.2 Feature ConstructionTable 1 shows the 11 feature templates that wereused in our experiments to create 60, 109 features.On the around 300,000 parses for 10,000 sentencesin our final training set, 10, 986 features were active,resulting in a matrix of active features times parsesthat has 66 million non-zero entries.
The scale ofthis experiment is comparable to experiments where3http://www2.parc.com/istl/groups/nltt/fsbank/Table 1: Feature templatesname parameters activation conditionLocal Templatescs label label constituent label is present in parsecs adj label parent label, constituent child label ischild label child of constituent parent labelcs right branch constituent has right childcs conj nonpar depth non-parallel conjuncts within depth levelsfs attrs attrs f-structure attribute is one of attrsfs attr value attr, value attribute attr has value valuefs attr subsets attr sum of cardinalities of subsets of attrlex subcat pred, args sets verb pred has one of args sets as argumentsNon-Local (Top-Down) Templatescs embedded label, size chain of size constituentslabeled label embedded into one anothercs sub label ancestor label, constituent descendant labeldescendant label is descendant of ancestor labelfs aunt subattr aunts, parents, one of descendants is descendant of one ofdescendants parents which is a sister of one of auntsmuch larger, but sparser feature sets are employed4.The reason why the matrix of non-zeroes is lesssparse in our case is that most of our feature tem-plates are instantiated to linguistically motivatedcases, and only a few feature templates encode allpossible conjunctions of simple feature tests.
Re-dundant features are introduced mostly by the lat-ter templates, whereas the former features are gen-eralizations over possible combinations of grammarconstants.
We conjecture that feature sets like thisare typical for natural language applications.Efficient feature detection is achieved by a com-bination of hashing and dynamic programming onthe packed representation of c- and f-structures(Maxwell and Kaplan, 1993).
Features can be de-scribed as local and non-local, depending on the sizeof the graph that has to be traversed in their compu-tation.
For each local template one of the parame-ters is selected as a key for hashing.
Non-local fea-tures are treated as two (or more) local sub-features.Packed structures are traversed depth-first, visitingeach node only once.
Only the features keyed onthe label of the current node are considered formatching.
For each non-local feature, the contextsof matching subfeatures are stored at the respectivenodes, propagated upward in dynamic programingfashion, and conjoined with contexts of other sub-features of the feature.
Fully matched features areassociated with the corresponding contexts resultingin a feature-annotated and/or-forest.
This annotated4For example, Malouf (2002) reports a matrix of non-zeroesthat has 55 million entries for a shallow parsing experimentwhere 260,000 features were employed.and/or forest is exploited for dynamic programmingcomputation in estimation and best parse selection.6.3 Experimental ResultsTable 2 shows the results of an evaluation of fivedifferent systems of the test split of the PARC 700dependency bank.
The presented systems are unreg-ularized maximum-likelihood estimation of a log-linear model including the full feature set (mle),standardized maximum-likelihood estimation as de-scribed in Sect.
4 (std), `0 regularization usingfrequency-based cutoff, `1 regularization using n-best grafting, and `2 regularization using a Gaus-sian prior.
All `p regularization runs use a standard-ization of the feature space.
Special regularizationparameters were adjusted on the heldout split, re-sulting in a cutoff threshold of 16, and penaliza-tion factors of 20 and 100 for `1 and `2 regular-ization respectively, with an optimal choice of 100features to be added in each n-best grafting step.Performance of these systems is evaluated firstlywith respect to F-score on matching dependency re-lations.
Note that the F-score values on the PARC700 dependency bank range between a lower boundof 68.0% for averaging over all parses and an upperbound of 83.6% for the parses producing the bestpossible matches.
Furthermore, compression of thefull feature set by feature selection, number of con-jugate gradient iterations, and computation time (inhours:minutes of elapsed time) are reported.55All experiments were run on one CPU of a dual processorAMD Opteron 244 with 1.8GHz clock speed and 4GB of mainmemory.Table 2: F-score, compression, number of iterations,and elapsed time for unregularized and standardizedmaximum-likelihood estimation, and `0, `1, and `2regularization on test split of PARC 700 dependencybank.mle std `0 `2 `1F-score 77.9 78.1 78.1 78.9 79.3compr.
0 0 18.4 0 82.7cg its.
761 371 372 34 226time 129:12 66:41 60:47 6:19 5:25Unregularized maximum-likelihood estimationusing the full feature set exhibits severe overtrainingproblems, as the relation of F-score to the numberof conjugate gradient iterations shows.
Standard-ization of input data can alleviate this problem byimproving convergence behavior to half the num-ber of conjugate gradient iterations.
`0 regulariza-tion achieves its maximum on the heldout data for athreshold of 16, which results in an estimation runthat is slightly faster than standardized estimationusing all features, due to a compression of the fullfeature set by 18%.
`2 regularization benefits froma very tight prior (standard deviation of 0.1 corre-sponding to penalty 100) that was chosen on theheldout set.
Despite the fact that no reduction of thefull feature set is achieved, this estimation run in-creases the F-score to 78.9% and improves compu-tation time by a factor of 20 compared to unregular-ized estimation using all features.
`1 regularizationfor n-best grafting, however, even improves uponthis result by increasing the F-score to 79.3%, fur-ther decreasing computation time to 5:25 hours, at acompression of the full feature set of 83%.77.57878.57979.51 10 100 1000 10000101001000F-score Num CG IterationsFeatures Added At Each StepF-score3333333Num CG Iterations+++++++Figure 2: n-best grafting with n of features addedat each step plotted against F-score on test set andconjugate gradient iterations.As shown in Fig.
2, for feature selection from lin-guistically motivated feature sets with only a mod-erate amount of truly redundant features, it is crucialto choose the right number n of features to be addedin each grafting step.
The number of conjugate gra-dient iterations decreases rapidly in the number offeatures added at each step, whereas F-score evalu-ated on the test set does not decrease (or increasesslightly) until more than 100 features are added ineach step.
100-best grafting thus reduces estimationtime by a factor of 10 at no loss in F-score comparedto 1-best grafting.
Further increasing n results in asignificant drop in F-score, while smaller n is com-putationally expensive, and also shows slight over-training effects.Table 3: F-score, compression, number of itera-tions, and elapsed time for gradient-based incre-mental feature selection without regularization, andwith `2, and `1 regularization on test split of PARC700 dependency bank.mle-ifs `2-ifs `1F-score 78.8 79.1 79.3compr.
88.1 81.7 82.7cg its.
310 274 226time 6:04 6:56 5:25In another experiment we tried to assess the rel-ative contribution of regularization and incrementalfeature selection to the `1-grafting technique.
Re-sults of this experiments are shown in Table 3.
Inthis experiment we applied incremental feature se-lection using the gradient test described above to un-regularized maximum-likelihood estimation (mle-ifs) and `2-regularized maximum-likelihood estima-tion (`2-ifs).
Threshold parameters ?
are adjustedon the heldout set, in addition to and independentof regularization parameters such as the varianceof the Gaussian prior.
Results are compared to `1-regularized grafting as presented above.
For all runsa number of 100 features to be added in each graft-ing step is chosen.
The best result for the mle-ifs runis achieved at a threshold of 25, yielding an F-scoreof 78.8%.
This shows that incremental feature se-lection is a powerful tool to avoid overfitting.
A fur-ther improvement in F-score to 79.1% is achievedby combining incremental feature selection with the`2 regularizer at a variance of 0.1 for the Gaussianprior and a threshold of 15.
Both runs provide ex-cellent compression rates and convergence times.However, they are still outperformed by the `1 runthat achieves a slight improvement in F-score to79.3% and a slightly better runtime.
Furthermore,by integrating regularization naturally into thresh-olding for feature selection, a separate thresholdingparameter is avoided in `1-based incremental fea-ture selection.A theoretical account of the savings in com-putational complexity that can be achieved by n-best grafting can be given as follows.
Perkins etal.
(2003) assess the computational complexity forstandard gradient-based optimization with the fullfeature set by ?
cmp2?
, for a multiple c of p lineminimizations for p derivatives over m data points,each of which has cost ?
.
In contrast, for grafting,the cost is assessed by adding up the costs for fea-ture testing and optimization for s grafting steps as?
(msp+13cms3)?
.
For n-best grafting as proposedin this paper, the number of steps can be decom-posed into s = n ?
t for n features added at eachof t steps.
This results in a cost of ?
mtp for fea-ture testing, and ?
13cmn2t3?
for optimization.
Ifwe assume that t  n  s, this indicates consid-erable savings compared to both 1-best grafting andstandard gradient-based optimization.7 Discussion and ConclusionA related approach to `1 regularization andconstraint-relaxation for maximum-entropy mod-eling has been presented by Kazama and Tsujii(2003).
In this approach, constraint relaxation isdone by allowing two-sided inequality constraints?Bi ?
p?[fi]?
p[fi] ?
Ai, Ai, Bi > 0in entropy maximization.
The dual function is theregularized likelihood function1mm?j=1p???
(xj |yj)?n?i=1?iAi ?n?i=1?iBiwhere the two parameter vectors ?
and ?
replaceour parameter vector ?, and ?i, ?i ?
0.
This reg-ularizer corresponds to a simplification of double-sided exponentials to a one-sided exponential dis-tribution which is non-zero only for non-negativeparameters.
The use of one-sided exponential pri-ors for log-linear models has also been proposedby Goodman (2003), however, without a motiva-tion in a maximum entropy framework.
The fact thatKazama and Tsujii (2003) allow for lower and up-per bounds of different size requires the parameterspace to be doubled in their approach.
Furthermore,similar to Goodman (2003), the requirement to workwith a one-sided strictly positive exponential dis-tribution makes it necessary to double the featurespace to account for (dis)preferences in terms ofstrictly positive parameter values.
These are consid-erable computational and implementational disad-vantages of these approaches.
More importantly, anintegration of `1 regularization into incremental fea-ture selection was not considered.Incremental feature selection has been proposedfirstly by Della Pietra et al (1997) in a likelihood-based framework.
In this approach, an approximategain in likelihood for adding a feature to the modelis used as feature selection criterion, and thresholdson this gain are used as stopping criterion.
Maxi-mization of approximate likelihood gains and gra-dient feature testing both are greedy approxima-tions to the true gain in the objective function -grafting can be seen as applying one iteration ofNewton?s method, where the weight of the newlyadded feature is initialized at 0, to calculate the ap-proximate likelihood gain.
Efficiency and accuracyof both approaches are comparable, however, thegrafting framework provides a well-defined mathe-matical basis for feature selection and optimizationby incorporating selection thresholds naturally aspenalty factors of the regularizer.
The idea of addingn-best features in each selection step also has beeninvestigated earlier in the likelihood-based frame-work (see for example McCallum (2003)).
How-ever, the possible improvements in computationalcomplexity and generalization performance due ton-best selection were not addressed explicitly.
Fur-ther improvements of efficiency of grafting are pos-sible by applying Zhou et al?s (2003) technique ofrestricting feature selection in each step to the top-ranked features from previous stages.In sum, we presented an application of `1 regu-larization to likelihood maximization for log-linearmodels that has a simple interpretation as boundedconstraint relaxation in terms of maximum entropyestimation.
The presented n-best grafting methoddoes not require specialized algorithms or simplifi-cations of the prior, but allows for an efficient, math-ematically well-defined combination of feature se-lection and regularization.
In an experimental eval-uation, we showed n-best grafting to outperform `0,1-best `1, `2 regularization and standard incremen-tal feature selection in terms of computational effi-ciency and generalization performance.ReferencesStephen Boyd and Lieven Vandenberghe.
2004.Convex Optimization.
Cambridge UniversityPress.Stanley F. Chen and Ronald Rosenfeld.
1999.A gaussian prior for smoothing maximum en-tropy models.
Technical Report CMU-CS-99-108, Carnegie Mellon University, Pittsburgh, PA.Stephen Della Pietra, Vincent Della Pietra, and JohnLafferty.
1997.
Inducing features of randomfields.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 19(4):380?393.Joshua Goodman.
2003.
Exponential priorsfor maximum entropy models.
UnpublishedManuscript, Microsoft Research, Redmont, WA.Mark Johnson, Stuart Geman, Stephen Canon, ZhiyiChi, and Stefan Riezler.
1999.
Estimators forstochastic ?unification-based?
grammars.
In Pro-ceedings of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL?99),College Park, MD.Ronald M. Kaplan, Stefan Riezler, Tracy H. King,John T. Maxwell III, and Alexander Vasserman.2004.
Speed and accuracy in shallow and deepstochastic parsing.
In Proceedings of the HumanLanguage Technology conference / North Ameri-can chapter of the Association for ComputationalLinguistics annual meeting (HLT/NAACL?04),Boston, MA.Jun?ichi Kazama and Jun?ichi Tsujii.
2003.
Eval-uation and extension of maximum entropy mod-els with inequality constraints.
In Proceedings ofEMNLP?03, Sapporo, Japan.Guy Lebanon and John Lafferty.
2001.
Boostingand maximum likelihood for exponential models.In Advances in Neural Information Processing 14(NIPS?01), Vancouver.Robert Malouf.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
InProceedings of Computational Natural LanguageLearning (CoNLL?02), Taipei, Taiwan.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of english: The Penntreebank.
Computational Linguistics, 19(2):313?330.John Maxwell and Ron Kaplan.
1993.
The inter-face between phrasal and functional constraints.Computational Linguistics, 19(4):571?589.Andrew McCallum.
2003.
Efficiently inducing fea-tures of conditional random fields.
In Proceed-ings of the 19th Conference on Uncertainty in Ar-tificial Intelligence (UAI?03), Acapulco, Mexico.Thomas Minka.
2001.
Algorithms for maximum-likelihood logistic regression.
Department ofStatistics, Carnegie Mellon University.Andrew Y. Ng.
2004.
Feature selection, l1 vs. l2regularization, and rotational invariance.
In Pro-ceedings of the 21st International Conference onMachine Learning, Banff, Canada.Simon Perkins, Kevin Lacker, and James Theiler.2003.
Grafting: Fast, incremetal feature selectionby gradient descent in function space.
MachineLearning, 3:1333?1356.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell, and MarkJohnson.
2002.
Parsing the Wall Street Jour-nal using a Lexical-Functional Grammar and dis-criminative estimation techniques.
In Proceed-ings of the 40th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL?02),Philadelphia, PA.Robert Tibshirani.
1996.
Regression shrinkage andselection via the lasso.
Journal of the Royal Sta-tistical Society.
Series B, 58(1):267?288.Yaqian Zhou, Fuliang Weng, Lide Wu, and HaukeSchmidt.
2003.
A fast algorithm for feature se-lection in conditional maximum entropy mod-eling.
In Proceedings of EMNLP?03, Sapporo,Japan.Appendix: Proof of Proposition 2Following Boyd and Vandenberghe (2004), the con-vex conjugate of function g : IRn ?
IR isg?
(w) = supu{n?i=1wiui ?
g(u)}and the dual norm ?
?
??
of norm ?
?
?
on IRn is?w??
= supu{n?i=1wiui| ?u?
?
1} (4)and the dual norm of the `1 norm is the `?
norm?w??
= ?w??
for ?u?
= ?u?11 (5)We show that the convex conjugate ofg(u) = ?
?u?11, for ?
> 0is g?
(w) ={0 ?w??
?
??
otherwiseProof.
Let ?w??
?
?, then?i wiui ??u?11?w??
(from 4 and 5)?
?u?11?
(since ?w??
??).
Then ?i wiui ?
?u?11?
?
0 and u = 0 maxi-mizes it with maximum value g?
(w) = 0.Let ?w??
> ?, then ?z s.t.
?z?11 ?
1 and?i wizi > ?
(from 4 and 5).
For u = tz, let t ?
?, then?i wiui??
?u?11 = t(?i wizi??
?z?11) ??
(since?i wizi?
?
?z?11 > 0), thus g?
(w) = ?.
