Book ReviewsGraph-Based Natural Language Processing and Information RetrievalRada Mihalcea and Dragomir Radev(University of North Texas and University of Michigan)Cambridge, UK: Cambridge University Press, 2011, viii+192 pp; hardbound,ISBN 978-0-521-89613-9, $65.00Reviewed byChris BiemannTechnische Universita?t DarmstadtGraphs are ubiquitous.
There is hardly any domain in which objects and their relationscannot be intuitively represented as nodes and edges in a graph.
Graph theory is awell-studied sub-discipline of mathematics, with a large body of results and a largenumber of efficient algorithms that operate on graphs.
Like many other disciplines, thefields of natural language processing (NLP) and information retrieval (IR) also dealwith data that can be represented as a graph.
In this light, it is somewhat surprisingthat only in recent years the applicability of graph-theoretical frameworks to languagetechnology became apparent and increasingly found its way into publications in thefield of computational linguistics.
Using algorithms that take the overall graph structureof a problem into account, rather than characteristics of single objects or (unstructured)sets of objects, graph-based methods have been shown to improve a wide range of NLPtasks.
In a short but comprehensive overview of the field of graph-based methods forNLP and IR, Rada Mihalcea and Dragomir Radev list an extensive number of techniquesand examples from a wide range of research papers by a large number of authors.This book provides an excellent review of this research area, and serves both as anintroduction and as a survey of current graph-based techniques in NLP and IR.
Becausethe few existing surveys in this field concentrate on particular aspects, such as graphclustering (Lancichinetti and Fortunato 2009) or IR (Liu 2006), a textbook on the topicwas very much needed and this book surely fills this gap.The book is organized in four parts and contains a total of nine chapters.
Thefirst part gives an introduction to notions of graph theory, and the second part coversnatural and random networks.
The third part is devoted to graph-based IR, and partIV covers graph-based NLP.
Chapter 1 lays the groundwork for the remainder of thebook by introducing all necessary concepts in graph theory, including the notation,graph properties, and graph representations.
In the second chapter, a glimpse is offeredinto the plethora of graph-based algorithms that have been developed independentlyof applications in NLP and IR.
Sacrificing depth for breadth, this chapter does agreat job in touching on a wide variety of methods, including minimum spanningtrees, shortest-path algorithms, cuts and flows, subgraph matching, dimensionalityreduction, random walks, spreading activation, and more.
Algorithms are explainedconcisely, using examples, pseudo-code, and/or illustrations, some of which are verywell suited for classroom examples.
Network theory is presented in Chapter 3.
Theterm network is here used to refer to naturally occurring relations, as opposed to graphsbeing generated by an automated process.
After presenting the classical Erdo?s-Re?nyirandom graph model and showing its inadequacy to model power-law degree distri-butions following Zipf?s law, scale-free small-world networks are introduced.
Further,?
2012 Association for Computational LinguisticsComputational Linguistics Volume 38, Number 1several centrality measures, as well as other topics in network theory, are definedand exemplified.Establishing the connection to NLP, Chapter 4 introduces networks constructedfrom natural language.
Co-occurrence networks and syntactic dependency networksare examined quantitatively.
Results on the structure of semantic networks such asWordNet are presented, as well as a range of similarity networks between lexical units.This chapter will surely inspire the reader to watch out for networks in his/her owndata.
Chapter 5 turns to link analysis for the Web.
The PageRank algorithm is de-scribed at length, variants for undirected and weighted graphs are introduced, and thealgorithm?s application to topic-sensitive analysis and query-dependent link analysisis discussed.
This chapter is the only one that touches on core IR, and this is also theonly chapter with content that can be found in other textbooks (e.g., Liu 2011).
Still,this chapter is an important prerequisite for the chapter on applications.
It would havebeen possible to move the description of the algorithms to Chapter 2, however, omittingthis part.The topic of Chapter 6 is text clustering with graph-based methods, outliningthe Fiedler method, the Kernighan?Lin method, min-cut clustering, betweenness, andrandom walk clustering.
After defining measures on cluster quality for graphs, spectraland non-spectral graph clustering methods are briefly introduced.
Most of the chapteris to be understood as a presentation of general graph clustering methods rather thantheir application to language.
For this, some representative methods for different coreideas were selected.
Part IV on graph-based NLP contains the chapters probably mostinteresting to readers working in computational linguistics.
In Chapter 7, graph-basedmethods for lexical semantics are presented, including detection of semantic classes,synonym detection using random walks on semantic networks, semantic distance onWordNet, and textual entailment using graph matching.
Methods for word sense andname disambiguation with graph clustering and random walks are described.
The chap-ter closes with graph-based methods for sentiment lexicon construction and subjectivityclassification.Graph-based methods for syntactic processing are presented in Chapter 8: anunsupervised part-of-speech tagging algorithm based on graph clustering, minimumspanning trees for dependency parsing, PP-attachment with random walks over syn-tactic co-occurrence graphs, and coreference resolution with graph cuts.
In the finalchapter, many of the algorithms introduced in the previous chapters are applied toNLP applications as diverse as summarization, passage retrieval, keyword extraction,topic identification and segmentation, discourse, machine translation, cross-languageIR, term weighting, and question answering.As someone with a background in graph-based NLP, I enjoyed reading this book.The writing style is concise and clear, and the authors succeed in conveying the mostimportant points from an incredibly large number of works, viewed from the graph-based perspective.
I also liked the extensive use of examples?throughout, almost halfof the space is used for figures and tables illustrating the methods, which some readersmight perceive as unbalanced, however.
With just under 200 pages and a topic as broadas this, it necessarily follows that many of the presented methods are exemplified andtouched upon rather than discussed in great detail.
Although this sometimes leads tothe situation that some passages can only be understood with background knowledge,it is noteworthy that every chapter includes a section on further reading.
In this way,the book serves as an entry point to a deeper engagement with graph-based methodsfor NLP and IR, and it encourages readers to see their NLP problem from a graph-basedview.220Book ReviewsFor a future edition, however, I have a few wishes: It would be nice if the figures andexamples were less detached from the text and explained more thoroughly.
At times, itwould be helpful to present deeper insights and to connect the methodologies, ratherthan just presenting them next to each other.
Also, some of the definitions in Chapter 2could be less confusing and structured better.Because this book emphasizes graph-based aspects for language processing ratherthan aiming at exhaustively treating the numerous tasks that benefit from graph-basedmethods, it cannot replace a general introduction to NLP or IR: For students withoutprior knowledge in NLP and IR, a more guided and focused approach to the topicwould be required.
The target audience is, rather, NLP researchers and professionalswho want to add the graph-based view to their arsenal of methods, and to becomeinspired by this rapidly growing research area.
It is equally suited for people workingin graph algorithms to learn about graphs in language as a field of application for theirwork.
I will surely consult this volume in the future to supplement the preparation oflectures because of its comprehensive references and its richness in examples.ReferencesLancichinetti, Andrea and Santo Fortunato.2009.
Community detection algorithms:A comparative analysis.
PhysicalReview E, 80:056117.Liu, Bing.
2011.
Web Data Mining:Exploring Hyperlinks, Contents,and Usage Data (second edition).Berlin, Springer.Liu, Yi.
2006.
Graph-based learningmodels for information retrieval:A survey.
Available at: www.cse.msu.edu/?rongjin/semisupervised/graph.pdf.Chris Biemann is Juniorprofessor (assistant professor) for Language Technology at DarmstadtUniversity of Technology.
His current research interests include statistical semantics, graph-based methods for unsupervised acquisition, and topic modeling.
Biemann?s address isUKP lab, Computer Science Department, Hochschulstr.
10, 64289 Darmstadt, Germany; e-mail:biemann@tk.informatik.tu-darmstadt.de.221
