Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1944?1953,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsN -gram language models for massively parallel devicesNikolay Bogoychev and Adam LopezUniversity of EdinburghEdinburgh, United KingdomAbstractFor many applications, the query speed ofN -gram language models is a computa-tional bottleneck.
Although massively par-allel hardware like GPUs offer a poten-tial solution to this bottleneck, exploitingthis hardware requires a careful rethink-ing of basic algorithms and data structures.We present the first language model de-signed for such hardware, using B-trees tomaximize data parallelism and minimizememory footprint and latency.
Comparedwith a single-threaded instance of KenLM(Heafield, 2011), a highly optimized CPU-based language model, our GPU imple-mentation produces identical results witha smaller memory footprint and a sixfoldincrease in throughput on a batch querytask.
When we saturate both devices, theGPU delivers nearly twice the throughputper hardware dollar even when the CPUimplementation uses faster data structures.Our implementation is freely available athttps://github.com/XapaJIaMnu/gLM1 IntroductionN -gram language models are ubiquitous in speechand language processing applications such as ma-chine translation, speech recognition, optical char-acter recognition, and predictive text.
Becausethey operate over large vocabularies, they are oftena computational bottleneck.
For example, in ma-chine translation, Heafield (2013) estimates thatdecoding a single sentence requires a million lan-guage model queries, and Green et al (2014) esti-mate that this accounts for more than 50% of de-coding CPU time.To address this problem, we turn to mas-sively parallel hardware architectures, exempli-Figure 1: Theoretical floating point performanceof CPU and GPU hardware over time (Nvidia Cor-poration, 2015).fied by general purpose graphics processing units(GPUs), whose memory bandwidth and compu-tational throughput has rapidly outpaced that ofCPUs over the last decade (Figure 1).
Exploitingthis increased power is a tantalizing prospect forany computation-bound problem, so GPUs havebegun to attract attention in natural language pro-cessing, in problems such as parsing (Canny etal., 2013; Hall et al, 2014), speech recognition(Chong et al, 2009; Chong et al, 2008), andphrase extraction for machine translation (He etal., 2015).
As these efforts have shown, it isnot trivial to exploit this computational power, be-cause the GPU computational model rewards dataparallelism, minimal branching, and minimal ac-cess to global memory, patterns ignored by manyclassic NLP algorithms (Section 2).We present the first language model data struc-ture designed for this computational model.
Ourdata structure is a trie in which individual nodesare represented by B-trees, which are searchedin parallel (Section 3) and arranged compactly in1944memory (Section 4).
Our experiments across arange of parameters in a batch query setting showthat this design achieves a throughput six timeshigher than KenLM (Heafield, 2011), a highly effi-cient CPU implementation (Section 5).
They alsoshow the effects of device saturation and of datastructure design decisions.2 GPU computational modelGPUs and other parallel hardware devices have adifferent computational profile from widely-usedx86 CPUs, so data structures designed for serialmodels of computation are not appropriate.
Toproduce efficient software for a GPU we must befamiliar with its design (Figure 2).2.1 GPU designA GPU consists of many simple computationalcores, which have neither complex caches norbranch predictors to hide latencies.
Because theyhave far fewer circuits than CPU cores, GPU coresare much smaller, and many more of them can fiton a device.
So the higher throughput of a GPU isdue to the sheer number of cores, each executing asingle thread of computation (Figure 2).
Each corebelongs to a Streaming Multiprocessor (SM), andall cores belonging to a SM must execute the sameinstruction at each time step, with exceptions forbranching described below.
This execution modelis very similar to single instruction, multiple data(SIMD) parallelism.1Computation on a GPU is performed by an in-herently parallel function or kernel, which definesa grid of data elements to which it will be applied,each processed by a block of parallel threads.Once scheduled, the kernel executes in parallel onall cores allocated to blocks in the grid.
At min-imum, it is allocated to a single warp?32 coreson our experimental GPU.
If fewer cores are re-quested, a full warp is still allocated, and the un-used cores idle.A GPU offers several memory types, which dif-fer in size and latency (Table 1).
Unlike a CPUprogram, which can treat memory abstractly, aGPU program must explicitly specify in whichphysical memory each data element resides.
Thischoice has important implications for efficiencythat entail design tradeoffs, since memory closer1Due to differences in register usage and exceptions forbranching, this model is not pure SIMD.
Nvidia calls itSIMT (single instruction, multiple threads).Figure 2: GPU memory hierarchy and computa-tional model (Nvidia Corporation, 2015).Memory type Latency SizeRegister 0 4BShared 4?8 16KB?96KBGlobal GPU 200?800 2GB?12GBCPU 10K+ 16GB?1TBTable 1: Latency (in clock cycles) and size of dif-ferent GPU memory types.
Estimates are adaptedfrom Nvidia Corporation (2015) and depend onseveral aspects of hardware configuration.to a core is small and fast, while memory furtheraway is large and slow (Table 1).2.2 Designing efficient GPU algorithmsTo design an efficient GPU application we mustobserve the constraints imposed by the hardware,which dictate several important design principles.Avoid branching instructions.
If a branchinginstruction occurs, threads that meet the branchcondition run while the remainder idle (a warp di-vergence).
When the branch completes, threadsthat don?t meet the condition run while the firstgroup idles.
So, to maximize performance, codemust be designed with little or no branching.Use small data structures.
Total memory on astate-of-the-art GPU is 12GB, expected to rise to24GB in the next generation.
Language modelsthat run on CPU frequently exceed these sizes, soour data structures must have the smallest possiblememory footprint.Minimize global memory accesses.
Data inthe CPU memory must first be transferred to thedevice.
This is very slow, so data structures mustreside in GPU memory.
But even when they1945Data structure Size Query Ease of Construction Losslessspeed backoff timeTrie (Heafield, 2011) Small Fast Yes Fast YesProbing hash table (Heafield, 2011) Larger Faster Yes Fast YesDouble array (Yasuhara et al, 2013) Larger Fastest Yes Very slow YesBloom filter (Talbot and Osborne, 2007) Small Slow No Fast NoTable 2: A survey of language model data structures and their computational properties.reside in global GPU memory, latency is high, sowherever possible, data should be accessed fromshared or register memory.Access memory with coalesced reads.
Whena thread requests a byte from global memory,it is copied to shared memory along with manysurrounding bytes (between 32 and 128 dependingon the architecture).
So, if consecutive threadsrequest consecutive data elements, the data iscopied in a single operation (a coalesced read),and the delay due to latency is incurred only oncefor all threads, increasing throughput.3 A massively parallel language modelLet w be a sentence, wiits ith word, and N theorder of our model.
An N -gram language modeldefines the probability of w as:P (w) =|w|?i=1P (wi|wi?1...wi?N+1) (1)A backoff language model (Chen and Goodman,1999) is defined in terms of n-gram probabil-ities P (wi|wi?1...wi?n+1) for all n from 1 toN , which are in turn defined by n-gram pa-rameters?P (wi...wi?n+1) and backoff parameters?(wi?1...wi?n+1).
Usually?P (wi...wi?n+1) and?
(wi?1...wi?n+1) are probabilities conditionedon wi?1...wi?n+1, but to simplify the followingexposition, we will simply treat them as numericparameters, each indexed by a reversed n-gram.
Ifparameter?P (wi...wi?n+1) is nonzero, then:P (wi|wi?1...wi?n+1) =?P (wi...wi?n+1)Otherwise:P (wi|wi?1...wi?n+1) =P (wi|wi?1...wi?n+2)?
?
(wi?1...wi?n+1)This recursive definition means that the probabil-ity P (wi|wi?1...wi?N+1) required for Equation 1may depend on multiple parameters.
If r (< N ) isthe largest value for which?P (wi|wi?1...wi?r+1)is nonzero, then we have:P (wi|wi?1...wi?N+1) = (2)?P (wi...wi?r+1)N?n=r+1?
(wi?1...wi?n+1)Our data structure must be able to efficiently ac-cess these parameters.3.1 Trie language modelsWith this computation in mind, we surveyed sev-eral popular data structures that have been usedto implement N -gram language models on CPU,considering their suitability for adaptation to GPU(Table 2).
Since a small memory footprint is cru-cial, we implemented a variant of the trie datastructure of Heafield (2011).
We hypothesized thatits slower query speed compared to a probing hashtable would be compensated for by the throughputof the GPU, a question we return to in Section 5.A trie language model exploits two impor-tant guarantees of backoff estimators: first, if?P (wi...wi?n+1) is nonzero, then?P (wi...wi?m+1)is also nonzero, for all m < n; second, if?
(wi?1...wi?n+1) is one, then ?
(wi?1...wi?p+1)is one, for all p > n. Zero-valued n-gram parameters and one-valued backoff pa-rameters are not explicitly stored.
To com-pute P (wi|wi?1...wi?N+1), we iteratively retrieve?P (wi...wi?m+1) for increasing values of m un-til we fail to find a match, with the final nonzerovalue becoming?P (wi...wi?r+1) in Equation 2.We then iteratively retrieve ?
(wi?1...wi?n+1) forincreasing values of n starting from r + 1 andcontinuing until n = N or we fail to find amatch, multiplying all retrieved terms to computeP (wi|wi?1...wi?N+1) (Equation 2).
The trie isdesigned to execute these iterative parameter re-trievals efficiently.Let ?
be a our vocabulary, ?nthe set ofall n-grams over the vocabulary, and ?
[N ]the1946AustraliaisofonemanyonehumanPolandareisareisexistisFigure 3: Fragment of a trie showing the path ofN -gram is one of in bold.
A query for theN -gramevery one of traverses the same path, but since ev-ery is not among the keys in the final node, it re-turns the n-gram parameter?P (of|one) and returnsto the root to seek the backoff parameter ?(everyone).
Based on image from Federico et al (2008).set ?1?
... ?
?N.
Given an n-gram keywi...wi?n+1?
?
[N ], our goal is to retrieve value?
?P (wi...wi?n+1), ?(wi...wi?n+1)?.
We assume abijection from ?
to integers in the range 1, ..., |?|,so in practice all keys are sequences of integers.When n = 1, the set of all possible keys is just?.
For this case, we can store keys with nontriv-ial values in a sorted array A and their associatedvalues in an array V of equal length so that V [j] isthe value associated with key A[j].
To retrieve thevalue associated with key k, we seek j for whichA[j] = k and return V [j].
Since A is sorted, jcan be found efficiently with binary or interpolatedsearch (Figure 4).When n > 1, queries are recursive.
Forn < N , for every wi...wi?n+1for which?P (wi...wi?n+1) > 0 or ?
(wi...wi?n+1) <1, our data structure contains associated arraysKwi...wi?n+1and Vwi...wi?n+1.
When key kis located in Awi...wi?n+1[j], the value storedat Vwi...wi?n+1[j] includes the address of arraysAwi...wi?n+1kand Vwi...wi?n+1k.
To find the valuesassociated with an n-gram wi...wi?n+1, we firstsearch the root arrayA for j1such thatA[j1] = wi.We retrieve the address ofAwifrom V [j1], and wethen search for j2such that Awi[j2] = wi?1.
Wecontinue to iterate this process until we find thevalue associated with the longest suffix of our n-gram stored in the trie.
We therefore iterativelyretrieve the parameters needed to compute Equa-tion 2, returning to the root exactly once if backoffparameters are required.3.1.1 K-ary search and B-treesOn a GPU, the trie search algorithm describedabove is not efficient because it makes extensiveuse of binary search, an inherently serial algo-rithm.
However, there is a natural extension ofbinary search that is well-suited to GPU: K-arysearch (Hwu, 2011).
Rather than divide an arrayin two as in binary search, K-ary search divides itinto K equal parts and performs K ?
1 compar-isons simultaneously (Figure 5).To accommodate large language models, thecomplete trie must reside in global memory, andin this setting, K-ary search on an array is inef-ficient, since the parallel threads will access non-consecutive memory locations.
To avoid this, werequire a data structure that places the K elementscompared byK-ary search in consecutive memorylocations so that they can be copied from global toshared memory with a coalesced read.
This datastructure is a B-tree (Bayer and McCreight, 1970),which is widely used in databases, filesystems andinformation retrieval.Informally, a B-tree generalizes binary trees inexactly the same way that K-ary search general-izes binary search (Figure 6).
More formally, aB-tree is a recursive data structure that replacesarrays A and V at each node of the trie.
A B-tree node of size K consists of three arrays: a 1-indexed array B of K ?
1 keys; a 1-indexed arrayV of K ?
1 associated values so that V [j] is thevalue associated with key B[j]; and, if the node isnot a leaf, a 0-indexed array C of K addresses tochild B-trees.
The keys in B are sorted, and thesubtree at address pointed to by child C[j] repre-sents only key-value pairs for keys between B[j]and B[j + 1] when 1 ?
j < K, keys less thanB[1] when j = 0, or keys greater than B[K] whenj = K.To find a key k in a B-tree, we start at theroot node, and we seek j such that B[j] ?
k <B[j + 1].
If B[j] = k we return V [j], otherwise ifthe node is not a leaf node we return the result ofrecursively querying the B-tree node at the addressC[j] (C[0] if k < B[1] or C[K] if k > B[K]).
Ifthe key is not found in array B of a leaf, the queryfails.Our complete data structure is a trie in whicheach node except the root is a B-tree (Figure 7).Since the root contains all possible keys, its keysare simply represented by an array A, which canbe indexed in constant time without any search.1947Figure 4: Execution of a binary search for key 15.
Each row represents a time step and highlights theelement compared to the key.
Finding key 15 requires four time steps and four comparisons.Figure 5: Execution of K-ary search with the same input as Figure 4, for K = 8.
The first time stepexecutes seven comparisons in parallel, and the query is recovered in two time steps.Figure 6: In a B-tree, the elements compared in K-ary search are consecutive in memory.
We also showthe layout of an individual entry.4 Memory layout and implementationEach trie node represents a unique n-gramwi...wi?n+1, and if a B-tree node within thetrie node contains key wi?n, then it mustalso contain the associated values?P (wi...wi?n),?
(wi...wi?n), and the address of the trie node rep-resenting wi...wi?n(Figure 6, Figure 3).
The en-tire language model is laid out in memory as asingle byte array in which trie nodes are visitedin breadth-first order and the B-tree representationof each node is also visited in breadth-first order(Figure 7).Since our device has a 64-bit architecture, point-ers can address 18.1 exabytes of memory, far morethan available.
To save space, our data struc-ture does not store global addresses; it insteadstores the difference in addresses between the par-ent node and each child.
Since the array is alignedto four bytes, these relative addresses are dividedby four in the representation, and multiplied byfour at runtime to obtain the true offset.
This en-ables us to encode relative addresses of 16GB, stilllarger than the actual device memory.
We esti-mate that relative addresses of this size allow usto store a model containing around one billion n-grams.2Unlike CPU language model implementa-tions such as those of Heafield (2011) and Watan-abe et al (2009), we do not employ further com-pression techniques such as variable-byte encod-ing or LOUDS, because their runtime decompres-sion algorithms require branching code, which ourimplementation must avoid.We optimize the node representation for coa-lesced reads by storing the keys of each B-treeconsecutively in memory, followed by the corre-sponding values, also stored consecutively (Figure6).
When the data structure is traversed, only keyarrays are iteratively copied to shared memory un-til a value array is needed.
This design minimizesthe number of reads from global memory.4.1 ConstructionThe canonical B-tree construction algorithm (Cor-men et al, 2009) produces nodes that are notfully saturated, which is desirable for B-trees that2We estimate this by observing that a model containing 423Mn-grams takes 3.8Gb of memory, and assuming an approxi-mately linear scaling, though there is some variance depend-ing on the distribution of the n-grams.1948Figure 7: Illustration of the complete data structure, showing a root trie node as an array representingunigrams, and nine B-trees, each representing a single trie node.
The trie nodes are numbered accordingto the order in which they are laid out in memory.Figure 8: Layout of a single B-tree node for K = 4.
Relative addresses of the four child B-tree nodes(array C) are followed by three keys (array B), and three values (array V ), each consisting of an n-gramprobability, backoff, and address of the child trie node.support insertion.
However, our B-trees are im-mutable, and unsaturated nodes of unpredictablesize lead to underutilization of threads, warp di-vergence, and deeper trees that require more iter-ations to query.
So, we use a construction algo-rithm inspired by Cesarini and Soda (1983) andRosenberg and Snyder (1981).
It is implementedon CPU, and the resulting array is copied to GPUmemory to perform queries.Since the entire set of keys and values is knownin advance for each n-gram, our construction al-gorithm receives them in sorted order as the arrayA described in Section 3.1.
The procedure thensplits this array into K consecutive subarrays ofequal size, leaving K ?
1 individual keys betweeneach subarray.3TheseK?1 keys become the keysof the root B-tree.
The procedure is then appliedrecursively to each subarray.
When applied to anarray whose size is less than K, the algorithm re-turns a leaf node.
When applied to an array whose3Since the size of the array may not be exactly divisible byK, some subarrays may differ in length by one.size is greater than or equal toK but less than 2K,it splits the array into a node containing the firstK ?
1 keys, and a single leaf node containing theremaining keys, which becomes a child of the first.4.2 Batch queriesTo fully saturate our GPU we execute manyqueries simultaneously.
A grid receives the com-plete set of N -gram queries and each block pro-cesses a single query by performing a sequence ofK-ary searches on B-tree nodes.5 ExperimentsWe compared our open-source GPU languagemodel gLM with the CPU language modelKenLM (Heafield, 2011).45KenLM can use twoquite different language model data structures:a fast probing hash table, and a more compactbut slower trie, which inspired our own languagemodel design.
Except where noted, our B-tree4https://github.com/XapaJIaMnu/gLM5https://github.com/kpu/kenlm/commit/94954431949node size K = 31, and we measure throughputin terms of query speed, which does not includethe cost of initializing or copying data structures,or the cost of moving data to or from the GPU.We performed our GPU experiments on anNvidia Geforce GTX, a state-of-the-art GPU, re-leased in the first quarter of 2015 and costing 1000USD.
Our CPU experiments were performed ontwo different devices: one for single-threaded testsand one for multi-threaded tests.
For the single-threaded CPU tests, we used an Intel Quad Core i74720HQ CPU released in the first quarter of 2015,costing 280 USD, and achieving 85% of the speedof a state-of-the-art consumer-grade CPU whensingle-threaded.
For the multi-threaded CPU testswe used two Intel Xeon E5-2680 CPUs, offeringa combined 16 cores and 32 threads, costing atthe time of their release 3,500 USD together.
To-gether, their performance specifications are sim-ilar to the recently released Intel Xeon E5-2698v3 (16 cores, 32 threads, costing 3,500USD).
Thedifferent CPU configurations are favorable to theCPU implementation in their tested condition: theconsumer-grade CPU has higher clock speeds insingle-threaded mode than the professional-gradeCPU; while the professional-grade CPUs providemany more cores (though at lower clock speeds)when fully saturated.
Except where noted, CPUthroughput is reported for the single-threaded con-dition.Except where noted, our language model isthe Moses 3.0 release English 5-gram languagemodel, containing 88 million n-grams.6Ourbenchmark task computes perplexity on data ex-tracted from the Common Crawl dataset usedfor the 2013 Workshop on Machine Translation,which contains 74 million words across 3.2 mil-lion sentences.7Both gLM and KenLM produceidentical perplexities, so we are certain that ourimplementation is correct.
Except where noted,the faster KenLM Probing backend is used.
Theperplexity task has been used as a basic test ofother language model implementations (Osborneet al, 2014; Heafield et al, 2015).5.1 Query speedWhen compared to single-threaded KenLM, ourresults (Table 3) show that gLM is just over six6http://www.statmt.org/moses/RELEASE-3.0/models/fr-en/lm/europarl.lm.17http://www.statmt.org/wmt13/training-parallel-commoncrawl.tgzLM (threads) Throughput Size (GB)KenLM probing (1) 10.3M 1.8KenLM probing (16) 49.8M 1.8KenLM probing (32) 120.4M 1.8KenLM trie (1) 4.5M 0.8gLM 65.5M 1.2Table 3: Comparison of gLM and KenLM onthroughput (N -gram queries per second) and datastructure size.times faster than the fast probing hash table, andnearly fifteen times faster than the trie data struc-ture, which is quite similar to our own, thoughslightly smaller due to the use of compression.The raw speed of the GPU is apparent, since wewere able to obtain our results with a relativelyshort engineering effort when compared to that ofKenLM, which has been optimized over severalyears.When we fully saturate our professional-gradeCPU, using all sixteen cores and sixteen hyper-threads, KenLM is about twice as fast as gLM.However, our CPU costs nearly four times as muchas our GPU, so economically, this comparison fa-vors the GPU.On first glance, the scaling from one to six-teen threads is surprisingly sublinear.
This is notdue to vastly different computational power of theindividual cores, which are actually very simi-lar.
It is instead due to scheduling, cache con-tention, and?most importantly?the fact that ourCPUs implement dynamic overclocking: the baseclock rate of 2.7 GHz at full saturation increasesto 3.5 GHz when the professional CPU is under-utilized, as when single-threaded; the rates for theconsumer-grade CPU similarly increase from 2.6to 3.6 GHz.85.2 Effect of B-tree node sizeWhat is the optimal K for our B-tree node size?We hypothesized that the optimal size would beone that approaches the size of a coalesced mem-ory read, which should allow us to maximizeparallelism while minimizing global memory ac-cesses and B-tree depth.
Since the size of a coa-lesced read is 128 bytes and keys are four bytes,we hypothesized that the optimal node size wouldbe around K = 32, which is also the size ofa warp.
We tested this by running experiments8Intel calls this Intel Turbo Boost.1950Figure 9: Effect of BTree node size on throughput(ngram queries per second)that varied K from 5 to 59, and the results (Fig-ure 9) confirmed our hypothesis.
As the node sizeincreases, throughput increases until we reach anode size of 33, where it steeply drops.
This re-sult highlights the importance of designing datastructures that minimize global memory accessand maximize parallelism.We were curious about what effect this nodesize had on the depth of the B-trees representingeach trie node.
Measuring this, we discovered thatfor bigrams, 88% of the trie nodes have a depth ofone?we call these B-stumps, and they can be ex-haustively searched in a single parallel operation.For trigrams, 97% of trie nodes are B-stumps, andfor higher order n-grams the percentage exceeds99%.5.3 Saturating the GPUA limitation of our approach is that it is only ef-fective in high-throughput situations that continu-ally saturate the GPU.
In situations where a lan-guage model is queried only intermittently or onlyin short bursts, a GPU implementation may notbe useful.
We wanted to understand the pointat which this saturation occurs, so we ran ex-periments varying the batch size sent to our lan-guage model, comparing its behavior with thatof KenLM.
To understand situations in which theGPU hosts the language model for query by anexternal GPU, we measure query speed with andwithout the cost of copying queries to the device.Our results (Figure 10) suggest that the deviceis nearly saturated once the batch size reaches athousand queries, and fully saturated by ten thou-sand queries.
Throughput remains steady as batchsize increases beyond this point.
Even with thecost of copying batch queries to GPU memory,Figure 10: Throughput (N -gram queries per sec-ond) vs. batch size for gLM, KenLM probing, andKenLM trie.Regular LM Big LMKenLM 10.2M 8.2MKenLM Trie 4.5M 3.0MgLM 65.5M 55MTable 4: Throughput comparison (ngram queriesper second) between gLM and KenLM with a 5times larger model and a regular language model.throughput is more than three times higher thanthat of single threaded KenLM.
We have not in-cluded results of multi-threaded KenLM scalingon Figure 10 but they are similar to the single-threaded case: throughput (as shown on Table3) plateaus at around one hundred sentences perthread.5.4 Effect of model sizeTo understand the effect of model size on queryspeed, we built a language model with 423 millionn-grams, five times larger than our basic model.The results (Table 4) show an 18% slowdown forgLM and 20% slowdown for KenLM, showingthat model size affects both implementations sim-ilarly.5.5 Effect of N -gram order on performanceAll experiments so far use an N -gram order offive.
We hypothesized that lowering the ngram or-der of the model would lead to faster query time(Table 5).
We observe that N -gram order af-fects throughput of the GPU language model muchmore than the CPU one.
This is likely due to ef-fects of backoff queries, which are more optimizedin KenLM.
At higher orders, more backoff queriesoccur, which reduces throughput for gLM.19515-gram 4-gram 3-gramKenLM 10.2M 9.8M 11.5MKenLM Trie 4.5M 4.5M 5.2MgLM 65.5M 71.9M 93.7MTable 5: Throughput comparison (ngram queriesper second) achieved using lower order ngrammodels.5.6 Effect of templated codeOur implementation initially relied on hard-codedvalues for parameters such as B-tree node sizeand N -gram order, which we later replaced withparameters.
Surprisingly, we observed that thisled to a reduction in throughput from 65.6 mil-lion queries per second to 59.0 million, which wetraced back to the use of dynamically allocatedshared memory, as well as compiler optimizationsthat only apply to compile-time constants.
To re-move this effect, we heavily templated our code,using as many compile-time constants as possi-ble, which improves throughput but enables us tochange parameters through recompilation.5.7 Bottlenecks: computation or memory?On CPU, language models are typically memory-bound: most cycles are spent in random mem-ory accesses, with little computation between ac-cesses.
To see if this is true in gLM we exper-imented with two variants of the benchmark inFigure 3: one in which the GPU core was under-clocked, and one in which the memory was un-derclocked.
This effectively simulates two varia-tions in our hardware: A GPU with slower coresbut identical memory, and one with slower mem-ory, but identical processing speed.
We found thatthroughput decreases by about 10% when under-clocking the cores by 10%.
On the other hand,underclocking memory by 25% reduced through-put by 1%.
We therefore conclude that gLM iscomputation-bound.
We expect that gLM willcontinue to improve on parallel devices offeringhigher theoretical floating point performance.6 ConclusionOur language model is implemented on a GPU,but its general design (and much of the actualcode) is likely to be useful to other hardwarethat supports SIMD parallelism, such as the XeonPhi.
Because it uses batch processing, our on-chiplanguage model could be integrated into a ma-chine translation decoder using strategies similarto those used to integrate an on-network languagemodel nearly a decade ago (Brants et al, 2007).An alternative method of integration would be tomove the decoder itself to GPU.
For phrase-basedtranslation, this would require a translation modeland dynamic programming search algorithm onGPU.
Translation models have been implementedon GPU by He et al (2015), while related searchalgorithms for (Chong et al, 2009; Chong et al,2008) and parsing (Canny et al, 2013; Hall et al,2014) have been developed for GPU.
We intend toexplore these possibilities in future work.AcknowledgementsThis work was conducted within thescope of the Horizon 2020 Innovation Ac-tion Modern MT, which has received funding fromthe European Unions Horizon 2020 research andinnovation programme under grant agreement No645487.We thank Kenneth Heafield, Ulrich Germann,Rico Sennrich, Hieu Hoang, Federico Fancellu,Nathan Schneider, Naomi Saphra, Sorcha Gilroy,Clara Vania and the anonymous reviewers for pro-ductive discussion of this work and helpful com-ments on previous drafts of the paper.
Any errorsare our own.ReferencesR.
Bayer and E. McCreight.
1970.
Organization andmaintenance of large ordered indices.
In Proceed-ings of the 1970 ACM SIGFIDET (Now SIGMOD)Workshop on Data Description, Access and Control,SIGFIDET ?70, pages 107?141.T.
Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.2007.
Large language models in machine transla-tion.
In In Proceedings of EMNLP-CoNLL.J.
Canny, D. Hall, and D. Klein.
2013.
A multi-teraflopconstituency parser using GPUs.
In Proceedings ofEMNLP.F.
Cesarini and G. Soda.
1983.
An algorithm to con-struct a compact B-tree in case of ordered keys.
In-formation Processing Letters, 17(1):13?16.S.
F. Chen and J. Goodman.
1999.
An empiricalstudy of smoothing techniques for language model-ing.
Computer Speech & Language, 13(4):359?393.J.
Chong, Y. Yi, A. Faria, N. R. Satish, and K. Keutzer.2008.
Data-parallel large vocabulary continuous1952speech recognition on graphics processors.
Techni-cal Report UCB/EECS-2008-69, EECS Department,University of California, Berkeley, May.J.
Chong, E. Gonina, Y. Yi, and K. Keutzer.
2009.A fully data parallel WFST-based large vocabularycontinuous speech recognition on a graphics pro-cessing unit.
In Proceedings of Interspeech.T.
H. Cormen, C. E. Leiserson, R. L. Rivest, andC.
Stein.
2009.
Introduction to Algorithms, ThirdEdition.
The MIT Press, 3rd edition.M.
Federico, N. Bertoldi, and M. Cettolo.
2008.IRSTLM: an open source toolkit for handling largescale language models.
In Proceedings of Inter-speech, pages 1618?1621.
ISCA.S.
Green, D. Cer, and C. Manning.
2014.
Phrasal:A toolkit for new directions in statistical machinetranslation.
In Proceedings of WMT.D.
Hall, T. Berg-Kirkpatrick, and D. Klein.
2014.Sparser, better, faster GPU parsing.
In Proceedingsof ACL.H.
He, J. Lin, and A. Lopez.
2015.
Gappy patternmatching on GPUs for on-demand extraction of hi-erarchical translation grammars.
TACL, 3:87?100.K.
Heafield, R. Kshirsagar, and S. Barona.
2015.Language identification and modeling in specializedhardware.
In Proceedings of ACL-IJCNLP, July.K.
Heafield.
2011.
KenLM: faster and smaller lan-guage model queries.
In Proceedings of WMT,pages 187?197, July.K.
Heafield.
2013.
Efficient Language Modeling Al-gorithms with Applications to Statistical MachineTranslation.
Ph.D. thesis, Carnegie Mellon Univer-sity, September.W.-m. W. Hwu.
2011.
GPU Computing Gems Emer-ald Edition.
Morgan Kaufmann Publishers Inc., SanFrancisco, CA, USA, 1st edition.Nvidia Corporation.
2015.
Nvidia CUDACompute Unified Device Architecture Pro-gramming Guide.
Nvidia Corporation.https://docs.nvidia.com/cuda/cuda-c-programming-guide/.M.
Osborne, A. Lall, and B. V. Durme.
2014.
Ex-ponential reservoir sampling for streaming languagemodels.
In Proceedings of ACL, pages 687?692.A.
L. Rosenberg and L. Snyder.
1981.
Time- andspace-optimality in B-trees.
ACM Trans.
DatabaseSyst., 6(1):174?193, Mar.D.
Talbot and M. Osborne.
2007.
Smoothed Bloom fil-ter language models: Tera-scale LMs on the cheap.In Proceedings of EMNLP-CoNLL, pages 468?476.T.
Watanabe, H. Tsukada, and H. Isozaki.
2009.
Asuccinct N-gram language model.
In Proc.
of ACL-IJCNLP.M.
Yasuhara, T. Tanaka, J. ya Norimatsu, and M. Ya-mamoto.
2013.
An efficient language model usingdouble-array structures.
In EMNLP, pages 222?232.1953
