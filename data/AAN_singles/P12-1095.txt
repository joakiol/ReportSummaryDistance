Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 902?911,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsModeling the Translation of Predicate-Argument Structure for SMTDeyi Xiong, Min Zhang?, Haizhou LiHuman Language TechnologyInstitute for Infocomm Research1 Fusionopolis Way, #21-01 Connexis, Singapore 138632{dyxiong, mzhang, hli}@i2r.a-star.edu.sgAbstractPredicate-argument structure contains rich se-mantic information of which statistical ma-chine translation hasn?t taken full advantage.In this paper, we propose two discriminative,feature-based models to exploit predicate-argument structures for statistical machinetranslation: 1) a predicate translation modeland 2) an argument reordering model.
Thepredicate translation model explores lexicaland semantic contexts surrounding a verbalpredicate to select desirable translations forthe predicate.
The argument reordering modelautomatically predicts the moving directionof an argument relative to its predicate af-ter translation using semantic features.
Thetwo models are integrated into a state-of-the-art phrase-based machine translation systemand evaluated on Chinese-to-English transla-tion tasks with large-scale training data.
Ex-perimental results demonstrate that the twomodels significantly improve translation accu-racy.1 IntroductionRecent years have witnessed increasing efforts to-wards integrating predicate-argument structures intostatistical machine translation (SMT) (Wu and Fung,2009b; Liu and Gildea, 2010).
In this paper, we takea step forward by introducing a novel approach to in-corporate such semantic structures into SMT.
Givena source side predicate-argument structure, we at-tempt to translate each semantic frame (predicateand its associated arguments) into an appropriate tar-get string.
We believe that the translation of predi-cates and reordering of arguments are the two central?Corresponding authorissues concerning the transfer of predicate-argumentstructure across languages.Predicates1 are essential elements in sentences.Unfortunately they are usually neither correctlytranslated nor translated at all in many SMT sys-tems according to the error study by Wu and Fung(2009a).
This suggests that conventional lexical andphrasal translation models adopted in those SMTsystems are not sufficient to correctly translate pred-icates in source sentences.
Thus we propose adiscriminative, feature-based predicate translationmodel that captures not only lexical information(i.e., surrounding words) but also high-level seman-tic contexts to correctly translate predicates.Arguments contain information for questions ofwho, what, when, where, why, and how in sentences(Xue, 2008).
One common error in translating ar-guments is about their reorderings: arguments areplaced at incorrect positions after translation.
In or-der to reduce such errors, we introduce a discrim-inative argument reordering model that uses theposition of a predicate as the reference axis to es-timate positions of its associated arguments on thetarget side.
In this way, the model predicts movingdirections of arguments relative to their predicateswith semantic features.We integrate these two discriminative models intoa state-of-the-art phrase-based system.
Experimen-tal results on large-scale Chinese-to-English transla-tion show that both models are able to obtain signif-icant improvements over the baseline.
Our analysison system outputs further reveals that they can in-deed help reduce errors in predicate translations andargument reorderings.1We only consider verbal predicates in this paper.902The paper is organized as follows.
In Section 2,we will introduce related work and show the signif-icant differences between our models and previouswork.
In Section 3 and 4, we will elaborate the pro-posed predicate translation model and argument re-ordering model respectively, including details aboutmodeling, features and training procedure.
Section5 will introduce how to integrate these two modelsinto SMT.
Section 6 will describe our experimentsand results.
Section 7 will empirically discuss howthe proposed models improve translation accuracy.Finally we will conclude with future research direc-tions in Section 8.2 Related WorkPredicate-argument structures (PAS) are exploredfor SMT on both the source and target side in someprevious work.
As PAS analysis widely employsglobal and sentence-wide features, it is computa-tionally expensive to integrate target side predicate-argument structures into the dynamic programmingstyle of SMT decoding (Wu and Fung, 2009b).Therefore they either postpone the integration of tar-get side PASs until the whole decoding procedure iscompleted (Wu and Fung, 2009b), or directly projectsemantic roles from the source side to the target sidethrough word alignments during decoding (Liu andGildea, 2010).There are other previous studies that explore onlysource side predicate-argument structures.
Komachiand Matsumoto (2006) reorder arguments in sourcelanguage (Japanese) sentences using heuristic rulesdefined on source side predicate-argument structuresin a pre-processing step.
Wu et al (2011) automatethis procedure by automatically extracting reorder-ing rules from predicate-argument structures and ap-plying these rules to reorder source language sen-tences.
Aziz et al (2011) incorporate source lan-guage semantic role labels into a tree-to-string SMTsystem.Although we also focus on source side predicate-argument structures, our models differ from the pre-vious work in two main aspects: 1) we propose twoseparate discriminative models to exploit predicate-argument structures for predicate translation and ar-gument reordering respectively; 2) we consider ar-gument reordering as an argument movement (rel-ative to its predicate) prediction problem and usea discriminatively trained classifier for such predic-tions.Our predicate translation model is also related toprevious discriminative lexicon translation models(Berger et al, 1996; Venkatapathy and Bangalore,2007; Mauser et al, 2009).
While previous modelspredict translations for all words in vocabulary, weonly focus on verbal predicates.
This will tremen-dously reduce the amount of training data required,which usually is a problem in discriminative lexi-con translation models (Mauser et al, 2009).
Fur-thermore, the proposed translation model also dif-fers from previous lexicon translation models in thatwe use both lexical and semantic features.
Our ex-perimental results show that semantic features areable to further improve translation accuracy.3 Predicate Translation ModelIn this section, we present the features and the train-ing process of the predicate translation model.3.1 ModelFollowing the context-dependent word models in(Berger et al, 1996), we propose a discriminativepredicate translation model.
The essential compo-nent of our model is a maximum entropy classifierpt(e|C(v)) that predicts the target translation e fora verbal predicate v given its surrounding contextC(v).
The classifier can be formulated as follows.pt(e|C(v)) =exp(?i ?ifi(e, C(v)))?e?
exp(?i ?ifi(e?, C(v)))(1)where fi are binary features, ?i are weights of thesefeatures.
Given a source sentence which containsN verbal predicates {vi}N1 , our predicate translationmodel Mt can be denoted asMt =N?i=1pt(evi |C(vi)) (2)Note that we do not restrict the target translatione to be a single word.
We allow e to be a phraseof length up to 4 words so as to capture multi-wordtranslations for a verbal predicate.
For example, aChinese verb ?u1(issue)?
can be translated as ?tobe issued?
or ?have issued?
with modality words.903This will increase the number of classes to be pre-dicted by the maximum entropy classifier.
But ac-cording to our observation, it is still computation-ally tractable (see Section 3.3).
If a verbal predicateis not translated, we set e = NULL so that we canalso capture null translations for verbal predicates.3.2 FeaturesThe apparent advantage of discriminative lexicontranslation models over generative translation mod-els (e.g., conventional lexical translation model asdescribed in (Koehn et al, 2003)) is that discrim-inative models allow us to integrate richer contexts(lexical, syntactic or semantic) into target translationprediction.
We use two kinds of features to predicttranslations for verbal predicates: 1) lexical featuresand 2) semantic features.
All features are in the fol-lowing binary form.f(e, C(v)) ={1, if e = ?
and C(v).?
= ?0, else(3)where the symbol ?
is a placeholder for a possibletarget translation (up to 4 words), the symbol ?
indi-cates a contextual (lexical or semantic) element forthe verbal predicate v, and the symbol ?
representsthe value of ?.Lexical Features: The lexical element ?
isextracted from the surrounding words of verbalpredicate v. We use the preceding 3 words andthe succeeding 3 words to define the lexical con-text for the verbal predicate v. Therefore ?
?
{w?3, w?2, w?1, v, w1, w2, w3}.Semantic Features: The semantic element ?
isextracted from the surrounding arguments of ver-bal predicate v. In particular, we define a seman-tic window centered at the verbal predicate with6 arguments {A?3, A?2, A?1, A1, A2, A3} whereA?3 ?
A?1 are arguments on the left side of vwhile A1 ?
A3 are those on the right side.
Differ-ent verbal predicates have different number of argu-ments in different linguistic scenarios.
We observeon our training data that the number of arguments for96.5% verbal predicates on each side (left/right) isnot larger than 3.
Therefore the defined 6-argumentsemantic window is sufficient to describe argumentcontexts for predicates.For each argument Ai in the defined seman-f(e, C(v)) = 1 if and only ife = adjourn and C(v).Ah?3 =Sn?e = adjourn and C(v).Ar?1 = ARGM-TMPe = adjourn and C(v).Ah1 =Ue = adjourn and C(v).Ar2 = nulle = adjourn and C(v).Ah3 = nullTable 1: Semantic feature examples.tic window, we use its semantic role (i.e., ARG0,ARGM-TMP and so on) Ari and head word Ahi todefine semantic context elements ?.
If an argumentAi does not exist for the verbal predicate v 2, we setthe value of both Ari and Ahi to null.Figure 1 shows a Chinese sentence with itspredicate-argument structure and English transla-tion.
The verbal predicate ?>?/adjourn?
(in bold)has 4 arguments: one in an ARG0 agent role, onein an ARGM-ADV adverbial modifier role, one inan ARGM-TMP temporal modifier role and the lastone in an ARG1 patient role.
Table 1 shows severalsemantic feature examples of this verbal predicate.3.3 TrainingIn order to train the discriminative predicate transla-tion model, we first parse source sentences and la-beled semantic roles for all verbal predicates (seedetails in Section 6.1) in our word-aligned bilingualtraining data.
Then we extract all training events forverbal predicates which occur at least 10 times inthe training data.
A training event for a verbal predi-cate v consists of all contextual elements C(v) (e.g.,w1, Ah1 ) defined in the last section and the targettranslation e. Using these events, we train one max-imum entropy classifier per verbal predicate (16,121verbs in total) via the off-the-shelf MaxEnt toolkit3.We perform 100 iterations of the L-BFGS algorithmimplemented in the training toolkit for each verbalpredicate with both Gaussian prior and event cutoffset to 1 to avoid overfitting.
After event cutoff, wehave an average of 140 classes (target translations)per verbal predicate with the maximum number ofclasses being 9,226.
The training takes an average of52.6 seconds per verb.
In order to expedite the train-2For example, the verb v has only two arguments on its leftside.
Thus argument A?3 doest not exist.3Available at: http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html904The [Security Council] will adjourn for [4 days] [starting Thursday]Sn?1 ?2 [g3 ?o4 m?5] >?6 [o7 U8]ARG0ARGM-ADVARGM-TMPARG1Figure 1: An example of predicate-argument structure in Chinese and its aligned English translation.
The bold word inChinese is the verbal predicate.
The subscripts on the Chinese sentence show the indexes of words from left to right.ing, we run the training toolkit in a parallel manner.4 Argument Reordering ModelIn this section we introduce the discriminative ar-gument reordering model, features and the trainingprocedure.4.1 ModelSince the predicate determines what arguments areinvolved in its semantic frame and semantic framestend to be cohesive across languages (Fung et al,2006), the movements of predicate and its argumentsacross translations are like the motions of a planetand its satellites.
Therefore we consider the reorder-ing of an argument as the motion of the argumentrelative to its predicate.
In particular, we use the po-sition of the predicate as the reference axis.
The mo-tion of associated arguments relative to the referenceaxis can be roughly divided into 3 categories4: 1) nochange across languages (NC); 2) moving from theleft side of its predicate to the right side of the predi-cate after translation (L2R); and 3) moving from theright side of its predicate to the left side of the pred-icate after translation (R2L).Let?s revisit Figure 1.
The ARG0, ARGM-ADVand ARG1 are located at the same side of their predi-cate after being translated into English, therefore thereordering category of these three arguments is as-signed as ?NC?.
The ARGM-TMP is moved fromthe left side of ?>?/adjourn?
to the right side of?adjourn?
after translation, thus its reordering cate-gory is L2R.In order to predict the reordering category foran argument, we propose a discriminative argu-ment reordering model that uses a maximum en-4Here we assume that the translations of arguments are notinterrupted by their predicates, other arguments or any wordsoutside the arguments in question.
We leave for future researchthe task of determining whether arguments should be translatedas a unit or not.tropy classifier to calculate the reordering categorym ?
{NC, L2R, R2L} for an argument A as fol-lows.pr(m|C(A)) =exp(?i ?ifi(m, C(A)))?m?
exp(?i ?ifi(m?, C(A)))(4)where C(A) indicates the surrounding context of A.The features fi will be introduced in the next sec-tion.
We assume that motions of arguments are in-dependent on each other.
Given a source sentencewith labeled arguments {Ai}N1 , our discriminativeargument reordering model Mr is formulated asMr =N?i=1pr(mAi |C(Ai)) (5)4.2 FeaturesThe features fi used in the argument reorderingmodel still takes the binary form as in Eq.
(3).
Table2 shows the features that are used in the argumentreordering model.
We extract features from both thesource and target side.
On the source side, the fea-tures include the verbal predicate, the semantic roleof the argument, the head word and the boundarywords of the argument.
On the target side, the trans-lation of the verbal predicate, the translation of thehead word of the argument, as well as the boundarywords of the translation of the argument are used asfeatures.4.3 TrainingTo train the argument reordering model, we first ex-tract features defined in the last section from ourbilingual training data where source sentences areannotated with predicate-argument structures.
Wealso study the distribution of argument reorderingcategories (i.e.,NC, L2R and R2L) in the trainingdata, which is shown in Table 3.
Most arguments,accounting for 82.43%, are on the same side of theirverbal predicates after translation.
The remaining905Features of an argument A for reorderingsrcits verbal predicate Apits semantic role Arits head word Ahthe leftmost word of Athe rightmost word of Atgtthe translation of Apthe translation of Ahthe leftmost word of the translation of Athe rightmost word of the translation of ATable 2: Features adopted in the argument reorderingmodel.Reordering Category PercentNC 82.43%L2R 11.19%R2L 6.38%Table 3: Distribution of argument reordering categoriesin the training data.arguments (17.57%) are moved either from the leftside of their predicates to the right side after transla-tion (accounting for 11.19%) or from the right sideto the left side of their translated predicates (ac-counting for 6.38%).After all features are extracted, we use the maxi-mum entropy toolkit in Section 3.3 to train the maxi-mum entropy classifier as formulated in Eq.
(4).
Weperform 100 iterations of L-BFGS.5 Integrating the Two Models into SMTIn this section, we elaborate how to integrate the twomodels into phrase-based SMT.
In particular, we in-tegrate the models into a phrase-based system whichuses bracketing transduction grammars (BTG) (Wu,1997) for phrasal translation (Xiong et al, 2006).Since the system is based on a CKY-style decoder,the integration algorithms introduced here can beeasily adapted to other CKY-based decoding sys-tems such as the hierarchical phrasal system (Chi-ang, 2007).5.1 Integrating the Predicate TranslationModelIt is straightforward to integrate the predicate trans-lation model into phrase-based SMT (Koehn et al,2003; Xiong et al, 2006).
We maintain wordalignments for each phrase pair in the phrase ta-ble.
Given a source sentence with its predicate-argument structure, we detect all verbal predicatesand load trained predicate translation classifiers forthese verbs.
Whenever a hypothesis covers a newverbal predicate v, we find the target translation efor v through word alignments and then calculate itstranslation probability pt(e|C(v)) according to Eq.
(1).The predicate translation model (as formulated inEq.
(2)) is integrated into the whole log-linear modeljust like the conventional lexical translation modelin phrase-based SMT (Koehn et al, 2003).
Thetwo models are independently estimated but comple-mentary to each other.
While the lexical translationmodel calculates the probability of a verbal predi-cate being translated given its local lexical context,the discriminative predicate translation model is ableto employ both lexical and semantic contexts to pre-dict translations for verbs.5.2 Integrating the Argument ReorderingModelBefore we introduce the integration algorithm forthe argument reordering model, we define twofunctions A and N on a source sentence and itspredicate-argument structure ?
as follows.?
A(i, j, ?
): from the predicate-argument struc-ture ?
, the function finds all predicate-argumentpairs which are completely located within thespan from source word i to j.
For example, inFigure 1, A(3, 6, ?)
= {(>?, ARGM-TMP)}while A(2, 3, ?)
= {}, A(1, 5, ?)
= {} becausethe verbal predicate ?>??
is located outsidethe span (2,3) and (1,5).?
N (i, k, j, ?
): the function finds all predicate-argument pairs that cross the two neighboringspans (i, k) and (k+1, j).
It can be formulatedas A(i, j, ?)?
(A(i, k, ?
)?A(k + 1, j, ?
)).We then define another function Pr to calculatethe argument reordering model probability on all ar-guments which are found by the previous two func-tions A and N as follows.Pr(B) =?A?Bpr(mA|C(A)) (6)906where B denotes either A or N .Following (Chiang, 2007), we describe the algo-rithm in a deductive system.
It is shown in Figure2.
The algorithm integrates the argument reorderingmodel into a CKY-style decoder (Xiong et al, 2006).The item [X, i, j] denotes a BTG node X spanningfrom i to j on the source side.
For notational con-venience, we only show the argument reorderingmodel probability for each item, ignoring all othersub-model probabilities such as the language modelprobability.
The Eq.
(7) shows how we calculate theargument reordering model probability when a lex-ical rule is applied to translate a source phrase c toa target phrase e. The Eq.
(8) shows how we com-pute the argument reordering model probability for aspan (i, j) in a dynamic programming manner whena merging rule is applied to combine its two sub-spans in a straight (X ?
[X1, X2]) or inverted or-der (X ?
?X1, X2?).
We directly use the probabili-ties Pr(A(i, k, ?))
and Pr(A(k + 1, j, ?))
that havebeen already obtained for the two sub-spans (i, k)and (k + 1, j).
In this way, we only need to calcu-late the probability Pr(N (i, k, j, ?))
for predicate-argument pairs that cross the two sub-spans.6 ExperimentsIn this section, we present our experiments onChinese-to-English translation tasks, which aretrained with large-scale data.
The experiments areaimed at measuring the effectiveness of the proposeddiscriminative predicate translation model and argu-ment reordering model.6.1 SetupThe baseline system is the BTG-based phrasal sys-tem (Xiong et al, 2006).
Our training corpora5consist of 3.8M sentence pairs with 96.9M Chinesewords and 109.5M English words.
We ran GIZA++on these corpora in both directions and then appliedthe ?grow-diag-final?
refinement rule to obtain wordalignments.
We then used all these word-alignedcorpora to generate our phrase table.
Our 5-gramlanguage model was trained on the Xinhua sectionof the English Gigaword corpus (306 million words)5The corpora include LDC2004E12, LDC2004T08,LDC2005T10, LDC2003E14, LDC2002E18, LDC2005T06,LDC2003E07 and LDC2004T07.using the SRILM toolkit (Stolcke, 2002) with modi-fied Kneser-Ney smoothing.To train the proposed predicate translation modeland argument reordering model, we first parsed allsource sentences using the Berkeley Chinese parser(Petrov et al, 2006) and then ran the Chinese se-mantic role labeler6 (Li et al, 2010) on all sourceparse trees to annotate semantic roles for all verbalpredicates.
After we obtained semantic roles on thesource side, we extracted features as described inSection 3.2 and 4.2 and used these features to trainour two models as described in Section 3.3 and 4.3.We used the NIST MT03 evaluation test data asour development set, and the NIST MT04, MT05as the test sets.
We adopted the case-insensitiveBLEU-4 (Papineni et al, 2002) as the evaluationmetric.
Statistical significance in BLEU differenceswas tested by paired bootstrap re-sampling (Koehn,2004).6.2 ResultsOur first group of experiments is to investigatewhether the predicate translation model is able toimprove translation accuracy in terms of BLEU andwhether semantic features are useful.
The experi-mental results are shown in Table 4.
From the table,we have the following two observations.?
The proposed predicate translation modelsachieve an average improvement of 0.57 BLEUpoints across the two NIST test sets when allfeatures (lex+sem) are used.
Such an improve-ment is statistically significant (p < 0.01).
Ac-cording to our statistics, there are 5.07 verbalpredicates per sentence in NIST04 and 4.76verbs per sentence in NIST05, which accountfor 18.02% and 16.88% of all words in NIST04and 05 respectively.
This shows that not onlyverbal predicates are semantically important,they also form a major part of the sentences.Therefore, whether verbal predicates are trans-lated correctly or not has a great impact on thetranslation accuracy of the whole sentence 7.6Available at: http://nlp.suda.edu.cn/?jhli/.7The example in Table 6 shows that the translations ofverbs even influences reorderings and translations of neighbor-ing words.907X ?
c/e[X, i, j] : Pr(A(i, j, ?
))(7)X ?
[X1, X2] or ?X1, X2?
[X1, i, k] : Pr(A(i, k, ?))
[X2, k + 1, j] : Pr(A(k + 1, j, ?
))[X, i, j] : Pr(A(i, k, ?))
?
Pr(A(k + 1, j, ?))
?
Pr(N (i, k, j, ?
))(8)Figure 2: Integrating the argument reordering model into a BTG-style decoder.Model NIST04 NIST05Base 35.52 33.80Base+PTM (lex) 35.71+ 34.09+Base+PTM (lex+sem) 36.10++** 34.35++*Table 4: Effects of the proposed predicate translationmodel (PTM).
PTM (lex): predicate translation modelwith lexical features; PTM (lex+sem): predicate transla-tion model with both lexical and semantic features; +/++:better than the baseline (p < 0.05/0.01).
*/**: betterthan Base+PTM (lex) (p < 0.05/0.01).Model NIST04 NIST05Base 35.52 33.80Base+ARM 35.82++ 34.29++Base+ARM+PTM 36.19++ 34.72++Table 5: Effects of the proposed argument reorderingmodel (ARM) and the combination of ARM and PTM.++: better than the baseline (p < 0.01).?
When we integrate both lexical and semanticfeatures (lex+sem) described in Section 3.2, weobtain an improvement of about 0.33 BLEUpoints over the system where only lexical fea-tures (lex) are used.
Such a gain, which is sta-tistically significant, confirms the effectivenessof semantic features.Our second group of experiments is to validatewhether the argument reordering model is capableof improving translation quality.
Table 5 shows theresults.
We obtain an average improvement of 0.4BLEU points on the two test sets over the base-line when we incorporate the proposed argument re-ordering model into our system.
The improvementson the two test sets are both statistically significant(p < 0.01).Finally, we integrate both the predicate translationmodel and argument reordering model into the finalsystem.
The two models collectively achieve an im-provement of up to 0.92 BLEU points over the base-line, which is shown in Table 5.7 AnalysisIn this section, we conduct some case studies toshow how the proposed models improve translationaccuracy by looking into the differences that theymake on translation hypotheses.Table 6 displays a translation example whichshows the difference between the baseline andthe system enhanced with the predicate translationmodel.
There are two verbal predicates ?` /headto?
and ??\/attend?
in the source sentence.
Inorder to get the most appropriate translations forthese two verbal predicates, we should adopt differ-ent ways to translate them.
The former should betranslated as a corresponding verb word or phrasewhile the latter into a preposition word ?for?.
Unfor-tunately, the baseline incorrectly translates the twoverbs.
Furthermore, such translation errors even re-sult in undesirable reorderings of neighboring words?
?|?/Bethlehem and ??g/mass?.
This indi-cates that verbal predicate translation errors maylead to more errors, such as inappropriate reorder-ings or lexical choices for neighboring words.
Onthe contrary, we can see that our predicate transla-tion model is able to help select appropriate wordsfor both verbs.
The correct translations of these twoverbs also avoid incorrect reorderings of neighbor-ing words.Table 7 shows another example to demonstratehow the argument reordering model improve re-orderings.
The verbal predicate ?
?1/carry out?has three arguments, ARG0, ARG-ADV and ARG1.The ARG1 argument should be moved from theright side of the predicate to its left side after trans-lation.
The ARG0 argument can either stay on theleft side or move to right side of the predicate.
Ac-908Base[?Z] &?
` ?|?
?\ [?S?]
?g[thousands of] followers to Mass in Bethlehem [Christmas Eve]Base+PTM[?Z] &?
` ?|?
?\ [?S?]
?g[thousands of] devotees [rushed to] Bethlehem for [Christmas Eve] massRef thousands of worshippers head to Bethlehem for Christmas Midnight massTable 6: A translation example showing the difference between the baseline and the system with the predicate transla-tion model (PTM).
Phrase alignments in the two system outputs are shown with dashed lines.
Chinese words in boldare verbal predicates.PAS [k'?@/J?wX?]
??
?1 [??????]ARG0ARGM-ADVARG1Base[k'?]
@ /J [?wX?]
??
[?1??]
[????
]the more [important consultations] also set disaster [warning system]Base+ARMk' [?
@] /J [?wX?]
[??
?1] [??]
[????
]more [important consultations] on [such a] disaster [warning system] [should be carried out]Ref more important discussions will be held on the disaster warning systemTable 7: A translation example showing the difference between the baseline and the system with the argument re-ordering model (ARM).
The predicate-argument structure (PAS) of the source sentence is also displayed in the firstrow.cording to the phrase alignments of the baseline,we clearly observe three serious translation errors:1) the ARG0 argument is translated into separategroups which are not adjacent on the target side;2) the predicate is not translated at all; and 3) theARG1 argument is not moved to the left side of thepredicate after translation.
All of these 3 errors areavoided in the Base+ARM system output as a re-sult of the argument reordering model that correctlyidentifies arguments and moves them in the right di-rections.8 Conclusions and Future WorkWe have presented two discriminative models toincorporate source side predicate-argument struc-tures into SMT.
The two models have been inte-grated into a phrase-based SMT system and evalu-ated on Chinese-to-English translation tasks usinglarge-scale training data.
The first model is the pred-icate translation model which employs both lexicaland semantic contexts to translate verbal predicates.The second model is the argument reordering modelwhich estimates the direction of argument move-ment relative to its predicate after translation.
Ex-perimental results show that both models are able tosignificantly improve translation accuracy in termsof BLEU score.In the future work, we will extend our predicatetranslation model to translate both verbal and nom-inal predicates.
Nominal predicates also frequentlyoccur in Chinese sentences and thus accurate trans-lations of them are desirable for SMT.
We also wantto address another translation issue of arguments asshown in Table 7: arguments are wrongly translatedinto separate groups instead of a cohesive unit (Wuand Fung, 2009a).
We will build an argument seg-mentation model that follows (Xiong et al, 2011) todetermine whether arguments should be translatedas a unit or not.909ReferencesWilker Aziz, Miguel Rios, and Lucia Specia.
2011.
Shal-low semantic trees for smt.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages316?322, Edinburgh, Scotland, July.
Association forComputational Linguistics.Adam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22(1):39?71.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Pascale Fung, Wu Zhaojun, Yang Yongsheng, and DekaiWu.
2006.
Automatic learning of chinese english se-mantic structure mapping.
In IEEE/ACL 2006 Work-shop on Spoken Language Technology (SLT 2006),Aruba, December.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Human Language Technology Confer-ence of the North American Chapter of the Associationfor Computational Linguistics, pages 58?54, Edmon-ton, Canada, May-June.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP 2004, pages 388?395, Barcelona, Spain, July.Mamoru Komachi and Yuji Matsumoto.
2006.
Phrasereordering for statistical machine translation based onpredicate-argument structure.
In In Proceedings of theInternational Workshop on Spoken Language Trans-lation: Evaluation Campaign on Spoken LanguageTranslation, pages 77?82.Junhui Li, Guodong Zhou, and Hwee Tou Ng.
2010.Joint syntactic and semantic parsing of chinese.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, pages 1108?1117, Uppsala, Sweden, July.
Association for Compu-tational Linguistics.Ding Liu and Daniel Gildea.
2010.
Semantic rolefeatures for machine translation.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics (Coling 2010), pages 716?724, Beijing,China, August.
Coling 2010 Organizing Committee.Arne Mauser, Sas?a Hasan, and Hermann Ney.
2009.
Ex-tending statistical machine translation with discrimi-native and trigger-based lexicon models.
In Proceed-ings of the 2009 Conference on Empirical Methods inNatural Language Processing, pages 210?218, Singa-pore, August.
Association for Computational Linguis-tics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 433?440, Sydney, Aus-tralia, July.
Association for Computational Linguistics.Andreas Stolcke.
2002.
Srilm?an extensible languagemodeling toolkit.
In Proceedings of the 7th Inter-national Conference on Spoken Language Processing,pages 901?904, Denver, Colorado, USA, September.Sriram Venkatapathy and Srinivas Bangalore.
2007.Three models for discriminative machine translationusing global lexical selection and sentence reconstruc-tion.
In Proceedings of SSST, NAACL-HLT 2007 /AMTA Workshop on Syntax and Structure in Statisti-cal Translation, pages 96?102, Rochester, New York,April.
Association for Computational Linguistics.Dekai Wu and Pascale Fung.
2009a.
Can semanticrole labeling improve smt.
In Proceedings of the13th Annual Conference of the EAMT, pages 218?225,Barcelona, May.Dekai Wu and Pascale Fung.
2009b.
Semantic roles forsmt: A hybrid two-pass model.
In Proceedings of Hu-man Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics, Companion Vol-ume: Short Papers, pages 13?16, Boulder, Colorado,June.
Association for Computational Linguistics.Xianchao Wu, Katsuhito Sudoh, Kevin Duh, HajimeTsukada, and Masaaki Nagata.
2011.
Extracting pre-ordering rules from predicate-argument structures.
InProceedings of 5th International Joint Conference onNatural Language Processing, pages 29?37, ChiangMai, Thailand, November.
Asian Federation of Natu-ral Language Processing.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 521?528, Sydney,Australia, July.
Association for Computational Lin-guistics.Deyi Xiong, Min Zhang, and Haizhou Li.
2011.
Amaximum-entropy segmentation model for statisticalmachine translation.
IEEE Transactions on Audio,Speech and Language Processing, 19(8):2494?2505.910Nianwen Xue.
2008.
Labeling chinese predicateswith semantic roles.
Computational Linguistics,34(2):225?255.911
