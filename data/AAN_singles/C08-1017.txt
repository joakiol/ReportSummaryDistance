Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 129?136Manchester, August 2008Latent Morpho-Semantic Analysis:Multilingual Information Retrieval with Character N-Gramsand Mutual InformationPeter A. Chew, Brett W. BaderSandia National LaboratoriesP.
O.
Box 5800Albuquerque, NM 87185, USA{pchew,bwbader}@sandia.govAhmed AbdelaliNew Mexico State UniversityP.O.
Box 30002, Mail Stop 3CRLLas Cruces, NM 88003-8001, USAahmed@crl.nmsu.eduAbstractWe describe an entirely statistics-based,unsupervised, and language-independent approach to multilingualinformation retrieval, which we call La-tent Morpho-Semantic Analysis(LMSA).
LMSA overcomes some of theshortcomings of related previous ap-proaches such as Latent SemanticAnalysis (LSA).
LMSA has an impor-tant theoretical advantage over LSA: itcombines well-known techniques in anovel way to break the terms of LSAdown into units which correspond moreclosely to morphemes.
Thus, it has aparticular appeal for use with morpho-logically complex languages such asArabic.
We show through empirical re-sults that the theoretical advantages ofLMSA can translate into significantgains in precision in multilingual infor-mation retrieval tests.
These gains arenot matched either when a standardstemmer is used with LSA, or whenterms are indiscriminately broken downinto n-grams.1 IntroductionAs the linguistic diversity of textual resourcesincreases, and need for access to those resourcesgrows, there is also greater demand for efficient?
2008.
Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Somerights reserved.information retrieval (IR) methods which aretruly language-independent.
In the ideal but pos-sibly unattainable case, an IR algorithm wouldproduce equally reliable results for any languagepair: for example, a query in English would re-trieve equally good results in Arabic as inFrench.A number of developments in recent yearshave brought that goal more within reach.
One ofthe factors that severely hampered early attemptsat machine translation, for example, was the lackof available computing power.
However,Moore?s Law, the driving force of change incomputing since then, has opened the way forrecent progress in the field, such as StatisticalMachine Translation (SMT) (Koehn et al 2003).Even more closely related to the topic of the pre-sent paper, implementations of the SingularValue Decomposition (SVD) (which is at theheart of LSA), and related algorithms such asPARAFAC2 (Harshman 1972), have becomeboth more widely available and more powerful.SVD, for example, is available in both commer-cial off-the-shelf packages and at least one open-source implementation designed to run on a par-allel cluster (Heroux et  al.
2005).Despite these advances, there are (as yet) notfully surmounted obstacles to working with cer-tain language pairs, particularly when the lan-guages are not closely related.
This isdemonstrated in Chew and Abdelali (2008).
Atleast in part, this has to do with the lexical statis-tics of the languages concerned.
For example,because Arabic has a much richer morphologicalstructure than English and French (meaning isvaried through the addition of prefixes and suf-fixes rather than separate terms such as parti-cles), it has a considerably higher type-to-token129ratio.
Exactly this type of language-specific sta-tistical variation seems to lead to difficulties forstatistics-based techniques such as LSA, as evi-denced by lower cross-language information re-trieval (CLIR) precision for Arabic/English thanfor French/English (Chew and Abdelali 2008).In this paper, we present a strategy for over-coming these difficulties.
In section 2, we outlinethe basic problem and the thinking behind ourapproach: that breaking words down into mor-phemes, or at least morphologically significantsubconstituents, should enable greater inter-language comparability.
This in turn should intheory lead to improved CLIR results.
Severalalternatives for achieving this are considered insection 3.
One of these, a novel combination ofmutual-information-based morphological tokeni-zation (a step beyond simple n-gram tokeniza-tion) and SVD, is what we call LMSA.
Section 4discusses the framework for testing our intui-tions, and the results of these tests are presentedand discussed in section 5.
Finally, we drawsome conclusions and outline possible directionsfor future research in section 6.2 The problemIn many approaches to IR, the underlyingmethod is to represent a corpus as a term-by-document matrix in which each row correspondsto a unique term2, and each column to a docu-ment in the corpus.
The standard LSA frame-work (Deerwester et al 1990) is no different,except that the (sparse) term-by-document matrixX is subjected to SVD,X = USVT (1)where U is a smaller but dense term-by-conceptmatrix, S is a diagonal matrix of singular values,and V is a dense document-by-concept matrix forthe documents used in training.
Effectively, Uand V respectively map the terms and documentsto a single set of arbitrary concepts, such thatsemantically related terms or documents (as de-termined by patterns of co-occurrence) will tendto be similar; similarity is usually measured bytaking the cosine between two (term or docu-ment) vectors.
New documents can also be pro-jected into the LSA ?semantic space?
bymultiplying their document vectors (formed inexactly the same way as the columns for X) by2 Pragmatically, terms can be defined very straightforwardlyin the regular expressions language as sequences of charac-ters delimited by non-word characters.the product US-1, to yield document-by-conceptvectors.
LSA is a completely unsupervised ap-proach to IR in that associations between termssimply fall out when SVD is applied to the data.With cross-language or multilingual LSA, theapproach differs little from that just outlined.
Theonly required modification is in the training data:the term-by-document matrix must be formedfrom a parallel corpus, in which each documentis the combination of text from the parallel lan-guages (as described in Berry et al 1994).Clearly, this IR model cannot be deployed to anylanguages not in the parallel corpus used fortraining SVD.
However, recent work (Chew etal.
2007) shows not only that there is no limit (atleast up to a certain point) to the number of lan-guages that can be processed in parallel, but thatprecision actually increases for given languagepairs as more other languages are included.
Inpractice, the factors which limit the addition ofparallel languages are likely to be computationalpower and the availability of parallel alignedtext.
As noted in section 1, the first of these isless and less of an issue; and regarding the sec-ond, parallel corpora (which are the mainstay ofmany current approaches to computational lin-guistics and IR, particularly in real-world appli-cations) are becoming increasingly available.Substantially all of the Bible, in particular, is al-ready electronically available in at least 40-50languages from diverse language families (BiolaUniversity 2005-2006).Yet, there are clearly variations in how wellCLIR works.
In previous results (Chew et al2007, Chew and Abdelali 2008) it is noticeablein particular that the results for Arabic and Rus-sian (the two most morphologically complexlanguages for which they present results) areconsistently poorer than they are for other lan-guages.
To our knowledge, no solution for thishas been proposed and validated.
Ideally, a solu-tion would both make sense theoretically (or lin-guistically) and be statistics-based rather thanrule-based, consistent with the general frame-work of LSA and other recent developments inthe field, such as SMT, and avoiding the need tobuild a separate grammar for every new language?
an expensive undertaking.Translation Types Tokens RatioEnglish (KJV) 12,335 789,744 1.56%French (Darby) 20,428 812,947 2.51%Spanish (RV 1909) 28,456 704,004 4.04%Russian (Syn 1876) 47,226 560,524 8.43%Arabic (S. Van Dyke) 55,300 440,435 12.56%Table 1.
Lexical statistics in a parallel corpus130To begin to assess the problem, one can com-pare the lexical statistics for the Bible fromChew et al (2007), which should be directlycomparable since they are from a parallel corpus.These are arranged in Table 1 in order of type-to-token ratio.This ordering also corresponds to the orderingof languages on a scale from ?analytic?
to ?syn-thetic?
: meaning is shaped in the former by theuse of particles and word order, and in the latterby inflection and suffixation.
Some examplesillustrating differences between Russian andEnglish in this respect are given in Table 2.English I read you read they readRussian KLMNO KLMNPQR KLMNOMTable 2.
Analytic versus synthetic languagesThe element in Russian, of course, which cor-responds to ?read?
is the stem ?KLMN?, but this isembedded within a larger term.
Hence, in allthree examples, Russian takes one term to ex-press what in English takes two terms.
The sameoccurs (although to a lesser extent) in English, inwhich ?read?
and ?reads?
are treated as distinctterms.
Without any further context (such as sen-tences in which these terms are instantiated), thesimilarity in meaning between ?read?
and ?reads?will be readily apparent to any linguist, simplybecause of the shared orthography and morphol-ogy.
But for an approach like standard LSA inwhich terms are defined simply as distinct enti-ties delimited by non-word characters, the mor-phology is considered immaterial ?
it is invisible.The only way a standard term-based approachcan detect any similarity between ?read?
and?reads?
is through the associations of terms indocuments.
Clearly, then, such an approach op-erates under a handicap.Two unfortunate consequences will inevitablyresult from this.
First, some terms will be treatedas out-of-vocabulary even when at least some ofthe semantics could perhaps have been derivedfrom a part of the term.
For example, if the train-ing corpus contains ?read?
and ?reads?
but not?reading?, valuable information is lost every time?reading?
is encountered in a new document towhich LSA might be deployed.
Secondly, asso-ciations that should be made between in-vocabulary terms will also be missed.
Perhaps areason that more attention has not been devotedto this is that the problem can largely be disre-garded in highly analytic languages like English.But, as previous results such as Chew and Abde-lali?s (2008) show, for a language like Arabic,the adverse consequences of a morphology-blindapproach are more severe.
The question then is:how can information which is clearly available inthe training corpus be more fully leveraged with-out sacrificing efficiency?3 Possible solutions3.1 Replacing terms with n-gramsAt first glance, one might think that stemmingwould be an answer.
Stemming has been shownto improve IR, in particular for morphologicallycomplex languages (recent examples, includingwith Arabic, are Lavie et al 2004 and Abdou etal.
2005).
We are not aware, however, of anyprevious results that show unequivocally thatstemming is beneficial specifically in CLIR.Chew and Abdelali (2008) examine the use of alight stemmer for Arabic (Darwish 2002), andwhile this does result in a small overall increasein overall precision, there is paradoxically noincrease for Arabic.
The problem may be that theapproach for Arabic needs to be matched by asimilar approach for other languages in the paral-lel corpus.
However, since stemmers are usuallytailored to particular languages ?
and may evenbe unavailable for some languages ?
use of exist-ing stemmers may not always be an option.Another more obviously language-independent approach is to replace terms withcharacter n-grams3.
This is feasible for more orless any language, regardless of script.
Moreover,implementation of a similar idea is described inMcNamee and Mayfield (2004) and applied spe-cifically to CLIR.
However, McNamee and May-field?s CLIR results are solely for Europeanlanguages written in the Roman script.
This iswhy they are able to obtain, in their words, ?sur-prisingly good results?
without translation [ofthe query]?, and without using LSA in any form.With related languages in the same script, andparticularly when n-grams are used in place ofterms, the existence of cognates means that manytranslations can easily be identified, since theyprobably share many of the same n-grams (e.g.French ?parisien?
versus English ?Parisian?
).When languages do not all share the same scriptor come from the same language family, how-ever, the task can be considerably harder.Since the approach of n-gram tokenization hasthe advantages of being entirely statistically-3 Hereafter, we use the term ?n-grams?
to refer specificallyto character (not word) n-grams.131based and language-independent, however, weexamined whether it could be combined withLSA to allow CLIR (including cross-script re-trieval), and whether this would lead to any ad-vantage over term-based LSA.
Our intuition wasthat some (although not all) n-grams would cor-respond to morphologically significant subcon-stituents of terms, such as ?read?
from ?reading?,and therefore associations at the morpheme levelmight be facilitated.
The steps for this approachare listed in Table 3.1 Form a term-by-document array from the paral-lel corpus as described above2 For each term, list all (overlapping) n-grams43 Replace terms in the term-by-document arraywith n-grams, to form an n-gram-by-documentarray4 Subject the n-gram-by-document array to SVDto produce an n-gram-by-concept U matrix, sin-gular values (the diagonal S matrix), and docu-ment-by-concept V matrix5 Project new documents into the semantic spaceby multiplying their vectors by US-1Table 3.
Steps for n-gram-based LSAUnder all approaches, we selected the samelog-entropy term weighting scheme that we usedfor standard LSA.
Thus, whether a term t standsfor a wordform or an n-gram, its weighted fre-quency W in a particular document k is given by:W = log2 (F + 1) ?
(1 + Ht / log2 (N))X (2)where F is the raw frequency of t in k, Ht is theentropy of the term or n-gram across all docu-ments, N is the number of documents in the cor-pus, and X is some arbitrary constant (a power towhich the global weight is raised).
We havefound that an X > 1 improves precision by chang-ing the relative distribution of weighted frequen-cies.
Common terms with high entropy becomemuch less influential in the SVD.It should be noted that step (2) in Table 3 issimilar to McNamee and Mayfield?s approach,except that we did not include word-spanning n-grams, owing to computational constraints.
Wealso tried two variants of step (2), one in whichall n-grams were of the same length (as perMcNamee and Mayfield 2004), and one in whichn-grams of different lengths were mixed.
Underthe second of these, the number of rows in boththe term-by-document and U matrices is ofcourse considerably larger.
For example, Table 44 As an example, for ?cat?, the complete list of overlappingn-grams would be ?c?, ?a?, ?t?, ?ca?, ?at?, and ?cat?.shows that the number of rows in the n-gram-by-document matrix for English (EN) under the firstvariant (with n = 6) is 19,801, while under thesecond (with n Z 6) it is 58,907.
Comparable sta-tistics are given for Arabic (AR), Spanish (ES),French (FR) and Russian (RU).n= AR EN ES FR RU1 35 27 41 41 472 939 516 728 708 8273 11,127 4,267 5,563 5,067 7,8084 40,835 13,927 19,686 15,948 30,7025 53,671 20,369 35,526 25,253 54,6476 39,822 19,801 42,408 28,274 65,308Total 146,429 58,907 103,952 75,291 159,339Table 4.
Number of distinct n-grams by languageand length, up to length 6, based on Bible text3.2 Replacing terms with morphemes:LMSAWe also attempted a related approach withnon-overlapping n-grams.
This set ofexperiments was guided by the intuition that notall n-grams are morphologically significant.Before we discuss the details of this approach,consider the English example ?comingle?.
Here,?co?
+ ?mingle?
are likely to be more significantto the overall meaning than ?coming?
+ ?le?
?
infact, the presence of the n-gram ?coming?
couldbe misleading in this case.
One way to model thiswould be to change the weighting scheme.
Theproblem with this is that the weighting for onetoken has to be contingent on the weighting foranother in the same term.
Otherwise, in thisexample, the n-gram ?coming?
would presumablyreceive a high weighting based on its frequencyelsewhere in the corpus.An alternative is to select the tokenizationwhich maximizes mutual information (MI).Brown et al (1992) describe one application ofMI to identify word collocations; Kashioka et al(1998) describe another, based on MI of charac-ter n-grams, for morphological analysis of Japa-nese.
The pointwise MI of a pair s1 and s2 asadjacent symbols isMI = log P(s1 s2) ?
log P(s1) ?
log P(s2) (3)If s1 follows s2 less often than expected on thebasis of their independent frequencies, then MI isnegative; otherwise, it is positive.In our application, we want to consider allcandidate tokenizations, sum MI for each candi-date, and rule out all but one candidate.
A to-132kenization is a candidate if it exhaustively parsesthe entire string and has no overlapping tokens.Thus, for ?comingle?, co+mingle, coming+le,comingle, c+o+m+i+n+g+l+e, etc., are some ofthe candidates, but comi+ngl and com+mingleare not.
To obtain MI, we need to compute thelog probability (logp) of every n-gram in the cor-pus.
If Sk (k = 1, ?, K) denotes the set of all n-grams of length k, and sn is a particular n-gram oflength n, then we compute logp for sn as:logp = log F(sn) ?
log [ (F(Sn))  (4)where F(sn) is the frequency of sn in the corpus,and [ (F(Sn)) is the sum of the frequencies of allSn in the corpus.5 In all cases, logp is negative,and MI is maximized when the magnitude of thesum of logp for all elements in the tokenization(also negative) is minimized, i.e.
closest to zero.Tokenizations consisting of one, two or moreelements (respective examples are comingle,co+mingle, and co+ming+le) will all receive ascore, although those with fewer elements willtend to be favored.We considered some minor variants in the set-tings for this approach in which word-initial andword-final n-grams were indexed separately fromword-medial n-grams.
Guided by McNamee andMayfield?s (2004) finding that there is an optimal(language-dependent) value of k for Sk, we alsovaried the maximum length of n-grams allowedin tokenizations.
Under all settings, we followedsteps 3-5 from Table 3 (including SVD) fromhere on.This approach (which we call latent morpho-semantic analysis), then, is like LSA, except thatthe types and tokens are statistically-derivedmorphemes rather than terms.
Whatever LMSAvariant is used, the underlying approach to mor-phological tokenization is completely language-independent.
Example output is shown in Table 5for wordforms from the Russian lemma]^P_`abNMR_c ?to crawl?, where the commonstem (or at least an approximation thereof) is cor-rectly identified.Wordform Tokenization]^P_`abNOdP`e_c ]^P_`abNO dP`e_c]^P_`abNOdL`L_c ]^P_`abNO dL`L_c]^P_`abNOdL`_c ]^P_`abNO dL`_c]^P_`abNOdLf_c ]^P_`abNO dLf_cTable 5.
Examples of MI-based tokenization5 Note that (4) is closely related to the ?weighted mutualinformation?
measure used in Goldsmith (2001: 172).We do not directly test the accuracy of thesetokenizations.
Rather, measures of CLIR preci-sion (described in section 4) indirectly validateour morphological tokenizations.4 Testing frameworkTo assess our results on a basis comparable withprevious work, we used the same training andtest data as used in Chew et al (2007) and Chewand Abdelali (2008).
The training data consistsof the text of the Bible in 31,226 parallel chunks,corresponding generally to verses, in Arabic,English, French, Russian and Spanish.
The testdata is the text of the Quran in the same 5 lan-guages, in 114 parallel chunks corresponding tosuras (chapters).Questions are sometimes raised as to how rep-resentative the Bible and/or Quran are of modernlanguage.
However, there is little question thatthe number and diversity of parallel languagescovered by the Bible6 is not matched elsewhere(Resnik et al 1999), even by more mainstreamparallel corpora such as Europarl (Koehn 2002)7.The diversity of languages covered is a particu-larly important criterion for our purposes, sincewe would like to look at methods which enhanceretrieval for languages across the analytic-synthetic spectrum.
The Bible also has the ad-vantage of being readily available in electronicform: we downloaded all our data in a tab-delimited, verse-indexed format from the ?Un-bound Bible?
website mentioned above (BiolaUniversity, 2005-2006).In accordance with previous work, we split thetest set into each of the 10 possible language-paircombinations: AR-EN, AR-FR, EN-FR, and soon.
For each language pair and test, 228 distinct?queries?
were submitted ?
each query consistingof one of the 228 sura ?documents?.
To assess theaggregate performance of the framework, weused average precision at 1 document, hereafter?P1?
(1 if the translation of the document rankedhighest, zero otherwise ?
thus, a fairly strictmeasure of precision).
We also measured preci-sion on a basis not used by Chew et al (2007) orChew and Abdelali (2008): multilingual preci-sion at 5 documents (hereafter ?MP5?).
For this,6 At December 31, 2006, complete translations existed in429 languages, and partial translations in 2,426 languages(Bible Society 2007).7 Since the Europarl text is extracted from the proceedingsof the European Parliament, the languages represented aregenerally closely-related to one another (most being Ger-manic or Romance).133each of the 570 documents (114 suras, each in 5languages) is submitted as a query.
The resultsare drawn from the pool of all five languages, soMP5 represents the percentage, on average, ofthe top 5 documents which are translations of thequery.
This measure is still stricter than P1 (thisis a mathematical necessity) because the retrievaltask is harder.
Essentially, MP5 measures howwell similar documents cluster across languages,while P1 measures how reliably document trans-lations are retrieved when the target language isknown.5 Results and DiscussionThe following tables show the results of ourtests.
First, we present in Table 6 the results us-ing standard LSA, in which terms are sequencesof characters delimited by non-word characters.Here, in essence, we reperformed an experimentin Chew and Abdelali (2008).P1 (overall average: 0.8796)AR EN ES FR RUAR 1.0000 0.7544 0.7193 0.7368 0.7544EN 0.7719 1.0000 0.9123 0.9386 0.9474ES 0.6316 0.9298 1.0000 0.9298 0.8947FR 0.7719 0.9035 0.9298 1.0000 0.9386RU 0.7719 0.9298 0.9035 0.9211 1.0000MP5: AR 0.4456, EN 0.7211, ES 0.6649,FR 0.7614, RU 0.6947; overall average: 0.6575Table 6.
Results with standard LSAOur results differ from Chew and Abdelali?s(2008) ?
our precision is higher ?
because we usea different value of X in equation (2) above (here,1.8 rather than 1).
Generally, we selected X so asto maximize MP5; discussion of this is beyondthe scope of this paper, and not strictly relevantin any case, since we present like-for-like com-parisons throughout this section.
However, Table6 shows clearly that our results replicate thosepreviously published, in that precision for Arabic(the most ?synthetic?
of the five languages) isconsistently lower than for the other four.The next set of results (in Table 7) is for LSAwith SVD of an array in which the rows corre-spond to all overlapping, but not word-spanning,n-grams of fixed length.
The best results here, forn=4, are essentially no better on average thanthose obtained with standard LSA.
However,averaging across languages obscures the fact thatresults for Arabic have significantly improved(for example, where Arabic documents are usedas queries, MP5 is now 0.6205 instead of0.4456).
Still, the fact that average MP5 is essen-tially unchanged means that this is at the expenseof results for other languages.n = Average P1 Average MP53 0.8340 0.49514 0.8779 0.67615 0.8232 0.63656 0.6957 0.51977 0.5321 0.3986Table 7.
Results with LSA / overlapping n-gramsof fixed lengthNow we present results in Table 8 where SVDis performed on an array in which the rows cor-respond to all overlapping, but not word-spanning, n-grams of any length (varying maxi-mum length).n Z Average P1 Average MP53 0.8235 0.39094 0.9039 0.62565 0.9095 0.68396 0.8863 0.67167 0.8635 0.6470Table 8.
Results with LSA / overlapping n-gramsof variable lengthHere, the best results (with n<=5) more clearlyimprove upon LSA: the increases in both P1 andMP5, though each only about 0.03 in absoluteterms, are highly significant (p < 0.005).
Verylikely this is related to the fact that when n-gramsare used in place of words, the out-of-vocabularyproblem is alleviated.
But there is quite a highcomputational cost, which will become apparentin Table 10 and the discussion accompanying it.A practical advantage of the ?morpheme?-by-document array of LMSA, on the other hand, isthat this cost is substantially reduced.
This is be-cause, as already mentioned, the vast majority ofn-grams are eliminated from consideration.However, does taking this step significantly hurtperformance?
The results for LMSA presented inTable 9 provide an answer to this.For P1, the results are comparable to standardLSA when we select settings of n Z 7 (maximumpermitted morpheme length) or above.
But underthe stricter MP5 measure, LMSA not only sig-nificantly outperforms standard LSA (p < 0.001,at n Z 9); the results are also superior to thoseobtained under any other method we tested.
Theimprovement in MP5 is comparable to that forP1 ?
0.677 to 0.707 ?
when Chew and Abdelali(2008) use the Darwish Arabic light stemmer toprovide input to LSA; our approach, however,has the advantage that it is fully unsupervised.134n Z Average P1 Average MP54 0.6947 0.44115 0.8151 0.61026 0.8614 0.67937 0.8709 0.69128 0.8663 0.68569 0.8765 0.690910 0.8772 0.6740Table 9.
Results with LMSA8As when n-grams are used without MI, fewertypes are out-of-vocabulary: for example, withcertain settings for LMSA, we found that thepercentage of out-of-vocabulary types droppedfrom 65% under LSA to 29% under LMSA, andthe effect was even more marked for Arabictaken individually (78.5% to 34.4%).
This is de-spite the fact mentioned above that LMSA arraysare more economical than LSA arrays: in fact, asTable 10 shows, 22% more economical (the sizeof the U matrix output by SVD, used to createvectors for new documents, is determined solelyby the number of rows, or types).
Note also thatboth LSA and LMSA are significantly more eco-nomical than SVD with overlapping n-grams.Technique Rows NonzerosLSA 163,745 2,684,938LSA with overlappingn-grams (where n Z 5)527,506 45,878,062LMSA 127,722 3,215,078Table 10.
Comparative matrix sizesEven the results in Table 9 can still be im-proved upon.
Following McNamee and May-field?s insight that different length n-grams maybe optimal for different languages, we attemptedto improve precision further by varying n inde-pendently by language.
For all languages butArabic, n Z 9 seems to work well (either increas-ing or decreasing maximum n resulted in a dropin precision), but by setting n Z 6 for Arabic, P1increased to 0.8874 and MP5 to 0.7368.
As com-parison of Table 11 with Table 6 shows, some ofthe most significant individual increases were forArabic.
It should however be noted that the op-timal value for n may be dataset-dependent.Since n is a maximum length (unlike inMcNamee and Mayfield?s experiments), onemight expect that increasing n should never re-8 These results are with the stipulation that word-initial andword-final n-grams are distinguished from word-medial n-grams.
We also ran experiments in which this distinctionwas not made.
Detailed results are not presented here; suf-fice it to say that when word-medial and other morphemeswere not distinguished, precision was hurt somewhat (low-ering it often by several percentage points).sult in a drop in precision.
We believe the benefitto limiting the size of n is connected to Brown etal.
?s (1992: 470) observation that ?as n increases,the accuracy of an n-gram model increases, butthe reliability of our parameter estimates, drawnas they must be from a limited training text, de-creases?.
Effectively, the probabilities used in MIare unrepresentatively high for longer n-grams(this becomes clear if one considers the extremeexample of an n-gram the same length as thetraining corpus).P1 (overall average: 0.8874)AR EN ES FR RUAR 1.0000 0.7895 0.7719 0.7281 0.7807EN 0.8158 1.0000 0.9298 0.9298 0.9123ES 0.7807 0.9474 1.0000 0.9123 0.8684FR 0.7632 0.9035 0.9474 1.0000 0.8947RU 0.7456 0.9298 0.9298 0.9035 1.0000MP5: AR 0.5140, EN 0.8035, ES 0.8228,FR 0.8035, RU 0.7404; overall average: 0.7368Table 11.
Best results with LMSAIf setting a maximum value for n makes sensein general, the idea of a lower maximum forArabic in particular also seems reasonable sinceArabic words, generally written as they are with-out vowels, contain on average fewer charactersthan the other four languages, and contain rootswhich are usually three or fewer characters long.6 ConclusionIn this paper, we have demonstrated LMSA, alinguistically (specifically, morphologically)more sophisticated alternative to LSA.
By com-puting mutual information of character n-gramsof non-fixed length, we are able to obtain an ap-proximation to a morpheme-by-document matrixwhich can substitute for the commonly-usedterm-by-document matrix.
At the same time, be-cause mutual information is based entirely onstatistics, rather than grammar rules, all the ad-vantages of LSA (language-independence, speedof implementation and fast run-time processing)are retained.
In fact, some of these advantagesmay be increased since the number of indexitems is often lower.Although from a linguist?s point of view thetheoretical advantages of LMSA may be intrinsi-cally satisfying, the benefit is not confined to thetheoretical realm.
Our empirical results show thatLMSA also brings practical benefits, particularlywhen performing IR with morphologically com-plex languages like Arabic.
Principally, thisseems to be due to two factors: alleviation of the135out-of-vocabulary problem and improvement inthe associations made by SVD.We believe that the results we have presentedmay point the way towards still more sophisti-cated types of analysis, particularly for multilin-gual text.
We would like to explore, for example,whether it is possible to use tensor decomposi-tion methods like PARAFAC2 to leverage asso-ciations between n-grams, words, documents andlanguages to still better advantage.Finally, it is worth pointing out that our ap-proach offers an indirect way to test our statis-tics-based approach to morphological analysis.The better our ?morphemes?
correspond to mini-mal semantic units (as theory dictates theyshould), the more coherently our system shouldwork overall.
In this case, our final arbiter of thesystem?s overall performance is CLIR precision.In short, our initial attempts appear to showthat statistics-based morphological analysis canbe integrated into a larger information retrievalarchitecture with some success.AcknowledgementSandia is a multiprogram laboratory operatedby Sandia Corporation, a Lockheed Martin Com-pany, for the United States Department of En-ergy?s National Nuclear Security Administrationunder contract DE-AC04-94AL85000.ReferencesS.
Abdou, P. Ruck, and J. Savoy.
2005.
Evaluation ofStemming, Query Expansion and Manual IndexingApproaches for the Genomic Task.
Proceedings ofTREC 2005.M.
W. Berry, S. T.
Dumais., and G. W. O?Brien.1994.
Using Linear Algebra for Intelligent Infor-mation Retrieval.
SIAM: Review 37, 573-595.Bible Society.
2006.
A Statistical Summary of Lan-guages with the Scriptures.
Accessed Jan 5 2007 athttp://www.biblesociety.org/latestnews/latest341-slr2005stats.html.Biola University.
2005-2006.
The Unbound Bible.Accessed Jan 29 2008 athttp://www.unboundbible.org/.P.
F. Brown, P. V. deSouza, R. L. Mercer, V. J. DellaPietra, and J. C. Lai.
1992.
Class-Based n-gramModels of Natural Language.
Computational Lin-guistics 18(4), 467-479.P.
Chew and A. Abdelali.
2007.
Benefits of the ?Mas-sively Parallel Rosetta Stone?
: Cross-Language In-formation Retrieval with over 30 Languages.Proceedings of the Association for ComputationalLinguistics, 872-879.P.
Chew and A. Abdelali.
2008.
The Effects of Lan-guage Relatedness on Multilingual Information Re-trieval: A Case Study With Indo-European andSemitic Languages.
Proceedings of the Workshopon Cross-Language Information Access.K.
Darwish.
2002.
Building a shallow Arabic morpho-logical analyzer in one day.
Proceedings of the As-sociation for Computational Linguistics, 47-54.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K.Landauer and R. Harshman.
1990.
Indexing by La-tent Semantic Analysis.
Journal of the AmericanSociety for Information Science 41:6, 391-407.J.
Goldsmith.
2001.
Unsupervised Learning of theMorphology of a Natural Language.
Computa-tional Linguistics 27(2), 153-198.R.
A. Harshman.
1972.
PARAFAC2: Mathematicaland Technical Notes.
UCLA Working Papers inPhonetics 22, 30-47.M.
Heroux, R. Bartlett, V. Howle, R. Hoekstra, J. Hu,T.
Kolda, R. Lehoucq, K. Long, R. Pawlowski, E.Phipps, A. Salinger, H. Thornquist, R. Tuminaro, J.Willenbring, A. Williams, and K. Stanley.
2005.An Overview of the Trilinos Project.
ACM Trans-actions on Mathematical Software 31:3, 397-423.H.
Kashioka, Y. Kawata, Y. Kinjo, A. Finch and E.W.
Black.
1998.
Use of Mutual Information BasedCharacter Clusters in Dictionary-less Morphologi-cal Analysis of Japanese.
Proceedings of the 17thInternational Conference on Computational Lin-guistics Vol.
1: 658-662.P.
Koehn, F. J. Och, and D. Marcu.
2003.
StatisticalPhrase-Based Translation.
Proceedings of the JointConference on Human Language Technologies andNAACL, 48-54.P.
Koehn.
2002.
Europarl: a Multilingual Corpus forEvaluation of Machine Translation.
Unpublished.Accessed Jan 29 2008 athttp://www.iccs.inf.ed.ac.uk/~pkoehn/publications/europarl.pdf.A.
Lavie, E. Peterson, K. Probst, S. Wintner, and Y.Eytani.
2004.
Rapid Prototyping of a Transfer-Based Hebrew-to-English Machine TranslationSystem.
Proceedings of the TMI-04.P.
McNamee and J. Mayfield.
2004.
Character N-Gram Tokenization for European Language TextRetrieval.
Information Retrieval 7, 73-97.P.
Resnik, M. Broman Olsen, and M. Diab.
1999.
TheBible as a Parallel Corpus: Annotating the "Bookof 2000 Tongues".
Computers and the Humanities33: 129-153.136
