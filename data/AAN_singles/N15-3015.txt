Proceedings of NAACL-HLT 2015, pages 71?75,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsA Web Application for Automated Dialect AnalysisSravana ReddyNeukom InstituteDartmouth CollegeHanover, NH.sravana@cs.dartmouth.eduJames N. StanfordLinguistics and Cognitive ScienceDartmouth CollegeHanover, NH.james.n.stanford@dartmouth.eduAbstractSociolinguists are regularly faced with thetask of measuring phonetic features fromspeech, which involves manually transcribingaudio recordings ?
a major bottleneck to an-alyzing large collections of data.
We harnessautomatic speech recognition to build an on-line end-to-end web application where usersupload untranscribed speech collections andreceive formant measurements of the vow-els in their data.
We demonstrate this toolby using it to automatically analyze PresidentBarack Obama?s vowel pronunciations.1 IntroductionThere has been recent interest in technologies for theautomated analysis of web-scale corpora in sociolin-guistics, the study of language usage and variation insociety.
The subfield of sociophonetics is concernedwith how certain speech sounds are manifested, giv-ing rise to distinctive speech accents.
While therehave been computational tools developed for socio-phoeticians in the last few years, they require thatthe speech is manually transcribed at the word level,which is painstaking for large corpora.Our insight is that, for many types of record-ings, transcriptions produced by current automaticspeech recognition (ASR) systems are not signifi-cantly worse than manual transcriptions for the pur-pose of measuring certain key phonetic character-istics of speakers, such as their vowel formants ?which are essential to dialect research.We have created an open-access website, DARLA(short for Dartmouth Linguistic Automation)1,where linguists and other researchers working onspeech dialects can upload their data, and receiveautomatic transcriptions of the recordings as well asmeasurements of the speakers?
vowels.
We envisionthis tool being used by linguists for a first-pass quali-tative study of dialect features in speech data withoutthe effort of manual transcription.We choose to implement the system online ratherthan as a downloadable toolkit to eliminate the over-head of program installation for users.
Furthermore,since this is an ongoing project, it is seamless to in-corporate new features in a web application ratherthan pushing updates to a desktop program.
DARLAcurrently supports English speech.Details about our methods as well as studies usingsociolinguistic data appear in Reddy and Stanford(2015).
In this paper, we focus on describing theinterface and an overview of the system components.2 Background2.1 Vowel FormantsEvery vowel sound is associated with a set of reso-nance frequencies, or formants, characteristic to thevowel as well as the speaker.
Sociophoneticians typ-ically study how the first two formants of stressedvowels, denoted by F1and F2, systematically dif-fer across speakers of the language.
For example, asshown in Fig.
1, a speaker saying the vowel EY2(thefirst vowel in paper) with a Southern accent would1http://darla.dartmouth.edu2We use the standard CMU Arpabet phoneme set(http://www.speech.cs.cmu.edu/cgi-bin/cmudict)71have a higher F1and lower F2than a Northern USspeaker for the same vowel.Figure 1: Words and phonemes aligned to speech(represented by its waveform and frequency spec-trogram, visualized in Praat).
The vowel formantsare the dark ?bands?, or local frequency peaks.Northern US SpeakerSouthern US Speaker2.2 MotivationWe observe that the stressed vowel error rate of ourautomatic speech recognition system is about a thirdof the word error rate for several different test cor-pora.
Unlike typical applications of ASR like dicta-tion or command-and-control systems where accu-rate word recognition is the primary objective, per-fect transcription accuracy is not always necessary.For many sociophonetic purposes, it is sufficient toget the vowel correct.
Errors like depend in place ofspend that retain the identity of the stressed vowelaccount for many of the word errors.
Furthermore,with the opportunity to easily analyze speech con-taining several examples of each vowel type, a fewerrors will make little difference to the overall di-alect analysis.3 Existing WorkDARLA is inspired by two online tools used by thephonetics and sociolinguistics communities:1.
FAVE (Rosenfelder et al, 2011), short forForced Alignment Vowel Extraction, takes as input aspeech file along with word-level manual transcrip-tions.
It performs Viterbi alignment of the phonemesin the transcription to the speech using HMM-basedacoustic models.
The locations of vowels are iden-tified from the alignment, and the vowel formantsmeasured at the appropriate locations using LinearPredictive Coding, which in turn is computed by thePraat toolkit for phonetics (Boersma and Weenink,2014).Other programs for phoneme alignment includethe ProsodyLab Aligner (Gorman et al, 2011) andWebMAUS (Kisler et al, 2012).
Recently, Winkel-mann and Raess (2014) developed a web tool forspectral analysis and visualization of speech.The key difference between our system and priorwork is that we do not require any transcriptions forthe input speech.2.
The NORM suite for vowel normalization andplotting (Thomas and Kendall, 2007) lets users up-load formant measurements, and generates scatter-plots of the first two formants.4 System Description4.1 InputFig.
2 is a screenshot of the interface, which is im-plemented in HTML and Javascript, and connectedto the server through CGI and Ajax.
Users uploadtheir speech data and can optionally select param-eters for the ASR decoder.
The options consist ofa dialect-specific acoustic model, and the type ofspeech: free speech or dictation, for which we use ahigh language model scaling factor, or lists of words?
commonly used in sociophonetic research ?
forwhich a lower scaling factor is appropriate.
Oncethe upload is complete, users are prompted to en-ter a speaker ID and sex for each file (Fig.
3), usedas parameters for formant extraction.
The inputs arevalidated and sanitized on the client and server sides.4.2 Back-End ComputationThe system currently contains an HMM-basedspeech recognizer built using the CMU Sphinxtoolkit3, with acoustic and language models that wetrained on a variety of American English speechcorpora (broadcast news and telephone conversa-tions).
We currently have one dialect-specific acous-tic model for Southern speech, trained on portionsof the Switchboard corpus (Godfrey and Holliman,3http://cmusphinx.sourceforge.net72Figure 2: Input interface for the completely automated vowel extraction system.Figure 3: Speaker information prompt.1993).
The feature representation uses 13 MFCCs,deltas, and delta-deltas sampled every 10ms.Long audio files are split into smaller segments,and down-sampled to 16 kHz (or 8 kHz if the orig-inal sampling rate is below 16 kHz).
We use Pock-etSphinx for decoding, and HTK to force-align theoutput transcriptions to produce phoneme-to-audioalignments.
The system then converts the align-ments to TextGrid format4, and uses the formant ex-traction portion of the FAVE code5to measure theformant values for all the vowel tokens in the tran-scriptions.
The processing is distributed over eightCPUs so simultaneous jobs can be supported.Since the transcriptions are likely to contain er-rors, we filter out low-confidence vowel tokensbased on the acoustic likelihood of the word contain-ing that token under the acoustic model.
Previouswork on identifying potential errors in the transcrip-tion suggests using models of duration in addition toacoustic features (Das et al, 2010), which we plan4Conversion was facilitated by the Python TextGrid libraryavailable at http://github.com/kylebgorman/textgrid.py5https://github.com/JoFrhwld/FAVE73to incorporate.
We also filter out function words, un-stressed vowel tokens, and tokens with high formantbandwidths (indicating that the formant values maynot be reliable).
Finally, we generate scatter plots ofthe mean values of the first two formants for eachvowel type using the R vowels package6.4.3 OutputThe results are e-mailed to the user once the taskis completed.
The e-mail includes scatter plots ofthe first two vowel formants for each speaker, andthe complete raw formant data in a CSV file whichis adapted from the output of FAVE.
This file con-tains the raw formant measurements of every vowel,including the unfiltered tokens, the formant band-widths, the phonetic contexts, adjacent words, andother relevant information.Phonetic contexts are particularly important sincemany vowel shift patterns are context-dependent.We separate the phonetic contexts into place, man-ner, and voicing features ?
for example, the soundP would be represented as {place: bilabial, man-ner: stop, and voicing: unvoiced}.
Probabilities arecomputed under the acoustic model for each of thesefeatures.
This allows researchers to discard low-probability contexts, or incorporate the probabilitiesas a gradient measure of the phonetic environment.The e-mail also includes the filtered formant mea-surements formatted in a tab-separated file for inputto the NORM plotting suite in case the user wantsmore plotting options, and the aligned ASR tran-scriptions as TextGrid files, which can be opened byPraat and visualized as in Fig.
1.
The user can thencheck the transcriptions and alignments, make cor-rections as needed, and re-run the formant extractionstep using FAVE for more accurate vowel measure-ments if desired.5 Case Study: Obama?s State of the UnionWe ran the audio of US President Barack Obama?s2015 State of the Union address7through our sys-tem.
The audio of the address is reasonably clean,but the speech is sometimes interrupted by clap-ping sounds and background noise.
The record-ing is a just over an hour long, and contains 67936http://cran.r-project.org/web/packages/vowels7The speech and transcripts are taken fromhttp://www.americanrhetoric.com/barackobamaspeeches.htmwords according to the manual transcript.
The de-coding, alignment, and formant extraction pipelinetakes about 90 minutes to complete.The ASR transcriptions show a 42% word errorrate, and a total stressed vowel error rate of 13%.
Ofthe filtered tokens, the stressed vowel error rate iseven better at 9%.The mean formants from the ASR transcriptionsare similar to the formants extracted from the man-ual text (Fig.
4).
The largest discrepancies are invowels like OY which occur less frequently.Figure 4: Plot of formants averaged over filtered to-kens of stressed vowels.
This plot shows Obama?svowels as exhibited in the 2015 State of the Union,analyzed using ASR as well as manual transcriptionsfor comparison.
This is the scatterplot that the userreceives in the e-mailed output (except that the man-ual transcription results will not be included).lllllllllllllll2000 1800 1600 1400 1200 1000700600500400Vowel SpaceF2F1l Obama_ManualObama_AutomatedIYAYEHAAIHUWAOAHOWEYAEEROYAWUHEHAYAAIHIY UWAOAHOWEYAEUHERAWOYObama?s regional background is often describedas a mix of Hawai?i where he spent most of his child-hood, Kansas (his mother?s home), and Chicagowhere he worked for much of his professional life.Sociolinguists have shown that children usually ac-quire most of their dialect features from peers in thelocal community, not their parents (Labov, 1991).We therefore expect to find influences from Hawai?i74and Chicago, and perhaps also a politician?s ten-dency to appeal to a wider audience: in this case,a general northern US audience.The results in Fig.
4 indicate that Obama has amix of conservative Northern US vowels with someMidland and Southern influences, based on soci-olinguistic dialect descriptions (Labov et al, 2006;Labov, 2007; Eckert, 2008).
(1) In this data, Obama does not show an ad-vanced stage of the Northern Cities Vowel ChainShift (NCS) prevalent in Chicago.
The F1ofObama?s AE vowel is lower than average, which isa prevalent pattern in Chicago, but also in other re-gions of the US.
(2) He shows clear evidence of ?fronting?
(highF2) of the vowels UW (boot) and UH (hood).
Thispattern is common in the West and other regions,and is spreading to the North.
(3) His AO and AA vowels are distinct, which iscommon for Chicago and the Inland North and theSouth, but interestingly, not the West and Hawai?i.
(4) Finally, his AW (bout) is somewhat fronted ?
afeature of the Midland and South.We also analyzed Obama?s previous State of theUnion addresses and found that his vowels have re-mained remarkably stable since 2011.6 Future WorkSince our system is an ongoing project, we willbe rolling out several new features in the upcom-ing months.
We are developing an interface to al-low users to make corrections to the speech recog-nition transcriptions (with low-confidence regionshighlighted), and receive updated formant measure-ments.
In the longer term, we hope to expand be-yond vowel formants by developing phonetic fea-ture classifiers for other dialect variables such asrhoticity, nasality, and prosody.
Finally, since thespeech recognizer is the most vital component ofthe system, we are working on improving the ASRerror rate by incorporating state-of-the-art technolo-gies that use deep neural nets.AcknowledgmentsWe would like to thank Irene Feng for programmingassistance, and the developers of FAVE and NORMfor permission to use their formant measurement andplotting code.
We are grateful for the feedback re-ceived from the sociolinguistics community at theNWAV conference and during pilot testing of theapplication.
The first author was supported by aNeukom Fellowship, and further development of thetool is being supported by a Neukom CompX grant.ReferencesPaul Boersma and David Weenink.
2014.
Praat: doingphonetics by computer [computer program].
Availableat http://www.praat.org/.Rajarshi Das, Jonathan Izak, Jiahong Yuan, and MarkLiberman.
2010.
Forced alignment under adverseconditions.
Unpublished manuscript.Penelope Eckert.
2008.
Where do ethnolects stop?
In-ternational Journal of Bilingualism, 12:25?42.John Godfrey and Edward Holliman.
1993.Switchboard-1 Release 2 LDC97S62.
LinguisticData Consortium, Philadelphia.Kyle Gorman, Jonathan Howell, and Michael Wagner.2011.
Prosodylab-aligner: A tool for forced alignmentof laboratory speech.
Canadian Acoustics, 39(3):192?93.Thomas Kisler, Florian Schiel, and Han Sloetjes.
2012.Signal processing via web services: the use case Web-MAUS.
In Proceedings of Digital Humanities.William Labov, Sharon Ash, and Charles Boberg.
2006.The Atlas of North American English (ANAE).
Mou-ton, Berlin.William Labov.
1991.
Sociolinguistic patterns.
Univer-sity of Pennsylvania Press, Philadelphia.William Labov.
2007.
Transmission and diffusion.
Lan-guage, 83(2):344?387.Sravana Reddy and James N. Stanford.
2015.
Towardcompletely automated vowel extraction: IntroducingDARLA.
Manuscript.
Under review at LinguisticsVanguard.Ingrid Rosenfelder, Josef Fruehwald, Keelan Evanini,Scott Seyfarth, Kyle Gorman, Hilary Prichard,and Jiahong Yuan.
2011.
FAVE (Forced Align-ment and Vowel Extraction) Program Suitev1.2 doi:10.5281/zenodo.12325.
Available athttp://fave.ling.upenn.edu.Erik Thomas and Tyler Kendall.
2007.NORM: The vowel normalization and plot-ting suite [online resource].
Available athttp://ncslaap.lib.ncsu.edu/tools/norm/.Raphael Winkelmann and Georg Raess.
2014.
Introduc-ing a web application for labeling, visualizing speechand correcting derived speech signals.
In Proceedingsof LREC.75
