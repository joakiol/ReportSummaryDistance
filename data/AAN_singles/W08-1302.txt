Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 9?16Manchester, August 2008Exploring an Auxiliary Distribution based approach toDomain Adaptation of a Syntactic Disambiguation ModelBarbara PlankUniversity of GroningenThe NetherlandsB.Plank@rug.nlGertjan van NoordUniversity of GroningenThe NetherlandsG.J.M.van.Noord@rug.nlAbstractWe investigate auxiliary distribu-tions (Johnson and Riezler, 2000) fordomain adaptation of a supervised parsingsystem of Dutch.
To overcome the limitedtarget domain training data, we exploit anoriginal and larger out-of-domain modelas auxiliary distribution.
However, ourempirical results exhibit that the auxiliarydistribution does not help: even when verylittle target training data is available theincorporation of the out-of-domain modeldoes not contribute to parsing accuracy onthe target domain; instead, better resultsare achieved either without adaptation orby simple model combination.1 IntroductionModern statistical parsers are trained on large an-notated corpora (treebanks) and their parametersare estimated to reflect properties of the trainingdata.
Therefore, a disambiguation component willbe successful as long as the treebank it was trainedon is representative for the input the model gets.However, as soon as the model is applied to an-other domain, or text genre (Lease et al, 2006),accuracy degrades considerably.
For example, theperformance of a parser trained on the Wall StreetJournal (newspaper text) significantly drops whenevaluated on the more varied Brown (fiction/non-fiction) corpus (Gildea, 2001).A simple solution to improve performance ona new domain is to construct a parser specificallyc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.for that domain.
However, this amounts to hand-labeling a considerable amount of training datawhich is clearly very expensive and leads to an un-satisfactory solution.
In alternative, techniques fordomain adaptation, also known as parser adap-tation (McClosky et al, 2006) or genre porta-bility (Lease et al, 2006), try to leverage ei-ther a small amount of already existing annotateddata (Hara et al, 2005) or unlabeled data (Mc-Closky et al, 2006) of one domain to parse datafrom a different domain.
In this study we examinean approach that assumes a limited amount of al-ready annotated in-domain data.We explore auxiliary distributions (Johnson andRiezler, 2000) for domain adaptation, originallysuggested for the incorporation of lexical selec-tional preferences into a parsing system.
We gaugethe effect of exploiting a more general, out-of-domain model for parser adaptation to overcomethe limited amount of in-domain training data.
Theapproach is examined on two application domains,question answering and spoken data.For the empirical trials, we use Alpino (van No-ord and Malouf, 2005; van Noord, 2006), a ro-bust computational analyzer for Dutch.
Alpinoemploys a discriminative approach to parse selec-tion that bases its decision on a Maximum Entropy(MaxEnt) model.
Section 2 introduces the MaxEntframework.
Section 3 describes our approach ofexploring auxiliary distributions for domain adap-tation.
In section 4 the experimental design andempirical results are presented and discussed.2 Background: MaxEnt ModelsMaximum Entropy (MaxEnt) models are widelyused in Natural Language Processing (Berger etal., 1996; Ratnaparkhi, 1997; Abney, 1997).
Inthis framework, a disambiguation model is speci-9fied by a set of feature functions describing prop-erties of the data, together with their associatedweights.
The weights are learned during the train-ing procedure so that their estimated value deter-mines the contribution of each feature.
In the taskof parsing, features appearing in correct parses aregiven increasing weight, while features in incorrectparses are given decreasing weight.
Once a modelis trained, it can be applied to parse selection thatchooses the parse with the highest sum of featureweights.During the training procedure, the weights vec-tor is estimated to best fit the training data.
Inmore detail, given m features with their corre-sponding empirical expectation Ep?
[fj] and a de-fault model q0, we seek a model p that has mini-mum Kullback-Leibler (KL) divergence from thedefault model q0, subject to the expected-valueconstraints: Ep[fj] = Ep?
[fj], where j ?
1, ...,m.In MaxEnt estimation, the default model q0isoften only implicit (Velldal and Oepen, 2005) andnot stated in the model equation, since the modelis assumed to be uniform (e.g.
the constant func-tion 1?
(s)for sentence s, where ?
(s) is the set ofparse trees associated with s).
Thus, we seek themodel with minimum KL divergence from the uni-form distribution, which means we search modelp with maximum entropy (uncertainty) subject togiven constraints (Abney, 1997).In alternative, if q0is not uniform then p iscalled a minimum divergence model (accordingto (Berger and Printz, 1998)).
In the statisticalparsing literature, the default model q0that canbe used to incorporate prior knowledge is also re-ferred to as base model (Berger and Printz, 1998),default or reference distribution (Hara et al, 2005;Johnson et al, 1999; Velldal and Oepen, 2005).The solution to the estimation problem of find-ing distribution p, that satisfies the expected-value constraints and minimally diverges fromq0, has been shown to take a specific parametricform (Berger and Printz, 1998):p?
(?, s) =1Z?q0expPmj=1?jfj(?)
(1)with m feature functions, s being the input sen-tence, ?
a corresponding parse tree, and Z?thenormalization equation:Z?=?????q0expPmj=1?jfj(??)
(2)Since the sum in equation 2 ranges over all pos-sible parse trees ??
?
?
admitted by the gram-mar, calculating the normalization constant ren-ders the estimation process expensive or even in-tractable (Johnson et al, 1999).
To tackle thisproblem, Johnson et al (1999) redefine the esti-mation procedure by considering the conditionalrather than the joint probability.P?
(?|s) =1Z?q0expPmj=1?jfj(?)
(3)with Z?as in equation 2, but instead, summingover ??
?
?
(s), where ?
(s) is the set of parsetrees associated with sentence s. Thus, the proba-bility of a parse tree is estimated by summing onlyover the possible parses of a specific sentence.Still, calculating ?
(s) is computationally veryexpensive (Osborne, 2000), because the number ofparses is in the worst case exponential with respectto sentence length.
Therefore, Osborne (2000) pro-poses a solution based on informative samples.
Heshows that is suffices to train on an informativesubset of available training data to accurately es-timate the model parameters.
Alpino implementsthe Osborne-style approach to Maximum Entropyparsing.
The standard version of the Alpino parseris trained on the Alpino newspaper Treebank (vanNoord, 2006).3 Exploring auxiliary distributions fordomain adaptation3.1 Auxiliary distributionsAuxiliary distributions (Johnson and Riezler,2000) offer the possibility to incorporate informa-tion from additional sources into a MaxEnt Model.In more detail, auxiliary distributions are inte-grated by considering the logarithm of the proba-bility given by an auxiliary distribution as an addi-tional, real-valued feature.
More formally, given kauxiliary distributions Qi(?
), then k new auxiliaryfeatures fm+1, ..., fm+kare added such thatfm+i(?)
= logQi(?)
(4)where Qi(?)
do not need to be proper probabilitydistributions, however they must strictly be posi-tive ??
?
?
(Johnson and Riezler, 2000).The auxiliary distributions resemble a referencedistribution, but instead of considering a singlereference distribution they have the advantagethat several auxiliary distributions can be inte-grated and weighted against each other.
John-10son establishes the following equivalence betweenthe two (Johnson and Riezler, 2000; Velldal andOepen, 2005):Q(?)
=k?i=1Qi(?
)?m+i (5)where Q(?)
is the reference distribution andQi(?)
is an auxiliary distribution.
Hence, the con-tribution of each auxiliary distribution is regulatedthrough the estimated feature weight.
In general,a model that includes k auxiliary features as givenin equation (4) takes the following form (Johnsonand Riezler, 2000):P?
(?|s) =?ki=1Qi(?)?m+iZ?expPmj=1?jfj(?)
(6)Due to the equivalence relation in equation (5)we can restate the equation to explicitly show thatauxiliary distributions are additional features1.P?(?|s)=Qki=1[expfm+i(?)]?m+iZ?expPmj=1?jfj(?)(7)=1Z?kYi=1expfm+i(?)??m+iexpPmj=1?jfj(?)
(8)=1Z?expPki=1fm+i(?)??m+iexpPmj=1?jfj(?)(9)=1Z?expPm+kj=1?jfj(?
)with fj(?)
= logQ(?)
for m < j ?
(m + k)(10)3.2 Auxiliary distributions for adaptationWhile (Johnson and Riezler, 2000; van Noord,2007) focus on incorporating several auxiliary dis-tributions for lexical selectional preferences, inthis study we explore auxiliary distributions for do-main adaptation.We exploit the information of the more gen-eral model, estimated from a larger, out-of-domaintreebank, for parsing data from a particular tar-get domain, where only a small amount of train-ing data is available.
A related study is Haraet al (2005).
While they also assume a limitedamount of in-domain training data, their approach1Note that the step from equation (6) to (7) holds by re-stating equation (4) as Qi(?)
= expfm+i(?
)differs from ours in that they incorporate an origi-nal model as a reference distribution, and their es-timation procedure is based on parse forests (Haraet al, 2005; van Noord, 2006), rather than infor-mative samples.
In this study, we want to gaugethe effect of auxiliary distributions, which have theadvantage that the contribution of the additionalsource is regulated.More specifically, we extend the target modelto include (besides the original integer-valued fea-tures) one additional real-valued feature (k=1)2.Its value is defined to be the negative logarithmof the conditional probability given by OUT , theoriginal, out-of-domain, Alpino model.
Hence, thegeneral model is ?merged?
into a single auxiliaryfeature:fm+1= ?logPOUT(?|s) (11)The parameter of the new feature is estimated us-ing the same estimation procedure as for the re-maining model parameters.
Intuitively, our auxil-iary feature models dispreferences of the generalmodel for certain parse trees.
When the Alpinomodel assigns a high probability to a parse candi-date, the auxiliary feature value will be small, closeto zero.
In contrast, a low probability parse tree inthe general model gets a higher feature value.
To-gether with the estimated feature weight expectedto be negative, this has the effect that a low prob-ability parse in the Alpino model will reduce theprobability of a parse in the target domain.3.3 Model combinationIn this section we sketch an alternative approachwhere we keep only two features under the Max-Ent framework: one is the log probability assignedby the out-domain model, the other the log proba-bility assigned by the in-domain model:f1= ?logPOUT(?|s), f2= ?logPIN(?|s)The contribution of each feature is again scaledthrough the estimated feature weights ?1, ?2.We can see this as a simple instantiation of modelcombination.
In alternative, data combination isa domain adaptation method where IN and OUT-domain data is simply concatenated and a newmodel trained on the union of data.
A potential andwell known disadvantage of data combination isthat the usually larger amount of out-domain data2Or alternatively, k ?
1 (see section 4.3.1).11?overwhelms?
the small amount of in-domain data.Instead, Model combination interpolates the twomodels in a linear fashion by scaling their contri-bution.
Note that if we skip the parameter esti-mation step and simply assign the two parametersequal values (equal weights), the method reducesto POUT(?|s) ?
PIN(?|s), i.e.
just multiplyingthe respective model probabilities.4 Experiments and Results4.1 Experimental designThe general model is trained on the Alpino Tree-bank (van Noord, 2006) (newspaper text; approx-imately 7,000 sentences).
For the domain-specificcorpora, in the first set of experiments (section 4.3)we consider the Alpino CLEF Treebank (ques-tions; approximately 1,800 sentences).
In the sec-ond part (section 4.4) we evaluate the approachon the Spoken Dutch corpus (Oostdijk, 2000)(CGN, ?Corpus Gesproken Nederlands?
; spokendata; size varies, ranging from 17 to 1,193 sen-tences).
The CGN corpus contains a variety ofcomponents/subdomains to account for the variousdimensions of language use (Oostdijk, 2000).4.2 Evaluation metricThe output of the parser is evaluated by comparingthe generated dependency structure for a corpussentence to the gold standard dependency structurein a treebank.
For this comparison, we representthe dependency structure (a directed acyclic graph)as a set of named dependency relations.
To com-pare such sets of dependency relations, we countthe number of dependencies that are identical inthe generated parse and the stored structure, whichis expressed traditionally using precision, recalland f-score (Briscoe et al, 2002).Let Dipbe the number of dependencies producedby the parser for sentence i, Digis the number ofdependencies in the treebank parse, and Diois thenumber of correct dependencies produced by theparser.
If no superscript is used, we aggregate overall sentences of the test set, i.e.,:Dp=?iDipDo=?iDioDg=?iDigPrecision is the total number of correct dependen-cies returned by the parser, divided by the over-all number of dependencies returned by the parser(precision = Do/Dp); recall is the number ofcorrect system dependencies divided by the totalnumber of dependencies in the treebank (recall =Do/Dg).
As usual, precision and recall can becombined in a single f-score metric.An alternative similarity score for dependencystructures is based on the observation that for agiven sentence of n words, a parser would be ex-pected to return n dependencies.
In such cases,we can simply use the percentage of correct de-pendencies as a measure of accuracy.
Such a la-beled dependency accuracy is used, for instance,in the CoNLL shared task on dependency parsing(?labeled attachment score?
).Our evaluation metric is a variant of labeleddependency accuracy, in which we do allow forsome discrepancy between the number of returneddependencies.
Such a discrepancy can occur,for instance, because in the syntactic annotationsof Alpino (inherited from the CGN) words cansometimes be dependent on more than a singlehead (called ?secondary edges?
in CGN).
A fur-ther cause is parsing failure, in which case a parsermight not produce any dependencies.
We argueelsewhere (van Noord, In preparation) that a metricbased on f-score can be misleading in such cases.The resulting metric is called concept accuracy, in,for instance, Boros et al (1996).3CA = Do?imax(Dig,Dip)The concept accuracy metric can be characterizedas the mean of a per-sentence minimum of recalland precision.
The resulting CA score thereforeis typically slightly lower than the correspondingf-score, and, for the purposes of this paper, equiv-alent to labeled dependency accuracy.4.3 Experiments with the QA dataIn the first set of experiments we focus on theQuestion Answering (QA) domain (CLEF corpus).Besides evaluating our auxiliary based approach(section 3), we conduct separate baseline experi-ments:?
In-domain (CLEF): train on CLEF (baseline)?
Out-domain (Alpino): train on Alpino?
Data Combination (CLEF+Alpino): train a model onthe combination of data, CLEF ?
Alpino3In previous publications and implementations defini-tions were sometimes used that are equivalent to: CA =Domax(Dg,Dp)which is slightly different; in practice the dif-ferences can be ignored.12Dataset In-dom.
Out-dom.
Data Combination Aux.distribution Model Combinationsize (#sents) CLEF Alpino CLEF+Alpino CLEF+Alpino aux CLEF aux+Alpino aux equal weightsCLEF 2003 (446) 97.01 94.02 97.21 97.01 97.14 97.46CLEF 2004 (700) 96.60 89.88 95.14 96.60 97.12 97.23CLEF 2005 (200) 97.65 87.98 93.62 97.72 97.99 98.19CLEF 2006 (200) 97.06 88.92 95.16 97.06 97.00 96.45CLEF 2007 (200) 96.20 92.48 97.30 96.33 96.33 96.46Table 1: Results on the CLEF test data; underlined scores indicate results > in-domain baseline (CLEF)?
Auxiliary distribution (CLEF+Alpino aux): addingthe original Alpino model as auxiliary feature to CLEF?
Model Combination: keep only two featuresPOUT(?|s) and PIN(?|s).
Two variants: i) estimatethe parameters ?1, ?2(CLEF aux+Alpino aux); ii)give them equal values, i.e.
?1=?2=?1 (equal weights)We assess the performance of all of these mod-els on the CLEF data by using 5-fold cross-validation.
The results are given in table 1.The CLEF model performs significantly betterthan the out-of-domain (Alpino) model, despite ofthe smaller size of the in-domain training data.In contrast, the simple data combination resultsin a model (CLEF+Alpino) whose performance issomewhere in between.
It is able to contribute insome cases to disambiguate questions, while lead-ing to wrong decisions in other cases.However, for our auxiliary based approach(CLEF+Alpino aux) with its regulated contribu-tion of the general model, the results show thatadding the feature does not help.
On most datasetsthe same performance was achieved as by the in-domain model, while on only two datasets (CLEF2005, 2007) the use of the auxiliary feature resultsin an insignificant improvement.In contrast, simple model combination workssurprisingly well.
On two datasets (CLEF 2004and 2005) this simple technique reaches a sub-stantial improvement over all other models.
Ononly one dataset (CLEF 2006) it falls slightly offthe in-domain baseline, but still considerably out-performs data combination.
This is true for bothmodel combination methods, with estimated andequal weights.
In general, the results show thatmodel combination usually outperforms data com-bination (with the exception of one dataset, CLEF2007), where, interestingly, the simplest modelcombination (equal weights) often performs best.Contrary to expectations, the auxiliary based ap-proach performs poorly and could often not evencome close to the results obtained by simple modelcombination.
In the following we will explore pos-sible reasons for this result.Examining possible causes One possible pointof failure could be that the auxiliary feature wassimply ignored.
If the estimated weight would beclose to zero the feature would indeed not con-tribute to the disambiguation task.
Therefore, weexamined the estimated weights for that feature.From that analysis we saw that, compared to theother features, the auxiliary feature got a weightrelatively far from zero.
It got on average a weightof ?0.0905 in our datasets and as such is amongthe most influential weights, suggesting it to be im-portant for disambiguation.Another question that needs to be asked, how-ever, is whether the feature is modeling properlythe original Alpino model.
For this sanity check,we create a model that contains only the singleauxiliary feature and no other features.
The fea-ture?s weight is set to a constant negative value4.The resulting model?s performance is assessed onthe complete CLEF data.
The results (0% columnin table 3) show that the auxiliary feature is indeedproperly modeling the general Alpino model, asthe two result in identical performance.4.3.1 Feature template class modelsIn the experiments so far the general model was?packed?
into a single feature value.
To checkwhether the feature alone is too weak, we exam-ine the inclusion of several auxiliary distributions(k > 1).
Each auxiliary feature we add representsa ?submodel?
corresponding to an actual featuretemplate class used in the original model.
The fea-ture?s value is the negative log-probability as de-fined in equation 11, where OUT corresponds tothe respective Alpino submodel.The current Disambiguation Model of Alpinouses the 21 feature templates (van Noord and Mal-ouf, 2005).
Out of this given feature templates,we create two models that vary in the number ofclasses used.
In the first model (?5 class?
), we cre-ate five (k = 5) auxiliary distributions correspond-ing to five clusters of feature templates.
They are4Alternatively, we may estimate its weight, but as it doesnot have competing features we are safe to assume it constant.13defined manually and correspond to submodels forPart-of-Speech, dependencies, grammar rule ap-plications, bilexical preferences and the remainingAlpino features.
In the second model (?21 class?
),we simply take every single feature template as itsown cluster (k = 21).We test the two models and compare them toour baseline.
The results of this experiment aregiven in table 2.
We see that both the 5 class andthe 21 class model do not achieve any considerableimprovement over the baseline (CLEF), nor overthe single auxiliary model (CLEF+Alpino aux).Dataset (#sents) 5class 21class CLEF+Alpino aux CLEFCLEF2003 (446) 97.01 97.04 97.01 97.01CLEF2004 (700) 96.57 96.60 96.60 96.60CLEF2005 (200) 97.72 97.72 97.72 97.65CLEF2006 (200) 97.06 97.06 97.06 97.06CLEF2007 (200) 96.20 96.27 96.33 96.20Table 2: Results on CLEF including several auxil-iary features corresponding to Alpino submodels4.3.2 Varying amount of training dataOur expectation is that the auxiliary feature is atleast helpful in the case very little in-domain train-ing data is available.
Therefore, we evaluate theapproach with smaller amounts of training data.We sample (without replacement) a specificamount of training instances from the original QAdata files and train models on the reduced train-ing data.
The resulting models are tested with andwithout the additional feature as well as modelcombination on the complete data set by usingcross validation.
Table 3 reports the results of theseexperiments for models trained on a proportion ofup to 10% CLEF data.
Figure 1 illustrates the over-all change in performance.Obviously, an increasing amount of in-domaintraining data improves the accuracy of the models.However, for our auxiliary feature, the results intable 3 show that the models with and without theauxiliary feature result in an overall almost iden-tical performance (thus in figure 1 we depict onlyone of the lines).
Hence, the inclusion of the aux-iliary feature does not help in this case either.
Themodels achieve similar performance even indepen-dently of the available amount of in-domain train-ing data.Thus, even on models trained on very little in-domain training data (e.g.
1% CLEF training data)the auxiliary based approach does not work.
Iteven hurts performance, i.e.
depending on the spe-cific dataset, the inclusion of the auxiliary feature868890929496980  10  20  30  40  50  60CA% training dataVarying amount of training data (CLEF 2004)Aux.distr.
(CLEF+Alp_aux)Out-dom (Alpino)Mod.Comb.
(CLEF_aux+Alpino_aux)Figure 1: Amount of in-domain training data ver-sus concept accuracy (Similar figures result fromthe other CLEF datasets) - note that we depict onlyaux.distr.
as its performance is nearly indistin-guishable from the in-domain (CLEF) baselineresults in a model whose performance lies even be-low the original Alpino model accuracy, for up to acertain percentage of training data (varying on thedataset from 1% up to 10%).In contrast, simple model combination is muchmore beneficial.
It is able to outperform almostconstantly the in-domain baseline (CLEF) andour auxiliary based approach (CLEF+Alpino aux).Furthermore, in contrast to the auxiliary based ap-proach, model combination never falls below theout-of-domain (Alpino) baseline, not even in thecase a tiny amount of training data is available.This is true for both model combinations (esti-mated versus equal weights).We would have expected the auxiliary feature tobe useful at least when very little in-domain train-ing data is available.
However, the empirical re-sults reveal the contrary5.
We believe the reasonfor this drop in performance is the amount of avail-able in-domain training data and the correspondingscaling of the auxiliary feature?s weight.
Whenlittle training data is available, the weight cannotbe estimated reliably and hence is not contributingenough compared to the other features (exempli-fied in the drop of performance from 0% to 1%5As suspected by a reviewer, the (non-auxiliary) featuresmay overwhelm the single auxiliary feature, such that possi-ble improvements by increasing the feature space on such asmall scale might be invisible.
We believe this is not the case.Other studies have shown that including just a few featuresmight indeed help (Johnson and Riezler, 2000; van Noord,2007).
(e.g., the former just added 3 features).140% 1% 5% 10%Dataset no aux = Alp.
no aux +aux m.c.
eq.w.
no aux +aux m.c.
eq.w.
no aux +aux m.c.
eq.w.CLEF2003 94.02 94.02 91.93 91.93 95.59 93.65 93.83 93.83 95.74 95.17 94.80 94.77 95.72 95.72CLEF2004 89.88 89.88 86.59 86.59 90.97 91.06 93.62 93.62 93.42 92.95 94.79 94.82 96.26 95.85CLEF2005 87.98 87.98 87.34 87.41 91.35 89.15 95.90 95.90 97.92 97.52 96.31 96.37 98.19 97.25CLEF2006 88.92 88.92 89.64 89.64 92.16 91.17 92.77 92.77 94.98 94.55 95.04 95.04 95.04 95.47CLEF2007 92.48 92.48 91.07 91.13 95.44 93.32 94.60 94.60 95.63 95.69 94.21 94.21 95.95 95.43Table 3: Results on the CLEF data with varying amount of training datatraining data in table 3).
In such cases it is morebeneficial to just apply the original Alpino modelor the simple model combination technique.4.4 Experiments with CGNOne might argue that the question domain israther ?easy?, given the already high baseline per-formance and the fact that few hand-annotatedquestions are enough to obtain a reasonablemodel.
Therefore, we examine our approach onCGN (Oostdijk, 2000).The empirical results of testing using cross-validation within a subset of CGN subdomainsare given in table 4.
The baseline accuraciesare much lower on this more heterogeneous, spo-ken, data, leaving more room for potential im-provements over the in-domain model.
How-ever, the results show that the auxiliary based ap-proach does not work on the CGN subdomains ei-ther.
The approach is not able to improve even ondatasets where very little training data is available(e.g.
comp-l), thus confirming our previous find-ing.
Moreover, in some cases the auxiliary fea-ture rather, although only slightly, degrades perfor-mance (indicated in italic in table 4) and performsworse than the counterpart model without the ad-ditional feature.Depending on the different characteristics ofdata/domain and its size, the best model adapta-tion method varies on CGN.
On some subdomainssimple model combination performs best, while onothers it is more beneficial to just apply the origi-nal, out-of-domain Alpino model.To conclude, model combination achieves in mostcases a modest improvement, while we haveshown empirically that our domain adaptationmethod based on auxiliary distributions performsjust similar to a model trained on in-domain data.5 ConclusionsWe examined auxiliary distributions (Johnson andRiezler, 2000) for domain adaptation.
Whilethe auxiliary approach has been successfully ap-plied to lexical selectional preferences (Johnsonand Riezler, 2000; van Noord, 2007), our empir-ical results show that integrating a more generalinto a domain-specific model through the auxil-iary feature approach does not help.
The auxil-iary approach needs training data to estimate theweight(s) of the auxiliary feature(s).
When littletraining data is available, the weight cannot be es-timated appropriately and hence is not contributingenough compared to the other features.
This re-sult was confirmed on both examined domains.
Weconclude that the auxiliary feature approach is notappropriate for integrating information of a moregeneral model to leverage limited in-domain data.Better results were achieved either without adapta-tion or by simple model combination.Future work will consist in investigating other pos-sibilities for parser adaptation, especially semi-supervised domain adaptation, where no labeledin-domain data is available.ReferencesAbney, Steven P. 1997.
Stochastic attribute-value grammars.Computational Linguistics, 23:597?618.Berger, A. and H. Printz.
1998.
A comparison of criteriafor maximum entropy / minimum divergence feature selec-tion.
In In Proceedings of the 3nd Conference on Empir-ical Methods in Natural Language Processing (EMNLP),pages 97?106, Granada, Spain.Berger, Adam, Stephen Della Pietra, and Vincent Della Pietra.1996.
A maximum entropy approach to natural languageprocessing.
Computational Linguistics, 22(1):39?72.Boros, M., W. Eckert, F. Gallwitz, G. Go?rz, G. Hanrieder, andH.
Niemann.
1996.
Towards understanding spontaneousspeech: Word accuracy vs. concept accuracy.
In Pro-ceedings of the Fourth International Conference on SpokenLanguage Processing (ICSLP 96), Philadelphia.Briscoe, Ted, John Carroll, Jonathan Graham, and AnnCopestake.
2002.
Relational evaluation schemes.
In Pro-ceedings of the Beyond PARSEVAL Workshop at the 3rd In-ternational Conference on Language Resources and Eval-uation, pages 4?8, Las Palmas, Gran Canaria.Gildea, Daniel.
2001.
Corpus variation and parser perfor-mance.
In Proceedings of the 2001 Conference on Empir-ical Methods in Natural Language Processing (EMNLP).Hara, Tadayoshi, Miyao Yusuke, and Jun?ichi Tsujii.
2005.Adapting a probabilistic disambiguation model of an hpsg15comp-a (1,193) - Spontaneous conversations (?face-to-face?)
comp-b (525) - Interviews with teachers of DutchDataSet no aux + aux Alpino Mod.Comb.
Mod.Comb.
Dataset no aux + aux Alpino Mod.Comb.
Mod.Combeq.weights eq.weightsfn000250 63.20 63.28 62.90 63.91 63.99 fn000081 66.20 66.39 66.45 67.26 66.85fn000252 64.74 64.74 64.06 64.87 64.96 fn000089 62.41 62.41 63.88 64.35 64.01fn000254 66.03 66.00 65.78 66.39 66.44 fn000086 62.60 62.76 63.17 63.59 63.77comp-l (116) - Commentaries/columns/reviews (broadcast) comp-m (267) - Ceremonious speeches/sermonsDataSet no aux + aux Alpino Mod.Comb.
Model.Comb.
Dataset no aux + aux Alpino Mod.Comb.
Mod.Combeq.weights eq.weightsfn000002 67.63 67.63 77.30 76.96 72.40 fn000271 59.25 59.25 63.78 64.94 61.76fn000017 64.51 64.33 66.42 66.30 65.74 fn000298 70.33 70.19 74.55 74.83 72.70fn000021 61.54 61.54 64.30 64.10 63.24 fn000781 72.26 72.37 73.55 73.55 73.04Table 4: Excerpt of results on various CGN subdomains (# of sentences in parenthesis).parser to a new domain.
In Proceedings of the Interna-tional Joint Conference on Natural Language Processing.Johnson, Mark and Stefan Riezler.
2000.
Exploiting auxiliarydistributions in stochastic unification-based grammars.
InProceedings of the first conference on North Americanchapter of the Association for Computational Linguistics,pages 154?161, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.Johnson, Mark, Stuart Geman, Stephen Canon, Zhiyi Chi,and Stefan Riezler.
1999.
Estimators for stochastic?unification-based?
grammars.
In Proceedings of the 37thAnnual Meeting of the ACL.Lease, Matthew, Eugene Charniak, Mark Johnson, and DavidMcClosky.
2006.
A look at parsing and its applications.In Proceedings of the Twenty-First National Conference onArtificial Intelligence (AAAI-06), Boston, Massachusetts,16?20 July.McClosky, David, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the Human Language Technology Conference ofthe NAACL, Main Conference, pages 152?159, New YorkCity, USA, June.
Association for Computational Linguis-tics.Oostdijk, Nelleke.
2000.
The Spoken Dutch Corpus:Overview and first evaluation.
In Proceedings of Sec-ond International Conference on Language Resources andEvaluation (LREC), pages 887?894.Osborne, Miles.
2000.
Estimation of stochastic attribute-value grammars using an informative sample.
In Proceed-ings of the Eighteenth International Conference on Com-putational Linguistics (COLING 2000).Ratnaparkhi, A.
1997.
A simple introduction to maximumentropy models for natural language processing.
Technicalreport, Institute for Research in Cognitive Science, Univer-sity of Pennsylvania.van Noord, Gertjan and Robert Malouf.
2005.
Wide coverageparsing with stochastic attribute value grammars.
Draftavailable from http://www.let.rug.nl/?vannoord.
A prelim-inary version of this paper was published in the Proceed-ings of the IJCNLP workshop Beyond Shallow Analyses,Hainan China, 2004.van Noord, Gertjan.
2006.
At Last Parsing Is NowOperational.
In TALN 2006 Verbum Ex Machina, ActesDe La 13e Conference sur Le Traitement Automatique desLangues naturelles, pages 20?42, Leuven.van Noord, Gertjan.
2007.
Using self-trained bilexicalpreferences to improve disambiguation accuracy.
In Pro-ceedings of the Tenth International Conference on ParsingTechnologies.
IWPT 2007, Prague., pages 1?10, Prague.van Noord, Gertjan.
In preparation.
Learning efficient pars-ing.Velldal, E. and S. Oepen.
2005.
Maximum entropy mod-els for realization ranking.
In Proceedings of MT-Summit,Phuket, Thailand.16
