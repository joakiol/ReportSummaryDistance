Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1127?1136,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsUsing Cross-Entity Inference to Improve Event ExtractionYu Hong     Jianfeng Zhang     Bin Ma     Jianmin Yao     Guodong Zhou     Qiaoming ZhuSchool of Computer Science and Technology, Soochow University, Suzhou City, China{hongy, jfzhang, bma, jyao, gdzhou, qmzhu}@suda.edu.cnAbstractEvent extraction is the task of detecting certainspecified types of events that are mentioned inthe source language data.
The state-of-the-artresearch on the task is transductive inference(e.g.
cross-event inference).
In this paper, wepropose a new method of event extraction bywell using cross-entity inference.
In contrast toprevious inference methods, we regard entity-type consistency as key feature to predict eventmentions.
We adopt this inference method toimprove the traditional sentence-level event ex-traction system.
Experiments show that we canget 8.6% gain in trigger (event) identification,and more than 11.8% gain for argument (role)classification in ACE event extraction.1 IntroductionThe event extraction task in ACE (Automatic Con-tent Extraction) evaluation involves three challeng-ing issues: distinguishing events of different types,finding the participants of an event and determin-ing the roles of the participants.The recent researches on the task show theavailability of transductive inference, such as thatof the following methods: cross-document, cross-sentence and cross-event inferences.
Transductiveinference is a process to use the known instances topredict the attributes of unknown instances.
As anexample, given a target event, the cross-event in-ference can predict its type by well using the re-lated events co-occurred with it within the samedocument.
From the sentence:(1)He left the company.it is hard to tell whether it is a Transport event inACE, which means that he left the place; or anEnd-Position event, which means that he retiredfrom the company.
But cross-event inference canuse a related event ?Then he went shopping?
withinthe same document to identify it as a Transportevent correctly.As the above example might suggest, the avail-ability of transductive inference for event extrac-tion relies heavily on the known evidences of anevent occurrence in specific condition.
However,the evidence supporting the inference is normallyunclear or absent.
For instance, the relation amongevents is the key clue for cross-event inference topredict a target event type, as shown in the infer-ence process of the sentence (1).
But event relationextraction itself is a hard task in Information Ex-traction.
So cross-event inference often suffersfrom some false evidence (viz., misleading by un-related events) or lack of valid evidence (viz., un-successfully extracting related events).In this paper, we propose a new method oftransductive inference, named cross-entity infer-ence, for event extraction by well using the rela-tions among entities.
This method is firstlymotivated by the inherent ability of entity types inrevealing event types.
From the sentences:(2)He left the bathroom.
(3)He left Microsoft.it is easy to identify the sentence (2) as a Transportevent in ACE, which means that he left the place,because nobody would retire (End-Position type)from a bathroom.
And compared to the entities insentence (1) and (2), the entity ?Microsoft?
in (3)would give us more confidence to tag the ?left?event as an End-Position type, because people areused to giving the full name of the place wherethey retired.The cross-entity inference is also motivated bythe phenomenon that the entities of the same typeoften attend similar events.
That gives us a way topredict event type based on entity-type consistency.From the sentence:(4)Obama beats McCain.it is hard to identify it as an Elect event in ACE,which means Obama wins the Presidential Election,1127or an Attack event, which means Obama roughssomebody up.
But if we have the priori knowledgethat the sentence ?Bush beats McCain?
is an Electevent, and ?Obama?
was a presidential contenderjust like ?Bush?
(strict type consistency), we haveample evidence to predict that the sentence (4) isalso an Elect event.Indeed above cross-entity inference for event-type identification is not the only use of entity-typeconsistency.
As we shall describe below, we canmake use of it at all issues of event extraction:y For event type: the entities of the same typeare most likely to attend similar events.
And theevents often use consistent or synonymous trigger.y For event argument (participant): the enti-ties of the same type normally co-occur with simi-lar participants in the events of the same type.y For argument role: the arguments of thesame type, for the most part, play the same roles insimilar events.With the help of above characteristics of entity,we can perform a step-by-step inference in thisorder:y Step 1: predicting event type and labelingtrigger given the entities of the same type.y Step 2: identifying arguments in certain eventgiven priori entity type, event type and trigger thatobtained by step 1.y Step 3: determining argument roles in certainevent given entity type, event type, trigger and ar-guments that obtained by step 1 and step 2.On the basis, we give a blind cross-entity infer-ence method for event extraction in this paper.
Inthe method, we first regard entities as queries toretrieve their related documents from large-scalelanguage resources, and use the global evidencesof the documents to generate entity-type descrip-tions.
Second we determine the type consistency ofentities by measuring the similarity of the type de-scriptions.
Finally, given the priori attributes ofevents in the training data, with the help of the en-tities of the same type, we perform the step-by-stepcross-entity inference on the attributes of testevents (candidate sentences).In contrast to other transductive inference meth-ods on event extraction, the cross-entity inferencemakes every effort to strengthen effects of entitiesin predicting event occurrences.
Thus the inferen-tial process can benefit from following aspects: 1)less false evidence, viz.
less false entity-type con-sistency (the key clue of cross-entity inference),because the consistency can be more precisely de-termined with the help of fully entity-type descrip-tion that obtained based on the related informationfrom Web; 2) more valid evidence, viz.
more enti-ties of the same type (the key references for theinference), because any entity never lack its con-geners.2 Task DescriptionThe event extraction task we addressing is that ofthe Automatic Content Extraction (ACE) evalua-tions, where an event is defined as a specific occur-rence involving participants.
And event extractiontask requires that certain specified types of eventsthat are mentioned in the source language data bedetected.
We first introduce some ACE terminol-ogy to understand this task more easily:y Entity: an object or a set of objects in one ofthe semantic categories of interest, referred to inthe document by one or more (co-referential) entitymentions.y Entity mention: a reference to an entity (typi-cally, a noun phrase).y Event trigger: the main word that most clear-ly expresses an event occurrence (An ACE eventtrigger is generally a verb or a noun).y Event arguments: the entity mentions thatare involved in an event (viz., participants).y Argument roles: the relation of arguments tothe event where they participate.y Event mention: a phrase or sentence withinwhich an event is described, including trigger andarguments.The 2005 ACE evaluation had 8 types of events,with 33 subtypes; for the purpose of this paper, wewill treat these simply as 33 separate event typesand do not consider the hierarchical structureamong them.
Besides, the ACE evaluation plandefines the following standards to determine thecorrectness of an event extraction:y A trigger is correctly labeled if its event typeand offset (viz., the position of the trigger word intext) match a reference trigger.y An argument is correctly identified if its eventtype and offsets match any of the reference argu-ment mentions, in other word, correctly recogniz-ing participants in an event.y An argument is correctly classified if its rolematches any of the reference argument mentions.Consider the sentence:1128(5) It has refused in the last five years to revokethe license of a single doctor for committing medi-cal errors.1The event extractor should detect an End-Position event mention, along with the triggerword ?revoke?, the position ?doctor?, the personwhose license should be revoked, and the time dur-ing which the event happened:Event type End-PositionTrigger revokea single doctor Role=Persondoctor Role=Position Argumentsthe last five years Role=Time-withinTable 1: Event extraction exampleIt is noteworthy that event extraction depends onprevious phases like name identification, entitymention co-reference and classification.
Thereinto,the name identification is another hard task in ACEevaluation and not the focus in this paper.
So weskip the phase and instead directly use the entitylabels provided by ACE.3 Related WorkAlmost all the current ACE event extraction sys-tems focus on processing one sentence at a time(Grishman et al, 2005; Ahn, 2006; Hardyet al2006).
However, there have been several studiesusing high-level information from a wider scope:Maslennikov and Chua (2007) use discoursetrees and local syntactic dependencies in a pattern-based framework to incorporate wider context torefine the performance of relation extraction.
Theyclaimed that discourse information could filter noi-sy dependency paths as well as increasing the reli-ability of dependency path extraction.Finkel et al (2005) used Gibbs sampling, a sim-ple Monte Carlo method used to perform approxi-mate inference in factored probabilistic models.
Byusing simulated annealing in place of Viterbi de-coding in sequence models such as HMMs, CMMs,and CRFs, it is possible to incorporate non-localstructure while preserving tractable inference.They used this technique to augment an informa-tion extraction system with long-distance depend-ency models, enforcing label consistency andextraction template consistency constraints.Ji and Grishman (2008) were inspired from thehypothesis of ?One Sense Per Discourse?
(Ya-1 Selected from the file ?CNN_CF_20030304.1900.02?
inACE-2005 corpus.rowsky, 1995); they extended the scope from asingle document to a cluster of topic-related docu-ments and employed a rule-based approach topropagate consistent trigger classification andevent arguments across sentences and documents.Combining global evidence from related docu-ments with local decisions, they obtained an appre-ciable improvement in both event and eventargument identification.Patwardhan and Riloff (2009) proposed an eventextraction model which consists of two compo-nents: a model for sentential event recognition,which offers a probabilistic assessment of whethera sentence is discussing a domain-relevant event;and a model for recognizing plausible role fillers,which identifies phrases as role fillers based uponthe assumption that the surrounding context is dis-cussing a relevant event.
This unified probabilisticmodel allows the two components to jointly makedecisions based upon both the local evidence sur-rounding each phrase and the ?peripheral vision?.Gupta and Ji (2009) used cross-event informa-tion within ACE extraction, but only for recoveringimplicit time information for events.Liao and Grishman (2010) propose documentlevel cross-event inference to improve event ex-traction.
In contrast to Gupta?s work, Liao do notlimit themselves to time information for events, butrather use related events and event-type consis-tency to make predictions or resolve ambiguitiesregarding a given event.4 MotivationIn event extraction, current transductive inferencemethods focus on the issue that many events aremissing or spuriously tagged because the local in-formation is not sufficient to make a confident de-cision.
The solution is to mine credible evidencesof event occurrences from global information andregard that as priori knowledge to predict unknownevent attributes, such as that of cross-documentand cross-event inference methods.However, by analyzing the sentence-level base-line event extraction, we found that the entitieswithin a sentence, as the most important local in-formation, actually contain sufficient clues forevent detection.
It is only based on the premise thatwe know the backgrounds of the entities before-hand.
For instance, if we knew the entity ?vesu-vius?
is an active volcano, we could easily identify1129the word ?erupt?, which co-occurred with the en-tity, as the trigger of a ?volcanic eruption?
eventbut not that of a ?spotty rash?.In spite of that, it is actually difficult to use anentity to directly infer an event occurrence becausewe normally don?t know the inevitable connectionbetween the background of the entity and the eventattributes.
But we can well use the entities of thesame background to perform the inference.
In de-tail, if we first know entity(a) has the same back-ground with entity(b), and we also know thatentity(a), as a certain role, participates in a specificevent, then we can predict that entity(b) might par-ticiptes in a similar event as the same role.Consider the two sentences2 from ACE corpus:(5) American case for war against Saddam.
(6) Bush should torture the al Qaeda chief op-erations officer.The sentences are two event mentions whichhave the same attributes:Event type AttackTrigger warAmerican Role=Attacker(5)ArgumentsSaddam Role=TargetEvent type AttackTrigger tortureBush Role=Attacker(6)Arguments...Qaeda chief ... Role=TargetTable 2: Cross-entity inference exampleFrom the sentences, we can find that the entities?Saddam?
and ?Qaeda chief?
have the same back-ground (viz., terrorist leader), and they are both thearguments of Attack events as the role of Target.So if we previously know any of the event men-tions, we can infer another one with the help of theentities of the same background.In a word, the cross-entity inference, we pro-posed for event extraction, bases on the hypothesis:Entities of the consistent type normally partici-pate in similar events as the same role.As we will introduce below, some statistical da-ta from ACE training corpus can support the hy-pothesis, which show the consistency of event typeand role in event mentions where entities of thesame type occur.4.1 Entity Consistency and DistributionWithin the ACE corpus, there is a strong entityconsistency: if one entity mention appears in a type2 They are extracted from the files ?CNN_CF_20030305.1900.00-1?
and ?CNN_CF_20030303.1900.06-1?
respectively.of event, other entity mentions of the same typewill appear in similar events, and even use thesame word to trigger the events.
To see this wecalculated the conditional probability (in the ACEcorpus) of a certain entity type appearing in the 33ACE event subtypes.050100150200250Be?BornMarryDivorceInjureDieTransportTransfer?Transfer?Start?OrgMerge?Declare?End?OrgAttackDemonstrMeetPhone?Start?End?NominateElectArrest?JailRelease?Trial?Charge?SueConvictSentenceFineExecuteExtraditeAcquitAppealPardonEvent typeFrequencyPopulation?CenterExplodingAirFigure 1.
Conditional probability of a certain entitytype appearing in the 33 ACE event subtypes (Hereonly the probabilities of Population-Center, Ex-ploding and Air entities as examples)050100150200250PersonPlaceBuyerSellerBeneficiaryPriceArtifactOriginDestinationGiverRecipientMoneyOrgAgentVictimInstrumentEntityAttackerTargetDefendantAdjudicatorProsecutorPlaintiffCrimePositionSentenceVehicleTime?AfterTime?BeforeTime?At?Time?At?EndTime?Time?Time?HoldsTime?RoleFrequencyPopulation?CenterExplodingAirFigure 2.
Conditional probability of an entity typeappearing as the 34 ACE role types (Here only theprobabilities of Population-Center, Exploding andAir entities as examples)As there are 33 event subtypes and 43 entitytypes, there are potentially 33*43=1419 entity-event combinations.
However, only a few of theseappear with substantial frequency.
For example,the Population-Center entities only occur in 4types of event mentions with the conditional prob-ability more than 0.05.
From Table 3, we can findthat only Attack and Transport events co-occurfrequently with Population-Center entities (seeFigure 1 and Table 3).Event Cond.Prob.
Freq.Transport 0.368 197Attack 0.295 158Meet 0.073 39Die 0.069 37Table 3: Events co-occurring with Population-Center with the conditional probability > 0.05Actually we find that most entity types appear inmore restricted event mentions than Population-Center entity.
For example, Air entity only co-occurs with 5 event types (Attack, Transport, Die,Transfer-Ownership and Injure), and Exploding1130entity co-occurs with 4 event types (see Figure 1).Especially, they only co-occur with one or twoevent types with the conditional probability morethan 0.05.Evnt.<=5 5<Evnt.<=10 Evnt.>10Freq.
> 0 24 7 12Freq.
>10 37 4 2Freq.
>50 41 1 1Table 4: Distribution of entity-event combinationcorresponding to different co-occurrence frequencyTable 4 gives the distributions of whole ACEentity types co-occurring with event types.
We canfind that there are 37 types of entities (out of 43 intotal) appearing in less than 5 types of event men-tions when entity-event co-occurrence frequency islarger than 10, and only 2 (e.g.
Individual) appear-ing in more than 10 event types.
And when the fre-quency is larger than 50, there are 41 (95%) entitytypes co-occurring with less than 5 event types.These distributions show the fact that most in-stances of a certain entity type normally participatein events of the same type.
And the distributionsmight be good predictors for event type detectionand trigger determination.Air (Entity type)AttackeventFighter plane (subtype 1):?MiGs?
?enemy planes?
?warplanes?
?alliedaircraft?
?U.S.
jets?
?a-10 tank killer?
?b-1bomber?
?a-10 warthog?
?f-14 aircraft?
?apache helicopter?Spacecraft (subtype 2):?russian soyuz capsule?
?soyuz?Civil aviation (subtype 3):?airliners?
?the airport?
?Hooters Air execu-tive?TransporteventPrivate plane (subtype 4):?Marine One?
?commercial flight?
?privateplane?Table 5: Event types co-occurred with Air entitiesBesides, an ACE entity type actually can be di-vided into more cohesive subtypes according tosimilarity of background of entity, and such a sub-type nearly always co-occur with unique eventtype.
For example, the Air entities can be roughlydivided into 4 subtypes: Fighter plane, Spacecraft,Civil aviation and Private plane, within which theFighter plane entities all appear in Attack eventmentions, and other three subtypes all co-occurwith Transport events (see Table 5).
This consis-tency of entities in a subtype is helpful to improvethe precision of the event type predictor.4.2 Role Consistency and DistributionThe same thing happens for entity-role combina-tions: entities of the same type normally play thesame role, especially in the event mentions of thesame type.
For example, the Population-Centerentities occur in ACE corpus as only 4 role types:Place, Destination, Origin and Entity respectivelywith conditional probability 0.615, 0.289, 0.093,0.002 (see Figure 2).
And They mainly appear inTransport event mentions as Place, and in Attackas Destination.
Particularly the Exploding entitiesonly occur as Instrument and Artifact respectivelywith the probability 0.986 and 0.014.
They almostentirely appear in Attack events as Instrument.Evnt.<=5 5<Evnt.<=10 Evnt.>10Freq.
> 0 32 5 6Freq.
>10 38 3 2Freq.
>50 42 1 0Table 6: Distribution of entity-role combinationcorresponding to different co-occurrence frequencyTable 6 gives the distributions of whole entity-role combinations in ACE corpus.
We can find thatthere are 38 entity types (out of 43 in total) occuras less than 5 role types when the entity-role co-occurrence frequency is larger than 10.
There are42 (98%) when the frequency is larger than 50, andonly 2 (e.g.
Individual) when larger than 10.
Thedistributions show that the instances of an entitytype normally occur as consistent role, which ishelpful for cross-entity inference to predict roles.5 Cross-entity ApproachIn this section we present our approach to usingblind cross-entity inference to improve sentence-level ACE event extraction.Our event extraction system extracts events in-dependently for each sentence, because the defini-tion of event mention constrains them to appear inthe same sentence.
Every sentence that at least in-volves one entity mention will be regarded as acandidate event mention, and a randomly selectedentity mention from the candidate will be the star-ing of the whole extraction process.
For the entitymention, information retrieval is used to mine itsbackground knowledge from Web, and its type isdetermined by comparing the knowledge withthose in training corpus.
Based on the entity type,the extraction system performs our step-by-stepcross-entity inference to predict the attributes of1131the candidate event mention: trigger, event type,arguments, roles and whether or not being an eventmention.
The main frame of our event extractionsystem is shown in Figure 3, which includes bothtraining and testing processes.Figure 3.
The frame of cross-entity inference for event extraction (including training and testing processes)In the training process, for every entity type inthe ACE training corpus, a clustering technique(CLUTO toolkit)3 is used to divide it into differentcohesive subtypes, each of which only contains theentities of the same background.
For instance, theAir entities will be divided into Fighter plane,Spacecraft, Civil aviation, Private plane, etc (seeTable 5).
And for each subtype, we mine eventmentions where this type of entities appear fromACE training corpus, and extract all the wordswhich trigger the events to establish correspondingtrigger list.
Besides, a set of support vector ma-chine (SVM) based classifiers are also trained:y Argument Classifier: to distinguish argumentsof a potential trigger from non-arguments4;y Role Classifier: to classify arguments by ar-gument role;y Reportable-Event Classifier (Trigger Classi-fier): Given entity types, a potential trigger, anevent type, and a set of arguments, to determinewhether there is a reportable event mention.3http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=ADA4395084 It is noteworthy that a sentence may include more than oneevent (more than one trigger).
So it is necessary to distinguisharguments of a potential trigger from that of others.In the test process, for each candidate eventmention, our event extraction system firstly pre-dicts its triggers and event types: given an ran-domly selected entity mention from the candidate,the system determines the entity subtype it belong-ing to and the corresponding trigger list, and thenall non-entity words in the candidate are scannedfor a instance of triggers from the list.
When aninstance is found, the system tags the candidate asthe event type that the most frequently co-occurswith the entity subtype in the events that triggeredby the instance.
Secondly the argument classifier isapplied to the remaining mentions in the candidate;for any argument passing that classifier, the roleclassifier is used to assign a role to it.
Finally, onceall arguments have been assigned, the reportable-event classifier is applied to the candidate; if theresult is successful, this event mention is reported.5.1 Further Division of Entity TypeOne of the most important pretreatments beforeour blind cross-entity inference is to divide theACE entity type into more cohesive subtype.
Thegreater consistency among backgrounds of entitiesin such a subtype might be good to improve theprecision of cross-entity inference.1132For each ACE entity type, we collect all entitymentions of the type from training corpus, and re-gard each such mention as a query to retrieve the50 most relevant documents from Web.
Then weselect 50 key words that the most weighted byTFIDF in the documents to roughly describe back-ground of entity.
After establishing the vectorspace model (VSM) for each entity mention of thetype, we adopt a clustering toolkit (CLUTO) tofurther divide the mentions into different subtypes.Finally, for each subtype, we describe its centroidby using 100 key words which the most frequentlyoccurred in relevant documents of entities of thesubtype.In the test process, for an entity mention in acandidate event mention, we determine its type bycomparing its background against all centroids ofsubtypes in training corpus, and the subtype whosecentroid has the most Cosine similarity with thebackground will be assigned to the entity.
It isnoteworthy that global information from the Webis only used to measure the entity-background con-sistency and not directly in the inference process.Thus our event extraction system actually still per-forms a sentence-level inference based on localinformation.5.2 Cross-Entity InferenceOur event extraction system adopts a step-by-step cross-entity inference to predict event.
As dis-cussed above, the first step is to determine the trig-ger in a candidate event mention and tag its eventtype based on consistency of entity type.
Given thedomain of event mention that restrained by theknown trigger, event type and entity subtype, thesecond step is to distinguish the most probable ar-guments that co-occurring in the domain from thenon-arguments.
Then for each of the arguments,the third step can use the co-occurring argumentsin the domain as important contexts to predict itsrole.
Finally, the inference process determineswhether the candidate is a reportable event men-tion according to a confidence coefficient.
In thefollowing sections, we focus on introducing thethree classifiers: argument classifier, role classifierand reportable-event classifier.5.2.1   Cross-Entity Argument ClassifierFor a candidate event mention, the first stepgives its event type, which roughly restrains thedomain of event mentions where the arguments ofthe candidate might co-occur.
On the basis, givenan entity mention in the candidate and its type (seethe pretreatment process in section 5.1), the argu-ment classifier could predict whether other entitymentions co-occur with it in such a domain, if yes,all the mentions will be the arguments of the can-didate.
In other words, if we know an entity of acertain type participates in some event, we willthink of what entities also should participate in theevent.
For instance, when we know a defendantgoes on trial, we can conclude that the judge, law-yer and witness should appear in court.Argument ClassifierFeature 1: an event type (an event-mention domain)Feature 2: an entity subtypeFeature 3: entity-subtype co-occurrence in domainFeature 4: distance to triggerFeature 5: distances to other argumentsFeature 6: co-occurrence with trigger in clauseRole ClassifierFeature 1 and Feature 2Feature 7: entity-subtypes of argumentsReportable-Event ClassifierFeature 1Feature 8: confidence coefficient of trigger in domainFeature 9: confidence coefficient of role in domainTable 7: Features selected for SVM-based cross-entity classifiersA SVM-based argument classifier is used to de-termine arguments of candidate event mention.Each feature of this classifier is the conjunction of:y The subtype of an entityy The event type we are trying to assign an ar-gument toy A binary indicator of whether this entity sub-type co-occurs with other subtypes in such anevent type (There are 266 entity subtypes, and so266 features for each instance)Some minor features, such as another binary indi-cator of whether arguments co-occur with triggerin the same clause (see Table 7).5.2.2 Cross-Entity Role ClassifierFor a candidate event mention, the argumentsthat given by the second step (argument classifier)provide important contextual information for pre-dicting what role the local entity (also one of thearguments) takes on.
For instance, when citizens(Arg1) co-occur with terrorist (Arg2), most likelythe role of Arg1 is Victim.
On the basis, with thehelp of event type, the prediction might be more1133precise.
For instance, if the Arg1 and Arg2 co-occur in an Attack event mention, we will havemore confidence in the Victim role of Arg1.Besides, as discussed in section 4, entities of thesame type normally take on the same role in simi-lar events, especially when they co-occur with sim-ilar arguments in the events (see Table 2).Therefore, all instances of co-occurrence model{entity subtype, event type, arguments} in trainingcorpus could provide effective evidences for pre-dicting the role of argument in the candidate eventmention.
Based on this, we trained a SVM-basedrole classifier which uses following features:y Feature 1 and Feature 2 (see Table 7)y Given the event domain that restrained by theentity and event types, an indicator of what sub-types of arguments appear in the domain.
(266 en-tity subtypes make 266 features for each instance)5.2.3 Reportable-Event ClassifierAt this point, there are still two issues need to beresolved.
First, some triggers are common wordswhich often mislead the extraction of candidateevent mention, such as ?it?, ?this?, ?what?, etc.These words only appear in a few event mentionsas trigger, but when they once appear in trigger list,a large quantity of noisy sentences will be regardedas candidates because of their commonness in sen-tences.
Second, some arguments might be taggedas more than one role in specific event mentions,but as ACE event guideline, one argument onlytakes on one role in a sentence.
So we need to re-move those with low confidence.A confidence coefficient is used to distinguishthe correct triggers and roles from wrong ones.
Thecoefficient calculate the frequency of a trigger (or arole) appearing in specific domain of event men-tions and that in whole training corpus, then com-bines them to represent its confidence degree, justlike TFIDF algorithm.
Thus, the more typical trig-gers (or roles) will be given high confidence.Based on the coefficient, we use a SVM-basedclassifier to determine the reportable events.
Eachfeature of this classifier is the conjunction of:y An event type (domain of event mentions)y Confidence coefficients of triggers in domainy Confidence coefficients of roles in the domain.6 ExperimentsWe followed Liao (2010)?s evaluation and ran-domly select 10 newswire texts from the ACE2005 training corpus as our development set,which is used for parameter tuning, and then con-duct a blind test on a separate set of 40 ACE 2005newswire texts.
We use the rest of the ACE train-ing corpus (549 documents) as training data for ourevent extraction system.To compare with the reported work on cross-event inference (Liao, 2010) and its sentence-levelbaseline system, we cross-validate our method on10 separate sets of 40 ACE texts, and report theoptimum, worst and mean performances (see Table8) on the data by using Precision (P), Recall (R)and F-measure (F).
In addition, we also report theperformance of two human annotators on 40 ACEnewswire texts (a random blind test set): oneknows the rules of event extraction; the otherknows nothing about it.6.1 Main ResultsFrom the results presented in Table 8, we cansee that using the cross-entity inference, we canimprove the F score of sentence-level event extrac-tion for trigger classification by 8.59%, argumentclassification by 11.86%, and role classification by11.9% (mean performance).
Compared to thecross-event inference, we gains 2.87% improve-ment for argument classification, and 3.81% forrole classification (mean performance).
Especially,our worst results also have better performancesthan cross-event inference.Nonetheless, the cross-entity inference hasworse F score for trigger determination.
As we cansee, the low Recall score weaken its F score (seeTable 8).
Actually, we select the sentence which atleast includes one entity mention as candidateevent mention, but lots of event mentions in ACEnever include any entity mention.
Thus we havemissed some mentions at the starting of inferenceprocess.In addition, the annotator who knows the rulesof event extraction has a similar performance trendwith systems: high for trigger classification, mid-dle for argument classification, and low for roleclassification (see Table 8).
But the annotator whonever works in this field obtains a different trend:higher performance for argument classification.This phenomenon might prove that the step-by-step inference is not the only way to predicateevent mention because human can determine ar-guments without considering triggers and eventtypes.1134PerformanceSystem/Human Trigger (%) Argument (%) Role (%)P R F P R F P R FSentence-level baseline 67.56 53.54 59.74 46.45 37.15 41.29 41.02 32.81 36.46Cross-event inference 68.71 68.87 68.79 50.85 49.72 50.28 45.06 44.05 44.55Cross-entity inference (optimum) 73.4 66.2 69.61 56.96 55.1 56 49.3 46.59 47.9Cross-entity inference (worst) 71.3 64.17 66.1 51.28 50.3 50.78 46.3 44.3 45.28Cross-entity inference (mean) 72.9 64.3 68.33 53.4 52.9 53.15 51.6 45.5 48.36Human annotation 1 (blind) 58.9 59.1 59.0 62.6 65.9 64.2 50.3 57.69 53.74Human annotation 2 (know rules) 74.3 76.2 75.24 68.5 75.8 71.97 61.3 68.8 64.86Table 8: Overall performance on blind test data6.2 Influence of Clustering on InferenceA main part of our blind inference system is theentity-type consistency detection, which reliesheavily on the correctness of entity clustering andsimilarity measurement.
In training, we usedCLUTO clustering toolkit to automatically gener-ate different types of entities based on their back-ground-similarities.
In testing, we use K-nearestneighbor algorithm to determine entity type.Fighter plane (subtype 1 in Air entities):?warplanes?
?allied aircraft?
?U.S.
jets?
?a-10 tank killer?
?b-1 bomber?
?a-10 warthog?
?f-14 aircraft?
?apache heli-copter?
?terrorist?
?Saddam?
?Saddam Hussein?
?Bagh-dad?
?Table 9: Noises in subtype 1 of ?Air?
entities (Theblod fonts are noises)We obtained 129 entity subtypes from trainingset.
By randomly inspecting 10 subtypes, we foundnearly every subtype involves no less than 19.2%noises.
For example, the subtype 1 of ?Air?
in Ta-ble 5 lost the entities of ?MiGs?
and ?enemyplanes?, but involved ?terrorist?, ?Saddam?, etc(See Table 9).
Therefore, we manually clusteredthe subtypes and retry the step-by-step cross-entityinference.
The results (denoted as ?Visible 1?)
areshown in Table 10, within which, we additionallyshow the performance of the inference on therough entity types provided by ACE (denoted as?Visible 2?
), such as the type of ?Air?, ?Popula-tion-Center?, ?Exploding?, etc., which normallycan be divided into different more cohesive sub-types.
And the ?Blind?
in Table 10 denotes theperformances on our subtypes obtained by CLUTO.It is surprised that the performances (see Table10, F-score) on ?Visible 1?
entity subtypes are justa little better than ?Blind?
inference.
So it seemsthat the noises in our blind entity types (CLUTOclusters) don?t hurt the inference much.
But by re-inspecting the ?Visible 1?
subtypes, we found thattheir granularities are not enough small: the 89manual entity clusters actually can be divided intomore cohesive subtypes.
So the improvements ofinference on noise-free ?Visible 1?
subtypes arepartly offset by loss on weakly consistent entitiesin the subtypes.
It can be proved by the poor per-formances on ?Visible 2?
subtypes which are muchmore general than ?Visible 1?.
Therefore, a rea-sonable clustering method is important in our in-ference process.F-score Trigger  Argument RoleBlind 68.33 53.15 48.36Visible 1 69.15 53.65 48.83Visible 2 51.34 43.40 39.95Table 10: Performances on visible VS blind7 Conclusions and Future WorkWe propose a blind cross-entity inference methodfor event extraction, which well uses the consis-tency of entity mention to achieve sentence-leveltrigger and argument (role) classification.
Experi-ments show that the method has better perform-ance than cross-document and cross-eventinferences in ACE event extraction.The inference presented here only considers thehelpfulness of entity types of arguments to roleclassification.
But as a superior feature, contextualroles can provide more effective assistance to roledetermination of local argument.
For instance,when an Attack argument appears in a sentence, aTarget might be there.
So if we firstly identifysimple roles, such as the condition that an argu-ment has only a single role, and then use the rolesas priori knowledge to classify hard ones, may beable to further improve performance.AcknowledgmentsWe thank Ruifang He.
And we acknowledge thesupport of the National Natural Science Founda-tion of China under Grant Nos.
61003152,60970057, 90920004.1135ReferencesDavid Ahn.
2006.
The stages of event extraction.
InProc.
COLING/ACL 2006 Workshop on Annotatingand Reasoning about Time and Events.Sydney, Aus-tralia.Jenny Rose Finkel, Trond Grenager and ChristopherManning.
2005.
Incorporating Non-local Informationinto Information Extraction Systems by Gibbs Sam-pling.
In Proc.
43rd Annual Meeting of the Associa-tion for Computational Linguistics, pages 363?370,Ann Arbor, MI, June.Prashant Gupta and Heng Ji.
2009.
Predicting UnknownTime Arguments based on Cross-Event Propagation.In Proc.
ACL-IJCNLP 2009.Ralph Grishman, David Westbrook and Adam Meyers.2005.
NYU?s English ACE 2005 System Description.In Proc.
ACE 2005 Evaluation Workshop, Gaithers-burg, MD.Hilda Hardy, Vika Kanchakouskaya and Tomek Strzal-kowski.
2006.
Automatic Event Classification UsingSurface Text Features.
In Proc.
AAAI06 Workshop onEvent Extraction and Synthesis.
Boston, MA.Heng Ji and Ralph Grishman.
2008.
Refining EventExtraction through Cross-Document Inference.
InProc.
ACL-08: HLT, pages 254?262, Columbus, OH,June.Shasha Liao and Ralph Grishman.
2010.
Using Docu-ment Level Cross-Event Inference to Improve EventExtraction.
In Proc.
ACL-2010, pages 789-797, Upp-sala, Sweden, July.Mstislav Maslennikov and Tat-Seng Chua.
2007.
AMulti resolution Framework for Information Extrac-tion from Free Text.
In Proc.
45th Annual Meeting ofthe Association of Computational Linguistics, pages592?599, Prague, Czech Republic, June.Siddharth Patwardhan and Ellen Riloff.
2007.
EffectiveInformation Extraction with Semantic Affinity Pat-terns and Relevant Regions.
In Proc.
Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, 2007, pages 717?727, Prague, Czech Re-public, June.Siddharth Patwardhan and Ellen Riloff.
2009.
A UnifiedModel of Phrasal and Sentential Evidence for Infor-mation Extraction.
In Proc.
Conference on EmpiricalMethods in Natural Language Processing 2009,(EMNLP-09).David Yarowsky.
1995.
Unsupervised Word Sense Dis-ambiguation Rivaling Supervised Methods.
In Proc.ACL 1995.
Cambridge, MA.1136
