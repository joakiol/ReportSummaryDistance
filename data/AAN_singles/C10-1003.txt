Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 19?27,Beijing, August 2010Robust Measurement and Comparison of Context Similarity for FindingTranslation PairsDaniel Andrade?, Tetsuya Nasukawa?, Jun?ichi Tsujii?
?Department of Computer Science, University of Tokyo{daniel.andrade, tsujii}@is.s.u-tokyo.ac.jp?IBM Research - Tokyonasukawa@jp.ibm.comAbstractIn cross-language information retrieval itis often important to align words that aresimilar in meaning in two corpora writ-ten in different languages.
Previous re-search shows that using context similar-ity to align words is helpful when nodictionary entry is available.
We sug-gest a new method which selects a sub-set of words (pivot words) associated witha query and then matches these wordsacross languages.
To detect word associa-tions, we demonstrate that a new Bayesianmethod for estimating Point-wise MutualInformation provides improved accuracy.In the second step, matching is done ina novel way that calculates the chance ofan accidental overlap of pivot words us-ing the hypergeometric distribution.
Weimplemented a wide variety of previouslysuggested methods.
Testing in two con-ditions, a small comparable corpora pairand a large but unrelated corpora pair,both written in disparate languages, weshow that our approach consistently out-performs the other systems.1 IntroductionTranslating domain-specific, technical terms fromone language to another can be challenging be-cause they are often not listed in a general dictio-nary.
The problem is exemplified in cross-lingualinformation retrieval (Chiao and Zweigenbaum,2002) restricted to a certain domain.
In this case,the user might enter only a few technical terms.However, jargons that appear frequently in thedata set but not in general dictionaries, impair theusefulness of such systems.
Therefore, variousmeans to extract translation pairs automaticallyhave been proposed.
They use different clues,mainly?
Spelling distance or transliterations, whichare useful to identify loan words (Koehn andKnight, 2002).?
Context similarity, helpful since two wordswith identical meaning are often used in sim-ilar contexts across languages (Rapp, 1999).The first type of information is quite specific; itcan only be helpful in a few cases, and can therebyengender high-precision systems with low recall,as described for example in (Koehn and Knight,2002).
The latter is more general.
It holds formost words including loan words.
Usually thecontext of a word is defined by the words whichoccur around it (bag-of-words model).Let us briefly recall the main idea for usingcontext similarity to find translation pairs.
First,the degree of association between the query wordand all content words is measured with respect tothe corpus at hand.
The same is done for everypossible translation candidate in the target cor-pus.
This way, we can create a feature vectorfor the query and all its possible translation can-didates.
We can assume that, for some contentwords, we have valid translations in a general dic-tionary, which enables us to compare the vectorsacross languages.
We will designate these contentwords as pivot words.
The query and its trans-lation candidates are then compared using theirfeature vectors, where each dimension in the fea-ture vector contains the degree of association to19one pivot word.
We define the degree of associa-tion, as a measurement for finding words that co-occur, or which do not co-occur, more often thanwe would expect by pure chance.1We argue that common ways for comparingsimilarity vectors across different corpora performworse because they assume that degree of associa-tions are very similar across languages and can becompared without much preprocessing.
We there-fore suggest a new robust method including twosteps.
Given a query word, in the first step wedetermine the set of pivots that are all positivelyassociated with statistical significance.
In the sec-ond step, we compare this set of pivots with the setof pivots extracted for a possible translation can-didate.
For extracting positively associated piv-ots, we suggest using a new Bayesian method forestimating the critical Pointwise Mutual Informa-tion (PMI) value.
In the second step, we use anovel measure to compare the sets of extractedpivot words which is based on an estimation ofthe probability that pivot words overlap by purechance.
Our approach engenders statistically sig-nificant improved accuracy for aligning transla-tion pairs, when compared to a variety of previ-ously suggested methods.
We confirmed our find-ings using two very different pairs of comparablecorpora for Japanese and English.In the next section, we review previous relatedwork.
In Section 3 we explain our method indetail, and argue that it overcomes subtle weak-nesses of several previous efforts.
In Section 4, weshow with a series of cross-lingual experimentsthat our method, in some settings, can lead to con-siderable improvement in accuracy.
Subsequentlyin Section 4.2, we analyze our method in contrastto the baseline by giving two examples.
We sum-marize our findings in Section 5.2 Related WorkExtracting context similarity for nouns and thenmatching them across languages to find trans-lation pairs was pioneered in (Rapp, 1999) and(Fung, 1998).
The work in (Chiao and Zweigen-baum, 2002), which can be regarded as a varia-1For example ?car?
and ?tire?
are expected to have a high(positive) degree of association, and ?car?
and ?apple?
is ex-pected to have a high (negative) degree of association.tion of (Fung, 1998), uses tf.idf, but suggests tonormalize the term frequency by the maximumnumber of co-occurrences of two words in the cor-pus.
All this work is closely related to our workbecause they solely consider context similarity,whereas context is defined using a word window.The work in (Rapp, 1999; Fung, 1998; Chiao andZweigenbaum, 2002) will form the baselines forour experiments in Section 4.2 This baseline isalso similar to the baseline in (Gaussier et al,2004), which showed that it can be difficult to beatsuch a feature vector approach.In principle our method is not restricted to howcontext is defined; we could also use, for exam-ple, modifiers and head words, as in (Garera etal., 2009).
Although, we found in a preliminaryexperiment that using a dependency parser to dif-ferentiate between modifiers and head words likein (Garera et al, 2009), instead of a bag-of-wordsmodel, in our setting, actually decreased accuracydue to the narrow dependency window.
How-ever, our method could be combined with a back-translation step, which is expected to improvetranslation quality as in (Haghighi et al, 2008),which performs indirectly a back-translation bymatching all nouns mutually exclusive across cor-pora.
Notably, there also exist promising ap-proaches which use both types of information,spelling distance, and context similarity in a jointframework, see (Haghighi et al, 2008), or (De?jeanet al, 2002) which include knowledge of a the-saurus.
In our work here, we concentrate on theuse of degrees of association as an effective meansto extract word translations.In this application, to measure association ro-bustly, often the Log-Likelihood Ratio (LLR)measurement is suggested (Rapp, 1999; Morin etal., 2007; Chiao and Zweigenbaum, 2002).
Theoccurrence of a word in a document is modeledas a binary random variable.
The LLR measure-ment measures stochastic dependency between2Notable differences are that we neglected word order, incontrast to (Rapp, 1999), as it is little useful to compare itbetween Japanese and English.
Furthermore in contrast to(Fung, 1998) we use only one translation in the dictionary,which we select by comparing the relative frequencies.
Wealso made a second run of the experiments where we man-ually selected the correct translations for the first half of themost frequent pivots ?
Results did not change significantly.20two such random variables (Dunning, 1993), andis known to be equal to Mutual Information that islinearly scaled by the size of the corpus (Moore,2004).
This means it is a measure for how muchthe occurrence of word A makes the occurrenceof word B more likely, which we term positiveassociation, and how much the absence of wordA makes the occurrence of word B more likely,which we term negative association.
However, ourexperiments show that only positive association isbeneficial for aligning words cross-lingually.
Infact, LLR can still be used for extracting posi-tive associations by filtering in a pre-processingstep words with possibly negative associations(Moore, 2005).
Nevertheless a problem whichcannot be easily remedied is that confidence es-timates using LLR are unreliable for small samplesizes (Moore, 2004).
We suggest a more princi-pled approach that measures from the start onlyhow much the occurrence of word A makes theoccurrence of word B more likely, which is des-ignated as Robust PMI.Another point that is common to (Rapp, 1999;Morin et al, 2007; Chiao and Zweigenbaum,2002; Garera et al, 2009; Gaussier et al, 2004)is that word association is compared in a fine-grained way, i.e.
they compare the degree of asso-ciation3 with every pivot word, even when it is lowor exceptionally high.
They suggest as a compar-ison measurement Jaccard similarity, Cosine sim-ilarity, and the L1 (Manhattan) distance.3 Our ApproachWe presume that rather than similarity betweendegree (strength of) of associations, the existenceof common word associations is a more reliablemeasure for word similarity because the degreesof association are difficult to compare for the fol-lowing reasons:?
Small differences in the degree of associa-tion are not statistically significantTaking, for example, two sample sets from3To clarify terminology, where possible, we will try todistinguish between association and degree of association.For example word ?car?
has the association ?tire?, whereasthe degree of association with ?tire?
is a continuous number,like 5.6.the same corpus, we will in general measuredifferent degrees of association.?
Differences in sub-domains / sub-topicsCorpora sharing the same topic can still dif-fer in sub-topics.?
Differences in style or languageDifferences in word usage.
4Other information that is used in vector ap-proaches such as that in (Rapp, 1999) is nega-tive association, although negative association isless informative than positive.
Therefore, if it isused at all, it should be assigned a much smallerweight.Our approach caters to these points, by first de-ciding whether a pivot word is positively associ-ated (with statistical significance) or whether itis not, and then uses solely this information forfinding translation pairs in comparable corpora.
Itis divisible into two steps.
In the first, we use aBayesian estimated PointwiseMutual Information(PMI) measurement to find the pivots that are pos-itively associated with a certain word with highconfidence.
In the second step, we compare twowords using their associated pivots as features.The similarity of feature sets is calculated usingpointwise entropy.
The words for which featuresets have high similarity are assumed to be relatedin meaning.3.1 Extracting positively associated words ?Feature SetsTo measure the degree of positive association be-tween two words x and y, we suggest the useof information about how much the occurrenceof word x makes the occurrence of word y morelikely.
We express this using Pointwise MutualInformation (PMI), which is defined as follows:PMI(x, y) = log p(x, y)p(x) ?
p(y) = logp(x|y)p(x) .Therein, p(x) is the probability that word x oc-curs in a document; p(y) is defined analogously.Furthermore, p(x, y) is the probability that both4For example, ?stop?
is not the only word to describe thefact that a car halted.21words occur in the same document.
A positive as-sociation is given if p(x|y) > p(x).
In relatedworks that use the PMI (Morin et al, 2007), theseprobabilities are simply estimated using relativefrequencies, asPMI(x, y) = logf(x,y)nf(x)nf(y)n,where f(x), f(y) is the document frequencyof word x and word y, and f(x, y) is the co-occurrence frequency; n is the number of docu-ments.
However, using relative frequencies to es-timate these probabilities can, for low-frequencywords, produce unreliable estimates for PMI(Manning and Schu?tze, 2002).
It is therefore nec-essary to determine the uncertainty of PMI esti-mates.
The idea of defining confidence intervalsover PMI values is not new (Johnson, 2001); how-ever, the problem is that exact calculation is verycomputationally expensive if the number of docu-ments is large, in which case one can approximatethe binomial approximation for example with aGaussian, which is, however only justified if nis large and p, the probability of an occurrence,is not close to zero (Wilcox, 2009).
We suggestto define a beta distribution over each probabil-ity of the binary events that word x occurs, i.e.
[x], and analogously [x|y].
It was shown in (Ross,2003) that a Bayesian estimate for Bernoulli trialsusing the beta distribution delivers good credibil-ity intervals5, importantly, when sample sizes aresmall, or when occurrence probabilities are closeto 0.
Therefore, we assume thatp(x|y) ?
beta(?
?x|y, ?
?x|y), p(x) ?
beta(?
?x, ?
?x)where the parameters for the two beta distribu-tions are set to?
?x|y = f(x, y) + ?x|y ,?
?x|y = f(y) ?
f(x, y) + ?x|y , and?
?x = f(x) + ?x, ?
?x = n ?
f(x) + ?x .Prior information related to p(x) and the con-ditional probability p(x|y) can be incorporated5In the Bayesian notation we refer here to credibility in-tervals instead of confidence intervals.by setting the hyper-parameters of the beta-distribtutions.6 These can, for example, belearned from another unrelated corpora pair andthen weighted appropriately by setting ?+ ?.
Forour experiments, we use no information beyondthe given corpora pair; the conditional priors aretherefore set equal to the prior for p(x).
Even ifwe do not know which word x is, we have a notionabout p(x) because Zipf?s law indicates to us thatwe should expect it to be small.
A crude estima-tion is therefore the mean word occurrence proba-bility in our corpus as?
= 1|all words|?x?
{all words}f(x)n .We give this estimate a total weight of one obser-vation.
That is, we set?
= ?
, ?
= 1 ?
?
.From a practical perspective, this can be inter-preted as a smoothing when sample sizes aresmall, which is often the case for p(x|y).
Becausewe assume that p(x|y) and p(x) are random vari-ables, PMI is consequently also a random variablethat is distributed according to a beta distributionratio.7 For our experiments, we apply a generalsampling strategy.
We sample p(x|y) and p(x) in-dependently and then calculate the ratio of timesPMI > 0 to determine P (PMI > 0).8 We willrefer to this method as Robust PMI (RPMI).Finally we can calculate, for any word x, the setof pivot words which have most likely a positiveassociation with word x.
We require that this setbe statistically significant: the probability of oneor more words being not a positive association issmaller than a certain p-value.96The hyper-parameters ?
and ?, can be intuitively inter-preted in terms of document frequency.
For example ?x isthe number of times we belief the word x occurs, and ?x thenumber of times we belief that x does not occur in a corpus.Analogously ?x|y and ?x|y can be interpreted with respectto the subset of the corpus where the word y occurs, insteadof the whole corpus.
Note however, that ?
and ?
do not nec-essarily have to be integers.7The resulting distribution for the general case of a betadistribution ratio was derived in (Pham-Gia, 2000).
Unfortu-nately, it involves the calculation of a Gauss hyper-geometricfunction that is computationally expensive for large n.8For experiments, we used 100, 000 samples for each es-timate of P (PMI > 0).9We set, for all of our experiments, the p-value to 0.01.22As an alternative for determining the probabil-ity of a positive association using P (PMI > 0),we calculate LLR and assume that approximatelyLLR ?
?2 with one degree of freedom (Dunning,1993).
Furthermore, to ensure that only positiveassociation counts, we set the probability to zeroif p(x, y) < p(x) ?
p(y), where the probabilitiesare estimated using relative frequencies (Moore,2005).
We refer to this as LLR(P); lacking thiscorrection, it is LLR.3.2 Comparing Word Feature Sets AcrossCorporaSo far, we have explained a robust means to ex-tract the pivot words that have a positive associa-tion with the query.
The next task is to find a sen-sible way to use these pivots to compare the querywith candidates from the target corpus.
A simplemeans to match a candidate with a query is to seehow many pivots they have in common, i.e.
usingthe matching coefficient (Manning and Schu?tze,2002) to score candidates.
This similarity mea-sure produces a reasonable result, as we will showin the experiment section; however, in our erroranalysis, we found out that this gives a bias tocandidates with higher frequencies, which is ex-plainable as follows.
Assuming that a word A hasa fixed number of pivots that are positively associ-ated, then depending on the sample size?the doc-ument frequency in the corpus?not all of theseare statistically significant.
Therefore, not all truepositive associations are included in the featureset to avoid possible noise.
If the document fre-quency increases, then we can extract more sta-tistically significant positive associations and thecardinality of the feature set increases.
This con-sequently increases the likelihood of having morepivots that overlap with pivots from the query?sfeature set.
For example, imagine two candidatewords A and B, for which feature sets of both in-clude the feature set of the query, i.e.
a completematch, howeverA?s feature set is much larger thanB?s feature set.
In this case, the information con-veyed by having a complete match with the queryword?s feature set is lower in the case of A?s fea-ture set than in case of B?s feature set.
Therefore,we suggest its use as a basis of our similarity mea-sure, the degree of pointwise entropy of having anestimate of m matches, asInformation(m, q, c) = ?
log(P (matches = m)).Therein, P (matches = m) is the likelihood that acandidate word with c pivots has m matches withthe query word, which has q pivots.
Letting w bethe total number of pivot words, we can then cal-culate that the probability that the candidate withc pivots was selected by chanceP (matches = m) =( qm)?
(w?qc?m)(wc) .Note that this probability equals a hypergeometricdistribution.10 The smaller P (matches = m) is,the less likely it is that we obtain m matches bypure chance.
In other words, if P (matches = m)is very small, m matches are more than we wouldexpect to occur by pure chance.11Alternatively, in our experiments, we also con-sider standard similarity measurements (Manningand Schu?tze, 2002) such as the Tanimoto coeffi-cient, which also lowers the score of candidatesthat have larger feature sets.4 ExperimentsIn our experiments, we specifically examine trans-lating nouns, mostly technical terms, which occurin complaints about cars collected by the JapaneseMinistry of Land, Infrastructure, Transport andTourism (MLIT)12, and in complaints about carscollected by the USA National Highway TrafficSafety Administration (NHTSA)13.
We create foreach data collection a corpus for which a doc-ument corresponds to one car customer report-ing a certain problem in free text.
The com-plaints are, in general, only a few sentences long.10` qm?
is the number of possible combinations of pivotswhich the candidate has in common with the query.
There-fore, ` qm??`w?qc?m?
is the number of possible different featuresets that the candidate can have such that it sharesm commonpivots with the query.
Furthermore, `wc?
is the total numberof possible feature sets the candidate can have.11The discussion is simplified here.
It can also be thatP (matches = m) is very small, if there are less occur-rences of m that we would expect to occur by pure chance.However, this case can be easily identified by looking at thegradient of P (matches = m).12http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html13http://www-odi.nhtsa.dot.gov/downloads/index.cfm23To verify whether our results can be generalizedover other pairs of comparable corpora, we ad-ditionally made experiments using two corporaextracted from articles of Mainichi Shinbun, aJapanese newspaper, in 1995 and English articlesfrom Reuters in 1997.
There are two notable dif-ferences between those two pairs of corpora: thecontent is much less comparable, Mainichi re-ports more national news than world news, andsecondly, Mainichi and Reuters corpora are muchlarger than MLIT/NHTSA.14For both corpora pairs, we extracted agold-standard semi-automatically by looking atJapanese nouns and their translations with docu-ment frequency of at least 50 for MLIT/NHTSA,and 100 for Mainichi/Reuters.
As a dictionary weused the Japanese-English dictionary JMDic15.In general, we preferred domain-specific termsover very general terms, i.e.
for example forMLIT/NHTSA the noun ??
?injection?
waspreferred over ????
?installation?.
We ex-tracted 100 noun pairs for MLIT/NHTSA andMainichi/Reuters, each.
Each Japanese nounwhich is listed in the gold-standard forms a querywhich is input into our system.
The resultingranking of the translation candidates is automat-ically evaluated using the gold-standard.
There-fore, synonyms that are not listed in the gold stan-dard are not recognized, engendering a conserva-tive estimation of the translation accuracy.
Be-cause all methods return a ranked list of trans-lation candidates, the accuracy is measured us-ing the rank of the translation listed in the gold-standard.16 The Japanese corpora are prepro-cessed with MeCab (Kudo et al, 2004); the En-glish corpora with Stepp Tagger (Tsuruoka et al,2005) and Lemmatizer (Okazaki et al, 2008).
Asa dictionary we use the Japanese-English dictio-nary JMDic17.
In line with related work (Gaussieret al, 2004), we remove a word pair (Japanesenoun s, English noun t) from the dictionary, if soccurs in the gold-standard.
Afterwards we define14MLIT/MLIT has each 20,000 documents.Mainichi/Reuters corpora 75,935 and 148,043 documents,respectively.15http://www.csse.monash.edu.au/ jwb/edict doc.html16In cases for which there are several translations listed forone word, the rank of the first is used.17http://www.csse.monash.edu.au/ jwb/edict doc.htmlthe pivot words by consulting the remaining dic-tionary.4.1 Crosslingual ExperimentWe compare our approach used for extract-ing cross-lingual translation pairs against severalbaselines.
We compare to LLR + Manhattan(Rapp, 1999) and our variation LLR(P) + Man-hattan.
Additionally, we compare TFIDF(MSO)+ Cosine, which is the TFIDF measure, whereasthe Term Frequency is normalized using the max-imal word frequency and the cosine similarityfor comparison suggested in (Fung, 1998).
Fur-thermore, we implemented two variations of this,TFIDF(MPO) + Cosine and TFIDF(MPO) + Jac-card coefficient, which were suggested in (Chiaoand Zweigenbaum, 2002).
In fact, TFIDF(MPO)is the TFIDF measure, whereas the Term Fre-quency is normalized using the maximal word pairfrequency.
The results are displayed in Figure 1.Our approach clearly outperforms all baselines;notably it has Top 1 accuracy of 0.14 and Top 20accuracy of 0.55, which is much better than thatfor the best baseline, which is 0.11 and 0.44, re-spectively.experiment that are similar to those of our cross-lingual experi ent, we use the same pivot wordsand the same gold standard as that used for theMLIT/NHTSA experiments, for which a pair (A,translation of A) is changed to (A, A): that is, theword becomes the translation of itself.
The resultof the monolingual experiment in Table 2 showsthat our method performs slightly worse than thebaseline, LLR + Manhattan, i.e.
LLR with L1 nor-malization and L1 distance(Rapp, 1999).
Further-more, LLR(P) + Manhattan using only positive as-sociations also performs slightly worse.Top 1 Top 10 Top 20LLR + Manhattan 0.94 0.99 0.99LLR(P) + Man attan 0.89 1.0 1.0RPMI + Entropy 0.79 0.94 0.95Table 2: Monolingual NHTSA experiment.In our main experiment, we compare our ap-proach used for extracting cross-lingual transla-ti n pairs ag inst seve al baselines.
As before,we compare LLR + Manhattan (Rapp, 1999) andthe variation LLR(P) + Manhattan.
Addition-ally, we compare TFIDF(MSO) + Cosine, whichis the TFIDF measure, whereas the Term Fre-quency is normalized using the maximal word fre-quency and the cosine similarity for comparisonsuggested in (Fung, 1998).
Furthermore, we im-plemented two variations of this, TFIDF(MPO) +Cosine and TFIDF(MPO) + Jaccard coefficient,which were suggested in (Chiao and Zweigen-baum, 2002).
In fact, TFIDF(MPO) is the TFIDFmeasure, whereas the Term Frequency is normal-ized using the maximal word pair frequency.14The results are displayed in Figure 1.
Our ap-proach clearly outperforms all baselines; notablyit has top 1 accuracy of 0.14 and top 20 accuracyof 0.55, which is much better than that for the bestbaseline, which is 0.11 and 0.44, respectively.We next leave the proposed framework con-stant, but change the mode of estimating positiveassociations and the way to match feature sets.As alternatives for estimating the probability thatthere is a positive association, we test LLR(P) andLLR.
As alternatives for comparing feature sets,we investigate the matching coefficient (match-ing), cosine similarity (cosine), Tanimoto coeffi-14We tried, like originally suggested, using maximumcount of every occurring word pair, i.e.
(content word, con-tent word), but using maximum of all pairs (content word,pivot word) improves always slightly accuracy.
Therefore forwe chose the latter as a baseline.??????????????
?
??
??
??
????????????????????????????????????????????????????
?Figure 1: Percentile ranking of our approachRPMI + Entropy against various previous sug-gested methods.cient (tani), and overlap coefficient (over) (Man-ning and Schu?tze, 2002).
The result of every com-bination is displayed concisely in Table 3 using themedian rank.
In our experience, the median rankis a good choice of measure of location for ourproblem because we have, in general, a skeweddistribution over the ranks.
The cases in whichthe median ranks are close to RPMI + entropy aremagnified in 4.It is readily apparent that most alternatives per-form clearly worse.
Looking at Table 4, we cansee that only RPMI + Entropy, and LLR(P) +Entropy, perform similar.
Pointwise entropy in-creases the accuracy (Top 1) over the matchingcoefficient and is clearly superior to other similar-ity measures.
Overlap similarity performs well incontrast to other standard measurements becauseother measures punish words with a high numberof associated pivots too severely.
However, ourapproach of using pointwise entropy as a measureof similarity performs best because it more ade-quately punishes words with a high number of as-sociated pivots.
Finally, LLR(P) presents a clearedge over LLR, which suggests that indeed onlypositive associations seem to matter in a cross-lingual setting.Entropy Matching Cosine Tani OverRPMI 13.0 17.0 24.0, 37.5 36.0LLR(P) 16.0 15.0 22.5 34.0 25.5LLR 23.5 22.0 27.5 50.5 50.0Table 3: Evaluation MatrixFinally, we aim to clarify whether these re-sults are specific to a certain type of compara-ble corpora pair or if they hold more generally.Therefore, we conduct the same experiments us-ing the very different comparable corpora pairMainichi/Reuters.
When comparing to the bestFigure 1: Crosslingual ExperimentMLIT/NHTSA ?
Percentile Ranking of RPMI+ Entropy Against Various Previous SuggestedMethods.We next leave the proposed framework con-stant, but change the mode of estimating posi-tive associations and the way to match featuresets.
As alternatives for estimating the proba-bility that there is a positive association, we testLLR(P) and LLR.
As alternatives for comparingfeature sets, we investigate the matching coef-ficient (match), cosine similarity (cosine), Tan-imoto coefficient (tani), and overlap coefficient24(over) (Manning and Schu?tze, 2002).
The re-sult of every combination is displayed conciselyin Table 1 using the median rank18.
The casesin which the median ranks are close to RPMI +Entropy are magnified in Table 2.
We can seethere that RPMI + Entropy, and LLR(P) + En-tropy perform nearly equally.
All other combina-tions perform worse, especially in Top 1 accuracy.Finally, LLR(P) presents a clear edge over LLR,which suggests that indeed only positive associa-tions seem to matter in a cross-lingual setting.Entropy Match Cosine Tani OverRPMI 13.0 17.0 24.0 37.5 36.0LLR(P) 16.0 15.0 22.5 34.0 25.5LLR 23.5 22.0 27.5 50.5 50.0Table 1: Crosslingual experiment MLIT/NHTSA?
Evaluation matrix showing the median ranks ofseveral combinations of association and similaritymeasures.Top 1 Top 10 Top 20RPMI + Entropy 0.14 0.46 0.55RPMI + Matching 0.08 0.41 0.57LLR(P) + Entropy 0.14 0.46 0.55LLR(P) + Matching 0.08 0.44 0.55Table 2: Accuracies for crosslingual experimentMLIT/NHTSA.Finally we conduct an another experiment usingthe corpora pair Mainichi/Reuters which is quitedifferent from MLIT/NHTSA.
When comparingto the best baselines in Table 3 we see that ourapproach again performs best.
Furthermore, theexperiments displayed in Table 4 suggest that Ro-bust PMI and pointwise entropy are better choicesfor positive association measurement and similar-ity measurement, respectively.
We can see thatTop 1 Top 10 Top 20RPMI + Entropy 0.15 0.38 0.46LLR(P) + Manhattan 0.10 0.26 0.33TFIDF(MPO) + Cos 0.05 0.12 0.18Table 3: Accuracies for crosslingual experimentMainichi/Reuters ?
Comparison to best baselines.18A median rank of i, means that 50% of the correct trans-lations have a rank higher than i.Top 1 Top 10 Top 20RPMI + Entropy 0.15 0.38 0.46RPMI + Matching 0.08 0.30 0.35LLR(P) + Entropy 0.13 0.36 0.47LLR(P) + Matching 0.08 0.29 0.37Table 4: Accuracies for crosslingual experimentMainichi/Reuters ?
Comparison to alternatives.the overall best baseline turns out to be LLR(P) +Manhattan.
Comparing the rank from each wordfrom the gold-standard pairwise, we see that ourapproach, RPMI + Entropy, is significantly betterthan this baseline in MLIT/NHTSA as well as inMainichi/Reuters.194.2 AnalysisIn this section, we provide two representative ex-amples extracted from the previous experimentswhich sheds light into a weakness of the stan-dard feature vector approach which was used as abaseline before.
The two example queries and thecorresponding responses of LLR(P) + Manhattanand our approach are listed in Table 5.
Further-more in Table 6 we list the pivot words with thehighest degree of association (here LLR values)for the query and its correct translation.
We cansee that a query and its translation shares somepivots which are associated with statistical signif-icance20.
However it also illustrates that the ac-tual LLR value is less insightful and can hardly becompared across these two corpora.Let us analyze the two examples in more de-tail.
In Table 6, we see that the first query ??
?gear?21 is highly associated with???
?shift?.However, on the English side we see that gear ismost highly associated with the pivot word gear.Note that here the word gear is also a pivot wordcorresponding to the Japanese pivot word ??
?gear (wheel)?.22 Since in English the word gear(shift) and gear (wheel) is polysemous, the surfaceforms are the same leading to a high LLR value of19Using pairwise test with p-value 0.05.20Note that for example, an LLR value bigger than 11.0means the chances that there is no association is smaller than0.001 using that LLR ?
?2.21For a Japanese word, we write the English translationwhich is appropriate in our context, immediately after it.22In other words, we have the entry (?
?, gear) in ourdictionary but not the entry (?
?, gear).
The first pair isused as a pivot, the latter word pair is what we try to find.25gear.
Finally, the second example query ????pedal?
shows that words which, not necessarilyalways, but very often co-occur, can cause rela-tively high LLR values.
The Japanese verb ??
?to press?
is associated with ???
with a highLLR value ?
4 times higher than ??
?return??
which is not reflected on the English side.
Insummary, we can see that in both cases the degreeof associations are rather different, and cannot becompared without preprocessing.
However, it isalso apparent that in both examples a simple L1normalization of the degree of associations doesnot lead to more similarity, since the relative dif-ferences remain.??
?gear?Method Top 3 candidates Rankbaseline jolt, lever, design 284filtering reverse, gear, lever 2???
?pedal?Method Top 3 candidates Rankbaseline mj, toyota, action 176filtering pedal, situation, occasion 1Table 5: List of translation suggestions usingLLR(P) + Manhattan (baseline) and our method(filtering).
The third column shows the rank ofthe correct translation.??
gearPivots LLR(P) Pivots LLR(P)??
?shift?
154 gear 7064???
?shift?
144 shift 1270???
?come out?
116 reverse 314???
pedalPivots LLR(P) Pivots LLR(P)??
?press?
628 floor 1150??
?return?
175 stop 573?
?foot?
127 press 235Table 6: Shows the three pivot words which havethe highest degree of association with the query(left side) and the correct translation (right side).5 ConclusionsWe introduced a new method to compare con-text similarity across comparable corpora using aBayesian estimate for PMI (Robust PMI) to ex-tract positive associations and a similarity mea-surement based on the hypergeometric distribu-tion (measuring pointwise entropy).
Our experi-ments show that, for finding cross-lingual trans-lations, the assumption that words with similarmeaning share positive associations with the samewords is more appropriate than the assumptionthat the degree of association is similar.
Our ap-proach increases Top 1 and Top 20 accuracy ofup to 50% and 39% respectively, when comparedto several previous methods.
We also analyzedthe two components of our method separately.
Ingeneral, Robust PMI yields slightly better per-formance than the popular LLR, and, in contrastto LLR, allows to extract positive associations aswell as to include prior information in a principledway.
Pointwise entropy for comparing feature setscross-lingually improved the translation accuracyclearly when compared with standard similaritymeasurements.AcknowledgmentWe thank Dr. Naoaki Okazaki and the anony-mous reviewers for their helpful comments.
Fur-thermore we thank Daisuke Takuma, IBM Re-search - Tokyo, for mentioning previous workon statistical corrections for PMI.
This work waspartially supported by Grant-in-Aid for SpeciallyPromoted Research (MEXT, Japan).
The first au-thor is supported by the MEXT Scholarship andby an IBM PhD Scholarship Award.ReferencesChiao, Y.C.
and P. Zweigenbaum.
2002.
Lookingfor candidate translational equivalents in special-ized, comparable corpora.
In Proceedings of the In-ternational Conference on Computational Linguis-tics, pages 1?5.
International Committee on Com-putational Linguistics.De?jean, H., E?.
Gaussier, and F. Sadat.
2002.
An ap-proach based on multilingual thesauri and modelcombination for bilingual lexicon extraction.
InProceedings of the International Conference onComputational Linguistics, pages 1?7.
InternationalCommittee on Computational Linguistics.Dunning, T. 1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Lin-guistics, 19(1):61?74.Fung, P. 1998.
A statistical view on bilinguallexicon extraction: from parallel corpora to non-parallel corpora.
Lecture Notes in Computer Sci-ence, 1529:1?17.26Garera, N., C. Callison-Burch, and D. Yarowsky.2009.
Improving translation lexicon induction frommonolingual corpora via dependency contexts andpart-of-speech equivalences.
In Proceedings of theConference on Computational Natural LanguageLearning, pages 129?137.
Association for Compu-tational Linguistics.Gaussier, E., J.M.
Renders, I. Matveeva, C. Goutte,and H. Dejean.
2004.
A geometric view on bilin-gual lexicon extraction from comparable corpora.In Proceedings of the Annual Meeting of the Asso-ciation for Computational Linguistics, pages 526?533.
Association for Computational Linguistics.Haghighi, A., P. Liang, T. Berg-Kirkpatrick, andD.
Klein.
2008.
Learning bilingual lexicons frommonolingual corpora.
In Proceedings of the AnnualMeeting of the Association for Computational Lin-guistics, pages 771?779.
Association for Computa-tional Linguistics.Johnson, M. 2001.
Trading recall for precision withconfidence-sets.
Technical report, Brown Univer-sity.Koehn, P. and K. Knight.
2002.
Learning a translationlexicon from monolingual corpora.
In Proceedingsof ACL Workshop on Unsupervised Lexical Acquisi-tion, volume 34, pages 9?16.
Association for Com-putational Linguistics.Kudo, T., K. Yamamoto, and Y. Matsumoto.
2004.Applying conditional random fields to Japanesemorphological analysis.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 230?237.
Association for Com-putational Linguistics.Manning, C.D.
and H. Schu?tze.
2002.
Foundationsof Statistical Natural Language Processing.
MITPress.Moore, R.C.
2004.
On log-likelihood-ratios and thesignificance of rare events.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 333?340.
Association forComputational Linguistics.Moore, R.C.
2005.
A discriminative framework forbilingual word alignment.
In Proceedings of theConference on Human Language Technology andEmpirical Methods in Natural Language Process-ing, pages 81?88.
Association for ComputationalLinguistics.Morin, E., B. Daille, K. Takeuchi, and K. Kageura.2007.
Bilingual terminology mining-using brain,not brawn comparable corpora.
In Proceedings ofthe Annual Meeting of the Association for Compu-tational Linguistics, volume 45, pages 664?671.
As-sociation for Computational Linguistics.Okazaki, N., Y. Tsuruoka, S. Ananiadou, and J. Tsu-jii.
2008.
A discriminative candidate generator forstring transformations.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 447?456.
Association for Com-putational Linguistics.Pham-Gia, T. 2000.
Distributions of the ratios of in-dependent beta variables and applications.
Com-munications in Statistics.
Theory and Methods,29(12):2693?2715.Rapp, R. 1999.
Automatic identification of wordtranslations from unrelated English and Germancorpora.
In Proceedings of the Annual Meetingof the Association for Computational Linguistics,pages 519?526.
Association for Computational Lin-guistics.Ross, T.D.
2003.
Accurate confidence intervals forbinomial proportion and Poisson rate estimation.Computers in Biology and Medicine, 33(6):509?531.Tsuruoka, Y., Y. Tateishi, J. Kim, T. Ohta, J. Mc-Naught, S. Ananiadou, and J. Tsujii.
2005.
De-veloping a robust part-of-speech tagger for biomed-ical text.
Lecture Notes in Computer Science,3746:382?392.Wilcox, R.R.
2009.
Basic Statistics: UnderstandingConventional Methods and Modern Insights.
Ox-ford University Press.27
