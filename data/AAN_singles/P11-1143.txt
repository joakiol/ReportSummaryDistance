Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1425?1434,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsImproving Question Recommendation by Exploiting Information NeedShuguang LiDepartment of Computer ScienceUniversity of York, YO10 5DD, UKsgli@cs.york.ac.ukSuresh ManandharDepartment of Computer ScienceUniversity of York, YO10 5DD, UKsuresh@cs.york.ac.ukAbstractIn this paper we address the problem of ques-tion recommendation from large archives ofcommunity question answering data by ex-ploiting the users?
information needs.
Ourexperimental results indicate that questionsbased on the same or similar information needcan provide excellent question recommenda-tion.
We show that translation model can beeffectively utilized to predict the informationneed given only the user?s query question.
Ex-periments show that the proposed informationneed prediction approach can improve the per-formance of question recommendation.1 IntroductionThere has recently been a rapid growth in the num-ber of community question answering (CQA) ser-vices such as Yahoo!
Answers1, Askville2 andWikiAnswer3 where people answer questions post-ed by other users.
These CQA services have built upvery large archives of questions and their answers.They provide a valuable resource for question an-swering research.
Table 1 is an example from Ya-hoo!
Answers web site.
In the CQA archives, thetitle part is the user?s query question, and the user?sinformation need is usually expressed as natural lan-guage statements mixed with questions expressingtheir interests in the question body part.In order to avoid the lag time involved with wait-ing for a personal response and to enable high quali-1http://answers.yahoo.com2http://askville.amazon.com3http://wiki.answers.comty answers from the archives to be retrieved, we needto search CQA archives of previous questions thatare closely associated with answers.
If a questionis found to be interesting to the user, then a previ-ous answer can be provided with very little delay.Question search and question recommendation areproposed to facilitate finding highly relevant or po-tentially interesting questions.
Given a user?s ques-tion as the query, question search tries to returnthe most semantically similar questions from thequestion archives.
As the complement of questionsearch, we define question recommendation as rec-ommending questions whose information need is thesame or similar to the user?s original question.
Forexample, the question ?What aspects of my com-puter do I need to upgrade ...?
with the informa-tion need ?...
making a skate movie, my computerfreezes, ...?
and the question ?What is the most costeffective way to expend memory space ...?
with in-formation need ?...
in need of more space for mu-sic and pictures ...?
are both good recommendationquestions for the user in Table 1.
So the recommend-ed questions are not necessarily identical or similarto the query question.In this paper, we discuss methods for questionrecommendation based on using the similarity be-tween information need in the archive.
We alsopropose two models to predict the information needbased on the query question even if there?s no infor-mation need expressed in the body of the question.We show that with the proposed models it is possi-ble to recommend questions that have the same orsimilar information need.The remainder of the paper is structured as fol-1425Q Title If I want a faster computershould I buy more memory or s-torage space?
...Q Body I edit pictures and videos so Ineed them to work quickly.
Anyadvice?Answer ...
If you are running out of s-pace on your hard drive, then... to boost your computer speedusually requires more RAM ...Table 1: Yahoo!
Answers question examplelows.
In section 2, we briefly describe the relatedwork on question search and recommendation.
Sec-tion 3 addresses in detail how we measure the sim-ilarity between short texts.
Section 4 describes twomodels for information need prediction that we usefor the experiment.
Section 5 tests the performanceof the proposed models for the task of question rec-ommendation.
Section 7 is the conclusion of thispaper.2 Related Work2.1 Question SearchBurke et al (1997) combined a lexical metric and asimple semantic knowledge-based (WordNet) simi-larity method to retrieve semantically similar ques-tions from frequently asked question (FAQ) data.Jeon et al (2005a) retrieved semantically similarquestions from Korean CQA data by calculating thesimilarity between their answers.
The assumptionbehind their research is that questions with very sim-ilar answers tend to be semantically similar.
Jeonet al (2005b) also discussed methods for groupingsimilar questions based on using the similarity be-tween answers in the archive.
These grouped ques-tion pairs were further used as training data to es-timate probabilities for a translation-based questionretrieval model.
Wang et al (2009) proposed a treekernel framework to find similar questions in the C-QA archive based on syntactic tree structures.
Wanget al (2010) mined lexical and syntactic features todetect question sentences in CQA data.2.2 Question RecommendationWu et al (2008) presented an incremental auto-matic question recommendation framework basedon probabilistic latent semantic analysis.
Questionrecommendation in their work considered both theusers?
interests and feedback.
Duan et al (2008)made use of a tree-cut model to represent question-s as graphs of topic terms.
Questions were recom-mended based on this topic graph.
The recommend-ed questions can provide different aspects around thetopic of the query question.The above question search and recommendationresearch provide different ways to retrieve question-s from large archives of question answering data.However, none of them considers the similarity ordiversity between questions by exploring their infor-mation needs.3 Short Text Similarity MeasuresIn question retrieval systems accurate similaritymeasures between documents are crucial.
Most tra-ditional techniques for measuring the similarity be-tween two documents mainly focus on comparingword co-occurrences.
The methods employing thisstrategy for documents can usually achieve good re-sults, because they may share more common wordsthan short text snippets.
However the state-of-the-art techniques usually fail to achieve desired resultsdue to short questions and information need texts.In order to measure the similarity between shorttexts, we make use of three kinds of text similari-ty measures: TFIDF based, Knowledge based andLatent Dirichlet Allocation (LDA) based similaritymeasures in this paper.
We will compare their per-formance for the task of question recommendationin the experiment section.3.1 TFIDFBaeza-Yates and Ribeiro-Neto (1999) provides a T-FIDF method to calculate the similarity between twotexts.
Each document is represented by a term vec-tor using TFIDF score.
The similarity between twotext Di and Dj is the cosine similarity in the vectorspace model:cos(Di, Dj) =DTi Dj?Di?
?Dj?1426This method is used in most information retrievalsystems as it is both efficient and effective.
Howev-er if the query text contains only one or two wordsthis method will be biased to shorter answer texts(Jeon et al, 2005a).
We also found that in CQA datashort contents in the question body cannot provideany information about the users?
information needs.Based on the above two reasons, in the test data setswe do not include the questions whose informationneed parts contain only a few noninformative words.3.2 Knowledge-based MeasureMihalcea et al (2006) proposed several knowledge-based methods for measuring the semantic level sim-ilarity of texts to solve the lexical chasm problem be-tween short texts.
These knowledge-based similaritymeasures were derived from word semantic similar-ity by making use of WordNet.
The evaluation on aparaphrase recognition task showed that knowledge-based measures outperform the simpler lexical levelapproach.We follow the definition in (Mihalcea et al, 2006)to derive a text-to-text similarity metric mcs for twogiven texts Di and Dj :mcs(Di, Dj) =?w?DimaxSim(w,Dj) ?
idf(w)?w?Diidf(w)+?w?DjmaxSim(w,Di) ?
idf(w)?w?Djidf(w)For each word w in Di, maxSim(w,Dj) com-putes the maximum semantic similarity between wand any word in Dj .
In this paper we choose lin(Lin, 1998) and jcn (Jiang and Conrath, 1997) tocompute the word-to-word semantic similarity.We only choose nouns and verbs for calculatingmcs.
Additionally, when w is a noun we restrictthe words in document Di (and Dj) to just nouns.Similarly, when w is a verb, we restrict the words indocument Di (and Dj) to just verbs.3.3 Probabilistic Topic ModelCelikyilmaz et al (2010) presented probabilistictopic model based methods to measure the similar-ity between question and candidate answers.
Thecandidate answers were ranked based on the hiddentopics discovered by Latent Dirichlet Allocation (L-DA) methods.In contrast to the TFIDF method which measures?common words?, short texts are not compared toeach other directly in probabilistic topic models.
In-stead, the texts are compared using some ?third-party?
topics that relate to them.
A passage D in theretrieved documents (document collection) is repre-sented as a mixture of fixed topics, with topic z get-ting weight ?
(D)z in passage D and each topic is adistribution over a finite vocabulary of words, withword w having a probability ?
(z)w in topic z. GibbsSampling can be used to estimate the correspondingexpected posterior probabilities P (z|D) = ??
(D)z andP (w|z) = ??
(z)w (Griffiths and Steyvers, 2004).In this paper we use two LDA based similaritymeasures in (Celikyilmaz et al, 2010) to measurethe similarity between short information need texts.The first LDA similarity method uses KL divergenceto measure the similarity between two documentsunder each given topic:simLDA1(Di, Dj) =1KK?k=110W (D(z=k)i ,D(z=k)j )W (D(z=k)i , D(z=k)j ) =?KL(D(z=k)i ?D(z=k)i +D(z=k)j2)?KL(D(z=k)j ?D(z=k)i +D(z=k)j2)W (D(z=k)i , D(z=k)j ) calculates the similarity be-tween two documents under topic z = k using KLdivergence measure.
D(z=k)i is the probability distri-bution of words in document Di given a fixed topicz.The second LDA similarity measure from (Grif-fiths and Steyvers, 2004) treats each document as aprobability distribution of topics:simLDA2(Di, Dj) = 10W (??(Di),??
(Dj))where ??
(Di) is document Di?s probability distribu-tion of topics as defined earlier.14274 Information Need Prediction usingStatistical Machine Translation ModelThere are two reasons that we need to predict in-formation need.
It is often the case that the queryquestion does not have a question body part.
So weneed a model to predict the information need partbased on the query question in order to recommendquestions based on the similarity of their informa-tion needs.
Another reason is that information needprediction plays a crucial part not only in QuestionAnswering but also in information retrieval (Liu etal., 2008).
In this paper we propose an informationneed prediction method based on a statistical ma-chine translation model.4.1 Statistical Machine Translation Model(f(s), e(s)), s = 1,...,S is a parallel corpus.
In asentence pair (f, e), source language String, f =f1f2...fJ has J words, and e = e1e2...eI has I word-s. And alignment a = a1a2...aJ represents the map-ping information from source language words to tar-get words.Statistical machine translation models estimatePr(f|e), the translation probability from source lan-guage string e to target language string f (Och et al,2003):Pr(f|e) =?aPr(f, a|e)EM-algorithm is usually used to train the align-ment models to estimate lexicon parameters p(f |e).In E-step, the counts for one sentence pair (f ,e)are:c(f |e; f, e) =?aPr(a|f, e)?i,j?
(f, fj)?
(e, eaj )Pr(a|f, e) = Pr(f, a|e)/Pr(a|e)In the M-step, lexicon parameters become:p(f |e) ?
?sc(f |e; f(s), e(s))Different alignment models such as IBM-1 toIBM-5 (Brown et al, 1993) and HMM model (Ochand Ney, 2000) provide different decompositions ofPr(f ,a|e).
For different alignment models differ-ent approaches were proposed to estimate the cor-responding alignments and parameters.
The detail-s can be found in (Och et al, 2003; Brown et al,1993).4.2 Information Need PredictionAfter estimating the statistical translation probabili-ties, we treat the information need prediction as theprocess of ranking words by p(w|Q), the probabilityof generating word w from question Q:P (w|Q) = ?
?t?QPtr(w|t)P (t|Q)+(1??
)P (w|C)The word-to-word translation probabilityPtr(w|t) is the probability of word w is translatedfrom a word t in question Q using the translationmodel.
The above formula uses linear interpolationsmoothing of the document model with the back-ground language model P (t|C).
?
is the smoothingparameter.
P (t|Q) and P (t|C) are estimated usingthe maximum likelihood estimator.One important consideration is that statistical ma-chine translation models first estimate Pr(f|e) andthen calculate Pr(e|f) using Bayes?
theorem to min-imize ordering errors (Brown et al, 1993):Pr(e|f) =Pr(f|e)Pr(e)Pr(f)But in this paper, we skip this step as we found outthe order of words in information need part is notan important factor.
In our collected CQA archive,question title and information need pairs can be con-sidered as a type of parallel corpus, which is usedfor estimating word-to-word translation probabili-ties.
More specifically, we estimated the IBM-4model by GIZA++4 with the question part as thesource language and information need part as the tar-get language.5 Experiments and Results5.1 Text PreprocessingThe questions posted on community QA sites oftencontain spelling or grammar errors.
These errors in-4http://fjoch.com/GIZA++.html1428Test c Test tMethods MRR Precision@5 Precision@10 MRR Precision@5 Precision@10TFIDF 84.2% 67.1% 61.9% 92.8% 74.8% 63.3%Knowledge1 82.2% 65.0% 65.6% 78.1% 67.0% 69.6%Knowledge2 76.7% 54.9% 59.3% 61.6% 53.3% 58.2%LDA1 92.5% 68.8% 64.7% 91.8% 75.4% 69.8%LDA2 61.5% 55.3% 60.2% 52.1% 57.4% 54.5%Table 2: Question recommendation results without information need predictionTest c Test tMethods MRR Precision@5 Precision@10 MRR Precision@5 Precision@10TFIDF 86.2% 70.8% 64.3% 95.1% 77.8% 69.3%Knowledge1 82.2% 65.0% 66.6% 76.7% 68.0% 68.7%Knowledge2 76.7% 54.9% 60.2% 61.6% 53.3% 58.2%LDA1 95.8% 72.4% 68.2% 96.2% 79.5% 69.2%LDA2 61.5% 55.3% 58.9% 68.1% 58.3% 53.9%Table 3: Question recommendation results with information need predicted by translation modelfluence the calculation of similarity and the perfor-mance of information retrieval (Zhao et al, 2007;Bunescu and Huang, 2010).
In this paper, we usean open source software afterthedeadline5 to auto-matically correct the spelling errors in the questionand information need texts first.
We also made useof Web 1T 5-gram6 to implement an N-Gram basedmethod (Cheng et al, 2008) to further filter out thefalse positive corrections and re-rank correction sug-gestions (Mudge, 2010).
The texts are tagged byBrill?s Part-of-Speech Tagger7 as the rule-based tag-ger is more robust than the state-of-art statistical tag-gers for raw web contents.
This tagging informa-tion is only used for WordNet similarity calculation.Stop word removal and lemmatization are appliedto the all the raw texts before feeding into machinetranslation model training, the LDA model estimat-ing and similarity calculation.5.2 Construction of Training and Testing SetsWe made use of the questions crawled from Yahoo!Answers for the estimating models and evaluation.More specifically, we obtained 2 million questionsunder two categories at Yahoo!
Answers: ?travel?5http://afterthedeadline.com6http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T137http://www.umiacs.umd.edu/ jimmylin/resources.html(1 million), and ?computers&internet?
(1 million).Depending on whether the best answers have beenchosen by the asker, questions from Yahoo!
answerscan be divided into ?resolved?
and ?unresolved?
cat-egories.
From each of the above two categories, werandomly selected 200 resolved questions to con-struct two testing data sets: ?Test t?
(?travel?
), and?Test c?
(?computers&internet?).
In order to mea-sure the information need similarity in our experi-ment we selected only those questions whose infor-mation needs part contained at least 3 informativewords after stop word removal.
The rest of the ques-tions ?Train t?
and ?Train c?
under the two categoriesare left for estimating the LDA topic models and thetranslation models.
We will show how we obtainthese models later.5.3 Experimental SetupFor each question (query question) in ?Test t?
or?Test c?, we used the words in the question title partas the main search query and the other words in theinformation need part as search query expansion toretrieve candidate recommended questions from Ya-hoo!
Answers website.
We obtained an average of154 resolved questions under ?travel?
or ?computer-s&internet?
category, and three assessors were in-volved in the manual judgments.Given a question returned by a recommendation1429method, two assessors are asked to label it with?good?
or ?bad?.
The third assessor will judge theconflicts.
The assessors are also asked to read the in-formation need and answer parts.
If a recommendedquestion is considered to express the same or similarinformation need, the assessor will label it ?good?
;otherwise, the assessor will label it as ?bad?.Three measures for evaluating the recommenda-tion performance are utilized.
They are Mean Re-ciprocal Rank (MRR), top five prediction accura-cy (precision@5) and top ten prediction accuracies(precision@10) (Voorhees and Tice, 2004; Cao etal., 2008).
In MRR the reciprocal rank of a queryquestion is the multiplicative inverse of the rank ofthe first ?good?
recommended question.
The top fiveprediction accuracy for a query question is the num-ber of ?good?
recommended questions out of the topfive ranked questions and the top ten accuracy is cal-culated out of the top ten ranked questions.5.4 Similarity MeasureThe first experiment conducted question recommen-dation based on their information need parts.
Dif-ferent text similarity methods described in section3 were used to measure the similarity between theinformation need texts.
In TFIDF similarity mea-sure (TFIDF), the idf values for each word werecomputed from frequency counts over the entireAquaint corpus8.
For calculating the word-to-wordknowledge-based similarity, a WordNet::SimilarityJava implementation9 of the similarity measures lin(Knowledge2) and jcn (Knowledge1) is used in thispaper.
For calculating topic model based similarity,we estimated two LDA models from ?Train t?
and?Train c?
using GibbsLDA++10.
We treated eachquestion including the question title and the infor-mation need part as a single document of a sequenceof words.
These documents were preprocessed be-fore being fed into LDA model.
1800 iterations forGibbs sampling 200 topics parameters were set foreach LDA model estimation.The results in table 2 show that TFIDF and LDA1methods perform better for recommending questionsthan the others.
After further analysis of the ques-tions recommended by both methods, we discov-8http://ldc.upenn.edu/Catalog/docs/LDC2002T319http://cogs.susx.ac.uk/users/drh21/10http://gibbslda.sourceforge.netQ1: If I want a faster computer should I buymore memory or storage space?InfoN If I want a faster computer should I buymore memory or storage space?
What-s the difference?
I edit pictures andvideos so I need them to work quickly....RQ1 Would buying 1gb memory upgrademake my computer faster?InfoN I have an inspiron B130.
It has 512mbmemory now.
I would add another 1gbinto 2nd slot ...RQ2 whats the difference between memoryand hard drive space on a computer andwhy is.....?InfoN see I am starting edit videos on my com-puter but i am running out of space.
whyis so expensive to buy memory but notexternal drives?
...Q2: Where should my family go for springbreak?InfoN ... family wants to go somewhere fora couple days during spring break ...prefers a warmer climate and we live inIL, so it shouldn?t be SUPER far away.... a family road trip.
...RQ1 Whats a cheap travel destination forspring break?InfoN I live in houston texas and i?m trying tofind i inexpensive place to go for springbreak with my family.My parents don?twant to spend a lot of money due to theeconomy crisis, ... a fun road trip...RQ2 Alright you creative deal-seekers, I needsome help in planning a spring breaktrip for my familyInfoN Spring break starts March 13th and goesuntil the 21st ... Someplace WARM!!
!Family-oriented hotel/resort ... NorthAmerican Continent (Mexico, America,Jamaica, Bahamas, etc.)
Cost= Around$5,000 ...Table 4: Question recommendation results by LDA mea-suring the similarity between information needs1430ered that the ordering of the recommended questionsfrom TFIDF and LDA1 are quite different.
TFIDFsimilarity method prefers texts with more commonwords, while the LDA1 method can find the rela-tion between the non-common words between shorttexts based on a series of third-party topics.
The L-DA1 method outperforms the TFIDF method in twoways: (1) the top recommended questions?
informa-tion needs share less common words with the queryquestion?s; (2) the top recommended questions spanwider topics.
The questions highly recommended byLDA1 can suggest more useful topics to the user.Knowledge-based methods are also shown to per-form worse than TFIDF and LDA1.
We found thatsome words were mis-tagged so that they were notincluded in the word-to-word similarity calculation.Another reason for the worse performance is that thewords out of the WordNet dictionary were also notincluded in the similarity calculation.The Mean Reciprocal Rank score for TFIDF andLDA1 are more than 80%.
That is to say, we are ableto recommend questions to the users by measuringtheir information needs.
The first two recommendedquestions for Q1 and Q2 using LDA1 method areshown in table 4.
InfoN is the information need partassociated with each question.In the preprocessing step, some words were suc-cessfully corrected such as ?What should I do thissaturday?
... and staying in a hotell ...?
and ?myfaimly is traveling to florda ...?.
However, there arestill a small number of texts such as ?How come myGforce visualization doesn?t work??
and ?Do i needan Id to travel from new york to maimi??
failed tobe corrected.
So in the future, a better method isexpected to correct these failure cases.5.5 Information Need PredictionThere are some retrieved questions whose informa-tion need parts are empty or become empty or al-most empty (one or two words left) after the prepro-cessing step.
The average number of such retrievedquestions for each query question is 10 in our exper-iment.
The similarity ranking scores of these ques-tions are quite low or zero in the previous experi-ment.
In this experiment, we will apply informationneed prediction to the questions whose informationneeds are missing in order to find out whether weimprove the recommendation task.The question and information need pairs in both?Train t?
and ?Train c?
training sets were used totrain two IBM-4 translation models by GIZA++toolkit.
These pairs were also preprocessed beforetraining.
And the pairs whose information need partbecome empty after preprocessing were disregard-ed.During the experiment, we found that some of thegenerated words in the information need parts arethemselves.
This is caused by the self translationproblem in translation model: the highest transla-tion score for a word is usually given to itself ifthe target and source languages are the same (Xueet al, 2008).
This has always been a tough ques-tion: not using self-translated words can reduce re-trieval performance as the information need partsneed the terms to represent the semantic meanings;using self-translated words does not take advantageof the translation approach.
To tackle this problem,we control the number of the words predicted by thetranslation model to be exactly twice the number ofwords in the corresponding preprocessed question.The predicted information need words for the re-trieved questions are shown in Table 5.
In Q1, the in-formation need behind question ?recommend web-site for custom built computer parts?
may implythat the users need to know some information aboutbuilding computer parts such as ?ram?
and ?moth-erboard?
for a different purpose such as ?gaming?.While in Q2, the user may want to compare comput-ers in different brands such as ?dell?
and ?mac?
orconsider the ?price?
factor for ?purchasing a laptopfor a college student?.We also did a small scale comparison between thegenerated information needs against the real ques-tions whose information need parts are not empty.Q3 and Q4 in Table 5 are two examples.
The orig-inal information need for Q3 is ?looking for beauti-ful beaches and other things to do such as museum-s, zoos, shopping, and great seafood?
in CQA.
Thegenerated content for Q3 contains words in widertopics such as ?wedding?, ?surf ?
and the price infor-mation (?cheap?).
This reflects that there are someother users asking similar questions with the sameor other interests.From the results in Table 3, we can see that theperformance of most similarity methods were im-proved by making use of information need predic-1431tion.
Different similarity measures received differ-ent degrees of improvement.
LDA1 obtained thehighest improvement followed by the TFIDF basedmethod.
These two approaches are more sensitive tothe contents generated by a translation model.However we found out that in some cases the L-DA1 model failed to give higher scores to good rec-ommendation questions.
For example, Q5, Q6, andQ7 in table 5 were retrieved as recommendation can-didates for the query question in Table 1.
All of thethree questions were good recommendation candi-dates, but only Q6 ranked fifth while Q5 and Q7were out of the top 30 by LDA1 method.
Moreover,in a small number of cases bad recommendationquestions received higher scores and jeopardized theperformance.
For example, for query question ?Howcan you add subtitles to videos??
with informationneed ?...
add subtitles to a music video ... got offyoutube ...download for this ...?, a retrieved ques-tion ?How would i add a music file to a video clip....?
was highly recommended by TFIDF approachas predicted information need contained ?youtube?,?video?, ?music?, ?download ?, ... .The MRR score received an improvement from92.5% to 95.8% in the ?Test c?
and from 91.8% to96.2% in ?Test t?.
This means that the top one ques-tion recommended by our methods can be quite wellcatering to the users?
information needs.
The topfive precision and the top ten precision scores us-ing TFIDF and LDA1 methods also received dif-ferent degrees of improvement.
Thus, we can im-prove the performance of question recommendationby predicting information needs.6 ConclusionsIn this paper we addressed the problem of recom-mending questions from large archives of commu-nity question answering data based on users?
infor-mation needs.
We also utilized a translation mod-el and a LDA topic model to predict the informa-tion need only given the user?s query question.
D-ifferent information need similarity measures werecompared to prove that it is possible to satisfy user?sinformation need by recommending questions fromlarge archives of community QA.
The Latent Dirich-let alocation based approach was proved to perfor-m better on measuring the similarity between shortQ1: Please recommend A good website forCustom Built Computer parts?InfoN custom, site, ram, recommend, price,motherboard, gaming, ...Q2: What is the best laptop for a college stu-dent?InfoN know, brand, laptop, college, buy, price,dell, mac, ...Q3: What is the best Florida beach for a honey-moon?InfoN Florida, beach, honeymoon, wedding, surf,cheap, fun, ...Q4: Are there any good clubs in ManchesterInfoN club, bar, Manchester, music, age, fun,drink, dance, ...Q5: If i buy a video card for my computer willthat make it faster?InfoN nvidia, video, ati, youtube, card, buy, win-dow, slow, computer, graphics, geforce,faster, ...Q6: If I buy a bigger hard drive for my laptop,will it make my computer run faster or justincrease the memory?InfoN laptop, ram, run, buy, bigger, memory,computer, increase, gb, hard, drive, faster,...Q7: Is there a way I can make my computerwork faster rather than just increasing theram or harware space?InfoN space, speed, ram, hardware, main, gig, s-low, computer, increase, work, gb, faster,...Table 5: Information need prediction examples usingIBM-4 translation model1432texts in the semantic level than traditional method-s.
Experiments showed that the proposed transla-tion based language model for question informationneed prediction further enhanced the performance ofquestion recommendation methods.ReferencesRicardo A. Baeza-Yates and Berthier Ribeiro-Neto.1999.
Modern Information Retrieval.
Addison-WesleyLongman Publishing Co., Inc., Boston, MA, USA.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, Robert L. Mercer.
1993.
The mathematics ofstatistical machine translation: parameter estimation.Computational Linguistics, v.19 n.2, June 1993.Razvan Bunescu and Yunfeng Huang.
2010.
Learning theRelative Usefulness of Questions in Community QA.Proceedings of the Conference on Empirical Method-s in Natural Language Processing (EMNLP) , Cam-bridge, MA.Robin D. Burke and Kristian J. Hammond and VladimirA.
Kulyukin and Steven L. Lytinen and Noriko To-muro and Scott Schoenberg.
1997.
Question answer-ing from frequently-asked question files: Experienceswith the FAQ Finder system.
AI Magazine, 18, 57C66.Yunbo Cao, Huizhong Duan, Chin-Yew Lin, Yong Yu,and Hsiao-Wuen Hon.
2008.
Recommending Ques-tions Using the MDL-based Tree Cut Model.
In: Proc.of the 17th Int.
Conf.
on World Wide Web, pp.
81-90.Asli Celikyilmaz and Dilek Hakkani-Tur and GokhanTur.
2010.
LDA Based Similarity Modeling for Ques-tion Answering.
In NAACL 2010 C Workshop on Se-mantic Search.Charibeth Cheng, Cedric Paul Alberto, Ian AnthonyChan, and Vazir Joshua Querol.
2008.
SpellCheF:Spelling Checker and Corrector for Filipino.
Journalof Research in Science, Computing and Engineering,North America, 4, sep. 2008.Lynn Silipigni Connaway and Chandra Prabha.
2005.
Anoverview of the IMLS Project ?Sense-making the in-formation confluence: The whys and hows of collegeand university user satisficing of information needs?.Presented at Library of Congress Forum, AmericanLibrary Association Midwinter Conference, Boston,MA, Jan 16, 2005.Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and YongYu.
2008.
Searching questions by identifying ques-tion topic and question focus.
In HLT-ACL, pages156C164.Thomas L. Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
Natl Acad Sci 101:5228C5235.Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee.
2005a.Finding semantically similar questions based on theiranswers.
In Proc.
of SIGIR05.Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee.
2005b.Finding similar questions in large question and an-swer archives.
In CIKM, pages 84C90.Jay J. Jiang and David W. Conrath.
1997.
Semantic sim-ilarity based on corpus statistics and lexical taxono-my.
In Proceedings of International Conference on Re-search in Computational Linguistics, Taiwan.Dekang Lin.
1998.
An Information-Theoretic Definitionof Similarity.
In Proceedings of the Fifteenth Interna-tional Conference on Machine Learning (ICML ?98),Jude W. Shavlik (Ed.).
Morgan Kaufmann PublishersInc., San Francisco, CA, USA, 296-304.Yandong Liu, Jiang Bian, and Eugene Agichtein.
2008.Predicting information seeker satisfaction in commu-nity question answering.
In Proceedings of the 31stannual international ACM SIGIR conference on Re-search and development in information retrieval (SI-GIR ?08).
ACM, New York, NY, USA, 483-490.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In Proceedings of the 21stnational conference on Artificial intelligence (AAAI?06), pages 775C780.
AAAI Press.Raphael Mudge.
2010.
The design of a proofreading soft-ware service.
In Proceedings of the NAACL HLT 2010Workshop on Computational Linguistics and Writing:Writing Processes and Authoring Aids (CL&W ?10).Association for Computational Linguistics, Morris-town, NJ, USA, 24-32.Franz Josef Och, Hermann Ney.
2000.
A comparison ofalignment models for statistical machine translation.Proceedings of the 18th conference on Computationallinguistics, July 31-August 04, Saarbrucken, Germany.Franz Josef Och, Hermann Ney.
2003.A Systematic Com-parison of Various Statistical Alignment Models.
Com-putational Linguistics, volume 29, number 1, pp.
19-51 March 2003.Jahna Otterbacher, Gunes Erkan, Dragomir R. Radev.2009.
Biased LexRank: Passage retrieval using ran-dom walks with question-based priors.
InformationProcessing and Management: an International Journal,v.45 n.1, p.42-54, January, 2009.Chandra Prabha, Lynn Silipigni Connaway, LawrenceOlszewski, Lillie R. Jenkins.
2007.
What is enough?Satisficing information needs.
Journal of Documenta-tion (January, 63,1).Ellen Voorhees and Dawn Tice.
2000.
The TREC-8 ques-tion answering track evaluation.
In Text RetrievalConference TREC-8, Gaithersburg, MD.Kai Wang, Yanming Zhao, and Tat-Seng Chua.
2009.A syntactic tree matching approach to finding similar1433questions in community-based qa services.
In SIGIR,pages 187C194.Kai Wang and Tat-Seng Chua.
2010.
Exploiting salientpatterns for question detection and question retrievalin community-based question answering.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics (COLING ?10).
Association forComputational Linguistics, Stroudsburg, PA, USA,1155-1163.Hu Wu, Yongji Wang, and Xiang Cheng.
2008.
Incremen-tal probabilistic latent semantic analysis for automaticquestion recommendation.
In RecSys.Xiaobing Xue, Jiwoon Jeon, W. Bruce Croft.
2008.
Re-trieval models for question and answer archives.
InSIGIR?08, pages 475C482.
ACM.Shiqi Zhao, Ming Zhou, and Ting Liu.
2007.
LearningQuestion Paraphrases for QA from Encarta Logs.
InProceedings of International Joint Conferences on Ar-tificial Intelligence (IJCAI), pages 1795-1800.1434
