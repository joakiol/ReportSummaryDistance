Enhancing Detection through Linguistic Indexing and TopicExpansionTomek St rza lkowsk i ,  Gees  C .
Stein and G.  Bowden WiseGE Corporate Research & Development1 Research CircleNiskayuna, NY 12309strzalkowski@crd.ge.comAbstractNatural language processing techniques may holda tremendous potential for overcoming the inade-quacies of purely quantitative methods of text in-formation retrieval.
Under the Tipster contracts inphases I through III, GE group has set out to ex-plore this potential through development and evalu-ation of new text processing techniques.
This workresulted in some significant advances and in a betterunderstanding on how NLP may benefit IR.
Tipsterresearch as laid a critical groundwork for futurework.In this paper we summarize GE work on documentdetection in Tipster Phase III.
Our summarizationresearch is described in a separate paper appearingin this volume.BackgroundThe main thrust of this project has been to demon-strate that robust if relatively shallow NLP can helpto derive better representation of text documentsfor indexing and search purposes than any simpleword and string-based methods commonly used instatistical full-text retrieval.
This was based on thepremise that linguistic processing can uncover cer-tain critical semantic aspects of document content,something that simple word counting cannot do,thus leading to more accurate representation.
Theproject's progress has been rigorously evaluated in aseries of five Text Retrieval Conferences (TREC's)organized by the U.S. Government under the guid-ance of NIST and DARPA.
Since 1995, the projectscope widened substantially to include several paral-lel efforts at GE, Rutgers, Lockheed Martin Corpo-ration, New York University, University of Helsinki,and Swedish Institute for Computer Science (SICS).We have also collaborated with SRI Internationalduring TREC-6.
At TREC we demonstrated thatNLP can be done efficiently on a very large scale,and that it can have a significant impact on IR.
Atthe same time, it became clear that exploiting theTable 1: Performance gains attributed to NLP in-dexing vs. query lengthT-21 115 terms T -3 :70  terms T -4 :10  termsRUNS Base +NL Base +NL Base +NLPrec.
0.22 0.31 0.22 0.27 0.20 0.22change +40% +20% +10%full potential of linguistic processing is harder thanoriginally anticipated.Not surprisingly, we have noticed that the amo-unt of improvement in recall and precision whichwe could attribute to NLP, appeared to be relatedto the quality of the initial search request, whichin turn seemed unmistakably related to its length(cf.
Table 1).
Long and descriptive queries re-sponded well to NLP, while terse one-sentence s archdirectives showed hardly any improvement.
Thiswas not particularly surprising or even new, con-sidering that the shorter queries tended to containhighly discriminating words in them, and that wasjust enough to achieve the optimal performance.
Onthe other hand, comparing various evaluation cat-egories at TREC, it was also quite clear that thelonger queries just did better than the short ones,no matter what their level of processing.
Further-more, while the short queries needed no better in-dexing than with simple words, their performanceremained inadequate, and one definitely could usebetter queries.
Therefore, we started looking intoways to build full-bodied search queries, either auto-matically or interactively, out of users' initial searchstatements.TREC-5 (1996), therefore, marks a shift in ourapproach away from text representation issues andtowards query development problems.
While ourTREC-5 system still performs extensive text pro-cessing in order to extract phrasal and other in-dexing terms, our main focus moved on to queryconstruction using words, sentences, and entire pas-139sages to expand initial search specifications in an at-tempt to cover their various angles, aspects and con-texts.
Based on the observations that NLP is moreeffective with highly descriptive queries, we designedan expansion method in which entire passages fromrelated, though not necessarily relevant documentswere quite liberally imported into the user queries.This method appeared to have produced a dramaticimprovement in the performance of several differ-ent statistical search engines that we tested boost-ing the average precision by anywhere from 40% toas much as 130%.
Therefore, topic expansion ap-pears to lead to a genuine, sustainable advance inIR effectiveness.
Moreover, we show in TREC-6 andTREC-7 that this process can be automated whilemaintaining the performance gains.The other notable new feature of our TREC-5 sys-tem is the stream architecture.
It is a system of par-allel indexes built for a given collection, with eachindex reflecting a different ext representation strat-egy.
These indexes are called streams because theyrepresents different streams of data derived from theunderlying text archive.
A retrieval process earchesall or some of the streams, and the final rankingis obtained by merging individual stream search re-sults.
This allows for an effective combination ofalternative document representation and retrievalstrategies, in particular various NLP and non-NLPmethods.
The resulting meta-search system can beoptimized by maximizing the contribution of eachstream.
It is also a convenient vehicle for an objec-tive evaluation of streams against one another.NLP-Based  Index ing  in  In fo rmat ionRet r ieva lIn information retrieval (IR), a typical task is tofetch relevant documents from a large archive inresponse to a user's query, and rank these docu-ments according to relevance.
This has been usuallyaccomplished using statistical methods (often cou-pled with manual encoding) that (a) select terms(words, phrases, and other units) from documentsthat are deemed to best represent their content, and(b) create an inverted index file (or files) that pro-vide an easy access to documents containing theseterms.
A subsequent search process will attempt omatch preprocessed user queries against erm-basedrepresentations of documents in each case determin-ing a degree of relevance between the two whichdepends upon the number and types of matchingterms.
Although many sophisticated search andmatching methods are available, the fundamentalproblem remains to be an adequate representationof content for both the documents and the queries.In term-based representation, a document (aswell as a query) is transformed into a collection ofweighted terms (or surrogates representing combina-tions of terms), derived directly from the documenttext or indirectly through thesauri or domain maps.The representation is anchored on these terms, andthus their careful selection is critical.
Since eachunique term can be thought to add a new dimen-sionality to the representation, it is equally criticalto weigh them properly against one another so thatthe document is placed at the correct position inthe N-dimensional term space3 Our goal is to havethe documents on the same topic placed close to-gether, while those on different topics placed suffi-ciently apart.
The above should hold for any topics,a daunting task indeed, which is additionally com-plicated by the fact that we often do not know howto compute terms weights.
The statistical weight-ing formulas, based on terms distribution within thedatabase, such as tf*idf, are far from optimal, andthe assumptions of term independence which areroutinely made are false in most cases.
This situ-ation is even worse when single-word terms are in-termixed with phrasal terms and the term indepen-dence becomes harder to justify.There are a number of ways to obtain "phrases"from text.
These include generating simple col-locations, statistically validated N-grams, part-of-speech tagged sequences, syntactic structures, andeven semantic oncepts.
Some of these techniquesare aimed primarily at identifying multi-word termsthat have come to function like ordinary words, forexample "white collar" or "electric car", and cap-turing other co-occurrence idiosyncrasies associatedwith certain types of texts.
This simple approachhas proven quite effective for some systems, for ex-ample the Cornell group reported (Buckley et al,1995) that adding simple collocations to the list ofavailable terms can increase retrieval precision by asmuch as 10%.Other more advanced techniques of phrase ex-traction, including extended N-grams and syn-tactic parsing, attempt to uncover "concepts",which would capture underlying semantic uniformityacross various surface forms of expression.
Syntac-tic phrases, for example, appear reasonable indi-cators of content, arguably better than proximity-based phrases, since they can adequately deal withword order changes and other structural variations(e.g., "college junior" vs. "junior in college" vs. "ju-nior college").
A subsequent regularization process,1in a vector-space model term weights are representedas coordinate values; in a probabilistic model estimatesof prior probabilities are used.140where alternative structures are reduced to a "nor-mal form", helps to achieve the desired uniformity,for example, "college+junior" will represent a col-lege for juniors, while "junior+college" will representa junior in a college.
A more radical normalizationwould have also "verb object", "noun rel-clause",etc.
converted into collections of such ordered pairs.This head+modifier normalization has been used inour system, and is further described in this paper.In order to obtain the head+modifier pairs of re-spectable quality, we used a full-scale robust syntac-tic parsing (TTP) In 1998, in collaboration with theUniversity of Helsinki, we used their Functional De-pendency Grammar system to perform all linguis-tic analysis of TREC data and to derive multipledependency-based in exing streams.S t ream-based  In fo rmat ion  Ret r ieva lMode lThe stream model was conceived to facilitate a thor-ough evaluation and optimization of various textcontent representation methods, including simplequantitative techniques as well as those requiringcomplex linguistic processing.
Our system encom-passes a number  of statistical and  natural languageprocessing techniques that capture different aspectsof document  content: combin ing these into a coher-ent whole  was  in itself a ma jor  challenge.
There-fore, we  designed a distributed representation mode lin which alternative methods  of document  indexing(which we call "streams") are strung together to per-form in parallel.
St reams are built using a mixture ofdifferent indexing approaches, term extracting andweighting strategies, even different search engines.The  final results are produced by merg ing rankedlists of documents  obtained f rom searching allstreams with appropriately preprocessed queries,i.e., phrases for phrase stream, names  for namesstream, etc.
The  merg ing process weights contribu-tions f rom each stream using a combinat ion that wasfound the most  effective in training runs.
This allowsfor an easy combinat ion of alternative retrieval androuting methods,  creating a meta-search strategywhich max imizes  the contribution of each stream.Among the advantages of the stream architecturewe may include the following:?
stream organization makes  it easier to comparethe contributions of different indexing features orrepresentations.
For example,  it is easier to designexperiments which allow us to decide if a certainrepresentation adds information which is not con-tributed by other streams.?
it provides a convenient testbed to exper imentwith algorithms designed to merge the resultsobtained using different IR engines and/or tech-niques.?
it becomes easier to fine-tune the system in orderto obtain optimum performance?
it allows us to use any combination of IR engineswithout having to adapt them in any way.Advanced L ingu is t i c  S t reamsHead+Modi f ie r  Pa i rs  S t reamOur linguistically most advanced stream is thehead+modifier pairs stream.
In this stream, docu-ments are reduced to collections of word pairs de-rived via syntactic analysis of text followed by anormalization process intended to capture seman-tic uniformity across a variety of surface forms,e.g., "information retrieval", "retrieval of informa-tion", "retrieve more information", "informationthat is retrieved", etc.
are all reduced to "re-trieve+information" pair, where "retrieve" is a heador operator, and "information" is a modifier or ar-gument.
It has to be noted that while the head-modifier relation may suggest semantic dependence,what we obtain here is strictly syntactic, eventhough the semantic relation is what we are reallyafter.
This means in particular that the inferencesof the kind where a head+modifier s taken as a spe-cialized instance of head, are inherently risky, be-cause the head is not necessarily a semantic head,and the modifier is not necessarily a semantic modi-fier, and in fact the opposite may be the case.
In theexperiments that we describe here, we have gener-ally refrained from semantic interpretation of head-modifier relationship, treating it primarily as anordered relation between otherwise equal elements.Nonetheless, even this simplified relationship has al-ready allowed us to cut through a variety of sur-face forms, and achieve what we thought was a non-trivial level of normalization.
The apparent lack ofsuccess of linguistically-motivated in exing in infor-mation retrieval may suggest hat we haven't stillgone far enough.In our system, the head+modifier pairs stream isderived through a sequence of processing steps thatinclude:1.
Part-of-speech tagging2.
Lexicon-based word normalization (extended"stemming" )3.
Syntactic analysis with TTP  parser4.
Extraction of head+modifier pairs1415.
Corpus-based disambiguation of long nounphrasesSyntactic analysis with  TTP  Parsing revealsfiner syntactic relationships between words andphrases in a sentence, relationships that are hardto determine accurately without a comprehensivegrammar.
Some of these relationships do conveysemantic dependencies, e.g., in Poland is attackedby Germany the subject+verb and verb+object re-lationships uniquely capture the semantic relation-ship of who attacked whom.
The surface word-orderalone cannot be relied on to determine which rela-tionship holds.
From the onset, we assumed thatcapturing semantic dependencies may be critical foraccurate text indexing.
One way to approach thisis to exploit the syntactic structures produced by afairly comprehensive parser.TTP  (Tagged Text Parser) is based on the Lin-guistic String Grammar developed by Sager (Sager1981) .
The parser currently encompasses some 400grammar productions, but it is by no means com-plete.
The parser's output is a regularized parse treerepresentation f each sentence, that is, a represen-tation that reflects the sentence's logical predicate-argument structure.
For example, logical subjectand logical object are identified in both passive andactive sentences, and noun phrases are organizedaround their head elements.
The parser is equippedwith a powerful skip-and-fit recovery mechanismthat allows it to operate ffectively in the face of ill-formed input or under a severe time pressure.
TTPhas been shown to produce parse structures whichare no worse than those generated by full-scale lin-guistic parsers when compared to hand-coded Tree-bank parse trees (Strzalkowski and Scheyen 1996).Ext rac t ing  head+modi f ie r  pairs Syntacticphrases extracted from TTP  parse trees arehead+modifier pairs.
The head in such a pair is acentral element of a phrase (main verb, main noun,etc.
), while the modifier is one of the adjunct ar-guments of the head.
It should be noted that theparser's output is a predicate-argument structurecentered around main elements of various phrases.The following types of pairs are considered: (1) ahead noun and its left adjective or noun adjunct,(2) a head noun and the head of its right adjunct,(3) the main verb of a clause and the head of its ob-ject phrase, and (4) the head of the subject phraseand the main verb.
These types of pairs account formost of the syntactic variants for relating two words(or simple phrases) into pairs carrying compatiblesemantic content.
This also gives the pair-basedrepresentation sufficient flexibility to effectively cap-ture content elements even in complex expressions.There are of course exceptions.
For example, thethree-word phrase "former Soviet president" wouldbe broken into two pairs "former president" and "So-viet president", both of which denote things thatare potentially quite different from what the originalphrase refers to, and this fact may have potentiallya negative ffect on retrieval precision.
This is oneplace where a longer phrase appears more appropri-ate.
Below is a small sample of head+modifier pairsextracted (proper names are not included):original text:While serving in South Vietnam, a number of U.S. Soldierswere reported as having been exposed to the defoliant AgentOrange.
The issue is veterans entitlement, or the award-ing of monetary compensation and/or medical assistancefor physical damages caused by Agent Orange.head+modif ier  pairs:damage-t-physical, causeq-damage, award-q-assist, award---{-compensate, compensate-q-monetary, ssist-l-medical, en-title-.I-veteranCorpus-based disambiguation of long nounphrases The notorious structural ambiguity ofnominal compounds remains a serious difficulty inobtaining quality head-modifier pairs.
What itmeans is that word order information cannot be reli-ably used to determine relationships between wordsin complex phrases, which is required to decomposelonger phrases into meaningful head+modifier pairs.In order to cope with ambiguity, the pair extractorlooks at the distribution statistics of the compoundterms to decide whether the association between anytwo words (nouns and adjectives) in a noun phrase isboth syntactically valid and semantically significant.For example, we may accept language+natural andprocessing+language from "natural language pro-cessing" as correct, however, case+trading wouldmake a mediocre term when extracted from "insidertrading case".
On the other hand, it is important oextract trading+insider to be able to match docu-ments containing phrases "insider trading sanctionsact" or "insider trading activity".
Phrasal termsare extracted in two phases.
In the first phase,only unambiguous head-modifier pairs are gener-ated, while all structurally ambiguous noun phrasesare passed to the second phase "as is".
In the sec-ond phase, the distributional statistics gathered inthe first phase are used to predict he strength of al-ternative modifier-modified links within ambiguousphrases.
For details, the reader is referred to (Strza-lkowski et al 1995).142Simple Noun Phrase StreamIn contrast to the elaborate process of generatingthe head+modifier pairs, unnormalized noun groupsare collected from part-of-speech tagged text using afew regular expression patterns.
No attempt is madeto disambiguate, normalize, or get at the internalstructure of these phrases, other than the stemmingwhich has been applied to text prior to the phraseextraction step.
The following phrase patterns havebeen used, with phrase length arbitrarily limited tothe maximum 7 words:1. a sequence of modifiers (adjectives, participles,etc.)
followed by at least one noun, such as: "cry-onic suspension", "air traffic control system";2. proper noun sequences modifying a noun, such as:"u.s. citizen", "china trade";3. proper noun sequences (possibly containing '&'):"warren commission", "national air traffic con-troller".The motivation for having a phrase stream issimilar to that for head+modifier pairs since bothstreams attempt to capture significant multi-wordindexing terms.
The main difference is the lack ofnormalization, which makes the comparison betweenthese two streams particularly interesting.Name StreamProper names, of people, places, events, organiza-tions, etc., are often critical in deciding relevanceof a document.
Since names are traditionally cap-italized in English text, spotting them is relativelyeasy, most of the time.
Many names are composed ofmore than a single word, in which case all words thatmake up the name are capitalized, except for prepo-sitions and such, e.g., The United States of America.It is important hat all names recognized in text, in-cluding those made up of multiple words, e.g., SouthAfrica or Social Security, are represented as tokens,and not broken into single words, e.g., South andAfrica, which may turn out to be different namesaltogether by themselves.
On the other hand, weneed to make sure that variants of the same nameare indeed recognized as such, e.g., U.S. PresidentBill Clinton and President Clinton, with a degree ofconfidence.
One simple method, which we use in oursystem, is to represent a compound name dually, asa compound token and as a set of single-word terms.This way, if a corresponding full name variant can-not be found in a document, its component wordsmatches can still add to the document score.Other StreamsStems Stream The stems stream is the simplest,yet the most effective of all streams, a backboneof the multistream model.
It consists of stemmedsingle-word tokens (plus hyphenated phrases) takendirectly from the document ext (exclusive of stop-words).
The stems stream provides the most com-prehensive, though not very accurate, image of thetext it represents, and therefore it is able to out-perform other streams that we used thus far.
Webelieve however, that this representation model hasreached its limits, and that further improvement canonly be achieved in combination with other text rep-resentation methods.
This appears consistent withthe results reported at TREC.Unstemmed-Word  St ream In some experi-ments, notably in routing where incoming doc-uments are assigned to one or more "standing"queries, or profiles, we used also a plain text stream.This stream was obtained by indexing the text of thedocuments "as is" without stemming or any otherprocessing and running the unprocessed text of thequeries against that index.
The purpose of havingthis stream was to see if and when the lexical form ofwords can help to increase precision, while possiblysacrificing recall in some types of queries.
In routing,where queries are extensively tuned through train-ing, having multiple word forms allows, in theory atleast, finer~grained adjustments.Fragments Stream For the routing experimentswe also used a stream of fragments.
This was the re-sult of splitting the documents of the stems streaminto fragments of constant length (1024 characters)and indexing each fragment as if it were a separatedocument.
The queries used with this stream werethe same as with the stems stream.
Unlike in theregular stream, where the entire documents were re-trieved, here each document fragment was scoredand ranked independently.
The rank of a documentwas determined by the highest-scoring fragment con-tained by this document.
This stream was motivatedby the large body of work into passage-level retrieval(Callan 1994), (Kwok et al 1993), and its primarypurpose was to provide a benchmark for the localitystream described next.Stream Merg ing and Weight ingThe results obtained from different streams are listsof documents ranked in order of relevance: thehigher the rank of a retrieved document, the morerelevant it is presumed to be.
In order to obtain thefinal retrieval result, ranking lists obtained from eachstream have to be combined together by a process143Table 2: Precision improvements over stems-only re-trievalshort queries long queriesStreams merged ~ change ~ changeAll s t reams ..1.5.4 ..1.20.94Stems-t -Phrases+Pairs  ..f-6.6 ..1..1.22.85S tems+Phrases  ..1.7.0 ..1.24.94S tems+Pa i rs  ..1.2.2 ..1..1.15.27S tems+Names ..1.0.6 ..1,.1,2.59known as merging or fusion.
The final ranking is de-rived by calculating the combined relevance scoresfor all retrieved ocuments.
The following are theprimary factors affecting this process:1. document relevancy scores from each stream2.
retrieval precision distribution estimates withinranks from various streams, e.g., projected pre-cision between ranks 10 and 20, etc.;3.
the overall effectiveness of each stream (e.g.
mea-sured as average precision on training data)4. the number of streams that retrieve a particulardocument, and5.
the ranks of this document within each stream.Generally, a stronger (i.e., better performing)stream will more effect on shaping the final ranking.A document which is retrieved at a high rank fromsuch a stream is more likely to end up ranked highin the final result.
In addition, the performance ofeach stream within a specific range of ranks is takeninto account.
For example, if phrases tream tendsto pack relevant documents between the top 10thand 20th retrieved ocuments (but not so much into1-10) we would give premium weights to the docu-ments found in this region of phrase-based ranking,etc.
Table 2 gives some additional data on the ef-fectiveness of stream merging.
Further details areavailable in a TREC conference article (Strzalkowskiet al 1997).Note that long text queries benefit more from lin-guistic processing.TREC-7  par t i c ipat ionIn TREC-7, the GE/Rutgers/SICS/Helsinki teamhas performed runs in the main ad-hoc task.
Weused two retrieval engines, SMART and InQuery,built into the stream model architecture.
The pro-cessing of TREC data was performed at Helsinki us-ing the commercial Functional Dependency Gram-mar (FDG) text processing toolkit.
Six linguisticstreams have been produced, as described below.Processed text streams were sent via ftp to Rutgersfor indexing using their version of Inquery system.Additionally, 4 steams produced by GE NLToolsetfor TREC-6 were reused in SMART indexing.Adhoc topics were processed at GE using bothautomatic and manual topic expansion.
We used theinteractive Query Expansion Tool to expand topicswith automatically generated summaries of top 30documents retrieved by the original topic.
Manualintervention was restricted to accept/reject decisionson summaries.
We observed time limit of 10 minutesper topic.Automatic topics expansion was done by replac-ing human summary selection by an automatic pro-cedure, which accepted only the summaries that ob-tained sufficiently high scores.Two sets of expanded topics (automatic and man-ual) were sent to Helsinki for NL processing, andthen on to Rutgers for retrieval.
Rankings were ob-tained from each stream index and then merged us-ing a combined strategy developed at GE and SICS.TREC-7  Submiss ions1.
SUMMARIZAT ION-BASED MANUALLY-ASS ISTEDTOPIC EXPANSION.
This multi-stream anual In-query run was produced with manually expandedtopics.
Summaries used in expansion were derivedfrom top-ranked ocuments retrieved by SMARTusing the initial topics (title and description fieldsonly).2.
SUMMARIZAT ION-BASED AUTOMATIC  TOP IC  EX-PANSION I.
This single-stream automatic Inqueryrun was produced with automatically expandedtopics.
Plain stems stream and syntactic nounphrase stream were combined and converted into asingle Inquery-syntax representation (tokens andquoted strings).3.
SUMMARIZAT ION-BASED AUTOMATIC  TOP IC  EX-PANSION II.
This multi-stream automatic run wasproduced using SMART rather than Inquery.
Au-tomatically expanded queries were NL processedusing GE NLToolset.He ls ink i  NLP  SystemWe used Helsinki's Functional Dependency Gram-mar (FDG) includes the EngCG-2 tagger and de-pendency syntax which links phrase heads to theirmodifiers and verbs to their complements and ad-juncts.FDG was applied to the whole corpus, with theoutput passed to the stream extractor.
The streamswere generated as follows:?
SIMPLE STREAMS1441.
STEM: just stemmed words, stopwords removed.2.
NAME: all proper names3.
AAN: simple noun phrases with attributives.Basically adjective-noun sequences minus someexceptions.?
D IRECT DEPENDENCY STREAMS1.
sv: subject-verb pairs where the subject is anoun phrase.2.
vo: verb-complement pairs.
The complementincludes objects and some object-like adverbialclasses.?
INDIRECT DEPENDENCY STREAMS1.
NOFN: N1 ... o\]  ... N2  pairs, where N1 and N2are heads of simple noun phrases.2.
sc: subject-complement pairs where the com-plement modifies the subject, e.g., f l owers  groww i ld  - w i ld+f lower .Topic Expansion ExperimentsIn this section we discuss a semi-interactive ap-proach to information retrieval which consists of twotasks performed in a sequence.
First, the system as-sists the searcher in building a comprehensive state-ment of information eed, using automatically gen-erated topical summaries of sample documents.
Sec-ond, the detailed statement of information eed isautomatically processed by a series of natural lan-guage processing routines in order to derive an op-timal search query for a statistical information re-trieval system.
We investigate the role of automateddocument summarization i  building effective searchstatements.In the opening section of this paper we arguedthat the quality of the initial search topic, or user'sinformation need statement is the ultimate factorin the performance of an information retrieval sys-tem.
This means that the query must provide a suf-ficiently accurate description of what constitutes therelevant information, as well as how to distinguishthis from related but not relevant information.
Wealso pointed out that today's NLP techniques arenot advanced enough to deal effectively with seman-tics and meaning, and instead they rely on syntacticand other surface forms to derive representations ofcontent.In order to overcome these limitations, many IRsystems allow varying degrees of user interactionthat facilitates query optimization and calibrationto closer match user's information seeking goals.
Apopular technique here is relevance feedback, wherethe user or the system judges the relevance of a sam-ple of results returned from an initial search, and thequery is subsequently rebuilt to reflect this informa-tion.
Automatic relevance feedback techniques canlead to a very close mapping of known relevant doc-uments, however, they also tend to overfit, which inturn reduces their ability of finding new documentson the same subject.
Therefore, a serious challengefor information retrieval is to devise methods forbuilding better queries, or in assisting user to doSO.Building effective search topicsWe have been experimenting with manual and auto-matic natural language query (or topic, in TRECparlance) building techniques.
This differs frommost query modification techniques used in IR inthat our method is to reformulate the user's state-ment of information eed rather than the search sys-tem's internal representation f it, as relevance feed-back does.
Our goal is to devise a method of full-text expansion that would allow for creating exhaus-tive search topics such that: (1) the performanceof any system using the expanded topics would besignificantly better than when the system is run us-ing the original topics, and (2) the method of topicexpansion could eventually be automated or semi-automated so as to be useful to a non-expert user.Note that the first of the above requirements effec-tively calls for a free text, unstructured, but highlyprecise and exhaustive description of user's searchstatement.
The preliminary results from TRECevaluations how that such an approach is indeedvery effective.One way to view query expansion is to make theuser query resemble more closely the documents it isexpected to retrieve.
This may include both content,as well as some other aspects uch as composition,style, language type, etc.
If the query is indeed madeto resemble a "typical" relevant document, then sud-denly everything about this query becomes a validsearch criterion: words, collocations, phrases, var-ious relationships, etc.
Unfortunately, an averagesearch query does not look anything like this, mostof the time.
It is more likely to be a statement speci-fying the semantic riteria of relevance.
This meansthat except for the semantic or conceptual resem-blance (which we cannot model very well as yet)much of the appearance of the query (which we canmodel reasonably well) may be, and often is, quitemisleading for search purposes.
Where can we getthe right queries?In today's information retrieval, query expansionusually is typically limited to adding, deleting or145re-weighting of terms.
For example, content termsfrom documents judged relevant are added to thequery while weights of all terms are adjusted in or-der to reflect the relevance information.
An alter-native to term-only expansion is a full-text expan-sion described in (Strzalkowski et al 1997).
In thisapproach, search topics are expanded by pasting inentire sentences, paragraphs, and other sequencesdirectly from any text document.
To make this pro-cess efficient, an initial search is performed with theunexpanded queries and the top N (10-30) returneddocuments are used for query expansion.
Thesedocuments, irrespective of their overall relevancy tothe search topic, are scanned for passages contain-ing concepts referred to in the query.
The result-ing expanded queries undergo further text process-ing steps, before the search is run again.
We needto note that the expansion material was found inboth relevant and non-relevant documents, benefit-ing the final query all the same.
In fact, the presenceof such text in otherwise non-relevant documentsunderscores the inherent limitations of distribution-based term reweighting used in relevance feedback.Summar izat ion -based  top ic  expans ionWe used our automatic text summarizer to de-rive query-specific summaries of documents returnedfrom the first round of retrieval.
The summarieswere usually 1 or 2 consecutive paragraphs selectedfrom the original document text.
The initial purposewas to show to the user, by the way of a quick-readabstract, why a document has been retrieved.
If thesummary appeared relevant and moreover capturedsome important aspect of relevant information, thenthe user had an option to paste it into the query,thus increasing the chances of a more successful sub-sequent search.
Note again that it wasn't importantif the summarized ocuments were themselves rele-vant, although they usually were.The topic expansion interaction proceeds as fol-lows:1.
The initial natural anguage statement of informa-tion need is submitted to SMART-based NLIR re-trieval engine via a Query Expansion Tool (QET)interface.
The statement is converted into an in-ternal search query and run against the TRECdatabase.
22.
NLIR returns top N (=30) documents from thedatabase that match the search query.2TREC-6 database consisted of approx.
2 GBytes ofdocuments from Associated Press newswire, Wall StreetJournal, Financial Times, Federal Register, FBIS andother sources (Harman & Voorhees 1998)..
The user determines a topic for the summarizer.By default, it is the title field of the initial searchstatement (see below).4.
The summarizer is invoked to automatically sum-marize each of the N documents with respect othe selected topic.5.
The user reviews the summaries (spending ap-prox.
5-15 seconds per summary) and de-selectsthese that are not relevant to the search state-ment.6.
All remaining summaries are automatically at-tached to the search statement..
The expanded search statement is passed througha series of natural language processing steps andthen submitted for the final retrieval.Imp lementat ion  and  eva luat ionWe have developed an automatic text summarizeras part of our Tipster Phase I I I  contract.
This workis described in a separate paper included in this vol-ume.We have included the summarizer as a helper ap-plication within the user interface to the natural an-guage information retrieval system.
In this applica-tion, the summarizer is used to derive query-relatedsummaries of documents returned from databasesearch.
The summarization method used here is thesame as for generic summaries described thus far,with the following exceptions:1.
The passage-search "query" is derived from theuser's document search query rather than fromthe document itle.2.
The distance of a passage from the beginningof the document is not considered towards itssummary-worthiness.The topical summaries are read by the users toquickly decide their relevance to the search topicand, if desired, to expand the initial informationsearch statement in order to produce a significantlymore effective query.
The following example showsa topical (query-guided summary) and compares itto the generic summary (we abbreviate SGML forbrevity).IN IT IAL  SEARCH STATEMENT:< t i t le  > Evidence of I ranian support  for Lebanese hostagetakers.< desc > Document  will give data  linking Iran to groupsin Lebanon which seize and hold Western hostages.F IRST RETRIEVED DOCUMENT (T ITLE) :146Table 3: Performance improvement for expandedqueriesqueries: original or iginal expanded expandedSYSTEM SMART NLIR SMART NLIRPRECIS IONAverage 0.1429 0.1837 0.2672 0.2859%change +28.5  +87.0  +100.0At 10 docs 0.3000 0.3840 0.5060 0.5200%change +28.0  +68.6  +73.3At 30 docs 0.2387 0.2747 0.3887 0.3940%change +15.0  +62.8  +65.0At 100 doc 0.1600 0.1736 0.2480 0.2574%change +8.5  +55.0  +60.8Recall  0.57 0.53 0.61 0.62%change -7.0 +7.0  +8.7Arab  Hi jackers '  Demands  Simi lar  To Those of Hostage-Takers in LebanonSUMMARIZER TOPIC :Ev idence of I ran ian suppor t  for Lebanese hostage takersTOP ICAL  SUMMARY (used for expansion):Mugniyeh,  36, is a key f igure in the secur i ty  apparatus  ofHezbol lah,  or Par ty  of God,  an I ran ian-backed Shiite move-ment  bel ieved to be the umbre l la  for fact ions holding mostof the 22 foreign hostages in Lebanon.GENERIC  SUMMARY (for compar ison) :The demand made by hi jackers of a Kuwai t i  jet  is the sameas that  made by Moslems hold ing Amer icans  hostage inLebanon - f reedom for 17 pro- I ran ian extremists  ja i led inKuwai t  for bombing  U.S. and  French embassies there in1983.PART IALLY  EXPANDED SEARCH STATEMENT:< t it le > Ev idence of I ran ian suppor t  for Lebanese hostagetakers.< desc > Document  will give data  l inking I ran to groupsin Lebanon which seize and  hold Western  hostages.< expd > Mugniyeh,  36, is a key f igure in the secur i tyapparatus  of Hezbol lah,  or Par ty  of God,  an I ran ian-backedShi ite movement  bel ieved to be the umbre l la  for fact ionshold ing most  of the 22 foreign hostages in Lebanon.TREC Eva luat ion  Resu l tsTable 3 lists selected runs performed with theNLIR system on TREC-6 database using 50 queries(TREC topics) numbered 301 through 350.
Theexpanded query runs are contrasted with runs ob-tained using TREC original topics using NLIR aswell as Cornell's SMART (version 11) which serveshere as a benchmark.
The first two columns areautomatic runs, which means that there was no hu-man intervention in the process at any time.
Sincequery expansion requires human decision on sum-mary selection, these runs (columns 3 and 4) areclassified as "manual", although most of the processis automatic.
As can be seen, query expansion pro-duces an impressive improvement in precision at alllevels.
Recall figures are shown at 1000 retrieveddocuments.Query expansion appears to produce consistentlyhigh gains not only for different sets of queries butalso for different systems: we asked other groupsparticipating in TREC to run search using our ex-panded queries, and they reported similarly largeimprovements.Finally, we may note that NLP-based indexing hasalso a positive effect on overall performance, but theimprovements are relatively modest, particularly onthe expanded queries.
A similar effect of reduced ef-fectiveness of linguistic indexing has been reportedalso in connection with improved term weightingtechniques.Automatic Topic ExpansionIn TREC-7 we started experimenting with com-pletely automated topic expansion.
We used thesame approach to expansion as outlined below withthe following modifications:1.
Top 100 documents retrieved by the initial, un-expanded topic are summarized, rather than 30used in manual mode.
This is because we needto rely on a strict notion of topicality of the sum-mary, and therefore must look at more documentsto obtain any expansion.
From a user's perspec-tive, this is entirely transparent, however.2.
We replace human selection of expanding sum-maries by an automatic functions that measurethe overlap between the summary and the topic.This overlap, measured over content terms (i.e.,with exclusion of common words and certain otherwords), should be high enough to prevent falsematches, while not too high to allow for topic vari-ants to be matched..
The summary parameters (i.e., length, spread,etc) is set to normalize its size in such as wayas to support effective topicality detection.
Forexample, straight 10short documents (too short!
)or for very long documents (too long!
).Preliminary tests conducted using TREC-6 datashowed a significant increase in precision over un-expanded queries, although still not as large as inmanual expansion.
These experiments require con-tinuation.ConclusionsWe have developed a method to derive quick-readsummaries from news-like texts using a number ofshallow NLP and simple quantitative techniques.The summary is assembled out of passages extractedfrom the original text, based on a pre-determinedDMS template.
This approach as produced a veryefficient and robust summarizer for news-like texts.147We used the summarizer, via the QET inter-face, to build effective search queries for an informa-tion retrieval system.
This has been demonstratedto produce dramatic performance improvements inTREC evaluations.
We believe that this query ex-pansion approach will also prove useful in searchingvery large databases where obtaining a full indexmay be impractical or impossible, and accurate sam-pling will become critical.Information Retrieval has made tremendous ad-vances over the last 30 years in terms of accuracy,efficiency and robustness.
It has also been widelycommercialized in recent years, particularly on theInternet.
In spite of this progress, many challengesremain, and more research is needed to achieve per-formance levels that would approach human-level ac-curacy.
We believe that this requires a tighter inte-gration of NLP.Acknowledgements  We would like to acknowl-edge significant contributions to this project fromthe following people: Jose Perez-Carballo, LouiseGuthrie, Fang Lin, Jussi Karlgren, Pasi Tapanainen,Timo Jarvinen, Jim Leistensnider, Troy Straszheim,and Wang Jin.
We thank Chris Buckley and DonnaHarman for providing their IR engines (SMART andPrise) for this research.
This paper is based uponwork supported in part by the Defense Advanced Re-search Projects Agency under Tipster Phase-3 Con-tract 97-F157200-000.ReferencesCallan, Jamie.
1994.
"Passage-Level Evidence in Document Re-trieval."
Proceedings of ACM SIGIR'94.
pp.
302-310.Harman, Donna, and Ellen Voorhees (eds).
1998.
The Text Re-trieval Conference (TREC-6).
NIST Special Publication (to ap-pear).Kwok, K.L., L. Papadopoulos and Kathy Y.Y.
Kwan.
1993.
"Re-trieval Experiments with a Large Collection using PIRCS."
Pro-ceedings of TREC-1 conference, NIST special publication 500-207, pp.
153-172.Sager, Naomi.
1981.
Natural Language Information Processing.Addison-Wesley.Strzalkowski, Tomek, Louise Guthrie, Jussi Karlgren, Jim Leis-tensnider, Fang Lin, Jose Perez-Carballo, Troy Straszheim, JinWang, and Jon Wilding.
1997.
"Natural Language InformationRetrieval: TREC-5 Report."
Proceedings of TREC-5 confer-ence.Strzalkowski, Tomek and Jose Perez Carballo.
1994.
"RecentDevelopments in Natural Language Text Retrieval."
Proceed-ings of the First Text REtrieval Conference (TREC-2), NISTSpecial Publication 500-215, National Institute of Standardsand Technology, Gaithersburg, MD.
pp.
123-136.Strzalkowski, Tomek, Jose Perez-Carballo and Mihnea Mari-nescu.
1995.
"Natural Language Information Retrieval: TREC-3 Report."
Proceedings of the Third Text REtrieval Conference(TREC-3), NIST Special Publication 500-225, pp.
39-53.Strzalkowski, Tomek, Jose Perez-Carballo and Mihnea Mari-nescu.
1996.
"Natural Language Information Retirievah TREC-4 Report."
Proceedings of the Fourth Text REtrieval Conference(TREC-4), NIST Special Publication 500-236.Strzalkowski, Tomek.
1995.
"Natural Language Information Re-trieval" Information Processing and Management, Vol.
31, No.3, pp.
397-417.
Pergamon/Elsevier.Strzalkowski, Tomek, and Peter Scheyen.
1996.
"An Evaluationof TTP Parser: a preliminary report."
In H. Bunt, M. Tomita(eds), Recent Advances in Parsing Technology, Kluwer Aca-demic Publishers, pp.
201-220.Strzalkowski, Tomek, Fang Lin, Jose Perez-Carballo, and JinWang.
1997.
"Natural Language Information Retrieval: TREC-6 Report."
Proceedings of TREC-6 conference.148
