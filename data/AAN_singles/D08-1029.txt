Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 274?283,Honolulu, October 2008. c?2008 Association for Computational LinguisticsWho is Who and What is What:Experiments in Cross-Document Co-ReferenceAlex BaronBBN Technologies10 Moulton StreetCambridge, MA 02138abaron@bbn.comMarjorie FreedmanBBN Technologies10 Moulton StreetCambridge, MA 02138mfreedma@bbn.comAbstractThis paper describes a language-independent,scalable system for both challenges of cross-document co-reference: name variation andentity disambiguation.
We provide system re-sults from the ACE 2008 evaluation in bothEnglish and Arabic.
Our English system?s ac-curacy is 8.4% relative better than an exactmatch baseline (and 14.2% relative better overentities mentioned in more than one docu-ment).
Unlike previous evaluations, ACE2008 evaluated both name variation and entitydisambiguation over naturally occurringnamed mentions.
An information extractionengine finds document entities in text.
We de-scribe how our architecture designed for the10K document ACE task is scalable to aneven larger corpus.
Our cross-document ap-proach uses the names of entities to find aninitial set of document entities that could referto the same real world entity and then uses anagglomerative clustering algorithm to disam-biguate the potentially co-referent documententities.
We analyze how different aspects ofour system affect performance using ablationstudies over the English evaluation set.
In ad-dition to evaluating cross-document co-reference performance, we used the results ofthe cross-document system to improve the ac-curacy of within-document extraction, andmeasured the impact in the ACE 2008 within-document evaluation.1 IntroductionCross-document entity co-reference is the problemof identifying whether mentions from differentdocuments refer to the same or distinct entities.There are two principal challenges: the same entitycan be referred to by more than one name string(e.g.
Mahmoud Abbas and Abu Mazen) and thesame name string can be shared by more than oneentity (e.g.
John Smith).
Algorithms for solving thecross-document co-reference problem are neces-sary for systems that build knowledge bases fromtext, question answering systems, and watch listapplications.There are several challenges in evaluating anddeveloping systems for the cross-document co-reference task.
(1) The annotation process requiredfor evaluation and for training is expensive; an an-notator must cluster a large number of entitiesacross a large number of documents.
The annotatormust read the context around each instance of anentity to make reliable judgments.
(2) On randomlyselected text, a baseline of exact string match willdo quite well, making it difficult to evaluate pro-gress.
(3) For a machine, there can easily be a scal-ability challenge since the system must cluster alarge number of entities.Because of the annotation challenges, manyprevious studies in cross-document co-referencehave focused on only the entity disambiguationproblem (where one can use string retrieval to col-lect many documents that contain same name); orhave used artificially ambiguated data.Section 2 describes related work; section 3 in-troduces ACE, where the work was evaluated; sec-tion 4 describes the underlying informationextraction engine; sections 5 and 6 address thechallenges of coping with name variation and dis-ambiguating entities; sections 7, 8, and 9 presentempirical results, improvement of entity extraction274within documents using cross-document corefer-ence, and a difference in performance on personversus organization entities.
Section 10 discussesthe scalability challenge.
Section 11 concludes.2 Related WorkPerson disambiguation given a person namestring.
Bagga and Baldwin (1998b) produced oneof the first works in cross-document co-reference.Their work presented a vector space model for theproblem of entity disambiguation, clustering 197articles that contained the name ?John Smith?.Participants in the 2007 Sem-Eval Web PeopleSearch(WEPS) task clustered 100-document setsbased on which person a name string of interestreferenced.
WEPS document sets were collectedby selecting the top 100 web search results to que-ries about a name string (Artiles, et al, 2007).Mann and Yarowsky (2003) and Gooi andAllan (2004) used artificially ambiguous data toallow for much larger experiments in clusteringdocuments around a known person of interest.Clustering different variants of the same name.Lloyd et.
al (2006) use a combination of ?morpho-logical similarity?
and ?contextual similarity?
tocluster name variants that refer to the same entity.Clustering and disambiguation.
The John Hop-kins 2007 Summer Workshop produced a cross-document annotated version of the ACE 2005 cor-pus (18K document entities, 599 documents) con-sisting of 5 entity types (Day, et.
al, 2007).
Therewas little ambiguity or variation in the corpus.
Par-ticipants demonstrated that disambiguation im-provements could be achieved with a Metropolis-Hastings clustering algorithm.
The study assumedhuman markup of document-level entities.Our work.
The work reported in this paper ad-dresses both entity clustering and name variationfor both persons and organizations in a corpus of10K naturally occurring documents selected to befar richer than the ACE 2005 data by NIST andLDC.
We investigated a new approach in bothEnglish and Arabic, and evaluated on document-level entities detected by information extraction.3 ACE EvaluationNIST?s ACE evaluation measures system perform-ance on a predetermined set of entities, relations,and events.
For the 2008 global entity detectionand recognition task (GEDR)1, system perform-ance was measured on named instances of personand organization entities.
The GEDR task was runover both English and Arabic documents.
Partici-pants processed over 10K documents for each lan-guage.
References were produced for about 400documents per language (NIST, 2008).
The evalua-tion set included documents from several genresover a 10 year time period.
Document counts areprovided in Table 1.
This evaluation differed fromprevious community cross-document coreferenceevaluations in that it (a) covered both organizationsand people; (b) required processing a relativelylarge data set; (c) evaluated entity disambiguationand name variation simultaneously; and (d) meas-ured cross-document co-reference over system-detected document-level entities and mentions.English Arabicbroadcast conversation 8 38broadcast news  72 19meeting  18 ---newswire 237 314telephone 18 12usenet 15 15weblog 47 14Table 1: Documents per genre in ACE2008 test setThe evaluation set was selected to include in-teresting cases for cross-document co-reference(e.g cases with spelling variation and entities withshared names).
This is necessary because annota-tion is difficult to produce and naturally sampleddata has a high percentage of entities resolvablewith string match.
The selection techniques wereunknown to ACE participants.4 Extraction System OverviewOur cross-document co-reference system relies onSERIF, a state-of-the-art information extraction(IE) system (Ramshaw, et.
al, 2001) for document-level information extraction.
The IE system usesstatistically trained models to detect and classifymentions, link mentions into entities, and detectand classify relations and events.
English and Ara-bic SERIF share the same general models, al-though there are differences in the specific featuresused by the models.
Arabic SERIF does not per-form event detection.
While Arabic SERIF does1NIST?s evaluation of cross-document co-reference.275make use of some morphological features, thecross-document co-reference system, which fo-cused specifically on entity names, does not usethese features.Figure 1 and Figure 2 illustrate the architectureand algorithms of the cross-document co-referencesystem respectively.
Our system separately ad-dresses two aspects of the cross-document co-reference problem: name variation (Section  5) andentity disambiguation (Section  6).
This leads to ascalable solution as described in Section  10.Figure 1: Cross-document Co-reference ArchitechureThe features used by the cross-document co-reference system can be divided into four classes:World Knowledge (W), String Similarity (S), Pre-dictions about Document Context (C), and Meta-data (M).
Name variation (V) features operate overunique corpus name strings.
Entity disambiguationfeatures (D) operate over document-level entityinstances.
During disambiguation, the agglomera-tive clustering algorithm merges two clusters whenconditions based on the features are met.
For ex-ample, two clusters are merged when they share atleast half the frequently occurring nouns that de-scribe an entity (e.g.
president).
As shown inTable 2, features from the same class were oftenused in both variation and disambiguation.
Allclasses of features were used in both English andArabic.
Because very little training data was avail-able, both the name variation system and the dis-ambiguation system use manually tuned heuristicsto combine the features.
Tuning was done usingthe ACE2008 pilot data (LDC, 2008b), documentsfrom the SemEval WEPS task (Artiles, et al,2007), and some internally annotated documents.Internal annotation was similar in style to theWEPS annotation and did not include full ACEannotation.
Annotators simply clustered documentsbased on potentially confusing entities.
Internalannotation was done for ~100 names in both Eng-lish and Arabic.Feature Class Stage ClassWikipedia knowledge D, V WWeb-mined aliases V WWord-based similarity  D, V SCharacter-based similarity V STranslation dictionaries V SCorpus Mined Aliases D, V CSERIF extraction D,V CPredicted Document Topics D CMetadata (source, date, etc.)
D MTable 2: Features for Cross-Document Co-Reference5 Name VariationThe name variation component (Block 1 of Figure1) collects all name strings that appear in thedocument set and provides a measure of similaritybetween each pair of name strings.2 Regions (A)and (B) of Figure 2 illustrate the input and outputof the name variation component.This component was initially developed forquestion answering applications, where whenasked the question ?Who is George Bush??
relevantanswers can refer to both George W and GeorgeHW (the question is ambiguous).
However whenasked ?Who leads al Qaeda??
the QA system mustbe able to identify spelling variants for the name alQaeda.
For the cross-document co-reference prob-lem, separating the name variation componentfrom the disambiguation component improves thescalability of the system (described in Section  10).The name variation component makes use of avariety of features including web-mined alias lists,aliases mined from the corpus (e.g ?John aka J?
),statistics about the relations and co-reference deci-sions predicted by SERIF, character-based editdistance, and token subset trees.
The token subsettrees algorithm measures similarity using wordoverlap by building tree-like structures from theunique corpus names based on overlapping tokens.Translation dictionaries (pulled from machine2For the majority of pairs, this similarity score will be 0.InputDocumentsIE SystemCross-DocumentName VariationEntityFeaturizerName SimilarityDBEntity-basedFeature DBClusters DBWorldKnowledge DBOutputDocumentsEntityDisambiguationInformationExtraction DB(1)(2)276translation training and cross-language links inWikipedia) account for names that have a canoni-cal form in one language but may appear in manyforms in another language.Figure 2: Cross-document Co-reference ProcessThe features are combined with hand-tunedweights resulting in a unidirectional similarityscore for each pair of names.
The similarity be-tween two name strings is also influenced by thesimilarity between the contexts in which the twonames appear (for example the modifiers or titlesthat precede a name).
This information allows thesystem to be more lenient with edit distance whenthe strings appear in a highly similar context, forexample increasing the similarity score between?Iranian President Ahmadinejad?
and ?IranianPresident Nejad.
?6 Entity DisambiguationWe use a complete link agglomerative cluster-ing algorithm for entity disambiguation.
To makeagglomerative clustering feasible over a 10Kdocument corpus, rather than clustering all docu-ment-level entities together, we run agglomerativeclustering over subsets of the corpus entities.
Foreach name string, we select the set of names thatthe variation component chose as valid variants.
InFigure 2 region C, we have selected MahmoudAbbas and 3 variants.We then run a three stage agglomerative clus-tering algorithm over the set of document entitiesthat include any of the name string variants or theoriginal name.
Figure 2 region D illustrates threedocument-level entities.The name variation links are not transitive, andtherefore a name string can be associated withmore than one clustering instance.
Furthermoredocument-level entities can include more than onename string.
However once a document-level en-tity has been clustered, it remains linked to entitiesthat were a part of that initial clustering.
Becauseof this, the order in which the algorithm selectsname strings is important.
We sort the name stringsso that those names about which we have the mostinformation and believe are less likely to be am-biguous are clustered first.
Name strings that aremore ambiguous or about which less information isavailable are clustered later.The clustering procedure starts by initializingsingleton clusters for each document entity, exceptthose document entities that have already partici-pated in an agglomerative clustering process.
Forthose entities that have already been clustered, theclustering algorithm retrieves the existing clusters.The merging decisions are based on the similar-ity between two clusters as calculated through fea-ture matches.
Many features are designed tocapture the context of the document in which enti-ties appear.
These features include the documenttopics (as predicted by the unsupervised topic de-tection system (Sista, et al, 2002), the publicationdate and source of a document, and the othernames that appear in the document (as predicted bySERIF).
Other features are designed to provideinformation about the specific context in which anentity appears for example: the noun phrases thatrefer to an entity and the relationships and eventsin which an entity participates (as predicted bySERIF).
Finally some features, such as theuniqueness of a name in Wikipedia are designed toprovide the disambiguation component with worldknowledge about the entity.
Since each clusterrepresents a global entity, as clusters grow throughmerges, the features associated with the clustersexpand.
For example, the set of associated docu-ment topics the global entity participates in grows.While we have experimented with statisticallylearning the threshold for merging, because of thesmall amount of available training data, thisthreshold was set manually for the evaluation.Abu Abbas, Abu Mazen, Adam Smith,A Smith, Andy Smith, Mahmoud Abbas,Muhammed Abbas ?.
(A) Name Strings:(B) Name StringPairs with Score:0.9 Mahmoud AbbasAbu Mazen0.7 Mahmoud AbbasAbu Abbas0.8 Mahmoud AbbasMuhammad Abbas?.
(C) Set of EquivalentName Strings:Abu Mazen,Mahmoud Abbas,Muhammed Abbas,Abu Abbas(D) Document EntityMentions:Palestinian President Mahmoud Abbas ... Abbas saidAbu Abbas was arrested ?
Abbas hijacked?
election of Abu Mazen?
(E) Entity Clusters:Abu MazenMahmoud AbbasPalestinian Leaderconvicted terroristMuhammed AbbasAbu Abbas277Clustering over these subsets of similar stringshas the additional benefit of limiting the number ofglobal decisions that are affected by a mistake inthe within-document entity linking.
For example, ifin one document, the system linked Hillary Clintonto Bill Clinton; assuming that the two names arenot chosen as similar variants, we are likely to endup with a cluster made largely of mentions ofHillary with one spurious mention of Bill and aseparate cluster that contains all other mentions ofBill.
In this situation, an agglomerative clusteringalgorithm that linked over the full set of document-level entities is more likely to be led astray andcreate a single ?Bill and Hillary?
entity.7 Experimental ResultsTable 3 and Table 4  include preliminary ACEresults3 for the highest, lowest, and average systemin the local and cross-document tasks respectively.While a single participant could submit more thanone entry, these numbers reflect only the primarysubmissions.
The ACE scorer maps system pro-duced entities to reference entities and producesseveral metrics.
For the within-document task,metrics include ACE Value, B3, and a variant ofB3 weighted to reflect ACE value weightings.
Forthe cross-document task, the B3 metric is replacedwith F (NIST, 2008).
ACE value has traditionallybeen the official metric of the ACE evaluation.
Itputs a higher cost on certain classes of entities (e.g.people are more important than facilities), certainclasses of mentions (e.g.
names are more importantthan pronouns), and penalizes systems for mistakesin type and subtype detection as well as linkingmistakes.
Assigning a mention to the wrong entityis very costly in terms of value score.
If the men-tion is a name, a system is penalized 1.0 for themissed mention and an additional 0.75 for a men-tion false alarm.
We will report ACE Value andvalue weighted B3/F.
Scores on the local task arenot directly comparable to scores on the globaltask.
The local entity detection and recognitiontask (LEDR) includes entity detection for five(rather than two) classes of entities and includespronoun and nominal (e.g.
?the group?)
mentions inaddition to names.3Results in this paper use v2.1 of the references and v17 ofthe ACE scorer.
Final results will be posted tohttp://www.nist.gov/speech/tests/ace/2008/English ArabicVal B3Val Val B3ValTop 52.6 71.5 43.6 69.1Average -53.3 50.0 17.3 47.6Low4 -269.1 25.8 -9.1 26.1BBN-A-edropt 52.1 71.5 43.0 68.9BBN-B-st-mg 52.6 71.5 43.6 69.1BBN-B-st-mg-fix557.2 77.4 44.6 71.3Table 3: ACE 2008 Within-Document Results (LEDR)English ArabicVal FVal Val FValTop 53.0 73.8 28.2 58.7Average 21.1 59.1 24.7 56.8Low -64.1 31.6 21.2 54.8BBN-B-med 53.0 73.8 28.2 58.7BBN-B-low 53.2 73.8 28.7 59.3BBN-B-med-fix5 61.7 77 31.4 60.1Table 4: ACE 2008 Cross-Document Results (GEDR)Our cross-document co-reference system usedBBN-A-edropt as input.
BBN-B-st-mg is the resultof using cross-document co-reference to improvelocal results (Section  9).
For cross-document co-reference, our primary submission, BBN-B-med,was slightly outperformed by an alternate systemBBN-B-low.
The two submissions differed only ina parameter setting for the topic detection system(BBN-B-low requires more documents to predict a?topic?).
BBN-A-st-mg-fix and BBN-B-med-fixare the result of post-processing the BBN output toaccount for a discrepancy between the training andevaluation material.5In addition to releasing results, NIST also re-leased the references.
Table 5 includes the ACEscore for our submitted English system and thescore when the system was run over only the 415documents with references.
The system performsslightly better when operating over the full docu-ment set.
This suggests that the system is usinginformation from the corpus even when it is notdirectly scored.4There was a swap in rank between metrics, so the low num-bers reflect two different systems.5There were discrepancies between the ACE evaluation andtraining material with respect to the portions of text thatshould be processed.
Therefore our initial system included anumber of spurious entities.
NIST has accepted revised outputthat removes these entities.
Experiments in this paper reflectthe corrected system.278FVal10K documents processed (415 scored)(BBN-B-med-fix)77Only 415 documents processed 76.3Table 5: Full English System ACE Evaluation ResultsWe have run a series of ablation experimentsover the 415 files in the English test set to evaluatethe effectiveness of different feature classes.
Theseexperiments were run using only the annotatedfiles (and not the full 10K document set).
We rantwo simple baselines.
The first baseline (?NoLink?)
does not perform any cross-document co-reference, all document entities are independentglobal entities.
The second baseline (?ExactMatch?)
links document-level entities using exactstring match.
We ran 6 variations of our system:o Configuration 1 is the most limited system.
Ituses topics and IE system output for disambigua-tion, and aliases mined from the documents forthe name variation component.o Configuration 2 includes Configuration 1 fea-tures with the addition of string similarity (editdistance, token subset trees) algorithms for thename variation stage.o Configuration 3 includes Configuration 2 fea-tures and adds context-based features (e.g.
titlesand premodifiers) for name variation.o Configuration 4 adds information from docu-ment metadata to the disambiguation component.o Configuration 5 adds web-mined information(alias lists, Wikipedia, etc.)
to both the variationand disambiguation components.
This is the con-figuration that was used for our NIST submission.o Configuration 5a is identical to Configuration5 except that the string-based edit distance wasremoved from the name variation component.As noted previously, the ACE collection wasselected to include challenging entities.
The selec-tion criteria of the corpus (which are not known byACE participants) can affect the importance of fea-tures.
For example, a corpus that included very fewtransliterated names would make less use of fea-tures based on edit distance.Figure 3 and Figure 4 show performance (withvalue weighted F) on the eight conditions over sys-tem predicted within-document extraction and ref-erence within-document extraction respectively.Figure 3 also includes configuration 5 run over all10K documents.
We provide two sets of results.The first evaluates system performance over allentities.
The relatively high score of the ?No Link?baseline indicates that a high percentage of thedocument-level entities in the corpus are only men-tioned in one document.
The second set of num-bers measures system performance on thoseentities appearing in more than one referencedocument.
While this metric does not give a com-plete picture of the cross-document co-referencetask (sometimes a singleton entity must be disam-biguated from a large entity that shares the samename); it does provide useful insights given thefrequency of singleton entities.System Document Level Entities30405060708090100Split AllExact Match 1 2 3 4 5a 55 (10kdocs)ConfigurationValueWeightedFAll EntitiesEntities in > 1DocumentsFigure 3: Performance on System Document EntitiesReference Document Level Entities30405060708090100Split All ExactMatch1 2 3 4 5a 5ConfigurationValueWeightedFAll EntitiesEntities in >1DocumentsFigure 4: Performance on Perfect Document EntitiesOverall system performance improved as fea-tures were added.
Configuration 1, which disam-biguated entities with a small set of features,performed worse than a more aggressive exactstring match strategy.
The nature of our agglom-erative clustering algorithm leads to entity mergesonly when there is sufficient evidence for themerge.
The relatively high performance of the ex-act match strategy suggests that in the ACE corpus,most entities that shared a name string referred to279the same entity, and therefore aggressive mergingleads to better performance.
As additional featuresare added, our system becomes more confident andmerges more document-level entities.With the addition of string similarity measures(Configuration 2) our system outperforms the exactmatch baseline.
The submitted results on systementities (Configuration 5) provide a 8.4% relativereduction in error over the exact match baseline.
Ifscored only on entities that occur in more than onedocument, Configuration 5 gives a 14.2% relativeredution in error over the exact match baseline.The context based features (Configuration 3) al-low for more aggressive edit-distance-based namevariation when two name strings frequently occurin the same context.
In Configuration 3, ?SheikHassan Nasrallah?
was a valid variant of ?HassanNasrallah?
because both name strings were com-monly preceded by ?Hezbollah leader?.
Similarly,?Dick Cheney?
became a valid variant of ?RichardBruce Cheney?
because both names were precededby ?vice president?.
In Configuration 2 the entitiesincluded in both sets of name strings had remainedunmerged because the strings were not consideredvalid variants.
With the addition of contextual in-formation (Configuration 3), the clustering algo-rithm created a single global entity.
For the ?DickCheney?
cluster, this was correct.
?Sheik HassanNassrallah?
was a more complex instance, in somecases linking was correct, in others it was not.The impact of the metadata features (Configu-ration 4) was both positive and negative.
An articleabout the ?Arab League Secretary General AmruMoussa?
was published on the same day in thesame source as an article about ?Intifada Fatahmovement leader Abu Moussa?.
With the additionof metadata features, these two distinct global enti-ties were merged.
However, the addition of meta-data features correctly led to the merging of threeinstances of the name ?Peter?
in ABC news text(all referring ABC?s Peter Jennings).Web-mined information (Configuration 5) pro-vides several variation and disambiguation fea-tures.
As we observed, the exact match baselinehas fairly high accuracy but is obviously also tooaggressive of a strategy.
However, for certain veryfamous global entities, any reference to the name(especially in corpora made of primarily news text)is likely to be a reference to a single global entity.Because these people/organizations are famous,and commonly mentioned, many of the topic andextraction based features will provide insufficientevidence for merging.
The same famous personwill be mentioned in many different contexts.
Weuse Wikipedia as a resource for such entities.
If aname is unambiguous in Wikipedia, then we mergeall instances of this name string.
In the evaluationcorpus, this led to the merging of many differentinstances of ?Osama Bin Laden?
into a single en-tity.
Web-mined information is also a resource foraliases and acronyms.
These alias lists, allowed usto merge ?Abu Muktar?
with ?Khadafi Montanio?and ?National Liberation Army?
with ?ELN?.Interestingly, removing the string edit distancealgorithm (System 5a), is a slight improvementover System 5.
Initial error analysis has shown thatwhile the string edit distance algorithm did im-prove accuracy on some entities (e.g linking ?SamAlito?
with ?Sam Elito?
and linking ?Andres Pas-trana?
with ?Andreas Pastrana?
); in other cases,the algorithm allowed the system to overlink twoentities, for example linking ?Megawati Soekar-noputri?
and her sister ?Rachmawati Sukarnoputri?.8 Improving Document-Level Extractionwith Global InformationIn addition to evaluating the cross-document sys-tem performance on the GEDR task, we ran a pre-liminary set of experiments using the cross-document co-reference system to improve within-document extraction.
Global output modifiedwithin-document extraction in two ways.First, the cross-document co-reference systemwas used to modify the within-document system?ssubtype classification.
In addition to evaluatingentity links and type classification, the ACE taskmeasures subtype classification.
For example, fororganization entities, systems distinguish betweenMedia and Entertainment organizations.
The IEsystem uses all mentions in a given entity to assigna subtype.
The cross-document co-reference sys-tem has merged several document-level entities,and therefore has even more information withwhich to assign subtypes.
The cross-document sys-tem also has access to a set of manual labels thathave been assigned to Wikipedia categories.Secondly, we used the cross-document co-reference system?s linking decisions to mergewithin-document entities.
If the cross-documentco-reference system merged two entities in the280same document, then those entities were merged inthe within-document output.Table 6 includes results for our within-document IE system, the IE system with improvedsubtypes, and the IE system with improved sub-types and merged entities.B3Val ValLocal 77.3 56.7+ Subtypes 77.3 56.9+ Merge 77.4 57.2Table 6: Within-document ResultsWhile these preliminary experiments yield rela-tively small improvements in accuracy, an analysisof the system?s output suggests that the mergingapproach is quite promising.
The output that hasbeen corrected with global merges includes thelinking entities with ?World Knowledge?
acronyms(e.g.
linking ?FARC?
with ?Armed RevolutionaryForces of Colombia?
); linking entities despitedocument-level extraction mistakes (e.g.
?LadyThatcher?
with ?Margaret Thatcher?
); and linkingentities despite spelling mistakes in a document(e.g linking ?Avenajado?
with ?Robert Aventa-jado?).
However, as we have already seen, thecross-document co-reference system does makemistakes and these mistakes can propagate to thewithin-document output.In particular, we have noticed that the cross-document system has a tendency to link personnames with the same last name when both namesappear in a single document.
As we think about theset of features used for entity disambiguation, wecan see why this would be true.
These names mayhave enough similarity to be considered equivalentnames.
Because they appear in the same document,they will have the same publication date, documentsource, and document topics.
Adjusting the cross-document system to either use a slightly differentapproach to cluster document-level entities fromthe same document or at the very least to be moreconservative in applying merges that are the resultprimarily of document metadata and context to thewithin-document output could improve accuracy.9 Effect of LEDR on GEDRUnlike previous evaluations of cross-document co-reference performance, the ACE 2008 evaluationincluded both person and organization entities.
Wehave noticed that the performance of the cross-document co-reference system on organizationslags behind the performance of the system on peo-ple.
In contrast, for LEDR, the extraction system?sperformance is quite similar between the two entityclasses.
Furthermore, the difference betweenglobal organization and person accuracy in theGEDR is smaller when the GEDR task performedwith perfect document-level extraction.
Scores areshown in Table 7.
These differences suggest thatpart of the reason for the low performance on or-ganizations in GEDR is within-document accuracy.LEDR GEDR-SystemGEDR-PerfectB3Val Val FVal Val FVal ValOrg 75.1 51.7 67.8 45.9 91.5 84.0Per 76.2 52.9 83.2 71.4 94.3 89.5Table 7: Performance on ORG and PER EntitiesThe LEDR task evaluates names, nominals, andpronouns.
GEDR, however only evaluates overname strings.
To see if this was a part of the differ-ence in accuracy, we removed all pronoun andnominal mentions from both the IE system?s localoutput and the reference set.
As shown in Table 8,the gap in performance between organizations andpeople is much larger in this setting.LEDR- Name OnlyB3Val ValORG 82.6 83.0PER 90.1 90.4Table 8: Local Performance on Name Only TaskBecause the GEDR task focuses exclusively onnames and excludes nominals and pronouns, mis-takes in mention type labeling (e.g.
labeling aname as a nominal) become misses and falsealarms rather than type substitutions.
As the task iscurrently defined, type substitutions are much lesscostly than a missing or false alarm entity.Intuitively, correctly labeling the name of a per-son as a name and not a nominal is simple.
Thedistinction for organizations may be fuzzier.
Forexample the string ?the US Department of Justice?could conceivably contain one name, two names,or a name and a nominal.
The ACE guidelines(LDC, 2008a) suggest that this distinction can bedifficult to make, and in fact have a lengthy set ofrules for classifying such cases.
However, theserules can seem unintuitive, and may be difficult formachines to learn.
For example ?Justice Depart-ment?
is not a name but ?Department of Justice?
is.In some sense, this is an artificial distinction en-forced by the task definition, but the accuracy281numbers suggest that the distinction has a negativeeffect on system evaluation.10 ScalabilityOne of the challenges for systems participatingin the ACE task was the need to process a rela-tively large document set (10K documents).
Inquestion answering applications, our name varia-tion algorithms have been applied to even largercorpora (up to 1M documents).
There are two fac-tors that make our solution scalable.First, much of the name variation work ishighly parallelizable.
Most of the time spent in thisalgorithm is spent in the name string edit distancecalculation.
This is also the only algorithm in thename variation component that scales quadraticallywith the number of name strings.
However, eachcalculation is independent, and could be done si-multaneously (with enough machines).
For the10K document set, we ran this algorithm on onemachine, but when working with larger documentsets, these computations were run in parallel.Second, the disambiguation algorithm clusterssubsets of document-level entities, rather than run-ning the clustering over all entities in the documentset.
In the English ACE corpus, the IE systemfound more than 135K document-level entities thatwere candidates for global entity resolution.
Therewere 62,516 unique name strings each of whichwas used to initialize an agglomerative clusteringinstance.
As described in Section  6, a documententity is only clustered one time.
Consequently,36% of these clustering instances are ?skipped?because they contain only already clustered docu-ment entities.
Even the largest clustering instancecontained only 1.4% of the document-level enti-ties.The vast majority of agglomerative clusteringinstances disambiguated a small number of docu-ment-level entities and ran quickly.
99.7% of theagglomerative clustering runs took less than 1 sec-ond.
99.9% took 90 seconds or less.A small number of clustering instances in-cluded a large number of document entities, andtook significant time.
The largest clustering in-stance, initialized with the name string ?Xinhua,?contained 1848 document-level entities (1.4% ofthe document-level entities in the corpus).
Thisinstance took 2.6 hours (27% of the total timespent running agglomerative clustering).
Anotherfrequent entity ?George Bush?
took 1.2 hours.As described in Section  6, the clustering proce-dure can combine unresolved document-level enti-ties into existing global entities.
For large clustersets (e.g entities referred to by the string ?Xinhua?
),speed would be improved by running many smallerclustering instances on subsets of the document-level entities and then merging the results.11 Conclusions and Future WorkWe have presented a cross-document co-referenceclustering algorithm for linking entities across acorpus of documents that?
addresses both the challenges of name varia-tion and entity disambiguation.?
is language-independent,?
is scalableAs measured in ACE 2008, for English our sys-tem produced an .8.4% relative reduction in errorover a baseline that used exact match of namestrings.
When measured on only entities that ap-peared in more than one document, the systemgave a 14.2% relative reduction in error.
For theArabic task, our system produced a 7% reductionin error over exact match (12.4% when scored overentities that appear in more than one document).We have shown how a variety of features are im-portant for addressing different aspects of thecross-document co-reference problem.
Our currentfeatures are merged with hand-tuned weights.
Asadditional development data becomes available, webelieve it would be feasible to statistically learn theweights.
With statistically learned weights, a largerfeature set could improve accuracy even further.Global information from the cross-documentco-reference system improved within-documentinformation extraction.
This suggests both that adocument-level IE system operating over a largecorpus text can improve its accuracy with informa-tion that it learns from the corpus; and also thatintegrating an IE system more closely with asource of world knowledge (e.g.
a knowledgebase) could improve extraction accuracy.AcknowledgementsThis work was supported by the United States De-partment of Homeland Security.
Elizabeth Boscheeand Ralph Weischedel provided useful insightsduring this work.282ReferencesArtiles, Javier, Julio Gonzalo.
& Felisa Verdejo.. 2005.A Testbed for People Searching Strategies.
In theWWW.
SIGIR 2005 Conference.
Salvador, Brazil.Artiles, Javier, Julio Gonzalo.
& Satochi Sekine.. 2007.The SemEval-2007 WePS Evaluation: Establishing abenchmark for the Web People Search Task.
Pro-ceedings of the 4th International Workshop on Se-mantic Evaluations (SemEval-2007), pages 64?69,Prague, Czech.Bagga, Amit & Breck Baldwin.
1998a.
Algorithms forScoring Coreference Chains.
In Proceedings of theLinguistic Coreference Workshop at the First Inter-national Conference on Language Resources andEvaluation (LREC'98), pages 563-566.Bagga, Amit & Breck Baldwin.
1998b.
Entity-BasedCross-Document Coreferencing Using the VectorSpace Model.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Lin-guistics and the 17th International Conference onComputational Linguistics (COLING-ACL?98), pages79-85.Day, David.,Jason Duncan, Claudio Guiliano, Rob Hall,Janet Hitzeman,Su Jian, Paul McNamee, GideonMann, Stanley Yong & Mike Wick.
2007.
CDC Fea-tures.
Johns Hopkins Summer Workshop on Cross-Document Entity Disambiguation.http://www.clsp.jhu.edu/ws2007/groups/elerfed/documents/fullCDED.pptGooi, Chung Heong & James Allan.
2004.
Cross-document coreference on a large scale corpus.
InHuman Language Technology Conf.
North AmericanChapter Association for Computational Linguistics,pages 9?16, Boston, Massachusetts, USA.Lloyd, Levon., Andrew Mehler & Steven Skiena 2006.Identifying Co-referential Names Across Large Cor-pora.
Combinatorial Pattern Matching.
2006, pages12-23, Barcelona, Spain.Linguistic Data Consortium 2008a.
ACE (AutomaticContent Extraction) English Annotation Guidelinesfor Entities Version 6.6 2008.06.13. .
LinguisticData Consortium, Philadelphia.http://projects.ldc.upenn.edu/ace/docs/English-Entities-Guidelines_v6.6.pdfLinguistic Data Consortium, 2008b.
ACE 2008 XDOCPilot Data V2.1.
LDC2007E64.
Linguistic DataConsortium, Philadelphia.Mann, Gideon S. & Yarowsky, David.
2003.
Unsuper-vised Personal Name Disambiguation In Proceedingsof the seventh conference on Natural language learn-ing at HLT-NAACL, pages 33-40.NIST Speech Group.
2008.
The ACE 2008 evaluationplan: Assessment of Detection and Recognition ofEntities and Relations Within and Across Docu-ments.http://www.nist.gov/speech/tests/ace/2008/doc/ace08-evalplan.v1.2d.pdfRamshaw, Lance, E. Boschee, S. Bratus, S. Miller, R.Stone, R. Weischedel and A. Zamanian: ?Experi-ments in Multi-Modal Automatic Content Extrac-tion?
; in Proc.
of HLT-01, San Diego, CA, 2001.Sista, S, R. Schwartz, T. Leek, and J. Makhoul.
An Al-gorithm for Unsupervised Topic Discovery fromBroadcast News Stories.
In Proceedings of ACMHLT, San Diego, CA, 2002.283
