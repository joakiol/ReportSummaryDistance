Fast Context-Free Parsing Requires Fast Boolean MatrixMultiplicationLi l l ian LeeDivision of Engineering and Applied SciencesHarvard University33 Oxford StreetCambridge, MA 012138llee~eecs, harvard, eduAbst ractValiant showed that Boolean matrixmultiplication (BMM) can be used forCFG parsing.
We prove a dual re-sult: CFG parsers running in timeO(\[Gl\[w\[ 3-e) on a grammar G and astring w can be used to multiply m x mBoolean matrices in time O(m3-e/3).In the process we also provide a formaldefinition of parsing motivated by aninformal notion due to Lang.
Our re-sult establishes one of the first limita-tions on general CFG parsing: a fast,practical CFG parser would yield afast, practical BMM algorithm, whichis not believed to exist.1 In t roduct ionThe context-free grammar (CFG) formalismwas developed uring the birth of the field ofcomputational linguistics.
The standard meth-ods for CFG parsing are the CKY algorithm(Kasami, 1965; Younger, 1967) and Earley's al-gorithm (Earley, 1970), both of which have aworst-case running time of O(gN 3) for a CFG(in Chomsky normal form) of size g and a stringof length N. Graham et al (1980) give a vari-ant of Earley's algorithm which runs in timeO(gN3/log N).
Valiant's parsing method is theasymptotically fastest known (Valiant, 1975).It uses Boolean matrix multiplication (BMM)to speed up the dynamic programming in theCKY algorithm: its worst-case running time isO(gM(N)), where M(rn) is the time it takes tomultiply two m x m Boolean matrices together.The standard method for multiplying ma-trices takes time O(m3).
There exist matrixmultiplication algorithms with time complexityO(m3-J); for instance, Strassen's has a worst-case running time of O(m 2"sl) (Strassen, 1969),and the fastest currently known has a worst-caserunning time of O(m 2"376) (Coppersmith andWinograd, 1990).
Unfortunately, the constantsinvolved are so large that these fast algorithms(with the possible exception of Strassen's) can-not be used in practice.
As matrix multi-plication is a very well-studied problem (seeStrassen's historical account (Strassen, 1990,section 10)), it is highly unlikely that simple,practical fast matrix multiplication algorithmsexist.
Since the best BMM algorithms all relyon general matrix multiplication 1, it is widelybelieved that there are no practical O(m 3-~)BMM algorithms.One might therefore hope to find a wayto speed up CFG parsing without relying onmatrix multiplication.
However, we show inthis paper that fast CFG parsing requiresfast Boolean matrix multiplication in a precisesense: any parser running in time O(gN 3-e)that represents parse data in a retrieval-efficientway can be converted with little computationaloverhead into a O(m 3-e/3) BMM algorithm.Since it is very improbable that practical fastmatrix multiplication algorithms exist, we thusestablish one of the first nontrivial imitationson practical CFG parsing.1The "four Russians" algorithm (Arlazarov et al,1970), the fastest BMM algorithm that does not sim-ply use ordinary matrix multiplication, has worst-caserunning time O(mS/log m).Our technique, adapted from that used bySatta (1994) for tree-adjoining grammar (TAG)parsing, is to show that BMM can be efficientlyreduced to CFG parsing.
Satta's result does notapply to CFG parsing, since it explicitly relieson the properties of TAGs that allow them togenerate non-context-free languages.2 Def in i t ionsA Boolean matrix is a matrix with entries fromthe set {0, 1}.
A Boolean matrix multiplicationalgorithm takes as input two m x m Boolean ma-trices A and B and returns their Boolean prod-uct A x B,  which is the m ?
m Boolean matrixC whose entries c~j are defined bym= V (a,k A bkj).k=lThat is, c.ij = 1 if and only if there exists anumber k, 1 < k < m, such that aik = bkj = 1.We use the usual definition of a context-freegrammar (CFG) as a 4-tuple G = (E, V, R, S),where E is the set of terminals, V is the setof nonterminals, R is the set of productions,and S C V is the start symbol.
Given a stringw ~ WlW2.
.
.WN over E*, where each wi is anelement of E, we use the notation ~ to denotethe substring wiwi+l " " " Wj - lWj  ?We will be concerned with the notion ofc-derivations, which are substring derivationsthat are consistent with a derivation of an entirestring.
Intuitively, A =~* w~i is a c-derivation ifit is consistent with at least one parse of w.Def in i t ion 1 Let G = (E, V, R, S) be a CFG,and let w = w lw2.
.
.wN,  wi E ~.
A nontermi-J hal A E V c-derives (consistently derives) w i ifand only if the following conditions hold:?
A ~*  w~, and?
S =::~* i - - lA  N'u\] 1 14wi t  1 .
(These conditions together imply that S ~*  w.)We would like our results to apply to all"practical" parsers, but what does it mean fora parser to be practical?
First, we would liketo be able to retrieve constituent informationfor all possible parses of a string (after all,the recovery of structural information is whatdistinguishes parsing algorithms from recogni-tion algorithms); such information is very use-ful for applications like natural anguage under-standing, where multiple interpretations for asentence may result from different constituentstructures.
Therefore, practical parsers shouldkeep track of c-derivations.
Secondly, a parsershould create an output structure from whichinformation about constituents can be retrievedin an efficient way - -  Satta (1994) points out anobservation of Lang to the effect that one canconsider the input string itself to be a retrieval-inefficient representation of parse information.In short, we require practical parsers to outputa representation of the parse forest for a stringthat allows efficient retrieval of parse informa-tion.
Lang in fact argues that parsing meansexactly the production of a shared forest struc-ture "from which any specific parse can be ex-tracted in time linear with the size of the ex-tracted parse tree" (Lang, 1994, pg.
487), andSatta (1994) makes this assumption as well.These notions lead us to equate practicalparsers with the class of c-parsers, which keeptrack of c-derivations and may also calculategeneral substring derivations as well.Def in i t ion 2 A c-parser is an algorithm thattakes a CFG grammar G = (E ,V ,R ,S)  andstring w E E* as input and produces output~G,w; J:G,w acts as an oracle about parse in-formation, as follows:?
If A c-derives w~, then .7:G,w(A,i,j) ="yes ".If A ~*  J :which implies that A does not ?
W ic-derive wJi ), then :7:G,w( A, i, j ) = "no".?
J:G,w answers queries in constant ime.Note that the answer 5~c,w gives can be arbi-J trary if A :=v* J but A does not c-derive w i .
w iThe constant-time constraint encodes the no-tion that information extraction is efficient; ob-serve that this is a stronger condition than thatcalled for by Lang.\ ]0We define c-parsers in this way to make theclass of c-parsers as broad as possible.
If wehad changed the first condition to "If A derives.
.
.
",  then Earley parsers would be excluded,since they do not keep track of all substringderivations.
If we had written the second con-dition as "If A does not c-derive ur~i , then ... ",then CKY parsers would not be c-parsers, sincethey keep track of all substring derivations, notjust c-derivations.
So as it stands, the class ofc-parsers includes tabular parsers (e.g.
CKY),where 5rG,w is the table of substring deriva-tions, and Earley-type parsers, where ~'G,~ isthe chart.
Indeed, it includes all of the parsingalgorithms mentioned in the introduction, andcan be thought of as a formalization of Lang'sinformal definition of parsing.3 The  reduct ionWe will reduce BMM to c-parsing, thus prov-ing that any c-parsing algorithm can be usedas a Boolean matrix multiplication algorithm.Our method, adapted from that of Satta (1994)(who considered the problem of parsing withtree-adjoining grammars), is to encode informa-tion about Boolean matrices into a CFG.
Thus,given two Boolean matrices, we need to producea string and a grammar such that parsing thestring with respect o the grammar yields out-put from which information about the productof the two matrices can be easily retrieved.We can sketch the behavior of the grammaras follows.
Suppose entries aik in A and bkj inB are both 1.
Assume we have some way tobreak up array indices into two parts so thati can be reconstructed from il and i2, j canbe reconstructed from jl and J2, and k can bereconstructed from kl and k2.
(We will describea way to do this later.)
Then, we will havethe following derivation (for a quantity 5 to bedefined later) :Cil ,Jl ~ Ail ,kl Bkl ,jlderived by Ail,k I derived by Bkl,jlThe key thing to observe is that Cil,jt generatestwo nonterminals whose "inner" indices match,and that these two nonterminals generate sub-strings that lie exactly next to each other.
The"inner" indices constitute a check on kl, and thesubstring adjacency constitutes a check on k2.Let A and B be two Boolean matrices, eachof size m x m, and let C be their Boolean matrixproduct, C = A x B.
In the rest of this section,we consider A, B, C, and m to be fixed.
Setn = \[ml/3\], and set 5 = n+2.
We will beconstructing a string of length 35; we choose 5slightly larger than n in order to avoid havingepsilon-productions i  our grammar.Recall that c/j is non-zero if and only if wecan find a non-zero aik and a non-zero ~ j  suchthat k -- k. In essence, we need simply checkfor the equality of indices k and k. We willbreak matrix indices into two parts: our gram-mar will check whether the first parts of k andare equal, and our string will check whetherthe second parts are also equal, as we sketchedabove.
Encoding the indices ensures that thegrammar is of as small a size as possible, whichwill be important for our time bound results.Our index encoding function is as follows.
Leti be a matrix index, 1 < i < m. Then we definethe funct ion/( i )  -- ( f l ( i ) ,  f2(i)) byf l ( i )  = \[i/nJ (0 < f l ( i )  <_ n2), andf2(i) = (i mod n) + 2 (2_ f2 ( i ) _<n+l ) .Since f l  and f2 are essentially the quotient andremainder of integer division of i by n, we canretrieve i from ( f l ( i ) , f2( i ) ) .
We will use thenotational shorthand of using subscripts insteadof the functions fl and f2, that is, we write iland i2 for fl(i) and f2(i).It is now our job to create a CFG G =(E, ~/: R, S) and a string w that encode infor-mation about A and B and express constraintsabout their product C. Our plan is to includea set of nonterminals {Cp,q : 1 < p,q < n 2} inV so that cij = 1 if and only if Cil,jl c-derivesw j2+2~ In section 3.11 we describe a version i2of G and prove it has this c-derivation property.Then, in section 3.2 we explain that G can easilybe converted to Chomsky normal form in sucha way as to preserve c-derivations.11We choose the set of terminals to be E ={we : l<g<3n+6},  and choose the stringto be parsed to be w = WlW2.
"'w3n+6.We consider w to be made up of threeparts, x, y, and z, each of size 6: w =WlW2 ? "
" Wn+2 Wn+3 ? "
" W2n+4 W2n+5 " " " W3n+6.
~ ~ - ~  ,z ~ zObserve that for any i, 1 < i < m, wi.~ lieswithin x, wi2+~ lies within y, and wi~+2~ lieswithin z, sincei2 E \[2, n+l \ ] ,i2 + 6 ~ \[n + 4, 2n + 3\], andi2 + 26 E \[2n + 6,3n + 5\].3.1 The  grammarNow we begin building the grammar G =(E, V, R, S).
We start with the nonterminalsV = {S} and the production set R = ~.
Weadd nonterminal W to V for generating arbi-trary non-empty substrings of w; thus we needthe productions(W-rules) W > wtWlwe,  1 < g < 3n + 6.Next we encode the entries of the input matricesA and B in our grammar.
We include sets ofnon-terminals { Ap,q : 1 < p, q < n 2 } and { Bp,q :1 < p, q < n2}.
Then, for every non-zero entryaij in A, we add the production(A-rules) Ai~,j~ > wi~Wwj2+~.For every non-zero entry bij in B, we add theproduction(B-rules) BQ, j l  > zoi2+l+6Wzoj2+26.We need to represent entries of C, so we cre-ate nonterminals {Cp,q : 1 < p, q <_ n 2 } and pro-ductions(C - ru les )  Cp,q > Ap,rBr,q, 1 < p, q, r < n 2.Finally, we complete the construction withproductions for the start symbol S:(S-rules) S > WCp,qW, l <_ p,q < n 2.We now prove the following result about thegrammar and string we have just described.Theorem 1 For 1 <_ i , j  < m, the entry cijin C is non-zero if and only if Ci~,jl c-derivesW j2 +26i2Proof.
Fix i and j.Let us prove the :'only if" direction first.Thus, suppose c~j = 1.
Then there exists a ksuch that aik = bkj = 1.
Figure 1 sketches howCil,j~ c-derives w~.
-~+2~ iSCla im 1 Ci~,j~ 0*  w. ~)+2~ i2The production Cil,jl > Ah,k~Bkx,j ~is one ofthe C-rules in our grammar.
Since aik = 1,Aix,k~ > wi2 Wwk2+~ is one of our A-rules, andsince bkj -: 1, Bk l , j  I ) Wk2+l+sWwj2+2 6 isone of our B-rules.
Finally, since i2 + 1 < (k2 +6) -- 1 and (k2 + 1 +6)  + 1 <__ (j2 +2~) - 1,we have W 0"  .k2+~-1 and W =~* w j2+2~-~ wi2+l  k2+2+6 'since both substrings are of length at least one.Therefore,Cil ,jl o Ail ,kl Bkl ,jl=:~* Wi2 WWk2+~ Wk2+l+6Wwj2+26derived by Aq,k~ derivedby B~,~:=~ , j2+26Wi 2 ,and Claim 1 follows,C la im 2 S 0"  " i~-lc~ ~,,3n+6 Wl ~i l  ,jl uJ j2+26+l ?This claim is essentially trivial, since bythe definition of the S-rules, we know thatS =~* WCil , j l  W.  We need only show that nei-w3n+6 ther w~ "2-1 nor j2+26+1 is the empty string (andhence can be derived by W); since 1 < i2 - 1and j2 + 26 + 1 <__ 3n + 6, the claim holds.Claims 1 and 2 together prove that Cil,jl c-derives W j2+26 i  , as required.
2Next we prove the "if" direction.
Sup-pose Cil,j~ c-derives W j2+26 which by definition i2 'means Cil,jl o*  W j2+26 Then there must be i2a derivation resulting from the application of aC-rule as follows:Ci l , j l  0 Ai l ,k,  Bk, , j l  =~* w~.
.
'2+2ci i22This proof would have been simpler if we had al-lowed W to derive the empty string.
However, we avoidepsi lon-productions in order to facil itate the conversionto Chomsky normal form, discussed later.12WSCil,j~ WW 1 .. .
Wi  2 .
.
.
Wk2+SWk2+lq-  ~ .. .
Wj2+28 ... W3n+6x y zFigure 1: Schematic of the derivation process when aik -~ bkj ---- 1.
The substrings derived by Ail,k~and Bkl, j l  lie right next to each other.for some k ~.
It must be the case that for some~, Ail,k' =:~* w ~.
and Bk',jl 0"  ~ j~+2~ But  z2 ~?+1 "then we must have the productions Ail,k'wi2Wwt and Bk',jl > ?.l)?+lWWj2+2 5 with ~ =k" + ~ for some k".
But we can only have suchproductions if there exists a number k such thatkl = k t, k2 = k n, aik = 1, and bkj ---- 1; and thisimplies that cij = 1.
?Examinat ion of the proof reveals that we havealso shown the following two corollaries.Coro l la ry  1 For 1 < i, j  < m, cij = 1 if andonly if Cil,jl =:b* j2+2~ Wi 2Coro l la ry  2 S =~* w if and only if C is notthe all-zeroes matrix.Let us now calculate the size of G. V consistsof O((n2) 2) = O(m 4/3) nonterminals.
R con-tains O(n) W-rules and O((n2) 2) = O(m 4/3)S-rules.
There are at most m 2 A-rules, sincewe have an A-rule for each non-zero entry in A;similarly, there are at most m 2 B-rules.
Andlastly, there are (n2) 3 = O(m 2) C-rules.
There-fore, our grammar is of size O(m2); since G en-codes matrices A and B, it is of optimal size.3.2 Chomsky  normal  fo rmWe would like our results to be true for thelargest class of parsers possible.
Since someparsers require the input grammar to be inChomsky normal form (CNF), we therefore wishto construct a CNF version G ~ of G. However,in order to preserve time bounds, we desire thatO(IG'I) = O(\]GI), and we also require that The-orem 1 holds for G ~ as well as G.The standard algorithm for converting CFGsto CNF can yield a quadratic blow-up in thesize of the grammar and thus is clearly un-satisfactory for our purposes.
However, sinceG contains no epsilon-productions or unit pro-ductions, it is easy to see that we can convertG simply by introducing a small (O(n)) num-ber of nonterminals without changing any c-derivations for the Cp,q.
Thus, from now on wewill simply assume that G is in CNF.3.3 T ime boundsWe are now in a position to prove our relationbetween time bounds for Boolean matr ix multi-plication and time bounds for CFG parsing.13Theorem 2 Any c-parser P with running timeO(T(g)t(N)) on grammars of size g andstrings of length N can be converted intoa BMM algorithm Mp that runs in timeO(max(m 2, T(m2)t(mU3))).
In particular, if Ptakes time O(gN3-e), then l~/Ip runs in time0(m3-~/3).Proof.
Me acts as follows.
Given two Booleanm x m matrices A and B, it constructs G andw as described above.
It feeds G and w to P,which outputs $'c,w- To compute the prod-uct matrix C, Me queries for each i and j,1 < i, j  < m, whether Ci~,jl derives wJ ~+2~- -  - -  't 2(we do not need to ask whether Cil,j~ c-derivesw'\]J ~+26 because of corollary 1), setting cij appro- i2priately.
By definition of c-parsers, each suchquery takes constant ime.
Let us compute therunning time of Me.
It takes O(m 2) time toread the input matrices.
Since G is of sizeO(rn 2) and Iwl = O(ml/3), it takes O(m 2) timeto build the input to P, which then computes5rG,w in time O(T(m2)t(ml/3)).
Retrieving Ctakes O(m2).
So the total time spent by Mp isO(max(m 2, T(m2)t(mU3))), as was claimed.In the case where T(g) = g and t(N) = N 3-e,Mp has a running time of O(m2(ml/3) a-e) =O(m 2+1-?/3) = O(m3-e'/3).
IIThe case in which P takes time linear in thegrammar size is of the most interest, since innatural language processing applications, thegrammar tends to be far larger than the stringsto be parsed.
Observe that theorem 2 trans-lates the running time of the standard CFGparsers, O(gN3), into the running time of thestandard BMM algorithm, O(m3).
Also, a c-parser with running time O(gN 2"43) would yielda matrix multiplication algorithm rivalling thatof Strassen's, and a c-parser with running timebetter than O(gN H2) could be converted intoa BMM method faster than Coppersmith andWinograd.
As per the discussion above, even ifsuch parsers exist, they would in all likelihoodnot be very practical.
Finally, we note that ifa lower bound on BMM of the form f~(m 3-a)were found, then we would have an immediatelower bound of ~(N 3-3a) on c-parsers runningin time linear in g.4 Re la ted  resu l ts  and  conc lus ionWe have shown that fast practical CFG parsingalgorithms yield fast practical BMM algorithms.Given that fast practical BMM algorithms areunlikely to exist, we have established a limita-tion on practical CFG parsing.Valiant (personal communication) otes thatthere is a reduction of m ?
m Boolean matrixmultiplication checking to context-free recog-nition of strings of length m2; this reduc-tion is alluded to in a footnote of a paperby Harrison and Havel (1974).
However, thisreduction converts a parser running in timeO(Iwl 1"5) to a BMM checking algorithm run-ning in time O(m 3) (the running time of thestandard multiplication method), whereas ourresult says that sub-cubic practical parsers arequite unlikely; thus, our result is quite a bitstronger.Seiferas (1986) gives a simple proof ofN 2 an ~t(lo-Q-W) lower bound (originally due toGallaire (1969)) for the problem of on-line lin-ear CFL recognition by multitape Turing ma-chines.
However, his results concern on-linerecognition, which is a harder problem thanparsing, and so do not apply to the general off-line parsing case.Finally, we recall Valiant's reduction ofCFG parsing to boolean matrix multiplication(Valiant, 1975); it is rather pleasing to have thereduction cycle completed.5 AcknowledgmentsI thank Joshua Goodman, Rebecca Hwa, JonKleinberg, and Stuart Shieber for many helpfulcomments and conversations.
Thanks to LesValiant for pointing out the "folklore" reduc-tion.
This material is based upon work sup-ported in part by the National Science Foun-dation under Grant No.
IRI-9350192.
I alsogratefully acknowledge partial support froman NSF Graduate Fellowship and an AT&TGRPW/ALFP grant.
Finally, thanks to Gior-gio Satta, who mailed me a preprint of hisBMM/TAG paper several years ago.14ReferencesArlazarov, V. L., E. A. Dinic, M. A. Kronrod, andI.
A. Farad~ev.
1970.
On economical construc-tion of the transitive closure of an oriented graph.Soviet Math.
Dokl., 11:1209-1210.
English trans-lation of the Russian article in Dokl.
Akad.
NaukSSSR 194 (1970).Coppersmith, Don and Shmuel Winograd.
1990.Matrix multiplication via arithmetic progression.Journal of Symbolic Computation, 9(3):251-280.Special Issue on Computational A gebraic Com-plexity.Earley, Jay.
1970.ing algorithm.13(2):94-102.An efficient context-free pars-Communications of the A CM,Gallaire, Herv& 1969.
Recognition time of context-free languages by on-line turing machines.
Infor-mation and Control, 15(3):288-295, September.Graham, Susan L., Michael A. Harrison, and Wal-ter L. Ruzzo.
1980.
An improved context-freerecognizer.
A CM Transactions on ProgrammingLanguages and Systems, 2(3):415-462.Harrison, Michael and Ivan Havel.
1974.
On theparsing of deterministic languages.
Journal of theACM, 21(4):525-548, October.Kasami, Tadao.
1965.
An efficient recognition andsyntax algorithm for context-free languages.
Sci-entific Report AFCRL-65-758, Air Force Cam-bridge Research Lab, Bedford, MA.Lang, Bernard.
1994.
Recognition can beharder than parsing.
Computational Intelligence,10(4):486-494, November.Satta, Giorgio.
1994.
Tree-adjoining rammar pars-ing and boolean matrix multiplication.
Computa-tional Linguistics, 20(2):173-191, June.Seiferas, Joel.
1986.
A simplified lower boundfor context-free-language recognition.
Informa-tion and Control, 69:255-260.Strassen, Volker.
1969.
Gaussian elimination is notoptimal.
Numerische Mathematik, 14(3):354-356.Strassen, Volker.
1990.
Algebraic complexity the-ory.
In Jan van Leeuwen, editor, Handbook ofTheoretical Computer Science, volume A. ElsevierScience Publishers, chapter 11, pages 633-672.Valiant, Leslie G. 1975.
General context-free r cog-nition in less than cubic time.
Journal of Com-puter and System Sciences, 10:308-315.Younger, Daniel H. 1967.
Recognition and parsingof context-free languages in time n 3.
Informationand Control, 10(2):189-208.15
