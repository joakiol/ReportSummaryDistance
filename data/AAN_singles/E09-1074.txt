Proceedings of the 12th Conference of the European Chapter of the ACL, pages 648?656,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsPredicting Strong Associations on the Basis of Corpus DataYves PeirsmanResearch Foundation ?
Flanders &QLVL, University of LeuvenLeuven, Belgiumyves.peirsman@arts.kuleuven.beDirk GeeraertsQLVL, University of LeuvenLeuven, Belgiumdirk.geeraerts@arts.kuleuven.beAbstractCurrent approaches to the prediction ofassociations rely on just one type of in-formation, generally taking the form ofeither word space models or collocationmeasures.
At the moment, it is an openquestion how these approaches compareto one another.
In this paper, we willinvestigate the performance of these twotypes of models and that of a new ap-proach based on compounding.
The bestsingle predictor is the log-likelihood ratio,followed closely by the document-basedword space model.
We will show, how-ever, that an ensemble method that com-bines these two best approaches with thecompounding algorithm achieves an in-crease in performance of almost 30% overthe current state of the art.1 IntroductionAssociations are words that immediately come tomind when people hear or read a given cue word.For instance, a word like pepper calls up salt,and wave calls up sea.
Aitchinson (2003) andSchulte im Walde and Melinger (2005) show thatsuch associations can be motivated by a numberof factors, from semantic similarity to colloca-tion.
Current computational models of associa-tion, however, tend to focus on one of these, by us-ing either collocation measures (Michelbacher etal., 2007) or word space models (Sahlgren, 2006;Peirsman et al, 2008).
To this day, two gen-eral problems remain.
First, the literature lacksa comprehensive comparison between these gen-eral types of models.
Second, we are still lookingfor an approach that combines several sources ofinformation, so as to correctly predict a larger va-riety of associations.Most computational models of semantic rela-tions aim to model semantic similarity in particu-lar (Landauer and Dumais, 1997; Lin, 1998; Pado?and Lapata, 2007).
In Natural Language Process-ing, these models have applications in fields likequery expansion, thesaurus extraction, informa-tion retrieval, etc.
Similarly, in Cognitive Science,such models have helped explain neural activa-tion (Mitchell et al, 2008), sentence and discoursecomprehension (Burgess et al, 1998; Foltz, 1996;Landauer and Dumais, 1997) and priming patterns(Lowe and McDonald, 2000), to name just a fewexamples.
However, there are a number of appli-cations and research fields that will surely bene-fit from models that target the more general phe-nomenon of association.
For instance, automat-ically predicted associations may prove useful inmodels of information scent, which seek to ex-plain the paths that users follow in their searchfor relevant information on the web (Chi et al,2001).
After all, if the visitor of a web shopclicks on music to find the prices of iPods, thisbehaviour is motivated by an associative relationdifferent from similarity.
Other possible applica-tions lie in the field of models of text coherence(Landauer and Dumais, 1997) and automated es-say grading (Kakkonen et al, 2005).
In addition,all research in Cognitive Science that we have re-ferred to above could benefit from computationalmodels of association in order to study the effectsof association in comparison to those of similarity.Our article is structured as follows.
In sec-tion 2, we will discuss the phenomenon of asso-ciation and introduce the variety of relations thatit is motivated by.
Parallel to these relations, sec-tion 3 presents the three basic types of approachesthat we use to predict strong associations.
Sec-tion 4 will first compare the results of these threeapproaches, for a total of 43 models.
Section 5will then show how these results can be improvedby the combination of several models in an ensem-ble.
Finally, section 6 wraps up with conclusionsand an outlook for future research.648cue associationamfibie (?amphibian?)
kikker (?frog?
)peper (?pepper?)
zout (?salt?
)roodborstje (?robin?)
vogel (?bird?
)granaat (?grenade?)
oorlog (?war?
)helikopter (?helicopter?)
vliegen (?to fly?
)werk (?job?)
geld (?money?
)acteur (?actor?)
film (?film?
)cello (?cello?)
muziek (?music?
)kruk (?stool?)
bar (?bar?
)Table 1: Examples of cues and their strongest as-sociation.2 AssociationsThere are several reasons why a word may be asso-ciated to its cue.
According to Aitchinson (2003),the four major types of associations are, in or-der of frequency, co-ordination (co-hyponyms likepepper and salt), collocation (like salt and wa-ter), superordination (insect as a hypernym of but-terfly) and synonymy (like starved and hungry).As a result, a computational model that is able topredict associations accurately has to deal with awide range of semantic relations.
Past systems,however, generally use only one type of informa-tion (Wettler et al, 2005; Sahlgren, 2006; Michel-bacher et al, 2007; Peirsman et al, 2008; Wand-macher et al, 2008), which suggests that they arerelatively restricted in the number of associationsthey will find.In this article, we will focus on a set of Dutchcue words and their single strongest association,collected from a large psycholinguistic experi-ment.
Table 1 gives a few examples of such cue?association pairs.
It illustrates the different typesof linguistic phenomena that an association maybe motivated by.
The first three word pairs arebased on similarity.
In this case, strong associ-ations can be hyponyms (as in amphibian?frog),co-hyponyms (as in pepper?salt) or hypernyms oftheir cue (as in robin?bird).
The next three pairsrepresent semantic links where no relation of sim-ilarity plays a role.
Instead, the associations seemto be motivated by a topical relation to their cue,which is possibly reflected by their frequent co-occurrence in a corpus.
The final three word pairssuggest that morphological factors might play arole, too.
Often, a cue and its association formthe building blocks of a compound, and it is possi-ble that one part of a compound calls up the other.The examples show that the process of compound-ing can go in either direction: the compound mayconsist of cue plus association (as in cellomuziek?cello music?
), or of association plus cue (as infilmacteur ?film actor?).
While it is not clear if itis the compounds themselves that motivate the as-sociation, or whether it is just the topical relationbetween their two parts, they might still be able tohelp identify strong associations.3 ApproachesMotivated by the three types of cue?associationpairs that we identified in Table 1, we study threesources of information (two types of distributionalinformation, and one type of morphological infor-mation) that may provide corpus-based evidencefor strong associatedness: collocation measures,word space models and compounding.3.1 Collocation measuresProbably the most straightforward way to pre-dict strong associations is to assume that a cueand its strong association often co-occur in text.As a result, we can use collocation measureslike point-wise mutual information (Church andHanks, 1989) or the log-likelihood ratio (Dunning,1993) to predict the strong association for a givencue.
Point-wise mutual information (PMI) tellsus if two words w1 and w2 occur together more orless often than expected on the basis of their indi-vidual frequencies and the independence assump-tion:PMI(w1, w2) = log2P (w1, w2)P (w1) ?
P (w2)The log-likelihood ratio compares the like-lihoods L of the independence hypothesis (i.e.,p = P (w2|w1) = P (w2|?w1)) and the de-pendence hypothesis (i.e., p1 = P (w2|w1) 6=P (w2|?w1) = p2), under the assumption that thewords in a text are binomially distributed:log ?
= logL(P (w2|w1); p) ?
L(P (w2|?w1); p)L(P (w2|w1); p1) ?
L(P (w2|?w1); p2)3.2 Word Space ModelsA respectable proportion (in our data about 18%)of the strong associations are motivated by se-mantic similarity to their cue.
They can be syn-onyms, hyponyms, hypernyms, co-hyponyms or649antonyms.
Collocation measures, however, are notspecifically targeted towards the discovery of se-mantic similarity.
Instead, they model similaritymainly as a side effect of collocation.
Thereforewe also investigated a large set of computationalmodels that were specifically developed for thediscovery of semantic similarity.
These so-calledword space models or distributional models of lex-ical semantics are motivated by the distributionalhypothesis, which claims that semantically simi-lar words appear in similar contexts.
As a result,they model each word in terms of its contexts ina corpus, as a so-called context vector.
Distribu-tional similarity is then operationalized as the sim-ilarity between two such context vectors.
Thesemodels will thus look for possible associations bysearching words with a context vector similar tothe given cue.Crucial in the implementation of word spacemodels is their definition of context.
In the cur-rent literature, there are basically three popular ap-proaches.
Document-based models use some sortof textual entity as features (Landauer and Du-mais, 1997; Sahlgren, 2006).
Their context vec-tors note what documents, paragraphs, articles orsimilar stretches of text a target word appears in.Without dimensionality reduction, in these mod-els two words will be distributionally similar ifthey often occur together in the same paragraph,for instance.
This approach still bears some simi-larity to the collocation measures above, since itrelies on the direct co-occurrence of two wordsin text.
Second, syntax-based models focus onthe syntactic relationships in which a word takespart (Lin, 1998).
Here two words will be sim-ilar when they often appear in the same syntac-tic roles, like subject of fly.
Third, word-based models simply use as features the wordsthat appear in the context of the target, withoutconsidering the syntactic relations between them.Context is thus defined as the set of n wordsaround the target (Sahlgren, 2006).
Obviously, thechoice of context size will again have a major in-fluence on the behaviour of the model.
Syntax-based and word-based models differ from collo-cation measures and document-based models inthat they do not search for words that co-occurdirectly.
Instead, they look for words that oftenoccur together with the same context words orsyntactic relations.
Even though all these modelswere originally developed to model semantic sim-ilarity relations, syntax-based models have beenshown to favour such relations more than word-based and document-based models, which mightcapture more associative relationships (Sahlgren,2006; Van der Plas, 2008).3.3 CompoundingAs we have argued before, one characteristic ofcues and their strong associations is that they cansometimes be combined into a compound.
There-fore we developed a third approach which dis-covers for every cue the words in the corpus thatin combination with it lead to an existing com-pound.
Since in Dutch compounds are generallywritten as one word, this is relatively easy.
We at-tached each candidate association to the cue (bothin the combination cue+association and associ-ation+cue), following a number of simple mor-phological rules for compounding.
We then de-termined if any of these hypothetical compoundsoccurred in the corpus.
The possible associa-tions that led to an observed compound were thenranked according to the frequency of that com-pound.1 Note that, for languages where com-pounds are often spelled as two words, like En-glish, our approach will have to recognize multi-word units to deal with this issue.3.4 Previous researchIn previous research, most attention has gone outto the first two of our models.
Sahlgren (2006)tries to find associations with word space mod-els.
He argues that document-based models arebetter suited to the discovery of associations thanword-based ones.
In addition, Sahlgren (2006) aswell as Peirsman et al (2008) show that in word-based models, large context sizes are more effec-tive than small ones.
This supports Wandmacheret al?s (2008) model of associations, which uses acontext size of 75 words to the left and right of thetarget.
However, Peirsman et al (2008) find thatword-based distributional models are clearly out-performed by simple collocation measures, par-ticularly the log-likelihood ratio.
Such colloca-tion measures are also used by Michelbacher et al(2007) in their classification of asymmetric associ-ations.
They show the chi-square metric to be a ro-bust classifier of associations as either symmetricor asymmetric, while a measure based on condi-tional probabilities is particularly suited to model1If both compounds cue+association and association+cueoccurred in the corpus, their frequencies were summed.650lll l l l l l l l2 4 6 8 1025102050100context sizemedianrankofmost frequentassociation l word?based no stoplistword?based stoplistpmi statisticlog?likelihood statisticcompound?basedsyntax?baseddocument?basedFigure 1: Median rank of the strong associations.the magnitude of asymmetry.
In a similar vein,Wettler et al (2005) successfully predict associa-tions on the basis of co-occurrence in text, in theframework of associationist learning theory.
De-spite this wealth of systems, it is an open questionhow their results compare to each other.
More-over, a model that combines several of these sys-tems might outperform any basic approach.4 ExperimentsOur experiments were inspired by the associationprediction task at the ESSLLI-2008 workshop ondistributional models.
We will first present thisprecise setup and then go into the results and theirimplications.4.1 SetupOur data was the Twente Nieuws Corpus (TwNC),which contains 300 million words of Dutch news-paper articles.
This corpus was compiled at theUniversity of Twente and subsequently parsed bythe Alpino parser at the University of Gronin-gen (van Noord, 2006).
The newspaper arti-cles in the corpus served as the contextual fea-tures for the document-based system; the depen-dency triples output by Alpino were used as in-put for the syntax-based approach.
These syntacticfeatures of the type subject of fly coveredeight syntactic relations ?
subject, direct object,prepositional complement, adverbial prepositionalphrase, adjective modification, PP postmodifica-tion, apposition and coordination.
Finally, the col-location measures and word-based distributionalmodels took into account context sizes rangingfrom one to ten words to the left and right of thetarget.Because of its many parameters, the precise im-plementation of the word space models deserves abit more attention.
In all cases, we used the con-text vectors in their full dimensionality.
While thisis somewhat of an exception in the literature, ithas been argued that the full dimensionality leadsto the best results for word-based models at least(Bullinaria and Levy, 2007).
For the syntax-basedand word-based approaches, we only took into ac-count features that occurred at least two times to-gether with the target.
For the word-based models,we experimented with the use of a stoplist, whichallowed us to exclude semantically ?empty?
wordsas features.
The simple co-occurrence frequenciesin the context vectors were replaced by the point-wise mutual information between the target andthe feature (Bullinaria and Levy, 2007; Van derPlas, 2008).
The similarity between two vectorswas operationalized as the cosine of the angle be-651similar related, not similarmodels mean med rank1 mean med rank1pmi context 10 16.4 4 23% 25.2 9 10%log-likelihood ratio context 10 12.8 2 41% 18.0 3 31%syntax-based 16.3 4 22% 61.9 70 2%word-based context 10 stoplist 10.7 3 27% 36.9 17 12%document-based 10.1 3 26% 20.2 4 26%compounding 80.7 101 5% 51.9 26 12%Table 2: Performance of the models on semantically similar cue-association pairs and related but notsimilar pairs.med = median; rank1 = number of associations at rank 1tween them.
This measure is more or less stan-dard in the literature and leads to state-of-the-artresults (Schu?tze, 1998; Pado?
and Lapata, 2007;Bullinaria and Levy, 2007).
While the cosine is asymmetric measure, however, association strengthis asymmetric.
For example, snelheid (?speed?
)triggered auto (?car?)
no fewer than 55 times inthe experiment, whereas auto evoked snelheid amere 3 times.
Like Michelbacher et al (2007), wesolve this problem by focusing not on the similar-ity score itself, but on the rank of the association inthe list of nearest neighbours to the cue.
We thusexpect that auto will have a much higher rank inthe list of nearest neighbours to snelheid than viceversa.Our Gold Standard was based on a large-scalepsycholinguistic experiment conducted at the Uni-versity of Leuven (De Deyne and Storms, 2008).In this experiment, participants were asked to listthree different associations for all cue words theywere presented with.
Each of the 1425 cues wasgiven to at least 82 participants, resulting in a to-tal of 381,909 responses.
From this set, we tookonly noun cues with a single strong association.This means we found the most frequent associ-ation to each cue, and only included the pair inthe test set if the association occurred at least 1.5times more often than the second most frequentone.
This resulted in a final test set of 593 cue-association pairs.
Next we brought together all theassociations in a set of candidate associations, andcomplemented it with 1000 random words fromthe corpus with a frequency of at least 200.
Fromthese candidate words, we had each model selectthe 100 highest scoring ones (the nearest neigh-bours).
Performance was then expressed as themedian and mean rank of the strongest associationin this list.
Associations absent from the list auto-matically received a rank of 101.
Thus, the lowerthe rank, the better the performance of the system.While there are obviously many more ways of as-sembling a test set and scoring the several systems,we found these all gave very similar results to theones reported here.4.2 Results and discussionThe median ranks of the strong associations for allmodels are plotted in Figure 1.
The means showthe same pattern, but give a less clear indication ofthe number of associations that were suggested inthe top n most likely candidates.
The most suc-cessful approach is the log-likelihood ratio (me-dian 3 with a context size of 10, mean 16.6),followed by the document-based model (median4, mean 18.4) and point-wise mutual informa-tion (median 7 with a context size of 10, mean23.1).
Next in line are the word-based distribu-tional models with and without a stoplist (high-est medians at 11 and 12, highest means at 30.9and 33.3, respectively), and then the syntax-basedword space model (median 42, mean 51.1).
Theworst performance is recorded for the compound-ing approach (median 101, mean 56.7).
Overall,corpus-based approaches that rely on direct co-occurrence thus seem most appropriate for the pre-diction of strong associations to a cue.
This isprobably a result of two factors.
First, collocationitself is an important motivation for human asso-ciations (Aitchinson, 2003).
Second, while col-location approaches in themselves do not targetsemantic similarity, semantically similar associa-tions are often also collocates to their cues.
This isparticularly the case for co-hyponyms, like pepperand salt, which score very high both in terms ofcollocation and in terms of similarity.Let us discuss the results of all models in a bit652lllcue frequencyIndexmedianrank ofstrongest associationhigh mid low125102050100lllassociation frequencyIndexmedianrank ofstrongest associationhigh mid low125102050100 l pmi context 10log?likelihood context 10syntax?basedword?based context 10 stoplistdocument?basedcompoundingFigure 2: Performance of the models in three cue and association frequency bands.more detail.
A first factor of interest is the dif-ference between associations that are similar totheir cue and those which are related but not simi-lar.
Most of our models show a crucial differencein performance with respect to these two classes.The most important results are given in Table 2.The log-likelihood ratio gives the highest numberof associations at rank 1 for both classes.
Par-ticularly surprising is its strong performance withrespect to semantic similarity, since this relationis only a side effect of collocation.
In fact, thelog-likelihood ratio scores better at predicting se-mantically similar associations than related but notsimilar associations.
Its performance moreoverlies relatively close to that of the word space mod-els, which were specifically developed to modelsemantic similarity.
This underpins the observa-tion that even associations that are semanticallysimilar to their cues are still highly motivated bydirect co-occurrence in text.
Interestingly, only thecompounding approach has a clear preference forassociations that are related to their cue, but notsimilar.A second factor that influences the performanceof the models is frequency.
In order to test itsprecise impact, we split up the cues and their as-sociations in three frequency bands of compara-ble size.
For the cues, we constructed a bandfor words with a frequency of less than 500 inthe corpus (low), between 500 and 2,500 (mid)and more than 2,500 (high).
For the associations,we had bands for words with a frequency of lessthan 7,500 (low), between 7,500 and 20,000 (mid)and more than 20,000 (high).
Figure 2 showsthe performance of the most important models inthese frequency bands.
With respect to cue fre-quency, the word space models and compound-ing approach suffer most from low frequenciesand hence, data sparseness.
The log-likelihoodratio is much more robust, while point-wise mu-tual information even performs better with low-frequency cues, although it does not yet reachthe performance of the document-based systemor the log-likelihood ratio.
With respect to asso-ciation frequency, the picture is different.
Herethe word-based distributional models and PMI per-form better with low-frequency associations.
Thedocument-based approach is largely insensitive toassociation frequency, while the log-likelihood ra-tio suffers slightly from low frequencies.
The per-formance of the compounding approach decreasesmost.
What is particularly interesting about thisplot is that it points towards an important differ-ence between the log-likelihood ratio and point-wise mutual information.
In its search for nearestneighbours to a given cue word, the log-likelihoodratio favours frequent words.
This is an advanta-geous feature in the prediction of strong associa-tions, since people tend to give frequent words asassociations.
PMI, like the syntax-based and word-based models, lacks this characteristic.
It thereforefails to discover mid- and high-frequency associa-tions in particular.Finally, despite the similarity in results betweenthe log-likelihood ratio and the document-basedword space model, there exists substantial varia-tion in the associations that they predict success-fully.
Table 3 gives an overview of the top ten as-sociations that are predicted better by one modelthan the other, according to the difference be-653model cue?association pairsdocument-based model cue?billiards, amphibian?frog, fair?doughnut ball, sperm whale?sea,map?trip, avocado?green, carnivore?meat, one-wheeler?circus,wallet?money, pinecone?woodlog-likelihood ratio top?toy, oven?hot, sorbet?ice cream, rhubarb?sour, poppy?red,knot?rope, pepper?red, strawberry?red, massage?oil, raspberry?redTable 3: A comparison of the document-based model and the log-likelihood ratio on the basis of thecue?target pairs with the largest difference in log ranks between the two approaches.tween the models in the logarithm of the rank ofthe association.
The log-likelihood ratio seemsto be biased towards ?characteristics?
of the tar-get.
For instance, it finds the strong associativerelation between poppy, pepper, strawberry, rasp-berry and their shared colour red much better thanthe document-based model, just like it finds the re-latedness between oven and hot and rhubarb andsour.
The document-based model recovers moreassociations that display a strong topical connec-tion with their cue word.
This is thanks to its re-liance on direct co-occurrence within a large con-text, which makes it less sensitive to semantic sim-ilarity than word-based models.
It also appears tohave less of a bias toward frequent words than thelog-likelihood ratio.
Note, for instance, the pres-ence of doughnut ball (or smoutebol in Dutch) asthe third nearest neighbour to fair, despite the factit occurs only once (!)
in the corpus.
This com-plementarity between our two most successful ap-proaches suggests that a combination of the twomay lead to even better results.
We therefore in-vestigated the benefits of a committee-based or en-semble approach.5 Ensemble-based prediction of strongassociationsGiven the varied nature of cue?association rela-tions, it could be beneficial to develop a model thatrelies on more than one type of information.
En-semble methods have already proved their effec-tiveness in the related area of automatic thesaurusextraction (Curran, 2002), where semantic similar-ity is the target relation.
Curran (2002) exploredthree ways of combining multiple ordered sets ofwords: (1) mean, taking the mean rank of eachword over the ensemble; (2) harmonic, taking theharmonic mean; (3) mixture, calculating the meansimilarity score for each word.
We will study onlythe first two of these approaches, as the differentmetrics of our models cannot simply be combinedin a mean relatedness score.
More particularly, wewill experiment with ensembles taking the (har-monic) mean of the natural logarithm of the ranks,since we found these to perform better than thoseworking with the original ranks.2Table 4 compares the results of the most im-portant ensembles with that of the single best ap-proach, the log-likelihood ratio with a context sizeof 10.
By combining the two best approachesfrom the previous section, the log-likelihood ra-tio and the document-based model, we alreadyachieve a substantial increase in performance.
Themean rank of the association goes from 3 to 2,the mean from 16.6 to 13.1 and the number ofstrong associations with rank 1 climbs from 194to 223.
This is a statistically significant increase(one-tailed paired Wilcoxon test, W = 30866,p = .0002).
Adding another word space modelto the ensemble, either a word-based or syntax-based model, brings down performance.
However,the addition of the compound model does lead to aclear gain in performance.
This ensemble finds thestrongest association at a median rank of 2, and amean of 11.8.
In total, 249 strong associations (outof a total 593) are presented as the best candidateby the model ?
an increase of 28.4% comparedto the log-likelihood ratio.
Hence, despite its poorperformance as a simple model, the compound-based approach can still give useful informationabout the strong association of a cue word whencombined with other models.
Based on the origi-nal ranks, the increase from the previous ensem-ble is not statistically significant (W = 23929,p = .31).
If we consider differences at the startof the neighbour list more important and comparethe logarithms of the ranks, however, the increasebecomes significant (W = 29787.5, p = 0.0008).Its precise impact should thus further be investi-gated.2In the case of the harmonic mean, we actually take thelogarithm of rank+1, in order to avoid division by zero.654mean harmonic meansystems med mean rank1 med mean rank1loglik10 (baseline) 3 16.6 194loglik10 + doc 2 13.1 223 3 13.4 211loglik10 + doc + word10 3 13.8 182 3 14.2 187loglik10 + doc + syn 3 14.4 179 4 14.7 184loglik10 + doc + comp 2 11.8 249 2 12.2 221Table 4: Results of ensemble methods.loglik10 = log-likelihood ratio with context size 10;doc = document-based model;word10 = word-based model with context size 10 and a stoplist;syn = syntax-based model;comp = compound-based model;med = median; rank1 = number of associations at rank 1Let us finally take a look at the types of strongassociations that still tend to receive a low rank inthis ensemble system.
The first group consists ofadjectives that refer to an inherent characteristic ofthe cue word that is rarely mentioned in text.
Thisis the case for tennis ball?yellow, cheese?yellow,grapefruit?bitter.
The second type brings togetherpolysemous cues whose strongest association re-lates to a different sense than that represented byits corpus-based nearest neighbour.
This appliesto Dutch kant, which is polysemous between sideand lace.
Its strongest association, Bruges, isclearly related to the latter meaning, but its corpus-based neighbours ball and water suggest the for-mer.
The third type reflects human encyclopaedicknowledge that is less central to the semantics ofthe cue word.
Examples are police?blue, love?red,or triangle?maths.
In many of these cases, it ap-pears that the failure of the model to recover thestrong associations results from corpus limitationsrather than from the model itself.6 Conclusions and future researchIn this paper, we explored three types of basic ap-proaches to the prediction of strong associationsto a given cue.
Collocation measures like the log-likelihood ratio simply recover those words thatstrongly collocate with the cue.
Word space mod-els look for words that appear in similar contexts,defined as documents, context words or syntac-tic relations.
The compounding approach, finally,searches for words that combine with the target toform a compound.
The log-likelihood ratio witha large context size emerged as the best predic-tor of strong association, followed closely by thedocument-based word space model.
Moreover,we showed that an ensemble method combiningthe log-likelihood ratio, the document-based wordspace model and the compounding approach, out-performed any of the basic methods by almost30%.In a number of ways, this paper is only a firststep towards the successful modelling of cue?association relations.
First, the newspaper cor-pus that served as our data has some restrictions,particularly with respect to diversity of genres.
Itwould be interesting to investigate to what degreea more general corpus ?
a web corpus, for in-stance ?
would be able to accurately predict awider range of associations.
Second, the mod-els themselves might benefit from some additionalfeatures.
For instance, we are curious to findout what the influence of dimensionality reductionwould be, particularly for document-based wordspace models.
Finally, we would like to extendour test set from strong associations to more asso-ciations for a given target, in order to investigatehow well the discussed models predict relative as-sociation strength.ReferencesJean Aitchinson.
2003.
Words in the Mind.
An Intro-duction to the Mental Lexicon.
Blackwell, Oxford.John A. Bullinaria and Joseph P. Levy.
2007.
Ex-tracting semantic representations from word co-occurrence statistics: A computational study.
Be-haviour Research Methods, 39:510?526.Curt Burgess, Kay Livesay, and Kevin Lund.
1998.Explorations in context space: Words, sentences,discourse.
Discourse Processes, 25:211?257.655Ed H. Chi, Peter Pirolli, Kim Chen, and James Pitkow.2001.
Using information scent to model user infor-mation needs and actions on the web.
In Proceed-ings of the ACM Conference on Human Factors andComputing Systems (CHI 2001), pages 490?497.Kenneth Ward Church and Patrick Hanks.
1989.
Wordassociation norms, mutual information and lexicog-raphy.
In Proceedings of ACL-27, pages 76?83.James R. Curran.
2002.
Ensemble methods for au-tomatic thesaurus extraction.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP-2002), pages 222?229.Simon De Deyne and Gert Storms.
2008.
Word asso-ciations: Norms for 1,424 Dutch words in a contin-uous task.
Behaviour Research Methods, 40:198?205.Ted Dunning.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19:61?74.Peter W. Foltz.
1996.
Latent Semantic Analysis fortext-based research.
Behaviour Research Methods,Instruments, and Computers, 29:197?202.Tuomo Kakkonen, Niko Myller, Jari Timonen, andErkki Sutinen.
2005.
Automatic essay grading withprobabilistic latent semantic analysis.
In Proceed-ings of the 2nd Workshop on Building EducationalApplications Using NLP, pages 29?36.Thomas K. Landauer and Susan T. Dumais.
1997.
Asolution to Plato?s problem: The Latent SemanticAnalysis theory of acquisition, induction and rep-resentation of knowledge.
Psychological Review,104(2):211?240.Dekang Lin.
1998.
Automatic retrieval and cluster-ing of similar words.
In Proceedings of COLING-ACL98, pages 768?774, Montreal, Canada.Will Lowe and Scott McDonald.
2000.
The di-rect route: Mediated priming in semantic space.In Proceedings of COGSCI 2000, pages 675?680.Lawrence Erlbaum Associates.Lukas Michelbacher, Stefan Evert, and HinrichSchu?tze.
2007.
Asymmetric association measures.In Proceedings of the International Conference onRecent Advances in Natural Language Processing(RANLP-07).Tom M. Mitchell, Svetlana V. Shinkareva, An-drew Carlson, Kai-Min Chang, Vicente L. Malva,Robert A. Mason, and Marcel Adam Just.
2008.Predicting human brain activity associated with themeanings of nouns.
Science, 320:1191?1195.Sebastian Pado?
and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Yves Peirsman, Kris Heylen, and Dirk Geeraerts.2008.
Size matters.
Tight and loose context defini-tions in English word space models.
In Proceedingsof the ESSLLI Workshop on Distributional LexicalSemantics, pages 9?16.Magnus Sahlgren.
2006.
The Word-Space Model.Using Distributional Analysis to Represent Syntag-matic and Paradigmatic Relations Between Wordsin High-dimensional Vector Spaces.
Ph.D. thesis,Stockholm University, Stockholm, Sweden.Sabine Schulte im Walde and Alissa Melinger.
2005.Identifying semantic relations and functional prop-erties of human verb associations.
In Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing, pages 612?619.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?124.Lonneke Van der Plas.
2008.
Automatic Lexico-Semantic Acquisition for Question Answering.Ph.D.
thesis, University of Groningen, Groningen,The Netherlands.Gertjan van Noord.
2006.
At last parsing is now oper-ational.
In Piet Mertens, Ce?drick Fairon, Anne Dis-ter, and Patrick Watrin, editors, Verbum Ex Machina.Actes de la 13e Confe?rence sur le Traitement Au-tomatique des Langues Naturelles (TALN), pages20?42.Tonio Wandmacher, Ekaterina Ovchinnikova, andTheodore Alexandrov.
2008.
Does Latent Seman-tic Analysis reflect human associations?
In Pro-ceedings of the ESSLLI Workshop on DistributionalLexical Semantics, pages 63?70.Manfred Wettler, Reinhard Rapp, and Peter Sedlmeier.2005.
Free word associations correspond to contigu-ities between words in texts.
Journal of QuantitativeLinguistics, 12(2/3):111?122.656
