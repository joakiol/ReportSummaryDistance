Proceedings of NAACL HLT 2007, pages 332?339,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsMulti-document Relationship Fusion viaConstraints on Probabilistic DatabasesGideon MannDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003gideon.mann@gmail.comAbstractPrevious multi-document relationship ex-traction and fusion research has focusedon single relationships.
Shifting the fo-cus to multiple relationships allows for theuse of mutual constraints to aid extrac-tion.
This paper presents a fusion methodwhich uses a probabilistic database modelto pick relationships which violate fewconstraints.
This model allows improvedperformance on constructing corporatesuccession timelines from multiple doc-uments with respect to a multi-documentfusion baseline.1 IntroductionSingle document information extraction of namedentities and relationships has received much atten-tion since the MUC evaluations1 in the mid-90s (Ap-pelt et al, 1993; Grishman and Sundheim, 1996).Recently, there has been increased interest in theextraction of named entities and relationships frommultiple documents, since the redundancy of infor-mation across documents has been shown to be apowerful resource for obtaining high quality infor-mation even when the extractors have access to littleor no training data (Etzioni et al, 2004; Hasegawaet al, 2004).
Much of the recent work in multi-document relationship extraction has focused onthe extraction of isolated relationships (Agichtein,2005; Pasca et al, 2006), but often the goal, as in1http://www.itl.nist.gov/iaui/894.02/related projects/muc/single document tasks like MUC, is to extract a tem-plate or a relational database composed of relatedfacts.With databases containing multiple relationships,the semantics of the database impose constraintson possible database configurations.
This paperpresents a statistical method which picks relation-ships which violate few constraints as measured bya probabilistic database model.
The constraints arehard constraints, and robust estimates are achievedby accounting for the underlying extraction/fusionuncertainty.This method is applied to the problem of con-structing management succession timelines whichhave a rich set of semantic constraints.
Using con-straints on probabilistic databases yields F-Measureimprovements of 5 to 18 points on a per-relationshipbasis over a state-of-the-art multi-document extrac-tion/fusion baseline.
The constraints proposed inthis paper are used in a context of minimally super-vised information extractors and present an alterna-tive to costly manual annotation.2 Semantic Constraints on DatabasesThis paper considers management successiondatabases where each record has three fields: aCEO?s name and the start and end years for thatperson?s tenure as CEO (Table 1 Column 1).
Eachrecord is represented by three binary logical pred-icates: ceo(c,x), start(x,y1), end(x,y2), where c isa company, x is a CEO?s name, and y1 and y2 areyears.22All of the relationships in this paper are defined to be bi-nary relationships.
When extracting relationships of higher ar-332Explicit Implicit Logical ConstraintsRelationships Relationships (a partial list)ceo(c, x) precedes(x1,x2) ceo(c,x1), precedes(x1,x2) ?
ceo(c,x2), end(x1,y), start(x2, y)start(x, y) inoffice(x, y) start(x,y1), inoffice(x,y2), end(x, y3) ?
y1 ?
y2 ?
y3end(x, y) predates(x1, x2) inoffice(x1, y1), inoffice(x2, y2) , y1 < y2 ?
predates(x1,x2)precedes(x1,x2) , inoffice(x1,y1), inoffice(x2,y2) ?
y1 ?
y2Table 1: Database semantics can provide 1) a method to augment the explicit relationships in the databasewith implicit relationships and 2) logical constraints on these explicit and implicit relationships.
In the abovetable, c is a company, xi is a person, and yi is a time.In this setting, database semantics allow for thederivation of other implicit relationships from thedatabase: the immediate predecessor of a given CEO(precedes(x1,x2)), all predecessors of a given CEO(predates(x1,x2)) and all years the CEO was in of-fice (inoffice(x,y3)), where x2 is a CEO?s name andt3 a year (Table 1 Column 2).These implicit relationships and the original ex-plicit relationships are governed by a series of se-mantic relations which impose constraints on thepermissible database configurations (Table 1 Col-umn 3).
For example, it will always be true that aCEO?s start date precedes their end date:?x : start(x, y1), end(x, y2) ?
y1 ?
y2.Multi-document extraction of single relationshipsexploits redundancy and variety of expression to ex-tract accurate information from across many docu-ments.
However, these are not the only benefits ofextraction from a large document collection.
As wellas being rich in redundant information, large docu-ment collections also contain a wealth of secondaryrelationships which are related to the relationship ofinterest via constraints as described above.
Thesesecondary relationships can yield benefits to aug-ment those achieved by redundancy.3 Multi-Document Database FusionThere are typically two steps in the extraction of sin-gle relationships from multiple documents.
In thefirst step, a relationship extractor goes through thecorpus, finds all possible relationships r in all sen-tences s and gives them a score p(r|s).
Next, theity, typically binary relationships are combined (McDonald etal., 2005).relationships are fused across sentences to generateone score ?r for each relationship.This paper proposes a third step which combinesthe fusion scores across relationships.
This sectionfirst presents a probabilistic database model gener-ated from fusion scores and then shows how to usethis model for multi-document fusion.3.1 A Probabilistic Database ModelA relationship r is defined to be a 3-tuple rt,a,b =r(t, a, b), where t is the type of the relationship (e.g.start), and a and b are the arguments of the binaryrelationship.3To construct a probabilistic database for a givencorpus, the weights generated in relationship fusionare normalized to provide the conditional probabilityof a relationship given its type:p(rt,a,b1 |t1) =?rt,a,b1?ri:rti=t?rt,a,bi,where ?r is the fusion score generated by the extrac-tion/fusion system.4 By applying a prior over typesp(t), a distribution p(r1, t1) can be derived.
Givenstrong independence assumptions, the probability ofan ordered database configurationR = r1..n of typest1..n is:p(r1..n, t1..n) =n?i=1p(ri, ti).
(1)3For readibility in future examples, ?a?
and ?b?
are replacedby the types of their arguments.
For example, for start the yearin which the CEO starts is referred to as ryear .4The following fusion method does not depend on a par-ticular extraction/fusion architecture or training methodology,merely this conditional probability distribution.333As proposed, the model in Equation 1 is faultysince the relationships in a database are not inde-pendent.
Given a set of database constraints, certaindatabase configurations are illegal and should be as-signed zero probability.
To address this, the modelin Equation 1 is augmented with constraints that ex-plicitly set the probability of a database configura-tion to zero when they are violated.A database constraint is a logical formula?(r1..pi(?
)), where pi(?)
is the arity of the constraint?.
For the constraints presented in this paper, allconstraints ?
are modeled with two terms ??
and ??where:?(r1..pi(?))
=(??(r1..pi(?))
?
??(r1..pi(?
))).For a set of relationships, a constraint holds if ?(?
)is true, and the constraint applies if ??(?)
is true.
Aconstraint ?(?)
can only be violated (false) when theconstraint applies, since: (false ?
X) = true.In application to a database, each constraint ?
isquantified over the database to become a quantifiedconstraint ?r1..n .
For example, the constraints that aperson?s start date must come before their end date isuniversally quantified over all pairs of relationshipsin a configuration R = r1..n:?r1..n = ?r1,r2 ?
R : ?
(r1, r2) =(rt1 = start, rt2 = end, rceo1 = rceo2 )?
(ryear1 < ryear2 ).This constraint applies to start and end relationshipswhose CEO argument matches and is violated whenthe years are not in order.
If the quantified constraint?r1..n is true for a given database configuration r1..nthen it holds.To ensure that only legal database configurationsare assigned positive probabilities, Equation 1 isaugmented with a factor?
?r1..n ={1 if ?r1..nholds0 otherwise .To include a constraint ?, the database model inEquation 1 is extended to be:p?
(r1..n, t1..n) =1Z(?ip(ri, ti))?
?r1..n,where Z is the partition function and corresponds tothe total probability of all database configurations.
Aset of constraints ?1..Q = ?1..?Q can be integratedsimilarly:p?1..Q(r1..n, t1..n) =1Z(?ip(ri, ti))?q?
?qr1..n(2)With these added constraints, the probabilisticdatabase model assigns non-zero probability only todatabases which don?t violate any constraints.3.2 Constraints on Probabilistic Databases forRelationship RescoringThough the constrained probabilistic databasemodel in Equation 2 is theoretically appealing, itwould be infeasible to calculate its partition func-tion which requires enumeration of all legal 2ndatabases.
This section proposes two methods forre-scoring relationships with regards to how likelythey are to be present in a legal database configu-ration using the model proposed above.
The firstmethod is a confidence estimate based on how likelyit is that ?
holds for a given relationship r1:??
(r1, t1) = Ep(r2..n,t2..n)[??r1..pi(?)]=?r2..pi(?)(?pi(?
)i=2 p(ri, ti))??r1..pi(?)?r2..pi(?)(?pi(?
)i=2 p(ri, ti))=?r2..pi(?)p?
(r1..n, t1..n)?r2..pi(?
)p(r1..n, t1..n),where the expectation that the constraint holdsis equivalent to the likelihood ratio between thedatabase probability models with and without con-straints.
In effect, this model measures the expec-tation that the constraint holds for a finite database?look-ahead?
of size pi(?)?
1.With this method, for a constraint to reduce theconfidence in a particular relationship by half, halfof all configurations would have to violate the con-straint.5 Since inconsistencies are relatively rare, fora given relationship ??
(r, t) ?
1 (i.e.
almost allsmall databases are legal).5Assuming equal probability for all relationships.334To remedy this, another factor ??
is defined simi-larly to ?
?, except that it takes a value of 1 only if theconstraint applies to that database configuration.
Anapplicability probability model is then defined as:p?
(r1..n, t1..n) =1Z(?ip(ri, ti))?
?r1..n.The second confidence estimate is based on howlikely it is that the constraint is holds in cases whereit applies (i.e.
is not violated):??,?
(r1, t1)= Ep?(r2..n,t2..n)[??r1..pi(?)]=?r2..pi(?)(?pi(?
)i=2 p(ri, ti))??r1..pi(?)??r1..pi(?)?r2..pi(?)(?pi(?
)i=2 p(ri, ti))??r1..pi(?
).When the constraint doesn?t apply it cannot be vio-lated, so this confidence estimate ignores those con-figurations that can?t be affected by the constraint.Recall that ??
(r, t) is the likelihood ratio be-tween the probability of configurations in which rholds for constraint ?
and all configurations.
In con-trast, ??,?
(r, t) is the likelihood ratio between thedatabase configurations where r applies and holdsfor ?
and the database configurations where ?
ap-plies.
In the later ratio, for confidence in a particularrelationship to be cut in half, only half of the con-figurations which might actually contain an incon-sistency would be required to produce a violation.6As a result, ??,?
(r, t) gives a much higher penalty torelationships which create inconsistencies than does??
(r, t).In order to apply multiple constraints, indepen-dent database look-aheads are generated for eachconstraint q:?
?1..Q,?1..Q(r1, t1) =?q?
?q ,?q(r1, t1).For a particular relationship type, these confidencescores are calculated and then used to rank the rela-6For example, for a start relationship and the constraint thata CEO must start before they end, this method would only ex-amine configurations of one start and one end relationship forthe same CEO.
The confidence in a particular start date wouldbe halved if half of the proposed end dates for a given CEOoccurred before it.tionships via:c?
?1..Q,?1..Q(r1, t1) = p(r1, t1)?q?
?q ,?q(r1, t1)(3)Databases with different precision/recall trade offscan be selected by descending the ranked list.74 ExperimentsIn order to test the fusion method proposed above,human annotators manually constructed truth dataof complete chief executive histories for 18 Fortune-500 companies using online resources.
Extractionfrom these documents is particularly difficult be-cause these data have vast differences in genre andstyle and are considerably noisy.
Furthermore, thetask is complicated to start with.8A corpus was created for each company by is-suing a Google query for ?CEO-of-Company ORCompany-CEO?, and collecting the top ranked doc-uments, generating up to 1000 documents per com-pany.
The data was then split randomly into training,development and testing sets of 6, 4, and 8 compa-nies.Training : Anheuser-Busch, Hewlett-Packard,Lenner, McGraw-Hill, Pfizer, RaytheonDev.
: Boeing, Heinz, Staples, TextronTest : General Electric, General Motors,Gannett, The Home Depot, IBM,Kroger, Sears, UPSGround truth was created from the entire web, butsince the corpus for each company is only a smallweb snapshot, the experimental results are not simi-lar to extraction tasks like MUC and ACE in that thecorpus is not guaranteed to contain the informationnecessary to build the entire database.
In particular,7One thing to note is that since all relationships are givenconfidence estimates separately, this process may result ulti-mately in a database where constraints are violated.
A potentialsolution, which is not explored here, would be to incrementallyadd relationships to the database from the ranked list only iftheir addition doesn?t make the database inconsistent.8For example, in certain companies, the title of the chiefexecutive has changed over the years, often going from ?Presi-dent?
to ?Chief Executive Officer?.
To make things more com-plicated, after the change, the role of ?President?
may still hangon as a subordinate to the CEO!3351) Only one start or end per person.
?r1, r2 : ?
(r1, r2) = (rtype1 = rtype2 = (start ?
end), rceo1 = rceo2 ) ?
(ryear1 = ryear2 )2) Only a CEO?s start or end dates belong in the database.
?r1?r2 : ?
(r1, r2) = (rtype1 = start ?
end, rtype2 = ceo) ?
(rceo1 = rceo2 )3) Start dates come before end dates.
?r1, r2 : ?
(r1, r2) = (rtype1 = start, rtype2 = end, rceo1 = rceo2 ) ?
(ryear1 ?
ryear2 )4) Can?t be in the middle of someone else?s tenure.
?r1, r2, r3 : ?
(r1, r2, r3) = (rtype1 = start ?
inoffice, rtype2 = end ?
inoffice, rtype3 = start ?
inoffice ?
end,rceo1 = rceo2 6= rceo3 , ryear1 < ryear2 ) ?
(ryear3 ?
ryear1 ?
ryear3 ?
ryear2 )5) CEO?s are only in office after their start.
?r1, r2 : ?
(r1, r2) = (rtype1 = start, rtype2 = inoffice, rceo1 = rceo2 ) ?
(ryear1 ?
ryear2 )6) CEO?s are only in office before their end.
?r1, r2 : ?
(r1, r2) = (rtype1 = inoffice, rtype2 = end, rceo1 = rceo2 ) ?
(ryear1 ?
ryear2 )7) Someone?s end is the same as their successor?s start.
?r1, r2, r3 : ?
(r1, r2, r3) =(rtype1 = end, rtype2 = start, rtype3 = precedes, rceo1 = rfirst3 , rceo2 = rsecond3 ) ?
(ryear1 = ryear2 )8) All of the someone?s dates (start, inoffice, end) are before their successors.
?r1, r2, r3 : ?
(r1, r2, r3) = (rtype1 = start ?
end ?
inoffice, rtype2 = start ?
inoffice ?
end, rtype3 = precedes,rceo1 = rfirst3 , rceo2 = rsecond3 ) ?
(ryear1 ?
ryear2 )9) Only CEO succession in the database.
?r1?r1, r2 : ?
(r1, r2, r3) = (rtype1 = precedes, rtype2 = rtype3 = ceo) ?
(rfirst1 = rceo2 , rsecond1 = rceo3 )Table 2: For a CEO succession database like the one presented in Table 1, the above constraints must holdif the database is consistent.many CEOs from pre-Internet years were either in-frequently mentioned or not mentioned at all in thedatabase.9 In the following experiments, recall is re-ported for facts that were retrieved by the extractionsystem.4.1 Relationship Extraction and FusionA two-class maximum-entropy classifier was trainedfor each relationship type.
Each classifier takes asentence and two marked entities (e.g.
a person anda year)10 and gives the probability that a relation-ship between the two entities is supported by thesentence.
For each relationship type, one of the ele-ments is designated as the ?hook?
in order to gener-ate likely negative examples.11 In training, all entitypairs are collected from the corpus.
The pairs whose?hook?
element doesn?t appear in the database arethrown out.
The remaining pairs are then marked9Another consequence is that assessing the effectiveness ofthe relationships extraction on a per-extraction basis is difficult.Because there are no training sentences where it is known thatthe sentence contains the relationship of interest, grading per-extraction results can be deceptive.10The person tagger used is the named-entity tagger fromOpenNLP tools and the year tagger simply finds any four digitnumbers between 1950 and 2010.11For the CEO relationship, the company was taken to bethe hook.
For the other relationships the hook was the primaryCEO.by exact match to the database.
In testing, the rela-tionship extractor yields the probability p(r|s) of anentity pair relationship r in a particular sentence s.The features used in the classifier are: unigramsbetween the given information and the target, dis-tance in words between the given information andthe target, and the exact string between the given in-formation and the target (if less that 3 words long).After extraction from individual sentences, the re-lationships are fused together such that there is onescore for each unique entity pair.
In the case of per-son names, normalization was performed to mergecoreferent but lexically distinct names (e.g.
?PhilCondit?
and ?Philip M.
Condit?
).In the following experiments, the baseline fusionscore is:?r =?sp(r|s) (4)4.2 Experimental ResultsGiven the management succession database pro-posed in Section 2, Table 2 enumerates a set of quan-tified constraints.
Information extraction and fusionwere run separately for each company to create aprobabilistic database.
In this section, various con-straint sets are applied, either individually or jointly,and evaluated in two ways.
The first measures per-relationship precision/recall using the model pro-336Second (8)First Before00.10.20.30.40.50.60.70.80.90  0.2  0.4  0.6  0.8  1BaselineOnly One (1)CEOs Only (2)Start Before End (3)Inoffice After Start (5)2,52,3,5,Figure 1: Precision/Recall curve for start(x,t) rela-tionships.
The joint constraint ?2,3,5,8?
is the bestperforming, even though constraints ?3?
and ?8?
(not pictured) alone don?t perform well.posed and the second looks at the precision/recallof a heterogeneous database with many relationshiptypes.
Both evaluations examine the ranked lists ofrelationships, where the relationships are ranked byrescoring via constraints on probabilistic databases(Equation 3) and compared to the baseline fusionscore (Equation 4).
The evaluations use two stan-dard metrics, interpolated precision at recall level i(PRi), and MaxF1:PRi = maxj?iPRj,MaxF1 = maxi21PRi+ 1Ri.Figures 1, 2, and 3 show precision/recall curvesfor the application of various sets of constraints.
Ta-ble 3 lists the MaxF1 scores for each of the con-straint variants.
For start and end, the majority ofconstraints are beneficial.
For precedes, only theconstraint that improved performance constraintsboth people in the relationship to be CEOs.
Acrossall relationships, performance is hurt when using theconstraint that there could only be one relationshipof each type for a given CEO.
The reason behindthis is that the confidence estimate based on thisconstraint favors relationships with few competitors,and those relationships are typically for people whoare infrequent in the corpus (and therefore unlikelyto be CEOs).The best-performing constraint sets yield between5 and 18 points of improvement on Max F1 (Ta-ble 3).
Surprisingly, the gains from joint con-00.10.20.30.40.50.60.70  0.2  0.4  0.6  0.8  1BaselineOnly One (1)CEOs Only (2)Inoffice Before End (6)2,6Figure 2: Precision/Recall curve for end(x,t) rela-tionships alone.
The joint constraint ?2,6?
is the bestperforming.00.10.20.30.40.50.60  0.2  0.4  0.6  0.8  1BaselineEnd is Start (7)First Before Second (8)Only CEO Succession (9)Figure 3: Precision/Recall for precedes(x,y) rela-tionships alone.
Though the constraint ?First BeforeSecond (8)?
helps performance on start(x,t) relation-ships, the only constraint which aids here is ?OnlyCEOs Succession (9)?.straints are sometimes more than their additivegains.
?2,3,5,6,8?
is 6 points better for the start rela-tionship than ?2,3,5,6?, but the gains from ?8?
aloneare negligible.These performance gains on the individual rela-tionship types also lead to gains when generating anentire database (Figure 4).
The highest performingconstraint is the ?CEOs Only (2)?
constraint, whichoutperforms the joint constraints of the previous sec-tion.
One reason the joint constraints don?t do aswell here is that each constraint makes the confi-dence estimate smaller and smaller.
This doesn?thave an effect when judging the relationship typesindividually, but when combining the relationshipsresults, the fused relationships types (start, end) be-337Max F1Constraint Set Start End Pre.
DB?
(baseline) 31.2 35.8 34.5 37.9Only One (1) 10.5 7.2 - 38.1CEOs Only (2) or (9) 43.3 39.4 39.4 42.9Start Before End (3) 40.8 32.8 - 40.9No Overlaps (4) 31.5 35.9 - 36.8Inoffice After Start (5) 32.5 - - 38.2Inoffice Before End (6) - 36.5 - 37.4End is Start (7) 7.3 8.0 20.7 39.2First before Second (8) 31.4 35.6 26.3 38.12,5,6 43.3 40.8 - 42.72,3,5,6 43.9 43.3 - 42.22,3,5,6,8 49.3 43.9 26.3 40.9Table 3: Max F1 scores for three relationshipsStart(x,t), End(x,t) and Precedes(x,y)) in isolationand within the context of whole database DB.
Thejoint constraints perform best for the explicit rela-tionships in isolation.
Using constraints on implicitderived fields (Inoffice and Precedes) provides ad-ditional benefit above constraints strictly on explicitdatabase fields (start, end, ceo).come artificially lower ranked than the unfused rela-tionship type (ceo).
The best performing contrainedprobabilistic database approach beats the baselineby 5 points.5 Related WorkTechniques for information extraction from min-imally supervised data have been explored byBrin (1998), Agichtein and Gravano (2000), andRavichandran and Hovy (2002).
Those techniquespropose methods for estimating extractors from ex-ample relationships and a corpus which contains in-stances of those relationships.Nahm and Mooney (2002) explore techniques forextracting multiple relationships in single documentextraction.
They learn rules for predicting certainfields given other extracted fields (i.e.
a someonewho lists Windows as a specialty is likely to knowMicrosoft Word).Perhaps the most related work to what is pre-sented here is previous research which uses databaseinformation as co-occurrence features for informa-tion extraction in a multi-document setting.
MannInoffice Before End (6)00.10.20.30.40.50.60.70.80.910  0.2  0.4  0.6  0.8  1BaselineCEOs Only (2)Start Before End (3)2, Inoffice After Start (5)2,3,5,6Figure 4: Precision/Recall curve for whole databasereconstruction.
Performance curves using con-straints dominate the baseline.and Yarowsky (2005) present an incremental ap-proach where co-occurrence with a known relation-ship is a feature added in training and test.
Cu-lotta et al (2006) introduce a data mining approachwhere discovered relationships from a database areused as features in extracting new relationships.
Thedatabase constraints presented in this paper providea more general framework for jointly conditioningmultiple relationships.
Additionally, this constraint-based approach can be applied without special train-ing of the extraction/fusion system.In the context of information fusion of single rela-tionships across multiple documents, Downey et al(2005) propose a method that models the probabili-ties of positive and negative extracted classifications.More distantly related, Sutton andMcCallum (2004)and Finkel et al (2005) propose graphical modelsfor combining information about a given entity frommultiple mentions.In the field of question answering, Prager et al(2004) answer a question about the list of composi-tions produced by a given subject by looking for re-lated information about the subject?s birth and death.Their method treats supporting information as fixedhard constraints on the original questions and are ap-plied in an ad-hoc fashion.
This paper proposes aprobabilistic method for using constraints in the con-text of database extraction and applies this methodover a larger set of relations.Richardson and Domingos (2006) propose amethod for reasoning about databases and logicalconstraints using Markov Random Fields.
Their338model applies reasoning starting from a knowndatabase.
In this paper the database is built from ex-traction/fusion of relationships from web pages andcontains a significant amount of noise.6 ConclusionThis paper has presented a probabilistic method forfusing extracted facts in the context of database ex-traction when there exist logical constraints betweenthe fields in the database.
The method estimates theprobability than the inclusion of a given relationshipwill violate database constraints by taking into ac-count the uncertainty of the other extracted relation-ships.
Along with the relationships explicitly listedin the database, constraints are formed over implicitfields directly recoverable from the explicit listed re-lationships.The construction of CEO succession timelines us-ing minimally trained extractors from web text is aparticularly challenging problem because of noiseresulting from the wide variation in genre in the cor-pora and errors in extraction.
The use of constraintson probabilistic databases is effective in resolvingmany of these errors, leading to improved precisionand recall of retrieved facts, with F-measure gains of5 to 18 points.The method presented in this paper combinessymbolic and statistical approaches to natural lan-guage processing.
Logical constraints are mademore robust by taking into account the uncertaintyof the extracted information.
An interesting areaof future work is the application of data mining tosearch for appropriate constraints to integrate intothis model.AcknowledgementsThis work was supported in part by DoD contract #HM1582-06-1-2013.
Any opinions, findings and conclusions or recom-mendations expressed in this material belong to the author anddo not necessarily reflect those of the sponsor.ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: Extracting re-lations from large plain-text collections.
In Proceedings ofICDL, pages 85?94.E.
Agichtein.
2005.
Extracting Relations from Large Text Col-lections.
Ph.D. thesis, Columbia University.D.
Appelt, J. Hobbs, J.
Bear, D. Israel, and M. Tyson.
1993.FASTUS: a finite-state processor for information extractionfrom real-world text.
In Proceedings of IJCAI.S.
Brin.
1998.
Extracting patterns and relations from the worldwide web.
In WebDB Workshop at 6th International Confer-ence on Extending Database Technology, EDBT?98, pages172?183.A.
Culotta, A. McCallum, and J. Betz.
2006.
Integrating prob-abilistic extraction models and data mining to discover rela-tions and patterns in text.
In HLT-NAACL, pages 296?303,New York, NY, June.D.
Downey, O. Etzioni, and S. Soderland.
2005.
A probabilisticmodel of redundancy in information extraction.
In IJCAI.O.
Etzioni, M. Cafarella, D. Downey, S. Kok, A-M. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.
2004.
Web-scale information extraction in knowitall.
In WWW.J.
Finkel, T. Grenager, , and C. Manning.
2005.
Incorporatingnon-local information into information extraction systems bygibbs sampling.
In ACL.R.
Grishman and B. Sundheim.
1996.
Message understandingconference-6: A brief history.
In Proceedings of COLING.T.
Hasegawa, S. Sekine, and R. Grishman.
2004.
Discoveringrelations amoung named entities from large corpora.
In ACL.G.
Mann and D. Yarowsky.
2005.
Multi-field information ex-traction and cross-document fusion.
In ACL.R.
McDonald, F. Pereira, S. Kulick, S. Winters, Y. Jin, andP.
White.
2005.
Simple algorithms for complex relationshipextraction with applications to biomedical ie.
In Proceed-ings of ACL.U.
Nahm and R. Mooney.
2002.
Text mining with informationextraction.
In Proceedings of the AAAI 2220 Spring Sympo-sium on Mining Answers from Texts and Knowledge Bases,pages 60?67.M.
Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006.Organizing and searching the world wide web of facts - stepone: The one-million fact extracion challenge.
In AAAI.J.
Prager, J. Chu-Carroll, and K. Czuba.
2004.
Question an-swering by constraint satisfaction: Qa-by-dossier with con-straints.
In Proceedings of ACL, pages 574?581.D.
Ravichandran and E. Hovy.
2002.
Learning surface textpatterns for a question answering system.
In Proceedings ofACL, pages 41?47.M.
Richardson and P. Domingos.
2006.
Markov logic net-works.
Machine Learning, 62:107?136.C.
Sutton and A. McCallum.
2004.
Collective segmenta-tion and labeling of distant entities in information extraction.Technical Report TR # 04-49, University of Massachusetts,July.
Presented at ICML Workshop on Statistical RelationalLearning and Its Connections to Other Fields.339
