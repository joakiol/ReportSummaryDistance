Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 209?212,Sydney, July 2006. c?2006 Association for Computational LinguisticsDescription of the NCU Chinese Word Segmentation and Named EntityRecognition System for SIGHAN Bakeoff 2006Yu-Chieh Wu Jie-Chi Yang Qian-Xiang LinDept.
of Computer Science andInformation EngineeringGraduate Institute of Net-work Learning TechnologyDept.
of Computer Science andInformation EngineeringNational Central University National Central University National Central UniversityTaoyuan, Taiwan Taoyuan, Taiwan Taoyuan, Taiwanbcbb@db.csie.ncu.edu.tw yang@cl.ncu.edu.tw 93522083@cc.ncu.edu.twAbstractAsian languages are far from most west-ern-style in their non-separate word se-quence especially Chinese.
Thepreliminary step of Asian-like languageprocessing is to find the word boundariesbetween words.
In this paper, we presenta general purpose model for both Chineseword segmentation and named entity rec-ognition.
This model was built on theword sequence classification with prob-ability model, i.e., conditional randomfields (CRF).
We used a simple feature setfor CRF which achieves satisfactory clas-sification result on the two tasks.
Ourmodel achieved 91.00 in F rate in UPUC-Treebank data, and 78.71 for NER task.1 IntroductionWith the rapid expansion of text media sourcessuch as news articles, technical reports, there is anincreasing demand for text mining and processing.Among different cultures and countries, the Asianlanguages are far from the other languages, there isnot an explicit boundary between words, for exam-ple Chinese.
Similar to English, the preliminarystep of most natural language processing is to ?to-kenize?
each word.
In Chinese, the word tokeniza-tion is also known as word segmentation orChinese word tokenization.To support the above targets, it is necessary todetect the boundaries between words in a givensentence.
In tradition, the Chinese word segmenta-tion technologies can be categorized into threetypes, (heuristic) rule-based, machine learning, andhybrid.
Among them, the machine learning-basedtechniques showed excellent performance in manyresearch studies (Peng et al, 2004; Zhou et al,2005; Gao et al, 2004).
This method treats theword segmentation problem as a sequence of wordclassification.
The classifier online assigns either?boundary?
or ?non-boundary?
label to each wordby learning from the large annotated corpora.
Ma-chine learning-based word segmentation method isquite similar to the word sequence inference tech-niques, such as part-of-speech (POS) tagging,phrase chunking (Wu et al, 2006a) and named en-tity recognition (Wu et al, 2006b).In this paper, we present a prototype for Chineseword segmentation and named entity recognitionbased on the word sequence inference model.Unlike previous researches (Zhou et al, 2005; Shi,2005), we argue that without using the word seg-mentation information, Chinese named entity rec-ognition task can also be viewed as a variant wordsegmentation technique.
Therefore, the two taskscan be accomplished without adapting the wordsequence inference model.
The preliminary ex-perimental result show that in the word segmenta-tion task, our method can achieve 91.00 in F ratefor the UPUC Chinese Treebank data, while it at-209tends 78.76 F rate for the Microsoft Chinesenamed entity recognition task.The rest of this paper is organized as follows.Section 2 describes the word sequence inferencemodel and the used learner.
Experimental resultand evaluations are reported in section 3.
Finally,in section 4, we draw conclusion and future re-marks.2 System DescriptionIn this section, we firstly describe the overall sys-tem architecture for the word segmentation andnamed entity recognition tasks.
In section 2.2, theemployed classification model- conditional randomfields (CRF) is then presented.2.1 Word Sequence ClassificationSimilar to English text chunking (Ramshaw andMarcus, 1995; Wu et al, 2006a), the word se-quence classification model aims to classify eachword via encoding its context features.
An examplecan be shown in Figure 1.
In Figure1, the model isclassifying the Chinese character ???
(country).The second row in Figure 1 means the correspond-ing category of each in the word-segmentation(WS) task, while the third row indicates the classin the named entity recognition (NER) task.
Forthe WS task, there are only two word types, B-CP(Begin of Chinese phrase) and I-CP (Interior ofChinese phrase).
In contrast, the word types in theNER task depend on the pre-defined named class.For example, both in MSR and CityU datasets,person, location, and organization should be identi-fied.
In this paper, we used the similar IOB2 repre-sentation style (Wu et al, 2006a) to express theChinese word structures.By encoding with IOB style, both WS and NERproblems can be viewed as a sequence of wordclassification.
During testing, we seek to find theoptimal word type for each Chinese character.These types strongly reflect the actual wordboundaries for Chinese words or named entityphrases.To effect classify each character, in this paper,we employ 13 feature templates to capture the con-text information of it.
Table 1 lists the adopted fea-ture templates.Table 1: Feature template used for both Chi-nese word segmentation and named entity rec-ognition tasksFeatureTypeExamples Feature Type ExamplesW-2 ?
W0 + W+1 ?+?W-1 ?
W+1 + W+2 ?+?W0 ?
W+1 + W+2 ?+?W+1 ?
W-2+W-1+W0 ?+?+?W+2 ?
W-1+W0+W+1 ?+?+?W-2 + W-1 ?+?
W0+W+1+W+2 ?+?+?W-1 + W0 ?+?2.2 Conditional Random FieldsConditional random field (CRF) was an extensionof both Maximum Entropy Model (MEMs) andHidden Markov Models (HMMs) that was firstlyintroduced by (Lafferty et al, 2001).
CRF definedconditional probability distribution P(Y|X) of givensequence given input sentence where Y is the?class label?
sequence and X denotes as the obser-vation word sequence.A CRF on (X,Y) is specified by a feature vectorF of local context and the corresponding featureweight ?.
The F can be treated as the combinationof state transition and observation value in conven-tional HMM.
To determine the optimal label se-quence, the CRF uses the following equation toestimate the most probability.
),(maxarg),|(maxarg xyFxyPyyy??
==CP: Chinese word phrase   LOC: Location   ORG: Organization   O: Non-named entity wordFigure 1: Sequence of word classification model210The most probable label sequence y can be effi-ciently extracted via the Viterbi algorithm.
How-ever, training a CRF is equivalent to estimate theparameter set?for the feature set.
In this paper, wedirectly use the quasi-Newton L-BFGS 1  method(Nocedal and Wright, 1999) to iterative update theparameters.3 Evaluations and Experimental Result3.1 Dataset and EvaluationsWe evaluated our model in the close track onUPUC Chinese Treebank for Chinese word seg-mentation task, and CityU corpus for Chinese NERtask.
Both settings are the same for the two tasks.The evaluations of the two tasks were mainlymeasured by the three metrics, namely recall, pre-cision, and f1-measurement.
However, the evalua-tion style for the NER and WS is quite different.
InWS, participant should reformulate the testing datainto sentence level whereas the NER was evaluatedin the token-level.
Table 2 lists the results of thetwo tasks with our preliminary model.Table 2: Official results on the word segmenta-tion and named entity recognition tasksDataset F1-measureWord segmentation UPUC 91.00Named entity recognition CityU 78.71Table 3: Experimental results for the threeChinese word segmentation datasetsClosed Task CityU MSR UPUCRecall 0.958 0.940 0.917Precision 0.926 0.906 0.904F-measure 0.942 0.923 0.9103.2 Experimental Result on Word Segmenta-tion TaskTo explore the effectiveness of our method, we goon extend our model to the other three tasks for theWS track, namely CityU, MSR.
Table3 shows theexperimental results of our model in the all closeWS track except for CKIP corpus.
These results donot officially provided by the SIGHAN due to thetime limitation.1 http://www-unix.mcs.anl.gov/tao/3.3 Experimental Result on Named EntityRecognition TaskIn the second experiment, we focus on directlyadapting our method for the NER track.
Table 4lists the experimental result of our method in theCityU and MSR datasets.
It is worth to note thatdue to the different evaluation style in NER tracks,our tokenization rules did not consistent with theSIGHAN provided testing tokens.
Our preliminarytokenization rules produced 371814 characters forthe testing data, while there are 364356 tokens inthe official provided testing set.
Such a big troubledeeply earns the actual performance of our model.To propose a reliable and actual result, we directlyevaluate our method in the official provided testingset again.
As shown in Table 4, the our methodachieved 0.787 in F rate with non-correct version.In contrast, after correcting the Chinese tokeniza-tion rules as well as SIGHAN official providedtokens, our method significantly improved from0.787 to 0.868.
Similarly, our method performedvery on the MSR track which reached 0.818 in Frate.Table 4: Experimental results for MSR andCity closed NER tasksClosed Task City (officialresult)City(correct)MSRRecall 0.697 0.931 0.752Precision 0.935 0.814 0.896F-measure 0.787 0.868 0.8184 Conclusions and Future WorkChinese word segmentation is the most importantfoundations for many Chinese linguistic technolo-gies such as text categorization and informationretrieval.
In this paper, we present simple Chineseword segmentation and named entity recognitionmodels based on the conventional sequence classi-fication technique.
The main focus of our work isto provide a light-weight and simple model thatcould be easily ported to different domains andlanguages.
Without any prior knowledge and rules,such a simple technique shows satisfactory resultson both word segmentation and named entity rec-ognition tasks.
To reach state-of-the-art this modelstill needs to employed more detail feature enginesand analysis.
In the future, one of the main direc-tions is to extend this model toward full unsuper-211vised learning from large un-annotated text.
Min-ing from large unlabeled data have been showedbenefits to improve the original accuracy.
Thus,not only the more stochastic feature analysis, butalso adjust the learner from unlabeled data are im-portant future remarks.ReferencesLafferty, J., McCallum, A., and Pereira, F. 2001.Conditional Random Field: Probabilistic modelsfor segmenting and labeling sequence data.
In Pro-ceedings of the International Conference on Ma-chine Learning.Gao, J., Wu, A., Li, M., Huang, C. N., Li, H., Xia, X.,and Qin, H. 2004.
Adaptive Chinese word segmen-tation.
In Proceedings the 41st Annual Meeting ofthe Association for Computational Linguistics, pp.21-26.Lance A. Ramshaw and Mitchell P. Marcus.
1995.Text chunking using transformation-based learning.In Proceedings of the 3rd Workshop on VeryLarge Corpora, pages 82-94.Nocedal, J., and Wright, S. 1999.
Numerical optimi-zation.
Springer.Peng, F., Feng, F., and McCallum, A.
2004.
Chinesesegmentation and new word detection using condi-tional random fields.
In Porceedings of the Com-putational Linguistics, pp.
562-568.Shi, W. 2005.
Chinese Word Segmentation Based OnDirect Maximum Entropy Model.
In Proceedingsof the Fourth SIGHAN Workshop on ChineseLanguage Processing.Wu, Y. C., Chang, C. H. and Lee, Y. S. 2006a.
Ageneral and multi-lingual phrase chunking modelbased on masking method.
Lecture Notes in Com-puter Science (LNCS): Computational Linguisticsand Intelligent Text Processing, 3878: 144-155.Wu, Y. C., Fan, T. K., Lee Y. S. and Yen, S. J.
2006b.Extracting named entities using support vectormachines," Lecture Notes in Bioinformatics(LNBI): Knowledge Discovery in Life ScienceLiterature, (3886): 91-103.Wu, Y. C., Lee, Y. S., and Yang, J. C. 2006c.
TheExploration of Deterministic and Efficient De-pendency Parsing.
In Proceedings of the 10th Con-ference on Natural Language Learning (CoNLL).Zhou, J., Dai, X., Ni, R., Chen, J.
2005.
.A HybridApproach to Chinese Word Segmentation aroundCRFs.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing.212
