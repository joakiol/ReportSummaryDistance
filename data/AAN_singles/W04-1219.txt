Exploring Deep Knowledge Resources in Biomedical Name RecognitionZHOU GuoDong    SU JianInstitute for Infocomm Research21 Heng Mui Keng TerraceSingapore 119613Email: {zhougd, sujian}@i2r.a-star.edu.sgAbstractIn this paper, we present a named entityrecognition system in the biomedical domain.
Inorder to deal with the special phenomena in thebiomedical domain, various evidential features areproposed and integrated through a HiddenMarkov Model (HMM).
In addition, a SupportVector Machine (SVM) plus sigmoid is proposedto resolve the data sparseness problem in oursystem.
Besides the widely used lexical-levelfeatures, such as word formation pattern,morphological pattern, out-domain POS andsemantic trigger, we also explore the name aliasphenomenon, the cascaded entity namephenomenon, the use of both a closed dictionaryfrom the training corpus and an open dictionaryfrom the database term list SwissProt and the aliaslist LocusLink, the abbreviation resolution and in-domain POS using the GENIA corpus.1.
The Baseline System1.1 Hidden Markov ModelIn this paper, we use the Hidden Markov Model(HMM) as described in Zhou et al(2002).
Givenan output sequence O , the system findsthe most likely state sequence S  thatmaximizes  as follows:nn ooo ...211 =)nn sss ...211 =|( 11nn OSP?
?==+?=nininiinnnOsPsPSPOSP111111)|(log)(log)(log)|(log(1)From Equation (1), we can see that:?
The first term can be computed by applyingchain rules.
In ngram modeling (Chen et al1996),each tag is assumed to be dependent on the N-1previous tags.?
The second term is the summation of logprobabilities of all the individual tags.?
The third term corresponds to the ?lexical?component (dictionary) of the tagger.The idea behind the model is that it tries toassign each output an appropriate tag (state), whichcontains boundary and class information.
Forexample, ?TCF 1 binds stronger than NF kB toTCEd DNA?.
The tag assigned to token ?TCF?should indicate that it is at the beginning of anentity name and it belongs to the ?Protein?
class;and the tag assigned to token ?binds?
shouldindicate that it does not belong to an entity name.Here, the Viterbi algorithm (Viterbi 1967) isimplemented to find the most likely tag sequence.The problem with the above HMM lies in thedata sparseness problem raised by P  in thethird term of Equation (1).
In this paper, a SupportVector Machine (SVM) plus sigmoid is proposedto resolve this problem in our system.
)|( 1ni Os1.2 Support Vector Machine plus SigmoidSupport Vector Machines (SVMs) are a popularmachine learning approach first presented byVapnik (1995).
Based on the structural riskminimization of statistical learning theory, SVMsseek an optimal separating hyper-plane to dividethe training examples into two classes and makedecisions based on support vectors which areselected as the only effective examples in thetraining set.
However, SVMs produce an un-calibrated value that is not probability.
That is, theunthresholded output of an SVM can berepresented as??+?
?=SViiii bxxkyaxf ),()(                 (2)To map the SVM output into the probability, wetrain an additional sigmoid model(Platt 1999):)exp(11)|1(BAffyp ++==                (3)Basically, SVMs are binary classifiers.Therefore, we must extend SVMs to multi-class(e.g.
K) classifiers.
For efficiency, we apply theone vs. others strategy, which builds K classifiersso as to separate one class from all others, insteadof the pairwise strategy, which builds K*(K-1)/2classifiers considering all pairs of classes.Moreover, we only apply the simple linear kernel,although other kernels (e.g.
polynomial kernel) andpairwise strategy can have better performance.961.3 FeaturesVarious widely used lexical-level features areexplored in the baseline system.?
Word Formation Pattern (FWFP): The purposeof this feature is to capture capitalization,digitalization and other word formationinformation.
In this paper, the same feature as inShen et al2003 is used.?
Morphological Pattern (FMP): Morphologicalinformation, such as prefix and suffix, isconsidered as an important cue for terminologyidentification.
Same as Shen et al2003, we use astatistical method to get the most usefulprefixes/suffixes from the training data.?
Part-of-Speech (FPOS): Since many of thewords in biomedical entity names are in lowercase,capitalization information in the biomedicaldomain is not as evidential as that in the newswiredomain.
Moreover, many biomedical entity namesare descriptive and very long.
Therefore, POS mayprovide useful evidence about the boundaries ofbiomedical entity names.
In the baseline system, anout-domain POS using the PENN TreeBank isapplied.?
Head Noun Trigger (FHEAD): The head noun,which is the major noun of a noun phrase, oftendescribes the function or the property of the nounphrase.
In this paper, we automatically extractunigram and bigram head nouns from the trainingdata, and rank them by frequency.
For each entityclass, we select 50% of top ranked head nouns ashead noun triggers.2.
Deep Knowledge ResourcesBesides the widely used lexical-level features asdescribed above, we also explore the name aliasphenomenon, the cascaded entity namephenomenon, the use of both a closed dictionaryfrom the training corpus and an open dictionaryfrom the database term list SwissProt and the aliaslist LocusLink, the abbreviation resolution and in-domain POS using the GENIA corpus.2.1 Name Alias ResolutionA novel name alias feature is proposed to resolvethe name alias phenomenon.
The intuition behindthis feature is the name alias phenomenon thatrelevant entities will be referred to in many waysthroughout a given text and thus success of namedentity recognition is conditional on success atdetermining when one noun phrase refers to thevery same entity as another noun phrase.During decoding, the entity names alreadyrecognized from the previous sentences of thedocument are stored in a list.
When the systemencounters an entity name candidate (e.g.
a wordwith a special word formation pattern), a namealias algorithm (similar to Schwartz et al2003) isinvoked to first dynamically determine whether theentity name candidate might be alias for apreviously recognized name in the recognized list.The name alias feature FALIAS is represented asENTITYnLm (L indicates the locality of the namealias phenomenon).
Here ENTITY indicates theclass of the recognized entity name and n indicatesthe number of the words in the recognized entityname while m indicates the number of the words inthe recognized entity name from which the namealias candidate is formed.
For example, when thedecoding process encounters the word ?TCF?, theword ?TCF?
is proposed as an entity namecandidate and the name alias algorithm is invokedto check if the word ?TCF?
is an alias of arecognized named entity.
If ?T cell Factor?
is a?Protein?
name recognized earlier in thedocument, the word ?TCF?
is determined as analias of ?T cell Factor?
with the name alias featureProtein3L3 by taking the three initial letters of thethree-word ?protein?
name ?T cell Factor?.2.2 Cascaded Entity Name ResolutionIt is found (Shen et al2003) that 16.57% of entitynames in GENIA V3.0 have cascadedconstructions, e.g.<RNA><DNA>CIITA</DNA> mRNA</RNA>.Therefore, it is important to resolve suchphenomenon.Here, a pattern-based module is proposed toresolve the cascaded entity names while the aboveHMM is applied to recognize embedded entitynames and non-cascaded entity names.
In theGENIA corpus, we find that there are six usefulpatterns of cascaded entity name constructions:?
<ENTITY> := <ENTITY> + head noun, e.g.<PROTEIN> binding motif?<DNA>?
<ENTITY> := <ENTITY>  + <ENTITY>?
<ENTITY> := modifier + <ENTITY>, e.g.anti <Protein>?<Protein>?
<ENTITY> := <ENTITY>  + word +<ENTITY>?
<ENTITY> :=  modifier + <ENTITY> + headnoun?
<ENTITY> := <ENTITY> +  <ENTITY>  +head nounIn our experiments, all the rules of above sixpatterns are extracted from the cascaded entitynames in the GENIA V3.0 to deal with the97cascaded entity name phenomenon where the<ENTITY> above is restricted to the fivecategories in the shared task: Protein, DNA, RNA,CellLine, CellType.2.3 Abbreviation ResolutionWhile the name alias feature is useful to detect theinter-sentential name alias phenomenon, it isunable to identify the inner-sentential name aliasphenomenon: the inner-sentential abbreviation.Such abbreviations widely occur in the biomedicaldomain.In our system, we present an effective andefficient algorithm to recognize the inner-sententialabbreviations more accurately by mapping them totheir full expanded forms.
In the GENIA corpus,we observe that the expanded form and itsabbreviation often occur together via parentheses.Generally, there are two patterns: ?expanded form(abbreviation)?
and ?abbreviation (expandedform)?.Our algorithm is based on the fact that it ismuch harder to classify an abbreviation than itsexpanded form.
Generally, the expanded form ismore evidential than its abbreviation to determineits class.
The algorithm works as follows: Given asentence with parentheses, we use a similaralgorithm as in Schwartz et al(2003) to determinewhether it is an abbreviation with parentheses.
Ifyes, we remove the abbreviation and theparentheses from the sentence.
After the sentenceis processed, we restore the abbreviation withparentheses to its original position in the sentence.Then, the abbreviation is classified as the sameclass of the expanded form, if the expanded form isrecognized as an entity name.
In the meanwhile,we also adjust the boundaries of the expanded formaccording to the abbreviation, if necessary.
Finally,the expanded form and its abbreviation are storedin the recognized list of biomedical entity namesfrom the document to help the resolution offorthcoming occurrences of the same abbreviationin the document.2.4 DictionaryIn our system, two different features are exploredto capture the existence of an entity name in aclosed dictionary and an open dictionary.
Here, theclosed dictionary is constructed by extracting allentity names from the training data while the opendictionary (~700,000 entries) is combined from thedatabase term list Swissport and the alias listLocusLink.
The closed dictionary feature isrepresented as ClosedENTITYn (Here ENTITYindicates the class of the entity name and nindicates the number of the words in the entityname) while the open dictionary feature isrepresented as Openn (Here n indicates the numberof the words in the entity name.
We don?tdifferentiate the class of the entity name since theopen dictionary only contains protein/gene namesand their aliases).2.5 In-domain POSWe also examine the impact of an in-domain POSfeature instead of an out-domain POS featurewhich is trained on PENN TreeBank.
Here, the in-domain POS is trained on the GENIA corpusV3.02p.3.
EvaluationTable 1 shows the performance of the baselinesystem and the impact of deep knowledgeresources while Table 2-4 show the detailedperformance using the provided scoring algorithm.Table 1 shows that:?
The baseline system achieves F-measure of60.3 while incorporation of deep knowledgeresources can improve the performance by 12.2 to72.5 in F-measure.?
The replacement of the out-domain POS within-domain POS improves the performance by 3.8in F-measure.
This suggests in-domain POS canmuch improve the performance.?
The name alias feature in name alias resolutionslightly improves the performance by 0.9 in F-measure.?
The cascaded entity name resolution improvesthe performance by 3.1 in F-measure.
Thissuggests that the cascaded entity name resolution isvery useful due to the fact that about 16% of entitynames have cascaded constructions.?
The abbreviation resolution improves theperformance by 2.1 in F-measure.?
The small closed dictionary improves theperformance by 1.5 in F-measure.
In themeanwhile, the large open dictionary improves theperformance by 1.2 in F-measure largely due to theperformance improvement for the protein class.
Itis interesting that the small closed dictionarycontributes more than the large open dictionarydoes.
This may be due to the high ambiguity in theopen dictionary and that the open dictionary onlycontains protein and gene names.Table 1: Impact of Deep Knowledge ResourcesPerformance FBaseline 60.398+In-domain POS +3.8+Name Alias Feature +0.9+Cascaded Entity Name Res.
+3.1+Abbreviation Resolution +2.1+Small Closed Dictionary +1.5+Large Open Dictionary +1.2+All Deep Knowledge Resources +12.2Table 2: Final Detailed Performance: full correctanswer(# of correctanswers)P R FProtein (4015) 69.01 79.24 73.77DNA (772) 66.84 73.11 69.83RNA (75) 64.66 63.56 64.10Cell Line (329) 53.85 65.80 59.23Cell Type (1391) 78.06 72.41 75.13Overall (6582) 69.42 75.99 72.55Table 3: Final Detailed Performance: correct leftboundary with correct class information(# of correctanswers)P R FProtein (4239) 72.86 83.66 77.89DNA (798) 69.09 75.57 72.18RNA (76) 65.52 64.41 64.96Cell Line (346) 56.63 69.20 62.29Cell Type (1418) 79.57 73.82 76.59Overall (6877) 72.53 79.39 75.80Table 4: Final Detailed Performance: correct rightboundary with correct class information(# of correctanswers)P R FProtein (4285) 73.65 84.57 78.73DNA (854) 73.94 80.87 77.25RNA (83) 71.55 70.34 70.94Cell Line (383) 62.68 76.60 68.95Cell Type (1532) 85.97 79.75 82.74Overall (7137) 75.27 82.39 78.674.
ConclusionIn the paper, we have explored various deepknowledge resources such as the name aliasphenomenon, the cascaded entity namephenomenon, the use of both a closed dictionaryfrom the training corpus and an open dictionaryfrom the database term list SwissProt and the aliaslist LocusLink, the abbreviation resolution and in-domain POS using the GENIA corpus.In the near future, we will further improve theperformance by investigating more on conjunctionand disjunction construction and the combinationof coreference resolution.AcknowledgementWe thank ZHANG Zhuo for providing thedatabase entity name list SwissProt and the aliaslist LocusLink.ReferencesChen and Goodman.
1996.
An Empirical Study ofSmoothing Technniques for LanguageModeling.
In Proceedings of the 34th AnnualMeeting of the Association of ComputationalLinguistics (ACL?1996).
pp310-318.
Santa Cruz,California, USA.Ohta T., Tateisi Y., Kim J., Mima H., and Tsujii J.2002.
The GENIA corpus: An annotatedresearch abstract corpus in molecular biologydomain.
In Proc.
of HLT 2002.Platt J.
1999.
Probabilistic Outputs for SupportVector Machines and comparisions toregularized Likelihood Methods.
MIT Press.Schwartz A.S. and Hearst M.A.
2003.
A SimpleAlgorithm for Identifying AbbreviationDefinitions in Biomedical Text.
In Proc.
of thePacific Symposium on Biocomputing (PSB2003) Kauai.Shen Dan, Zhang Jie, Zhou GuoDong, Su Jian andTan Chew Lim, Effective Adaptation of aHidden Markov Model-based Named EntityRecognizer for Biomedical Domain,Proceedings of ACL?2003 Workshop on NaturalLanguage Processing in Biomedicine, Sapporo,Japan, 11 July 2003. pp49-56.Vapnik V. 1995.
The Nature of StatisticalLearning Theory.
NY, USA: Springer-Verlag.Viterbi A.J.
1967.
Error bounds for convolutionalcodes and an asymptotically optimum decodingalgorithm.
IEEE Transactions on InformationTheory, 260-269.Zhou G.D. and Su J.
2002.
Named EntityRecognition using an HMM-based ChunkTagger.
In Proc.
of the 40th Annual Meeting ofthe Association for Computational Linguistics(ACL), 473-480.99
