The Complexity of Recognitionof Linguistically Adequate Dependency GrammarsPeter NeuhausNorbert Br i i kerComputat iona l  L inguist ics Research GroupFreiburg University,  Fr iedr ichstrage 50D-79098 Freiburg, Germanyemail: { neuhaus,nobi } @coling.uni-freiburg.deAbstractResults of computational complexity exist fora wide range of phrase structure-based gram-mar formalisms, while there is an apparentlack of such results for dependency-based for-malisms.
We here adapt a result on the com-plexity of ID/LP-grammars tothe dependencyframework.
Contrary to previous tudies onheavily restricted ependency grammars, weprove that recognition (and thus, parsing) oflinguistically adequate dependency grammarsis~A/T'-complete.1 IntroductionThe introduction of dependency grammar (DG) intomodern linguistics is marked by Tesni~re (1959).
Hisconception addressed idactic goals and, thus, did notaim at formal precision, but rather at an intuitive un-derstanding of semantically motivated ependency re-lations.
An early formalization was given by Gaifman(1965), who showed the generative capacity of DG to be(weakly) equivalent to standard context-free grammars.Given this equivalence, interest in DG as a linguisticframework diminished considerably, although many de-pendency grammarians view Gaifman's conception as anunfortunate one (cf.
Section 2).
To our knowledge, therehas been no other formal study of DG.This is reflectedby a recent study (Lombardo & Lesmo, 1996), whichapplies the Earley parsing technique (Earley, 1970) toDG, and thereby achieves cubic time complexity for theanalysis of DG.
In their discussion, Lombardo & Lesmoexpress their hope that slight increases in generative ca-pacity will correspond toequally slight increases incom-putational complexity.
It is this claim that we challengehere.After motivating non-projective analyses for DG, weinvestigate various variants of DG and identify the sep-aration of dominance and precedence as a major part ofcurrent DG theorizing.
Thus, no current variant of DG(not even Tesni~re's original formulation) is compatiblewith Gaifman' s conception, which seems to be motivatedby formal considerations only (viz., the proof of equiva-lence).
Section 3 advances our proposal, which cleanlyseparates dominance and precedence r lations.
This is il-lustrated in the fourth section, where we give a simple en-coding of an A/P-complete problem in a discontinuousDG.
Our proof of A/79-completeness, however, does notrely on discontinuity, but only requires unordered trees.It is adapted from a similar proof for unordered context-free grammars (UCFGs) by Barton (1985).2 Versions of Dependency GrammarThe growing interest in the dependency concept (whichroughly corresponds to the O-roles of GB, subcatego-rization in HPSG, and the so-called omain of localityof TAG) again raises the issue whether non-lexical cat-egories are necessary for linguistic analysis.
After re-viewing several proposals in this section, we argue in thenext section that word order - -  the description of whichis the most prominent difference between PSGs and DGs- -  can adequately be described without reference to non-lexical categories.Standard PSG trees are projective, i.e., no branchescross when the terminal nodes are projected onto theinput string.
In contrast o PSG approaches, DG re-quires non-projective analyses.
As DGs are restrictedto lexical nodes, one cannot, e.g., describe the so-calledunbounded ependencies without giving up projectiv-ity.
First, the categorial approach employing partial con-stituents (Huck, 1988; Hepple, 1990) is not available,since there are no phrasal categories.
Second, the coin-dexing (Haegeman, 1994) or structure-sharing (Pollard& Sag, 1994) approaches are not available, since thereare no empty categories.Consider the extracted NP in "Beans, I know Johnlikes" (cf.
also to Fig.1 in Section 3).
A projective treewould require "Beans" to be connected to either "I" or"know" - none of which is conceptually directly relatedto "Beans".
It is "likes" that determines syntactic fea-337tures of "Beans" and which provides a semantic role forit.
The only connection between "know" and "Beans" isthat the finite verb allows the extraction of "Beans", thusdefining order restrictions for the NP.
This has led someDG variants to adopt a general graph structure with mul-tiple heads instead of trees.
We will refer to DGs allow-ing non-projective analyses as discontinuous DGs.Tesni~re (1959) devised a bipartite grammar theorywhich consists of a dependency omponent and a trans-lation component (' translation' used in a technical sensedenoting a change of category and grammatical func-tion).
The dependency omponent defines four main cat-egories and possible dependencies between them.
Whatis of interest here is that there is no mentioning of orderin TesniSre's work.
Some practitioneers of DG have al-lowed word order as a marker for translation, but they donot prohibit non-projective trees.Gaifman (1965) designed his DG entirely analogousto context-free phrase structure grammars.
Each wordis associated with a category, which functions like thenon-terminals in CFG.
He then defines the following ruleformat for dependency grammars:(1) X (Y , , .
.
.
, Y~, ,, Y~+I, .
.
.
,  Y,,)This rule states that a word of category X governs wordsof category Y1,... , Yn which occur in the given order.The head (the word of category X) must occur betweenthe i-th and the (i + 1)-th modifier.
The rule can beviewed as an ordered tree of depth one with node labels.Trees are combined through the identification of the rootof one tree with a leaf of identical category of anothertree.
This formalization is restricted to projective treeswith a completely specified order of sister nodes.
As wehave argued above, such a formulation cannot capture se-mantically motivated ependencies.2.1 Current Dependency GrammarsToday's DGs differ considerably from Gaifman's con-ception, and we will very briefly sketch various order de-scriptions, showing that DGs generally dissociate dom-inance and precedence by some mechanism.
All vari-ants share, however, the rejection of phrasal nodes (al-though phrasal features are sometimes allowed) and theintroduction of edge labels (to distinguish different de-pendency relations).Meaning-Text Theory (Mer 5uk, 1988) assumes sevenstrata of representation.
The rules mapping from the un-ordered ependency trees of surface-syntactic represen-tations onto the annotated lexeme sequences of deep-morphological representations include global orderingrules which allow discontinuities.
These rules have notyet been formally specified (Mel' 5uk & Pertsov, 1987,p.
187f), but see the proposal by Rambow & Joshi (1994).Word Grammar (Hudson, 1990) is based on generalgraphs.
The ordering of two linked words is specified to-gether with their dependency relation, as in the proposi-tion "object of verb succeeds it".
Extraction is analyzedby establishing another dependency, visitor, between theverb and the extractee, which is required to precede theverb, as in "visitor of verb precedes it".
Resulting incon-sistencies, e.g.
in case of an extracted object, are notresolved, however.Lexicase (Starosta, 1988; 1992) employs complex fea-ture structures to represent lexical and syntactic enti-ties.
Its word order description is much like that ofWord Grammar (at least at some level of abstraction),and shares the above inconsistency.Dependency Unification Grammar (Hellwig, 1988)defines a tree-like data structure for the representation fsyntactic analyses.
Using morphosyntactic features withspecial interpretations, a word defines abstract positionsinto which modifiers are mapped.
Partial orderings andeven discontinuities can thus be described by allowing amodifier to occupy a position defined by some transitivehead.
The approach cannot restrict discontinuities prop-erly, however.Slot Grammar (McCord, 1990) employs a number ofrule types, some of which are exclusively concerned withprecedence.
So-called head/slot and slot/slot orderingrules describe the precedence in projective trees, refer-ring to arbitrary predicates over head and modifiers.
Ex-tractions (i.e., discontinuities) are merely handled by amechanism built into the parser.This brief overview of current DG flavors shows thatvarious mechanisms (global rules, general graphs, proce-dural means) are generally employed to lift the limitationto projective trees.
Our own approach presented belowimproves on these proposals because it allows the lexi-calized and declarative formulation of precedence con-straints.
The necessity of non-projective analyses in DGresults from examples like "Beans, 1 know John likes"and the restriction to lexical nodes which prohibits gap-threading and other mechanisms tied to phrasal cate-gories.3 A Dependency Grammar with WordOrder DomainsWe now sketch a minimal DG that incorporates onlyword classes and word order as descriptional dimensions.The separation of dominance and precedence presentedhere grew out of our work on German, and retains the lo-cal flavor of dependency specification, while at the sametime covering arbitrary discontinuities.
It is based on a(modal) logic with model-theoretic interpretation, whichis presented in more detail in (Br~ker, 1997).338f  know/ /~ , , ,@x i ~ e s  ~ )Idl d2Figure 1: Word order domains in "Beans, I know Johnlikes"3.1 Order SpecificationOur initial observation is that DG cannot use binaryprecedence constraints as PSG does.
Since DG analysesare hierarchically flatter, binary precedence constraintsresult in inconsistencies, asthe analyses of Word Gram-mar and Lexicase illustrate.
In PSG, on the other hand,the phrasal hierarchy separates the scope of precedencerestrictions.
This effect is achieved in our approach bydefining word order domains as sets of words, whereprecedence restrictions apply only to words within thesame domain.
Each word defines a sequence of order do-mains, into which the word and its modifiers are placed.Several restrictions are placed on domains.
First,the domain sequence must mirror the precedence of thewords included, i.e., words in a prior domain must pre-cede all words in a subsequent domain.
Second, the orderdomains must be hierarchically ordered by set inclusion,i.e., be projective.
Third, a domain (e.g., dl in Fig.l)can be constrained to contain at most one partial depen-dency tree.
l We will write singleton domains as "_" ,while other domains are represented by " - " .
The prece-dence of words within domains is described by binaryprecedence r strictions, which must be locally satisfiedin the domain with which they are associated.
Consid-ering Fig.
1 again, a precedence r striction for "likes" toprecede its object has no effect, since the two are in dif-ferent domains.
The precedence constraints are formu-lated as a binary relation "~" over dependency labels,including the special symbol "self" denoting the head.Discontinuities can easily be characterized, since a wordmay be contained in any domain of (nearly) any of itstransitive heads.
If a domain of its direct head containsthe modifier, a continuous dependency results.
If, how-ever, a modifier is placed in a domain of some transitivehead (as "Beans" in Fig.
1), discontinuities occur.
Bound-ing effects on discontinuities are described by specifyingthat certain dependencies may not be crossed.
2 For thetFor details, cf.
(Br6ker, 1997).2German data exist that cannot be captured by the (morecommon) bounding of discontinuities by nodes of a certainpurpose of this paper, we need not formally introduce thebounding condition, though.A sample domain structure is given in Fig.l, with twodomains dl and d2 associated with the governing verb"know" (solid) and one with the embedded verb "likes"(dashed).
dl may contain only one partial dependencytree, the extracted phrase, d2 contains the rest of the sen-tence.
Both domains are described by (2), where the do-main sequence is represented as "<<".
d2 contains twoprecedence r strictions which require that "know" (rep-resented by self) must follow the subject (first precedenceconstraint) and precede the object (second precedenceconstraint).
(2) __ { } << ----.
{ (subject -.< self), (self --< object)}3.2 Formal DescriptionThe following notation is used in the proof.
A lexiconLez maps words from an alphabet E to word classes,which in turn are associated with valencies and domainsequences.
The set C of word classes is hierarchicallyordered by a subclass relation(3) i saccCxCA word w of class c inherits the valencies (and domainsequence) from c, which are accessed by(4) w.valenciesA valency (b, d, c) describes a possible dependency re-lation by specifying a flag b indicating whether the de-pendency may be discontinuous, the dependency name d(a symbol), and the word class c E C of the modifier.
Aword h may govern a word m in dependency d if h de-fines a valency (b, d, c) such that (m isao c) and m canconsistently be inserted into a domain of h (for b = - )or a domain of a transitive head of h (for b = +).
Thiscondition is written as(5) governs(h,d,m)A DG is thus characterized by(6) G = (Lex, C, isac, E)The language L(G) includes any sequence of wordsfor which a dependency tree can be constructed such thatfor each word h governing a word m in dependency d,governs(h, d, m) holds.
The modifier of h in dependencyd is accessed by(7) h.mod(d)category.3394 The complexity of DG RecognitionLombardo & Lesmo (1996, p.728) convey their hope thatincreasing the flexibility of their conception of DG will" .
.
.
imply the restructuring of some parts of the rec-ognizer, with a plausible increment of the complexity".We will show that adding a little (linguistically required)flexibility might well render ecognition A/P-complete.To prove this, we will encode the vertex cover problem,which is known to be A/P-complete, in a DG.4.1 Encoding the Vertex Cover Problem inDiscontinuous DGA vertex cover of a finite graph is a subset of its ver-tices such that (at least) one end point of every edge isa member of that set.
The vertex cover problem is todecide whether for a given graph there exists a vertexcover with at most k elements.
The problem is known tobe A/7~-complete (Garey & Johnson, 1983, pp.53-56).Fig.
2 gives a simple example where {c, d} is a vertexcover.a bXdFigure 2: Simple graph with vertex cover {c, d}.A straightforward encoding of a solution in the DGformalism introduced in Section 3 defines a root words of class S with k valencies for words of class O. Ohas IWl subclasses denoting the nodes of the graph.
Anedge is represented by two linked words (one for eachend point) with the governing word corresponding tothe node included in the vertex cover.
The subordinatedword is assigned the class R, while the governing wordis assigned the subclass of O denoting the node it repre-sents.
The latter word classes define a valency for wordsof class R (for the other end point) and a possibly discon-tinuous valency for another word of the identical class(representing the end point of another edge which is in-cluded in the vertex cover).
This encoding is summarizedin Table 1.The input string contains an initial s and for each edgethe words representing its end points, e.g.
"saccdadb-dcb" for our example.
If the grammar allows the con-struction of a complete dependency tree (cf.
Fig.
3 forone solution), this encodes a solution of the vertex coverproblem.$%I l l l l l l l l l  bI l t l l l l l l l  II I I I I I I I I I  I$ac  c da  dbdc  bFigure 3: Encoding a solution to the vertex cover prob-lem from Fig.
2.4.2 Formal Proof using Continuous DGThe encoding outlined above uses non-projective trees,i.e., crossing dependencies.
In anticipation of counterarguments such as that the presented ependency gram-mar was just too powerful, we will present he proof us-ing only one feature supplied by most DG formalisms,namely the free order of modifiers with respect o theirhead.
Thus, modifiers must be inserted into an order do-main of their head (i.e., no + mark in valencies).
Thisversion of the proof uses a slightly more complicated en-coding of the vertex cover problem and resembles theproof by Barton (1985).Definition 1 (Measure)Let II ?
II be a measure for the encoded input length of acomputational problem.
We require that if S is a set orstring and k E N then ISl > k implies IlSll ___ Ilkll andthat for any tuple I1("" , z , .
.
")11 - Ilzll holds.
<Definition 2 (Vertex Cover Problem)A possible instance of the vertex cover problem is a triple(V, E, k) where (V, E) is a finite graph and IvI > kN.
The vertex cover problem is the set VC of all in-stances (V, E, k) for which there exists a subset V' C_ Vand a function f : E ---> V I such that IV'l <_ k andV(Vm,Vn) E E:  f((vm,Vn)) E {Vm,Vn}.
<1Definition 3 (DG recognition problem)A possible instance of the DG recognition problem is atuple (G, a) where G = (Lex, C, i sac,  ~) is a depen-dency grammar as defined in Section 3 and a E E +.
TheDG recognition problem DGR consists of all instances(G, a) such that a E L(G).
<1For an algorithm to decide the VC problem consider adata structure representing the vertices of the graph (e.g.,a set).
We separate the elements of this data structure340classes valencies order domainS {(- ,  markl,O), (- ,  mark2,0)} --{(self-~ mark1), (mark1 -.< mark2)}A isac 0 {(- ,  unmrk, R), (+, same, A)} ={(unmrk -K same), (self -4 same)}B isac O {(- ,  unmrk, R), (+, same, B)} ={(unmrk --< same), (self -.< same)}(7 isac O {(- ,  unmrk, R), (+, same, C)} ~{(unmrk --4 same), (self -4 same)}D isac O {(- ,  unmrk, R), (+, same, D)} -{(unmrk --.< same), (self -~ same)}R {} --{}\[ word \[ classes Is {s}a {A,R}b {B,R}c {C,R}d {D,R}Table 1: Word classes and lexicon for vertex cover problem from Fig.
2into the (maximal) vertex cover set and its complementset.
Hence, one end point of every edge is assigned tothe vertex cover (i.e., it is marked).
Since (at most) allIEI edges might share a common vertex, the data struc-ture has to be a multiset which contains IEI copies ofeach vertex.
Thus, marking the IVI - k complement ver-tices actually requires marking IVI - k times IE\[ iden-tical vertices.
This will leave (k - 1) * IEI unmarkedvertices in the input structure.
To achieve this algorithmthrough recognition of a dependency grammar, the mark-ing process will be encoded as the filling of appropriatevalencies of a word s by words representing the vertices.Before we prove that this encoding can be generated inpolynomial time we show that:Lemma 1The DG recognition problem is in the complexity classAlp.
\[\]Let G = (Lex, C, isac,  Z) and a E \]E +.
We givea nondeterministic algorithm for deciding whether a =(Sl- .
-  sn) is in L(G).
Let H be an empty set initially:1.
Repeat until IHI = Iol(a) i.
For every Si E O r choose a lexicon entryci E Lex(si).ii.
From the ci choose one word as the headh0.iii.
Let H := {ho} and M := {cili E\[1, IOrl\]} \ H.(b) Repeat until M = 0:i.
Choose a head h E H and a valency(b, d, c) E h.valencies and a modifier m EM.ii.
If governs(h, d, m) holds then establish thedependency relation between h and the m,and add m to the set H.iii.
Remove m from M.The algorithm obviously is (nondeterministically)polynomial in the length of the input.
Given that(G, g) E DGR, a dependency tree covering the wholeinput exists and the algorithm will be able to guess thedependents of every head correctly.
If, conversely, thealgorithm halts for some input (G, or), then there neces-sarily must be a dependency tree rooted in ho completelycovering a.
Thus, (G, a) E DGR.
\[\]Lemma 2Let (V, E, k) be a possible instance of the vertex coverproblem.
Then a grammar G(V, E, k) and an inputa(V, E, k) can be constructed in time polynomial inII (v, E, k)II such that(V, E, k) E VC ?
:::::v (G(V, E, k), a(V, E, k)) E DGR\[\]For the proof, we first define the encoding and showthat it can be constructed in polynomial time.
Then weproceed showing that the equivalence claim holds.
Theset of classes is G =aef {S, R, U} U {Hdi e \[1, IEI\]} U{U~, ?1i e \[1, IVI\]}.
In the isac hierarchy the classes Uishare the superclass U, the classes V~ the superclass R.Valencies are defined for the classes according to Table 2.Furthermore, we define E =dee {S} U {vii/ E \[1, IVl\]}.The lexicon Lex associates words with classes as givenin Table 2.We setG(V, E, k) =clef ( Lex, C, i sac,  ~)anda(V, E, k) =def s Vl ' ' "  V l " ' "  y IV \ [  " " " VlV ~IEI IEIFor an example, cf.
Fig.
4 which shows a dependencytree for the instance of the vertex cover problem fromFig.
2.
The two dependencies Ul and u2 represent thecomplement of the vertex cover.It is easily seen 3 that \[\[(G(V,E,k),a(V,E,k))\[\[ ispolynomial in \[\[V\[\[, \[\[E\[\[ and k. From \[El _> k and Def-inition 1 it follows that H(V,E,k)\[I >_ \[IE\]\[ _> \]\[k\[\[ _> k.3The construction requires 2 ?
\[V\[ + \[El + 3 word classes,IV\[ + 1 terminals in at most \[El + 2 readings each.
S definesIV\[ + k ?
IE\[ - k valencies, Ui defines \[E\[ - 1 valencies.
Thelength of a is IV\[ ?
\[E\[ + 1.341word class valenciesVvi ?
V Vi isac R { }Vvi ?
V Ui isac U {( - ,  rz, V/),--.
, ( - ,  rlEl_l, V/)}Vei E E Hi {}S {(-, u,, u ) , .
.
.
,  ( - ,  u,v,_,, v) ,( - ,  hi, H i ) , - ' - ,  ( - ,  hie I, HIEI),( - ,  n,  R), ?
?
?
, ( - ,  r(k-,)l~l, R)}I order I={ } word \]={ } "i-{}-{}word classes{U.~}U{Hjl3vm,v.
?
v :ej = (vm, v,,)^s {s}Table 2: Word classes and lexicon to encode vertex cover problem$aaaa  bbbbFigure 4: Encoding a solution to the vertex cover prob-lem from Fig.
2.Hence, the construction of (G(V, E, k), a(V, E, k)) canbe done in worst-case time polynomial in II(V,E,k)ll.We next show the equivalence of the two problems.Assume (V, E, k) ?
VC: Then there exists a subsetV' C_ V and a function f : E --+ V' such that IV'l <_ kand V(vm,v,~) ?
E : f((vm,vn)) ?
{(vm,Vn)}.
Adependency tree for a(V, E, k) is constructed by:1.
For every ei ?
E, one word f(ei) is assigned classHi and governed by s in valency hi.2.
For each vi ?
V \ V', IEI - I words vi are assignedclass R and governed by the remaining copy of viin reading Ui through valencies rl to rlEl_l.3.
The vi in reading Ui are governed by s through thevalencies uj (j ?
\[1, IWl - k\]).4.
(k - 1) ?
IEI words remain in a.
These receivereading R and are governed by s in valencies r~ (j ?\[1, (k - 1)IEI\]).The dependency tree rooted in s covers the whole in-put a(V, E, k).
Since G(V, E, k) does not give any fur-ther restrictions this implies a( V, E, k) ?
L ( G ( V, E, k ) )and, thus, (G(V, E, k), a(V, E, k)) ?
DGR.Conversely assume (G(V, E, k), a(V, E, k)) ?
DGR:Then a(V, E, k) ?
L(G(V, E, k)) holds, i.e., there ex-ists a dependency tree that covers the whole input.
Sinces cannot be governed in any valency, it follows that smust be the root.
The instance s of S has IEI valenciesof class H, (k -  1) * \[E I valencies of class R, and IWl - kvalencies of class U, whose instances in turn have IE I -  1valencies of class R. This sums up to IEI * IVl potentialdependents, which is the number of terminals in a be-sides s. Thus, all valencies are actually filled.
We definea subset Vo C_ V by Vo =def {V E VI3i e \[1, IYl - k\]8.mod(ul) = v}.
I.e.,(1) IVol = IV I -  kThe dependents ofs in valencies hl are from the set V'Vo.
We define a function f : E --+ V \ Vo by f(ei) =defs.mod(hi) for all ei E E. By construction f(ei) is anend point of edge ei, i.e.
(2) V(v,,,,v,d e E:  f((v,.,,,v,4,) e {v,,,,v,.,}We define a subset V' C V by V' =def {f(e)le ?
E}.Thus(3) Ve ?
E :  f(e) ?
V'By construction of V' and by (1) it follows(4) IV'l < IY l -  IVol = kFrom (2), (3), and (4) we induce (V, E, k) ?
VC.
?Theorem 3The DG recognition problem is in the complexity classAf l)C. \[\]The Af:P-completeness of the DG recognition problemfollows directly from lemmata 1and 2.
?5 Conc lus ionWe have shown that current DG theorizing exhibits afeature not contained in previous formal studies of DG,namely the independent specification of dominance andprecedence constraints.
This feature leads to a A/'7%complete recognition problem.
The necessity of this ex-tension approved by most current DGs relates to the factthat DG must directly characterize dependencies whichin PSG are captured by a projective structure and addi-tional processes such as coindexing or structure sharing(most easily seen in treatments of so-called unbounded342dependencies).
The dissociation of tree structure andlinear order, as we have done in Section 3, neverthelessseems to be a promising approach for PSG as well; see avery similar proposal for HPSG (Reape, 1989).The .N'79-completeness result also holds for the dis-continuous DG presented in Section 3.
This DG cancharacterize at least some context-sensitive languagessuch as anbnc n, i.e., the increase in complexity corre-sponds to an increase of generative capacity.
We conjec-ture that, provided a proper formalization ofthe other DGversions presented in Section 2, their .A/P-completenesscan be similarly shown.
With respect to parser design,this result implies that the well known polynomial timecomplexity of chart- or tabular-based parsing techniquescannot be achieved for these DG formalisms in gen-eral.
This is the reason why the PARSETALK text under-standing system (Neuhaus & Hahn, 1996) utilizes pecialheuristics in a heterogeneous chart- and backtracking-based parsing approach.ReferencesBarton, Jr., G. E. (1985).
On the complexity of ID/LPparsing.
Computational Linguistics, 11(4):205-218.Br6ker, N. (1997).
Eine Dependenzgrammatikzur Kopplung heterogener Wissenssysteme aufmodaUogischer Basis, (Dissertation).
Freiburg,DE: Philosophische Fakult~it, Albert-Ludwigs-Universit~it.Earley, J.
(1970).
An efficient context-free parsing algo-rithm.
Communications of the ACM, 13(2):94-102.Gaifman, H. (1965).
Dependency s stems and phrase-structure systems.
Information & Control, 8:304--337.Garey, M. R. & D. S. Johnson (1983).
Computersand Intractability: A Guide to the Theory of NP-completeness (2.
ed.).
New York, NY: Freeman.Haegeman, L. (1994).
Introduction to Government andBinding.
Oxford, UK: Basil Blackwell.Hellwig, E (1988).
Chart parsing according to the slotand filler principle.
In Proc.
of the 12 th Int.
Conf.on Computational Linguistics.
Budapest, HU, 22-27Aug 1988, Vol.
1, pp.
242-244.Hepple, M. (1990).
Word order and obliqueness in cat-egorial grammar.
In G. Barry & G. Morill (Eds.
),Studies in categorial grammar, pp.
47--64.
Edin-burgh, UK: Edinburgh University Press.Huck, G. (1988).
Phrasal verbs and the categories ofpostponement.
In R. Oehrle, E. Bach & D.
Wheeler(Eds.
), Categorial Grammars and Natural Lan-guage Structures, pp.
249-263.
Studies in Linguis-tics and Philosophy 32.
Dordrecht, NL: D. Reidel.Hudson, R. (1990).
English Word Grammar.
Oxford,UK: Basil Blackwell.Lombardo, V. & L. Lesmo (1996).
An earley-type r cog-nizer for dependency grammar.
In Proc.
of the 16 thInt.
Conf.
on Computational Linguistics.
Copen-hagen, DK, 5-9 Aug 1996, Vol.
2, pp.
723-728.McCord, M. (1990).
Slot grammar: A system for simplerconstruction of practical natural language gram-mars.
In R. Studer (Ed.
), Natural Language andLogic, pp.
118-145.
Berlin, Heidelberg: Springer.Mer ~uk, I.
(1988).
Dependency S ntax: Theory andPractice.
New York, NY: SUNY State UniversityPress of New York.Mel' 6uk, I.
& N. Pertsov (1987).
Surface Syntax of En-glish: A Formal Model within the MTT Framework.Amsterdam, NL: John Benjamins.Neuhaus, R & U. Hahn (1996).
Restricted parallelism inobject-oriented l xical parsing.
In Proc.
of the 16 thInt.
Conf.
on Computational Linguistics.
Copen-hagen, DK, 5-9 Aug 1996, pp.
502-507.Pollard, C. & I.
Sag (1994).
Head-Driven Phrase Struc-ture Grammar.
Chicago, IL: University of ChicagoPress.Rambow, O.
& A. Joshi (1994).
A formal ook at DGsand PSGs, with consideration of word-order phe-nomena.
In L. Wanner (Ed.
), Current Issues inMeaning-Text-Theory.
London: Pinter.Reape, M. (I 989).
A logical treatment ofsemi-free wordorder and discontinuous constituents.
In Proc.
of the27 th Annual Meeting of the Association for Compu-tational Linguistics.
Vancouver, BC, 1989, pp.
103-110.Starosta, S. (1988).
The Case for Lexicase.
London:Pinter.Starosta, S. (1992).
Lexicase revisited.
Department ofLinguistics, University of Hawaii.Tesni~re, L. ((1969) 1959).
Elements de Syntaxe Struc-turale (2.
ed.).
Paris, FR: Klincksieck.343
