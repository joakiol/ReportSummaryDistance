Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1284?1293,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLooking at Unbalanced Specialized Comparable Corporafor Bilingual Lexicon ExtractionEmmanuel Morin and Amir HazemUniversit?e de Nantes, LINA UMR CNRS 62412 rue de la houssini`ere, BP 92208, 44322 Nantes Cedex 03, France{emmanuel.morin,amir.hazem}@univ-nantes.frAbstractThe main work in bilingual lexicon ex-traction from comparable corpora is basedon the implicit hypothesis that corpora arebalanced.
However, the historical context-based projection method dedicated to thistask is relatively insensitive to the sizesof each part of the comparable corpus.Within this context, we have carried outa study on the influence of unbalancedspecialized comparable corpora on thequality of bilingual terminology extractionthrough different experiments.
Moreover,we have introduced a regression modelthat boosts the observations of word co-occurrences used in the context-based pro-jection method.
Our results show that theuse of unbalanced specialized comparablecorpora induces a significant gain in thequality of extracted lexicons.1 IntroductionThe bilingual lexicon extraction task from bilin-gual corpora was initially addressed by using par-allel corpora (i.e.
a corpus that contains sourcetexts and their translation).
However, despitegood results in the compilation of bilingual lex-icons, parallel corpora are scarce resources, es-pecially for technical domains and for languagepairs not involving English.
For these reasons,research in bilingual lexicon extraction has fo-cused on another kind of bilingual corpora com-prised of texts sharing common features such asdomain, genre, sampling period, etc.
without hav-ing a source text/target text relationship (McEneryand Xiao, 2007).
These corpora, well known nowas comparable corpora, have also initially beenintroduced as non-parallel corpora (Fung, 1995;Rapp, 1995), and non-aligned corpora (Tanakaand Iwasaki, 1996).
According to Fung and Che-ung (2004), who range bilingual corpora from par-allel corpora to quasi-comparable corpora goingthrough comparable corpora, there is a continuumfrom parallel to comparable corpora (i.e.
a kind offiliation).The bilingual lexicon extraction task from com-parable corpora inherits this filiation.
For instance,the historical context-based projection method(Fung, 1995; Rapp, 1995), known as the standardapproach, dedicated to this task seems implicitlyto lead to work with balanced comparable corporain the same way as for parallel corpora (i.e.
eachpart of the corpus is composed of the same amountof data).In this paper we want to show that the assump-tion that comparable corpora should be balancedfor bilingual lexicon extraction task is unfounded.Moreover, this assumption is prejudicial for spe-cialized comparable corpora, especially when in-volving the English language for which many doc-uments are available due the prevailing positionof this language as a standard for internationalscientific publications.
Within this context, ourmain contribution consists in a re-reading of thestandard approach putting emphasis on the un-founded assumption of the balance of the spe-cialized comparable corpora.
In specialized do-mains, the comparable corpora are traditionally ofsmall size (around 1 million words) in comparisonwith comparable corpus-based general language(up to 100 million words).
Consequently, the ob-servations of word co-occurrences which is the ba-sis of the standard approach are unreliable.
Tomake them more reliable, our second contributionis to contrast different regression models in orderto boost the observations of word co-occurrences.This strategy allows to improve the quality of ex-tracted bilingual lexicons from comparable cor-pora.12842 Bilingual Lexicon ExtractionIn this section, we first describe the standard ap-proach that deals with the task of bilingual lexi-con extraction from comparable corpora.
We thenpresent an extension of this approach based on re-gression models.
Finally, we discuss works relatedto this study.2.1 Standard ApproachThe main work in bilingual lexicon extractionfrom comparable corpora is based on lexical con-text analysis and relies on the simple observationthat a word and its translation tend to appear inthe same lexical contexts.
The basis of this obser-vation consists in the identification of ?first-orderaffinities?
for each source and target language:?First-order affinities describe what other wordsare likely to be found in the immediate vicinityof a given word?
(Grefenstette, 1994, p. 279).These affinities can be represented by context vec-tors, and each vector element represents a wordwhich occurs within the window of the word tobe translated (e.g.
a seven-word window approxi-mates syntactic dependencies).
In order to empha-size significant words in the context vector and toreduce word-frequency effects, the context vectorsare normalized according to an association mea-sure.
Then, the translation is obtained by compar-ing the source context vector to each translationcandidate vector after having translated each ele-ment of the source vector with a general dictio-nary.The implementation of the standard approachcan be carried out by applying the followingthree steps (Rapp, 1999; Chiao and Zweigenbaum,2002; D?ejean et al, 2002; Morin et al, 2007;Laroche and Langlais, 2010, among others):Computing context vectors We collect all thewords in the context of each word i and counttheir occurrence frequency in a window ofn words around i.
For each word i of thesource and the target languages, we obtaina context vector viwhich gathers the set ofco-occurrence words j associated with thenumber of times that j and i occur togethercooc(i, j).
In order to identify specific wordsin the lexical context and to reduce word-frequency effects, we normalize context vec-tors using an association score such as Mu-tual Information, Log-likelihood, or the dis-counted log-odds (LO) (Evert, 2005) (seeequation 1 and Table 1 where N = a + b +c + d).Transferring context vectors Using a bilingualdictionary, we translate the elements of thesource context vector.
If the bilingual dictio-nary provides several translations for an ele-ment, we consider all of them but weight thedifferent translations according to their fre-quency in the target language.Finding candidate translations For a word to betranslated, we compute the similarity be-tween the translated context vector and alltarget vectors through vector distance mea-sures such as Jaccard or Cosine (see equa-tion 2 where associjstands for ?associationscore?, vkis the transferred context vector ofthe word k to translate, and vlis the con-text vector of the word l in the target lan-guage).
Finally, the candidate translations ofa word are the target words ranked followingthe similarity score.j ?ji a = cooc(i, j) b = cooc(i,?j)?i c = cooc(?i, j) d = cooc(?i,?j)Table 1: Contingency tableLO(i, j) = log(a +12) ?
(d +12)(b +12) ?
(c +12)(1)Cosinevkvl=?tassocltassockt??tassoclt2?
?tassockt2(2)This approach is sensitive to the choice of pa-rameters such as the size of the context, the choiceof the association and similarity measures.
Themost complete study about the influence of theseparameters on the quality of word alignment hasbeen carried out by Laroche and Langlais (2010).The standard approach is used by most re-searchers so far (Rapp, 1995; Fung, 1998; Pe-ters and Picchi, 1998; Rapp, 1999; Chiao andZweigenbaum, 2002; D?ejean et al, 2002; Gaussieret al, 2004; Morin et al, 2007; Laroche andLanglais, 2010; Prochasson and Fung, 2011;1285References Domain Languages Source/Target SizesTanaka and Iwasaki (1996) Newspaper EN/JP 30/33 million wordsFung and McKeown (1997) Newspaper EN/JP 49/60 million bytes of dataRapp (1999) Newspaper GE/EN 135/163 million wordsChiao and Zweigenbaum (2002) Medical FR/EN 602,484/608,320 wordsD?ejean et al (2002) Medical GE/EN 100,000/100,000 wordsMorin et al (2007) Medical FR/JP 693,666/807,287 wordsOtero (2007) European Parliament SP/EN 14/17 million wordsIsmail and Manandhar (2010) European Parliament EN/SP 500,000/500,000 sentencesBouamor et al (2013) Financial FR/EN 402,486/756,840 words- Medical FR/EN 396,524/524,805 wordsTable 2: Characteristics of the comparable corpora used for bilingual lexicon extractionBouamor et al, 2013, among others) with the im-plicit hypothesis that comparable corpora are bal-anced.
As McEnery and Xiao (2007, p. 21) ob-serve, a specialized comparable corpus is builtas balanced by analogy with a parallel corpus:?Therefore, in relation to parallel corpora, it ismore likely for comparable corpora to be designedas general balanced corpora.?.
For instance, Ta-ble 2 describes the comparable corpora used in themain work dedicated to bilingual lexicon extrac-tion for which the ratio between the size of thesource and the target texts is comprised between1 and 1.8.In fact, the assumption that words which havethe same meaning in different languages shouldhave the same lexical context distributions doesnot involve working with balanced comparablecorpora.
To our knowledge, no attention1hasbeen paid to the problem of using unbalancedcomparable corpora for bilingual lexicon extrac-tion.
Since the context vectors are computed fromeach part of the comparable corpus rather thanthrough the parts of the comparable corpora, thestandard approach is relatively insensitive to dif-ferences in corpus sizes.
The only precaution forusing the standard approach with unbalanced cor-pora is to normalize the association measure (forinstance, this can be done by dividing each entryof a given context vector by the sum of its associ-ation scores).2.2 Prediction ModelSince comparable corpora are usually small in spe-cialized domains (see Table 2), the discrimina-1We only found mention of this aspect in Diab and Finch(2000, p. 1501) ?In principle, we do not have to have thesame size corpora in order for the approach to work?.tive power of context vectors (i.e.
the observa-tions of word co-occurrences) is reduced.
Oneway to deal with this problem is to re-estimateco-occurrence counts by a prediction function(Hazem and Morin, 2013).
This consists in as-signing to each observed co-occurrence count ofa small comparable corpora, a new value learnedbeforehand from a large training corpus.In order to make co-occurrence counts morediscriminant and in the same way as Hazemand Morin (2013), one strategy consists in ad-dressing this problem through regression: giventraining corpora of small and large size (abun-dant in the general domain), we predict word co-occurrence counts in order to make them morereliable.
We then apply the resulting regressionfunction to each word co-occurrence count as apre-processing step of the standard approach.
Ourwork differs from Hazem and Morin (2013) in twoways.
First, while they experienced the linear re-gression model, we propose to contrast differentregression models.
Second, we apply regressionto unbalanced comparable corpora and study theimpact of prediction when applied to the sourcetexts, the target texts and both source and targettexts of the used comparable corpora.We use regression analysis to describe the rela-tionship between word co-occurrence counts in alarge corpus (the response variable) and word co-occurrence counts in a small corpus (the predictorvariable).
As most regression models have alreadybeen described in great detail (Christensen, 1997;Agresti, 2007), the derivation of most models isonly briefly introduced in this work.As we can not claim that the prediction of wordco-occurrence counts is a linear problem, we con-sider in addition to the simple linear regression1286model (Lin), a generalized linear model which isthe logistic regression model (Logit) and non lin-ear regression models such as polynomial regres-sion model (Polyn) of order n. Given an inputvector x ?
Rm, where x1,...,xmrepresent fea-tures, we find a prediction y?
?
Rmfor the co-occurrence count of a couple of words y ?
R us-ing one of the regression models presented below:y?Lin= ?0+ ?1x (3)y?Logit=11 + exp(?
(?0+ ?1x))(4)y?Polyn= ?0+ ?1x + ?2x2+ ... + ?nxn(5)where ?iare the parameters to estimate.Let us denote by f the regression function andby cooc(wi, wj) the co-occurrence count of thewords wiand wj.
The resulting predicted value ofcooc(wi, wj), noted ?cooc(wi, wj) is given by thefollowing equation:?cooc(wi, wj) = f(cooc(wi, wj)) (6)2.3 Related WorkIn the past few years, several contributions havebeen proposed to improve each step of the stan-dard approach.Prochasson et al (2009) enhance the represen-tativeness of the context vector by strengtheningthe context words that happen to be transliteratedwords and scientific compound words in the targetlanguage.
Ismail and Manandhar (2010) also sug-gest that context vectors should be based on themost important contextually relevant words (in-domain terms), and thus propose a method for fil-tering the noise of the context vectors.
In anotherway, Rubino and Linar`es (2011) improve the con-text words based on the hypothesis that a word andits candidate translations share thematic similari-ties.
Yu and Tsujii (2009) and Otero (2007) pro-pose, for their part, to replace the window-basedmethod by a syntax-based method in order to im-prove the representation of the lexical context.To improve the transfer context vectors step,and increase the number of elements of translatedcontext vectors, Chiao and Zweigenbaum (2003)and Morin and Prochasson (2011) combine a stan-dard general language dictionary with a special-ized dictionary, whereas D?ejean et al (2002) usethe hierarchical properties of a specialized the-saurus.
Koehn and Knight (2002) automaticallyinduce the initial seed bilingual dictionary by us-ing identical spelling features such as cognatesand similar contexts.
As regards the problem ofwords ambiguities, Bouamor et al (2013) carriedout word sense disambiguation process only inthe target language whereas Gaussier et al (2004)solve the problem through the source and targetlanguages by using approaches based on CCA(Canonical Correlation Analysis) and multilingualPLSA (Probabilistic Latent Semantic Analysis).The rank of candidate translations can be im-proved by integrating different heuristics.
For in-stance, Chiao and Zweigenbaum (2002) introducea heuristic based on word distribution symme-try.
From the ranked list of candidate translations,the standard approach is applied in the reversedirection to find the source counterparts of thefirst target candidate translations.
And then onlythe target candidate translations that had the ini-tial source word among the first reverse candidatetranslations are kept.
Laroche and Langlais (2010)suggest a heuristic based on the graphic similaritybetween source and target terms.
Here, candidatetranslations which are cognates of the word to betranslated are ranked first among the list of trans-lation candidates.3 Linguistic ResourcesIn this section, we outline the different textual re-sources used for our experiments: the comparablecorpora, the bilingual dictionary and the terminol-ogy reference lists.3.1 Specialized Comparable CorporaFor our experiments, we used two specializedFrench/English comparable corpora:Breast cancer corpus This comparable corpus iscomposed of documents collected from theElsevier website2.
The documents were takenfrom the medical domain within the sub-domain of ?breast cancer?.
We have auto-matically selected the documents publishedbetween 2001 and 2008 where the title or thekeywords contain the term cancer du sein inFrench and breast cancer in English.
We col-lected 130 French documents (about 530,000words) and 1,640 English documents (about2http://www.elsevier.com12877.4 million words).
We split the English doc-uments into 14 parts each containing about530,000 words.Diabetes corpus The documents making up theFrench part of the comparable corpus havebeen craweled from the web using threekeywords: diab`ete (diabetes), alimentation(food), and ob?esit?e (obesity).
After a man-ual selection, we only kept the documentswhich were relative to the medical domain.As a result, 65 French documents were ex-tracted (about 257,000 words).
The Englishpart has been extracted from the medicalwebsite PubMed3using the keywords: dia-betes, nutrition and feeding.
We only keptthe free fulltext available documents.
As a re-sult, 2,339 English documents were extracted(about 3,5 million words).
We also split theEnglish documents into 14 parts each con-taining about 250,000 words.The French and English documents were thennormalised through the following linguistic pre-processing steps: tokenisation, part-of-speech tag-ging, and lemmatisation.
These steps were car-ried out using the TTC TermSuite4that appliesthe same method to several languages includingFrench and English.
Finally, the function wordswere removed and the words occurring less thantwice in the French part and in each English partwere discarded.
Table 3 shows the number of dis-tinct words (# words) after these steps.
It alsoindicates the comparability degree in percentage(comp.)
between the French part and each Englishpart of each comparable corpus.
The comparabil-ity measure (Li and Gaussier, 2010) is based onthe expectation of finding the translation for eachword in the corpus and gives a good idea abouthow two corpora are comparable.
We can noticethat all the comparable corpora have a high degreeof comparability with a better comparability of thebreast cancer corpora as opposed to the diabetescorpora.
In the remainder of this article, [breastcancer corpus i] for instance stands for the breastcancer comparable corpus composed of the uniqueFrench part and the English part i (i ?
[1, 14]).3.2 Bilingual DictionaryThe bilingual dictionary used in our experimentsis the French/English dictionary ELRA-M00333http://www.ncbi.nlm.nih.gov/pubmed/4http://code.google.com/p/ttc-projectBreast cancer Diabetes# words (comp.)
# words (comp.
)FrenchPart 1 7,376 4,982EnglishPart 1 8,214 (79.2) 5,181 (75.2)Part 2 7,788 (78.8) 5,446 (75.9)Part 3 8,370 (78.8) 5,610 (76.6)Part 4 7,992 (79.3) 5,426 (74.8)Part 5 7,958 (78.7) 5,610 (75.0)Part 6 8,230 (79.1) 5,719 (73.6)Part 7 8,035 (78.3) 5,362 (75.6)Part 8 8,008 (78.8) 5,432 (74.6)Part 9 8,334 (79.6) 5,398 (74.2)Part 10 7,978 (79.1) 5,059 (75.6)Part 11 8,373 (79.4) 5,264 (74.9)Part 12 8,065 (78.9) 4,644 (73.4)Part 13 7,847 (80.0) 5,369 (74.8)Part 14 8,457 (78.9) 5,669 (74.8)Table 3: Number of distinct words (# words) anddegree of comparability (comp.)
for each compa-rable corporaavailable from the ELRA catalogue5.
This re-source is a general language dictionary which con-tains only a few terms related to the medical do-main.3.3 Terminology Reference ListsTo evaluate the quality of terminology extrac-tion, we built a bilingual terminology referencelist for each comparable corpus.
We selectedall French/English single words from the UMLS6meta-thesaurus.
We kept only i) the French sin-gle words which occur more than four times in theFrench part and ii) the English single words whichoccur more than four times in each English parti7.
As a result of filtering, 169 French/Englishsingle words were extracted for the breast can-cer corpus and 244 French/English single wordswere extracted for the diabetes corpus.
It shouldbe noted that the evaluation of terminology ex-traction using specialized comparable corpora of-5http://www.elra.info/6http://www.nlm.nih.gov/research/umls7The threshold sets to four is required to build a bilin-gual terminology reference list composed of about a hundredwords.
This value is very low to obtain representative contextvectors.
For instance, Prochasson and Fung (2011) showedthat the standard approach is not relevant for infrequent words(since the context vectors are very unrepresentative i.e.
poorin information).1288Breast cancer corpus1 2 3 4 5 6 7 8 9 10 11 12 13 14Balanced 26.1 26.2 21.0 27.0 22.8 27.1 26.3 25.8 29.2 23.3 21.7 29.6 29.1 26.1Unbalanced 26.1 31.9 34.7 36.0 37.7 36.4 36.6 37.2 39.8 40.5 40.6 42.3 40.9 41.6Diabetes corpus1 2 3 4 5 6 7 8 9 10 11 12 13 14Balanced 13.6 13.5 11.9 14.6 14.6 11.0 16.5 10.5 12.9 13.3 15.2 11.8 13.0 14.3Unbalanced 13.6 17.5 18.9 21.2 23.4 23.8 24.8 24.7 24.7 24.4 24.8 25.2 26.0 24.9Table 4: Results (MAP %) of the standard approach using the balanced and unbalanced comparablecorporaten relies on lists of a small size: 95 singlewords in Chiao and Zweigenbaum (2002), 100 inMorin et al (2007), 125 and 79 in Bouamor etal.
(2013).4 Experiments and ResultsIn this section, we present experiments to evaluatethe influence of comparable corpus size and pre-diction models on the quality of bilingual termi-nology extraction.We present the results obtained for the terms be-longing to the reference list for English to Frenchdirection measured in terms of the Mean AveragePrecision (MAP) (Manning et al, 2008) as fol-lows:MAP (Ref) =1|Ref ||Ref |?i=11ri(7)where |Ref | is the number of terms of the refer-ence list and rithe rank of the correct candidatetranslation i.4.1 Standard Approach EvaluationIn order to evaluate the influence of corpus size onthe bilingual terminology extraction task, two ex-periments have been carried out using the standardapproach.
We first performed an experiment usingeach comparable corpus independently of the oth-ers (we refer to these corpora as balanced corpora).We then conducted a second experiment where wevaried the size of the English part of the compara-ble corpus, from 530,000 to 7.4 million words forthe breast cancer corpus in 530,000 words steps,and from 250,000 to 3.5 million words for the di-abetes corpus in 250,000 words steps (we refer tothese corpora as unbalanced corpora).
In the ex-periments reported here, the size of the contextwindow w was set to 3 (i.e.
a seven-word windowthat approximates syntactic dependencies), the re-tained association and similarity measures werethe discounted log-odds and the Cosine (see Sec-tion 2.1).
The results shown were those that givethe best performance for the comparable corporaused individually.Table 4 shows the results of the standard ap-proach on the balanced and the unbalanced breastcancer and diabetes comparable corpora.
Eachcolumn corresponds to the English part i (i ?
[1, 14]) of a given comparable corpus.
The firstline presents the results for each individual com-parable corpus and the second line presents the re-sults for the cumulative comparable corpus.
Forinstance, the column 3 indicates theMAP obtainedby using a comparable corpus that is composed i)only of [breast cancer corpus 3] (MAP of 21.0%),and ii) of [breast cancer corpus 1, 2 and 3] (MAPof 34.7%).As a preliminary remark, we can notice that theresults differ noticeably according to the compa-rable corpus used individually (MAP variation be-tween 21.0% and 29.6% for the breast cancer cor-pora and between 10.5% and 16.5% for the dia-betes corpora).
We can also note that the MAPof all the unbalanced comparable corpora is al-ways higher than any individual comparable cor-pus.
Overall, starting with a MAP of 26.1% asprovided by the balanced [breast cancer corpus 1],we are able to increase it to 42.3% with the un-balanced [breast cancer corpus 12] (the variationobserved for some unbalanced corpora such as[diabetes corpus 12, 13 and 14] can be explainedby the fact that adding more data in the sourcelanguage increases the error rate of the translationphase of the standard approach, which leads to theintroduction of additional noise in the translatedcontext vectors).1289Balanced breast cancer corpus1 2 3 4 5 6 7 8 9 10 11 12 13 14No prediction 26.1 26.2 21.0 27.0 22.8 27.1 26.3 25.8 29.2 23.3 21.7 29.6 29.1 26.1Sourcepred26.5 26.0 23.0 30.0 25.4 30.1 28.3 29.4 32.1 24.9 24.4 30.5 30.1 29.0Targetpred19.5 20.0 17.2 23.4 19.9 23.1 21.4 21.6 24.1 19.3 18.1 26.6 24.3 22.6Sourcepred+ Targetpred23.9 21.9 20.5 25.8 23.5 25.3 24.1 26.1 27.4 22.5 21.0 25.6 28.5 24.6Balanced diabetes corpus1 2 3 4 5 6 7 8 9 10 11 12 13 14No prediction 13.6 13.5 11.9 14.6 14.6 11.0 16.5 10.5 12.9 13.3 15.2 11.8 13.0 14.3Sourcepred13.9 14.3 12.6 15.5 14.9 10.9 17.6 11.1 14.0 14.2 16.4 13.3 13.5 15.7Targetpred09.8 09.0 08.3 11.9 10.1 08.0 15.9 07.3 10.8 10.0 10.1 08.8 10.8 10.2Sourcepred+ Targetpred10.9 11.0 09.0 13.6 11.8 08.6 15.4 07.7 12.8 11.5 11.9 10.5 11.7 11.8Table 5: Results (MAP %) of the standard approach using the Lin regression model on the balancedbreast cancer and diabetes corpora (comparison of predicting the source side, the target side and bothsides of the comparable corpora)4.2 Prediction EvaluationThe aim of this experiment is two-fold: first, wewant to evaluate the usefulness of predicting wordco-occurrence counts and second, we want to findout whether it is more appropriate to apply predic-tion to the source side, the target side or both sidesof the bilingual comparable corpora.Breast cancer DiabetesNo prediction 29.6 16.5Lin 30.5 17.6Poly230.6 17.5Poly330.4 17.6Logit 22.3 13.6Table 6: Results (MAP %) of the standard ap-proach using different regression models on thebalanced breast cancer and diabetes corpora4.2.1 Regression Models ComparisonWe contrast the prediction models presented inSection 2.2 to findout which is the most appropri-ate model to use as a pre-processing step of thestandard approach.
We chose the balanced corporawhere the standard approach has shown the bestresults in the previous experiment, namely [breastcancer corpus 12] and [diabetes corpus 7].Table 6 shows a comparison between thestandard approach without prediction noted Noprediction and the standard approach with pre-diction models.
We contrast the simple linear re-gression model (Lin) with the second and the thirdorder polynomial regressions (Poly2and Poly3)and the logistic regression model (Logit).
Wecan notice that except for the Logit model, all theregression models outperform the baseline (Noprediction).
Also, as we can see, the resultsobtained with the linear and polynomial regres-sions are very close.
This suggests that both linearand polynomial regressions are suitable as a pre-processing step of the standard approach, whilethe logistic regression seems to be inappropriateaccording to the results shown in Table 6.That said, the gain of regression models is notsignificant.
This may be due to the regression pa-rameters that have been learned from a trainingcorpus of the general domain.
Another reason thatcould explain these results is the prediction pro-cess.
We applied the same regression functionto all co-occurrence counts while learning mod-els for low and high frequencies should have beenmore appropriate.
In the light of the above results,we believe that prediction can be beneficial to ourtask.4.2.2 Source versus Target PredictionTable 5 shows a comparison between the standardapproach without prediction noted No predictionand the standard approach based on the predic-tion of the source side noted Sourcepred, the tar-get side noted Targetpredand both sides notedSourcepred+Targetpred.
If prediction can not re-place a large amount of data, it aims at increasingco-occurrence counts as if large amounts of datawere at our disposal.
In this case, applying pre-diction to the source side may simulate a config-uration of using unbalanced comparable corporawhere the source side is n times bigger than thetarget side.
Predicting the target side only, may12901 2 3 4 5 6 7 8 9 10 11 12 13 1405101520253035404550[English-i]-French breast cancer corpusMAP(%)BalancedBalanced+PredictionUnbalancedUnbalanced+Prediction(a)1 2 3 4 5 6 7 8 9 10 11 12 13 1405101520253035[English-i]-French diabetes corpusMAP(%)BalancedBalanced+PredictionUnbalancedUnbalanced+Prediction(b)Figure 1: Results (MAP %) of the standard approach using the best configurations of the predictionmodels (Lin for Balanced + Prediction and Poly2for Unbalanced + Prediction) on the breastcancer and the diabetes corporaleads us to the opposite configuration where thetarget side is n times bigger than the source side.Finally, predicting both sides may simulate a largecomparable corpora on both sides.
In this experi-ment, we chose to use the linear regression model(Lin) for the prediction part.
That said, the otherregression models have shown the same behavioras Lin.We can see that the best results are obtained bythe Sourcepredapproach for both comparable cor-pora.
We can also notice that predicting the tar-get side and both sides of the comparable corporadegrades the results.
It is not surprising that pre-dicting the target side only leads to lower results,since it is well known that a better characterizationof a word to translate (given from the source side)leads to better results.
We can deduce from Ta-ble 5 that source prediction is the most appropriateconfiguration to improve the quality of extractedlexicons.
This configuration which simulates theuse of unbalanced corpora leads us to think thatusing prediction with unbalanced comparable cor-pora should also increase the performance of thestandard approach.
This assumption is evaluatedin the next Subsection.4.3 Predicting Unbalanced CorporaIn this last experiment we contrast the standardapproach applied to the balanced and unbalancedcorpora noted Balanced and Unbalanced withthe standard approach combined with the predic-tion model noted Balanced + Prediction andUnbalanced + Prediction.Figure 1(a) illustrates the results of the exper-iments conducted on the breast cancer corpus.We can see that the Unbalanced approach sig-nificantly outperforms the baseline (Balanced).The big difference between the Balanced andthe Unbalanced approaches would indicate thatthe latter is optimal.
We can also notice that theprediction model applied to the balanced corpus(Balanced + Prediction) slightly outperformsthe baseline while the Unbalanced+Predictionapproach significantly outperforms the three otherapproaches (moreover the variation observed withthe Unbalanced approach are lower than theUnbalanced + Prediction approach).
Overall,the prediction increases the performance of thestandard approach especially for unbalanced cor-pora.The results of the experiments conducted onthe diabetes corpus are shown in Figure 1(b).
Asfor the previous experiment, we can see that theUnbalanced approach significantly outperformsthe Balanced approach.
This confirms the unbal-anced hypothesis and would motivate the use ofunbalanced corpora when they are available.
Wecan also notice that the Balanced + Predictionapproach slightly outperforms the baseline whilethe Unbalanced+Prediction approach gives thebest results.
Here also, the prediction increases theperformance of the standard approach especiallyfor unbalanced corpora.
It is clear that in addi-tion to the benefit of using unbalanced comparable1291corpora, prediction shows a positive impact on theperformance of the standard approach.5 ConclusionIn this paper, we have studied how an unbalancedspecialized comparable corpus could influence thequality of the bilingual lexicon extraction.
This as-pect represents a significant interest when workingwith specialized comparable corpora for which thequantity of the data collected may differ depend-ing on the languages involved, especially when in-volving the English language as many scientificdocuments are available.
More precisely, our dif-ferent experiments show that using an unbalancedspecialized comparable corpus always improvesthe quality of word translations.
Thus, the MAPgoes up from 29.6% (best result on the balancedcorpora) to 42.3% (best result on the unbalancedcorpora) in the breast cancer domain, and from16.5% to 26.0% in the diabetes domain.
Addition-ally, these results can be improved by using a pre-diction model of the word co-occurrence counts.Here, the MAP goes up from 42.3% (best resulton the unbalanced corpora) to 46.9% (best resulton the unbalanced corpora with prediction) in thebreast cancer domain, and from 26.0% to 29.8%in the diabetes domain.
We hope that this studywill pave the way for using specialized unbalancedcomparable corpora for bilingual lexicon extrac-tion.AcknowledgmentsThis work is supported by the French National Re-search Agency under grant ANR-12-CORD-0020.ReferencesAlan Agresti.
2007.
An Introduction to CategoricalData Analysis (2nd ed.).
Wiley & Sons, Inc., Hobo-ken, New Jersey.Dhouha Bouamor, Nasredine Semmar, and PierreZweigenbaum.
2013.
Context vector disambigua-tion for bilingual lexicon extraction from compa-rable corpora.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (ACL?13), pages 759?764, Sofia, Bulgaria.Yun-Chuang Chiao and Pierre Zweigenbaum.
2002.Looking for candidate translational equivalents inspecialized, comparable corpora.
In Proceedings ofthe 19th International Conference on ComputationalLinguistics (COLING?02), pages 1208?1212, Tapei,Taiwan.Yun-Chuang Chiao and Pierre Zweigenbaum.
2003.The Effect of a General Lexicon in Corpus-BasedIdentification of French-English Medical WordTranslations.
In The New Navigators: from Profes-sionals to Patients, Actes Medical Informatics Eu-rope, pages 397?402.Ronald Christensen.
1997.
Log-Linear Models andLogistic Regression.
Springer-Verlag, Berlin.Herv?e D?ejean, Fatia Sadat, and?Eric Gaussier.
2002.An approach based on multilingual thesauri andmodel combination for bilingual lexicon extraction.In Proceedings of the 19th International Conferenceon Computational Linguistics (COLING?02), pages218?224, Tapei, Taiwan.Mona T. Diab and Steve Finch.
2000.
A StatisticalWord-Level Translation Model for Comparable Cor-pora.
In Proceedings of the 6th International Con-ference on Computer-Assisted Information Retrieval(RIAO?00), pages 1500?1501, Paris, France.Stefan Evert.
2005.
The Statistics of Word Cooccur-rences: Word Pairs and Collocations.
Ph.D. thesis,Universit?at Stuttgart, Germany.Pascale Fung and Percy Cheung.
2004.
Multi-level bootstrapping for extracting parallel sentencesfrom a quasi-comparable corpus.
In Proceedingsof the 20th International Conference on Computa-tional Linguistics (COLING?04), pages 1051?1057,Geneva, Switzerland.Pascale Fung and Kathleen McKeown.
1997.
FindingTerminology Translations from Non-parallel Cor-pora.
In Proceedings of the 5th Annual Workshopon Very Large Corpora (VLC?97), pages 192?202,Hong Kong.Pascale Fung.
1995.
Compiling Bilingual LexiconEntries from a non-Parallel English-Chinese Cor-pus.
In Proceedings of the 3rd Annual Workshopon Very Large Corpora (VLC?95), pages 173?183,Cambridge, MA, USA.Pascale Fung.
1998.
A Statistical View on Bilin-gual Lexicon Extraction: From Parallel Corpora toNon-parallel Corpora.
In David Farwell, LaurieGerber, and Eduard Hovy, editors, Proceedings ofthe 3rd Conference of the Association for MachineTranslation in the Americas (AMTA?98), pages 1?16, Langhorne, PA, USA.
?Eric Gaussier, Jean-Michel Renders, Irena Matveeva,Cyril Goutte, and Herv?e D?ejean.
2004.
AGeometric View on Bilingual Lexicon Extractionfrom Comparable Corpora.
In Proceedings of the42nd Annual Meeting of the Association for Com-putational Linguistics (ACL?04), pages 526?533,Barcelona, Spain.Gregory Grefenstette.
1994.
Corpus-Derived First,Second and Third-Order Word Affinities.
In Pro-ceedings of the 6th Congress of the European As-sociation for Lexicography (EURALEX?94), pages279?290, Amsterdam, The Netherlands.1292Amir Hazem and Emmanuel Morin.
2013.
Wordco-occurrence counts prediction for bilingual ter-minology extraction from comparable corpora.
InProceedings of the Sixth International Joint Confer-ence on Natural Language Processing (IJCNLP?13),pages 1392?1400, Nagoya, Japan.Azniah Ismail and Suresh Manandhar.
2010.
Bilinguallexicon extraction from comparable corpora usingin-domain terms.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics(COLING?10), pages 481?489, Beijing, China.Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
InProceedings of the ACL-02 Workshop on Unsuper-vised Lexical Acquisition (ULA?02), pages 9?16,Philadelphia, PA, USA.Audrey Laroche and Philippe Langlais.
2010.
Revis-iting Context-based Projection Methods for Term-Translation Spotting in Comparable Corpora.
InProceedings of the 23rd International Conferenceon Computational Linguistics (COLING?10), pages617?625, Beijing, China.Bo Li and?Eric Gaussier.
2010.
Improving corpuscomparability for bilingual lexicon extraction fromcomparable corpora.
In Proceedings of the 23rd In-ternational Conference on Computational Linguis-tics (COLING?10), pages 644?652, Beijing, China.Christopher D. Manning, Prabhakar Raghavan, andHinrich Schtze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press, New York,NY, USA.Anthony McEnery and Zhonghua Xiao.
2007.
Paral-lel and comparable corpora: What are they up to?In Gunilla Anderman and Margaret Rogers, editors,Incorporating Corpora: Translation and the Lin-guist, Multilingual Matters, chapter 2, pages 18?31.Clevedon, UK.Emmanuel Morin and Emmanuel Prochasson.
2011.Bilingual lexicon extraction from comparable cor-pora enhanced with parallel corpora.
In Proceedingsof the 4th Workshop on Building and Using Compa-rable Corpora (BUCC?11), pages 27?34, Portland,OR, USA.Emmanuel Morin, B?eatrice Daille, Koichi Takeuchi,and Kyo Kageura.
2007.
Bilingual TerminologyMining ?
Using Brain, not brawn comparable cor-pora.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics(ACL?07), pages 664?671, Prague, Czech Republic.Pablo Gamallo Otero.
2007.
Learning bilingual lexi-cons from comparable english and spanish corpora.In Proceedings of the 11th Conference on MachineTranslation Summit (MT Summit XI), pages 191?198, Copenhagen, Denmark.Carol Peters and Eugenio Picchi.
1998.
Cross-language information retrieval: A system for com-parable corpus querying.
In Gregory Grefenstette,editor, Cross-language information retrieval, chap-ter 7, pages 81?90.
Kluwer Academic Publishers.Emmanuel Prochasson and Pascale Fung.
2011.
RareWord Translation Extraction from Aligned Compa-rable Documents.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics (ACL?11), pages 1327?1335, Portland, OR,USA.Emmanuel Prochasson, Emmanuel Morin, and KyoKageura.
2009.
Anchor points for bilingual lexiconextraction from small comparable corpora.
In Pro-ceedings of the 12th Conference on Machine Trans-lation Summit (MT Summit XII), pages 284?291, Ot-tawa, Canada.Reinhard Rapp.
1995.
Identify Word Translations inNon-Parallel Texts.
In Proceedings of the 35th An-nual Meeting of the Association for ComputationalLinguistics (ACL?95), pages 320?322, Boston, MA,USA.Reinhard Rapp.
1999.
Automatic Identification ofWord Translations from Unrelated English and Ger-man Corpora.
In Proceedings of the 37th AnnualMeeting of the Association for Computational Lin-guistics (ACL?99), pages 519?526, College Park,MD, USA.Rapha?el Rubino and Georges Linar`es.
2011.
A multi-view approach for term translation spotting.
In Pro-ceedings of the 12th International Conference onComputational Linguistics and Intelligent Text Pro-cessing (CICLing?11), pages 29?40, Tokyo, Japan.Kumiko Tanaka and Hideya Iwasaki.
1996.
Extractionof Lexical Translations from Non-Aligned Corpora.In Proceedings of the 16th International Conferenceon Computational Linguistics (COLING?96), pages580?585, Copenhagen, Denmark.Kun Yu and Junichi Tsujii.
2009.
Extracting bilin-gual dictionary from comparable corpora with de-pendency heterogeneity.
In Proceedings of the2013 Conference of the North American Chapterof the Association for Computational Linguistics:Human Language Technologies (NAACL-HLT?09),pages 121?124, Boulder, CO, USA.1293
