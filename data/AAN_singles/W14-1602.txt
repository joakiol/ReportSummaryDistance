Proceedings of the Eighteenth Conference on Computational Language Learning, pages 11?20,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsDomain-Specific Image CaptioningRebecca Mason and Eugene CharniakBrown Laboratory for Linguistic Information Processing (BLLIP)Brown University, Providence, RI 02912{rebecca,ec}@cs.brown.eduAbstractWe present a data-driven framework forimage caption generation which incorpo-rates visual and textual features with vary-ing degrees of spatial structure.
We pro-pose the task of domain-specific imagecaptioning, where many relevant visualdetails cannot be captured by off-the-shelfgeneral-domain entity detectors.
We ex-tract previously-written descriptions froma database and adapt them to new queryimages, using a joint visual and textualbag-of-words model to determine the cor-rectness of individual words.
We imple-ment our model using a large, unlabeleddataset of women?s shoes images and nat-ural language descriptions (Berg et al.,2010).
Using both automatic and humanevaluations, we show that our caption-ing method effectively deletes inaccuratewords from extracted captions while main-taining a high level of detail in the gener-ated output.1 IntroductionBroadly, the task of image captioning is: given aquery image, generate a natural language descrip-tion of the image?s visual content.
Both the im-age understanding and language generation com-ponents of this task are challenging open problemsin their respective fields.
A wide variety of ap-proaches have been proposed in the literature, forboth the specific task of caption generation as wellas related problems in understanding images andtext.Typically, image understanding systems use su-pervised algorithms to detect visual entities andconcepts in images.
However, these typically re-quire accurate hand-labeled training data, whichis not available in most specific domains.
Ideally,1.
Extract existing human-authored caption according tosimilarity of coarse visual features.Query Image Nearest-NeighborNearest-neighbor caption: This sporty sneaker clog keepsfoot cool and comfortable and fully supported.2.
Estimate correctness of extracted words using domain-specific joint model of text and visual bag-of-word features.This sporty sneaker clog keeps foot cool and comfortable andfully supported.3.
Compress extracted caption to adapt its content whilemaintaining grammatical correctness.Output: This clog keeps foot comfortable and supported.a domain-specific image captioning system wouldlearn in a less supervised fashion, using captionedimages found on the web.This paper focuses on image caption genera-tion for a specific domain ?
images of women?sshoes, collected from online shopping websites.Our framework has three main components.
Weextract an existing description from a databaseof human-captions, by projecting query imagesinto a multi-dimensional space where structurallysimilar images are near each other.
We alsotrain a joint topic model to discover the latenttopics which generate both captions and images.We combine these two approaches using sentencecompression to delete modifying details in the ex-tracted caption which are not relevant to the queryimage.Our captioning framework is inspired by sev-eral recent approaches at the intersection of Nat-ural Language Processing and Computer Vision.Previous work such as Farhadi et al.
(2010) andOrdonez et al.
(2011) explore extractive methodsfor image captioning, but these rely on general-domain visual detection systems, and only gener-11ate extractive captions.
Other models learn corre-spondences between domain-specific images andnatural language captions (Berg et al., 2010; Fengand Lapata, 2010b) but cannot generate descrip-tions for new images without the use of auxil-iary text.
Kuznetsova et al.
(2013) propose asentence compression model for editing imagecaptions, but their compression objective is notconditioned on a query image, and their systemalso requires general-domain visual detections.This paper proposes an image captioning frame-work which extends these ideas and culminates inthe first domain-specific image caption generationsystem.More broadly, our goal for image caption gener-ation is to work toward less supervised captioningmethods which could be used to generate detailedand accurate descriptions for a variety of long-taildomains of captioned image data, such as in natureand medicine.2 Related WorkOur framework for domain-specific image cap-tioning consists of three main components: ex-tractive caption generation, image understandingthrough topic modeling, and sentence compres-sion.1These methods have previously been ap-plied individually to related tasks such as gen-eral domain image captioning and annotation.
Webriefly describe some of the related work:2.1 Extractive Caption GenerationIn previous work on image caption extraction, cap-tions are generated by retrieving human-authoreddescriptions from visually similar images.
Farhadiet al.
(2010) and Ordonez et al.
(2011) retrievewhole captions to apply to a query image, whileKuznetsova et al.
(2012) generate captions usingtext retrieved from multiple sources.
The descrip-tions are related to visual concepts in the queryimage, but these models use visual similarity toapproximate textual relevance; they do not modelimage and textual features jointly.2.2 Image UnderstandingRecent improvements in state-of-the-art visual ob-ject class detections (Felzenszwalb et al., 2010)1A research proposal for this framework and other imagecaptioning ideas was previously presented at NAACL Stu-dent Research Workshop in 2013 (Mason, 2013).
This paperpresents a completed project including implementation de-tails and experimental results.have enabled much recent work in image captiongeneration (Farhadi et al., 2010; Ordonez et al.,2011; Kulkarni et al., 2011; Yang et al., 2011;Mitchell et al., 2012; Yu and Siskind, 2013).
How-ever, these systems typically rely on a small num-ber of detection types, e.g.
the twenty object cate-gories from the PASCAL VOC challenge.2Theseobject categories include entities which are com-monly described in general domain images (peo-ple, cars, cats, etc) but these require labeled train-ing data which is not typically available for the vi-sually relevant entities in specific domains.Our caption generation system employs a multi-modal topic model from our previous work (Ma-son and Charniak, 2013) which generates descrip-tive words, but lacks the spatial structure neededto generate a full sentence caption.
Other previ-ous work uses topic models to learn the semanticcorrespondence between images and labels (e.g.Blei and Jordan (2003)), but learning from naturallanguage descriptions is considerably more diffi-cult because of polysemy, hypernymy, and mis-alginment between the visual content of an im-age and the content humans choose to describe.The MixLDA model (Feng and Lapata, 2010b;Feng and Lapata, 2010a) learns from news imagesand natural language descriptions, but to generatewords for a new image it requires both a queryimage and query text in the form of a news arti-cle.
Berg et al.
(2010) use discriminative modelsto discover visual attributes from online shoppingimages and captions, but their models do not gen-erate descriptive words for unseen images.2.3 Sentence CompressionTypical models for sentence compression (Knightand Marcu, 2002; Furui et al., 2004; Turner andCharniak, 2005; Clarke and Lapata, 2008) have asummarization objective: reduce the length of asource sentence without changing its meaning.
Incontrast, our objective is to change the meaning ofthe source sentence, letting its overall correctnessrelative to the query image determine the lengthof the output.
Our objective differs from that ofKuznetsova et al.
(2013), who compress imagecaption sentences with the objective of creating acorpus of generally transferrable image captions.Their compression objective is to maximize theprobability of a caption conditioned on the source2http://pascallin.ecs.soton.ac.uk/challenges/VOC/12Two adjustable bucklestraps top a classic rubberrain boot grounded by athick lug sole for excellentwet-weather traction.Available in Plus Size.
Fauxsnake skin flats with a largecrossover buckle at the toe.Padded insole for a comfort-able all day fit.Glitter-covered elastic up-per in a two-piece dress san-dal style with round opentoe.
Single vamp strap withcontrasting trim matchingelasticized heel strap criss-crosses at instep.Explosive!
These whiteleather joggers are sure tomake a big impression.
De-tails count, including a toeoverlay, millennium trimand lightweight raised sole.Table 1: Example data from the Attribute Discovery Dataset (Berg et al., 2010).
See Section 3.image, while our objective is conditioned on thequery image that we are generating a caption for.Additionally, their model also relies on general-domain trained visual detections.3 Dataset and PreprocessingThe dataset we use is the women?s shoes sec-tion of the publicly available Attribute Discov-ery Dataset3from Berg et al.
(2010), which con-sists of product images and captions scraped fromthe shopping website Like.com.
We use thewomen?s shoes section of the dataset which has14764 captioned images.
Product descriptions de-scribe many different attributes such as styles, col-ors, fabrics, patterns, decorations, and affordances(activities that can be performed while wearing theshoe).
Some examples are shown in Table 1.For preprocessing in our framework, we first de-termine an 80/20% train test split.
We define a tex-tual vocabulary of ?descriptive words?, which arenon-function words ?
adjectives, adverbs, nouns(except proper nouns), and verbs.
This gives usa total of 9578 descriptive words in the trainingset, with an average of 16.33 descriptive words percaption.4 Image Captioning Framework4.1 ExtractionTo repeat, our overall process is to first find a cap-tion sentence from our database to use as a tem-plate, and then correct the template sentences us-ing sentence compresion.
We compress by remov-3http://tamaraberg.com/attributesDataset/index.htmling details that are probably not correct for the testimage.
For example, if the sentence describes ?ared slipper?
but the shoe in the query image is yel-low, we want to remove ?red?
and keep the rest.As in this simple example, the basic paradigmfor compression is to keep the head words ofphrases (?slipper?)
and remove modifiers.
Thuswe want to extraction stage of our scheme to bemore likely to find a candidate sentence with cor-rect head words, figuring that the compressionstage can edit the mistakes.
Our hypothesis is thatheadwords tend to describe more spatially struc-tured visual concepts, while modifier words de-scribe those that are more easily represented usinglocal or unstructured features.4Table 2 containsadditional example captions with parses.GIST (Oliva and Torralba, 2001) is a com-monly used feature in Computer Vision whichcoarsely localizes perceptual attributes (e.g.
roughvs smooth, natural vs manmade).
By computingthe GIST of the images, we project them into amulti-dimensional Euclidean space where imageswith semantically similar structures are locatednear each other.
Thus the extraction stage of ourcaption generation process selects a sentence fromthe GIST nearest-neighbor to the query image.54.2 Joint Topic ModelThe second component of our framework incorpo-rates visual and textual features using a less struc-tured model.
We use a multi-modal topic model4For example, the color ?red?
can be described using abag of random pixels, while a ?slipper?
is a spatial configura-tion of parts in relationship to each other.5See Section 5.1 for additional implementation details.13Table 2: Example parses of women?s shoes descriptions.
Our hypothesis is that the headwords in phrasesare more likely to describe visual concepts which rely on spatial locations or relationships, while modi-fiers words can be represented using less-structured visual bag-of-words features.to learn the latent topics which generate bag-of-words features for an image and its caption.The bag-of-words model for Computer Visionrepresents images as a mixture of topics.
Mea-sures of shape, color, texture, and intensity arecomputed at various points on the image and clus-tered into discrete ?codewords?
using the k-meansalgorithm.6Unlike text words, an individual code-word has little meaning on its own, but distri-butions of codewords can provide a meaningful,though unstructured, representation of an image.An image and its caption do not express exactlythe same information, but they are topically re-lated.
We employ the Polylingual Topic Model(Mimno et al., 2009), which is originally used tomodel corresponding documents in different lan-guages that are topically comparable, but not par-allel translations.
In particular, we employ ourprevious work (Mason and Charniak, 2013) whichextends this model to topically similar images andnatural language captions.
The generative processfor a captioned image starts with a single topicdistribution drawn from concentration parameter?
and base measure m:?
?
Dir(?, ?m) (1)Modality-specific latent topic assignments zimgand ztxtare drawn for each of the text words andcodewords:zimg?
P (zimg|?)
=?n?zimgn(2)6While space limits a more detailed explanation of visualbag-of-word features, Section 5.2 provides a brief overviewof the specific visual attributes used in this model.ztxt?
P (ztxt|?)
=?n?ztxtn(3)Observed words are generated according to theirprobabilities in the modality-specific topics:wimg?
P (wimg|zimg,?img) = ?imgwimgn|zimgn(4)wtxt?
P (wtxt|ztxt,?txt) = ?txtwtxtn|ztxtn(5)Given the uncaptioned query image qimgandthe trained multi-modal topic model, it is now pos-sible to infer the shared topic proportion for qimgusing Gibbs sampling:P (zn= t|qimg, z\n,?img, ?m)?
?imgqimgn|t(Nt)\n+ ?mt?tNt?
1 + ?
(6)4.3 Sentence CompressionLet w = w1, w2, ..., wnbe the words in the ex-tracted caption for qimg.
For each word, we de-fine a binary decision variable ?, such that ?i= 1if wiis included in the output compression, and?i= 0 otherwise.
Our objective is to find valuesof ?
which generate a caption for qimgwhich isboth semantically and grammatically correct.We cast this problem as an Integer Linear Pro-gram (ILP), which has previously been used forthe standard sentence compression task (Clarkeand Lapata, 2008; Martins and Smith, 2009).
ILPis a mathematical optimization method for deter-mining the optimal values of integer variables inorder to maximize an objective given a set of con-straints.144.3.1 ObjectiveThe ILP objective is a weighted linear combina-tion of two measures which represent the correct-ness and fluency of the output compression:Correctness: Recall in Section 3 we definedwords as either descriptive words or functionwords.
For each descriptive word, we estimateP (wi|qimg), using topic proportions estimated us-ing Equation 6:P (wi|qimg) =?tP (wi|ztxtt)P (zt|qimg) (7)This is used to find I(wi), a function of the likeli-hood of each word in the extracted caption:I(wi) ={P (wi|qimg)?
P (wi), if descriptive0, function word(8)This function considers the prior probability of wibecause frequent words often have a high posteriorprobability even when they are inaccurate.
Thusthe sum?ni=1?i?
I(wi) is the overall measure ofthe correctness of a proposed caption conditionedon qimg.Fluency: We formulate a trigram languagemodel as an ILP, which requires additional binarydecision variables: ?i= 1 if wibegins the out-put compression, ?ij= 1 if the bigram sequencewi, wjends the compression, ?ijk= 1 if the tri-gram sequence wi, wj, wkis in the compression,and a special ?start token?
?0= 1.
This languagemodel favors shorter sentences, which is not nec-essarily the objective for image captioning, so weintroduce a weighting factor, ?, to lessen the ef-fect.Here is the combined objective, using P to rep-resent logP :max z =(n?i=1?i?
P (wi|start)+n?2?i=1n?1?j=i+1n?k=j+1?ijk?
P (wk|wi, wj)+n?1?i=0n?j=i+1?ij?
P (end|wi, wj))?
?+n?i=1?i?
I(wi) (9)Sequential1.
)?i?i= 12.)
?k?
?k?
?k?2i=0?k?1j=1?ijk= 0?k : k ?
1...n3.)
?j??j?1i=0?nk=j+1?ijk?
?j?1i=0?ij= 0?j : j ?
1...n4.)?n?1j=i+1?nk=j+1?ijk??nj=i+1?ij??i?1h=0?hi?
?i= 0?i : i ?
1...n5.
)?n?1i=0?nj=i+1?ij= 1Modifier1.
If head of the extracted sentence= wi, then?i= 12.
If wiis head of a noun phrase, then ?i= 13.
Punctuation and coordinating conjunctionsfollow special rules (below).
Otherwise, ifheadof(wi) = wj, then ?i?
?jOther1.?i?i?
32.
Define valid use of puncutation and coordi-nating conjunctions.Table 3: Summary of ILP constraints.4.3.2 ILP ConstraintsThe ILP constraints ensure both the mathematicalvalidity of the model, and the grammatical correct-ness of its output.
Table 3 summarizes the list ofconstraints.
Sequential constraints are defined asin Clarke (2008) ensure that the ordering of the tri-grams is valid, and that the mathematical validityof the model holds.5 Implementation Details5.1 ExtractionGIST features are computed using code by Olivaand Torralba (2001)7.
GIST is computed with im-ages converted to grayscale; since color featurestend to act as modifiers in this domain.
Nearest-neighbors are selected according to minimum dis-tance from qimgto both a regularly-oriented and ahorizontally-flipped training image.Only one sentence from the first nearest-neighbor caption is extracted.
In the case of multi-sentence captions, we select the first suitable sen-tence according to the following criteria 1.)
hasat least five tokens, 2.)
does not contain NNP orNNPS (brand names), 3.)
does not fail to parseusing Stanford Parser (Klein and Manning, 2003).If the nearest-neighbor caption does not have anysentences meeting these criteria, caption sentencesfrom the next nearest-neighbor(s) are considered.7http://people.csail.mit.edu/torralba/code/spatialenvelope/155.2 Joint Topic ModelWe use the Joint Topic Model that we imple-mented in our previous work; please see Masonand Charniak (2013) for the full model and imple-mentation details.
The topic model is trained with200 topics using the polylingual topic model im-plementation from MALLET8.
Briefly, the code-words represent the following attributes:SHAPE: SIFT (Lowe, 1999) describes theshapes of detected edges in the image, using de-scriptors which are invariant to changes in rotationand scale.COLOR: RGB (red, green, blue) and HSV (hue,saturation, value) pixel values are sampled from acentral area of the image to represent colors.TEXTURE: Textons (Leung and Malik, 2001)are computed by convolving images with Gaborfilters at multiple orientations and scales, thensampling the outputs at random locations.INTENSITY: HOG (histogram of gradients)(Dalal and Triggs, 2005) describes the directionand intensity of changes in light.
These featuresare computed on the image over a densely sam-pled grid.5.3 CompressionThe sentence compression ILP is implemented us-ing the CPLEX optimization toolkit9.
The lan-guage model weighting factor in the objective is?
= 10?3, which was hand-tuned according toobserved output.
The trigram language modelis trained on training set captions using Berke-leyLM (Pauls and Klein, 2011) with Kneser-Neysmoothing.
For the constraints, we use parsesfrom Stanford Parser (Klein and Manning, 2003)and the ?semantic head?
variation of the Collinsheadfinder Collins (1999).6 Evaluation6.1 SetupWe compare the following systems and baselines:KL (EXTRACTION): The top performing ex-tractive model from Feng and Lapata (2010a), andthe second-best captioning model overall.
Usingestimated topic distributions from our joint model,we extract the source with minimum KL Diver-gence from qimg.8http://mallet.cs.umass.edu/9http://www-01.ibm.com/software/integration/optimization/cplex-optimization-studio/ROUGE-2 Average 95% Confidence int.KL (EXTRACTION)P .06114 ( .05690 - .06554 )R .02499 ( .02325 - .02686)F .03360 ( .03133 - .03600 )GIST (EXTRACTION)P .10894 ( .09934 - .11921 )R .05474 ( .04926 - .06045)F .06863 ( .06207 - .07534)LM-ONLY (COMPRESSION)P .13782 ( .12602 - .14864 )R .02437 ( .02193 - .02700 )F .03864 ( .03512 - .04229)SYSTEM (COMPRESSION)P .16752 (.15679 -.17882 )R .05060 ( .04675 - .05524 )F .07204 ( .06685 - .07802 )Table 4: ROUGE-2 (bigram) scores.
The pre-cision of our system compression (bolded) sig-nificantly improves over the caption that it com-presses (GIST), without a significant decrease inrecall.GIST (EXTRACTION): The sentence extractedusing GIST nearest-neighbors, and the uncom-pressed source for the compression systems.LM-ONLY (COMPRESSION): We include thisbaseline to demonstrate that our model is effec-tively conditioning output compressions on qimg,as opposed to simply generalizing captions as inKuznetsova et al.
(2013)10.
We modify the com-pression ILP to ignore the content objective andonly maximize the trigram language model (stillsubject to the constraints).SYSTEM (COMPRESSION): Our full system.Unfortunately, we cannot compare our systemagainst prior work in general-domain image cap-tioning, because those models use visual detec-tion systems which train on labeled data that is notavailable in our domain.6.2 Automatic EvaluationWe perform automatic evaluation using similar-ity measures between automatically generated andhuman-authored captions.
Note that currentlyour system and baselines only generate single-sentence captions, but we compare against entire10Technically their model is conditioned on the source im-age, in order to address alignment issues which are not appli-cable in our setup.16BLEU@1KL (EXTRACTION) .2098GIST (EXTRACTION) .4259LM-ONLY (COMPRESSION) .4780SYSTEM (COMPRESSION) .4841Table 5: BLEU@1 scores of generated captionsagainst human authored captions.
Our model(bolded) has the highest BLEU@1 score with sig-nificance.held-out captions in order to increase the amountof text we have to compare against.ROUGE (Lin, 2004) is a summarization eval-uation metric which has also been used to eval-uate image captions (Yang et al., 2011).
It isusually a recall-oriented measure, but we also re-port precision and f-measure because our sen-tence compressions do not improve recall.
Table 4shows ROUGE-2 (bigram) scores computed with-out stopwords.We observe that our system very significantlyimproves ROUGE-2 precision of the GIST ex-tracted caption, without significantly reducing re-call.
While LM-Only also improves precisionagainst GIST extraction, it indiscriminately re-moves some words which are relevant to thequery image.
We also observe that GIST extrac-tion strongly outperforms the KL model, whichdemonstrates the importance of visual structure.We also report BLEU (Papineni et al., 2002)scores, which are the most popularly accepted au-tomatic metric for captioning evaluation (Farhadiet al., 2010; Kulkarni et al., 2011; Ordonez etal., 2011; Kuznetsova et al., 2012; Kuznetsovaet al., 2013).
Results are very similar to theROUGE-2 precision scores, except the differencebetween our system and LM-Only is less pro-nounced because BLEU counts function words,while ROUGE does not.6.3 Human EvaluationWe perform human evaluation of compressionsgenerated by our system and LM-Only.
Users areshown the query image, the original uncompressedcaption, and a compressed caption, and are askedtwo questions: does the compression improve theaccuracy of the caption, and is the compressiongrammatical.We collect 553 judgments from six women whoare native English-speakers and knowledgeableQuery Image GIST Nearest-NeighborExtraction: Shimmering snake-embossed leather upper in aslingback evening dress sandal style with a round open toe.Compression: Shimmering upper in a slingback eveningdress sandal style with a round open toe.Query Image GIST Nearest-NeighborExtraction: This sporty sneaker clog keeps foot cool andcomfortable and fully supported.Compression: This clog keeps foot comfortable and sup-ported.Query Image GIST Nearest-NeighborExtraction: Italian patent leather peep-toe ballet flat with asignature tailored grosgrain bow.Compression: leather ballet flat with a signature tailoredgrosgrain bow.Query Image GIST Nearest-NeighborExtraction: Platform high heel open toe pump with horsebitavailable in silver guccissima leather with nickel hardwarewith leather sole.Compression: Platform high heel open toe pump withhorsebit available in leather with nickel hardware withleather sole.Table 6: Example output from our full system.Red underlined words indicate the words whichare deleted by our compression model.17SYSTEM LM-ONLYYes No Yes NoCompressionimprovesaccuracy63.2% 36.8% 42.6% 57.4%Compression isgrammatical73.1% 26.9% 82.2% 17.8%Table 7: Human evaluation results.about fashion.11Users were recruited via emailand did the study over the internet.Table 7 reports the results of the human evalu-ation.
Users report 63.2% of SYSTEM compres-sions improve accuracy over the original, whilethe other 36.8% did not improve accuracy.
(Keepin mind that a bad compression does not make thecaption less accurate, just less descriptive.)
LM-ONLY improves accuracy for less than half of thecaptions, which is significantly worse than SYS-TEM captions (Fisher exact test, two-tailed p lessthan 0.01).Users find LM-Only compressions to be slightlymore grammatical than System compressions, butthe difference is not significant.
(p > 0.05)7 ConclusionWe introduce the task of domain-specific imagecaptioning and propose a captioning system whichis trained on online shopping images and natu-ral language descriptions.
We learn a joint topicmodel of vision and text to estimate the correct-ness of extracted captions, and use a sentencecompression model to propose a more accurateoutput caption.
Our model exploits the connectionbetween image and sentence structure, and can beused to improve the accuracy of extracted imagecaptions.The task of domain-specific image captiongeneration has been overlooked in favor of thegeneral-domain case, but we believe the domain-specific case deserves more attention.
Whileimage captioning can be viewed as a complexgrounding problem, a good image caption shoulddo more than label the objects in the image.
Whenan expert looks at images in a specific domain, heor she makes inferences that would not be made bya non-expert.
Providing this information to non-11About 15% of output compressions are the same for bothsystems, and about 10% have no deleted words in the outputcompression.
We include the former in the human evaluation,but not the latter.Query Image GIST Nearest-NeighborExtraction: Classic ballet flats with decorative canvasstrap and patent leather covered buckle.Compression: Classic ballet flats covered.Query Image GIST Nearest-NeighborExtraction: This shoe is the perfect shoe for you , fea-turing an open toe and a lace up upper with a high heel, and a two tone color .Compression: This shoe is the shoe , featuring an open toeand upper with a high heel .Table 8: Examples of bad performance.
The topexample is a parse error, while the bottom exam-ple deletes modifiers that are not part of the imagedescription.expert users in the form of an image caption willgreatly expand the utility for automatic image cap-tioning.ReferencesTamara L. Berg, Alexander C. Berg, and Jonathan Shih.2010.
Automatic attribute discovery and charac-terization from noisy web data.
In Proceedings ofthe 11th European conference on Computer vision:Part I, ECCV?10, pages 663?676, Berlin, Heidel-berg.
Springer-Verlag.David M. Blei and Michael I. Jordan.
2003.
Modelingannotated data.
In Proceedings of the 26th annualinternational ACM SIGIR conference on Researchand development in informaion retrieval, SIGIR ?03,pages 127?134, New York, NY, USA.
ACM.James Clarke and Mirella Lapata.
2008.
Global infer-ence for sentence compression an integer linear pro-gramming approach.
J. Artif.
Int.
Res., 31(1):399?429, March.James Clarke.
2008.
Global Inference for SentenceCompression: An Integer Linear Programming Ap-proach.
Dissertation, University of Edinburgh.Michael John Collins.
1999.
Head-driven statisticalmodels for natural language parsing.
Ph.D. thesis,Philadelphia, PA, USA.
AAI9926110.18N.
Dalal and B. Triggs.
2005.
Histograms of orientedgradients for human detection.
In Computer Visionand Pattern Recognition, 2005.
CVPR 2005.
IEEEComputer Society Conference on, volume 1, pages886 ?893 vol.
1, june.Ali Farhadi, Mohsen Hejrati, Mohammad AminSadeghi, Peter Young, Cyrus Rashtchian, JuliaHockenmaier, and David Forsyth.
2010.
Every pic-ture tells a story: generating sentences from images.In Proceedings of the 11th European conference onComputer vision: Part IV, ECCV?10, pages 15?29,Berlin, Heidelberg.
Springer-Verlag.P.
F. Felzenszwalb, R. B. Girshick, D. McAllester, andD.
Ramanan.
2010.
Object detection with discrim-inatively trained part based models.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,32(9):1627?1645.Yansong Feng and Mirella Lapata.
2010a.
How manywords is a picture worth?
automatic caption genera-tion for news images.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, ACL ?10, pages 1239?1249, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Yansong Feng and Mirella Lapata.
2010b.
Topic mod-els for image annotation and text illustration.
InHLT-NAACL, pages 831?839.Sadaoki Furui, Tomonori Kikuchi, Yousuke Shinnaka,and Chiori Hori.
2004.
Speech-to-text and speech-to-speech summarization of spontaneous speech.IEEE TRANS.
ON SPEECH AND AUDIO PRO-CESSING, 12(4):401?408.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Kevin Knight and Daniel Marcu.
2002.
Summa-rization beyond sentence extraction: a probabilis-tic approach to sentence compression.
Artif.
Intell.,139(1):91?107, July.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, SimingLi, Yejin Choi, Alexander C. Berg, and Tamara L.Berg.
2011.
Baby talk: Understanding and gener-ating simple image descriptions.
In CVPR, pages1601?1608.Polina Kuznetsova, Vicente Ordonez, Alexander C.Berg, Tamara L. Berg, and Yejin Choi.
2012.
Col-lective generation of natural image descriptions.
InACL.Polina Kuznetsova, Vicente Ordonez, Alexander Berg,Tamara Berg, and Yejin Choi.
2013.
Generalizingimage captions for image-text parallel corpus.
InACL.T.
Leung and J. Malik.
2001.
Representing and rec-ognizing the visual appearance of materials usingthree-dimensional textons.
International Journal ofComputer Vision, 43(1):29?44.Chin-Yew Lin.
2004.
Rouge: A package for auto-matic evaluation of summaries.
In Stan SzpakowiczMarie-Francine Moens, editor, Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81, Barcelona, Spain, July.
Associa-tion for Computational Linguistics.D.G.
Lowe.
1999.
Object recognition from local scale-invariant features.
In Computer Vision, 1999.
TheProceedings of the Seventh IEEE International Con-ference on, volume 2, pages 1150 ?1157 vol.2.Andr?e F. T. Martins and Noah A. Smith.
2009.
Sum-marization with a joint model for sentence extractionand compression.
In Proceedings of the Workshopon Integer Linear Programming for Natural Lan-gauge Processing, ILP ?09, pages 1?9, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.R.
Mason and E. Charniak.
2013.
Annotation of onlineshopping images without labeled training examples.Workshop on Vision and Language (WVL).Rebecca Mason.
2013.
Domain-independent caption-ing of domain-specific images.
NAACL Student Re-search Workshop.David Mimno, Hanna M. Wallach, Jason Naradowsky,David A. Smith, and Andrew McCallum.
2009.Polylingual topic models.
In Proceedings of the2009 Conference on Empirical Methods in Nat-ural Language Processing: Volume 2 - Volume2, EMNLP ?09, pages 880?889, Stroudsburg, PA,USA.
Association for Computational Linguistics.Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-maguchi, Karl Stratos, Xufeng Han, Alyssa Men-sch, Alexander C. Berg, Tamara L. Berg, and HalDaum?e III.
2012.
Midge: Generating image de-scriptions from computer vision detections.
In Euro-pean Chapter of the Association for ComputationalLinguistics (EACL).Aude Oliva and Antonio Torralba.
2001.
Modeling theshape of the scene: A holistic representation of thespatial envelope.
International Journal of ComputerVision, 42:145?175.V.
Ordonez, G. Kulkarni, and T.L.
Berg.
2011.Im2text: Describing images using 1 million cap-tioned photographs.
In NIPS.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.19Adam Pauls and Dan Klein.
2011.
Faster and smallern-gram language models.
In Proceedings of ACL,Portland, Oregon, June.
Association for Computa-tional Linguistics.Jenine Turner and Eugene Charniak.
2005.
Super-vised and unsupervised learning for sentence com-pression.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,ACL ?05, pages 290?297, Stroudsburg, PA, USA.Association for Computational Linguistics.Yezhou Yang, Ching Lik Teo, Hal Daum?e III, and Yian-nis Aloimonos.
2011.
Corpus-guided sentence gen-eration of natural images.
In Empirical Methodsin Natural Language Processing (EMNLP), Edin-burgh, Scotland.Haonan Yu and Jeffrey Mark Siskind.
2013.
Groundedlanguage learning from video described with sen-tences.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), volume 1, pages 53?63,Sofia, Bulgaria.
Association for Computational Lin-guistics.20
