Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 623?633,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsTowards a model of formal and informal address in EnglishManaal FaruquiComputer Science and EngineeringIndian Institute of TechnologyKharagpur, Indiamanaalfar@gmail.comSebastian Pad?Institute of Computational LinguisticsHeidelberg UniversityHeidelberg, Germanypado@cl.uni-heidelberg.deAbstractInformal and formal (?T/V?)
address in dia-logue is not distinguished overtly in mod-ern English, e.g.
by pronoun choice likein many other languages such as French(?tu?/?vous?).
Our study investigates thestatus of the T/V distinction in English liter-ary texts.
Our main findings are: (a) humanraters can label monolingual English utter-ances as T or V fairly well, given sufficientcontext; (b), a bilingual corpus can be ex-ploited to induce a supervised classifier forT/V without human annotation.
It assignsT/V at sentence level with up to 68% accu-racy, relying mainly on lexical features; (c),there is a marked asymmetry between lex-ical features for formal speech (which areconventionalized and therefore general) andinformal speech (which are text-specific).1 IntroductionIn many Indo-European languages, there are twopronouns corresponding to the English you.
Thisdistinction is generally referred to as the T/V di-chotomy, from the Latin pronouns tu (informal, T)and vos (formal, V) (Brown and Gilman, 1960).The V form (such as Sie in German and Vous inFrench) can express neutrality or polite distanceand is used to address social superiors.
The Tform (German du, French tu) is employed towardsfriends or addressees of lower social standing, andimplies solidarity or lack of formality.English used to have a T/V distinction until the18th century, using you as V pronoun and thoufor T. However, in contemporary English, you hastaken over both uses, and the T/V distinction is notmarked anymore.
In NLP, this makes generationin English and translation into English easy.
Con-versely, many NLP tasks suffer from the lack ofinformation about formality, e.g.
the extraction ofsocial relationships or, notably, machine transla-tion from English into languages with a T/V dis-tinction which involves a pronoun choice.In this paper, we investigate the possibility torecover the T/V distinction for (monolingual) sen-tences of 19th and 20th-century English such as:(1) Can I help you, Sir?
(V)(2) You are my best friend!
(T)After describing the creation of an English corpusof T/V labels via annotation projection (Section 3),we present an annotation study (Section 4) whichestablishes that taggers can indeed assign T/V la-bels to monolingual English utterances in contextfairly reliably.
Section 5 investigates how T/V isexpressed in English texts by experimenting withdifferent types of features, including words, seman-tic classes, and expressions based on PolitenessTheory.
We find word features to be most reliable,obtaining an accuracy of close to 70%.2 Related WorkThere is a large body of work on the T/V distinc-tion in (socio-)linguistics and translation studies,covering in particular the conditions governingT/V usage in different languages (Kretzenbacheret al 2006; Sch?pbach et al 2006) and the diffi-culties in translation (Ardila, 2003; K?nzli, 2010).However, many observations from this literatureare difficult to operationalize.
Brown and Levin-son (1987) propose a general theory of politenesswhich makes many detailed predictions.
They as-sume that the pragmatic goal of being polite givesrise to general communication strategies, such asavoiding to lose face (cf.
Section 5.2).In computational linguistics, it is a commonobservation that for almost every language pair,there are distinctions that are expressed overtly623Please permit me to askyou a question.Darf ich Sie etwas fragen?Step 2: copy T/V classlabel to English sentenceStep 1: German pronounprovides overt T/V labelVVprojectionFigure 1: T/V label induction for English sentences ina parallel corpus with annotation projectionin one language, but remain covert in the other.Examples include morphology (Fraser, 2009) andtense (Schiehlen, 1998).
A technique that is oftenapplied in such cases is annotation projection, theuse of parallel corpora to copy information from alanguage where it is overtly realized to one whereit is not (Yarowsky and Ngai, 2001; Hwa et al2005; Bentivogli and Pianta, 2005).The phenomenon of formal and informal ad-dress has been considered in the contexts of transla-tion into (Hobbs and Kameyama, 1990; Kanayama,2003) and generation in Japanese (Bateman, 1988).Li and Yarowsky (2008) learn pairs of formal andinformal constructions in Chinese with a para-phrase mining strategy.
Other relevant recent stud-ies consider the extraction of social networks fromcorpora (Elson et al 2010).
A related study is(Bramsen et al 2011) which considers anothersociolinguistic distinction, classifying utterancesas ?upspeak?
and ?downspeak?
based on the socialrelationship between speaker and addressee.This paper extends a previous pilot study(Faruqui and Pad?, 2011).
It presents more an-notation, investigates a larger and better motivatedfeature set, and discusses the findings in detail.3 A Parallel Corpus of Literary TextsThis section discusses the construction of T/V goldstandard labels for English sentences.
We obtainthese labels from a parallel English?German cor-pus using the technique of annotation projection(Yarowsky and Ngai, 2001) sketched in Figure 1:We first identify the T/V status of German pro-nouns, then copy this T/V information onto thecorresponding English sentence.3.1 Data Selection and PreparationAnnotation projection requires a parallel corpus.We found commonly used parallel corpora like EU-ROPARL (Koehn, 2005) or the JRC Acquis corpus(Steinberger et al 2006) to be unsuitable for ourstudy since they either contain almost no directaddress at all or, if they do, just formal address (V).Fortunately, for many literary texts from the 19thand early 20th century, copyright has expired, andthey are freely available in several languages.We identified 110 stories and novels among thetexts provided by Project Gutenberg (English) andProject Gutenberg-DE (German)1 that were avail-able in both languages, with a total of 0.5M sen-tences per language.
Examples are Dickens?
DavidCopperfield or Tolstoy?s Anna Karenina.
We ex-cluded plays and poems, as well as 19th-centuryadventure novels by Sir Walter Scott and James F.Cooper which use anachronistic English for stylis-tic reasons, including words that previously (untilthe 16th century) indicated T (?thee?, ?didst?
).We cleaned the English and German novels man-ually by deleting the tables of contents, prologues,epilogues, as well as chapter numbers and titlesoccurring at the beginning of each chapter to ob-tain properly parallel texts.
The files were thenformatted to contain one sentence per line usingthe sentence splitter and tokenizer provided withEUROPARL (Koehn, 2005).
Blank lines wereinserted to preserve paragraph boundaries.
Allnovels were lemmatized and POS-tagged usingTreeTagger (Schmid, 1994).2 Finally, they weresentence-aligned using Gargantuan (Braune andFraser, 2010), an aligner that supports one-to-manyalignments, and word-aligned in both directionsusing Giza++ (Och and Ney, 2003).3.2 T/V Gold Labels for English UtterancesAs Figure 1 shows, the automatic construction ofT/V labels for English involves two steps.Step 1: Labeling German Pronouns as T/V.German has three relevant personal pronouns forthe T/V distinction: du (T), sie (V), and ihr (T/V).However, various ambiguities makes their interpre-tation non-straightforward.The pronoun ihr can both be used for plural Taddress or for a somewhat archaic singular or plu-ral V address.
In principle, these usages shouldbe distinguished by capitalization (V pronounsare generally capitalized in German), but manyT instances in our corpora informal use are nev-ertheless capitalized.
Additional, ihr can be the1http://www.gutenberg.org, http://gutenberg.spiegel.de/2It must be expected that the tagger degrades on thisdataset; however we did not quantify this effect.624dative form of the 3rd person feminine pronoun sie(she/her).
These instances are neutral with respectto T/V but were misanalysed by TreeTagger as in-stances of the T/V lemma ihr.
Since TreeTaggerdoes not provide person information, and we didnot want to use a full parser, we decided to omitihr/Ihr from consideration.3Of the two remaining pronouns (du and sie), duexpresses (singular) T. A minor problem is pre-sented by novels set in France, where du is used asan nobiliary particle.
These instances can be recog-nised reliably since the names before and after duare generally unknown to the German tagger.
Thuswe do not interpret du as T if the word precedingor succeeding it has ?unknown?
as its lemma.The V pronoun, sie, doubles as the pronoun forthird person (she/they) when not capitalized.
Wetherefore interpret only capitalized instances of Sieas V. Furthermore, we ignore utterance-initial po-sitions, where all words are capitalized.
This isdefined as tokens directly after a sentence bound-ary (POS $.)
or after a bracket (POS $().These rules concentrate on precision rather thanrecall.
They leave many instances of German sec-ond person pronouns unlabeled; however, this isnot a problem since we do not currently aim atobtaining complete coverage on the English sideof our parallel corpus.
From the 0.5M German sen-tences, about 14% of the sentences were labeledas T or V (37K for V and 28K for T).
In a randomsample of roughly 300 German sentences whichwe analysed, we did not find any errors.
This putsthe precision of our heuristics at above 99%.Step 2: Annotation Projection.
We now copythe information over onto the English side.
Weoriginally intended to transfer T/V labels betweenGerman and English word-aligned pronouns.
How-ever, we pronouns are not necessarily translatedinto pronouns; additionally, we found word align-ment accuracy for pronouns to be far from perfect,due to the variability in function word translation.For these reason, we decided to look at T/V labelsat the level of complete sentences, ignoring wordalignment.
This is generally unproblematic ?
ad-dress is almost always consistent within sentences:of the 65K German sentences with T or V labels,only 269 (< 0.5%) contain both T and V. Our pro-jection on the English side results in 25K V and3Instances of ihr as possessive pronoun occurred as well,but could be filtered out on the basis of the POS tag.Comparison No context In contextA1 vs. A2 75% (.49) 79% (.58)A1 vs. GS 60% (.20) 70% (.40)A2 vs. GS 65% (.30) 76% (.52)(A1 ?
A2) vs. GS 67% (.34) 79% (.58)Table 1: Manual annotation for T/V on a 200-sentencesample.
Comparison among human annotators (A1 andA2) and to projected gold standard (GS).
All cells showraw agreement and Cohen?s ?
(in parentheses).18K T sentences4, of which 255 (0.6%) are labeledas both T and V. We exclude these sentences.Note that this strategy relies on the direct cor-respondence assumption (Hwa et al 2005), thatis, it assumes that the T/V status of an utterance isnot changed in translation.
We believe that this isa reasonable assumption, given that T/V is deter-mined by the social relation between interlocutors;but see Section 4 for discussion.3.3 Data SplittingFinally, we divided our English data into train-ing, development and test sets with 74 novels(26K sentences), 19 novels (9K sentences) and13 novels (8K sentences), respectively.
The cor-pus is available for download at http://www.nlpado.de/~sebastian/data.shtml.4 Human Annotation of T/V for EnglishThis section investigates how well the T/V distinc-tion can be made in English by human raters, andon the basis of what information.
Two annotatorswith near native-speaker competence in Englishwere asked to label 200 random sentences fromthe training set as T or V. Sentences were first pre-sented in isolation (?no context?).
Subsequently,they were presented with three sentences pre- andpost-context each (?in context?
).Table 1 shows the results of the annotationstudy.
The first line compares the annotationsof the two annotators against each other (inter-annotator agreement).
The next two lines comparethe taggers?
annotations against the gold standardlabels projected from German (GS).
The last linecompares the annotator-assigned labels to the GSfor the instances on which the annotators agree.For all cases, we report raw accuracy and Co-hen?s ?
(1960), i.e.
chance-corrected agreement.4Our sentence aligner supports one-to-many alignmentsand often aligns single German to multiple English sentences.625We first observe that the T/V distinction is con-siderably more difficult to make for individualsentences (no context) than when the discourse isavailable.
In context, inter-annotator agreement in-creases from 75% to 79%, and agreement with thegold standard rises by 10%.
It is notable that thetwo annotators agree worse with one another thanwith the gold standard (see below for discussion).On those instances where they agree, Cohen?s ?reaches 0.58 in context, which is interpreted asapproaching good agreement (Fleiss, 1981).
Al-though far from perfect, this inter-annotator agree-ment is comparable to results for the annotationof fine-grained word sense or sentiment (Navigli,2009; Bermingham and Smeaton, 2009).An analysis of disagreements showed that manysentences can be uttered in both T and V contextsand cannot be labeled without context:(3) ?And perhaps sometime you may see her.
?This case (gold label: V) is disambiguated by theprevious sentence which indicates a hierarchicalsocial relation between speaker and addressee:(4) ?And she is a sort of relation of your lord-ship?s,?
said Dawson.
.
.
.Still, even a three-sentence window is often notsufficient, since the surrounding sentences may bejust as uninformative.
In these cases, more globalinformation about the situation is necessary.
Evenwith perfect information, however, judgments cansometimes deviate, as there are considerable ?greyareas?
in T/V usage (Kretzenbacher et al 2006).In addition, social rules like T/V usage varyin time and between countries (Sch?pbach et al2006).
This helps to explain why annotators agreebetter with one another than with the gold standard:21st century annotators tend to be unfamiliar with19th century T/V usage.
Consider this examplefrom a book written in second person perspective:(5) Finally, you acquaint Caroline with thefatal result: she begins by consoling you.
?One hundred thousand francs lost!
Weshall have to practice the strictest econ-omy?, you imprudently add.5Here, the author and translator use V to refer to thereader, while today?s usage would almost certainly5H.
de Balzac: Petty Troubles of Married Lifebe T, as presumed by both annotators.
Conver-sations between lovers or family members formanother example, where T is modern usage, butthe novels tend to use V:(6) [...] she covered her face with the otherto conceal her tears.
?Corinne!
?, said Os-wald, ?Dear Corinne!
My absence hasthen rendered you unhappy!
?6In sum, our annotation study establishes that theT/V distinction, although not realized by differentpronouns in English, can be recovered manuallyfrom text, provided that discourse context is avail-able.
A substantial part of the errors is due to socialchanges in T/V usage.5 Monolingual T/V ModelingThe second part of the paper explores the auto-matic prediction of the T/V distinction for Englishsentences.
Given the ability to create an Englishtraining corpus with T/V labels with the annotationprojection methods described in Section 3.2, wecan phrase T/V prediction for English as a standardsupervised learning task.
Our experiments havea twin motivation: (a), on the NLP side, we aremainly interested in obtaining a robust classifierto assign the labels T and V to English sentences;(b), on the sociolinguistic side, we are interested ininvestigating through which features the categoriesT and V are expressed in English.5.1 Classification FrameworkWe phrase T/V labeling as a binary classificationtask at the sentence level, performing the classifica-tion with L2-regularized logistic regression usingthe LibLINEAR library (Fan et al 2008).
Logis-tic regression defines the probability that a binaryresponse variable y takes some value as a logit-transformed linear combination of the features fi,each of which is assigned a coefficient ?i.p(y = 1) =11 + e?zwith z =?i?ifi (7)Regularization incorporates the size of the coef-ficient vector ?
into the objective function, sub-tracting it from the likelihood of the data given themodel.
This allows the user to trade faithfulnessto the data against generalization.76A.L.G.
de Sta?l: Corinne7We use LIBLINEAR?s default parameters and set thecost (regularization) parameter to 0.01.626p(C|V )p(C|T ) Words4.59 Mister, sir, Monsieur, sirrah, .
.
.2.36 Mlle., Mr., M., Herr, Dr., .
.
.1.60 Gentlemen, patients, rascals, .
.
.Table 2: 3 of the 400 clustering-based semantic classes(classes most indicative for V)5.2 Feature TypesWe experiment with three features types that arecandidates to express the T/V English distinction.Word Features.
The intuition to use word fea-tures draws on the parallel between T/V and infor-mation retrieval tasks like document classification:some words are presumably correlated with formaladdress (like titles), while others should indicateinformal address (like first names).
In a prelimi-nary experiment, we noticed that in the absence offurther constraints, many of the most indicative fea-tures are names of persons from particular novelswhich are systematically addressed formally (likePhileas Fogg from J. Vernes?
Around the world ineighty days) or informally (like Mowgli, Baloo,and Bagheera from R. Kipling?s Jungle Book).These features clearly do not generalize to newbooks.
We therefore added a constraint to removeall features which did not occur in at least threenovels.
To reduce the number of word features to areasonable order of magnitude, we also performeda ?2-based feature selection (Manning et al 2008)on the training set.
Preliminary experiments es-tablished that selecting the top 800 word featuresyielded a model with good generalization.Semantic Class Features.
Our second featuretype is semantic class features.
These can be seenas another strategy to counteract the sparsenessat the level of word features.
We cluster wordsinto 400 semantic classes on the basis of distribu-tional and morphological similarity features whichare extracted from an unlabeled English collec-tion of Gutenberg novels comprising more than100M tokens, using the approach by Clark (2003).These features measure how similar tokens are toone another in terms of their occurrences in thedocument and are useful in Named Entity Recog-nition (Finkel and Manning, 2009).
As featuresin the T/V classification of a given sentence, wesimply count for each class the number of tokensin this class present in the current sentence.
Forillustration, Table 2 shows the three classes mostindicative for V, ranked by the ratio of probabilitiesfor T and V, estimated on the training set.Politeness Theory Features.
The third featuretype is based on the Politeness Theory (Brownand Levinson, 1987).
Brown and Levinson?s pre-diction is that politeness levels will be detectablein concrete utterances in a number of ways, e.g.a higher use of conjunctive or hedges in politespeech.
Formal address (i.e., V as opposed to T) isone such expression.
Politeness Theory thereforepredicts that other politeness indicators should cor-relate with the T/V classification.
This holds inparticular for English, where pronoun choice isunavailable to indicate politeness.We constructed 16 features on the basis of Po-liteness Theory predictions, that is, classes of ex-pressions indicating either formality or informality.From a computational perspective, the problemwith Politeness Theory predictions is that they areonly described qualitatively and by example, with-out detailed lists.
For each feature, we manuallyidentified around 10 words or multi-word relevantexpressions.
Table 3 shows these 16 features withtheir intended classes and some example expres-sions.
Similar to the semantic class features, thevalue of each politeness feature is the sum of thefrequencies of its members in a sentence.5.3 Context: Size and TypeAs our annotation study in Section 4 found, con-text is crucial for human annotators, and this pre-sumably carries over to automatic methods humanannotators: if the features for a sentence are com-puted just on that sentence, we will face extremelysparse data.
We experiment with symmetrical win-dow contexts, varying the size between n = 0 (justthe target sentence) and n = 10 (target sentenceplus 10 preceding and 10 succeeding sentences).This kind of simple ?sentence context?
makes animportant oversimplification, however.
It lumps to-gether material from different speech turns as wellas from ?narrative?
sentences, which may generatemisleading features.
For example, narrative sen-tences may refer to protagonists by their full namesincluding titles (strong features for V) even whenthese protagonists are in T-style conversations:(8) ?You are the love of my life?, said SirPhileas Fogg.8 (T)8J.
Verne: Around the world in 80 days627Class Example expressions Class Example expressionsInclusion (T) let?s, shall we Exclamations (T) hey, yeahSubjunctive I (T) can, will Subjunctive II (V) could, wouldProximity (T) this, here Distance (V) that, thereNegated question (V) didn?t I, hasn?t it Indirect question (V) would there, is thereIndefinites (V) someone, something Apologizing (V) bother, pardonPolite adverbs (V) marvellous, superb Optimism (V) I hope, would youWhy + modal (V) why would(n?t) Impersonals (V) necessary, have toPolite markers (V) please, sorry Hedges (V) in fact, I guessTable 3: 16 Politeness theory-based features with intended classes and example expressionsExample (8) also demonstrates that narrative mate-rial and direct speech may even be mixed withinindividual sentences.For these reasons, we introduce an alternativeconcept of context, namely direct speech context,whose purpose is to exclude narrative material.
Wecompute direct speech context in two steps: (a),segmentation of sentences into chunks that areeither completely narrative or speech, and (b), la-beling of chunks with a classifier that distinguishesthese two classes.
The segmentation step (a) takesplace with a regular expression that subdivides sen-tences on every occurrence of quotes (?
, ?
, ?
, ?,etc.).
As training data for the classification step(b), we manually tagged 1000 chunks from ourtraining data as either B-DS (begin direct speech),I-DS (inside direct speech) and O (outside directspeech, i.e.
narrative material).9 We used thisdataset to train the CRF-based sequence taggerMallet (McCallum, 2002) using all tokens, includ-ing punctuation, as features.10 This tagger is usedto classify all chunks in our dataset, resulting inoutput like the following example:(9)(B-DS) ?I am going to see his Ghost!
(I-DS) It will be his Ghost not him!?
(O) Mr. Lorry quietly chafed thehands that held his arm.11Direct speech chunks belonging to the same sen-tence are subsequently recombined.We define the direct speech context of size n fora given sentence as the n preceding and followingdirect speech chunks that are labeled B-DS or I-DSwhile skipping any chunks labeled O.
Note thatthis definition of direct speech context still lumps9The labels are chosen after IOB notation conventions(Ramshaw and Marcus, 1995).10We also experimented with rule-based chunk labelingbased on quotes, but found the use of quotes too inconsistent.11C.
Dickens: A tale of two cities.???????
???
?0 2 4 6 8 1061626364656667Context size (n)Accuracy (%)??
????
?
?
?
?
?Figure 2: Accuracy vs. number of sentences in context(empty circles: sentence context; solid circles: directspeech context)together utterances by different speakers and cantherefore yield misleading features in the case ofasymmetric conversational situations, in additionto possible direct speech misclassifications.6 Experimental Evaluation6.1 Evaluation on the Development SetWe first perform model selection on the develop-ment set and then validate our results on the testset (cf.
Section 3.3).Influence of Context.
Figure 2 shows the influ-ence of size and type of context, using only wordsas features.
Without context, we obtain a perfor-mance of 61.1% (sentence context) and of 62.9%(direct speech context).
These numbers beat therandom baseline (50.0%) and the frequency base-line (59.1%).
The addition of more context furtherimproves performance substantially for both con-text types.
The ideal context size is fairly large,namely 7 sentences and 8 direct speech chunks, re-628Model AccuracyRandom Baseline 50.0Frequency Baseline 59.1Words 67.0?
?SemClass 57.5PoliteClass 59.6Words + SemClass 66.6?
?Words + PoliteClass 66.4?
?Words + PoliteClass + SemClass 66.2?
?Raw human IAA (no context) 75.0Raw human IAA (in context) 79.0Table 4: T/V classification accuracy on the develop-ment set (direct speech context, size 8).
??
: Significantdifference to frequency baseline (p<0.01)spectively.
This indicates that sparseness is indeeda major challenge, and context can become largebefore the effects mentioned in Section 5.3 counter-act the positive effect of more data.
Direct speechcontext outperforms sentence context throughout,with a maximum accuracy of 67.0% as comparedto 65.2%, even though it shows higher variation,which we attribute to the less stable nature of thedirect speech chunks and their automatically cre-ated labels.
From now on, we adopt a direct speechcontext of size 8 unless specified differently.Influence of Features.
Table 4 shows the resultsfor different feature types.
The best model (wordfeatures only) is highly significantly better thanthe frequency baseline (which it beats by 8%) asdetermined by a bootstrap resampling test (Noreen,1989).
It gains 17% over the random baseline,but is still more than 10% below inter-annotatoragreement in context, which is often seen as anupper bound for automatic models.Disappointingly, the comparison of the featuregroups yields a null result: We are not able toimprove over the results for just word features witheither the semantic class or the politeness features.Neither feature type outperforms the frequencybaseline significantly (p>0.05).
Combinations ofthe different feature types also do worse than justwords.
The differences between the best model(just words) and the combination models are allnot significant (p>0.05).
These negative resultswarrant further analysis.
It follows in Section 6.3.6.2 Results on the Test SetTable 5 shows the results of evaluating modelswith the best feature set and with different contextsizes on the test set, in order to verify that we didModel Accuracy ?
to dev setFrequency baseline 59.3 + 0.2Words (no context) 62.5 - 0.4Words (context size 6) 67.3 + 1.0Words (context size 8) 67.5 + 0.5Words (context size 10) 66.8 + 1.0Table 5: T/V classification accuracy on the test set anddifferences to dev set results (direct speech context)not overfit on the development set when pickingthe best model.
The tendencies correspond wellto the development set: the frequency baseline isalmost identical, as are the results for the differentmodels.
The differences to the development setare all equal to or smaller than 1% accuracy, andthe best result at 67.5% is 0.5% better than on thedevelopment set.
This is a reassuring result, as ourmodel appears to generalize well to unseen data.6.3 Analysis by Feature TypesThe results from Section 6.1 motivate further anal-ysis of the individual feature types.Analysis of Word Features.
Word features areby far the most effective features.
Table 6 liststhe top twenty words indicating T and V (rankedby the ratio of probabilities for the two classeson the training set).
The list still includes someproper names like Vrazumihin or Louis-Gaston(even though all features have to occur in at leastthree novels), but they are relatively infrequent.The most prominent indicators for the formal classV are titles (monsieur, (ma)?am) and instances offormulaic language (Permit (me), Excuse (me)).There are also some terms which are not straight-forward indicators of formal address (angelic, stub-bornness), but are associated with a high register.There is a notable asymmetry between T andV.
The word features for T are considerably moredifficult to interpret.
We find some forms of earlierperiod English (thee, hast, thou, wilt) that resultfrom occasional archaic passages in the novels aswell first names (Louis-Gaston, Justine).
Never-theless, most features are not straightforward toconnect to specifically informal speech.Analysis of Semantic Class Features.
Weranked the semantic classes we obtained by distri-butional clustering in a similar manner to the wordfeatures.
Table 2 shows the top three classes in-dicative for V. Almost all others of the 400 clustersdo not have a strong formal/informal association629Top 20 words for V Top 20 words for TWord w P (w|V )P (w|T ) Word wP (w|T )P (w|V )Excuse 36.5 thee 94.3Permit 35.0 amenable 94.3?ai 29.2 stuttering 94.3?am 29.2 guardian 94.3stubbornness 29.2 hast 92.0flights 29.2 Louis-Gaston 92.0monsieur 28.6 lease-making 92.0Vrazumihin 28.6 melancholic 92.0mademoiselle 26.5 ferry-boat 92.0angelic 26.5 Justine 92.0Allow 24.5 Thou 66.0madame 21.2 responsibility 63.8delicacies 21.2 thou 63.8entrapped 21.2 Iddibal 63.8lack-a-day 21.2 twenty-fifth 63.8ma 21.0 Chic 63.8duke 18.0 allegiance 63.8policeman 18.0 Jouy 63.8free-will 18.0 wilt 47.0Canon 18.0 shall 47.0Table 6: Most indicative word features for T or Vbut mix formal, informal, and neutral vocabulary.This tendency is already apparent in class 3: Gen-tlemen is clearly formal, while rascals is informal.patients can belong to either class.
Even in class1, we find Sirrah, a contemptuous term used in ad-dressing a man or boy with a low formality score(p(w|V )/p(w|T ) = 0.22).
From cluster 4 onward,none of the clusters is strongly associated with ei-ther V or T (p(c|V )/p(c|T ) ?
1).Our interpretation of these observations is thatin contrast to text categorization, there is no clear-cut topical or domain difference between T and V:both categories co-occur with words from almostany domain.
In consequence, semantic classes donot, in general, represent strong unambiguous indi-cators.
Similar to the word features, the situationis worse for T than for V: there still are reasonablystrong features for V, the ?marked?
case, but it ismore difficult to find indicators for T.Analysis of politeness features.
A major reasonfor the ineffectiveness of the Politeness Theory-based features seems to be their low frequency:in the best model, with a direct speech context ofsize 8, only an average of 7 politeness featureswas active for any given sentence.
However, fre-quency was not the only problem ?
the politenessfeatures were generally unable to discriminate wellbetween T and V. For all features, the values ofp(f |V )/p(f |T ) are between 0.9 and 1.3, that is,the features were only weakly indicative of one ofthe classes.
Furthermore, not all features turnedout to be indicative of the class we designed themfor.
The best indicator for V was the Indefinitesfeature (somehow, someone cf.
Table 3), as ex-pected.
In contrast, the best indicator for T was theNegation question feature which was supposedlyan indicator for V (didn?t I, haven?t we).A majority of politeness features (13 of the 16)had p(f |V )/p(f |T ) values above 1, that is, wereindicative for the class V. Thus for this feature type,like for the others, it appears to be more difficult toidentify T than to identify V. This negative resultcan be attributed at least in part to our method ofhand-crafting lists of expressions for these features.The inadvertent inclusion of overly general termsV might be responsible for the features?
inabilityto discriminate well, while we have presumablymissed specific terms which has hurt coverage.This situation may in the future be remedied withthe semi-automatic acquisition of instantiations ofpoliteness features.6.4 Analysis of Individual NovelsOne possible hypothesis regarding the difficultyof finding indicators for the class T is that indi-cators for T tend to be more novel-specific thanindicators for V, since formal language is moreconventionalized (Brown and Levinson, 1987).
Ifthis were the case, then our strategy of buildingwell-generalizing models by combining text fromdifferent novels would naturally result in modelsthat have problems with picking up T features.To investigate this hypothesis, we trained mod-els with the best parameters as before (8-sentencedirect speech context, words as features).
How-ever, this time we trained novel-specific models,splitting each novel into 50% training data and50% testing data.
We required novels to containmore than 200 labeled sentences.
This ruled outmost short stories, leaving us with 7 novels in thetest set.
The results are shown in Table 7 and showa clear improvement.
The accuracy is 13% higherthan in our main experiment (67% vs. 80%), eventhough the models were trained on considerablyless data.
Six of the seven novels perform abovethe 67.5% result from the main experiment.The top-ranked features for T and V show amuch higher percentage of names for both T andV than in the main experiment.
This is to be ex-630Novel AccuracyH.
Beecher-Stove: Uncle Tom?s Cabin 90.0J.
Spyri: Cornelli 88.3E.
Zola: Lourdes 83.9H.
de Balzac: Cousin Pons 82.3C.
Dickens: The Pickwick Papers 77.7C.
Dickens: Nicholas Nickleby 74.8F.
Hodgson Burnett: Little Lord 61.6All (micro average) 80.0Table 7: T/V prediction models for individual novels(50% of each novel for training and 50% testing)pected, since this experiment does not restrict itselfto features that occurred in at least three novels.The price we pay for this is worse generalization toother novels.
There is also still a T/V asymmetry:more top features are shared among the V lists ofindividual novels and with the main experimentV list than on the T side.
Like in the main exper-iment (cf.
Section 6.3), V features indicate titlesand other features of elevated speech, while T fea-tures mostly refer to novel-specific protagonistsand events.
In sum, these results provide evidencefor a difference in status of T and V.7 Discussion and ConclusionsIn this paper, we have studied the distinctionbetween formal and information (T/V) address,which is not expressed overtly through pronounchoice or morphosyntactic marking in modern En-glish.
Our hypothesis was that the T/V distinctioncan be recovered in English nevertheless.
Our man-ual annotation study has shown that annotators canin fact tag monolingual English sentences as T orV with reasonable accuracy, but only if they havesufficient context.
We exploited the overt informa-tion from German pronouns to induce T/V labelsfor English and used this labeled corpus to train amonolingual T/V classifier for English.
We exper-imented with features based on words, semanticclasses, and Politeness Theory predictions.With regard to our NLP goal of building a T/Vclassifier, we conclude that T/V classification isa phenomenon that can be modelled on the basisof corpus features.
A major factor in classifica-tion performance is the inclusion of a wide contextto counteract sparse data, and more sophisticatedcontext definitions improve results.
We currentlyachieve top accuracies of 67%-68%, which stillleave room for improvement.
We next plan tocouple our T/V classifier with a machine trans-lation system for a task-based evaluation on thetranslation of direct address into German and otherlanguages with different T/V pronouns.Considering our sociolinguistic goal of deter-mining the ways in which English realizes the T/Vdistinction, we first obtained a negative result: onlyword features perform well, while semantic classesand politeness features do hardly better than a fre-quency baseline.
Notably, there are no clear ?topi-cal?
divisions between T and V, like for examplein text categorization: almost all words are veryweakly correlated with either class, and seman-tically similar words can co-occur with differentclasses.
Consequently, distributionally determinedsemantic classes are not helpful for the distinction.Politeness features are difficult to operationalizewith sufficiently high precision and recall.An interesting result is the asymmetry betweenthe linguistic features for V and T at the lexicallevel.
V language appears to be more convention-alized; the models therefore identified formulaicexpressions and titles as indicators for V. On theother hand, very few such generic features exist forthe class T; consequently, the classifier has a hardtime learning good discriminating and yet genericfeatures.
Those features that are indicative of T,such as first names, are highly novel-specific andwere deliberately excluded from the main exper-iment.
When we switched to individual novels,the models picked up such features, and accuracyincreased ?
at the cost of lower generalizabilitybetween novels.
A more technical solution to thisproblem would be the training of a single-classclassifier for V, treating T as the ?default?
class(Tax and Duin, 1999).Finally, an error analysis showed that many er-rors arise from sentences that are too short or un-specific to determine T or V reliably.
This pointsto the fact that T/V should not be modelled as asentence-level classification task in the first place:T/V is not a choice made for each sentence, butone that is determined once for each pair of inter-locutors and rarely changed.
In future work, wewill attempt to learn social networks from novels(Elson et al 2010), which should provide con-straints on all instances of communication betweena speaker and an addressee.
However, the big ?
andunsolved, as far as we know ?
challenge is to au-tomatically assign turns to interlocutors, given thevaried and often inconsistent presentation of directspeech turns in novels.631ReferencesJohn Ardila.
2003.
(Non-Deictic, Socio-Expressive)T-/V-Pronoun Distinction in Spanish/English FormalLocutionary Acts.
Forum for Modern LanguageStudies, 39(1):74?86.John A. Bateman.
1988.
Aspects of clause politeness inJapanese: An extended inquiry semantics treatment.In Proceedings of ACL, pages 147?154, Buffalo,New York.Luisa Bentivogli and Emanuele Pianta.
2005.
Ex-ploiting parallel texts in the creation of multilingualsemantically annotated resources: the MultiSemCorCorpus.
Journal of Natural Language Engineering,11(3):247?261.Adam Bermingham and Alan F. Smeaton.
2009.
Astudy of inter-annotator agreement for opinion re-trieval.
In Proceedings of ACM SIGIR, pages 784?785.Philip Bramsen, Martha Escobar-Molano, Ami Patel,and Rafael Alonso.
2011.
Extracting social powerrelationships from natural language.
In Proceedingsof ACL/HLT, pages 773?782, Portland, OR.Fabienne Braune and Alexander Fraser.
2010.
Im-proved unsupervised sentence alignment for symmet-rical and asymmetrical parallel corpora.
In Coling2010: Posters, pages 81?89, Beijing, China.Roger Brown and Albert Gilman.
1960.
The pronounsof power and solidarity.
In Thomas A. Sebeok, edi-tor, Style in Language, pages 253?277.
MIT Press,Cambridge, MA.Penelope Brown and Stephen C. Levinson.
1987.
Po-liteness: Some Universals in Language Usage.
Num-ber 4 in Studies in Interactional Sociolinguistics.Cambridge University Press.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of EACL, pages 59?66, Bu-dapest, Hungary.J.
Cohen.
1960.
A Coefficient of Agreement for Nomi-nal Scales.
Educational and Psychological Measure-ment, 20(1):37?46.David Elson, Nicholas Dames, and Kathleen McKe-own.
2010.
Extracting social networks from literaryfiction.
In Proceedings of ACL, pages 138?147, Up-psala, Sweden.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Manaal Faruqui and Sebastian Pad?.
2011.
?I ThouThee, Thou Traitor?
: Predicting formal vs. infor-mal address in English literature.
In Proceedings ofACL/HLT 2011, pages 467?472, Portland, OR.Jenny Rose Finkel and Christopher D. Manning.
2009.Nested named entity recognition.
In Proceedings ofEMNLP, pages 141?150, Singapore.Joseph L. Fleiss.
1981.
Statistical methods for ratesand proportions.
John Wiley, New York, 2nd edi-tion.Alexander Fraser.
2009.
Experiments in morphosyn-tactic processing for translating to and from German.In Proceedings of the EACL MT workshop, pages115?119, Athens, Greece.Jerry Hobbs and Megumi Kameyama.
1990.
Trans-lation by abduction.
In Proceedings of COLING,pages 155?161, Helsinki, Finland.Rebecca Hwa, Philipp Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrap-ping parsers via syntactic projection across paralleltexts.
Journal of Natural Language Engineering,11(3):311?325.Hiroshi Kanayama.
2003.
Paraphrasing rules for au-tomatic evaluation of translation into Japanese.
InProceedings of the Second International Workshopon Paraphrasing, pages 88?93, Sapporo, Japan.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofthe 10th Machine Translation Summit, pages 79?86,Phuket, Thailand.Heinz L. Kretzenbacher, Michael Clyne, and DorisSch?pbach.
2006.
Pronominal Address in German:Rules, Anarchy and Embarrassment Potential.
Aus-tralian Review of Applied Linguistics, 39(2):17.1?17.18.Alexander K?nzli.
2010.
Address pronouns as a prob-lem in French-Swedish translation and translationrevision.
Babel, 55(4):364?380.Zhifei Li and David Yarowsky.
2008.
Mining andmodeling relations between formal and informal Chi-nese phrases from web corpora.
In Proceedings ofEMNLP, pages 1031?1040, Honolulu, Hawaii.Christopher D. Manning, Prabhakar Raghavan, andHinrich Sch?tze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press, Cambridge,UK, 1st edition.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Roberto Navigli.
2009.
Word Sense Disambiguation:a survey.
ACM Computing Surveys, 41(2):1?69.Eric W. Noreen.
1989.
Computer-intensive Methodsfor Testing Hypotheses: An Introduction.
John Wileyand Sons Inc.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51.Lance Ramshaw and Mitch Marcus.
1995.
Text chunk-ing using transformation-based learning.
In Proceed-ing of the 3rd ACL Workshop on Very Large Corpora,Cambridge, MA.Michael Schiehlen.
1998.
Learning tense transla-tion from bilingual corpora.
In Proceedings ofACL/COLING, pages 1183?1187, Montreal, Canada.632Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, pages 44?49, Manchester, UK.Doris Sch?pbach, John Hajek, Jane Warren, MichaelClyne, Heinz Kretzenbacher, and Catrin Norrby.2006.
A cross-linguistic comparison of address pro-noun use in four European languages: Intralingualand interlingual dimensions.
In Proceedings of theAnnual Meeting of the Australian Linguistic Society,Brisbane, Australia.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Toma?
Erjavec, and Dan Tufis.
2006.The JRC-Acquis: A multilingual aligned parallel cor-pus with 20+ languages.
In Proceedings of LREC,pages 2142?2147, Genoa, Italy.David M. J.
Tax and Robert P. W. Duin.
1999.
Sup-port vector domain description.
Pattern RecognitionLetters, 20:1191?1199.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual POS taggers and NP bracketers via robustprojection across aligned corpora.
In Proceedings ofNAACL, pages 200?207, Pittsburgh, PA.633
