Proceedings of the 7th Workshop on Statistical Machine Translation, pages 401?409,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsConstructing Parallel Corpora for Six Indian Languages via CrowdsourcingMatt Post?
and Chris Callison-Burch??
and Miles Osborne?
?Human Langage Technology Center of Excellence, Johns Hopkins University?Center for Language and Speech Processing, Johns Hopkins University?School of Informatics, University of EdinburghAbstractRecent work has established the efficacy ofAmazon?s Mechanical Turk for constructingparallel corpora for machine translation re-search.
We apply this to building a collec-tion of parallel corpora between English andsix languages from the Indian subcontinent:Bengali, Hindi, Malayalam, Tamil, Telugu,and Urdu.
These languages are low-resource,under-studied, and exhibit linguistic phenom-ena that are difficult for machine translation.We conduct a variety of baseline experimentsand analysis, and release the data to the com-munity.1 IntroductionThe quality of statistical machine translation (MT)systems is strongly related to the amount of paral-lel text available for the language pairs.
However,most language pairs have little or no readily availablebilingual training data available.
As a result, mostcontemporary MT research tends to opportunisti-cally focus on language pairs with large amounts ofparallel data.A consequence of this bias is that language ex-hibiting certain linguistic phenomena are underrep-resented, including languages with complex mor-phology and languages with divergent word order-ings.
In this paper, we describe our work gather-ing and refining document-level parallel corpora be-tween English and each of six verb-final languagesspoken on the Indian subcontinent: Bengali, Hindi,Malayalam, Tamil, Telugu, and Urdu.
This paper?scontributions are as follows:?
We apply an established protocol for usingAmazon?s Mechanical Turk (MTurk) to collectparallel data to train and evaluate translationsystems for six Indian languages.?
We investigate the relative performance of syn-tactic translation models over hierarchical ones,showing that syntax results in higher BLEUscores in most cases.?
We explore the impact of training data qualityon the quality of the resulting model.?
We release the corpora to the research commu-nity under the Creative Commons Attribution-Sharealike 3.0 Unported License (CC BY-SA3.0).12 Why Indian languages?Indian languages are important objects of study fora number of reasons.
These languages are low-resource languages in terms of the availability ofMT systems2 (and NLP tools in general) yet togetherthey represent nearly half a billion native speakers(Table 1).
Their speakers are well-educated, withmany of them speaking English either natively or as asecond language.
Together with the degree of Inter-net penetration in India, it is reasonably straightfor-ward to find and hire non-expert translators throughcrowdsourcing services like Amazon?s MechanicalTurk.1joshua-decoder.org/indian-parallel-corpora2See sampark.iiit.ac.in/sampark/web/index.php/content for a notable growing effort.401???????
????
????????????
????
?senator her remarks preparedFigure 1: An example of SOV word ordering in Tamil.Translation: The senator prepared her remarks.???
??
?
?
?walk CONT PAST 1pFigure 2: An example of the morphology of the Bengaliword ????????
?, meaning [I] was walking.
CONT denotesthe continuous aspect, while PAST denotes past tense.In addition to a general desire to collect suitabletraining corpora for low-resource languages, Indianlanguages demonstrate a variety of linguistic phe-nomena that are divergent from English and under-studied.
One example is head-finalness, exhibitedmost obviously in a subject-object-verb (SOV) pat-tern of sentence structure, in contrast to the gen-eral SVO ordering of English sentences.
One ofthe motivations underlying linguistically-motivatedsyntactic translation systems like GHKM (Galley etal., 2004; Galley et al, 2006) or SAMT (Zollmannand Venugopal, 2006) is to describe such transfor-mations.
This difference in word order has the po-tential to serve as a better test bed for syntax-basedMT3 compared to translating between English andEuropean languages, most of which largely share itsword order.
Figure 1 contains an example of SOVreordering in Tamil.A second important phenomenon present in theselanguages is a high degree of morphological com-plexity relative to English (Figure 2).
Indian lan-guages can be highly agglutinative, which meansthat words are formed by concatenating morpholog-ical affixes that convey information such as tense,person, number, gender, mood, and voice.
Mor-phological complexity is a considerable hindrance atall stages of the MT pipeline, but particularly align-ment, where inflectional variations mask patternsfrom alignment tools that treat words as atoms.3Weuse hierarchical to denote translation grammars that useonly a single nonterminal (Chiang, 2007), in contrast to syntac-tic systems, which make use of linguistic annotations (Zollmannand Venugopal, 2006; Galley et al, 2006).language script family L1Bengali ?????
Indo-Aryan 181MHindi ????
??????
Indo-Aryan 180MMalayalam ??????
Dravidian 35MTamil ?????
Dravidian 65MTelugu ??????
Dravidian 69MUrdu ????
Indo-Aryan 60MTable 1: Languages.
L1 is the worldwide number of na-tive speakers according to Lewis (2009).3 Data collectionThe source of the documents for our translation taskfor each of the languages in Table 1 was the set ofthe top-100 most-viewed documents from each lan-guage?s Wikipedia.
These lists were obtained us-ing page view statistics compiled from dammit.lt/wikistats over a one year period.
We did not applyany filtering for topic or content.
Table 2 containsa manually categorized list of documents for Hindi,with some minimal annotations indicating how thedocuments relate to those in the other languages.These documents constitute a diverse set of topics,including culture, the internet, and sex.We collected the parallel corpora using a three-step process designed to ensure the integrity of thenon-professional translations.
The first step was tobuild a bilingual dictionary (?3.1).
These dictionar-ies were used to bootstrap the experimental controlsin the collection of four translations of each sourcesentence (?3.2).
Finally, as a measure of data qual-ity, we independently collect votes on the which ofthe four redundant translations is the best (?3.3).3.1 DictionariesA key component of managing MTurk workers is toensure that they are competently and conscientiouslyundertaking the tasks.
As non-speakers of all of theIndian languages, we had no simple and scalable wayto judge the quality of the workers?
translations.
Oursolutionwas to bootstrap the process by first buildingbilingual dictionaries for each of the datasets.
Thedictionaries were then used to produce glosses of thecomplete source sentences, which we compared tothe translations produced by the workers as a roughmeans of manually gauging trust (?3.2).The dictionaries were built in a separate MTurk402PLACES PEOPLE PEOPLE TECHNOLOGY LANGUAGE AND RELIGIONAgra A. P. J. Abdul Kalam Premchand Blog CULTURE Bhagavad GitaBihar Aishwarya Rai Rabindranath Tagore Google Ayurveda DiwaliChina Akbar Rani Lakshmibai Hindi Web Resources Constitution of India HanumanDelhi Amitabh Bachchan Sachin Tendulkar Internet Cricket HinduismHimalayas Barack Obama Sarojini Naidu Mobile phone English language HinduismIndia Bhagat Singh Subhas Chandra Bose News aggregator Hindi Cable News HoliMumbai Dainik Jagran Surdas RSS Hindi literature IslamNepal Gautama Buddha Swami Vivekananda Wikipedia Hindi-Urdu grammar MahabharataPakistan Harivansh Rai Bachchan Tulsidas YouTube Horoscope PuranasRajasthan Indira Gandhi Indian cuisine QuranRed Fort Jaishankar Prasad THINGS SEX Sanskrit RamayanaTaj Mahal Jawaharlal Nehru Air pollution Anal sex Standard Hindi ShivaUnited States Kabir Earth Kama Sutra ShivaUttar Pradesh Kalpana Chawla Essay Masturbation EVENTS Taj Majal: Shiva Temple?Mahadevi Varma Ganges Penis History of India VedasMeera General knowledge Sex positions World War II VishnuMohammed Rafi Global warming Sexual intercourseMohandas Karamchand Gandhi Pollution VaginaMother Teresa Solar energyNavbharat Times TerrorismTable 2: The 100 most viewed Hindi Wikipedia articles (titles translated to English using inter-language links andGoogle translate and manually categorized).
Entries in bold were present in the top 100 lists of at least four of theIndian top 100 lists.
Earth, India,World War II, and Wikipedia were in the top 100 lists of all six languages.language entries translationsBengali 4,075 6,011Hindi - -Malayalam 41,502 144,505Tamil 11,592 69,128Telugu 12,193 38,532Urdu 26,363 113,911Table 3: Dictionary statistics.
Entries is the number ofsource-language types, while translations lists the num-ber of words or phrases they translated to (i.e., the num-ber of pairs in the dictionary).
Controls for Hindi wereobtained using Google translate, the only one of these lan-guages that were available at the outset of this project.task, in which workers were asked to translate sin-gle words and short phrases from the complete set ofWikipedia documents.
For each word, MTurk work-ers were presented with three sentences containingthat word, which provided context.
The control forthis task was obtained from the Wikipedia article ti-tles which are linked across languages, and can thusbe assumed to be translations of each other.
Workerswho performed too poorly on these known transla-tions had their work rejected.Table 3 lists the size of the dictionaries we con-structed.3.2 TranslationsWith the dictionaries in hand, we moved on to trans-late the entireWikipedia documents.
Each human in-telligence task (HIT) posted onMTurk contained tensequential source-language sentences from a doc-ument, and asked the worker to enter a free-formtranslation for each.
We collected four translationsfrom different translators for each source sentence.To discourage cheating through cutting-and-pastinginto automatic translation systems, sentences werepresented as images.
Workers were paid $0.70 perHIT.
We then manually determined whether to ac-cept or reject a worker?s HITs based on a review ofeach worker?s submissions, which included a com-parison of the translations to a monotonic gloss (pro-duced with the dictionary), the percentage of emptytranslations, the amount of time the worker took tocomplete the HIT, geographic location (self-reportedand geolocated by way of the worker?s IP address),and by comparing different translations of the samesource segments against one another.We obtained translations of the source-languagedocuments in a relatively short amount of time.
Fig-ure 3 depicts the number of translations collected asa function of the amount of time from the posting ofthe task.
Malayalam provided the highest through-put, generating half a million words in just under a403320 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30800,0000100,000200,000300,000400,000500,000600,000700,000MalayalamTamilTeluguHindiUrduBengaliFigure 3: The total volume of translations (measured inEnglish words) as a function of elapsed days.
For Malay-alam, we collected half a million words of translations injust under a week.week.
For comparison, the Europarl corpus (Koehn,2005) has about 50million words of English for eachof the Spanish and French parallel corpora.As has been previously reported (Zbib et al,2012), cost is another advantage of building train-ing data on Mechanical Turk.
Germann (2001) putsthe cost of professionally translated English at about$0.30 perword for translation fromTamil.
Our trans-lations were obtained for less than $0.01 per word.The rate of collection could likely be increased byraising these payments, but it is unclear whetherquality would be affected by raising the base pay(although it could be improved by paying for sub-sequent quality control HITs, like editing).The tradeoff for low-cost translations is increasedvariance in translation quality when compared to themore consistently-good professional translations.Figure 4 contains some hand-picked examples of thesorts of translations we obtained.
Later, in the Exper-iments section (?4), we will investigate the effectsthis variance in translation quality has on the qual-ity of the models that can be constructed.
For now,the variancemotivated the collection of an additionaldataset, described in the next section.3.3 VotesA prevailing issue with translations collected onMTurk is the prevalence of low-quality translations.Quality suffers for a variety of reasons: Turkerslack formal training, often translate into a nonna-tive tongue, may give insufficient attention to thetask, and likely desire to maximize their throughput(and thus their wage).
Unlike Zaidan and Callison-Burch (2011), who embed controls containing sourcelanguage sentences with known professional trans-lations, we had no professionally translated data.Therefore, we could not measure the BLEU score ofthe Turkers.Motivated by desire to have some measure of therelative quality and variance of the translations, wedesigned another task in which we presented an in-dependent set of Turkers with an original sentenceand its four translations, and asked them to vote onwhich was best.4 Five independent workers votedon the translations of each source sentence.
Tallyingthe resulting votes, we found that roughly 65% ofthe sentences had five votes cast on just one or twoof the translations, and about 95% of the sentenceshad all the votes cast on one, two, or three sentences.This suggests both (1) that there was a difference inthe quality of the translations, and (2) the voters wereable to discern these differences, and took their taskseriously enough to report them.3.4 Data setsFor each parallel corpus, we created a standardizedtest set in the following manner.
We first manu-ally assigned each of the Wikipedia documents foreach language into one of the following nine cate-gories: EVENTS, LANGUAGE AND CULTURE,PEOPLE, PLACES, RELIGION, SEX, TECHNOL-OGY, THINGS, or MISC.
We then assigned doc-uments to training, development, development test,and test sets in round-robin fashion using a ratio ofroughly 7:1:1:1.
For training data, each source sen-tence was repeated four times in order to allow itto be paired with each of its translations.
For thedevelopment and test sets, the multiple translationsserved as alternate references.
Table 4 lists sentence-and word-level statistics for the datasets for each lan-guage pair (these counts are prior to any tokeniza-tion).4We did not collect votes for Malayalam.404??????
15,2007???
????????????
??????
??????
????
??????
??????????
?.In March 15,2007 Wiki got a place in Oxford English dictionary.On March 15, 2007 wiki was included in the Oxford English dictionary.
(5)ON MARCH 15, 2007, WIKI FOUND A PLACE IN THE OXFORD ENGLISH DICTIONARYMarch 15, 2007 oxford english index of wiki?s place.Figure 4: An example of the variance in translation quality for the human translations of a Tamil sentence; the format-ting of the translations has been preserved exactly.
The parenthesized number indicates the number of votes receivedin the voting task (?3.3).language dict train dev devtest testBengali 16k 539k 63k 61k 69k6k 20k 914 907 1kHindi 0 1,249k 67k 98k 74k0 37k 1k 993 1kMalayalam 410k 664k 61k 68k 70k144k 29k 1k 1k 1kTamil 189k 747k 62k 53k 54k69k 35k 1k 1k 1kTelugu 106k 951k 52k 45k 49k38k 43k 1k 916 1kUrdu 253k 1,198k 67k 49k 42k113k 33k 736 777 605Table 4: Data set sizes for each language pair: words inthe first row, parallel sentences in the second.
(The dictio-naries contains short phrases in addition to words, whichaccounts for the difference in dictionary word and linecounts.
)4 ExperimentsIn this section, we present experiments on the col-lected data sets in order to quantify their perfor-mance.
The experiments aim to address the follow-ing questions:1.
How well can we translate the test sets?2.
Do linguistically motivated translation modelsimprove translation results?3.
What is the effect of data quality onmodel qual-ity?4.1 SetupA principal point of comparison in this paper is be-tween Hiero grammars (Chiang, 2007) and SAMTgrammars (Zollmann and Venugopal, 2006), the lat-ter of which make use of linguistic annotations toimprove nonterminal reordering.
These grammarswere trained with the Thrax grammar extractor us-ing its default settings, and translated using Joshua(Weese et al, 2011).
We tuned with minimum error-rate training (Och, 2003) using Z-MERT (Zaidan,2009) and present the mean BLEU score on testdata over three separate runs (Clark et al, 2011).MBR reranking (Kumar and Byrne, 2004) was ap-plied to Joshua?s 300-best (unique) output, and eval-uation was conducted with case-insensitive BLEUwith four references.The training data was produced by pairing asource sentence with each of its four translations.We also added the dictionaries to the training data.We built five-gram language models from the targetside of the training data using interpolated Kneser-Ney smoothing.
We also experimented with a larger-scale language model built from English Gigaword,but, notably, found a drop of over a point in BLEUscore.
This points forward to some of the difficul-ties encountered with the lack of text normalization,discussed in ?5.4.2 Baseline translationsWe begin by presenting BLEU scores for Hiero andSAMT translations of each of the six Indian languagetest sets (Table 5).
For comparison purposes, wealso present BLEU scores from Google translationsof these languages (where available).We observe that systems built with SAMT gram-mars improve measurably above the Hiero models,with the exception of Tamil and Telugu.
As an ex-ternal reference point, the Google baseline transla-tion scores far surpass the results of any of our sys-tems, but were likely constructed from much largerdatasets.Table 6 lists some manually-selected examples of405language Hiero SAMT diff GoogleBengali 12.72 13.53 +0.81 20.01Hindi 15.53 17.29 +1.76 25.21Malayalam 13.72 14.28 +0.56 -Tamil 9.81 9.85 +0.04 13.51Telugu 12.46 12.61 +0.15 16.03Urdu 19.53 20.99 +1.46 23.09Table 5: BLEU scores translating into English (four ref-erences).
BLEU scores are the mean of three MERT runs.the sorts of translations we obtained from our sys-tems.
While anecdotal and not characteristic of over-all quality, together with the generally good BLEUscores, these examples provide a measure of the abil-ity to obtain good translations from this dataset.4.3 Voted training dataWe noted above the high variance in the quality ofthe translations obtained on MTurk.
For data col-lection efforts, there is a question of how much timeand effort to invest in quality control, since it comesat the expense of simply collecting more data.
Wecan either collect additional redundant translations(to increase quality) or translate more foreign sen-tences (to increase coverage).To test this, we constructed two smaller datasets,each making use of only one of the four translationsof each source sentence:?
Selected randomly?
Selected by choosing the translation that re-ceived a plurality of the votes (?3.3), breakingties randomly (best)We again included the dictionaries in the trainingdata (where available).
Table 7 contains results onthe same test sets as before.
These results do notclearly indicate that quality control through redun-dant translations are worth the extra expense.
Novot-ney and Callison-Burch (2010) had a similar findingfor crowdsourced transcriptions.5 Further AnalysisThe previous section has shown that reasonableBLEU scores can be obtained from baseline transla-tion systems built from these corpora.
While trans-lation quality is an issue (for example, very lit-??????????
?????
????
?in srilanka solar governmentchola rule in sri lankain srilanka chozhas ruledchola reign in sri lankaFigure 5: An example of inconsistent orthography.
Wordsin bold are translations of the second Tamil word.eral translations, etc), the previous section?s voteddataset experiments suggest this is not one of themost important issues to address.In this section, we undertake a manual analysis ofthe collected datasets to inform future work.
Thereare a number of issues that arise due to non-Romanscripts, high-variance translation quality, and the rel-atively small amount of training data.5.1 Orthographic issuesManual analysis demonstrates that inconsistencieswith orthography are a serious problem.
An exam-ple of this can be found in Figure 5, which containsa set of translations of a Tamil sentence.
In particu-lar, the spelling of the Tamil word ?????
has threedifferent realizations among the sentence?s transla-tions.
The discrepancy between zha and la is dueto phonetic variants (phonetic similarity may alsoaccount for the word solar).
This discrepancy ispresent throughout the training and test data, wherethe -la variant is preferred to -zha by about 6:1 (thecounts are 848 and 142, respectively).In addition to mistakes potentially caused by for-eign scripts, there are many mistakes that are sim-ply spelling errors.
Table 8 contains examples ofmisspellings (along with their counts) in the train-ing portion of the Urdu-English dataset.
As a pointof comparison, there are no misspellings of the wordin Europarl.Such errors are present in many collections, ofcourse, but they are particularly harmful in smalldatasets, and they appear to be especially prevalentin datasets like these, translated as they were by non-native speakers.
Whether caused by Turker care-lessness or difficulty in translation from non-Romanscripts, these are common issues, solutions for whichcould yield significant improvement in translationperformance.406Bengali ??
?????
????
????
????
??????????????
???????
???
?Hiero in this time dhaka university was established on the year 1921 .SAMT in this time dhaka university was established in 1921 .Malayalam ?????????
????????????
??????????
?
??????
5 , 700 ?k ??????
???????????????
.Hiero the surface temperature of sun 5 , 700 degree k to down to .SAMT temperature in the surface of the sun 5 , 700 degree k to down to .Table 6: Some example translations.Hiero SAMTlanguage random best random bestBengali 9.43 9.29 9.65 9.50Hindi 11.74 12.18 12.61 12.69Tamil 7.73 7.48 7.88 7.76Telugu 10.49 10.61 10.75 10.72Urdu 13.51 14.26 14.63 16.03Table 7: BLEU scores translating into English on a quar-ter of the training data (plus dictionary), selected in twoways: best (result of vote), and random.
There is littledifference, suggesting quality control may not be terriblyimportant.
We did not collect votes for Malayalam.misspelling countjapenese 91japans 40japenes 9japenies 3japeneses 3japeneese 1japense 1Table 8: Misspellings of japanese (947) in the trainingportion of the Urdu-English data, along with their counts.5.2 AlignmentsInconsistent orthography fragments the trainingdata, exacerbating problems already present due tomorpohological richness.
One place this is mani-fested is during alignment, where different spellingsmask patterns from the standard alignment tech-niques.
We observe a large number of poor align-ments, due to interactions among these problems,as well as the small size of the training data, well-documented alignment mistakes (such as garbagecollecting), and the divergent sentence structures.
Inparticular, it seems that the defacto alignment heuris-tics may be particularly ill-suited to these languagepairs and data conditions.
Figure 6 (top) contains anexample of a particularly poor alignment producedby the default alignment heuristic, the grow-diag-and method described in Koehn et al (2003).As a means of testing this, we varied the align-ment combination heuristics using five alternativesdescribed in Koehn et al (2003) and available in thesymal program distributed with Moses (Koehn etal., 2007).
Experiments on Tamil produce a rangeof BLEU scores between 7.45 and 10.19 (each resultis the average of three MERT runs).
If we plot gram-mar size versus BLEU score, we observe a generaltrend that larger grammars seem to positively cor-relate with BLEU score.
We tested this more gen-erally across languages using the Berkeley aligner5(Liang et al, 2006) instead of GIZA alignments, andfound a consistent increase in BLEU score for theHiero grammars, often putting them on par with theoriginal SAMT results (Table 9).
Manual analysissuggests that the Berkeley aligner produces fewer,more reasonable-looking alignments than the Mosesheuristics (Figure 6).
This suggest a fruitful ap-proaches in revisiting assumptions underlying align-ment heuristics.6 Related WorkCrowdsourcing datasets has been found to be helpfulfor many tasks in natural language processing.
Ger-mann (2001) showed that humans could perform sur-prisingly well with very poor translations obtainedfrom non-expert translators, in part likely becausecoarse-level translational adequacy is sufficient forthe tasks they evaluated.
That work was also pitchedas a rapid resource acquisition task, meant to test ourability to quickly build systems in emergency set-tings.
This work further demonstrates the ability toquickly acquire training data for MT systems with5code.google.com/p/berkeleyaligner/407X?X X?X X?X?X XX??X??"#$??'()?+??./0??3???.aasaiwasthefirstsuccessfullmovieforajithkumar.?X????
?X X?aasaiwasthefirstsuccessfullmovieforajithkumar.?"#$??'()?+??./0??3??
?.Figure 6: A bad Tamil alignment produced with thegrow-diag-and alignment combination heuristic (top); theBerkeley aligner is better (bottom).
A ?
is a correctguess, an X marks a false positive, and a ?
denotes a falsenegative.
Hiero?s extraction heuristics yield 4 rules forthe top alignment and 16 for the bottom.reasonable translation accuracy.Closely related to our work here is that of Novot-ney and Callison-Burch (2010), who showed thattranscriptions for training speech recognition sys-tems could be obtained from Mechanical Turk withnear baseline recognition performance and at a sig-nificantly lower cost.
They also showed that redun-dant annotation was not worthwhile, and suggestedthat money was better spent obtaining more data.Separately, Ambati and Vogel (2010) probed theMTurk worker pool for workers capable of translat-ing a number of low-resource languages, includingHindi, Telugu, and Urdu, demonstrating that suchworkers could be found and quantifying acceptablegrammar sizepair GIZA++ Berkeley BLEU gainBengali 15m 27m 13.54 +0.82Hindi 34m 60m 16.47 +0.94Malayalam 12m 27m 12.70 -1.02Tamil 19m 30m 10.10 +0.29Telugu 28m 46m 13.36 +0.90Urdu 38m 58m 20.41 +0.88Table 9: Hiero translation results using Berkeley align-ments instead of GIZA++ heuristics.
The gain columnsdenotes improvements relative to the Hiero systems in Ta-ble 5.
In many cases (bold gains), the BLEU scores areat or above even the SAMT models from that table.wages and collection rates.The techniques described here are similar to thosedescribed in Zaidan and Callison-Burch (2011), whoshowed that crowdsourcing with appropriate qualitycontrols could be used to produce professional-leveltranslations for Urdu-English translation.
This pa-per extends that work by applying their techniquesto a larger set of Indian languages and scaling it totraining-data-set sizes.7 SummaryWe have described the collection of six parallel cor-pora containing four-way redundant translations ofthe source-language text.
The Indian languages ofthese corpora are low-resource and understudied,and exhibit markedly different linguistic propertiescompared to English.
We performed baseline exper-iments quantifying the translation performance of anumber of systems, investigated the effect of dataquality on model quality, and suggested a number ofapproaches that could improve the quality of modelsconstructed from the datasets.
The parallel corporaprovide a suite of SOV languages for translation re-search and experiments.Acknowledgments We thank Lexi Birch for dis-cussions about strategies for selecting and assem-bling the data sets.
This research was supported inpart by gifts from Google and Microsoft, the Euro-MatrixPlus project funded by the EuropeanCommis-sion (7th Framework Programme), and a DARPAgrant entitled ?Crowdsourcing Translation?.
Theviews in this paper are the authors?
alone.408ReferencesVamshi Ambati and Stephan Vogel.
2010.
Can crowdsbuild parallel corpora for machine translation systems?In Proceedings of the NAACL HLT 2010 Workshop onCreating Speech and Language Data with Amazon?sMechanical Turk, Los Angeles, California.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisticalmachine translation: Controlling for optimizer insta-bility.
In ACL, pages 176?181.
Association for Com-putational Linguistics.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc.NAACL, Boston, Massachusetts, USA, May.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.ACL, Sydney, Australia, July.Ulrich Germann.
2001.
Building a statistical ma-chine translation system from scratch: how muchbang for the buck can we expect?
In ACL work-shop on Data-driven methods in machine translation,Toulouse, France, July.
Association for ComputationalLinguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.NAACL, Edmonton, Alberta, Canada, May?June.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ond?ej Bojar, Alexandra Constantin,and Evan Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedings ofthe Annual Meeting of the ACL on Interactive Posterand Demonstration Sessions, Prague, Czech Republic,June.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
InMachine translationsummit.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine translation.In Proc.
NAACL, Boston, Massachusetts, USA, May.M.
Paul Lewis, editor.
2009.
Ethnologue: Languages ofthe World.
SIL International, Dallas, TX, USA, six-teenth edition.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In HLT-NAACL, pages 104?111.Association for Computational Linguistics.Scott Novotney and Chris Callison-Burch.
2010.
Cheap,fast and good enough: Automatic speech recognitionwith non-expert transcription.
In Proc.
NAACL, LosAngeles, California, June.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
ACL, Sapporo,Japan, July.JonathanWeese, Juri Ganitkevitch, Chris Callison-Burch,Matt Post, and Adam Lopez.
2011.
Joshua 3.0:Syntax-based machine translation with the thrax gram-mar extractor.
InProceedings of the SixthWorkshop onStatistical Machine Translation.Omar F. Zaidan and Chris Callison-Burch.
2011.
Crowd-sourcing translation: professional quality from non-professionals.
In Proc.
ACL, Portland, Oregon, USA,June.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.Rabih Zbib, Erika Malchiodi, Jacob Devlin, DavidStallard, Spyros Matsoukas, Richard Schwartz, JohnMakhoul, Omar F. Zaidan, and Chris Callison-Burch.2012.
Machine translation of arabic dialects.
In Proc.NAACL, Montreal, June.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of the Workshop on Statistical MachineTranslation, New York, New York, USA, June.409
