Efficient Confirmation Strategy for Large-scale Text RetrievalSystems with Spoken Dialogue InterfaceKazunori Komatani Teruhisa Misu Tatsuya Kawahara Hiroshi G. OkunoGraduate School of InformaticsKyoto UniversityYoshida-Hommachi, Sakyo, Kyoto 606-8501, Japan{komatani,kawahara,okuno}@i.kyoto-u.ac.jpAbstractAdequate confirmation for keywords is in-dispensable in spoken dialogue systemsto eliminate misunderstandings caused byspeech recognition errors.
Spoken lan-guage also inherently includes out-of-domain phrases and redundant expressionssuch as disfluency, which do not contributeto task achievement.
It is necessary toappropriately make confirmation for im-portant portions.
However, a set of key-words necessary to achieve the tasks can-not be predefined in retrieval for a large-scale knowledge base unlike conventionaldatabase query tasks.
In this paper, wedescribe two statistical measures for iden-tifying portions to be confirmed.
A rele-vance score represents the matching degreewith the target knowledge base.
A sig-nificance score detects portions that conse-quently affect the retrieval results.
Thesemeasures are defined based on informationthat is automatically derived from the tar-get knowledge base.
An experimental eval-uation shows that our method improved thesuccess rate of retrieval by generating con-firmation more efficiently than using a con-ventional confidence measure.1 IntroductionInformation retrieval systems with spoken lan-guage have been studied (Harabagiu et al, 2002;Hori et al, 2003).
They require both automaticspeech recognition (ASR) and information re-trieval (IR) technologies.
As a straight mani-festation to create these systems, we can thinkof using ASR results as an input for IR systemsthat retrieve a text knowledge base (KB).
How-ever, two problems occur in the characteristicsof speech.1.
Speech recognition errors2.
Redundancy included in spoken languageexpressionsOne is an ASR error, which is basically in-evitable in speech communications.
Therefore,an adequate confirmation is indispensable inspoken dialogue systems to eliminate the mis-understandings caused by ASR errors.If keywords to be confirmed are defined, thesystem can confirm them using confidence mea-sures (Komatani and Kawahara, 2000; Hazenet al, 2000) to manage the errors.
In con-ventional tasks for spoken dialogue systems inwhich their target of retrieval was well-defined,such as the relational database, keywords thatare important to achieve the tasks correspondto items in the relational database.
Most spo-ken dialogue systems that have been developed,such as airline information systems (Levin et al,2000; Potamianos et al, 2000; San-Segundo etal., 2000) and train information systems (Allenet al, 1996; Sturm et al, 1999; Lamel et al,1999), are categorized here.
However, it is notfeasible to define such keywords in retrieval foroperation manuals (Komatani et al, 2002) orWWW pages, where the target of retrieval isnot organized and is written as natural languagetext.Another problem is that a user?s utterancesmay include redundant expressions or out-of-domain phrases.
A speech interface has beensaid to have the advantage of ease of input.
Thismeans that redundant expressions, such as dis-fluency and irrelevant phrases, are easily input.These do not directly contribute to task achieve-ment and might even be harmful.
ASR resultsthat may include such redundant portions arenot adequate for an input of IR systems.A novel method is described in this paperthat automatically detects necessary portionsfor task achievement from the ASR results ofa user?s utterances; that is, the system deter-mines if each part of the ASR results is neces-sary for the retrieval.
We introduce two mea-sures for each portion of the results.
One is arelevance score (RS) with the target document [HOWTO] Use Speech Recognition inWindows XPThe information in this article applies to:?
Microsoft Windows XP Professional?
Microsoft Windows XP Home EditionSummary: This article describes how to usespeech recognition in Windows XP.
If youinstalled speech recognition with MicrosoftOffice XP, or if you purchased a new com-puter that has Office XP installed, you canuse speech recognition in all Office pro-grams as well as other programs for whichit is enabled.Detail information: Speech recognition en-ables the operating system to convert spo-ken words to written text.
An internaldriver, called a speech recognition engine,recognizes words and converts them totext.
The speech recognition engine ... Figure 1: Example of one article in databaseset.
The score is computed with a documentlanguage model and is used for making confir-mation prior to the retrieval.
The other is a sig-nificance score (SS) in the document matching.It is computed after the retrieval using N-bestresults and is used for prompting the user forpost-selection if necessary.
Information neces-sary to define these two measures, such as a doc-ument language model and retrieval results forN-best candidates of the ASR, can be automat-ically derived from the target knowledge base.Therefore, the system can detect the portionsnecessary for the retrieval and make the confir-mation efficiently without defining the keywordsmanually.2 Text Retrieval System forLarge-scale Knowledge BaseOur task involves text retrieval for a large-scale knowledge base.
As the target domain,we adopted a software support knowledge baseprovided by the Microsoft Corporation.
Theknowledge base consists of the following threecomponents: glossary, frequently asked ques-tions (FAQ), and a database of support articles.Figure 1 is an example of the database.
Theknowledge base is very large-scale, as shown inTable 1.The Dialog Navigator (Kiyota et al, 2002)was developed in the University of Tokyo as aTable 1: Document set (Knowledge base)Text collection # of texts # of charactersGlossary 4,707 700,000FAQ 11,306 6,000,000Support articles 23,323 22,000,000text retrieval system for this knowledge base.The system accepts a typed-text input as ques-tions from users and outputs a result of the re-trieval.
The system interprets input sentencestaking a syntactic dependency and synonymousexpression into consideration for matching itwith the knowledge base.
The system can alsonavigate for the user when he/she makes vaguequestions based on scenarios (dialog card) thatwere described manually beforehand.
Hundredsof the dialog cards have been prepared to askquestions back to the users.
If a user questionmatches its input part, the system generates aquestion based on its description.We adopted the Dialog Navigator as a back-end system and constructed a text retrieval sys-tem with a spoken dialogue interface.
We theninvestigated a confirmation strategy to interpretthe user?s utterances robustly by taking into ac-count the problems that are characteristic ofspoken language, as previously described.3 Confirmation Strategy usingRelevance Score and SignificanceScoreMaking confirmations for every portion that hasthe possibility to be an ASR error is tedious.This is because every erroneous portion doesnot necessarily affect the retrieval results.
Wetherefore take the influence of recognition er-rors for retrieval into consideration, and controlgeneration of confirmation.We make use of N-best results of the ASRfor the query and test if a significant differenceis caused among N-best sets of retrieved can-didates.
If there actually is, we then make aconfirmation on the portion that makes the dif-ference.
This is regarded as a posterior confir-mation.
On the other hand, if a critical erroroccurs in the ASR result, such as those in theproduct name in software support, the follow-ing retrieval would make no sense.
Therefore,we also introduce a confirmation prior to theretrieval for critical words.The system flow including the confirmation issummarized below.1.
Recognize a user?s utterance.Speech inputSystem UserASR(N-best candidates)Calculation ofrelevance scoreLanguage modelfor ASRLanguage modeltrained with KBConfirmation usingrelevance scoreCriticalwordsConfirmation forinfluential wordsReply or rephraseMatching with KB withweighting by relevance scoreretrievalresultretrievalresultretrievalresultConfirmation using significance scoreFinal resultDisplay the resultConfirmation for differencebetween candidatesReply or rephraseDialogNavigator(text retrieval)Target textKB TFIDFFigure 2: System flow2.
Calculate a relevance score for each phraseof ASR results.3.
Make a confirmation for critical words witha low relevance score.4.
Retrieve the knowledge base using the Dia-log Navigator for N-best candidates of theASR.5.
Calculate significance scores and generatea confirmation based on them.6.
Show the retrieval results to the user.This flow is also shown in Figure 2 and ex-plained in the following subsections in detail.3.1 Definition of Relevance ScoreWe use test-set perplexity for each portion ofthe ASR results as one of the criteria in deter-mining whether the portion is influential or notfor the retrieval.
The language model to cal-culate the perplexity was trained only with thetarget knowledge base.
It is different from thatused in the ASR.The perplexity is defined as an exponential ofentropy per word, and it represents the averagenumber of the next words when we observe aword sequence.
The perplexity can be denotedas the following equation because we assume anergodicity on language and use a trigram as alanguage model.log PP = ?
1n?klog P (wk|wk?1, wk?2)This perplexity PP represents the degree thata given word sequence, w1w2 ?
?
?wn, matchesthe knowledge base with which the languagemodel P (wn|wn?1, wn?2) was trained.
If theperplexity is small, it indicates the sequence ap-pears frequently in the knowledge base.
On thecontrary, the perplexity for a portion includingthe ASR errors increases because it is contex-tually less frequent.
The perplexity for out-of-domain phrases similarly increases because theyscarcely appear in the knowledge base.
It en-ables us to detect a portion that is not influen-tial for retrieval or those portions that includeASR errors.
Here, a phrase, called bunsetsu1in Japanese, is adopted as a portion for whichthe perplexity is calculated.
We use a syntac-tic parser KNP (Kurohashi and Nagao, 1994)to divide the ASR results into the phrases2.1Bunsetsu is a commonly used linguistic unit inJapanese, which consists of one or more content wordsand zero or more functional words that follow.2As the parser was designed for written language, thedivision often fails for portions including ASR errors.The division error, however, does not affect the wholesystem?s performance because the perplexity for the er-roneous portions increases, indicating they are irrelevant. User utterance:?Atarashiku katta XP no pasokon de fax kinouwo tsukau niha doushitara iidesu ka??
(Please tell me how to use the facsimile func-tion in the personal computer with WindowsXP that I recently bought.
)Speech recognition result:?Atarashiku katta XP no pasokon de fax kinouwo tsukau ni sono e ikou??
[The underlined part was incorrectly recognizedhere.
]Division into phrases (bunsetsu):?Atarashiku / katta / XP no / pasokon de / faxkinou wo / tsukau ni / sono / e / ikou?
?Calculation of perplexity:phrases (their context) PP RS(<S>) Atarashiku (katta) 499.57 0.86(atarashiku) katta (XP) 2079.83 0.47(katta) XP no (pasokon) 105.64 0.99(no) pasokon de (FAX) 185.92 0.95(de) FAX kinou wo (tsukau) 236.23 0.89(wo) tsukau ni (sono) 98.40 0.99(ni) sono (e) 1378.72 0.62(sono) e (ikou) 144.58 0.96(e) ikou (</S>) 27150.00 0.00<S>, </S> denote the beginning and end of a sen-tence. Figure 3: Example of calculating perplexity(PP ) and relevance score (RS)We then calculate the perplexity for thephrases (bunsetsu) to which the preceding andfollowing words are attached.
We then definethe relevance score by applying a sigmoid-liketransform to the perplexity using the followingequation.
Thus, the score ranges between 0 and1 by the transform and can be used as a weightfor each bunsetsu.RS =11 + exp(?
?
(log PP ?
?
))Here, ?
and ?
are constants and are empiri-cally set to 2.0 and 11.0.
An example of calcu-lating the relevance score is shown in Figure 3.In this sample, a portion, ?Atarashiku katta (=that I bought recently)?, that appeared in thebeginning of the utterance does not contributeto any retrieval.
A portion at the end of the sen-tence was incorrectly recognized because it mayhave been pronounced weakly.
The perplexityfor these portions increases as a result, and therelevance score correspondingly decreases.3.2 Confirmation for Critical Wordsusing Relevance ScoreCritical words should be confirmed before theretrieval.
This is because a retrieval result willbe severely damaged if the portions are not cor-rectly recognized.
We define a set of words thatare potentially critical using tf?idf values, whichare often used in information retrieval.
Theycan be derived from the target knowledge baseautomatically.
We regard a word with the max-imum tf?idf values in each document as beingits representative, and the words that are rep-resentative in more documents are regarded asbeing more important.
When the amount ofdocuments represented by the more importantwords exceeds 10% out of the whole number ofdocuments, we define a set of the words as beingcritical.
As a result, 35 words were selected aspotentially critical ones in the knowledge base,such as ?set up?, ?printer?, and ?
(Microsoft) Of-fice?.We use the relevance score to determinewhether we should make a confirmation for thecritical words.
If a critical word is containedin a phrase whose relevance score is lower thanthreshold ?, the system makes a confirmation.We set threshold ?
through the preliminary ex-periment.
The confirmation is done by present-ing the recognition results to the user.
Users caneither confirm or discard or correct the phrasebefore passing it to the following matching mod-ule.3.3 Weighted Matching usingRelevance ScoreA phrase that has a low relevance score is likelyto be an ASR error or a portion that does notcontribute to retrieval.
We therefore use the rel-evance score RS as a weight for phrases duringthe matching with the knowledge base.
This re-lieves damage to the retrieval by ASR errors orredundant expressions.3.4 Significance Score using RetrievalResultsThe significance score is defined by using pluralretrieval results corresponding to N-best candi-dates of the ASR.
Ambiguous portions duringthe ASR appear as the differences between theN-best candidates.
The score represents the de-gree to which the portions are influential.The significance score is calculated for por-tions that are different among N-best candi-dates.
We define the significance score SS(n,m)as the difference between the retrieval results ofn-th and m-th candidates.
The value is definedby the equation,SS(n,m) = 1 ?
|res(n) ?
res(m)|2|res(n)||res(m)| .Here, res(n) denotes a set of retrieval resultsfor the n-th candidate, and |res(n)| denotes thenumber of elements in the set.
That is, the sig-nificance score decreases if the retrieval resultshave a large common part.Figure 4 has an example of calculating thesignificance score.
In this sample, the portionsof ?suuzi (numerals)?
and ?suushiki (numeralexpressions)?
differ between the first and sec-ond candidates of the ASR.
As the retrieval re-sults for each candidate, 14 and 15 items areobtained, respectively.
The number of com-mon items between the two retrieval results is8.
Then, the significance score for the portionis 0.70 by the above equation.3.5 Confirmation using SignificanceScoreThe confirmation is also made for the portionsdetected by the significance score.
If the scoreis higher than a threshold, the system makesthe confirmation by presenting the difference tousers3.
Here, we set the number of N-best can-didates of the ASR to 3, and the threshold forthe score is set to 0.5.In the confirmation phrase, if a user selectsfrom the list, the system displays the corre-sponding retrieval results.
If the score is lowerthan the threshold, the system does not makethe confirmation and presents retrieval resultsof the first candidate of the ASR.
If a userjudges all candidates as inappropriate, the sys-tem rejects the current candidates and promptshim/her to utter the query again.4 Experimental EvaluationWe implemented and evaluated our method asa front-end of Dialog Navigator.
The front-endworks on a Web browser, Internet Explorer 6.0.Julius (Lee et al, 2001) for SAPI4 was used as aspeech recognizer on PCs.
The system presentsa confirmation to users on the display.
He or shereplies to the confirmation by selecting choiceswith the mouse.3Confirmation will be generated practically if one ofthe significance scores between the first candidate andothers exceeds the threshold.4http://julius.sourceforge.jp/sapi/ [#1 candidate of ASR]?WORD2002 de suuzi wo nyuryoku suruhouhou wo oshiete kudasai.?
(Please tell methe way to input numerals in WORD 2002.
)Retrieval results (# of the results was 14.)1.
Input the present date and time in Word2.
WORD: Add a space between Japanese andalphanumeric characters3.
WORD: Check the form of inputted char-acters4.
WORD: Input a handwritten signature5.
WORD: Put watermark characters into thebackground of a character6.
...[#2 candidate of ASR]?WORD2002 de suushiki wo nyuryoku suruhouhou wo oshiete kudasai.?
(Please tellme the way to input numerical expressions inWORD 2002.
)Retrieval results (# of the results was 15.)1.
Insert numerical expressions in Word2.
Input the present date and time in Word3.
Input numerical expressions in Spreadsheet4.
Input numerical expressions in PowerPoint5.
Input numerical expressions in Excel6.
...Significance scoreSS(1, 2) = 1 ?
8214?15 = 0.70(# of common items in the retrieval resultswas 8.
) Figure 4: Example of calculating significancescoreWe collected the test data by 30 subjects whohad not used our system.
Each subject was re-quested to retrieve support information for 14tasks, which consisted of 11 prepared scenarios(query sentences are not given) and 3 sponta-neous queries.
Subjects were allowed to utterthe sentence again up to 3 times per task if a rel-evant retrieval result was not obtained.
We ob-tained 651 utterances for 420 tasks in total.
Theaverage word accuracy of the ASR was 76.8%.4.1 Evaluation of Success Rate ofRetrievalWe calculated the success rates of retrieval forthe collected speech data.
We regarded the re-trieval as having succeeded when the retrievalresults contained an answer for the user?s initialquestion.
We set three experimental conditions:Table 2: Comparison of success rate of retrieval with method using only ASR results# utterances Transcription ASR results Our method651 520 421 457(79.9%) (64.7%) (70.2%)1.
Transcription: A correct transcription ofuser utterances, which was made manually,was used as an input to the Dialog Naviga-tor.
This condition corresponds to a case of100% ASR accuracy, indicating an utmostperformance obtained by improvements inthe ASR and our dialogue strategy.2.
ASR results: The first candidate of theASR was used as an input (baseline).3.
Our method: The N-best candidates of theASR were used as an input, and confirma-tion was generated based on our methodusing both the relevance and significancescores.
It was assumed that the usersresponded appropriately to the generatedconfirmation.Table 2 lists the success rate.
The rate whenthe transcription was used as the input was79.9%.
The remaining errors included thosecaused by irrelevant user utterances and thosein the text retrieval system.
Our method at-tained a better success rate than the conditionwhere the first candidate of the ASR was used.Improvement of 36 cases (5.5%) was obtained byour method, including 30 by the confirmationsand 14 by weighting during the matching usingthe relevance score, though the retrieval failedeight times as side effects of the weighting.We further investigated the results shown inTable 2.
Table 3 lists the relations between thesuccess rate of the retrieval and the accuracyof the ASR per utterance.
The improvementrate out of the number of utterances was ratherhigh between 40% and 60%.
This means thatour method was effective not only for utteranceswith high ASR accuracy but also for those witharound 50% accuracy.
That is, an appropriateconfirmation was generated even for utteranceswhose ASR accuracy was not very high.4.2 Evaluation of ConfirmationEfficiencyWe also evaluated our method from the numberof generated confirmations.
Our method gener-ated 221 confirmations.
This means that con-firmations were generated once every three ut-terances on the average.
The 221 confirmationsconsisted of 66 prior to the retrieval using therelevance score and 155 posterior to the retrievalusing the significance score.We compared our method with a conventionalone, which used a confidence measure (CM)based on N-best candidates of the ASR (Ko-matani and Kawahara, 2000)5.
In this method,the system generated confirmation only for con-tent words with a confidence measure lowerthan ?1.
The thresholds to generate confirma-tion (?1) were set to 0.4, 0.6, and 0.8.
If a con-tent word that was confirmed was rejected bythe user, the retrieval was executed after remov-ing a phrase that included it.The number of confirmations and retrievalsuccesses are shown in Table 4.
Our methodachieved a higher success rate with a less num-ber of confirmations (less than half) comparedwith the case of ?1 = 0.8 in the conventionalmethod.
Thus, the generated confirmationsbased on the two scores were more efficient.The confidence measure used in the conven-tional method only reflects the acoustic andlinguistic likelihood of the ASR results.
Ourmethod, however, reflects the domain knowl-edge because the two scores are derived by ei-ther a language model trained with the targetknowledge base or by retrieval results for theN-best candidates.
The domain knowledge canbe introduced without any manual deliberation.The experimental results show that the scoresare appropriate to determine whether a confir-mation should be generated or not.5 ConclusionWe described an appropriate confirmation strat-egy for large-scale text retrieval systems with aspoken dialogue interface.
We introduced twomeasures, relevance score and significance score,for ASR results.
The measures are useful to con-trol confirmation efficiently for portions includ-ing either ASR errors or redundant expressions.The portions to be confirmed are determined5We used a word-level CM only because defining se-mantic categories for content words is required to cal-culate the concept-level CM.
Because the semantic cate-gory corresponded to items in a relational database, wecannot use the concept-level CM in this task.Table 3: Success rate of retrieval per ASR accuracyASR accuracy (%) # utterances ASR results Our method # improvement?40 37 9 11 2 ( 5.4%)?60 73 33 42 9 (12.3%)?80 194 116 129 13 ( 6.7%)?100 347 263 275 12 ( 3.5%)Total 651 421 457 36 ( 5.5%)Table 4: Comparison with method using confidence measure (CM)Our method CM (?1 = 0.4) CM (?1 = 0.6) CM (?1 = 0.8)# confirmation 221 77 254 484# success (success rate) 457 (70.2%) 427 (65.6%) 435 (66.8%) 445 (68.4%)using information that is automatically derivedfrom the target knowledge base, such as a statis-tical language model, tf?idf values, and retrievalresults.
An experimental evaluation shows thatour method can efficiently generate confirma-tions for better task achievement compared withthat using a conventional confidence measure ofthe ASR.
Our method is not dependent on thesoftware support task, and expected to be ap-plicable to general text retrieval tasks.6 AcknowledgmentsThe authors are grateful to Prof. Kurohashi andMr.
Kiyota at the University of Tokyo and Ms.Kido at Microsoft Corporation for their helpfuladvice.ReferencesJ.
F. Allen, B. W. Miller, E. K. Ringger, andT.
Sikorski.
1996.
A robust system for natu-ral spoken dialogue.
In Proc.
of the 34th An-nual Meeting of the ACL, pages 62?70.S.
Harabagiu, D. Moldovan, and J. Picone.2002.
Open-domain voice-activated questionanswering.
In Proc.
COLING, pages 502?508.T.
J. Hazen, T. Burianek, J. Polifroni, andS.
Seneff.
2000.
Integrating recognition con-fidence scoring with language understandingand dialogue modeling.
In Proc.
ICSLP.C.
Hori, T. Hori, H. Isozaki, E. Maeda, S. Kata-giri, and S. Furui.
2003.
Deriving disambigu-ous queries in a spoken interactive ODQAsystem.
In Proc.
IEEE-ICASSP.Y.
Kiyota, S. Kurohashi, and F. Kido.
2002.?Dialog Navigator?
: A question answeringsystem based on large text knowledge base.In Proc.
COLING, pages 460?466.K.
Komatani and T. Kawahara.
2000.
Flexiblemixed-initiative dialogue management usingconcept-level confidence measures of speechrecognizer output.
In Proc.
COLING, pages467?473.K.
Komatani, T. Kawahara, R. Ito, and H. G.Okuno.
2002.
Efficient dialogue strategy tofind users?
intended items from informationquery results.
In Proc.
COLING, pages 481?487.S.
Kurohashi and M. Nagao.
1994.
A syntacticanalysis method of long Japanese sentencesbased on the detection of conjunctive struc-tures.
Computational Linguistics, 20(4):507?534.L.
F. Lamel, S. Rosset, J-L. S. Gauvain, andS.
K. Bennacef.
1999.
The LIMSI ARISEsystem for train travel information.
In Proc.IEEE-ICASSP.A.
Lee, T. Kawahara, and K. Shikano.
2001.Julius ?
an open source real-time large vo-cabulary recognition engine.
In Proc.
EU-ROSPEECH, pages 1691?1694.E.
Levin, S. Narayanan, R. Pieraccini, K. Bia-tov, E. Bocchieri, G. Di Fabbrizio, W. Eck-ert, S. Lee, A. Pokrovsky, M. Rahim, P. Rus-citti, and M. Walker.
2000.
The AT&T-DARPA communicator mixed-initiative spo-ken dialogue system.
In Proc.
ICSLP.A.
Potamianos, E. Ammicht, and H.-K. J. Kuo.2000.
Dialogue management in the Bell labscommunicator system.
In Proc.
ICSLP.R.
San-Segundo, B. Pellom, W. Ward, andJ.
Pardo.
2000.
Confidence measures for dia-logue management in the CU communicatorsystem.
In Proc.
IEEE-ICASSP.J.
Sturm, E. Os, and L. Boves.
1999.
Issues inspoken dialogue systems: Experiences withthe Dutch ARISE system.
In Proc.
ESCAworkshop on Interactive Dialogue in Multi-Modal Systems.
