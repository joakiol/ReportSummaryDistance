Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 360?368,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPSemi-supervised Learning of Dependency Parsersusing Generalized Expectation CriteriaGregory DruckDept.
of Computer ScienceUniversity of MassachusettsAmherst, MA 01003gdruck@cs.umass.eduGideon MannGoogle, Inc.76 9th Ave.New York, NY 10011gideon.mann@gmail.comAndrew McCallumDept.
of Computer ScienceUniversity of MassachusettsAmherst, MA 01003mccallum@cs.umass.eduAbstractIn this paper, we propose a novel methodfor semi-supervised learning of non-projective log-linear dependency parsersusing directly expressed linguistic priorknowledge (e.g.
a noun?s parent is often averb).
Model parameters are estimated us-ing a generalized expectation (GE) objec-tive function that penalizes the mismatchbetween model predictions and linguisticexpectation constraints.
In a comparisonwith two prominent ?unsupervised?
learn-ing methods that require indirect biasingtoward the correct syntactic structure, weshow that GE can attain better accuracywith as few as 20 intuitive constraints.
Wealso present positive experimental resultson longer sentences in multiple languages.1 IntroductionEarly approaches to parsing assumed a grammarprovided by human experts (Quirk et al, 1985).Later approaches avoided grammar writing bylearning the grammar from sentences explicitlyannotated with their syntactic structure (Black etal., 1992).
While such supervised approaches haveyielded accurate parsers (Charniak, 2001), thesyntactic annotation of corpora such as the PennTreebank is extremely costly, and consequentlythere are few treebanks of comparable size.As a result, there has been recent interest inunsupervised parsing.
However, in order to at-tain reasonable accuracy, these methods have tobe carefully biased towards the desired syntac-tic structure.
This weak supervision has beenencoded using priors and initializations (Kleinand Manning, 2004; Smith, 2006), specializedmodels (Klein and Manning, 2004; Seginer,2007; Bod, 2006), and implicit negative evi-dence (Smith, 2006).
These indirect methods forleveraging prior knowledge can be cumbersomeand unintuitive for a non-machine-learning expert.This paper proposes a method for directly guid-ing the learning of dependency parsers with nat-urally encoded linguistic insights.
Generalizedexpectation (GE) (Mann and McCallum, 2008;Druck et al, 2008) is a recently proposed frame-work for incorporating prior knowledge into thelearning of conditional random fields (CRFs) (Laf-ferty et al, 2001).
GE criteria express a preferenceon the value of a model expectation.
For example,we know that ?in English, when a determiner is di-rectly to the left of a noun, the noun is usually theparent of the determiner?.
With GE we may adda term to the objective function that encourages afeature-rich CRF to match this expectation on un-labeled data, and in the process learn about relatedfeatures.
In this paper we use a non-projective de-pendency tree CRF (Smith and Smith, 2007).While a complete exploration of linguistic priorknowledge for dependency parsing is beyond thescope of this paper, we provide several promis-ing demonstrations of the proposed method.
Onthe English WSJ10 data set, GE training outper-forms two prominent unsupervised methods usingonly 20 constraints either elicited from a humanor provided by an ?oracle?
simulating a human.We also present experiments on longer sentencesin Dutch, Spanish, and Turkish in which we obtainaccuracy comparable to supervised learning withtens to hundreds of complete parsed sentences.2 Related WorkThis work is closely related to the prototype-driven grammar induction method of Haghighiand Klein (2006), which uses prototype phrasesto guide the EM algorithm in learning a PCFG.Direct comparison with this method is not possi-ble because we are interested in dependency syn-tax rather than phrase structure syntax.
However,the approach we advocate has several significant360advantages.
GE is more general than prototype-driven learning because GE constraints can be un-certain.
Additionally prototype-driven grammarinduction needs to be used in conjunction withother unsupervised methods (distributional simi-larity and CCM (Klein and Manning, 2004)) toattain reasonable accuracy, and is only evaluatedon length 10 or less sentences with no lexical in-formation.
In contrast, GE uses only the providedconstraints and unparsed sentences, and is used totrain a feature-rich discriminative model.Conventional semi-supervised learning requiresparsed sentences.
Kate and Mooney (2007) andMcClosky et al (2006) both use modified formsof self-training to bootstrap parsers from limitedlabeled data.
Wang et al (2008) combine a struc-tured loss on parsed sentences with a least squaresloss on unlabeled sentences.
Koo et al (2008) usea large unlabeled corpus to estimate cluster fea-tures which help the parser generalize with fewerexamples.
Smith and Eisner (2007) apply entropyregularization to dependency parsing.
The abovemethods can be applied to small seed corpora, butMcDonald1 has criticized such methods as work-ing from an unrealistic premise, as a significantamount of the effort required to build a treebankcomes in the first 100 sentences (both because ofthe time it takes to create an appropriate rubric andto train annotators).There are also a number of methods for unsu-pervised learning of dependency parsers.
Kleinand Manning (2004) use a carefully initialized andstructured generative model (DMV) in conjunc-tion with the EM algorithm to get the first positiveresults on unsupervised dependency parsing.
Asempirical evidence of the sensitivity of DMV toinitialization, Smith (2006) (pg.
37) uses three dif-ferent initializations, and only one, the method ofKlein and Manning (2004), gives accuracy higherthan 31% on the WSJ10 corpus (see Section 5).This initialization encodes the prior knowledgethat long distance attachments are unlikely.Smith and Eisner (2005) develop contrastiveestimation (CE), in which the model is encour-aged to move probability mass away from im-plicit negative examples defined using a care-fully chosen neighborhood function.
For instance,Smith (2006) (pg.
82) uses eight different neigh-borhood functions to estimate parameters for theDMV model.
The best performing neighborhood1R.
McDonald, personal communication, 2007function DEL1ORTRANS1 provides accuracy of57.6% on WSJ10 (see Section 5).
Another neigh-borhood, DEL1ORTRANS2, provides accuracy of51.2%.
The remaining six neighborhood func-tions provide accuracy below 50%.
This demon-strates that constructing an appropriate neighbor-hood function can be delicate and challenging.Smith and Eisner (2006) propose structural an-nealing (SA), in which a strong bias for local de-pendency attachments is enforced early in learn-ing, and then gradually relaxed.
This method issensitive to the annealing schedule.
Smith (2006)(pg.
136) use 10 annealing schedules in conjunc-tion with three initializers.
The best performingcombination attains accuracy of 66.7% on WSJ10,but the worst attains accuracy of 32.5%.Finally, Seginer (2007) and Bod (2006) ap-proach unsupervised parsing by constructingnovel syntactic models.
The development and tun-ing of the above methods constitute the encodingof prior domain knowledge about the desired syn-tactic structure.
In contrast, our framework pro-vides a straightforward and explicit method for in-corporating prior knowledge.Ganchev et al (2009) propose a related methodthat uses posterior constrained EM to learn a pro-jective target language parser using only a sourcelanguage parser and word alignments.3 Generalized Expectation CriteriaGeneralized expectation criteria (Mann and Mc-Callum, 2008; Druck et al, 2008) are terms ina parameter estimation objective function that ex-press a preference on the value of a model expec-tation.
Let x represent input variables (i.e.
a sen-tence) and y represent output variables (i.e.
a parsetree).
A generalized expectation term G(?)
is de-fined by a constraint function G(y,x) that returnsa non-negative real value given input and outputvariables, an empirical distribution p?
(x) over in-put variables (i.e.
unlabeled data), a model distri-bution p?
(y|x), and a score function S:G(?)
= S(Ep?(x)[Ep?
(y|x)[G(y,x)]]).In this paper, we use a score function that is thesquared difference of the model expectation of Gand some target expectation G?
:Ssq = ?(G??
Ep?(x)[Ep?
(y|x)[G(y,x)]])2 (1)We can incorporate prior knowledge into the train-ing of p?
(y|x) by specifying the from of the con-straint function G and the target expectation G?.361Importantly, G does not need to match a particularfeature in the underlying model.The complete objective function2 includes mul-tiple GE terms and a prior on parameters3, p(?)O(?
;D) = p(?)
+?GG(?
)GE has been applied to logistic regression mod-els (Mann and McCallum, 2007; Druck et al,2008) and linear chain CRFs (Mann and McCal-lum, 2008).
In the following sections we applyGE to non-projective CRF dependency parsing.3.1 GE in General CRFsWe first consider an arbitrarily structured condi-tional random field (Lafferty et al, 2001) p?
(y|x).We describe the CRF for non-projective depen-dency parsing in Section 3.2.
The probability ofan output y conditioned on an input x isp?
(y|x) =1Zxexp(?j?jFj(y,x)),where Fj are feature functions over the cliquesof the graphical model and Z(x) is a normaliz-ing constant that ensures p?
(y|x) sums to 1.
Weare interested in the expectation of constraint func-tion G(x,y) under this model.
We abbreviate thismodel expectation as:G?
= Ep?(x)[Ep?
(y|x)[G(y,x)]]It can be shown that partial derivative of G(?)
us-ing Ssq4 with respect to model parameter ?j is???jG(?)
= 2(G??G?)
(2)(Ep?(x)[Ep?
(y|x) [G(y,x)Fj(y,x)]?Ep?
(y|x) [G(y,x)]Ep?
(y|x) [Fj(y,x)]]).Equation 2 has an intuitive interpretation.
The firstterm (on the first line) is the difference between themodel and target expectations.
The second term2In general, the objective function could also include thelikelihood of available labeled data, but throughout this paperwe assume we have no parsed sentences.3Throughout this paper we use a Gaussian prior on pa-rameters with ?2 = 10.4In previous work, S was the KL-divergence from the tar-get expectation.
The partial derivative of the KL divergencescore function includes the same covariance term as abovebut substitutes a different multiplicative term: G?/G?.
(the rest of the equation) is the predicted covari-ance between the constraint function G and themodel feature function Fj .
Therefore, if the con-straint is not satisfied, GE updates parameters forfeatures that the model predicts are related to theconstraint function.If there are constraint functions G for all modelfeature functions Fj , and the target expectationsG?
are estimated from labeled data, then the glob-ally optimal parameter setting under the GE objec-tive function is equivalent to the maximum likeli-hood solution.
However, GE does not require sucha one-to-one correspondence between constraintfunctions and model feature functions.
This al-lows bootstrapping of feature-rich models with asmall number of prior expectation constraints.3.2 Non-Projective Dependency Tree CRFsWe now define a CRF p?
(y|x) for unlabeled, non-projective5 dependency parsing.
The tree y is rep-resented as a vector of the same length as the sen-tence, where yi is the index of the parent of wordi.
The probability of a tree y given sentence x isp?
(y|x) =1Zxexp( n?i=1?j?jfj(xi, xyi ,x)),where fj are edge-factored feature functions thatconsider the child input (word, tag, or other fea-ture), the parent input, and the rest of the sen-tence.
This factorization implies that dependencydecisions are independent conditioned on the in-put sentence x if y is a tree.
ComputingZx and theedge expectations needed for partial derivatives re-quires summing over all possible trees for x.By relating the sum of the scores of all possibletrees to counting the number of spanning trees in agraph, it can be shown that Zx is the determinantof the Kirchoff matrixK, which is constructed us-ing the scores of possible edges.
(McDonald andSatta, 2007; Smith and Smith, 2007).
Computingthe determinant takes O(n3) time, where n is thelength of the sentence.
To compute the marginalprobability of a particular edge k ?
i (i.e.
yi=k),the score of any edge k?
?
i such that k?
6= k isset to 0.
The determinant of the resulting modi-fied Kirchoff matrix Kk?i is then the sum of thescores of all trees that include the edge k ?
i. The5Note that we could instead define a CRF for projectivedependency parse trees and use a variant of the inside outsidealgorithm for inference.
We choose non-projective because itis the more general case.362marginal p(yi=k|x; ?)
can be computed by divid-ing this score by Zx (McDonald and Satta, 2007).Computing all edge expectations with this algo-rithm takes O(n5) time.
Smith and Smith (2007)describe a more efficient algorithm that can com-pute all edge expectations in O(n3) time using theinverse of the Kirchoff matrix K?1.3.3 GE for Non-Projective Dependency TreeCRFsWhile in general constraint functions G mayconsider multiple edges, in this paper we useedge-factored constraint functions.
In this caseEp?(y|x)[G(y,x)]Ep?
(y|x)[Fj(y,x)], the secondterm of the covariance in Equation 2, can becomputed using the edge marginal distributionsp?(yi|x).
The first term of the covarianceEp?
(y|x)[G(y,x)Fj(y,x)] is more difficult tocompute because it requires the marginal proba-bility of two edges p?
(yi, yi?
|x).
It is important tonote that the model p?
is still edge-factored.The sum of the scores of all trees that containedges k ?
i and k?
?
i?
can be computed by set-ting the scores of edges j ?
i such that j 6= k andj?
?
i?
such that j?
6= k?
to 0, and computing thedeterminant of the resulting modified Kirchoff ma-trixKk?i,k??i?
.
There areO(n4) pairs of possibleedges, and the determinant computation takes timeO(n3), so this naive algorithm takes O(n7) time.An improved algorithm computes, for each pos-sible edge k ?
i, a modified Kirchoff matrixKk?i that requires the presence of that edge.Then, the method of Smith and Smith (2007) canbe used to compute the probability of every pos-sible edge conditioned on the presence of k ?
i,p?(yi?
=k?|yi = k,x), using K?1k?i.
Multiplyingthis probability by p?
(yi=k|x) yields the desiredtwo edge marginal.
Because this algorithm pullsthe O(n3) matrix operation out of the inner loopover edges, the run time is reduced to O(n5).If it were possible to perform only one O(n3)matrix operation per sentence, then the gradientcomputation would take onlyO(n4) time, the timerequired to consider all pairs of edges.
Unfortu-nately, there is no straightforward generalizationof the method of Smith and Smith (2007) to thetwo edge marginal problem.
Specifically, Laplaceexpansion generalizes to second-order matrix mi-nors, but it is not clear how to compute second-order cofactors from the inverse Kirchoff matrixalone (c.f.
(Smith and Smith, 2007)).Consequently, we also propose an approxima-tion that can be used to speed up GE training atthe expense of a less accurate covariance compu-tation.
We consider different cases of the edgesk ?
i, and k?
?
i?.?
p?
(yi=k, yi?=k?|x)=0 when i=i?
and k 6=k?
(different parent for the same word), or wheni=k?
and k=i?
(cycle), because these pairs ofedges break the tree constraint.?
p?
(yi=k, yi?
=k?|x)=p?
(yi=k|x) when i=i?, k=k?.?
p?
(yi=k, yi?
=k?|x)?p?(yi=k|x)p?(yi?
=k?|x) when i 6= i?
and i 6= k?
or i?
6= k(different words, do not create a cycle).
Thisapproximation assumes that pairs of edgesthat do not fall into one of the above casesare conditionally independent given x. Thisis not true because there are partial trees inwhich k ?
i and k?
?
i?
can appear sepa-rately, but not together (for example if i = k?and the partial tree contains i?
?
k).Using this approximation, the covariance for onesentence is approximately equal ton?iEp?
(yi|x)[fj(xi, xyi ,x)g(xi, xyi ,x)]?n?iEp?
(yi|x)[fj(xi, xyi ,x)]Ep?
(yi|x)[g(xi, xyi ,x)]?n?i,kp?(yi=k|x)p?
(yk=i|x)fj(xi, xk,x)g(xk, xi,x).Intuitively, the first and second terms compute acovariance over possible parents for a single word,and the third term accounts for cycles.
Computingthe above takes O(n3) time, the time required tocompute single edge marginals.
In this paper, weuse the O(n5) exact method, though we find thatthe accuracy attained by approximate training isusually within 5% of the exact method.If G is not edge-factored, then we need to com-pute a marginal over three or more edges, makingexact training intractable.
An appealing alterna-tive to a similar approximation to the above woulduse loopy belief propagation to efficiently approx-imate the marginals (Smith and Eisner, 2008).In this paper g is binary and normalized by itstotal count in the corpus.
The expectation of g isthen the probability that it indicates a true edge.3634 Linguistic Prior KnowledgeTraining parsers using GE with the aid of linguistsis an exciting direction for future work.
In this pa-per, we use constraints derived from several basictypes of linguistic knowledge.One simple form of linguistic knowledge is theset of possible parent tags for a given child tag.This type of constraint was used in the devel-opment of a rule-based dependency parser (De-busmann et al, 2004).
Additional informationcan be obtained from small grammar fragments.Haghighi and Klein (2006) provide a list of proto-type phrase structure rules that can be augmentedwith dependencies and used to define constraintsinvolving parent and child tags, surrounding orinterposing tags, direction, and distance.
Finallythere are well known hypotheses about the direc-tion and distance of attachments that can be usedto define constraints.
Eisner and Smith (2005) usethe fact that short attachments are more commonto improve unsupervised parsing accuracy.4.1 ?Oracle?
constraintsFor some experiments that follow we use ?ora-cle?
constraints that are estimated from labeleddata.
This involves choosing feature templates(motivated by the linguistic knowledge describedabove) and estimating target expectations.
Oraclemethods used in this paper consider three simplestatistics of candidate constraint functions: countc?
(g), edge count c?edge(g), and edge probabilityp?(edge|g).
Let D be the labeled corpus.c?
(g) =?x?D?i?jg(xi, xj ,x)c?edge(g) =?
(x,y)?D?ig(xi, xyi ,x)p?
(edge|g) =c?edge(g)c?
(g)Constraint functions are selected according tosome combination of the above statistics.
Insome cases we additionally prune the candidateset by considering only certain templates.
Tocompute the target expectation, we simply usebin(p?
(edge|g)), where bin returns the closestvalue in the set {0, 0.1, 0.25, 0.5, 0.75, 1}.
Thiscan be viewed as specifying that g is very indica-tive of edge, somewhat indicative of edge, etc.5 Experimental Comparison withUnsupervised LearningIn this section we compare GE training with meth-ods for unsupervised parsing.
We use the WSJ10corpus (as processed by Smith (2006)), which iscomprised of English sentences of ten words orfewer (after stripping punctuation) from the WSJportion of the Penn Treebank.
As in previous worksentences contain only part-of-speech tags.We compare GE and supervised training of anedge-factored CRF with unsupervised learning ofa DMV model (Klein and Manning, 2004) usingEM and contrastive estimation (CE) (Smith andEisner, 2005).
We also report the accuracy of anattach-right baseline6.
Finally, we report the ac-curacy of a constraint baseline that assigns a scoreto each possible edge that is the sum of the targetexpectations for all constraints on that edge.
Pos-sible edges without constraints receive a score of0.
These scores are used as input to the maximumspanning tree algorithm, which returns the besttree.
Note that this is a strong baseline because itcan handle uncertain constraints, and the tree con-straint imposed by the MST algorithm helps infor-mation propagate across edges.We note that there are considerable differencesbetween the DMV and CRF models.
The DMVmodel is more expressive than the CRF becauseit can model the arity of a head as well as sib-ling relationships.
Because these features considermultiple edges, including them in the CRF modelwould make exact inference intractable (McDon-ald and Satta, 2007).
However, the CRF may con-sider the distance between head and child, whereasDMV does not model distance.
The CRF alsomodels non-projective trees, which when evaluat-ing on English is likely a disadvantage.Consequently, we experiment with two sets offeatures for the CRF model.
The first, restrictedset includes features that consider the head andchild tags of the dependency conjoined with thedirection of the attachment, (parent-POS,child-POS,direction).
With this feature set, the CRFmodel is less expressive than DMV.
The sec-ond full set includes standard features for edge-factored dependency parsers (McDonald et al,2005), though still unlexicalized.
The CRF can-not consider valency even with the full feature set,but this is balanced by the ability to use distance.6The reported accuracies with the DMV model and theattach-right baseline are taken from (Smith, 2006).364feature ex.
feature ex.MD?
VB 1.00 NNS?
VBD 0.75POS?
NN 0.75 PRP?
VBD 0.75JJ?
NNS 0.75 VBD?
TO 1.00NNP?
POS 0.75 VBD?
VBN 0.75ROOT?MD 0.75 NNS?
VBP 0.75ROOT?
VBD 1.00 PRP?
VBP 0.75ROOT?
VBP 0.75 VBP?
VBN 0.75ROOT?
VBZ 0.75 PRP?
VBZ 0.75TO?
VB 1.00 NN?
VBZ 0.75VBN?
IN 0.75 VBZ?
VBN 0.75Table 1: 20 constraints that give 61.3% accuracyon WSJ10.
Tags are grouped according to heads,and are in the order they appear in the sentence,with the arrow pointing from head to modifier.We generate constraints in two ways.
First,we use oracle constraints of the form (parent-POS,child-POS,direction) such that c?
(g) ?
200.We choose constraints in descending order ofp?(edge|g).
The first 20 constraints selected usingthis method are displayed in Table 1.Although the reader can verify that the con-straints in Table 1 are reasonable, we addition-ally experiment with human-provided constraints.We use the prototype phrase-structure constraintsprovided by Haghighi and Klein (2006), andwith the aid of head-finding rules, extract 14(parent-pos,child-pos,direction) constraints.7 Wethen estimated target expectations for these con-straints using our prior knowledge, without look-ing at the training data.
We also created a secondconstraint set with an additional six constraints fortag pairs that were previously underrepresented.5.1 ResultsWe present results varying the number of con-straints in Figures 1 and 2.
Figure 1 comparessupervised and GE training of the CRF model, aswell as the feature constraint baseline.
First wenote that GE training using the full feature set sub-stantially outperforms the restricted feature set,despite the fact that the same set of constraintsis used for both experiments.
This result demon-strates GE?s ability to learn about related but non-constrained features.
GE training also outper-forms the baseline8.We compare GE training of the CRF model7Because the CFG rules in (Haghighi and Klein, 2006)are ?flattened?
and in some cases do not generate appropriatedependency constraints, we only used a subset.8The baseline eventually matches the accuracy of the re-stricted CRF but this is understandable because GE?s abilityto bootstrap is greatly reduced with the restricted feature set.with unsupervised learning of the DMV modelin Figure 29.
Despite the fact that the restrictedCRF is less expressive than DMV, GE training ofthis model outperforms EM with 30 constraintsand CE with 50 constraints.
GE training of thefull CRF outperforms EM with 10 constraints andCE with 20 constraints (those displayed in Ta-ble 1).
GE training of the full CRF with the set of14 constraints from (Haghighi and Klein, 2006),gives accuracy of 53.8%, which is above the inter-polated oracle constraints curve (43.5% accuracywith 10 constraints, 61.3% accuracy with 20 con-straints).
With the 6 additional constraints, we ob-tain accuracy of 57.7% and match CE.Recall that CE, EM, and the DMV model in-corporate prior knowledge indirectly, and that thereported results are heavily-tuned ideal cases (seeSection 2).
In contrast, GE provides a method todirectly encode intuitive linguistic insights.Finally, note that structural annealing (Smithand Eisner, 2006) provides 66.7% accuracy onWSJ10 when choosing the best performing an-nealing schedule (Smith, 2006).
As noted in Sec-tion 2 other annealing schedules provide accuracyas low as 32.5%.
GE training of the full CRF at-tains accuracy of 67.0% with 30 constraints.6 Experimental Comparison withSupervised Training on LongSentencesUnsupervised parsing methods are typically eval-uated on short sentences, as in Section 5.
In thissection we show that GE can be used to trainparsers for longer sentences that provide compa-rable accuracy to supervised training with tens tohundreds of parsed sentences.We use the standard train/test splits of theSpanish, Dutch, and Turkish data from the 2006CoNLL Shared Task.
We also use standardedge-factored feature templates (McDonald et al,2005)10.
We experiment with versions of the dat-9Klein and Manning (2004) report 43.2% accuracy forDMV with EM on WSJ10.
When jointly modeling con-stituency and dependencies, Klein and Manning (2004) re-port accuracy of 47.5%.
Seginer (2007) and Bod (2006) pro-pose unsupervised phrase structure parsing methods that givebetter unlabeled F-scores than DMV with EM, but they donot report directed dependency accuracy.10Typical feature processing uses only supported features,or those features that occur on at least one true edge in thetraining data.
Because we assume that the data is unlabeled,we instead use features on all possible edges.
This generatestens of millions features, so we prune those features that oc-cur fewer than 10 total times, as in (Smith and Eisner, 2007).36510 20 30 40 50 60102030405060708090number of constraintsaccuracyconstraint baselineCRF restricted supervisedCRF supervisedCRF restricted GECRF GECRF GE humanFigure 1: Comparison of the constraint baseline andboth GE and supervised training of the restricted andfull CRF.
Note that supervised training uses 5,301parsed sentences.
GE with human provided con-straints closely matches the oracle results.10 20 30 40 50 601020304050607080number of constraintsaccuracyattach right baselineDMV EMDMV CECRF restricted GECRF GECRF GE humanFigure 2: Comparison of GE training of the re-stricted and full CRFs with unsupervised learning ofDMV.
GE training of the full CRF outperforms CEwith just 20 constraints.
GE also matches CE with20 human provided constraints.sets in which we remove sentences that are longerthan 20 words and 60 words.For these experiments, we use an oracleconstraint selection method motivated by thelinguistic prior knowledge described in Section 4.The first set of constraints specify the mostfrequent head tag, attachment direction, anddistance combinations for each child tag.
Specif-ically, we select oracle constraints of the type(parent-CPOS,child-CPOS,direction,distance)11.We add constraints for every g such thatc?edge(g) > 100 for max length 60 data sets, andc?edge(g)>10 times for max length 20 data sets.In some cases, the possible parent constraintsdescribed above will not be enough to providehigh accuracy, because they do not consider othertags in the sentence (McDonald et al, 2005).Consequently, we experiment with adding anadditional 25 sequence constraints (for what areoften called ?between?
and ?surrounding?
fea-tures).
The oracle feature selection method aims tochoose such constraints that help to reduce uncer-tainty in the possible parents constraint set.
Con-sequently, we consider sequence features gs withp?
(edge|gs=1) ?
0.75, and whose corresponding(parent-CPOS,child-CPOS,direction,distance)constraint g, has edge probability p?
(edge|g) ?0.25.
Among these candidates, we sort byc?
(gs=1), and select the top 25.We compare with the constraint baseline de-scribed in Section 5.
Additionally, we report11For these experiments we use coarse-grained part-of-speech tags in constraints.the number of parsed sentences required for su-pervised CRF training (averaged over 5 randomsplits) to match the accuracy of GE training usingthe possible parents + sequence constraint set.The results are provided in Table 2.
We firstobserve that GE always beats the baseline, espe-cially on parent decisions for which there are noconstraints (not reported in Table 2, but for exam-ple 53.8% vs. 20.5% on Turkish 20).
Second, wenote that accuracy is always improved by addingsequence constraints.
Importantly, we observethat GE gives comparable performance to super-vised training with tens or hundreds of parsed sen-tences.
These parsed sentences provide a tremen-dous amount of information to the model, as forexample in 20 Spanish length ?
60 sentences, atotal of 1,630,466 features are observed, 330,856of them unique.
In contrast, the constraint-basedmethods are provided at most a few hundred con-straints.
When comparing the human costs ofparsing sentences and specifying constraints, re-member that parsing sentences requires the devel-opment of detailed annotation guidelines, whichcan be extremely time-consuming (see also thediscussion is Section 2).Finally, we experiment with iterativelyadding constraints.
We sort constraints withc?
(g) > 50 by p?
(edge|g), and ensure that 50%are (parent-CPOS,child-CPOS,direction,distance)constraints and 50% are sequence constraints.For lack of space, we only show the results forSpanish 60.
In Figure 3, we see that GE beatsthe baseline more soundly than above, and that366possible parent constraints + sequence constraints complete treesbaseline GE baseline GEdutch 20 69.5 70.7 69.8 71.8 80-160dutch 60 66.5 69.3 66.7 69.8 40-80spanish 20 70.0 73.2 71.2 75.8 40-80spanish 60 62.1 66.2 62.7 66.9 20-40turkish 20 66.3 71.8 67.1 72.9 80-160turkish 60 62.1 65.5 62.3 66.6 20-40Table 2: Experiments on Dutch, Spanish, and Turkish with maximum sentence lengths of 20 and 60.
Observe that GEoutperforms the baseline, adding sequence constraints improves accuracy, and accuracy with GE training is comparable tosupervised training with tens to hundreds of parsed sentences.parent tag true predicteddet.
0.005 0.005adv.
0.018 0.013conj.
0.012 0.001pron.
0.011 0.009verb 0.355 0.405adj.
0.067 0.075punc.
0.031 0.013noun 0.276 0.272prep.
0.181 0.165direction true predictedright 0.621 0.598left 0.339 0.362distance true predicted1 0.495 0.5642 0.194 0.2063 0.066 0.0504 0.042 0.0375 0.028 0.0316-10 0.069 0.033> 10 0.066 0.039feature (distance) false pos.
occ.verb?
punc.
(>10) 1183noun?
prep.
(1) 1139adj.
?
prep.
(1) 855verb?
verb (6-10) 756verb?
verb (>10) 569noun?
punc.
(1) 512verb?
punc.
(2) 509prep.
?
punc.
(1) 476verb?
punc.
(4) 427verb?
prep.
(1) 422Table 3: Error analysis for GE training with possible parent + sequence constraints on Spanish 60 data.
On the left, thepredicted and true distribution over parent coarse part-of-speech tags.
In the middle, the predicted and true distributions overattachment directions and distances.
On the right, common features on false positive edges.100 200 300 400 500 600 700 8002530354045505560657075number of constraintsaccuracySpanish (maximum length 60)constraint baselineGEFigure 3: Comparing GE training of a CRF and constraintbaseline while increasing the number of oracle constraints.adding constraints continues to increase accuracy.7 Error AnalysisIn this section, we analyze the errors of the modellearned with the possible parent + sequence con-straints on the Spanish 60 data.
In Table 3, wepresent four types of analysis.
First, we presentthe predicted and true distributions over coarse-grained parent part of speech tags.
We can seethat verb is being predicted as a parent tag moreoften then it should be, while most other tags arepredicted less often than they should be.
Next, weshow the predicted and true distributions over at-tachment direction and distance.
From this we seethat the model is often incorrectly predicting leftattachments, and is predicting too many short at-tachments.
Finally, we show the most commonparent-child tag with direction and distance fea-tures that occur on false positive edges.
From thistable, we see that many errors concern the attach-ments of punctuation.
The second line indicates aprepositional phrase attachment ambiguity.This analysis could also be performed by a lin-guist by looking at predicted trees for selected sen-tences.
Once errors are identified, GE constraintscould be added to address these problems.8 ConclusionsIn this paper, we developed a novel method forthe semi-supervised learning of a non-projectiveCRF dependency parser that directly uses linguis-tic prior knowledge as a training signal.
It is ourhope that this method will permit more effectiveleveraging of linguistic insight and resources andenable the construction of parsers in languages anddomains where treebanks are not available.AcknowledgmentsWe thank Ryan McDonald, Keith Hall, John Hale, XiaoyunWu, and David Smith for helpful discussions.
This workwas completed in part while Gregory Druck was an internat Google.
This work was supported in part by the Centerfor Intelligent Information Retrieval, The Central IntelligenceAgency, the National Security Agency and National ScienceFoundation under NSF grant #IIS-0326249, and by the De-fense Advanced Research Projects Agency (DARPA) underContract No.
FA8750-07-D-0185/0004.
Any opinions, find-ings and conclusions or recommendations expressed in thismaterial are the author?s and do not necessarily reflect thoseof the sponsor.367ReferencesE.
Black, J. Lafferty, and S. Roukos.
1992.
Development andevaluation of a broad-coverage probabilistic grammar ofenglish language computer manuals.
In ACL, pages 185?192.Rens Bod.
2006.
An all-subtrees approach to unsupervisedparsing.
In ACL, pages 865?872.E.
Charniak.
2001.
Immediate-head parsing for languagemodels.
In ACL.R.
Debusmann, D. Duchier, A. Koller, M. Kuhlmann,G.
Smolka, and S. Thater.
2004.
A relational syntax-semantics interface based on dependency grammar.
InCOLING.G.
Druck, G. S. Mann, and A. McCallum.
2008.
Learningfrom labeled features using generalized expectation crite-ria.
In SIGIR.J.
Eisner and N.A.
Smith.
2005.
Parsing with soft and hardconstraints on dependency length.
In IWPT.Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.2009.
Dependency grammar induction via bitext projec-tion constraints.
In ACL.A.
Haghighi and D. Klein.
2006.
Prototype-driven grammarinduction.
In COLING.R.
J. Kate and R. J. Mooney.
2007.
Semi-supervised learningfor semantic parsing using support vector machines.
InHLT-NAACL (Short Papers).D.
Klein and C. Manning.
2004.
Corpus-based inductionof syntactic structure: Models of dependency and con-stituency.
In ACL.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In ACL.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting andlabeling sequence data.
In ICML.G.
Mann and A. McCallum.
2007.
Simple, robust, scal-able semi-supervised learning via expectation regulariza-tion.
In ICML.G.
Mann and A. McCallum.
2008.
Generalized expectationcriteria for semi-supervised learning of conditional ran-dom fields.
In ACL.D.
McClosky, E. Charniak, and M. Johnson.
2006.
Effectiveself-training for parsing.
In HLT-NAACL.Ryan McDonald and Giorgio Satta.
2007.
On the complex-ity of non-projective data-driven dependency parsing.
InProc.
of IWPT, pages 121?132.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In ACL, pages 91?98.R.
Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985.A Comprehensive Grammar of the English Language.Longman.Yoav Seginer.
2007.
Fast unsupervised incremental parsing.In ACL, pages 384?391, Prague, Czech Republic.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: training log-linear models on unlabeled data.
InACL, pages 354?362.Noah A. Smith and Jason Eisner.
2006.
Annealing struc-tural bias in multilingual weighted grammar induction.
InCOLING-ACL, pages 569?576.David A. Smith and Jason Eisner.
2007.
Bootstrappingfeature-rich dependency parsers with entropic priors.
InEMNLP-CoNLL, pages 667?677.David A. Smith and Jason Eisner.
2008.
Dependency parsingby belief propagation.
In EMNLP.David A. Smith and Noah A. Smith.
2007.
Probabilisticmodels of nonprojective dependency trees.
In EMNLP-CoNLL, pages 132?140.Noah A. Smith.
2006.
Novel Estimation Methods for Un-supervised Discovery of Latent Structure in Natural Lan-guage Text.
Ph.D. thesis, Johns Hopkins University.Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2008.Semi-supervised convex training for dependency parsing.In ACL, pages 532?540.368
