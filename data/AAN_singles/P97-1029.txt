Morphological Disambiguation by Voting ConstraintsKemal  Of lazer  and GSkhan T f i rDepar tment  of Computer  Engineer ing and In format ion  ScienceBi lkent University,  Bi lkent,  TR-06533,  Turkey{ko, tur}?cs, bilkent, edu.
trAbst ractWe present a constraint-based morpholog-ical disambiguation system in which indi-vidual constraints vote on matching mor-phological parses, and disambiguation ofall the tokens in a sentence is performedat the end by selecting parses that receivethe highest votes.
This constraint applica-tion paradigm makes the outcome of thedisambiguation i dependent of the rule se-quence, and hence relieves the rule devel-oper from worrying about potentially con-flicting rule sequencing.
Our results fordisambiguating Turkish indicate that usingabout 500 constraint rules and some addi-tional simple statistics, we can attain a re-call of 95-96~ and a precision of 94-95~with about 1.01 parses per token.
Our sys-tem is implemented in Prolog and we arecurrently investigating an efficient imple-mentation based on finite state transduc-ers.1 In t roduct ionAutomatic morphological disambiguation is an im-portant component in higher level analysis of naturallanguage text corpora.
There has been a large num-ber of studies in tagging and morphological disam-biguation using various techniques such as statisti-cal techniques, e.g., (Church, 1988; Cutting et al,1992; DeRose, 1988), constraint-based techniques(Karlsson et al, 1995; Voutilainen, 1995b; Vouti-lainen, Heikkil/i, and Anttila, 1992; Voutilainenand Tapanainen, 1993; Oflazer and KuruSz, 1994;Oflazer and T i l l  1996) and transformation-basedtechniques (Brilt, 1992; Brill, 1994; Brill, 1995).This paper presents a novel approach to constraintbased morphological disambiguation which relievesthe rule developer from worrying about conflictingrule ordering requirements.
The approach dependson assigning votes to constraints according to theircomplexity and specificity, and then letting con-straints cast votes on matching parses of a givenlexical item.
This approach does not reflect he out-come of matching constraints to the set of morpho-logical parses immediately.
Only after all applicablerules are applied to a sentence, all tokens are dis-ambiguated in parallel.
Thus, the outcome of therule applications i independent of the order of ruleapplications.
Rule ordering issue has been discussedby Voutilainen(1994), but he has recently indicated 1that insensitivity to rule ordering is not a propertyof their system (although Voutilainen(1995a) statesthat it is a very desirable property) but rather isachieved by extensively testing and tuning the rules.In the following sections, we present an overviewof the morphological disambiguation problem, high-lighted with examples from Turkish.
We thenpresent our approach and results.
We finally con-clude with a very brief outline of our investigationinto efficient implementations of our approach.2 Morpho log ica l  D isambiguat ionIn all languages, words are usually ambiguous intheir parts-of-speech or other morphological fea-tures, and may represent lexical items of differentsyntactic ategories, or morphological structures de-pending on the syntactic and semantic ontext.
Inlanguages like English, there are a very small numberof possible word forms that can be generated froma given root word, and a small number of part-of-speech tags associated with a given lexical form.
Onthe other hand, in languages like Turkish or Finnishwith very productive agglutinative morphology, itis possible to produce thousands of forms (or evenmillions (Hankamer, 1989)) from a given root wordand the kinds of ambiguities one observes are quitedifferent han what is observed in languages like En-glish.In Turkish, there are ambiguities of the sorttypically found in languages like English (e.g.,book/noun vs book/verb type).
However, the ag-glutinative nature of the language usually helps res-olution of such ambiguities due to the restrictionson morphotactics of subsequent morphemes.
On the1Voutilainen, Private communication.222other hand, this very nature introduces another kindof ambiguity, where a lexical form can be morpho-logically interpreted in many ways not usually pre-dictable in advance.
Furthermore, Turkish allowsvery productive derivational processes and the infor-mation about the derivational structure of a wordform is usually crucial for disambiguation (Oflazerand Tiir, 1996).Most kinds of morphological mbiguities that wehave observed in Turkish typically fall into one thefollowing classes: ~1.
the form is uninflected and assumes the defaultinflectional features, e.g.,I.
taS (made of stone)\[ \[CAT=ADJ\] \[ROOT=taS\]\]2. taS (stone)\[ \[CAT=NOUN\] [ROOT=taS\]\[AGR=3SG\] \[POSS=NONE\] [CASE=NOM\]3. taS (overflow!
)\[ \[CAT=VERB\] \[ROOT=t aS\] \[SENSE=POS\]\[TAMI=IMP\] \[AGR=2SQ\]\]2.
Lexically different affixes (conveying differentmorphological features) surface the same due tothe morphographemic context, e.g.,1.
ev+\ [n \ ] in  (of the house)\[ \[CAT=NOUN\] [ROOT=ev\]\[AGR=3SG\] \[PDSS=NONE\] [CASE=GEN\] \]2. ev+in (your house)\[ \[CAT=NOUN\] \[ROOT=ev\]\[AGR=3SG\] \[POSS=2SG\] \[CASE=NOM\]3.
The root of one of the parses is a prefix string ofthe root, of the other parse, and the parse withthe shorter root word has a suffix which surfacesas the rest of the longer root word, e.g.,1.
koyu+\[u\]n (your dark ( th ing) )\[ \[CAT=ADJ\] \[ROOT=koyu\] \[CONV=NOUN=NONE\]\[AGR=3SG\] \[POSS=2SG\] \[CASE=NOM\]\]2. koyun (sheep)\[ \[CAT=NOUN\] [ROOT=koyun\]\[AGR=3SG\] \[POSS=NONE\] [CASE=NOM\]3. koy+\[n\]un (of the bay)\[ \[CAT=NOUN\] [ROOT=koy\]\[AGR=3SG\] \[POSS=NONE\] \[CASE=GEN\] \]4. koy+un (ybur bay)\[ \[CAT=NOUN\] [R00T=koy\]\[AGR=bSG\] \[POSS=RSG\] \[CASE=NOM\]\]2Output of the morphological analyzer is edited forclarity, and English glosses have been given.
We havealso provided the morpheme structure, where \[...\]s, in-dicate elision.
Glosses are given as linear feature valuesequences corresponding to the morphemes (which arenot shown).
The feature names are as follows: CAT-majorcategory, TYPE-minor category, R00T-main root form, AGR-number and person agreement, P0SS - possessive agree-ment, CASE - surface case, CONV - conversion to the cat-egory following with a certain suffix indicated by theargument after that, TAMl-tense, aspect, mood marker1, SENSE-verbal polarity.
Upper cases in morphologicaloutput indicates one of the non-ASCII special Turkishcharacters: e.g., G denotes ~, U denotes /i, etc.5.
koy+\[y\]un (put!
)\[ \[CAT=VERB\] \[ROOT=koy\] \[SENSE=POS\]\[TAMI=IMP\] \[AGR=2PL\] \]4.
The roots take different numbers of unrelatedinflectional and/or derivational suffixes whichwhen concatenated turn out to have the samesurface form, e.g.,I.
yap+madan (without having done (it))\[ \[CAT=VERB\] \[ROOT=yap\] \[SENSE=POS\]\[CONV=ADVERB=MADAN\] \]2. yap+ma+dan (from doing (it))\[ \[CAT=VERB\] \[ROOT=yap\] \[SENSE=POS\]\[CONV=NOUN=MA\] \[TYPE=INFINITIVE\]\[AGR=3SG\] \[POSS=NONE\] \[CASE=ABL\] \]5.
One of the ambiguous parses is a lexicalizedform while another is form derived by a pro-ductive derivation as in 1 and 2 below.6.
The same suffix appears in different positions inthe morphotactic paradigm conveying differentinformation as in 2 and 3 below.1.
uygulama / (app l i ca t ion)\[ \[CAT=NOUN\] [ROOT=uygulama\]\[AGR=3SG\] \[POSS=NONE\] \[CASE=NDM\]2. uygula+ma / ((the act of) applying)\[ \[CAT=VERB\] \[ROOT=uygula\] \[SENSE=POS\]\[CONV=NOUN=MA\] \[TYPE=INFINITIVE\]\[AGR=3SG\] \[POSS=NONE\] \[CASE=NOM\]3. uygula+ma / (do not apply!
)\[ \[CAT=VERB\] \[ROOT=uygula\] \[SENSE=NEG\]\[TAMI=IMP\] \[AGR=2SG\]?
The main intent of our system is to achieve mor-phological disambiguation by choosing for a givenambiguous token, the correct parse in a given con-text.
It is certainly possible that a given token mayhave nmltiple correct parses, usually with the sameinflectional features, or with inflectional features notruled out by the syntactic ontext, but one will bethe "correct" parse usually on semantic grounds.We consider a token fully disambiguated if it hasonly one morphological parse remaining after auto-matic disambiguation.
We Consider a token as cor-rectly disambiguated, if one of the parses remainingfor that token is the correct intended parse.
We eval-uate the resulting disambiguated text by a numberof metrics defined as follows (Voutilainen, 1995a):#ParsesAmbiguity- #TokensRecall = #Tokens Correctly Disambiguated#TokensPrecision = #Tokens Correctly Disambiguated#ParsesIn the ideal case where each token is uniquely andcorrectly disambiguated with the correct parse, bothrecall and precision will be 1.0.
On the other hand, a223text where each token is annotated with all possibleparses, 3 the recall will be 1.0, but the precision willbe low.
The goal is to have both recall and precisionas high as possible.3 Const ra in t -based  Morpho log ica lD isambiguat ionThis section outlines our approach to constraint-based morphological disambiguation where con-straints vote on matching parses of sequential to-kens.3.1 Const ra in ts  on morpho log ica l  parsesWe describe constraints on the morphological parsesof tokens using rules with two componentsR= (Cl,C~.,...,C,~;V)where the Ci are (possibly hierarchical) feature con-straints on a sequence of the morphological parses,and V is an integer denoting the vote of the rule.To illustrate the flavor of our rules we can give thefollowing examples:1.
The following rule with two constraints matchesparses with case feature ablative, preceding aparse matching a postposition subcategorizingfor an ablative nominal form.\[ \ [case : abl\] , \[cat : postp,  subcat  : abl\] \]2.
The rule\[ \ [agr  : '2SG' ,  case  : gen \ ]  , \ [ ca t  :noun,  poss  : ' 2SG ' \ ]  \]matches a nominal form with a possessivemarker 2SG, following a pronoun with 2SGagreement and genitive case, enforcing the sim-plest form of noun phrase constraints.3.
In general constraints can make references totile derivational structure of the lexical formand hence be hierarchicah For instance, the fol-lowing rule is an example of a rule employing ahierarchical constraint:\[ \[cat : adj,  s tem : \[taml : narr\]  \] ,\[cat : noun, st em :no\] \]which matches tile derived participle reading ofa verb with narrative past tense, if it is followedby an underived noun parse.3.2 Determin ing  the  vote  o f  a ru leThere are a number of ways votes can be assignedto rules.
For the purposes of this work the vote ofa rule is determined by its static properties, but itis certainly conceivable that votes can be assignedor learned by using statistics from disambiguatedcorpora.
4 For static vote assignment, intuitively, wewould like to give high votes to rules that are morespecific: i.e., to rules that haveaAssuming no unknown words.4We have left this for future work.?
higher number of constraints,?
higher number of features in the constraints,?
constraints that make reference to nested stems(from which the current form is derived),?
constraints that make reference to very specificfeatures or values.Let R = (C1 ,C2 , ' " ,C~;V)  be a constraint rule.The vote V is determined asnv =i= lwhere V(Ci) is the contribution of constraint Ci tothe vote of the rule R. A (generic) constraint hasthe following form:C -- \[(fl : vl)(f2 : v2)&5... (fro : vm)\]where fi is the name of a morphological feature, andvi is one of the possible values for that feature.
Thecontribution of fi : vi in the vote of a constraintdepends on a number of factors:1.
The value vi may be a distinguished value thathas a more important function in disambigua-tion.
5 In this case, the weight of the featureconstraint is w(vi)(> 1).2.
The feature itself may be a distinguished featurewhich has more important function in disam-biguation.
In this case the weight of the featureis w(fi)(> 1).3.
If the feature fi refers to the stem of a de-rived form and the value part of the feature con-straint is a full fledged constraint C'  on the stemstructure, the weight of the feature constraint isfound by recursively computing the vote of C 'and scaling the resulting value by a factor (2 inour current system) to improve its specificity.4.
Otherwise, the weight of the feature constraintis 1.For example suppose we have the following con-straint:\[cat :noun, case : gen,stem:\[cat:adj, s tem: \ [cat :v \ ] ,  su f f i x=mis \ ] \ ]Assuming the value gen is a distinguished valuewith weight 4 (cf., factor 1 above), the vote of thisconstraint is computed as follows:1. cat  :noun contributes 1,2. case :gen  contributes 4,3. s tem:\ [cat :ad j ,  stem: \ [cat :v \ ] , su f f ix=mis \ ]contributes 8 computed as follows:(a) cat  :adj  contributes 1,5For instance, for Turkish we have noted thatthe genitive case marker is usually very helpful indisambiguation.224(b) suffYx=mS.s contributes 1,(c) stem: \ [ca t :v \ ]  contributes 2 = 2* 1, the 1being from cat  : v,(d) the sum 4 is scaled by 2 to give 8.4.
Votes from steps 1, 2 and 3(d) are added up togive 13 as the constraint vote.We also employ a set of rules which express pref-erences among the parses of single lexical form in-dependent of the context in which the form occurs.The weights for these rules are currently manuallydetermined.
These rules give negative votes to theparses which are not preferred or high votes to cer-tain parses which are always preferred.
Our experi-ence is that such preference rules depend on the kindof the text one is disambiguating.
For instance if oneis disambiguating a manual of some sort, imperativereadings of verbs are certainly possible, whereas innormal plain text with no discourse, such readingsare discouraged.3.3 Vot ing  and  se lect ing  parsesA rule R = (C1 ,62 , ' " ,  Cn; V) will match a se-quence of tokens wi, Wi+l, ?
?., wi+n-1 within a sen-tence wl through ws if some morphological parse ofevery token w j , i  < j < i + n - 1 is subsumed bythe corresponding constraint Cj-i+l.
When all con-straints match, the votes of all the matching parsesare incremented by V. If a given constraint matchesmore than one parse of a token, then the votes of allsuch matching parses are incremented.After all rules have been applied to all token po-sitions in a sentence and votes are tallied, morpho-logical parses are selected in the following manner.Let vt and Vh be the votes of the lowest and high-est scoring parses for a given token.
All parses withvotes equal to or higher than vt + m * (Vh -- vt) areselected with m (0 _< m _< 1) being a parameter.m = 1 selects the highest scoring parse(s).4 Resu l t s  f rom D isambiguat ingTurk i sh  TextWe have applied our approach to disambiguatingTurkish text.
Raw text is processed by a prepro-cessor which segments the text into sentences usingvarious heuristics about punctuation, and then to-kenizes and runs it through a wide-coverage high-performance morphological analyzer developed us-ing two-level morphology tools by Xerox (Kart-tunen, 1993).
The preprocessor module also per-forms a number of additional functions such asgrouping of lexicalizcd and non-lexicalized colloca-tions, compound verbs, etc., (Ofiazer and Kurubz,1994; Oflazer and Tiir, 1996).
The preprocessor alsouses a second morphological processor for dealingwith unknown words which recovers any derivationaland inflectional information from a word even if  theroot word is not known.
This unknown word pro-cessor has a (nominal) root lexicon which recognizesS +, where S is the Turkish surface alphabet (in thetwo-level morphology sense), but then tries to in-terpret an arbitrary postfix string of the unknownword, as a sequence of Turkish suffixes subject toall morphographemic constraints (Oflazer and Tfir,1996).We have applied our approach to four texts la-beled ARK, HIST, MAN, EMB, with statisticsgiven in Table 1.
The tokens considered are thosethat are generated after morphological nalysis, un-known word processing and any lexical coalescing isdone.
The words that are counted as unknown arethose that could not even be processed by the un-known noun processor as they violate Turkish mor-phographemic constraints.
Whenever an unknownword has more than one parse it is counted under theappropriate group.
6 The fourth and fifth columns inthis table give the average parses per token and theinitial precision assuming initial recall is 100%.We have disambiguated these texts using a rulebase of about 500 hand-crafted rules.
Most of therule crafting was done using the general linguisticconstraints and constraints that we derived from thefirst text, ARK.
In this sense, this text is our "train-ing data", while the other three texts were not con-sidered in rule crafting.Our results are summarized in Table 2.
The lastfour columns in this table present results for differ-ent values for the parameter n mentioned above,m = 1 denoting the case when only the highestscoring parse(s) is (are) selected.
The columns form < 1 are presented in order to emphasize that dras-tic loss of precision for those cases.
Even at m = 0.95there is considerable loss of precision and going upto m = 1 causes a dramatic increase in precisionwithout a significant loss in recall.
It can be seenthat we can attain very good recall and quite ac-ceptable precision with just voting constraint rules.Our experience is that we can in principle add highlyspecialized rules by covering a larger text base toimprove our recall and precision for the m = 1.
Apost-mortem analysis has shown that cases that havebeen missed are mostly due to morphosyntactic de-pendencies that span a context much wider that 5tokens that we currently employ.4.1 Us ing  root  and  contextua l  statisticsWe have employed two additional sources of infor-mation: root word usage statistics, and contextualstatistics.
We have statistics compiled from previ-ously disambiguated text, on root frequencies.
Afterthe application of constraints as described above, for6The reason for the (comparatively) high number ofunknown words in MAN, is that tokens found in suchtexts, like .\[10, denoting a function key in the computercan not be parsed as a Turkish root word!225Text Sent.ARK 492HIST 270MAN 204EMB 198Tokens7928521227565177Parses/Token1.8231.7971.8401.914Init.Prec.
00.55 0.15%0.56 0.02%0.54 0.65%0.52 0.09%DistributionofMorphological Parses1 2 3 4 >449.34% 30.93% 9.19% 8.46% 1.93%50.63% 30.68% 8.62% 8.36% 1.69%49.01% 31.70% 6.37% 8.91% 3.36%43.94% 34.58% 9.60% 9.46% 2.33%Table 1: Statistics on TextsVote Range Selected(m)TEXT 1.0 0.95 0.8 0.6ARK Rec.
98.05 98.47 98.69 98.77Prec.
94.13 87.65 84.41 82.43Amb.
1.042 1.123 1.169 1.200HIST Rec.
97.03 97.65 98.81 97.01Prec.
94.13 87.10 84.41 82.29Amb.
1.058 1.121 1.169 1.189'I~IAN Rec.
97.03 97.92 97.81 98.77Prec.
91.05 83.51 79.85 77.34Amb.
1.068 1.172 1.237 1.277EMB Rec.
96.51 97.48 97.76 97.94Prec.
91.28 84.36 77.87 75.79Amb.
1.057 1.150 1.255 1.292Table 2: Results with voting constraintsTEXT V V+R V+R+CARK Rec.
98.05 97.60 96.98Prec.
94.13 95.28 '96.19Amb.
1.042 1.024 1.008HIST Rec.
97:03 96.52 95.62Prec.
94.13 92.59 94.33Amb.
1.058 1.042 1.013MAN Rec.
97.03 96.47 95.84Prec.
91.05 93.08 94.47Amb.
1.058 1.042 1.014EMB Rec.
96.51 96.47 95.37Prec.
91.28 93.08 94.45Amb.
1.057 1.036 1.009Table 3: Results with voting constraints and rootstatistics, context statisticstokens which are still ambiguous with ambiguity re-sulting from different root words, we discard parsesif the frequencies of the root words for those parsesare considerably ower than the frequency of the rootof the highest scoring parse.
The results after apply-ing this step on top of voting, with m = 1, are shownin the fourth column of Table 3 (labeled V+R).On top of this, we use the following heuristic us-ing context statistics to eliminate any further ambi-guities.
For every remaining ambiguous token withunambiguous immediate l ft and right contexts (i.e.,the tokens in the immediate l ft and right are unam-biguous), we perform the following, by ignoring theroot/stem feature of ~he parses:1.
For every ambiguous parse in such an unam-biguous context, we count how many times, thisparse occurs unambiguously in exactly the sameunambiguous context, in the rest of the text.2.
We then choose the parse whose count is sub-stantially higher than the others.The results after applying this step on of the previ-ous two steps are shown in the last column of Table3 (labeled V+R+C).
One can see from the last threecolumns of this table, the impact of each of the steps.By ignoring root/stem features during this pro-cess, we essentially are considering just the top levelinflectional information of the parses.
This is verysimilar to Brill's use of contexts to induce transfor-mation rules for his tagger (Brill, 1992; Brill, 1995),but instead of generating transformation rules froma training text, we gather statistics and apply themto parses in the text being disambiguated.5 E f f i c ient  Imp lementat ionTechn iques  and  Extens ionsThe current implementation f the voting approachis meant to be a proof of concept implementationand is rather inefficient.
However, the use of regularrelations and finite state transducers (Kaplan andKay, 1994) provide a very efficient implementationmethod.
For this, we view the parses of the tokensmaking up a sentence as making up a acyclic a fi-nite state recognizer with the states marking wordboundaries and the ambiguous interpretations of thetokens as the state transitions between states, therightmost node denoting the final state, as depictedin Figure 1 for a sentence with 5 tokens.
In Figure 1,the transition labels are triples of the sort (wi, pj, O)for the jth parse of token i, with the 0 indicatingthe initial vote of the parse.
The rules imposingconstraints can also be represented as transducerswhich increment he votes of the matching transi-226(wl,pl,O) (w2,pl,O) (W3,pl,O) (w4,pl,O) (w5,pl,O)(wl,p3,0) (W2,p5,0) (w3,p4,0) (W4,p3,0) (W5,p4,0)Figure 1: Sentence as a finite state recognizer.tion labels by an appropriate amount.
~Such trans-ducers ignore and pass through unchanged, parsesthat they are not sensitive to.When a finite state recognizer corresponding tothe input sentence (which actually may be consid-ered as an identity transducer) is composed with aconstraint ransducer, one gets a slightly modifiedversion of the sentence transducer with possibly ad-ditional transitions and states, where the votes ofsome of the labels have been appropriately incre-lnented.
When the sentence transducer is composedwith all the constraint ransducers in sequence, allpossible votes are cast and the final sentence trans-ducer reflects all the votes.
The parse correspondingto each token with the highest vote can then be se-lected.
The key point here is that due to the natureof the composition operator, the constraint ransduc-ers can be composed off-line first, giving a single con-straint transducer and then this one is composed withevery sentence transducer once (See Figure 2).The idea of voting can further be extended to apath voting framework where rules vote on pathscontaining sequences of matching parses and thepath from the start state to the final stated withthe highest votes received, is then selected.
This canbe implemented again using finite state transducersas described above (except hat path vote is appor-tioned equally to relevant parse votes), but insteadof selecting highest scoring parses, one selects thepath from the start state to one of the final stateswhere the sum of the parse votes is maximum.
Wehave recently completed a prototype implementationof this approach (in C) for English (Brown Corpus)and have obtained quite similar results (Tiir, Of-lazer, and Oz-kan, 1997).6 Conc lus ionsWe have presented an approach to constraint-basedmorphological disambiguation which uses constraintvoting as its primary mechanism for parse selec-tion and alleviates the rule developer from worryingabout rule ordering issues.
Our approach is quitegeneral and is applicable to any language.
Rules de-scribing language specific linguistic constraints voteon matching parses of tokens, and at the end, parsesTSuggested by Lauri Karttunen (private communica-tion).for every token receiving the highest tokens are se-lected.
We have applied this approach to Turkish,a language with complex agglutinative word formsexhibiting morphological mbiguity phenomena notusually found in languages like English and have ob-tained quite promising results.
The convenience ofadding new rules in without worrying about whereexactly it goes in terms of rule ordering (some-thing that hampered our progress in our earlier workon disambiguating Turkish morphology (Oflazer andKuruSz, 1994; Oflazer and Tiir, 1996)), has also beena key positive point.
Furthermore, it is also possibleto use rules with negative votes to disallow impos-sible cases.
This has been quite useful for our workon tagging English (Tfir, Oflazer, and 0z-kan, 1997)where such rules with negative weights were used tofine tune the behavior of the tagger in various prob-lematic cases.The proposed approach is also amenable to anefficient implementation by finite state transducers(Kaplan and Kay, 1994).
By using finitestate trans-ducers, it is furthermore possible to use a bit moreexpressive rule formalism including for instance theKleene * operator so that one can use a much smallerset of rules to cover the same set of local linguisticphenomena.Our current and future work in this frameworkinvolves the learning of constraints and their votesfrom corpora, and combining learned and hand-crafted rules.7 AcknowledgmentsThis research as been supported in part by a NATOScience for Stability Grant TU-LANGUAGE.
Wethank Lauri Karttunen of Rank Xerox ResearchCentre in Grenoble for providing the Xerox two-levelmorphology tools on which the Turkish morpholog-ical analyzer was built.Re ferencesBrill, Eric.
1992.
A simple-rule based part-of-speechtagger.
In Proceedings of the Third Conferenceon Applied Natural Language Processing, Trento,Italy.Brill, Eric.
1994.
Some advances in rule-basedpart of speech tagging.
In Proceedings of the227(wl,pl, 0) (w2,pl, 0) (w3,pl, 0) (w4,pl, 0) (w5.pl, 0)(wl.p3,0) (w2,p5 ,0)  (W3,p4 .0)  (w4,p3 ,0)  (W5,p4.0)t Composition of the0 sentence transducerwith the constrainttransducerIIIItIsingle transducerIcomposed from all\]constraint\]transducersII'1 I I I IConstraintTransducer 1 (+VI)0ConstraintTransducer n (+Vn)Resulting sentence/ transducer aftercomposition(w2.
pl.
8~.~ (w3 ,pl, 4 !
~(wl,p3,4) ~ (w2,p5,3)Figure 2: Sentence and Constraint Transducers228Twelfth National Conference on Artificial Intel-ligence (AAAI-94), Seattle, Washington.Brill, Eric.
1995.
Transformation-based error-drivenlearning and natural language processing: A casestudy in part-of-speech tagging.
ComputationalLinguistics, 21(4):543-566, December.Church, Kenneth W. 1988.
A stochastic parts pro-gram and a noun phrase parser for unrestrictedtext.
In Proceedings of the Second Conferenceon Applied Natural Language Processing, Austin,Texas.Cutting, Doug, Julian Kupiec, Jan Pedersen, andPenelope Sibun.
1992.
A practical part-of-speechtagger.
In Proceedi~gs of the Third Conferenceon Applied Natural Language Processing, Trento,Italy.DeRose, Steven J.
1988.
Grammatical category dis-ambiguation by statistical optimization.
Compu-tational Linguistics, 14(1):31-39.Hankamer, Jorge.
1989.
Morphological parsing andthe lexicon.
In W. Marslen-Wilson, editor, LexicalRepresentation and Process.
MIT Press.Kaplan, Ronald M. and Martin Kay.
1994.
Regularmodels of phonological rule systems.
Computa-tional Linguistics, 20(3):331-378, September.Karlsson, Fred, Atro Voutilainen, Juha Heikkilii,and Arto Anttila.
1995.
Constraint Grammar-ALanguage-Independent System for Parsing Unre-stricted Text.
Mouton de Gruyter.Karttunen, Lauri.
1993.
Finite-state l xicon com-piler.
XEROX, Palo Alto Research Center- Tech-nical Report, April.Oflazer, Kemal and llker KuruSz.
1994.
Tag-ging and morphological disambiguation f Turk-ish text.
In Proceedil~gs ofthe 4 ~h Applied NaturalLanguage Processing Conference, pages 144-149.ACL, October.Oflazer, Kemal and GSkhan Tilt.
1996.
Combin-ing hand-crafted rules and unsupervised learn-ing in constraint-based morphological disambigua-tion.
In Eric Brill and Kenneth Church, editors,Proceedings of the ACL-SIGDAT Conference onEmpirical Methods in Natural Language Process-ing.Tfir, GSkhan, Kemal Oflazer, and Nihat Oz-kan. 1997.
Tagging English by pathvoting constraints.
Technical Report BU-CEIS-9704, Bilkent University, Department ofComputer Engineering and Information Sci-ence, Ankara, Turkey, March.
Available asftp ://ftp.
cs.
bilkent, edu.
tr/pub/t ech-rep-oft s/1997/BU-CEIS-9704 .ps.
z.Vouti\]ainen, Atro.
1994.
Three studies of grammar-based surface-syntactic parsing of unrestricted En-glish text.
Ph.D. thesis, Research Unit for Com-putational Linguistics, University of Hetsinki.Voutilainen, Atro.
1995a.
Morphological disana-biguation.
In Fred Karlsson, Atro Voutilainen,Juha Heikkil?, and Arto Anttila, editors, Con-straint Grammar-A Language-Independent Sys-tem for Parsing Unrestricted Text.
Mouton deGruyter, chapter 5.Voutilainen, Atro.
1995b.
A syntax-based part-of-speech analyzer.
In Proceedings of the SeventhConference of the European Chapter of the Asso-ciation of Computational Linguistics, Dublin, Ire-land.Voutilainen, Atro, Juha Heikkil?, and Arto Anttila.1992.
Constraint Grammar of English.
Universityof Helsinki.Voutilainen, Atro and Pasi Tapanainen.
1993.
Am-biguity resolution in a reduetionistic parser.
InProceedings of EACL'93, Utrecht, Holland.229
