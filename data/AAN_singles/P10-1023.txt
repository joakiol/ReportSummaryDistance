Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 216?225,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsBabelNet: Building a Very Large Multilingual Semantic NetworkRoberto NavigliDipartimento di InformaticaSapienza Universita` di Romanavigli@di.uniroma1.itSimone Paolo PonzettoDepartment of Computational LinguisticsHeidelberg Universityponzetto@cl.uni-heidelberg.deAbstractIn this paper we present BabelNet ?
avery large, wide-coverage multilingual se-mantic network.
The resource is automat-ically constructed by means of a method-ology that integrates lexicographic and en-cyclopedic knowledge from WordNet andWikipedia.
In addition Machine Transla-tion is also applied to enrich the resourcewith lexical information for all languages.We conduct experiments on new and ex-isting gold-standard datasets to show thehigh quality and coverage of the resource.1 IntroductionIn many research areas of Natural Language Pro-cessing (NLP) lexical knowledge is exploited toperform tasks effectively.
These include, amongothers, text summarization (Nastase, 2008),Named Entity Recognition (Bunescu and Pas?ca,2006), Question Answering (Harabagiu et al,2000) and text categorization (Gabrilovich andMarkovitch, 2006).
Recent studies in the diffi-cult task of Word Sense Disambiguation (Nav-igli, 2009b, WSD) have shown the impact of theamount and quality of lexical knowledge (Cuadrosand Rigau, 2006): richer knowledge sources canbe of great benefit to both knowledge-lean systems(Navigli and Lapata, 2010) and supervised classi-fiers (Ng and Lee, 1996; Yarowsky and Florian,2002).Various projects have been undertaken to makelexical knowledge available in a machine read-able format.
A pioneering endeavor was Word-Net (Fellbaum, 1998), a computational lexicon ofEnglish based on psycholinguistic theories.
Sub-sequent projects have also tackled the significantproblem of multilinguality.
These include Eu-roWordNet (Vossen, 1998), MultiWordNet (Piantaet al, 2002), the Multilingual Central Repository(Atserias et al, 2004), and many others.
How-ever, manual construction methods inherently suf-fer from a number of drawbacks.
First, maintain-ing and updating lexical knowledge resources isexpensive and time-consuming.
Second, such re-sources are typically lexicographic, and thus con-tain mainly concepts and only a few named enti-ties.
Third, resources for non-English languagesoften have a much poorer coverage since the con-struction effort must be repeated for every lan-guage of interest.
As a result, an obvious bias ex-ists towards conducting research in resource-richlanguages, such as English.A solution to these issues is to draw upona large-scale collaborative resource, namelyWikipedia1.
Wikipedia represents the perfect com-plement to WordNet, as it provides multilinguallexical knowledge of a mostly encyclopedic na-ture.
While the contribution of any individual usermight be imprecise or inaccurate, the continual in-tervention of expert contributors in all domains re-sults in a resource of the highest quality (Giles,2005).
But while a great deal of work has been re-cently devoted to the automatic extraction of struc-tured information from Wikipedia (Wu and Weld,2007; Ponzetto and Strube, 2007; Suchanek etal., 2008; Medelyan et al, 2009, inter alia), theknowledge extracted is organized in a looser waythan in a computational lexicon such as WordNet.In this paper, we make a major step towards thevision of a wide-coverage multilingual knowledgeresource.
We present a novel methodology thatproduces a very large multilingual semantic net-work: BabelNet.
This resource is created by link-ing Wikipedia to WordNet via an automatic map-ping and by integrating lexical gaps in resource-1http://download.wikipedia.org.
We use theEnglish Wikipedia database dump from November 3, 2009,which includes 3,083,466 articles.
Throughout this paper, weuse Sans Serif for words, SMALL CAPS for Wikipedia pagesand CAPITALS for Wikipedia categories.216high windblow gasgasbagwindhot-airballoongasclusterballooningMontgolfierbrothersFermi gasis-ahas-partis-ais-aWikipedia WordNetballoonBABEL SYNSETballoonEN, BallonDE,aerostatoES, globusCA,pallone aerostaticoIT,ballonFR, montgolfie`reFRWIKIPEDIA SENTENCES...world?s first hydrogen balloon flight....an interim balloon altitude record......from a British balloon near Be?court...+SEMCOR SENTENCES...look at the balloon and the......suspended like a huge balloon, in......the balloon would go up...Machine Translation systemFigure 1: An illustrative overview of BabelNet.poor languages with the aid of Machine Transla-tion.
The result is an ?encyclopedic dictionary?,that provides concepts and named entities lexical-ized in many languages and connected with largeamounts of semantic relations.2 BabelNetWe encode knowledge as a labeled directed graphG = (V,E) where V is the set of vertices ?
i.e.concepts2 such as balloon ?
andE ?
V ?R?V isthe set of edges connecting pairs of concepts.
Eachedge is labeled with a semantic relation from R,e.g.
{is-a, part-of , .
.
.
, }, where  denotes an un-specified semantic relation.
Importantly, each ver-tex v ?
V contains a set of lexicalizations of theconcept for different languages, e.g.
{ balloonEN,BallonDE, aerostatoES, .
.
.
, montgolfie`reFR }.Concepts and relations in BabelNet are har-vested from the largest available semantic lexi-con of English, WordNet, and a wide-coveragecollaboratively edited encyclopedia, the EnglishWikipedia (Section 3.1).
We collect (a) fromWordNet, all available word senses (as concepts)and all the semantic pointers between synsets (asrelations); (b) from Wikipedia, all encyclopedicentries (i.e.
pages, as concepts) and semanticallyunspecified relations from hyperlinked text.In order to provide a unified resource, we mergethe intersection of these two knowledge sources(i.e.
their concepts in common) by establishing amapping between Wikipedia pages and WordNetsenses (Section 3.2).
This avoids duplicate con-cepts and allows their inventories of concepts tocomplement each other.
Finally, to enable mul-tilinguality, we collect the lexical realizations ofthe available concepts in different languages by2Throughout the paper, unless otherwise stated, we usethe general term concept to denote either a concept or anamed entity.using (a) the human-generated translations pro-vided in Wikipedia (the so-called inter-languagelinks), as well as (b) a machine translation sys-tem to translate occurrences of the concepts withinsense-tagged corpora, namely SemCor (Miller etal., 1993) ?
a corpus annotated with WordNetsenses ?
and Wikipedia itself (Section 3.3).
Wecall the resulting set of multilingual lexicalizationsof a given concept a babel synset.
An overview ofBabelNet is given in Figure 1 (we label verticeswith English lexicalizations): unlabeled edges areobtained from links in the Wikipedia pages (e.g.BALLOON (AIRCRAFT) links to WIND), whereaslabeled ones from WordNet3 (e.g.
balloon1n has-part gasbag1n).
In this paper we restrict ourselvesto concepts lexicalized as nouns.
Nonetheless, ourmethodology can be applied to all parts of speech,but in that case Wikipedia cannot be exploited,since it mainly contains nominal entities.3 Methodology3.1 Knowledge ResourcesWordNet.
The most popular lexical knowledgeresource in the field of NLP is certainly WordNet,a computational lexicon of the English language.A concept in WordNet is represented as a synonymset (called synset), i.e.
the set of words that sharethe same meaning.
For instance, the concept windis expressed by the following synset:{ wind1n, air current1n, current of air1n },where each word?s subscripts and superscripts in-dicate their parts of speech (e.g.
n stands for noun)3We use in the following WordNet version 3.0.
We de-note with wip the i-th sense of a word w with part of speechp.
We use word senses to unambiguously denote the corre-sponding synsets (e.g.
plane1n for { airplane1n, aeroplane1n,plane1n }).
Hereafter, we use word sense and synset inter-changeably.217and sense number, respectively.
For each synset,WordNet provides a textual definition, or gloss.For example, the gloss of the above synset is: ?airmoving from an area of high pressure to an area oflow pressure?.Wikipedia.
Our second resource, Wikipedia,is a Web-based collaborative encyclopedia.
AWikipedia page (henceforth, Wikipage) presentsthe knowledge about a specific concept (e.g.
BAL-LOON (AIRCRAFT)) or named entity (e.g.
MONT-GOLFIER BROTHERS).
The page typically con-tains hypertext linked to other relevant Wikipages.For instance, BALLOON (AIRCRAFT) is linked toWIND, GAS, and so on.
The title of a Wikipage(e.g.
BALLOON (AIRCRAFT)) is composed ofthe lemma of the concept defined (e.g.
balloon)plus an optional label in parentheses which speci-fies its meaning if the lemma is ambiguous (e.g.AIRCRAFT vs. TOY).
Wikipages also provideinter-language links to their counterparts in otherlanguages (e.g.
BALLOON (AIRCRAFT) links tothe Spanish page AEROSTATO).
Finally, someWikipages are redirections to other pages, e.g.the Spanish BALO?N AEROSTA?TICO redirects toAEROSTATO.3.2 Mapping Wikipedia to WordNetThe first phase of our methodology aims to estab-lish links between Wikipages and WordNet senses.We aim to acquire a mapping ?
such that, for eachWikipage w, we have:?
(w) =??
?s ?
SensesWN(w) if a link can beestablished, otherwise,where SensesWN(w) is the set of senses of thelemma of w in WordNet.
For example, if our map-ping methodology linked BALLOON (AIRCRAFT)to the corresponding WordNet sense balloon1n,we would have ?
(BALLOON (AIRCRAFT)) = bal-loon1n.In order to establish a mapping between thetwo resources, we first identify the disambigua-tion contexts for Wikipages (Section 3.2.1) andWordNet senses (Section 3.2.2).
Next, we inter-sect these contexts to perform the mapping (seeSection 3.2.3).3.2.1 Disambiguation Context of a WikipageGiven a Wikipage w, we use the following infor-mation as disambiguation context:?
Sense labels: e.g.
given the page BALLOON(AIRCRAFT), the word aircraft is added to thedisambiguation context.?
Links: the titles?
lemmas of the pages linkedfrom the target Wikipage (i.e., outgoing links).For instance, the links in the Wikipage BAL-LOON (AIRCRAFT) include wind, gas, etc.?
Categories: Wikipages are typically classi-fied according to one or more categories.For example, the Wikipage BALLOON (AIR-CRAFT) is categorized as BALLOONS, BAL-LOONING, etc.
While many categories arevery specific and do not appear in Word-Net (e.g., SWEDISH WRITERS or SCIEN-TISTS WHO COMMITTED SUICIDE), weuse their syntactic heads as disambiguation con-text (i.e.
writer and scientist, respectively).Given a Wikipage w, we define its disambiguationcontext Ctx(w) as the set of words obtained fromall of the three sources above.3.2.2 Disambiguation Context of a WordNetSenseGiven a WordNet sense s and its synset S, we col-lect the following information:?
Synonymy: all synonyms of s in S. For in-stance, given the sense airplane1n and its cor-responding synset { airplane1n, aeroplane1n,plane1n }, the words contained therein are in-cluded in the context.?
Hypernymy/Hyponymy: all synonyms in thesynsets H such that H is either a hypernym(i.e., a generalization) or a hyponym (i.e., aspecialization) of S. For example, given bal-loon1n, we include the words from its hypernym{ lighter-than-air craft1n } and all its hyponyms(e.g.
{ hot-air balloon1n }).?
Sisterhood: words from the sisters of S. A sis-ter synset S?
is such that S and S?
have a com-mon direct hypernym.
For example, given bal-loon1n, it can be found that { balloon1n } and{ airship1n, dirigible1n } are sisters.
Thus air-ship and dirigible are included in the disam-biguation context of s.?
Gloss: the set of lemmas of the content wordsoccurring within the WordNet gloss of S.We thus define the disambiguation context Ctx(s)of sense s as the set of words obtained from all ofthe four sources above.2183.2.3 Mapping AlgorithmIn order to link each Wikipedia page to a WordNetsense, we perform the following steps:?
Initially, our mapping ?
is empty, i.e.
it linkseach Wikipage w to .?
For each Wikipage w whose lemma is monose-mous both in Wikipedia and WordNet we mapw to its only WordNet sense.?
For each remaining Wikipage w for which nomapping was previously found (i.e., ?
(w) = ),we assign the most likely sense to w based onthe maximization of the conditional probabili-ties p(s|w) over the senses s ?
SensesWN(w)(no mapping is established if a tie occurs).To find the mapping of a Wikipage w, we needto compute the conditional probability p(s|w) ofselecting the WordNet sense s given w. The senses which maximizes this probability is determinedas follows:?
(w) = argmaxs?SensesWN(w)p(s|w) = argmaxsp(s, w)p(w)= argmaxsp(s, w)The latter formula is obtained by observing thatp(w) does not influence our maximization, as it isa constant independent of s. As a result, determin-ing the most appropriate sense s consists of find-ing the sense s that maximizes the joint probabilityp(s, w).
We estimate p(s, w) as:p(s, w) =score(s, w)?s??SensesWN(w),w?
?SensesWiki(w)score(s?, w?
),where score(s, w) = |Ctx(s)?Ctx(w)|+ 1 (weadd 1 as a smoothing factor).
Thus, in our al-gorithm we determine the best sense s by com-puting the intersection of the disambiguation con-texts of s and w, and normalizing by the scoressummed over all senses of w in Wikipedia andWordNet.
More details on the mapping algorithmcan be found in Ponzetto and Navigli (2010).3.3 Translating Babel SynsetsSo far we have linked English Wikipages to Word-Net senses.
Given a Wikipage w, and provided itis mapped to a sense s (i.e., ?
(w) = s), we cre-ate a babel synset S ?W , where S is the WordNetsynset to which sense s belongs, and W includes:(i) w; (ii) all its inter-language links (that is, trans-lations of the Wikipage to other languages); (iii)the redirections to the inter-language links foundin the Wikipedia of the target language.
For in-stance, given that ?
(BALLOON) = balloon1n, thecorresponding babel synset is { balloonEN, Bal-lonDE, aerostatoES, balo?n aerosta?ticoES, .
.
.
,pallone aerostaticoIT }.
However, two issuesarise: first, a concept might be covered only inone of the two resources (either WordNet orWikipedia), meaning that no link can be estab-lished (e.g., FERMI GAS or gasbag1n in Figure1); second, even if covered in both resources, theWikipage for the concept might not provide anytranslation for the language of interest (e.g., theCatalan for BALLOON is missing in Wikipedia).In order to address the above issues and thusguarantee high coverage for all languages we de-veloped a methodology for translating senses inthe babel synset to missing languages.
Given aWordNet word sense in our babel synset of interest(e.g.
balloon1n) we collect its occurrences in Sem-Cor (Miller et al, 1993), a corpus of more than200,000 words annotated with WordNet senses.We do the same for Wikipages by retrieving sen-tences in Wikipedia with links to the Wikipage ofinterest.
By repeating this step for each Englishlexicalization in a babel synset, we obtain a col-lection of sentences for the babel synset (see leftpart of Figure 1).
Next, we apply state-of-the-artMachine Translation4 and translate the set of sen-tences in all the languages of interest.
Given a spe-cific term in the initial babel synset, we collect theset of its translations.
We then identify the mostfrequent translation in each language and add it tothe babel synset.
Note that translations are sense-specific, as the context in which a term occurs isprovided to the translation system.3.4 ExampleWe now illustrate the execution of our method-ology by way of an example.
Let us focus onthe Wikipage BALLOON (AIRCRAFT).
The wordis polysemous both in Wikipedia and WordNet.In the first phase of our methodology we aimto find a mapping ?
(BALLOON (AIRCRAFT)) toan appropriate WordNet sense of the word.
To4We use the Google Translate API.
An initial prototypeused a statistical machine translation system based on Moses(Koehn et al, 2007) and trained on Europarl (Koehn, 2005).However, we found such system unable to cope with manytechnical names, such as in the domains of sciences, litera-ture, history, etc.219this end we construct the disambiguation contextfor the Wikipage by including words from its la-bel, links and categories (cf.
Section 3.2.1).
Thecontext thus includes, among others, the follow-ing words: aircraft, wind, airship, lighter-than-air.
We now construct the disambiguation contextfor the two WordNet senses of balloon (cf.
Sec-tion 3.2.2), namely the aircraft (#1) and the toy(#2) senses.
To do so, we include words fromtheir synsets, hypernyms, hyponyms, sisters, andglosses.
The context for balloon1n includes: air-craft, craft, airship, lighter-than-air.
The con-text for balloon2n contains: toy, doll, hobby.
Thesense with the largest intersection is #1, so thefollowing mapping is established: ?
(BALLOON(AIRCRAFT)) = balloon1n.
After the first phase,our babel synset includes the following Englishwords from WordNet plus the Wikipedia inter-language links to other languages (we report Ger-man, Spanish and Italian): { balloonEN, BallonDE,aerostatoES, balo?n aerosta?ticoES, pallone aero-staticoIT }.In the second phase (see Section 3.3), we col-lect all the sentences in SemCor and Wikipedia inwhich the above English word sense occurs.
Wetranslate these sentences with the Google Trans-late API and select the most frequent transla-tion in each language.
As a result, we can en-rich the initial babel synset with the followingwords: mongolfie`reFR, globusCA, globoES, mon-golfieraIT.
Note that we had no translation forCatalan and French in the first phase, because theinter-language link was not available, and we alsoobtain new lexicalizations for the Spanish and Ital-ian languages.4 Experiment 1: Mapping EvaluationExperimental setting.
We first performed anevaluation of the quality of our mapping fromWikipedia to WordNet.
To create a gold stan-dard for evaluation we considered all lemmaswhose senses are contained both in WordNet andWikipedia: the intersection between the two re-sources contains 80,295 lemmas which corre-spond to 105,797 WordNet senses and 199,735Wikipedia pages.
The average polysemy is 1.3and 2.5 for WordNet senses and Wikipages, re-spectively (2.8 and 4.7 when excluding monose-mous words).
We then selected a random sam-ple of 1,000 Wikipages and asked an annotatorwith previous experience in lexicographic annota-P R F1 AMapping algorithm 81.9 77.5 79.6 84.4MFS BL 24.3 47.8 32.2 24.3Random BL 23.8 46.8 31.6 23.9Table 1: Performance of the mapping algorithm.tion to provide the correct WordNet sense for eachpage (an empty sense label was given, if no correctmapping was possible).
The gold-standard datasetincludes 505 non-empty mappings, i.e.
Wikipageswith a corresponding WordNet sense.
In order toquantify the quality of the annotations and the dif-ficulty of the task, a second annotator sense taggeda subset of 200 pages from the original sample.Our annotators achieved a ?
inter-annotator agree-ment (Carletta, 1996) of 0.9, indicating almostperfect agreement.Results and discussion.
Table 1 summarizes theperformance of our mapping algorithm againstthe manually annotated dataset.
Evaluation is per-formed in terms of standard measures of preci-sion, recall, and F1-measure.
In addition we calcu-late accuracy, which also takes into account emptysense labels.
As baselines we use the most fre-quent WordNet sense (MFS), and a random senseassignment.The results show that our method achieves al-most 80% F1 and it improves over the baselines bya large margin.
The final mapping contains 81,533pairs of Wikipages and word senses they map to,covering 55.7% of the noun senses in WordNet.As for the baselines, the most frequent sense isjust 0.6% and 0.4% above the random baseline interms of F1 and accuracy, respectively.
A ?2 testreveals in fact no statistical significant differenceat p < 0.05.
This is related to the random distri-bution of senses in our dataset and the Wikipediaunbiased coverage of WordNet senses.
So select-ing the first WordNet sense rather than any othersense for each target page represents a choice asarbitrary as picking a sense at random.5 Experiment 2: Translation EvaluationWe perform a second set of experiments concern-ing the quality of the acquired concepts.
This is as-sessed in terms of coverage against gold-standardresources (Section 5.1) and against a manually-validated dataset of translations (Section 5.2).220Language Word senses SynsetsGerman 15,762 9,877Spanish 83,114 55,365Catalan 64,171 40,466Italian 57,255 32,156French 44,265 31,742Table 2: Size of the gold-standard wordnets.5.1 Automatic EvaluationDatasets.
We compare BabelNet against gold-standard resources for 5 languages, namely: thesubset of GermaNet (Lemnitzer and Kunze, 2002)included in EuroWordNet for German, Multi-WordNet (Pianta et al, 2002) for Italian, the Mul-tilingual Central Repository for Spanish and Cata-lan (Atserias et al, 2004), and WOrdnet Libredu Franc?ais (Beno?
?t and Fis?er, 2008, WOLF) forFrench.
In Table 2 we report the number of synsetsand word senses available in the gold-standard re-sources for the 5 languages.Measures.
Let B be BabelNet, F our gold-standard non-English wordnet (e.g.
GermaNet),and let E be the English WordNet.
All the gold-standard non-English resources, as well as Babel-Net, are linked to the English WordNet: given asynset SF ?
F , we denote its corresponding babelsynset as SB and its synset in the English Word-Net as SE .
We assess the coverage of BabelNetagainst our gold-standard wordnets both in termsof synsets and word senses.
For synsets, we calcu-late coverage as follows:SynsetCov(B,F) =?SF?F?
(SB, SF )|{SF ?
F}|,where ?
(SB, SF ) = 1 if the two synsets SB andSF have a synonym in common, 0 otherwise.
Thatis, synset coverage is determined as the percentageof synsets of F that share a term with the corre-sponding babel synsets.
For word senses we cal-culate a similar measure of coverage:WordCov(B,F) =?SF?F?sF?SF??
(sF , SB)|{sF ?
SF : SF ?
F}|,where sF is a word sense in synset SF and??
(sF , SB) = 1 if sF ?
SB, 0 otherwise.
Thatis we calculate the ratio of word senses in ourgold-standard resource F that also occur in thecorresponding synset SB to the overall number ofsenses in F .However, our gold-standard resources coveronly a portion of the English WordNet, whereasthe overall coverage of BabelNet is much higher.We calculate extra coverage for synsets as follows:SynsetExtraCov(B,F) =?SE?E\F?
(SB, SE)|{SF ?
F}|.Similarly, we calculate extra coverage for wordsenses in BabelNet corresponding to WordNetsynsets not covered by the reference resource F .Results and discussion.
We evaluate the cov-erage and extra coverage of word senses andsynsets at different stages: (a) using only the inter-language links from Wikipedia (WIKI Links); (b)and (c) using only the automatic translations of thesentences from Wikipedia (WIKI Transl.)
or Sem-Cor (WN Transl.
); (d) using all available transla-tions, i.e.
BABELNET.Coverage results are reported in Table 3.
Thepercentage of word senses covered by BabelNetranges from 52.9% (Italian) to 66.4 (Spanish)and 86.0% (French).
Synset coverage ranges from73.3% (Catalan) to 76.6% (Spanish) and 92.9%(French).
As expected, synset coverage is higher,because a synset in the reference resource is con-sidered to be covered if it shares at least one wordwith the corresponding synset in BabelNet.Numbers for the extra coverage, which pro-vides information about the percentage of wordsenses and synsets in BabelNet but not in the gold-standard resources, are given in Figure 2.
The re-sults show that we provide for all languages a highextra coverage for both word senses ?
between340.1% (Catalan) and 2,298% (German) ?
andsynsets ?
between 102.8% (Spanish) and 902.6%(German).Table 3 and Figure 2 show that the best resultsare obtained when combining all available trans-lations, i.e.
both from Wikipedia and the machinetranslation system.
The performance figures suf-fer from the errors of the mapping phase (see Sec-tion 4).
Nonetheless, the results are generally high,with a peak for French, since WOLF has been cre-ated semi-automatically by combining several re-sources, including Wikipedia.
The relatively lowword sense coverage for Italian (55.4%) is, in-stead, due to the lack of many common words inthe Italian gold-standard synsets.
Examples in-clude whipEN translated as staffileIT but not as themore common frustaIT, playboyEN translated asvitaioloIT but not gigolo`IT, etc.2210%?500%?1000%?1500%?2000%?2500%?German?
Spanish?
Catalan?
Italian?
French?Wiki?Links?Wiki?Transl.?WN??Transl.?BabelNet?
(a) word senses0%?100%?200%?300%?400%?500%?600%?700%?800%?900%?1000%?German?
Spanish?
Catalan?
Italian?
French?Wiki?Links?Wiki?Transl.?WN??Transl.?BabelNet?
(b) synsetsFigure 2: Extra coverage against gold-standard wordnets: word senses (a) and synsets (b).Resource Method SENSES SYNSETSGerman WIKI{ Links 39.6 50.7Transl.
42.6 58.2WN Transl.
21.0 28.6BABELNET All 57.6 73.4Spanish WIKI{ Links 34.4 40.7Transl.
47.9 56.1WN Transl.
25.2 30.0BABELNET All 66.4 76.6CatalanWIKI{ Links 20.3 25.2Transl.
46.9 54.1WN Transl.
25.0 29.6BABELNET All 64.0 73.3ItalianWIKI{ Links 28.1 40.0Transl.
39.9 58.0WN Transl.
19.7 28.7BABELNET All 52.9 73.7French WIKI{ Links 70.0 72.4Transl.
69.6 79.6WN Transl.
16.3 19.4BABELNET All 86.0 92.9Table 3: Coverage against gold-standard wordnets(we report percentages).5.2 Manual EvaluationExperimental setup.
The automatic evaluationquantifies how much of the gold-standard re-sources is covered by BabelNet.
However, itdoes not say anything about the precision of theadditional lexicalizations provided by BabelNet.Given that our resource has displayed a remark-ably high extra coverage ?
ranging from 340%to 2,298% of the national wordnets (see Figure2) ?
we performed a second evaluation to assessits precision.
For each of our 5 languages, weselected a random set of 600 babel synsets com-posed as follows: 200 synsets whose senses ex-ist in WordNet only, 200 synsets in the intersec-tion between WordNet and Wikipedia (i.e.
thosemapped with our method illustrated in Section3.2), 200 synsets whose lexicalizations exist inWikipedia only.
Therefore, our dataset included600?
5 = 3,000 babel synsets.
None of the synsetswas covered by any of the five reference wordnets.The babel synsets were manually validated by ex-pert annotators who decided which senses (i.e.lexicalizations) were appropriate given the corre-sponding WordNet gloss and/or Wikipage.Results and discussion.
We report the results inTable 4.
For each language (rows) and for eachof the three regions of BabelNet (columns), wereport precision (i.e.
the percentage of synonymsdeemed correct) and, in parentheses, the over-all number of synonyms evaluated.
The resultsshow that the different regions of BabelNet con-tain translations of different quality: while on av-erage translations for WordNet-only synsets havea precision around 72%, when Wikipedia comesinto play the performance increases considerably(around 80% in the intersection and 95% withWikipedia-only translations).
As can be seen fromthe figures in parentheses, the number of trans-lations available in the presence of Wikipedia ishigher.
This quantitative difference is due to ourmethod collecting many translations from the redi-rections in the Wikipedia of the target language(Section 3.3), as well as to the paucity of examplesin SemCor for many synsets.
In addition, some ofthe synsets in WordNet with no Wikipedia coun-terpart are very difficult to translate.
Examplesinclude terms like stammel, crape fern, base-ball clinic, and many others for which we could222Language WN WN ?Wiki WikiGerman 73.76 (282) 78.37 (777) 97.74 (709)Spanish 69.45 (275) 78.53 (643) 92.46 (703)Catalan 75.58 (258) 82.98 (517) 92.71 (398)Italian 72.32 (271) 80.83 (574) 99.09 (552)French 67.16 (268) 77.43 (709) 96.44 (758)Table 4: Precision of BabelNet on synonyms inWordNet (WN), Wikipedia (Wiki) and their inter-section (WN ?
Wiki): percentage and total num-ber of words (in parentheses) are reported.not find translations in major editions of bilingualdictionaries.
In contrast, good translations wereproduced using our machine translation methodwhen enough sentences were available.
Examplesare: chaudre?e de poissonFR for fish chowderEN,grano de cafe?ES for coffee beanEN, etc.6 Related WorkPrevious attempts to manually build multilingualresources have led to the creation of a multi-tude of wordnets such as EuroWordNet (Vossen,1998), MultiWordNet (Pianta et al, 2002), Balka-Net (Tufis?
et al, 2004), Arabic WordNet (Blacket al, 2006), the Multilingual Central Repository(Atserias et al, 2004), bilingual electronic dic-tionaries such as EDR (Yokoi, 1995), and fully-fledged frameworks for the development of multi-lingual lexicons (Lenci et al, 2000).
As it is of-ten the case with manually assembled resources,these lexical knowledge repositories are hinderedby high development costs and an insufficient cov-erage.
This barrier has led to proposals that ac-quire multilingual lexicons from either paralleltext (Gale and Church, 1993; Fung, 1995, interalia) or monolingual corpora (Sammer and Soder-land, 2007; Haghighi et al, 2008).
The disam-biguation of bilingual dictionary glosses has alsobeen proposed to create a bilingual semantic net-work from a machine readable dictionary (Nav-igli, 2009a).
Recently, Etzioni et al (2007) andMausam et al (2009) presented methods to pro-duce massive multilingual translation dictionariesfrom Web resources such as online lexicons andWiktionaries.
However, while providing lexicalresources on a very large scale for hundreds ofthousands of language pairs, these do not encodesemantic relations between concepts denoted bytheir lexical entries.The research closest to ours is presented by deMelo and Weikum (2009), who developed a Uni-versal WordNet (UWN) by automatically acquir-ing a semantic network for languages other thanEnglish.
UWN is bootstrapped from WordNet andis built by collecting evidence extracted from ex-isting wordnets, translation dictionaries, and par-allel corpora.
The result is a graph containing800,000 words from over 200 languages in a hier-archically structured semantic network with over1.5 million links from words to word senses.
Ourwork goes one step further by (1) developing aneven larger multilingual resource including bothlexical semantic and encyclopedic knowledge, (2)enriching the structure of the ?core?
semantic net-work (i.e.
the semantic pointers from WordNet)with topical, semantically unspecified relationsfrom the link structure of Wikipedia.
This resultis essentially achieved by complementing Word-Net with Wikipedia, as well as by leveraging themultilingual structure of the latter.
Previous at-tempts at linking the two resources have been pro-posed.
These include associating Wikipedia pageswith the most frequent WordNet sense (Suchaneket al, 2008), extracting domain information fromWikipedia and providing a manual mapping toWordNet concepts (Auer et al, 2007), a modelbased on vector spaces (Ruiz-Casado et al, 2005),a supervised approach using keyword extraction(Reiter et al, 2008), as well as automaticallylinking Wikipedia categories to WordNet basedon structural information (Ponzetto and Navigli,2009).
In contrast to previous work, BabelNetis the first proposal that integrates the relationalstructure of WordNet with the semi-structured in-formation from Wikipedia into a unified, wide-coverage, multilingual semantic network.7 ConclusionsIn this paper we have presented a novel methodol-ogy for the automatic construction of a large multi-lingual lexical knowledge resource.
Key to our ap-proach is the establishment of a mapping betweena multilingual encyclopedic knowledge repository(Wikipedia) and a computational lexicon of En-glish (WordNet).
This integration process hasseveral advantages.
Firstly, the two resourcescontribute different kinds of lexical knowledge,one is concerned mostly with named entities, theother with concepts.
Secondly, while Wikipediais less structured than WordNet, it provides large223amounts of semantic relations and can be lever-aged to enable multilinguality.
Thus, even whenthey overlap, the two resources provide comple-mentary information about the same named enti-ties or concepts.
Further, we contribute a largeset of sense occurrences harvested from Wikipediaand SemCor, a corpus that we input to a state-of-the-art machine translation system to fill in the gapbetween resource-rich languages ?
such as English?
and resource-poorer ones.
Our hope is that theavailability of such a language-rich resource5 willenable many non-English and multilingual NLPapplications to be developed.Our experiments show that our fully-automatedapproach produces a large-scale lexical resourcewith high accuracy.
The resource includes millionsof semantic relations, mainly from Wikipedia(however, WordNet relations are labeled), andcontains almost 3 million concepts (6.7 labels perconcept on average).
As pointed out in Section5, such coverage is much wider than that of ex-isting wordnets in non-English languages.
WhileBabelNet currently includes 6 languages, links tofreely-available wordnets6 can immediately be es-tablished by utilizing the English WordNet as aninterlanguage index.
Indeed, BabelNet can be ex-tended to virtually any language of interest.
Infact, our translation method allows it to cope withany resource-poor language.As future work, we plan to apply our methodto other languages, including Eastern European,Arabic, and Asian languages.
We also intend tolink missing concepts in WordNet, by establish-ing their most likely hypernyms ?
e.g., a` la Snowet al (2006).
We will perform a semi-automaticvalidation of BabelNet, e.g.
by exploiting Ama-zon?s Mechanical Turk (Callison-Burch, 2009) ordesigning a collaborative game (von Ahn, 2006)to validate low-ranking mappings and translations.Finally, we aim to apply BabelNet to a variety ofapplications which are known to benefit from awide-coverage knowledge resource.
We have al-ready shown that the English-only subset of Ba-belNet alows simple knowledge-based algorithmsto compete with supervised systems in standardcoarse-grained and domain-specific WSD settings(Ponzetto and Navigli, 2010).
We plan in the nearfuture to apply BabelNet to the challenging task ofcross-lingual WSD (Lefever and Hoste, 2009).5BabelNet can be freely downloaded for research pur-poses at http://lcl.uniroma1.it/babelnet.6http://www.globalwordnet.org.ReferencesJordi Atserias, Luis Villarejo, German Rigau, EnekoAgirre, John Carroll, Bernardo Magnini, and PiekVossen.
2004.
The MEANING multilingual centralrepository.
In Proc.
of GWC-04, pages 80?210.So?ren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary Ive.2007.
Dbpedia: A nucleus for a web of open data.In Proceedings of 6th International Semantic WebConference joint with 2nd Asian Semantic Web Con-ference (ISWC+ASWC 2007), pages 722?735.Sagot Beno?
?t and Darja Fis?er.
2008.
Building a freeFrench WordNet from multilingual resources.
InProceedings of the Ontolex 2008 Workshop.William Black, Sabri Elkateb Horacio Rodriguez,Musa Alkhalifa, Piek Vossen, and Adam Pease.2006.
Introducing the Arabic WordNet project.
InProc.
of GWC-06, pages 295?299.Razvan Bunescu and Marius Pas?ca.
2006.
Using en-cyclopedic knowledge for named entity disambigua-tion.
In Proc.
of EACL-06, pages 9?16.Chris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazon?s Me-chanical Turk.
In Proc.
of EMNLP-09, pages 286?295.Jean Carletta.
1996.
Assessing agreement on classi-fication tasks: The kappa statistic.
ComputationalLinguistics, 22(2):249?254.Montse Cuadros and German Rigau.
2006.
Qualityassessment of large scale knowledge resources.
InProc.
of EMNLP-06, pages 534?541.Gerard de Melo and Gerhard Weikum.
2009.
Towardsa universal wordnet by learning from combined evi-dence.
In Proc.
of CIKM-09, pages 513?522.Oren Etzioni, Kobi Reiter, Stephen Soderland, andMarcus Sammer.
2007.
Lexical translation with ap-plication to image search on the Web.
In Proceed-ings of Machine Translation Summit XI.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Database.
MIT Press, Cambridge, MA.Pascale Fung.
1995.
A pattern matching methodfor finding noun and proper noun translations fromnoisy parallel corpora.
In Proc.
of ACL-95, pages236?243.Evgeniy Gabrilovich and Shaul Markovitch.
2006.Overcoming the brittleness bottleneck usingWikipedia: Enhancing text categorization withencyclopedic knowledge.
In Proc.
of AAAI-06,pages 1301?1306.William A. Gale and Kenneth W. Church.
1993.
Aprogram for aligning sentences in bilingual corpora.Computational Linguistics, 19(1):75?102.Jim Giles.
2005.
Internet encyclopedias go head tohead.
Nature, 438:900?901.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proc.
of ACL-08,pages 771?779.224Sanda M. Harabagiu, Dan Moldovan, Marius Pas?ca,Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu,Roxana Girju, Vasile Rus, and Paul Morarescu.2000.
FALCON: Boosting knowledge for answerengines.
In Proc.
of TREC-9, pages 479?488.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InComp.
Vol.
to Proc.
of ACL-07, pages 177?180.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof Machine Translation Summit X.Els Lefever and Veronique Hoste.
2009.
Semeval-2010 task 3: Cross-lingual Word Sense Disambigua-tion.
In Proc.
of the Workshop on Semantic Evalu-ations: Recent Achievements and Future Directions(SEW-2009), pages 82?87, Boulder, Colorado.Lothar Lemnitzer and Claudia Kunze.
2002.
Ger-maNet ?
representation, visualization, application.In Proc.
of LREC ?02, pages 1485?1491.Alessandro Lenci, Nuria Bel, Federica Busa, Nico-letta Calzolari, Elisabetta Gola, Monica Monachini,Antoine Ogonowski, Ivonne Peters, Wim Peters,Nilda Ruimy, Marta Villegas, and Antonio Zam-polli.
2000.
SIMPLE: A general framework for thedevelopment of multilingual lexicons.
InternationalJournal of Lexicography, 13(4):249?263.Mausam, Stephen Soderland, Oren Etzioni, DanielWeld, Michael Skinner, and Jeff Bilmes.
2009.Compiling a massive, multilingual dictionary viaprobabilistic inference.
In Proc.
of ACL-IJCNLP-09, pages 262?270.Olena Medelyan, David Milne, Catherine Legg, andIan H. Witten.
2009.
Mining meaning fromWikipedia.
Int.
J. Hum.-Comput.
Stud., 67(9):716?754.George A. Miller, Claudia Leacock, Randee Tengi, andRoss Bunker.
1993.
A semantic concordance.
InProceedings of the 3rd DARPA Workshop on HumanLanguage Technology, pages 303?308, Plainsboro,N.J.Vivi Nastase.
2008.
Topic-driven multi-documentsummarization with encyclopedic knowledge andactivation spreading.
In Proc.
of EMNLP-08, pages763?772.Roberto Navigli and Mirella Lapata.
2010.
An ex-perimental study on graph connectivity for unsuper-vised Word Sense Disambiguation.
IEEE Transac-tions on Pattern Anaylsis and Machine Intelligence,32(4):678?692.Roberto Navigli.
2009a.
Using cycles and quasi-cycles to disambiguate dictionary glosses.
In Proc.of EACL-09, pages 594?602.Roberto Navigli.
2009b.
Word Sense Disambiguation:A survey.
ACM Computing Surveys, 41(2):1?69.Hwee Tou Ng and Hian Beng Lee.
1996.
Integratingmultiple knowledge sources to disambiguate wordsenses: An exemplar-based approach.
In Proc.
ofACL-96, pages 40?47.Emanuele Pianta, Luisa Bentivogli, and Christian Gi-rardi.
2002.
MultiWordNet: Developing an alignedmultilingual database.
In Proc.
of GWC-02, pages21?25.Simone Paolo Ponzetto and Roberto Navigli.
2009.Large-scale taxonomy mapping for restructuringand integrating Wikipedia.
In Proc.
of IJCAI-09,pages 2083?2088.Simone Paolo Ponzetto and Roberto Navigli.
2010.Knowledge-rich Word Sense Disambiguation rival-ing supervised system.
In Proc.
of ACL-10.Simone Paolo Ponzetto and Michael Strube.
2007.
De-riving a large scale taxonomy from Wikipedia.
InProc.
of AAAI-07, pages 1440?1445.Nils Reiter, Matthias Hartung, and Anette Frank.2008.
A resource-poor approach for linking ontol-ogy classes to Wikipedia articles.
In Johan Bos andRodolfo Delmonte, editors, Semantics in Text Pro-cessing, volume 1 of Research in Computational Se-mantics, pages 381?387.
College Publications, Lon-don, England.Maria Ruiz-Casado, Enrique Alfonseca, and PabloCastells.
2005.
Automatic assignment of Wikipediaencyclopedic entries to WordNet synsets.
In Ad-vances in Web Intelligence, volume 3528 of LectureNotes in Computer Science.
Springer Verlag.Marcus Sammer and Stephen Soderland.
2007.
Build-ing a sense-distinguished multilingual lexicon frommonolingual corpora and bilingual lexicons.
In Pro-ceedings of Machine Translation Summit XI.Rion Snow, Dan Jurafsky, and Andrew Ng.
2006.
Se-mantic taxonomy induction from heterogeneous ev-idence.
In Proc.
of COLING-ACL-06, pages 801?808.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2008.
Yago: A large ontology fromWikipedia and WordNet.
Journal of Web Semantics,6(3):203?217.Dan Tufis?, Dan Cristea, and Sofia Stamou.
2004.BalkaNet: Aims, methods, results and perspectives.a general overview.
Romanian Journal on Scienceand Technology of Information, 7(1-2):9?43.Luis von Ahn.
2006.
Games with a purpose.
IEEEComputer, 6(39):92?94.Piek Vossen, editor.
1998.
EuroWordNet: A Multi-lingual Database with Lexical Semantic Networks.Kluwer, Dordrecht, The Netherlands.Fei Wu and Daniel Weld.
2007.
Automatically se-mantifying Wikipedia.
In Proc.
of CIKM-07, pages41?50.David Yarowsky and Radu Florian.
2002.
Evaluat-ing sense disambiguation across diverse parameterspaces.
Natural Language Engineering, 9(4):293?310.Toshio Yokoi.
1995.
The EDR electronic dictionary.Communications of the ACM, 38(11):42?44.225
