Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 33?40,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsWord Sense Disambiguation Improves Statistical Machine TranslationYee Seng Chan and Hwee Tou NgDepartment of Computer ScienceNational University of Singapore3 Science Drive 2Singapore 117543{chanys, nght}@comp.nus.edu.sgDavid ChiangInformation Sciences InstituteUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292, USAchiang@isi.eduAbstractRecent research presents conflicting evi-dence on whether word sense disambigua-tion (WSD) systems can help to improve theperformance of statistical machine transla-tion (MT) systems.
In this paper, we suc-cessfully integrate a state-of-the-art WSDsystem into a state-of-the-art hierarchicalphrase-based MT system, Hiero.
We showfor the first time that integrating a WSD sys-tem improves the performance of a state-of-the-art statistical MT system on an actualtranslation task.
Furthermore, the improve-ment is statistically significant.1 IntroductionMany words have multiple meanings, depending onthe context in which they are used.
Word sense dis-ambiguation (WSD) is the task of determining thecorrect meaning or sense of a word in context.
WSDis regarded as an important research problem and isassumed to be helpful for applications such as ma-chine translation (MT) and information retrieval.In translation, different senses of a word w in asource language may have different translations in atarget language, depending on the particular mean-ing of w in context.
Hence, the assumption is thatin resolving sense ambiguity, a WSD system will beable to help an MT system to determine the correcttranslation for an ambiguous word.
To determine thecorrect sense of a word, WSD systems typically usea wide array of features that are not limited to the lo-cal context of w, and some of these features may notbe used by state-of-the-art statistical MT systems.To perform translation, state-of-the-art MT sys-tems use a statistical phrase-based approach (Marcuand Wong, 2002; Koehn et al, 2003; Och andNey, 2004) by treating phrases as the basic unitsof translation.
In this approach, a phrase can beany sequence of consecutive words and is not nec-essarily linguistically meaningful.
Capitalizing onthe strength of the phrase-based approach, Chiang(2005) introduced a hierarchical phrase-based sta-tistical MT system, Hiero, which achieves signifi-cantly better translation performance than Pharaoh(Koehn, 2004a), which is a state-of-the-art phrase-based statistical MT system.Recently, some researchers investigated whetherperforming WSD will help to improve the perfor-mance of an MT system.
Carpuat and Wu (2005)integrated the translation predictions from a ChineseWSD system (Carpuat et al, 2004) into a Chinese-English word-based statistical MT system using theISI ReWrite decoder (Germann, 2003).
Though theyacknowledged that directly using English transla-tions as word senses would be ideal, they insteadpredicted the HowNet sense of a word and then usedthe English gloss of the HowNet sense as the WSDmodel?s predicted translation.
They did not incor-porate their WSD model or its predictions into theirtranslation model; rather, they used the WSD pre-dictions either to constrain the options available totheir decoder, or to postedit the output of their de-coder.
They reported the negative result that WSDdecreased the performance of MT based on their ex-periments.In another work (Vickrey et al, 2005), the WSDproblem was recast as a word translation task.
The33translation choices for a word w were defined as theset of words or phrases aligned to w, as gatheredfrom a word-aligned parallel corpus.
The authorsshowed that they were able to improve their model?saccuracy on two simplified translation tasks: wordtranslation and blank-filling.Recently, Cabezas and Resnik (2005) experi-mented with incorporating WSD translations intoPharaoh, a state-of-the-art phrase-based MT sys-tem (Koehn et al, 2003).
Their WSD system pro-vided additional translations to the phrase table ofPharaoh, which fired a new model feature, so thatthe decoder could weigh the additional alternativetranslations against its own.
However, they couldnot automatically tune the weight of this feature inthe same way as the others.
They obtained a rela-tively small improvement, and no statistical signifi-cance test was reported to determine if the improve-ment was statistically significant.Note that the experiments in (Carpuat and Wu,2005) did not use a state-of-the-art MT system,while the experiments in (Vickrey et al, 2005) werenot done using a full-fledged MT system and theevaluation was not on how well each source sentencewas translated as a whole.
The relatively small im-provement reported by Cabezas and Resnik (2005)without a statistical significance test appears to beinconclusive.
Considering the conflicting results re-ported by prior work, it is not clear whether a WSDsystem can help to improve the performance of astate-of-the-art statistical MT system.In this paper, we successfully integrate a state-of-the-art WSD system into the state-of-the-art hi-erarchical phrase-based MT system, Hiero (Chiang,2005).
The integration is accomplished by introduc-ing two additional features into the MT model whichoperate on the existing rules of the grammar, with-out introducing competing rules.
These features aretreated, both in feature-weight tuning and in decod-ing, on the same footing as the rest of the model,allowing it to weigh the WSD model predictionsagainst other pieces of evidence so as to optimizetranslation accuracy (as measured by BLEU).
Thecontribution of our work lies in showing for the firsttime that integrating a WSD system significantly im-proves the performance of a state-of-the-art statisti-cal MT system on an actual translation task.In the next section, we describe our WSD system.Then, in Section 3, we describe the Hiero MT sys-tem and introduce the two new features used to inte-grate the WSD system into Hiero.
In Section 4, wedescribe the training data used by the WSD system.In Section 5, we describe how the WSD translationsprovided are used by the decoder of the MT system.In Section 6 and 7, we present and analyze our ex-perimental results, before concluding in Section 8.2 Word Sense DisambiguationPrior research has shown that using Support VectorMachines (SVM) as the learning algorithm for WSDachieves good results (Lee and Ng, 2002).
For ourexperiments, we use the SVM implementation of(Chang and Lin, 2001) as it is able to work on multi-class problems to output the classification probabil-ity for each class.Our implemented WSD classifier uses the knowl-edge sources of local collocations, parts-of-speech(POS), and surrounding words, following the suc-cessful approach of (Lee and Ng, 2002).
For localcollocations, we use 3 features, w?1w+1, w?1, andw+1, where w?1 (w+1) is the token immediately tothe left (right) of the current ambiguous word oc-currence w. For parts-of-speech, we use 3 features,P?1, P0, and P+1, where P0 is the POS of w, andP?1 (P+1) is the POS of w?1 (w+1).
For surround-ing words, we consider all unigrams (single words)in the surrounding context of w. These unigrams canbe in a different sentence from w. We perform fea-ture selection on surrounding words by including aunigram only if it occurs 3 or more times in somesense of w in the training data.To measure the accuracy of our WSD classifier,we evaluate it on the test data of SENSEVAL-3 Chi-nese lexical-sample task.
We obtain accuracy thatcompares favorably to the best participating systemin the task (Carpuat et al, 2004).3 HieroHiero (Chiang, 2005) is a hierarchical phrase-basedmodel for statistical machine translation, based onweighted synchronous context-free grammar (CFG)(Lewis and Stearns, 1968).
A synchronous CFGconsists of rewrite rules such as the following:X ?
?
?, ??
(1)34where X is a non-terminal symbol, ?
(?)
is a stringof terminal and non-terminal symbols in the source(target) language, and there is a one-to-one corre-spondence between the non-terminals in ?
and ?
in-dicated by co-indexation.
Hence, ?
and ?
alwayshave the same number of non-terminal symbols.
Forinstance, we could have the following grammar rule:X ?
??
?t X 1 , go to X 1 every month to?
(2)where boxed indices represent the correspondencesbetween non-terminal symbols.Hiero extracts the synchronous CFG rules auto-matically from a word-aligned parallel corpus.
Totranslate a source sentence, the goal is to find itsmost probable derivation using the extracted gram-mar rules.
Hiero uses a general log-linear model(Och and Ney, 2002) where the weight of a deriva-tion D for a particular source sentence and its trans-lation isw(D) =?i?i(D)?i (3)where ?i is a feature function and ?i is the weight forfeature ?i.
To ensure efficient decoding, the ?i aresubject to certain locality restrictions.
Essentially,they should be defined as products of functions de-fined on isolated synchronous CGF rules; however,it is possible to extend the domain of locality ofthe features somewhat.
A n-gram language modeladds a dependence on (n?1) neighboring target-sidewords (Wu, 1996; Chiang, 2007), making decodingmuch more difficult but still polynomial; in this pa-per, we add features that depend on the neighboringsource-side words, which does not affect decodingcomplexity at all because the source string is fixed.In principle we could add features that depend onarbitrary source-side context.3.1 New Features in Hiero for WSDTo incorporate WSD into Hiero, we use the trans-lations proposed by the WSD system to help Hieroobtain a better or more probable derivation duringthe translation of each source sentence.
To achievethis, when a grammar rule R is considered duringdecoding, and we recognize that some of the ter-minal symbols (words) in ?
are also chosen by theWSD system as translations for some terminal sym-bols (words) in ?, we compute the following fea-tures:?
Pwsd(t | s) gives the contextual probability ofthe WSD classifier choosing t as a translationfor s, where t (s) is some substring of terminalsymbols in ?
(?).
Because this probability onlyapplies to some rules, and we don?t want to pe-nalize those rules, we must add another feature,?
Ptywsd = exp(?|t|), where t is the translationchosen by the WSD system.
This feature, witha negative weight, rewards rules that use trans-lations suggested by the WSD module.Note that we can take the negative logarithm ofthe rule/derivation weights and think of them ascosts rather than probabilities.4 Gathering Training Examples for WSDOur experiments were for Chinese to English trans-lation.
Hence, in the context of our work, a syn-chronous CFG grammar rule X ?
?
?, ??
gatheredby Hiero consists of a Chinese portion ?
and a cor-responding English portion ?, where each portion isa sequence of words and non-terminal symbols.Our WSD classifier suggests a list of Englishphrases (where each phrase consists of one or moreEnglish words) with associated contextual probabil-ities as possible translations for each particular Chi-nese phrase.
In general, the Chinese phrase mayconsist of k Chinese words, where k = 1, 2, 3, .
.
..However, we limit k to 1 or 2 for experiments re-ported in this paper.
Future work can explore en-larging k.Whenever Hiero is about to extract a grammarrule where its Chinese portion is a phrase of one ortwo Chinese words with no non-terminal symbols,we note the location (sentence and token offset) inthe Chinese half of the parallel corpus from whichthe Chinese portion of the rule is extracted.
The ac-tual sentence in the corpus containing the Chinesephrase, and the one sentence before and the one sen-tence after that actual sentence, will serve as the con-text for one training example for the Chinese phrase,with the corresponding English phrase of the gram-mar rule as its translation.
Hence, unlike traditionalWSD where the sense classes are tied to a specificsense inventory, our ?senses?
here consist of the En-glish phrases extracted as translations for each Chi-nese phrase.
Since the extracted training data may35be noisy, for each Chinese phrase, we remove En-glish translations that occur only once.
Furthermore,we only attempt WSD classification for those Chi-nese phrases with at least 10 training examples.Using the WSD classifier described in Section 2,we classified the words in each Chinese source sen-tence to be translated.
We first performed WSD onall single Chinese words which are either noun, verb,or adjective.
Next, we classified the Chinese phrasesconsisting of 2 consecutive Chinese words by simplytreating the phrase as a single unit.
When perform-ing classification, we give as output the set of En-glish translations with associated context-dependentprobabilities, which are the probabilities of a Chi-nese word (phrase) translating into each Englishphrase, depending on the context of the Chineseword (phrase).
After WSD, the ith word ci in everyChinese sentence may have up to 3 sets of associ-ated translations provided by the WSD system: a setof translations for ci as a single word, a second setof translations for ci?1ci considered as a single unit,and a third set of translations for cici+1 consideredas a single unit.5 Incorporating WSD during DecodingThe following tasks are done for each rule that isconsidered during decoding:?
identify Chinese words to suggest translationsfor?
match suggested translations against the En-glish side of the rule?
compute features for the ruleThe WSD system is able to predict translationsonly for a subset of Chinese words or phrases.Hence, we must first identify which parts of theChinese side of the rule have suggested translationsavailable.
Here, we consider substrings of length upto two, and we give priority to longer substrings.Next, we want to know, for each Chinese sub-string considered, whether the WSD system sup-ports the Chinese-English translation represented bythe rule.
If the rule is finally chosen as part of thebest derivation for translating the Chinese sentence,then all the words in the English side of the rule willappear in the translated English sentence.
Hence,we need to match the translations suggested by theWSD system against the English side of the rule.
Itis for these matching rules that the WSD featureswill apply.The translations proposed by the WSD systemmay be more than one word long.
In order for aproposed translation to match the rule, we requiretwo conditions.
First, the proposed translation mustbe a substring of the English side of the rule.
Forexample, the proposed translation ?every to?
wouldnot match the chunk ?every month to?.
Second, thematch must contain at least one aligned Chinese-English word pair, but we do not make any otherrequirements about the alignment of the other Chi-nese or English words.1 If there are multiple possi-ble matches, we choose the longest proposed trans-lation; in the case of a tie, we choose the proposedtranslation with the highest score according to theWSD model.Define a chunk of a rule to be a maximal sub-string of terminal symbols on the English side of therule.
For example, in Rule (2), the chunks would be?go to?
and ?every month to?.
Whenever we finda matching WSD translation, we mark the wholechunk on the English side as consumed.Finally, we compute the feature values for therule.
The feature Pwsd(t | s) is the sum of the costs(according to the WSD model) of all the matchedtranslations, and the feature Ptywsd is the sum ofthe lengths of all the matched translations.Figure 1 shows the pseudocode for the rule scor-ing algorithm in more detail, particularly with re-gards to resolving conflicts between overlappingmatches.
To illustrate the algorithm given in Figure1, consider Rule (2).
Hereafter, we will use symbolsto represent the Chinese and English words in therule: c1, c2, and c3 will represent the Chinese words??
?, ??
?, and ?t?
respectively.
Similarly, e1, e2,e3, e4, and e5 will represent the English words go,to, every, month, and to respectively.
Hence, Rule(2) has two chunks: e1e2 and e3e4e5.
When the ruleis extracted from the parallel corpus, it has thesealignments between the words of its Chinese andEnglish portion: {c1?e3,c2?e4,c3?e1,c3?e2,c3?e5},which means that c1 is aligned to e3, c2 is aligned to1In order to check this requirement, we extended Hiero tomake word alignment information available to the decoder.36Input: rule R considered during decoding with its own associated costRLc = list of symbols in Chinese portion of RWSDcost = 0i = 1while i ?
len(Lc):ci = ith symbol in Lcif ci is a Chinese word (i.e., not a non-terminal symbol):seenChunk = ?
// seenChunk is a global variable and is passed by reference to matchWSDif (ci is not the last symbol in Lc) and (ci+1 is a terminal symbol): then ci+1=(i+1)th symbol in Lc, else ci+1 = NULLif (ci+1!=NULL) and (ci, ci+1) as a single unit has WSD translations:WSDc = set of WSD translations for (ci, ci+1) as a single unit with context-dependent probabilitiesWSDcost = WSDcost + matchWSD(ci, WSDc, seenChunk)WSDcost = WSDcost + matchWSD(ci+1, WSDc, seenChunk)i = i + 1else:WSDc = set of WSD translations for ci with context-dependent probabilitiesWSDcost = WSDcost + matchWSD(ci, WSDc, seenChunk)i = i + 1costR = costR + WSDcostmatchWSD(c, WSDc, seenChunk):// seenChunk is the set of chunks of R already examined for possible matching WSD translationscost = 0ChunkSet = set of chunks in R aligned to cfor chunkj in ChunkSet:if chunkj not in seenChunk:seenChunk = seenChunk ?
{ chunkj }Echunkj = set of English words in chunkj aligned to cCandidatewsd = ?for wsdk in WSDc:if (wsdk is sub-sequence of chunkj) and (wsdk contains at least one word in Echunkj )Candidatewsd = Candidatewsd ?
{ wsdk }wsdbest = best matching translation in Candidatewsd against chunkjcost = cost + costByWSDfeatures(wsdbest) // costByWSDfeatures sums up the cost of the two WSD featuresreturn costFigure 1: WSD translations affecting the cost of a rule R considered during decoding.e4, and c3 is aligned to e1, e2, and e5.
Although allwords are aligned here, in general for a rule, some ofits Chinese or English words may not be associatedwith any alignments.In our experiment, c1c2 as a phrase has a list oftranslations proposed by the WSD system, includ-ing the English phrase ?every month?.
matchWSDwill first be invoked for c1, which is aligned to onlyone chunk e3e4e5 via its alignment with e3.
Since?every month?
is a sub-sequence of the chunk andalso contains the word e3 (?every?
), it is noted asa candidate translation.
Later, it is determined thatthe most number of words any candidate translationhas is two words.
Since among all the 2-word candi-date translations, the translation ?every month?
hasthe highest translation probability as assigned by theWSD classifier, it is chosen as the best matchingtranslation for the chunk.
matchWSD is then invokedfor c2, which is aligned to only one chunk e3e4e5.However, since this chunk has already been exam-ined by c1 with which it is considered as a phrase, nofurther matching is done for c2.
Next, matchWSD isinvoked for c3, which is aligned to both chunks of R.The English phrases ?go to?
and ?to?
are among thelist of translations proposed by the WSD system forc3, and they are eventually chosen as the best match-ing translations for the chunks e1e2 and e3e4e5, re-spectively.6 ExperimentsAs mentioned, our experiments were on Chinese toEnglish translation.
Similar to (Chiang, 2005), wetrained the Hiero system on the FBIS corpus, usedthe NIST MT 2002 evaluation test set as our devel-opment set to tune the feature weights, and the NISTMT 2003 evaluation test set as our test data.
Using37System BLEU-4 Individual n-gram precisions1 2 3 4Hiero 29.73 74.73 40.14 21.83 11.93Hiero+WSD 30.30 74.82 40.40 22.45 12.42Table 1: BLEU scoresFeaturesSystem Plm(e) P (?|?)
P (?|?)
Pw(?|?)
Pw(?|?)
Ptyphr Glue Ptyword Pwsd(t|s) PtywsdHiero 0.2337 0.0882 0.1666 0.0393 0.1357 0.0665 ?0.0582 ?0.4806 - -Hiero+WSD 0.1937 0.0770 0.1124 0.0487 0.0380 0.0988 ?0.0305 ?0.1747 0.1051 ?0.1611Table 2: Weights for each feature obtained by MERT training.
The first eight features are those used byHiero in (Chiang, 2005).the English portion of the FBIS corpus and the Xin-hua portion of the Gigaword corpus, we trained a tri-gram language model using the SRI Language Mod-elling Toolkit (Stolcke, 2002).
Following (Chiang,2005), we used the version 11a NIST BLEU scriptwith its default settings to calculate the BLEU scores(Papineni et al, 2002) based on case-insensitive n-gram matching, where n is up to 4.First, we performed word alignment on the FBISparallel corpus using GIZA++ (Och and Ney, 2000)in both directions.
The word alignments of bothdirections are then combined into a single set ofalignments using the ?diag-and?
method of Koehnet al (2003).
Based on these alignments, syn-chronous CFG rules are then extracted from the cor-pus.
While Hiero is extracting grammar rules, wegathered WSD training data by following the proce-dure described in section 4.6.1 Hiero ResultsUsing the MT 2002 test set, we ran the minimum-error rate training (MERT) (Och, 2003) with thedecoder to tune the weights for each feature.
Theweights obtained are shown in the row Hiero ofTable 2.
Using these weights, we run Hiero?s de-coder to perform the actual translation of the MT2003 test sentences and obtained a BLEU score of29.73, as shown in the row Hiero of Table 1.
This ishigher than the score of 28.77 reported in (Chiang,2005), perhaps due to differences in word segmenta-tion, etc.
Note that comparing with the MT systemsused in (Carpuat and Wu, 2005) and (Cabezas andResnik, 2005), the Hiero system we are using rep-resents a much stronger baseline MT system uponwhich the WSD system must improve.6.2 Hiero+WSD ResultsWe then added the WSD features of Section 3.1 intoHiero and reran the experiment.
The weights ob-tained by MERT are shown in the row Hiero+WSDof Table 2.
We note that a negative weight is learntfor Ptywsd.
This means that in general, the modelprefers grammar rules having chunks that matchesWSD translations.
This matches our intuition.
Us-ing the weights obtained, we translated the test sen-tences and obtained a BLEU score of 30.30, asshown in the row Hiero+WSD of Table 1.
The im-provement of 0.57 is statistically significant at p <0.05 using the sign-test as described by Collins et al(2005), with 374 (+1), 318 (?1) and 227 (0).
Us-ing the bootstrap-sampling test described in (Koehn,2004b), the improvement is statistically significantat p < 0.05.
Though the improvement is modest, it isstatistically significant and this positive result is im-portant in view of the negative findings in (Carpuatand Wu, 2005) that WSD does not help MT.
Fur-thermore, note that Hiero+WSD has higher n-gramprecisions than Hiero.7 AnalysisIdeally, the WSD system should be suggesting high-quality translations which are frequently part of thereference sentences.
To determine this, we note theset of grammar rules used in the best derivation fortranslating each test sentence.
From the rules of eachtest sentence, we tabulated the set of translationsproposed by the WSD system and check whetherthey are found in the associated reference sentences.On the entire set of NIST MT 2003 evaluation testsentences, an average of 10.36 translations proposed38No.
of All test sentences +1 from Collins sign-testwords in No.
of % match No.
of % matchWSD translations WSD translations used reference WSD translations used reference1 7087 77.31 3078 77.682 1930 66.11 861 64.923 371 43.13 171 48.544 124 26.61 52 28.85Table 3: Number of WSD translations used and proportion that matches against respective reference sen-tences.
WSD translations longer than 4 words are very sparse (less than 10 occurrences) and thus they arenot shown.by the WSD system were used for each sentence.When limited to the set of 374 sentences whichwere judged by the Collins sign-test to have bettertranslations from Hiero+WSD than from Hiero, ahigher number (11.14) of proposed translations wereused on average.
Further, for the entire set of testsentences, 73.01% of the proposed translations arefound in the reference sentences.
This increased toa proportion of 73.22% when limited to the set of374 sentences.
These figures show that having more,and higher-quality proposed translations contributedto the set of 374 sentences being better translationsthan their respective original translations from Hi-ero.
Table 3 gives a detailed breakdown of thesefigures according to the number of words in eachproposed translation.
For instance, over all the testsentences, the WSD module gave 7087 translationsof single-word length, and 77.31% of these trans-lations match their respective reference sentences.We note that although the proportion of matching 2-word translations is slightly lower for the set of 374sentences, the proportion increases for translationshaving more words.After the experiments in Section 6 were com-pleted, we visually inspected the translation outputof Hiero and Hiero+WSD to categorize the ways inwhich integrating WSD contributes to better trans-lations.
The first way in which WSD helps is whenit enables the integrated Hiero+WSD system to out-put extra appropriate English words.
For example,the translations for the Chinese sentence ?.
.
.?
???q??R?Rz???????tZ?
are as follows.?
Hiero: .
.
.
or other bad behavior ?, will be moreaid and other concessions.?
Hiero+WSD: .
.
.
or other bad behavior ?, willbe unable to obtain more aid and other conces-sions.Here, the Chinese words ??Rz?
are not trans-lated by Hiero at all.
By providing the correct trans-lation of ?unable to obtain?
for ?
? Rz?, thetranslation output of Hiero+WSD is more complete.A second way in which WSD helps is by correct-ing a previously incorrect translation.
For example,for the Chinese sentence ?.
.
.?
?
\ ) ? |??.
.
.
?, the WSD system helps to correct Hiero?soriginal translation by providing the correct transla-tion of ?all ethnic groups?
for the Chinese phrase???:?
Hiero: .
.
.
, and people of all nationalitiesacross the country, .
.
.?
Hiero+WSD: .
.
.
, and people ofall ethnic groups across the country, .
.
.We also looked at the set of 318 sentences thatwere judged by the Collins sign-test to be worsetranslations.
We found that in some situations,Hiero+WSD has provided extra appropriate Englishwords, but those particular words are not used in thereference sentences.
An interesting example is thetranslation of the Chinese sentence ???
i?
??8q??R?Rz?????.?
Hiero: Australian foreign minister said thatNorth Korea bad behavior will be more aid?
Hiero+WSD: Australian foreign minister saidthat North Korea bad behavior will beunable to obtain more aidThis is similar to the example mentioned earlier.
Inthis case however, those extra English words pro-vided by Hiero+WSD, though appropriate, do not39result in more n-gram matches as the reference sen-tences used phrases such as ?will not gain?, ?will notget?, etc.
Since the BLEU metric is precision based,the longer sentence translation by Hiero+WSD getsa lower BLEU score instead.8 ConclusionWe have shown that WSD improves the transla-tion performance of a state-of-the-art hierarchicalphrase-based statistical MT system and this im-provement is statistically significant.
We have alsodemonstrated one way to integrate a WSD systeminto an MT system without introducing any rulesthat compete against existing rules, and where thefeature-weight tuning and decoding place the WSDsystem on an equal footing with the other modelcomponents.
For future work, an immediate stepwould be for the WSD classifier to provide trans-lations for longer Chinese phrases.
Also, differentalternatives could be tried to match the translationsprovided by the WSD classifier against the chunksof rules.
Finally, besides our proposed approach ofintegrating WSD into statistical MT via the intro-duction of two new features, we could explore otheralternative ways of integration.AcknowledgementsYee Seng Chan is supported by a Singapore Millen-nium Foundation Scholarship (ref no.
SMF-2004-1076).
David Chiang was partially supported un-der the GALE program of the Defense AdvancedResearch Projects Agency, contract HR0011-06-C-0022.ReferencesC.
Cabezas and P. Resnik.
2005.
Using WSD techniques forlexical selection in statistical machine translation.
Technicalreport, University of Maryland.M.
Carpuat and D. Wu.
2005.
Word sense disambiguationvs.
statistical machine translation.
In Proc.
of ACL05, pages387?394.M.
Carpuat, W. Su, and D. Wu.
2004.
Augmenting ensembleclassification for word sense disambiguation with a kernelPCA model.
In Proc.
of SENSEVAL-3, pages 88?92.C.
C. Chang and C. J. Lin, 2001.
LIBSVM: a library for sup-port vector machines.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.D.
Chiang.
2005.
A hierarchical phrase-based model for sta-tistical machine translation.
In Proc.
of ACL05, pages 263?270.D.
Chiang.
2007.
Hierarchical phrase-based translation.
Toappear in Computational Linguistics, 33(2).M.
Collins, P. Koehn, and I. Kucerova.
2005.
Clause restruc-turing for statistical machine translation.
In Proc.
of ACL05,pages 531?540.U.
Germann.
2003.
Greedy decoding for statistical machinetranslation in almost linear time.
In Proc.
of HLT-NAACL03,pages 72?79.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statistical phrase-based translation.
In Proc.
of HLT-NAACL03, pages 48?54.P.
Koehn.
2003.
Noun Phrase Translation.
Ph.D. thesis, Uni-versity of Southern California.P.
Koehn.
2004a.
Pharaoh: A beam search decoder for phrase-based statistical machine translation models.
In Proc.
ofAMTA04, pages 115?124.P.
Koehn.
2004b.
Statistical significance tests for machinetranslation evaluation.
In Proc.
of EMNLP04, pages 388?395.Y.
K. Lee and H. T. Ng.
2002.
An empirical evaluation ofknowledge sources and learning algorithms for word sensedisambiguation.
In Proc.
of EMNLP02, pages 41?48.P.
M. II Lewis and R. E. Stearns.
1968.
Syntax-directed trans-duction.
Journal of the ACM, 15(3):465?488.D.
Marcu and W. Wong.
2002.
A phrase-based, joint proba-bility model for statistical machine translation.
In Proc.
ofEMNLP02, pages 133?139.F.
J. Och and H. Ney.
2000.
Improved statistical alignmentmodels.
In Proc.
of ACL00, pages 440?447.F.
J. Och and H. Ney.
2002.
Discriminative training and max-imum entropy models for statistical machine translation.
InProc.
of ACL02, pages 295?302.F.
J. Och and H. Ney.
2004.
The alignment template approachto statistical machine translation.
Computational Linguis-tics, 30(4):417?449.F.
J. Och.
2003.
Minimum error rate training in statistical ma-chine translation.
In Proc.
of ACL03, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.
BLEU:A method for automatic evaluation of machine translation.In Proc.
of ACL02, pages 311?318.A.
Stolcke.
2002.
SRILM - an extensible language modelingtoolkit.
In Proc.
of ICSLP02, pages 901?904.D.
Vickrey, L. Biewald, M. Teyssier, and D. Koller.
2005.Word-sense disambiguation for machine translation.
InProc.
of EMNLP05, pages 771?778.D.
Wu.
1996.
A polynomial-time algorithm for statistical ma-chine translation.
In Proc.
of ACL96, pages 152?158.40
