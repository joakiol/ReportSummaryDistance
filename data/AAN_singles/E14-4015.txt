Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 74?78,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsAccelerated Estimation of Conditional Random Fields using aPseudo-Likelihood-inspired Perceptron VariantTeemu RuokolainenaMiikka SilfverbergbMikko KurimoaKrister Lind?nbaDepartment of Signal Processing and Acoustics, Aalto University, firstname.lastname@aalto.fibDepartment of Modern Languages, University of Helsinki, firstname.lastname@helsinki.fiAbstractWe discuss a simple estimation approachfor conditional random fields (CRFs).
Theapproach is derived heuristically by defin-ing a variant of the classic perceptron al-gorithm in spirit of pseudo-likelihood formaximum likelihood estimation.
The re-sulting approximative algorithm has a lin-ear time complexity in the size of the la-bel set and contains a minimal amount oftunable hyper-parameters.
Consequently,the algorithm is suitable for learning CRF-based part-of-speech (POS) taggers inpresence of large POS label sets.
Wepresent experiments on five languages.Despite its heuristic nature, the algorithmprovides surprisingly competetive accura-cies and running times against referencemethods.1 IntroductionThe conditional random field (CRF) model (Laf-ferty et al., 2001) has been successfully appliedto several sequence labeling tasks in natural lan-guage processing, including part-of-speech (POS)tagging.
In this work, we discuss accelerating theCRF model estimation in presence of a large num-ber of labels, say, hundreds or thousands.
Large la-bel sets occur in POS tagging of morphologicallyrich languages (Erjavec, 2010; Haverinen et al.,2013).CRF training is most commonly associated withthe (conditional) maximum likelihood (ML) crite-rion employed in the original work of Lafferty etal.
(2001).
In this work, we focus on an alternativetraining approach using the averaged perceptronalgorithm of Collins (2002).
While yielding com-petitive accuracy (Collins, 2002; Zhang and Clark,2011), the perceptron algorithm avoids extensivetuning of hyper-parameters and regularization re-quired by the stochastic gradient descent algo-rithm employed in ML estimation (Vishwanathanet al., 2006).
Additionally, while ML and percep-tron training share an identical time complexity,the perceptron is in practice faster due to sparserparameter updates.Despite its simplicity, running the perceptron al-gorithm can be tedious in case the data containsa large number of labels.
Previously, this prob-lem has been addressed using, for example, k-bestbeam search (Collins and Roark, 2004; Zhang andClark, 2011; Huang et al., 2012) and paralleliza-tion (McDonald et al., 2010).
In this work, weexplore an alternative strategy, in which we mod-ify the perceptron algorithm in spirit of the classicpseudo-likelihood approximation for ML estima-tion (Besag, 1975).
The resulting novel algorithmhas linear complexity w.r.t.
the label set size andcontains only a single hyper-parameter, namely,the number of passes taken over the training dataset.We evaluate the algorithm, referred to as thepseudo-perceptron, empirically in POS taggingon five languages.
The results suggest that theapproach can yield competitive accuracy com-pared to perceptron training accelerated using aviolation-fixed 1-best beam search (Collins andRoark, 2004; Huang et al., 2012) which also pro-vides a linear time complexity in label set size.The rest of the paper is as follows.
In Section 2,we describe the pseudo-perceptron algorithm anddiscuss related work.
In Sections 3 and 4, wedescribe our experiment setup and the results, re-spectively.
Conclusions on the work are presentedin Section 5.2 Methods2.1 Pseudo-Perceptron AlgorithmThe (unnormalized) CRF model for input andoutput sequences x = (x1, x2, .
.
.
, x|x|) and74y = (y1, y2, .
.
.
, y|x|), respectively, is written asp (y |x;w) ?
exp(w ??
(y, x))=|x|?i=nexp(w ?
?
(yi?n, .
.
.
, yi, x, i)),(1)where w denotes the model parameter vector, ?the vector-valued global feature extracting func-tion, ?
the vector-valued local feature extractingfunction, and n the model order.
We denote thetag set as Y .
The model parameters w are esti-mated based on training data, and test instancesare decoded using the Viterbi search (Lafferty etal., 2001).Given the model definition (1), the param-eters w can be estimated in a straightforwardmanner using the structured perceptron algo-rithm (Collins, 2002).
The algorithm iteratesover the training set a single instance (x, y) ata time and updates the parameters accordingto the rule w(i)= w(i?1)+ ??
(x, y, z), where??
(x, y, z) for the ith iteration is written as??
(x, y, z) = ?
(x, y)??
(x, z).
The predic-tion z is obtained asz = arg maxu?Y(x)w ??
(x, u) (2)by performing the Viterbi search overY(x) = Y ?
?
?
?
?
Y , a product of |x| copiesof Y .
In case the perceptron algorithm yieldsa small number of incorrect predictions on thetraining data set, the parameters generalize wellto test instances with a high probability (Collins,2002).The time complexity of the Viterbi search isO(|x| ?
|Y|n+1).
Consequently, running the per-ceptron algorithm can become tedious if the la-bel set cardinality |Y| and/or the model order nis large.
In order to speed up learning, we definea variant of the algorithm in the spirit of pseudo-likelihood (PL) learning (Besag, 1975).
In anal-ogy to PL, the key idea of the pseudo-perceptron(PP) algorithm is to obtain the required predictionsover single variables yiwhile fixing the remainingvariables to their true values.
In other words, in-stead of using the Viterbi search to find the z as in(2), we find a z?for each position i ?
1..|x| asz?= arg maxu?Y?i(x)w ??
(x, u) , (3)with Y?i(x) = {y1}??
?
??{yi?1}?Y?{yi+1}??
?
?
?
{y|x|}.
Subsequent to training, test instancesare decoded in a standard manner using the Viterbisearch.The appeal of PP is that the time complexityof search is reduced to O(|x| ?
|Y|), i.e., linearin the number of labels in the label set.
On theother hand, we no longer expect the obtained pa-rameters to necessarily generalize well to test in-stances.1Consequently, we consider PP a heuris-tic estimation approach motivated by the ratherwell-established success of PL (Kor?c and F?rstner,2008; Sutton and McCallum, 2009).2Next, we study yet another heuristic pseudo-variant of the perceptron algorithm referred to asthe piecewise-pseudo-perceptron (PW-PP).
Thisalgorithm is analogous to the piecewise-pseudo-likelihood (PW-PL) approximation presented bySutton and McCallum (2009).
In this variant, theoriginal graph is first split into smaller, possiblyoverlapping subgraphs (pieces).
Subsequently, weapply the PP approximation to the pieces.
We em-ploy the approach coined factor-as-piece by Sut-ton and McCallum (2009), in which each piececontains n + 1 consecutive variables, where n isthe CRF model order.The PW-PP approach is motivated by the resultsof Sutton and McCallum (2009) who found PW-PL to increase stability w.r.t.
accuracy comparedto plain PL across tasks.
Note that the piecewiseapproximation in itself is not interesting in chain-structured CRFs, as it results in same time com-plexity as standard estimation.
Meanwhile, thePW-PP algorithm has same time complexity as PP.2.2 Related workPreviously, impractical running times of percep-tron learning have been addressed most notablyusing the k-best beam search method (Collins andRoark, 2004; Zhang and Clark, 2011; Huang etal., 2012).
Here, we consider the ?greedy?
1-bestbeam search variant most relevant as it shares thetime complexity of the pseudo search.
Therefore,in the experimental section of this work, we com-pare the PP and 1-best beam search.We are aware of at least two other learning ap-proaches inspired by PL, namely, the pseudo-maxand piecewise algorithms of Sontag et al.
(2010)and Alahari et al.
(2010), respectively.
Com-pared to these approaches, the PP algorithm pro-vides a simpler estimation tool as it avoids the1We leave formal treatment to future work.2Meanwhile, note that pseudo-likelihood is a consistentestimator (Gidas, 1988; Hyv?rinen, 2006).75hyper-parameters involved in the stochastic gradi-ent descent algorithms as well as the regularizationand margin functions inherent to the approaches ofAlahari et al.
(2010) and Sontag et al.
(2010).
Onthe other hand, Sontag et al.
(2010) show that thepseudo-max approach achieves consistency givencertain assumptions on the data generating func-tion.
Meanwhile, as discussed in previous section,we consider PP a heuristic and do not provide anygeneralization guarantees.
To our understanding,Alahari et al.
(2010) do not provide generalizationguarantees for their algorithm.3 Experimental Setup3.1 DataFor a quick overview of the data sets, see Table 1.Penn Treebank.
The first data set we consideris the classic Penn Treebank.
The complete tree-bank is divided into 25 sections of newswire textextracted from the Wall Street Journal.
We splitthe data into training, development, and test setsusing the sections 0-18, 19-21, and 22-24, accord-ing to the standardly applied division introducedby Collins (2002).Multext-East.
The second data we consider isthe multilingual Multext-East (Erjavec, 2010) cor-pus.
The corpus contains the novel 1984 byGeorge Orwell.
From the available seven lan-guages, we utilize the Czech, Estonian and Ro-manian sections.
Since the data does not have astandard division to training and test sets, we as-sign the 9th and 10th from each 10 consecutivesentences to the development and test sets, respec-tively.
The remaining sentences are assigned to thetraining sets.Turku Dependency Treebank.
The third datawe consider is the Finnish Turku DependencyTreebank (Haverinen et al., 2013).
The treebankcontains text from 10 different domains.
We usethe same data split strategy as for Multext East.3.2 Reference MethodsWe compare the PP and PW-PP algorithms withperceptron learning accelerated using 1-best beamsearch modified using the early update rule(Huang et al., 2012).
While Huang et al.
(2012)experimented with several violation-fixing meth-ods (early, latest, maximum, hybrid), they ap-peared to reach termination at the same rate inlang.
train.
dev.
test tags train.
tagseng 38,219 5,527 5,462 45 45rom 5,216 652 652 405 391est 5,183 648 647 413 408cze 5,402 675 675 955 908fin 5,043 630 630 2,355 2,141Table 1: Overview on data.
The training (train.
),development (dev.)
and test set sizes are given insentences.
The columns titled tags and train.
tagscorrespond to total number of tags in the data setand number of tags in the training set, respectively.POS tagging.
Our preliminary experiments usingthe latest violation updates supported this.
Conse-quently, we employ the early updates.We also provide results using the CRFsuitetoolkit (Okazaki, 2007), which implements a 1st-order CRF model.
To best of our knowledge,CRFsuite is currently the fastest freely availableCRF implementation.3In addition to the averagedperceptron algorithm (Collins, 2002), the toolkitimplements several training procedures (Nocedal,1980; Crammer et al., 2006; Andrew and Gao,2007; Mejer and Crammer, 2010; Shalev-Shwartzet al., 2011).
We run CRFsuite using these algo-rithms employing their default parameters and thefeature extraction scheme and stopping criteriondescribed in Section 3.3.
We then report resultsprovided by the most accurate algorithm on eachlanguage.3.3 Details on CRF Training and DecodingWhile the methods discussed in this work are ap-plicable for nth-order CRFs, we employ 1st-orderCRFs in order to avoid overfitting the relativelysmall training sets.We employ a simple feature set including wordforms at position t?
2, .
.
.
, t+ 2, suffixes of wordat position t up to four letters, and three ortho-graphic features indicating if the word at positiont contains a hyphen, capital letter, or a digit.All the perceptron variants (PP, PW-PP, 1-bestbeam search) initialize the model parameters withzero vectors and process the training instances inthe order they appear in the corpus.
At the endof each pass, we apply the CRFs using the latestaveraged parameters (Collins, 2002) to the devel-opment set.
We assume the algorithms have con-verged when the model accuracy on development3See benchmark results at http://www.chokkan.org/software/crfsuite/benchmark.html76has not increased during last three iterations.
Af-ter termination, we apply the averaged parametersyielding highest performance on the developmentset to test instances.Test and development instances are decoded us-ing a combination of Viterbi search and the tagdictionary approach of Ratnaparkhi (1996).
In thisapproach, candidate tags for known word formsare limited to those observed in the training data.Meanwhile, word forms that were unseen duringtraining consider the full label set.3.4 Software and HardwareThe experiments are run on a standard desktopcomputer.
We use our own C++-based implemen-tation of the methods discussed in Section 2.4 ResultsThe obtained training times and test set accuracies(measured using accuracy and out-of-vocabulary(OOV) accuracy) are presented in Table 2.
Thetraining CPU times include the time (in minutes)consumed by running the perceptron algorithmvariants as well as evaluation of the developmentset accuracy.
The column labeled it.
correspondsto the number of passes over training set made bythe algorithms before termination.We summarize the results as follows.
First, PW-PP provided higher accuracies compared to PP onRomanian, Czech, and Finnish.
The differenceswere statistically significant4on Czech.
Second,while yielding similar running times comparedto 1-best beam search, PW-PP provided higheraccuracies on all languages apart from Finnish.The differences were significant on Estonian andCzech.
Third, while fastest on the Penn Treebank,the CRFsuite toolkit became substantially slowercompared to PW-PP when the number of labelswere increased (see Czech and Finnish).
The dif-ferences in accuracies between the best perform-ing CRFsuite algorithm and PP and PW-PP weresignificant on Czech.5 ConclusionsWe presented a heuristic perceptron variant forestimation of CRFs in the spirit of the classic4We establish significance (with confidence level 0.95)using the standard 1-sided Wilcoxon signed-rank test per-formed on 10 randomly divided, non-overlapping subsets ofthe complete test sets.method it.
time (min) acc.
OOVEnglishPP 9 6 96.99 87.97PW-PP 10 7 96.98 88.111-best beam 17 8 96.91 88.33Pas.-Agg.
9 1 97.01 88.68RomanianPP 9 8 96.81 83.66PW-PP 8 7 96.91 84.381-best beam 17 10 96.88 85.32Pas.-Agg.
13 9 97.06 84.69EstonianPP 10 8 93.39 78.10PW-PP 8 6 93.35 78.661-best beam 23 15 92.95 75.65Pas.-Agg.
15 12 93.27 77.63CzechPP 11 26 89.37 70.67PW-PP 16 41 89.84 72.521-best beam 14 19 88.95 70.90Pegasos 15 341 90.42 72.59FinnishPP 11 58 87.09 58.58PW-PP 11 56 87.16 58.501-best beam 21 94 87.38 59.29Pas.-Agg.
16 693 87.17 57.58Table 2: Results.
We report CRFsuite results pro-vided by most accurate algorithm on each lan-guage: the Pas.-Agg.
and Pegasos refer to the al-gorithms of Crammer et al.
(2006) and Shalev-Shwartz et al.
(2011), respectively.pseudo-likelihood estimator.
The resulting ap-proximative algorithm has a linear time complex-ity in the label set cardinality and contains onlya single hyper-parameter, namely, the number ofpasses taken over the training data set.
We eval-uated the algorithm in POS tagging on five lan-guages.
Despite its heuristic nature, the algo-rithm provided competetive accuracies and run-ning times against reference methods.AcknowledgementsThis work was financially supported by Langnet(Finnish doctoral programme in language stud-ies) and the Academy of Finland under the grantno 251170 (Finnish Centre of Excellence Pro-gram (2012-2017)).
We would like to thank Dr.Onur Dikmen for the helpful discussions duringthe work.77ReferencesKarteek Alahari, Chris Russell, and Philip H.S.
Torr.2010.
Efficient piecewise learning for conditionalrandom fields.
In Computer Vision and PatternRecognition (CVPR), 2010 IEEE Conference on,pages 895?901.Galen Andrew and Jianfeng Gao.
2007.
Scalable train-ing of L1-regularized log-linear models.
In Proceed-ings of the 24th international conference on Ma-chine learning, pages 33?40.Julian Besag.
1975.
Statistical analysis of non-latticedata.
The statistician, pages 179?195.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of the 42nd Annual Meeting on Association forComputational Linguistics, page 111.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing, volume 10, pages 1?8.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
The Journal of Ma-chine Learning Research, 7:551?585.Toma?
Erjavec.
2010.
Multext-east version 4: Multi-lingual morphosyntactic specifications, lexicons andcorpora.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10).Basilis Gidas.
1988.
Consistency of maximum like-lihood and pseudo-likelihood estimators for Gibbsdistributions.
In Stochastic differential systems,stochastic control theory and applications, pages129?145.Katri Haverinen, Jenna Nyblom, Timo Viljanen,Veronika Laippala, Samuel Kohonen, Anna Missil?,Stina Ojala, Tapio Salakoski, and Filip Ginter.
2013.Building the essential resources for Finnish: theTurku Dependency Treebank.
Language Resourcesand Evaluation.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages142?151.Aapo Hyv?rinen.
2006.
Consistency of pseudolike-lihood estimation of fully visible Boltzmann ma-chines.
Neural Computation, 18(10):2283?2292.Filip Kor?c and Wolfgang F?rstner.
2008.
Approximateparameter learning in conditional random fields: Anempirical investigation.
Pattern Recognition, pages11?20.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth In-ternational Conference on Machine Learning, pages282?289.Ryan McDonald, Keith Hall, and Gideon Mann.
2010.Distributed training strategies for the structured per-ceptron.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 456?464.Avihai Mejer and Koby Crammer.
2010.
Confidencein structured-prediction using confidence-weightedmodels.
In Proceedings of the 2010 conference onempirical methods in natural language processing,pages 971?981.Jorge Nocedal.
1980.
Updating quasi-Newton matri-ces with limited storage.
Mathematics of computa-tion, 35(151):773?782.Naoaki Okazaki.
2007.
CRFsuite: a fast implemen-tation of conditional random fields (CRFs).
URLhttp://www.chokkan.org/software/crfsuite.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedingsof the conference on empirical methods in naturallanguage processing, volume 1, pages 133?142.Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro,and Andrew Cotter.
2011.
Pegasos: Primal esti-mated sub-gradient solver for SVM.
MathematicalProgramming, 127(1):3?30.David Sontag, Ofer Meshi, Tommi Jaakkola, and AmirGloberson.
2010.
More data means less inference:A pseudo-max approach to structured learning.
InAdvances in Neural Information Processing Systems23, pages 2181?2189.Charles Sutton and Andrew McCallum.
2009.
Piece-wise training for structured prediction.
Machinelearning, 77(2):165?194.S.V.N.
Vishwanathan, Nicol Schraudolph, MarkSchmidt, and Kevin Murphy.
2006.
Acceleratedtraining of conditional random fields with stochas-tic gradient methods.
In Proceedings of the 23rd in-ternational conference on Machine learning, pages969?976.Yue Zhang and Stephen Clark.
2011.
Syntactic pro-cessing using the generalized perceptron and beamsearch.
Computational Linguistics, 37(1):105?151.78
