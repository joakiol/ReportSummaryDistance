Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84?89,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsThe KIT-LIMSI Translation System for WMT 2014?Quoc Khanh Do,?Teresa Herrmann,?
?Jan Niehues,?Alexandre Allauzen,?Franc?ois Yvon and?Alex Waibel?LIMSI-CNRS, Orsay, France?Karlsruhe Institute of Technology, Karlsruhe, Germany?surname@limsi.fr?firstname.surname@kit.eduAbstractThis paper describes the joined submis-sion of LIMSI and KIT to the SharedTranslation Task for the German-to-English direction.
The system consistsof a phrase-based translation system us-ing a pre-reordering approach.
The base-line system already includes several mod-els like conventional language models ondifferent word factors and a discriminativeword lexicon.
This system is used to gen-erate a k-best list.
In a second step, thelist is reranked using SOUL language andtranslation models (Le et al., 2011).Originally, SOUL translation models wereapplied to n-gram-based translation sys-tems that use tuples as translation unitsinstead of phrase pairs.
In this article,we describe their integration into the KITphrase-based system.
Experimental re-sults show that their use can yield sig-nificant improvements in terms of BLEUscore.1 IntroductionThis paper describes the KIT-LIMSI system forthe Shared Task of the ACL 2014 Ninth Work-shop on Statistical Machine Translation.
The sys-tem participates in the German-to-English trans-lation task.
It consists of two main components.First, a k-best list is generated using a phrase-based machine translation system.
This systemwill be described in Section 2.
Afterwards, the k-best list is reranked using SOUL (Structured OUt-put Layer) models.
Thereby, a neural network lan-guage model (Le et al., 2011), as well as severaltranslation models (Le et al., 2012a) are used.
Adetailed description of these models can be foundin Section 3.
While the translation system usesphrase pairs, the SOUL translation model uses tu-ples as described in the n-gram approach (Mari?noet al., 2006).
We describe the integration of theSOUL models into the translation system in Sec-tion 3.2.
Section 4 summarizes the experimen-tal results and compares two different tuning al-gorithms: Minimum Error Rate Training (Och,2003) and k-best Batch Margin Infused RelaxedAlgorithm (Cherry and Foster, 2012).2 Baseline systemThe KIT translation system is an in-house imple-mentation of the phrase-based approach and in-cludes a pre-ordering step.
This system is fullydescribed in Vogel (2003).To train translation models, the provided Eu-roparl, NC and Common Crawl parallel corporaare used.
The target side of those parallel corpora,the News Shuffle corpus and the GigaWord cor-pus are used as monolingual training data for thedifferent language models.
Optimization is donewith Minimum Error Rate Training as describedin Venugopal et al.
(2005), using newstest2012and newstest2013 as development and test data,respectively.Compound splitting (Koehn and Knight, 2003)is performed on the source side (German) of thecorpus before training.
Since the web-crawledCommon Crawl corpus is noisy, this corpus isfirst filtered using an SVM classifier as describedin Mediani et al.
(2011).The word alignment is generated using theGIZA++ Toolkit (Och and Ney, 2003).
Phraseextraction and scoring is done using the Mosestoolkit (Koehn et al., 2007).
Phrase pair proba-bilities are computed using modified Kneser-Neysmoothing (Foster et al., 2006).We apply short-range reorderings (Rottmannand Vogel, 2007) and long-range reorder-ings (Niehues and Kolss, 2009) based on part-of-speech tags.
The POS tags are generated usingthe TreeTagger (Schmid, 1994).
Rewriting rules84based on POS sequences are learnt automaticallyto perform source sentence reordering accordingto the target language word order.
The long-rangereordering rules are further applied to the trainingcorpus to create reordering lattices to extract thephrases for the translation model.
In addition,a tree-based reordering model (Herrmann et al.,2013) trained on syntactic parse trees (Raffertyand Manning, 2008; Klein and Manning, 2003)is applied to the source sentence.
In additionto these pre-reordering models, a lexicalizedreordering model (Koehn et al., 2005) is appliedduring decoding.Language models are trained with the SRILMtoolkit (Stolcke, 2002) using modified Kneser-Neysmoothing (Chen and Goodman, 1996).
The sys-tem uses a 4-gram word-based language modeltrained on all monolingual data and an additionallanguage model trained on automatically selecteddata (Moore and Lewis, 2010).
The system fur-ther applies a language model based on 1000 auto-matically learned word classes using the MKCLSalgorithm (Och, 1999).
In addition, a bilinguallanguage model (Niehues et al., 2011) is used aswell as a discriminative word lexicon (DWL) us-ing source context to guide the word choices in thetarget sentence.3 SOUL models for statistical machinetranslationNeural networks, working on top of conventionaln-gram back-off language models (BOLMs), havebeen introduced in (Bengio et al., 2003; Schwenk,2007) as a potential means to improve discretelanguage models.
The SOUL model (Le et al.,2011) is a specific neural network architecture thatallows us to estimate n-gram models using largevocabularies, thereby making the training of largeneural network models feasible both for target lan-guage models and translation models (Le et al.,2012a).3.1 SOUL translation modelsWhile the integration of SOUL target languagemodels is straightforward, SOUL translation mod-els rely on a specific decomposition of the jointprobability P (s, t) of a sentence pair, where s is asequence of I reordered source words (s1, ..., sI)11In the context of the n-gram translation model, (s, t) thusdenotes an aligned sentence pair, where the source words arereordered.and t contains J target words (t1, ..., tJ).
In then-gram approach (Mari?no et al., 2006; Crego etal., 2011), this segmentation is a by-product ofsource reordering, and ultimately derives from ini-tial word and phrase alignments.
In this frame-work, the basic translation units are tuples, whichare analogous to phrase pairs, and represent amatching u = (s, t) between a source phrase sand a target phrase t.Using the n-gram assumption, the joint proba-bility of a segmented sentence pair using L tupelsdecomposes as:P (s, t) =L?i=1P (ui|ui?1, ..., ui?n+1) (1)A first issue with this decomposition is that theelementary units are bilingual pairs.
Therefore,the underlying vocabulary and hence the numberof parameters can be quite large, even for smalltranslation tasks.
Due to data sparsity issues, suchmodels are bound to face severe estimation prob-lems.
Another problem with Equation (1) is thatthe source and target sides play symmetric roles,whereas the source side is known, and the tar-get side must be predicted.
To overcome someof these issues, the n-gram probability in Equa-tion (1) can be factored by first decomposing tu-ples in two (source and target) parts, and then de-composing the source and target parts at the wordlevel.Let skidenote the kthword of source part of thetuple si.
Let us consider the example of Figure 1,s111corresponds to the source word nobel, s411tothe source word paix, and similarly t211is the tar-get word peace.
We finally define hn?1(tki) as thesequence of the n?1 words preceding tkiin the tar-get sentence, and hn?1(ski) as the n?1 words pre-ceding skiin the reordered source sentence: in Fig-ure 1, h3(t211) thus refers to the three word contextreceive the nobel associated with the target wordpeace.
Using these notations, Equation 1 can berewritten as:P (s, t) =L?i=1[|ti|?k=1P(tki|hn?1(tki), hn?1(s1i+1))?|si|?k=1P(ski|hn?1(t1i), hn?1(ski))](2)This decomposition relies on the n-gram assump-tion, this time at the word level.
Therefore, this85s?8: ?t?8: tos?9: recevoirt?9: receives?10: let?10: thes?11: nobel de la paixt?11: nobel peaces?12: prixt?12: prizeu8u9u10u11u12s :   ....t :   ....?
recevoir le prix nobel de la paixorg :   ............Figure 1: Extract of a French-English sentence pair segmented into bilingual units.
The original (org)French sentence appears at the top of the figure, just above the reordered source s and the target t. Thepair (s, t) decomposes into a sequence of L bilingual units (tuples) u1, ..., uL.
Each tuple uicontains asource and a target phrase: siand ti.model estimates the joint probability of a sentencepair using two sliding windows of length n, onefor each language; however, the moves of thesewindows remain synchronized by the tuple seg-mentation.
Moreover, the context is not limitedto the current phrase, and continues to includewords in adjacent phrases.
Equation (2) involvestwo terms that will be further denoted as TrgSrcand Src, respectively P(tki|hn?1(tki), hn?1(s1i+1))and P(ski|hn?1(t1i), hn?1(ski)).
It is worth notic-ing that the joint probability of a sentence paircan also be decomposed by considering the fol-lowing two terms: P(ski|hn?1(ski), hn?1(t1i+1))and P(tki|hn?1(s1i), hn?1(tki)).
These two termswill be further denoted by SrcTrg and Trg.
There-fore, adding SOUL translation models means that4 scores are added to the phrase-based systems.3.2 IntegrationDuring the training step, the SOUL translationmodels are trained as described in (Le et al.,2012a).
The main changes concern the inferencestep.
Given the computational cost of computingn-gram probabilities with neural network models,a solution is to resort to a two-pass approach: thefirst pass uses a conventional system to producea k-best list (the k most likely hypotheses); inthe second pass, probabilities are computed by theSOUL models for each hypothesis and added asnew features.
Then the k-best list is reordered ac-cording to a combination of all features includingthese new features.
In the following experiments,we use 10-gram SOUL models to rescore 300-best lists.
Since the phrase-based system describedin Section 2 uses source reordering, the decoderwas modified in order to generate k-best lists thatcontain necessary word alignment information be-tween the reordered source sentence and its asso-ciated target hypothesis.
The goal is to recoverthe information that is illustrated in Figure 1 andto apply the n-gram decomposition of a sentencepair.These (target and bilingual) neural networkmodels produce scores for each hypothesis in thek-best list; these new features, along with the fea-tures from the baseline system, are then providedto a new phase which runs the traditional Mini-mum Error Rate Training (MERT ) (Och, 2003), ora recently proposed k-best Batch Margin InfusedRelaxed Algorithm (KBMIRA ) (Cherry and Fos-ter, 2012) for tuning purpose.
The SOUL mod-els used for this year?s evaluation are similar tothose described in Allauzen et al.
(2013) and Leet al.
(2012b).
However, since compared to theseevaluations less parallel data is available for theGerman-to-English task, we use smaller vocabu-laries of about 100K words.4 ResultsWe evaluated the SOUL models on the German-to-English translation task using two systems togenerate the k-best lists.
The first system usedall models of the baseline system except the DWLmodel and the other one used all models.Table 1 summarizes experimental results interms of BLEU scores when the tuning is per-formed using KBMIRA.
As described in Section3, the probability of a phrase pair can be decom-posed into products of words?
probabilities in 2different ways: we can first estimate the probabil-ity of words in the source phrase given the context,and then the probability of the target phrase givenits associated source phrase and context words(see Equation (2)); or inversely we can generatethe target side before the source side.
The for-mer proceeds by adding Src and TrgSrc scores as86No DWL DWLSoul models Dev Test Dev TestNo 26.02 27.02 26.27 27.46Target 26.30 27.42 26.43 27.85Translation st 26.46 27.70 26.66 28.04Translation ts 26.48 27.41 26.61 28.00All Translation 26.50 27.86 26.70 28.08All SOUL models 26.62 27.84 26.75 28.10Table 1: Results using KBMIRANo DWL DWLSoul models Dev Test Dev TestNo 26.02 27.02 26.27 27.46Target 26.18 27.09 26.44 27.54Translation st 26.36 27.59 26.66 27.80Translation ts 26.44 27.69 26.63 27.94All Translation 26.53 27.65 26.69 27.99All SOUL models 26.47 27.68 26.66 28.01Table 2: Results using MERT.
Results in bold correpond to the submitted system.2 new features into the k-best list, and the latter byadding Trg and SrcTrg scores.
These 2 methodscorrespond respectively to the Translation ts andTranslation st lines in the Table 1.
The 4 trans-lation models may also be added simultaneously(All Translations).
The first line gives baselineresults without SOUL models, while the Targetline shows results in adding only SOUL languagemodel.
The last line (All SOUL models) showsthe results for adding all neural network modelsinto the baseline systems.As evident in Table 1, using the SOUL trans-lation models yields generally better results thanusing the SOUL target language model, yieldingabout 0.2 BLEU point differences on dev and testsets.
We can therefore assume that the SOULtranslation models provide richer information that,to some extent, covers that contained in the neuralnetwork language model.
Indeed, these 4 trans-lation models take into account not only lexi-cal probabilities of translating target words givensource words (or in the inverse order), but also theprobabilities of generating words in the target side(Trg model) as does a language model, with thesame context length over both source and targetsides.
It is therefore not surprising that adding theSOUL language model along with all translationmodels (the last line in the table) does not give sig-nificant improvement compared to the other con-figurations.
The different ways of using the SOULtranslation models perform very similarly.Table 2 summarizes the results using MERT in-stead of KBMIRA.
We can observe that using KB-MIRA results in 0.1 to 0.2 BLEU point improve-ments compared to MERT.
Moreover, this impactbecomes more important when more features areconsidered (the last line when all 5 neural net-work models are added into the baseline systems).In short, the use of neural network models yieldsup to 0.6 BLEU improvement on the DWL sys-tem, and a 0.8 BLEU gain on the system withoutDWL.
Unfortunately, the experiments with KB-MIRA were carried out after the the submissiondate.
Therefore the submitted system correspondsto the last line of table 2 indicated in bold.5 ConclusionWe presented a system with two main features: aphrase-based translation system which uses pre-reordering and the integration of SOUL target lan-guage and translation models.
Although the trans-lation performance of the baseline system is al-ready very competitive, the rescoring by SOULmodels improve the performance significantly.
Inthe rescoring step, we used a continuous languagemodel as well as four continuous translation mod-87els.
When combining the different SOUL models,the translation models are observed to be more im-portant in increasing the translation performancethan the language model.
Moreover, we observe aslight benefit to use KBMIRA instead of the stan-dard MERT tuning algorithm.
It is worth noticingthat using KBMIRA improves the performancebut also reduces the variance of the final results.As future work, the integration of the SOULtranslation models could be improved in differ-ent ways.
For SOUL translation models, thereis a mismatch between translation units used dur-ing the training step and those used by the de-coder.
The former are derived using the n-gram-based approach, while the latter use the conven-tional phrase extraction heuristic.
We assume thatreducing this mismatch could improve the overallperformance.
This can be achieved for instanceusing forced decoding to infer a segmentation ofthe training data into translation units.
Then theSOUL translation models can be trained usingthis segmentation.
For the SOUL target languagemodel, in these experiments we only used the En-glish part of the parallel data for training.
Resultsmay be improved by including all the monolingualdata.AcknowledgmentsThe research leading to these results has receivedfunding from the European Union Seventh Frame-work Programme (FP7/2007-2013) under grantagreement n?287658 as well as the French Ar-maments Procurement Agency (DGA) under theRAPID Rapmat project.ReferencesAlexandre Allauzen, Nicolas P?echeux, Quoc KhanhDo, Marco Dinarelli, Thomas Lavergne, Aur?elienMax, Hai-Son Le, and Franc?ois Yvon.
2013.Limsi@ wmt13.
In Proceedings of the Eighth Work-shop on Statistical Machine Translation, pages 60?67.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.S.F.
Chen and J. Goodman.
1996.
An empirical studyof smoothing techniques for language modeling.
InProceedings of the 34th Annual Meeting on Associa-tion for Computational Linguistics (ACL ?96), pages310?318, Santa Cruz, California, USA.Colin Cherry and George Foster.
2012.
Batch tun-ing strategies for statistical machine translation.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 427?436.
Association for Computational Lin-guistics.Josep M. Crego, Franois Yvon, and Jos B. Mari?no.2011.
N-code: an open-source Bilingual N-gramSMT Toolkit.
Prague Bulletin of Mathematical Lin-guistics, 96:49?58.George F. Foster, Roland Kuhn, and Howard Johnson.2006.
Phrasetable smoothing for statistical machinetranslation.
In EMNLP, pages 53?61.Teresa Herrmann, Jan Niehues, and Alex Waibel.2013.
Combining Word Reordering Methods ondifferent Linguistic Abstraction Levels for Statisti-cal Machine Translation.
In Proceedings of the Sev-enth Workshop on Syntax, Semantics and Structurein Statistical Translation, Altanta, Georgia, USA,June.
Association for Computational Linguistics.Dan Klein and Christopher D. Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of ACL2003.Philipp Koehn and Kevin Knight.
2003.
EmpiricalMethods for Compound Splitting.
In EACL, Bu-dapest, Hungary.Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,Chris Callison-Burch, Miles Osborne, and DavidTalbot.
2005.
Edinburgh System Description forthe 2005 IWSLT Speech Translation Evaluation.
InProceedings of the International Workshop on Spo-ken Language Translation (IWSLT), Pittsburgh, PA,USA.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proceedings of ACL 2007, Demonstration Ses-sion, Prague, Czech Republic.Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.
2011.
Structuredoutput layer neural network language model.
In Pro-ceedings of ICASSP, pages 5524?5527.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012a.
Continuous space translation models withneural networks.
pages 39?48, Montr?eal, Canada,June.
Association for Computational Linguistics.Hai-Son Le, Thomas Lavergne, Alexandre Al-lauzen, Marianna Apidianaki, Li Gong, Aur?elienMax, Artem Sokolov, Guillaume Wisniewski, andFranc?ois Yvon.
2012b.
Limsi@ wmt?12.
In Pro-ceedings of the Seventh Workshop on Statistical Ma-chine Translation, pages 330?337.
Association forComputational Linguistics.88Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego,Adri`a de Gispert, Patrick Lambert, Jos?e A.R.
Fonol-losa, and Marta R. Costa-Juss`a.
2006.
N-gram-based machine translation.
Computational Linguis-tics, 32(4):527?549.Mohammed Mediani, Eunah Cho, Jan Niehues, TeresaHerrmann, and Alex Waibel.
2011.
The KITEnglish-French Translation systems for IWSLT2011.
In Proceedings of the Eight Interna-tional Workshop on Spoken Language Translation(IWSLT).R.C.
Moore and W. Lewis.
2010.
Intelligent selectionof language model training data.
In Proceedings ofthe ACL 2010 Conference Short Papers, pages 220?224, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Jan Niehues and Muntsin Kolss.
2009.
A POS-BasedModel for Long-Range Reorderings in SMT.
InFourth Workshop on Statistical Machine Translation(WMT 2009), Athens, Greece.Jan Niehues, Teresa Herrmann, Stephan Vogel, andAlex Waibel.
2011.
Wider Context by Using Bilin-gual Language Models in Machine Translation.
InSixth Workshop on Statistical Machine Translation(WMT 2011), Edinburgh, UK.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
1999.
An Efficient Method for De-termining Bilingual Word Classes.
In EACL?99.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics-Volume 1, pages 160?167.
As-sociation for Computational Linguistics.Anna N. Rafferty and Christopher D. Manning.
2008.Parsing Three German Treebanks: Lexicalized andUnlexicalized Baselines.
In Proceedings of theWorkshop on Parsing German.Kay Rottmann and Stephan Vogel.
2007.
Word Re-ordering in Statistical Machine Translation with aPOS-Based Distortion Model.
In Proceedings ofthe 11th International Conference on Theoreticaland Methodological Issues in Machine Translation(TMI), Sk?ovde, Sweden.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In InternationalConference on New Methods in Language Process-ing, Manchester, United Kingdom.Holger Schwenk.
2007.
Continuous space lan-guage models.
Computer Speech and Language,21(3):492?518, July.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In International Confer-ence on Spoken Language Processing, Denver, Col-orado, USA.Ashish Venugopal, Andreas Zollman, and Alex Waibel.2005.
Training and Evaluating Error MinimizationRules for Statistical Machine Translation.
In Work-shop on Data-drive Machine Translation and Be-yond (WPT-05), Ann Arbor, Michigan, USA.Stephan Vogel.
2003.
SMT Decoder Dissected: WordReordering.
In International Conference on NaturalLanguage Processing and Knowledge Engineering,Beijing, China.89
