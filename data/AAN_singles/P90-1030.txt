Computational structure of generative phonologyand its relation to language comprehension.Eric Sven Ristad*MIT Artificial Intelligence Lab545 Technology SquareCambridge, MA 02139AbstractWe analyse the computational complexity ofphonological models as they have developed overthe past twenty years.
The major results atethat generation and recognition are undecidablefor segmental models, and that recognition is NP-hard for that portion of segmental phonology sub-sumed by modern autosegmental models.
Formalrestrictions are evaluated.1 IntroductionGenerative linguistic theory and human languagecomprehension may both be thought of as com-putations.
The goal of language comprehensionis to construct structural descriptions of linguisticsensations, while the goal of generative theory isto enumerate all and only the possible (grammat-ical) structural descriptions.
These computationsare only indirectly related.
For one, the input tothe two computations is not the same.
As we shallsee below, the most we might say is that generativetheory provides an extensional chatacterlsation flanguage comprehension, which is a function fromsurface forms to complete representations, includ-ing underlying forms.
The goal of this article isto reveal exactly what generative linguistic theorysays about language comprehension in the domainof phonology.The article is organized as follows.
In the nextsection, we provide a brief overview of the com-putational structure of generative phonology.
Insection 3, we introduce the segmental model ofphonology, discuss its computational complexity,and prove that even restricted segmental mod-els are extremely powerful (undecidable).
Subse-quently, we consider various proposed and plausi-ble restrictions on the model, and conclude thateven the maximally restricted segmental model islikely to be intractable.
The fourth section in-.troduces the modern autosegmental (nonlinear)model and discusses its computational complexity.
"The author is supported by a IBM graduatefellowship and eternally indebted to Morris Halleand Michael Kenstowicz for teaching him phonol-ogy.
Thanks to Noam Chomsky, Sandiway Fong, andMichael Kashket for their comments and assistance.235We prove that the natural problem of construct-ing an autosegmental representation f an under-specified surface form is NP-hard.
The articleconcludes by arguing that the complexity proofsare unnatural despite being true of the phonolog-ical models, because the formalism of generativephonology is itself unnatural.The central contributions of this article ate:(i) to explicate the relation between generativetheory and language processing, and argue thatgenerative theories are not models of languageusers primarily because they do not consider theinputs naturally available to language users; and(ii) to analyze the computational complexity ofgenerative phonological theory, as it has developedover the past twenty years, including segmentaland autosegmental models.2 Computational structureof generative phonologyThe structure of a computation may be describedat many levels of abstraction, principally includ-ing: (i) the goal of the computation; (ii) its in-put/output specification (the problem statement),(iii) the algorithm and representation for achiev-ing that specification, and (iv) the primitive opera-tions in which terms the algorithm is implemented(the machine architecture).Using this framework, the computational struc-ture of generative phonology may be described asfollows:?
The computational goal of generative phonol-ogy (as distinct from it's research goals) is toenumerate the phonological dictionaries of alland only the possible human languages.?
The problem statement is to enumerate theobserved phonological dictionary of s particu-lax language from some underlying dictionaryof morphemes (roots and affixes) and phono-logical processes that apply to combinationsof underlying morphemes.?
The algorithm by which this is accomplishedis a derivational process g ('the grammar')from underlying forms z to surface formsy = g(z ) .
Underlying forms are constructedby combining (typically, with concatenationor substitution) the forms stored in the under-lying dictionary of morphemes.
Linguistic re-lations are represented both in the structuraldescriptions and the derivational process.The structural descriptions of phonology arerepresentations of perceivable distinctions be-tween linguistic sounds, such as stress lev-els, syllable structure, tone, and articula-tory gestures.
The underlying and surfaceforms are both drawn from the same classof structural descriptions, which consist ofboth segmental strings and autosegmental re-lations.
A segmental string is a string ofsegments with some representation of con-stituent structur.
In the SPE theory of Chom-sky and Halle (1968) concrete boundary sym-bols are used; in Lexical Phonology, abstractbrackets are used.
Each segment is a set ofphonological features, which are abstract ascompared with phonetic representations, al-though both are given in terms of phoneticfeatures.
Suprasegmental relations are rela-tions among segments, rather than propertiesof individual segments.
For example, a syl-lable is a hierarchical relation between a se-quence of segments (the nucleus of the syl-lable) and the less sonorous segments thatimmediately preceed and follow it (the onsetand coda, respectively).
Syllables must sat-isfy certain universal constraints, such as thesonority sequencing constraint, as well as lan-guage particular ones.a The derivntional process is implemented byan ordered sequence of unrestricted rewritingrules that are applied to the current deriva-tion string to obtain surface forms.According to generative phonology, comprehen-sion consists of finding a structural description fora given surface form.
In effect, the logical prob-lem of language comprehension is reduced to theproblem of searching for the underlying form thatgenerates a given surface form.
When the sur-face form does not transparently identify its cor-responding underlying form, when the space ofpossible underlying forms is large, or when thegrammar g is computationally complex, the logicalproblem of language comprehension can quicklybecome very difficult.In fact, the language comprehension problemis intractable for all segmental theories.
For ex-ample, in the formal system of The Sound Pat.tern of English (SPE) the comprehension prob-lem is undecidable.
Even if we replace the seg-mental representation of cyclic boundaries withthe abstract constituents of Lexical Phonology,and prohibit derivational rules from readjustingconstituent boundaries, comprehension remainsPSPACE-complete.
Let us now turn to the tech-nical details.3 Segmental PhonologyThe essential components of the segmental modelmay be briefly described as follows.
The set offeatures includes both phonological features anddiacritics and the distinguished feature segmentthat marks boundaries.
(An example diacritic isablaut ,  a feature that marks stems that mustundergo a change vowel quality, such as tense-conditioned ablaut in the English sing, sang, sungalternation.)
As noted in SPE, "technically speak-ing, the number of diacritic features hould be atleast as large as the number of rules in the phonol-ogy.
Hence, unless there is a bound on the lengthof a phonology, the set \[of eatures\] should be un-limited."
(fn.1, p.390) Features may be specifiedq- or - or by an integral value 1, 2 , .
.
.
,  N where  Nis the maximal deg/ee of differentiation permittedfor any linguistic feature.
Note that N may varyfrom language to language, because languages ad-mit different degrees of differentiation i such fea-tures as vowel height, stress, and tone.
A set offeature specifications i  called a unit or sometimesa segment.
A string of units is called a matriz ora segmental string.A elementary rule is of the form ZXAYWZXBYW where A and B may be ~b or any unit,A ~ B; X and Y may be matrices (strings ofunits), and Z and W may be thought of a brack-ets labelled with syntactic ategories uch as 'S'or 'N' and so forth.
A comple= rule is a finiteschema for generating a (potentially infinite) setof elementary rules.
1 The rules are organised into1Following 3ohnson (1972), we may define schenmas follows.
The empty string and each unit is s schema;schema may be combined by the operations of union,intersection, egation, kleene star, and exponentiationover the set of units.
Johnson also introduces variablesand Boolean conditions into the schema.
This "schemalanguage" is a extremely powerful characterisation fthe class of regular languages over the alphabet ofunits; it is not used by practicing phonologists.
Be-cause a given complex rule can represent an infinite setof elementary ules, Johnson shows how the iterated,exhaustive application of one complex rule to a givensegmental string can "effect virtually any computablemapping," (p.10) ie., can simulate any TNI computa-tion.
Next, he proposes a more restricted "simultane-ous" mode of application for a complex rule, which isonly capable of performing a finite-state mapping inany application.
This article considers the indepen-dent question of what computations can be performedby a set of elementary ules, and hence provides looselower bounds for Johnson's model.
We note in pass-ing, however, that the problem of simply determiningwhether a given rule is subsumed by one of Johnson'sschema is itself intractable, requiring at least exponen-236lineat sequence R,,R2, .
.
.Rn,  and they ate ap-plied in order to an underlying matrix to obtain asurface matrix.Ignoring a great many issues that are importantfor linguistic reasons but izrelevant for our pur-poses, we may think of the derivational process asfollows.
The input to the derivation, or "underly-ing form," is a bracketed string of morphemes, theoutput of the syntax.
The output of the derivationis the "surface form," a string of phonetic units.The derivation consists of a series of cycles.
Oneach cycle, the ordered sequence of rules is ap-plied to every maximal string of units containingno internal brackets, where each P~+, applies (ordoesn't apply) to the result of applying the imme-diately preceding rule Ri, and so forth.
Each ruleapplies simultaneously to all units in the currentderivations\] string.
For example, if we apply therule A --* B to the string AA, the result is thestring BB.
At the end of the cycle, the last ruleP~ erases the innermost brackets, and then thenext cycle begins with the rule R1.
The deriva-tion terminates when all the brackets ate erased.Some phonological processes, such as the as-similation of voicing across morpheme boundaries,are very common across the world's languages.Other processes, such as the atbitraty insertionof consonants or the substitution of one unit foranother entirely distinct unit, ate extremely rateor entirely unattested.
For this reason, all ade-quate phonological theories must include an ex-plicit measure of the naturalness of a phonologi-cal process.
A phonological theory must also de-fine a criterion to decide what constitutes two in-dependent phonological processes and what con-stitutes a legitimate phonological generalization.Two central hypotheses of segmental phonologyare (i) that the most natural grammaxs containthe fewest symbols and (ii) a set of rules rep-resent independent phonological processes whenthey cannot be combined into a single rule schemaaccording to the intricate notational system firstdescribed in SPE.
(Chapter 9 of Kenstowicz andKisseberth (1979) contains a less technical sum-maty of the SPE system and a discussion of sub-sequent modifications and emendations to it.
)3 .1  Complex i ty  o f  segmenta lrecogn i t ion  and  generat ion .Let us say a dictionary D is a finite set of theunderlying phonological forms (matrices) of mor-phemes.
These morphemes may be combined byconcatenation a d simple substitution (a syntacticcategory is replaced by a morpheme of that cate-gory) to form a possibly infinite set of underlyingforms.
Then we may characterize the two centralcomputations of phonology as follows.tial space.The phonological generation problem (PGP) is:Given a completely specified phonological matrixz and a segmental grammar g, compute the sur-face form y : g(z) of z.The phonological recognition problem (PRP) is:Given a (partially specified) surface form y, a dic-tionary D of underlying forms, and a segmentalgrammar g, decide if the surface form y = g(=)can be derived from some underlying form z ac-cording to the grammar g, where z constructedfrom the forms in D.Lenuna 3.1 The segmental model can directlysimulate the computation of any deterministic~Turing machine M on any input w, using onlyelementary rules.Proof .
We sketch the simulation.
The underlyingform z will represent the TM input w, while thesurface form y will represent the halted state of Mon w. The immediate description of the machine(tape contents, head position, state symbol) is rep-resented in the string of units.
Each unit repre-sents the contents of a tape square.
The unit rep-resenting the currently scanned tape square willalso be specified for two additional features, torepresent the state symbol of the machine and thedirection in which the head will move.
Therefore,three features ate needed, with a number of spec-ifications determined by the finite control of themachine M. Each transition of M is simulated bya phonological rule.
A few rules ate also needed tomove the head position around, and to erase theentire derivation string when the simulated m~chine halts.There are only two key observations, which donot appear to have been noticed before.
The firstis that contraty to populat misstatement, phono-logical rules ate not context-sensitive.
Rather,they ate unrestricted rewriting rules because theycan perform deletions as well as insertions.
(Thisis essential to the reduction, because it allowsthe derivation string to become atbitatily long.
)The second observation is that segmental rulescan f~eely manipulate (insert and delete) bound-ary symbols, and thus it is possible to prolong thederivation indefinitely: we need only employ a ruleR,~_, at the end of the cycle that adds an extraboundary symbol to each end of the derivationstring, unless the simulated machine has halted.The remaining details are omitted, but may befound in Ristad (1990).
\ [ \ ]The immediate consequences are:Theorem I PGP is undecidable.Proof .
By reduction to the undecidable prob-lem w 6 L(M)?
of deciding whether a given TMM accepts an input w. The input to the gen-eration problem consists of an underlying formz that represents w and a segmental grammar237g that simulates the computations of M accord-ing to \]emma 3.1.
The output is a surface formy : g(z) that represents he halted configurationof the TM, with all but the accepting unit erased.\[\]Theorem 2 PRP is undecidable.Proof.
By reduction to the undecidable prob-lem L(M) =?~b of deciding whether a given TMM accepts any inputs.
The input to the recog-nition problem consists of a surface form y thatrepresents he halted accepting state of the TM,a trivial dictionary capable of generating E*, anda segmental grammar g that simulates the com-putations of the TM according to lemma 3.1.
Theoutput is an underlying form z that represents heinput that M accepts.
The only trick is to con-struct a (trivial) dictionary capable of generatingall possible underlying forms E*.
\ [ \ ]An important corollary to lemma 3.1 is that wecan encode a universal Turing machine in a seg-mental grammax.
If we use the four-symbol seven-state "smallest UTM" of Minsky (1969), then theresulting segmental model contains no more thanthree features, eight specifications, and 36 verysimple rules (exact details in Ristad, 1990).
Asmentioned above, a central component of the seg-mental theory is an evaluation metric that favorssimpler (ie., shorter) grammars.
This segmentalgrammar of universal computation appears to con-tain significantly fewer symbols than a segmentalgrammar for any natural language.
Therefore, thiscorollary presents severe conceptual nd empiricalproblems for the segmental theory.Let us now turn to consider the range of plau-sible restrictions on the segmental model.
Atfirst glance, it may seem that the single mostimportant computational restriction is to preventrules from inserting boundaries.
Rules that ma-nipulate boundaries axe called readjustment rules.They axe needed for two reasons.
The first is toreduce the number of cycles in a given deriva-tion by deleting boundaries and flattening syntac-tic structure, for example to prevent he phonol-ogy from assigning too many degrees of stressto a highly-embedded sentence.
The second isto reaxrange the boundaries given by the syn-tax when the intonational phrasing of an utter-ance does not correspond to its syntactic phras-ing (so-called "bracketing paradoxes").
In thiscase, boundaries are merely moved around, whilepreserving the total number of boundaries in thestring.
The only way to accomplish this kind ofbracket readjustment in the segmental model iswith rules that delete brackets and rules that in-sert brackets.
Therefore, if we wish to excluderules that insert boundaries, we must provide analternate mechanism for boundary readjustment.For the sake of axgument--and because it is nottoo hard to construct such a boundary readjust-ment mechanism--let us henceforth adopt his re-striction.
Now how powerful is the segmentalmodel?Although the generation problem is now cer-taiuly decidable, the recognition problem remainsundecidable, because the dictionary and syntaxare both potentially infinite sources of bound-aries: the underlying form z needed to generateany given surface form according to the grammarg could be axbitradly long and contain an axbi-traxy number of boundaries.
Therefore, the com-plexity of the recognition problem is unaffectedby the proposed restriction on boundary readjust-ments.
The obvious restriction then is to addi-tionally limit the depth of embeddings by somefixed constant.
(Chomsky and Halle flirt withthis restriction for the linguistic reasons mentionedabove, but view it as a performance limitation,and hence choose not to adopt it in their theoryof linguistic ompetence.
)Lernma 3.2 Each derivational cycle can directlysimulate any polynomial time alternating Turingmachine (ATM) M computation.Proof.
By reduction from a polynomial-depthATM computation.
The input to the reduction isan ATM M on input w. The output is a segmen-tad grammar g and underlying form z s.t.
the sur-face form y = g(z) represents a halted acceptingcomputation iff M accepts ~v in polynomial time.The major change from lemma 3.1 is to encodethe entire instantaneous description of the ATMstate (ie., tape contents, machine state, head po-sition) in the features of a single unit.
To do thisrequires a polynomial number of features, one foreach possible tape squaxe, plus one feature for themachine state and another for the head position.Now each derivation string represents a level ofthe ATM computation tree.
The transitions of theATM computation axe encoded in a block B as fol-lows.
An AND-transition is simulated by a tripleof rules, one to insert a copy of the current state,and two to implement the two transitions.
An OR-transition is simulated by a pair of disjunctively-ordered rules, one for each of the possible succes-sor states.
The complete rule sequence consistsof a polynomial number of copies of the block B.The last rules in the cycle delete halting states,so that the surface form is the empty string (orreasonably-sized string of 'accepting' units) whenthe ATM computation halts and accepts.
If, onthe other hand, the surface form contains any non-halting or nonaccepting units, then the ATM doesnot accept its input w in polynomial time.
Thereduction may clearly be performed in time poly-nomial in the size of the ATM and its input.
\ [ \ ]Because we have restricted the number of em-beddings in an underlying form to be no more than238a fixed language-universal constant, no derivationcan consist of more than a constant number ofcycles.
Therefore, lemma 3.2 establishes the fol-lowing theorems:Theorem 3 PGP with bounded embeddings isPSPA CE.hard.Proof.
The proof is an immediate consequence oflemma 3.2 and a corollary to the Chandra-Kosen-Stockmeyer theorem (1981) that equates polyno-mial time ATM computations and PSPACE DTMcomputations.
\ [ \ ]Theozem 4 PRP with bounded embeddings isPSPA CE-hard.Proof.
The proof follows from lemma 3.2 andthe Chandra-Kosen-Stockmeyer result.
The dic-tionary consists of the lone unit that encodes theATM starting configuration (ie., input w, startstate, head on leftmost square).
The surface stringis either the empty string or a unit that representsthe halted accepting ATM configuration.
\ [ \ ]There is some evidence that this is the mostwe can do, at least for the PGP.
The requirementthat the reduction be polynomial time limits usto specifying a polynomial number of features anda polynomial number of rules.
Since each featurecorresponds to a tape square, ie., the ATM spaceresource, we are limited to PSPACE ATM compu-tations.
Since each phonological rule correspondsto a next-move relation, ie., one time step of theATM, we are thereby limited to specifying PTIMEATM computations.For the PRP, the dictionary (or syntax-interface) provides the additional ability tonondeterministically guess an arbitrarily long,boundary-free underlying form z with which togenerate a given surface form g(z).
This abilityremains unused in the preceeding proof, and it isnot too hard to see how it might lead to undecid-ability.We conclude this section by summarizing therange of linguistically plausible formal restrictionson the derivational process:Feature system.
As Chomsky and Halle noted,the SPE formal system is most naturally seenas having a variable (unbounded) set of fea-tures and specifications.
This is because lan-guages differ in the diacritics they employ, aswell as differing in the degrees of vowel height,tone, and stress they allow.
Therefore, the setof features must be allowed to vary from lan-guage to language, and in principle is limitedonly by the number of rules in the phonol-ogy; the set of specifications must likewise beallowed to vary from language to language.It is possible, however, to postulate the ex-istence of a large, fixed, language-universalset of phonological features and a fixed upperlimit to the number N of perceivable distinc-tions any one feature is capable of supporting.If we take these upper limits seriously, thenthe class of reductions described in lemma 3.2would no longer be allowed.
(It will be pos-sible to simulate any ~ computation i asingle cycle, however.
)Rule for m__At.
Rules that delete, change, ex-change, or insert segments--as well as rulesthat manipulate boundaries--are crucial tophonological theorizing, and therefore cannotbe crudely constrained.
More subtle and in-direct restrictions are needed.
One approachis to formulate language-universal constraintson phonological representations, and to allowa segment to be altered only when it violatessome constraint.McCarthy (1981:405) proposes a morphemerule constraint (MRC) that requires all mor-phological rules to be of the form A ---, B /Xwhere A is a unit or ~b, and B and X are(possibly null) strings of units.
(X is the im-mediate context of A, to the right or left.
)It should be obvious that the MRC doesnot constrain the computational complexityof segmental phonology.4 Autosegmental PhonologyIn the past decade, generative phonology hasseen a revolution in the linguistic treatment ofsuprasegmental phenomena such as tone, har-mony, infixation, and stress assignment.
Althoughthese autosegmental models have yet to be for-malised, they may be briefly described as follows.Rather than one-dimensional strings of segments,representations may be thought of as "a three-dimensional object hat for concreteness one mightpicture as a spiral-bound notebook," whose spineis the segmental string and whose pages containsimple constituent structures that are indendentof the spine (Halle 1985).
One page represents hesequence of tones associated with a given articu-lation.
By decoupling the representation f tonalsequences from the articulation sequence, it is pos-sible for segmental sequences of different lengthsto nonetheless be associated to the same tone se-quence.
For example, the tonal sequence Low-High-High, which is used by English speakers toexpress urprise when answering a question, mightbe associated to a word containing any numberof syllables, from two (Brazi 0 to twelve (floccin-auccinihilipilification) and beyond.
Other pages(called "planes") represent morphemes, yllablestructure, vowels and consonants, and the tree ofarticulatory (ie., phonetic) features.2394.1 Complex i ty  o f  autosegmenta lrecogn i t ion .In this section, we prove that the PRP for au-tosegmental models is NP-hard, a significant re-duction in complexity from the undecidable andPSPACE-hard computations of segmental theo-ries.
(Note however that autosegmental repre-sentations have augmented--but not replaced--portions of the segmental model, and therefore,unless something can be done to simplify segmen-tal derivations, modern phonology inherits the in-tractability of purely segmental pproaches.
)Let us begin by thinking of the NP-complete3-Satisfiability problem (3SAT) as a set of inter-acting constraints.
In particular, every satisfiableBoolean formula in 3-CNF is a string of clausesC1, C2, .
.
.
,  Cp in the variables zl, z=, .
.
.
,  z ,  thatsatisfies the following three constraints: (i) nega-tion: a variable =j and its negation ~ have op-posite truth values; (ii) clausal satisfaction: everyclause C~ = (a~VbiVc/) contains a true literal (a lit-eral is a variable or its negation); (iii) consistencyof truth assignments: every unnegated literal ofa given variable is assigned the same truth value,either 1 or 0.Lemma 4.1 Autosegmental representations canenforce the 3SAT constraints.ProoL  The idea of the proof is to encode negationand the truth values of variables in features; toenforce clausal satisfication with a local autoseg-mental process, such as syllable structure; andto ensure consistency of truth assignments witha nonlocal autosegmental process, such as a non-concatenative morphology or long-distance assim-ilation (harmony).
To implement these ideas wemust examine morphology, harmony, and syllablestructure.Morphology.
In the more familiar languagesof the world, such as Romance languages, mor-phemes are concatenated to form words.
In otherlanguages, uch as Semitic languages, a morphememay appear more that once inside another mor-pheme (this is called infixation).
For example, theArabic word katab, meaning 'he wrote', is formedfrom the active perfective morpheme a doubly in-fixed to the ktb morpheme.
In the autosegmentalmodel, each morpheme is assigned its own plane.We can use this system of representation to ensureconsistency of truth assigments.
Each Booleanvariable z~ is represented by a separate morphemep~, and every literal of =i in the string of formulaliterals is associated to the one underlying mor-pheme p~.Harmony.
Assimilation is the common phono-logical process whereby some segment comes toshare properties of an adjacent segment.
In En-glish, consonant nasality assimilates to immedi-ately preceding vowels; assimilation also occurs240across morpheme boundaries, as the varied surfaceforms of the prefx in- demonstrate: in+logical - ,illogical and in-l-probable --, improbable.
In otherlanguages, assimilation is unbounded and can af-fect nonadjacent segments: these assimilation pro-cesses are called harmony systems.
In the Turkiclanguages all sutFtx vowels assimilate the backnesssfeature of the last stem vowel; in Capanshua, vow-els and glides that precede a word-final deletednasal (an underlying nasal segment absent fromthe surface form) are all nasalized.
In the autoseg-mental model, each harmonic feature is assignedits own plane.
As with morpheme-infixation, wecan represent each Boolean variable by a harmonicfeature, and thereby ensure consistency of truthassignments.Syllable structure.
Words are partitioned intosyllables.
Each syllable contains one or more vow-ds V (its nucleus) that may be preceded or fol-lowed by consonants C. For example, the Ara-bic word ka.tab consists of two syIlabhs, the two-segment syllable CV and the three-segment dosedsyllable CVC.
Every segment is assigned a sonor-ity value, hrhich (intuitively) is proportional to theopenness of the vocal cavity.
For example, vowelsare the most sonorous egments, while stops suchas p or b are the least sonorous.
Syllables obey alanguage-universal onority sequencing constraint(SSC), which states that the nucleus is the sonor-ity peak of a syllable, and that the sonority ofadjacent segments wiftly and monotonically de-creases.
We can use the SSC to ensure that everyclause C~ contains a true literal as follows.
Thecentred idea is to make literal truth correspond tothe stricture feature, so that a true literal (repre-sented as a vowel) is more sonorous than a falseliteral (represented as a consonant).
Each clauseC~ - (a~ V b~ V c~) is encoded as a segmental stringC - z ,  - zb - zc, where C is a consonant of sonor-ity 1.
Segment zG has sonority 10 when literalat is true, 2 otherwise; segment =s has sonority 9when literal bi is true, 5 otherwise; and segment zchas sonority 8 when literal q is true, 2 otherwise.Of the eight possible truth values of the three lit-erals and ~he corresponding syllabifications, 0nlythe syllabification corresponding to three false lit-erals is excluded by the SSC.
In that case, thecorresponding string of four consonants C-C-C-Chas the sonority sequence 1-2-5-2.
No immediatelypreceeding or following segment of any sonoritycan result in a syllabification that obeys the SSC.Therefore, all Boolean clauses must contain a trueliteral.
(Complete proof in Ristad, 1990) \ [ \ ]The direct consequence of this lemma 4.1 is:Theorem 5 PRP for the autosegraental model isNP-hard.Proof .
By reduction to 3SAT.
The idea is toconstruct a surface form that completely identi-ties the variables and their negation or lack ofit, but does not specify the truth values of thosevariables.
The dictionary will generate all possi-ble underlying forms (infixed morphemes or har-monic strings), one for each possible truth as-signment, and the autosegmental representationof lemma 4.1 will ensure that generated formulasare in fact satisfiable.
\ [ \ ]5 Conc lus ion .In my opinion, the preceding proofs are unnatural,despite being true of the phonological models, be-cause the phonological models themselves are un-natural.
Regarding segmental models, the unde-cidability results tell us that the empirical contentof the SPE theory is primarily in the particularrules postulated for English, and not in the ex-tremely powerful and opaque formal system.
Wehave also seen that symbol-minimization is a poormetric for naturalness, and that the complex no-rational system of SPE (not discussed here) is aninadequate characterization f the notion of "ap-propriate phonological generalisation.
"2Because not every segmental grammar g gener-ates a natural set of sound patterns, why shouldwe have any faith or interest in the formal system?The only justification for these formal systemsthen is that they are good programming languagesfor phonological processes, that clearly captureour intuitions about human phonology.
But seg-mental theories are not such good programminglanguages.
They are notationally-constrained anhighly-articulated, which limits their expressivepower; they obscurely represent phonological re-lations in rules and in the derivation process it-self, and hide the dependency relations and inter-actions among phonological processes in rule or-dering, disjunctive ordering, blocks, and cyclicity, sYet, despite all these opaque notational con-straints, it is possible to write a segmental gram-mar for any decidable set.A third unnatural feature is that the goal ofenumerating structural descriptions has an indi-rect and computationally costly connection to thegoal of language comprehension, which is to con-struct a structural description of a given utter-ance.
When information is missing from the sur-face form, the generative model obligates itselfto enumerate all possible underlying forms thatmight generate the surface form.
When the gen-erative process is lengthy, capable of deletions, orcapable of enforcing complex interactions betweennonlocal and local relations, then the logical prob-lem of language comprehension will be intractable.Natural phonological processes eem to avoidcomplexity and simplify interactions.
It is hardto find an phonological constraint that is absoluteand inviolable.
There are always exceptions, ex-ceptions to the exceptions, and so forth.
Deletionprocesses like apocope, syncopy, cluster simplica-tion and stray erasure, as well as insertions, seemto be motivated by the necessity of modifying arepresentation to satisfy a phonological constraint,not to exclude representations or to generate com-plex sets, as we have used them here.Finally, the goal of enumerating structural de-scriptions might not be appropriate for phonologyand morphology, because the set of phonologicalwords is only finite and phrase-level phonology iscomputationally simple.
There is no need or ra-tional for employing such a powerful derivationalsystem when all we are trying to do is capturethe relatively little systematicity in a finite set ofrepresentations.6 References.2The explication of what constitutes a "naturalrule" is significantly more elusive than the symbol-minimization metric suggests.
Explicit symbol-counting is rarely performed by practicing phonolo-gists, and when it is, it results in unnatural rules.Moreover, the goal of constructing the smallest gram-mar for a given (infinite) set is not attainable in prin-ciple, because it requires us to solve the undecid-able TM equivalence problem.
Nor does the symbol-counting metzlc constrain the generative or computa-tional power of the formalism.
Worst of all, the UTMsimulation suggested above shows that symbol countdoes not correspond to "naturalness."
In fact, twoof the simplest grammars generate ~ and ~' ,  both ofwhich are extremely unnatural.3A further difficulty for autosegmental models (notbrought out by the proof) is that the interactionsamong planes is obscured by the current practice ofimposing an absolute order on the construction ofplanes in the derivation process.
For example, in En-glish phonology, syllable structure is constructed be-Chandra, A., D. Kozen, and L. Stockmeyer, 1981.Alternation.
3.
A CM 28(1):114-133.Chomsky, Noam and Morris Halle.
1968.
TheSound Pattern of English.
New York: HarperRow.Halle, Morris.
1985.
"Speculations about the rep-resentation of words in memory."
In PhoneticLinguistics, Essays in Honor of Peter Lade-\]oged, V. Fromkin, ed.
Academic Press.Johnson, C. Douglas.
1972.
Formal Aspects ofPhonological Description.
The Hague: Mou-ton.Kenstowicz, Michael and Charles Kisseberth.1979.
Generative Phonology.
New York:fore stress is assigned, and then recomputed on the ba-sis of the resulting stress assignment.
A more naturalapproach would be to let stress and syllable structurecomputations intermingle in a nondirectional process.241Academic Press.McCarthy, John.
1981.
"A prosodic theory ofnonconcatenative morphology."
LinguisticInquiry 12, 373-418.Minsky, Marvin.
1969.
Computation: finite andinfinite machines.
Englewood Cliffs: PrenticeHall.Ristad, Eric S. 1990.
Computational structure ofhuman language.
Ph.D dissertation, MIT De-partment of Electrical Engineering and Com-puter Science.242
