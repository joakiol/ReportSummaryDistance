Syntagmatic Kernels:a Word Sense Disambiguation Case StudyClaudio Giuliano and Alfio Gliozzo and Carlo StrapparavaITC-irst, Istituto per la Ricerca Scientifica e TecnologicaI-38050, Trento, ITALY{giuliano,gliozzo,strappa}@itc.itAbstractIn this paper we present a family of ker-nel functions, named Syntagmatic Ker-nels, which can be used to model syn-tagmatic relations.
Syntagmatic relationshold among words that are typically collo-cated in a sequential order, and thus theycan be acquired by analyzing word se-quences.
In particular, Syntagmatic Ker-nels are defined by applying a Word Se-quence Kernel to the local contexts of thewords to be analyzed.
In addition, thisapproach allows us to define a semi su-pervised learning schema where externallexical knowledge is plugged into the su-pervised learning process.
Lexical knowl-edge is acquired from both unlabeled dataand hand-made lexical resources, such asWordNet.
We evaluated the syntagmatickernel on two standard Word Sense Dis-ambiguation tasks (i.e.
English and Ital-ian lexical-sample tasks of Senseval-3),where the syntagmatic information playsa crucial role.
We compared the Syntag-matic Kernel with the standard approach,showing promising improvements in per-formance.1 IntroductionIn computational linguistics, it is usual to deal withsequences: words are sequences of letters and syn-tagmatic relations are established by sequences ofwords.
Sequences are analyzed to measure morpho-logical similarity, to detect multiwords, to representsyntagmatic relations, and so on.
Hence modelingsyntagmatic relations is crucial for a wide varietyof NLP tasks, such as Named Entity Recognition(Gliozzo et al, 2005a) and Word Sense Disambigua-tion (WSD) (Strapparava et al, 2004).In general, the strategy adopted to model syntag-matic relations is to provide bigrams and trigrams ofcollocated words as features to describe local con-texts (Yarowsky, 1994), and each word is regardedas a different instance to classify.
For instance, oc-currences of a given class of named entities (suchas names of persons) can be discriminated in textsby recognizing word patterns in their local contexts.For example the token Rossi, whenever is precededby the token Prof., often represents the name of aperson.
Another task that can benefit from modelingthis kind of relations is WSD.
To solve ambiguity itis necessary to analyze syntagmatic relations in thelocal context of the word to be disambiguated.
Inthis paper we propose a kernel function that can beused to model such relations, the Syntagmatic Ker-nel, and we apply it to two (English and Italian)lexical-sample WSD tasks of the Senseval-3 com-petition (Mihalcea and Edmonds, 2004).In a lexical-sample WSD task, training data areprovided as a set of texts, in which for each texta given target word is manually annotated with asense from a predetermined set of possibilities.
Tomodel syntagmatic relations, the typical supervisedlearning framework adopts as features bigrams andtrigrams in a local context.
The main drawback ofthis approach is that non contiguous or shifted col-57locations cannot be identified, decreasing the gener-alization power of the learning algorithm.
For ex-ample, suppose that the verb to score has to be dis-ambiguated into the sentence ?Ronaldo scored thegoal?, and that the sense tagged example ?the foot-ball player scores#1 the first goal?
is provided fortraining.
A traditional feature mapping would ex-tract the bigram w+1 w+2:the goal to represent theformer, and the bigram w+1 w+2:the first to indexthe latter.
Evidently such features will not match,leading the algorithm to a misclassification.In the present paper we propose the SyntagmaticKernel as an attempt to solve this problem.
TheSyntagmatic Kernel is based on a Gap-WeightedSubsequences Kernel (Shawe-Taylor and Cristian-ini, 2004).
In the spirit of Kernel Methods, thiskernel is able to compare sequences directly in theinput space, avoiding any explicit feature mapping.To perform this operation, it counts how many timesa (non-contiguous) subsequence of symbols u oflength n occurs in the input string s, and penalizesnon-contiguous occurrences according to the num-ber of the contained gaps.
To define our Syntag-matic Kernel, we adapted the generic definition ofthe Sequence Kernels to the problem of recognizingcollocations in local word contexts.In the above definition of Syntagmatic Kernel,only exact word-matches contribute to the similar-ity.
One shortcoming of this approach is that (near-)synonyms will never be considered similar, lead-ing to a very low generalization power of the learn-ing algorithm, that requires a huge amount of datato converge to an accurate prediction.
To solve thisproblem we provided external lexical knowledge tothe supervised learning algorithm, in order to definea ?soft-matching?
schema for the kernel function.For example, if we consider as equivalent the termsRonaldo and football player, the proposition ?Thefootball player scored the first goal?
is equivalent tothe sentence ?Ronaldo scored the first goal?, pro-viding a strong evidence to disambiguate the latteroccurrence of the verb.We propose two alternative soft-matching criteriaexploiting two different knowledge sources: (i) handmade resources and (ii) unsupervised term similar-ity measures.
The first approach performs a soft-matching among all those synonyms words in Word-Net, while the second exploits domain relations, ac-quired from unlabeled data, for the same purpose.Our experiments, performed on two standardWSD benchmarks, show the superiority of the Syn-tagmatic Kernel with respect to a classical flat vectorrepresentation of bigrams and trigrams.The paper is structured as follows.
Section 2 in-troduces the Sequence Kernels.
In Section 3 theSyntagmatic Kernel is defined.
Section 4 explainshow soft-matching can be exploited by the Collo-cation Kernel, describing two alternative criteria:WordNet Synonymy and Domain Proximity.
Sec-tion 5 gives a brief sketch of the complete WSDsystem, composed by the combination of differentkernels, dealing with syntagmatic and paradigmaticaspects.
Section 6 evaluates the Syntagmatic Kernel,and finally Section 7 concludes the paper.2 Sequence KernelsThe basic idea behind kernel methods is to embedthe data into a suitable feature space F via a map-ping function ?
: X ?
F , and then use a linear al-gorithm for discovering nonlinear patterns.
Insteadof using the explicit mapping ?, we can use a kernelfunction K : X ?
X ?
R, that corresponds to theinner product in a feature space which is, in general,different from the input space.Kernel methods allow us to build a modular sys-tem, as the kernel function acts as an interface be-tween the data and the learning algorithm.
Thusthe kernel function becomes the only domain spe-cific module of the system, while the learning algo-rithm is a general purpose component.
Potentiallyany kernel function can work with any kernel-basedalgorithm.
In our system we use Support Vector Ma-chines (Cristianini and Shawe-Taylor, 2000).Sequence Kernels (or String Kernels) are a fam-ily of kernel functions developed to compute theinner product among images of strings in high-dimensional feature space using dynamic program-ming techniques (Shawe-Taylor and Cristianini,2004).
The Gap-Weighted Subsequences Kernel isthe most general Sequence Kernel.
Roughly speak-ing, it compares two strings by means of the num-ber of contiguous and non-contiguous substrings ofa given length they have in common.
Non contigu-ous occurrences are penalized according to the num-ber of gaps they contain.58Formally, let ?
be an alphabet of |?| symbols,and s = s1s2 .
.
.
s|s| a finite sequence over ?
(i.e.si ?
?, 1 6 i 6 |s|).
Let i = [i1, i2, .
.
.
, in], with1 6 i1 < i2 < .
.
.
< in 6 |s|, be a subset of theindices in s: we will denote as s[i] ?
?n the sub-sequence si1si2 .
.
.
sin .
Note that s[i] does not nec-essarily form a contiguous subsequence of s. Forexample, if s is the sequence ?Ronaldo scored thegoal?
and i = [2, 4], then s[i] is ?scored goal?.
Thelength spanned by s[i] in s is l(i) = in ?
i1 + 1.The feature space associated with the Gap-WeightedSubsequences Kernel of length n is indexed by I =?n, with the embedding given by?nu(s) =Xi:u=s[i]?l(i), u ?
?n, (1)where ?
?
]0, 1] is the decay factor used to penalizenon-contiguous subsequences1 .
The associate ker-nel is defined asKn(s, t) = ?
?n(s), ?n(t)?
=Xu??n?nu(s)?nu(t).
(2)An explicit computation of Equation 2 is unfea-sible even for small values of n. To evaluate moreefficiently Kn, we use the recursive formulation pro-posed in (Lodhi et al, 2002; Saunders et al, 2002;Cancedda et al, 2003) based on a dynamic program-ming implementation.
It is reported in the followingequations:K?0(s, t) = 1, ?s, t, (3)K?i(s, t) = 0, if min(|s|, |t|) < i, (4)K?
?i (s, t) = 0, if min(|s|, |t|) < i, (5)K?
?i (sx, ty) =(?K?
?i (sx, t), if x 6= y;?K?
?i (sx, t) + ?2K?i?1(s, t), otherwise.
(6)K?i(sx, t) = ?K?i(s, t) + K?
?i (sx, t), (7)Kn(s, t) = 0, if min(|s|, |t|) < n, (8)Kn(sx, t) = Kn(s, t) +Xj:tj=x?2K?n?1(s, t[1 : j ?
1]),(9)K ?n and K ?
?n are auxiliary functions with a sim-ilar definition as Kn used to facilitate the compu-tation.
Based on all definitions above, Kn can be1Notice that by choosing ?
= 1 sparse subsequences arenot penalized.
On the other hand, the kernel does not take intoaccount sparse subsequences with ?
?
0.computed in O(n|s||t|).
Using the above recursivedefinition, it turns out that computing all kernel val-ues for subsequences of lengths up to n is not signif-icantly more costly than computing the kernel for nonly.In the rest of the paper we will use the normalisedversion of the kernel (Equation 10) to keep the val-ues comparable for different values of n and to beindependent from the length of the sequences.K?
(s, t) = K(s, t)pK(s, s)K(t, t).
(10)3 The Syntagmatic KernelAs stated in Section 1, syntagmatic relations holdamong words arranged in a particular temporal or-der, hence they can be modeled by Sequence Ker-nels.
The Syntagmatic Kernel is defined as a linearcombination of Gap-Weighted Subsequences Ker-nels that operate at word and PoS tag level.
In partic-ular, following the approach proposed by Canceddaet al (2003), it is possible to adapt sequence kernelsto operate at word level by instancing the alphabet ?with the vocabulary V = {w1, w2, .
.
.
, wk}.
More-over, we restricted the generic definition of the Gap-Weighted Subsequences Kernel to recognize collo-cations in the local context of a specified word.
Theresulting kernel, called n-gram Collocation Kernel(KnColl), operates on sequences of lemmata around aspecified word l0 (i.e.
l?3, l?2, l?1, l0, l+1, l+2, l+3).This formulation allows us to estimate the number ofcommon (sparse) subsequences of lemmata (i.e.
col-locations) between two examples, in order to capturesyntagmatic similarity.Analogously, we defined the PoS Kernel (KnPoS)to operate on sequences of PoS tags p?3, p?2, p?1,p0, p+1, p+2, p+3, where p0 is the PoS tag of l0.The Collocation Kernel and the PoS Kernel aredefined by Equations 11 and 12, respectively.KColl(s, t) =n?l=1K lColl(s, t) (11)andKPoS(s, t) =n?l=1K lP oS(s, t).
(12)Both kernels depend on the parameter n, the lengthof the non-contiguous subsequences, and ?, the de-59cay factor.
For example, K2Coll allows us to repre-sent all (sparse) bi-grams in the local context of aword.Finally, the Syntagmatic Kernel is defined asKSynt(s, t) = KColl(s, t) + KPoS(s, t).
(13)We will show that in WSD, the Syntagmatic Ker-nel is more effective than standard bigrams and tri-grams of lemmata and PoS tags typically used asfeatures.4 Soft-Matching CriteriaIn the definition of the Syntagmatic Kernel only ex-act word matches contribute to the similarity.
Toovercome this problem, we further extended the def-inition of the Gap-Weigthed Subsequences Kernelgiven in Section 2 to allow soft-matching betweenwords.
In order to develop soft-matching criteria,we follow the idea that two words can be substi-tuted preserving the meaning of the whole sentenceif they are paradigmatically related (e.g.
synomyns,hyponyms or domain related words).
If the meaningof the proposition as a whole is preserved, the mean-ing of the lexical constituents of the sentence willnecessarily remain unchanged too, providing a vi-able criterion to define a soft-matching schema.
Thiscan be implemented by ?plugging?
external paradig-matic information into the Collocation kernel.Following the approach proposed by (Shawe-Taylor and Cristianini, 2004), the soft-matchingGap-Weighted Subsequences Kernel is now calcu-lated recursively using Equations 3 to 5, 7 and 8,replacing Equation 6 by the equation:K?
?i (sx, ty) = ?K?
?i (sx, t) + ?2axyK?i?1(s, t),?x, y, (14)and modifying Equation 9 to:Kn(sx, t) = Kn(s, t) +|t|Xj?2axtjK?n?1(s, t[1 : j ?
1]).
(15)where axy are entries in a similarity matrix A be-tween symbols (words).
In order to ensure that theresulting kernel is valid, A must be positive semi-definite.In the following subsections, we describe two al-ternative soft-matching criteria based on WordNetSynonymy and Domain Proximity.
In both cases, toshow that the similarity matrices are a positive semi-definite we use the following result:Proposition 1 A matrix A is positive semi-definiteif and only if A = BTB for some real matrix B.The proof is given in (Shawe-Taylor and Cristianini,2004).4.1 WordNet SynonymyThe first solution we have experimented exploits alexical resource representing paradigmatic relationsamong terms, i.e.
WordNet.
In particular, we usedWordNet-1.7.1 for English and the Italian part ofMultiWordNet2.In order to find a similarity matrix between terms,we defined a vector space where terms are repre-sented by the WordNet synsets in which such termsappear.
Hence, we can view a term as vector inwhich each dimension is associated with one synset.The term-by-synset matrix S is then the matrixwhose rows are indexed by the synsets.
The en-try xij of S is 1 if the synset sj contains the termwi, and 0 otherwise.
The term-by-synset matrix Sgives rise to the similarity matrix A = SST be-tween terms.
Since A can be rewritten as A =(ST )TST = BTB, it follows directly by Proposi-tion 1 that it is positive semi-definite.It is straightforward to extend the soft-matchingcriterion to include hyponym relation, but weachieved worse results.
In the evaluation section wewill not report such results.4.2 Domain ProximityThe approach described above requires a large scalelexical resource.
Unfortunately, for many languages,such a resource is not available.
Another possibilityfor implementing soft-matching is introducing thenotion of Semantic Domains.Semantic Domains are groups of stronglyparadigmatically related words, and can be acquiredautomatically from corpora in a totally unsuper-vised way (Gliozzo, 2005).
Our proposal is to ex-ploit a Domain Proximity relation to define a soft-matching criterion on the basis of an unsupervisedsimilarity metric defined in a Domain Space.
TheDomain Space can be determined once a Domain2http://multiwordnet.itc.it60Model (DM) is available.
This solution is evidentlycheaper, because large collections of unlabeled textscan be easily found for every language.A DM is represented by a k ?
k?
rectangular ma-trix D, containing the domain relevance for eachterm with respect to each domain, as illustrated inTable 1.
DMs can be acquired from texts by exploit-MEDICINE COMPUTER SCIENCEHIV 1 0AIDS 1 0virus 0.5 0.5laptop 0 1Table 1: Example of Domain Model.ing a lexical coherence assumption (Gliozzo, 2005).To this aim, Term Clustering algorithms can be used:a different domain is defined for each cluster, andthe degree of association between terms and clusters,estimated by the unsupervised learning algorithm,provides a domain relevance function.
As a clus-tering technique we exploit Latent Semantic Analy-sis (LSA), following the methodology described in(Gliozzo et al, 2005b).
This operation is done off-line, and can be efficiently performed on large cor-pora.LSA is performed by means of SVD of the term-by-document matrixT representing the corpus.
TheSVD algorithm can be exploited to acquire a domainmatrix D from a large corpus in a totally unsuper-vised way.
SVD decomposes the term-by-documentmatrix T into three matrices T = V?kUT where?k is the diagonal k ?
k matrix containing the ksingular values of T. D = V?k?
where k?
k.Once a DM has been defined by the matrixD, theDomain Space is a k?
dimensional space, in whichboth texts and terms are represented by means ofDomain Vectors (DVs), i.e.
vectors representing thedomain relevances among the linguistic object andeach domain.
The DV ~w?i for the term wi ?
V is theith row of D, where V = {w1, w2, .
.
.
, wk} is thevocabulary of the corpus.The term-by-domain matrix D gives rise to theterm-by-term similarity matrix A = DDT amongterms.
It follows from Proposition 1 that A is posi-tive semi-definite.5 Kernel Combination for WSDTo improve the performance of a WSD system, itis possible to combine different kernels.
Indeed,we followed this approach in the participation toSenseval-3 competition, reaching the state-of-the-art in many lexical-sample tasks (Strapparava et al,2004).
While this paper is focused on SyntagmaticKernels, in this section we would like to spend somewords on another important component for a com-plete WSD system: the Domain Kernel, used tomodel domain relations.Syntagmatic information alone is not sufficient todefine a full kernel for WSD.
In fact, in (Magniniet al, 2002), it has been claimed that knowing thedomain of the text in which the word is located is acrucial information for WSD.
For example the (do-main) polysemy among the COMPUTER SCIENCEand the MEDICINE senses of the word virus canbe solved by simply considering the domain of thecontext in which it is located.This fundamental aspect of lexical polysemy canbe modeled by defining a kernel function to esti-mate the domain similarity among the contexts ofthe words to be disambiguated, namely the DomainKernel.
The Domain Kernel measures the similarityamong the topics (domains) of two texts, so to cap-ture domain aspects of sense distinction.
It is a vari-ation of the Latent Semantic Kernel (Shawe-Taylorand Cristianini, 2004), in which a DM is exploitedto define an explicit mapping D : Rk ?
Rk?
fromthe Vector Space Model (Salton and McGill, 1983)into the Domain Space (see Section 4), defined bythe following mapping:D(~tj) = ~tj(IIDFD) = ~t?j (16)where IIDF is a k ?
k diagonal matrix such thatiIDFi,i = IDF (wi), ~tj is represented as a row vector,and IDF (wi) is the Inverse Document Frequency ofwi.
The Domain Kernel is then defined by:KD(ti, tj) =?D(ti),D(tj)???D(tj),D(tj)??D(ti),D(ti)?
(17)The final system for WSD results from a com-bination of kernels that deal with syntagmatic andparadigmatic aspects (i.e.
PoS, collocations, bag ofwords, domains), according to the following kernel61combination schema:KC(xi, xj) =n?l=1Kl(xi, xj)?Kl(xj , xj)Kl(xi, xi)(18)6 EvaluationIn this section we evaluate the Syntagmatic Kernel,showing that it improves over the standard featureextraction technique based on bigrams and trigramsof words and PoS tags.6.1 Experimental settingsWe conducted the experiments on two lexical sam-ple tasks (English and Italian) of the Senseval-3competition (Mihalcea and Edmonds, 2004).
Inlexical-sample WSD, after selecting some targetwords, training data is provided as a set of texts.For each text a given target word is manually anno-tated with a sense from a predetermined set of pos-sibilities.
Table 2 describes the tasks by reportingthe number of words to be disambiguated, the meanpolysemy, and the dimension of training, test andunlabeled corpora.
Note that the organizers of theEnglish task did not provide any unlabeled material.So for English we used a domain model built fromthe training partition of the task (obviously skippingthe sense annotation), while for Italian we acquiredthe DM from the unlabeled corpus made availableby the organizers.#w pol # train # test # unlabEnglish 57 6.47 7860 3944 7860Italian 45 6.30 5145 2439 74788Table 2: Dataset descriptions.6.2 Performance of the Syntagmatic KernelTable 3 shows the performance of the SyntagmaticKernel on both data sets.
As baseline, we reportthe result of a standard approach consisting on ex-plicit bigrams and trigrams of words and PoS tagsaround the words to be disambiguated (Yarowsky,1994).
The results show that the Syntagmatic Ker-nel outperforms the baseline in any configuration(hard/soft-matching).
The soft-matching criteriafurther improve the classification performance.
Itis interesting to note that the Domain Proximitymethodology obtained better results than WordNetStandard approachEnglish ItalianBigrams and trigrams 67.3 51.0Syntagmatic KernelHard matching 67.7 51.9Soft matching (WordNet) 67.3 51.3Soft matching (Domain proximity) 68.5 54.0Table 3: Performance (F1) of the Syntagmatic Ker-nel.Synonymy.
The different results observed betweenItalian and English using the Domain Proximitysoft-matching criterion are probably due to the smallsize of the unlabeled English corpus.In these experiments, the parameters n and ?
areoptimized by cross-validation.
For KnColl, we ob-tained the best results with n = 2 and ?
= 0.5.
ForKnPoS , n = 3 and ?
?
0.
The domain cardinality k?was set to 50.Finally, the global performance (F1) of the fullWSD system (see Section 5) on English and Italianlexical sample tasks is 73.3 for English and 61.3 forItalian.
To our knowledge, these figures representthe current state-of-the-art on these tasks.7 Conclusion and Future WorkIn this paper we presented the Syntagmatic Kernels,i.e.
a set of kernel functions that can be used tomodel syntagmatic relations for a wide variety ofNatural Language Processing tasks.
In addition, weproposed two soft-matching criteria for the sequenceanalysis, which can be easily modeled by relax-ing the constraints in a Gap-Weighted SubsequencesKernel applied to local contexts of the word to beanalyzed.
Experiments, performed on two lexicalsample Word Sense Disambiguation benchmarks,show that our approach further improves the stan-dard techniques usually adopted to deal with syntag-matic relations.
In addition, the Domain Proximitysoft-matching criterion allows us to define a semi-supervised learning schema, improving the overallresults.For the future, we plan to exploit the SyntagmaticKernel for a wide variety of Natural Language Pro-cessing tasks, such as Entity Recognition and Re-lation Extraction.
In addition we are applying thesoft matching criteria here defined to Tree Kernels,62in order to take into account lexical variability inparse trees.
Finally, we are going to further improvethe soft-matching criteria here proposed by explor-ing the use of entailment criteria for substitutability.AcknowledgmentsThe authors were partially supported by the Onto-Text Project, funded by the Autonomous Provinceof Trento under the FUP-2004 research program.ReferencesN.
Cancedda, E. Gaussier, C. Goutte, and J.M.
Renders.2003.
Word-sequence kernels.
Journal of MachineLearning Research, 32(6):1059?1082.N.
Cristianini and J. Shawe-Taylor.
2000.
An introduc-tion to Support Vector Machines.
Cambridge Univer-sity Press.A.
Gliozzo, C. Giuliano, and R. Rinaldi.
2005a.
Instancefiltering for entity recognition.
ACM SIGKDD Explo-rations, special Issue on Natural Language Processingand Text Mining, 7(1):11?18, June.A.
Gliozzo, C. Giuliano, and C. Strapparava.
2005b.
Do-main kernels for word sense disambiguation.
In Pro-ceedings of the 43rd annual meeting of the Associationfor Computational Linguistics (ACL-05), pages 403?410, Ann Arbor, Michigan, June.A.
Gliozzo.
2005.
Semantic Domains in Computa-tional Linguistics.
Ph.D. thesis, ITC-irst/University ofTrento.H.
Lodhi, J. Shawe-Taylor, N. Cristianini, andC.
Watkins.
2002.
Text classification using stringkernels.
Journal of Machine Learning Research,2(3):419?444.B.
Magnini, C. Strapparava, G. Pezzulo, and A. Gliozzo.2002.
The role of domain information in wordsense disambiguation.
Natural Language Engineer-ing, 8(4):359?373.R.
Mihalcea and P. Edmonds, editors.
2004.
Proceed-ings of SENSEVAL-3: Third International Workshopon the Evaluation of Systems for the Semantic Analy-sis of Text, Barcelona, Spain, July.G.
Salton and M.H.
McGill.
1983.
Introduction to mod-ern information retrieval.
McGraw-Hill, New York.C.
Saunders, H. Tschach, and J. Shawe-Taylor.
2002.Syllables and other string kernel extensions.
In Pro-ceedings of 19th International Conference onMachineLearning (ICML02).J.
Shawe-Taylor and N. Cristianini.
2004.
Kernel Meth-ods for Pattern Analysis.
Cambridge University Press.C.
Strapparava, C. Giuliano, and A. Gliozzo.
2004.
Pat-tern abstraction and term similarity for word sense dis-ambiguation: IRST at Senseval-3.
In Proceedings ofSENSEVAL-3: Third International Workshop on theEvaluation of Systems for the Semantic Analysis ofText, Barcelona, Spain, July.D.
Yarowsky.
1994.
Decision lists for lexical ambiguityresolution: Application to accent restoration in span-ish and french.
In Proceedings of the 32nd AnnualMeeting of the ACL, pages 88?95, Las Cruces, NewMexico.63
