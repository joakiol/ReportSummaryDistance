Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 582?592,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsBilingual Active Learning for Relation Classification via Pseudo Paral-lel CorporaLonghua Qian    Haotian Hui   Ya?nan Hu   Guodong Zhou*   Qiaoming ZhuNatural Language Processing LabSchool of Computer Science and Technology, Soochow University1 Shizi Street, Suzhou, China 215006{qianlonghua,20134227019,20114227025,gdzhou,qmzhu}@suda.edu.cnAbstractActive learning (AL) has been proven ef-fective to reduce human annotation ef-forts in NLP.
However, previous studieson AL are limited to applications in asingle language.
This paper proposes abilingual active learning paradigm for re-lation classification, where the unlabeledinstances are first jointly chosen in termsof their prediction uncertainty scores intwo languages and then manually labeledby an oracle.
Instead of using a parallelcorpus, labeled and unlabeled instancesin one language are translated into onesin the other language and all instances inboth languages are then fed into a bilin-gual active learning engine as pseudoparallel corpora.
Experimental results onthe ACE RDC 2005 Chinese and Englishcorpora show that bilingual active learn-ing for relation classification signifi-cantly outperforms monolingual activelearning.1 IntroductionSemantic relation extraction between named en-tities (aka.
entity relation extraction or more con-cisely relation extraction) is an important subtaskof Information Extraction (IE) as well as NaturalLanguage Processing (NLP).
With its aim toidentify and classify the semantic relationshipbetween two entities (ACE 2002-2007), relationextraction is of great significance to many NLPapplications, such as question answering, infor-mation fusion, social network construction, andknowledge mining and population etc.
* Corresponding authorIn the literature, the mainstream research onrelation extraction adopts statistical machinelearning methods, which can be grouped intosupervised learning (Zelenko et al, 2003; Culottaand Soresen, 2004; Zhou et al, 2005; Zhang etal., 2006; Qian et al, 2008; Chan and Roth,2011), semi-supervised learning (Zhang et al,2004; Chen et al, 2006; Zhou et al, 2008; Qianet al, 2010) and unsupervised learning (Hase-gawa et al, 2004; Zhang et al, 2005) in terms ofthe amount of labeled training data they need.Usually the extraction performance dependsheavily on the quality and quantity of the labeleddata, however, the manual annotation of a large-scale corpus is labor-intensive and time-consuming.
In the last decade researchers haveturned to another effective learning paradigm--active learning (AL), which, given a small num-ber of labeled instances and a large number ofunlabeled instances, selects the most informativeunlabeled instances to be manually annotated andadd them into the training data in an iterativefashion.
Essentially active learning attempts todecrease the quantity of labeled instances by en-hancing their quality, gauged by their informa-tiveness to the learner.
Since its emergence, ac-tive learning has been successfully applied tomany tasks in NLP (Engelson and Dagan, 1996;Hwa, 2004; Tomanek et al, 2007; Settles andCraven, 2008).It is trivial to validate, as we will do later inthis paper, that active learning can also alleviatethe annotation burden for relation extraction inone language while retaining the extraction per-formance.
However, there are cases when wemay exploit relation extraction in multiple lan-guages and there are corpora with relation in-stances annotated for more than one language,such as the ACE RDC 2005 English and Chinesecorpora.
Hu et al (2013) shows that supervisedrelation extraction in one language (e.g.
Chinese)582can be enhanced by relation instances translatedfrom another language (e.g.
English).
This dem-onstrates that there is some complementarinessbetween relation instances in two languages, par-ticularly when the training data is scarce.
Onenatural question is: Can this characteristic bemade full use of so that active learning canmaximally benefit relation extraction in two lan-guages?
To the best of our knowledge, so far theissue of joint active learning in two languageshas yet been addressed.
Moreover, the success ofjoint bilingual learning may lend itself to manyinherent multilingual NLP tasks such as POStagging (Yarowsky and Ngai, 2001), name entityrecognition (Yarowsky et al, 2001), sentimentanalysis (Wan, 2009), and semantic role labeling(Sebastian and Lapata, 2009) etc.This paper proposes a bilingual active learn-ing (BAL) paradigm to relation classificationwith a small number of labeled relation instancesand a large number of unlabeled instances in twolanguages (non-parallel).
Instead of using a par-allel corpus which should have entity/relationalignment information and is thus difficult toobtain, this paper employs an off-the-shelf ma-chine translator to translate both labeled andunlabeled instances from one language into theother language, forming pseudo parallel corpora.These translated instances along with the originalinstances are then fed into a bilingual activelearning engine.
Findings obtained from experi-ments with relation classification on the ACE2005 corpora show that this kind of pseudo-parallel corpora can significantly improve theclassification performance for both languages ina BAL framework.The rest of the paper is organized as follows.Section 2 reviews the previous work on relationextraction while Section 3 describes our baselinesystems.
Section 4 elaborates on the bilingualactive learning paradigm and Section 5 discussesthe experimental results.
Finally conclusions anddirections for future work are presented in Sec-tion 6.2 Related WorkWhile there are many studies in monolingualrelation extraction, there are only a few on multi-lingual relation extraction in the literature.Monolingual relation extraction: A widerange of studies on relation extraction focus onmonolingual resources.
As far as representationof relation instances is concerned, there are fea-ture-based methods (Zhao et al, 2004; Zhou etal., 2005; Chan and Roth, 2011) and kernel-based methods (Zelenko et al, 2003; Zhang et al,2006; Qian et al, 2008), mainly for the Englishlanguage.
Both methods are also widely used inrelation extraction in other languages, such asthose in Chinese relation extraction (Che et al,2005; Li et al, 2008; Yu et al, 2010).Multilingual relation extraction: There areonly two studies related to multilingual relationextraction.
Kim et al (2010) propose a cross-lingual annotation projection approach whichuses parallel corpora to acquire a relation detec-tor on the target language.
However, the map-ping of two entities involved in a relation in-stance may leads to errors.
Therefore, Kim andLee (2012) further employ a graph-based semi-supervised learning method, namely LabelPropagation (LP), to indirectly propagate labelsfrom the source language to the target languagein an iterative fashion.
Both studies transfer rela-tion annotations via parallel corpora from theresource-rich language (English) to the resource-poor language (Korean), but not vice versa.Based on a small number of labeled instancesand a large number of unlabeled instances inboth languages, our method differs from theirs inthat we adopt a bilingual active learning para-digm via machine translation and improve theperformance for both languages simultaneously.Active Learning in NLP: Active learninghas become an active research topic due to itspotential to significantly reduce the amount oflabeled training data while achieving comparableperformance with supervised learning.
It hasbeen successfully applied to many NLP applica-tions, such as POS tagging (Engelson and Dagan,1996; Ringger et al, 2007), word sense disam-biguation (Chan and Ng, 2007; Zhu and Hovy,2007), sentiment detection (Brew et al, 2010; Liet al, 2012), syntactical parsing (Hwa, 2004;Osborne and Baldridge, 2004), and named entityrecognition (Shen et al, 2004; Tomanek et al,2007; Tomanek and Hahn, 2009) etc.Different from these AL studies on a singletask, Reichart et al (2008) introduce a multi-taskactive learning (MTAL) paradigm, where unla-beled instances are selected for two annotationtasks (i.e.
named entity and syntactic parse tree).They demonstrate that MTAL in the same lan-guage outperforms one-sided and random selec-tion AL.
From a different perspective, we pro-pose an active learning framework for the sametask, but across two different languages.Another related study (Haffari and Sarkar,2009) deals with active learning for multilingual583machine translation, which make use of multilin-gual corpora to decrease human annotation ef-forts by selecting highly informative sentencesfor a newly added language in multilingual paral-lel corpora.
While machine translation inherentlydeals with multilingual parallel corpora, our taskfocuses on relation extraction by pseudo parallelcorpora in two languages.3 Baseline SystemsThis section first introduces the fundamental su-pervised learning method, and then describes abaseline active learning algorithm.3.1 Supervised LearningWe adopt the feature-based method for funda-mental supervised relation classification, ratherthan the tree kernel-based method, since activelearning needs a large number of iterations andthe kernel-based method usually performs muchslower than the feature-based one.
Following is alist of our used features, much similar to Zhou etal.
(2005):a) Lexical features of entities and their contextsWM1: bag-of-words in the 1st entity mentionHM1: headword of M1WM2: bag-of-words in the 2nd entity mentionHM2: headword of M2HM12: combination of HM1 and HM2WBNULL: when no word in betweenWBFL: the only one word in betweenWBF: the first word in between when at leasttwo words in betweenWBL: the last word in between when at leasttwo words in betweenWBO: other words in between except the firstand last words when at least three words inbetweenb) Entity typeET12: combination of entity typesEST12: combination of entity subtypesEC12: combination of entity classesc) Mention levelML12: combination of entity mention levelsMT12: combination of LDC mention typesd) Overlap#WB: number of other mentions in between#MB: number of words in betweenM1>M2 or M1<M2: flag indicating whetherM2/M1 is included in M1/M2.3.2 Active Learning AlgorithmWe use a pool-based active learning procedurewith uncertainty sampling (Scheffer et al, 2001;Culotta and McCallum, 2005; Kim et al, 2006)for both Chinese and English relation classifica-tion as illustrated in Fig.
1.
During iterations abatch of unlabeled instances are chosen in termsof their informativeness to the current classifier,labeled by an oracle and in turn added into thelabeled data to retrain the classifier.
Due to ourfocus on the effectiveness of bilingual activelearning on relation classification, we only useuncertainty sampling without incorporating morecomplex measures, such as diversity and repre-sentativeness (Settles and Craven, 2008), andleave them for future work.Input:- L, labeled data set- U, unlabeled data set- n, batch sizeOutput:- SVM, classifierRepeat:1.
Train a single classifier SVM on L2.
Run the classifier on U3.
Find at most n instances in U that the classifierhas the highest prediction uncertainty4.
Have these instances labeled by an oracle5.
Add them into LUntil: certain number of instances are labeled orcertain performance is reachedAlgorithm uncertainty-based active learningFigure 1.
Pool-based active learning with uncer-tainty samplingSince the SVMLIB package used in this papercan output probabilities assigned to the class la-bels on an instance, we have three uncertaintymetrics readily available, i.e., least confidence(LC), margin (M) and entropy (E).
The NERexperimental results on multiple corpora (Settlesand Craven, 2008) show that there is no singleclear winner among these three metrics.
Thisconclusion is also validated by our preliminaryexperiments on the task of active learning rela-tion extraction, thus we adopt the LC metric forsimplicity.
Specifically, with a sequence of Kprobabilities for a relation instance at some itera-tion, denoted as {p1,p2,?pK} in the descendingorder, the LC metric of the relation instance canbe simply picked as the first one, i.e.1pHLC =     (1)Where K denotes the total number of relationclasses.
Note that this metric actually reflectsprediction reliability (i.e.
reverse uncertainty)rather than uncertainty in order to facilitate joint584confidence calculation for two languages (cf.?4.4).
Intuitively, the smaller the HLC is, the lessconfident the prediction is.4 Bilingual Active Learning for Rela-tion ClassificationIn this section, we elaborate on the bilingual ac-tive learning for relation extraction.4.1 Problem DefinitionWith Chinese and English (designated as c and e)as two languages used in our study, this paperintends to address the task of bilingual relationclassification, i.e., assigning relation labels tocandidate instances that have semantic relation-ships.
Suppose we have a small number of la-beled instances in both languages, denoted as Lcand Le (non-parallel) respectively, and a largenumber of unlabeled instances in both languages,denoted as Uc and Ue (non-parallel).
The test in-stances in both languages are represented as Tcand Te.
In order to take full advantage of bilin-gual resources, we translate both labeled andunlabeled instances in one language to ones inthe other language as follows:Lc ?
LetUc ?
UetLe ?
LctUe ?
LctThe objective is to learn SVM classifiers inboth languages, denoted as SVMc and SVMe re-spectively, in a BAL fashion to improve theirclassification performance.4.2 Bilingual Active Learning FrameworkCurrently, AL is widely used in NLP tasks in asingle language, i.e., during iterations unlabeledinstances least confident only in one languageare picked and manually labeled to augment thetraining data.
The only exception is AL for ma-chine translation (Haffari et al, 2009; Haffariand Sarkar, 2009), whose purpose is to select themost informative sentences in the source lan-guage to be manually translated into the targetlanguage.
Previous studies (Reichart et al, 2008;Haffari and Sarkar, 2009) show that multi-taskactive learning (MTAL) can yield promisingoverall results, no matter whether they are twodifferent tasks or the task of machine translationon multiple language pairs.
If a specific NLPtask on two languages, such as relation classifi-cation, can be regarded as two tasks, it is reason-able to argue that these two tasks can benefiteach other when jointly performed in the BALframework.
Yet, to our knowledge, this issueremains unexplored.An important issue for bilingual learning ishow to obtain two language views for relationinstances from multilingual resources.
There arethree solutions to this problem, i.e.
parallel cor-pora (Lu et al, 2011), translated corpora (aka.pseudo parallel corpora) (Wan 2009), and bilin-gual lexicons (Oh et al, 2009).
We adopt the onewith pseudo parallel corpora, using the machinetranslation method to generate instances fromone language to the other in the BAL paradigm,as depicted in Fig.
2.English ViewLabeledChinese Instances(Lc)Labeled TranslatedEnglish Instances(Let)LabeledEnglish Instances (Le)Labeled TranslatedChinese Instances(Lct)MachineTranslationMachineTranslationUnlabeledChinese Instances(Uc)UnlabeledTranslated ChineseInstances (Uct)Unlabeled TranslatedEnglish Instances (Uet)UnlabeledEnglish Instances(Ue)MachineTranslationMachineTranslationChinese ViewBilingualactive learningTestChinese Instances(Tc)TestEnglish Instances(Te)Figure 2.
Framework of bilingual active learningIn order to make full use of pseudo parallelcorpora, translated labeled and unlabeled in-stances are augmented in the following two ways:z For labeled Chinese instances (Lc) and Eng-lish instances (Le), their translated counter-parts (Let and Lct), along with their labels, aredirectly added into the labeled instances in theother language;z For unlabeled Chinese instances (Uc) andEnglish instances (Ue), during an active learn-ing iteration the top n unlabeled instances inUc and Uet which are least confidently jointly585predicted by SVMc and SVMe are labeled byan oracle and added to Lc and Le respectively.(cf.
?4.4)4.3 Instance Projection via MTAmong the several off-the-shelf machine transla-tion services, we select the Google Translator1because of its high quality and easy accessibility.Both the mentions of relation instances and thementions of two involved entities are first trans-lated into the other language via machine transla-tion.
Then, two entities in the original instanceare aligned with their counterparts in the trans-lated instance in order to form an aligned bilin-gual relation instance pair.Instance translationAll the positive instances in the ACE 2005 Chi-nese and English corpora are translated to an-other language respectively, i.e.
Chinese to Eng-lish and vice versa.
The relation instance is rep-resented as the word sequence between two enti-ties.
This word sequence, rather than the wholesentence, is then translated to another languageby the Google Translator.
The reason is that, al-though this sequence loses partial contextual in-formation of the relation instance, its translationquality is supposed to be better.
Our preliminaryexperiments indicate that the addition of contex-tual information fail to benefit the task.
Aftertranslation, word segmentation is performed onChinese instances translated from English whiletokenization is needed for translated English in-stances.Entity alignmentThe objective of entity alignment is to build amapping from the entities in the original in-stances to the entities in the translated instances.Put in another way, entity alignment automati-cally marks the entity mentions in the translatedinstance, thereby the feature vector correspond-ing to the translated instance can be constructed.Entity alignment is vital in cross-language rela-tion extraction whose difficulty lies in the factthat the same entity mention as an isolated phraseand as an integral phrase in the relation instancecan be translated to different phrases.
For exam-ple, the Chinese entity mention ????
(officer)is translated to ?officer?
in isolation, it is, how-ever, translated to ?officials?
when in the relationinstance ????
???
(Syrian officials).1 http://translate.google.comInput:- Me, entity mention in English- Re, relation instance in English- Mct, translation of Me in Chinese- Rc, translation of Re in Chinese- L, a lexicon consisting of entries like (ei, ci, pi),where pi is the translation probability from ei to ci- ?, probability thresholdOutput:- Mc, the counterpart of Me in RcSteps:1.
If Mct can be exactly found in Rc, then returnMct2.
If the rightmost part of Mct can be found in Rc,then this part can be returned3.
For very word we in Me,a) If there exists a word wc in Rc and (we, wc, p)in L and p>?, then (we, wc) is a match of two wordsb) Return a successive sequence of matchingwords wc4.
Return nullAlgorithm entity alignmentFigure 3.
Entity alignment algorithmTherefore, we devise some heuristics to alignentity mentions between Chinese and English.The basic idea is that the word sequence in onemention successively matches the word sequencein the other mention.
Take entity alignment fromEnglish to Chinese as an example, given entitymention Me in relation instance Re in English andtheir respective translations Mct and Rc in Chi-nese, the objective of entity alignment is to findMc, the counterpart of Me in Rc.
The procedure ofentity alignment algorithm can be described inFig.
3.In the algorithm, the probability threshold ?
isempirically set to 0.002 where the precision andrecall of entity alignment are balanced.
Our lexi-con is derived from the FBIS parallel corpus(#LDC2003E14), which is widely used in ma-chine translation between English and Chinese.
Itshould be noted that the process of relation trans-lation and entity alignment are far from perfec-tion, leading to reduction in the number of in-stances being mapping to the other language, i.e.|Lc| > |Let||Uc| > |Uet||Le| > |Lct||Ue| > |Lct|4.4 Bilingual Active Learning AlgorithmThe basic idea of our BAL paradigm is that,while unlabeled instances uncertain in one lan-586guage are informative to the learner in that lan-guage, unlabeled instances jointly uncertain inboth languages are informative to the learners inboth languages, thus potentially improving clas-sification performance for both languages morethan their individual active learners do.
Thisidea is embodied in the BAL algorithm in Fig.
4,where n is the batch size, i.e., the number of in-stances selected, labeled and augmented at eachiteration.Figure 4.
Bilingual active learning algorithmThe key point of this algorithm lies in Step 5and Step 6, where unlabeled instances from Ucand Ue are selected and labeled respectively.Take Chinese for an example, when gauging theprediction uncertainty for an unlabeled instancein Uc, not only its own uncertainty measure Hcpredicted by SVMc is considered, but also theuncertainty measure Het for its translation coun-terpart in Uet, which is predicted by SVMe, is con-sidered.
Generally, in order to jointly considerthese two measures, there are three methods tocompute their means, namely, arithmetic mean,geometric mean and harmonic mean.
Preliminaryexperiments show that among these three means,there is no single winner, so we simply take thegeometric mean defined as follows:etcg HHH *=    (2)Considering that we adopt the LC measure asthe uncertainty score, when an instance in Uccan?t find its translation counterpart in Uet due totranslation error or entity alignment failure, Het isset to 1, i.e.
the maximum.
Since the bigger H is,the more confident the prediction is, the lesslikely the instance will be chosen, in this way wediscourage the unlabeled instances without trans-lation counterparts.5 ExperimentationWe have systematically evaluated our BAL para-digm on the relation classification task usingACE RDC 2005 RDC Chinese and English cor-pora.5.1 Experimental SettingsCorpora and PreprocessingWe use the ACE 2005 RDC Chinese and Englishcorpora as the benchmark data (hereafter we re-fer to them as the Chinese corpus (ACE2005c)and the English corpus (ACE2005e) respec-tively).
Both corpora have the same en-tity/relation hierarchies, which define 7 entitytypes, 6 major relation types.
However, the Chi-nese corpus contains 633 documents and 9,147positive relation instances while the English cor-pus only contains 498 files and 6,253 positiveinstances.
Therefore, in order to balance the cor-pus scale to fairly evaluate bilingual active learn-ing impact on relation classification, we ran-domly select 458 Chinese files and thus get6,268 positive instances, comparable to the Eng-lish corpus.Preprocessing steps for both corpora includesentence splitting and tokenization (word seg-mentation for Chinese using ICTCLAS2).
Then,positive relation mentions with word sequencesbetween two entities and their feature vectors areextracted from sentences while negative relationmentions are simply discarded because we focuson the task of relation classification.
After entityand relation mentions in one language are trans-2 http://ictclas.org/587lated into the other language using the Googletranslator, entity alignment is performed betweenrelation mentions and their translations.
Finally4,747 Chinese relation mentions are successfullytranslated and aligned from English and viceversa, 4,936 English relation mentions are trans-lated and aligned from Chinese.SVMLIB (Chang and Lin, 2011) is selected asour classifier since it supports multi-class classi-fication.
The training parameters C (SVM) is setto 2.4 according to our previous work on relationextraction (Qian et al, 2010).
Relation classifica-tion performance is evaluated using the standardPrecision (P), Recall (R) and their harmonic av-erage (F1) as well as deficiency measure (cf.
lat-ter in this section.).
Overall performance scoresare averaged over 10 runs.
For each run, 1/40and 1/5 randomly selected instances are used asthe training and test set respectively while theremaining instances are used as the unlabeled setfor further labeling during active learning itera-tions.Methods for ComparisonFor fair comparison, two baseline methods ofsupervised learning are included to augment theirtraining sets with labeled instances during itera-tions.
However, these labeled instances are cho-sen randomly from the corpus.SL-MO (Supervised Learning with monolin-gual labeled instances): only the monolinguallabeled instances are fed to the SVM classifiersfor both Chinese and English relation classifica-tion respectively.
The initial training data onlycontain Lc and Le for Chinese and English respec-tively.SL-CR (Supervised Learning with cross-lingual labeled instances): in addition to mono-lingual labeled instances (SL-MO), the trainingdata for supervised learning contain labeled in-stances translated from the other language.
Thatis, the initial training data contain Lc and Lct forChinese, or Le and Let for English.
More impor-tant, at each iteration not only the labeled in-stances are added to the training data of its ownlanguage, but their translated instances are alsoadded to the training data of the other language.AL-MO (Active Learning with monolingualinstances): labeled and unlabeled data for activelearning only contain monolingual instances.
Notranslated instances are involved.
That is, thedata contain Lc and Uc for Chinese, or Le and Uefor English respectively.
This is the normal ac-tive learning method applied to a single language.AL-CR (Active Learning with cross-lingualinstances): both the manually labeled instancesand their translated ones are added to the respec-tive training data.
The initial training data con-tain Lc and Lct for Chinese, or Le and Let for Eng-lish.
At each iteration, the n least confidentlyclassified instances in Uc and Ue are labeled andadded to the Chinese/English training data re-spectively.
Their translated instances in Uet andUct are also added to the English/Chinese trainingdata respectively.AL-BI (Active Learning with bilingual la-beled and unlabeled instances): similar to AL-CR with the exception that the unlabeled in-stances are chosen not by uncertainty scores inone language, but by the joint uncertainty scoresin two languages.
(cf.
?4.4)Evaluation MetricAlthough learning curves are often used to evalu-ate the performance for active learning, it is pref-erable to quantitatively compare various activelearning methods using a statistical metric defi-ciency (Schein and Ungar, 2007) defined as:??==?
?= ni inni innREFFREFFALFREFFREFALdef11))()(())()((),(     (3)Where n is the number of iterations involved inactive learning and Fi is the F1-score of relationclassification at the ith iteration.
REF is the base-line active learning method and AL is an im-proved variant of REF, such as AL-CR or AL-BI.
Essentially this deficiency metric measuresthe degree to which REF outperforms AL.
Thus,smaller deficiency value (i.e.
<1.0) indicates ALoutperforms REF while a larger value (i.e.
>1.0)indicates AL underperforms REF.5.2 Experimental Results and AnalysisComparison of overall deficiencyTable 1 compares the deficiency scores of rela-tion classification on the Chinese (ACE2005c)and English corpora (ACE2005e) for variouslearning methods, i.e., SL-CR, AL-MO, AL-CRand AL-BI.
Particularly, SL-MO is used as thebaseline system against which deficiency scoresfor other methods are computed.
The batch size nis set to 100 and iterations stop after all the unla-beled instances have run out of.
Deficiencyscores are averaged over 10 runs and the bestones are highlighted in bold font.
Each run has adifferent test set and a different seed set.588(a) Chinese      (b) EnglishFigure 5.
Deficiency comparison for different batch sizes(a) Chinese      (b) EnglishFigure 6.
Learning curves for different methodsThe table shows that among the three activelearning methods, bilingual active learning (AL-BI) achieves the best performance for both Chi-nese and English relation classification.
Thisdemonstrates that, bilingual active learning withjointly selecting the unlabeled instances can notonly enhance relation classification for its ownlanguage, but also help relation classification forthe other language due to the complementarynature of relation instances between Chinese andEnglish.Corpora SL-CR AL-MO AL-CR AL-BIACE2005c 0.934 0.383 0.323 0.254ACE2005e 0.779 0.405 0.298 0.160Table 1.
Deficiency comparison of differentmethodsThe table also shows the consistent utility ofcross-lingual information for relation classifica-tion for both languages.
When cross-lingual in-formation is augmented, SL-CR outperformsSL-MO and AL-CR outperforms AL-MO.Comparison of different batch sizesFigure 5(a) and 5(b) illustrate the deficiencyscores for four learning methods (SL-CR, AL-MO, AL-CR and AL-BI) against the SL-MOmethod with different batch sizes (n), where pre-fixes ?C?
and ?E?
denote Chinese and Englishrespectively.
The horizontal axes denote therange of n (<=1000) while the vertical ones de-note the deficiency scores.The figures show that the deficiency scoresfor three active learning methods run virtuallyparallel with each other while they increase mo-notonously with the batch size n. This suggeststhat for both Chinese and English AL-BI consis-tently performs best against other methods acrossa wide range of batch sizes, though the overalladvantage of three active learning methods gen-erally diminish.Comparison of learning curvesIn order to gain an intuition into how the per-formance evolves when the labeled instances areadded into the training data during iterations, wedepict the learning curves for various learningmethods on the Chinese and English corpora inFig.
6(a) and 6(b) respectively.
The horizontalaxes denote learning iterations while the verticalones denote F1-scores.
For simplicity of illustra-tion the F1-scores are collected from one of the10 runs.75777981838587899193950 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48C-SL-MOC-SL-CRC-AL-MOC-AL-CRC-AL-BI757779818385878991930 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48E-SL-MOE-SL-CRE-AL-MOE-AL-CRE-AL-BI0.10.20.30.40.50.60.70.80.91.0100 200 300 400 500 600 700 800 900 1000C-SL-CRC-AL-MOC-AL-CRC-AL-BI0.10.20.30.40.50.60.70.80.9100 200 300 400 500 600 700 800 900 1000E-SL-CRE-AL-MOE-AL-CRE-AL-BI589The figures clearly demonstrate the perform-ance difference for both languages among fivemethods at the beginning of iterations while F1-scores converge at the end of iterations.
Particu-larly at the very outset, AL-BI outperforms othermethods, quickly jumps to a very high pointcomparable to its best performance.
However,after the 10th iteration the performance scores forthe three AL variants tend to show trivial differ-ence probably because most highly informativeinstances have already been added to the trainingdata.Comparison of annotation scaleIn order to better compare BAL with other ALmethods Figure 7 zooms out partial data on threeAL methods in Fig.
6 and rescale the data forAL-MO, where ?C?
and ?E?
denote Chinese andEnglish respectively.
Likewise, the vertical axisdenotes F1-scores while the horizontal axis de-notes the number of instances labeled for AL-CR and AL-BI.
However, for AL-MO that num-ber is doubled.
This figure tries to answer thequestion: to label n respective instances in bothlanguages for BAL or to labeled 2n instances injust one language for monolingual AL, can theformer rival the latter?8082848688909294100 200 300 400 500 600 700 800 900 1000C-AL-MO (2n)C-AL-CRC-AL-BIE-AL-MO (2n)E-AL-CRE-AL-BIFigure 7.
Comparison of annotation scale amongthree AL methodsThe figure shows that for both Chinese andEnglish, when the number of instances (n) to belabeled is no greater than 400, AL-BI with n in-stances can achieve comparable performancewith AL-MO with 2n instances.
It implies thatwhen the labeled instances are limited, labelinginstances, half in one language and half in theother for BAL, is competitive against labelingthe same total number of instances in just onelanguage for monolingual AL, not to mentionthat the former can generate two relation extrac-tors on two languages.6 ConclusionThis paper proposes a bilingual active learningparadigm for Chinese and English relation classi-fication.
Given a small number of relation in-stances and a large number of unlabeled relationinstances in both languages, we translate both thelabeled and unlabeled instances in one languageto the other as pseudo parallel corpora.
After en-tity alignment, these labeled and unlabeled in-stances in both languages are fed into a bilingualactive learning engine.
Experiments with the taskof relation classification on the ACE RDC 2005Chinese and English corpora show that bilingualactive learning can significantly outperformsmonolingual active learning for both Chinese andEnglish simultaneously.
Moreover, we demon-strate that BAL across two languages can com-pete against monolingual AL when the annota-tion scale is limited, though the overall numberof labeled instances remains the same.For future work, on one hand, we plan tocombine uncertainty sampling with diversity andinformativeness measures; on the other hand, weintend to combine BAL with semi-supervisedlearning to further reduce human annotation ef-forts.AcknowledgmentsThis research is supported by Grants 61373096,61305088, 61273320, and 61331011 under theNational Natural Science Foundation of China;Project 2012AA011102 under the ?863?
Na-tional High-Tech Research and Development ofChina; Grant 11KJA520003 under the EducationBureau of Jiangsu, China.
We would like tothank the excellent and insightful commentsfrom the three anonymous reviewers.
Thanksalso go to my colleague Dr. Shoushan Li for hishelpful suggestions.ReferenceACE.
2002-2007.
Automatic Content Extraction.http://www.ldc.upenn.edu/Projects/ACE/A.
Brew, D. Greene, and P. Cunningham.
2010.
Usingcrowdsourcing and active learning to track senti-ment in online media.
ECAI?2010: 145?150.Y.S.
Chan and D. Roth.
2011.
Exploiting Syntactico-Semantic Structures for Relation Extraction.ACL?2011: 551-560Y.S.
Chan and H.T.
Ng.
2007.
Domain adaptationwith active learning for word sense disambiguation.ACL?2007.590C.C.
Chang and C.J.
Lin.
2011.
LIBSVM: a libraryfor support vector machines.
ACM Transactions onIntelligent Systems and Technology, 2(27):1-27.W.X.
Che, T. Liu, and S. Li.
2005.
Automatic Extrac-tion of Entity Relation (in Chinese).
Journal ofChinese Information Processing, 19(2): 1-6.J.X.
Chen, D.H. Ji, and C. L. Tan.
2006.
Relation Ex-traction using Label Propagation-based Semi-supervised Learning.
ACL/COLING?2006: 129-136.A.
Culotta and J. Sorensen.
2004.
Dependency treekernels for relation extraction.
ACL?2004: 423-439.A.
Culotta and A. McCallum.
2005.
Reducing label-ing effort for stuctured prediction tasks.
AAAI?2005:746?751.S.
P. Engelson and I. Dagan.
1996.
Minimizing man-ual annotation cost in supervised training from cor-pora.
ACL?1996: 319?326.G.
Haffari, M. Roy, and A. Sarkar.
2009.
Activelearning for statistical phrase-based machine trans-lation.
NAACL?2009: 415?423.G.
Haffari and A. Sarkar.
2009.
Active learning formultilingual statistical machine translation.ACL/IJCNLP?2009: 181?189.T.
Hasegawa, S. Sekine, and R. Grishman.
2004.
Dis-covering Relations among Named Entities fromLarge Corpora.
ACL?2004.Y.N.
Hu, J.G.
Shu, L.H.
Qian, and Q.M.
Qiao.
2013.Cross-lingual Relation Extraction based on Ma-chine Translation (in Chinese).
Journal of ChineseInformation Processing, 27(5): 191-197.R.
Hwa.
2004.
Sample selection for statistical parsing.Computational Linguistics, 30(3): 253?276.S.
Kim, M. Jeong, J. Lee, and G.G.
Lee.
2010.
ACross-lingual Annotation Projection Approach forRelation Detection.
COLING?2010: 564-571.S.
Kim and G.G.
Lee.
2012.
A Graph-based Cross-lingual Projection Approach for Weakly Super-vised Relation Extraction.
ACL?2012: 48-53.S.
Kim, Y.
Song, K. Kim, J.W.
Cha, and G.G.
Lee.2006.
MMR-based active machine learning for bionamed entity recognition.
HLT-NAACL?2006: 69?72.W.J.
Li, P. Zhang, F.R.
Wei, Y.X.
Hou, and Q. Lu.2008.
A Novel Feature-based Approach to ChineseEntity Relation Extraction.
ACL?2008: 89-92.S.S.
Li, S.F.
Ju, G.D. Zhou, and X.J.
Li.
2012.
Activelearning for imbalanced sentiment classifica-tion.
EMNLP-CoNLL?2012: 139-148.B.
Lu, C.H.
Tan, C. Cardie, and B.K.
Tsou.
2011.Joint Bilingual Sentiment Classification withUnlabeled Parallel Corpora.
ACL?2011: 320-330.J.
Oh, K. Uchimoto, and K. Torisawa.
2009.
Bilin-gual Co-Training for Monolingual Hyponymy-Relation Acquisition.
ACL?2009: 432-440.M.
Osborne and J. Baldridge.
2004.
Ensemble basedactive learning for parse selection.
HLT-NAACL?2004: 89?96.L.H.
Qian, G.D. Zhou, F. Kong, and Q.M.
Zhu.
2010.Clustering-based Stratified Seed Sampling forSemi-Supervised Relation Classification.EMNLP2010: 346-355.L.H.
Qian, G.D. Zhou, Q.M.
Zhu, and P.D.
Qian.2008.
Exploiting constituent dependencies for treekernel-based semantic relation extraction.
COL-ING?2008: 697-704.R.
Reichart, K. Tomanek, U. Hahn, and A. Rappoport.2008.
Multi-task active learning for linguistic an-notations.
ACL?2008: 861-869.E.
Ringger, P. McClanahan, R. Haertel, G. Busby, M.Carmen, J. Carroll, K. Seppi, and D. Lonsdale.2007.
Active learning for part-of-speech tagging:Accelerating corpus annotation.
In Proceedings ofthe Linguistic Annotation Workshop at ACL?2007:101?108.T.
Scheffer, C. Decomain, and S. Wrobel.
2001.
Ac-tive hidden Markov models for information extrac-tion.
In Proceedings of the International Confer-ence on Advances in Intelligent Data Analysis(CAIDA), pages 309?318.A.
I. Schein and L. H. Ungar.
2007.
Active learningfor logistic regression: an evaluation.
MachineLearning, 68(3): 235-265.P.
Sebastian and M. Lapata.
2009.
Cross-lingual an-notation projection of semantic roles.
Journal ofArtificial Intelligence Research, 36(1): 307-340.B.
Settles and M. Craven.
2008.
An Analysis of Ac-tive Learning Strategies for Sequence LabelingTasks.
EMNLP?2008: 1070?1079.D.
Shen, J. Zhang, J. Su, G.D. Zhou and C.-L. Tan.2004.
Multi-criteria-based active learning fornamed entity recognition.
ACL?2004.K.
Tomanek and U. Hahn.
2009.
Semi-SupervisedActive Learning for Sequence Labeling.
ACL-IJCNLP?2009: 1039-1047.K.
Tomanek, J. Wermter, and U. Hahn.
2007.
An ap-proach to text corpus construction which cuts an-notation costs and maintains reusability of anno-tated data.
EMNLP-CoNLL?2007: 486?495.X.J.
Wan.
2009.
Co-Training for Cross-Lingual Sen-timent Classification.
ACL-AFNLP?2009: 235-243.D.
Yarowsky and G. Ngai.
2001.
Inducing multilin-gual POS taggers and NP bracketers via robust pro-jection across aligned corpora.
NAACL?2001: 1-8.591D.
Yarowsky, G. Ngai, and R. Wicentorski.
2001.Inducing multilingual text analysis tools via robustprojection across aligned corpora.
HLT?2001:1-8.H.H.
Yu, L.H.
Qian, G.D. Zhou, and Q.M.
Zhu.
2010.Chinese Semantic Relation Extraction based onUnified Syntactic and Entity Semantic Tree (inChinese).
Journal of Chinese Information Process-ing, 24(5): 17-23.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Ker-nel Methods for Relation Extraction.
Journal ofMachine Learning Research, 3: 1083-1106.Z.
Zhang.
2004.
Weakly-supervised relation classifi-cation for Information Extraction.
CIKM?2004.M.
Zhang, J. Su, D. M. Wang, G. D. Zhou, and C. L.Tan.
2005.
Discovering Relations between NamedEntities from a Large Raw Corpus Using TreeSimilarity-Based Clustering.
IJCNLP?2005: 378-389.M.
Zhang, J. Zhang, J. Su, and G.D. Zhou.
2006.
AComposite Kernel to Extract Relations betweenEntities with both Flat and Structured Features.ACL/COLING?2006: 825-832.S.B.
Zhao and R. Grishman.
2005.
Extracting rela-tions with integrated information using kernelmethods.
ACL?2005: 419-426.G.D.
Zhou, J.H.
Li, L.H.
Qian, and Q.M.
Zhu.
2008.Semi-Supervised Learning for Relation Extraction.IJCNLP?2008: 32-38.G.D.
Zhou, J. Su, J. Zhang, and M. Zhang.
2005.
Ex-ploring various knowledge in relation extraction.ACL?2005: 427-434.J.B.
Zhu and E. Hovy.
2007.
Active learning for wordsense disambiguation with methods for addressingthe class imbalance problem.
EMNLP-CoNLL?2007: 783-790.592
