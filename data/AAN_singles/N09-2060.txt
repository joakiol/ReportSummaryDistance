Proceedings of NAACL HLT 2009: Short Papers, pages 237?240,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSemantic classification with WordNet kernelsDiarmuid O?
Se?aghdhaComputer LaboratoryUniversity of CambridgeUnited Kingdomdo242@cl.cam.ac.ukAbstractThis paper presents methods for performinggraph-based semantic classification using ker-nel functions defined on the WordNet lexi-cal hierarchy.
These functions are evaluatedon the SemEval Task 4 relation classificationdataset and their performance is shown to becompetitive with that of more complex sys-tems.
A number of possible future develop-ments are suggested to illustrate the flexibilityof the approach.1 IntroductionThe estimation of semantic similarity betweenwords is one of the longest-established tasks in Nat-ural Language Processing and many approaches tothe problem have been proposed.
The two domi-nant lexical similarity paradigms are distributionalsimilarity, which compares words on the basis oftheir observed co-occurrence behaviour in corpora,and semantic network similarity, which compareswords based on their position in a graph such asthe WordNet hierarchy.
In this paper we considermeasures of network similarity for the purpose ofsupervised classification with kernel methods.
Theutility of kernel functions related to popular distribu-tional similarity measures has recently been demon-strated by O?
Se?aghdha and Copestake (2008); weshow here that kernel analogues of WordNet simi-larity can likewise give good performance on a se-mantic classification task.2 Kernels derived from graphsKernel-based classifiers such as support vector ma-chines (SVMs) make use of functions called kernelfunctions (or simply kernels) to compute the similar-ity between data points (Shawe-Taylor and Cristian-ini, 2004).
Valid kernels are restricted to the set ofpositive semi-definite (psd) functions, i.e., those thatcorrespond to an inner product in some vector space.Kernel methods have been widely adopted in NLPover the past decade, in part due to the good perfor-mance of SVMs on many tasks and in part due to theability to exploit prior knowledge about a given taskthrough the choice of an appropriate kernel function.In this section we consider kernel functions that usespectral properties of a graph to compute the sim-ilarity between its nodes.
The theoretical founda-tions and some machine learning applications of theadopted approach have been developed by Kondorand Lafferty (2002), Smola and Kondor (2003) andHerbster et al (2008).Let G be a graph with vertex set V = v1, .
.
.
, vnand edge set E ?
V ?
V .
We assume that G isconnected and undirected and that all edges have apositive weight wij > 0.
Let A be the symmetricn?nmatrix with entriesAij = wij if an edge existsbetween vertices vi and vj , and Aij = 0 otherwise.Let D be the diagonal matrix with entries Dii =?j?V Aij .
The graph Laplacian L is then definedasL = D?A (1)The normalised Laplacian is defined as L?
=D?
12LD?
12 .
Both L?
and L are positive semi-definite, but they are typically used as starting points237for the derivation of kernels rather than as kernelsthemselves.Let ?1 ?
?
?
?
?
?n be the eigenvalues of L andu1, .
.
.
, un the corresponding eigenvectors.
Notethat un = 0 for all graphs.
L is singular and hencehas no well-defined inverse, but its pseudoinverseL+ is defined asL+ =n?1?i=1?
?1i uiuTi (2)L+ is positive definite, and its entries are related tothe resistance distance between points in an elec-trical circuit (Herbster et al, 2008) and to the av-erage commute-time distance, i.e., the average dis-tance of a random walk from one node to anotherand back again (Fouss et al, 2007).
The similar-ity measure defined by L+ hence takes informationabout the connectivity of the graph into account aswell as information about adjacency.
An analogouspseudoinverse L?+ can be defined for the normalisedLaplacian.A second class of graph-based kernel functionsare the diffusion kernels introduced by Kondor andLafferty (2002).
The kernel Ht is defined as Ht =e?tL?, or equivalently:Ht =n?1?i=1exp(?t?
?i)u?iu?Ti (3)where t > 0, and ?
?1 ?
?
?
?
?
?
?n and u?1, .
.
.
, u?nare the eigenvalues and eigenvectors of L?+ respec-tively.
Ht can be interpreted in terms of heat diffu-sion or the distribution of a lazy random walk ema-nating from a given point at a time point t.3 Methodology3.1 Graph constructionWordNet (Fellbaum, 1998) is a semantic network inwhich nodes correspond to word senses (or synsets)and edges correspond to relations between senses.In this work we restrict ourselves to the noun com-ponent of WordNet and use only hyponymy and in-stance hyponymy relations for graph construction.The version of WordNet used is WordNet 3.0.To evaluate the utility of the graph-based kernelsdescribed in Section 2 for computing lexical sim-ilarity, we use the dataset developed for the taskon Classifying Semantic Relations Between Nom-inals at the 2007 SemEval competition (Girju etal., 2007).
The dataset comprises candidate exam-ple sentences for seven two-argument semantic rela-tions, with 140 training sentences and approximately80 test sentences for each relation.
It is a particularlysuitable task for evaluating WordNet kernels, as thecandidate relation arguments for each sentence aretagged with their WordNet sense and it has been pre-viously shown that a kernel model based on distribu-tional lexical similarity can attain very good perfor-mance (O?
Se?aghdha and Copestake, 2008).3.2 Calculating the WordNet kernelsThe noun hierarchy in WordNet 3.0 contains 82,115senses; computing kernel similarities on a graph ofthis size raises significant computational issues.
Thecalculation of the Laplacian pseudoinverse is com-plicated by the fact that while L and L?
are verysparse, their pseudoinverses are invariably dense andrequire very large amounts of memory.
To circum-vent this problem, we follow Fouss et al (2007)in computing L+ and L?+ one column at a timethrough a Cholesky factorisation procedure.
Onlythose columns required for the classification taskneed be calculated, and the kernel computation foreach relation subtask can be performed in a mat-ter of minutes.
Calculating the diffusion kernel in-volves an eigendecomposition of L?, meaning thatcomputing the kernel exactly is infeasible.
The so-lution used here is to approximate Ht by using them smallest components of the spectrum of L?
whencomputing (3); from (2) it can be seen that a similarapproximation can be made to speed up computationof L+ and L?+.3.3 Experimental setupFor all kernels and relation datasets, the kernel ma-trix for each argument position was precomputedand normalised so that every diagonal entry equalled1.
A small number of candidate arguments are notannotated with a WordNet sense or are assigned anon-noun sense; these arguments were assumed tohave self-similarity equal to 1 and zero similarity toall other arguments.
This does not affect the pos-itive semi-definiteness of the kernel matrices.
Theper-argument kernel matrices were summed to givethe kernel matrix for each relation subtask.
The ker-238Full graph m = 500 m = 1000Kernel Acc F Acc F Acc FB 72.1 68.4 - - - -L+ 73.3 69.4 73.2 70.5 73.6 70.6L?+ 72.5 70.0 72.7 70.0 74.1 71.0Ht - - 68.6 64.7 69.8 65.1Table 1: Results on SemEval Task 4nels described in Section 2 were compared to a base-line kernel B.
This baseline represents each word asa binary feature vector describing its synset and allits hypernym synsets in the WordNet hierarchy, andcalculates the linear kernel between vectors.All experiments were run using the LIBSVM sup-port vector machine library (Chang and Lin, 2001).For each relation the SVM cost parameter was op-timised in the range (2?6, 2?4, .
.
.
, 212) throughcross-validation on the training set.
The diffusionkernel parameter t was optimised in the same way,in the range (10?3, 10?2, .
.
.
, 103).4 ResultsMacro-averaged accuracy and F-score for each ker-nel are reported in Table 1.
There is little differencebetween the Laplacian and normalised Laplacianpseudoinverses; both achieve better performancethan the baselineB.
The results also suggest that thereduced-eigenspectrum approximations to L+ andL?+ may bring benefits in terms of performance aswell as efficiency via a smoothing effect.
The bestperformance is attained by the approximation to L?+with m = 1, 000 eigencomponents.
The heat ker-nelHt fares less well; the problem here may be thatthe optimal range for the t parameter has not beenidentified.Comparing these results to those of the partici-pants in the 2007 SemEval task, the WordNet-basedlexical similarity model fares very well.
All versionsof L+ and L?+ attain higher accuracy than all but oneof 15 systems in the competition and higher F-scorethan all but three.
Even the baseline B ranks aboveall but the top three systems, suggesting that this toocan be a useful model.
This is in spite of the fact thatall systems which made use of the sense annotationsalso used a rich variety of other information sourcessuch as features extracted from the sentence context,while the models presented here use only the graphstructure of WordNet.15 Related workThere is a large body of work on using WordNetto compute measures of lexical similarity (Budanit-sky and Hirst, 2006).
However, many of these mea-sures are not amenable for use as kernel functions asthey rely on properties which cannot be expressedas a vector inner product, such as the lowest com-mon subsumer of two vertices.
Hughes and Ram-age (2007) present a lexical similarity model basedon random walks on graphs derived from WordNet;Rao et al (2008) propose the Laplacian pseudoin-verse on such graphs as a lexical similarity measure.Both of these works share aspects of the current pa-per; however, neither address supervised learning orpresent an application-oriented evaluation.Extracting features from WordNet for use in su-pervised learning is a standard technique (Scott andMatwin, 1999).
Siolas and d?Alche-Buc (2000) andBasili et al (2006) use a measure of lexical similar-ity from WordNet as an intermediary to smooth bag-of-words kernels on documents.
Siolas and d?Alche-Buc use an inverse path-based similarity measure,while Basili et al use a measure of ?conceptual den-sity?
that is not proven to be positive semi-definite.6 Conclusion and future workThe main purpose of this paper has been to demon-strate how kernels that capture spectral aspects ofgraph structure can be used to compare nodes ina lexical hierarchy and thus provide a kernelisedmeasure of WordNet similarity.
As far as we areaware, these measures have not previously been in-vestigated in the context of semantic classification.The resulting WordNet kernels have been evaluatedon the SemEval Task 4 dataset and shown to attaina higher level of performance than many more com-plicated systems that participated in that task.Two obvious shortcomings of the kernels dis-cussed here are that they are defined on sensesrather than words and that they are computed on a1Of course, information about lexical similarity is not suf-ficient to classify all examples.
In particular, the models pre-sented here perform relatively badly on the ORIGIN-ENTITYand THEME-TOOL relations, while scoring better than allSemEval entrants on INSTRUMENT-AGENCY and PRODUCT-PRODUCER.239rather impoverished graph structure (the WordNethyponym hierarchy is quite tree-like).
One of thesignificant benefits of spectral graph kernels is thatthey can be computed on arbitrary graphs and aremost powerful when graphs have a rich connectiv-ity structure.
Some potential future directions thatwould make greater use of this flexibility include thefollowing:?
A simple extension from sense-kernels toword-kernels involves adding word nodes tothe WordNet graph, with an edge linking eachword to each of its possible senses.
This is sim-ilar to the graph construction method of Hughesand Ramage (2007) and Rao et al (2008).However, preliminary experiments on the Se-mEval Task 4 dataset indicate that further re-finement of this approach may be necessaryin order to match the performance of kernelsbased on distributional lexical similarity (O?Se?aghdha and Copestake, 2008).?
Incorporating other WordNet relations such asmeronymy and topicality gives a way of ker-nelising semantic association or relatedness;one application of this might be in develop-ing supervised methods for spelling correction(Budanitsky and Hirst, 2006).?
A WordNet graph can be augmented with in-formation from other sources, such as linksbased on corpus-derived similarity.
Alterna-tively, the graph-based kernel functions couldbe applied to graphs constructed from parsedcorpora (Minkov and Cohen, 2008).ReferencesRoberto Basili, Marco Cammisa, and Alessandro Mos-chitti.
2006.
A semantic kernel to classify texts withvery few training examples.
Informatica, 30(2):163?172.Alexander Budanitsky and Graeme Hirst.
2006.
Evalu-ating WordNet-based measures of lexical semantic re-latedness.
Computational Linguistics, 32(1):13?47.Chih-Chung Chang and Chih-Jen Lin.
2001.
LIB-SVM: A library for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge, MA.Francois Fouss, Alain Pirotte, Jean-Michel Renders, andMarco Saerens.
2007.
Random-walk computation ofsimilarities between nodes of a graph with applicationto collaborative recommendation.
IEEE Transactionson Knowledge and Data Engineering, 19(3):355?369.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.SemEval-2007 Task 04: Classification of semanticrelations between nominals.
In Proceedings of the4th International Workshop on Semantic Evaluations(SemEval-07).Mark Herbster, Massimiliano Pontil, and Sergio RojasGaleano.
2008.
Fast prediction on a tree.
In Pro-ceedings of the 22nd Annual Conference on Neural In-formation Processing Systems (NIPS-08).Thad Hughes and Daniel Ramage.
2007.
Lexical seman-tic relatedness with random graph walks.
In Proceed-ings of the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning (EMNLP-CoNLL-07).Risi Imre Kondor and John Lafferty.
2002.
Diffusionkernels on graphs and other discrete input spaces.
InProceedings of the 19th International Conference onMachine Learning (ICML-02).Einat Minkov and William W. Cohen.
2008.
Learninggraph walk based similarity measures for parsed text.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing (EMNLP-08).Diarmuid O?
Se?aghdha and Ann Copestake.
2008.
Se-mantic classification with distributional kernels.
InProceedings of the 22nd International Conference onComputational Linguistics (COLING-08).Delip Rao, David Yarowsky, and Chris Callison-Burch.2008.
Affinity measures based on the graph Lapla-cian.
In Proceedings of the 3rd TextGraphs Workshopon Graph-based Algorithms for NLP.Sam Scott and Stan Matwin.
1999.
Feature engineeringfor text classification.
In Proceedings of the 16th In-ternational Conference on Machine Learning (ICML-99).John Shawe-Taylor and Nello Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge UniversityPress, Cambridge.Georges Siolas and Florence d?Alche-Buc.
2000.
Sup-port vector machines based on a semantic kernel fortext categorization.
In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Net-works.Alexander J. Smola and Risi Kondor.
2003.
Kernels andregularization on graphs.
In Proceedings of the the16th Annual Conference on Learning Theory and 7thWorkshop on Kernel Machines (COLT-03).240
