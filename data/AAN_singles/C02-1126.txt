Recovering latent information in treebanksDavid Chiang and Daniel M. BikelUniversity of PennsylvaniaDept of Computer and Information Science200 S 33rd StreetPhiladelphia PA 19104 USA{dchiang,dbikel}@cis.upenn.eduAbstractMany recent statistical parsers rely on a preprocess-ing step which uses hand-written, corpus-specificrules to augment the training data with extra infor-mation.
For example, head-finding rules are usedto augment node labels with lexical heads.
In thispaper, we provide machinery to reduce the amountof human effort needed to adapt existing models tonew corpora: first, we propose a flexible notation forspecifying these rules that would allow them to beshared by different models; second, we report on anexperiment to see whether we can use Expectation-Maximization to automatically fine-tune a set ofhand-written rules to a particular corpus.1 IntroductionMost work in statistical parsing does not operatein the realm of parse trees as they appear in manytreebanks, but rather on trees transformed via aug-mentation of their node labels, or some other trans-formation (Johnson, 1998).
This methodology is il-lustrated in Figure 1.
The information included inthe node labels?
augmentations may include lexicalitems, or a node label suffix to indicate the node is anargument and not an adjunct; such extra informationmay be viewed as latent information, in that it is notdirectly present in the treebank parse trees, but maybe recovered by some means.
The process of recov-ering this latent information has largely been limitedto the hand-construction of heuristics.
However, asis often the case, hand-constructed heuristics maynot be optimal or very robust.
Also, the effort re-quired to construct such rules can be considerable.In both respects, the use of such rules runs counterto the data-driven approach to statistical parsing.In this paper, we propose two steps to addressthis problem.
First, we define a new, fairly simplesyntax for the identification and transformation ofnode labels that accommodates a wide variety ofnode-label augmentations, including all those thatModelparsed data +annotated data +annotated data Training Decoding parsed dataFigure 1: Methodology for the development of a sta-tistical parser.
A + indicates augmentation.are performed by existing statistical parsers that wehave examined.
Second, we explore a novel use ofExpectation-Maximization (Dempster et al, 1977)that iteratively reestimates a parsing model usingthe augmenting heuristics as a starting point.
Specif-ically, the EM algorithm we use is a variant ofthe Inside-Outside algorithm (Baker, 1979; Lari andYoung, 1990; Hwa, 1998).
The reestimation adjuststhe model?s parameters in the augmented parse-treespace to maximize the likelihood of the observed(incomplete) data, in the hopes of finding a betterdistribution over augmented parse trees (the com-plete data).
The ultimate goal of this work is to mini-mize the human effort needed when adapting a pars-ing model to a new domain.2 Background2.1 Head-lexicalizationMany of the recent, successful statistical parsershave made use of lexical information or an im-plicit lexicalized grammar, both for English and,more recently, for other languages.
All of theseparsers recover the ?hidden?
lexicalizations in atreebank and find the most probable lexicalized treewhen parsing, only to strip out this hidden infor-mation prior to evaluation.
Also, in all these pars-ing efforts lexicalization has meant finding headsof constituents and then propagating those lexicalheads to their respective parents.
In fact, nearlyidentical head-lexicalizations were used in the dis-S(caught?VBD)NP(boy?NN)DETTheNNboyADVP(also?RB)RBalsoVP(caught?VBD)VBDcaughtNP(ball?NN)DETtheNNballFigure 2: A simple lexicalized parse tree.criminative models described in (Magerman, 1995;Ratnaparkhi, 1997), the lexicalized PCFG modelsin (Collins, 1999), the generative model in (Char-niak, 2000), the lexicalized TAG extractor in (Xia,1999) and the stochastic lexicalized TAG modelsin (Chiang, 2000; Sarkar, 2001; Chen and Vijay-Shanker, 2000).
Inducing a lexicalized structurebased on heads has a two-pronged effect: it notonly allows statistical parsers to be sensitive to lex-ical information by including this information inthe probability model?s dependencies, but it alsodetermines which of all possible dependencies?both syntactic and lexical?will be included in themodel itself.
For example, in Figure 2, the nontermi-nal NP(boy?NN) is dependent on VP(caught?VBD)and not the other way around.2.2 Other tree transformationsLexicalization via head-finding is but one of manypossible tree transformations that might be use-ful for parsing.
As explored thoroughly by John-son (1998), even simple, local syntactic trans-formations on training trees for an unlexicalizedPCFG model can have a significant impact on pars-ing performance.
Having picked up on this idea,Collins (1999) devises rules to identify arguments,i.e., constituents that are required to exist on a par-ticular side of a head child constituent dominatedby a particular parent.
The parsing model can thenprobabilistically predict sets of requirements on ei-ther side of a head constituent, thereby incorporat-ing a type of subcategorization information.
Whilethe model is augmented to include this subcat-prediction feature, the actual identification of argu-ments is performed as one of many preprocessingsteps on training trees, using a set of rules sim-ilar to those used for the identification of heads.Also, (Collins, 1999) makes use of several othertransformations, such as the identification of sub-jectless sentences (augmenting S nodes to becomeSG) and the augmentation of nonterminals for gapthreading.
Xia (1999) combines head-finding withargument identification to extract elementary treesfor use in the lexicalized TAG formalism.
Other re-searchers investigated this type of extraction to con-struct stochastic TAG parsers (Chiang, 2000; Chenand Vijay-Shanker, 2000; Sarkar, 2001).2.3 Problems with heuristicsWhile head-lexicalization and other tree transfor-mations allow the construction of parsing modelswith more data-sensitivity and richer representa-tions, crafting rules for these transformations hasbeen largely an art, with heuristics handed downfrom researcher to researcher.
What?s more, ontop of the large undertaking of designing and im-plementing a statistical parsing model, the use ofheuristics has required a further effort, forcing theresearcher to bring both linguistic intuition and,more often, engineering savvy to bear whenevermoving to a new treebank.
For example, in the rulesets used by the parsers described in (Magerman,1995; Ratnaparkhi, 1997; Collins, 1999), the sets ofrules for finding the heads of ADJP, ADVP, NAC,PP and WHPP include rules for picking either therightmost or leftmost FW (foreign word).
The ap-parently haphazard placement of these rules thatpick out FW and the rarity of FW nodes in the datastrongly suggest these rules are the result of engi-neering effort.
Furthermore, it is not at all apparentthat tree-transforming heuristics that are useful forone parsing model will be useful for another.
Fi-nally, as is often the case with heuristics, those usedin statistical parsers tend not to be data-sensitive,and ironically do not rely on the words themselves.3 Rule-based augmentationIn the interest of reducing the effort required to con-struct augmentation heuristics, we would like a no-tation for specifying rules for selecting nodes inbracketed data that is both flexible enough to encodethe kinds of rule sets used by existing parsers, andintuitive enough that a rule set for a new languagecan be written easily without knowledge of com-puter programming.
Such a notation would simplifythe task of writing new rule sets, and facilitate ex-perimentation with different rules.
Moreover, ruleswritten in this notation would be interchangeablebetween different models, so that, ideally, adapta-tion of a model to a new corpus would be trivial.We define our notation in two parts: a structurepattern language, whose basic patterns are speci-fications of single nodes written in a label patternlanguage.3.1 Structure patternsMost existing head-finding rules and argument-finding rules work by specifying parent-child rela-tions (e.g., NN is the head of NP, or NP is an argu-ment of VP).
A generalization of this scheme thatis familiar to linguists and computer scientists alikewould be a context-free grammar with rules of theformA?
A1 ?
?
?
(Ai)l ?
?
?
An,where the superscript l specifies that if this rule getsused, the ith child of A should be marked with thelabel l.However, there are two problems with such an ap-proach.
First, writing down such a grammar wouldbe tedious to say the least, and impossible if wewant to handle trees with arbitrary branching fac-tors.
So we can use an extended CFG (Thatcher,1967), a CFG whose right-hand sides are regular ex-pressions.
Thus we introduce a union operator (?
)and a Kleene star (?)
into the syntax for right-handsides.The second problem that our grammar may beambiguous.
For example, the grammarX?
YhY ?
YYhcould mark with an h either the first or second sym-bol of YY.
So we impose an ordering on the rules ofthe grammar: if two rules match, the first one wins.In addition, we make the ?
operator noncommuta-tive: ??
?
tries to match ?
first, and ?
only if it doesnot match ?, as in Perl.
(Thus the above grammarwould mark the first Y.)
Similarly, ??
tries to matchas many times as possible, also as in Perl.But this creates a third and final problem: in thegrammarX?
(YYh ?
Yh)(YY ?
Y),it is not defined which symbol of YYY should bemarked, that is, which union operator takes priorityover the other.
Perl circumvents this problem by al-ways giving priority to the left.
In algebraic terms,concatenation left-distributes over union but doesnot right-distribute over union in general.However, our solution is to provide a pair of con-catenation operators: , which gives priority to theleft, and ?, which gives priority to the right:X ?
(YYh ?
Yh)  (YY ?
Y) (1)X ?
(YYh ?
Yh) ?
(YY ?
Y) (2)Rule (1) marks the second Y in YYY, but rule (2)marks the first Y.
More formally,?
?
(?
?
?)
= (?
?
?)
?
(?
?
?)(?
?
?)
?
= (?
?)
?
(?
?
)But if ?
contains no unions or Kleene stars, then?
?
= ?
?
?
(?
??)?
?
= ?
?
?
(?
??
)So then, consider the following rules:VP ?
??
VBh  ?
?, (3)VP ?
??
?
VBh ?
??.
(4)where ?
is a wildcard pattern which matches anysingle label (see below).
Rule (3) mark with an hthe rightmost VB child of a VP, whereas rule (4)marks the leftmost VB.
This is because the Kleenestar always prefers to match as many times as possi-ble, but in rule (3) the first Kleene star?s preferencetakes priority over the last?s, whereas in rule (4) thelast Kleene star?s preference takes priority over thefirst?s.Consider the slightly more complicated exam-ples:VP ?
??
?
(VBh ?MDh) ?
??
(5)VP ?
??
?
((VBh ?MDh)  ??)
(6)Rule (5) marks the leftmost child which is either aVB or a MD, whereas rule (6) marks the leftmostVB if any, or else the leftmost MD.
To see why thisso, consider the string MD VB X.
Rule (5) wouldmark the MD as h, whereas rule (6) would markthe VB.
In both rules VB is preferred over MD, andsymbols to the left over symbols to the right, but inrule (5) the leftmost preference (that is, the prefer-ence of the last Kleene star to match as many timesas possible) takes priority, whereas in rule (6) thepreference for VB takes priority.3.2 Label patternsSince nearly all treebanks have complex nontermi-nal alphabets, we need a way of concisely specify-ing classes of labels.
Unfortunately, this will neces-sarily vary somewhat across treebanks: all we candefine that is truly treebank-independent is the ?pattern, which matches any label.
For Penn Tree-bank II style annotation (Marcus et al, 1993), inwhich a nonterminal symbol is a category togetherwith zero or more functional tags, we adopt the fol-lowing scheme: the atomic pattern a matches anylabel with category a or functional tag a; more-over, we define Boolean operators ?, ?, and ?.
ThusNP ?
?ADV matches NP?SBJ but not NP?ADV.13.3 SummaryUsing the structure pattern language and the la-bel pattern language together, one can fully encodethe head/argument rules used by Xia (which resem-ble (5) above), and the family of rule sets used byBlack, Magerman, Collins, Ratnaparkhi, and others(which resemble (6) above).
In Collins?
version ofthe head rules, NP and PP require special treatment,but these can be encoded in our notation as well.4 Unsupervised learning of augmentationsIn the type of approach we have been discussingso far, hand-written rules are used to augment thetraining data, and this augmented training data isthen used to train a statistical model.
However, if wetrain the model by maximum-likelihood estimation,the estimate we get will indeed maximize the likeli-hood of the training data as augmented by the hand-written rules, but not necessarily that of the trainingdata itself.
In this section we explore the possibilityof training a model directly on unaugmented data.A generative model that estimates P(S ,T,T +)(where T+ is an augmented tree) is normally usedfor parsing, by computing the most likely (T,T +)for a given S .
But we may also use it for augment-ing trees, by computing the most likely T + for agiven sentence-tree pair (S ,T ).
From the latter per-spective, because its trees are unaugmented, a tree-bank is a corpus of incomplete data, warranting theuse of unsupervised learning methods to reestimatea model that includes hidden parameters.
The ap-proach we take below is to seed a parsing modelusing hand-written rules, and then use the Inside-Outside algorithm to reestimate its parameters.
Theresulting model, which locally maximizes the likeli-hood of the unaugmented training data, can then beused in two ways: one might hope that as a parser,it would parse more accurately than a model whichonly maximizes the likelihood of training data aug-mented by hand-written rules; and that as a tree-augmenter, it would augment trees in a more data-sensitive way than hand-written rules.4.1 Background: tree adjoining grammarThe parsing model we use is based on the stochas-tic tree-insertion grammar (TIG) model described1Note that unlike the noncommutative union operator ?, thedisjunction operator ?
has no preference for its first argument.by Chiang (2000).
TIG (Schabes and Waters, 1995)is a weakly-context free restriction of tree adjoin-ing grammar (Joshi and Schabes, 1997), in whichtree fragments called elementary trees are com-bined by two composition operations, substitutionand adjunction (see Figure 3).
In TIG there arecertain restrictions on the adjunction operation.Chiang?s model adds a third composition operationcalled sister-adjunction (see Figure 3), borrowedfrom D-tree substitution grammar (Rambow et al,1995).2There is an important distinction between derivedtrees and derivation trees (see Figure 3).
A deriva-tion tree records the operations that are used to com-bine elementary trees into a derived tree.
Thus thereis a many-to-one relationship between derivationtrees and derived trees: every derivation tree speci-fies a derived tree, but a derived tree can be the resultof several different derivations.The model can be trained directly on TIG deriva-tions if they are available, but corpora like thePenn Treebank have only derived trees.
Just asCollins uses rules to identify heads and argumentsand thereby lexicalize trees, Chiang uses nearly thesame rules to reconstruct derivations: each trainingexample is broken into elementary trees, with eachhead child remaining attached to its parent, each ar-gument broken into a substitution node and an ini-tial root, and each adjunct broken off as a modifierauxiliary tree.However, in this experiment we view the derivedtrees in the Treebank as incomplete data, and try toreconstruct the derivations (the complete data) usingthe Inside-Outside algorithm.4.2 ImplementationThe expectation step (E-step) of the Inside-Outsidealgorithm is performed by a parser that computes allpossible derivations for each parse tree in the train-ing data.
It then computes inside and outside prob-abilities as in Hwa?s experiment (1998), and usesthese to compute the expected number of times eachevent occurred.
For the maximization step (M-step),we obtain a maximum-likelihood estimate of the pa-rameters of the model using relative-frequency es-2The parameters for sister-adjunction in the present modeldiffer slightly from the original.
In the original model, all themodifier auxiliary trees that sister-adjoined at a particular po-sition were generated independently, except that each sister-adjunction was conditioned on whether it was the first at thatposition.
In the present model, each sister-adjunction is condi-tioned on the root label of the previous modifier tree.NPNNPJohnSNP?
VPVBleaveVPMDshouldVP?NPNNtomorrow(?1)(?2)(?)
(?)?
?2?11?2?2,1SNPNNPJohnVPMDshouldVPVBleaveNPNNtomorrowDerivation tree Derived treeFigure 3: Grammar and derivation for ?John should leave tomorrow.?
In this derivation, ?1 gets substituted,?
gets adjoined, and ?
gets sister-adjoined.timation, just as in the original experiment, as ifthe expected values for the complete data were thetraining data.Smoothing presents a special problem.
There areseveral several backoff levels for each parameterclass that are combined by deleted interpolation.
Let?1, ?2 and ?3 be functions from full history con-texts Y to less specific contexts at levels 1, 2 and3, respectively, for some parameter class with threebackoff levels (with level 1 using the most specificcontexts).
Smoothed estimates for parameters in thisclass are computed as follows:e = ?1e1 + (1 ?
?1)(?2e2 + (1 ?
?2)e3)where ei is the estimate of p(X | ?i(Y)) for somefuture context X, and the ?i are computed by theformula found in (Bikel et al, 1997), modified touse the multiplicative constant 5 found in the similarformula of (Collins, 1999):?i =(1 ?di?1di) (11 + 5ui/di)(7)where di is the number of occurrences in training ofthe context ?i(Y) (and d0 = 0), and ui is the numberof unique outcomes for that context seen in training.There are several ways one might incorporate thissmoothing into the reestimation process, and wechose to depart as little as possible from the orig-inal smoothing method: in the E-step, we use thesmoothed model, and after the M-step, we use theoriginal formula (7) to recompute the smoothingweights based on the new counts computed fromthe E-step.
While simple, this approach has two im-portant consequences.
First, since the formula forthe smoothing weights intentionally does not maxi-mize the likelihood of the training data, each itera-tion of reestimation is not guaranteed to increase the87.387.3587.487.4587.587.5587.60 5 10 15 20F-measureIterationFigure 4: English, starting with full rule setlikelihood of the training data.
Second, reestimationtends to increase the size of the model in memory,since smoothing gives nonzero expected counts tomany events which were unseen in training.
There-fore, since the resulting model is quite large, if anevent at a particular point in the derivation foresthas an expected count below 10?15, we throw it out.4.3 ExperimentWe first trained the initial model on sections 02?21of the WSJ corpus using the original head rules, andthen ran the Inside-Outside algorithm on the samedata.
We tested each successive model on someheld-out data (section 00), using a beam width of10?4, to determine at which iteration to stop.
TheF-measure (harmonic mean of labeled precision andrecall) for sentences of length ?
100 for each itera-tion is shown in Figure 4.
We then selected the ninthreestimated model and compared it with the initialmodel on section 23 (see Figure 7).
This model didonly marginally better than the initial model on sec-tion 00, but it actually performs worse than the ini-tial model on section 23.
One explanation is that the84.584.5584.684.6584.784.7584.884.8584.984.958585.050 5 10 15 20 25 30 35 40F-measureIterationFigure 5: English, starting with simplified rule set7373.0573.173.1573.273.2573.373.3573.473.4573.50 5 10 15 20 25 30 35 40F-measureIterationFigure 6: Chinese, starting with full rule sethead rules, since they have been extensively fine-tuned, do not leave much room for improvement.To test this, we ran two more experiments.The second experiment started with a simplifiedrule set, which simply chooses either the leftmost orrightmost child of each node as the head, depend-ing on the label of the parent: e.g., for VP, the left-most child is chosen; for NP, the rightmost childis chosen.
The argument rules, however, were notchanged.
This rule set is supposed to represent thekind of rule set that someone with basic familiaritywith English syntax might write down in a few min-utes.
The reestimated models seemed to improve onthis simplified rule set when parsing section 00 (seeFigure 5); however, when we compared the 30threestimated model with the initial model on section23 (see Figure 7), there was no improvement.The third experiment was on the Chinese Tree-bank, starting with the same head rules used in(Bikel and Chiang, 2000).
These rules were origi-nally written by Xia for grammar development, andalthough we have modified them for parsing, theyhave not received as much fine-tuning as the Englishrules have.
We trained the model on sections 001?270 of the Penn Chinese Treebank, and reestimatedit on the same data, testing it at each iteration onsections 301?325 (Figure 6).
We selected the 38threestimated model for comparison with the initialmodel on sections 271?300 (Figure 7).
Here we didobserve a small improvement: an error reduction of3.4% in the F-measure for sentences of length ?
40.4.4 DiscussionOur hypothesis that reestimation does not improveon the original rule set for English because thatrule set is already fine-tuned was partially borneout by the second and third experiments.
The modeltrained with a simplified rule set for English showedimprovement on held-out data during reestimation,but showed no improvement in the final evaluation;however, the model trained on Chinese did show asmall improvement in both.
We are uncertain as towhy the gains observed during the second experi-ment were not reflected in the final evaluation, butbased on the graph of Figure 5 and the results onChinese, we believe that reestimation by EM canbe used to facilitate adaptation of parsing modelsto new languages or corpora.It is possible that our method for choosingsmoothing weights at each iteration (see ?4.2) iscausing some interference.
For future work, morecareful methods should be explored.
We wouldalso like to experiment on the parsing model ofCollins (1999), which, because it can recombinesmaller structures and reorder subcategorizationframes, might open up the search space for betterreestimation.5 ConclusionEven though researchers designing and implement-ing statistical parsing models have worked in themethodology shown in Figure 1 for several yearsnow, most of the work has focused on finding ef-fective features for the model component of themethodology, and on finding effective statisticaltechniques for parameter estimation.
However, therehas been much behind-the-scenes work on the ac-tual transformations, such as head finding, and mostof this work has consisted of hand-tweaking exist-ing heuristics.
It is our hope that by introducing thisnew syntax, less toil will be needed to write non-terminal augmentation rules, and that human effortwill be lessened further by the use of unsupervisedmethods such as the one presented here to producebetter models for parsing and tree augmentation.?
100 words ?
40 wordsModel Step LR LP CB 0CB ?
2 CB LR LP CB 0CB ?
2 CBOriginal initial 86.95 87.02 1.21 62.38 82.33 87.68 87.76 1.02 65.30 84.86Original 9 86.37 86.71 1.26 61.42 81.79 87.18 87.48 1.06 64.41 84.23Simple initial 84.50 84.18 1.54 57.57 78.35 85.46 85.17 1.29 60.71 81.11Simple 30 84.21 84.50 1.53 57.95 77.77 85.12 85.35 1.30 60.94 80.62Chinese initial 75.30 76.77 2.72 45.95 67.05 78.37 80.03 1.79 52.82 74.75Chinese 38 75.20 77.99 2.66 47.69 67.63 78.79 81.06 1.69 54.15 75.08Figure 7: Results on test sets.
Original = trained on English with original rule set; Simple = English, sim-plified rule set.
LR = labeled recall, LP = labeled precision; CB = average crossing brackets, 0 CB = nocrossing brackets, ?
2 CB = two or fewer crossing brackets.
All figures except CB are percentages.AcknowledgmentsThis research was supported in part by NSF grantSBR-89-20230.
We would like to thank AnoopSarkar, Dan Gildea, Rebecca Hwa, Aravind Joshi,and Mitch Marcus for their valuable help.ReferencesJames K. Baker.
1979.
Trainable grammars for speechrecognition.
In Proceedings of the Spring Conferenceof the Acoustical Society of America, pages 547?550.Daniel M. Bikel and David Chiang.
2000.
Two statisti-cal parsing models applied to the Chinese Treebank.In Proceedings of the Second Chinese Language Pro-cessing Workshop, pages 1?6.Daniel M. Bikel, Scott Miller, Richard Schwartz,and Ralph Weischedel.
1997.
Nymble: a high-performance learning name-finder.
In Proceedings ofthe Fifth Conference on Applied Natural LanguageProcessing (ANLP 1997), pages 194?201.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of ANLP-NAACL2000, pages132?139.John Chen and K. Vijay-Shanker.
2000.
Automated ex-traction of TAGs from the Penn Treebank.
In Pro-ceedings of the Sixth International Workshop on Pars-ing Technologies (IWPT 2000), pages 65?76, Trento.David Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining grammar.
InProceedings of ACL-2000, pages 456?463.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univ.
ofPennsylvania.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
J. Roy.
Stat.
Soc.
B, 39:1?38.Rebecca Hwa.
1998.
An empirical evaluation of prob-abilistic lexicalized tree insertion grammars.
In Pro-ceedings of COLING-ACL ?98, pages 557?563.Mark Johnson.
1998.
PCFG models of linguistic treerepresentations.
Computational Linguistics, 24:613?632.Aravind K. Joshi and Yves Schabes.
1997.
Tree-adjoining grammars.
In Grzegorz Rosenberg and ArtoSalomaa, editors, Handbook of Formal Languagesand Automata, volume 3, pages 69?124.
Springer-Verlag, Heidelberg.K.
Lari and S. J.
Young.
1990.
The estimation ofstochastic context-free grammars using the inside-outside algorithm.
Computer Speech and Language,4:35?56.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proceedings of ACL ?95, pages276?283.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19:313?330.Owen Rambow, K. Vijay-Shanker, and David Weir.1995.
D-tree grammars.
In Proceedings of ACL ?95,pages 151?158.Adwait Ratnaparkhi.
1997.
A linear observed time sta-tistical parser based on maximum entropy models.
InProceedings of the Second Conference on EmpiricalMethods in Natural Language Processing (EMNLP-2).Anoop Sarkar.
2001.
Applying co-training methods tostatistical parsing.
In Proceedings of NAACL-2001,pages 175?182.Yves Schabes and Richard C. Waters.
1995.
Tree in-sertion grammar: a cubic-time parsable formalismthat lexicalizes context-free grammar without chang-ing the trees produced.
Computational Linguistics,21:479?513.J.
W. Thatcher.
1967.
Characterizing derivation trees ofcontext-free grammars through a generalization of fi-nite automata theory.
J. Comp.
Sys.
Sci., 1:317?322.Fei Xia.
1999.
Extracting tree adjoining grammars frombracketed corpora.
In Proceedings of the 5th Nat-ural Language Processing Pacific Rim Symposium(NLPRS-99), pages 398?403.
