Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1316?1325,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsCan You Repeat That?Using Word Repetition to Improve Spoken Term DetectionJonathan Wintrode and Sanjeev KhudanpurCenter for Language and Speech ProcessingJohns Hopkins Universityjcwintr@cs.jhu.edu , khudanpur@jhu.eduAbstractWe aim to improve spoken term detec-tion performance by incorporating con-textual information beyond traditional N-gram language models.
Instead of taking abroad view of topic context in spoken doc-uments, variability of word co-occurrencestatistics across corpora leads us to fo-cus instead the on phenomenon of wordrepetition within single documents.
Weshow that given the detection of one in-stance of a term we are more likely tofind additional instances of that term in thesame document.
We leverage this bursti-ness of keywords by taking the most con-fident keyword hypothesis in each docu-ment and interpolating with lower scor-ing hits.
We then develop a principledapproach to select interpolation weightsusing only the ASR training data.
Us-ing this re-weighting approach we demon-strate consistent improvement in the termdetection performance across all five lan-guages in the BABEL program.1 IntroductionThe spoken term detection task arises as a key sub-task in applying NLP applications to spoken con-tent.
Tasks like topic identification and named-entity detection require transforming a continu-ous acoustic signal into a stream of discrete to-kens which can then be handled by NLP and otherstatistical machine learning techniques.
Given asmall vocabulary of interest (1000-2000 words ormulti-word terms) the aim of the term detectiontask is to enumerate occurrences of the keywordswithin a target corpus.
Spoken term detection con-verts the raw acoustics into time-marked keywordoccurrences, which may subsequently be fed (e.g.as a bag-of-terms) to standard NLP algorithms.Although spoken term detection does not re-quire the use of word-based automatic speechrecognition (ASR), it is closely related.
If wehad perfectly accurate ASR in the language ofthe corpus, term detection is reduced to an exactstring matching task.
The word error rate (WER)and term detection performance are clearly corre-lated.
Given resource constraints, domain, chan-nel, and vocabulary limitations, particularly forlanguages other than English, the errorful tokenstream makes term detection a non-trivial task.In order to improve detection performance, andrestricting ourselves to an existing ASR systemor systems at our disposal, we focus on leverag-ing broad document context around detection hy-potheses.
ASR systems traditionally use N-gramlanguage models to incorporate prior knowledgeof word occurrence patterns into prediction of thenext word in the token stream.
N-gram mod-els cannot, however, capture complex linguistic ortopical phenomena that occur outside the typical3-5 word scope of the model.
Yet, though manylanguage models more sophisticated than N-gramshave been proposed, N-grams are empirically hardto beat in terms of WER.We consider term detection rather than the tran-scription task in considering how to exploit topiccontext, because in evaluating the retrieval of cer-tain key terms we need not focus on improvingthe entire word sequence.
Confidence scores froman ASR system (which incorporate N-gram prob-abilities) are optimized in order to produce themost likely sequence of words rather than the ac-curacy of individual word detections.
Looking atbroader document context within a more limitedtask might allow us to escape the limits of N-gramperformance.
We will show that by focusing oncontextual information in the form of word repe-tition within documents, we obtain consistent im-provement across five languages in the so calledBase Phase of the IARPA BABEL program.13161.1 Task OverviewWe evaluate term detection and word repetition-based re-scoring on the IARPA BABEL trainingand development corpora1for five languages Can-tonese, Pashto, Turkish, Tagalog and Vietnamese(Harper, 2011).
The BABEL task is modeled onthe 2006 NIST Spoken Term Detection evaluation(NIST, 2006) but focuses on limited resource con-ditions.
We focus specifically on the so called notarget audio reuse (NTAR) condition to make ourmethod broadly applicable.In order to arrive at our eventual solution, wetake the BABEL Tagalog corpus and analyze wordco-occurrence and repetition statistics in detail.Our observation of the variability in co-occurrencestatistics between Tagalog training and develop-ment partitions leads us to narrow the scope ofdocument context to same word co-occurrences,i.e.
word repetitions.We then analyze the tendency towards within-document repetition.
The strength of this phe-nomenon suggests it may be more viable for im-proving term-detection than, say, topic-sensitivelanguage models.
We validate this by develop-ing an interpolation formula to boost putative wordrepetitions in the search results, and then inves-tigate a method for setting interpolation weightswithout manually tuning on a development set.We then demonstrate that the method general-izes well, by applying it to the 2006 English dataand the remaining four 2013 BABEL languages.We demonstrate consistent improvements in alllanguages in both the Full LP (80 hours of ASRtraining data) and Limited LP (10 hours) settings.2 MotivationWe seek a workable definition of broad docu-ment context beyond N-gram models that will im-prove term detection performance on an arbitraryset of queries.
Given the rise of unsupervised la-tent topic modeling with Latent Dirchlet Alloca-tion (Blei et al, 2003) and similar latent variableapproaches for discovering meaningful word co-occurrence patterns in large text corpora, we oughtto be able to leverage these topic contexts insteadof merely N-grams.
Indeed there is work in theliterature that shows that various topic models, la-tent or otherwise, can be useful for improving lan-1Language collection releases IARPA-babel101-v0.4c,IARPA-babel104b-v0.4bY, IARPA-babel105b-v0.4, IARPA-babel106-v0.2g and IARPA-babel107b-v0.7 respectively.guage model perplexity and word error rate (Khu-danpur and Wu, 1999; Chen, 2009; Naptali etal., 2012).
However, given the preponderance ofhighly frequent non-content words in the compu-tation of a corpus?
WER, it?s not clear that a 1-2%improvement in WER would translate into an im-provement in term detection.Still, intuition suggests that knowing the topiccontext of a detected word ought to be usefulin predicting whether or not a term does belongin that context.
For example, if we determinethe context of the detection hypothesis is aboutcomputers, containing words like ?monitor,?
?in-ternet?
and ?mouse,?
then we would be more con-fident of a term such as ?keyboard?
and less con-fident of a term such as ?cheese board?.
The dif-ficulty in this approach arises from the variabil-ity in word co-occurrence statistics.
Using topicinformation will be helpful if ?monitor,?
?key-board?
and ?mouse?
consistently predict that ?key-board?
is present.
Unfortunately, estimates of co-occurrence from small corpora are not very consis-tent, and often over- or underestimate concurrenceprobabilities needed for term detection.We illustrate this variability by looking at howconsistent word co-occurrences are between twoseparate corpora in the same language: i.e., if weobserve words that frequently co-occur with a key-word in the training corpus, do they also co-occurwith the keywords in a second held-out corpus?Figure 1, based on the BABEL Tagalog corpus, sug-gests this is true only for high frequency keywords.Figure 1: Correlation between the co-occurrencecounts in the training and held-out sets for a fixedkeyword (term) and all its ?context?
words.Each point in Figure 1 represents one of 3551317(a) High frequency keyword ?bukas?
(b) Low frequency keyword ?Davao?Figure 2: The number of times a fixed keyword k co-occurs with a vocabulary word w in the trainingspeech collection ?
T (k,w) ?
versus the search collection ?
D(k,w).Tagalog keywords used for system developmentby all BABEL participants.
For each keyword k,we count how often it co-occurs in the same con-versation as a vocabulary word w in the ASRtraining data and the development data, and des-ignate the counts T (k,w) and D(k,w) respec-tively.
The x-coordinate of each point in Figure 1is the frequency of k in the training data, and they-coordinate is the correlation coefficient ?kbe-tween T (k,w) and D(k,w).
A high ?kimpliesthat wordsw that co-occur frequently with k in thetraining data also do so in the search collection.To further illustrate how Figure 1 was obtained,consider the high-frequency keyword bukas (count= 879) and the low-frequency keyword Davao(count = 11), and plot T (k, ?)
versus D(k, ?
),as done in Figure 2.
The correlation coefficients?bukasand ?Davaofrom the two plots end up as twopoints in Figure 1.Figure 1 suggests that (k,w) co-occurrences areconsistent between the two corpora (?k> 0.8) forkeywords occurring 100 or more times.
However,if the goal is to help a speech retrieval system de-tect content-rich (and presumably infrequent) key-words, then using word co-occurrence informa-tion (i.e.
topic context) does not appear to betoo promising, even though intuition suggests thatsuch information ought to be helpful.In light of this finding, we will restrict the typeof context we use for term detection to the co-occurrence of the term itself elsewhere within thedocument.
As it turns out this ?burstiness?
ofwords within documents, as the term is defined byChurch and Gale in their work on Poisson mix-tures (1995), provides a more reliable frameworkfor successfully exploiting document context.2.1 Related WorkA number of efforts have been made to augmenttraditional N-gram models with latent topic infor-mation (Khudanpur and Wu, 1999; Florian andYarowsky, 1999; Liu and Liu, 2008; Hsu andGlass, 2006; Naptali et al, 2012) including someof the early work on Probabilistic Latent SemanticAnalysis by Hofmann (2001).
In all of these casesWER gains in the 1-2% range were observed byinterpolating latent topic information with N-grammodels.The re-scoring approach we present is closelyrelated to adaptive or cache language models (Je-linek, 1997; Kuhn and De Mori, 1990; Kneser andSteinbiss, 1993).
The primary difference betweenthis and previous work on similar language mod-els is the narrower focus here on the term detec-tion task, in which we consider each search term inisolation, rather than all words in the vocabulary.Most recently, Chiu and Rudnicky (2013) lookedat word bursts in the IARPA BABEL conversationalcorpora, and were also able to successfully im-prove performance by leveraging the burstiness oflanguage.
One advantage of the approach pro-posed here, relative to their approach, is its sim-plicity and its not requiring an additional tuningset to estimate parameters.In the information retrieval community, cluster-ing and latent topic models have yielded improve-ments over traditional vector space models.
Wewill discuss in detail in the following section re-lated works by Church and Gale (1995, 1999, and2000).
Work by Wei and Croft (2006) and Chen(2009) take a language model-based approach to1318(a) fwversus IDFw?
(b) Obsered versus predicted IDFwFigure 3: Tagalog corpus frequency statistics, unigramsinformation retrieval, and again, interpolate latenttopic models with N-grams to improve retrievalperformance.
However, in many text retrievaltasks, queries are often tens or hundreds of wordsin length rather than short spoken phrases.
In theseefforts, the topic model information was helpful inboosting retrieval performance above the baselinevector space or N-gram models.Clearly topic or context information is relevantto a retrieval type task, but we need a stable, con-sistent framework in which to apply it.3 Term and Document FrequencyStatisticsTo this point we have assumed an implicit propertyof low-frequency words which Church and Galestate concisely in their 1999 study of inverse doc-ument frequency:Low frequency words tend to be richin content, and vice versa.
But notall equally frequent words are equallymeaningful.
Church and Gale (1999).The typical use of Document Frequency (DF) ininformation retrieval or text categorization is toemphasize words that occur in only a few docu-ments and are thus more ?rich in content?.
Closeexamination of DF statistics by Church and Galein their work on Poisson Mixtures (1995) resultedin an analysis of the burstiness of content words.In this section we look at DF and burstinessstatistics applying some of the analyses of Churchand Gale (1999) to the BABEL Tagalog corpus.We observe, in 648 Tagalog conversations, simi-lar phenomena as observed by Church and Gale on89,000 AP English newswire articles.
We proceedin this fashion to make a case for why burstinessought to help in the term detection task.For the Tagalog conversations, as with En-glish newswire, we observe that the document fre-quency, DFw, of a word w is not a linear functionof word frequency fwin the log domain, as wouldbe expected under a naive Poisson generative as-sumption.
The implication of deviations from aPoisson model is that words tend to be concen-trated in a small number of documents rather thanoccurring uniformly across the corpus.
This is theburstiness we leverage to improve term detection.The first illustration of word burstiness can beseen by plotting observed inverse document fre-quency, IDFw, versus fwin the log domain (Fig-ure 3a).
We use the same definition of IDFwasChurch and Gale (1999):IDFw= ?
log2DFwN, (1)where N is the number of documents (i.e.
conver-sations) in the corpus.There is good linear correlation (?
= 0.73) be-tween log fwand IDFw.
Yet, visually, the rela-tionship in Figure 3a is clearly not linear.
In con-trast, the AP English data exhibits a correlation of?
= 0.93 (Church and Gale, 1999).
Thus the devi-ation in the Tagalog corpus is more pronounced,i.e.
words are less uniformly distributed acrossdocuments.A second perspective on word burstiness thatfollows from Church and Gale (1999) is that aPoisson assumption should lead us to predict:?IDFw= ?
log2(1?
e?fwN).
(2)1319Figure 4: Difference between observed and pre-dicted IDFwfor Tagalog unigrams.For the AP newswire, Church and Gale found thelargest deviation between the predicted?IDFwandobserved IDFwto occur in the middle of the fre-quency range.
We see a somewhat different pic-ture for Tagalog speech in Figure 3b.
ObservedIDFwvalues again deviate significantly from theirpredictions (2), but all along the frequency range.There is a noticeable quantization effect occur-ring in the high IDF range, given that our N is atleast a factor of 100 smaller than the number ofAP articles they studied: 648 vs. 89,000.
Figure 4also shows the difference between and observedIDFwand Poisson estimate?IDFwand further il-lustrates the high variance in IDFwfor low fre-quency words.Two questions arise: what is happening with in-frequent words, and why does this matter for termdetection?
To look at the data from a differentperspective, we consider the random variable k,which is the number of times a word occurs in aparticular document.
In Figure 5 we plot the fol-lowing ratio, which Church and Gale (1995) defineas burstiness :Ew[k|k > 0] =fwDFw(3)as a function of fw.
We denote this as E[k] andcan interpret burstiness as the expected word countgiven we see w at least once.In Figure 5 we see two classes of words emerge.A similar phenomenon is observed concerningadaptive language models (Church, 2000).
Ingeneral, we can think of using word repetitionsto re-score term detection as applying a limitedform of adaptive or cache language model (Je-linek, 1997).
Likewise, Katz attempts to captureFigure 5: Tagalog burstiness.these two classes in his G model of word frequen-cies (1996).For the first class, burstiness increases slowlybut steadily as w occurs more frequently.
Let uslabel these Class A words.
Since our corpus sizeis fixed, we might expect this to occur, as moreword occurrences must be pigeon-holed into thesame number of documentsLooking close to the y-axis in Figure 5, we ob-serve a second class of exclusively low frequencywords whose burstiness ranges from highly con-centrated to singletons.
We will refer to these asClass B words.
If we take the Class A concentra-tion trend as typical, we can argue that most ClassB words exhibit a larger than average concentra-tion.
In either case we see evidence that both highand low frequency words tend towards repeatingwithin a document.3.1 Unigram ProbabilitiesIn applying the burstiness quantity to term detec-tion, we recall that the task requires us to locate aparticular instance of a term, not estimate a count,hence the utility of N-gram language models pre-dicting words in sequence.We encounter the burstiness property of wordsagain by looking at unigram occurrence probabili-ties.
We compare the unconditional unigram prob-ability (the probability that a given word token isw) with the conditional unigram probability, giventhe term has occurred once in the document.
Wecompute the conditional probability for w usingfrequency information.1320Figure 6: Difference between conditional and un-conditional unigram probabilities for TagalogP (w|k > 0) =fw?DFw?D:w?D|D|(4)Figure 6 shows the difference between con-ditional and unconditional unigram probabilities.Without any other information, Zipf?s law sug-gests that most word types do not occur in a partic-ular document.
However, conditioning on one oc-currence, most word types are more likely to occuragain, due to their burstiness.Finally we measure the adaptation of a word,which is defined by Church and Gale (1995) as:Padapt(w) = Pw(k > 1|k > 0) (5)When we plot adaptation versus fw(Figure 7)we see that all high-frequency and a significantnumber of low-frequency terms have adaptationgreater that 50%.
To be precise, 26% of all to-kens and 25% of low-frequency (fw< 100) haveat least 50% adaptation.
Given that adaptation val-ues are roughly an order of magnitude higher thanthe conditional unigram probabilities, in the nexttwo sections we describe how we use adaptationto boost term detection scores.4 Term Detection Re-scoringWe summarize our re-scoring of repeated wordswith the observation: given a correct detection,the likelihood of additional terms in the same doc-uments should increase.
When we observe a termdetection score with high confidence, we boost theother lower-scoring terms in the same document toreflect this increased likelihood of repeated terms.Figure 7: Tagalog word adaptation probabilityFor each term t and document d we propose in-terpolating the ASR confidence score for a partic-ular detection tdwith the top scoring hit in dwhichwe?ll call?td.S(td) = (1?
?
)Pasr(td|O) + ?Pasr(?td|O) (6)We will we develop a principled approach to se-lecting ?
using the adaptation property of the cor-pus.
However to verify that this approach is worthpursuing, we sweep a range of small ?
values, onthe assumption that we still do want to mostly relyon the ASR confidence score for term detection.For the Tagalog data, we let ?
range from 0 (thebaseline) to 0.4 and re-score each term detectionscore according to (6).
Table 1 shows the resultsof this parameter sweep and yields us 1 to 2% ab-solute performance gains in a number of term de-tection metrics.?
ATWV P (Miss)0.00 0.470 0.4300.05 0.481 0.4220.10 0.483 0.4200.15 0.484 0.4180.20 0.483 0.4160.25 0.480 0.4170.30 0.477 0.4170.35 0.475 0.4150.40 0.471 0.4130.45 0.465 0.4130.50 0.462 0.410Table 1: Term detection scores for swept ?
valueson Tagalog development data1321The primary metric for the BABEL program, Ac-tual Term Weighted Value (ATWV) is defined byNIST using a cost function of the false alarm prob-ability P (FA) and P (Miss), averaged over a setof queries (NIST, 2006).
The manner in which thecomponents of ATWV are defined:P (Miss) = 1?Ntrue(term)/fterm(7)P (FA) = Nfalse/Durationcorpus(8)implies that cost of a miss is inversely proportionalto the frequency of the term in the corpus, but thecost of a false alarm is fixed.
For this reason, wereport both ATWV and the P (Miss) component.A decrease in P (Miss) reflects the fact that weare able to boost correct detections of the repeatedterms.4.1 Interpolation WeightsWe would prefer to use prior knowledge ratherthan naive tuning to select an interpolation weight?.
Our analysis of word burstiness suggests thatadaptation, is a reasonable candidate.
Adaptationalso has the desirable property that we can esti-mate it for each word in the training vocabularydirectly from training data and not post-hoc on aper-query basis.
We consider several different es-timates and we can show that the favorable resultextends across languages.Intuition suggests that we prefer per-term in-terpolation weights related to the term?s adapta-tion.
But despite the strong evidence of the adapta-tion phenomenon in both high and low-frequencywords (Figure 7), we have less confidence in theadaptation strength of any particular word.As with word co-occurrence, we consider if es-timates of Padapt(w) from training data are con-sistent when estimated on development data.
Fig-ure 8 shows the difference between Padapt(w)measured on the two corpora (for words occurringin both).We see that the adaptation estimates are onlyconsistent between corpora for high-frequencywords.
Using this Padapt(w) estimate directly ac-tually hurts ATWV performance by 4.7% absoluteon the 355 term development query set (Table 2).Given the variability in estimating Padapt(w),an alternative approach would be take?Pwas anupper bound on ?, reached as the DFwincreases(cf.
Equation 9).
We would discount the adapta-tion factor when DFwis low and we are unsure ofFigure 8: Difference in adaptation estimates be-tween Tagalog training and development corporaInterpolation Weight ATWV P (Miss)None 0.470 0.430Padapt(w) 0.423 0.474(1?
e?DFw)Padapt(w) 0.477 0.415??
= 0.20 0.483 0.416Table 2: Term detection performance using vari-ous interpolation weight strategies on Tagalog devdatathe effect.
?w= (1?
e?DFw) ?
?Padapt(w) (9)This approach shows a significant improvement(0.7% absolute) over the baseline.
However, con-sidering this estimate in light of the two classes ofwords in Figure 5, there are clearly words in ClassB with high burstiness that will be ignored by try-ing to compensate for the high adaptation variabil-ity in the low-frequency range.Alternatively, we take a weighted average of?w?s estimated on training transcripts to obtain asingle ??
per language (cf.
Equation 10).??
= Avgw[(1?
e?DFw)?
?Padapt(w)](10)Using this average as a single interpolation weightfor all terms gives near the best performance aswe observed in our parameter sweep.
Table 2contrasts the results for using the three differentinterpolation heuristics on the Tagalog develop-ment queries.
Using the mean ??
instead of indi-vidual ?w?s provides an additional 0.5% absolute1322Language ??
ATWV (%?)
P (Miss) (%?
)Full LP settingTagalog 0.20 0.523 (+1.1) 0.396 (-1.9)Cantonese 0.23 0.418 (+1.3) 0.458 (-1.9)Pashto 0.19 0.419 (+1.1) 0.453 (-1.6)Turkish 0.14 0.466 (+0.8) 0.430 (-1.3)Vietnamese 0.30 0.420 (+0.7) 0.445 (-1.0)English (Dev06) 0.20 0.670 (+0.3) 0.240 (-0.4)Limited LP settingTagalog 0.22 0.228 (+0.9) 0.692 (-1.7)Cantonese 0.26 0.205 (+1.0) 0.684 (-1.3)Pashto 0.21 0.206 (+0.9) 0.682 (-0.9)Turkish 0.16 0.202 (+1.1) 0.700 (-0.8)Vietnamese 0.34 0.227 (+1.0) 0.646 (+0.4)Table 3: Word-repetition re-scored results for available CTS term detection corporaimprovement, suggesting that we find additionalgains boosting low-frequency words.5 ResultsNow that we have tested word repetition-basedre-scoring on a small Tagalog development setwe want to know if our approach, and particu-larly our ??
estimate is sufficiently robust to applybroadly.
At our disposal, we have the five BABELlanguages ?
Tagalog, Cantonese, Pashto, Turk-ish and Vietnamese ?
as well as the developmentdata from the NIST 2006 English evaluation.
TheBABEL evaluation query sets contain roughly 2000terms each and the 2006 English query set con-tains roughly 1000 terms.The procedure we follow for each languagecondition is as follows.
We first estimate adap-tation probabilities from the ASR training tran-scripts.
From these we take the weighted aver-age as described previously to obtain a single in-terpolation weight ??
for each training condition.We train ASR acoustic and language models fromthe training corpus using the Kaldi speech recog-nition toolkit (Povey et al, 2011) following thedefault BABEL training and search recipe which isdescribed in detail by Chen et al (2013).
Lastly,we re-score the search output by interpolating thetop term detection score for a document with sub-sequent hits according to Equation 6 using the ?
?estimated for this training condition.For each of the BABEL languages we considerboth the FullLP (80 hours) and LimitedLP (10hours) training conditions.
For the English sys-tem, we also train a Kaldi system on the 240 hoursof the Switchboard conversational English cor-pus.
Although Kaldi can produce multiple typesof acoustic models, for simplicity we report resultsusing discriminatively trained Subspace GaussianMixture Model (SGMM) acoustic output densi-ties, but we do find that similar results can be ob-tained with other acoustic model configurations.Using our final algorithm, we are able to boostrepeated term detections and improve results in alllanguages and training conditions.
Table 3 listscomplete results and the associated estimates for??.
For the BABEL languages, we observe improve-ments in ATWV from 0.7% to 1.3% absolute andreductions in the miss rate of 0.8% to 1.9%.
Theonly test for which P (Miss) did not improve wasthe Vietnamese Limited LP setting, although over-all ATWV did improve, reflecting a lower P (FA).In all conditions we also obtain ?
estimateswhich correspond to our expectations for partic-ular languages.
For example, adaptation is low-est for the agglutinative Turkish language wherelonger word tokens should be less likely to re-peat.
For Vietnamese, with shorter, syllable lengthword tokens, we observe the lowest adaptation es-timates.Lastly, the reductions in P (Miss) suggests thatwe are improving the term detection metric, whichis sensitive to threshold changes, by doing whatwe set out to do, which is to boost lower confi-dence repeated words and correctly asserting them1323as true hits.
Moreover, we are able to accomplishthis in a wide variety of languages.6 ConclusionsLeveraging the burstiness of content words, wehave developed a simple technique to consis-tently boost term detection performance acrosslanguages.
Using word repetitions, we effectivelyuse a broad document context outside of the typi-cal 2-5 N-gram window.
Furthermore, we see im-provements across a broad spectrum of languages:languages with syllable-based word tokens (Viet-namese, Cantonese), complex morphology (Turk-ish), and dialect variability (Pashto).Secondly, our results are not only effective butalso intuitive, given that the interpolation weightparameter matches our expectations for the bursti-ness of the word tokens in the language on whichit is estimated.We have focused primarily on re-scoring resultsfor the term detection task.
Given the effective-ness of the technique across multiple languages,we hope to extend our effort to exploit our hu-man tendency towards redundancy to decoding orother aspects of the spoken document processingpipeline.AcknowledgementsThis work was partially supported by the In-telligence Advanced Research Projects Activity(IARPA) via Department of Defense U.S. ArmyResearch Laboratory (DoD / ARL) contract num-ber W911NF-12-C-0015.
The U.S. Governmentis authorized to reproduce and distribute reprintsfor Governmental purposes notwithstanding anycopyright annotation thereon.
Disclaimer: Theviews and conclusions contained herein are thoseof the authors and should not be interpreted asnecessarily representing the official policies orendorsements, either expressed or implied, ofIARPA, DoD/ARL, or the U.S. Government.Insightful discussions with Chiu and Rudnicky(2013) are also gratefully acknowledged.ReferencesDavid Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet Allocation.
Journal of MachineLearning Research, 3:993?1022.Guoguo Chen, Sanjeev Khudanpur, Daniel Povey, JanTrmal, David Yarowsky, and Oguz Yilmaz.
2013.Quantifying the value of pronunciation lexicons forkeyword search in low resource languages.
In Inter-national Conference on Acoustics, Speech and Sig-nal Processing (ICASSP).
IEEE.Berlin Chen.
2009.
Latent topic modelling of wordco-occurence information for spoken document re-trieval.
In Proceedings of the International Con-ference on Acoustics, Speech and Signal Processing(ICASSP), pages 3961?3964.
IEEE.Justin Chiu and Alexander Rudnicky.
2013.
Usingconversational word bursts in spoken term detection.In Proceedings of the 14th Annual Conference ofthe International Speech Communication Associa-tion, pages 2247?2251.
ISCA.Kenneth Church and William Gale.
1995.
Pois-son Mixtures.
Natural Language Engineering,1(2):163?190.Kenneth Church and William Gale.
1999.
Inverse Foc-ument Frequency (IDF): A measure of deviationsfrom Poisson.
In Natural Language Processing Us-ing Very Large Corpora, pages 283?295.
Springer.Kenneth Church.
2000.
Empirical estimates of adap-tation: the chance of two Noriegas is closer to p/2than p 2.
In Proceedings of the 18th Conferenceon Computational Linguistics, volume 1, pages 180?186.
ACL.Radu Florian and David Yarowsky.
1999.
Dynamicnonlocal language modeling via hierarchical topic-based adaptation.
In Proceedings of the 37th annualmeeting of the Association for Computational Lin-guistics, pages 167?174.
ACL.Mary Harper.
2011.
IARPA Solicitation IARPA-BAA-11-02.
http://www.iarpa.gov/solicitations_babel.html.Thomas Hofmann.
2001.
Unsupervised learningby probabilistic latent semantic analysis.
MachineLearning, 42(1):177?196.Bo-June Paul Hsu and James Glass.
2006.
Style &topic language model adaptation using HMM-LDA.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing.
ACL.Fred Jelinek.
1997.
Statistical Methods for SpeechRecognition.
MIT Press.Slava Katz.
1996.
Distribution of content words andphrases in text and language modelling.
NaturalLanguage Engineering, 2(1):15?59.Sanjeev Khudanpur and Jun Wu.
1999.
A maxi-mum entropy language model integrating n-gramsand topic dependencies for conversational speechrecognition.
In Proceedings of the InternationalConference on Acoustics, Speech, and Signal Pro-cessing (ICASSP), volume 1, pages 553?556.
IEEE.1324Reinhard Kneser and Volker Steinbiss.
1993.
On thedynamic adaptation of stochastic language models.In Proceedings of the International Conference onAcoustics, Speech, and Signal Processing (ICASSP),volume 2, pages 586?589.
IEEE.Roland Kuhn and Renato De Mori.
1990.
A cache-based natural language model for speech recogni-tion.
Transactions on Pattern Analysis and MachineIntelligence, 12(6):570?583.Yang Liu and Feifan Liu.
2008.
Unsupervised lan-guage model adaptation via topic modeling basedon named entity hypotheses.
In Proceedings of theInternational Conference on Acoustics, Speech andSignal Processing, (ICASSP), pages 4921?4924.IEEE.Welly Naptali, Masatoshi Tsuchiya, and Seiichi Naka-gawa.
2012.
Topic-dependent-class-based n-gramlanguage model.
Transactions on Audio, Speech,and Language Processing, 20(5):1513?1525.NIST.
2006.
The Spoken Term Detection (STD)2006 Evaluation Plan.
http://www.itl.nist.gov/iad/mig/tests/std/2006/docs/std06-evalplan-v10.pdf.
[Online;accessed 28-Feb-2013].Daniel Povey, Arnab Ghoshal, Gilles Boulianne, LukasBurget, Ondrej Glembek, Nagendra Goel, MirkoHannemann, Petr Motlicek, Yanmin Qian, PetrSchwarz, et al 2011.
The Kaldi speech recogni-tion toolkit.
In Proceedings of the Automatic SpeechRecognition and Understanding Workshop (ASRU).Xing Wei and W Bruce Croft.
2006.
LDA-based doc-ument models for ad-hoc retrieval.
In Proceedingsof the ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 178?185.ACM.1325
