Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1405?1410,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsDependency language models for sentence completionJoseph GubbinsComputer LaboratoryUniversity of Cambridgejsg52@cam.ac.ukAndreas VlachosComputer LaboratoryUniversity of Cambridgeav308@cam.ac.ukAbstractSentence completion is a challenging seman-tic modeling task in which models mustchoose the most appropriate word from agiven set to complete a sentence.
Althougha variety of language models have been ap-plied to this task in previous work, none of theexisting approaches incorporate syntactic in-formation.
In this paper we propose to tacklethis task using a pair of simple language mod-els in which the probability of a sentence isestimated as the probability of the lexicalisa-tion of a given syntactic dependency tree.
Weapply our approach to the Microsoft ResearchSentence Completion Challenge and show thatit improves on n-gram language models by 8.7percentage points, achieving the highest accu-racy reported to date apart from neural lan-guage models that are more complex and ex-pensive to train.1 IntroductionThe verbal reasoning sections of standardised testssuch as the Scholastic Aptitude Test (SAT) fea-ture problems where a partially complete sentenceis given and the candidate must choose the wordor phrase from a list of options which completesthe sentence in a logically consistent way.
Sen-tence completion is a challenging semantic mod-elling problem.
Systematic approaches for solvingsuch problems require models that can judge theglobal coherence of sentences.
Such measures ofglobal coherence may prove to be useful in variousapplications, including machine translation and nat-ural language generation (Zweig and Burges, 2012).Most approaches to sentence completion employlanguage models which use a window of immedi-ate context around the missing word and choose theword that results in the completed sentence with thehighest probability (Zweig and Burges, 2012; Mnihand Teh, 2012).
However, such language modelsmay fail to identify sentences that are locally co-herent but are improbable due to long-range syntac-tic/semantic dependencies.
Consider, for example,completing the sentenceI saw a tiger which was really very ...with either fierce or talkative.
A language modelrelying on up to five words of immediate contextwould ignore the crucial dependency between themissing word and the noun tiger.In this paper we tackle sentence completion us-ing language models based on dependency gram-mar.
These models are similar to standard n-gramlanguage models, but instead of using the linear or-dering of the words in the sentence, they generatewords along paths in the dependency tree of the sen-tence.
Unlike other approaches incorporating syntaxinto language models (e.g., Chelba et al 1997), ourmodels are relatively easy to train and estimate, andcan exploit standard smoothing methods.
We applythem to the Microsoft Research Sentence Comple-tion Challenge (Zweig and Burges, 2012) and showan improvement of 8.7 points in accuracy over n-gram models, giving the best results to date for anymethod apart from the more computationally de-manding neural language models.1405Figure 1: Dependency tree example2 Unlabelled Dependency LanguageModelsIn dependency grammar, each word in a sentence isassociated with a node in a dependency tree (Figure1).
We define a dependency tree as a rooted, con-nected, acyclic directed graph together with a map-ping from the nodes of the tree to a set of gram-matical relation labels R. We define a lexicaliseddependency tree as a dependency tree along with amapping from the vertices of the tree to a vocabularyV .We seek to model the probability distribution ofthe lexicalisation of a given dependency tree.
Wewill use this as a language model; we neglect thefact that a given lexicalised dependency tree cancorrespond to more than one sentence due to vari-ations in word order.
Let ST be a lexicalised de-pendency tree, where T is the unlexicalised tree andlet w1w2 .
.
.
wm be an ordering of the words corre-sponding to a breadth-first enumeration of the tree.In order for this representation to be unique, whenwe parse a sentence, we will use the unique breadth-first ordering where the children of any node appearin the same order as they did in the sentence.
Wedefine w0 to be a special symbol denoting the rootof the tree.
We denote the grammatical relation be-tween wk and its parent by gk ?
R.We apply the chain rule to the words in the tree inthe order of this breadth-first enumeration:P[ST |T ] =m?i=1P[wi|(wk)i?1k=0, T ] (1)Given a word wi, we define the ancestor sequenceA(w) to be the subsequence of (wk)i?1k=0 describ-ing the path from the root node to the parent ofw, where each element of the sequence is the par-ent of the next element.
For example in Figure 1,A(w8) = (w0, w1, w3).
We make the following twoassumptions:?
that each word wi is conditionally independentof the words outside of its ancestor sequence(wk)i?1k=0?A(wi)c, given the ancestor sequenceA(wi);?
that the words are independent of the labels(gk)mk=1.Using these assumptions, we can write the probabil-ity as:P[ST |T ] =m?i=1P[wi|A(wi)] (2)Given a training data corpus consisting of sen-tences parsed into dependency trees, the maximumlikelihood estimator for the probability P[wi|A(wi)]is given by the proportion of cases where the ances-tor sequence A(wi) was followed by wi.
Let C(?)
bethe count of the number of observations of a patternin the corpus.
We haveP?
[wi|A(wi)] =C((A(wi), wi))?w?V C((A(wi), w))(3)As is the case for n-gram language models, we can?thope to observe all possible sequences of words nomatter how big the corpus.
To deal with this datasparsity issue, we take inspiration from n-gram mod-els and assume a Markov property of order (N ?1):P[w|A(w)] = P[w|A(N?1)(w)] (4)where A(N?1)(w) denotes the sequence of up to(N ?
1) closest ancestors of w.The maximum likelihood estimator for this prob-ability is:P?
[wi|A(N?1)(wi)] =C((A(N?1)(wi), wi))?w?V C((A(N?1)(wi), w))We have arrived at a model which is quite similarto n-gram language models.
The main difference1406is that each word in the tree can have several chil-dren, while in the n-gram models it can only be fol-lowed by one word.
Thus the sum in the denomina-tor above does not simplify to the count of the ances-tor sequence in the way that it does for n-gram lan-guage models.
However, we can calculate and storethe denominators easily during training, so that wedo not need to sum over the vocabulary each time weevaluate the estimator.
We refer to this model as theorder N unlabelled dependency language model.As is the case for n-gram language models, evenfor low values of N, we will often encounter se-quences (A(N?1)(w), w) which were not observedin training.
In order to avoid assigning zero prob-ability to the entire sentence, we need to use asmoothing method.
We can use any of the smooth-ing methods used for n-gram language models.
Forsimplicity, we use stupid backoff smoothing (Brantset al 2007).3 Labelled Dependency Language ModelsWe assumed above that the words are generated in-dependently from the grammatical relations.
How-ever, we are likely to ignore valuable information indoing so.
To illustrate this point, consider the fol-lowing pair of sentences:You ate an applensubjdobjdetAn apple ate youdet nsubj dobjThe dependency trees of the two sentences arevery similar, with only the grammatical relations be-tween ate and its arguments differing.
The unla-belled dependency language model will assign thesame probability to both of the sentences as it ig-nores the labels of grammatical relations.
In orderto be able to distinguish between them, the natureof the grammatical relations between the words inthe dependency tree needs to be incorporated in thelanguage model.
We relax the assumption that thewords are independent of the labels of the parse tree,assuming instead the each word is conditionally in-dependent of the words and labels outside its ances-tor path given the words and labels in its ancestorpath.
We define G(wi) to be the sequence of gram-matical relations between the successive elements of(A(wi), wi).
G(wi) is the sequence of grammaticalrelations found on the path from the root node towi.
For example, in Figure 1, G(w8) = (g1, g3, g8).With our modified assumption we have:P[ST |T ] =m?i=1P[wi|A(wi), G(wi)] (5)Once again we apply a Markov assumption.Let G(N?1)(w) be the sequence of grammat-ical relations between successive elements of(A(N?1)(w), w).
With an (N ?
1)th order Markovassumption, we have:P[ST |T ] =m?i=1P[wi|A(N?1)(wi), G(N?1)(wi)]The maximum likelihood estimator for the probabil-ity is once again given by the ratio of the counts oflabelled paths.
We refer to this model as the orderN labelled dependency language model.4 Dataset and Implementation DetailsWe carried out experiments using the MicrosoftResearch Sentence (MSR) Completion Challenge(Zweig and Burges, 2012).
This consists of a setof 1,040 sentence completion problems taken fromfive of the Sherlock Holmes novels by Arthur Co-nan Doyle.
Each problem consists of a sentencein which one word has been removed and replacedwith a blank and a set of 5 candidate words to com-plete the sentence.
The task is to choose the can-didate word which, when inserted into the blank,gives the most probable complete sentence.
The setof candidates consists of the original word and 4imposter words with similar distributional statistics.Human judges were tasked with choosing imposterwords which would lead to grammatically correctsentences and such that, with some thought, the cor-rect answer should be unambiguous.
The trainingdata set consists of 522 19th century novels fromProject Gutenberg.
We parsed the training data us-ing the Nivre arc-eager deterministic dependencyparsing algorithm (Nivre and Scholz, 2004) as im-plemented in MaltParser (Nivre et al 2006).
Wetrained order N labelled and unabelled dependency1407I saw a tiger which was really verya.
fierceb.
talkativeI saw a tiger which was really very fierceROOTP[?fierce?]
= P[saw|ROOT]?
P[I|ROOT, saw]?
P[tiger|ROOT, saw]?
P[a|saw, tiger]?
P[fierce|saw, tiger]?P[which|tiger, fierce]?
P[was|tiger, fierce]?
P[really|tiger, fierce]?
P[very|tiger, fierce]PARSEEVALUATE PROBABILITYFigure 2: Procedure for evaluating sentence completion problemsN Unlab-SB Lab-SB Ngm-SB Ngm-KN2 43.2% 43.0% 28.1% 27.8%3 48.3% 49.8% 38.5% 38.4%4 48.3% 50.0% 40.8% 41.1%5 47.4% 49.9% 41.3% 40.8%Table 1: Summary of results for Sentence Completionlanguage models for 2 ?
N ?
5.
Words whichoccured fewer than 5 times were excluded from thevocabulary.
In order to have a baseline to compareagainst, we also trained n-gram language modelswith Kneser-Ney smoothing and stupid backoff us-ing the Berkeley Language Modeling Toolkit (Paulsand Klein, 2011).To test a given language model, we calculated thescores it assigned to each candidate sentence andchose the completion with the highest score.
Forthe dependency language models we parsed the sen-tence with each of the 5 possible completions andcalculated the probability in each case.
Figure 2 il-lustrates an example of this process for the order 3unlabelled model.5 ResultsTable 1 summarises the results.
Unlab-SB is the or-der N unlabelled dependency language model withStupid Backoff, Lab-SB is the order N labelleddependency language model with Stupid Backoff,Ngm-SB is the n-gram language model with StupidBackoff and Ngm-KN is the interpolated Kneser-Ney smoothed n-gram language model.Both of the dependency language models outper-fomed the n-gram language models by a substantialMethod Accuracyn-grams (Various) 39% - 41%Skip-grams (Mikolov) 48%Unlabelled Dependency Model 48.3%Average LSA (Zweig) 49%Labelled Dependency Model 50.0%Log-bilinear Neural LM (Mnih) 54.7%Recurrent Neural LM (Mikolov) 55.4%Table 2: Comparison against previous resultsmargin for all orders considered.
The best result wasachieved by the order 4 labelled dependency modelwhich is 8.7 points in accuracy better than the best n-gram model.
Furthermore, the labelled dependencymodels outperformed their unlabelled counterpartsfor every order except 2.Comparing against previous work (Table 2), theperformance of our n-gram baseline is slightly betterthan the accuracy reported by other authors (Mnihand Teh, 2012; Zweig et al 2012) for models of thistype.
The performance of the labelled dependencylanguage model is superior to the results reportedfor any single model method, apart from those rely-ing on neural language models (Mnih and Teh, 2012;Mikolov et al 2013) .
However the superior perfor-mance of neural networks comes at the cost of longtraining times.
The best result achieved in Zweig etal.
(2012) using a single method was 49% accuracywith a method based on LSA.
Mikolov et al(2013)also reported accuracy of 48% for a method calledskip-grams, which uses a log-linear classifier to pre-dict which words will appear close to each other insentences.14086 Related Work and DiscussionThe best-known language model based on depen-dency parsing is that of Chelba et al(1997).
Thismodel writes the probability in the familiar left-to-right chain rule decomposition in the linear orderof the sentence, conditioning the probability of thenext word on the linear trigram context, as well assome part of the dependency graph information re-lating to the words on its left.
The language mod-els we propose are far simpler to train and compute.A somewhat similar model to our unlabelled depen-dency language model was proposed in Graham andvan Genabith (2010).
However they seem to haveused different probability estimators which ignorethe fact that each node in the dependency tree canhave multiple children.
Other research on syntac-tic language modelling has focused on using phrasestructure grammars (Pauls and Klein, 2012; Char-niak, 2001; Roark, 2001; Hall and Johnson, 2003).The linear complexity of deterministic dependencyparsing makes dependency language models such asours more scalable than these approaches.The most similar task to sentence completion islexical substitution (McCarthy and Navigli, 2007).The main difference between them is that in the lat-ter the word to be substituted provides a very im-portant clue in choosing the right candidate, whilein sentence completion this is not available.
An-other related task is selectional preference modeling(Se?aghdha, 2010; Ritter et al 2010), where the aimis to assess the plausibility of possible syntactic ar-guments for a given word.The dependency language models described inthis paper assign probabilities to full sentences.
Lan-guage models which require full sentences can beused in automatic speech recognition (ASR) and ma-chine translation (MT).
The approach is to use a con-ventional ASR or MT decoder to produce an N-bestlist of the most likely candidate sentences and thenre-score these with the language model.
This wasdone by Chelba et al(1997) for ASR using a de-pendency language model and by Pauls and Klein(2011) for MT using a PSG-based syntactic lan-guage model.7 ConclusionWe have proposed a pair of language models whichare probabilistic models for the lexicalisation of agiven dependency tree.
These models are simpleto train and evaluate and are scalable to large datasets.
We applied them to the Microsoft ResearchSentence Completion Challenge.
They performedsubstantially better than n-gram language models,achieving the best result reported for any singlemethod except for the more expensive and complexto train neural language models.AcknowledgmentsAndreas Vlachos is funded by the EuropeanCommunity?s Seventh Framework Programme(FP7/2007-2013) under grant agreement no.270019 (SPACEBOOK project www.spacebook-project.eu).
The authors would like to thank Dr.Stephen Clark for his helpful comments.ReferencesThorsten Brants, Ashok C Popat, Peng Xu, Franz J Och,and Jeffrey Dean.
2007.
Large Language Mod-els in Machine Translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, pages 858?867.
Association forComputational Linguistics.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proceedings of the 39th AnnualMeeting on Association for Computational Linguis-tics, pages 124?131.
Association for ComputationalLinguistics.Ciprian Chelba, David Engle, Frederick Jelinek, Vic-tor Jimenez, Sanjeev Khudanpur, Lidia Mangu, HarryPrintz, Eric Ristad, Ronald Rosenfeld, Andreas Stol-cke, et al1997.
Structure and performance of adependency language model.
In Proceedings of Eu-rospeech, volume 5, pages 2775?2778.Yvette Graham and Josef van Genabith.
2010.
Deep syn-tax language models and statistical machine transla-tion.
In Proceedings of the 4th Workshop on Syntaxand Structure in Statistical Translation, pages 118?126.
Coling 2010 Organizing Committee, August.Keith Hall and Mark Johnson.
2003.
Language mod-eling using efficient best-first bottom-up parsing.
InIEEE Workshop on Automatic Speech Recognition andUnderstanding, pages 507?512.
IEEE.1409Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word representa-tions in vector space.
arXiv preprint arXiv:1301.3781.Andriy Mnih and Yee W Teh.
2012.
A fast and sim-ple algorithm for training neural probabilistic languagemodels.
In Proceedings of the 29th International Con-ference on Machine Learning, pages 1751?1758.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of English text.
In Proceedings ofthe 20th International Conference on ComputationalLinguistics, page 64.
Association for ComputationalLinguistics.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
Malt-parser: A data-driven parser-generator for dependencyparsing.
In Proceedings of LREC, volume 6, pages2216?2219.Adam Pauls and Dan Klein.
2011.
Faster and SmallerN-Gram Language Models.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 258?267.
Association for Computational Lin-guistics.Adam Pauls and Dan Klein.
2012.
Large-scale syntac-tic language modeling with treelets.
In Proceedings ofthe 50th Annual Meeting of the Association for Com-putational Linguistics: Long Papers-Volume 1, pages959?968.
Association for Computational Linguistics.Alan Ritter, Oren Etzioni, et al2010.
A latent dirich-let alcation method for selectional preferences.
InProceedings of the 48th Annual Meeting of the Associ-ation for Computational Linguistics, pages 424?434.Association for Computational Linguistics.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.Diarmuid O Se?aghdha.
2010.
Latent variable modelsof selectional preference.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 435?444.
Association for Computa-tional Linguistics.Geoffrey Zweig and Chris JC Burges.
2012.
A challengeset for advancing language modeling.
In Proceedingsof the NAACL-HLT 2012 Workshop: Will We Ever Re-ally Replace the N-gram Model?
On the Future ofLanguage Modeling for HLT, pages 29?36.
Associa-tion for Computational Linguistics.Geoffrey Zweig, John C Platt, Christopher Meek,Christopher JC Burges, Ainur Yessenalina, and QiangLiu.
2012.
Computational approaches to sentencecompletion.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguistics:Long Papers-Volume 1, pages 601?610.
Associationfor Computational Linguistics.1410
