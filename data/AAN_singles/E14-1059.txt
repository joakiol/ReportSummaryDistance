Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 560?568,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsDiscriminating Rhetorical Analogies in Social MediaChristoph Lofi Christian Nieke Nigel CollierNational Inst.
of Informatics TU Braunschweig National Inst.
of Informatics2-1-2 Hitotsubashi,Chiyoda-ku, Tokyo101-8430, JapanM?hlenpfordtstr.
2338106 BraunschweigGermany2-1-2 Hitotsubashi,Chiyoda-ku, Tokyo101-8430, Japanlofi@nii.ac.jp nieke@ifis.cs.tu-bs.de       collier@nii.ac.jpAbstractAnalogies are considered to be one of the coreconcepts of human cognition and communica-tion, and are very efficient at encoding com-plex information in a natural fashion.
How-ever, computational approaches towards large-scale analysis of the semantics of analogies arehampered by the lack of suitable corpora withreal-life example of analogies.
In this paper wetherefore propose a workflow for discriminat-ing and extracting natural-language analogystatements from the Web, focusing on analo-gies between locations mined from travel re-ports, blogs, and the Social Web.
For realizingthis goal, we employ feature-rich supervisedlearning models which we extensively evalu-ate.
We also showcase a crowd-supportedworkflow for building a suitable Gold datasetused for this purpose.
The resulting system isable to successfully learn to identify analogiesto a high degree of accuracy (F-Score 0.9) byusing a high-dimensional subsequence featurespace.1 IntroductionAnalogies are one of the core concepts of humancognition (Hofstadter, 2001), and it has been sug-gested that analogical inference is  the ?thing thatmakes us smart?
(Gentner, 2003).
An analogy canbe seen as a pattern of speech leading to a cogni-tive process that transfers some high-level mean-ing from one particular subject (often called theanalogue or the source) to another subject, usuallycalled the target.
When using analogies, one em-phasizes that the ?essence?
of source and target issimilar, i.e.
their most discriminating and proto-typical processes and properties are perceived in asimilar way.The nature of analogies has been discussed andstudied since the ancient Greeks, however compu-tational approaches are still rather limited and intheir infancy.
One reason for this is that text cor-pora containing analogies are crucial to study thesyntactic and semantic patterns of analogies in or-der to make progress on automated understandingtechniques.
For example, to learn about their dis-tribution and the attribute-value pairs that arecompared.
However, to the best of our knowledge,no such corpus is freely available.
We will there-fore in this paper present a method for creatingsuch a corpus in an efficient fashion, and make ourcorpus available for further research efforts.As an example, consider this brief statement:?West Shinjuku (a Tokyo district) is like Lower Manhat-tan?
It allows readers who know New York, butnot Tokyo, to infer some of the more significantproperties of the unknown district (e.g., it is an im-portant business district, hosts the headquarters ofmany companies, features many skyscrapers,etc.).
However, automatically understanding anal-ogies is surprisingly hard due to the extensive do-main knowledge required in order to perform ana-logical reasoning.
For example, an analogy repos-itory containing such domain knowledge has toprovide information on which attributes of sourceand target are generally considered comparable.
Incontrast to Linked Open Data or typical ontolo-gies, such analogical knowledge is consensual, i.e.there is no undisputable truth to analogical infor-mation, but a statement can be considered ?good?analogical knowledge if its?
semantics are per-ceived similarly by enough people (Lofi & Nieke,2013).
For example, while many properties ofWest Shinjuku and Lower Manhattan are dissimi-lar, nonetheless most people will immediately rec-ognize dominant similarities.In order to build an analogy repositories, a largenumber of actual analogy statements reflecting thediversity of people?s opinions are required foranalysis.
In this paper, we make a start on this taskby proposing a workflow for reliably extractingsuch statements by using feature-rich supervised560learning models, and demonstrate its effectivenessfor analogies between different places.
Our contri-butions in this paper are as follows:?
First, we build a suitable Gold corpus for train-ing and testing supervised learning models, fo-cusing on analogies between places.
This cor-pus will be based upon content mined fromsearch engines and social media.?
We show the effectiveness, but also the chal-lenges of crowd-sourcing as a technique forscreening and refining potential Gold corpusdocuments.
This process results in multi-sen-tence text snippets containing an analogy ex-tracted from these documents.?
We design and evaluate supervised learningmodels with rich feature sets to recognize anal-ogy statements automatically, allowing us tosubstitute crowd-sourcing with automatedtechniques for further expanding the corpus.?
We extensively evaluate our models, and dis-cuss their strengths and shortcomings.2 Processing AnalogiesThere exist several approaches for modeling andcapturing the semantics of analogies, among themmany formal ones relying, for example, on struc-tural mapping (Gentner, 1983).
These types of ap-proaches aim at mapping characteristics and rela-tionships of source and target, usually relying onfactual domain knowledge given in propositionalnetworks.
One example typically used in this con-text is the Rutherford analogy ?Atoms are like the So-lar System?, which can be derived by outlining sim-ilarities between the nucleus and the sun, whichare both heavy masses in the center of their respec-tive system, and electrons and planets, which re-volve around the center attracted by a strong force(here, the coulomb force is analog to the gravita-tional force).
This model resulted in several theo-retical computational models (e.g.
(Gentner &Gunn, 2001)).The most extensively researched subset of analo-gies are 4-term analogies between two word pairs(mason, stone)::(carpenter, wood).
Here, processinganalogies boils down to measuring the relationalsimilarity of the word pairs, i.e.
a mason workswith stone as a carpenter works with wood.However, measuring the similarity between enti-ties or relationships is a difficult task.
While moststructure-mapping approaches rely on processingfacts, e.g.
as extracted from ontologies orknowledge networks, supporters of perceptualanalogies claim that this similarity has to be meas-ured on a high perceptional level (Chalmers,French, & Hofstadter, 1992), i.e.
there can be ananalogy if people perceive or believe relations orproperties to be as similar even if there are no hardfacts supporting it (Kant, 1790), or even whenfacts oppose it.
More formally, two entities ?
and?
can be seen as being analogous (written as ?
??)
when their relevant relationships and propertiesare perceived sufficiently similar (Lofi & Nieke,2013).
This type of consensual analogy is of highrelevance in natural communication (in fact, mostanalogies we discovered in our data are of thistype), but very hard to learn as there are no corporafor studying analogies readily available.
Further-more, this definition opens up other challenges:What are the relevant characteristics between twoentities?
When are they perceived as being simi-lar?
And when does an analogy hold true?With this work, we aim at paving the way for fu-ture research on this challenging set of problemsby providing a workflow for mining analogy ex-amples from the Web and Social Media.
To illus-trate this, consider the following example ex-tracted from our Gold corpus:?Tokyo, like Disneyland, is sterile.
It?s too clean andreally safe, which are admirable traits, but also unre-alistic.
Tokyo is like a bubble where people can livetheir lives in a very naive and enchanted way becausereal problems do not exist.?(No.
5310 in corpus)This perceptual analogy between Tokyo and Dis-neyland is hard to explain when only relying ontypical structured knowledge like Linked OpenData or ontologies, and thus requires specializeddata repositories which can be built up using real-world examples as provided by our approach.Unfortunately, actually detecting the use of ananalogies in natural text, a requirement for build-ing sufficiently large test corpora, is not an easytask, as there are only subtle syntactic and mor-phological differences between an analogy and asimple comparison.
These differences cannot begrasped by simple classification models.
For ex-ample, while many rhetorical analogies containphrases as ?is like?
or ?, like?, as for example in?West Shinjuku is like Lower Manhattan?
or ?Tokyo islike Disneyland as it is very sterile?
there is a plethoraof very similar sentences which do not express ananalogy (?Shinjuku is like this: ??
or ?Tokyo, like therest of Japan, ??).
These subtle differences, whichare hard to grasp with handcrafted patterns and areoften found in the surrounding context, can bemodeled by our approach as outlined in section 5.5613 Related WorkThere exist several works on the semantics ofanalogies from a cognitive, philosophical, or lin-guistic perspective, such as (Dedre Gentner, KeithJ.
Holyoak, & Boicho N. Kokinov, 2001),(Itkonen, 2005), or (Shelley, 2003).Hearst-like patterns (Hearst, 1992), which we useas a first and very crude filter during the construc-tion of the Gold dataset, have frequently been em-ployed in recent years, especially in the area of ex-tracting hyponyms, e.g., (Snow, Jurafsky, & Ng,2004) which also aims at learning new extractionpatterns based on word dependency trees.
But alsoapproaches for dealing with analogies are fre-quently based on patterns applied to text corpora.Most of these approaches are tailored for solvinggeneral analogy challenges given in a 4-term mul-tiple-choice format, and are usually evaluated onthe US-based SAT challenge dataset (part of thestandardized aptitude test for college admission).SAT challenges are in 4-term analogy form, e.g.
?ostrich is to bird AS a) cub is to bear OR b) lion is to cat?,and the focus of those approaches is on heuristi-cally assessing similarity of two given wordspairs, to find the statistically more plausible an-swer.
For example, (Bollegala, Matsuo, &Ishizuka, 2009), (Nakov & Hearst, 2008), or(Turney, 2008) approach this challenge by usingpattern-based Web search and subsequent analysisof the resulting snippets.
In contrast to these ap-proaches, we do not focus on word pair similarity,but given one entity, we aim at finding other enti-ties which are seen as analogous in a specific do-main (in our case analogies between locations andplaces).
Being focused on a special domain oftenrenders approaches relying on thesauri like Word-Net or CoreLex unusable, as many of the wordsrelevant to the domain are simply not contained.Closely related to analogy processing is the detec-tion of metaphors or metonyms, which are a spe-cial form of analogy.
Simplified, a metaphor is ananalogy between two entities with the additionalsemantics that one entity can substitute the otherand vice versa).
While early approaches to meta-phor identification relied on hand-crafted patterns(Wilks, 1978), newer ones therefore heavily ex-ploit the interchangeability of the entities (Beust,Ferrari, & Perlerin, 2003) or (Shutova, 2010), andcannot be used for general analogy processingwithout extensive adoption.
These approaches of-ten also rely on some reasoning techniques basedon thesauri, but also other approaches based on1 http://data.l3s.de/dataset/analogy-text-snippetsmining and corpus analysis became popular.
Forexample in (Shutova, Sun, & Korhonen, 2010) asystem is presented which, starting from a smallseed set of manually annotated metaphorical ex-pressions, is capable of harvesting a large numberof metaphors of similar syntactic structure from acorpus.Detecting analogies also has some similaritieswith relation extraction, e.g.
(Bunescu & Mooney,2006) using Subsequence Kernels.
However, thetask is slightly more difficult than simply miningfor a ?similar_to?
relation, which is addressed byour approach in section 5.4 Building the Gold DatasetAs the goal of this paper is to supply the tools forcreating a large corpus of analogies from the Web,we require a reliable mechanism for automaticallyclassifying if a text snippet contains an analogy ornot.
Such classification requires a Gold datasetwhich we construct in this section and which wemake available to the community for download1.As we expect the number of analogies in a com-pletely random collection of web documents to beextremely low, we first start by collecting a set ofweb documents that are likely to contain an anal-ogy by applying some easy-to-implement but ra-ther coarse techniques as follows:In order to obtain a varied set of text snippets (i.e.short excerpts from larger Web documents), wefirst used a Web search engine (Google SearchAPI) with simple Hearst-like patterns for crawlingpotentially relevant websites.
These patterns wereselected manually based on analysis of sampleWeb data by three experts.
In contrast to other ap-proaches relying on extraction patters, e.g.
(Turney, 2008) or (Bollegala et al., 2009), our pat-terns are semi-open, e.g.
?# * similar to * as?, where# is replaced by one of 19 major cities we used forcorpus extraction.
* is a wildcard, therefore onlyone entity of the analogy is fixed by the pattern.Each pattern is created by combining one base part(in this case, ?# * similar to *?)
with an extensionpart (?as?).
We used 17 different base parts, and 14different extensions, resulting in 238 different ex-traction patterns before inserting the city names.Using Web search, we initially obtained 109,121search results and used them to crawl 22,360 doc-uments, for which we extracted the text snippetssurrounding the occurrence of the pattern (2 pre-ceding and 2 succeeding sentences).
The intentionof our open Hearst-like patterns is to obtain a wide562variety of text snippets which are not limited tosimple analogy cases, so most snippets obtainedwill actually not be analogies at all.
Therefore, ad-ditional filtering is required to find those which doactually contain an analogy between places.
Un-like e.g.
(Turney, 2008) where patterns of the form?
[0..1] X [0..3] Y [0..1]?, with X and Y two given en-tities, are used, we chose a more general approachand filtered out all snippets not containing at leasttwo different locations (and hence no place anal-ogy, locations provided by Stanford CoreNLPNER tagger), which left 14,141 snippets.Since we lacked the means to manually classify allof these snippets as a Gold set, we randomly se-lected a subset of 8000 snippets, and performed acrowd-sourcing based filtering to detect potentialanalogies, as described in the following.Crowd-Sourcing-Based FilteringUnder certain circumstances, crowd-sourcing canbe very effective for handling large tasks requiringhuman intelligence without relying on expensiveexperts.
In contrast to using expert annotators,crowd-workers are readily and cheaply availableeven for ad-hoc tasks.
In this paper, we used mi-cro-task crowd-sourcing, i.e.
a central platformlike for example Amazon Mechanical Turk2  orCrowdFlower3 assigns small tasks (called HITs,human-intelligence tasks) to workers for monetarycompensation.
HITs usually consist of multiplework units taking only a few minutes to process,and therefore pay few cents.Crowd-sourcing has been shown to be effectivefor language processing related tasks, e.g.
in(Snow, O?Connor, Jurafsky, & Ng, 2008)  it wasused to annotate text corpora, and the authorsfound that for this task, the combination of threecrowd judgments roughly provides the quality ofone expert worker.
However, the quality can varydue to potential incompetence and maliciousnessof workers, making quality control mandatory.The two basic tools for quality control in crowd-sourcing are majority votes and Gold units, whichare both used in our process.
Gold units are tasksfor which the correct answer is known, and theyare transparently mixed into normal HITs distrib-uted to workers.
If workers repeatedly provide anincorrect judgment for gold units, they are consid-ered malicious, are not paid, and their judgmentsare excluded from the results.Therefore, we continued to classify the selected8,000 snippets using 90 gold units.
5 snippets aregrouped within each HIT, for which we pay USD2 https://www.mturk.com/$0.04.
For each snippet, 3 judgments are elicited.In total, 336 workers participated in categorizing87 snippets on average (some top contributors cat-egorized up to 1,975 snippets).
As a result 895snippets are classified as containing an analogywith a confidence of over 90% (confidence iscomputed as a weighted majority vote of workerjudgments and worker reliability; with worker re-liability resulting from workers failing or passinggold units in previous tasks).A brief manual inspection showed that these re-sults cannot be trusted blindly (a correctness of78% compared to an expert judgment was meas-ured in a small sample), so we performed an expertinspection on all potential analogy snippets, revis-ing the crowd judgments where necessary.
Fur-thermore, we manually tagged the names of theanalogous locations.
This resulted in 542 snippetswhich are now manually judged as analogies and353 snippets that were manually judged as not be-ing an analogy.
For this task, worker performanceis extremely asymmetrical as it is much easier forcrowd-workers to reach an agreement for negativeexamples than for positive ones, and there were3,023 snippets classified as no analogies with100% confidence.
This intuition was supported bya short evaluation in which we sampled 314(10.3%) random snippets from this set and foundnone that had been misclassified.
Therefore, thenegative examples of our Gold set consist of thesnippets manually re-classified by our expert an-notators, and the snippets which had been classi-fied with 100% confidence by the crowd-workers.This leaves out 4,082 snippets for which no clearconsensus could be reached, and which are thusexcluded from the Gold set.5 Classifiers and Feature ExtractionUsing crowd-sourcing for finding analogy state-ments is a tedious and still quite expensive task.Therefore, we aim at automating the processes ofdetecting analogies in a given text snippet by de-signing multiple rich feature sets for machinelearning-based classification models, allowing usto discover new analogies quicker and cheaper.Dataset DescriptionOur complete Gold dataset of 3,918 text snippetsshows a ratio of positive to negative examples ofroughly 1:8.
For training and evaluation, we per-form four stratified random selections on the Goldset to obtain 4 training sets with 2/3 of the overall3 http://crowdflower.com/563size (2,611), and respective test sets with 1/3 size(1,307).
In each set, the original ratio between pos-itive example (analogies) and negative examples(not analogies) is retained.
We prefer this ap-proach over n-fold cross-validation as some of ourmodels are expensive to train.All snippets in the Gold set consist of 5 sentences,with 105 words per snippet on average.
This aver-age does not significantly vary between positivelyand negatively classified snippets (94 vs. 106).The overall vocabulary contains 31,878 uniquewords, with 6,960 words in the positive and30,234 in the negative subset.
5,316 of thesewords are shared between both sets (76% of thosein the Gold set).
This observation implies that thelanguage in our snippets is highly varied and farfrom saturated (for the significantly smaller posi-tive set, 12.84 new words per snippet are added tothe vocabulary on average, while for the largernegative subset, this value only drops to 8.95).This situation looks similar for locations, whichplay a central role in this classification task: theoverall number of different locations encounteredin all snippets is 2,631, with 0.86 new locationsper snippet in the positive set and 0.73 in the neg-ative set.
On average, there are 3.18 locationsmentioned in a given snippet, again with no sig-nificant differences in the positive and negativesubset (3.67 vs. 3.10).
Please refer to Table 1 forexhaustive statistics.Unigram (Bag-of-Word) Feature ModelAs our evaluation baseline, we use a straight-for-ward unigram (bag-of-word) feature model fortraining a support vector machine.
No stop wordsare removed, and the feature vectors are normal-ized to the average length of training snippets.Furthermore, we only retain the 5000 most fre-quent features, and skip any which occur only in asingle snippet.
For this experiments (and all otherlater experiments using a SVM), we used theLibSVM implementation (Chang & Lin, 2011)with a linear kernel due to the size of the featurespace.N-Gram-based Feature ModelOur first approach to increasing classificationquality of the baseline is expanding the featurespace to also include n-grams.
We tested differentversions of this model with lexical word-level n-grams, part-of-speech n-grams, and both of themsimultaneously.
In all cases, we include n-gramswith a length of 1 to 4 words, and similar to the4 http://nlp.stanford.edu/software/corenlp.shtmlbaseline, the top-5000 features are retained andvalues are normalized to the training snippetlength, with a minimal frequency of 2.
The re-quired part-of-speech labels are obtained by usingthe Stanford CoreNLP library4.The three resultingfeature models have been trained and evaluatedwith three classification algorithms which areknown to provide good performance in similarclassification tasks: a support vector machine clas-sifier (as in 5.2), a Na?ve Bayes classifier (from theWeka library5), and Weka?s J48 implementationof the C4.5 classifier (Quinlan, 1993) (with prun-ing confidence 0.25 and min.
leaf distance 2).Shortest Path Feature ModelIn this subsection we design the Shortest Path fea-ture model, a model aiming at exploiting some ofthe specific properties of place analogies.
By def-inition, only text snippets featuring two differentplaces can be a place analogy.
The Shortest Pathmodel furthermore assumes that both these loca-tions occur in a single sentence (which is tested in6.3), and that there is a meaningful lexical orgrammatical dependency between these occur-rences.
For actually building our feature space, werely on typed dependency parses (Marneffe,MacCartney, & Manning, 2006) of the snippets,and extract the shortest path in the resulting de-pendency tree between both locations (also usingStanford CoreNLP).
This path represents the col-lapsed and propagated dependencies between bothlocations, i.e.
basic tokens as ?on?
or ?by?
are inte-grated in the edge labels and don?t appear asnodes.
We considered three variations of this ap-proach: paths built using lexical labels, path withpart-of-speech labels, and a combination of both.During the construction of our Gold set, we man-ually annotated the two relevant places for allanalogies.
Therefore this approach can be applied5 http://www.cs.waikato.ac.nz/ml/weka/Table 1: Characteristics of Gold Datacharacteristic all  positive negative# of snippets 3,918 542 3,376# of snippets intraining set2,611 361 2,250# of snippets in testset1,307 181 1,126vocabulary size 31,878 6,960 30,234voc.
/ #snippets 8.14 12.84 8.95locationvocabulary size2,631 468 2,459loc.voc.
/ #snipts.
0.67 0.86 0.73# words / s.       + 105 94 106# locations / s. 3.18 3.67 3.10+ #/s.
: average count per snippet564directly for positive training examples.
However,for negative snippets, no relevant locations havebeen annotated.
Hence, for all negative snippets intraining and all snippets in the test set, we assumethat all locations which appear in a snippet (as de-termined by a NER tagger) are relevant, and weextract all shortest paths between any of them.
Onaverage this results in 5.6 paths extracted from anygiven snippet.
The extracted paths are generalizedby replacing the locations with a generic, and thefinal feature model results from constructing a bi-nary feature representing whether a given path oc-curs or not.As with the n-gram-based feature model, we trainand evaluate SVM, Na?ve Bayes, and J48 classifi-ers with our feature vector (parameters as in 5.3).Please note that building this model is computa-tionally significantly more expensive than the n-gram-based approach as it requires named entityrecognition, and typed dependency parsing (we re-quired roughly 30 minutes per training / test set onour Intel i7 laptop).Subsequence Pattern Feature ModelBasically, this approach aims at creating some-thing similar to the most common sub forests ofall snippets, or skip-grams (Guthrie, Allison, Liu,Guthrie, & Wilks, 2006), i.e.
results can be seen asa hybrid between ?tree patterns?
(as e.g.
the Short-est Path) and n-grams.
The intention is to avoid theproblem of overly local patterns, allowing the pat-terns to work even in the presence of fill words andsubsequences added to a sentence.
For this, we uti-lize the PrefixSpan algorithm (Pei et al., 2001) todetect common, reappearing subsequence in thetraining set, i.e.
sequences of words that appear ina given order, ignoring anything in-between.
Incontrast to the shortest path approach, this modelfocuses on multiple sentences simultaneously, andtherefore is a significant contribution over state-of-the-art techniques.As before, we used lexical, part-of-speech, andcombined features.
The general idea of this ap-proach is to use the PrefixSpan algorithm to minesubsequence patterns from positive gold snippets(the primitives), and use these as binary features ina classification step, for which we trained threeclassifiers as described in 5.3.In case of the lexical labels, we use the PrefixSpanalgorithm to return all subsequences that appear atleast 10 times (this value is dependent on charac-teristics of the dataset and has to be tuned manu-ally) in the relevant part (i.e.
the minimal set ofconsecutive sentences that include both locations)of the positive training set snippets.
Depending onthe training set used, this resulted in about 40kcommon subsequences.
To avoid unspecific pat-terns, we filtered out all sequences that did notcontain both locations, which reduces the numberto about 15k in average.
We then replaced the ac-tual locations with a generic, which allows build-ing a regular expression from the pattern that al-lows any number of words in-between each part ofthe sequence.
Before applying a pattern to an un-known snippet, we also replace all (NER tagged)locations with a generic.
For example, ?LOCA-TION * is * like * LOCATION?
would match ?Tokyois also a lot like Seoul?
using regular expressions.The part-of-speech version is similar to the lexicalone, but tries to create more generic and open pat-terns by mining subsequences from the POS rep-resentation of the relevant snippet part.
For filter-ing, all patterns that do not contain two ?NNP?
tagsand appear less than 60 times are removed (the fil-ter threshold is increased as POS patterns are moregeneric).
We get around 60k to 80k patterns be-fore, and ~10k to 20k primitive patterns after fil-tering which are used as binary features.
Finally,we merged lexical and POS patterns and thus al-lowed the classifiers to use any of the features.
Astrongly truncated version of a rule tree created us-ing J48 classification with POS subsequence prim-itives is shown in Figure 1.
Please note that due tothe open nature of the primitives and their inde-pendence, combining several of them in a featurevector will create extremely complex patternsquite easily.
Even a vector that contains only thepatterns *A*B* and *A*C* would create matchesfor ABC, ACB, ABAC, ACAB, AACB, AABC and allowany kind of additional fill words in between.
How-ever, this approach is computationally expensive(testing/training was around 6 hours on average).6 EvaluationIn the following we evaluate the effectiveness ofour analogy classifiers and models.
We primarilyrely on the informedness measure (Powers, 2007)for quantifying performance.
In contrast to usingonly precision, recall, or F-Measure, it respects allFigure 1: Example Classification Tree*NNP*NNP* * *NNP*NNP*NN*NN* **NNP*NNP*.
* **NN*NN*NN*NNP*IN*NNP* *NOT-ANALOGY(243.0/8.0 correct)ANALOGY(281.0/3.0 correct)NO MATCHNO MATCHMATCHMATCHNO MATCHNOT-ANALOGY(1938.0/22.0 correct)*NNP*IN*NNP*NN* *MATCHMATCH565error types, false positives (FP) and false negatives(FN), but also true positives (TP) and true nega-tives (TN), making it a fair and unbiased measurefor classification.
Furthermore, it compensates bi-ased class distributions in datasets, e.g.
as in ourdataset the ratio of positive to negative snippets is1:8, even an ?always no?
classifier has a correct-ness of 85%, but will have an informedness of 0.Informedness is given by:????????????
= ??????
+ ?????????
?
1with: ??????
=????+?
?and  ?????????????
=????+?
?In Table 2, we provide the average informedness,the percentage of correctly classified snippets, F-measure, precision, recall, and inverse recall (truenegative rate) for all experiments.
A discussion ofthese results follows in the next section.Classifier PerformanceOur straight-forward baseline approach, using uni-grams and an SVM classifier results in a reasona-ble informedness of 0.5.
Expanding the featurespace to lexical n-grams slightly increases perfor-mance, while using more generic part-of-speechn-grams results in weaker results.
Combiningboth, however, generally leads to better classifica-tion results.
When comparing different classifica-tion algorithms, it shows that SVMs are most in-formed when classifying n-grams-based features,followed by J48.
Both techniques will result inmoderate recall values around 0.5 and precisionaround 0.6, with a rather high true negative (inv.Recall rate) of 0.9.
This changes quite signifi-cantly for Na?ve Bayes, which is more likely toclassify a snippet as positive, therefore leading tohigher recall values, but also much lower in-formedness, precision, and inverse recall.
Conse-quently, the best approach is using SVM with alexical-POS combined feature space, leading to aninformedness of 0.55.Shortest Path was intended to achieve higher pre-cision results by exploiting additional semanticknowledge of the underlying problem.
Unfortu-nately, it performs poorly if not used with a SVM,but even then it achieves inferior overall resultsthan the best n-gram approach (informedness 0.4).This is due to some of its necessary assumptionsnot holding true (see section 6.4).In contrast, our subsequence-based modelachieves a higher informedness score of 0.85 and0.87 in the best cases.
While the lexical variantsperform not as well, the more generic variants us-ing POS allow for reliable classification.
Combin-ing the lexical and the POS features does unfortu-nately not increase the performance further (quitecontrary, the scores generally decrease for com-bined features).
A possible explanation is overfit-ting caused by the increased feature space.Significance TestsAs our Gold set is of limited size, we performedstatistical tests to investigate whether the differ-ences reported in the last subsection are actuallyTable 2: Classifier Result Comparison with respect to the Gold classificationClassifier Informed.
% Correct F-Measure Precision Recall Inv.
RecallAlways No  0.00 0.85 - - 0 1Unigram Lexical SVM 0.50 0.88 0.59 0.63 0.55 0.94n-Gram Lexical SVM 0.53 0.89 0.63 0.68 0.58 0.95n-Gram  POS SVM 0.42 0.87 0.52 0.58 0.48 0.94n-Gram Lex & POS SVM 0.55 0.90 0.65 0.73 0.59 0.96n-Gram  Lexical Na?ve Bayes 0.33 0.48 0.36 0.22 0.93 0.41n-Gram  POS Na?ve Bayes 0.38 0.61 0.39 0.26 0.81 0.58n-Gram  Lex & POS Na?ve Bayes 0.48 0.75 0.47 0.35 0.73 0.75n-Gram  Lexical J48 (C4.5) 0.45 0.87 0.55 0.58 0.52 0.93n-Gram  POS J48 (C4.5) 0.37 0.85 0.47 0.51 0.45 0.92n-Gram  Lex & POS J48 (C4.5) 0.44 0.87 0.54 0.57 0.52 0.93Shortest Path SVM 0.40  0.90 0.53 0.71 0.43 0.97Shortest Path Na?ve Bayes 0.27  0.87 0.40 0.55 0.32 0.96Shortest Path J48 (C4.5) 0.26 0.89 0.40 0.77 0.27 0.99Subseq.
Lexical  SVM 0.39 0.87 0.24 0.51 0.46 0.94Subseq.
Lexical Na?ve Bayes 0.53 0.79 0.49 0.36 0.73 0.80Subseq.
Lexical J48 (C4.5) 0.34 0.86 0.44 0.47 0.41 0.93Subseq.
POS SVM 0.84 0.97 0.87 0.89 0.85 0.98Subseq.
POS Na?ve Bayes 0.72 0.81 0.57 0.41 0.93 0.79Subseq.
POS J48 (C4.5) 0.85 0.97 0.90 0.93 0.86 0.99Subseq.
Lex & POS SVM 0.77 0.95 0.83 0.87 0.79 0.98Subseq.
Lex & POS Na?ve Bayes 0.70 0.80 0.56 0.41 0.91 0.79Subseq.
Lex & POS J48 (C4.5) 0.87 0.97 0.90 0.92 0.88 0.98566significant or result from noise.
We used an in-stance-based test relying on the theory of approx-imate randomization (Noreen, 1989)6 to perform100k iterations of randomized testing of the hy-pothesis that the pairwise performance differencesof selected approaches are actually significant (ex-cluding those pairs where the significance is obvi-ous).
First, we compared our baseline, lexical uni-grams with SVM to using lexical n-grams to testwhether using n-grams actually contributed to thequality, and found the difference to be significant(sign-test p<0.024).
However, for SVM-basedclassification, the higher reported performance foralso including POS features in addition to lexicaln-grams could not be shown to be significant(p>0.4).
Finally, we tested if the choice betweenSVM or J48 is significant for our two best subse-quence-based approaches, and confirmed thisclearly (sign-test: p<0.006).
According to the re-ported subsequence results, combining lexical fea-tures with part-of-speech features counter-intui-tively lowers the performance when using SVM orNa?ve Bayes and the positive effect on J48 wasshown to be insignificant (p>0.68).
Therefore, weassume that lexical features don?t make a substan-tial contribution when POS features are present.Error AnalysisFor only 2,845 of all 3,918 snippets, two differentlocations (regardless of their relevance to the anal-ogy) are mentioned in the same sentence.
This se-verely hampers the effectiveness of our ShortestPath approach, which is limited to cases whereboth locations appear in the same sentence.
Thosesnippets (344 on average / test set) are then classi-fied as ?not analogy?, decreasing the recall.
Theoverall impact of this shortcoming is still low, asonly 4% of these snippets are analogies.
Our otherapproaches are unaffected.Interestingly, we see what one might call the ?in-verse problem?
when using the other two models(n-gram and subsequence) that search for the pres-ence of certain terms or sequences, but do not ex-plicitly connect them to the locations.
They tendto create false positives by detecting statementsthat contain 2 locations and an analogy, but notbetween these locations.
Consider:?They say New York is the City of Dreams.
I say Lon-don is the theatre where it all happens?(No.
5627 in corpus).Another source for false positives is when an anal-ogy is not stated, but is requested:6 Implementation at: http://www.clips.ua.ac.be/scripts/art?What districts of Paris are similar to Shepherd'sBush or Ealing (both in West London??(No.
8505 in corpus)7 Summary and OutlookWe demonstrated approaches for discriminatinganalogy statements from the Web and Social Me-dia.
Our two major contributions are: a) We cre-ated a Gold dataset containing 3,918 example textsnippets, of which 542 are positively identified asanalogies.
This dataset was extracted from 109kpotential documents resulting from a Web searchwith manually crafted Hearst-like patterns.
Thedataset was consequently refined by using a com-bination of filters, crowd-sourcing, and expertjudgments.
We also discussed the challenges aris-ing from a crowd-sourcing in such a setting.b) Us-ing the Gold dataset, we designed and evaluated aset of machine learning models for classifying textsnippets automatically with respect to containingplace analogies.
Besides more traditional n-grambased models, we also designed novel models re-lying on feature spaces resulting from shortestpath analysis of the typed dependency tree, andhigh-dimensional feature spaces built from fil-tered subsequence patterns mined using the Pre-fixSpan algorithm.
In an exhaustive evaluation,the latter approach, which bridges between lexicaland structural features, could be shown to providesignificantly superior performance with a maxi-mal informedness of 0.87 compared to 0.55 for thenext best approach.In future work, classification performance can befurther increased by better handling of currentproblem cases, e.g.
analogies with out-of-domaintargets (analogies between locations and other en-tity classes, analogies between other entities butunrelated locations nearby, etc.)
or ambiguoussentence constructions.
Also, our approach can beadopted to other domains relevant to Web-basedinformation systems like movies, cars, books, ore-commerce products in general.However, the more challenging next step is actu-ally analyzing the semantics of the retrieved anal-ogies, i.e.
extracting the triggers of why peoplechose to compare the source and target.
Achievingthis challenge will allow building analogy reposi-tories containing perceived similarities betweenentities and is a mandatory building block for ac-tually implementing an analogy-enabled infor-mation system.567ReferencesBeust, P., Ferrari, S., & Perlerin, V. (2003).
NLP modeland tools for detecting and interpretingmetaphors in domain-specific corpora.
In Conf.on Corpus Linguistics.
Lancaster, UK.Bollegala, D. T., Matsuo, Y., & Ishizuka, M. (2009).Measuring the similarity between implicitsemantic relations from the web.
In 18th Int.
Conf.on World Wide Web (WWW).
Madrid, Spain.doi:10.1145/1526709.1526797Bunescu, R. C., & Mooney, R. J.
(2006).
SubsequenceKernels for Relation Extraction.
In Conf.
onAdvances in Neural Information ProcessingSystems (NIPS).
Vancouver, Canada.Chalmers, D. J., French, R. M., & Hofstadter, D. R.(1992).
High-level perception, representation,and analogy: A critique of artificial intelligencemethodology.
Journal of Experimental &Theoretical Artificial Intelligence, 4(3), 185?211.doi:10.1080/09528139208953747Chang, C.-C., & Lin, C.-J.
(2011).
LIBSVM: A libraryfor support vector machines.
ACM Transactionson Intelligent Systems and Technology (TIST),2(3).Dedre Gentner, Keith J. Holyoak, & Boicho N.Kokinov (Eds.).
(2001).
The analogical mind:perspectives from cognitive science (Vol.
0, p.541).
MIT Press.Gentner, D. (1983).
Structure-mapping: A theoreticalframework for analogy.
Cognitive science, 7,155?170.Gentner, D. (2003).
Why We?re So Smart.
In Languagein Mind: Advances in the Study of Language andThought (pp.
195?235).
MIT Press.Gentner, D., & Gunn, V. (2001).
Structural alignmentfacilitates the noticing of differences.
Memory &Cognition, 29(4), 565?77.Guthrie, D., Allison, B., Liu, W., Guthrie, L., & Wilks,Y.
(2006).
A closer look at skip-gram modelling.In Int.
Conf.
on Language Resources andEvaluation (LREC).
Genoa, Italy.Hearst, M. A.
(1992).
Automatic acquisition ofhyponyms from large text corpora.
In Int.
Conf.on Computational Linguistics (COLING).
Nantes,France.Hofstadter, D. R. (2001).
Analogy as the Core ofCognition.
In The Analogical Mind (pp.
499?538).Itkonen, E. (2005).
Analogy as structure and process:Approaches in linguistics, cognitive psychologyand philosophy of science.
John Benjamins PubCo.Kant, I.
(1790).
Critique of Judgement.Lofi, C., & Nieke, C. (2013).
Modeling Analogies forHuman-Centered Information Systems.
In 5th Int.Conf.
On Social Informatics (SocInfo).
Kyoto,Japan.Marneffe, M.-C. de, MacCartney, B., & Manning, C. D.(2006).
Generating typed dependency parsesfrom phrase structure parses.
In Int.
Conf.
onLanguage Resources and Evaluation (LREC).Genoa, Italy.Nakov, P., & Hearst, M. A.
(2008).
Solving relationalsimilarity problems using the web as a corpus.
InProc.
of ACL:HLT.
Columbus, USA.Noreen, E. W. (1989).
Computer-intensive Methods forTesting Hypotheses: An Introduction.
John Wiley& Sons, New York, NY, USA.Pei, J., Han, J., Mortazavi-asl, B., Pinto, H., Chen, Q.,Dayal, U., & Hsu, M. (2001).
PrefixSpan: MiningSequential Patterns Efficiently by Prefix-Projected Pattern Growth.
IEEE ComputerSociety.Powers, D. M. W. (2007).
Evaluation: From Precision,Recall and F-Factor to ROC, Informedness,Markedness & Correlation.
Flinders UniversityAdelaide Technical Report SIE07001.Quinlan, R. (1993).
C4.5: Programs for MachineLearning.
San Mateo, USA: Morgan KaufmannPublishers, Inc.Shelley, C. (2003).
Multiple Analogies In Science AndPhilosophy.
John Benjamins Pub.Shutova, E. (2010).
Models of metaphor in NLP.
InAnnual Meeting of the Association forComputational Linguistics (ACL).Shutova, E., Sun, L., & Korhonen, A.
(2010).
Metaphoridentification using verb and noun clustering.
InInt.
Conf.
on Computational Linguistics(COLING).
Beijing, China.Snow, R., Jurafsky, D., & Ng, A.
(2004).
Learningsyntactic patterns for automatic hypernymdiscovery.
In Advances in Neural InformationProcessing Systems (NIPS).
Vancouver, Canada.Snow, R., O?Connor, B., Jurafsky, D., & Ng, A.
(2008).Cheap and fast---but is it good?
Evaluating non-expert annotations for natural language tasks.
InEmpirical Methods in Natural LanguageProcessing (EMNLP).
Honolulu, USA.Turney, P. (2008).
A uniform approach to analogies,synonyms, antonyms, and associations.
In Int.Conf.
on Computational Linguistics (COLING).Manchester, UK.Wilks, Y.
(1978).
Making preferences more active.Artificial Intelligence, 11(3), 197?223.568
