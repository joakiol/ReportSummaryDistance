Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 218?224,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsFrame-Semantic Role Labeling with Heterogeneous AnnotationsMeghana Kshirsagar?Sam Thomson?Nathan Schneider?Jaime Carbonell?Noah A. Smith?Chris Dyer?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA?School of Informatics, University of Edinburgh, Edinburgh, Scotland, UKAbstractWe consider the task of identifying and la-beling the semantic arguments of a predi-cate that evokes a FrameNet frame.
Thistask is challenging because there are onlya few thousand fully annotated sentencesfor supervised training.
Our approach aug-ments an existing model with features de-rived from FrameNet and PropBank andwith partially annotated exemplars fromFrameNet.
We observe a 4% absolute in-crease in F1versus the original model.1 IntroductionPaucity of data resources is a challenge forsemantic analyses like frame-semantic parsing(Gildea and Jurafsky, 2002; Das et al, 2014) usingthe FrameNet lexicon (Baker et al, 1998; Fillmoreand Baker, 2009).1Given a sentence, a frame-semantic parse maps word tokens to frames theyevoke, and for each frame, finds and labels its ar-gument phrases with frame-specific roles.
An ex-ample appears in figure 1.In this paper, we address this argument iden-tification subtask, a form of semantic role label-ing (SRL), a task introduced by Gildea and Juraf-sky (2002) using an earlier version of FrameNet.Our contribution addresses the paucity of annotateddata for training using standard domain adaptationtechniques.
We exploit three annotation sources:?
the frame-to-frame relations in FrameNet, byusing hierarchical features to share statisticalstrength among related roles (?3.2),?
FrameNet?s corpus of partially-annotated ex-emplar sentences, by using ?frustratinglyeasy?
domain adaptation (?3.3), and?Corresponding author: mkshirsa@cs.cmu.edu1http://framenet.icsi.berkeley.edudo you want me to hold off until  I finish July and August ?Experiencer EventEnd_pointAgentACTIVITY_FINISH: complete.v  conclude.v  finish.v ?HOLDING_OFF_ON: hold off.v  wait.vDESIRING: eager.a  hope.n  hope.v  interested.a  itch.v  want.v  wish.n  wish.v ?Focal_participantAgent Desirable_action: ?ActivityA1A1A0 A1 finish-v-01stay-v-01want-v-01A3A0FrameNetthe people really want us to stay the course and finish the job .PropBankAM-ADVFigure 1: Part of a sentence from FrameNet full-text an-notation.
3 frames and their arguments are shown: DESIR-ING is evoked by want, ACTIVITY_FINISH by finish, and HOLD-ING_OFF_ON by hold off.
Thin horizontal lines representingargument spans are labeled with role names.
(Not shown: Julyand August evoke CALENDRIC_UNIT and fill its Unit role.
)do you want me to hold off until  I finish July and August ?Experiencer EventEnd_pointAgentACTIVITY_FINISH: complete.v  conclude.v  finish.v ?HOLDING_OFF_ON: hold off.v  wait.vDESIRING: eager.a  hope.n  hope.v  interested.a  itch.v  want.v  wish.n  wish.v ?Focal_participantAgent Desirable_action: ?ActivityA1A1A0 A1 finish-v-01stay-v-01want-v-01A3A0FrameNetthe people really want us to stay the course and finish the job .PropBankAM-ADVFigure 2: A PropBank-annotated sentence from OntoNotes(Hovy et al, 2006).
The PB lexicon defines rolesets (verbsense?specific frames) and their core roles: e.g., finish-v-01?cause to stop?, A0 ?intentional agent?, A1 ?thing finishing?, andA2 ?explicit instrument, thing finished with?.
(finish-v-03, bycontrast, means ?apply a finish, as to wood?.)
Clear similaritiesto the FrameNet annotations in figure 1 are evident, thoughPB uses lexical frames rather than deep frames and makessome different decisions about roles (e.g., want-v-01 has noanalogue to Focal_participant).?
a PropBank-style SRL system, by using guidefeatures (?3.4).2These expansions of the training corpus and thefeature set for supervised argument identificationare integrated into SEMAFOR (Das et al, 2014),the leading open-source frame-semantic parserfor English.
We observe a 4% F1improvementin argument identification on the FrameNet testset, leading to a 1% F1improvement on the fullframe-semantic parsing task.
Our code and mod-els are available at http://www.ark.cs.cmu.edu/SEMAFOR/.2 FrameNetFrameNet represents events, scenarios, and rela-tionships with an inventory of frames (such as2Preliminary experiments training on PropBank annota-tions mapped to FrameNet via SemLink 1.2.2c (Bonial et al,2013) hurt performance, likely due to errors and coveragegaps in the mappings.218SHOPPING and SCARCITY).
Each frame is associ-ated with a set of roles (or frame elements) calledto mind in order to understand the scenario, andlexical predicates (verbs, nouns, adjectives, andadverbs) capable of evoking the scenario.
For ex-ample, the BODY_MOVEMENT frame has Agent andBody_part as its core roles, and lexical entries in-cluding verbs such as bend, blink, crane, and curtsy,plus the noun use of curtsy.
In FrameNet 1.5, thereare over 1,000 frames and 12,000 lexical predi-cates.2.1 HierarchyThe FrameNet lexicon is organized as a network,with several kinds of frame-to-frame relationslinking pairs of frames and (subsets of) their ar-guments (Ruppenhofer et al, 2010).
In this work,we consider two kinds of frame-to-frame relations:Inheritance: E.g., ROBBERY inherits fromCOMMITTING_CRIME, which inherits from MIS-DEED.
Crucially, roles in inheriting framesare mapped to corresponding roles in inher-ited frames: ROBBERY.Perpetrator links toCOMMITTING_CRIME.Perpetrator, which links toMISDEED.Wrongdoer, and so forth.Subframe: This indicates a subevent within acomplex event.
E.g., the CRIMINAL_PROCESS framegroups together subframes ARREST, ARRAIGN-MENT and TRIAL.
CRIMINAL_PROCESS.Defendant,for instance, is mapped to ARREST.Suspect,TRIAL.Defendant, and SENTENCING.Convict.We say that a parent of a role is one that haseither the Inheritance or Subframe relation to it.There are 4,138 Inheritance and 589 Subframelinks among role types in FrameNet 1.5.Prior work has considered various ways of group-ing role labels together in order to share statisti-cal strength.
Matsubayashi et al (2009) observedsmall gains from using the Inheritance relation-ships and also from grouping by the role name(SEMAFOR already incorporates such features).Johansson (2012) reports improvements in SRL forSwedish, by exploiting relationships between bothframes and roles.
Baldewein et al (2004) learnlatent clusters of roles and role-fillers, reportingmixed results.
Our approach is described in ?3.2.2.2 AnnotationsStatistics for the annotations appear in table 1.Full-text (FT): This portion of the FrameNet cor-pus consists of documents and has about 5,000sentences for which annotators assigned framesFull-Text Exemplarstrain test train testSentences 2,780 2,420 137,515 4,132Frames 15,019 4,458 137,515 4,132Overt arguments 25,918 7,210 278,985 8,417TYPESFrames 642 470 862 562Roles 2,644 1,420 4,821 1,224Unseen frames vs. train: 46 0Roles in unseen frames vs. train: 178 0Unseen roles vs. train: 289 38Unseen roles vs. combined train: 103 32Table 1: Characteristics of the training and test data.
(Thesestatistics exclude the development set, which contains 4,463frames over 746 sentences.
)and arguments to as many words as possible.
Be-ginning with the SemEval-2007 shared task onFrameNet analysis, frame-semantic parsers havebeen trained and evaluated on the full-text data(Baker et al, 2007; Das et al, 2014).3The full-textdocuments represent a mix of genres, prominentlyincluding travel guides and bureaucratic reportsabout weapons stockpiles.Exemplars: To document a given predicate, lexi-cographers manually select corpus examples andannotate them only with respect to the predicatein question.
These singly-annotated sentencesfrom FrameNet are called lexicographic exem-plars.
There are over 140,000 sentences containingargument annotations and relative to the FT dataset,these contain an order of magnitude more frameannotations and over two orders of magnitude moresentences.
As these were manually selected, therate of overt arguments per frame is noticeablyhigher than in the FT data.
The exemplars formedthe basis of early studies of frame-semantic rolelabeling (e.g., Gildea and Jurafsky, 2002; Thomp-son et al, 2003; Fleischman et al, 2003; Litkowski,2004; Kwon et al, 2004).
Exemplars have not yetbeen exploited successfully to improve role label-ing performance on the more realistic FT task.42.3 PropBankPropBank (PB; Palmer et al, 2005) is a lexicon andcorpus of predicate?argument structures that takesa shallower approach than FrameNet.
FrameNetframes cluster lexical predicates that evoke sim-3Though these were annotated at the document level,and train/development/test splits are by document, the frame-semantic parsing is currently restricted to the sentence level.4Das and Smith (2011, 2012) investigated semi-supervisedtechniques using the exemplars and WordNet for frame iden-tification.
Hermann et al (2014) also improve frame iden-tification by mapping frames and predicates into the samecontinuous vector space, allowing statistical sharing.219ilar kinds of scenarios In comparison, PropBankframes are purely lexical and there are no formalrelations between different predicates or their roles.PropBank?s sense distinctions are generally coarser-grained than FrameNet?s.
Moreover, FrameNet lex-ical entries cover many different parts of speech,while PropBank focuses on verbs and (as of re-cently) eventive noun and adjective predicates.
Anexample with PB annotations is shown in figure 2.3 ModelWe use the model from SEMAFOR (Das et al,2014), detailed in ?3.1, as a starting point.
We ex-periment with techniques that augment the model?straining data (?3.3) and feature set (?3.2, ?3.4).3.1 BaselineIn SEMAFOR, the argument identification task istreated as a structured prediction problem.
Letthe classification input be a dependency-parsedsentence x, the token(s) p constituting the pred-icate in question, and the frame f evoked by p (asdetermined by frame identification).
We use theheuristic procedure described by (Das et al, 2014)for extracting candidate argument spans for thepredicate; call this spans(x, p, f ).
spans alwaysincludes a special span denoting an empty or non-overt role, denoted ?.
For each candidate argumenta ?
spans(x, p, f ) and each role r, a binary featurevector ?
(a,x, p, f ,r) is extracted.
We use the fea-ture extractors from (Das et al, 2014) as a baseline,adding additional ones in our experiments (?3.2??3.4).
Each a is given a real-valued score by alinear model:scorew(a ?
x, p, f ,r) =w??
(a,x, p, f ,r) (1)The model parameters w are learned from data (?4).Prediction requires choosing a joint assignmentof all arguments of a frame, respecting the con-straints that a role may be assigned to at most onespan, and spans of overt arguments must not over-lap.
Beam search, with a beam size of 100, is usedto find this argmax.53.2 Hierarchy FeaturesWe experiment with features shared between re-lated roles of related frames in order to capture5Recent work has improved upon global decoding tech-niques (Das et al, 2012; T?ckstr?m et al, 2015).
We expectsuch improvements to be complementary to the gains due tothe added features and data reported here.statistical generalizations about the kinds of argu-ments seen in those roles.
Our hypothesis is thatthis will be beneficial given the small number oftraining examples for individual roles.All roles that have a common parent based onthe Inheritance and Subframe relations will sharea set of features in common.
Specifically, for eachbase feature ?
which is conjoined with the role rin the baseline model (?
?
"role=r"), and for eachparent r?of r, we add a new copy of the featurethat is the base feature conjoined with the parentrole, (?
?"parent_role=r?").
We experimented withusing more than one level of the hierarchy (e.g.,grandparents), but the additional levels did not im-prove performance.3.3 Domain Adaptation and ExemplarsDaum?
(2007) proposed a feature augmentationapproach that is now widely used in superviseddomain adaptation scenarios.
We use a variantof this approach.
Let Dexdenote the exemplarstraining data, and Dftdenote the full text trainingdata.
For every feature ?
(a,x, p, f ,r) in the basemodel, we add a new feature ?ft(?)
that fires onlyif ?(?)
fires and x ?Dft.
The intuition is that eachbase feature contributes both a ?general?
weightand a ?domain-specific?
weight to the model; thus,it can exhibit a general preference for specific roles,but this general preference can be fine-tuned forthe domain.
Regularization encourages the modelto use the general version over the domain-specific,if possible.3.4 Guide FeaturesAnother approach to domain adaptation is to train asupervised model on a source domain, make predic-tions using that model on the target domain, thenuse those predictions as additional features whiletraining a new model on the target domain.
Thesource domain model is effectively a form of pre-processing, and the features from its output areknown as guide features (Johansson, 2013; Konget al, 2014).6In our case, the full text data is our target do-main, and PropBank and the exemplars data are oursource domains, respectively.
For PropBank, werun the SRL system of Illinois Curator 1.1.4 (Pun-6This is related to the technique of model stacking, wheresuccessively richer models are trained by cross-validation onthe same dataset (e.g., Cohen and Carvalho, 2005; Nivre andMcDonald, 2008; Martins et al, 2008).220yakanok et al, 2008)7on verbs in the full-text data.For the exemplars, we train baseline SEMAFORon the exemplars and run it on the full-text data.We use two types of guide features: one encodesthe role label predicted by the source model, andthe other indicates that a span a was assigned somerole.
For the exemplars, we use an additional fea-ture to indicate that the predicted role matches therole being filled.4 LearningFollowing SEMAFOR, we train using a local ob-jective, treating each role and span pair as an in-dependent training instance.
We have made twomodifications to training which had negligible im-pact on full-text accuracy, but decreased trainingtime significantly:8?
We use the online optimization methodAdaDelta (Zeiler, 2012) with minibatches, in-stead of the batch method L-BFGS (Liu andNocedal, 1989).
We use minibatches of size4,000 on the full text data, and 40,000 on theexemplar data.?
We minimize squared structured hinge lossinstead of a log-linear loss.
Let ((x, p, f ,r),a)be the ith training example.
Then the squaredhinge loss is given by Lw(i) =(maxa?{w??
(a?,x, p, f ,r)+1{a?
/= a}}?w??
(a,x, p, f ,r))2We learn w by minimizing the `2-regularized aver-age loss on the dataset:w?
= argminw1NN?i=1Lw(i)+12?
?w?22(2)5 Experimental SetupWe use the same FrameNet 1.5 data and train/testsplits as Das et al (2014).
Automatic syntactic de-pendency parses from MSTParserStacked (Martinset al, 2008) are used, as in Das et al (2014).Preprocessing.
Out of 145,838 exemplar sen-tences, we removed 4,191 sentences which hadno role annotations.
We removed sentences that ap-peared in the full-text data.
We also merged spanswhich were adjacent and had the same role label.7http://cogcomp.cs.illinois.edu/page/software_view/SRL8With SEMAFOR?s original features and training data, theresult of the above changes is that full-text F1decreases from59.3% to 59.1%, while training time (running optimization toconvergence) decreases from 729 minutes to 82 minutes.Training Configuration Model P R F1(Features) Size (%) (%) (%)FT (Baseline) 1.1 65.6 53.8 59.1FT (Hierarchy) 1.9 67.2 54.8 60.4Exemplarsguide????
FT 1.2 65.2 55.9 60.2FT+Exemplars (Basic) 5.0 66.0 58.2 61.9FT+Exemplars (DA) 5.8 65.7 59.0 62.2PB-SRLguide????
FT 1.2 65.0 54.8 59.5Combining the best methodsPB-SRLguide????
FT+Exemplars 5.5 67.4 58.8 62.8FT+Exemplars (Hierarchy) 9.3 66.0 60.4 63.1Table 2: Argument identification results on the full-text testset.
Model size is in millions of features.Hyperparameter tuning.
We determined thestopping criterion and the `2regularization parame-ter ?
by tuning on the FT development set, search-ing over the following values for ?
: 10?5, 10?7,10?9, 10?12.Evaluation.
A complete frame-semantic parsingsystem involves frame identification and argumentidentification.
We perform two evaluations: one as-suming gold-standard frames are given, to evaluateargument identification alone; and one using theoutput of the system described by Hermann et al(2014), the current state-of-the-art in frame identi-fication, to demonstrate that our improvements areretained when incorporated into a full system.6 ResultsArgument Identification.
We present precision,recall, and F1-measure microaveraged across thetest instances in table 2, for all approaches.
Theevaluation used in Das et al (2014) assesses bothframes and arguments; since our focus is on SRL,we only report performance for arguments, ren-dering our scores more interpretable.
Under ourargument-only evaluation, the system of Das et al(2014) gets 59.3% F1.The first block shows baseline performance.
Thenext block shows the benefit of FrameNet hierarchyfeatures (+1.2% F1).
The third block shows thatusing exemplars as training data, especially withdomain adaptation, is preferable to using them asguide features (2.8% F1vs.
0.9% F1).
PropBankSRL as guide features offers a small (0.4% F1) gain.The last two rows of table 2 show the perfor-mance upon combining the best approaches.
Bothuse full-text and exemplars for training; the firstuses PropBank SRL as guide features, and the sec-ond adds hierarchy features.
The best result is the2210 200 400 600 800 1000 1200 1400050100150Frame Element, ordered by test set frequencyTestExamples(a) Frequency of each role appearing in the test set.0 200 400 600 800 1000 1200 14000.00.20.40.60.8Frame Element, ordered by test set frequencyF1Baseline (FT)FT + ExemplarsFT + Exemplars + PBFT + Exemplars + Siblings(b) F1of the best methods compared with the baseline.Figure 3: F1for each role appearing in the test set, ranked byfrequency.
F1values have been smoothed with loess, witha smoothing parameter of 0.2.
?Siblings?
refers to hierarchyfeatures.latter, gaining 3.95% F1over the baseline.Role-level evaluation.
Figure 3(b) shows F1perframe element, for the baseline and the three bestmodels.
Each x-axis value is one role, sorted bydecreasing frequency (the distribution of role fre-quencies is shown in figure 3(a)).
For frequentroles, performance is similar; our models achievegains on rarer roles.Full system.
When using the frame output ofHermann et al (2014), F1improves by 1.1%, from66.8% for the baseline, to 67.9% for our combinedmodel (from the last row in table 2).7 ConclusionWe have empirically shown that auxiliary semanticresources can benefit the challenging task of frame-semantic role labeling.
The significant gains comefrom the FrameNet exemplars and the FrameNet hi-erarchy, with some signs that the PropBank schemecan be leveraged as well.We are optimistic that future improvements tolexical semantic resources, such as crowdsourcedlexical expansion of FrameNet (Pavlick et al, 2015)as well as ongoing/planned changes for PropBank(Bonial et al, 2014) and SemLink (Bonial et al,2013), will lead to further gains in this task.
More-over, the techniques discussed here could be furtherexplored using semi-automatic mappings betweenlexical resources (such as UBY; Gurevych et al,2012), and correspondingly, this task could be usedto extrinsically validate those mappings.Ours is not the only study to show benefit fromheterogeneous annotations for semantic analysistasks.
Feizabadi and Pad?
(2015), for example,successfully applied similar techniques for SRL ofimplicit arguments.9Ultimately, given the diversityof semantic resources, we expect that learning fromheterogeneous annotations in different corpora willbe necessary to build automatic semantic analyzersthat are both accurate and robust.AcknowledgmentsThe authors are grateful to Dipanjan Das for his as-sistance, and to anonymous reviewers for their help-ful feedback.
This research has been supported bythe Richard King Mellon Foundation and DARPAgrant FA8750-12-2-0342 funded under the DEFTprogram.ReferencesCollin Baker, Michael Ellsworth, and Katrin Erk.
2007.SemEval-2007 Task 19: frame semantic structureextraction.
In Proc.
of SemEval, pages 99?104.Prague, Czech Republic.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proc.of COLING-ACL, pages 86?90.
Montreal, Quebec,Canada.
URL http://framenet.icsi.berkeley.edu.Ulrike Baldewein, Katrin Erk, Sebastian Pad?, andDetlef Prescher.
2004.
Semantic role labellingwith similarity-based generalization using EM-based clustering.
In Rada Mihalcea and Phil Ed-monds, editors, Proc.
of SENSEVAL-3, the ThirdInternational Workshop on the Evaluation of Sys-tems for the Semantic Analysis of Text, pages 64?68.Barcelona, Spain.Claire Bonial, Julia Bonn, Kathryn Conger, Jena D.Hwang, and Martha Palmer.
2014.
PropBank: se-mantics of new predicate types.
In Nicoletta Calzo-lari, Khalid Choukri, Thierry Declerck, Hrafn Lofts-son, Bente Maegaard, Joseph Mariani, AsuncionMoreno, Jan Odijk, and Stelios Piperidis, editors,Proc.
of LREC, pages 3013?3019.
Reykjav?k, Ice-land.Claire Bonial, Kevin Stowe, and Martha Palmer.
2013.Renewing and revising SemLink.
In Proc.
of the9They applied frustratingly easy domain adaptation tolearn from FrameNet alng with a PropBank-like dataset ofnominal frames.2222nd Workshop on Linked Data in Linguistics (LDL-2013): Representing and linking lexicons, terminolo-gies and other language data, pages 9?17.
Pisa,Italy.William W. Cohen and Vitor R. Carvalho.
2005.Stacked sequential learning.
In Proc.
of IJCAI, pages671?676.
Edinburgh, Scotland, UK.Dipanjan Das, Desai Chen, Andr?
F. T. Martins,Nathan Schneider, and Noah A. Smith.
2014.Frame-semantic parsing.
Computational Linguis-tics, 40(1):9?56.
URL http://www.ark.cs.cmu.edu/SEMAFOR.Dipanjan Das, Andr?
F. T. Martins, and Noah A. Smith.2012.
An exact dual decomposition algorithm forshallow semantic parsing with constraints.
In Proc.of *SEM, pages 209?217.
Montr?al, Canada.Dipanjan Das and Noah A. Smith.
2011.
Semi-supervised frame-semantic parsing for unknownpredicates.
In Proc.
of ACL-HLT, pages 1435?1444.Portland, Oregon, USA.Dipanjan Das and Noah A. Smith.
2012.
Graph-basedlexicon expansion with sparsity-inducing penalties.In Proc.
of NAACL-HLT, pages 677?687.
Montr?al,Canada.Hal Daum?, III.
2007.
Frustratingly easy domain adap-tation.
In Proc.
of ACL, pages 256?263.
Prague,Czech Republic.Parvin Sadat Feizabadi and Sebastian Pad?.
2015.Combining seemingly incompatible corpora for im-plicit semantic role labeling.
In Proc.
of *SEM,pages 40?50.
Denver, Colorado, USA.Charles J. Fillmore and Collin Baker.
2009.
A framesapproach to semantic analysis.
In Bernd Heine andHeiko Narrog, editors, The Oxford Handbook of Lin-guistic Analysis, pages 791?816.
Oxford UniversityPress, Oxford, UK.Michael Fleischman, Namhee Kwon, and Eduard Hovy.2003.
Maximum entropy models for FrameNet clas-sification.
In Michael Collins and Mark Steedman,editors, Proc.
of EMNLP, pages 49?56.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Iryna Gurevych, Judith Eckle-Kohler, Silvana Hart-mann, Michael Matuschek, Christian M. Meyer, andChristian Wirth.
2012.
UBY - a large-scale unifiedlexical-semantic resource based on LMF.
In Proc.
ofEACL, pages 580?590.
Avignon, France.Karl Moritz Hermann, Dipanjan Das, Jason Weston,and Kuzman Ganchev.
2014.
Semantic frame iden-tification with distributed word representations.
InProc.
of ACL, pages 1448?1458.
Baltimore, Mary-land, USA.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:the 90% solution.
In Proc.
of HLT-NAACL, pages57?60.
New York City, USA.Richard Johansson.
2012.
Non-atomic classification toimprove a semantic role labeler for a low-resourcelanguage.
In Proc.
of *SEM, pages 95?99.
Montr?al,Canada.Richard Johansson.
2013.
Training parsers on incom-patible treebanks.
In Proc.
of NAACL-HLT, pages127?137.
Atlanta, Georgia, USA.Lingpeng Kong, Nathan Schneider, SwabhaSwayamdipta, Archna Bhatia, Chris Dyer, andNoah A. Smith.
2014.
A dependency parser fortweets.
In Proc.
of EMNLP, pages 1001?1012.Doha, Qatar.Namhee Kwon, Michael Fleischman, and Eduard Hovy.2004.
FrameNet-based semantic parsing using max-imum entropy models.
In Proc.
of Coling, pages1233?1239.
Geneva, Switzerland.Ken Litkowski.
2004.
SENSEVAL-3 task: Automaticlabeling of semantic roles.
In Rada Mihalcea andPhil Edmonds, editors, Proc.
of SENSEVAL-3, theThird International Workshop on the Evaluation ofSystems for the Semantic Analysis of Text, pages 9?12.
Barcelona, Spain.Dong C. Liu and Jorge Nocedal.
1989.
On the LimitedMemory BFGS Method for Large Scale Optimiza-tion.
Math.
Program., 45(3):503?528.Andr?
F. T. Martins, Dipanjan Das, Noah A. Smith, andEric P. Xing.
2008.
Stacking dependency parsers.
InProc.
of EMNLP, pages 157?166.
Honolulu, Hawaii.Yuichiroh Matsubayashi, Naoaki Okazaki, and Jun?ichiTsujii.
2009.
A comparative study on generalizationof semantic roles in FrameNet.
In Proc.
of ACL-IJCNLP, pages 19?27.
Suntec, Singapore.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proc.
of ACL-HLT, pages 950?958.Columbus, Ohio, USA.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: an annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Ellie Pavlick, Travis Wolfe, Pushpendre Rastogi,Chris Callison-Burch, Mark Drezde, and BenjaminVan Durme.
2015.
FrameNet+: Fast paraphrastictripling of FrameNet.
In Proc.
of ACL-IJCNLP.
Bei-jing, China.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inferencein semantic role labeling.
Computational Linguis-tics, 34(2):257?287.
URL http://cogcomp.cs.illinois.edu/page/software_view/SRL.Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.Petruck, Christopher R. Johnson, and Jan Schef-fczyk.
2010.
FrameNet II: extended theory and prac-tice.
URL https://framenet2.icsi.berkeley.edu/docs/r1.5/book.pdf.223Oscar T?ckstr?m, Kuzman Ganchev, and Dipanjan Das.2015.
Efficient inference and structured learning forsemantic role labeling.
Transactions of the Associa-tion for Computational Linguistics, 3:29?41.Cynthia A. Thompson, Roger Levy, and Christopher D.Manning.
2003.
A generative model for semanticrole labeling.
In Machine Learning: ECML 2003,pages 397?408.Matthew D. Zeiler.
2012.
ADADELTA: An adap-tive learning rate method.
arXiv:1212.5701 [cs].URL http://arxiv.org/abs/1212.5701, arXiv:1212.5701.224
