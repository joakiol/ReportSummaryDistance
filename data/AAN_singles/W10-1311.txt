Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 80?88,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTowards a noisy-channel model of dysarthria in speech recognitionFrank RudziczUniversity of Toronto, Department of Computer ScienceToronto, Ontario, Canadafrank@cs.toronto.eduAbstractModern automatic speech recognition is inef-fective at understanding relatively unintelligi-ble speech caused by neuro-motor disabilitiescollectively called dysarthria.
Since dysarthriais primarily an articulatory phenomenon, weare collecting a database of vocal tract mea-surements during speech of individuals withcerebral palsy.
In this paper, we demonstratethat articulatory knowledge can remove am-biguities in the acoustics of dysarthric speak-ers by reducing entropy relatively by 18.3%,on average.
Furthermore, we demonstratethat dysarthric speech is more precisely por-trayed as a noisy-channel distortion of anabstract representation of articulatory goals,rather than as a distortion of non-dysarthricspeech.
We discuss what implications theseresults have for our ongoing development ofspeech systems for dysarthric speakers.1 IntroductionDysarthria is a set of congenital and traumaticneuro-motor disorders that impair the physical pro-duction of speech and affects approximately 0.8% ofindividuals in North America (Hosom et al, 2003).Causes of dysarthria include cerebral palsy (CP),multiple sclerosis, Parkinson?s disease, and amy-otrophic lateral sclerosis (ALS).
These impairmentsreduce or remove normal control of the primary vo-cal articulators but do not affect the abstract produc-tion of meaningful, syntactically correct language.The neurological origins of dysarthria involvedamage to the cranial nerves that control the speecharticulators (Moore and Dalley, 2005).
Spasticdysarthria, for instance, is partially caused by le-sions in the facial and hypoglossal nerves, whichcontrol the jaw and tongue respectively (Duffy,2005), resulting in slurred speech and a less differ-entiable vowel space (Kent and Rosen, 2004).
Sim-ilarly, damage to the glossopharyngeal nerve can re-duce control over vocal fold vibration (i.e., phona-tion), resulting in guttural or grating raspiness.
In-adequate control of the soft palate caused by disrup-tion of the vagus nerve may lead to a disproportion-ate amount of air released through the nose duringspeech (i.e., hypernasality).Unfortunately, traditional automatic speechrecognition (ASR) is incompatible with dysarthricspeech, often rendering such software inaccessibleto those whose neuro-motor disabilities might makeother forms of interaction (e.g., keyboards, touchscreens) laborious.
Traditional representations inASR such as hidden Markov models (HMMs)trained for speaker independence that achieve84.8% word-level accuracy for non-dysarthricspeakers might achieve less than 4.5% accuracygiven severely dysarthric speech on short sentences(Rudzicz, 2007).
Our research group is currentlydeveloping new ASR models that incorporate em-pirical knowledge of dysarthric articulation for usein assistive applications (Rudzicz, 2009).
Althoughthese models have increased accuracy, the disparityis still high.
Our aim is to understand why ASRfails for dysarthric speakers by understanding theacoustic and articulatory nature of their speech.In this paper, we cast the speech-motor interfacewithin the mathematical framework of the noisy-channel model.
This is motivated by the charac-80terization of dysarthria as a distortion of parallelbiological pathways that corrupt motor signals be-fore execution (Kent and Rosen, 2004; Freund etal., 2005), as in the examples cited above.
Withinthis information-theoretic framework, we aim to in-fer the nature of the motor signal distortions givenappropriate measurements of the vocal tract.
That is,we ask the following question: Is dysarthric speecha distortion of typical speech, or are they both distor-tions of some common underlying representation?2 Dysarthric articulation dataSince the underlying articulatory dynamics ofdysarthric speech are intrinsically responsible forcomplex acoustic irregularities, we are collectinga database of dysarthric articulation.
Time-alignedmovement and acoustic data are measured usingtwo systems.
The first infers 3D positions of sur-face facial markers given stereo video images.
Thesecond uses electromagnetic articulography (EMA),in which the speaker is placed within a cube thatproduces a low-amplitude electromagnetic field, asshown in figure 1.
Tiny sensors within this field al-low the inference of articulator positions and veloci-ties to within 1 mm of error (Yunusova et al, 2009).Figure 1: Electromagnetic articulograph system.We have so far recorded one male speaker withALS, five male speakers with CP, four femalespeakers with CP, and age- and gender-matchedcontrols.
Measurement coils are placed as inother studies (e.g., the University of Edinburgh?sMOCHA database (Wrench, 1999) and the Uni-versity of Wisconsin-Madison?s x-ray microbeamdatabase (Yunusova et al, 2008)).
Specifically, weare interested in the positions of the upper and lowerlip (UL and LL), left and right mouth corners (LMand RM), lower incisor (LI), and tongue tip, blade,and dorsum (TT, TB, and TD).
Unfortunately, a fewof our male CP subjects had a severe gag reflex, andwe found it impossible to place more than one coilon the tongue for these few individuals.
Therefore,of the tongue positions, only TT is used in this study.All articulatory data are smoothed with third-ordermedian filtering in order to minimize measurement?jitter?.
Figure 2 shows the degree of lip aperture(i.e., the distance between UL and LL) over time fora control and a dysarthric speaker repeating the se-quence /ah p iy/.
Here, the dysarthric speech is no-tably slower and has more excessive movement.0 1 2 3 4 5 612345Time (s)Lip aperture (cm)ControlDysarthricFigure 2: Lip aperture over time for four iterations of /ahp iy/ given a dysarthric and control speaker.Our dysarthric speech data include random repeti-tions of phonetically balanced short sentences origi-nally used in the TIMIT database (Zue et al, 1989),as well as pairs of monosyllabic words identifiedby Kent et al (1989) as having relevant articula-tory contrasts (e.g., beat versus meat as a stop-nasal contrast).
All articulatory data are alignedwith associated acoustic data, which are transformedto Mel-frequency cepstral coefficients (MFCCs).Phoneme boundaries and pronunciation errors arebeing transcribed by a speech-language pathologistto the TIMIT phoneset.
Table 1 shows pronuncia-tion errors according to manner of articulation fordysarthric speech.
Plosives are mispronounced mostoften, with substitution errors exclusively caused byerrant voicing (e.g.
/d/ for /t/).
By comparison, only815% of corresponding plosives in total are mispro-nounced in regular speech.
Furthermore, the preva-lence of deleted affricates in word-final positions, al-most all of which are alveolar, does not occur in thecorresponding control data.SUB (%) DEL (%)i m f i m fplosives 13.8 18.7 7.1 1.9 1.0 12.1affricates 0.0 8.3 0.0 0.0 0.0 23.2fricatives 8.5 3.1 5.3 22.0 5.5 13.2nasals 0.0 0.0 1.5 0.0 0.0 1.5glides 0.0 0.7 0.4 11.4 2.5 0.9vowels 0.9 0.9 0.0 0.0 0.2 0.0Table 1: Percentage of phoneme substitution (SUB) anddeletion (DEL) errors in word-initial (i), word-medial(m), and word-final (f) positions across categories ofmanner for dysarthric data.Table 2 shows the relative durations of the fivemost common vowels and sonorant consonants inour database between dysarthric and control speech.Here, dysarthric speakers are significantly slowerthan their control counterparts at the 95% confidenceinterval for /eh/ and at the 99.5% confidence intervalfor all other phonemes.Phonemeduration (?
(?2), in ms) Avg.Dysarthric Control diff./ah/ 189.3 (19.2) 120.1 (4.0) 69.2/ae/ 211.6 (16.4) 140.0 (4.4) 71.6/eh/ 160.5 (7.4) 107.3 (2.6) 53.2/iy/ 177.1 (86.7) 105.8 (93.1) 71.3/er/ 220.5 (27.9) 148.6 (59.8) 71.9/l/ 138.5 (8.0) 91.8 (2.4) 46.7/m/ 173.5 (13.4) 94.7 (2.1) 78.8/n/ 168.4 (14.4) 90.9 (2.3) 77.5/r/ 138.8 (8.3) 95.3 (3.4) 43.5/w/ 151.5 (12.0) 84.5 (1.3) 67.0Table 2: Average lengths (and variances in parentheses)in milliseconds for the five most common vowels andsonorant consonants for dysarthric and control speakers.The last column is the average difference in millisecondsbetween dysarthric and control subjects.Processing and annotation of further data fromadditional dysarthric speakers is ongoing, includingmeasurements of all three tongue positions.3 Entropy and the noisy-channel modelWe wish to measure the degree of statistical disorderin both acoustic and articulatory data for dysarthricand non-dysarthric speakers, as well as the a posteri-ori disorder of one type of data given the other.
Thisquantification will inform us as to the relative mer-its of incorporating knowledge of articulatory be-haviour into ASR systems for dysarthric speakers.Entropy, H(X), is a measure of the degree of uncer-tainty in a random variable X .
When X is discrete,this value is computed with the familiarH(X) =?n?i=1p(xi) logb p(xi),where b is the logarithm base, xi is a value of X ,of which there are n possible, and p(xi) is its prob-ability.
When our observations are continuous, asthey are in our acoustic and articulatory database,we must use differential entropy defined byH(X) =?
?Xf (X) log f (X)dX ,where f (X) is the probability density function of X .For a number of distributions f (X), the differentialentropy has known forms (Lazo and Rathie, 1978).For example, if f (X) is a multivariate normal,fX(x1, ...,xN) =exp(?12(x??)T??1(x??
))(2pi)N/2 |?|1/2H(X) = 12 ln((2pie)N |?|),(1)where ?
and ?
are the mean and covariances of thedata.
However, since we observe that both acous-tic and articulatory data follow non-Gaussian dis-tributions, we choose to represent these spaces bymixtures of Gaussians.
Huber et al (2008) have de-veloped an accurate algorithm for estimating differ-ential entropy of Gaussian mixtures based on itera-tively merging Gaussians and the approximationH?
(X) =L?i=1?i(?
log?i + 12 log((2pie)N |?i|),where ?i is the weight of the ith(1?
i?
L) Gaussianand ?i is that Gaussian?s covariance matrix.
Thismethod is used to approximate entropies in the fol-lowing study, with L = 32.
Note that while differen-tial entropies can be negative and not invariant under82change of variables, other properties of entropy areretained (Huber et al, 2008), such as the chain rulefor conditional entropyH(Y |X) = H(Y,X)?H(X),which describes the uncertainty in Y given knowl-edge of X , and the chain rule for mutual informationI(Y ;X) = H(X)+H(Y )?H(X ,Y ),which describes the mutual dependence between Xand Y .
Here, we quantize entropy with the nat,which is the natural logarithmic unit, e (?
1.44 bits).3.1 The noisy channelThe noisy-channel theorem states that informationpassed through a channel with capacity C at a rateR ?
C can be reliably recovered with an arbitrarilylow probability of error given an appropriate coding.Here, a message from a finite alphabet is encoded,producing signal x ?
X .
That signal is then distortedby a medium which transmits signal y ?
Y accord-ing to some distribution P(Y |X).
Given that there issome probability that the received signal, y, is cor-rupted, the message produced by the decoder maydiffer from the original (Shannon, 1949).To what extent can we describe the effects ofdysarthria within an information-theoretic noisychannel model?
We pursue two competing hypothe-ses within this general framework.
The first hypoth-esis models the assumption that dysarthric speech isa distorted version of typical speech.
Here, signalX and Y represent the vocal characteristics of thegeneral and dysarthric populations, respectively, andP(Y |X) models the distortion between them.
Thesecond hypothesis models the assumption that bothdysarthric and typical speech are distorted versionsof some common abstraction.
Here, Yd and Yc repre-sent the vocal characteristics of dysarthric and con-trol speakers, respectively, and X represents a com-mon, underlying mechanism and that P(Yd |X) andP(Yc |X) model distortions from that mechanism.These two hypotheses are visualized in figure 3.
Ineach of these cases, signals can be acoustic, articu-latory, or some combination thereof.3.2 Common underlying abstractionsIn order to test our hypothesis that both dysarthricand control speakers share a common high-level ab-P(Y | X) Dysarthric speechsignal, YTypical speechsignal, X(a) Dysarthric speech as a distortion of control speechP(Y |X|? )
Dysarthric speechsignal, Y T?tract speechsignal, ?
P(Y?
|X|? )
??tr?
speechsignal, Y?
(b) Dysarthric and control speech as distortions of a commonabstractionFigure 3: Sections of noisy channel models that mimicthe neuro-motor interface.straction of the vocal tract that is in both cases dis-torted during articulation, we incorporate the the-ory of task dynamics (Saltzman and Munhall, 1989).This theory represents the interface between the lex-ical intentions and vocal tract realizations of speechas a sequence of overlapping gestures, which arecontinuous dynamical systems that describe goal-oriented reconfigurations of the vocal tract, such asbilabial closure during /m/.
Figure 4 shows an ex-ample of overlapping gestures for the word pub.TBCD closedopenGLO openclosedLA openclosed100 200 300 400Time (ms)Figure 4: Canonical example pub from Saltzmanand Munhall (1989) representing overlapping goals fortongue blade constriction degree (TBCD), lip aperture(LA), and glottis (GLO).
Boxes represent the present ofdiscretized goals, such as lip closure.
Black curves repre-sent the output of the TADA system.The open-source TADA system (Nam and Gold-stein, 2006) estimates the positions of various artic-ulators during speech according to parameters thathave been carefully tuned by the authors of TADAaccording to a generic, speaker-independent repre-sentation of the vocal tract (Saltzman and Munhall,1989).
Given a word sequence and a syllable-to-gesture dictionary, TADA produces the continuous83tract variable paths that are necessary to produce thatsequence.
This takes into account various physio-logical aspects of human speech production, such asinterarticulator co-ordination and timing (Nam andSaltzman, 2003).In this study, we use TADA to produce estimatesof a global, high-level representation of speech com-mon to both dysarthric and non-dysarthric speakersalike.
Given a word sequence uttered by both typesof speaker, we produce five continuous curves pre-scribed by that word sequence in order to match ouravailable EMA data.
Those curves are lip apertureand protrusion (LA and LP), tongue tip constrictionlocation and degree (TTCL and TTCD, representingfront-back and top-down positions of the tongue tip,respectively), and lower incisor height (LIH).
Thesecurves are then compared against actually observedEMA data, as described below.4 ExperimentsFirst, in section 4.1, we ask whether the incorpo-ration of articulatory data is theoretically useful inreducing uncertainty in dysarthric speech.
Second,in section 4.2, we ask which of the two noisy chan-nel models in figure 3 best describe the observed be-haviour of dysarthric speech.Data for this study are collected as described as insection 2.
Here, we use data from three dysarthricspeakers with cerebral palsy (males M01 and M04,and female F03), as well as their age- and gender-matched counterparts from the general population(males MC01 and MC03, and female FC02).
Forthis study we restrict our analysis to 100 phrases ut-tered in common by all six speakers.4.1 EntropyWe measure the differential entropy of acoustics(H(Ac)), of articulation (H(Ar)), and of acousticsgiven knowledge of the vocal tract (H(Ac |Ar)) inorder to obtain theoretical estimates as to the utilityof articulatory data.
Table 3 shows these quantitiesacross the six speakers in this study.
As expected,the acoustics of dysarthric speakers are much moredisordered than for non-dysarthric speakers.
Oneunexpected finding is that there is very little differ-ence between speakers in terms of their entropy ofarticulation.
Although dysarthric speakers clearlylack articulatory dexterity, this implies that theynonetheless articulate with a level of consistencysimilar to their non-dysarthric counterparts1.
How-ever, the equivocation H(Ac |Ar) is an order of mag-nitude lower for non-dysarthric speakers.
This im-plies that there is very little ambiguity left in theacoustics of non-dysarthric speakers if we have si-multaneous knowledge of the vocal tract, but thatquite a bit of ambiguity remains for our dysarthricspeakers, despite significant reductions.Speaker H(Ac) H(Ar) H(Ac |Ar)Dys.M01 66.37 17.16 50.30M04 33.36 11.31 26.25F03 42.28 19.33 39.47Average 47.34 15.93 38.68Ctrl.MC01 24.40 21.49 1.14MC03 18.63 18.34 3.93FC02 16.12 15.97 3.11Average 19.72 18.60 2.73Table 3: Differential entropy, in nats, across dysarthricand control speakers for acoustic ac and articulatory ardata.Table 4 shows the average mutual information be-tween acoustics and articulation for each type ofspeaker, given knowledge of the phonological man-ner of articulation.
In table 1 we noted a prevalenceof pronunciation errors among dysarthric speakersfor plosives, but table 4 shows no particularly lowcongruity between acoustics and articulation for thismanner of phoneme.
Those pronunciation errorstended to be voicing errors, which would involve theglottis, which is not measured in this study.Table 4 appears to imply that there is little mu-tual information between acoustics and articulationin vowels across all speakers.
However, this is al-most certainly the result of our exclusion of tongueblade and tongue dorsum measurements in order tostandardize across speakers who could not managethese sensors.
Indeed, the configuration of the en-tire tongue is known to be useful in discriminat-ing among the vowels (O?Shaughnessy, 2000).
Anad hoc analysis including all three tongue sensorsfor speakers F03, MC01, MC03, and FC02 revealedmutual information between acoustics and articula-1This is borne out in the literature (Kent and Rosen, 2004).84MannerI(Ac;Ar)Dys.
Ctrl.plosives 10.92 16.47affricates 8.71 9.23fricatives 9.30 10.94nasals 13.29 15.10glides 11.92 12.68vowels 6.76 7.15Table 4: Mutual information I(Ac;Ar) of acoustics andarticulation for dysarthric and control subjects, acrossphonological manners of articulation.tion of 16.81 nats for F03 and 18.73 nats for thecontrol speakers, for vowels.
This is compared withmutual information of 11.82 nats for F03 and 13.88nats for the control speakers across all other man-ners.
The trend seems to be that acoustics are betterpredicted given more tongue measurements.In order to better understand these results, wecompare the distributions of the vowels in acousticspace across dysarthric and non-dysarthric speech.Vowels in acoustic space are characterized by thesteady-state positions of the first two formants (F1and F2) as determined automatically by applying thepre-emphasized Burg algorithm (Press et al, 1992).We fit Gaussians to the first two formants for eachof the vowels in our data, as exemplified in fig-ure 5 and compute the entropy within these distri-butions.
Surprisingly, the entropies of these distri-butions were relatively consistent across dysarthric(34.6 nats) and non-dysarthric (33.3 nats) speech,with some exceptions (e.g., iy).
However, vowelspaces overlap considerably more in the dysarthriccase signifying that, while speakers with CP can benearly as acoustically consistent as non-dysarthricspeakers, their targets in that space are not as dis-cernible.
Some research has shown larger varianceamong dysarthric vowels relative to our findings(Kain et al, 2007).
This may partially be due to ouruse of natural connected speech as data, rather thanrestrictive consonant-vowel-consonant non-words.4.2 Noisy channelOur task is to determine whether dysarthric speechis best represented as a distorted version of typi-cal speech, or if both dysarthric and typical speechought to be viewed as distortions of a common ab-300 400 500 600 700 800 900 1000 1100 1200 1300 14001000150020002500 iyih ehuwaaDysarthric maleF1 (Hz)F2 (Hz)ah300 400 500 600 700 800 900 1000 1100 1200 1300 14001000150020002500 iy ihehuw aaNon?dysarthric maleF1 (Hz)F2 (Hz) ahFigure 5: Contours showing first standard deviation inF1 versus F2 space for distributions of six representativevowels in continuous speech for the dysarthric and non-dysarthric male speakers.stract representation.
To explore this question, wedesign a transformation system that produces themost likely observation in one data space given itscounterpart in another and the statistical relationshipbetween the two spaces.
This transformation in ef-fect implements the noisy channel itself.To accomplish this, we learn probability distri-butions over our EMA data.
First, we collect alldysarthric data together and all non-dysarthric datatogether.
We then consider the acoustic (Ac) andarticulatory (Ar) subsets of these data.
In eachcase, we train Gaussian mixtures, each with 60 com-ponents, over 90% of the data in both dysarthricand non-dysarthric speech.
Here, each of the 60phonemes in the data is represented by one Gaussiancomponent, with the weight of that component de-termined by the relative proportion of 10 ms framesfor that phoneme.
Similarly, all training word se-quences are passed to TADA, and we train a mixtureof Gaussians on its articulatory output.Across all Gaussian mixtures, we end up with 5Gaussians tuned to various aspects of each phonemep: its dysarthric acoustics and articulation (NAcp (Yd)and NArp (Yd)), its control acoustics and articula-tion (NAcp (Yd) and NArp (Yd)), and its prescribed ar-ticulation from TADA (NArp (X)).
Each GaussianNAp(B) is represented by its mean ?
(A,B)p and its85covariance, ?
(A,B)p .
Furthermore, we compute thecross-covariance matrix between Gaussians for agiven phoneme (e.g., ?(Ac,Yc)?
(Ac,Yd)p is the cross-covariance matrix of the acoustics of the control (Yc)and dysarthric (Yd) speech for phoneme p).
Giventhese parameters, we estimate the most likely framein one domain given its counterpart in another.
Forexample, if we are given a frame of acoustics froma control speaker, we can synthesize the most likelyframe of acoustics for a dysarthric speaker, given anapplication of the noisy channel proposed by Hosomet al (2003) used to transform dysarthric speech tomake it more intelligible.
Namely, given a frame ofacoustics yc from a control speaker, we can estimatethe acoustics of a dysarthric speaker yd with:fAc(yc) =E(yd |yc)=P?i=1hi(yc)[?
(Ac,Yd)i +?(Ac,Yc)?
(Ac,Yd)i ?(?(Ac,Yc)i)?1?(yc??
(Ac,Yc)i)],(2)wherehi(yc) =?iN(yc;?
(Ac,Yc)i ,?
(Ac,Yc)i)?Pj=1 ?
jN(yc;?
(Ac,Yc)j ,?
(Ac,Yc)j) ,where ?p is the proportion of the frames of phonemep in the data.
Transforming between different typesand sources of data is accomplished merely by sub-stituting in the appropriate Gaussians above.We now measure how closely the transformeddata spaces match their true target spaces.
In eachcase, we transform test utterances (recorded, or syn-thesized with TADA) according to functions learnedin training (i.e., we use the remaining 10% of thedata for each speaker type).
These transformedspaces are then compared against their target spacein our data.
Table 5 shows the Gaussian mixturephoneme-level Kullback-Leibler divergences givenvarious types of source and target data, weighted bythe relative proportions of the phonemes.
Each pairof N-dimensional Gaussians (Ni with mean ?i andcovariance ?i) for a given phone and data type isKL divergence(10?2 nats)Type 1 Type 2 Acous.
Artic.Ctrl.
Dys.
25.36 3.23Ctrl.
?
Dys.
Dys.
17.78 2.11TADA?
Ctrl.
Ctrl.
N/A 1.69TADA?
Dys.
Dys.
N/A 1.84Table 5: Average weighted phoneme-level Kullback-Leibler divergences.compared withDKL(N0 ||N1) =12(ln(|?1||?0|)+ trace(?
?11 ?0)+(?1??0)T?
?11 (?1?
?0)?N).Our baseline shows that control and dysarthricspeakers differ far more in their acoustics than intheir articulation.
When our control data (bothacoustic and articulatory) are transformed to matchthe dysarthric data, the result is predictably moresimilar to the latter than if the conversion had nottaken place.
This corresponds to the noisy channelmodel of figure 3(a), whereby dysarthric speech ismodelled as a distortion of non-dysarthric speech.However, when we model dysarthric and controlspeech as distortions of a common, abstract repre-sentation (i.e., task dynamics) as in figure 3(b), theresulting synthesized articulatory spaces are moresimilar to their respective observed data than thearticulation predicted by the first noisy channelmodel.
Dysarthric articulation predicted by trans-formations from task-dynamics space differ signifi-cantly from those predicted by transformations fromcontrol EMA data at the 95% confidence interval.5 DiscussionThis paper demonstrates a few acoustic and articu-latory features in speakers with cerebral palsy.
First,these speakers are likely to mistakenly voice un-voiced plosives, and to delete fricatives regardless oftheir word position.
We suggest that it might be pru-dent to modify the vocabularies of ASR systems toaccount for these expected mispronunciations.
Sec-ond, dysarthric speakers produce sonorants signifi-cantly slower than their non-dysarthric counterparts.86This may present an increase in insertion errors inASR systems (Rosen and Yampolsky, 2000).Although not quantified in this paper, we detectthat a lack of articulatory control can often leadto observable acoustic consequences.
For example,our dysarthric data contain considerable involuntarytypes of velopharyngeal or glottal noise (often as-sociated with respiration), audible swallowing, andstuttering.
We intend to work towards methods ofexplicitly identifying regions of non-speech noise inour ASR systems for dysarthric speakers.We have considered the amount of statistical dis-order (i.e., entropy) in both acoustic and articula-tory data in dysarthric and non-dysarthric speak-ers.
The use of articulatory knowledge reduces thedegree of this disorder significantly for dysarthricspeakers (18.3%, relatively), though far less than fornon-dysarthric speakers (86.2%, relatively).
In real-world applications we are not likely to have access tomeasurements of the vocal tract; however, many ap-proaches exist that estimate the configuration of thevocal tract given only acoustic data (Richmond et al,2003; Toda et al, 2008), often to an average error ofless than 1 mm.
The generalizability of such workto new speakers (particularly those with dysarthria)without training is an open research question.We have argued for noisy channel models ofthe neuro-motor interface assuming that the path-way of motor command to motor activity is a lin-ear sequence of dynamics.
The biological realityis much more complicated.
In particular, the path-way of verbal motor commands includes severalsources of sensory feedback (Seikel et al, 2005) thatmodulate control parameters during speech (Gracco,1995).
These senses include exteroceptive stimuli(auditory and tactile), and interoceptive stimuli (par-ticularly proprioception and its kinesthetic sense)(Seikel et al, 2005), the disruption of which can leadto a number of production changes.
For instance,Abbs et al (1976) showed that when conduction inthe mandibular branches of the trigeminal nerve isblocked, the resulting speech has considerably morepronunciation errors, although is generally intelligi-ble.
Barlow (1989) argues that the redundancy ofsensory messages provides the necessary input to themotor planning stage, which relates abstract goals tomotor activity in the cerebellum.
As we continue todevelop our articulatory ASR models for dysarthricspeakers, one potential avenue for future research in-volves the incorporation of feedback from the cur-rent state of the vocal tract to the motor planningphase.
This would be similar, in premise, to theDIVA model (Guenther and Perkell, 2004).In the past, we have shown that ASR systems thatadapt non-dysarthric acoustic models to dysarthricdata offer improved word-accuracy rates, but witha clear upper bound approximately 75% below thegeneral population (Rudzicz, 2007).
Incorporat-ing articulatory knowledge into such adaptation im-proved accuracy further, but with accuracy stillapproximately 60% below the general population(Rudzicz, 2009).
In this paper, we have demon-strated that dysarthric articulation can be more ac-curately represented as a distortion of an underlyingmodel of abstract speech goals than as a distortion ofnon-dysarthric articulation.
These results will guideour continued development of speech systems aug-mented with articulatory knowledge, particularly theincorporation of task dynamics.AcknowledgmentsThis research is funded by Bell University Labs, theNatural Sciences and Engineering Research Councilof Canada, and the University of Toronto.ReferencesJames H. Abbs, John W. Folkins, and Murali Sivarajan.1976.
Motor Impairment following Blockade of theInfraorbital Nerve: Implications for the Use of Anes-thetization Techniques in Speech Research.
Journal ofSpeech and Hearing Research, 19(1):19?35.H.B.
Barlow.
1989.
Unsupervised learning.
NeuralComputation, 1(3):295?311.Joseph R Duffy.
2005.
Motor Speech Disorders:Substrates, Differential Diagnosis, and Management.Mosby Inc.Hans-Joachim Freund, Marc Jeannerod, Mark Hallett,and Ramo?n Leiguarda.
2005.
Higher-order motor dis-orders: From neuroanatomy and neurobiology to clin-ical neurology.
Oxford University Press.Vincent L. Gracco.
1995.
Central and peripheral compo-nents in the control of speech movements.
In Freder-icka Bell-Berti and Lawrence J. Raphael, editors, In-troducing Speech: Contempory Issues, for KatherineSafford Harris, chapter 12, pages 417?431.
AmericanInstitute of Physics press.87Frank H. Guenther and Joseph S. Perkell.
2004.
A neuralmodel of speech production and its application to stud-ies of the role of auditory feedback in speech.
In BenMaassen, Raymond Kent, Herman Peters, Pascal VanLieshout, and Wouter Hulstijn, editors, Speech MotorControl in Normal and Disordered Speech, chapter 4,pages 29?49.
Oxford University Press, Oxford.John-Paul Hosom, Alexander B. Kain, Taniya Mishra,Jan P. H. van Santen, Melanie Fried-Oken, and Jan-ice Staehely.
2003.
Intelligibility of modifications todysarthric speech.
In Proceedings of the IEEE Inter-national Conference on Acoustics, Speech, and SignalProcessing (ICASSP ?03), volume 1, pages 924?927,April.Marco F. Huber, Tim Bailey, Hugh Durrant-Whyte, andUwe D. Hanebeck.
2008.
On entropy approximationfor Gaussian mixture random vectors.
In Proceed-ings of the 2008 IEEE International Conference on InMultisensor Fusion and Integration for Intelligent Sys-tems, pages 181?188, Seoul, South Korea.Alexander B. Kain, John-Paul Hosom, Xiaochuan Niu,Jan P.H.
van Santen, Melanie Fried-Oken, and Jan-ice Staehely.
2007.
Improving the intelligibil-ity of dysarthric speech.
Speech Communication,49(9):743?759, September.Ray D. Kent and Kristin Rosen.
2004.
Motor control per-spectives on motor speech disorders.
In Ben Maassen,Raymond Kent, Herman Peters, Pascal Van Lieshout,and Wouter Hulstijn, editors, Speech Motor Controlin Normal and Disordered Speech, chapter 12, pages285?311.
Oxford University Press, Oxford.Ray D. Kent, Gary Weismer, Jane F. Kent, and John C.Rosenbek.
1989.
Toward phonetic intelligibility test-ing in dysarthria.
Journal of Speech and Hearing Dis-orders, 54:482?499.Aida C. G. Verdugo Lazo and Pushpa N. Rathie.
1978.On the entropy of continuous probability distributions.IEEE Transactions on Information Theory, 23(1):120?122, January.Keith L. Moore and Arthur F. Dalley.
2005.
ClinicallyOriented Anatomy, Fifth Edition.
Lippincott, Williamsand Wilkins.Hosung Nam and Louis Goldstein.
2006.
TADA (TAskDynamics Application) manual.Hosung Nam and Elliot Saltzman.
2003.
A compet-itive, coupled oscillator model of syllable structure.In Proceedings of the 15th International Congress ofPhonetic Sciences (ICPhS 2003), pages 2253?2256,Barcelona, Spain.Douglas O?Shaughnessy.
2000.
Speech Communications?
Human and Machine.
IEEE Press, New York, NY,USA.William H. Press, Saul A. Teukolsky, William T. Vetter-ling, and Brian P. Flannery.
1992.
Numerical Recipesin C: the art of scientific computing.
Cambridge Uni-versity Press, second edition.Korin Richmond, Simon King, and Paul Taylor.
2003.Modelling the uncertainty in recovering articulationfrom acoustics.
Computer Speech and Language,17:153?172.Kristin Rosen and Sasha Yampolsky.
2000.
Automaticspeech recognition and a review of its functioning withdysarthric speech.
Augmentative & Alternative Com-munication, 16(1):48?60, Jan.Frank Rudzicz.
2007.
Comparing speaker-dependentand speaker-adaptive acoustic models for recognizingdysarthric speech.
In Proceedings of the Ninth Inter-national ACM SIGACCESS Conference on Computersand Accessibility, Tempe, AZ, October.Frank Rudzicz.
2009.
Applying discretized articulatoryknowledge to dysarthric speech.
In Proceedings ofthe 2009 IEEE International Conference on Acoustics,Speech, and Signal Processing (ICASSP09), Taipei,Taiwan, April.Elliot L. Saltzman and Kevin G. Munhall.
1989.
A dy-namical approach to gestural patterning in speech pro-duction.
Ecological Psychology, 1(4):333?382.J.
Anthony Seikel, Douglas W. King, and David G.Drumright, editors.
2005.
Anatomy & Physiology:for Speech, Language, and Hearing.
Thomson Del-mar Learning, third edition.Claude E. Shannon.
1949.
A Mathematical Theory ofCommunication.
University of Illinois Press, Urbana,IL.Tomoki Toda, Alan W. Black, and Keiichi Tokuda.2008.
Statistical mapping between articulatory move-ments and acoustic spectrum using a Gaussian mix-ture model.
Speech Communication, 50(3):215?227,March.Alan Wrench.
1999.
The MOCHA-TIMIT articulatorydatabase, November.Yana Yunusova, Gary Weismer, John R. Westbury, andMary J. Lindstrom.
2008.
Articulatory movementsduring vowels in speakers with dysarthria and healthycontrols.
Journal of Speech, Language, and HearingResearch, 51:596?611, June.Yana Yunusova, Jordan R. Green, and Antje Mefferd.2009.
Accuracy Assessment for AG500, Electromag-netic Articulograph.
Journal of Speech, Language,and Hearing Research, 52:547?555, April.Victor Zue, Stephanie Seneff, and James Glass.
1989.Speech Database Development: TIMIT and Beyond.In Proceedings of ESCA Tutorial and Research Work-shop on Speech Input/Output Assessment and SpeechDatabases (SIOA-1989), volume 2, pages 35?40,Noordwijkerhout, The Netherlands.88
