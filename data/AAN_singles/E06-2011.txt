Esfinge ?
a Question Answering System in the Web using the WebLu?s Fernando CostaLinguateca at SINTEF ICTPb 124 Blindern,0314 Oslo, Norwayluis.costa@sintef.noAbstractEsfinge is a general domain Portuguesequestion answering system.
It tries totake advantage of the great amount of in-formation existent in the World WideWeb.
Since Portuguese is one of the mostused languages in the web and the webitself is a constantly growing source ofupdated information, this kind of tech-niques are quite interesting and promis-ing.1 IntroductionThere are some question answering systems forPortuguese like the ones developed by the Uni-versity of ?vora (Quaresma and Rodrigues,2005) and Priberam (Amaral et al 2005), butthese systems rely heavily on the pre-processingof document collections.
Esfinge explores a dif-ferent approach: instead of investing in pre-processing corpora, it tries to use the redundancyexistent in the web to find its answers.
In addi-tion it has an interface on the web where every-one can pose questions to the system(http://www.linguateca.pt/Esfinge/).Esfinge is based on the architecture proposedin (Brill, 2003).
Brill suggests that it is possibleto obtain interesting results, applying simpletechniques to large quantities of data.
The Portu-guese web can be an interesting resource for sucharchitecture.
Nuno Cardoso (p.c.)
is compiling acollection of pages from the Portuguese web andthis collection will amount to 8.000.000 pages.Using the techniques described in (Aires andSantos, 2002) one can estimate that Google andAltavista index 34,900,000 and 60,500,000 pagesin Portuguese respectively.The system is described in detail in (Costa,2005a, 2005b).2 System ArchitectureThe inputs to the system are questions in naturallanguage.
Esfinge begins by transforming thesequestions into patterns of plausible answers.
Asan example, take the question Onde fica Braga?
(Where is Braga located?).
This generates thepattern ?Braga fica?
(?Braga is located?)
with ascore of 20, that can be used to search for docu-ments that might contain an answer to the ques-tion.
The patterns used by the system have thesame syntax as the one commonly used in searchengines, quoted text meaning a phrase pattern.Then, these patterns are searched in the Web(using Google at the moment) and the systemextracts the first 100 document snippets createdby the search engine.
Some tests performed withEsfinge showed that certain types of sites maycompromise the quality of the returned answers.With that in mind, the system uses a list of ad-dress patterns which are not to be considered (itdoes not consider documents stored in addressesthat match these patterns).
The patterns in thislist (such as blog, humor, piadas) were createdmanually based on the fore mentioned tests.The next step involves the extraction of wordn-grams (length 1 to 3) from the document pas-sages obtained previously.
The system uses theNgram Statistic Package (Banerjee and Pedersen,2003) for that purpose.These n-grams are scored using the formula:N-gram score = ?
(F * S * L), through thefirst 100 snippets resulting from the web search;where F is the n-gram frequency, S is the scoreof the search pattern that recovered the documentand  L is the n-gram length.Identifying the type of question can be quite use-ful in the task of searching for an answer.
For127example a question beginning with When sug-gests that most likely the answer will be a date.Esfinge has a module that uses the named entityrecognition (NER) system SIEMES to detectspecific types of answers.
This NER system de-tects and classifies named entities in a widerange of categories (Sarmento, submitted).
Es-finge used a sub-set of these categories, namelyHuman, Country, Settlement (including cities,villages, etc), Geographical Locations (locationswith no political entailment, like for exampleAfrica), Date and Quantity.
When the type ofquestion leads to one or more of those namedentity categories, the 200 best scored word n-grams from the previous modules are submittedto SIEMES.
The results from the NER systemare then analysed in order to check whether itrecognizes named entities classified as one of thedesired categories.
If such named entities arerecognized, their position in the ranking of pos-sible answers is pushed to the top (and they willskip the filter ?Interesting PoS?
describedahead).In the next module the list of possible answers(by ranking order) is submitted to several filters:x A filter that discards words contained inthe questions.
Ex: the answer Eslov?quiais not desired for the question Qual ?
acapital da Eslov?quia?
(What is the capitalof Slovakia?)
and should be discarded.x A filter that rejects answers included in alist of ?undesired answers?.
This list in-cludes very frequent words that do not an-swer questions alone (like pes-soas/persons, nova/new, lugar/place,grandes/big, exemplo/example).
It wasbuilt with the help of Esfinge log (whichrecords all the answers analysed by thesystem).
Later some other answers wereadded to this list, as a result of tests per-formed with the system.
The list includesnow 92 entries.x A filter that uses the morphological ana-lyzer jspell (Sim?es and Almeida, 2002) tocheck the PoS of the various tokens ineach answer.
This filter rejects the answerswhose first and last answer are not com-mon or proper nouns, adjectives or num-bers.
Using this simple technique it is pos-sible to discard incomplete answers begin-ning or ending with prepositions or inter-jections for example.Figure 1 describes the algorithm steps related tonamed entity recognition/classification in the n-grams and n-gram filtering.Figure 1.
Named entity recognition/classificationand filtering in the n-gramsThe final answers of the system are the bestscored candidate answers that manage to gothrough all the previously described filters.
Thereis a final step in the algorithm where the systemsearches for longer answers.
These are answersthat include one of the best candidate answersand also pass all the filters.
For example, the bestscored answer for the question Who is the Britishprime minister?
might be just Tony.
However, ifthe system manages to recover the n-gram TonyBlair and this n-gram also passes all the filters, itwill be the returned answer.Figure 2 gives an overview of the several stepsof the question answering algorithm.Figure 2.
The architecture of Esfinge128Figure 3 shows how the system returns the re-sults.
Each answer is followed by some passagesof documents from where the answers were ex-tracted.
Clicking on a passage, the user navigatesto the document from which the passage was ex-tracted.
This enables the user to check whetherthe answer is appropriate or to find more infor-mation related to the formulated question.Figure 3.
Esfinge answers to the question ?Whois the Russian president?
?At the moment, Esfinge is installed in a Pentium4 ?
2.4 GHz machine running Red Hat Linux 9,with 1 GB of RAM memory and it can take fromone to two minutes to answer a question.Figure 4 shows the modules and data flow inthe QA system.
The external modules are repre-sented as white boxes, while the modules spe-cifically developed for the QA system are repre-sented as grey boxes.Figure 4.
Modules and data flow3 ResultsIn order to measure the evolution and the per-formance of the different techniques used, Es-finge participated in the QA task at CLEF in2004 and 2005 (Vallin et al 2005).In this task the participants receive 200 ques-tions prepared by the organization and a docu-ment collection.
The systems are then supposedto return the answers to each question, indicatingalso the documents that support each of the an-swers.
The questions are mainly factoid (ex: Whois the president of South Africa?
), but there arealso some definitions (ex: Who is AngelinaJolie?
).Esfinge needed some extra features to partici-pate in the QA task at CLEF.
While in its origi-nal version, the document retrieval task was leftto Google, in CLEF it is necessary to search inthe CLEF document collection in order to returnthe documents supporting the answers.
For thatpurpose this document collection was encodedwith CQP (Christ et al 1999) and a documentretrieval module was added to the system.Two different strategies were tested.
In thefirst one, the system searched the answers in theWeb and used the CLEF document collection toconfirm these answers.
In the second experiment,Esfinge searched the answers in the CLEFdocument collection only.Table 1 presents the results obtained by Es-finge at CLEF 2004 and 2005.
Due to these par-ticipations some errors were detected and cor-rected.
The table also includes the results ob-tained by the current version of the system withthe CLEF questions in 2004 and 2005, as well asthe results of the best system (U. Amsterdam)and the best system for Portuguese (University of?vora) in 2004 and 2005 (where Priberam?s sys-tem for Portuguese got the best results among allthe systems).System NumberofquestionsNumber (%)of exactanswersEsfinge 199 30 (15%)Esfinge (currentversion)199 55 (28%)Best system forPortuguese199 56 (28%)CLEF2004Best system 200 91 (46%)Esfinge 200 48 (24%)Esfinge (currentversion)200 61 (31%)CLEF2005Best system 200 129 (65%)Table 1.
Results at CLEF 2004 and 2005We tried to investigate whether CLEF questionsare the most appropriate to evaluate a system likeEsfinge.
With that intention 20 questions werepicked randomly and Google was queried tocheck whether it was possible to find answers inthe first 100 returned snippets.
For 5 of the ques-tions no answers were found, there were few oc-currences of the right answer (3 or less) for 8 ofthe questions and for only 7 of the questionsthere was some redundancy (4 or more right an-129swers).
There are more details about the evalua-tion of the system in (Costa, 2006).4 ConclusionsEven though the results in CLEF 2005 improvedcompared to CLEF 2004, they are still far fromthe results obtained by the best systems.
How-ever, there are not many question answering sys-tems developed for Portuguese and the existingones rely heavily on the pre-processing of docu-ment collections.
Esfinge tries to explore a dif-ferent angle, namely the use of the web as a cor-pus where information can be found and ex-tracted.
It is not proved that CLEF questions arethe most appropriate to evaluate the system.
Inthe experiment described in the previous section,it was possible to get some answer redundancy inthe web for less than half of the analyzed ques-tions.
We plan to study search engine logs, inorder to find whether it is possible to build aquestion collection with real users?
questions.Since Esfinge is a project in the scope of Lin-guateca (http://www.linguateca.pt), it followsLinguateca?s main assumptions.
For example,the one stating that all research results should bemade public.
The web interface, where everyonecan freely test the system was the first step inthat direction, and now the source code of mod-ules used in the system is freely available tomake it more useful for other researchers in thisarea.AcknowledgementsI thank Diana Santos for reviewing previous ver-sions of this paper, Alberto Sim?es for the hintson using the perl module ?jspell?.
Lu?s Sar-mento, Lu?s Cabral and Ana Sofia Pinto for sup-porting the use of the NER system SIEMES.This work is financed by the Portuguese Funda-?
?o para a Ci?ncia e Tecnologia through grantPOSI/PLP/43931/2001, co-financed by POSI.ReferencesRachel Aires & Diana Santos.
"Measuring the Web inPortuguese".
In Brian Matthews, Bob Hopgood &Michael Wilson (eds.
), Euroweb 2002 conference(Oxford, UK, 17-18 December 2002), pp.
198-199.Carlos Amaral et al 2005.
?Priberam?s question an-swering system for Portuguese?.
In Cross Lan-guage Evaluation Forum: Working Notes for theCLEF 2005 Workshop (CLEF 2005) (Vienna, Aus-tria, 21-23 September 2005).Satanjeev Banerjee and Ted Pedersen.
2003.
?TheDesign, Implementation, and Use of the NgramStatistic Package?.
In: Proceedings of the FourthInternational Conference on Intelligent Text Proc-essing and Computational Linguistics (MexicoCity, February 2003) pp.
370-381.Eric Brill.
2003.
?Processing Natural Language with-out Natural Language Processing?.
Proceedings ofthe Fourth International Conference on IntelligentText Processing and Computational Linguistics.Mexico City, pp.
360-9.Oliver Christ, Bruno M. Schulze, Anja Hofmann, andEsther K?nig.
1999.
The IMS Corpus Workbench:Corpus Query Processor (CQP): User's Manual.University of Stuttgart, March 8, 1999 (CQP V2.2)Lu?s Costa.
2005.
?First Evaluation of Esfinge - aQuestion Answering System for Portuguese?.
InCarol Peters et al(eds.
), Multilingual InformationAccess for Text, Speech and Images: 5th Workshopof the Cross-Language Evaluation Forum (CLEF2004) (Bath, UK, 15-17 September 2004), Heidel-berg, Germany: Springer.
Lecture Notes in Com-puter Science, pp.
522-533.Lu?s Costa.
2005.
?20th Century Esfinge (Sphinx)solving the riddles at CLEF 2005?.
In Cross Lan-guage Evaluation Forum: Working Notes for theCLEF 2005 Workshop (CLEF 2005) (Vienna, Aus-tria, 21-23 September 2005).Lu?s Costa.
2006.
?Component evaluation in a ques-tion answering system?.
In Proceedings of LREC2006 (Genoa, Italy, 24-26 May 2006).Paulo Quaresma and Irene Rodrigues.
2005.
?A LogicProgramming-based Approach to theQA@CLEF05 Track?.
In Cross Language Evalua-tion Forum: Working Notes for the CLEF 2005Workshop (CLEF 2005) (Vienna, Austria, 21-23September 2005).Lu?s Sarmento.
?SIEM?S ?
a named entity recognizerfor Portuguese relying on similarity rules?
(submit-ted).Alberto Sim?es and Jos?
Jo?o Almeida.
2002.?Jspell.pm - um m?dulo de an?lise morfol?gicapara uso em Processamento de LinguagemNatural?.
In: Gon?alves, A.
& Correia, C.N.
(eds.
):Actas do XVII Encontro da Associa?
?o Portuguesade Lingu?stica (APL 2001) (Lisboa, 2-4 Outubro2001).
APL Lisboa, pp.
485-495.Alessandro Vallin et al 2005.
?Overview of theCLEF 2005 Multilingual Question AnsweringTrack?.
In Cross Language Evaluation Forum:Working Notes for the CLEF 2005 Workshop(CLEF 2005) (Vienna, Austria, 21-23 September2005).130
