First Joint Workshop on Statistical Parsing of Morphologically Rich Languagesand Syntactic Analysis of Non-Canonical Languages, pages 74?81 Dublin, Ireland, August 23-29 2014.The effect of disfluencies and learner errors on the parsing of spokenlearner languageAndrew Caines Paula ButteryInstitute for Automated Language Teaching and AssessmentDepartment of Theoretical and Applied LinguisticsUniversity of Cambridge, Cambridge, U.K.(apc38|pjb48)@cam.ac.ukAbstractNLP tools are typically trained on written data from native speakers.
However, research intolanguage acquisition and tools for language teaching & proficiency assessment would benefitfrom accurate processing of spoken data from second language learners.
In this paper we dis-cuss manual annotation schemes for various features of spoken language; we also evaluate theautomatic tagging of one particular feature (filled pauses) ?
finding a success rate of 81%; and weevaluate the effect of using our manual annotations to ?clean up?
the transcriptions for sentenceparsing, resulting in a 25% improvement in parse success rate by completely cleaning the texts ofdisfluencies and errors.
We discuss the need to adapt existing NLP technology to non-canonicaldomains such as spoken learner language, while emphasising the worth of continued integrationof manual and automatic annotation.1 IntroductionNatural language processing (NLP) tools are typically trained on written data from native speakers.However, research into language acquisition and tools for language proficiency assessment & languageteaching ?
such as learner dialogue and feedback systems ?
would benefit from accurate processing ofspoken data from second language learners.
Being able to convert the text from unparseable to parseableform will enable us to (a) posit a target hypothesis that the learner intended to produce, and (b) providefeedback on this target based on the information removed or repaired in achieving that parseable form.To proceed towards this goal, we need to adapt current NLP tools to the non-canonical domain ofspoken learner language in a persistent fashion rather than use ad hoc post-processing steps to ?correct?the non-canonical data.
Outcomes of this approach have been reported in the literature (e.g.
Rimell &Clark (2009) in the biomedical domain; Caines & Buttery (2010) for spoken language).
These fullyadaptive approaches require large amounts of annotated data to be successful and, as we intend to workalong these lines in future, the discussion in this paper is pointed in that direction.The work presented here will act as a foundation for more permanent adaptations to existing tools.We annotate transcriptions of speech for linguistic features that are known to interfere with standardNLP to assess whether large-scale annotation of these features will be useful for training purposes.
Ob-vious instances of this include disfluencies (e.g.
filled pauses, false starts, repetition), formal errors ofmorphology and syntax, as well as ?errors?
of word and phrase selection1.Since manual annotation is costly in terms of time and often money, one might question whether somany feature types are strictly necessary or even helpful for the task in hand.
Indeed, filled pausessuch as ?oh?
and ?um?
are already accounted for in the part-of-speech (POS) tagset we use (CLAWS2(Garside, 1987)); and one might also argue that lexico-semantic errors might be dismissed a priori onthe assumption that both the original and proposed forms are of the same POS (and thus won?t affecta parser that performs tagging before the parse).
We investigate the contribution of these features toThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1The word ?error?
appears here in quotes as it might be argued that questionable lexical selections are more a matter ofinfelicity and improbability than any strict dichotomy; we put this concern aside for now as a matter for future research.74parsing success.
From a theoretical perspective we are interested in these features with regard to secondlanguage acquisition and therefore need to analyse them closely.In this paper we describe our initial efforts to address the challenge of parsing learner speech withtools trained on native speaker writing.
We also present empirical results that demonstrate the utility ofannotated spoken transcription with respect to both tagging and parsing.
We investigate: [i] the frequencyof disfluencies, formal errors of morpho-syntax, and idiomatic errors of lexico-semantics in a corpus ofspoken learner language; [ii] the accuracy of part-of-speech labels produced by the tagger associatedwith the Robust Accurate Statistical Parsing System (RASP (Briscoe et al., 2006)) for a particular typeof disfluency (the filled pause)2; [iii] parse success rates and parse likelihoods using the RASP Systemwith the texts in various ?modes?
ranging from unaltered transcription to fully edited and corrected3.We find that in our spoken learner corpus of 2262 words, (i) around a quarter of words are annotatedas disfluencies or errors; (ii) 81% of filled pauses were correctly tagged, meaning 1 in 5 are incorrectlytagged; (iii) mean parse likelihood for the text ?as is?, unaltered, is ?2.599 with a parse success rate of47%, whereas completely ?cleaned up?
text improves those scores to ?1.995 and 72%4.
We discuss theimplications of these results below, along with the background context for our study and a more detaileddescription of our investigations.2 BackgroundPrevious analyses of the NLP of learner language include various experiments on the tagging and parsingof errorful text.
Geertzen et al.
(2013) employed the Stanford parser on written English learner data andachieved labelled and unlabelled attachment scores (LAS, UAS)5 of 89.6% and 92.1%.
They found thaterrors at the morphological level lead to incorrect POS-tagging, which in turn can result in an erroneousparse.
Others have focused only on the POS-tagging of written learner corpora ?
for example withEnglish (van Rooy and Sch?fer, 2003) and French learner data (Thou?sny, 2011) ?
demonstrating thatpost-hoc corrections for the most frequent tagging errors results in significant parse error reduction.In other investigations of standard NLP tools on learner corpora, Ott & Ziai (2010) report generalrobustness using MaltParser on written German learner language; however, they found that by manuallycorrecting POS tags, LAS improved from 79.15% to 85.71% and UAS from 84.81% to 90.22%.
Wagner& Foster (2009) ran a series of parsing experiments using parallel errorful/corrected corpora, includinga spoken learner corpus in which the likelihood of the highest ranked tree for corrected sentences washigher than that of uncorrected sentences in 69.6% of 500 instances.
Taken together, these studies suggestthat existing NLP tools remain robust to learner data, even more so if the original texts can be correctedand if the tagging stage is in some way verified, or adapted (e.g.
Zinsmeister et al.
(2014)).On the other hand, D?az-Negrillo et al.
(2010) argue that treating learner data as a ?noisy variant?
ofnative language glosses over systematic differences between the two, and instead call for a dedicated tag-ging format for ?interlanguage?, one that encodes distributional, morphological and lexical information.For instance, ?walks?
in ?John has walks?
would morphologically be tagged as a present tense 3rd-personverb, but distributionally tagged as a past participle6.
This is the kind of adaptation of existing tools thatwe advocate, though we would add that this system should be available for not just interlanguage but alldata, allowing for non-canonical language use by native speakers as much as learners.As for spoken language, Caines & Buttery (2010) among others suggest that adaptation can also bemade to the parser, such that it enters a ?speech-aware mode?
in which the parser refers to additionaland/or replacement rules adapted to the particular features of spoken language.
They demonstrated thiswith the omission of auxiliary verbs in progressive aspect sentences (?you talking to me?
?, ?how you2The RASP POS-tagger was evaluated on the 560 randomly selected sentences from The Wall Street Journal that constitutethe PARC dependency bank (DepBank; (King et al., 2003)) and achieved 97% accuracy (Briscoe et al., 2006).3The RASP parser achieves a 79.7% microaveraged F1 score on grammatical relations in DepBank (Briscoe and Carroll,2006).4N.B.
the closer the parse likelihood to zero, the more probable the parse in the English language.5LAS indicates the proportion of tokens that are assigned both the correct head and the correct dependency label; UASindicates the proportion of tokens assigned the correct head, irrespective of dependency label.6We thank reviewer #1 for this example.75doing??)
and achieved a 30% improvement in parsing success rate for this construction type.3 The corpusOur speech data consist of recordings from Business Language Testing Service (BULATS) speakingtests7.
In the test, learners are required to undertake five tasks; we exclude the tasks involving briefquestion-answering (?can you tell me your full name?
?, ?where are you from?
?, etc) and elicited imitation,leaving us with three free-form speech tasks.
For this particular test the tasks were: [a] talk about someadvice from a colleague (monologue), [b] talk about a series of charts from Business Today magazine(monologue), [c] give advice on starting a new retail business (dialogue with examiner).In our full dataset the candidates come from India, Pakistan and Brazil, with various first languages(L1) including Hindi, Gujarati, Malayalam, Urdu, Pashto and Portuguese, and an age range of 16 to 47at the time of taking the test.
However, in this analysis we have only sampled recordings from candidatesdeemed to be at ?B2?
upper intermediate level on the CEFR scale8, so that the proficiency level oflanguage used (and how that relates to NLP) is controlled for.
In addition the L1s in our sample areGujarati, Punjabi and Urdu only.
This gives us a sample corpus of 2262 tokens in ?as-is?
format (i.e.
thetrue transcriptions before any corrections are made).4 Manual annotationThe recordings were manually transcribed and annotated for various features falling into three categoriesdescribed and exemplified in the following non-exhaustive list.?
disfluencies ?
interruptions to the flow of otherwise fluent speech;?
<fp> filled pauses (tokens such as uh, er, um that serve to fill time and hold the turn) and <repn="n"> repetition (the speaker repeats a word or phrase one or several times):?or the other way is to <fp>um</fp> <rep n="1">is to</rep> raise finance??
<false> false starts ?
the speaker begins to express a word or phrase which he then corrects:?in two thousand eight it was <false>thirty five p</false> thirty percent??
formal errors of morpho-syntax, such as number agreement, verb inflection and word order errors;?
noun form: ?for becoming a chartered <NS type="FN"><i>accountants</i><c>accountant</c></NS>??
missing verb: ?as the charts <NS type="MV"><c>show</c></NS> its sales increased??
word order: ?<NS type="W"><i>how it would be help for you mention</i><c>mention howit helped you</c></NS>??
idiomatic ?errors?
?
infelicities in lexical selection, failure to express intended meaning, or less-than-natural phrasing;?
idiomatic: ?all my class <NS type="ID"><i>fellows</i><c>mates</c></NS>??
idiomatic: ?to <NS type="ID"><i>get in</i><c>make a</c></NS> profit??
replace quantifier: ?for a bank to grant us <NS type="RQ"><i>some</i><c>a</c></NS> loan?The annotation scheme for formal and idiomatic errors comes from the project to annotate the CambridgeLearner Corpus (Briscoe et al., 2010).
The ?error zone?
is denoted by <NS> tags, with any originaltoken(s) enclosed by <i> and any proposed correction enclosed by <c>.
The various error types aredefined in Nicholls (2003) and the categories are similar to the ones given: either self-defining (?ID?
foridiom error, ?W?
for word order, etc) or a combination of operation plus part-of-speech (?FN?
form ofnoun, ?MV?
missing verb, ?RQ?
replace quantifier, etc).In Table 1 we report the number of errors and disfluencies found in our corpus along with a relativefrequency per 100 words.
Just under a quarter of the thousand tokens in our corpus are affected bydisfluencies and errors, with the former being far more prevalent.7We thank Cambridge English Language Assessment for releasing these recordings for this pilot study; for further informa-tion on BULATS go to http://www.bulats.org/8The ?Common European Framework of Reference for Languages?
: a schema for grading an individual learner?s languagelevel.
For further information go to http://www.coe.int/lang-CEFR76All transcription and annotation has been carried out by a single annotator (the first author).
It wouldbe interesting to obtain measures of inter-annotator agreement to assess the extent to which the nature oferror judgement (particularly in judgements as to idiomaticity) is subjective.type instances in corpus relative frequency (per 100 words)disfluency 316 14formal error 143 6idiomatic error 70 3total 529 23Table 1: Error counts in our corpus5 Automated annotation: part-of-speech taggingSince filled pauses such as ?er?
and ?um?
are included in the CLAWS2 tagset used by the RASP Systemas UH, ?interjection?, one might question the worth of manually annotating filled pauses (FPs).
Of thedisfluency set, it might be one small time-saving to leave these to the tagger.
However, ?interjection?
isnot a homogeneous set, as UH also covers exclamations of surprise (?oh?)
and assent (?yes?).
Moreover,we find that the POS-tagging of tokens annotated as FPs is not entirely appropriate in this non-canonicaldomain.
Table 2 shows that the majority of FPs are correctly tagged UH, though others are tagged asnouns (NN), verbs (VV), adjectives (JJ), adverbs (RR) and foreign words (&FW)9.Token UH &FW JJ NN RR VV totaler 104 0 0 0 0 0 104mm 0 0 0 8 0 0 8uh 0 0 0 1 2 4 7um 2 5 0 0 0 0 7nuh 0 0 0 0 2 1 3buh 0 0 0 1 0 1 2other 2 0 1 0 0 0 3total 108 5 1 10 6 9 134Table 2: POS tagging of filled pausesOne possible solution is to append a dictionary of known FP tokens to the tagger, and specify thatthey should be tagged UH, or even better, a new tag such as FP.
But as the Table demonstrates, thereare standard, highly frequent FPs such as ?er?, ?uh?
and ?um?, and then there are novel forms such as?nuh?, ?buh?
and ?nna?
which we found to be rather idiosyncratic ?
i.e.
there might be novel FPs for everyindividual.
Moreover, the introduction of a closed class depends on consistent transcription practice, notnecessarily a given with even a lone annotator, let alone more than one.Automatic identification and repair of disfluencies is a well-developed research topic, with continuingrefinements to joint parsing and disfluency detection models (e.g.
Qian & Liu (2013), Rasooli & Tetreault(2014), Honnibal & Johnson (2014)), plus applied work in the domains of automatic speech recognition(Fitzgerald et al., 2009) and machine translation (Cho et al., 2014).
We note the linguistic rules includedin the Lease, Johnson & Charniak (2006) tree adjoining grammar (TAG) noisy-channel model ?
lexical,POS and syntactic rules that reduce errors in the TAG model.
This is another case of improvements toNLP tools thanks to data-driven linguistic insight, and a design that we could incorporate into our workon automated assessment and feedback.9The ?other?
filled pauses are singleton forms: eh, nna, ah.776 Automated annotation: sentence parsingIn this section we report the results of our parsing experiment in which transcribed learner utteranceswere processed by the RASP system in four different forms:(A) as-is: without alteration;(B) less-disfluency: with disfluencies removed;(C) less-form-error: with morpho-syntactic errors corrected;(D) less-lex-error: with semantic/idiomatic improvements.We investigated the effect on the parsing output of each transcription format compared to the (A) formatas a baseline.
We processed each format in turn singularly, as well as cumulative combinations of (B),(C) and (D) in every possible order.
The results are set out in Table 3, with mean likelihoods of thehighest ranked parse for each sentence (?
)10, differences between this mean and the baseline whereapplicable (?base), and success rates in terms of non-fragmentary tree outputs (i.e.
parses labelled otherthan ?T/frag?
in the RASP System).mode ?
?base ?T/frag mode ?
?base ?T/frag mode ?
?base ?T/frag(A) ?2.599 0 .471 (A) ?2.599 0 .471 (A) ?2.599 0 .471(B) ?2.094 +.505 .623 (BC) ?2.032 +.567 .689 (BCD) ?1.995 +.604 .715(C) ?2.574 +.025 .484 (BD) ?2.049 +.550 .649 - - -(D) ?2.563 +.036 .503 (CD) ?2.545 +.054 .523 - - -Table 3: Mean parse likelihoods, deltas to baseline and parse success rates in all transcription modesAs can be seen in Table 3, the removal of disfluencies (B) is the single move of greatest benefit toparse likelihood scores and parse tree success rates compared to the ?as-is?
baseline (A).
The correctionof morpho-syntactic (C) and idiomatic errors (D) have a lesser effect.
All pairings have a positive effecton parse likelihoods, especially those featuring disfluency removal (B); and the three ?corrective?
stepscombined (BCD) have the greatest effect of all.However, we show by analysis of two candidates in our corpus that these effects can differ on anindividual basis.
In Figure 1, the candidate on the left has a less pronounced effect of disfluency removal(B) compared to the baseline (A) than the candidate on the right.
The effect of both formal (C) andidiomatic (D) error correction are also seen to make improvements over (A), which is not the case forthe second candidate.
Such observations serve as a reminder that when generalising about overall corpuspatterns we collapse over so many individual language models.
It may well turn out that disfluencies arean especially idiosyncratic type of language use, an avenue we will explore in future work.7 DiscussionIn this paper we have investigated NLP of transcribed learner speech, questioning how tools trained onnative speaker written data would handle such data.
We found that the majority (81%) of filled pauseswere correctly tagged ?UH?, though this only covers three of eleven FP forms (er, um, eh).
We propose adictionary of FPs and a specific FP POS-tag, while suggesting that the dictionary will not catch all novelFPs (since they seem to be idiosyncratic) and that we can turn to state-of-the-art research on automateddisfluency detection to help us.We also showed that sentence parsing could be improved from a 47% ?success?
rate (i.e.
non-fragmentary (T/frag in RASP parlance) parse trees) in the ?as-is?
transcriptions, to 72% in transcrip-tions with disfluencies removed and errors corrected (see Table 3).
We found that disfluency removalis the main contributor to this improvement, though this was found to be somewhat idiosyncratic (as inFigure 1).10Note that parse likelihoods have been normalised for word length, as they increase in a near-linear manner according to thenumber of terminal nodes in a tree.78l l lllllllllllS2BWWT9EVS S3R66XVRQ2?6?4?20A B C D BC BD CD ABC A B C D BC BD CD ABCtranscription modeparselikelihood(normalisedfor wordlength)Figure 1: Parse likelihoods for each transcription mode, for two individuals in our corpus; the whiskersindicate the largest and smallest observation within 1.5?IQR (inter-quartile range; the distance betweenfirst and third quartiles), while the upper hinge indicates the third quartile (75th percentile), the middleis the median, the lower hinge is the first quartile (25th percentile), and the points are outliers.The motivation for this work is to investigate what is required to convert texts from unparseable toparseable form.
The steps taken to achieve this can be used to inform automated learner dialogue orfeedback systems.
We note that automated assessment may be improved by parse trees but may wellbe performed without them: it can proceed on the basis of superficial detection of features known tocorrelate with high grades (possibly including certain disfluency types, for instance).
But to be able todiagnose how the learner can improve, we need a deeper structural analysis of the text ?
i.e.
requiringthat the text is in parseable form.
Our manual annotations are one step towards this goal.Our annotations also indicate that spoken learner data features many disfluencies and errors, with overa quarter of the 2262-word testset affected in some way.
Automatic error detection (and correction) isa burgeoning field (see for example the work on learner data by Briscoe et al.
(2010), Andersen (2011)and Kochmar & Briscoe (2014), as well as the most recent shared task on grammatical error correctionat CoNLL-2014 (Ng et al., 2014)).
Such studies are based on written language.
We envisage addingspeech-specific information and adaptations to such systems on the basis of our fuller annotation project.Indeed, it so happens that the problem of NLP in the spoken domain is one we address here withlearner data.
However, we do not assume that the problem of adapting or building NLP tools for spokendata is substantially different for native speaker data.
We intend to collect recordings of native speakersundertaking the same tasks as the BULATS candidates, allowing for comparative studies of errors anddisfluencies in native and learner data, with the task and topic variables held constant as far as possible.Finally, we emphasise that we intend to add to the corpus with more annotated data from a widerrange of L1s and a wider range of proficiency levels.
We can then investigate the possible effects of morevaried syntactic complexity, lexical diversity and error types.79AcknowledgmentsWe thank Cambridge English Language Assessment for funding this work and providing the data.
Wealso thank Ted Briscoe, Mike McCarthy and ?istein Andersen for their support and advice, and we aregrateful to the three anonymous reviewers for their helpful comments and suggestions.References?istein E. Andersen.
2011.
Semi-automatic ESOL error annotation.
English Profile Journal, 2:e1.Ted Briscoe and John Carroll.
2006.
Evaluating the accuracy of an unlexicalized statistical parser on the PARCDepBank.
In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions.
Association for Com-putational Linguistics.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.
The second release of the RASP System.
In Proceedingsof the COLING/ACL 2006 Interactive Presentations Session.
Association for Computational Linguistics.Ted Briscoe, Ben Medlock, and ?istein E. Andersen.
2010.
Automated assessment of ESOL free text examina-tions.
University of Cambridge Computer Laboratory Technical Reports, 790.Andrew Caines and Paula J.
Buttery.
2010.
?You talking to me??
A predictive model for zero auxiliary construc-tions.
In Proceedings of the Workshop on Natural Language Processing and Linguistics, Finding the CommonGround, Annual Meeting of the Association for Computational Linguistics (ACL) 2010.
Association for Com-putational Linguistics.Eunah Cho, Jan Niehues, and Alex Waibel.
2014.
Tight integration of speech disfluency removal into SMT.
InProceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics(EACL 2014).
Association for Computational Linguistics.Ana D?az-Negrillo, Detmar Meurers, Salvador Valera, and Holger Wunsch.
2010.
Towards interlanguage POSannotation for effective learner corpora in SLA and FLT.
Language Forum, 36:139?154.Alison Edwards.
2014.
The progressive aspect in the Netherlands and the ESL/EFL continuum.
World Englishes,33:173?194.Erin Fitzgerald, Keith Hall, and Frederick Jelinek.
2009.
Reconstructing false start errors in spontaneous speechtext.
In Proceedings of the 12th Conference of the European Chapter of the Association for ComputationalLinguistics (EACL 2009).
Association for Computational Linguistics.Roger Garside.
1987.
The CLAWS word-tagging system.
In Roger Garside, Geoffrey Leech, and GeoffreySampson, editors, The Computational Analysis of English: A Corpus-based Approach.
London: Longman.Jeroen Geertzen, Theodora Alexopoulou, and Anna Korhonen.
2013.
Automatic linguistic annotation of largescale L2 databases: the EF-Cambridge Open Language Database (EFCAMDAT).
In Proceedings of the 31stSecond Language Research Forum.
Somerville, MA: Cascadilla Proceedings Project.Matthew Honnibal and Mark Johnson.
2014.
Joint incremental disfluency detection and dependency parsing.Transactions of the Association for Computational Linguistics, 2:131?142.Tracy H. King, Richard Crouch, Stefan Riezler, Mary Dalrymple, and Ronald M. Kaplan.
2003.
The PARC700Dependency Bank.
In Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora(LINC 2003).Ekaterina Kochmar and Ted Briscoe.
2014.
Detecting learner errors in the choice of content words using com-positional distributional semantics.
In Proceedings of the 25th International Conference on ComputationalLinguistics (COLING 2014).
Association for Computational Linguistics.Matthew Lease, Mark Johnson, and Eugene Charniak.
2006.
Recognizing disfluencies in conversational speech.IEEE Transactions on Audio, Speech, and Language Processing, 14:1566?1573.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and ChristopherBryant.
2014.
The CoNLL-2014 Shared Task on Grammatical Error Correction.
In Eighteenth Conferenceon Computational Natural Language Learning, Proceedings of the Shared Task.
Association for ComputationalLinguistics.80Diane Nicholls.
2003.
The Cambridge Learner Corpus: error coding and analysis for lexicography and ELT.
InDawn Archer, Paul Rayson, Andrew Wilson, and Tony McEnery, editors, Proceedings of the Corpus Linguistics2003 conference; UCREL technical paper number 16.
Lancaster University.Niels Ott and Ramon Ziai.
2010.
Evaluating dependency parsing performance on German learner language.
InProceedings of the Ninth International Workshop on Treebanks and Linguistic Theories (NEALT 2010).Xian Qian and Yang Liu.
2013.
Disfluency detection using multi-step stacked learning.
In Proceedings of the2013 Conference of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies (NAACL-HLT).Mohammad Sadegh Rasooli and Joel Tetreault.
2014.
Non-monotonic parsing of Fluent umm I Mean disfluentsentences.
In Proceedings of the 14th Conference of the European Chapter of the Association for ComputationalLinguistics (EACL 2014).
Association for Computational Linguistics.Laura Rimell and Stephen Clark.
2009.
Porting a lexicalized-grammar parser to the biomedical domain.
Journalof Biomedical Informatics, 42:852?865.Sylvie Thou?sny.
2011.
Increasing the reliability of a part-of-speech tagging tool for use with learner language.
InProceedings of the Pre-conference Workshop on Automatic Analysis of Learner Language, CALICO Conference2009.Bertus van Rooy and Lande Sch?fer.
2003.
An evaluation of three POS taggers for the tagging of the TswanaLearner English Corpus.
In Proceedings of the Corpus Linguistics 2003 Conference.
Lancaster University.Joachim Wagner and Jennifer Foster.
2009.
The effect of correcting grammatical errors on parse probabilities.
InProceedings of the 11th International Conference on Parsing Technologies.Heike Zinsmeister, Ulrich Heid, and Kathrin Beck.
2014.
Adapting a part-of-speech tagset to non-standard text:the case of STTS.
In Proceedings of the Ninth International Conference on Language Resources and Evaluation(LREC 2014).
European Language Resources Association.81
