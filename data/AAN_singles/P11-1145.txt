Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1445?1455,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Bayesian Model for Unsupervised Semantic ParsingIvan TitovSaarland UniversitySaarbruecken, Germanytitov@mmci.uni-saarland.deAlexandre KlementievJohns Hopkins UniversityBaltimore, MD, USAaklement@jhu.eduAbstractWe propose a non-parametric Bayesian modelfor unsupervised semantic parsing.
Follow-ing Poon and Domingos (2009), we considera semantic parsing setting where the goal is to(1) decompose the syntactic dependency treeof a sentence into fragments, (2) assign eachof these fragments to a cluster of semanti-cally equivalent syntactic structures, and (3)predict predicate-argument relations betweenthe fragments.
We use hierarchical Pitman-Yor processes to model statistical dependen-cies between meaning representations of pred-icates and those of their arguments, as wellas the clusters of their syntactic realizations.We develop a modification of the Metropolis-Hastings split-merge sampler, resulting in anefficient inference algorithm for the model.The method is experimentally evaluated by us-ing the induced semantic representation forthe question answering task in the biomedicaldomain.1 IntroductionStatistical approaches to semantic parsing have re-cently received considerable attention.
While somemethods focus on predicting a complete formal rep-resentation of meaning (Zettlemoyer and Collins,2005; Ge and Mooney, 2005; Mooney, 2007), othersconsider more shallow forms of representation (Car-reras and Ma`rquez, 2005; Liang et al, 2009).
How-ever, most of this research has concentrated on su-pervised methods requiring large amounts of labeleddata.
Such annotated resources are scarce, expensiveto create and even the largest of them tend to havelow coverage (Palmer and Sporleder, 2010), moti-vating the need for unsupervised or semi-supervisedtechniques.Conversely, research in the closely related taskof relation extraction has focused on unsupervisedor minimally supervised methods (see, for example,(Lin and Pantel, 2001; Yates and Etzioni, 2009)).These approaches cluster semantically equivalentverbalizations of relations, often relying on syn-tactic fragments as features for relation extractionand clustering (Lin and Pantel, 2001; Banko et al,2007).
The success of these methods suggests thatsemantic parsing can also be tackled as clusteringof syntactic realizations of predicate-argument rela-tions.
While a similar direction has been previouslyexplored in (Swier and Stevenson, 2004; Abend etal., 2009; Lang and Lapata, 2010), the recent workof (Poon and Domingos, 2009) takes it one stepfurther by not only predicting predicate-argumentstructure of a sentence but also assigning sentencefragments to clusters of semantically similar expres-sions.
For example, for a pair of sentences on Fig-ure 1, in addition to inducing predicate-argumentstructure, they aim to assign expressions ?Steelers?and ?the Pittsburgh team?
to the same semanticclass Steelers, and group expressions ?defeated?and ?secured the victory over?.
Such semantic rep-resentation can be useful for entailment or questionanswering tasks, as an entailment model can ab-stract away from specifics of syntactic and lexicalrealization relying instead on the induced semanticrepresentation.
For example, the two sentences inFigure 1 have identical semantic representation, andtherefore can be hypothesized to be equivalent.1445Ravens??defeated?
?SteelersRavenas?dftSlrtScuh?vl io?
?y?nlRavv?n Pbbfv?voRavens?secured?the?victory?over?the?Pittsburgh?teamio?
?y?nlRavenas?lrtS dftSbbgfh?ncuh?vlRavv?n Pbbfv?vovmfdFigure 1: An example of two different syntactic trees with a common semantic representation WinPrize(Ravens,Steelers).From the statistical modeling point of view, jointlearning of predicate-argument structure and dis-covery of semantic clusters of expressions can alsobe beneficial, because it results in a more compactmodel of selectional preference, less prone to thedata-sparsity problem (Zapirain et al, 2010).
In thisrespect our model is similar to recent LDA-basedmodels of selectional preference (Ritter et al, 2010;Se?aghdha, 2010), and can even be regarded as theirrecursive and non-parametric extension.In this paper, we adopt the above definition of un-supervised semantic parsing and propose a Bayesiannon-parametric approach which uses hierarchicalPitman-Yor (PY) processes (Pitman, 2002) to modelstatistical dependencies between predicate and ar-gument clusters, as well as distributions over syn-tactic and lexical realizations of each cluster.
Ournon-parametric model automatically discovers gran-ularity of clustering appropriate for the dataset, un-like the parametric method of (Poon and Domingos,2009) which have to perform model selection anduse heuristics to penalize more complex models ofsemantics.
Additional benefits generally expectedfrom Bayesian modeling include the ability to en-code prior linguistic knowledge in the form of hy-perpriors and the potential for more reliable model-ing of smaller datasets.
More detailed discussion ofrelation between the Markov Logic Network (MLN)approach of (Poon and Domingos, 2009) and ournon-parametric method is presented in Section 3.Hierarchical Pitman-Yor processes (or their spe-cial case, hierarchical Dirichlet processes) have pre-viously been used in NLP, for example, in the con-text of syntactic parsing (Liang et al, 2007; John-son et al, 2007).
However, in all these cases theeffective size of the state space (i.e., the numberof sub-symbols in the infinite PCFG (Liang et al,2007), or the number of adapted productions in theadaptor grammar (Johnson et al, 2007)) was notvery large.
In our case, the state space size equalsthe total number of distinct semantic clusters, and,thus, is expected to be exceedingly large even formoderate datasets: for example, the MLN model in-duces 18,543 distinct clusters from 18,471 sentencesof the GENIA corpus (Poon and Domingos, 2009).This suggests that standard inference methods for hi-erarchical PY processes, such as Gibbs sampling,Metropolis-Hastings (MH) sampling with uniformproposals, or the structured mean-field algorithm,are unlikely to result in efficient inference: for ex-ample in standard Gibbs sampling all thousands ofalternatives should be considered at each samplingmove.
Instead, we use a split-merge MH samplingalgorithm, which is a standard and efficient infer-ence tool for non-hierarchical PY processes (Jainand Neal, 2000; Dahl, 2003) but has not previouslybeen used in hierarchical setting.
We extend thesampler to include composition-decomposition ofsyntactic fragments in order to cluster fragments ofvariables size, as in the example Figure 1, and alsoinclude the argument role-syntax alignment movewhich attempts to improve mapping between seman-tic roles and syntactic paths for some fixed predicate.Evaluating unsupervised models is a challengingtask.
We evaluate our model both qualitatively, ex-amining the revealed clustering of syntactic struc-tures, and quantitatively, on a question answeringtask.
In both cases, we follow (Poon and Domingos,2009) in using the corpus of biomedical abstracts.Our model achieves favorable results significantlyoutperforming the baselines, including state-of-the-art methods for relation extraction, and achievesscores comparable to those of the MLN model.The rest of the paper is structured as follows.
Sec-tion 2 begins with a definition of the semantic pars-ing task.
Sections 3 and 4 give background on theMLN model and the Pitman-Yor processes, respec-tively.
In Sections 5 and 6, we describe our modeland the inference method.
Section 7 provides bothqualitative and quantitative evaluation.
Finally, ad-1446ditional related work is presented in Section 8.2 Semantic ParsingIn this section, we briefly define the unsupervisedsemantic parsing task and underlying aspects and as-sumptions relevant to our model.Unlike (Poon and Domingos, 2009), we do notuse the lambda calculus formalism to define our taskbut rather treat it as an instance of frame-semanticparsing, or a specific type of semantic role label-ing (Gildea and Jurafsky, 2002).
The reason for thisis two-fold: first, the frame semantics view is morestandard in computational linguistics, sufficient todescribe induced semantic representation and conve-nient to relate our method to the previous work.
Sec-ond, lambda calculus is a considerably more power-ful formalism than the predicate-argument structureused in frame semantics, normally supporting quan-tification and logical connectors (for example, nega-tion and disjunction), neither of which is modeledby our model or in (Poon and Domingos, 2009).In frame semantics, the meaning of a predicateis conveyed by a frame, a structure of related con-cepts that describes a situation, its participants andproperties (Fillmore et al, 2003).
Each frame ischaracterized by a set of semantic roles (frame el-ements) corresponding to the arguments of the pred-icate.
It is evoked by a frame evoking element (apredicate).
The same frame can be evoked by differ-ent but semantically similar predicates: for exam-ple, both verbs ?buy?
and ?purchase?
evoke frameCommerce buy in FrameNet (Fillmore et al, 2003).The aim of the semantic role labeling task is toidentify all of the frames evoked in a sentence andlabel their semantic role fillers.
We extend this taskand treat semantic parsing as recursive prediction ofpredicate-argument structure and clustering of argu-ment fillers.
Thus, parsing a sentence into this rep-resentation involves (1) decomposing the sentenceinto lexical items (one or more words), (2) assigninga cluster label (a semantic frame or a cluster of ar-gument fillers) to every lexical item, and (3) predict-ing argument-predicate relations between the lexicalitems.
This process is illustrated in Figure 1.
Forthe leftmost example, the sentence is decomposedinto three lexical items: ?Ravens?, ?defeated?and ?Steelers?, and they are assigned to clustersRavens, WinPrize and Steelers, respectively.Then Ravens and Steelers are selected as aWinner and an Opponent in the WinPrize frame.In this work, we define a joint model for the label-ing and argument identification stages.
Similarly tocore semantic roles in FrameNet, semantic roles aretreated as frame-specific in our model, as our modeldoes not try to discover any correspondences be-tween roles in different frames.As you can see from the above description, frames(which groups predicates with similar meaning suchas the WinPrize frame in our example) and clus-ters of argument fillers (Ravens and Steelers) aretreated in our definition in a similar way.
For con-venience, we will refer to both types of clusters assemantic classes.1This definition of semantic parsing is closely re-lated to a realistic relation extraction setting, as bothclustering of syntactic forms of relations (or extrac-tion patterns) and clustering of argument fillers forthese relations is crucial for automatic constructionof knowledge bases (Yates and Etzioni, 2009).In this paper, we make three assumptions.
First,we assume that each lexical item corresponds to asubtree of the syntactic dependency graph of thesentence.
This assumption is similar to the ad-jacency assumption in (Zettlemoyer and Collins,2005), though ours may be more appropriate for lan-guages with free or semi-free word order, where syn-tactic structures are inherently non-projective.
Sec-ond, we assume that the semantic arguments are lo-cal in the dependency tree; that is, one lexical itemcan be a semantic argument of another one only ifthey are connected by an arc in the dependency tree.This is a slight simplification of the semantic rolelabeling problem but one often made.
Thus, the ar-gument identification and labeling stages consist oflabeling each syntactic arc with a semantic role la-bel.
In comparison, the MLN model does not explic-itly assume contiguity of lexical items and does notmake this directionality assumption but their clus-tering algorithm uses initialization and clusterizationmoves such that the resulting model also obeys bothof these constraints.
Third, as in (Poon and Domin-gos, 2009), we do not model polysemy as we assume1Semantic classes correspond to lambda-form clusters in(Poon and Domingos, 2009) terminology.1447that each syntactic fragment corresponds to a singlesemantic class.
This is not a model assumption andis only used at inference as it reduces mixing time ofthe Markov chain.
It is not likely to be restrictive forthe biomedical domain studied in our experiments.As in some of the recent work on learning se-mantic representations (Eisenstein et al, 2009; Poonand Domingos, 2009), we assume that dependencystructures are provided for every sentence.
This as-sumption allows us to construct models of seman-tics not Markovian within a sequence of words (seefor an example a model described in (Liang et al,2009)), but rather Markovian within a dependencytree.
Though we include generation of the syntac-tic structure in our model, we would not expect thatthis syntactic component would result in an accuratesyntactic model, even if trained in a supervised way,as the chosen independence assumptions are over-simplistic.
In this way, we can use a simple gener-ative story and build on top of the recent success insyntactic parsing.3 Relation to the MLN ApproachThe work of (Poon and Domingos, 2009) modelsjoint probability of the dependency tree and its latentsemantic representation using Markov Logic Net-works (MLNs) (Richardson and Domingos, 2006),selecting parameters (weights of first-order clauses)to maximize the probability of the observed depen-dency structures.
For each sentence, the MLN in-duces a Markov network, an undirected graphicalmodel with nodes corresponding to ground atomsand cliques corresponding to ground clauses.The MLN is a powerful formalism and allows formodeling complex interaction between features ofthe input (syntactic trees) and latent output (seman-tic representation), however, unsupervised learn-ing of semantics with general MLNs can be pro-hibitively expensive.
The reason for this is thatMLNs are undirected models and when learned tomaximize likelihood of syntactically annotated sen-tences, they would require marginalization over se-mantic representation but also over the entire spaceof syntactic structures and lexical units.
Given thecomplexity of the semantic parsing task and the needto tackle large datasets, even approximate methodsare likely to be infeasible.
In order to overcomethis problem, (Poon and Domingos, 2009) group pa-rameters and impose local normalization constraintswithin each group.
Given these normalization con-straints, and additional structural constraints satis-fied by the model, namely that the clauses shouldbe engineered in such a way that they induce tree-structured graphs for every sentence, the parameterscan be estimated by a variant of the EM algorithm.The class of such restricted MLNs is equivalentto the class of directed graphical models over thesame set of random variables corresponding to frag-ments of syntactic and semantic structure.
Giventhat the above constraints do not directly fit into theMLN methodology, we believe that it is more nat-ural to regard their model as a directed model withan underlying generative story specifying how thesemantic structure is generated and how the syntac-tic parse is drawn for this semantic structure.
Thisview would facilitate understanding what kind offeatures can easily be integrated into the model, sim-plify application of non-parametric Bayesian tech-niques and expedite the use of inference techniquesdesigned specifically for directed models.
Our ap-proach makes one step in this direction by proposinga non-parametric version of such generative model.4 Hierarchical Pitman-Yor ProcessesThe central component of our non-parametricBayesian model are Pitman-Yor (PY) processes,which are a generalization of the Dirichlet processes(DPs) (Ferguson, 1973).
We use PY processes tomodel distributions of semantic classes appearing asan argument of other semantic classes.
We also usethem to model distributions of syntactic realizationsfor each semantic class and distributions of syntacticdependency arcs for argument types.
In this sectionwe present relevant background on PY processes.For a more detailed consideration we refer the readerto (Teh et al, 2006).The Pitman-Yor process over a set S, denotedPY (?, ?,H), is a stochastic process whose samplesG0 constitute probability measures on partitions ofS.
In practice, we do not need to draw measures,as they can be analytically marginalized out.
Theconditional distribution of xj+1 given the previousj draws, with G0 marginalized out, follows (Black-1448well and MacQueen, 1973)xj+1|x1, .
.
.
xj ?K?k=1jk ?
?j+??
?k +K?
+ ?j+?H.
(1)where ?1, .
.
.
, ?K are K values assigned tox1, x2, .
.
.
, xj .
The number of times ?k was as-signed is denoted jk, so that j =?Kk=1 jk.
Theparameter ?
< 1 controls how heavy the tail of thedistribution is: when it approaches 1, a new value isassigned to every draw, when ?
= 0 the PY processreduces to DP.
The expected value of K scales asO(?n?)
with the number of draws n, while it scalesonly logarithmically for DP processes.
PY processesare expected to be more appropriate for many NLPproblems, as they model power-law type distribu-tions common for natural language (Teh, 2006).Hierarchical Dirichlet Processes (HDP) or hierar-chical PY processes are used if the goal is to drawseveral related probability measures for the sameset S. For example, they can be used to generatetransition distributions of a Markov model, HDP-HMM (Teh et al, 2006; Beal et al, 2002).
Forsuch a HMM, the top-level state proportions aredrawn from the top-level stick breaking construction?
?
GEM(?, ?
), and then the individual transi-tion distributions for every state z = 1, 2, .
.
.
?z aredrawn from PY (?, ?
?, ??).
The parameters ??
and??
control how similar the individual transition dis-tributions ?z are to the top-level state proportions ?,or, equivalently, how similar the transition distribu-tions are to each other.5 A Model for Semantic ParsingOur model of semantics associates with each seman-tic class a set of distributions which govern the gen-eration of corresponding syntactic realizations2 andthe selection of semantic classes for its arguments.Each sentence is generated starting from the root ofits dependency tree, recursively drawing a seman-tic class, its syntactic realization, arguments and se-mantic classes for the arguments.
Below we de-scribe the model by first defining the set of the modelparameters and then explaining the generation of in-2Syntactic realizations are syntactic tree fragments, andtherefore they correspond both to syntactic and lexical varia-tions.dividual sentences.
The generative story is formallypresented in Figure 2.We associate with each semantic class c, c =1, 2, .
.
.
, a distribution of its syntactic realizations?c.
For example, for the frame WinPrize illus-trated in Figure 1 this distribution would concen-trate at syntactic fragments corresponding to lexicalitems ?defeated?, ?secured the victory?
and ?won?.The distribution is drawn from DP (w(C), H(C)),where H(C) is a base measure over syntactic sub-trees.
We use a simple generative process to definethe probability of a subtree, the underlying model issimilar to the base measures used in the Bayesiantree-substitution grammars (Cohn et al, 2009).
Westart by generating a word w uniformly from thetreebank distribution, then we decide on the num-ber of dependents of w using the geometric distribu-tion Geom(q(C)).
For every dependent we generatea dependency relation r and a lexical form w?
fromP (r|w)P (w?|r), where probabilities P are based onadd-0.1 smoothed treebank counts.
The process iscontinued recursively.
The smaller the parameterq(C), the lower is the probability assigned to largersub-trees.Parameters ?c,t and ?+c,t, t = 1, .
.
.
, T , de-fine a distribution over vectors (m1,m2, .
.
.
,mT )where mt is the number of times an argument oftype t appears for a given semantic frame occur-rence3.
For the frame WinPrize these parameterswould enforce that there exists exactly one Winnerand exactly one Opponent for each occurrence ofWinPrize.
The parameter ?c,t defines the probabil-ity of having at least one argument of type t. If 0 isdrawn from ?c,t then mt = 0, otherwise the numberof additional arguments of type t (mt ?
1) is drawnfrom the geometric distribution Geom(?+c,t).
Thisgenerative story is flexible enough to accommodateboth argument types which appear at most once persemantic class occurrence (e.g., agents), and argu-ment types which frequently appear multiple timesper semantic class occurrence (e.g., arguments cor-responding to descriptors).Parameters ?c,t, t = 1, .
.
.
, T , define the dis-3For simplicity, we assume that each semantic class has Tassociated argument types, note that this is not a restrictive as-sumption as some of the argument types can remain unused,and T can be selected to be sufficiently large to accommodateall important arguments.1449Parameters:?
?
GEM(?0, ?0) [top-level proportions of classes]?root ?
PY (?root, ?root, ?)
[distrib of sem classes at root]for each sem class c = 1, 2, .
.
.
:?c ?
DP (w(C), H(C)) [distribs of synt realizations]for each arg type t = 1, 2, .
.
.
T :?c,t ?
Beta(?0, ?1) [first argument generation]?+c,t ?
Beta(?+0 , ?+1 ) [geom distr for more args]?c,t ?
DP (w(A), H(A)) [distribs of synt paths]?c,t ?
PY (?, ?, ?)
[distrib of arg fillers]Data Generation:for each sentence:croot ?
?root [choose sem class for root]GenSemClass(croot)GenSemClass(c):s ?
?c [draw synt realization]for each arg type t = 1, .
.
.
, T :if [n ?
?c,t] = 1: [at least one arg appears]GenArgument(c, t) [draw one arg]while [n ?
?+c,t] = 1: [continue generation]GenArgument(c, t) [draw more args]GenArgument(c, t):ac,t ?
?c,t [draw synt relation]c?c,t ?
?c,t [draw sem class for arg]GenSemClass(c?c,t) [recurse]Figure 2: The generative story for the Bayesian model forunsupervised semantic parsing.tributions over syntactic paths for the argumenttype t. In our example, for argument typeOpponent, this distribution would associate mostof the probability mass with relations pp over, dobjand pp against.
These distributions are drawn fromDP (w(A), H(A)).
In this paper we only considerpaths consisting of a single relation, therefore thebase probability distributionH(A) is just normalizedfrequencies of dependency relations in the treebank.The crucial part of the model are the selection-preference parameters ?c,t, the distributions of se-mantic classes c?
for each argument type t of classc.
For arguments Winner and Opponent of theframe WinPrize these distributions would assignmost of the probability mass to semantic classes de-noting teams or players.
Distributions ?c,t are drawnfrom a hierarchical PY process: first, top-level pro-portions of classes ?
are drawn fromGEM(?0, ?0),and then the individual distributions ?c,t over c?
arechosen from PY (?, ?, ?
).For each sentence, we first generate a class corre-sponding to the root of the dependency tree from theroot-specific distribution of semantic classes ?root.Then we recursively generate classes for the entiresentence.
For a class c, we generate the syntacticrealization s and for each of the T types, decidehow many arguments of that type to generate (seeGenSemClass in Figure 2).
Then we generate eachof the arguments (see GenArgument) by first gen-erating a syntactic arc ac,t, choosing a class as itsfiller c?c,t and, finally, recursing.6 InferenceIn our model, latent states, modeled with hierarchi-cal PY processes, correspond to distinct semanticclasses and, therefore, their number is expected tobe very large for any reasonable model of semantics.As a result, many standard inference techniques,such as Gibbs sampling, or the structured mean-fieldmethod are unlikely to result in tractable inference.One of the standard and most efficient samplers fornon-hierarchical PY processes are split-merge MHsamplers (Jain and Neal, 2000; Dahl, 2003).
In thissection we explain how split-merge samplers can beapplied to our model.6.1 Split and Merge MovesOn each move, split-merge samplers decide eitherto merge two states into one (in our case, merge twosemantic classes), or split one state into two.
Thesemoves can be computed efficiently for our model ofsemantics.
Note that for any reasonable model ofsemantics only a small subset of the entire set of se-mantic classes can be used as an argument for somefixed semantic class due to selectional preferencesexhibited by predicates.
For instance, only teams orplayers can fill arguments of the frame WinPrizein our running example.
As a result, only a smallnumber of terms in the joint distribution has to beevaluated on every move we may consider.When estimating the model, we start with assign-ing each distinct word (or, more precisely, a tupleof a word?s stem and its part-of-speech tag) to anindividual semantic class.
Then, we would iterateby selecting a random pair of class occurrences, anddecide, at random, whether we attempt to perform asplit-merge move or a compose-decompose move.14506.2 Compose and Decompose MovesThe compose-decompose operations modify syntac-tic fragments assigned to semantic classes, com-posing two neighboring dependency sub-trees ordecomposing a dependency sub-tree.
If the tworandomly-selected syntactic fragments s and s?
cor-respond to different classes, c and c?, we attemptto compose them into s?
and create a new semanticclass c?.
All occurrences of s?
are assigned to this newclass c?.
For example, if two randomly-selected oc-currences have syntactic realizations ?secure?
and?victory?
they can be composed to obtain the syn-tactic fragment ?securedobj???
victory?.
This frag-ment will be assigned to a new semantic class whichcan later be merged with other classes, such as theones containing syntactic realizations ?defeat?
or?win?.Conversely, if both randomly-selected syntacticfragments are already composed in the correspond-ing class, we attempt to split them.6.3 Role-Syntax Alignment MoveMerge, compose and decompose moves require re-computation of mapping between argument types(semantic roles) and syntactic fragments.
Comput-ing the best statistical mapping is infeasible andproposing a random mapping will result in manyattempted moves being rejected.
Instead we usea greedy randomized search method called Gibbsscan (Dahl, 2003).
Though it is a part of the above 3moves, this alignment move is also used on its ownto induce semantic arguments for classes (frames)with a single syntactic realization.The Gibbs scan procedure is also used during thesplit move to select one of the newly introducedclasses for each considered syntactic fragment.6.4 Informed ProposalsSince the number of classes is very large, selectingexamples at random would result in a relatively lowproportion of moves getting accepted, and, conse-quently, in a slow-mixing Markov chain.
Instead ofselecting both class occurrences uniformly, we se-lect the first occurrence from a uniform distributionand then use a simple but effective proposal distri-bution for selecting the second class occurrence.Let us denote the class corresponding to the firstoccurrence as c1 and its syntactic realization as s1with a head word w1.
We begin by selecting uni-formly randomly whether to attempt a compose-decompose or a split-merge move.If we chose a compose-decompose move, we lookfor words (children) which can be attached belowthe syntactic fragment s1.
We use the normalizedcounts of these words conditioned on the parent s1 toselect the second word w2.
We then select a randomoccurrence of w2; if it is a part of syntactic realiza-tion of c1 then a decompose move is attempted.
Oth-erwise, we try to compose the corresponding clus-ters together.If we selected a split-merge move, we use a dis-tribution based on the cosine similarity of lexicalcontexts of the words.
The context is representedas a vector of counts of all pairs of the form (headword, dependency type) and (dependent, depen-dency type).
So, instead of selecting a word occur-rence uniformly, each occurrence of every word w2is weighted by its similarity to w1, where the simi-larity is based on the cosine distance.As the moves are dependent only on syntactic rep-resentations, all the proposal distributions can becomputed once at the initialization stage.47 Empirical EvaluationWe induced a semantic representation over a collec-tion of texts and evaluated it by answering questionsabout the knowledge contained in the corpus.
Weused the GENIA corpus (Kim et al, 2003), a datasetof 1999 biomedical abstracts, and a set of questionsproduced by (Poon and Domingos, 2009).
A exam-ple question is shown in Figure 3.All model hyperpriors were set to maximize theposterior, except for w(A) and w(C), which were setto 1.e?10 and 1.e?35, respectively.
Inference wasrun for around 300,000 sampling iterations until thepercentage of accepted split-merge moves becamelower than 0.05%.Let us examine some of the induced semanticclasses (Table 1) before turning to the question an-swering task.
Almost all of the clustered syntactic4In order to minimize memory usage, we used frequencycut-off of 10.
For split-merge moves, we select words basedon the cosine distance if the distance is below 0.95 and samplethe remaining words uniformly.
This also reduces the requiredmemory usage.1451Class Variations1 motif, sequence, regulatory element, response ele-ment, element, dna sequence2 donor, individual, subject3 important, essential, critical4 dose, concentration5 activation, transcriptional activation, transactiva-tion6 b cell, t lymphocyte, thymocyte, b lymphocyte, tcell, t-cell line, human lymphocyte, t-lymphocyte7 indicate, reveal, document, suggest, demonstrate8 augment, abolish, inhibit, convert, cause, abrogate,modulate, block, decrease, reduce, diminish, sup-press, up-regulate, impair, reverse, enhance9 confirm, assess, examine, study, evaluate, test, re-solve, determine, investigate10 nf-kappab, nf-kappa b, nfkappab, nf-kb11 antiserum, antibody, monoclonal antibody, ab, an-tisera, mab12 tnfalpha, tnf-alpha, il-6, tnfTable 1: Examples of the induced semantic classes.realizations have a clear semantic connection.
Clus-ter 6, for example, clusters lymphocytes with the ex-ception of thymocyte, a type of cell which gener-ates T cells.
Cluster 8 contains verbs roughly corre-sponding to Cause change of position on ascale frame in FrameNet.
Verbs in class 9 are usedin the context of providing support for a finding oran action, and many of them are listed as evokingelements for the Evidence frame in FrameNet.Argument types of the induced classes also showa tendency to correspond to semantic roles.
For ex-ample, an argument type of class 2 is modeled asa distribution over two argument parts, prep of andprep from.
The corresponding arguments define theorigin of the cells (transgenic mouse, smoker, volun-teer, donor, .
.
.
).We now turn to the QA task and compare ourmodel (USP-BAYES) with the results of baselinesconsidered in (Poon and Domingos, 2009).
The firstset of baselines looks for answers by attempting tomatch a verb and its argument in the question withthe input text.
The first version (KW) simply re-turns the rest of the sentence on the other side of theverb, while the second (KW-SYN) uses syntactic in-formation to extract the subject or the object of theverb.Other baselines are based on state-of-the-art re-lation extraction systems.
When the extracted rela-tion and one of the arguments match those in a givenTotal Correct AccuracyKW 150 67 45%KW-SYN 87 67 77%TR-EXACT 29 23 79%TR-SUB 152 81 53%RS-EXACT 53 24 45%RS-SUB 196 81 41%DIRT 159 94 59%USP-MLN 334 295 88%USP-BAYES 325 259 80%Table 2: Performance on the QA task.question, the second argument is returned as an an-swer.
The systems include TextRunner (TR) (Bankoet al, 2007), RESOLVER (RS) (Yates and Etzioni,2009) and DIRT (Lin and Pantel, 2001).
The EX-ACT versions of the methods return answers whenthey match the question argument exactly, and theSUB versions produce answers containing the ques-tion argument as a substring.Similarly to the MLN system (USP-MLN), wegenerate answers as follows.
We use our trainedmodel to parse a question, i.e.
recursively decom-pose it into lexical items and assign them to seman-tic classes induced at training.
Using this semanticrepresentation, we look for the type of an argumentmissing in the question, which, if found, is reportedas an answer.
It is clear that overly coarse clustersof argument fillers or clustering of semantically re-lated but not equivalent relations can hurt precisionfor this evaluation method.Each system is evaluated by counting the answersit generates, and computing the accuracy of thoseanswers.5 Table 2 summarizes the results.
First,both USP models significantly outperform all otherbaselines: even though the accuracy of KW-SYNand TR-EXACT are comparable with our accuracy,the number of correct answers returned by USP-Bayes is 4 and 11 times smaller than those of KW-SYN and TR-EXACT, respectively.
While we arenot beating the MLN baseline, the difference is notsignificant.
The effective number of questions is rel-atively small (less than 80 different questions are an-swered by any of the models).
More than 50% ofUSP-BAYES mistakes were due to wrong interpre-tation of only 5 different questions.
From anotherpoint of view, most of the mistakes are explained5The true recall is not known, as computing it would requireexhaustive annotation of the entire corpus.1452Question: What does cyclosporin A suppress?Answer: expression of EGR-2Sentence: As with EGR-3 , expression of EGR-2 was blockedby cyclosporin A .Question: What inhibits tnf-alpha?Answer: IL -10Sentence: Our previous studies in human monocytes havedemonstrated that interleukin ( IL ) -10 inhibits lipopolysac-charide ( LPS ) -stimulated production of inflammatory cy-tokines , IL-1 beta , IL-6 , IL-8 , and tumor necrosis factor (TNF ) -alpha by blocking gene transcription .Figure 3: An example of questions, answers by our modeland the corresponding sentences from the dataset.by overly coarse clustering corresponding to just 3classes, namely, 30%, 25% and 20% of errors aredue to the clusters 6, 8 and 12 (Figure 1), respec-tively.
Though all these clusters have clear semanticinterpretation (white blood cells, predicates corre-sponding to changes and cykotines associated withcancer progression, respectively), they appear to betoo coarse for the QA method we use in our exper-iments.
Though it is likely that tuning and differ-ent heuristics may result in better scores, we chosenot to perform excessive tuning, as the evaluationdataset is fairly small.8 Related WorkThere is a growing body of work on statistical learn-ing for different versions of the semantic parsingproblem (e.g., (Gildea and Jurafsky, 2002; Zettle-moyer and Collins, 2005; Ge and Mooney, 2005;Mooney, 2007)), however, most of these methodsrely on human annotation, or some weaker forms ofsupervision (Kate and Mooney, 2007; Liang et al,2009; Titov and Kozhevnikov, 2010; Clarke et al,2010) and very little research has considered the un-supervised setting.In addition to the MLN model (Poon and Domin-gos, 2009), another unsupervised method has beenproposed in (Goldwasser et al, 2011).
In that work,the task is to predict a logical formula, and the onlysupervision used is a lexicon providing a small num-ber of examples for every logical symbol.
A form ofself-training is then used to bootstrap the model.Unsupervised semantic role labeling with a gen-erative model has also been considered (Grenagerand Manning, 2006), however, they do not attemptto discover frames and deal only with isolated pred-icates.
Another generative model for SRL has beenproposed in (Thompson et al, 2003), but the param-eters were estimated from fully annotated data.The unsupervised setting has also been consid-ering for the related problem of learning narrativeschemas (Chambers and Jurafsky, 2009).
However,their approach is quite different from our Bayesianmodel as it relies on similarity functions.Though in this work we focus solely on the un-supervised setting, there has been some success-ful work on semi-supervised semantic-role label-ing, including the Framenet version of the prob-lem (Fu?rstenau and Lapata, 2009).
Their methodexploits graph alignments between labeled and un-labeled examples, and, therefore, crucially relies onthe availability of labeled examples.9 Conclusions and Future WorkIn this work, we introduced a non-parametricBayesian model for the semantic parsing problembased on the hierarchical Pitman-Yor process.
Themodel defines a generative story for recursive gener-ation of lexical items, syntactic and semantic struc-tures.
We extend the split-merge MH sampling algo-rithm to include composition-decomposition moves,and exploit the properties of our task to make it effi-cient in the hierarchical setting we consider.We plan to explore at least two directions in ourfuture work.
First, we would like to relax some ofunrealistic assumptions made in our model: for ex-ample, proper modeling of alterations requires jointgeneration of syntactic realizations for predicate-argument relations (Grenager and Manning, 2006;Lang and Lapata, 2010), similarly, proper model-ing of nominalization implies support of argumentsnot immediately local in the syntactic structure.
Thesecond general direction is the use of the unsuper-vised methods we propose to expand the coverage ofexisting semantic resources, which typically requiresubstantial human effort to produce.AcknowledgementsThe authors acknowledge the support of the MMCI Clus-ter of Excellence, and thank Chris Callison-Burch, AlexisPalmer, Caroline Sporleder, Ben Van Durme and theanonymous reviewers for their helpful comments andsuggestions.1453ReferencesO.
Abend, R. Reichart, and A. Rappoport.
2009.
Unsu-pervised argument identification for semantic role la-beling.
In Proceedings of ACL-IJCNLP, pages 28?36,Singapore.Michele Banko, Michael J Cafarella, Stephen Soderland,Matt Broadhead, and Oren Etzioni.
2007.
Open in-formation extraction from the web.
In Proc.
of the In-ternational Joint Conference on Artificial Intelligence(IJCAI), pages 2670?2676.Matthew J. Beal, Zoubin Ghahramani, and Carl E. Ras-mussen.
2002.
The infinite hidden markov model.
InMachine Learning, pages 29?245.
MIT Press.David Blackwell and James B. MacQueen.
1973.
Fergu-son distributions via polya urn schemes.
The Annalsof Statistics, 1(2):353?355.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introductionto the CoNLL-2005 Shared Task: Semantic Role La-beling.
In Proceedings of the 9th Conference on Natu-ral Language Learning, CoNLL-2005, Ann Arbor, MIUSA.Nathanael Chambers and Dan Jurafsky.
2009.
Unsu-pervised learning of narrative schemas and their par-ticipants.
In Proc.
of the Annual Meeting of the As-sociation for Computational Linguistics and Interna-tional Joint Conference on Natural Language Process-ing (ACL-IJCNLP).James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing from theworld?s response.
In Proc.
of the Conference on Com-putational Natural Language Learning (CoNLL).Trevor Cohn, Sharon Goldwater, and Phil Blunsom.2009.
Inducing compact but accurate tree-substitutiongrammars.
In HLT-NAACL, pages 548?556.David B. Dahl.
2003.
An improved merge-split samplerfor conjugate dirichlet process mixture models.
Tech-nical Report 1086, Department of Statistics, Univer-sity of Wiscosin - Madison, November.Jacob Eisenstein, James Clarke, Dan Goldwasser, andDan Roth.
2009.
Reading to learn: Constructingfeatures from semantic abstracts.
In Proceedings ofEMNLP.Thomas S. Ferguson.
1973.
A bayesian analysis ofsome nonparametric problems.
The Annals of Statis-tics, 1(2):209?230.C.
J. Fillmore, C. R. Johnson, and M. R. L. Petruck.2003.
Background to framenet.
International Journalof Lexicography, 16:235?250.Hagen Fu?rstenau and Mirella Lapata.
2009.
Graph align-ment for semi-supervised semantic role labeling.
InProceedings of Empirical Methods in Natural Lan-guage Processing (EMNLP).Ruifang Ge and Raymond J. Mooney.
2005.
A statisticalsemantic parser that integrates syntax and semantics.In Proceedings of the Ninth Conference on Computa-tional Natural Language Learning (CONLL-05), AnnArbor, Michigan.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-belling of semantic roles.
Computational Linguistics,28(3):245?288.Dan Goldwasser, Roi Reichart, James Clarke, and DanRoth.
2011.
Confidence driven unsupervised semanticparsing.
In Proc.
of the Meeting of Association forComputational Linguistics (ACL), Portland, OR, USA.Trond Grenager and Christoph Manning.
2006.
Unsu-pervised discovery of a statistical verb lexicon.
In Pro-ceedings of Empirical Methods in Natural LanguageProcessing (EMNLP).Sonia Jain and Radford Neal.
2000.
A split-mergemarkov chain monte carlo procedure for the dirichletprocess mixture model.
Journal of Computational andGraphical Statistics, 13:158?182.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for PCFGs via Markovchain Monte Carlo.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics,Rochester, USA.Rohit J. Kate and Raymond J. Mooney.
2007.
Learninglanguage semantics from ambigous supervision.
InAssociation for the Advancement of Artificial Intelli-gence (AAAI), pages 895?900.Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichiTsujii.
2003.
Genia corpus?a semantically annotatedcorpus for bio-textmining.
Bioinformatics, 19:i180?i182.Joel Lang and Mirella Lapata.
2010.
Unsupervised in-duction of semantic roles.
In Proceedings of the 48rdAnnual Meeting of the Association for ComputationalLinguistics (ACL), Uppsala, Sweden.Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.2007.
The infinite PCFG using hierarchical dirich-let processes.
In Joint Conf.
on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages688?697, Prague, Czech Republic.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning semantic correspondences with less supervi-sion.
In Proc.
of the Annual Meeting of the Associationfor Computational Linguistics and International JointConference on Natural Language Processing (ACL-IJCNLP).Dekang Lin and Patrick Pantel.
2001.
Dirt ?
discoveryof inference rules from text.
In Proc.
of InternationalConference on Knowledge Discovery and Data Min-ing, pages 323?328.1454Raymond J. Mooney.
2007.
Learning for semantic pars-ing.
In Proceedings of the 8th International Confer-ence on Computational Linguistics and Intelligent TextProcessing, pages 982?991.Alexis Palmer and Caroline Sporleder.
2010.
Evaluatingframenet-style semantic parsing: the role of coveragegaps in framenet.
In Proceedings of the Conference onComputational Linguistics (COLING-2000), Beijing.Jim Pitman.
2002.
Poisson-dirichlet and gem invari-ant distributions for split-and-merge transformationsof an interval partition.
Combinatorics, Probabilityand Computing, 11:501?514.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, (EMNLP-09).Matt Richardson and Pedro Domingos.
2006.
Markovlogic networks.
Machine Learning, 62:107?136.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A latentdirichlet alocation method for selectional preferences.In Proceedings of the 48rd Annual Meeting of the As-sociation for Computational Linguistics (ACL), Upp-sala, Sweden.Diarmuid O?
Se?aghdha.
2010.
Latent variable modelsof selectional preference.
In Proceedings of the 48rdAnnual Meeting of the Association for ComputationalLinguistics (ACL), Uppsala, Sweden.R.
Swier and S. Stevenson.
2004.
Unsupervised seman-tic role labelling.
In Proceedings of EMNLP, pages95?102, Barcelona, Spain.Y.
W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006.Hierarchical Dirichlet processes.
Journal of the Amer-ican Statistical Association, 101(476):1566?1581.Y.
W. Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proceed-ings of the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 985?992.Cynthia A. Thompson, Roger Levy, and Christopher D.Manning.
2003.
A generative model for semantic rolelabeling.
In In Senseval-3, pages 397?408.Ivan Titov and Mikhail Kozhevnikov.
2010.
Bootstrap-ping semantic analyzers from non-contradictory texts.In Proceedings of the 48rd Annual Meeting of the As-sociation for Computational Linguistics (ACL), Upp-sala, Sweden.Alexander Yates and Oren Etzioni.
2009.
Unsupervisedmethods for determining object and relation synonymson the web.
Journal of Artificial Intelligence Research,34:255?296.B.
Zapirain, E. Agirre, L. L. Ma`rquez, and M. Surdeanu.2010.
Improving semantic role classification with se-lectional prefrences.
In Proceedings of the Meetingof the North American chapter of the Association forComputational Linguistics (NAACL 2010), Los Ange-les.Luke Zettlemoyer and Michael Collins.
2005.
Learn-ing to map sentences to logical form: Structured clas-sification with probabilistic categorial grammar.
InProceedings of the Twenty-first Conference on Uncer-tainty in Artificial Intelligence, Edinburgh, UK, Au-gust.1455
