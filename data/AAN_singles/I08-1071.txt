Augmenting Wikipedia with Named Entity TagsWisam DakkaColumbia University1214 Amsterdam AvenueNew York, NY 10027wisam@cs.columbia.eduSilviu CucerzanMicrosoft Research1 Microsoft WayRedmond, WA 98052silviu@microsoft.comAbstractWikipedia is the largest organized knowledgerepository on the Web, increasingly employedby natural language processing and search tools.In this paper, we investigate the task of labelingWikipedia pages with standard named entitytags, which can be used further by a range of in-formation extraction and language processingtools.
To train the classifiers, we manually anno-tated a small set of Wikipedia pages and then ex-trapolated the annotations using the Wikipediacategory information to a much larger trainingset.
We employed several distinct features foreach page: bag-of-words, page structure, ab-stract, titles, and entity mentions.
We report highaccuracies for several of the classifiers built.
Asa result of this work, a Web service that classi-fies any Wikipedia page has been made availableto the academic community.1 IntroductionWikipedia, one of the most frequently visited websites nowadays, contains the largest amount ofknowledge ever gathered in one place by volunteercontributors around the world (Poe, 2006).
EachWikipedia article contains information about oneentity or concept, gathers information aboutentities of one particular type of entities (the so-called list pages), or provides information abouthomonyms (disambiguation pages).
As of July2007, Wikipedia contains close to two millionarticles in English.
In addition to the English-language version, there are 200 versions in otherlanguages.
Wikipedia has about 5 millionregistered contributors, averaging more than 10edits per contributor.Natural language processing and search tools cangreatly benefit from Wikipedia by using it as anauthoritative source of common knowledge and byexploiting its interlinked structure anddisambiguation pages, or by extracting concept co-occurrence information.
This paper presents asuccessful study on enriching the Wikipedia datawith named entity tags.
Such tags could beemployed by disambiguation systems such asBunescu and Paca (2006) and Cucerzan (2007), inmining relationships between named entities, or inextracting useful facet terms from news articles(e.g., Dakka and Ipeirotis, 2008).In this work, we classify the Wikipedia pagesinto categories similar to those used in the CoNLLshared tasks (Tjong Kim Sang, 2002; Tjong KimSang and De Meulder, 2003) and ACE(Doddington et al, 2004).
To the best of ourknowledge, this is the first attempt to perform suchclassification on the English language version ofthe collection.
1  Although the task settings aredifferent, the results we obtained are comparablewith those previously reported in documentclassification tasks.We examined the Wikipedia pages to extractseveral feature groups for our classification task.We also observed that each entity/concept has atleast two pseudo-independent views (page-basedfeatures and link-based features), which allow theuse a co-training method to boost the performanceof classifiers trained separately on each view.The classifier that achieved the best accuracy onout test set was applied then to all Wikipedia pagesand its classifications are provided to the academiccommunity for use in future studies through a Webservice.21Watanabe et al (2007) have reported recently experi-ments on categorizing named entities in the Japaneseversion of Wikipedia using a graph-based approach.2The Web service is available at wikinet.stern.nyu.edu.5452 Related WorkThis study is related to the area of named entityrecognition, which has supported extensive evalua-tions (CoNLL and ACE).
Since the introduction ofthis task in MUC-6 (Grishman and Sundheim,1996), numerous systems using various ways ofexploiting entity-specific and local context featureswere proposed, from relatively simple character-based models such as Cucerzan and Yarowsky(2002) and Klein et al (2003) to complex modelsmaking use of various lexical, syntactic, morpho-logical, and orthographical information, such asWacholder et al (1997), Fleischman and Hovy(2002), and Florian et al (2003).
While the task weaddress is not the conventional named entity rec-ognition but rather document classification, ourclasses are a derived from the labels traditionallyemployed in named entity recognition, followingthe CoNLL and ACE guidelines, as described inSection 3.The areas of text categorization and documentclassification have also been extensively re-searched over time.
These task have the goal ofassigning to each document in a collection one orseveral labels from a given set, such as News-groups (Lang, 1995), Reuters (Reuters, 1997), Ya-hoo!
(Mladenic, 1998), Open Directory Project(Chakrabarti et al, 2002), and Hoover?s Online(Yang et al, 2002).
Various supervised machinelearning algorithms have been applied successfullyto the document classification problem (e.g.,Joachims, 1999; Quinlan, 1993; Cohen, 1995).Dumais et al (1998) and Yang and Liu (1999) re-ported that support vector machines (SVM) and K-Nearest Neighbor performed the best in text cate-gorization.
We adopted SVM as our algorithm ofchoice because of these findings and also becauseSVMs have been shown robust to noise in the fea-ture set in several studies.
While Joachims (1998)and Rogati and Yang (2002) reported no improve-ment in SVM performance after applying a featureselection step, Gabrilovich and Markovitch (2004)showed that for collection with numerous redun-dant features, aggressive feature selection allowedSVMs to actually improve their performance.However, performing an extensive investigation ofclassification performance across various machinelearning algorithms has been beyond the purposeof this work, in which we ran classification ex-periments using SVMs and compared them onlywith the results of similar systems employingNa?ve Bayes.In addition to the traditional bag-of-words,which has been extensively used for the documentclassification task (e.g.
Sebastiani, 2002), we em-ployed various other Wikipedia-specific featuresets.
Some of these have been previously employedfor various tasks by Gabrilovich and Markovitch,(2006); Overell and Ruger (2006), Cucerzan(2007), and Suchanek et al (2007).3 Classifying Wikipedia PagesThe Wikipedia pages that we analyzed in this studycan be divided into three types:Disambiguation Page (DIS): is a special kind ofpage that usually contains the word ?disambigua-tion?
in its title, and that contains several possibledisambiguations of a term.Common Page (COMM): refers to a commonobject rather than a named entity.
Generally, if thename of an object or concept appears non-capitalized in text then it is very likely that the ob-ject or the concept is of common nature (heuristicpreviously employed by Bunescu and Paca, 2006).For example, the Wikipedia page ?Guitar?
refers toa common object rather than a named entity.Named Entity Page: refers to a specific objector set of objects in the world, which is/are com-monly referred to using a certain proper nounphrase.
For example, any particular person is anamed entity, though the concept of ?people?
isnot a named entity.
Note that most names are am-biguous.
?Apollo?
can refer to more than 30 differ-ent entities of different types, for example, the Fin-nish rock band of the late 1960s/early 1970s , theGreek god of light, healing, and poetry, and theseries of space missions run by NASA.To classify the named entities in Wikipedia, weadopted a restricted version of the ACE guidelines(ACE), using four main entity classes (also similarto the classes employed in the CoNLL evaluations):Animated Entities (PER): An animate entitycan be either of type human or non-human.
Hu-man entities are either humans that are known tohave lived (e.g., ?Leonardo da Vinci?, ?BritneySpears?, ?Gotthard of Hildesheim?, ?SaintGodehard?)
or humanoid individuals in fictionalworks, such as books, movies, TV shows, andcomics (e.g., ?Harry Potter?, ?Batman?, ?Sonny?546the robot from the movie ?I, Robot?).
Fictionalcharacters also include mythological figures anddeities (e.g.
?Zeus?, ?Apollo?, ?Jupiter?).
The fic-tional nature of a character must be explicitly indi-cated.
Non-human entities are any particular ani-mal or alien that has lived or that is described in afictional work and can be singled out using a name.Organization Entities (ORG): An organizationentity must have some formally established asso-ciation.
Typical examples are businesses (e.g.,?Microsoft?, ?Ford?
), governmental bodies (e.g.,?United States Congress?
), non-governmental or-ganizations (e.g., ?Republican Party?, ?AmericanBar Association?
), science and health units (e.g.,?Massachusetts General Hospital?
), sports organi-zations and teams (e.g., ?Angolan Football Federa-tion?, ?San Francisco 49ers?
), religious organiza-tions (e.g., ?Church of Christ?
), and entertainmentorganizations, including formally organized musicgroups (e.g., ?San Francisco Mime Troupe?, therock band ?The Police?).
Industrial sectors andindustries (e.g., ?Petroleum industry?)
are alsotreated as organization entities, as well as all mediaand publications.Location Entities (LOC): These are physical lo-cations (regions in space) defined by geographical,astronomical, or political criteria.
They are of threetypes: Geo-Political entities are composite entitiescomprised of a physical location, a population, agovernment, and a nation (or province, state,county, city, etc.).
A Wikipedia page that mentionsall these components should be labeled as Geo-Political Entity (e.g., ?Hawaii?, ?European Union?,?Australia?, and ?Washington, D.C.?).
Locationsare places defined on a geographical or astronomi-cal basis and do not constitute a political entity.These include mountains, rivers, seas, islands, con-tinents (e.g., ?the Solar system?, ?Mars?, ?HudsonRiver?, and ?Mount Rainier?).
Facilities are arti-facts in the domain of architecture and civil engi-neering, such as buildings and other permanentman-made structures and real estate improvements:airports, highways, streets, etc.Miscellaneous Entities (MISC): About 25% ofthe named entities in Wikipedia are not of thetypes listed above.
By examining several hundredexamples, we concluded that the majority of thesenamed entities can be classified in one of the fol-lowing classes: Events refer to historical events oractions with some certain duration, such as wars,sport events, and trials (e.g., ?Gulf War?, ?2006FIFA World Cup?, ?Olympic Games?, ?O.J.
Simp-son trial?).
Works of art refer to named works thatare imaginative in nature.
Examples include books,movies, TV programs, etc.
(e.g., the ?Batman?movie, ?The Tonight Show?, the ?Harry Potter?books).
Artifacts refer to man-made objects orproducts that have a name and cannot generally belabeled as art.
This includes mass-produced mer-chandise and lines of products (e.g.
the camera?Canon PowerShot Pro1?, the series ?Canon Pow-erShot?, the type of car ?Ford Mustang?, the soft-ware ?Windows XP?).
Finally Processes includeall named physical and chemical processes (e.g.,?Ettinghausen effect?).
Abstract formulas or algo-rithms that have a name are also labeled as proc-esses (e.g., ?Naive Bayes classifier?
).4 Features Used.
Independent ViewsWhen creating a Wikipedia page and introducinga new entity, contributors can refer to other relatedWikipedia entities, which may or may not havecorresponding Wikipedia pages.
This way of gen-erating content creates an internal web graph and,interesting, results in the presence of two differentand pseudo-independent views for each entity.
Wecan represent an entity using the content written onthe entity page, or alternatively, using the contextfrom a reference on the related page.
For example,Figures 1 and 2 show the two independent views ofthe entity ?Gwen Stefani?.Figure 1.
A partial list of contextual references takenfrom Wikipedia for the named entity ?Gwen Stefani?.
(There are over 600 such references.
)1 such as ?Let Me Blow Ya Mind?
by Eve and [[GwenStefani]] (whom he would produce2 In the video ?
[[Cool (song)?Cool]]?, [[Gwen Stefani]]is made-up as Monroe.3 ?
[[South Side (song)?South Side]]?
(featuring [[GwenStefani]]) #14 US4  [[1969]] - [[Gwen Stefani]], American singer ([[NoDoubt]])5 [[Rosie Gaines]], [[Carmen Electra]], [[Gwen Stefani]],[[Chuck D]], [[Angie Stone]],6 In late [[2004]], [[Gwen Stefani]] released a hit songcalled ?Rich Girl?
which7 [[Gwen Stefani]] - lead singer of the band [[NoDoubt]], who is now a successful8 [[Social Distortion]], and [[TSOL]].
[[Gwen Stefani]],lead vocalist of the [[alternative rock]]9 main proponents (along with [[Gwen Stefani]] and[[Ashley Judd]]) in bringing back the10 The [[United States?American]] singer [[GwenStefani]] references Harajuku in several547Figure 2.
Wikipedia page for the named entity ?GwenStefani?.
Other than the regular text, information suchas surface and disambiguated entities, structure proper-ties, and section titles can be easily extracted.We utilize this important observation to extract ourfeatures based on these two independent views:page-based features and context features.
We dis-cuss these in greater detail next.4.1 Page-Based FeaturesA typical Wikipedia page is usually written andedited by several contributors.
Each page includesa rich set of information including the followingelements: titles, section titles, paragraphs, multi-media objects, hyperlinks, structure data, surfaceentities and their disambiguations.
Figure 2 showssome of these elements in the page dedicated tosinger ?Gwen Stefani?.
We use the Wikipedia pageXML syntax to draw a set of different page-basedfeature vectors, including the following:Bag of Words (BOW): This vector is the termfrequency representation of the entire page.Structured Data (STRUCT): Many Wikipediapages contain useful data organized in tables andother structural representations.
In Figure 2, we seethat contributors have used a table representationto list different properties about Gwen Stefani.
Weextract for each page, using the Wikipedia syntax,the bag-of-words feature vector that corresponds tothis structured data only.Figure 3.
The abstract provided by Wikipedia for?Gwen Stefani?.
Note the concatenation of ?Stefani?and ?Some?, which results in a new word, and is a rele-vant example of noise encountered in Wikipedia text.First Paragraph (FPAR): We examined severalhundred pages, and observed that a human couldlabel most of the pages by reading only the firstparagraph.
Therefore, we built the feature vectorthat contains the bag-of-word representation of thepage?s first paragraph.Abstract (ABS): For each page, Wikipedia pro-vides a summary of several lines about the entitydescribed on the page.
We use this summary todraw another bag-of-word feature vector based onthe provided abstracts only.
For example, Figure 3shows the abstract for the entity ?Gwen Stefani?.Surface Forms and Disambiguations (SFD):Contributors use the Wikipedia syntax to link fromone entity page to another.
In the page of Figure 2,for example, we have references to several otherWikipedia entities, such as ?hip hop?, ?R&B?, and?Bush?.
Wikipedia page syntax lets us extract thedisambiguated meaning of each of these references,which are ?Hip hop music,?
?Rhythm and blues,?and ?Bush band?, respectively.
For each page, weextract all the surface forms used by contributors intext (such as ?hip hop?)
and their disambiguatedmeanings (such as ?Hip hop music?
), and buildfeature vectors to represent them.4.2 Context FeaturesFigure 1 shows some of the ways contributors toWikipedia refer to the entity ?Gwen Stefani?.
TheWikipedia version that we analyzed contains about35 million references to entities in the collection.On average, each page has five references to otherentities.We decided to make use of the text surroundingthese references to draw contextual features, whichcan capture both syntactic and semantic propertiesof the referenced entity.
For each entity reference,we compute the feature vectors by using a textwindow of three words to the left and to the rightof the reference.<abstract>Gwen Rene StefaniSome sources give Stefani?s first nameas Gwendolyn, but her first name is simply Gwen.
Her list-ing on the California Birth Index from the Center for HealthStatistics gives a birth name of Gwen Rene Stefani.</abstract>548BOW 1,821,966 ABS 372,909SFD 847,857 BCON 35,178,120STRUCT 159,645 FPAR 781,938Table 1.
Number of features in each group, as obtainedby examining all the Wikipedia pages.We derived a unigram context model and a bigramcontext model, following the findings of previouswork that such models benefit from employinginformation about the position of words relative tothe targeted term:Unigram Context (UCON): The feature vectoris constructed in a way that preserves the positionalinformation of words in the context.
Each feature ftiin the vector represents the total number of times aterm t appears in position i around the entity.Bigram Context (BCON): The bigram-basedcontext model was built in a similar way to UCON,so that relative positional information is preserved.5 ChallengesFor our classification task, we faced severalchallenges.
First, many Wikipedia entities haveonly a partial list of the feature groups discussedabove.
For example, contributors may refer to enti-ties that do not exist in Wikipedia but might beadded in the future.
Also, not all the page-basedfeatures groups are available for every entity page.For instance, abstracts and structure features areonly available for 68% and 79% of the pages, re-spectively.
Second, we only had available severalhundred labeled examples (as described in Section6.1).
Third, the feature space is very large com-pared to the typical text classification problem (seeTable 1), and a substantial amount of noise plaguesthe data.
A further investigation revealed that thedifference in the dimensionality compared to textclassification stems from the way Wikipedia pagesare created: contributors make spelling errors, in-troduce new words, and frequently use slang, acro-nyms, and other languages than English.We utilize all the features groups described inSection 4 and various combinations of them.
Thisprovides us with greater flexibility to use classifi-ers trained on different feature groups whenWikipedia entities miss certain types of features.In addition, we try to take advantage of the inde-pendent views of each entity by employing a co-training procedure (Blum and Mitchell, 1998; Ni-gam and Ghani, 2000).
In previous work, this hasbeen shown to boost the performance of the weakclassifiers on certain feature groups.
For example,it is interesting to determine whether we can usethe STRUCT view of a Wikipedia pages to boostthe performance of the classifiers based on context.Alternatively, we can employ co-training on theSTRUCT and SFD features, hypothesized as twoindependent views of the data.6 Experiments and Findings6.1 Training DataWe experimented with two data sets: HumanJudged Data (HJD): This set was obtained in anannotation effort that followed the guidelines pre-sented in Section 3.
Due to the cost of the labelingprocedure, this set was limited to a small randomset of 800 Wikipedia pages.
Human Judged DataExtended (HJDE): The initial classification resultsobtained using a small subset of HJD hinted to theneed for more training data.
Therefore, we deviseda procedure that takes advantage of the fact thatWikipedia contributors have assigned many of thepages to one or more lists.
For example, the page?List of novelists?
contains a reference to ?OrhanPamuk?, which is part of the HJD and is labeled asPER.
Our extension procedure first uses the pagesin the training set from HJD to extract the lists inWikipedia that contain references to them and thenprojects the entity labels of the seeds to all ele-ments in the lists.
Unfortunately, not all theWikipedia lists contain only references named enti-ties of the same category.
Furthermore, some listsare hierarchical and include sub-lists of differentclasses.
To overcome these issues, we examinedonly leaf lists and manually filtered all the lists thatby definition could have pages of different catego-ries.
Finally, we filtered out all list pages that con-tain entities in two or more entity classes (as de-scribed in Section 3).Our partially manual extension procedure is asfollows: 1) Pick a random sample of 400 entitiesfrom HJD along with their human judged labels; 2)Extract all the lists that contain any entity from thislabeled sample; 3) Filter out the lists that containentities from different entity classes (PER, ORG,LOC, MISC, and COM); 4) propagate the entitylabels of the known entities in the lists to the otherreferenced entities; 5) Choose a random samplefrom all labeled pages with respect to the entityclass distribution observed in HJD.549PER MISC ORG LOC COMM41% 25.1% 11.2% 11.7% 11%Table 2.
The distribution of labels in the HJDE data set.Our extension procedure resulted initially in 770lists, which were then reduced to 501.
In step (5),we chose a maximal random sample from all la-beled pages in HJDE so that it matched the entityclass distribution in the original HJD training set(shown in Table 2).6.2 ClassificationFrom the numerous machine learning algorithmsavailable for our classification task (e.g., Joachims,1999; Quinlan, 1993; Cohen, 1995), we chose tothe SVMs (Vapnik, 1995), and the Na?ve Bayes(John and Langley, 1995) algorithms because bothcan output probability estimates for their predic-tions, which are necessary for the co-training pro-cedure.
We use an implementation of SVM (Platt,1999) with linear kernels and the Na?ve Bayes im-plementation from the machine learning toolkitWeka3.
Our implementation of co-training fol-lowed that of Nigam and Ghani (2000).Using the HJDE data, we experimented withlearning a classifier for each feature group dis-cussed in Section 4.
We report the results for twoclassification tasks: binary classification to identifyall the Wikipedia pages of type PER, and 5-foldclassification (PER, COM, ORG, LOC, and MISC).To reduce the feature space, we built a term fre-quency dictionary taken from one year?s worth ofnews data and restrict our feature space to containonly terms with frequency values higher than 10.6.3 Results on Bag-of-wordsThis feature group is of particular interest, since ithas been widely used for document classificationand also, because every Wikipedia page has aBOW representation.
We experimented with thetwo classification tasks for this feature group.
Forthe binary classification task, both SVM and Na?veBayes performed remarkably well, obtaining accu-racies of 0.962 and 0.914, respectively.
Table 3shows detailed performance numbers for SVM andNa?ve Bayes for the multi-class task.
Unlike in thebinary case, Na?ve Bayes falls short of achievingresults similar to those from SVM, which obtainsan average F-measure of 0.928 and an average pre-cision of 0.931.Precision Recall F-measureSVM NB SVM NB SVM NBPER 0.944 0.918 0.959 0.771 0.951 0.838MISC 0.927 0.824 0.920 0.687 0.924 0.750ORG 0.940 0.709 0.928 0.701 0.934 0.705LOC 0.958 0.459 0.949 0.863 0.954 0.599COMM 0.887 0.680 0.869 0.714 0.878 0.697Table 3.
Precision, recall, and F1 measure for the multi-class classification task.
Results are obtained usingSVM and Na?ve Bayes after a stratified cross-validationusing HJDE data set and the bag-of-words features.SFD 83.14% ABS 68.96%STRUCT 79.55% BCON 83.57%Table 4.
Percentage of available examples HJDE foreach feature group.Precision Recall F-measureSVM NB SVM NB SVM NBBOW 0.901 0.858 0.894 0.880 0.897 0.869SFD 0.851 0.775 0.830 0.882 0.840 0.825STRUCT 0.888 0.840 0.875 0.856 0.881 0.848FPAR 0.867 0.872 0.854 0.896 0.860 0.884ABS 0.861 0.833 0.852 0.885 0.857 0.858BCON 0.311 0.245 0.291 0.334 0.300 0.283Table 5.
Average precision, recall, and F1 measure val-ues for the multi-class task.
Results are obtained usingSVM and Na?ve Bayes across the different featuregroups on the test set of HJDE.6.4 Results on Other Feature GroupsWe present now the results obtained using othergroups of features.
We omit the results on UCONdue to their similarity with BCON.
Recall thatthese features may not be present in all Wikipediapages.
Table 4 shows the availability of these fea-tures in the HJDE set.
The lack of one featuregroup has a negative impact on the results of thecorresponding classifier, as shown in Table 5.
No-ticeably, the results of the STRUCT features arevery encouraging and confirm our hypothesis thatsuch features are distinctive in identifying the typeof the page.
While results using STRUCT andFPAR are high, they are lower than the results ob-tained on BOW.
In general, using SVM with BOWperformed better than any other feature set, averag-ing 0.897 F-measure on test set.
This could be be-cause when using BOW, we have a larger trainingset than any other feature group.
SVM withSTRUCT and Na?ve Bayes with FPAR performed550second and third best, with average F1 measurevalues of 0.881 and 0.860, respectively.
The resultsalso show that it is difficult to learn if a page isCOMM in all learning combination.
This could berelated to the membership complexity of that class.Finally, the results on the bigram contextual fea-tures, namely BCON, for both SVM and Na?veBayes are not encouraging and surprisingly low.6.5 Results for Co-trainingMotivated by the fact that some feature groups canbe seen as independent views of the data, we useda co-training procedure to boost the classificationaccuracy.
One combination of views that we exam-ined is BCON with BOW, hoping to boost theclassification performance of the bigram contextfeatures, as this classifier could be used for entitiesin any new text, not only for Wikipedia pages .Unfortunately, the results were not encouraging ineither of the cases (SVM and Na?ve Bayes) and fornone of the other feature groups used instead ofBOW.
This indicates that the context features ex-tracted have limited power and that further investi-gation of extracting relevant context features fromWikipedia is necessary.7 Conclusions and Future WorkIn this paper, we presented a study on the classifi-cation of Wikipedia pages with named entity labels.We explored several alternatives for extractinguseful page-based and context-based features suchas the traditional bag-of-words, page structure, hy-perlink text, abstracts, section titles, and n-gramcontextual features.
While the classification withpage features resulted in high classification accu-racy, context-based and structural features did notwork similarly well, either alone or in a co-trainingsetup.
This motivates future work to extract bettersuch features.
We plan to examine employing moresophisticated ways both for extracting contextualfeatures and for using the implicit Wikipedia graphstructure in a co-training setup.Recently, the Wikipedia foundation has beentaken steps toward enforcing a more systematicway to add useful structured data on each page bysuggesting templates to use when a new page getsadded to the collection.
This suggests that in a not-so-distant future, we may be able to utilize thestructured data features as attribute-value pairsrather than as bags of words, which is prone to los-ing valuable semantic information.Finally, we have applied our classifier to allWikipedia pages to determine their labels andmade these data available in the form of a Webservice, which can positively contribute to futurestudies that employ the Wikipedia collection.ReferencesACE Project.
At http://www.nist.gov/speech/history/in-dex.htmReuters-1997.
1997.
Reuters-21578 text categorizationtest collection.
At http://www.daviddlewis.com/re-sources/testcollections/reuters21578A.
Blum and T. Mitchell.
1998.
Combining labeled andunlabeled data with co-training.
In Proceedings ofCOLT?98, pages 92?100.A.
Borthwick, J.
Sterling, E. Agichtein, and R. Grish-man.
1998.
NYU: Description of the MENE namedentity system as used in MUC.
In Proceedings ofMUC-7.R.
Bunescu and M. Pasca.
2006.
Using encyclopedicknowledge for named entity disambiguation.
In Pro-ceedings of EACL-2006, pages 9?16.S.
Chakrabarti, M.M.
Joshi, K. Punera, and D.M.
Pen-nock.
2002.
The structure of broad topics on the web.In: Proceedings of WWW ?02, pages 251?262.W.W.
Cohen.
1995.
Fast effective rule induction.
InProceedings of ICML?95.S.
Cucerzan.
2007.
Large-Scale Named Entity Disam-biguation Based on Wikipedia Data.
In Proceedingsof EMNLP-CoNLL 2007, pages 708?716.S.
Cucerzan and D. Yarowsky.
2002.
Language Inde-pendent NER using a Unified Model of Internal andContextual Evidence, in Proceedings of CoNLL 2002,pages 171?174.W.
Dakka and P. G. Ipeirotis.
2008.
Automatic Extrac-tion of Useful Facet Terms from Text Documents.
InProceedings of ICDE 2008 (to appear).G.
Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,S.
Strassel, and R. Weischedel.
2004.
ACE pro-gram ?
task definitions and performance measures.
InProceedings of LREC, pages 837?840.S.
Dumais, J. Platt, D. Heckerman, and M. Sahami.1998.
Inductive learning algorithms and representa-tions for text categorization.
In Proceedings ofCIKM ?98, pages 148?155.M.
Fleischman and E. Hovy.
2002.
Fine Grained Classi-fication of Named Entities.
In Proceedings of COL-ING?02, pages 267?273.551R.
Florian, A. Ittycheriah, H. Jing, and T. Zhang,Named Entity Recognition through Classifier Com-bination, in Proceedings of CoNLL 2003, pages 168?171.E.
Gabrilovich and S. Markovitch.
2004.
Text categori-zation with many redundant features: using aggres-sive feature selection to make SVMs competitivewith c4.5.
In Proceedings of ICML ?04, page 41.E.
Gabrilovich and S. Markovitch.
2006.
Overcoming-the brittleness bottleneck using Wikipedia: Enhanc-ing text categorization with encyclopedic knowledge.In Proceedings of AAI 2006.R.
Grishman and B. Sundheim.
1996.
Message Under-standing Conference - 6: A brief history.
In Proceed-ings of COLING, 466-471.T.
Joachims.
1998.
Text categorization with supportvector machines: Learning with many relevant fea-tures.
In Proceedings of ECML ?98, pages 137?142.T.
Joachims.
1999.
Making large-scale support vectormachine learning practical.
Advances in kernel meth-ods: support vector learning, pages 169?184.G.H.
John and P. Langley.
1995.
Estimating continuousdistributions in Bayesian classifiers.
Proceedings ofthe Eleventh Conference on Uncertainty in ArtificialIntelligence, pages 338?345.D.
Klein, J. Smarr, H. Nguyen, and C. D. Manning.2003.
Named Entity Recognition with Character-Level Models, in Proceedings of CoNLL 2003.K.
Lang.
1995.
NewsWeeder: Learning to filter netnews.In Proceedings of ICML?95, pages 331?339.D.
Mladenic.
1998.
Feature subset selection in textlearning.
In Proceedings of ECML ?98, pages 95?100.K.
Nigam and R. Ghani.
2000.
Analyzing the effective-ness and applicability of co-training.
In Proceedingsof CIKM?00, pages 86?93.S.E.
Overell and S. Ruger.
2006.
Identifying andgrounding descriptions of places.
In Workshop onGeographic Information Retrieval, SIGIR 2006.J.C.
Platt.
1999.
Fast training of support vector ma-chines using sequential minimal optimization.
Ad-vances in kernel methods: support vector learning,pages 185?208.M.
Poe.
2006.
The hive: Can thousands of wikipediansbe wrong?
How an attempt to build an online ency-clopedia touched off history?s biggest experiment incollaborative knowledge.
The Atlantic Monthly, Sep-tember 2006.J.R.
Quinlan.
1993.
C4.5: programs for machine learn-ing.
Morgan Kaufmann Publishers Inc.M.
Rogati and Y. Yang.
2002.
High-performing featureselection for text classification.
In Proceedings ofCIKM ?02, pages 659?661.F.
Sebastiani.
2002.
Machine learning in automated textcategorization.
ACM Computing.
Surveys, 34(1):1?47.F.M.
Suchanek, G. Kasneci, and G. Weikum.
2007.Yago: A Core of Semantic Knowledge.
In Proceed-ings of WWW 2007.E.F.
Tjong Kim Sang and F. De Meulder.
2003.
Intro-duction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proceed-ings of CoNLL-2003, pages 142?147.E.
F. Tjong Kim Sang.
2002.
Introduction to theCoNLL-2002 shared task: Language-independentnamed entity recognition.
In Proceedings of CoNLL-2002, pages 155?158.V.N.
Vapnik.
1995.
The nature of statistical learningtheory.
Springer-Verlag New York, Inc.N.
Wacholder., Y. Ravin, and M. Choi.
1997.
Disam-biguation of proper names in text.
In Proceedings ofANLP?97, pages 202-208.Y.
Watanabe, M. Asahara, and Y. Matsumoto.
2007.
AGraph-based Approach to Named Entity Categoriza-tion in Wikipedia using Conditional Random Fields.In Proc.
of EMNLP-CoNLL 2007, pages 649-657.Y.
Yang and X. Liu.
1999.
A re-examination of textcategorization methods.
In Proceedings of SIGIR ?99,pages 42?49.Y.
Yang, S. Slattery, and R. Ghani.
2002.
A study ofapproaches to hypertext categorization.
Journal of.Intelligent.
Information.
Systems., 18(2-3):219?241.552
