Improving Summarization Performance by Sentence Compression ?A Pilot StudyChin-Yew LinUniversity of Southern California/Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292, USAcyl@isi.eduAbstractIn this paper we study the effectiveness ofapplying sentence compression on an ex-traction based multi-document summari-zation system.
Our results show that puresyntactic-based compression does not im-prove system performance.
Topic signa-ture-based reranking of compressedsentences does not help much either.However reranking using an oracleshowed a significant improvement re-mains possible.Keywords: Text Summarization, SentenceExtraction, Sentence Compression,Evaluation.1 IntroductionThe majority of systems participating in the pastDocument Understanding Conference (DUC, 2002)(a large scale summarization evaluation effortsponsored by the United States government), andthe Text Summarization Challenge (Fukusima andOkumura, 2001) (sponsored by Japanese govern-ment) are extraction based.
Extraction-based auto-matic text summarization systems extract parts oforiginal documents and output the results as sum-maries (Chen et al, 2003; Edmundson, 1969;Goldstein et al, 1999; Hovy and Lin, 1999; Kupiecet al, 1995; Luhn, 1969).
Other systems based oninformation extraction (McKeown et al, 2002;Radev and McKeown, 1998; White et al, 2001)and discourse analysis (Marcu, 1999; Strzalkowskiet al, 1999) also exist but they are not yet usablefor general-domain summarization.
Our study fo-cuses on the effectiveness of applying sentencecompression techniques to improve the perform-ance of extraction-based automatic text summariza-tion systems.Sentence compression aims to retain the most sali-ent information of a sentence, rewritten in a shortform (Knight and Marcu, 2000).
It can be used todeliver compressed content to portable devices(Buyukkokten et al, 2001; Corston-Oliver, 2001)or as a reading aid for aphasic readers (Carroll etal., 1998) or the blind (Grefenstette, 1998).
Earlierresearch in sentence compression focused on com-pressing single sentences, and were evaluated on asentence by sentence basis.
For example, Jing(2000) trained her system on a set of 500 sentencesfrom the Benton Foundation(http://www.benton.org) and their reduced formswritten by humans.
The results were evaluated atthe parse tree level against the reduced trees; whileKnight and Marcu (2000) trained their system on aset of 1,067 sentences from Ziff-Davis magazinearticles and evaluated their results on grammatical-ity and importance rated by humans.
Both reportedsuccess in their evaluation criteria.
However, nei-ther of them reported their techniques?
effective-ness in improving the overall performance ofautomatic text summarization systems.
The goal ofthis pilot study is set to answer this question andprovide a guideline for future research.Section 2 gives an overview of Knight and Marcu?ssentence compression algorithm that we used tocompressed summary sentences.
Section 3 de-scribes the multi-document summarization system,NeATS, which was used as our testbed.
Section 4introduces a recall-based unigram co-occurrenceautomatic evaluation metric.
Section 5 presents theexperimental design.
Section 6 shows the empiricalresults.
Section 7 concludes this paper and dis-cusses future directions.2 A Noisy-Channel Model for SentenceCompressionKnight and Marcu (K&M) (2000) introduced twosentence compression algorithms, one based on thenoisy-channel model and the other decision-based.We use the noisy-channel model in our experi-ments since it is able to generate a list of rankedcandidates, while the decision-based is not.?
Source model P(s) ?
The compressed sen-tence language model.
This would assign lowprobability to short sentences with undesir-able features, for example, ungrammatical ortoo short.?
Channel model P(t | s) ?
Given a compressedsentence s, the channel model assigns theprobability of an original sentence, t, whichcould have been generated by s.?
Decoder ?
Given the original sentence t, findthe best short sentence s generated from t, i.e.maximizing P(s | t).
This is equivalent tomaximizing P(t | s)?P(s).We used K&M?s sentence compression algorithmas it was and did not retrain on new corpus.
Wealso adopted the compression length-adjusted logprobability to avoid the tendency of selecting veryshort compressions.
Figure 1 shows a list of com-pressions for the sentence ?In Louisiana, the hurri-cane landed with wind speeds of about 120 milesper hour and caused severe damage in smallcoastal centres such as Morgan City, Franklin andNew Iberia.?
ranked according to their length-adjusted log-probability.3 NeATS ?
a Multi-Document SummarizationSystemNeATS (Lin and Hovy, 2002) is an extraction-based multi-document summarization system.
It isamong the top two performers in DUC 2001 and2002 (Over and Liggett, 2002).
It consists of threemain components:?
Content Selection ?
The goal of content selec-tion is to identify important concepts men-tioned in a document collection.
NeATScomputes the likelihood ratio ?
(Dunning,1993) to identify key concepts in unigrams, bi-grams, and trigrams, and clusters these con-cepts in order to identify major subtopicswithin the main topic.
Each sentence in thedocument set is then ranked, using the keyconcept structures.
These n-gram key conceptsare called topic signatures (Lin and Hovy2000).
We used key n-grams to rerank com-pressions in our experiments.?
Content Filtering ?
NeATS uses three differentfilters: sentence position, stigma words, andmaximum marginal relevancy.
Sentence posi-tion has been used as a good content filter sincethe late 60s (Edmundson, 1969).
We apply asimple sentence filter that only retains the 10lead sentences.
Some sentences start withstigma words such as conjunctions, quotationmarks, pronouns, and the verb ?say?
and its de-rivatives usually cause discontinuity in summa-ries.
We simply reduce the scores of thesesentences to demote their ranks and avoid in-cluding them in summaries of small sizes.
ToNumber of Words Adjusted Log-Prob Raw Log-Prob Sentence14 -9.212 -128.967 In Louisiana, the hurricane landed with wind speeds of about 120 miles per hour.14 -9.216 -129.022 The hurricane landed and caused severe damage in small centres such as Morgan City.12 -9.252 -111.020 In Louisiana, the hurricane landed with wind speeds and caused severe damage.14 -9.315 -130.406 In Louisiana the hurricane landed with wind speeds of about 120 miles per hour.12 -9.372 -112.459 In Louisiana the hurricane landed with wind speeds and caused severe damage.12 -9.680 -116.158 The hurricane landed with wind speeds of about 120 miles per hour.10 -9.821 -98.210 The hurricane landed with wind speeds and caused severe damage.13 -9.986 -129.824 The hurricane landed and caused damage in small centres such as Morgan City.13 -10.023 -130.299 In Louisiana hurricane landed with wind speeds of about 120 miles per hour.13 -10.048 -130.620 The hurricane landed and caused severe damage in centres such as Morgan City.9 -10.053 -90.477 In Louisiana, the hurricane landed and caused severe damage.13 -10.091 -131.183 In Louisiana, hurricane landed with wind speeds of about 120 miles per hour.13 -10.104 -131.356 In Louisiana, the hurricane landed and caused severe damage in small coastal centres.9 -10.213 -91.915 In Louisiana the hurricane landed and caused severe damage.11 -10.214 -112.351 In Louisiana hurricane landed with wind speeds and caused severe damage.Figure 1.
Top 15 compressions ranked by their adjusted log-probability for sentence ?In Louisi-ana, the hurricane landed with wind speeds of about 120 miles per hour and caused severe damagein small coastal centres such as Morgan City, Franklin and New Iberia.
?address the redundancy problem, we use a sim-plified version of CMU?s MMR (Goldstein etal., 1999) algorithm.
A sentence is added tothe summary if and only if its content has lessthan X percent overlap with the summary.?
Content Presentation ?
To ensure coherence ofthe summary, NeATS pairs each sentence withan introduction sentence.
It then outputs the fi-nal sentences in their chronological order.We ran NeATS to generate summaries of differentsizes that were used as our test bed.
The topic sig-natures created in the process were used to rerankcompressions.
We describe the automatic evalua-tion metric used in our experiments in the next sec-tion.4 Unigram Co-Occurrence MetricIn a recent study (Lin and Hovy, 2003a), weshowed that the recall-based unigram co-occurrence automatic scoring metric correlateshighly with human evaluation and has high recalland precision in predicting the statistical signifi-cance of results comparing with its human counter-part.
The idea is to measure the content similaritybetween a system extract and a manual summaryusing simple n-gram overlap.
A similar idea calledIBM BLEU score has proved successful in auto-matic machine translation evaluation (NIST, 2002;Papineni et al, 2001).
For summarization, we canexpress the degree of content overlap in terms of n-gram matches as the following equation:)1()()(}{}{?
??
??
???
???
?=UnitsModelC CgramnUnitsModelC Cgramnmatchn gramnCountgramnCountCModel units are segments of manual summaries.They are typically either sentences or elementarydiscourse units as defined by Marcu (1999).
Count-match(n-gram) is the maximum number of n-gramsco-occurring in a system extract and a model unit.Count(n-gram) is the number of n-grams in themodel unit.
Notice that the average n-gram cover-age score, Cn, as shown in equation 1, is a recall-based metric, since the denominator of equation 1[0.40-0.50)18,284[0.50-0.60)115,240[0.70-0.80)212,11601000020000300004000050000600007000080000900001000001100001200001300001400001500001600001700001800001900002000002100002200000.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00Unigram Co-occurrence Scores#ofInstances100 ?
5 Words150 ?
5 Words200 ?
5 Words200 Words1G Avg: 0.63CMP Ratio: 0.52150 Words1G Avg: 0.55CMP Ratio: 0.64100 Words1G Avg: 0.42CMP Ratio: 0.76Figure 2.
AP900424-0035 100, 150, and 200 words oracle extract instance distributions.is the sum total of the number of n-grams occurringin the model summary instead of the system sum-mary and only one model summary is used for eachevaluation.
In summary, the unigram co-occurrencestatistics we use in the following sections are basedon the following formula:)2(logexp),( ???????
?= ?=jinnn CwjiNgramWhere j ?
i, i and j range from 1 to 4, and wn is1/(j-i+1).
Ngram(1, 4) is a weighted variable lengthn-gram match score similar to the IBM BLEUscore; while Ngram(k, k), i.e.
i = j = k, is simply theaverage k-gram co-occurrence score Ck.
In thisstudy, we set i = j = 1, i.e.
unigram co-occurrencescore.With an automatic scoring metric defined, we de-scribe the experimental setup in the next section.5 Experimental DesignsAs stated in the introduction, we aim to investigatethe effectiveness of sentence compression on over-all system performance.
If we can have a losslesscompression function that compresses a given sen-tence to a minimal length and still retains the mostimportant content of the sentence then we would beable to pack more information content into a fixedsize summary.
Figure 2 illustrates this effect<multi size="225" docset="d19d" org-size="227" comp-size="227">Lawmakers clashed on 06/23/1988 over the question of counting illegal aliens in the 1990 Census, debating whether followingthe letter of the Constitution results in a system that is unfair to citizens.
The forum was a Census subcommittee hearing on billswhich would require the Census Bureau to figure out whether people are in the country legally and, if not, to delete them from thecounts used in reapportioning seats in the House of Representatives.
Simply put, the question was who should be counted as aperson and who, if anybody, should not.
The point at issue in Senate debate on a new immigration bill was whether illegal aliensshould be counted in the process that will reallocate House seats among states after the 1990 census.
The national head count willbe taken April 1, 1990.
In a blow to California and other states with large immigrant populations, the Senate voted on 09/29/1989to bar the Census Bureau from counting illegal aliens in the 1990 population count.
At stake are the number of seats in Congressfor California, Florida, New York, Illinois, Pennsylvania and other states that will be reapportioned on the basis of next year'scensus.
Federal aid to states also is frequently based on population counts, so millions of dollars in grants and other funds madeavailable on a per capita basis would be affected.</multi>Figure 3.
227-word summary for topic D19 (?Aliens?
).<multi size="225" docset="d19d" org-size="227" comp-size="98">Lawmakers clashed over question of counting illegal aliens Census debating whether results.
Forum was a Census hearing, todelete them from the counts.
Simply put question was who should be counted and who, if anybody, should not.
Point at issue indebate on an immigration bill was whether illegal aliens should be counted.
National count will be taken April 1, 1990.
Senatevoted to bar Census Bureau from counting illegal aliens.
At stake are number of seats for California New York.
Aid to states isfrequently based on population counts, so millions would be affected.</multi>Figure 4.
Compressed summary for topic D19 ("Aliens"), 98 words.<DOC><TEXT><S SNTNO="1">Elizabeth Taylor battled pneumonia at her hospital, assisted by a ventilator, doctors say.</S><S SNTNO="2">Hospital officials described her condition late Monday as stabilizing after a lung biopsy to determine the causeof the pneumonia.</S><S SNTNO="3">Analysis of the tissue sample was expected to be complete by Thursday.</S><S SNTNO="4">Ms.
Sam, spokeswoman said "it is serious, but they are really pleased with her progress.</S><S SNTNO="5">She's not well.</S><S SNTNO="6">She's not on her deathbed or anything.</S><S SNTNO="7">Another spokeswoman, Lisa Del Favaro, said Miss Taylor's family was at her bedside.</S><S SNTNO="8">During a nearly fatal bout with pneumonia in 1961, Miss Taylor underwent a tracheotomy to help herbreathe.</S></TEXT>Figure 5.
A manual summary for document AP900424-0035.graphically.
For document AP900424-0035, whichconsists of 23 sentences or 417 words, we generatethe full permutation set of sentence extracts, i.e., allpossible 100?5, 150?5, and 200?5 words extracts.The 100?5 words extract at average compressionratio of 0.76 has most of its unigram co-occurrencescore instances (18,284/61,762 ?
30%) fallingwithin the interval between 0.40 and 0.50, i.e., theexpected performance of an extraction-based sys-tem would be between 0.40 and 0.50.
The 150?5words extract at lower compression ratio of 0.64has most of its instances between 0.50 and 0.60(115,240/377,933 ?
30%) and the 200?5 wordsextract at compression ratio of 0.52 has most of itsinstances between 0.70 and 0.80 (212,116/731,819?
29%).
If we can compress 150 or 200-wordsummaries into 100 words and retain their impor-tant content, we would be to achieve an average30% to 50% increase in performance.The question is: can an off-the-shelf sentencecompression algorithm such as K&M?s noisy-channel model achieve this?
If the answer is yes,then how much performance gain can be achieved?If not, are there other ways to use sentencecompression to improve system performance?
Toimprove system performance?
To answer thesequestions, we conduct the following experimentsover 30 DUC 2001 topic sets:(1) Run NeATS through the 30 DUC 2001topic sets and generate summaries ofsize: 100, 120, 125, 130, 140, 150, 160,175, 200, 225, 250, 275, 300, 325, 350,375, and 400.
(2) Run K&M?s sentence compression algo-rithm over all summary sentences (runKM).
For each summary sentence, wehave a set of candidate compressions.See Figure 1 for example.
(3) Rerank each candidate compression setusing different scoring methods:a. Rerank each candidate compressionset using topic signatures (run SIG).b.
Rerank each candidate compressionset using combination of KM andSIG scores using linear interpola-tion of topic signature score (SIG)and K&M?s log-probability score(KM).
We use the following for-mula in this experiment:Avg Var Std AvgCR VarCR StdCRKM 0.227 0.005 0.068 0.412 0.016 0.125ORACLE 0.287 0.006 0.078 0.471 0.009 0.092ORG 0.253 0.006 0.075 0.000 0.000 0.000SIG 0.244 0.006 0.078 0.537 0.007 0.085SIGKMa 0.242 0.006 0.077 0.370 0.015 0.123SIGKMb 0.248 0.006 0.079 0.372 0.014 0.119Table 1.
Result table for six runs.
Avg: mean unigram co-occurrence scores of30 topics, Var: variance, Std: standard deviation, AvgCR: mean compressionratio, VarCR: variance of compression ratio, and StdCR: standard deviation ofcompression ratio.KM ORACLE ORG SIG SIGKMa SIGKMbKM - -17.123 -7.681 -4.975 -4.474 -6.199ORACLE - 9.237 11.39 11.98 10.181ORG - 2.411 2.949 1.208SIG - 0.508 -1.168SIGKMa - -1.682SIGKMb -Sentence Compression Z-Test (30 instances) Pairwise Observed Z-Score 95% (Size: 100)Table 2.
Pairwise Z-test for six runs shown in Table 1 (?
= 5%).
Light gray(green) indicates runs on the column that are significantly better than runs onthe row; dark gray indicates significantly worse.SIGKM=?
?SIG + (1-?)?KM?
is set to 2/3 (run SIGKMa).c.
Rerank each candidate compressionset using SIG score first and thenKM is used to break ties (runSIGKMb).d.
Rerank each candidate compressionset using unigram co-occurrencescore against manual references.This gives the upper bound for theK&M?s algorithm applied to theoutput generated by NeATS (runORACLE).
(4) Select the best compression combination.For a given length constraint, for exam-ple 100 words, we produce the final re-sult by selecting a compressed summaryacross different summary sizes for eachtopic that fits the length limit (<= 100?5words), and output them as the finalsummary.
For example, we found that a227-word summary for topic D19 couldbe compressed to 98 words using thetopic signature reranking method.
Thecompressed summary would then be se-lected as the final summary for topicD19.
Figure 3 shows the original 227-word summary and Figure 4 shows itscompressed version.There were 30 test topics in DUC 2001 andeach topic contained about 10 documents.
Foreach topic, four summaries of approximately 50,100, 200, and 400 words were created manuallyas the ?ideal?
model summaries.
We used the setof 100-word manual summaries as our refer-ences in our experiments.
An example manualsummary is shown in Figure 5.
We report re-sults of these experiments in the next section.6 ResultsTables 1 and 2 summarize the results.
Analyzing allruns according to these two tables, we made thefollowing observations.
(1) Selecting compressed sentences usinglength-adjusted scores (K&M) without anymodification performed significantly worse(at ?
= 5%, table cells marked in dark grayin Table 2) than all other runs.
This indi-cates we cannot rely on pure syntactic-based compression to improve overall sys-tem performance although the compressionalgorithm performed well in the individualsentence level.
(2) The original run (ORG) achieved an aver-age unigram co-occurrence score of 0.253and was significantly better than all otherruns except the ORACLE and SIGKMbruns.
This result was a little bit discourag-ing; it means that no/most reranking is notuseful, and indicates that we need to investmore time in finding a better way to rankthe compressed sentences.
Pure syntactic(noisy-channel model), shallow semantic(by topic signatures), or simple combina-tions of them did not improve system per-formance and in some cases even degradedit.
(3) Comparing the ORACLE (0.287) run withthe average human performance of 0.270(not shown in the Tables), we should re-main optimistic about finding a better rank-ing algorithm to select the bestcompression.
However, the low humanperformance posts a challenge for machinelearning algorithms to learn this function.We provided more in-depth discussion ofthis issue in other papers (Lin and Hovy,2002; Lin and Hovy 2003b).
(4) That the ORACLE run did not achievehigher score also implied the following:a.
The sentence compression algo-rithm that we used might dropsome important content.
Thereforethe compressed summaries did notachieve 20% increase in perform-ance as Figure 1 might suggestwhen systems were allowed tooutput 100% longer  summary thanthe given constraint (i.e.
if a 100-word summary is requested, a sys-tem can provide a 200-word sum-mary in response.)b.
The way we generated our com-pressed summaries was not effec-tive.
We might need to optimizeand select compressions accordingto a global optimization function.For example, if some importantcontent is mentioned in sentencesalready included in a summary, wewould want to take this into ac-count and to add compressionswith new information to the finalsummary.7 ConclusionsIn this paper we presented an empirical study of theeffectiveness of applying sentence compression toimprove summarization performance.
We used agood sentence compression algorithm, comparedthe performance of five different ranking algo-rithms, and found that pure a-sentence-at-a-timesyntactic or shallow semantic-based reranking wasnot enough to boost system performance.
However,the significant difference between the ORACLErun and the original run (ORG) indicated there ispotential in sentence compression but we need tofind a better compression selection function thatshould take into account global cross-sentence op-timization.
This indicated local optimization at thesentence level such as Knight and Marcu?s (2000)noisy-channel model is not enough when our goalis to find the best compressed summaries not thebest compressed sentences.
In the future, wewould like to apply a similar methodology to dif-ferent text units, for example, sub-sentence unitssuch as elementary discourse unit (Marcu, 1999)and a larger corpus, for example, DUC 2002 andDUC 2003.
We want to explore compression tech-niques to go beyond simple sentence extraction.ReferencesO.
Buyukkokten, H. Garcia-Molina, A. Paepcke.2001.
Seeing the Whole in Parts: Text Summa-rization for Web Browsing on Handheld De-vices.
The 10th International WWWConference (WWW10).
Hong Kong, China.J.
Carroll, G. Minnen, Y. Canning, S. Devlin, andJ.
Tait.
1998.
Practical Simplification of Eng-lish Newspaper Text to Assist Aphasic Read-ers.
In Proceedings of AAAI-98 Workshop onIntegrating Artificial Intelligence and AssistiveTechnology, Madison, WI, USA.H.H.
Chen, J.J. Kuo, and T.C.
Su 2003.
Clusteringand Visualization in a Multi-Lingual Multi-Document Summarization System.
In Proceed-ings of 25th European Conference on Informa-tion Retrieval Research, Lecture Note inComputer Science, April 14-16, Pisa, Italy.S.
Corston-Oliver.
2001.
Text Compaction for Dis-play on Very Small Screens.
In Proceedings ofthe Workshop on Automatic Summarization(WAS 2001), Pittsburgh, PA, USA.DUC.
2002.
The Document Understanding Confer-ence.
http://duc.nist.gov.T.
Dunning.
1993.
Accurate Methods for the Statis-tics of Surprise and Coincidence.
Computa-tional Linguistics 19, 61?74.H.P.
Edmundson.
1969.
New Methods in Auto-matic Abstracting.
Journal of the Associationfor Computing Machinery.
16(2).T.
Fukusima and M. Okumura.
2001.
Text Summa-rization Challenge Text SummarizationEvaluation in Japan.
In Proceedings of theWorkshop on Automatic Summarization (WAS2001), Pittsburgh, PA, USA.J.
Goldstein, M. Kantrowitz, V. Mittal, and J. Car-bonell.
1999.
Summarizing Text Documents:Sentence Selection and Evaluation Metrics.
InProceedings of the 22nd International ACMConference on Research and Development inInformation Retrieval (SIGIR-99), Berkeley,CA, USA, 121?128.G.
Grefenstette.
1998.
Producing Intelligent Tele-graphic Text Reduction to Provide an AudioScanning Service for the Blind.
In WorkingNotes of the AAAI Spring Symposium on In-telligent Text Summarization, Stanford Univer-sity, CA, USA, 111?118.E.
Hovy and C.-Y.
Lin.
1999.
Automatic TextSummarization in SUMMARIST.
In I. Maniand M. Maybury (eds), Advances in AutomaticText Summarization, 81?94.
MIT Press.H.
Jing.
2000.
Sentence simplification in automatictext summarization.
In the Proceedings of the6th Applied Natural Language Processing Con-ference (ANLP'00).
Seattle, Washington, USA.K.
Knight and D. Marcu.
2000.
Statistics-BasedSummarization ?
Step One: Sentence Com-pression.
In Proceedings of AAAI-2000, Aus-tin, TX, USA.J.
Kupiec, J. Pederson, and F. Chen.
1995.
ATrainable Document Summarizer.
In Proceed-ings of the 18th International ACM Conferenceon Research and Development in InformationRetrieval (SIGIR-95), Seattle, WA, USA, 68?73.C.-Y.
Lin and E. Hovy.
2000.
The Automated Ac-quisition of Topic Signatures for Text Summa-rization.
In Proceedings of the 18thInternational Conference on ComputationalLinguistics (COLING 2000), Saarbr?cken,Germany.C.-Y.
Lin and E. Hovy.
2002.
From Single toMulti-document Summarization: A PrototypeSystem and its Evaluation.
In Proceedings ofthe 40th Anniversary Meeting of the Associa-tion for Computational Linguistics (ACL-2002), Philadelphia, PA, U.S.A.C.-Y.
Lin and E. Hovy.
2002.
Manual and Auto-matic Evaluations of Summaries.
InProceedings of the Workshop on AutomaticSummarization, post-conference workshop ofACL-2002, pp.
45-51, Philadelphia, PA, USA.C.-Y.
Lin and E. Hovy.
2003a.
Automatic Evalua-tion of Summaries Using N-gram Co-occurrence Statistics.
In Proceedings of the2003 Human Language Technology Confer-ence (HLT-NAACL 2003), Edmonton, Can-ada.C.-Y.
Lin and E. Hovy 2003b.
The Potential andLimitations of Sentence Extraction for Summa-rization.
In Proceedings of the Workshop onAutomatic Summarization post-conferenceworkshop of HLT-NAACL-2003, Edmonton,Canada.H.P.
Luhn.
1969.
The Automatic Creation of Lit-erature Abstracts.
IBM Journal of Research andDevelopment.
2(2).D.
Marcu.
1999.
Discourse trees are good indica-tors of importance in text.
In I. Mani and M.Maybury (eds), Advances in Automatic TextSummarization, 123?136.
MIT Press.K.
McKeown, Barzilay, D. Evans, V. Hatzivassi-loglou, J. L. Klavans, A. Nenkova, C. Sable, B.Schiffman, S. Sigelman.
2002.
Tracking andSummarizing News on a Daily Basis with Co-lumbia?s Newsblaster.
In Proceedings of Hu-man Language Technology Conference 2002(HLT 2002).
San Diego, CA, USA.NIST.
2002.
Automatic Evaluation of MachineTranslation Quality using N-gram Co-Occurrence Statistics.P.
Over and W. Liggett.
2002.
Introduction toDUC-2002: an Intrinsic Evaluation of GenericNews Text Summarization Systems.
In Pro-ceedings of Workshop on Automatic Summari-zation (DUC 2002), Philadelphia, PA, USA.http://www-nlpir.nist.gov/projects/duc/pubs/2002slides/overview.02.pdfK.
Papineni, S. Roukos, T. Ward, W.-J.
Zhu.
2001.Bleu: a Method for Automatic Evaluation ofMachine Translation.
IBM Research ReportRC22176 (W0109-022).D.R.
Radev and K.R.
McKeown.
1998.
GeneratingNatural Language Summaries from MultipleOn-line Sources.
Computational Linguistics,24(3):469?500.T.
Strzalkowski, G. Stein, J. Wang, and B, Wise.
ARobust Practical Text Summarizer.
1999.
In I.Mani and M. Maybury (eds), Advances inAutomatic Text Summarization, 137?154.
MITPress.M.
White, T. Korelsky, C. Cardie, V. Ng, D.Pierce, and K. Wagstaff.
2001.
MultidocumentSummarization via Information Extraction.
InProceedings of Human Language TechnologyConference 2001 (HLT 2001), San Diego, CA,USA.
