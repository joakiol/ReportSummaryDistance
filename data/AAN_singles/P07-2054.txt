Proceedings of the ACL 2007 Demo and Poster Sessions, pages 213?216,Prague, June 2007. c?2007 Association for Computational LinguisticsExtending MARIE: an N -gram-based SMT decoderJosep M. CregoTALP Research CenterUniversitat Polite`cnica de CatalunyaBarcelona, 08034jmcrego@gps.tsc.upc.eduJose?
B. Marin?oTALP Research CenterUniversitat Polite`cnica de CatalunyaBarcelona,08034canton@gps.tsc.upc.eduAbstractIn this paper we present several extensions ofMARIE1, a freely available N -gram-based sta-tistical machine translation (SMT) decoder.
Theextensions mainly consist of the ability to ac-cept and generate word graphs and the intro-duction of two new N -gram models in the log-linear combination of feature functions the de-coder implements.
Additionally, the decoder isenhanced with a caching strategy that reducesthe number of N -gram calls improving the over-all search efficiency.
Experiments are carried outover the Eurpoean Parliament Spanish-Englishtranslation task.1 IntroductionResearch on SMT has been strongly boosted in the lastfew years, partially thanks to the relatively easy develop-ment of systems with enough competence as to achieverather competitive results.
In parallel, tools and tech-niques have grown in complexity, which makes it diffi-cult to carry out state-of-the-art research without sharingsome of this toolkits.
Without aiming at being exhaus-tive, GIZA++2, SRILM3 and PHARAOH4 are probablythe best known examples.We introduce the recent extensions made to an N -gram-based SMT decoder (Crego et al, 2005), which al-lowed us to tackle several translation issues (such as re-ordering, rescoring, modeling, etc.)
successfully improv-ing accuracy, as well as efficiency results.As far as SMT can be seen as a double-sided prob-lem (modeling and search), the decoder emerges as a keycomponent, core module of any SMT system.
Mainly,1http://gps-tsc.upc.es/soft/soft/marie2http://www.fjoch.com/GIZA++.html3http://www.speech.sri.com/projects/srilm/4http://www.isi.edu/publications/licensed-sw/pharaoh/any technique aiming at dealing with a translation prob-lem needs for a decoder extension to be implemented.Particularly, the reordering problem can be more effi-ciently (and accurate) addressed when tightly coupledwith decoding.
In general, the competence of a decoderto make use of the maximum of information in the globalsearch is directly connected with the likeliness of suc-cessfully improving translations.The paper is organized as follows.
In Section 2 weand briefly review the previous work on decoding withspecial attention to N -gram-based decoding.
Section 3describes the extended log-linear combination of featurefunctions after introduced the two new models.
Section4 details the particularities of the input and output wordgraph extensions.
Experiments are reported on section 5.Finally, conclusions are drawn in section 6.2 Related WorkThe decoding problem in SMT is expressed by the nextmaximization: argmaxtI1??
P (tI1|sJ1 ), where sJ1 is thesource sentence to translate and tI1 is a possible transla-tion of the set ?
, which contains all the sentences of thelanguage of tI1.Given that the full search over the whole set of tar-get language sentences is impracticable (?
is an infiniteset), the translation sentence is usually built incremen-tally, composing partial translations of the source sen-tence, which are selected out of a limited number of trans-lation candidates (translation units).The first SMT decoders were word-based.
Hence,working with translation candidates of single sourcewords.
Later appeared the phrase-based decoders,which use translation candidates composed of sequencesof source and target words (outperforming the word-based decoders by introducing the word context).
In thelast few years syntax-based decoders have emerged aim-ing at dealing with pair of languages with different syn-tactical structures for which the word context introduced213Figure 1: Generative process.
Phrase-based (left) and N -gram-based (right) approaches.in phrase-based decoders is not sufficient to cope withlong reorderings.Like standard phrase-based decoders, MARIE em-ploys translation units composed of sequences of sourceand target words.
In contrast, the translation con-text is differently taken into account.
Whereas phrase-based decoders employ translation units uncontextual-ized, MARIE takes the translation unit context into ac-count by estimating the translation model as a standardN -gram language model (N -gram-based decoder).Figure 1 shows that both approaches follow the samegenerative process, but they differ on the structure oftranslation units.
In the example, the units ?s1#t1?
and?s2 s3#t2 t3?
of the N -gram-based approach are usedconsidering that both appear sequentially.
This fact canbe understood as using a longer unit that includes both(longer units are drawn in grey).MARIE follows the maximum entropy framework,where we can define a translation hypothesis t given asource sentence s, as the target sentence maximizing alog-linear combination of feature functions:t?I1 = arg maxtI1{ M?m=1?mhm(sJ1 , tI1)}(1)where ?m corresponds to the weighting coefficients ofthe log-linear combination, and the feature functionshm(s, t) to a logarithmic scaling of the probabilities ofeach model.
See (Marin?o et al, 2006) for further detailson the N -gram-based approach to SMT.3 N-gram Feature FunctionsTwo language models (LM) are introduced in equation 1,aiming at helping the decoder to find the right transla-tions.
Both are estimated as standard N -gram LM.3.1 Target-side N -gram LMThe first additional N -gram LM is destinated to be ap-plied over the target sentence (tagged) words.
Hence,as the original target LM (computed over raw words),it is also used to score the fluency of target sentences,but aiming at achieving generalization power through us-ing a more generalized language (such as a language ofPart-of-Speech tags) instead of the one composed of rawwords.
Part-Of-Speech tags have successfully been usedin several previous experiments.
however, any other tagcan be applied.Several sequences of target tags may apply to any giventranslation unit (which are passed to the decoder before itstarts the search).
For instance, regarding a translationunit with the english word ?general?
in its target side, ifPOS tags were used as target tagged tags, there would ex-ist at least two different tag options: noun and adjective.In the search, multiple hypotheses are generated con-cerning different target tagged sides (sequences of tags)of a single translation unit.
Therefore, on the one side, theoverall search is extended towards seeking the sequenceof target tags that better fits the sequence of target rawwords.
On the other side, this extension is hurting theoverall efficiency of the decoder as additional hypothesesappear in the search stacks while not additional transla-tion hypotheses are being tested (only differently tagged).This extended feature may be used toghether with alimitation of the number of target tagged hypotheses pertranslation unit.
The use of a limited number of thesehypotheses implies a balance between accuracy and effi-ciency.3.2 Source-side N -gram LMThe second N -gram LM is applied over the input sen-tence tagged words.
Obviously, this model only makessense when reordering is applied over the source wordsin order to monotonize the source and target word order.In such a case, the tagged LM is learnt over the trainingset with reordered source words.Hence, the new model is employed as a reorderingmodel.
It scores a given source-side reordering hypoth-esis according to the reorderings made in the trainingsentences (from which the tagged LM is estimated).
Asfor the previous extension, source tagged words are usedinstead of raw words in order to achieve generalizationpower.Additional hypotheses regarding the same translationunit are not generated in the search as all input sentencesare uniquely tagged.Figure 2 illustrates the use of a source POS-tagged N -214gram LM.
The probability of the sequence ?PRN VRBNAME ADJ?
is greater than the probability of the se-quence ?PRN VRB ADJ NAME?
for a model estimatedover the training set with reordered source words (withenglish words following the spanish word order).Figure 2: Source POS-tagged N -gram LM.3.3 Caching N -gramsThe use of several N -gram LM?s implies a reduction inefficiency in contrast to other models that can be imple-mented by means of a single lookup table (one access perprobability call).
The special characteristics of NgramLM?s introduce additional memory access to account forbackoff probabilities and lower Ngrams fallings.Many N -gram calls are requested repeatedly, produc-ing multiple calls of an entry.
A simple strategy to reduceadditional access consists of keeping a record (cache) forthose Ngram entries already requested.
A drawback forthe use of a cache consists of the additional memory ac-cess derived of the cache maintenance (adding new andchecking for existing entries).Figure 3: Memory access derived of an N -gram call.Figure 3 illustrates this situation.
The call for a 3-gramprobability (requesting for the probability of the sequenceof tokens ?a b c?)
may need for up to 6 memory access,while under a phrase-based translation model the finalprobability would always be reached after the first mem-ory access.
The additional access in the N -gram-basedapproach are used to provide lower N -gram and backoffprobabilities in those cases that upper N -gram probabili-ties do not exist.4 Word GraphsWord graphs are successfully used in SMT for several ap-plications.
Basically, with the objective of reducing theredundancy of N -best lists, which very often convey se-rious combinatorial explosion problems.A word graph is here described as a directed acyclicgraph G = (V,E) with one root node n0 ?
V .Edges are labeled with tokens (words or translation units)and optionally with accumulated scores.
We will use(ns(ne ??t??
s)), to denote an edge starting at node ns andending at node ne, with token t and score s. The fileformat of word graphs coincides with the graph file for-mat recognized by the CARMEL5 finite state automatatoolkit.4.1 Input GraphWe can mainly find two applications for which wordgraphs are used as input of an SMT system: the recog-nition output of an automatic speech recognition (ASR)system; and a reordering graph, consisting of a subset ofthe whole word permutations of a given input sentence.In our case we are using the input graph as a reorder-ing graph.
The decoder introduces reordering (distortionof source words order) by allowing only for the distor-tion encoded in the input graph.
Though, the graph isonly allowed to encode permutations of the input words.In other words, any path in the graph must start at noden0, finish at node nN (where nN is a unique ending node)and cover all the input words (tokens t) in whatever order,without repetitions.An additional feature function (distortion model) is in-troduced in the log-linear combination of equation 1:pdistortion(uk) ?kI?i=k1p(ni|ni?1) (2)where uk refers to the kth partial translation unit coveringthe source positions [k1, ..., kI ].
p(ni|ni?1) correspondsto the edge score s encoded in the edge (ns(ne ??t??
s)),where ni = ne and ni?1 = ns.One of the decoding first steps consists of building(for each input sentence) the set of translation units to beused in the search.
When the search is extended with re-ordering abilities the set must be also extended with thosetranslation units that cover any sequence of input wordsfollowing any of the word orders encoded in the inputgraph.
The extension of the units set is specially relevantwhen translation units are built from the tranining set withreordered source words.Given the example of figure 2, if the translation unit?translations perfect # traducciones perfectas?
is avail-able, the decoder should not discard it, as it providesa right translation.
Notwithstanding that its source sidedoes not follow the original word order of the input sen-tence.4.2 Output GraphThe goal of using an output graph is to allow for furtherrescoring work.
That is, to work with alternative transla-5http://www.isi.edu/licensed-sw/carmel/215tions to the single 1-best.
Therefore, our proposed outputgraph has some peculiarities that make it different to thepreviously sketched intput graph.The structure of edges remains the same, but obvi-ously, paths are not forced to consist of permutations ofthe same tokens (as far as we are interested into multipletranslation hypotheses), and there may also exist pathswhich do not reach the ending node nN .
These latterpaths are not useful in rescoring tasks, but allowed in or-der to facilitate the study of the search graph.
However,a very easy and efficient algorithm (O(n), being n thesearch size) can be used in order to discard them, beforerescoring work.
Additionally, given that partial modelcosts are needed in rescoring work, our decoder allowsto output the individual model costs computed for eachtranslation unit (token t).
Costs are encoded within thetoken s, as in the next example:(0 (1 "o#or{1.5,0.9,0.6,0.2}" 6))where the token t is now composed of the translation unit?o#or?, followed by (four) model costs.Multiple translation hypotheses can only be extractedif hypotheses recombinations are carefully saved.
As in(Koehn, 2004), the decoder takes a record of any recom-bined hypothesis, allowing for a rigorous N -best genera-tion.
Model costs are referred to the current unit while theglobal score s is accumulated.
Notice also that translationunits (not words) are now used as tokens.5 ExperimentsExperiments are carried out for a Spanish-to-Englishtranslation task using the EPPS data set, correspondingto session transcriptions of the European Parliament.Eff.
base +tpos +reor +sposBeam size = 50w/o cache 1, 820 2, 170 2, 970 3, 260w/ cache ?50 ?110 ?190 ?210Beam size = 100w/o cache 2, 900 4, 350 5, 960 6, 520w/ cache ?175 ?410 ?625 ?640Table 1: Translation efficiency results.Table 1 shows translation efficiency results (mea-sured in seconds) given two different beam search sizes.w/cache and w/o cache indicate whether the decoder em-ploys (or not) the cache technique (section 3.3).
Sev-eral system configuration have been tested: a baselinemonotonous system using a 4-gram translation LM anda 5-gram target LM (base), extended with a target POS-tagged 5-gram LM (+tpos), further extended by allow-ing for reordering (+reor), and finally using a source-sidePOS-tagged 5-gram LM (+spos).As it can be seen, the cache technique improves the ef-ficiency of the search in terms of decoding time.
Timeresults are further decreased (reduced time is shown forthe w/ cache setting) by using more N -gram LM and al-lowing for a larger search graph (increasing the beam sizeand introducing distortion).Further details on the previous experiment can beseen in (Crego and Marin?o, 2006b; Crego and Marin?o,2006a), where additionally, the input word graph and ex-tended N -gram tagged LM?s are successfully used to im-prove accuracy at a very low computational cost.Several publications can also be found in bibliographywhich show the use of output graphs in rescoring tasksallowing for clear accuracy improvements.6 ConclusionsWe have presented several extensions to MARIE, a freelyavailable N -gram-based decoder.
The extensions consistof accepting and generating word graphs, and introducingtwo N -gram LM?s over source and target tagged words.Additionally, a caching technique is applied over the N -gram LM?s.AcknowledgmentsThis work has been funded by the European Union un-der the integrated project TC-STAR - (IST-2002-FP6-5067-38), the Spanish Government under the projectAVIVAVOZ - (TEC2006-13694-C03) and the UniversitatPolite`cnica de Catalunya under UPC-RECERCA grant.ReferencesJ.M.
Crego and J.B. Marin?o.
2006a.
Integration ofpostag-based source reordering into smt decoding byan extended search graph.
Proc.
of the 7th Conf.
of theAssociation for Machine Translation in the Americas,pages 29?36, August.J.M.
Crego and J.B. Marin?o.
2006b.
Reordering experi-ments for n-gram-based smt.
1st IEEE/ACL Workshopon Spoken Language Technology, December.J.M.
Crego, J.B. Marin?o, and A. de Gispert.
2005.
Anngram-based statistical machine translation decoder.Proc.
of the 9th European Conference on Speech Com-munication and Technology, Interspeech?05, pages3193?3196, September.Ph.
Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
Proc.
of the 6th Conf.
of the Association for Ma-chine Translation in the Americas, pages 115?124, Oc-tober.J.B.
Marin?o, R.E.
Banchs, J.M.
Crego, A. de Gispert,P.
Lambert, J.A.R.
Fonollosa, and M.R.
Costa-jussa`.2006.
N-gram based machine translation.
Computa-tional Linguistics, 32(4):527?549.216
