Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1246?1256,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsSemi-supervised CCG Lexicon ExtensionEmily ThomfordeUniversity of Edinburghe.j.thomforde@sms.ed.ac.ukMark SteedmanUniversity of Edinburghsteedman@inf.ed.ac.ukAbstractThis paper introduces Chart Inference (CI),an algorithm for deriving a CCG categoryfor an unknown word from a partial parsechart.
It is shown to be faster and more pre-cise than a baseline brute-force method, andto achieve wider coverage than a rule-basedsystem.
In addition, we show the applicationof CI to a domain adaptation task for ques-tion words, which are largely missing in thePenn Treebank.
When used in combinationwith self-training, CI increases the precisionof the baseline StatCCG parser over subject-extraction questions by 50%.
An error analy-sis shows that CI contributes to the increase byexpanding the number of category types avail-able to the parser, while self-training adjuststhe counts.1 IntroductionUnseen lexical items are a major cause of error instrongly lexicalised parsers such as those based onCCG (Clark and Curran, 2003; Hockenmaier, 2003).The problem is especially acute for less privilegedlanguages, but even in the case of English, we areaware of many category types entirely missing fromthe Penn Treebank (Clark et al, 2004).In the case of totally unseen words, the standardmethod used by StatCCG (Hockenmaier, 2003) andmany other treebank parsers is part-of-speech back-off, which is quite effective, affording an F-score of93% over dependencies in ?00 in the optimal config-uration.
It is difficult to say how backing off affectsdependency errors, but when we examine categorymatch accuracy of the CCGBank-trained parser, wefind that POS backoff has been used on 19.6% of to-kens, which means that those tokens are unseen, ortoo infrequent in the training data to be included inthe lexicon.
Of the 3320 items the parser labelledincorrectly, 675 (20.3%) are words that are miss-ing from the lexicon entirely.1 In the best case, ifwe were able to learn lexical entries for those 675,we could transfer them to lexical treatment, whichis 93.5% accurate, rather than POS backoff, whichis 89.3% accurate.
Under these conditions, we pre-dict a further 631 word/category pairs to be taggedcorrectly by the parser, reducing the error rate from7.4% to 6% on ?00.
Further to reducing parsing er-ror, a robust method for learning words from un-labelled data would result in the recovery of inter-esting and important category types that are missingfrom our standard lexical resources.This paper introduces Chart Inference (CI) asa strategy for deducing a ranked set of possiblecategories for an unknown word using the partialchart formed from the known words that surroundit.
CCG (Steedman, 2000) is particularly suited tothis problem, because category types can be inferredfrom the types of the surrounding constituents.
CIis designed to take advantage of this property ofgenerative CCGBank-trained parser, and of accessto the full inventory of CCG combinators and non-combinatory unary rules from the trained model.
Itis capable of learning category types that are com-pletely missing from the lexicon, and is superior toexisting learning systems in both precision and effi-ciency.Four experiments are discussed in this paper.
Thefirst compares three word-learning methods for theirability to converge to a toy target lexicon.
The sec-1A further 269 (8%) are cases where the word is known, buthas not been seen with the correct category.1246ond and third compare the three methods based ontheir ability to correctly tag the all the words in asmall natural language corpus.
The final experimentshows how Chart Induction can be effectively usedin a domain adaptation task where a small numberof category types are known to be missing from thelexicon.2 Learning WordsThe methods used in this paper all operate undera restricted learning setting, over sentences whereall but one word is in the lexicon.
Since the learn-ing portion of the algorithm is unsupervised, it hasaccess to an essentially unlimited amount of unla-belled data, and it can afford to skip any sentencethat does not conform to the one-unseen-word re-striction.
Attempting two or more OOL words at atime from one sentence would compound the searchspace and the error rate.
We do not address the muchharder problem of hypothesising missing categoriesfor known words, which should presumably be han-dled by quite other methods, such as prior offlinegeneralization of the lexicon.2.1 A Brute-force SystemOne of the early lexical acquisition systems us-ing Categorial Grammar was that of Watkinson andManandhar (1999; 2000; 2001a; 2001b).
This sys-tem attempted to simultaneously learn a CG lexiconand annotate unlabelled text with parse derivations.Using a stripped-down parser that only utilised theforward- and backward-application rules, they iter-atively learned the lexicon from the feedback fromonline parsing.
The system decided which parse wasbest based on the lexicon, and then decided whichadditions to the lexicon to make based on principlesof compression.
After each change, the system re-examined the parses for previous sentences and up-dated them to reflect the new lexicon.They report fully convergent results on two toycorpora, but the parsing accuracy of the systemtrained on natural language data was far belowthe state of the art.
However, they do show cat-egorial grammar to be a promising basis for ar-tificial language acquisition, because CCG makeslearning the lexicon and learning the grammar thesame task (Watkinson and Manandhar, 1999).
Theyalso showed that seeding the lexicon with examplesof lexical items (closed-class words in their case),rather than just a list of possible category types, in-creased its chances of converging.
This approach ofautomating the learning process differs from the pre-vious language learning methods described, in thatit doesn?t require the specification of any particularpatterns, only knowledge of the grammar formalism.For this paper, as a baseline, we implementa generalised version of Watkinson and Manand-har?s mechanism for determining the category ?of a single OOL word in a sentence where therest of the words C1...CN are in the lexicon: ?
=argmaxParse(C1...Cn,?).
This is equivalent tobacking off to the set of all known category types;the learner returns the category that maximises theprobability of the completed parse tree.
We ignorethe optimisation and compression steps of the origi-nal system.2.2 A Rule-based SystemYao et al (2009a; 2009b) developed a learning sys-tem based on handwritten translation rules for de-ducing the category (X) of a single unknown wordin a sentence consisting of a sequence of partially-parsed constituents (A..N).Their system was based on a small inventory ofinference rules that eliminated ambiguity in the or-dering of arguments.
For example, one of the Level3 inference rules specifies the order of the argumentsin the deduced category:A X B C?
D?
X = ((D\A)/C)/BWithout this inductive bias the learner wouldhave to deal with the ambiguity of the options((D/C)/B)\A and ((D/C)\A)/B at minimum.
Inaddition they limited their learner to CG-compatibleparse structures and their constituent strings tolength 4.Their argument is that only this minimal bias isneeded to learn syntactic structures, including thefronting of polar interrogative auxiliaries and aux-iliary word order (should > have > been), from atraining set that did not explicitly contain full evi-dence for them.Although Yao et al (2009b) used the full set ofCCG combinators to generate learned categories,they employed a post-processing step to filter spu-rious categories by checking whether the category1247DERIVE([C1...Cn],?
,?
)if ?
= /0then return (?
)else if C1 =Cn = Xthen{?
= ?
+?
;DERIVE(X , /0,?)else??????????????????????
?if C1 /?
S,Xthen DERIVE([C2...Cn],?\C1,?
)if Cn /?
S,Xthen DERIVE([C1...Cn?1],?/Cn,?
)if ?
?
B and C1 ?
B/A and C1 /?
S,Xthen DERIVE([C2...Cn],A,?
)if ?
?
B and Cn ?
B\A and Cn /?
S,Xthen DERIVE([C1...Cn?1],A,?
)Figure 1: Generalised recursive rule-based algorithm,where [C1...Cn] is a sequence of categories, one of whichis X , ?
is a result category, and ?
is the (initially empty)category set.participated in a CG-only derivation (using applica-tion rules only).
This is effective in limiting spuri-ous derivations, but at the expense of reduced recallon those sentences for whose analysis CCG rules ofcomposition etc.
are crucial.Their rules were effective for their toy-scaledatasets, but for the purposes of this paper we haveimplemented a generalised version of the recursivealgorithm for use in wide-coverage parsing.
This al-gorithm is outlined in Figure 1.
It takes a sequenceof categorial constituents, all known except one (X),and builds a candidate set of categories (?)
for theunknown word by recursively applying Yao?s Level0 and Level 1 inference rules.2.3 Chart InferenceBoth Watkinson?s and Yao?s experiments were fullyconvergent over toy datasets, but did not scale to re-alistic corpora.
Watkinson attempted to learn fromthe LLL corpus (Kazakov et al, 1998), but attributedthe failure to the small amount of training data rela-tive to the corpus, and the naive initial category set.Yao?s method was only ever designed as a proof-of-concept to show how much of the language can belearned from partial evidence, and was not meant tobe run in earnest in a real-world learning setting.
Forone, the rules do not cover the full set of partial parseconditions, and further to that, they do not allow forpartial parses to be reanalysed within the learningframework.To that end, we have developed a learning algo-rithm that is capable of operating within the one-unknown-word-per-sentence learning setting estab-lished by the two baseline systems, that is able toinvent new category types, and that is able to takeadvantage of the full generality of CCG.
This sec-tion shows that it performs as well as the previoustwo systems on a toy corpus, and the next sectionproves that it more readily scales to natural languagedomains.Mellish (1989) established a two-stage bidirec-tional chart parser for diagnosing errors in input text.His method relied heavily on heuristic rules, andthe only evaluation he did was on number of cy-cles needed for each type of error, and number ofsolutions produced.
His method was designed foruse in producing parses where the original parserfailed, dealing with omissions, insertions, and mis-spelled/unknown words.
The only method used torank the possible solutions was heuristic scores.Kato (1994) implemented a revised system thatused a generalised top-down parser, rather than achart, and was able to get the number of cycles todecrease.In both cases the evaluation was only on a toy cor-pus, and they did not evaluate on whether the sys-tems diagnosed the errors correctly, or whether thesolution they offered was accurate.
They also hadto deal with cases where the error was ambiguous,for example, where an inserted word could be inter-preted as a misspelling or vice-versa.Where Mellish uses the two-stage parsing processto complete malformed parses, we use it to diagnoseunknown lexical items.
In addition, we work on thescale of a full grammar and wide-coverage parser,using modern lexical corpora.Our method is a wrapper for a naive generativeCCG parser StatOpenCCG (Christodoulopoulos,2008), a statistical extension to OpenCCG (Whiteand Baldridge, 2003).
In the general case, the parseris trained on all the labelled data available in a par-ticular learning setting, then the learner discoversnew lexical items from unlabelled text.
Like thebrute force and rule-based systems, it is vulnerable1248CCG Combinator Inverse CombinatorA/B B ?
A (>) X B ?
A ?
X = A/B if v(B) ?
1A/B X ?
A ?
X = BB A\B ?
A (<) X A\B ?
A ?
X = BB X ?
A ?
X = A\B if v(B) ?
1A/C C/B ?
A/B (>B) X C/B ?
A/B ?
X = A/CA/C X ?
A/B ?
X =C/BC\B A\C ?
A\B (<B) X A\C ?
A\B ?
X =C\BC\B X ?
A\B ?
X = A\CFigure 2: Derivation of inverse combinatorsP(target =C|R,S) = max{ P(HeadRight|R)P(HeadLe f t|R)}P(HeadRight|R) =??????????
?P[outside](R)?P[inside](S)?P(exp = le f t|R)?P(C|R,exp = le f t)?P(S|R,exp = le f t,C)??????????
?P(HeadLe f t|R) =??????????
?P[outside](R)?P[inside](S)?P(exp = right|R)?P(S|R,exp = right)?P(C|R,exp = right,S)??????????
?Figure 3: CI probability that the target is category C,given possible categories for result (R) and sister (S).to attachment errors and ambiguity from adverbials.The learning step consists in presenting the parserwith sentences all of whose words but one are in-lexicon.
The parser must have a statistical parsingmodel, which contains a seed lexicon, a set of CCGcombinators, and an optional set of unary and binaryrules learned from the training corpus.First the baseline bottom-up parser is called uponto produce a partial parse chart.
The learner takesthis partial chart and fills the top right cell with adistribution for the result category based on the endpunctuation.2Using this partial chart that contains at least oneentry for every leaf cell (except the one OOL tar-get cell) and at least one entry for the result, the2For simple corpora, only S is required, but realistic corporanecessitate a distribution over all result types, including nounphrases and fragments.learner steps through the chart in a top-down ver-sion of CYK (Younger, 1967).
For the top-downprocess, the standard combinators have to be refor-mulated to take an argument and a result as inputs,rather than two arguments as in the standard bottom-up case.
In addition, the learner has access to thenon-combinator rules from the parse model, whichhave been similarly inverted for top-down use.
Thisprocess continues until the target cell has been filled,and the ranked set of categories is returned.The probability that the target has a given cat-egory is calculated as the greater of the right- orleft-headed derivations, according to Figure 3.
Attraining time, the StatOpenCCG parser creates ahead-dependency model from the training corpus, inwhich we can look up the values for the expansionprobabilities.
Where a value is unavailable, it backsoff to a pre-specified value (default 0.0001).3 Thesystem requires a pruning parameter that limits eachcell to the top N most probable categories.
Here, weset N=10, to limit the search space and complexity.
4Figure 2 sets out the inventory of inverse combi-nators used in the top-down learning step.
Each stan-dard binary CCG combinator motivates two inversecombinators: one for each possible missing item.
Inthe two permissive instances where the sister cate-gory?s form is the unrestricted B, we limit the sis-ter?s valency to 1, in order to keep the learner fromgenerating spurious categories that could result fromthese two rules being overapplied.Figure 4 illustrates the workings of the learning3This backoff parameter allows adjustment of the expecta-tion of new category types and could be replaced with anothersmoothing method in subsequent implementations.4Further testing on the McGuffey corpus has shown the av-erage rank of correct tags in the category set to be 1.4.1249algorithm for the sentence The cat X her.
The greycells are filled as a partial chart by the parser, and thewhite cells are filled by the top-down learner.
Notethat taking rule probabilities into account makes thealgorithm robust to ambiguity.
The highest-rankinglexical category for her is NP[nb]/N, but the nexthighest (NP) is preferred in the derivation of thehighest-ranking category for the unknown word X.3 Experiment I: ConvergenceIn the following experiments, we compare Chart In-ference to the two baseline methods: Brute Force(BF), derived from Watkinson and Manandhar, andRule-Based (RB), derived from Yao et al This sec-tion investigates how robust the three systems are tochanges in theoriginal seed lexicon.3.1 CorpusFor this experiment we test the three systems on areconstructed version of Corpus 1 from Watkinsonand Manandhar?s experiments.5 The lexicon con-tains 40 word-category pairs, including the full stop(S\S), which was not in Watkinson?s experiment,and one example of noun-verb ambiguity (saw).
Thetest sentences are randomly generated from a simplePCFG over the lexicon, and are always presented tothe learners in the same order.3.2 MethodsIn order to directly compare the three learning meth-ods, we use the evaluation setting from Watkinsonand Manandhar (1999), which consists of a 40-entrytarget lexicon and a PCFG language model used torandomly generate 1000 sentences.
We then specifya seed lexicon and run the learner incrementally, sothat it deals with one sentence at a time, then feedsthe learned material back into the lexicon.
Watkin-son?s system was shown to fully convergent (theydefined convergence as cosine similarity betweenthe seed lexicon (~S) and the target lexicon (~T ) ex-ceeding 0.99), whenever the seed lexicon containedat least one instance of each of the category types inthe target lexicon (Watkinson and Manandhar, 1999)5The full corpus was not included in any of Watkinson?s pa-pers, but its properties were outlined to such an extent that itwas straightforward to recreate, though the reconstruction maydiffer from the original in the distribution of category types.
Thereconstructed corpus will be released shortly.0.50.55 0.60.65 0.70.75 0.80.85 0.90.95 11101001000Cosine similarity to targetSentences seenCI RB BFFigure 5: Learning curve for all three methods when theseed contains no ditransitives.
CI and RB are identical.0.55 0.60.65 0.70.75 0.80.85 0.90.95 11101001000Cosine similarity to targetSentences seenCIRBBFFigure 6: Learning curve for all three methods when seedcontains only three determiners and one noun.3.3 ResultsWhen run incrementally over this toy corpus, boththe RB and CI algorithms converge to the targetlexicon in an identical sigmoid learning curve (notshown).
However, when we start with an im-poverished seed, the algorithms?
behaviours startto diverge.
Figure 5 shows the learning curvefor the three methods when the seed lexiconomits all instances of the ditransitive category type((S\NP)/NP)/NP.
Both RB and CI converge iden-tically as expected, but BF, the lower curve, cannotlearn any category types that are not attested in theseed, so it plateaus at 95% similarity.When the seed is reduced to only three deter-miners and a noun, CI can still learn the complete12500 1 2 3NP[nb]/N : 0.08428 NP[nb] : 0.00138 S[dcl]\NP : 2.90E-5 S[dcl] : 1.0 0S[dcl]/NP : 2.90E-10The N : 0.01890 (S[dcl]\NP)/NP : 1.35E-7 S[dcl]\NP : 1.00E-41NP : 0.00174S/(S\NP) : 0.00152cat (S[dcl]\NP)/NP : 2.21E-9 S[dcl]\NP : 1.64E-62(S[dcl]\NP)\NP[nb] : 6.06E-19 S[dcl]\NP[nb] : 7.41E-17(S[dcl]/NP)\NP[nb] : 4.00E-23 (S[dcl]\NP)\N : 2.87E-17... ...X NP[nb]/N : 0.054673NP : 0.02439S/(S\NP) : 0.02124her .Figure 4: Example of a two-stage derivation using Chart Inference: Grey boxes are filled bottom-up by the partialparser; white boxes top-down by the learner.
The target cell (2,2) shows the correct category type as the highestprobability solution.lexicon, despite some initial missteps and a steepercurve.
However, the other two methods fail catas-trophically (Figure 6).
BF never gets going, since itcan only correctly learn the remaining nouns.
RB ispartially successful, but is thwarted by a bad deci-sion at 80% that quickly compounds to diverge fromthe target lexicon, ending up with higher coveragein the form of more lexical entries, but lower preci-sion, as the final similarity plateaus at the same levelas the original seed.4 Experiments II and III: CoverageNext, we compare the three learning methods on alarger corpus of natural language, to investigate howwell they perform at recovering a wide range of cat-egory types in complex settings.4.1 CorpusWe have constructed a small natural language lex-icon based on the first volume of a 6-volume 1836children?s primer, McGuffey?s Eclectic Reader.6Volume 1 of the McGuffey corpus (MG1) consistsof 546 sentences that have been manually annotatedwith CCG categories, automatically parsed, and thencorrected.
Volume 2 (MG2) comprises 801 sen-tences, annotated in the same manner as Volume 1,though not as reliably.
The McGuffey corpus makes6The raw text of William Holmes McGuffey?s EclecticReader is available as an e-book from Project Gutenberg athttp://www.gutenberg.org/ebooks/14640.
The annotated corpuswill be released shortly.an ideal seed for development purposes, as it con-tains a high proportion of simple declarative sen-tences, but also touches on questions, quotations,passives, and other complex constructions.4.2 MethodsIn the first of these two experiments we train andtest on the same corpus in one pass, attempting tolearn each word token in turn and comparing thelearned category set to the gold standard annota-tion.
Because we know that the lexicon contains allthe necessary entries to correctly parse all the sen-tences, this addresses the lexical coverage problemdiscussed in Section 1 of this paper.The second of these two experiments looks at amore realistic environment for word learning: theparser is initially trained on MG1, then tested onMG2.
We evaluate on the gold standard categoriesin MG2.
Since we are not guaranteed to have ac-cess to all the necessary word/category pairs in theseed lexicon, the precision and recall values for thissecond experiment will inevitably be lower than thefirst.Figure 7 outlines the process of producing newparsed sentences out of raw text.
The process be-gins like the previous experiment, but then the cat-egory set generated by the learner is passed back tothe parser, so it can incorporate this new informa-tion into its lexicon and produce a full parse.
TheHypothesis lexicon is cleared after every sentence.1251ParserRaw sentenceParsed sentencePartial ParseCategory SetHypothesislexiconOriginallexiconLearnerFigure 7: Learning framework for Experiments II-IV.4.3 ResultsTable 1 compares the category match accuracyacross the three systems in experiment II, as wellas the baseline that chose the the most probablecategory for the target word?s POS.
Two tasks arescored: Top One, where we evaluate the singlehighest-scoring category against the gold-standardtag, and Top Ten, where we check to see if the goldtag is in the set of the ten highest-probability cate-gories returned by the learner.CI achieves the best F-scores in both tasks, reach-ing 76% for Top One and 94% for Top Ten.
POSbackoff has an advantage in the Top Ten task, es-pecially in recall, since it returns an answer in ev-ery case, but CI still outperforms it on F-score.
BFachieves the highest precision in the Top One task,but takes 30 hours to do so, since it is searching overall possible categories.
RB is markedly worse inboth precision and recall, but also remarkably fast.CI combines the merits of both BF and RB, yield-ing a higher F-score than BF and a processing timesimilar to RB.In Experiment III, to test the limits of the learnerson truly OOL words, we again train on MG1, but testinstead on MG2.
We can then perform a meaningfulerror analysis on the results, showing how the threeword-learning methods compare in actual practice,in a realistic setting.Out of its 801 sentences in MG2, only 32 presentlearning opportunities for the learners, being be-tween 2 and 10 tokens long, containing no inter-nal punctuation or coordination, and containing onlyone OOL word.Table 2 shows the category match results of thethree systems on MG2.
Recall is calculated overthe set of learning opportunities, of which there areonly 32.
BF performs best in all metrics, but theCI results are reasonable.
The underlying reason forthis behaviour is that the 32 learning targets are allof common categories: over half of them are N orNP.
Since the Brute Force learner seeks simply tomaximise the tree probability, N and NP are its mostcommon guesses in general.5 Experiment IV: Domain AdaptationClark et al (2004) identified the problem with usingnews data to train a parser for a question answeringtask as the lack of lexical support for question words.Some lexical types were missing entirely.
The lexi-con for CCGBank ?02-21 contains 12 WH-questiontypes, notably lacking some important ones.
Clarket al note the absence of one category in partic-ular: (S[wq]/(S[dcl]\NP))/N, the category neededfor What President became Chief Justice after hispresidency?They attempt to adapt the discriminative C&Cparser (Clark and Curran, 2007) to the QA do-main by retraining on 500 hand-labelled questionsentences, then automatically parsing and hand-correcting an additional 671.
The entire set wasthen used in conjunction with CCGBank ?02-21 totrain a final parsing model.
Their per-word accuracyrose from a 68.5% baseline to 94.6% for the newlytrained model.In this experiment, we examine how close wecan get to those results by using Chart Inference tolearn WH-question words from the unlabelled ques-tion corpus.
If successful, this would eliminate thehuman-annotation step for domain adaptation of thekind investigated by (Clark et al, 2004).5.1 CorporaWe trained the initial parser on the CCG-Bank (Hockenmaier and Steedman, 2007; Hock-enmaier, 2003) training set (?02-21), consisting of39603 sentences of Wall Street Journal text (Marcuset al, 1993).
It is important to note that this trainingcorpus contains only 93 questions in total, so it isnot surprising that several category types for ques-tion words are entirely unrepresented.
It also rein-1252Top One Top TenP R F P R F Time (m)POS 64.91 64.91 64.91 92.55 92.55 92.55 1BF 80.53 65.84 72.45 95.97 78.32 86.25 1740RB 39.77 37.92 38.82 68.46 65.28 66.83 12CI 78.63 74.16 76.33 97.03 91.52 94.20 22Table 1: Exp.
II: Category match results for the three systems on the McGuffey corpus, training and testing on MG1.Top One Top TenP R F P R FBF 70.83 53.13 60.72 83.33 62.50 71.43RB 16.13 15.63 15.87 29.03 28.13 28.57CI 61.90 40.63 49.06 76.19 50.00 60.38Table 2: Exp.
III: Category match results for the three systems on the McGuffey corpus, training on MG1 and testingon MG2.
Common categories (N, NP, N/N) are overrepresented in the test data, leading to higher BF scores.
POStags not available for MG2, so no POS baseline is reported.forces the fact that this is a domain-adaptation task.We use the same 500-sentence test set as Rimelland Clark (2008b).
The test corpus consists of 488questions, each starting with What, When, How,Who or Where.
The learning corpus contains 1328questions in a similar distribution.Only three out of the five categories neededto parse What-questions are present in theCCGBank seed lexicon: S[wq]/(S[q]/NP),7,S[wq]/(S[dcl]\NP),8 and S[wq]/(S[q]/NP)/N.9For this experiment we focus on thesubject WH-element extraction category(S[wq]/(S[dcl]\NP))/N, as in Which cat is thegrandmother?.
This particular category was chosenas a point of investigation because it is OOL inCCGBank and is common enough to meaningfullyevaluate.5.2 MethodsThe baseline is the original StatCCG parser and lexi-con.
We also employ self-training (Charniak, 1997),in which a parser is used to parse a set of sentences,and then retrained using those output trees.
Self-training has had very little success in CCG appli-cations hitherto.
McClosky et al(2006) attributesuccess in self-training to a confluence of circum-7Object question category as in What is the Keystone State?8Subject question category as in What lays blue eggs?9Object WH-element extraction category as in What conti-nent is Scotland in?stances particular to their learning setting, which hasthe benefit of a discriminative re-ranker, both in theparsing case and in the learning case (McClosky etal., 2008).
We follow their recommendations thatthe best performance is achieved when all the train-ing sentences are parsed at once, rather than incre-mentally.We evaluate the success of CI in bootstrappingWh-question categories from the out-of-domain cor-pus in two ways.
First, we compare the CI output tothe gold standard categories labelled in Rimmell andClark (2008a).
Second, we add the parsed questionsinto the training set, then retrain and finally retest theparser.The parser was initially trained on CCGBank ?02-21 with a word frequency threshold of 5.10 It pro-duces partial parse charts in the cases where allwords in the sentence are in-lexicon, except for theWH-word target, for which the learner attempts toreturn a category motivated by that context.We run the learner on the set of 149 sentencesfrom the TREC Question-Answering corpus (Rimelland Clark, 2008b) that contain the word/categorypair What:(S[wq]/(S[dcl]\NP))/N.
For this exper-iment the end-punctuation distribution derived fromthe training corpus is replaced with a single value:P(S[wq]|?)
= 1.10StatCCG requires a parameter to trade off between trainingthe lexicon and the POS-backoff.1253BL CI CI+STAll Words 84.31 86.59 87.03POS=WHQ 53.40 56.19 59.54Word=What 55.87 60.83 65.42Cat=SubjExt 7.84 52.94 58.82Table 3: F-score over individual category matches.
Boldmeans significantly different from the Baseline.5.3 ResultsTable 3 shows the change in F-score throughout thisexperiment.
BL is the baseline condition, wherethe accuracy is predictably high over all the wordsin the sentence, but lower when we examine thequestion words only.
It is most telling that thebaseline F-score over words that should be taggedwith the subject WH-element-extraction category((S[wq]/(S[dcl]\NP))/N) is extremely low.
In fact,that seven percent represents only a handful of in-stances of Which, and none of What.
Applying ChartInference to the problem results in statistically sig-nificant increases in all metrics, but the biggest gainis in the last.
When we first apply CI, then self-trainover the full training corpus, we further increase allmetrics, and again the largest gain is over the targetcategory type specifically.
11 The reason for this canbe clearly seen when we evaluate the lexicons cre-ated by each method.Table 4 shows the differences in the impact onthe lexicon between baseline (BL), Chart Induc-tion (CI), and the combined method of CI and self-training (CI+ST).12 CI leaves the initial distributionunchanged while adding seven more category types.One of these is the category we are interested in:(S[wq]/(S[dcl]\NP))/N, which is previously asso-ciated with Which in the baseline lexicon.
The othersix are spurious categories, and have low counts.Combining the learning mechanisms by running firstCI, and then ST, has the effect of introducing the cat-egory we need, and then elevating the counts.
Theprobability for S[wq] is elevated as well, as a resultof misparses, but the whole process results in bet-11We also ran the experiment using ST only, which per-formed better than CI alone, but only over a different set consist-ing entirely of seen categories.
We do not report those figureshere because they are not commensurable with the CI results.12What has 31 categories in total in the baseline lexicon; herewe show only the [wq] types.ter category matches over the test set, as we saw inTable 3.5.4 Error AnalysisOf the previously known categories, the STstep overwhelmingly prefers three categories:one subject extraction category S[wq]/(S[dcl]\NP)and two object extraction S[wq]/(S[q]/NP) and(S[wq]/(S[q]/NP))/N.
The remaining categoriesare classified in Table 4 as either rare (R), spu-rious (*), or duplicate (D).
Rare categories, likeS[wq] are used for specialised cases (the sentenceWhat?)
which occur in PTB, but not in the QAcorpus.
Spurious categories, like (S[wq]/PP)/Nexist in the baseline parser, arising from er-rors in either the original PTB, or the transla-tion to CCGBank.
S[wq]/S[q] is only used whereS[wq]/(S[q]/NP) is meant, but fails to capture theextraction.
S[wq]/(S[dcl]/NP) is a misinterpretationof sentences requiring (S[wq]/(S[dcl]\NP))/N, butwithout capturing the extracted N.Five spurious categories are also introducedby the CI learning step.
(S[wq]/S[dcl])/N and(S[wq]/((S[dcl]\NP[expl])/NP))/N are spuriousforms of (S[wq]/(S[dcl]\NP))/N that arise whenthe constituent directly right of the target is mis-parsed; the former misses the extraction and the lat-ter adds an extra dummy subject.
S[wq]/N occurswhen the main verb of the sentence is treated asa participle, forming a complex nominal argument.
(S[wq]/N)/N and (S[wq]/(S[dcl]/(S[pt]\NP)))/Nare caused by similar verbal ambiguity.The classification of (S[wq]/S[inv])/N as a du-plicate category is linguistically motivated.
Ratherthan interpret the embedded sentence as declarative,the parser uses has:S[inv]/NP to interpret it insteadas an inverted sentence.
In essence, it cannot seethe difference between What companies have them?and What choice have they?
when the NPs lack acase distinction.
As such, it duplicates the work ofthe target (S[wq]/(S[dcl]\NP))/N, because the con-stituents S[dcl]\NP and S[inv] are often synonymousin practice.As seen in Table 4, the distinction between rareand spurious categories cannot be made on fre-quency alone, but the best categories are the oneswith the highest frequency.
Duplicate categories canbe considered spurious for the sake of parsing, but1254P(W |C) F P(C|W )?
C BL CI CI+ST BL CI CI+ST BL CI CI+STR S[wq] 0.09 0.09 0.17 1 1 2 0.006 0.005 0.002R S[wq]/PP 0.6 0.6 0.6 3 3 3 0.019 0.016 0.003?
(S[wq]/PP)/N 1 1 1 1 1 4 0.006 0.005 0.004?
S[wq]/(S[ad j]\NP) 0.5 0.5 0.5 1 1 1 0.006 0.005 0.001S[wq]/(S[dcl]\NP) 0.37 0.37 0.86 22 22 239 0.137 0.118 0.225S[wq]/(S[dcl]/NP) 1 1 1 1 1 8 0.006 0.005 0.008(S[wq]/(S[dcl]/NP))/N 0.5 0.5 0.5 1 1 1 0.006 0.005 0.001?
S[wq]/(S[ng]\NP) 1 1 1 1 1 2 0.006 0.005 0.002R S[wq]/S[poss] 0.83 0.83 0.83 5 5 5 0.031 0.027 0.005?
S[wq]/S[q] 0.03 0.03 0.12 2 2 9 0.012 0.011 0.008S[wq]/(S[q]/NP) 0.64 0.64 0.97 16 16 331 0.099 0.086 0.312(S[wq]/(S[q]/NP))/N 0.36 0.36 0.95 4 4 136 0.025 0.021 0.128(S[wq]/(S[dcl]\NP))/N - 0.5 0.96 - 4 75 - 0.021 0.071?
S[wq]/N - 1 1 - 8 12 - 0.043 0.011?
(S[wq]/S[dcl])/N - 1 1 - 8 28 - 0.043 0.026?
(S[wq]/N)/N - 1 1 - 4 7 - 0.021 0.007D (S[wq]/S[inv])/N - 1 1 - 3 78 - 0.016 0.074?
(S[wq]/(S[dcl]/(S[pt]\NP)))/N - 1 1 - 1 2 - 0.005 0.002?
(S[wq]/((S[dcl]\NP[expl])/NP))/N - 1 1 - 1 12 - 0.005 0.011Table 4: Exp.
IV: Lexical category distribution for the word What in the baseline ?02-21 of CCGBank (BL), afterChart Inference (CI), and after first applying Chart Inference, then self-training (CI+ST).
Column 1 classifies low-frequency categories as rare (R), spurious (*) or duplicate (D).
Cateogories above the middle line are present in theBaseline lexicon; below are induced.are linguistically interesting, and if they are frequentenough, that is possibly an indication that the struc-ture of the lexicon or the grammar is non-optimal.6 Conclusion and Future WorkChart Inference is a useful tool for finding OOL cat-egories.
It has been shown to outperform both thebrute-force and rule-based systems.
When used inconjunction with self-training, CI presents a valu-able framework for domain adaptation in the casewhere whole category types are missing from thelexicon.It remains to put Chart Inference into an appro-priate framework for improving coverage over thebaseline WSJ-trained StatCCG parser.
We estimatean upper bound of 20% error reduction possible overCCGBank ?00, if the lexicon is expanded to coverall the necessary word/category pairs.
Improvingglobal F-score for ?23 is of course very difficult.
Thelexical entries CI finds are by definition rare and atthe scale we are running, they are unlikely to occurin those 2000 sentences.
We believe our analysis ofthe lexical items themselves shows that we are learn-ing a high proportion of good lexical entries.The problem of discovering missing categoriesfor known words remains.
We have shown throughadapting to the question domain that it is possible tomake focused improvements when we can identifythe gaps in coverage (as in wh-question words), butin order to address the challenge of automatic lex-icon extension fully, quite different techniques forgeneralising lexical entries for seen words will berequire.AcknowledgementsThe authors would like to thank Steve Clarkand Laura Rimell for provision of the annotatedQA corpus, Christos Christodoulopoulos for theStatOpenCCG parser, Tejaswini Deoskar for edito-rial advice, and Luke Zettlemoyer for considerablemathematical assistance.
This work was partiallyfunded by EU ERC Advanced Fellowship 249520GRAMPLUS, IST Cognitive Systems IP EC-FP7-270273 ?XPERIENCE?
and a grant from WolfsonMicroelectronics.1255ReferencesEugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Proceed-ings of AAAI ?97, pages 598?603.Christos Christodoulopoulos.
2008.
Creating a natu-ral logic inference system with combinatory categorialgrammar.
Master?s thesis, School of Informatics, Uni-versity of Edinburgh.Stephen Clark and James R. Curran.
2003.
Log-linearmodels for wide-coverage CCG parsing.
In Proceed-ings of EMNLP ?03, pages 97?104, Morristown, NJ,USA.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics,33(4):493?552.Stephen Clark, Mark Steedman, and James Curran.
2004.Object-extraction and question-parsing using CCG.
InProceedings of EMNLP ?04, pages 111?118.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Compu-tational Linguistics, 33(3):355?396.Julia Hockenmaier.
2003.
Data and models for statis-tical parsing with Combinatory Categorial Grammar.Ph.D.
thesis, University of Edinburgh, Edinburgh, UK.Tsuneaki Kato.
1994.
Yet another chart-based techniquefor parsing ill-formed input.
In Proceedings of ANLC?94, pages 107?112, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.D.
Kazakov, S. Pulman, and S. Muggleton.
1998.
TheFraCas dataset and the LLL challenge.
Technical re-port, SRI International.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19:313?330, June.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of NAACL-HLT ?06, pages 152?159.David McClosky, Eugene Charniak, and Mark Johnson.2008.
When is self-training effective for parsing?
InProceedings of COLING ?08, pages 561?568, Morris-town, NJ, USA.Chris S. Mellish.
1989.
Some chart based techniques forparsing ill-formed input.
In Proceedings of the ACL?89, pages 102?109, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Laura Rimell and Stephen Clark.
2008a.
Adapting alexicalized-grammar parser to contrasting domains.
InProceedings of EMNLP ?08, pages 475?484, Strouds-burg, PA, USA.Laura Rimell and Stephen Clark.
2008b.
Constructing aparser evaluation scheme.
In COLING ?08: Proceed-ings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 44?50, Stroudsburg,PA, USA.Mark Steedman.
2000.
The Syntactic Process.
MITPress, Cambridge, MA, USA.Stephen Watkinson and Suresh Manandhar.
1999.
Un-supervised lexical learning of categorial grammars.
InACL?99: Workshop in Unsupervised Learning in Nat-ural Language Processing.Stephen Watkinson and Suresh Manandhar.
2000.
Un-supervised lexical learning with categorial grammarsusing the LLL corpus.
In James Cussens and SavsoDvzeroski, editors, Learning Language in Logic, vol-ume 1925 of Lecture Notes in Artificial Intelligence.Springer.Stephen Watkinson and Suresh Manandhar.
2001a.
Ac-quisition of large scale categorial grammar lexicons.In Proceedings of PACLING ?01.Stephen Watkinson and Suresh Manandhar.
2001b.
Apsychologically plausible and computationally effec-tive approach to learning syntax.
In Walter Daelemansand R?emi Zajac, editors, Proceedings of CoNLL ?01),pages 160 ?
167.Michael White and Jason Baldridge.
2003.
Adapt-ing chart realization to CCG.
In Proceedings of the9th European Workshop on Natural Language Gener-ation, pages 119?126.Xuchen Yao, Jianqiang Ma, Sergio Duarte, and CagriColtekin.
2009a.
An inference-rules based catego-rial grammar learner for simulating language acquisi-tion.
In Proceedings of the 18th Annual Belgian-DutchConference on Machine Learning, Tillburg.Xuchen Yao, Jianqiang Ma, Sergio Duarte, and CagriColtekin.
2009b.
Unsupervised syntax learning withcategorial grammars using inference rules.
In Pro-ceedings of The 14th Student Session of the EuropeanSummer School for Logic, Language, and Information,Bordeaux.Daniel H. Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andControl, 10(2):189?208.1256
