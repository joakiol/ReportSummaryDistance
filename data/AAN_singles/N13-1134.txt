Proceedings of NAACL-HLT 2013, pages 1142?1151,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsA Tensor-based Factorization Model of Semantic CompositionalityTim Van de CruysIRIT ?
UMR 5505CNRSToulouse, Francetim.vandecruys@irit.frThierry Poibeau?LaTTiCe ?
UMR 8094CNRS & ENSParis, Francethierry.poibeau@ens.frAnna KorhonenComputer Laboratory & DTAL?University of CambridgeUnited Kingdomanna.korhonen@cl.cam.ac.ukAbstractIn this paper, we present a novel method for thecomputation of compositionality within a distri-butional framework.
The key idea is that com-positionality is modeled as a multi-way interac-tion between latent factors, which are automat-ically constructed from corpus data.
We useour method to model the composition of sub-ject verb object triples.
The method consistsof two steps.
First, we compute a latent factormodel for nouns from standard co-occurrencedata.
Next, the latent factors are used to inducea latent model of three-way subject verb objectinteractions.
Our model has been evaluated ona similarity task for transitive phrases, in whichit exceeds the state of the art.1 IntroductionIn the course of the last two decades, significantprogress has been made with regard to the automaticextraction of lexical semantic knowledge from large-scale text corpora.
Most work relies on the distribu-tional hypothesis of meaning (Harris, 1954), whichstates that words that appear within the same contextstend to be semantically similar.
A large number ofresearchers have taken this dictum to heart, givingrise to a plethora of algorithms that try to capturethe semantics of words by looking at their distribu-tion in text.
Up till now, however, most work on theautomatic acquisition of semantics only deals withindividual words.
The modeling of meaning beyondthe level of individual words ?
i.e.
the combinationof words into larger units ?
is to a large degree leftunexplored.The principle of compositionality, often attributedto Frege, is the principle that states that the meaningof a complex expression is a function of the meaningof its parts and the way those parts are (syntactically)combined (Frege, 1892).
It is the fundamental prin-ciple that allows language users to understand themeaning of sentences they have never heard before,by constructing the meaning of the complex expres-sion from the meanings of the individual words.
Re-cently, a number of researchers have tried to reconcilethe framework of distributional semantics with theprinciple of compositionality (Mitchell and Lapata,2008; Baroni and Zamparelli, 2010; Coecke et al2010; Socher et al 2012).
However, the absolutegains of the systems remain a bit unclear, and a sim-ple method of composition ?
vector multiplication ?often seems to produce the best results (Blacoe andLapata, 2012).In this paper, we present a novel method for thejoint composition of a verb with its subject and di-rect object.
The key idea is that compositionality ismodeled as a multi-way interaction between latentfactors, which are automatically constructed fromcorpus data.
In order to adequately model the multi-way interaction between a verb and its subject andobjects, a significant part of our method relies ontensor algebra.
Additionally, our method makes useof a factorization model appropriate for tensors.The remainder of the paper is structured as follows.In section 2, we give an overview of previous workthat is relevant to the task of computing composition-ality within a distributional framework.
Section 3presents a detailed description of our method, in-cluding an overview of the necessary mathematical1142machinery.
Section 4 illustrates our method with anumber of detailed examples.
Section 5 presents aquantitative evaluation, and compares our methodto other models of distributional compositionality.Section 6, then, concludes and lays out a number ofdirections for future work.2 Previous WorkIn recent years, a number of methods have been de-veloped that try to capture compositional phenomenawithin a distributional framework.
One of the firstapproaches to tackle compositional phenomena in asystematic way is Mitchell and Lapata?s (2008) ap-proach.
They explore a number of different modelsfor vector composition, of which vector addition (thesum of each feature) and vector multiplication (theelementwise multiplication of each feature) are themost important.
They evaluate their models on anoun-verb phrase similarity task, and find that themultiplicative model yields the best results, alongwith a weighted combination of the additive and mul-tiplicative model.Baroni and Zamparelli (2010) present a methodfor the composition of adjectives and nouns.
In theirmodel, an adjective is a linear function of one vector(the noun vector) to another vector (the vector for theadjective-noun pair).
The linear transformation for aparticular adjective is represented by a matrix, andis learned automatically from a corpus, using partialleast-squares regression.Coecke et al(2010) present an abstract theoreti-cal framework in which a sentence vector is a func-tion of the Kronecker product of its word vectors,which allows for greater interaction between the dif-ferent word features.
A number of instantiations ofthe framework are tested experimentally in Grefen-stette and Sadrzadeh (2011a) and Grefenstette andSadrzadeh (2011b).
The key idea is that relationalwords (e.g.
adjectives or verbs) have a rich (multi-dimensional) structure that acts as a filter on theirarguments.
Our model uses an intuition similar totheirs.Socher et al(2012) present a model for composi-tionality based on recursive neural networks.
Eachnode in a parse tree is assigned both a vector anda matrix; the vector captures the actual meaning ofthe constituent, while the matrix models the wayit changes the meaning of neighbouring words andphrases.Closely related to the work on compositionalityis research on the computation of word meaning incontext.
Erk and Pado?
(2008, 2009) make use ofselectional preferences to express the meaning of aword in context; the meaning of a word in the pres-ence of an argument is computed by multiplying theword?s vector with a vector that captures the inverseselectional preferences of the argument.
Thater etal.
(2009, 2010) extend the approach based on se-lectional preferences by incorporating second-orderco-occurrences in their model.
And Dinu and La-pata (2010) propose a probabilistic framework thatmodels the meaning of words as a probability distri-bution over latent factors.
This allows them to modelcontextualized meaning as a change in the originalsense distribution.
Dinu and Lapata use non-negativematrix factorization (NMF) to induce latent factors.Similar to their work, our model uses NMF ?
albeitin a slightly different configuration ?
as a first steptowards our final factorization model.In general, latent models have proven to be usefulfor the modeling of word meaning.
One of the bestknown latent models of semantics is Latent Seman-tic Analysis (Landauer and Dumais, 1997), whichuses singular value decomposition in order to auto-matically induce latent factors from term-documentmatrices.
Another well known latent model of mean-ing, which takes a generative approach, is LatentDirichlet Allocation (Blei et al 2003).Tensor factorization has been used before for themodeling of natural language.
Giesbrecht (2010)describes a tensor factorization model for the con-struction of a distributional model that is sensitive toword order.
And Van de Cruys (2010) uses a tensorfactorization model in order to construct a three-wayselectional preference model of verbs, subjects, andobjects.
Our underlying tensor factorization ?
Tuckerdecomposition ?
is the same as Giesbrecht?s; andsimilar to Van de Cruys (2010), we construct a la-tent model of verb, subject, and object interactions.The way our model is constructed, however, is sig-nificantly different.
The former research does notuse any syntactic information for the constructionof the tensor, while the latter makes use of a morerestricted tensor factorization model, viz.
parallelfactor analysis (Harshman and Lundy, 1994).1143The idea of modeling compositionality by meansof tensor (Kronecker) product has been proposedin the literature before (Clark and Pulman, 2007;Coecke et al 2010).
However, the method presentedhere is the first that tries to capture compositionalphenomena by exploiting the multi-way interactionsbetween latent factors, induced by a suitable tensorfactorization model.3 Methodology3.1 Mathematical preliminariesThe methodology presented in this paper requiresa number of concepts and mathematical operationsfrom tensor algebra, which are briefly reviewed inthis section.
The interested reader is referred to Koldaand Bader (2009) for a more thorough introductionto tensor algebra (including an overview of variousfactorization methods).A tensor is a multidimensional array; it is the gen-eralization of a matrix to more than two dimensions,or modes.
Whereas matrices are only able to cap-ture two-way co-occurrences, tensors are able to cap-ture multi-way co-occurrences.1 Following prevail-ing convention, tensors are represented by boldfaceEuler script notation (X), matrices by boldface capi-tal letters (X), vectors by boldface lower case letters(x), and scalars by italic letters (x).The n-mode product of a tensor X ?
RI1?I2?...
?INwith a matrix U ?
RJ?In is denoted by X?n U, andis defined elementwise as(X?n U)i1...in?1 jin+1...iN =In?in=1xi1i2...iN u jin (1)The Kronecker product of matrices A ?
RI?J andB?RK?L is denoted by A?B.
The result is a matrixof size (IK)?
(JL), and is defined byA?B =????
?a11B a12B ?
?
?
a1JBa21B a22B ?
?
?
a2JB....... .
....aI1B aI2B .
.
.
aIJB?????
(2)1In this research, we limit ourselves to three-way co-occurrences of verbs, subject, and objects, modelled using athree-mode tensor.A special case of the Kronecker product is theouter product of two vectors a ?
RI and b ?
RJ , de-noted a?b.
The result is a matrix A ?
RI?J obtainedby multiplying each element of a with each elementof b.Finally, the Hadamard product, denoted A ?B,is the elementwise multiplication of two matricesA ?
RI?J and B ?
RI?J , which produces a matrixthat is equally of size I?
J.3.2 The construction of latent noun factorsThe first step of our method consists in the construc-tion of a latent factor model for nouns, based on theircontext words.
For this purpose, we make use of non-negative matrix factorization (Lee and Seung, 2000).Non-negative matrix factorization (NMF) minimizesan objective function ?
in our case the Kullback-Leibler (KL) divergence ?
between an original matrixVI?J and WI?KHK?J (the matrix multiplication ofmatrices W and H) subject to the constraint that allvalues in the three matrices be non-negative.
Param-eter K is set  I,J so that a reduction is obtainedover the original data.
The factorization model isrepresented graphically in figure 1.= xV W Hkknounscontext wordsnounscontext wordsFigure 1: Graphical representation of NMFNMF can be computed fairly straightforwardly,alternating between the two iterative update rulesrepresented in equations 3 and 4.
The update rulesare guaranteed to converge to a local minimum in theKL divergence.Ha?
?Ha?
?i WiaVi?(WH)i?
?k Wka(3)Wia?Wia??
Ha?Vi?(WH)i?
?v Hav(4)3.3 Modeling multi-way interactionsIn our second step, we construct a multi-way interac-tion model for subject verb object (svo) triples, based1144on the latent factors induced in the first step.
Ourlatent interaction model is inspired by a tensor factor-ization model called Tucker decomposition (Tucker,1966), although our own model instantiation differssignificantly.
In order to explain our method, wefirst revisit Tucker decomposition, and subsequentlyexplain how our model is constructed.3.3.1 Tucker decompositionTucker decomposition is a multilinear generaliza-tion of the well-known singular value decomposition,used in Latent Semantic Analysis.
It is also known ashigher order singular value decomposition (HOSVD,De Lathauwer et al(2000)).
In Tucker decomposi-tion, a tensor is decomposed into a core tensor, multi-plied by a matrix along each mode.
For a three-modetensor X ?
RI?J?L, the model is defined asX = G?1 A?2 B?3 C (5)=P?p=1Q?q=1R?r=1gpqrap ?bq ?
cr (6)Setting P,Q,R I,J,L, the core tensor G repre-sents a compressed, latent version of the original ten-sor X; matrices A ?RI?P, B ?RJ?Q, and C ?RL?Rrepresent the latent factors for each mode, whileG ?
RP?Q?R indicates the level of interaction be-tween the different latent factors.
Figure 2 shows agraphical representation of Tucker decomposition.2subjectsverbsobjects=objectskkkverbssubjectskkkFigure 2: A graphical representation of Tucker decompo-sition2where P = Q = R = K, i.e.
the same number of latent factorsK is used for each mode3.3.2 Reconstructing a Tucker model fromtwo-way factorsComputing the Tucker decomposition of a tensoris rather costly in terms of time and memory require-ments.
Moreover, the decomposition is not unique:the core tensor G can be modified without affectingthe model?s fit by applying the inverse modificationto the factor matrices.
These two drawbacks led usto consider an alternative method for the construc-tion of the Tucker model.
Specifically, we considerthe factor matrices as given (as the output from ourfirst step), and proceed to compute the core tensor G.Additionally, we do not use a latent representationfor the first mode, which means that the first mode isrepresented by its original instances.Our model can be straightforwardly applied to lan-guage data.
The core tensor G models the latentinteractions between verbs, subject, and objects.
Gis computed by applying the n-mode product to theappropriate mode of the original tensor (equation 7),G=X?2 WT ?3 WT (7)where XV?N?N is our original data tensor, consistingof the weighted co-occurrence frequencies of svotriples (extracted from corpus data), and WN?K isour latent factor matrix for nouns.
Note that we donot use a latent representation for the verb mode.
Tobe able to efficiently compute the similarity of verbs(both within and outside of compositional phrases),only the subject and object mode are represented bylatent factors, while the verb mode is representedby its original instances.
This means that our coretensor G will be of size V ?K?K.3 A graphicalrepresentation is given in figure 3.Note that both tensor X and factor matrices W arenon-negative, which means our core tensor G willalso be non-negative.3.4 The composition of svo triplesIn order to compute the composition of a particularsubject verb object triple ?s,v,o?, we first extract theappropriate subject vector ws and object vector wo(both of length K) from our factor matrix W, and3It is straightforward to also construct a latent factor modelfor verbs using NMF, and include it in the construction of ourcore tensor; we believe such a model might have interestingapplications, but we save this as an exploration for future work.1145subjectsverbsobjects=objectskkverbssubjectskkFigure 3: A graphical representation of our model instan-tiation without the latent verb modecompute the outer product of both vectors, resultingin a matrix Y of size K?K.Y = ws ?wo (8)Our second and final step is then to weight theoriginal verb matrix Gv of latent interactions (theappropriate verb slice of tensor G) with matrix Y,containing the latent interactions of the specific sub-ject and object.
This is carried out by taking theHadamard product of Gv and Y.Z = Gv ?Y (9)4 ExampleIn this section, we present a number of example com-putations that clarify how our model is able to capturecompositionality.
All examples come from actual cor-pus data, and are computed in a fully automatic andunsupervised way.Consider the following two sentences:(1) The athlete runs a race.
(2) The user runs a command.Both sentences contain the verb run, but they rep-resent clearly different actions.
When we computethe composition of both instances of run with theirrespective subject and object, we want our model toshow this difference.To compute the compositional representation ofsentences (1) and (2), we proceed as follows.
First,we extract the latent vectors for subject and object(wathlete and wrace for the first sentence, wuser andwcommand for the second sentence) from matrix W.Next, we compute the outer product of subject andobject ?
wathlete ?wrace and wuser ?wcommand ?
whichyields matrices Y?athlete,race?
and Y?user,command?.
Byvirtue of the outer product, the matrices Y ?
of sizeK?K ?
represent the level of interaction between thelatent factors of the subject and the latent factors ofthe object.
We can inspect these interactions by look-ing up the factor pairs (i.e.
matrix cells) with the high-est values in the matrices Y.
Table 1 presents the fac-tor pairs with highest value for matrix Y?athlete,race?
;table 2 represents the factor pairs with highest valuefor matrix Y?user,command?.
In order to render the fac-tors interpretable, we include the three most salientwords for the various factors (i.e.
the words with thehighest value for a particular factor).The examples in tables 1 and 2 give an impressionof the effect of the outer product: semantic featuresof the subject combine with semantic features of theobject, indicating the extent to which these featuresinteract within the expression.
In table 1, we noticethat animacy features (28, 195) and a sport feature(25) combine with a ?sport event?
feature (119).
Intable 2, we see that similar animacy features (40,195) and technological features (7, 45) combine withanother technological feature (89).Similarly, we can inspect the latent interactions ofthe verb run, which are represented in the tensor sliceGrun.
Note that this matrix contains the verb seman-tics computed over the complete corpus.
The mostsalient factor interactions for Grun are represented intable 3.Table 3 illustrates that different senses of the verbrun are represented within the matrix Grun.
The firsttwo factor pairs hint at the ?organize?
sense of theverb (run a seminar).
The third factor pair repre-sents the ?transport?
sense of the verb (the bus runsevery hour).4 And the fourth factor pair representsthe ?execute?
or ?deploy?
sense of run (run Linux,run a computer program).
Note that we only showthe factor pairs with the highest value; matrix G con-tains a value for each pairwise combination of thelatent factors, effectively representing a rich latentsemantics for the verb in question.The last step is to take the Hadamard product ofmatrices Y with verb matrix G, which yields our final4Obviously, hour is not an object of the verb, but due toparsing errors it is thus represented.1146factors subject object value?195,119?
people (.008), child (.008), adolescent (.007) cup (.007), championship (.006), final (.005) .007?25,119?
hockey (.007), poker (.007), tennis (.006) cup (.007), championship (.006), final (.005) .004?90,119?
professionalism (.007), teamwork (.007), confi-dence (.006)cup (.007), championship (.006), final (.005) .003?28,119?
they (.004), pupil (.003), participant (.003) cup (.007), championship (.006), final (.005) .003Table 1: Factor pairs with highest value for matrix Y?athlete,race?factors subject object value?7,89?
password (.009), login (.007), username (.007) filename (.007), null (.006), integer (.006) .010?40,89?
anyone (.004), reader (.004), anybody (.003) filename (.007), null (.006), integer (.006) .007?195,89?
people (.008), child (.008), adolescent (.007) filename (.007), null (.006), integer (.006) .006?45,89?
website (.004), Click (.003), site (.003) filename (.007), null (.006), integer (.006) .006Table 2: Factor pairs with highest value for matrix Y?user,command?matrices, Zrun,?athlete,race?
and Zrun,?user,command?.
TheHadamard product will act as a bidirectional filteron the semantics of both the verb and its subjectand object: interactions of semantic features that arepresent in both matrix Y and G will be highlighted,while the other interactions are played down.
Theresult is a representation of the verb?s semantics tunedto its particular subject-object combination.
Note thatthis final step can be viewed as an instance of functionapplication (Baroni and Zamparelli, 2010).
Alsonote the similarity to Grefenstette and Sadrzadeh?s(2011a,2011b) approach, who equally make use ofthe elementwise matrix product in order to weightthe semantics of the verb.We can now go back to our original tensor G, andcompute the most similar verbs (i.e.
the most similartensor slices) for our newly computed matrices Z.5If we do this for matrix Zrun,?athlete,race?, our modelcomes up with verbs finish (.29), attend (.27), andwin (.25).
If, instead, we compute the most similarverbs for Zrun,?user,command?, our model yields execute(.42), modify (.40), invoke (.39).Finally, note that the design of our model natu-rally takes into account word order.
Consider thefollowing two sentences:(3) man damages car(4) car damages man5Similarity is calculated by measuring the cosine of the vec-torized and normalized representation of the verb matrices.Both sentences contain the exact same words, but theprocess of damaging described in sentences (3) and(4) is of a rather different nature.
Our model is ableto take this difference into account: if we computeZdamage,?man,car?
following sentence (3), our modelyields crash (.43), drive (.35), ride (.35) as most sim-ilar verbs.
If we do the same for Zdamage,?car,man?
fol-lowing sentence (4), our model instead yields scare(.26), kill (.23), hurt (.23).5 Evaluation5.1 MethodologyIn order to evaluate the performance of our tensor-based factorization model of compositionality, wemake use of the sentence similarity task for transi-tive sentences, defined in Grefenstette and Sadrzadeh(2011a).
This is an extension of the similarity taskfor compositional models developed by Mitchell andLapata (2008), and constructed according to the sameguidelines.
The dataset contains 2500 similarityjudgements, provided by 25 participants, and is pub-licly available.6The data consists of transitive verbs, each pairedwith both a subject and an object noun ?
thus form-ing a small transitive sentence.
Additionally, a ?land-mark?
verb is provided.
The idea is to compose boththe target verb and the landmark verb with subjectand noun, in order to form two small compositional6http://www.cs.ox.ac.uk/activities/CompDistMeaning/GS2011data.txt1147factors subject object value?128,181?
Mathematics (.004), Science (.004), Economics(.004)course (.005), tutorial (.005), seminar (.005) .058?293,181?
organization (.007), association (.007), federa-tion (.006)course (.005), tutorial (.005), seminar (.005) .053?60,140?
rail (.011), bus (.009), ferry (.008) third (.004), decade (.004), hour (.004) .038?268,268?
API (.008), Apache (.007), Unix (.007) API (.008), Apache (.007), Unix (.007) .038Table 3: Factor combinations for Grunphrases.
The system is then required to come up witha suitable similarity score for these phrases.
The cor-relation of the model?s judgements with human judge-ments (scored 1?7) is then calculated using Spear-man?s ?
.
Two examples of the task are provided intable 4.p target subject object landmark sim19 meet system criterion visit 121 write student name spell 6Table 4: Two example judgements from the phrase simi-larity task defined by Grefenstette and Sadrzadeh (2011a)Grefenstette and Sadrzadeh (2011a) seem to cal-culate the similarity score contextualizing both thetarget verb and the landmark verb.
Another possibil-ity is to contextualize only the target verb, and com-pute the similarity score with the non-contextualizedlandmark verb.
In our view, the latter option pro-vides a better assessment of the model?s similar-ity judgements, since contextualizing low-similaritylandmarks often yields non-sensical phrases (e.g.
sys-tem visits criterion).
We provide scores for bothcontextualized and non-contextualized landmarks.We compare our results to a number of differentmodels.
The first is Mitchell and Lapata?s (2008)model, which computes the elementwise vector mul-tiplication of verb, subject and object.
The secondis Grefenstette and Sadrzadeh?s (2011b) best scoringmodel instantiation of the categorical distributionalcompositional model (Coecke et al 2010).
Thismodel computes the outer product of the subject andobject vector, the outer product of the verb vectorwith itself, and finally the elementwise product ofboth results.
It yields the best score on the transitivesentence similarity task reported to date.As a baseline, we compute the non-contextualizedsimilarity score for target verb and landmark.
The up-per bound is provided by Grefenstette and Sadrzadeh(2011a), based on interannotator agreement.5.2 Implementational detailsAll models have been constructed using the UKWACcorpus (Baroni et al 2009), a 2 billion word corpusautomatically harvested from the web.
From this data,we accumulate the input matrix V for our first NMFstep.
We use the 10K most frequent nouns, cross-classified by the 2K most frequent context words.7Matrix V is weighted using pointwise mutual infor-mation (PMI, Church and Hanks (1990)).A parsed version of the corpus is available, whichhas been parsed with MaltParser (Nivre et al 2006).We use this version in order to extract our svo triples.From these triples, we construct our tensor X, using1K verbs ?
10K subjects ?
10K objects.
Note onceagain that the subject and object instances in the sec-ond step are exactly the same as the noun instancesin the first step.
Tensor X has been weighted using athree-way extension of PMI, following equation 10(Van de Cruys, 2011).pmi3(x,y,z) = logp(x,y,z)p(x)p(y)p(z)(10)We set K = 300 as our number of latent factors.The value was chosen as a trade-off between a modelthat is both rich enough, and does not require anexcessive amount of memory (for the modeling ofthe core tensor).
The algorithm runs fairly effi-ciently.
Each NMF step is computed in a matter ofseconds, with convergence after 50?100 iterations.The construction of the core tensor is somewhat more7We use a context window of 5 words, both before and afterthe target word; a stop list was used to filter out grammaticalfunction words.1148evolved, but does not exceed a wall time of 30 min-utes.
Results have been computed on a machine withIntel Xeon 2.93Ghz CPU and 32GB of RAM.5.3 ResultsThe results of the various models are presented in ta-ble 5; multiplicative represents Mitchell and Lapata?s(2008) multiplicative model, categorical representsGrefenstette and Sadrzadeh?s (2011b) model, andlatent represents the model presented in this paper.model contextualized non-contextualizedbaseline .23multiplicative .32 .34categorical .32 .35latent .32 .37upper bound .62Table 5: Results of the different compositionality modelson the phrase similarity taskIn the contextualized version of the similarity task(in which the landmark is combined with subjectand object), all three models obtain the same result(.32).
However, in the non-contextualized version(in which only the target verb is combined with sub-ject and object), the models differ in performance.These differences are statistically significant.8 Asmentioned before, we believe the non-contextualizedversion of the task gives a better impression of thesystems?
ability to capture compositionality.
Thecontextualization of the landmark verb often yieldsnon-sensical combinations, such as system visits crite-rion.
We therefore deem it preferable to compute thesimilarity of the target verb in composition (systemmeets criterion) to the non-contextualized semanticsof the landmark verb (visit).Note that the scores presented in this evalua-tion (including the baseline score) are significantlyhigher than the scores presented in Grefenstette andSadrzadeh (2011b).
This is not surprising, since thecorpus we use ?
UKWAC ?
is an order of magni-tude larger than the corpus used in their research ?the British National Corpus (BNC).
Presumably, thescores are also favoured by our weighting measure.8 p < 0.01; model differences have been tested using stratifiedshuffling (Yeh, 2000).In our experience, PMI performs better than weight-ing with conditional probabilities.96 ConclusionIn this paper, we presented a novel method for thecomputation of compositionality within a distribu-tional framework.
The key idea is that composition-ality is modeled as a multi-way interaction betweenlatent factors, which are automatically constructedfrom corpus data.
We used our method to modelthe composition of subject verb object combinations.The method consists of two steps.
First, we com-pute a latent factor model for nouns from standardco-occurrence data.
Next, the latent factors are usedto induce a latent model of three-way subject verbobject interactions, represented by a core tensor.
Ourmodel has been evaluated on a similarity task for tran-sitive phrases, in which it matches and even exceedsthe state of the art.We conclude with a number of future work issues.First of all, we would like to extend our framework inorder to incorporate more compositional phenomena.Our current model is designed to deal with the latentmodeling of subject verb object combinations.
Wewould like to investigate how other compositionalphenomena might fit within our latent interactionframework, and how our model is able to tackle thecomputation of compositionality across a differingnumber of modes.Secondly, we would like to further explore thepossibilities of our model in which all three modesare represented by latent factors.
The instantiationof our model presented in this paper has two latentmodes, using the original instances of the verb modein order to efficiently compute verb similarity.
Wethink a full-blown latent interaction model mightprove to have interesting applications in a number ofNLP tasks, such as the paraphrasing of compositionalexpressions.Finally, we would like to test our method using anumber of different evaluation frameworks.
We thinktasks of similarity judgement have their merits, but ina way are also somewhat limited.
In our opinion, re-search on the modeling of compositional phenomenawithin a distributional framework would substantially9Contrary to the findings of Mitchell and Lapata (2008), whoreport a high correlation with human similarity judgements.1149benefit from new evaluation frameworks.
In particu-lar, we think of a lexical substitution or paraphrasingtask along the lines of McCarthy and Navigli (2009),but specifically aimed at the assessment of composi-tional phenomena.AcknowledgementsTim Van de Cruys and Thierry Poibeau are supportedby the Centre National de la Recherche Scientifique(CNRS, France), Anna Korhonen is supported by theRoyal Society (UK).ReferencesBrett W. Bader, Tamara G. Kolda, et al2012.
Matlab ten-sor toolbox version 2.5. http://www.sandia.gov/~tgkolda/TensorToolbox/.Marco Baroni and Roberto Zamparelli.
2010.
Nouns arevectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.
In Proceedingsof the 2010 Conference on Empirical Methods in Nat-ural Language Processing, pages 1183?1193, Cam-bridge, MA, October.
Association for ComputationalLinguistics.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: Acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evaluation,43(3):209?226.William Blacoe and Mirella Lapata.
2012.
A comparisonof vector-based representations for semantic compo-sition.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 546?556, Jeju Island, Korea, July.
Associationfor Computational Linguistics.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
The Journal of Ma-chine Learning Research, 3:993?1022.Kenneth W. Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information & lexicography.Computational Linguistics, 16(1):22?29.Stephen Clark and Stephen Pulman.
2007.
Combiningsymbolic and distributional models of meaning.
In Pro-ceedings of the AAAI Spring Symposium on QuantumInteraction, pages 52?55.Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark.2010.
Mathematical foundations for a compositionaldistributed model of meaning.
Lambek Festschrift, Lin-guistic Analysis, vol.
36, 36.Lieven De Lathauwer, Bart De Moor, and Joseph Vande-walle.
2000.
A multilinear singular value decomposi-tion.
SIAM Journal on Matrix Analysis and Applica-tions, 21(4):1253?1278.Georgiana Dinu and Mirella Lapata.
2010.
Measuringdistributional similarity in context.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1162?1172, Cambridge,MA, October.Katrin Erk and Sebastian Pado?.
2008.
A structured vectorspace model for word meaning in context.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 897?906, Waikiki,Hawaii, USA.Katrin Erk and Sebastian Pado?.
2009.
Paraphrase assess-ment in structured vector space: Exploring parametersand datasets.
In Proceedings of the Workshop on Geo-metrical Models of Natural Language Semantics, pages57?65, Athens, Greece.Gottlob Frege.
1892.
U?ber Sinn und Bedeutung.Zeitschrift fu?r Philosophie und philosophische Kritik,100:25?50.Eugenie Giesbrecht.
2010.
Towards a matrix-based dis-tributional model of meaning.
In Proceedings of theNAACL HLT 2010 Student Research Workshop, pages23?28.
Association for Computational Linguistics.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011a.Experimental support for a categorical compositionaldistributional model of meaning.
In Proceedings ofthe 2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 1394?1404, Edinburgh,Scotland, UK., July.
Association for ComputationalLinguistics.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011b.Experimenting with transitive verbs in a discocat.
InProceedings of the GEMS 2011 Workshop on GEomet-rical Models of Natural Language Semantics, pages62?66, Edinburgh, UK, July.
Association for Computa-tional Linguistics.Zellig S. Harris.
1954.
Distributional structure.
Word,10(23):146?162.Richard A Harshman and Margaret E Lundy.
1994.Parafac: Parallel factor analysis.
Computational Statis-tics & Data Analysis, 18(1):39?72.Tamara G. Kolda and Brett W. Bader.
2009.
Ten-sor decompositions and applications.
SIAM Review,51(3):455?500, September.Tamara G. Kolda and Jimeng Sun.
2008.
Scalable tensordecompositions for multi-aspect data mining.
In ICDM2008: Proceedings of the 8th IEEE International Con-ference on Data Mining, pages 363?372, December.Thomas Landauer and Susan Dumais.
1997.
A solutionto Plato?s problem: The Latent Semantic Analysis the-1150ory of the acquisition, induction, and representation ofknowledge.
Psychology Review, 104:211?240.Daniel D. Lee and H. Sebastian Seung.
2000.
Algorithmsfor non-negative matrix factorization.
In Advances inNeural Information Processing Systems 13, pages 556?562.Diana McCarthy and Roberto Navigli.
2009.
The En-glish lexical substitution task.
Language resources andevaluation, 43(2):139?159.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
proceedings of ACL-08: HLT, pages 236?244.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
Malt-parser: A data-driven parser-generator for dependencyparsing.
In Proceedings of LREC-2006, pages 2216?2219.Richard Socher, Brody Huval, Christopher D. Manning,and Andrew Y. Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceedingsof the 2012 Joint Conference on Empirical Methods inNatural Language Processing and Computational Nat-ural Language Learning, pages 1201?1211, Jeju Island,Korea, July.
Association for Computational Linguistics.Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
2009.Ranking paraphrases in context.
In Proceedings of the2009 Workshop on Applied Textual Inference, pages44?47, Suntec, Singapore.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations usingsyntactically enriched vector models.
In Proceedings ofthe 48th Annual Meeting of the Association for Compu-tational Linguistics, pages 948?957, Uppsala, Sweden.Ledyard R. Tucker.
1966.
Some mathematical notes onthree-mode factor analysis.
Psychometrika, 31(3):279?311.Tim Van de Cruys.
2010.
A non-negative tensor fac-torization model for selectional preference induction.Natural Language Engineering, 16(4):417?437.Tim Van de Cruys.
2011.
Two multivariate generaliza-tions of pointwise mutual information.
In Proceedingsof the Workshop on Distributional Semantics and Com-positionality, pages 16?20, Portland, Oregon, USA,June.
Association for Computational Linguistics.Alexander Yeh.
2000.
More accurate tests for the statis-tical significance of result differences.
In Proceedingsof the 18th conference on Computational linguistics,pages 947?953, Saarbru?cken, Germany.1151
