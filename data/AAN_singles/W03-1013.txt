Log-Linear Models for Wide-Coverage CCG ParsingStephen Clark and James R. CurranSchool of InformaticsUniversity of Edinburgh2 Buccleuch Place, Edinburgh.
EH8 9LWfstephenc,jamescg@cogsci.ed.ac.ukAbstractThis paper describes log-linear pars-ing models for Combinatory CategorialGrammar (CCG).
Log-linear models caneasily encode the long-range dependen-cies inherent in coordination and extrac-tion phenomena, which CCG was designedto handle.
Log-linear models have pre-viously been applied to statistical pars-ing, under the assumption that all possibleparses for a sentence can be enumerated.Enumerating all parses is infeasible forlarge grammars; however, dynamic pro-gramming over a packed chart can be usedto efficiently estimate the model parame-ters.
We describe a parellelised implemen-tation which runs on a Beowulf cluster andallows the complete WSJ Penn Treebankto be used for estimation.1 IntroductionStatistical parsing models have recently been de-veloped for Combinatory Categorial Grammar(CCG, Steedman (2000)) and used in wide-coverageparsers applied to the WSJ Penn Treebank (Clark etal., 2002; Hockenmaier and Steedman, 2002).
Anattraction of CCG is its elegant treatment of coor-dination and extraction, allowing recovery of thelong-range dependencies inherent in these construc-tions.
We would like the parsing model to includelong-range dependencies, but this introduces prob-lems for generative parsing models similar to thosedescribed by Abney (1997) for attribute-value gram-mars; hence Hockenmaier and Steedman do not in-clude such dependencies in their model, and Clarket al include the dependencies but use an incon-sistent model.
Following Abney, we propose a log-linear framework which incorporates long-range de-pendencies as features without loss of consistency.Log-linear models have previously been ap-plied to statistical parsing (Johnson et al, 1999;Toutanova et al, 2002; Riezler et al, 2002; Os-borne, 2000).
Typically, these approaches have enu-merated all possible parses for model estimationand finding the most probable parse.
For gram-mars extracted from the Penn Treebank (in our caseCCGbank (Hockenmaier, 2003)), enumerating allparses is infeasible.
One approach to this prob-lem is to sample the parse space for estimation, e.g.Osborne (2000).
In this paper we use a dynamic pro-gramming technique applied to a packed chart, simi-lar to those proposed by Geman and Johnson (2002)and Miyao and Tsujii (2002), which efficiently esti-mates the model parameters over the complete spacewithout enumerating parses.
The estimation methodis similar to the inside-outside algorithm used for es-timating a PCFG (Lari and Young, 1990).Miyao and Tsujii (2002) apply their estimationtechnique to an automatically extracted Tree Adjoin-ing Grammar using Improved Iterative Scaling (IIS,Della Pietra et al (1997)).
However, their modelhas significant memory requirements which limitsthem to using 868 sentences as training data.
We usea parallelised version of Generalised Iterative Scal-ing (GIS, Darroch and Ratcliff (1972)) on a Beowulfcluster which allows the complete WSJ Penn Tree-bank to be used as training data.This paper assumes a basic knowledge of CCG;see Steedman (2000) and Clark et al (2002) for anintroduction.2 The GrammarFollowing Clark et al (2002), we augment CCG lex-ical categories with head and dependency informa-tion.
For example, the extended category for per-suade is as follows:persuade :=((S[dcl]persuade\NP1)=(S[to]2\NPX))=NPX,3 (1)The feature [dcl] indicates a declarative sentence; theresulting S[dcl] is headed by persuade; and the num-bers indicate dependency relations.
The variable Xdenotes a head, identifying the head of the infiniti-val complement?s subject with the head of the ob-ject, thus capturing the object control relation.
Forexample, in Microsoft persuades IBM to buy Lotus,IBM fills the subject slot of buy.Formally, a dependency is defined as a 5-tuple:?hf ; f ; s; ha; l?, where hf is the head word of thefunctor, f is the functor category (extended withhead and dependency information), s is the argu-ment slot, and ha is the head word of the argument.The l is an additional field used to encode whetherthe dependency is long-range.
For example, the de-pendency encoding Lotus as the object of bought (asin IBM bought Lotus) is represented as follows:?bought; (S[dcl]bought\NP1)=NP2; 2; Lotus; null?
(2)If the object has been extracted using a relative pro-noun with the category (NP\NP)=(S[dcl]=NP) (as inthe company that IBM bought), the dependency is asfollows:?bought; (S[dcl]bought\NP1)=NP2; 2; company; ??
(3)where ?
is the category (NP\NP)=(S[dcl]=NP) as-signed to the relative pronoun.
A dependency struc-ture is simply a set of these dependencies.Every argument in every lexical category is en-coded as a dependency.
Unlike Clark et al, we donot require dependencies to be always marked onatomic categories.
For example, the marked up cat-egory for about (as in about 5,000 pounds) is:(NX=NX)Y=(N=N)Y,1 (4)If 5,000 has the category (NX=NX)5,000, the depen-dency relation marked on the (N=N)Y,1 argument in(4) allows the dependency between about and 5,000to be captured.Clark et al (2002) give examples showing howheads can fill dependency slots during a derivation,and how long-range dependencies can be recoveredthrough unification of co-indexed head variables.3 Log-Linear Models for CCGPrevious parsing models for CCG include a genera-tive model over normal-form derivations (Hocken-maier and Steedman, 2002) and a conditional modelover dependency structures (Clark et al, 2002).We follow Clark et al in modelling dependencystructures, but, unlike Clark et al, do so in termsof derivations.
An advantage of our approach isthat the model can potentially include derivation-specific features in addition to dependency informa-tion.
Also, modelling derivations provides a closelink between the model and the parsing algorithm,which makes it easier to define dynamic program-ming techniques for efficient model estimation anddecoding1, and also apply beam search to reduce thesearch space.The probability of a dependency structure,  ?
,given a sentence, S , is defined as follows:P(|S ) =?d?(;S )P(d; |S ) (5)where (; S ) is the set of derivations for S whichlead to  and  is the set of dependency structures.Note that (; S ) includes the non-standard deriva-tions allowed by CCG.
This model allows the pos-sibility of including features from the non-standardderivations, such as features encoding the use oftype-raising or function composition.A log-linear model of a parse, !
?
?, given asentence S , is defined as follows:P(!|S ) = 1ZS?ifi(!
)i (6)This model can be applied to any kind of parse, butfor this paper a parse, !, is a ?d; ?
pair (as givenin (5)).
The function fi is a feature of the parse1We use the term decoding to refer to the process of findingthe most probable dependency structure from a packed chart.which can be any real-valued function over the spaceof parses?.
In this paper fi(!)
is a count of the num-ber of times some dependency occurs in !.
Eachfeature fi has an associated weight i which is a pa-rameter of the model to be estimated.
ZS is a normal-ising constant which ensures that P(!|S ) is a proba-bility distribution:ZS =?!?
?(S )?ifi(!?
)i (7)where (S ) ?
?
is the set of possible parses for S .The advantage of a log-linear model is that thefeatures can be arbitrary functions over parses.
Thismeans that any dependencies ?
including overlap-ping and long-range dependencies ?
can be includedin the model, irrespective of whether those depen-dencies are independent.The theory underlying log-linear modelsis described in Della Pietra et al (1997) andBerger et al (1996).
Briefly, the log-linear form in(6) is derived by choosing the model with maximumentropy from a set of models that satisfy a certainset of constraints (Rosenfeld, 1996).
The constraintsare that, for each feature fi:?!
;S?P(S )P(!|S ) fi(!)
=?!;S?P(!
; S ) fi(!)
(8)where the sums are over all possible parse-sentencepairs and ?P(S ) is the relative frequency of sentenceS in the data.
The value on the left of (8) is theexpected value of fi according to the model, Ep fi,and the value on the right is the empirical expectedvalue of fi, Ep?
fi.Estimating the parameters of a log-linear modelrequires the values in (8) to be calculated for eachfeature.
Calculating the empirical expected val-ues requires a treebank of CCG derivations plusdependency structures.
For this we use CCG-bank (Hockenmaier, 2003), a corpus of normal-form CCG derivations derived semi-automaticallyfrom the Penn Treebank.
Following Clark et al,gold standard dependency structures are obtained foreach derivation by running a dependency-producingparser over the derivations.
The empirical expectedvalue of a feature fi is calculated as follows:Ep?
fi = 1NN?j=1fi(!
j) (9)where !1 : : : !N are the parses in the training data(consisting of a normal-form derivation plus depen-dency structure) and fi(!
j) is the number of times fiappears in parse !
j.2Parameter estimation also requires calculation ofexpected values of the features according to themodel, Ep fi.
This requires summing over all parses(derivation plus dependency structure) for the sen-tences in the data, a difficult task since the total num-ber of parses can grow exponentially with sentencelength.
For some sentences in CCGbank, the parserdescribed in Section 6 produces trillions of parses.The next section shows how a packed chart can ef-ficiently represent the parse space, and how GIS ap-plied to the packed chart can be used to estimate theparameters.4 Packed ChartsGeman and Johnson (2002) have proposed adynamic programming estimation method forpacked representations of unification-based parses.Miyao and Tsujii (2002) have proposed a similarmethod for feature forests which they apply tothe derivations of an automatically extracted Tree-Adjoining Grammar.
We apply Miyao and Tsujii?smethod to the derivations and dependency structuresproduced by our CCG parser.The dynamic programming method relies on apacked chart, in which chart entries of the sametype in the same cell are grouped together, and backpointers to the daughters keep track of how an indi-vidual entry was created.
The intuition behind thedynamic programming is that, for the purposes ofbuilding a dependency structure, chart entries of thesame type are equivalent.
Consider the followingcomposition of will with buy using the forward com-position rule:((S[dcl]will\NP)=NP)((S[dcl]will\NP)=(S[b]\NP)) ((S[b]buy\NP)=NP)The type of the resulting chart entry is deter-mined by the CCG category plus heads, in this case((S[dcl]will\NP)=NP), plus the dependencies yet tobe filled.
The dependencies are not shown, but there2An alternative is to use feature counts from all derivationsleading to the gold standard dependency structure, including thenon-standard derivations, to calculate Ep?
fi.are two subject dependencies on the first NP, oneencoding the subject of will and one encoding thesubject of buy3, and there is an object dependencyon the second NP encoding the object of buy.
En-tries of the same type are identical for the purposesof creating new dependencies for the remainder ofthe parsing.Any rule instantiation4 used by the parser createsboth a set of dependencies and a set of features.
Forthe previous example, one dependency is created:?will; (S[dcl]will\NPX,1)=(S[b]2\NPX); 2; buy?This dependency will be a feature created by the ruleinstantiation.
We also use less specific features, suchas the dependency with the words replaced by POStags.
Section 7 describes the features used.The feature forests of Miyao and Tsujii are de-fined in terms of conjunctive and disjunctive nodes.For our purposes, a conjunctive node is an individualentry in a cell, including the features created whenthe entry was derived, plus pointers to the entry?sdaughters.
A disjunctive node represents an equiva-lence class of nodes in a cell, using the type equiva-lence relation described above.
A conjunctive noderesults from either the combination of two disjunc-tive nodes using a binary rule, e.g.
forward composi-tion; or results from a single disjunctive node usinga unary rule, e.g.
type-raising; or is a leaf node (aword plus lexical category).Features in the model can only result from a sin-gle rule instantiation.
It is possible to define featurescovering a larger part of the dependency structure;for example we might encode all three elements ofthe triple in a PP-attachment as a single feature.
Thedisadvantage of using such features is that this re-duces the efficiency of the dynamic programming.Note, however, that the equivalence relation defin-ing disjunctive nodes takes into account unfilled de-pendencies, which may be long-range dependenciesbeing ?passed up?
the derivation tree.
This meansthat long-range dependencies can be features in ourmodel, even though the lexical items involved maybe far apart in the sentence.3In this example, the co-indexing of heads in the markedupcategory for will ((S[dcl]will\NPX,1)=(S[b]2\NPX)) ensures thesubject dependency for buy is ?passed up?
to the subject NPof the resulting category.4By rule instantiation we mean the local tree arising fromthe application of a CCG combinatory rule.The packed structure we have described is an ex-ample of a feature forest (Miyao and Tsujii, 2002),defined as follows:A feature forest  is a tuple ?C;D;R; ?
; ?
where C is a set of conjunctive nodes; D is a set of disjunctive nodes; R ?
D is a set of root disjunctive nodes;5 ?
: D?
2C is a conjunctive daughter function;  : C ?
2D is a disjunctive daughter function.For each feature function fi : ?
?
N , there isa corresponding feature function fi : C ?
N whichcounts the number of times fi appears on a particularconjunctive node.6 The value of fi for a parse is thenthe sum of the values of fi for each conjunctive nodein the parse.5 Estimation using GISGIS is a very simple algorithm for estimating the pa-rameters of a log-linear model.
The parameters areinitialised to some arbitrary constant and the follow-ing update rule is applied until convergence:(t+1)i = (t)i(Ep?
fiEp(t) fi)1C(10)where (t) is the iteration index and the constant Cis defined as max!
;S?i fi(!).
In practice C is max-imised over the sentences in the training data.
Im-plementations of GIS typically use a ?correction fea-ture?, but following Curran and Clark (2003) we donot use such a feature, which simplifies the algo-rithm.Calculating Ep(t) fi requires summing over allderivations which include fi for each packed chartin the training data.
The key to performing this sumefficiently is to write the sum in terms of inside andoutside scores for each conjunctive node.
The insideand outside scores can be defined recursively, as inthe inside-outside algorithm for PCFGs.
If the insidescore for a conjunctive node c is denoted c, and the5Miyao and Tsujii have a single root conjunctive node; thedisjunctive root nodes we define correspond to the roots of CCGderivations.6The value of fi(c) for c ?
C will typically be 0 or 1, but itis possible for the count to be greater than 1.inside outsidec1d2c6c4c3 c7c5c2d1d3d6c8 c10d4 d5c9Figure 1: Example feature forestoutside score denoted  c, then the expected value offi can be written as follows:7Ep fi =?S?P(S ) 1ZS?c?Csfi(c) c  c (11)where Cs is the set of conjunctive nodes for S .Consider the example feature forest in Figure 1.The figure shows the nodes used to calculate the in-side and outside scores for conjunctive node c5.
Theinside score for a disjunctive node, d, is the sum ofthe inside scores for its conjunctive node daughters:d =?c??
(d)c (12)The inside score for a conjunctive node, c, can thenbe defined recursively:c =?d?(c)d?ifi(c)i (13)The intuition for calculating outside scores is sim-ilar, but a little more involved.
The outside score fora conjunctive node,  c, is the outside score for itsdisjunctive node mother:c =  d where c ?
?
(d) (14)The outside score for a disjunctive node is a sumover the mother nodes, of the product of the outsidescore of the mother, the inside score of the sister, andthe feature weights on the mother.8 For example, the7The notation is taken from Miyao and Tsujii (2002).8Miyao and Tsujii (2002) ignore the feature weights on themother, but this ignores some of the probability mass for theoutside (at least for the feature forests we have defined).outside score of d4 in Figure 1 is the sum of the fol-lowing two values: the product of the outside scoreof c5, the inside score of d5 and the feature weightsat c5; and the product of the outside score of c2, theinside score of d3 and the feature weights at c2.
Therecursive definition is as follows.
The outside scorefor a root disjunctive node is 1, otherwise:d =?{c|d?(c)}?????????c?{d?|d??(c);d?,d}d??ifi(c)i?????????
(15)The normalisation constant ZS is the sum of theinside scores for the root disjunctive nodes:ZS =?dr?Rdr (16)In order to calculate inside scores, the scores fordaughter nodes need to be calculated before thescores for mother nodes (and vice versa for the out-side scores).
This can easily be achieved by orderingthe nodes in the bottom-up CKY parsing order.Note that the inside-outside approach can be com-bined with any maximum entropy estimation proce-dure, such as those evaluated by Malouf (2002).Finally, in order to avoid overfitting, we use aGaussian prior on the parameters of the model (Chenand Rosenfeld, 1999), which requires a slight modi-fication to the update rule in (10).
A Gaussian prioralso handles the problem of ?pseudo-maximal?
fea-tures (Johnson et al, 1999).6 The ParserThe parser is based on Clark et al (2002) and takesas input a POS-tagged sentence with a set of possi-ble lexical categories assigned to each word.
Thesupertagger of Clark (2002) provides the lexical cat-egories, with a parameter setting which assignsaround 4 categories per word on average.
The pars-ing algorithm is the CKY bottom-up chart-parsingalgorithm described in Steedman (2000).
The com-binatory rules used by the parser are functional ap-plication (forward and backward), generalised for-ward composition, backward composition, gener-alised backward-crossed composition, and type rais-ing.
There is also a coordination rule which con-joins categories of the same type.
Restrictions areplaced on some of the rules, such as that given bySteedman (2000, p.62) for backward-crossed com-position.Type-raising is applied to the categories NP, PPand S[adj]\NP (adjectival phrase), and is imple-mented by adding the relevant set of type-raisedcategories to the chart whenever an NP, PP orS[adj]\NP is present.
The sets of type-raised cate-gories are based on the most commonly used type-raising rule instantiations in sections 2-21 of CCG-bank, and contain 8 type-raised categories for NPand 1 each for PP and S[adj]\NP.The parser also uses a number of lexical rules andpunctuation rules.
These rules are based on thoseoccurring roughly more than 200 times in sections2-21 of CCGbank.
An example of a lexical rule usedby the parser is the following, which takes a passiveform of a verb and creates a nominal modifier:S[pss]\NP ?
NPX\NPX,1 (17)This rule is used to create NPs such as the roleplayed by Kim Cattrall.
Note that there is a de-pendency relation on the resulting category; in theprevious example role would fill a nominal modifierdependency headed by played.Currently, the only punctuation marks handled bythe parser are commas, and all other punctuation isremoved after the supertagging phase.
An exampleof a comma rule is the following:SX=S X ; ?
SX=S X (18)This rule takes a sentential modifier followed bya comma (for example Currently , in the sentenceabove in the text) and returns a sentential modifierof the same type.The next section describes the efficient implemen-tation of the parser and model estimator.7 Implementation7.1 Parser ImplementationThe non-standard derivations allowed by CCG, to-gether with the wide coverage grammar, result inextremely large charts.
This means that efficient im-plementation of the parsing process is imperative forperforming large-scale experiments.The packed chart prevents combinatorial explo-sion in the number of category combinations bygrouping equivalent categories into a single entry.The speed of the parser is heavily dependent on theefficiency of equivalence testing, and category uni-fication and construction.
These are performed effi-ciently by always creating categories in a canonicalform which can then be compared rapidly using hashfunctions over categories.The parser produces a packed chart from whichthe most probable dependency structure can be re-covered.
Since the same dependency structure canbe generated by more than one derivation, a depen-dency structure?s score is the sum of the log-linearscores for each derivation.
Finding the structurewith the highest score is not trivial, since filled de-pendencies are only stored at the conjunctive nodeswhere they are created.
This means that a depen-dency appearing in a structure can be created in dif-ferent parts of the chart for different derivations.
Wesolve this in practice using a hash function over de-pendencies, which can be used to quickly determinewhether two derivations lead to the same structure.For each node in the chart, we can keep track of thederivation leading to the set of dependencies withthe highest score for that node.7.2 Data GenerationData for model estimation is created in two steps.First, the parser is run over the normal-form deriva-tions in Sections 2-21 of CCGbank outputting thecorresponding dependencies and other features.
Thefeatures used in our preliminary implementation areas follows: dependency features; lexical category features; root category features.Dependency features are 5-tuples as defined inSection 2.
Further dependency features are formedby substituting POS tags for the words, which leadsto a total of 4 features for each dependency.
Lexicalcategory features are word category pairs on the leafnodes and root features are head-word category pairson root nodes.
Extra features are formed by replac-ing words with their POS tags.
The total number offeatures is 817,658, but we reduce this to 243,603 byonly including features which appear at least twicein the data.The second step of data generation involves usingthe parser to create a feature forest for each sentence,using the feature set extracted from CCGbank.
Theparser is interrupted if a sentence takes longer than60 seconds to process or if more than 500,000 con-junctive nodes are created in the chart.
If this oc-curs, the process is repeated but with a smaller num-ber of categories assigned to each word by the su-pertagger.
Approximately 93% of the sentences insections 2-21 can be processed in this way, giving36,400 training sentences.
Creating the forests takesapproximately one hour using 40 nodes of our Be-owulf cluster, and produces 19.9 GB of data.7.3 EstimationThe parse forests regularly represent trillions of pos-sible parses for a sentence.
The estimation pro-cess involves summing feature weights over all theseparses, a total which cannot be represented usingdouble precision arithmetic (limited to less than10308).
Our implementation uses the sum, ratherthan product, form of (6), so that logarithms can beused to avoid numerical overflow.
For converting thesum of products in Equation 15 to log space, we usea technique commonly used in speech recognition(p.c.
Simon King).We have implemented a parallel version of ourGIS code using the MPICH library (Gropp et al,1996), an open-source implementation of the Mes-sage Passing Interface (MPI) standard.
MPI parallelprogramming involves explicit synchronisation andinformation transfer between the parallel processesusing messages.
It is ideal for development of paral-lel programs for cluster architectures.GIS over parse forests is straightforward to par-allelise.
The parse forests are divided among themachines in the cluster (in our current implemen-tation, each machine receives 979 forests).
Eachmachine calculates the inside and outside scores foreach node in the parse forest and updates the es-timated feature expectations.
The feature expecta-tions are then summed across all of the machinesusing a global operation (called a reduce operation).Every machine receives this sum which is then usedto calculate the normal GIS weight update.
In ourpreliminary tests, each process used approximately750 MB of RAM, giving a total usage of 30 GB acrossthe cluster.
One iteration of GIS takes approximatelyGIS - CCG IIS - TAGnumber of features 243,603 5,715number of sentences 36,400 868avg.
num.
of nodes 52,000 17,412memory usage 30 GB 1.5 GBdisk usage 19.9 GB ?Table 1: Results compared with Miyao and Tsujii1 minute.
Given the large number of features, weestimate at least 1,000 iterations will be needed forconvergence.8 Conclusions and Further WorkTable 1 gives the overall statistics for the modelestimation process, and compares them withMiyao and Tsujii (2002).
These numbers representthe largest-scale parsing model of which we areaware.
Parsing and model estimation on this scaleintroduce a number of interesting theoretical andcomputational challenges.
We have demonstratedhow packed charts and feature forests can be com-bined to meet the theoretical challenges.
We havealso described an MPI implementation of GIS whichsolves the computational challenges.
These tech-niques are necessary for discriminative estimationtechniques applied to wide-coverage parsing.We have just begun the process of evaluatingparsing performance using the same test data asClark et al (2002).
We are especially interested inthe effectiveness of incorporating long-range depen-dencies as features, which CCG was designed to han-dle and for which we expect a log-linear model to beparticularly effective.AcknowledgementsWe would like to thank Mark Steedman, Julia Hock-enmaier, Jason Baldridge, David Chiang, YusukeMiyao, Mark Johnson, Yuval Krymolowski, TaraMurphy and the anonymous reviewers for their help-ful comments.
This research is supported by EPSRCgrant GR/M96889, and a Commonwealth scholar-ship and a Sydney University Travelling scholarshipto the second author.ReferencesSteven Abney.
1997.
Stochastic attribute-value gram-mars.
Computational Linguistics, 23(4):597?618.Adam Berger, Stephen Della Pietra, and Vincent DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguistics,22(1):39?71.Stanley Chen and Ronald Rosenfeld.
1999.
A Gaussianprior for smoothing maximum entropy models.
Tech-nical report, Carnegie Mellon University, Pittsburgh,PA.Stephen Clark, Julia Hockenmaier, and Mark Steedman.2002.
Building deep dependency structures with awide-coverage CCG parser.
In Proceedings of the 40thMeeting of the ACL, pages 327?334, Philadelphia, PA.Stephen Clark.
2002.
A supertagger for combinatory cat-egorial grammar.
In Proceedings of the TAG+ Work-shop, pages 19?24, Venice, Italy.James R. Curran and Stephen Clark.
2003.
Investigat-ing GIS and smoothing for maximum entropy taggers.In Proceedings of the 10th Meeting of the EACL (toappear), Budapest, Hungary.J.
N. Darroch and D. Ratcliff.
1972.
Generalized it-erative scaling for log-linear models.
The Annals ofMathematical Statistics, 43(5):1470?1480.Stephen Della Pietra, Vincent Della Pietra, and JohnLafferty.
1997.
Inducing features of random fields.IEEE Transactions Pattern Analysis and Machine In-telligence, 19(4):380?393.Stuart Geman and Mark Johnson.
2002.
Dynamicprogramming for parsing and estimation of stochas-tic unification-based grammars.
In Proceedings of the40th Meeting of the ACL, pages 279?286, Philadel-phia, PA.W.
Gropp, E. Lusk, N. Doss, and A. Skjellum.
1996.A high-performance, portable implementation of theMPI message passing interface standard.
ParallelComputing, 22(6):789?828, September.Julia Hockenmaier and Mark Steedman.
2002.
Gener-ative models for statistical parsing with CombinatoryCategorial Grammar.
In Proceedings of the 40th Meet-ing of the ACL, pages 335?342, Philadelphia, PA.Julia Hockenmaier.
2003.
Data and Models for Statis-tical Parsing with Combinatory Categorial Grammar.Ph.D.
thesis, University of Edinburgh.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,and Stefan Riezler.
1999.
Estimators for stochastic?unification-based?
grammars.
In Proceedings of the37th Meeting of the ACL, pages 535?541, Universityof Maryland, MD.K.
Lari and S. J.
Young.
1990.
The estimation of stochas-tic context-free grammars using the inside-outside al-gorithm.
Computer Speech and Language, 4(1):35?56.Robert Malouf.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
In Pro-ceedings of the Sixth Workshop on Natural LanguageLearning, pages 49?55, Taipei, Taiwan.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximum en-tropy estimation for feature forests.
In Proceedingsof the Human Language Technology Conference, SanDiego, CA.Miles Osborne.
2000.
Estimation of stochasticattribute-value grammars using an informative sam-ple.
In Proceedings of the 18th International Con-ference on Computational Linguistics, pages 586?592,Saarbru?cken, Germany.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell III, and Mark John-son.
2002.
Parsing the Wall Street Journal using aLexical-Functional Grammar and discriminative esti-mation techniques.
In Proceedings of the 40th Meet-ing of the ACL, pages 271?278, Philadelphia, PA.Ronald Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modeling.
Computer,Speech and Language, 10:187?228.Mark Steedman.
2000.
The Syntactic Process.
The MITPress, Cambridge, MA.Kristina Toutanova, Christopher Manning, StuartShieber, Dan Flickinger, and Stephan Oepen.
2002.Parse disambiguation for a rich HPSG grammar.
InProceedings of the First Workshop on Treebanksand Linguistic Theories, pages 253?263, Sozopol,Bulgaria.
