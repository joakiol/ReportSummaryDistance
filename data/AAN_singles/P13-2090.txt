Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 505?510,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsExploring Sentiment in Social Media: Bootstrapping Subjectivity Cluesfrom Multilingual Twitter StreamsSvitlana VolkovaCLSPJohns Hopkins UniversityBaltimore, MDsvitlana@jhu.eduTheresa WilsonHLTCOEJohns Hopkins UniversityBaltimore, MDtaw@jhu.eduDavid YarowskyCLSPJohns Hopkins UniversityBaltimore, MDyarowsky@cs.jhu.eduAbstractWe study subjective language in socialmedia and create Twitter-specific lexi-cons via bootstrapping sentiment-bearingterms from multilingual Twitter streams.Starting with a domain-independent, high-precision sentiment lexicon and a largepool of unlabeled data, we bootstrapTwitter-specific sentiment lexicons, us-ing a small amount of labeled data toguide the process.
Our experiments onEnglish, Spanish and Russian show thatthe resulting lexicons are effective forsentiment classification for many under-explored languages in social media.1 IntroductionThe language that people use to express opinionsand sentiment is extremely diverse.
This is true forwell-formed data, such as news and reviews, andit is particularly true for data from social media.Communication in social media is informal, ab-breviations and misspellings abound, and the per-son communicating is often trying to be funny,creative, and entertaining.
Topics change rapidly,and people invent new words and phrases.The dynamic nature of social media togetherwith the extreme diversity of subjective languagehas implications for any system with the goalof analyzing sentiment in this domain.
General,domain-independent sentiment lexicons have lowcoverage.
Even models trained specifically on so-cial media data may degrade somewhat over timeas topics change and new sentiment-bearing termscrop up.
For example, the word ?occupy?
wouldnot have been indicative of sentiment before 2011.Most of the previous work on sentiment lexiconconstruction relies on existing natural languageprocessing tools, e.g., syntactic parsers (Wiebe,2000), information extraction (IE) tools (Riloffand Wiebe, 2003) or rich lexical resources suchas WordNet (Esuli and Sebastiani, 2006).
How-ever, such tools and lexical resources are not avail-able for many languages spoken in social media.While English is still the top language in Twitter,it is no longer the majority.
Thus, the applicabil-ity of these approaches is limited.
Any method foranalyzing sentiment in microblogs or other socialmedia streams must be easily adapted to (1) manylow-resource languages, (2) the dynamic nature ofsocial media, and (3) working in a streaming modewith limited or no supervision.Although bootstrapping has been used for learn-ing sentiment lexicons in other domains (Turneyand Littman, 2002; Banea et al, 2008), it has notyet been applied to learning sentiment lexicons formicroblogs.
In this paper, we present an approachfor bootstrapping subjectivity clues from Twitterdata, and evaluate our approach on English, Span-ish and Russian Twitter streams.
Our approach:?
handles the informality, creativity and the dy-namic nature of social media;?
does not rely on language-dependent tools;?
scales to the hundreds of new under-exploredlanguages and dialects in social media;?
classifies sentiment in a streaming mode.To bootstrap subjectivity clues from Twitterstreams we rely on three main assumptions:i. sentiment-bearing terms of similar orienta-tion tend to co-occur at the tweet level (Tur-ney and Littman, 2002);ii.
sentiment-bearing terms of opposite orienta-tion do not co-occur at the tweet level (Ga-mon and Aue, 2005);iii.
the co-occurrence of domain-specific anddomain-independent subjective terms servesas a signal of subjectivity.5052 Related WorkMihalcea et.al (2012) classifies methods for boot-strapping subjectivity lexicons into two types:corpus-based and dictionary-based.Dictionary-based methods rely on existing lex-ical resources to bootstrap sentiment lexicons.Many researchers have explored using relations inWordNet (Miller, 1995), e.g., Esuli and Sabastiani(2006), Andreevskaia and Bergler (2006) for En-glish, Rao and Ravichandran (2009) for Hindi andFrench, and Perez-Rosas et al (2012) for Spanish.Mohammad et al (2009) use a thesaurus to aidin the construction of a sentiment lexicon for En-glish.
Other works (Clematide and Klenner, 2010;Abdul-Mageed et al, 2011) automatically expandsand evaluates German and Arabic lexicons.
How-ever, the lexical resources that dictionary-basedmethods need, do not yet exist for the majority oflanguages in social media.
There is also a mis-match between the formality of many language re-sources, such as WordNet, and the extremely in-formal language of social media.Corpus-based methods extract subjectivity andsentiment lexicons from large amounts of unla-beled data using different similarity metrics tomeasure the relatedness between words.
Hatzivas-siloglou and McKeown (1997) were the first to ex-plore automatically learning the polarity of wordsfrom corpora.
Early work by Wiebe (2000) iden-tifies clusters of subjectivity clues based on theirdistributional similarity, using a small amount ofdata to bootstrap the process.
Turney (2002) andVelikovich et al (2010) bootstrap sentiment lexi-cons for English from the web by using PointwiseMutual Information (PMI) and graph propaga-tion approach, respectively.
Kaji and Kitsuregawa(2007) propose a method for building sentimentlexicon for Japanese from HTML pages.
Baneaet al (2008) experiment with Lexical SemanticAnalysis (LSA) (Dumais et al, 1988) to bootstrapa subjectivity lexicon for Romanian.
Kanayamaand Nasukawa (2006) bootstrap subjectivity lexi-cons for Japanese by generating subjectivity can-didates based on word co-occurrence patterns.In contrast to other corpus-based bootstrappingmethods, we evaluate our approach on multiplelanguages, specifically English, Spanish, and Rus-sian.
Also, as our approach relies only on theavailability of a bilingual dictionary for translatingan English subjectivity lexicon and crowdsourcingfor help in selecting seeds, it is more scalable andbetter able to handle the informality and the dy-namic nature of social media.
It also can be effec-tively used to bootstrap sentiment lexicons for anylanguage for which a bilingual dictionary is avail-able or can be automatically induced from parallelcorpora.3 DataFor the experiments in this paper, we use threesets of data for each language: 1M unlabeledtweets (BOOT) for bootstrapping Twitter-specificlexicons, 2K labeled tweets for development data(DEV), and 2K labeled tweets for evaluation(TEST).
DEV is used for parameter tuning whilebootstrapping, and TEST is used to evaluating thequality of the bootstrapped lexicons.We take English tweets from the corpus con-structed by Burger et al (2011) which con-tains 2.9M tweets (excluding retweets) from 184Kusers.1 English tweets are identified automati-cally using a compression-based language identifi-cation (LID) tool (Bergsma et al, 2012).
Accord-ing to LID, there are 1.8M (63.6%) English tweets,which we randomly sample to create BOOT, DEVand TEST sets for English.
Unfortunately, Burger?scorpus does not include Russian and Spanish dataon the same scale as English.
Therefore, forother languages we construct a new Twitter corpusby downloading tweets from followers of region-specific news and media feeds.Sentiment labels for tweets in DEV and TESTsets for all languages are obtained using AmazonMechanical Turk.
For each tweet we collect an-notations from five workers and use majority voteto determine the final label for the tweet.
Snowet al (2008) show that for a similar task, labelingemotion and valence, on average four non-expertlabelers are needed to achieve an expert level ofannotation.
Table 1 gives the distribution of tweetsover sentiment labels for the development and testsets for English (E-DEV, E-TEST), Spanish (S-DEV, S-TEST), and Russian (R-DEV, R-TEST).Below are examples of tweets in Russian with En-glish translations labeled with sentiment:?
Positive: ?
??????
???????
????????
????
???????
(Planning for deliciousbreakfast and lots of movies);?
Negative: ????
???????
?, ?
?
???
??????
(I want to die and I will do that);1They provided the tweet IDs, and we used the TwitterCorpus Tools to download the tweets.506Data Positive Neg Both NeutralE-DEV 617 357 202 824E-TEST 596 347 195 862S-DEV 358 354 86 1,202S-TEST 317 387 93 1203R-DEV 452 463 156 929R-TEST 488 380 149 983Table 1: Sentiment label distribution in develop-ment DEV and test TEST datasets across languages.?
Both: ???????
????????
??????
????????
??
??
????.
????
??????
????-??
(I want to write about the movie rougherbut I will not.
Although the actors are good);?
Neutral: ??????
?????
?????
??????????????
??????
(Why clever thoughts comeonly at night?
).4 Lexicon BootstrappingTo create a Twitter-specific sentiment lexicon fora given language, we start with a general-purpose,high-precision sentiment lexicon2 and bootstrapfrom the unlabeled data (BOOT) using the labeleddevelopment data (DEV) to guide the process.4.1 High-Precision Subjectivity LexiconsFor English we seed the bootstrapping pro-cess with the strongly subjective terms from theMPQA lexicon3 (Wilson et al, 2005).
Theseterms have been previously shown to be high-precision for recognizing subjective sentences(Riloff and Wiebe, 2003).For the other languages, the subjective seedterms are obtained by translating English seedterms using a bilingual dictionary, and then col-lecting judgments about term subjectivity fromMechanical Turk.
Terms that truly are stronglysubjective in translation are used for seed termsin the new language, with term polarity projectedfrom the English.
Finally, we expand the lexiconswith plurals and inflectional forms for adverbs, ad-jectives and verbs.4.2 Bootstrapping ApproachTo bootstrap, first the new lexicon LB(0) is seededwith the strongly subjective terms from the orig-inal lexicon LI .
On each iteration i ?
1, tweetsin the unlabeled data are labeled using the lexicon2Other works on generating domain-specific sentimentlexicons e.g., from blog data (Jijkoun et al, 2010) also startwith a general, domain-specific lexicon.3http://www.cs.pitt.edu/mpqa/from the previous iteration, LB(i?1).
If a tweetcontains one or more terms from LB(i?1) it is con-sidered subjective, otherwise objective.
The polar-ity of subjective tweets is determined in a similarway: if the tweet contains ?
1 positive terms, tak-ing into account the negation, it is considered neg-ative; if it contains ?
1 negative terms, taking intoaccount the negation, it is considered positive.4 Ifit contains both positive and negative terms, it isconsidered to be both.
Then, for every term not inLB(i?1) that has a frequency ?
?freq, the proba-bility of that term being subjective is calculated asshown in Algorithm 1 line 10.
The top ?k termswith a subjective probability ?
?pr are then addedto LB(i).
The polarity of new terms is determinedbased on the probability of the term appearing inpositive or negative tweets as shown in line 18.5The bootstrapping process terminates when thereare no more new terms meeting the criteria to add.Algorithm 1 BOOTSTRAP (?, ?pr, ?freq, ?topK )1: iter = 0, ?
= 0.5, LB(~?)?
LI(?
)2: while (stop 6= true) do3: LiterB (~?)?
?,?LiterB (~?)?
?4: for each new term w ?
{V \ LB(~?)}
do5: for each tweet t ?
T do6: if w ?
t then7: UPDATE c(w,LB(~?
)), c(w,LposB (~?
)), c(w)8: end if9: end for10: psubj(w)?
c(w,LB(~?
))c(w)11: ppos(w)?
c(w,LposB (~?))c(w,LB(~?
))12: LiterB (~?)?
w, psubj(w), ppol(w)13: end for14: SORT LiterB (~?)
by psubj(w)15: while (K ?
?topK) do16: for each new term w ?
LiterB (~?)
do17: if [psubj(w) ?
?pr and cw ?
?freq then18: if [ppos(w) ?
0.5] then19: wpol ?
positive20: else21: wpol ?
negative22: end if23: ?LiterB (~?)?
?LiterB (~?)
+ wpol24: end if25: end for26: K = K + 127: end while28: if [?LiterB (~?)
== 0] then29: stop?
true30: end if31: LB(~?)?
LB(~?)
+ ?LiterB (~?
)32: iter = iter + 133: end while4If there is a negation in the two words before a sentimentterm, we flip its polarity.5Polarity association probabilities should sum up to 1ppos(w|LB(~?))
+ pneg(w|LB(~?))
= 1.507English Spanish RussianLEI LEB LSI LSB LRI LRBPos 2.3 16.8 2.9 7.7 1.4 5.3Neg 2.8 4.7 5.2 14.6 2.3 5.5Total 5.1 21.5 8.1 22.3 3.7 10.8Table 2: The original and the bootstrapped (high-lighted) lexicon term count (LI ?
LB) with polar-ity across languages (thousands).The set of parameters ~?
is optimized using a gridsearch on the development data using F-measurefor subjectivity classification.
As a result, for En-glish ~?
= [0.7, 5, 50] meaning that on each itera-tion the top 50 new terms with a frequency ?
5and probability ?
0.7 are added to the lexicon.For Spanish, the set of optimal parameters ~?
=[0.65, 3, 50] and for Russian - ~?
= [0.65, 3, 50].
InTable 2 we report size and term polarity from theoriginal LI and the bootstrapped LB lexicons.5 Lexicon EvaluationsWe evaluate our bootstrapped sentiment lexiconsEnglish LEB , Spanish LSB and Russian LRB by com-paring them with existing dictionary-expandedlexicons that have been previously shown to be ef-fective for subjectivity and polarity classification(Esuli and Sebastiani, 2006; Perez-Rosas et al,2012; Chetviorkin and Loukachevitch, 2012).
Forthat we perform subjectivity and polarity classifi-cation using rule-based classifiers6 on the test dataE-TEST, S-TEST and R-TEST.We consider how the various lexicons performfor rule-based classifiers for both subjectivity andpolarity.
The subjectivity classifier predicts thata tweet is subjective if it contains a) at least one,or b) at least two subjective terms from the lexi-con.
For the polarity classifier, we predict a tweetto be positive (negative) if it contains at least onepositive (negative) term taking into account nega-tion.
If the tweet contains both positive and nega-tive terms, we take the majority label.For English we compare our bootstrapped lex-icon LEB against the original lexicon LEI andstrongly subjective terms from SentiWordNet 3.0(Esuli and Sebastiani, 2006).
To make a faircomparison, we automatically expand SentiWord-Net with noun plural forms and verb inflectionalforms.
In Figure 1 we report precision, recall6Similar approach to a rule-based classification usingterms from he MPQA lexicon (Riloff and Wiebe, 2003).and F-measure results.
They show that our boot-strapped lexicon significantly outperforms Senti-WordNet for subjectivity classification.
For polar-ity classification we get comparable F-measure butmuch higher recall for LEB compared to SWN .
(a) Subj ?
1 (b) Subj ?
2 (c) PolarityLexicon Fsubj?1 Fsubj?2 FpolaritySWN 0.57 0.27 0.78LEI 0.71 0.48 0.82LEB 0.75 0.72 0.78Figure 1: Precision (x-axis), recall (y-axis) andF-measure (in the table) for English: LEI = ini-tial lexicon, LEB = bootstrapped lexicon, SWN =strongly subjective terms from SentiWordNet.For Spanish we compare our bootstrapped lex-icon LSB against the original LSI lexicon, and thefull and medium strength terms from the Span-ish sentiment lexicon constructed by Perez-Rosaset el.
(2012).
We report precision, recall and F-measure in Figure 2.
We observe that our boot-strapped lexicon yields significantly better perfor-mance for subjectivity classification compared toboth full and medium strength terms.
However,our bootstrapped lexicon yields lower recall andsimilar precision for polarity classification.
(a) Subj ?
1 (b) Subj ?
2 (c) PolarityLexicon Fsubj?1 Fsubj?2 FpolaritySM 0.44 0.17 0.64SF 0.47 0.13 0.66LSI 0.59 0.45 0.58LSB 0.59 0.59 0.55Figure 2: Precision (x-axis), recall (y-axis) and F-measure (in the table) for Spanish: LSI = initiallexicon, LSB = bootstrapped lexicon, SF = fullstrength terms; SM = medium strength terms.508For Russian we compare our bootstrapped lex-icon LRB against the original LRI lexicon, and theRussian sentiment lexicon constructed by Chetv-iorkin and Loukachevitchet (2012).
The externallexicon in Russian P was built for the domainof product reviews and does not include polarityjudgments for subjective terms.
As before, weexpand the external lexicon with the inflectionalforms for adverbs, adjectives and verbs.
We reportresults for Russian in Figure 3.
We find that forsubjectivity our bootstrapped lexicon shows betterperformance compared to the external lexicon (5kterms).
However, the expanded external lexicon(17k terms) yields higher recall with a significantdrop in precision.
Note that for Russian, we reportpolarity classification results for LRB and LRI lexi-cons only because P does not have polarity labels.
(a) Subj ?
1 (b) Subj ?
2 (c) PolarityLexicon Fsubj?1 Fsubj?2 FpolarityP 0.55 0.29 ?PX 0.62 0.47 ?LRI 0.46 0.13 0.73LRB 0.61 0.35 0.73Figure 3: Precision (x-axis), recall (y-axis) and F-measure for Russian: LRI = initial lexicon, LRB =bootstrapped lexicon, P = external sentiment lex-icon, PX = expanded external lexicon.We next perform error analysis for subjectiv-ity and polarity classification for all languages andidentify common errors to address them in future.For subjectivity classification we observe thatapplying part-of-speech tagging during the boot-strapping could improve results for all languages.We could further improve the quality of the lex-icon and reduce false negative errors (subjec-tive tweets classified as neutral) by focusing onsentiment-bearing terms such as adjective, adverbsand verbs.
However, POS taggers for Twitter areonly available for a limited number of languagessuch as English (Gimpel et al, 2011).
Other falsenegative errors are often caused by misspellings.77For morphologically-rich languages, our approach cov-ers different linguistic forms of terms but not their mis-spellings.
However, it can be fixed by an edit-distance check.We also find subjective tweets with philosophi-cal thoughts and opinions misclassified, especiallyin Russian, e.g., ??????
??
??????
??
???????
??????????
????????
?????
??
???
???-??
???
??
???????
??
????????
(Sometimes weare not ready to fulfill our dreams yet but, at thesame time, we do not want to scare them).
Suchtweets are difficult to classify using lexicon-basedapproaches and require deeper linguistic analysis.False positive errors for subjectivity classifica-tion happen because some terms are weakly sub-jective and can be used in both subjective andneutral tweets e.g., the Russian term ??????????
(brag) is often used as subjective, but in a tweet???????
??
?????
??????????
???????
(neverbrag about your future) it is used as neutral.
Simi-larly, the Spanish term buenas (good) is often usedsubjectively but it is used as neutral in the follow-ing tweet ?
@Diveke me falto el buenas!
jaja queonda que ha pasado?
(I miss the good times wehad, haha that wave has passed!
).For polarity classification, most errors happenbecause our approach relies on either positive ornegative polarity scores for a term but not both.8However, in the real world terms may sometimeshave both usages.
Thus, some tweets are misclas-sified (e.g., ?It is too warm outside?).
We canfix this by summing over weighted probabilitiesrather than over term counts.
Additional errorshappen because tweets are very short and conveymultiple messages (e.g., ?What do you mean byunconventional?
Sounds exciting!?)
Thus, our ap-proach can be further improved by adding wordsense disambiguation and anaphora resolution.6 ConclusionsWe propose a scalable and language independentbootstrapping approach for learning subjectivityclues from Twitter streams.
We demonstrate theeffectiveness of the bootstrapping procedure bycomparing the resulting subjectivity lexicons withstate-of the-art sentiment lexicons.
We performerror analysis to address the most common errortypes in the future.
The results confirm that theapproach can be effectively exploited and furtherimproved for subjectivity classification for manyunder-explored languages in social media.8During the bootstrapping we calculate probability for aterm to be positive and negative, e.g., p(warm|+) = 0.74and p(warm|?)
= 0.26.
But during polarity classificationwe rely on the highest probability score and consider it to be?the polarity?
for the term e.g., positive for warm.509ReferencesMuhammad Abdul-Mageed, Mona T. Diab, and Mo-hammed Korayem.
2011.
Subjectivity and senti-ment analysis of modern standard arabic.
In Pro-ceedings of ACL/HLT.Alina Andreevskaia and Sabine Bergler.
2006.
Min-ing wordnet for fuzzy sentiment: Sentiment tag ex-traction from WordNet glosses.
In Proceedings ofEACL.Carmen Banea, Rada Mihalcea, and Janyce Wiebe.2008.
A bootstrapping method for building subjec-tivity lexicons for languages with scarce resources.In Proceedings of LREC.Shane Bergsma, Paul McNamee, Mossaab Bagdouri,Clayton Fink, and Theresa Wilson.
2012.
Languageidentification for creating language-specific Twittercollections.
In Proceedings of 2nd Workshop onLanguage in Social Media.John D. Burger, John C. Henderson, George Kim, andGuido Zarrella.
2011.
Discriminating gender onTwittier.
In Proceedings of EMNLP.Ilia Chetviorkin and Natalia V. Loukachevitch.
2012.Extraction of Russian sentiment lexicon for productmeta-domain.
In Proceedings of COLING.Simon Clematide and Manfred Klenner.
2010.
Eval-uation and extension of a polarity lexicon for Ger-man.
In Proceedings of the 1st Workshop on Com-putational Approaches to Subjectivity and SentimentAnalysis.Susan T. Dumais, George W. Furnas, Thomas K. Lan-dauer, Scott Deerwester, and Richard Harshman.1988.
Using latent semantic analysis to improveaccess to textual information.
In Proceedings ofSIGCHI.Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-WordNet: A publicly available lexical resource foropinion mining.
In Proceedings of LREC.Michael Gamon and Anthony Aue.
2005.
Automaticidentification of sentiment vocabulary: exploitinglow association with known sentiment terms.
InProceedings of the ACL Workshop on Feature Engi-neering for Machine Learning in Natural LanguageProcessing.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor Twittier: annotation, features, and experiments.In Proceedings of ACL.Vasileios Hatzivassiloglou and Kathy McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proceedings of ACL.Valentin Jijkoun, Maarten de Rijke, and WouterWeerkamp.
2010.
Generating focused topic-specific sentiment lexicons.
In Proceedings of ACL.Nobuhiro Kaji and Masaru Kitsuregawa.
2007.
Build-ing lexicon for sentiment analysis from massivecollection of html documents.
In Proceedings ofEMNLP.Hiroshi Kanayama and Tetsuya Nasukawa.
2006.Fully automatic lexicon expansion for domain-oriented sentiment analysis.
In Proceedings ofEMNLP.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.2012.
Multilingual subjectivity and sentiment anal-ysis.
In Proceedings of ACL.George A. Miller.
1995.
Wordnet: a lexical databasefor English.
Communications of the ACM, 38(11).Saif Mohammad, Cody Dunne, and Bonnie Dorr.2009.
Generating high-coverage semantic orienta-tion lexicons from overtly marked words and a the-saurus.
In Proceedings of EMNLP.Veronica Perez-Rosas, Carmen Banea, and Rada Mi-halcea.
2012.
Learning sentiment lexicons in Span-ish.
In Proceedings of LREC.Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
In Proceed-ings of EACL.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceed-ings of EMNLP.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast ?
but is itgood?
: Evaluating non-expert annotations for natu-ral language tasks.
In Proceedings of EMNLP.Peter D. Turney and Michael L. Littman.
2002.
Un-supervised learning of semantic orientation from ahundred-billion-word corpus.
Computing ResearchRepository.Peter D. Turney.
2002.
Thumbs up or thumbs down?
:Semantic orientation applied to unsupervised classi-fication of reviews.
In Proceedings of ACL.Leonid Velikovich, Sasha Blair-Goldensohn, KerryHannan, and Ryan McDonald.
2010.
The viabil-ity of web-derived polarity lexicons.
In Proceedingsof NAACL.Janyce Wiebe.
2000.
Learning subjective adjectivesfrom corpora.
In Proceedings of AAAI.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of EMNLP.510
