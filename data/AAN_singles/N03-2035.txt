A Context-Sensitive Homograph Disambiguationin Thai Text-to-Speech SynthesisVirongrong Tesprasit, Paisarn Charoenpornsawat and Virach SornlertlamvanichInformation Research and Development DivisionNational Electronics and Computer Technology Center112 Phahon Yohtin, Rd., Klong 1, Klong Luang, Pathumthani 12120 THAILAND{virong, paisarn, virach}@nectec.or.thAbstractHomograph ambiguity is an original issue inText-to-Speech (TTS).
To disambiguatehomograph, several efficient approaches havebeen proposed such as part-of-speech (POS)n-gram, Bayesian classifier, decision tree, andBayesian-hybrid approaches.
These methodsneed words or/and POS tags surrounding thequestion homographs in disambiguation.Some languages such as Thai, Chinese, andJapanese have no word-boundary delimiter.Therefore before solving homograph ambigu-ity, we need to identify word boundaries.
Inthis paper, we propose a unique frameworkthat solves both word segmentation andhomograph ambiguity problems altogether.Our model employs both local and long-distance contexts, which are automatically ex-tracted by a machine learning technique calledWinnow.1 IntroductionIn traditional Thai TTS, it consists of four main mod-ules: word segmentation, grapheme-to-phoneme, pros-ody generation, and speech signal processing.
Theaccuracy of pronunciation in Thai TTS mainly dependson accuracies of two modules: word segmentation, andgrapheme-to-phoneme.
In word segmentation process, ifword boundaries cannot be identified correctly, it leadsThai TTS to the incorrect pronunciation such as a string???????
which can be separated into two different wayswith different meanings and pronunciations.
The firstone is ???
(eye)  ???
(round)?, pronounced [ta:0 klom0]and the other one is ????
(expose) ??
(wind)?, pro-nounced [ta:k1 lom0].
In grapheme-to-phoneme mod-ule, it may produce error pronunciations for ahomograph which can be pronounced more than oneway such as a word ??????
which can be pronounced[phlaw0] or [phe:0 la:0].
Therefore, to improve an accu-racy of Thai TTS, we have to focus on solving the prob-lems of word boundary ambiguity and homographambiguity which can be viewed as a disambiguationtask.A number of feature-based methods have been triedfor several disambiguation tasks in NLP, including deci-sion lists, Bayesian hybrids, and Winnow.
These meth-ods are superior to the previously proposed methods inthat they can combine evidence from various sources indisambiguation.
To apply the methods in our task, wetreat problems of word boundary and homograph ambi-guity as a task of word pronunciation disambiguation.This task is to decide using the context which was actu-ally intended.
Instead of using only one type of syntacticevidence as in N-gram approaches, we employ the syn-ergy of several types of features.
Following previousworks [4, 6], we adopted two types of features: contextwords, and collections.
Context-word feature is used totest for the presence of a particular word within +/- Kwords of the target word and collocation test for a pat-tern of up to L contiguous words and/or part-of-speechtags surrounding the target word.
To automatically ex-tract the discriminative features from feature space andto combine them in disambiguation, we have to investi-gate an efficient technique in our task.The problem becomes how to select and combinevarious kinds of features.
Yarowsky [11] proposed deci-sion list as a way to pool several types of features, andto solve the target problem by applying a single strong-est feature, whatever type it is.
Golding [3] proposed aBayesian hybrid method to take into account all avail-able evidence, instead of only the strongest one.
Themethod was applied to the task of context-sentitivespelling correction and was reported to be superior todecision lists.
Later, Golding and Roth [4] applied Win-now algorithm in the same task and found that the algo-rithm performs comparably to the Bayesian hybridmethod when using pruned feature sets, and is betterwhen using unpruned sets or unfamiliar test set.In this paper, we propose a unified framework insolving the problems of word boundary ambiguity andhomograph ambiguity altogether.
Our approach em-ploys both local and long-distance contexts, which canbe automatically extracted by a machine learning tech-nique.
In this task, we employ the machine learningtechnique called Winnow.
We then construct our systembased on the algorithm and evaluate them by comparingwith other existing approaches to Thai homograph prob-lems.22.12.234Problem DescriptionIn Thai TTS, there are two major types of text ambigui-ties which lead to incorrect pronunciation, namely wordboundary ambiguity and homograph ambiguity.Word Boundary Ambiguity (WBA)Thai as well as some other Asian languages has no wordboundary delimiter.
Identifying word boundary, espe-cially in Thai, is a fundamental task in Natural Lan-guage Processing (NLP).
However, it is not a simpleproblem because many strings can be segmented intowords in different ways.
Word boundary ambiguities forThai can be classified into two main categories definedby [6]: Context Dependent Segmentation Ambiguity(CDSA), and Context Independent Segmentation Ambi-guity (CISA).CISA can be almost resolved deterministically bythe text itself.
There is no need to consult any context.Though there are many possible segmentations, there isonly one plausible segmentation while other alternativesare very unlikely to occur, for example, a string???????????
which can be segmented into two differentways: ???
(go) ???
(carry) ??
(deviate) ??(color)?
[paj0ha:m4 he:4 si:4] and ???
(go) ??
(see)  ????(?queen)?
[paj0ha:4 ma:3 he:4 si:4].
Only the second choice is plausi-ble.
One may say that it is not semantically ambiguous.However, simple algorithms such as maximal matching[6, 9] and longest matching [6] may not be able to dis-criminate this kind of ambiguity.
Probabilistic wordsegmentation can handle this kind of ambiguity success-fully.CDSA needs surrounding context to decide whichsegmentation is the most probable one.
Though thenumber of possible alternatives occurs less than the con-text independent one, it is more difficult to disambigu-ate and causes more errors.
For example, a string???????
can be segmented into ???
????
(round eye) and????
???
(to expose wind) which can be pronounced[ta:0 klom0] and [ta:k1 lom0] respectively.Homograph AmbiguityThai homographs, which cannot be determined the cor-rect pronunciation without context, can be classifiedinto six main categories as follows:1.
Number such as 10400 in postcode, it can be pro-nounced [nvng1 su:n4 si:1 su:n4 su:n4] or [nvng1mv:n1 si:1 r@:ji3] in amount.2.
Abbreviation such as ?.?.
can be pronounced[sam4 nak2 nga:n0 kha:2 ra:t2 cha:3 ka:n0 phon0 la:3rv:an0] (Office Of The Civil Service Commission) or[kum0 pha:0 phan0] (February).3.
Fraction such as 25/2 can be pronounced [yi:2sip1 ha:2 thap3 s@:ng4] (for address) or [yi:2 sip1 ha:2su:an1 s@:ng4] (for fraction).4.
Proper Name such as ??????
is pronounced[som4 phon0] or [sa1 ma3 phon0].5.
Same Part of Speech such as ??????
(time) can bepronounced [phe:0 la:0], while ??????
(axe) is pro-nounced  [phlaw0].6.
Different Part of Speech such as ?????
is pro-nounced [nx:4] or [hx:n4].Previous ApproachesPOS n-gram approaches [7, 10] use statistics of POSbigram or trigram to solve the problem.
They can solveonly the homograph problem that has different POS tag.They cannot capture long distance word associations.Thus, they are inappropriate of resolving the cases ofsemantic ambiguities.Bayesian classifiers [8] use long distance word asso-ciations regardless of position in resolving semanticambiguity.
These methods can successful capture longdistance word association, but cannot capture local con-text information and sentence structure.Decision trees [2] can handle complex condition, butthey have a limitation in consuming very large parame-ter spaces and they solve a target problem by applyingonly the single strongest feature.Hybrid approach [3, 12] combines the strengths ofother techniques such as Bayesian classifier, n-gram,and decision list.
It can be capture both local and longdistance context in disambiguation task.Our ModelTo solve both word boundary ambiguity and homographambiguity, we treat these problems as the problem ofdisambiguating pronunciation.
We construct a confusionset by listing all of its possible pronunciations.
For ex-ample, C = {[ma:0 kwa:1], [ma:k2 wa:2]} is the confu-sion set of the string ?????????
which is a boundary-ambiguity string and C={[phe:0 la:0] ,[phlaw0]} is theconfusion set of the homograph ??????.
We obtain thefeatures that can discriminate each pronunciation in theset by Winnow based on our training set.4.1 WinnowWinnow algorithm used in our experiment is the algo-rithm described in [1].
Winnow is a neuron-like networkwhere several nodes are connected to a target node [4,5].
Each node called specialist looks at a particularvalue of an attribute of the target concept, and will votefor a value of the target concept based on its specialty;i.e.
based on a value of the attribute it examines.
Theglobal algorithm will then decide on weighted-majorityvotes receiving from those specialists.
The pair of (at-tribute=value) that a specialist examines is a candidateof features we are trying to extract.
The global algo-rithm updates the weight of any specialist based on thevote of that specialist.
The weight of any specialist isinitialized to 1.
In case that the global algorithm predictsincorrectly, the weight of the specialist that predictsincorrectly is halved and the weight of the specialist thatpredicts correctly is multiplied by 3/2.
The weight of aspecialist is halved when it makes a mistake even if theglobal algorithm predicts correctly.4.2 FeaturesTo train the algorithm to resolve pronunciation ambigu-ity, the context around a homograph or a boundary-ambiguity string is used to form features.
The featuresare the context words, and collocations.
Context wordsare used to test for the presence of a particular wordwithin +10 words and ?10 words from the target word.Collocations are patterns of up to 2 contiguous wordsand part-of-speech tags around the target word.
There-fore, the total number of features is 10; 2 features forcontext words, and 8 features for collocations.5 Preliminary ExperimentTo test the performance of the different approaches, weselect sentences containing Thai homographs andboundary ambiguity strings from our 25K-words corpusto use in benchmark tests.
Every sentence is manuallyseparated into words.
Their parts of speech and pronun-ciations are manually tagged by linguists.
The resultingcorpus is divided into two parts; the first part, about80% of corpus, is utilized for training and the rest isused for testing.In the experiment, we classify the data into threegroup depending on types of text ambiguity accordingto section 2: CDSA, CISA and Homograph, and com-pare the results from different approaches; Winnow,Bayseian hybrid [3] and POS trigram.
The results areshown in Table 1.Trigram Bayseian WinnowCDSA  73.02% 93.18% 95.67%CISA 98.25% 99.67% 99.70%Homograph 52.46% 94.25% 96.45%Table1: The result of comparing different approaches6 ConclusionIn this paper, we have successfully applied Winnow tothe task of Thai homograph disambiguation.
Winnowshown its ability to construct networks that extract thefeatures in data effectively.
The learned features, whichare context words and collocations, can capture usefulinformaambigushow thesian hmachinRefer[1] BlumMajoDom[2] BrieWad[3] GoldsentiWor[4] GoldSensMacCon[5] LittlutesLear[6] MekFeatthe N1997[7] MerProcFran[8] MosAuthMas[9] SoMacCom[10] Spbaseence[11] YaResoAsso[12] YaSyntHirsVerltion and make the task of Thai homograph dis-ity more accurate.
The experimental resultsat Winnow outperform trigram model and Bay-ybrid.
Our future works will investigate othere learning techniques such as SNoW and SVM.ences, A. Empirial Support for Winnow and Weighted-rity Algorithm: Results on a Calendar Schedulingain, Machine Learning.
1997, 26:5-23.man, L. et al Classification and Regression Trees.sworth & Brooks, Monterrey CA.1984.ing, A. R.  A Bayesian Hybrid Mehod for Context-tive Spelling Correction.
In Proceedings of the Thirdkshop on Very Large Corpora.
1995.ing, A. R. & Roth, D. Applying Winnow to Context-itive Spelling Correction.
In Lorenza Saitta, editor,hine Learning: Proceedings of the  13th Internationalference on  Machine Learning.
1996.estone, N. Learning Quickly when Irrelevant Attrib-Bound: A New Linear-Threshold Algorithm.
Machinening.
1988, 2:285-318.navin, S., Charoenpornsawat P. and Kijsirikul, B.ure-based Thai Word Segmentation.
Proceeding of ofatural Language Processing Pacific Rim Symposium..ialdo, B. Tagging text with a probabilistic model.
Ineedings of the IBM Natural Language ITL, Paris,ce.
1990.teller, F. and Wallace, D. Inference and Disputedorship: The Federalist Addision-Wesley, Reading,sachusetts.
1964.rnlertlamvanich, V. Word Segmentation for Thai.hine Translation System.
National Electronics andputer Technology Center (in Thai).
1993.roat, R.,Hirschberg, J. and Yarowsky, D. A corpus-d synthesizer.
In Proceedings, International Confer-on Spoken Language Processing, Banff.
1992rowsky, D. Decision Lists for Lexical Ambiguitylution.
In Proceeding of 32nd Annual Meeting of theciation for Computational Linguistics.
1994rowsky, D. Homograph Disambiguation in Speechhesis.
In J. van Santen, R. Sproat, J. Olive and J.chberg (eds.
), Progress in Speech Synthesis.
Springer-ag, pp.
159-175, 1996.
