Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 349?356,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsA Handsome Set of Metrics to Measure Utterance ClassificationPerformance in Spoken Dialog SystemsDavid Suendermann, Jackson Liscombe, Krishna Dayanidhi, Roberto Pieraccini?SpeechCycle Labs, New York, USA{david, jackson, krishna, roberto}@speechcycle.comAbstractWe present a set of metrics describingclassification performance for individualcontexts of a spoken dialog system as wellas for the entire system.
We show howthese metrics can be used to train and tunesystem components and how they are re-lated to Caller Experience, a subjectivemeasure describing how well a caller wastreated by the dialog system.1 IntroductionMost of the speech recognition contexts in com-mercial spoken dialog systems aim at mapping thecaller input to one out of a set of context-specificsemantic classes (Knight et al, 2001).
This is doneby providing a grammar to the speech recognizerat a given recognition context.
A grammar servestwo purposes:?
It constraints the lexical content the recog-nizer is able to recognize in this context (thelanguage model) and?
It assigns one out of a set of possible classesto the recognition hypothesis (the classifier).This basic concept is independent of the nature ofa grammar: it can be a rule-based one, manually orautomatically generated; it can comprise a statisti-cal language model and a classifier; it can consistof sets of grammars, language models, or classi-fiers; or it can be a holistic grammar, i.e., a sta-tistical model combining a language model and aclassification model in one large search tree.Most commercial dialog systems utilize gram-mars that return a semantic parse in one of thesecontexts:?
directed dialogs (e.g., yes/no contexts, menuswith several choices, collection of informa-tion out of a restricted set [Which type ofmodem do you have?
]?usually, less than 50classes)?
open-ended prompts (e.g.
for call routing,problem capture; likewise to collect infor-mation out of a restricted set [Tell me what?Patent pending.you are calling about today]?possibly sev-eral hundred classes (Gorin et al, 1997; Boyeand Wiren, 2007))?
information collection out of a huge (or infi-nite) set of classes (e.g., collection of phonenumbers, dates, names, etc.
)When the performance of spoken dialog sys-tems is to be measured, there is a multitude ofobjective metrics to do so, many of which featuremajor disadvantages.
Examples include?
Completion rate is calculated as the numberof completed calls divided by the total num-ber of calls.
The main disadvantage of thismetric is that it is influenced by many fac-tors out of the system?s control, such as callerhang-ups, opt-outs, or call reasons that fallout of the system?s scope.
Furthermore, thereare several system characteristics that impactthis metric, such as recognition performance,dialog design, technical stability, availabilityof back-end integration, etc.
As experienceshows, all of these factors can have unpre-dictable influence on the completion rate.
Onthe one hand, a simple wording change in theintroduction prompt of a system can makethis rate improve significantly, whereas, onthe other hand, major improvement of theopen-ended speech recognition grammar fol-lowing this very prompt may not have anyimpact.?
Average holding time is a common term forthe average call duration.
This metric is oftenconsidered to be quite controversial since it isunclear whether longer calls are preferred ordispreferred.
Consider the following two in-congruous behaviors resulting in longer callduration:?
The system fails to appropriately treatcallers, asking too many questions, per-forming redundant operations, actingunintelligently because of missing back-end integration, or letting the caller waitin never-ending wait music loops.?
The system is so well-designed that itengages callers to interact with the sys-tem longer.349?
Hang-up and opt-out rates.
These metricstry to encapsulate how many callers choosenot to use the dialog system, either becausethey hang up or because they request to speakwith a human operator.
However, it is unclearhow such events are related to dialog systemperformance.
Certainly, many callers mayhave a prejudice against speaking with auto-mated systems and may hang up or requesta human regardless of how well-performingthe dialog system is with cooperative users.Furthermore, callers who hang up may do sobecause they are unable to get their problemsolved or they may hang up precisely becausetheir problem was solved (instead of waitingfor the more felicitous post-problem-solvingdialog modules).?
Retry rate is calculated as the average num-ber of times that the system has to re-promptfor caller input because the caller?s previ-ous utterance was determined to be Out-of-Grammar.
The intuition behind this metricis that the lower the retry rate, the betterthe system.
However, this metric is prob-lematic because it is tied to grammar per-formance itself.
Consider a well-performinggrammar that correctly accepts In-Grammarutterances and rejects Out-of-Grammar utter-ances.
This grammar will cause the system toproduce retries for all Out-of-Grammar utter-ances.
Consider a poorly designed grammarthat accepts everything (incorrectly), evenbackground noise.
This grammar would de-crease the retry rate but would not be indica-tive of a well-performing dialog system.As opposed to these objective measures, there isa subjective measure directly related to the systemperformance as perceived by the user:?
Caller Experience.
This metric is used todescribe how well the caller is treated by thesystem according to its design.
Caller Expe-rience is measured on a scale between 1 (bad)and 5 (excellent).
This is the only subjectivemeasure in this list and is usually estimatedbased on averaging scores given by multi-ple voice user interface experts which listento multiple full calls.
Although this metricdirectly represents the ultimate design goalfor spoken dialog systems?i.e., to achievehighest possible user experience?it is veryexpensive to be repeatedly produced and notsuitable to be generated on-the-fly.Our former research has suggested, however,that it may be possible to automatically esti-mate Caller Experience based on several ob-jective measures (Evanini et al, 2008).
Thesemeasures include the overall number of no-matches and substitutions in a call, opera-tor requests, hang-ups, non-heard speech, thefact whether the call reason could be suc-cessfully captured and whether the call rea-son was finally satisfied.
Initial experimentsshowed a near-human accuracy of the auto-matic predictor trained on several hundredcalls with available manual Caller Experi-ence scores.
The most powerful objectivemetric turned out to be the overall numberof no-matches and substitutions, indicating ahigh correlation between the latter and CallerExperience.No-matches and substitutions are objective met-rics defined in the scope of semantic classificationof caller utterances.
They are part of a larger set ofsemantic classification metrics which we system-atically demonstrate in Section 2.
The remainderof the paper examines three case studies exploringthe usefulness and interplay of different evaluationmetrics, including:?
the correlation between True Total (one of theintroduced metrics) and Caller Experience inSection 3,?
the estimation of speech recognition and clas-sification parameters based on True Total andTrue Confirm Total (another metric) in Sec-tion 4, and?
the tuning of large-scale spoken dialog sys-tems to maximize True Total and its effect onCaller Experience in Section 5.2 Metrics for Utterance ClassificationAcoustic events processed by spoken dialog sys-tems are usually split into two main categories:In-Grammar and Out-of-Grammar.
In-Grammarutterances are all those that belong to one of thesemantic classes processable by the system logicin the given context.
Out-of-Grammar utterancescomprise all remaining events, such as utteranceswhose meanings are not handled by the grammaror when the input is non-speech noise.Spoken dialog systems usually respond toacoustic events after being processed by the gram-mar in one of three ways:?
The event gets rejected.
This is when the sys-tem either assumes that the event was Out-of-Grammar, or it is so uncertain about its(In-Grammar) finding that it rejects the utter-ance.
Most often, the callers get re-promptedfor their input.350Table 1: Event AcronymsI In-GrammarO Out-of-GrammarA AcceptR RejectC CorrectW WrongY ConfirmN Not-ConfirmTA True AcceptFA False AcceptTR True RejectFR False RejectTAC True Accept CorrectTAW True Accept WrongFRC False Reject CorrectFRW False Reject WrongFAC False Accept ConfirmFAA False Accept AcceptTACC True Accept Correct ConfirmTACA True Accept Correct AcceptTAWC True Accept Wrong ConfirmTAWA True Accept Wrong AcceptTT True TotalTCT True Confirm Total?
The event gets accepted.
This is when thesystem is certain to have correctly detectedan In-Grammar semantic class.?
The event gets confirmed.
This is when thesystem assumes to have correctly detected anIn-Grammar class but still is not absolutelycertain about it.
Consequently, the caller isasked to verify the class.
Historically, confir-mations are not used in many contexts wherethey would sound confusing or distracting,for instance in yes/no contexts (?I am sorry.Did you say NO????No!??
?This was NO,yes????No!!!?
).Based on these categories, an acoustic event andhow the system responds to it can be described byfour binary questions:1.
Is the event In-Grammar?2.
Is the event accepted?3.
Is the event correctly classified?4.
Is the event confirmed?Now, we can draw a diagram containing the firsttwo questions as in Table 2.
See Table 1 for allTable 2: In-Grammar?
Accepted?A RI TA FRO FA TRTable 3: In-Grammar?
Accepted?
Correct?A RC W C WI TAC TAW FRC FRWO FA TRacoustic event classification types used in the re-mainder of this paper.Extending the diagram to include the third ques-tion is only applicable to In-Grammar events sinceOut-of-Grammar is a single class and, therefore,can only be either falsely accepted or correctly re-jected as shown in Table 3.Further extending the diagram to accomodatethe fourth question on whether a recognized classwas confirmed is similarly only applicable if anevent was accepted, as rejections are never con-firmed; see Table 4.
Table 5 gives one example foreach of the above introduced events for a yes/nogrammar.When the performance of a given recognitioncontext is to be measured, one can collect a cer-tain number of utterances recorded in this context,look at the recognition and application logs to seewhether these utterances where accepted or con-firmed and which class they were assigned to, tran-scribe and annotate the utterances for their seman-tic class and finally count the events and dividethem by the total number of utterances.
If X is anevent from the list in Table 1, we want to refer tox as this average score, e.g., tac is the fraction oftotal events correctly accepted.
One characteristicof these scores is that they sum up to 1 for each ofthe Diagrams 2 to 4 as for examplea + r = 1, (1)i + o = 1, (2)ta + fr + fa + tr = 1.
(3)In order to enable system tuning and to reportsystem performance at-a-glance, the multitude ofmetrics must be consolidated into a single power-ful metric.
In the industry, one often uses weightsto combine metrics since they are assumed to havedifferent importance.
For instance, a False Ac-cept is considered worse than a False Reject sincethe latter allows for correction in the first retrywhereas the former may lead the caller down the351Table 5: Examples for utterance classification metrics.
This table shows the transcription of an utterance,the semantic class it maps to (if In-Grammar), a binary flag for whether the utterance is In-Grammar, therecognized class (i.e.
the grammar output), a flag for whether the recognized class was accepted, a flagfor whether the recognized class was correct (i.e.
matched the transcription?s semantic class), a flagfor whether the recognized class was confirmed, and the acronym of the type of event the respectivecombination results in.utterance class In-Grammar?
rec.
class accepted?
correct?
confirmed?
eventyeah YES 1 Iwhat 0 ONO 1 ANO 0 Rno no no NO 1 NO 1 Cyes ma?am YES 1 NO 0 W1 Y0 Ni said no NO 1 YES 1 TAoh my god 0 NO 1 FAi can?t tell 0 NO 0 TRyes always YES 1 YES 0 FRyes i guess so YES 1 YES 1 1 TACno i don?t think so NO 1 YES 1 0 TAWdefinitely yes YES 1 YES 0 1 FRCno man NO 1 YES 0 0 FRWsunshine 0 YES 1 1 FACchoices 0 NO 1 0 FAAright YES 1 YES 1 1 1 TACCyup YES 1 YES 1 1 0 TACAthis is true YES 1 NO 1 0 1 TAWCno nothing NO 1 YES 1 0 0 TAWATable 4: In-Grammar?
Accepted?
Correct?
Con-firmed?A RC W C WY TACC TAWCI N TACA TAWA FRC FRWY FACO N FAA TRwrong path.
However, these weights are heavilynegotiable and depend on customer, application,and even the recognition context, making it im-possible to produce a comprehensive and widelyapplicable consolidated metric.
This is why wepropose to split the set of metrics into two groups:good and bad.
The sought-for consolidated met-ric is the sum of all good metrics (hence, an over-all accuracy) or, alternatively, the sum of all badevents (overall error rate).
In Tables 3 and 4, goodmetrics are highlighted.
Accordingly, we definetwo consolidated metrics True Total and True Con-firm Total as follows:tt = tac + tr, (4)tct = taca + tawc + fac + tr.
(5)In the aforementioned special case that a recog-nition context never confirms, Equation 5 equalsEquation 4 since the confirmation terms tawc andfac disappear.The following sections report on three casestudies on the applicability of True Total and TrueConfirm Total to the tuning of spoken dialog sys-tems and how they relate to Caller Experience.3 On the Correlation between True Totaland Caller ExperienceAs motivated in Section 1, initial experiments onpredicting Caller Experience based on objectivemetrics indicated that there is a considerable cor-relation between Caller Experience and semantic352Table 6: Pearson correlation coefficient for sev-eral utterance classification metrics on the sourcedata.A RC WI 0.394 -0.160 ......-0.230......O -0.242 -0.155r(TT) = 0.378classification metrics such as those introduced inSection 2.
In the first of our case studies, this effectis to be deeper analyzed and quantified.
For thispurpose, we selected 446 calls from four differentspoken dialog systems of the customer service hot-lines of three major cable service providers.
Thespoken dialog systems comprised?
a call routing application?cf.
(Suendermannet al, 2008),?
a cable TV troubleshooting application,?
a broadband Internet troubleshooting appli-cation, and?
a Voice-over-IP troubleshootingapplication?see for instance (Acomb etal., 2007).The calls were evaluated by voice user interfaceexperts and Caller Experience was rated accordingto the scale introduced in Section 1.
Furthermore,all speech recognition utterances (4480) were tran-scribed and annotated with their semantic classes.Thereafter, all utterance classification metrics in-troduced in Section 2 were computed for every callindividually by averaging across all utterances ofa call.
Finally, we applied the Pearson correlationcoefficient (Rodgers and Nicewander, 1988) to thesource data points to correlate the Caller Experi-ence score of a single call to the metrics of thesame call.
This was done in Table 6.Looking at these numbers, whose magnitude israther low, one may be suspect of the findings.E.g., |r(FR)| > |r(TAW)| suggesting that FalseReject has a more negative impact on Caller Expe-rience than True Accept Wrong (aka Substitution)which is against common experience.
Reasons forthe messiness of the results are that?
Caller Experience is subjective and affectedby inter- and intra-expert inconsistency.
E.g.,in a consistency cross-validation test, we ob-served identical calls rated by one subject as1 and by another as 5.Figure 1: Dependency between Caller Experienceand True Total.?
Caller Experience scores are discrete, and,hence, can vary by ?1, even in case of strongconsistency.?
Although utterance classification metrics are(almost) objective metrics measuring the per-centage of how often certain events happenin average, this average generated for indi-vidual calls may not be very meaningful.
Forinstance, a very brief call with a single yes/noutterance correctly classified results in thesame True Total score like a series of 50 cor-rect recognitions in a 20-minutes conversa-tion.
While the latter is virtually impossible,the former happens rather often and domi-nates the picture.?
The sample size of the experiment conductedin the present case study (446 calls) is per-haps too small for deep analyses on eventsrarely happening in the investigated calls.Trying to overcome these problems, we com-puted all utterance classification metrics intro-duced in Section 2, grouping and averaging themfor the five distinct values of Caller Experience.As an example, we show the almost linear graphexpressing the relationship between True Total andCaller Experience in Figure 1.
Applying the Pear-son correlation coefficient to this five-point curveyields r = 0.972 confirming that what we see ispretty much a straight line.
Comparing this valueto the coefficients produced by the individual met-rics TAC, TAW, FR, FA, and TR as done in Ta-ble 7, shows that no other line is as straight as theone produced by True Total supposing its maxi-mization to produce spoken dialog systems withhighest level of user experience.353Table 7: Pearson correlation coefficient for sev-eral utterance classification metrics after group-ing and averaging.A RC WI 0.969 -0.917 ......-0.539......O -0.953 -0.939r(TT) = 0.9724 Estimating Speech Parameters byMaximizing True Total or TrueConfirm TotalThe previous section tried to shed some light onthe relationship between some of the utteranceclassification metrics and Caller Experience.
Wesaw that, on average, increasing Caller Experiencecomes with increasing True Total as the almost lin-ear curve of Figure 1 supposes.
As a consequence,much of our effort was dedicated to maximizingTrue Total in diverse scenarios.
Speech recogni-tion as well as semantic classification with all theircomponents (such as acoustic, language, and clas-sification models) and parameters (such as acous-tic and semantic rejection and confirmation confi-dence thresholds, time-outs, etc.)
was set up andtuned to produce highest possible scores.
This sec-tion gives two examples of how parameter settingsinfluence True Total.4.1 Acoustic Confirmation ThresholdWhen a speech recognizer produces a hypothesisof what has been said, it also returns an acousticconfidence score which the application can utilizeto decide whether to reject the utterance, confirmit, or accept it right away.
The setting of thesethresholds has obviously a large impact on CallerExperience since the application is to reject as fewvalid utterances as possible, not confirm every sin-gle input, but, at the same time, not falsely acceptwrong hypotheses.
It is also known that these set-tings can strongly vary from context to context.E.g., in announcements, where no caller input isexpected, but, nonetheless utterances like ?agent?or ?help?
are supposed to be recognized, rejectionmust be used much more aggressively than in col-lection contexts.
True Total or True Confirm To-tal are suitable measures to detect the optimumtradeoff.
Figure 2 shows the True Confirm Totalgraph for a collection context with 30 distinguish-able classes.
At a confidence value of 0.12, thereis a local and global maximum indicating the opti-mum setting for the confirmation threshold for thisgrammar context.Figure 2: Tuning the acoustic confirmation thresh-old.4.2 Maximum Speech Time-OutThis parameter influences the maximum time thespeech recognizer keeps recognizing once speechhas started until it gives up and discards the recog-nition hypothesis.
Maximum speech time-out isprimarily used to limit processor load on speechrecognition servers and avoid situations in whichline noise and other long-lasting events keep therecognizer busy for an unnecessarily long time.
Asit anecdotally happened to callers that they wereinterrupted by the dialog system, on the one hand,some voice user interface designers tend to choserather large values for this time-out setting, e.g.,15 or 20 seconds.
On the other hand, very longspeech input tends to produce more likely a clas-sification error than shorter ones.
Might there be asetting which is optimum from the utterance clas-sification point of view?To investigate this behavior, we took 115,885transcribed and annotated utterances collected inthe main collection context of a call routing ap-plication and aligned them to their utterance dura-Figure 3: Dependency between utterance durationand True Total.354Figure 4: Dependency between maximum speechtime-out and True Total.tions.
Then, we ordered the utterances in descend-ing order of their duration, grouped always 1000successive utterances together, and averaged overduration and True Total.
This generated 116 datapoints showing the relationship between the dura-tion of an utterance and its expected True Total,see Figure 3.The figure shows a clear maximum somewherearound 2.5 seconds and then descends with in-creasing duration towards zero.
Utterances witha duration of 9 seconds exhibited a very low TrueTotal score (20%).
Furthermore, it would appearthat one should never allow utterances to exceedfour second in this context.
However, upon fur-ther evaluation of the situation, we also have toconsider that long utterances occur much less fre-quently than short ones.
To integrate the frequencydistribution into this analysis, we produced an-other graph that shows the average True Total ac-cumulated over all utterances shorter than a cer-tain duration.
This simulates the effect of usinga different maximum speech time-out setting andis displayed in Figure 4.
We also show a graphon how many of the utterances would have beeninterrupted in Figure 5.The curve shows an interesting down-up-downtrajection which can be explained as follows:?
Acoustic events shorter than 1.0 seconds aremostly noise events which are correctly iden-tified since the speech recognizer could noteven build a search tree and returns an emptyhypothesis which the classifier, in turn, cor-rectly rejects.?
Utterances with a duration around 1.5s aredominated by single words which cannotproperly evaluated by the (trigram) languagemodel.
So, the acoustic model takes over themain work and, because of its imperfectness,lowers the True Total.Figure 5: Percentage of utterances interrupted bymaximum speech time-out.?
Utterances with a moderate number of wordsare best covered by the language model, sowe achieve highest accuracy for them (?3s).?
The longer the utterances continues after 4seconds, the less likely the language modeland classfier are to have seen such utterances,and True Total declines.Evaluating the case from the pure classifier per-formance perspective, the maximum speech time-out would have to be set to a very low value(around 3 seconds).
However, at this point, about20% of the callers would be interrupted.
The deci-sion whether this optimimum should be accepcteddepends on how elegantly the interruption can bedesigned:?I?m so sorry to interrupt, but I?m hav-ing a little trouble getting that.
So, let?stry this a different way.
?5 Continuous Tuning of a Spoken DialogSystem to Maximize True Total and ItsEffect on Caller ExperienceIn the last two sections, we investigated the corre-lation between True Total and Caller Experienceand gave examples on how system parameters canbe tuned by maximizing True Total.
The presentsection gives a practical example of how rigorousimprovement of utterance classification leads toreal improvement of Caller Experience.The application in question is a combination ofthe four systems listed in Section 3 which workin an interconnected fashion.
When callers accessthe service hotline, they are first asked to brieflydescribe their call reason.
After up to two follow-up questions to further disambiguate their reason,they are either connected to a human operator orone of the three automated troubleshooting sys-tems.
Escalation from one of them can connect355Figure 6: Increase of the True Total of a large-vocabulary grammar with more than 250 classesover release time.the caller to an agent, transfer the caller back tothe call router or to one of the other troubleshoot-ing systems.When the application was launched in June2008, its True Total averaged 78%.
During the fol-lowing three months, almost 2.2 million utteranceswere collected, transcribed, and annotated for theirsemantic classes to train statistical update gram-mars in a continuously running process (Suender-mann et al, 2009).
Whenever a grammar sig-nificantly outperformed the most recent baseline,it was released and put into production leadingto an incremental improvement of performancethroughout the application.
As an example, Fig-ure 6 shows the True Total increase of the top-levellarge-vocabulary grammar that distinguishes morethan 250 classes.
The overall performance of theapplication went up to more than 90% True Totalwithin three months of its launch.Having witnessed a significant gain of a spokendialog system?s True Total, we would now like toknow to what extent this improvement manifestsitself in an increase of Caller Experience.
Fig-ure 7 shows that, indeed, Caller Experience wasstrongly positively affected.
Over the same threemonth period, we achieved an iterative increasefrom an initial Caller Experience of 3.4 to 4.6.6 ConclusionSeveral of our investigations have suggested a con-siderable correlation between True Total, an objec-tive utterance classification metric, and Caller Ex-perience, a subjective score of overall system per-formance usually rated by expert listeners.
Thisobservation leads to our main conclusions:?
True Total and several of the other utteranceclassification metrics introduced in this papercan be used as input to a Caller Experiencepredictor?as tentative results in (Evanini etal., 2008) confirm.Figure 7: Increase of Caller Experience over re-lease time.?
Efforts towards improvement of speechrecognition in spoken dialog applicationsshould be focused on increasing True Totalsince this will directly influence Caller Expe-rience.ReferencesK.
Acomb, J. Bloom, K. Dayanidhi, P. Hunter,P.
Krogh, E. Levin, and R. Pieraccini.
2007.
Techni-cal Support Dialog Systems: Issues, Problems, andSolutions.
In Proc.
of the HLT-NAACL, Rochester,USA.J.
Boye and M. Wiren.
2007.
Multi-Slot Semantics forNatural-Language Call Routing Systems.
In Proc.of the HLT-NAACL, Rochester, USA.K.
Evanini, P. Hunter, J. Liscombe, D. Suendermann,K.
Dayanidhi, and R. Pieraccini:.
2008.
Caller Ex-perience: A Method for Evaluating Dialog Systemsand Its Automatic Prediction.
In Proc.
of the SLT,Goa, India.A.
Gorin, G. Riccardi, and J. Wright.
1997.
How MayI Help You?
Speech Communication, 23(1/2).S.
Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-ing, and I. Lewin.
2001.
Comparing Grammar-Based and Robust Approaches to Speech Under-standing: A Case Study.
In Proc.
of the Eurospeech,Aalborg, Denmark.J.
Rodgers and W. Nicewander.
1988.
Thirteen Waysto Look at the Correlation Coefficient.
The Ameri-can Statistician, 42(1).D.
Suendermann, P. Hunter, and R. Pieraccini.
2008.Call Classification with Hundreds of Classes andHundred Thousands of Training Utterances ... andNo Target Domain Data.
In Proc.
of the PIT, KlosterIrsee, Germany.D.
Suendermann, J. Liscombe, K. Evanini,K.
Dayanidhi, and R. Pieraccini.
2009.
FromRule-Based to Statistical Grammars: Continu-ous Improvement of Large-Scale Spoken DialogSystems.
In Proc.
of the ICASSP, Taipei, Taiwan.356
