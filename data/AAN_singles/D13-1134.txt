Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1354?1359,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsPredicting the resolution of referring expressions from user behaviorNikos Engonopoulos1 Mart?
?n Villalba1 Ivan Titov2 Alexander Koller11University of Potsdam, Germany 2University of Amsterdam, Netherlands{nikolaos.engonopoulos, martin.villalba}@uni-potsdam.detitov@uva.nl, koller@ling.uni-potsdam.deAbstractWe present a statistical model for predictinghow the user of an interactive, situated NLPsystem resolved a referring expression.
Themodel makes an initial prediction based on themeaning of the utterance, and revises it con-tinuously based on the user?s behavior.
Thecombined model outperforms its componentsin predicting reference resolution and when togive feedback.1 IntroductionSpeakers and listeners in natural communication areengaged in a highly interactive process.
In order toachieve some communicative goal, the speaker willperform an utterance which they believe has a highchance of achieving that goal.
They will then moni-tor the listener?s behavior to see whether this goal isactually being achieved.
This process is a core partof what is commonly called grounding in the dia-logue literature (see e.g.
(Clark, 1996; Traum, 1994;Paek and Horvitz, 1999; Hirst et al 1994)).
Inter-active computer systems that are to carry out an ef-fective and efficient conversation with a user mustmodel this grounding process, and should ideally re-spond to the user?s observed behavior in real time.For instance, if the user of a pedestrian navigationsystem takes a wrong turn, the system should inter-pret this as evidence of misunderstanding and bringthe user back on track.We focus here on the problem of predicting howthe user has resolved a referring expression (RE) thatwas generated by the system, i.e.
a noun phrase thatis intended to identify some object uniquely to thelistener.
A number of authors have recently offeredstatistical models for parts of this problem.
Gollandet al(2010) and Garoufi and Koller (2011) havepresented log-linear models for predicting how thelistener will resolve a given RE in a given scene;however, these models do not update the probabil-ity model based on observing the user?s reactions.Nakano et al(2007), Buschmeier and Kopp (2012),and Koller et al(2012) all predict what the listenerunderstood based on their behavior, but do not con-sider the RE itself in the model.
The models ofFrank and Goodman (2012) and Vogel et al(2013)aim at explaining the effect of implicatures on thelistener?s RE resolution process in terms of hypothe-sized interactions, but do not actually support a real-time interaction between a system and a user.In this paper, we show how to predict how thelistener has resolved an RE by combining a statis-tical model of RE resolution based on the RE itselfwith a statistical model of RE resolution based onthe listener?s behavior.
To our knowledge, this isthe first approach to combine two such models ex-plicitly.
We consider the RE grounding problem inthe context of interactive, situated natural languagegeneration (NLG) for the GIVE Challenge (Koller etal., 2010a), where NLG systems must generate real-time instructions in virtual 3D environments.
Ourevaluation is based on interaction corpora from theGIVE-2 and GIVE-2.5 Challenges, which containthe systems?
utterances along with the behavior ofhuman hearers in response to these utterances.
Wefind that the combined model predicts RE resolu-tion more accurately than each of the two compo-nent models alone.
We see this as a first step towardsimplementing an actual interactive system that per-forms human-like grounding based on our RE reso-lution model.1354Figure 1: An example scene in the GIVE environment.2 Problem definitionIn the GIVE Challenge, an interactive NLG systemfaces the task of guiding a human instruction fol-lower (IF) through a treasure-hunt game in a vir-tual 3D environment (see Fig.
1).
To complete thetask, the IF must press a number of buttons in thecorrect order; these buttons are the colored boxes inFig.
1, and are scattered all over the virtual environ-ment.
The IF can move around freely in the virtualenvironment, but has no prior knowledge about theworld.
The NLG system?s task is to guide the IF to-wards the successful completion of the treasure-hunttask.
To this end, it is continuously being informedabout the IF?s movements and visual field, and cangenerate written utterances at any time.
As a com-parative evaluation effort, the GIVE Challenges con-nected NLG systems to thousands of users over theInternet (see e.g.
Koller et al(2010a) for details).Many system utterances are manipulation instruc-tions, such as ?press the blue button?, containing anRE in the form of a definite NP.
We call a given partof an interaction between the system and the IF anepisode of that interaction if it starts with a manip-ulation instruction, ends with the IF performing anaction (i.e., pressing a button), and contains onlyIF movements and no further utterances in between.Not all manipulation instructions initiate an episode,because the system may decide to perform furtherutterances (not containing REs) before the IF per-forms their action.
An NLG system will choose theRE for an instruction at runtime out of potentiallymany semantically valid alternatives (?the blue but-ton?, ?the button next to the chair?, ?the button tothe right of the red button?, etc.).
Ideally, it will pre-dict which of these REs has the highest chance to beunderstood by the IF, given the current scene, andutter an instruction that uses this RE.After uttering the manipulation instruction, thesystem needs to ascertain whether the IF understoodthe RE correctly, i.e.
it must engage in grounding.A naive grounding mechanism might wait until theIF actually presses a button and check whether it wasthe right one.
This is what many NLG systems in theGIVE Challenges actually did.
However, this canmake the communication ineffective (IF performsmany useless actions) and risky (IF may press thewrong button and lose).
Thus, it is important that thesystem updates its prediction of how the IF resolvedthe RE continuously by observing the IF?s behavior,before the actual button press.
For instance, if theIF walks towards the target, this might reinforce thesystem?s belief in a correct understanding; turningaway or exiting the room could be strong evidenceof the opposite.
The system can then exploit the up-dated prediction to give the IF feedback (?no, theblue button?)
to prevent costly mistakes.We address these challenges by estimating theprobability distribution over the possible objects towhich the IF may resolve the RE.
We then updatethis distribution in real time by observing the IF?smovements.
More specifically, assume that a sys-tem tries to refer to some object a?
among some setA of available objects.
Given an RE r generated fora?
at time t0, the state of the world s at t0, and theobserved behavior ?
(t) of the user at t ?
t0, weestimate the probability p(a|r, s, ?
(t)) that the userresolved r to an object a ?
A.
When generating theinstruction, an optimal NLG system will use the REr that maximizes p(a?|r, s, ?(t0)).
It can then trackp(a|r, s, ?
(t)) for time points t > t0 throughout theepisode, and generate feedback when p(a?|r, s, ?
(t))exceeds p(a?|r, s, ?
(t)) for some a?
6= a?
; that is,when the updated probability distribution predictsthat the IF resolved r to an incorrect button.3 A model of RE resolutionIn order to model the distribution over possible ob-jects, we assume the following generative story:when receiving an instruction containing an RE r ata given world state s, the IF resolves it to an objecta; depending on the object a, the IF then moves to-wards it, exhibiting behavior ?.
These assumptionscorrespond to the following factorization:p(a, ?|r, s) = p(?|a)p(a|r, s)1355The posterior probability distribution over objects acan be obtained by applying the Bayes rule and us-ing the above assumptions:p(a|r, s, ?)
?
p(a|r, s)p(a|?
)/p(a)For simplicity, we assume a uniform p(a) over allobjects in a world.
We can thus represent p(a|r, s, ?
)as the normalized product of a semantic modelpsem(a|r, s) and an observational model pobs(a|?
).We use log-linear models for both, and train themseparately.
The feature functions we use only con-sider general properties of objects (such as color anddistance), and not the identity of the objects them-selves.
This means that we can train a model on onevirtual environment (containing a certain set of ob-jects), and then apply the model to another virtualenvironment, containing a different set of objects.Semantic model The semantic model estimatesfor each object a in the environment the initial prob-ability psem(a|r, s) that the IF will understand agiven RE r uttered in a scene s as referring to a. Itrepresents the meaning of r, contextualized to s, andis only ever evaluated at the time t0 of the utterance.The features used by this model are:?
Semantic features aim to encode whether ris a good description of a. IsColorModifyingevaluates to 1 if a?s color appears as an adjec-tive modifying the head noun of r, e.g.
?theblue button?.
IsRelPosModifying evaluates to1 if a?s relative position to the IF is mentionedas an adjective in r, e.g.
?the left button?.?
Confusion features capture the hypothesis thatthe IF may be confused by the description of alandmark when resolving the RE; e.g.
an RElike ?the button next to the red button?
mightconfuse the IF into pressing a red button, ratherthan the one meant by the system.
These arethe same features as in the Semantic case, butlooking for modifier keywords in the entire RE,including the head.?
Salience features account for the fact that anIF is more likely to resolve r to a if a was visu-ally salient in s. IsVisible evaluates to 1 if a isvisible to the IF in s. IsInRoom evaluates to 1if the IF and a are in the same room.
IsTarget-InFront evaluates to 1 if the angular distancetowards a, i.e.
the absolute angle between thecamera direction and the straight line from theIF to a, is less than pi4 .
VisualSalience approx-imates the visual salience of Kelleher and vanGenabith (2004), a weighted count of the num-ber of pixels on which a is rendered (pixels nearthe center of the screen have higher weights).Observational model The observational modelestimates for each object a the probability pobs(a|?
)that the IF will interact with a, given the IF?s recentbehavior ?
(t) = (?1, .
.
.
, ?n), where ?i is the stateof the world at time t?
(i?
1) ?
500ms, and n ?
1is the length of the observed behavior.
pobs is con-stantly re-evaluated for times t > t0 as the IF movesaround.
pobs uses the following features:?
Linear distance features assume that the clos-est button is also the one the IF understood.
In-Room returns the number of frames ?i in ?
inwhich the IF and a are in the same room.
But-tonDistance returns the distance between the IFand a at ?1 divided by a constant such that theresult never exceeds 1.
If a is neither in thesame room nor visible, the feature returns 1.?
Angular distance features analyze the direc-tion in which the IF looks.
TargetInFront re-turns the angular distance towards a at ?1.
An-gleToTarget returns TargetInFront divided bypi, or 1 if a is neither in the same room norvisible.
LinearRegAngleTo applies linear re-gression to a list of observed angular distancestowards a over all frames ?i, and returns theslope of the regression as a measure of varia-tion.
Negative values indicate that the IF turnedtowards a, while positive values mean the op-posite.
If a is neither visible nor in the sameroom as the IF at ?i, the angle is set to pi.?
Combined distance feature: a weighted sumof linear and angular distance towards a, calledoverall distance in Koller et al(2012).?
Salience features capture visual salience andits change over time.
Defining VSi as the resultof applying the psem feature VisualSalience to?i and a, LastVisualSalience returns VSn.
Lin-earRegVisualSalience applies linear regressionto all values VSi and returns the slope as a mea-sure of change in salience.
VisualSalienceSumreturns (?ni=1VSi) ?VS1.
This emphasizes thecontribution of VS1, which we assume is the1356most reliable predictor of the IF?s intentions.?
Binary features aim to detect concrete behav-ior patterns: LastIsVisible applies the psem fea-ture IsVisible to ?1, and IsClose evaluates to 1if the IF is close enough and correctly orientedto manipulate a in the GIVE environment at ?1.4 EvaluationData We evaluated our model using data from theGIVE-2 (Koller et al 2010b) and the GIVE-2.5Challenges (Striegnitz et al 2011), obtained fromGIVE Organizers (2012).
These datasets constituteinteraction corpora, in which the IF?s activities inthe virtual environment were recorded along withthe utterances automatically generated by the par-ticipating NLG systems.
The data consists of 1833games for GIVE-2 and 687 games for GIVE-2.5.To extract training data for our model from theGIVE-2.5 data, we first identified moments in therecorded data where the IF pressed a button.
Fromthese, we discarded all instances from the tutorialphase of the GIVE game and those that happenedwithin 200 ms after the previous utterance, as theseclearly didn?t happen in response to it.
This yielded6478 training instances for pobs, each consisting of?
at 1 second before the action, and the button awhich the IF pressed.
We chose n = 4 for rep-resenting ?, except to ensure that the features onlyconsidered IF behavior that happened in response toan utterance.
We achieved this by reducing n for thefirst few frames after each utterance, such that thetime of ?n was always after the time of the utter-ance.
Finally, we selected those instances which areepisodes in the sense of Section 2, i.e.
those in whichthe last utterance before the action contained an REr.
This gave us 3414 training instances for psem,each consisting of a, r, the time t0 of the utterance,and the world state s at time t0.We obtained test instances from the GIVE-2 datain the same way.
This yielded 5028 instances, eachrepresenting an episode.
We chose GIVE-2 for test-ing because the mean episode length is higher (3.3s,vs.
2.0s in GIVE-2.5), thus making the evaluationmore challenging.
Feature selection was done usingthe training data and a similar dataset from Koller etal.
(2012).
Note that the test data and training dataare based on distinct sets of three virtual environ-ll ll0.00.20.40.6Accuracyl combinedsemanticobservationalKGSCrandom visible(a)l ll l?3 ?2 ?1 00.00.20.40.6Time before action (sec)Accuracyl combinedsemanticobservationalKGSCrandom visible(b)Figure 2: Prediction accuracy for (a) all episodes, (b) un-successful episodes as a function of time.ments each, and were obtained with different NLGsystems and users.
This demonstrates the ability ofour model to generalize to unseen environments.An example video showing our models?
predic-tions on some training episodes can be found athttp://tinyurl.com/re-demo-v.Prediction accuracy We first evaluated the abil-ity of our model to predict the button to whichthe IF resolved each RE.
For each test instance?r, s, ?, a?, we compare the object returned byarg maxa p(a|r, s, ?
(t)) to the one manipulated bythe IF.
We call the proportion of correctly classifiedinstances the prediction accuracy.Fig.
2a compares our model?s prediction accuracyto that of several baselines.
We plot prediction ac-curacy as a function of the time at which the modelis queried for a prediction, by evaluating at 3s, 2s,1s, and 0s before the button press.
The graph isbased on the 2094 test instances with an episodelength of at least three seconds, to ensure that re-sults for different prediction times are comparable.As expected, prediction accuracy increases as we ap-proach the time of the action.
Furthermore, the com-bined model outperforms both psem and pobs reli-ably.
This indicates that the component models pro-1357vide complementary useful information.
Our modelalso outperforms two more baselines: KGSC pre-dicts that the IF will press the button with the min-imal overall distance, which is the distance metricused by the ?movement-based system?
of Koller etal.
(2012); random visible selects a random buttonfrom the ones that are currently visible to the IF.The fact that this last baseline does not approach 1at action time suggests that multiple buttons tend tobe visible when the IF presses one, confirming thatthe prediction task is not trivial.Correctly predicting the button that the IF willpress is especially useful, and challenging, in thosecases where the IF pressed a different button thanthe one the NLG system intended.
Fig.
2b showsa closer look at the 125 unsuccessful episodes of atleast three seconds in the test data.
These tend tobe hard instances, and thus as expected, predictionaccuracy drops for all systems.
However, by inte-grating semantic and observational information, thecombined model compensates better for this than allother systems, with an accuracy of 37.6% against31.2% for each individual component.Feedback appropriateness Second, we evaluatedthe ability of our model to predict whether the usermisunderstood the RE and requires feedback.
For allthe above models, we assumed a simple feedbackmechanism which predicts that the user misunder-stood the RE if p(a?)
?
p(a?)
> ?
for some objecta?
6= a?, where ?
is a confidence threshold; we used?
= 0.1 here.
We can thus test on recorded data inwhich no actual feedback can be given anymore.We evaluated the models on the 848 test episodesof at least 3s in which the NLG systems logged thebutton they tried to refer to.
The results are shownin Fig.
3 in terms of F1 measure.
Here precision isthe proportion of instances in which the IF pressedthe wrong button (i.e., where feedback should havebeen given) among the instances where the modelactually suggested feedback.
Recall is the propor-tion of instances in which the model suggested feed-back among the instances where the IF pressed thewrong button.
Again, the combined model outper-forms its components and the baselines, primarilydue to increased recall.
The difference is particu-larly pronounced early on, which would be useful ingiving timely feedback in an actual real-time system.l ll l?3 ?2 ?1 00.00.20.40.6Time before action (s)FeedbackF1l combinedsemanticobservationalKGSCrandom visibleFigure 3: Feedback F1-measure as a function of time.5 Conclusion and future workWe presented a statistical model for predicting howa user will resolve the REs generated by an interac-tive, situated NLG system.
The model continuouslyupdates an initial estimate based on the meaning ofthe RE with a model of the user?s behavior.
It out-performs its components and two baselines on pre-diction and feedback accuracy.Our model captures a real-time grounding processon the part of the interactive system.
We thus believethat it provides a solid foundation for detecting mis-understandings and generating suitable feedback inan end-to-end dialogue system.
We have presentedour model in terms of a situated dialogue setting,where clues about what the hearer understood can beobserved directly.
However, we believe that the fun-damental mechanism should apply to other domainsas well.
This would amount to finding observablelinguistic and non-linguistic clues of hearer under-standing that can be used as features of pobs.The immediate next step for future research isto extend our model to an implemented end-to-endsituated NLG system for the GIVE Challenge, andevaluate whether this actually improves task perfor-mance.
This requires, in particular, to compute theRE that is optimal with respect to psem.
We will fur-thermore improve pobs by switching to a more tem-porally dynamic probability model.Acknowledgments.
We thank KonstantinaGaroufi and the anonymous reviewers for theirinsightful comments and suggestions.
The first twoauthors were supported by the SFB 632 ?Informa-tion Structure?
; Titov?s work was supported by theCluster of Excellence at Saarland University.1358ReferencesHendrik Buschmeier and Stefan Kopp.
2012.
Adaptinglanguage production to listener feedback behaviour.In Proceedings of the Interdisciplinary Workshop onFeedback Behaviors in Dialog.Herbert C. Clark.
1996.
Using Language.
CambridgeUniversity Press.Michael C. Frank and Noah D. Goodman.
2012.
Predict-ing pragmatic reasoning in language games.
Science,336(6084):998.Konstantina Garoufi and Alexander Koller.
2011.
Com-bining symbolic and corpus-based approaches for thegeneration of successful referring expressions.
In Pro-ceedings of the 13th European Workshop on NaturalLanguage Generation (ENLG).GIVE Organizers.
2012.
Give challenge web-site: Corpora.
http://give-challenge.org/research/page.php?id=corpora.Dave Golland, Percy Liang, and Dan Klein.
2010.
Agame-theoretic approach to generating spatial descrip-tions.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing(EMNLP).Graeme Hirst, Susan McRoy, Peter Heeman, Philip Ed-monds, and Diane Horton.
1994.
Repairing conver-sational misunderstandings and non-understandings.Speech Communications, 15:213?229.J.
D. Kelleher and J. van Genabith.
2004.
Visual salienceand reference resolution in simulated 3-D environ-ments.
Artificial Intelligence Review, 21(3).Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-tine Cassell, Robert Dale, Johanna Moore, and JonOberlander.
2010a.
The First Challenge on Generat-ing Instructions in Virtual Environments.
In E. Krah-mer and M. Theune, editors, Empirical Methods inNatural Language Generation, number 5790 in LNCS,pages 337?361.
Springer.Alexander Koller, Kristina Striegnitz, Andrew Gargett,Donna Byron, Justine Cassell, Robert Dale, JohannaMoore, and Jon Oberlander.
2010b.
Report on theSecond NLG Challenge on Generating Instructions inVirtual Environments (GIVE-2).
In Proceedings of the6th International Natural Language Generation Con-ference (INLG).Alexander Koller, Konstantina Garoufi, Maria Staudte,and Matthew Crocker.
2012.
Enhancing referentialsuccess by tracking hearer gaze.
In Proceedings of the13th Annual SIGdial Meeting on Discourse and Dia-logue (SIGDIAL), Seoul.Yukiko Nakano, Kazuyoshi Murata, Mika Enomoto,Yoshiko Arimoto, Yasuhiro Asa, and HirohikoSagawa.
2007.
Predicting evidence of understandingby monitoring user?s task manipulation in multimodalconversations.
In Proceedings of the ACL 2007 Demoand Poster Sessions.Tim Paek and Eric Horvitz.
1999.
Uncertainty, utility,and misunderstanding: A decision-theoretic perspec-tive on grounding in conversational systems.
In AAAIFall Symposium on Psychological Models of Commu-nication in Collaborative Systems.Kristina Striegnitz, Alexandre Denis, Andrew Gargett,Konstantina Garoufi, Alexander Koller, and MarietTheune.
2011.
Report on the Second Second Chal-lenge on Generating Instructions in Virtual Environ-ments (GIVE-2.5).
In Proceedings of the 13th Eu-ropean Workshop on Natural Language Generation(ENLG).David Traum.
1994.
A computational theory of ground-ing in natural language conversation.
Ph.D. thesis,University of Rochester.Adam Vogel, Christopher Potts, and Dan Jurafsky.2013.
Implicatures and nested beliefs in approximateDecentralized-POMDPs.
In Proceedings of ACL.1359
