Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 262?272,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsOptimizing Semantic Coherence in Topic ModelsDavid MimnoPrinceton UniversityPrinceton, NJ 08540mimno@cs.princeton.eduHanna M. WallachUniversity of Massachusetts, AmherstAmherst, MA 01003wallach@cs.umass.eduEdmund Talley Miriam LeendersNational Institutes of HealthBethesda, MD 20892{talleye,leenderm}@ninds.nih.govAndrew McCallumUniversity of Massachusetts, AmherstAmherst, MA 01003mccallum@cs.umass.eduAbstractLatent variable models have the potentialto add value to large document collectionsby discovering interpretable, low-dimensionalsubspaces.
In order for people to use suchmodels, however, they must trust them.
Un-fortunately, typical dimensionality reductionmethods for text, such as latent Dirichlet allocation, often produce low-dimensional sub-spaces (topics) that are obviously flawed tohuman domain experts.
The contributions ofthis paper are threefold: (1) An analysis of theways in which topics can be flawed; (2) an au-tomated evaluation metric for identifying suchtopics that does not rely on human annotatorsor reference collections outside the trainingdata; (3) a novel statistical topic model basedon this metric that significantly improves topicquality in a large-scale document collectionfrom the National Institutes of Health (NIH).1 IntroductionStatistical topic models such as latent Dirichlet allocation (LDA) (Blei et al, 2003) provide a pow-erful framework for representing and summarizingthe contents of large document collections.
In ourexperience, however, the primary obstacle to accep-tance of statistical topic models by users the outsidemachine learning community is the presence of poorquality topics.
Topics that mix unrelated or loosely-related concepts substantially reduce users?
confi-dence in the utility of such automated systems.In general, users prefer models with larger num-bers of topics because such models have greater res-olution and are able to support finer-grained distinc-tions.
Unfortunately, we have observed that thereis a strong relationship between the size of topicsand the probability of topics being nonsensical asjudged by domain experts: as the number of topicsincreases, the smallest topics (number of word to-kens assigned to each topic) are almost always poorquality.
The common practice of displaying only asmall number of example topics hides the fact that asmany as 10% of topics may be so bad that they can-not be shown without reducing users?
confidence.The evaluation of statistical topic models has tra-ditionally been dominated by either extrinsic meth-ods (i.e., using the inferred topics to perform someexternal task such as information retrieval (Weiand Croft, 2006)) or quantitative intrinsic methods,such as computing the probability of held-out doc-uments (Wallach et al, 2009).
Recent work hasfocused on evaluation of topics as semantically-coherent concepts.
For example, Chang et al (2009)found that the probability of held-out documents isnot always a good predictor of human judgments.Newman et al (2010) showed that an automatedevaluation metric based on word co-occurrencestatistics gathered from Wikipedia could predict hu-man evaluations of topic quality.
AlSumait et al(2009) used differences between topic-specific dis-tributions over words and the corpus-wide distribu-tion over words to identify overly-general ?vacuous?topics.
Finally, Andrzejewski et al (2009) devel-oped semi-supervised methods that avoid specificuser-labeled semantic coherence problems.The contributions of this paper are threefold: (1)To identify distinct classes of low-quality topics,some of which are not flagged by existing evalua-tion methods; (2) to introduce a new topic ?coher-ence?
score that corresponds well with human co-herence judgments and makes it possible to identify262specific semantic problems in topic models withouthuman evaluations or external reference corpora; (3)to present an example of a new topic model thatlearns latent topics by directly optimizing a metricof topic coherence.
With little additional computa-tional cost beyond that of LDA, this model exhibitssignificant gains in average topic coherence score.Although the model does not result in a statistically-significant reduction in the number of topics marked?bad?, the model consistently improves the topic co-herence score of the ten lowest-scoring topics (i.e.,results in bad topics that are ?less bad?
than thosefound using LDA) while retaining the ability to iden-tify low-quality topics without human interaction.2 Latent Dirichlet AllocationLDA is a generative probabilistic model for docu-mentsW = {w(1),w(2), .
.
.
,w(D)}.
To generate aword token w(d)n in document d, we draw a discretetopic assignment z(d)n from a document-specific dis-tribution over the T topics ?d (which is itself drawnfrom a Dirichlet prior with hyperparameter ?
), andthen draw a word type for that token from the topic-specific distribution over the vocabulary ?z(d)n .
Theinference task in topic models is generally cast as in-ferring the document?topic proportions {?1, ...,?D}and the topic-specific distributions {?1 .
.
.
,?T }.The multinomial topic distributions are usuallydrawn from a shared symmetric Dirichlet prior withhyperparameter ?, such that conditioned on {?t}Tt=1and the topic assignments {z(1), z(2), .
.
.
,z(D)},the word tokens are independent.
In practice, how-ever, it is common to deal directly with the ?col-lapsed?
distributions that result from integratingover the topic-specific multinomial parameters.
Theresulting distribution over words for a topic t is thena function of the hyperparameter ?
and the numberof words of each type assigned to that topic, Nw|t.This distribution, known as the Dirichlet compoundmultinomial (DCM) or Po?lya distribution (Doyleand Elkan, 2009), breaks the assumption of condi-tional independence between word tokens given top-ics, but is useful during inference because the con-ditional probability of a word w given topic t takesa very simple form: P (w | t, ?)
= Nw|t+?Nt+|V|?
, whereNt =?w?
Nw?|t and |V| is the vocabulary size.The process for generating a sequence of wordsfrom such a model is known as the simple Po?lya urnmodel (Mahmoud, 2008), in which the initial prob-ability of word type w in topic t is proportional to?, while the probability of each subsequent occur-rence of w in topic t is proportional to the numberof times w has been drawn in that topic plus ?.
Notethat this unnormalized weight for each word type de-pends only on the count of that word type, and is in-dependent of the count of any other word type w?.Thus, in the DCM/Po?lya distribution, drawing wordtype w must decrease the probability of seeing allother word types w?
6= w. In a later section, we willintroduce a topic model that substitutes a general-ized Po?lya urn model for the DCM/Po?lya distribu-tion, allowing a draw of word type w to increase theprobability of seeing certain other word types.For real-world data, documents W are observed,while the corresponding topic assignments Z areunobserved and may be inferred using either vari-ational methods (Blei et al, 2003; Teh et al, 2006)or MCMC methods (Griffiths and Steyvers, 2004).Here, we use MCMC methods?specifically Gibbssampling (Geman and Geman, 1984), which in-volves sequentially resampling each topic assign-ment z(d)n from its conditional posterior given thedocuments W , the hyperparameters ?
and ?, andZ\d,n (the current topic assignments for all tokensother than the token at position n in document d).3 Expert Opinions of Topic QualityConcentrating on 300,000 grant and related jour-nal paper abstracts from the National Institutes ofHealth (NIH), we worked with two experts fromthe National Institute of Neurological Disorders andStroke (NINDS) to collaboratively design an expert-driven topic annotation study.
The goal of this studywas to develop an annotated set of baseline topics,along with their salient characteristics, as a first steptowards automatically identifying and inferring thekinds of topics desired by domain experts.13.1 Expert-Driven Annotation ProtocolIn order to ensure that the topics selected for anno-tation were within the NINDS experts?
area of ex-pertise, they selected 148 topics (out of 500), all as-sociated with areas funded by NINDS.
Each topic1All evaluated models will be released publicly.263t was presented to the experts as a list of the thirtymost probable words for that topic, in descending or-der of their topic-specific ?collapsed?
probabilities,Nw|t+?Nt+|V|?
.
In addition to the most probable words,the experts were also given metadata for each topic:The most common sequences of two or more con-secutive words assigned to that topic, the four topicsthat most often co-occurred with that topic, the mostcommon IDF-weighted words from titles of grants,thesaurus terms, NIH institutes, journal titles, andfinally a list of the highest probability grants andPubMed papers for that topic.The experts first categorized each topic as oneof three types: ?research?, ?grant mechanisms andpublication types?
or ?general?.2 The quality ofeach topic (?good?, ?intermediate?, or ?bad?)
wasthen evaluated using criteria specific to the typeof topic.
In general, topics were only annotatedas ?good?
if they contained words that could begrouped together as a single coherent concept.
Addi-tionally, each ?research?
topic was only consideredto be ?good?
if, in addition to representing a sin-gle coherent concept, the aggregate content of theset of documents with appreciable allocations to thattopic clearly contained text referring to the conceptinferred from the topic words.
Finally, for each topicmarked as being either ?intermediate?
or ?bad?, oneor more of the following problems (defined by thedomain experts) was identified, as appropriate:?
Chained: every word is connected to everyother word through some pairwise word chain,but not all word pairs make sense.
For exam-ple, a topic whose top three words are ?acids?,?fatty?
and ?nucleic?
consists of two distinctconcepts (i.e., acids produced when fats arebroken down versus the building blocks ofDNA and RNA) chained via the word ?acids?.?
Intruded: either two or more unrelated setsof related words, joined arbitrarily, or an oth-erwise good topic with a few ?intruder?
words.?
Random: no clear, sensical connections be-tween more than a few pairs of words.?
Unbalanced: the top words are all logicallyconnected to each other, but the topic combinesvery general and specific terms (e.g., ?signal2Equivalent to ?vacuous topics?
of AlSumait et al (2009).transduction?
versus ?notch signaling?
).Examples of a good general topic, a good researchtopic, and a chained research topic are in Table 1.3.2 Annotation ResultsThe experts annotated the topics independently andthen aggregated their results.
Interestingly, no top-ics were ever considered ?good?
by one expert and?bad?
by the other?when there was disagreementbetween the experts, one expert always believed thetopic to be ?intermediate.?
In such cases, the ex-perts discussed the reasons for their decisions andcame to a consensus.
Of the 148 topics selected forannotation, 90 were labeled as ?good,?
21 as ?inter-mediate,?
and 37 as ?bad.?
Of the topics labeled as?bad?
or ?intermediate,?
23 were ?chained,?
21 were?intruded,?
3 were ?random,?
and 15 were ?unbal-anced?.
(The annotators were permitted to assignmore than one problem to any given topic.
)4 Automated Metrics for PredictingExpert AnnotationsThe ultimate goal of this paper is to develop meth-ods for building models with large numbers of spe-cific, high-quality topics from domain-specific cor-pora.
We therefore explore the extent to which in-formation already contained in the documents beingmodeled can be used to assess topic quality.In this section we evaluate several methods forranking the quality of topics and compare theserankings to human annotations.
No method is likelyto perfectly predict human judgments, as individualannotators may disagree on particular topics.
Foran application involving removing low quality top-ics we recommend using a weighted combination ofmetrics, with a threshold determined by users.4.1 Topic SizeAs a simple baseline, we considered the extent towhich topic ?size?
(as measured by the number oftokens assigned to each topic via Gibbs sampling) isa good metric for assessing topic quality.
Figure 1(top) displays the topic size (number of tokens as-signed to that topic) and expert annotations (?good?,?intermediate?, ?bad?)
for the 148 topics manuallylabeled by annotators as described above.
This fig-ure suggests that topic size is a reasonable predic-264lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll40000 60000 80000 120000 160000Tokensgoodinterbadlllllllllllllllllllllllllllllllllllllllllllllllllll?600 ?500 ?400 ?300 ?200CoherencegoodinterbadFigure 1: Topic size is a good indicator of quality; thenew coherence metric is better.
Top shows expert-ratedtopics ranked by topic size (AP 0.89, AUC 0.79), bottomshows same topics ranked by coherence (AP 0.94, AUC0.87).
Random jitter is added to the y-axis for clarity.tor of topic quality.
Although there is some overlap,?bad?
topics are generally smaller than ?good?
top-ics.
Unfortunately, this observation conflicts withthe goal of building highly specialized, domain-specific topic models with many high-quality, fine-grained topics?in such models the majority of top-ics will have relatively few tokens assigned to them.4.2 Topic CoherenceWhen displaying topics to users, each topic t is gen-erally represented as a list of theM=5, .
.
.
, 20 mostprobable words for that topic, in descending orderof their topic-specific ?collapsed?
probabilities.
Al-though there has been previous work on automatedgeneration of labels or headings for topics (Mei etal., 2007), we choose to work only with the orderedlist representation.
Labels may obscure or detractfrom fundamental problems with topic coherence,and better labels don?t make bad topics good.The expert-driven annotation study described insection 3 suggests that three of the four types ofpoor-quality topics (?chained,?
?intruded?
and ?ran-dom?)
could be detected using a metric based onthe co-occurrence of words within the documentsbeing modeled.
For ?chained?
and ?intruded?
top-ics, it is likely that although pairs of words belong-ing to a single concept will co-occur within a singledocument (e.g., ?nucleic?
and ?acids?
in documentsabout DNA), word pairs belonging to different con-cepts (e.g., ?fatty?
and ?nucleic?)
will not.
For ran-dom topics, it is likely that few words will co-occur.This insight can be used to design a new metricfor assessing topic quality.
Letting D(v) be the doc-ument frequency of word type v (i.e., the numberof documents with least one token of type v) andD(v, v?)
be co-document frequency of word types vand v?
(i.e., the number of documents containing oneor more tokens of type v and at least one token oftype v?
), we define topic coherence asC(t;V (t)) =M?m=2m?1?l=1log D(v(t)m , v(t)l ) + 1D(v(t)l ), (1)where V (t) =(v(t)1 , .
.
.
, v(t)M ) is a list of the M mostprobable words in topic t. A smoothing count of 1is included to avoid taking the logarithm of zero.Figure 1 shows the association between the expertannotations and both topic size (top) and our coher-ence metric (bottom).
We evaluate these results us-ing standard ranking metrics, average precision andthe area under the ROC curve.
Treating ?good?
top-ics as positive and ?intermediate?
or ?bad?
topics asnegative, we get average precision values of 0.89 fortopic size vs. 0.94 for coherence and AUC 0.79 fortopic size vs. 0.87 for coherence.
We performed alogistic regression analysis on the binary variable ?isthis topic bad?.
Using topic size alone as a predic-tor gives AIC (a measure of model fit) 152.5.
Co-herence alone has AIC 113.8 (substantially better).Both predictors combined have AIC 115.8: the sim-pler coherence alone model provides the best perfor-mance.
We tried weighting the terms in equation 1by their corresponding topic?word probabilities andand by their position in the sorted list of the M mostprobable words for that topic, but we found that auniform weighting better predicted topic quality.Our topic coherence metric also exhibits goodqualitative behavior: of the 20 best-scoring topics,18 are labeled as ?good,?
one is ?intermediate?
(?un-balanced?
), and one is ?bad?
(combining ?cortex?and ?fmri?, words that commonly co-occur, but areconceptually distinct).
Of the 20 worst scoring top-ics, 15 are ?bad,?
4 are ?intermediate,?
and only one(with the 19th worst coherence score) is ?good.
?265Our coherence metric relies only upon word co-occurrence statistics gathered from the corpus beingmodeled, and does not depend on an external ref-erence corpus.
Ideally, all such co-occurrence infor-mation would already be accounted for in the model.We believe that one of the main contributions of ourwork is demonstrating that standard topic modelsdo not fully utilize available co-occurrence informa-tion, and that a held-out reference corpus is thereforenot required for purposes of topic evaluation.Equation 1 is very similar to pointwise mutual in-formation (PMI), but is more closely associated withour expert annotations than PMI (which achievesAUC 0.64 and AIC 170.51).
PMI has a long historyin language technology (Church and Hanks, 1990),and was recently used by Newman et al (2010) toevaluate topic models.
When expressed in terms ofcount variables as in equation 1, PMI includes anadditional term for D(v(t)m ).
The improved perfor-mance of our metric over PMI implies that what mat-ters is not the difference between the joint probabil-ity of words m and l and the product of marginals,but the conditional probability of each word giventhe each of the higher-ranked words in the topic.In order to provide intuition for the behavior ofour topic coherence metric, table 1 shows threeexample topics and their topic coherence scores.The first topic, related to grant-funded training pro-grams, is one of the best-scoring topics.
All pairsof words have high co-document frequencies.
Thesecond topic, on neurons, is more typical of qual-ity ?research?
topics.
Overall, these words occurless frequently, but generally occur moderately in-terchangeably: there is little structure to their co-variance.
The last topic is one of the lowest-scoringtopics.
Its co-document frequency matrix is shownin table 2.
The top two words are closely related:487 documents include ?aging?
at least once, 122include ?lifespan?, and 55 include both.
Meanwhile,the third word ?globin?
occurs with only one of thetop seven words?the common word ?human?.4.3 Comparison to word intrusionAs an additional check for both our expert annota-tions and our automated metric, we replicated the?word intrusion?
evaluation originally introduced byChang et al (2009).
In this task, one of the top tenmost probable words in a topic is replaced with al lll l lll ll l lll ll l lll lll l lll ll lll l l ll l lllllll l lll l lll ll l lllll ll ll lll llll lllll l40000 60000 80000 120000 160000048Comparison of Topic Size to Intrusion DetectionTokens assigned to topicCorrectGuessesll ll ll ll l ll ll ll l ll ll llll l lll llll ll ll ll lll lll lll lllll ll l llllll ll ll llll lllll l?600 ?500 ?400 ?300 ?200048Comparison of Coherence to Intrusion DetectionCoherenceCorrectGuessesGood TopicsCorrect GuessesFrequency0 2 4 6 8 1001535Bad TopicsCorrect GuessesFrequency0 2 4 6 8 1001535Figure 2: Top: results of the intruder selection task rel-ative to two topic quality metrics.
Bottom: marginal in-truder accuracy frequencies of good and bad topics.another word, selected at random from the corpus.The resulting set of words is presented, in a randomorder, to users, who are asked to identify the ?in-truder?
word.
It is very unlikely that a randomly-chosen word will be semantically related to any ofthe original words in the topic, so if a topic is ahigh quality representation of a semantically coher-ent concept, it should be easy for users to select theintruder word.
If the topic is not coherent, there maybe words in the topic that are also not semanticallyrelated to any other word, thus causing users to se-lect ?correct?
words instead of the real intruder.We recruited ten additional expert annotatorsfrom NINDS, not including our original annotators,and presented them with the intruder selection task,using the set of previously evaluated topics.
Re-sults are shown in figure 2.
In the first two plots,the x-axis is one of our two automated quality met-266Table 1: Example topics (good/general, good/research, chained/research) with different coherence scores (numberscloser to zero indicate higher coherence).
The chained topic combines words related to aging (indicated in plain text)and words describing blood and blood-related diseases (bold).
The only connection is the common word human.-167.1 students, program, summer, biomedical, training, experience, undergraduate, career, minority, student, ca-reers, underrepresented, medical students, week, science-252.1 neurons, neuronal, brain, axon, neuron, guidance, nervous system, cns, axons, neural, axonal, cortical,survival, disorders, motor-357.2 aging, lifespan, globin, age related, longevity, human, age, erythroid, sickle cell, beta globin, hb, senes-cence, adult, older, lcrTable 2: Co-document frequency matrix for the top words in a low-quality topic (according to our coherence metric),shaded to highlight zeros.
The diagonal (light gray) shows the overall document frequency for each word w. Thecolumn on the right is Nw|t.
Note that ?globin?
and ?erythroid?
do not co-occur with any of the aging-related words.aging 487 53 0 65 42 0 51 0 138 0 914lifespan 53 122 0 15 28 0 15 0 44 0 205globin 0 0 39 0 0 19 0 15 27 3 200age related 65 15 0 119 12 0 25 0 37 0 160longevity 42 28 0 12 73 0 6 0 20 1 159erythroid 0 0 19 0 0 69 0 8 23 1 110age 51 15 0 25 6 0 245 1 82 0 103sickle cell 0 0 15 0 0 8 1 43 16 2 93human 138 44 27 37 20 23 82 16 4347 157 91hb 0 0 3 0 1 1 0 2 5 15 73267rics (topic size and coherence) and the y-axis is thenumber of annotators that correctly identified thetrue intruder word (accuracy).
The histograms be-low these plots show the number of topics with eachlevel of annotator accuracy for good and bad top-ics.
For good topics (green circles), the annotatorswere generally able to detect the intruder word withhigh accuracy.
Bad topics (red diamonds) had moreuniform accuracies.
These results suggest that top-ics with low intruder detection accuracy tend to bebad, but some bad topics can have a high accuracy.For example, spotting an intruder word in a chainedtopic can be easy.
The low-quality topic recep-tors, cannabinoid, cannabinoids, ligands, cannabis,endocannabinoid, cxcr4, [virus], receptor, sdf1, isa typical ?chained?
topic, with CXCR4 linked tocannabinoids only through receptors, and otherwiseunrelated.
Eight out of ten annotators correctly iden-tified ?virus?
as the correct intruder.
Repeating thelogistic regression experiment using intruder detec-tion accuracy as input, the AIC value is 163.18?much worse than either topic size or coherence.5 Generalized Po?lya Urn ModelsAlthough the topic coherence metric defined aboveprovides an accurate way of assessing topic quality,preventing poor quality topics from occurring in thefirst place is preferable.
Our results in the previoussection show that we can identify low-quality top-ics without making use of external supervision; thetraining data by itself contains sufficient informationat least to reject poor combinations of words.In this section, we describe a new topic model thatincorporates the corpus-specific word co-occurrenceinformation used in our coherence metric directlyinto the statistical topic modeling framework.
Itis important to note that simply disallowing wordsthat never co-occur from being assigned to the sametopic is not sufficient.
Due to the power-law charac-teristics of language, most words are rare and willnot co-occur with most other words regardless oftheir semantic similarity.
It is rather the degreeto which the most prominent words in a topic donot co-occur with the other most prominent wordsin that topic that is an indicator of topic incoher-ence.
We therefore desire models that guide topicstowards semantic similarity without imposing hardconstraints.As an example of such a model, we present a newtopic model in which the occurrence of word type win topic t increases not only the probability of seeingthat word type again, but also increases the probabil-ity of seeing other related words (as determined byco-document frequencies for the corpus being mod-eled).
This new topic model retains the document?topic component of standard LDA, but replaces theusual Po?lya urn topic?word component with a gen-eralized Po?lya urn framework (Mahmoud, 2008).A sequence of i.i.d.
samples from a discrete dis-tribution can be imagined as arising by repeatedlydrawing a random ball from an urn, where the num-ber of balls of each color is proportional to the prob-ability of that color, replacing the selected ball af-ter each draw.
In a Po?lya urn, each ball is replacedalong with another ball of the same color.
Samplesfrom this model exhibit the ?burstiness?
property:the probability of drawing a ball of colorw increaseseach time a ball of that color is drawn.
This processrepresents the marginal distribution of a hierarchicalmodel with a Dirichlet prior and a multinomial like-lihood, and is used as the distribution over wordsfor each topic in almost all previous topic models.In a generalized Po?lya urn model, having drawn aball of color w, Avw additional balls of each colorv ?
{1, .
.
.
,W} are returned to the urn.
Given Wand Z , the conditional posterior probability of wordw in topic t implied by this generalized model isP (w | t,W,Z, ?,A) =?vNv|tAvw + ?Nt + |V|?, (2)where A is a W ?
W real-valued matrix, knownas the addition matrix or schema.
The simple Po?lyaurn model (and hence the conditional posterior prob-ability of word w in topic t under LDA) can be re-covered by setting the schema A to the identity ma-trix.
Unlike the simple Po?lya distribution, we do notknow of a representation of the generalized Po?lyaurn distribution that can be expressed using a con-cise set of conditional independence assumptions.
Astandard graphical model with plate notation wouldtherefore not be helpful in highlighting the differ-ences between the two models, and is not shown.Algorithm 1 shows pseudocode for a single Gibbssweep over the latent variables Z in standard LDA.Algorithm 2 shows the modifications necessary to2681: for d ?
D do2: for wn ?
w(d) do3: Nzi|di ?
Nzi|di ?
14: Nwi|zi ?
Nwi|zi ?
15: sample zi ?
(Nz|di + ?z)Nwi|z+??z?
(Nwi|z?+?
)6: Nzi|di ?
Nzi|di + 17: Nwi|zi ?
Nwi|zi + 18: end for9: end forAlgorithm 1: One sweep of LDA Gibbs sampling.1: for d ?
D do2: for wn ?
w(d) do3: Nzi|di ?
Nzi|di ?
14: for all v do5: Nv|zi ?
Nv|zi ?Avwi6: end for7: sample zi ?
(Nz|di + ?z)Nwi|z+??z?
(Nwi|z?+?
)8: Nzi|di ?
Nzi|di + 19: for all v do10: Nv|zi ?
Nv|zi +Avwi11: end for12: end for13: end forAlgorithm 2: One sweep of gen. Po?lya Gibbs sam-pling, with differences from LDA highlighted in red.support a generalized Po?lya urn model: rather thansubtracting exactly one from the count of the wordgiven the old topic, sampling, and then adding oneto the count of the word given the new topic, we sub-tract a column of the schema matrix from the entirecount vector over words for the old topic, sample,and add the same column to the count vector for thenew topic.
As long as A is sparse, this operationadds only a constant factor to the computation.Another property of the generalized Po?lya urnmodel is that it is nonexchangeable?the joint prob-ability of the tokens in any given topic is not invari-ant to permutation of those tokens.
Inference of ZgivenW via Gibbs sampling involves repeatedly cy-cling through the tokens in W and, for each one,resampling its topic assignment conditioned on Wand the current topic assignments for all tokens otherthan the token of interest.
For LDA, the samplingdistribution for each topic assignment is simply theproduct of two predictive probabilities, obtained bytreating the token of interest as if it were the last.For a topic model with a generalized Po?lya urn forthe topic?word component, the sampling distribu-tion is more complicated.
Specifically, the topic?word component of the sampling distribution is nolonger a simple predictive distribution?when sam-pling a new value for z(d)n , the implication of eachpossible value for subsequent tokens and their topicassignments must be considered.
Unfortunately, thiscan be very computationally expensive, particularlyfor large corpora.
There are several ways around thisproblem.
The first is to use sequential Monte Carlomethods, which have been successfully applied totopic models previously (Canini et al, 2009).
Thesecond approach is to approximate the true Gibbssampling distribution by treating each token as if itwere the last, ignoring implications for subsequenttokens and their topic assignments.
We find thatthis approximate method performs well empirically.5.1 Setting the Schema AInspired by our evaluation metric, we define A asAvv ?
?vD(v) (3)Avw ?
?vD(w, v)where each element is scaled by a row-specificweight ?v and each column is normalized to sumto 1.
Normalizing columns makes comparison tostandard LDA simpler, because the relative effect ofsmoothing parameter ?=0.01 is equivalent.
We set?v = log (D/D(v)), the standard IDF weight usedin information retrieval, which is larger for less fre-quent words.
The column for word type w can beinterpreted as word types with significant associa-tion with w. The IDF weighting therefore has theeffect of increasing the strength of association forrare word types.
We also found empirically that it ishelpful to remove off-diagonal elements for the mostcommon types, such as those that occur in more than5% of documents (IDF < 3.0).
Including nonzerooff-diagonal values in A for very frequent typescauses the model to disperse those types over manytopics, which leads to large numbers of extremelysimilar topics.
To measure this effect, we calcu-lated the Jensen-Shannon divergence between allpairs of topic?word distributions in a given model.For a model using off-diagonal weights for all word269?290?260100 TopicsCoherence50 300 550 800?290?260200 TopicsCoherence50 300 550 800?290?260300 TopicsCoherence50 300 550 800?290?260400 TopicsCoherence50 300 550 800?400?34010WorstCoher50 300 550 800?400?34010WorstCoher50 300 550 800?400?34010WorstCoher50 300 550 800?400?34010WorstCoher50 300 550 800?1700?1660IterationHOLP50 300 550 800?1700?1660IterationHOLP50 300 550 800?1700?1660IterationHOLP50 300 550 800?1700?1660IterationHOLP50 300 550 800Figure 3: Po?lya urn topics (blue) have higher average coherence and converge much faster than LDA topics(red).
The top plots show topic coherence (averaged over 15 runs) over 1000 iterations of Gibbs sampling.
Error barsare not visible in this plot.
The middle plot shows the average coherence of the 10 lowest scoring topics.
The bottomplots show held-out log probability (in thousands) for the same models (three runs each of 5-fold cross-validation).Name Docs Avg.
Tok.
Tokens VocabNIH 18756 114.64 ?
30.41 2150172 28702Table 3: Data set statistics.types, the mean of the 100 lowest divergences was0.29 ?
.05 (a divergence of 1.0 represents distribu-tions with no shared support) at T =200.
The aver-age divergence of the 100 most similar pairs of top-ics for standard LDA (i.e.,A = I) is 0.67?.05.
Thesame statistic for the generalized Po?lya urn modelwithout off-diagonal elements for word types withhigh document frequency is 0.822?
0.09.Setting the off-diagonal elements of the schemaA to zero for the most common word types also hasthe fortunate effect of substantially reducing prepro-cessing time.
We find that Gibbs sampling for thegeneralized Po?lya model takes roughly two to threetimes longer than for standard LDA, depending onthe sparsity of the schema, due to additional book-keeping needed before and after sampling topics.5.2 Experimental ResultsWe evaluated the new model on a corpus of NIHgrant abstracts.
Details are given in table 3.
Figure 3shows the performance of the generalized Po?lya urnmodel relative to LDA.
Two metrics?our new topiccoherence metric and the log probability of held-outdocuments?are shown over 1000 iterations at 50 it-eration intervals.
Each model was run over five foldsof cross validation, each with three random initial-izations.
For each model we calculated an overallcoherence score by calculating the topic coherencefor each topic individually and then averaging thesevalues.
We report the average over all 15 models ineach plot.
Held-out probabilities were calculated us-ing the left-to-right method of Wallach et al (2009),with each cross-validation fold using its own schemaA.
The generalized Po?lya model performs very wellin average topic coherence, reaching levels withinthe first 50 iterations that match the final score.
Thismodel has an early advantage for held-out proba-bility as well, but is eventually overtaken by LDA.This trend is consistent with Chang et al?s observa-tion that held-out probabilities are not always goodpredictors of human judgments (Chang et al, 2009).Results are consistent over T ?
{100, 200, 300}.In section 4.2, we demonstrated that our topic co-herence metric correlates with expert opinions oftopic quality for standard LDA.
The generalized270Po?lya urn model was therefore designed with thegoal of directly optimizing that metric.
It is pos-sible, however, that optimizing for coherence di-rectly could break the association between coher-ence metric and topic quality.
We therefore repeatedthe expert-driven evaluation protocol described insection 3.1.
We trained one standard LDA modeland one generalized Po?lya urn model, each withT = 200, and randomly shuffled the 400 resultingtopics.
The topics were then presented to the expertsfrom NINDS, with no indication as to the identity ofthe model from which each topic came.
As theseevaluations are time consuming, the experts evalu-ated the only the first 200 topics, which consisted of103 generalized Po?lya urn topics and 97 LDA top-ics.
AUC values predicting bad topics given coher-ence were 0.83 and 0.80, respectively.
Coherenceeffectively predicts topic quality in both models.Although we were able to improve the averageoverall quality of topics and the average quality ofthe ten lowest-scoring topics, we found that the gen-eralized Po?lya urn model was less successful reduc-ing the overall number of bad topics.
Ignoring one?unbalanced?
topic from each model, 16.5% of theLDA topics and 13.5% from the generalized Po?lyaurn model were marked as ?bad.?
While this resultis an improvement, it is not significant at p = 0.05.6 DiscussionWe have demonstrated the following:?
There is a class of low-quality topics that can-not be detected using existing word-intrusiontests, but that can be identified reliably using ametric based on word co-occurrence statistics.?
It is possible to improve the coherence scoreof topics, both overall and for the ten worst,while retaining the ability to flag bad topics, allwithout requiring semi-supervised data or ad-ditional reference corpora.
Although additionalinformation may be useful, it is not necessary.?
Such models achieve better performance withsubstantially fewer Gibbs iterations than LDA.We believe that the most important challenges in fu-ture topic modeling research are improving the se-mantic quality of topics, particularly at the low end,and scaling to ever-larger data sets while ensuringhigh-quality topics.
Our results provide critical in-sight into these problems.
We found that it should bepossible to construct unsupervised topic models thatdo not produce bad topics.
We also found that Gibbssampling mixes faster for models that use word co-occurrence information, suggesting that such meth-ods may also be useful in guiding online stochasticvariational inference (Hoffman et al, 2010).AcknowledgementsThis work was supported in part by the Centerfor Intelligent Information Retrieval, in part by theCIA, the NSA and the NSF under NSF grant # IIS-0326249, in part by NIH:HHSN271200900640P,and in part by NSF # number SBE-0965436.
Anyopinions, findings and conclusions or recommenda-tions expressed in this material are the authors?
anddo not necessarily reflect those of the sponsor.ReferencesLoulwah AlSumait, Daniel Barbara, James Gentle, andCarlotta Domeniconi.
2009.
Topic significance rank-ing of LDA generative models.
In ECML.David Andrzejewski, Xiaojin Zhu, and Mark Craven.2009.
Incorporating domain knowledge into topicmodeling via Dirichlet forest priors.
In Proceedings ofthe 26th Annual International Conference on MachineLearning, pages 25?32.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:993?1022, January.K.R.
Canini, L. Shi, and T.L.
Griffiths.
2009.
Onlineinference of topics with latent Dirichlet alocation.
InProceedings of the 12th International Conference onArtificial Intelligence and Statistics.Jonathan Chang, Jordan Boyd-Graber, Chong Wang,Sean Gerrish, and David M. Blei.
2009.
Reading tealeaves: How humans interpret topic models.
In Ad-vances in Neural Information Processing Systems 22,pages 288?296.Kenneth Church and Patrick Hanks.
1990.
Word asso-ciation norms, mutual information, and lexicography.Computational Linguistics, 6(1):22?29.Gabriel Doyle and Charles Elkan.
2009.
Accounting forburstiness in topic models.
In ICML.S.
Geman and D. Geman.
1984.
Stochastic relaxation,Gibbs distributions, and the Bayesian restoration ofimages.
IEEE Transaction on Pattern Analysis andMachine Intelligence 6, pages 721?741.271Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101(suppl.
1):5228?5235.Matthew Hoffman, David Blei, and Francis Bach.
2010.Online learning for latent dirichlet alocation.
In NIPS.Hosan Mahmoud.
2008.
Po?lya Urn Models.
Chapman& Hall/CRC Texts in Statistical Science.Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.2007.
Automatic labeling of multinomial topic mod-els.
In Proceedings of the 13th ACM SIGKDD Interna-tional Conference on Knowledge Discovery and DataMining, pages 490?499.David Newman, Jey Han Lau, Karl Grieser, and TimothyBaldwin.
2010.
Automatic evaluation of topic coher-ence.
In Human Language Technologies: The AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics.Yee Whye Teh, Dave Newman, and Max Welling.
2006.A collapsed variational Bayesian inference algorithmfor lat ent Dirichlet alocation.
In Advances in NeuralInformation Processing Systems 18.Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, andDavid Mimno.
2009.
Evaluation methods for topicmodels.
In Proceedings of the 26th Interational Con-ference on Machine Learning.Xing Wei and Bruce Croft.
2006.
LDA-based documentmodels for ad-hoc retrival.
In Proceedings of the 29thAnnual International SIGIR Conference.272
