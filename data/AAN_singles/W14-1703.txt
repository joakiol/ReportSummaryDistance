Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 25?33,Baltimore, Maryland, 26-27 July 2014.c?2014 Association for Computational LinguisticsThe AMU System in the CoNLL-2014 Shared Task:Grammatical Error Correction by Data-Intensiveand Feature-Rich Statistical Machine TranslationMarcin Junczys-Dowmunt Roman GrundkiewiczInformation Systems LaboratoryAdam Mickiewicz Universityul.
Umultowska 87, 61-614 Pozna?n, Poland{junczys,romang}@amu.edu.plAbstractStatistical machine translation toolkits likeMoses have not been designed with gram-matical error correction in mind.
In or-der to achieve competitive results in thisarea, it is not enough to simply add moredata.
Optimization procedures need to becustomized, task-specific features shouldbe introduced.
Only then can the decodertake advantage of relevant data.We demonstrate the validity of the aboveclaims by combining web-scale languagemodels and large-scale error-correctedtexts with parameter tuning according tothe task metric and correction-specific fea-tures.
Our system achieves a result of35.0% F0.5on the blind CoNLL-2014 testset, ranking on third place.
A similar sys-tem, equipped with identical models butwithout tuned parameters and specializedfeatures, stagnates at 25.4%.1 IntroductionThere has been an increasing interest in using sta-tistical machine translation (SMT) for the taskof grammatical error correction.
Among the 16teams that took part in the CoNLL-2013 SharedTask (Ng et al., 2013), four teams described ap-proaches that fully or partially used SMT in theirsystem.
While in the previous year the correc-tion task was restricted to just five error types,the CoNLL-2014 Shared Task (Ng et al., 2014)now requires a participating system to correct all28 error types present in NUCLE (Dahlmeier etal., 2013).
Since the high number of error typeshas made it harder to target each error categorywith dedicated components, SMT with its abil-ity to learn generic text transformations is now aneven more appealing approach.With out-of-the-box machine translation toolk-its like Moses (Koehn et al., 2007) being freelyavailable, the application of SMT to grammati-cal error correction seems straightforward.
How-ever, Moses has not been designed as a grammarcorrection system, the standard features and opti-mization methods are geared towards translationperformance measured by the metrics used in theSMT field.
Training Moses on data that is relevantfor grammatical error correction is a step in theright direction, but data alone is not enough.
Thedecoder needs to be able to judge the data based onrelevant features, parameter optimization needs tobe performed according to relevant metrics.This paper constitutes the description of theAdam Mickiewicz University (AMU) submissionto the CoNLL-2014 Shared Task on GrammaticalError Correction.
We explore the interaction oflarge-scale data, parameter optimization, and task-specific features in a Moses-based system.
Relatedwork is presented in the next section, the systemsetup is shortly described in Section 3.
Sections 4to 7 contain our main contributions.In Section 4, we describe our implementation offeature weights tuning according to the MaxMatch(M2) metric by Dahlmeier and Ng (2012b) whichis the evaluation metric of the current CoNLL-2014 Shared Task.
Sections 5 and 6 deal with thedata-intensive aspects of our paper.
We start byextending the baseline system with a Wikipedia-based language model and finish with a web-scalelanguage model estimated from CommonCrawldata.
Uncorrected/corrected data from the sociallanguage learner?s platform Lang-8 is used to ex-tend the translation models of our system.Task-specific dense and sparse features are in-troduced in Section 7.
These features are meant toraise the ?awareness?
of the decoder for grammat-ical error correction.
In Section 8, we discuss theresults of our submission and several intermediatesystems on the blind CoNLL-2014 test set.252 Related WorkBrockett et al.
(2006) use SMT to correct count-ability errors for a set of 14 mass nouns that poseproblems to Chinese ESL learners.
For this veryrestricted task they achieve a results of 61.81%corrected mistakes.
This work mentions minimumerror rate tuning according to BLEU.A Moses-based system is described by Mizu-moto et al.
(2011) who correct grammatical errorsof learners of Japanese.
This work is continued forEnglish in Mizumoto et al.
(2012).
The effect oflearner corpus size on various types of grammat-ical errors is investigated.
The additional large-scale data originates from the social learner?s plat-form Lang-8.
We use similar resources.Very interesting work is presented byDahlmeier and Ng (2012a).
A custom beam-search decoder for grammatical error correctionis introduced that incorporates discriminativeclassifiers for specific error categories such asarticles and prepositions.
The authors performparameter tuning and find PRO to work betterwith M21than MERT1.
The specialized decodertuned with M21is compared to Moses that has beentuned with BLEU.
As we show in Section 4.2, thiscannot be a fair comparison.The CoNLL-2013 Shared Task (Ng et al., 2013)saw a number of systems based entirely or par-tially on translation approaches.
Most notableare Yuan and Felice (2013) and Yoshimoto et al.(2013).
Yuan and Felice (2013) apply Moses to allfive error types of the shared task and extend theprovided training data by adding other learner?scorpora.
They also experiment with generatingartificial errors.
Improvement over the baselineare small, but their approach to generate errorsshows promise.
We successfully re-implementtheir baseline.
Yoshimoto et al.
(2013) use Mosesfor two error classes, prepositions and determin-ers, for other classes they find classifier-based ap-proaches and treelet language models to performbetter.
None of the CoNLL-2013 SMT-based sys-tems seems to use parameter tuning.3 General System SetupOur system is based on the phrase-based part ofthe statistical machine translation system Moses(Koehn et al., 2007).
Only plain text data is usedfor language model and translation model training.1This is different from our findings for Moses, but may bea property of their custom decoder.External linguistic knowledge is introduced dur-ing parameter tuning as the tuning metric relies onthe error annotation present in NUCLE.
Phrase ta-bles are binarized with the compact phrase table(Junczys-Dowmunt, 2012), no reordering modelsare used, the distortion limit is set to 0, effectivelyprohibiting any reordering.
Apart from that, ourbasic setup is very similar to that of Yuan and Fe-lice (2013).
We adapted their 4-fold cross valida-tion scheme on NUCLE to our needs and use asimilar baseline, now with 28 error types.4 Parameter TuningThe training of feature functions like translationmodels or language models is only half the workrequired to produce a state-of-the-art statisticalmachine translation system.
The other half relieson parameter tuning.During translation, Moses scores translations eof string f by a log-linear modellog p(e|f) =?i?ilog(hi(e, f))where hiare feature functions and ?iare featureweights.
Without parameter tuning, results maybe questionable as the choice of feature functionweights (everything else being identical) can turna mediocre system into a high-scoring system orrender a good system useless.
This is illustrated inSection 4.2 and by examples throughout the paper.All our modifications to MERT, PRO, kb-MIRAdiscussed in this section are publicly available2.4.1 Tuning SchemeTo accommodate for parameter tuning, we mod-ify the standard 4-fold cross validation procedure.The test set in each of the four training/testing runsis again divided into two halves.
The first half istreated as a tuning set, the second half as a test set.Next, tuning set and test set are inverted in orderto tune and test a second time.
Altogether, we per-form four separate translation model training stepsand eight tuning/testing steps.
Each tuning/test setconsists of ca.
7,000 sentences.
We call this pro-cedure 4?2-fold cross validation (4?2-CV).
Thisway the entire NUCLE corpus serves as training,test, and tuning set.
We also evaluate all our re-sults on the CoNLL-2013 gold standard (ST-2013)which has been made available with 28 error typesafter the previous shared task.2https://github.com/moses-smt/mosesdecoder/fscorer264?2-CV ST-2013Tuned with BLEU M20.5BLEU M20.5Untuned 85.52 14.02 70.38 19.05BLEU 88.31 1.27 72.62 1.12M20.587.76 15.43 71.99 16.73Original 89.51 0.00 72.67 0.00Table 1: Tuning with BLEU and M24.2 Tuning MetricWe refer to F0.5computed by the M2metric asM20.5.
Moses is bundled with several tuning toolsthat can tune parameter vectors according to dif-ferent MT tuning metrics.
The most widely usedis BLEU (Papineni et al., 2002).
We first attemptminimum error rate tuning (MERT) (Och, 2003)with BLEU, results are shown in Table 1.
WhileBLEU scores increase on both, 4?2-CV and ST-2013, the effect on M20.5is catastrophic3thoughnot surprising.
The baseline is so weak that it in-troduces more errors than corrections, thus lower-ing the similarity of the output and the referencebelow the level of the similarity of the input andthe reference.
MERT learns parameter weightsthat disable nearly all correction attempts.The obvious solution is to tune directly withM2.
M2provides per-sentence sufficient statisticsand can easily4be integrated with MERT.
We re-tune with M2and see an improvement on 4?2-CVbut a significant decrease for ST-2013.
BLEU in-creases for this system despite the drop in M2.This seems contradictory, but actually provesour point about the necessity of parameter tuning.Good luck should not be a basis for choosing pa-rameters, in the case of a blind submission we havea much better chance to reach good results bettingon optimized parameters.
As we see later, this sit-uation does not occur again for the more advancedsystems, tuned parameters do generally better.4.3 Parameter SmoothingBased on the results of Clark et al.
(2011), it hasbecome good practice to tune systems betweenthree and five times and report average results inorder to cope with optimizer instability.
Cettolo etal.
(2011) expand on this work and explore param-3Which might explain why none of the Moses-basedCoNLL-2013 systems used parameter tuning.4We run the original m2scorer Python code with an em-bedded Python interpreter in MERT?s C++ source code.System Concat.
AverageNUCLE 15.16 15.43NUCLE+CCLM 22.03 22.19Final 25.93 26.26Table 2: Effects of parameter weight smoothingon three selected systems for 4?2-CV (CoNLL-2014)eter smoothing methods for different parametervectors obtained on the same tuning sets.
They re-port that parameter vector centroids averaged overseveral tuning runs yield better than average re-sults and reduce variation.
Tuning three to fivetimes would require 24 to 40 tuning runs in oursetup.
However, we already have eight parame-ter vectors obtained from distinct tuning sets anddecide to average these parameters.
This way wehope to obtain a single vector of smoothed param-eters that represents the entire NUCLE corpus.Eventually, we retranslate the test sets accord-ing to 4-fold cross validation using the respectivetraining data with this parameter vector.
The sameparameters are later used with the full training datato translate the CoNLL-2013 test set and the blindCoNLL-2014 test set.
As it turns out, averagingparameter vectors across all parts has a consis-tently positive effect for M2.
This is shown inTable 2, systems mentioned in the table are intro-duced in Section 5 and Section 7.2.4.4 Tuning Sparse Feature WeightsTuning sparse features (Section 7.2) with M2poses an unexpected challenge.
Moses im-plements two methods for feature-rich tuning:PRO (Hopkins and May, 2011) and Batch k-bestMIRA (kb-MIRA) (Cherry and Foster, 2012) thatboth function as drop-in replacements for MERT.MERT cannot be used directly with sparse fea-tures.
When BLEU is used as a tuning metric,Koehn and Haddow (2012) report results for PROon a par with MERT for a system with only densefeatures.
Unfortunately, this cannot be confirmedfor M2; we consistently see worse results than forMERT using PRO or kb-MIRA.PRO and kb-MIRA operate on sentence-levelwhile MERT computes M2for the complete cor-pus.
Similar to Dahlmeier and Ng (2012a), we usesentence-level M2as an approximation.
We sus-pect that M2might not be distinctive enough in a2714.016.018.020.022.024.026.028.0106 107 108 109 1010 1011 1012M2  [%]Corpus size (tokens)4x2-CV CoNLL-2014ST-2013 CoNLL-2014Figure 1: Language model corpus size versus M2sentence-based scenario.Koehn and Haddow (2012) also explore amethod they call ?PRO-MERT?
where PRO andMERT are run in turns.
The parameter vectorcalculated by PRO serves as a starting point forMERT which optimizes dense features and in thecase of existing sparse features a scalar weight thatis multiplied with all sparse feature weights.While this method does not seem to have anyadvantage for BLEU-based tuning in a MT settingit has a positive effect on tuning with M2.
Re-sults for sparse features are now not worse thanwhen tuned with MERT alone in a dense fea-ture scenario.
Additionally to ?PRO-MERT?, weimplemented ?kb-MIRA-MERT?
which seems todisplay better convergence.
As in the case ofdense feature functions, we smooth sparse featureweights by averaging over all eight tuning steps.All reported results in this paper have beentuned according to M20.5, systems with dense fea-tures use MERT, systems with sparse features kb-MIRA-MERT.
All results are given for parametervectors that have been smoothed over eight opti-mizer runs from 4?2-CV.5 Adding Language Model DataWith parameter tuning working, we can now ex-plore the effects of adding feature functions to oursystem, starting with bigger language models.All systems use one 5-gram language modelthat has been estimated from the target side of theparallel data available for training.
In this section,only NUCLE is used as parallel data, four times3/4 of NUCLE for 4?2-CV and complete NUCLESystem 4?2-CV ST-2013NUCLE 15.43 16.73+WikiLM 19.18 23.10+CCLM10%21.57 25.71+CCLM 22.19 27.43Table 3: Results for increasing language modelssize on both shared task scenariosfor ST-2013.
If additional parallel data is added tothe training process (see Section 6), the target datais concatenated with NUCLE and a new 5-gramlanguage model is estimated.The additional language models discussed inthis section form separate feature functions,i.e.
they are weighted separately from the targetdata language model.
We experiment with threemodels that have been estimated using KenLM?s(Heafield, 2011) modified Kneser-Ney estimationtool (Heafield et al., 2013):WikiLM ?
a 3-gram model estimated from theentire English Wikipedia (2014-01-02).
Theraw text corpus consists of 3.2?109tokens.CCLM10%?
a 3-gram model estimated from10% of the English CommonCrawl data(4.4?1010tokens) described by Buck et al.(2014).
The full corpus data has been madepublicly available by the authors.CCLM ?
a 5-gram model estimated from the en-tire CommonCrawl data (4.4?1011tokens).This model has been created and made avail-able to us by Kenneth Heafield.
A newer ver-sion is publicly available (Buck et al., 2014).Results are shown in Table 3.
Improvementsseem to be proportionate to the order of magnitudeof the language model training corpora (Figure 1).M20.5improves by nearly 7% for 4?2-CV and bymore than 10% on ST-2013.6 Adding Translation Model DataSMT systems for grammatical error correction canbe trained on unannotated data.
For the 28 error-type task from CoNLL-2014, we do not need thelinguistically rich error annotations present in NU-CLE to add more training data.
It suffices to haveparallel data in which the source text contains er-rors and the target text has been corrected.
ForEnglish, such data is available.28System 4?2-CV ST-2013NUCLE+CCLM 22.19 27.43+L8-NAIST 23.34 31.20+L8 25.02 33.52NUCLE+CCLM 17.50 29.01+L8-NAIST 14.54 30.84+L8 17.48 30.14Table 4: Adding parallel data from Lang-8.
Topresults are for tuned systems, bottom results foruntuned systems.6.1 Lang-8Mizumoto et al.
(2011) published5a list oflearner?s corpora that were scraped from the so-cial language learning site Lang-8 (http://lang-8.com).
For our first experiments we useentries from ?Lang-8 Learner Corpora v1.0?
withEnglish as the learned language, we do not carefor the native language of the user.
Only entriesfor which at least one sentence has been correctedare taken into account.
Sentences without correc-tions from such entries are treated as error-free andmirrored on the target side of the corpus.
Even-tually, we obtain a corpus of 2,567,969 sentencepairs with 28,506,540 tokens on the uncorrectedsource side.
No noise-filtering is applied.
We callthis resource L8-NAIST.
Yoshimoto et al.
(2013)use this resource for sub-elements of their systemat the CoNLL-2013 Shared Task, but end up withhalf the number of sentences.
This seems to becaused by noise-reduction.We further investigate the effect of adding evengreater parallel resources.
Lang-8 is scraped foradditional entries and we manage to nearly doublethe size of the corpus to 3,733,116 sentences with51,259,679 tokens on the source side.
This jointresource is labeled L8.During training, the additional data is concate-nated with all training corpora in our setup (3/4 ofNUCLE for 4?2-CV and all of NUCLE for thefinal system).Results are presented in Table 4.
We extend theprevious best system NUCLE+CCLM with L8-NAIST and L8.
For tuned systems (top), resultsimprove for both evaluation settings with growingcorpus size.
In the case of untuned systems (bot-tom) results are entirely inconclusive.5http://cl.naist.jp/nldata/lang-86.2 Error SelectionYuan and Felice (2013) generate artificial errorsto add more training data to their system.
Weprefer actual errors, but the Lang-8 data may betoo error-prone as the general level of proficiencyseems to be lower than that of the NUCLE essays.We therefore select errors that match NUCLE er-ror types and replace all other errors with their cor-responding corrections.For each pair of sentences, a sequence of dele-tions and insertions is computed with the LCS al-gorithm (Maier, 1978) that transform the sourcesentence into the target sentence.
Adjacent deletedwords are concatenated, adjacent inserted wordsresult in a phrase insertion.
A deleted phrase fol-lowed directly by a phrase insertion is interpretedas a phrase substitution.
Substitutions are gener-alized if they consist of common substrings.
Gen-eralizations are encoded by the regular expression(\w{3,}) and a back-reference, e.g.
\1.
Ta-ble 5 contains the 20 most frequent patterns ex-tracted from NUCLE, 666 patterns with a fre-quency of five or higher remain.
Next, we performthe same computation for the to-be-adapted data.Edits that match patterns from our list are kept,other edits are replaced with their corrections.Although results (Table 6) with error selectionincrease for 4?2-CV, the NUCLE+CCLM+L8Aseems to generalize poorly to new data, there is asignificant drop for the external test set.
Comparedto NUCLE+CCLM+L8 (prec.
: 59.80, rec.
: 15.95)the error adapted (prec.
: 70.07, rec.
: 8.52) is muchmore conservative.Inspired by this, we also try a combination(NUCLE+CCLM+L8AT as in Adapted Tuning)of both systems by tuning with the adapted NU-CLE+CCLM+L8A, but applying the weights tothe unadapted system NUCLE+CCLM+L8.
Thisresults in a gain of 5% for ST-2013.
It seems thatthe unadapted Lang8 data introduces a substan-tial amount of noise that interferes with the tuningprocess.
Weights obtained from the cleaned dataseem to better approximate the true weight vectorand also work with unadapted data without sac-rificing recall.
In the remainder of the paper weuse this training/tuning scheme for all newly in-troduced systems.7 Task-Specific FeaturesThe systems presented so far relied on default fea-tures available in Moses.
In this section we will29Pattern Freq.
Pattern Freq.sub(?(\w{3,})?,?\1s?)
2441 ins(?an?)
222ins(?the?)
2364 sub(?(\w{3,})d?,?\1?)
181del(?the?)
1624 del(?of?)
178sub(?(\w{3,})s?,?\1?)
1110 sub(?is?,?are?)
166ins(?,?)
961 ins(?of?)
166ins(?a?)
663 del(?a?)
160sub(?(\w{3,})?,?\1d?)
253 sub(?(\w{3,})y?,?\1ies?)
150del(?,?)
244 ins(?to?)
148del(?.?)
227 sub(?is?,?was?)
147sub(?(\w{3,})?,?\1ed?)
222 sub(?the?,?a?)
132Table 5: 20 most frequent patterns extracted from NUCLE 3.0System 4?2-CV ST-2013NUCLE+CCLM+L8 25.02 33.52NUCLE+CCLM+L8A 26.82 28.67NUCLE+CCLM+L8AT 26.82 38.59Table 6: Results of error selectionextend the translation model with features tailoredto the task of grammatical error correction.7.1 Dense FeaturesIn Moses, translation models are described by aset of dense features: phrase translation probabil-ities, lexical scores, and a phrase penalty (Koehnet al., 2003).
In the grammatical error correctionscenario where source and target phrases are oftenidentical or similar, it might be useful to informthe decoder about the differences in a phrase pair.We extend translation models with a word-based Levenshtein distance feature (Levenshtein,1966) that captures the number of edit operationsrequired to turn the source phrase into the targetphrase.
Each phrase pair in the phrase table isscored with ed(s,t)where d is the word-based dis-tance function, s is the source phrase, t is the tar-get phrase.
The exponential function is used be-cause Moses relies on a log-linear model.
In thelog-linear model, the edit distances of all phrasepairs used to translate a sentence sum to the totalnumber of edits that have been applied to producethe target sentence.
Note that the Lang-8 data hasnot been processed for noise-reduction, this fea-ture should take care of the problem and penal-ize sentences that have diverged to much from thesource.
Table 7 contains examples of phrase pairsSource (s) Target (t) ed(s,t)a short time .
short term only .
20.0855a situation into a situation 2.7183a supermarket .
a supermarket .
1.0000able unable 2.7183Table 7: Dense Levenshtein feature examples.and their Levenshtein distance feature.We extend the currently best system NU-CLE+CCLM+L8AT with the Levenshtein dis-tance feature.
Results are shown in Table 8(+LD).
For 4?2-CV small improvements can beobserved, the effect is more significant for ST-2013.
It can be concluded that this very simplemodification of the standard translation model is abeneficial extension of SMT for grammatical cor-rection.7.2 Sparse FeaturesSparse features are a relatively new addition toMoses (Hasler et al., 2012).
Unlike dense fea-tures, they are optional and unrestricted in num-ber, thousands of different sparse features maybe used.
A verbose version of the above men-tioned LD feature is implemented as a sparsefeature.
Each edit operation is annotated withthe operation type and the words that take partin the operation.
The decoder can now learnto favor or penalize specific edits during tuning.As before in the case of error adaption patternsfrom Section 6.2, we generalize substitution op-erations if common substrings of a length equalto or greater than three characters appear in corre-sponding source and target phrases.
In the end,30System 4?2-CV ST-2013NUCLE+CCLM+L8AT 26.82 38.59+LD 27.34 40.21+SF 27.58 40.60Table 8: Results for dense Levenshtein distance(LD) and sparse pattern features (SF).
Each com-ponent extends the previous system cumulatively.we obtain sparse features that look exactly likethese patterns.
Features that correspond to pat-terns that had a frequency below 5 in NUCLE aremapped to del(OTHER), ins(OTHER), andsub(OTHER1,OTHER2).
Contrary to the Lev-enshtein distance feature, the sparse features arecomputed during decoding.Sparse features are added to the system whichhas already been extended with the dense Lev-enshtein feature.
Results in Table 8 (+SF) showsmall, but consistent gains.
LD and SF are linearlydependent as the total sum of triggered sparse fea-tures should be equal to the value of LD for a sen-tence, but we still observe positive effects.
Sparsefeature tuning is currently a work-around with du-bious effects, it can be expected that results mightbe more significant once this problem is solved.Based on these results, we choose the last systemNUCLE+CCLM+L8AT+LD+SF as our final sys-tem for the CoNLL-2014 Shared Task.8 Results for blind CoNLL-2014 test setOur final system achieves an official result of35.01% M20,5(?Submission?
in Table 9) on theblind CoNLL-2014 Shared Task test set (ST-2014).
Due to a tight time frame, this system suf-fered from missing words in an incorrectly filteredlanguage model and too few tuning iterations.
Af-ter the submission we retrained the same systemand achieve a score of 35.38% M20,5.
Table 9contains the results for incrementally added fea-tures, starting with the baseline, ending with thefinal system.
The addition of a web-scale lan-guage model results in similar improvements asfor 4?2-CV and ST-2013.
Additional unadaptedparallel training data from Lang-8 (+L8) has a verymodest effect on ST-2014.
This improves withthe mixed tuning scheme (+L8AT) which showsthat the gains for ST-2013 are not a one-time ef-fect.
Surprising are the substantial gains due tothe dense Levenshtein feature and the sparse fea-System P R M20.5Submission 41.62 21.40 35.01NUCLE 49.85 5.19 18.32+CCLM 50.39 9.90 27.72+L8 37.67 14.07 28.21+L8AT 37.02 17.94 30.53+LD 39.41 22.15 34.10+SF 41.72 22.00 35.38NUCLE 36.59 9.96 23.84+CCLM 27.92 18.68 25.41+L8 25.06 26.75 25.38+L8AT 24.49 34.89 26.04+LD 25.94 36.41 27.52+SF 25.94 36.41 27.52Table 9: Performance of chosen systems on theCoNLL-2014 test set.
Bottom results are untuned.tures.
We suspect that the task-specific featuresallow the decoder to better exploit the potential ofthe Lang-8 data.
This is verified by training NU-CLE+CCLM+LD+SF which scores only 25.82%.To support our claim concerning the impor-tance of parameter tuning, we also provide the per-formance of the same systems on ST-2014 withstandard parameters (bottom of Table 9).
Withone exception, we see significant improvementswith tuning.
The untuned systems display verysimilar results which would make it difficult tochoose among the configurations (untuned +LDand +LD+SF are actually the same system).
Onemight conclude incorrectly that the new featuresand additional resources have very little effect onthe final results and miss a gain of ca.
8%.Table 10 contains the ranking for all participat-ing systems.
Our system ranks on third place (seethe Shared Task proceedings (Ng et al., 2014) formore information on the other systems), loosingby 2.32% and 1.78% against the first two teams.We win with a quite significant margin of 4.13%over the next best system.
Compared to the top-two systems we suffer from lower recall, a prob-lem which should be attacked in the future.Participants were invited to submit alternativeanswers for evaluation, i.e.
answers that were gen-erated by their system and considered to be cor-rect alternatives to the provided gold standard.These answers were checked by human annota-tors.
Only three teams submitted alternative an-31Rank Team ID P R M20.51 CAMB 39.71 30.10 37.332 CUUI 41.78 24.88 36.793 AMU 41.62 21.40 35.014 POST 34.51 21.73 30.885 NTHU 35.08 18.85 29.926 RAC 33.14 14.99 26.687 UMC 31.27 14.46 25.378 PKU 32.21 13.65 25.329 NARA 21.57 29.38 22.7810 SJTU 30.11 5.10 15.1911 UFC 70.00 1.72 7.8412 IPN 11.28 2.85 7.0913 IITB 30.77 1.39 5.90Table 10: Shared Task results for submission with-out alternative answers.
AMU is our result.swers: CAMB, CUUI, and UMC.
The results forall teams improved when evaluated on these addi-tional answers, naturally those teams that submit-ted answers had the greatest gains.
Our result withadditional answers is 38.58%, we remain on thirdplace after CUUI (45.57%) and CAMB (43.55%)which switched places.
However, we do not con-sider the evaluation on alternative answers to bemeaningful as it is strongly biased.69 ConclusionsWe have shown that pure-surface phrase-basedSMT can be used to achieve state-of-the-art re-sults for grammatical error correction if suffi-ciently large resources are combined with cor-rectly executed parameter tuning and task-specificfeatures.
For noisy data, it seems beneficial to tuneon cleaned data, but noise can be useful when cor-recting unseen texts.Most of the previous work that we reviewedlacked the detail of parameter tuning that is com-monly applied in SMT.
In consequence, poten-tially useful contributions rarely improved over thebaselines or were beaten by classifier-based ap-proaches.
Many good features might have beenoverlooked or dismissed as unhelpful.
Our find-ings invite to re-evaluate these previous results.The tools we extended for parameter tuning ac-6We would accept alternative answers if all original sys-tem submissions were to be analyzed by annotators not asso-ciated with any team.
If this is not possible due to consid-erable costs and efforts, we would advocate to abandon thecurrent practice altogether.cording to the M2metric are publicly available andwe strongly suggest to use them in the future or toadapt them to the particular task at hand.
Param-eter tuning of sparse features according to the M2metric is ongoing research, but it seems the pro-posed work-around is a viable option.Since it is quite simple to implement the task-specific features introduced in this paper, we rec-ommend to use them whenever Moses is appliedin a similar setting.ReferencesChris Brockett, William B. Dolan, and Michael Ga-mon.
2006.
Correcting ESL errors using phrasalSMT techniques.
In Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and the 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 249?256,Stroudsburg, USA.
Association for ComputationalLinguistics.Christian Buck, Kenneth Heafield, and Bas van Ooyen.2014.
N-gram counts and language models from theCommon Crawl.
In Proceedings of the LanguageResources and Evaluation Conference, pages 3579?3584, Reykjav?k, Iceland.Mauro Cettolo, Nicola Bertoldi, and Marcello Fed-erico.
2011.
Methods for smoothing the optimizerinstability in SMT.
In MT Summit XIII: the Thir-teenth Machine Translation Summit, pages 32?39.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages427?436, Stroudsburg, USA.
Association for Com-putational Linguistics.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: Controlling for opti-mizer instability.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies, HLT ?11,pages 176?181, Stroudsburg, USA.
Association forComputational Linguistics.Daniel Dahlmeier and Hwee Tou Ng.
2012a.
A beam-search decoder for grammatical error correction.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,EMNLP-CoNLL ?12, pages 568?578, Stroudsburg,USA.
Association for Computational Linguistics.Daniel Dahlmeier and Hwee Tou Ng.
2012b.
Betterevaluation for grammatical error correction.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for Computational32Linguistics: Human Language Technologies, pages568?572, Stroudsburg, USA.
Association for Com-putational Linguistics.Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerenglish: The NUS Corpus of Learner English.
InProceedings of the Eighth Workshop on Innova-tive Use of NLP for Building Educational Applica-tions, pages 22?31, Atlanta, Georgia.
Associationfor Computational Linguistics.Eva Hasler, Barry Haddow, and Philipp Koehn.
2012.Sparse lexicalised features and topic adaptation forSMT.
In Proceedings of the 7th International Work-shop on Spoken Language Translation (IWSLT),pages 268?275.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modi-fied Kneser-Ney language model estimation.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics, pages 690?696,Sofia, Bulgaria.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, WMT?11, pages 187?197, Stroudsburg, USA.
Associationfor Computational Linguistics.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?11, pages 1352?1362, Stroudsburg, USA.Association for Computational Linguistics.Marcin Junczys-Dowmunt.
2012.
Phrasal rank-encoding: Exploiting phrase redundancy and trans-lational relations for phrase table compression.Prague Bull.
Math.
Linguistics, 98:63?74.Philipp Koehn and Barry Haddow.
2012.
Towardseffective use of training data in statistical machinetranslation.
In Proceedings of the 7th Workshop onStatistical Machine Translation, WMT ?12, pages317?321, Stroudsburg, USA.
Association for Com-putational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, pages 48?54, Stroudsburg, USA.
Associationfor Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Annual Meeting of the Association for Com-putational Linguistics.
The Association for Com-puter Linguistics.Vladimir I. Levenshtein.
1966.
Binary codes capableof correcting deletions, insertions and reversals.
So-viet Physics Doklady, 10:707?710.David Maier.
1978.
The complexity of some problemson subsequences and supersequences.
Journal of theACM, 25(2):322?336.Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-gata, and Yuji Matsumoto.
2011.
Mining revi-sion log of language learning SNS for automatedjapanese error correction of second language learn-ers.
In The 5th International Joint Conference onNatural Language Processing, pages 147?155.Tomoya Mizumoto, Yuta Hayashibe, Mamoru Ko-machi, Masaaki Nagata, and Yu Matsumoto.
2012.The effect of learner corpus size in grammatical er-ror correction of ESL writings.
In Proceedings ofCOLING 2012, pages 863?872.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, and Joel Tetreault.
2013.
The CoNLL-2013 shared task on grammatical error correction.In Proceedings of the 17th Conference on Compu-tational Natural Language Learning: Shared Task,pages 1?12, Sofia, Bulgaria.
Association for Com-putational Linguistics.Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Chris-tian Hadiwinoto, Raymond Hendy Susanto, , andChristopher Bryant.
2014.
The CoNLL-2014shared task on grammatical error correction.
In Pro-ceedings of the Eighteenth Conference on Compu-tational Natural Language Learning: Shared Task(CoNLL-2014 Shared Task), pages 1?14, Baltimore,USA.
Association for Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Com-putational Linguistics - Volume 1, ACL ?03, pages160?167, Stroudsburg, USA.
Association for Com-putational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, pages 311?318, Stroudsburg,USA.
Association for Computational Linguistics.Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,Keisuke Sakaguchi, Tomoya Mizumoto, YutaHayashibe, Mamoru Komachi, and Yuji Matsumoto.2013.
NAIST at 2013 CoNLL grammatical errorcorrection shared task.
In Proceedings of the 17thConference on Computational Natural LanguageLearning: Shared Task, pages 26?33, Sofia, Bul-garia.
Association for Computational Linguistics.Zheng Yuan and Mariano Felice.
2013.
Constrainedgrammatical error correction using statistical ma-chine translation.
In Proceedings of the 17th Con-ference on Computational Natural Language Learn-ing: Shared Task, pages 52?61, Sofia, Bulgaria.
As-sociation for Computational Linguistics.33
