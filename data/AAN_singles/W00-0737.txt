In: Proceedings of CoNLL-2000 and LLL-2000, pages 163-165, Lisbon, Portugal, 2000.Hybrid Text ChunkingGuoDong Zhou and J ian  Su  and TongGuan TeyKent Ridge Digital Labs21 Heng Mui Keng TerraceSingapore 119613{zhougd, sujian, tongguan}@krdl, org.sgAbst rac tThis paper proposes an error-driven HMM-based text chunk tagger with context-dependentlexicon.
Compared with standard HMM-basedtagger, this tagger incorporates more contextualinformation into a lexical entry.
Moreover, anerror-driven learning approach is adopted to de-crease the memory requirement by keeping onlypositive lexical entries and makes it possibleto further incorporate more context-dependentlexical entries.
Finally, memory-based learningis adopted to further improve the performanceof the chunk tagger.1 In t roduct ionThe idea of using statistics for chunking goesback to Church(1988), who used corpus frequen-cies to determine the boundaries of simple non-recursive noun phrases.
Skut and Brants(1998)modified Church's approach in a way permittingefficient and reliable recognition of structures oflimited depth and encoded the structure in sucha way that it can be recognised by a Viterbitagger.
Our approach follows Skut and Brants'way by employing HMM-based tagging methodto model the chunking process.2 HMM-based  Chunk  Tagger  w i thContext -dependent  Lex iconGiven a token sequence G~ = glg2""gn ,the goal is to find an optimal tag sequenceT~ = tit2.. "tn which maximizes log P(T~IG~):.
P (T~,G?
)log P(T~IG?)
= log P(T~) +log p(T~)P(G?
)The second item in the above equation is themutual information between the tag sequenceT~ and the given token sequence G~.
By as-suming that the mutual information betweenG~ and T~ is equal to the summation off mutualinformation between G~ and the individual tagti (l<i_<n):l P(T~, G~) n _ P(ti, G?
)o g p - ~ )  = E log P'(t~-P-~)i=1nMI(T~, G?)
= ~ MI(ti, G~),i=1we have:n P(ti, G~)log P(T~IG~)  = log P(T~)+~ log P(~i)P-~)i=1n n= log P (T~) -  ~ log P(ti) + ~ log P(tilG?
)i----1 i=1The first item of above equation can be solvedby chain rules.
Normally, each tag is assumedto be probabilistic dependent on the N-1 previ-ous tags.
Here, backoff bigram(N=2) model isused.
The second item is the summation of logprobabilities of all the tags.
Both the first itemand second item constitute the language modelcomponent while the third item constitutes thelexicon component.
Ideally the third item canbe estimated by the forward-backward algo-rithm(Rabiner 1989) recursively for the first-order(Rabiner 1989) or second-order HMMs.However, several approximations on it will beattempted later in this paper instead.
Thestochastic optimal tag sequence can be foundby maximizing the above equation over all thepossible tag sequences using the Viterbi algo-rithm.The main difference between our tagger andthe standard taggers lies in our tagger has acontext-dependent lexicon while others use acontext-independent lexicon.163For chunk tagger, we have gl = piwi whereW~ = wlw2""Wn is the word sequence andP~ = PlP2""Pn is the part-of-speech(POS)sequence.
Here, we use structural tags torepresenting chunking(bracketing and labeling)structure.
The basic idea of representingthe structural tags is similar to Skut andBrants(1998) and the structural tag consists ofthree parts:1) Structural relation.
The basic idea is sim-ple: structures of limited depth are encodedusing a finite number of flags.
Given a se-quence of input tokens(here, the word and POSpairs), we consider the structural relation be-tween the previous input token and the currentone.
For the recognition of chunks, it is suffi-cient to distinguish the following four differentstructural relations which uniquely identify thesub-structures of depth l(Skut and Brants usedseven different structural relations to identifythe sub-structures of depth 2).?
00: the current input token and the previ-ous one have the same parent?
90: one ancestor of the current input tokenand the previous input token have the sameparent?
09: the current input token and one an-cestor of the previous input token have thesame parent?
99 one ancestor of the current input tokenand one ancestor of the previous input to-ken have the same parentCompared with the B-Chunk and I-Chunkused in Ramshaw and Marcus(1995)~, structuralrelations 99 and 90 correspond to B-Chunkwhich represents the first word of the chunk,and structural relations 00 and 09 correspondto I-Chunk which represents each other in thechunk while 90 also means the beginning of thesentence and 09 means the end of the sentence.2)Phrase category.
This is used to identifythe phrase categories of input tokens.3)Part-of-speech.
Because of the limitednumber of structural relations and phrase cate-gories, the POS is added into the structural tagto represent more accurate models.Principally, the current chunk is dependenton all the context words and their POSs.
How-ever, in order to decrease memory require-ment and computational complexity, our base-line HMM-based chunk tagger only considersprevious POS, current POS and their word to-kens whose POSs are of certain kinds, such aspreposition and determiner etc.
The overallprecision, recall and F~=i rates of our baselinetagger on the test data of the shared task are89.58%, 89.56% and 89.57%.3 Error-driven LearningAfter analysing the chunking results, we findmany errors are caused by a limited number ofwords.
In order to overcome such errors, weinclude such words in the chunk dependencecontext by using error-driven learning.
First,the above HMM-based chunk tagger is used tochunk the training data.
Secondly, the chunktags determined by the chunk tagger are com-pared with the given chunk tags in the trainingdata.
For each word, its chunking error numberis summed.
Finally, those words whose chunk-ing error numbers are equal to or above a giventhreshold(i.e.
3) are kept.
The HMM-basedchunk tagger is re-trained with those words con-sidered in the chunk dependence ontext.The overall precision, recall and FZ=i ratesof our error-driven HMM-based chunk taggeron the test data of the shared task are 91.53%,92.02% and 91.774 Memory based LearningMemory-based learning has been widely usedin NLP tasks in the last decade.
Principally, itfalls into two paradigms.
First paradigm rep-resents examples as sets of features and car-ries out induction by finding the most simi-lar cases.
Such works include Daelemans eta1.
(1996) for POS tagging and Cardie(1993)for syntactic and semantic tagging.
Secondparadigm makes use of raw sequential dataand generalises by reconstructing test examplesfrom different pieces of the training data.
Suchworks include Bod(1992) for parsing, Argamonet a1.
(1998) for shallow natural anguage pat-terns and Daelemans et a1.
(1999) for shallowparsing.The memory-based method presented herefollows the second paradigm and makes use ofraw sequential data.
Here, generalization is per-formed online at recognition time by comparing164the new pattern to the ones in the training cor-pus.Given one of the N most probable chunk se-quences extracted by the error-driven HMM-based chunk tagger, we can extract a set ofchunk patterns, each of them with the format:XP 1 n n+l r~+l = poroPlrn Pn+l, where is thestructural relation between Pi and Pi+l.As an example, from the bracketed and la-beled sentence:\[NP He/PRP \] \[VP reckons/VSZ \]\[NP the/DT current/ J J  account/NNdeficit/NN \] \[VP will/MD narrow/VB\] \[ PP to /TO\ ]  \[NP only/RB #/#1.8/CD billion/CD \] \[PP in/IN \ ] \ [NPSeptember/NNP \] \[O ./.
\]we can extract following chunk patterns:NP=NULL 90 PRP 99 VBZVP=PRP 99 VBZ 99 DTNP=VBZ 99 DT JJ NN NN 99 MDPP=VB 99 TO 99 RBNP=TO 99 RB # CD CD 99 INPP=CD 99 IN 99 NNPNP=IN 99 NNP 99 .O=NNP 99 .
09 NULLFor every chunk pattern, we estimate its proba-bility by using memory-based learning.
If thechunk pattern exists in the training corpus,its probability is computed by the probabilityof such pattern among all the chunk patterns.Otherwise, its probability is estimated by themultiply of its overlapped sub-patterns.
Thenthe probability of each of the N most probablechunk sequences i adjusted by multiplying theprobabilities of its extracted chunk patterns.Table 1 shows the performance oferror-drivenHMM-based chunk tagger with memory-basedlearning.5 Conc lus ionIt is found that the performance with the help oferror-driven learning is improved by 2.20% andintegration of memory-based learning furtherimproves the performance by 0.35% to 92.12%.For future work, the experimentation  largescale task will be speculated in the near future.Finally, a closer integration of memory-basedmethod with HMM-based chunk tagger will alsobe conducted.test dataADJPADVPCONJPINTJLSTNPPPPRTSBARVPprecision76.17%78.25%46.67%20.00%00.00%92.19%96.09%72.36%83.56%92.77%recall70.78%78.52%77.78%5O.OO%OO.O0%92.59%96.94%83.96%79.81%92.85%all 91.99% 92.25%F~=i73.3778.3958.3328.5700.0092.3996.5177.7381.6492.8192.12Table 1: performance of chunkingReferencesS.
Argamon, I. Dagan, and Y. Krymolowski.
1998.A memory-based approach to learning shallownatural language patterns.
In COLING/ACL-1998, pages 67-73.
Montreal, Canada.R.
Bod.
1992.
A computational model of lan-guage performance: Data-oriented parsing.
InCOLING-1992, pages 855-859.
Nantes, France.C.
Cardie.
1993.
A case-based approach to knowl-edge acquisition for domain-specific sentence anal-ysis.
In Proceeding of the I1th National Con-ference on Artificial Intelligence, pages 798-803.Menlo Park, CA, USA.
AAAI Press.K.W.
Church.
1988.
A stochastic parts program andnoun phrase parser for unrestricted text.
In Pro-ceedings of Second Conference on Applied Natu-ral Language Processing, pages 136-143.
Austin,Texas, USA.W.
Daelemans, J. Zavrel, P. Berck, and S. Gillis.1996.
Mbt: A memory-based part-of-speech tag-ger generator.
In Proceeding of the Fourth Work-shop on Large Scale Corpora, pages 14-27.
ACLSIGDAT.W.
Daelemans, S. Buchholz, and J. Veenstra.
1999.Memory-based shallow parsing.
In CoNLL-1999,pages 53-60.
Bergen, Norway.L.R.
Rabiner.
1989.
A tutorial on hidden markovmodels and selected applications in speech recog-nition.
In Proceedings of the IEEE, volume 77,pages 257-286.Lance A. Ramshaw and Mitchell P. Marcus.
1995.Text chunking using transformation-based learn-ing.
In Proceedings of the Third ACL Work-shop on Very Large Corpora.
Cambridge, Mas-sachusetts, USA.W.
Skut and T. Brants.
1998.
Chunk tagger: sta-tistical recognition of noun phrases.
In ESSLLI-1998 Workshop on Automated Acquisition of Syn-tax and Parsing.
Saarbruucken, Germany.165
