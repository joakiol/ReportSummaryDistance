Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, page 8,Baltimore, Maryland, USA, 22 June 2014.c?2014 Association for Computational LinguisticsNew Directions in Vector Space Models of MeaningPhil Blunsom, Edward Grefenstetteand Karl Moritz Hermann?University of Oxfordfirst.last@cs.ox.ac.ukGeorgiana DinuCenter for Mind/Brain SciencesUniversity of Trentogeorgiana.dinu@unitn.it1 AbstractSymbolic approaches have dominated NLP as ameans to model syntactic and semantic aspects ofnatural language.
While powerful inferential toolsexist for such models, they suffer from an inabil-ity to capture correlation between words and toprovide a continuous model for word, phrase, anddocument similarity.
Distributed representationsare one mechanism to overcome these constraints.This tutorial will supply NLP researchers withthe mathematical and conceptual background tomake use of vector-based models of meaning intheir own research.
We will begin by motivatingthe need for a transition from symbolic represen-tations to distributed ones.
We will briefly coverhow collocational (distributional) vectors can beused and manipulated to model word meaning.
Wewill discuss the progress from distributional to dis-tributed representations, and how neural networksallow us to learn word vectors and condition themon metadata such as parallel texts, topic labels, orsentiment labels.
Finally, we will present variousforms of semantic vector composition, and discusstheir relative strengths and weaknesses, and theirapplication to problems such as language mod-elling, paraphrasing, machine translation and doc-ument classification.This tutorial aims to bring researchers up tospeed with recent developments in this fast-moving field.
It aims to strike a balance be-tween providing a general introduction to vector-based models of meaning, an analysis of diverg-ing strands of research in the field, and also beinga hands-on tutorial to equip NLP researchers withthe necessary tools and background knowledge tostart working on such models.
Attendees shouldbe comfortable with basic probability, linear alge-bra, and continuous mathematics.
No substantialknowledge of machine learning is required.
?Instructors listed in alphabetical order.2 Outline1.
Motivation: Meaning in space2.
Learning distributional models for words3.
Neural language modelling and distributedrepresentations(a) Neural language model fundamentals(b) Recurrent neural language models(c) Conditional neural language models4.
Semantic composition in vector spaces(a) Algebraic and tensor-based composition(b) The role of non-linearities(c) Learning recursive neural models(d) Convolutional maps and composition3 InstructorsPhil Blunsom is an Associate Professor at theUniversity of Oxford?s Department of ComputerScience.
His research centres on the probabilisticmodelling of natural languages, with a particularinterest in automating the discovery of structureand meaning in text.Georgiana Dinu is a postdoctoral researcherat the University of Trento.
Her research re-volves around distributional semantics with a fo-cus on compositionality within the distributionalparadigm.Edward Grefenstette is a postdoctoral researcherat Oxford?s Department of Computer Science.
Heworks on the relation between vector represen-tations of language meaning and structured logi-cal reasoning.
His work in this area was recentlyrecognised by a best paper award at *SEM 2013.Karl Moritz Hermann is a final-year DPhil stu-dent at the Department of Computer Science inOxford.
His research studies distributed and com-positional semantics, with a particular emphasison mechanisms to reduce task-specific and mono-lingual syntactic bias in such representations.8
