Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 37?45,Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational LinguisticsTransition-based Semantic Role LabelingUsing Predicate Argument ClusteringJinho D. ChoiDepartment of Computer ScienceUniversity of Colorado at Boulderchoijd@colorado.eduMartha PalmerDepartment of LinguisticsUniversity of Colorado at Bouldermpalmer@colorado.eduAbstractThis paper suggests two ways of improvingsemantic role labeling (SRL).
First, we intro-duce a novel transition-based SRL algorithmthat gives a quite different approach to SRL.Our algorithm is inspired by shift-reduce pars-ing and brings the advantages of the transition-based approach to SRL.
Second, we presenta self-learning clustering technique that effec-tively improves labeling accuracy in the testdomain.
For better generalization of the sta-tistical models, we cluster verb predicates bycomparing their predicate argument structuresand apply the clustering information to thefinal labeling decisions.
All approaches areevaluated on the CoNLL?09 English data.
Thenew algorithm shows comparable results toanother state-of-the-art system.
The cluster-ing technique improves labeling accuracy forboth in-domain and out-of-domain tasks.1 IntroductionSemantic role labeling (SRL) has sparked much in-terest in NLP (Shen and Lapata, 2007; Liu andGildea, 2010).
Lately, dependency-based SRL hasshown advantages over constituent-based SRL (Jo-hansson and Nugues, 2008).
Two main benefits canbe found.
First, dependency parsing is much fasterthan constituent parsing, whereas constituent pars-ing is usually considered to be a bottleneck to SRL interms of execution time.
Second, dependency struc-ture is more similar to predicate argument struc-ture than phrase structure because it specifically de-fines relations between a predicate and its argumentswith labeled arcs.
Unlike constituent-based SRLthat maps phrases to semantic roles, dependency-based SRL maps headwords to semantic roles be-cause there is no phrasal node in dependency struc-ture.
This may lead to a concern about getting theactual semantic chunks back, but Choi and Palmer(2010) have shown that it is possible to recover theoriginal chunks from the headwords with minimalloss, using a certain type of dependency structure.Traditionally, either constituent or dependency-based, semantic role labeling is done in two steps,argument identification and classification (Gildeaand Jurafsky, 2002).
This is from a general be-lief that each step requires a different set of fea-tures (Xue and Palmer, 2004), and training thesesteps in a pipeline takes less time than training themas a joint-inference task.
However, recent machinelearning algorithms can deal with large scale vectorspaces without taking too much training time (Hsiehet al, 2008).
Furthermore, from our experience independency parsing, handling these steps togetherimproves accuracy in identification as well as clas-sification (unlabeled and labeled attachment scoresin dependency parsing).
This motivates the develop-ment of a new semantic role labeling algorithm thattreats these two steps as a joint inference task.Our algorithm is inspired by shift-reduce pars-ing (Nivre, 2008).
The algorithm uses several transi-tions to identify predicates and their arguments withsemantic roles.
One big advantage of the transition-based approach is that it can use previously identi-fied arguments as features to predict the next argu-ment.
We apply this technique to our approach andachieve comparable results to another state-of-the-art system evaluated on the same data sets.37NO-PRED( ?1 , ?2, j, ?3, [i|?4], A )?
( [?1|j], ?2, i, ?3, ?4 , A )?j.
oracle(j) 6= predicateSHIFT( ?1 , ?2, j, [i|?3], ?4, A )?
( [?2|j], [ ] , i, [ ] , ?3, A )?j.
oracle(j) = predicate ?
?1 = [ ] ?
?4 = [ ]NO-ARC?
( [?1|i], ?2 , j, ?3, ?4, A )?
( ?1 , [i|?2], j, ?3, ?4, A )?j.
oracle(j) = predicate ?
?i.oracle(i, j) = {i 6?
j}NO-ARC?
( ?1, ?2, j, ?3 , [i|?4], A )?
( ?1, ?2, j, [?3|i], ?4 , A )?j.
oracle(j) = predicate ?
?i.oracle(i, j) = {j 6?
i}LEFT-ARC?L( [?1|i], ?2 , j, ?3, ?4, A )?
( ?1 , [i|?2], j, ?3, ?4, A ?
{iL?
j} )?j.
oracle(j) = predicate ?
?i.oracle(i, j) = {iL?
j}RIGHT-ARC?L( ?1, ?2, j, ?3 , [i|?4], A )?
( ?1, ?2, j, [?3|i], ?4 , A ?
{jL?
i} )?j.
oracle(j) = predicate ?
?i.oracle(i, j) = {jL?
i}Table 1: Transitions in our bidirectional top-down search algorithm.
For each row, the first line shows a transition andthe second line shows preconditions of the transition.For better generalization of the statistical models,we apply a self-learning clustering technique.
Wefirst cluster predicates in test data using automati-cally generated predicate argument structures, thencluster predicates in training data by using the previ-ously found clusters as seeds.
Our experiments showthat this technique improves labeling accuracy forboth in-domain and out-of-domain tasks.2 Transition-based semantic role labelingDependency-based semantic role labeling can beviewed as a special kind of dependency parsing inthe sense that both try to find relations betweenword pairs.
However, they are distinguished in twomajor ways.
First, unlike dependency parsing thattries to find some kind of relation between any wordpair, semantic role labeling restricts its search onlyto top-down relations between predicate and argu-ment pairs.
Second, dependency parsing requiresone head for each word, so the final output is a tree,whereas semantic role labeling allows multiple pred-icates for each argument.
Thus, not all dependencyparsing algorithms, such as a maximum spanningtree algorithm (Mcdonald and Pereira, 2006), can benaively applied to semantic role labeling.Some transition-based dependency parsing algo-rithms have been adapted to semantic role labelingand shown good results (Henderson et al, 2008;Titov et al, 2009).
However, these algorithms areoriginally designed for dependency parsing, so arenot necessarily customized for semantic role label-ing.
Here, we present a novel transition-based algo-rithm dedicated to semantic role labeling.
The keydifference between this algorithm and most othertransition-based algorithms is in its directionality.Given an identified predicate, this algorithm tries tofind top-down relations between the predicate andthe words on both left and right-hand sides, whereasother transition-based algorithms would considerwords on either the left or the right-hand side, butnot both.
This bidirectional top-down search makesmore sense for semantic role labeling because predi-cates are always assumed to be the heads of their ar-guments, an assumption that cannot be generalizedto dependency parsing, and arguments can appeareither side of the predicate.Table 1 shows transitions used in our algorithm.All parsing states are represented as tuples (?1, ?2,p, ?3, ?4, A), where ?1..4 are lists of word indicesand p is either a word index of the current predi-cate candidate or @ indicating no predicate candi-date.
?1,4 contain indices to be compared with p and?2,3 contain indices already compared with p. A is aset of labeled arcs representing previously identifiedarguments with respect to their predicates.
?
and?
indicate parsing directions.
L is a semantic rolelabel, and i, j represent indices of their correspond-ing word tokens.
The initial state is ([ ], [ ], 1, [ ],[2, .
.
.
, n], ?
), where w1 and wn are the first and thelast words in a sentence, respectively.
The final stateis (?1, ?2, @, [ ], [ ],A), i.e., the algorithm terminateswhen there is no more predicate candidate left.38John1wants2to3buy4a5car6Root0SBJROOTOPRDOBJIMNMODA0 A1A0 A1Figure 1: An example of a dependency tree with semantic roles.
The upper and lower arcs stand for syntactic andsemantic dependencies, respectively.
SBJ, OBJ, OPRD, IM, NMOD stand for a subject, object, object predicative,infinitive marker, and noun-modifier.
A0, A1 stand for ARG0, ARG1 in PropBank (Palmer et al, 2005).Transition ?1 ?2 p ?3 ?4 A0 [ ] [ ] 1 [ ] [2..6] ?1 NO-PRED [1] [ ] 2 [ ] [3..6]2 LEFT-ARC [ ] [1] 2 [ ] [3..6] A ?
{1?A0?
2}3 RIGHT-ARC [ ] [1] 2 [3] [4..6] A ?
{2 ?A1?
3}4 NO-ARC [ ] [1] 2 [3..4] [5..6]5 NO-ARC [ ] [1] 2 [3..5] [6]6 NO-ARC [ ] [1] 2 [3..6] [ ]7 SHIFT [1..2] [ ] 3 [ ] [4..6]8 NO-PRED [1..3] [ ] 4 [ ] [5..6]9 NO-ARC [1..2] [3] 4 [ ] [5..6]10 NO-ARC [1] [2..3] 4 [ ] [5..6]11 LEFT-ARC [ ] [1..3] 4 [ ] [5..6] A ?
{1?A0?
4}12 NO-ARC [ ] [1..3] 4 [5] [6]13 RIGHT-ARC [ ] [1..3] 4 [5..6] [ ] A ?
{4 ?A1?
6}14 SHIFT [1..4] [ ] 5 [ ] [6]15 NO-PRED [1..5] [ ] 6 [ ] [ ]16 NO-PRED [1..6] [ ] @ [ ] [ ]Table 2: Parsing states generated by our algorithm for the example in Figure 1.The algorithm uses six kinds of transitions.
NO-PRED is performed when an oracle identifies wj asnot a predicate.
All other transitions are performedwhen wj is identified as a predicate.
SHIFT is per-formed when both ?1 and ?4 are empty, meaningthat there are no more argument candidates left forthe predicate wj .
NO-ARC is performed when wiis identified as not an argument of wj .
LEFT-ARCLand RIGHT-ARCL are performed when wi is identi-fied as an argument of wj with a label L. These tran-sitions can be performed in any order as long as theirpreconditions are satisfied.
For our experiments, weuse the following generalized sequence:[ (NO-PRED)?
?
(LEFT-ARC?L |NO-ARC?)?
?
(RIGHT-ARC?L |NO-ARC?)?
?
SHIFT ]?Notice that this algorithm does not take separatesteps for argument identification and classification.By adding the NO-ARC transitions, we successfullymerge these two steps together without decrease inlabeling accuracy.1 Since each word can be a predi-cate candidate and each predicate considers all otherwords as argument candidates, a worst-case com-plexity of the algorithm is O(n2).
To reduce thecomplexity, Zhao et al (2009) reformulated a prun-ing algorithm introduced by Xue and Palmer (2004)for dependency structure by considering only directdependents of a predicate and its ancestors as ar-gument candidates.
This pruning algorithm can beeasily applied to our algorithm: the oracle can pre-filter such dependents and uses the information toperform NO-ARC transitions without consulting sta-tistical models.1We also experimented with the traditional approach ofbuilding separate classifiers for identification and classification,which did not lead to better performance in our case.39Table 2 shows parsing states generated by our al-gorithm.
Our experiments show that this algorithmgives comparable results against another state-of-the-art system.3 Predicate argument clusteringSome studies showed that verb clustering informa-tion could improve performance in semantic role la-beling (Gildea and Jurafsky, 2002; Pradhan et al,2008).
This is because semantic role labelers usuallyperform worse on verbs not seen during training, forwhich the clustering information can provide usefulfeatures.
Most previous studies used either bag-of-words or syntactic structure to cluster verbs; how-ever, this may or may not capture the nature of predi-cate argument structure, which is more semanticallyoriented.
Thus, it is preferable to cluster verbs bytheir predicate argument structures to get optimizedfeatures for semantic role labeling.In this section, we present a self-learning clus-tering technique that effectively improves labelingaccuracy in the test domain.
First, we perform se-mantic role labeling on the test data using the algo-rithm in Section 2.
Next, we cluster verbs in the testdata using predicate argument structures generatedby our semantic role labeler (Section 3.2).
Then, wecluster verbs in the training data using the verb clus-ters we found in the test data (Section 3.3).
Finally,we re-run our semantic role labeler on the test datausing the clustering information.
Our experimentsshow that this technique gives improvement to la-beling accuracy for both in and out-of domain tasks.3.1 Projecting predicate argument structureinto vector spaceBefore clustering, we need to project the predicateargument structure of each verb into vector space.Two kinds of features are used to represent thesevectors: semantic role labels and joined tags ofsemantic role labels and their corresponding wordlemmas.
Figure 2 shows vector representations ofpredicate argument structures of verbs, want andbuy, in Figure 1.Initially, all existing and non-existing features areassigned with a value of 1 and 0, respectively.
How-ever, assigning equal values to all existing featuresis not necessarily fair because some features havewant 1 1 1 1 00s 0sbuy 1 1 1 0 10s 0sA0 A1 john:A0 to:A1 car:A1... ...VerbFigure 2: Projecting the predicate argument structure ofeach verb into vector space.higher confidence, or are more important than theothers; e.g., ARG0 and ARG1 are generally predictedwith higher confidence than modifiers, nouns givemore important information than some other gram-matical categories, etc.
Instead, we assign each ex-isting feature with a value computed by the follow-ing equations:s(lj |vi) =11 + exp(?score(lj |vi))s(mj , lj) ={1 (wj 6= noun)exp( count(mj ,lj)?
?k count(mk,lk))vi is the current verb, lj is the j?th label of vi, andmj is lj?s corresponding lemma.
score(lj |vi) is ascore of lj being a correct argument label of vi; thisis always 1 for training data and is provided by ourstatistical models for test data.
Thus, s(lj |vi) is anapproximated probability of lj being a correct argu-ment label of vi, estimated by the logistic function.s(mj , lj) is equal to 1 if wj is not a noun.
If wj isa noun, it gets a value ?
1 given a maximum likeli-hood of mj being co-occurred with lj .2With the vector representation, we can apply anykind of clustering algorithm (Hofmann and Puzicha,1998; Kamvar et al, 2002).
For our experiments,we use k-best hierarchical clustering for test data,and k-means clustering for training data.3.2 Clustering verbs in test dataGiven automatically generated predicate argumentstructures in the test data, we apply k-best hierar-chical clustering; that is, a relaxation of classical hi-erarchical agglomerative clustering (from now on,HAC; Ward (1963)), to find verb clusters.
UnlikeHAC that merges a pair of clusters at each iteration,k-best hierarchical clustering merges k-best pairs at2Assigning different weights for nouns resulted in moremeaningful clusters in our experiments.
We will explore addi-tional grammatical category specific weighting schemes in fu-ture work.40each iteration (Lo et al, 2009).
Instead of merging afixed number of k-clusters, we use a threshold to dy-namically determine the top k-clusters.
Our studiesindicate that this technique produces almost as fine-grained clusters as HAC, yet converges much faster.Our algorithm for k-best hierarchical clustering ispresented in Algorithm 1. thup is a threshold that de-termines which k-best pairs are to be merged (in ourcase, kup = 0.8).
sim(ci, cj) is a similarity betweenclusters ci and cj .
For our experiments, we use co-sine similarity with average-linkage.
It is possiblethat other kinds of similarity metrics would workbetter, which we will explore as future work.
Con-ditions in line 15 ensure that each cluster is mergedwith at most one other cluster at each iteration, andconditions in line 17 force at least one cluster tobe merged with one other cluster at each iteration.Thus, the algorithm is guaranteed to terminate afterat most (n?
1) iterations.When the algorithm terminates, it returns a set ofone cluster with different hierarchical levels.
Forour experiments, we set another threshold, thlow, forearly break-out: if there is no cluster pair whose sim-ilarity is greater than thlow, we terminate the algo-rithm (in our case, thlow = 0.7).
A cluster set gen-erated by this early break-out contains several unitclusters that are not merged with any other cluster.All of these unit clusters are discarded from the setto improve set quality.
This is reasonable becauseour goal is not to cluster all verbs but to find a usefulset of verb clusters that can be mapped to verbs intraining data, which can lead to better performancein semantic role labeling.3.3 Clustering verbs in training dataGiven the verb clusters we found in the test data,we search for verbs that are similar to these clustersin the training data.
K-means clustering (Hartigan,1975) is a natural choice for this case because wealready know k-number of center clusters to beginwith.
Each verb in the training data is compared withall verb clusters in the test data, and merged with thecluster that gives the highest similarity.
To maintainthe quality of the clusters, we use the same thresh-old, thlow, to filter out verbs in the training data thatare not similar enough to any verb cluster in the testdata.
By doing so, we keep only verbs that are morelikely to be helpful for semantic role labeling.input : C = [c1, .., cn]: ci is a unit cluster.thup ?
R: threshold.output: C?
= [c1, .., cm]: cj is a unit or mergedcluster, where m ?
n.begin1while |C| > 1 do2L?
list()3for i ?
[1, |C| ?
1] do4for j ?
[i+ 1, |C|] do5t?
(i, j, sim(ci, cj))6L.add(t)7end8end9descendingSortBySimilarity(L)10S ?
set()11for k ?
[1, |L|] do12t?
L.get(k)13i?
t(0); j ?
t(1); sim?
t(2)14if i ?
S or j ?
S then15continue16if k = 1 or sim > thup then17C.add(ci ?
cj); S.add(i, j)18C.remove(ci, cj)19else20break21end22end23end24end25Algorithm 1: k-best hierarchical clustering.4 Features4.1 Baseline featuresFor a baseline approach, we use features similar toones used by Johansson and Nugues (2008).
All fea-tures are assumed to have dependency structures asinput.
Table 3 shows n-gram feature templates usedfor our experiments (f: form, m: lemma, p: POS tag,d: dependency label).
warg andwpred are the currentargument and predicate candidates.
hd(w) stands forthe head of w, lm(w), rm(w) stand for the leftmost,rightmost dependents of w, and ls(w), rs(w) standfor the left-nearest, right-nearest siblings of w, withrespect to the dependency structures.
Some of thesefeatures can be presented as a joined feature; e.g., acombination of warg?s POS tag and lemma.41Word tokens Featureswarg, wpred f,m,p,dwarg?1, hd, lm, rm, ls, rs (warg) m,pwpred?1, hd, lm, rm (wpred) m,pTable 3: N -gram feature templates.Besides the n-gram features, we use several struc-tural features such as dependency label set, subcat-egorization, POS path, dependency path, and depen-dency depth.
Dependency label set features are de-rived by collecting all dependency labels of wpred?sdirect dependents.
Unlike Johansson and Nugues,we decompose subcategorization features into twoparts: one representing the left-hand side and theother representing the right-hand side dependenciesof wpred.
For the predicate wants in Figure 3, wegenerate ?
?SBJ and ???
?OPRD as separate subcategoriza-tion features.wantsPRP:John TO:toVB:buySBJ OPRDIMFigure 3: Dependency structure used for subcategoriza-tion, path, and depth features.We also decompose path features into two parts:given the lowest common ancestor (LCA) of wargand wpred, we generate path features from warg tothe LCA and from the LCA to wpred, separately.For example, the predicate buy and the argumentJohn in Figure 3 have a LCA at wants, so we gen-erate two sets of path features, {?PRP, ?TO?VB}with POS tags, and {?SBJ, ?OPRD?IM} with depen-dency labels.
Such decompositions allow more gen-eralization of those features; even if one part is notmatched to the current parsing state, the other partcan still participate as a feature.
Throughout ourexperiments, these generalized features give slightlyhigher labeling accuracy than ungeneralized featuresalthough they form a smaller feature space.In addition, we apply dependency path features towpred?s highest verb chain, which often shares ar-guments with the predicate (e.g., John is a sharedargument of the predicate buy and its highest verbchain wants).
To retrieve the highest verb chain, weapply a simple heuristic presented below.
The func-tion getHighestVerbChain takes a predicate,pred, as input and returns its highest verb chain,vNode, as output.
If there is no verb chain for thepredicate, it returns null instead.
Note that thisheuristic is designed to work with dependency rela-tions and labels described by the CoNLL?09 sharedtask (Hajic?
et al, 2009).func getHighestVerbChain(pred)vNode = pred;regex = "CONJ|COORD|IM|OPRD|VC";while (regex.matches(vNode.deprel))vNode = vNode.head;if (vNode != pred) return vNode;else return null;Dependency depth features are a reduced form ofpath features.
Instead of specifying POS tags or de-pendency labels, we indicate paths with their depths.For instance, John and buy in Figure 3 have a depen-dency depth feature of ?1?2, which implies that thedepth between John and its LCA (wants) is 1, andthe depth between the LCA and buy is 2.Finally, we use four kinds of binary features: ifwarg is a syntactic head of wpred, if wpred is a syn-tactic head ofwarg, ifwpred is a syntactic ancestor ofwarg, and if wpred?s verb chain has a subject.
Eachfeature gets a value of 1 if true; otherwise, it gets avalue of 0.4.2 Dynamic and clustering featuresAll dynamic features are derived by using previ-ously identified arguments.
Two kinds of dynamicfeatures are used for our experiments.
One is a la-bel of the very last predicted numbered argument ofwpred.
For instance, the parsing state 3 in Table 2uses a label A0 as a feature to make its prediction,wantsA1?
to, and the parsing states 4 to 6 use a labelA1 as a feature to make their predictions, NO-ARC?s.With this feature, the oracle can narrow down thescope of expected arguments of wpred.
The other isa previously identified argument label of warg.
Theexistence of this feature implies that warg is alreadyidentified as an argument of some other predicate.For instance, when warg = John and wpred = buy inTable 2, a label A0 is used as a feature to make theprediction, JohnA0?
buy, because John is alreadyidentified as an A0 of wants.42Finally, we use wpred?s cluster ID as a feature.
Thedynamic and clustering features combine a verysmall portion of the entire feature set, but still give afair improvement to labeling accuracy.5 Experiments5.1 CorporaAll models are trained on Wall Street Journal sec-tions 2-21 and developed on section 24 using auto-matically generated lemmas and POS tags, as dis-tributed by the CoNLL?09 shared task (Hajic?
et al,2009).
CoNLL?09 data contains semantic roles forboth verb and noun predicates, for which we useonly ones related to verb predicates.
Furthermore,we do not include predicate sense classification as apart of our task, which is rather a task of word sensedisambiguation than semantic role labeling.For in-domain and out-of-domain evaluations,WSJ section 23 and the Brown corpus are used, alsodistributed by CoNLL?09.
To retrieve automaticallygenerated dependency trees as input to our semanticrole labeler, we train our open source dependencyparser, called ClearParser3, on the training set andrun the parser on the evaluation sets.
ClearParseruses a transition-based dependency parsing algo-rithm that gives near state-of-the-art results (Choiand Palmer, 2011), and mirrors our SRL algorithm.5.2 Statistical modelsWe use Liblinear L2-L1 SVM for learning; a linearclassification algorithm using L2 regularization andL1 loss function.
This algorithm is designed to han-dle large scale data: it assumes the data to be lin-early separable so does not use any kind of kernelspace (Hsieh et al, 2008).
As a result, it significantlyreduces training time compared to typical SVM, yetperforms accurately.
For our experiments, we usethe following learning parameters: c = 0.1 (cost),e = 0.2 (termination criterion), B = 0 (bias).Since predicate identification is already providedin the CoNLL?09 data, we do not train NO-PRED.SHIFT does not need to be trained in general be-cause the preconditions of SHIFT can be checkeddeterministically without consulting statistical mod-els.
NO-ARC?
and LEFT-ARC?L are trained to-gether using the one-vs-all method as are NO-ARC?3http://code.google.com/p/clearparser/and RIGHT-ARC?L .
Even with multi-classifications,it takes less than two minutes for the entire trainingusing Liblinear.5.3 Accuracy comparisonsTables 4 and 5 show accuracy comparisons betweenthree models evaluated on the WSJ and Brown cor-pora, respectively.
?Baseline?
uses the features de-scribed in Section 4.1.
?+Dynamic?
uses all baselinefeatures and the dynamic features described in Sec-tion 4.2.
?+Cluster?
uses all previous features and theclustering feature.
Even though our baseline systemalready has high performance, each model shows animprovement over its previous model (very slightfor ?+Cluster?).
The improvement is greater for theout-of-domain task, implying that the dynamic andclustering features help more on new domains.
Thedifferences between ?Baseline?
and ?+Dynamic?
arestatistically significant for both in and out-of domaintasks (Wilcoxon signed-rank test, treating each sen-tence as an individual event, p ?
0.025).Task P R F1BaselineAI 92.57 88.44 90.46AI+AC 87.20 83.31 85.21+DynamicAI 92.38 88.76 90.54AI+AC 87.33 83.91 85.59?+ClusterAI 92.62 88.90 90.72AI+AC 87.43 83.92 85.64JN (2008) AI+AC 88.46 83.55 85.93Table 4: Labeling accuracies evaluated on the WSJ (P:precision, R: recall, F1: F1-score, all in %).
?AI?
and?AC?
stand for argument identification and argument clas-sification, respectively.Task P R F1BaselineAI 90.96 81.57 86.01AI+AC 77.11 69.14 72.91+DynamicAI 90.90 82.25 86.36AI+AC 77.41 70.05 73.55?+ClusterAI 90.87 82.43 86.44AI+AC 77.47 70.28 73.70JN (2008) AI+AC 77.67 69.63 73.43Table 5: Labeling accuracies evaluated on the Brown.We also compare our results against another state-of-the-art system.
Unfortunately, no other system43has been evaluated with our exact environmental set-tings.
However, Johansson and Nugues (2008), whoshowed state-of-the-art performance in CoNLL?08,evaluated their system with settings very similar toours.
Their task was exactly the same as ours;given predicate identification, they evaluated theirdependency-based semantic role labeler for argu-ment identification and classification on the WSJand Brown corpora, distributed by the CoNLL?05shared task (Carreras and Ma`rquez, 2005).
Sincethe CoNLL?05 data was not dependency-based, theyapplied heuristics to build dependency-based predi-cate argument structures.
Their converted data mayappear to be a bit different from the CoNLL?09 datawe use (e.g., hyphenated words are tokenized by thehyphens in CoNLL?09 data whereas they are not inCoNLL?05 data), but semantic role annotations onheadwords should look very similar.Johansson and Nugues?s results are presented asJN (2008) in Tables 4 and 5.
Our final system showscomparable results against this system.
These re-sults are meaningful in two ways.
First, JN used agraph-based dependency parsing algorithm that gavehigher parsing accuracy for these test sets than thetransition-based dependency parsing algorithm usedin ClearParser (about 0.9% better in labeled attach-ment score).
Even with poorer parse output, our SRLsystem performed as well as theirs.
Furthermore,our system used only one set of features, whichmakes the feature engineering easier than JN?s ap-proach that used different sets of features for argu-ment identification and classification.6 Conclusion and future workThis paper makes two contributions.
First, we in-troduce a transition-based semantic role labeling al-gorithm that shows comparable performance againstanother state-of-the-art system.
The new algorithmtakes advantage of using previous predictions as fea-tures to make the next predictions.
Second, wesuggest a self-learning clustering technique that im-proves labeling accuracy slightly in both the do-mains.
The clustering technique shows potential forimproving performance in other new domains.These preliminary results are promising; however,there is still much room for improvement.
Since ouralgorithm is transition-based, many existing tech-niques such as k-best ranking (Zhang and Clark,2008) or dynamic programming (Huang and Sagae,2010) designed to improve transition-based parsingcan be applied.
We can also apply different kinds ofclustering algorithms to improve the quality of theverb clusters.
Furthermore, more features, such asnamed entity tags or dependency labels, can be usedto form a better representation of feature vectors forthe clustering.One of the strongest motivations for designing ourtransition-based SRL system is to develop a joint-inference system between dependency parsing andsemantic role labeling.
Since we have already de-veloped a dependency parser, ClearParser, basedon a parallel transition-based approach, it will bestraightforward to integrate this SRL system with theparser.
We will also explore the possiblity of addingempty categories during semantic role labeling.7 Related workNivre (2008) introduced several transition-based de-pendency parsing algorithms that have been widelyused.
Johansson and Nugues (2008) and Zhaoet al (2009) presented dependency-based semanticrole labelers showing state-of-the-art performancefor the CoNLL?08 and ?09 shared tasks in English.Scheible (2010) clustered predicate argument struc-tures using EM training and the MDL principle.Wagner et al (2009) used predicate argument clus-tering to improve verb sense disambiguation.AcknowledgmentsWe gratefully acknowledge the support of theNational Science Foundation Grants CISE-IIS-RI-0910992, Richer Representations for MachineTranslation, a subcontract from the Mayo Clinic andHarvard Children?s Hospital based on a grant fromthe ONC, 90TR0002/01, Strategic Health AdvancedResearch Project Area 4: Natural Language Pro-cessing, and a grant from the Defense AdvancedResearch Projects Agency (DARPA/IPTO) underthe GALE program, DARPA/CMO Contract No.HR0011-06-C-0022, subcontract from BBN, Inc.Any opinions, findings, and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewsof the National Science Foundation.44ReferencesX.
Carreras and L. Ma`rquez.
2005.
Introduction to theconll-2005 shared task: semantic role labeling.
In Pro-ceedings of the Ninth Conference on ComputationalNatural Language Learning.J.
D. Choi and M. Palmer.
2010.
Retrieving correct se-mantic boundaries in dependency structure.
In Pro-ceedings of ACL workshop on Linguistic Annotation.J.
D. Choi and M. Palmer.
2011.
Getting the most outof transition-based dependency parsing.
In Proceed-ings of the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistics, 28(3).J.
Hajic?, M. Ciaramita, R. Johansson, D. Kawahara,M.
A.
Mart?
?, L. Ma`rquez, A. Meyers, J. Nivre, S. Pado?,J.
S?te?pa?nek, P. Stran?a?k, M. Surdeanu, N. Xue, andY.
Zhang.
2009.
The conll-2009 shared task: Syntac-tic and semantic dependencies in multiple languages.In Proceedings of the 13th Conference on Computa-tional Natural Language Learning: Shared Task.J.
A. Hartigan.
1975.
Clustering Algorithms.
New York:John Wiley & Sons.J.
Henderson, P. Merlo, G. Musillo, and I. Titov.
2008.A latent variable model of synchronous parsing forsyntactic and semantic dependencies.
In Proceedingsof the Twelfth Conference on Computational NaturalLanguage Learning.T.
Hofmann and J. Puzicha.
1998.
Statistical models forco-occurrence data.
Technical report, MassachusettsInstitute of Technology.C.
Hsieh, K. Chang, C. Lin, S. S. Keerthi, and S. Sun-dararajan.
2008.
A dual coordinate descent methodfor large-scale linear svm.
In Proceedings of the 25thinternational conference on Machine learning.L.
Huang and K. Sagae.
2010.
Dynamic programmingfor linear-time incremental parsing.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics.R.
Johansson and P. Nugues.
2008.
Dependency-basedsemantic role labeling of PropBank.
In Proceedings ofthe 2008 Conference on Empirical Methods in NaturalLanguage Processing.S.
D. Kamvar, D. Klein, and C. D. Manning.
2002.
Inter-preting and extending classical agglomerative cluster-ing algorithms using a model-based approach.
In Pro-ceedings of the 9th International Conference on Ma-chine Learning.D.
Liu and D. Gildea.
2010.
Semantic role features formachine translation.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics.C.
Lo, J. Luo, and M. Shieh.
2009.
Hardware/softwarecodesign of resource constrained real-time systems.
InProceedings of the 5th International Conference on In-formation Assurance and Security.R.
Mcdonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In Pro-ceedings of the Annual Meeting of the European Amer-ican Chapter of the Association for ComputationalLinguistics.J.
Nivre.
2008.
Algorithms for deterministic incremen-tal dependency parsing.
Computational Linguistics,34(4).M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1).S.
Pradhan, W. Ward, and J. H. Martin.
2008.
Towardsrobust semantic role labeling.
Computational Linguis-tics: Special Issue on Semantic Role Labeling, 34(2).C.
Scheible.
2010.
An evaluation of predicate argumentclustering using pseudo-disambiguation.
In Proceed-ings of the 7th conference on International LanguageResources and Evaluation.D.
Shen and M. Lapata.
2007.
Using semantic rolesto improve question answering.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing and on Computational Natural Lan-guage Learning.I.
Titov, J. Henderson, P. Merlo, and G. Musillo.
2009.Online graph planarisation for synchronous parsing ofsemantic and syntactic dependencies.
In Proceedingsof the 21st International Joint Conference on ArtificialIntelligence.W.
Wagner, H. Schmid, and S. Schulte im Walde.2009.
Verb sense disambiguation using a predicate-argument-clustering model.
In Proceedings of theCogSci Workshop on Distributional Semantics beyondConcrete Concepts.J.
H. Ward.
1963.
Hierarchical grouping to optimize anobjective function.
Journal of the American StatisticalAssociation, 58(301).N.
Xue and M. Palmer.
2004.
Calibrating features for se-mantic role labeling.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing.Y.
Zhang and S. Clark.
2008.
A tale of two parsers: in-vestigating and combining graph-based and transition-based dependency parsing using beam-search.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing.H.
Zhao, W. Chen, and C. Kit.
2009.
Semantic depen-dency parsing of NomBank and PropBank: An effi-cient integrated approach via a large-scale feature se-lection.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing.45
