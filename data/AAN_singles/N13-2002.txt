Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 8?15,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsReducing Annotation Effort on Unbalanced Corpus based on Cost MatrixWencan Luo, Diane LitmanDepartment of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260, USA{wencan,litman}@cs.pitt.eduJoel ChanDepartment of PsychologyUniversity of PittsburghPittsburgh, PA 15260, USAchozen86@gmail.comAbstractAnnotated corpora play a significant role inmany NLP applications.
However, annota-tion by humans is time-consuming and costly.In this paper, a high recall predictor basedon a cost-sensitive learner is proposed as amethod to semi-automate the annotation ofunbalanced classes.
We demonstrate the ef-fectiveness of our approach in the context ofone form of unbalanced task: annotation oftranscribed human-human dialogues for pres-ence/absence of uncertainty.
In two datasets, our cost-matrix based method of uncer-tainty annotation achieved high levels of re-call while maintaining acceptable levels of ac-curacy.
The method is able to reduce humanannotation effort by about 80% without a sig-nificant loss in data quality, as demonstratedby an extrinsic evaluation showing that resultsoriginally achieved using manually-obtaineduncertainty annotations can be replicated us-ing semi-automatically obtained uncertaintyannotations.1 IntroductionAnnotated corpora are crucial for the developmentof statistical-based NLP tools.
However, the annota-tion of corpora is most commonly done by humans,which is time-consuming and costly.
To obtain ahigher quality annotated corpus, it is necessary tospend more time and money on data annotation.
Forthis reason, one often has to accept some tradeoffbetween data quality and human effort.A significant proportion of corpora are unbal-anced, where the distribution of class categories areheavily skewed towards one or a few categories.
Un-balanced corpora are common in a number of dif-ferent tasks, such as emotion detection (Ang etal., 2002; Alm et al 2005), sentiment classifica-tion (Li et al 2012), polarity of opinion (Carvalhoet al 2011), uncertainty and correctness of studentanswers in tutoring dialogue systems (Forbes-Rileyand Litman, 2011; Dzikovska et al 2012), textclassification (Forman, 2003), information extrac-tion (Hoffmann et al 2011), and so on1.In this paper, we present a semi-automated anno-tation method that can reduce annotation effort forthe class of binary unbalanced corpora.
Here is ourproposed annotation scheme: the first step is to builda high-recall classifier with some initial annotateddata with an acceptable accuracy via a cost-sensitiveapproach.
The second step is to apply this classifierto the rest of the unlabeled data, where the data arethen classified with positive or negative labels.
Thelast step is to manually check every positive labeland correct it if it is wrong.To apply this method to work in practice, two re-search questions have to be addressed.
The first oneis how to get a high-recall classifier.
High recallmeans only a low proportion of true positives aremisclassified (false negatives).
This property allowsfor only positive labels to be corrected by human an-notators in the third step, so that annotation effortmay be reduced.
A related and separate researchquestion concerns the overall quality of data whenfalse negatives are not corrected: will a dataset anno-tated with this method produce the same results as a1The unbalanced degrees - proportion of minority class cat-egory, of these corpora range from 3% to 24%.8fully manually annotated version of the same datasetwhen analyzed for substantive research questions?In this paper, we will answer the two researchquestions in the context of one form of binary un-balanced task2: annotation of transcribed human-human dialogue for presence/absence of uncertainty.The contribution of this paper is twofold.
First,an extrinsic evaluation demonstrates the utility ofour approach, by showing that results originallyachieved using manually-obtained uncertainty anno-tations can be replicated using semi-automaticallyobtained uncertainty annotations.
Second, a highrecall predictor based on a cost-sensitive learner isproposed as a method to semi-automate the annota-tion of unbalanced classes such as uncertainty.2 Related Work2.1 Reducing Annotation EffortA number of semi-supervised learning methods havebeen proposed in the literature for reducing annota-tion effort, such as active learning (Cohn et al 1994;Zhu and Hovy, 2007; Zhu et al 2010), co-training(Blum and Mitchell, 1998) and self-training (Mihal-cea, 2004).
Active learning reduces annotation bycarefully selecting more useful samples.
Co-trainingrelies on several conditional independent classifiersto tag new unlabeled data and self-training takesthe advantage of full unlabeled data.
These semi-supervised learning methods demonstrate that witha small proportion of annotated data, a classifier canachieve comparable performance with all annotateddata.
However, these approaches still need consid-erable annotation effort when a large corpus has tobe annotated.
In that case, all predicted labels haveto be rechecked by humans manually.
In addition,none of them take advantage of unbalanced data.Another class of effort reduction techniques ispre-annotation, which uses supervised machine-learning systems to automatically assign labels tothe whole data and subsequently lets human anno-tators correct them (Brants and Plaehn, 2000; Chiouet al 2001; Xue et al 2002; Ganchev et al 2007;Chou et al 2006; Rehbein et al 2012).Generally speaking, our annotation method be-longs to the class of pre-annotation methods.
How-2This annotation scheme can also benefit other kinds oftasks.ever, our method improves pre-annotation for unbal-anced data in two ways.
Firstly, we lower the thresh-old for achieving a high recall classifier.
Secondly,with pre-annotation, although people only performa binary decision of whether the automatic classifieris either right or wrong, they have to go through allthe unlabeled data one by one.
In contrast, in ourscheme, people go through only the positive predic-tions, which are much less than the whole unlabeleddata, due to the unbalanced structure of the data.What?s more, reducing the annotation effort is thegoal of this paper but not building a high recall clas-sifier such as Prabhakaran et al(2012) and Ambatiet al(2010).The approach proposed by Tetreault andChodorow (2008) is similar to us.
However, theyassumed they had a high recall classifier but did notexplicitly show how to build it.
In addition, theydid not provide extrinsic evaluation to see whether acorpus generated by pre-annotation is good enoughto be used in real applications.2.2 Uncertainty PredictionUncertainty is a lack of knowledge about internalstate (Pon-Barry and Shieber, 2011).
In this paper,we only focus on detection of uncertainty on text.Commonly used features are lexical features such asunigram (Forbes-Riley and Litman, 2011).
More-over, energy, dialogue features such as turn number,tutor goal, and metadata like gender are also con-sidered by Forbes-Riley and Litman (2011).
Un-certainty prediction is both substantively interesting(Chan et al 2012; Forbes-Riley and Litman, 2009)and pragmatically expeditious for our purposes, dueto its binary classification and typical unbalancedclass structure.CoNLL 2010 has launched a shared task to de-tect hedges and their scope in natural language texton two data sets: BioScope and Wikipedia (CoNLL,2010).
This first task to detect whether there is ahedge present or not present in a sentence is verysimilar to our uncertainty prediction task.
23 teamsparticipated in the shared task with the best re-call of 0.8772 on the BioScope, and 0.5528 on theWikipedia.
As we can see, uncertainty detection isnot trivial and it can be hard to get a high recall clas-sifier.In this paper, we focus on lexical features for our9purpose because lexical features are simple to ex-tract and sufficient for our scheme.
Even thoughother features may improve uncertainty predictionperformance, with the goal of reducing annotationeffort, such lexical features are shown to be goodenough for our task.3 The CorporaWe examine the following two data sets: the MarsExploration Rover (MER) mission (Tollinger et al2006; Paletz and Schunn, 2011) and the studentengineering team (Eng) dataset (Jang and Schunn,2012).
The MER scientists are evaluating datadownloaded from the Rover, discussing their workprocess, and/or making plans for the Rovers.
Theycome from a large team of about 100+ scien-tists/faculty, graduate students, and technicians.
Atany one time, conversations are between 2-10 peo-ple.
The Eng teams are natural teams of college un-dergraduates working on their semester-long prod-uct design projects.
The conversations involve 2-6individuals.
Audio and video are available for bothdata sets and transcripts are obtained with human an-notators.Our task is to annotate the transcribed human-human dialogues for presence/absence of uncer-tainty in each utterance.
There are 12,331 tran-scribed utterances in the MER data set, and 44,199transcribed utterances in the Eng data set.
Both datasets are unbalanced: in the MER data, 1641 of allthe 12,331 (13.3%) utterances are annotated as un-certain by trained human annotators; in the Eng data,only 1558 utterances are annotated, 221 of which areannotated as uncertain (14.2%).
96.5% of the utter-ances in the Eng data set have not been annotatedyet, raising the need for an efficient annotated tech-nique.
Both data sets are annotated by two trainedcoders with high inter-rater agreement, at Cohen?skappa of 0.75 (Cohen, 1960).
A sample dialoguesnippet from the MER corpus is shown in Table 1.The last column indicates whether the utterance islabeled as uncertainty or not: ?1?
means uncertaintyand ?0?
means certainty.The MER data serves as the initial annotated setand a high recall classifier will be trained on it; theEng data3 serves as a simulated unlabeled data set to3The Eng data in this paper denotes the annotated subset ofspeaker utterance uncertainty?S6 You can?t see the forest through the trees.
0S1 Yea, we never could see the [missing words] 1S6 No we had to get above it 0S4 We just went right through it 0S6 Yea 0S1 I still don?t, 0I?m not quite sure 1Table 1: Sample dialogue from the MER corpustest the performance of our annotation scheme.4 High Recall Classifier4.1 Basic ClassifierThe uncertainty prediction problem can be viewedas a binary classification problem.
It involves twosteps to build a high recall classifier for unbalanceddata.
The first step is to build up a simple classifier;the second step is to augment this classifier to favorhigh recall.Aiming for a simple classifier with high recall,only some lexical words/phrases are used as fea-tures here.
There are several resources for thewords/phrases of uncertainty prediction.
The mainresource is a guideline book used by our annotatorsshowing how to distinguish uncertainty utterance.
Itgives three different kinds of words/phrases, shownin Table 2 indicated by three superscripts ?+?, ?-?and ?*?.
The words/phrases with ?+?
show someevidence of uncertainty; ones with ?-?
mean thatthey show no evidence of uncertainty; others with?*?
may or may not show uncertainty.
The secondsource is from existing literature.
The words/phraseswith ?1?
are from (Hiraishi et al 2000) and oneswith ?2?
are from (Holms, 1999).For each word/phrase w, a binary feature is usedto indicate whether the word/phrase w is in the ut-terance or not.A Naive Bayes classifier is trained on the MERdata using these features and tested on the Eng data.The performances of the model on the train set andtest set are shown in Table 3.
Both weighted and un-weighted false positive (FP) Rate, Precision, Recalland F-Measure are reported.
However, in later ex-periments, we will focus on only the positive class(the uncertainty class).
A 0.689 recall means that510 out of 1641 positive utterances are missed usingthis model.the original Eng corpus.10as far as+ i hope+ somehow+ it will?
don?t remember?
maybe?
tends to?
doubtful1as far as i know+ i think+ something+ it wont?
essentially?
most?
that can vary?
good chance1as far as we know+ i thought+ something like this+ it would?
fairly?
mostly?
typically?
improbable1believe+ i wont+ worried that+ would it be?
for the most part?
normally?
uh?
possible1could+ im not sure+ you cannot tell+ about?
frequently?
pretty much?
um?
probable1guess+ may+ can?
almost?
generally?
quite?
usually?
relatively1guessed+ might+ i am?
any nonprecise amount?
hes?
should?
very?
roughly1guessing+ not really+ i can?
basically?
hopefully?
sometimes?
virtually?
tossup1i believe+ not sure+ i will?
believed?
i assumed that?
somewhat?
whatever?
unlikely1i cant really+ possibly+ i would?
cannot remember?
it sounds as?
somewhere?
you know?
of course2i feel+ probably+ it can?
can?t remember?
kind of?
stuff?
almost certain1 sort of2i guess+ really+ it is?
do not remember?
likely?
tend to?
almost impossible1Table 2: Words/phrases for uncertainty prediction.Data Set FP Rate Precision Recall F-Measure ClassMER.311 .954 .989 .971 0.011 .908 .689 .784 1.271 .948 .949 .946 (Weighted)Eng.475 .926 .981 .952 0.019 .817 .525 .639 1.41 .91 .916 .803 (Weighted)Table 3: Naive Bayes classifier performance on the MER(train set) and Eng (test set) with only the words/phrasesassume I didn?t know more or less some kindcouldn?t i don?t even know no idea supposedon?t know if not clear suspectdon?t think if it or thinkdon?t understand if we perhaps thoughtdoubt if you possibility uncleareither imagine potential what i understoodfigured kinda presumably wonderingi bet kinds of seemi can try like someTable 4: New words/phrases for uncertainty predictionAfter error analysis, a few new words/phrases areadded to the feature set, shown in Table 4.
By sup-plementing the original feature set in this way, wereran the training yielding our final baseline, theperformance on the training data (MER) and test-ing data (Eng) is shown in Table 5.
This time, wecompare different classifiers including Naive Bayes(NB), Decision Tree (DT) and Support Vector Ma-chine (SVM).
All of them are implemented using theopen source platform Weka (Hall et al 2009) withdefault parameters.As we can see, test recall is worse than train recall.Data Set Method TP FP Precision Recall F-MeasureMERNB .732 .016 .875 .732 .797DT .831 .013 .908 .831 .868SVM .811 .013 .905 .811 .855EngNB .679 .014 .888 .679 .769DT .665 .021 .84 .665 .742SVM .674 .022 .832 .674 .745Table 5: Performance with original and newwords/phrases as a feature set: train on the MERand test on the Eng data for class ?1?.
TP is true positive;FP is false positiveIn addition, although DT and SVM perform betterthan NB on train data set, they have similar perfor-mance on the test set.
Thus, the performance of thebaseline is not unacceptable, but neither is it stellar.In advance, it is not hard to build such a model, sinceonly simple features and classifiers are used here.4.2 Augmenting the Classifier using a CostMatrixIn our annotation framework, if the classifierachieves 100% recall, the annotated data will be per-fect because all the wrong predictions can be cor-rected.
That?s the reason why we are seeking for ahigh recall classifier.
A confusion matrix, is a com-mon way to represent classifier performance.
Highrecall is indexed by a low false negative (FN) rate;therefore, we aim to minimize FNs to achieve highrecall.Following this idea, we employ a cost-sensitivemodel, where the cost of FN is more than false pos-itive (FP).Following the same notation, we represent ourcost-sensitive classifier as a cost matrix.
In our costmatrix, classifying an actual class ?1?
as ?1?
costsCtp, an actual class ?0?
as ?1?
costs Cfp, an actualclass ?1?
to ?0?
costs Cfn, and ?0?
to ?0?
costs Ctn.To achieve a high recall, Cfn should be more thanCfp.We can easily achieve 100% recall by classifyingall samples to ?1?, but this would defeat our goal ofreducing human annotation effort, since all utteranceuncertainty predictions would need to be manuallycorrected.
Thus, at the same time of a high recall,we should also balance the total ratio of TP and FP.In our experiment, Ctp and Ctn are set to 0 sincethey are perfectly correct.
Additionally, Cfp = 1 allthe time and Cfn changes with different scales.
FPs11Cfn FP Rate Precision Recall F-Measure (TP + FP )/N1 .022 .831 .67 .742 .1142 .024 .825 .683 .748 .1173 .037 .771 .747 .759 .1385 .052 .726 .828 .774 .16210 .071 .674 .887 .766 .18715 .091 .622 .91 .739 .20720 .091 .622 .91 .739 .207Table 6: Test performance with cost matrixmean wrong predictions, but we can correct themduring the second pass to check them.
However, wecannot correct FNs without going through the wholedata set, so they are a more egregious detriment tothe quality of the annotated data.
During the exper-iment, Cfn varies from 1 to 20.
With increases inCfn, the cost of FN increases compared to FP.The cost-sensitive classifier is relying on Wekawith reweighting training instances.
In this task,SVM performed better than NB and DT.
Only SVMresults are included here due to space constraint.The test results are shown in Table 64.
The last col-umn in the two tables is the total proportion of pos-itive predictions (FP + TP ).
This value indicatesthe total amount of data that humans have to checkin the second pass to verify whether positive predic-tions are correct.
To reduce human annotation effort,we would like this value to be as low as possible.As shown in Table 6, with the increase ofCfn, therecall increases; however, the proportion of positivepredictions also increases.
Therefore, it is a tradeoffto achieve a high recall and a low ratio of TP and FP.For the test set, the recall increases with largerCfn, even with a small increase of Cfn from 1 to3.
Remarkably, the classifier gives us a high recallwhile keeping the proportion of positive predictionsat an acceptably low level.
When Cfn = 20 for thetest set, only 20.7% of the data need to be manuallychecked by humans, and less than 10% uncertain ut-terances (19 out of 221 for the Eng data) are missed.Now, we have achieved a high recall classifierwith an acceptable ratio of positive predictions.5 Extrinsic Evaluation of Semi-AutomatedAnnotationEven with a high recall classifier, some of the truepositive data are labeled incorrectly in the final an-4Only Cfn = 1, 2, 3, 5, 10, 15, 20 are reported here due topage limitsnotated corpus.
In addition, it also changes the dis-tribution of class labels.To test whether it hurts the overall data quality,we performed an analysis, which demonstrates thatthis annotation scheme is sufficient to produce qual-ity data.
We attempted to replicate an analysis on theEng data set, which examines the use of analogy, acognitive strategy where a source and target knowl-edge structure are compared in terms of structuralcorrespondences as a strategy for solving problemsunder uncertainty.
The analysis we attempt to repli-cate here focuses on examining how uncertainty lev-els change relative to baseline before, during, andafter the use of analogies.The overall Eng transcripts were segmented intoone of 5 block types: 1) pre-analogy (Lag -1) blocks,10 utterances just prior to an analogy episode, 2)during-analogy (Lag 0) blocks, utterances from thebeginning to end of an analogy episode, 3) post-analogy (Lag 1) blocks, 10 utterances immediatelyfollowing an analogy episode, 4) post-post-analogy(Lag 2) blocks, 10 utterances immediately follow-ing post-analogy utterances, and 5) baseline blocks,each block of 10 utterances at least 25 utterancesaway from the other block types.
The measure of un-certainty in each block was the proportion of uncer-tain utterances.
The sampling strategy for the base-line blocks was designed to provide an estimate ofuncertainty levels when the speakers were engagedin pre-analogy, during-analogy, or post-analogy con-versation, with the logic being that a certain amountof lag or spillover of uncertainty was assumed totake place surrounding analogy episodes.Figure 1 shows the relationship of block type tomean levels of uncertainty, comparing the patternwith human vs. classifier-supported uncertainty la-bels.
The classifier-generated labels were first pre-processed such that all FPs were removed, but FNsremain.
This re-analysis comparison thus providesa test of whether the recall rate is high enough thatknown statistical effects are not substantially alteredor removed.
To examine how different settings ofCfn might impact overall performance, we used la-bels (corrected for false positives) for 4 different lev-els of Cfn (1, 5, 10, 20) from the Table 6.In the Eng data analyses, the main findings werethat analogy was triggered by local spikes in un-certainty levels (Lag -1 > baseline), replicating re-12Baseline Lag ?1 Lag 0 Lag 1 Lag 200.050.10.150.20.25Block typeMean%uncertaintyinblockHumanCfn=20Cfn=10Cfn=5Cfn=1Figure 1: Mean % uncertainty by block type and labelsource (Eng data set)Table 7: Standardized mean difference (Cohen?s d) frombaseline by block type and label source (the Eng data set)(Note: ?*?
denotes p < .05, ?**?
denotes p < .01)Block typeLag -1 Lag 0 Lag 1 Lag 2Human 0.54?
0.4 0.79??
0.46?Cfn = 20 0.57?
0.3 0.78??
0.44Cfn = 10 0.58??
0.32 0.73??
0.47?Cfn = 5 0.57?
0.34 0.66??
0.48?Cfn = 1 0.42 0.25 0.54?
0.40sults from prior work with the MER dataset (Chanet al 2012); in contrast to the findings in MER,uncertainty did not reduce to baseline levels follow-ing analogy (Lags 1 and 2 > baseline).
Figure 1plots the relationship of block type to mean levelsof uncertainty in this data set, comparing the pat-tern with human vs. classifier-generated uncertaintylabels.
Table 7 shows the standardized mean differ-ence (Cohen?s d) (Cohen, 1988) from baseline byblock type and label source.
The pattern of effects(Lag -1 > baseline, Lags 1 and 2 > baseline) re-mains substantially unchanged with the exception ofthe Lag 2 vs. baseline comparison falling short ofstatistical significance (although note that the stan-dardized mean difference remains very similar) forCfn ranging from 20 to 5, although we can observea noticeable attenuation of effect sizes from Cfn of5 and below, and a loss of statistical significancefor the main effect of uncertainty being significantlyhigher than baseline for Lag -1 blocks when Cfn =1.The re-analysis clearly demonstrates that the re-call rate of the classifier is sufficient to not substan-tially alter or miss known statistical effects.
We canreasonably extrapolate that using this classifier foruncertainty annotation in other datasets should besatisfactory.6 Conclusion and DiscussionIn this paper, a simple high recall classifier is pro-posed based on a cost matrix to semi-automate theannotation of corpora with unbalanced classes.
Thisclassifier maintains a good balance between high re-call and high FP and NP ratio.
In this way, humanscan employ this classifier to annotate new data withsignificantly reduced effort (approximately 80% lesseffort, depending on the degree of imbalance in thedata).
Although the classifier does introduce somemisclassified samples to the final annotation, an ex-trinsic evaluation demonstrates that the recall rate ishigh enough and the performance does not sacrificedata quality.Like other semi-supervised or supervised meth-ods for supporting annotation, our annotationscheme has some limitations that should be noted.Firstly, an initial annotated data set is needed to de-rive a good performance classifier and the amountof annotated data is dependent on the specific task5.Secondly, the features and machine learning algo-rithms used in semi-supervised annotation are alsodomain specific.
At the same time, there are someunique challenges and opportunities that can be fur-ther investigated for our annotation scheme on un-balanced data.
For example, even though the costmatrix method can achieve a high recall for binaryclassification problem, whether it can be generalizedto other tasks (e.g., multi-class classification tasks)is an unanswered question.
Another open questionis how the degree of unbalance between classes inthe corpora affects overall annotation quality.
Wesuggest that if the data is not unbalanced, the totalamount of effort that can be reduced will be lower.AcknowledgmentsThe collection of the engineering data was supportedby NSF grants SBE-0738071, SBE-0823628, andSBE-0830210.
Analogy analysis was supported byNSF grant SBE-1064083.5For a new task, a new feature set is usually derived.13ReferencesCecilia Ovesdotter Alm, Dan Roth and Richard Sproat.2005.
Emotions from text: Machine learning fortext-based emotion prediction.
In Proceedings ofHLT/EMNLP 2005.Bharat Ram Ambati, Mridul Gupta, Samar Husain andDipti Misra Sharma.
2010.
A high recall error identi-fication tool for Hindi treebank validation.
In Proceed-ings of The 7th International Conference on LanguageResources and Evaluation (LREC), Valleta, Malta.Jeremy Ang, Rajdip Dhillon, Ashley Krupski, ElizabethShriberg and Andreas Stolcke.
2002.
Prosody-basedautomatic detection of annoyance and frustration inhuman-computer Dialog.
In INTERSPEECH-02.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Pro-ceedings of the eleventh annual conference on Compu-tational learning theory, p.92-100, July 24-26, Madi-son, Wisconsin, United StatesThorsten Brants and Oliver Plaehn.
2000.
Interactivecorpus annotation.
In Proceedings of LREC-2000.Paula Carvalho, Lu?
?s Sarmento, Jorge Teixeira and Ma?rioJ.
Silva.
2011.
Liars and saviors in a sentiment an-notated corpus of comments to political debates.
InProceedings of the Association for Computational Lin-guistics (ACL 2011), Portland, OR.Joel Chan, Susannah B. F. Paletz and Christian D.Schunn.
2012.
Analogy as a strategy for supportingcomplex problem solving under uncertainty.
Memory& Cognition, 40, 1352-1365.Fu-Dong Chiou, David Chiang andMartha Palmer.
2001.Facilitating treebank annotation using a statisticalparser.
In HLT?01.
ACL.Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su,Wei Ku1, Ting-Yi Sung and Wen-Lian Hsu.
2006.A semi-automatic method for annotating a biomedicalproposition bank.
In Proceedings of FLAC-2006.David Cohn, Richard Ladner and Alex Waibel.
1994.Improving generalization with active learning.
Ma-chine Learning, 15 (2), 201-221.Jacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Measure-ment, 20, 37-46.Jacob Cohen.
1988.
Statistical power analysis for thebehavioral sciences (2nd ed.).
Lawrence Erlbaum.CoNLL-2010 Shared Task.
2010.
In Fourteenth Con-ference on Computational Natural Language Learning,Proceedings of the Shared Task.Myroslava Dzikovska, Peter Bell, Amy Isard and Jo-hanna D. Moore.
2012.
Evaluating language under-standing accuracy with respect to objective outcomesin a dialogue system.
EACL 2012: 471-481.Kate Forbes-Riley and Diane Litman.
2009.
Adaptingto student uncertainty improves tutoring dialogues.
InProceedings 14th International Conference on Artifi-cial Intelligence in Education (AIED2009), pp.
33-40.Kate Forbes-Riley and Diane Litman.
2011.
Bene-fits and challenges of real-time uncertainty detectionand adaptation in a spoken dialogue computer tutor.Speech Communication, v53, pp.
1115-1136.George Forman 2003.
An Extensive empirical study offeature selection metrics for text classification.
Jour-nal of Machine Learning Research, 3, 1289-1305.Kuzman Ganchev, Fernando Pereira, Mark Mandel,Steven Carroll and Peter White.
2007.
Semi-automated named entity annotation.
In Proceedingsof the linguistic annotation workshop, pp.
53-56Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann and Ian H.Witten.
2009.The WEKA data mining software: An update.SIGKDD Explorations, 11(1).Taka Hiraishi, Buruhani Nyenzi, Jim Penman and SemereHabetsion.
2000.
Quantifying uncertainties in prac-tice.
In Revised 1996 IPCC guidelines for nationalgreenhouse gas inventories.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer and Daniel S. Weld.
2011.
Knowledge-based weak supervision for information extraction ofoverlapping relations.
In ACL.Janet Holmes.
1999.
Women, men, and politeness.
Lon-don, SAGE publications, pp:86-96Jooyoung Jang and Christian Schunn.
2012.
Physicaldesign tools support and hinder innovative engineer-ing design.
Journal of Mechanical Design, vol.
134,no.
4, pp.
041001-1-041001-9.Shoushan Li, Shengfeng Ju, Guodong Zhou and XiaojunLi.
2012.
Active learning for imbalanced sentimentclassification.
EMNLP-CoNLL 2012: 139-148Rada Mihalcea.
2004.
Co-training and self-training forword sense disambiguation.
In Proceedings of the8th Conference on Computational Natural LanguageLearning (CoNLL, Boston, MA).
33-40.Susannah B. F. Paletz and Christian D. Schunn.
2011.Assessing group-level participation in fluid teams:Testing a new metric.
Behav Res 43:522-536.Heather Pon-Barry and Stuart M. Shieber 2011.
Rec-ognizing uncertainty in speech.
EURASIP Journal onAdvances in Signal Processing.Vinodkumar Prabhakaran, Michael Bloodgood, MonaDiab, Bonnie Dorr, Lori Levin, Christine D. Piatko,Owen Rambow and Benjamin Van Durme 2012 Sta-tistical modality tagging from rule-based annotationsand crowdsourcing.
In Proceedings of ACL Workshopon Extra-propositional aspects of meaning in compu-tational linguistics (ExProM).14Ines Rehbein, Josef Ruppenhofer and Caroline Sporleder.2012.
Is it worth the effort?
Assessing the benefits ofpartial automatic pre-labeling for frame-semantic an-notation.
Language Resources and Evaluation, Vol.46,No.1.
pp.
1-23Joel R. Tetreault and Martin Chodorow.
Native judg-ments of non-native usage: experiments in preposi-tion error detection.
In Proceedings of the Workshopon Human Judgements in Computational Linguistics,p.24-32, Manchester, United Kingdom.Irene V. Tollinger, Christian D. Schunn and Alonso H.Vera.
2006.
What changes when a large team becomesmore expert?
Analyses of speedup in the Mars Explo-ration Rovers science planning process.
In Proceed-ings of the 28th Annual Conference of the CognitiveScience Society (pp.
840-845).
Mahwah, NJ: Erlbaum.Nianwen Xue, Fu-Dong Chiou andMartha Palmer.
2002.Building a large-scale annotated chinese corpus.
InProceedings of the 19th international conference onComputational linguistics.
ACL.Jingbo Zhu and Eduard Hovy.
2007.
Active learning forword sense disambiguation with methods for address-ing the class imbalance problem.
In Proceedings ofthe 2007 Joint Conference on Empirical Methods inNatural Language Processing and Computational Nat-ural Language Learning, 783-790.Jingbo Zhu, Huizhen Wang, Eduard H. Hovy andMatthew Y. Ma.
2010.
Confidence-based stoppingcriteria for active learning for data annotation.
ACMTransactions on Speech and Language Processing, 6,124.15
