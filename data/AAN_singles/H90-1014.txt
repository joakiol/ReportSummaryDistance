Evaluating Natural Language GeneratedDatabase RecordsRita McCardel lDepartment of DefenseFort Meade, Maryland 20755ABSTRACTWith the onslaught of various natural language pro-cessing (NLP) systems and their respective applica-tions comes the inevitable task of determining a wayin which to compare and thus evaluate the output ofthese systems.
This paper focuses on one such evalua-tion technique that originated from the text understand-ing system called Project MURASAKI.
This evaluationtechnique quantitatively and qualitatively measures thematch (or distance) from the output of one text under-standing system to the expected output of another.Introduct ionPro jec t  MURASAKIThe purpose of Project MURASAKI is to develop aforeign language text understanding system that willdemonstrate the extensibility of message understandingtechnology3 In its current design, Project MURASAKIwill process Spanish and Japanese text and extract in-formation in order to generate records in both naturallanguage databases, respectively.
The fields within thesedatabase records will contain a natural anguage phraseor expression in that respective language.The domain of Project MURASAKI is the diseaseAIDS.
The associated software system will include ageneral domain model of AIDS in the knowledge base.Within this model, there will be five subdomains:inc idence repor ts  records the occurrence of AIDSand HIV infection in countries and regions,among various populations,test ing pol icies covers measures to test groups forAIDS,campaigns  describes measures adopted to combatAIDS,new technolog ies  lists new equipment and materialused in detecting and preventing AIDS, and1Thus, it is no_...t to be confused as a message undel~tandingproject, but rather a multi-paragraph (i.e., text) understandingproject \[51.A IDS research  details the various vaccines and treat-ments that are being developed to preventAIDS.The subdomains of inc idence repor ts ,  test ing  poli-cies and campaigns  are found in the Spanish text whilethe topics of inc idence repor ts ,  new technologiesand A IDS research  are covered in the Japanese text.Project MURASAKI will demonstrate a sufficientlevel of full text understanding to be able to identifythe existence of factual information within either a givenSpanish or Japanese text that belongs within a partic-ular Spanish or Japanese language database.
Then, itwill determine what information in that text constitutesa single record in the selected atabase.The balance of this paper will focus on the evaluationtechnique: why it was chosen, some basic assumptionsunderlying it, as well as the design and application of thistechnique.
To illustrate various technical points of thistechnique, examples will be given using text excerptedfrom the Spanish AIDS corpus and its associated (gener-ated) Spanish database records.
Appendix A contains asample Spanish AIDS text (Text #124) and its Englishtranslation.
2 Appendix B contains a record from theIncidence Reporting database that was generated fromText #124.
Similarly, Appendix C contains a recordfrom the Testing Policies database that was also gener-ated from Text #124.The  Need fo r  a B lack  BoxGiven the overall design of this foreign language textunderstanding program, there arose the need for devel-oping a general purpose evaluation technique\[l\].
Thistechnique would compare the actual, computer generatedoutput of one such system to the expected, human gener-ated output of another.
That is to say, given some sam-ple piece of (foreign language) text as input, some pre-defined system output (namely, for project MURASAKI,the generation of a finite number of database records)could be manually generated so that a determinationas to the correct performance of the computer systemwas made.
Given this type of "correct" output, it could2In the MURASAKI text corpus, there do not exist any Englishtranslations for any of the text.therefore be possible to measure the performance of anautomated system based on this type of well-defined in-put /output  pairs.
It was precisely this type of ratio-nale that led to the development of a b lack box  eval-uat ion  - -  evaluation primarily focused on what a sys-tem produces externally rather than what a system doesinternally.
In direct contrast o this type of evaluation isglass box  eva luat ion - -  "looking inside the system andfinding ways of measuring how well it does something,rather than whether or not it does it" \[5\].With the development of the MURASAKI evaluationtechnique, comes the notion of two types of measures:a quantitative measure and a qualitative measure.
Thequant i ta t ive  measure  determines the number of cor-rect (and/or incorrect) records that have been generatedin any one database while the qua l i ta t ive  measureevaluates the "correctness" of any database record field.BackgroundSome Assumpt ionsGiven the overall design of Project MURASAKI, thereare a few assumptions, or rather, some groundwork thatneeds to be laid, in order to proceed in the developmentof this evaluation technique.
These assumptions are ex-plained as follows:?
Given the nature of the AIDS text corpus, any onetext could possibly generate one or more recordsin one or more databases.
This fact is loosely re-ferred to as domain complexity.
(Furthermore, forany record, all fields may not be filled.)?
Given the structure of the AIDS domain model, it isjust as easy (or hard) to distinguish one subdomainfrom another.
That is, each database is as likelyto have a record generated in it as another.
Thishypothesis is known as subdomain differentiation.?
Upon the determination of what the expected outputof Project MURASAKI should resemble, a correctrecord (in any database) is uniquely identified bythe contents of its key fields plus the contents of oneor more non-key fields.
This statement constitutesthe definition of a correct record.
3Generated  Output :  What  Cou ld  GoWrong?After a thorough analysis of the system flow for ProjectMURASAKI and given a typical AIDS text as system in-put, the following list represents all possible undesirablesituations that could arise:3 All appropriate information should be extracted from the textand placed in the correct database.
A change in any of the keyfields will result in the generation of a new record.
For example,if data from a different ime period is presented in the text, a keyfield change is required, and a new record is generated.
If data froma new region is presented, a new record is generated.
Examplesof key and non-key fields are found in Appendices B and C. Keyfields, which are found in the thick, darkened boxes, are the samethroughout each database.1.
Generate one or more records in the wrongdatabase.2.
Not  generate one or more records in the correctdatabase.3.
Generate too  many records in the correctdatabase, i.e., over-generate.4.
Generate too  few records in the correct database,i.e., under-generate.5.
Generate too  many fields in the correct record.6.
Generate too  few fields in the correct record.7.
Generate the wrong answer in the fields.Situations 1 and 2 illustrate what could go wrong atthe database level while scenarios 3 and 4 depict possi-ble problems arising at the database record level.
Theremaining criteria (namely 5, 6 and 7) shows what couldhappen at the database record field level.
However, themore crucial way of viewing these problems is not somuch in where  (i.e., at what level) these events occur,but rather in how these problems can be detected andthus measured for evaluation purposes.
It is with thismotivation that the following categorization was derived:a quantitative measure could be designed to account forthe problems that could arise at both the database anddatabase record levels while a qualitative measure couldcomparably be designed for evaluation at the databaserecord field level.In the next section, two examples are given depict-ing how the quantitative measure accounts for problemsarising at the first two levels.
(Note: 'rec.'
is the abbre-viation for record in these examples.
)A Quantitative MeasureBackgroundA scoring function is used for the quantitative measureto calculate an aggregate score for the number of correctrecords (as defined previously) generated ('gem' in thefollowing examples) for a given MURASAKI text.
Thisscoring function assigns one point for the generation of acorrect record ('coL') and -p  points, where 0 < p < 1,for the generation of an incorrect record ('inc.
').Some Quest ionsGiven the two examples in Table 1, the following ques-tions come to mind:?
What should be the value of p?
!?
i ?
17 Does 2" 3" 4"bounding it between 0 and 1 imply any linguisticrestrictions on focus or coverage of the text?
Orrather, should these bounds become parameters ofthis measure?Ex.
# i :  DB #I  DB #2 DB #3 TOTAL Ex.
#2: DB #i  DB #2 DB #3 TOTAL3 tee.
2 rec.
1 rec.
6 Text 124 3 rec.
1 rec.
0 rec.
4 Text xxxwhat if,where2 gen. 2 gen. 2 gen.1 cor.
2 cor.
2 inc.1 inc.(1 inc.)1-2p 2 -2p a-4p 6what if,where4 gen. 0 gen. 1 gen.3 cor.
1 inc. 1 inc.1 inc.3-p -p -pTable 1: Examples of How the Quantitative Measure Works3-3p4,?
Which is worse: to over-generate or under-generate?That is, should we have one penalty for one andanother penalty for the other?
(In Example #1 ofTable 1, the extra, or over-generated, record is alsopenalized by -p  points.
)* What happens if the numerator is negative?
Orequal to 0?
Should the score in these cases be 0??
If the score for a single text is Texti, then should thescoring algorithm for the overall (average) Quanti-tative Score be ~ where i = 1, 2, N andN ' " "  " 'N is the total number of text?A Qualitative MeasureBackgroundBefore proceeding into the design of the qualitative mea-sure, some background is needed in order to motivatethis measure.
For Project MURASAKI, a databasefield is defined to be logically equivalent o that of aSLOT while the contents of that field is equivalentto its F ILLER.
4 The slots define three types of DO-MAINS:  (1) unordered, e.g., OCCUPATIONS, (2) or-dered, e.g., MONTHS-OF-THE-YEAR and (3) contin-uous, e.g., HEIGHT.
The slot fillers have three types ofATTRIBUTES: (1) symbolic, e.g., (temperature(valuetepid)), (2) numeric, e.g., (weight(value 141.3)) and (3)hybrid, e.g., (test_results(value(i,000 people were de-ported))).
Also, the slot fillers have three types of CAR-D INAL ITY :  (1) single, e.g., (sex(value male)), (2) enu-merated, e.g., (subjects(value(math physics art))) and(3) range, e.g., (age(value(0 100))).The notion of IMPORTANCE VALUES (IVs) areintroduced here and are used to numerically describehow easy/hard it was (is) to extract a particular field's(or slot's) information from the text.
These importancevalues are assigned to both the key and the non-key fieldsof a database record for each of the five databases.
5 Im-portance values are integers from 1 to 10, inclusive, andare interpreted as follows:4The origination of this knowledge representat ion scheme(KRS) was taken from \[4\].
The appl icat ion of this KRS to ProjectMURASAKI  was taken from\[l\].5 Recall that  each database,  for both  Spanish and Japanese,  cor-responds to one of the five different subdomains  within the AIDSdomain  model.IV In terpreta t ion10 very easy to extract:5 moderately easy /hard  to extract:1 very hard  to extractWith this view of importance values 6, the extractionprocess for Project MURASAKI may now be consideredas two subprocesses; that is, extraction plus deduction.For example, the key field fuente (meaning "source")may be filled with OMS or any one of the other period-icals and technical papers that are listed in the headerline of each text (reference Appendix A, where the fuenteis El Pa(s).
Since the fuente field is constrained to onlya few possible fillers, an importance value of 9 has beenassigned to it.
7Scoring Functions & Algor i thmScoring functions are also used for the qualitative mea-sure to calculate an aggregate penalty for the fields (bothkey and non-key) in a database record.
There are threetypes of scoring functions based upon the cardinality ofthe slot fillers: (1) single, (2) enumerated and (3) range, sAn example of an ordered domain with single fillers isthat of TEMPERATURE:(make-frame TEMPERATURE(instance-of (value field))(database-in (value z))(element-type (value symbol))(domain-type (value ordered))(cardinality (value single))(elements (value cold cool tepidlukewarm warm hot scalding)))6l_nt'orrnal feedback thus far has indicated that  these values aregeared to hav ing more emphasis  placed on the records that  containeasier fields and less on the harder ones, thus  not  rewarding thosewho perform well on the harder  fields.ran  importance value of 10 would have been assigned had it notbeen for the fact that  in some instances,  the "deduction" port ion ofthe extract ion process for this field specifies the conversion of somesources to their respective acronym, e.g., OMS is OrganizacidnMundial de la Salud (WHO).Sin Project  MURASAKI ,  only slots that  contain single fillershave been identif ied thus far.66(The filler x in the database-in slot represents the sin-gle character identification value for a particular AIDSdatabase.)
Continuing with this example, if the follow-ing actual output (AO) were to be matched against whatwas expected (EO, expected output),AO: (temperature (value cool))EO: (temperature (value lukewarm))the penalty assigned to this mismatch would depend ontwo variables: (1) D, the distance between the fillers inthe ordered set of values and (2) C, the size of the do-main.
The scoring function that relates these two vari-ables isWxD P - - -  (1) f(c)where W is the numerical weight on the distance betweenthe fillers and :P is a damping function on the size of thedomain.As mentioned before, an example of an unordered o-main with single fillers is OCCUPATIONS.
Since the dis-tance, D, is not meaningful for this example, the penaltyassigned to the match becomes a function merely of thesize of the domain (and hence the probability of the cor-rect filler appearing):WP -  ~(C) (2)Consider the slot CASOS_NOTIFICADOS from theIncidence (I) Reporting database.
It is a continuous do-main with (single) numeric fillers and its attribute ntryis the following:(make-frame CASOS_NOTIFICADOS(instance-of (value field))(database-in (value I))(element-type (value number))(domain-type (value continuous))(cardinality (value single))(unit-size (value 1))(elements (value (0 1200.000))))As before, suppose we are trying to match theCASOS_NOTIFICADOS slots between the actual out-put and the expected output:AO: (casos_notificados (value 2.700))EO: (casos_notificados (value 2.781))Since only numbers can be represented in a continuousdomain, the elements of the domain are defined by giv-ing the endpoints of the domain (or closed interval) andthe unit size of representation is used in computing thedistance between fillers.
When defined in this manner,the same scoring function that was used for an ordereddomain with single fillers (namely Equation 1) can beused to compute the penalty for continuous domain setsas well.The overall Score for a single database record is?
Pi) (3)for i = 1, 2, ..., (number of fields in that databaserecord).
The Pi's are the computed penalties betweeneach field of the actual output and the expected outputfor that particular database record.
The IVy's are theimportance values for the corresponding fields of thatdatabase record.The Scoring Algorithm that computes the overall qual-itative measure for the entire text corpus is given below:for each TEXTfor each DB RECORDfor each DB RECORD FIELDif EO_field and  AO_field are equalthen  no penaltyelsebegincompute penalty ;;; based onappropriate scoring functionweight penalty ;;; according tothe IV of that fieldadd weighted penaltyto total record penaltyendSome Unreso lved  I ssuesSo far, fields that contain either numeric fillers or singleword fillers (fillers that are both easily "distanceable")have been discussed.
However, one would think that themore linguistically complex fields, i.e., those containinggenerated natural anguage phrases, would be more of atrue test for the qualitative measure of this evaluationtechnique.
Consider, for example, a non-key field likepoblaci6n ("population") (from Appendix C):AO: poblaei6n inmigrantesEO: poblac i6npersonas que pretendlan entrar en el pals ("people whotry to enter the country")How should one extend the current notion of the qual-ititative measure to include evaluating the distance be-tween natural anguage phrases of this kind?
It wouldappear that poblac i6n would be an unordered omaincontaining symbolic information.
However, what are theelements of this domain?
Should they have cardinalitysingle?
Should they include only those phrases that weregenerated from the expected output or should they addi-tionally include al_!
semantically equivalent phrases, i.e.,those containing a common set of semantic primitives orattributes, as well?
If the latter situation were to pre-vail, then, in the example listed above, should a penaltybe assessed?
If so, by how much?
Or rather, should onegroup together all semantically equivalent phrases andthen determine the distance between these classes?Consider another example of an unordered domainfield from the Testing Policies Database:AO: resu l tados  han deportado a 1000 personasque resultaronEO: resu l tados  desde 1985, han deportado a 1000personas que resultaronShould this non-key field be defined as having both asymbolic and numeric, i.e., hybrid, attribute?
If so,should a scoring function based on symbolic and numerictext be designed?
Given the example above, should apenalty be assigned for lack of a specific time element(in the actual output) or are these phrases emanticallyequivalent?A possible algorithmic extension to the current quali-tative measure is outlined as follows:1. for a given database field, obta in  and examine  allpossible fillers,2.
g roup/c lass i fy  semantically equivalent phrases(by those that share common semantic primi-tives/attributes, e.g., theme, agent, actor, time,etc.)
and then3.
ca lcu late the distance between each group/class(through determining by just how many semanticprimitives/attributes they differ from each other).If this approach were taken, the scoring function of Equa-tion i would be applicable where D would be the distancebetween classes of fillers rather than just between thefillers themselves.ConclusionIt is hoped that this evaluation technique will prove ef-fective for Project MURASAKI and thus become thebasis on which to develop a general purpose evaluationtool.
Research continues on answering those quant i ta -t ive questions and on resolving those qua l i ta t ive  issues.AcknowledgementsI would like to thank Roberta Merchant, Mary EllenOkurowski and John Prange for their assistance and sup-port with this work.
Also, I would like to thank Tom Do-err who was instrumental with the preparation of thisdocument.
But most of all, I would like to thank mymorn for everything.
It is in her memory that this paperwill be presented.References\[1\] MeCardell, R. 1990.
"An Evaluation Technique forSTUP Database Records".
An unpublished ocu-ment.\[2\] McCardell, R. 1988.
"Lexical Selection for NaturalLanguage Generation".
Thesis Proposal, ComputerScience Department, University of Maryland Balti-more County.\[3\]\[4\]\[5\]Merchant, R. and M. E. Okurowski.
Personal Com-munciation.
January ~ February, 1990.Nirenburg, S., R. McCardell, E. Nyberg, P. Werner,S.
Huffman, E. Kenschaft and I. Nirenburg.
1988.DIOGENES-88, CMU Technical Report CMU-CMT-88-107, Center for Machine Translation, CarnegieMellon University.Palmer, M., T. Finin, and S. M. Walter.
1989.
"Workshop on the Evaluation of Natural Lan-guage Processing Systems".
RADC-TR-89-302, Fi-nal Technical Report, Unisys Paoli Research Center.Append ix  A: Sample SpanishA IDS Text and Translat ion#~124 08ju189 E1 Pals Madrid palabras 899Los Emi ra tos  Arabes  Unidos han  depor tado ,desde 1985, a 1.000The United Arab Emirates has deported, since 1985,1,000personas  que resu l ta ron  posl t ivas en las pruebasde detecc i6n  del S IDA ypeople who tested positive on AIDS screening tests andque pretend lan  ent rar  en el pals.
Un  por tavozde su emba jada  enwho tried to enter the country.
An embassyspokesperson iEspaf ia  man i fes t6  que "es las soluci6n menosmala" ,  ya  que la nac i6n "esSpain said that "it is the less harmful solution", becausethe nation "ismuy pequef ia ,  t iene menos  de medio  mi l l6n dehab i tantes  y no puedevery small, it has less than half a million inhabitants,and it cannothacer  f rente  a los en fermos" .
La Organizac i6nMund ia l  de la Salud hacare for the patients".
The World Health Organizationreg is t rado  10.000 nuevos  casos de S IDA en elpasado mes de junio,registered 10,000 new cases of A IDS last June,ascend iendo el ndmero  to ta l  a 167.373.
Espaf iat lene 2.781 casosraising the total number to 167,373.
Spain has 2,781casesreg ist rados.registered.9This is the header line for Text #124.
This article was re-ported in the El Pais newspaper, located in Madrid, on July 8,1989 and contains 89 words.68Appendix B: An Incidence Report ing Database RecordINCIDENCIA DEL SIDAar'tfculo 124-021 fecha 00iun89 fuente El Palsregion todo el mundofuente de la information OMSVIH" varones mujerescategoriainfectados por VIH (porcentaje)infectados por VIH (estimados)infectados por VIH (notificados)modo de transmisionprevalencia: % de populaci6n detasa de progresion ai SIDA:tasa de progresi6n al SIDA:tasa de progresion al SIDA:tasa de progresi6n al SIDA:perfodo de duplicaci6nincremento mensualnifios% para% para% para% parameses%afiosafiosafiosafiosSlDA: varones mujeres nifioscasos notificados 10.000 nuevos casos en iunio 1989casos estimadosprevalencia:tasa de letalidadtasa de letalidadfallecidosfallecidosrelacibn m:fperiodo de duplicationpara afio(s)% de populaci6n de% / casos notificados en% / casos notificados antes de(n~mero)% de los casos notificadosmesesAppendix C: A Testing Policies Database RecordPRUEBAS CONTRA EL SIDAarticulo 124-01T fecha 08iul89 fuente El Paisregion Los Emiratos ,~rabes Unidosfuente de la informaciOn portavoz de Los Emiratos .a, rabes Unidos en Espafiaautoridad de acci6nnivel de acciOnperiodopoblaciOn personas que pretendian entrar en el paispoblaci6npoblaciOnpoblaci6nlocal de la pruebatipo de pruebatipo de pruebatipo de pruebaresultados desde 1985, han deport:ado a 1.000 personas que resultaron~ositivas
