Proceedings of the SIGDIAL 2013 Conference, pages 433?441,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsMulti-domain learning and generalization in dialog state trackingJason D. WilliamsMicrosoft Research, Redmond, WA, USAjason.williams@microsoft.comAbstractStatistical approaches to dialog state track-ing synthesize information across multi-ple turns in the dialog, overcoming somespeech recognition errors.
When traininga dialog state tracker, there is typicallyonly a small corpus of well-matched dia-log data available.
However, often there isa large corpus of mis-matched but relateddata ?
perhaps pertaining to different se-mantic concepts, or from a different dialogsystem.
It would be desirable to use thisrelated dialog data to supplement the smallcorpus of well-matched dialog data.
Thispaper addresses this task as multi-domainlearning, presenting 3 methods which syn-thesize data from different slots and differ-ent dialog systems.
Since deploying a newdialog state tracker often changes the re-sulting dialogs in ways that are difficult topredict, we study how well each methodgeneralizes to unseen distributions of dia-log data.
Our main result is the finding thata simple method for multi-domain learn-ing substantially improves performance inhighly mis-matched conditions.1 IntroductionSpoken dialog systems interact with users via nat-ural language to help them achieve a goal.
As theinteraction progresses, the dialog manager main-tains a representation of the state of the dialog in aprocess called dialog state tracking.
For example,in a bus schedule information system, the dialogstate might indicate the user?s desired bus route,origin, and destination.
Dialog state tracking isdifficult because errors in automatic speech recog-nition (ASR) and spoken language understanding(SLU) are common, and can cause the system tomisunderstand the user?s needs.
At the same time,state tracking is crucial because the system relieson the estimated dialog state to choose actions ?for example, which bus schedule information topresent to the user.Most commercial systems use hand-craftedrules for state tracking, selecting the SLU resultwith the highest confidence score observed so far,and discarding alternatives.
In contrast, statisti-cal approaches compute a posterior distributionover many hypotheses for the dialog state, andin general these have been shown to be superior(Horvitz and Paek, 1999; Williams and Young,2007; Young et al 2009; Thomson and Young,2010; Bohus and Rudnicky, 2006; Metallinou etal., 2013).Unfortunately, when training a dialog statetracker, there is rarely a large corpus of matcheddata available.
For example, a pilot version of thesystem may be fielded in a controlled environmentto collect a small initial corpus.
Yet there is of-ten a large quantity of mis-matched dialog dataavailable.
For example, dialog data might be avail-able from another dialog system ?
such as an ear-lier version with a different recognizer, dialog con-troller, and user population ?
or from a related task?
such as searching for restaurants instead of ho-tels.In this paper, we tackle the general problem ofhow to make use of disparate sources of datawhen training a dialog state tracker.
For exam-ple, should a tracker for each slot be trained onsmall sets of slot-specific data, or should data fromall slots be combined somehow?
Can dialog datafrom another system be used to build effectivetracker for a new system for which no data (yet)exists?
Once data from the new system is avail-able, is the old data still useful?These inter-related questions can be formalizedas multi-domain learning and generalization.Multi-domain learning (MDL) refers to the taskof building a model ?
here, a state tracker ?
for433a target domain using training data from both thetarget domain and a different but related domain.Generalization refers to the ability of a model toperform well in a domain unlike that seen in anyof the training data.
Both multi-domain learningand generalization are active research topics in themachine learning community, with broad applica-tions.
(Joshi et al 2012) provides a comparison ofpopular methods on several (non-dialog) tasks, in-cluding sentiment classification in on-line productreviews.In dialog state tracking, there are a variety ofproperties that could be cast as a ?domain?.
In thispaper, we explore two obvious domains: differentdialog systems, and different slots, where slots areinformational sub-units of the dialog state, such asthe origin, bus route, and departure time in a bustimetables spoken dialog system.
We apply sev-eral methods for MDL across varied dialog sys-tems, slots, and combinations of both.
MDL isattractive for dialog state tracking because the dis-tribution across slots and systems is related butnot identical.
For example, the ranges of speechrecognition confidence scores for two slots such asbus route and date may be different, or one systemmay use confirmations much more often than an-other.
Despite these differences, there are usefulpatterns: regardless of the slot or system, higherconfidence scores and responses of ?yes?
to con-firmations provide more certainty.
The hope is thatMDL can provide a principled way of using allavailable data to maximize accuracy.An important problem in dialog state tracking isthat deploying a new tracker into production willproduce a new distribution of dialog data that maybe unlike data observed at training time in waysthat are difficult to predict.
As a result, it is impor-tant to test the generalization of dialog state track-ing models on data that differs from the trainingdistribution.
In this paper, we evaluate each of theMDL approaches on multiple held-out datasets,ranging from well-matched to very mis-matched?
i.e., dialog data from the same dialog system, amodified version of the dialog system, and a com-pletely different dialog system.We show that dialog data from multiple existingsystems can be used to build good state trackersfor a completely new system, and that a simpleform of MDL improves generalization substan-tially.
We also find that, if well-matched data fromthat new system is available, the effect (positive ornegative) of MDL is slight.
Since in practice thelevel of mis-match can be difficult to predict, thissuggests that training with (a particular form of)MDL is the safest approach.This paper is organized as follows.
Section 2describes the algorithm used for state tracking andthe dialog data employed.
Section 3 then intro-duces methods for multi-domain learning.
Section4 presents results and Section 5 briefly concludes.2 PreliminariesWe begin by describing the core model used fordialog state tracking, and the source data.
Both ofthese will be important for the development of themulti-domain learning methods in Section 3.2.1 Dialog state tracking modelThere are two dominant approaches to statisti-cal methods for dialog state tracking.
Genera-tive approaches use generative models that capturehow the SLU results are generated from hiddendialog states (Horvitz and Paek, 1999; Williamsand Young, 2007; Young et al 2009; Thomsonand Young, 2010).
In contrast, discriminative ap-proaches use conditional models, trained in a dis-criminative fashion to directly estimate the distri-bution over a set of state hypotheses based on alarge set of informative features (Bohus and Rud-nicky, 2006).
Previous work has found that dis-criminative approaches yield better performance(Metallinou et al 2013), so we base our experi-ments on a discriminative model.We will assume that each dialog state hypothe-sis is described by a feature vector x, consisting of|x| = X features.
For example, a feature might bethe confidence score of the most recent recognitionresult corresponding to the hypothesis.
Featurescan also be included which describe the currentdialog context, such as how many times the targetslot has been requested or confirmed.
At a turn ina dialog with index i, there are N(i) dialog statehypotheses, each described by X features.
We de-note the concatenation of all N(i) feature vectorsas X(i), which has size XN(i).The dialog state tracking task is to take as in-put the complete feature vector X(i), and output adistribution over the N(i) hypotheses, plus an ad-ditional meta-hypothesis REST that indicates thatnone of the hypotheses is correct.
For training, la-bels y(i) indicate which of the N(i) hypotheses iscorrect, or else if none of them is correct.
By con-434Feats/hypGroup |X| |X?| Corpus Dialogs Mismatch to training dataA 90 54643 TRAIN2 None ?
same distribution715 TEST1 Low750 TEST2 MediumB 90 3161020 TRAIN3 None ?
same distribution438 TEST3 LowC 90 0 TEST4 HighTable 1: Corpora used in this paper.
|X| denotes the number of common features, and |X?| denotes thenumber of system-specific features.
The data in systems TEST1 and TEST3 has low mis-match to thetraining data because they use very similar dialog managers as in TRAIN2 and TRAIN3, respectively.The system in corpus TEST2 used a different dialog manager from TRAIN2, but the same set of systemactions, speech recognizer, and TTS, resulting in a medium level of mis-match.
The system in corpusTEST4 was completely different from any system in the training data.
On average there were approxi-mately 13 system turns and 13 user turns per dialog across all corpora.
The TRAIN* corpora are usedfor training, and the TEST* corpora are used for testing.
Complete details of the corpora are given in(Williams et al 2013).struction the hypotheses are disjoint; with the ad-dition of the REST meta-hypothesis, exactly onehypothesis is correct by construction.
After the di-alog state tracker has output its distribution, thisdistribution is passed to a separate, downstreamprocess that chooses what action to take next (e.g.,how to respond to the user).Note that the dialog state tracker is not predict-ing the contents of the dialog state hypotheses:the dialog state hypotheses?
contents and featuresare given by some external process ?
for exam-ple, simply enumerating all SLU values observedso far in the dialog.
Rather, the task is to pre-dict a probability distribution over the hypotheses,where the probability assigned to a hypothesis in-dicates the probability that it is correct.In our previous work, we developed adiscriminatively-trained maximum-entropy modelfor dialog state tracking (Metallinou et al 2013).The model estimates a single weight for eachfeature in x; to keep learning tractable, theseweights are shared across all state hypotheses be-ing scored.
The model includes L1 and L2 regu-larization.
This model was found to out-performgenerative models, rule-based approaches typi-cally used in industry, and competing discrimina-tive approaches.
The complete details are givenin (Metallinou et al 2013) and are not crucial tothis paper, because the multi-domain learning ap-proaches used here will not modify the learningalgorithm, but rather modify the features, as de-scribed below.2.2 Dialog dataWe use dialog data and evaluation methods fromthe Dialog State Tracking Challenge (Williamset al 2013; Williams et al 2012).
This datacomes from public deployments of dialog systemswhich provide bus schedule information for Pitts-burgh, USA.
Three different research groups ?
de-noted Groups A, B, and C ?
provided dialog sys-tems.
Each group used completely different sys-tems, composed of different speech recognizers,acoustic and language models, language under-standing, dialog design, and text-to-speech.
Thedifferences between systems from different groupswas substantial: for example, Group A and Csystems allowed users to provide any informationat any time, whereas Group B systems followeda highly directed flow, separately collecting eachslot.
In addition, Groups A and B fielded severalversions of their systems over a multi-year period?
these versions differed in various ways, such asacoustic models, confidence scoring model, statetracking method and parameters, number of sup-ported bus routes, presence of minor bugs, anduser population.
Differences across versions andgroups yielded differences in overall performanceand distributions in the data (Black et al 2011;Williams, 2012).
Following the dialog state track-ing challenge, we use these differences to test theability of dialog state tracking methods to gener-alize to new, unseen distributions of dialog data.Table 1 lists the groups, datasets, and the relative435match/mis-match between training and test data.In this data, there are 9 slots: the bus route, date,time, and three components each for the originand destination, roughly corresponding to streets,neighborhoods, and points-of-interest like univer-sities.
In this paper we will build trackers that op-erate on slots independently ?
i.e., at each turn, atotal of 9 trackers will each output a ranked list ofdialog state hypotheses for its slot.1 The state hy-potheses consist of all of the values for that slot ob-served so far in the dialog ?
either in an SLU resultor output by the system ?
plus the meta-hypothesisREST that represents the case that none of the ob-served values is correct.Each dialog state hypothesis is described by aset of features extracted from the dialog data.
TheDialog State Tracking Challenge provides datafrom all systems in a standard format, from whichwe extracted 90 features per dialog state hypoth-esis.
We refer to these as common features, be-cause they are available for all systems.
We de-note the concatenation of all common features forall hypotheses at a given turn as XA, XB , or XC ,subscripted based on the system from which theywere extracted.
In addition, the challenge dataincludes system-specific information.
From theGroup A and B logs we extracted 54 and 316system-specific features per hypothesis, respec-tively.
We denote the concatenation of all system-specific features for all hypotheses at a given turnas X?A or X?B , subscripted based on the systemfrom which they were extracted.
Group C logsprovided no additional system-specific informa-tion.
Examples of features are provided in the Ap-pendix.3 Multi-domain learning methods3.1 Models for multi-domain learningIn multi-domain learning (MDL), data instancesare of the form (X(i), y(i), d(i)), where X(i) arefeatures for instance i, y(i) is the label for instancei, and d(i) is the domain of instance i, where thereare a total of D domains.
The goal is to build agood model for Pd(y|X) ?
i.e., to predict the la-bel of an instance given its features and domain.A baseline model uses only data from domain d totrain Pd(y|X); MDL tackles the problem of howto build models that use data from all domains toimprove on this baseline.
In this paper, we con-1For simplicity, in this paper we do not consider joint statehypotheses, which include more than one slot.sider the fully-supervised case, where all of thetraining data has been labeled.We explore four ways of constructing models.First, in the IND baseline model, we build D sep-arate models using only data from a single do-main.
Next, in the POOL model, the data from alldomains is simply pooled together into one largecorpus; the single model trained on this corpus isused in all domains.
Each feature vector is aug-mented to include an indicator of the domain d(i)from which it originated, as this has been found toconfer much of the benefit of more complex MDLalgorithms (Joshi et al 2012).
The POOL modelcan be viewed as the simplest form of MDL.Next, the MDL1 model employs a simplebut powerful method for MDL developed by(Daume III, 2007).
For each data instance, a syn-thetic feature vector is formed with D + 1 blocksof size |X|.
Each block is set to all zeros, exceptfor block d(i) and block D + 1 which are both setto X(i).
For example, with D = 3 domains, thesynthetic feature vector for X(i) from domain 1would be ?X(i),0,0,X(i)?, and for X(j) from do-main 2 would be ?0,X(j),0,X(j)?, where 0 is avector of zeros of size |X|.
This synthetic corpusis then used to train a single model which is usedin any domain.This approach has been found to be successfulon a variety of machine learning tasks, includingseveral NLP tasks (Daume III, 2007).
To explainthe intuition, consider a single feature componentof X, X[k], which appears D + 1 times in thesynthetic feature vectors.
For model estimation,assume a standard loss function with a term thatpenalizes classification errors, and a regularizationterm that penalizes non-zero feature weights.
Intu-itively, if an individual scalar feature X[k] behavesdifferently in the domains, the classifier will preferthe per-domain copies, and assign a zero weight tothe final copy, reducing the error term of the lossfunction, at the expense of a small increase in theregularization term.
On the other hand, if an indi-vidual scalar feature X[k] behaves similarly acrossdomains, the model will prefer to assign a singlenon-zero weight to the final copy and zeros to theper-domain copies, as this will reduce the regular-ization term in the loss function.
In other words,the classifier will prefer the shared copy when do-ing so has little impact to accuracy ?
i.e., the clas-sifier chooses on a feature-by-feature basis whento keep domains separate, and when to pool do-436Synthetic feature vector encoding for data from:Method Target Slot Slot 1 Slot 2 ?
?
?
Slot 9SLOTIND1 X1 not used ?
?
?
not used2 not used X2 ?
?
?
not used?
?
?
?
?
?
?
?
?
?
?
?
?
?
?9 not used not used ?
?
?
X9SLOTPOOL all X1 X2 ?
?
?
X3SLOTMDL1 all X1,0, .
.
.
,0,X1 0,X2, .
.
.
,0,X2 ?
?
?
0,0, .
.
.
,X9,X9SLOTMDL21 X1,0,X1 0,X2,X2 ?
?
?
0,X9,X92 0,X1,X1 X2,0,X2 ?
?
?
0,X9,X9?
?
?
?
?
?
?
?
?
?
?
?
?
?
?9 0,X1,X1 0,X2,X2 ?
?
?
X9,0,X9Table 2: Synthetic features constructed for each multi-domain learning method applied to slots.
Here,the subscript on X indicates the slot it describes.mains.When the number of domains D is large,MDL1 can produce large, sparse synthetic featurevectors, confounding training.
MDL2 addressesthis by constructing D separate models; in modeld, data from all domains except d is pooled intoone meta-domain.
Then the procedure in MDL1is followed.
For example, for model d = 1, in-stances X(i) from domain d(i) = 1 is representedas ?X(i),0,X(i)?
; data from all other domainsd(i) 6= 1 is represented as ?0,X(i),X(i)?.
Thissynthetic data is then used to train a model for do-main 1.3.2 Application to dialog state trackingIn this study, we consider two orthogonal dimen-sions of domain ?
systems and slots ?
and combi-nations of the two.Multi-domain learning across slots meansbuilding a tracker for one slot using dialog datapertaining to that slot, plus data pertaining to otherslots.
In the experiments below, this is done bytreating each of the 9 slots as a domain and apply-ing each of the four MDL methods above.
Table 2specifies the precise form of the synthetic featurevectors for each method.Multi-domain learning across systems meansbuilding a tracker for one dialog system using dia-log data collected with that system, plus data fromother dialog systems.
Each of the two corpora inthe training data ?
TRAIN2 from Group A andTRAIN3 from Group B ?
is treated as a domain.Since only the common features are shared acrossdomains (i.e., systems), model complexity can bereduced by building different models dependingon the target group ?
the group the model willbe tested on ?
and including system-specific fea-tures only for the target group.
For example, whena model will be trained on data from Groups Aand B, then tested on data from Group A, we in-clude common features from A and B but system-specific features from only A.
Table 3 specifies theprecise form of the synthetic feature vectors foreach method.
Also, when MDL is applied acrosssystems, there are only 2 sources of training data,so MDL2 is identical to MDL1 (and thus isn?tshown in the results).Applying multi-domain learning to both sys-tems and slots is done by composing the two fea-ture synthesis steps.
This process is simple but canincrease the size of synthetic feature vectors by upto an order of magnitude.3.3 Evaluation methodIn the experiments below, we train dialog statetrackers that output a scored list of dialog statehypotheses for each slot at each turn in the dia-log.
For evaluation, we measure the fraction ofoutput lists where the top dialog state hypothesisis correct.
A dialog state hypothesis is correct ifit corresponds to a slot value which has been rec-ognized correctly.
The dialog state tracker mayinclude the meta-hypothesis REST among its hy-potheses ?
this meta-hypothesis is labeled as cor-rect if no correct values have yet been recognizedfor this slot.Since most turns contain no information aboutmost slots, we limit evaluation to turns where newinformation for a slot appears either in the speechrecognition output, or in the system output.
For437Synthetic feature vectorencoding for data from:Method Target group Group A Group BSYSTEMIND A XA,X?A not usedB not used XB,X?BSYSTEMIND-A C XA not usedSYSTEMIND-B C not used XBSYSTEMPOOLA XA,X?A XB,0B XA,0 XB,X?BC XA XBSYSTEMMDL A XA,X?A,0,XA 0,0,XB,XBB 0,0,XA,XA XB,X?B,0,XBTable 3: Synthetic features constructed for each multi-domain learning method applied to systems.
Here,the subscript on X indicates the system it originated from.
Asterisk super-scripts indicate system-specificfeatures, which are only included for the group the tracker will be tested on (i.e., the target group).example, in turn i, if a system confirms a bus route,and a date appears in the speech recognition out-put, both of these slots in turn i will be includedwhen computing average accuracy.
If the time slotappears in neither the system output nor anywherein the speech recognition output of turn i, then thetime slot in turn i is excluded when computing av-erage accuracy.
The accuracy computation itselfwas done by the scoring tool from the Dialog StateTracking Challenge, using the schedule2 accuracymetric for all slots (Williams et al 2013; Williamset al 2012).For comparison, we also report performance ofa simple rule-based tracker.
For each slot, thistracker scans over all values recognized so far inthe dialog, and returns the value which has beenrecognized with the highest local SLU confidencescore.4 ResultsWe first evaluated performance of multi-domainlearning in isolation, excluding the effects of gen-eralization.
To do this, we divided TRAIN2 andTRAIN3 in half, using the first halves for train-ing and the second halves for testing.
This ex-periment gives an indication of the performance ofmulti-domain learning if conditions in deploymentmatch the training data.Results are shown in Figure 1a-1b.
Here, theeffects of multi-domain learning across systemsand slots is rather small, and inconsistent.
For ex-ample, pooling slot data yields best performanceon TRAIN3, and worst performance in TRAIN2.Applying MDL across systems yields best perfor-mance for TRAIN3, but not for TRAIN2.
Overall,when training and test data are very well-matched,MDL has little effect.Of course, in practice, training and test data willnot be well-matched, so we next evaluated per-formance of multi-domain learning including theeffects of generalization.
Here we trained usingthe complete TRAIN2 and TRAIN3 corpora, andtested on TEST1, TEST2, TEST3, and TEST4.Results are shown in Figures 1c-1f.
The dom-inant trend is that, at high levels of mis-match asin TEST3 and TEST4, simply pooling together allavailable data yields a large increase in accuracycompared to all other methods.
The majority ofthe increase is due to pooling across slots, thoughpooling across systems yields a small additionalgain.
This result echos past work, where poolingdata is often competitive with more sophisticatedmethods for multi-domain learning (Joshi et al2012).In our case, one possible reason for this resultis that simply pooling the data introduces a sort ofregularization: note that the models with SLOT-POOL and SYSTEMPOOL have the highest ratioof training data to model parameters.
The MDLmethods also use all the data, but via their largersynthetic feature vectors, they increase the numberof model parameters.
The smaller model capacityof the POOL models limit the ability to completelyfit the training data.
This limitation can be a li-ability for matched conditions ?
see for exampleFigure 1a ?
but may help the model to generalize43872%74%76%78%80%82%84%SystemInd SystemPool SystemMDL1SlotInd SlotPool RuleTrackerSlotMDL1 SlotMDL2(a) Evaluation on TRAIN2 (Group A), in which there is min-imal mis-match between the training and test data.66%68%70%72%74%76%78%SystemInd SystemPool SystemMDL1(b) Evaluation on TRAIN3 (Group B), in which there is min-imal mis-match between the training and test data.70%72%74%76%78%80%82%SystemInd SystemPool SystemMDL1(c) Evaluation on TEST1 (Group A), in which there is lowmis-match between the training and test data.52%54%56%58%60%62%64%SystemInd SystemPool SystemMDL1(d) Evaluation on TEST3 (Group B), in which there is lowmis-match between the training and test data.59%61%63%65%67%69%71%SystemInd SystemPool SystemMDL1(e) Evaluation on TEST2 (Group A), in which there ismedium mis-match between the training and test data.58%60%62%64%66%68%70%SystemInd-A SystemInd-B SystemPool(f) Evaluation on TEST4 (Group C), in which there is highmis-match between all of the training data and test data.Figure 1: Average accuracy of different approaches to multi-domain learning in dialog state tracking.Squares show SLOTIND, circles SLOTPOOL, unshaded diamonds SLOTMDL1, and shaded diamondsSLOTMDL2.
The solid line shows performance of a simple rule-based tracker, which is not trained ondata.
In all plots, the vertical axis is shown on the same scale for comparability (12% from bottom to top),and indicates average accuracy of the top dialog state (c.f., Section 3.3).
In panels 1a and 1b, training isdone on the first halves of TRAIN2 and TRAIN3, and testing on the second halves.
In the other panels,training uses all of TRAIN2 and TRAIN3.
In panel 1f, the categories for TEST4 ?
for which there is noin-domain data ?
are different than the other panels.439in mis-matched conditions.5 ConclusionThis paper has examined multi-domain learningand generalization in dialog state tracking.
Twodimensions of domain have been studied ?
learn-ing across slots and learning across systems ?
andthree simple methods for multi-domain learninghave been studied.
By using corpora of real di-alogs from the Dialog State Tracking Challenge,generalization has been studied through varyinglevels of mis-match between training and test data.The results show that simply pooling togetherdata yields large benefits in highly mis-matchedconditions and has little effect in well-matchedconditions.
In practice of course, the level of mis-match a new tracker will produce is difficult to pre-dict.
So the safest strategy seems to be to alwayspool together all available data.There are a variety of issues to examine in futurework.
First, the MDL methods used in this studywere chosen for their simplicity and versatility: byaugmenting features, no changes were required tothe learning method.
There exist other methods ofMDL which do modify the learning, and in somecases yield better performance.
It would be inter-esting to test them next, perhaps including meth-ods that can construct deeper representations thanthe maximum entropy model used here.More broadly, this study has been limited to su-pervised multi-domain learning, in which labeleddata from multiple domains is available at trainingtime.
It would clearly be desirable to develop amethod for unsupervised adaptation, in which themodel is adjusted as the unlabeled test data is ex-perienced.For now, the contribution of this study is to pro-vide at least an initial recommendation to prac-titioners on how to best make use of disparatesources of dialog data when building a statisticaldialog state tracker.AcknowledgementsThanks to Dan Bohus for making his machinelearning software available.ReferencesAlan W Black, Susanne Burger, Alistair Conkie, He-len Hastie, Simon Keizer, Oliver Lemon, NicolasMerigaud, Gabriel Parent, Gabriel Schubiner, BlaiseThomson, Jason D. Williams, Kai Yu, Steve Young,and Maxine Eskenazi.
2011.
Spoken dialog chal-lenge 2010: Comparison of live and control test re-sults.
In Proc SIGdial Workshop on Discourse andDialogue, Portland, Oregon.Dan Bohus and Alex Rudnicky.
2006.
A ?K hypothe-ses + other?
belief updating model.
In Proc Amer-ican Association for Artificial Intelligence (AAAI)Workshop on Statistical and Empirical Approachesfor Spoken Dialogue Systems, Boston.Hal Daume III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages256?263, Prague, Czech Republic, June.
Associa-tion for Computational Linguistics.Eric Horvitz and Tim Paek.
1999.
A computationalarchitecture for conversation.
In Proc 7th Interna-tional Conference on User Modeling (UM), Banff,Canada, pages 201?210.Mahesh Joshi, Mark Dredze, William W Cohen, andCarolyn Rose.
2012.
Multi-domain learning: Whendo domains matter?
In Proc Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,Jeju, Korea.Angeliki Metallinou, Dan Bohus, and Jason D.Williams.
2013.
Discriminative state tracking forspoken dialog systems.
In Proc Association forComputational Linguistics, Sofia.Blaise Thomson and Steve Young.
2010.
Bayesianupdate of dialogue state: A POMDP framework forspoken dialogue systems.
Computer Speech andLanguage, 24(4):562?588.Jason D Williams and Steve Young.
2007.
Partiallyobservable Markov decision processes for spokendialog systems.
Computer Speech and Language,21(2):393?422.Jason D Williams, Antoine Raux, Deepak Ramachan-dran, and Alan W Black.
2012.
Dialog statetracking challenge handbook.
Technical report, Mi-crosoft Research.Jason D. Williams, Antoine Raux, Deepak Ramachan-dran, and Alan Black.
2013.
The dialog state track-ing challenge.
In Submitted to SigDial 2013.Jason D. Williams.
2012.
Challenges and oppor-tunities for state tracking in statistical spoken dia-log systems: Results from two public deployments.IEEE Journal of Selected Topics in Signal Process-ing, Special Issue on Advances in Spoken DialogueSystems and Mobile Interface, 6(8):959?970.Steve Young, Milica Gas?ic?, Simon Keizer, Franc?oisMairesse, Jost Schatzmann, Blaise Thomson, andKai Yu.
2009.
The hidden information state model:a practical framework for POMDP-based spoken di-alogue management.
Computer Speech and Lan-guage, 24(2):150?174.440AppendixExample common features extracted for all systemsNumber of times slot value has been observed in any previous speech recognition resultWhether the most recent speech recognition result includes this slot valueThe highest rank on the speech recognition N-best list that this slot value has been observedThe number of times this slot has been requested by the systemWhether the system requested this slot in the current turnThe number of items on the current speech recognition N-best listWhether confirmation for this slot has been attemptedIf confirmation for this slot has been attempted, whether the user was recognized as saying ?yes?The fraction of recognitions of this slot value in the training set which were correctThe fraction of dialogs in the training set in which the user requested this slot valueExample system-specific features extracted for Group A systemsAcoustic model scoreAverage word confidence scoreWhether barge-in was triggeredDecoder scoreLanguage model scoreMaximum and minimum confidence score of any wordEstimated speaking rateEstimated speaker gender (male/female)Example system-specific features extracted for Group B systemsScore of best path through the word confusion networkLowest score of any word on the best path through the word confusion networkNumber of speech frames foundDecoder costGarbage model likelihoodNoise model likelihoodAverage difference in decoder cost, per frame, between the best path and any path through the latticeWhether barge-in was triggeredTable 4: Examples of features used for dialog state tracking.
Group C logs provided no system-specificinformation.441
