Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 59?65, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsUNITOR-CORE TYPED: Combining Text Similarityand Semantic Filters through SV RegressionDanilo Croce, Valerio Storch and Roberto BasiliDepartment of Enterprise EngineeringUniversity of Roma, Tor Vergata00133 Roma, Italy{croce,storch,basili}@info.uniroma2.itAbstractThis paper presents the UNITOR system thatparticipated in the *SEM 2013 shared task onSemantic Textual Similarity (STS).
The task ismodeled as a Support Vector (SV) regressionproblem, where a similarity scoring functionbetween text pairs is acquired from examples.The proposed approach has been implementedin a system that aims at providing high ap-plicability and robustness, in order to reducethe risk of over-fitting over a specific datasets.Moreover, the approach does not require anymanually coded resource (e.g.
WordNet), butmainly exploits distributional analysis of un-labeled corpora.
A good level of accuracy isachieved over the shared task: in the TypedSTS task the proposed system ranks in 1st and2nd position.1 IntroductionSemantic Textual Similarity (STS) measures the de-gree of semantic equivalence between two phrasesor texts.
An effective method to compute similaritybetween sentences or semi-structured material hasmany applications in Natural Language Processing(Mihalcea et al 2006) and related areas such asInformation Retrieval, improving the effectivenessof semantic search engines (Sahami and Heilman,2006), or databases, using text similarity in schemamatching to solve semantic heterogeneity (Islam andInkpen, 2008).This paper describes the UNITOR system partic-ipating in both tasks of the *SEM 2013 shared taskon Semantic Textual Similarity (STS), described in(Agirre et al 2013):?
the Core STS tasks: given two sentences, s1and s2, participants are asked to provide a scorereflecting the corresponding text similarity.
It isthe same task proposed in (Agirre et al 2012).?
the Typed-similarity STS task: given twosemi-structured records t1 and t2, containingseveral typed fields with textual values, partic-ipants are asked to provide multiple similarityscores: the types of similarity to be studied in-clude location, author, people involved, time,events or actions, subject and description.In line with several participants of the STS 2012challenge, such as (Banea et al 2012; Croce et al2012a; S?aric?
et al 2012), STS is here modeled asa Support Vector (SV) regression problem, where aSV regressor learns the similarity function over textpairs.
The semantic relatedness between two sen-tences is first modeled in an unsupervised fashionby several similarity functions, each describing theanalogy between the two texts according to a spe-cific semantic perspective.
We aim at capturing sep-arately syntactic and lexical equivalences betweensentences and exploiting either topical relatedness orparadigmatic similarity between individual words.Such information is then combined in a supervisedschema through a scoring function y = f(~x) overindividual measures from labeled data through SVregression: y is the gold similarity score (providedby human annotators), while ~x is the vector of thedifferent individual scores, provided by the chosensimilarity functions.For the Typed STS task, given the specificity ofthe involved information and the heterogeneity oftarget scores, individual measures are not applied toentire texts.
Specific phrases are filtered accordingto linguistic policies, e.g.
words characterized byspecific Part-of-Speech (POS), such as nouns andverbs, or Named Entity (NE) Category, i.e.
men-59tions to specific name classes, such as of a PER-SON, LOCATION or DATE.
The former allows tofocus the similarity functions over entities (nouns)or actions (verbs), while the latter allows to focus onsome aspects connected with the targeted similarityfunctions, such as person involved, location or time.The proposed approach has been implemented ina system that aims at providing high applicabilityand robustness.
This objective is pursued by adopt-ing four similarity measures designed to avoid therisk of over-fitting over each specific dataset.
More-over, the approach does not require any manuallycoded resource (e.g.
WordNet), but mainly exploitsdistributional analysis of unlabeled corpora.
Despiteof its simplicity, a good level of accuracy is achievedover the 2013 STS challenge: in the Typed STS taskthe proposed system ranks 1st and 2nd position (outof 18); in the Core STS task, it ranks around the 37thposition (out of 90) and a simple refinement to ourmodel makes it 19th.In the rest of the paper, in Section 2, the employedsimilarity functions are described and the applica-tion of SV regression is presented.
Finally, Section3 discusses results on the *SEM 2013 shared task.2 Similarity functions, regression andlinguistic filteringThis section describes the approach behind the UN-ITOR system.
The basic similarity functions andtheir combination via SV regressor are discussed inSection 2.1, while the linguistic filters are presentedin Section 2.2.2.1 STS functionsEach STS function depends on a variety of linguisticaspects in data, e.g.
syntactic or lexical information.While their supervised combination can be derivedthrough SV regression, different unsupervised esti-mators of STS exist.Lexical Overlap.
A basic similarity function ismodeled as the Lexical Overlap (LO) between sen-tences.
Given the sets Wa and Wb of words oc-curring in two generic texts ta and tb, LO is esti-mated as the Jaccard Similarity between the sets, i.e.LO= |Wa?Wb||Wa?Wb| .
In order to reduce data sparseness,lemmatization is applied and each word is enrichedwith its POS to avoid the confusion between wordsfrom different grammatical classes.Compositional Distributional Semantics.
Othersimilarity functions are obtained by accounting forthe syntactic composition of the lexical informationinvolved in the sentences.
Basic lexical informationis obtained by a co-occurrence Word Space that isbuilt according to (Sahlgren, 2006; Croce and Pre-vitali, 2010).
Every word appearing in a sentence isthen projected in such space.
A sentence can be thusrepresented neglecting its syntactic structure, by ap-plying an additive linear combination, i.e.
the so-called SUM operator.
The similarity function be-tween two sentences is then the cosine similarity be-tween their corresponding vectors.A second function is obtained by applying a Dis-tributional Compositional Semantics operator, inline with the approaches introduced in (Mitchell andLapata, 2010), and it is adopted to account for se-mantic composition.
In particular, the approach de-scribed in (Croce et al 2012c) has been applied.It is based on space projection operations over ba-sic geometric lexical representations: syntactic bi-grams are projected in the so called Support Sub-space (Annesi et al 2012), aimed at emphasiz-ing the semantic features shared by the compoundwords.
The aim is to model semantics of syntac-tic bi-grams as projections in lexically-driven sub-spaces.
In order to extend this approach to handleentire sentences, we need to convert them in syn-tactic representations compatible with the compo-sitional operators proposed.
A dependency gram-mar based formalism captures binary syntactic re-lations between the words, expressed as nodes ina dependency graph.
Given a sentence, the parsestructure is acquired and different triples (w1, w2, r)are generated, where w1 is the relation governor, w2is the dependent and r is the grammatical type.
In(Croce et al 2012c) a simple approach is defined,and it is inspired by the notion of Soft Cardinal-ity, (Jimenez et al 2012).
Given a triple set T ={t1, .
.
.
, tn} extracted from a sentence S and a sim-ilarity sim(ti, tj), the Soft Cardinality is estimatedas |S|?sim u?|T |ti (?|T |tj sim(ti, tj)p)?1, where pa-rameter p controls the ?softness?
of the cardinality:with p = 1 element similarities are unchanged whilehigher value will tend to the Classical Cardinalitymeasure.
Notice that differently from the previous60usage of the Soft Cardinality notion, we did not ap-ply it to sets of individual words, but to the sets ofdependencies (i.e.
triples) derived from the two sen-tences.
The sim function here can be thus replacedby any compositional operator among the ones dis-cussed in (Annesi et al 2012).
Given two sen-tences, higher Soft Cardinality values mean that theelements in both sentences (i.e.
triples) are different,while the lower values mean that common triples areidentical or very similar, suggesting that sentencescontain the same kind of information.
Given the setsof triples A and B extracted from the two candidatesentences, our approach estimates a syntactically re-stricted soft cardinality operator, the Syntactic SoftCardinality (SSC) as SSC(A,B) = 2|A?B|?|A|?+|B|?
, asa ?soft approximation?
of Dice?s coefficient calcu-lated on both sets1.capture::vVBNROOTmarine::nNNSPREP-BYmexico::nNNPPREP-INlord::nNNNSUBJdrug::nNNNNFigure 1: Lexical Centered Tree (LCT)Convolution kernel-based similarity.
The similar-ity function is here the Smoothed Partial Tree Ker-nel (SPTK) proposed in (Croce et al 2011).
SPTKis a generalized formulation of a Convolution Ker-nel function (Haussler, 1999), i.e.
the Tree Kernel(TK), by extending the similarity between tree struc-tures with a function of node similarity.
The maincharacteristic of SPTK is its ability to measure thesimilarity between syntactic tree structures, whichare partially similar and whose nodes can differ butare semantically related.
One of the most importantoutcomes is that SPTK allows ?embedding?
exter-nal lexical information in the kernel function onlythrough a similarity function among lexical nodes,namely words.
Moreover, SPTK only requires thissimilarity to be a valid kernel itself.
This means thatsuch lexical information can be derived from lexicalresources or it can be automatically acquired by aWord Space.
The SPTK is applied to a specific treerepresentation that allowed to achieve state-of-the-1Notice that, since the intersection |A ?
B|?
tends to be toostrict, we approximate it from the union cardinality estimation|A|?
+ |B|?
?
|A ?B|?.art results on several complex semantic tasks, suchas Question Classification (Croce et al 2011) orVerb Classification (Croce et al 2012b): each sen-tence is represented through the Lexical CenteredTree (LCT), as shown in Figure 1 for the sentence?Drug lord captured by Marines in Mexico?.
It is de-rived from the dependency parse tree: nodes reflectlexemes and edges encode their syntactic dependen-cies; then, we add to each lexical node two leftmostchildren, encoding the grammatical function and thePOS-Tag respectively.Combining STSs with SV Regression The similar-ity functions described above provide scores captur-ing different linguistic aspects and an effective wayto combine such information is made available bySupport Vector (SV) regression, described in (Smolaand Scho?lkopf, 2004).
The idea is to learn a higherlevel model by weighting scores according to spe-cific needs implicit in training data.
Given similar-ity scores ~xi for the i-th sentence pair, the regressorlearns a function yi = f(~xi), where yi is the scoreprovided by human annotators.
Moreover, since thecombination of kernel is still a kernel, we can ap-ply polynomial and RBF kernels (Shawe-Taylor andCristianini, 2004) to the regressor.2.2 Semantic constraints for the Typed STSTyped STS insists on records, i.e.
sequence of typedtextual fields, rather than on individual sentences.Our aim is to model the typed task with the samespirit as the core one, through a combination ofdifferent linguistic evidences, which are modeledthrough independent kernels.
The overall similaritymodel described in 2.1 has been thus applied also tothe typed task according to two main model changes:?
Semantic Modeling.
Although SV regressionis still applied to model one similarity type,each type depends on a subset of the multipleevidences originating from individual fields:one similarity type acts as a filter on the set offields, on which kernels will be then applied.?
Learning Constraints.
The selected fields pro-vide different evidences to the regression steps.Correspondingly, each similarity type corre-sponds to specific kernels and features for itsfields.
These constraints are applied by select-ing features and kernels for each field.61dcTitle dcSubject dcDescription dcCreator dcDate dcSourceauthor - - PER ?
- -people inv.
PER PER PER - - -time DATE DATE DATE - ?
-location LOC LOC LOC - - -event N , V , N ?
V N , V , N ?
V N , V , N ?
V - - -subject N , V , J , N ?
J ?
V N , V , J , N ?
J ?
V - - - -description - - N , V , J , N ?
J ?
V - - -general + + + ?
?
?Table 1: Filtering Schema adopted for the Typed STS task.Notice how some kernels loose significance in thetyped STS task.
Syntactic information is no usefulso that no tree kernel and compositional kernel isapplied here.
Most of the fields are non-sentential2.Moreover, not all morpho-syntactic information areextracted as feature from some fields.
Filters usu-ally specify some syntactic categories or Named En-tities (NEs): they are textual mentions to specificreal-world categories, such as of PERSONS (PER),LOCATIONS (LOC) or DATES.
They are detectedin a field and made available as feature to the cor-responding kernel: this introduces a bias on typedmeasures and emphasizes specific semantic aspects(e.g.
places LOC or persons PER, in location or au-thor measures, respectively).
For example, in thesentence ?The chemist R.S.
Hudson began manufac-turing soap in the back of his small shop in WestBomich in 1837?, when POS tag filters are applied,only verbs (V), nouns (N) or adjectives (J) can beselected as features.
This allows to focus on spe-cific actions, e.g.
the verb ?manufacture?, entities,e.g.
nouns ?soap?
and ?shop?, or some properties,e.g.
the adjective ?small?.
When Named Entity cat-egories are used, a mention to a person like ?R.S.Hudson?
or to a location, e.g.
?West Bomich?, ordate, e.g.
?1837?, can be useful to model the theperson involved, the location or time similarity mea-sures, respectively.The Semantic Modeling and the Learning Con-straints system adopted to model the Typed STStask are defined in Table 1.
There rows are thedifferent target similarities, while columns indicatedocument fields, such as dcTitle, dcSubject,dcDescription, dcCreator, dcDate and2The dcDescription is also made of multiple sen-tences and it reduces the applicability of SPTK and SSC: parsetrees have no clear alignment.dcSource, as described in the *SEM 2013 sharedtask description.
Each entry in the Table representsthe feature set for that fields, i.e.
POS tags (i.e.
V ,N , J) or Named Entity classes.
The ???
symbolcorresponds to all features, i.e.
no restriction isapplied to any POS tag or NE class.
Finally, thegeneral similarity function makes use of every NEclass and POS tags adopted for that field in anymeasure, as expressed by the special notation +, i.e.
?all of the above features?.Every feature set denoted in the Table 1 sup-ports the application of a lexical kernel, such asthe LO described in Section 2.1.
When differentPOS tags are requested (such as N and V ) mul-tiple feature sets and kernels are made available.The ?-?
symbol means that the source field is fullyneglected from the SV regression.
As an exam-ple, the SV regressor for the location similarityhas been acquired considering the fields dcTitle,dcSubject, dcDescription.
Only features usedfor the kernel correspond to LOCATIONs (LOC).
Foreach of the three feature, the LO and SUM simi-larity function has been applied, giving rise to aninput 6-dimensional feature space for the regressor.Differently, in the subject similarity, nouns, adjec-tives and verbs are the only features adopted fromthe fields dcSubject, dcTitle, so that 8 featuresets are used to model these fields, giving rise to a16-dimensional feature space.3 Results and discussionThis section describes results obtained in the *SEM2013 shared task.
The experimental setup of differ-ent similarity functions is described in Section 3.1.Results obtained over the Core STS task and TypedSTS task are described in Section 3.2 and 3.3.623.1 Experimental setupIn all experiments, sentences are processed with theStanford CoreNLP3 system, for Part-of-Speech tag-ging, lemmatization, named entity recognition4 anddependency parsing.In order to estimate the basic lexical similarityfunction employed in the SUM, SSC and SPTKoperators, a co-occurrence Word Space is acquiredthrough the distributional analysis of the UkWaCcorpus (Baroni et al 2009), a Web document col-lection made of about 2 billion tokens.
The samesetting of (Croce et al 2012a) has been adoptedfor the space acquisition.
The same setup describedin (Croce et al 2012c) is applied to estimate theSSC function.
The similarity between pairs of syn-tactically restricted word compound is evaluatedthrough a Symmetric model: it selects the best 200dimensions of the space, selected by maximizing thecomponent-wise product of each compound as in(Annesi et al 2012), and combines the similarityscores measured in each couple subspace with theproduct function.
The similarity score in each sub-space is obtained by summing the cosine similarityof the corresponding projected words.
The ?soft car-dinality?
is estimated with the parameter p = 2.The estimation of the semantically Smoothed Par-tial Tree Kernel (SPTK) is made available by an ex-tended version of SVM-LightTK software5 (Mos-chitti, 2006) implementing the smooth matchingbetween tree nodes.
Similarity between lexicalnodes is estimated as the cosine similarity in theco-occurrence Word Space described above, as in(Croce et al 2011).
Finally, SVM-LightTK is em-ployed for the SV regression learning to combinespecific similarity functions.3.2 Results over the Core STSIn the Core STS task, the resulting text similarityscore is measured by the regressor: each sentencepair from all datasets is modeled according to a 13dimensional feature space derived from the differentfunctions introduced in Section 2.1, as follows.The first 5 dimensions are derived by applying3http://nlp.stanford.edu/software/corenlp.shtml4The TIME and DURATION classes are collapsed withDATE, while the PERSON and LOCATION classes are consid-ered without any modification.5http://disi.unitn.it/moschitti/Tree-Kernel.htmRun1 Run2 Run3 Run?1headlines .635 (50) .651 (39) .603 (58) .671 (30)OnWN .574 (33) .561 (36) .549 (40) .637 (25)FNWN .352 (35) .358 (32) .327 (44) .459 (07)SMT .328 (39) .310 (49) .319 (44) .348 (21)Mean .494 (37) .490 (42) .472 (52) .537 (19)Table 2: Results over the Core STS taskthe LO operator over lemmatized words in the noun,verb, adjective and adverb POS categories: 4 ker-nels look at individual categories, while a fifth ker-nel insists on the union of all POS.
A second set of5 dimensions is derived by the same application ofthe SUM operator to the same syntactic selection offeatures.
The SPTK is then applied to estimate thesimilarity between the LCT structures derived fromthe dependency parse trees of sentences.
Then, theSPTK is applied to derive an additional score with-out considering any specific similarity function be-tween lexical nodes; in this setting, the SPTK can beconsidered as a traditional Partial Tree Kernel (Mos-chitti, 2006), in order to capture a more strict syn-tactical similarity between texts.
The last score isgenerated by applying the SSC operator.We participated in the *SEM challenge with threedifferent runs.
The main difference between eachrun is the dataset employed in the training phaseand the employed kernel within the regressor.
With-out any specific information about the test datasets,a strategy to prevent the regressor to over-fit train-ing material has been applied.
We decided to usea training dataset that achieved the best results overdatasets radically different from the training materialin the STS challenge of Semeval 2012.
In particular,for the FNWN and OnWN datasets, we arbitrarilyselected the training material achieving best resultsover the 2012 surprise.OnWN; for the headlines andSMT datasets we maximized performance trainingover surprise.SMTnews.
In Run1 the SVM regres-sor is trained using dataset combinations providingbest results according to the above criteria: MSR-par, MSRvid, SMTeuroparl and surprise.OnWN areemployed against FNWN and OnWN; MSRpar,SMTeuroparl and surprise.SMTnews are employedagainst headline and SMT.
A linear kernel is ap-plied when training the regressor.
In Run2, differ-ently from the previous one, the SVM regressor is63rank general author people inv.
time location event subject description meanRun1 1 .7981 .8158 .6922 .7471 .7723 .6835 .7875 .7996 .7620Run2 2 .7564 .8076 .6758 .7090 .7351 .6623 .7520 .7745 .7341Table 3: Results over the Typed STS tasktrained using all examples from the training datasets.A linear kernel is applied when training the regres-sor.
Finally, in Run3 the same training dataset selec-tion schema of Run1 is applied and a gaussian kernelis employed in the regressor.Table 2 reports the general outcome for the UN-ITOR systems in term of Pearson Correlation.
Thebest system, based on the linear kernel, ranks aroundthe 35th position (out of 90 systems), that reflectsthe mean rank of all the systems in the ranking ofthe different datasets.
The gaussian kernel, em-ployed for the Run3 does not provide any contri-bution, as it ranks 50th.
We think that the mainreason of these results is due to the intrinsic dif-ferences between training and testing datasets thathave been heuristically coupled.
This is first mo-tivated by lower rank achieved by Run2.
More-over, it is in line with the experimental findings of(Croce et al 2012a), where a performance drop isshown when the regressor is trained over data thatis not constrained over the corresponding source.In Run?1 we thus optimized the system by manu-ally selecting the training material that does providesbest performance on the test dataset: MSRvid, SM-Teuroparl and surprise.OnWN are employed againstOnWN; surprise.OnWN against FNWN, SMTeu-roparl against headlines; SMTeuroparl and sur-prise.SMTnews against SMT.
A linear kernel withinthe regressor allow to reach the 19th position, evenreducing the complexity of the representation to afive dimensional feature space: LO and SUM with-out any specific filter, SPTK, PTK and SSC.3.3 Results over the Typed STSSV regression has been also applied to the TypedSTS task through seven type-specific regressors plusa general one.
Each SV regressor insists on the LOand SUM kernel as applied to the features in Table1.
Notice that it was mainly due to the lack of richsyntactic structures in almost all fields.As described in Section 2.2, a specific modelingstrategy has been applied to derive the feature spaceof each target similarity.
For example, the regres-sor associated with the event similarity score is fedwith 18 scores.
Each of the 3 fields, , i.e.
dcTitle,dcSubject and dcDescription, provides the 2kernels (LO and SUM) with 3 feature sets (i.e.
N ,V and N ?
V ).
In particular, the general simi-larity function considers all extracted features foreach field, giving rise to a space of 51 dimensions.We participated in the task with two different runs,whose main difference is the adopted kernel withinthe SV regressor.
In Run1, a linear kernel is used,while in Run2 a RBF kernel is applied.Table 3 reports the general outcome for the UN-ITOR system.
The adopted semantic modeling, aswell as the selection of the proper information, e.g.the proper named entity, allows the system to rankin the 1st and 2nd positions (out of 18 systems).
Theproposed selection schema in Table 1 is very effec-tive, as confirmed by the results for almost all typedsimilarity scores.
Again, the RBF kernel does notimprove result over the linear kernel.
The impactof the proposed approach can be noticed for veryspecific scores, such as time and location, especiallyfor text pairs where structured information is absent,such as in the dcDate field.
Moreover, the regres-sor is not affected by the differences between train-ing and test dataset as for the previous Core STStask.
A deep result analysis showed that some simi-larity scores are not correctly estimated within pairsshowing partial similarities.
For example, the eventsor actions typed similarity is overestimated for thetexts pairs ?The Octagon and Pavilions, PavilionGarden, Buxton, c 1875?
and ?The Beatles, The Oc-tagon, Pavillion Gardens, St John?s Road, Buxton,1963?
because they mention the same location (i.e.
?Pavillion Gardens?
).Acknowledgements This work has been partiallysupported by the Regione Lazio under the projectPROGRESS-IT (FILAS-CR-2011-1089) and theItalian Ministry of Industry within the ?Industria2015?
Framework, project DIVINO (MI01 00234).64ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: A piloton semantic textual similarity.
In *SEM 2012, pages385?393, Montre?al, Canada, 7-8 June.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*sem 2013 sharedtask: Semantic textual similarity, including a pilot ontyped-similarity.
In *SEM 2013: The Second JointConference on Lexical and Computational Semantics.Association for Computational Linguistics.Paolo Annesi, Valerio Storch, and Roberto Basili.
2012.Space projections as distributional models for seman-tic composition.
In CICLing (1), Lecture Notes inComputer Science, pages 323?335.
Springer.Carmen Banea, Samer Hassan, Michael Mohler, andRada Mihalcea.
2012.
Unt: A supervised synergisticapproach to semantic text similarity.
In *SEM 2012,pages 635?642, Montre?al, Canada, 7-8 June.
Associa-tion for Computational Linguistics.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Danilo Croce and Daniele Previtali.
2010.
Manifoldlearning for the semi-supervised induction of framenetpredicates: An empirical investigation.
In Proceed-ings of the GEMS 2010 Workshop, pages 7?16, Upp-sala, Sweden.Danilo Croce, Alessandro Moschitti, and Roberto Basili.2011.
Structured lexical similarity via convolutionkernels on dependency trees.
In Proceedings ofEMNLP, Edinburgh, Scotland, UK.Danilo Croce, Paolo Annesi, Valerio Storch, and RobertoBasili.
2012a.
Unitor: Combining semantic text simi-larity functions through sv regression.
In *SEM 2012,pages 597?602, Montre?al, Canada, 7-8 June.Danilo Croce, Alessandro Moschitti, Roberto Basili, andMartha Palmer.
2012b.
Verb classification using dis-tributional similarity in syntactic and semantic struc-tures.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 263?272, Jeju Island, Ko-rea, July.Danilo Croce, Valerio Storch, Paolo Annesi, and RobertoBasili.
2012c.
Distributional compositional seman-tics and text similarity.
2012 IEEE Sixth InternationalConference on Semantic Computing, 0:242?249.David Haussler.
1999.
Convolution kernels on discretestructures.
Technical report, University of Santa Cruz.Aminul Islam and Diana Inkpen.
2008.
Semantictext similarity using corpus-based word similarity andstring similarity.
ACM Trans.
Knowl.
Discov.
Data,2:10:1?10:25, July.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2012.
Soft cardinality: A parameterized sim-ilarity function for text comparison.
In *SEM 2012,pages 449?453, Montre?al, Canada, 7-8 June.
Associa-tion for Computational Linguistics.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In In AAAI06.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive Science,34(8):1388?1429.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.
InECML, pages 318?329, Berlin, Germany, September.Machine Learning: ECML 2006, 17th European Con-ference on Machine Learning, Proceedings.Mehran Sahami and Timothy D. Heilman.
2006.
A web-based kernel function for measuring the similarity ofshort text snippets.
In Proceedings of the 15th inter-national conference on World Wide Web, WWW ?06,pages 377?386, New York, NY, USA.
ACM.Magnus Sahlgren.
2006.
The Word-Space Model.
Ph.D.thesis, Stockholm University.John Shawe-Taylor and Nello Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge UniversityPress, New York, NY, USA.Alex J. Smola and Bernhard Scho?lkopf.
2004.
A tutorialon support vector regression.
Statistics and Comput-ing, 14(3):199?222, August.Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,and Bojana Dalbelo Bas?ic?.
2012.
Takelab: Systemsfor measuring semantic text similarity.
In *SEM 2012,pages 441?448, Montre?al, Canada, 7-8 June.65
