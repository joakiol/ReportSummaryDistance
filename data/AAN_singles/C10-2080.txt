Coling 2010: Poster Volume, pages 701?709,Beijing, August 2010Reexamination on Potential for Personalization in Web SearchDaren Li1  Muyun Yang1  Haoliang Qi2  Sheng Li1  Tiejun Zhao11School of Computer ScienceHarbin Institute of Technology{drli|ymy|tjzhao}@mtlab.hit.edu.cn, lisheng@hit.edu.cn2School of Computer ScienceHeilongjiang Institute of Technologyhaoliang.qi@gmail.comAbstractVarious strategies have been proposedto enhance web search through utiliz-ing individual user information.
How-ever, considering the well acknowl-edged recurring queries and repetitiveclicks among users, it is still an openissue whether using individual user in-formation is a proper direction of ef-forts in improving the web search.
Inthis paper, we first quantitatively dem-onstrate that individual user informa-tion is more beneficial than commonuser information.
Then we statisticallycompare the benefit of individual andcommon user information throughKappa statistic.
Finally, we calculatepotential for personalization to presentan overview of what queries can bene-fit more from individual user informa-tion.
All these analyses are conductedon both English AOL log and ChineseSogou log, and a bilingual perspectivestatistics consistently confirms ourfindings.1 IntroductionMost of traditional search engines are designedto return identical result to the same queryeven for different users.
However, it has beenfound that majority of queries are quite ambi-guous (Cronen-Townsend et al, 2002) as wellas too short (Silverstein et al, 1999) to de-scribe the exact informational needs of users.Different users may have completely differentinformation needs under the same query (Jan-sen et al, 2000).
For example, when users is-sue a query ?Java?
to a search engine, theirneeds can be something ranging from a pro-gramming language to a kind of coffee.In order to solve this problem, personalizedsearch is proposed, which is a typical strategyof utilizing individual user information.
Pitkowet al (2002) describe personalized search asthe contextual computing approach which fo-cuses on understanding the information con-sumption patterns of each user, the variousinformation foraging strategies and applica-tions they employ, and the nature of the infor-mation itself.
After that, personalized searchhas gradually developed into one of the hottopics in information retrieval.
As for variouspersonalization models proposed recently, Douet al (2007), however, reveal that they actuallyharms the results for certain queries while im-proving others.
This result based on a large-scale experiment challenges not only the cur-rent personalization methods but also the mo-tivation to improve web search by the persona-lized strategies.In addition, the studies on query logs rec-orded by search engines consistently report theprevailing repeated query submissions by largenumber of users (Silverstein et al, 1999; Spinket al, 2001).
It is reported that the 25 most fre-quent queries from the AltaVista cover 1.5%of the total query submissions, despite beingonly 0.00000016% of unique queries (Silvers-tein et al, 1999).
As a result, the previous us-ers?
activities may serve as valuable informa-tion, and technologies focusing on common701user information, such as collaborative filter-ing (or recommendation) may be a better reso-lution to web search.
Therefore, the justifica-tion of utilizing individual user informationdeserves further discussion.To address this issue, this paper conducts abilingual perspective of survey on two large-scale query logs publically available: the AOLin English and the Sogou1 in Chinese.
First wequantitatively investigate the evidences forexploiting common user information and indi-vidual user information in these two logs.
Af-ter that we introduce Kappa statistic to meas-ure the consistency of users?
implicit relevancejudgment inferred from clicks.
It is tentativelyrevealed that using individual user informationis what requires web search to face with aftercommon user information is well exploited.Finally, we study the distribution of potentialfor personalization over the whole logs to gen-erally disclose what kind of query deserves forindividual user information.The remainder of this paper is structured asfollows.
Section 2 introduces previous me-thods employing individual and common userinformation.
In Section 3, we quantitativelycompare the evidences for exploiting commonuser information and individual user informa-tion.
In Section 4, we introduce Kappa statisticto measure the consistency of users?
clicks onthe same query and try to statistically presentthe development direction of current websearch.
Section 5 figures out utilizing individu-al user information as a research issue afterwell exploiting common user information.
Sec-tion 6 presents the potential for personalizationcurve, trying to outline which kind of queriesbenefit the most from individual user informa-tion.
Conclusions and future work are detailedin Section 7.2 Related WorkWith the rapid expansion of World Wide Web,it becomes more and more difficult to find re-levant information through one-size-fits-allinformation retrieval service provided by clas-sical search engines.
Two kinds of user infor-mation are mainly used to enhance search en-1 A famous Chinese search engine with a large number ofChinese web search users.gines: common user information and individu-al user information.
We separately review theprevious works focusing on using these twokinds of information.Among various attempts to improve the per-formance of search engine, collaborative websearch is the one to take advantage of the repe-tition of users?
behaviors, which we call com-mon user information.
Since there is no unifieddefinition on collaborative web search, in thispaper, we believe that the collaborative websearch assumes that community search activi-ties can provide valuable search knowledge,and sharing this knowledge facilitates improv-ing traditional search engine results (Smyth,2007).
An important technique of collaborativeweb search is Collaborative Filtering (CF, alsoknown as collaborative recommendation), inwhich, items are recommended to an activeuser based on historical co-occurrence databetween users and items (Herlocker et al,1999).
A number of researchers have exploredalgorithms for collaborative filtering and thealgorithms can be categorized into two classes:memory-based CF and model-based CF.Memory-based CF methods apply a nearest-neighbor-like scheme to predict a user?s rat-ings based on the ratings given by like-mindedusers (Yu et al, 2004).
The model-based ap-proaches expand memory-based CF to build adescriptive model of group-based user prefe-rences and use the model to predict the ratings.Examples of model-based approaches includeclustering models (Kohrs et al, 1999) and as-pect models (J.
Canny, 2002).The other way to improve web search is per-sonalized web search, focusing on learning theindividual preferences instead of others?
beha-viors, which is called individual user informa-tion.
Early works learn user profiles from theexplicit description of users to filter search re-sults (Chirita et al, 2005).
However, most ofusers are not willing to provide explicit feed-back on search results and describe their inter-ests (Carroll et al, 1987).
Therefore, recentresearches on the personalized search focus onmodeling user preference from different typesof implicit data, such as query history (Sperettaet al, 2005), browsing history (Sugiyama et al,2004), clickthrough data (Sun et al, 2005),immediate search context (Shen et al, 2005)and other personal information (Teevan et al,7022005).
So far, there is still no proper compari-son between the two solutions.
It is still anopen question which kind of information ismore effective to build the web search model.Considering the difficulty in collecting pri-vate information, using individual user infor-mation seems less promising as the cost-effective solution to web search.
To addressthis issue, some researches about the value ofpersonalization have been conducted.
Teevanet al (2007) have done a ground breaking jobto quantify the benefit for the search engines ifsearch results were tailored to satisfy each user.The possible improvement by the personalizedsearch, named potential for personalization, ismeasured by a gap between the relevance ofindividualized rankings and group rankingbased on NDCG.
However, it is less touchedfor the position of individual user informationin contrast with common user information inlarge scale query log and how to balance theusage of common and individual informationin information retrieval model.This paper tentatively examines individualuser information against common user infor-mation on two large-scale search engine logsin following aspects: the evidence from clickson the same query, Kappa statistic for thewhole queries, and overall distribution of que-ries in terms of number of submissions andKappa value.
The bilingual statistics consis-tently reveals the tendency of using individualuser information as an equally important issueas (if not more than) using common user in-formation) issue for researches on web search.3 Quantitative Evidences for UsingCommon or Individual User Infor-mationTo quantitatively investigate the value ofcommon user information and individual userinformation in query log, we discriminate theevidence for using the two different types ofuser information as follows:(1) Evidence for using common user infor-mation: if there were multiple users who haveexactly the same click sets on one query, wesuppose those clicks sets, together with thequery, as the evidence for exploiting commonuser information.
It is clear that such queriesare able to be better responded with other?ssearch results.
Note that common user infor-mation is hard to be clearly defined, in order tosimplify the quantitative statistics we give astrict definition.
Further analysis will be shownin following sections.
(2) Evidence for using individual user in-formation: if a user?s click set on a query wasnot the same as any other?s, for that query, thesearch intent of the user who issue that querycan be better inferred from his/her individualinformation than common user information.We suppose this kind of clicks, together withthe related queries, as the evidence for exploit-ing individual user information.Since users may have different search in-tents when they issue the same query, a querycan be an evidence for using both common andindividual user information.
In our statistics, ifa query has both duplicate click sets andunique click set, the query is not only countedby the first category but also the second cate-gory.The statistics of the two categories are con-ducted in the query log of both English andChinese search engines.
We use a subset ofAOL Query Log from March 1, 2006 to May31, 2006 and Sogou Query Log from March 1,2007 to March 31, 2007.
The basic statistics ofAOL and Sogou log are shown in Table 1.
No-tice that the queries in raw AOL and Sogou logwithout clicks are removed in this study.Item AOL Sogou#days 92 31#users 6,614,960 7,488,754#queries 7,840,348 8,019,229#unique queries 4,811,649 4,580,836#clicks 12,984,610 17,607,808Table 1: Basic statistics of AOL & Sogou logTable 2 summarizes the statistics of differ-ent evidence categories over AOL and Sogoulog.
Note that click set refers to the set ofclicks related to a query submission instead ofa unique query.
As for evidence for usingcommon and individual user information, thereis no clear distinction in terms of number ofrecords, number of users in two logs.
However,in terms of unique query and distinct click set,one can?t fail to find that evidence for usingindividual user information clearly exceeds703LogThe Condition NumberRepeated queries Click Records User Unique QueryDistinctClick SetAOL3,745,088(47.77% of totalquery submissions)Same 2,438,284 277,416 382,267 461,460Different 2,563,245 343,846 542,593 1,349,892Sogou4,252,167(53.02% of totalquery submissions)Same 2,469,363 1,380,951 228,315 358,346Different 5,481,832 1,545,817 752,047 2,171,872Table 2: Different click behaviors on repeated queriesthat for using common user information, espe-cially in Sogou log.
Therefore, though makinguse of common and individual user informa-tion can address equally well for half users andhalf visits to the search engine, the fact thatmuch more unique queries and click sets ac-tually claims the significance of needing indi-vidual user information to personalize webresults.
And methods exploiting individual us-er information provide a much more challeng-ing task in terms of problem space, though onemay argue utilizing common user informationis much easier to attack.4 Kappa Statistics for Individual andCommon user informationSection 3 has shown the evidence for usingindividual user information is prevailing thancommon user information in quantity for theunique queries in search engines.
However,these counts deserve a further statistical cha-racterization.
In this section, we introduceKappa statistic to depict the overall consisten-cy of users?
clicks in query logs.4.1 KappaKappa is a statistical measure introduced toaccess the agreement among different raters.There are two types of Kappa.
One is Cohen?sKappa (Cohen, 1960), which measures onlythe degree of agreement between two raters.The other is Fleiss?s Kappa (Fleiss, 1971),which generalizes Cohen?s Kappa to measureagreement among more than two raters, de-noted as:eePPP?
?=1?where, P is the probability that a randomlyselected rater agree with another on a random-ly selected subject.
eP is the expected probabil-ity of agreement if all raters made ratings bychance.
If we use Kappa to measure the consis-tency of relevance judgment by different raters,P can be interpreted as the probability thattwo random selected raters consistently rate arandom selected search result as relevant ornon-relevant one.
Similarly, eP can also beconstrued as the expected probability of iden-tical relevance judgment rated by different ra-ters all by chance.Teevan et al (2008) used Fleiss?s Kappa tomeasure the inter-rater reliability of differentraters?
explicit relevance judgments.
We ex-pand their work and employ Fleiss?s Kappa tomeasure the consistency of implicit relevancejudgments by users on the same query2.
Hereclicks are treated as a proxy for relevance:documents clicked by a user are judged as re-levant and those not clicked as non-relevant(Teevan et al, 2008).
As we all know that theresult set of one query may change over time,so we select the longest time span to calculateKappa value of a query, during which the re-sult set of it preserves unchanged.
From Kappavalue of each query, we can statistically interp-ret to which extent users share consistent intenton the same query according to Table 3 (Lan-dis and Koch, 1977).
Though the interpretationin Table 3 is not accepted with no doubt, it cangive us an intuition about what extent ofagreement consistency is.
In other words,Kappa is a measure with statistical sense.Meanwhile, Kappa values of queries with2 There may be more than two users who submitted thesame query.704(a).
AOL                                                                  (b).
SogouFigure 1: Number of unique queries and query submissions as a function of Kappa value.?
Interpretation< 0 No agreement0.0 ?
0.20 Slight agreement0.21 ?
0.40 Fair agreement0.41 ?
0.60 Moderate agreement0.61 ?
0.80 Substantial agreement0.81 ?
1.00 Almost perfect agreementTable 3:  Kappa Interpretationvarious sizes of click sets are also comparable.That is also the reason we choose Kappa tomeasure consistency.4.2 Distribution of KappaAs introduced in Section 2, common user in-formation is supposed to be the repetition ofusers?
behaviors.
We consider that the amountof repetition of users?
clicks on one query isquantified by the consistency of its clicks.
Tostatistically present the scale of repetition incurrent query log, we try to give an overviewof consistency level of two commercial querylogs.Figure 1 plots distribution of Kappa value ofthe two logs in the coordinate with logarithmicY-axis.
About 34.5% unique queries (44.0%query submissions) in AOL log and only13.9% unique queries (15.2% query submis-sions) in Sogou log have high Kappa valuesabove 0.6.
According to Table 3, click sets ofthese queries can be regarded as somewhatconsistent.
These queries can be roughly re-solved by using common user information.
Onthe other hand, for the rest of queries whichconstitute majority of the logs, users?
click setsare rather diversified, which are hard to be sa-tisfied by returning the same result list to them.As a whole, the queries in both AOL and So-gou can be characterized as less consistently inthe clicks according to Kappa value, which is astatistical support for exploiting individual userinformation.5 Individual or Common user infor-mation: A Tendency ViewThe above analyses quantitative analyses haveshown that the repetition of search is not thestatistically dominant factor, with the impres-sion that employing individual user informa-tion is equally, if not more, important thancommon user information.
This section tries tofurther reveal this issue so as to balance theposition of individual user information andcommon user information from a researchpoint.Intuitively, a query can be characterized bythe number of people issuing it, i.e.
query fre-quency if we remove the resubmissions of onequery by the same people.
We try to depict theabove mentioned query submissions and Kap-pa values as a function of number of peoplewho issue the queries in Figure 2.
In Figure 2,different numbers of users who issue the samequery are shown on the x-axis, and the y-axisrepresents the number of different entities (leftscale) and the average Kappa value (right scale)of the queries.
We find that the number of que-ries becomes very small when the number ofusers in a group grows over 10, so we set avariant step length for them: with the lengthstep of the group size falling between 2 and 10set as 1, between 11 and 100 as 10, between101 and 1000 as 100 and above 1000 as 1000.705(a).
AOL                                                                      (b).
SogouFigure 2: Average Kappa value of queries as a function of number of people in a group who issuethe same query (line) and the number of submissions of the queries issued by the same size ofgroup (dark columns).According to Figure 2(a), Kappa values ofthe queries in AOL log with more than 20 us-ers are above 0.6, which indicates rather con-sistent clicks for them, accounting for about29.4% of all query submissions.
While forthose queries visited by less than 20 users, theKappa value declines gradually from 0.6 withthe drop of users.
For these queries occupyingmajority of query submissions, exploiting in-dividual user information is supposed to be abetter solution since the clicks on them are ra-ther individualized.According to Figure 2(b), though Kappavalues of queries increase similarly withpeople submitting them in AOL, the overallconsistency of the queries in Sogou log ismuch lower: with a Kappa value below 0.6even for the queries visited by a large numberof users.
This fact indicates that Chinese usersmay be less consistent in their search intents,or partially reflects that the Chinese as a non-inflection language has more ambiguity, whichcan also be implied from Table 2.
Therefore,individual user information may be more ef-fective than common user information in So-gou log.Summarized from Figure 2, it is sensiblethat common user information is appropriatefor the queries in the right-most of X-axis.With most number of visiting people, suchqueries bear rather consistent clicks thoughcovering only a small proportion of the distinctquery set.
Moving from the right to the left, wecan find the majority of queries yield a lessKappa value, for which the individualizedclicks require individual user information tomeet the needs of each user.
In this sense, howto exploit individual user information is pre-destined as the next issue of information re-trieval if common user information was to bewell utilized.6 Queries for PersonalizationSince using individual user information is anon-negligible issue in IR research, a subse-quent issue is what queries can benefit in whatextent from individual user information.
In thissection, we try to give an overview for thisissue via a measure named potential for perso-nalization.6.1 Potential for PersonalizationPotential for personalization proposed by Tee-van et al (2007) is used to measure the norma-lized Discounted Cumulative Gain (NDCG)improvement between the best ranking of theresults to a group and individuals.
NDCG is awell-known measure of the quality of a searchresult (J?rvelin and Kek?l?inen, 2000).The best ranking of the results to a group isthe ranking with highest NDCG based on re-levance judgments of the users in the group.For the queries with explicit judgments, thebest ranking can be generated as follows: re-sults that all raters thought were relevant areranked first, followed by those that mostpeople thought were relevant but a few peoplethought were irrelevant, until the  results mostpeople though were irrelevant.
In other word,706(a)  AOL                                                                    (b)   SogouFigure 3: Number of unique queries and query submissions as a function of potential forpersonalization(a)  AOL                                                                       (b) SogouFigure 4: The average NDCG of group best ranking as a function of number of people in group(solid line), combining with the distribution of  the number of unique queries issued by the samesize of group (dark columns)the best ranking always tries to put the resultsthat have the highest collective gain first to getthe highest NDCG.The previous work has shown that the im-plicit click-based potential for personalizationis strongly related to variation in explicitjudgments (J. Teevan et al, 2008).
In this pa-per, we continue using click-based potentialfor personalization to measure the variation.Assuming the clicked results as relevant, wecan calculate the potential for personalizationof each query over the web search query log topresent what kind of query can benefit morefrom personalization.6.2 Potential for Personalization Distribu-tion over Query LogsTeevan et al (2007) have depicted a potentialfor personalization curve based on explicitjudgment to characterize the benefit that couldbe obtained by personalizing search results foreach user.
We continue using potential for per-sonalization based on click-through to roughlyreveal what kind of query can benefit morefrom personalization.First we investigate the number of uniquequeries with different potential for personaliza-tion, which is shown in Figure 3.
We find thatthere are about 53.9% unique queries in AOLlog and 32.4% unique queries in Sogou log,whose potential for personalization is 0.
Forthese queries, current web search is able to re-turn perfect results to all users.
However, forthe rest of queries, even the best group rankingof results can?t satisfy everyone who issues thequery.
So these queries should be better servedby individual user information, covering70746.1% unique queries in AOL and 67.6% inSogou.Then, in order to further interpret what kindof query individual user information is neededmost, we further relate potential for personali-zation to the number of users who submit thequeries over AOL and Sogou query log asshown in Figure 4.
For clarity?s sake, we alsoset the same step length as in Figure 2.According to Figure 4, the curve of potentialfor personalization is approximately U-shapedin both AOL log and Sogou Log.
As the num-ber of users in one group increases, perfor-mance of the best non-personalized rankingsfirst declines, then flattens out and finallypromotes3.
Note that the left part of the curveis very similar to what Teevan et al (2007)showed in their work.Again in Figure 4, the queries which havethe most potential for personalization are theones which are issued by more than 6 and lessthan 20 users in AOL log.
While in Sogou log,the queries issued by more than 6 and less than4000 users have the most potential for persona-lization.
Such different findings are probablycaused by the content of query.
There aremany recommended queries in the homepageof Sogou search engine, most of which are in-formational query and clicked by a large num-ber of users.
Even when the size of group whoissue the same query becomes very big, thequery still has a wide variation of users?
beha-viors.
So the consistency level of queries inSogou log is much lower than the queries inAOL log at the same size of group.7 Conclusion and Future WorkIn this paper, we try to justify the position ofindividual user information comparing withcommon user information.
It is shown that ex-ploiting individual user information is a non-trivial issue challenging the IR communitythrough the analysis of both English and Chi-nese large scale search logs.We first classify the repetitive queries into 2categories according to whether the corres-ponding clicks are unique among different us-ers.
We find that quantitatively the queries and3 Note that the different step length dims the actual U-shape in the figure.clicks deserving for individual user informa-tion is much bigger than those deserving forcommon user information.After that we use Kappa statistic to presentthat the overall consistency of query clicks re-coded in search logs is pretty low, which statis-tically reveals that the repetition is not the do-minant factor and individual user informationis more desired to enhance most queries in cur-rent query log.We also explore the distribution of Kappavalues over different numbers of users in thegroup who issue the same query, concludingthat how to utilize individual user informationto improve the performance of web search en-gine is the next research issue confronted bythe IR community when the repeated search ofusers are properly exploited.Finally, potential for personalization is cal-culated over the two query logs to present anoverview of what kind of queries that the op-timal group-based retrieval model fails, whichis supposed to benefit most from individualuser information.One possible enrichment to this work maycome from the employment of content analysisbased on text processing techniques.
The dif-ferent clicks, which are the basis of our exami-nation, may have similar or even exact contentin their web pages.
Though the manual checkfor a small scale sampling from the Sogou logyields less than 1% probability for such case,the content based examination will be definite-ly more convincing than simple click counts.In addition, the queries for the two types ofuser information are not examined for theircontents or the related information needs.
Con-tent analysis or linguistic view to these querieswould be more informative.
Both of these is-sues are to be addressed in our future work.AcknowledgementThis work is supported by the Key Project ofNatural Science Foundation of China (GrantNo.60736044), and National 863 Project(Grant No.2006AA010108).
The authors aregrateful for the anonymous reviewers for theirvaluable comments.708ReferencesCanny John.
2002.
Collaborative filtering with pri-vacy via factor analysis.
In Proceedings of SI-GIR?
02, pages 45-57.Carroll M. John and Mary B. Rosson.
1987.
Para-dox of the active user.
Interfacing thought: cog-nitive aspect of human-computer interaction,pages 80-111.Chirita A. Paul, Wofgang Nejdl, Raluca Paiu, andChristian Kohlschutter.
2005.
Using odp metada-ta to personalize search.
In Proceedings of SI-GIR ?05, pages 178-185.Cohen Jacob.
1960.
A coefficient of agreement fornominal scales.
Educational and PsychologicalMeasurement, 20: 37-46Dou Zhicheng, Ruihua Song, and Ju-Rong Wen.2007.
A Large-scale Evaluation and Analysis ofPersonalized Search Strategies.
In Proceedingsof WWW ?07, pages 581-590.Fleiss L. Joseph.
1971.
Measuring nominal scaleagreement among many raters.
PsychologicalBulletin, 76(5): 378-382.Herlocker L. Jonathan, Joseph A. Konstan, AlBorchers, and John Riedl.
1999.
An algorithmicframework for performing collaborative filtering.In Proceedings of SIGIR ?99, pages 230-237.Jansen J. Bernard, Amanda Spink, and Tefko Sara-cevic.
2000.
Real life, real users, and real needs:a study and analysis of user queries on the web.Information Processing and Management, pages207-227.J?rvelin Kalervo and Jaana Kek?l?inen.
2000.
IRevaluation methods for retrieving highly relevantdocuments.
In Proceedings of SIGIR ?00, pages41-48.Kohrs Arnd and Bernard Merialdo.
1999.
Cluster-ing for collaborative filtering applications.
InProceedings of CIMCA ?99, pages 199-204.Landis J. Richard and Gary.
G. Koch.
1977.
Themea-surement of observer agreement for cate-gorical data.
Biometrics 33: 159-174.Pitkow James, Hinrich Schutze, Todd Cass, RobCooley, Don Turnbull, Andy Edmonds, EytanAdar and Thomas Breuel.
2002.
Personalizedsearch.
ACM, 45(9):50-55.Shen Xuehua, Bin Tan and ChengXiang Zhai.
2005Implicit user modeling for personalized search.In Proceedings of CIKM ?05, pages 824-831.Silverstein Craig, Monika Henzinger, Hannes Ma-rais and Michael Moricz.
1999.
Analysis of avery large web search engine query log.
SIGIRForum, 33(1):6-12.Smyth Barry.
2007.
A Community-Based Approachto Personalizing Web Search.
IEEE Computer,40(8): 42-50.Speretta Mirco and Susan Gauch.
PersonalizedSearch based on user search histories.
2005.
InProceedings of WI ?05, pages 622-628.Spink Amanda, Dietmar Wolfram, Major Jansen,Tefko Saracevic.
2001.
Searching the web: Thepublic and their queries.
Journal of the AmericanSociety for Information Science and Technology,52(3), 226-234Sugiyama Kazunari, Kenji Hatano, and MasatoshiYoshikawa.
2004.
Adaptive web search based onuser profile constructed without any effort fromusers.
In Proceedings of WWW ?04, pages 675-684.Sun Jian-Tao, Hua-Jun Zeng, Huan Liu, YuchangLu and Zheng Chen.
2005.
CubeSVD: a novelapproach to personalized web search.
In Pro-ceedings of WWW?05, pages 382-390.Teevan Jaime, Susan T. Dumais, and Eric Horvitz.2005.
Personalizing search via automated analy-sis of interests and activities.
In Proceedings ofSIGIR ?05, pages 449-456.Teevan Jaime, Susan T. Dumais and Eric Horvitz.2007.
Characterizing the value of personalizingsearch.
In Proceedings of SIGIR ?07, pages 757-758.Teevan Jaime, Susan T. Dumais and Daniel J.Liebling.
2008.
To personalize or Not to Perso-nalize: Modeling Queries with Variation in UserIntent.
In Proceedings of SIGIR ?08, pages 163-170.Townsend Steve Cronen and W. Bruce Croft.
2002.Quantifying query ambiguity.
In Proceedings ofHLT ?02, pages 613-622.Yu Kai, Anton Schwaighofer, Volker Tresp, Xiao-wei Xu, Hans-Peter Kriegel.
2004.
ProbabilisticMemory-based Collaborative Filtering.
In IEEETransactions on Knowledge and Data Engineer-ing, pages 56-59.709
