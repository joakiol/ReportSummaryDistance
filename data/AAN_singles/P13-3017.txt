Proceedings of the ACL Student Research Workshop, pages 117?122,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAddressing Ambiguity in Unsupervised Part-of-Speech Induction withSubstitute VectorsVolkan CirikArtificial Intelligence LaboratoryKoc University, Istanbul, Turkeyvcirik@ku.edu.trAbstractWe study substitute vectors to solve thepart-of-speech ambiguity problem in anunsupervised setting.
Part-of-speech tag-ging is a crucial preliminary process inmany natural language processing applica-tions.
Because many words in natural lan-guages have more than one part-of-speechtag, resolving part-of-speech ambiguity isan important task.
We claim that part-of-speech ambiguity can be solved usingsubstitute vectors.
A substitute vector isconstructed with possible substitutes of atarget word.
This study is built on pre-vious work which has proven that wordsubstitutes are very fruitful for part-of-speech induction.
Experiments show thatour methodology works for words withhigh ambiguity.1 IntroductionLearning syntactic categories of words (i.e.
part-of-speech or POS tagging) is an important pre-processing step for many natural language pro-cessing applications because grammatical rulesare not functions of individual words, instead, theyare functions of word categories.
Unlike super-vised POS tagging systems, POS induction sys-tems make use of unsupervised methods.
Theycategorize the words without any help of annotateddata.POS induction is a popular topic and severalstudies (Christodoulopoulos et al 2010) havebeen performed.
Token based methods (Berg-Kirkpatrick and Klein, 2010; Goldwater and Grif-fiths, 2007) categorize word occurrences into syn-tactic groups.
Type based methods (Clark, 2003;Blunsom and Cohn, 2011) on the other hand, cat-egorize word types and yield the ambiguity prob-lem unlike the token based methods.Type based methods suffer from POS ambigu-ity because one POS tag is assigned to each wordtype.
However, occurrences of many words mayhave different POS tags.
Two examples below aredrawn from the dataset we worked on.
They il-lustrate a situation where two occurrences of the?offers?
have different POS tags.
In the first sen-tence ?offers?
is a noun, whereas, in the secondsentence it is a verb.
(1) ?Two rival bidders for ConnaughtBioSciences extended their offers to ac-quire the Toronto-based vaccine manu-facturer Friday.?
(2) ?The company currently offers aword-processing package for personalcomputers called Legend.
?In this study, we try to extend the state-of-the-art unsupervised POS tagger (Yatbaz et al 2012)by solving the ambiguity problem it suffers be-cause it has a type based approach.
The clusteringbased studies (Schu?tze, 1995) (Mintz, 2003) rep-resent the context of a word with a vector usingneighbour words.
Similarly, (Yatbaz et al 2012)proposes to use word context.
They claim that thesubstitutes of a word have similar syntactic cate-gories and they are determined by the context ofthe word.In addition, we suggest that the occurrenceswith different part-of-speech categories of a wordshould be seen in different contexts.
In otherwords, if we categorize the contexts of a word typewe can determine different POS tags of the word.We represent the context of a word by construct-ing substitute vectors using possible substitutes ofthe word as (Yatbaz et al 2012) suggests.Table 1 illustrates the substitute vector of the oc-currence of ?offers?
in (1).
There is a row for eachword in the vocabulary.
For instance, probabilityof occurring ?agreement?
in the position of ?of-fers?
is 80% in this context.
To resolve ambiguity117Probability Substitute Word0.80 agreement0.03 offer0.01 proposal0.01 bid0.01 attempt0.01 bids.
.. .. .Table 1: Substitute Vector for ?offers?
in abovesentence.of a target word, we separate occurrences of theword into different groups depending on the con-text information represented by substitute vectors.We conduct two experiments.
In the first ex-periment, for each word type we investigated, weseparate all occurences into two categories usingsubstitute vectors.
In the second one we guess thenumber of the categories we should separate foreach word type.
Both experiments achieve bet-ter than (Yatbaz et al 2012) for highly ambigu-ous words.
The level of ambiguity can be mea-sured with perplexity of word?s gold tag distribu-tion.
For instance,the gold tag perplexity of word?offers?
in the Penn Treebank Wall Street Journalcorpus we worked on equals to 1.966.
Accord-ingly, the number of different gold tags of ?of-fers?
is 2.
Whereas, perplexity of ?board?
equalsto 1.019.
Although the number of different tagsfor ?board?
is equal to 2, only a small fractionof the tags of board differs from each other.
Wecan conclude that ?offers?
is more ambiguous than?board?.In this paper we present a method to solve POSambiguity for a type based POS induction ap-proach.
For the rest of the paper, we explain ouralgorithm and the setup of our experiments.
Lastlywe present the results and a conclusion.2 AlgorithmWe claim that if we categorize contexts a wordtype occurs in, we can address ambiguity by sep-arating its occurrences before POS induction.
Inorder to do that, we represent contexts of wordoccurrences with substitute vectors.
A substi-tute vector is formed by the whole vocabulary ofwords and their corresponding probabilities of oc-curring in the position of the target word.
To cal-culate these probabilities, as described in (Yatbazet al 2012), a 4-gram language model is builtwith SRILM (Stolcke, 2002) on approximately126 million tokens of Wall Street Journal data(1987-1994) extracted from CSR-III Text (Graffet al 1995).We generate substitute vectors for all tokens inour dataset.
We want to cluster occurrences of ourtarget words using them.
In each substitute vector,there is a row for every word in the vocabulary.As a result, the dimension of substitute vectors isequal to 49,206.
Thus, in order not to suffer fromthe curse of dimensionality, we reduce dimensionsof substitute vectors.Before reducing the dimensions of these vec-tors, distance matrices are created using Jensendistance metric for each word type in step (a) ofFigure 1.
We should note that these matrices arecreated with substitute vectors of each word type,not with all of the substitute vectors.In step (b) of Figure 1, to reduce dimensionality,the ISOMAP algorithm (Tenenbaum et al 2000)is used.
The output vectors of the ISOMAP al-gorithm are in 64 dimensions.
We repeated ourexperiments for different numbers of dimensionsand the best results are achieved when vectors arein 64 dimensions.In step (c) of Figure 1, after creating vectorsin lower dimension, using a modified k-meansalgorithm (Arthur and Vassilvitskii, 2007) 64-dimensional vectors are clustered for each wordtype.
The number of clusters given as an input tok-means varies with experiments.
We induce num-ber of POS tags of a word type at this step.Previous work (Yatbaz et al 2012) demon-strates that clustering substitute vectors of all wordtypes alone has limited success in predicting part-of-speech tag of a word.
To make use of both wordidentity and context information of a given type,we use S-CODE co-occurrence modeling (Maronet al 2010) as (Yatbaz et al 2012) does.Given a pair of categorical variables, the S-CODE model represents each of their values on aunit sphere such that frequently co-occurring val-ues are located closely.
We construct the pairs tofeed S-CODE as follows.In step (d) of Figure 1, the first part of the pair isthe word identity concatenated with cluster ids wegot from the previous step.
The cluster ids separateword occurrences seen in different context groups.By doing that, we make sure that the occurrences118Figure 1: General Flow of The Algorithmof a same word can be separated on the unit sphereif they are seen in different context groups.The second part of the pair is a substitute word.For an instance of a target word, we sample a sub-stitute word according to the target word?s sub-stitute vector probabilities.
If occurrences of twodifferent or the same word types have the samesubstitutes, they should be seen in the similar con-texts.
As a result, words occurring in the simi-lar contexts will be close to each other on the unitsphere.
Furthermore, they will have the same POStags.
We should note that the co-occurrence inputfile contains all word types.In step (e) of Figure 1, on the output of the S-CODE sphere, the words occurring in the simi-lar contexts and having the same word-identity areclosely located.
Thus, we observe clusters on theunit sphere.
For instance, verb occurrences of ?of-fers?
are close to each other on the unit sphere.They are also close to other verbs.
Furthermore,119they are separated with occurrences of ?offers?which are nouns.Lastly, in step (f) of Figure 1, we run k-meansclustering method on the S-CODE sphere and splitword-substitute word pairs into 45 clusters be-cause the treebank we worked on uses 45 part-of-speech tags.
The output of clustering inducespart-of-speech categories of words tokens.3 ExperimentsIn this section, the setup of each experiment willbe presented.
The experiments are conducted onPenn Treebank Wall Street Journal corpus.
Thereare 1,173,766 tokens and, 49,206 types.
Out of49,206 word types, 1183 of them are chosen astarget words.
They are fed to the algorithm de-scribed above.
Occurrences of these target wordscorrespond to 37.55% of the whole data.
Thesetarget words are seen in the dataset more than 100times and less than 4000 times.
This subset is cho-sen as such because word types occurring morethan 4000 times are all with low gold tag perplex-ity.
They also increase computation time dramat-ically.
We exclude word types occurring less than100 times, because the clustering algorithm run-ning on 64-dimension vectors does not work accu-rately.
To avoid providing noisy results, the exper-iments are repeated 10 times.
We report many-to-one scores of the experiments.
The many-to-oneevaluation assigns each cluster to its most frequentgold-tag.
Overall result demonstrates the percent-age of correctly assigned instances and standarddeviation in paranthesis.3.1 BaselineBecause we are trying to improve (Yatbaz et al2012), we select the experiment on Penn Tree-bank Wall Street Journal corpus in that work asour baseline and replicate it.
In that experiment,POS induction is done by using word identitiesand context information represented by substitutewords.
Strictly one tag is assigned to each wordtype.
As a result, this method inaccurately inducesPOS tags for the occurrences of word types withhigh gold tag perplexity.
The many-to-one accu-racy of this experiment is 64%.3.2 UpperboundIn this experiment, for each word occurence, weconcatenate the gold tag for the first part of thepairs in the co-occurence input file.
Thus, weskipped steps (a), (b), (c).
The purpose of thisexperiment is to set an upperbound for all experi-ments since we cannot cluster the word tokens anybetter than the gold tags.
The many-to-one accu-racy of this experiment is 67.2%.3.3 Experiment 1In the algorithm section, we mention that after di-mensionality reduction step, we cluster the vec-tors to separate tokens of a target word seen in thesimilar contexts.
In this experiment, we set thenumber of clusters for each type to 2.
In otherwords, we assume that the number of differentPOS tags of each word type is equal to 2.
Nev-ertheless, separating all the words into 2 clustersresults in some inaccuracy in POS induction.
Thatis because not all words have POS ambiguity andsome have more than 2 different POS tags How-ever, the main purpose of this experiment is to ob-serve whether we can increase the POS inductionaccuracy for ambiguous types with our approach.The many-to-one accuracy of this experiment is63.8%.3.4 Experiment 2In the previous experiment, we set the number ofclusters for each word type to 2.
However, thenumber of different POS tags differs for each wordtype.
More importantly, around 41% of our targettokens belongs to unambiguous word types.
Also,around 36% of our target tokens comes from wordtypes whose gold perplexity is below 1.5.
Thatmeans, the Experiment 1 splits most of our wordtypes that should not be separated.In this experiment, instead of splitting all types,we guess which types should be splitted.
Also, weguess the number of clusters for each type.
Weuse gap statistic (Tibshirani et al 2001) on 64-dimensional vectors.
The Gap statistic is a sta-tistical method to guess the number of clustersformed in given data points.
We expect that substi-tute vectors occurring in the similar context shouldbe closely located in 64-dimensional space.
Thus,gap statistic can provide us the number of groupsformed by vectors in 64-dimensional space.
Thatnumber is possibly equal to the number of thenumber of different POS tags of the word types.The many-to-one accuracy of this experiment is63.4%.1203.5 Experiment 3In this experiment, we set the number of clustersfor each type to gold number of tags of each type.The purpose of this experiment is to observe howthe accuracy of number of tags given, which isused at step (c), affects the system.
The many-to-one accuracy of this experiment is 63.9%.3.6 Overall ResultsIn this section we present overall results of theexperiments.
We present our results in 3 sepa-rated tables because the accuracy of these methodsvaries with the ambiguity level of word types.In Table 2, many-to-one scores of three exper-iments are presented.
Since we exclude some ofthe word types, our results correspond to 37.55%of the data.
In Table 3, results for the word typeswhose gold tag perplexity is lower than 1.5 arepresented.
They correspond to 29.11% of the data.Lastly, in Table 4, we present the results for wordtypes whose gold tag perplexity is greater than 1.5.Experiment Many-to-One ScoreBaseline .64 (.01)Experiment 1 .638 (.01)Experiment 2 .634 (.01)Experiment 3 .639 (.02)Table 2: Results for the target words correspond-ing to 37.55% of the data.Experiment Many-to-One ScoreBaseline .693 (.02)Experiment 1 .682 (.01)Experiment 2 .68 (.01)Experiment 3 .684 (.02)Table 3: Results for Target Words with gold tagperplexity ?1.5 which corresponds to 29.11% ofthe data.Experiment Many-to-One ScoreBaseline .458 (.01)Experiment 1 .484 (.01)Experiment 2 .474 (.02)Experiment 3 .483 (.02)Table 4: Results for Target Words with gold tagperplexity ?1.5 which corresponds to 8.44% ofthe data..4 ConclusionTable 2 shows that the baseline experiment isslightly better than our experiments.
That is be-cause our experiments inaccurately induce morethan one tag to unambiguous types.
Additionally,most of our target words have low gold tag per-plexity.
Table 3 supports this claim.
In Table 4,we observe that our methods outscore the baselinesignificantly.
That is because, when ambiguity in-creases, the baseline method inaccurately assignsone POS tag to word types.
On the other hand, thegap statistic method is not fully efficient in guess-ing the number of clusters.
It sometimes separatesunambiguous types or it does not separate highlyambiguous word types.
As a result, there is a slightdifference between the results of our experiments.Additionally, the results of our experimentsshow that, accurately guessing number of clustersplays a crucial role in this approach.
Even usingthe gold number of different tags in Experiment 3does not result in a significantly accurate system.That is because, the number of different tags doesnot reflect the perplexity of a word type.The results show that, POS ambiguity can beaddressed by using substitute vectors for wordtypes with high ambiguity.
The accuracy of thisapproach correlates with the level of ambiguity ofword types.
Thus, the detection of the level of am-biguity for word types should be the future direc-tion of this research.
We again propose that substi-tute vector distributions could be useful to extractperplexity information for a word type.AcknowledgmentsI would like to thank the members of the Koc Uni-versity Artificial Intelligence Laboratory for theirhelp and support.
Additionally, I would like tothank two anonymous reviewers and Murat Sey-han for their comments and suggestions.ReferencesD.
Arthur and S. Vassilvitskii.
2007. k-means++: Theadvantages of careful seeding.
In Proceedings of theeighteenth annual ACM-SIAM symposium on Dis-crete algorithms, pages 1027?1035.
Society for In-dustrial and Applied Mathematics.Taylor Berg-Kirkpatrick and Dan Klein.
2010.
Phylo-genetic grammar induction.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, pages 1288?1297, Uppsala,121Sweden, July.
Association for Computational Lin-guistics.Phil Blunsom and Trevor Cohn.
2011.
A hierarchi-cal pitman-yor process hmm for unsupervised partof speech induction.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages865?874, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2010.
Two decades of unsuper-vised pos induction: how far have we come?
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 575?584, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In Proceedings of the tenth conference on Eu-ropean chapter of the Association for ComputationalLinguistics - Volume 1, EACL ?03, pages 59?66,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Sharon Goldwater and Tom Griffiths.
2007.
A fullybayesian approach to unsupervised part-of-speechtagging.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 744?751, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.David Graff, Roni Rosenfeld, and Doug Paul.
1995.Csr-iii text.
Linguistic Data Consortium, Philadel-phia.Yariv Maron, Michael Lamar, and Elie Bienenstock.2010.
Sphere embedding: An application to part-of-speech induction.
In J. Lafferty, C. K. I. Williams,J.
Shawe-Taylor, R.S.
Zemel, and A. Culotta, ed-itors, Advances in Neural Information ProcessingSystems 23, pages 1567?1575.T.H.
Mintz.
2003.
Frequent frames as a cue for gram-matical categories in child directed speech.
Cogni-tion, 90(1):91?117.Hinrich Schu?tze.
1995.
Distributional part-of-speechtagging.
In Proceedings of the seventh conferenceon European chapter of the Association for Compu-tational Linguistics, EACL ?95, pages 141?148, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.Andreas Stolcke.
2002.
Srilm-an extensible lan-guage modeling toolkit.
In Proceedings Interna-tional Conference on Spoken Language Processing,pages 257?286, November.J.B.
Tenenbaum, V. Silva, and J.C. Langford.
2000.A global geometric framework for nonlinear dimen-sionality reduction.
Science, 290(5500):2319.R.
Tibshirani, G. Walther, and T. Hastie.
2001.
Es-timating the number of data clusters via the gapstatistic.
Journal of the Royal Statistical Society B,63:411?423.Mehmet Ali Yatbaz, Enis Sert, and Deniz Yuret.
2012.Learning syntactic categories using paradigmaticrepresentations of word context.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 940?951, JejuIsland, Korea, July.
Association for ComputationalLinguistics.122
