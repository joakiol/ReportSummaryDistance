Proceedings of the 7th Workshop on Statistical Machine Translation, pages 468?479,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsDirect Error Rate Minimization for Statistical Machine TranslationTagyoung Chung?University of RochesterRochester, NY 14627, USAchung@cs.rochester.eduMichel GalleyMicrosoft ResearchRedmond, WA 98052, USAmgalley@microsoft.comAbstractMinimum error rate training is often the pre-ferred method for optimizing parameters ofstatistical machine translation systems.
MERTminimizes error rate by using a surrogate rep-resentation of the search space, such as N -best lists or hypergraphs, which only offeran incomplete view of the search space.
Inour work, we instead minimize error rate di-rectly by integrating the decoder into the min-imizer.
This approach yields two benefits.First, the function being optimized is the trueerror rate.
Second, it lets us optimize param-eters of translations systems other than stan-dard linear model features, such as distortionlimit.
Since integrating the decoder into theminimizer is often too slow to be practical, wealso exploit statistical significance tests to ac-celerate the search by quickly discarding un-promising models.
Experiments with a phrase-based system show that our approach is scal-able, and that optimizing the parameters thatMERT cannot handle brings improvements totranslation results.1 IntroductionMinimum error rate training (Och, 2003) is a com-mon method for optimizing linear model parame-ters, which is an important part of building good ma-chine translation systems.
MERT minimizes an arbi-trary loss function, usually an evaluation metric suchas BLEU (Papineni et al, 2002) or TER (Snoveret al, 2006) from a surrogate representation of thesearch space, such as the N -best candidate transla-tions of a development set.
Much of the recent work?
This research was conducted during the author?s intern-ship at Microsoft Research.on minimum error rate training focused on improv-ing the method by Och (2003).
Recent efforts ex-tended MERT to work on lattices (Macherey et al,2008) and hypergraphs (Kumar et al, 2009).
Ran-dom restarts and random walks (Moore and Quirk,2008) are commonly used to combat the fact thesearch space is highly non-convex, often with mul-tiple minima.Several problems still remain with MERT, threeof which are addressed by this work.
First, the N -best error surface explored by MERT is generallynot the same as the true error surface, which meansthat the error rate at an optimum1 of the N -best er-ror surface is not guaranteed to be any close to anoptimum of the true error surface.
Second, mostSMT decoders make search errors, yet MERT ig-nores the fact that the error surface of an error-pronedecoder differs from the one of an exact decoder(Chang and Collins, 2011).
MERT calculates an en-velope from candidate translations and assumes alltranslations on the envelope are reachable by the de-coder, but these translations may become unreach-able due to search errors.
Third, MERT is only usedto tune linear model parameters, yet SMT systemshave many free decoder parameters?such as distor-tion limit and beam size?that are not handled byMERT.
MERT does not provide a principled way toset these parameters.In order to overcome these issues, we explore theapplication of direct search methods (Wright, 1995)to SMT.
To do this, we integrate the decoder andthe evaluation metric inside the objective function,1The optimum found by MERT (Och, 2003) is generally notglobally optimal.
An alternative that optimizes N -best lists ex-actly is presented by Galley and Quirk (2011), and we do notdiscuss it further here.468which takes source sentences and a set of weights asinputs, and outputs the evaluation score (e.g., BLEUscore) computed on the decoded sentences.
Since itis impractical to calculate derivatives of this func-tion, we use derivative-free optimization methodssuch as the downhill simplex method (Nelder andMead, 1965) and Powell?s method (Powell, 1964),which generally handle such difficult search condi-tions relatively well.
This approach confers severalbenefits over MERT.
First, the function being opti-mized is the true error rate.
Second, integrating thedecoder inside the objective function forces the op-timizer to account for possible search errors.
Third,contrary to MERT, our approach does not require in-put parameters to be those of a linear model, so ourapproach can tune a broader range of features, in-cluding non-linear and hidden-state parameters (e.g.,distortion limit, beam size, and weight vector ap-plied to future cost estimates).In this paper, we make direct search reasonablyfast thanks to two speedup techniques.
First, weuse a model selection acceleration technique calledracing (Moore and Lee, 1994) in conjunction withrandomization tests (Riezler and Maxwell, 2005) toavoid decoding the entire development set at eachfunction evaluation.
This approach discards thecurrent model whenever performance on the trans-lated subset of the development data is deemed sig-nificantly worse in comparison to the current bestmodel.
Second, we store and re-use search graphsacross function evaluations, which eliminates someof the redundancy of regenerating the same transla-tions in different optimization steps.Our experiments with a strong phrase-based trans-lation system show that the direct search approach isan effective alternative to MERT.
The speed of directsearch is generally comparable to MERT, and trans-lation accuracy is generally superior.
The non-linearand hidden-state features tuned in this work bringgains on three language pairs, with improvementsranging between 0.27 and 0.35 BLEU points.2 Direct error rate minimizationMost current machine translation systems use a log-linear model:p(e|f) ?
exp(?i?ihi(e, f))where f is a source sentence, e is a target sentence,hi is a feature function, and ?i is the weight of thisfeature.
Given a source sentence f , finding the besttarget sentence e?
according to the model is a searchproblem, which is called decoding:e?
= argmaxeexp(?i?ihi(e, f))The target sentence e?
is automatically evaluatedagainst a reference translation r using any metricthat is known to be relatively well correlated withhuman judgment, such as BLEU or TER.
Let us re-fer to such error function as E(?).
Then, the processof finding the best set of weights ??
according to anerror function E is another search:??
= argmin?E(r; argmaxeexp(?i?ihi(e, f)))The typical MERT process solves the problem in aniterative fashion.
At each step i, it produces N -bestlists by decoding with ?
?i, then uses these lists tofind ??i+1.
Och (2003) presents an efficient multi-directional line search algorithm, which is based onthe fact that the error count along each line is piece-wise constant and thus easy to optimize exactly.
Theprocess is repeated until a certain convergence crite-rion is met, or until no new candidate sentences areadded to the pool.
The left side of Figure 1 summa-rizes this process.Though simple and effective, there are several lim-itations to this approach.
The primary reason is thatit can only tune parameters that are part of the log-linear model.
Aside from having parameters fromthe log-linear model, decoders generally have freeparameters ?
that needs to be set manually, suchas beam size and distortion limit.
These decoder-related parameters have complex interactions withlinear model parameters, thus, ideally, we wouldwant to tune them jointly with decoder parameterssuch as distortion limit.Direct search addresses these problems by includ-ing all feature parameters and all decoder-related pa-rameters within the optimization framework.
Fig-ure 1 contrasts MERT with direct search.
Ratherthan optimizing candidate pools of translations, di-rect search treats the decoder and the evaluation tool469decodercandidatepool?
?BLEUoptimizationinput fother params ?model params ?output erepeatdecoder?
?, ?
?1-best BLEUoptimizationinput fmodel params ?other params ?Figure 1: Comparison of MERT (left) and direct search (right).as a single function:?
(f, r;?,?)
= E(r; argmaxeexp(?i?ihi(e, f)))Then, it uses an optimization method to minimizethe function:argmin?,??
(f, r;?,?
)This formulation solves the problem mentioned pre-viously, since we jointly optimize ?
and ?, thusaccounting for the dependencies between the two.However, there are two problems to address with di-rect error minimization.
First, this approach requiresthe entire development set to be re-decoded everytime the function is evaluated, which can be pro-hibitively expensive.
To address this problem, wepresent several methods to speed up the search pro-cess in Section 5.
Second, since the gradient of stan-dard evaluation metrics such as BLEU is not knownand since methods for estimating the gradient numer-ically require too many function evaluations, we can-not use common search methods that use derivativesof a function.
Therefore, we need robust derivative-free optimization methods.
We discuss such opti-mization methods in Section 3.3 Derivative-free optimizationAs discussed in the previous sections, we need torely on derivative-free optimization methods for di-rect search.
We consider two such optimizationmethods:Powell?s method For each iteration, Powell?smethod tries to find a good direction along which thefunction can be minimized.
This direction is deter-mined by searching along each standard base vector.Then, a line search is performed along the directionby using line search methods such as golden sectionsearch or Fibonacci search.
The process is repeateduntil convergence.
We implement the golden sec-tion search as presented by Press et al (1992) in ourexperiments.
Although the golden section search isonly exact when the function is unimodal, we foundthat it works quite well in practice.
More details arepresented by Powell (1964).Nelder-Mead method This approach sets up asimplex on the search space, which is a polytopewith D + 1 vertices when there are D dimensions,and successively moves the simplex to a lower pointto find a minimum of the function.
The simplex ismoved using different actions, which are taken whencertain conditions are met.
The basic idea behindthese actions is to replace the worst point in the sim-plex with a new and better point, thereby moving thesimplex towards a minimum.
This method has theadvantage of being able to deal with ?bumpy?
func-tions and depending on the configuration of the sim-plex at the time, it is possible to escape some localminima.
This is often refer to as downhill simplexmethod and more details are presented by Nelderand Mead (1965).4 ParametersIn this section, we discuss the parameters that weoptimize with direct search, in addition to standard470linear model parameters:4.1 Distortion limitDistortion limit is one of decoder parameters thatsets a limit on the number of words the decoderis allowed to skip when deciding which sourcephrase to translate in order to allow reordering.
Fig-ure 2 shows a translation example from English toJapanese.
Every word jumped over incurs a dis-tortion cost, which is usually one of the transla-tion model parameters, which thereby discouragesreordering of words unless language model supportsthe reordering.Since having a large distortion limit leads toslower decoding, having the smallest possible dis-tortion limit that still facilitates correct reorderingwould be ideal.
Not only this speeds up translation,but this also leads to better translation quality byminimizing search errors.
Since a larger distortionlimit means there are more possible re-orderings oftranslations, it is prone to more search errors.
In fact,there are evidences that tuning the distortion limitis beneficial in improving quality of translation bylimiting search errors.
Galley and Manning (2008)conduct a line search along increments of distor-tion limit and separately tune the translation modelparameters for each increment of distortion limit.The result shows significant difference in translationquality when distortion limit is tuned along with themodel parameters.
Separately tuning model param-eters for different distortion limit is necessary be-cause model parameters are coupled with distortionlimit.
A representative example: when distortionlimit is zero, the distortion penalty feature can haveany weight and not affect BLEU scores, but this isnot the case when distortion limit is larger than zero.Tuning distortion limit in direct search in conjunc-tion with related features such linear distortion elim-inates the need for a line search for distortion limit.4.2 Polynomial featuresMost phrase-based decoders typically use a dis-tortion penalty feature to discourage (or maybesometimes encourage) reordering.
Whereas distor-tion limit is a hard constraint?since the decodernever considers jumps larger than the given limit?distortion penalty is a soft constraint, since it penal-izes reordering proportionally to the length of theI did not see the book you borrowed??
????
???
??
???
?+5-3-3Figure 2: Reordering in phrase-based translation.
A min-imum distortion limit of five is needed to correctly trans-late this example.
The source sentence is relatively sim-ple but a relatively large distortion limit is needed to ac-commodate the correct reordering due to typological dif-ference between two languages.jump.
The total distortion penalty is calculated asfollows:D(e, f) = ?d?j|dj |pdwhere ?d is the weight for distortion penalty feature,and dj is the size of the jump needed to translatethe j-th phrase pair.
For example, in Figure 2, thetotal distortion penalty feature value is 11, which ismultiplied with ?d to get the total distortion cost oftranslating the example sentence.
Although pd is typ-ically set to one (linear), one may consider polyno-mial distortion penalty (Green et al, 2010).
Green etal.
(2010) show that setting pd to a higher value thanone improves the translation quality, but uses a pre-determined value for pd.
Instead of manually settingthe value of pd, it can be given a value tuned with di-rect search.
Although we only discussed distortionpenalty here, it is straightforward to tune pi for eachfeature hi(e, f)pi using direct error rate minimiza-tion, where hi(e, f) is any linear model feature ofthe decoder.4.3 Future cost estimatesSince beam search involves pruning, it is crucial tohave good future cost estimation in order to min-imize the number of search errors (Koehn et al,2003).
The concept of future cost estimation is re-lated to heuristic functions in the A* search algo-rithm.
The total cost f(x) of a partial translationhypothesis is estimated by combining g(x), which isthe actual current cost from the beginning of a sen-tence to point x and h(x), which is the future cost471estimate from point x to the end of the sentence:f(x) = g(x) + h(x)In SMT decoding, the same feature weight vec-tor is generally used when computing g(x) and h(x).However, this may not be ideal since future cost esti-mators use different heuristics depending on the fea-tures.
For example, the future cost estimator (Greenet al, 2010) for linear distortion always underesti-mates completion cost, which is generally deemeda good property.
Unfortunately, some features haveestimators that tend to overestimate completion cost,as it is the case with the language model.
This prob-lem is illustrated in Figure 3.
The Figure showsthat the ratio between the estimated total cost andthe actual total cost converges to 1.0.
However,in earlier stages of translations, the estimated fu-ture cost for language model is larger than it shouldbe, which leads to higher total estimated cost.
Inthe A* search parlance, we are using an inadmissi-ble heuristic since the future cost is overestimated,which leads to suboptimal search.
This suggests thatseparately tuning parameters that are involved in thefuture cost estimation will lead to better pruning de-cisions.
This essentially doubles the number of lin-ear model parameters, since for every feature used infuture cost estimation, we create a counterpart andtune its weight independently.4.4 Search parametersIn addition to the parameters listed above, we alsotune general decoder parameters that affect thesearch quality: beam size and parameters controllinghistogram pruning and threshold pruning.
While itmakes sense to set these parameters automaticallyinstead of manually, the methods we have presentedthus far are not particularly fit for this type of pa-rameters.
Indeed, if the sole goal is to maximizetranslation quality (e.g., as measured by standardBLEU), a larger beam size and less pruning is usu-ally preferable.
To address this problem, we opti-mize these three parameters using a slightly differentobjective function.
When tuning any of these threefeatures, the goal of translation is to get the most ac-curate translation given a pre-defined time limit, sowe change the objective to be a time-sensitive objec-tive function.
Much akin to brevity penalty in BLEU,0 0.2 0.4 0.6 0.8 1012345Figure 3: y axis is ratio between estimated total cost vs.actual total cost of language model for thousands of trans-lations.
1.0 means the estimated total cost and the actualtotal cost are exactly the same, and anything higher than1.0 means the future cost has been overestimated therebyinflating the estimated total cost.
The x-axis representshow much translation has been completed.
0.1 means10% of a sentence has been translated.we define time penalty as:TP(?
)={1.0 ti ?
tdexp(1 ?
titd)ti > tdwhere TP(?
)is a time penalty that is multipliedto BLEU, ti is the time it takes to translate devel-opment set under current parameters, and td is thedesired time limit for translating the developmentset.
With this error metric, we still optimize forthe translation quality as long as the translation hap-pens within desired time td.
With the modified time-sensitive BLEU score as error metric, direct searchmay tune the parameters that have the speed and ac-curacy trade-off that we want.25 Speeding up direct searchOptimizing the true error surface is generally morecomputationally expensive than with any surrogateerror surface, since each function evaluation usuallyrequires decoding or re-decoding the entire devel-opment set.
Since SMT tuning sets used for error2A disadvantage of using time in the definition of TP`?
?is that it adds non-determinism that can make optimization un-stable.
Our solution is to replace time with pseudo-time, a de-terministic substitute expressed as a linear combination of thenumber of n-gram lookups and hypothesis expansions (thesetwo quantities correlate quite well with decoding time).472rate minimization often comprise one thousand sen-tences or more, each function evaluation can takeminutes or more.
However, this problem is some-what mitigated by the fact that translating in batchesis highly parallelizable.
Since MERT (Och, 2003) isalso easily parallelizable, we need to resort to otherspeedup techniques to make direct search a practi-cal alternative to MERT.
We now present two tech-niques that make optimization of the true error sur-face more efficient.5.1 A racing algorithm for speeding up SMTmodel selectionError rate minimization as presented in this papercan be seen as a form of model selection, whichhas been the focus of a lot of work in the learn-ing literature.
The most popular approaches tomodel selection?such as minimizing cross valida-tion error?tend to be very slow in practice; there-fore, researchers have addressed the problem of ac-celerating model selection using statistical tests.Prior to considering the SMT case, we review oneof these methods in the case of leave-one-out crossvalidation (LOOCV).
Racing for model selection(Maron and Moore, 1994; Moore and Lee, 1994)works as follows: we are given a collection of Nmmodels and Nd data points, and we must find themodel that minimizes the mean e?j = 1Nd?i ej(i),where ej(i) is the classification error of model Mjon the ith datapoint when trained on all datapointsexcept the ith point.
The models are evaluated con-currently, and at any given step k ?
[1, Nd], eachmodel Mj is associated with two pieces of informa-tion: the current estimate of its mean error rate, andthe estimate of its variance.
As evaluations progress,we eliminate any model that is significantly worsethan any other model.3 We also note that the Rac-ing technique first randomizes the order of the datapoints to ensure that prefixes of the dataset are gen-3The details of these statistical tests are not so importanthere since we use different ones in the case of SMT, but webriefly summarize them as follows: Maron and Moore (1994)use a non-parametric method (Hoeffding bounds (Hoeffding,1963)) for confidence estimation, and places confidence inter-vals on the mean value of the random variable representingej(i).
A model is discarded if its confidence interval no longeroverlaps with the confidence interval of the current best model.Moore and Lee (1994) use a similar technique, but relies onBayesian statistics instead of Hoeffding bounds.erally representative of the entire set.In this work, we use Racing to speed up directsearch for SMT, but this requires two main adjust-ments compared to the LOOCV case.
First, ourmodels have real-valued parameters, so we cannotexhaustively evaluate the set of all models since it isinfinite.
Instead, we use direct search to select whichmodels compete against each other during Racing.In the case of Powell?s method, all points of a gridalong the current search direction are evaluated inparallel using Racing, before we turn to the nextline search.
In the case of the downhill simplex op-timizer and in the case of line searches other thangrid search (e.g., golden section search), the use ofRacing is more difficult because the function eval-uations requested by these optimizers have depen-dencies that generally prevent concurrent functionevaluations.
Since functions in downhill simplex areevaluated in sequence and not in parallel, our solu-tion is to race the current model against our currentbest model.4 When the evaluation of a model M isinterrupted because it is deemed significantly worsethan the current best model M?
, the error rate of Mon the entire development set is extrapolated fromits relative performance on the decoded subset.5The second main difference with the LOOCVcase is that we do not use confidence intervals to de-termine which of two or models are best.
In SMT, itis common to use either bootstrap resampling (Efronand Tibshirani, 1993; Och, 2003) or randomizationtests (Noreen, 1989).
In this paper, we use the ran-domization test for discarding unpromising models,since this statistical test was shown to be less likelyto cause type-I errors6 than bootstrap methods (Rie-zler and Maxwell, 2005).
Since both kinds of statisti-cal tests involve a time-consuming sampling step, it4Since Racing only discards suboptimal models, the currentbest model M?
is one for which we have decoded the entire de-velopment set.
Once a new model M is evaluated, we performat step j a significance test to determine whether M ?s transla-tion of sentences 1 .
.
.
j is better or worse than M?
translationfor the same range of sentences.
If M is significantly worse, wediscard it.
If M?
is worse, we continue evaluating the perfor-mance of M , since we need M ?s output for the full developmentset if M eventually becomes the new best model.5For example, if error rates of M?
and M are respectively10% and 11% on the subset decoded by both models and M?
?serror on the entire set is 20%, M ?s extrapolated error is 22%.6A type I error rejects a null hypothesis that is true.473is somewhat wasteful to perform a new test after thedecoding of each sentence, so we translate sentencesin small batches of K sentences before performingeach randomization test.7We finally note that Racing no longer guaranteesthat the error function observed by the optimizer isthe true error function.
Racing causes some approx-imations of the error function, but the degree of ap-proximation is designed to be small in regions withlow error rates, and Racing ensures that the mostpromising function evaluations in our progression to-wards an optimum are unaffected.
In contrast, theapproximation of the error function computed fromN -best lists or lattice does not share this property.8To further speed up function evaluations in directsearch, we employ a method meant to deal with mod-els that are nearly identical, a situation in which Rac-ing usually does not help much.
Indeed, when twomodels produce very similar outputs, we often needto run the race through every sentence of the devel-opment set since none of the two models end up be-ing significantly better.
A solution to this problemconsists of discarding models that are nearly identi-cal to other models, where similarity between mod-els is solely measured from their outputs.9 To dothis, we resort again to a randomization test: Giventwo models Ma and Mb, this test performs randompermutations between outputs of Ma and Mb, that is,it determines for each sentence of index i whether ornot to permute the two model outputs, with proba-bility p = 0.5.
When Ma and Mb are very similar,these permutations have little effect, even when werepeat this sampling process many times.
To copewith this problem, we slightly modify the random-7In our experiments, we set K = 50.
Some other practi-cal considerations: the significance level used for discardingunpromising models is p ?
.05.
The randomization test is asampling-based technique, for which we must specify a samplesize R. In this paper, we use R = 5000.8In the case of N -best MERT, it is not even guaranteed thatwe find the true error rate of our current best model M whilesearching the N -best error surface.
In fact, if we take the pa-rameters of our best model M and re-decode the developmentset, we may get an error rate that is different from what was pre-dicted from the N -best list.
With direct search and Racing, nosuch approximation affects our current best model.9Measuring model similarity only based on parameter val-ues is less effective, since features and other parameters aresometimes redundant, and two models may behave similarlywhile having fairly distinct parameter values.ization test to discard one of the two nearly iden-tical models.
Specifically, we compute the gap?measured in error rate?between the best random-ized output and the worst randomized output.
If thisgap is lower than a pre-defined threshold, we onlykeep the best model.10 This adjustment to the sig-nificance test makes direct search reasonably fast,since Racing is effective during the initial steps ofsearch (when steps tend to be relatively big, andwhen differences in error rate are pretty significant),and our modification to randomization tests helpswhile search converges towards an optimum usingincreasingly smaller steps.5.2 Lattice-based decodingWe use another technique to speed up direct searchby storing and re-using search graphs, which con-sist of lattices in the case of phrase-based decod-ing (Och et al, 1999) and hypergraphs in the caseof hierarchical decoding (Chiang, 2005).
The suc-cessive expansion of translation options in order toconstruct the search graph is generally done fromscratch, but this can be wasteful when the samesentences are translated multiple times, as it is thecase with direct search.
Even when the parame-ters of the decoder change across function evalua-tions, some partial translation are likely to be con-structed multiple times, and this is more likely tohappen when changes in parameters are relativelysmall.
To overcome this inefficiency, we memoizehypotheses expansions made in all function evalu-ations, which then allows us to reuse some edges(or hyperedges) from previous iterations to constructthe current graph (or hypergraph).
Since featurevalues?including expensive features like languagemodel score?are stored into each edge, the speedupis roughly proportional to the percentage of edgeswe can reuse.A more radical way of exploiting search graphsof previous iterations is to use them as constraints ina forced decoding approach.
In this framework, thedecoder takes as input not only an input sentence,but also a constraining search graph.
During decod-ing, it is forced to discard any translation hypothe-10In the case where we compare our current best model anda model that is currently being evaluated, we discard the latter.In our experiments with BLEU, we discard if the gap is smallerthan 0.1 BLEU point.474decoderconstraineddecoder?
?, ?
?1-best BLEUoptimizationinput fother params ?model params ?lattice ?repeatFigure 4: Lattice-constrained decoding for direct search.ses that violate the constraining search graph.
Thismakes the memoization method presented in the pre-vious paragraph maximally efficient, since lattice-constrained decoding has all linear model featurevalues already pre-computed.
While this approachis similar in spirit to lattice-based MERT (Machereyet al, 2008), there is a crucial difference.
The opti-mization steps in lattice MERT bypass the decoder,but the lattice-based approach presented here doesnot.
The distinction is important when it comes totuning non-linear and hidden state parameters of thedecoder.
For instance, the initial lattice may havebeen constructed with a distortion limit of 4, whilethe current model specifies a distortion limit of 2.At that stage, optimization via lattice-constrained de-coding instead of lattice-based MERT ensures thatwe will never select a path of the input lattice thatcorresponds to a distortion limit of more than 2.
Thisis important since the error rate must reflect the factthat jumps of two or more words are not allowed.Figure 4 shows how direct search with lattice-constrained decoding is structured.
Similarly toMERT and as opposed to straight direct search, opti-mization is repeated multiple times.
Since each opti-mization in the lattice-constrained case does not re-quire recomputing any features, it usually turns intovery significant gains in terms of translation speed,though it also causes a small loss of translation ac-curacy in general.
The overall approach depicted inFigure 4 works as follows: a first set of lattices isgenerated using an initial ?0 and ?0.
We then rundirect search with a decoder constrained on this setof lattices.
After optimization has converged, the op-Train MERT dev.
TestKorean-English 7.9M 1000 6000Arabic-English 11.1M 1000 6000Farsi-English 739K 1000 2000Table 1: Size of bitexts in number of sentence pairs.timal ??
and ??
are provided as input ?1 and ?1 tostart a new iteration of this process.
Note that theconstraining lattices built at each iteration are alwaysmerged with those of the previous ones, so constrain-ing lattices grow over time.
The two stopping crite-ria are similar to MERT: if the norm of the differencebetween the previous parameter vector?including?
and ?
?and the current vector falls below a pre-defined tolerance value, we do not continue to thenext iteration.
Alternatively, if a new pass of un-constrained decoding generates lattices that are sub-sumed by lattices constructed at previous iteration,we stop and do not run the next optimization step.6 Experiments6.1 SetupFor our experiments, we use a phrase-based transla-tion system similar to Moses (Koehn et al, 2007).Our decoder uses many of the same features asMoses, including four phrasal and lexicalized trans-lation scores, phrase penalty, word penalty, a lan-guage model score, linear distortion, and six lexical-ized reordering scores.
Unless specified otherwise,the decoder?s stack size is 50, and the number oftranslation options per input phrase is 25.Table 1 summarizes the amount of training dataused to train translation systems from Korean, Ara-bic, and Farsi into English.
These data sets aredrawn from various sources, which include news,web, and technical data, as well as United Nationsdata in the case of Arabic.
In order to get the senseof how presented techniques generalize, we evalu-ate our systems on a fairly broad domain.
We usedevelopment and test sets are a mix of news, web,and technical data.
All systems translate into En-glish, for which we built a 5-gram language modelwith cutoff counts 1, 1, 1, 2, 3 for unigrams to 5-grams, using a corpus of roughly seven billion En-glish words.
This includes the target side of the par-allel training data, plus a significant amount of datagathered from the web.475# Minimizer Optimized parameters Arabic Korean Farsi1 MERT with grid search lin, DL 29.12 (14.6) 23.30 (20.8) 32.16 (11.7)2 Direct search (simplex) lin, DL 29.07 (1.2) 23.42 (4.4) 32.22 (1.3)3 Direct search (Powell) lin, DL 29.20 (2.3) 23.39 (5.6) 32.28 (2.1)4 Direct search (Powell) lin, extended, DL 29.39 (4.4) 23.61 (8.9) 32.51 (4.9)5 Lattice-constrained (Powell) lin, extended, DL 29.27 (0.7) 23.43 (1.3) 32.42 (1.1)6 Direct search (Powell) lin, extended, DL, search 29.31 (6.5) 23.46 (9.7) 32.62 (6.2)Table 2: BLEU-4 scores (%) with one reference, translating into English; the numbers in parentheses are times inhours to run parameter optimization end-to-end.
?Lin?
refers to Moses linear model features; ?extended?
refers to non-linear and hidden state features (polynomial features, future cost); ?DL?
refers to distortion limit; ?search?
is the set ofparameters controlling search quality (parameters controlling beam size, histogram pruning, and threshold pruning).Our baseline system is trained for each languagepair by running minimum error rate training (Och,2003) on 1000 sentences.
Each iteration of MERTutilizes 19 random starting points, plus the points ofconvergence at all previous iterations of MERT, anda uniform weight vector.
That is, the first iterationof MERT uses 20 starting points, the second uses 21points, etc.
Since MERT is not able to directly opti-mize search parameters such as distortion limit andbeam size, our baseline system uses grid search tooptimize them.
To make this search more tractable,we only perform the grid search for a single param-eter: the distortion limit.
For each language pair,the grid search consists of repeating MERT for eightdistinct distortion limits ranging from 3 to 10.
Theoptimal distortion limits found for Korean, Arabic,and Farsi, are 8, 5, and 6, respectively.11 To ensurethat the comparison with our approach is consistent,this grid search is made on the MERT dev set itself.The next subsection contrasts the different directsearch methods presented in this paper.
Note thatall these experiments use the speedup techniquesbased on statistical significance test presented in Sec-tion 5.
Indeed, we found that using these techniquesresulted in faster speeds without affecting the searchin any significant way.
Models tuned with or withoutsignificance tests often ended up identical.6.2 ResultsThe main results are shown in Table 2, and are com-puted using standard BLEU-4 (Papineni et al, 2002)11We rerun MERT for each different distortion limit becauseof the dependencies between this parameter and linear modelfeatures, particularly linear distortion and lexicalized reorderingscores.
A linear model that is effective with a distortion limit of4 can be suboptimal for a limit of 8.using one reference translation, and ignoring case.Row 1 displays results of the MERT baseline, witha distortion limit that was found optimal using agrid search on the development set.
Rows 2 and 3show results of direct error rate minimization withdownhill simplex and Powell?s method, where di-rect search optimizes both linear model parametersand the distortion limit.
We see here that the per-formance of direct search is comparable and some-times better than MERT, but the benefit of directsearch here is that it does not require an external gridsearch to find an effective distortion limit (each di-rect search is initialized with a distortion limit of 10).Row 4 shows the performance of Powell?s methodusing the extended parameter set (Section 4), whichincludes model weights for future costs and polyno-mial features.
We lack space to present an exten-sive analysis of the relative impact of the differentnon-linear features and parameters discussed in thispaper, but we generally find that the following pa-rameters work best: distortion limit, polynomial dis-tortion penalty, and weight of future cost estimate ofthe language model.
The fact that Moses-style futurecost estimation for language models often overesti-mates probably explains why the latter feature helps.In the last row of Table 2, optimization is doneusing the time-sensitive variant of BLEU presentedin Section 4.4, and the set of parameters tuned hereincludes all the previous ones, in addition to beamsize, and the two parameters controlling histogramand threshold pruning in beam search.
Clearly, run-ning direct search to directly optimize BLEU wouldyield a very large beam size and would set pruningparameters that are so permissive that they would al-most completely disable pruning.
The benefit of us-ing the time-sensitive variant of BLEU is that direct476search is forced to find parameter weights that of-fer a good balance between accuracy and speed.
Tomake our results in row 6 as comparable as possibleto row 4, we use the running time (on the develop-ment set) of row 4 as a time constraint for the modelof row 6, which is to decode the entire developmentset at least as fast.
In other words, the system ofrow 6 is optimized to be no slower than the systemof row 4, and is otherwise penalized due to the timepenalty.
The effect of this is that translation speed attuning time is almost the same, and speed of systems4 and 6 is roughly the same at test time.
A com-parison between rows 4 and 6 suggests that tuningsearch parameters such as beam size and without af-fecting time does not provide much gain in terms oftranslation quality, but the method nevertheless hasone advantage: one can target a specific translationspeed without having to manually tune any param-eter such as beam size, and without even having todecide which parameter to manually tune.Times to run optimizations end-to-end are re-ported in parentheses in Table 2 and they take intoaccount the time to run the grid search in the caseof MERT.
Times to decode test sets are not reportedhere since they are roughly the same across all mod-els.
While translation accuracy with MERT and di-rect search is roughly the same when the underly-ing parameter set is the same, direct search wins inrunning time when it comes to optimizing search pa-rameters like distortion limit.
Since each grid searchruns MERT eight times, MERT is generally fasterthan direct search, but the difference of speed re-mains reasonable if the number of tuned parametersis the same, and direct search is rarely twice as slow.We finally discuss the case of lattice-constraineddecoding, which is shown in row 5 of Table 2.
Thismethod is not applicable when tuning parametersthat affect search thoroughness (row 6), such asbeam size.
The reason is that lattice-constraineddecoding is a form of forced decoding that con-siderably narrows the search space.
Under a con-strained decoding setting, it appears that a largebeam size seldom affects translation speed, but thisis misleading and largely due to constraints cre-ated by the lattice.
We thus evaluate the lattice-constrained case without tuning ?search?
features,and find that direct search is significantly faster us-ing lattice-constrained, with only a slight degrada-tion of translation quality.
Lattice constraints areaugmented 2-5 times before it converges.7 Related workThe use of derivative-free optimization methods totune machine translation parameters has been triedbefore.
Bender et al (2004) used the Nelder-Meadmethod to tune model parameters for a phrase-basedtranslation system.
However, their way of makingdirect search fast and practical is to set distortionlimit to zero, which results in poor translation qual-ity for many language pairs.
Zens et al (2007) alsouse the Nelder-Mead method to tune parameters in alog-linear model to maximize expected BLEU.
Zhaoand Chen (2009) proposes changes to Nelder-Meadmethod to better fit parameter tuning in their ma-chine translation setting.
They show the modifica-tion brings better search of parameters over the regu-lar Nelder-Mead method.
Our work is related to thesearch-based structured prediction (SEARN) modelof Daume?
(2006), in the sense that direct search alsoaccounts for what happens during search (includingsearch errors) to try to find parameters that are notonly good for prediction, but for search as well.8 ConclusionThis paper addressed the problem of minimizing er-ror rate at a corpus level.
We show that a techniqueto directly minimize the true error rate, rather thanone estimated from a surrogate representation suchas an N -best list, is in fact feasible.
We present twotechniques that make this minimization significantlyfaster, to the point where this technique is a viablealternative to MERT.
In the case where free param-eters of the decoder (such as distortion limit) alsoneed to be optimized, our technique is in fact muchfaster.
We also optimize non-linear and hidden statefeatures that cannot be tuned using MERT, whichyield improvements in translation accuracy.
Experi-ments on large test sets yield gains on three languagepairs, and our best configuration outperforms MERTby 0.27 to 0.35 BLEU points using a baseline systemtrained on large amounts of data.AcknowledgmentsWe thank anonymous reviewers, Chris Quirk, KristinaToutanova, and Anthony Aue for valuable suggestions.477ReferencesOliver Bender, Richard Zens, Evgeny Matusov, and Her-mann Ney.
2004.
Alignment templates: the RWTHSMT system.
In Proc.
of the International Workshopon Spoken Language Translation, pages 79?84, Kyoto,Japan.Yin-Wen Chang and Michael Collins.
2011.
Exact de-coding of phrase-based translation models through La-grangian relaxation.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 26?37, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Conference of the Association forComputational Linguistics (ACL-05), pages 263?270,Ann Arbor, MI.Hal Daume?, III.
2006.
Practical Structured LearningTechniques for Natural Language Processing.
Ph.D.thesis, University of Southern California, Los Angeles,CA, USA.B.
Efron and R. J. Tibshirani.
1993.
An Introduction tothe Bootstrap.
Chapman & Hall, New York.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 848?856, Honolulu, Hawaii, October.
Associa-tion for Computational Linguistics.Michel Galley and Chris Quirk.
2011.
Optimal searchfor minimum error rate training.
In Proceedings ofthe 2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 38?49, Edinburgh, Scot-land, UK., July.
Association for Computational Lin-guistics.Spence Green, Michel Galley, and Christopher D. Man-ning.
2010.
Improved models of distortion cost forstatistical machine translation.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 867?875, Los Angeles,California, June.
Association for Computational Lin-guistics.Wassily Hoeffding.
1963.
Probability inequalities forsums of bounded random variables.
Journal of theAmerican Statistical Association, 58(301):13?30.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Meeting of the North American chap-ter of the Association for Computational Linguistics(NAACL-03), Edmonton, Alberta.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,and Evan Herbst.
2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedings ofACL, Demonstration Session, pages 177?180.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error rate train-ing and minimum Bayes-risk decoding for translationhypergraphs and lattices.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 163?171, Sun-tec, Singapore, August.
Association for ComputationalLinguistics.Wolfgang Macherey, Franz Och, Ignacio Thayer, andJakob Uszkoreit.
2008.
Lattice-based minimum errorrate training for statistical machine translation.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 725?734,Honolulu, Hawaii, October.
Association for Computa-tional Linguistics.Oded Maron and Andrew W. Moore.
1994.
Hoeffdingraces: Accelerating model selection search for classi-fication and function approximation.
In Advances inneural information processing systems 6, pages 59?66.Morgan Kaufmann.Andrew Moore and Mary Soon Lee.
1994.
Efficientalgorithms for minimizing cross validation error.
InW.
W. Cohen and H. Hirsh, editors, Proceedings of the11th International Confonference on Machine Learn-ing, pages 190?198.
Morgan Kaufmann.Robert C. Moore and Chris Quirk.
2008.
Randomrestarts in minimum error rate training for statisticalmachine translation.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics(Coling 2008), pages 585?592, Manchester, UK, Au-gust.
Coling 2008 Organizing Committee.J.
A. Nelder and R. Mead.
1965.
A simplex methodfor function minimization.
Computer Journal, 7:308?313.Eric W. Noreen.
1989.
Computer-Intensive Methodsfor Testing Hypotheses : An Introduction.
Wiley-Interscience.Franz Josef Och, Christoph Tillmann, Hermann Ney, andLehrstuhl Fiir Informatik.
1999.
Improved alignmentmodels for statistical machine translation.
In Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing and Very Large Corpora,pages 20?28.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In Proceedings of the41th Annual Conference of the Association for Compu-tational Linguistics (ACL-03).478Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Conference of the Association for Compu-tational Linguistics (ACL-02).M.
J. D. Powell.
1964.
An efficient method for findingthe minimum of a function of several variables withoutcalculating derivatives.
The Computer Journal, 7:155?162.William H. Press, Saul A. Teukolsky, William T. Vetter-ling, and Brian P. Flannery.
1992.
Numerical recipesin C (2nd ed.
): the art of scientific computing.
Cam-bridge University Press, New York, NY, USA.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance test-ing for MT.
In Proceedings of the ACL Workshop onIntrinsic and Extrinsic Evaluation Measures for Ma-chine Translation and/or Summarization, pages 57?64,June.Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
In Pro-ceedings of Association for Machine Translation in theAmericas, pages 223?231.M H Wright.
1995.
Direct search methods: Oncescorned, now respectable.
Numerical Analysis,344:191?208.Richard Zens, Sasa Hasan, and Hermann Ney.
2007.
Asystematic comparison of training criteria for statisti-cal machine translation.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 524?532.Bing Zhao and Shengyuan Chen.
2009.
A simplexArmijo downhill algorithm for optimizing statisticalmachine translation decoding parameters.
In Proceed-ings of Human Language Technologies: The 2009 An-nual Conference of the North American Chapter of theAssociation for Computational Linguistics, Compan-ion Volume: Short Papers, pages 21?24, Boulder, Col-orado, June.
Association for Computational Linguis-tics.479
