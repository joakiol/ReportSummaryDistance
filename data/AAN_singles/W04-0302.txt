Stochastically Evaluating the Validity of Partial Parse Trees inIncremental ParsingYoshihide Kato1, Shigeki Matsubara2 and Yasuyoshi Inagaki3Graduate School of International Development, Nagoya University 1Information Technology Center, Nagoya University 2Furo-cho, Chikusa-ku, Nagoya, 464-8601 JapanFaculty of Information Science and Technology, Aichi Prefectural University 31522-3 Ibaragabasama, Kumabari, Nagakute-cho, Aichi-gun, 480-1198 Japanyosihide@gsid.nagoya-u.ac.jpAbstractThis paper proposes a method for evaluating thevalidity of partial parse trees constructed in incre-mental parsing.
Our method is based on stochasticincremental parsing, and it incrementally evaluatesthe validity for each partial parse tree on a word-by-word basis.
In our method, incremental parserreturns partial parse trees at the point where the va-lidity for the partial parse tree becomes greater thana threshold.
Our technique is effective for improv-ing the accuracy of incremental parsing.1 IntroductionReal-time spoken language processing systems,such as simultaneous machine interpretation sys-tems, are required to quickly respond to users?
utter-ances.
To fulfill the requirement, the system needsto understand spoken language at least incremen-tally (Allen et al, 2001; Inagaki and Matsubara,1995; Milward and Cooper, 1994), that is, to ana-lyze each input sentence from left to right and ac-quire the content.Several incremental parsing methods have beenproposed to date (Costa et al, 2001; Haddock,1987; Matsubara et al, 1997; Milward, 1995;Roark, 2001).
These methods construct candidatepartial parse trees for initial fragments of the inputsentence on a word-by-word basis.
However, thesemethods contain local ambiguity problems that par-tial parse trees representing valid syntactic relationscan not be determined without using informationfrom the rest of the input sentence.On the other hand, Marcus proposed a methodof deterministically constructing valid partial parsetrees by looking ahead several words (Marcus,1980), while Kato et al proposed an incrementalparsing which delays the decision of valid partialparse trees (Kato et al, 2000).
However, it is hard tosay that these methods realize broad-coverage incre-mental parsing.
The method in the literature (Mar-cus, 1980) uses lookahead rules, which are con-structed by hand, but it is not clear whether broadcoverage lookahead rules can be obtained.
Theincremental parsing in the literature (Kato et al,2000), which is based on context free grammar, isinfeasible to deal with large scale grammar, becausethe parser exhaustively searches all candidate partialparse trees in top-down fashion.This paper proposes a probabilistic incrementalparser which evaluates the validity of partial parsetrees.
Our method extracts a grammar from a tree-bank, and the incremental parsing uses a beam-search strategy so that it realizes broad-coverageparsing.
To resolve local ambiguity, the parser in-crementally evaluates the validity of partial parsetrees on a word-by-word basis, and delays the deci-sion of which partial parse trees should be returned,until the validity for the partial parse tree becomesgreater than a threshold.
Our technique is effectivefor improving the accuracy of incremental parsing.This paper is organized as follows: The nextsection proposes a probabilistic incremental parser.Section 3 discusses the validity of partial parse treeconstructed in incremental parsing.
Section 4 pro-poses a method of incrementally evaluating the va-lidity of partial parse tree.
In section 5, we report anexperimental evaluation of our method.2 TAG-based Incremental ParsingOur incremental parsing is based on tree adjoininggrammar (TAG) (Joshi, 1985).
This section pro-poses a TAG-based incremental parsing method.2.1 TAG for Incremental ParsingFirstly, we propose incremental-parsing-orientedTAG (ITAG).
An ITAG comprises two sets of ele-mentary trees just like TAG: initial trees and auxil-iary trees.
The difference between ITAG and TAGis the form of elementary trees.
Every ITAG ini-tial tree is leftmost-expanded.
A tree is leftmost-expanded if it is of the following forms:1.
[t]X , where t is a terminal symbol and X is anonterminal symbol.SNP VPPRPIVPVB NPfoundNPDT NNaNNdime NPDT NNtheNNwoodInitial trees:1 25 7 810VPVB NPfound3 ADJPPPIN NPinNPNP* PPIN NPinVPVP*Auxiliary trees:1 2NPDT NNa6 JJ NPDT NNthe9 JJVPVBfound4Figure 1: Examples of ITAG elementary trees2.
[?X1 ?
?
?Xk]X , where ?
is a leftmost expandedtree, X1, .
.
.
, Xk, X are nonterminal symbols.On the other hand, every ITAG auxiliary tree is ofthe following form:[X?
?X1 ?
?
?Xk]Xwhere ?
is a leftmost expanded tree and X ,X1, .
.
.
, Xk are nonterminal symbols.
X?
is calleda foot node.
Figure 1 shows examples of ITAG ele-mentary trees.These elemental trees can be combined by usingtwo operations: substitution and adjunction.substitution The substitution operation replaces aleftmost nonterminal leaf of a partial parse tree?
with an initial tree ?
having the same nonter-minal symbol at its root.
We write s?
for theoperation of substituting ?
and s?(?)
for theresult of applying s?
to ?.adjunction The adjunction operation splits a par-tial parse tree ?
at a nonterminal node havingno nonterminal leaf, and inserts an auxiliarytree ?
having the same nonterminal symbol atits root.
We write a?
for the operation of ad-joining ?
and a?(?)
for the result of applyinga?
to ?.The substitution operation is similar to rule expan-sion of top-down incremental parsing such as (Mat-subara et al, 1997; Roark, 2001).
Furthermore,by introducing the adjunction operation to incre-mental parsing, we can expect that local ambiguityof left-recursive structures is decreased (Lombardoand Sturt, 1997).Our proposed incremental parsing is based onITAG.
When i-th word wi is scanned, the parsercombines elementary trees for wi with partial parsetrees for w1 ?
?
?wi?1 to construct the partial parsetrees for w1 ?
?
?wi?1wi.As an example, let us consider incremental pars-ing of the following sentence by using ITAG shownin Figure 1:I found a dime in the wood.
(1)Table 1 shows the process of tree constructionfor the sentence (1).
When the word ?found?
isscanned, partial parse trees #3, #4 and #5 are con-structed by applying substitution operations to par-tial parse tree #2 for the initial fragment ?I?.
Whenthe word ?in?
is scanned, partial parse trees #12 and#13 are constructed by applying adjunction opera-tions to partial parse tree #10 for the initial frag-ment ?I found a dime?.
This example shows thatthe ITAG based incremental parsing is capable ofconstructing partial parse trees of initial fragmentsfor every word input.2.2 ITAG Extraction from TreebankHere, we propose a method for extracting an ITAGfrom a treebank to realize broad-coverage incre-mental parsing.
Our method decomposes parse treesin treebank to obtain ITAG elementary trees.
Thedecomposition is as follows:?
for each node ?1 having no left-sibling, if theparent ?p has the same nonterminal symbol as?1, split the parse tree at ?1 and ?p, and com-bine the upper tree and the lower tree.
?1 ofintermediate tree is a foot node.?
for each node ?2 having only one left-sibling,if the parent ?p does not have the same nonter-minal symbol as the left-sibling ?1 of ?2, splitthe parse tree at ?2.?
for the other node ?
in the parse tree, split theparse tree at ?.For example, The initial trees ?1, ?2, ?5, ?7 ?8 and?10 and the auxiliary tree ?2 are extracted from theparse tree #18 in Table 1.Our proposed tree extraction is similar to the TAGextractions proposed in the literatures (Chen andVijay-Shanker, 2000; Chiang, 2003; Xia, 1999).The main difference between these methods is theposition of nodes at which parse trees are split.While the methods in the literatures (Chen andVijay-Shanker, 2000; Chiang, 2003; Xia, 1999) uti-lize a head percolation rule to split the parse trees atcomplement nodes, our method splits the parse treesTable 1: Incremental parsing process of ?I found a dime in the wood.
?word # partial parse tree1 sI 2 [[[I]prp]npvp]sfound 3 [[[I]prp]np[[found]vbnp]vp]s4 [[[I]prp]np[[found]vbnp adjp]vp]s5 [[[I]prp]np[[found]vb]vp]sa 6 [[[I]prp]np[[found]vb[[a]dtnn]np]vp]s7 [[[I]prp]np[[found]vb[[a]dtjj nn]np]vp]s8 [[[I]prp]np[[found]vb[[a]dtnn]npadjp]vp]s9 [[[I]prp]np[[found]vb[[a]dtjj nn]npadjp]vp]sdime 10 [[[I]prp]np[[found]vb[[a]dt[dime]nn]np]vp]s11 [[[I]prp]np[[found]vb[[a]dt[dime]nn]npadjp]vp]sin 12 [[[I]prp]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]innp]pp]vp]s13 [[[I]prp]np[[found]vb[[[a]dt[dime]nn]np[[in]innp]pp]np]vp]sthe 14 [[[I]prp]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]in[[the]dtnn]np]pp]vp]s15 [[[I]prp]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]in[[the]dtjj nn]np]pp]vp]s16 [[[I]prp]np[[found]vb[[[a]dt[dime]nn]np[[in]in[[the]dtnn]np]pp]np]vp]s17 [[[I]prp]np[[found]vb[[[a]dt[dime]nn]np[[in]in[[the]dtjj nn]np]pp]np]vp]swood 18 [[[I]prp]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]in[[the]dt[wood]nn]np]pp]vp]s19 [[[I]prp]np[[found]vb[[[a]dt[dime]nn]np[[in]in[[the]dt[wood]nn]np]pp]np]vp]sat left recursive nodes and nodes having left-sibling.The elementary trees extracted by our method are ofthe forms described in section 2.1, and can be com-bined from left to right on a word-by-word basis.The property is suitable for incremental parsing.
Onthe other hand, the elementary trees obtained by themethod based on head information does not neces-sarily have this property 1.2.3 Probabilistic ITAGThis section describes probabilistic ITAG (PITAG)which is utilized by evaluating partial parse trees inincremental parsing.
PITAG assigns a probabilityto the event that an elementary tree is combined bysubstitution or adjunction with another tree.We induce the probability by maximum likeli-hood estimation.
Let ?
be an initial tree and X bethe root symbol of ?.
The probability that ?
is sub-stituted is calculated as follows:P (s?)
= C(s?)???
?I(X) C(s??
)(2)where C(s?)
is the count of the number of times ofapplying substitution s?
in the treebank, and I(X)is the set of initial trees whose root is labeled withX .1For example, the tree extraction based on head informa-tion splits the parse tree #18 at the node labeled with dt to ob-tain the elementary tree [a]dt for ?a?.
However, the tree [a]dtcannot be combined with the partial parse tree for ?I found?,since substitution node labeled with dt exists in the initial tree[dt[dime]nn]np for ?dime?
and not the partial parse trees for ?Ifound?.Let ?
be a auxiliary tree and X be the root symbolof ?.
The probability that ?
is adjoined is calculatedas follows:P (a?)
= C(a?
)C(X) (3)where C(X) is the count of the number of occur-rences of symbol X .
The probability that adjunctionis not applied is calculated as follows:P (nilX) = 1???
?A(X)P (a?)
(4)where nilX means that the adjunction is not appliedto a node labeled with X , and A(X) is the set of allauxiliary trees whose root is labeled X .In this PITAG formalism, the probability that el-ementary trees are combined at each node dependsonly on the nonterminal symbol of that node 2.The probability of a parse tree is calculated by theproduct of the probability of the operations whichare used in construction of the parse tree.
For ex-ample, the probability of each operation is given asshown in Table 2.
The probability of the partialparse tree #12, which is constructed by using s?1 ,s?2 , s?5 , s?7 , nilNP and a?2 , is 1 ?
0.7 ?
0.3 ?0.5?
0.7?
0.7 = 0.05145.We write P (?)
for the probability of a partialparse tree ?.2The PITAG formalism corresponds to SLG(1) in the liter-ature (Carroll and Weir, 2003).Table 2: Probability of operationsoperation probabilitys?1 1.0s?2 0.7s?7 , s?10 0.5s?5 , s?8 0.3s?4 , s?6 , s?9 0.2s?3 0.1a?1 0.3a?2 0.7nilNP 0.7nilV P 0.32.4 Parsing StrategiesIn order to improve the efficiency of the parsing, weadapt two parsing strategies as follows:?
If two partial parse trees have the same se-quence of nodes to which ITAG operations areapplicable, then the lower probability tree canbe safely discarded.?
The parser only keeps n-best partial parse trees.3 Validity of Partial Parse TreesThis section gives some definitions about the valid-ity of a partial parse tree.
Before describing the va-lidity of a partial parse tree, we define the subsump-tion relation between partial parse trees.Definition 1 (subsumption relation) Let ?
and ?be partial parse trees.
Then we write ?
?
?
, ifs?(?)
= ?
, for some initial tree ?
or a?(?)
= ?
,for some auxiliary tree ?.
Let ??
be the reflexivetransitive closure of ?.
We say that ?
subsumes ?
,if ?
??
?
.
2That ?
subsumes ?
means that ?
is the result of ap-plying a substitution or an adjunction to ?.
Figure 2shows the subsumption relation between the partialparse trees constructed for the sentence (1).If a partial parse tree for an initial fragment repre-sents a syntactic relation correctly, the partial parsetree subsumes the correct parse tree for the inputsentence.
We say that such a partial parse tree isvalid.
The validity of a partial parse tree is definedas follows:Definition 2 (valid partial parse tree) Let ?
be apartial parse tree and w1 ?
?
?wn be an input sen-tence.
We say that ?
is valid for w1 ?
?
?wn if ?
sub-sumes the correct parse tree for w1 ?
?
?wn.
2#1 I #2 #3 #6#7found a dime #10 #12 #14 #18#19#16#13in the woodsubsumption relation#4 #8#9#11#15#17#5Figure 2: Subsumption relation between partialparse trees#1 I #2 #3 #6#7found a dime #10 #12 #14 #18#19#16#13in the woodsubsumption relation#4 #8#9#11#15#17valid partial parse tree#5Figure 3: Valid partial parse treesFor example, assume that the #18 is correct parsetree for the sentence (1).
Then partial parse tree #3is valid for the sentence (1), because #3 ??
#18.
Onthe other hand, partial parse tree #4 and #5 are notvalid for (1).
Figure 3 shows the valid partial parsetrees for the sentence (1).4 Evaluating the Validity of Partial ParseTreeThe validity of a partial parse tree for an initial frag-ment depends on the rest of the sentence.
For ex-ample, the validity of the partial parse trees #3, #4and #5 depends on the remaining input that followsthe word ?found.?
This means that the validity dy-namically varies for every word input.
We define aconditional validity of partial parse tree:V (?
| w1 ?
?
?wj) =???Sub(?,w1??
?wj) P (?)??
?T (w1??
?wj) P (?
)(5)where ?
is a partial parse tree for an initial frag-ment w1 ?
?
?wi(i ?
j), T (w1 ?
?
?wj) is the set ofconstructed partial parse trees for the initial frag-ment w1 ?
?
?wj and Sub(?,w1 ?
?
?wj) is the subsetof T (w1 ?
?
?wj) whose elements are subsumed by ?.The equation (5) represents the validity of ?
on thecondition w1 ?
?
?wj .
?
is valid for input sentenceif and only if some partial parse tree for w1 ?
?
?wjsubsumed by ?
is valid.
The equation 5 is the ratioof such partial parse trees to the constructed partialparse trees.4.1 Output Partial Parse TreesKato et al proposed a method of delaying the deci-sion of which partial parse trees should be returnedas the output, until the validity of partial parse treesare guaranteed (Kato et al, 2000).
The idea ofdelaying the decision of the output is interesting.However, delaying the decision until the validity areguaranteed may cause the loss of incrementality ofthe parsing.To solve the problem, in our method, the in-cremental parser returns high validity partial parsetrees rather than validity guaranteed partial parsetrees.When the j-th word wj is scanned, our incremen-tal parser returns the following partial parse:argmax{?
:V (?,w1???wj)??}l(?)
(6)where ?
is a threshold between [0, 1] and l(?)
isthe length of the initial fragment which is yieldedby ?.
The output partial parse tree is the one forthe longest initial fragment in the partial parse treeswhose validity are greater than a threshold ?.4.2 An ExampleLet us consider a parsing example for the sentence(1).
We assume that the threshold ?
= 0.8.Let us consider when the partial parse tree#3, which is valid for (1), is returned as output.When the word ?found?
is scanned, partial parsetrees #3, #4 and #5 are constructed.
That is,T (I found) = {#3,#4,#5}.
As shown in Figure2, Sub(#3, I found) = {#3}.
Furthermore,P (#3) = 0.7, P (#4) = 0.1 and P (#5) = 0.2.Therefore, V alidity(#3, I found) =0.7/(0.7 + 0.1 + 0.2) = 0.7.
BecauseV alidity(#3, I found) < ?, partial parse tree#3 is not returned as the output at this point.
Theparser only keeps #3 as a candidate partial parsetree.When the next word ?a?
is scanned, partial parsetrees #6, #7, #8 and #9 are constructed, whereP (#6) = 0.21, P (#7) = 0.14, P (#8) = 0.03 andP (#9) = 0.02.
Sub(#3, I found a) = {#6,#7}.Therefore, V alidity(#3, I found a) = (0.21 +0.14)/(0.21+0.14+0.03+0.02) = 0.875.
BecauseV alidity(#3, I found a) ?
?, partial parse tree #3is returned as the output.Table 3 shows the output partial parse tree for ev-ery word input.Our incremental parser delays the decision of theoutput as shown in this example.Table 3: Output partial parse treesinput word output partial parse treeI #2founda #3dime #10in #12thewood #185 Experimental ResultsTo evaluate the performance of our proposedmethod, we performed a parsing experiment.
Theparser was implemented in GNU Common Lisp on aLinux PC.
In the experiment, the inputs of the incre-mental parser are POS sequences rather than wordsequences.
We used 47247 initial trees and 2931auxiliary trees for the experiment.
The elementarytrees were extracted from the parse trees in sec-tions 02-21 of the Wall Street Journal in Penn Tree-bank (Marcus et al, 1993), which is transformedby using parent-child annotation and left factoring(Roark and Johnson, 1999).
We set the beam-widthat 500.The labeled precision and recall of the parsingare 80.8% and 78.5%, respectively for the section23 in Penn Treebank.
We used the set of sentencesfor which the outputs of the incremental parser areidentical to the correct parse trees in the Penn Tree-bank.
The number of these sentences is 451.
Theaverage length of these sentences is 13.5 words.We measured the delays and the precisions for va-lidity thresholds 0.5, 0.6, 0.7, 0.8, 0.9 and 1.0.We define the degree of delay as follows: Lets = w1 ?
?
?wn be an input sentence and oj(s) bethe partial parse tree that is the output when the j-thword wj is scanned.
We define the degree of delaywhen j-th word is scanned as follows:D(j, s) = j ?
l(oj(s)) (7)We define maximum delay Dmax(s) and averagedelay Dave(s) as follows:Dmax(s) = max1?j?nD(j, s) (8)Dave(s) = 1nn?j=1D(j, s) (9)The precision is defined as the percentage of validpartial parse trees in the output.Moreover, we measured the precision of the pars-ing whose delay is always 0 and which returns theTable 4: Precisions and delaysprecision(%) Dmax Dave?
= 1.0 100.0 11.9 6.4?
= 0.9 97.3 7.5 2.9?
= 0.8 95.4 6.4 2.2?
= 0.7 92.5 5.5 1.8?
= 0.6 88.4 4.5 1.3?
= 0.5 83.0 3.4 0.9baseline 73.6 0.0 0.00246810121470 75 80 85 90 95 100delay(number of words)precision(%)Dmax3333333Dave??????
?baseline22Figure 4: Relation between precision and delaypartial parse tree having highest probability.
We callit the parsing baseline.Table 4 shows the precisions and delays.
Figure4 illustrates the relation between the precisions anddelays.The experimental result demonstrates that thereis a precision/delay trade-off.
Our proposed methodincreases the precision in comparison with the base-line, while returning the output is delayed.
When?
= 1, it is guaranteed that the output partial parsetrees are valid, that is, our method is similar to themethod in the literature (Kato et al, 2000).
In com-parison with this case, our method when ?
< 1 dra-matically decreases the delay.Although the result does not necessarily demon-strates that our method is the best one, it achievesboth high-accuracy and short-delay to a certain ex-tent.6 Concluding RemarksIn this paper, we have proposed a method of evalu-ating the validity that a partial parse tree constructedin incremental parsing becomes valid.
The methodis based on probabilistic incremental parsing.
Whena word is scanned, the method incrementally calcu-lates the validity for each partial parse tree and re-turns the partial parse tree whose validity is greaterthan a threshold.
Our method delays the decision ofwhich partial parse tree should be returned.To evaluate the performance of our method, weconducted a parsing experiment using the PennTreebank.
The experimental result shows that ourmethod improves the accuracy of incremental pars-ing.The experiment demonstrated a precision/delaytrade-off.
To evaluate overall performance of in-cremental parsing, we would like to investigate asingle measure into which delay and precision arecombined.AcknowledgementThis work is partially supported by the Grant-in-Aidfor Scientific Research of the Ministry of Education,Science, Sports and Culture, Japan (No.
15300044),and The Tatematsu Foundation.ReferencesJ.
Allen, G. Ferguson, and A. Stent.
2001.
An Ar-chitecture for More Realistic Conversational Sys-tems.
In Proceedings of International Confer-ence of Intelligent User Interfaces, pages 1?8.J.
Carroll and D. Weir.
2003.
Encoding FrequencyInformation in Stochastic Parsing Models.
InR.
Bod, R. Scha, and K. Sima?an, editors, Data-Oriented Parsing, pages 43?60.
CSLI Publica-tions, Stanford.J.
Chen and K. Vijay-Shanker.
2000.
AutomatedExtraction of TAGs from the Penn Treebank.
InProceedings of the 6th International Workshop onParsing Technologies, pages 65?76.D.
Chiang.
2003.
Statistical Parsing with an Auto-matically Extracted Tree Adjoining Grammar.
InR.
Bod, R. Scha, and K. Sima?an, editors, Data-Oriented Parsing, pages 299?316.
CSLI Publica-tions, Stanford.F.
Costa, V. Lombardo, P. Frasconi, and Soda G.2001.
Wide Coverage Incremental Parsing byLearning Attachment Preferences.
In Proceed-ings of the 7th Congress of the Italian Associationfor Artificial Intelligence, pages 297?307.N.
J. Haddock.
1987.
Incremental Interpretationand Combinatory Categorial Grammar.
In Pro-ceedings of the 10th International Joint Confer-ence on Artificial Intelligence, pages 661?663.Y.
Inagaki and S. Matsubara.
1995.
Models for In-cremental Interpretation of Natural Language.
InProceedings of the 2nd Symposium on NaturalLanguage Processing, pages 51?60.A.
K. Joshi.
1985.
Tree Adjoining Grammar: HowMuch Context-Sensitivity is required to providereasonable structural descriptions?
In D. R.Dowty, L. Karttunen, and A. Zwicky, editors,Natural Language Parsing, pages 206?250.
Cam-bridge University Press, Cambridge.Y.
Kato, S. Matsubara, K. Toyama, and Y. Ina-gaki.
2000.
Spoken Language Parsing based onIncremental Disambiguation.
In Proceedings ofthe 6th International Conference on Spoken Lan-guage Processing, volume 2, pages 999?1002.V.
Lombardo and P. Sturt.
1997.
Incremental Pro-cessing and Infinite Local Ambiguity.
In Pro-ceedings of the 19th Annual Conference of theCognitive Science Siciety, pages 448?453.M.
P. Marcus, B. Santorini, and M. A.Marcinkiewicz.
1993.
Building a Large Anno-tated Corpus of English: the Penn Treebank.Computational Linguistics, 19(2):310?330.M Marcus.
1980.
A Theory of Syntactic Recog-nition for Natural Language.
MIT Press, Cam-brige, MA.S.
Matsubara, S. Asai, K. Toyama, and Y. Inagaki.1997.
Chart-based Parsing and Transfer in In-cremental Spoken Language Translation.
In Pro-ceedings of the 4th Natural Language ProcessingPacific Rim Symposium, pages 521?524.D.
Milward and R. Cooper.
1994.
Incremental In-terpretation: Applications, Theory, and Relation-ship to Dynamic Semantics.
In Proceedings ofthe 15th International Conference on Computa-tional Linguistics, pages 748?754.D.
Milward.
1995.
Incremental Interpretation ofCategorial Grammar.
In Proceedings of the 7thConference of European Chapter of the Associ-ation for Computational Linguistics, pages 119?126.B.
Roark and M. Johnson.
1999.
Efficient Prob-abilistic Top-down and Left-corner Parsing.
InProceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics, pages421?428.B.
Roark.
2001.
Probabilistic Top-Down Parsingand Language Modeling.
Computational Lin-guistics, 27(2):249?276.F.
Xia.
1999.
Extracting Tree Adjoining Gram-mars from Bracketed corpora.
In Proceedings ofthe 5th Natural Language Processing Pacific RimSymposium, pages 398?403.
