Heterogeneous Automatic MT EvaluationThrough Non-Parametric Metric CombinationsJesu?s Gime?nez and Llu?
?s Ma`rquezTALP Research Center, LSI DepartmentUniversitat Polite`cnica de CatalunyaJordi Girona Salgado 1?3, E-08034, Barcelona{jgimenez,lluism}@lsi.upc.eduAbstractCombining different metrics into a singlemeasure of quality seems the most directand natural way to improve over the qualityof individual metrics.
Recently, several ap-proaches have been suggested (Kulesza andShieber, 2004; Liu and Gildea, 2007; Al-brecht and Hwa, 2007a).
Although basedon different assumptions, these approachesshare the common characteristic of beingparametric.
Their models involve a num-ber of parameters whose weight must beadjusted.
As an alternative, in this work,we study the behaviour of non-parametricschemes, in which metrics are combinedwithout having to adjust their relative im-portance.
Besides, rather than limiting tothe lexical dimension, we work on a wideset of metrics operating at different linguis-tic levels (e.g., lexical, syntactic and se-mantic).
Experimental results show thatnon-parametric methods are a valid meansof putting different quality dimensions to-gether, thus tracing a possible path towardsheterogeneous automatic MT evaluation.1 IntroductionAutomatic evaluation metrics have notably acceler-ated the development cycle of MT systems in thelast decade.
There exist a large number of metricsbased on different similarity criteria.
By far, themost widely used metric in recent literature is BLEU(Papineni et al, 2001).
Other well-known metricsare WER (Nie?en et al, 2000), NIST (Doddington,2002), GTM (Melamed et al, 2003), ROUGE (Linand Och, 2004a), METEOR (Banerjee and Lavie,2005), and TER (Snover et al, 2006), just to namea few.
All these metrics take into account informa-tion at the lexical level1, and, therefore, their re-liability depends very strongly on the heterogene-ity/representativity of the set of reference transla-tions available (Culy and Riehemann, 2003).
Inorder to overcome this limitation several authorshave suggested taking advantage of paraphrasingsupport (Zhou et al, 2006; Kauchak and Barzilay,2006; Owczarzak et al, 2006).
Other authors havetried to exploit information at deeper linguistic lev-els.
For instance, we may find metrics based on fullconstituent parsing (Liu and Gildea, 2005), and ondependency parsing (Liu and Gildea, 2005; Amigo?et al, 2006; Mehay and Brew, 2007; Owczarzak etal., 2007).
We may find also metrics at the levelof shallow-semantics, e.g., over semantic roles andnamed entities (Gime?nez and Ma`rquez, 2007), andat the properly semantic level, e.g., over discourserepresentations (Gime?nez, 2007).However, none of current metrics provides, in iso-lation, a global measure of quality.
Indeed, all met-rics focus on partial aspects of quality.
The mainproblem of relying on partial metrics is that we mayobtain biased evaluations, which may lead us to de-rive inaccurate conclusions.
For instance, Callison-Burch et al (2006) and Koehn and Monz (2006)have recently reported several problematic cases re-lated to the automatic evaluation of systems ori-ented towards maximizing different quality aspects.Corroborating the findings by Culy and Riehemann(2003), they showed that BLEU overrates SMT sys-tems with respect to other types of systems, such1ROUGE and METEOR may consider morphological vari-ations.
METEOR may also look up for synonyms in WordNet.319as rule-based, or human-aided.
The reason is thatSMT systems are likelier to match the sublanguage(e.g., lexical choice and order) represented by theset of reference translations.
We argue that, in orderto perform more robust, i.e., less biased, automaticMT evaluations, different quality dimensions shouldbe jointly taken into account.A natural solution to this challenge consists incombining the scores conferred by different metrics,ideally covering a heterogeneous set of quality as-pects.
In the last few years, several approaches tometric combination have been suggested (Kuleszaand Shieber, 2004; Liu and Gildea, 2007; Albrechtand Hwa, 2007a).
In spite of working on a lim-ited set of quality aspects, mostly lexical features,these approaches have provided effective means ofcombining different metrics into a single measure ofquality.
All these methods implement a parametriccombination scheme.
Their models involve a num-ber of parameters whose weight must be adjusted(see further details in Section 2).As an alternative path towards heterogeneous MTevaluation, in this work, we explore the possibilityof relying on non-parametric combination schemes,in which metrics are combined without having to ad-just their relative importance (see Section 3).
Wehave studied their ability to integrate a wide set ofmetrics operating at different linguistic levels (e.g.,lexical, syntactic and semantic) over several evalu-ation scenarios (see Section 4).
We show that non-parametric schemes offer a valid means of puttingdifferent quality dimensions together, effectivelyyielding a significantly improved evaluation quality,both in terms of human likeness and human accept-ability.
We have also verified that these methods portwell across test beds.2 Related WorkApproaches to metric combination require two im-portant ingredients:Combination Scheme, i.e., how to combine sev-eral metric scores into a single score.
Aspointed out in Section 1, we distinguish be-tween parametric and non-parametric schemes.Meta-Evaluation Criterion, i.e., how to evaluatethe quality of a metric combination.
The twomost prominent meta-evaluation criteria are:?
Human Acceptability: Metrics are evalu-ated in terms of their ability to capture thedegree of acceptability to humans of auto-matic translations, i.e., their ability to em-ulate human assessors.
The underlying as-sumption is that ?good?
translations shouldbe acceptable to human evaluators.
Hu-man acceptability is usually measured onthe basis of correlation between automaticmetric scores and human assessments oftranslation quality2.?
Human Likeness: Metrics are evaluated interms of their ability to capture the fea-tures which distinguish human from au-tomatic translations.
The underlying as-sumption is that ?good?
translations shouldresemble human translations.
Humanlikeness is usually measured on the basisof discriminative power (Lin and Och,2004b; Amigo?
et al, 2005).In the following, we describe the most relevantapproaches to metric combination suggested in re-cent literature.
All are parametric, and most of themare based on machine learning techniques.
We dis-tinguish between approaches relying on human like-ness and approaches relying on human acceptability.2.1 Approaches based on Human LikenessThe first approach to metric combination basedon human likeness was that by Corston-Oliver etal.
(2001) who used decision trees to distinguishbetween human-generated (?good?)
and machine-generated (?bad?)
translations.
They focused onevaluating only the well-formedness of automatictranslations (i.e., subaspects of fluency), obtaininghigh levels of classification accuracy.Kulesza and Shieber (2004) extended the ap-proach by Corston-Oliver et al (2001) to take intoaccount other aspects of quality further than fluencyalone.
Instead of decision trees, they trained SupportVector Machine (SVM) classifiers.
They used fea-tures inspired by well-known metrics such as BLEU,NIST, WER, and PER.
Metric quality was evaluatedboth in terms of classification accuracy and correla-tion with human assessments at the sentence level.2Usually adequacy, fluency, or a combination of the two.320A significant improvement with respect to standardindividual metrics was reported.Gamon et al (2005) presented a similar approachwhich, in addition, had the interesting property thatthe set of human and automatic translations couldbe independent, i.e., human translations were not re-quired to correspond, as references, to the set of au-tomatic translations.2.2 Approaches based on Human AcceptabilityQuirk (2004) applied supervised machine learningalgorithms (e.g., perceptrons, SVMs, decision trees,and linear regression) to approximate human qualityjudgements instead of distinguishing between hu-man and automatic translations.
Similarly to thework by Gamon et al (2005) their approach doesnot require human references.More recently, Albrecht and Hwa (2007a; 2007b)re-examined the SVM classification approach byKulesza and Shieber (2004) and, inspired by thework of Quirk (2004), suggested a regression-basedlearning approach to metric combination, with andwithout human references.
The regression modellearns a continuous function that approximates hu-man assessments in training examples.As an alternative to methods based on machinelearning techniques, Liu and Gildea (2007) sug-gested a simpler approach based on linear combina-tions of metrics.
They followed a Maximum Corre-lation Training, i.e., the weight for the contributionof each metric to the overall score was adjusted soas to maximize the level of correlation with humanassessments at the sentence level.As expected, all approaches based on human ac-ceptability have been shown to outperform that ofKulesza and Shieber (2004) in terms of human ac-ceptability.
However, no results in terms of humanlikeness have been provided, thus leaving these com-parative studies incomplete.3 Non-Parametric Combination SchemesIn this section, we provide a brief description of theQARLA framework (Amigo?
et al, 2005), which is,to our knowledge, the only existing non-parametricapproach to metric combination.
QARLA is non-parametric because, rather than assigning a weightto the contribution of each metric, the evaluation ofa given automatic output a is addressed through aset of independent probabilistic tests (one per met-ric) in which the goal is to falsify the hypothesis thata is a human reference.
The input for QARLA is aset of test cases A (i.e., automatic translations), a setof similarity metrics X, and a set of models R (i.e.,human references) for each test case.
With such atestbed, QARLA provides the two essential ingredi-ents required for metric combination:Combination Scheme Metrics are combined insidethe QUEEN measure.
QUEEN operates underthe unanimity principle, i.e., the assumptionthat a ?good?
translation must be similar toall human references according to all metrics.QUEENX(a) is defined as the probability, overR ?
R ?
R, that, for every metric in X, theautomatic translation a is more similar to a hu-man reference r than two other references, r?and r?
?, to each other.
Formally:QUEENX,R(a) = Prob(?x ?
X : x(a, r) ?
x(r?, r??
))where x(a, r) stands for the similarity betweena and r according to the metric x. Thus,QUEEN allows us to combine different similar-ity metrics into a single measure, without hav-ing to adjust their relative importance.
Besides,QUEEN offers two other important advantageswhich make it really suitable for metric com-bination: (i) it is robust against metric redun-dancy, i.e., metrics covering similar aspects ofquality, and (ii) it is not affected by the scaleproperties of metrics.
The main drawback ofthe QUEEN measure is that it requires at leastthree human references, when in most casesonly a single reference translation is available.Meta-evaluation Criterion Metric quality is eval-uated using the KING measure of human like-ness.
All human references are assumed to beequally optimal and, while they are likely tobe different, the best similarity metric is theone that identifies and uses the features thatare common to all human references, group-ing them and separating them from automatictranslations.
Based on QUEEN, KING repre-sents the probability that a human reference321does not receive a lower score than the score at-tained by any automatic translation.
Formally:KINGA,R(X) = Prob(?a ?
A : QUEENX,R?
{r}(r) ?QUEENX,R?
{r}(a))KING operates, therefore, on the basis of dis-criminative power.
The closest measure toKING is ORANGE (Lin and Och, 2004b), whichis, however, not intended for the purpose ofmetric combination.Apart from being non-parametric, QARLA ex-hibits another important feature which differentiatesit form other approaches; besides considering thesimilarity between automatic translations and hu-man references, QARLA also takes into account thedistribution of similarities among human references.However, QARLA is not well suited to port fromhuman likeness to human acceptability.
The reasonis that QUEEN is, by definition, a very restrictivemeasure ?a ?good?
translation must be similar toall human references according to all metrics.
Thus,as the number of metrics increases, it becomes eas-ier to find a metric which does not satisfy the QUEENassumption.
This causes QUEEN values to get closeto zero, which turns correlation with human assess-ments into an impractical meta-evaluation measure.We have simulated a non-parametric schemebased on human acceptability by working on uni-formly averaged linear combinations (ULC) of met-rics.
Our approach is similar to that of Liu andGildea (2007) except that in our case all the metricsin the combination are equally important3.
In otherwords, ULC is indeed a particular case of a paramet-ric scheme, in which the contribution of each metricis not adjusted.
Formally:ULCX(a,R) =1|X|?x?Xx(a,R)where X is the metric set, and x(a,R) is the simi-larity between the automatic translation a and the setof references R, for the given test case, according tothe metric x.
Since correlation with human assess-ments at the system level is vaguely informative (itis often estimated on very few system samples), we3That would be assuming that all metrics operate in the samerange of values, which is not always the case.AE04 CE04 AE05 CE05#human references 5 5 5 4#system outputs 5 10 7 10#outputsassessed 5 10 6 5#sentences 1,353 1,788 1,056 1,082#sentencesassessed 347 447 266 272Table 1: Description of the test bedsevaluate metric quality in terms of correlation withhuman assessments at the sentence level (Rsnt).
Weuse the sum of adequacy and fluency to simulate aglobal assessment of quality.4 Experimental WorkIn this section, we study the behavior of the twocombination schemes presented in Section 3 in thecontext of four different evaluation scenarios.4.1 Experimental SettingsWe use the test beds from the 2004 and 2005NIST MT Evaluation Campaigns (Le and Przy-bocki, 2005)4.
Both campaigns include two differ-ent translations exercises: Arabic-to-English (?AE?
)and Chinese-to-English (?CE?).
Human assessmentsof adequacy and fluency are available for a subsetof sentences, each evaluated by two different humanjudges.
See, in Table 1, a brief numerical descrip-tion including the number of human references andsystem outputs available, as well as the number ofsentences per output, and the number of system out-puts and sentences per system assessed.For metric computation, we have used the IQMTv2.1, which includes metrics at different linguisticlevels (lexical, shallow-syntactic, syntactic, shallow-semantic, and semantic).
A detailed description maybe found in (Gime?nez, 2007)5.4.2 Evaluating Individual MetricsPrior to studying the effects of metric combination,we study the isolated behaviour of individual met-rics.
We have selected a set of metric representa-tives from each linguistic level.
Table 2 shows meta-evaluation results for the test beds described in Sec-tion 4.1, according both to human likeness (KING)4http://www.nist.gov/speech/tests/summaries/2005/mt05.htm5The IQMT Framework may be freely downloaded fromhttp://www.lsi.upc.edu/?nlp/IQMT.322KING RsntLevel Metric AE04 CE04 AE05 CE05 AE04 CE04 AE05 CE051-WER 0.70 0.51 0.48 0.61 0.53 0.47 0.38 0.471-PER 0.64 0.43 0.45 0.58 0.50 0.51 0.29 0.401-TER 0.73 0.54 0.53 0.66 0.54 0.50 0.38 0.49BLEU 0.70 0.49 0.52 0.59 0.50 0.46 0.36 0.39NIST 0.74 0.53 0.55 0.68 0.53 0.55 0.37 0.46Lexical GTM.e1 0.67 0.49 0.48 0.61 0.41 0.50 0.26 0.29GTM.e2 0.69 0.52 0.51 0.64 0.49 0.54 0.43 0.48ROUGEL 0.73 0.59 0.49 0.65 0.58 0.60 0.41 0.52ROUGEW 0.75 0.62 0.54 0.68 0.59 0.57 0.48 0.54METEORwnsyn 0.75 0.56 0.57 0.69 0.56 0.56 0.35 0.41SP-Op-* 0.66 0.48 0.49 0.59 0.51 0.57 0.38 0.41SP-Oc-* 0.65 0.44 0.46 0.59 0.55 0.58 0.42 0.41Shallow SP-NISTl 0.73 0.51 0.55 0.66 0.53 0.54 0.38 0.44Syntactic SP-NISTp 0.79 0.60 0.56 0.70 0.46 0.49 0.37 0.39SP-NISTiob 0.69 0.48 0.49 0.59 0.32 0.36 0.27 0.26SP-NISTc 0.60 0.42 0.39 0.52 0.26 0.27 0.16 0.16DP-HWCw 0.58 0.40 0.42 0.53 0.41 0.08 0.35 0.40DP-HWCc 0.50 0.32 0.33 0.41 0.41 0.17 0.38 0.32DP-HWCr 0.56 0.40 0.37 0.46 0.42 0.16 0.39 0.43DP-Ol-* 0.58 0.48 0.41 0.52 0.52 0.48 0.36 0.37Syntactic DP-Oc-* 0.65 0.45 0.44 0.55 0.49 0.51 0.43 0.41DP-Or-* 0.71 0.57 0.54 0.64 0.55 0.55 0.50 0.50CP-Op-* 0.67 0.47 0.47 0.60 0.53 0.57 0.38 0.46CP-Oc-* 0.66 0.51 0.49 0.62 0.57 0.59 0.45 0.50CP-STM 0.64 0.42 0.43 0.58 0.39 0.13 0.34 0.30NE-Oe-** 0.65 0.45 0.46 0.57 0.47 0.56 0.32 0.39Shallow SR-Or-* 0.48 0.22 0.34 0.41 0.28 0.10 0.32 0.21Semantic SR-Orv 0.36 0.13 0.24 0.27 0.27 0.12 0.25 0.24DR-Or-* 0.62 0.47 0.50 0.55 0.47 0.46 0.43 0.37Semantic DR-Orp-* 0.58 0.42 0.43 0.50 0.37 0.35 0.36 0.26Optimal Combination 0.79 0.64 0.61 0.70 0.64 0.63 0.54 0.61Table 2: Metric Meta-evaluationand human acceptability (Rsnt), computed over thesubsets of sentences for which human assessmentsare available.The first observation is that the two meta-evaluation criteria provide very similar metric qual-ity rankings for a same test bed.
This seems to in-dicate that there is a relationship between the twometa-evaluation criteria employed.
We have con-firmed this intuition by computing the Pearson cor-relation coefficient between values in columns 1 to4 and their counterparts in columns 5 to 8.
Thereexists a high correlation (R = 0.79).A second observation is that metric quality variessignificantly from task to task.
This is due to the sig-nificant differences among the test beds employed.These are related to three main aspects: languagepair, translation domain, and system typology.
Forinstance, notice that most metrics exhibit a lowerquality in the case of the ?AE05?
test bed.
The reasonis that, while in the rest of test beds all systems arestatistical, the ?AE05?
test bed presents the particu-larity of providing automatic translations producedby heterogeneous MT systems (i.e., systems belong-ing to different paradigms)6.
The fact that most sys-tems are statistical also explains why, in general,lexical metrics exhibit a higher quality.
However,highest levels of quality are not in all cases attainedby metrics at the lexical level (see highlighted val-ues).
In fact, there is only one metric, ?ROUGEW ?
(based on lexical matching), which is consistentlyamong the top-scoring in all test beds according toboth meta-evaluation criteria.
The underlying causeis simple: current metrics do not provide a globalmeasure of quality, but account only for partial as-pects of it.
Apart from evincing the importance ofthe meta-evaluation process, these results stronglysuggest the need for conducting heterogeneous MTevaluations.6Specifically, all systems are statistical except one which ishuman-aided.323Opt.K(AE.04) = {SP-NISTp}Opt.K(CE.04) = {ROUGEW , SP-NISTp, ROUGEL}Opt.K(AE.05) = {METEORwnsyn, SP-NISTp, DP-Or-*}Opt.K(CE.05) = {SP-NISTp}Opt.R(AE.04) = {ROUGEW ,ROUGEL,CP-Oc-*,METEORwnsyn,DP-Or-*,DP-Ol-*,GTM.e2,DR-Or-*,CP-STM}Opt.R(CE.04) = {ROUGEL,CP-Oc-*,ROUGEW , SP-Op-*,METEORwnsyn,DP-Or-*,GTM.e2, 1-WER,DR-Or-*}Opt.R(AE.05) = {DP-Or-*,ROUGEW }Opt.R(CE.05) = {ROUGEW ,ROUGEL,DP-Or-*,CP-Oc-*, 1-TER,GTM.e2,DP-HWCr,CP-STM}Table 3: Optimal metric sets4.3 Finding Optimal Metric CombinationsIn that respect, we study the applicability of the twocombination strategies presented.
Optimal metricsets are determined by maximizing over the corre-sponding meta-evaluation measure (KING or Rsnt).However, because exploring all possible combina-tions was not viable, we have used a simple algo-rithm which performs an approximate search.
First,individual metrics are ranked according to theirquality.
Then, following that order, metrics areadded to the optimal set only if in doing so the globalquality increases.
Since no training is required it hasnot been necessary to keep a held-out portion of thedata for test (see Section 4.4 for further discussion).Optimal metric sets are displayed in Table 3.
In-side each set, metrics are sorted in decreasing qualityorder.
The ?Optimal Combination?
line in Table 2shows the quality attained by these sets, combinedunder QUEEN in the case of KING optimization, andunder ULC in the case of optimizing over Rsnt.
Inmost cases optimal sets consist of metrics operat-ing at different linguistic levels, mostly at the lexicaland syntactic levels.
This is coherent with the find-ings in Section 4.2.
Metrics at the semantic levelare selected only in two cases, corresponding to theRsnt optimization in ?AE04?
and ?CE04?
test beds.Also in two cases, corresponding to the KING opti-mization in ?AE04?
and ?CE05?
test beds, it has notbeen possible to find any metric combination whichoutperforms the best individual metric.
This is nota discouraging result.
After all, in these cases, thebest metric alone achieves already a very high qual-ity (0.79 and 0.70, respectively).
The fact that a sin-gle feature suffices to discern between manual andautomatic translations indicates that MT systems areeasily distinguishable, possibly because of their lowquality and/or because they are all based on the sametranslation paradigm.4.4 PortabilityIt can be argued that metric set optimization is itselfa training process; each metric would have an asso-ciated binary parameter controlling whether it is se-lected or not.
For that reason, in Table 4, we haveanalyzed the portability of optimal metric sets (i)across test beds and (ii) across combination strate-gies.
As to portability across test beds (i.e., acrosslanguage pairs and years), the reader must focuson the cells for which the meta-evaluation criterionguiding the metric set optimization matches the cri-terion used in the evaluation, i.e., the top-left andbottom-right 16-cell quadrangles.
The fact that the4 values in each subcolumn are in a very similarrange confirms that optimal metric sets port wellacross test beds.
We have also studied the portabil-ity of optimal metric sets across combination strate-gies.
In other words, although QUEEN and ULCare thought to operate on metric combinations re-spectively optimized on the basis of human likenessand human acceptability, we have studied the effectsof applying either measure over metric combina-tions optimized on the basis of the alternative meta-evaluation criterion.
In this case, the reader mustcompare top-left vs. bottom-left (KING) and top-right vs. bottom-right (Rsnt) 16-cell quadrangles.
Itcan be clearly seen that optimal metric sets, in gen-eral, do not port well across meta-evaluation criteria,particularly from human likeness to human accept-ability.
However, interestingly, in the case of ?AE05?
(i.e., heterogeneous systems), the optimal metric setports well from human acceptability to human like-ness.
We speculate that system heterogeneity hascontributed positively for the sake of robustness.5 ConclusionsAs an alternative to current parametric combinationtechniques, we have presented two different meth-324Metric KING RsntSet AE04 CE04 AE05 CE05 AE04 CE04 AE05 CE05Opt.K(AE.04) 0.79 0.60 0.56 0.70 0.46 0.49 0.37 0.39Opt.K(CE.04) 0.78 0.64 0.57 0.67 0.49 0.51 0.39 0.43Opt.K(AE.05) 0.74 0.63 0.61 0.66 0.48 0.51 0.39 0.42Opt.K(CE.05) 0.79 0.60 0.56 0.70 0.46 0.49 0.37 0.39Opt.R(AE.04) 0.62 0.56 0.52 0.49 0.64 0.61 0.53 0.58Opt.R(CE.04) 0.68 0.59 0.55 0.56 0.63 0.63 0.51 0.57Opt.R(AE.05) 0.75 0.64 0.59 0.69 0.62 0.60 0.54 0.57Opt.R(CE.05) 0.64 0.56 0.51 0.52 0.63 0.57 0.53 0.61Table 4: Portability of combination strategiesods: a genuine non-parametric method based on hu-man likeness, and a parametric method based humanacceptability in which the parameter weights are setequiprobable.
We have shown that both strategiesmay yield a significantly improved quality by com-bining metrics at different linguistic levels.
Besides,we have shown that these methods generalize wellacross test beds.
Thus, a valid path towards hetero-geneous automatic MT evaluation has been traced.We strongly believe that future MT evaluation cam-paigns should benefit from these results specially forthe purpose of comparing systems based on differentparadigms.
These techniques could also be used tobuild better MT systems by allowing system devel-opers to perform more accurate error analyses andless biased adjustments of system parameters.As an additional result, we have found that thereis a tight relationship between human acceptabilityand human likeness.
This result, coherent with thefindings by Amigo?
et al (2006), suggests that thetwo criteria are interchangeable.
This would be apoint in favour of combination schemes based on hu-man likeness, since human assessments ?which areexpensive to acquire, subjective and not reusable?are not required.
We also interpret this result as anindication that human assessors probably behave inmany cases in a discriminative manner.
For each testcase, assessors would inspect the source sentenceand the set of human references trying to identifythe features which ?good?
translations should com-ply with, for instance regarding adequacy and flu-ency.
Then, they would evaluate automatic transla-tions roughly according to the number and relevanceof the features they share and the ones they do not.For future work, we plan to study the inte-gration of finer features as well as to conduct arigorous comparison between parametric and non-parametric combination schemes.
This may involvereproducing the works by Kulesza and Shieber(2004) and Albrecht and Hwa (2007a).
This wouldalso allow us to evaluate their approaches in terms ofboth human likeness and human acceptability, andnot only on the latter criterion as they have beenevaluated so far.AcknowledgementsThis research has been funded by the Spanish Min-istry of Education and Science, project OpenMT(TIN2006-15307-C03-02).
Our NLP group hasbeen recognized as a Quality Research Group (2005SGR-00130) by DURSI, the Research Departmentof the Catalan Government.
We are thankful to En-rique Amigo?, for his generous help and valuablecomments.
We are also grateful to the NIST MTEvaluation Campaign organizers, and participantswho agreed to share their system outputs and humanassessments for the purpose of this research.ReferencesJoshua Albrecht and Rebecca Hwa.
2007a.
A Re-examination of Machine Learning Approaches forSentence-Level MT Evaluation.
In Proceedings ofACL, pages 880?887.Joshua Albrecht and Rebecca Hwa.
2007b.
Regressionfor Sentence-Level MT Evaluation with Pseudo Refer-ences.
In Proceedings of ACL, pages 296?303.Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, and Fe-lisa Verdejo.
2005.
QARLA: a Framework for theEvaluation of Automatic Sumarization.
In Proceed-ings of the 43th Annual Meeting of the Association forComputational Linguistics.Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??sMa`rquez.
2006.
MT Evaluation: Human-Like vs. Hu-man Acceptable.
In Proceedings of COLING-ACL06.325Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
In Pro-ceedings of ACL Workshop on Intrinsic and ExtrinsicEvaluation Measures for Machine Translation and/orSummarization.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the Role of BLEU in Ma-chine Translation Research.
In Proceedings of EACL.Simon Corston-Oliver, Michael Gamon, and ChrisBrockett.
2001.
A Machine Learning Approach tothe Automatic Evaluation of Machine Translation.
InProceedings of ACL, pages 140?147.Christopher Culy and Susanne Z. Riehemann.
2003.
TheLimits of N-gram Translation Evaluation Metrics.
InProceedings of MT-SUMMIT IX, pages 1?8.George Doddington.
2002.
Automatic Evaluationof Machine Translation Quality Using N-gram Co-Occurrence Statistics.
In Proceedings of the 2nd IHLT.Michael Gamon, Anthony Aue, and Martine Smets.2005.
Sentence-Level MT evaluation without refer-ence translations: beyond language modeling.
In Pro-ceedings of EAMT.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2007.
LinguisticFeatures for Automatic Evaluation of HeterogeneousMT Systems.
In Proceedings of the ACL Workshop onStatistical Machine Translation.Jesu?s Gime?nez.
2007.
IQMT v 2.1.
TechnicalManual.
Technical report, TALP Research Center.LSI Department.
http://www.lsi.upc.edu/?nlp/IQMT/-IQMT.v2.1.pdf.David Kauchak and Regina Barzilay.
2006.
Paraphras-ing for Automatic Evaluation.
In Proceedings of NLH-NAACL.Philipp Koehn and Christof Monz.
2006.
Manual andAutomatic Evaluation of Machine Translation betweenEuropean Languages.
In Proceedings of the Workshopon Statistical Machine Translation, pages 102?121.Alex Kulesza and Stuart M. Shieber.
2004.
A learningapproach to improving sentence-level MT evaluation.In Proceedings of the 10th International Conferenceon Theoretical and Methodological Issues in MachineTranslation.Audrey Le and Mark Przybocki.
2005.
NIST 2005 ma-chine translation evaluation official results.
Technicalreport, NIST, August.Chin-Yew Lin and Franz Josef Och.
2004a.
Auto-matic Evaluation of Machine Translation Quality Us-ing Longest Common Subsequence and Skip-BigramStatics.
In Proceedings of ACL.Chin-Yew Lin and Franz Josef Och.
2004b.
ORANGE: aMethod for Evaluating Automatic Evaluation Metricsfor Machine Translation.
In Proceedings of COLING.Ding Liu and Daniel Gildea.
2005.
Syntactic Featuresfor Evaluation of Machine Translation.
In Proceed-ings of ACL Workshop on Intrinsic and Extrinsic Eval-uation Measures for Machine Translation and/or Sum-marization.Ding Liu and Daniel Gildea.
2007.
Source-LanguageFeatures and Maximum Correlation Training for Ma-chine Translation Evaluation.
In Proceedings of the2007 Meeting of the North American chapter of the As-sociation for Computational Linguistics (NAACL-07).Dennis Mehay and Chris Brew.
2007.
BLEUATRE:Flattening Syntactic Dependencies for MT Evaluation.In Proceedings of the 11th Conference on Theoreti-cal and Methodological Issues in Machine Translation(TMI).I.
Dan Melamed, Ryan Green, and Joseph P. Turian.2003.
Precision and Recall of Machine Translation.In Proceedings of HLT/NAACL.Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Her-mann Ney.
2000.
Evaluation Tool for Machine Trans-lation: Fast Evaluation for MT Research.
In Proceed-ings of the 2nd LREC.Karolina Owczarzak, Declan Groves, Josef Van Gen-abith, and Andy Way.
2006.
Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation.
InProceedings of the 7th Conference of the Associa-tion for Machine Translation in the Americas (AMTA),pages 148?155.Karolina Owczarzak, Josef van Genabith, and AndyWay.
2007.
Dependency-Based Automatic Evalua-tion for Machine Translation.
In Proceedings of SSST,NAACL-HLT/AMTA Workshop on Syntax and Struc-ture in Statistical Translation.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic evalu-ation of machine translation, RC22176, IBM.
Techni-cal report, IBM T.J. Watson Research Center.Chris Quirk.
2004.
Training a Sentence-Level Ma-chine Translation Confidence Metric.
In Proceedingsof LREC.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, , and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human Anno-tation.
In Proceedings of AMTA, pages 223?231.Liang Zhou, Chin-Yew Lin, and Eduard Hovy.
2006.Re-evaluating Machine Translation Results with Para-phrase Support.
In Proceedings of EMNLP.326
