Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1692?1701,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsSOLAR: Scalable Online Learning Algorithms for RankingJialei Wang1, Ji Wan2,3, Yongdong Zhang2, Steven C. H. Hoi3?1Department of Computer Science, The University of Chicago, USA2Key Laboratory of Intelligent Information Processing, ICT, CAS, China3School of Information Systems, Singapore Management University, Singaporejialei@cs.uchicago.edu, {wanji,zhyd}@ict.ac.cn, chhoi@smu.edu.sgAbstractTraditional learning to rank methods learnranking models from training data in abatch and offline learning mode, whichsuffers from some critical limitations, e.g.,poor scalability as the model has to be re-trained from scratch whenever new train-ing data arrives.
This is clearly non-scalable for many real applications inpractice where training data often arrivessequentially and frequently.
To overcomethe limitations, this paper presents SO-LAR ?
a new framework of Scalable On-line Learning Algorithms for Ranking, totackle the challenge of scalable learning torank.
Specifically, we propose two novelSOLAR algorithms and analyze their IRmeasure bounds theoretically.
We conductextensive empirical studies by comparingour SOLAR algorithms with conventionallearning to rank algorithms on benchmarktestbeds, in which promising results vali-date the efficacy and scalability of the pro-posed novel SOLAR algorithms.1 IntroductionLearning to rank [27, 8, 29, 31, 7] aims to learnsome ranking model from training data using ma-chine learning methods, which has been activelystudied in information retrieval (IR).
Specifically,consider a document retrieval task, given a query,a ranking model assigns a relevance score to eachdocument in a collection of documents, and thenranks the documents in decreasing order of rele-vance scores.
The goal of learning to rank is tobuild a ranking model from training data of a set ofqueries by optimizing some IR performance mea-sures using machine learning techniques.
In lit-erature, various learning to rank techniques have?The corresponding author.
This work was done whenthe first two authors visited Dr Hoi?s group.been proposed, ranging from early pointwise ap-proaches [15, 30, 28], to popular pairwise [26, 18,3], and recent listwise approaches [5, 38].
Learn-ing to rank has many applications, including doc-ument retrieval, collaborative filtering, online ad,answer ranking for online QA in NLP [33], etc.Most existing learning to rank techniques fol-low batch and offline machine learning methodol-ogy, which typically assumes all training data areavailable prior to the learning task and the rank-ing model is trained by applying some batch learn-ing method, e.g., neural networks [3] or SVM [4].Despite being studied extensively, the batch learn-ing to rank methodology has some critical limi-tations.
One of serious limitations perhaps is itspoor scalability for real-world web applications,where the ranking model has to be re-trained fromscratch whenever new training data arrives.
Thisis apparently inefficient and non-scalable sincetraining data often arrives sequentially and fre-quently in many real applications [33, 7].
Besides,batch learning to rank methodology also suffersfrom slow adaption to fast-changing environmentof web applications due to the static ranking mod-els pre-trained from historical batch training data.To overcome the above limitations, this paperinvestigates SOLAR ?
a new framework of Scal-able Online Learning Algorithms for Ranking,which aims to learn a ranking model from a se-quence of training data in an online learning fash-ion.
Specifically, by following the pairwise learn-ing to rank framework, we formally formulate thelearning problem, and then present two differentSOLAR algorithms to solve the challenging tasktogether with the analysis of their theoretical prop-erties.
We conduct an extensive set of experi-ments by evaluating the performance of the pro-posed algorithms under different settings by com-paring them with both online and batch algorithmson benchmark testbeds in literature.As a summary, the key contributions of this pa-1692per include: (i) we present a new framework ofScalable Online Learning Algorithms for Rank-ing, which tackles the pairwise learning to rank-ing problem via a scalable online learning ap-proach; (ii) we present two SOLAR algorithms:a first-order learning algorithm (SOLAR-I) and asecond-order learning algorithm (SOLAR-II); (iii)we analyze the theoretical bounds of the proposedalgorithms in terms of standard IR performancemeasures; and (iv) finally we examine the efficacyof the proposed algorithms by an extensive set ofempirical studies on benchmark datasets.The rest of this paper is organized as follows.Section 2 reviews related work.
Section 3 givesproblem formulations of the proposed frameworkand presents our algorithms, followed by theoret-ical analysis in Section 4.
Section 5 presents ourexperimental results, and Section 6 concludes thiswork and indicates future directions.2 Related WorkIn general, our work is related to two topics in in-formation retrieval and machine learning: learn-ing to rank and online learning.
Both of them havebeen extensively studied in literature.
Below webriefly review important related work in each area.2.1 Learning to RankMost of the existing approaches to learning to rankcan be generally grouped into three major cate-gories: (i) pointwise approaches, (ii) pairwise ap-proaches, and (iii) listwise approaches.The pointwise approaches treat ranking as aclassification or regression problem for predictingthe ranking of individual objects.
For example,[12, 19] formulated ranking as a regression prob-lem in diverse forms.
[30] formulated ranking abinary classification of relevance on document ob-jects, and solved it by discriminative models (e.g.,SVM).
In [15], Perceptron [32] ranking (known as?Prank?)
[15] formulated it as online binary clas-sification.
[28] cast ranking as multiple classifica-tion or multiple ordinal classification tasks.The pairwise approaches treat the documentpairs as training instances and formulate rankingas a classification or regression problem from acollection of pairwise document instances.
Ex-ample of pairwise learning to rank algorithmsinclude: neural network approaches such asRankNet [3] and LambdaRank [2], SVM ap-proaches such as RankSVM [26], boosting ap-proaches such as RankBoost [18], regression al-gorithms such as GBRank [43], and probabilisticranking algorithms such as FRank [35].
The pair-wise group is among one of widely and success-fully applied approaches.
Our work generally be-longs to this group.The listwise approaches treat a list of docu-ments for a query as a training instance and at-tempt to learn a ranking model by optimizingsome loss defined on the predicted list and theground-truth list.
In general, there are two typesof approaches.
The first is to directly optimizesome IR metrics, such as Mean Average Pre-cision (MAP) and Normalized Discounted Cu-mulative Gain (NDCG) [25].
Examples includeAdaRank by boosting [39], SVM-MAP by op-timizing MAP [42], PermuRank [40], and Sof-tRank [34] based on a smoothed approxima-tion to NDCG, and NDCG-Boost by optimizingNDCG [37], etc.
The other is to indirectly opti-mize the IR metrics by defining some listwise lossfunction, such as ListNet [5] and ListMLE [38].Despite being studied actively, most existingworks generally belong to batch learning meth-ods, except a few online learning studies.
Forexample, Prank [15] is probably the first onlinepointwise learning to ranking algorithm.
UnlikePrank, our work focuses online pairwise learningto rank technique, which significantly outperformsPrank as observed in our empirical studies.
Be-sides, our work is also related to another existingwork in [10], but differs considerably in several as-pects: (i) they assume the similarity function is de-fined in a bi-linear form which is inappropriate fordocument retrieval applications; (ii) their trainingdata is given in the form of triplet-image instance(p1, p2, p3), while our training data is given ina pairwise query-document instance (qt, d1t, d2t);(iii) they only apply first order online learning al-gorithms, while we explore both first-order andsecond-order online algorithms.
Finally, we notethat our work differs from another series of on-line learning to rank studies [21, 22, 23, 36, 41]which attempt to explore reinforcement learningor multi-arm bandit techniques for learning to rankfrom implicit/partial feedback, whose formulationand settings are very different.2.2 Online LearningOur work is closely related to studies of onlinelearning [24], representing a family of efficient1693and scalable machine learning algorithms.
In liter-ature, a variety of online algorithms have been pro-posed, mainly in two major categories: first-orderalgorithms and second-order algorithms.
The no-table examples of first-order online learning meth-ods include classical Perceptron [32], and Passive-Aggressive (PA) learning algorithms [13].
Unlikefirst-order algorithms, second-order online learn-ing [6], e.g., Confidence-Weighted (CW) learn-ing [16], usually assumes the weight vector fol-lows a Gaussian distribution and attempts to up-date the mean and covariance for each receivedinstance.
In addition, Adaptive Regularizationof Weights Learning (AROW) [14] was proposedto improve robustness of CW.
More other on-line learning methods can be found in [24].
Inthis work, we apply both first-order and second-order online learning methods for online learningto rank.3 SOLAR ?
Online Learning to RankWe now present SOLAR ?
a framework of Scal-able Online Learning Algorithms for Ranking,which applies online learning to build rankingmodels from sequential training instances.3.1 Problem FormulationWithout loss of generality, consider an onlinelearning to rank problem for document retrieval,where training data instances arrive sequentially.Let us denote byQ a query space and denote byDa document space.
Each instance received at timestep t is represented by a triplet (q(i)t, d(1)t, d(2)t),where q(i)t?
Q denotes the i-th query in the entirecollection of queries Q, d(1)t?
D and d(2)t?
Ddenote a pair of documents for prediction of rank-ing w.r.t.
the query q(i)t. Without loss of clarity,for the rest of this paper, we simplify the notationq(i)t, d(1)t, d(2)tas qit, d1t, d2t, respectively.We also denote by yt?
{+1,?1} the trueranking order of the pairwise instances at step tsuch that if yt= +1, document d1tis ranked be-fore d2t; otherwise d1tis ranked after d2t.
We in-troduce a mapping function ?
: Q ?
D ?
Rnthat creates a n-dimensional feature vector froma query-document pair.
For example, consider?
(q, d) ?
Rn, one way to extract one of the nfeatures is based on term frequency, which countsthe number of times the query term of q occursin document d. We also introduce wt?
Rnasthe ranking model to be learned at step t, which isused to form the target ranking function below:f(qit, d1t, d2t) = w>t?
(qit, d1t, d2t) = wt>(?
(qit, d1t)?
?
(qit, d2t))Assume that we have a total of Q queries{q(i)}Qi=1, each of which is associated with a totalof Didocuments and a total of Titraining tripletinstances.
In a practical document retrieval task,the online learning to rank framework operates inthe following procedure:(i) Given a query q1, an initial model w1is firstapplied to rank the set of documents for thequery, which are then returned to users;(ii) We then collect user?s feedback (e.g., click-through data) as the ground truth labels forthe ranking orders of a collection of T1triplettraining instances;(iii) We then apply an online learning algorithm toupdate the ranking model from the sequenceof T1triplet training instances;(iv) We repeat the above by applying the updatedranking model to process the next query.For a sequence of T triplet training instances,the goal of online learning to rank is to optimizethe sequence of ranking models w1, .
.
.
,wTdur-ing the entire online learning process.
In gen-eral, the proposed online learning to rank schemeis evaluated by measuring the online cumulativeMAP [1] or online cumulative NDCG [25].
Letus denote by NDCGiand MAPithe NDCG andMAP values for query qi, respectively, which aredefined as follows:NDCGi=1NnDi?r=1G(l(pif(r)))D(r) (1)MAPi=1m?s:l(pif(s))=1?j?sI{l(pif(j))=1}s(2)where I{?
}is an indicator function that out-puts 1 when the statement is true and 0 other-wise; G(K) = 2K?
1,D(K) =1log2(1+K),Nn= maxpi?mr=1G(l(pi(r)))D(r), l(r) is thecorresponding labels as K-level ratings, pifdenotea rank list produced by ranking function f , m isthe number of relevant documents.
The online cu-mulative IR measure is defined as the average ofthe measure over a sequence of Q queries:NDCG =1QQ?i=1NDCGiMAP =1QQ?i=1MAPi(3)16943.2 First-order SOLAR AlgorithmThe key challenge of online learning to rank ishow to optimize the ranking model wtwhen re-ceiving a training instance (qit, d1t, d2t) and its truelabel ytat each time step t. In the following, weapply the passive-aggressive online learning tech-nique [13] to solve this challenge.
First of all, weformulate the problem as an optimization:wt+1= arg minw12?w ?wt?2+ C`(w; (qit, d1t, d2t), yt)2(4)where `(wt) is a hinge loss defined as `(wt) =max(0, 1?
ytwt>(?
(qit, d1t)?
?
(qit, d2t))), and Cis a penalty cost parameter.The above optimization formulation aims toachieve a trade-off between two concerns: (i) theupdated ranking model should not be deviated toomuch from the previous ranking modelwt; and (ii)the updated ranking model should suffer a smallloss on the triplet instance (qit, d1t, d2t).
Their trade-off is essentially controlled by the penalty cost pa-rameter C. Finally, we can derive the followingproposition for the solution to the above.Proposition 1.
This optimization in (4) has thefollowing closed-form solution:wt+1= wt+ ?tyt(?
(qit, d1t)?
?
(qit, d2t)) (5)where ?tis computed as follows:?t=max(0, 1?wt>yt(?
(qit, d1t)?
?
(qit, d2t)))??
(qit, d1t)?
?
(qit, d2t))?2+12C(6)It is not difficult to derive the result in theabove proposition by following the similar ideaof passive aggressive online learning [13].
Weomit the detailed proof here.
We can see that ifwt>yt(?
(qit, d1t)?
?
(qit, d2t)) ?
1, then the modelremains unchanged, which means that if the cur-rent ranking model can correctly rank the order ofd1tand d2tw.r.t.
query qitat a large margin, we cankeep our model unchanged at this round; other-wise, we will update the current ranking model bythe above proposition.
Figure 1 gives the frame-work of the proposed online learning to rank al-gorithms.
We denote by the first-order learning torank algorithm as ?SOLAR-I?
for short.3.3 Second-order SOLAR AlgorithmThe previous algorithm only exploits first-orderinformation of the ranking model wt.
Inspiredby recent studies in second-order online learn-ing [6, 16, 14], we explore second-order algo-rithms for online learning to rank.Algorithm 1: SOLAR ?
Scalable Online Learning toRank1: Initialize w1= 0, t = 12: for i = 1, 2, .
.
.
, Q do3: receive a query qiand documents for ranking4: rank the documents by current model wt5: acquire user?s feedback in triplet instances6: for j = 1, .
.
.
, Tido7: update wt+1with (qit, d1t, d2t) and ytbyEqn.
(5) (SOLAR-I) or by Eqn.
(8) (SOLAR-II)8: t = t + 19: end for10: end forFigure 1: SOLAR: scalable online learning to rankSpecifically, we cast the online learning to rank-ing problem into a probabilistic framework, inwhich we model feature confidence for a linearranking modelwwith a Gaussian distribution withmean w ?
Rdand covariance ?
?
Rd?d.
Themean vector w is used as the model of the rankingfunction, and the covariance matrix ?
representsour confidence on the model: the smaller the valueof ?p,p, the more confident the learner has over thep-th feature wpof the ranking model w.Following the similar intuition of the abovesection, we want to optimize our ranking modelN (w,?)
by achieving the following trade-off: (i)to avoid being deviated too much from the previ-ous model N (wt,?t); (ii) to ensure that it suffersa small loss on current triplet instance; and (iii) toattain a large confidence on the current instance.Similar to [16], we employ the Kullback-Leiblerdivergence to measure the distance between thecurrent model w to be optimized and the previousmodel wt, and the regularization terms includeboth the loss suffered at current triplet instance andthe confidence on current triplet instance.Specifically, we formulate the optimization ofsecond-order online learning to rank as:{wt+1,?t+1} = arg minw,?DKL(N (w,?
)||N (wt,?t))+`(w)2+ ?(?)2?(7)?(?)
= (?
(qit, d1t)?
?
(qit, d2t))>?(?
(qit, d1t)?
?
(qit, d2t))where ?
is the trade-off parameter.
The follow-ing proposition gives the closed-form solution.Proposition 2.
This optimization problem in (7)has the following closed-form solution:wt+1= wt+ ?t?tyt(?
(qit, d1t)?
?
(qit, d2t)) (8)?t+1= ?t?
(1/?t)?tA?t(9)where A, ?t, and ?tare computed as follows:A = (?
(qit, d1t)?
?
(qit, d2t))(?
(qit, d1t)?
?
(qit, d2t))>?t= (?
(qit, d1t)?
?
(qit, d2t))>?t(?
(qit, d1t)?
?
(qit, d2t)) + ?
?t= max(0, 1?
ytwt>(?
(qit, d1t)?
?
(qit, d2t)))/?t1695The above can be proved by following [14].
Weomit the details.
We denote the above algorithm as?SOLAR-II?
for short.4 Theoretical AnalysisIn this section, we theoretically analyze the twoproposed algorithms by proving some online cu-mulative IR measure bounds for both of them.In order to prove the IR measure bounds for theproposed algorithms, we first need to draw the re-lationships between the cumulative IR measuresand the sum of pairwise squared hinge losses.
Tothis purpose, we introduce the following Lemma.Lemma 4.1.
For one query qiand its related doc-uments, the NDCG and MAP is lower bounded byits sum of pairwise squared hinge loss suffered byrank model w.NDCGi?
1?
?NDCG?t`2(w, (qit, d1t, d2t))MAPi?
1?
?MAP?t`2(w, (qit, d1t, d2t))where ?NDCGand ?MAPare constant specifiedby the properties of IR measures: ?NDCG=G(K?1)D(1)Nnand ?MAP=1m, G(K) = 2K?1,D(K) =1log2(1+K),Nn= maxpi?mr=1G(l(pi(r)))D(r), l(r) is thecorresponding labels as K-level ratings, pi is ranklist, m is the number of relevant documents.Sketch Proof.
Using the essential loss ideadefined in [11], from Theorem 1 of [11] wecould see the essential loss is an upper boundof measure-based ranking errors; besides, theessential loss is the lower bound of the sum ofpairwise squared hinge loss, using the propertiesof squared hinge loss, which is non-negative, non-increasing and satisfy `2(0) = 1.The above lemma indicates that if we couldprove bounds for the online cumulative squaredhinge loss compared to the best ranking modelwith all data beforehand, we could obtain the cu-mulative IR measures bounds.
Fortunately thereare strong theoretical loss bounds for the proposedonline learning to ranking algorithms.
The follow-ing shows the theorem of such loss bounds for theproposed SOLAR algorithms.Theorem 1.
For the SOLAR-I algorithm with Qqueries, for any rank model u, suppose R =maxi,t??
(qit, d1t) ?
?
(qit, d2t))?, the cumulativesquared hinge loss is bounded byQ?i=1Ti?t=1`2t(wt) ?
(R2+12C)(?u?2+ 2CQ?i=1Ti?t=1`2t(u)) (10)The proof for Theorem 1 can be found in Ap-pendix A.
By combining the results of Lemma 1and Theorem 1, we can easily derive the cumula-tive IR measure bound of the SOLAR-I algorithm.Theorem 2.
For the SOLAR-I algorithm with Qqueries, for any ranking model u, the NDCG andMAP performances are respectively bounded byNDCG ?
1?
?NDCGQ(R2+12C)(?u?2+ 2CQ?iTi?t=1`2t(u))MAP ?
1?
?MAPQ(R2+12C)(?u?2+ 2CQ?iTi?t=1`2t(u))The analysis of the SOLAR-II algorithm wouldbe much more complex.
Let us denote byM(M = |M|) the set of example indicesfor which the algorithm makes a mistake, andby U(U = |U|) the set of example indicesfor which there is an update but not a mis-take.
Let XA=?(qit,d1t,d2t)?M?U(?
(qit, d1t) ??
(qit, d2t))(?
(qit, d1t) ?
?
(qit, d2t))T. The theorembelow give the squared hinge loss bound.Theorem 3.
For the SOLAR-II al-gorithm with Q queries, Let ?t=(?
(qit, d1t) ?
?
(qit, d2t))T?t(?
(qit, d1t) ?
?
(qit, d2t))of examples in M?
U at time t, K and k is themaximum and minimum value of ?t, respectively.
?Tbe the final covariance matrix and uTbe thefinal mean vector.
For any ranking model u, thesquared hinge loss is bounded byQ?i=1Ti?t=1`2t(wt) ?K + ?k + ?
(a+Q?i=1Ti?t=1`t(u))+(K + ?
)(log det(??1T)?a2?2uT?
?1Tu)where a =??
?u?2+ utXAu?log(det(I +1?XA)) + UThe proof for Theorem 3 can be found in Ap-pendix B.
Now, by combining the Lemma 1 andTheorem 3, we can derive the cumulative IR mea-sure bound achieved by the proposed SOLAR-IIalgorithm.Theorem 4.
For the SOLAR-II algorithm with Qqueries, for any ranking model u, the NDCG andMAP performances are respectively bounded byNDCG ?
1?
?NDCG(K + ?
)Q(k + ?)(a+Q?iTi?t=1`t(u))?
?NDCGbQMAP ?
1?
?MAP(K + ?
)Q(k + ?)(a+Q?iTi?t=1`t(u))?
?MAPbQwhere b = (K + ?
)(log det(??1T)?a2?2uT?
?1Tu)The above theorems show that our online algo-rithm is no much worse than that of the best rank-ing model u with all data beforehand.16965 ExperimentsWe conduct extensive experiments to evaluate theefficacy of our algorithms in two major aspects:(i) to examine the learning efficacy of the pro-posed SOLAR algorithms for online learning torank tasks; (ii) to directly compare the proposedSOLAR algorithms with the state-of-the-art batchlearning to rank algorithms.
Besides, we alsoshow an application of our algorithms for trans-fer learning to rank tasks to demonstrate the im-portance of capturing changing search intentiontimely in real web applications.
The results arein the supplemental file due to space limitation.5.1 Experimental Testbed and MetricsWe adopt the popular benchmark testbed for learn-ing to rank: LETOR1[31].
To make a com-prehensive comparison, we perform experimentson all the available datasets in LETOR3.0 andLETOR4.0.
The statistics are shown in Table 1.For performance evaluation metrics, we adoptthe standard IR measures, including ?MAP?,?NDCG@1?, ?NDCG@5?, and ?NDCG@10?.Table 1: LETOR datasets used in the experiments.Dataset #Queries #features avg#Docs/queryOHSUMED 106 45 152.26MQ2007 1692 46 41.14MQ2008 784 46 19.40HP2003 150 64 984.04HP2004 75 64 992.12NP2003 75 64 991.04NP2004 75 64 984.45TD2003 50 64 981.16TD2004 50 64 988.615.2 Evaluation of Online Rank PerformanceThis experiment evaluates the online learning per-formance of the proposed algorithms for onlinelearning to rank tasks by comparing them withthe existing ?Prank?
algorithm [15], a Perceptron-based pointwise online learning to rank algorithm,and a recently proposed ?Committee Perceptron(Com-P)?
algorithm [17], which explores the en-semble learning for Perceptron.
We evaluate theperformance in terms of both online cumulativeNDCG and MAP measures.
As it is an onlinelearning task, the parameter C of SOLAR-I isfixed to 10?5and the parameter ?
of SOLAR-IIis fixed to 104for all the datasets, as suggestedby [17], we set the number of experts in ?Com-P?
to 20.
All experiments were conducted over 10random permutations of each dataset, and all re-sults were averaged over the 10 runs.1http://research.microsoft.com/en-us/um/beijing/projects/letor/Table 2 give the results of NDCG on all thedatasets, where the best results were bolded.
Sev-eral observations can be drawn as follows.First of all, among all the algorithms, we foundthat both SOLAR-I and SOLAR-II achieve sig-nificantly better performance than Prank, whichproves the efficacy of the proposed pairwise al-gorithms.
Second, we found that Prank (point-wise) performs extremely poor on several datasets(HP2003, HP2004, NP2003, NP2004, TD2003,TD2004).
By looking into the details, we foundthat it is likely because Prank (pointwise), as apointwise algorithm, is highly sensitive to the im-balance of training data, and the above datasets areindeed highly imbalanced in which very few doc-uments are labeled as relevant among about 1000documents per query.
By contrast, the pairwise al-gorithm performs much better.
This observationfurther validates the importance of the proposedpairwise SOLAR algorithms that are insensitiveto imbalance issue.
Last, by comparing the twoSOLAR algorithms, we found SOLAR-II outper-forms SOLAR-I in most cases, validating the effi-cacy of exploiting second-order information.5.3 Batch v.s.
Online Learning5.3.1 Comparison of ranking performanceThis experiment aims to directly compare the pro-posed algorithms with the state-of-the-art batch al-gorithms in a standard learning to rank setting.We choose four of the most popular and cutting-edge batch algorithms that cover both pairwise andlistwise approaches, including RankSVM [20],AdaRank [39], RankBoost [18], and ListNet [5].For comparison, we follow the standard setting:each dataset is divided into 3 parts: 60% for train-ing, 20% for validation to select the best parame-ters, and 20% for testing.
We use the training datato learn the ranking model by the proposed SO-LAR algorithms, the validation data to select thebest parameters, and use the test data to evaluateperformance.
For SOLAR-I, we choose the bestparameter C from [10?3.5, 10?6.5] via grid searchon the validation set; and similarly for SOLAR-II,we choose the best parameter ?
from [103, 106].Following [31], we adopt 5 division versions of allthe datasets, and report the average performance.The results are shown in Table 3, where the bestperformances were bolded2.
Several observationscan drawn from the results.2Results of the baseline algorithms are taken from LETOR.1697Table 2: Evaluation of NDCG performance of online learning to rank algorithms.AlgorithmOHSUMED MQ2007 MQ2008NDCG@1 NDCG@5 NDCG@10 NDCG@1 NDCG@5 NDCG@10 NDCG@1 NDCG@5 NDCG@10Prank(Pointwise) 0.2689 0.2253 0.2221 0.2439 0.2748 0.3039 0.2369 0.3352 0.4036Prank(Pairwise) 0.4456 0.3953 0.3904 0.2777 0.3010 0.3294 0.2834 0.3823 0.4403Com-P 0.4327 0.3993 0.3934 0.3640 0.3828 0.4135 0.3378 0.4415 0.4885SOLAR-I 0.5060 0.4479 0.4337 0.3760 0.3973 0.4271 0.3490 0.4584 0.5022SOLAR-II 0.5352 0.4635 0.4461 0.3897 0.4095 0.4383 0.3594 0.4680 0.5107AlgorithmHP2003 HP2004 NP2003NDCG@1 NDCG@5 NDCG@10 NDCG@1 NDCG@5 NDCG@10 NDCG@1 NDCG@5 NDCG@10Prank(Pointwise) 0.0033 0.0047 0.0050 0.0053 0.0083 0.0088 0.0033 0.0051 0.0075Prank(Pairwise) 0.5267 0.6491 0.6745 0.5107 0.6438 0.6717 0.4033 0.5926 0.6255Com-P 0.6487 0.7744 0.7884 0.5640 0.7163 0.7392 0.5227 0.7146 0.7417SOLAR-I 0.6993 0.7796 0.7917 0.5347 0.7072 0.7335 0.5527 0.7486 0.7792SOLAR-II 0.7020 0.7959 0.8079 0.5413 0.7146 0.7419 0.5693 0.7621 0.7895AlgorithmNP2004 TD2003 TD2004NDCG@1 NDCG@5 NDCG@10 NDCG@1 NDCG@5 NDCG@10 NDCG@1 NDCG@5 NDCG@10Prank(Pointwise) 0.0080 0.0100 0.0100 0.0040 0.0063 0.0056 0.0040 0.0018 0.0025Prank(Pairwise) 0.4213 0.6039 0.6290 0.1920 0.1707 0.1737 0.2773 0.2235 0.2071Com-P 0.4867 0.6989 0.7226 0.3300 0.2717 0.2635 0.3427 0.2988 0.2794SOLAR-I 0.5613 0.7649 0.7869 0.2160 0.2968 0.2916 0.2533 0.2750 0.2625SOLAR-II 0.5627 0.7667 0.7858 0.2960 0.3251 0.3245 0.2893 0.2874 0.28060 20 40 60 80 100 1200.30.320.340.360.380.40.420.440.46Number of samplesOnlinecumulative MAPPrank(Pointwise)Prank(Pairwise)Com?PSOLAR?ISOLAR?II0 200 400 600 800 1000 1200 1400 16000.40.420.440.460.480.50.520.54Number of samplesOnlinecumulative MAPPrank(Pointwise)Prank(Pairwise)Com?PSOLAR?ISOLAR?II0 50 100 15000.10.20.30.40.50.60.70.8Number of samplesOnlinecumulative MAPPrank(Pointwise)Prank(Pairwise)Com?PSOLAR?ISOLAR?II0 10 20 30 40 50 60 70 8000.10.20.30.40.50.60.7Number of samplesOnlinecumulative MAPPrank(Pointwise)Prank(Pairwise)Com?PSOLAR?ISOLAR?IIOSHUMED 2007MQ 2003HP 2004HP0 50 100 15000.10.20.30.40.50.60.7Number of samplesOnlinecumulative MAPPrank(Pointwise)Prank(Pairwise)Com?PSOLAR?ISOLAR?II0 10 20 30 40 50 60 70 8000.10.20.30.40.50.60.70.8Number of samplesOnlinecumulative MAPPrank(Pointwise)Prank(Pairwise)Com?PSOLAR?ISOLAR?II0 10 20 30 40 5000.050.10.150.20.25Number of samplesOnlinecumulative MAPPrank(Pointwise)Prank(Pairwise)Com?PSOLAR?ISOLAR?II0 10 20 30 40 50 60 70 8000.020.040.060.080.10.120.140.160.180.2Number of samplesOnlinecumulative MAPPrank(Pointwise)Prank(Pairwise)Com?PSOLAR?ISOLAR?II2003NP 2004NP 2003TD 2004TDFigure 2: Evaluation of MAP performances of Online Learning to Rank algorithmsFirst of all, we found that no single algorithmbeats all the others on all the datasets.
Second,on all the datasets, we found that the SOLARalgorithms are generally achieve comparable tothe state-of-the-art batch algorithms.
On somedatasets, e.g., ?MQ2008?, ?MQ2007?
?HP2003?,?TD2003?, the proposed online algorithms caneven achieve best performances in terms of MAP.This encouraging result proves the efficacy of theproposed algorithms as an efficient and scalableonline solution to train ranking models.
Sec-ond, among the two proposed online algorithms,SOLAR-II still outperforms SOLAR-I in mostcases, which again shows the importance of ex-ploiting second-order information.5.3.2 Scalability EvaluationThis experiment aims to examine the scalabilityof the proposed SOLAR algorithms.
We com-0 100 200 300 400 500 600 700 80010?1100101102103104Number of queries receivedTimecostSOLAR?ISOLAR?IIRankSVMFigure 3: Scalability Evaluation on ?2008MQ?pare it with RankSVM [20], a widely used and ef-ficient batch algorithm.
For implementation, weadopt the code from [9]3, which is known to bethe fastest implementation.
Figure 3 illustrates thescalability evaluation on ?2008MQ?
dataset.
Fromthe results, we observe that SOLAR is much faster(e.g., 100+ times faster on this dataset)and signifi-cantly more scalable than RankSVM.3http://olivier.chapelle.cc/primal/1698Table 3: Evaluation of NDCG of Online vs Batch Learning to Rank algorithms.AlgorithmOHSUMED MQ2007 MQ2008NDCG@1 NDCG@5 NDCG@10 NDCG@1 NDCG@5 NDCG@10 NDCG@1 NDCG@5 NDCG@10RankSVM 0.4958 0.4164 0.4140 0.4096 0.4142 0.4438 0.3626 0.4695 0.2279AdaRank-NDCG 0.5330 0.4673 0.4496 0.3876 0.4102 0.4369 0.3826 0.4821 0.2307RankBoost 0.4632 0.4494 0.4302 0.4134 0.4183 0.4464 0.3856 0.4666 0.2255ListNet 0.5326 0.4432 0.4410 0.4002 0.4170 0.4440 0.3754 0.4747 0.2303SOLAR-I 0.5111 0.4668 0.4497 0.3886 0.4101 0.4361 0.3677 0.4634 0.5086SOLAR-II 0.5397 0.4690 0.4490 0.4104 0.4149 0.4435 0.3720 0.4771 0.5171AlgorithmHP2003 HP2004 NP2003NDCG@1 NDCG@5 NDCG@10 NDCG@1 NDCG@5 NDCG@10 NDCG@1 NDCG@5 NDCG@10RankSVM 0.6933 0.7954 0.8077 0.5733 0.7512 0.7687 0.5800 0.7823 0.8003AdaRank-NDCG 0.7133 0.8006 0.8050 0.5867 0.7920 0.8057 0.5600 0.7447 0.7672RankBoost 0.6667 0.8034 0.8171 0.5067 0.7211 0.7428 0.6000 0.7818 0.8068ListNet 0.7200 0.8298 0.8372 0.6000 0.7694 0.7845 0.5667 0.7843 0.8018SOLAR-I 0.7067 0.8036 0.8056 0.5467 0.7325 0.7544 0.5800 0.7664 0.7935SOLAR-II 0.7000 0.8068 0.8137 0.5733 0.7394 0.7640 0.5667 0.7691 0.7917AlgorithmNP2004 TD2003 TD2004NDCG@1 NDCG@5 NDCG@10 NDCG@1 NDCG@5 NDCG@10 NDCG@1 NDCG@5 NDCG@10RankSVM 0.5067 0.7957 0.8062 0.3200 0.3621 0.3461 0.4133 0.3240 0.3078AdaRank-NDCG 0.5067 0.7122 0.7384 0.3600 0.2939 0.3036 0.4267 0.3514 0.3163RankBoost 0.4267 0.6512 0.6914 0.2800 0.3149 0.3122 0.5067 0.3878 0.3504ListNet 0.5333 0.7965 0.8128 0.4000 0.3393 0.3484 0.3600 0.3325 0.3175SOLAR-I 0.5733 0.7814 0.7976 0.2600 0.3060 0.3071 0.3600 0.3119 0.3049SOLAR-II 0.5733 0.7830 0.8013 0.3000 0.3652 0.3462 0.3333 0.3167 0.30566 Conclusions and Future WorkThis paper presented SOLAR ?
a new frameworkof Scalable Online Learning Algorithms for Rank-ing.
SOLAR overcomes the limitations of tradi-tional batch learning to rank for real-world on-line applications.
Our empirical results concludedthat SOLAR algorithms share competitive efficacyas the state-of-the-art batch algorithms, but enjoysalient properties which are critical to many appli-cations.
Our future work include (i) extending ourtechniques to the framework of listwise learningto rank; (ii) modifying the framework to handlelearning to ranking with ties; and (iii) conductingmore in-depth analysis and comparisons to othertypes of online learning to rank algorithms in di-verse settings, e.g., partial feedback [41, 22].Appendix Proof of Theorem 1Proof.
Let ?t= ?wt?
u?2??wt+1?
u?2, thenT?t=1?t= ?u?2?
?wT+1?
u?2?
?u?2Further, ?tcan be expressed as:?t= ?2?tyt(wt?
u) ?
(?
(qit, d1t)?
?
(qit, d2t))??t??
(qit, d1t)?
?
(qit, d2t))?2?
?t(2`t(wt)?
?t?
2`t(u)).We thus have?u?2?T?t=1(2?t`t(wt)?
?2t??
(qit, d1t)?
?
(qit, d2t))?2?
2?t`t(u))?T?t=1(2?t`t(wt)?
?2t??
(qit, d1t)?
?
(qit, d2t))?2?
2?t`t(u)?(?t?2C??2C`t(u))2)?T?t=1(2?t`t(wt)?
?2t(??
(qit, d1t)?
?
(qit, d2t)?2+12C)?
2C`t(u)2)=T?t=1(`t(wt)2??
(qit, d1t)?
?
(qit, d2t))?2+12C?
2C`t(u)2)Combining the above concludes the theorem.Appendix B: Proof of Theorem 3Proof.
Using the Cauchy-Schwarz inequality, wehave uTT??1TuT?(uT??1TuT)2uT??1Tu.
Notice that someinequalities could be easily obtained by extendingthe Lemma3, Lemma 4 and Theorem 2 of [14] tothe pairwise setting as follows:uT?
?1TuT?M + U ?
?t?M?U`t(u)?,?t?M?U?tr(?t+ ?)?
log(det(??1T))uTT?
?1TuT=?t?M?U?tr(?t+ ?)+?t?M?U1?
`2t(wt)?t+ ?,M + U ?
a+?t?M?U`t(u)where a =??
?u?2+ utXAu?log(det(I +1?XA)) + U.We thus have?t?M?U`2t(wt)?t+ ??
?t?M?U?tr(?t+ ?
)+?t?M?U1?t+ ??
(M + U ??t?M?U`t(u))2r2uT??1Tu?
log(det(?
?1T)) +?t?M?U1?t+ ??a2r2uT??1Tu?
log(det(??1T))?a2r2uT?
?1Tu+M + Uk + ??
log(det(??1T))?a2r2uT?
?1Tu+a+?t?M?U`t(u)k + ?Combining the above, we achieve the final result:Q?i=1Ti?t=1`2t(wt) ?K + ?k + ?
(a+Q?i=1Ti?t=1`t(u))+(K + ?
)(log det(??1T)?a2?2uT?
?1Tu)1699AcknowledgmentsThis work was supported by Singapore MOEtier 1 research grant (C220/MSS14C003) and theNational Nature Science Foundation of China(61428207).References[1] R. A. Baeza-Yates and B.
A. Ribeiro-Neto.Modern Information Retrieval - the conceptsand technology behind search, Second edi-tion.
Pearson Education Ltd., Harlow, Eng-land, 2011.
[2] C. J. C. Burges, R. Ragno, and Q. V. Le.Learning to rank with nonsmooth cost func-tions.
In NIPS, pages 193?200, 2006.
[3] C. J. C. Burges, T. Shaked, E. Renshaw,A.
Lazier, M. Deeds, N. Hamilton, and G. N.Hullender.
Learning to rank using gradientdescent.
In ICML, pages 89?96, 2005.
[4] Y. Cao, J. Xu, T.-Y.
Liu, H. Li, Y. Huang, andH.-W. Hon.
Adapting ranking svm to doc-ument retrieval.
In SIGIR, pages 186?193,2006.
[5] Z. Cao, T. Qin, T.-Y.
Liu, M.-F. Tsai, andH.
Li.
Learning to rank: from pairwise ap-proach to listwise approach.
In ICML, pages129?136, 2007.
[6] N. Cesa-Bianchi, A. Conconi, and C. Gen-tile.
A second-order perceptron algorithm.SIAM J.
Comput., 34(3):640?668, 2005.
[7] O. Chapelle and Y. Chang.
Yahoo!
learningto rank challenge overview.
In Yahoo!
Learn-ing to Rank Challenge, pages 1?24, 2011.
[8] O. Chapelle, Y. Chang, and T.-Y.
Liu.
Fu-ture directions in learning to rank.
Journalof Machine Learning Research - ProceedingsTrack, 14:91?100, 2011.
[9] O. Chapelle and S. S. Keerthi.
Efficient al-gorithms for ranking with svms.
Inf.
Retr.,13(3):201?215, 2010.
[10] G. Chechik, V. Sharma, U. Shalit, andS.
Bengio.
Large scale online learning ofimage similarity through ranking.
J. Mach.Learn.
Res., 11:1109?1135, Mar.
2010.
[11] W. Chen, T.-Y.
Liu, Y. Lan, Z. Ma, andH.
Li.
Ranking measures and loss functionsin learning to rank.
In NIPS, pages 315?323,2009.
[12] W. S. Cooper, F. C. Gey, and D. P. Dabney.Probabilistic retrieval based on staged logis-tic regression.
In SIGIR?98, pages 198?210.ACM, 1992.
[13] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y.
Singer.
Online passive-aggressive algorithms.
Journal of MachineLearning Research, 7:551?585, 2006.
[14] K. Crammer, A. Kulesza, and M. Dredze.Adaptive regularization of weight vectors.
InNIPS, pages 414?422, 2009.
[15] K. Crammer and Y.
Singer.
Pranking withranking.
In NIPS, pages 641?647, 2001.
[16] M. Dredze, K. Crammer, and F. Pereira.Confidence-weighted linear classification.
InICML, pages 264?271, 2008.
[17] J. L. Elsas, V. R. Carvalho, and J. G. Car-bonell.
Fast learning of document rankingfunctions with the committee perceptron.
InWSDM, pages 55?64, 2008.
[18] Y. Freund, R. D. Iyer, R. E. Schapire, andY.
Singer.
An efficient boosting algorithm forcombining preferences.
Journal of MachineLearning Research, 4:933?969, 2003.
[19] F. C. Gey.
Inferring probability of relevanceusing the method of logistic regression.
In InProceedings of ACM SIGIR?94, pages 222?231.
Springer-Verlag, 1994.
[20] R. Herbrich, T. Graepel, and K. Obermayer.Large margin rank boundaries for ordinal re-gression.
In Advances in Large Margin Clas-sifiers, pages 115?132, 2000.
[21] K. Hofmann.
Fast and reliable online learn-ing to rank for information retrieval.
Phd the-sis, University of Amsterdam, Amsterdam,05/2013 2013.
[22] K. Hofmann, A. Schuth, S. Whiteson, andM.
de Rijke.
Reusing historical interactiondata for faster online learning to rank forir.
In Proceedings of the sixth ACM in-ternational conference on Web search anddata mining, WSDM, pages 183?192, Rome,Italy, 2013.
[23] K. Hofmann, S. Whiteson, and M. Rijke.Balancing exploration and exploitation inlistwise and pairwise online learning to rankfor information retrieval.
Inf.
Retr., 16(1):63?90, Feb. 2013.1700[24] S. C. Hoi, J. Wang, and P. Zhao.
Li-bol: A library for online learning algorithms.The Journal of Machine Learning Research,15(1):495?499, 2014.
[25] K. J?arvelin and J. Kek?al?ainen.
Ir evaluationmethods for retrieving highly relevant docu-ments.
In SIGIR, pages 41?48, 2000.
[26] T. Joachims.
Optimizing search engines us-ing clickthrough data.
In KDD, pages 133?142, 2002.
[27] H. Li.
Learning to rank for information re-trieval and natural language processing.
Syn-thesis Lectures on Human Language Tech-nologies, 7(3):1?121, 2014.
[28] P. Li, C. J. C. Burges, and Q. Wu.
Mcrank:Learning to rank using multiple classificationand gradient boosting.
In NIPS, 2007.
[29] T.-Y.
Liu.
Learning to Rank for InformationRetrieval.
Springer, 2011.
[30] R. Nallapati.
Discriminative models for in-formation retrieval.
In SIGIR?04, pages 64?71, Sheffield, United Kingdom, 2004.
[31] T. Qin, T.-Y.
Liu, J. Xu, and H. Li.
Letor: Abenchmark collection for research on learn-ing to rank for information retrieval.
Inf.Retr., 13(4):346?374, 2010.
[32] F. Rosenblatt.
The perceptron: A probabilis-tic model for information storage and organi-zation in the brain.
Psych.
Rev., 7:551?585,1958.
[33] M. Surdeanu, M. Ciaramita, andH.
Zaragoza.
Learning to rank answerson large online qa collections.
In ACL, pages719?727, 2008.
[34] M. Taylor, J. Guiver, S. Robertson, andT.
Minka.
Softrank: optimizing non-smoothrank metrics.
In Proceedings of the in-ternational conference on Web search andweb data mining, WSDM, pages 77?86, PaloAlto, California, USA, 2008.
ACM.
[35] M.-F. Tsai, T.-Y.
Liu, T. Qin, H.-H. Chen,and W.-Y.
Ma.
Frank: a ranking method withfidelity loss.
In SIGIR?07, pages 383?390,Amsterdam, The Netherlands, 2007.
[36] E. Tsivtsivadze, K. Hoffman, and T. Hes-kes.
Large scale co-regularized ranking.In J. F?urnkranz and E. H?ullermeier, edi-tors, ECAI Workshop on Preference Learn-ing, 2012.
[37] H. Valizadegan, R. Jin, R. Zhang, and J. Mao.Learning to rank by optimizing ndcg mea-sure.
In NIPS, pages 1883?1891, 2009.
[38] F. Xia, T.-Y.
Liu, J. Wang, W. Zhang, andH.
Li.
Listwise approach to learning to rank:theory and algorithm.
In ICML?08, pages1192?1199, Helsinki, Finland, 2008.
[39] J. Xu and H. Li.
Adarank: a boosting al-gorithm for information retrieval.
In SIGIR,pages 391?398, 2007.
[40] J. Xu, T.-Y.
Liu, M. Lu, H. Li, and W.-Y.
Ma.Directly optimizing evaluation measures inlearning to rank.
In SIGIR?08, pages 107?114, Singapore, Singapore, 2008.
ACM.
[41] Y. Yue, J. Broder, R. Kleinberg, andT.
Joachims.
The k-armed dueling banditsproblem.
J. Comput.
Syst.
Sci., 78(5):1538?1556, 2012.
[42] Y. Yue, T. Finley, F. Radlinski, andT.
Joachims.
A support vector method foroptimizing average precision.
In SIGIR?07,pages 271?278, Amsterdam, The Nether-lands, 2007.
ACM.
[43] Z. Zheng, K. Chen, G. Sun, and H. Zha.A regression framework for learning rank-ing functions using relative relevance judg-ments.
In SIGIR?07, pages 287?294, Ams-terdam, The Netherlands, 2007.1701
