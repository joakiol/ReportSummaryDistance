Sentence Ordering based on Cluster Adjacency in Multi-DocumentSummarizationJi Donghong, Nie YuInstitute for Infocomm ResearchSingapore, 119613{dhji, ynie}@i2r.a-star.edu.sgABSTRACTIn this paper, we propose a cluster-adjacency based method toorder sentences for multi-document summarization tasks.Given a group of sentences to be organized into a summary,each sentence was mapped to a theme in source documents bya semi-supervised classification method, and adjacency ofpairs of sentences is learned from source documents based onadjacency of clusters they belong to.
Then the ordering of thesummary sentences can be derived with the first sentencedetermined.
Experiments and evaluations on DUC04 datashow that this method gets better performance than otherexisting sentence ordering methods.1.
IntroductionThe issue of how to extract information from sourcedocuments is one main topic of summarization area.
Beingthe last step of multi-document summarization tasks,sentence ordering attracts less attention up to now.
But sincea good summary should be fluent and readable to humanbeing, sentence ordering which organizes texts into the finalsummary could not be ignored.Sentence ordering is much harder for multi-documentsummarization than for single-document summarization(McKeown et al, 2001; Barzilay and Lapata, 2005).
Themain reason is that unlike single document, multi-documentsdon?t provide a natural order of texts to be the basis ofsentence ordering judgment.
This is more obvious forsentence extraction based summarization systems.Majority ordering is one way of sentence ordering(McKeown et al, 2001; Barzilay et al, 2002).
This methodgroups sentences in source documents into different themesor topics based on summary sentences to be ordered, and theorder of summary sentences is determined based on the orderof themes.
The idea of this method is reasonable since thesummary of multi-documents usually covers several topics insource documents to achieve representative, and the themeordering can suggest sentence ordering somehow.
However,there are two challenges for this method.
One is how tocluster sentences into topics, and the other is how to ordersentences belonging to the same topic.
Barzilay et al (2002)combined topic relatedness and chronological orderingtogether to order sentences.
Besides chronological ordering,sentences were also grouped into different themes andordered by the order of themes learned from sourcedocuments.
The results show that topic relatedness can helpchronological ordering to improve the performance.Probabilistic model was also used to order sentences.Lapata (2003) ordered sentences based on conditionalprobabilities of sentence pairs.
The conditional probabilitiesof sentence pairs were learned from a training corpus.
Withconditional probability of each sentence pairs, theapproximate optimal global ordering was achieved with asimple greedy algorithm.
The conditional probability of apair of sentences was calculated by conditional probability offeature pairs occurring in the two sentences.
The experimentresults show that it gets significant improvement comparedwith randomly sentence ranking.Bollegala et al (2005) combined chronological ordering,probabilistic ordering and topic relatedness ordering together.They used a machine learning approach to learn the way ofcombination of the three ordering methods.
The combinedsystem got better results than any of the three individualmethods.Nie et al (2006) used adjacency of sentence pairs to ordersentences.
Instead of the probability of a sentence sequenceused in probabilistic model, the adjacency model usedadjacency value of sentence pairs to order sentences.Sentence adjacency is calculated based on adjacency offeature pairs within the sentence pairs.
Adjacency betweentwo sentences means how closely they should be put togetherin a set of summary sentences.
Although there is no orderinginformation provided by sentence adjacency, an optimalordering of summary sentences can be derived by use ofadjacency information of all sentence pairs if the firstsentence is properly selected.In this paper, we propose a new sentence ordering methodnamed cluster-adjacency based ordering.
Like the feature-adjacency based ordering mentioned above, the orderingprocess still depends on sentence adjacency.
But we clustersentences first and use cluster adjacency instead of featureadjacency to calculate sentence adjacency.
The advantage ofthis change is to avoid the sensitivity of the adjacency745information to limited number of individual features, whichusually needs manual intervention.The remainder of this paper is organized as follows.
Insection 2, we specify the motivation of this method.
Insection 3, we talk about the sentence classification using asemi-supervised method.
In section 4, we discuss theprocedure for sentence ordering.
In section 5, we presentexperiments and evaluation.
In section 6, we give theconclusion and future work.2.
MotivationMajority ordering assumes that sentences in the summarybelong to different themes or topics, and the ordering ofsentences in the summary can be determined by the occurringsequence of themes in source documents.
In order to derivethe order of themes, Barzilay et al (2002) presented themesand their relations as a directed graph.
In the graph, nodesdenote themes; an edge from one node to another denotes theoccurring of one theme before another in a source document,and the weight of an edge is set to be the frequency of thetheme pair co-occurring in the texts.
Each theme is given aweight that equals to the difference between its outgoingedges and incoming edges.
By finding and removing a themewith the biggest weight in the graph recursively, an orderingof themes is determined.Probabilistic ordering method treats the ordering as a taskof finding the sentence sequence with the biggest probability(Lapata, 2003).
For a sentence sequence T= S1, S2,?,Sn ,suppose that the probability of any given sentence isdetermined only by its previous sentence, the probability of asentence sequence can be generated based on the conditionprobabilities P(Si|Si-1) of all adjacent sentence pairs in thesequence.
The condition probability P(Si|Si-1) can be furtherresolved as the product of condition probabilities of featurepairs P(fl|fm), where fl is the feature in Si, fm  is the feature inSi-1.By finding the sentence with the biggest conditionprobability with the previous one recursively, an ordering ofsentences is determined.
A null sentence is normallyintroduced at the beginning of each source document to findthe first sentence (Lapata, 2003).Both majority ordering and probabilistic orderingdetermine text sequences in the summary based on those inthe source documents.
The intuition behind the idea is thatthe ordering of summary sentences tends to be consistentwith those of document sentences.
However, we notice thatsome important information might be lost in the process.Consider examples below:Example 1: Source Document  = ??ABA?
?Example 2: Source Document 1 = ??AB?
?Source Document 2 = ??BA?
?Here A and B denote two themes.
Let?s assume that A and Bare both denoted by the summary sentences.
In bothexamples, the frequency of A preceding B equals to that of Bpreceding A, thus no sequence preference could be learnedfrom the two examples, and we can only estimate aprobability of 0.5 following one by another.
With suchestimation, the intuition that A and B shall be put adjacentlyalthough their ordering is not clear would be difficult tocapture.An adjacency based ordering (Nie et al, 2006) wasproposed to capture such adjacency information betweentexts during sentence ordering.
It uses adjacency of sentencepairs to order summary sentences.
Adjacency between twosentences can be seen as how closely they should be puttogether in an output summary.
In general, sentenceadjacency is derived from that of feature pairs withinsentences.
Note that there is no clue to decide the sequence oftwo sentences purely based on their adjacency value.However, if the first sentence has been decided, the totalsentence sequence can be derived according to the adjacencyvalues by recursively selecting one having the biggestadjacency value with the most recently selected.For adjacency based ordering, a problem is how tocalculate the adjacency value between two sentences.
Forfeature-adjacency based ordering, the sentence adjacency iscalculated based on that of feature pairs within the twosentences.
But a sentence may contain many single wordfeatures, and there may exist many noisy features, especiallyfor longer sentences.
To eliminate the impact of noisyfeatures, one simple method is to select top n most adjacentfeature pairs among the two sentences (Nie et al, 2006).However, the parameter heavily influences the performance,as shown in Table 1, where each row gives the result of a runwith the same window range and different top n adjacentfeature pairs.Win_range?
( ??top-n=1)?
( ??top-n=2)?
( ??top-n=3)?
( ??top-n=4)?
( ??top-n=5)?
( ?
?top-n=10)2 0.184 0.213 0.253 0.262 0.261 0.2243 0.251 0.252 0.273 0.274 0.257 0.2134 0.201 0.253 0.268 0.316 0.272 0.248Table 1.
Feature-Adjacency Based OrderingThe heavy reliance on the manually pre-defined parameteris an obstacle for implementation of the feature-adjacencybased ordering, since it?s hard to determine the most suitablevalue for the parameter across different tasks.
More generally,the feature-adjacency method depends on limited number ofindividual features, which normally needs very strong featureselection techniques to be effective.
To avoid the sensitivityto individual features, we propose a cluster-adjacency basedsentence ordering.
Although the clustering will also useindividual features, the noisy ones would be lower weightedvia appropriate weighting schemes.Assuming there are n summary sentences to be ordered,we cluster sentences in source documents into n clustersbased on the n summary sentences.
Each cluster represents asummary sentence.
Then we use the cluster adjacency insteadof feature adjacency to produce sentence adjacency.
Sincefeatures are not directly used in calculating sentence746adjacency, the setting of the parameter to remove noisyfeatures is no more needed.
In addition, we expect theclustering to determine the themes properly and reduce theaffect of noisy features.3.
Sentence ClusteringAssume there are K summary sentences to be ordered, andthere are N sentences in source documents, we cluster the Nsentences into K clusters using a semi-supervisedclassification method, Label Propagation (Zhu andGhahramani, 2003).
The advantage of this method is that itcan exploit the closeness between unlabeled data duringclassification, thus ensuring a better classification result evenwith very fewer labeled data.
This is exactly the situationhere, where each summary sentence can be seen as the onlyone labeled data for the class.Following are some notations for the label propagationalgorithm in sentence classification:{rj} (1?j?K): the K summary sentences{mj} (1?j?N): the N document sentences to be classifiedX = {xi} (1?i?K+N) refers to the union set of the abovetwo categories of sentences, i.e.
xi (1?i?K) represents the Ksummary sentences, xi (K+1?i?K+N+1) represents the Nsentences to be classified.
That is, the first K sentences arelabeled sentences while the remaining N sentences are to bere-ranked.
C = {cj} (1?j?K) denotes the class set ofsentences, each one in which is labeled by a summarysentence.
Y0 ?Hs?K (s=K+N) represents initial soft labelsattached to each sentence, where Yij0= 1 if xi is cj and 0otherwise.
Let YL0 be top l=K rows of Y0, which correspondsto the labeled data, and YU0 be the remaining N rows, whichcorresponds to the unlabeled data.
Here, each row in YU0 isinitialized according to the similarity of a sentence with thesummary sentences.In the label propagation algorithm, the manifold structurein X is represented as a connected graph and the labelinformation of any vertex in the graph is propagated tonearby vertices through weighted edges until the propagationprocess converges.
Here, each vertex corresponds to asentence, and the edge between any two sentences xi and xj isweighted by wij to measure their similarity.
Here wij is definedas follows: wij = exp(-dij2/ ?
2) if i ?
j and wii = 0 (1?i,j?l+u),where dij is the distance between xi and xj, and ?
is a scale tocontrol the transformation.
In this paper, we set ?
as theaverage distance between summary sentences.
Moreover, theweight wij between two sentences xi and xj is transformed to aprobability tij = P(j?i) =wij/(?sk=1wkj), where tij is theprobability to propagate a label from sentence xj to sentencexi.
In principle, larger weights between two sentences meaneasy travel and similar labels between them according to theglobal consistency assumption applied in this algorithm.Finally, tij is normalized row by row as in (1), which is tomaintain the class probability interpretation of Y.
The s ?
smatrix is denoted asT as in (1).During the label propagation process, the label distributionof the labeled data is clamped in each loop and acts likeforces to push out labels through unlabeled data.
With thispush originates from labeled data, the label boundaries willbe pushed much faster along edges with larger weights andsettle in gaps along those with lower weights.
Ideally, we canexpect that wij across different classes should be as small aspossible and wij within a same class as big as possible.
In thisway, label propagation happens within a same class mostlikely.?
== sk ikijij ttt 1)1((2)  YTTIY tY LuluuUtU01)(lim?
???
?== .???????
?=uuullullTTTTT)3(This algorithm has been shown to converge to a uniquesolution (Zhu and Ghahramani, 2003) with u=M and l=K asin (2), where I is u ?
u identity matrix.
uuT  and ulT  areacquired by splitting matrix T after the l-th row and the l-thcolumn into 4 sub-matrices as in (3).In theory, this solution can be obtained without iterationand the initialization of YU0 is not important, since YU0does not affect the estimation of UY .
However, theinitialization of Y?U0 helps the algorithm converge quicklyin practice.
In this paper, each row in YU0 is initializedaccording the similarity of a sentence with the summarysentences.
Fig.
1 gives the classification procedure.INPUT{xi} (1?i?K): set of summary sentences as labeled data;{xi} (K+1?i?K+N+1): set of document sentences;Algorithm: Label_Propagation({rj}, {mj})BEGINSet the iteration index t=0BEGIN DO LoopPropagate the label by Yt+1 = T Yt;Clamp labeled data by replacing top l row of Yt+1 with YL0END DO Loop when Yt converges;ENDFig.
1 Label propagation for sentence classificationThe output of the classification is a set of sentence clusters,and the number of the clusters equals to the number ofsummary sentences.
In each cluster, the members can beordered by their membership probabilities.
In fact, the semi-supervised classification is a kind of soft labeling (Tishbyand Slonim, 2000; Zhou et al, 2003), in which each sentencebelongs to different clusters, but with different probabilities.For sentence ordering task here, we need to get hard clusters,in which each sentence belongs to only one cluster.
Thus, weneed to cut the soft clusters to hard ones.
To do that, for eachcluster, we consider every sentence inside according to theirdecreasing order of their membership probabilities.
If asentence belongs to the current cluster with the highestprobability, then it is selected and kept.
The selection repeatsuntil a sentence belongs to another cluster with higherprobability.4.
Sentence OrderingGiven a set of summary sentences {S1,?,SK}, sentences ofthe source documents are clustered into K groups G1,?,GK,747where Si is corresponding with Gi.
For each pair of sentencesSi and Sj, the adjacency of Si and Sj can be defined as theadjacency of Gi and Gj, defined in (4).
)()(),( 2,jijiji GfGfGGfC =  (4)Here f(Gi) and f(Gj) respectively denote the frequency ofcluster Gi and Gj in source documents, f(Gi, Gj)  denotes thefrequency of Gi and Gj co-occurring in the source documentswithin a limited window range.The first sentence S1 can be determined according to (5)based on the adjacency between null clusters (containingonly the null sentence) and any sentence clusters.
)max(arg ,1 joS CTS j ?=(5)Here C0,j denotes how close the sentence Sj and a nullsentence are.
By adding a null sentence at the beginning ofeach source document as S0 , and assuming it contains onenull sentence, C0,j can be calculated with equation (4).Given an already ordered sentence sequence, S1, S2,?,Si,whose sentence set R is subset of the whole sentence set T,the task of finding the (i+1)th sentence can be described as:)max(arg ,1 jiS CRTSij ?
?+ =(6)Now the sentence sequence become S1, S2,?,Si, Si+1.
Byrepeating the step the whole sequence can be derived.5.
Experiments and EvaluationIn this section, we describe the experiments with cluster-adjacency based ordering, and compared it with majorityordering, probability-based ordering and feature-adjacencybased ordering respectively.
Some methods [e.g., 8] testedordering models using external training corpus and extractedsentence features such as nouns, verbs and dependenciesfrom parsed tress.
In this paper, we only used the raw inputdata, i.e., source input documents, and didn?t use anygrammatical knowledge.
For feature-adjacency based model,we used single words except stop words as features torepresent sentences.
For cluster-adjacency based model, weused the same features to produce vector representations forsentences.5.1 Test Set and Evaluation MetricsRegarding test data, we used DUC04 data.
DUC 04 provided50 document sets and four manual summaries for eachdocument set in its Task2.
Each document set consists of 10documents.
Sentences of each summary were taken as inputsto ordering models, with original sequential informationbeing neglected.
The output ordering of various models wereto be compared with that specified in manual summaries.A number of metrics can be used to evaluate the differencebetween two orderings.
In this paper, we used Kendall?s ?
?
[9], which is defined as:2/)1()__(21 ?
?= NNinversionsofnumber?
(7)Here N is the number of objects to be ordered (i.e.,sentences).
Number_of_inversions is the minimal number ofinterchanges of adjacent objects to transfer an ordering intoanother.
Intuitively, ?
can be considered as how easily anordering can be transferred to another.
The value of ?
rangesfrom -1 to 1, where 1 denotes the best situation ---- twoorderings are the same, and -1 denotes the worst situation---completely converse orderings.
Given a standard ordering,randomly produced orderings of the same objects would getan average ?
of 0.
For examples, Table 2 gives three numbersequences, their natural sequences and the corresponding ?values.Examples  Natural sequences ?
values1  2  4  3 1 2 3 4 0.671  5  2  3  4 1 2 3 4 5 0.42  1  3 1 2 3 0.33Table 2.
Ordering Examples5.2 ResultsIn the following, we used Rd, Mo, Pr, Fa and Ca to denoterandom ordering, majority ordering, probabilistic model,feature-adjacency based model and cluster-adjacency basedmodel respectively.
Normally, for Fa and Ca, the windowsize is set as 3 sentences, and for Fa, the noise eliminationparameter ( ?top-n) is set as 4.Table 3 gives automatic evaluation results.
We can see thatMo and Pr got very close ?
values (0.143 vs. 0.144).Meanwhile, Fa got better results (0.316), and the Ca achievedthe best performance (0.415).
The significance tests suggestthat the difference between the ?
values of Fa and Mo or Pr issignificant, and so is the difference between the values of Caand Fa, where *, **, ~ represent p-values <=0.01, (0.01,0.05], and >0.05.Models ?
Significance SVMRd -0.007Mo 0.143  0.153~Pr 0.144Fa 0.316 **Ca 0.415 * 0.305**Table 3.
Automatic evaluation resultsBoth Mo and Ca use the themes acquired by theclassification.
In comparison, we also used SVM to do theclassification, and Table 3 lists the ?
values for Mo and Ca.SVM is a typical supervised classification, which only usesthe comparison between labeled data and unlabeled data.
So,it generally requires a large number of training data to beeffective.
The results show that the difference between theperformance of Mo with LP (0.143) and SVM (0.153) is notsignificant, while the difference between the performance ofCa with LP (0.415) and SVM (0.305) is significant.In general, if an ordering gets a positive ?
value, theordering can be considered to be better than a random one.748On the contrary, for a negative ?
value, the ordering can beconsidered to be worse than a random one.
For a zero ?value, the ordering is in fact close to a random one.
So,percentage of ?
values reflects quality of the orderings tosome extent.
Table 4 shows the percentage of positiveordering, negative orderings and median orderings fordifferent models.
It demonstrates that the cluster-adjacencybased model produced the most positive orderings and theleast negative orderings.Models PositiveOrderingsNegativeOrderingsMedianOrderingsRd 99  (49.5%) 90 (45.0%) 11  (5.5%)Mo 123  (61.5%) 64  (32.0%) 13  (6.5%)Pr 125  (62.5%) 59  (29.5%) 16  (8.0%)Fa 143  (71.5%) 38  (19.0%) 19  (9.5%)Ca 162  (81.0%) 31  (15.5%) 7  (3.5%)Table 4.
Positive, Negative and Median OrderingsTo see why the cluster-adjacency model achieved betterperformance, we checked about the determination of the firstsentence between different models, since that it is extremelyimportant for Pr, Fa and Ca, and it will influence laterselections.
Either in Pr or in Fa and Ca, it was assumed thatthere is one null sentence at the beginning of each sourcedocument.
In Pr, to determine the first sentence is to find onewhich is the most likely to follow the assumed null sentence,while in the two adjacency models, to determine the firstsentence means to select one which is the closest to the nullsentence.
Table 5 shows the comparison.Models Correct selection of 1st sentencesRd 22 (14.0%)Mo 53 (26.5%)Pr 81 (41.5%)Fa 119 (59.5%)Ca 131 (65.5%)Table 5.
First sentence determinationTable 5 indicates that cluster-adjacency model performedbest in selection of the first sentence in the summaries.Another experiment we did is about how likely the k+1thsentence can be correctly selected if assuming that top ksentences have been successfully acquired.
This is also usefulto explain why a model performs better than others.
Fig.
2shows the comparison of the probabilities of correctdetermination of the k+1th sentence between differentmodels.
Fig.
2 demonstrates that the probabilities of thecorrect k+1th sentence selection in cluster-adjacency modelare generally higher than those in other methods, whichindicates that the cluster-adjacency model is moreappropriate for the data.00.20.40.60.811 2 3 4 5 6 7CaFaMOPrFig.
2. k+1th sentence determinationTable 6 gives the experiment results of the cluster-adjacency model with varying window ranges.
In general, thecluster-adjacency model got better performance than feature-adjacency model without requirement of setting the noiseelimination parameters.
This can be seen as an advantage ofCa over Fa.
However, we can see that the adjacency windowsize still influenced the performance as it did for Fa.Window size ?
values2 0.3143 0.4154 0.3985 0.356Table 6.
Ca performance with different window sizeAs a concrete example, consider a summary (D31050tG) inFig.
3, which includes 6 sentences as the following.0.
After 2 years of wooing the West by signing international accords,apparently relaxing controls on free speech, and releasing and exilingthree dissenters, China cracked down against political dissent in Dec1998.1.
Leaders of the China Democracy Party (CDP) were arrested and threewere sentenced to jail terms of 11 to 13 years.2.
The West, including the US, UK and Germany, reacted strongly.3.
Clinton's China policy of engagement was questioned.4.
China's Jiang Zemin stated economic reform is not a prelude todemocracy and vowed to crush any challenges to the Communist Partyor "social stability".5.
The CDP vowed to keep working, as more leaders awaited arrest.Fig.
3.
A sample summaryTable 7 gives the ordering generated by various models.Models Output ?
valuesPr 4 0 1 3 5 2 0.20Mo 1 4 3 0 2 5 0.20Fa 0 1 4 3 5 2 0.47Ca 1 2 0 3 4 5 0.73Table 7.
Comparison: an exampleFrom Table 7, we have several findings.
First, sentence 3, 4and 5 were close in the sequence in terms of their adjacencyvalues, so in both Fa and Ca, once one of them was selected,the other two would follow.
However, the closeness betweenthem was not reflected in both Pr and Mo.
Second, while Cacorrectly made 1 followed by 2, Fa didn?t.
The reason may bethat although sentence 1 and 2 had higher cluster-adjacencyvalue, their feature-adjacency value may be lower than thatbetween sentence 1 and 4, since sentence 1 and 4 sharedmore features, and only considering a limited number offeatures may make them get higher feature-adjacency value.At the same time, during classification in Ca, other differentfeatures (other than ?China?, ?democracy?, etc) would cometo distinguish between sentence 1 and 4, so cluster centers ofsentence 1 and 4 would have bias toward the distinguishingfeatures.
Thus, their adjacency value tended to be lower inCa, and in fact, they fell apart in the sequence.
Third, Fasuccessfully got the first sentence, while Ca didn?t.
To seethe reason, we checked the summaries, and found that somesummaries started with theme 0 and some more with theme 1,since theme 1 had part of the features in theme 0 and theymay have contribution to feature-adjacency value, topic 1749tended to have higher feature-adjacency value.
This is notcontradicting with higher cluster-adjacency value betweentheme Null and theme 1.
In fact, we found putting sentence 1at the beginning was also acceptable subjectively.In manual evaluation, the number of inversions was definedas the minimal number of interchanges of adjacent objects totransfer the output ordering to an acceptable ordering judgedby human.
We have three people participating in theevaluation, and the minimal, maximal and average numbersof interchanges for each summary among the three personswere selected for evaluation respectively.
The Kendall?s ?
ofall 5 runs are listed in Table 8.?
valuesModels Average  Minimal  MaximalRd 0.106 0.202 0.034Mo 0.453 0.543 0.345Pr 0.465 0.524 0.336Fa 0.597 0.654 0.423Ca 0.665 0.723 0.457Table 8.
Manual evaluation results on 10 summariesFrom table 7, we can find that all models get higherKendall?s ?
values than in automatic evaluation, and the twoadjacency models achieved better results than Pr and Moaccording to the three measures.
As example, Table 9 lists thesubjective evaluation for the sample summary in Fig.
3.Models Output Subjectiveordering?
valuesPr 4 0 1 3 5 2 401235 0.73Mo 1 4 3 0 2 5 140235 0.73Fa 0 1 4 3 5 2 014235 0.73Ca 1 2 0 3 4 5 120345 1.0Table 9.
Subjective evaluation: an example6.
Conclusion and Future WorkIn this paper we propose a cluster-adjacency based model forsentence ordering in multi-document summarization.
It learnsadjacency information of sentences from the sourcedocuments and order sentences accordingly.
Compared withthe feature-adjacency model, the cluster-adjacency methodproduces sentence adjacency from cluster adjacency.
Themajor advantage of this method is that it focuses on a kind ofglobal adjacency (cluster on the whole), and avoidssensitivity to limited number of features, which in general isdifficult.
In addition, with semi-supervised classification, thismethod is expected to determine appropriate themes in sourcedocuments and achieve better performance.Although the cluster-adjacency based ordering modelsolved the problem of noise elimination required by thefeature-adjacency based ordering, how to set anotherparameter properly, i.e., the window range, is still unclear.We guess it may depend on length of source documents.
Thelonger the source documents are, the bigger adjacencywindow size may be expected.
But more experiments areneeded to prove it.In addition, the adjacency based model mainly uses onlyadjacency information to order sentences.
Although itappears to perform better than models using only sequentialinformation on DUC2004 data set, if some sequentialinformation could be learned definitely from the sourcedocuments, it should be better to combine the adjacencyinformation and sequential information.ReferenceRegina Barzilay, Noemie Elhadad, and Kathleen R. McKeown.
2001.Sentence ordering in multidocument summarization.
Proceedings of theFirst International Conference on Human Language Technology Research(HLT-01), San Diego, CA, 2001, pp.
149?156..Barzilay, R N. Elhadad, and K. McKeown.
2002.
Inferring strategies forsentence ordering in multidocument news summarization.
Journal ofArtificial Intelligence Research, 17:35?55.Sasha Blair-Goldensohn, David Evans.
Columbia University at DUC 2004.In Proceedings of the 4th Document Understanding Conference (DUC2004).
May, 2004.Danushka Bollegala, Naoaki Okazaki, Mitsuru Ishizuka.
2005.
A machinelearning approach to sentence ordering for multidocument summarizationand it?s evaluation.
IJCNLP 2005, LNAI 3651, pages 624-635, 2005.McKeown K., Barzilay R. Evans D., Hatzivassiloglou V., Kan M., SchiffmanB., &Teufel, S. (2001).
Columbia multi-document summarization:Approach and evaluation.
In Proceedings of DUC.Mirella Lapata.
Probabilistic text structuring: Experiments with sentenceordering.
Proceedings of the annual meeting of ACL, 2003., pages 545?552, 2003.Nie Yu, Ji Donghong and Yang Lingpeng.
An adjacency model for sentenceordering in multi-document Asian Information Retrieval Symposium(AIRS2006), Singapore., Oct. 2006.Advaith Siddharthan, Ani Nenkova and Kathleen McKeown.
SyntacticSimplication for Improving Content Selection in Multi-DocumentSummarization.
In Proceeding of COLING 2004, Geneva, Switzerland.Tishby, N, Slonim, N. (2000) Data clustering by Markovian relaxation andthe Information Bottleneck Method.
NIPS 13.Szummer M. and T. Jaakkola.
(2001) Partially labeled classification withmarkov random walks.
NIPS14.Zhu, X., Ghahramani, Z., & Lafferty, J.
(2003) Semi-Supervised LearningUsing Gaussian Fields and Harmonic Functions.
ICML-2003.Zhou D., Bousquet, O., Lal, T.N., Weston J.
& Schokopf B.
(2003).
Learningwith local and Global Consistency.
NIPS 16. pp: 321-328750
