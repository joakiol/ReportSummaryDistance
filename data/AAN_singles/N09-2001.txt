Proceedings of NAACL HLT 2009: Short Papers, pages 1?4,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsCohesive Constraints in A Beam Search Phrase-based DecoderNguyen Bach and Stephan VogelLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{nbach, stephan.vogel}@cs.cmu.eduColin CherryMicrosoft ResearchOne Microsoft WayRedmond, WA, 98052, USAcollinc@microsoft.comAbstractCohesive constraints allow the phrase-based decoderto employ arbitrary, non-syntactic phrases, and en-courage it to translate those phrases in an order thatrespects the source dependency tree structure.
Wepresent extensions of the cohesive constraints, suchas exhaustive interruption count and rich interrup-tion check.
We show that the cohesion-enhanced de-coder significantly outperforms the standard phrase-based decoder on English?Spanish.
Improvementsbetween 0.5 and 1.2 BLEU point are obtained onEnglish?Iraqi system.1 IntroductionPhrase-based machine translation is driven by a phrasaltranslation model, which relates phrases (contiguous seg-ments of words) in the source to phrases in the tar-get.
This translation model can be derived from a word-aligned bitext.
Translation candidates are scored accord-ing to a linear model combining several informative fea-ture functions.
Crucially, this model incorporates trans-lation model scores and n-gram language model scores.The component features are weighted to minimize atranslation error criterion on a development set (Och,2003).
Decoding the source sentence takes the form ofa beam search through the translation space, with inter-mediate states corresponding to partial translations.
Thedecoding process advances by extending a state with thetranslation of a source phrase, until each source word hasbeen translated exactly once.
Re-ordering occurs whenthe source phrase to be translated does not immediatelyfollow the previously translated phrase.
This is penalizedwith a discriminatively-trained distortion penalty.
In or-der to calculate the current translation score, each statecan be represented by a triple:?
A coverage vector HC indicates which source wordshave already been translated.?
A span f?
indicates the last source phrase translatedto create this state.?
A target word sequence stores context needed by thetarget language model.As cohesion concerns only movement in the source, wecan completely ignore the language model context, mak-ing state effectively an (f?
,HC ) tuple.To enforce cohesion during the state expansion pro-cess, cohesive phrasal decoding has been proposed in(Cherry, 2008; Yamamoto et al, 2008).
The cohesion-enhanced decoder enforces the following constraint: oncethe decoder begins translating any part of a source sub-tree, it must cover all the words under that subtree beforeit can translate anything outside of it.
This notion can beapplied to any projective tree structure, but we use de-pendency trees, which have been shown to demonstrategreater cross-lingual cohesion than other structures (Fox,2002).
We use a tree data structure to store the depen-dency tree.
Each node in the tree contains surface wordform, word position, parent position, dependency typeand POS tag.
We use T to stand for our dependency tree,and T (n) to stand for the subtree rooted at node n. Eachsubtree T (n) covers a span of contiguous source words;for subspan f?
covered by T (n), we say f?
?
T (n).Cohesion is checked as we extend a state (f?h,HC h)with the translation of f?h+1, creating a new state(f?h+1,HC h+1).
Algorithm 1 presents the cohesioncheck described by Cherry (2008).
Line 2 selects focalpoints, based on the last translated phrase.
Line 4 climbsfrom each focal point to find the largest subtree that needsto be completed before the translation process can moveelsewhere in the tree.
Line 5 checks each such subtreefor completion.
Since there are a constant number of fo-cal points (always 2) and the tree climb and completionchecks are both linear in the size of the source, the entirecheck can be shown to take linear time.The selection of only two focal points is motivated bya ?violation free?
assumption.
If one assumes that the1Algorithm 1 Interruption Check (Coh1) (Cherry, 2008)Input: Source tree T , previous phrase f?h, currentphrase f?h+1, coverage vector HC1: Interruption ?
False2: F ?
the left and right-most tokens of f?h3: for each of f ?
F do4: Climb the dependency tree from f until you reachthe highest node n such that f?h+1 /?
T (n).5: if n exists and T (n) is not covered in HCh+1then6: Interruption ?
True7: end if8: end for9: Return InterruptionFigure 1: A candidate translation where Coh1 does not firetranslation represented by (f?h,HC h) contains no cohe-sion violations, then checking only the end-points of f?his sufficient to maintain cohesion.
However, once a softcohesion constraint has been implemented, this assump-tion no longer holds.2 Extensions of Cohesive Constraints2.1 Exhaustive Interruption Check (Coh2)Because of the ?violation free?
assumption, Algorithm 1implements the design decision to only suffer a violationpenalty once, when cohesion is initially broken.
How-ever, this is not necessarily the best approach, as the de-coder does not receive any further incentive to return tothe partially translated subtree and complete it.For example, Figure 1 illustrates a translation candi-date of the English sentence ?the presidential electionof the united states begins tomorrow?
into French.
Weconsider f?4 = ?begins?, f?5 = ?tomorrow?.
The decoderalready translated ?the presidential election?
making thecoverage vector HC 5 = ?1 1 1 0 0 0 0 1 1?.
Algorithm 1tells the decoder that no violation has been made by trans-lating ?tomorrow?
while the decoder should be informedthat there exists an outstanding violation.
Algorithm 1found the violation when the decoder previously jumpedfrom ?presidential?
to ?begins?, and will not find anotherviolation when it jumps from ?begins?
to ?tomorrow?.Algorithm 2 is a modification of Algorithm 1, chang-ing only line 2.
The resulting system checks all previ-Algorithm 2 Exhaustive Interruption Check (Coh2)Input: Source tree T , previous phrase fh, currentphrase fh+1, coverage vector HC1: Interruption ?
False2: F ?
{f |HCh(f) = 1}3: for each of f ?
F do4: Climb the dependency tree from f until you reachthe highest node n such that f?h+1 /?
T (n).5: if n exists and T (n) is not covered in HC h+1then6: Interruption ?
True7: end if8: end for9: Return InterruptionAlgorithm 3 Interruption Count (Coh3)Input: Source tree T , previous phrase f?h, currentphrase f?h+1, coverage vector HC1: ICount ?
02: F ?
the left and right-most tokens of f?h3: for each of f ?
F do4: Climb the dependency tree from f until you reachthe highest node n such that f?h+1 /?
T (n).5: if n exists then6: for each of e ?
T (n) and HCh+1(e) = 0 do7: ICount = ICount+ 18: end for9: end if10: end for11: Return ICountously covered tokens, instead of only the left and right-most tokens of f?h+1, and therefore makes no violation-free assumption.
For the example above, Algorithm 2will inform the decoder that translating ?tomorrow?
alsoincurs a violation.
Because |F | is no longer constant,the time complexity of Coh2 is worse than Coh1.
How-ever, we can speed up the interruption check algorithmby hashing cohesion checks, so we only need to run Al-gorithm 2 once per (f?h+1,HC h+1) .2.2 Interruption Count (Coh3) and ExhaustiveInterruption Count (Coh4)Algorithm 1 and 2 described above interpret an inter-ruption as a binary event.
As it is possible to leave severalwords untranslated with a single jump, some interrup-tions may be worse than others.
To implement this obser-vation, an interruption count is used to assign a penaltyto cohesion violations, based on the number of words leftuncovered in the interrupted subtree.
We initialize the in-terruption count with zero.
At any search state when thecohesion violation is detected the count is incremented by2Algorithm 4 Exhaustive Interruption Count (Coh4)Input: Source tree T , previous phrase fh, currentphrase fh+1, coverage vector HC1: ICount ?
02: F ?
{f |HCh(f) = 1}3: for each of f ?
F do4: Climb the dependency tree from f until you reachthe highest node n such that f?h+1 /?
T (n).5: if n exists then6: for each of e ?
T (n) and HCh+1(e) = 0 do7: ICount = ICount+ 18: end for9: end if10: end for11: Return ICountone.
The modification of Algorithm 1 and 2 lead to Inter-ruption Count (Coh3) and Exhaustive Interruption Count(Coh4) algorithms, respectively.
The changes only hap-pen in lines 1, 5 and 6.
We use an additional bit vectorto make sure that if a node has been reached once duringan interruption check, it should not be counted again.
Forthe example in Section 2.1, Algorithm 4 will return 4 forICount (?of?
; ?the?
; ?united?
; ?states?
).2.3 Rich Interruption Constraints (Coh5)The cohesion constraints in Sections 2.1 and 2.2 do notleverage node information in the dependency tree struc-tures.
We propose the rich interruption constraints (Coh5)algorithm to combine four constraints which are Interrup-tion, Interruption Count, Verb Count and Noun Count.The first two constraints are identical to what was de-scribed above.
Verb and Noun count constraints are en-forcing the following rule: a cohesion violation will bepenalized more in terms of the number of verb and nounwords that have not been covered.
For example, we wantto translate the English sentence ?the presidential elec-tion of the united states begins tomorrow?
to French withthe dependency structure as in Figure 1.
We consider f?h= ?the united states?, f?h+1 = ?begins?.
The coverage bitvector HC h+1 is ?0 0 0 0 1 1 1 1 0?.
Algorithm 5 will re-turn true for Interruption, 4 for ICount (?the?
; ?pres-idential?
; ?election?
; ?of?
), 0 for V erbCount and 1 forNounCount (?election?
).3 ExperimentsWe built baseline systems using GIZA++ (Och and Ney,2003), Moses?
phrase extraction with grow-diag-final-end heuristic (Koehn et al, 2007), a standard phrase-based decoder (Vogel, 2003), the SRI LM toolkit (Stol-cke, 2002), the suffix-array language model (Zhang andVogel, 2005), a distance-based word reordering modelAlgorithm 5 Rich Interruption Constraints (Coh5)Input: Source tree T , previous phrase f?h, currentphrase f?h+1, coverage vector HC1: Interruption ?
False2: ICount, V erbCount,NounCount ?
03: F ?
the left and right-most tokens of f?h4: for each of f ?
F do5: Climb the dependency tree from f until you reachthe highest node n such that f?h+1 /?
T (n).6: if n exists then7: for each of e ?
T (n) and HCh+1(e) = 0 do8: Interruption ?
True9: ICount = ICount+ 110: if POS of e is ?VB?
then11: V erbCount ?
V erbCount+ 112: else if POS of e is ?NN?
then13: NounCount ?
NounCount+ 114: end if15: end for16: end if17: end for18: Return Interruption, ICount, V erbCount,NounCountwith a window of 3, and the maximum number of targetphrases restricted to 10.
Results are reported using low-ercase BLEU (Papineni et al, 2002).
All model weightswere trained on development sets via minimum-error ratetraining (MERT) (Och, 2003) with 200 unique n-best listsand optimizing toward BLEU.
We used the MALT parser(Nivre et al, 2006) to obtain source English dependencytrees and the Stanford parser for Arabic (Marneffe et al,2006).
In order to decide whether the translation outputof one MT engine is significantly better than another one,we used the bootstrap method (Zhang et al, 2004) with1000 samples (p < 0.05).
We perform experiments onEnglish?Iraqi and English?Spanish.
Detailed corpusstatistics are shown in Table 1.
Table 2 shows results inlowercase BLEU and bold type is used to indicate high-est scores.
An italic text indicates the score is statisticallysignificant better than the baseline.English?Iraqi English?SpanishEnglish Iraqi English Spanishsentence pairs 654,556 1,310,127unique sent.
pairs 510,314 1,287,016avg.
sentence length 8.4 5.9 27.4 28.6# words 5.5 M 3.8 M 35.8 M 37.4 Mvocabulary 34 K 109 K 117 K 173 KTable 1: Corpus statisticsOur English-Iraqi data come from the DARPATransTac program.
We used TransTac T2T July 20073English?Iraqi English?Spanishjuly07 june08 ncd07 nct07Baseline 31.58 23.58 33.18 32.04+Coh1 32.63 24.45 33.49 32.72+Coh2 32.51 24.73 33.52 32.81+Coh3 32.43 24.19 33.37 32.87+Coh4 32.32 24.66 33.47 33.20+Coh5 31.98 24.42 33.54 33.27Table 2: Scores of baseline and cohesion-enhanced systems onEnglish?Iraqi and English?Spanish systems(july07) as the development set and TransTac T2T June2008 (june08) as the held-out evaluation set.
Each test sethas 4 reference translation.
We applied the suffix-arrayLM up to 6-gram with Good-Turing smoothing.
Our co-hesion constraints produced improvements ranging be-tween 0.5 and 1.2 BLEU point on the held-out evaluationset.We used the Europarl and News-Commentary parallelcorpora for English?Spanish as provided in the ACL-WMT 2008 shared task evaluation.
The baseline sys-tem used the parallel corpus restricting sentence lengthto 100 words for word alignment and a 4-gram SRILM with modified Kneyser-Ney smoothing.
We usednc-devtest2007(ncd07) as the development set and nc-test2007(nct07) as the held-out evaluation set.
Each testset has 1 translation reference.
We obtained improve-ments ranging between 0.7 and 1.2 BLEU.
All cohesionconstraints perform statistically significant better than thebaseline on the held-out evaluation set.4 ConclusionsIn this paper, we explored cohesive phrasal decoding, fo-cusing on variants of cohesive constraints.
We proposedfour novel cohesive constraints namely exhaustive inter-ruption check (Coh2), interruption count (Coh3), exhaus-tive interruption count (Coh4) and rich interruption con-straints (Coh5).
Our experimental results show that withcohesive constraints the system generates better transla-tions in comparison with strong baselines.
To ensure therobustness and effectiveness of the proposed approaches,we conducted experiments on 2 different language pairs,namely English?Iraqi and English?Spanish.
These ex-periments also covered a wide range of training corpussizes, ranging from 600K sentence pairs up to 1.3 mil-lion sentence pairs.
All five proposed approaches givepositive results.
The improvements on English?Spanishare statistically significant at the 95% level.
We observea consistent pattern indicating that the improvements arestable in both language pairs.AcknowledgmentsThis work is in part supported by the US DARPA TransTac pro-grams.
Any opinions, findings, and conclusions or recommen-dations expressed in this material are those of the authors anddo not necessarily reflect the views of DARPA.
We would liketo thank Qin Gao and Matthias Eck for helpful conversations,Johan Hall and Joakim Nirve for helpful suggestions on train-ing and using the English dependency model.
We also thanksthe anonymous reviewers for helpful comments.ReferencesColin Cherry.
2008.
Cohesive phrase-based decoding for statis-tical machine translation.
In Proceedings of ACL-08: HLT,pages 72?80, Columbus, Ohio, June.
Association for Com-putational Linguistics.Heidi J.
Fox.
2002.
Phrasal cohesion and statistical machinetranslation.
In Proceedings of EMNLP?02, pages 304?311,Philadelphia, PA, July 6-7.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan,Wade Shen, Christine Moran, Richard Zens, Chris Dyer, On-drej Bojar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machine transla-tion.
In Proceedings of ACL?07, pages 177?180, Prague,Czech Republic, June.Marie-Catherine Marneffe, Bill MacCartney, and ChristopherManning.
2006.
Generating typed dependency parses fromphrase structure parses.
In Proceedings of LREC?06, Genoa,Italy.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
MaltParser:A data-driven parser-generator for dependency parsing.
InProceedings of LREC?06, Genoa, Italy.Franz J. Och and Hermann Ney.
2003.
A systematic compar-ison of various statistical alignment models.
In Computa-tional Linguistics, volume 1:29, pages 19?51.Franz Josef Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proceedings of ACL?03, pages160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
BLEU: A method for automatic evaluation ofmachine translation.
In Proceedings of ACL?02, pages 311?318, Philadelphia, PA, July.Andreas Stolcke.
2002.
SRILM ?
An extensible language mod-eling toolkit.
In Proc.
Intl.
Conf.
on Spoken Language Pro-cessing, volume 2, pages 901?904, Denver.Stephan Vogel.
2003.
SMT decoder dissected: Word reorder-ing.
In Proceedings of NLP-KE?03, pages 561?566, Bejing,China, Oct.Hirofumi Yamamoto, Hideo Okuma, and Eiichiro Sumita.2008.
Imposing constraints from the source tree on ITGconstraints for SMT.
In Proceedings of the ACL-08: HLT,SSST-2, pages 1?9, Columbus, Ohio, June.
Association forComputational Linguistics.Ying Zhang and Stephan Vogel.
2005.
An efficient phrase-to-phrase alignment model for arbitrarily long phrase and largecorpora.
In Proceedings of EAMT?05, Budapest, Hungary,May.
The European Association for Machine Translation.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.
Inter-preting BLEU/NIST scores: How much improvement do weneed to have a better system?
In Proceedings of LREC?04,pages 2051?2054.4
