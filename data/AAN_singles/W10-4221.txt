Cross-Linguistic Attribute Selection for REG:Comparing Dutch and EnglishMarie?t TheuneUniversity of TwenteThe NetherlandsM.Theune@utwente.nlRuud KoolenTilburg UniversityThe NetherlandsR.M.F.Koolen@uvt.nlEmiel KrahmerTilburg UniversityThe NetherlandsE.J.Krahmer@uvt.nlAbstractIn this paper we describe a cross-linguisticexperiment in attribute selection for refer-ring expression generation.
We used agraph-based attribute selection algorithmthat was trained and cross-evaluated onEnglish and Dutch data.
The results indi-cate that attribute selection can be done ina largely language independent way.1 IntroductionA key task in natural language generation is refer-ring expression generation (REG).
Most work onREG is aimed at producing distinguishing descrip-tions: descriptions that uniquely characterize a tar-get object in a visual scene (e.g., ?the red sofa?
),and do not apply to any of the other objects in thescene (the distractors).
The first step in generatingsuch descriptions is attribute selection: choosing anumber of attributes that uniquely characterize thetarget object.
In the next step, realization, the se-lected attributes are expressed in natural language.Here we focus on the attribute selection step.
Weinvestigate to which extent attribute selection canbe done in a language independent way; that is,we aim to find out if attribute selection algorithmstrained on data from one language can be success-fully applied to another language.
The languageswe investigate are English and Dutch.Many REG algorithms require training data, be-fore they can successfully be applied to generatereferences in a particular domain.
The Incremen-tal Algorithm (Dale and Reiter, 1995), for exam-ple, assumes that certain attributes are more pre-ferred than others, and it is assumed that determin-ing the preference order of attributes is an empir-ical matter that needs to be settled for each newdomain.
The graph-based algorithm (Krahmer etal., 2003), to give a second example, similarlyassumes that certain attributes are preferred (are?cheaper?)
than others, and that data are requiredto compute the attribute-cost functions.Traditional text corpora have been argued to beof restricted value for REG, since these typicallyare not ?semantically transparent?
(van Deemteret al, 2006).
Rather what seems to be needed isdata collected from human participants, who pro-duce referring expressions for specific targets insettings where all properties of the target and itsdistractors are known.
Needless to say, collectingand annotating such data takes a lot of time and ef-fort.
So what to do if one wants to develop a REGalgorithm for a new language?
Would this requirea new data collection, or could existing data col-lected for a different language be used?
Clearly,linguistic realization is language dependent, but towhat extent is attribute selection language depen-dent?
This is the question addressed in this paper.Below we describe the English and Dutch cor-pora used in our experiments (Section 2), thegraph-based algorithm we used for attribute se-lection (Section 3), and the corpus-based attributecosts and orders used by the algorithm (Section 4).We present the results of our cross-linguistic at-tribute selection experiments (Section 5) and endwith a discussion and conclusions (Section 6).2 Corpora2.1 English: the TUNA CorpusFor English data, we used the TUNA corpus ofobject descriptions (Gatt et al, 2007).
This cor-pus was created by presenting the participants inan on-line experiment with a visual scene consist-ing of seven objects and asking them to describeone of the objects, the target, in such a way that itcould be uniquely identified.
There were two ex-perimental conditions: in the +LOC condition, theparticipants were free to describe the target objectusing any of its properties, including its locationon the screen, whereas in the -LOC condition theywere discouraged (but not prevented) from men-tioning object locations.
The resulting object de-scriptions were annotated using XML and com-bined with an XML representation of the visualscene, listing all objects and their properties interms of attribute-value pairs.
The TUNA corpusis split into two domains: one with descriptions offurniture and one with descriptions of people.The TUNA corpus was used for the comparativeevaluation of REG systems in the TUNA Chal-lenges (2007-2009).
For our current experiments,we used the TUNA 2008 Challenge training anddevelopment sets (Gatt et al, 2008) to train andevaluate the graph-based algorithm on.2.2 Dutch: the D-TUNA CorpusFor Dutch, we used the D(utch)-TUNA corpus ofobject descriptions (Koolen and Krahmer, 2010).The collection of this corpus was inspired by theTUNA experiment described above, and was doneusing the same visual scenes.
There were threeconditions: text, speech and face-to-face.
Thetext condition was a replication (in Dutch) of theTUNA experiment: participants typed identify-ing descriptions of target referents, distinguishingthem from distractor objects in the scene.
In theother two conditions participants produced spo-ken descriptions for an addressee, who was eithervisible to the speaker (face-to-face condition) ornot (speech condition).
The resulting descriptionswere annotated semantically using the XML anno-tation scheme of the English TUNA corpus.The procedure in the D-TUNA experiment dif-fered from that used in the original TUNA exper-iment in two ways.
First, the D-TUNA experi-ment used a laboratory-based set-up, whereas theTUNA study was conducted on-line in a relativelyuncontrolled setting.
Second, participants in theD-TUNA experiment were completely preventedfrom mentioning object locations.3 Graph-Based Attribute SelectionFor attribute selection, we use the graph-based al-gorithm of Krahmer et al (2003), one of thehighest scoring attribute selection methods in theTUNA 2008 Challenge (Gatt et al (2008), table11).
In this approach, a visual scene with tar-get and distractor objects is represented as a la-belled directed graph, in which the objects aremodelled as nodes and their properties as loopingedges on the corresponding nodes.
To select theattributes for a distinguishing description, the al-gorithm searches for a subgraph of the scene graphthat uniquely refers to the target referent.
Startingfrom the node representing the target, it performs adepth-first search over the edges connected to thesubgraph found so far.
The algorithm?s output isthe cheapest distinguishing subgraph, given a par-ticular cost function that assigns costs to attributes.By assigning zero costs to some attributes, e.g.,the type of an object, the human tendency to men-tion redundant attributes can be mimicked.
How-ever, as shown by Viethen et al (2008), merelyassigning zero costs to an attribute is not a suffi-cient condition for inclusion; if the graph searchterminates before the free attributes are tried, theywill not be included.
Therefore, the order in whichattributes are tried must be explicitly controlled.Thus, when using the graph-based algorithm forattribute selection, two things must be specified:(1) the cost function, and (2) the order in which theattributes should be searched.
Both can be basedon corpus data, as described in the next section.4 Costs and OrdersFor our experiments, we used the graph-based at-tribute selection algorithm with two types of costfunctions: Stochastic costs and Free-Na?
?ve costs.Both reflect (to a different extent) the relative at-tribute frequencies found in a training corpus: themore frequently an attribute occurs in the trainingdata, the cheaper it is in the cost functions.Stochastic costs are directly based on the at-tribute frequencies in the training corpus.
Theyare derived by rounding ?log2(P (v)) to the firstdecimal and multiplying by 10, where P (v) is theprobability that attribute v occurs in a description,given that the target object actually has this prop-erty.
The probability P (v) is estimated by deter-mining the frequency of each attribute in the train-ing corpus, relative to the number of target ob-jects that possess this attribute.
Free-Na?
?ve costsmore coarsely reflect the corpus frequencies: veryfrequent attributes are ?free?
(cost 0), somewhatfrequent attributes have cost 1 and infrequent at-tributes have cost 2.
Both types of cost functionsare used in combination with a stochastic ordering,where attributes are tried in the order of increasingstochastic costs.In total, four cost functions were derived fromthe English corpus data and four cost functions de-rived from the Dutch corpus data.
For each lan-guage, we had two Stochastic cost functions (onefor the furniture domain and one for the people do-main), and two Free-Na?
?ve cost functions (idem),giving eight different cost functions in total.
Foreach language we determined two attribute ordersto be used with the cost functions: one for the fur-niture domain and one for the people domain.4.1 English Costs and OrderFor English, we used the Stochastic and Free-Na?
?ve cost functions and the stochastic order fromKrahmer et al (2008).
The Stochastic costsand order were derived from the attribute frequen-cies in the combined training and developmentsets of the TUNA 2008 Challenge (Gatt et al,2008), containing 399 items in the furniture do-main and 342 items in the people domain.
TheFree-Na?
?ve costs are simplified versions of thestochastic costs.
?Free?
attributes are TYPE inboth domains, COLOUR for the furniture domainand HASBEARD and HASGLASSES for the peopledomain.
Expensive attributes (cost 2) are X- andY-DIMENSION in the furniture domain and HAS-SUIT, HASSHIRT and HASTIE in the people do-main.
All other attributes have cost 1.4.2 Dutch Costs and OrderThe Dutch Stochastic costs and order were de-rived from the attribute frequencies in a set of 160items (for both furniture and people) randomly se-lected from the text condition in the D-TUNA cor-pus.
Interestingly, our Stochastic cost computa-tion method led to an assignment of 0 costs tothe COLOUR attribute in the furniture domain, thusenabling the Dutch Stochastic cost function to in-clude colour as a redundant property in the gener-ated descriptions.
In the English stochastic costs,none of the attributes are free.
Another differenceis that in the furniture domain, the Dutch stochas-tic costs for ORIENTATION attributes are muchlower than the English costs (except with valueFRONT); in the people domain, the same holds forattributes such as HASSUIT and HASTIE.
Thesecost differences, which are largely reflected in theDutch Free-Na?
?ve costs, do not seem to be causedby differences in expressibility, i.e., the ease withwhich the attributes can be expressed in the twolanguages (Koolen et al, 2010); rather, they maybe due to the fact that the human descriptions in D-TUNA do not include any DIMENSION attributes.Language Furniture PeopleTraining Test Dice Acc.
Dice Acc.Dutch Dutch 0.92 0.63 0.78 0.28English 0.83 0.55 0.73 0.29English Dutch 0.87 0.58 0.75 0.25English 0.67 0.29 0.67 0.24Table 1: Evaluation results for stochastic costs.Language Furniture PeopleTraining Test Dice Acc.
Dice Acc.Dutch Dutch 0.94 0.70 0.78 0.28English 0.83 0.55 0.73 0.29English Dutch 0.94 0.70 0.78 0.28English 0.83 0.55 0.73 0.29Table 2: Evaluation results for Free-Na?
?ve costs.5 ResultsAll cost functions were applied to both Dutch andEnglish test data.
As Dutch test data, we used a setof 40 furniture items and a set of 40 people items,randomly selected from the text condition in theD-TUNA corpus.
These items had not been usedfor training the Dutch cost functions.
As Englishtest data, we used a subset of the TUNA 2008 de-velopment set (Gatt et al, 2008).
To make the En-glish test data comparable to the Dutch ones, weonly included items from the -LOC condition (seeSection 2.1).
This resulted in 38 test items for thefurniture domain, and 38 for the people domain.Tables 1 and 2 show the results of applying theDutch and English cost functions (with Dutch andEnglish attribute orders respectively) to the Dutchand English test data.
The evaluation metrics used,Dice and Accuracy (Acc.
), both evaluate human-likeness by comparing the automatically selectedattribute sets to those in the human test data.
Diceis a set-comparison metric ranging between 0 and1, where 1 indicates a perfect match between sets.Accuracy is the proportion of system outputs thatexactly match the corresponding human data.
Theresults were computed using the ?teval?
evaluationtool provided to participants in the TUNA 2008Challenge (Gatt et al, 2008).To determine significance, we applied repeatedmeasures analyses of variance (ANOVA) to theevaluation results, with three within factors: train-ing language (Dutch or English), cost function(Stochastic or Free-Na?
?ve), and domain (furnitureor people), and one between factor representingtest language (Dutch or English).An overall effect of cost function shows that theFree-Na?
?ve cost functions generally perform betterthan the Stochastic cost functions (Dice: F(1,76) =34.853, p < .001; Accuracy: F(1,76) = 13.052, p =.001).
Therefore, in the remainder of this sectionwe mainly focus on the results for the Free-Na?
?vecost functions (Table 2).As can be clearly seen in Table 2, Dutch andEnglish Free-Na?
?ve cost functions give almost thesame scores in both the furniture and the peopledomain, when applied to the same test language.The English Free-Na?
?ve cost function performsslightly better than the Dutch one on the Dutchpeople data, but this difference is not significant.An overall effect of test language shows that thecost functions (both Stochastic and Free-Na?
?ve)generally give better Dice results on the Dutchdata than for the English data (Dice: F(1,76) =7.797, p = .007).
In line with this, a two-way in-teraction between test language and training lan-guage (Dice: F(1,76) = 6.870, p = .011) shows thatboth the Dutch and the English cost functions per-form better on the Dutch data than on the Englishdata.
However, the overall effect of test languagedid not reach significance for Accuracy, presum-ably due to the fact that the Accuracy scores on theEnglish people data are slightly higher than thoseon the Dutch people data.Finally, the cost functions generally performbetter in the furniture domain than in the peopledomain (Dice: F(1,76) = 10.877, p = .001; Accu-racy: F(1,76) = 16.629, p < .001).6 DiscussionThe results of our cross-linguistic attribute selec-tion experiments show that Free-Na?
?ve cost func-tions, which only roughly reflect the attribute fre-quencies in the training corpus, have an overallbetter performance than Stochastic cost functions,which are directly based on the attribute frequen-cies.
This holds across the two languages we in-vestigated, and corresponds with the findings ofKrahmer et al (2008), who compared Stochas-tic and Free-Na?
?ve functions that were trained andevaluated on English data only.
The difference inperformance is probably due to the fact that Free-Na?
?ve costs are less sensitive to the specifics ofthe training data (and are therefore more generallyapplicable) and do a better job of mimicking thehuman tendency towards redundancy.Moreover, we found that Free-Na?
?ve cost func-tions trained on different languages (English orDutch) performed equally well when tested on thesame data (English or Dutch), in both the furnitureand people domain.
This suggests that attributeselection can in fact be done in a language inde-pendent way, using cost functions that have beenderived from corpus data in one language to per-form attribute selection for another language.Our results did show an effect of test languageon performance: both English and Dutch costfunctions performed better when tested on theDutch D-TUNA data than on the English TUNAdata.
However, this difference does not seem tobe caused by language-specific factors but ratherby the quality of the respective test sets.
Althoughthe English test data were restricted to the -LOCcondition, in which using DIMENSION attributeswas discouraged, still more than 25% of the En-glish test data (both furniture and people) includedone or more DIMENSION attributes, which werenever selected for inclusion by either the Englishor the Dutch Free-Na?
?ve cost functions.
The Dutchtest data, on the other hand, did not include anyDIMENSION attributes.
In addition, the Englishtest data contained more non-unique descriptionsof target objects than the Dutch data, in particu-lar in the furniture domain.
These differences maybe due to the fact that data collection was donein a more controlled setting for D-TUNA than forTUNA.
In other words, the seeming effect of testlanguage does not contradict our main conclusionthat attribute selection is largely language inde-pendent, at least for English and Dutch.The success of our cross-linguistic experimentsmay have to do with the fact that English andDutch hardly differ in the expressibility of objectattributes (Koolen et al, 2010).
To determine thefull extent to which attribute selection can be donein a language-dependent way, additional experi-ments with less similar languages are necessary.AcknowledgementsWe thank the TUNA Challenge organizers for theEnglish data and the evaluation tool used in ourexperiments; Martijn Goudbeek for helping withthe statistical analysis; and Pascal Touset, IvoBrugman, Jette Viethen, and Iris Hendrickx fortheir contributions to the graph-based algorithm.This research is part of the VICI project ?Bridg-ing the gap between psycholinguistics and com-putational linguistics: the case of referring expres-sions?, funded by the Netherlands Organization forScientific Research (NWO Grant 277-70-007).ReferencesR.
Dale and E. Reiter.
1995.
Computational interpre-tation of the Gricean maxims in the generation of re-ferring expressions.
Cognitive Science, 19(2):233?263.A.
Gatt, I. van der Sluis, and K. van Deemter.
2007.Evaluating algorithms for the generation of refer-ring expressions using a balanced corpus.
In Pro-ceedings of the 11th European Workshop on NaturalLanguage Generation (ENLG 2007), pages 49?56.A.
Gatt, A. Belz, and E. Kow.
2008.
The TUNA Chal-lenge 2008: Overview and evaluation results.
InProceedings of the 5th International Natural Lan-guage Generation Conference (INLG 2008), pages198?206.R.
Koolen and E. Krahmer.
2010.
The D-TUNA cor-pus: A Dutch dataset for the evaluation of referringexpression generation algorithms.
In Proceedingsof the 7th international conference on Language Re-sources and Evaluation (LREC 2010).R.
Koolen, A. Gatt, M. Goudbeek, and E. Krahmer.2010.
Overspecification in referring expressions:Causal factors and language differences.
Submitted.E.
Krahmer, S. van Erk, and A. Verleg.
2003.
Graph-based generation of referring expressions.
Compu-tational Linguistics, 29(1):53?72.E.
Krahmer, M. Theune, J. Viethen, and I. Hendrickx.2008.
Graph: The costs of redundancy in refer-ring expressions.
In Proceedings of the 5th Inter-national Natural Language Generation Conference(INLG 2008), pages 227?229.K.
van Deemter, I. I. van der Sluis, and A. Gatt.
2006.Building a semantically transparent corpus for thegeneration of referring expressions.
In Proceedingsof the 4th International Natural Language Genera-tion Conference (INLG 2006), pages 130?132.J.
Viethen, R. Dale, E. Krahmer, M. Theune, andP.
Touset.
2008.
Controlling redundancy in refer-ring expressions.
In Proceedings of the Sixth In-ternational Conference on Language Resources andEvaluation (LREC 2008), pages 239?246.
