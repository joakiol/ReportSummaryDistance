Proceedings of the Workshop on Embodied Language Processing, pages 67?74,Prague, Czech Republic, June 28, 2007. c?2007 Association for Computational LinguisticsDesign and validation of ECA gestures to improvedialogue system robustnessBeatriz L?pez, ?lvaro Hern?ndez, David D?az,Rub?n Fern?ndez, Luis Hern?ndezGAPS, Signal, Systems and RadiocommunicationsDepartmentUniversidad Polit?cnica de MadridCiudad Universitaria s/n, 28040 Madrid, Spainalvaro@gaps.ssr.upm.esDoroteo TorreATVS, Escuela Polit?cnica SuperiorUniversidad Aut?noma de MadridCiudad Universitaria de Cantoblanco,28049 Madrid, SpainDoroteo.torre@uam.esAbstractIn this paper we present validation teststhat we have carried out on gestures thatwe have designed for an embodied conver-sational agent (ECAs), to assess theirsoundness with a view to applying saidgestures in a forthcoming experiment toexplore the possibilities ECAs can offer toovercome typical robustness problems inspoken language dialogue systems(SLDSs).
The paper is divided into twoparts: First we carry our a literature reviewto acquire a sense of the extent to whichECAs can help overcome user frustrationduring human-machine interaction.
Thenwe associate tentative, yet specific, ECAgestural behaviour with each of the maindialogue stages, with special emphasis onproblem situations.
In the second part wedescribe the tests we have carried out tovalidate our ECA?s gestural repertoire.
Theresults obtained show that users generallyunderstand and naturally accept the ges-tures, to a reasonable degree.
This encour-ages us to proceed with the next stage ofresearch: evaluating the gestural strategy inreal dialogue situations with the aim oflearning about how to favour a more effi-cient and pleasant dialogue flow for the us-ers.1 IntroductionSpoken language dialogue systems and embodiedconversational agents are being introduced in arapidly increasing number of Human-ComputerInteraction (HCI) applications.
The technologiesinvolved in SLDSs (speech recognition, dialoguedesign, etc.)
are mature enough to allow the crea-tion of trustworthy applications.
However, robust-ness problems still arise in concrete limited dia-logue systems because there are many errorsources that may cause the system to performpoorly.
A common example is that users tend torepeat their previous utterance with some frustra-tion when error recovery mechanisms come intoplay, which does not help the recognition process,and as a result using the system seems slow andunnatural (Boyce, 1999).At the same time, embodied conversationalagents (ECAs) are gaining prominence in HCI sys-tems, since they make for more user-friendly ap-plications while increasing communication effec-tiveness.
There are many studies on the effects ?from psychological to efficiency in goal achieve-ment?
ECAs have on users of a variety of applica-tions, see Bickmore et al (2004) and Brave et al(2005), but still very few (Bell and Gustafson,2003) on the impact of ECAs in directed dialoguesituations where robustness is a problem.Our research explores the potential of ECAs toassist in, or resolve, certain difficult dialogue situa-tions that have been identified by various leadingauthors in the field (Cassell and Thorisson, 1999;Cassell and Stone, 1999), as well as a few we our-67selves suggest.
After identifying the problematicsituations of the dialogue we suggest a gesturalstrategy for the ECA to respond to such problemsituations.
Then we propose an experimentalframework, for forthcoming tests, to study the po-tential benefits of adding nonverbal communica-tion in complex dialogue situations.
In the studywe present here we focus on preliminary validationof our gestural repertoire through user tests.
Weconclude by presenting our results and suggestingthe direction our research will take from this point.2 How ECA technology can improve in-teraction with SLDSsThere are many nonverbal elements of communi-cation in everyday life that are important becausethey convey a considerable amount of informationand qualify the spoken message, sometimes evento the extent that what is meant is actually the op-posite of what is said (Krauss et al, 1996).
ECAsoffer the possibility to combine several communi-cation modes such as speech and gestures, makingit possible, in theory, to create interfaces withwhich human-machine interaction is much morenatural and comfortable.
In fact, they are alreadybeing employed to improve interaction (Massaro etal., 2000).These are some common situations with SLDSsin which an ECA could have a positive effect:Efficient turn management: The body languageand expressiveness of agents are important notonly to reinforce the spoken message, but also toregulate the flow of the dialogue, as Cassell pointsout (in Bickmore et al, 2004).Improving error recovery: The process of rec-ognition error recovery usually leads to a certaindegree of user frustration (see Oviatt and VanGent,1996).
Indeed, it is common, once an error occurs,to enter into an error spiral in which the system istrying to recover, the user gets ever more frustrated,and this frustration interferes in the recognitionprocess making the situation worse (Oviatt et al,1998).
ECAs may help reduce frustration, and bydoing so make error recovery more effective (Hone,2005).Correct understanding of the state of the dia-logue: Sometimes the user doesn?t know whetheror not things are going normally (Oviatt, 1994).This sometimes leads the dialogue to error statesthat could be avoided.
The expressive capacity ofECAs could be used to reflect with greater claritythe state the system takes the dialogue to be in.3 Suggesting ECA behaviour for eachdialogue situationA variety of studies have been carried out on be-havioural strategies for embodied conversationalagents (Poggi, 2001; Cassell et al, 2000; Cassell etal., 2001; Chovil, 1992; Kendon, 1990), which dealwith behaviour in hypothetical situations and interms of the informational goals of each particularinteraction (be it human-human or human-machine).
We direct our attention to the overalldialogue systems dynamics, focussing specificallyon typical robustness problems and how to favoursmooth sailing through the different stages of thedialogue.
We draw from existing research under-taken to try to understand the effects different ges-tures displayed by ECAs have on people, and weapply this knowledge to a real dialogue system.
InTable 1 we show the basic set of gestures we areusing as a starting point.
They are based mainly ondescriptions in Bickmore (et al, 2004) and Cassell(et al, 2000), and on recommendations in Casselland Thorisson (1999), Cassell (et al, 2001), Chovil(1992), Kendon (1990) and San-Segundo (et al,2001), to which we have added a few suggestionsof our own.Dialogue stageECA behaviour(movements, gestures and other cues)Initiation(welcoming theuser)1.
Welcome message: look at the camera,smile, wave hand2.
Explanation of the task: zoom in3.
Zoom out, lights dimGive turnLook directly at the user, raise eyebrows.Camera zooms out.
Lights dim.Take turn Look directly at the user, raise hands into ges-ture space.
Camera zooms in.
Light getsbrighter.Wait Slight leaning back, one arm crossed and theother touching the cheek shift of body weightHelpBeat gesture with the hands.
Change of postureError recoverywith correctionLean towards the camera, beat gestureConfirmation(highconfidence)Nod, smile, eyes fully openConfirmation(lowconfidence)Slight leaning of the head to one side, stopsmiling, mildly squintTable 1: Gesture repertoire for the main dialoguestages683.1 InitiationThe inclusion of an ECA at this stage ?humanises?the system (Oviatt and Adams, 2000).
This is aproblem, first because once a user has such highexpectations the system can only end up disap-pointing her, and secondly because the user willtend to use more natural (and thus complex) com-munication, which the system is unable to handle,and the experience will ultimately be frustrating.On the other hand, especially in the case of newusers, contact with a dialoguing animated charactermay have the effect that the user?s level of atten-tion to the actual information that is being given isreduced (Schaumburg, 2001; Catrambone, 2002).Thus the goal is to present a human-like interfacethat is, at the same time, less striking and thus lessdistracting at first contact, and one that clearly?sets the rules?
of the interaction and makes surethat the user keeps it framed within the capabilityof the system.We have designed a welcome gesture for ourECA based on the recommendations in Kendon(1990), to test whether or not it fosters a sense ofease in the user and helps her concentrate on thetask at hand.
Playing with the zoom, the size andthe position of the ECA on the screen may alsoprove to be useful to frame the communication bet-ter (see Table 1).3.2 Turn ManagementTurn management involves two basic actions:taking turn and giving turn.
Again, in Table 1 weshow the corresponding ECA gestures we will starttesting with.
Note that apart from the ECA gestures,we also play with zoom and light intensity: whenit?s the ECA?s turn to speak the camera zooms-inslightly and the light becomes brighter, and whenit?s the user?s turn the camera zooms out and thelights dim.
The idea is that, hopefully, the user willassociate each camera shot and level of light inten-sity with each of the turn modes, and so knowwhen she is expected to speak.The following are some typical examples ofproblem situations together with further considera-tions about ECA behaviour that could help avoidor recover from them:?
The user tries to interrupt at a point atwhich the barge-in feature is not active.
Ifthis happens the system does not processwhat the user has said, and when the systemfinally returns to listening mode there is si-lence from both parts: the system expectsinput from the user, and the user expects ananswer.
Often both finally break the silenceat the same time and the cycle begins again,or, if the system caught part of the user?s ut-terance, a recognition error will most likelyoccur and the system will fall into a recogni-tion error recovery subdialogue that the userdoes not expect.
To help avoid such faultyevents the ECAs demeanour should indicateas clearly as possible that the user is not be-ing listened to at that particular moment.Speaking while looking away, perhaps atsome object, and absence of attention cues(such as nodding) are possible ways to showthat the user is not expected to interrupt theECA.
Since our present dialogue systemproduces fairly short utterances for the ECA,we are somewhat limited as to the activestrategies to build into the ECA?s behaviour.However, there are at least three cues theuser could read to realise that the systemdidn?t listen to what she said.
The first is thefact that the system carries on speaking, ig-noring the user?s utterance.
Second, at theend of the system?s turn the ECA will per-form a specific give-turn gesture.
And third,after giving the turn the ECA will remainstill and silent for a few seconds before per-forming a waiting gesture (leaning backslightly with her arms crossed, shifting thebody weight from one leg to another; seeTable 1).
In addition, if the user still remainssilent after yet another brief waiting periodthe system will offer help.
It will be interest-ing to see at which point users realise thatthe system didn?t register their utterance.?
A similar situation occurs if the Voice Ac-tivity Detector (VAD) fails and the systemdoesn?t capture the user?s entire utterance,or when the user simply doesn?t say any-thing when she is expected to (?no input?
).Again, both system and user end up waitingfor each other to say something.
And again,the strategy we use is to have the ECA dis-play a waiting posture.?
It can also happen that the user doesn?tspeak but the VAD ?thinks?
she did, per-haps after detecting some background noise69(a ?phantom input?).
The dialogue system?sreaction to something the user didn?t say cancause surprise and confusion in the user.Here the visible reactions of an ECA mighthelp the user understand what has happenedand allow her to steer the dialogue back ontrack.3.3 Recognition Confidence SchemeOnce the user utterance has been recognised, in-formation confirmation strategies are commonlyused in dialogue systems.
Different strategies aretaken depending on the level of confidence in thecorrectness of the user locution as captured by thespeech recognition unit (San-Segundo et al, 2001).Our scheme is as follows:?
High confidence: if recognition confidenceis high enough to safely assume that no errorhas occurred, the dialogue strategy is mademore fluent, with no confirmations beingsought by the system.?
Intermediate confidence: the result is re-garded as uncertain and the system tries im-plicit confirmation (by including the uncer-tain piece of information in a question aboutsomething else.)
This, combined with amixed initiative approach, allows the user tocorrect the system if an error did occur.?
Low confidence: in this case recognitionhas probably failed.
When this happens thedialogue switches to a more guided strategy,with explicit confirmation of the collectedinformation and no mixed initiative.
Theuser?s reply may confirm that the systemunderstood correctly, in which case the dia-logue continues to flow normally, or, on theother hand, it may show that there was arecognition error.
In this case an error re-covery mechanism begins.In addition to the dialogue strategies, ECAscould also be used to reflect in their manner thelevel of confidence that the system has understoodthe user, in accordance with the confirmation dia-logue strategies.
While the user speaks, our ECAwill, if the recognition confidence level is high,nod her head (Cassell et al, 2000), smile and haveher eyes fully open to give the user feedback thateverything is going well and the system is under-standing.
If, on the other hand, confidence is low,in order to make it clearer to the user that theremight be some problem with recognition and thatextra care should be taken, an option might be forthe ECA to gesture in such a way as to show thatshe isn?t quite sure she?s understood but is makingan effort to.
We have attempted to create this effectby having the ECA lean her head slightly to oneside, stop smiling and mildly squint.
Our goal,once again, is to find out whether these cues doindeed help users realise what the situation is.
Thisis especially important if it helps to avoid the well-known problem of falling into error spirals when arecognition error occurs in a spoken dialogue sys-tem (Bulyko et al, 2005).
In the case of intermedi-ate recognition confidence followed by a mixedinitiative strategy involving implicit confirmation,specific gestures could also be envisaged.
We havechosen not to include specific gestures for thesesituations in our first trials, however, so as not toobscure our observations for the high and low con-fidence cases.
A neutral stance for the intermediateconfidence level should be a useful referenceagainst which to compare the other two cases.3.4 Recognition ProblemsWe will consider those situations in which the sys-tem finds the user?s utterance incomprehensible(no-match situations) and those in which the sys-tem gets the user?s message wrong (recognitionserrors).
When a no-match occurs there are twoways in which an ECA can be useful.
First, whatthe character should say must be carefully pon-dered to ensure that the user is aware that the sys-tem didn?t understand what she said and that theimmediate objective is to solve this particularproblem.
This knowledge can make the user morepatient with the system and tolerate better the un-expected lengthening of the interaction (Goldberg,2003).
Second, the ECAs manner should try tokeep the user in a positive attitude.
A commonproblem in no-match and error recovery situationsis that the user becomes irritated or hyperarticu-lates in an attempt to make herself understood,which in fact increases the probability of yet an-other no-match or a recognition error.
This weshould obviously try to avoid.The ECA behaviour strategy we will test in no-match situations is to have the character lean to-wards the camera and raise her eyebrows (the ideabeing to convey a sense of surprise coupled withfriendly interest).
We have based our gesture on70one given in (Fagerberg et al, 2003).
If the userpoints out to the system that there has been a rec-ognition error in a way that gives the correct in-formation at the same time, then the ECA will con-firm the corrected information with special empha-sis in speech and gesture.
For this purpose we havedesigned a beat gesture with both hands (see Table1).3.5 Help offers and requestIt will be interesting to see whether the fact thathelp is offered by an animated character (the ECA)is regarded by users to be more user-friendly thanotherwise.
If users feel more comfortable with theECA, perhaps they will show greater initiative inrequesting help from the system; and when it isoffered by the system (when a problem situationoccurs), the presence of a friendly ECA might helpcontrol user frustration.
While the ECA is givingthe requested information, she will perform a beatgesture with both hands for emphasis, and she willalso change posture.
The idea is to see whether thiscaptures the interest of the user, makes her moreconfident and the experience more pleasant or, onthe contrary, it distracts the user and makes helpdelivery less effective.Figure 1 illustrates a dialogue sequence includ-ing the association between the different dialoguestrategies and the ECA gesture sequences after auser?s utterance.4 Experimental set upGestures and nonverbal communication are cul-ture-dependent.
This is an important fact to takeinto account because a single gesture might be in-terpreted in different ways depending on the user?sculture (Kleinsmith et al, 2006).
Therefore, a nec-essary step prior to the evaluation of the varioushypotheses put forward in the previous section is totest the gestures we have implemented for ourECA, within the framework designed for our study.This implies validating the gestures for Spanishusers, since we have based them on studies withinthe Anglo-Saxon culture.4.1 ProcedureFor the purpose of testing the gesture repertoiredeveloped for our ECA we have conceived anevaluation environment that simulates a realisticmobile videotelephony application that allows us-ers to remotely check the state (e.g., on/off) of sev-eral household devices (lights, heating, etc.).
Ourdialogue system incorporates mixed initiative, er-ror recovery subdialogues, context-dependent helpand the production of guided or flexible dialoguesaccording to the confidence levels of the speechrecogniser.
Our environment uses Nuance Com-munications?
speech recognition technology(www.nuance.com).
The ECA character has beendesigned by Haptek (www.haptek.com).During the gesture validation tests users didn?tinteract directly with the dialogue system.
We firstasked the users to watch a system simulator (avideo recording of a user interacting with the sys-tem), so that they could see the ECA performingthe gestures in the context of a real dialogue.After watching the simulation the users wereasked to fill out a questionnaire.
The questionnaireallowed users to view isolated clips of eachFigure 1: Dialogue strategies and related gesture sequence71of the dialogue gestures (the eight that had ap-peared in the video).
To each gesture clip were as-sociated questions basically covering the followingthree aspects:?
Gesture interpretation: Users are asked tointerpret each gesture, choosing one fromamong several given options (the same op-tions for all gestures).
The aim is to seewhether the meaning and intention of eachgesture are clear.
In addition users told uswhether they thought they had seen the ges-ture in the previous dialogue sample.?
Gesture design: Do users think the gestureis well made and does it look natural?
Toanswer this question we asked users to ratethe quality, expressiveness and clarity of theECAs gesture (on a 9-point Likert scale).?
User expectations: Users rated how usefulthey thought each gesture was (on a 9-pointLikert scale).
The idea is to juxtapose theutility function of the gestures in the users?mental model to our own when we designedthem, and evaluate the similarity.
In additionwe collected suggestions as to how the usersthought the gestures could be improved.4.2 ResultsWe recruited 17 test users (most of them studentsbetween 20 and 25 years of age) for our trial.
Theresults concerning the three previously mentionedaspects are shown in Table 2.
In the case of thegesture interpretation, we present the percentageof the users who interpreted each gesture ?cor-rectly?
(i.e., as we had intended when we designedthem).
Depending on this percentage we label eachgesture as ?Good?, ?Average?, or ?Bad?.
For eachof the parameters for gesture design and user ex-pectations we give the mean and the standard de-viation of the Likert scale scores.
We label the av-erage scores as ?Low?
(Likert score between 1 and3), Medium (4-6) or ?High?
(7-9).We now discuss the results separately for eachof the dimensions:Regarding user expectations, the values for eachgesture are High except for two of them, valued asMedium.
These two gestures are the welcome ges-ture and the gesture for offering help.
In the case ofthe welcome gesture, users probably believe thebeginning of the dialogue is already well enoughdefined when the ECA starts to speak.
If so, usersmight see an element of redundancy in the wel-come gesture, lowering its perceived utility in thedialogue process.
On the other hand, the help ges-ture utility might be valued lower than the rest be-cause many users didn?t seem to understand itspurpose (the clarity of the Help gesture was theleast valued of all, ?=5.117).
Nevertheless, thegeneral user impressions of the utility of the evalu-ated gesture repertoire fairly high.In relation to gesture design, we can see that,overall, the marks for quality and expressivenessare high.
This implies our gesture design is, on thewhole, adequate.
Regarding the clarity of the ges-tures, three of them are valued as Medium.
Theseare the gestures expressing Give Turn, Error Re-covery and Help offer.
This could be related to theprevailing opinion among users that there are a fewconfusing gestures, although they are better under-stood in the context of the application, when youlisten to what the ECA says.Only half of the gestures were properly inter-preted by the users.
Those that weren?t (Give Turn,Take Turn, Error Recovery and the Help gesture)are, we realize, the subtlest in the repertoire, so weasked ourselves if there could be relation betweena bad interpretation of the gesture and the whetherthat user didn?t remember seeing the gesture in thedialogue.
In Figure 2 we show the number of userswho claimed they hadn?t seen the ECA gesturesduring the dialogue sample.
The coloured bars rep-resent the overall accuracy in the interpretation ofthe gesture.
We may observe that the gestures thata larger number of users hadn?t seen in the dia-logue, and therefore, hadn?t an image of in propercontext, tended also to be considered more unclear.We may conclude that some gestures need to beevaluated in context.
In any case, and in spite ofthe uncertainty we have found regarding the inter-pretation of certain gestures, we believe the posi-tive evaluation by the users for the expressivenessand the quality of the gestures justifies us in vali-dating our gestural repertoire for the next researchstage where we will evaluate how well our ECAgestures function under real interaction conditions(taking into account objective data related to dia-logue efficiency).72Table 2:  Results of the gesture validation tests.Figure 2: Interpretation vs. ?visibility?
of the ges-tures.5 Conclusions and future lines of workIn this article we have identified a range of prob-lem situations that may arise in dialogue systems,and defined various strategies for using an ECA toimprove user-machine interaction throughout thewhole dialogue.
We have developed an experimen-tal set up for a user validation of ECA gestures inthe dialogue system and have obtained quantitativeresults and user opinions to improve the design ofthe gestures.
The results of this validation allow usto be in a position to begin testing our dialoguesystem and evaluate our ECA gestures in the con-text of a real dialogue.In future experiments we will attempt to go onestep further and analyse how empathic emotions vs.self-oriented behaviour (see Brave et al, 2005)may affect the resolution of a variety of dialoguesituations.
To this end we plan to design ECA pro-totypes that incorporate specific emotions, hopingto learn how best to connect empathically with theuser, and what effects this may have on dialoguedynamics and the overall user perception of thesystem.ReferencesLinda Bell and Joakim Gustafson, 2003.
Child andAdult SpeakerAdaptation during Error Resolution ina Publicly Available Spoken Dialogue System.
Pro-ceedings of Eurospeech 03, Geneve, Schweiz.Timothy W. Bickmore, Justine Cassell, Jan van Kup-pevelt, Laila Dybkjaer and Niels Ole Bernsen,  2004.
(atural, Intelligent and Effective Interaction withMultimodal Dialogue Systems, chapter Social Dia-logue with Embodied Conversational Agents.
KluwerAcademic.Susan J. Boyce, 1999.
Spoken natural language dia-logue systems: user interface issues for the future.
InHuman Factors and Voice Interactive Systems.
D.Gardner-Bonneau Ed.
Norwell, Massachusetts, Klu-wer Academic Publishers: 37-62.Scott Brave, Clifford Nass, Kevin Hutchinson, 2005.Computers that care: investigating the effects of ori-INTERPRETATION DESIGN EXPECTATIONSGood Interpretation (%) Quality Clarity Expressiveness UsefulnessG1Wellcome88.23Good7.117 (0.927)High7.588 (1.277)High6.764 (1.147)High5.647  (2.119)MediumG2Give Turn35.29Average6.647 (1.057)High5.823 (1.333)Medium6.470  (1.007)High6.588 (1.543)HighG3Take Turn23.53Bad7.117 (1.166)High6.705 (1.447)High6.941 (1.444)High6.647 (1.271)HighG4Wait82.35Good7.058 (1.088)High7.176 (1.185)High7.176 (0.727)High6.588 (1.622)HighG5Confirmation(Low confidence)76.47Good8.294 (0.587)High8.058 (1.028)High8.058 (1.028)High7.941 (1.028)HighG6Confirmation (Highconfidence)94.11Good7.529 (1.124)High7.529 (1.124)High7.705(1.263)High7.588 (1.175)HighG7Error Recovery41.17Average6.941 (1.088)High5.588 (2.032)Medium6.529 (1.462)High6.058 (1.390)HighG8Help35.29Average6.823 (1.185)High5.117 (1.932)Medium6.058(1.560)High5.529 (1.771)Medium73entation of emotion exhibited by an embodied com-puter agent.
Int.
J. Human-Computer Studies, Nr.
62,Issue 2, pp.
161-178.Ivan Bulyko, Katrin Kirchhoff, Mari Ostendorf, JulieGoldberg, 2005 Error correction detection and re-sponse generation in a spoken dialogue system.Speech Communication 45, 271-288.Justine Cassell, Kristinn R. Thorisson, 1999.
The powerof a nod and aglance: envelope vs. emotional feed-back in animated conversational agents.
Applied Ar-tificial Intelligence, vol.13, pp.519-538.Justine Cassell and Matthew Stone, 1999.
Living Handto Mouth: Psychological Theories about Speech andGesture in Interactive Dialogue Systems.
Proceed-ings of the AAAI 1999 Fall Symposium on Psycho-logical Models of Communication in CollaborativeSystems, pp.
34-42.
November 5-7, North Falmouth,MA, 1999.Justine Cassell, Timothy W. Bickmore, HannesVilhj?lmsson and Hao Yan, 2000.
More than just apretty face: affordances of embodiment.
In Proceed-ings of the 5th international Conference on intelligentUser interfaces.Justine Cassell, Yukiko I. Nakano, Timothy W. Bick-more, Candace L. Sidner and  Charles Rich, 2001.
(on-verbal cues for discourse structure.
In Proceed-ings of the 39th Annual Meeting on Association ForComputational Linguistics.Richard Catrambone, 2002 Anthropomorphic agents asa user interface paradigm: Experimental findingsand a framework for research.
In: Proceedings of the24th Annual Conference of the Cognitive ScienceSociety (pp.
166-171), Fairfax, VA, August.Nicole Chovil, 1992.
Discourse-Oriented Facial Dis-plays in Conversation.
Research on Language andSocial Interaction, 25, 163-194.Petra Fagerberg, Anna St?hl, Kristina H?
?k, 2003.
De-signing Gestures for Afective Input: an Analysis ofShape, Effort and Valence.
In Proceedings of MobileUbiquitious and Multimedia, Norrk?ping, Sweden.Julie Goldberg, Mari Ostendorf, Katrin Kirchhoff, 2003.The impact of response wording in error correctionsubdialogs, In EHSD-2003, 101-106.Kate Hone, 2005.
Animated Agents to reduce user frus-tration, in The 19th British HCI Group Annual Con-ference, Edinburgh, UK.Adam Kendon, 1990.
Conducting interaction: patternsof behaviour in focused encounters, Cambridge Uni-versity Press.Andrea Kleinsmith, P. Ravindra De Silva, Nadia Bian-chi-Berthouze, 2006 Cross-cultural differences inrecognizing affect from body posture Interacting withcomputers 10  1371-1389Robert M. Krauss, Yihsiu Chen and Purnima Chawla,1996 (onverbal behavior and nonverbal communica-tion: What do conversational hand gestures tell us?In M. Zanna (Ed.
), Advances in experimental socialpsychology (pp.
389 450).San Diego, CA: AcademicPress.Dominic W. Massaro, Michael M. Cohen, JonasBeskow and Ronald A. Cole,  2000.Developing andevaluating conversational agents.
In Embodied Con-versational Agents MIT Press, Cambridge, MA, 287-318.Sharon Oviatt.
1994.
Interface techniques for minimiz-ing disfluent input to spoken language systems.
InProc.
CHI'94 (pp.
205-210) Boston, ACM Press,1994Sharon Oviatt and Robert VanGent, 1996, Error resolu-tion duringmultimodal humancomputer interaction.Proc.
International Conference on Spoken LanguageProcessing, 1 204-207.Sharon Oviatt, Margaret MacEachern, and Gina-AnneLevow, G.,1998.
Predicting hyperarticulate speechduring human-computer error resolution.
SpeechCommunication, vol.24, 2, 1-23.Sharon Oviatt, and Bridget Adams, 2000.
Designingand evaluating conversational interfaces with ani-mated characters.
Embodied conversational agents,MIT Press: 319-345.Isabella Poggi, 2001.
How to decide which gesture tomake according to our goals and our contextualknowledge.
Paper presented at Gesture Workshop2001 London 18th-20th April, 2001Ruben San-Segundo, Juan M. Montero, Javier Ferreiros,Ricardo C?rdoba, Jose M. Pardo, 2001 DesigningConfirmation Mechanisms and Error Recover Tech-niques in a Railway Information System for Spanish.SIGDIAL.
Septiembre 1-2,  Aalborg (Dinamarca).Heike Schaumburg, 2001.
Computers as tools or associal actors?the users' perspective on anthropomor-phic agents.International Journal of Cooperative In-formation Systems.10, 1, 2, 217-234.74
