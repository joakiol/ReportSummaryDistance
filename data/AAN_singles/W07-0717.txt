Proceedings of the Second Workshop on Statistical Machine Translation, pages 128?135,Prague, June 2007. c?2007 Association for Computational LinguisticsMixture-Model Adaptation for SMTGeorge Foster and Roland KuhnNational Research Council Canadafirst.last@nrc.gc.caAbstractWe describe a mixture-model approach toadapting a Statistical Machine TranslationSystem for new domains, using weights thatdepend on text distances to mixture compo-nents.
We investigate a number of variantson this approach, including cross-domainversus dynamic adaptation; linear versusloglinear mixtures; language and transla-tion model adaptation; different methods ofassigning weights; and granularity of thesource unit being adapted to.
The bestmethods achieve gains of approximately oneBLEU percentage point over a state-of-theart non-adapted baseline system.1 IntroductionLanguage varies significantly across different gen-res, topics, styles, etc.
This affects empirical mod-els: a model trained on a corpus of car-repair manu-als, for instance, will not be well suited to an appli-cation in the field of tourism.
Ideally, models shouldbe trained on text that is representative of the areain which they will be used, but such text is not al-ways available.
This is especially the case for bilin-gual applications, because parallel training corporaare relatively rare and tend to be drawn from spe-cific domains such as parliamentary proceedings.In this paper we address the problem of adaptinga statistical machine translation system by adjust-ing its parameters based on some information abouta test domain.
We assume two basic settings.
Incross-domain adaptation, a small sample of parallelin-domain text is available, and it is used to optimizefor translating future texts drawn from the same do-main.
In dynamic adaptation, no domain informa-tion is available ahead of time, and adaptation isbased on the current source text under translation.Approaches developed for the two settings can becomplementary: an in-domain development corpuscan be used to make broad adjustments, which canthen be fine tuned for individual source texts.Our method is based on the classical techniqueof mixture modeling (Hastie et al, 2001).
Thisinvolves dividing the training corpus into differentcomponents, training a model on each part, thenweighting each model appropriately for the currentcontext.
Mixture modeling is a simple frameworkthat encompasses many different variants, as de-scribed below.
It is naturally fairly low dimensional,because as the number of sub-models increases, theamount of text available to train each, and thereforeits reliability, decreases.
This makes it suitable fordiscriminative SMT training, which is still a chal-lenge for large parameter sets (Tillmann and Zhang,2006; Liang et al, 2006).Techniques for assigning mixture weights dependon the setting.
In cross-domain adaptation, knowl-edge of both source and target texts in the in-domainsample can be used to optimize weights directly.
Indynamic adaptation, training poses a problem be-cause no reference text is available.
Our solutionis to construct a multi-domain development samplefor learning parameter settings that are intended togeneralize to new domains (ones not represented inthe sample).
We do not learn mixture weights di-rectly with this method, because there is little hope128that these would be well suited to new domains.
In-stead we attempt to learn how weights should be setas a function of distance.
To our knowledge, this ap-proach to dynamic adaptation for SMT is novel, andit is one of the main contributions of the paper.A second contribution is a fairly broad investiga-tion of the large space of alternatives defined by themixture-modeling framework, using a simple genre-based corpus decomposition.
We experimented withthe following choices: cross-domain versus dynamicadaptation; linear versus loglinear mixtures; lan-guage and translation model adaptation; various textdistance metrics; different ways of converting dis-tance metrics into weights; and granularity of thesource unit being adapted to.The remainder of the paper is structured follows:section 2 briefly describes our phrase-based SMTsystem; section 3 describes mixture-model adapta-tion; section 4 gives experimental results; section 5summarizes previous work; and section 6 concludes.2 Phrase-based Statistical MTOur baseline is a standard phrase-based SMT sys-tem (Koehn et al, 2003).
Given a source sentence s,this tries to find the target sentence t?
that is the mostlikely translation of s, using the Viterbi approxima-tion:t?
= argmaxtp(t|s) ?
argmaxt,ap(t,a|s),where alignment a = (s?1, t?1, j1), ..., (s?K , t?K , jK);t?k are target phrases such that t = t?1 .
.
.
t?K ; s?k aresource phrases such that s = s?j1 .
.
.
s?jK ; and s?k isthe translation of the kth target phrase t?k.To model p(t,a|s), we use a standard loglinearapproach:p(t,a|s) ?
exp[?i?ifi(s, t,a)](1)where each fi(s, t,a) is a feature function, andweights ?i are set using Och?s algorithm (Och,2003) to maximize the system?s BLEU score (Pa-pineni et al, 2001) on a development corpus.
Thefeatures used in this study are: the length oft; a single-parameter distortion penalty on phrasereordering in a, as described in (Koehn et al,2003); phrase translation model probabilities; and4-gram language model probabilities log p(t), us-ing Kneser-Ney smoothing as implemented in theSRILM toolkit.Phrase translation model probabilities are featuresof the form: log p(s|t,a) ?
?Kk=1 log p(s?k|t?k).We use two different estimates for the conditionalprobabilities p(t?|s?)
and p(s?|t?
): relative frequenciesand ?lexical?
probabilities as described in (Zens andNey, 2004).
In both cases, the ?forward?
phraseprobabilities p(t?|s?)
are not used as features, but onlyas a filter on the set of possible translations: for eachsource phrase s?
that matches some ngram in s, onlythe 30 top-ranked translations t?
according to p(t?|s?
)are retained.To derive the joint counts c(s?, t?)
from whichp(s?|t?)
and p(t?|s?)
are estimated, we use the phrase in-duction algorithm described in (Koehn et al, 2003),with symmetrized word alignments generated usingIBM model 2 (Brown et al, 1993).3 Mixture-Model AdaptationOur approach to mixture-model adaptation can besummarized by the following general algorithm:1.
Split the corpus into different components, ac-cording to some criterion.2.
Train a model on each corpus component.3.
Weight each model according to its fit with thetest domain:?
For cross-domain adaptation, set param-eters using a development corpus drawnfrom the test domain, and use for all fu-ture documents.?
For dynamic adaptation, set global param-eters using a development corpus drawnfrom several different domains.
Set mix-ture weights as a function of the distancesfrom corpus components to the currentsource text.4.
Combine weighted component models into asingle global model, and use it to translate asdescribed in the previous section.We now describe each aspect of this algorithm inmore detail.1293.1 Corpus DecompositionWe partition the corpus into different genres, definedas being roughly identical to corpus source.
This isthe simplest way to exploit heterogeneous trainingmaterial for adaptation.
An alternative, which wehave not explored, would be to cluster the corpusautomatically according to topic.3.2 Component ModelsWe adapt both language and translation model fea-tures within the overall loglinear combination (1).To train translation models on each corpus com-ponent, we used a global IBM2 model for wordalignment (in order to avoid degradation in align-ment quality due to smaller training corpora), thenextracted component-specific relative frequenciesfor phrase pairs.
Lexical probabilities were also de-rived from the global IBM2 model, and were notadapted.The procedure for training component-specificlanguage models on the target halves of each cor-pus component is identical to the procedure for theglobal model described in section 2.
In addition tothe component models, we also used a large staticglobal model.3.3 Combining FrameworkThe most commonly-used framework for mixturemodels is a linear one:p(x|h) =?c?cpc(x|h) (2)where p(x|h) is either a language or translationmodel; pc(x|h) is a model trained on component c,and ?c is the corresponding weight.
An alternative,suggested by the form of the global model, is a log-linear combination:p(x|h) =?cpc(x|h)?cwhere we write ?c to emphasize that in this casethe mixing parameters are global weights, like theweights on the other features within the loglinearmodel.
This is in contrast to linear mixing, where thecombined model p(x|h) receives a loglinear weight,but the weights on the components do not partici-pate in the global loglinear combination.
One conse-quence is that it is more difficult to set linear weightsusing standard minimum-error training techniques,which assume only a ?flat?
loglinear model.3.4 Distance MetricsWe used four standard distance metrics to cap-ture the relation between the current source or tar-get text q and each corpus component.1 All aremonolingual?they are applied only to source textor only to target text.The tf/idf metric commonly used in informationretrieval is defined as cos(vc,vq), where vc andvq are vectors derived from component c and doc-ument q, each consisting of elements of the form:?p?
(w) log p?doc(w), where p?
(w) is the relative fre-quency of word w within the component or docu-ment, and pdoc(w) is the proportion of componentsit appears in.Latent Semantic Analysis (LSA) (Deerwester etal., 1990) is a technique for implicitly capturing thesemantic properties of texts, based on the use ofSingular Value Decomposition to produce a rank-reduced approximation of an original matrix of wordand document frequencies.
We applied this tech-nique to all documents in the training corpus (as op-posed to components), reduced the rank to 100, thencalculated the projections of the component and doc-ument vectors described in the previous paragraphinto the reduced space.Perplexity (Jelinek, 1997) is a standard way ofevaluating the quality of a language model on a testtext.
We define a perplexity-based distance metricpc(q)1/|q|, where pc(q) is the probability assigned toq by an ngram language model trained on compo-nent c.The final distance metric, which we call EM, isbased on expressing the probability of q as a word-level mixture model: p(q) = ?|q|i=1?c dcpc(wi|hi),where q = w1 .
.
.
w|q|, and pc(w|h) is the ngramprobability of w following word sequence h in com-ponent c. It is straighforward to use the EM algo-rithm to find the set of weights d?c,?c that maxi-mizes the likelihood of q.
The weight d?c is definedas the distance to component c. For all experimentsdescribed below, we used a probability differencethreshold of 0.001 as the EM convergence criterion.1Although we refer to these metrics as distances, most arein fact proximities, and we use the convention throughout thathigher values mean closer.1303.5 Learning Adaptive ParametersOur focus in this paper is on adaptation via mixtureweights.
However, we note that the usual loglinearparameter tuning described in section 2 can also beconsidered adaptation in the cross-domain setting,because learned preferences for word penalty, rel-ative LM/TM weighting, etc, will reflect the targetdomain.
This is not the case for dynamic adapta-tion, where, in the absence of an in-domain devel-opment corpus, the only information we can hope toglean are the weights on adapted models comparedto other features of the system.The method used for adapting mixture weightsdepends on both the combining framework (loglin-ear versus linear), and the adaptive setting (cross-domain versus dynamic), as described below.3.5.1 Setting Loglinear Mixture WeightsWhen using a loglinear combining framework asdescribed in section 3.3, mixture weights are setin the same way as the other loglinear parameterswhen performing cross-domain adaptation.
Loglin-ear mixture models were not used for dynamic adap-tation.3.5.2 Setting Linear Mixture WeightsFor both adaptive settings, linear mixture weightswere set as a function of the distance metrics de-scribed in section 3.4.
Given a set of metrics{D1, .
.
.
, Dm}, let di,c be the distance from the cur-rent text to component c according to metric Di.
Asimple approach to weighting is to choose a singlemetric Di, and set the weights in (2) to be propor-tional to the corresponding distances:?c = di,c/?c?di,c?
.
(3)Because different distance metrics may capturecomplementary information, and because optimalweights might be a non-linear function of distance,we also experimented with a linear combination ofmetrics transformed using a sigmoid function:?c =m?i=1?i1 + exp(ai(bi ?
di,c)) (4)where ?i reflects the relative predictive power of Di,and the sigmoid parametes ai and bi can be set toselectively suppress contributions from componentsthat are far away.
Here we assume that ?i absorbsa normalization constant, so that the ?c?s sum to 1.In this approach, there are three parameters per dis-tance metric to learn: ?i, ai, and bi.
In general, theseparameters are also specific to the particular modelbeing adapted, ie the LM or the TM.To optimize these parameters, we fixed globalloglinear weights at values obtained with Och?s al-gorithm using representative adapted models basedon a single distance metric in (3), then used theDownhill Simplex algorithm (Press et al, 2002) tomaximize BLEU score on the development corpus.For tractability, we followed standard practice withthis technique and considered only monotonic align-ments when decoding (Zens and Ney, 2004).The two approaches just described avoid condi-tioning ?c explicitly on c. This is necessary fordynamic adaptation, since any genre preferenceslearned from the development corpus cannot be ex-pected to generalize.
However, it is not necessaryfor cross-domain adaptation, where the genre of thedevelopment corpus is assumed to represent the testdomain.
Therefore, we also experimented with us-ing Downhill Simplex optimization to directly learnthe set of linear weights ?c that yield maximumBLEU score on the development corpus.A final variant on setting linear mixture weights isa hybrid between cross-domain and dynamic adap-tation.
In this approach, both the global loglinearweights and, if they are being used, the mixture pa-rameters ?i, ai, bi are set to characterize the test do-main as in cross-domain adaptation.
When trans-lating, however, distances to the current source textare used in (3) or (4) instead of distances to the in-domain development corpus.
This obviously limitsthe metrics used to ones that depend only on sourcetext.4 ExperimentsAll experiments were run on the NIST MT evalua-tion 2006 Chinese data set.
Table 1 summarizes thecorpora used.
The training corpus was divided intoseven components according to genre; in all casesthese were identical to LDC corpora, with the excep-tion of the Newswire component, which was amal-gamated from several smaller corpora.
The target131genre for cross-domain adaptation was newswire,for which high-quality training material is avail-able.
The cross-domain development set NIST04-nw is the newswire subset of the NIST 2004 evalu-ation set, and the dynamic adaptation developmentset NIST04-mix is a balanced mixed-genre subset ofNIST 2004.
The NIST 2005 evaluation set was usedfor testing cross-domain adaptation, and the NIST2006 evaluation set (both the ?GALE?
and ?NIST?parts) was used to test dynamic adaptation.Because different development corpora are usedfor cross-domain and dynamic adaptation, wetrained one static baseline model for each of theseadaptation settings, on the corresponding develop-ment set.All results given in this section are BLEU scores.role corpus genres senttrain FBIS04 nw 182kHK Hans proceedings 1,375kHK Laws legal 475kHK News press release 740kNewswire nw 26kSinorama news mag 366kUN proceedings 4,979kdev NIST04-nw nw 901NIST04-mix nw, sp, ed 889test NIST05 nw 1,082NIST06-GALE nw, ng, bn, bc 2,276NIST06-NIST nw, ng, bn 1,664Table 1: Corpora.
In the genres column: nw =newswire, sp = speeches, ed = editorial, ng = news-group, bn = broadcast news, and bc = broadcast con-versation.4.1 Linear versus Loglinear CombinationTable 2 shows a comparison between linear andloglinear mixing frameworks, with uniform weightsused in the linear mixture.
Both types of mixturemodel are better than the baseline, but the linearmixture is slightly better than the loglinear mix-ture.
This is quite surprising, because these resultsare on the development set: the loglinear modeltunes its component weights on this set, whereasthe linear model only adjusts global LM and TMweights.
We speculated that this may have been dueto non-smooth component models, and tried varioussmoothing schemes, including Kneser-Ney phrasetable smoothing similar to that described in (Fosteret al, 2006), and binary features to indicate phrase-pair presence within different components.
Nonehelped, however, and we conclude that the problemis most likely that Och?s algorithm is unable to finda good maximimum in this setting.
Due to this re-sult, all experiments we describe below involve lin-ear mixtures only.combination adapted modelLM TM LM+TMbaseline 30.2 30.2 30.2loglinear mixture 30.9 31.2 31.4uniform linear mixture 31.2 31.1 31.8Table 2: Linear versus loglinear combinations onNIST04-nw.4.2 Distance Metrics for WeightingTable 3 compares the performance of all distancemetrics described in section 3.4 when used on theirown as defined in (3).
The difference between themis fairly small, but appears to be consistent acrossLM and TM adaptation and (for the LM metrics)across source and target side matching.
In general,LM metrics seem to have a slight advantage over thevector space metrics, with EM being the best overall.We focus on this metric for most of the experimentsthat follow.metric source text target textLM TM LM TMtf/idf 31.3 31.3 31.1 31.1LSA 31.5 31.6perplexity 31.6 31.3 31.7 31.5EM 31.7 31.6 32.1 31.3Table 3: Distance metrics for linear combination onthe NIST04-nw development set.
(Entries in the topright corner are missing due to lack of time.
)Table 4 shows the performance of the parame-terized weighting function described by (4), withsource-side EM and LSA metrics as inputs.
Thisis compared to direct weight optimization, as boththese techniques use Downhill Simplex for param-eter tuning.
Unfortunately, neither is able to beat132the performance of the normalized source-side EMmetric on its own (reproduced on the first line fromtable 3).
In additional tests we verified that this alsoholds for the test corpus.
We speculate that this dis-appointing result is due to compromises made in or-der to run Downhill Simplex efficiently, includingholding global weights fixed, using only a singlestarting point, and running with monotone decoding.weighting LM TMEM-src, direct 31.7 31.6EM-src + LSA-src, parameterized 31.0 30.0direct optimization 31.7 30.2Table 4: Weighting techniques for linear combina-tion on the NIST04-nw development set.4.3 Cross-Domain versus Dynamic AdaptationTable 5 shows results for cross-domain adaptation,using the source-side EM metric for linear weight-ing.
Both LM and TM adaptation are effective, withtest-set improvements of approximately 1 BLEUpoint over the baseline for LM adaptation and some-what less for TM adaptation.
Performance also im-proves on the NIST06 out-of-domain test set (al-though this set includes a newswire portion as well).However, combined LM and TM adaptation is notbetter than LM adaptation on its own, indicating thatthe individual adapted models may be capturing thesame information.model dev testnist04- nist05 nist06-nw nistbaseline 30.2 30.3 26.5EM-src LM 31.7 31.2 27.8EM-src TM 31.6 30.9 27.3EM-src LM+TM 32.5 31.2 27.7Table 5: Cross-Domain adaptation results.Table 6 contains results for dynamic adaptation,using the source-side EM metric for linear weight-ing.
In this setting, TM adaptation is much lesseffective, not significantly better than the baseline;performance of combined LM and TM adaptationis also lower.
However, LM adaptation improvesover the baseline by up to a BLEU point.
The per-formance of cross domain adaptation (reproducedfrom table 5 on the second line) is slightly better forthe in-domain test set (NIST05), but worse than dy-namic adaptation on the two mixed-domain sets.model dev testnist04- nist05 nist06- nist06-mix nist galebaseline 31.9 30.4 27.6 12.9cross LM n/a 31.2 27.8 12.5LM 32.8 30.8 28.6 13.4TM 32.4 30.7 27.6 12.8LM+TM 33.4 30.8 28.5 13.0Table 6: Dynamic adaptation results, using src-sideEM distances.model NIST05baseline 30.3cross EM-src LM 31.2cross EM-src TM 30.9hybrid EM-src LM 30.9hybrid EM-src TM 30.7Table 7: Hybrid adaptation results.Table 7 shows results for the hybrid approach de-scribed at the end of section 3.5.2: global weightsare learned on NIST04-nw, but linear weights arederived dynamically from the current test file.
Per-formance drops slightly compared to pure cross-domain adaptation, indicating that it may be impor-tant to have a good fit between global and mixtureweights.4.4 Source GranularityThe results of the final experiment, to determine theeffects of source granularity on dynamic adaptation,are shown in table 8.
Source-side EM distances areapplied to the whole test set, to genres within the set,and to each document individually.
Global weightswere tuned specifically for each of these conditions.There appears to be little difference among these ap-proaches, although genre-based adaptation perhapshas a slight advantage.133granularity dev testnist04- nist05 nist06- nist06-mix nist galebaseline 31.9 30.4 27.6 12.9file 32.4 30.8 28.6 13.4genre 32.5 31.1 28.9 13.2document 32.9 30.9 28.6 13.4Table 8: The effects of source granularity on dy-namic adaptation.5 Related WorkMixture modeling is a standard technique in ma-chine learning (Hastie et al, 2001).
It has beenwidely used to adapt language models for speechrecognition and other applications, for instance us-ing cross-domain topic mixtures, (Iyer and Osten-dorf, 1999), dynamic topic mixtures (Kneser andSteinbiss, 1993), hierachical mixtures (Florian andYarowsky, 1999), and cache mixtures (Kuhn and DeMori, 1990).Most previous work on adaptive SMT focuses onthe use of IR techniques to identify a relevant sub-set of the training corpus from which an adaptedmodel can be learned.
Byrne et al(2003) use co-sine distance from the current source document tofind relevant parallel texts for training an adaptedtranslation model, with background information forsmoothing alignments.
Hildebrand et al(1995) de-scribe a similar approach, but apply it at the sentencelevel, and use it for language model as well as trans-lation model adaptation.
They rely on a perplexityheuristic to determine an optimal size for the rele-vant subset.
Zhao et al(2004) apply a slightly differ-ent sentence-level strategy to language model adap-tation, first generating an nbest list with a baselinesystem, then finding similar sentences in a monolin-gual target-language corpus.
This approach has theadvantage of not limiting LM adaptation to a parallelcorpus, but the disadvantage of requiring two trans-lation passes (one to generate the nbest lists, and an-other to translate with the adapted model).Ueffing (2006) describes a self-training approachthat also uses a two-pass algorithm.
A baseline sys-tem generates translations that, after confidence fil-tering, are used to construct a parallel corpus basedon the test set.
Standard phrase-extraction tech-niques are then applied to extract an adapted phrasetable from the system?s own output.Finally, Zhang et al(2006) cluster the paralleltraining corpus using an algorithm that heuristicallyminimizes the average entropy of source-side andtarget-side language models over a fixed number ofclusters.
Each source sentence is then decoded us-ing the language model trained on the cluster thatassigns highest likelihood to that sentence.The work we present here is complementaryto both the IR approaches and Ueffing?s methodbecause it provides a way of exploiting a pre-established corpus division.
This has the potentialto allow sentences having little surface similarity tothe current source text to contribute statistics thatmay be relevant to its translation, for instance byraising the probability of rare but pertinent words.Our work can also be seen as extending all previousapproaches in that it assigns weights to componentsdepending on their degree of relevance, rather thanassuming a binary distinction between relevant andnon-relevant components.6 Conclusion and Future WorkWe have investigated a number of approaches tomixture-based adaptation using genres for Chi-nese to English translation.
The most successfulis to weight component models in proportion tomaximum-likelihood (EM) weights for the currenttext given an ngram language model mixture trainedon corpus components.
This resulted in gains ofaround one BLEU point.
A more sophisticated ap-proach that attempts to transform and combine mul-tiple distance metrics did not yield positive results,probably due to an unsucessful optmization proce-dure.Other conclusions are: linear mixtures are moretractable than loglinear ones; LM-based metrics arebetter than VS-based ones; LM adaptation workswell, and adding an adapted TM yields no improve-ment; cross-domain adaptation is optimal, but dy-namic adaptation is a good fallback strategy; andsource granularity at the genre level is better thanthe document or test-set level.In future work, we plan to improve the optimiza-tion procedure for parameterized weight functions.We will also look at bilingual metrics for cross-134domain adaptation, and investigate better combina-tions of cross-domain and dynamic adaptation.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent Della J.Pietra, and Robert L. Mercer.
1993.
The mathematicsof Machine Translation: Parameter estimation.
Com-putational Linguistics, 19(2):263?312, June.W.
Byrne, S. Khudanpur, W. Kim, S. Kumar, P. Pecina,P.
Virga, P. Xu, and D. Yarowsky.
2003.
The JHU2003 Chinese-English Machine Translation System.In MT Summit IX, New Orleans, September.S.
Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-dauer, and R. Harshman.
1990.
Indexing by latentsemantic analysis.
JASIS, 41(6):391?407.Radu Florian and David Yarowsky.
1999.
Dynamic non-local language modeling via hierarchical topic-basedadaptation.
In ACL 1999, pages 167?174, CollegePark, Maryland, June.George Foster, Roland Kuhn, and Howard Johnson.2006.
Phrasetable smoothing for statistical machinetranslation.
In EMNLP 2006, Sydney, Australia.Trevor Hastie, Robert Tibshirani, and Jerome Friedman.2001.
The Elements of Statistical Learning.
Springer.Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,and Alex Waibel.
1995.
Adaptation of the transla-tion model for statistical machine translation based oninformation retrieval.
In EAMT 1995, Budapest, May.R.
Iyer and M. Ostendorf.
1999.
Modeling long dis-tance dependence in language: Topic mixtures vs. dy-namic cache models.
In IEEE Trans on Speech andLanguage Processing, 1999.Frederick Jelinek.
1997.
Statistical Methods for SpeechRecognition.
MIT Press.Reinhard Kneser and Volker Steinbiss.
1993.
On thedynamic adaptation of stochastic language models.In ICASSP 1993, pages 586?589, Minneapolis, Min-nesota.
IEEE.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In NAACL2003, pages 127?133.Roland Kuhn and Renato De Mori.
1990.
A cache-basednatural language model for speech recognition.
IEEETrans on PAMI, 12(6):570?583, June.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminative ap-proach to machine translation.
In ACL 2006Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In ACL 2003, Sapporo,July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: A method for automaticevaluation of Machine Translation.
Technical ReportRC22176, IBM, September.William H. Press, Saul A. Teukolsky, William T. Vetter-ling, and Brian P. Flannery.
2002.
Numerical Recipesin C++.
Cambridge University Press, Cambridge,UK.Christoph Tillmann and Tong Zhang.
2006.
A discrimi-native global training algorithm for statistical MT.
InACL 2006.Nicola Ueffing.
2006.
Self-training for machine trans-lation.
In NIPS 2006 Workshop on MLIA, Whistler,B.C., December.Richard Zens and Hermann Ney.
2004.
Improvementsin phrase-based statistical machine translation.
InHLT/NAACL 2004, Boston, May.R.
Zhang, H. Yamamoto, M. Paul, H. Okuma, K. Yasuda,Y.
Lepage, E. Denoual, D. Mochihashi, A. Finch, andE.
Sumita.
2006.
The NiCT-ATR statistical machinetranslation system for the IWSLT 2006 evaluation.
InIWSLT 2006.Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language model adaptation for statistical machinetranslation with structured query models.
In COLING2004, Geneva, August.135
