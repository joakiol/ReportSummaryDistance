Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 116?126,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsInducing Word Senses to Improve Web Search Result ClusteringRoberto Navigli and Giuseppe CrisafulliDipartimento di InformaticaSapienza Universita` di Romanavigli@di.uniroma1.it, crisafulli.giu@gmail.comAbstractIn this paper, we present a novel approach toWeb search result clustering based on the au-tomatic discovery of word senses from rawtext, a task referred to as Word Sense Induc-tion (WSI).
We first acquire the senses (i.e.,meanings) of a query by means of a graph-based clustering algorithm that exploits cycles(triangles and squares) in the co-occurrencegraph of the query.
Then we cluster the searchresults based on their semantic similarity tothe induced word senses.
Our experiments,conducted on datasets of ambiguous queries,show that our approach improves search resultclustering in terms of both clustering qualityand degree of diversification.1 IntroductionOver recent years increasingly huge amounts oftext have been made available on the Web.
Popularsearch engines such as Yahoo!
and Google usuallydo a good job at retrieving a small number of rel-evant results from such an enormous collection ofWeb pages (i.e.
retrieving with high precision, lowrecall).
However, current search engines are still fac-ing the lexical ambiguity issue (Furnas et al, 1987)?
i.e.
the linguistic property owing to which anyparticular word may convey different meanings.
Ina recent study (Sanderson, 2008) ?
conducted us-ing WordNet (Miller et al, 1990) and Wikipedia assources of ambiguous words ?
it was reported thataround 3% of Web queries and 23% of the mostfrequent queries are ambiguous.
Examples include:?buy B-52?
(a cocktail?
a bomber?
a DJ worksta-tion?
tickets for a band?
), ?Alexander Smith quotes?
(the novelist?
the poet?
), ?beagle search?
(dogs?
theLinux search tool?
the landing spacecraft?
).Ambiguity is often the consequence of the lownumber of query words entered on average by Webusers (Kamvar and Baluja, 2006).
While averagequery length is increasing ?
it is now estimated ataround 3 words per query1 ?
many search enginessuch as Google have already started to tackle thequery ambiguity issue by reranking and diversify-ing their results, so as to prevent Web pages that aresimilar to each other from ranking too high on thelist.In the past few years, Web clustering engines(Carpineto et al, 2009) have been proposed as asolution to the lexical ambiguity issue in Web In-formation Retrieval.
These systems group search re-sults, by providing a cluster for each specific aspect(i.e., meaning) of the input query.
Users can then se-lect the cluster(s) and the pages therein that best an-swer their information needs.
However, many Webclustering engines group search results on the ba-sis of their lexical similarity.
For instance, considerthe following snippets returned for the beagle searchquery:1.
Beagle is a search tool that ransacks your...2.
...the beagle disappearing in search of game...3.
Beagle indexes your files and searches...While snippets 1 and 3 both concern the Linuxsearch tool, they do not have any content word in1http://www.hitwise.com/us/press-center/press-releases/google-searches-apr-09116common except our query words.
As a result, theywill most likely be assigned to two different clusters.In this paper we present a novel approach to Websearch result clustering which is based on the auto-matic discovery of word senses from raw text ?
atask referred to as Word Sense Induction (WSI).
Atthe core of our approach is a graph-based algorithmthat exploits cycles in the co-occurrence graph ofthe input query to detect the query?s meanings.
Ourexperiments on two datasets of ambiguous queriesshow that our WSI approach boosts search resultclustering in terms of both clustering quality and de-gree of diversification.2 Related WorkWeb directories.
A first, historical solution toquery ambiguity is that of Web directories, thatis taxonomies providing categories to which Webpages are manually assigned (e.g., the Open Direc-tory Project ?
http://dmoz.org).
Given a query,search results are organized by category.
This ap-proach has three main weaknesses: first, it is static,thus it needs manual updates to cover new pages;second, it covers only a small portion of the Web;third, it classifies Web pages based on coarse cate-gories.
This latter feature of Web directories makesit difficult to distinguish between instances of thesame kind (e.g., pages about artists with the samesurname classified as Arts:Music:Bands andArtists).
While methods for the automatic clas-sification of Web documents have been proposed(e.g., (Liu et al, 2005b; Xue et al, 2008)) and someproblems have been effectively tackled (Bennett andNguyen, 2009), these approaches are usually super-vised and still suffer from relying on a predefinedtaxonomy of categories.Semantic Information Retrieval (SIR).
A dif-ferent direction consists of associating explicit se-mantics (i.e., word senses or concepts) with queriesand documents, that is, performing Word Sense Dis-ambiguation (WSD, see Navigli (2009)).
SIR is per-formed by indexing and/or searching concepts ratherthan terms, thus potentially coping with two linguis-tic phenomena: expressing a single meaning withdifferent words (synonymy) and using the same wordto express various different meanings (polysemy).Over the years, different methods for SIR have beenproposed (Krovetz and Croft, 1992; Voorhees, 1993;Mandala et al, 1998; Gonzalo et al, 1999; Kim etal., 2004; Liu et al, 2005a, inter alia).
However, con-trasting results have been reported on the benefits ofthese techniques: it has been shown that WSD hasto be very accurate to benefit Information Retrieval(Sanderson, 1994) ?
a result that was later debated(Gonzalo et al, 1999; Stokoe et al, 2003).
Also,it has been reported that WSD has to be very pre-cise on minority senses and uncommon terms, ratherthan on frequent words (Krovetz and Croft, 1992;Sanderson, 2000).SIR relies on the existence of a reference dictio-nary to perform WSD (typically, WordNet) and thussuffers from its static nature and its inherent paucityof most proper nouns.
This latter problem is partic-ularly important for Web searches, as users tend toretrieve more information about named entities (e.g.,singers, artists, cities) than concepts (e.g., abstractinformation about singers or artists).Search Result Clustering.
A more popular ap-proach to query ambiguity is that of search resultclustering.
Typically, given a query, the system startsfrom a flat list of text snippets returned from one ormore commonly-available search engines and clus-ters them on the basis of some notion of textual simi-larity.
At the root of the clustering approach lies vanRijsbergen?s (1979) cluster hypothesis: ?closely as-sociated documents tend to be relevant to the samerequests?, whereas documents concerning differentmeanings of the input query are expected to belongto different clusters.Approaches to search result clustering can beclassified as data-centric or description-centric(Carpineto et al, 2009).
The former focus more onthe problem of data clustering than on presenting theresults to the user.
A pioneering example is Scat-ter/Gather (Cutting et al, 1992), which divides thedataset into a small number of clusters and, after theselection of a group, performs clustering again andproceeds iteratively.
Developments of this approachhave been proposed which improve on cluster qual-ity and retrieval performance (Ke et al, 2009).
Otherdata-centric approaches use agglomerative hierar-chical clustering (e.g., LASSI (Yoelle Maarek andPelleg, 2000)), rough sets (Ngo and Nguyen, 2005)or exploit link information (Zhang et al, 2008).Description-centric approaches are, instead, more117focused on the description to produce for eachcluster of search results.
Among the most popularand successful approaches are those based on suf-fix trees (Zamir et al, 1997; Zamir and Etzioni,1998), including later developments (Crabtree et al,2005; Bernardini et al, 2009).
Other methods inthe literature are based on formal concept analy-sis (Carpineto and Romano, 2004), singular valuedecomposition (Osinski and Weiss, 2005), spectralclustering (Cheng et al, 2005), spectral geometry(Liu et al, 2008), link analysis (Gelgi et al, 2007),and graph connectivity measures (Di Giacomo et al,2007).
Search result clustering has also been viewedas a supervised salient phrase ranking task (Zeng etal., 2004).Diversification.
Another recent research topicdealing with the query ambiguity issue is diversifi-cation, which aims to rerank top search results basedon criteria that maximize their diversity.
One of thefirst examples of diversification algorithms is basedon the use of similarity functions to measure thediversity among documents and between documentand query (Carbonell and Goldstein, 1998).
Othertechniques use conditional probabilities to deter-mine which document is most different from higher-ranking ones (Chen and Karger, 2006) or use affinityranking (Zhang et al, 2005), based on topic varianceand coverage.
More recently, an algorithm called Es-sential Pages (Swaminathan et al, 2009) has beenproposed to reduce information redundancy and re-turn Web pages that maximize coverage with respectto the input query.Word Sense Induction (WSI).
In contrast to theabove approaches, we perform WSI to dynamicallyacquire an inventory of senses of the input query.Instead of performing clustering on the basis of thesurface similarity of Web snippets, we use our in-duced word senses to group snippets.
Very littlework on this topic exists: vector-based WSI was suc-cessfully shown to improve bag-of-words ad-hoc In-formation Retrieval (Schu?tze and Pedersen, 1995)and preliminary studies (Udani et al, 2005; Chenet al, 2008) have provided interesting insights intothe use of WSI for Web search result clustering.A more recent attempt at automatically identify-ing query meanings is based on the use of hiddentopics (Nguyen et al, 2009).
However, in this ap-proach topics ?
estimated from a universal dataset ?are query-independent and thus their number needsto be established beforehand.
In contrast, we aimto cluster snippets based on a dynamic and finer-grained notion of sense.3 ApproachWeb search result clustering is usually performed inthree main steps:1.
Given a query q, a search engine (e.g., Yahoo!)
isused to retrieve a list of results R = (r1, .
.
.
, rn);2.
A clustering C = (C0, C1, .
.
.
, Cm) of the resultsin R is obtained by means of a clustering algo-rithm;3.
The clusters in C are optionally labeled with anappropriate algorithm (e.g., see Zamir and Etzioni(1998) and Carmel et al (2009)) for visualizationpurposes.Our key idea is to improve step 2 by means of aWord Sense Induction algorithm: given a query q,we first dynamically induce, from a text corpus, theset of word senses of q (Section 3.1); next, we clus-ter the Web results on the basis of the word sensespreviously induced (Section 3.2).3.1 Word Sense InductionWord Sense Induction algorithms are unsupervisedtechniques aimed at automatically identifying theset of senses denoted by a word.
These methods in-duce word senses from text by clustering word oc-currences based on the idea that a given word ?used in a specific sense ?
tends to co-occur with thesame neighbouring words (Harris, 1954).
Severalapproaches to WSI have been proposed in the litera-ture (see Navigli (2009) for a survey), ranging fromclustering based on context vectors (e.g., Schu?tze(1998)) to word clustering (e.g., Lin (1998)) andco-occurrence graphs (e.g., Widdows and Dorow(2002)).Successful approaches such as HyperLex(Ve?ronis, 2004) ?
a graph algorithm based on theidentification of hubs in co-occurrence graphs ?have to cope with a high number of parameters tobe tuned (Agirre et al, 2006).
To deal with thisissue we propose two variants of a simple, yeteffective, graph-based algorithm for WSI, that we118describe hereafter.
The algorithm consists of twosteps: graph construction and identification of wordsenses.3.1.1 Graph constructionGiven a target query q, we build a co-occurrencegraph Gq = (V,E) such that V is a set of contextwords related to q and E is the set of undirectededges, each denoting a co-occurrence between pairsof words in V .
To determine the set of co-occurringwords V , we use the Google Web1T corpus (Brantsand Franz, 2006), a large collection of n-grams (n =1, .
.
.
, 5) ?
i.e., windows of n consecutive tokens ?occurring in one terabyte of Web documents.
First,for each content word w we collect the total num-ber c(w) of its occurrences and the number of timesc(w,w?)
that w and w?
occur together in any 5-gram(we include inflected forms in the count); second,we use the Dice coefficient to determine the strengthof co-occurrence between w and w?:Dice(w,w?)
=2c(w,w?
)c(w) + c(w?).
(1)The rationale behind Dice is that dividing by thesum of total counts of the two words drastically de-creases the ranking of words that tend to co-occurfrequently with many other words (e.g., new, old,nice, etc.
).The graph Gq = (V,E) is built as follows:?
Our initial vertex set V (0) contains all the con-tent words from the snippet results of query q(excluding stopwords); then, we add to V (0) thehighest-ranking words co-occurring with q in theWeb1T corpus, i.e., those words w for whichDice(q, w) ?
?
(the threshold ?
is established ex-perimentally, see Section 4.1).
We set V := V (0)and E := ?.?
For each word w ?
V (0), we select the high-est ranking words co-occurring with w in Web1T,that is those words w?
for which Dice(w,w?)
??.
We add each of these words to V (note thatsome w?
might already be in V (0)) and thecorresponding edge {w,w?}
to E with weightDice(w,w?).
Finally, we remove disconnectedvertices.3.1.2 Identification of word sensesThe main idea behind our approach is that edgesin the co-occurrence graph participating in cyclesare likely to connect vertices (i.e., words) belongingto the same meaning component.
Specifically, we fo-cus on cycles of length 3 and 4, called respectivelytriangles and squares in graph theory.For each edge e, we calculate the ratio of trianglesin which e participates:Tri(e) =# triangles e participates in# triangles e could participate in(2)where the numerator is the number of cycles oflength 3 in which e = {w,w?}
participates, and thedenominator is the total number of neighbours of wand w?.
Similarly, we define a measure Sqr(e) ofthe ratio of squares (i.e., cycles of length 4) an edgee participates in to the number of possible squares ecould potentially participate in:Sqr(e) =# squares e participates in# squares e could participate in(3)where the numerator is the number of squares con-taining e and the denominator is the number of pos-sible distinct pairs of neighbours of w and w?.
If notriangle (or square) exists for e, the value of the cor-responding function is set to 0.In order to disconnect the graph and determinethe meaning components, we remove all the edgeswhose Tri (or Sqr) value is below a threshold ?.
Theresulting connected components represent the wordsenses induced for the query q.
Notice that the num-ber of senses is dynamically chosen based on the co-occurrence graph and the algorithm?s thresholds.Our triangular measure is the edge counterpartof the clustering coefficient (or curvature) for ver-tices, previously used to perform WSI (Widdowsand Dorow, 2002).
However, it is our hunch thatmeasuring the ratio of squares an edge participatesin provides a stronger clue of how important thatedge is within a meaning component.
In Section 4,we will corroborate this idea with our experiments.3.1.3 An exampleAs an example, let q = beagle.
Two steps are per-formed:1191.
Graph construction.
We build the co-occurrencegraph Gbeagle = (V,E), an excerpt of which isshown in Figure 1(a).2.
Identification of word senses.
We calculate theSqr values of each edge in the graph.
The edgese whose Sqr(e) < ?
are removed (we assume?
= 0.25).
For instance, Sqr({ dog, breed }) = 12 ,as the edge participates in the square dog ?
breed?
puppy ?
canine ?
dog, but it could also haveparticipated in the potential square dog ?
breed?
puppy ?
search ?
dog.
In fact, the other neigh-bours of dog are canine, puppy and search, andthe other neighbour of breed is puppy, thus thesquare can only be closed by connecting puppyto either canine or search.
In our example, theonly edges whose Sqr is below ?
are: { dog,puppy }, { dog, search } and { linux, mission }(they participate in no square).
We remove theseedges and select the resulting connected compo-nents as the senses of the query beagle (shown inFigure 1(b)).
Note that, if we selected trianglesas our pruning measure, we should also removethe following edges { search, index }, { index,linux }, { linux, system } and { system, search }.In fact, these edges do not participate in any tri-angle (while they do participate in a square).
As aresult, we would miss the computer science senseof the query.3.2 Clustering of Web resultsGiven our query q, we submit it to a search engine,which returns a list of relevant search results R =(r1, .
.
.
, rn).
We process each result ri by consid-ering the corresponding text snippet and transform-ing it to a bag of words bi (we apply tokenization,stopwords and target word removal, and lemmatiza-tion2).
For instance, given the snippet:?the beagle is a breed of medium-sized dog?,we produce the following bag of words:{ breed, medium, size, dog }.As a result of the above processing, we obtain alist of bags of words B = (b1, .
.
.
, bn).
Now, ouraim is to cluster our Web results R, i.e., the corre-sponding bags of words B.
To this end, rather than2We use the WordNet lemmatizer.dogpuppycanineindexlinuxmissionsystemsearchbreedmarslanderspacecraft(a)dogpuppycanineindexlinuxmissionsystemsearchbreedmarslanderspacecraft(b)Figure 1: The beagle example: (a) graph construction,?weak?
edges (according to Sqr) drawn in bold, (b) theword senses induced after edge removal.considering the interrelationships between them (asis done in traditional search result clustering), weintersect each bag of words bi ?
B with the senseclusters {S1, .
.
.
, Sm} acquired as a result of ourWord Sense Induction algorithm (cf.
Section 3.1).The sense cluster with the largest intersection withbi is selected as the most likely meaning of ri.
For-mally:Sense(ri) =8><>:argmaxj=1,...,m|bi ?
Sj | ifmaxj|bi ?
Sj | > 00 else(4)where 0 denotes that no sense is assigned to result ri,as the intersection is empty for all senses Sj .
Oth-erwise the function returns the index of the sensehaving the largest overlap with bi ?
the bag of wordsassociated with the search result ri.
As a result ofsense assignment for each ri ?
R, we obtain a clus-tering C = (C0, C1, .
.
.
, Cm) such that:Cj = {ri ?
R : Sense(ri) = j}, (5)that is, Cj contains the search results classified withthe j-th sense of query q (C0 includes unassignedresults).
Finally, we sort the clusters in our clus-tering C based on their ?quality?.
For each clusterCj ?
C \ {C0}, we determine its similarity with120the corresponding meaning Sj by calculating the fol-lowing formula:avgsim(Cj , Sj) =?ri?Cjsim(ri, Sj)|Cj |.
(6)The formula determines the average similarity be-tween the search results in cluster Cj and the corre-sponding sense cluster Sj .
The similarity between asearch result ri and Sj is determined as the normal-ized overlap between its bag of words bi and Sj :sim(ri, Sj) = sim(bi, Sj) =|bi ?
Sj ||bi|.
(7)Finally, we rank the elements ri within each clus-ter Cj by their similarity sim(ri, Sj).
We note thatthe ranking and optimality of clusters can be im-proved with more sophisticated techniques (Crab-tree et al, 2005; Kurland, 2008; Kurland andDomshlak, 2008; Lee et al, 2008, inter alia).
How-ever, this is outside the scope of this paper.4 Experiments4.1 Experimental SetupTest Sets.
We conducted our experiments on twodatasets:?
AMBIENT (AMBIguous ENTries), a recentlyreleased dataset which contains 44 ambiguousqueries3.
The sense inventory for the mean-ings (i.e., subtopics)4 of queries is given byWikipedia disambiguation pages.
For instance,given the beagle query, its disambiguation pagein Wikipedia provides the meanings of dog, Marslander, computer search service, beer brand, etc.The top 100 Web results of each query returnedby the Yahoo!
search engine were tagged withthe most appropriate query senses according toWikipedia (amounting to 4400 sense-annotatedsearch results).
To our knowledge, this is cur-rently the largest dataset of ambiguous queriesavailable on-line.
Other datasets, such as thosefrom the TREC competitions, are not focused ondistiguishing the subtopics of a query.3http://credo.fub.it/ambient4In the following, we use the terms subtopic and word senseinterchangeably.dataset queriesqueries by length avg.1 2 3 4 polys.AMBIENT 44 35 6 3 0 17.9MORESQUE 114 0 47 36 31 6.7Table 1: Statistics on the datasets of ambiguous queries.?
MORESQUE (MORE Sense-tagged QUEry re-sults), a new dataset of 114 ambiguous querieswhich we developed as a complement to AMBI-ENT following the guidelines provided by its au-thors.
In fact, our aim was to study the behaviourof Web search algorithms on queries of differ-ent lengths, ranging from 1 to 4 words.
How-ever, the AMBIENT dataset is composed mostlyof single-word queries.
MORESQUE providesdozens of queries of length 2, 3 and 4, togetherwith the 100 top results from Yahoo!
for eachquery annotated as in the AMBIENT dataset(overall, we tagged 11,400 snippets).
We decidedto carry on using Yahoo!
mainly for homogeneityreasons.We report the statistics on the composition of thetwo datasets in Table 1.
Given that the snippets couldpossibly be annotated with more than one Wikipediasubtopic, we also determined the average numberof subtopics per snippet.
This amounted to 1.01 forAMBIENT and 1.04 for MORESQUE for snippetswith at least one subtopic annotation (51% and 53%of the respective datasets).
We can thus concludethat multiple subtopic annotations are infrequent.Parameters.
Our graph-based algorithms havetwo parameters: the Dice threshold ?
for graphconstruction (Section 3.1.1) and the threshold ?for edge removal (Section 3.1.2).
The best pa-rameters, used throughout our experiments, were(?
= 0.00033, ?
= 0.45) with triangles and (?
=0.00033, ?
= 0.33) with squares.
The parametervalues were obtained as a result of tuning on a smallin-house development dataset.
The dataset was builtby automatically identifying monosemous wordsand creating pseudowords following the schemeproposed by Schu?tze (1998).Systems.
We compared Triangles and Squaresagainst the best systems reported by Bernardini etal.
(2009, cf.
Section 2):121?
Lingo (Osinski and Weiss, 2005): a Web clus-tering engine implemented in the Carrot2 open-source framework5 that clusters the most frequentphrases extracted using suffix arrays.?
Suffix Tree Clustering (STC) (Zamir and Et-zioni, 1998): the original Web search clusteringapproach based on suffix trees.?
KeySRC (Bernardini et al, 2009): a state-of-the-art Web clustering engine built on top of STC withpart-of-speech pruning and dynamic selection ofthe cut-off level of the clustering dendrogram.?
Essential Pages (EP) (Swaminathan et al, 2009):a recent diversification algorithm that selects fun-damental pages which maximize the amount ofinformation covered for a given query.?
Yahoo!
: the original search results returned bythe Yahoo!
search engine.The first three of the above are Web search resultclustering approaches, whereas the last two producelists of possibly diversified results (cf.
Section 2).4.2 Experiment 1: Clustering QualityMeasure.
While assessing the quality of cluster-ing is a notably hard problem, given a gold standardG we can calculate the Rand index (RI) of a cluster-ing C, a common quality measure in the literature,determined as follows (Rand, 1971; Manning et al,2008):RI(C) =?(w,w?
)?W?W,w 6=w?
?(w,w?)|{(w,w?)
?
W ?W : w 6= w?
}|(8)where W is the union set of all the words in C and?(w,w?)
= 1 if any two words w and w?
are in thesame cluster both in C and in the gold standard G orthey are in two different clusters in both C and G,otherwise ?(w,w?)
= 0.
In other words, we calcu-late the percentage of word pairs that are in the sameconfiguration in both C and G. For the gold standardG we use the clustering induced by the sense annota-tions provided in our datasets for each snippet (i.e.,each cluster contains the snippets manually associ-ated with a particular Wikipedia subtopic).
Similarlyto what was done in Section 3.2, untagged results aregrouped together in a special cluster of G.5http://project.carrot2.orgSystem AMBIENT MORESQUE AllSquares 72.59 65.41 67.28Triangles 66.13 64.47 64.93Lingo 62.75 52.68 55.49STC 61.48 51.52 54.29KeySRC 66.49 55.82 58.78Table 2: Results by Rand index (percentages).Results.
The results of all systems on the AM-BIENT and MORESQUE datasets according tothe average Rand index are shown in Table 26.In accordance with previous results in the litera-ture, KeySRC performed generally better than theother search result clustering systems, especiallyon smaller queries.
Our Word Sense Induction sys-tems, Squares and Triangles, outperformed all othersystems by a large margin, thus showing a higherclustering quality (with the exception of KeySRCperforming better than Triangles on AMBIENT).Interestingly, all clustering systems perform morepoorly on longer queries (i.e., on the MORESQUEdataset), however our WSI systems, and especiallyTriangles, are more robust across query lengths.Compared to Triangles, the Squares algorithm per-forms better, confirming our hunch that Squares is amore solid graph pattern.4.3 Experiment 2: DiversificationMeasure.
Search result clustering can also be usedto diversify the top results returned by a search en-gine.
Thus, for each query q, one natural way ofmeasuring a system?s performance is to calculate thesubtopic recall-at-K (Zhai et al, 2003) given by thenumber of different subtopics retrieved for q in thetop K results returned:S-recall@K =|?Ki=1 subtopics(ri)|M(9)where subtopics(ri) is the set of subtopics manuallyassigned to the search result ri and M is the numberof subtopics for query q (note that in our experimentsM is the number of subtopics occurring in the 100results retrieved for q, so S-recall@100 = 1).
How-ever, this measure is only suitable for systems re-turning ranked lists (such as Yahoo!
and EP).
Given6For reference systems we used the implementations ofBernardini et al (2009) and Osinski and Weiss (2005).122System K=3 K=5 K=10 K=15 K=20Squares 51.9 63.4 75.8 83.3 87.4Triangles 50.8 62.4 75.2 82.7 86.6Yahoo!
49.2 60.0 72.9 78.5 82.7EP 40.6 53.2 68.6 77.2 83.3KeySRC 44.3 55.8 72.0 79.1 83.2Table 3: S-recall@K on all queries (percentages).a clustering C = (C0, C1, .
.
.
, Cm), we flatten it to alist as follows: we add to the initially empty list thefirst element of each cluster Cj (j = 1, .
.
.
,m); thenwe iterate the process by selecting the second ele-ment of each cluster Cj such that |Cj | ?
2, and soon.
The remaining elements returned by the searchengine, but not included in any cluster of C \ {C0},are appended to the bottom of the list in their orig-inal order.
Note that the elements are selected fromeach cluster according to their internal ranking (e.g.,for our algorithms we use Formula 7 introduced inSection 3.2).Results.
For the sake of clarity and to save space,we selected the best systems from our previous ex-periment, namely Squares, Triangles and KeySRC,and compared their output with the original snippetlist returned by Yahoo!
and the output of the EP di-versification algorithm (cf.
Section 4.1).The S-recall@K (with K = 3, 5, 10, 15, 20) cal-culated on AMBIENT+MORESQUE is reportedin Table 3.
Squares and Triangles show the high-est degree of diversification, with a subtopic recallgreater than all other systems, and with Squares con-sistently performing better than Triangles.
It is inter-esting to observe that KeySRC performs worse thanYahoo!
with low values of K and generally betterwith higher values of K.Given that the two datasets complement eachother in terms of query lengths (with AMBIENThaving queries of length ?
2 and MORESQUEwith many queries of length ?
3), we studied the S-recall@K trend for the two datasets.
The results areshown in Figures 2 and 3.
While KeySRC does notshow large differences in the presence of short andlong ambiguous queries, our graph-based algorithmsdo.
For instance, as soon as K = 3 the Squares al-gorithm obtains S-recall values of 37% and 57.5%on AMBIENT and MORESQUE, respectively.
The00.10.20.30.40.50.60.70.80.9110  20  30  40  50  60  70  80  90  100S-recall-at-Knumber of results KSquaresTrianglesYahoo!EPKeysrcFigure 2: Results by S-recall@K on AMBIENT.0.20.30.40.50.60.70.80.9110  20  30  40  50  60  70  80  90  100S-recall-at-Knumber of results KSquaresTrianglesYahoo!EPKeysrcFigure 3: S-recall@K on MORESQUE.difference decreases as K increases, but is still sig-nificant when K = 10.
We hypothesize that, becausethey are less ambiguous, longer queries are easierto diversify with the aid of WSI.
However, we notethat, even with low values of K, Squares and Tri-angles obtain higher S-recall than the other systems(with KeySRC competing on AMBIENT whenK ?15).
Finally, we observe that ?
with low values of K?
the Squares algorithm performs significantly betterthan Triangles on shorter queries, and only slightlybetter on longer ones.5 DiscussionResults.
Our results show that our graph-based al-gorithms are able to consistently produce clusters ofbetter quality than all other systems tested in ourexperiments.
The results on S-recall@K show thatour approach can also be used effectively as a diver-sification technique, performing better than a very123recent proposal such as Essential Pages.
The lat-ter outperforms Yahoo!
and KeySRC when K ?30 on AMBIENT, whereas on MORESQUE it per-forms generally worse until higher values of K arereached.
If we analyze the entire dataset of 158queries by length, EP works best after examining atleast 20 results on 1- and 2-word ambiguous queries,whereas on longer queries a larger number of docu-ments (?
30) needs to be analyzed before surpassingYahoo!
performance.The above considerations might not seem intu-itive at first glance, as the average polysemy oflonger queries is lower (17.9 on AMBIENT vs. 6.7on MORESQUE according to our gold standard).However, we note that while the kind of ambigu-ity of 1-word queries is generally coarser (e.g., bea-gle as dog vs. lander vs. search tool), with longerqueries we often encounter much finer sense distinc-tions (e.g., Across the Universe as song by The Bea-tles vs. a 2007 film based on the song vs. a Star Treknovel vs. a rock album by Trip Shakespeare, etc.
).Word Sense Induction is able to deal better with thislatter kind of ambiguity as discriminative words be-come part of the meanings acquired.Performance issues.
Inducing word senses fromthe query graph comes at a higher computationalcost than other non-semantic clustering techniques.Indeed, the most time-consuming phase of our ap-proach is the construction of the query graph, whichrequires intensive querying of our database of co-occurrences calculated from the Web1T corpus.While graphs can be precomputed or cached, previ-ously unseen queries will still require the construc-tion of new graphs.
Instead, triangles and squares, aswell as the resulting connected components, can becalculated on the fly.6 ConclusionsIn this paper we have presented a novel approachto Web search result clustering.
Our key idea is toinduce senses for the target query automatically bymeans of a graph-based algorithm focused on thenotion of cycles.
The results of a Web search engineare then mapped to the query senses and clusteredaccordingly.The paper provides three novel contributions.First, we show that WSI boosts the quality of searchresult clustering and improves the diversification ofthe snippets returned as a flat list.
We provide a clearindication on the usefulness of a loose notion ofsense to cope with ambiguous queries.
This is incontrast to research on Semantic Information Re-trieval, which has obtained contradictory and ofteninconclusive results.
The main advantage of WSIlies in its dynamic production of word senses thatcover both concepts (e.g., beagle as a breed of dog)and instances (e.g., beagle as a specific instance ofa space lander).
In contrast, static dictionaries suchas WordNet ?
typically used in Word Sense Dis-ambiguation ?
by their very nature encode mainlyconcepts.
Second, we propose two simple, yet ef-fective, graph algorithms to induce the senses ofour queries.
The best performing approach is basedon squares (cycles of length 4), a novel graph pat-tern in WSI.
Third, we contribute a new dataset of114 ambiguous queries and 11,400 sense-annotatedsnippets which complements an existing dataset ofambiguous queries7.
Given the lack of ambiguousquery datasets available (Sanderson, 2008), we hopeour new dataset will be useful in future compara-tive experiments.
Finally, we note that our approachneeded very little tuning.
Moreover, its requirementof a Web corpus of n-grams is not a stringent one, assuch corpora are available for several languages andcan be produced for any language of interest.As regards future work, we intend to combineour clustering algorithm with a cluster labeling al-gorithm.
We also aim to implement a number ofWord Sense Induction algorithms and compare themin the same evaluation framework with more Websearch and Web clustering engines.
Finally, it shouldbe possible to use precisely the same approach pre-sented in this paper for document clustering, bygrouping the contexts in which the target query oc-curs ?
and we will also experiment on this in thefuture.AcknowledgmentsWe thank Google for providing the Web1T corpusfor research purposes.
We also thank MassimilianoD?Amico for producing the output of KeySRC andEP, and Stanislaw Osinski and Dawid Weiss for their7The MORESQUE dataset is available at the followingURL: http://lcl.uniroma1.it/moresque124help with Lingo and STC.
Additional thanks go toJim McManus, Senja Pollak and the anonymous re-viewers for their useful comments.ReferencesEneko Agirre, David Mart?
?nez, Oier Lo?pez de Lacalle,and Aitor Soroa.
2006.
Evaluating and optimizingthe parameters of an unsupervised graph-based WSDalgorithm.
In Proc.
of TextGraphs ?06, pages 89?96,New York, USA.Paul N. Bennett and Nam Nguyen.
2009.
Refined ex-perts: improving classification in large taxonomies.
InProc.
of SIGIR ?09, pages 11?18, Boston, MA, USA.Andrea Bernardini, Claudio Carpineto, and Massimil-iano D?Amico.
2009.
Full-subtopic retrieval withkeyphrase-based search results clustering.
In Proc.
ofWI ?09, pages 206?213, Milan, Italy.Thorsten Brants and Alex Franz.
2006.
Web 1t 5-gram,ver.
1, ldc2006t13.
In LDC, PA, USA.Jaime Carbonell and Jade Goldstein.
1998.
The use ofmmr, diversity-based reranking for reordering docu-ments and producing summaries.
In Proc.
of SIGIR?98, pages 335?336, Melbourne, Australia.David Carmel, Haggai Roitman, and Naama Zwerdling.2009.
Enhancing cluster labeling using Wikipedia.
InProc.
of SIGIR ?09, pages 139?146, MA, USA.Claudio Carpineto and Giovanni Romano.
2004.
Ex-ploiting the potential of concept lattices for informa-tion retrieval with CREDO.
Journal of UniversalComputer Science, 10(8):985?1013.Claudio Carpineto, Stanislaw Osin?ski, Giovanni Ro-mano, and Dawid Weiss.
2009.
A survey of web clus-tering engines.
ACM Computing Surveys, 41(3):1?38.Harr Chen and David R. Karger.
2006.
Less is more:probabilistic models for retrieving fewer relevant doc-uments.
In Proc.
of SIGIR ?06, pages 429?436, Seat-tle, WA, USA.Jiyang Chen, Osmar R.
Za?
?ane, and Randy Goebel.
2008.An unsupervised approach to cluster web search re-sults based on word sense communities.
In Proc.
ofWI-IAT 2008, pages 725?729, Sydney, Australia.David Cheng, Santosh Vempala, Ravi Kannan, and GrantWang.
2005.
A divide-and-merge methodology forclustering.
In Proc.
of PODS ?05, pages 196?205,New York, NY, USA.Daniel Crabtree, Xiaoying Gao, and Peter Andreae.2005.
Improving web clustering by cluster selec-tion.
In Proc.
of WI ?05, pages 172?178, Compie`gne,France.Douglass R. Cutting, David R. Karger, Jan O. Pedersen,and John W. Tukey.
1992.
Scatter/gather: A cluster-based approach to browsing large document collec-tions.
In Proc.
of SIGIR ?92, pages 318?329, Copen-hagen, Denmark.Emilio Di Giacomo, Walter Didimo, Luca Grilli, andGiuseppe Liotta.
2007.
Graph visualization tech-niques for web clustering engines.
IEEE Transactionson Visualization and Computer Graphics, 13(2):294?304.George W. Furnas, Thomas K. Landauer, Louis Gomez,and Susan Dumais.
1987.
The vocabulary problemin human-system communication.
Communications ofthe ACM, 30(11):964?971.Fatih Gelgi, Hasan Davulcu, and Srinivas Vadrevu.
2007.Term ranking for clustering web search results.
InProc.
of WebDB ?07, Beijing, China.Julio Gonzalo, Anselmo Penas, and Felisa Verdejo.
1999.Lexical ambiguity and Information Retrieval revisited.In Proc.
of EMNLP/VLC 1999, pages 195?202, Col-lege Park, MD, USA.Zellig Harris.
1954.
Distributional structure.
Word,10:146?162.Maryam Kamvar and Shumeet Baluja.
2006.
A largescale study of wireless search behavior: Google mo-bile search.
In Proc.
of CHI ?06, pages 701?709, NewYork, NY, USA.Weimao Ke, Cassidy R. Sugimoto, and Javed Mostafa.2009.
Dynamicity vs. effectiveness: studying onlineclustering for scatter/gather.
In Proc.
of SIGIR ?09,pages 19?26, MA, USA.Sang-Bum Kim, Hee-Cheol Seo, and Hae-Chang Rim.2004.
Information Retrieval using word senses: rootsense tagging approach.
In Proc.
of SIGIR ?04, pages258?265, Sheffield, UK.Robert Krovetz and William B. Croft.
1992.
Lexical am-biguity and Information Retrieval.
ACM Transactionson Information Systems, 10(2):115?141.Oren Kurland and Carmel Domshlak.
2008.
A rank-aggregation approach to searching for optimal query-specific clusters.
In Proc.
of SIGIR ?08, pages 547?554, Singapore.Oren Kurland.
2008.
The opposite of smoothing: a lan-guage model approach to ranking query-specific doc-ument clusters.
In Proc.
of SIGIR ?08, pages 171?178,Singapore.Kyung Soon Lee, W. Bruce Croft, and James Allan.2008.
A cluster-based resampling method for pseudo-relevance feedback.
In Proc.
of SIGIR ?08, pages 235?242, Singapore.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proc.
of the 17th COLING, pages768?774, Montreal, Canada.125Shuang Liu, Clement Yu, and Weiyi Meng.
2005a.
WordSense Disambiguation in queries.
In Proc.
of CIKM?05, pages 525?532, Bremen, Germany.Tie-Yan Liu, Yiming Yang, Hao Wan, Hua-Jun Zeng,Zheng Chen, and Wei-Ying Ma.
2005b.
Support vec-tor machines classification with a very large-scale tax-onomy.
SIGKDD Explor.
Newsl., 7(1):36?43.Ying Liu, Wenyuan Li, Yongjing Lin, and Liping Jing.2008.
Spectral geometry for simultaneously clusteringand ranking query search results.
In Proc.
of SIGIR?08, pages 539?546, Singapore.Rila Mandala, Takenobu Tokunaga, and Hozumi Tanaka.1998.
The use of WordNet in Information Retrieval.In Proc.
of the COLING-ACL workshop on Usage ofWordnet in Natural Language Processing, pages 31?37, Montreal, Canada.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, New York, NY,USA.George A. Miller, Richard T. Beckwith, Christiane D.Fellbaum, Derek Gross, and Katherine Miller.
1990.WordNet: an online lexical database.
InternationalJournal of Lexicography, 3(4):235?244.Roberto Navigli.
2009.
Word Sense Disambiguation: asurvey.
ACM Computing Surveys, 41(2):1?69.Chi Lang Ngo and Hung Son Nguyen.
2005.
A methodof web search result clustering based on rough sets.
InProc.
of WI ?05, pages 673?679, Compie`gne, France.Cam-Tu Nguyen, Xuan-Hieu Phan, Susumu Horiguchi,Thu-Trang Nguyen, and Quang-Thuy Ha.
2009.Web search clustering and labeling with hidden top-ics.
ACM Transactions on Asian Language Informa-tion Processing, 8(3):1?40.Stanislaw Osinski and Dawid Weiss.
2005.
A concept-driven algorithm for clustering search results.
IEEEIntelligent Systems, 20(3):48?54.William M. Rand.
1971.
Objective criteria for the eval-uation of clustering methods.
Journal of the AmericanStatistical Association, 66(336):846?850.Mark Sanderson.
1994.
Word Sense Disambiguationand Information Retrieval.
In Proc.
of SIGIR ?94,pages 142?151, Dublin, Ireland.Mark Sanderson.
2000.
Retrieving with good sense.
In-formation Retrieval, 2(1):49?69.Mark Sanderson.
2008.
Ambiguous queries: test collec-tions need more sense.
In Proc.
of SIGIR ?08, pages499?506, Singapore.Hinrich Schu?tze and Jan Pedersen.
1995.
InformationRetrieval based on word senses.
In Proceedings ofSDAIR?95, pages 161?175, Las Vegas, Nevada, USA.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?124.Christopher Stokoe, Michael J. Oakes, and John I. Tait.2003.
Word Sense Disambiguation in Information Re-trieval revisited.
In Proc.
of SIGIR ?03, pages 159?166, Canada.Ashwin Swaminathan, Cherian V. Mathew, and DarkoKirovski.
2009.
Essential pages.
In Proc.
of WI ?09,pages 173?182, Milan, Italy.Goldee Udani, Shachi Dave, Anthony Davis, and TimSibley.
2005.
Noun sense induction using web searchresults.
In Proc.
of SIGIR ?05, pages 657?658, Sal-vador, Brazil.Cornelis Joost van Rijsbergen.
1979.
Information Re-trieval.
Butterworths, second edition.Jean Ve?ronis.
2004.
HyperLex: lexical cartographyfor Information Retrieval.
Computer Speech and Lan-guage, 18(3):223?252.Ellen M. Voorhees.
1993.
Using WordNet to disam-biguate word senses for text retrieval.
In Proc.
of SI-GIR ?93, pages 171?180, Pittsburgh, PA, USA.Dominic Widdows and Beate Dorow.
2002.
A graphmodel for unsupervised lexical acquisition.
In Proc.of the 19th COLING, pages 1?7, Taipei, Taiwan.Gui-Rong Xue, Dikan Xing, Qiang Yang, and Yong Yu.2008.
Deep classification in large-scale text hierar-chies.
In Proc.
of SIGIR ?08, pages 619?626, Singa-pore.Israel Ben-Shaul Yoelle Maarek, Ron Fagin and Dan Pel-leg.
2000.
Ephemeral document clustering for webapplications.
IBM Research Report RJ 10186.Oren Zamir and Oren Etzioni.
1998.
Web documentclustering: a feasibility demonstration.
In Proc.
of SI-GIR ?98, pages 46?54, Melbourne, Australia.Oren Zamir, Oren Etzioni, Omid Madani, and Richard M.Karp.
1997.
Fast and intuitive clustering of web docu-ments.
In Proc.
of KDD ?97, pages 287?290, NewportBeach, California.Hua-Jun Zeng, Qi-Cai He, Zheng Chen, Wei-Ying Ma,and Jinwen Ma.
2004.
Learning to cluster websearch results.
In Proc.
of SIGIR ?04, pages 210?217,Sheffield, UK.ChengXiang Zhai, William W. Cohen, and John Lafferty.2003.
Beyond independent relevance: Methods andevaluation metrics for subtopic retrieval.
In Proc.
ofSIGIR ?03, pages 10?17, Toronto, Canada.Benyu Zhang, Hua Li, Yi Liu, Lei Ji, Wensi Xi, WeiguoFan, Zheng Chen, and Wei-Ying Ma.
2005.
Improv-ing web search results using affinity graph.
In Proc.
ofSIGIR ?05, pages 504?511, Salvador, Brazil.Xiaodan Zhang, Xiaohua Hu, and Xiaohua Zhou.
2008.A comparative evaluation of different link types on en-hancing document clustering.
In Proc.
of SIGIR ?08,pages 555?562, Singapore.126
