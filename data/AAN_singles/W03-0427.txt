Memory-based one-step named-entity recognition: Effects of seed listfeatures, classifier stacking, and unannotated dataIris Hendrickx and Antal van den BoschILK / Computational LinguisticsTilburg University, The Netherlands{I.H.E.Hendrickx,Antal.vdnBosch}@uvt.nl1 OutlineWe present a memory-based named-entity recognitionsystem that chunks and labels named entities in a one-shot task.
Training and testing on CoNLL-2003 sharedtask data, we measure the effects of three extensions.First, we incorporate features that signal the presence ofwordforms in external, language-specific seed (gazetteer)lists.
Second, we build a second-stage stacked classifierthat corrects first-stage output errors.
Third, we add se-lected instances from classified unannotated data to thetraining material.
The system that incorporates all attainsan overall F-rate on the final test set of 78.20 on Englishand 63.02 on German.2 Data and featuresThe CoNLL-2003 shared task (Tjong Kim Sang andDe Meulder, 2003) supplied datasets in two languages,English and German, using four named entity cate-gories: persons, organisations, locations, and ?miscellanynames?.
Manual annotation has been performed at theUniversity of Antwerp.
Apart from tokenized wordforms,the data provides predicted PoS-tags and chunks.Additionally we computed the following features witheach wordform, largely following those used by the best-performing submission of the 2002 shared task (Carreraset al, 2002):?
Orthographic features represented as binaryfeatures: Begin cap, All caps, Internal cap,Contains digit, Contains digit en alpha, Initial,Lower case, First word?
The wordform?s first letter and last three letters (asthree separate features)?
The direct output of the memory-based lemmatizer(Van den Bosch and Daelemans, 1999), provid-ing PoS tag, morphological features, and spellingchange information?
PoS tag from a slow but accurate version of thememory-based tagger trained on a portion of theBritish National Corpus, according to the CLAWS-5tagset (for English data only)For example, for the English word Indian the followingfeature representation is made: Indian NNP I-NP1 0 0 0 0 0 0 0 I i a n AJ0-NN1 N-sI-MISC, where NNP is the provided PoS tag, I-NP thechunk tag; the binary features represent the orthographicfeatures (where in this case only Begin cap is positive);AJO-NN1 is the PoS tag of the BNC-trained-tagger;N-s is the lemmatizer output for noun-singular; the lastelement, I-MISC, is the annotated class label.In our experiments we construct instances aroundwordforms, where we take a windowed snapshot of theword in its direct local context.
By default, we select awindow of two words to the left and right.
For all fivewords in each input instance (feature vector), in principleall of the above features are included.3 Experimental setupIn two subsections we briefly detail how the memory-based learner works, and how we optimized its param-eters through an automatic process called iterative deep-ening.3.1 Memory-based learningMemory-based learning is a supervised inductive learn-ing algorithm for learning classification tasks.
Memory-based learning treats a set of training instances as pointsin a multi-dimensional feature space, and stores them assuch in an instance base in memory (rather than perform-ing some abstraction over them).New (test) instances are classified by matching themto all instances in memory, and by calculating with eachmatch the distance, given by a distance function betweenthe new instance X and each of the n memory instancesY1...n. Classification in memory-based learning is per-formed by the k-NN algorithm that searches for the k?nearest neighbours?
among the memory instances ac-cording to the distance function.
The majority class ofthe k nearest neighbours then determines the class of thenew instance X .
Cf.
(Daelemans et al, 2002) for algo-rithmic details and background.3.2 Iterative deepeningIterative deepening (ID) is a heuristic search algorithmfor the optimization of algorithmic parameter and fea-ture selection, that combines classifier wrapping (usingthe training material internally to test experimental vari-ants) (Kohavi and John, 1997) with progressive samplingof training material (Provost et al, 1999).
We start witha large pool of experiments, each with a unique combina-tion of input features and algorithmic parameter settings.In the first step, each attempted setting is applied to asmall amount of training material and tested on a fixedamount of held-out data (a held-out part of the trainingset).
Only the best settings are kept; all others are re-moved from the pool of competing settings.
In subse-quent iterations, this step is repeated, cutting the num-ber of settings in the pool by a half and retaining thebest-performing half, while at the same time doubling theamount of training material.We selected 10% of the training set as held-out data.Six iterations were performed with increasing trainingset sizes, starting with 2000 instances, and doubling witheach iteration up to 128,000 training instances, resultingin 16 best settings after the last iteration.
Selection ofthe best experiments was based on their overall F-rate ascomputed by the conlleval script.The initial pool of experiments was created by system-atically varying parameters of the memory-based learnerand some limited feature selections, (for details, cf.
(Daelemans et al, 2002)):?
Basic distance function: Overlap or modified valuedifference metric (MVDM)?
Feature weighting: gain ratio, information gain, ?2,or shared variance?
k in the k-NN classifier: 5, 9, 13, 15, 17, 19, 21, 25,and 29?
Distance weighting: none, linear-inverse, inverse,exponential decay with ?=1 and ?=4?
Feature selection: apart from the wordform and itsprovided CoNLL-2003 PoS tag, create a local win-dow of either no, 1, or 2 wordforms to the left andright of the focus word.
For all words in a window,all features are selected.The first round of the ID process therefore tests 2 ?4 ?
9 ?
5 ?
3 = 1080 systematic permutations of theseparameter settings and feature selection.4 Extensions4.1 Seed list featuresThe first extension is to incorporate language-specificseed-list (gazetteer) information.
Rather than using theselists external to the classifier, we encode them as internalfeatures associated to wordforms.
For each of the fournamed entity classes we gathered one list of names, con-taining material garnered from name sites on the inter-net, from the training set (for the MISC category), andfrom the CELEX English lexical data base (Baayen etal., 1993).
These lists vary in size from 1269 names to78,732 names.
Each wordform in the training and testdata is then enriched with four binary features, each rep-resenting whether the word occurs in the respective seedlist.
One problem with seed lists is that a word can occurin more than one seed list, so that more than one of thesefour bits may be active.4.2 Second-stage stackingThe second extension is to use second-stage stacking.Stacking in general (Wolpert, 1992) encompasses a classof meta-learning systems that learn to correct errors madeby lower-level classifiers.
We adopt the particular methodpioneered in (Veenstra, 1998) in which classifications of afirst memory-based classifier are added as windowed fea-tures to the instances presented to the second classifier.Since the second-stage classifier also computes the sim-ilarities between instances using these extra features, itis able, in principle, to recognise and correct reoccurringpatterns of errors within sub-sentential sequences.
Thiscould correct errors made due to the ?blindness?
of thefirst-stage classifier, which is unaware of its own classifi-cations left or right of the wordform in the current focusposition.
We used stacking on top of the first extension.4.3 Unannotated dataFor both languages a large unannotated dataset wasmade available for extracting data or information.
Al-ternative to using this data to expand or bootstrap seedlists (Cucerzan and Yarowsky, 1999; Buchholz andVan den Bosch, 2000), we use the unannotated corpusto select useful instances to be added directly to the train-ing set.
Not unlike (Yarowsky, 1995) we use confidenceof our classifier on unannotated data to enrich itself; thatis, by adding confidently-classified instances to the mem-ory.
We make the simple assumption that entropy in theclass distribution in the nearest neighbour set computedin the classification of a new instance is correlated withthe reliability of the classification, when k > 1.
When knearest neighbours all vote for the same class, the entropyof that class vote is 0.0.
Alternatively, when the votes tie,the entropy is maximal.A secondary heuristic assumption is that it is proba-bly not useful to add (almost) exact matches to the mem-ory, since adding those is likely to have little effect onthe performance of the k-NN classifier.
More effect canbe expected from adding instances to memory that have alow-entropy class distribution in their nearest neighbourset and of which the nearest neighbours are at a relativelyPrecision Recall F?=1English devel.
84.54% 87.16% 85.83English test 77.01% 80.74% 78.83German devel.
64.01% 52.29% 57.56German test 66.71% 56.47% 61.16Table 1: Overall results (precision, recall, F-rate) of theinitial system on the test sets of both languages.settings mvdm feat.
weight k dist.
wEng, initial yes gain ratio 21 IL 1Eng, seedlist yes gain ratio 5 ID 1Ger, initial yes gain ratio 21 IL 2Ger, seedlist yes gain ratio 9 ID 2Table 2: Optimal parameter settings estimated by itera-tive deepening.
?w?
stands for window.large distance.
A large distance entails that the instancescontains previously unseen feature values (words), andassuming that the predicted class label is correct, thesenew values can be valuable in matching and thereforeclassifying new test material better.We applied our selection method to the first 2 millionwords of the unannotated English dataset.
For Germanwe were able to process 0.25 million words.
First weapplied the classifier with two extensions, seed list infor-mation and second stage stacking, to classify the unan-notated data.
We selected instances with an entropy inthe class distribution lower than 0.05 and a distance ofthe nearest neighbour of at least 0.1.
For English, in total179,391 instances (9%) were selected from the unanno-tated dataset and added to the training set.
For German.markedly less instances were selected: 467 (0.19%).5 Results5.1 Initial classifier: Iterative deepeningIterative deepening produced estimations of optimal pa-rameter settings for our initial systems for the two lan-guages, displayed in the first and third row of Table 2.With this setting we achieved an overall F-rate of 78.83for English and 61.16 for German.
Table 1 lists the fullevaluation results.5.2 First and second extension: seed list featuresand stackingWe have also performed iterative deepening in the exper-iment with the seed list information.
This altered the bestsetting found by the iterative deepening process (the sec-ond and fourth rows of Table 2).
The results on the En-glish development set are slightly better than the initialsystem, as can be seen in Table 3.
The classifier withPrecision Recall F?=1English devel.
85.04% 87.26% 86.14English test 75.03% 79.75% 77.32German devel.
65.27% 50.76% 57.11German test 69.31% 55.70% 61.77Table 3: Overall results (precision, recall, F-rate) of thesystem with seed-list features on the test sets of both lan-guages.Precision Recall F?=1English devel.
85.98% 87.63% 86.80English test 76.26% 80.21% 78.18German devel.
68.80% 52.29% 59.42German test 71.19% 56.38% 62.93Table 4: Overall results (precision, recall, F-rate) of thesystem with seed-list features and second-stage stackingon the test sets of both languages.seed list information performs worse on the English testset than the one without seed lists.
The reverse effect isseen on the German data.
On the development set, usingthe seed list information gave a slight lower performance,but on the test set it has a slightly positive effect.Our second extension, stacking, improves on all over-all F-scores of both languages as compared to the seed-list extended systems, as shown in Table 4.5.3 Third extension: Selecting instances fromunannotated dataThe three extensions, using seed list information, per-forming second stage stacking and adding informationfrom unannotated data, are combined in the final experi-ment.
This experiment achieves the highest result on theEnglish development set, and on both German test sets,as listed in Table 5.
The positive effect of adding selectedunannotated data on the German test sets is rather mini-mal, but we added only a very small amount of unlabeledmaterial.
The performance on the English test set is notbetter than the initial classifier.6 DiscussionIn this paper we have presented a memory-based named-entity recognition system that chunks and labels namedentities in one shot.
We reported on three extensions; in-corporating seed list information, second-stage stackingand adding selected instances from classified unannotateddata to the training material.First, we trained and tested a basic classifier withoutany of the extensions.
Subsequently, we found that (i) in-corporating seed list information as binary features doesnot always help; only in two of the four test sets theseedlists had a positive effect.
There can be several ex-planations for this, such as the quality of the seed lists,the chosen parameter setting from the iterative deepeningprocess or overestimated weights given to the features bythe classifier.
Due to the tight time schedule we could notfurther investigate this.Second, second-stage stacking improves generalisa-tion performance consistently on all test sets as comparedto the seed-list extended systems.Third, only in the final experiment we added selectedclassified instances from unannotated data.
This gave anadditional reasonable boost in performance on the En-glish development set, it attains an overall F-rate of 86.97(an error reduction of 8%) over the initial classifier.
Thesame effect was seen on both German test sets, on whichthe combination of the three extensions achieved a Fscoreof 59.58 ( 5% error reduction ) and 63.02 ( 5% error re-duction).
This effect is not seen on the English test set;here the initial classifier performs best.
This can partlybe explained by the fact that the last two extensions werebuilt upon the first extension, which had a markedly lowerscore than the initial classifier to begin with.In sum, our results suggest that two of the three ex-tensions, the stacking method, and the unlabeled instanceselection method, have been consistently helpful.
Seedlist features, however, have not.ReferencesR.
H. Baayen, R. Piepenbrock, and H. van Rijn.
1993.The CELEX lexical data base on CD-ROM.
LinguisticData Consortium, Philadelphia, PA.S.
Buchholz and A.
Van den Bosch.
2000.
Integratingseed names and n-grams for a named entity list andclassifier.
In LREC-2000 (Second International Con-ference on Language Resources and Evaluation) Pro-ceedings.
Vol.
II, pages 1215?1221.X.
Carreras, L. Marques, and L. Padro.
2002.
Namedentity extraction using AdaBoost.
In Proceedings ofCoNLL-2002, pages 167?170.S.
Cucerzan and D. Yarowsky.
1999.
Language indepen-dent named entity recognition combining morpholog-ical and contextual evidence.
In Proceedings of 1999Joint SIGDAT Conference on EMNLP and VLC.W.
Daelemans, J. Zavrel, K. Van der Sloot, andA.
Van den Bosch.
2002.
TiMBL: Tilburg MemoryBased Learner, version 4.3, reference guide.
TechnicalReport ILK-0210, ILK, Tilburg University.R.
Kohavi and G. John.
1997.
Wrappers for featuresubset selection.
Artificial Intelligence Journal, 97(1?2):273?324.F.
Provost, D. Jensen, and T. Oates.
1999.
Efficient pro-gressive sampling.
In Proceedings of the Fifth Interna-English devel.
Precision Recall F?=1LOC 89.42% 91.13% 90.27MISC 91.36% 80.26% 85.45ORG 74.32% 83.30% 78.55PER 90.16% 91.53% 90.84Overall 86.16% 87.80% 86.97English test Precision Recall F?=1LOC 80.81% 86.33% 83.48MISC 66.96% 75.93% 71.16ORG 69.24% 73.99% 71.54PER 83.98% 82.00% 82.98Overall 76.33% 80.17% 78.20German devel.
Precision Recall F?=1LOC 61.90% 69.60% 65.52MISC 83.25% 32.97% 47.23ORG 67.55% 49.32% 57.01PER 73.40% 54.96% 62.86Overall 68.88% 52.49% 59.58German test Precision Recall F?=1LOC 63.36% 64.83% 64.09MISC 75.60% 32.84% 45.79ORG 62.90% 48.90% 55.02PER 83.47% 67.62% 74.71Overall 71.15% 56.55% 63.02Table 5: Results on the test sets of the variant combiningall three extensions to the initial classifier.tional Conference on Knowledge Discovery and DataMining, pages 23?32.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.Introduction to the conll-2003 shared task: Language-independent named entity recognition.
In Proceedingsof CoNLL-2003.
Edmonton, Canada.A.
Van den Bosch and W. Daelemans.
1999.
Memory-based morphological analysis.
In Proceedings of the37th Annual Meeting of the Association for Compu-tational Linguistics, pages 285?292, New Brunswick,NJ.
ACL.J.
Veenstra.
1998.
Fast np chunking using memory-based learning techniques.
In Proceedings of Bene-learn 1998, pages 71?79.D.
H. Wolpert.
1992.
On overfitting avoidance as bias.Technical Report SFI TR 92-03-5001, The Santa FeInstitute.D.
Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In ACL33,pages 189?196, Cambridge, MA.
