Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2060?2065,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLearning to Identify Metaphors from a Corpus of ProverbsGo?zde O?zbal?
and Carlo Strapparava?
and Serra Sinem Tekirog?lu?
and Daniele Pighin?
?FBK-Irst, Trento, Italy, ?Google - Zu?rich, Switzerlandgozbalde@gmail.com, {strappa, tekiroglu}@fbk.eu, biondo@google.comAbstractIn this paper, we experiment with a re-source consisting of metaphorically annotatedproverbs on the task of word-level metaphorrecognition.
We observe that existing featuresets do not perform well on this data.
Wedesign a novel set of features to better cap-ture the peculiar nature of proverbs and wedemonstrate that these new features are signif-icantly more effective on the metaphoricallydense proverb data.1 IntroductionRecent years have seen a growing attention to-wards attempts to understand figurative language intext (Steen et al, 2010, Shutova and Teufel, 2010,Turney et al, 2011, Neuman et al, 2013, Klebanovet al, 2015).
Recently, O?zbal et al (2016) publisheda resource consisting of 1,054 proverbs annotatedwith metaphors at the word and sentence level, mak-ing it possible for the first time to test existing mod-els for metaphor detection on such data.
More thanin other genres, such as news, fiction and essays, inproverbs metaphors can resolve a significant amountof the figurative meaning (Faycel, 2012).
The rich-ness of proverbs in terms of metaphors is very fas-cinating from a linguistic and cultural point of view.Due to this richness, proverbs constitute a challeng-ing benchmark for existing computational models ofmetaphoricity.In this paper, we devise novel feature sets es-pecially tailored to cope with the peculiarities ofproverbs, which are generally short and figurativelyrich.
To the best of our knowledge, this is thefirst attempt to design a word-level metaphor rec-ognizer specifically tailored to such metaphoricallyrich data.
Even though some of the resources that weuse (e.g., imageability and concreteness) have beenused for this task before, we propose new ways ofencoding this information, especially with respect tothe density of the feature space and the way that thecontext of each word is modeled.
On the proverbdata, the novel features result in compact modelsthat significantly outperform existing features de-signed for word-level metaphor detection in othergenres (Klebanov et al, 2014), such as news and es-says.
By also testing the new features on these othergenres, we show that their generalization power isnot limited to proverbs.2 BackgroundIn this section we provide a brief overview of theefforts of the NLP community to build metaphordatasets and utilize them to develop computationaltechniques for metaphor processing.
Steen et al(2010) construct the Amsterdam Metaphor Cor-pus (VUAMC) by annotating a subset of BNCBaby1.
Linguistic metaphors in VUAMC are an-notated by utilizing the Metaphor Annotation Pro-cedure (MIP) proposed by Group (2007).
VUAMCcontains 200,000 words in sentences sampled fromvarious genres (news, fiction, academic, and conver-sations) and 13.6% of the words are annotated asmetaphoric (Shutova, 2010).
Another metaphor an-notation study following the MIP procedure is con-ducted by Shutova and Teufel (2010).
A subset of1http://www.natcorp.ox.ac.uk/corpus/babyinfo.html2060the British National Corpus (BNC) (Burnard, 2000)is annotated to reveal word-level verb metaphorsand to determine the conceptual mappings of themetaphorical verbs.Turney et al (2011) introduce an algorithm toclassify word-level metaphors expressed by an ad-jective or a verb based on their concreteness levelsin association with the nouns they collocate.
Sim-ilarly, Neuman et al (2013) extend the concrete-ness model with a selectional preference approachto detect metaphors formed of concrete concepts.They focus on three types of metaphors: i) IS-A,ii) verb-noun, iii) adjective-noun.
Rather than re-stricting the identification task to a particular POSor metaphoric structure, Hovy et al (2013) aim torecognize any word-level metaphors given an un-restricted text, and they create a corpus containingsentences where one target token for each sentenceis annotated as metaphorical or literal.
They useSVM and CRF models with dependency tree-kernelsto capture the anomalies in semantic patterns.
Kle-banov et al (2014) propose a supervised approachto predict the metaphoricity of all content words in arunning text.
Their model combines unigram, topicmodel, POS and concreteness features and it is eval-uated on VUAMC and a set of essays written for alarge-scale assessment of college graduates.
Follow-ing this study, Klebanov et al (2015) improve theirmodel by re-weighting the training examples and re-designing the concreteness features.The experiments in this paper are carried out onPROMETHEUS (O?zbal et al, 2016), a dataset con-sisting of 1,054 English proverbs and their equiv-alents in Italian.
Proverbs are annotated with word-level metaphors, overall metaphoricity, meaning andcentury of first appearance.
For our experiments, weonly use the word-level annotations on the Englishdata.3 Word-level metaphor detectionSimilarly to Klebanov et al (2014), we classify eachcontent word (i.e., adjective, noun, verb or adverb)appearing in a proverb as being used metaphoricallyor not.
Out of 1,054 proverbs in PROMETHEUS, werandomly sample 800 for training, 127 for develop-ment and 127 for testing.
We carry out the develop-ment of new features on the development set; thenwe compare the performance of different feature setsusing 10-fold cross validation on the combination ofthe development and training data.
Finally, we testthe most meaningful configurations on the held-outtest data.
As a baseline, we use a set of featuresvery similar to the one proposed by Klebanov et al(2014).
To obtain results more easily comparablewith Klebanov et al (2014), we use the same clas-sifier, i.e., logistic regression, in the implementationbundled with the scikit-learn package (Pedregosa etal., 2011).
For all the experiments, we adjust theweight of the examples proportionally to the inverseof the class frequency.3.1 Baseline features (B)Unigrams (uB): Klebanov et al (2014) use all con-tent word forms as features without stemming orlemmatization.
To reduce sparsity, we consider lem-mas along with their POS tag.Part-of-speech (pB): The coarse-grained part-of-speech (i.e., noun, adjective, verb or adverb) of con-tent words2.Concreteness (cB): We extract the concretenessfeatures from the resource compiled by Brysbaert etal.
(2014).
Similarly to Klebanov et al (2014), themean concreteness ratings, ranging from 1 to 5, arebinned in 0.25 increments.
We also add a binary fea-ture which encodes the information about whetherthe lemma is found in the resource.Topic models (tB): We use Latent Dirichlet Alloca-tion (LDA) (Blei et al, 2003) using Gibbs samplingfor parameter estimation and inference (Griffiths,2002).
We run LDA on the full British National Cor-pus (Consortium and others, 2001) to estimate 100topics, using 2000 Gibbs sampling iterations, andkeeping the first 1000 words for each topic.
As topicmodel features for a lemma, we use the conditionalprobability of the topic given the lemma for each ofthe 100 topics generated by LDA.
Besides, we use abinary feature that encodes whether the lemma ex-ists in the LDA model.3.2 Novel features (N )We introduce five feature sets that capture other as-pects of the data which we consider to be meaningfulfor the peculiar characteristics of proverbs.2Klebanov et al (2014) consider the Penn Treebank tagsetgenerated by Stanford POS tagger.2061Imageability (i) and Concreteness (c): Imageabil-ity and concreteness of the metaphor constituentswere found to be highly effective in metaphor iden-tification by several studies in the literature (Turneyet al, 2011, Broadwell et al, 2013, Neuman et al,2013, Tsvetkov et al, 2014).
We obtain the image-ability and concreteness scores of each lemma fromthe resource constructed by Tsvetkov et al (2014),as it accounts for both dimensions.
The imageabil-ity (concreteness) feature set contains the followingfour features:?
Has score: A binary feature that indicateswhether the lemma exists in the relevant re-source.?
Score value: The imageability (concreteness)score of the lemma.?
Average sentence score: The average image-ability (concreteness) score of the other lem-mas in the sentence.?
Score difference: The difference between Av-erage sentence score and Score value.The last two features take the context of the targetlemma into account and encode the intuition thatmetaphorical lemmas often have higher imageability(concreteness) than the rest of the sentence (Broad-well et al, 2013).Metaphor counts (m): This feature set consists ofthree features.
The first two features encode thenumber of times a lemma-POS pair is used as ametaphor and a non-metaphor in the data.
The thirdfeature evaluates to the difference between thesecounts3.Standard domains (ds) and normalized domains(dn): These features reflect our intuition that thereis a strong prior for some domains to be used asa source for metaphors.
This notion is backed bythe analysis of PROMETHEUS carried out by O?zbalet al (2016).
We also expect that words which areclearly out of context with respect to the rest of thesentence are more likely to be used as metaphors.The correlation between word and sentence domainsdescribed below aims to model such phenomenon.For each lemma-POS pair, we collect the domaininformation from WordNet Domains4 (Magnini etal., 2002, Bentivogli et al, 2004) for the standard3 Counts are estimated on training folds.
To reduce over-fitting, lemmas are randomly sampled with a probability of 2/3.4We always select the first sense of the lemma-POS.Feature sets C P R FB# 0.9 0.666 0.832 0.738N?
0.6 0.785 0.884 0.833B ?N?
0.6 0.798 0.875 0.834N \ i?
0.6 0.788 0.886 0.833N \ c?
0.6 0.782 0.888 0.831N \m?# 0.6 0.780 0.824 0.799N \ d?s 1.0 0.787 0.842 0.815N \ d?n 1.0 0.789 0.884 0.832N \ (ds ?
dn)# 1.0 0.746 0.704 0.724N \ s?
1.0 0.776 0.909 0.836(N \ (ds ?
dn)) ?
t#B 0.6 0.751 0.705 0.724Table 1: Cross-validation performance on the proverb trainingand development data.
The meta-parameter C is the inverse ofthe regularization strength.
?
: significantly different from Bwith p < .001; #: s.d.
from N with p < .001.domains feature set, which consists of 167 features(1 real valued, 166 binary).
It includes a binary in-dicator set to 1 if the lemma is found in WordNetDomains.
A domain vector consisting of 164 binaryindicators mark the domains to which the lemma be-longs.
Then, we compute a sentence domain vectorby summing the vectors for all the other lemmas inthe sentence, and we encode the Pearson correlationcoefficient between the two vectors (lemma and sen-tence) as a real valued feature.
Finally, a binary fea-ture accounts for the cases in which no other lemmain the sentence has associated domain information.The same process is repeated for the normalizeddomains.
For normalization, we use a reduced setof domains (43 distinct domains) by considering themiddle level of the WordNet Domains hierarchy.
Forinstance, VOLLEY or BASKETBALL domains aremapped to the SPORT domain.
Normalization al-ready proved to be beneficial in tasks such as wordsense disambiguation (Gliozzo et al, 2004).
It al-lows for a good level of abstraction without losingrelevant information and it helps to overcome datasparsity.
The set of normalized domain features (dn)consists of 46 features (45 binary, 1 real valued).Dense signals (s): This set includes three binaryfeatures which summarize the concreteness, image-ability and metaphor count feature sets.
The first(second) feature is set to 1 if the imageability (con-creteness) of the lemma is higher than the average2062Features P R FB# 0.75 0.70 0.73N?
0.86 0.83 0.85N \ s?
0.82 0.87 0.85B ?N?
0.87 0.85 0.86Table 2: Performance on the proverb test data.
?
: significantlydifferent from B with p < .001.
#: significantly different fromN with p < .001.Genre Features C P R FNewsB 1.0 0.475 0.742 0.576N 1.0 0.576 0.479 0.522B ?N 1.0 0.615 0.539 0.574AcademicB 0.6 0.489 0.733 0.568N 0.6 0.572 0.494 0.511B ?N 1.0 0.539 0.648 0.569ConversationB 0.6 0.292 0.799 0.416N 0.6 0.304 0.626 0.393B ?N 1.0 0.299 0.731 0.406FictionB 0.6 0.349 0.695 0.460N 0.6 0.430 0.418 0.421B ?N 0.6 0.409 0.551 0.465Table 3: Cross-validation performance on VUAMC.
B is al-ways significantly different from N (p < .001), and B ?
N isalways significantly different from both B and N (p < .001).imageability (concreteness) of the rest of the sen-tence.
The third feature is set to 1 if the lemma wasobserved more frequently as a metaphor than not, asestimated on training data.3.3 ResultsTable 1 shows the results of the 10-fold cross valida-tion on the English proverb data.
The value reportedin the column labeled C is the optimal inverse ofregularization strength, determined via grid-searchin the interval [0.1, 1.0] with a step of 0.1.
Usingonly baseline features (B) we measure an average F1score of 0.738.
The performance goes up to 0.833when the novel features are used in isolation (N )(statistically significant with p < 0.001).
We believethat the difference in performance is at least in partdue to the sparser B features requiring more datato be able to generalize.
But most importantly, un-like B, N accounts for the context and the peculiar-ity of the target word with respect to the rest of thesentence.
The combination of the two feature sets(B ?N ) very slightly improves over N (0.834), butthe difference is not significant.
The second block ofrows in Table 1 presents a summary of the ablationtests that we conducted to assess the contribution ofthe different feature groups.
Each lowercase letterindicates one of the feature sets introduced in theprevious section.
All configurations reported, exceptN \ (ds ?
dn), significantly outperform B.
In twocases, N \m and N \ (ds?dn), there is a significantloss of performance with respect to N .
The worstperformance is observed when all the domain fea-tures are removed (i.e., N \ (ds?dn)).
These resultssuggest that the prior knowledge about the domainof a word and the frequency of its metaphorical useare indeed strong predictors of a word metaphoricityin context.
The fact that N \dn and N \ds do not re-sult in the same loss of performance as N \(ds?dn)indicates that both dn and ds are adequately expres-sive to model the figuratively rich proverb data.
Inone case (i.e., N \ s), the F1 measure is slightlyhigher than N , even though the difference does notappear to be statistically significant.
Our intuition isthat each of the three binary indicators is a very goodpredictor of metaphoricity per se, and due to the rel-atively small size of the data the classifier may tendto over-fit on these features.
As another configura-tion, the last row shows the results obtained by re-placing our domain features ds and dn with the topicfeatures t from B.
With this experiment, we aim tounderstand the extent to which the two features areinterchangeable.
The results are significantly worsethan N , which is a further confirmation of the suit-ability of the domain features to model the proverbsdataset.We then evaluated the best configuration from thecross-fold validation (N \ s) and the three featuresets B, N and B ?
N on the held-out test data.The results of this experiment reported in Table 2are similar to the cross-fold evaluation, and in thiscase the contribution of N features is even more ac-centuated.
Indeed, the absolute F1 of N and B ?Nis slightly higher on test data, while the f-measure ofB decreases slightly.
This might be explained by thelow-dimensionality of N , which makes it less proneto overfitting the training data.
On test data, N \ sis not found to outperform N .
Interestingly, N \ sis the only configuration having higher recall thanprecision.
As shown by the feature ablation experi-2063ments, one of the main reasons for the performancedifference between N and B is the ability of the for-mer to model domain information.
This finding canbe further confirmed by inspecting the cases whereB misclassifies metaphors that are correctly detectedby N .
Among these, we can find several examplesincluding words that belong to domains often usedas a metaphor source, such as ?grist?
(domain: ?gas-tronomy?)
in ?All is grist that comes to the mill?,or ?horse?
(domain: ?animals?)
in ?You can take ahorse to the water , but you can?t make him drink?.Finally, Table 3 shows the effect of the differentfeature sets on VUAMC used by Klebanov et al(2014).
We use the same 12-fold data split as Kle-banov et al (2014), and also in this case we per-form a grid-search to optimize the meta-parameterC of the logistic regression classifier.
The best valueof C identified for each genre and feature set isshown in the column labeled C. On this data, Nfeatures alone are significantly outperformed by B(p < 0.01).
On the other hand, for the genres?academic?
and ?fiction?, combining N and B fea-tures improves classification performance over B,and the difference is always statistically significant.Besides, the addition of N always leads to more bal-anced models, by compensating for the relativelylower precision of B.
Due to the lack of a separatetest set, as in the original setup by Klebanov et al(2014), and to the high dimensionality of B?s lex-icalized features, we cannot rule out over-fitting asan explanation for the relatively good performanceof B on this benchmark.
It should also be noted thatthe results reported in (Klebanov et al, 2014) are notthe same, due to the mentioned differences in the im-plementation of the features and possibly other dif-ferences in the experimental setup (e.g., data filter-ing, pre-processing and meta-parameter optimiza-tion).
In particular, our implementation of the Bfeatures performs better than reported by Klebanovet al (2014) on all four genres, namely: 0.52 vs.0.51 for ?news?, 0.51 vs. 0.28 for ?academic?, 0.39vs.
0.28 for ?conversation?
and 0.42 vs. 0.33 for?fiction?.Even though the evidence is not conclusive, theseresults suggest that the insights derived from theanalysis of PROMETHEUS and captured by the fea-ture set N can also be applied to model word-levelmetaphor detection across very different genres.
Inparticular, we believe that our initial attempt to en-code context and domain information for metaphordetection deserves further investigation.4 ConclusionWe designed a novel set of features inspired by theanalysis of PROMETHEUS, and used it to train andtest models for word-level metaphor detection.
Thecomparison against a strong set of baseline featuresdemonstrates the effectiveness of the novel featuresat capturing the metaphoricity of words for proverbs.In addition, the novel features show a positive con-tribution for metaphor detection on ?fiction?
and?academic?
genres.
The experimental results alsohighlight the peculiarities of PROMETHEUS, whichstands out as an especially dense, metaphoricallyrich resource for the investigation of the linguisticand computational aspects of figurative language.ReferencesLuisa Bentivogli, Pamela Forner, Bernardo Magnini, andEmanuele Pianta.
2004.
Revising the Wordnet Do-mains Hierarchy: Semantics, Coverage and Balanc-ing.
In Proceedings of the Workshop on MultilingualLinguistic Ressources, pages 101?108.
Association forComputational Linguistics.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent Dirichlet Allocation.
The Journal of Ma-chine Learning Research, 3:993?1022.George Aaron Broadwell, Umit Boz, Ignacio Cases,Tomek Strzalkowski, Laurie Feldman, Sarah Taylor,Samira Shaikh, Ting Liu, Kit Cho, and Nick Webb.2013.
Using Imageability and Topic Chaining to Lo-cate Metaphors in Linguistic Corpora.
In Social Com-puting, Behavioral-Cultural Modeling and Prediction,pages 102?110.
Springer.Marc Brysbaert, Amy Beth Warriner, and Victor Kuper-man.
2014.
Concreteness Ratings for 40 ThousandGenerally Known English Word Lemmas.
BehaviorResearch Methods, 46(3):904?911.Lou Burnard.
2000.
Reference guide for the British Na-tional Corpus (World Edition).BNC Consortium et al 2001.
The British National Cor-pus, version 2 (BNC World).
Distributed by OxfordUniversity Computing Services.Dahklaoui Faycel.
2012.
Food Metaphors in TunisianArabic Proverbs.
Rice Working Papers in Linguistics3/1.2064Alfio Gliozzo, Carlo Strapparava, and Ido Dagan.
2004.Unsupervised and Supervised Exploitation of Seman-tic Domains in Lexical Disambiguation.
ComputerSpeech & Language, 18(3):275?299.Tom Griffiths.
2002.
Gibbs Sampling in the GenerativeModel of Latent Dirichlet Allocation.Pragglejaz Group.
2007.
MIP: A Method for IdentifyingMetaphorically Used Words in Discourse.
Metaphorand Symbol, 22(1):1?39.Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-ney Sanders, and Eduard Hovy.
2013.
Identi-fying Metaphorical Word Use with Tree Kernels.Meta4NLP 2013, page 52.Beata Beigman Klebanov, Chee Wee Leong, MichaelHeilman, and Michael Flor.
2014.
Different Texts,Same Metaphors: Unigrams and Beyond.
In Proceed-ings of the Second Workshop on Metaphor in NLP,pages 11?17.Beata Beigman Klebanov, Chee Wee Leong, and MichaelFlor.
2015.
Supervised Word-Level Metaphor Detec-tion: Experiments with Concreteness and Reweightingof Examples.
NAACL HLT 2015, page 11.Bernardo Magnini, Carlo Strapparava, Giovanni Pezzulo,and Alfio Gliozzo.
2002.
The Role of Domain Infor-mation in Word Sense Disambiguation.
Natural Lan-guage Engineering, 8(04):359?373.Yair Neuman, Dan Assaf, Yohai Cohen, Mark Last,Shlomo Argamon, Newton Howard, and OphirFrieder.
2013.
Metaphor Identification in Large TextsCorpora.
PloS one, 8(4):e62343.Go?zde O?zbal, Carlo Strapparava, and Serra SinemTekirog?lu.
2016.
PROMETHEUS: A Corpus ofProverbs Annotated with Metaphors.
In Proceed-ings of the Tenth International Conference on Lan-guage Resources and Evaluation (LREC 2016), Paris,France, may.
European Language Resources Associa-tion (ELRA).F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine Learning in Python.Journal of Machine Learning Research, 12:2825?2830.Ekaterina Shutova and Simone Teufel.
2010.
MetaphorCorpus Annotated for Source-Target Domain Map-pings.
In LREC, volume 2, pages 2?2.Ekaterina Shutova.
2010.
Automatic Metaphor Interpre-tation as a Paraphrasing Task.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 1029?1037.
Associationfor Computational Linguistics.Gerard J Steen, Aletta G Dorst, J Berenike Herrmann,Anna A Kaal, and Tina Krennmayr.
2010.
Metaphorin Usage.
Cognitive Linguistics, 21(4):765?796.Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, EricNyberg, and Chris Dyer.
2014.
Metaphor Detectionwith Cross-Lingual Model Transfer.
In Proceedings ofthe 52nd Annual Meeting of the Association for Com-putational Linguistics, pages 248?258.
Association forComputational Linguistics.Peter D Turney, Yair Neuman, Dan Assaf, and Yohai Co-hen.
2011.
Literal and Metaphorical Sense Identi-fication through Concrete and Abstract Context.
InProceedings of the 2011 Conference on the EmpiricalMethods in Natural Language Processing, pages 680?690.2065
