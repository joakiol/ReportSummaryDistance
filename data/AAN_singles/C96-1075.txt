Multi-l ingual Translation of Spontaneously Spoken Languagein a Limited DomainAlon Lavie, Donna Gates, Marsal Gavaldh,Laura Mayfield, Alex Waibel and Lor i  Lev inCenter  for Mach ine  Trans la t ionCarneg ie  Mel lon Un ivers i ty5000 Forbes  Ave. ,  P i t t sburgh ,  PA 15213emai l  : l av ie@cs .cmu.eduAbst rac tJANUS is a multi-lingual speech-to-speech translation system designed tofacilitate communication between twoparties engaged in a spontaneous con-versation in a limited domain.
In anattempt o achieve both robustness andtranslation accuracy we use two (lifter-ent translation components: the GLRmodule, designed to be more accu-rate, and the Phoenix module, designedto be more robust.
We analyze thestrengths and weaknesses of each of theapproaches and describe our work oncombining them.
Another recent focushas been on developing a detailed end-to-end evaluation procedure to measurethe performance and effectiveness of thesystem.
We present our most recentSpanish-to-English performance evalua-tion results.1 In t roduct ionJANUS is a multi-lingual speech-to-speech trans-lation system designed to facilitate communica-tion between two parties engaged in a sponta-neous conversation in a limited domain.
In thispaper we describe the current design and perfor-mance of the machine translation module of oursystem.
The analysis of spontaneous speech re-quires dealing with problenls such as speech dis-fluencies, looser notions of grammaticality and thelack of clearly marked sentence boundaries.
Theseproblems are further exacerbated by errors of thespeech recognizer.
We describe how our machinetranslation system is designed to effectively han-dle these and other problems, hi an attemptto achieve both robustness and translation accu-racy we use two different ranslation components:the (JLlt.
module, designed to be more accurate,and the Phoenix module, designed to be more ro-bust.
Both modules follow an interlingua-basedapproach.
The translation modules are designedto be language-independent in the sense that theyeach consist of a general processor that applies in-dependently specified knowledge about differentlanguages.
This facilitates the easy adaptation ofthe system to new languages and domains.
We an-alyze the strengths and weaknesses of each of thetranslation approaches and describe our work oncombining them.
Our current system is designedto translate spontaneous dialogues in the Schedul-ing domain, with English, Spanish and German asboth source and target languages.
A recent focushas been on developing a detailed end-to-end eval-uation procedure to measure the performance andeffectiveness of the system.
We describe this pro-cedure in the latter part of the paper, and presentour most recent Spanish-to-English performanceevaluation results.2 System Overv iewThe JANUS System is a large scale multi-lingualspeech-to-speech translation system designed tofacilitate communication between two parties en-gaged in a spontaneous conversation i  a limiteddomain.
A diagram of the architecture of the sys-tem is shown in Figure 1.
The system is com-posed of three main components: a speech recog-nizer, a machine translation (MT) module and aspeech synthesis module.
The speech recognitioncomponent of the system is described elsewhere(Woszczyna et al 1994).
For speech synthesis, weuse a commercially available speech synthesizer.The MT module is composed of two separatetranslation sub-modules which operate indepen-dently.
The first is the GLR module, designedto be more accurate.
The second is the Phoenixmodule, designed to be more robust.
Both mod-ules follow an interlingua-based approach.
Thesource language input string is first analyzed bya parser, which produces a language-independentinterlingua content representation.
The interlin-gua is then passed to a generation component,which produces an output string in the target lan-guage.The discourse processor is a component of theGLR translation module.
It disaInbiguates thespeech act of each sentence, normalizes temporal442S  .
 i n  Source Language > JI r IOU ~ "FGenKit enerator \[.
.
.
.I s ?oo.
IFigure 1: The  JANUS Systemexpressions, and incorporates the sentence into adiscourse plan tree.
'the discourse processor alsoupdates a calendar which keeps track of what thespeakers haw'~ said about their schedules.
The dis-course processor is described in greater detail else.-where (R,osd et 31.
1995).3 The  QLR Trans lat ion  Modu leThe (\]LR.
* parser (Lavie and Tomita 11993; I,avie1994) is a parsing system based on Tomita's Gen-eralized LI~ parsing algorithm (Tomita 1987).
Theparser skips parts of the utterance that it cannotincorporate into a well-formed sentence structure.Thus it is well-suited to doinains ill which non-grammaticality is coal i t ion.
The  parser conductsa search for the maximal subset of the originalinput that is covered by the grammar.
This isdone using a beam search heuristic that limits tilecombinations of skipped words considered by theparser, and ensures that it operates within feasibletime and space bonnds.The GI,R* parser was implemented as an ex-tension to the G LR parsing system, a unification-I>ased practical natural language system ('lbmita1990).
The grammars we develop for the ,IAN USsystem are designed to produce \[eature struc-tures that correspond to a frame-based language-independent representation f the meaning of theinput utterance.
For a given input utterance., theparser produces a set; of interlingua texts, or ILTs.The main components of an ILT are the speechact (e.g., suggest, accept, reject), the sentencetype (e.g., s tate ,  query- i~,  fragment), and themain semantic frame (e.g., free, busy).
An ex-ample of nn ILl' is shown in Figure 2.
A detailedIUI' Specitication was designed as a formal de~scription of the allowable ILTs.
All parser outputmust conform to this ILl' Speeitication.
The GLRunification based formalism allows the grammarsto construct precise and very detailed ILTs.
Thisin turn allows the G LI{ translation module to pro-duce highly accurate translations for well-formedinput.The G LR* parser also includes everal tools de-signed to address the difficulties of parsing spon-taneous peech.
To cope with high levels of am-biguity, the parser includes a statis|,ical disam-biguation module, in which probabilities are at-tached directly to the actions in the LR parsingtable.
The parser can identify sentence bound-aries within each hypothesis with the help of astatistical method that determines the probabil-ity of a boundary at; each point in the utterance.The parser must also determine the "best" parsefrom among tit(; diflZrent parsable subsets of aninput.
This is don(; using a collection of parseevaluation measures which are combined into anintegrated heuristic for evaluating and ranking theparses produced by the parser.
Additionally, aparse quality heuristic allows the parser to self-443((frame *free)(who ((frame*i)))(when ((frame *.simple-time)(day-of-week wednesday)(time-of-day morning)))(a-speech-act (*multiple* *suggesic *aec(,pt))(sentence-type *state)))Sentence: 1could do it Wednesday morning too.Figure 2: An Example 112'judge the quality of tile parse chosen as best, andto detect cases in which important information islikely to have been skipt)ed.Target language generation in the (;LR modtdeis clone using GenKit (Tomita and Nyberg 1988),a unification-based generation system.
With well-developed generation grammars, GenKit results invery accurate translation for well-specified IUI%.4 The Phoenix Translation ModuleThe ,IANUS Phoenix translation module (May-field et el.
1995) is an extension of the PhoenixSpoken Language System (Ward 1991; Ward1994).
The translation component consists of at)arsing module and a generation module.
Trans-lation between any of the four source languages(English, German, SpanisIL Korean) and five tar-get languages (English, German, Spanish, Korean,Japanese) is possible, although we currently focusonly on a few of these language pairs.Unlike the GI, R method which attempts to con-struct a detailed tur  for a given input utterance,the Phoenix approach attempts to only identifythe key semantic concepts represented in the ut-terance and their underlying structure.
WhereasGLR* is general enough to support both seman-tic and syntactic grammars (or some combinationof both types), the Phoenix approach was specifi-cally designed for semantic grammars.
Grammat-ical constraints are introduced at the phrase level(as opposed to the sentence level) and regulatesemantic ategories.
This allows the ungrammat-icalities that often occur between phrases to beignored and reflects tile fact that syntactically in-correct spontaneous speech is often semanticallywell-formed.The parsing grammar specifies patterns whichrepresent concepts in the domain.
The patternsare composed of words of the input string as wellas other tokens for constituent concepts.
Elements(words or tokens) in a pattern may be specified asol)tional or repeating (as in a Kleene star mecha-nisln).
Each concept, irrespective of its level in thehierarchy, is represented by a separate grammarfile.
These grammars are compiled into RecursiveTransition Networks (RTNs).The interlingua meaning representation of aninput utterance is derived directly from the 1)arsetree constructed by the parse.r, by extracting therepresented structure of concepts.
This represen-tation is usually less detailed than tile correspond-ing GLR IlfF representation, and thus often re-suits in a somewhat less accurate translation.
Theset of semantic oncept okens for the Schedulingdomain was initially developed from a set of 45example English dialogues.
Top-level tokens, alsocalled slots, represent speech acts, such as sugges-tion or agreement.
Intermediate-level tokens dis-tingnish between points and intervals in time, forexample; lower-level tokens cat)ture the speciiicsof the utterance, such as days of the week, andrepresent he only words that are translated di-rectly via lookup tables.
'File parser matches as much of the inl)ut ut-terance as it can to the patterns pecified by theI~TNs.
Out-of-lexicon words are ignored, unlessthey occur in specific locations where open con-cepts are permitted.
A word that is already knownto the system, however, can cause a concept pat-tern not to match if it occurs in a position un-specified in the grammar.
A failed concept doesnot cause the entire parse to fail.
The parser canignore any number of words in between top-levelconcepts, handling out-of-domain or otherwise un-expected input.
Tile parser has no restrictionson the order in which slots ca~ occur.
This cancause added ambiguity in the segmentation of theutterance into concepts.
The parser uses a dis-ambiguation algorithm that attempts to cover thelargest number of words using the smallest num-ber of concepts.Figure 3 shows an example of a speaker ut-terance and the parse that was produced usingthe Phoenix parser.
The parsed speech recog-nizer outpnt is shown with unknown (-) and un-expected (*) words marked.
These segments ofthe input were ignored by the parser.
The rele-vant concepts, however, are extracted, and strungtogether they provide a general meaning represen-tation of what the speaker actually said.Generation in the Phoenix module is accom-plished using a sirnple strategy that sequentiallygenerates target language text for each of the toplevel concepts in the parse analysis.
Each con-cept has one or more tixed phrasings in the targetlanguage.
Variables such as times and dates areextracted from the parse analysis and translateddirectly.
The result is a meaningfifl translation,but can have a telegraphic feel.5 Combining the GLR andPhoenix Translation Modules5.1 S t rengths  and Weaknesses  of theApproachesAs already described, both the GLR* parser andtile Phoenix parser were specifically designed tohandle tile problems associated with analyzingspontaneous peech, llowever, each of the ap-444( ) r ig ina l  utter\[tn(.'('.
:S\[ QIJE 'I'E \[ 'At{E('E ' \[ 'EN(IO t,;1~ MAt{TES I)IE(\]IO(:\[{OY EL MII;;RCOI,ES DIE(~INIII.
;VE I,IPII{.ES TOI)O El, I)\[APODR(AMOS 11{ 1)E MATINI;" O SEA t.;N I,A TAI{I)E VEI{EL I,A I 'EL\[ f :UI ,A(I{oughly " Yes what  do you th ink \[ have Tuesday  the Iteenth and Wednesday  the nznetccnth  f~cc all day we ,-ou~dqo see tlze matmde so zn the a f te rnoon see the the 7novzc."
)As decoded hy  the.
recogn izer :%NOISE% SII Q1JEI TE I 'AI{ECE %NOISE% ' I 'EN(;O ELMARTES I)IEC, IOCI\[O Y H ,  MIE1RCOLES I ) IEC INUI 'VELIBRES TOI)O I.,L DI1A PODI{ l lAMOS Itt, I)E MATINE1'X:NOISI.
;% O SEA I,A TAI{DE A VEI{ LAParsed  :'~<S> sil quel tc parece lento  (:\[ mattes di,~iocht> y (~1miclrcoles diecinueve libres todo el d i la  podr i lamos  *11% *I)E-MATINF, 1 o sea la tarde a ver I,A %</S>Parse  Tree  (~ Sen lant i c  l~e l ) resentat ion) :\[rcst .
.
.
.
.
t\] ( \[yes\] ( S l l  ))\[yourt:urn\] ( QIII,\]I TE \['AII, I~CE )\[give'info\] ( \ [myavai labi l i ty\]  ( q'ENGO \[temp'loc\]( \[teml .
.
.
.
.
1\] ( \[point:\] ( \[date\] ( EL \[d'o'w\] ( MAI'JI'ES ))\[,late\] ( \[clay'oral\] ( I ) IEC IOCI lO  ) \[,:onj\] ( Y ) El, \[d'o'w\](MIEI I '~COI , I ' ;S) )  \[date\] ( \ [dayord \ ]  ( I ) IE ( : \ [NUEVE) ) ) ) )1,11 HIES ))\[givemfo\] ( \[my'availabil ity\] ( [temp'loc\]( \[temporal\] ( \[range\] ( \[entire\] ( TOl )O )El, \[,mit\]( Jr'unit\] ( I ) I IA  ) ) ) ) ) I 'ODRI1AMOS ))\[suggest\] ( \[suggest'meeting\] ( \[temp'loc\] ( \[temporal\]( O SEA \[point\] ( I,A \[t'o'd\] ( TARDE ))))A W,:Et ))Generat(.~d:English = <Yes what do you think?
1 could meet: Tuesdayeighteenth and Wednesday the nineteenth 1 couhl meet thewhole day do you want to try to get together m the afternoon>Figure 3: A Phoenix Spanish to English Transla-tion Examl)h:proaches has some clear strengths and weaknesses.Although designed t<> COl)e with speech disth|en-cies, (;LR* can graeehdly tolerate only moderatelevels of deviation from the grammar.
When theinput is only slightly ungrammatical, nd containsrelatively minor distluencies, (ILR* produces pre-cise and detailed IH's that result in high qualitytranslations.
The (ILl{* parser has <lifliculties inparsing long utterances that are highly dislluent,or that significantly deviate from the grammar.In many such cases, (I LH,* succeeds to parse onlya small fragment of the entire utterance, and im-portant input segments end up being sldl)t)ed.
1l)hoenix is signitlcantly better suited to analyzingsuch utterances.
Because Phoenix is capable ofskipping over input segments that <1o not corre-spond to any top level semantic concept, it canfar better recover from out-of-domain se.gments inthe input, and "restart" itself on an in-domainsegment that follows.
However, this sometime.s re-suits in the parser picking up and mis-translatinga small parsal)le phrase within an out-of-domainIRccent work on a method for pre-brcaking theutterance at sentence boundaries prior to parsing havesigniii(:antly reduced this l)rol)lem.segtnent.
To handle this problem, we are.
attempt-ing to develop methods for automatically detect-ing out-of-domain segments in an utterance (seesection 7).Because the Phoenix approach ignores smallfmlction words in the mt)ut , its translation resultsare by design bound to be less accurate.
However,the ability to ignore function words is of great ben-ellt when working with speech recognition output,in which such words are often mistaken.
By keyingon high-conlidence words l>hoenix takes advan-tage of the strengths of the speech decoder.
At thecurrent time, Phoenix uses only very simple dis-ambiguation heuristics, does not employ any dis-course knowledge, and does not have a mechanismsimilar to the parse quality heuristic of GLR*,which allows the parser to self-assess the qualityof the produced result.5.2 Combin ing  the Two ApproachesI{ecause ach of the two translation methods ap-pears to perform better on different ypes of utter-ances, they may hopefldly be combined in a waythat takes adwmtage of the strengths of each ofthem.
One strategy that we have investigated isto use the l'hoeIfiX module as a back-up to the(1 Lt{ module.
The parse result of GLR* is trans-lated whenever it is judged by the parse qualityheuristic to be "Good".
Whenever the parse resultt~'om GLI{* is judged as "Bad", the translation isgenerated from the corresponding output of thePhoenix parser.
Results of using this combinationscheme are presented in the next section.
We art:in the process of investigating some more sophisti-cated methods for combining the two translationat)proaehes.6 Eva luat ion6.1  The Ewduat ion  P rocedureIn order to assess the overall eflhctiveness of thetwo translation contponents, we developed a de-tailed end-to-end evaluation procedure (Gates el;hi.
1996).
We evaluate the translation moduleson both transcribed and spee.ch recognized input.The evMuation of transcribed inl)ut allows us toassess how well our translation modnles wouhl\[unction with "perfect" speech recognition.
'lhst-ing is performed on a set; of "unseen" dialogues,that were not used for developing the translationmodules or training the speech recognizer.
'\['he translation of an utterance is manuallyevaluated by assigning it a grade or a set of gradesbased on the number of sentences in the utter-alice.
'file utterances are broken clown into sen-tences for evaluation in order to give more weightto longer utterances, and so that utterances con-taining both in and out-of-domain sentences canbe .iudged more accurately.Each sentence is cla,ssified first as either relevantto the scheduling domain (in-domain) or not rel-445evant to the scheduling domain (out-of-domain).Each sentence is then assigned one of four gradesfor translation quality: (1) Perfect - a fluent trans-lation with all information conveyed; (2) OK -all important information translated correctly butsome unimportant details missing, or the transla-tion is awkward; (3) Bad - unacceptable transla-tion; (4) Recognition Error - unacceptable trans-lation due to a speech recognition error.
Thesegrades are used for both in-domain and out-of-domain sentences.
However, if an out-of-domainsentence is automatically detected as such by theparser and is not translated at all, it is given an"OK" grade.
The evaluations are performed byone or more independent graders.
When morethan one grader is used, the results are averagedtogether.6.2 Resu l tsFigure 4 shows the evaluation results for 16 un-seen Spanish dialogues containing 349 utterancestranslated into English.
Acceptable is the sum of"Perfect" and "OK" sentences.
For speech recog-nized input, we used the first-best hypotheses ofthe speech recognizer.Two trends have been observed from this eval-uation as well as other evaluations that we haveconducted.
First, The GLR translation moduleperforms better than the Phoenix module on tran-scribed input and produces a higher percentage of"Perfect" translations, thus confirming the GLRapproach is more accurate.
This also indicatesthat GLR performance should improve with bet-ter speech recognition and improved pre-parsingutterance segmentation.
Second, the Phoenixmodule performs better than GLR on the first-best hypotheses from the speech recognizer, a re-sult of the Phoenix approach being more robust.These results indicate that combining the twoapproaches has the potential to improve the trans-lation performance.
Figure 5 shows the results ofcombining the two translation methods using thesimple method described in the previous section.The GLR* parse quality judgement is used to de-termine whether to output the GLR translationor the Phoenix translation.
The results were eval-uated only for in-domain sentences, since out-of-domain sentences are unlikely to benefit from thisstrategy.
The combination of the two translationapproaches resulted in a slight increase in the per-centage of acceptable translations on transcribedinput (compared to both approaches separately).On speech recognized input, although the over-all percentage of acceptable translations does notimprove, the percentage of "Perfect" translationswas higher.
22In a more recent evaluation, this combinationmethod resulted in a 9.5% improvement in acceptabletranslations of speech recognized in-domain sentences.Although some variation between test sets is to be ex-7 Conc lus ions  and  Future  WorkIn this paper we described the design of the twotranslation modules used in the .JANUS system,outlined their strengths and weaknesses and de-scribed our etforts to combine the two approaches.A newly developed end-to-end evaluation proce-dure allows us to assess the overall performanceof the system using each of the translations meth-ods separately or both combined.Our evaluations have confirmed that the GLRapproach provides more accurate translations,while the Phoenix approach is more robust.
Com-bining the two approaches using the parse qual-ity judgement of the (ILl{* parser results in im-proved performance.
We are currently investigat-ing other methods for combining the two transla-tion approaches.
Since (\]LR* performs much bet-ter when long utterances are broken into sentencesor sub-utterances which are parsed separately, weare looking into the possibility of using Phoenixto detect such boundaries.
We are also develop-ing a parse quality heuristic for the Phoenix parserusing statistical and other methods.Another active research topic is the automaticdetection of out-of-domain segments and utter-ances.
Our experience has indicated that a largeproportion of bad translations arise from thetranslation of small parsable fragments withinout-of-domain phrases.
Several approaches arennder consideration.
For the Phoenix parser, wehave implemented a simple method that looks forsmall islands of parsed words among non-parsedwords and rejects them.
On a recent test set, weachieved a 33% detection rate of out-of-domainparses with no false alarms.
Another approach weare pursuing is to use word salience measures toidentify and reject out-of-domain segments.We are also working on tightening the couplingof the speech recognition and translation modulesof our system.
We are developing lattice parsingversions of both the GLR* and Phoenix parsers, sothat multiple speech hypotheses can be efficientlyanalyzed in parallel, in search of an interpretationthat is most likely to be correct.AcknowledgementsThe work reported in this paper was funded inpart by grants from ATR - Interpreting Telecom-munications Research Laboratories of Japan, theUS Department of Defense, and the VerbmobilProject of the Federal Republic of Germany.We would like to thank all members of theJANUS teams at the University of Karlsruhe andCarnegie Mellon University for their dedicatedwork on our many evaluations.pected, this result strengthens our belief in the poten-tial of this approach.446In l)omain (605 sentences)(;LI{* Phoenixtranscribed speech lst-best transcribed speech lst-bestPerfect 65.2 34.7 53.3 35.5OK 18,8 12.2 25.3 26.3Bad 16.0 29.2 21.4 17.1l{ecog Err ** 23.9Out of I)omain (d85 sentences)** 21.1Perfect 58.5 29.7 44.2 29.3O K 26.7 42.4 44.6 41.1Bad 7.5 9.1l{ecog Err14.8 11.220.4 **Acceptable (l'erfect + OK)20.5In l)om 84.0 46.9 78.6 61.8Out of Dora 85.2 72. l 88.8 70.4All l)om 84.5 58.2 82.9 65.5ol (,LI{ and l'hoenix.
Cross-grading of 16 dialogues.
Figure 4: September 1995 e.wduation " ' * ' 'In Domain (605 sentences)G L R* wid, Phoenixtranscribed speech lst-bestPerfect 65.4 39.7OK 20.8 21.2Bad 13.8 15.2Recog Err ** 23.9Acceptable (Perfect + OK)\[l In Do,,, I\[- 86.2 I 60.9 llFigure 5: September 1995 evaluation of (ILR* combined with Phoenix.
Cross-grading of 16 dialogues.Re ferencesD.
Gates, A. bavie, L. Levin, A. Waibel,M.
GavaldS., L. Mayfield, M. Woszczyna andP.
Zhan.
End-to-end Evaluation in JANUS:a Speech-to-speech Translation System, To ap-pear in Proceedings of ECAI Workshop on Dia-logue Processing in Spoken Language Systems,Budapest, Hungary, August 1996.A.
l,avie and M. ToInita.
GLR* - An EJficientNoise ,5'kippmg Parsing Algorithm for ContextFree Grammars, Proceedings of the third In-ternational Workshop on Parsing Technologies(IW PT-9a), Tilburg, The Netherlands, August1993.A.
Lavie.
An Integrated Heuristic Scheme forPartial Parse Evaluation, Proceedings of the32nd Annual Meeting of the ACL (ACL-94),Las Cruces, New Mexico, June 1994.L.
Mayfield, M. (lavaldh, Y-H. Seo, B. Suhm,W.
Ward, A. Wail)el.
"Parsing Real Inl)ut inJANUS: a Concept-Based Al)proach."
In Pro-eeedings of TMI 9,5.(:.
P. l{os& B.
Di Eugenio, L. S. Levin, and(;.
Van Ess-I)ykema.
Discourse processing ofdialogues with multiple threads.
In Proceedingsof ACL'95, ftoston, MA, 1995.M.
Tomita.
An Efficient Augmented Context-freeParsing Algorithm.
Computational Linguistics,13(1-2) :3 l -46, 1987.M.
Tomita.
Tile Generalized LR Parser/CompilerVersion 8.4.
In Proceedings of Interna-tional (:onference on Computational Linguis-tics (COLING'90), pages 59-63, llelsinki, Fin-land, 1990.M.
Tomita and E. H. Nyberg 3rd.
Genera-tion Kit and Transformation Kit, Version 3.2:User's Manual.
Technical Report (\]MU-CMT-88-MEMO, Carnegie Mellon University, Pitts-burgh, PA, October \[988.W.
Ward.
"Understanding Spontaneous Speech:tile Phoenix System."
In Proceedings ofI(MSb'P-91, 1991.W.
Ward.
"Extracting Information in Sponta-neous Speech."
In Proceedings of InternationalCoT@rence on Spoken Language, 1994.M.
Woszczyna, N. Aoki-Waibel, F. D. Buo,N.
Coccaro, T. Horiguchi, K. and Kemp,A.
Lavie, A. McNair, T. Polzin, 1.
Rogina, (J. P.Ros6, T. Schultz, B. Suhm, M. Tomita, andA.
Waibel.
JANUS-93: Towards SpontaneousSpeech Translation.
In Proceedings of IEEEInternational Conference on Acoustics, Speechand Signal Processing (ICASSP'9~), 1994.447
