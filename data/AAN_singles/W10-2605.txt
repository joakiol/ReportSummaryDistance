Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 31?36,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsUsing Domain Similarity for Performance EstimationVincent Van AschCLiPS - University of AntwerpAntwerp, BelgiumVincent.VanAsch@ua.ac.beWalter DaelemansCLiPS - University of AntwerpAntwerp, BelgiumWalter.Daelemans@ua.ac.beAbstractMany natural language processing (NLP)tools exhibit a decrease in performancewhen they are applied to data that is lin-guistically different from the corpus usedduring development.
This makes it hard todevelop NLP tools for domains for whichannotated corpora are not available.
Thispaper explores a number of metrics thatattempt to predict the cross-domain per-formance of an NLP tool through statis-tical inference.
We apply different sim-ilarity metrics to compare different do-mains and investigate the correlation be-tween similarity and accuracy loss of NLPtool.
We find that the correlation betweenthe performance of the tool and the sim-ilarity metric is linear and that the lattercan therefore be used to predict the perfor-mance of an NLP tool on out-of-domaindata.
The approach also provides a way toquantify the difference between domains.1 IntroductionDomain adaptation has recently turned into abroad field of study (Bellegarda, 2004).
Many re-searchers note that the linguistic variation betweentraining and testing corpora is an important fac-tor in assessing the performance of an NLP toolacross domains.
For example, a tool that has beendeveloped to extract predicate-argument structuresfrom abstracts of biomedical research papers, willexhibit a lower performance when applied to legaltexts.However, the notion of domain is mostly arbi-trarily used to refer to some kind of semantic area.There is unfortunately no unambiguous measureto assert a domain shift, except by observing theperformance loss of an NLP tool when appliedacross different domains.
This means that we typ-ically need annotated data to reveal a domain shift.In this paper we will show how unannotated datacan be used to get a clearer view on how datasetsdiffer.
This unsupervised way of looking at datawill give us a method to measure the difference be-tween data sets and allows us to predict the perfor-mance of an NLP tool on unseen, out-of-domaindata.In Section 2 we will explain our approach indetail.
In Section 3 we deal with a case studyinvolving basic part-of-speech taggers, applied todifferent domains.
An overview of related workcan be found in Section 4.
Finally, Section 5 con-cludes this paper and discusses options for furtherresearch.2 ApproachWhen developing an NLP tool using supervisedlearning, annotated data with the same linguisticproperties as the data for which the tool is devel-oped is needed, but not always available.
In manycases, this means that the developer needs to col-lect and annotate data suited for the task.
Whenthis is not possible, it would be useful to have amethod that can estimate the performance on cor-pus B of an NLP tool trained on corpus A in anunsupervised way, i.e., without the necessity to an-notate a part of B.In order to be able to predict in an unsupervisedway the performance of an NLP tool on differentcorpora, we need a way to measure the differencesbetween the corpora.
The metric at hand shouldbe independent from the annotation labels, so thatit can be easily applied on any given corpus.
Theaim is to find a metric such that the correlation be-tween the metric and the performance is statisti-cally significant.
In the scope of this article theconcept metric stands for any way of assigning asufficiently fine-grained label to a corpus, usingonly unannotated data.
This means that, in ourview, a metric can be an elaborate mixture of fre-quency counts, rules, syntactic pattern matching or31even machine learner driven tools.
However, in theremainder of this paper we will only look at fre-quency based similarity metrics since these met-rics are easily applicable and the experiments con-ducted using these metrics were already encourag-ing.3 Experimental design3.1 CorpusWe used data extracted from the British NationalCorpus (BNC) (2001) and consisting of writtenbooks and periodicals1.
The BNC annotators pro-vided 9 domain codes (i.e.
wridom), making itpossible to divide the text from books and peri-odicals into 9 subcorpora.
These annotated se-mantic domains are: imaginative (wridom1), nat-ural & pure science (wridom2), applied science(wridom3), social science (wridom4), world af-fairs (wridom5), commerce & finance (wridom6),arts (wridom7), belief & thought (wridom8), andleisure (wridom9).The extracted corpus contains sentences inwhich every token is tagged with a part-of-speechtag as defined by the BNC.
Since the BNC hasbeen tagged automatically, using the CLAWS4 au-tomatic tagger (Leech et al, 1994) and the Tem-plate Tagger (Pacey et al, 1997), the experimentsin this article are artificial in the sense that they donot learn real part-of-speech tags but rather part-of-speech tags as they are assigned by the auto-matic taggers.3.2 Similarity metricsTo measure the difference between two corporawe implemented six similarity metrics: Re?nyi2(Re?nyi, 1961), Variational (L1) (Lee, 2001),Euclidean (Lee, 2001), Cosine (Lee, 2001),Kullback-Leibler (Kullback and Leibler, 1951)and Bhattacharyya coefficient (Comaniciu et al,2003; Bhattacharyya, 1943).
We selected thesemeasures because they are well-described and pro-duce results for this task in an acceptable timespan.The metrics are computed using the relative fre-quencies of words.
For example, to calculate the1This is done by selecting texts with BNC category codesfor text type (i.e.
alltyp3 (written books and periodicals)) andfor medium (i.e.
wrimed1 (book), wrimed2 (periodical), andwrimed3 (miscellaneous: published)).2The Re?nyi divergence has a parameter ?
and Kullback-Leibler is a special case of the Re?nyi divergence, viz.
with?
= 1.Re?nyi divergence between corpus P and corpus Qthe following formula is applied:Re?nyi(P ;Q;?)
= 1(?
?1) log2(?k p1?
?k q?k)pk is the relative frequency of a token k in thefirst corpus P , and qk is the relative frequency oftoken k in the second corpus Q. ?
is a free param-eter and with ?
= 1 the Re?nyi divergence becomesequivalent to the Kullback-Leibler divergence.R?nyi 0.99EuclideanLESS SIMILARMORE SIMILARsocial-artsocial-beliefsocial-worldsocial-imaginativeart-socialMORE SIMILARLESS SIMILARsocial-artsocial-beliefsocial-worldsocial-imaginativeFigure 1: A visual comparison of two similaritymetrics: Re?nyi with ?
= 0.99 and Euclidean.Figure 1 gives an impression of the differencebetween two similarity metrics: Re?nyi (?
= 0.99)and Euclidean.
Only four domain combinationsare shown for the sake of clarity.
From the graphit can be observed that the social and imaginativedomains are the least similar in both cases.
Be-sides the different ordering, there is also a differ-ence in symmetry.
Contrary to the symmetric Eu-clidean metric, the Re?nyi scores differ, dependingon whether social constitutes the test set and artthe training set, or vice versa.
The dashed line onFigure 1 (left) is a reverse score, namely for art-social.
A divergence score may diverge a lot fromits reverse score.In practice, the best metric to choose is the met-ric that gives the best linear correlation betweenthe metric and the accuracy of an NLP tool appliedacross domains.
We tested 6 metrics: Re?nyi, Vari-ational (L1), Euclidean, Cosine, Kullback-Leibler,and the Bhattacharyya coefficient.
For Re?nyi, wetested four different ?-values: 0.95, 0.99, 1.05,and 1.1.
Most metrics gave a linear correlationbut for our experiments with data-driven POS tag-ging, the Re?nyi metric with ?
= 0.99 was the best32according to the Pearson product-moment corre-lation.
For majority this correlation was 0.91, forMbt 0.93, and for SVMTool 0.93.3.3 Part-of-speech taggingThe experiments carried out in the scope of thisarticle are all part-of-speech (POS) tagging tasks.There are 91 different POS labels in the BNC cor-pus which are combinations of 57 basic labels.
Weused three algorithms to assign part-of-speech la-bels to the words from the test corpus:Majority This algorithm assigns the POS labelthat occurs most frequently in the training set for agiven word, to the word in the test set.
If the worddid not occur in train, the overall most frequent tagwas used.Memory based POS tagger (Daelemans andvan den Bosch, 2005) A machine learner thatstores examples in memory (Mbt) and uses thekNN algorithm to assign POS labels.
The defaultsettings were used.SVMTool POS tagger (Gime?nez and Ma?rquez,2004) Support vectors machines in a sequentialsetup are used to assign the POS labels.
The de-fault settings were used.3.4 Results and analysisFigure 2 shows the outcome of 72 cross-validationexperiments on the data from the British NationalCorpus.
The graph for the majority baseline isshown in Figure 2a.
The results for the memorybased tagger are shown in Figure 2b and the graphfor SVMTool is displayed in Figure 2c.For every domain, the data is divided into fiveparts.
For all pairs of domains, each part fromthe training domain is paired with each part fromthe testing domain.
This results in a 25 cross-validation cross-domain experiment.
A data pointin Figure 2 is the average outcome of such a 25fold experiment.
The abscissa of a data pointis the Re?nyi similarity score between the train-ing and testing component of an experiment.
The?
parameter was set to 0.99.
We propose thatthe higher (less negative) the similarity score, themore similar training and testing data are.The ordinate is the accuracy of the POS taggingexperiment.
The dotted lines are the 95% predic-tion intervals for every data point.
These bound-aries are obtained by linear regression using allother data points.
The interpretation of the inter-vals is that any point, given all other data points25 20 15 10 5R?nyi divergence score with alpha=0.9974767880828486889092Majorityaccuracy (%)(72 data points)Majority accuracy prediction95% prediction interval(a) Majority POS tagger.25 20 15 10 5R?nyi divergence score with alpha=0.9986878889909192939495Mbtaccuracy (%)(72 data points)Mbt accuracy prediction95% prediction interval(b) Memory based POS tagger.25 20 15 10 5R?nyi divergence score with alpha=0.99888990919293949596SVMTool accuracy (%)(72 data points)SVMTool accuracy prediction95% prediction interval(c) SVMTool POS tagger.Figure 2: The varying accuracy of three POS tag-gers with varying distance between train and testcorpus of different domains.from the graph, can be predicted with 95% cer-tainty, to lie between the upper and lower intervalboundary at the similarity score of that point.
Theaverage difference between the lower and the up-per interval boundary is 4.36% for majority, 1.92%for Mbt and 1.59% for SVMTool.
This means that,33Majority Mbt SVMToolaverage accuracy 84.94 91.84 93.48standard deviation 2.50 1.30 1.07Table 1: Average accuracy and standard deviation on 72 cross-validation experiments.when taking the middle of the interval as the ex-pected accuracy, the maximum error is 0.8% forSVMTool.
Since the difference between the bestand worst accuracy score is 4.93%, using linear re-gression means that one can predict the accuracythree times better.
For Mbt with a range of 5.84%between best and worst accuracy and for majoritywith 12.7%, a similar figure is obtained.Table 1 shows the average accuracies of the al-gorithms for all 72 experiments.
For this article,the absolute accuracy of the algorithms is not un-der consideration.
Therefore, no effort has beenmade to improve on these accuracy scores.
Onecan see that the standard deviation for SVMTooland Mbt is lower than for majority, suggesting thatthese algorithms are less susceptible to domainvariation.The good linear fit for the graphs of Figure 2cannot be reproduced with every algorithm.
Foralgorithms that do not have a sufficiently strong re-lation between training corpus and assigned classlabel, the linear relation is lost.
Clearly, it remainsfeasible to compute an interval for the data points,but as a consequence of the non-linearity, the pre-dicted intervals would be similar or even biggerthan the difference between the lowest and highestaccuracy score.In Figure 3 the experiments of Figure 2 arereproduced using test and training sets from thesame domain.
Since we used the same data setsas for the out-of-domain experiments, we had tocarry out 20 fold cross-validation for these exper-iments.
Because of this different setup the resultsare shown in a different figure.
There is a datapoint for every domain.Although the average distance between test andtraining set are smaller for in-domain experiments,we still observe a linear relation for Mbt and SVM,for majority there is still a visual hint of linearity.For in-domain the biggest difference between testand train set is for the leisure domain (Re?nyi score:-6.0) which is very close to the smallest out-of-domain difference (-6.3 for social sciences?worldaffairs).
This could mean that the random varia-tion between test and train can approach the varia-6.5 6.0 5.5 5.0 4.5 4.0R?nyi divergence score with alpha=0.9985868788899091accuracy (%)(9 data points)Majority accuracy prediction95% prediction interval(a) Majority POS tagger.6.5 6.0 5.5 5.0 4.5 4.0R?nyi divergence score with alpha=0.9992.593.093.594.094.595.095.5accuracy (%)(9 data points)Mbt accuracy prediction95% prediction interval(b) Memory based POS tagger.6.5 6.0 5.5 5.0 4.5 4.0R?nyi divergence score with alpha=0.9993.594.094.595.095.596.096.5accuracy (%)(9 data points)SVMTool accuracy prediction95% prediction interval(c) SVMTool POS tagger.Figure 3: The varying accuracy of three POS tag-gers with varying distance between train and testcorpus of the same domain.34tion between domains but this observation is madein abstraction from the different data set sizes forin and out of domain experiments.
For majoritythe average accuracy over all domains is 88.25%(stdev: 0.87), for Mbt 94.07% (0.63), and forSVMTool 95.06% (0.59).
Which are, as expected,higher scores than the figures in Table 1.4 Related WorkIn articles dealing with the influence of domainshifts on the performance of an NLP tool, thein-domain data and out-of-domain data are takenfrom different corpora, e.g., sentences from moviesnippets, newspaper texts and personal weblogs(Andreevskaia and Bergler, 2008).
It can be ex-pected that these corpora are indeed dissimilarenough to consider them as separate domains, butno objective measure has been used to define themas such.
The fact that the NLP tool produceslower results for cross-domain experiments can betaken as an indication of the presence of sepa-rate domains.
A nice overview paper on statisti-cal domain adaptation can be found in Bellegarda(2004).A way to express the degree of relatedness,apart from this well-known accuracy drop, can befound in Daume?
and Marcu (2006).
They proposea domain adaptation framework containing a pa-rameter pi.
Low values of pi mean that in-domainand out-of-domain data differ significantly.
Theyalso used Kullback-Leibler divergence to computethe similarity between unigram language models.Blitzer et al (2007) propose a supervised wayof measuring the similarity between the two do-mains.
They compute the Huber loss, as a proxyof the A-distance (Kifer et al, 2004), for everyinstance that they labeled with their tool.
The re-sulting measure correlates with the adaptation lossthey observe when applying a sentiment classifi-cation tool on different domains.5 Conclusions and future workThis paper showed that it is possible to narrowdown the prediction of the accuracy of an NLPtool on an unannotated corpus by measuring thesimilarity between this unannotated corpus and thecorpus the tagger was trained on in an unsuper-vised way.
A prerequisite to be able to make a reli-able prediction, is to have sufficient annotated datato measure the correlation between the accuracyand a metric.
We observed that, in order to make aprediction interval that is narrower than the differ-ence between the lowest and highest accuracy onthe annotated corpora, the algorithm used, shouldcapture sufficient information from training.The observation that it is feasible to make re-liable predictions using unannotated data, can beof help when training a system for a task in a do-main for which no annotated data is available.
Asa first step, the metric resulting in the best linearfit between the metric and the accuracy should besearched.
If a linear relation can be established,one can take annotated training data from the do-main that is closest to the unannotated corpus andassume that this will give the best accuracy score.In this article we implemented a way to mea-sure the similarity between two corpora.
One maydecide to use such a metric to categorize the avail-able corpora for a given task into groups, depend-ing on their similarity.
It should be noted that inorder to do this, a symmetric metric should beused.
Indeed, an asymmetric metric like the Re?nyidivergence will give a different value dependingon whether the similarity between corpus P andcorpus Q is measured as Re?nyi(P ;Q;?)
or asRe?nyi(Q;P ;?
).Further research should explore the usability oflinear regression for other NLP tasks.
Althoughno specific adaptation to the POS tagging task wasmade, it may not be straightforward to find a lin-ear relation for more complicated tasks.
For suchtasks, it may be useful to insert n-grams into themetric.
Or, if a parser was first applied to the data,it is possible to insert syntactic features in the met-ric.
Of course, these adaptations may influencethe efficiency of the metric, but if a good linearrelation between the metric and the accuracy canbe found, the metric is useful.
Another option tomake the use of the metric less task dependent isby not using the distribution of the tokens but byusing distributions of the features used by the ma-chine learner.
Applying this more generic setup ofour experiments to other NLP tools may lead to thediscovery of a metric that is generally applicable.AcknowledgmentsThis research was made possible through finan-cial support from the University of Antwerp(BIOGRAPH GOA-project).35ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
TheTheory of Parsing, Translation and Compiling, vol-ume 1.
Prentice-Hall, Englewood Cliffs, NJ.Alina Andreevskaia and Sabine Bergler.
2008.
WhenSpecialists and Generalists Work Together: Over-coming Domain Dependence in Sentiment Tagging.Proceedings of the 46th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies (ACL-08:HLT), 290?298.
As-sociation for Computational Linguistics.
Columbus,Ohio, USA.Jerome R. Bellegarda.
2004.
Statistical languagemodel adaptation: review and perspectives.
SpeechCommunication, 42:93?108.Anil Bhattacharyya.
1943.
On a measure of divergencebetween two statistical populations defined by theirprobability distributions.
Bulletin of the CalcuttaMathematical Society, 35:99?109.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, Bollywood, Boom-boxes andBlenders: Domain Adaptation for Sentiment Clas-sification.
Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,440?447.
Association for Computational Linguis-tics.
Prague, Czech Republic.British National Corpus Consortium.
2001.
TheBritish National Corpus, version 2 (BNC World).Distributed by Oxford University ComputingServices on behalf of the BNC Consortium.http://www.natcorp.ox.ac.uk (Last accessed: April2, 2010).Dorin Comaniciu, Visvanathan Ramesh, and PeterMeer.
2003.
Kernel-Based Object Tracking.
IEEETransactions on Pattern Analysis and Machine In-telligence, 25(5):564?575.Walter Daelemans and Antal van den Bosch.
2005.Memory-Based Language Processing.
CambridgeUniversity Press, Cambridge, UK.Hal Daume?
III and Daniel Marcu.
2006.
DomainAdaptation for Statistical Classifiers.
Journal of Ar-tificial Intelligence Research, 26:101?126.T.
Mark Ellison and Simon Kirby.
2006.
MeasuringLanguage Divergence by Intra-Lexical Comparison.Proceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the ACL, 273?280.
Association for Computa-tional Linguistics.
Sidney, Australia.Jesu?s Gime?nez and Llu?
?s Ma?rquez.
2004.
SVMTool:A general POS tagger generator based on SupportVector Machines.
Proceedings of the 4th Interna-tional Conference on Language Resources and Eval-uation (LREC?04), 43?46.
European Language Re-sources Association.
Lisbon, Portugal.Daniel Kifer, Shai Ben-David, and Johannes Gehrke.2004.
Detecting change in data streams.
Proceed-ings of the 30th Very Large Data Bases Conference(VLDB?04), 180?191.
VLDB Endowment.
Toronto,Canada.Solomon Kullback and Richard.
A. Leibler.
1951.
OnInformation and Sufficiency.
The Annals of Mathe-matical Statistics, 22(1):79?86.Lillian Lee.
2001.
On the Effectivenessof the Skew Divergence for Statistical Lan-guage Analysis.
8th International Workshopon Artificial Intelligence and Statistics (AISTATS2001), 65?72.
Florida, USA.
Online reposi-tory http://www.gatsby.ucl.ac.uk/aistats/aistats2001(Last accessed: April 2, 2010).Geoffrey Leech, Roger Garside, and Michael Bryant.1994.
CLAWS4: The tagging of the British Na-tional Corpus.
Proceedings of the 15th InternationalConference on Computational Linguistics (COLING94), 622?628.
Kyoto, Japan.Michael Pacey, Steven Fligelstone, and Paul Rayson.1997.
How to generalize the task of annotation.Corpus Annotation: Linguistic Information fromComputer Text Corpora, 122?136.
London: Long-man.Alfre?d Re?nyi.
1961.
On measures of informationand entropy.
Proceedings of the 4th Berkeley Sym-posium on Mathematics, Statistics and Probability,1:547?561.
University of California Press.
Berke-ley, California, USA.36
