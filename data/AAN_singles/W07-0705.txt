Proceedings of the Second Workshop on Statistical Machine Translation, pages 33?39,Prague, June 2007. c?2007 Association for Computational LinguisticsCan We Translate Letters?David Vilar, Jan-T. Peter and Hermann NeyLehrstuhl fu?r Informatik 6RWTH Aachen UniversityD-52056 Aachen, Germany{vilar,peter,ney}@cs.rwth-aachen.deAbstractCurrent statistical machine translation sys-tems handle the translation process as thetransformation of a string of symbols intoanother string of symbols.
Normally thesymbols dealt with are the words in differ-ent languages, sometimes with some addi-tional information included, like morpho-logical data.
In this work we try to pushthe approach to the limit, working not on thelevel of words, but treating both the sourceand target sentences as a string of letters.We try to find out if a nearly unmodifiedstate-of-the-art translation system is able tocope with the problem and whether it is ca-pable to further generalize translation rules,for example at the level of word suffixes andtranslation of unseen words.
Experimentsare carried out for the translation of Catalanto Spanish.1 IntroductionMost current statistical machine translation systemshandle the translation process as a ?blind?
transfor-mation of a sequence of symbols, which representthe words in a source language, to another sequenceof symbols, which represent words in a target lan-guage.
This approach allows for a relative simplic-ity of the models, but also has drawbacks, as re-lated word forms, like different verb tenses or plural-singular word pairs, are treated as completely differ-ent entities.Some efforts have been made e.g.
to integratemore information about the words in the form of PartOf Speech tags (Popovic?
and Ney, 2005), using addi-tional information about stems and suffixes (Popovic?and Ney, 2004) or to reduce the morphological vari-ability of the words (de Gispert, 2006).
State of theart decoders provide the ability of handling differentword forms directly in what has been called factoredtranslation models (Shen et al, 2006).In this work, we try to go a step further and treatthe words (and thus whole sentences) as sequencesof letters, which have to be translated into a new se-quence of letters.
We try to find out if the trans-lation models can generalize and generate correctwords out of the stream of letters.
For this approachto work we need to translate between two relatedlanguages, in which a correspondence between thestructure of the words can be found.For this experiment we chose a Catalan-Spanishcorpus.
Catalan is a romance language spoken in thenorth-east of Spain and Andorra and is consideredby some authors as a transitional language betweenthe Iberian Romance languages (e.g.
Spanish) andGallo-Romance languages (e.g.
French).
A commonorigin and geographic proximity result in a similar-ity between Spanish and Catalan, albeit with enoughdifferences to be considered different languages.
Inparticular, the sentence structure is quite similar inboth languages and many times a nearly monotoni-cal word to word correspondence between sentencescan be found.
An example of Catalan and Spanishsentences is given in Figure 1.The structure of the paper is as follows: In Sec-tion 2 we review the statistical approach to machinetranslation and consider how the usual techniquescan be adapted to the letter translation task.
In Sec-33Catalan Perque` a mi m?agradaria estar-hi dues, una o dues setmanes, me?s o menys, depenent delpreu i cada hotel.Spanish Porque a m??
me gustar?
?a quedarme dos, una o dos semanas, ma?s o menos, dependiendo delprecio y cada hotel.English Because I would like to be there two, one or two weeks, more or less, depending on theprice of each hotel.Catalan Si baixa aqu??
tenim una guia de la ciutat que li podem facilitar en la que surt informacio?sobre els llocs me?s interessants de la ciutat.Spanish Si baja aqu??
tenemos una gu?
?a de la ciudad que le podemos facilitar en la que sale infor-macio?n sobre los sitios ma?s interesantes de la ciudad.English If you come down here we have a guide book of the city that you can use, in there isinformation about the most interesting places in the city.Figure 1: Example Spanish and Catalan sentences (the English translation is provided for clarity).tion 3 we present the results of the letter-based trans-lation and show how to use it for improving transla-tion quality.
Although the interest of this work ismore academical, in Section 4 we discuss possiblepractical applications for this approach.
The paperconcludes in Section 5.2 From Words To LettersIn the standard approach to statistical machine trans-lation we are given a sentence (sequence of words)fJ1 = f1 .
.
.
fJ in a source language which is to betranslated into a sentence e?I1 = e?1 .
.
.
e?I in a targetlanguage.
Bayes decision rule states that we shouldchoose the sentence which maximizes the posteriorprobabilitye?I1 = argmaxeI1p(eI1|fJ1 ) , (1)where the argmax operator denotes the search pro-cess.
In the original work (Brown et al, 1993) theposterior probability p(eI1|fJ1 ) is decomposed fol-lowing a noisy-channel approach, but current state-of-the-art systems model the translation probabil-ity directly using a log-linear model(Och and Ney,2002):p(eI1|fJ1 ) =exp(?Mm=1 ?mhm(eI1, fJ1 ))?e?I1exp(?Mm=1 ?mhm(e?I1, fJ1 )) ,(2)with hm different models, ?m scaling factors andthe denominator a normalization factor that can beignored in the maximization process.
The ?m areusually chosen by optimizing a performance mea-sure over a development corpus using a numericaloptimization algorithm like the downhill simplex al-gorithm (Press et al, 2002).The most widely used models in the log lin-ear combination are phrase-based models in source-to-target and target-to-source directions, ibm1-likescores computed at phrase level, also in source-to-target and target-to-source directions, a target lan-guage model and different penalties, like phrasepenalty and word penalty.This same approach can be directly adapted to theletter-based translation framework.
In this case weare given a sequence of letters FJ1 correspondingto a source (word) string fJ1 , which is to be trans-lated into a sequence of letters EI1 corresponding toa string eI1 in a target language.
Note that in this casewhitespaces are also part of the vocabulary and haveto be generated as any other letter.
It is also impor-tant to remark that, without any further restrictions,the word sequences eI1 corresponding to a generatedletter sequence EI1 are not even composed of actualwords.2.1 Details of the Letter-Based SystemThe vocabulary of the letter-based translation sys-tem is some orders of magnitude smaller than thevocabulary of a full word-based translation system,at least for European languages.
A typical vocabu-lary size for a letter-based system would be around70, considering upper- and lowercase letter, digits,34whitespace and punctuation marks, while the vocab-ulary size of a word-based system like the ones usedin current evaluation campaigns is in the range oftens or hundreds of thousands words.
In a normalsituation there are no unknowns when carrying outthe actual translation of a given test corpus.
The sit-uation can be very different if we consider languageslike Chinese or Japanese.This small vocabulary size allows us to deal witha larger context in the models used.
For the phrase-based models we extract all phrases that can be usedwhen translating a given test corpus, without anyrestriction on the length of the source or the tar-get part1.
For the language model we were able touse a high-order n-gram model.
In fact in our ex-periments a 16-gram letter-based language model isused, while state-of-the-art translation systems nor-mally use 3 or 4-grams (word-based).In order to better try to generate ?actual words?in the letter-based system, a new model was addedin the log-linear combination, namely the count ofwords generated that have been seen in the trainingcorpus, normalized with the length of the input sen-tence.
Note however that this models enters as an ad-ditional feature function in the model and it does notconstitute a restriction of the generalization capabil-ities the model can have in creating ?new words?.Somehow surprisingly, an additional word languagemodel did not help.While the vocabulary size is reduced, the averagesentence length increases, as we consider each let-ter to be a unit by itself.
This has a negative impactin the running time of the actual implementation ofthe algorithms, specially for the alignment process.In order to alleviate this, the alignment process wassplit into two passes.
In the first part, a word align-ment was computed (using the GIZA++ toolkit (Ochand Ney, 2003)).
Then the training sentences weresplit according to this alignment (in a similar way tothe standard phrase extraction algorithm), so that thelength of the source and target part is around thirtyletters.
Then, a letter-based alignment is computed.2.2 Efficiency IssuesSomewhat counter-intuitively, the reduced vocabu-lary size does not necessarily imply a reduced mem-1For the word-based system this is also the case.ory footprint, at least not without a dedicated pro-gram optimization.
As in a sensible implementa-tions of nearly all natural language processing tools,the words are mapped to integers and handled assuch.
A typical implementation of a phrase table isthen a prefix-tree, which is accessed through theseword indices.
In the case of the letter-based transla-tion, the phrases extracted are much larger than theword-based ones, in terms of elements.
Thus the to-tal size of the phrase table increases.The size of the search graph is also larger forthe letter-based system.
In most current systemsthe generation algorithm is a beam search algorithmwith a ?source synchronous?
search organization.As the length of the source sentence is dramaticallyincreased when considering letters instead of words,the total size of the search graph is also increased, asis the running time of the translation process.The memory usage for the letter system can ac-tually be optimized, in the sense that the letters canact as ?indices?
themselves for addressing the phrasetable and the auxiliary mapping structure is not nec-essary any more.
Furthermore the characters can bestored in only one byte, which provides a signifi-cant memory gain over the word based system wherenormally four bytes are used for storing the indices.These gains however are not expected to counteractthe other issues presented in this section.3 Experimental ResultsThe corpus used for our experiment was built in theframework of the LC-STAR project (Conejero et al,2003).
It consists of spontaneous dialogues in Span-ish, Catalan and English2 in the tourism and travel-ling domain.
The test corpus (and an additional de-velopment corpus for parameter optimization) wasrandomly extracted, the rest of the sentences wereused as training data.
Statistics for the corpus canbe seen in Table 1.
Details of the translation systemused can be found in (Mauser et al, 2006).The results of the word-based and letter-basedapproaches can be seen in Table 2 (rows with la-bel ?Full Corpus?).
The high BLEU scores (up tonearly 80%) denote that the quality of the trans-lation is quite good for both systems.
The word-2The English part of the corpus was not used in our experi-ments.35Spanish CatalanTraining Sentences 40 574Running Words 482 290 485 514Vocabulary 14 327 12 772Singletons 6 743 5 930Test Sentences 972Running Words 12 771 12 973OOVs [%] 1.4 1.3Table 1: Corpus Statisticsbased system outperforms the letter-based one, asexpected, but the letter-based system also achievesquite a good translation quality.
Example transla-tions for both systems can be found in Figure 2.
Itcan be observed that most of the words generatedby the letter based system are correct words, and inmany cases the ?false?
words that the system gen-erates are very close to actual words (e.g.
?elos?
in-stead of ?los?
in the second example of Figure 2).We also investigated the generalization capabili-ties of both systems under scarce training data con-ditions.
It was expected that the greater flexibilityof the letter-based system would provide an advan-tage of the approach when compared to the word-based approach.
We randomly selected subsets ofthe training corpus of different sizes ranging from1 000 sentences to 40 000 (i.e.
the full corpus) andcomputed the translation quality on the same testcorpus as before.
Contrary to our hopes, however,the difference in BLEU score between the word-based and the letter-based system remained fairlyconstant, as can be seen in Figure 3, and Table 2for representative training corpus sizes.Nevertheless, the second example in Figure 2 pro-vides an interesting insight into one of the possi-ble practical applications of this approach.
In theexample translation of the word-based system, theword ?centreamericans?
was not known to the sys-tem (and has been explicitly marked as unknown inFigure 2).
The letter-based system, however, wasable to correctly learn the translation from ?centre-?to ?centro-?
and that the ending ?-ans?
in Catalanis often translated as ?-anos?
in Spanish, and thusa correct translation has been found.
We thus choseto combine both systems, the word-based system do-ing most of the translation work, but using the letter-based system for the translation of unknown words.The results of this combined approach can be foundin Table 2 under the label ?Combined System?.
Thecombination of both approaches leads to a 0.5% in-crease in BLEU using the full corpus as training ma-terial.
This increase is not very big, but is it over aquite strong baseline and the percentage of out-of-vocabulary words in this corpus is around 1% of thetotal words (see Table 1).
When the corpus size isreduced, the gain in BLEU score becomes more im-portant, and for the small corpus size of 1 000 sen-tences the gain is 2.5% BLEU.
Table 2 and Figure 3show more details.4 Practical ApplicationsThe approach described in this paper is mainly ofacademical interest.
We have shown that letter-based translation is in principle possible betweensimilar languages, in our case between Catalan andSpanish, but can be applied to other closely relatedlanguage pairs like Spanish and Portuguese or Ger-man and Dutch.
The approach can be interesting forlanguages where very few parallel training data isavailable.The idea of translating unknown words in a letter-based fashion can also have applications to state-of-the-art translation systems.
Nowadays most auto-matic translation projects and evaluations deal withtranslation from Chinese or Arabic to English.
Forthese language pairs the translation of named en-tities poses an additional problem, as many timesthey were not previously seen in the training dataand they are actually one of the most informativewords in the texts.
The ?translation?
of these enti-ties is in most cases actually a (more or less pho-netic) transliteration, see for example (Al-Onaizanand Knight, 2002).
Using the proposed approach forthe translation of these words can provide a tighterintegration in the translation process and hopefullyincrease the translation performance, in the sameway as it helps for the case of the Catalan-Spanishtranslation for unseen words.Somewhat related to this problem, we can find anadditional application in the field of speech recog-nition.
The task of grapheme-to-phoneme conver-sion aims at increasing the vocabulary an ASR sys-tem can recognize, without the need for additional36BLEU WER PERWord-Based System Full Corpus 78.9 11.4 10.610k 74.0 13.9 13.21k 60.0 21.3 20.1Letter-Based System Full Corpus 72.9 14.7 13.510k 69.8 16.5 15.11k 55.8 24.3 22.8Combined System Full Corpus 79.4 11.2 10.410k 75.2 13.4 12.61k 62.5 20.2 19.0Table 2: Translation results for selected corpus sizes.
All measures are percentages.Source (Cat) Be?, en principi seria per a les vacances de Setmana Santa que so?n les segu?ents que tenimara, entrant a juliol.Word-Based Bueno, en principio ser?
?a para las vacaciones de Semana Santa que son las siguientes quetenemos ahora, entrando en julio.Letter-Based Bueno, en principio ser?
?a para las vacaciones de Semana Santa que son las siguientes quetenemos ahora, entrando bamos en julio .Reference Bueno, en principio ser?
?a para las vacaciones de Semana Santa que son las siguientes quetenemos ahora, entrando julio.Source (Cat) Jo li recomanaria per exemple que intente?s apropar-se a algun pa?
?s ve??
tambe?
com poden serels pa?
?sos centreamericans, una mica me?s al nord Panama?.Word-Based Yo le recomendar?
?a por ejemplo que intentase acercarse a algu?n pa?
?s vecino tambie?n comopueden ser los pa?
?ses UNKNOWN centreamericans, un poco ma?s al norte Panama?.Letter-Based Yo le recomendar?
?a por ejemplo que intentaseo acercarse a algu?n pa?
?s ve??
tambie?n comopueden ser elos pa?
?ses centroamericanos, un poco ma?s al norte Panama?.Combined Yo le recomendar?
?a por ejemplo que intentase acercarse a algu?n pa?
?s vecino tambie?n comopueden ser los pa?
?ses centroamericanos, un poco ma?s al norte Panama?.Reference Yo le recomendar?
?a por ejemplo que intentase acercarse a algu?n pa?
?s vecino tambie?n comopueden ser los pa?
?ses centroamericanos, un poco ma?s al norte Panama?.Figure 2: Example translations of the different approaches.
For the word-based system an unknown wordhas been explicitly marked.37505560657075800  5000  10000  15000  20000  25000  30000  35000  40000Word-BasedLetter-BasedCombinedFigure 3: Translation quality depending of the corpus size.acoustic data.
The problem can be formulated as atranslation from graphemes (?letters?)
to a sequenceof graphones (?pronunciations?
), see for example(Bisani and Ney, 2002).
The proposed letter-basedapproach can also be adapted to this task.Lastly, a combination of both, word-based andletter-based models, working in parallel and perhapstaking into account additional information like baseforms, can be helpful when translating from or intorich inflexional languages, like for example Spanish.5 ConclusionsWe have investigated the possibility of building aletter-based system for translation between relatedlanguages.
The performance of the approach is quiteacceptable, although, as expected, the quality of theword-based approach is superior.
The combinationof both techniques, however, allows the system totranslate words not seen in the training corpus andthus increase the translation quality.
The gain is spe-cially important when the training material is scarce.While the experiments carried out in this work aremore interesting from an academical point of view,several practical applications has been discussed andwill be the object of future work.AcknowledgementsThis work was partly funded by the DeutscheForschungsgemeinschaft (DFG) under the project?Statistische Textu?bersetzung?
(NE 572/5-3).ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
Machinetransliteration of names in arabic text.
In Proceed-ings of the ACL-02 workshop on Computational ap-proaches to semitic languages, pages 1?13, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Max Bisani and Hermann Ney.
2002.
Investigationson joint-multigram models for grapheme-to-phonemeconversion.
In Proceedings of the 7th InternationalConference on Spoken Language Processing, vol-ume 1, pages 105?108, Denver, CO, September.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-38mation.
Computational Linguistics, 19(2):263?311,June.D.
Conejero, J. Gimnez, V. Arranz, A. Bonafonte, N. Pas-cual, N. Castell, and A. Moreno.
2003.
Lexica andcorpora for speech-to-speech translation: A trilingualapproach.
In European Conf.
on Speech Commu-nication and Technology, pages 1593?1596, Geneva,Switzerland, September.Adria` de Gispert.
2006.
Introducing Linguistic Knowl-edge into Statistical Machine Translation.
Ph.D. the-sis, Universitat Polite`cnica de Catalunya, Barcelona,October.Arne Mauser, Richard Zens, Evgeny Matusov, Sas?aHasan, and Hermann Ney.
2006.
The RWTH Statisti-cal Machine Translation System for the IWSLT 2006Evaluation.
In Proc.
of the International Workshop onSpoken Language Translation, pages 103?110, Kyoto,Japan.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proc.
of the 40th AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 295?302, Philadelphia, PA, July.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51, March.Maja Popovic?
and Hermann Ney.
2004.
Towards theUse of Word Stems and Suffixes for Statistical Ma-chine Translation.
In 4th International Conference onLanguage Resources and Evaluation (LREC), pages1585?1588, Lisbon, Portugal, May.Maja Popovic?
and Hermann Ney.
2005.
ExploitingPhrasal Lexica and Additional Morpho-syntactic Lan-guage Resources for Statistical Machine Translationwith Scarce Training Data.
In 10th Annual Conferenceof the European Association for Machine Translation(EAMT), pages 212?218, Budapest, Hungary, May.William H. Press, Saul A. Teukolsky, William T. Vetter-ling, and Brian P. Flannery.
2002.
Numerical Recipesin C++.
Cambridge University Press, Cambridge,UK.Wade Shen, Richard Zens, Nicola Bertoldi, and MarcelloFederico.
2006.
The JHU Workshop 2006 IWSLTSystem.
In Proc.
of the International Workshop onSpoken Language Translation, pages 59?63, Kyoto,Japan.39
