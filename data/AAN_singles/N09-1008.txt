Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 65?73,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsImproved Reconstruction of Protolanguage Word FormsAlexandre Bouchard-Co?te??
Thomas L. Griffiths?
Dan Klein?
?Computer Science Division ?Department of PsychologyUniversity of California at BerkeleyBerkeley, CA 94720AbstractWe present an unsupervised approach to re-constructing ancient word forms.
The presentwork addresses three limitations of previouswork.
First, previous work focused on faith-fulness features, which model changes be-tween successive languages.
We add marked-ness features, which model well-formednesswithin each language.
Second, we introduceuniversal features, which support generaliza-tions across languages.
Finally, we increasethe number of languages to which these meth-ods can be applied by an order of magni-tude by using improved inference methods.Experiments on the reconstruction of Proto-Oceanic, Proto-Malayo-Javanic, and ClassicalLatin show substantial reductions in error rate,giving the best results to date.1 IntroductionA central problem in diachronic linguistics is the re-construction of ancient languages from their moderndescendants (Campbell, 1998).
Here, we considerthe problem of reconstructing phonological forms,given a known linguistic phylogeny and known cog-nate groups.
For example, Figure 1 (a) shows a col-lection of word forms in several Oceanic languages,all meaning to cry.
The ancestral form in this casehas been presumed to be /taNis/ in Blust (1993).
Weare interested in models which take as input manysuch word tuples, each representing a cognate group,along with a language tree, and induce word formsfor hidden ancestral languages.The traditional approach to this problem has beenthe comparative method, in which reconstructionsare done manually using assumptions about the rel-ative probability of different kinds of sound change(Hock, 1986).
There has been work attempting toautomate part (Durham and Rogers, 1969; Eastlack,1977; Lowe and Mazaudon, 1994; Covington, 1998;Kondrak, 2002) or all of the process (Oakes, 2000;Bouchard-Co?te?
et al, 2008).
However, previous au-tomated methods have been unable to leverage threeimportant ideas a linguist would employ.
We ad-dress these omissions here, resulting in a more pow-erful method for automatically reconstructing an-cient protolanguages.First, linguists triangulate reconstructions frommany languages, while past work has been lim-ited to small numbers of languages.
For example,Oakes (2000) used four languages to reconstructProto-Malayo-Javanic (PMJ) and Bouchard-Co?te?
etal.
(2008) used two languages to reconstruct Clas-sical Latin (La).
We revisit these small datasetsand show that our method significantly outperformsthese previous systems.
However, we also show thatour method can be applied to a much larger dataset (Greenhill et al, 2008), reconstructing Proto-Oceanic (POc) from 64 modern languages.
In ad-dition, performance improves with more languages,which was not the case for previous methods.Second, linguists exploit knowledge of phonolog-ical universals.
For example, small changes in vowelheight or consonant place are more likely than largechanges, and much more likely than change to ar-bitrarily different phonemes.
In a statistical system,one could imagine either manually encoding or auto-matically inferring such preferences.
We show thatboth strategies are effective.Finally, linguists consider not only how languageschange, but also how they are internally consistent.Past models described how sounds do (or, more of-ten, do not) change between nodes in the tree.
Toborrow broad terminology from the Optimality The-ory literature (Prince and Smolensky, 1993), suchmodels incorporated faithfulness features, captur-ing the ways in which successive forms remainedsimilar to one another.
However, each languagehas certain regular phonotactic patterns which con-65strain these changes.
We encode such patterns us-ing markedness features, characterizing the internalphonotactic structure of each language.
Faithfulnessand markedness play roles analogous to the channeland language models of a noisy-channel system.
Weshow that markedness features improve reconstruc-tion, and can be used efficiently.2 Related workOur focus in this section is on describing the prop-erties of the two previous systems for reconstruct-ing ancient word forms to which we compare ourmethod.
Citations for other related work, such assimilar approaches to using faithfulness and marked-ness features, appear in the body of the paper.In Oakes (2000), the word forms in a given pro-tolanguage are reconstructed using a Viterbi multi-alignment between a small number of its descendantlanguages.
The alignment is computed using hand-set parameters.
Deterministic rules characterizingchanges between pairs of observed languages are ex-tracted from the alignment when their frequency ishigher than a threshold, and a proto-phoneme inven-tory is built using linguistically motivated rules andparsimony.
A reconstruction of each observed wordis first proposed independently for each language.
Ifat least two reconstructions agree, a majority voteis taken, otherwise no reconstruction is proposed.This approach has several limitations.
First, it is nottractable for larger trees, since the time complexityof their multi-alignment algorithm grows exponen-tially in the number of languages.
Second, deter-ministic rules, while elegant in theory, are not robustto noise: even in experiments with only four daugh-ter languages, a large fraction of the words could notbe reconstructed.In Bouchard-Co?te?
et al (2008), a stochastic modelof sound change is used and reconstructions are in-ferred by performing probabilistic inference over anevolutionary tree expressing the relationships be-tween languages.
The model does not support gener-alizations across languages, and has no way to cap-ture phonotactic regularities within languages.
As aconsequence, the resulting method does not scale tolarge phylogenies.
The work we present here ad-dresses both of these issues, with a richer modeland faster inference allowing improved reconstruc-tion and increased scale.3 ModelWe start this section by introducing some notation.Let ?
be a tree of languages, such as the examplesin Figure 3 (c-e).
In such a tree, the modern lan-guages, whose word forms will be observed, are theleaves of ?
.
All internal nodes, particularly the root,are languages whose word forms are not observed.Let L denote all languages, modern and otherwise.All word forms are assumed to be strings ??
in theInternational Phonological Alphabet (IPA).1We assume that word forms evolve along thebranches of the tree ?
.
However, it is not the casethat each cognate set exists in each modern lan-guage.
Formally, we assume there to be a knownlist of C cognate sets.
For each c ?
{1, .
.
.
, C}let L(c) denote the subset of modern languages thathave a word form in the c-th cognate set.
For eachset c ?
{1, .
.
.
, C} and each language ` ?
L(c), wedenote the modern word form by wc`.
For cognateset c, only the minimal subtree ?
(c) containing L(c)and the root is relevant to the reconstruction infer-ence problem for that set.From a high-level perspective, the generative pro-cess is quite simple.
Let c be the index of the cur-rent cognate set, with topology ?(c).
First, a wordis generated for the root of ?
(c) using an (initiallyunknown) root language model (distribution overstrings).
The other nodes of the tree are drawn incre-mentally as follows: for each edge ` ?
`?
in ?
(c) usea branch-specific distribution over changes in stringsto generate the word at node `?.In the remainder of this section, we clarify the ex-act form of the conditional distributions over stringchanges, the distribution over strings at the root, andthe parameterization of this process.3.1 Markedness and FaithfulnessIn Optimality Theory (OT) (Prince and Smolensky,1993), two types of constraints influence the selec-tion of a realized output given an input form: faith-fulness andmarkedness constraints.
Faithfulness en-1The choice of a phonemic representation is motivated bythe fact that most of the data available comes in this form.
Dia-critics are available in a smaller number of languages and mayvary across dialects, so we discarded them in this work.66t aa?n giis####a?ng/angi//a?i//ta?i//angi//a?i//ta?i/?S?Ix1x2x3x7y1y2y3y7x4y4y5y6x5x6?ng1[Insert]1[Subst]1[(n g)@Kw]1[??g@Kw]1[?
?g]1[(n)@Kw]1[(g)@Kw]Language Word formProto Oceanic /taNis/Lau /aNi/Kwara?ae /angi/Taiof /taNis/Table 1: A cognate set from the Austronesian dataset.
Allword forms mean to cry.constrain these changes.
We encode such patternsusing markedness features, characterizing the inter-nal phonotactic structure of each language.
Faith-fulness and marked ess play roles analogous to thechannel and language models of a noisy-channelsystem.
We show that markedness features greatlyimprove reconstruction quality, and we show how towork with them efficiently.2 Related WorkOur focus in this section is on describing the prop-erties of the two previous systems for reconstruct-ing ancient word forms to which we compare ourmethod.
Citations for other related work, such assimilar approaches to using faithfulness and marked-ness features, appear in the body of the paper.In Oakes (2000), the word forms in a given proto-language are reconstructed using a Viterbi multi-alignment between a small number of its descendantlanguages.
The alignment is computed using hand-set parameters.
Deterministic rules characterizingchanges between pairs of observed languages are ex-tracted from the alignment when their frequency ishigher than a threshold, and a proto-phoneme inven-tory is built using linguistically motivated rules andparsimony.
A reconstruction of each observed wordis first proposed independently for each language.
Ifat least two reconstructions agree, a majority voteis taken, otherwise no reconstruction is proposed.This approach has several limitations.
First, it isnot tractable for larger trees since the complexity ofthe multi-alignment algorithm grows exponentiallyin the number of languages.
Second, determinis-tic rules, while elegant in theory, are not robust tonoise: even in experiments with only four daughterlanguages, a large fraction of the words could not bereconstructed.In Bouchard-Co?te?
et al (2008), a stochastic modelof sound change is used and reconstructions are in-ferred by performing probabilistic inference over anevolutionary tree expressing the relationships be-tween languages.
Use of approximate inference andstochastic rules addresses some of the limitations of(Oakes, 2000), but the resulting method is computa-tionally demanding and consequently does not scaleto large phylogenies.
The high computational costof probabilistic inference also limits the features thatcan be included in the model (omitting global fea-tures supporting generalizations across languages,and markedness features within languages).
Thework we present here addresses both of these issues,with faster inference and a richer model allowing in-creased scale and improved reconstruction.3 ModelWe start this section by introducing some notation.Let ?
be a tree of languages, such as the examples inFigure 4 (c-e).
In such a tree, the modern languages,whose word forms will be observed, are the leaves"1 .
.
.
"m. All internal nodes, particularly the root,are languages " whose word forms are not observed.Let L denote all languages, modern and otherwise.All word forms are assumed to be strings ??
in theInternational Phonological Alphabet (IPA).1As a first approximation, we assume that wordforms evolve along the branches of the tree ?
.
How-ever, it is not the case that each cognate set existsin each modern langugage.
Formally, we assumethere to be a known list of C cognate sets.
For eachc ?
{1, .
.
.
, C} let L(c) denote the subset of mod-ern languages that have a word form in the c-th cog-nate set.
For each set c ?
{1, .
.
.
, C} and each lan-guage " ?
L(c), we denote the modern word formby wc!.
For cognate set c, only the minimal subtree?
(c) containing L(c) and the root is relevant to thereconstruction inference problem for that set.From a high-level perspective, the generative pro-cess is quite simple.
Let c be the index of the cur-rent cognate set, with topology ?(c).
First, a wordis generated for the root of ?
(c) using an (initiallyunknown) root language model (distribution overstrings).
The other nodes of the tree are drawn in-crementally as follows: for each edge " ?
"?
in ?
(c)1The choice of a phonemic representation is motivated bythe fact that most of the data available comes in this form.
Dia-critics are available in a smaller number of languages and mayvary across dialects, so we discarted them in this work.
(a) (b)(f)(c)(d)(e)..?Figure 1: (a) A cognate set from the Austronesian dataset.All word orms mean to cry.
(b-d) The mutation modelused in this paper.
(b) The mutation of POc /taNis/ toKw.
/angi/.
(c) Graphical model depicting the dependen-cie among variables in one step of the mutation Markovchain.
(d) Active features for one step in this process.
(e-f) Comparison of two inference procedures on trees:Single sequence resampling (e) draws one sequence at atime, conditio ed on its parent and children, while ances-try resampling (f) draws an aligned slice from all wordssimultaneously.
In large trees, the latter is ore efficienthan the former.courages similarity between the input and outputwhile markedness favors well-formed output.Viewed from this perspective, previous comput -tional approaches to reconstruction are based almostxclusively n faithf lnes , ex r ssed thr ug a mu-tation model.
Only the words in the language at theroot of the tree, if any, are explicitly encouraged tobe w ll-formed.
In ontrast, we incorporate con-straints on markedness for each language with bothgeneral and branc -specific constraints on faithful-ness.
This is done using a lexicalized stochasticstring transducer (Varadarajan et al, 2008).We now make precise the conditional distribu-tions over pairs of evolving strings, referring to Fig-ure 1 (b-d).
Consider a language `?
evolving to `for cognate set c. Assume we have a word formx = wcl?
.
The generative process for producingy = wcl works as follows.
First, we considerx to be composed of characters x1x2 .
.
.
xn, withthe first and last being a special boundary symbolx1 = # ?
?
which is never deleted, mutated, orcreated.
The process generates y = y1y2 .
.
.
yn inn chunks yi ?
?
?, i ?
{1, .
.
.
, n}, one for each xi.The yi?s may be a single character, multiple charac-ters, or even empty.
In the example shown, all threeof these cases occur.T generat yi, we define a mutation Markovchain that incrementally adds zero or more charac-ters to an initially empty yi.
First, we decide whetherthe current phoneme in the top word t = xi will bedeleted, in which case yi =  as in the example of/s/ being deleted.
If t is not deleted, we chose a sin-gle substitution character in the bottom word.
Thisis the case both when /a/ is unchanged and when /N/substitutes to /n/.
We writeS = ??{?}
for this setof outcomes, where ?
is the special outcome indi-cating deletion.
Importantly, the probabilities of thismultinomial can depend on both the previous char-acter gen rated so far (i.e.
the rightmost characterp of yi?1) and the current character in the previousgeneration string (t).
As we will see shortly, this al-lows modelling markedness and faithfulness at everybranch, jointly.
This multinomial decision acts asthe initial distribution of the mutation Markov chain.We consider insertions only if a deletion was notselected in the first step.
Here, we draw from amultinomial overS , where this time the special out-come ?
corresponds to stopping insertions, and theother elements ofS correspond to symbols that areappende to yi.
In this case, the conditioning envi-ronment is t = xi and the current rightmost symbolp in yi.
Insertions continue until ?
is selected.
Inthe example, w follow the substitution of /N/ to /n/with an insertion of /g/, followed by a decision tostop that yi.
We will use ?S,t,p,` and ?I,t,p,` to denotethe probabilities ver the substitution and insertiondecisions in the current branch `?
?
`.A similar process generates the word at the root` of a tree, treating this word as a single stringy1 generated from a dummy ancestor t = x1.
Inthis case, only the insertion probabilities matter, andwe separately parameterize these probabilities with?R,t,p,`.
There is no actual dependence on t at theroot, but this formulation allows us to unify the pa-rameterization, with each ?
?,t,p,` ?
R|?|+1 where?
?
{R,S, I}.3.2 ParameterizationInstead of directly estimating the transition proba-bilities of the mutation Markov chain (as the param-eters of a collection of multinomial distributions) we67express them as the output of a log-linear model.
Weused the following feature templates:OPERATION identifies whether an operation in themutation Markov chain is an insertion, a deletion,a substitution, a self-substitution (i.e.
of the formx ?
y, x = y), or the end of an insertion event.Examples in Figure 1 (d): 1[Subst] and 1[Insert].MARKEDNESS consists of language-specific n-gram indicator functions for all symbols in ?.
Onlyunigram and bigram features are used for computa-tional reasons, but we show in Section 5 that thisalready captures important constraints.
Examples inFigure 1 (d): the bigram indicator 1[(n g)@Kw] (Kwstands for Kwara?ae, a language of the SolomonIslands), the unigram indicators 1[(n)@Kw] and1[(g)@Kw].FAITHFULNESS consists of indicators for muta-tion events of the form 1[x ?
y], where x ?
?,y ?
S .
Examples: 1[N ?
n], 1[N ?
n@Kw].Feature templates similar to these can be foundfor instance in Dreyer et al (2008) and Chen (2003),in the context of string-to-string transduction.
Notealso the connection with stochastic OT (Goldwaterand Johnson, 2003; Wilson, 2006), where a log-linear model mediates markedness and faithfulnessof the production of an output form from an under-lying input form.3.3 Parameter sharingData sparsity is a significant challenge in protolan-guage reconstruction.
While the experiments wepresent here use an order of magnitude more lan-guages than previous computational approaches, theincrease in observed data also brings with it addi-tional unknowns in the form of intermediate pro-tolanguages.
Since there is one set of parametersfor each language, adding more data is not sufficientfor increasing the quality of the reconstruction: weshow in Section 5.2 that adding extra languages canactually hurt reconstruction using previous methods.It is therefore important to share parameters acrossdifferent branches in the tree in order to benefit fromhaving observations from more languages.As an example of useful parameter sharing, con-sider the faithfulness features 1[/p/ ?
/b/] and1[/p/ ?
/r/], which are indicator functions for theappearance of two substitutions for /p/.
We wouldlike the model to learn that the former event (a sim-ple voicing change) should be preferred over the lat-ter.
In Bouchard-Co?te?
et al (2008), this has to belearned for each branch in the tree.
The difficulty isthat not all branches will have enough informationto learn this preference, meaning that we need to de-fine the model in such a way that it can generalizeacross languages.We used the following technique to address thisproblem: we augment the sufficient statistics ofBouchard-Co?te?
et al (2008) to include the currentlanguage (or language at the bottom of the currentbranch) and use a single, global weight vector in-stead of a set of branch-specific weights.
Gener-alization across branches is then achieved by usingfeatures that ignore `, while branch-specific featuresdepend on `.For instance, in Figure 1 (d), 1[N ?
n] isan example of a universal (global) feature sharedacross all branches while 1[N ?
n@Kw] is branch-specific.
Similarly, all of the features in OPERA-TION, MARKEDNESS and FAITHFULNESS have uni-versal and branch-specific versions.3.4 Objective functionConcretely, the transition probabilities of the muta-tion and root generation are given by:??,t,p,`(?)
= exp{?
?, f(?, t, p, `, ?)?
}Z(?, t, p, `, ?)
?
?
(?, t, ?
),where ?
?
S , f : {S, I,R}????
?L?S ?
Rkis the sufficient statistics or feature function, ?
?, ?
?denotes inner product and ?
?
Rk is a weight vector.Here, k is the dimensionality of the feature space ofthe log-linear model.
In the terminology of exponen-tial families, Z and ?
are the normalization functionand reference measure respectively:Z(?, t, p, `, ?)
= ????Sexp{?
?, f(?, t, p, `, ??)?}?
(?, t, ?)
=??????
?0 if ?
= S, t = #, ?
6= #0 if ?
= R, ?
= ?0 if ?
6= R, ?
= #1 o.w.Here, ?
is used to handle boundary conditions.We will also need the following notation: letP?(?),P?(?|?)
denote the root and branch probabil-ity models described in Section 3.1 (with transitionprobabilities given by the above log-linear model),I(c), the set of internal (non-leaf) nodes in ?
(c),pa(`), the parent of language `, r(c), the root of ?
(c)68and W (c) = (??)|I(c)|.
We can summarize our ob-jective function as follows:CXc=1logX~w?W (c)P?(wc,r(c))Y`?I(c)P?
(wc,`|wc,pa(`)) ?
||?||222?2The second term is a standard L2 regularizationpenalty (we used ?2 = 1).4 Learning algorithmLearning is done using a Monte Carlo variant of theExpectation-Maximization (EM) algorithm (Demp-ster et al, 1977).
The M step is convex and com-puted using L-BFGS (Liu et al, 1989); but the Estep is intractable (Lunter et al, 2003), so we useda Markov chain Monte Carlo (MCMC) approxima-tion (Tierney, 1994).
At E step t = 1, 2, .
.
.
, wesimulated the chain for O(t) iterations; this regimeis necessary for convergence (Jank, 2005).In the E step, the inference problem is to com-pute an expectation under the posterior over stringsin a protolanguage given observed word forms at theleaves of the tree.
The typical approach in biologyor historical linguistics (Holmes and Bruno, 2001;Bouchard-Co?te?
et al, 2008) is to use Gibbs sam-pling, where the entire string at a single node in thetree is sampled, conditioned on its parent and chil-dren.
This sampling domain is shown in Figure 1 (e),where the middle word is completely resampled butadjacent words are fixed.
We will call this methodSingle Sequence Resampling (SSR).
While concep-tually simple, this approach suffers from problemsin large trees (Holmes and Bruno, 2001).
Con-sequently, we use a different MCMC procedure,called Ancestry Resampling (AR) that alleviatesthe mixing problems (Figure 1 (f)).
This methodwas originally introduced for biological applications(Bouchard-Co?te?
et al, 2009), but commonalities be-tween the biological and linguistic cases make itpossible to use it in our model.Concretely, the problem with SSR arises when thetree under consideration is large or unbalanced.
Inthis case, it can take a long time for informationfrom the observed languages to propagate to the rootof the tree.
Indeed, samples at the root will ini-tially be independent of the observations.
AR ad-dresses this problem by resampling one thin verticalslice of all sequences at a time, called an ancestry.For the precise definition, see Bouchard-Co?te?
et al(2009).
Slices condition on observed data, avoidingthe problems mentioned above, and can propagateinformation rapidly across the tree.5 ExperimentsWe performed a comprehensive set of experimentsto test the new method for reconstruction outlinedabove.
In Section 5.1, we analyze in isolation theeffects of varying the set of features, the number ofobserved languages, the topology, and the numberof iterations of EM.
In Section 5.2 we compare per-formance to an oracle and to three other systems.Evaluation of all methods was done by computingthe Levenshtein distance (Levenshtein, 1966) be-tween the reconstruction produced by each methodand the reconstruction produced by linguists.
Weaveraged this distance across reconstructed words toreport a single number for each method.
We showin Table 2 the average word length in each corpus;note that the Latin average is much larger, givingan explanation to the higher errors in the Romancedataset.
The statistical significance of all perfor-mance differences are assessed using a paired t-testwith significance level of 0.05.5.1 Evaluating system performanceWe used the Austronesian Basic VocabularyDatabase (Greenhill et al, 2008) as the basis fora series of experiments used to evaluate the per-formance of our system and the factors relevant toits success.
The database includes partial cognacyjudgments and IPA transcriptions, as well as a fewreconstructed protolanguages.
A reconstruction ofProto-Oceanic (POc) originally developed by Blust(1993) using the comparative method was the basisfor evaluation.We used the cognate information provided inthe database, automatically constructing a globaltree2 and set of subtrees from the cognate set in-dicator matrix M(`, c) = 1[` ?
L(c)], c ?
{1, .
.
.
, C}, ` ?
L. For constructing the global tree,we used the implementation of neighbor joining inthe Phylip package (Felsenstein, 1989).
We useda distance based on cognates overlap, dc(`1, `2) =?Cc=1 M(`1, c)M(`2, c).
We bootstrapped 10002The dataset included a tree, but it was out of date as ofNovember 2008 (Greenhill et al, 2008).69NggelaBugotuTapeAvavaNeveeiNamanNeseSantaAnaNahavaqNatiKwaraaeSolLauKwameraToloMarshallesPuloAnnaChuukeseAKSaipanCaroPuluwateseWoleaianPuloAnnanCarolinianWoleaiChuukeseNaunaPaameseSouAnutaVaeakauTauTakuuTokelauTonganSamoanIfiraMeleMTikopiaTuvaluNiueFutunaEastUveaEastRennelleseEmaeKapingamarSikaianaNukuoroLuangiuaHawaiianMarquesanTahitianthRurutuanMaoriTuamotuMangarevaRarotonganPenrhynRapanuiEasPukapukaMwotlapMotaFijianBauNamakirNgunaArakiSouthSaaRagaPeteraraMaItEsPtSndJvMadMal POc LaPMJFigure 3: Phylogenetic trees for three language families.Clockwise, from the top left: Romance, Austronesian andProto-Malayo-Javanic.formance of our system and the factors relevant toits success.
The database contained, as of Novem-ber 2008, 124,468 lexical items from 587 languagesmostly from the Austronesian language family.
Thedatabase includes partial cognacy judgments andIPA transcriptions, as well as a few reconstructedproto-languages.
A reconstruction of Proto Oceanic(POc) originally developed by (Blust, 1993) usingthe comparative method was the basis for evaluation.We used the cognate information provided in thedatabase, automatically constructing a global tree2and set of subtrees from the cognate set indicatormatrix M(!, c) = 1[!
?
L(c)], c ?
{1, .
.
.
, C}, !
?L.
For constructing the global tree, we used theimplementation of neighbor joining in the Phylippackage (Felsenstein, 1989).
The distance ma-trix used the Hamming distance of cognate indi-cators, dc(!1, !2) = ?Cc=1 M(!1, c)M(!2, c).
Webootstrapped 1000 samples and formed an accurate(90%) consensus tree.
The tree obtained is not bi-nary, but the AR inference algorithm scales linearlyin the branching factor of the tree (in contrast, SSRscales exponentially (Lunter et al, 2003)).The first claim we verified experimentally is thathaving more observed languages aids reconstructionof proto-languages.
To test this hypothesis we addedobserved modern languages in increasing order ofdistance dc to the target reconstruction of POc sothat the languages that are most useful for POc re-construction are added first.
This prevents the ef-fects of adding a close language after several distant2The dataset included a tree, but as of November 2008, itwas generated automatically and ?has [not] been updated in awhile.
?0 10 20 30 40 50 60 701.41.61.822.22.42.6Number of modern languagesErrorFigure 4: Mean distance to the target reconstruction ofproto Oceanic as a function of the number of modern lan-guages used by the inference procedure.ones being confused with an improvement producedby increasing the number of languages.The results are reported in Figure 4.
They con-firm that large-scale inference is desirable for auto-matic proto-language reconstruction: going from 2-to-4, 4-to-8, 8-to-16, 16-to-32 languages all signifi-cantly helped reconstruction.
There was still an av-erage edit distance improvement of 0.05 from 32 to64 languages, altough this was not statistically sig-nificant.We then conducted a number of experiments in-tended to assess the robustness of the system, and toidentify the contribution made by different factors itincorporates.
First, we ran the system with 20 dif-ferent random seeds and assessed the stability of thesolution found.
In each cases, learning was stableand helded performances.
See Figure 5.Next, we found that all of the following ablationssignificantly hurts reconstruction: using a flat treein which all languages are equidistant from the re-constructed root and from each other instead of theconsensus tree, dropping the markedness features,disabling sharing across branches and dropping thefaithfulness features.
The results of these experi-ments are shown in Table 2.For comparison, we also included in the sametable the performance of a semi-supervised systemtrained by K-fold validation.
The system was ranK time, with disjoint 1 ?
K?1 of the POc.
wordsgiven to the system (as observations in the graph-Condition Edit dist.Unsupervised full system 1.87-FAITHFULNESS 2.02-MARKEDNESS 2.18-Sharing 1.99-Topology 2.06Semi-supervised system 1.75Table 2: Effects of ablation of various aspects of ourunsupervised system on mean edit distance to protoOceanic.
-Sharing corresponds to the subset of the fea-tures in OPERATION, FAITHFULNESS and MARKEDNESSthat condition on the current language, -Topolo y corre-sponds to using a flat topology where the only edges inthe tree connect modern languages to proto Oc anic.
Thesemi-supervised system i described in text.
All dif-ferences (compared to the unsupervised full system) arestatistically significant.ical model) fo each run.
It is semi-supervised inthe sense that gold reconstruction for many internalnodes are not avail bl (such as th common ances-tor of Kw.
nd Lau in Fi re 6).3Figure 6 shows the results of a concrete run over32 languages, zooming in to a pair of the Solomoniclanguages and the cognate set from Table 1.
In theexample shown, the reconstruction is as good as theoracle, though off by one character (the final /s/ isnot resent in any of the 32 inputs and thereforeis not reconstructed).
The diagrams show, for boththe global and the local features, the expectationsof each substitution superimposed on an IPA soundch rt, as well as a list of the top changes.
Darkerlines indicate higher counts.
T is run did not usen tural class constraints, but it can be seen that lin-guistically plausibl substitutions are learned.
Theglobal features prefer a range of voic ng changes,manner changes, adjace t vowel motion, and so on,including mu ations like /s/ to /h/ which are commonbut poorly repre ented in a naive attribute-based nat-ural class scheme.
On the other hand, the features l -cal to the lang ag Kwara?a (Kw.)
pick out the sub-set of these change which are active in that branch,such as /s/?/t/ fortition.3We also tried a fully supervised system where a flat topol-ogy is used so that all of these latent internal nodes are avoided;but it did not perform as well.0 2 4 6 8 10 12 14 16 18 201.822.22.42.62.833.23.43.6EM IterationErrorFigure 5: Mean distance to the target reconstruction ofPOc as a function of the EM iteration.5.2 Comparisons against other methodsThe first two competing methods, PRAGUE andBCLKG, are described in Oakes (2000) andBouchard-Co?te?
et al (2008) respectively and sum-marized them in Section 1.
Neither approach scaleswell to large datasets.
In the first case, the bottleneckis the complexity of computing multi-alignmentswithout guide trees and the vanishing probabilitythat independent reconstructions agree.
In the sec-ond case, the problem comes from slow mixing ofthe inference algorithm and the unregularized pro-liferation of parameters.
For this reason, we built athird baseline that scales well in large datasets.This third baseline, CENTROID, computes thecentroid of the observed word forms in Leven-shtein distance.
Let L(x, y) denote the Lev-enshtein distance between word forms x andy.
Ideally, we would like the baseline toreturn argminx???
?y?O L(x, y), where O ={y1, .
.
.
, y|O|} is the set of observed word forms.Note that the optimum is not changed if we restrictthe minimization to be taken on x ?
?(O)?
suchthat m ?
|x| ?
M where m = mini |yi|,M =maxi |yi| and?
(O) is the set of characters occurringin O.
Even with this restriction, this optimizationis intractable.
As an approximation, we consideredonly strings built by at most k contiguous substringstaken from the word forms in O.
If k = 1, then itis equivalent to taking the min over x ?
O.
At theother end of the spectrum, if k = M , it is exact.This scheme is exponential in k, but since words arerelatively short, we found that k = 2 often finds theErrorN.
of m d rn lang.
EM iteration100 20300 601.41.82.22.61.82.433.6Figure 2: Left: Mean distance to the target reconstructionof POc as a function of the number of modern languagesused by the inference procedure.
Right: Mean distanceand confidenc intervals as a function of th EM it ration,averag d over 20 random seeds an ran on 4 languages.samples nd forme a accurate (90%) consen ustre .
The tree obtained is o binary, but the ARinfer ce algorithm scales lin arly in the branchingfactor of the tree (in contrast, SSR scale exp nen-tially (Lunter et al, 2003)).T e first laim we ver fied experimentally is thathaving more observed languages aids reconstructionof protolanguages.
To t t this hypothesis we addedobserved mod rn l nguage in increasing order ofdistance dc to the target reconstruction of POc sothat the languages that are most useful for POc re-construction are added first.
This prevents the ef-fects of adding a close language after several distantones being confused with an improvement producedby increasing the number of languages.The results are reported in Figure 2 (a).
They con-firm that large-scale inference is desirable for au-tomatic protolanguage reconstruction: reconstruc-tion improved statistically significantly with each in-crease except from 32 to 64 languages, where theaverage edit distance improvement was 0.05.We then conducted a number of experiments in-tended to assess the robustness of the system, and toidentify the contribution made by different factors itincorporates.
First, we ran the system with 20 dif-ferent random seeds to assess the stability of the so-lutions found.
In each case, learning was stable andaccuracy improved during training.
See Figure 2 (b).Next, we found that all of the following ablationssignificantly hurt reconstruction: using a flat tree (inwhich all languages are equidistant from the recon-structed root and from each other) instead of the con-sensus tree, dropping the markedness features, drop-Condition Edit dist.Unsupervised full system 1.87-FAITHFULNESS 2.02-MARKEDNESS 2.18-Sharing 1.99-Topology 2.06Semi-supervised system 1.75Table 1: Effects of ablation of various aspects of ourunsupervised system on mean edit distance to POc.-Sharing corresponds to the restriction to the subset of thefeatures in OPERATION, FAITHFULNESS and MARKED-NESS that are branch-specific, -Topology corresponds tousing a flat topology where the only edges in the tree con-nect modern languages to POc.
The semi-supervised sys-tem is described in the text.
All differences (compared tothe unsupervised full system) are statistically significant.ping the faithfulness features, and disabling sharingacross branches.
The results of these experimentsare shown in Table 1.For comparison, we also included in the sametable the performance of a semi-supervised systemtrained by K-fold validation.
The system was ranK = 5 times, with 1?K?1 of the POc words givento the system as observations in the graphical modelfor each run.
It is semi-supervised in the sense thatgold reconstruction for many internal nodes are notavailable in the dataset (for example the common an-cestor of Kwara?ae (Kw.)
and Lau in Figure 3 (b)),so they are still not filled.3Figure 3 (b) shows the results of a concrete runover 32 languages, zooming in to a pair of theSolomonic languages and the cognate set from Fig-ure 1 (a).
In the example shown, the reconstruc-tion is as good as the ORACLE (described in Sec-tion 5.2), though off by one character (the final /s/is not present in any of the 32 inputs and thereforeis not reconstructed).
In (a), diagrams show, forboth the global and the local (Kwara?ae) features,the expectations of each substitution superimposedon an IPA sound chart, as well as a list of the topchanges.
Darker lines indicate higher counts.
Thisrun did not use natural class constraints, but it can3We also tried a fully supervised system where a flat topol-ogy is used so that all of these latent internal nodes are avoided;but it did not perform as well?this is consistent with the-Topology experiment of Table 1.70be seen that linguistically plausible substitutions arelearned.
The global features prefer a range of voic-ing changes, manner changes, adjacent vowel mo-tion, and so on, including mutations like /s/ to /h/which are common but poorly represented in a naiveattribute-based natural class scheme.
On the otherhand, the features local to the language Kwara?aepick out the subset of these changes which are ac-tive in that branch, such as /s/?/t/ fortition.5.2 Comparisons against other methodsThe first two competing methods, PRAGUE andBCLKG, are described in Oakes (2000) andBouchard-Co?te?
et al (2008) respectively and sum-marized in Section 1.
Neither approach scales wellto large datasets.
In the first case, the bottleneck isthe complexity of computing multi-alignments with-out guide trees and the vanishing probability that in-dependent reconstructions agree.
In the second case,the problem comes from the unregularized prolifera-tion of parameters and slow mixing of the inferencealgorithm.
For this reason, we built a third baselinethat scales well in large datasets.This third baseline, CENTROID, computes thecentroid of the observed word forms in Leven-shtein distance.
Let L(x, y) denote the Lev-enshtein distance between word forms x andy.
Ideally, we would like the baseline toreturn argminx???
?y?O L(x, y), where O ={y1, .
.
.
, y|O|} is the set of observed word forms.Note that the optimum is not changed if we restrictthe minimization to be taken on x ?
?(O)?
suchthat m ?
|x| ?
M where m = mini |yi|,M =maxi |yi| and ?
(O) is the set of characters occurringin O.
Even with this restriction, this optimizationis intractable.
As an approximation, we consideredonly strings built by at most k contiguous substringstaken from the word forms in O.
If k = 1, then itis equivalent to taking the min over x ?
O.
At theother end of the spectrum, if k = M , it is exact.This scheme is exponential in k, but since words arerelatively short, we found that k = 2 often finds thesame solution as higher values of k. The differencewas in all the cases not statistically significant, so wereport the approximation k = 2 in what follows.We also compared against an oracle, denoted OR-ACLE, which returns argminy?OL(y, x?
), where x?is the target reconstruction.
We will denote it by OR-Comparison CENTROID PRAGUE BCLKGProtolanguage POc PMJ LaHeldout (prop.)
243 (1.0) 79 (1.0) 293 (0.5)Modern languages 70 4 2Cognate sets 1321 179 583Observed words 10783 470 1463Mean word length 4.5 5.0 7.4Table 2: Experimental setup: number of held-out proto-word from (absolute and relative), of modern languages,cognate sets and total observed words.
The split forBCLKG is the same as in Bouchard-Co?te?
et al (2008).ACLE.
This is superior to picking a single closestlanguage to be used for all word forms, but it is pos-sible for systems to perform better than the oraclesince it has to return one of the observed word forms.We performed the comparison against Oakes(2000) and Bouchard-Co?te?
et al (2008) on the samedataset and experimental conditions as those used inthe respective papers (see Table 2).
Note that thesetup of Bouchard-Co?te?
et al (2008) provides super-vision (half of the Latin word forms are provided);all of the other comparisons are performed in a com-pletely unsupervised manner.The PMJ dataset was compiled by Nothofer(1975), who also reconstructed the correspondingprotolanguage.
Since PRAGUE is not guaranteed toreturn a reconstruction for each cognate set, only 55word forms could be directly compared to our sys-tem.
We restricted comparison to this subset of thedata.
This favors PRAGUE since the system only pro-poses a reconstruction when it is certain.
Still, oursystem outperformed PRAGUE, with an average dis-tance of 1.60 compared to 2.02 for PRAGUE.
Thedifference is marginally significant, p = 0.06, partlydue to the small number of word forms involved.We also exceeded the performance of BCLKG onthe Romance dataset.
Our system?s reconstructionhad an edit distance of 3.02 to the truth against 3.10for BCLKG.
However, this difference was not signifi-cant (p = 0.15).
We think this is because of the highlevel of noise in the data (the Romance dataset is theonly dataset we consider that was automatically con-structed rather than curated by linguists).
A secondfactor contributing to this small difference may bethat the the experimental setup of BCLKG used veryfew languages, while the performance of our systemimproves markedly with more languages.71NggelaBugotuTapeAvavaNeveeiNamanNeseSantaAnaNahavaqNatiKwaraaeSolLauKwameraToloMarshallesPuloAnnaChuukeseAKSaipanCaroPuluwateseWoleaianPuloAnnanCarolinianWoleaiChuukeseNaunaPaameseSouAnutaVaeakauTauTakuuTokelauTonganSamoanIfiraMeleMTikopiaTuvaluNiueFutunaEastUveaEastRennelleseEmaeKapingamarSikaianaNukuoroLuangiuaHawaiianMarquesanTahitianthRurutuanMaoriTuamotuMangarevaRarotonganPenrhynRapanuiEasPukapukaMwotlapMotaFijianBauNamakirNgunaArakiSouthSaaRagaPeteraraMaItEsPtSndJvMadMal POc LaPMJ/a?i/ (Lau)/angi/ (Kw.
)/a?i//ta?i/ (POc)....Universala ??
el ??
rs ??
hk ??
gr ??
lKwaN ??
ng ??
ks ??
te ??
io ??
a1Universala ??
el ??
rs ??
hk ??
gr ??
lKwaN ??
ng ??
ks ??
te io a1?
????????
?
??????????
??
?????????
?fgdb cn?mjkhvtsrqp ?z??
x?
????????
?
??????????
??
?????????
?fgdb cn?mjkhvtsrqp ?z??
x(a) (b) (c) (d) (e)Figure 3: (a) A visualization of two learned faithfulness parameters: on the top, from the universal features, onthe bottom, for one particular branch.
Each pair of phonemes have a link with grayscale value proportional to theexpectation of a transition between them.
The five strongest links are also included at the right.
(b) A sample takenfrom our POc experiments (see text).
(c-e) Phylogenetic trees for three language families: Proto-Malayo-Javanic,Austronesian and Romance.We conducted another experiment to verify thisby running both systems in larger trees.
Because theRomance dataset had only three modern languagestranscribed in IPA, we used the Austronesian datasetto perform the test.
The results were all significant inthis setup: while our method went from an edit dis-tance of 2.01 to 1.79 in the 4-to-8 languages exper-iment described in Section 5.1, BCLKG went from3.30 to 3.38.
This suggests that more languages canactually hurt systems that do not support parametersharing.Since we have shown evidence that PRAGUE andBCLKG do not scale well to large datasets, wealso compared against ORACLE and CENTROID in alarge-scale setting.
Specifically, we compare to theexperimental setup on 64 modern languages used toreconstruct POc described before.
Encouragingly,while the system?s average distance (1.49) does notattain that of the ORACLE (1.13), we significantlyoutperform the CENTROID baseline (1.79).5.3 Incorporating prior linguistic knowledgeThe model also supports the addition of prior lin-guistic knowledge.
This takes the form of featuretemplates with more internal structure.
We per-formed experiments with an additional feature tem-plate:STRUCT-FAITHFULNESS is a structured version ofFAITHFULNESS, replacing x and y with their natu-ral classes N?
(x) and N?
(y) where ?
indexes typesof classes, ranging over {manner, place, phonation,isOral, isCentral, height, backness, roundedness}.This feature set is reminiscent of the featurized rep-resentation of Kondrak (2000).We compared the performance of the system withand without STRUCT-FAITHFULNESS to check if thealgorithm can recover the structure of natural classesin an unsupervised fashion.
We found that with2 or 4 observed languages, FAITHFULNESS under-performed STRUCT-FAITHFULNESS, but for largertrees, the difference was not significant.
FAITH-FULNESS even slightly outperformed its structuredcousin with 16 observed languages.6 ConclusionBy enriching our model to include important fea-tures like markedness, and by scaling up to muchlarger data sets than were previously possible, weobtained substantial improvements in reconstruc-tion quality, giving the best results on past datasets.
While many more complex phenomena arestill unmodeled, from reduplication to borrowing tochained sound shifts, the current approach signifi-cantly increases the power, accuracy, and efficiencyof automatic reconstruction.AcknowledgmentsWe would like to thank Anna Rafferty and our re-viewers for their comments.
This work was sup-ported by a NSERC fellowship to the first author andNSF grant number BCS-0631518 to the second au-thor.72ReferencesR.
Blust.
1993.
Central and central-Eastern Malayo-Polynesian.
Oceanic Linguistics, 32:241?293.A.
Bouchard-Co?te?, P. Liang, D. Klein, and T. L. Griffiths.2008.
A probabilistic approach to language change.
InAdvances in Neural Information Processing Systems20.A.
Bouchard-Co?te?, M. I. Jordan, and D. Klein.
2009.Efficient inference in phylogenetic InDel trees.
In Ad-vances in Neural Information Processing Systems 21.L.
Campbell.
1998.
Historical Linguistics.
The MITPress.S.
F. Chen.
2003.
Conditional and joint models forgrapheme-to-phoneme conversion.
In Proceedings ofEurospeech.M.
A. Covington.
1998.
Alignment of multiple lan-guages for historical comparison.
In Proceedings ofACL 1998.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society.
Se-ries B (Methodological), 39(1):1?38.M.
Dreyer, J. R. Smith, and J. Eisner.
2008.
Latent-variable modeling of string transductions with finite-state methods.
In Proceedings of EMNLP 2008.S.
P. Durham and D. E. Rogers.
1969.
An applicationof computer programming to the reconstruction of aproto-language.
In Proceedings of the 1969 confer-ence on Computational linguistics.C.
L. Eastlack.
1977.
Iberochange: A program tosimulate systematic sound change in Ibero-Romance.Computers and the Humanities.J.
Felsenstein.
1989.
PHYLIP - PHYLogeny InferencePackage (Version 3.2).
Cladistics, 5:164?166.S.
Goldwater and M. Johnson.
2003.
Learning OTconstraint rankings using a maximum entropy model.Proceedings of the Workshop on Variation within Op-timality Theory.S.
J. Greenhill, R. Blust, and R. D. Gray.
2008.
TheAustronesian basic vocabulary database: From bioin-formatics to lexomics.
Evolutionary Bioinformatics,4:271?283.H.
H. Hock.
1986.
Principles of Historical Linguistics.Walter de Gruyter.I.
Holmes and W. J. Bruno.
2001.
Evolutionary HMM:a Bayesian approach to multiple alignment.
Bioinfor-matics, 17:803?820.W.
Jank.
2005.
Stochastic variants of EM: Monte Carlo,quasi-Monte Carlo and more.
In Proceedings of theAmerican Statistical Association.G.
Kondrak.
2000.
A new algorithm for the alignment ofphonetic sequences.
In Proceedings of NAACL 2000.G.
Kondrak.
2002.
Algorithms for Language Recon-struction.
Ph.D. thesis, University of Toronto.V.
I. Levenshtein.
1966.
Binary codes capable of correct-ing deletions, insertions and reversals.
Soviet PhysicsDoklady, 10, February.D.
C. Liu, J. Nocedal, and C. Dong.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming, 45:503?528.J.
B. Lowe and M. Mazaudon.
1994.
The reconstructionengine: a computer implementation of the comparativemethod.
Comput.
Linguist., 20(3):381?417.G.
A. Lunter, I. Miklo?s, Y. S. Song, and J. Hein.
2003.An efficient algorithm for statistical multiple align-ment on arbitrary phylogenetic trees.
Journal of Com-putational Biology, 10:869?889.B.
Nothofer.
1975.
The reconstruction of Proto-Malayo-Javanic.
M. Nijhoff.M.
P. Oakes.
2000.
Computer estimation of vocabu-lary in a protolanguage from word lists in four daugh-ter languages.
Journal of Quantitative Linguistics,7(3):233?244.A.
Prince and P. Smolensky.
1993.
Optimality theory:Constraint interaction in generative grammar.
Techni-cal Report 2, Rutgers University Center for CognitiveScience.L.
Tierney.
1994.
Markov chains for exploring posteriordistributions.
The Annals of Statistics, 22(4):1701?1728.A.
Varadarajan, R. K. Bradley, and I. H. Holmes.
2008.Tools for simulating evolution of aligned genomic re-gions with integrated parameter estimation.
GenomeBiology, 9:R147.C.
Wilson.
2006.
Learning phonology with substantivebias: An experimental and computational study of ve-lar palatalization.
Cognitive Science, 30.5:945?982.73
