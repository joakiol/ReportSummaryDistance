Identifying Similar and Co-referring Documents Across LanguagesPattabhi R K Rao TAU-KBC Research Centre,MIT Campus, Anna University,Chennai-44, India.pattabhi@au-kbc.orgSobha LAU-KBC Research Centre,MIT Campus, Anna University,Chennai-44, India.sobha@au-kbc.orgAbstractThis paper presents a methodology forfinding similarity and co-reference ofdocuments across languages.
The similaritybetween the documents is identified ac-cording to the content of the whole docu-ment and co-referencing of documents isfound by taking the named entities presentin the document.
Here we use Vector SpaceModel (VSM) for identifying both similar-ity and co-reference.
This can be applied incross-lingual search engines where usersget documents of very similar content fromdifferent language documents.1 IntroductionIn this age of information technology revolution,the growth of technology and easy accessibility hascontributed to the explosion of text data on the webin different media forms such as online newsmagazines, portals, emails, blogs etc in differentlanguages.
This represents 80% of the unstructuredtext content available on the web.
There is an ur-gent need to process such huge amount of text us-ing Natural Language Processing (NLP) tech-niques.
One of the significant challenges with theexplosion of text data is to organize the documentsinto meaningful groups according to their content.The work presented in this paper has two partsa) finding multilingual cross-document similarityand b) multilingual cross-document entity co-referencing.
The present work analyzes the docu-ments and identifies whether the documents aresimilar and co-referring.
Two objects are said to besimilar, when they have some common propertiesbetween them.
For example, two geometrical fig-ures are said to be similar if they have the sameshape.
Hence similarity is a measure of degree ofresemblance between two objects.Two documents are said to be similar if theircontents are same.
For example a document D1describes about a bomb blast incident in a city anddocument D2 also describes about the same bombblast incident, its cause and investigation details,then D1 and D2 are said to be similar.
But ifdocument D3 talks of terrorism in general and ex-plains bomb blast as one of the actions in terrorismand not a particular incident which D1 describes,then documents D1 and D3 are dissimilar.
The taskof finding document similarity differs from thetask of document clustering.
Clustering is a task ofcategorization of documents based on domain/field.In the above example, documents D1, D2, D3 canbe said to be in a cluster of crime domain.
Whendocuments are similar they share common nounphrases, verb phrases and named entities.
While indocument clustering, sharing of named entities andnoun phrases is not essential but still there can besome noun phrases and named entities in common.Cross-document co-referencing of entities refers tothe identification of same entities across the docu-ments.
When the named entities present in thedocuments which are similar and also co-referencing, then the documents are said to be co-referring documents.The paper is further organized as follows.
Insection 2, the motivation behind this paper is ex-plained and in 3 the methodology used is described.Results and discussions are dealt in section 4 andconclusion in section 5.2 MotivationDekang Lin (1998) defines similarity from the in-formation theoretic perspective and is applicable ifthe domain has probabilistic model.
In the pastdecade there has been significant amount of workdone on finding similarity of documents and orga-nizing the documents according to their content.Similarity of documents are identified using differ-ent methods such as Self-Organizing Maps (SOMs)(Kohonen et al 2000; Rauber, 1999), based on On-tologies and taxanomy (Gruber, 1993; Resnik,1995), Vector Space Model (VSM) with similaritymeasures like Dice similarity, Jaccard?s similarity,cosine similarity (Salton, 1989).
Bagga (Bagga etal., 1998) have used VSM in their work for findingco-references across the documents for Englishdocuments.
Chung and Allan (2004) have workedon cross-document co-referencing using large scalecorpus, where they have said ambiguous namesfrom the same domain (here for example, politics)are harder to disambiguate when compared tonames from different domains.
In their workChung and Allan compare the effectiveness of dif-ferent statistical methods in cross-document co-reference resolution task.
Harabagiu and Maiorano(2000) have worked on multilingual co-referenceresolution on English and Romanian languagetexts.
In their system, ?SWIZZLE?
they use a data-driven methodology which uses aligned bilingualcorpora, linguistic rules and heuristics of Englishand Romanian documents to find co-references.
Inthe Indian context, obtaining aligned bilingual cor-pora is difficult.
Document similarity between In-dian languages and English is tough since the sen-tence structure differs and Indian languages areagglutinative in nature.
In the recent years therehas been some work done in the Indian languages,(Pattabhi et al 2007) have used VSM for multilin-gual cross-document co-referencing, for Englishand Tamil, where no bilingual aligned corpora isused.One of the methods used in cross-lingual infor-mation retrieval (CLIR) is Latent Semantic Analy-sis (LSA) in conjunction with multilingual parallelaligned corpus.
This approach works well for in-formation retrieval task where it has to retrievemost similar document in one language to a querygiven in another language.
One of the drawbacksof using LSA in multilingual space for the tasks ofdocument clustering, document similarity is that itgives similar documents more based on the lan-guage than by topic of the documents in differentlanguages (Chew et al 2007).
Another drawbackof LSA is that the reduced dimension matrix is dif-ficult to interpret semantically.
The examples inTable 1, illustrate this.Before Reduction After Reduction1.
{(car),(truck),(flower)} {(1.2810*car+0.5685*truck),(flower)2 {(car),(bottle),(flower)} {(1.2810*car+0.5685*bottle),(flower)Table 1.
LSA ExampleIn the first example the component(1.2810*car+0.5685*truck) can be inferred as?Vehicle?
but in cases such as in second example,the component (1.2810*car+0.5685*bottle) doesnot have any interpretable meaning in natural lan-guage.
In LSA the dimension reduction factor ?k?has very important role to play and the value of ?k?can be found by doing several experiments.
Theprocess of doing dimension reduction in LSA iscomputationally expensive.
When LSA is used, itreduces the dimensions statistically and when thereis no parallel aligned corpus, this can not be inter-preted semantically.Hence, in the present work, we propose VSMwhich is computationally simple, along with cosinesimilarity measure to find document similarity aswell as entity co-referencing.
We have taken Eng-lish and three Dravidian languages viz.
Tamil, Te-lugu and Malayalam for analysis.3 MethodologyIn VSM, each document is represented by a vectorwhich specifies how many times each term occursin the document (the term frequencies).
Thesecounts are weighted to reflect the importance ofeach term and weighting is the inverse documentfrequency (idf).
If a term t occurs in n documentsin the collection then the ?idf?
is the inverse of logn.
This vector of weighted counts is called a "bagof words" representation.
Words such as "stopwords" (or function words) are not included in therepresentation.The documents are first pre-processed, to getsyntactic and semantic information for each wordin the documents.
The preprocessing of documentsinvolves sentence splitting, morph analysis, part-of-speech (POS) tagging, text chunking and namedentity tagging.
The documents in English are pre-processed using Brill?s Tagger (Brill, 1994) forPOS tagging and fn-TBL (Ngai and Florian, 2001)for text chunking.
The documents in Indian lan-guages are preprocessed, using  a generic engine(Arulmozhi et al, 2006) for POS tagging, and textchunking based on TBL (Sobha and Vijay, 2006).For both English and Indian language documentsthe named entity tagging is done using Named En-tity Recognizer (NER) which was developed basedon conditional random field (CRF).
The tagsetused by the NER tagger is a hierarchical tagset,consists of mainly i) ENAMEX, ii) NUMEX andiii) TIMEX.
Inside the ENAMEX there are mainly11 subtype?s viz.
a) Person b) Organization c) Lo-cation d) Facilities e) Locomotives f) Artifacts g)Entertainment h) Cuisines i) Organisms j) Plants k)Disease.
For the task of multilingual cross-document entities co-referencing, the documentsare further processed for anaphora resolutionwhere the corresponding antecedents for each ana-phor are tagged in the document.
For documents inEnglish and Tamil, anaphora resolution is doneusing anaphora resolution system.
For documentsin Malayalam and Telugu anaphora resolution isdone manually.
After the preprocessing of docu-ments, the language model is built by computingthe term frequency ?
inverse document frequency(tf-idf) matrix.
For the task of finding multilingualcross-document similarity, we have performed fourdifferent experiments.
They are explained below:E1: The terms are taken from documents afterremoving the stop words.
These are raw termswhere no preprocessing of documents is done; theterms are unique words in the document collection.E2: The terms taken are the words inside thenoun phrases, verb phrases and NER expressionsafter removing the stop words.E3: The whole noun phrase/verb phrase/NERexpression is taken to be a single term.E4: The noun phrase/NER expression alongwith the POS tag information is taken as a singleterm.The first experiment is the standard VSM im-plementation.
The rest three experiments differ inthe way the terms are taken for building the VSM.For building the VSM model which is common forall language document texts, it is essential thatthere should be translation/transliteration tool.
Firstthe terms are collected from individual languagedocuments and a unique list is formed.
After that,using the translation/transliteration tool the equiva-lent terms in language L2 for language L1 arefound.
The translation is done using a bilingualdictionary for the terms present in the dictionary.For most of the NERs only transliteration is possi-ble since those are not present in the dictionary.The transliteration tool is developed based on thephoneme match it is a rule based one.
All the In-dian language documents are represented in romannotation (wx-notation) for the purpose of process-ing.After obtaining equivalent terms in all lan-guages, the VSM model is built.
Let S1 and S2 bethe term vectors representing the documents D1and D2, then their similarity is given by equation(1) as shown below.Sim(S1,S2) = ?
(W1j x W2j )                      -- (1)tjWhere,tj is a term present in both vectors S1and S2.W1j is the weight of term tj in S1 andW2j is the weight of term tj in S2.The weight of term tj in the vector S1 is calculatedby the formula given by equation (2), below.Wij=(tf*log(N/df))/[sqrt(Si12+Si22+?
?+Sin2)] --(2)Where,tf = term frequency of term tjN=total number of documents in the collectiondf = number of documents in the collection thatthe term tj    occurs in.sqrt represents square rootThe denominator [sqrt(Si12+Si22+?
?+Sin2)] is the co-sine normalization factor.
This cosine normalizationfactor is the Euclidean length of the vector Si, where ?i?is the document number in the collection and Sin2 is thesquare of the product of (tf*log(N/df)) for term tn in thevector Si.For the task of multilingual cross-document en-tity co-referencing, the words with-in the anaphortagged sentences are considered as terms for build-ing the language model.4 Results and DiscussionThe corpus used for experiments is collected fromonline news magazines and online news portals.The sources in English include ?The Hindu?,?Times of India?, ?Yahoo News?, ?New YorkTimes?, ?Bangkok Post?, ?CNN?, ?WISC?, ?TheIndependent?.
The sources for Tamil include ?Di-namani?, ?Dinathanthi?, ?Dinamalar?, ?Dina-karan?, and ?Yahoo Tamil?.
The work was primar-ily done using English and Tamil.
Later on thiswas extended for Malayalam and Telugu.
The datasources for Malayalam are ?Malayala Manorama?,?Mathrubhumi?, ?Deshabhimani?, ?Deepika?
andsources for Telugu include ?Eenadu?, ?Yahoo Te-lugu?
and ?Andhraprabha?.
First we discuss aboutEnglish and Tamil and Later Telugu and Malaya-lam.The domains of the news taken include sports,business, politics, tourism etc.
The news articleswere collected using a crawler, and hence we findin the collection, a few identical news articles be-cause they appear in different sections of the newsmagazine like in Front page section, in state sec-tion and national section.The dataset totally consists of 1054 Englishnews articles, 390 Tamil news articles.
Here wediscuss results in two parts; in the first part resultspertaining to document similarity are explained.
Insecond part we discuss results on multilingualcross-document entity co-referencing.4.1 Document SimilarityThe data collection was done in four instances,spread in a period of two months.
At the first in-stance two days news was crawled from differentnews sources in English as well as Tamil.
In thefirst set 1004 English documents and 297 Tamildocuments were collected.In this set when manually observed (humanjudgment) it was found that there are 90 similardocuments forming 31 groups, rest of the docu-ments were not similar.
This is taken as gold stan-dard for the evaluation of the system output.As explained in the previous section, on this setthe four experiments were performed.
In the firstexperiment (E1), no preprocessing of the docu-ments was done except that the stop words wereremoved and the language model was built.
In thisit was observed that the number of similar docu-ments is 175 forming 25 groups.
Here it was ob-served that along with actual similar documents,system also gives other not similar documents (ac-cording to gold standard) as similar ones.
This isdue to the fact there is no linguistic informationgiven to the system, hence having words alonedoes not tell the context, or in which sense it isused.
And apart from that named entities whensplit don?t give exact meaning, for example inname of hotels ?Leela Palace?
and ?Mysore Pal-ace?, if split into words yields three words,?Leela?, ?Mysore?, and ?Palace?.
In a particulardocument, an event at hotel Leela Palace is de-scribed and the hotel is referred as Leela Palace orby Palace alone.
Another document describesabout Dussera festival at Mysore Palace.
Now herethe system identifies both these documents to besimilar even though both discuss about differentevents.
The precision of the system was observedto be 51.4%, where as the recall is 100% since allthe documents which were similar in the gold stan-dard is identified.
Here while calculating the preci-sion; we are considering the number of documentsthat are given by the system as similar to the num-ber of documents similar according to the goldstandard.Hence to overcome the above discussed prob-lem, we did the second experiment (E2) whereonly words which occur inside the noun phrases,verb phrases and named entities are considered asterms for building the language model.
Here it isobserved that the number of similar documents is140 forming 30 groups.
This gives a precision of64.2% and 100% recall.
Even though we find asignificant increase in the precision but still thereare large number of false positives given by thesystem.
A document consists of noun phrases andverb phrases, when the individual tokens insidethese phrases are taken; it is equivalent to takingalmost the whole document.
This reduces thenoise.
The problem of ?Leela Palace?
and ?MysorePalace?
as explained in the previous paragraph stillpersists here.In the third experiment (E3) the whole nounphrase, verb phrase and named entity is consideredas a single term for building the language model.Here the phrases are not split into individual to-kens; the whole phrase is a single term for lan-guage model.
This significantly reduces the num-ber of false positives given by the system.
The sys-tem identifies 106 documents as similar documentsforming 30 groups.
Now the precision of the sys-tem is 84.9%.
In this experiment, the problem of?Leela Palace?
and ?Mysore Palace?
is solved.Though this problem was solved the precision ofthe system is low, hence we performed the fourth(E4) experiment.In the fourth experiment (E4), the part-of-speech(POS) information is given along with the phrasefor building the language model.
It is observed thatthe precision of the system increases.
The numberof similar documents identified is 100 forming 31groups.
This gives a precision of 90% and a recallof 100%.Another important factor which plays a crucialrole in implementation of language model or VSMis the threshold point.
What is the threshold pointthat is to be taken?
For obtaining an answer for thisquestion, few experiments were performed by set-ting the threshold at various points in the range0.75 to 0.95.
When the threshold was set at 0.75the number of similar documents identified by thesystem was larger, not true positives but insteadfalse positives.
Hence the recall was high and pre-cision was low at 50%.
When the threshold wasmoved up and set at 0.81, the number of similardocuments identified was more accurate and thenumber of false positives got reduced.
The preci-sion was found to be 66%.
When the thresholdwas moved up still further and set at 0.90, it wasfound that the system identified similar documentswhich were matching with the human judgment.The precision of the system was found to be 90%.The threshold was moved up further to 0.95, think-ing that the precision would further improve, butthis resulted in documents which were actuallysimilar to be filtered out by the system.
Hence thethreshold chosen was 0.9, since the results ob-tained at this threshold point had matched the hu-man judgment.
For the experiments E1, E2, E3 andE4 explained above, the threshold is fixed at 0.9.A new set of data consisting of 25 documentsfrom 5 days news articles is collected.
This is com-pletely taken from single domain, terrorism.
Thesenews articles describe specifically the Hyderabadbomb blast, which occurred on August 25th 2007.All these 25 documents were only English docu-ments from various news magazines.
This data setwas collected specifically to observe the perform-ance of the system, when the documents belongingto single domain are given.
In the new data set,from terrorism domain, human judgment for docu-ment similarity was found to have 13 similar docu-ments forming 3 groups.
While using this data setthe noun phrases, verb phrases and named entitiesalong with POS information were taken as terms tobuild the language model and the threshold was setat 0.9, it was observed that the system finds 14documents to be similar forming 3 groups.
Here,out of 14 similar documents, only 12 documentsmatch with the human judgment and one documentwhich ought to be identified was not identified bythe system.
The document which was not identifieddescribed about the current event, that is, bombblast on 25th August in the first paragraph and thenthe rest of the document described about the simi-lar events that occurred in the past.
Hence the simi-larity score obtained for this document with respectto other documents in the group was 0.84 which islower than the threshold fixed.
Hence the recall ofthe system is 92.3% and the precision of the sys-tem is 85.7%.Another data set consisting of 114 documentswas taken from tourism domain.
The documentswere both in Tamil and English, 79 documents inTamil and 35 documents in English.
This data setdescribes various pilgrim places and temples inSouthern India.
The human annotators have found21 similar documents which form a group of three.These similar documents describe about LordSiva?s and Lord Murugan?s temples.
The systemobtained 25 documents as similar and grouped intothree groups.
Out of 25 documents obtained assimilar, four were dissimilar.
These dissimilardocuments described non-Siva temples in the sameplace.
In these dissimilar documents the names ofofferings, festivals performed were referred by thesame names as in the rest of the documents of thegroup, hence these documents obtained similarityscore of 0.96 with respect to other documents inthe group.
Here we get a precision of 84% and arecall of 100%.A new data set consisting of 46 documents wastaken from various news magazines.
This set con-sists of 24 English documents, 11 Tamil docu-ments, 7 Malayalam documents and 4 Telugudocuments.
This data set describes the earthquakein Indonesia on 12th September 2007 and tsunamiwarning in other countries.
The news articles werecollected on two days 13th and 14th September2007.The documents collected were in different fontencoding schemes.
Hence before doing naturallanguage processing such as morph-analysis, POStagging etc, the documents were converted to acommon roman notation (wx-notation) using thefont converter for each encoding scheme.Here we have used multilingual dictionaries ofplace; person names etc for translation.
The lan-guage model is built by taking noun phrases andverb phrases along with POS information were asterms.
In this set human annotators have found 45documents to be similar and have grouped theminto one group.
The document which was identi-fied as dissimilar describes about a Tamil filmshooting at Indonesia being done during the quaketime.
The system had identified all the 46 docu-ments including the film shooting document in thecollection to be similar and put into one group.
The?film shooting?
document consisted of two para-graphs about the quake incident, other two para-graphs consisted of statement by the film producerstating that the whole crew is safe and the shootingis temporarily suspended for next few days.
Sincethis document also contained the content describ-ing the earthquake found in other documents of thegroup, the system identified this ?film shooting?document to be similar.
Here one interesting pointwhich was found was that all the documents gave avery high similarity score greater than 0.95.
Hencethe precision of the system is 97.8% and recall100%.The summary of all these experiments with dif-ferent dataset is shown in the table 2 below.SNo Dataset Preci-sion %Recall%1 English 1004 and Tamil297 documents90.0 100.02 English 25 ?
terrorismdomain documents85.7 92.33 35 English Docs andTamil 79 docs - Tour-ism domain84.0 100.04 46 Docs on EarthQuake incident ?
24English, 11 Tamil, 7Malayalam, 4 Telugu97.8 100.0Average 89.3 % 98.07%Table 2.
Summary of Results for Documentsimilarity for four different data sets4.2 Document Co-referencingThe documents that were identified as similar onesare taken for entity co-referencing.
In this work theidentification of co-referencing documents is donefor English and Tamil.
In this section first we dis-cuss the co-referencing task for English documentsin terrorism domain, then for documents in Englishand Tamil in Tourism domain.
In the end of thissection we discuss about documents in English andTamil, which are not domain specific.In the first experiment, the document collection interrorism domain is taken for co-referencing task.This data set of 25 documents in terrorism domainconsists of 60 unique person names.
In this workwe consider only person names for entity co-referencing.
In this data set, 14 documents areidentified as similar ones by the system.
These 14documents consist of 26 unique person names.
.The language model is built using only namedentity terms and the noun, verb phrases occurringin the same sentence where the named entity oc-curs.
POS information is also provided with theterms.
Here we find that out of 26 entities, the sys-tem co-references correctly for 24 entities, eventhough the last names are same.
The results ob-tained for these named entities is shown in the be-low table Table 3.EntityNameNo.
of linkscontainingthe entityCorrectResponsesobtainedTotal Re-sponses ob-tainedPrecision%Recall %Y S Ra-jasekharReddy7 7 7 100 100IndrasenaReddy1 1 1 100 100K JanaReddy1 1 1 100 100ShivarajPatil2 2 2 100 100ManmohanSingh4 4 4 100 100Abdul Sha-helMohammad1 1 2 50 100MohammadAbdullah1 1 2 50 100MohammadAmjad1 1 1 100 100MohammadYunus1 1 1 100 100Ibrahim 1 1 1 100 100DawoodIbrahim1 1 1 100 100MadhukarGupta3 3 3 100 100N Chandra-babu Naidu2 2 2 100 100TasnimAslam2 2 2 100 100MahenderAgrawal1 1 1 100 100SomnathChatterjee2 2 2 100 100PervezMusharaff2 2 2 100 100Sonia Gan-dhi2 2 2 100 100Taslima 1 1 1 100 100NasrinBandaruDattatreya1 1 1 100 100L K Advani 2 2 2 100 100Average 95.2 100Table 3.
Results for entity co-referencing for Eng-lish documents in terrorism domainThe system identifies the entity names endingwith ?Reddy?
correctly.
These names in the docu-ments occur along with definite descriptions whichhelps the system in disambiguating these names.For example ?Y S Rajasekhar Reddy?
in most casesis referred to as ?Dr.
Reddy?
along with the defi-nite description ?chief minister?.
Similarly theother name ?K Jana Reddy?
occurs with the defi-nite description ?Home minister?.
Since here weare taking full noun phrases as terms for buildinglanguage model, this helps obtaining good results.For entities such as ?Abdul Shahel Mohammad?and ?Mohammad Abdullah?, it is observed that theboth names are referred in the documents as?Mohammad?
and surrounding phrases do nothave any distinguishing phrases such as definitedescriptions, which differentiate these names.
Boththese entities have been involved in mastermindingof the Hyderabad bomb blast.
Hence the systemcouldn?t disambiguate between these two namedentities and identifies both to be same, hence itfails here.In the second experiment, the data set in Tour-ism domain consisting of 79 Tamil Documents and35 English documents is taken for the task of co-referencing.
In this data set 25 documents wereidentified as similar.
Now these similar documentsof 25 are considered for entity co-referencing task.There are 35 unique names of Gods.
Here in thisdomain, one of the interesting points is that, thereare different names to refer to a single God.
Forexample Lord Murugan, is also referred by othernames such as ?Subramanyan?, ?Saravana?, ?Kart-tikeyan?, ?Arumukan?
etc.
Simialrly for Lord Sivais referred by ?Parangirinathar?, ?Dharbaranes-wara?
etc.
It is observed that in certain documentsthe alias names are not mentioned along withcommon names.
In these instances even humanannotators found it tough for co-referencing, hencethe system could not identify the co-references.This problem of alias names can be solved by hav-ing a thesaurus and using it for disambiguation.The results obtained for these named entities areshown in the table 4, below.EntityNameNo.
oflinks  con-taining theentityCorrectResponsesobtainedTotal Re-sponsesobtainedPrecision%Recall %Murugan 7 7 8 87.5 100Shiva 10 9 9 100 90Parvathi 10 9 11 81.8 90Nala 5 5 5 100 100Damayan-thi2 2 2 100 100Narada 3 3 3 100 100Sanees-warar6 6 7 85.7 100Deivayani 4 4 4 100 100Vishnu 2 2 2 100 100Vinayaka 3 3 3 100 100Indra 2 2 2 100 100Thiruna-vukkarasar1 1 1 100 100Mayan 2 2 2 100 100Average 96.5 98.4Table 4.
Results for entity co-referencing forEnglish and Tamil Documents in Tourism domainThe co-referencing system could disambiguate adocument which was identified as similar by thesystem and dissimilar by the human annotator.Another experiment is performed where bothEnglish and Tamil Documents are taken for entityco-referencing.
In this experiment we have takenthe data set in which there are 1004 English docu-ments and 297 Tamil documents.
The documentsare not domain specific.
Here 100 documents areidentified as similar ones, which contains of 64English and 36 Tamil documents.
Now we con-sider these 100 similar documents for entity co-referencing.
In the 100 similar documents, thereare 520 unique named entities.
The table (Table 5)below shows results of few interesting named enti-ties in this set of 100 similar documents.EntityNameNo.
of linkscontainingthe entityCorrectResponsesobtainedTotal Re-sponses ob-tainedPrecision%Recall  %Karunanidhi 7 7 7 100 100Manmohan Singh 15 14 16 87.5 93.3Sonia Gandhi 54 54 58 93.1 100Shivaraj Patil 8 8 10 80 100Prathibha Patil 24 24 26 92.3 100Lalu Prasad 5 5 5 100 100Atal Bihari Va-jpayee4 4 4 100 100Abdul Kalam 22 22 22 100 100Sania Mirza 10 10 10 100 100Advani 8 8 8 100 100Average 95.3 99.3Table 5.
Results for entity co-referencing forEnglish and Tamil Documents not of any specificdomain5 ConclusionThe VSM method is a well known statisticalmethod, but here it has been applied for multilin-gual cross-document similarity, which is a first ofits kind.
Here we have tried different experimentsand found that using phrases with its POS informa-tion as terms for building language model is givinggood performance.
In this we have got an averageprecision of 89.3 and recall of 98.07% for docu-ment similarity.
Here we have also worked on mul-tilingual cross-document entity co-referencing andobtained an average precision of 95.6 % and recallof 99.2 %.
The documents taken for multilingualcross-document co-referencing are similar docu-ments identified by the similarity system.
Consid-ering similar documents, helps indirectly in gettingcontextual information for co-referencing entities,because obtaining similar documents removesdocuments which are not in the same context.Hence this helps in getting good precision.
Herewe have worked on four languages viz.
English,Tamil, Malayalam and Telugu.
This can be appliedfor other languages too.
Multilingual documentsimilarity and co-referencing, helps in retrievingsimilar documents across languages.ReferencesArulmozhi Palanisamy and Sobha Lalitha Devi.
2006.HMM based POS Tagger for a Relatively Free WordOrder Language, Journal of Research on ComputingScience, Mexico.
18:37-48.Bagga, Amit and Breck Baldwin.
1998.
Entity-BasedCross-Document Coreferencing Using the VectorSpace Model, Proceedings of the 36th Annual Meet-ing of the Association for Computational Linguisticsand the 17th International Conference on Computa-tional Linguistics (COLING-ACL'98):79-85.Brill, Eric.
1994.
Some Advances in transformationBased Part of Speech Tagging, Proceedings of theTwelfth International Conference on Artificial Intel-ligence (AAAI-94), Seattle, WAPeter A. Chew,  Brett W. Bader, Tamara G. Kolda, Ah-med Abdelali.
2007.
Cross-Language InformationRetrieval Using PARAFAC2, In the ProceedingsThirteenth International Conference on KnowledgeDiscovery and Data Mining (KDD?
07), San Jose,California.
:143-152.Chung Heong Gooi and James Allan.
2004.
Cross-Document Coreference on a Large Scale Corpus,Proceedings of HLT-NAACL: 9-16.Dekang Lin.
1998.
An Information-Theoretic Definitionof Similarity, Proceedings of International Confer-ence on Machine Learning, Madison, Wisconsin,July.T.
R. Gruber.
1993.
A translation approach to portableontologies, Knowledge Acquisition, 5(2):199?220.Harabagiu M Sanda and Steven J Maiorano.
2000.
Mul-tilingual Coreference Resolution, Proceedings of 6thApplied Natural Language Processing Conference:142?149.Kohonen, Teuvo Kaski, Samuel Lagus, Krista Salojarvi,Jarkko Honkela, Jukka Paatero,Vesa Saarela, Anti.2000.
Self organisation of a massive document col-lection, IEEE Transactions on Neural Networks,11(3): 574-585.G.
Ngai and R. Florian.
2001.
Transformation-BasedLearning in the Fast Lane, Proceedings of theNAACL'2001, Pittsburgh, PA: 40-47R K Rao Pattabhi, L Sobha, and Amit Bagga.
2007.Multilingual cross-document co-referencing, Pro-ceedings of 6th Discourse Anaphora and AnaphorResolution Colloquium (DAARC), March 29-30,2007, Portugal:115-119Rauber, Andreas Merkl, Dieter.
1999.
The SOMLibdigital library system,  In the Proceedings of the 3rdEuropean Conference on Research and AdvancedTechnology for Digital Libraries (ECDL'99), Paris,France.
Berlin: 323-341.P.
Resnik.
1995.
Using information content to evaluatesemantic similarity in taxonomy, Proceedings ofIJCAI: 448?453.Salton, Gerald.
1989.
Automatic Text Processing: TheTransformation, Analysis and Retrieval of Informa-tion by Computer, Reading, MA: Addison WesleySobha L, and Vijay Sundar Ram.
2006.
Noun PhraseChunker for Tamil, Proceedings of the First NationalSymposium on Modeling and Shallow Parsing of In-dian Languages (MSPIL), IIT Mumbai, India: 194-198.
