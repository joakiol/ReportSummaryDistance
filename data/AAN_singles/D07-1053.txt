Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
506?513, Prague, June 2007. c?2007 Association for Computational LinguisticsMethods to integrate a language model with semanticinformation for a word prediction componentTonio WandmacherLaboratoire d?Informatique (LI)Universit?
Fran?ois Rabelais de Tours3 place Jean-Jaur?s, 41000 Blois, Francetonio.wandmacher@univ-tours.frJean-Yves AntoineLaboratoire d?Informatique (LI)Universit?
Fran?ois Rabelais de Tours3 place Jean-Jaur?s, 41000 Blois, Francejean-yves.antoine@univ-tours.frAbstractMost current word prediction systems makeuse of n-gram language models (LM) to es-timate the probability of the following wordin a phrase.
In the past years there havebeen many attempts to enrich such lan-guage models with further syntactic or se-mantic information.
We want to explore thepredictive powers of Latent SemanticAnalysis (LSA), a method that has beenshown to provide reliable information onlong-distance semantic dependencies be-tween words in a context.
We present andevaluate here several methods that integrateLSA-based information with a standardlanguage model: a semantic cache, partialreranking, and different forms of interpola-tion.
We found that all methods show sig-nificant improvements, compared to the 4-gram baseline, and most of them to a sim-ple cache model as well.1 Introduction: NLP for AAC systemsAugmented and Alternative Communication(AAC) is a field of research which concerns naturallanguage processing as well as human-machineinteraction, and which aims at restoring the com-municative abilities of disabled people with severespeech and motion impairments.
These people canbe for instance cerebrally and physically handi-capped persons or they suffer from a locked-insyndrome due to a cerebral apoplexy.
Whatever thedisease or impairment considered, oral communica-tion is impossible for these persons who have inaddition serious difficulties to control physicallytheir environment.
In particular, they are not able touse standard input devices of a computer.
Most ofthe time, they can only handle a single switch de-vice.
As a result, communicating with an AAC sys-tem consists of typing messages by means of a vir-tual table of symbols (words, letters or icons)where the user successively selects the desireditems.Basically, an AAC system, such as FASTY(Trost et al 2005) or SIBYLLE (Schadle et al 2004),consists of four components.
At first, one finds aphysical input interface connected to the computer.This device is adapted to the motion capacities ofthe user.
When the latter must be restricted to asingle switch (eye glimpse or breath detector, forinstance), the control of the environment is reducedto a mere Yes/No command.Secondly, a virtual keyboard is displayed onscreen.
It allows the user to select successively thesymbols that compose the intended message.
InSIBYLLE, key selection is achieved by pointing let-ters through a linear scan procedure: a cursor suc-cessively highlights each key of the keyboard.The last two components are a text editor (towrite e-mails or other documents) and a speechsynthesis module, which is used in case of spokencommunication.
The latest version of SIBYLLEworks for French and German, and it is usable withany Windows?
application (text editor, webbrowser, mailer...), which means that the use of aspecific editor is no longer necessary.The main weakness of AAC systems results fromthe slowness of message composition.
On average,disabled people cannot type more than 1 to 5 wordsper minute; moreover, this task is very tiring.
Theuse of NLP techniques to improve AAC systems istherefore of first importance.506Figure 1: User interface of the SIBYLLE AAC systemTwo complementary approaches are possible tospeed up communication.
The first one aims atminimizing the duration of each item selection.Considering a linear scan procedure, one could forinstance dynamically reorganize the keyboard inorder to present the most probable symbols at first.The second strategy tries to minimize the numberof keystrokes to be made.
Here, the system tries topredict the words which are likely to occur just af-ter those already typed.
The predicted word is theneither directly displayed after the end of the in-serted text (a method referred to as ?word comple-tion?, cf.
Boissi?re and Dours, 1996), or a list of N-best (typically 3 to 7) predictions is provided on thevirtual keyboard.
When one of these predictionscorresponds to the intended word, it can be selectedby the user.
As can be seen in figure 1, the interfaceof the SIBYLLE system presents such a list of mostprobable words to the user.Several approaches can be used to carry outword prediction.
Most of the commercial AAC sys-tems make only use of a simple lexicon: in this ap-proach, the context is not considered.On the other hand, stochastic language modelscan provide a list of word suggestions, dependingon the n-1 (typically n = 3 or 4) last inserted words.It is obvious that such a model cannot take into ac-count long-distance dependencies.
There have beenattempts to integrate part-of-speech information(Fazly and Hirst, 2003) or more complex syntacticmodels (Schadle et al 2004) to achieve a betterprediction.
In this paper, we will nevertheless limitour study to a standard 4-gram model as a baselineto make our results comparable.
Our main aim ishere to investigate the use of long-distance seman-tic dependencies to dynamically adapt the predic-tion to the current semantic context of communica-tion.
Similar work has been done by Li and Hirst(2005) and Matiasek and Baroni (2003), who ex-ploit Pointwise Mutual Information (PMI; Churchand Hanks, 1989).
Trnka et al (2005) dynamicallyinterpolate a high number of topic-oriented modelsin order to adapt their predictions to the currenttopic of the text or conversation.Classically, word predictors are evaluated by anobjective metric called Keystroke Saving Rate(ksr):1001 ?????????
?=apn kkksr  (1)with kp, ka being the number of keystrokesneeded on the input device when typing a messagewith (kp) and without prediction (ka = number ofcharacters in the text that has been entered, n =length of the prediction list, usually n = 5).
As507Trost et al (2005) and Trnka et al (2005), we as-sume that one additional keystroke is required forthe selection of a word from the list and that aspace is automatically inserted afterwards.
Notealso that words, which have already occurred in thelist, will not reappear after the next character hasbeen inserted.The perplexity measure, which is frequentlyused to assess statistical language models, provedto be less accurate in this context.
We still presentperplexities as well in order to provide comparativeresults.2 Language modeling and semantics2.1 Statistical Language ModelsFor about 10 to 15 years statistical language model-ing has had a remarkable success in various NLPdomains, for instance in speech recognition, ma-chine translation, Part-of-Speech tagging, but alsoin word prediction systems.
N-gram based lan-guage models (LM) estimate the probability of oc-currence for a word, given a string of n-1 precedingwords.
However, computers have only recentlybecome powerful enough to estimate probabilitieson a reasonable  amount of training data.
More-over, the larger n gets, the more important the prob-lem of combinatorial explosion for the probabilityestimation becomes.
A reasonable trade-off be-tween performance and number of estimated eventsseems therefore to be an n of 3 to 5, including so-phisticated techniques in order to estimate theprobability of unseen events (smoothing methods).Whereas n-gram-like language models are al-ready performing rather well in many applications,their capacities are also very limited in that theycannot exploit any deeper linguistic structure.Long-distance syntactic relationships are neglectedas well as semantic or thematic constraints.In the past 15 years many attempts have beenmade to enrich language models with more com-plex syntactic and semantic models, with varyingsuccess (cf.
(Rosenfeld, 1996), (Goodman, 2002)or in a word prediction task: (Fazly and Hirst,2003), (Schadle, 2004), (Li and Hirst, 2005)).
Wewant to explore here an approach based on LatentSemantic Analysis (Deerwester et al 1990).2.2 Latent Semantic AnalysisSeveral works have suggested the use of LatentSemantic Analysis (LSA) in order to integrate se-mantic similarity to a language model (cf.
Belle-garda, 1997; Coccaro and Jurafsky, 1998).
LSAmodels semantic similarity based on co-occurrencedistributions of words, and it has shown to be help-ful in a variety of NLP tasks, but also in the domainof cognitive modeling (Landauer et al 1997).LSA is able to relate coherent contexts to spe-cific content words, and it is good at predicting theoccurrence of a content word in the presence ofother thematically related terms.
However, since itdoes not take word order into account (?bag-of-words?
model) it is very poor at predicting theiractual position within the sentence, and it is com-pletely useless for the prediction of function words.Therefore, some attempts have been made to inte-grate the information coming from an LSA-basedmodel with standard language models of the n-gram type.In the LSA model (Deerwester et al 1990) aword wi is represented as a high-dimensional vec-tor, derived by Singular Value Decomposition(SVD) from a term ?
document (or a term ?
term)co-occurrence matrix of a training corpus.
In thisframework, a context or history h (= w1, ... , wm)can be represented by the sum of the (already nor-malized) vectors corresponding to the words it con-tains (Landauer et al 1997):?==miiwh1rr(2)This vector reflects the meaning of the preceding(already typed) section, and it has the same dimen-sionality as the term vectors.
It can thus be com-pared to the term vectors by well-known similaritymeasures (scalar product, cosine).2.3 Transforming LSA similarities into prob-abilitiesWe make the assumption that an utterance or atext to be entered is usually semantically cohesive.We then expect all word vectors to be close to thecurrent context vector, whose corresponding wordsbelong to the semantic field of the context.
Thisforms the basis for a simple probabilistic model ofLSA: After calculating the cosine similarity foreach word vector iwrwith the vector hrof the cur-rent context, we could use the normalized similari-ties as probability values.
This probability distribu-tion however is usually rather flat (i.e.
the dynamic508range is low).
For this reason a contrasting (or tem-perature) factor ?
is normally applied (cf.
Coccaroand Jurafsky, 1998), which raises the cosine tosome power (?
is normally between 3 and 8).
Afternormalization we obtain a probability distributionwhich can be used for prediction purposes.
It iscalculated as follows:( )( )?
?
?=k?k?iiLSAhhwhhwhwP)(cos),cos()(cos),cos()(minmin rrrrrr(3)wi is a word in the vocabulary, h is the current con-text (history)iwrand hrare their corresponding vec-tors in the LSA space; cosmin( hr) returns the lowestcosine value measured for hr).
The denominatorthen normalizes each similarity value to ensure that?
=nk kLSA hwP 1),( .Let us illustrate the capacities of this model bygiving a short example from the French version ofour own LSA predictor:Context: ?Mon p?re ?tait professeur en math?matiqueset je pense que ?
(?My dad has been a professor in mathemat-ics and I think that ?
)Rank Word P1.
professeur (?professor?)
0.01172. math?matiques (?mathematics?)
0.01093. enseign?
(participle of ?taught?)
0.00834. enseignait (?taught?)
0.00535. mathematicien (?mathematician?)
0.00496. p?re (?father?)
0.00467. math?matique (?mathematics?)
0.00458. grand-p?re (?grand-father?)
0.00439. sciences (?sciences?)
0.003610. enseignant (?teacher?)
0.0032Example 1: Most probable words returned by theLSA model for the given context.As can be seen in example 1, all ten predictedwords are semantically related to the context, theyshould therefore be given a high probability of oc-currence.
However, this example also shows thedrawbacks of the LSA model: it totally neglects thepresence of function words as well as the syntacticstructure of the current phrase.
We therefore needto find an appropriate way to integrate the informa-tion coming from a standard n-gram model and theLSA approach.2.4 Density as a confidence measureMeasuring relation quality in an LSA space,Wandmacher (2005) pointed out that the reliabilityof LSA relations varies strongly between terms.
Healso showed that the entropy of a term does notcorrelate with relation quality (i.e.
number of se-mantically related terms in an LSA-generated termcluster), but he found a medium correlation (Pear-son coeff.
= 0.56) between the number of semanti-cally related terms and the average cosine similar-ity of the m nearest neighbors (density).
The closerthe nearest neighbors of a term vector are, the moreprobable it is to find semantically related terms forthe given word.
In turn, terms having a high densityare more likely to be semantically related to a givencontext (i.e.
their specificity is higher).We define the density of a term wi as follows:?=?=mjijiim wNNwmwD1))(,cos(1)( rr  (4)In the following we will use this measure (withm=100) as a confidence metric to estimate the reli-ability of a word being predicted by the LSA com-ponent, since it showed to give slightly better re-sults in our experiments than the entropy measure.3 Integrating semantic informationIn the following we present several different meth-ods to integrate semantic information as it is pro-vided by an LSA model into a standard LM.3.1 Semantic cache modelCache (or recency promotion) models have shownto bring slight but constant gains in language mod-eling (Kuhn and De Mori, 1990).
The underlyingidea is that words that have already occurred in atext are more likely to occur another time.
There-fore their probability is raised by a constant or ex-ponentially decaying factor, depending on the posi-tion of the element in the cache.
The idea of a de-caying cache function is that the probability of re-occurrence depends on the cosine similarity of theword in the cache and the word to be predicted.The highest probability of reoccurrence is usuallyafter 15 to 20 words.Similar to Clarkson and Robinson (1997), we im-plemented an exponentially decaying cache oflength l (usually between 100 and 1000), using the509following decay function for a word wi and its posi-tion p in the cache.2)(5,0),( ??????
??=?
?pid epwf  (5)?
= ?/3 if p < ?
and  ?
= l/3 if p ?
?.
The func-tion returns 0 if wi is not in the cache, and it is 1 ifp = ?.
A typical graph for (5) can be seen in figure(2).Figure 2: Decay function with ?=20 and l=300.We extend this model by calculating for each ele-ment having occurred in the context its m nearestLSA neighbors ( ),( ?wNNoccmr, using cosine simi-larity), if their cosine lies above a threshold ?, andadd them to the cache as well, right after the wordthat has occurred in the text (?Bring your friends?-strategy).
The size of the cache is adapted accord-ingly (for ?, ?
and l), depending on the number ofneighbors added.
This results in the followingcache function:),(),()(1 cospwfwwf?wP idliioccicache ?
?
?=    (6)with l = size of the cache.
?
is a constant  con-trolling the influence of the component (usually ?
?0.1/l); wiocc is a word that has already recently oc-curred in the context and is therefore added as astandard cache element, whereas wi is a nearestneighbor to wiocc.
fcos(wiocc, wi) returns the cosinesimilarity between ioccwrand iwr, with cos( ioccwr, iwr )> ?
(Rem: wi with cos( ioccwr, iwr ) ?
?
have not beenadded to the cache).
Since cos( iwr, iwr )=1, termshaving actually occurred before will be given fullweight, whereas all wi being only nearest LSAneighbors to wiocc will receive a weight correspond-ing to their cosine similarity with wiocc , which isless than 1 (but larger than ?
).fd(wi,p) is the decay factor for the current posi-tion p of wi in the cache, calculated as shown inequation (5).3.2 Partial rerankingThe underlying idea of partial reranking is to re-gard only the best n candidates from the basic lan-guage model for the semantic model in order toprevent the LSA model from making totally im-plausible (i.e.
improbable) predictions.
Words be-ing improbable for a given context will be disre-garded as well as words that do not occur in thesemantic model (e.g.
function words), because LSAis not able to give correct estimates for this groupof words (here the base probability remains un-changed).For the best n candidates their semantic probabilityis calculated and each of these words is assigned anadditional value, after a fraction of its base prob-ability has been subtracted (jackpot strategy).For a given context h we calculate the ordered setBESTn(h) = <w1, ?
, wn>, so that P(w1|h) ?P(w2|h) ??
?P(wn|h)For each wi in BESTn(h) we then calculate itsreranking probability as follows:)),(()(),cos()( iniiiRR whBestIwDhw?wP ???=rr(7)?
is a weighting constant controlling the overallinfluence of the reranking process, cos( iwr, iwr ) re-turns the cosine of the word?s vector and the cur-rent context vector, D(wi) gives the confidencemeasure of wi and I is an indicator function being1, iff wi ?BEST(h), and 0 otherwise.3.3 Standard interpolationInterpolation is the standard way to integrate in-formation from heterogeneous resources.
While fora linear combination we simply add the weightedprobabilities of two (or more) models, geometricinterpolation multiplies the probabilities, which areweighted by an exponential coefficient (0?
?1?1):Linear Interpolation (LI):)()1()()(' 11 isibi wP?wP?wP ?
?+?=    (8)510Geometric Interpolation (GI):?=???
?= nj?js?jb?is?ibiwPwPwPwPwP1)11(1)11(1)()()()()('     (9)The main difference between the two methods isthat the latter takes the agreement of two modelsinto account.
Only if each of the single models as-signs a high probability to a given event will thecombined probability be assigned a high value.
Ifone of the models assigns a high probability andthe other does not the resulting probability will belower.3.4 Confidence-weighted interpolationWhereas in standard settings the coefficients arestable for all probabilities, some approaches useconfidence-weighted coefficients that are adaptedfor each probability.
In order to integrate n-gramand LSA probabilities, Coccaro and Jurafsky(1998) proposed an entropy-related confidencemeasure for the LSA component, based on the ob-servation that words that occur in many differentcontexts (i.e.
have a high entropy), cannot well bepredicted by LSA.
We use here a density-basedmeasure (cf.
section 2.2), because we found it morereliable than entropy in preliminary tests.
For inter-polation purposes we calculate the coefficient ofthe LSA component as follows:)( ii wD??
?= , iff D(wi) > 0; 0 otherwise (10)with ?
being a weighting constant to control theinfluence of the LSA predictor.
For all experi-ments, we set ?
to 0.4 (i.e.
0 ?
?i ?
0.4), whichproved to be optimal in pre-tests.4 ResultsWe calculated our baseline n-gram model on a 44million word corpus from the French daily LeMonde (1998-1999).
Using the SRI toolkit (Stol-cke, 2002)1 we computed a 4-gram LM over a con-trolled 141,000 word vocabulary, using modifiedKneser-Ney discounting (Goodman, 2001), and weapplied Stolcke pruning (Stolcke, 1998) to reducethe model to a manageable size (?
= 10-7).1SRI Toolkit: www.speech.sri.com.The LSA space was calculated on a 100 millionword corpus from Le Monde (1996 ?
2002).
Usingthe Infomap toolkit2, we generated a term ?
termco-occurrence matrix for an 80,000 word vocabu-lary (matrix size = 80,000 ?
3,000), stopwordswere excluded.
After several pre-tests, we set thesize of the co-occurrence window to ?100.
The ma-trix was then reduced by singular value decomposi-tion to 150 columns, so that each word in the vo-cabulary was represented by a vector of 150 di-mensions, which was normalized to speed up simi-larity calculations (the scalar product of two nor-malized vectors equals the cosine of their angle).Our test corpus consisted of 8 sections from theFrench newspaper Humanit?, (January 1999, from5,378 to 8,750 words each), summing up to 58,457words.
We then calculated for each test set the key-stroke saving rate based on a 5-word list (ksr5) andperplexity for the following settings3:1.
4-gram LM only (baseline)2.
4-gram + decaying cache (l = 400)3.
4-gram + LSA using linear interpolationwith ?LSA = 0.11 (LI).4.
4-gram + LSA using geometric interpola-tion, with ?LSA = 0.07 (GI).5.
4-gram + LSA using linear interpolationand (density-based) confidence weighting(CWLI).6.
4-gram + LSA using geometric interpola-tion and (density-based) confidenceweighting (CWGI).7.
4-gram + partial reranking (n = 1000, ?
=0.001)8.
4-gram + decaying semantic cache(l = 4000; m = 10; ?
= 0.4, ?
= 0.0001)Figures 3 and 4 display the overall results in termsof ksr and perplexity.2Infomap Project: http://infomap-nlp.sourceforge.net/3All parameter settings presented here are based on results ofextended empirical pre-tests.
We used held-out developmentdata sets that have randomly been chosen from the Humanit?corpus.
(8k to 10k words each).
The parameters being pre-sented here were optimal for our test sets.
For reasons of sim-plicity we did not use automatic optimization techniques suchas the EM algorithm (cf.
Jelinek, 1990).511Figure 3: Results (ksr5) for all methods tested.Figure 4: Results (perplexity) for all methodstested.Using the results of our 8 samples, we performedpaired t tests for every method with the baseline aswell as with the cache model.
All gains for ksrturned out to be highly significant (sig.
level <0.001), and apart from the results for CWLI, allperplexity reductions were significant as well (sig.level < 0.007), with respect to the cache results.
Wecan therefore conclude that, with exception ofCWLI, all methods tested have a beneficial effect,even when compared to a simple cache model.
Thehighest gain in ksr (with respect to the baseline)was obtained for the confidence-weighted geo-metric interpolation method (CWGI; +1.05%), thehighest perplexity reduction was measured for GIas well as for CWGI (-9.3% for both).
All othermethods (apart from IWLI) gave rather similar re-sults (+0.6 to +0.8% in ksr, and -6.8% to -7.7% inperplexity).We also calculated for all samples the correla-tion between ksr and perplexity.
We measured aPearson coefficient of -0.683 (Sig.
level < 0.0001).At first glance, these results may not seem over-whelming, but we have to take into account thatour ksr baseline of 57.9% is already rather high,and at such a level, additional gains become hard toachieve (cf.
Lesher et al 2002).The fact that CWLI performed worse than evensimple LI was not expected, but it can be explainedby an inherent property of linear interpolation: Ifone of the models to be interpolated overestimatesthe probability for a word, the other cannot com-pensate for it (even if it gives correct estimates),and the resulting probability will be too high.
Inour case, this happens when a word receives a highconfidence value; its probability will then be over-estimated by the LSA component.5 Conclusion and further workAdapting a statistical language model with seman-tic information, stemming from a distributionalanalysis like LSA, has shown to be a non-trivialproblem.
Considering the task of word predictionin an AAC system, we tested different methods tointegrate an n-gram LM with LSA: A semanticcache model, a partial reranking approach, andsome variants of interpolation.We evaluated the methods using two differentmeasures, the keystroke saving rate (ksr) and per-plexity, and we found significant gains for allmethods incorporating LSA information, comparedto the baseline.
In terms of ksr the most successfulmethod was confidence-weighted geometric inter-polation (CWGI; +1.05% in ksr); for perplexity,the greatest reduction was obtained for standard aswell as for confidence-weighted geometric interpo-lation (-9.3% for both).
Partial reranking and thesemantic cache gave very similar results, despitetheir rather different underlying approach.We could not provide here a comparison withother models that make use of distributional infor-mation, like the trigger approach by Rosenfeld(1996), Matiasek and Baroni (2003) or the modelpresented by Li and Hirst (2005), based on Point-wise Mutual Information (PMI).
A comparison ofthese similarities with LSA remains to be done.Finally, an AAC system has not only the func-tion of simple text entering but also of providingcognitive support to its user, whose communicativeabilities might be totally depending on it.
There-fore, she or he might feel a strong improvement ofthe system, if it can provide semantically plausiblepredictions, even though the actual gain in ksrmight be modest or even slightly decreasing.
Forthis reason we will perform an extended qualitative512analysis of the presented methods with personswho use our AAC system SIBYLLE.
This is one ofthe main aims of the recently started ESAC_IMCproject.
It is conducted at the Functional Reeduca-tion and Rehabilitation Centre of Kerpape, Brit-tany, where SIBYLLE is already used by 20 childrensuffering from traumatisms of the motor cortex.They appreciate the system not only for communi-cation but also for language learning purposes.Moreover, we intend to make the word predictorof SIBYLLE publicly available (AFM Voltaire pro-ject) in the not-too-distant future.AcknowledgementsThis research is partially founded by the UFA(Universit?
Franco-Allemande) and the Frenchfoundations APRETREIMC (ESAC_IMC project)and AFM (VOLTAIRE project).
We also want tothank the developers of the SRI and the Infomaptoolkits for making their programs available.ReferencesBellegarda, J.
(1997): ?A Latent Semantic AnalysisFramework for Large-Span Language Modeling?,Proceedings of the Eurospeech 97, Rhodes, Greece.Boissi?re Ph.
and Dours D. (1996).
?VITIPI : Versatileinterpretation of text input by persons with impair-ments?.
Proceedings ICCHP'1996.
Linz, Austria.Church, K. and Hanks, P. (1989).
?Word associationnorms, mutual information and lexicography?.
Pro-ceedings of ACL, pp.
76-83.Clarkson, P. R. and Robinson, A.J.
(1997).
?LanguageModel Adaptation using Mixtures and an Exponen-tially Decaying Cache?, in Proc.
of the IEEEICASSP-97, Munich.Coccaro, N. and Jurafsky, D. (1998).
?Towards betterintegration of semantic predictors in statistical lan-guage modeling?, Proc.
of the ICSLP-98, Sydney.Deerwester, S. C., Dumais, S., Landauer, T., Furnas, G.and Harshman, R. (1990).
?Indexing by Latent Se-mantic Analysis?, JASIS  41(6), pp.
391-407.Fazly, A. and Hirst, G. (2003).
?Testing the efficacy ofpart-of-speech information in word completion?,Proceedings of the Workshop on Language Modelingfor Text Entry Methods on EACL, Budapest.Goodman, J.
(2001): ?A Bit of Progress in LanguageModeling?, Extended Version Microsoft ResearchTechnical Report MSR-TR-2001-72.Jelinek, F. (1990): ?Self-organized Language Models forSpeech Recognition?, In: A. Waibel and K.-F.
Lee(eds.
), Readings in Speech Recognition, MorganKaufman Publishers, pp.
450-506.Kuhn, R. and De Mori, R. (1990).
?A Cache-BasedNatural Language Model for Speech Reproduction?,IEEE Transactions on Pattern Analysis and MachineIntelligence, 12 (6), pp.
570-583.Landauer, T. K., Laham, D., Rehder, B. and Schreiner,M.
E. (1997).
?How well can passage meaning be de-rived without using word order?
A comparison ofLSA and humans?, Proceedings of the 19th annualmeeting of the Cognitive Science Society, pp.
412-417, Erlbaum Mawhwah, NJ.Lesher, G. W., Moulton, B. J, Higginbotham, D.J.
andAlsofrom, B.
(2002).
?Limits of human word predic-tion performance?, Proceedings of the CSUN 2002.Li, J., Hirst, G. (2005).
?Semantic knowledge in a wordcompletion task?, Proc.
of the 7th Int.
ACM Confer-ence on Computers and Accessibility, Baltimore.Matiasek, H. and Baroni, M. (2003).
?Exploiting longdistance collocational relations in predictive typing?,Proceedings of the EACL-03 Workshop on LanguageModeling for Text Entry Methods, Budapest.Rosenfeld, R. (1996).
?A maximum entropy approach toadaptive statistical language modelling?, ComputerSpeech and Language, 10 (1), pp.
187-228.Schadle I., Antoine J.-Y., Le P?v?dic B., Poirier F.(2004).
?Sibyl - AAC system using NLP tech-niques?.
Proc.
ICCHP?2004, Paris, France.
LNCS3118, Springer Verlag.Stolcke, A.
(1998): ?Entropy-based pruning of backofflanguage models?.
Proc.s of the DARPA BroadcastNews Transcription and Understanding Workshop.Stolcke, A.
(2002): ?SRILM - An Extensible LanguageModeling Toolkit?, in Proc.
of the Intl.
Conferenceon Spoken Language Processing, Denver, Colorado.Trnka, K., Yarrington, D., McCoy, K. F. and Penning-ton, C. (2006): ?Topic Modeling in Fringe Word Pre-diction for AAC?, In Proceedings of the 2006 Inter-national Conference on Intelligent User Interfaces,pp.
276 ?
278, Sydney, Australia.Trost, H., Matiasek, J. and Baroni, M. (2005): ?TheLanguage Component of the FASTY Text PredictionSystem?, Applied Artificial Intelligence, 19 (8), pp.743-781.Wandmacher, T. (2005): ?How semantic is Latent Se-mantic Analysis?
?, in Proceedings ofTALN/RECITAL 2005, Dourdan, France, 6-10 june.513
