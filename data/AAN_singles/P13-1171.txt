Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1744?1753,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsQuestion Answering Using Enhanced Lexical Semantic ModelsWen-tau Yih Ming-Wei Chang Christopher Meek Andrzej PastusiakMicrosoft ResearchRedmond, WA 98052, USA{scottyih,minchang,meek,andrzejp}@microsoft.comAbstractIn this paper, we study the answersentence selection problem for ques-tion answering.
Unlike previous work,which primarily leverages syntactic analy-sis through dependency tree matching, wefocus on improving the performance us-ing models of lexical semantic resources.Experiments show that our systems canbe consistently and significantly improvedwith rich lexical semantic information, re-gardless of the choice of learning algo-rithms.
When evaluated on a bench-mark dataset, the MAP and MRR scoresare increased by 8 to 10 points, com-pared to one of our baseline systems usingonly surface-form matching.
Moreover,our best system also outperforms perviouswork that makes use of the dependencytree structure by a wide margin.1 IntroductionOpen-domain question answering (QA), whichfulfills a user?s information need by outputting di-rect answers to natural language queries, is a chal-lenging but important problem (Etzioni, 2011).State-of-the-art QA systems often implement acomplicated pipeline architecture, consisting ofquestion analysis, document or passage retrieval,answer selection and verification (Ferrucci, 2012;Moldovan et al, 2003).
In this paper, we focuson one of the key subtasks ?
answer sentence se-lection.
Given a question and a set of candidatesentences, the task is to choose the correct sen-tence that contains the exact answer and can suf-ficiently support the answer choice.
For instance,although both of the following sentences containthe answer ?Jack Lemmon?
to the question ?Whowon the best actor Oscar in 1973??
only the firstsentence is correct.A1: Jack Lemmon won the Academy Award forBest Actor for Save the Tiger (1973).A2: Oscar winner Kevin Spacey said that JackLemmon is remembered as always makingtime for other people.One of the benefits of answer sentence selec-tion is that the output can be provided directly tothe user.
Instead of outputting only the answer, re-turning the whole sentence often adds more valueas the user can easily verify the correctness with-out reading a lengthy document.Answer sentence selection can be naturally re-duced to a semantic text matching problem.
Con-ceptually, we would like to measure how closethe question and sentence can be matched seman-tically.
Due to the variety of word choices andinherent ambiguities in natural languages, bag-of-words approaches with simple surface-form wordmatching tend to produce brittle results with poorprediction accuracy (Bilotti et al, 2007).
As aresult, researchers put more emphasis on exploit-ing both the syntactic and semantic structure inquestions/sentences.
Representative examples in-clude methods based on deeper semantic anal-ysis (Shen and Lapata, 2007; Moldovan et al,2007) and on tree edit-distance (Punyakanok etal., 2004; Heilman and Smith, 2010) and quasi-synchronous grammar (Wang et al, 2007) thatmatch the dependency parse trees of questions andsentences.
However, such approaches often re-quire more computational resources.
In additionto applying a syntactic or semantic parser duringrun-time, finding the best matching between struc-tured representations of sentences is not trivial.For example, the computational complexity of treematching is O(V 2L4), where V is the number ofnodes and L is the maximum depth (Tai, 1979).Instead of focusing on the high-level seman-tic representation, we turn our attention in thiswork to improving the shallow semantic compo-1744nent, lexical semantics.
We formulate answer se-lection as a semantic matching problem with a la-tent word-alignment structure as in (Chang et al,2010) and conduct a series of experimental stud-ies on leveraging recently proposed lexical seman-tic models.
Our main contributions in this workare two key findings.
First, by incorporating theabundant information from a variety of lexical se-mantic models, the answer selection system canbe enhanced substantially, regardless of the choiceof learning algorithms and settings.
Compared tothe previous work, our latent alignment model im-proves the result on a benchmark dataset by a widemargin ?
the mean average precision (MAP) andmean reciprocal rank (MRR) scores are increasedby 25.6% and 18.8%, respectively.
Second, whilethe latent alignment model performs better thanunstructured models, the difference diminishes af-ter adding the enhanced lexical semantics infor-mation.
This may suggest that compared to in-troducing complex structured constraints, incorpo-rating shallow semantic information is both moreeffective and computationally inexpensive in im-proving the performance, at least for the specificword alignment model tested in this work.The rest of the paper is structured as follows.We first survey the related work in Sec.
2.
Sec.
3defines the problem of answer sentence selection,along with the high-level description of our solu-tion.
The enhanced lexical semantic models andthe learning frameworks we explore are presentedin Sec.
4 and Sec.
5, respectively.
Our experimen-tal results on a benchmark QA dataset is shown inSec.
6.
Finally, Sec.
7 concludes the paper.2 Related WorkWhile the task of question answering has a longhistory dated back to the dawn of artificial in-telligence, early systems like STUDENT (Wino-grad, 1977) and LUNAR (Woods, 1973) are typ-ically designed to demonstrate natural languageunderstanding for a small and specific domain.The Text REtrieval Conference (TREC) QuestionAnswering Track was arguably the first large-scale evaluation of open-domain question answer-ing (Voorhees and Tice, 2000).
The task is de-signed in an information retrieval oriented setting.Given a factoid question along with a collectionof documents, a system is required to return theexact answer, along with the document that sup-ports the answer.
In contrast, the Jeopardy!
TVquiz show provides another open-domain questionanswering setting, in which IBM?s Watson systemfamously beat the two highest ranked players (Fer-rucci, 2012).
Questions in this game are presentedin a statement form and the system needs to iden-tify the true question and to give the exact answer.A short sentence or paragraph to justify the answeris not required in either TREC-QA or Jeopardy!As any QA system can virtually be decomposedinto two major high-level components, retrievaland selection (Echihabi and Marcu, 2003), the an-swer selection problem is clearly critical.
Limitingthe scope of an answer to a sentence is first high-lighted by Wang et al (2007), who argued that itwas more informative to present the whole sen-tence instead of a short answer to users.Observing the limitations of the bag-of-wordsmodels, Wang et al (2007) proposed a syntax-driven approach, where each pair of question andsentence are matched by their dependency trees.The mapping is learned by a generative probabilis-tic model based on a Quasi-synchronous Gram-mar formulation (Smith and Eisner, 2006).
Thisapproach was later improved by Wang and Man-ning (2010) with a tree-edit CRF model that learnsthe latent alignment structure.
In contrast, gen-eral tree matching methods based on tree-edit dis-tance have been first proposed by Punyakanok etal.
(2004) for a similar answer selection task.
Heil-man and Smith (2010) proposed a discriminativeapproach that first computes a tree kernel func-tion between the dependency trees of the questionand candidate sentence, and then learns a classifierbased on the tree-edit features extracted.Although lexical semantic information derivedfrom WordNet has been used in some of theseapproaches, the research has mainly focusedon modeling the mapping between the syntac-tic structures of questions and sentences, pro-duced from syntactic analysis.
The potential im-provement from enhanced lexical semantic mod-els seems to have been deliberately overlooked.13 Problem DefinitionWe consider the answer selection problem in asupervised learning setting.
For a set of ques-tions {q1, ?
?
?
, qm}, each question qi is associatedwith a list of labeled candidate answer sentences1For example, Heilman and Smith (2010) emphasized that?The tree edit model, which does not use lexical semanticsknowledge, produced the best result reported to date.
?1745What is the fastest car in the world?The Jaguar XJ220 is the dearest, fastest and most sought after car on the planet.Figure 1: An example pair of question and answer sentence, adapted from (Harabagiu and Moldovan,2001).
Words connected by solid lines are clear synonyms or hyponym/hypernym; words with weakersemantic association are linked by dashed lines.
{(yi1 , si1), (yi1 , si2), ?
?
?
, (yin , sin)}, where yij =1 indicates that sentence sij is a correct answer toquestion qi, and 0 otherwise.
Using this labeleddata, our goal is to learn a probabilistic classifierto predict the label of a new, unseen pair of ques-tion and sentence.Fundamentally, what the classifier predicts iswhether the sentence ?matches?
the question se-mantically.
In other words, does s have the an-swer that satisfies the semantic constraints pro-vided in the question?
Without representing thequestion and sentence in logic or syntactic trees,we take a word-alignment view for solving thisproblem.
We assume that there is an underly-ing structure h that describes how q and s canbe associated through the relations of the wordsin them.
Figure 1 illustrates this setting using arevised example from (Harabagiu and Moldovan,2001).
In this figure, words connected by solidlines are clear synonyms or hyponym/hypernym;words connected by dashed lines indicate that theyare weakly related.
With this alignment structure,features like the degree of mapping or whether allthe content words in the question can be mappedto some words in the sentence can be extracted andhelp improve the classifier.
Notice that the struc-ture representation in terms of word-alignment isfairly general.
For instance, if we assume a naivecomplete bipartite matching, then effectively it re-duces to the simple bag-of-words model.Typically, the ?ideal?
alignment structure is notavailable in the data, and previous work exploitedmostly syntactic analysis (e.g., dependency trees)to reveal the latent mapping structure.
In thiswork, we focus our study on leveraging the low-level semantic cues from recently proposed lexicalsemantic models.
As will be shown in our experi-ments, such information not only improves a latentstructure learning method, but also makes a simplebipartite matching approach extremely strong.24 Lexical Semantic ModelsIn this section, we introduce the lexical seman-tic models we adopt for solving the semanticmatching problem in answer selection.
To go be-yond the simple, limited surface-form matching,we aim to pair words that are semantically re-lated, specifically measured by models of wordrelations including synonymy/antonymy, hyper-nymy/hyponymy (the Is-A relation) and general se-mantic word similarity.4.1 Synonymy and AntonymyAmong all the word relations, synonymy is per-haps the most basic one and needs to be handledreliably.
Although sets of synonyms can be eas-ily found in thesauri or WordNet synsets, suchresources typically cover only strict synonyms.When comparing two words, it is more useful toestimate the degree of synonymy as well.
For in-stance, ship and boat are not strict synonyms be-cause a ship is usually viewed as a large boat.Knowing that two words are somewhat synony-mous could be valuable in determining whetherthey should be mapped.In order to estimate the degree of synonymy, weleverage a recently proposed polarity-inducing la-tent semantic analysis (PILSA) model (Yih et al,2012).
Given a thesaurus, the model first con-structs a signed d-by-n co-occurrence matrix W ,where d is the number of word groups and n isthe size of the vocabulary.
Each row consists of a2Proposed by an anonymous reviewer, one justification ofthis word-alignment approach, where syntactic analysis playsa less important role, is that there are often few sensible com-binations of words.
For instance, knowing only the set ofwords {?car?, ?fastest?, ?world?
}, one may still guess cor-rectly the question ?What is the fastest car in the world?
?1746group of synonyms and antonyms of a particularsense and each column represents a unique word.Values of the elements in each row vector are theTFIDF values of the corresponding words in thisgroup.
The notion of polarity is then induced bymaking the values of words in the antonym groupsnegative, and the matrix is generalized by a low-rank approximation derived by singular-value de-composition (SVD) in the end.
This design has anintriguing property ?
if the cosine score of two col-umn vectors are positive, then the two correspond-ing words tend to be synonymous; if it?s negative,then the two words are antonymous.
The degree ismeasured by the absolute value.Following the setting described in (Yih et al,2012), we construct a PILSA model based on theEncarta thesaurus and enhance it with a discrimi-native projection matrix training method.
The es-timated degrees of both synonymy and antonymyare used our experiments.34.2 Hypernymy and HyponymyThe Class-Inclusion or Is-A relation is commonlyobserved between words in questions and answersentences.
For example, to correctly answer thequestion ?What color is Saturn?
?, it is crucial thatthe selected sentence mentions a specific kind ofcolor, as in ?Saturn is a giant gas planet withbrown and beige clouds.?
Another example is?Who wrote Moonlight Sonata?
?, where composein ?Ludwig van Beethoven composed the Moon-light Sonata in 1801.?
is one kind of write.Traditionally, WordNet taxonomy is the linguis-tic resource for identifying hypernyms and hy-ponyms, applied broadly to many NLP problems.However, WordNet has a number of well-knownlimitations including its rather limited or skewedconcept distribution and the lack of the coverageof the Is-A relation (Song et al, 2011).
For in-stance, when a word refers to a named entity, theparticular sense and meaning is often not encoded.As a result, relations such as ?Apple?
is-a ?com-pany?
and ?Jaguar?
is-a ?car?
cannot be found inWordNet.
Similar to the case in synonymy, theIs-A relation defined in WordNet does not providea native, real-valued degree of the relation, whichcan only be roughly approximated using the num-ber of links on the taxonomy path connecting two3Mapping two antonyms may be desired if one of them isin the scope of negation (Morante and Blanco, 2012; Blancoand Moldovan, 2011).
However, we do not attempt to resolvethe negation scope in this work.concepts (Resnik, 1995).In order to remedy these issues, we aug-ment WordNet with the Is-A relations found inProbase (Wu et al, 2012).
Probase is a knowledgebase that establishes connections between 2.7 mil-lion concepts, discovered automatically by apply-ing Hearst patterns (Hearst, 1992) to 1.68 billionWeb pages.
Its abundant concept coverage dis-tinguishes it from other knowledge bases, such asFreebase (Bollacker et al, 2008) and WikiTaxon-omy (Ponzetto and Strube, 2007).
Based on thefrequency of term co-occurrences, each Is-A rela-tion from Probase is associated with a probabilityvalue, indicating the degree of the relation.We verified the quality of Probase Is-A relationsusing a recently proposed SemEval task of rela-tional similarity (Jurgens et al, 2012) in a com-panion paper (Zhila et al, 2013), where a subsetof the data is to measure the degree of two wordshaving a class-inclusion relation.
Probase?s pre-diction correlates well with the human annotationsand achieves a high Spearman?s rank correlationcoefficient score, ?
= 0.619.
In comparison, theprevious best system (Rink and Harabagiu, 2012)in the task only reaches ?
= 0.233.
These appeal-ing qualities make Probase a robust lexical seman-tic model for hypernymy/hyponymy.4.3 Semantic Word SimilarityThe third lexical semantic model we introduce tar-gets a general notion of word similarity.
Unlikesynonymy and hyponymy, word similarity is onlyloosely defined when two words can be associatedby some implicit relation.4 The general word sim-ilarity model can be viewed as a ?back-off?
so-lution when the exact lexical relation (e.g., part-whole and attribute) is not available or cannot beaccurately detected.Among various word similarity models (Agirreet al, 2009; Reisinger and Mooney, 2010;Gabrilovich and Markovitch, 2007; Radinsky etal., 2011), the vector space models (VSMs) basedon the idea of distributional similarity (Turneyand Pantel, 2010) are often used as the core com-ponent.
Inspired by (Yih and Qazvinian, 2012),which argues the importance of incorporating het-erogeneous vector space models for measuringword similarity, we leverage three different VSMsin this work: Wiki term-vectors, recurrent neural4Instead of making the distinction, word similarity hererefers to the larger set of relations commonly covered by wordrelatedness (Budanitsky and Hirst, 2006).1747network language model (RNNLM) and a conceptvector space model learned from click-throughdata.
Semantic word similarity is estimated usingthe cosine score of the corresponding word vectorsin these VSMs.Contextual term-vectors created using theWikipedia corpus have shown to perform wellon measuring word similarity (Reisinger andMooney, 2010).
Following the setting suggestedby Yih and Qazvinian (2012), we create term-vectors representing about 1 million words by ag-gregating terms within a window of [?10, 10] ofeach occurrence of the target word.
The vectorsare further refined by applying the same vocabu-lary and feature pruning techniques.A recurrent neural network languagemodel (Mikolov et al, 2010) aims to esti-mate the probability of observing a word given itspreceding context.
However, one by-product ofthis model is the word embedding learned in itshidden-layer, which can be viewed as capturingthe word meaning in some latent, conceptualspace.
As a result, vectors of related words tendto be close to each other.
For this word similaritymodel, we take a 640-dimensional version ofRNNLM vectors, which is trained using theBroadcast News corpus of 320M words.5The final word relatedness model is a projec-tion model learned from the click-through data ofa commercial search engine (Gao et al, 2011).Unlike the previous two models, which are cre-ated or trained using a text corpus, the input forthis model is pairs of aggregated queries and ti-tles of pages users click.
This parallel data isused to train a projection matrix for creating themapping between words in queries and documentsbased on user feedback, using a Siamese neuralnetwork (Yih et al, 2011).
Each row vector ofthis matrix is the dense vector representation ofthe corresponding word in the vocabulary.
Perhapsdue to its unique information source, we found thisparticular word embedding seems to complementthe other two VSMs and tends to improve the wordsimilarity measure in general.5 Learning QA Matching ModelsIn this section, we investigate the effectiveness ofvarious learning models for matching questionsand sentences, including the bag-of-words setting5http://www.fit.vutbr.cz/?imikolov/rnnlm/and the framework of learning latent structures.5.1 Bag-of-Words ModelThe bag-of-words model treats each question andsentence as an unstructured bag of words.
Whencomparing a question with a sentence, the modelfirst matches each word in the question to eachword in the sentence.
It then aggregates featuresextracted from each of these word pairs to rep-resent the whole question/sentence pair.
A bi-nary classifier can be trained easily using any ma-chine learning algorithm in this standard super-vised learning setting.Formally, let x = (q, s) be a pair of question qand sentence s. Let Vq = {wq1 , wq2 , ?
?
?
, wqm}and Vs = {ws1 , ws2 , ?
?
?
, wsn} be the sets ofwords in q and s, respectively.
Given a word pair(wq, ws), where wq ?
Vq and ws ?
Vs, featurefunctions ?1, ?
?
?
, ?d map it to a d-dimensionalreal-valued feature vector.We consider two aggregate functions for defin-ing the feature vectors of the whole ques-tion/answer pair: average and max.
?avgj (q, s) =1mn?wq?Vqws?Vs?j(wq, ws) (1)?maxj (q, s) = maxwq?Vqws?Vs?j(wq, ws) (2)Together, each question/sentence pair is repre-sented by a 2d-dimensional feature vector.We tested two learning algorithms in this set-ting: logistic regression and boosted decisiontrees (Friedman, 2001).
The former is the log-linear model widely used in the NLP communityand the latter is a robust non-linear learning algo-rithm that has shown great empirical performance.The bag-of-words model does not require an ad-ditional inference stage as in structured learning,which may be computationally expensive.
Nev-ertheless, its lack of structure information couldlimit the expressiveness of the model and make itdifficult to capture more sophisticated semanticsin the sentences.
To address this concern, we in-vestigate models of learning latent structures next.5.2 Learning Latent StructuresOne obvious issue of the bag-of-words model isthat words in the unrelated part of the sentencemay still be paired with words in the question,which introduces noise to the final feature vector.1748This is observed in many question/sentence pairs,such as the one below.Q: Which was the first movie that James Deanwas in?A: James Dean, who began as an actor on TVdramas, didn?t make his screen debut until1951?s ?Fixed Bayonet.
?While this sentence correctly answers the ques-tion, the fact that James Dean began as a TVactor is unrelated to the question.
As a result,an ?ideal?
word alignment structure should notlink words in this clause to those in the ques-tion.
In order to leverage the latent structured in-formation, we adapt a recently proposed frame-work of learning constrained latent representa-tions (LCLR) (Chang et al, 2010).
LCLR can beviewed as a variant of Latent-SVM (Felzenszwalbet al, 2009) with different learning formulationsand a general inference framework.
The idea ofLCLR is to replace the decision function of a stan-dard linear model ?T?
(x) witharg maxh?T?
(x, h), (3)where ?
represents the weight vector and h repre-sents the latent variables.In this answer selection task, x = (q, s) rep-resents a pair of question q and candidate sen-tence s. As described in Sec.
3, h refers to thelatent alignment between q and s. The intuitionbehinds Eq.
(3) is: candidate sentence s correctlyanswers question q if and only if the decision canbe supported by the best alignment h.The objective function of LCLR is defined as:min?12 ||?||2 + C?i?2is.t.
?i ?
1?
yi maxh ?T?
(x, h)Note that the alignment is latent, so LCLR usesthe binary labels in the training data as feedbackto find the alignment for each example.The computational difficulty of the inferenceproblem (Eq.
(3)) largely depends on the con-straints we enforce in the alignment.
Complicatedconstraints may result in a difficult inference prob-lem, which can be solved by integer linear pro-gramming (Roth and Yih, 2007).
In this work,we considered several sets of constraints for thealignment task, including a two-layer phrase/wordalignment structure, but found that they generallyperformed the same.
Therefore, we chose themany-to-one alignment6, where inference can besolved exactly using a simple greedy algorithm.6 ExperimentsWe present our experimental results in this sec-tion by first introducing the data and evaluationmetrics, followed by the results of existing sys-tems and some baseline methods.
We then showthe positive impact of adding information of wordrelations from various lexical semantics models,with some discussion on the limitation of theword-matching approach.6.1 Data & Evaluation MetricsThe answer selection dataset we used was orig-inally created by Wang et al (2007) based onthe QA track of past Text REtrieval Confer-ences (TREC-QA).
Questions in this dataset areshort factoid questions, such as ?What is Crips?gang color??
In average, each question is associ-ated with approximately 33 answer candidate sen-tences.
A pair of question and sentence is judgedpositive if the sentence contains the exact answerkey and can provide sufficient context as support-ing evidence.The training set of the data contains manu-ally labeled 5,919 question/sentence pairs fromTREC 8-12.
The development and testing setsare both from TREC 13, which contain 1,374and 1,866 pairs, respectively.
The task is treated asa sentence ranking problem for each question andthus evaluated in Mean Average Precision (MAP)and Mean Reciprocal Rank (MRR), using the offi-cial TREC evaluation program.
Following (Wanget al, 2007), candidate sentences with more than40 words are removed from evaluation, as well asquestions with only positive or negative candidatesentences.6.2 Baseline MethodsSeveral systems have been proposed and testedusing this dataset.
Wang et al (2007) pre-sented a generative probabilistic model based ona Quasi-synchronous Grammar formulation andwas later improved by Wang and Manning (2010)with a tree-edit CRF model that learns the la-tent alignment structure.
In contrast, Heilman and6Each word in the question needs to be linked to a wordin the sentence.
Each word in the sentence can be linked tozero or multiple words in the question.1749System MAP MRRWang et al (2007) 0.6029 0.6852Heilman and Smith (2010) 0.6091 0.6917Wang and Manning (2010) 0.5951 0.6951Table 1: Test set results of existing methods, takenfrom Table 3 of (Wang and Manning, 2010).Dev TestBaseline MAP MRR MAP MRRRandom 0.5243 0.5816 0.4708 0.5286Word Cnt 0.6516 0.7216 0.6263 0.6822Wgt Word Cnt 0.7112 0.7880 0.6531 0.7071Table 2: Results of three baseline methods.Smith (2010) proposed a discriminative approachthat first computes a tree kernel function betweenthe dependency trees of the question and candidatesentence, and then learns a classifier based on thetree-edit features extracted.
Table 1 summarizestheir results on the test set.
All these systems in-corporated lexical semantics features derived fromWordNet and named entity features.In order to further estimate the difficulty ofthis task and dataset, we tested three simple base-lines.
The first is random scoring, which sim-ply assigns a random score to each candidate sen-tence.
The second one, word count, is to counthow many words in the question that also occur inthe answer sentence, after removing stopwords7,and lowering the case.
Finally, the last base-line method, weighted word count, is basically thesame as identical word matching, but the count isre-weighted using the IDF value of the questionword.
This is similar to the BM25 ranking func-tion (Robertson et al, 1995).
The results of thesethree methods are shown in Table 1.Somewhat surprisingly, we find that word countis fairly strong and performs comparably to previ-ous systems.8 In addition, weighting the questionwords with their IDF values further improves theresults.6.3 Incorporating Rich Lexical SemanticsWe test the effectiveness of adding rich lexicalsemantics information by creating examples ofdifferent feature sets.
As described in Sec.
5,7We used a list of 101 stopwords, including articles, pro-nouns and punctuation.8The finding has been confirmed by the lead authorof (Wang et al, 2007).all the features are based on the properties ofthe pair of a word from the question and aword from the candidate sentence.
Stopwordsare first removed from both questions and sen-tences and all words are lower-cased.
Featuresused in the experiments can be categorized intosix types: identical word matching (I), lemmamatching (L), WordNet (WN), enhanced Lexi-cal Semantics (LS), Named Entity matching (NE)and Answer type checking (Ans).
Inspired bythe weighted word count baseline, all features ex-cept (Ans) are weighted by the IDF value of thequestion word.
In other words, the IDF values helpdecide the importance of word pairs to the model.Staring from the our baseline model, weightedword count, the identical word matching (I) fea-ture checks whether the pair of words are thesame.
Instead of checking the surface form ofthe word, lemma matching (L) verifies whetherthe two words have the same lemma form.
Ar-guably the most common source of word rela-tions, WordNet (WN) provides the primitive fea-tures of whether two words could belong to thesame synset in WordNet, could be antonyms andwhether one is a hypernym of the other.
Alter-natively, the enhanced lexical semantics (LS) fea-tures apply the models described in Sec.
4 to theword pair and use their estimated degree of syn-onymy, antonymy, hyponymy and semantic relat-edness as features.
Named entity matching (NE)checks whether two words are individually partof some named entities with the same type.
Fi-nally, when the question word is the WH-word, wecheck if the paired word belongs to some phrasethat has the correct answer type using simple rules,such as ?Who should link to a word that is part ofa named entity of type Person.?
We created exam-ples in each round of experiments by augmentingthese features in the same order, and observed howadding different information helped improve themodel performance.Three models are included in our study.
Forthe unstructured, bag-of-words setting, we testedlogistic regression (LR) and boosted decisiontrees (BDT).
As mentioned in Sec.
5, the featuresfor the whole question/sentence pair are the aver-age and max of features of all the word pairs.
Forthe structured-output setting, we used the frame-work of learning constrained latent representa-tion (LCLR) and required that each question wordneeded to be mapped to a word in the sentence.1750LR BDT LCLRFeature set MAP MRR MAP MRR MAP MRR1: I 0.6531 0.7071 0.6323 0.6898 0.6629 0.72792: I+L 0.6744 0.7223 0.6496 0.6923 0.6815 0.72703: I+L+WN 0.7039 0.7705 0.6798 0.7450 0.7316 0.79214: I+L+WN+LS 0.7339 0.8107 0.7523 0.8455 0.7626 0.82315: All 0.7374 0.8171 0.7495 0.8450 0.7648 0.8255Table 3: Test results of various models and feature groups.
Logistic regression (LR) and boosted decisiontrees (BDT) are the two unstructured models.
LCLR is the algorithm for learning latent structures.Feature groups are identical word matching (I), lemma matching (L), WordNet (WN) and enhancedLexical Semantics (LS).
All includes these four plus Named Entity matching (NE) and Answer typechecking (Ans).Hyper-parameters are selected using the ones thatachieve the best MAP score on the developmentset.
Results of these models and feature sets arepresented in Table 3.We make two observations from the results.First, while incorporating more information of theword pairs in general helps, it is clear that map-ping words beyond surface-form matching withthe help of WordNet (Line #3 vs. #2) is impor-tant.
Moreover, when richer information fromother lexical semantic models is available, the per-formance can be further improved (Line #4 vs.#3).
Overall, by simply incorporating more in-formation on word relations, we gain approxi-mately 10 points in both MAP and MRR com-pared to surface-form matching (Line #4 vs. #2),consistently across all three models.
However,adding more information like named entity match-ing and answer type verification does not seem tohelp much (Line #5 vs. #4).
Second, while thestructured-output model usually performs betterthan both unstructured models (LCLR vs. LR &BDT), the performance gain diminishes after moreinformation of word pairs is available (e.g., Lines#4 and #5).6.4 Limitation of Word Matching ModelsAlthough we have demonstrated the benefits ofleveraging various lexical semantic models to helpfind the association between words, the problem ofquestion answering is nevertheless far from solvedusing the word-based approach.
Examining theoutput of the LCLR model with all features on thedevelopment set, we found that there were threemain sources of errors, including uncovered or in-accurate entity relations, the lack of robust ques-tion analysis and the need of high-level semanticrepresentation and inference.
While the first twocan be improved by, say, using a better named en-tity tagger, incorporating other knowledge basesand building a question classifier, how to solve thethird problem is tricky.
Below is an example:Q: In what film is Gordon Gekko the main char-acter?A: He received a best actor Oscar in 1987 for hisrole as Gordon Gekko in ?Wall Street?.This is a correct answer sentence because ?win-ning a best actor Oscar?
implies that the role Gor-don Gekko is the main character.
It is hard to be-lieve that a pure word-matching model would beable to solve this type of ?inferential question an-swering?
problem.7 ConclusionsIn this paper, we present an experimental studyon solving the answer selection problem using en-hanced lexical semantic models.
Following theword-alignment paradigm, we find that the richlexical semantic information improves the modelsconsistently in the unstructured bag-of-words set-ting and also in the framework of learning latentstructures.
Another interesting finding we haveis that while the latent structured model, LCLR,performs better than the other two unstructuredmodels, the difference diminishes after more in-formation, including the enhanced lexical seman-tic knowledge and answer type verification, hasbeen incorporated.
This may suggest that addingshallow semantic information is more effectivethan introducing complex structured constraints,at least for the specific word alignment model weexperimented with in this work.1751In the future, we plan to explore several di-rections.
First, although we focus on improv-ing TREC-style open-domain question answeringin this work, we would like to apply the pro-posed technology to other QA scenarios, suchas community-based QA (CQA).
For instance,the sentence matching technique can help map agiven question to some questions in an existingCQA database (e.g., Yahoo!
Answers).
More-over, the answer sentence selection scheme couldalso be useful in extracting the most related sen-tences from the answer text to form a summaryanswer.
Second, because the task of answer sen-tence selection is very similar to paraphrase de-tection (Dolan et al, 2004) and recognizing tex-tual entailment (Dagan et al, 2006), we would liketo investigate whether systems for these tasks canbe improved by incorporating enhanced lexical se-mantic knowledge as well.
Finally, we would liketo improve our system for the answer sentence se-lection task and for question answering in general.In addition to following the directions suggestedby the error analysis presented in Sec.
6.4, we planto use logic-like semantic representations of ques-tions and sentences, and explore the role of lexicalsemantics for handling questions that require in-ference.AcknowledgmentsWe are grateful to Mengqiu Wang for providingthe dataset and helping clarify some issues in theexperiments.
We also thank Chris Burges and Hoi-fung Poon for valuable discussion and the anony-mous reviewers for their useful comments.ReferencesE.
Agirre, E. Alfonseca, K. Hall, J. Kravalova,M.
Pas?ca and A. Soroa.
2009.
A study on similarityand relatedness using distributional and WordNet-based approaches.
In Proceedings of NAACL, pages19?27.M.
Bilotti, P. Ogilvie, J. Callan, and E. Nyberg.
2007.Structured retrieval for question answering.
In Pro-ceedings of SIGIR, pages 351?358.E.
Blanco and D. Moldovan.
2011.
Semantic repre-sentation of negation using focus detection.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies (ACL-HLT 2011).K.
Bollacker, C. Evans, P. Paritosh, T. Sturge, andJ.
Taylor.
2008.
Freebase: a collaboratively cre-ated graph database for structuring human knowl-edge.
In ACM Conference on Management of Data(SIGMOD), pages 1247?1250.A.
Budanitsky and G. Hirst.
2006.
EvaluatingWordNet-based measures of lexical semantic re-latedness.
Computational Linguistics, 32:13?47,March.M.
Chang, D. Goldwasser, D. Roth, and V. Srikumar.2010.
Discriminative learning over constrained la-tent representations.
In Proceedings of NAACL.I.
Dagan, O. Glickman, and B. Magnini, editors.
2006.The PASCAL Recognising Textual Entailment Chal-lenge, volume 3944.
Springer-Verlag, Berlin.W.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsu-pervised construction of large paraphrase corpora:Exploiting massively parallel news sources.
In Pro-ceedings of COLING.A.
Echihabi and D. Marcu.
2003.
A noisy-channelapproach to question answering.
In Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 16?23.Oren Etzioni.
2011.
Search needs a shake-up.
Nature,476(7358):25?26.P.
Felzenszwalb, R. Girshick, D. McAllester, andD.
Ramanan.
2009.
Object detection with discrim-inatively trained part based models.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,99(1).D.
Ferrucci.
2012.
Introduction to ?This is Wat-son?.
IBM Journal of Research and Development,56(3.4):1?1.J.
Friedman.
2001.
Greedy function approximation:a gradient boosting machine.
Annals of Statistics,29(5):1189?1232.E.
Gabrilovich and S. Markovitch.
2007.
Computingsemantic relatedness using Wikipedia-based explicitsemantic analysis.
In AAAI Conference on ArtificialIntelligence (AAAI).J.
Gao, K. Toutanova, and W. Yih.
2011.Clickthrough-based latent semantic models for websearch.
In Proceedings of SIGIR, pages 675?684.S.
Harabagiu and D. Moldovan.
2001.
Open-domaintextual question answering.
Tutorial of NAACL-2001.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of COLING,pages 539?545.M.
Heilman and N. Smith.
2010.
Tree edit models forrecognizing textual entailments, paraphrases, andanswers to questions.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 1011?1019.1752D.
Jurgens, S. Mohammad, P. Turney, and K. Holyoak.2012.
SemEval-2012 Task 2: Measuring degrees ofrelational similarity.
In Proceedings of the Sixth In-ternational Workshop on Semantic Evaluation (Se-mEval 2012), pages 356?364.T.
Mikolov, M. Karafia?t, L. Burget, J.
Cernocky?, andS.
Khudanpur.
2010.
Recurrent neural networkbased language model.
In Annual Conference ofthe International Speech Communication Associa-tion (INTERSPEECH), pages 1045?1048.D.
Moldovan, M. Pas?ca, S. Harabagiu, and M. Sur-deanu.
2003.
Performance issues and error analy-sis in an open-domain question answering system.ACM Transactions on Information Systems (TOIS),21(2):133?154.D.
Moldovan, C. Clark, S. Harabagiu, and D. Hodges.2007.
COGEX: A semantically and contextually en-riched logic prover for question answering.
Journalof Applied Logic, 5(1):49?69.R.
Morante and E. Blanco.
2012.
*SEM 2012 sharedtask: Resolving the scope and focus of negation.
InProceedings of the First Joint Conference on Lexicaland Computational Semantics, pages 265?274.S.
Ponzetto and M. Strube.
2007.
Deriving a largescale taxonomy from wikipedia.
In AAAI Confer-ence on Artificial Intelligence (AAAI).V.
Punyakanok, D. Roth, and W. Yih.
2004.
Mappingdependencies trees: An application to question an-swering.
In International Symposium on ArtificialIntelligence and Mathematics (AI & Math).K.
Radinsky, E. Agichtein, E. Gabrilovich, andS.
Markovitch.
2011.
A word at a time: computingword relatedness using temporal semantic analysis.In WWW ?11, pages 337?346.J.
Reisinger and R. Mooney.
2010.
Multi-prototypevector-space models of word meaning.
In Proceed-ings of NAACL.P.
Resnik.
1995.
Using information content to evaluatesemantic similarity in a taxonomy.
In InternationalJoint Conference on Artificial Intelligence (IJCAI).B.
Rink and S. Harabagiu.
2012.
UTD: Determiningrelational similarity using lexical patterns.
In Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation (SemEval 2012), pages 413?418.S.
Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford.
1995.
Okapi at TREC-3.In Text REtrieval Conference (TREC), pages 109?109.D.
Roth and W. Yih.
2007.
Global inference for entityand relation identification via a linear programmingformulation.
In Lise Getoor and Ben Taskar, editors,Introduction to Statistical Relational Learning.
MITPress.D.
Shen and M. Lapata.
2007.
Using semantic rolesto improve question answering.
In Proceedings ofEMNLP-CoNLL, pages 12?21.D.
Smith and J. Eisner.
2006.
Quasi-synchronousgrammars: Alignment by soft projection of syntacticdependencies.
In Proceedings of the HLT-NAACLWorkshop on Statistical Machine Translation, pages23?30.Y.
Song, H. Wang, Z. Wang, H. Li, and W. Chen.
2011.Short text conceptualization using a probabilisticknowledgebase.
In International Joint Conferenceon Artificial Intelligence (IJCAI), pages 2330?2336.K.
Tai.
1979.
The tree-to-tree correction problem.
J.ACM, 26(3):422?433, July.P.
Turney and P. Pantel.
2010.
From frequency tomeaning: Vector space models of semantics.
Jour-nal of Artificial Intelligence Research, 37(1):141?188.E.
Voorhees and D. Tice.
2000.
Building a questionanswering test collection.
In Proceedings of SIGIR,pages 200?207.M.
Wang and C. Manning.
2010.
Probabilistic tree-edit models with structured latent variables for tex-tual entailment and question answering.
In Proceed-ings of COLING.M.
Wang, N. Smith, and T. Mitamura.
2007.
What isthe Jeopardy model?
A quasi-synchronous grammarfor QA.
In Proceedings of EMNLP-CoNLL.T.
Winograd.
1977.
Five lectures on artificial intelli-gence.
In A. Zampolli, editor, Linguistic StructuresProcessing, pages 399?520.
North Holland.W.
Woods.
1973.
Progress in natural language under-standing: An application to lunar geology.
In Pro-ceedings of the National Computer Conference andExposition (AFIPS), pages 441?450.W.
Wu, H. Li, H. Wang, and K. Zhu.
2012.
Probase:a probabilistic taxonomy for text understanding.
InACM Conference on Management of Data (SIG-MOD), pages 481?492.W.
Yih and V. Qazvinian.
2012.
Measuring word relat-edness using heterogeneous vector space models.
InProceedings of NAACL-HLT 2012, pages 616?620.W.
Yih, K. Toutanova, J. Platt, and C. Meek.
2011.Learning discriminative projections for text similar-ity measures.
In ACL Conference on Natural Lan-guage Learning (CoNLL), pages 247?256.W.
Yih, G. Zweig, and J. Platt.
2012.
Polarity inducinglatent semantic analysis.
In Proceedings of EMNLP-CoNLL, pages 1212?1222.A.
Zhila, W. Yih, C. Meek, G. Zweig, and T. Mikolov.2013.
Combining heterogeneous models for mea-suring relational similarity.
In Proceedings of HLT-NAACL.1753
