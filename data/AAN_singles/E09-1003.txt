Proceedings of the 12th Conference of the European Chapter of the ACL, pages 16?23,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsOn the use of Comparable Corpora to improve SMT performanceSadaf Abdul-Rauf and Holger SchwenkLIUM, University of Le Mans, FRANCESadaf.Abdul-Rauf@lium.univ-lemans.frAbstractWe present a simple and effective methodfor extracting parallel sentences fromcomparable corpora.
We employ a sta-tistical machine translation (SMT) systembuilt from small amounts of parallel textsto translate the source side of the non-parallel corpus.
The target side texts areused, along with other corpora, in the lan-guage model of this SMT system.
Wethen use information retrieval techniquesand simple filters to create French/Englishparallel data from a comparable news cor-pora.
We evaluate the quality of the ex-tracted data by showing that it signifi-cantly improves the performance of anSMT systems.1 IntroductionParallel corpora have proved be an indispens-able resource in Statistical Machine Translation(SMT).
A parallel corpus, also called bitext, con-sists in bilingual texts aligned at the sentence level.They have also proved to be useful in a range ofnatural language processing applications like au-tomatic lexical acquisition, cross language infor-mation retrieval and annotation projection.Unfortunately, parallel corpora are a limited re-source, with insufficient coverage of many lan-guage pairs and application domains of inter-est.
The performance of an SMT system heav-ily depends on the parallel corpus used for train-ing.
Generally, more bitexts lead to better per-formance.
Current resources of parallel corporacover few language pairs and mostly come fromone domain (proceedings of the Canadian or Eu-ropean Parliament, or of the United Nations).
Thisbecomes specifically problematic when SMT sys-tems trained on such corpora are used for generaltranslations, as the language jargon heavily used inthese corpora is not appropriate for everyday lifetranslations or translations in some other domain.One option to increase this scarce resourcecould be to produce more human translations, butthis is a very expensive option, in terms of bothtime and money.
In recent work less expensive butvery productive methods of creating such sentencealigned bilingual corpora were proposed.
Theseare based on generating ?parallel?
texts from al-ready available ?almost parallel?
or ?not muchparallel?
texts.
The term ?comparable corpus?
isoften used to define such texts.A comparable corpus is a collection of textscomposed independently in the respective lan-guages and combined on the basis of similarityof content (Yang and Li, 2003).
The raw mate-rial for comparable documents is often easy to ob-tain but the alignment of individual documents is achallenging task (Oard, 1997).
Multilingual newsreporting agencies like AFP, Xinghua, Reuters,CNN, BBC etc.
serve to be reliable producersof huge collections of such comparable corpora.Such texts are widely available from LDC, in par-ticular the Gigaword corpora, or over the WEBfor many languages and domains, e.g.
Wikipedia.They often contain many sentences that are rea-sonable translations of each other, thus potentialparallel sentences to be identified and extracted.There has been considerable amount of work onbilingual comparable corpora to learn word trans-lations as well as discovering parallel sentences.Yang and Lee (2003) use an approach based ondynamic programming to identify potential paral-lel sentences in title pairs.
Longest common subsequence, edit operations and match-based scorefunctions are subsequently used to determine con-fidence scores.
Resnik and Smith (2003) pro-pose their STRAND web-mining based systemand show that their approach is able to find largenumbers of similar document pairs.Works aimed at discovering parallel sentences16French: Au total, 1,634 million d?e?lecteurs doivent de?signer les 90 de?pute?s de la prochaine le?gislatureparmi 1.390 candidats pre?sente?s par 17 partis, dont huit sont repre?sente?s au parlement.Query: In total, 1,634 million voters will designate the 90 members of the next parliament among 1.390candidates presented by 17 parties, eight of which are represented in parliament.Result: Some 1.6 million voters were registered to elect the 90 members of the legislature from 1,390candidates from 17 parties, eight of which are represented in parliament, several civilian organisationsand independent lists.French: ?Notre implication en Irak rend possible que d?autres pays membres de l?Otan, commel?Allemagne par exemple, envoient un plus gros contingent?
en Afghanistan, a estime?
M.Belka au coursd?une confe?rence de presse.Query: ?Our involvement in Iraq makes it possible that other countries members of NATO, suchas Germany, for example, send a larger contingent in Afghanistan, ?said Mr.Belka during a pressconference.Result: ?Our involvement in Iraq makes it possible for other NATO members, like Germany forexample, to send troops, to send a bigger contingent to your country, ?Belka said at a press conference,with Afghan President Hamid Karzai.French: De son co?te?, Mme Nicola Duckworth, directrice d?Amnesty International pour l?Europe etl?Asie centrale, a de?clare?
que les ONG demanderaient a` M.Poutine de mettre fin aux violations desdroits de l?Homme dans le Caucase du nord.Query: For its part, Mrs Nicole Duckworth, director of Amnesty International for Europe and CentralAsia, said that NGOs were asking Mr Putin to put an end to human rights violations in the northernCaucasus.Result: Nicola Duckworth, head of Amnesty International?s Europe and Central Asia department, saidthe non-governmental organisations (NGOs) would call on Putin to put an end to human rights abusesin the North Caucasus, including the war-torn province of Chechnya.Figure 1: Some examples of a French source sentence, the SMT translation used as query and the poten-tial parallel sentence as determined by information retrieval.
Bold parts are the extra tails at the end ofthe sentences which we automatically removed.include (Utiyama and Isahara, 2003), who usecross-language information retrieval techniquesand dynamic programming to extract sentencesfrom an English-Japanese comparable corpus.They identify similar article pairs, and then, treat-ing these pairs as parallel texts, align their sen-tences on a sentence pair similarity score and useDP to find the least-cost alignment over the doc-ument pair.
Fung and Cheung (2004) approachthe problem by using a cosine similarity measureto match foreign and English documents.
Theywork on ?very non-parallel corpora?.
They thengenerate all possible sentence pairs and select thebest ones based on a threshold on cosine simi-larity scores.
Using the extracted sentences theylearn a dictionary and iterate over with more sen-tence pairs.
Recent work by Munteanu and Marcu(2005) uses a bilingual lexicon to translate someof the words of the source sentence.
These trans-lations are then used to query the database to findmatching translations using information retrieval(IR) techniques.
Candidate sentences are deter-mined based on word overlap and the decisionwhether a sentence pair is parallel or not is per-formed by a maximum entropy classifier trainedon parallel sentences.
Bootstrapping is used andthe size of the learned bilingual dictionary is in-creased over iterations to get better results.Our technique is similar to that of (Munteanuand Marcu, 2005) but we bypass the need of thebilingual dictionary by using proper SMT transla-tions and instead of a maximum entropy classifierwe use simple measures like the word error rate(WER) and the translation error rate (TER) to de-cide whether sentences are parallel or not.
Usingthe full SMT sentences, we get an added advan-tage of being able to detect one of the major errorsof this technique, also identified by (Munteanu andMarcu, 2005), i.e, the cases where the initial sen-tences are identical but the retrieved sentence has17a tail of extra words at sentence end.
We try tocounter this problem as detailed in 4.1.We apply this technique to create a parallel cor-pus for the French/English language pair using theLDC Gigaword comparable corpus.
We show thatwe achieve significant improvements in the BLEUscore by adding our extracted corpus to the alreadyavailable human-translated corpora.This paper is organized as follows.
In the nextsection we first describe the baseline SMT systemtrained on human-provided translations only.
Wethen proceed by explaining our parallel sentenceselection scheme and the post-processing.
Sec-tion 4 summarizes our experimental results andthe paper concludes with a discussion and perspec-tives of this work.2 Baseline SMT systemThe goal of SMT is to produce a target sentencee from a source sentence f .
Among all possibletarget language sentences the one with the highestprobability is chosen:e?
= arg maxePr(e|f) (1)= arg maxePr(f |e) Pr(e) (2)where Pr(f |e) is the translation model andPr(e) is the target language model (LM).
This ap-proach is usually referred to as the noisy source-channel approach in SMT (Brown et al, 1993).Bilingual corpora are needed to train the transla-tion model and monolingual texts to train the tar-get language model.It is today common practice to use phrases astranslation units (Koehn et al, 2003; Och andNey, 2003) instead of the original word-based ap-proach.
A phrase is defined as a group of sourcewords f?
that should be translated together into agroup of target words e?.
The translation model inphrase-based systems includes the phrase transla-tion probabilities in both directions, i.e.
P (e?|f?
)and P (f?
|e?).
The use of a maximum entropy ap-proach simplifies the introduction of several addi-tional models explaining the translation process :e?
= arg maxPr(e|f)= arg maxe{exp(?i?ihi(e, f))} (3)The feature functions hi are the system mod-els and the ?i weights are typically optimized tomaximize a scoring function on a developmentSMT baselinesystemphrasetable3.3G4?gramLMFr EnautomatictranslationsEnwordswords275Mup toFr Enhuman translationswords116Mup toFigure 2: Using an SMT system used to translatelarge amounts of monolingual data.set (Och and Ney, 2002).
In our system fourteenfeatures functions were used, namely phrase andlexical translation probabilities in both directions,seven features for the lexicalized distortion model,a word and a phrase penalty, and a target languagemodel.The system is based on the Moses SMTtoolkit (Koehn et al, 2007) and constructed as fol-lows.
First, Giza++ is used to perform word align-ments in both directions.
Second, phrases andlexical reorderings are extracted using the defaultsettings of the Moses SMT toolkit.
The 4-gramback-off target LM is trained on the English partof the bitexts and the Gigaword corpus of about3.2 billion words.
Therefore, it is likely that thetarget language model includes at least some ofthe translations of the French Gigaword corpus.We argue that this is a key factor to obtain goodquality translations.
The translation model wastrained on the news-commentary corpus (1.56Mwords)1 and a bilingual dictionary of about 500kentries.2 This system uses only a limited amountof human-translated parallel texts, in comparisonto the bitexts that are available in NIST evalua-tions.
In a different versions of this system, theEuroparl (40M words) and the Canadian Hansardcorpus (72M words) were added.In the framework of the EuroMatrix project, atest set of general news data was provided for theshared translation task of the third workshop on1Available at http://www.statmt.org/wmt08/shared-task.html2The different conjugations of a verb and the singular andplural form of adjectives and nouns are counted as multipleentries.18ENSMTFRused as queriesper day articlescandidate sentence pairs parallelsentences+?5 day articlesfrom English GigawordEnglishtranslations GigawordFrench174M words133M wordstailremovalsentences withextra words at ends+24.3M wordsparallelnumber / tablecomparisonlengthremovingWER/TER26.8M wordsFigure 3: Architecture of the parallel sentence extraction system.SMT (Callison-Burch et al, 2008), called new-stest2008 in the following.
The size of this cor-pus amounts to 2051 lines and about 44 thousandwords.
This data was randomly split into two partsfor development and testing.
Note that only onereference translation is available.
We also noticedseveral spelling errors in the French source texts,mainly missing accents.
These were mostly auto-matically corrected using the Linux spell checker.This increased the BLEU score by about 1 BLEUpoint in comparison to the results reported in theofficial evaluation (Callison-Burch et al, 2008).The system tuned on this development data is usedtranslate large amounts of text of French Gigawordcorpus (see Figure 2).
These translations will bethen used to detect potential parallel sentences inthe English Gigaword corpus.3 System ArchitectureThe general architecture of our parallel sentenceextraction system is shown in figure 3.
Start-ing from comparable corpora for the two lan-guages, French and English, we propose to trans-late French to English using an SMT system as de-scribed above.
These translated texts are then usedto perform information retrieval from the Englishcorpus, followed by simple metrics like WER andTER to filter out good sentence pairs and even-tually generate a parallel corpus.
We show that aparallel corpus obtained using this technique helpsconsiderably to improve an SMT system.We shall also be trying to answer the followingquestion over the course of this study: do we needto use the best possible SMT systems to be able toretrieve the correct parallel sentences or any ordi-nary SMT system will serve the purpose ?3.1 System for Extracting Parallel Sentencesfrom Comparable CorporaLDC provides large collections of texts from mul-tilingual news reporting agencies.
We identifiedagencies that provided news feeds for the lan-guages of our interest and chose AFP for ourstudy.3We start by translating the French AFP texts toEnglish using the SMT systems discussed in sec-tion 2.
In our experiments we considered onlythe most recent texts (2002-2006, 5.5M sentences;about 217M French words).
These translations arethen treated as queries for the IR process.
The de-sign of our sentence extraction process is based onthe heuristic that considering the corpus at hand,we can safely say that a news item reported onday X in the French corpus will be most proba-bly found in the day X-5 and day X+5 time pe-riod.
We experimented with several window sizesand found the window size of ?5 days to be themost accurate in terms of time and the quality ofthe retrieved sentences.Using the ID and date information for each sen-tence of both corpora, we first collect all sentencesfrom the SMT translations corresponding to thesame day (query sentences) and then the corre-sponding articles from the English Gigaword cor-3LDC corpora LDC2007T07 (English) and LDC2006T17(French).19pus (search space for IR).
These day-specific filesare then used for information retrieval using a ro-bust information retrieval system.
The Lemur IRtoolkit (Ogilvie and Callan, 2001) was used forsentence extraction.
The top 5 scoring sentencesare returned by the IR process.
We found no evi-dence that retrieving more than 5 top scoring sen-tences helped get better sentences.
At the end ofthis step, we have for each query sentence 5 po-tentially matching sentences as per the IR score.The information retrieval step is the most timeconsuming task in the whole system.
The timetaken depends upon various factors like size of theindex to search in, length of the query sentenceetc.
To give a time estimate, using a ?5 day win-dow required 9 seconds per query vs 15 secondsper query when a ?7 day window was used.
Thenumber of results retrieved per sentence also hadan impact on retrieval time with 20 results tak-ing 19 seconds per query, whereas 5 results taking9 seconds per query.
Query length also affectedthe speed of the sentence extraction process.
Butwith the problem at we could differentiate amongimportant and unimportant words as nouns, verbsand sometimes even numbers (year, date) could bethe keywords.
We, however did place a limit ofapproximately 90 words on the queries and the in-dexed sentences.
This choice was motivated by thefact that the word alignment toolkit Giza++ doesnot process longer sentences.A Krovetz stemmer was used while building theindex as provided by the toolkit.
English stopwords, i.e.
frequently used words, such as ?a?
or?the?, are normally not indexed because they areso common that they are not useful to query on.The stop word list provided by the IR Group ofUniversity of Glasgow4 was used.The resources required by our system are min-imal : translations of one side of the comparablecorpus.
We will be showing later in section 4.2of this paper that with an SMT system trained onsmall amounts of human-translated data we can?retrieve?
potentially good parallel sentences.3.2 Candidate Sentence Pair SelectionOnce we have the results from information re-trieval, we proceed on to decide whether sentencesare parallel or not.
At this stage we choose thebest scoring sentence as determined by the toolkit4http://ir.dcs.gla.ac.uk/resources/linguistic utils/stop wordsand pass the sentence pair through further filters.Gale and Church (1993) based their align programon the fact that longer sentences in one languagetend to be translated into longer sentences in theother language, and that shorter sentences tend tobe translated into shorter sentences.
We also usethe same logic in our initial selection of the sen-tence pairs.
A sentence pair is selected for fur-ther processing if the length ratio is not more than1.6.
A relaxed factor of 1.6 was chosen keepingin consideration the fact that French sentences arelonger than their respective English translations.Finally, we discarded all sentences that contain alarge fraction of numbers.
Typically, those are ta-bles of sport results that do not carry useful infor-mation to train an SMT.Sentences pairs conforming to the previous cri-teria are then judged based on WER (Levenshteindistance) and translation error rate (TER).
WERmeasures the number of operations required totransform one sentence into the other (insertions,deletions and substitutions).
A zero WER wouldmean the two sentences are identical, subsequentlylower WER sentence pairs would be sharing mostof the common words.
However two correct trans-lations may differ in the order in which the wordsappear, something that WER is incapable of tak-ing into account as it works on word to word ba-sis.
This shortcoming is addressed by TER whichallows block movements of words and thus takesinto account the reorderings of words and phrasesin translation (Snover et al, 2006).
We used bothWER and TER to choose the most suitable sen-tence pairs.4 Experimental evaluationOur main goal was to be able to create an addi-tional parallel corpus to improve machine transla-tion quality, especially for the domains where wehave less or no parallel data available.
In this sec-tion we report the results of adding these extractedparallel sentences to the already available human-translated parallel sentences.We conducted a range of experiments by addingour extracted corpus to various combinations of al-ready available human-translated parallel corpora.We experimented with WER and TER as filters toselect the best scoring sentences.
Generally, sen-tences selected based on TER filter showed betterBLEU and TER scores than their WER counterparts.
So we chose TER filter as standard for2018.51919.52020.52121.5220  2  4  6  8  10  12  14  16BLEUscoreFrench words for training [M]newsbitexts onlyTER filterWERFigure 4: BLEU scores on the Test data using anWER or TER filter.our experiments with limited amounts of humantranslated corpus.
Figure 4 shows this WER vsTER comparison based on BLEU and TER scoreson the test data in function of the size of train-ing data.
These experiments were performed withonly 1.56M words of human-provided translations(news-commentary corpus).4.1 Improvement by sentence tail removalTwo main classes of errors common in suchtasks: firstly, cases where the two sentences sharemany common words but actually convey differ-ent meaning, and secondly, cases where the twosentences are (exactly) parallel except at sentenceends where one sentence has more informationthan the other.
This second case of errors can bedetected using WER as we have both the sentencesin English.
We detected the extra insertions at theend of the IR result sentence and removed them.Some examples of such sentences along with tailsdetected and removed are shown in figure 1.
Thisresulted in an improvement in the SMT scores asshown in table 1.This technique worked perfectly for sentenceshaving TER greater than 30%.
Evidently theseare the sentences which have longer tails whichresult in a lower TER score and removing themimproves performance significantly.
Removingsentence tails evidently improved the scores espe-cially for larger data, for example for the data sizeof 12.5M we see an improvement of 0.65 and 0.98BLEU points on dev and test data respectively and1.00 TER points on test data (last line table 1).The best BLEU score on the development datais obtained when adding 9.4M words of automat-ically aligned bitexts (11M in total).
This corre-Limit Word BLEU BLEU TERTER tail Words Dev Test Testfilter removal (M) data data data0 1.56 19.41 19.53 63.1710 no 1.58 19.62 19.59 63.11yes 19.56 19.51 63.2420 no 1.7 19.76 19.89 62.49yes 19.81 19.75 62.8030 no 2.1 20.29 20.32 62.16yes 20.16 20.22 62.0240 no 3.5 20.93 20.81 61.80yes 21.23 21.04 61.4945 no 4.9 20.98 20.90 62.18yes 21.39 21.49 60.9050 no 6.4 21.12 21.07 61.31yes 21.70 21.70 60.6955 no 7.8 21.30 21.15 61.23yes 21.90 21.78 60.4160 no 9.8 21.42 20.97 61.46yes 21.96 21.79 60.3365 no 11 21.34 21.20 61.02yes 22.29 21.99 60.1070 no 12.2 21.21 20.84 61.24yes 21.86 21.82 60.24Table 1: Effect on BLEU score of removing extrasentence tails from otherwise parallel sentences.sponds to an increase of about 2.88 points BLEUon the development set and an increase of 2.46BLEU points on the test set (19.53 ?
21.99) asshown in table 2, first two lines.
The TER de-creased by 3.07%.Adding the dictionary improves the baselinesystem (second line in Table 2), but it is not nec-essary any more once we have the automaticallyextracted data.Having had very promising results with our pre-vious experiments, we proceeded onto experimen-tation with larger human-translated data sets.
Weadded our extracted corpus to the collection ofNews-commentary (1.56M) and Europarl (40.1M)bitexts.
The corresponding SMT experimentsyield an improvement of about 0.2 BLEU pointson the Dev and Test set respectively (see table 2).4.2 Effect of SMT qualityOur motivation for this approach was to be ableto improve SMT performance by ?creating?
paral-lel texts for domains which do not have enoughor any parallel corpora.
Therefore only the news-21total BLEU score TERBitexts words Dev Test TestNews 1.56M 19.41 19.53 63.17News+Extracted 11M 22.29 21.99 60.10News+dict 2.4M 20.44 20.18 61.16News+dict+Extracted 13.9M 22.40 21.98 60.11News+Eparl+dict 43.3M 22.27 22.35 59.81News+Eparl+dict+Extracted 51.3M 22.47 22.56 59.83Table 2: Summary of BLEU scores for the best systems on the Dev-data with the news-commentarycorpus and the bilingual dictionary.1919.52020.52121.52222.52  4  6  8  10  12  14BLEUscoreFrench words for training [M]news + extractedbitexts onlydevtestFigure 5: BLEU scores when using news-commentary bitexts and our extracted bitexts fil-tered using TER.commentary bitext and the bilingual dictionarywere used to train an SMT system that producedthe queries for information retrieval.
To investi-gate the impact of the SMT quality on our sys-tem, we built another SMT system trained on largeamounts of human-translated corpora (116M), asdetailed in section 2.
Parallel sentence extrac-tion was done using the translations performed bythis big SMT system as IR queries.
We foundno experimental evidence that the improved au-tomatic translations yielded better alignments ofthe comaprable corpus.
It is however interesting tonote that we achieve almost the same performancewhen we add 9.4M words of autoamticallly ex-tracted sentence as with 40M of human-provided(out-of domain) translations (second versus fifthline in Table 2).5 Conclusion and discussionSentence aligned parallel corpora are essential forany SMT system.
The amount of in-domain paral-lel corpus available accounts for the quality of thetranslations.
Not having enough or having no in-domain corpus usually results in bad translationsfor that domain.
This need for parallel corpora,has made the researchers employ new techniquesand methods in an attempt to reduce the dire needof this crucial resource of the SMT systems.
Ourstudy also contributes in this regard by employingan SMT itself and information retrieval techniquesto produce additional parallel corpora from easilyavailable comparable corpora.We use automatic translations of comparablecorpus of one language (source) to find the cor-responding parallel sentence from the comparablecorpus in the other language (target).
We onlyused a limited amount of human-provided bilin-gual resources.
Starting with about a total 2.6Mwords of sentence aligned bilingual data and abilingual dictionary, large amounts of monolin-gual data are translated.
These translations arethen employed to find the corresponding match-ing sentences in the target side corpus, using infor-mation retrieval methods.
Simple filters are usedto determine whether the retrieved sentences areparallel or not.
By adding these retrieved par-allel sentences to already available human trans-lated parallel corpora we were able to improve theBLEU score on the test set by almost 2.5 points.Almost one point BLEU of this improvement wasobtained by removing additional words at the endof the aligned sentences in the target language.Contrary to the previous approaches as in(Munteanu and Marcu, 2005) which used smallamounts of in-domain parallel corpus as an initialresource, our system exploits the target languageside of the comparable corpus to attain the samegoal, thus the comparable corpus itself helps tobetter extract possible parallel sentences.
The Gi-gaword comparable corpora were used in this pa-per, but the same approach can be extended to ex-22tract parallel sentences from huge amounts of cor-pora available on the web by identifying compara-ble articles using techniques such as (Yang and Li,2003) and (Resnik and Y, 2003).This technique is particularly useful for lan-guage pairs for which very little parallel corporaexist.
Other probable sources of comparable cor-pora to be exploited include multilingual ency-clopedias like Wikipedia, encyclopedia Encartaetc.
There also exist domain specific compara-ble corpora (which are probably potentially par-allel), like the documentations that are done in thenational/regional language as well as English, orthe translations of many English research papers inFrench or some other language used for academicproposes.We are currently working on several extensionsof the procedure described in this paper.
We willinvestigate whether the same findings hold forother tasks and language pairs, in particular trans-lating from Arabic to English, and we will try tocompare our approach with the work of Munteanuand Marcu (2005).
The simple filters that we arecurrently using seem to be effective, but we willalso test other criteria than the WER and TER.
Fi-nally, another interesting direction is to iterate theprocess.
The extracted additional bitexts could beused to build an SMT system that is better opti-mized on the Gigaword corpus, to translate againall the sentence from French to English, to per-form IR and the filtering and to extract new, po-tentially improved, parallel texts.
Starting withsome million words of bitexts, this process mayallow to build at the end an SMT system thatachieves the same performance than we obtainedusing about 40M words of human-translated bi-texts (news-commentary + Europarl).6 AcknowledgmentsThis work was partially supported by the HigherEducation Commission, Pakistan through theHEC Overseas Scholarship 2005 and the FrenchGovernment under the project INSTAR (ANRJCJC06 143038).
Some of the baseline SMT sys-tems used in this work were developed in a coop-eration between the University of Le Mans and thecompany SYSTRAN.ReferencesP.
Brown, S. Della Pietra, Vincent J. Della Pietra, andR.
Mercer.
1993.
The mathematics of statisti-cal machine translation.
Computational Linguistics,19(2):263?311.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InThird Workshop on SMT, pages 70?106.Pascale Fung and Percy Cheung.
2004.
Mining very-non-parallel corpora: Parallel sentence and lexiconextraction via bootstrapping and em.
In DekangLin and Dekai Wu, editors, EMNLP, pages 57?63,Barcelona, Spain, July.
Association for Computa-tional Linguistics.William A. Gale and Kenneth W. Church.
1993.
Aprogram for aligning sentences in bilingual corpora.Computational Linguistics, 19(1):75?102.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrased-based machine translation.In HLT/NACL, pages 127?133.Philipp Koehn et al 2007.
Moses: Open source toolkitfor statistical machine translation.
In ACL, demon-stration session.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguis-tics, 31(4):477?504.Douglas W. Oard.
1997.
Alternative approaches forcross-language text retrieval.
In In AAAI Sympo-sium on Cross-Language Text and Speech Retrieval.American Association for Artificial Intelligence.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In ACL, pages 295?302.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignementmodels.
Computational Linguistics, 29(1):19?51.Paul Ogilvie and Jamie Callan.
2001.
Experimentsusing the Lemur toolkit.
In In Proceedings of theTenth Text Retrieval Conference (TREC-10), pages103?108.Philip Resnik and Noah A. Smith Y.
2003.
The webas a parallel corpus.
Computational Linguistics,29:349?380.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In ACL.Masao Utiyama and Hitoshi Isahara.
2003.
Reliablemeasures for aligning Japanese-English news arti-cles and sentences.
In Erhard Hinrichs and DanRoth, editors, ACL, pages 72?79.Christopher C. Yang and Kar Wing Li.
2003.
Auto-matic construction of English/Chinese parallel cor-pora.
J.
Am.
Soc.
Inf.
Sci.
Technol., 54(8):730?742.23
