The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 281?288,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsNAIST at the HOO 2012 Shared TaskKeisuke Sakaguchi, Yuta Hayashibe, Shuhei Kondo, Lis KanashiroTomoya Mizumoto, Mamoru Komachi, Yuji MatsumotoGraduate School of Information ScienceNara Institute of Science and Technology8916-5, Takayama, Ikoma, Nara 630-0192, Japan{ keisuke-sa, yuta-h, shuhei-k, lis-k, tomoya-m, komachi, matsu }@is.naist.jpAbstractThis paper describes the Nara Institute of Sci-ence and Technology (NAIST) error correc-tion system in the Helping Our Own (HOO)2012 Shared Task.
Our system targets prepo-sition and determiner errors with spelling cor-rection as a pre-processing step.
The re-sult shows that spelling correction improvesthe Detection, Correction, and Recognition F-scores for preposition errors.
With regard topreposition error correction, F-scores were notimproved when using the training set with cor-rection of all but preposition errors.
As fordeterminer error correction, there was an im-provement when the constituent parser wastrained with a concatenation of treebank andmodified treebank where all the articles ap-pearing as the first word of an NP were re-moved.
Our system ranked third in preposi-tion and fourth in determiner error corrections.1 IntroductionResearchers in natural language processing have fo-cused recently on automatic grammatical error de-tection and correction for English as a Second Lan-guage (ESL) learners?
writing.
There have been a lotof papers on these challenging tasks, and remark-ably, an independent session for grammatical errorcorrection took place in the ACL-2011.The Helping Our Own (HOO) shared task (Daleand Kilgarriff, 2010) is proposed for improving thequality of ESL learners?
writing, and a pilot run withsix teams was held in 2011.The HOO 2012 shared task focuses on the cor-rection of preposition and determiner errors.
Therehas been a lot of work on correcting preposition anddeterminer errors, where discriminative models suchas Maximum Entropy and Averaged Perceptron (DeFelice and Pulman, 2008; Rozovskaya and Roth,2011) and/or probablistic language models (Gamon,2010) are generally used.In addition, it is pointed out that spelling andpunctuation errors often disturb grammatical errorcorrection.
In fact, some teams reported in theHOO 2011 that they corrected spelling and punc-tuation errors before correcting grammatical errors(Dahlmeier et al, 2011).Our strategy for HOO 2012 follows the aboveprocedure.
In other words, we correct spelling er-rors at the beginning, and then train classifiers forcorrecting preposition and determiner errors.
Theresult shows our system achieved 24.42% (third-ranked) in F-score for preposition error correc-tion, 29.81% (fourth-ranked) for determiners, and27.12% (fourth-ranked) for their combined.In this report, we describe our system architec-ture and the experimental results.
Sections 2 to 4describe the system for correcting spelling, prepo-sition, and determiner errors.
Section 5 shows theexperimental design and results.2 System Architecture for SpellingCorrectionSpelling errors in second language learners?
writingoften disturb part-of-speech (POS) tagging and de-pendency parsing, becoming an obstacle for gram-matical error detection and correction tasks.
For ex-ample, POS tagging for learners?
writing fails be-281e.g.
I think it is *verey/very *convent/convenient for the group.without spelling error correction: ...
(?it?, ?PRP?
), (?is?, ?VBZ?
), (?verey?, ?PRP?
), (?convent?, ?NN?
), ...with spelling error correction : ...
(?It?, ?PRP?
), (?is?, ?VBZ?
), (?very?, ?RB?
), (?convenient?, ?JJ?
), ...Figure 1: POS tagging for learners?
writing without and with spelling error correction.cause of misspelled words (Figure 1).1To reduce errors derived from misspelled words,we conduct spelling error correction as a pre-processing task.
The procedure of spelling error cor-rection we use is as follows.
First of all, we look formisspelled words and suggest candidates by GNUAspell2, an open-source spelling checker.
The can-didates are ranked by the probability of 5-gram lan-guage model built from Google N-gram (Web 1T5-gram Version 1)3 (Brants and Franz, 2006) withIRST LM Toolkit (Federico and Cettolo, 2007).4 Fi-nally, according to the rank, we changed the mis-spelled word into the 1-best candidate word.In a preliminary experiment, where we use theoriginal CLC FCE dataset,5 our spelling error cor-rection obtains 52.4% of precision, 72.2% of recall,and 60.7% of F-score.We apply the spelling error correction to the train-ing and test sets provided, and use both spelling-error and spelling-error-free sets for comparison.3 System Architecture for PrepositionError CorrectionThere are so many prepositions in English.
Becauseit is difficult to perform multi-class classification,we focus on twelve prepositions: of, in, for, to, by,with, at, on, from, as, about, since, which accountfor roughly 91% of preposition usage (Chodorow etal., 2010).The errors are classified into three categories ac-cording to their ways of correction.
First, replace-ment error indicates that learners use a wrongpreposition.
For instance, with in Example (1) is a1The example is extracted from the CLC FCE dataset andpart-of-speech tagged by Natural Language Toolkit (NLTK).http://www.nltk.org/2GNU Aspell 0.60.6.1 http://aspell.net/3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T134irstlm5.70 http://sourceforge.net/projects/irstlm/5In the CLC FCE dataset, misspelled words are correctedand tagged with a label ?S?.replacement error.I went there withby bus.
(1)Second, insertion error points out they incor-rectly inserted a preposition, such as ?about?
in Ex-ample (2).6We discussed aboutNONE the topic.
(2)Third, deletion error means they fail to writeobligatory prepositions.
For example, ?NONE?
inExample (3) is an deletion error.This is the place to relax NONEin.
(3)Replacement and insertion error correction can beregarded as a multi-class classification task at eachpreposition occurrence.
However, deletion errorsdiffer from the other two types of errors in that theymay occur at any place in a sentence.
Therefore, webuild two models, a combined model for replace-ment and insertion errors and a model for deletionerrors, taking the difference into account.For the model of replacement and insertion errors,we simultaneously perform error detection and cor-rection with a single model.For the model of deletion errors, we only checkwhether direct objects of verbs need prepositions,because it is time consuming to check all the gapsbetween words.
Still, it covers most deletion errors.7We merge the outputs of the two models to get thefinal output.We used two types of training sets extracted fromthe original CLC-FCE dataset.
One is the ?gold?set, where training sentences are corrected exceptfor preposition errors.
In the gold set, spelling er-rors are also corrected to the gold data in the corpus.The other is the ?original?
set, which includes the6?NONE?
means there are no words.72,407 out of 5,324 preposition errors in CLC-FCE are be-tween verbs and nouns.282Type Name Description (NP and PRED refer a noun phrase and a predicate.
)Lexical Token n-gram Token n-grams in a 2 word window around the prepositionPOS n-gram POS n-grams in a 2 word window around the prepositionHEAD PREC VP The head verb in the preceding verb phraseHEAD PREC NP The head noun in the preceding noun phraseHEAD FOLLOW NP The head noun in the following noun phraseParsing HEAD Head of the prepositionHEAD POS POS of the headCOMP Complement of the prepositionCOMPLEMENT POS POS of the complementHEAD RELATION Prep-Head relation nameCOMPLEMENT RELATION Prep-Comp relation namePhrase Structure PARENT TAG TAG of the preposition?s parentGRANDPARENT TAG TAG of the preposition?s grandparentPARENT LEFT Left context of the preposition parentPARENT RIGHT Right context of the preposition?s parentWeb N-gram COUNT For the frequency fprep,i of i (3 to 5) window size phrase includingthe preposition prep, the value of log100(fi + 1)PROPORTION The proportion pprep,i (i is 3 to 5).pprep,i = fprep,i?k?T fk,i, given the set of target prepositions T .Semantic WORDNET CATEGORY WordNet lexicographer classes which are about 40 broad semanticcategories for all words used as surface features.
As De Felice andPulman (2008) did not perform word sense disambiguation, neitherdid we.Table 1: Baseline features for English preposition error correction.original CLC-FCE plain sentences.We performed sentence splitting using the im-plementation of Kiss and Strunk (2006) in NLTK2.0.1rc2.
We conducted dependency parsing byStanford parser 1.6.9.8We used the features described in (Tetreault et al,2010) as shown in Table 1 with Maximum Entropy(ME) modeling (Berger et al, 1996) as a multi-classclassifier.
We used the implementation of MaximumEntropy Modeling Toolkit9 with its default parame-ters.
For web n-gram calculation, we used GoogleN-gram with a search system for giga-scale n-gramcorpus, called SSGNC 0.4.6.104 System Architecture for DeterminerError CorrectionWe focused on article error correction in the deter-miner error correction subtask, because the errorsrelated to articles significantly outnumber the errorsunrelated to them.
Though more than twenty typesof determiners are involved in determiner error cor-rections of the HOO training set, over 90% of errors8http://nlp.stanford.edu/software/lex-parser.shtml9https://github.com/lzhang10/maxent10http://code.google.com/p/ssgnc/are related to three articles a, an and the.
We definedarticle error correction as a multi-class classificationproblem with three classes, a, the and null article,and assumed that target articles are placed at the leftboundary of a noun phrase (NP).
The indefinite ar-ticle an was normalized to a in training and testing,and restored to an later in an example-based post-processing step.
If the system output was a and theword immediately after a appeared more frequentlywith an than with a in the training corpus, a was re-stored to an.
If the word appeared equally frequentlywith a and an or didn?t appear in the training corpus,a was restored to an if the word?s first character wasone of a, e, i, o, u.Each input sentence was parsed using the Berke-ley Parser11 with two models, ?normal?
and?mixed?.
The ?normal?
model was trained on a tree-bank of normal English sentences.
In preliminaryexperiments, the ?normal?
model sometimes mis-judged the span of NPs in ESL writers?
sentencesdue to missing articles.
So we trained the ?mixed?model on a concatenation of the normal treebankand a modified treebank in which all the articles ap-pearing as the first word of an NP were removed.
By11version 1.1, http://code.google.com/p/berkeleyparser/283Name DescriptionHeadNounWord The word form of the head nounHeadNounTag The POS tag of the head nounObjOfPrep Indicates that the head noun is an object of a prepositionPrepWord The word form of the prepositionPrepHeadWord The word form of the preposition?s syntactic parentPrepHeadTag The POS tag of the preposition?s syntactic parentContextWindowTagThe POS tag of the words in a 3 word windowaround the candidate position for the articleContextWindowWordThe word form of the word immediately followingthe candidate position for the articleModByDetWord The word form of the determiner that modifies the head nounModByAdjWord The word form of the adjective that modifies the head nounModByAdjTag The POS tag of the adjective that modifies the head nounModByPrep Indicates that the head noun is modified by a prepositionModByPrepWord The word form of the preposition that modifies the head nounModByPossesive Indicates that the head noun is modified by a possesiveModByCardinal Indicates that the head noun is modified by a cardinal numberModByRelative Indicates that the head noun is modified by a relative clauseTable 2: Feature templates for English determiner correction.augmenting the training data for the parser modelwith sentences lacking articles, the span of NPs thatlack an article might have better chance of being cor-rectly recognized.
In addition, dependency informa-tion was extracted from the parse using the Stanfordparser 1.6.9.For each NP in the parse, we extracted a featurevector representation.
We used the feature templatesshown in Table 2, which are inspired by (De Felice,2008) and adapted to the CFG representation.For the parser models, we trained the ?normal?model on the WSJ part of Penn Treebank sections02-21 with the NP annotation by Vadas and Curran(2007).
The ?mixed?
model was trained on the con-catenation of the WSJ part and its modified version.For the classification model, we used the written partof the British National Corpus (BNC) in addition tothe CLC FCE Dataset, because the amount of in-domain data was limited.
In examples taken fromthe CLC FCE Dataset, the true labels after the cor-rection were used.
In examples taken from the BNC,the article of each NP was used as the label.
Wetrained a linear classifier using opal12 with the PA-Ialgorithm.
We also used the feature augmentation12http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/opal/Subsystem ParametersRun Spelling Preposition Determiner0 no change gold mixed1 no change gold normal2 no change original mixed3 no change original normal4 corrected gold mixed5 corrected gold normal6 corrected original mixed7 corrected original normalTable 3: Distinct configurations of the system.approach of (Daume?
III, 2007) for domain adapta-tion.5 Experiment and ResultPreviously undisclosed data extracted from theCLC-FCE dataset was provided as a test set by theHOO organizers.
The test set includes 100 essaysand each contains 180.1 word tokens on average.We defined eight distinct configurations basedon our subsystem parameters (Table 3).
The offi-cial task evaluation uses three metrics (Detection,Recognition, and Correction), and three measuresPrecision, Recall, and F-score were computed13 for13For details about the evaluation metrics, see http://284Detection Correction RecognitionRun R P F R P F R P F0 29.58 34.09 31.67 19.86 22.90 21.27 26.71 30.78 28.601 28.69 36.41 32.09 19.42 24.64 21.72 25.82 32.77 28.882?
28.91 37.21 32.54 20.97 26.98 23.60 26.26 33.80 29.563 28.03 40.18 33.02 20.52 29.43 24.18 25.38 36.39 29.904 30.24 33.66 31.86 20.75 23.09 21.86 27.37 30.46 28.835 29.13 35.57 32.03 19.64 23.98 21.60 26.26 32.07 28.886 29.35 36.23 32.43 21.41 26.43 23.65 26.26 32.42 29.027 28.25 38.67 32.65 20.30 27.29 23.46 25.16 34.44 29.08Table 4: Result for preposition and determiner errors combined before revisions.
?We re-evaluated the Run2 because we submitted the Run2 with the same condition as Run0.Detection Correction RecognitionSpelling Preposition R P F R P F R P Fno change gold 25.00 34.70 29.06 14.40 20.00 16.74 20.76 28.82 24.13no change original 23.30 42.63 30.13 16.52 30.23 21.36 19.91 36.43 25.75corrected gold 26.69 34.80 30.21 15.25 19.88 17.26 22.45 29.28 25.41corrected original 24.57 41.13 30.76 16.52 27.65 20.68 20.33 34.04 25.46Table 5: Result for preposition errors before revisions.each metric.Table 4 to Table 9 show the overall results of oursystems.
In terms of the effect of pre-processing,spelling correction improved the F-score of Detec-tion, Correction, and Recognition for preposition er-rors after revision, whereas there were fluctuationsin other conditions.
This may be because there werea few spelling errors corrected in the test set.14 An-other reason why no stable improvement was foundin determiner error correction is because spellingcorrection often produces nouns that affect the de-terminer error detection and correction more sensi-tively than prepositions.
For example, a misspelledword *freewho / free who was corrected as freezer.This type of error may have increased false posi-tives.
The example *National Filharmony / the Na-tional Philharmony was corrected as National Flem-ing, where the proper noun Fleming does not need adeterminer and this type of error increased false neg-atives.As for preposition error correction, the classifierperformed better when it was trained with the ?origi-nal?
set rather than the error-corrected (all but prepo-sition errors) ?gold?
set.
The reason for this is thatthe gold set is trained with the test set that containscorrecttext.org/hoo2012/eval.html14There was one spelling correction per document in average.several types of errors which the original CLC-FCEdataset alo contains.
Therefore, the ?original?
clas-sifier is more optimised and suitable for the test setthan the ?gold?
one.For determiner error correction, the ?mixed?model improved precision and F-score in the addi-tional experiments.5.1 Error Analysis of Preposition CorrectionWe briefly analyze some errors in our proposedmodel according to the three categories of errors.First, most replacement errors require deep under-standing of context.
For instance, for in Example (4)must be changed to to.
However, modifications of isalso often used, so it is hard to decide either to or ofis suitable based on the values of N-gram frequen-cies.Its great news to hear you have been givenextra money and that you will spend it inmodifications forto the cinema.
(4)Second, most insertion errors need a grammaticaljudgement rather than a semantic one.
For instance,?in?
in Example (5) must be changed to ?NONE.
?Their love had always been kept inNONE se-cret(5)In order to correct this error, we need to recog-285Detection Correction RecognitionSpelling Determiner R P F R P F R P Fno change mixed 34.10 33.18 33.63 25.80 25.11 25.45 33.17 32.28 32.72no change normal 32.25 37.43 34.65 24.88 28.87 26.73 31.33 36.36 33.66corrected mixed 33.64 32.30 32.95 26.72 25.66 26.18 32.71 31.41 32.05corrected normal 31.33 35.78 33.41 24.42 27.89 26.04 30.41 34.73 32.43Table 6: Result for determiner errors before revisions.Detection Correction RecognitionRun R P F R P F R P F0 31.28 37.65 34.18 22.62 27.22 24.71 28.54 34.35 31.171 30.44 40.33 34.69 22.19 29.41 25.30 27.69 36.69 31.562?
31.07 41.76 35.63 23.04 30.96 26.42 28.11 30.96 32.243 30.23 45.25 36.24 22.62 33.86 27.12 27.27 40.82 32.694 31.92 37.10 34.31 23.46 27.27 25.22 29.17 33.90 31.365 30.86 39.35 34.59 22.41 28.57 25.11 28.11 35.84 31.516 31.71 40.87 35.71 23.89 30.79 26.90 28.75 37.05 32.387 30.65 43.80 36.06 22.83 32.62 26.86 27.69 39.57 32.58Table 7: Result for preposition and determiner errors combined after revisions.
?We re-evaluated the Run2 because we submitted the Run2 with the same condition as Run0.nize ?keep?
takes an object and a complement; inExample (5) ?love?
is the object and ?secret?
isthe complement of ?keep?
while the former is left-extraposed.
A rule-based approach may be bettersuited for these cases than a machine learning ap-proach.Third, most deletion errors involve discriminationbetween transitive and intransitive.
For instance,?NONE?
in Example (6) must be changed to ?for?,because ?wait?
is intransitive.I?ll wait NONEfor your next letter.
(6)To deal with these errors, we may use rich knowl-edge about verbs such as VerbNet (Kipper et al,2000) and FrameNet (Baker et al, 1998) in orderto judge whether a verb is transitive or intransitive.5.2 Error Analysis of Determiner CorrectionWe conducted additional experiments for determinererrors and report the results here because the sub-mitted system contained a bug.
In the submit-ted system, while the test data were parsed by the?mixed?
model, the training data and the test datawere parsed by the default grammar provided withBerkeley Parser.
Moreover, though there were about5.5 million sentences in the BNC corpus, only about2.7 million of them had been extracted.
Thoughthese errors seem to have improved the performance,it is difficult to specify which errors had positive ef-fects.Table 10 shows the result of additional experi-ments.
Unlike the submitted system, the ?mixed?model contributed toward a higher precision and F-score.
Though the two parser models parsed thesentences differently, the difference in the syntacticanalysis of test sentences did not always led to dif-ferent output by the downstream classifiers.
On thecontrary, the classifiers often returned different out-puts even for an identically parsed sentence.
In fact,the major source of the performance gap between thetwo models was the number of the wrong outputsrather than the number of correct ones.
While the?mixed?
model without spelling correction returned146 outputs, of which 83 were spurious, the ?nor-mal?
model without spelling correction produced209 outputs, of which 143 were spurious.
This maysuggest the difference of the two models can be at-tributed to the difference in the syntactic analysis ofthe training data.One of the most frequent types of errors com-mon to the two models were those caused by mis-spelled words.
For example, when your letter wasmisspelled to be *yours letter, it was regarded as an286Detection Correction RecognitionSpelling Preposition R P F R P F R P Fno change gold 26.63 38.23 31.40 17.62 25.29 20.77 23.36 33.52 27.53no change original 26.22 49.61 34.31 18.44 34.88 24.12 22.54 42.63 29.49corrected gold 28.27 38.12 32.47 18.44 24.86 21.17 25.00 33.70 28.70corrected original 27.86 48.22 35.32 19.26 33.33 24.41 24.18 41.84 30.64Table 8: Result for preposition errors after revisions.Detection Correction RecognitionSpelling Determiner R P F R P F R P Fno change mixed 35.37 36.32 35.84 27.94 28.69 28.31 34.06 34.97 34.51no change normal 33.62 41.17 37.01 27.07 33.15 29.80 32.31 39.57 35.57corrected mixed 34.93 35.39 35.16 28.82 29.20 29.01 33.62 34.07 33.84corrected normal 32.75 39.47 35.79 26.63 32.10 29.11 31.44 37.89 34.36Table 9: Result for determiner errors after revisions.Detection Correction RecognitionSpelling Determiner R P F R P F R P Fno change mixed 27.39 43.15 33.51 23.04 36.30 28.19 27.39 43.15 33.51no change normal 28.69 31.57 30.06 22.61 24.88 23.69 28.69 31.57 30.06corrected mixed 27.39 41.44 31.98 22.61 34.21 27.22 26.96 40.79 32.46corrected normal 30.43 33.33 31.82 24.34 26.67 25.45 30.00 32.86 31.36Table 10: Result of additional experiments for determiner errors after revisions.NP without a determiner resulting in a false posi-tive such as *a yours letter.
Among the other typesof errors, several seemed to be caused by the infor-mation from the context window.
For instance, thesystem output for It was last month and ... was itwas *the last month and ....
It is likely that the wordlast triggered the misinsertion here.
Such kind oferrors might be avoided by conjunctive features ofcontext information and the head word.
Last but notleast, compound errors were also frequent and prob-ably the most difficult to solve.
For example, it isquite difficult to correct *for a month to per monthif we are dealing with determiner errors and prepo-sition errors separately.
A more sophisticated ap-proach such as joint modeling seems necessary tocorrect this kind of errors.6 ConclusionThis report described the architecture of our prepo-sition and determiner error correction system.
Theexperimental result showed that spelling correctionadvances the performance of Detection, Correctionand Recognition for preposition errors.
In terms ofpreposition error correction, F-scores were not im-proved when the error-corrected dataset was used.As to determiner error correction, there was an im-provement when the constituent parser was trainedon a concatenation of treebank and modified tree-bank where all the articles appearing as the firstword of an NP were removed.AcknowledgementsThis work was partly supported by the National In-stitute of Information and Communications Tech-nology Japan.287ReferencesCollin F Baker, Charles J Fillmore, and John B Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of the 36th Annual Meeting of the Association forComputational Linguistics, pages 86?90, Montreal,Quebec, Canada.Adam L. Berger, Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A Maximum Entropy Ap-proach to Natural Language Processing.
Computa-tional Linguistics, 22(1):39?71.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramCorpus Version 1.1.
Linguistic Data Consortium.Martin Chodorow, Michael Gamon, and Joel Tetreault.2010.
The Utility of Article and Preposition Error Cor-rection Systems for English Language Learners: Feed-back and Assessment.
Language Testing, 27(3):419?436.Daniel Dahlmeier, Hwee Tou Ng, and Thanh Phu Tran.2011.
NUS at the HOO 2011 Pilot Shared Task.
InProceedings of the 13th European Workshop on Nat-ural Language Generation, pages 257?259, Nancy,France.Robert Dale and Adam Kilgarriff.
2010.
Helping OurOwn: Text Massaging for Computational Linguisticsas a New Shared Task.
In Proceedings of the 6th In-ternational Natural Language Generation Conference,pages 261?266, Trim, Co. Meath, Ireland.Hal Daume?
III.
2007.
Frustratingly Easy Domain Adap-tation.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages256?263, Prague, Czech Republic.Rachele De Felice and Stephen G. Pulman.
2008.
AClassifier-Based Approach to Preposition and Deter-miner Error Correction in L2 English.
In Proceed-ings of the 22nd International Conference on Compu-tational Linguistics, pages 169?176, Manchester, UK.Rachele De Felice.
2008.
Automatic Error Detection inNon-native English.
Ph.D. thesis, University of Ox-ford.Marcello Federico and Mauro Cettolo.
2007.
EfficientHandling of N-gram Language Models for StatisticalMachine Translation.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages88?95, Prague, Czech Republic.Michael Gamon.
2010.
Using Mostly Native Data toCorrect Errors in Learners?
Writing.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 163?171, Los An-geles, California.Karin Kipper, Hoa Trang Dang, and Martha Palmer.2000.
Class-based Construction of a Verb Lexicon.
InProceedings of the 7th National Conference on Artifi-cial Intelligence, pages 691?696, Austin, Texas, USA.Tibor Kiss and Jan Strunk.
2006.
Unsupervised Multi-lingual Sentence Boundary Detection.
ComputationalLinguistics, 32(4):485?525.Alla Rozovskaya and Dan Roth.
2011.
Algorithm Selec-tion and Model Adaptation for ESL Correction Tasks.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 924?933, Portland, Ore-gon, USA.Joel Tetreault, Jennifer Foster, and Martin Chodorow.2010.
Using Parse Features for Preposition Selec-tion and Error Detection.
In Proceedings of the 47thAnnual Meeting of the Association for ComputationalLinguistics Short Papers, pages 353?358, Uppsala,Sweden.David Vadas and James Curran.
2007.
Adding NounPhrase Structure to the Penn Treebank.
In Proceedingsof the 45th Annual Meeting of the Association of Com-putational Linguistics, pages 240?247, Prague, CzechRepublic.288
