Proceedings of the 8th Workshop on Asian Language Resources, pages 88?94,Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language ProcessingWord Segmentation for Urdu OCR SystemMisbah AkramNational University of Computerand Emerging Sciencesmisbahakram@gmail.comSarmad HussainCenter for Language Engineering,Al-Khawarizmi Institute of ComputerScience, University of Engineering andTechnology, Lahore, Pakistansarmad.hussain@kics.edu.pkAbstractThis paper presents a technique for wordsegmentation for the Urdu OCR system.Word segmentation or word tokeniza-tion is a preliminary task for Urdu lan-guage processing.
Several techniquesare available for word segmentation inother languages.
A methodology is pro-posed for word segmentation in this pa-per which determines the boundaries ofwords given a sequence of ligatures,based on collocation of ligatures andwords in the corpus.
Using this tech-nique, word identification rate of96.10% is achieved, using trigram prob-abilities normalized over the number ofligatures and words in the sequence.1 IntroductionUrdu uses Nastalique style of Arabic scriptfor writing, which is cursive in nature.
Charac-ters join together to form ligatures, which endeither with a space or with a non-joining charac-ter.
A word may be composed of one of moreligatures.
In Urdu, space is not used to separatetwo consecutive words in a sentence; insteadreaders themselves identify the boundaries ofwords, as the sequence of ligatures, as they readalong the text.
Space is used to get appropriatecharacter shapes and thus it may even be usedwithin a word to break the word into constituentligatures (Naseem 2007, Durrani 2008).
There-fore, like other languages (Theeramunkong &Usanavasin, 2001; Wan and Liu, 2007; Khanka-sikam & Muansuwan, 2005; Haruechaiyasak etal., 2008; Haizhou & Baosheng, 1998), wordsegmentation or word tokenization is a prelimi-nary task for Urdu language processing.
It hasapplications in many areas like spell checking,POS tagging, speech synthesis, information re-trieval etc.
This paper focuses on the word seg-mentation problem from the point of view ofOptical Character Recognition (OCR) System.As space is not visible in typed and scanned text,spacing cues are not available to the OCR forword separation and therefore segmentation hasto be done more explicitly.
This word segmenta-tion model for Urdu OCR system takes input inthe form of a sequence of ligatures recognizedby an OCR to construct a sequence of wordsfrom them.2 Literature ReviewMany languages, e.g., English, French,Hindi, Nepali, Sinhala, Bengali, Greek, Russian,etc.
segment text into a sequence of words usingdelimiters such as space, comma and semi colonetc., but on the other hand many Asian languag-es like Urdu, Persian, Arabic, Chinese,Dzongkha, Lao and Thai have no explicit wordboundaries.
In such languages, words are seg-mented using more advanced techniques, whichcan be categorized into three methods:(i) Dictionary/lexicon based approaches(ii) Linguistic knowledge based approaches(iii) Machine learning based approach-es/statistical approaches(Haruechaiyasak et al, 2008)Longest matching (Poowarawan, 1986; RichardSproat, 1996) and maximum matching (Sproatet al, 1996; Haizhou & Baosheng, 1998) areexamples of lexicon based approaches.
Thesetechniques segment text using the lexicon.
Their88accuracy depends on the quality and size of thedictionary.N-Grams (Chang et al, 1992; Li Haizhouet al, 1997; Richard Sproat, 1996; Dai & Lee,1994; Aroonmanakun, 2002) and Maximumcollocation (Aroonmanakun, 2002) are Linguis-tic knowledge based approaches, which alsorely very much on the lexicon.
These approach-es select most likely segmentation from the setof possible segmentations using a probabilisticor cost-based scoring mechanism.Word segmentation using decision trees(Sornlertlamvanich et al, 2000; Theeramun-kong & Usanavasin, 2001) and similar othertechniques fall in the third category of wordsegmentation techniques.
These approaches usea corpus in which word boundaries are explicit-ly marked.
These approaches do not require dic-tionaries.
In these approaches ambiguity prob-lems are handled by providing a sufficientlylarge set of training examples to enable accurateclassification.A knowledge based approach has beenadopted for earlier work on Urdu word segmen-tation (Durrani 2007; also see Durrani and Hus-sain 2010).
In this technique word segmentationof Urdu text is achieved by employing know-ledge based on the Urdu linguistics and script.The initial segmentations are ranked using min-word, unigram and bigram techniques.
It reports95.8 % overall accuracy for word segmentationof Urdu text.
Mukund et al (2009) propose us-ing character model along with linguistic rulesand report 83% precision.
Lehal (2009) propos-es a two stage process, which first uses Urdulinguistic knowledge, and then uses statisticalinformation of Urdu and Hindi (also usingtransliteration into Hindi) in the second stagefor words not addressed in the first stage, re-porting an accuracy of 98.57%.These techniques use characters or words inthe input, whereas an OCR outputs a series ofligatures.
The current paper presents work doneusing statistical methods as an alternative,which works with ligatures as input.3 MethodologyCurrent work uses the co-occurrence in-formation of ligatures and words to construct astatistical model, based on manually cleanedand segmented training corpora.
Ligature andword statistics are derived from these corpora.In the decoding phase, first all sequences ofwords are generated from input set of ligaturesand ranking of these sequences is done based onlexical lookup.
Top k sequences are selected forfurther processing, based on the number of validwords.
Finally, the probability of each of the ksequences is calculated for the final decision.Details are described in the subsequent sections.3.1 Data collection and preparationAn existing lexicon of 49630 unique wordsis used (derived from Ijaz et al 2007).
The cor-pus used for building ligature grams consists ofhalf a million words.
Of these, 300,000 wordsare taken from the Sports, Consumer Informa-tion and Culture/Entertainment domains of the18 million word corpus (Ijaz et al 2007),100,000 words are obtained from Urdu-Nepali-English Parallel Corpus (available atwww.PANL10n.net), and another 100,000words are taken from a previously POS taggedcorpus (Sajjad, 2007; tags of this corpus are re-moved before further processing).
This corpusis manually cleaned for word segmentation er-rors, by adding missing spaces between wordsand replacing spaces with Zero Width Non-Joiner (ZWNJ) within words.
For the computa-tion of word grams, the 18 million word corpusof Urdu is used (Ijaz et al 2007).3.2 Count and probability calculationsTable 1 and Table 2 below give the countsfor unigram, bigrams and trigram of the liga-tures and the words derived from the corporarespectively.LigatureTokensLigatureUnigramLigatureBigramsLigatureTrigrams1508078 10215 35202 65962Table 1.
Unigram, bigram and trigram counts ofthe ligature corpusWordTokensWordUnigramsWordBigramsWordTrigrams17352476 157379 1120524 8143982Table 2.
Unigram, bigram and trigram counts ofthe word corpusAfter deriving word unigrams, bigrams,and trigrams, the following cleaning of corpus is89performed.
In the 18 million word corpus, cer-tain words are combined due to missing space,but are separate words.
Some of these wordsoccur with very high frequency in the corpus.For example ??????
(ho ga, ?will be?)
exists assingle word rather than two words due to miss-ing space.
To solve this space insertion problem,a list of about 700 words with frequency greaterthan 50 is obtained from the word unigrams.Each word of the list is manually reviewed andspace is inserted, where required.
Then theseerror words are removed from the word unigramand added to the word unigram frequency list astwo or three individual words incrementing re-spective counts.For the space insertion problem in wordbigrams, each error word in joined-word list(700-word list) is checked.
Where these errorwords occurs in a bigram word frequency list,for example ????
?????
(kiya ho ga ?will havedone?)
exists in the bigram list and contains????"
" error word, then this bigram entry ????
????
?is removed from the bigram list and counts of?
??
??
?
and ????
???
are increased by the count of????
?????.
If these words do not exist in the wordbigram list then they are added as a new bi-grams with the count of ????
?????.
Same proce-dure is performed for the word trigrams.The second main issue is with word-affixes,which are sometimes separated by spaces fromthe words.
Therefore, in calculations, these aretreated as separate words and exist as bigramentries in the list rather than a unigram entry.For example "???
???"
(sehat+mand, ?healthy?
)exists as a bigram entry but in Urdu it is a singleword.
To cope with this problem, a list ofword-affixes is used.
If any entry of word bi-gram matches with an affix, then this word iscombined by removing spurious space from it(and inserting ZWNJ, if required to maintain itsglyph shape).
Then this word is inserted in theunigram list with its original bigram count andunigram list updated accordingly.
Same proce-dure is performed if a trigram word matcheswith an affix.After cleaning, unigram, bigram and tri-gram counts for both words and ligatures arecalculated.
To avoid data sparseness One CountSmoothing (Chen & Goodman, 1996) is applied.3.3 Word sequences generation from inputThe input, in the form of sequence of liga-tures is used to generate all possible words.These sequences are then ranked based on realwords.
For this purpose, a tree of these se-quences is incrementally built.
The first ligatureis added as a root of tree, and at each level twoto three additional nodes are added.
For exam-ple the second level of the tree contains the fol-lowing tree nodes.?
Current ligature forms a separate word, se-parated with space, from the sequence at itsparent, l1 l2?
Current ligature concatenates, without aspace, with the sequence at its parent, l1l2?
Current ligature concatenates, without aspace, with the sequence at its parent butwith an additional, l1ZWNJl2For each node, at each level of the tree, a nu-meric value is assigned, which is the sum ofsquares of the number of ligatures in each wordwhich is in the dictionary.
If a word does notexist in dictionary then it does not contribute tothe total sum.
If a node-string has only one wordand this word does not occur in the dictionary asa valid word then it is checked that this wordmay occur at the start of any dictionary entry.
Inthis case numeric value is also assigned.After assignment, nodes are ranked ac-cording to these values and best k (beam value)nodes are selected.
These selected nodes arefurther ranked using statistical methods dis-cussed below.3.4 Best word segmentation selectionFor selection of the most probable wordsegmentation sequence word and ligature mod-els are used.
For word probabilities the follow-ing is used.PW =      argmax ?
SPwTo reduce the complexity of computing, Mar-kov assumption are taken to give bigram andtrigram approximations (e.g., see Jurafsky &Martin 2006) as given below.PW =      argmax ?
S?
P w|w            PW =      argmax ?
S?
Pw|wwSimilarly the ligature models are built bytaking the assumption that sentences are made90up of sequences of ligatures rather than wordsand space is also a valid ligature.
By taking theMarkov bigram and trigram assumption for liga-ture grams we get the following.PL =      argmax ?
S?
P l|l    PL =      argmax ?
S?
P l|ll    Given the ligatures, e.g.
as input from and OCR, we can formulate the decoding problem as the following equation.PW|L =      argmax ?
SPw| l   where  w = w,w,w3,w4,?w  and l = l,l,l3,l4,?l ; n represents number of words and m represents the number of liga-tures.
This equation also represents that m number of ligatures can be assigned to n number of words.
By applying the Bayesian theorem we get the following derivation.PW|L =      argmax ?
S P<=>?@.PP=>   As P l is same for all w , so the denomi-nator does not change the equation, simplify-ing to the following expression.
PW|L = argmax ?
SPl|w.
Pw           where  Pl|w =P l,l,l3, ?
l|w     =Pl|w ?
P l|wl ?
Pl3|wll  ?
Pl4|wlll3 ?
?
Pl|wlll3?
l Assuming that a ligature l depends only on the word sequence w and its previous liga-ture l, and not the ligature history, the above equation can be simplifed as follows.
Pl|w = P l|w ?
P l|wl ?
Pl3|wl?
Pl4|wl3 ?
?
Pl|wl                     =  ?
P l|wl   Further, if it is assumed that l depends on the word in which it appears, not whole word sequence, the equation can be further simpli-fied to the following as probability of l with-in a word is 1.
Pl|w =  ?
P l|l   Thus, considering bigrams, PW|L =argmax ?
S EFP l|lG FPw|w This gives the maximum probable word se-quence among all the alternative word se-quences.
The precision of the equation can be taken at bigram or trigram level for both ligature and word, giving the following pos-sibilities.
Additionally, normalization is also done to better compare different sequences, as each sequences has different number of words and ligatures per word.?
Ligature trigram and word bigram basedtechniquePW =      argmax ?
S?
P l|ll  ??
Pw|w ?
Ligature bigram and word trigram basedtechnique PW =      argmax ?
S?
P l|l  ??
Pw|ww ?
Ligature trigram and word trigram basedtechnique PW =      argmax ?
S?
P l|ll  ??
Pw|ww ?
Normalized ligature bigram and word bi-gram based techniquePW =      argmax ?
S?
P l|l  NLL ??
Pw|w   NWL?
Normalized ligature trigram and word bi-gram based techniquePW =   argmax ?
S N?
P l|ll  NLL O ??
Pw|w   NWL?
Normalized ligature bigram and word tri-gram based techniquePW =      argmax ?
S?
P l|l  NLL ??
Pw|ww   NWL?
Normalized ligature trigram and word tri-gram based techniquePW =  argmax ?
S?
P l|ll  NLL ??
Pw|ww   NWLIn the current work, all the above tech-niques are used and the best sequence from eachone is shortlisted.
Then the word sequencewhich occurs the most times in this shortlist isfinally selected.91NL represents the number of ligature bi-grams or trigrams and NW represents the num-ber of word bigram or trigrams that exist in thegiven sentence.4 Results and DiscussionThe model is tested on a corpus of 150 sen-tences composed of 2156 words and 6075 liga-tures.
In these sentences, 62 words are unknown,i.e.
the words that do not exist in our dictionary.The average length of the sentence is 14 wordsand 40.5 ligatures.
The average length of wordis 2.81 ligatures.
All the techniques are testedwith a beam value, k, of 10, 20, 30, 40, and 50.The results can be viewed from two pers-pectives: sentence identification rate, and wordidentification rate.
A sentence is consideredincorrect even if one word of the sentence isidentified wrongly.
The technique gives thesentence identification rate of 76% at the beamvalue of 30.
At word level, Normalized Liga-ture Trigram Word Trigram Technique outper-forms other techniques and gives a 96.10%word identification rate at the beam value of 50.The normalized data gives much better pre-diction compared to the un-normalized data.Sentence identification errors depend heavi-ly on the unknown words.
For example, at thebeam value of 30 we predict 38 incorrect sen-tences, of which 25 sentence level errors are dueto unknown-words and 13 errors are due toknown word identification errors.
Thus improv-ing system vocabulary will have significant im-pact on accuracy.Many of the word errors are caused due toinsufficient cleaning of word the larger corpus.Though the words with frequency greater than50 from the 18 million word corpus have beencleaned, the lower frequency words cause theseerrors.
For example word list still contains"???????
"(bunyad per, ?depends on?
),  ???????"
" (setaqseem, ?divided by?)
with frequency of 40and 5 respectively, and each should be twowords with a space between them.
If low fre-quency words are also cleaned results will fur-ther improve, though it would take a lot of ma-nual effort.BeamValueTotal Sentencesidentified%ageTotal WordsIdentified%ageTotal knownwords identified %ageTotal unknownwords identified %age10 110/150 73.33% 2060/2156 95.55% 2024/2092 96.75% 36/64 56.25%20 112/150 74.67% 2066/2156 95.83% 2027/2092 96.89% 39/64 60.94%30 114/150 76% 2062/2156 95.64% 2019/2083 96.93% 43/73 58.90%40 105/150 70% 2037/2156 94.48% 2000/2092 95.60% 37/64 57.81%50 106/150 70.67% 2040/2156 94.62% 2000/2092 95.60% 40/64 62.50%Table 3.
Results changing beam width k of the treeTechnique Total sentencesidentified%age Total wordsidentified%age Total knownwords Identified%age Total unknownwords identified%ageLigature Bigram 50/150 33.33% 1835/2156 85.11% 1806/2092 86.33% 29/64 45.31%Ligature Bigram WordBigram68/150 45.33% 1900/2156 88.13% 1865/2092 89.15% 35/64 54.69%Ligature Bigram WordTrigram83/150 55.33% 1960/2156 90.91% 1924/2092 91.97% 36/64 56.25%Ligature Trigram 16/150 10.67% 1637/2156 75.93% 1610/2092 76.96% 27/64 42.19%Ligature Trigram WordBigram42/150 28% 1776/2156 82.38% 1746/2092 83.46% 30/64 46.88%Ligature Trigram WordTrigram62/150 41.33% 1868/2156 86.64% 1835/2092 87.72% 33/64 51.56%Normalized LigatureBigram Word Bigram90/150 60% 2067/2156 95.87% 2024/2092 96.75% 43/64 67.19%Normalized LigatureBigram Word Trigram100/150 66.67% 2070/2156 96.01% 2028/2092 96.94% 42/64 65.63%Normalized LigatureTrigram Word Bigram93/150 62% 2071/2156 96.06% 2030/2092 97.04% 41/64 64.06%Normalized LigatureTrigram Word Trigram101/150 67.33% 2072/2156 96.10% 2030/2092 97.04% 42/64 65.63%Word Bigram 47/150 31.33% 1827/2156 84.74% 1796/2092 85.85% 31/64 48.44%Word Trigram 74/150 49.33% 1937/2156 89.84% 1903/2092 90.97% 34/64 53.13%92Table 4.
Results for all techniques for the beam value of 50Errors are also caused if an alternate liga-ture sequence exists.
For example the propernoun "?????"
(kartak) is not identifiec as it doesnot exist in dictionary, but the alternate twoword sequence "???
??"
(kar tak, ?till the car?
)is valid.This work uses the knowledge of ligaturegrams and word grams.
It can be further en-hanced by using the character grams.
We havetried to clean the corpus.
Further cleaning andadditional corpus will improve the results aswell.
Improvement can also be achieved byhandling abbreviations and English words trans-literated in the text.
The unknown word detec-tion rate can be increased by applying POS tag-ging to further help rank the multiple possiblesentences.5 ConclusionsThis work presents an initial effort on sta-tistical solution of word segmentation, especial-ly for Urdu OCR systems.
This work develops acleaned corpus of half a million Urdu words forstatistical training of ligature based data, whichis now available for the research community.
Inaddition, the work develops a statistical modelfor word segmentation using ligature and wordstatistics.
Using ligature statistics improvesupon using just the word statistics.
Furthernormalization has significant impact on accura-cy.ReferencesAroonmanakun, W. (2002).
Collocation and ThaiWord Segmentation.
In Proceedings of the FifthSymposium on Natural Language Processing &The Fifth Oriental COCOSDA Workshop, (pp.68-75).
Pathumthani.Chang, Jyun-Shen, Chen, S.-D., Zhen, Y., Liu, X.-Z.,& Ke, S.-J.
(1992).
Large-corpus-based methodsfor Chinese personal name recognition.
Journal ofChinese Information Processing , 6 (3), 7-15.Chen, F., & Goodman, T. (1996).
An EmpiricalStudy of Smoothing Techniques for LanguageModeling.
In the Proceedings of the 34th AnnualMeeting of the Association for ComputationalLinguistics, (pp.
310-318).Church, K. W., & Gale, W. A.
(1991).
A comparisonof the enhanced Good-Turing and deleted estima-tion methods for estimating probabilities of Eng-lish bigrams.
Computer Speech and Language , 5,19-54.Dai, J.-C., & Lee, H.-J.
(1994).
Paring with Tag In-formation in a probabilistic generalized LR pars-er.
International Conference on Chinese Compu-ting.
Singapore.Durrani, N. (2007).
Typology of word and automaticword Segmentation in Urdu text corpus.
Thesis,National University of Computer & EmergingSciences, Lahore , Pakistan.Durrani, N., Hussain, S. (2010).
Urdu Word Segmen-tation, In the 11th Annual Conference of theNorth American Chapter of the Association forComputational Linguistics (NAACL HLT 2010),Los Angeles, US.Haizhou, L., & Baosheng, Y.
(1998).
Chinese WordSegmentation.
In Proceedings of the 12th PacificAsia Conference on Language, Information andComputation, (pp.
212-217).Haruechaiyasak, C., Kongyoung, S., & Dailey, M. N.(2008).
A Comparative Study on Thai Word Seg-mentation Approaches.Hussain, S. (2008).
Resources for Urdu LanguageProcessing.
In Proceedings of the Sixth Workshopon Asian Language Resources.Ijaz, M., Hussain, S. (2007).
Corpus Based UrduLexicon Development, in the Proceedings of Con-ference on Language Technology (CLT07), Uni-versity of Peshawar, Pakistan.Jurafsky, D., & Martin., l. J(2000).
Speech and Lan-guage Processing: An Introduction to /aturalLanguage Processing (1st ed.).
ComputationalLinguistics and Speech Recognition Prentice Hall.Khankasikam, K., & Muansuwan, N. (n.d.).
ThaiWord Segmentation a Lexical Semantic Ap-proach.Khankasikam, K., & Muansuwan, N. (2005).
ThaiWord Segmentation a Lexical Semantic Approach.In the Proceedings of the Tenth Machine Transla-tion Summit, (pp.
331-338).
Thailand.Lehal, G. (2009).
A Two Stage Word SegmentationSystem for Handling Space Insertion Problem inUrdu Script.
World Academy of Science, Engi-neering and Technology 60.93MacKay, D. J., & Peto, L. C. (1995).
A HierarchicalDirichlet Language Mode.
Natural Language En-gineering , 1 (3), 1-19.Mukund, S. & Srihari, R. (2009).
/E Tagging forUrdu based on Bootstrap POS Learning.
In theProceedings of CLIAWS3, Third InternationalCross Lingual Information Access Workshop,pages 61?69, Boulder, Colorado, USA.Naseem, T., & Hussain, S. (2007).
Spelling ErrorTrends in Urdu, In the Proceedings of Conferenceon Language Technology (CLT07), University ofPeshawar, Pakistan.Pascale, F., & Dekai, W. (1994).
Statistical augmen-tation of a Chinese machine readable dictionary.Proceedings of the Second Annual Workshop onVery Large Corpora, (pp.
69-85).Poowarawan, Y.
(1986).
Dictionary-based Thai Syl-lable Separation.
Proceedings of the Ninth Elec-tronics Engineering Conference.Richard Sproat, C. S. (1996).
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese.Computational Linguistics , 22 (3).Sajjad, H. (2007).
Statistical Part-of-Speech for Ur-du.
MS thesis, National University of Computerand Emerging Sciencies, Centre for Research inUrdu Language Processing, Lahore , Pakistan.Sornlertlamvanich, V., Potipiti, T., & charoenporn,T.
(2000).
Automatic Corpus-Based Thai WordAlgorithm Extraction with the C4.5 Learning.Proceedings of the 18th conference on Computa-tional linguistics.Sproat, R., Shih, C., Gale, W., & Chang, N. (1996).A Stochastic Finite-State Word-Segmentation Al-gorithm for Chinese.
Computational Linguistics ,22 (3).Theeramunkong, T., & Usanavasin, S. (2001).
/on-Dictionary-Based Thai Word Segmentation UsingDecision Trees.
Proceedings of the first interna-tional conference on Human language technologyresearch.Urdu-Nepali-English Parallel Corpus.
(n.d.).
Re-trieved from Center for Research in Urdu Lan-guage Processing: http://www.crulp.org/software/ling_resources/urdunepalienglishparallelcorpus.htmWang, X.-J., Liu, W., & Qin, Y.
(2007).
A Search-based Chinese Word Segmentation Method.
16thInternational World Wide Web Conference.Wong, P.-k., & Chan, C. (1996).
Chinese WordSegmentation based on Maximum Matching andWord Binding Force.
In Proceedings of the 16thconference on Computational linguistics.94
