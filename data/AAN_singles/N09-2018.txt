Proceedings of NAACL HLT 2009: Short Papers, pages 69?72,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsWeb and Corpus Methods for Malay Count Classifier PredictionJeremy Nicholson and Timothy BaldwinNICTA Victoria Research LaboratoriesUniversity of Melbourne, VIC 3010, Australia{jeremymn,tim}@csse.unimelb.edu.auAbstractWe examine the capacity of Web and corpusfrequency methods to predict preferred countclassifiers for nouns in Malay.
The observedF-score for the Web model of 0.671 consid-erably outperformed corpus-based frequencyand machine learning models.
We expect thatthis is a fruitful extension for Web?as?corpusapproaches to lexicons in languages other thanEnglish, but further research is required inother South-East and East Asian languages.1 IntroductionThe objective of this paper is to extend a Malaylexicon with count classifier information for nomi-nal types.
This is done under the umbrella of deeplexical acquisition: the process of automatically orsemi-automatically learning linguistic structures foruse in linguistically rich language resources such asprecision grammars or wordnets (Baldwin, 2007).One might call Malay a ?medium-density?
lan-guage: some NLP resources exist, but substantiallyfewer than those for English, and they tend to beof low complexity.
Resources like the Web seempromising for bootstrapping further resources, aidedin part by simple syntax and a Romanised ortho-graphic system.
The vast size of the Web has beendemonstrated to combat the data sparseness prob-lem, for example, in Lapata and Keller (2004).We examine using a similar ?first gloss?
strategyto Lapata and Keller (akin to ?first sense?
in WSD,in this case, identifying the most basic surface formthat a speaker would use to disambiguate betweenpossible classes), where the Web is used a corpus toquery a set of candidate surface forms, and the fre-quencies are used to disambiguate the lexical prop-erty.
Due to the heterogeneity of the Web, we expectto observe a significant amount of blocking from In-donesian, a language with which Malay is some-what mutually intelligible (Gordon, 2005).
Hence,we contrast this approach with observing the cuesdirectly from a corpus strictly of Malay, as well as acorpus-based supervised machine learning approachwhich does not rely on a presupplied gloss.2 Background2.1 Count ClassifiersA count classifier (CL) is a noun that occurs in aspecifier phrase with one of a set of (usually nu-meric) specifiers; the specifier phrase typically oc-curs in apposition or as a genitive modifier (GEN) tothe head noun.
In many languages, including manySouth-East Asian, East Asian, and African families,almost all nouns are uncountable and can only becounted through specifier phrases.
A Malay exam-ple, where biji is the count classifier (CL) for fruit, isgiven in (1).
(1) tigathreebijiCLpisangbanana?three bananas?Semantically, a lexical entry for a noun will in-clude a default (sortal) count classifier which se-lects for a particular semantic property of the lemma.Usually this is a conceptual class (e.g.
HUMAN orANIMAL) or a description of some relative dimen-sional property (e.g.
FLAT or LONG-AND-THIN).Since each count classifier has a precise seman-tics, using a classifier other than the default can co-erce a given lemma into different semantics.
For ex-ample, raja ?king?
typically takes orang ?person?as a classifier, as in 2 orang raja ?2 kings?, but cantake on an animal reading with ekor ?animal?
in 2ekor raja ?2 kingfishers?.
An unintended classifier69can lead to highly marked or infelicitious readings,such as #2 biji raja ?2 (chess) kings?.Most research on count classifiers tends to discussgenerating a hierarchy or taxonomy of the classi-fiers available in a given language (e.g.
Bond andPaik (1997) for Japanese and Korean, or Shirai etal.
(2008) cross-linguistically) or using language-specific knowledge to predict tokens (e.g.
Bond andPaik (2000)) or both (e.g.
Sornlertlamvanich et al(1994)).2.2 Malay DataLittle work has been done on NLP for Malay, how-ever, a stemmer (Adriani et al, 2007) and a prob-abilistic parser for Indonesian (Gusmita and Manu-rung, 2008) have been developed.
The mutually in-telligibility suggests that Malay resources could pre-sumably be extended from these.In our experiments, we make use of a Malay?English translation dictionary, KAMI (Quah et al,2001), which annotates about 19K nominal lexicalentries for count classifiers.
To limit very low fre-quency entries, we cross-reference these with a cor-pus of 1.2M tokens of Malay text, described in Bald-win and Awab (2006).
We further exclude the twonon-sortal count classifiers that are attested as de-fault classifiers in the lexicon, as their distribution isheavily skewed and not lexicalised.In all, 2764 simplex common nouns are attestedat least once in the corpus data.
We observe 2984unique noun?to?default classifier assignments.
Pol-ysemy leads to an average of 1.08 count classifiersassigned to a given wordform.
The most difficultexemplars to classify, and consequently the most in-teresting ones, correspond to the dispreferred countclassifiers of the multi-class wordforms: direct as-signment and frequency thresholding was observedto perform poorly.
Since this task is functionallyequivalent to the subcat learning problem, strategiesfrom that field might prove helpful (e.g.
Korhonen(2002)).The final distribution of the most frequent classesis as follows:CL: orang buah batang ekor OTHERFreq: 0.389 0.292 0.092 0.078 0.149Of the 49 classes, only four have a relative frequencygreater than 3% of the types: orang for people,batang for long, thin objects, ekor for animals, andbuah, the semantically empty classifier, for when noother classifiers are suitable (e.g.
for abstract nouns);orang and buah account for almost 70% of the types.3 Experiment3.1 MethodologyLapata and Keller (2004) look at a set of generationand analysis tasks in English, identify simple surfacecues, and query a Web search engine to approximatethose frequencies.
They then use maximum likeli-hood estimation or a variety of normalisation meth-ods to choose an output.For a given Malay noun, we attempt to select thedefault count classifier, which is a generation taskunder their framework, and semantically most simi-lar to noun countability detection.
Specifier phrasesalmost always premodify nouns in Malay, so the setof surface cues we chose was satu CL NOUN ?one/aNOUN?.1 This was observed to have greater cov-erage than dua ?two?
and other non-numeral spec-ifiers.
49 queries were performed for each head-word, and maximum likelihood estimation was usedto select the predicted classifier (i.e.
taking most fre-quently observed cue, with a threshold of 0).
Fre-quencies from the same cues were also obtainedfrom the corpus of Baldwin and Awab (2006).We contrasted this with a machine learning modelfor Malay classifiers, designed to be language-independent (Nicholson and Baldwin, 2008).
A fea-ture vector is constructed for each headword by con-catenating context windows of four tokens to the leftand right of each instance of the headword in the cor-pus (for eight word unigram features per instance).These are then passed into two kinds of maximumentropy model: one conditioned on all 49 classes,and one cascaded into a suite of 49 separate binaryclassifiers designed to predict each class separately.Evaluation is via 10-fold stratified cross-validation.A majority class baseline was also examined, whereevery headword was assigned the orang class.For the corpus-based methods, if the frequency ofevery cue is 0, no prediction of classifier is made.Similarly, the suite can predict a negative assign-1satu becomes cliticised to se- in this construction, so thatinstead of cues like satu buah raja, satu orang raja, ..., we havecues like sebuah raja, seorang raja, ....70Method Web Corpus Suite Entire BasePrec.
.736 .908 .652 .570 .420Rec.
.616 .119 .379 .548 .389F?
= 1 .671 .210 .479 .559 .404Table 1: Performance of the five systems.Back-off Web Suite Entire orang buahPrec.
.736 .671 .586 .476 .389Rec.
.616 .421 .561 .441 .360F?
= 1 .671 .517 .573 .458 .374Table 2: Performance of corpus frequency assignment(Corpus in Table 1), backed-off to the other systems.ment for each of the 49 classes.
Consequently, pre-cision is calculated as the fraction of correctly pre-dicted instances to the number of examplars wherea prediction was made.
Only the suite of classifierscould natively handle multi-assignment of classes:recall was calculated as the fraction of correctly pre-dicted instances to all 2984 possible headword?classassignments, despite the fact that four of the systemscould not make 220 of the classifications.3.2 ResultsThe observed precision, recall, and F-scores of thevarious systems are shown in Table 1.
The bestF-score is observed for the Web frequency system,which also had the highest recall.
The best precisionwas observed for the corpus frequency system, butwith very low recall ?
about 85% of the wordformscould not be assigned to a class (the correspondingfigure for the Web system was about 9%).
Conse-quently, we attempted a number of back-off strate-gies so as to improve the recall of this system.The results for backing off the corpus frequencysystem to the Web model, the two maximum entropymodels, and two baselines (the majority class, andthe semantically empty classifier) are shown in Ta-ble 2.
Using a Web back-off was nearly identical tothe basic Web system: most of the correct assign-ments being made by the corpus frequency systemwere also being captured through Web frequencies,which indicates that these are the easier, high fre-quency entries.
Backing off to the machine learn-ing models performed the same or slightly betterthan using the machine learning model by itself.
Ittherefore seems that the most balanced corpus-basedmodel should take this approach.The fact that the Web frequency system had thebest performance belies the ?noisiness?
of the Web,in that one expects to observe errors caused bycarelessness, laziness (e.g.
using buah despite amore specific classifier being available), or noise(e.g.
Indonesian count classifier attestation; more onthis below).
While the corpus of ?clean?, hand-constructed data did have a precision improvementover the Web system, the back-off demonstrates thatit was not substantially better over those entries thatcould be classified from the corpus data.4 DiscussionAs with many classification tasks, the Web-basedmodel notably outperformed the corpus-based mod-els when used to predict count classifiers of Malaynoun types, particularly in recall.
In a type-wise lex-icon, precision is probably the more salient evalua-tion metric, as recall is more meaningful on tokens,and a low-precision lexicon is often of little utility;the Web system had at least comparable precisionfor the entries able to be classified by the corpus-based systems.We expected that the heterogeneity of the Web,particularly confusion caused by a preponderance ofIndonesian, would cause performance to drop, butthis was not the case.
The Ethnologue estimates thatthere are more speakers of Indonesian than Malay(Gordon, 2005), and one would expect the Web dis-tribution to reflect this.
Also, there are systematicdifferences in the way count classifiers are used inthe two languages, despite the intelligibility; com-pare ?five photographs?
: lima keping foto in Malayand lima lembar foto, lima foto in Indonesian.While the use of count classifiers is obligatory inMalay, it is optional in Indonesian for lower reg-isters.
Also, many classifiers that are available inMalay are not used in Indonesian, and the small setof Indonesian count classifiers that are not used inMalay do not form part of the query set, so no confu-sion results.
Consequently, it seems that greater dif-ficulty would arise when attempting to predict countclassifiers for Indonesian nouns, as their optional-ity and blocking from Malay cognates would intro-duce noise in cases where language identificationhas not been used to generate the corpus (like the71Web) ?
hand-constructed corpora might be neces-sary in that case.
Furthermore, the Web system ben-efits from a very simple surface form, namely se-CL NOUN: languages that permit floating quantifica-tion, like Japanese, or require classifiers for stativeverb modification, like Thai, would need many morequeries or lower-precision queries to capture most ofthe cues available from the corpus.
We intend to ex-amine these phenomena in future work.An important contrast is noted between the ?un-supervised?
methods of the corpus-frequency sys-tems and the ?supervised?
machine learning meth-ods.
One presumed advantage of unsupervised sys-tems is the lack of pre-annotated training data re-quired.
In this case, a comparable time investmentby a lexicographer would be required to generate theset of surface forms for the corpus-frequency mod-els.
The performance dictates that the glosses for theWeb system give the most value for lexicographerinput; however, for other languages or other lexicalproperties, generating a set of high-precision, high-recall glosses is often non-trivial.
If the Web is notused, having both training data and high-precision,low-recall glosses is valuable.5 ConclusionWe examine an approach for using Web and cor-pus data to predict the preferred generation form forcounting nouns in Malay, and observed greater pre-cision than machine learning methods that do notrequire a presupplied gloss.
Most Web?as?corpusresearch tends to focus on English; as the Web in-creases in multilinguality, it becomes an importantresource for medium- and low-density languages.This task was quite simple, with glosses amenable toWeb approaches, and is promising for automaticallyextending the coverage of a Malay lexicon.
How-ever, we expect that the Malay glosses will blockreadings of Indonesian classifiers, and classifiers inother languages will require different strategies; weintend to examine this in future work.AcknowledgementsWe would like to thank Francis Bond for his valuable in-put on this research.
NICTA is funded by the Australiangovernment as represented by Department of Broadband,Communication and Digital Economy, and the AustralianResearch Council through the ICT Centre of Excellenceprogramme.ReferencesM.
Adriani, J. Asian, B. Nazief, S.M.M.
Tahaghoghi,and H.E.
Williams.
2007.
Stemming Indonesian:A confix-stripping approach.
ACM Transactions onAsian Language Information Processing, 6.T.
Baldwin and S. Awab.
2006.
Open source corpus anal-ysis tools for Malay.
In Proc.
of the 5th InternationalConference on Language Resources and Evaluation,pages 2212?5, Genoa, Italy.T.
Baldwin.
2007.
Scalable deep linguistic processing:Mind the lexical gap.
In Proc.
of the 21st Pacific AsiaConference on Language, Information and Computa-tion, pages 3?12, Seoul, Korea.F.
Bond and K. Paik.
1997.
Classifying correspondencein Japanese and Korean.
In Proc.
of the 3rd Confer-ence of the Pacific Association for Computational Lin-guistics, pages 58?67, Tokyo, Japan.F.
Bond and K. Paik.
2000.
Reusing an ontology togenerate numeral classifiers.
In Proc.
of the 19th In-ternational Conference on Computational Linguistics,pages 90?96, Saarbru?cken, Germany.R.G.
Gordon, Jr, editor.
2005.
Ethnologue: Languagesof the World, Fifteenth Edition.
SIL International.R.H.
Gusmita and Ruli Manurung.
2008.
Some initialexperiments with Indonesian probabilistic parsing.
InProc.
of the 2nd International MALINDO Workshop,Cyberjaya, Malaysia.A.
Korhonen.
2002.
Subcategorization Acquisition.Ph.D.
thesis, University of Cambridge, Cambridge,UK.M.
Lapata and F. Keller.
2004.
The web as a base-line: Evaluating the performance of unsupervisedweb-based models for a range of NLP tasks.
In Proc.of the 4th International Conference on Human Lan-guage Technology Research and 5th Annual Meetingof the NAACL, pages 121?128, Boston, USA.J.
Nicholson and T. Baldwin.
2008.
Learning countclassifier preferences of Malay nouns.
In Proc.
of theAustralasian Language Technology Association Work-shop, pages 115?123, Hobart, Australia.C.K.
Quah, F. Bond, and T. Yamazaki.
2001.
De-sign and construction of a machine-tractable Malay-English lexicon.
In Proc.
of the 2nd Biennial Confer-ence of ASIALEX, pages 200?205, Seoul, Korea.K.
Shirai, T. Tokunaga, C-R. Huang, S-K. Hsieh, T-Y.
Kuo, V. Sornlertlamvanich, and T. Charoenporn.2008.
Constructing taxonomy of numerative classi-fiers for Asian languages.
In Proc.
of the Third Inter-national Joint Conference on Natural Language Pro-cessing, Hyderabad, India.V.
Sornlertlamvanich, W. Pantachat, and S. Meknavin.1994.
Classifier assignment by corpus-based ap-proach.
In Proc.
of the 15th International Conferenceon Computational Linguistics, pages 556?561, Kyoto,Japan.72
