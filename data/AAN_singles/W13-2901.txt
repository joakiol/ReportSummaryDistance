Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 1?10,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSentence Simplification as Tree TransductionDan FeblowitzComputer Science DepartmentPomona CollegeClaremont, CAdjf02007@mymail.pomona.eduDavid KauchakComputer Science DepartmentMiddlebury CollegeMiddlebury, VTdkauchak@middlebury.eduAbstractIn this paper, we introduce a syntax-basedsentence simplifier that models simplifi-cation using a probabilistic synchronoustree substitution grammar (STSG).
To im-prove the STSG model specificity we uti-lize a multi-level backoff model with addi-tional syntactic annotations that allow forbetter discrimination over previous STSGformulations.
We compare our approachto T3 (Cohn and Lapata, 2009), a re-cent STSG implementation, as well astwo state-of-the-art phrase-based sentencesimplifiers on a corpus of aligned sen-tences from English and Simple EnglishWikipedia.
Our new approach performssignificantly better than T3, similarly tohuman simplifications for both simplicityand fluency, and better than the phrase-based simplifiers for most of the evalua-tion metrics.1 IntroductionText simplification is aimed at reducing the read-ing and grammatical complexity of text while re-taining the meaning.
Text simplification has ap-plications for children, language learners, peoplewith disabilities (Carroll et al 1998; Feng, 2008)and in technical domains such as medicine (El-hadad, 2006), and can be beneficial as a prepro-cessing step for other NLP applications (Vickreyand Koller, 2008; Miwa et al 2010).
In this paperwe introduce a new probabilistic model for sen-tence simplification using synchronous tree sub-stitution grammars (STSG).Synchronous grammars can be viewed as simul-taneously generating a pair of recursively relatedstrings or trees (Chiang, 2006).
STSG grammarrules contain pairs of tree fragments called ele-mentary trees (Eisner, 2003; Cohn and Lapata,2009; Yamangil and Shieber, 2010).
The leavesof an elementary tree can be either terminal, lex-ical nodes or aligned nonterminals (also referredto as variables or frontier nodes).
Because ele-mentary trees may have any number of internalnodes structured in any way STSGs allow for morecomplicated derivations not expressible with othersynchronous grammars.To simplify an existing tree, an STSG gram-mar is used as a tree transducer.
Figure 1 showssome example simplification STSG rules writtenin transductive form.
As a transducer the gram-mar rules take an elementary tree and rewrite it asthe tree on the right-hand side of the rule.
For ex-ample, the first rule in Figure 1 would make thetransformationSVP1VPADVPRBoccasionallyMDmayNP0SVP1NP0,,ADVPRBsometimeschanging ?may occasionally?
to ?sometimes ,?
andmoving the noun phrase from the beginning of thesentence to after the comma.
The indices on thenonterminals indicate alignment and transductioncontinues recursively on these aligned nontermi-nals until no nonterminals remain.
In the exampleabove, transduction would continue down the treeon the NP and VP subtrees.
A probabilistic STSGhas a probability associated with each rule.One of the key challenges in learning an STSGfrom an aligned corpus is determining the rightlevel of specificity for the rules: too general andthey can be applied in inappropriate contexts; toospecific, and the rules do not apply in enough con-texts.
Previous work on STSG learning has regu-lated the rule specificity based on elementary treedepth (Cohn and Lapata, 2009), however, this ap-proach has not worked well for text simplifica-1S(NP0 VP(MD(may) ADVP(RB(occasionally))) VP1) ?
S(ADVP(RB(sometimes)) ,(,) NP0 VP1)NP(NNS0) ?
NP(NNS0)NP(JJ0 NNS1) ?
NP(JJ0 NNS1)VP(VB0 PP(IN(in) NP1)) ?
VP(VB0 NP1)VB(assemble), ?
VB(join)JJ(small) ?
JJ(small)NNS(packs) ?
NNS(packs)NNS(jackals) ?
NNS(jackals)Figure 1: Example STSG rules representing the maximally general set for the aligned trees in Figure 2.The rules are written in transductive form.
Aligned nonterminals are indicated by indices.tion (Coster and Kauchak, 2011a).
In this pa-per, we take a different approach and augment thegrammar with additional information to increasethe specificity of the rules (Galley and McKeown,2007).
We combine varying levels of grammaraugmentation into a single probabilistic backoffmodel (Yamangil and Nelken, 2008).
This ap-proach creates a model that uses specific ruleswhen the context has been previously seen in thetraining data and more general rules when the con-text has not been seen.2 Related WorkOur formulation is most closely related to the T3model (Cohn and Lapata, 2009), which is alsobased on the STSG formalism.
T3 was devel-oped for the related problem of text compression,though it supports the full range of transforma-tion operations required for simplification.
We usea modified version of their constituent alignmentand rule extraction algorithms to extract the ba-sic STSG rules with three key changes.
First, T3modulates the rule specificity based on elemen-tary tree depth, while we use additional grammarannotations combined via a backoff model allow-ing for a broader range of context discrimination.Second, we learn a probabilistic model while T3learns the rule scores discriminatively.
T3?s dis-criminative training is computationally prohibitivefor even modest sized training sets and a proba-bilistic model can be combined with other proba-bilities in a meaningful way.
Third, our implemen-tation outputs an n-best list which we then rerankbased on a trained log-linear model to select thefinal candidate.Zhu et al(2010) suggest a probabilistic, syntax-based approach to text simplification.
Unlike theSTSG formalism, which handles all of the trans-formation operations required for sentence simpli-fication in a unified framework, their model usesa combination of hand-crafted components, eachdesigned to handle a different transformation op-eration.
Because of this model rigidity, their sys-tem performed poorly on evaluation metrics thattake into account the content and relative to othersimplification systems (Wubben et al 2012).Woodsend and Lapata (2011) introduce a quasi-synchronous grammar formulation and pose thesimplification problem as an integer linear pro-gram.
Their model has similar representational ca-pacity to an STSG, though the learned models tendto be much more constrained, consisting of <1000rules.
With this limited rule set, it is impossibleto model all of the possible lexical substitutionsor to handle simplifications that are strongly con-text dependent.
This quasi-synchronous grammarapproach performed better than Zhu et al(2010)in a recent comparison, but still performed worsethan recent phrase-based approaches (Wubben etal., 2012).A number of other approaches exist that useSimple English Wikipedia to learn a simplifica-tion model.
Yatskar et al(2010) and Biran etal.
(2011) learn lexical simplifications, but do nottackle the more general simplification problem.Coster and Kauchak (2011a) and Wubben et al(2012) use a modified phrase-based model basedon a machine translation framework.
We compareagainst both of these systems.
Qualitatively, wefind that phrasal models do not have the represen-tative power of syntax-based approaches and tendto only make small changes when simplifying.Finally, there are a few early rule-based sim-plification systems (Chandrasekar and Srinivas,1997; Carroll et al 1998) that provide motivationfor recent syntactic approaches.
Feng (2008) pro-vides a good overview of these.3 Probabilistic Tree-to-TreeTransductionWe model text simplification as tree-to-tree trans-duction with a probabilistic STSG acquired from2S1VPVP4PP6NP6NNS8packsJJ7smallINinVB5assembleADVPRBoccasionallyMDmayNP2NNS3jackalsS1VP4NP6NNS8packsJJ7smallVB5joinNP2NNS3jackals,,ADVPRBsometimesFigure 2: An example pair of constituent aligned trees generated by the constituent alignment algorithm.Aligned constituents are indicated with a shared index number (e.g.
NP2 is aligned to NP2).a parsed, sentence-aligned corpus between normaland simplified sentences.
To learn the grammar,we first align tree constituents based on an in-duced word alignment then extract grammar rulesthat are consistent with the constituent alignment.To improve the specificity of the grammar weaugment the original rules with additional lexi-cal and positional information.
To simplify a sen-tence based on the learned grammar, we generatea finite-state transducer (May and Knight, 2006)and use the transducer to generate an n-best listof simplifications.
We then rerank the n-best listof simplifications using a trained log-linear modeland output the highest scoring simplification.
Thesubsections below look at each of these steps inmore detail.
Throughout the rest of this paper, wewill refer to the unsimplified text/trees as normaland the simplified variants as simple.3.1 Rule ExtractionGiven a corpus of pairs of trees representing nor-mal and simplified sentences, the first step is toextract a set of basic STSG production rules fromeach tree pair.
We used a modified version of thealgorithm presented by Cohn and Lapata (2009).Due to space constraints, we only present herea brief summary of the algorithm along with ourmodifications to the original algorithm.
See Cohnand Lapata (2009) for more details.Word-level alignments are learned usingGiza++ (Och and Ney, 2000) then tree nodes (i.e.constituents) are aligned if: there exists at leastone pair of nodes below them that is aligned andall nodes below them are either aligned to a nodeunder the other constituent or unaligned.
Giventhe constituent alignment, we then extract theSTSG production rules.
Because STSG rules canhave arbitrary depth, there are often many possiblesets of rules that could be extracted from a pairof trees.1 Following Cohn and Lapata (2009)we extract the maximally general rule set froman aligned pair of input trees that is consistentwith the alignment: the set of rules capable ofsynchronously deriving the original aligned treepair consisting of rules with the smallest depth.Figure 2 shows an example tree pair that hasbeen constituent aligned and Figure 1 shows theextracted STSG rules.We modify the constituent alignment algorithmfrom Cohn and Lapata (2009) by adding the re-quirement that if node b with parent a are bothaligned to node z and its parent y, we only alignthe pairs (a, y) and (b, z), i.e.
align the childrenand align the parents.
This eliminates a commonoccurrence where too many associations are madebetween a pair of preterminal nodes and their chil-dren.
For example, for the sentences shown in Fig-ure 2 the word alignment contains ?assemble?aligned to ?join?.
Under the original definitionfour aligned pairs would be generated:VBassembleVBjoinbut only two under our revised definition:VBassembleVBjoinThis revised algorithm reduces the size of thealignment, decreasing the number of cases whichmust be checked during grammar extraction whilepreserving the intuitive correspondence.1There is always at least one set of rules that can generatea tree pair consisting of the entire trees.33.2 Grammar GenerationDuring the production rule extraction process, weselect the production rules that are most general.More general rules allow the resulting transducerto handle more potential inputs, but can also re-sult in unwanted transformations.
When generat-ing the grammar, this problem can be mitigated byalso adding more specific rules.Previous approaches have modulated rule speci-ficity by incorporating rules of varying depth inaddition to the maximally general rule set (Cohnand Lapata, 2009), though this approach can beproblematic.
Consider the aligned subtrees rootedat nodes (VP4, VP4) in Figure 2.
An STSG learn-ing algorithm that controls rule specificity basedon depth must choose between generating the rule:VP(VB0 PP(IN(in) NP1)) ?
VP(VB0 NP1)which drops the preposition, or a deeper rule thatincludes the lexical leaves such as:VP(VB(assemble) PP(IN(in) NP1))?
VP(VB(join) NP1)orVP(VB(assemble) PP(IN(in) NP(JJ0 NNS1))) ?VP(VB(join) NP(JJ0 NNS1))If either of the latter rule forms is chosen, theapplicability is strongly restricted because of thespecificity and lexical requirement.
If the formerrule is chosen and we apply this rule we couldmake the following inappropriate transformation:VPPPNPNNcafeteriaDTtheINinVBeatVPNPNNcafeteriaDTtheVBeatsimplifying ?eat in the cafeteria?
to ?eat the cafe-teria?.We adopt a different approach to increase therule specificity.
We augment the production rulesand resulting grammar with several parse tree an-notations shown previously to improve SCFG-based sentence compression (Galley and McKe-own, 2007) as well as parsing (Collins, 1999): par-ent annotation, head-lexicalization, and annotationwith the part of speech of the head word.Following Yamangil and Nelken (2008), welearn four different models and combine them intoa single backoff model.
Each model level in-creases specificity by adding additional rule anno-tations.
Model 1 contains only the original pro-duction rules.
Model 2 adds parent annotation,Model 3 adds the head child part of speech andModel 4 adds head child lexicalization.
The headchild was determined using the set of rules fromCollins (1999).
Figure 3 shows the four differentmodel representations for the VP rule above.3.3 Probability EstimationWe train each of the four models individually us-ing maximum likelihood estimation over the train-ing corpus, specifically:p(s|n) =count(s ?
n)count(n)where s and n are tree fragments with that level?sannotation representing the right and left sides ofthe rule respectively.During simplification, we start with the mostspecific rules, i.e.
Model 4.
If a tree fragmentwas not observed in the training data at that modellevel, we repeatedly try a model level simpler untila model is found with the tree fragment (Yamangiland Nelken, 2008).
We then use the probabilitydistribution given by that model.
A tree fragmentonly matches at a particular level if all of the anno-tation attributes match for all constituents.
If noneof the models contain a given tree fragment we in-troduce a rule that copies the tree fragment withprobability one.Two types of out-of-vocabulary problems canoccur and the strategy of adding copy rules pro-vides robustness against both.
In the first, an inputcontains a tree fragment whose structure has neverbeen seen in training.
In this case, copy rules allowthe structure to be reproduced, leaving the systemto make more informed changes lower down in thetree.
In the second, the input contains an unknownword.
This only affects transduction at the leavesof the tree since at the lower backoff levels nodesare not annotated with words.
Adding copy rulesallows the program to retain, replace, or delete un-seen words based only on the probabilities of ruleshigher up for which it does have estimates.
In bothcases, the added copy rules make sure that any in-put tree will have an output.3.4 Decoding and RerankingGiven a parsed sentence to simplify and the prob-abilistic STSG grammar, the last step is to find themost likely transduction (i.e.
simplification) of theinput tree based on the grammar.
To accomplishthis, we convert the STSG grammar into an equiv-alent finite tree-to-tree transducer: each STSG4Model 1: VP (VB0 PP (IN(in) NP1))?
VP (VB0 NP1)Model 2: VP?VP (VB?VP0 PP?VP (IN?PP (in) NP?PP1))?
VP?S (VB?VP0 NP?VP1)Model 3: VP[VB]?VP (VB?VP0 PP[NNS]?VP (IN?PP (in) NP[NNS]?PP1))?VP[VB]?S (VB?VP0 NP[NNS]?VP1)Model 4: VP[VB-assemble]?VP (VB[assemble]?VP0 PP[NNS-packs]?VP (IN[in]?PP (in) NP[NNS-packs]?PP1))?VP[VB-join]?S (VB[join]?VP0 NP[NNS-packs]?VP1)Figure 3: The four levels of rule augmentation for an example rule ranging from Model 1 with noadditional annotations to Model 4 with all annotations.
The head child and head child part of speech areshown in square brackets and the parent constituent is annotated with ?.grammar rule represents a state transition and isweighted with the grammar rule?s probability.
Wethen use the Tiburon tree automata package (Mayand Knight, 2006) to apply the transducer to theparsed sentence.
This yields a weighted regulartree grammar that generates every output tree thatcan result from rewriting the input tree using thetransducer.
The probability of each output tree inthis grammar is equal to the product of the proba-bilities of all rewrite rules used to produce it.Using this output regular tree grammar andTiburon, we generate the 10,000 most probableoutput trees for the input parsed sentence.
Wethen rerank this candidate list based on a log-linearcombination of features:- The simplification probability based on theSTSG backoff model.- The probability of the output tree?s yield, asgiven by an n-gram language model trained onthe simple side of the training corpus using theIRSTLM Toolkit (Federico et al 2008).- The probability of the sequence of the part ofspeech tags in the output tree, as given by an n-gram model trained on the part of speech tags ofthe simple side of the training corpus.- A two-sided length penalty decreasing the scoreof output sentences whose length, normalized bythe length of the input, deviates from the trainingcorpus mean, found empirically to be 0.85.The first feature represents the simplification like-lihood based on the STSG grammar describedabove.
The next two features ensure that outputsare well-formed according to the language usedin Simple English Wikipedia.
Finally, the lengthpenalty is used to prevent both over-deletion andover-insertion of out-of-source phrases.
In addi-tion, the length feature mean could be reduced orincreased to encourage shorter or longer simplifi-cations if desired.The weights of the log-linear model are opti-mized using random-restart hill-climbing search(Russell and Norvig, 2003) to maximize BLEU(Papineni et al 2002) on a development set.24 Experiment SetupTo train and evaluate the systems we used the dataset from Coster and Kauchak (2011b) consistingof 137K aligned sentence pairs between SimpleEnglish Wikipedia and English Wikipedia.
Thesentences were parsed using the Berkeley Parser(Petrov and Klein, 2007) and the word alignmentsdetermined using Giza++ (Och and Ney, 2000).We used 123K sentence pairs for training, 12K fordevelopment and 1,358 for testing.We compared our system (SimpleTT ?
simpletree transducer) to three other simplification ap-proaches:T3: Another STSG-based approach (Cohn and La-pata, 2009).
Our approach shares similar con-stituent alignment and rule extraction algorithms,but our approach differs in that it is generativeinstead of discriminative, and T3 increases rulespecificity by increasing rule depth, while we em-ploy a backoff model based on grammar augmen-tation.
In addition, we employ n-best rerankingbased on a log-linear model that incorporates anumber of additional features.The code for T3 was obtained from the au-thors.3 Due to performance limitations, T3 wasonly trained on 30K sentence pairs.
T3 was run onthe full training data for two weeks, but it neverterminated and required over 100GB of memory.The slow algorithmic step is the discriminativetraining, which cannot be easily parallelized.
T3was tested for increasing amounts of data up to2BLEU was chosen since it has been used successfully inthe related field of machine translation, though this approachis agnostic to evaluation measure.3http://staffwww.dcs.shef.ac.uk/people/T.Cohn/t3/530K training pairs and the results on the automaticevaluation measures did not improve.Moses-Diff: A phrase-based approach based onthe Moses machine translation system (Koehn etal., 2007) that selects the simplification from the10-best output list that is most different from theinput sentence (Wubben et al 2012).
Moses-Diffhas been shown to perform better than a numberof recent syntactic systems including Zhu et al(2010) and Woodsend and Lapata (2011).Moses-Del: A phrase-based approach also basedon Moses which incorporates phrasal deletion(Coster and Kauchak, 2011b).
The code was ob-tained from the authors.For an additional data point to understand thebenefit of the grammar augmentation, we alsoevaluated a deletion-only system previously usedfor text compression and a variant of that sys-tem that included the grammar augmentation de-scribed above.
K&M is a synchronous contextfree grammar-based approach (Knight and Marcu,2002) and augm-K&M adds the grammar aug-mentation along with the four backoff levels.There are currently no standard evaluation met-rics for text simplification.
Following previouswork (Zhu et al 2010; Coster and Kauchak,2011b; Woodsend and Lapata, 2011; Wubbenet al 2012) we evaluated the systems usingautomatic metrics to analyze different systemcharacteristics and human evaluations to judge thesystem quality.Automatic Evaluation- BLEU (Papineni et al 2002): BLEU measuresthe similarity between the system output and ahuman reference and has been used successfullyin machine translation.
Higher BLEU scores arebetter, indicating an output that is more similarto the human reference simplification.- Oracle BLEU: For each test sentence we gener-ate the 1000-best output list and greedily selectthe entry with the highest sentence-level BLEUscore.
We then calculate the BLEU score overthe entire test set for all such greedily selectedsentences.
The oracle score provides an analy-sis of the generation capacity of the model andgives an estimate of the upper bound on theBLEU score attainable through reranking.- Length ratio: The ratio of the length of the orig-inal, unsimplified sentence and the system sim-plified sentence.Human EvaluationFollowing previous work (Woodsend and Lapata,2011; Wubben et al 2012) we had humans judgethe three simplification systems and the humansimplifications from Simple English Wikipedia(denoted SimpleWiki)4 based on three metrics:simplicity, fluency and adequacy.
Simplicity mea-sures how simple the output is, fluency measuresthe quality of the language and grammatical cor-rectness of the output, and adequacy measureshow well the content is preserved.
For the flu-ency experiments, the human evaluators were justshown the system output.
For simplicity and ade-quacy, in addition to the system output, the orig-inal, unsimplified sentence was also shown.
Allmetrics were scored on a 5-point Likert scale withhigher indicating better.We used Amazon?s Mechanical Turk (MTurk)5to collect the human judgements.
MTurk has beenused by many NLP researchers, has been shownto provide results similar to other human annota-tors and allows for a large population of annotatorsto be utilized (Callison-Burch and Dredze, 2010;Gelas et al 2011; Zaidan and Callison-Burch,2011).We randomly selected 100 sentences from thetest set where all three systems made some changeto the input sentence.
We chose sentences whereall three systems made a change to focus on thequality of the simplifications made by the systems.For each sentence we collected scores from 10judges, for each of the systems, for each of thethree evaluation metrics (a total of 100*10*3*3 =9000 annotations).
The scores from the 10 judgeswere averaged to give a single score for each sen-tence and metric.
Judges were required to bewithin the U.S. and have a prior acceptance rateof 95% or higher.5 ResultsAutomatic evaluationTable 1 shows the results of the automatic eval-uation metrics.
SimpleTT performs significantlybetter than T3, the other STSG-based model, andobtains the second highest BLEU score behindonly Moses-Del.
SimpleTT has the highest oracleBLEU score, indicating that the syntactic model ofSimpleTT allows for more diverse simplifications4T3 was not included in the human evaluation due to thevery poor quality of the output based on both the automaticmeasures and based on a manual review of the output.5https://www.mturk.com/6System BLEU Oracle LengthRatioSimpleTT 0.564 0.663 0.849Moses-Diff 0.543 ??
0.960Moses-Del 0.605 0.642 0.991T3 0.244 ???
0.581K&M 0.406 0.602 0.676augm-K&M 0.498 0.609 0.826corpus mean ?
?
0.85Table 1: Automatic evaluation scores for all sys-tems tested and the mean values from the trainingcorpus.
?Moses-Diff uses the n-best list to choosecandidates and therefore is not amenable to oraclescoring.
?
?T3 only outputs the single best simpli-fication.than the phrase-based models and may be moreamenable to future reranking techniques.
Sim-pleTT also closely matches the in-corpus meanof the length ratio seen by human simplifications,though this can be partially explained by the lengthpenalty in the log-linear model.Moses-Del obtains the highest BLEU score, butaccomplishes this with only small changes to theinput sentence: the length of the simplified sen-tences are only slightly different from the original(a length ratio of 0.99).
Moses-Diff has the low-est BLEU score of the three simplification systemsand while it makes larger changes than Moses-Del it still makes much smaller changes than Sim-pleTT and the human simplifications.T3 had significant problems with over-deletingcontent as indicated by the low length ratio whichresulted in a very low BLEU score.
This issuehas been previously noted by others when usingT3 for text compression (Nomoto, 2009; Marsi etal., 2010).The two deletion-only systems performedworse than the three simplification systems.
Com-paring the two systems shows the benefit of thegrammar augmentation: augm-K&M has a signif-icantly higher BLEU score than K&M and alsoavoided the over-deletion that occurred in the orig-inal K&M system.
The additional specificity ofthe rules allowed the model to make better deci-sions for which content to delete.Human evaluationTable 2 shows the human judgement scores forthe simplification approaches for the three differ-ent metrics averaged over the 100 sentences andTable 3 shows the pairwise statistical significancecalculations between each system based on a two-simplicity fluency adequacySimpleWiki 3.45 3.93 3.42SimpleTT 3.55 3.80 3.09Moses-Diff 3.07 3.64 3.91Moses-Del 3.19 3.74 3.86Table 2: Human evaluation scores on a 5-pointLikert scale averaged over 100 sentences.tailed paired t-test.
Overall, SimpleTT performedwell with simplicity and fluency scores that werecomparable to the human simplifications.
Sim-pleTT was too aggressive at removing content, re-sulting in lower adequacy scores.
This phenom-ena was also seen in the human simplifications andmay be able to be corrected in future variations byadjusting the sentence length target.The human evaluations highlight the trade-offbetween the simplicity of the output and theamount of content preserved.
For simplicity, Sim-pleTT and the human simplifications performedsignificantly better than both the phrase-based sys-tems.
However, simplicity does come with a cost;both SimpleTT and the human simplifications re-duced the length of the sentences by 15% on aver-age.
This content reduction resulted in lower ad-equacy than the phrase-based systems.
A similartrade-off has been previously shown for text com-pression, balancing content versus the amount ofcompression (Napoles et al 2011).For fluency, SimpleTT again scored similarly tothe human simplifications.
SimpleTT performedsignificantly better than Moses-Diff and slightlybetter than Moses-Del, though the difference wasnot statistically significant.As an aside, Moses-Del performs slightly bet-ter than Moses-Diff overall.
They perform simi-larly on adequacy and Moses-Del performs betteron simplicity and Moses-Diff performs worse rel-ative to the other systems on fluency.Qualitative observationsSimpleTT tended to simplify by deleting prepo-sitional, adjective, and adverbial phrases, and bytruncating conjunctive phrases to one of their con-juncts.
This often resulted in outputs that weresyntactically well-formed with only minor infor-mation loss, for example, it converts?The Haiti national football team is the na-tional team of Haiti and is controlled by theFe?de?ration Hat?
?enne de Football.
?to7SimplicitySimpleWiki Moses-Diff Moses-DelSimpleTT ???
??
?SimpleWiki ???
??
?Moses-Diff ?FluencySimpleWiki Moses-Diff Moses-DelSimpleTT ?SimpleWiki ???
?Moses-DiffAdequacySimpleWiki Moses-Diff Moses-DelSimpleTT ??
???
??
?SimpleWiki ???
??
?Moses-DiffTable 3: Pairwise statistical significance test re-sults between systems for the human evaluationsbased on a paired t-test.
The number of arrows de-notes significance with one, two and three arrowsindicating p < 0.05, p < 0.01 and p < 0.001respectively.
The direction of the arrow points to-wards the system that performed better.
?The Haiti national football team is the na-tional football team of Haiti.
?which only differs from the human reference byone word.SimpleTT also produces a number of interestinglexical and phrasal substitutions, including:football striker ?
football playerfootball defender ?
football playerin order to ?
toknown as ?
calledmember ?
partT3, on the other hand, tended to over-delete con-tent, for example simplifying:?In earlier times, they frequently lived on theoutskirts of communities, generally in squalor.
?to just?A lived?.As we saw in the automatic evaluation results,the phrase-based systems tended to make fewerchanges to the input and those changes it did maketended to be more minor.
Moses-Diff was moreaggressive about making changes, though it wasmore prone to errors since the simplifications cho-sen were more distant from the input sentence thanother options in the n-best list.6 Conclusions and Future workIn this paper, we have introduced a new prob-abilistic STSG approach for sentence simplifica-tion, SimpleTT.
We improve upon previous STSGapproaches by: 1) making the model probabilisticinstead of discriminative, allowing for an efficient,unified framework that can be easily interpretedand combined with other information sources, 2)increasing the model specificity using four levelsof grammar annotations combined into a singlemodel, and 3) incorporating n-best list rerankingcombining the model score, language model prob-abilities and additional features to choose the fi-nal output.
SimpleTT performs significantly betterthan previous STSG formulations for text simpli-fication.
In addition, our approach was rated byhuman judges similarly to human simplificationsin both simplicity and fluency and it scored bet-ter than two state-of-the-art phrase-based sentencesimplification systems along many automatic andhuman evaluation metrics.There are a number of possible directions forextending the capabilities of SimpleTT and relatedsystems.
First, while some sentence splitting canoccur in SimpleTT due to sentence split and mergeexamples in the training data, SimpleTT does notexplicitly model this.
Sentence splitting could beincorporated as another probabilistic componentin the model (Zhu et al 2010).
Second, in thiswork, like many previous researchers, we assumeSimple English Wikipedia as our target simplic-ity level.
However, the difficulty of Simple En-glish Wikipedia varies across articles and there aremany domains where the desired simplicity variesdepending on the target consumer.
In the future,we plan to explore how varying algorithm param-eters (for example the length target) affects thesimplicity level of the output.
Third, one of thebenefits of SimpleTT and other probabilistic sys-tems is they can generate an n-best list of can-didate simplifications.
Better reranking of outputsentences could close this gap across all these sys-tems, without requiring deep changes to the under-lying model.ReferencesOr Biran, Samuel Brody, and Noem?ie Elhadad.
2011.Putting it simply: A context-aware approach to lexi-cal simplification.
In Proceedings of ACL.Chris Callison-Burch and Mark Dredze.
2010.
Creat-8ing speech and language data with Amazon?s Me-chanical Turk.
In Proceedings of NAACL-HLTWorkshop on Creating Speech and Language Datawith Amazon?s Mechanical Turk.John Carroll, Gido Minnen, Yvonne Canning, SiobhanDevlin, and John Tait.
1998.
Practical simplifica-tion of English newspaper text to assist aphasic read-ers.
In Proceedings of AAAI Workshop on Integrat-ing AI and Assistive Technology.Raman Chandrasekar and Bangalore Srinivas.
1997.Automatic induction of rules for text simplification.In Knowledge Based Systems.David Chiang.
2006.
An introduction to synchronousgrammars.
Part of a tutorial given at ACL.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
Journal of ArtificialIntelligence Review.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.William Coster and David Kauchak.
2011a.
Learningto simplify sentences using Wikipedia.
In Proceed-ings of the Workshop on Monolingual Text-To-TextGeneration.William Coster and David Kauchak.
2011b.
SimpleEnglish Wikipedia: A new text simplification task.In Proceedings of ACL.Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proceedingsof ACL.Noemie Elhadad.
2006.
Comprehending technicaltexts: predicting and defining unfamiliar terms.
InProceedings of AMIA.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: An open source toolkit forhandling large scale language models.
In Proceed-ings of Interspeech, Brisbane, Australia.Lijun Feng.
2008.
Text simplification: A survey.CUNY Technical Report.Michel Galley and Kathleen McKeown.
2007.
Lex-icalized Markov grammars for sentence compres-sion.
In Proceedings of HLT-NAACL.Hadrien Gelas, Solomon Teferra Abate, Laurent Be-sacier, and Francois Pellegrino.
2011.
Evaluation ofcrowdsourcing transcriptions for African languages.In Interspeech.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: a probabilistic ap-proach to sentence compression.
Artificial Intelli-gence.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of ACL.Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Wal-ter Daelemans.
2010.
On the limits of sentencecompression by deletion.
In Empirical Methods inNLG.Jonathan May and Kevin Knight.
2006.
Tiburon: Aweighted tree automata toolkit.
In Proceedings ofCIAA.Makoto Miwa, Rune Saetre, Yusuke Miyao, andJun?ichi Tsujii.
2010.
Entity-focused sentence sim-plification for relation extraction.
In Proceedings ofCOLING.Courtney Napoles, Benjamin Van Durme, and ChrisCallison-Burch.
2011.
Evaluating sentence com-pression: pitfalls and suggested remedies.
In Pro-ceedings of the Workshop on Monolingual Text-To-Text Generation.Tadashi Nomoto.
2009.
A comparison of model freeversus model intensive approaches to sentence com-pression.
In Proceedings of EMNLP.Franz Och and Hermann Ney.
2000.
Improved statisti-cal alignment models.
In Proceedings of ACL.Kishore Papineni, Kishore Papineni, Salim Roukos,Salim Roukos, Todd Ward, Todd Ward, Wei jingZhu, and Wei jing Zhu.
2002.
BLEU: A methodfor automatic evaluation of machine translation.
InProceedings of ACL.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HTL-NAACL.Stuart Russell and Peter Norvig.
2003.
Artificial intel-ligence: A modern approach.David Vickrey and Daphne Koller.
2008.
Sentencesimplification for semantic role labeling.
In Pro-ceedings of ACL.Kristian Woodsend and Mirella Lapata.
2011.
Learn-ing to simplify sentences with quasi-synchronousgrammar and integer programming.
In Proceedingsof EMNLP.Sander Wubben, Antal van den Bosch, and EmielKrahmer.
2012.
Sentence simplification by mono-lingual machine translation.
In Proceedings of ACL.Elif Yamangil and Rani Nelken.
2008.
Miningwikipedia revision histories for improving sentencecompression.
In Proceedings of HLT-NAACL.9Elif Yamangil and Stuart Shieber.
2010.
Bayesian syn-chronous tree-substitution grammar induction andits application to sentence compression.
In Proceed-ings of ACL.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of sim-plicity: Unsupervised extraction of lexical simpli-fications from Wikipedia.
In Proceedings of HLT-NAACL.Omar F. Zaidan and Chris Callison-Burch.
2011.Crowdsourcing translation: Professional qualityfrom non-professionals.
In Proceedings of ACL.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of ICCL.10
