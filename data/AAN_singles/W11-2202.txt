Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 2?12,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsStructured Databases of Named Entities from Bayesian NonparametricsJacob Eisenstein Tae Yano William W. Cohen Noah A. Smith Eric P. XingSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{jacobeis,taey,wcohen,nasmith,epxing}@cs.cmu.eduAbstractWe present a nonparametric Bayesian ap-proach to extract a structured database of enti-ties from text.
Neither the number of entitiesnor the fields that characterize each entity areprovided in advance; the only supervision isa set of five prototype examples.
Our methodjointly accomplishes three tasks: (i) identify-ing a set of canonical entities, (ii) inferring aschema for the fields that describe each entity,and (iii) matching entities to their references inraw text.
Empirical evaluation shows that theapproach learns an accurate database of enti-ties and a sensible model of name structure.1 IntroductionConsider the task of building a set of structuredrecords from a collection of text: for example, ex-tracting the names of people or businesses fromblog posts, where each full name decomposes intofields corresponding to first-name, last-name, title,etc.
To instruct a person to perform this task, onemight begin with a few examples of the records tobe obtained; assuming that the mapping from text torecords is relatively straightforward, no additionalinstruction would be necessary.
In this paper, wepresent a method for training information extractionsoftware in the same way: starting from a small tableof partially-complete ?prototype?
records (Table 1),our system learns to add new entries and fields tothe table, while simultaneously aligning the recordsto text.We assume that the dimensionality of the databaseis unknown, so that neither the number of entriesJohn McCain Sen. Mr.George Bush W. Mr.Hillary Clinton Rodham Mrs.Barack Obama Sen.Sarah PalinTable 1: A set of partially-complete prototype records,which constitutes the only supervision for the system.nor the number of fields is specified in advance.
Toaccommodate this uncertainty, we apply a Bayesianmodel which is nonparametric along three dimen-sions: the assignment of text mentions to entities(making popular entries more likely while always al-lowing new entries); the alignment of individual texttokens to fields (encouraging the re-use of commonfields, but permitting the creation of new fields); andthe assignment of values to entries in the databaseitself (encouraging the reuse of values across entriesin a given field).
By adaptively updating the con-centration parameter of stick-breaking distributioncontrolling the assignment of values to entries in thedatabase, our model can learn domain-specific infor-mation about each field: for example, that titles areoften repeated, while names are more varied.Our system?s input consists of a very small proto-type table and a corpus of text which has been au-tomatically segmented to identify names.
Our de-sired output is a set of structured records in whicheach field contains a single string ?
not a distribu-tion over strings, which would be more difficult tointerpret.
This requirement induces a tight proba-bilistic coupling between the assignment of text tocells in the table, so special care is required to ob-2tain efficient inference.
Our procedure alternatesbetween two phases.
In the first phase, we per-form collapsed Gibbs sampling on the assignmentsof string mentions to rows and columns in the table,while marginalizing the values of the table itself.
Inthe second phase, we apply Metropolis-Hastings toswap the values of columns in the table, while simul-taneously relabeling the affected strings in the text.Our model performs three tasks: it constructs aset of entities from raw text, matches mentions intext with the entities to which they refer, and discov-ers general categories of tokens that appear in names(such as titles and first names).
We are aware ofno existing system that performs all three of thesetasks jointly.
We evaluate on a dataset of politicalblogs, measuring our system?s ability to discovera set of reference entities (recall) while maintain-ing a compact number of rows and columns (pre-cision).
With as few as five partially-complete pro-totype examples, our approach gives accurate tablesthat match well against a manually-annotated refer-ence list.
Our method outperforms a baseline single-link clustering approach inspired by one of the mostsuccessful entries (Elmacioglu et al, 2007) in theSEMEVAL ?Web People Search?
shared task (Ar-tiles et al, 2007).2 Task DefinitionIn this work, we assume that a bag of M mentionsin text have been identified.
The mth mention wmis a sequence of contiguous word tokens (its lengthis denoted Nm) understood to refer to a real-worldentity.
The entities (and the mapping of mentions toentities) are not known in advance.
While our focusin this paper is names of people, the task is definedin a more generic way.Formally, the task is to construct a table x whererows correspond to entities and columns to func-tional fields.
The number of entities and the num-ber of fields are not prespecified.
x?,j denotes thejth column of x, and xi,j is a single word type fill-ing the cell in row i, column j.
An example is Ta-ble 1, where the fields are first-name, last-name, ti-tle, middle-name, and so on.
In addition to the table,we require that each mention be mapped to an en-tity (i.e., a row in the table).
Success at this tasktherefore requires (i) identifying entities, (ii) discov-ering the internal structure of mentions (effectivelycanonicalizing them), and (iii) mapping mentionsto entities (therefore resolving coreference relation-ships among mentions).
Note that this task differsfrom previous work on knowledge base population(e.g., McNamee, 2009) because the schema is notformally defined in advance; rather, the number offields and their meaning must be induced from justa few prototype examples.To incorporate partial supervision, a subset of thetable x is specified manually by an annotator.
Wedenote this subset of ?prototypes?
by x?
; for entriesthat are unspecified by the user, we write x?i,j = ?.Prototypes are not assumed to provide complete in-formation for any entity.3 ModelWe now craft a nonparametric generative story thatexplains both the latent table and the observed men-tions.
The model incorporates three nonparamet-ric components, allowing an unbounded number ofrows (entities) and columns (fields), as well as an un-bounded number of values per column (field values).A plate diagram for the graphical model is shown inFigure 1.A key point is that the column distributions ?range over possible values at the entity level, notover mentions in text.
For example, ?2 might bethe distribution over possible last names and ?3 thedistribution over elected office titles.
Note that ?2would contain a low value for the last name Obama?
which indicates that few people have this lastname ?
even though a very high proportion of men-tions in our data include the string Obama.The user-generated entries (x?)
can still be treatedas the outcome of the generative process: using ex-changeability, we treat these entries as the first sam-ples drawn in each column.
In this work, we treatthem as fully observed, but it is possible to treatthem as noisy and incorporate a stochastic depen-dency between xi,j and x?i,j .4 InferenceWe now develop sampling-based inference for themodel described in the previous section.
We be-gin with a token-based collapsed Gibbs sampler, andthen add larger-scale Metropolis-Hastings moves.3?
?2x ?
?w r ?rc ?r?c ?cFigure 1: A plate diagram for thetext-and-tables graphical model.The upper plate is the table x, andthe lower plate is the set of textualmentions.
Notation is defined in thegenerative model to the right.?
Generate the table entries.
For each column j,?
Draw a concentration parameter ?j from a log-normal distribution,log?j ?
N (?, ?2).?
Draw a distribution over strings from a Dirichlet process ?j ?DP(?j , G0), where the base distribution G0 is a uniform distributionover strings in a fixed character alphabet, up to an arbitrary finite length.?
For each row i, draw the entry xi,j ?
?j .?
Generate the text mentions.?
Draw a prior distribution over rows from a stick-breaking distribution,?r ?
Stick(?r).?
Draw a prior distribution over columns from a stick-breaking distribu-tion, ?c ?
Stick(?c).?
For each mention wm,?
Draw a row in the table rm ?
?r.?
For each word token wm,n (n ?
{1, .
.
.
, Nm}),?
Draw a column in the table cm,n ?
?c.?
Set the text wm,n = xrm,cm,n .4.1 Gibbs samplingA key aspect of the generative process is that theword token wm,n is completely determined by thetable x and the row and column indicators rm andcm,n: given that a token was generated by row iand column j of the table, it must be identical tothe value of xi,j .
Using Bayes?
rule, we can reversethis deterministic dependence: given the values forthe row and column indices, the entries in the tableare restricted to exact matches with the text men-tions that they generate.
This allows us to marginal-ize the unobserved entries in the table.
We can alsomarginalize the distributions ?r, ?c, and ?j , usingthe standard collapsed Gibbs sampling equations forDirichlet processes.
Thus, sampling the row and col-umn indices is all that is required to explore the en-tire space of model configurations.4.1.1 Conditional probability for word tokensThe conditional sampling distributions for bothrows and columns will marginalize the table (be-sides the prototypes x?).
To do this, we must beable to compute P (wm,n | rm = i, cm,n =j, x?,w?
(m,n), r?m, c?
(m,n), ?j), which representsthe probability of generating word wm,n, givenrm = i and cm,n = j.
The notation w?
(m,n), r?m,and c?m,n represent the words, row indices, and col-umn indices for all mentions besides wm,n.
For sim-plicity, we will elide these variables in much of thesubsequent notation.We first consider the case where we have a user-specified entry for the row and column ?i, j??
thatis, if x?ij 6= ?.
Then the probability is simply,P (wm,n | rm = i, cm,n = j, x?, .
.
.)
={1, if x?ij = wm,n0, if x?ij 6= wm,n.
(1)Because the table cell xij is observed, we do notmarginalize over it; we have a generative probabilityof one if the word matches, and zero otherwise.
Ifthe table cell xij is not specified by the user, then wemarginalize over its possible values.
For any givenxij , the probability P (wm,n | xij , rm = i, cm,n =j) is still a delta function, so we have:?P (wm,n | xrm,cm,n)P (xrm,cm,n | .
.
.)
dxrm,cm,n= P (x = wm,n | w?
(m,n), r?m, c?
(m,n), x?, .
.
.
)The integral is equal to the probability of the valueof the cell xrm,cm,n being identical to the stringwm,n, given assignments to all other variables.
Tocompute this probability, we again must considertwo cases: if the cell xi,j has generated some otherstring wm?,n?
then its value must be identical to that4string; otherwise it is unknown.
More formally, forany cell ?i, j?, if ?wm?,n?
: rm?
= i ?
cm?,n?
=j ?
?m?, n??
6= ?m,n?, then P (xi,j = wm?,n?)
= 1;all other strings have zero probability.
If xi,j has notgenerated any other entry, then its probability is con-ditioned on the other elements of the table x. Theknown elements of this table are themselves deter-mined by either the user entries x?
or the observa-tionsw?(m,n).
We can define these known elementsas x?, where x?ij = ?
if x?ij = ?
?
@?m,n?
: rm =i ?
cm,n = j.
Then we can apply the standard Chi-nese restaurant process marginalization to obtain:P (xij | x??
(i,j), ?)
={ N(x??(i,j)=xij)N(x??
(i,j) 6=?
)+?, N(x??
(i,j) = xij) > 0?N(x??
(i,j) 6=?
)+?, N(x??
(i,j) = xij) = 0(2)In our implementation, we maintain the table x?,updating it as we resample the row and column as-signments.
To construct the conditional distributionfor any given entry, we first consult this table, andthen compute the probability in Equation 2 for en-tries where x?ij = ?.4.1.2 Sampling columnsWe can now derive sampling equations for thecolumn indices cm,n.
We first apply Bayes?
ruleto obtain P (cm,n | wm,n, rm, .
.
.)
?
P (cm,n |c?
(m,n), ?c)?P (wm,n | cm,n, rm, x?, .
.
.).
The like-lihood term P (wm,n | cm,n, .
.
.)
is defined in theprevious section; we can compute the first factor us-ing the standard Dirichlet process marginalizationover ?c.
Writing N(c?
(m,n) = j) for the count ofoccurrences of column j in the set c?
(m,n), we ob-tainP (cm,n = j | c?
(m,n), ?c) ={ N(c?(m,n)=j)N(c?
(m,n))+?c, if N(c?
(m,n) = j) > 0?cN(c?
(m,n))+?c, if N(c?
(m,n) = j) = 0(3)4.1.3 Sampling rowsIn principle the row indicators can be sampledidentically to the columns, with the caveat that thegenerative probability P (wm | rm, .
.
.)
is a productacross all Nm tokens in wm.1 However, because of1This relies on the assumption that the values of {cm,n} aremutually independent given c?m.
Future work might applythe tight probabilistic coupling between the row andcolumn indicators, straightforward Gibbs samplingmixes slowly.
Instead, we marginalize the columnindicators while sampling r. Only the likelihoodterm is affected by this change:P (wm | rm,w?m, r?m, .
.
.
)=?jP (c = j | c?m, ?c)P (wm,n | cm,n = j, rm, x?, ?).
(4)The tokens are conditionally independent given therow, so we factor and then explicitly marginalizeover each cm,n.
The chain rule gives the form inEquation 4, which contains terms for the prior overcolumns and the likelihood of the word; these aredefined in Equations 2 and 3.
Note that neither theinferred table x?
nor the heldout column counts c?minclude counts from any of the cells in row m.4.2 Column swapsSuppose that during initialization, we encounter thestring Barry Obama before encountering BarackObama.
We would then put Barry in the first-namecolumn, and put Barack in some other column fornicknames.
After making these initial decisions,they would be very difficult to undo using Gibbssampling ?
we would have to first shift all instancesof Barry to another column, then move an instanceof Barack to the first-name column, and then movethe instances of Barry to the nickname column.
Torectify this issue, we perform sampling on the tableitself, swapping the columns of entries in the table,while simultaneously updating the relevant columnindices of the mentions.In the proposal, we select at random a row t andindices i and j.
In the table, we will swap xt,i withxt,j ; in the text we will swap the values of each cm,nwhenever rm = t and cm,n = i or j.
This pro-posal is symmetric, so no Hastings correction is re-quired.
Because we are simultaneously updating thetable and the column indices, the generative likeli-hood of the words is unchanged; the only changesa more structured model of the ways that fields are combinedwhen mentioning an entity.
For example, a first-order Markovmodel could learn that family names often follow given names,but the reverse rarely occurs (in English).5in the overall likelihood come from the column in-dices and the values of the cells in the table.
Lettingx?, c?
indicate the state of the table and column in-dices after the proposed move, we will accept withprobability,Paccept(x?
x?)
= min(1,P (c?
)P (x?
)P (c)P (x))(5)We first consider the ratio of the table probabili-ties, P (x?|?
)P (x|?)
.
Recall that each column of x is drawnfrom a Dirichlet process; appealing to exchangeabil-ity, we can treat the row t as the last element drawn,and compute the probabilities P (xt,i | x?
(t,i), ?i),with x?
(t,i) indicating the elements of the column iexcluding row t. This probability is given by Equa-tion 2.
For a swap of columns i and j, we computethe ratio:P (xt,i | x?
(t,j), ?j)P (xt,j | x?
(t,i), ?i)P (xt,i | x?
(t,i), ?i)P (xt,j | x?
(t,j), ?j)(6)Next we consider the ratio of the column proba-bilities, P (c?
)P (c) .
Again we can apply exchangeabil-ity, P (c) = P ({cm : rm = t} | {cm?
: rm?
6=t})P ({cm?
: rm?
6= t}).
The second term P ({cm?
:rm?
6= t}) is unaffected by the move, and so is iden-tical in both the numerator and denominator of thelikelihood ratio; probabilities from columns otherthan i and j also cancel in this way.
The remainingratio can be simplified to,(P (c = j | c?t, ?c)P (c = i | c?t, ?c))N(r=t?c=i)?N(r=t?c=j)(7)where the counts N() are from the state of the sam-pler before executing the proposed move.
The prob-ability P (c = i | c?t, ?c) is defined in Equation 3,and the overall acceptance ratio for column swaps isthe product of (6) and (7).4.3 HyperparametersThe concentration parameters ?r and ?c help to con-trol the number of rows and columns in the ta-ble, respectively.
These parameters are updated totheir maximum likelihood values using gradient-based optimization, so our overall inference pro-cedure is a form of Monte Carlo Expectation-Maximization (Wei and Tanner, 1990).The concentration parameters ?j control the di-versity of each column in the table: if ?j is low thenwe expect a high degree of repetition, as with titles;if ?j is high then we expect a high degree of diver-sity.
When the sampling procedure adds a new col-umn, there is very little information for how to setits concentration parameter, as the conditional like-lihood will be flat.
Consequently, greater care mustbe taken to handle these priors appropriately.We place a log-normal hyperprior on the col-umn concentration parameters, log?j ?
N (?, ?2).The parameters of the log-normal are shared acrosscolumns, which provides additional information toconstrain the concentration parameters of newly-created columns.
We then use Metropolis-Hastingsto sample the values of each ?j , using the joint like-lihood,P (?j , x?
(j) | ?, ?2) ?exp(?
(log?j ?
?
)2)?kjj ?(?j)2?2?
(nj + ?j),where x?
(j) is column j of the inferred table, nj isthe number of specified entries in column j of thetable x?
and kj is the number of unique entries inthe column; see Rasmussen (2000) for a derivation.After repeatedly sampling several values of ?j foreach column in the table, we update ?
and ?2 to theirmaximum-likelihood estimates.5 Temporal ProminenceAndy Warhol predicted, ?in the future, everyone willbe world-famous for fifteen minutes.?
A model oftemporal dynamics that accounts for the fleeting andfickle nature of fame might yield better performancefor transient entities, like Joe the Plumber.
Amongseveral alternatives for modeling temporal dynamicsin latent variable models, we choose a simple non-parametric approach: the recurrent Chinese restau-rant process (RCRP; Ahmed and Xing, 2008).
Thecore idea of the RCRP is that time is partitioned intoepochs, with a unique Chinese restaurant process ineach epoch.
Each CRP has a prior which takes theform of pseudo-counts computed from the counts inprevious epochs.
We employ the simplest version ofthe RCRP, a first-order Markov model in which theprior for epoch t is equal to the vector of counts forepoch t?
1:6P (r(t)m = i|r(t)1...m?1, r(t?1), ?r) ?
{N(r(t)1...m?1 = i) + N(r(t?1) = i), if > 0;?r, otherwise.
(8)The count of row i in epoch t ?
1 is writtenN(r(t?1) = i); the count in epoch t for mentions1 to m ?
1 is written N(r(t)1...m?1 = i).
As before,we can apply exchangeability to treat each mentionas the last in the epoch, so during inference we canreplace this with the count N(r(t)?m).
Note that thereis zero probability of drawing an entity that has nocounts in epochs t or t ?
1 but exists in some otherepoch; the probability mass ?r is reserved for draw-ing a new entity, and the chance of this matchingsome existing entity from another epoch is vanish-ingly small.During Gibbs sampling, we also need to considerthe effect of r(t)m on the subsequent epoch t + 1.While space does not permit a derivation, the result-ing probability is proportional toP (r(t+1)|r(t)?m, r(t)m = i, ?r) ???????
?1 if N(r(t+1) = i) = 0,N(r(t+1)=i)?rif N(r(t)?m = i) = 0,1 + N(r(t+1)=i)N(r(t)?m=i)if N(r(t)?m = i) > 0.
(9)This favors entities which are frequent in epocht+ 1 but infrequent in epoch t.The move to a recurrent Chinese restaurant pro-cess does not affect the sampling equations for thecolumns c, nor the concentration parameters of thetable, ?.
The only part of the inference procedurethat needs to be changed is the optimization of thehyperparameter ?r; the log-likelihood is now thesum across all epochs, and each epoch makes a con-tribution to the gradient.6 Evaluation SetupOur model jointly performs three tasks: identifyinga set of entities, discovering the set of fields, andmatching mention strings with the entities and fieldsto which they refer.
We are aware of no prior workthat performs these tasks jointly, nor any dataset thatis annotated for all three tasks.2 Consequently, wefocus our quantitative evaluation on what we take tobe the most important subtask: identifying the enti-ties which are mentioned in raw text.
We annotatea new dataset of blog text for this purpose, and de-sign precision and recall metrics to reward systemsthat recover as much of the reference set as possi-ble, while avoiding spurious entities and fields.
Wealso perform a qualitative analysis, noting the areaswhere our method outperforms string matching ap-proaches, and where there is need for further im-provement.Data Evaluation was performed on a corpusof blogs describing United States politics in2008 (Eisenstein and Xing, 2010).
We ran the Stan-ford Named Entity Recognition system (Finkel etal., 2005) to obtain a set of 25,000 candidate men-tions which the system judged to be names of peo-ple.
We then pruned strings that appeared fewer thanfour times and eliminated strings with more thanseven tokens (these were usually errors).
The result-ing dataset has 19,247 mentions comprising 45,466word tokens, and 813 unique mention strings.Gold standard We develop a reference set of 100entities for evaluation.
This set was created by sort-ing the unique name strings in the training set by fre-quency, and manually merging strings that referencethe same entity.
We also manually discarded stringsfrom the reference set if they resulted from errors inthe preprocessing pipeline (tokenization and namedentity recognition).
Each entity is represented bythe set of all word tokens that appear in its refer-ences; there are a total of 231 tokens for the 100 en-tities.
Most entities only include first and last names,though the most frequent entities have many more:for example, the entity Barack Obama has knownnames: {Barack, Obama, Sen., Mr.}.Metrics We evaluate the recall and precision ofa system?s response set by matching against thereference set.
The first step is to create a bipar-tite matching between response and reference enti-ties.3 Using a cost function that quantifies the sim-2Recent work exploiting Wikipedia disambiguation pagesfor evaluating cross-document coreference suggests an appeal-ing alternative for future work (Singh et al, 2011).3Bipartite matchings are typical in information extractionevaluation metrics (e.g., Doddington et al, 2004).7ilarity of response and reference entities, we opti-mize the matching using the Kuhn-Munkres algo-rithm (Kuhn, 1955).
For recall, the cost functioncounts the number of shared word tokens, dividedby the number of word tokens in the reference enti-ties; the recall is one minus the average cost of thebest matching (with a cost of one for reference enti-ties that are not matched, and no cost for unmatchedresponse entities).
Precision is computed identically,but we normalize by the number of word tokens inthe response entity.
Precision assigns a penalty ofone to unmatched response entities and no penaltyfor unmatched reference entities.Note that this metric grossly underrates the preci-sion of all systems: the reference set is limited to 100entities, but it is clear that our text mentions manyother people.
This is harsh but fair: all systems arepenalized equally for identifying entities that are notpresent in the reference set, and the ideal system willrecover the fifty reference entities (thus maximizingrecall) while keeping the table as compact as possi-ble (thus maximizing precision).
However, the rawprecision values have little meaning outside the con-text of a direct comparison under identical experi-mental conditions.Systems The initial seed set for our system con-sists of a partial annotation of five entities (Table 1)?
larger seed sets did not improve performance.
Werun the inference procedure described in the previ-ous section for 20,000 iterations, and then obtain afinal database by taking the intersection of the in-ferred tables x?
obtained at every 100 iterations, start-ing with iteration 15,000.
To account for varianceacross Markov chains, we perform three differentruns.
We evaluate a non-temporal version of ourmodel (as described in Sections 3 and 4), and a tem-poral version with 5 epochs.
For the non-temporalversion, a non-parallel C implementation had a wallclock sampling time of roughly 16 hours; the tem-poral version required 24 hours.We compare against a baseline that incrementallyclusters strings into entities using a string edit dis-tance metric, based on the work of Elmacioglu etal.
(2007).
Starting from a configuration in whicheach unique string forms its own cluster, we incre-mentally merge clusters using the single-link crite-rion, based on the minimum Jaccard edit distance0.2 0.3 0.4 0.5 0.6 0.700.10.20.3recallprecisionbaselineatemporal modeltemporal modelFigure 2: The precision and recall of our models, as com-pared to the curve defined by the incremental clusteringbaseline.
Each point indicates a unique sampling run.Bill Clinton Benazir BhuttoNancy Pelosi SpeakerJohn Kerry Sen. RobertsMartin King Dr. Jr. LutherBill NelsonTable 2: A subset of the entity database discovered byour model, hand selected to show highlight interestingsuccess and failure cases.between each pair of clusters.
This yields a series ofoutputs that move along the precision-recall curve,with precision increasing as the clusters encompassmore strings.
There is prior work on heuristics forselecting a stopping point, but we compare our re-sults against the entire precision-recall curve (Man-ning et al, 2008).7 ResultsThe results of our evaluation are shown in Figure 2.All sampling runs from our models lie well beyondthe precision-recall curve defined by the baselinesystem, demonstrating the ability to achieve reason-able recall with a far more compact database.
Thebaseline system can achieve nearly perfect recall bycreating one entity per unique string, but as it mergesstrings to improve precision, its recall suffers sig-nificantly.
As noted above, perfect precision is notpossible on this task, because the reference set cov-ers only a subset of the entities that appear in thedata.
However, the numbers do measure the abilityto recover the reference entities in the most compacttable possible, allowing a quantitative comparison ofour models and the baseline approach.8Table 2 shows a database identified by the atem-poral version of our model.
The most densely-populated columns in the table correspond to well-defined name parts: columns 1 and 2 are almostexclusively populated with first and last names re-spectively, and column 3 is mainly populated by ti-tles.
The remaining columns are more of a grabbag.
Column 4 correctly captures Jr. for MartinLuther King; column 5 correctly captures Luther,but mistakenly contains Roberts (thus merging theJohn Kerry and John Roberts entities), and Bhutto(thus helping to merge the Bill Clinton and BenazirBhutto entities).The model successfully distinguishes some, butnot all, of the entities that share tokens.
For example,the model separates Bill Clinton from Bill Nelson;it also separates John McCain from John Kerry(whom it mistakenly merges with John Roberts).The ability to distinguish individuals who share firstnames is due in part to the model attributing a lowconcentration parameter to first names, meaning thatsome repetition in the first name column is expected.The model correctly identifies several titles and al-ternative names, including the rare title Speaker forNancy Pelosi; however, it misses others, such as theSenator title for Bill Nelson.
This may be due inpart to the sample merging procedure used to gener-ate this table, which requires that a cell contain thesame value in at least 80% of the samples.Many errors may be attributed to slow mixing.After mistakenly merging Bhutto and Clinton atan early stage, the Gibbs sampler ?
which treatseach mention independently ?
is unable to sep-arate them.
Given that several other mentions ofBhutto are already in the row occupied by Clin-ton, the overall likelihood would benefit little fromcreating a new row for a single mention, thoughmoving all such mentions simultaneously would re-sult in an improvement.
Larger scale Metropolis-Hastings moves, such as split-merge or type-basedsampling (Liang et al, 2010) may help.8 Related WorkInformation Extraction A tradition of researchin information extraction focuses on processing rawtext to fill in the fields of manually-defined tem-plates, thus populating databases of events or re-lations (McNamee and Dang, 2009).
While earlyapproaches focused on surface-level methods suchas wrapper induction (Kushmerick et al, 1997),more recent work in this area includes Bayesiannonparametrics to select the number of rows in thedatabase (Haghighi and Klein, 2010a).
However,even in such nonparametric work, the form of thetemplate and the number of slots are fixed in ad-vance.
Our approach differs in that the number offields and their meaning is learned from data.
Recentwork by Chambers and Jurafsky (2011) approachesa related problem, applying agglomerative cluster-ing over sentences to detect events, and then clus-tering syntactic constituents to induce the relevantfields of each event entity.
As described in Section 6,our method performs well against an agglomerativeclustering baseline, though a more comprehensivecomparison of the two approaches is an importantstep for future work.Name Segmentation and Structure A relatedstream of research focuses specifically on names:identifying them in raw text, discovering their struc-ture, and matching names that refer to the same en-tity.
We do not undertake the problem of named en-tity recognition (Tjong Kim Sang, 2002), but ratherapply an existing NER system as a preprocessingstep (Finkel et al, 2005).
Typical NER systemsdo not attempt to discover the internal structure ofnames or a database of canonical names, althoughthey often use prefabricated ?gazetteers?
of namesand name parts as features to improve performance(Borthwick et al, 1998; Sarawagi and Cohen, 2005).Charniak (2001) shows that it is possible to learn amodel of name structure, either by using coreferenceinformation as labeled data, or by leveraging a smallset of hand-crafted constraints.
Elsner et al (2009)develop a nonparametric Bayesian model of namestructure using adaptor grammars, which they use todistinguish types of names (e.g., people, places, andorganizations).
Li et al (2004) use a set of manually-crafted ?transformations?
of name parts to build amodel of how a name might be rendered in multi-ple different ways.
While each of these approachesbears on one or more facets of the problem that weconsider here, none provides a holistic treatment ofname disambiguation and structure.9Resolving Mentions to Entities The problem ofresolving mentions to entities has been approachfrom a variety of different perspectives.
There isan extensive literature on probabilistic record link-age, in which database records are compared to de-termine if they are likely to have the same real-worldreferents (e.g., Felligi and Sunter, 1969; Bilenkoet al, 2003).
Most approaches focus on pairwiseassessments of whether two records are the same,whereas our method attempts to infer a single coher-ent model of the underlying relational data.
Somemore recent work in record linkage has explicitlyformulated the task of inferring a latent relationalmodel of a set of observed datasets (e.g., Cohenet al, 2000; Pasula et al, 2002; Bhattacharya andGetoor, 2007); however, to our knowledge, theseprior models have all exploited some predefineddatabase schema (i.e., set of columns), which ourmodel does not require.
Many of these prior mod-els have been applied to bibliographic data, wheredifferent conventions and abbreviations lead to im-perfect matches in different references to the samepublication.
In our task, we consider name mentionsin raw text; such mentions are short, and may notoffer as many redundant clues for linkage as biblio-graphic references.In natural language processing, coreference res-olution is the task of grouping entity mentions(strings), in one or more documents, based on theircommon referents in the world.
Although much ofcoreference resolution has on the single documentsetting, there has been some recent work on cross-document coreference resolution (Li et al, 2004;Haghighi and Klein, 2007; Poon and Domingos,2008; Singh et al, 2011).
The problem we consideris related to cross-document coreference, althoughwe take on the additional challenge of providinga canonicalized name for each referent (the corre-sponding table row), and in inferring a structuredrepresentation of entity names (the table columns).For this reason, our evaluation focuses on the in-duced table of entities, rather than the clustering ofmention strings.
The best coreference systems de-pend on carefully crafted, problem-specific linguis-tic features (Bengtson and Roth, 2008) and exter-nal knowledge (Haghighi and Klein, 2010b).
Futurework might consider how to exploit such features forthe more holistic information extraction setting.9 ConclusionThis paper presents a Bayesian nonparametric ap-proach to recover structured records from text.
Us-ing only a small set of prototype records, we are ableto recover an accurate table that jointly identifies en-tities and internal name structure.
In our view, themain advantage of a Bayesian approach comparedto more heuristic alternatives is that it facilitates in-corporation of additional information sources whenavailable.
In this paper, we have considered onesuch additional source, incorporating temporal con-text using the recurrent Chinese restaurant process.We envision enhancing the model in several otherrespects.
One promising direction is the incorpo-ration of name structure, which could be capturedusing a first-order Markov model of the transitionsbetween name parts.
In the nonparametric setting,a transition matrix is unbounded along both dimen-sions, and this can be handled by a hierarchicalDirichlet process (HDP; Teh et al2006).4 We en-vision other potential applications of the HDP: forexample, learning ?topics?
of entities which tend toappear together (i.e., given a mention of MahmoudAbbas in the American press, a mention of Ben-jamin Netanyahu is likely), and handling document-specific burstiness (i.e., given that an entity is men-tioned once in a document, it is much more likelyto be mentioned again).
Finally, we would liketo incorporate lexical context from the sentences inwhich each entity is mentioned, which might help todistinguish, say, computer science researchers whoshare names with former defense secretaries or pro-fessional basketball players.Acknowledgments This research was enabledby AFOSR FA95501010247, DARPA grantN10AP20042, ONR N000140910758, NSF DBI-0546594, IIS-0713379, IIS-0915187, IIS-0811562,an Alfred P. Sloan Fellowship, and Google?s supportof the Worldly Knowledge project at CMU.
Wethank the reviewers for their thoughtful feedback.4One of the reviewers proposed to draw entire column se-quences from a Dirichlet process.
Given the relatively smallnumber of columns and canonical name forms, this may be astraightforward and effective alternative to the HDP.10ReferencesAmr Ahmed and Eric P. Xing.
2008.
Dynamic non-parametric mixture models and the recurrent Chineserestaurant process with applications to evolutionaryclustering.
In International Conference on Data Min-ing.Javier Artiles, Julio Gonzalo, and Satoshi Sekine.
2007.The SemEval-2007 WePS evaluation: establishing abenchmark for the web people search task.
In Pro-ceedings of the 4th International Workshop on Seman-tic Evaluations, SemEval ?07, pages 64?69.
Associa-tion for Computational Linguistics.Eric Bengtson and Dan Roth.
2008.
Understandingthe value of features for coreference resolution.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?08,pages 294?303, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Indrajit Bhattacharya and Lise Getoor.
2007.
Collec-tive entity resolution in relational data.
ACM Trans.Knowl.
Discov.
Data, 1(1), March.Mikhail Bilenko, William W. Cohen, Stephen Fien-berg, Raymond J. Mooney, and Pradeep Ravikumar.2003.
Adaptive name-matching in information in-tegration.
IEEE Intelligent Systems, 18(5):16?23,September/October.A.
Borthwick, J.
Sterling, E. Agichtein, and R. Grishman.1998.
Exploiting diverse knowledge sources via max-imum entropy in named entity recognition.
In SixthWorkshop on Very Large Corpora New Brunswick,New Jersey.
Association for Computational Linguis-tics.Nathanael Chambers and Dan Jurafsky.
2011.
Template-based information extraction without the templates.
InProceedings of ACL.Eugene Charniak.
2001.
Unsupervised learning of namestructure from coreference data.
In Proceedings of theSecond Meeting of the North American Chapter of theAssociation for Computational Linguistics.William W. Cohen, Henry Kautz, and David McAllester.2000.
Hardening soft information sources.
In Pro-ceedings of the Sixth International Conference onKnowledge Discovery and Data Mining, pages 255?259.George Doddington, Alexis Mitchell, Mark Przybocki,Lance Ramshaw, Stephanie Strassel, and RalphWeischedel.
2004.
The automatic content extraction(ace) program: Tasks, data, and evaluation.
In 4thinternational conference on language resources andevaluation (LREC?04).Jacob Eisenstein and Eric Xing.
2010.
The CMU 2008political blog corpus.
Technical report, Carnegie Mel-lon University.Ergin Elmacioglu, Yee Fan Tan, Su Yan, Min-Yen Kan,and Dongwon Lee.
2007.
Psnus: Web people namedisambiguation by simple clustering with rich features.In Proceedings of the Fourth International Workshopon Semantic Evaluations (SemEval-2007), pages 268?271, Prague, Czech Republic, June.
Association forComputational Linguistics.Micha Elsner, Eugene Charniak, and Mark Johnson.2009.
Structured generative models for unsupervisednamed-entity clustering.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 164?172, Boulder,Colorado, June.
Association for Computational Lin-guistics.I.
P. Felligi and A.
B. Sunter.
1969.
A theory for recordlinkage.
Journal of the American Statistical Society,64:1183?1210.Jenny R. Finkel, Trond Grenager, and Christopher Man-ning.
2005.
Incorporating non-local informationinto information extraction systems by Gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 363?370, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric bayesianmodel.
In Proceedings of the 45th Annual Meetingof the Association of Computational Linguistics, pages848?855, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Aria Haghighi and Dan Klein.
2010a.
An entity-levelapproach to information extraction.
In Proceedingsof the ACL 2010 Conference Short Papers, ACLShort?10, pages 291?295, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Aria Haghighi and Dan Klein.
2010b.
Coreference reso-lution in a modular, entity-centered model.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 385?393, Los An-geles, California, June.
Association for ComputationalLinguistics.Harold W. Kuhn.
1955.
The Hungarian method for theassignment problem.
Naval Research Logistic Quar-terly, 2:83?97.Nicholas Kushmerick, Daniel S. Weld, and RobertDoorenbos.
1997.
Wrapper induction for informationextraction.
In Proceedings of IJCAI.Xin Li, Paul Morie, and Dan Roth.
2004.
Identificationand tracing of ambiguous names: Discriminative andgenerative approaches.
In Proceedings of AAAI, pages419?424.11Percy Liang, Michael I. Jordan, and Dan Klein.
2010.Type-Based MCMC.
In Human Language Technolo-gies: The 2010 Annual Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics, pages 573?581, Los Angeles, California,June.
Association for Computational Linguistics.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, 1 edition, July.Paul McNamee and Hoa Trang Dang.
2009.
Overviewof the TAC 2009 knowledge base population track.
InProceedings of the Text Analysis Conference (TAC).Hanna Pasula, Bhaskara Marthi, Brian Milch, Stuart Rus-sell, and Ilya Shpitser.
2002.
Identity uncertainty andcitation matching.
In Advances in Neural ProcessingSystems 15, Vancouver, British Columbia.
MIT Press.Hoifung Poon and Pedro Domingos.
2008.
Joint un-supervised coreference resolution with Markov Logic.In Proceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages 650?659, Honolulu, Hawaii, October.
Association for Com-putational Linguistics.Carl E. Rasmussen.
2000.
The Infinite Gaussian MixtureModel.
In In Advances in Neural Information Process-ing Systems 12, volume 12, pages 554?560.Sunita Sarawagi and William W. Cohen.
2005.
Semi-Markov conditional random fields for information ex-traction.
In Lawrence K. Saul, Yair Weiss, and Le?onBottou, editors, Advances in Neural Information Pro-cessing Systems 17, pages 1185?1192.
MIT Press,Cambridge, MA.Sameer Singh, Amarnag Subramanya, Fernando Pereira,and Andrew McCallum.
2011.
Large-scale cross-document coreference using distributed inference andhierarchical models.
In Association for Computa-tional Linguistics: Human Language Technologies(ACL HLT).Yee W. Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101:1566?1581, December.Erik F. Tjong Kim Sang.
2002.
Introduction tothe CoNLL-2002 shared task: Language-independentnamed entity recognition.
In Proceedings of the SixthConference on Natural Language Learning.Greg C. G. Wei and Martin A. Tanner.
1990.
A MonteCarlo Implementation of the EM Algorithm and thePoor Man?s Data Augmentation Algorithms.
Journalof the American Statistical Association, 85(411):699?704.12
