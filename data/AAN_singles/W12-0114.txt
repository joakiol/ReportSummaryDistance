Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 101?112,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsDesign of a hybrid high quality machine translation systemKurt EberleJohanna Gei?Mireia Ginest?-RosellBogdan BabychAnthony HartleyReinhard RappLingenio GmbH Serge SharoffKarlsruher Stra?e 1069 126 Heidelberg, GermanyMartin ThomasCentre for Translation StudiesUniversity of LeedsLeeds, LS2 9JT, UK[k.eberle,j.geiss,m.ginesti-rosell]@lingenio.de[B.Babych,A.Hartley,R.Rapp,S.Sharoff,M.Thomas]@leeds.ac.ukAbstractThis paper gives an overview of theongoing FP7 project HyghTra (2010 ?2014).
The HyghTra project is conductedin a partnership between academia andindustry involving the University of Leedsand Lingenio GmbH (company).
It adopts ahybrid and bootstrapping approach to theenhancement of MT quality by applyingrule-based analysis and statisticalevaluation techniques to both parallel andcomparable corpora in order to extractlinguistic information and enrich the lexicaland syntactic resources of the underlying(rule-based) MT system that is used foranalysing the corpora.
The project placesspecial emphasis on the extension ofsystems to new language pairs andcorresponding rapid, automated creation ofhigh quality resources.
The techniques arefielded and evaluated within an existingcommercial MT environment.1 MotivationStatistical Machine Translation (SMT) has beenaround for about 20 years, and for roughly half ofthis time SMT and the 'traditional' Rule-basedMachine Translation (RBMT) have been seen ascompeting paradigms.
During the last decadehowever, there is a trend and growing interest incombining the two methodologies.
In our approachthese two approaches are viewed ascomplementary.Advantages of SMT are low cost and robustness,but definite disadvantages of (pure) SMT are that itneeds huge amounts of data, which for manylanguage pairs are not available and are unlikely tobecome available in the future.
Also, SMT tends todisregard important classificatory knowledge (suchas morphosyntactic, categorical and lexical classfeatures), which can be provided and usedrelatively easily within non-statisticalrepresentations.On the other hand, advantages of RBMT are thatits (grammar and lexical) rules and information areunderstandable by humans and can be exploited fora lot of applications outside of translation(dictionaries, text understanding, dialogue systems,etc.
).The slot grammar approach used in Lingeniosystems (cf.
McCord 1989, Eberle 2001) is aprime example of such linguistically richrepresentations that can be used for a number ofdifferent applications.
Fig.1 shows this by avisualization of (an excerpt of) the entry for theambiguous German verb einstellen in the databasethat underlies (a)  the Lingenio translationproducts, where it links up with corresponding setof the transfer rules, and (b) Lingenio?s dictionaryproduct TranslateDict, which is primarily intendedfor human translators.101Fig 1 a) data base entry einstellen('translation' represents links between SL and T entries)Fig 1 b) product entry einstellenThe obvious disadvantages of RBMT are high cost,weaknesses in dealing with incorrect input and inmaking correct choices with respect to ambiguouswords, structures, and transfer equivalents.SMT output is often surprisingly good with respectto short distance collocations, but often missescorrect choices are missed in cases whereselectional restrictions take effect on distant words.RBMT output is generally good if the parserassigns the correct analysis to a sentence and  if thetarget words can be correctly chosen from the setof alternatives.
However, in the presence ofambiguous words and structures, and wherelinguistic information is lacking, the decisions maybe wrong.Given the complementarity of SMT and RBMTand their very different strengths and weaknesses,we take a view that an optimized MT architecturemust comprise elements of both paradigms.
Thekey issue therefore lies in the identification of suchelements and how to connect them to each other.We propose a specific type of hybrid translation ?hybrid high quality translation (HyghTra), wherecore RBMT systems are created and enhanced by arange of reliable statistical techniques.2 Development MethodologyMany hybrid systems described in the literaturehave attempted to put some analytical abstractionon top of an SMT kernel.1 In our view this is notthe best option because, according to theunderlying philosophy, SMT is linguisticallyignorant at the beginning and learns all linguisticrules automatically from corpora.
However, theextracted information is typically represented inhuge data sets which are not readable by humans ina natural way.
This means that this type ofarchitecture does not easily provide interfaces forincorporating linguistic knowledge in a canonicaland simple way.Thus we approach the problem from the other end,, integrating information derived from corporausing statistical methods into RBMT systems.Provided the underlying RBMT systems arelinguistically sound and sufficiently modular instructure, we believe this to have greater potentialfor generating high quality output.We currently use and carry out the following workplan:(I) Creation of MT systems(with rule-based core MT information andstatistical extension and training):(a) We start out with declarative analysis andgeneration components of the consideredlanguages, and with basic bilingual dictionariesconnecting to one another the entries of relativelysmall vocabularies comprising the most frequentwords of each language in a given translation pair(cf.
Fig 1 a).
(b) Having completed this phase, we extend thedictionaries and train the analysis-, transfer- andgeneration-components of the rule-based coresystems using monolingual and bilingual corpora.1 A prominent early example is Frederking andcolleagues (Frederking & Nirenburg, 1994).
For anoverview of  hybrid MT till the late nineties see Streiteret al (1999).
More recent  approaches include Groves &Way (2006a, 2006b).
Commercial implementationsinclude AppTek (http://www.apptek.com) and LanguageWeaver (http://www.languageweaver.com).
An ongoingMT important project investigating hybrid methods isEuroMatrixPlus (http://www.euromatrixplus.net/)102(II) Error detection and improvement cycle:(a) We automatically discover the most frequentproblematic grammatical constructions andmultiword expressions for commercial RBMT andSMT systems using automatic construction-basedevaluation as proposed in (Babych and Hartley,2009) and develop a framework for fixingcorresponding grammar rules and extendinggrammatical coverage of the systems in a semi-automatic way.
This shortens development time forcommercial MT and contributes to yieldingsignificantly higher translation quality.
(III) Extension to other languages:Structural similarity and translation by pivotlanguages is used to obtain extension to furtherlanguages:High-quality translation between closely relatedlanguages (e.g., Russian and Ukrainian orPortuguese and Spanish) can be achieved withrelatively simple resources (using linguisticsimilarity, but also homomorphism assumptionswith respect to parallel text, if available), whilegreater efforts are put into ensuring better-qualitytranslation between more distant languages (e.g.German and Russian).
According to our priorresearch (Babych et al, 2007b) the pipelinebetween languages of different similarity results inimproved translation quality for a larger number oflanguage pairs (e.g., MT from Portuguese orUkrainian into German is easier if there are high-quality analysis and transfer modules for Spanishand Russian into German (respectively).
Of course,(III) draws heavily on the detailed analysis and MTsystems that the industrial partner in HyghTraprovides for a number of languages.In the following sections we give more details ofthe work currently done with regard to (I) and withregard to parts of (II): the creation of a new MTsystem following the strategy sketched.
We cannotgo further into detail with (II) and (III) here, whichwill become a priority for future research.3 Creation of a new systemEarly pilot studies covering some aspects of thestrategy described here (using information frompivot languages and similarity) showed promisingresults (Rapp, 1999; Rapp & Mart?n Vide, 2007;see also Koehn & Knight, 2002).We expect that the proposed semi-automaticcreation of a new MT system as sketched abovewill work best if one of the two languages involvedis already 'known' by modules to which the systemhas access.
Against the background of the pipelineapproach mentioned above in (III), this means thatwe assume an analysis and translation system thatcontinuously grows by 'learning' new languageswhere 'learning' is facilitated by information aboutthe languages already 'known' and by exploitingsimilarity assumptions ?
and, of course, by beingfed with information prepared and provided by thehuman 'companion' of the system.From this perspective, we assume the followingsteps of extending the system (with work done bythe 'companion' and work done by the system)1.
Acquire parallel and comparable corpora.2.
Define a core of the morphology of the newlanguage and compile a basic dictionary for themost frequent words and translations.Morphological representations and features fornew languages are derived both manually andautomatically, as proposed in (Babych et al,2012 (in preparation)).3.
Using established alignment technology (e.g.Giza++) and parallel corpora, generate a firstextension of this dictionary.4.
Expand the dictionary of step 3 usingcomparable corpora as proposed in a study byRapp (1999).
This is applicable mainly to singleword units.5.
Expand coverage of multiword-units usingnovel technology.6.
Cross-validate the new dictionary with respectto available ones by transitivity.7.
Integrate the new dictionary into the new MTsystem as developing from reusing componentsand adding new components as in 8.8.
Complete morphology and spell out declarativeanalysis and generation grammar for the newlanguage.9.
Automatically evaluate the translations of themost frequent grammatical constructions andmultiword expressions in a machine-translatedcorpus, prioritising support for theseconstructions with a type of risk-assessmentframework proposed in Babych and Hartley(2008).10.
Extend support for high-priority constructionssemi-automatically by mining correct103translations from parallel corpora.11.
Train and evaluate the new grammar andtransfer of the new MT system using the newdictionary on the basis of available parallelcorpora.The following sections give an overview of thedifferent steps.Step 1: Acquire parallel and comparablecorporaAs our parallel corpus, we use the Europarl.
Thesize of the current version is up to 40 millionwords per language, and several of the languageswe are currently considering are covered.
Also, wemake use of other parallel corpora such as theCanadian Hansards (Proceedings of the CanadianParliament) for the English?French language pair.For non-EU Languages (mainly Russian), weintend to conduct a pilot study to establish thefeasibility of retrieving parallel corpora from theweb, a problem for which various approaches havebeen proposed (Resnik, 1999; Munteanu & Marcu,2005; Wu & Fung, 2005).In addition to the parallel corpora, we will needlarge monolingual corpora in the future (at least200 million words) for each of the six languages.Here, we intend to use newspaper corporasupplemented with text collections downloadablefrom the web.The corpora are stored in a database that allowsfor assigning analyses of different depth and natureto the sentences and for alignment between thesentences and their analyses.
The architecture ofthis database and the corresponding analysis andevaluation frontend is described in (Eberle et al2010, 2012).
Section Results contains examples ofsuch representations.Step 2: Compile a basic dictionary for the mostfrequent wordsA prerequisite of the suggested hybrid approachwith rule-based kernel is to define morphologicalclassifications for the new language(s).
This isdone exploiting similarities to the classifications asavailable for the existing languages.
Currently, thishas been carried out for Dutch (on the basis ofGerman) and for Spanish (on the basis ofFrench/other Romance languages).
The mostfrequent words (the basic vocabulary of alanguage) are typically also the most ambiguousones.
Since the Lingenio systems are lexicallydriven transfer systems (cf.
Eberle 2001), wedefine (a) structural conditions,  which inform thechoice of the possible target words (single wordsor multiword expressions) and (b)restructuringconditions, as necessary (cf.
Fig 1 a:  attributes'transfer conditions' and 'structural change').
Inorder to ensure quality this must be done by humanlexicographers and therefore costly for a largedictionary.
However, we manually create only verysmall basic dictionaries and extend these (semi-automatically) step 3 and those which follow.Some important morphosyntactic features of thelanguage are derived from a monolingual corpusannotated with publicly available part-of-speechtaggers and lemmatisers.
However, these toolsoften do not explicitly represent linguistic featuresneeded for the generation stage in RBMT.
In(Babych et al, 2012) we propose a systematicapproach to recovering such missing generation-oriented representations from grammar models andstatistical combinatorial properties of annotatedfeatures.Step 3: Generating dictionary extensions fromparallel corporaBased on parallel corpora, dictionaries can bederived using established techniques of automaticsentence alignment and word alignment.
Forsentence alignment, the length-based Gale &Church aligner (1993) can be used, or ?alternatively ?
Dan Melamed?s GSA-algorithm(Geometric Sentence Alignment; Melamed, 1999).For segmentation of text we use correspondingLingenio-tools (unpublished).2For word alignment Giza++ (Och & Ney, 2003) isthe standard tool.
Given a word alignment, theextraction of a (SMT) dictionary is relativelystraightforward.
With the exception of sentencesegmentation, these algorithms are largelylanguage independent and can be used for all of thelanguages that we consider.
We did this for anumber of language pairs on the basis of the2  If these cannot be applied because of  lack ofinformation about a language, we intend to use thealgorithm by Kiss & Strunk (2006).
An open-sourceimplementation of parts of the Kiss & Strunk algorithmis available from Patrick Tschorn athttp://www.denkselbst.de/sentrick/index.html.104Europarl-texts considered (as stored in ourdatabase).
In order to optimize the results we usethe dictionaries of step 1 as set of cognates (cf.Simard at al 1992, Gough & Way 2004), as well asother words easily obtainable from the internet thatcan be used for this purpose (like company namesand other named entities with cross-languageidentity and terminology translations).
Using themorphology component of the new language andthe categorial information from the transferrelation, we compute the basic forms of theinflected words found.
Later, we intend to furtherimprove the accuracy of word alignment byexploiting chunk type syntactic information of thenarrow context of the words (cf.
Eberle & Rapp2008).
An early stage variant of this is already usedin Lingenio products.
The corresponding functionAutoLearn<word> extracts new word relations onthe basis of existing dictionaries and (partial)syntactic analyses.
(Fig 2 gives an example).Fig 2 AutoLearn<word>: new entries usingtransfer links and syntactic analysisGiven the relatively small size of the availableparallel corpora, we expect that the automaticallygenerated dictionaries will comprise about 20,000entries each (This corresponds to first results onthe basis of German?English).
This is far toosmall for a serious general purpose MT system.Note that, in comparison, the English?Germandictionary used in the current Lingenio MTproduct comprises more than 480,000 keywordsand phrases.Step 4: Expanding dictionaries usingcomparable corpora (word equations)In order to expand the dictionaries using a set ofmonolingual comparable corpora, the basicapproach pioneered by Fung & McKeown (1997)and Rapp (1995, 1999) is to be further developedand refined in the second phase of the project as toobtain a practical tool that can be used in anindustrial context.The basic assumption underlying the approachis that across languages there is a correlationbetween the co-occurrences of words that aretranslations of each other.
If ?
for example ?
in atext of one language two words A and B co-occurmore often than expected by chance, then in a textof another language those words that aretranslations of A and B should also co-occur morefrequently than expected.
It is further assumed thata small dictionary (as generated in step 2) isavailable at the beginning, and that the aim is toexpand this basic lexicon.
Using a corpus of thetarget language, first a co-occurrence matrix iscomputed whose rows are all word types occurringin the corpus and whose columns are all targetwords appearing in the basic lexicon.
Next a wordof the source language is considered whosetranslation is to be determined.
Using the source-language corpus, a co-occurrence vector for thisword is computed.
Then all known words in thisvector are translated to the target language.
As thebasic lexicon is small, only some of thetranslations are known.
All unknown words arediscarded from the vector and the vector positionsare sorted in order to match the vectors of thetarget-language matrix.
Using standard measuresfor vector similarity, the resulting vector iscompared to all vectors in the co-occurrencematrix of the target language.
The vector with thehighest similarity is considered to be thetranslation of our source-language word.From a previous pilot study (Rapp, 1999) it canbe expected that this methodology achieves anaccuracy in the order of 70%, which means thatonly a relatively modest amount of manual post-editing is required.The automatically generated results areimproved and the amount of post-editing isreduced by exploiting sense (disambiguation)information as available from the analysiscomponent for the 'known' language of the newlanguage pair.. Also we try to exploit categorialand underspecified syntactic information of thecontexts of the words similar to what has beensuggested for improving word alignment in theprevious step (see also Fig.2).
Also, as the frequentwords are already covered by the basic lexicon(whose production from parallel corpora on thebasis of a manually compiled kernel does not show105an ambiguity problem of similar significance), andas experience shows that most low frequencywords in a full-size lexicon tend to beunambiguous, the ambiguity problem is reducedfurther for the words investigated and extracted bythis comparison method.Step 5: Expanding dictionaries usingcomparable corpora (multiword units)In order to account for technical terms, idioms,collocations, and typical short phrases, animportant feature of an MT lexicon is a highcoverage of multiword units.
Very recent workconducted at the University of Leeds (Sharoff etal., 2006) shows that dictionary entries for suchmultiword units can be derived from comparablecorpora if a dictionary of single words is available.It could even be shown that this methodology canbe superior to deriving multiword-units fromparallel corpora (Babych et al, 2007).
This is amajor breakthrough as comparable corpora are fareasier to acquire than parallel corpora.
It evenopens up the possibility of building domain-specific dictionaries by using texts from differentdomains.The outline of the algorithm is as follows:?
Extract collocations from a corpus of thesource language (Smadja, 1993)?
To translate a collocation, look up all itswords using any dictionary?
Generate all possible permutations(sequences) of the word translations?
Count the occurrence frequencies of thesesequences in a corpus of the targetlanguage and test for significance?
Consider the most significant sequence tobe the translation of the source languagecollocationOf course, in later steps of the project, we willexperiment on filtering these sequences byexploiting structural knowledge similarly to whatwas described in the two previous steps.
This canbe obtained on the basis of the declarative analysiscomponent of the new language which isdeveloped in parallel.Step 6: Cross-validate dictionariesThe combination of the corpus-based methods forautomatic dictionary generation as described insteps 3 to 5 will lead to high coverage dictionariesas the availability of very large monolingualcorpora is no major problem for our languages.However, as all steps are error prone, it can beexpected that a considerable number of dictionaryentries (e.g.
50%) are not correct.
To facilitate (butnot eliminate) the manual verification of thedictionary, we will  perform an automatic cross-check which utilizes the dictionaries?
property oftransitivity.
What we mean by this is that if wehave two dictionaries, one translating fromlanguage A to language B, the other from languageB to language C, then we can also translate fromlanguage A to C by use of the intermediatelanguage (or interlingua) B.
That is, the property oftransitivity, although having some limitations dueto ambiguity problems, can be exploited toautomatically generate a raw dictionary for A to C.Lingenio  has some experience with this methodhaving exploited it for extending and improving itsEnglish ?
French dictionaries using French ?German and German ?
English.As the corpus-based approach (steps 3 to 5)allows us to also generate this type of dictionaryvia comparable corpora, we have two differentways to generate a dictionary for a particularlanguage pair.
This means that we can validate onewith the other.
Furthermore, with increasingnumber of language pairs created, there are moreand more languages that can serve as interlingua or'pivot': This, step by step, gives an increasingpotential for mutual cross-validation.Specific attention will be paid to automating asfar as possible the creation of selectionalrestrictions to be assigned to the transfer relationsof the new dictionaries in all steps of dictionarycreation (2?6).
We will try to do this on the basisof the analysis components as available for thelanguages considered: These are: a completelyworked out analysis component for the 'old'language, a declarative (chunk parsing) componentfor the new one (compare the two following stepsfor this).Step 7: Integrate dictionaries in existingmachine translation systemsLingenio has a relatively rich infrastructure forautomatic importation of various kinds of lexicalinformation into the database used by the analysesand translation systems.
If necessary theinformation on hand (for instance fromconventional dictionaries of publishing houses) is106completed and normalized during or beforeimportation.
This may be executed completelyautomatically ?
by using the existing analysescomponents and resources respectively asdatabases ?
or interactively ?
by asking thelexicographer for additional information, if needed.For example, there may be a list of multiwordexpressions to be imported into the database.
Inorder to have available correct syntactic andsemantic information for these expressions, theyare analysed by the parser of the correspondinglanguage.
From the analysis found, the informationnecessary to describe the new lemma in the lexiconwith respect to semantic type and syntacticstructure is obtained.
The same information is usedto automatically create correct restructuringconstraints for translation relations which use thenew lemma as target.
If the parser does not find asound syntactic description, for example becausesome basic information or the expression ismissing in the lexical database, the lexicographer isasked for the missing information or is handedover the expression to code it manually.Using these tools importation of new lexicalinformation, as provided in the previous steps, isconsiderably accelerated.Step 8: Compile rule bases for new languagepairsAlthough experience clearly shows thatconstruction and maintenance of the dictionaries isby far the most expensive task in (rule-based)Machine Translation, the grammars (analysis andgeneration) must of course be developed andmaintained also.
Lingenio has longstandingexperience with the development of grammars,dictionaries and all other components of RBMT.The used grammar formalism (slot grammar,cf.
McCord 1991) is unification based and itsstructuring focuses on dependency, where phrasesare analysed into heads and grammatical roles ?
socalled (complement and adjunct) slots.The grammar formalism and basic rule typesare designed in a very general way in order toallow good portability from one language toanother such that spelling out the declarative partof a grammar does not take very much time (2-4person months approx.
for relatively similarlanguages like Romance languages according toour experience).
The portation of linguistic rules tonew languages is also facilitated by the modulardesign with clearly defined interfaces that make itrelatively straightforward to integrate informationfrom corpora.Given a parallel corpus as acquired in step 1,the following procedure defines grammar develop-ment:1.
Define a declarative grammar for the newlanguage and train this grammar on the parallel-corpus according to the following steps:2.
Use a chunk parser for the grammar on thebasis of an efficient part-of-speech tagger forthe new language.3.
Combine the chunk analyses of the sentence,according to suggestions for packed syntacticstructures (cf.
Schiehlen 2001 and others) andunderspecified representation structuresrespectively (cf.
Eberle, 2004, and others),such that the result represents a disjunction ofthe possible analyses of the sentence.4.
Filter the alternatives of the representation byusing mapping constraints between source andtarget sentence as can be computed from thelexical transfer relations and the structuralanalysis of the sentence.
For instance, if weknow, as in the example of the last section, thatin the source sentence there is a relative clausewith lexical elements A, B, .
.
.
modifying ahead H and that there are translations TH, TA,TB, .
.
.
of H, A, B,.
.
.
, in the target sentencewhich, among other possibilities, can besupposed to stand in a similar structuralrelation there, then we prefer this relation tothe competing structural possibilities.
(Fig.
3 insection results shows the correspondingselection for a German-Spanish example in theproject database).5.
For each of the remaining structuralpossibilities of the thus revised underspecifiedrepresentation, take its lexical material andunderspecified structuring as a context for itssuccessful firing.
For instance, if thepossibility is left that O is the direct object ofVP, where VP is an underspecified verbalphrase and O an underspecified nominalphrase (i.e.
where details of the substructuringare not spelled out), take the sentence as areference for direct object complementationand O and VP as contexts which accept thiscomplementation.1076.
Develop more abstract conditions from theconditions learned according to (5) andintegrate the different cases.7.
Tune the results using standard methods ofcorpus-based linguistics.
Among other thingsthis means: Distinguish between training andtest corpora, adjust weights according to theresults of test runs, etc.The basic idea of the proposed learning procedureis similar to that used with respect to learninglexical transfer relations: Do not define thestatistical model for the ?ignorant?
state, where thesurface items of the bilingual corpora areconsidered.
Instead, define it for appropriatemaximally abstract analyses of the sentences(which, of course, must be availableautomatically), because, then, much smaller sets ofdata will do.
Here, the important question is: Whatis the most abstract level of representation that canbe reached automatically and which shows reliableresults?
We think that it is the level ofunderspecified syntactic description as used in theprocedure above.The result of training the grammar is a set ofrules which assign weights and contexts to eachfiller rule of the declarative grammar and thusallow to estimate how likely it is that a particularrule is applied in a particular context in comparisonwith other rules (Fig.
4 and 5 in section resultsgive an overview of the relevance of  grammarrules and their triggering conditions w.r.t.German).We mentioned that the task of translating textsinto each other does not presuppose that eachambiguity in a source sentence is resolved.
On thecontrary, translation should be ambiguitypreserving (cf.
Kay, Gawron & Norvig 1994,compare the example above).
It is obvious thatunderspecified syntactic representations assuggested here are also especially suited forpreserving ambiguities appropriately.Step 9: Automatically evaluate translations ofthe most frequent grammatical constructionsand multiword expressions in a machine-translated corpusIn a later work package of the project, we will runa large parallel corpus through available(competitive) MT engines, which will be enhancedby automatic dictionaries developed during theprevious stages.
On the source-language side of thecorpus we will automatically generate lists offrequent multiword expressions (MWEs) andgrammatical constructions using the methodologyproposed in (Sharoff et al, 2006).
For each of theidentified MWEs and constructions we willgenerate a parallel concordance using open-sourceCSAR architecture developed by the Leeds team(Sharoff, 2006).
The concordance will begenerated by running queries to the sentence-aligned parallel corpora and will return lists ofcorresponding sentences from gold-standardhuman translations and corresponding sentencesgenerated by MT.
Each of these concordances willbe automatically evaluated using standard MTevaluation metrics, such as BLEU.
Under thesesettings parallel concordances will be used asstandard MT evaluation corpora in an automatedMT evaluation scenario.Normally BLEU gives reliable results for MTcorpora over 7000 words.
However, in (Babychand Hartley, 2009; Babych and Hartley, 2008) wedemonstrated that if the corpus is constructed inthis controlled way, where evaluated fragments ofsentences are selected as local contexts for specificmultiword expressions or grammaticalconstructions, then BLEU scores have another?island of stability?
for much smaller corpora,which now may consist of only five or morealigned concordance lines.
This concordance-basedevaluation scenario gives correct predictions oftranslation quality for the local context of each ofthe evaluated expressions.The scores for the evaluated MWEs andconstructions will be put in a risk-assessmentframework, where we will balance the frequencyof constructions and their translation quality.
Thetop priority receive the most frequent expressionsthat are the most problematic ones for a particularMT engine, i.e., with queries with lowest BLEUscores for their concordances.
This framework willallow MT developers to work down the priority listand correct or extend coverage for thoseconstructions which will have the biggest impacton MT quality.Step 10: Extend support for high-priorityconstructions semi-automatically by miningcorrect translations from parallel corporaAt this stage we will automate the procedure ofcorrecting errors and extending coverage for108problematic MWEs and grammaticalconstructions, identified in Step 9.
For this we willexploit alignment between source-languagesentences and gold-standard human translations.
Inthe target human translations we will identifylinguistically-motivated multiword expressions,e.g., using part-of-speech patterns or tf-idfdistribution templates (Babych et al, 2007) andrun standard alignment tools (e.g., GIZA++) forfinding the most probable candidate MWEs thatcorrespond to the problematic source-languageexpressions.
Source and target MWEs paired inthis way will form the basis for automatically-generated grammar rules.
The rules will normallygeneralise several pairs of MWEs, and may beunderspecified for certain lexical or morphologicalfeatures.
Later such rules will be manually checkedand corrected by language specialists in MTdevelopment teams that work on specifictranslation directions.This procedure will allow to speed up the grammardevelopment procedure for large-scale MT projectsand will focus on grammatical constructions withthe highest impact on MT quality, establishingthem as a top priority for MT developers.
InHyghTra and with respect to the languagesconsidered there, this procedure will be integratedinto the grammar development and optimization ofstep 8, in particular it will be related to step 4 ofthe procedure sketched there.
With regard tointegration, we aim at an interleaved architecture inthe long run.Step 11: Bootstrap the systemIn Step 11, the new grammar and the transfer ofthe new MT system and the new dictionary may bemutually trained further using the steps before andapplying the system to additional corpora.4 ResultsDeclarative slot grammars for Dutch and Spanishhave been developed using the patterns of Germanand French ?
where declarative  means that therehas been used no relevant semantic or otherinformation in order to spell out weighting orfilters for rule application -- the only constraintbeing morphosyntactic accessibility.
The necessarymorphological information has been adaptedsimilarly from the corresponding model languages.The basic dictionaries have been compiledmanually (Dutch) or extracted from a conventionalelectronic dictionary (translateDict Spanish).For a subset of the Spanish corpus (referencesentences of the grammar, parts of the open sourceLeeds corpus (Sharoff, 2006), and Europarl),syntactic analyses have been computed and storedin the database.
As the number of analyses growsextremely with the length of sentences, onlyrelatively short sentences (up to 15 words)  havebeen considered.
These analyses are currentlycompared to the analyses of the Germantranslations of the corresponding sentences (onetranslation per sentence), which are taken as a kindof 'gold' standard as the German analysiscomponent (as part of the translation products) hasproven to be sufficiently reliable.
On the basis ofthe comparison a preference on the competitiveanalyses of the Spanish sentence is entailed andused for defining a statistical evaluationcomponent for the Spanish grammar.
Fig.3 showsthe corresponding representations in the databasefor the sentence Aumenta la demana de energ?ael?ctrica por la ola de calor3  and its translation dieNachfrage nach Strom steigt wegen derHitzewelle/the demand for electricity increasesbecause of the heat-wave.Fig.3 Selection of analyses via correspondences(prefer first Spanish analysis because of subj-congruity)The analyses are associated with the correspondingcreation protocols, which are structured lists whoseitems describe, via the identifiers, which rule hasbeen applied when and to what structures in theprocess of creating the analysis.
From the selectionof a best analysis for a sentence, we can entail thecircumstances under which the application ofparticular rules are preferred.
This has been carried3 Sentence taken from the online newspaper El D?a deConcepci?n del Uruguay109out - not yet for the 'new' language Spanish, but forthe 'known' language German, in order to obtain ameasure about how correctly the existing grammarevaluation component can be replaced by theresults of the corresponding statistical study.Fig.4  Frequency of applications of rulesclusterapplicationssimilarity feas  mod feas head383, 384,.. 0,86 sent, ... emosentaffv,..557,558,566,.. 0,68 denselb,.. gebv, ...Fig.5  Preliminary constraints related to grammarrule clustersFig.4 shows the distribution of rule usages withinthe training set of analyses (of approx.30.000sentences).
390 different rules were used with atotal of 133708 rule applications.
The subject rule(383) and the noun determiner rule (46) the mostused rules (35% of all applications).
Fig 5.illustrates the preliminary results of a clusteringalgorithm where different rule applications aregrouped into clusters and the key features of thehead and modifier phrases for each cluster areextracted.Currently, we try to determine further and tarethe linguistic features and the weighting whichmodels best the evaluation for German.
(The goldstandard that is used in this test is the set ofanalyses mentioned above).
The investigations arenot yet completed, but preliminary results on thebasis of the morphosyntactic and semanticproperties of the neighboring elements arepromising.
After consolidation, the findings will betransferred to Spanish on the basis of the selectionprocedure illustrated in Fig.
3.
The next step ofgrammar training in the immediate future willconsist of  changing the focus to underspecifiedanalyses as described in step 85 ConclusionsThe project tries to make state-of-the-art statisticalmethods available for dictionary development andgrammar development for a rule-based dominatedindustrial setting and to exploit such methodsthere.With regard to SMT dictionary creation, it goesbeyond the current state of the art as it also aims atdeveloping and applying algorithms for the semi-automatic generation of bilingual dictionaries fromunrelated monolingual (i.e., comparable) corporaof the source and the target language, instead ofusing relatively literally translated (i.e., parallel)texts only.
Comparable corpora are far easier toobtain than parallel corpora.
Therefore theapproach offers a solution to the serious dataacquisition bottleneck in SMT.
This approach isalso more cognitively plausible than previoussuggestions on this topic, since human bilingualityis normally not based on memorizing parallel texts.Our suggestion models human capacity to translatetexts using linguistic knowledge acquired frommonolingual data, so it also exemplifies manymore features of a truly self-learning MT system(shared also by a human translator).In addition, the proposal suggests a newmethod for spelling out grammars and parsers forlanguages by splitting grammars into declarativekernels and trainable decision algorithms and byexploiting cross-linguistic knowledge foroptimizing the results of the corresponding parsers.For developing different components anddictionaries for the system a bootstrappingarchitecture is suggested that uses the acquiredlexical information for training the grammar of thenew language, which in turn uses the(underspecified) parser results for optimizing thelexical information in the corresponding translationdictionaries.
We expect that the suggested methodssignificantly improve translation quality andreduce the costs of creating new language pairs forMachine Translation.
The preliminary resultsobtained so far in the project appear promising.6 AcknowledgmentsThis research is supported by a Marie Curie IAPPproject taking place within the 7th EuropeanCommunity Framework Programme (Grantagreement no.
: 251534)1107 ReferencesArmstrong, S.; Kempen, M.; McKelvie, D.; Petitpierre, D.;Rapp, R.; Thompson, H. (1998).
Multilingual Corporafor Cooperation.
Proceedings of the 1st InternationalConference on Linguistic Resources and Evaluation(LREC), Granada, Vol.
2, 975?980.Babych, B., Hartley, A., Sharoff S.; Mudraya, O.
(2007).Assisting Translators in Indirect Lexical Transfer.Proceedings of the 45th Annual Meeting of the ACL.Babych, B., Anthony Hartley, & Serge Sharoff (2007b)Translating from under-resourced languages:comparing direct transfer against pivot translation.Proceedings of MT Summit XI, 10-14 September2007, Copenhagen, Denmark, 29-35Babych, B.
& Hartley, A.
(2008).
Automated MT Evaluationfor Error Analysis: Automatic Discovery of PotentialTranslation Errors for Multiword Expressions.
ELRAWorkshop on Evaluation ?Looking into the Future ofEvaluation: When automatic metrics meet task-basedand performance-based approaches?.
Marrakech,Morocco 27 May 2008.
Proceedings of LREC?08.Babych, B. and Hartley, A.
(2009).
Automated error analysisfor multiword expressions: using BLEU-type scoresfor automatic discovery of potential translation errors.Linguistica Antverpiensia, New Series (8/2009):Journal of translation and interpreting studies.
SpecialIssue on Evaluation of Translation Technology.Babych, B., Babych, S. and Eberle, K. (2012).
Derivinggeneration-oriented MT resources from corpora: casestudy and evaluation of de/het classification for DutchNoun (in preparation)Baroni, M.; Bernardini, S. (2004).
BootCaT: Bootstrappingcorpora and terms from the web.
Proceedings ofLREC 2004.Callison-Burch, C., Miles Osborne, & Philipp Koehn: Re-evaluating the role of BLEU in machine translationresearch.
EACL-2006: 11th Conference of theEuropean Chapter of the Association forComputational Linguistics, Trento, Italy, April 3-7,2006; pp.249-256Charniak, E.; Knight, K.; Yamada, K. (2003).
Syntax-basedlanguage models for statistical machine translation".Proceedings of MT Summit IX.Eberle, Kurt (2001).
FUDR-based MT, head switching and thelexicon.
Proceedings of the the eighth MachineTranslation Summit, Santiage de Compostela.Eberle, Kurt (2004).
Flat underspecified representation and itsmeaning for a fragment of German.Habilitationsschrift, Universit?t Stuttgart.Eberle, K.; Rapp, R. (2008).
Rapid Construction ofExplicative Dictionaries Using Hybrid MachineTranslation.
In: Storrer, A.;  Geyken, A.; Siebert, A.;W?rzner, K._M (eds.)
Text Resources and LexicalKnowledge: Selected Papers from the 9th Conferenceon Natural Language Processing KONVENS 2008.Berlin: Mouton de Gruyter..Eckart,K., Eberle, K.; Heid, U.
(2010) An infrastructure formore reliable corpus analysis.
Proceedings of theWorkshop on Web Services and Processing Pipelinesin HLT of LREC-2010 , Valetta.Eberle, K.; Eckart,K., Heid, U.,Haselbach, B.
(2012) Atool/database interface for multi-level analyses.Proceedings of LREC-2012 , Istanbul.Frederking, R.; Nirenburg, S.; Farwell, D.;  Helmreich, S.;Hovy, E.; Knight, K.; Beale, S.; Domashnev, C.;Attardo, D.; Grannes, D.; Brown, R. (1994).
IntegratedTranslation from Multiple Sources within the PanglossMARK II Machine Translation System.
Proceedingsof Machine Translation of the Americas, 73?80.Frederking, Robert and Sergei Nirenburg (1994).
Three headsare better than one.
In: Proceedings of ANLP-94,Stuttgart, Germany.Fung, P.; McKeown, K. (1997).
Finding terminologytranslations from non-parallel corpora.
Proceedings ofthe 5th Annual Workshop on Very Large Corpora,Hong Kong: August 1997, 192-202.Gale, W.A.
; Church, K.W.
(1993).
A progrm for aligningsentences in bilingual corpora.
ComputationalLinguistics, 19(1), 75?102.Gonz?lez, J.; Antonio L. Lagarda, Jos?
R. Navarro, LauraEliodoro, Adri?
Gim?nez, Francisco Casacuberta, JoanM.
de Val and Ferran Fabregat (2004).
SisHiTra: ASpanish-to-Catalan hybrid machine translation system.Berlin: Springer LNCS.Gough, N., Way, A.
(2004).
Example-Based ControlledTranslation.
Proceedings of the Ninth Workshop of theEuropean Association for Machine Translation,Valetta, Malta.Groves, D. & Way, A.
(2006b).
Hybridity in MT: Experimentson the Europarl Corpus.
In Proceedings of the 11thConference of the European Association for MachineTranslation, Oslo, Norway, 115?124.Groves, D.; Way, A.
(2006a).
Hybrid data-driven models ofmachine translation.
Machine Translation, 19(3?4).Special Issue on Example-Based Machine Translation.301?323.Habash, N.; Dorr, B.
(2002).
Handling translationdivergences: Combining statistical and symbolictechniques in generation-heavy machine translation.Proceedings of AMTA-2002, Tiburon, California,USA.Kiss, T.; Strunk, J.
(2006): Unsupervised multilingualsentence boundary detection.
ComputationalLinguistics 32(4), 485?525.Koehn, P. (2005).
Europarl: A Parallel Corpus for StatisticalMachine Translation.
Proceedings of MT Summit X,Phuket, ThailandKoehn, P.; Knight, K. (2002).
Learning a translation lexiconfrom monolingual corpora.
In: Proceedings of ACL-02Workshop on Unsupervised Lexical Acquisition,Philadelphia PA.Language Industry Monitor (1992).
Statistical methodsgaining ground.
In: Language Industry Monitor,September/October 1992 issue.111McCord, M. (1989).
A new version of the machine translationsystem LMT.
Journal of Literary and LinguisticComputing, 4, 218?299.McCord, M. (1991).
The slot grammar system.
In: Wedekind,J., Rohrer, C.(eds): Unification in Grammar, MIT-Press.Melamed, I. Dan (1999).
Bitext maps and aligment via patternrecognition.
Computational Linguistics, 25(1), 107?130.Munteanu, D.S.
; Marcu, D. (2005).
Improving machinetranslation performance by exploiting non-parallelcorpora.
Computational Linguistics, 31(4), 477?504.Och, F.J.; Ney, H. (2002).
Discriminative trainig andmaximum entropy models for statistical machinetranslation.
Proceedings of the  Annual Meeting of theAssociation for Computational Linguistics,Philadelphia, PA, 295?302.Och, F.J.; Ney, H. (2003).
A systematic comparison of variousstatistical alignment models.
ComputationalLinguistics, 29(1), 19?51.Papineni, K.; Roukos, S.; Ward, T.; Zhu, W. (2002).
BLEU: Amethod for automatic evaluation of machinetranslation.
In: Proceedings of the 40th AnnualMeeting of the ACL, Philadelphia, PA, 311?318.Rapp, R. (1995).
Identifying word translations in non-paralleltexts.
In: Proceedings of the 33rd Meeting of theAssociation for Computational Linguistics.Cambridge, MA, 1995, 320?322Rapp, R. (1999).
Automatic identification of word translationsfrom unrelated English and German corpora.
In:Proceedings of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics 1999, CollegePark, Maryland.
519?526.Rapp, R. (2004).
A freely available automatically generatedthesaurus of related words.
In: Proceedings of theFourth International Conference on LanguageResources and Evaluation (LREC), Lisbon, Vol.
II,395?398.Rapp, R.; Martin Vide, C. (2007).
Statistical machinetranslation without parallel corpora.
In: Georg Rehm,Andreas Witt, Lothar Lemnitzer (eds.
): DataStructures for Linguistic Resources and Applications.Proceedings of the Biennial GLDV Conference 2007.T?bingen: Gunter Narr.
231?240Resnik, R. (1999).
Mining the web for bilingual text.Proceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics.Sato, S.; Nagao, M. (1990).
Toward memory-basedtranslation.
Proceedings of COLING 1990, 247?252.Schiehlen, M. (2001) Syntactic Underspecification.
In: SpecialResearch Area 340 ?
Final report, University ofStuttgart.Sharoff, S. (2006) Open-source corpora: using the net to fishfor linguistic data.
In International Journal of CorpusLinguistics 11(4), 435?462.Sharoff, S.; Babych, B.; Hartley, A.
(2006).
Using comparablecorpora to solve problems difficult for humantranslators.
In: Proceedings of COLING/ACL 2006,739?746.Sharoff, S. (2006).
A uniform interface to large-scalelinguistic resources.
In Proceedings of the FifthLanguage Resources and Evaluation Conference,LREC-2006, Genoa.Simard, M., Foster, G., Isabelle, P. (1992).
Using Cognates toAlign Sentences in Bilingual Corpora.
Proceeedings ofthe International Conference on Theoretical andMethodological Issues, Montr?al.Smadja, F. (1993).
Retrieving collocations from text: Xtract.Computational Linguistics, 19(1), 143?177.Streiter, O., Carl, M., Haller, J.
(eds)(1999).
HybridApproaches to Machine Translation.
IAI workingpapers 36.Streiter, O.; Carl, M.; Iomdin, L.L.
: 2000, A VirtualTranslation Machine for Hybrid Machine Translation'.In: Proceedings of the Dialogue'2000 InternationalSeminar in Computational Linguistics andApplications.
Tarusa, Russia.Streiter, O.; Iomdin, L.L.
(2000).
Learning Lessons fromBilingual Corpora: Benefits for Machine Translation.International Journal of Corpus Linguistics, 5(2), 199?230.Thurmair, G. (2005).
Hybrid architectures for machinetranslation systems.
Language Resources andEvaluation, 39 (1), 91?108.Thurmair, G. (2006).
Using corpus information to improveMT quality.
Proceedings of the LR4Trans-IIIWorkshop, LREC, Genova.Thurmair, G. (2007) Automatic evaluation in MT systemproduction.
MT Summit XI Workshop: Automaticprocedures in MT evaluation, 11 September 2007,Copenhagen, Denmark,Veronis, Jean (2006).
Technologies du Langue.
Actualit?s ?Comentaires ?
R?flexions.
Translation.
Systran orReverso?http://aixtal.blogspot.com/2006/01/translation-systran-or-reverso.htmlWu, D., Fung, P. (2005).
Inversion transduction grammarconstraints for mining parallel sentences from quasi-comparable corpora.
Second International JointConference on Natural Language Processing(IJCNLP-2005).
Jeju, Korea.112
