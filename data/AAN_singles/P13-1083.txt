Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 841?851,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsPart-of-Speech Induction in Dependency Trees for Statistical MachineTranslationAkihiro Tamura?,?, Taro Watanabe?, Eiichiro Sumita?,Hiroya Takamura?, Manabu Okumura??
National Institute of Information and Communications Technology{akihiro.tamura, taro.watanabe, eiichiro.sumita}@nict.go.jp?
Precision and Intelligence Laboratory, Tokyo Institute of Technology{takamura, oku}@pi.titech.ac.jpAbstractThis paper proposes a nonparametricBayesian method for inducing Part-of-Speech (POS) tags in dependency treesto improve the performance of statisticalmachine translation (SMT).
In particular,we extend the monolingual infinite treemodel (Finkel et al, 2007) to a bilin-gual scenario: each hidden state (POS tag)of a source-side dependency tree emits asource word together with its aligned tar-get word, either jointly (joint model), orindependently (independent model).
Eval-uations of Japanese-to-English translationon the NTCIR-9 data show that our in-duced Japanese POS tags for dependencytrees improve the performance of a forest-to-string SMT system.
Our independentmodel gains over 1 point in BLEU by re-solving the sparseness problem introducedin the joint model.1 IntroductionIn recent years, syntax-based SMT has madepromising progress by employing either depen-dency parsing (Lin, 2004; Ding and Palmer, 2005;Quirk et al, 2005; Shen et al, 2008; Mi and Liu,2010) or constituency parsing (Huang et al, 2006;Liu et al, 2006; Galley et al, 2006; Mi and Huang,2008; Zhang et al, 2008; Cohn and Blunsom,2009; Liu et al, 2009; Mi and Liu, 2010; Zhanget al, 2011) on the source side, the target side,or both.
However, dependency parsing, whichis a popular choice for Japanese, can incorporateonly shallow syntactic information, i.e., POS tags,compared with the richer syntactic phrasal cate-gories in constituency parsing.
Moreover, exist-ing POS tagsets might not be optimal for SMTbecause they are constructed without consideringthe language in the other side.
Consider the ex-amples in Figure 1.
The Japanese noun ????
in?
?
??
??
?
?????
?
???????
?
????
?
?You can not use the Internet  .I  pay  usage fees  .noun particle particlenoun noun verb auxiliary verbnoun particle noun noun verbparticle[Example 1][Example 2]Japanese POS:Japanese POS:Figure 1: Examples of Existing Japanese POSTags and Dependency StructuresExample 1 corresponds to the English verb ?use?,while that in Example 2 corresponds to the Englishnoun ?usage?.
Thus, Japanese nouns act like verbsin English in one situation, and nouns in Englishin another.
If we could discriminate POS tags fortwo cases, we might improve the performance of aJapanese-to-English SMT system.In the face of the above situations, this pa-per proposes an unsupervised method for inducingPOS tags for SMT, and aims to improve the perfor-mance of syntax-based SMT by utilizing the in-duced POS tagset.
The proposed method is basedon the infinite tree model proposed by Finkel etal.
(2007), which is a nonparametric Bayesianmethod for inducing POS tags from syntactic de-pendency structures.
In this model, hidden statesrepresent POS tags, the observations they generaterepresent the words themselves, and tree structuresrepresent syntactic dependencies between pairs ofPOS tags.The proposed method builds on this model byincorporating the aligned words in the other lan-guage into the observations.
We investigate twotypes of models: (i) a joint model and (ii) an in-dependent model.
In the joint model, each hid-den state jointly emits both a source word and itsaligned target word as an observation.
The in-dependent model separately emits words in twolanguages from hidden states.
By inferring POS841tags based on bilingual observations, both mod-els can induce POS tags by incorporating infor-mation from the other language.
Consider, for ex-ample, inducing a POS tag for the Japanese word ????
in Figure 1.
Under a monolingual inductionmethod (e.g., the infinite tree model), the ???
?in Example 1 and 2 would both be assigned thesame POS tag since they share the same observa-tion.
However, our models would assign separatetags for the two different instances since the ????
in Example 1 and Example 2 could be disam-biguated by encoding the target-side information,either ?use?
or ?usage?, in the observations.Inference is efficiently carried out by beam sam-pling (Gael et al, 2008), which combines slicesampling and dynamic programming.
Experi-ments are carried out on the NTCIR-9 Japanese-to-English task using a binarized forest-to-stringSMT system with dependency trees as its sourceside.
Our bilingually-induced tagset signifi-cantly outperforms the original tagset and themonolingually-induced tagset.
Further, our inde-pendent model achieves a more than 1 point gainin BLEU, which resolves the sparseness problemintroduced by the bi-word observations.2 Related WorkA number of unsupervised methods have beenproposed for inducing POS tags.
Early methodshave the problem that the number of possible POStags must be provided preliminarily.
This limita-tion has been overcome by automatically adjust-ing the number of possible POS tags using non-parametric Bayesian methods (Finkel et al, 2007;Gael et al, 2009; Blunsom and Cohn, 2011; Sirtsand Aluma?e, 2012).
Gael et al (2009) appliedinfinite HMM (iHMM) (Beal et al, 2001; Tehet al, 2006), a nonparametric version of HMM,to POS induction.
Blunsom and Cohn (2011)used a hierarchical Pitman-Yor process prior to thetransition and emission distribution for sophisti-cated smoothing.
Sirts and Aluma?e (2012) built amodel that combines POS induction and morpho-logical segmentation into a single learning prob-lem.
Finkel et al (2007) proposed the infinitetree model, which represents recursive branchingstructures over infinite hidden states and inducesPOS tags from syntactic dependency structures.
Inthe following, we overview the infinite tree model,which is the basis of our proposed model.
In par-ticular, we will describe the independent childrenH ?k?k?
z1z2 z3x1 x2 x3k=1,?,CHkk~),...,(Dirichlet~|?
??
?piFigure 2: A Graphical Representation of the FiniteTree Modelmodel (Finkel et al, 2007), where children aredependent only on their parents, used in our pro-posed model1.2.1 Finite Tree ModelWe first review the finite tree model, which canbe graphically represented in Figure 2.
LetTt denote the tree whose root node is t. Anode t has a hidden state zt (the POS tag)and an observation xt (the word).
The prob-ability of a tree Tt, pT (Tt), is recursively de-fined: pT (Tt) = p(xt|zt)?t??c(t)p(zt?
|zt)pT (Tt?
),where c(t) is the set of the children of t.Let each hidden state variable have C possiblevalues indexed by k. For each state k, there isa parameter ?k which parameterizes the observa-tion distribution for that state: xt|zt ?
F (?zt).
?kis distributed according to a prior distribution H:?k ?
H .Transitions between states are governed byMarkov dynamics parameterized by pi, where?ij = p(zc(t) = j|zt = i) and pik are the transitionprobabilities from the parent?s state k. pik is dis-tributed according to a Dirichlet distribution withparameter ?
: pik|?
?
Dirichlet(?, .
.
.
, ?).
Thehidden state of each child zt?
is distributed accord-ing to a multinomial distributionpizt specific to theparent?s state zt: zt?
|zt ?
Multinomial(pizt).2.2 Infinite Tree ModelIn the infinite tree model, the number of possiblehidden states is potentially infinite.
The infinitemodel is formed by extending the finite tree modelusing a hierarchical Dirichlet process (HDP) (Tehet al, 2006).
The reason for using an HDP rather1Finkel et al (2007) originally proposed three types ofmodels: besides the independent children model, the simul-taneous children model and the markov children model.
Al-though we could apply the other two models, we leave thisfor future work.842H ?k?k?0 z1z2 z3x1 x2 x3??
?Hkk~),(DP~,|)(GEM~|00?
????pi??
?Figure 3: A Graphical Representation of the Infi-nite Tree Modelthan a simple Dirichlet process (DP)2 (Ferguson,1973) is that we have to introduce coupling acrosstransitions from different parent?s states.
A similarmeasure was adopted in iHMM (Beal et al, 2001).HDP is a set of DPs coupled through a sharedrandom base measure which is itself drawn froma DP: each Gk ?
DP(?0, G0) with a shared basemeasure G0, and G0 ?
DP(?,H) with a globalbase measure H .
From the viewpoint of the stick-breaking construction3 (Sethuraman, 1994), theHDP is interpreted as follows: G0 =??k?=1?k??
?k?and Gk =??k?=1?kk???k?
, where ?
?
GEM(?
),pik ?
DP(?0,?
), and ?k?
?
H .We regard each Gk as two coindexed distribu-tions: pik, a distribution over the transition prob-abilities from the parent?s state k, and ?k?
, an ob-servation distribution for the state k?.
Then, theinfinite tree model is formally defined as follows:?|?
?
GEM(?),pik|?0,?
?
DP(?0,?
),?k ?
H,zt?
|zt ?
Multinomial(pizt),xt|zt ?
F (?zt).Figure 3 shows the graphical representation of theinfinite tree model.
The primary difference be-2DP is a measure on measures.
It has two parameters, ascaling parameter ?
and a base measure H: DP (?,H).3Sethuraman (1994) showed a definition of a measureG ?
DP(?0, G0).
First, infinite sequences of i.i.d variables(?
?k)?k=1 and (?k)?k=1 are generated: ?
?k|?0 ?
Beta(1, ?0),?k ?
G0.
Then, G is defined as: ?k = ?
?k?k?1l=1 (1 ?
?
?l),G = ?
?k=1 ?k?
?k .
If pi is defined by this process, then wewrite pi ?
GEM(?0).H ?k?k?0??
?
z1z2 z3z4 z5 z6???+pay?
??????+fees?
???+usage???+I?
??
?Figure 4: An Example of the Joint Modeltween Figure 2 and Figure 3 is whether the numberof copies of the state is finite or not.3 Bilingual Infinite Tree ModelWe propose a bilingual variant of the infinite treemodel, the bilingual infinite tree model, which uti-lizes information from the other language.
Specifi-cally, the proposed model introduces bilingual ob-servations by embedding the aligned target wordsin the source-side dependency trees.
This paperproposes two types of models that differ in theirprocesses for generating observations: the jointmodel and the independent model.3.1 Joint ModelThe joint model is a simple application of the in-finite tree model under a bilingual scenario.
Themodel is formally defined in the same way as inSection 2.2 and is graphically represented simi-larly to Figure 3.
The only difference from theinfinite tree model is the instances of observations(xt).
Observations in the joint model are the com-bination of source words and their aligned targetwords4, while observations in the monolingual in-finite tree model represent only source words.
Foreach source word, all the aligned target words arecopied and sorted in alphabetical order, and thenconcatenated into a single observation.
Therefore,a single target word may be emitted multiple timesif the target word is aligned with multiple sourcewords.
Likewise, there may be target words whichmay not be emitted by our model, if the targetwords are not aligned.Figure 4 shows the process of generating Exam-ple 2 in Figure 1 through the joint model, wherealigned words are jointly emitted as observations.In Figure 4, the POS tag of ????
(z5) generates4When no target words are aligned, we simply add aNULL target word.843H ?k?k?0?
?
z1z2 z3z4 z5H?
?'k??
payI???????
NONE NONEusagefees z6?
'~',~),(DP~,|)(GEM~|00 HH kkk ??
????pi??
?Figure 5: A Graphical Representation of the Inde-pendent Modelthe string ???+usage?
as the observation (x5).Similarly, the POS tag of ????
in Example 1would generate the string ???+use?.
Hence, thismodel can assign different POS tags to the two dif-ferent instances of the word ???
?, based on thedifferent observation distributions in inference.3.2 Independent ModelThe joint model is prone to a data sparseness prob-lem, since each observation is a combination of asource word and its aligned target word.
Thus, wepropose an independent model, where each hiddenstate generates a source word and its aligned targetword separately.
For the aligned target side, we in-troduce an observation variable x?t for each zt anda parameter ?
?k for each state k, which parame-terizes a distinct distribution over the observationsx?t for that state.
?
?k is distributed according to aprior distribution H ?.
Specifically, the indepen-dent model is formally defined as follows:?|?
?
GEM(?),pik|?0,?
?
DP(?0,?
),?k ?
H, ?
?k ?
H ?,zt?
|zt ?
Multinomial(pizt),xt|zt ?
F (?zt), x?t|zt ?
F ?(?
?zt).When multiple target words are aligned to a singlesource word, each aligned word is generated sepa-rately from observation distribution parameterizedby ?
?k.Figure 5 graphs the process of generating Ex-ample 2 in Figure 1 using the independent model.x?t and ?
?k are introduced for aligned target words.The state of ????
(z5) generates the Japaneseword ????
as x5 and the English word ?usage?as x?5.
Due to this factorization, the independentmodel is less subject to the sparseness problem.3.3 Introduction of Other FactorsWe assumed the surface form of aligned targetwords as additional observations in previous sec-tions.
Here, we introduce additional factors, i.e.,the POS of aligned target words, in the observa-tions.
Note that POSs of target words are assignedby a POS tagger in the target language and are notinferred in the proposed model.First, we can simply replace surface forms oftarget words with their POSs to overcome thesparseness problem.
Second, we can incorporateboth information from the target language as ob-servations.
In the joint model, two pieces of in-formation are concatenated into a single observa-tion.
In the independent model, we introduce ob-servation variables (e.g., x?t and x?
?t ) and parame-ters (e.g., ?
?k and ??
?k) for each information.
Specif-ically, x?t and ?
?k are introduced for the surfaceform of aligned words, and x?
?t and ??
?k for the POSof aligned words.
Consider, for example, Example1 in Figure 1.
The POS tag of ????
generates thestring ???+use+verb?
as the observation in thejoint model, while it generates ???
?, ?use?, and?verb?
independently in the independent model.3.4 POS RefinementWe have assumed a completely unsupervised wayof inducing POS tags in dependency trees.
An-other realistic scenario is to refine the existing POStags (Finkel et al, 2007; Liang et al, 2007) sothat each refined sub-POS tag may reflect the in-formation from the aligned words while preserv-ing the handcrafted distinction from original POStagset.
Major difference is that we introduce sep-arate transition probabilities pisk and observationdistributions (?sk, ?
?sk ) for each existing POS tag s.Then, each node t is constrained to follow the dis-tributions indicated by the initially assigned POStag st, and we use the pair (st, zt) as a state repre-sentation.3.5 InferenceIn inference, we find the state set that maximizesthe posterior probability of state transitions givenobservations (i.e., P (z1:n|x1:n)).
However, wecannot evaluate the probability for all possiblestates because the number of states is infinite.Finkel et al (2007) presented a sampling algo-rithm for the infinite tree model, which is based onthe Gibbs sampling in the direct assignment rep-resentation for iHMM (Teh et al, 2006).
In the844Gibbs sampling, individual hidden state variablesare resampled conditioned on all other variables.Unfortunately, its convergence is slow in HMMsettings because sequential data is likely to havea strong correlation between hidden states (Gaelet al, 2008).We present an inference procedure based onbeam sampling (Gael et al, 2008) for the jointmodel and the independent model.
Beam sam-pling limits the number of possible state transi-tions for each node to a finite number using slicesampling (Neal, 2003), and then efficiently sam-ples whole hidden state transitions using dynamicprogramming.
Beam sampling does not sufferfrom slow convergence as in Gibbs sampling bysampling the whole state variables at once.
In ad-dition, Gael et al (2008) showed that beam sam-pling is more robust to initialization and hyperpa-rameter choice than Gibbs sampling.Specifically, we introduce an auxiliary variableut for each node in a dependency tree to limitthe number of possible transitions.
Our procedurealternates between sampling each of the follow-ing variables: the auxiliary variables u, the stateassignments z, the transition probabilities pi, theshared DP parameters ?, and the hyperparameters?0 and ?.
We can parallelize procedures in sam-pling u and z because the slice sampling for u andthe dynamic programing for z are independent foreach sentence.
See Gael el al.
(2009) for details.The only difference between inferences in thejoint model and the independent model is in com-puting the posterior probability of state transi-tions given observations (e.g., p(z1:n|x1:n) andp(z1:n|x1:n, x?1:n)) in sampling z.
In the follow-ing, we describe each sampling stage.
See Teh etal., (2006) for details of sampling pi, ?, ?0 and ?.Sampling u:Each ut is sampled from the uniform distribu-tion on [0, ?zd(t)zt ], where d(t) is the parent oft: ut ?
Uniform(0, ?zd(t)zt).
Note that ut is apositive number, since each transition probability?zd(t)zt is larger than zero.Sampling z:Possible values k of zt are divided into the twosets using ut: a finite set with ?zd(t)k > ut andan infinite set with ?zd(t)k ?
ut.
The beamsampling considers only the former set.
Owingto the truncation of the latter set, we can computethe posterior probability of a state zt given ob-servations for all t (t = 1, .
.
.
, T ) using dynamicprogramming as follows:In the joint model, p(zt|x?
(t), u?
(t)) ?p(xt|zt) ??zd(t):?zd(t)zt>utp(zd(t)|x?
(d(t)), u?
(d(t))),and in the independent model,p(zt|x?
(t), x??
(t), u?
(t)) ?
p(xt|zt) ?
p(x?t|zt)??zd(t):?zd(t)zt>utp(zd(t)|x?
(d(t)), x??
(d(t)), u?
(d(t))),where x?
(t) (or u?
(t)) denotes the set of xt (or ut)on the path from the root node to the node t in atree.In our experiments, we assume that F (?k)is Multinomial(?k) and H is Dirichlet(?, .
.
.
, ?
),which is the same in Finkel et al (2007).
Un-der this assumption, the posterior probability of anobservation is as follows: p(xt|zt) =n?xtk + ?n?
?k + N?,where n?xk is the number of observations x withstate k, n?
?k is the number of hidden states whosevalues are k, and N is the total number of observa-tions x.
Similarly, p(x?t|zt) =n?x?tk + ??n?
?k + N ??
?, whereN ?
is the total number of observations x?.When the posterior probability of a state ztgiven observations for all t can be computed,we first sample the state of each leaf node andthen perform backtrack sampling for every otherzt where the zt is sampled given the samplefor zc(t) as follows: p(zt|zc(t), x1:T , u1:T ) ?p(zt|x?
(t), u?(t))?t?
?c(t) p(zt?
|zt, ut?
).Sampling pi:We introduce a count variable nij ?
n,which is the number of observations withstate j whose parent?s state is i. Then,we sample pi using the Dirichlet distri-bution: (?k1, .
.
.
, ?kK ,?
?k?=K+1 ?kk?)
?Dirichlet(nk1 + ?0?1, .
.
.
, nkK +?0?K , ?0?
?k?=K+1 ?k?
), where K is thenumber of distinct states in z.Sampling ?
:We introduce a set of auxiliary variables m, wheremij ?
m is the number of elements of pijcorresponding to ?i.
The conditional distribu-tion of each variable is p(mij = m|z,?, ?0) ?S(nij ,m)(?0?j)m, where S(n,m) are unsignedStirling numbers of the first kind5.5S(0, 0) = S(1, 1) = 1, S(n, 0) = 0 for n > 0,S(n,m) = 0 for m > n, and S(n + 1,m) = S(n,m ?1) + nS(n,m) for others.845The parameters ?
are sampled using the Dirich-let distribution: (?1, .
.
.
, ?K ,?
?k?=K+1 ?k?)
?Dirichlet(m?1, .
.
.
,m?K , ?
), where m?k =?Kk?=1 mk?k.Sampling ?0:?0 is parameterized by a gamma hyperpriorwith hyperparameters ?a and ?b.
We introducetwo types of auxiliary variables for each state(k = 1, .
.
.
,K), wk ?
[0, 1] and vk ?
{0, 1}.The conditional distribution of each wk isp(wk|?0) ?
w?0k (1?wk)n?k?1 and that of each vkis p(vk|?0) ?
(n?k?0)vk, where n?k =?Kk?=1 nk?k.The conditional distribution of ?0 given wkand vk (k = 1, .
.
.
,K) is p(?0|w,v) ???a?1+m..?
?Kk=1 vk0 e??0(?b?
?Kk=1 logwk), wherem??
=?Kk?=1?Kk?
?=1 mk?k??
.Sampling ?:?
is parameterized by a gamma hyperprior withhyperparameters ?a and ?b.
We introduce anauxiliary variable ?, whose conditional distribu-tion is p(?|?)
?
??
(1 ?
?)m???1.
The con-ditional distribution of ?
given ?
is p(?|?)
???a?1+Ke??(?b?log?
).4 ExperimentWe tested our proposed models under theNTCIR-9 Japanese-to-English patent translationtask (Goto et al, 2011), consisting of approxi-mately 3.2 million bilingual sentences.
Both thedevelopment data and the test data consist of 2,000sentences.
We also used the NTCIR-7 develop-ment data consisting of 2,741 sentences for devel-opment testing purposes.4.1 Experimental SetupWe evaluated our bilingual infinite tree modelfor POS induction using an in-house developedsyntax-based forest-to-string SMT system.
Inthe training process, the following steps are per-formed sequentially: preprocessing, inducing aPOS tagset for a source language, training a POStagger and a dependency parser, and training aforest-to-string MT model.Step 1.
PreprocessingWe used the first 10,000 Japanese-English sen-tence pairs in the NTCIR-9 training data for in-ducing a POS tagset for Japanese6.
The Japanesesentences were segmented using MeCab7, and theEnglish sentences were tokenized and POS taggedusing TreeTagger (Schmid, 1994), where 43 and58 types of POS tags are included in the Japanesesentences and the English sentences, respectively.The Japanese POS tags come from the second-level POS tags in the IPA POS tagset (Asahara andMatsumoto, 2003) and the English POS tags arederived from the Penn Treebank.
Note that theJapanese POS tags are used for initialization ofhidden states and the English POS tags are usedas observations emitted by hidden states.Word-by-word alignments for the sentencepairs are produced by first running GIZA++ (Ochand Ney, 2003) in both directions and then com-bining the alignments using the ?grow-diag-final-and?
heuristic (Koehn et al, 2003).
Note that weran GIZA++ on all of the NTCIR-9 training datain order to obtain better alignements.The Japanese sentences are parsed usingCaboCha (Kudo and Matsumoto, 2002), whichgenerates dependency structures using a phrasalunit called a bunsetsu8, rather than a word unit asin English or Chinese dependency parsing.
Sincewe focus on the word-level POS induction, eachbunsetsu-based dependency tree is converted intoits corresponding word-based dependency tree us-ing the following heuristic9: first, the last func-tion word inside each bunsetsu is identified asthe head word10; then, the remaining words aretreated as dependents of the head word in the samebunsetsu; finally, a bunsetsu-based dependencystructure is transformed to a word-based depen-dency structure by preserving the head/modifierrelationships of the determined head words.Step 2.
POS InductionA POS tag for each word in the Japanese sentencesis inferred by our bilingual infinite tree model, ei-6Due to the high computational cost, we did not use allthe NTCIR-9 training data.
We leave scaling up to a largerdataset for future work.7http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html8A bunsetsu is the smallest meaningful sequence con-sisting of a content word and accompanying function words(e.g., a noun and a particle).9We could use other word-based dependency trees suchas trees by the infinite PCFG model (Liang et al, 2007)and syntactic-head or semantic-head dependency trees inNakazawa and Kurohashi (2012), although it is not our majorfocus.
We leave this for future work.10If no function words exist in a bunsetsu, the last contentword is treated as the head word.846ther jointly (Joint) or independently (Ind).
Wealso performed monolingual induction of Finkel etal.
(2007) for comparison (Mono).
In each model,a sequence of sampling u, z, pi, ?, ?0, and ?
isrepeated 10,000 times.
In sampling ?0 and ?, hy-perparameters ?a, ?b, ?a, and ?b are set to 2, 1,1, and 1, respectively, which is the same setting inGael et al (2008).
In sampling z, parameters ?, ??,.
.
., are set to 0.01.
In the experiments, three typesof factors for the aligned English words are com-pared: surface forms (?s?
), POS tags (?P?
), and thecombination of both (?s+P?).
Further, two types ofinference frameworks are compared: induction(IND) and refinement (REF ).
In both frame-works, each hidden state zt is first initialized tothe POS tags assigned by MeCab (the IPA POStagset), and then each state is updated throughthe inference procedure described in Section 3.5.Note that in REF , the sampling distribution overzt is constrained to include only states that are arefinement of the initially assigned POS tag.Step 3.
Training a POS Tagger and aDependency ParserIn this step, we train a Japanese dependency parserfrom the 10,000 Japanese dependency trees withthe induced POS tags which are derived from Step2.
We employed a transition-based dependencyparser which can jointly learn POS tagging anddependency parsing (Hatori et al, 2011) under anincremental framework11.
Note that the learnedparser can identify dependencies between wordsand attach an induced POS tag for each word.Step 4.
Training a Forest-to-String MTIn this step, we train a forest-to-string MT modelbased on the learned dependency parser in Step 3.We use an in-house developed hypergraph-basedtoolkit, cicada, for training and decoding with atree-to-string model, which has been successfullyemployed in our previous work for system com-bination (Watanabe and Sumita, 2011) and onlinelearning (Watanabe, 2012).
All the Japanese andEnglish sentences in the NTCIR-9 training dataare segmented in the same way as in Step 1, andthen each Japanese sentence is parsed by the de-pendency parser learned in Step 3, which simul-taneously assigns induced POS tags and word de-pendencies.
Finally, a forest-to-string MT modelis learned with Zhang et al, (2011), which ex-tracts translation rules by a forest-based variant of11http://triplet.cc/software/corbit/IND REFBS 27.54Mono 27.66 26.83Joint[s] 28.00 28.00Joint[P] 26.36 26.72Joint[s+P] 27.99 27.82Ind[s] 28.00 27.93Ind[P] 28.11 28.63Ind[s+P] 28.13 28.62Table 1: Performance on Japanese-to-EnglishTranslation Measured by BLEU (%)the GHKM algorithm (Mi and Huang, 2008) af-ter each parse tree is restructured into a binarizedpacked forest.
Parameters are tuned on the devel-opment data using xBLEU (Rosti et al, 2011) asan objective and L-BFGS (Liu and Nocedal, 1989)as an optimization toolkit, since it is stable and lessprone to randomness, unlike MERT (Och, 2003)or PRO (Hopkins and May, 2011).
The develop-ment test data is used to set up hyperparameters,i.e., to terminate tuning iterations.When translating Japanese sentences, a parsetree for each sentence is constructed in the sameway as described earlier in this step, and then theparse trees are translated into English sentencesusing the learned forest-to-string MT model.4.2 Experimental ResultsTable 1 shows the performance for the test datameasured by case sensitive BLEU (Papineni etal., 2002).
We also present the performance ofour baseline forest-to-string MT system (BS) us-ing the original IPA POS tags.
In Table 1, num-bers in bold indicate that the systems outperformthe baselines, BS and Mono.
Under the Mosesphrase-based SMT system (Koehn et al, 2007)with the default settings, we achieved a 26.80%BLEU score.Table 1 shows that the proposed systems outper-form the baseline Mono.
The differences betweenthe performance of Ind[s+P] and Mono are statis-tically significant in the bootstrap method (Koehn,2004), with a 1% significance level both in INDand REF .
The results indicate that integrating thealigned target-side information in POS inductionmakes inferred tagsets more suitable for SMT.Table 1 also shows that the independent modelis more effective for SMT than the joint model.This means that sparseness is a severe problem in847Model IND REFJoint[s+P] 164 620Ind[s+P] 102 517IPA POS tags 42Table 2: The Number of POS TagsPOS induction when jointly encoding bilingual in-formation into observations.
Additionally, all thesystems using the independent model outperformBS.
The improvements are statistically significantin the bootstrap method (Koehn, 2004), with a 1%significance level.
The results show that the pro-posed models can generate more favorable POStagsets for SMT than an existing POS tagset.In Table 1, REF s are at least comparable to, orbetter than, INDs except for Mono.
This showsthat REF achieves better performance by preserv-ing the clues from the original POS tagset.
How-ever, REF may suffer sever overfitting problemfor Mono since no bilingual information was in-corporated.
Further, when the full-level IPA POStags12 were used in BS, the system achieved a27.49% BLEU score, which is worse than the re-sult using the second-level IPA POS tags.
Thismeans that manual refinement without bilingualinformation may also cause an overfitting problemin MT.5 Discussion5.1 Comparison to the IPA POS TagsetTable 2 shows the number of the IPA POS tagsused in the experiments and the POS tags inducedby the proposed models.
This table shows thateach induced tagset contains more POS tags thanthe IPA POS tagset.
In the experimental data,some of Japanese verbs correspond to genuine En-glish verbs, some are nominalized, and others cor-respond to English past participle verbs or presentparticiple verbs which modify other words.
Re-spective examples are ?I use a card.
?, ?Using theindex is faster.
?, and ?I explain using an exam-ple.
?, where all the underlined words correspondto the same Japanese word, ???
?, whose IPAPOS tag is a verb.
Ind[s+P] in REF generatedthe POS tagset where the three types are assignedto separate POS groups.The Japanese particle ???
is sometimes at-tached to nouns to give them adverb roles.
For12377 types of full-level IPA POS tags were included in ourexperimental data.Tagging DependencyIND REF IND REFOriginal 90.37 93.62Mono 90.75 88.04 91.77 91.51Joint[s] 89.08 86.73 91.55 91.14Joint[P] 80.54 79.98 91.06 91.29Joint[s+P] 87.56 84.92 91.31 91.10Ind[s] 87.62 84.33 92.06 92.58Ind[P] 90.21 88.50 92.85 93.03Ind[s+P] 89.57 86.12 92.96 92.78Table 3: Tagging and Dependency Accuracy (%)example, ???
(mutual) ???
is translated asthe adverb ?mutually?
in English.
Other times,it is attached to words to make them the objectsof verbs.
For example, ??
(he) ??????(give)?
is translated as ?give him?.
The POS tagsby Ind[s+P] in REF discriminated the two types.These examples show that the proposed mod-els can disambiguate POS tags that have differentfunctions in English, whereas the IPA POS tagsettreats them jointly.
Thus, such discrimination im-proves the performance of a forest-to-string SMT.5.2 Impact of Tagging and DependencyAccuracyThe performance of our methods depends not onlyon the quality of the induced tag sets but also onthe performance of the dependency parser learnedin Step 3 of Section 4.1.
We cannot directly eval-uate the tagging accuracy of the parser trainedthrough Step 3 because we do not have any datawith induced POS tags other than the 10,000-sentence data gained through Step 2.
Thus we splitthe 10,000 data into the first 9,000 data for train-ing and the remaining 1,000 for testing, and thena dependency parser was learned in the same wayas in Step 3.Table 3 shows the results.
Original is the per-formance of the parser learned from the trainingdata with the original POS tagset.
Note that the de-pendency accuracies are measured on the automat-ically parsed dependency trees, not on the syntac-tically correct gold standard trees.
Thus Originalachieved the best dependency accuracy.In Table 3, the performance for our bilingually-induced POSs, Joint and Ind, are lower thanOriginal and Mono.
It seems performing pars-ing and tagging with the bilingually-induced POStagset is too difficult when only monolingual in-848formation is available to the parser.
However, ourbilingually-induced POSs, except for Joint[P ],with the lower accuracies are more effective forSMT than the monolingually-induced POSs andthe original POSs, as indicated in Table 1.
Thetagging accuracies for Joint[P ] both in IND andREF are significantly lower than the others, whilethe dependency accuracies do not differ signifi-cantly.
The lower tagging accuracies may directlyreflect the lower translation qualities for Joint[P ]in Table 1.6 ConclusionWe proposed a novel method for inducing POStags for SMT.
The proposed method is a non-parametric Bayesian method, which infers hiddenstates (i.e., POS tags) based on observations repre-senting not only source words themselves but alsoaligned target words.
Our experiments showedthat a more favorable POS tagset can be inducedby integrating aligned information, and further-more, the POS tagset generated by the proposedmethod is more effective for SMT than an existingPOS tagset (the IPA POS tagset).Even though we employed word alignmentfrom GIZA++ with potential errors, large gainswere achieved using our proposed method.
Wewould like to investigate the influence of align-ment errors in the future.
In addition, we are plan-ning to prove the effectiveness of our proposedmethod for language pairs other than Japanese-to-English.
We are also planning to introduce ourproposed method to other syntax-based SMT, suchas a string-to-tree SMT and a tree-to-tree SMT.AcknowledgmentsWe thank Isao Goto for helpful discussions andanonymous reviewers for valuable comments.
Wealso thank Jun Hatori for helping us to apply hissoftware, Corbit, to our induced POS tagsets.ReferencesMasayuki Asahara and Yuji Matsumoto.
2003.IPADIC User Manual.
Technical report, Japan.Matthew J. Beal, Zoubin Ghahramani, and Carl E. Ras-mussen.
2001.
The Infinite Hidden Markov Model.In Advances in Neural Information Processing Sys-tems, pages 577?584.Phil Blunsom and Trevor Cohn.
2011.
A HierarchicalPitman-Yor Process HMM for Unsupervised Part ofSpeech Induction.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics, pages 865?874.Trevor Cohn and Phil Blunsom.
2009.
A BayesianModel of Syntax-Directed Tree to String GrammarInduction.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 352?361.Yuan Ding and Martha Palmer.
2005.
Machine Trans-lation Using Probabilistic Synchronous DependencyInsertion Grammars.
In Proceedings of the 43rd An-nual Meeting of the Association for ComputationalLinguistics, pages 541?548.Thomas S. Ferguson.
1973.
A Bayesian Analysisof Some Nonparametric Problems.
The Annals ofStatistics, 1(2):209?230.Jenny Rose Finkel, Trond Grenager, and Christo-pher D. Manning.
2007.
The Infinite Tree.
In Pro-ceedings of the 45th Annual Meeting of the Associa-tion of Computational Linguistics, pages 272?279.Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, andZoubin Ghahramani.
2008.
Beam Sampling forthe Infinite Hidden Markov Model.
In Proceedingsof the 25th International Conference on MachineLearning, pages 1088?1095.Jurgen Van Gael, Andreas Vlachos, and ZoubinGhahramani.
2009.
The infinite HMM for unsuper-vised PoS tagging.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 2 - Volume 2, pages 678?687.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable Inference and Trainingof Context-Rich Syntactic Translation Models.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics, pages 961?968.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, andBenjamin K. Tsou.
2011.
Overview of the PatentMachine Translation Task at the NTCIR-9 Work-shop.
In Proceedings of the 9th NTCIR Workshop,pages 559?578.Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, andJun?ichi Tsujii.
2011.
Incremental Joint POS Tag-ging and Dependency Parsing in Chinese.
In Pro-ceedings of 5th International Joint Conference onNatural Language Processing, pages 1216?1224.Mark Hopkins and Jonathan May.
2011.
Tuning asRanking.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1352?1362.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.A Syntax-Directed Translator with Extended Do-main of Locality.
In Proceedings of the Workshop on849Computationally Hard Problemsand Joint Inferencein Speech and Language Processing, pages 1?8.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the 2003 Human Language TechnologyConference: North American Chapter of the Associ-ation for Computational Linguistics, pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constrantin, and Evan Herbst.
2007.
Moses:Open Source Toolkit for Statistical Machine Trans-lation.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics on In-teractive Poster and Demonstration Sessions, pages177?180.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods in Nat-ural Language Processing, pages 388?395.Taku Kudo and Yuji Matsumoto.
2002.
Japanese De-pendency Analysis using Cascaded Chunking.
InProceedings of the 6th Conference on Natural Lan-guage Learning, pages 63?69.Percy Liang, Slav Petrov, Michael I. Jordan, and DanKlein.
2007.
The Infinite PCFG using Hierarchi-cal Dirichlet Processes.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pages 688?697.Dekang Lin.
2004.
A Path-based Transfer Model forMachine Translation.
In Proceedings of the 20th In-ternational Conference on Computational Linguis-tics, pages 625?630.Dong C. Liu and Jorge Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming B, 45(3):503?528.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-String Alignment Template for Statistical MachineTranslation.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Compu-tational Linguistics, pages 609?616.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improv-ing Tree-to-Tree Translation with Packed Forests.In Proceedings of the 47th Annual Meeting of theAssociation for Computational Linguistics and the4th International Joint Conference on Natural Lan-guage Processing of the Asian Federation of NaturalLanguage Processing, pages 558?566.Haitao Mi and Liang Huang.
2008.
Forest-basedTranslation Rule Extraction.
In Proceedings of the2008 Conference on Empirical Methods in NaturalLanguage Processing, pages 206?214.Haitao Mi and Qun Liu.
2010.
Constituency to De-pendency Translation with Forests.
In Proceedingsof the 48th Annual Conference of the Association forComputational Linguistics, pages 1433?1442.Toshiaki Nakazawa and Sadao Kurohashi.
2012.Alignment by Bilingual Generation and Monolin-gual Derivation.
In Proceedings of the 24th Inter-national Conference on Computational Linguistics,pages 1963?1978.Radford M. Neal.
2003.
Slice Sampling.
Annals ofStatistics, 31:705?767.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29:19?51.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof the 41st Annual Meeting of the Association forComputational Linguistics, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency Treelet Translation: Syntactically In-formed Phrasal SMT.
In Proceedings of the 43rdAnnual Conference of the Association for Computa-tional Linguistics, pages 271?279.Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2011.
Expected BLEUTraining for Graphs: BBN System Description forWMT11 System Combination Task.
In Proceedingsof the Sixth Workshop on Statistical Machine Trans-lation, pages 159?165.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedings ofthe International Conference on New Methods inLanguage Processing, pages 44?49.Jayaram Sethuraman.
1994.
A Constructive Definitionof Dirichlet Priors.
Statistica Sinica, 4(2):639?650.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.A New String-to-Dependency Machine TranslationAlgorithm with a Target Dependency LanguageModel.
In Proceedings of the 46th Annual Confer-ence of the Association for Computational Linguis-tics: Human Language Technologies, pages 577?585.Kairit Sirts and Tanel Aluma?e.
2012.
A Hierarchi-cal Dirichlet Process Model for Joint Part-of-Speechand Morphology Induction.
In Proceedings of the2012 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 407?416.850Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,and David M. Blei.
2006.
Hierarchical DirichletProcesses.
Journal of the American Statistical Asso-ciation, 101(476):1566?1581.Taro Watanabe and Eiichiro Sumita.
2011.
MachineTranslation System Combination by Confusion For-est.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 1249?1257.Taro Watanabe.
2012.
Optimized Online Rank Learn-ing for Machine Translation.
In Proceedings of the2012 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 253?262.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008.
A Tree Se-quence Alignment-based Tree-to-Tree TranslationModel.
In Proceedings of the 46th Annual Confer-ence of the Association for Computational Linguis-tics: Human Language Technologies, pages 559?567.Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.2011.
Binarized Forest to String Translation.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 19?24.851
