Proceedings of the 7th Workshop on Statistical Machine Translation, pages 422?432,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsAnalysing the Effect of Out-of-Domain Data on SMT SystemsBarry Haddow and Philipp KoehnSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, Scotland{bhaddow,pkoehn}@inf.ed.ac.ukAbstractIn statistical machine translation (SMT), it isknown that performance declines when thetraining data is in a different domain from thetest data.
Nevertheless, it is frequently nec-essary to supplement scarce in-domain train-ing data with out-of-domain data.
In this pa-per, we first try to relate the effect of the out-of-domain data on translation performance tomeasures of corpus similarity, then we sep-arately analyse the effect of adding the out-of-domain data at different parts of the train-ing pipeline (alignment, phrase extraction, andphrase scoring).
Through experiments in 2 do-mains and 8 language pairs it is shown thatthe out-of-domain data improves coverage andtranslation of rare words, but may degrade thetranslation quality for more common words.1 IntroductionIn statistical machine translation (SMT), domainadaptation can be thought of as the problem of train-ing a system on data mainly drawn from one do-main (e.g.
parliamentary proceedings) and tryingto maximise its performance on a different domain(e.g.
news).
There is likely to be some parallel datasimilar to the test data, but as such data is expen-sive to create, it tends to be scarce.
The concept of?domain?
is rarely given a precise definition, but it isnormally understood that data from the same domainis in some sense similar (for example in the wordsand grammatical constructions used) and data fromdifferent domains shows less similarities.
Data fromthe same domain as the test set is usually referredto as in-domain and data from a different domain isreferred to as out-of-domain.The aim of this paper is to shed some light onwhat domain actually is, and why it matters.
Thefact that a mismatch between training and test datadomains reduces translation performance has beenobserved in previous studies, and will be confirmedhere for multiple data sets and languages, but thequestion of why domain matters for performance hasnot been fully addressed in the literature.
Exper-iments in this paper will be conducted on phrase-based machine translation (PBMT) systems, butsimilar conclusions are likely to apply to other typesof SMT systems.
Furthermore, we will mainly beconcerned with the effect of domain on the transla-tion model, since it depends on parallel data whichis more likely to be in short supply than monolingualdata, and domain adaptation for language modellinghas been more thoroughly studied.The effect of a shift of domain in the parallel datais complicated by the fact that training a translationmodel is a multi-stage process.
First the paralleldata is word-aligned, normally using the IBM mod-els (Brown et al, 1994), then phrases are extractedusing some heuristics (Och et al, 1999) and scoredusing a maximum likelihood estimate.
Since the ef-fect of domain may be felt at the alignment stage, theextraction stage, or the scoring stage, we have de-signed experiments to try to tease these apart.
Exper-iments comparing the effect of domain at the align-ment stage with the extraction and scoring stageshave already been presented by (Duh et al, 2010), sowe focus more on the differences between extractionand scoring.
In other words, we examine whetheradding more data (in or out-of domain) helps im-prove coverage of the phrase table, or helps improvethe scoring of phrases.A further question that we wish to address is422whether adding out-of-domain parallel data affectswords with different frequencies to different de-grees.
For example, a large out-of-domain data setmay improve the translation of rare words by pro-viding better coverage, but degrade translation ofmore common words by providing erroneous out-of-domain translations.
In fact, the evidence presentedin Section 3.5 will show a much clearer effect on lowfrequency words than on medium or high frequencywords, but the total token count of these low fre-quency words is still small, so they don?t necessarilyhave much effect on overall measures of translationquality.In summary, the main contributions of this paperare:?
It presents experiments on 8 language pairsand 2 domains showing the effect on BLEU ofadding out-of-domain data.?
It provides evidence that the difference be-tween in and out-of domain translation perfor-mance is correlated with differences in worddistribution and out-of-vocabulary rates.?
It develops a method for separating the effectsof phrase extraction and scoring, showing thatgood coverage is nearly always more importantthan good scoring, and that out-of-domain datacan adversely affect phrase scores.?
It shows that adding out-of-domain data clearlyimproves translation of rare words, but mayhave a small negative effect on more commonwords.2 Related WorkThe most closely related work to the current one isthat of (Duh et al, 2010).
In this paper they considerthe domain adaptation problem for PBMT, and in-vestigate whether the out-of-domain data helps moreat the word alignment stage, or at the phrase extrac-tion and scoring stages.
Extensive experiments on4 different data sets, and 10 different language pairsshow mixed results, with the overall conclusion be-ing that it is difficult to predict how best to includeout-of-domain data in the PBMT training pipeline.Unlike in the current work, Duh et al do not sepa-rate phrase extraction and scoring in order to anal-yse the effect of domain on them separately.
Theymake the point that adding extra out-of-domain datamay degrade translation by introducing unwantedlexical ambiguity, showing anecdotal evidence forthis.
Similar arguments were presented in (Sennrich,2012).A recent paper which does attempt to tease apartphrase extraction and scoring is (Bisazza et al,2011).
In this work, the authors try to improve asystem trained on in-domain data by including extraentries (termed ?fill-up?)
from out-of-domain data ?this is similar to the nc+epE and st+epE systemsin Section 3.4.
It is shown by Bisazza et al that thisfill-up technique has a similar effect to using MERTto weight the in and out-of domain phrase tables.
Inthe experiments in Section 3.4 we confirm that fill-up techniques mostly provide better results than us-ing a concatenation of in and out-of domain data.There has been quite a lot of work on finding waysof weighting in and out-of domain data for SMT(as opposed to simply concatenating the data sets),both for language and translation modelling.
Inter-polating language models using perplexity is fairlywell-established (e.g.
Koehn and Schroeder (2007)),but for phrase-tables it is unclear whether perplexityminimisation (Foster et al, 2010; Sennrich, 2012) orlinear or log-linear interpolation (Foster and Kuhn,2007; Civera and Juan, 2007; Koehn and Schroeder,2007) is the best approach.
Also, other authors (Fos-ter et al, 2010; Niehues and Waibel, 2010; Shah etal., 2010) have tried to weight the input sentences orextracted phrases before the phrase tables are built.In this type of approach, the main problem is howto train the weights of the sentences or phrases, andeach of the papers has followed a different approach.Instead of weighting the out-of-domain data,some authors have investigated data selection meth-ods for domain adaptation (Yasuda et al, 2008;Mansour et al, 2011; Schwenk et al, 2011; Axelrodet al, 2011).
This is effectively the same as usinga 1-0 weighting for input sentences, but has the ad-vantage that it is usually easier to tune a thresholdthan it is to train weights for all input sentences orphrases.
The other advantage of doing data selectionis that it can potentially remove noisy (e.g.
incor-rectly aligned) data.
However it will be seen later inthis paper that out-of-domain data can usually con-tribute something useful to the translation system,so the 1-0 weighting of data-selection may be some-what heavy-handed.4233 Experiments3.1 Corpora and BaselinesThe experiments in this paper used data fromthe WMT09 and WMT11 shared tasks (Callison-Burch et al, 2009; Callison-Burch et al, 2011), aswell as OpenSubtitles data1 released by the OPUSproject (Tiedemann, 2009).From the WMT data, both news-commentary-v6 (nc) and europarl-v6 (ep) were used fortraining translation models and language mod-els, with nc-devtest2007 used for tuning andnc-test2007 for testing.
The experiments wererun on all language pairs used in the WMT sharedtasks, i.e.
English (en) into and out of Spanish (es),German (de), French (fr) and Czech (cs).From the OpenSubtitles (st) data, we chose8 language pairs ?
English to and from Span-ish, French, Czech and Dutch (nl) ?
selected be-cause they have at least 200k sentences of paralleldata available.
2000 sentence tuning and test sets(st-dev and st-devtest) were selected fromthe parallel data by extracting every nth sentence,and a 200k sentence training corpus was selectedfrom the remaining data.Using test sets from both news-commentary andOpenSubtitles gives two domain adaptation tasks,where in both cases the out-of-domain data is eu-roparl, a significantly larger training set than the in-domain data.
The three data sets in use in this paperare summarised in Table 1.The translation systems consisted of phrase ta-bles and lexicalised reordering tables estimated us-ing the standard Moses (Koehn et al, 2007) train-ing pipeline, and 5-gram Kneser-Ney smoothed lan-guage models estimated using the SRILM toolkit(Stolcke, 2002), with KenLM (Heafield, 2011) usedat runtime.
Separate language models were builton the target side of the in-domain and out-of-domain training data, then linearly interpolated us-ing SRILM to minimise perplexity on the tuningset (e.g.
Koehn and Schroeder (2007)).
Tuningof models used minimum error rate training (Och,2003), repeated 3 times and averaged (Clark etal., 2011).
Performance is evaluated using case-insensitive BLEU (Papineni et al, 2002), as imple-1www.opensubtitles.orgmented using the Moses multi-bleu.pl script.Name Languagepairstrain tune testEuroparl en?fr 1.8M n/a n/a(ep) en?es 1.8M n/a n/aen?de 1.7M n/a n/aen?cs 460k n/a n/aen?nl 1.8M n/a n/aNews en?fr 114k 1000 2000Commentary en?es 130k 1000 2000(nc) en?de 135k 1000 2000en?cs 122k 1000 2000Subtitles en?fr 200k 2000 2000(st) en?es 200k 2000 2000en?nl 200k 2000 2000en?cs 200k 2000 2000Table 1: Summary of the data sets used, with ap-proximate sentence counts3.2 Comparing In-domain and Out-of-domainDataThe aim of this section is to provide both a quali-tative and quantitative comparison of the three datasets used in this paper.Firstly, consider the extracts from the English sec-tions of the three training sets shown in Figure 1.The first extract, from the Europarl corpus, shows aformal style with long sentences.
However this isstill spoken text so contains a preponderance of firstand second person forms.
In terms of subject mat-ter, the corpus covers a broad range of topics, but allfrom the angle of European legislation, institutionsand policies.
Where languages (e.g.
English, Frenchand Spanish) have new world and old world variants,Europarl sticks to the old world variants.The extract from the News Commentary corpusagain shows a formal tone, but because this is newsanalysis, it tends to favour the third person.
It is writ-ten text, and covers a wider range of subjects thanEuroparl, and also encompasses both new and oldworld versions of the European languages.The Subtitles text shown in the last example ap-pears qualitatively more different from the othertwo.
It is spoken text, like Europarl, but consists ofshort, informal sentences with many colloquialisms,as well as possible optical character recognition er-424Although, as you will have seen, the dreaded ?millennium bug?
failed to materialise, still the people in anumber of countries suffered a series of natural disasters that truly were dreadful.You have requested a debate on this subject in the course of the next few days, during this part-session.In the meantime, I should like to observe a minute?
s silence, as a number of Members have requested, onbehalf of all the victims concerned, particularly those of the terrible storms, in the various countries of theEuropean Union.
(a) EuroparlDesperate to hold onto power, Pervez Musharraf has discarded Pakistan?s constitutional framework anddeclared a state of emergency.His goal?To stifle the independent judiciary and free media.Artfully, though shamelessly, he has tried to sell this action as an effort to bring about stability and helpfight the war on terror more effectively.
(b) News commentaryI?il call in 30 minutes to checkIs your mother here, too?Why are you outside?It?s no fun listening to women?s talkWell, why don?t we go in together(c) OpenSubtitlesFigure 1: Extracts from the English portion of the training corporarors.
It is likely to contain a mixture of regional vari-ations of the languages, reflecting the diversity of thefilm sources.In order to obtain a quantitative measure of do-main differences, we used both language model(LM) perplexity, and out-of-vocabulary (OOV) rate,in the two test domains.
For the nc domain, per-plexity was compared by training trigram LMs (withSRILM and Kneser-Ney smoothing) on each of theep, nc and ep+nc training sets, taking the inter-section of the ep and nc vocabularies as the LMvocabulary.
The perplexities of the nc test set wercalculated using each of the LMs.
A correspond-ing set of LMs was trained to compare perplexitieson the st test set, and all perplexity comparisonswere performed on all five languages.
The SRILMtoolkit was also used to calculate OOV rates on thetest set, by training language models with an openvocabulary, and using no unknown word probabilityestimation.The perplexities and OOV rates on each test cor-pora are shown in Figure 2.
The pattern of perplexi-ties is quite distinct across the two test domains, withthe perplexity from out-of-domain data relativelymuch higher for the st test set.
The in-domain dataLM also shows the lowest perplexity consistently onthis test set, whilst for nc, the in-domain LM hasa similar perplexity to the ep+nc LM.
In fact for3/5 languages (fr,cs and de) the ep+nc LM has thelowest perplexity.With regard to the OOV rates, it is notable thatfor nc the rate is actually higher for the in-domainLM than the out-of-domain LM in three of the lan-guages: French, German and Spanish.
The mostlikely reason for this is that these languages havea relatively rich morphology, so the larger out-of-domain corpus (Table 1) gives greater cover-age of the different grammatical suffixes.
Czechshows a different pattern because in this case theout-of-domain corpus is not much bigger than thein-domain corpus, and English is morphologicallymuch simpler so the increase in corpus size does nothelp the OOV rate so much.425cs de en es fr3?gram Perplexity0100200300400500600700cs de en es frepncep+ncOOV050010001500(a) Test on news commentary.cs en es fr nl3?gram Perplexity0100200300400cs en es fr nlepstep+stOOV050010001500(b) Test on subtitles.Figure 2: Comparison of perplexities and OOV rates on in-domain test data3.3 Comparing Translation Performance of Inand Out-of-domain SystemsTranslation performance was measured on each ofthe test sets (nc and st) using systems built fromjust the in-domain parallel data, from just the out-of-domain parallel data, and on a concatenation of thein and out-of domain data.
In other words, systemsbuilt from the ep, nc and ep+nc parallel texts wereevaluated on the nc test data, and systems built fromep, st and ep+st were evaluated on the st testdata.
In all cases, the parallel training set was usedto build both the phrase table and the lexicalised re-ordering models, the language model was the inter-polated one described in Section 3.1, and the systemwas tuned on data from the same domain as the testset.From Figure 3 it is clear that the difference be-tween the in and out-of domain training sets is muchbigger for st than for nc.
The BLEU scores on ncfor the nc trained systems are on average 1.3 BLEUpoints higher than those for the ep trained systems,whilst the scores on st gain an average of 6.0 BLEUpoints when the training data is switched from epto st.
The patterns are quite consistent across lan-guages for the st tested systems, with the gainsvarying just from 5.2 to 7.2.
However for the nctested systems there are some language pairs whichshow a gain of more than 2 BLEU points when mov-ing from out-of to in-domain training data (cs-en,en-es and es-en), whereas en-fr shows no change.The main link between the perplexity and OOV re-sults in Figure 2 and the BLEU score variations inFigure 3 is that the larger in/out differences betweenthe two domains is reflected in larger BLEU differ-ences.
However it is also notable that the two lan-guages which display a rise in perplexity betweennc and ep+nc are es and en, and for both es-en anden-es the ep+nc translation system performs worsethan the nc trained system.The BLEU gain from concatenating the in and out-of domain data, over just using the in-domain datacan be quite small.
For the nc domain this averagesat 0.5 BLEU (with 3/8 language pairs showing a de-crease), whilst for the st domain the average gain isonly 0.2 BLEU (with again 3/8 language pairs show-ing a decrease).
So even though adding the out-of-domain data increases the training set size by a fac-tor of 10 in most cases, its effect on BLEU score issmall.426cs?en en?cs de?en en?de fr?en en?fr es?en en?esepncep+ncBleu05101520253035(a) Test on news commentarycs?en en?cs nl?en en?nl fr?en en?fr es?en en?esepstep+stBleu0510152025(b) Test on subtitlesFigure 3: Comparison of translation performance using models from in-domain, out-of-domain and jointdata.3.4 Why Does Adding Parallel Data Help?In the previous section it was found that, across alllanguage pairs and both data sets, adding in-domaindata to an out-of-domain training set nearly alwayshas a positive impact on performance, whilst addingout-of-domain data to an in-domain training set cansometimes have a small positive effect.
In this sec-tion several experiments are performed with ?inter-mediate?
phrase tables (built from a single parallelcorpus, augmented with some elements of the otherparallel corpus) in order to determine how differentaspects of the extra data affect performance.
In par-ticular, the experiments are designed to show the ef-fect of the extra data on the alignments, the phrasescoring and the phrase coverage, whether adding in-domain data to an existing out-of-domain trainedsystem, or vice-versa.For each of the language pairs used in this paper,and each of the two domains, two series of experi-ments were run comparing systems built from a sin-gle parallel training set, intermediate systems, andsystems built from a concatenation of the in and out-of-domain parallel data sets.
Only the parallel datawas varied, the language models were as describedin Section 3.1, and the lexicalised reordering mod-els were built from both training sets in all cases,except for the systems built from a single paralleldata set2.
This gives a total of four series of experi-ments, where the ordered pair of data sets (x,y) wasset to one of (ep,nc), (nc,ep), (ep,st), (st,ep).In each of these series, the following translation sys-tems were trained:x The translation table and lexicalised reorderingmodel were estimated from the x corpus alone.x+y The translation system built from the x and yparallel corpora concatenated.x+yA As x but using the additional y corpus to cre-ate the alignments.
This means that GIZA++was run across the entire x+y corpus, but onlythe x section of it was used to extract and scorephrases.x+yW As x+yA but using the phrase scores fromthe x+y phrase table.
This is effectively thex+y system, with any entries in the phrase tablethat are just found in the y corpus removed.2Further experiments were run using the parallel data from asingle data set to build the translation model, and both data setsto build the lexicalised reordering model, but the difference inscore compared to the x system was small (< 0.1 BLEU)427x+yE As x+yA but adding the extra entries fromthe x+y phrase table.
This is effectively thex+y system, but with the scores on all phrasesthat are found in x phrase table set to their val-ues from that table.All systems were tuned and tested on the appro-priate in-domain data set (either nc or st).
Notethat in the intermediate systems, the phrase tablescores may no longer correspond to valid probabilitydistributions, but this is not important as the proba-bilistic interpretation is never used in decoding any-way.The graphs in Figure 4 show the performancecomparison between the single corpus systems, theintermediate systems, and the concatenated corpussystems, averaged across all 8 language pairs.
Table2 shows the full results broken down by languagepair, for completeness, but the patterns are reason-ably consistent across language pair.Firstly, compare the x+yW and x+yE systems, i.e.the systems where we add just the weights from thesecond parallel data set versus those where we addjust the entries.
When x is the out-of-domain (ep)data, then it is clearly more profitable to update thephrase-table entries than the weights from the in-domain data.
In fact for the systems tested on st,the difference is quite striking with a +5.7 BLEUgain for the ep+stE system over the baseline epsystem, but only a +1.5 gain for the ep+stW sys-tem.
For the systems tested on the nc, adding theentries from nc gives a larger gain in BLEU thanadding the weights (+1.3 versus +0.8), but bothimprove the BLEU scores over the ep+ncA system.The conclusion is that the extra entries from the in-domain data (the ?fill-up?
of Bisazza et al (2011))are more important than the improvements in phrasescoring that in-domain data may provide.Looking at the other two sets of x+yW and x+yEsystems, i.e.
those where x is the in-domain data,tells another story.
In this case, the results on boththe nc and st test sets (Figure 4(b)) suggest that it isgenerally more useful to use the out-of-domain dataas only a source of extra phrase-table entries.
This isbecause the x+epE systems are the highest scoringin both cases, scoring higher than systems built fromall the data concatenated by margins of 0.5 (for nc)and 0.4 (for st).
This pattern is consistent acrossall the language pairs for nc, and across 5 of the 8language pairs for st.
Using the out-of-domain dataset to update only the weights (the x+epW systems)generally degrades performance when compared tothe systems that only use the ep data at alignmenttime (the x+epA systems).The size of the effect of adding extra data to thealignment stage only is mixed (as observed by (Duhet al, 2010)), but in general all the x+yA systemsshow an improvement over the x systems.
In fact,for the st domain, adding ep at the alignment stageis the only consistent way to improve BLEU.
Addingthe weights, entries, or the complete out-of-domaindata set does not always help.3.5 Word Precision Versus FrequencyThe final set of experiments addresses the questionof whether the change of translation quality whenadding out-of-domain has a different effect depend-ing on word frequency.
To do this, the systemstrained on in-domain only are compared with thesystems trained on all data concatenated, using atechnique for measuring the precision of the trans-lation for each word type.To calculate the precision of a word type, it isnecessary to examine each translated sentence tosee which source words were translated correctly.This is done by recording the word alignment in thephrase mappings and tracking it through the transla-tion process.
If a word is produced multiple times inthe translation, but occurs a fewer number of timesin the reference, then it is assigned partial credit.Many-to-many word alignments are treated simi-larly.
Precision for each word type is then calculatedin the usual way, as the number of times that wordappears correctly in the output, divided by the to-tal number of appearances.
The word types are thenbinned according to the log2 of their frequency inthe in-domain corpus and the average precision foreach bin calculated, then these are in turn averagedacross language pairs.The graphs in Figure 5 compare the in-domainsource frequency versus precision relationship forsystems built using just the in-domain data, and sys-tems built using both in and out-of domain data.There is a consistent increase in precision for lowerfrequency words (occurring less than 30 times intraining), but the total number of occurrences ofthese words is low, so they contribute less to over-428epep+ncAep+ncWep+ncE ep+ncBleu0510152025+0.4 +0.8 +1.3 +1.7(a) Start with ep, test on nc.ncnc+epAnc+epWnc+epE ep+ncBleu051015202530+0.5 +0.3 +1.0 +0.5(b) Start with nc, test on nc.epep+stAep+stWep+stE ep+stBleu05101520+0.6 +1.5+5.7 +6.2(c) Start with ep, test on st.stst+epAst+epWst+epEep+stBleu05101520+0.3?0.2 +0.6 +0.2(d) Start with st, test on stFigure 4: Showing the performance change when starting with either in or out-of domain data, and addingelements of the other data set.
The ?A?
indicates that the second data set is only used for alignments, the ?W?indicates that it contributes alignments and phrase scores, and the ?E?
indicates that it contributes alignmentsand phrase entries.
The figures above each bar shows the performance change relative to the single corpussystem.System cs-en en-cs de-en en-de fr-en en-fr es-en en-esep 23.3 13.4 25.5 17.5 28.9 29.2 35.4 34.5ep+ncA 23.5 (+0.2) 13.8 (+0.4) 25.9 (+0.4) 17.9 (+0.4) 29.3 (+0.4) 29.6 (+0.4) 35.7 (+0.3) 34.9 (+0.5)ep+ncW 24.0 (+0.7) 14.2 (+0.8) 26.3 (+0.8) 18.2 (+0.7) 29.4 (+0.5) 29.8 (+0.6) 36.3 (+0.9) 35.6 (+1.1)ep+ncE 26.2 (+2.9) 14.0 (+0.6) 27.0 (+1.5) 18.5 (+1.0) 29.7 (+0.9) 30.0 (+0.8) 37.0 (+1.7) 35.7 (+1.3)nc 26.1 (+2.9) 14.3 (+0.9) 26.7 (+1.2) 18.0 (+0.6) 29.3 (+0.4) 29.1 (-0.1) 37.6 (+2.2) 36.5 (+2.1)nc+epA 26.8 (+3.5) 14.6 (+1.2) 27.5 (+2.0) 18.5 (+1.0) 30.4 (+1.5) 29.9 (+0.7) 37.7 (+2.3) 36.4 (+2.0)nc+epW 26.6 (+3.3) 14.4 (+1.0) 27.4 (+1.9) 18.4 (+1.0) 29.5 (+0.6) 29.8 (+0.6) 37.2 (+1.8) 36.5 (+2.0)nc+epE 27.4 (+4.1) 14.7 (+1.3) 28.1 (+2.6) 19.0 (+1.5) 30.9 (+2.0) 30.2 (+1.0) 38.4 (+3.0) 36.9 (+2.4)ep+nc 26.9 (+3.6) 14.2 (+0.8) 27.4 (+1.9) 18.8 (+1.3) 30.4 (+1.5) 30.0 (+0.8) 37.4 (+2.0) 36.4 (+2.0)System cs-en en-cs nl-en en-nl fr-en en-fr es-en en-esep 10.9 6.9 18.2 15.7 14.5 13.8 19.1 17.1ep+stA 11.9 (+1.0) 7.5 (+0.6) 19.0 (+0.8) 16.3 (+0.5) 15.0 (+0.5) 14.1 (+0.3) 19.8 (+0.7) 17.8 (+0.7)ep+stW 12.2 (+1.3) 8.1 (+1.2) 20.0 (+1.7) 17.4 (+1.7) 15.8 (+1.3) 14.9 (+1.1) 20.8 (+1.7) 18.8 (+1.8)ep+stE 18.0 (+7.1) 12.4 (+5.5) 22.5 (+4.2) 20.6 (+4.9) 19.6 (+5.1) 19.9 (+6.1) 25.6 (+6.5) 23.3 (+6.3)st 18.0 (+7.2) 12.2 (+5.3) 23.4 (+5.1) 21.3 (+5.6) 19.7 (+5.2) 19.8 (+6.0) 26.3 (+7.2) 23.2 (+6.1)st+epA 18.4 (+7.6) 12.4 (+5.5) 23.6 (+5.4) 21.3 (+5.6) 20.2 (+5.7) 20.1 (+6.3) 26.4 (+7.3) 23.5 (+6.5)st+epW 18.2 (+7.3) 12.2 (+5.3) 22.4 (+4.2) 21.0 (+5.3) 19.9 (+5.4) 19.8 (+6.0) 25.8 (+6.7) 23.2 (+6.1)st+epE 19.1 (+8.3) 12.5 (+5.6) 24.0 (+5.8) 21.7 (+6.0) 20.6 (+6.1) 20.9 (+7.1) 26.0 (+6.9) 23.7 (+6.6)ep+st 18.5 (+7.6) 12.5 (+5.6) 23.0 (+4.8) 21.2 (+5.5) 20.4 (+5.9) 20.2 (+6.5) 26.0 (+6.9) 23.8 (+6.8)Table 2: Complete scores for the experiments described in Section 3.4 and summarised in Figure 4.
Namingof the systems is explained in the text, and in the caption for Figure 4429unk  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17log2(count)0.00.10.20.30.40.50.60.70.8Precisionncep+nc(a) News commentaryunk  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17log2(count)0.00.10.20.30.40.50.60.70.80.9Precisionstep+st(b) SubtitlesFigure 5: Performance comparison of in-domain systems versus systems built from in and out-of domaindata concatenated.
Precision is plotted against log2 of in-domain training frequency, and averaged across all8 language pairs.
The width of the bars indicates the average total number of occurrences in the test set.all measures of translation quality.
For the wordswith moderate training set frequencies, the precisionis actually slightly higher for the systems built withjust in-domain data, an effect that is more markedfor the st domain.4 ConclusionsIn this paper we have attempted to give an in-depth analysis of the domain adaptation problem fortwo different domain adaptation problems in phrase-based MT.
The differences between the two prob-lems are clearly illustrated by the results in Fig-ures 2 and 3, where we see that the difference be-tween the in-domain and out-of-domain data arelarger for the OpenSubtitles domain than for theNews-Commentary domain.
This can be detectedby the differences in word distribution and out-of-vocabulary rates observed in Figure 2, and is re-flected by the differing translation results in Figure3.However, the experiments of Sections 3.4 and 3.5show some common themes emerging in the two do-mains.
In both cases, the out-of-domain data helpsmost when it is just allowed to add entries (i.e.
?fillin?)
the phrase-table, and using the scores providedby out-of-domain data has a tendency to be harmfulto translation quality.
The precision results of Sec-tion 3.5 show out-of-domain data (when it is sim-ply added to the training set) mainly helping withthe low frequency words, and having a neutral orharmful effect for higher frequency words.
This ex-plains why approaches which try to weight the out-of-domain data in some way (e.g.
corpus weightingor instance weighting) can be more successful than430simply concatenating data sets.
It also suggests thatthe way forward is to look for methods that use theout-of-domain data mainly for rarer words, and notto change translations which have a lot of evidencein the in-domain data.5 AcknowledgmentsThis work was supported by the EuroMatrixPlus3and Accept4 projects funded by the European Com-mission (7th Framework Programme).ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011.Domain Adaptation via Pseudo In-Domain Data Se-lection.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,pages 355?362, Edinburgh, Scotland, UK., July.
As-sociation for Computational Linguistics.Arianna Bisazza, Nick Ruiz, and Marcello Federico.2011.
Fill-up versus Interpolation Methods for Phrase-based SMT Adaptation.
In Proceedings of IWSLT.Peter F. Brown, Stephen Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1994.
The Mathe-matics of Statistical Machine Translation: ParameterEstimation.
Computational Linguistics, 19(2):263?311.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28, Athens, Greece,March.
Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011 Work-shop on Statistical Machine Translation.
In Proceed-ings of the Sixth Workshop on Statistical MachineTranslation, pages 22?64, Edinburgh, Scotland, July.Association for Computational Linguistics.Jorge Civera and Alfons Juan.
2007.
Domain Adaptationin Statistical Machine Translation with Mixture Mod-elling.
In Proceedings of the Second Workshop on Sta-tistical Machine Translation, pages 177?180, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.2011.
Better hypothesis testing for statistical machinetranslation: Controlling for optimizer instability.
InProceedings of ACL.3www.euromatrixplus.net4www.accept.unige.chKevin Duh, Katsuhito Sudoh, and Hajime Tsukada.2010.
Analysis of translation model adaptation in sta-tistical machine translation.
In Proceedings of IWSLT.George Foster and Roland Kuhn.
2007.
Mixture-modeladaptation for SMT.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages128?135, Prague, Czech Republic, June.
Associationfor Computational Linguistics.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative Instance Weighting for Domain Adap-tation in Statistical Machine Translation.
In Proceed-ings of the 2010 Conference on Empirical Methods inNatural Language Processing, pages 451?459, Cam-bridge, MA, October.
Association for ComputationalLinguistics.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages187?197, Edinburgh, Scotland, July.
Association forComputational Linguistics.Philipp Koehn and Josh Schroeder.
2007.
Experimentsin Domain Adaptation for Statistical Machine Transla-tion.
In Proceedings of the Second Workshop on Sta-tistical Machine Translation, pages 224?227, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
In Pro-ceedings of the ACL Demo Sessions, pages 177?180,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Saab Mansour, Joern Wuebker, and Hermann Ney.
2011.Combining Translation and Language Model Scoringfor Domain-Specific Data Filtering.
In Proceedings ofIWSLT.Jan Niehues and Alex Waibel.
2010.
Domain Adapta-tion in Statistical Machine Translation using FactoredTranslation Models.
In Proceedings of EAMT.Franz J. Och, Christoph Tillman, and Hermann Ney.1999.
Improved Alignment Models for Statistical Ma-chine Translation.
In Joint SIGDAT Conference onEmpirical Methods in Natural Language Processingand Very Large Corpora.Franz J. Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedings ofACL.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for Automatic Eval-uation of Machine Translation.
In Proceedings of 40th431Annual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Holger Schwenk, Patrik Lambert, Lo?
?c Barrault,Christophe Servan, Sadaf Abdul-Rauf, Haithem Afli,and Kashif Shah.
2011.
LIUM?s SMT Machine Trans-lation Systems for WMT 2011.
In Proceedings ofthe Sixth Workshop on Statistical Machine Translation,pages 464?469, Edinburgh, Scotland, July.
Associa-tion for Computational Linguistics.Rico Sennrich.
2012.
Perplexity Minimization for Trans-lation Model Domain Adaptation in Statistical Ma-chine Translation.
In Proceedings of EACL.Kashif Shah, Lo?
?c Barrault, and Holger Schwenk.
2010.Translation Model Adaptation by Resampling.
In Pro-ceedings of the Joint Fifth Workshop on Statistical Ma-chine Translation and MetricsMATR, pages 392?399,Uppsala, Sweden, July.
Association for ComputationalLinguistics.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proc.
Intl.
Conf.
on Spo-ken Language Processing, vol.
2, pages 901?904.Jo?rg Tiedemann.
2009.
News from OPUS - A Collectionof Multilingual Parallel Corpora with Tools and Inter-faces.
In N. Nicolov, K. Bontcheva, G. Angelova, andR.
Mitkov, editors, Recent Advances in Natural Lan-guage Processing (vol V), pages 237?248.
John Ben-jamins, Amsterdam/Philadelphia.Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, andEiichiro Sumita.
2008.
Method of Selecting Train-ing Data to Build a Compact and Efficient TranslationModel.
In Proceedings of IJCNLP.432
