Exploring Deep Belief Network for Chinese Relation ExtractionYu Chen1, Wenjie Li2, Yan Liu2, Dequan Zheng1, Tiejun Zhao11School of Computer Science and Technology, Harbin Institute of Technology, China{chenyu, dqzheng, tjzhao}@mtlab.hit.edu.cn2Department of Computing, The Hong Kong Polytechnic University, Hong Kong{cswjli, csyliu}@comp.polyu.edu.hkAbstractRelation extraction is a fundamentaltask in information extraction thatidentifies the semantic relationshipsbetween two entities in the text.
In thispaper, a novel model based on DeepBelief Network (DBN) is firstpresented to detect and classify therelations among Chinese entities.
Theexperiments conducted on theAutomatic Content Extraction (ACE)2004 dataset demonstrate that theproposed approach is effective inhandling high dimensional featurespace including character N-grams,entity types and the positioninformation.
It outperforms the state-of-the-art learning models such asSVM or BP neutral network.1 IntroductionInformation Extraction (IE) is to automaticallypull out the structured information required bythe users from a large volume of plain text.
Itnormally includes three sequential tasks, i.e.,entity extraction, relation extraction and eventextraction.
In this paper, we limit our focus onrelation extraction.In early time, pattern-based approaches werethe main focus of most research studies inrelation extraction.
Although pattern-basedapproaches achieved reasonably good results,they have some obvious flaws.
It requiresexpensive handcraft work to assemble patternsand not all relations can be identified by a setof reliable patterns (Willy Yap, 2009).
Also,once the interest of task is transferred to adifferent domain or a different language,patterns have to be revised or even rewritten.That is to say, the discovered patterns areheavily dependent on the task in a specificdomain or on a particular corpus.Naturally, a vast amount of work was spenton feature-based machine learning approachesin later years.
In this camp, relation extractionis typically cast as a classification problem,where the most important issue is to train amodel to scale and measure the similarity offeatures reflecting relation instances.
Theentity semantic information expressing relationwas often formulated as the lexical andsyntactic features, which are identical to acertain linear vector in high dimensions.
Manylearning models are capable of self-trainingand classifying these vectors according tosimilarity, such as Support Vector Machine(SVM) and Neural Network (NN).Recently, kernel-based approaches havebeen developing rapidly.
These approachesinvolved kernels of structure representations,like parse tree or dependency tree, in similaritycalculation.
In fact, feature-based approachescan be viewed as the special and simplifiedkinds of kernel-based approaches.
They useddot-product as the kernel function and did notrange over the intricate structure information(Ji, et al 2009).Relation extraction in Chinese receivedquite limited attention as compared to Englishand other western languages.
The main reasonis the unique characteristic of Chinese, such asmore flexible grammar, lack of boundaryinformation and morphological variations etc(Sun and Dong, 2009).
Especially, the existingChinese syntactic analysis tools at currentstage are not yet reliable to capture thevaluable structured information.
It is urgent todevelop approaches that are in particularsuitable for Chinese relation extraction.In this paper, we explore the use of DeepBelief Network (DBN), a new feature-basedmachine learning model for Chinese relationextraction.
It is a neural network modeldeveloped under the deep learning architecturethat is claimed by Hinton (2006) to be able toautomatically learn a deep hierarchy offeatures with increasing levels of abstractionfor the complex problems like natural languageprocessing (NLP).
It avoids assemblingpatterns that express the semantic relationinformation and meanwhile it succeeds toproduce accurate model that is not confined tothe parsing results.The rest of this paper is structured in thefollowing manner.
Section 2 reviews theprevious work on relation extraction.
Section 3presents task definition, briefly introduces theDBN model and the feature construction.Section 4 provides the experimental results.Finally, Section 5 concludes the paper.2 Related WorkOver the past decades, relation extraction hadcome to a significant progress from simplepattern-based approaches to adapted self-training machine learning approaches.Brin (1998) used Dual Iterative PatternRelation Expansion, a bootstrapping-basedsystem, to find the largest common substringsas patterns.
It had the ability of searchingpatterns automatically and was good for largequantity of uniform contexts.
Chen (2006)proposed graph algorithm called labelpropagation, which transferred the patternsimilarity to probability of propagating thelabel information from any vertex to its nearbyvertices.
The label matrix indicated the relationtype.Feature-based approaches utilized the linearvector of carefully chosen lexical and syntacticfeatures derived from different levels of textanalysis and ranging from part-of-speech (POS)tagging to full parsing and dependency parsing(Zhang 2009).
Jing and Zhai (2007) defined aunified graphic representation of features thatserved as a general framework in order tosystematically explore the information atdiverse levels in three subspaces and finallyestimated the effectiveness of these features.They reported that the basic unit feature wasgenerally sufficient to achieve state-of-artperformance.
Meanwhile, over-inclusioncomplex features were harmful.Kernel-based approaches utilize kernelfunctions on structures between two entities,such as sequences and trees, to measure thesimilarity between two relation instances.Zelenok (2003) applied parsing tree kernelfunction to distinguish whether there was anexisting relationship between two entities.However, they limited their task on Person-affiliation and organization-location.The previous work mainly concentrated onrelation extraction in English.
Relatively, lessattention was drawn on Chinese relationextraction.
However, its importance is beinggradually recognized.
For instance, Zhang et al(2008) combined position information, entitytype and context features in a feature-basedapproach and Che (2005) introduced the editdistance kernel over the original Chinese stringrepresentation.DBN is a new feature-based approach forNLP tasks.
According to the work by Hinton(2006), DBN consisted of several layersincluding multiple Restricted BoltzmannMachine (RBM) layers and a BackPropagation (BP) layer.
It was reported toperform very well in many classificationproblems (Ackley, 1985), which is from theorigin of its ability to scale gracefully and becomputationally tractable when applied to highdimensional feature vectors.
Furthermore, toagainst the combinations of feature wereintricate, it detected invariant representationsfrom local translations of the input by deeparchitecture.3 Deep Belief Network for ChineseRelation Extraction3.1 Task DefinitionRelation extraction, promoted by theAutomatic Content Extraction (ACE) program,is a task of finding predefined semanticrelations between pairs of entities from thetexts.
According to the ACE program, an entityis an object or a set of objects in the worldwhile a relation is an explicitly or implicitlystated relationship between entities.
The taskcan be formalized as:1 2( , , )e e s r?
(1)where1e  and 2e  are the two entities in asentence s  under concern and r  is the relationbetween them.
We call the triple1 2( , , )e e s  therelation candidate.
According to the ACE 2004guideline 1 , five relation types are defined.They are:Role: it represents an affiliation between aPerson entity and an Organization, Facility,or GPE (a Geo-political entity) entities.Part: it represents the part-whole relationshipbetween Organization, Facility and GPEentities.At: it represents that a Person, Organization,GPE, or Facility entity is location at aLocation entities.Near: it represents the fact that a Person,Organization, GPE or Facility entity is near(but not necessarily ?At?)
a Location orGPE entities.Social: it represents personal and professionalaffiliations between Person entities.3.2 Deep Belief Networks (DBN)DBN often consists of several layers,including multiple RBM layers and a BP layer.As illustrated in Figure 1, each RBM layerlearns its parameters independently andunsupervisedly.
RBM makes the parametersoptimal for the relevant RBM layer and detectcomplicated features, but not optimal for thewhole model.
There is a supervised BP layeron top of the model which fine-tunes the wholemodel in the learning process and generates theoutput in the inference process.
RBM keepsinformation as more as possible when ittransfers vectors to next layer.
It makesnetworks to avoid local optimum.
RBM is alsoadopted to ensure the efficiency of the DBNmodel.Fig.
1.
The structure of a DBN.1 available at http://www.nist.gov/speech/tests/ace/.Deep architecture of DBN represents manyfunctions compactly.
It is expressible byintegrating different levels of simple functions(Y. Bengio and Y. LeCun).
Upper layers aresupposed to represent more ?abstract?
conceptsthat explain the input data whereas lowerlayers extract ?low-level features?
from thedata.
In addition, none of the RBM guaranteesthat all the information conveyed to the outputis accurate or important enough.
The learnedinformation produced by preceding RBM layerwill be continuously refined through the nextRBM layer to weaken the wrong orinsignificant information in the input.
Multiplelayers filter valuable features.
The units in thefinal layer share more information from thedata.
This increases the representation powerof the whole model.
The final feature vectorsused for classification consist of sophisticatedfeatures which reflect the structuredinformation, promote better classificationperformance than direct original feature vector.3.3 Restricted Boltzmann Machine (RBM)In this section, we will introduce RBM, whichis the core component of DBN.
RBM isBoltzmann Machine with no connection withinthe same layer.
An RBM is constructed withone visible layer and one hidden layer.
Eachvisible unit in the visible layer V  is anobserved variableiv  while each hidden unit inthe hidden layer H  is a hidden variablejh.
Itsjoint distribution is( , ) exp( ( , )) T T Th Wv b x c hp v h E v h e ?
??
?
?
(2)In RBM, 2( , ) {0,1}v h ?
and ( , , )W b c?
?
arethe parameters that need to be estimated?Wis the weight tying visible layer and hiddenlayer.
b is the bias of units v and c is the bias ofunits h.To learn RBM, the optimum parameters areobtained by maximizing the joint distribution( , )p v h  on the training data (Hinton, 1999).
Atraditional way is to find the gradient betweenthe initial parameters and the expectedparameters.
By modifying the previousparameters with the gradient, the expectedparameters can gradually approximate thetarget parameters as0( 1) ( ) log ( )WP vW W W ??
?
??
??
?
?
(3)where ?
is a parameter controlling the leaningrate.
It determines the speed of W convergingto the target.Traditionally, the Monte Carlo Markovchain (MCMC) is used to calculate this kind ofgradient.0 0log ( , )p v h h v h vw ?
??
?
??
(4)where log ( , )p v h  is the log probability of thedata.0 0h vdenotes the multiplication of theaverage over the data states and its relevantsample in hidden unit.h v?
?denotes themultiplication of the average over the modelstates in visible units and its relevant sample inhidden units.Fig.
2.
Learning RBM with CD-basedgradient estimationHowever, MCMC requires estimating anexponential number of terms.
Therefore, ittypically takes a long time to converge toh v?
?.
Hinton (2002) introduced an alternativealgorithm, i.e., the contrastive divergence (CD)algorithm, as a substitution.
It is reported thatCD can train the model much more efficientlythan MCMC.
To estimate the distribution ( )p x ,CD considers a series of distributions { ( )np x }which indicate the distributions in n steps.
Itapproximates the gap of two differentKullback-Leiler divergences as0( || ) ( || )n nCD KL p p KL p p?
??
?
(5)Maximizing the log probability of the data isexactly the same as minimizing the Kullback?Leibler divergence between the distribution ofthe data0p  and the equilibrium distributionp?
defined by the model.In our experiments, we set n to be 1.
Itmeans that in each step of gradient calculation,the estimate of the gradient is used to adjustthe weight of RBM as Equation 6.0 0 1 1log ( , )p v h h v h vW?
?
??
(6)Figure 2 below illustrates the process oflearning RBM with CD-based gradientestimation.3.4 Back-Propagation (BP)The RBM layers provide an unsupervisedanalysis on the structures of data set.
Theyautomatically detect sophisticated featurevectors.
The last layer in DBN is the BP layer.It takes the output from the last RBM layer andapplies it in the final supervised learningprocess.
In DBN, not only is the supervised BPlayer used to generate the final categories, butit is also used to fine-tune the whole network.Specifically speaking, when the BP layer ischanged during its iterating process, thechanges are passed to the other RBM layers ina top-to-bottom sequence.3.5 The Feature SetDBN is able to detect high level hiddenfeatures from lexical, syntactic and/or positioncharacteristic.
As mentioned in related work,over-inclusion complex features are harmful.We therefore involve only three kinds of lowlevel features in this study.
They are describedbelow.3.5.1 Character-based FeaturesSince Chinese text is written without wordboundaries, the word-level features are limitedby the efficiency of word segmentation results.In the paper presented by H. Jing (2003) andsome others, they observed that pure character-based models can even outperform word-basedmodels.
Li et al?s (2008) work relying oncharacter-based features also achievedsignificant performance in relation extraction.We denote the character dictionary as D={d1,d2, ?, dN}.
In our experiment, N is 1500.
Toan e, it?s character-based feature vector isV(e)={ v1, v2, ?, vN }.
Each unit vi can bevalued as Equation 8.???????
?01ededviii(7)3.5.2 Entity Type FeaturesAccording to the ACE 2004 guideline, thereare five entity types in total, including Person,Organization, GPE, Location, and Facility.
Werecognize and classify the relation between therecognized entities.
The entities in ACE 2004corpus were labeled with these five types.Type features are distinctive for classification.For example, the entities of Location cannotappear in the Role relation.3.5.3 Relative Position FeaturesWe define three types of position featureswhich depict the relative structures betweenthe two entities, including Nested, Adjacentand Separated.
For each relation candidatetriple1 2( , , )e e s , let .starte  and .ende  denotethe starting and end positions of e  in adocument.
Table 1 summarizes the conditionsfor each type, where }2,1{, ?ji  and ji ?
.Type ConditionNested ( .start, .end) ( .start, .end)i i j je e e e?Adjacent .end= .start-1i je eSeparated ( .start< .start)&( .end+1< .start)i j i je e e eTable 1.
The internal postion structure featuresbetween two named entitiesWe combine the character-based features oftwo entities, their type information andposition information as the feature vector ofrelation candidate.3.6 Order of Entity PairA relation is basically an order pair.
Forexample, ?Bank of China in Hong Kong?conveys the ACE-style relation ?At?
betweentwo entities ?Bank of China (Organization)?and ?Hong Kong (Location)?.
We can say thatBank of China can be found in Hong Kong,but not vice verse.
The identified relation issaid to be correct only when both its type andthe order of the entity pair are correct.
Wedon?t explicitly incorporate such orderrestriction as an individual feature but use thespecified rules to sort the two entities in arelation once the relation type is recognized.As for those symmetric relation types, theorder needs not to be concerned.
Either order isconsidered correct in the ACE standard.
As forthose asymmetric relation types, we simplyselect the first (in adjacent and separatedstructure) or outer (in nested structures) as thefirst entity.
In most cases, this treatment leadsto the correct order.
We also make use ofentity types to verify (and rectify if necessary)this default order.
For example, considering?At?
is a relation between a Person,Organization, GPE, or Facility entity and aLocation entity, the Location entity must beplaced after the Person, Organization, GPE, orFacility entity in a relation.4 Experiments and Evaluations4.1 Experiment SetupThe experiments are conducted on the ACE2004 Chinese relation extraction dataset,which consists of 221 documents selected frombroadcast news and newswire reports.
Thereare 2620 relation instances and 11800 pairs ofentities have no relationship in the dataset.
Thesize of the feature space is 3017.We examine the proposed DBN modelusing 4-fold cross-validation.
The performanceis measured by precision, recall, and F-measure.2*Precision*Recall-measure= Precision+RecallF(8)In the following experiments, we plan to testthe effectiveness of the DBN model in threeways:Detection Only: For each relation candidate,we only recognize whether there is a certainrelationship between the two entities, nomatter what type of relation they hold.Detection and Classification in Sequence:For each relation candidate, when it isdetected to be an instance of relation, itproceeds to detect the type of the relationthe two entities hold.Detection and Classification in Combination:We define N+1 relation label, N for relationtypes defined by ACE and one for NULLindicating there is no relationship betweenthe two entities.
In this way, the processesof detection and classification are combined.We will compare DBN with a well-knownSupport Vector Machine model (labeled asSVM in the tables) and a traditional BP neutralnetwork model (labeled as NN (BP only)).Among them, SVM has been successfullyapplied in many classification applications.
Weuse the LibSVM toolkit 2  to implement theSVM model.4.2 Evaluation on Detection OnlyWe first evaluate relation detection, whereonly two output classes are concerned, i.e.NULL (which means no relation recognized)and RELATION.
The parameters used in DBN,SVM and NN (BP only) are tunedexperimentally and the results with the bestparameter settings are presented in Table 2.
Ineach of our experiments, we test manyparameters of SVM and chose the best set ofthat to show below.Regarding the structure of DBN, weexperiment with different combinations of unitnumbers in the RBM layers.
Finally we chooseDBN with three RBM layers and one BP layer.And the numbers of units in each RBM layerare 2400, 1800 and 1200 respectively, which isthe best size of each layer in our experiment.Our empirical results showed that the numbersof units in adjoining layers should not decreasethe dimension of feature vector too much whencasting the vector transformation.
NN has thesame structure as DBN.
As for SVM, wechoose the linear kernel with the penaltyparameter C=0.3, which is the best penaltycoefficient, and set the other parameters asdefault after comparing different kernels andparameter values.Model Precision Recall F-measureDBN 67.8% 70.58% 69.16%SVM 73.06% 52.42% 61.04%NN (BPonly)51.51% 61.77% 56.18%Table 2.
Performances of DBN, SVM and NNmodels for detection onlyAs showed in Table 2, with their bestparameter settings, DBN performs much better2 http://www.csie.ntu.edu.tw/~cjlin/libsvm/than both SVM and NN (BP only) in terms ofF-measure.
It tells that DBN is quite good inthis binary classification task.
Since RBM is afast approach to approximate global optimumof networks, its advantage over NN (BP only)is clearly demonstrated in their results.4.3 Evaluation on Detection andClassification in SequenceIn the next experiment, we go one step further.If a relation is detected, we classified it intoone of the 5 pre-defined relation types.
Forrelation type classification, DBN and NN (BPonly) have the same structures as they are inthe first experiment.
We adopt SVM linearkernel again and set C to 0.09 and otherparameters as default.
The overall performanceof detection and classification of three modelsare illustrated in Table 3 below.
DBN again ismore effective than SVM and NN.Model Precision Recall F-measureDBN 63.67% 59% 61.25%SVM 67.78% 47.43% 55.81%NN  61% 45.62% 52.2%Table 3.
Performances of DBN and otherclassification models for detection andclassification in sequence4.4 Evaluation on Detection andClassification in CombinationIn the third experiment, we unify relationdetection and relation type classification intoone classification task.
All the candidates aredirectly classified into one of the 6 classes,including 5 relation types and a NULL class.Parameter settings of the three models in thisexperiment are identical to those in the secondexperiment, except that C in SVM is set to 0.1.Model Precision Recall F-measureDBN 65.8% 59.15% 62.3%SVM 75.25% 44.07% 55.59%NN (BPonly)63.2% 45.7% 53.05%Table 4.
Performances of DBN, SVM and NNmodels for detection and classification incombinationAs demonstrated, DBN outperforms bothSVM and NN (BP only) in all these threeexperiments consistently.
In this regard, theadvantages of DBN over the other two modelsare apparent.
RBM approximates expectedparameters rapidly and the deep DBNarchitecture yields stronger representativenessof complicated, efficient features.Comparing the results of the second and thethird experiments, SVM perform better(although not quite significantly) whendetection and classification are in sequencethan in combination.
This finding is consistentwith our previous work (to be added later).
Itcan possibly be that preceding detection helpsto deal with the severe unbalance problem, i.e.there are much more relation candidates thatdon?t hold pre-defined relations.
However,DBN obtaining the opposite result cause bythat the amount of examples we have is notsufficient for DBN to self-train itself well fortype classification.
We will further exam thisissue in our feature work.4.5 Evaluation on DBN StructureNext, we compare the performance of DBNwith different structures by changing thenumber of RBM layers.
All the candidates aredirectly classified into 6 types in thisexperiment.DBN  Precision Recall F-measure3 RBMs +BP65.8% 59.15% 62.3%2 RBMs +BP65.22% 57.1% 60.09%1 RBM +BP64.35% 55.5% 59.6%Table 5.
Performance RBM with differentlayersThe results provided in Table 5 show thatthe performance can be improved when moreRBM layers are incorporated.
Multiple RBMlayers enhance representation power.
Since itwas reported by Hinton (2006) that three RBMlayer is enough to detect the complex featuresand more RBM layer are of less help, we donot try to go beyond the three layers in thisexperiment.
Note that the improvement is moreobvious from two layers to three layers thanfrom one layer to two layers.4.6 Error AnalysisFinally, we provide the test results forindividual relation types in Table 6.
We cansee that the proposed model performs better on?Role?
and ?Part?
relations.
When taking acloser look at their relation instancedistributions, the instances of these two typescomprise over 63% percents of all the relationinstances in the dataset.
Clearly their betterresults benefit from the amount of training data.It further implies that if we have more trainingdata, we should be able to train a morepowerful DBN.
The same characteristic is alsoobserved in Table 7 which shows thedistributions of the identified relations againstthe gold standard.
However, the sizes of ?At?relation instances and ?Role?
relation instancesare similar, its result is much worse.
Webelieve it is from the origin of that the positionfeature is not distinctive for ?At?
relation, asshown in Table 8.
?Near?
and ?Social?
are twosymmetric relation types.
Ideally, they shouldhave better results.
But due to quite smallnumber of training examples, you can see thatthey are actually the types with the worst F-measure.Type Precision Recall F-measureRole 65.19% 69.2% 67.14%Part 67.86% 71.43% 69.59%At 51.15% 60% 55.22%Near 15.38% 33.33% 20.05%Social 25% 35.71% 29.41%Table 6.
Performance of DBN for eachrelation typeR P A N S NullRole (R) 191 1 5 0 0 96Part (P) 1 95 12 0 0 32At (A) 4 8 111 2 1 91Near (N) 0 1 0 2 0 10Social (S) 1 0 0 0 5 14Table 7.
Distribution of the identified relationsType Adjacent  Separated NestedRole 7 63 223Part 1 17 122At 21 98 98Near 0 8 5Social 10 10 10IdentifiedStandardTable 8.
Statistic of position featureThe main mistakes observed in Table 7 arewrongly classifying a ?Part?
relation as a ?At?relations.
We further inspect these 12 mistakesand find that it is indeed difficult to distinct thetwo types for the given entity pairs.
Here is atypical example: entity 1: ?????
(theDemocratic Party of the United States, definedas an organization entity), entity 2: ??
(theUnited States, defined as a GPE entity).Therefore, the major problem we have to faceis how to effectively recall more relations.Given the limited training resources, it isneeded to well explore the appropriate externalknowledge or the Web resources.5 ConclusionsIn this paper we present our recent work onapplying a novel machine learning model,namely Deep Belief Network, to Chineserelation extraction.
DBN is demonstrated tobe effective for Chinese relation extractionbecause of its strong representativeness.
Weconduct a series of experiments to prove thebenefits of DBN.
Experimental results clearlyshow the strength of DBN which obtainsbetter performance than other existing modelssuch as SVM and the traditional BP neutralnetwork.
In the future, we will explore if it ispossible to incorporate the appropriateexternal knowledge in order to recall morerelation instances, given the limited trainingresource.ReferencesAckley D., Hinton G. and Sejnowski T. 1985.
Alearning algorithm for Boltzmann machines,Cognitive Science, 9.Brin Sergey.
1998.
Extracting patterns and relationsfrom world wide web, In Proceedings ofWebDB Workshop at 6th InternationalConference on Extending DatabaseTechnology (WebDB?98), 172-183.Che W.X.
Improved-Edit-Distance Kernel forChinese Relation Extraction, In Dale, R.,Wong,K.-F., Su, J., Kwong, O.Y.
(eds.)
IJCNLP2005.LNCS(LNAI).
vol.
2651.H.
Jing, R. Florian, X. Luo, T. Zhang, A.Ittycheriah.
2003.
How to get a Chinese name(entity): Segmentation and combination issues.In proceedings of EMNLP.
200-207.Hinton, G.. 1999.
Products of experts.
InProceedings of the Ninth International.Conference on Artificial Neural Networks(ICANN).
Vol.
1, 1?6.Hinton, G. E. 2002.
Training products of experts byminimizing contrastive divergence, NeuralComputation, 14(8), 1711?1800.Hinton G. E., Osindero S. and Teh Y.
2006.
A fastlearning algorithm for deep belief nets, NeuralComputation, 18.
1527?1554.Ji Zhang, You Ouyang, Wenjie Li and YuexianHou.
2009.
A Novel Composite KernelApproach to Chinese Entity RelationExtraction.
in Proceedings of the 22ndInternational Conference on the ComputerProcessing of Oriental Languages, Hong Kong,pp240-251.Ji Zhang, You Ouyang, Wenjie Li, and YuexianHou.
2009.
Proceedings of the 22ndInternational Conference on ComputerProcessing of Oriental Languages.
236-247.Jiang J. and Zhai C. 2007.
A SystematicExploration of the Feature Space for RelationExtraction, In Proceedings of NAACL/HLT,113?120.Jinxiu Chen, Donghong Ji, Chew L., Tan andZhengyu Niu.
2006.
Relation extraction usinglabel propagation based semi-supervisedlearning, In Proceedings of ACL?06, 129?136.Li W.J., Zhang P., Wei F.R., Hou Y.X.
and Lu, Q.2008.
A Novel Feature-based Approach toChinese Entity Relation Extraction, InProceeding of ACL 2008 (Companion Volume),89?92Sun Xia and Dong Lehong, 2009.
Feature-basedApproach to Chinese Term Relation Extraction.International Conference on Signal ProcessingSystems.Willy Yap and Timothy Baldwin.
2009.Experiments on Pattern-based RelationLearning.
Proceeding of the 18th ACMconference on Information and knowledgemanagement.
1657-1660.Y.
Bengio and Y. LeCun.
2007.
Scaling learningalgorithms towards ai.
Large-Scale KernelMachines.
MIT Press.Zelenko D. Aone C and Richardella A.
2003.Kernel Methods for Relation Extraction,Journal of Machine Learning Research2003(2), 1083?1106.Zhang P., Li W.J., Wei F.R., Lu Q. and Hou Y.X.2008.
Exploiting the Role of Position Featurein Chinese Relation Extraction, In Proceedingsof the 6th International Conference onLanguage Resources and Evaluation (LREC).
