Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 945?954,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsA Generative Entity-Mention Model for Linking Entities withKnowledge BaseXianpei Han        Le SunInstitute of Software, Chinese Academy of SciencesHaiDian District, Beijing, China.
{xianpei, sunle}@nfs.iscas.ac.cnAbstractLinking entities with knowledge base (entitylinking) is a key issue in bridging the textualdata with the structural knowledge base.
Due tothe name variation problem and the nameambiguity problem, the entity linking decisionsare critically depending on the heterogenousknowledge of entities.
In this paper, we proposea generative probabilistic model, called entity-mention model, which can leverageheterogenous entity knowledge (includingpopularity knowledge, name knowledge andcontext knowledge) for the entity linking task.In our model, each name mention to be linkedis modeled as a sample generated through athree-step generative story, and the entityknowledge is encoded in the distribution ofentities in document P(e), the distribution ofpossible names of a specific entity P(s|e), andthe distribution of possible contexts of aspecific entity P(c|e).
To find the referent entityof a name mention, our method combines theevidences from all the three distributions P(e),P(s|e) and P(c|e).
Experimental results showthat our method can significantly outperformthe traditional methods.1 IntroductionIn recent years, due to the proliferation ofknowledge-sharing communities like Wikipedia 1and the many research efforts for the automatedknowledge base population from Web like theRead the Web2 project, more and more large-scaleknowledge bases are available.
These knowledgebases contain rich knowledge about the world?sentities, their semantic properties, and the semanticrelations between each other.
One of the mostnotorious examples is Wikipedia: its 2010 English1 http://www.wikipedia.org/2 http://rtw.ml.cmu.edu/version contains more than 3 million entities and20 million semantic relations.
Bridging theseknowledge bases with the textual data can facilitatemany different tasks such as entity search,information extraction and text classification.
Forexample, as shown in Figure 1, knowing the wordJordan in the document refers to a basketballplayer and the word Bulls refers to a NBA teamwould be helpful in classifying this document intothe Sport/Basketball class.After a standout career at the University,joined the in 1984.Michael Jeffrey JordanNBA PlayerBasketball PlayerChicago BullsNBASport OrganizationNBA TeamKnowledge BaseEmployer-ofIS-AIS-A IS-AIS-AIS-APart-ofJordanBullsFigure 1.
A Demo of Entity LinkingA key issue in bridging the knowledge base withthe textual data is linking the entities in adocument with their referents in a knowledge base,which is usually referred to as the Entity Linkingtask.
Given a set of name mentions M = {m1,m2, ?, mk} contained in documents and aknowledge base KB containing a set of entities E ={e1, e2, ?, en}, an entity linking system is afunction : M E?
?
which links these namementions to their referent entities in KB.
Forexample, in Figure 1 an entity linking systemshould link the name mention Jordan to the entityMichael Jeffrey Jordan and the name mentionBulls to the entity Chicago Bulls.The entity linking task, however, is not trivialdue to the name variation problem and the nameambiguity problem.
Name variation means that anentity can be mentioned in different ways such asfull name, aliases, acronyms and misspellings.
For945example, the entity Michael Jeffrey Jordan can bementioned using more than 10 names, such asMichael Jordan, MJ and Jordan.
The nameambiguity problem is related to the fact that aname may refer to different entities in differentcontexts.
For example, the name Bulls can refer tomore than 20 entities in Wikipedia, such as theNBA team Chicago Bulls, the football team BelfastBulls and the cricket team Queensland Bulls.Complicated by the name variation problem andthe name ambiguity problem, the entity linkingdecisions are critically depending on theknowledge of entities (Li et al, 2004; Bunescu &Pasca, 2006; Cucerzan, 2007; Milne & Witten,2008 and Fader et al, 2009).
Based on the previouswork, we found that the following three types ofentity knowledge can provide critical evidence forthe entity linking decisions:?
Popularity Knowledge.
The popularityknowledge of entities tells us the likelihood of anentity appearing in a document.
In entity linking,the entity popularity knowledge can provide apriori information to the possible referent entitiesof a name mention.
For example, without any otherinformation, the popularity knowledge can tell thatin a Web page the name ?Michael Jordan?
willmore likely refer to the notorious basketball playerMichael Jeffrey Jordan, rather than the lesspopular Berkeley professor Michael I.
Jordan.?
Name Knowledge.
The name knowledgetells us the possible names of an entity and thelikelihood of a name referring to a specific entity.For example, we would expect the nameknowledge tells that both the ?MJ?
and ?MichaelJordan?
are possible names of the basketballplayer Michael Jeffrey Jordan, but the ?MichaelJordan?
has a larger likelihood.
The nameknowledge plays the central role in resolving thename variation problem, and is also helpful inresolving the name ambiguity problem.?
Context Knowledge.
The contextknowledge tells us the likelihood of an entityappearing in a specific context.
For example, giventhe context ?__wins NBA MVP?, the name?Michael Jordan?
should more likely refer to thebasketball player Michael Jeffrey Jordan than theBerkeley professor Michael I. Jordan.
Contextknowledge is crucial in solving the nameambiguities.Unfortunately, in entity linking system, themodeling and exploitation of these types of entityknowledge is not straightforward.
As shown above,these types of knowledge are heterogenous,making it difficult to be incorporated in the samemodel.
Furthermore, in most cases the knowledgeof entities is not explicitly given, making itchallenging to extract the entity knowledge fromdata.To resolve the above problems, this paperproposes a generative probabilistic model, calledentity-mention model, which can leverage theheterogeneous entity knowledge (includingpopularity knowledge, name knowledge andcontext knowledge) for the entity linking task.
Inour model, each name mention is modeled as asample generated through a three-step generativestory, where the entity knowledge is encoded inthree distributions: the entity popularity knowledgeis encoded in the distribution of entities indocument P(e), the entity name knowledge isencoded in the distribution of possible names of aspecific entity P(s|e), and the entity contextknowledge is encoded in the distribution ofpossible contexts of a specific entity P(c|e).
TheP(e), P(s|e) and P(c|e) are respectively called theentity popularity model, the entity name model andthe entity context model.
To find the referent entityof a name mention, our method combines theevidences from all the three distributions P(e),P(s|e) and P(c|e).
We evaluate our method on bothWikipedia articles and general newswiredocuments.
Experimental results show that ourmethod can significantly improve the entity linkingaccuracy.Our Contributions.
Specifically, the maincontributions of this paper are as follows:1) We propose a new generative model, theentity-mention model, which can leverageheterogenous entity knowledge (includingpopularity knowledge, name knowledge andcontext knowledge) for the entity linking task;2) By modeling the entity knowledge asprobabilistic distributions, our model has astatistical foundation, making it different frommost previous ad hoc approaches.This paper is organized as follows.
The entity-mention model is described in Section 2.
Themodel estimation is described in Section 3.
Theexperimental results are presented and discussed inSection 4.
The related work is reviewed in Section5.
Finally we conclude this paper in Section 6.9462 The Generative Entity-Mention Modelfor Entity LinkingIn this section we describe the generative entity-mention model.
We first describe the generativestory of our model, then formulate the model andshow how to apply it to the entity linking task.2.1 The Generative StoryIn the entity mention model, each name mention ismodeled as a generated sample.
For demonstration,Figure 2 shows two examples of name mentiongeneration.
As shown in Figure 2, the generativestory of a name mention is composed of three steps,which are detailed as follows:(i) Firstly, the model chooses the referententity e of the name mention from the givenknowledge base, according to the distribution ofentities in document P(e).
In Figure 2, the modelchooses the entity ?Michael Jeffrey Jordan?
for thefirst name mention, and the entity ?Michael I.Jordan?
for the second name mention;(ii) Secondly, the model outputs the name s ofthe name mention according to the distribution ofpossible names of the referent entity P(s|e).
InFigure 2, the model outputs ?Jordan?
as the nameof the entity ?Michael Jeffrey Jordan?, and the?Michael Jordan?
as the name of the entity?Michael I.
Jordan?
;(iii) Finally, the model outputs the context c ofthe name mention according to the distribution ofpossible contexts of the referent entity P(c|e).
InFigure 2, the model outputs the context ?joinsBulls in 1984?
for the first name mention, and thecontext ?is a professor in UC Berkeley?
for thesecond name mention.2.2 ModelBased on the above generative story, theprobability of a name mention m (its context is cand its name is s) referring to a specific entity ecan be expressed as the following formula (here weassume that s and c are independent given e):( , , )P(m,e)= P s c e = P(e)P(s |e)P(c|e)This model incorporates the three types of entityknowledge we explained earlier: P(e) correspondsto the popularity knowledge, P(s|e) corresponds tothe name knowledge and P(c|e) corresponds to thecontext knowledge.Knowledge BaseMichael Jeffrey JordanMichael I. JordanJordan Michael JordanJordan joins Bulls in1984.Michael Jordan is aprofessor in UC Berkeley.EntityNameMentionFigure 2.
Two examples of name mentiongenerationGiven a name mention m, to perform entitylinking, we need to find the entity e whichmaximizes the probability P(e|m).
Then we canresolve the entity linking task as follows:( , )e argmax argmax ( ) ( | ) ( | )( ) eeP m e P e P s e P c eP m?
?Therefore, the main problem of entity linking is toestimate the three distributions P(e), P(s|e) andP(c|e), i.e., to extract the entity knowledge fromdata.
In Section 3, we will show how to estimatethese three distributions.Candidate Selection.
Because a knowledge baseusually contains millions of entities, it is time-consuming to compute all P(m,e) scores between aname mention and all the entities contained in aknowledge base.
To reduce the time required, theentity linking system employs a candidate selectionprocess to filter out the impossible referentcandidates of a name mention.
In this paper, weadopt the candidate selection method ofNLPR_KBP system (Han and Zhao, 2009), themain idea of which is first building a name-to-entity dictionary using the redirect links,disambiguation pages, anchor texts of Wikipedia,then the candidate entities of a name mention areselected by finding its name?s corresponding entryin the dictionary.3 Model EstimationSection 2 shows that the entity mention model candecompose the entity linking task into theestimation of three distributions P(e), P(s|e) andP(c|e).
In this section, we describe the details of theestimation of these three distributions.
We first947introduce the training data, then describe theestimation methods.3.1 Training DataIn this paper, the training data of our model is a setof annotated name mentions M = {m1, m2, ?, mn}.Each annotated name mention is a triple m={s, e,c}, where s is the name, e is the referent entity andc is the context.
For example, two annotated namementions are as follows:?
Jordan | Michael Jeffrey Jordan | ?
wins his first NBAMVP in 1991.?
NBA | National Basketball Association | ?
is the pre-eminent men's professional basketball league.In this paper, we focus on the task of linkingentities with Wikipedia, even though the proposedmethod can be applied to other resources.
We willonly show how to get the training data fromWikipedia.
In Wikipedia, a hyperlink between twoarticles is an annotated name mention (Milne &Witten, 2008): its anchor text is the name and itstarget article is the referent entity.
For example, infollowing hyperlink (in Wiki syntax), the NBA isthe name and the National Basketball Associationis the referent entity.
?He won his first [[National Basketball Association |NBA]] championship with the Bulls?Therefore, we can get the training data bycollecting all annotated name mentions from thehyperlink data of Wikipedia.
In total, we collectedmore than 23,000,000 annotated name mentions.3.2 Entity Popularity ModelThe distribution P(e) encodes the popularityknowledge as a distribution of entities, i.e., theP(e1) should be larger than P(e2) if e1 is morepopular than e2.
For example, on the Web theP(Michael Jeffrey Jordan) should be higher thanthe P(Michael I. Jordan).
In this section, weestimate the distribution P(e) using a model calledentity popularity model.Given a knowledge base KB which contains Nentities, in its simplest form, we can assume thatall entities have equal popularity, and thedistribution P(e) can be estimated as:( ) 1P e N?However, this does not reflect well the realsituation because some entities are obviously morepopular than others.
To get a more preciseestimation, we observed that a more popular entityusually appears more times than a less popularentity in a large text corpus, i.e., more namementions refer to this entity.
For example, inWikipedia the NBA player Michael Jeffrey Jordanappears more than 10 times than the Berkeleyprofessor Michael I. Jordan.
Based on the aboveobservation, our entity popularity model uses theentity frequencies in the name mention data set Mto estimate the distribution P(e) as follows:( ) 1( ) Count eP e M N??
?where Count(e) is the count of the name mentionswhose referent entity is e, and the |M| is the totalname mention size.
The estimation is furthersmoothed using the simple add-one smoothingmethod for the zero probability problem.
Forillustration, Table 1 shows three selected entities?popularity.Entity PopularityNational Basketball Association 1.73*10-5Michael Jeffrey Jordan(NBA player) 8.21*10-6Michael I. Jordan(Berkeley Professor) 7.50*10-8Table 1.
Three examples of entity popularity3.3 Entity Name ModelThe distribution P(s|e) encodes the nameknowledge of entities, i.e., for a specific entity e,its more frequently used name should be assigned ahigher P(s|e) value than the less frequently usedname, and a zero P(s|e) value should be assignedto those never used names.
For instance, we wouldexpect the P(Michael Jordan|Michael JeffreyJordan) to be high, P(MJ|Michael Jeffrey Jordan)to be relative high and P(Michael I.Jordan|Michael Jeffrey Jordan) to be zero.Intuitively, the name model can be estimated byfirst collecting all (entity, name) pairs from thename mention data set, then using the maximumlikelihood estimation:( , )( | ) ( , )sCount e sP s e Count e s?
?where the Count(e,s) is the count of the namementions whose referent entity is e and name is s.However, this method does not work well becauseit cannot correctly deal with an unseen entity or anunseen name.
For example, because the name?MJ?
doesn?t refer to the Michael Jeffrey Jordan inWikipedia, the name model will not be able toidentify ?MJ?
as a name of him, even ?MJ?
is apopular name of Michael Jeffrey Jordan on Web.948To better estimate the distribution P(s|e), thispaper proposes a much more generic model, calledentity name model, which can capture thevariations (including full name, aliases, acronymsand misspellings) of an entity's name using astatistical translation model.
Given an entity?sname s, our model assumes that it is a translationof this entity?s full name f using the IBM model 1(Brown, et al, 1993).
Let ?
be the vocabularycontaining all words may be used in the name ofentities, the entity name model assumes that aword in ?
can be translated through the followingfour ways:1) It is retained (translated into itself);2) It is translated into its acronym;3) It is omitted(translated into the word NULL);4) It is translated into another word (misspellingor alias).In this way, all name variations of an entity arecaptured as the possible translations of its fullname.
To illustrate, Figure 3 shows how the fullname ?Michael Jeffrey Jordan?
can be transaltedinto its misspelling name ?Micheal Jordan?.Figure 3.
The translation from Michael JeffereyJordan to Micheal JordanBased on the translation model, P(s|e) can bewritten as:01( | )( 1)fsslli jlijfP(s |e) t s fl????
?
?
?where ?
is a normalization factor, f is the full nameof entity e, lf is the length of f, ls is the length of thename s, si the ith word of s, fj is the jth word of f andt(si|fj) is the lexical translation probability whichindicates the probability of a word fj in the fullname will be written as si in the output name.Now the main problem is to estimate the lexicaltranslation probability t(si|fj).
In this paper, we firstcollect the (name, entity full name) pairs from allannotated name mentions, then get the lexicaltranslation probability by feeding this data set intoan IBM model 1 training system (we use theGIZA++ Toolkit3).Table 2 shows several resulting lexicaltranslation probabilities through the above process.3 http://fjoch.com/GIZA++.htmlWe can see that the entity name model can capturethe different name variations, such as the acronym(Michael?M), the misspelling (Michael?Micheal)and the omission (St. ?
NULL).Full name word Name word ProbabilityMichael Michael 0.77Michael M 0.008Michael Micheal 2.64*10-4Jordan Jordan 0.96Jordan J 6.13*10-4St.
NULL 0.14Sir NULL 0.02Table 2.
Several lexical translation probabilities3.4 Entity Context ModelThe distribution P(c|e) encodes the contextknowledge of entities, i.e., it will assign a highP(c|e) value if the entity e frequently appears in thecontext c, and will assign a low P(c|e) value if theentity e rarely appears in the context c. Forexample, given the following two contexts:C1: __wins NBA MVP.C2: __is a researcher in machine learning.Then P(C1|Michael Jeffrey Jordan) should be highbecause the NBA player Michael Jeffrey Jordanoften appears in C1 and the P(C2|Michael JeffreyJordan) should be extremely low because he rarelyappears in C2.__ wi s NBA MVP.__is a professor in UCBerkeley.Michael Jeffrey Jordan(NBA Player)NBA=0.03MVP=0.008Basketball=0.02player=0.005win=0.00008professor=0...Michael I. Jordan(Berkeley Professor)professor=0.003Berkeley=0.002machine learning=0.1researcher = 0.006NBA = 0MVP=0...Figure 4.
Two entity context modelsTo estimate the distribution P(c|e), we propose amethod based on language modeling, called entitycontext model.
In our model, the context of eachname mention m is the word window surroundingm, and the window size is set to 50 according tothe experiments in (Pedersen et al, 2005).Specifically, the context knowledge of an entity eis encoded in an unigram language model:{ ( )}e eM P t?where Pe(t) is the probability of the term tappearing in the context of e. In our model, theterm may indicate a word, a named entity(extracted using the Stanford Named EntityMichael Jeffrey JordanMicheal Jordan NULLFull NameName949Recognizer 4 ) or a Wikipedia concept (extractedusing the method described in (Han and Zhao,2010)).
Figure 4 shows two entity context modelsand the contexts generated using them.Now, given a context c containing n termst1t2?tn, the entity context model estimates theprobability P(c|e) as:1 2 1 2( | ) ( ... | ) ( ) ( ).... ( )n e e e e nP c e P t t t M P t P t P t?
?So the main problem is to estimate Pe(t), theprobability of a term t appearing in the context ofthe entity e.Using the annotated name mention data set M,we can get the maximum likelihood estimation ofPe(t) as follows:_( )( ) ( )ee MLetCount tP t Count t?
?where Counte(t) is the frequency of occurrences ofa term t in the contexts of the name mentionswhose referent entity is e.Because an entity e?s name mentions are usuallynot enough to support a robust estimation of Pe(t)due to the sparse data problem (Chen andGoodman, 1999), we further smooth Pe(t) using theJelinek-Mercer smoothing method (Jelinek andMercer, 1980):_( ) ( ) (1 ) ( )e e ML gP t P t P t?
??
?
?where Pg(t) is a general language model which isestimated using the whole Wikipedia data, and theoptimal value of ?
is set to 0.2 through a learningprocess shown in Section 4.3.5 The NIL Entity ProblemBy estimating P(e), P(s|e) and P(c|e), our methodcan effectively link a name mention to its referententity contained in a knowledge base.Unfortunately, there is still the NIL entity problem(McNamee and Dang, 2009), i.e., the referententity may not be contained in the givenknowledge base.
In this situation, the namemention should be linked to the NIL entity.Traditional methods usually resolve this problemwith an additional classification step (Zheng et al2010): a classifier is trained to identify whether aname mention should be linked to the NIL entity.Rather than employing an additional step, ourentity mention model seamlessly takes into accountthe NIL entity problem.
The start assumption of4 http://nlp.stanford.edu/software/CRF-NER.shtmlour solution is that ?If a name mention refers to aspecific entity, then the probability of this namemention is generated by the specific entity?s modelshould be significantly higher than the probabilityit is generated by a general language model?.Based on the above assumption, we first add apseudo entity, the NIL entity, into the knowledgebase and assume that the NIL entity generates aname mention according to the general languagemodel Pg, without using any entity knowledge;then we treat the NIL entity in the same way asother entities: if the probability of a name mentionis generated by the NIL entity is higher than allother entities in Knowledge base, we link the namemention to the NIL entity.
Based on the abovediscussion, we compute the three probabilities ofthe NIL entity: P(e), P(s|e) and P(c|e) as follows:1P(NIL) M N?
?
( )gt sP(s | NIL) P t???
( )gt cP(c | NIL) P t??
?4 ExperimentsIn this section, we assess the performance of ourmethod and compare it with the traditionalmethods.
In following, we first explain theexperimental settings in Section 4.1, 4.2 and 4.3,then evaluate and discuss the results in Section 4.4.4.1 Knowledge BaseIn our experiments, we use the Jan. 30, 2010English version of Wikipedia as the knowledgebase, which contains over 3 million distinct entities.4.2 Data SetsTo evaluate the entity linking performance, weadopted two data sets: the first is WikiAmbi, whichis used to evaluate the performance on Wikipediaarticles; the second is TAC_KBP, which is used toevaluate the performance on general newswiredocuments.
In following, we describe these twodata sets in detail.WikiAmbi: The WikiAmbi data set contains 1000annotated name mentions which are randomlyselected from Wikipedia hyperlinks data set (asshown in Section 3.1, the hyperlinks betweenWikipedia articles are manually annotated namementions).
In WikiAmbi, there were 207 distinct950names and each name contains at least twopossible referent entities (on average 6.7 candidatereferent entities for each name) 5 .
In ourexperiments, the name mentions contained in theWikiAmbi are removed from the training data.TAC_KBP: The TAC_KBP is the standard dataset used in the Entity Linking task of the TAC2009 (McNamee and Dang, 2009).
The TAC_KBPcontains 3904 name mentions which are selectedfrom English newswire articles.
For each namemention, its referent entity in Wikipedia ismanually annotated.
Overall, 57% (2229 of 3904)name mentions?s referent entities are missing inWikipedia, so TAC_KBP is also suitable toevaluate the NIL entity detection performance.The above two data sets can provide a standardtestbed for the entity linking task.
However, therewere still some limitations of these data sets: First,these data sets only annotate the salient namementions in a document, meanwhile many NLPapplications need all name mentions are linked.Second, these data sets only contain well-formeddocuments, but in many real-world applications theentity linking often be applied to noisy documentssuch as product reviews and microblog messages.In future, we want to develop a data set which canreflect these real-world settings.4.3 Evaluation CriteriaWe adopted the standard performance metrics usedin the Entity Linking task of the TAC 2009(McNamee and Dang, 2009).
These metrics are:?
Micro-Averaged Accuracy (Micro-Accuracy): measures entity linking accuracyaveraged over all the name mentions;?
Macro-Averaged Accuracy (Macro-Accuracy): measures entity linking accuracyaveraged over all the target entities.As in TAC 2009, we used Micro-Accuracy as theprimary performance metric.4.4 Experimental ResultsWe compared our method with three baselines: (1)The first is the traditional Bag of Words basedmethod (Cucerzan, 2007): a name mention?sreferent entity is the entity which has the highestcosine similarity with its context ?
we denoted it asBoW; (2) The second is the method described in5 This is because we want to create a highly ambiguous testdata set(Medelyan et al, 2008), where a name mention?sreferent entity is the entity which has the largestaverage semantic relatedness with the namemention?s unambiguous context entities ?
wedenoted it as TopicIndex.
(3) The third one is thesame as the method described in (Milne & Witten,2008), which uses learning techniques to balancethe semantic relatedness, commoness and contextquality ?
we denoted it as Learning2Link.4.4.1 Overall PerformanceWe conduct experiments on both WikiAmbi andTAC_KBP datasets with several methods: thebaseline BoW; the baseline TopicIndex; thebaseline Learning2Link; the proposed methodusing only popularity  knowledge (Popu), i.e., theP(m,e)=P(e); the proposed method with onecomponent of the model is ablated(this is used toevaluate the independent contributions of the threecomponents), correspondingly Popu+Name(i.e.,the P(m,e)=P(e)P(s|e)), Name+Context(i.e., theP(m,e)=P(c|e)P(s|e)) and Popu+Context (i.e., theP(m,e)=P(e)P(c|e)); and the full entity mentionmodel (Full Model).
For all methods, theparameters were configured through 10-fold crossvalidation.
The overall performance results areshown in Table 3 and 4.Micro-Accuracy Macro-AccuracyBoW 0.60 0.61TopicIndex 0.66 0.49Learning2Link 0.70 0.54Popu 0.39 0.24Popu + Name 0.50 0.31Name+Context 0.70 0.68Popu+Context 0.72 0.73Full Model 0.80 0.77Table 3.
The overall results on WikiAmbi datasetMicro-Accuracy Macro-AccuracyBoW 0.72 0.75TopicIndex 0.80 0.76Learning2Link 0.83 0.79Popu 0.60 0.53Popu + Name 0.63 0.59Name+Context 0.81 0.78Popu+Context 0.84 0.83Full Model 0.86 0.88Table 4.
The overall results on TAC-KBP datasetFrom the results in Table 3 and 4, we can make thefollowing observations:1) Compared with the traditional methods,our entity mention model can achieve a significant951performance improvement: In WikiAmbi andTAC_KBP datasets, compared with the BoWbaseline, our method respectively gets 20% and14% micro-accuracy improvement; compared withthe TopicIndex baseline, our method respectivelygets 14% and 6% micro-accuracy improvement;compared with the Learning2Link baseline, ourmethod respectively gets 10% and 3% micro-accuracy improvement.2) By incorporating more entity knowledge,our method can significantly improve the entitylinking performance: When only using thepopularity knowledge, our method can onlyachieve 49.5% micro-accuracy.
By adding thename knowledge, our method can achieve 56.5%micro-accuracy, a 7% improvement over the Popu.By further adding the context knowledge, ourmethod can achieve 83% micro-accuracy, a 33.5%improvement over Popu and a 26.5% improvementover Popu+Name.3) All three types of entity knowledgecontribute to the final performance improvement,and the context knowledge contributes the most:By respectively ablating the popularity knowledge,the name knowledge and the context knowledge,the performance of our model correspondinglyreduces 7.5%, 5% and 26.5%.NIL Entity Detection Performance.
Tocompare the performances of resolving the NILentity problem, Table 5 shows the micro-accuracies of different systems on the TAC_KBPdata set (where All is the whole data set, NIL onlycontains the name mentions whose referent entityis NIL, InKB only contains the name mentionswhose referent entity is contained in theknowledge base).
From Table 5 we can see that ourmethod can effectively detect the NIL entitymeanwhile retaining the high InKB accuracy.All NIL InKBBoW 0.72 0.77 0.65TopicIndex 0.80 0.91 0.65Learning2Link 0.83 0.90 0.73Full Model 0.86 0.90 0.79Table 5.
The NIL entity detection performance onthe TAC_KBP data set4.4.2 Optimizing ParametersOur model needs to tune one parameter: theJelinek-Mercer smoothing parameter ?
used in theentity context model.
Intuitively, a smaller ?means that the general language model plays amore important role.
Figure 5 plots the tradeoff.
Inboth WikiAmbi and TAC_KBP data sets,  Figure 5shows that a ?
value 0.2 will result in the bestperformance.Figure 5.
The micro-accuracy vs. ?4.4.3 Detailed AnalysisTo better understand the reasons why and how theproposed method works well, in this Section weanalyze our method in detail.The Effect of Incorporating HeterogenousEntity Knowledge.
The first advantage of ourmethod is the entity mention model canincorporate heterogeneous entity knowledge.
TheTable 3 and 4 have shown that, by incorporatingheterogenous entity knowledge (including thename knowledge, the popularity knowledge andthe context knowledge), the entity linkingperformance can obtain a significant improvement.Figure 6.
The performance vs. training mentionsize on WikiAmbi data setThe Effect of Better Entity KnowledgeExtraction.
The second advantage of our methodis that, by representing the entity knowledge asprobabilistic distributions, our model has astatistical foundation and can better extract theentity knowledge using more training data throughthe entity popularity model, the entity name modeland the entity context model.
For instance, we cantrain a better entity context model P(c|e) usingmore name mentions.
To find whether a better952entity knowledge extraction will result in a betterperformance, Figure 6 plots the micro-accurayalong with the size of the training data on namementions for P(c|e) of each entity e.  From Figure6, we can see that when more training data is used,the performance increases.4.4.4 Comparision with State-of-the-ArtPerformanceWe also compared our method with the state-of-the-art entity linking systems in the TAC 2009KBP track (McNamee and Dang, 2009).
Figure 7plots the comparison with the top fiveperformances in TAC 2009 KBP track.
FromFigure 7, we can see that our method canoutperform the state-of-the-art approaches:compared with the best ranking system, ourmethod can achieve a 4% performanceimprovement.Figure 7.
A comparison with top 5 TAC 2009KBP systems5 Related WorkIn this section, we briefly review the related work.To the date, most entity linking systems employedthe context similarity based methods.
The essentialidea was to extract the discriminative features of anentity from its description, then link a namemention to the entity which has the largest contextsimilarity with it.
Cucerzan (2007) proposed a Bagof Words based method, which represents eachtarget entity as a vector of terms, then thesimilarity between a name mention and an entitywas computed using the cosine similarity measure.Mihalcea & Csomai (2007), Bunescu & Pasca(2006), Fader et al (2009) extended the BoWmodel by incorporating more entity knowledgesuch as popularity knowledge, entity categoryknowledge, etc.
Zheng et al (2010), Dredze et al(2010), Zhang et al (2010) and Zhou et al (2010)employed the learning to rank techniques whichcan further take the relations between candidateentities into account.
Because the contextsimilarity based methods can only represent theentity knowledge as features, the main drawback ofit was the difficulty to incorporate heterogenousentity knowledge.Recently there were also some entity linkingmethods based on inter-dependency.
Thesemethods assumed that the entities in the samedocument are related to each other, thus thereferent entity of a name mention is the entitywhich is most related to its contextual entities.Medelyan et al (2008) found the referent entity ofa name mention by computing the weightedaverage of semantic relatedness between thecandidate entity and its unambiguous contextualentities.
Milne and Witten (2008) extendedMedelyan et al (2008) by adopting learning-basedtechniques to balance the semantic relatedness,commoness and context quality.
Kulkarni et al(2009) proposed a method which collectivelyresolves the entity linking tasks in a document asan optimization problem.
The drawback of theinter-dependency based methods is that they areusually specially designed to the leverage ofsemantic relations, doesn?t take the other types ofentity knowledge into consideration.6 Conclusions and Future WorkThis paper proposes a generative probabilisticmodel, the entity-mention model, for the entitylinking task.
The main advantage of our model is itcan incorporate multiple types of heterogenousentity knowledge.
Furthermore, our model has astatistical foundation, making the entity knowledgeextraction approach different from most previousad hoc approaches.
Experimental results show thatour method can achieve competitive performance.In our method, we did not take into account thedependence between entities in the same document.This aspect could be complementary to those weconsidered in this paper.
For our future work, wecan integrate such dependencies in our model.AcknowledgmentsThe work is supported by the National NaturalScience Foundation of China under Grants no.60773027, 60736044, 90920010, 61070106 and61003117, and the National High TechnologyDevelopment 863 Program of China under Grantsno.
2008AA01Z145.
Moreover, we sincerely thankthe reviewers for their valuable comments.953ReferencesAdafre, S. F. & de Rijke, M. 2005.
Discovering missinglinks in Wikipedia.
In: Proceedings of the 3rdinternational workshop on Link discovery.Bunescu, R. & Pasca, M. 2006.
Using encyclopedicknowledge for named entity disambiguation.
In:Proceedings of EACL, vol.
6.Brown,  P., Pietra, S. D.,  Pietra, V. D., and Mercer, R.1993.
The mathematics of statistical machinetranslation: parameter estimation.
ComputationalLinguistics, 19(2), 263-31.Chen, S. F. & Goodman, J.
1999.
An empirical study ofsmoothing techniques for language modeling.
InComputer Speech and Language, London; Orlando:Academic Press, c1986-, pp.
359-394.Cucerzan, S. 2007.
Large-scale named entitydisambiguation based on Wikipedia data.
In:Proceedings of EMNLP-CoNLL, pp.
708-716.Dredze, M., McNamee, P., Rao, D., Gerber, A.
& Finin,T.
2010.
Entity Disambiguation for Knowledge BasePopulation.
In: Proceedings of the 23rd InternationalConference on Computational Linguistics.Fader, A., Soderland, S., Etzioni, O.
& Center, T. 2009.Scaling Wikipedia-based named entitydisambiguation to arbitrary web text.
In: Proceedingsof  Wiki-AI Workshop at IJCAI, vol.
9.Han, X.
& Zhao, J.
2009.
NLPR_KBP in TAC 2009KBP Track: A Two-Stage Method to Entity Linking.In: Proceeding of Text Analysis Conference.Han, X.
& Zhao, J.
2010.
Structural semanticrelatedness: a knowledge-based method to namedentity disambiguation.
In: Proceedings of the 48thAnnual Meeting of the Association forComputational Linguistics.Jelinek, Frederick and Robert L. Mercer.
1980.Interpolated estimation of Markov source parametersfrom sparse data.
In: Proceedings of the Workshopon Pattern Recognition in Practice.Kulkarni, S., Singh, A., Ramakrishnan, G. &Chakrabarti, S. 2009.
Collective annotation ofWikipedia entities in web text.
In: Proceedings of the15th ACM SIGKDD international conference onKnowledge discovery and data mining, pp.
457-466.Li, X., Morie, P. & Roth, D. 2004.
Identification andtracing of ambiguous names: Discriminative andgenerative approaches.
In: Proceedings of theNational Conference on Artificial Intelligence, pp.419-424.McNamee, P. & Dang, H. T. 2009.
Overview of theTAC 2009 Knowledge Base Population Track.
In:Proceeding of Text Analysis Conference.Milne, D. & Witten, I. H. 2008.
Learning to link withWikipedia.
In: Proceedings of the 17th ACMconference on Conference on information andknowledge management.Milne, D., et al  2006.
Mining Domain-SpecificThesauri from Wikipedia: A case study.
In Proc.
ofIEEE/WIC/ACM WI.Medelyan, O., Witten, I. H. & Milne, D. 2008.
Topicindexing with Wikipedia.
In: Proceedings of theAAAI WikiAI workshop.Mihalcea, R. & Csomai, A.
2007.
Wikify!
: linkingdocuments to encyclopedic knowledge.
In:Proceedings of the sixteenth ACM conference onConference on information and knowledgemanagement, pp.
233-242.Pedersen, T., Purandare, A.
& Kulkarni, A.
2005.
Namediscrimination by clustering similar contexts.Computational Linguistics and Intelligent TextProcessing, pp.
226-237.Zhang, W., Su, J., Tan, Chew Lim  & Wang, W. T.2010.
Entity Linking Leveraging AutomaticallyGenerated Annotation.
In: Proceedings of the 23rdInternational Conference on ComputationalLinguistics (Coling 2010).Zheng, Z., Li, F., Huang, M. & Zhu, X.
2010.
Learningto Link Entities with Knowledge Base.
In: TheProceedings of the Annual Conference of the NorthAmerican Chapter of the ACL.Zhou, Y., Nie, L., Rouhani-Kalleh, O., Vasile, F. &Gaffney, S. 2010.
Resolving Surface Forms toWikipedia Topics.
In: Proceedings of the 23rdInternational Conference on ComputationalLinguistics (Coling 2010),  pp.
1335-1343.954
