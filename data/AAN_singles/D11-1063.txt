Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 680?690,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLiteral and Metaphorical Sense Identificationthrough Concrete and Abstract ContextPeter D. TurneyInst.
for Info.
Tech.NRC CanadaOttawa, Canadapeter.turney@nrc-cnrc.gc.caYair NeumanDept.
of EducationBen-Gurion Univ.Beer-Sheva, Israelyneuman@bgu.ac.ilDan AssafDept.
of EducationBen-Gurion Univ.Beer-Sheva, Israeldan.assaf4@googlemail.comYohai CohenGilasio CodingTel-Aviv, Israelyohai@gilasio.comAbstractMetaphor is ubiquitous in text, even in highlytechnical text.
Correct inference about tex-tual entailment requires computers to distin-guish the literal and metaphorical senses ofa word.
Past work has treated this problemas a classical word sense disambiguation task.In this paper, we take a new approach, basedon research in cognitive linguistics that viewsmetaphor as a method for transferring knowl-edge from a familiar, well-understood, or con-crete domain to an unfamiliar, less understood,or more abstract domain.
This view leads tothe hypothesis that metaphorical word usageis correlated with the degree of abstractness ofthe word?s context.
We introduce an algorithmthat uses this hypothesis to classify a wordsense in a given context as either literal (de-notative) or metaphorical (connotative).
Weevaluate this algorithm with a set of adjective-noun phrases (e.g., in dark comedy, the adjec-tive dark is used metaphorically; in dark hair,it is used literally) and with the TroFi (TropeFinder) Example Base of literal and nonliteralusage for fifty verbs.
We achieve state-of-the-art performance on both datasets.1 IntroductionMetaphor is a natural consequence of our abilityto reason by analogy (Gentner et al, 2001).
It isso common in our daily language that we rarelynotice it (Lakoff and Johnson, 1980).
Identifyingmetaphorical word usage is important for reasoningabout the implications of text.Past work on the problem of distinguishing lit-eral and metaphorical senses has approached it asa classical word sense disambiguation (WSD) task(Birke and Sarkar, 2006).
Here, we take a differ-ent approach to the problem.
Lakoff and Johnson(1980) argue that metaphor is a method for trans-ferring knowledge from a concrete domain to an ab-stract domain.
Therefore we hypothesize that the de-gree of abstractness in a word?s context is correlatedwith the likelihood that the word is used metaphori-cally.
This hypothesis is the basis for our algorithmfor distinguishing literal and metaphorical senses.Consider the following sentences:L: He shot down my plane.?
C1: He fired at my plane.9 A1: He refuted my plane.M : He shot down my argument.9 C2: He fired at my argument.?
A2: He refuted my argument.The literal sense of shot down in L invokes knowl-edge from the domain of war.
The metaphorical us-age of shot down in M transfers knowledge fromthe concrete domain of war to the abstract domainof debate (Lakoff and Johnson, 1980).The entailments of L and M depend on the in-tended senses of shot down.
L entails the concretefired at in C1 (because, in order to literally shootsomething down, you must first fire at it) but not theabstract refuted in A1 (except perhaps as a joke).
Onthe other hand, M entails refuted in A2 but not firedat in C2 (except perhaps as a novel metaphor).In semiotics, Danesi (2003) argues that metaphortransfers associations from the source domain to thetarget domain.
The metaphorical usage of shot downin M carries associations of violence and destruc-680tion that are not conveyed by A2.To make correct inferences about textual entail-ment, computers must be able to distinguish the lit-eral and metaphorical senses of a word.
Since rec-ognizing textual entailment (RTE) is a core problemfor NLP, with applications in Question Answering,Information Retrieval, Information Extraction, andText Summarization, it follows that distinguishingliteral and metaphorical senses is a problem for awide variety of NLP tasks.
The ability to recognizemetaphorical word usage is a core requirement inthe Intelligence Advanced Research Projects Activ-ity (IARPA) Metaphor Program (Madrigal, 2011).1Our approach to the problem of distinguishing lit-eral and metaphorical senses is based on an algo-rithm for calculating the degree of abstractness ofwords.
For instance, plane in L is rated 0.36396 (rel-atively concrete), whereas argument in M is rated0.64617 (relatively abstract), which suggests that theverb shot down is used literally in L, whereas it isused metaphorically in M .
Our abstractness ratingalgorithm is similar to Turney and Littman?s (2003)algorithm for rating words according to their seman-tic orientation.To classify a word usage as literal or metaphori-cal, based on the context, we use supervised learningwith logistic regression.
The abstractness rating al-gorithm is used to generate feature vectors from aword?s context and training data is used to learn alogistic regression model that relates degrees of ab-stractness to the classes literal and metaphorical.We evaluate our algorithm with three experi-ments.
The first experiment involves one hundredadjective-noun phrases labeled denotative (literal) orconnotative (metaphorical or nonliteral) by five an-notators, according to the sense of the adjective.2For instance, deep snow is labeled denotative anddeep appreciation is labeled connotative.
The algo-rithm is able to predict the labels of the annotatorswith an average accuracy of 79%.The next two experiments use the TroFi (TropeFinder) Example Base of literal and nonliteral usagefor fifty verbs.3 The fifty verbs occur in 3,737 sen-tences from The 1987-89 Wall Street Journal (WSJ)Corpus Release 1.
In each sentence, the target verb1See http://www.iarpa.gov/solicitations metaphor.html.2The labeled phrases are available from Yair Neuman.3Available at http://www.cs.sfu.ca/ anoop/students/jbirke/.is labeled L (literal) or N (nonliteral), according tothe sense of the verb that is invoked by the sentence.A subset of twenty-five of the fifty verbs was usedby Birke and Sarkar (2006).In our second experiment, we duplicate the setupof Birke and Sarkar (2006) so that we can com-pare our results with theirs.
In particular, a sepa-rate model is learned for each individual verb.
Weachieve an average f-score of 63.9%, compared toBirke and Sarkar?s (2006) 64.9%.In the third experiment, we train the algorithmon the twenty-five new verbs that were not used byBirke and Sarkar (2006) and then we test it on theold verbs.
That is, the algorithm is tested with verbsthat it has never seen before.
The training verbs aremerged to build a single model, instead of buildinga separate model for each individual verb.
In thisexperiment, the average f-score is 68.1%.The next section presents our algorithm for calcu-lating the degree of abstractness of words.
In Sec-tion 3, we review related work.
The experiments aredescribed in Section 4.
We discuss the results of theexperiments in Section 5 and conclude in Section 6.2 Abstractness and ConcretenessConcrete words refer to things, events, and proper-ties that we can perceive directly with our senses,such as trees, walking, and red.4 Abstract words re-fer to ideas and concepts that are distant from im-mediate perception, such as economics, calculating,and disputable.
In this section, we describe an algo-rithm that can automatically calculate a numericalrating of the degree of abstractness of a word on ascale from 0 (highly concrete) to 1 (highly abstract).For example, the algorithm rates purvey as 1, donutas 0, and immodestly as 0.5.The algorithm is a variation of Turney andLittman?s (2003) algorithm that rates words accord-ing to their semantic orientation.
Positive seman-tic orientation indicates praise (honest, intrepid)and negative semantic orientation indicates criticism(disturbing, superfluous).
The algorithm calculatesthe semantic orientation of a given word by com-paring it to seven positive words and seven nega-4The word red has an abstract political sense, but our ab-stractness rating algorithm does not distinguish word senses.The more frequent concrete sense of red dominates, resultingin an abstractness rating of 0.24984 (highly concrete).681tive words that are used as paradigms of positive andnegative semantic orientation:Positive paradigm words: good, nice, excellent,positive, fortunate, correct, and superior.Negative paradigm words: bad, nasty, poor, nega-tive, unfortunate, wrong, and inferior.Likewise, here we calculate the abstractness ofa given word by comparing it to twenty abstractwords and twenty concrete words that are used asparadigms of abstractness and concreteness.Turney and Littman (2003) experimented withtwo measures of semantic similarity, pointwise mu-tual information (PMI) (Church and Hanks, 1989)and latent semantic analysis (LSA) (Landauer andDumais, 1997).
These measures take a pair of wordsas input and generate a numerical similarity rating asoutput.
The semantic orientation of a given word iscalculated as the sum of its similarity with the posi-tive paradigm words minus the sum of its similaritywith the negative paradigm words.
Likewise, herewe calculate the abstractness of a given word by thesum of its similarity with twenty abstract paradigmwords minus the sum of its similarity with twentyconcrete paradigm words.
We then use a linear nor-malization to map the calculated abstractness valueto range from 0 to 1.Our algorithm for calculating abstractness uses aform of LSA to measure semantic similarity.
This isdescribed in detail in Section 2.1.
Although Turneyand Littman (2003) manually selected their fourteenparadigm words, here we use a supervised learningalgorithm to choose our forty paradigm words, asexplained in Section 2.2.The MRC Psycholinguistic Database MachineUsable Dictionary (Coltheart, 1981) includes 4,295words rated with degrees of abstractness by humansubjects in psycholinguistic experiments.5 The rat-ings range from 158 (highly abstract) to 670 (highlyconcrete).
Table 1 gives some examples.We used half of the 4,295 MRC words to train oursupervised learning algorithm and the other half tovalidate the algorithm.
On the testing set, the algo-rithm attains a correlation of 0.81 with the dictionaryratings.
This indicates that the algorithm agrees wellwith human judgements of the degrees of abstract-ness of words.5Available at http://ota.oucs.ox.ac.uk/headers/1054.xml.Abstract Words Rating Concrete Words Ratingas 158 ape 654of 180 grasshopper 660apt 183 tomato 662however 186 milk 670Table 1: Examples of abstract and concrete words fromthe MRC Dictionary (Coltheart, 1981).2.1 Measuring Semantic SimilarityThe variation of LSA that we use here is similarto Rapp?s (2003) work.
We modeled our similaritymeasure on Rapp?s due to the high score of 92.5%that he achieved on a set of 80 multiple-choice syn-onym questions from the Test of English as a For-eign Language (TOEFL).
The core idea is to repre-sent words with vectors and calculate the similarityof two words by the cosine of the angle between thetwo corresponding vectors.
The values of the ele-ments in the vectors are derived from the frequenciesof the words in a large corpus of text.
This generalapproach is known as a Vector Space Model (VSM)of semantics (Salton et al, 1975).We began with a corpus of 5?1010 words (280 gi-gabytes of plain text) gathered from university web-sites by a webcrawler.6 We then indexed this cor-pus with the Wumpus search engine (Bu?ttcher andClarke, 2005).7 We selected our vocabulary from theterms (words and phrases) in the WordNet lexicon.8By querying Wumpus, we obtained the frequency ofeach WordNet term in our corpus.
We selected allWordNet terms with a frequency of 100 or more inour corpus.
This resulted in a set of 114,501 terms.Next we used Wumpus to search for up to 10,000phrases per term, where a phrase consists of thegiven term plus four words to the left of the term andfour words to the right of the term.
These phraseswere used to build a word?context frequency matrixF with 114,501 rows and 139,246 columns.
A rowvector in F corresponds to a term in WordNet andthe columns in F correspond to contexts (the wordsto the left and right of a given term in a given phrase)in which the term appeared.The columns in F are unigrams (single words)in WordNet with a frequency of 100 or more inthe corpus.
A given unigram is represented by two6Collected by Charles Clarke at the University of Waterloo.7Wumpus is available at http://www.wumpus-search.org/.8WordNet is available at http://wordnet.princeton.edu/.682columns, one marked left and one marked right.Suppose r is the term corresponding to the i-th rowin F and c is the term corresponding to the j-th col-umn in F. Let c be marked left.
Let fij be the cellin the i-th row and j-th column of F. The numericalvalue in the cell fij is the number of phrases foundby Wumpus in which the center term was r and cwas the unigram closest to r on the left side of r.That is, fij is the frequency with which r was foundin the context c in our corpus.A new matrix X, with the same number of rowsand columns as in F, was formed by calculatingthe Positive Pointwise Mutual Information (PPMI)of each cell in F (Turney and Pantel, 2010).
Thefunction of PPMI is to emphasize cells in whichthe frequency fij is statistically surprising, andhence particularly informative.
This matrix was thensmoothed with a truncated Singular Value Decom-position (SVD), which decomposes X into the prod-uct of three matrices Uk?kVTk .
Finally, the termswere represented by the matrix Uk?pk, which has114,501 rows (one for each term) and k columns(one for each latent contextual factor).
The semanticsimilarity of two terms is given by the cosine of thetwo corresponding rows in Uk?pk.
For more detail,see Turney and Pantel (2010).There are two parameters in Uk?pk that need tobe set.
The parameter k controls the number of la-tent factors and the parameter p adjusts the weightsof the factors, by raising the corresponding singu-lar values in ?pk to the power p. The parameter k iswell-known in the literature on LSA, but p is less fa-miliar.
The use of p was suggested by Caron (2001).Based on our past experience, we set k to 1000 andp to 0.5.
We did not explore any alternative settingsof these parameters for measuring abstractness.2.2 Measuring AbstractnessNow that we have Uk?pk, all we need in orderto measure abstractness is some paradigm words.We used the MRC Psycholinguistic Database Ma-chine Usable Dictionary (Coltheart, 1981) to guideour search for paradigm words.
We split the 4,295MRC words into 2,148 for training (searching forparadigm words) and 2,147 for testing (evaluationof the final set of paradigm words).
We beganwith an empty set of paradigm words and addedwords from the 114,501 rows of Uk?pk, one wordat a time, alternating between adding a word to theconcrete paradigm words and then adding a wordto the abstract paradigm words.
At each step, weadded the paradigm word that resulted in the high-est Pearson correlation with the ratings of the train-ing words.
This is a form of greedy forward searchwithout backtracking.
We stopped the search afterforty paradigm words were found, in order to pre-vent overfitting of the training data.Table 2 shows the forty paradigm words and theorder in which they were selected.
At each step, thecorrelation increases on the training set, but even-tually it must decrease on the testing set.
Afterforty steps, the training set Pearson correlation was0.8600.
At this point, we stopped the search forparadigm words and calculated the testing set Pear-son correlation, which was 0.8064.
This shows asmall amount of overfitting of the training data.
Thetesting set Spearman correlation was 0.8216.For another perspective on the performance of thealgorithm, we measured its accuracy on the testingset, by creating a binary classification task from thetesting data.
We calculated the median of the rat-ings of the 2,147 words in the test set.
Every wordwith an abstractness above the median was assignedto the class 1 and every word with an abstractnessbelow the median was assigned to the class 0.
Wethen used the algorithm to guess the rating of eachword in the test set, calculated the median guess, andlikewise assigned the guesses to classes 1 and 0.
Theguesses were 84.65% accurate.After generating the paradigm words with thetraining set and evaluating them with the testingset, we then used them to assign abstractness rat-ings to every term in the matrix.
The result of thisis that we now have a set of 114,501 terms (wordsand phrases) with abstractness ratings ranging from0 to 1.9 Based on the testing set performance, weestimate these 114,501 ratings would have a Pearsoncorrelation of 0.81 with human ratings and an accu-racy of 85% on binary (abstract or concrete) classi-fication.We chose to limit the search to forty paradigmwords based on our past experience with semanticorientation (Turney and Littman, 2003).
To validatethis choice, we allowed the algorithm to continue9The 114,501 rated terms are available from Peter Turney.683Concrete Paradigm Words Abstract Paradigm WordsOrder Word Correlation Order Word Correlation1 donut 0.4447 2 sense 0.61653 antlers 0.6582 4 indulgent 0.69735 aquarium 0.7150 6 bedevil 0.73837 nursemaid 0.7476 8 improbable 0.75909 pyrethrum 0.7658 10 purvey 0.776211 swallowwort 0.7815 12 pigheadedness 0.788413 strongbox 0.7920 14 ranging 0.797315 sixth-former 0.8009 16 quietus 0.806717 restharrow 0.8089 18 regularisation 0.812319 recorder 0.8148 20 creditably 0.818821 sawmill 0.8212 22 arcella 0.824823 vulval 0.8270 24 nonproductive 0.829925 tenrecidae 0.8316 26 couth 0.834027 hairpiece 0.8363 28 repulsion 0.840029 sturnus 0.8414 30 palsgrave 0.843831 gadiformes 0.8451 32 goof-proof 0.846933 cobbler 0.8481 34 meshuga 0.850335 bullet 0.8521 36 dillydally 0.853837 dioxin 0.8550 38 reliance 0.857039 usa 0.8585 40 lumbus 0.8600Table 2: The forty paradigm words and the Pearson correlation on the training set.searching until one hundred paradigm words werefound.
This resulted in a training set Pearson corre-lation of 0.8963, but the testing set correlation wasonly 0.8097, which shows a significant amount ofoverfitting of the training data.
Although the test-ing set correlation is slightly higher with one hun-dred paradigm words, we chose to base the follow-ing experiments on the forty paradigm words, be-cause the difference between 0.8064 and 0.8097 isnot significant, and the gap between the training andtesting correlation (0.8963 versus 0.8097) indicatesa problematic amount of overfitting.
Furthermore,the execution time of the algorithm increases as theparadigm set increases.We generated abstractness ratings for a large vo-cabulary of 114,501 words in order to maximize thevariety of text genres and the range of applicationsfor which our list of abstractness ratings would beuseful.
As a consequence of this large vocabulary,many of the words in Table 2 are rare and obscure;however, the measure of quality of the algorithm isthe correlation with the testing set (0.81), not thefamiliarity of the words in the table.
We includethe table here so that other researchers can exper-iment with these paragidm words.
The table maygive some insight into the internal functioning of thealgorithm, but the main output of the algorithm isthe list of 114,501 words with abstractness ratings,not the list of paradigm words in Table 2.3 Related WorkHere we discuss related work on metaphor and thenwork on measuring abstractness.
As far as we know,our approach is the first in computational linguis-tics to bring these two themes together, althoughthe connection is well-known in cognitive linguistics(Lakoff and Johnson, 1980) and cognitive psychol-ogy (Gentner et al, 2001).3.1 MetaphorThe most closely related work is Birke and Sarkar?s(2006) research on distinguishing literal and nonlit-eral usage of verbs.
A later paper (Birke and Sarkar,2007) provides more detail on their active learn-ing system, briefly mentioned in the earlier paper.Birke and Sarkar (2006; 2007) treat the problem asa classical word sense disambiguation task (Navigli,2009).
A model is learned for each verb indepen-684dently from the other verbs.
This approach cannothandle a new verb without additional training.Hashimoto and Kawahara (2009) discuss workon a similar problem, distinguishing idiomatic us-age from literal usage.
They also approach this asa classical word sense disambiguation task.
Idiomsare somewhat different from metaphors, in that themeaning of an idiom (e.g., kick the bucket) is oftendifficult to derive from the meanings of the compo-nent words, unlike most metaphors.Nissim and Markert (2003) use supervised learn-ing to distinguish metonymic usage from literal us-age.
They take a classical WSD approach, learn-ing a separate model for each target word.
As withBirke and Sarkar (2006; 2007) and Hashimoto andKawahara (2009), the core idea is to learn to clas-sify word usage from similarity of context.
Unlikethese approaches, our algorithm generalizes beyondthe specific semantic content of the context, payingattention only to the degrees of abstractness of thecontext.Martin (1992) presents a knowledge-based ap-proach to interpreting metaphors.
This approach re-quires complex hand-coded rules, which are specificto a given domain (e.g., interpreting metaphoricalquestions from computer users, such as, ?How canI kill a process?
?, in an online help system).
Theknowledge base cannot handle words that are nothand-coded in its rules and a new set of rules mustbe constructed for each new application domain.Dolan (1995) describes an algorithm for extract-ing metaphors from a dictionary.
Some suggestiveexamples are given, but the algorithm is not evalu-ated in any systematic way.Mason (2004) takes a corpus-based approach tometaphor.
His algorithm is based on a statisticalapproach to discovering the selectional restrictionsof verbs.
It then uses these restrictions to discovermetaphorical mappings, such as, ?Money flows likea liquid.?
Although the system can discover somemetaphorical mappings, it was not designed to dis-tinguish literal and metaphorical usages of words.3.2 AbstractnessChangizi (2008) uses the hypernym hierarchy inWordNet to calculate the abstractness of a word.A word near the top of the hierarchy is consid-ered abstract and a word near the bottom is con-sidered concrete.
It seems to us that the WordNethypernym hierarchy captures the general?specificcontinuum, which might not be the same as theabstract?concrete continuum.
It would be interest-ing to see how much correspondence there is be-tween Changizi?s measure of abstractness and theratings in the MRC Psycholinguistic Database Ma-chine Usable Dictionary (Coltheart, 1981).
Also,note that adjectives and adverbs are outside of Word-Net?s hypernym hierarchy, and thus cannot be ratedby Changizi?s algorithm.Xing et al (2010) also use WordNet, but in a dif-ferent way.
They define the concreteness of a wordsense (a WordNet synset) to be 1 if the given wordsense is a hyponym of physical entity in the Word-Net hypernym hierarchy; otherwise the concretenessis 0.
We believe that, although physical entities areconcrete, so are redness and walking, which are nothyponyms of physical entity.
The category physicalentity only partially captures concreteness.4 ExperimentsIn the following experiments, we use the abstract-ness ratings of Section 2.2 to generate features forsupervised machine learning.
The learning algo-rithm we apply is logistic regression (Le Cessie andVan Houwelingen, 1992), as implemented in Weka(Witten and Frank, 2005).10 In all experiments, weused the Weka parameter settings R = 0.2 (for ro-bust ridge regression) and M = ?1 (for unlimitediterations).4.1 AdjectivesFor this experiment, we selected five adjectives,dark, deep, hard, sweet, and warm.
For each ofthe five adjectives, we identified twenty word pairsin which the first word is the adjective and thesecond word is a noun.
These pairs were identi-fied through the Corpus of Contemporary AmericanEnglish (COCA)11 (Davies, 2009) by seeking thenouns that follow each adjective in the corpus andsorting the candidate adjective-noun pairs by fre-quency.
We required a minimum pointwise mutualinformation (PMI) of 3 between the adjective andthe noun.
In some of the pairs, the adjective was10Weka is available at http://www.cs.waikato.ac.nz/ml/weka/.11Available at http://www.americancorpus.org/.685used in a denotative (literal) sense (dark hair) and inothers it was used in a connotative (nonliteral) sense(dark humor).
Table 3 gives some examples.Adjective-Noun Pairs Noun Abstractnessdark glasses 0.26826dark chocolate 0.28211dark energy 0.66207dark mood 0.61858Table 3: Some examples of adjective-noun pairs and theabstractness rating of the noun.In this experiment, we used the abstractness rat-ing of the noun (the context) to predict whether theadjective (the target) was used in a metaphorical orliteral sense.
Table 3 supports this idea, but it is easyto find counterexamples.
Although dark mood ismetaphorical, bad mood is literal.
The difference isthat dark has an abstractness rating of 0.43356 (rel-atively concrete), whereas bad has an abstractnessrating of 0.63326 (relatively abstract).
Metaphor re-sults when a concrete word is imported into an ab-stract context (Lakoff and Johnson, 1980).
Ideally,we should be comparing the abstractness of the tar-get to the abstractness of the context.
However, inour data, the target words are mostly concrete; thuswe can focus on the context and ignore the target.We discuss this point further in Section 5.Five judges, undergraduate students in psychol-ogy, were asked to judge whether the use of the ad-jective is a denotation or a connotation.
The instruc-tions were as follows:Denotation is the most direct or specificmeaning of a word or expression whileconnotation is the meaning suggested bythe word that goes beyond its literal mean-ing.
For instance, the meaning of bitter isdenotative in bitter lemon and connotativein bitter relations.
In each of the followingpairs, you will be asked to judge whether(1) the meaning of the first word is denota-tive or connotative and (2) to what extentit is denotative or connotative on a scaleranging from 1 to 4.The judges were blind to the research hypothe-sis.
Each judge received a booklet with the itemsorganized by the groups of adjectives and presentedin a random order.
Overall, each subject was askedto evaluate one hundred pairs.
Interjudge reliabilitywas high, with Cronbach?s Alpha equal to 0.95.Our feature vectors for each pair contained onlyone element, the abstractness rating of the noun inthe pair.
We used logistic regression with ten-foldcross-validation to predict each judge?s denotativeand connotative labels.
The results are summarizedin Table 4.
On average, we were able to predict ajudge?s labels with 79% accuracy.Judge Accuracy Majority1 0.730 0.5902 0.810 0.5703 0.840 0.5604 0.790 0.5105 0.780 0.520Average 0.790 0.550Table 4: The accuracy of logistic regression at predictingthe labels of each judge.Table 4 also shows the size of the majority class(the most common label) for each judge.
For allof the judges, the accuracy was significantly greaterthan the size of the majority class (Fisher Exact test,95% confidence level).
The results support our hy-pothesis that the abstractness of the context is pre-dictive of whether an adjective is used in a literal ormetaphorical sense.4.2 Known VerbsFor this experiment, we used the TroFi (TropeFinder) Example Base of literal and nonliteral usagefor fifty verbs.12 To compare our results with Birkeand Sarkar?s (2006) results, we use the same subsetof twenty-five of the fifty verbs.
These twenty-fiveverbs appear in 1,965 sentences, manually labeledL (literal) or N (nonliteral), according to the senseof the target verb.
The verbs also appeared in somesentences labeled U (unannotated), but we ignoredthese sentences (although they could be useful forsemi-supervised learning).The label nonliteral is intended to be a broad cat-egory that includes metaphorical as a special case.Other types of nonliteral usage include idiomaticand metonymical, but it seems that most of the non-literal cases in TroFi are in fact metaphorical, and12Available at http://www.cs.sfu.ca/ anoop/students/jbirke/.686hence our hypothesis about the correlation of ab-stract context with metaphorical sense is appropriatefor classifying the TroFi sentences.Two examples of sentences from TroFi follow.Both contain the target verb absorb.
The first sen-tence is literal and the second is nonliteral.L: An Energy Department spokesman says the sul-fur dioxide might be simultaneously recover-able through the use of powdered limestone,which tends to absorb the sulfur.N: He said that MMWEC will have to absorb only$4 million in additional annual costs now paidby the Vermont utilities.To generate feature vectors for the sentences, wefirst applied the OpenNLP part-of-speech tagger tothe sentences.13 We then looked for each word inour list of 114,501 abstractness ratings (Section 2.2).If the word was not found in the list, we applied theMorpha morphological analyzer to identify the stemof the word (e.g., the stem of managing is manage)(Minnen et al, 2001).14 We then looked for the stemin our list.
If it was still not found, we skipped it.For each sentence, we created a vector with fivefeatures:1. the average abstractness ratings of all nouns,excluding proper nouns2.
the average abstractness ratings of all propernouns3.
the average abstractness ratings of all verbs, ex-cluding the target verb4.
the average abstractness ratings of all adjectives5.
the average abstractness ratings of all adverbsWhen there were no words for a given part ofspeech, we set the average to a default value of 0.5.Two examples of feature vectors follow, correspond-ing to the two TroFi sentences above.L: ?0.3873, 0.5397, 0.6375, 0.2641, 0.5835?N: ?0.6120, 0.3726, 0.6699, 0.5612, 0.5000?The intuition here is that the weight of each con-text word, in predicting the class of the target verb,may depend on the part of speech of the context13Available at http://incubator.apache.org/opennlp/.14Available at http://www.informatics.susx.ac.uk/research/groups/nlp/carroll/morph.html.word.
We leave it to the logistic regression algo-rithm to determine the appropriate weighting, basedon the training data.
(See Table 7 in the next sec-tion.
)Following Birke and Sarkar?s (2006) approach,we treated each group of sentences for a given targetverb as a separate learning problem.
For each verb,we used ten-fold cross-validation to learn and testlogistic regression models.
To measure the perfor-mance of the models, we used three different scores,macro-averaged accuracy and two forms of macro-averaged f-score.Birke and Sarkar (2006) explain their scoring asfollows:Literal recall is defined as (correct literalsin literal cluster / total correct literals).Literal precision is defined as (correct lit-erals in literal cluster / size of literal clus-ter).
If there are no literals, literal recallis 100%; literal precision is 100% if thereare no nonliterals in the literal cluster and0% otherwise.
The f-score is defined as(2 ?
precision ?
recall) / (precision + re-call).
Nonliteral precision and recall aredefined similarly.
Average precision is theaverage of literal and nonliteral precision;similarly for average recall.
For overallperformance, we take the f-score of aver-age precision and average recall.The overall score is a macro-average, in which eachverb has equal weight, regardless of how many sen-tences it appears in.Every verb in TroFi has at least one literal usageand one nonliteral usage, so there is no issue withthe definition of recall as 100% when there are no lit-erals or no nonliterals.
However, we believe that thedefinition of precision as 100% when no sentence isassigned to the literal or nonliteral cluster gives toohigh a score to the trivial algorithm of always guess-ing the majority class.
The minority class will thenalways have a precision of 100%.
Therefore we usea modified f-score in which the precision of a classis 0% if the algorithm never guesses that class.
Werefer to Birke and Sarkar?s (2006) score as f-score(0/0 = 1) and to our own score as f-score (0/0 = 0).Table 5 summarizes our results.
Concrete-Abstract refers to our own algorithm.
Birke-Sarkar687refers to the best result reported by Birke and Sarkar(2006), using a form of active learning.
MajorityClass is the simple strategy of always guessing themajority class.
Probability Matching is the strategyof randomly guessing each class with a probabilityequal to the size of the class.Algorithm Accuracy F-score F-score(0/0=0) (0/0=1)Concrete-Abstract 0.734 0.631 0.639Birke-Sarkar NA NA 0.649Majority Class 0.697 0.408 0.629Probability Matching 0.605 0.500 0.500Table 5: The performance with known verbs.We used a paired t-test to evaluate the statisticalsignificance of the results in Table 5.
The num-bers are in bold font when the performance of analgorithm is significantly below the performance ofConcrete-Abstract.
In no case is any score signifi-cantly above the performance of Concrete-Abstract,at the 95% confidence level.
NA indicates scores thatwere not calculated by Birke and Sarkar (2006).4.3 Unknown VerbsFor the final experiment, we again used the TroFiExample Base, but with a different experimentalsetup.
Instead of ten-fold cross-validation, we usedthe twenty-five verbs in Birke and Sarkar (2006) fortesting (we call these the old verbs) and the othertwenty-five verbs (the new verbs) for training.
Thetwenty-five old (testing) verbs appear in 1,965 sen-tences and the twenty-five new (training) verbs ap-pear in 1,772 sentences.
For this experiment, weno longer learn a separate logistic regression modelfor each verb.
All of the 1,772 training sentencesare used together to learn a single logistic regressionmodel, which is then evaluated on the testing sen-tences.Table 6 summarizes our results.
Since the testingset is exactly the same as in Section 4.2, we can com-pare the performance directly with the performancein the preceding section and with Birke and Sarkar?s(2006) results.Again, we used a paired t-test to evaluate the sta-tistical significance of the results in Table 6.
Thenumbers are in bold font when the performance ofan algorithm is significantly below the performanceAlgorithm Accuracy F-score F-score(0/0=0) (0/0=1)Concrete-Abstract 0.686 0.673 0.681Birke-Sarkar NA NA 0.649Majority Class 0.697 0.408 0.629Probability Matching 0.605 0.500 0.500Table 6: The performance with unknown verbs.of Concrete-Abstract.
In no case is any score signifi-cantly above the performance of Concrete-Abstract,at the 95% confidence level.Table 7 shows the coefficients in the logistic re-gression model that was learned on the training data.The items numbered from 1 to 5 are the five featuresdescribed in Section 4.2.
The sixth item is the con-stant term in the regression equation.
We see that theabstractness of the nouns (excluding proper nouns)has the largest weight in predicting whether the tar-get verb is in class N.Feature Coefficient1 AvgNounAbs 11.41172 AvgPropAbs 0.72503 AvgVerbAbs -0.55284 AvgAdjAbs 1.14785 AvgAdvAbs -0.20136 Intercept -5.9436Table 7: The logistic regression coefficients for class N.5 DiscussionIt is a strength of our approach that it can classifyverbs that it has never seen before, as we see in Sec-tion 4.3.
The feature vectors in all three experimentsare based only on the context; the target adjective orverb is not used in the vectors.
This avoids the needfor gathering training data on every verb or adjectivefor which we want to determine whether it is beingused metaphorically or literally, since the algorithmis not sensitive to the specific target word.On the other hand, the performance might im-prove if the target word were included in the fea-ture vectors.
If metaphor is a method for transfer-ring knowledge from concrete domains to abstractdomains, then it follows that highly abstract targetwords will tend to be used literally in most con-texts.
For instance, the highly abstract verb epito-mize (with an abstractness rating of 0.85861) is per-haps almost always used in a literal sense.
There-688fore it would seem that the abstractness rating of thetarget word could be a useful clue for determiningwhether the sense is literal or metaphorical.We experimented with including the abstractnessrating of the target word as a feature, but the im-pact on performance was not significant for eitherthe adjectives or the verbs.
We hypothesize that thismay be due to the relatively narrow range in the ab-stractness of the adjectives and verbs in our data.The abstractness ratings of the adjectives vary from0.43356 for dark to 0.56637 for hard.
The abstract-ness ratings of the fifty verbs range from 0.28756 forplant to 0.71628 for lend, but 80% of the verbs liein the range from 0.41879 for fly to 0.59912 for rest.It seems possible that the abstractness rating of thetarget word would be useful with a dataset in whichthe target?s abstractness varied substantially.In future work, we would like to gather data fortarget words with a wider range of abstractness.
Weexpect that such data would show some benefit to in-cluding information on the abstractness of the targetword in the feature vector.We also expect that a hybrid of classical wordsense disambiguation, such as Birke and Sarkar?s(2006) algorithm, with abstractness ratings wouldperform better than either approach alone.
Abstract-ness may provide a good rough estimate of whethera word usage is literal or metaphorical, but it seemslikely that knowledge of the specific target word inquestion will be required for a highly precise answer.This is another worthwhile topic for future research.Currently there is no algorithm that identifieswhat kind of concepts and relations are grafted fromthe source domain to the target domain by metaphor-ical inference.
The algorithm presented in this pa-per may be used within a constraints-based modelof metaphor (Neuman and Nave, 2009) to addressthis challenge.Recently there has been some interest in visual-ness, picturability, and imagability, the degree towhich a word is associated with visual imagery (De-schacht and Moens, 2007).
Although Xing et al(2010) use the term concreteness in their work, theirresearch is concerned with predicting the difficultyof queries for image retrieval.
It could be argued thatXing et al should be trying to capture imagability,not concreteness.The MRC Psycholinguistic Database (Coltheart,1981) includes words rated for imagability.
Our al-gorithm for rating the abstractness of words (Sec-tion 2) could easily be trained with the MRC imaga-bility ratings instead of the abstractness ratings.
Infuture work, it would be interesting to evaluateimagability ratings on the TroFi Example Base.
Itwould also be worthwhile to see whether our algo-rithm can be adapted for image retrieval (Xing et al,2010) and image annotation (Deschacht and Moens,2007).6 ConclusionMetaphor is ubiquitous, yet recognizing textualentailment is a challenge when words are usedmetaphorically.
An algorithm for distinguishingmetaphorical and literal senses of a word will facil-itate correct textual inference, which will improvethe many NLP applications that depend on textualinference.We have introduced a new algorithm for measur-ing the degree of abstractness of a word.
Inspired byresearch in cognitive linguistics (Lakoff and John-son, 1980), we hypothesize that the degree of ab-stractness of the context in which a given word ap-pears is predictive of whether the word is used ina metaphorical or literal sense.
This hypothesis issupported by three experiments.A strength of this approach to the problem of dis-tinguishing metaphorical and literal senses is thatit readily generalizes to new words, outside of thetraining data.
We do not claim that abstractness isa complete solution to the problem, but it may be avaluable component in any practical system for pro-cessing metaphorical text.AcknowledgmentsPart of the work of Yair Neuman, Dan Assaf, andYohai Cohen has been supported by a grant from theIsrael Ministry of Defense.
Thanks to the EMNLPreviewers for their helpful comments.ReferencesJulia Birke and Anoop Sarkar.
2006.
A clustering ap-proach for the nearly unsupervised recognition of non-literal language.
In Proceedings of the 11th Confer-ence of the European Chapter of the Association for689Computational Linguistics (EACL 2006), pages 329?336.Julia Birke and Anoop Sarkar.
2007.
Active learning forthe identification of nonliteral language.
In Proceed-ings of the Workshop on Computational Approaches toFigurative Language at HLT/NAACL-07, pages 21?28.Stefan Bu?ttcher and Charles Clarke.
2005.
Efficiency vs.effectiveness in terabyte-scale information retrieval.In Proceedings of the 14th Text REtrieval Conference(TREC 2005), Gaithersburg, MD.John Caron.
2001.
Experiments with LSA scor-ing: Optimal rank and basis.
In Proceedings ofthe SIAM Computational Information Retrieval Work-shop, pages 157?169, Raleigh, NC.Mark Changizi.
2008.
Economically organized hierar-chies in WordNet and the Oxford English Dictionary.Cognitive Systems Research, 9(3):214?228.Kenneth Church and Patrick Hanks.
1989.
Word associ-ation norms, mutual information, and lexicography.
InProceedings of the 27th Annual Conference of the As-sociation of Computational Linguistics, pages 76?83,Vancouver, British Columbia.Max Coltheart.
1981.
The MRC psycholinguisticdatabase.
Quarterly Journal of Experimental Psychol-ogy, 33A(4):497?505.Marcel Danesi.
2003.
Metaphorical ?networks?
and ver-bal communication: A semiotic perspective on humandiscourse.
Sign Systems Studies, 31:341?363.Mark Davies.
2009.
The 385+ million word Corpus ofContemporary American English (1990?2008+): De-sign, architecture, and linguistic insights.
Interna-tional Journal of Corpus Linguistics, 14(2):159?190.Koen Deschacht and Marie-Francine Moens.
2007.
Textanalysis for automatic image annotation.
In Proceed-ings of the 45th Annual Meeting of the Association ofComputational Linguistics, pages 1000?1007.William B. Dolan.
1995.
Metaphor as an emergent prop-erty of machine-readable dictionaries.
In Proceedingsof the AAAI 1995 Spring Symposium Series: Repre-sentation and Acquisition of Lexical Knowledge: Pol-ysemy, Ambiguity and Generativity, pages 27?32.Dedre Gentner, Brian F. Bowdle, Phillip Wolff, and Con-suelo Boronat.
2001.
Metaphor is like analogy.
InD.
Gentner, K. J. Holyoak, and B. N. Kokinov, editors,The analogical mind: Perspectives from Cognitive Sci-ence, pages 199?253.
MIT Press, Cambridge, MA.Chikara Hashimoto and Daisuke Kawahara.
2009.
Com-pilation of an idiom example database for supervisedidiom identification.
Language Resources and Evalu-ation, 43(4):355?384.George Lakoff and Mark Johnson.
1980.
Metaphors WeLive By.
University Of Chicago Press, Chicago, IL.Thomas K. Landauer and Susan T. Dumais.
1997.
A so-lution to Plato?s problem: The latent semantic analysistheory of the acquisition, induction, and representationof knowledge.
Psychological Review, 104(2):211?240.Saskia Le Cessie and J.C. Van Houwelingen.
1992.Ridge estimators in logistic regression.
Applied Statis-tics, 41(1):191?201.Alexis Madrigal.
2011.
Why are spy researchers build-ing a ?Metaphor Program??
The Atlantic, May 25.James H. Martin.
1992.
Computer understanding ofconventional metaphoric language.
Cognitive Science,16(2):233?270.Zachary Mason.
2004.
CorMet: A computational,corpus-based conventional metaphor extraction sys-tem.
Computational Linguistics, 30(1):23?44.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Natu-ral Language Engineering, 7(3):207?223.Roberto Navigli.
2009.
Word sense disambiguation: Asurvey.
ACM Computing Surveys, 41(2):1?69.Yair Neuman and Ophir Nave.
2009.
Metaphor-basedmeaning excavation.
Information Sciences, 179:2719?2728.Malvina Nissim and Katja Markert.
2003.
Syntacticfeatures and word similarity for supervised metonymyresolution.
In Proceedings of the 41st Annual Meet-ing of the Association for Computational Linguistics(ACL-03), pages 56?63, Sapporo, Japan.Reinhard Rapp.
2003.
Word sense discovery based onsense descriptor dissimilarity.
In Proceedings of theNinth Machine Translation Summit, pages 315?322.Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975.A vector space model for automatic indexing.
Com-munications of the ACM, 18(11):613?620.Peter D. Turney and Michael L. Littman.
2003.
Measur-ing praise and criticism: Inference of semantic orien-tation from association.
ACM Transactions on Infor-mation Systems, 21(4):315?346.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.Journal of Artificial Intelligence Research, 37:141?188.Ian H. Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques withJava Implementations.
Morgan Kaufmann, San Fran-cisco.Xing Xing, Yi Zhang, and Mei Han.
2010.
Query dif-ficulty prediction for contextual image retrieval.
InAdvances in Information Retrieval, volume 5993 ofLecture Notes in Computer Science, pages 581?585.Springer.690
