Evaluation and Improvementof Cross-Lingual Question Answering StrategiesAnne-Laure Ligozat and Brigitte Grau and Isabelle Robba and Anne VilnatLIMSI-CNRS91403 Orsay Cedex, Francefirstname.lastname@limsi.frAbstractThis article presents a bilingual questionanswering system, which is able to processquestions and documents both in Frenchand in English.
Two cross-lingual strate-gies are described and evaluated.
First, westudy the contribution of biterms trans-lation, and the influence of the comple-tion of the translation dictionaries.
Then,we propose a strategy for transferring thequestion analysis from one language to theother, and we study its influence on theperformance of our system.1 IntroductionWhen a question is asked in a certain languageon the Web, it can be interesting to look for theanswer to the question in documents written inother languages in order to increase the number ofdocuments returned.
The CLEF evaluation cam-paign for cross-language question answering sys-tems addresses this issue by encouraging the deve-lopment of such systems.The objective of question answering systemsis to return precise answers to natural-languagequestions, instead of the list of documents usuallyreturned by a search engine.
The opening to mul-tilingualism of question answering systems raisesissues both for the Information Retrieval and theInformation Extraction points of view.This article presents a cross-language questionanswering system able to treat questions and docu-ments either in French or in English.
Two differentstrategies for shifting language are evaluated, andseveral possibilities of evolution are presented.2 Presentation of our question answeringsystemOur bilingual question answering system hasparticipated in the CLEF 2005 evaluation cam-paign 1.
The CLEF QA task aims at evaluating dif-ferent question answering systems on a given setof questions, and a given corpus of documents, thequestions and the documents being either in thesame language (except English) or in two diffe-rents languages.
Last year, our system participatedin the French to English task, for which the ques-tions are in French and the documents to search inEnglish.This system is composed of several modulesthat are presented Figure 1.
The first module ana-lyses the questions, and tries to detect a few oftheir characteristics, that will enable us to find theanswers in the documents.
Then the collection isprocessed thanks to MG search engine 2.
The do-cuments returned are reindexed according to thepresence of the question terms, and more preci-sely to the number and type of these terms ; next,a module recognizes the named entities, and thesentences from the documents are weighted accor-ding to the information on the question.
Finally,different processes are applied depending on theexpected answer type, in order to extract answersfrom the sentences.3 Cross-language strategies for questionanswering systemsTwo main approaches are possible to deal withmultilingualism in question answering systems :1Multilingual Question Answering task at the Cross Lan-guage Evaluation Forum, http ://clef-qa.itc.it/2MG for Managing Gigabyteshttp ://www.cs.mu.oz.au/mg/EACL 2006 Workshop on Multilingual Question Answering - MLQA0623EnglishtermsFusionEnglishanswersEnglishquestionsCollectionFrenchquestionsSelectionNamed entity taggingAnswer extractionReindexing and rankingSentence weightingDocument processingAnswer extractionEnglishFocusAnswer typeSemantically linked wordsMain verbTermsSyntactic relationsQuestion analysistranslationanswers2 lists of ranked(a)(b)SearchengineFIG.
1 ?
Architecture of our cross-language question answering systemquestion translation and term-by-term translation.These approaches have been implemented andevaluated by many systems in the CLEF evalua-tions, which gives a wide state-of-the-art of thisdomain and of the possible cross-language strate-gies.The first approach consists in translating thewhole question into the target language, and thenprocessing the question analysis in this target lan-guage.
This approach is the most widely used, andhas for example been chosen by the following sys-tems : (Perret, 2004), (Jijkoun et al, 2004), (Neu-mann and Sacaleanu, 2005), (de Pablo-Sa?nchez etal., 2005), (Tanev et al, 2005).
Among these sys-tems, several have measured the performance lossbetween their monolingual and their bilingual sys-tems.
Thus, the English-French version of (Perret,2004) has a 11 % performance loss (in terms of ab-solute loss), dropping from 24.5% to 13.5% of cor-rect answers.
The English-Dutch version of (Jij-koun et al, 2004)?s system has an approximative10% performance loss of correct answers : the per-centage of correct answers drops from 45.5% to35%.
As for (de Pablo-Sa?nchez et al, 2005), theylose 6% of correct answers between their Spanishmonolingual system and their English-Spanish bi-lingual system.
(Hartrumpf, 2005) also conductedan experiment by translating the questions fromEnglish to German, and reports a drop from about50% of performance.For their cross-language system, (Neumann andSacaleanu, 2004) chose to use several machinetranslation tools, and to gather the different trans-lations into a ?bag of words?
that is used to ex-pand queries.
Synonyms are also added to the?bag of words?
and EuroWordNet 3 is used to3Multilingual database with wordnets for several Euro-disambiguate.
They lose quite few correct ans-wers between their German monolingual systemand their German-English bilingual system, withwhich they obtain respectively 25 and 23.5% ofcorrect answers.Translating the question raises two main pro-blems : syntactically incorrect questions may beproduced, and the resolution of translation am-biguities may be wrong.
Moreover, the unknownwords such as some proper names are not or in-correctly translated.
We will describe later severalpossibilities to deal with these problems, as wellas our own solution.Other systems such as (Sutcliffe et al, 2005) or(Tanev et al, 2004) use a term-by-term translation.In this approach, the question is analyzed in thesource language and then the information retur-ned by the question analysis is translated into thetarget language.
(Tanev et al, 2004), who partici-pated in the Bulgarian-English and Italian-Englishtasks in 2004, translate the question keywords byusing bilingual dictionaries and MultiWordNet 4.In order to limit the noise stemming from the dif-ferent translations and to have a better cohesion,they validate the translations in two large cor-pora, AQUAINT and TIPSTER.
This system got ascore of 22.5% of correct answers in the bilingualtask, and 28% in the monolingual task in 2004.
(Sutcliffe et al, 2005) combine two translationtools and a dictionary to translate phrases.
Even-tually, (Laurent et al, 2005) also translate wordsor idioms, by using English as a pivot language.The performance of this system is of 64% of cor-rect answers for the French monolingual task, andpean languages, http ://www.illc.uva.nl/EuroWordNet/4Multilingual lexical database in which the Italian Word-Net is strictly aligned with Princeton WordNet, http ://multi-wordnet.itc.itEACL 2006 Workshop on Multilingual Question Answering - MLQA062439.5% for the English-French bilingual task.4 Adopted approachIn order to deal with the conversion from Frenchto English in our system, two strategies are ap-plied in parallel.
They differ on what is translatedto treat the question asked in French.
The first sub-system called MUSQAT proceeds to the questionanalysis in French, and then translates the ques-tion terms extracted by this question analysis mo-dule, following the - - - arrows in Figure 1.
Thesecond sub-system makes use of a machine trans-lation tool (Reverso 5) to obtain translations of thequestions and then our English monolingual sys-tem called QALC is applied, following the ..-.. ar-rows in Figure 1 .
These strategies will be detailedlater in the article.If they represent the most common strategies forthis kind of task, an original feature of our systemis the implementation of both strategies, which en-ables us to merge the results obtained by followingthese strategies, in order to improve the global per-formance of our system.In Table 1, we present an analysis of the resultswe obtained for the CLEF evaluation campaign.We evaluate the results obtained at two differentpoints of the question-answering process, i.e.
af-ter the sentence selection (point (a) in Figure 1),and after the answer extraction (point (b) in Fi-gure 1).
At point (a), we count how many ques-tions (among the global evaluation set of 200 ques-tions) have an appropriate answer in the first fivesentences.
At point (b), we distinguish the answersthe analysis process labels as named entities (NE),from the others, since the corresponding answe-ring processes are different.
We also detail howmany answers are ranked first, or in the first fiveranks, as we take into account the first five ans-wers.As illustrated in Table 1, the two strategies fordealing with multilingualism give quite differentresults, which can be explained by each strategycharacteristics.MUSQAT proceeds to the question analysiswith French questions correctly expressed, andwhich analysis is therefore more reliable.
Yet, theterms translations are then obtained from everypossible translation of each term, and thus withouttaking account any context ; moreover, they de-pend on the quality of the dictionaries used, and5http ://www.reverso.net/MUSQAT Reverso+QALC% %(a) : Sentences first 5 41 46with an answer ranks(b) : Correct rank 1 18 14NE answersfirst 5 26 17ranks(b) : Correct rank 1 16 13other answersfirst 5 23 20ranks(b) : Total rank 1 17 13(NE + non NE)first 5 24 19ranksFinal result 19(fusion of both strategies)TAB.
1 ?
Performance of our system in CLEF2005introduce noise because of the erroneous transla-tions.In MUSQAT, we do not only translate mono-terms (i.e.
terms composed of single word) : thebiterms (composed of two words) of the Frenchquestions are also extracted by the question analy-sis.
Every sequence of two terms which are taggedas adjective/common noun or proper noun/propernoun... constitutes a biterm.
Each word of the bi-term is translated, and then the existence of thecorresponding biterm built in English is checkedin the corpus.
The biterms thus obtained are thenused by the further modules of the system.
Takingbiterms into account is useful since they providea minimal context to the words forming them, aswell for the translation as for the re-indexing andre-ranking of the documents (see Figure 1), as ex-plained in (Ferret et al, 2002).
Moreover, the pre-sence of the biterm translations in the corpus is akind of validation of the monoterms translations.As for translating the question, which is imple-mented by Reverso+QALC, it presents the advan-tage of giving a unique translation of the questionterms, which is quite reliable.
But the grammati-cality or realism of the question are not assured,and thus the question analysis, based on regularexpression patterns, can be disturbed.In this work, we tried to evaluate each strategyEACL 2006 Workshop on Multilingual Question Answering - MLQA0625and to bypass their drawbacks : on the one hand(Section 5), by examining how the biterm transla-tion in MUSQAT could be more reliable, and onthe other hand (Section 6) by improving the ques-tion analysis, by relying on the French questions,for QALC.5 Biterm translationThe translation of terms and biterms present inthe question is achieved using two dictionaries.The first of them, which was used last year forour participation to CLEF is Magic-Dic 6.
It is adictionary under GPL licence, which was retainedfor its capacity to evolve.
Indeed users can sub-mit new translations which are controlled beforebeing integrated.
Yet, it is quite incomplete.
Thisyear we used FreeDict as well (FreeDict is also un-der GPL licence), to fill in the gaps of Magic-Dic.FreeDict added 424 translations to the 690 termsalready obtained.
By mixing both sets of transla-tions we obtained 463 additional biterms, makinga total of 777 biterms.Nevertheless, whatever the quality and the sizeof the dictionaries are, the problem of biterm trans-lation remains the same : since biterms are not inthe dictionaries, the only way for us to get theirtranslation is to combine all the different termtranslations.
The main drawback of this approachis the generated noise, for none of the terms consti-tuting the biterm is disambiguated.
For example,three different translations are found for the bi-term Conseil de de?fense : defense council, defenseadvice and defense counsel ; but only the first ofthose should be finally retained by our system.To reduce this noise, an interesting possibility isto validate the obtained biterms by searching themor their variants in the complete collection of do-cuments.
(Grefenstette, 1999) reports a quite simi-lar experiment in the context of a machine trans-lation task : he uses the Web in order to order thepossible translations of noun phrases, and in par-ticular noun biterms.
Fastr (Jacquemin, 1996) isa parser which takes as input a corpus and a listof terms (multi or monoterms) and outputs the in-dexed corpus in which terms and their variants arerecognized.
Hence, Fastr is quite adequate for bi-terms validation : it tags all the biterms present inthe collection, whether in their original form or ina variant that can be semantic or syntactic.In order to validate the biterms, the complete6http ://magic-dic.homeunix.netcollection of the CLEF campaign (500 Mbyte) wasfirst tagged using the TreeTagger, then Fastr wasapplied.
The results are presented Table 2 : 39.5%of the 777 biterms were found in the collection, ina total of 63,404 occurrences.
Thus there is an ave-rage of 206 occurrences for each biterm.
If we donot take into account the biterm which is the mostrepresented (last year with 30,981 occurrences),this average falls to 105.
The 52 biterms which arefound in their original form only are most of thetime names of persons.
Lastly, biterms that are ne-ver found in their original form, are often consti-tuted of one term badly translated, for example thebiterm oil importation is not present in the collec-tion but its variant import of oil is found 28 times.Then, it may be interesting to replace these bitermsby the most represented of their variants.Whenever a biterm is thus validated (found inthe collection beyond a chosen threshold), thetranslation of its terms is itself validated, othertranslations being discarded.
Thus, biterm valida-tion enables us to validate monoterm translations.Then, the following step will be to evaluate howthis new set of terms and biterms improves the re-sults of MUSQAT.After CLEF 2005 evaluation, we had at our dis-posal the set of questions in their English originalversion (this set was provided by the organizers).We had also the English translation (far less cor-rect) provided by the automatic translator Reverso.As we can see it Table 3, for each set of ques-tions the number of terms and biterms is nearlythe same.
In the set of translations given by Re-verso, we manually examined how many bitermswere false and found that here again the figureswere close to those of the original version.
Thereare two main reasons for which a biterm may befalse :?
in two thirds of cases, the association itself isfalse : the two terms should not have been as-sociated ; it is the case for example of manycountry from the question How many coun-tries joined the international coalition to res-tore the democratic government in Haiti ?
7?
in one third of cases, one of the terms isnot translated or translated with an erroneousterm, like movement zapatiste coming fromthe question What carry the courtiers of themovement zapatiste in Mexico ?
87This sentence is an example of very good translation gi-ven by Reverso8This sentence is an example of bad translation given byEACL 2006 Workshop on Multilingual Question Answering - MLQA0626Total Number of biterms 777Number of biterms found in the collection 307 - 39.5%Number of biterms found in their original form only 52 - 17%Number of biterms found with semantic variations only 150 - 54%TAB.
2 ?
Magic-Dic and FreeDict biterms validated by FastrQuestions Questions Questionsin French translated in English in Englishby Reverso (original version)Terms 1180 1122 1163Biterms 272 204 261False Biterms 33 38 27Common Biterms - 106TAB.
3 ?
Biterms in the different sets of questionsHowever, we calculated that among the 204 bi-terms given by Reverso, 106 are also present in theoriginal set of questions in English.
Among the 98remaining biterms, 38 are false (for the reasons gi-ven above).
Then, there are 60 biterms which areneither erroneous nor present in the original ver-sion.
Some of them contain a term which has beentranslated using a different word, but that is never-theless correct ; yet, most of these 60 biterms havea different syntax from those constructed from theoriginal version, which is due to the syntax of thequestions translated by Reverso.This leads us to conclude that even if Reversoproduces syntactically erroneous questions, thevocabulary it chooses is most of the time adequate.Yet, it is still interesting to use also the bitermsconstructed from the dictionaries since they aremuch more numerous and provide variants of thebiterms returned by Reverso.6 Multilingual question analysisWe have developed for the evaluations a ques-tion analysis in both languages.
It is based on themorpho-syntactic tagging and the syntactic analy-sis of the questions.
Then different elements aredetected from both analyses : recognition of theexpected answer type, of the question category, ofthe temporal context...There are of course lexicons and patterns whichare specific to each language, but the core of themodule is independent from the language.
ThisReverso, which should have produced What do supporters ofthe Zapatistas in Mexico wear ?module was evaluated on corpora of similar ques-tions in French and in English, and its results onboth languages are quite close (around 90% of re-call and precision for the expected answer typefor example ; for more details, see (Ligozat et al,2006)).As presented above, our system relies on twodistinct strategies to answer to a cross-languagequestion :?
Either the question is analyzed in the ori-ginal language, and next translated term-by-term.
The question analysis is then more re-liable since it processes a grammatically cor-rect question ; yet, the translation of terms hasno context to rely on.?
Or the question is first translated into thetarget language before being analyzed.
Al-though this strategy improves the translation,its main inconvenient is that each translationerror has strong consequences on the ques-tion analysis.
We will now try to evaluate towhich extent the translation errors actuallyinfluence our question analysis and to find so-lutions to avoid minimize this influence in theReverso+QALC system.An error in the question translation can lead towrong terms or an incorrect English construction.Thus, the translation of the question ?Combien ya-t-il d?habitants en France ??
(?How many inhabi-tants are there in France ??)
is ?How much is thereof inhabitants in France ?
?.In order to evaluate our second strategy, Re-verso+QALC, using question translation and thena monolingual system, it is interesting to estimateEACL 2006 Workshop on Multilingual Question Answering - MLQA0627the influence of a such a coarse translation on theresults of our system.In order to avoid these translating problems, itis possible to adapt either the input or the out-put of the translating module.
(Ahn et al, 2004)present an example of a system processing pre-and post-corrections thanks to surface reformu-lation rules.
However, this type of correction ishighly dependent on the kind of questions to pro-cess, as well as on the errors of the translation toolthat is used.We suggest to use another kind of processing,which makes the most of the cross-lingual charac-ter of the task, in order to improve the analysis ofthe translated questions and to take into accountthe possibilities of errors in these questions.Our present system already takes into accountsome of the most frequent translation errors, byallowing the question analysis module to loosensome of its rules in case the question be transla-ted.
Thus, a definition question such as ?Qu?est-ce que l?UNITA ?
?, translated ?What UNITA ?
?by our translating tool, instead of ?What is theUNITA ?
?, will nevertheless be correctly analyzedby our rules : indeed, the pattern WhatGN will beconsidered as corresponding to a definition ques-tion, while on a non-translated question, only thepattern WhatBeGN will be allowed.In order to try and improve our processing ofapproximations in the translated questions, the so-lution we suggest here consists in making thequestion analysis in both the source and the targetlanguages, and in reporting the information (or atleast part of it) returned by the source analysis intothe target analysis.
This is possible first becauseour system treats both the languages in a parallelway, and second, some of the information retur-ned by the question analysis module use the sameterms in English and in French, like for examplethe question category or the expected Named En-tity type.More precisely, we propose, in the task withFrench questions and English documents, to ana-lyse the French questions, and their English trans-lations, and then to report the question categoryand the expected answer type of the French ques-tions into the English question analysis.
The in-formation found in the source language should bemore reliable since obtained on a real question.For example, for the question ?Combien decommunaute?s Di Mambro a-t-il cre?e ??
(?Howmany communities has Di Mambro created ??
),Reverso?s translation is ?How many Di Mambrocommunities has he create ??
which prevents thequestion analysis module to analyze it correctly.The French analysis is thus used, which providesthe question category combien (how many) and theexpected named entity type NUMBER.
This infor-mation is reported in the English analysis file.These characteristics of the question are used attwo different steps of the question answering pro-cess : when selecting the candidate sentences andwhen extracting the answers.
Improving their re-liability should then enable us to increase the num-ber of correct answers after these two steps.In order to test this strategy, we conducted anexperiment based on the CLEF 2005 FR-EN task,and the 200 corresponding French questions.
Welaunched the question answering system on threequestion files :?
The first question file (here called Englishfile) contained the original English questions(provided by the CLEF organizers).
This filewill be considered as a test file, since the re-sults of our system on this file represent thosethat would be reached without translation er-rors.?
The second file (called Translated file) contai-ned the translated questions analysis.?
The last file (called Improved file) containedthe same analysis, but for which the questioncategory and the expected answer type werereplaced by those of the French analysis.Then we searched for the number of correct ans-wers for each input question file after the sentenceselection and after the answer extraction.
The re-sults obtained by our system on each file are pre-sented on Figure 2, Figure 3 and Figure 4.
Thesefigures present the number of questions expectinga named entity answer, expecting another kind ofanswer, and the total number of questions, as wellas the results of our system on each type of ques-tion : the number of correct questions are given atthe first five ranks, and at the first rank, first for thesentences (?long answers?)
and then for the shortanswers.These results show that the information trans-fer from the source language to the target lan-guage significantly improves the system?s results ;the number of correct answers increases in everycase.
It increases from 34 on the translated ques-tions file to 36 on the improved file, and from 52EACL 2006 Workshop on Multilingual Question Answering - MLQA0628FIG.
2 ?
QALC?s results (i.e.
number of correctanswers) on the 200 questionsFIG.
3 ?
Results on the named entities questionsFIG.
4 ?
Results on the non named entities ques-tionsto 55 for the first 5 ranks.
These results are closerto those of the monolingual system, which returns41 correct answers at the first rank, and 59 on thefirst 5 ranks.It is interesting to see that the difference bet-ween the monolingual and the bilingual systemsis less noticeable after the sentence selection stepthan after the answer extraction step, which tendsto prove that the last step of our process is moresensitive to translation errors.
Moreover, this expe-riment shows that this step can be improved thanksto an information transfer between the source andthe target languages.
In order to extend this stra-tegy, we could also match each French questionterm to its English equivalent, in order to trans-late all the information given by the French analy-sis into English.
Thus, the question analysis errorswould be minimized.7 ConclusionThe originality of our cross-language questionanswering system is to use in parallel the twomost widely used strategies for shifting language,which enables us to benefit from the advantagesof each strategy.
Yet, each method presents draw-backs, that we tried to evaluate in this article, andto bypass.For the term-by-term translation, we make themost of the question biterms in order to restrict thepossible translation ambiguities.
By validating thebiterms in the document collection, we have im-proved the quality of both the biterms and the mo-noterms translations.
We hope this improvementwill lead to a better selection of the candidate sen-tences from the documents.For the question translation, we use the infor-mation deduced from the source language to avoidthe problems coming from a bad or approximativetranslation.
This strategy enables us to solve someof the problems coming from non-grammaticaltranslations ; matching each term of the Frenchquestion with its English equivalent would enableus to transfer all the information of the French ana-lysis.
But the disambiguation errors of the transla-tion remain.ReferencesKisuh Ahn, Beatrix Alex, Johan Bos, Tiphaine Del-mas, Jochen L. Leidner, and Matthew B. Smillie.2004.
Cross-lingual question answering with QED.EACL 2006 Workshop on Multilingual Question Answering - MLQA0629In Working Notes, CLEF Cross-Language Evalua-tion Forum, pages 335?342, Bath, UK.Ce?sar de Pablo-Sa?nchez, Ana Gonza?lez-Ledesma,Jose?
Luis Mart?
?nez-Ferna?ndez, Jose?
Maria Guirao,Paloma Martinez, and Antonio Moreno.
2005.MIRACLE?s 2005 approach to cross-lingual ques-tion answering.
In Working Notes, CLEF Cross-Language Evaluation Forum, Vienna, Austria.Olivier Ferret, Brigitte Grau, Martine Hurault-Plantet,Gabriel Illouz, Christian Jacquemin, Laura Mon-ceaux, Isabelle Robba, and Anne Vilnat.
2002.
HowNLP can improve question answering.
KnowledgeOrganization, 29(3-4).Gregory Grefenstette.
1999.
The world wide web asa resource for example-based machine translationtasks.
In ASLIB Conference on Translating and theComputer, volume 21, London, UK.Sven Hartrumpf.
2005.
University of Hagen atQA@CLEF 2005 : Extending knowledge and dee-pening linguistic processing for question answering.In Working Notes, CLEF Cross-Language Evalua-tion Forum, Vienna, Austria.Christian Jacquemin.
1996.
A symbolic and surgicalacquisition of terms through variation.
Connectio-nist, Statistical and Symbolic Approaches to Lear-ning for Natural Language Processing, pages 425?438.Valentin Jijkoun, Gilad Mishne, Maarten de Rijke, Ste-fan Schlobach, David Ahn, and Karin Muller.
2004.The University of Amsterdam at QA@CLEF2004.In Working Notes, CLEF Cross-Language Evalua-tion Forum, pages 321?325, Bath, UK.Dominique Laurent, Patrick Se?gue?la, and SophieNe`gre.
2005.
Cross lingual question answeringusing QRISTAL for CLEF 2005.
In Working Notes,CLEF Cross-Language Evaluation Forum, Vienna,Austria.Anne-Laure Ligozat, Brigitte Grau, Isabelle Robba,and Anne Vilnat.
2006.
L?extraction des re?ponsesdans un syste`me de question-re?ponse.
In TraitementAutomatique des Langues Naturelles (TALN 2006),Leuven, Belgium.Gu?nter Neumann and Bogdan Sacaleanu.
2004.Experiments on robust NL question interpretationand multi-layered doument annotation for a cross-language question / answering system.
In WorkingNotes, CLEF Cross-Language Evaluation Forum,pages 311?320, Bath, UK.Gu?nter Neumann and Bogdan Sacaleanu.
2005.
DF-KI?s LT-lab at the CLEF 2005 multiple languagequestion answering track.
In Working Notes, CLEFCross-Language Evaluation Forum, Vienna, Aus-tria.Laura Perret.
2004.
Question answering system for theFrench language.
In Working Notes, CLEF Cross-Language Evaluation Forum, pages 295?305, Bath,UK.Richard F.E.
Sutcliffe, Michael Mulcahy, Igal Gabbay,Aoife O?Gorman, Kieran White, and Darina Slat-tery.
2005.
Cross-language French-English ques-tion answering using the DLT system at CLEF 2005.In Working Notes, CLEF Cross-Language Evalua-tion Forum, Vienna, Austria.Hristo Tanev, Matteo Negri, Bernardo Magnini, andMilen Kouylekov.
2004.
The DIOGENE ques-tion answering system at CLEF-2004.
In WorkingNotes, CLEF Cross-Language Evaluation Forum,pages 325?333, Bath UK.Hristo Tanev, Milen Kouylekov, Bernardo Magnini,Matteo Negri, and Kiril Simov.
2005.
Exploitinglinguistic indices and syntactic structures for multi-lingual question answering : ITC-irst at CLEF 2005.In Working Notes, CLEF Cross-Language Evalua-tion Forum, Vienna, Austria.EACL 2006 Workshop on Multilingual Question Answering - MLQA0630
