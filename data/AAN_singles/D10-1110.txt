Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1129?1139,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsLearning Recurrent Event Queries for Web SearchRuiqiang Zhang and Yuki Konda and Anlei DongPranam Kolari and Yi Chang and ZhaohuiZhengYahoo!
Inc701 First Avenue, Sunnyvale, CA94089AbstractRecurrent event queries (REQ) constitute aspecial class of search queries occurring atregular, predictable time intervals.
The fresh-ness of documents ranked for such queries isgenerally of critical importance.
REQ forms asignificant volume, as much as 6% of querytraffic received by search engines.
In thiswork, we develop an improved REQ classi-fier that could provide significant improve-ments in addressing this problem.
We ana-lyze REQ queries, and develop novel featuresfrom multiple sources, and evaluate them us-ing machine learning techniques.
From histor-ical query logs, we develop features utilizingquery frequency, click information, and userintent dynamics within a search session.
Wealso develop temporal features by time seriesanalysis from query frequency.
Other gener-ated features include word matching with re-current event seed words and time sensitiv-ity of search result set.
We use Naive Bayes,SVM and decision tree based logistic regres-sion model to train REQ classifier.
The re-sults on test data show that our models outper-formed baseline approach significantly.
Ex-periments on a commercial Web search en-gine also show significant gains in overall rel-evance, and thus overall user experience.1 IntroductionREQ pertains to queries about events which oc-cur at regular, predictable time intervals, most oftenweekly, monthly, annually, bi-annually, etc.
Natu-rally, users issue REQ periodically.
REQ usually re-fer to:Organized public events such as festivals, confer-ences, expos, sports competitions, elections: winterolympics, boston marathon, the International OceanResearch Conference, oscar night.Public holidays and other noteworthy dates: labor day,date of Good Friday, Thanksgiving, black friday.Products with annual model releases, such as car models:ford explorer, prius.Lottery drawings: California lotto results.TV shows and programs which are currently running:American idol, Inside Edition.Cultural related activities: presidential election, tax re-turn, 1040 form.Our interest in studying REQ arises from the chal-lenge imposed on Web search ranking.
To illustratethis, we show an example in Fig.
1 that snapshotsthe real ranking results of the query, EMNLP, is-sued in 2010 when the authors composed this pa-per, on Google search engine.
It is obvious theranking is not satisfactory because the page aboutEMNLP2008 is on the first position in 2010.
Ide-ally, the page about EMNLP2010 on the 6th positionshould be on the first position even if users don?texplicitly issue the query, EMNLP 2010, becauseEMNLP is a REQ.
The query, ?EMNLP?, implic-itly, without a year qualifier, needs to be served themost recent pages about ?EMNLP?.A better search ranking result cannot be achievedif we do not categorize ?EMNLP?
as a REQ, andprovide special ranking treatment to such queries.Existing search engines adopt a fairly involved rank-ing algorithm to order Web search results by con-sidering many factors.
Time is an important fac-tor but not the most critical.
The page?s rank-ing score mostly depends on other features suchas tf-idf (Salton and McGill, 1983), BM25 (Jones1129Figure 1: A real problematic ranking result by Google fora REQ query, ?EMNLP?.
The EMNLP2010 page shouldbe on the 1st position.et al, 2000), anchor text, historical clicks, pager-ank (Brin and Page, 1998), and overall page qual-ity.
New pages about EMNLP2010 obtain less fa-vorable feature values than the pages of 2009 earlierin terms of anchor text, click or pagerank becausethey have existed for a shorter time and haven?t ac-cumulated sufficient popularity to make them standout.
Without special treatment, the new pages about?EMNLP2010?
will typically not be ranked appro-priately for the users.Typically, a recurrent event is associated with aroot, and spawns a large set of queries.
Oscar,for instance, is a recurrent event about the annualAcademy Award.
Based on this, queries like ?oscarbest actress?, ?oscar best dress?, ?oscar best movieaward?, are all recurrent event queries.
As such,REQ is a highly frequent category of query in Websearch.
By Web search query log analysis, we ob-serve that there about 5-6% queries of total queryvolume belongs to this category.In this work, we learn if a query is in the REQclass, by effectively combining multiple features.Our features are developed through analysis of his-torical query logs.
We discuss our approaches in de-tail in Section 3.
We then develop a REQ classi-fier where all the features are integrated by machinelearning models.
We use Naive Bayes, SVM and de-cision tree based logistic regression models.
Thesemodels are described in Section 4.
Our experimentsfor REQ classifier and Web search ranking are de-tailed in Section 5 and 6.2 Related WorkWe found our work were related to two other prob-lems: general query classification and time-sensitivequery classification.
For general query classifica-tion, the task is to assign a Web search query toone or more predefined categories based on its top-ics.
In the query classification contest in KDD-CUP 2005 (Li et al, 2005), seven categories and67 sub-categories were defined.
The winning so-lution (Shen et al, 2005) used multiple classifiersintegrated by ensemble method.
The difficulties forquery classification are from short queries, lack oflabeled data, and query sense ambiguity.
Most pop-ular studies use query log, web search results, unla-beled data to enrich query classification (Shen et al,2006; Beitzel et al, 2005), or use document classifi-cation to predict query classification (Broder et al, ).General query classification is also studied for queryintent detection by (Li et al, 2008).There are many prior works to study the time sen-sitivity issue in web search.
For example, Baeza-Yates et al (Baeza-Yates et al, 2002) studied the re-lation between the web dynamics, structure and pagequality, and demonstrated that PageRank is biasedagainst new pages.
In T-Rank Light and T-Rank al-gorithms (Berberich et al, 2005), both activity (i.e.,update rates) and freshness (i.e., timestamps of mostrecent updates) of pages and links are taken into ac-count for link analysis.
Cho et al (Cho et al, 2005)proposed a page quality ranking function in order toalleviate the problem of popularity-based ranking,and they used the derivatives of PageRank to fore-cast future PageRank values for new pages.
Pandeyet al (Pandey et al, 2005) studied the tradeoff be-tween new page exploration and high-quality pageexploitation, which is based on a ranking method torandomly promote some new pages so that they canaccumulate links quickly.More recently, Dong et al (Dong et al, 2010a)1130proposed a machine-learned framework to improveranking result freshness, in which novel features,modeling algorithms and editorial guideline are usedto deal with time sensitivities of queries and doc-uments.
In another work (Dong et al, 2010b), theyuse micro-blogging data (e.g., Twitter data) to detectfresh URLs.
Novel and effective features are alsoextracted for fresh URLs so that ranking recency inweb search is improved.Perhaps the most related work to this paper isthe query classification approach used in (Zhanget al, 2009) and (Metzler et al, 2009), in whichyear qualified queries (YQQs) are detected basedon heuristic rules.
For example, a query contain-ing a year stamp is an explicit YQQ; if the yearstamp is removed from this YQQ, the remaining partof this query is also a YQQ, which is called im-plicit YQQ.
Different ranking approaches were usedin (Zhang et al, 2009) and (Metzler et al, 2009)where (Zhang et al, 2009) boosted pages of the mostlatest year while (Metzler et al, 2009) promotedpages of the most influential years.
Similarly, Nuneset al (Nunes, 2007) applied information extractiontechniques to identify temporal expression in websearch queries, and found 1.5% of queries contain-ing temporal expression.Dong et al (Dong et al, 2010a) proposed abreaking-news query classifier with high accuracyand reasonable coverage, which works not by mod-eling each individual topic and tracking it over time,but by modeling each discrete time slot, and compar-ing the models representing different time slots.
Thebuzziness of a query is computed as the languagemodel likelihood difference between different timeslots.
In this approach, both query log and newscontents are exploited to compute language modellikelihood.Diaz (Diaz, 2009) determined the newsworthinessof a query by predicting the probability of a userclicks on the news display of a query.
In this frame-work, the data sources of both query log and newscorpus are leveraged to compute contextual features.Furthermore, the online click feedback also plays acritical role for future click prediction.Konig et al (Knig et al, 2009) estimated theclick-through rate for dedicated news search resultwith a supervised model, which is to satisfy therequirement of adapting quickly to emerging newsevent.
Some additional corpora such as blog crawland Wikipedia is used for buzziness inference.
Com-pared with (Diaz, 2009), different feature and learn-ing algorithms are used.Elsas et al (Elsas and Dumais, 2010) studiedimproving relevance ranking by detecting documentcontent change to leverage temporal information.3 Feature GenerationTo better understand our work, we first introducethree terms.
We subdivide all raw queries in querylog into three categories: Explicit Timestamp, Im-plicit Timestamp, and No Timestamp.
An ExplicitTimestamp query contains at least one token being atime indicator.
For example, emnlp 2010, 2007 De-cember holiday calendar, amsterdam weather sum-mer 2009, Google Q1 reports 2010.
These queriesare considered to conatin time indicators, becausewe can regard {2010, 2007, 2009} as year indica-tor, december as month indicator, {summer, Q1(firstquarter)} as seasonal indicator.
To simplify ourwork, we only consider the year indicators, 2010,2007, 2009.
Such year indicators are also the mostimportant and most popular indicators, as noted in(Zhang et al, 2009).
Any query containing at leastone year indicator is an Explicit Timestamp query.Due to word sense ambiguity, some queries labeledas Explicit Timestamp by this method may have noconnection with time such as Windows Office 2007,2010 Sunset Boulevard, or call number 2008.
In thiswork, we tolerate this type of error because wordsense disambiguation is a peripheral problem for thistask.Implicit Timestamp queries are resulted by re-moving all year indicators from the correspondingExplicit Timestamp queries.
For example, the Im-plicit Timestamp query of emnlp 2010 is emnlp.All other queries are No Timestamp queries becausethey have never been found together with a year in-dicator.Classifying queries into the above three cate-gories depends on the used query log.
A searchengine company partner provided us a query logfrom 08/01/2009 to 02/29/2010 for this research.We found the proportions of the three categoriesin this query log are 13.8% (Explicit), 17.1% (Im-plicit) and 69.1% (No Timestamp).
These numbers1131could be slightly different depending on the sourceof query logs.
Note that 17.1% of Implicit Times-tamp queries in the query log is a significant num-ber.
However, not all Implicit Timestamp queriesare REQ.
Many Implicit Timestamp queries have notime sense.
They belong to Implicit Timestamp justbecause users issued the query with a year indica-tor through varied intents.
For example, ?google?
isfound to be an Implicit Timestamp query since therewere many ?google 2008?
or ?google 2009?
in thequery log.The next few sections introduce our work in rec-ognizing recurrent event time sense for ImplicitTimestamp queries.
We first focus on features.There are many features that were exploited in REQclassifier.
We extract these features from query log,query session log, click log, search results, time se-ries and NLP morphological analysis.3.1 Query log analysisThe following features are extracted from query loganalysis:QueryDailyFrequency: the total counts of thequery divided by the number of the days in the pe-riod.ExplicitQueryRatio: Ratio of number of countsquery was issued with year and number of countsquery was issued with or without year.
This featureis the method used by (Zhang et al, 2009).UniqExplicitQueryCount: Number of uniq Ex-plicit Timestamp queries associated with query.
Forexample, if a query was issued with query+2009 andquery+2008, this feature?s value is two.ChiSquareYearDist: this feature is the distance be-tween two distributions: one is frequency distribu-tion over years for all REQ queries.
The other isthat for single REQ query.
It is calculated throughfollowing steps: (a) Aggregate the frequencies forall queries for all years.
Suppose we observe allyears from 2001 to 2010.
So we can get vector,E = ( a f10sum1 ,a f09sum1 , ...,a f01sum1 ) where a fi is the frequencysum of year 20i for all REQ queries.
sum1 =a f10 + a f09 + ... + a f01, the sum of all year fre-quency.
(b) Given a query, suppose we observethis query?s yearly frequency distribution is , Oq =(q f10, q f09, , ..., q f01).
q fi is this query?s frequencyfor the year 20i.
Pad the slot with zeros if no fre-quency found.
The expected distribution for thisquery is, Eq = ( sum2?a f10sum1 ,sum2?a f09sum1 , ...,sum2?a f01sum1 ),where sum2 = q f10 + q f09 + ... + q f01 is sum ofall year frequency for the query.
(d) Calculate CHI-squared value to represent the different yearly fre-quency distribution between Eq and Oq according to?2 =?Ni=1(Oqi ?Eqi )2Eqi.
Using CHI square distance as amethod is widely used for statistical hypothesis test.We found it to be a useful feature for REQ classifier.3.2 Query reformulationIf users cannot find the newest page by issuing Im-plicit Timestamp query, they may re-issue the queryusing an Explicit Timestamp query.
We can detectthis change in a search session (a 30 minutes periodfor each query).
By finding this kind of behaviorfrom users, we next extract three features.UserSwitch: Number of unique users that switchedfrom Implicit Timestamp queries to Explicit Times-tamp queries.YearSwitch: Number of unique year-like tokensswitched by users in a query session.NormalizedUserSwitch: Feature UserSwitch di-vided by QueryDailyFrequency.3.3 Click log analysisIf a query is time sensitive, users may click apage that displays the year indicator on title orurl.
An example that shows year indicator onurl is www.lsi.upc.edu/events/emnlp2010/call.html.Search engine click log saves all users?
click infor-mation.
We used click log to derive the followingfeatures.YearUrlTop5CTR: Aggregated click through rate(CTR) of all top five URLs containing a year in-dicator.
CTR of an URL is defined as the numberof clicks of an URL divided by the number of pageviews.YearUrlFPCTR: Aggregated click through rate(CTR) of all first page URLs containing a year in-dicator.3.4 Search engine result setFor each Implicited Timestamp query, we can scrapethe search engine to get search results.
We count thenumber of titles and urls that contain year indicator.We use this number as a feature, and generate 6 fea-tures.1132TitleYearTop5: the number of titles containing ayear indication on the top 5 results.
This value is4 in Fig.
1.TitleYearTop10: the number of titles containing ayear indication on the top 10 results.
This value is 6in Fig.
1.TitleYearTop30: the number of titles containing ayear indication on the top 30 results.UrlYearTop5: the number of urls containing a yearindication on the top 5 results.
This value is 1 inFig.
1.UrlYearTop10: the number of urls containing a yearindication on the top 10 results.UrlYearTop30: the number of titles containing ayear indication on the top 30 results.3.5 Time series analysisRecurrent event query has periodic occurrence pat-tern in time series.
Top graph of Figure 2 shows thefrequency change of the query, ?Oscar?.
The annualevent usually starts from Oscar nomination as ear-lier as last year December to award announcementof February this year.
So a small spike and a bigspike are observed in the graph to indicate nomina-tion period and ceremony period.
There are a periodof silence between the two periods.
The frequencypattern keeps unchanged each year.
We show threeyears (2007,8,9) in the graph.
By making use of re-current event queries?
periodic properties, we calcu-lated the query period as a new feature.We use autocorrelation to calculate the period.R(?)
=?N?
?t=1 (xt ?
?)(xt+?
?
?){?N?
?t=1 (xt ?
?)2(xt+?
?
?
)2}1/2where x(t) is query daily frequency.
N is the num-ber of days used for this query.
We can get maxi-mum of 3 years data for some queries but only a fewmonths for others.
R(?)
is autocorrelation function.Peaks (the local biggest R(?)
given a time window)can be detected from R(?)
plot.
The period T is cal-culated as the duration between two neighbor peaks.T = 365 for the query, ?Oscar?.
The bottom graphof Fig.
2 shows the autocorrelation function plot forthe query Oscar.3.6 Recurrent event seed word listMany recurrent event queries share some commonwords that have recurrent time sense.
We list mostnew results top schedulefootball festival movie worldshow day best taxresult calendar honda forddownload exam nfl missawards toyota tour saleamerican fair list pictureselection game basketball cupTable 1: Top recurrent event seed wordsfrequently used recurrent seeds in Table 1.
Thoseseeds are likely combined with other words to formnew recurrent event queries.
For example, the seed,?new?, can be used by queries ?new bmw cars?,?whitney houston new songs?, ?apple new iphone?,or ?hairstyle new?.To generate the seed list, we tokenized all thequeries from Implicit Timestamp queries and splitall the tokens.
We then sort and unique all the to-kens, and submit top tokens to professional editorswho are asked to pick 8,000 seeds from the top fre-quent tokens.
Some top tokens were removed if theyare not qualified to form recurrent event queries.
Theeditors took about four days to do the judgment ac-cording to the token?s time sense and examples ofrecurrent event queries.
However, this is a one-timeeffort.
A token will be in the seed if there are manyrecurrent event examples formed by this token, byeditors?
judgment.Table 1 shows 32 top seeds.
Some seeds connectwith time such as, ?new, schedule, day, best, calen-dar?
; some relate to sports, ?football, game, nfl, tour,basketball, cup?
; some about cars, ?honda, ford, toy-ota?.
The reason why ?miss?
is in the seeds is thatthere are many annual events about beauty contestsuch as ?miss america, miss california, miss korea?.We use the seed list to generate the followingthree features:AveNumberTokensSeeds: number of tokens that isin the seed list divided by number of tokens in thequery.AveNumberTokensNotSeeds: number of tokensthat is not in the seed list divided by number of to-kens in the query.DiffNumberTokensSeeds: The difference of theabove two values.1133-0 .200.20.40.60.811.21.41.61 15 29 43 57 71 85 99 113 127 141 155 169 183 197 211 225 239 253 267 281 295 309 323 337 351 365 379 393 407 421 435 449 463 477 491 505 519 533 547 561 575 589 603 617 631 645 659 673 687 701 715 729 743 757 771 785 799 813 827 841 855 869 883 897Figure 2: Frequency waveform(top) and corresponding autocorrelation curve (bottom) for query Oscar.4 Learning Approach for REQThe REQ classification is a typical machine learn-ing task.
Given M observed samples used for train-ing data, {(x0, y0), (x1, y1), ?
?
?
, (xM, yM)} where xi isa feature vector we developed in last section for agiven query.
yi is the observation value, {+1,?1},indicating the class of REQ and non-REQ.
The taskis to find the class probability given an unknown fea-ture vector, x?, that is,p(y = c|x?
), c = +1,?1.
(1)There are a lot of machine learning methods ap-plicable to implement Eq.
1.
In this work, weadopted three representative methods.The first method is Naive Bayes method.
Thismethod treats features independent.
If x is enx-tended into feature vector, x = {x0, x1, ?
?
?
, xN} then,p(y = c|x) = 1Zp(c)i=N?i=0p(xi|c)The second method is SVM.
In this work we usedthe tool for our experiments, LIBSVM (Chang andLin, 2001).
Because SVM is a well known approachand widely used in many classification task, we skipto describe how to use this tool.
Readers can turn tothe reference for more details.The third method is based on decision tree basedlogistic regression model.
The probability is givenby the formula below,p(y = c|x) = 11 + e?
f (x)(2)We employ Gradient Boosted Decision Tree algo-rithm (Friedman, 2001) to learn the function f (X).Gradient Boosted Decision Tree is an additive re-gression algorithm consisting of an ensemble oftrees, fitted to current residuals, gradients of the lossfunction, in a forward step-wise manner.
It itera-tively fits an additive model asft(x) = Tt(x;?)
+ ?T?t=1?tTt(x;?t)such that certain loss function L(yi, fT (x + i)) isminimized, where Tt(x;?t) is a tree at iteration t,weighted by parameter ?t, with a finite number ofparameters, ?t and ?
is the learning rate.
At iterationt, tree Tt(x; ?)
is induced to fit the negative gradientby least squares.The optimal weights of trees ?t are determined by?t = argmin?N?iL(yi, ft?1(xi) + ?T (xi, ?
))Each node in the trees represents a split on a fea-ture.
The tuneable parameters in such a machine-learnt model include the number of leaf nodes ineach tree, the relative contribution of score fromeach tree called the shrinkage, and total number ofshallow decision trees.The relative importance of a feature S i, in suchforests of decision trees, is aggregated over all the1134m shallow decision trees (Breiman et al, 1984) asfollows:S 2i =1MM?m=1L?1?n=1wl ?
wrwl + wr(ylyr)2I(vt = i) (3)where vt is the feature on which a split occurs, yland yr are the mean regression responses from theright, and left sub-tree, and wl and wr are the corre-sponding weights to the means, as measured by thenumber of training examples traversing the left andright sub-trees.5 REQ Learner EvaluationWe collected 6,000 queries labeled as either Recur-rent or Non-recurrent by professional human edi-tors.
The 6,000 queries were sampled from ImplicitTimestamp queries according to frequency distribu-tion to be representative.
We split the queries into5,000 for training and 1,000 for test.
For each query,we calculated features?
values as described in Sec-tion 3.The Naive Bayes method used single Gaussianfunction for each independent feature.
Mean andvariance were calculated from the training data.As for LIBSVM, we used C-SVC, linear functionas kernel and 1.0 of shrinkage.The parameters used in the regression model were20 of trees, 20 of nodes and 0.8 of learning rate(shrinkage).The test results are shown in Fig.
3, recall-precision curve.
We set a series of threshold to theprobability of c = +1 calculated by Eq.
1 so thatwe can get the point values of recall and precision inFig.
3.
For example, if we set a threshold of 0.6, aquery with a probability larger than 0.6 is classifiedas REQ.
Otherwise, it is non-REQ.
The precisionis a measure of correctly classified REQ queries di-vided by all classified REQ queries.
The recall is ameasure of correctly classified REQ queries dividedby all REQ queries in test data.In addition to the three plots, we also show theresults using only one feature, ExplicitQueryRatio,for comparison with the classification method usedby (Zhang et al, 2009).All the three models us-ing all features performed better than the existingmethod using ExplicitQueryRatio.
The highest im-provement was achieved by GBDT regression tree0.50.550.60.650.70.750.80.850.90.950.4  0.5  0.6  0.7  0.8  0.9  1PrecisionRecallExplicitQueryRatioGBDTreeNaive BayesSVMFigure 3: Comparison of precision and recall rate be-tween our method and the existing method.model.
The results of Naive Bayes were lower thanSVM and GBDTree.
This model is weaker becauseit treats features independently.
Typically SVMs andGBDT gives comparable results on a large class ofproblems.
Since for this task we use features fromdifferent sources, the feature values are designed tohave larger dynamic range, which is better handledby GBDT.The features?
importance ranked by Equation 3is shown in Table 2.
We list the top 10 features.The No.1 important feature is ExplicitQueryRatio.The second and seventh features are from search ses-sion analysis by counting users who changed queriesfrom Implicit Timestamp to Explicit Timestamp.This is a strong source of features.
The time se-ries analysis feature is ranked No.3.
Calculation ofthis feature needs two years query log to be muchmore effective, but we didn?t get so large data formany queries.
One of the features from recurrentevent seed list is ranked No.4.
This is also an impor-tant feature source.
The ChiSquareYearDist featureis ranked 5th, that proves the recurrent event queryfrequency has a statistical distribution pattern overyears.
TitleYearTop30 and TitleYearTop10 that arederived from scraping results are ranked the 9th and10th important.Fig.
4 shows the distribution of feature values for1135Feature Rank ScoreExplicitQueryRatio 1 100NormalizedUserSwitch 2 71.7AutoCorrelation 3 54.0AveNumberTokenSeeds 4 48.7ChiSquareYearDist 5 36.3YearUrlFPCTR 6 19.1UserSwitch 7 11.7QueryDailyFreq 8 10.7TitleYearTop30 9 10.6TitleYearTop10 10 5.8Table 2: Top 10 most important features: rank and im-portance score (100 is maximum)12345678910Figure 4: Feature value distribution of all data(blue=REQ, red=non-REQ)each sample of the 6,000 data, where each point rep-resents a query and each line represents a feature?svalue for all queries.
One point is a query.
The fea-tures are ordered according to feature importance ofTable 2.
The ?blue?
points indicate REQ queries andthe ?red?
points, non-REQ queries.
Some featuresare continuous like the 1st and 2nd.
Some featurevalues are discrete like the last two indicating Ti-tleYearTop30 and TitleYearTop10.
There are ?red?samples in the 4th feature but overlapped with andcovered by ?blue?
samples visually.In the Table 3, we show F-Measure values as wegradually added features from the feature, Explicit-QueryRatio, according to feature importance in Ta-ble 2.
We listed the F-Measure values under threethreshold, 0.6, 0.7 and 0.8.
Higher threshold will in-crease classifier precision rate but reduce recall rate.F-Measure is a metric combining precision rate andrecall rate.
It is clearly observed that the classifierperformance is improved as more features are used.ThresholdFeature 0.6 0.7 0.8ExplicitQueryRatio 0.833 0.833 0.752+NormalizedUserSwitch 0.840 0.837 0.791+AutoCorrelation 0.850 0.839 0.823+AveNumberTokenSeeds 0.857 0.854 0.834+ChiSquareYearDist 0.857 0.864 0.839+YearUrlFPCTR 0.869 0.867 0.837+UserSwitch 0.862 0.862 0.846+QueryDailyFreq 0.860 0.852 0.847+TitleYearTop30 0.854 0.853 0.843+TitleYearTop10 0.858 0.861 0.852+All 0.876 0.867 0.862Table 3: F-Measures as varying thresholds by adding topfeatures.Query Probabilityncaa men?s basketball tournament 0.999bmw 328i sedan reviews 0.999new apple iphone release 0.932sigir 0.920new york weather in april 0.717academy awards reviews 0.404google ipo 0.120adidas jp 0.082Table 4: Probabilities of example queries by GBDT treeclassifierSome query examples, and their scores from ourmodel are listed in Table 4.
The last two exam-ples, google ipo and adidas jp, have very low values,and are not REQs.
The first four queries are typicalREQs.
They have higher values of features Explicit-QueryRatio,Normalized UserSwitch and YearUrlF-PCTR.
Although both new apple iphone release re-views and academy awards reviews are about re-views, academy awards reviews has lower valueof NormalizedUserSwitch and ChiSquareYearDistcould be the reason for a lower score.6 Web Search RankingIn this section, we use the approach proposedby (Zhang et al, 2009) to test the REQ classifierfor Web search ranking.
In their approach, searchranking is altered by boosting pages with most re-cent year if the query is a REQ.
The year indicator1136DCG@5 DCG@1bucket #(query) Organic Our?s % over Organic Organic Ours % over Organic[0.0,0.1] 59 6.87 6.96 1.48(-2.3) 4.08 4.19 2.69(-1.07)[0.1,0.2] 76 5.86 6.01 2.52(0.98) 2.88 2.91 1.14(1.69)[0.2,0.3] 85 6.33 6.41 1.24(2.12) 3.7 3.7 0.0(0.8)[0.3,0.4] 75 5.18 5.24 1.18(-0.7) 2.92 2.95 1.14(1.37)[0.4,0.5] 78 4.96 4.82 -2.84(-1.35) 2.5 2.42 -3.06(0)[0.5,0.6] 84 5.4 5.37 -0.45(-0.3) 2.82 2.85 1.05(-1.5)[0.6,0.7] 78 4.78) 5.19) 8.42(3.64) 2.56 2.83 10.75(4.1)[0.7,0.8] 80 4.45 4.60 3.41(3.19) 2.21 2.26 1.98(2.8)[0.8,0.9] 78 4.81 4.96 3.15(4.79) 2.32 2.33 0.55(0.65)[0.9,1.0] 107 5.08 5.50 8.41*(4.41) 2.64 3.09 16.78*(1.36)[0.0,1.0] 800 5.33 5.47 2.74*(2.17) 2.83 2.93 3.6*(1.26)Table 5: REQ learner improves search engine organic results.
The numbers in the brackets are by Zhang?s methods.Direct comparison with Zhang?s method is valid only in the last line, using all queries.
A sign ???
indicates statisticalsignificance (p-value<0.05)can be detected either from title or URL of the re-sult.
For clarity, we re-write their ranking functionas below,F(q, d) = R(q, d) + [e(do, dn) + k]e??
(q)where the ranking function, F(q, d), consists oftwo parts: the base function R(q, d) plus boosting.If the query q is not a REQ, boosting is set to zero.Otherwise, boosting is decided by e(do, dn), k, ?
and?(q).
e(do, dn) is the difference of base ranking scorebetween the oldest page and the newest page.
If thenewest page has a lower ranking score than the old-est page, then the difference is added to the newestpage to promote the ranking of the newest page.?
(q) is the confidence score of a REQ query.
It isthe value of Eq.
1. ?
and k are two empirical param-eters.
(Zhang et al, 2009)?s work has experimentedthe effects of using different value of ?
and k (?
= 0equals to no discounts for ranking adjustment).
Weused ?
= 0.4 and k = 0.3 which were the best con-figuration in (Zhang et al, 2009).For evaluating our methods, we randomly ex-tracted 800 queries from the Implicit Timestampqueries.
We scraped a commercial search engine us-ing the 800 queries.
We extracted the top five searchresults for each query under three configures: or-ganic search engine results, (Zhang et al, 2009)?smethod and ours using REQ classifier.
We askedhuman editors to judge all the scraped (query, url)pairs.
Editors assign five grades according to rel-evance between query and articles: Perfect, Excel-lent, Good, Fair, and Bad.
For example, a ?Perfect?grade means the content of the url match exactly thequery intent.We use Discounted Cumulative Gain(DCG) (Jarvelin and Kekalainen, 2002) at rank k asour primary evaluation metrics to measure retrievalperformance.
DCG is defined as,DCG@k =k?i=12r(i) ?
1log2(1 + i)where r(i) ?
{0 .
.
.
4} is the relevance grade of the ithranked document.The Web search ranking results are shown in Ta-ble 5.
We used GBDT tree learning methods be-cause it achieved the best results.
We divided 800test queries into 10 buckets according to the classi-fier probability.
The bucket, [0.0,0.1], contains thequery with a classifier probability greater than 0 butless than 0.1.
Our results are compared with organicsearch results, but we also show the improvementsover search organic by (Zhang et al, 2009) in thebrackets.
Because Zhang?s approach output differ-ent classifier values from Ours for the same query,buckets of the same range in the Table contain dif-ferent queries.
Hence, it is inappropriate to compare1137Zhang?s with Ours for the same buckets except thelast row where we used all the queries.Our classifier?s overall performance is much bet-ter than the organic search results.
We achieved2.74% DCG@5 gain and 3.6% DCG@1 gain overorganic search for all queries.
The gains are higherthan (Zhang et al, 2009)?s results with regards toimprovement over organic results.
By direct com-parison, Ours was 2.7% better than Zhangs signif-icantly in terms of DCG@1 by Wilcoxon signifi-cant test.
DCG@5 is 1.1% better, but not signifi-cant.
The table also show that the higher bucketswith higher probability achieved higher DCG gainthan the lower buckets overall.
Our approach ob-served 16.78% DCG@1 gain for bucket [0.9,1.0].This shows that our methods are very effective.7 ConclusionsWe found most of REQ are long tail queries thatpose a major challenge to Web search.
We havedemonstrated learning REQ is important for Websearch.
this type of queries can?t be solved in tra-ditional ranking method.
We found building a REQclassifier was a good solution.
Our work describedusing machine learning method to build REQ clas-sifier.
Our proposed methods are novel compar-ing with traditional query classification methods.We identified and developed features from querylog, search session, click and time series analysis.We applied several ML approaches including NaiveBayes, SVM and GBDT tree to implement REQlearner.
Finally, we show through ranking experi-ments that the methods we proposed are very effec-tive and beneficial for search engine ranking.AcknowledgementsWe express our thanks to who have assisted usto complete this work, especially, to Fumiaki Ya-maoka, Toru Shimizu, Yoshinori Kobayashi, Mit-suharu Makita, Garrett Kaminaga, Zhuoran Chen.ReferencesR.
Baeza-Yates, F. Saint-Jean, and C. Castillo.
2002.Web dynamics, age and page qualit.
String Process-ing and Information Retrieval, pages 453?461.Steven M. Beitzel, Eric C. Jensen, Ophir Frieder, DavidGrossman, David D. Lewis, Abdur Chowdhury, andAleksandr Kolcz.
2005.
Automatic web query classi-fication using labeled and unlabeled training data.
InSIGIR ?05, pages 581?582.K.
Berberich, M. Vazirgiannis, and G. Weikum.
2005.Time-aware authority rankings.
Internet Math,2(3):301?332.L.
Breiman, J. Friedman, R. Olshen, and C. Stone.
1984.Classification and Regression Trees.
Wadsworth andBrooks, Monterey, CA.S.
Brin and L. Page.
1998.
The anatomy of a large-scale hypertextual web search engine.
Proceedings ofInternational Conference on World Wide Web.Andrei Z. Broder, Marcus Fontoura, EvgeniyGabrilovich, Amruta Joshi, Vanja Josifovski, andTong Zhang.
Robust classification of rare queriesusing web knowledge.
In SIGIR ?07, pages 231?238.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: alibrary for support vector machines.
Software avail-able at http://www.csie.ntu.edu.tw/ cjlin/libsvm.J.
Cho, S. Roy, and R. Adams.
2005.
Page quality: Insearch of an unbiased web ranking.
Proc.
of ACM SIG-MOD Conference.F.
Diaz.
2009.
Integration of news content into web re-sults.
Proceedings of the Second ACM InternationalConference on Web Search and Data Mining (WSDM),pages 182?191.Anlei Dong, Yi Chang, Zhaohui Zheng, Gilad Mishne,Jing Bai, Ruiqiang Zhang, Karolina Buchner, CiyaLiao, and Fernando Diaz.
2010a.
Towards recencyranking in web search.
Proceedings of the Third ACMInternational Conference on Web Search and DataMining (WSDM), pages 11?20.Anlei Dong, Ruiqiang Zhang, Pranam Kolari, JingBai, Fernando Diaz, Yi Chang, Zhaohui Zheng, andHongyuan Zha.
2010b.
Time is of the essence: im-proving recency ranking using twitter data.
19th Inter-national World Wide Web Conference (WWW), pages331?340.Jonathan L. Elsas and Susan T. Dumais.
2010.
Lever-aging temporal dynamics of document content in rele-vance ranking.
In WSDM, pages 1?10.J.
H. Friedman.
2001.
Greedy function approximation:A gradient boosting machine.
Annals of Statistics,29(5):1189?1232.Kalervo Jarvelin and Jaana Kekalainen.
2002.
Cumu-lated gain-based evaluation of IR techniques.
ACMTransactions on Information Systems, 20:2002.K.
Sparck Jones, S. Walker, and S. E. Robertson.
2000.A probabilistic model of information retrieval: devel-opment and comparative experiments.
Inf.
Process.Manage., 36(6):779?808.A.
C. Knig, M. Gamon, and Q. Wu.
2009.
Click-throughprediction for news queries.
Proc.
of SIGIR, pages347?354.1138Ying Li, Zijian Zheng, and Honghua (Kathy) Dai.2005.
Kdd cup-2005 report: facing a great challenge.SIGKDD Explor.
Newsl., 7(2):91?99.Xiao Li, Ye yi Wang, and Alex Acero.
2008.
Learningquery intent from regularized click graphs.
In In SI-GIR 2008, pages 339?346.
ACM.Donald Metzler, Rosie Jones, Fuchun Peng, and RuiqiangZhang.
2009.
Improving search relevance for im-plicitly temporal queries.
In SIGIR ?09: Proceed-ings of the 32nd international ACM SIGIR conferenceon Research and development in information retrieval,pages 700?701.S.
Nunes.
2007.
Exploring temporal evidence in webinformation retrieval.
BCS IRSG Symposium: FutureDirections in Information Access.S.
Pandey, S. Roy, C. Olston, J. Cho, and S. Chakrabarti.2005.
Shuffling a stacked deck: The case for partiallyrandomized ranking of search engine results.
VLDB.G.
Salton and M. J. McGill.
1983.
Introduction to mod-ern information retrieval.
McGraw-Hill, NY.Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey JunfengPan, Kangheng Wu, Jie Yin, and Qiang Yang.
2005.Q2c@ust: our winning solution to query classificationin kddcup 2005.
SIGKDD Explor.
Newsl., 7(2):100?110.Dou Shen, Rong Pan, Jian-Tao Sun, Jeffrey Junfeng Pan,Kangheng Wu, Jie Yin, and Qiang Yang.
2006.
Queryenrichment for web-query classification.
ACM Trans.Inf.
Syst., 24(3):320?352.Ruiqiang Zhang, Yi Chang, Zhaohui Zheng, DonaldMetzler, and Jian-yun Nie.
2009.
Search resultre-ranking by feedback control adjustment for time-sensitive query.
In HLT-NAACL ?09, pages 165?168.1139
