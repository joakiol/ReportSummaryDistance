Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 70?78,ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational LinguisticsReestimation of Reified Rules in Semiring Parsing and BiparsingMarkus Saers and Dekai WuHuman Language Technology CenterDept.
of Computer Science and EngineeringHong Kong University of Science and Technology{masaers|dekai}@cs.ust.hkAbstractWe show that reifying the rules from hyper-edge weights to first-class graph nodes au-tomatically gives us rule expectations in anykind of grammar expressible as a deductivesystem, without any explicit algorithm for cal-culating rule expectations (such as the inside-outside algorithm).
This gives us expecta-tion maximization training for any grammarclass with a parsing algorithm that can bestated as a deductive system, for free.
Havingsuch a framework in place accelerates turn-over time for experimenting with new gram-mar classes and parsing algorithms?to imple-ment a grammar learner, only the parse forestconstruction has to be implemented.1 IntroductionWe propose contextual probability as a quantity thatmeasures how often something has been used ina corpus, and when calculated for rules, it givesus everything needed to calculate rule expectationsfor expectation maximization.
For labeled spans incontext-free parses, this quantity is called outsideprobability, and in semiring (bi-) parsing, it is calledreverse value.
The inside-outside algorithm for rees-timating context-free grammar rules uses this quan-tity for the symbols occurring in the parse forest.Generally, the contextual probability is:The contextual probability of somethingis the sum of the probabilitiesof all contexts where it was used.For symbols participating in a parse, we could stateit like this:The contextual probability of an itemis the sum of the probabilitiesof all contexts where it was used.. .
.
which is exactly what we mean with outsideprobability.
In semiring (bi-) parsing, this quantityis called reverse value, but in this framework it isalso defined for rules, which means that we couldrestate our boxed statement as:The contextual probability of a ruleis the sum of the probabilitiesof all contexts where it was used.This opens up an interesting line of inquiry into whatthis quantity might represent.
In this paper we showthat the contextual probabilities of the rules containprecisely the new information needed in order to cal-culate the expectations needed to reestimate the ruleprobabilities.
This line of inquiry was discoveredwhile working on a preterminalized version of lin-ear inversion transduction grammars (LITGs), so wewill use these preterminalized LITGs (Saers and Wu,2011) as an example throughout this paper.We will start by examining semiring parsing(parsing as deductive systems over semirings, Sec-tion 3), followed by a section on how this relates toweighted hypergraphs, a common representation ofparse forests (Section 4).
This reveals a disparity be-tween weighted hypergraphs and semiring parsing.It seems like we are forced to choose between theinside-outside algorithm for context-free grammars70on the one side, and the flexibility of grammar for-malism and parsing algorithm development affordedby semiring (bi-) parsing.
It is, however, possible tohave both, which we will show in Section 5.
Anintegral part of this unification is the concept of con-textual probability.
Finally, we will offer some con-clusions in Section 6.2 BackgroundA common view on probabilistic parsing?be itbilingual or monolingual?is that it involves theconstruction of a weighted hypergraph (Billot andLang, 1989; Manning and Klein, 2001; Huang,2008).
This is an appealing conceptualization, as itseparates the construction of the parse forest (the ac-tual hypergraph) from the probabilistic calculationsthat need to be carried out.
The calculations are,in fact, given by the hypergraph itself.
To get theprobability of the sentence (pair) being parsed, onesimply have to query the hypergraph for the valueof the goal node.
It is furthermore possible to ab-stract away the calculations themselves, by definingthe hypergraph over an arbitrary semiring.
When theBoolean semiring is used, the value of the goal nodewill be true if the sentence (pair) is a member of thelanguage (or transduction) defined by the grammar,and false otherwise.
When the probabilistic semir-ing is used, the probability of the sentence (pair) isattained, and with the tropical semiring, the proba-bility of the most likely tree is attained.
To furthergeneralize the building of the hypergraph?the pars-ing algorithm?a deductive system can be used.
Bydefining a hand-full of deductive rules that describehow items can be constructed, the full complexi-ties of a parsing algorithm can be very succinctlysummarized.
Deductive systems to represent parsersand semirings to calculate the desired values for theparses were introduced in Goodman (1999).In this paper we will reify the grammar rulesby moving them from the meta level to the objectlevel?effectively making them first-class citizens ofthe parse trees, which are no longer weighted hyper-graphs, but mul/add-graphs.
This move allows usto calculate rule expectations for expectation maxi-mization (Dempster et al, 1977) as part of the pars-ing process, which significantly shortens turn-overtime for experimenting with different grammar for-malisms.Another approach which achieve a similar goal isto use a expectation semiring (Eisner, 2001; Eisner,2002; Li and Eisner, 2009).
In this semiring, all val-ues are pairs of probabilities and expectations.
Theinside-outside algorithm with the expectation semir-ing requires the usual inside and outside calcula-tions over the probability part of the semiring val-ues, followed by a third traversal over the parse for-est to populate the expectation part of the semiringvalues.
The approach taken in this paper also re-quires the usual inside and outside calculations, buto third traversal of the parse forest.
Instead, the pro-posed approach requires two passes over the rulesof the grammar per EM iteration.
The asymptotictime complexities are thus equivalent for the two ap-proaches.2.1 NotationWe will use w to mean a monolingual sentence,and index the individual tokens from 0 to |w| ?
1.This means that w = w0, .
.
.
, w|w|?1.
We will fre-quently use spans from this sentence, and denotethem wi..j , which is to be interpreted as array slices,that is: including the token at position i, but ex-cluding the token at position j (the interval [i, j)over w, or wi, .
.
.
, wj?1).
A sentence w thus cor-responds to the span w0..|w|.
We will also assumethat there exists a grammar G = ?N,?, S,R?
or atransduction grammar (over languages L0 and L1)G = ?N,?,?, S,R?
(depending on the context),where N is the set of nonterminal symbols, ?
is aset of (L0) terminal symbols, ?
is a set of (L1) ter-minal symbols, S ?
N is the dedicated start symboland R is a set of rules appropriate to the grammar.A stochastic grammar is further assumed to have aparameterization function ?, that assigns probabili-ties to all the rules in R. For general L0 tokens wewill use lower case letters from the beginning of thealphabet, and for L1 from the end of the alphabet.For specific sentences we will use e = e0..|e| to rep-resent an L0 sentence and f = f0..|f | to represent anL1 sentence.3 Semiring parsingSemiring parsing was introduced in Goodman(1999), as a unifying approach to parsing.
The gen-71eral idea is that any parsing algorithm can be ex-pressed as a deductive system.
The same algorithmcan then be used for both traditional grammars andstochastic grammars by changing the semiring usedin the deductive system.
This approach thus sepa-rates the algorithm from the specific calculations itis used for.Definition 1.
A semiring is a tuple ?A,?,?,0,1?,where A is the set the semiring is defined over, ?
isan associative, commutative operator over A, withidentity element 0 and ?
is an associative operatorover A distributed over ?, with identity element 1.Semirings can be intuitively understood by consid-ering the probabilistic semiring: ?R+,+,?, 0, 1?,that is: the common meaning of addition andmultiplication over the positive real numbers (in-cluding zero).
Although this paper will have aheavy focus on the probabilistic semiring, sev-eral other exists.
Among the more popular arethe Boolean semiring ?{>,?},?,?,?,>?
and thetropical semiring ?R+ ?
{?
},min,+,?, 0?
(or?R?
?
{??},max,+,?
?, 0?
which can be usedfor probabilities in the logarithmic domain).The deductive systems used in semiring parsinghave three components: an item representation, agoal item and a set of deductive rules.
TakingCKY parsing (Cocke, 1969; Kasami and Torii, 1969;Younger, 1967) as an example, the items would havethe form Ai,j , which is to be interpreted as the spanwi..j of the sentence being parsed, labeled with thenonterminal symbol A.
The goal item would beS0,|w|: the whole sentence labeled with the startsymbol of the grammar.
Since the CKY algorithmis a very simple parsing algorithm, it only has twodeductive rules:A?
a, Ia(wi..j)Ai,j0?i?j?|w| (1)Bi,k, Ck,j , A?
BCAi,j(2)Where Ia(?)
is the terminal indicator function for thesemiring.
The general form of a deductive rule isthat the conditions (entities over the line) yield theconsequence (the entity under the line) given thatthe side conditions (to the right of the line) are satis-fied.
We will make a distinction between conditionsthat are themselves items, and conditions that arenot.
The non-item conditions will be called axioms,and are exemplified above by the indicator function(Ia(wi..j) which has a value that depends only on thesentence) and the rules (A?
a andA?
BC whichhave values that depends only on the grammar).The indicator function might seem unnecessary,but allows us to reason under uncertainty regardingthe input.
In this paper, we will assume that we haveperfect knowledge of the input (but for generality,we will not place it as a side condition).
The func-tion is defined such that:?a ?
??
: Ia(w) ={1 if a = w0 otherwiseAn important concept of semiring parsing is thatthe deductive rules also specify how to arrive at thevalue of the consequence.
Since it is the first valuecomputed for a node, we will call it ?, and the gen-eral way to calculate it given a deductive rule and the?-values of the conditions is:?
(b) =n?i=1?
(ai) iffa1, .
.
.
, anbc1,...,cmIf the same consequence can be produced in severalways, the values are summed using the ?
operator:?
(b) =?n,a1,...,ansuch thata1,...,anbn?i=1?
(ai)The ?-values of axioms depend on what kind of ax-iom it is.
For the indicator function, the ?-value isthe value of the function, and for grammar rules, the?-value is the value assigned to the rule by the pa-rameterization function ?
of the grammar.The ?-value of a consequence corresponds to thevalue of everything leading up to that consequence.If we are parsing with a context-free grammar andthe probabilistic semiring, this corresponds to the in-side probability.3.1 Reverse valuesWhen we want to reestimate rule probabilities, it isnot enough to know the probabilities of arriving atdifferent consequences, we also need to know howlikely we are to need the consequences as a condi-tion for other deductions.
These values are called72S ?
AA0,|e|,0,|f |,As,s,u,u, A?
/G,Bs?,t,u?,v, B ?
[XA], X ?
a/x , Ia/x (es..s?/fu..u?
)As,t,u,v0?s?s?,0?u?u?,Bs,t?,u,v?
, B ?
[AX], X ?
a/x , Ia/x (et?..t/fv?..v )As,t,u,vt??t?|e|,v?
?v?|f |,Bs?,t,u,v?
, B ?
?XA?, X ?
a/x , Ia/x (es..s?/fv?..v )As,t,u,v0?s?s?,v?
?v?|f |,Bs,t?,u?,v, B ?
?AX?, X ?
a/x , Ia/x (et?..t/fu..u?
)As,t,u,vt?
?t?|e|,0?u?u?Figure 2: Deductive system describing a PLITG parser.
The symbols A, B and S are nonterminal symbols, while Xrepresents a preterminal symbol.S ?
AA0,|e|,0,|f |,As,s,u,u, A?
/G,Bs?,t,u?,v, B ?
[a/x A], Ia/x (es..s?/fu..u?
)As,t,u,v0?s?s?,0?u?u?,Bs,t?,u,v?
, B ?
[A a/x ], Ia/x (et?..t/fv?..v )As,t,u,vt??t?|e|,v?
?v?|f |,Bs?,t,u,v?
, B ?
?a/x A?, Ia/x (es..s?/fv?..v )As,t,u,v0?s?s?,v?
?v?|f |,Bs,t?,u?,v, B ?
?A a/x ?, Ia/x (et?..t/fu..u?
)As,t,u,vt?
?t?|e|,0?u?u?Figure 1: Deductive system describing an LITG parser.reverse values in Goodman (1999), and outsideprobabilities in the inside-outside algorithm (Baker,1979).
In this paper we will call them contextualvalues, or ?-values (since they are the second valuewe calculate).The way to calculate the reverse values is to startwith the goal node and work your way back to theaxioms.
The reverse value is calculated to be:?
(x) =?n,i,b,a1,...,ansuch thata1,...,anb ?x=ai?(b)??
{j|1?j?n,j 6=i}?
(aj)That is: the reverse value of the consequence com-bined with the values of all sibling conditions is cal-culated and summed for all deductive rules wherethe item is a condition.3.2 SPLITGAfter we introduced stochastic preterminalizedLITGs (Saers, 2011, SPLITG), the idea of express-ing them in term of semiring parsing occurred.
Thisis relatively straight forward, producing a compactset of deductive rules similar to that of LITGs.
ForLITGs, the items take the form of bispans labeledwith a symbol.
We will represent these bispans asAs,t,u,v, where A is the label, and the two spans be-ing labeled are es..t and fu..v. Since we usually dotop-down parsing, the goal item is a virtual item (G)than can only be reached by rewriting a nontermi-nal to the empty bistring ( / ).
Figure 1 shows thedeductive rules for LITG parsing.A preterminalized LITG promote preterminalsymbols to a distinct class of symbols in the gram-mar, which is only allowed to rewrite into bitermi-nals.
Factoring out the terminal productions in thisfashion allows the grammar to define one probabilitydistribution over all the biterminals, which is usefulfor bilexica induction.
It also means that the LITGrules that produce biterminals have to be replacedby two rules in a PLITG, resulting in the deductiverules in Figure 2.4 Weighted hypergraphsA hypergraph is a graph where the nodes are con-nected with hyperedges.
A hyperedge is an edgethat can connect several nodes with one node?it has73Figure 3: A weighted hyperedge between three nodes,based on the rule A ?
BC.
The tip of the arrow pointsto the head of the edge, and the two ends are the tails.
Thedashed line idicates where the weight of the edge comesfrom.one head, but may have any number of tails.
Intu-itively, this is a good match to context-free gram-mars, since each rule connects one symbol on theleft hand side (the head of the hyperedge) with anynumber of symbols on the right hand side (the tailsof the hyperedge).
During parsing, one node is con-structed for each labeled (bi-) span, and the nodesare connected with hyperedges based on the validapplications of rules.
A hyperedge will be repre-sented as [h : t1, .
.
.
, tn] where h is the head and tiare the tails.When this is applied to weighted grammar, eachhyperedge can be associated with a weight, makingthe hypergraph weighted.
Every time an edge is tra-versed, its weight is combined with the value travel-ling through the edge.
Weights are assigned to hy-peredges via a weighting function w(?
).Figure 3 contains an illustration of a weightedhyperedge.
The arrow indicates the edge itself,whereas the dotted line indicates where the weightcomes from.
Since each hyperedge corresponds toexactly one rule from a stochastic context-free gram-mar, we can use the inside-outside algorithm (Baker,1979) to calculate inside and outside probabilities aswell as to reestimate the probabilities of the rules.What we cannot easily do, however, is to change theparsing algorithm or grammar formalism.If the weighted hyperedge approach was a one-to-one mapping to the semiring parsing approach, wecould, but it is not.
The main difference is that rulesare part of the object level in semiring parsing, butFigure 4: The same hyperedge as in Figure 3, where therule has been promoted to first-class citizen.
The hyper-edge is no longer weighted.part of the meta level in weighted hypergraphs.
Toaddress this disparity, we will reify the rules in theweighted hypergraph to make them nodes.
Figure 4shows the same hyperedge as Figure 3, but with therule as a proper node rather than a weight associ-ated with the hyperedge.
These hyperedges are ag-nostic to what the tail nodes represent, so we can nolonger use the inside-outside algorithm to reestimatethe rule probabilities.
We can, however, still calcu-late inside probabilities.
In the weighted hyperedgeapproach, the inside probability of a node is:?
(p) =?n,q1,...,qnsuch that[p:q1,...qn]w([p : q1, .
.
.
, qn])?n?i=1?
(qi)Whereas with the rules reified, the weight simplymoved into the tail product:?
(p)?n,q1,...,qnsuch that[p:q1,...qn]n?i=1?
(qi)By virtue of the deductive system used to build thehypergraph, we also have the reverse values, whichcorrespond to outside probability:?
(x) =?i,p,n,q1,...,qnsuch that[p:q1,...qn]?x=qi?(p)??
{j|1?j?n,j 6=i}?
(qj)This means that we have the inside and outside prob-abilities of the nodes, and we could shoe-horn it intothe reestimation part of the inside-outside algorithm.74It also means that we have ?-values for the rules,which we are calculating as a side-effect of movingthem into the object level.
In Section 5, we will takea closer look at the semantics of the contextual prob-abilities that we are in fact calculating for the reifiedrules, and see how they can be used in reestimationof the rules.4.1 SPLITGUsing the hypergraph parsing framework forSPLITGs turns out to be non-trivial.
Where the stan-dard LITG uses one rule to rewrite a nonterminal intoanother nonterminal and a biterminal, the SPLITGrewrites a nonterminal to a preterminal and a non-terminal, and rewrites the preterminal into a biter-minal.
This causes problems within the hypergraphframework, where each rule application should cor-respond to one hyperedge.
As it stands we have twooptions:1.
Let each rule correspond to one hyperedge,which means that we need to introduce preter-minal nodes into the hypergraph.
This hasa clear drawback for bracketing grammars,1since it is now necessary to keep different sym-bols apart.
It also produces larger hypergraphs,since the number of nodes is inflated.2.
Let hypergraphs be associated with one or tworules, which means that we need to redefine hy-peredges so that there are two different weight-ing functions: one for the nonterminal weightand one for the preterminal weight.
Althoughall hyperedges are associated with one nonter-minal rule, some hyperedges are not associatedwith any preterminal rule, making the pretermi-nal weighting function partly defined.Both of these approaches work in practice, but nei-ther is completely satisfactory since they both rep-resent work-arounds to shoe-horn the parsing algo-rithm (as stated in the deductive system) into a for-malism that is not completely compatible.
By reify-ing the rules into the object level, we rid ourselvesof this inconvenience, as we no longer differentiatebetween different types of conditions.1A bracketing grammar is a grammar where |N | = 1.5 Reestimation of reified rulesAs has been amply hinted at, the contextual prob-abilities (outside probabilities, reverse values or ?-values) contain all new information we need aboutthe rules to reestimate their probability in an expec-tation maximization (Dempster et al, 1977) frame-work.
To show that this is indeed the case, wewill rewrite the reestimation formulas of the inside-outside algorithm (Baker, 1979) so that they arestated in terms of contextual probability for therules.In general, a stochastic context-free grammar canbe estimated from examples of trees generated bythe grammar by means of relative frequency.
Thisis also true for expectation maximization with thecaveat that we have multiple hypotheses over eachsentence (pair), and therefore calculate expectationsrather than discrete frequency counts.
We thus com-pute the updated parameterization function ??
basedon expectations from the current parameterizationfunction:??
(?|p) =E?
[p?
?]E?
[p]Where p ?
N and ?
?
{?
?
N}+ (or ?
?{(?????
)?N}+ for transduction grammars).
Theexpectations are calculated from the sentences in acorpus C:E?
[x] =?w?CE?
[x|w]The exact way of calculating the expectation on xgiven a sentence depends on what x is.
For nonter-minal symbols, the expectations are given by:E?
[p|w] =E?
[p,w]E?
[w]=?0?i?j?|w| Pr (pi,j ,w|G)Pr (w|G)=?0?i?j?|w| ?(pi,j)?(pi,j)?(S0,|w|)?
(S0,|w|)For nonterminal rules, the expectations are shown inFigure 5.
The most noteworthy step is the last one,where we use the fact that the summation is overthe equivalence of the rule?s reverse value.
Each75E?
[p?
qr|w] =E?
[p?
qr,w]E?
[w]=?0?i?k?j?|w| Pr(w0..i, pi,j , wj..|w|?
?G)Pr (wi..k|qi,k, G) Pr (wk..j |rk,j , G) ?
(qr|p)Pr (w|G)=?0?i?k?j?|w| ?(pi,j)?(qi,k)?(rk,j)?
(qr|p)?(S0,|w|)?(S0,|w|)=?
(qr|p)?0?i?k?j?|w| ?(pi,j)?(qi,k)?(rk,j)?(S0,|w|)?(S0,|w|)=?(p?
qr)?(p?
qr)?(S0,|w|)?
(S0,|w|)Figure 5: Expected values for nonterminal rules in a specific sentence.E?
[p?
a|w] =E?
[p?
a,w]E?
[w]=?0?i?j?|w| Pr(w0..i, pi,j , wj..|w|??G)Ia(wi..j)?
(a|p)Pr (w|G)=?0?i?j?|w| ?(pi,j)Ia(wi..j)?
(a|p)?(S0,|w|)?(S0,|w|)=?
(a|p)?0?i?j?|w| ?(pi,j)Ia(wi..j)?(S0,|w|)?(S0,|w|)=?(p?
a)?(p?
a)?(S0,|w|)?
(S0,|w|)Figure 6: Expected values of terminal rules in a specific sentence.?(pi,j)?(qi,k)?
(rk,j) term of the summation corre-sponds to one instance where the rule was used inthe parse.
Furthermore, the ?
value is the outsideprobability of the consequence of the deductive ruleapplied, and the two ?
values are the inside prob-abilities of the sibling conditions of that deductiverule.
The entire summation thus corresponds to ourdefinition of the reverse value of a rule, or its outsideprobability.In Figure 6, the same process is carried out for ter-minal rules.
Again, the summation is over all possi-ble ways that we can combine the inside probabilityof the sibling conditions of the rule with the outsideprobability of the consequence.Since the expected values of both terminal andnonterminal rules have the same form, we can gen-eralize the formula for any production ?:E?
[p?
?|w] =?(p?
?)?(p?
?)?(S0,|w|)?
(S0,|w|)Finally, plugging it all into the original rule estima-tion formula, we have:??
(?|p) =E?
[p?
?]E?
[p]=?w?C?(p??)?(p??)?(S0,|w|)?(S0,|w|)?w?C?0?i?j?|w|?(pi,j)?(pi,j)?(S0,|w|)?
(S0,|w|)= ?(p?
?)?w?C?(p??)?(S0,|w|)?(S0,|w|)?w?C?0?i?j?|w|?(pi,j)?(pi,j)?(S0,|w|)?
(S0,|w|)Rather than keeping track of the expectations of non-terminals, they can be calculated from the rule ex-pectations by marginalizing the productions:E?
[p] =??E?
[p?
?
]76Figure 7: The same hyperedge as in Figures 3 and 4, rep-resented as a mul/add-subgraph.5.1 SPLITGSince this view of EM and parsing generalizes to de-ductive systems with multiple rules as conditions,we can apply it to the deductive system of SPLITGs.It is, however, also interesting to note how the hy-pergraph view of parsing is changed by this.
Weeffectively removed the weights from the edges, butkept the feature that values of nodes depend entirelyon the values connected by incoming hyperedges.
Ifwe assume the values to be from the Boolean semir-ing, the hypergraphs we ended up with are in factand/or-graphs.
That is: each node in the hypergraphcorresponds to an or-node, and each hyperedge cor-responds to an and-node.
We note that this can begeneralized to any semiring, since or is equivalent to?
and and is equivalent to ?
for the Boolean semir-ing, we can express a hypergraph over an arbitrarysemiring as a mul/add-graph.2 Figure 7 shows howa hyperedge looks in this new graph form.
The ?-value of a node is calculated by combining the val-ues of all incoming edges using the operator of thenode.
The ?-values are also calculated using the op-erator of the node, but with the edges reversed.
Forthis to work properly, the mul-nodes need to behavesomewhat different from add-nodes: each incomingedge has to be reversed one at a time, as illustratedin Figure 8.6 ConclusionsWe have shown that the reification of rules into theparse forest graphs allows for a unified frameworkwhere all calculations are performed the same way,2Because it is much easier to pronounce than ?/?-graph.Figure 8: Reverse values (?)
are calculated by track-ing backwards through all possible paths.
This producesthree different paths for the mul/add-subgraph from Fig-ure 7.
Arrows pointing downward propagate ?-valueswhile arrows pointing upward propagate ?-values.and where the calculations for the rules encompassall information needed to reestimate them using ex-pectation maximization.
The contextual probabilityof a rule?its outside probability?holds all infor-mation needed to calculate expectations, which canbe exploited by promoting the rules to first-class cit-izens of the parse forest.
We have also seen how thisreification of the rules helped solve a real transla-tion problem?induction of stochastic preterminal-ized linear inversion transduction grammars usingexpectation maximization.AcknowledgmentsThis work was funded by the Defense AdvancedResearch Projects Agency (DARPA) under GALEContract Nos.
HR0011-06-C-0023 and HR0011-06-C-0023, and the Hong Kong Research GrantsCouncil (RGC) under research grants GRF621008,GRF612806, DAG03/04.EG09, RGC6256/00E, andRGC6083/99E.
Any opinions, findings and conclu-sions or recommendations expressed in this materialare those of the authors and do not necessarily re-flect the views of the Defense Advanced ResearchProjects Agency.
We would also like to thank thethree anonymous reviewers, whose feedback madethis a better paper.ReferencesJames K. Baker.
1979.
Trainable grammars for speechrecognition.
In Speech Communication Papers for the97th Meeting of the Acoustical Society of America,pages 547?550, Cambridge, Massachusetts.Sylvie Billot and Bernard Lang.
1989.
The structure ofshared forests in ambiguous parsing.
In Proceedings77of the 27th annual meeting on Association for Compu-tational Linguistics, ACL?89, pages 143?151, Strouds-burg, Pennsylvania, USA.John Cocke.
1969.
Programming languages and theircompilers: Preliminary notes.
Courant Institute ofMathematical Sciences, New York University.Arthur Pentland Dempster, Nan M. Laird, and Don-ald Bruce Rubin.
1977.
Maximum likelihood fromincomplete data via the em algorithm.
Journal of theRoyal Statistical Society.
Series B (Methodological),39(1):1?38.Jason Eisner.
2001.
Expectation semirings: FlexibleEM for finite-state transducers.
In Gertjan van No-ord, editor, Proceedings of the ESSLLI Workshop onFinite-State Methods in Natural Language Processing(FSMNLP).
Extended abstract (5 pages).Jason Eisner.
2002.
Parameter estimation for probabilis-tic finite-state transducers.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 1?8, Philadelphia, July.Joshua Goodman.
1999.
Semiring parsing.
Computa-tional Linguistics, 25(4):573?605.Liang Huang.
2008.
Forest-based Algorithms in Natu-ral Language Processing.
Ph.D. thesis, University ofPennsylvania.Tadao Kasami and Koji Torii.
1969.
A syntax-analysisprocedure for unambiguous context-free grammars.Journal of the Association for Computing Machinery,16(3):423?431.Zhifei Li and Jason Eisner.
2009.
First- and second-orderexpectation semirings with applications to minimum-risk training on translation forests.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 40?51, Singapore,August.Christopher D. Manning and Dan Klein.
2001.
Parsingand hypergraphs.
In Proceedings of the 2001 Interna-tional Workshop on Parsing Technologies.Markus Saers and Dekai Wu.
2011.
Principled inductionof phrasal bilexica.
In Proceedings of the 15th AnnualConference of the European Association for MachineTranslation, Leuven, Belgium, May.Markus Saers.
2011.
Translation as Linear Transduc-tion: Models and Algorithms for Efficient Learning inStatistical Machine Translation.
Ph.D. thesis, UppsalaUniversity, Department of Linguistics and Philology.Daniel H. Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andControl, 10(2):189?208.78
