Adapting a Semantic Question Answering System to the WebSven HartrumpfIntelligent Information and Communication Systems (IICS)University of Hagen (FernUniversita?t in Hagen)58084 Hagen, GermanySven.Hartrumpf@fernuni-hagen.deAbstractThis paper describes how a question an-swering (QA) system developed for small-sized document collections of several mil-lion sentences was modified in order towork with a monolingual subset of theweb.
The basic QA system relies on com-plete sentence parsing, inferences, and se-mantic representation matching.
The ex-tensions and modifications needed for use-ful and quick answers from web docu-ments are discussed.
The main extensionis a two-level approach that first accesses aweb search engine and downloads some ofits document hits and then works similarto the basic QA system.
Most modifica-tions are restrictions like a maximal num-ber of documents and a maximal lengthof investigated document parts; they en-sure acceptable answer times.
The result-ing web QA system is evaluated on theGerman test collection from QA@CLEF2004.
Several parameter settings andstrategies for accessing the web search en-gine are investigated.
The main results are:precision-oriented extensions and exper-imentally derived parameter settings areneeded to achieve similar performance onthe web as on small-sized document col-lections that show higher homogeneity andquality of the contained texts; adapting asemantic QA system to the web is feasible,but answering a question is still expensivein terms of bandwidth and CPU time.1 IntroductionThere are question answering (QA) systems in-tended for small-sized document collections (tex-tual QA systems) and QA systems aiming atthe web (web(-based) QA systems).
In this pa-per, a system of the former type (InSicht, see(Hartrumpf, 2005b)) is transformed into one ofthe latter type (called InSicht-W3 for short).
InSect.
2, the textual QA system for German is pre-sented.
The extensions and modifications requiredto get it working with the web as a virtual doc-ument collection are described and discussed inSect.
3.
The resulting system is evaluated on awell-known test collection and compared to thebasic QA system (Sect.
4).
After the conclusion,some directions for further research are indicated.2 The Basic QA SystemThe semantic1 QA system that was turned into aweb QA system is InSicht.
It relies on completesentence parsing, inferences, and semantic repre-sentation matching and comprises six main steps.In the document processing step, all docu-ments from a given collection are transformed intoa standard XML format (CES, corpus encodingstandard, see http://www.cs.vassar.edu/CES/) withword, sentence, and paragraph borders markedup by XML elements w, s, and p, respectively.Then, all preprocessed documents are parsed byWOCADI (Hartrumpf, 2003) yielding a syntacticdependency structure and, more importantly, a se-mantic representation, a semantic network of theMultiNet formalism (Helbig, 2006) for each doc-ument sentence.
The parser can produce intersen-tential coreference links for a document.In the second step (query processing), the user?squestion is parsed by WOCADI.
Determining thesentence type (here, often a subtype of question)is especially important because it controls someparts of two later steps: query expansion and an-swer generation.Next comes query expansion: Equivalent andsimilar semantic networks are derived from theoriginal query network by means of lexico-1Semantic in the sense that formal semantic representa-tions of documents and questions are automatically producedby a parser and form the system center.EACL 2006 Workshop on Multilingual Question Answering - MLQA0661semantic relations from HaGenLex (Hagen Ger-man Lexicon, (Hartrumpf et al, 2003)) and a lex-ical database (GermaNet), equivalence rules, andinferential rules like entailments for situations (ap-plied in backward chaining).
The result is a setof disjunctively connected semantic networks thattry to cover many possible kinds of representationsof sentences possibly containing an explicit or im-plicit answer to the user?s question.In the fourth step (semantic network matching),all document sentences matching at least one ofthe semantic networks from query expansion arecollected.
A two-level approach is chosen for effi-ciency reasons.
First, an index of concepts (disam-biguated words with IDs from the lexicon) is con-sulted with the relevant concepts from the querynetworks.
Second, the retrieved documents arecompared sentence network by sentence networkto find a match with a query network.Answer generation is next: Natural language(NL) generation rules are applied to semantic net-works that match a query network in order to gen-erate an NL answer string from the deep seman-tic representations.
The sentence type and the se-mantic network control the selection of generationrules.
The rules also act as a filter for uninforma-tive or bad answers.
The results are tuples of gen-erated answer string, numerical score, supportingdocument ID, and supporting sentence ID.To deal with different answer candidates, ananswer selection step is required.
It realizes aquite simple but successful strategy that combinesa preference for more frequent answers and a pref-erence for more elaborate answers.
The best an-swers (by default only the single best answer) andthe supporting sentences are presented to the userthat posed the question.The first step, document processing, is run off-line in InSicht; it is run online in InSicht-W3 toavoid unacceptable parsing costs before the sys-tem can be used.
The remaining five steps will beleft mostly unchanged for InSicht-W3, in parts justdifferently parameterized.3 QA System Extensions for the Web3.1 A Naive Approach to Web-based QAThe simplest approach to turning InSicht into aweb QA system would be to collect German webpages and work with the resulting document col-lection as described in Sect.
2.
However, a deepsemantic analyzer will need several years to parseall web pages in German (even if excluding thepages from the much larger deep web).
The fol-lowing formula provides a rough estimate2 of theCPU years needed:t =#documents ?#sent per document [sent]parser speed [sent/h]=500,000,000 ?320 [sent]4000 [sent/h]= 40,000,000h ?
4,566aThis long time indicates that the naive approachis currently not an option.
Therefore a multi-levelapproach was investigated.
In this paper, only atwo-level approach is discussed, which has beentried in shallow QA systems.3.2 A Two-Level Approach to Web-based QAAs in other applications, a web search engine canbe used (as a first level) to preselect possibly rel-evant documents for the task at hand (here, an-swering a question posed by a user).
In InSicht-W3, the web is accessed when similar and equiva-lent semantic networks have been generated dur-ing query expansion.
Clearly, one cannot di-rectly use this semantic representation for retriev-ing document URLs from any service out there onthe web?at least till the arrival of a web anno-tated with formal NL semantics.
One must trans-form the semantic networks to an adequate level;in many web search engines, this is the level ofsearch terms connected in a Boolean formula us-ing and and or.To derive a search engine query from a seman-tic network, all lexical concepts from the seman-tic network are collected.
For example, questionqa04 068 from QA@CLEF 2004 in example (1)leads to the semantic network depicted in Fig.
1(and related semantic networks; for a QA@CLEF2004 question, 4.8 additional semantic networkson average).
(1) WoWheresitztsitsHugoHugoLacourLacourhinterbehindGittern?bars?
?Where is Hugo Lacour imprisoned?
?The lexical concepts are roughly speaking thoseconcepts (represented as nodes in graphical form)whose names are not of the form c1, c2, etc.
Thelexical concepts are named after the lemma of the2The average number of sentences per document has beendetermined from a sample of 23,800 preprocessed web docu-ments in InSicht-W3.EACL 2006 Workshop on Multilingual Question Answering - MLQA0662Figure 1: Semantic network for CLEF question qa04 068: Wo sitzt Hugo Lacour hinter Gittern?
(?Whereis Hugo Lacour imprisoned??).
Some edges are shown folded below the node name.lacour.0fec28naSUB nachname.1.1??
?GENER spQUANT oneCARD 1ETYPE 0??
?VAL csooc37?wh-questionlSUB underspecified-location.0[GENER sp]c27naSUB vorname.1.1??
?GENER spQUANT oneCARD 1ETYPE 0??
?VALcsc26dSUB mensch.1.1???????
?FACT realGENER spQUANT oneREFER detCARD 1ETYPE 0VARIA con???????
?ATTR ccooATTRcc OOc2stSUBS sitzen.1.1TEMP present.0[GENER sp]LOCssLOCss OOSCAR csoohugo.0fec32dPRED gitter.1.1??
?FACT realQUANT multREFER indetETYPE 1???c36l??
?FACT realQUANT multREFER indetETYPE 1???
*HINTER csoocorresponding word plus a numerical homographidentifier and a numerical reading identifier.As current web search engines provide no (oronly rough) lemmatization for German, one mustgo one step further away from the semantic net-work by generating full forms for each word be-longing to a lexical concept.
(Others have trieda similar approach; for example, Neumann andSacaleanu (2005) construct IR queries which canbe easily adapted to different IR engines.)
In theexample, the node sitzen.1.1 (to sit) leads to 26different full forms (shown below in example (2)).These forms can be used as search terms; as alter-natives, they are connected disjunctively.
Thus foreach semantic network from query expansion, aconjunction of disjunctions of word forms is con-structed.
Word forms can belong to words notoccurring in the question because query expan-sion can introduce new concepts (and thereby newwords), e.g.
by inferential rules.
The Booleanformulae for several semantic networks are con-nected disjunctively and the resulting formula issimplified by applying logical equivalences.
Whatone can pass to a web search engine as a prese-lection query for question (1) is shown in prefixnotation in example (2):(2) (or(and(or Gitter Gittern Gitters)(or Hugo Hugos)(or Lacour Lacours)(or gesessen gesessene gesessenem gesesse-nen gesessener gesessenes sa?
sa?en sa?estsa?et sa?t sitze sitzen sitzend sitzende sitzen-dem sitzenden sitzender sitzendes sitzest sitzetsitzt sa?
?e sa?
?en sa?
?est sa??et)).
.
.
; query derived from second query network)If a search engine does not offer Boolean operators(or limits Boolean queries to some hundred char-acters), one can transform the Boolean query intoDNF and issue for each disjunct a search enginequery that requires all its terms (for example, byprepending a plus sign to every term).There are several problems related to break-ing down a semantic network representation intoa Boolean formula over word forms.
For example,lexical concepts that do not stem from a surfaceform in the question should be excluded from thequery.
An example is the concept nachname.1.1(?last name?)
which the named entity subparser ofWOCADI installs in the semantic network whenconstructing the semantic representation of a hu-man being whose last name is specified in the text(see Fig.
1).The generation of a preselection query de-scribed above indicates that there is a fundamentallevel mismatch between the first level (web searchengine) and the second level (an NL understand-ing system like the QA system InSicht): wordsEACL 2006 Workshop on Multilingual Question Answering - MLQA0663(plus proximity information3) vs. concepts within(possibly normalized) semantic representations ofsentences (or texts).
This level mismatch deteri-orates the precision of the first level and therebythe recall of the second level.
The latter might besurprising.
The precision of the first level (viewedin isolation) is not problematic because the secondlevel achieves the same precision no matter howlow the precision of the first level is.
But precisiondoes matter because the limited number of docu-ments that the first level can return and that thesecond level can efficiently process might lead tonot seeing relevant documents (i.e.
lower recall) orat least to seeing them much later (which can im-ply unacceptable answer times).
Or to put it differ-ently: Very low precision in the first level can leadto lower recall of the whole multi-level system ifthere are any retrieval set limits for the first level.At the end of the first level, the retrieved webdocuments are simplified by a script that uses thetext dump function of the lynx web browser to ex-tract the textual content.
The resulting documentrepresentations can be processed by InSicht.3.3 Deficiencies of Web Search EnginesWeb search engines typically restrict the lengthof queries, e.g.
to 1000 bytes, 1500 bytes, or 10terms.
But the rich morphology of languages likeGerman can lead to long and complex preselectionqueries so that one needs to reduce preselectionqueries as far as possible (e.g.
by dropping rareword forms).
An additional strategy is to split thequery into several independent subqueries.The QA system is currently based on match-ing semantic representations with the granularityof a sentence only.
Thus one would achieve muchbetter precision if the web search engine alloweda same-sentence operator or at least a proximityoperator (often named NEAR ) of say 50 wordswhich ignores linguistic units like sentences andparagraphs but suffices for our goals.
Althoughsome web search engines once had some kind ofproximity operator, currently none of the searchengines with large indices and Boolean queriesseems to offer this facility.3.4 Pragmatic Limits for Web-based QAPragmatic limits are needed to ensure acceptableanswer times when dealing with the vast number3At least some years ago and hopefully in the future again,see Sect.
3.3.of web documents.
The following constraining pa-rameters are implemented in InSicht-W3:number of documents d (BT-R) 4 The numberof documents retrieved from a search engineis limited by InSicht-W3.
A separate queryis sent to the search engine for each top-leveldisjunct, which corresponds to one semanticnetwork.
For each of the q subqueries, d/qdocuments are retrieved, at most.Many search engines employ a maximalnumber of hits h. (A popular choice is cur-rently 1000.)
Due to other filtering con-straints (like snippet testing as defined be-low), the limit h can inhibit the performanceof InSicht-W3 even if h is greater than d.document format (BT-R) Only documents en-coded as HTML or plain text are covered.Other formats are omitted by adding a formatconstraint to the preselection query, by inves-tigating the URL, and by interpreting the re-sult of the UNIX program file.document language (BPT-R) Only documentsthat are mainly written in German are treated.A language identification module is availablein InSicht-W3, but the selection of documentlanguage in the web search engine wasprecise enough.document length l (T-R) If a document is longerthan l bytes it is shortened to the first l bytes(current value: 1,000,000 bytes).document URL (BPT-R) The URL of a docu-ment must belong to certain top-level do-mains (.at, .ch, .de, .info, .net, .org) and mustnot be on a blacklist.
Both conditions aim athigher quality pages.
The blacklist is shortand should be replaced by one that is collab-oratively maintained on the web.snippet testing (BPT-R) Many search enginesreturn a text snippet for each search hit thattypically contains all word forms from thesearch engine query.
Sometimes the snip-pet does not fulfill the preselection query.
Astronger constraint is that a snippet passage4Trade-offs (and goals) are indicated in parentheses, be-tween bandwidth (B), precision (P), (answer) time (T), on theone hand, and recall (R), on the other hand.EACL 2006 Workshop on Multilingual Question Answering - MLQA0664without ellipsis (.
.
. )
must fulfill the preselec-tion query.
This realizes an imperfect same-sentence or similar proximity operator.word form selection (BT-R) To shorten prese-lection queries for highly inflecting wordcategories (e.g.
German verbs), InSicht-W3omits full forms that are less likely to be con-tained in answering document sentences (e.g.verb forms that can only be imperative or firstand second person).
For adjectives, only theforms matching the degree (positive, compar-ative, and superlative in German) used in thequestion are included.
The most restrictiveparameter setting is to choose only one wordform per content word, e.g.
the word form oc-curring in the question.These constraints can be checked before the sec-ond level starts.The following parameters are applicable on thesecond level:number of parsed sentences s (T-R) For eachdocument, at most s sentences are parsed(current value: 100).sentence language (PT-R) As many web pagescontain passages in several languages, it issometimes important to discard sentencesthat seem to be in a different language thanthe document itself.
If the percentage of un-known words in a sentences reaches a thresh-old, the sentence is considered to be writtenin a foreign languagesentence selection (T) Only a sentence whose setof word forms fulfills the preselection querysent to the web search engine is parsed byWOCADI.
This realizes a same-sentence op-erator on the second level.
Of course, thisoperator would be more helpful in the websearch engine (see Sect.
3.3).3.5 Parameter Settings of the QA SystemIn general, the QA system must be parameterizeddifferently when going from a high quality, low-redundancy, small-sized collection (like archivesof some newspapers) to a mixed quality, high-redundancy, large-sized collection (like a consid-erable subset of the web).
For example, the varia-tion of concepts (like synonyms, hyponyms, andhyperonyms) is less important on the web thanin smaller collections due to its large redundancyand increased chance to find a formulation whosesemantic representation contains exactly the con-cepts from the question.
This is helpful becausefull concept variations lead for some questions tovery long preselection queries.3.6 Language-specific ProblemsInSicht-W3 has to deal with several problems inthe German language and German web pages,which might be less problematic for English orsome other languages.
On the one hand, the richmorphology requires to generate word forms forcontent words derived from query networks; onthe other hand, to avoid very long preselectionqueries the generation must be limited (e.g.
withthe word form selection parameter).The morphological phenomenon of separableprefix verbs in German requires special treatment.Certain word forms of such verbs must appear asone orthographic word or two orthographic words(often separated by many intervening words).
Thechoice depends on the word order of the sentenceor clause that contains the verb.
InSicht-W3?s pre-selection queries contain both variants, e.g.
for theword form absitzt the following formula is gener-ated: (or absitzt (and ab sitzt)).In German web pages, two orthography sys-tems can be found, the old one and the new oneintroduced in 1998.
As both systems are oftenmixed in pages (and question sets), the NL pars-ing must be flexible and exploit the links betweenold spellings, new spellings, and spelling vari-ants.
The parser normalizes all words and conceptnames to the old orthography.
For preselectionqueries, word forms for all orthography systemsmust be generated.Not only the orthography system might changein a web page, also the language itself mightswitch (often without correct markup), e.g.
be-tween English and German.
Therefore the sen-tence language parameter from Sect.
3.4 was in-troduced.
Such effects also occur for other lan-guages.German texts contain important characters thatare not covered by the ASCII character set.
(Themost frequent ones are a?, o?, u?, A?, O?, U?, and ?.)
Un-fortunately, different character encodings are usedfor German web pages, sometimes with a mis-match of declared (or implied) encoding and ac-tually used encoding.
This problem has been suc-cessfully delegated to the lynx web browser (seeend of Sect.
3.2).EACL 2006 Workshop on Multilingual Question Answering - MLQA0665Table 1: Evaluation results for the German questions from QA@CLEF 2004.
InSicht used all 200questions, InSicht-W3 only 170 questions (see text).setup answers (%) K1system document collection non-empty emptyright inexact wrong right wrongInSicht QA@CLEF document collection 30.5 2.5 0.0 10.0 57.0 0.28InSicht-W3 web (as virtual document collection) 22.9 2.9 2.4 0.0 71.8 0.184 Evaluation of the Web QA SystemThe resulting web QA system is evaluated on anestablished set of test questions: QA@CLEF 2004(Magnini et al, 2005).
Some questions in thisset have an important implicit temporal context.For example, the correct answer to questions likeqa04 090 in example (3) critically depends on thetime this present tense question is uttered.
(3) WerWhoistisPra?sidentpresidentvonofUNICE?UNICE?In QA@CLEF, a supported correct answer to thisquestion can be the name of a UNICE presi-dent only from a certain time period becausethe QA@CLEF document collection consists ofnewspaper and newswire articles from 1994 and1995 (and because there are no documents aboutthe history of UNICE).
On the web, there are ad-ditional correct answers.5 Questions with implicittime restriction (30 cases) are excluded from theevaluation so that 170 questions remain for eval-uation in InSicht-W3.
Alternatives would be torefine these questions by making the temporal re-striction explicit or to extend the gold standard byanswers that are to be considered correct if work-ing on the web.Table 1 contains evaluation results for InSicht-W3: the percentages of right, inexact, and wronganswers (separately for non-empty answers andempty answers) and the K1-measure (see (Her-rera et al, 2005) for a definition).
For compar-ison, the results of the textual QA system InSichton the QA@CLEF document collection are shownin the first row.
The percentages for non-empty an-swers differ for right answers and wrong answers.In both aspects, InSicht-W3 is worse than InSicht.The main reason for these changes is that the struc-ture and textual content of documents on the web5Or just one correct answer, which differs from the onein QA@CLEF, when one interprets the present tense in thequestion as referring to today.are much more diverse than in the QA@CLEF col-lection.
For example, InSicht-W3 regarded somesequences of words from unrelated link anchortexts as a sentence, which led to some wrong an-swers especially for definition questions.
Alsofor empty answers, InSicht-W3 performs worsethan InSicht.
This is in part due to the too opti-mistic assumption during answer assessment thatall German questions from QA@CLEF 2004 havea correct answer on the web (therefore the col-umn labelled right empty answers contains 0.0for InSicht-W3).
However, InSicht-W3 found 12right answers that InSicht missed.
So there is apotential for a fruitful system combination.The impact of some interesting parameters wasevaluated by varying parameter settings (Table 2).The query network quality qmin (column 2) is avalue between 0 (worst) and 1 (best) that mea-sures how far away a derived query network isfrom the original semantic network of the ques-tion.
For example, omitting information from thesemantic network for the question (like the firstname of a person if also the last name is specified),leads to a reduction of the initial quality value of1.
The value in column 2 indicates at what thresh-old variant query networks are ignored.
Column3 corresponds to the parameter of word form se-lection (see Sect.
3.4).
The two runs with d = 300and qmin = 0.8 show that morphological genera-tion pays off, but the effect is smaller than in otherdocument collections.
This is probably due to thefact that the large and redundant web often con-tains answers with the exact word forms from thequestion.
Another interesting aspect found in Ta-ble 2 is that even with d = 500 results still im-prove; it remains to be seen at what number ofdocuments performance stays stable (or maybe de-grades).
The impact of a lower quality thresholdqmin was not significant.
This might change if oneoperates with a larger parameter d and more infer-ential rules.EACL 2006 Workshop on Multilingual Question Answering - MLQA0666Table 2: Influence of parameters on InSicht-W3 results.
Parameter d is the maximal number of doc-uments used from the search engine results (see Sect.
3.4); qmin is the minimal query network quality.parameter setting resultsd qmin morph.generat.#docs.from firstlevelper quest.#sent.matchingpre.
queryper quest.%docs.withmatchingsent.right non-emptyansw.
(%)inexactansw.
(%)wrongnon-emptyansw.
(%)K1100 0.9 frequent 18.1 30.5 59.7 18.8 4.1 1.8 0.13200 0.9 frequent 34.4 59.0 60.4 20.6 3.5 2.9 0.14300 0.9 frequent 45.5 76.9 60.2 22.4 3.5 2.4 0.16300 0.8 frequent 48.4 84.2 61.7 22.4 3.5 2.4 0.16300 0.8 none 57.3 91.3 61.5 19.4 3.5 3.5 0.12500 0.8 frequent 68.9 119.8 62.2 22.9 2.9 2.4 0.18Error analysis and classification is difficult assoon as one steps to the vast web.
One could startwith a classification of error reasons for wrongempty answers.
The error class that can be mosteasily determined is that the search engine re-turned no results for the preselection query.
40of the 170 questions (23.5%) belong to this class.This might surprise users that believe that theycan find nearly every bit of information on theweb.
But there are areas where this assumption iswrong: many QA@CLEF questions relate to veryspecific events of the year 1994 or 1995.
Twelveyears ago, the web publishing rate was much lowerthan today; and even if the relevant pieces of in-formation were on the web at that time (or in thefollowing years), they might have been moved toarchives or removed in recent years.
Other er-ror reasons are very difficult and labor-intensiveto separate:1.
Result pages from the first level do not con-tain an answer, but if one requests more docu-ments (e.g.
by raising the parameter d) resultpages with an answer will be found.2.
Result pages from the first level contain ananswer, but one or more components of thetextual QA system cause that the answeris not found.
Subclasses could be definedby adapting the hierarchy that Hartrumpf(2005a) applied to evaluate InSicht.On average, the 40th search engine result isthe first that contains a right answer (in the bestInSicht-W3 run).6 Although this result is for thesystem and not for a real user (they differ in theirstrengths and weaknesses in NL understanding), itindicates that human users of web search enginesmight find the correct answer quite late if at all.This is a strong argument for more advanced, sec-ond level tools like a semantic QA system: Howmany users will read through 40 search engine re-sults and (possibly) 40 underlying web pages?The answer time of InSicht-W3 currentlyranges between 15 and 1200 seconds.
The doc-ument download time was excluded because it de-pended on too many external factors (caches, in-tranet effects, parallel vs. sequential downloads)to be measured consistently and reliably.5 Related WorkThere are few QA systems for German.
The sys-tem described by Neumann and Xu (2003) workson German web pages.
Its general approach dif-fers from InSicht because it relies on shallow, butrobust methods, while InSicht builds on sentenceparsing and semantic representations.
In this re-spect, InSicht resembles the (English) textual QAsystem presented by Harabagiu et al (2001).
Incontrast to InSicht, this system applies a theoremprover and a large knowledge base to validate can-didate answers.
An interesting combination ofweb and textual QA is presented by Vicedo et al(2005): English and Spanish web documents areused to enhance textual QA in Spanish.6This high number is in part an artifact of the preselectionquery generation.
In a more thorough analysis, human userscould be given the NL question and be asked to formulate acorresponding search engine query.EACL 2006 Workshop on Multilingual Question Answering - MLQA0667One of the first web QA systems for Englishwas Mulder (Kwok et al, 2001).
Mulder parsesonly the text snippets returned by the search en-gine, while InSicht-W3 parses the underlying webpages because the text snippets often have omis-sions (?...?)
so that full parsing becomes problem-atic or impossible.
InSicht-W3?s approach needsmore time, especially if the web pages are not inthe local cache that InSicht-W3 maintains in orderto reduce its bandwidth requirements.6 Conclusion and PerspectivesIn this paper, an existent textual QA system wasextended and modified to work successfully onthe German web as a virtual document collection.The main results are: precision-oriented exten-sions and experimentally derived parameter set-tings are needed to achieve similar performanceon the vast web as on small-sized document col-lections that show higher homogeneity and qualityof the contained texts; taking a semantic QA sys-tem to the web is feasible as demonstrated in thispaper, but answering a question is still expensivein terms of bandwidth and CPU time.There are several interesting directions for fu-ture work.
The first level of InSicht-W3 can be im-proved by finding a better suited search engine orby building and running a new one in a distributedmanner.
Ideally, it should support arbitrarily com-plex Boolean queries, parameterized proximityoperators like NEAR/N (N ?
{1,2, .
.
.
,100}), oreven linguistically informed operators like same-sentence and same-paragraph.The second level (the textual QA system) canbe improved by acquiring more inferential knowl-edge to allow better query expansion.
The match-ing approach can be extended from the unit sen-tence to a larger linguistic unit like paragraph, text,and even text collection.
Distributed architecturesand algorithms can reduce answer times.ReferencesSanda Harabagiu, Dan Moldovan, Marius Pas?ca, RadaMihalcea, Mihai Surdeanu, Ra?zvan Bunescu, Rox-ana G?
?rju, Vasile Rus, and Paul Mora?rescu.
2001.The role of lexico-semantic feedback in open-domain textual question-answering.
In Proceedingsof the 39th Annual Meeting of the Association forComputational Linguistics (ACL-2001), pages 274?281, Toulouse, France.Sven Hartrumpf, Hermann Helbig, and Rainer Oss-wald.
2003.
The semantically based computer lex-icon HaGenLex ?
Structure and technological en-vironment.
Traitement automatique des langues,44(2):81?105.Sven Hartrumpf.
2003.
Hybrid Disambiguation inNatural Language Analysis.
Der Andere Verlag, Os-nabru?ck, Germany.Sven Hartrumpf.
2005a.
Question answering usingsentence parsing and semantic network matching.
In(Peters et al, 2005), pages 512?521.Sven Hartrumpf.
2005b.
University of Hagenat QA@CLEF 2005: Extending knowledge anddeepening linguistic processing for question an-swering.
In Carol Peters, editor, Results of theCLEF 2005 Cross-Language System EvaluationCampaign, Working Notes for the CLEF 2005 Work-shop.
Centromedia, Wien, Austria.Hermann Helbig.
2006.
Knowledge Representationand the Semantics of Natural Language.
Springer,Berlin.Jesu?s Herrera, Anselmo Pen?as, and Felisa Verdejo.2005.
Question answering pilot task at CLEF 2004.In (Peters et al, 2005), pages 581?590.Cody Kwok, Oren Etzioni, and Daniel S. Weld.
2001.Scaling question answering to the web.
ACM Trans-actions on Information Systems, 19(3):242?262.Bernardo Magnini, Alessandro Vallin, Christelle Ay-ache, Gregor Erbach, Anselmo Pen?as, Maartende Rijke, Paulo Rocha, Kiril Simov, and RichardSutcliffe.
2005.
Overview of the CLEF 2004 mul-tilingual question answering track.
In (Peters et al,2005), pages 371?391.Gu?nter Neumann and Bogdan Sacaleanu.
2005.Experiments on robust NL question interpretationand multi-layered document annotation for a cross-language question/answering system.
In (Peters etal., 2005), pages 411?422.Gu?nter Neumann and Feiyu Xu.
2003.
Mining an-swers in German web pages.
In Proceedings of theInternational Conference on Web Intelligence (WI-2003), Halifax, Canada.Carol Peters, Paul Clough, Julio Gonzalo, Gareth J. F.Jones, Michael Kluck, and Bernardo Magnini, ed-itors.
2005.
Multilingual Information Access forText, Speech and Images: 5th Workshop of theCross-Language Evaluation Forum, CLEF 2004,volume 3491 of LNCS.
Springer, Berlin.Jose?
L. Vicedo, Maximiliano Saiz, Rube?n Izquierdo,and Fernando Llopis.
2005.
Does English helpquestion answering in Spanish?
In (Peters et al,2005), pages 552?556.EACL 2006 Workshop on Multilingual Question Answering - MLQA0668
