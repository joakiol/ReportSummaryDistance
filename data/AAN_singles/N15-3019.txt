Proceedings of NAACL-HLT 2015, pages 91?95,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsCroVeWA: Crosslingual Vector-Based Writing AssistanceHubert Soyer1?, Goran Topi?c1, Pontus Stenetorp2?and Akiko Aizawa11National Institute of Informatics, 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan2University of Tokyo, 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan{soyer,goran topic,aizawa}@nii.ac.jp pontus@stenetorp.seAbstractWe present an interactive web-based writ-ing assistance system that is based on recentadvances in crosslingual compositional dis-tributed semantics.
Given queries in Japaneseor English, our system can retrieve semanti-cally related sentences from high quality En-glish corpora.
By employing crosslinguallyconstrained vector space models to representphrases, our system naturally sidesteps sev-eral difficulties that would arise from directword-to-text matching, and is able to providenovel functionality like the visualization of se-mantic relationships between phrases interlin-gually and intralingually.1 IntroductionWriting high quality texts in a foreign language re-quires years of study and a deep comprehension ofthe language.
With a society that is becoming moreand more international, the ability to express ideasin English has become the basis of fruitful commu-nication and collaboration.In this work, we propose a tool to provide non-native speakers of English with help in their transla-tion or writing process.
Instead of relying on man-ually created dictionaries, many existing tools lever-age parallel bilingual corpora, using a concordancerto provide translation suggestions together with theircontexts.
Notable examples relevant to this demon-stration are linguee.com and tradooit.com.Given a word or a phrase in a foreign language,?Currently at Google DeepMind.
?Currently at University College London.these systems present example sentences contain-ing the query in the source language as well as thetarget language, showing the correct usage of theword/phrase, and at the same time providing trans-lation candidates.Many applications rely on direct word-to-textmatching and are therefore prone to missing seman-tically similar contexts that, although similar andrelevant, do not share any words with the query.
In-stead of matching words directly, we propose a sys-tem that employs crosslingually constrained vectorrepresentations (embeddings) of words and phrasesto retrieve English sentences that are similar to agiven phrase or word in a different language (query).These vector representations not only allow for effi-cient crosslingual lookups in databases consisting ofmillions of sentences, but can also be employed tovisualize intralingual and interlingual semantic rela-tionships between phrases.2 Related WorkVarious types of neural network models have beenproposed to induce distributed word representationsand leveraging these word embeddings as featureshas proven viable in achieving state-of-the-art re-sults for a variety of tasks (Baroni et al, 2014; Col-lobert and Weston, 2008).Recently, methods that attempt to compose em-beddings not only of words but of whole phrases (Leand Mikolov, 2014; Socher et al, 2011) have en-abled vector representations to be applied for tasksthat are defined over phrases, sentences, or even doc-uments.
The most relevant work for this paper arerecent approaches that allow for the induction of91word and phrase embeddings not only from mono-lingual text but using bilingual resources to con-strain vector representations crosslingually.
(Soyeret al, 2015; Hermann and Blunsom, 2014; Cho etal., 2014; Chandar A P et al, 2014).
Embeddingslearned using these methods not only possess mean-ingful properties within a language, but also inter-lingually.3 Crosslingual Vector-Based WritingAssistance (CroVeWA)Our system harnesses crosslingually constrainedword and phrase representations to retrieve and vi-sualize sentences related to given queries, using dis-tances in the word/phrase vector space as a measureof semantic relatedness.
Currently, our system sup-ports the lookup of Japanese and English queries inEnglish text.Our system encourages refining retrieved resultsand viewing relations in different contexts by sup-porting multiple queries.
All queries and their cor-responding results are visualized together to aid abetter understanding of their relationships.
To il-lustrate the differences to phrase vector-based sen-tence retrieval, we also offer a retrieval optionbased on direct word-to-text matching using theEDICT Japanese-English dictionary (Breen, 2004)and Apache Lucene1for sentence retrieval.To the best of our knowledge, our system is thefirst to provide writing assistance using vector rep-resentations of words and phrases.3.1 Inducing Crosslingually Constrained WordRepresentationsWe employ the approach presented in Soyer et al(2015) to learn bilingually constrained representa-tions of Japanese and English words.
The methoddraws from sentence-parallel bilingual text to con-strain word vectors crosslingually, handles text ona phrase level ensuring the compositionality of theinduced word embeddings, and is agnostic to howphrase representations are assembled from word rep-resentations.
In addition, unlike previously pro-posed models, the model can draw not only frombilingual sentence aligned data but also from arbi-trary monolingual data in either language.
Figure 11https://lucene.apache.org/core/depicts an overview over the method.The method optimizes the vectors that representeach word in subject to a bilingual and a monolin-gual objective.
These objectives operate on a phraselevel, where each phrase is represented by a singlevector.
Composing a single vector of a given phrasemeans looking up the word vector for each wordin a lookup table shared among all sentences of thephrase-language, and applying a composition func-tion to collapse all word vectors of a phrase into asingle phrase vector.
The composition function usedin this work is the arithmetic mean.The bilingual objective ensures that vectors ofJapanese sentences are close to the vectors of theirEnglish translations present in the sentence-parallelcorpus.
It minimizes the squared euclidean distancebetween the sentence vector of a Japanese sentenceand the vector of its English translation.
With thearithmetic mean as the sentence composition func-tion, this notion of translational closeness is directlypropagated back into the embeddings of the indi-vidual words that appear in each sentence.
If aJapanese and an English word consistently co-occurin the translation pairs of the sentence-parallel cor-pus, their vectors will be moved close to each other,capturing that they are likely to be related in mean-ing.The monolingual objective exploits the insightthat sub-phrases generally tend to be closer in mean-ing to the phrases they are contained in, than tomost other arbitrary phrases.
It punishes a large eu-clidean distance between the vector representationof a phrase and its sub-phrase, and at the same timerewards a large distance between the vector of thephrase and the embedding of another phrase chosenat random.Both the monolingual objective and the bilingualobjective are combined to leverage monolingual andbilingual resources at the same time.
Using thearithmetic mean to compose phrase vectors discardsword-order as well as sentence-length information,and allows our system to handle even single wordsor ungrammatical sequences of words.Currently we use Japanese and English resourcesto learn word embeddings, but plan to add more lan-guages in the future.
The bilingual sentence-parallel92Figure 1: Overview of the method that was used to induce crosslingually constrained word representations.
Themethod can draw from bilingual sentence-parallel data as well as monolingual data.resource used is the ASPEC corpus2, which fea-tures sentence-aligned text from scientific paper ab-stracts.
For monolingual data, we use subsets of theJapanese and English Wikipedia.3.2 Finding Related English Sentences for aJapanese QueryInducing crosslingually constrained word represen-tations leaves us with two sets of vectors, one cor-responding to Japanese words and one to Englishwords.
Given a query in Japanese, we look up thevectors for each individual query word, composethem into a single query vector and find the near-est neighbors in a set of pre-computed vectors ofEnglish sentences.
Since the word and phrase vec-tors are crosslingually constrained, we expect the re-trieved English nearest neighbors to be semanticallyrelated to the Japanese query.
In contrast to conven-tional word matching techniques, our vector-basedapproach does not require Japanese translations ofthe English sentences we consider during the search,nor does it require a Japanese-English dictionary.Another difference to word matching techniquesfollows from the way word vectors are arrangedwithin the same language.
Generally, words that2http://orchid.kuee.kyoto-u.ac.jp/ASPEC/appear in similar contexts will be placed close toeach other in the vector space, and so the differ-ence between choosing a word over a closely re-lated neighbor will be relatively small when com-posing a phrase vector.
Interchangeability of syn-onyms or semantically similar words is therefore au-tomatically supported as a property of the word rep-resentations, and the system can retrieve sentencessimilar in meaning regardless of the exact choice ofwords.Following Mikolov et al (2013) we use the co-sine similarity as a measure of similarity betweenembeddings.
For nearest neighbor retrieval we em-ploy the FLANN Python module (Muja and Lowe,2009) which exploits the clustered nature of vectorrepresentations to efficiently find an approximate setof nearest neighbors.3.3 VisualizationIn contrast to direct word matching, vector-representation-based matching retrieves not only alist of related sentences, but also a semantic vectorspace position for each query and result.
In order tovisualize the high-dimensional output vectors of thesearch we reduce their dimensionality to two.Generally, reducing dimensionality involves dis-93carding information.
Commonly employed methodsfor this task such as, Principal Component Analy-sis or t-SNE (Van der Maaten and Hinton, 2008),failed to provide satisfactory results for our pur-poses.
Instead, we apply a novel variant of multi-dimensional scaling (Kruskal, 1964) where we pri-oritize the preservation of query-to-result distancesover the preservation of result-to-result distances.This yields a visually more interpretable output,with queries being the points of orientation.An interactive plot of the resulting 2D points, il-lustrates the relationships between the different sen-tences, puts the retrieved results into context andaids the user?s understanding of the meaning and re-latedness of sentences.
Being able to visualize theserelationships is another aspect that sets our systemapart from previously proposed word-to-text match-ing approaches.3.4 Demonstration SystemWe will guide the audience through the featuresof our application using a set of example queriesthat highlight the merits and drawbacks of vector-based crosslingual sentence retrieval.
Based onthese query examples we will introduce novel func-tionality, such as our system?s visualization of se-mantic relationships or its feature for query auto-generation.
The interactive nature of our tool al-lows us to incorporate requests and comments intothe demonstration, helping to clarify questions andto explain properties of our system.Our system is built as a web application and there-fore only requires the user to have a modern browserand an Internet connection.
Figure 2 shows a screen-shot of the user interface, which consists of thequery input bar at the top of the screen, a result liston the left and a visualization panel on the right.
Forclarity, we have annotated the screenshot: annota-tions with white background show results and theirpositions in the visualization, while the ones withred background provide translations of the Japanesequeries.In the query input bar users can customize thesearch through a variety of options.
Via the queryinput field a user can submit Japanese queries whichcan be a single word, a phrase or any sequence ofwords.
Pushing the Auto-Generate button will splitthe entered text into semantically related groups ofwords and submit these groups as separate queriesto visualize the relatedness of different parts of theentered text.
Since not every potential user might befamiliar with Japanese we provide an English sim-ulation mode to input English queries and retrieveEnglish results.
We refer to this mode as simula-tion because the lookup from English to English isnot crosslingual.
For comparison, and as an ex-tension to the vector-based sentence retrieval, wealso provide a dictionary-based word-to-text match-ing search mode using the Japanese-English EDICTdictionary.
Clicking the Samples button invokes adialog that presents example queries to choose from.We currently provide three corpora to search, whereeach corpus covers a different domain.
The ASPECcorpus consists of Japanese and English scientificpaper abstracts related to natural sciences and en-gineering, the Wikipedia corpus comprises 10 mil-lion randomly selected sentences from the EnglishWikipedia, and the PubMed corpus features 5 mil-lion sentences from the PubMed Central collectionof medical paper abstracts.Queries make up the first entries in the result list,with fully colored backgrounds.
In the visualizationpanel queries are represented by larger points.
If twoor more queries have been submitted we additionallyprovide a Query Average to retrieve results that arerelated to all submitted queries.
Every result entrythat follows the queries is colored according to theclosest query from those that retrieved it.
The filllevel of the background of each result item indicatesits similarity to the Query Average.
Hovering thecursor over a list entry will highlight its correspond-ing point and vice versa.
Clicking on a list entrywill auto-generate queries from its text, clusteringrelated words together to provide a visualization ofthe different topics the text consists of.The annotations in Figure 2 illustrate how the sys-tem?s visualization can aid a user in understandingthe meaning of a sentence.
The distance betweena result and a query indicates their semantic close-ness.
Sentences located close to a query point arestrongly related to the query (canal and river or sub-way station and railway station), phrases in betweenthe queries feature aspects of both submitted queries(flooding, subway and city).94Figure 2: Annotated screenshot of CroVeWA.AcknowledgementsThis work was supported by the Data Centric ScienceResearch Commons Project at the Research Organiza-tion of Information and Systems and by the Japan Societyfor the Promotion of Science KAKENHI Grant Number13F03041.ReferencesMarco Baroni, Georgiana Dinu, and Germ?an Kruszewski.2014.
Don?t count, predict!
A systematic compari-son of context-counting vs. context-predicting seman-tic vectors.
In Proceedings of the 52nd ACL, pages238?247.James Breen.
2004.
JMDict: a Japanese-multilingualdictionary.
In Proceedings of the Workshop on Multi-lingual Linguistic Ressources, pages 71?79.
ACL.Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle,Mitesh Khapra, Balaraman Ravindran, Vikas CRaykar, and Amrita Saha.
2014.
An autoencoderapproach to learning bilingual word representations.In Z. Ghahramani, M. Welling, C. Cortes, N.D.Lawrence, and K.Q.
Weinberger, editors, Advancesin Neural Information Processing Systems 27, pages1853?1861.
Curran Associates, Inc.Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.
2014.
Learning phrase repre-sentations using RNN encoder?decoder for statisticalmachine translation.
In Proceedings of EMNLP 2014,pages 1724?1734.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof the 25th ICML, pages 160?167.
ACM.Karl Moritz Hermann and Phil Blunsom.
2014.
Multilin-gual models for compositional distributed semantics.In Proceedings of the 52nd ACL, pages 58?68.Joseph B Kruskal.
1964.
Multidimensional scaling byoptimizing goodness of fit to a nonmetric hypothesis.Psychometrika, 29(1):1?27.Quoc Le and Tomas Mikolov.
2014.
Distributed repre-sentations of sentences and documents.
In Proceed-ings of The 31st ICML, pages 1188?1196.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In Proceedings of Workshop atICLR.Marius Muja and David G. Lowe.
2009.
Fast approxi-mate nearest neighbors with automatic algorithm con-figuration.
In International Conference on ComputerVision Theory and Application VISSAPP?09), pages331?340.
INSTICC Press.Richard Socher, Jeffrey Pennington, Eric H Huang, An-drew Y Ng, and Christopher D Manning.
2011.
Semi-supervised recursive autoencoders for predicting sen-timent distributions.
In Proceedings of EMNLP, pages151?161.
Association for Computational Linguistics.Hubert Soyer, Pontus Stenetorp, and Aizawa Akiko.2015.
Leveraging monolingual data for crosslingualcompositional word representations.
In Proceedingsof ICLR.
to appear.Laurens Van der Maaten and Geoffrey Hinton.
2008.
Vi-sualizing data using t-SNE.
JMLR, 9(2579-2605):85.95
