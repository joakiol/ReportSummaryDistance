SESS ION 8: SPEECH IIKai-Fu LeeApple Computer Inc.20525 Mariani Avenue, MS 7lABCupertino, CA 95014This session contains four papers that describe newtechniques and recent advances in acoustic modeling.
This is anextremely important area of research.
Throughout he pasttwenty years, as computers became more powerful and speechdata more abundant, new directions in acoustic modeling furtheradvanced the state of the art.
The first two papers describenovel techniques that may lead to paradigm shifts.
The secondtwo papers propose extensions to current echniques, in orderto deal with the variability found in very large vocabularies.The fwst paper, presented by Steve Austin of BBN, describeda method for integrating neural networks (NNs) and hiddenMarkov models (HMMs).
The motivation of this work was toovercome two problems of HMMs: their conditionalindependence assumptions and the difficulty in integratingsegmental features.
These problems are more easily addressedusing neural networks that examine one segment rather thanone frame at a time.
However, a full search strategy usingsegmental models is currently prohibitive, as discovered byBBN and BU from their work on stochastic segment models.Austin proposed to combine HMMs and NNs by using HMMs topropose the N-best sentence hypotheses.
These hypotheseswere rescored using both HMMs and NNs.
Finally, the HMMand NN scores were llnearly combined to determine the final topchoice.
The HMMs were the standard BBN context-dependantmodels.
A single NN was constructed to discriminate allcontext-independent phones, using a fixed-length segmentresampled from the actual segment proposed by the HMM N-best algorithm.
The linear combination weights were trainedfrom a set of tuning sentences not used in the training.Although the resulting BINs performed substantially worse thanthe HMMs, the combined result was slightly better than theHMMs.
One of the major contributions of this paper is a newparadigm in integrating heterogeneous knowledge sources (thesame strategy was used by Ostendorf, et al in a paper inSession 2).Mari Ostendorf rom BU presented the second paper, "ADynamical System Approach to Continuous SpeechRecognition."
The motivation of this work is very similar tothe first paper-- improved time correlation modeling.
Theproposed approach makes the assumptions that speech ismodeled as a Gaussian process at the frame-rate l vel, and thatthe underlying trajectory in phase space is not invariant undertime-warping transformations.
The speech model used is thenbased on a stochastic linear system model which incorporates amodeling/observation noise term.
This system was evaluatedon the TIMIT database, and slight improvements over previoustechniques were reported.
Because the number of systemparameters was constrained by the correlation invarianceassumption, it appeared that this approach as greater potentialwith increased speech coefficients.
In response to a question,Ostendorf pointed out that the training and test sets includeddifferent speakers and sentences.The third paper, presented by Hsiao-Wuen Hon of CMU,described recent improvements in the vocabulary-independentwork at CMU.
The goal of this work is to develop acousticmodels that work well on any task, without task-specifictraining.
This requires rich acoustic models that generalize tonew words.
Previously, Hun has reported about a 30% increasein errors for vocabulary-independent r cognition.
In this work,he incorporated second order differences, additional training,inter-word triphones, and decision-tzee clustering, and obtaineda 13% reduction of errors from a vocabulary-dependent sys em.One of the major findings was that inter-word triphones areeffective even when training and testing tasks are disjoint.This disproved the suspicion that inter-word triphones areeffective because they capture grammatical constraints.
Asecond finding was that decision tree based allophones (similarto the final paper) reduced errors substantially, while an earlierstudy from CMU found little benefit.
The main difference isthat this study started with many more detailed but poorlytrained models, which benefited from the generalizationcapabilities of the decision tree.
The final result of a lowervocabulary-independent result was surprisingly good, but itremains to be verified on the same speakers.
Also, it remainsto be shown that the latest vocabulary-dependent techniques(semi-continuous models, sex-dependent models) are effectiveunder vocabulary-dependent conditions.The final paper, presented by P.S.
Gopalakrishnan of IBM,gives a detailed treatment of decision tree clustering ofallophones.
Their approach involves first collecting a largecorpus of speech, and then automatically segmenting intophone labels, and storing the five left and five right phoneticneighbors.
This ensemble of very detailed phonetic segmentswere then clustered using a decision tree that asked questionsabout the classes of phonetic neighbors.
The leaf nodes of thistree were used as the final allophonic units in a 5000-wordcontinuous peech recognition system.
During recognition,simpler models were used until the next word is hypothesized.At that point, the current word is rescored with the appropriatemodels.
The allophonic models yielded a substantially betterresult than phones and an IBM implementation f within-wordtriphones.
It was also shown that extending contexts to fiveleft and right neighbors gave some improvement, and that thecurrent training dataset could support about 45 allophonicmodels per phone.
The algorithm in this paper differed fromthe previous paper in several minor ways.
Both showed that asthe vocabulary increased, decision tree based algorithms thatutilize a priori knowledge about context can improvegeneralization.
(An M1T paper in session 2 also used decisiontree clustering.)
Some questions were raised about thedifferences among the standard triphones, IBM triphones, andIBM allophones that ask only about one left and rightneighbors.
Compared to the standard (inter-word) triphones,IBM triphones are within-word only and do not utilize acomplicated smoothing algorithm, while the IBM allophonesthat ask only about one neighbor are clustered models (similar247to CMU's generalized triphones).
So the latter might be abetter baseline compared to the current DARPA systems, whichmakes the contribution of the 5-neighbor allophone systemsmaller, but still appreciable.With only a few minutes left, the discussion centered aroundthe issue: is context-dependent modeling better than complexcontext-independent models?
The consensus was that whilecontext-independent models are more easily trained in moredetail (more mixture densities, states, etc.
), they lack theconstraints of context-dependent models.
The powerfulcontextual constraints make context-dependent models sharperand more accurate.
The last two papers and other earlier workclearly illustrated this point.
However, the first two papersused only context-independent models and still achieved goodresults.
This suggests that the new approaches in the first twopapers are very promising.
On the other hand, theirapplicability relies upon their extensibility to context-dependent modeling.248
