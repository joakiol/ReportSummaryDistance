Evaluation of Features for Sentence Extractionon Different Types of CorporaChikashi Nobata?, Satoshi Sekine?
and Hitoshi Isahara??
Communications Research Laboratory3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan{nova, isahara}@crl.go.jp?
Computer Science Department, New York University715 Broadway, 7th floor, New York, NY 10003, USAsekine@cs.nyu.eduAbstractWe report evaluation results for our sum-marization system and analyze the result-ing summarization data for three differ-ent types of corpora.
To develop a ro-bust summarization system, we have cre-ated a system based on sentence extractionand applied it to summarize Japanese andEnglish newspaper articles, obtained someof the top results at two evaluation work-shops.
We have also created sentence ex-traction data from Japanese lectures andevaluated our system with these data.
Inaddition to the evaluation results, we an-alyze the relationships between key sen-tences and the features used in sentenceextraction.
We find that discrete combi-nations of features match distributions ofkey sentences better than sequential com-binations.1 IntroductionOur ultimate goal is to create a robust summariza-tion system that can handle different types of docu-ments in a uniform way.
To achieve this goal, wehave developed a summarization system based onsentence extraction.
We have participated in eval-uation workshops on automatic summarization forboth Japanese and English written corpora.
We havealso evaluated the performance of the sentence ex-traction system for Japanese lectures.
At both work-shops we obtained some of the top results, and forthe speech corpus we obtained results comparablewith those for the written corpora.
This means thatthe features we use are worth analyzing.Sentence extraction is one of the main methodsrequired for a summarization system to reduce thesize of a document.
Edmundson (1969) proposed amethod of integrating several features, such as thepositions of sentences and the frequencies of wordsin an article, in order to extract sentences.
He man-ually assigned parameter values to integrate featuresfor estimating the significance scores of sentences.On the other hand, machine learning methods canalso be applied to integrate features.
For sentenceextraction from training data, Kupiec et al (1995)and Aone et al (1998) used Bayes?
rule, Lin (1999)and Nomoto and Matsumoto (1997) generated a de-cision tree, and Hirao et al (2002) generated anSVM.In this paper, we not only show evaluation resultsfor our sentence extraction system using combina-tions of features but also analyze the features for dif-ferent types of corpora.
The analysis gives us someindication about how to use these features and howto combine them.2 Summarization dataThe summarization data we used for this researchwere prepared from Japanese newspaper articles,Japanese lectures, and English newspaper articles.By using these three types of data, we could com-pare two languages and also two different types ofcorpora, a written corpus and a speech corpus.2.1 Summarization data from Japanesenewspaper articlesText Summarization Challenge (TSC) is an evalua-tion workshop for automatic summarization, whichis run by the National Institute of Informatics inJapan (TSC, 2001).
Three tasks were presented atTSC-2001: extracting important sentences, creatingsummaries to be compared with summaries preparedby humans, and creating summaries for informa-tion retrieval.
We focus on the first task here, i.e.,the sentence extraction task.
At TSC-2001, a dryrun and a formal run were performed.
The dry rundata consisted of 30 newspaper articles and manu-ally created summaries of each.
The formal run dataconsisted of another 30 pairs of articles and sum-maries.
The average number of sentences per articlewas 28.5 (1709 sentences / 60 articles).
The news-paper articles included 15 editorials and 15 news re-ports in both data sets.
The summaries were createdfrom extracted sentences with three compression ra-tios (10%, 30%, and 50%).
In our analysis, we usedthe extraction data for the 10% compression ratio.In the following sections, we call these summa-rization data the ?TSC data?.
We use the TSC dataas an example of a Japanese written corpus to eval-uate the performance of sentence extraction.2.2 Summarization data from JapaneselecturesThe speech corpus we used for this experimentis part of the Corpus of Spontaneous Japanese(CSJ) (Maekawa et al, 2000), which is being cre-ated by NIJLA, TITech, and CRL as an ongoingjoint project.
The CSJ is a large collection of mono-logues, such as lectures, and it includes transcrip-tions of each speech as well as the voice data.
Weselected 60 transcriptions from the CSJ for both sen-tence segmentation and sentence extraction.
Sincethese transcription data do not have sentence bound-aries, sentence segmentation is necessary beforesentence extraction.
Three annotators manually gen-erated sentence segmentation and summarization re-sults.
The target compression ratio was set to 10%.The results of sentence segmentation were unifiedto form the key data, and the average number ofsentences was 68.7 (4123 sentences / 60 speeches).The results of sentence extraction, however, werenot unified, but were used separately for evaluation.In the following sections, we call these summa-rization data the ?CSJ data?.
We use the CSJ data asan example of a Japanese speech corpus to evaluatethe performance of sentence extraction.2.3 Summarization data from Englishnewspaper articlesDocument Understanding Conference (DUC) is anevaluation workshop in the U.S. for automatic sum-marization, which is sponsored by TIDES of theDARPA program and run by NIST (DUC, 2001).At DUC-2001, there were two types of tasks:single-document summarization (SDS) and multi-document summarization (MDS).
The organizers ofDUC-2001 provided 30 sets of documents for a dryrun and another 30 sets for a formal run.
These datawere shared by both the SDS and MDS tasks, andthe average number of sentences was 42.5 (25779sentences / 607 articles).
Each document set had atopic, such as ?Hurricane Andrew?
or ?Police Mis-conduct?, and contained around 10 documents rele-vant to the topic.
We focus on the SDS task here, forwhich the size of each summary output was set to100 words.
Model summaries for the articles werealso created by hand and provided.
Since these sum-maries were abstracts, we created sentence extrac-tion data from the abstracts by word-based compar-ison.In the following sections, we call these summa-rization data the ?DUC data?.
We use the DUC dataas an example of an English written corpus to eval-uate the performance of sentence extraction.3 Overview of our sentence extractionsystemIn this section, we give an overview of our sentenceextraction system, which uses multiple components.For each sentence, each component outputs a score.The system then combines these independent scoresby interpolation.
Some components have more thanone scoring function, using various features.
Theweights and function types used are decided by op-timizing the performance of the system on trainingdata.Our system includes parts that are either commonto the TSC, CSJ, and DUC data or specific to one ofthese data sets.
We stipulate which parts are specific.3.1 Features for sentence extraction3.1.1 Sentence positionWe implemented three functions for sentence po-sition.
The first function returns 1 if the position ofthe sentence is within a given threshold N from thebeginning, and returns 0 otherwise:P1.
Scorepst(Si)(1 ?
i ?
n) = 1(if i < N)= 0(otherwise)The threshold N is determined by the number ofwords in the summary.The second function is the reciprocal of the po-sition of the sentence, i.e., the score is highest forthe first sentence, gradually decreases, and goes to aminimum at the final sentence:P2.
Scorepst(Si) =1iThese first two functions are based on the hypoth-esis that the sentences at the beginning of an articleare more important than those in the remaining part.The third function is the maximum of the recipro-cal of the position from either the beginning or theend of the document:P3.
Scorepst(Si) = max(1i ,1n?
i+ 1)This method is based on the hypothesis that the sen-tences at both the beginning and the end of an articleare more important than those in the middle.3.1.2 Sentence lengthThe second type of scoring function uses sen-tence length to determine the significance of sen-tences.
We implemented three scoring functions forsentence length.
The first function only returns thelength of each sentence (Li):L1.
Scorelen(Si) = LiThe second function sets the score to a negativevalue as a penalty when the sentence is shorter thana certain length (C):L2.
Scorelen(Si) = 0 (if Li ?
C)Li ?
C (otherwise)The third function combines the above two ap-proaches, i.e., it returns the length of a sentence thathas at least a certain length, and otherwise returns anegative value as a penalty:L3.
Scorelen(Si) = Li (if Li ?
C)= Li ?
C (otherwise)The length of a sentence means the number of let-ters, and based on the results of an experiment withthe training data, we set C to 20 for the TSC andCSJ data.
For the DUC data, the length of a sen-tence means the number of words, and we set C to10 during the training stage.3.1.3 Tf*idfThe third type of scoring function is based on termfrequency (tf) and document frequency (df).
We ap-plied three scoring functions for tf*idf, in which theterm frequencies are calculated differently.
The firstfunction uses the raw term frequencies, while theother two are two different ways of normalizing thefrequencies, as follows, where DN is the number ofdocuments given:T1.
tf*idf(w) = tf(w) log DNdf(w)T2.
tf*idf(w) = tf(w)-1tf(w) logDNdf(w)T3.
tf*idf(w) = tf(w)tf(w)+1 logDNdf(w)For the TSC and CSJ data, we only used the thirdmethod (T3), which was reported to be effectivefor the task of information retrieval (Robertson andWalker, 1994).
The target words for these functionsare nouns (excluding temporal or adverbial nouns).For each of the nouns in a sentence, the system cal-culates a Tf*idf score.
The total score is the sig-nificance of the sentence.
The word segmentationwas generated by Juman3.61 (Kurohashi and Nagao,1999).
We used articles from the Mainichi newspa-per in 1994 and 1995 to count document frequen-cies.For the DUC data, the raw term frequency (T1)was selected during the training stage from amongthe three tf*idf definitions.
A list of stop words wereused to exclude functional words, and articles fromthe Wall Street Journal in 1994 and 1995 were usedto count document frequencies.3.1.4 HeadlineWe used a similarity measure of the sentence tothe headline as another type of scoring function.
Thebasic idea is that the more words in the sentenceoverlap with the words in the headline, the more im-portant the sentence is.
The function estimates therelevance between a headline (H) and a sentence(Si) by using the tf*idf values of the words (w) inthe headline:Scorehl(Si) =?w?H?Sitf(w)tf(w)+1 logDNdf(w)?w?Htf(w)tf(w)+1 logDNdf(w)We also evaluated another method based on thisscoring function by using only named entities (NEs)instead of words for the TSC data and DUC data.Only the term frequency was used for NEs, becausewe judged that the document frequency for an entitywas usually quite small, thereby making the differ-ences between entities negligible.3.1.5 PatternsFor the DUC data, we used dependency patternsas a type of scoring function.
These patterns wereextracted by pattern discovery during informationextraction (Sudo et al, 2001).
The details of this ap-proach are not explained here, because this featureis not among the features we analyze in Section 5.The definition of the function appears in (Nobata etal., 2002).3.2 Optimal weightOur system set weights for each scoring function inorder to calculate the total score of a sentence.
Thetotal score (Si) is defined from the scoring functions(Scorej()) and weights (?j) as follows:TotalScore(Si) =?j?jScorej(Si) (1)We estimated the optimal values of these weightsfrom the training data.
After the range of eachweight was set manually, the system changed thevalues of the weights within a range and summarizedthe training data for each set of weights.
Each scorewas recorded after the weights were changed, andthe weights with the best scores were stored.A particular scoring method was also selected inthe cases of features with more than one definedscoring methods.
We used the dry run data fromeach workshop as TSC and DUC training data.
Forthe TSC data, since the 30 articles contained 15 ed-itorials and 15 news reports, we estimated optimalvalues separately for editorials and news reports.
Forthe CSJ data, we used 50 transcriptions for trainingand 10 for testing, as mentioned in Section 2.2.Table 1: Evaluation results for the TSC data.Ratio 10% 30% 50% Avg.System 0.363 (1) 0.435 (5) 0.589 (2) 0.463 (2)Lead 0.284 0.432 0.586 0.4344 Evaluation resultsIn this section, we show our evaluation results on thethree sets of data for the sentence extraction systemdescribed in the previous section.4.1 Evaluation results for the TSC dataTable 1 shows the evaluation results for our sys-tem and some baseline systems on the task of sen-tence extraction at TSC-2001.
The figures in Ta-ble 1 are values of the F-measure1.
The ?System?column shows the performance of our system and itsrank among the nine systems that were applied to thetask, and the ?Lead?
column shows the performanceof a baseline system which extracts as many sen-tences as the threshold from the beginning of a doc-ument.
Since all participants could output as manysentences as the allowed upper limit, the values ofthe recall, precision, and F-measure were the same.Our system obtained better results than the baselinesystems, especially when the compression ratio was10%.
The average performance was second amongthe nine systems.4.2 Evaluation results for the DUC dataTable 2 shows the results of a subjective evalua-tion in the SDS task at DUC-2001.
In this subjec-tive evaluation, assessors gave a score to each sys-tem?s outputs, on a zero-to-four scale (where four isthe best), as compared with summaries made by hu-mans.
The figures shown are the average scores overall documents.
The ?System?
column shows the per-formance of our system and its rank among the 12systems that were applied to this task.
The ?Lead?1The definitions of each measurement are as follows:Recall (REC) = COR / GLDPrecision (PRE) = COR / SYSF-measure = 2 * REC * PRE / (REC + PRE),where COR is the number of correct sentences marked by thesystem, GLD is the total number of correct sentences markedby humans, and SYS is the total number of sentences marked bythe system.
After calculating these scores for each transcription,the average is calculated as the final score.Table 2: Evaluation results for the DUC data (sub-jective evaluation).System Lead Avg.Grammaticality 3.711 (5) 3.236 3.580Cohesion 3.054 (1) 2.926 2.676Organization 3.215 (1) 3.081 2.870Total 9.980 (1) 9.243 9.126Table 3: Evaluation results for the CSJ data.AnnotatorsA B C Avg.REC 0.407 0.331 0.354 0.364PRE 0.416 0.397 0.322 0.378F 0.411 0.359 0.334 0.368column shows the performance of a baseline systemthat always outputs the first 100 words of a givendocument, while the ?Avg.?
column shows the aver-age for all systems.
Our system ranked 5th in gram-maticality and was ranked at the top for the othermeasurements, including the total value.4.3 Evaluation results for the CSJ dataThe evaluation results for sentence extraction withthe CSJ data are shown in Table 3.
We compared thesystem?s results with each annotator?s key data.
Asmentioned previously, we used 50 transcriptions fortraining and 10 for testing.These results are comparable with the perfor-mance on sentence segmentation for written doc-uments, because the system?s performance for theTSC data was 0.363 when the compression ratio wasset to 10%.
The results of our experiments thus showthat for transcriptions, sentence extraction achievesresults comparable to those for written documents,if the are well defined.4.4 Contributions of featuresTable 4 shows the contribution vectors for each setof training data.
The contribution here means theproduct of the optimized weight and the standarddeviation of the score for the test data.
The vec-tors were normalized so that the sum of the com-ponents is equal to 1, and the selected function typesfor the features are also shown in the table.
Our sys-tem used the NE-based headline function (HL (N))for the DUC data and the word-based function (HLTable 4: Contribution (weight?
s.d.)
of each featurefor each set of summarization data.TSCFeatures Editorial Report DUC CSJPst.
P3.
0.446 P1.
0.254 P1.
0.691 P3.
0.055Len.
L3.
0.000 L3.
0.000 L2.
0.020 L2.
0.881Tf*idf T3.
0.169 T3.
0.185 T1.
0.239 T3.
0.057HL (W) 0.171 0.292 - 0.007HL (N) 0.214 0.269 0.045 -Pattern - - 0.005 -(W)) for the CSJ data, and both functions for theTSC data.
The columns for the TSC data show thecontributions when the compression ratio was 10%.We can see that the feature with the biggest con-tribution varies among the data sets.
While the posi-tion feature was the most effective for the TSC andDUC data, the length feature was dominant for theCSJ data.
Most of the short sentences in the lectureswere specific expressions, such as ?This is the resultof the experiment.?
or ?Let me summarize my pre-sentation.?.
Since these sentences were not extractedas key sentences by the annotators, it is believed thatthe function giving short sentences a penalty scorematched the manual extraction results.5 Analysis of the summarization dataIn Section 4, we showed how our system, whichcombines major features, has performed well ascompared with current summarization systems.However, the evaluation results alone do not suffi-ciently explain how such a combination of featuresis effective.
In this section, we investigate the corre-lations between each pair of features.
We also matchfeature pairs with distributions of extracted key sen-tences as answer summaries to find effective combi-nation of features for sentence extraction.5.1 Correlation between featuresTable 5 shows Spearman?s rank correlation coeffi-cients among the four features.
Significantly corre-lated feature pairs are indicated by ???(?
= 0.001).Here, the word-based feature is used as the headlinefeature.
We see the following tendencies for any ofthe data sets:?
?Position?
is relatively independent of the other features.?
?Length?
and ?Tf*idf?
have high correlation2.Table 5: Rank correlation coefficients between fea-tures.TSC ReportFeatures Length Tf*idf HeadlinePosition 0.019 -0.095 -0.139Length ?
0.546?
0.338?Tf*idf ?
?
0.696?TSC EditorialFeatures Length Tf*idf HeadlinePosition -0.047 -0.099 0.046Length ?
0.532?
0.289?Tf*idf ?
?
0.658?DUC DataFeatures Length Tf*idf HeadlinePosition -0.130?
-0.108?
-0.134?Length ?
0.471?
0.293?Tf*idf ?
?
0.526?CSJ DataFeatures Length Tf*idf HeadlinePosition -0.092?
-0.069?
-0.106?Length ?
0.460?
0.224?Tf*idf ?
?
0.533??
?TF*idf?
and ?Headline ?
also have high correlation.These results show that while combinations of thesefour features enabled us to obtain good evaluationresults, as shown in Section 4, the features are notnecessarily independent of one another.5.2 Combination of featuresTables 6 and 7 show the distributions of extractedkey sentences as answer summaries with two pairsof features: sentence position and the tf*idf value,and sentence position and the headline information.In these tables, each sentence is ranked by each ofthe two feature values, and the rankings are split ev-ery 10 percent.
For example, if a sentence is rankedin the first 10 percent by sentence position and thelast 10 percent by the tf*idf feature, the sentence be-longs to the cell with a position rank of 0.1 and atf*idf rank of 1.0 in Table 6.Each cell thus has two letters.
The left letter is thenumber of key sentences, and the right letter is theratio of key sentences to all sentences in the cell.
Theleft letter shows how the number of sentences differsfrom the average when all the key sentences appearequally, regardless of the feature values.
Let T be2Here we used equation T1 for the tf*idf feature, and thescore of each sentence was normalized with the sentence length.Hence, the high correlation between ?Length?
and ?Tf*idf?
isnot trivial.the total number of key sentences, M(= T100) be theaverage number of key sentences in each range, andS be the standard deviation of the number of keysentences among all cells.
The number of key sen-tences for cell Ti,j is then categorized according toone of the following letters:A: Ti,j ?
M + 2SB: M + S ?
Ti,j < M + 2SC: M ?
S ?
Ti,j < M + SD: M ?
2S ?
Ti,j < M ?
SE: Ti,j < M ?
2SO: Ti,j = 0-: No sentences exist in the cell.Similarly, the right letter in a cell shows how the ra-tio of key sentences differs from the average ratiowhen all the key sentences appear equally, regard-less of feature values.
Let N be the total numberof sentences, m(= TN ) be the average ratio of keysentences, and s be the standard deviation of the ra-tio among all cells.
The ratio of key sentences forcell ti,j is then categorized according to one of thefollowing letters:a: ti,j ?
m+ 2sb: m+ s ?
ti,j < m+ 2sc: m?
s ?
ti,j < m+ sd: m?
2s ?
ti,j < m?
se: ti,j < m?
2so: ti,j = 0-: No sentences exist in the cell.When key sentences appear uniformly regardless offeature values, every cell is defined as ?Cc?.
Weshow both the range of the number of key sentencesand the ratio of key sentences, because both are nec-essary to show how effectively a cell has key sen-tences.
If a cell includes many sentences, the num-ber of key sentences can be large even though theratio is not.
On the other hand, when the ratio of keysentences is large and the number is not, the contri-bution to key sentence extraction is small.Table 6 shows the distributions of key sentenceswhen the features of sentence position and tf*idfwere combined.
For the DUC data, both the num-ber and ratio of key sentences were large when thesentence position was ranked within the first 20 per-cent and the value of the tf*idf feature was rankedin the bottom 50 percent (i.e., Pst.
?
0.2, Tf*idf ?0.5).
On the other hand, both the number and ratioof key sentences were large for the CSJ data whenthe sentence position was ranked in the last 10 per-cent and the value of the tf*idf feature was rankedTable 6: Distributions of key sentences based on the combination of the sentence position (Pst.)
and tf*idffeatures.DUC dataTf*idfPst.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00.1 Cc Cc Cc Cb Ba Ba Aa Aa Aa Aa0.2 Cd Cc Cc Cc Cc Cc Bb Bb Bb Bb0.3 Cd Cc Cc Cc Cc Cc Cc Cc Cc Cc0.4 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc0.5 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc0.6 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc0.7 Dd Cc Cc Cc Cc Cc Cc Cc Cc Cc0.8 Cd Dd Cc Cc Cc Cc Cc Cc Cc Cc0.9 Dd Dd Cc Cc Cc Cc Cc Cc Cc Cc1.0 Dd Dd Cc Cc Cc Cc Cc Cc Cc CcCSJ dataTf*idfPst.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00.1 Cc Cc Cc Cc Bc Cc Ab Bb Bb Bb0.2 Oo Oo Cc Cc Cc Cc Cc Bb Bc Cc0.3 Oo Cc Cc Cc Cc Cc Cc Cc Cc Cc0.4 Cc Cc Cc Cc Oo Cc Cc Cc Cc Cc0.5 Oo Cc Oo Oo Cc Oo Cc Cc Cc Cc0.6 Cc Cc Cc Cc Cc Cc Cc Cc Cc Cc0.7 Oo Cc Cc Cc Cc Cc Cc Cc Cc Cc0.8 Oo Cc Cc Oo Oo Cc Cc Cc Cc Cc0.9 Cc Cc Cc Cc Cc Cc Cc Cc Cc Bb1.0 Cc Cc Ba Bb Bb Aa Aa Bb Aa Aaafter the first 30 percent (i.e., Pst.
= 1.0, Tf*idf ?0.3),.
When the tf*idf feature was low, the numberand ratio of key sentences were not large, regardlessof the sentence position values.
These results showthat the tf*idf feature is effective when the valuesare used as a filter after the sentences are ranked bysentence position.Table 7 shows the distributions of key sentenceswith the combination of the sentence position andheadline features.
About half the sentences did notshare words with the headlines and had a value of0 for the headline feature.
As a result, the cells inthe middle of the table do not have correspondingsentences.
The headline feature cannot be used asa filter, unlike the tf*idf feature, because many keysentences are found when the value of the headlinefeature is 0.
A high value of the headline feature is,however, a good indicator of key sentences when itis combined with the position feature.
The ratio ofkey sentences was large when the headline rankingwas high and the sentence was near the beginning(at Pst.
?
0.2, Headline ?
0.7) for the DUC data.For the CSJ data, the ratio of key sentences was alsolarge when the headline ranking was within the top10 percent (Pst.
= 0.1, Headline = 1.0), as well asfor the sentences near the ends of speeches.These results indicate that the number and ratioof key sentences sometimes vary discretely accord-ing to the changes in feature values when featuresare combined for sentence extraction.
That is, theperformance of a sentence extraction system can beimproved by categorizing feature values into sev-eral ranges and then combining ranges.
While mostsentence extraction systems use sequential combi-nations of features, as we do in our system basedon Equation 1, the performance of these systemscan possibly be improved by introducing the cat-egorization of feature values, without adding anynew features.
We have shown that discrete combi-nations match the distributions of key sentences intwo different corpora, the DUC data and the CSJdata.
This indicates that discrete combinations ofcorpora are effective across both different languagesand different types of corpora.
Hirao et al (2002)reported the results of a sentence extraction systemusing an SVM, which categorized sequential featurevalues into ranges in order to make the features bi-nary.
Some effective combinations of the binary fea-Table 7: Distributions of key sentences based onthe combination of the sentence position (Pst.)
andheadline features.DUC dataHeadlinePst.
0.1 0.2?0.5 0.6 0.7 0.8 0.9 1.00.1 Ab -- -- Ca Ba Ba Aa0.2 Ac -- -- Cb Cc Ca Ca0.3 Ac -- -- Cc Cc Cb Cb0.4 Ac -- -- Cc Cc Cc Cb0.5 Ac -- -- Cc Cc Cc Cc0.6 Bc -- -- Cc Cc Cc Cc0.7 Bc -- -- Cc Cc Cc Cc0.8 Ac -- -- Cd Cc Cc Cc0.9 Bd -- -- Cd Cc Cc Cc1.0 Bd -- -- Cd Cc Cc CcCSJ dataHeadlinePst.
0.1 0.2?0.5 0.6 0.7 0.8 0.9 1.00.1 Bc -- Cc Cc Bb Cc Aa0.2 Bc -- Cc Cb Cc Cc Bb0.3 Cc -- Cc Cc Cc Cc Cc0.4 Cc -- Oo Cc Cc Cc Cc0.5 Cc -- Oo Cc Oo Cc Cc0.6 Cc -- Cc Cc Cc Cc Cc0.7 Cc -- Oo Cc Cc Cc Cc0.8 Cc -- Cc Cc Cc Cc Cc0.9 Ac -- Ca Cc Cc Cc Cb1.0 Ab -- Ca Aa Ba Ba Batures in that report also indicate the effectiveness ofdiscrete combinations of features.6 ConclusionWe have shown evaluation results for our sentenceextraction system and analyzed its features for dif-ferent types of corpora, which included corpora dif-fering in both language (Japanese and English) andtype (newspaper articles and lectures).
The sys-tem is based on four major features, and it achievedsome of the top results at evaluation workshops in2001 for summarizing Japanese newspaper articles(TSC) and English newspaper articles (DUC).
ForJapanese lectures, the sentence extraction systemalso obtained comparable results when the sentenceboundary was given.Our analysis of the features used in this sentenceextraction system has shown that they are not neces-sarily independent of one another, based on the re-sults of their rank correlation coefficients.
The anal-ysis also indicated that the categorization of featurevalues matches the distribution of key sentences bet-ter than sequential feature values.There are several features that were not describedhere but are also used in sentence extraction sys-tems, such as some specific lexical expressions andsyntactic information.
In our future work, we willanalyze and use these features to improve the per-formance of our sentence extraction system.ReferencesC.
Aone, M. E. Okurowski, and J. Gorlinsky.
1998.
Train-able, Scalable Summarization Using Robust NLP and Ma-chine Learning.
In Proc.
of COLING-ACL?98, pages 62?66.DUC.
2001. http://duc.nist.gov.
Document UnderstandingConference.H.
Edmundson.
1969.
New methods in automatic abstracting.Journal of ACM, 16(2):264?285.T.
Hirao, H. Isozaki, E. Maeda, and Y. Matsumoto.
2002.
Ex-tracting Important Sentences with Support Vector Machines.In Proc.
of COLING-2002.J.
Kupiec, J. Pedersen, and F. Chen.
1995.
A Trainable Docu-ment Summarizaer.
In Proc.
of SIGIR?95, pages 68?73.S.
Kurohashi and M. Nagao, 1999.
Japanese MorphologicalAnalyzing System: JUMAN version 3.61.
Kyoto University.Chin-Yew Lin.
1999.
Training a selection function for extrac-tion.
In Proc.
of the CIKM?99.K.
Maekawa, H. Koiso, S. Furui, and H. Isahara.
2000.
Spon-taneous Speech Corpus of Japanese.
In Proc.
of LREC2000,pages 947?952.C.
Nobata, S. Sekine, H. Isahara, and R. Grishman.
2002.
Sum-marization System Integrated with Named Entity Taggingand IE pattern Discovery.
In Proceedings of the LREC-2002Conference, pages 1742?1745, May.T.
Nomoto and Y. Matsumoto.
1997.
The Reliability of HumanCoding and Effects on Automatic Abstracting (in Japanese).In IPSJ-NL 120-11, pages 71?76, July.S.
E. Robertson and S. Walker.
1994.
Some simple effec-tive approximations to the 2-poisson model for probabilisticweighted retreival.
In Proc.
of SIGIR?94.K.
Sudo, S. Sekine, and R. Grishman.
2001.
Automatic patternacquisition for japanese information extraction.
In Proc.
ofHLT-2001.TSC.
2001.
Proceedings of the Second NTCIR Workshopon Research in Chinese & Japanese Text Retrieval and TextSummarization (NTCIR2).
National Institute of Informat-ics.
