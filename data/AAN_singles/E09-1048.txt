Proceedings of the 12th Conference of the European Chapter of the ACL, pages 415?423,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsAutomatic Single-Document Key Fact Extraction from Newswire ArticlesItamar KastnerDepartment of Computer ScienceQueen Mary, University of London, UKitk1@dcs.qmul.ac.ukChristof MonzISLA, University of AmsterdamAmsterdam, The Netherlandschristof@science.uva.nlAbstractThis paper addresses the problem of ex-tracting the most important facts from anews article.
Our approach uses syntac-tic, semantic, and general statistical fea-tures to identify the most important sen-tences in a document.
The importanceof the individual features is estimated us-ing generalized iterative scaling methodstrained on an annotated newswire corpus.The performance of our approach is evalu-ated against 300 unseen news articles andshows that use of these features results instatistically significant improvements overa provenly robust baseline, as measuredusing metrics such as precision, recall andROUGE.1 IntroductionThe increasing amount of information that is avail-able to both professional users (such as journal-ists, financial analysts and intelligence analysts)and lay users has called for methods condensinginformation, in order to make the most importantcontent stand out.
Several methods have been pro-posed over the last two decades, among whichkeyword extraction and summarization are themost prominent ones.
Keyword extraction aimsto identify the most relevant words or phrases ina document, e.g., (Witten et al, 1999), while sum-marization aims to provide a short (commonly 100words), coherent full-text summary of the docu-ment, e.g., (McKeown et al, 1999).
Key fact ex-traction falls in between key word extraction andsummarization.
Here, the challenge is to identifythe most relevant facts in a document, but not nec-essarily in a coherent full-text form as is done insummarization.Evidence of the usefulness of key fact extractionis CNN?s web site which since 2006 has most of itsnews articles preceded by a list of story highlights,see Figure 1.
The advantage of the news highlightsas opposed to full-text summaries is that they aremuch ?easier on the eye?
and are better suited forquick skimming.So far, only CNN.com offers this service and weare interested in finding out to what extent it canbe automated and thus applied to any newswiresource.
Although these highlights could be eas-ily generated by the respective journalists, manynews organization shy away from introducing anadditional manual stage into the workflow, wherepushback times of minutes are considered unac-ceptable in an extremely competitive news busi-ness which competes in terms of seconds ratherthan minutes.
Automating highlight generationcan help eliminate those delays.Journalistic training emphasizes that news arti-cles should contain the most important informa-tion in the beginning, while less important infor-mation, such as background or additional details,appears further down in the article.
This is alsothe main reason why most summarization systemsapplied to news articles do not outperform a sim-ple baseline that just uses the first 100 words of anarticle (Svore et al, 2007; Nenkova, 2005).On the other hand, most of CNN?s story high-lights are not taken from the beginning of the ar-ticles.
In fact, more than 50% of the highlightsstem from sentences that are not among the first100 words of the articles.
This makes identify-ing story highlights a much more challenging taskthan single-document summarization in the newsdomain.In order to automate story highlight identifica-tion we automatically extract syntactic, semantic,415Figure 1: CNN.com screen shot of a story excerptwith highlights.and purely statistical features from the document.The weights of the features are estimated usingmachine learning techniques, trained on an anno-tated corpus.
In this paper, we focus on identify-ing the relevant sentences in the news article fromwhich the highlights were generated.
The systemwe have implemented is named AURUM: AUto-matic Retrieval of Unique information with Ma-chine learning.
A full system would also containa sentence compression step (Knight and Marcu,2000), but since both steps are largely indepen-dent of each other, existing sentence compressionor simplification techniques can be applied to thesentences identified by our approach.The remainder of this paper is organized as fol-lows: The next section describes the relevant workdone to date in keyfact extraction and automaticsummarization.
Section 3 lays out our featuresand explains how they were learned and estimated.Section 4 presents the experimental setup and ourresults, and Section 5 concludes with a short dis-cussion.2 Related WorkAs mentioned above, the problem of identifyingstory highlight lies somewhere between keywordextraction and single-document summarization.The KEA keyphrase extraction system (Wittenet al, 1999) mainly relies on purely statisticalfeatures such as term frequencies, using the tf.idfmeasure from Information Retrieval,1 as well ason a term?s position in the text.
In addition to tf.idfscores, Hulth (2004) uses part-of-speech tags andNP chunks and complements this with machinelearning; the latter has been used to good resultsin similar cases (Turney, 2000; Neto et al, 2002).The B&C system (Barker and Cornacchia, 2000),also used linguistic methods to a very limited ex-tent, identifying NP heads.INFORMATIONFINDER (Krulwich and Burkey,1996) requires user feedback to train the system,whereby a user notes whether a given documentis of interest to them and specifies their own key-words which are then learned by the system.Over the last few years, numerous single-as well as multi-document summarization ap-proaches have been developed.
In this paper wewill focus mainly on single-document summariza-tion as it is more relevant to the issue we aim toaddress and traditionally proves harder to accom-plish.
A good example of a powerful approach isa method named Maximum Marginal Relevancewhich extracts a sentence for the summary onlyif it is different than previously selected ones,thereby striving to reduce redundancy (Carbonelland Goldstein, 1998).More recently, the work of Svore et al (2007)is closely related to our approach as it has also ex-ploited the CNN Story Highlights, although theirfocus was on summarization and using ROUGEas an evaluation and training measure.
Their ap-proach also heavily relies on additional data re-sources, mainly indexed Wikipedia articles andMicrosoft Live query logs, which are not readilyavailable.Linguistic features are today used mostly insummarization systems, and include the standardfeatures sentence length, n-gram frequency, sen-tence position, proper noun identification, similar-ity to title, tf.idf, and so-called ?bonus?/?stigma?words (Neto et al, 2002; Leite et al, 2007; Pol-lock and Zamora, 1975; Goldstein et al, 1999).On the other hand, for most of these systems, sim-ple statistical features and tf.idf still turn out to bethe most important features.Attempts to integrate discourse models havealso been made (Thione et al, 2004), hand in handwith some of Marcu?s (1995) earlier work.1tf(t, d) = frequency of term t in document d.idf(t,N) = inverse frequency of documents d containingterm t in corpus N , log( |N||dt| )416Regarding syntax, it seems to be used mainlyin sentence compression or trimming.
The algo-rithm used by Dorr et al (2003) removes subor-dinate clauses, to name one example.
While ourapproach does not use syntactical features as such,it is worth noting these possible enhancements.3 ApproachIn this section we describe which features wereused and how the data was annotated to facilitatefeature extraction and estimation.3.1 Training DataIn order to determine the features used for pre-dicting which sentences are the sources for storyhighlights, we gathered statistics from 1,200 CNNnewswire articles.
An additional 300 articles wereset aside to serve as a test set later on.
The arti-cles were taken from a wide range of topics: poli-tics, business, sport, health, world affairs, weather,entertainment and technology.
Only articles withstory highlights were considered.For each article we extracted a number of n-gram statistics, where n ?
{1, 2, 3}.n-gram score.
We observed the frequency andprobability of unigrams, bigrams and trigrams ap-pearing in both the article body and the highlightsof a given story.
An important phrase (of lengthn ?
3) in the article would likely be used againin the highlights.
These phrases were ranked andscored according to the probability of their appear-ing in a given text and its highlights.Trigger phrases.
These are phrases which causeadjacent words to appear in the highlights.
Overthe entire set, such phrases become significant.
Wespecified a limit of 2 words to the left and 4 wordsto the right of a phrase.
For example, the word ac-cording caused other words in the same sentenceto appear in the highlights nearly 25% of the time.Consider the highlight/sentence pair in Table 1:highlight: 61 percent of those polled now say it was notworth invading Iraq, poll saysText: Now, 61 percent of those surveyed say it wasnot worth invading Iraq, according to the poll.Table 1: Example highlight with source sentence.The word according receives a score of 3 since{invading, Iraq, poll} are all in the highlight.
Itshould be noted that the trigram {invading Iraqaccording} would receive an identical score, since{not, worth, poll} are in the highlights as well.Spawned phrases.
Conversely, spawnedphrases occur frequently in the highlights and inclose proximity to trigger phrases.
Continuingthe example in Table 1, {invading, Iraq, poll, not,worth} are all considered to be spawned phrases.Of course, simply using the identities of wordsneglects the issue of lexical paraphrasing, e.g.,involving synonyms, which we address to someextent by using WordNet and other features de-scribed in this Section.
Table 2 gives an exampleinvolving paraphrasing.highlight: Sources say men were planning to shoot sol-diers at Army baseText: The federal government has charged five al-leged Islamic radicals with plotting to kill U.S.soldiers at Fort Dix in New Jersey.Table 2: An example of paraphrasing between ahighlight and its source sentence.Other approaches have tried to select linguisticfeatures which could be useful (Chuang and Yang,2000), but these gather them under one headingrather than treating them as separate features.
Theidentification of common verbs has been used bothas a positive (Turney, 2000) and as a negativefeature (Goldstein et al, 1999) in some systems,whereas we score such terms according to a scale.Turney also uses a ?final adjective?
measure.
Useof a thesaurus has also shown to improve results inautomatic summarization, even in multi-documentenvironments (McKeown et al, 1999) and otherlanguages such as Portuguese (Leite et al, 2007).3.2 Feature SelectionBy manually inspecting the training data, the lin-guistic features were selected.
AURUM has twotypes of features: sentence features, such as theposition of the sentence or the existence of a nega-tion word, receive the same value for the entiresentence.
On the other hand, word features areevaluated for each of the words in the sentence,normalized over the number of words in the sen-tence.Our features resemble those suggested by previ-ous works in keyphrase extraction and automaticsummarization, but map more closely to the jour-nalistic characteristics of the corpus, as explainedin the following.417Figure 2: Positions of sentences from which high-lights (HLs) were generated.3.2.1 Sentence FeaturesThese are the features which apply once for eachsentence.Position of the sentence in the text.
Intuitively,facts of greater importance will be placed at thebeginning of the text, and this is supported by thedata, as can be seen in Figure 2.
Only half of thehighlights stem from sentences in the first fifth ofthe article.
Nevertheless, selecting sentences fromonly the first few lines is not a sure-fire approach.Table 3 presents an article in which none of thefirst four sentences were in the highlights.
Whilethe baseline found no sentences, AURUM?s perfor-mance was better.The sentence positions score is defined as pi =1 ?
(log i/logN), where i is the position of thesentence in the article and N the total number ofsentences in the article.Numbers or dates.
This is especially evidentin news reports mentioning figures of casualties,opinion poll results, or financial news.Source attribution.
Phrasings such as accord-ing to a source or officials say.Negations.
Negations are often used for intro-ducing new or contradictory information: ?Kellyis due in a Chicago courtroom Friday for yet an-other status hearing, but there?s still no trial datein sight.2?
We selected a number of typical nega-tion phrases to this end.Causal adverbs.
Manually compiled list ofphrases, including in order to, hoping for and be-cause.2This sentence was included in the highlightsTemporal adverbs.
Manually compiled list ofphrases, such as after less than, for two weeks andThursday.Mention of the news agency?s name.
Journal-istic scoops and other exclusive nuggets of infor-mation often recall the agency?s name, especiallywhen there is an element of self-advertisementinvolved, as in ?.
.
.
The debates are being heldby CNN, WMUR and the New Hampshire UnionLeader.?
It is interesting to note that an oppositeapproach has previously been taken (Goldstein etal., 1999), albeit involving a different corpus.Story Highlights:?Memorial Day marked by parades, cookouts, cer-emonies?
AAA: 38 million Americans expected to travel atleast 50 miles during weekend?
President Bush gives speech at Arlington NationalCemetery?
Gulf Coast once again packed with people cele-brating holiday weekendFirst sentences of article:1.
Veterans and active soldiers unfurled a 90-by-100-foot U. S. flag as the nation?s top commanderin the Middle East spoke to a Memorial Day crowdgathered in Central Park on Monday.2.
Navy Adm. William Fallon, commander of U. S.Central Command, said America should rememberthose whom the holiday honors.3.
?Their sacrifice has enabled us to enjoy the thingsthat we, I think in many cases, take for granted,?Fallon said.4.
Across the nation, flags snapped in the wind overdecorated gravestones as relatives and friends paidtribute to their fallen soldiers.Sentences the Highlights were derived from:5.
Millions more kicked off summer with trips tobeaches or their backyard grills.6.
AAA estimated 38 million Americans wouldtravel 50 miles or more during the weekend ?
up1.7 percent from last year ?
even with gas aver-aging $3.20 a gallon for self-service regular.7.
In the nation?s capital, thousands of motorcy-cles driven by military veterans and their loved onesroared through Washington to the Vietnam VeteransMemorial.9.
President Bush spoke at nearby Arlington Na-tional Cemetery, honoring U. S. troops who havefought and died for freedom and expressing his re-solve to succeed in the war in Iraq.21.
Elsewhere, Alabama?s Gulf Coast was onceagain packed with holiday-goers after the damagefrom hurricanes Ivan and Katrina in 2004 and 2005kept the tourists away.Table 3: Sentence selection outside the first foursentences (correctly identified sentence by AURUMin boldface).4183.2.2 Word FeaturesThese features are tested on each word in the sen-tence.?Bonus?
words.
A list of phrases similar to sen-sational, badly, ironically, historic, identified fromthe training data.
This is akin to ?bonus?/?stigma?words (Neto et al, 2002; Leite et al, 2007; Pol-lock and Zamora, 1975; Goldstein et al, 1999).Verb classes.
After exploring the training datawe manually compiled two classes of verbs,each containing 15-20 inflected and uninflectedlexemes, talkVerbs and actionVerbs.talkVerbs include verbs such as {report, men-tion, accuse} and actionVerbs refer to verbssuch as {provoke, spend, use}.
Both lists also con-tain the WordNet synonyms of each word in thelist (Fellbaum, 1998).Proper nouns.
Proper nouns and other parts ofspeech were identified running Charniak?s parser(Charniak, 2000) on the news articles.3.2.3 Sentence ScoringThe overall score of a sentence is computed as theweighted linear combination of the sentence andword scores.
The score ?
(s) of sentence s is de-fined as follows:?
(s) = wposppos(s) +n?k=1wkfk +|s|?j=1m?k=1wkgjkEach of the sentences s in the article was testedagainst the position feature ppos(s) and againsteach of the sentence features fk, see Section 3.2.1,where pos(s) returns the position of sentence s.Each word j of sentence s is tested against all ap-plicable word features gjk, see Section 3.2.2.
Aweight (wpos and wk) is associated with each fea-ture.
How to estimate the weights is discussednext.3.3 Parameter EstimationThere are various optimization methods that allowone to estimate the weights of features, includ-ing generalized iterative scaling and quasi-Newtonmethods (Malouf, 2002).
We opted for general-ized iterative scaling as it is commonly used forother NLP tasks and off-the-shelf implementationsexist.
Here we used YASMET.33A maximum entropy toolkit by Franz Josef Och, http://www.fjoch.com/YASMET.htmlWe used a development set of 240 news arti-cles to train YASMET.
As YASMET is a supervisedoptimizer, we had to generate annotated data onwhich it was to be trained.
For each document inthe development set, we labeled each sentence asto whether a story highlight was generated from it.For instance, in the article presented in Figure 3,sentences 5, 6, 7, 9 and 21 were marked as high-light sources, whereas all other sentences in thedocument were not.4When annotating, all sentences that were di-rectly relevant to the highlights were marked, withpreference given to those appearing earlier in thestory or containing more precise information.
Atthis point it is worth noting that while the over-lap between different editors is unknown, the high-lights were originally written by a number of dif-ferent people, ensuring enough variation in thedata and helping to avoid over-fitting to a specificeditor.4 Experiments and ResultsThe CNN corpus was divided into a training setand a development and test set.
As we hadonly 300 manually annotated news articles and wewanted to maximize the number of documents us-able for parameter estimation, we applied cross-folding, which is commonly used for situationswith limited data.
The dev/test set was randomlypartitioned into five folds.
Four of the five foldswere used as development data (i.e.
for parame-ter estimation with YASMET), while the remainingfold was used for testing.
The procedure was re-peated five times, each time with four folds usedfor development and a separate one for testing.Cross-folding is safe to use as long as there areno dependencies between the folds, which is safeto assume here.Some statistics on our training and develop-ment/test data can be found in Table 4.Corpus subset Dev/Test TrainDocuments 300 1220Avg.
sentences per article 33.26 31.02Avg.
sentence length 20.62 20.50Avg.
number of highlights 3.71 3.67Avg.
number of highlight sources 4.32 -Avg.
highlight length in words 10.26 10.28Table 4: Characteristics of the evaluation corpus.4The annotated data set is available at: http://www.science.uva.nl/?christof/data/hl/.419Most summarization evaluation campaigns,such as NIST?s Document Understanding Confer-ences (DUC), impose a maximum length on sum-maries (e.g., 75 characters for the headline gen-eration task or 100 words for the summarizationtask).
When identifying sentences from whichstory highlights are generated, the situation isslightly different, as the number of story highlightsis not fixed.
On the other hand, most stories havebetween three and four highlights, and on aver-age between four and five sentences per story fromwhich the highlights were generated.
This varia-tion led to us to carry out two sets of experiments:In the first experiment (fixed), the number ofhighlight sources is fixed and our system alwaysreturns exactly four highlight sources.
In the sec-ond experiment (thresh), our system can returnbetween three and six highlight sources, depend-ing on whether a sentence score passes a giventhreshold.
The threshold ?
was used to allocatesentences si of article a to the highlight list HLby first finding the highest-scoring sentence forthat article ?(sh).
The threshold score was thus?
?
?
(sh) and sentences were judged accordingly.The algorithm used is given in Figure 3.initialize HL, shsort si in s by ?
(si)set sh = s0for each sentence si in article a:if |HL| < 3include sielse if (?
?
?
(sh) ?
?
(si))&& (|HL| ?
5)include sielsediscard sireturn HLFigure 3: Procedure for selecting highlightsources.All scores were compared to a baseline, whichsimply returns the first n sentences of a newsarticle.
n = 4 in the fixed experiment.For the thresh experiment, the baseline al-ways selected the same number of sentences asAURUM-thresh, but from the beginning of thearticle.
Although this is a very simple baseline, itis worth reiterating that it is also a very compet-itive baseline, which most single-document sum-marization systems fail to beat due to the nature ofnews articles.Since we are mainly interested in determiningto what extent our system is able to correctly iden-tify the highlight sources, we chose precision andrecall as evaluation metrics.
Precision is the per-centage of all returned highlight sources which arecorrect:Precision =|R ?
T ||R|where R is the set of returned highlight sourcesand T is the set of manually identified true sourcesin the test set.
Recall is defined as the percentageof all true highlight sources that have been cor-rectly identified by the system:Recall =|R ?
T ||T |Precision and recall can be combined by using theF-measure, which is the harmonic mean of thetwo:F-measure =2(precision ?
recall)precision+ recallTable 5 shows the results for both experiments(fixed and thresh) as an average over thefolds.
To determine whether the observed differ-ences between two approaches are statistically sig-nificant and not just caused by chance, we appliedstatistical significance testing.
As we did not wantto make the assumption that the score differencesare normally distributed, we used the bootstrapmethod, a powerful non-parametric inference test(Efron, 1979).
Improvements at a confidence levelof more than 95% are marked with ??
?.We can see that our approach consistentlyoutperforms the baseline, and most of theimprovements?in particular the F-measurescores?are statistically significant at the 0.95level.
As to be expected, AURUM-fixedachieves higher precision gains, whileAURUM-thresh achieves higher recall gains.
Inaddition, for 83.3 percent of the documents, oursystem?s F-measure score is higher than or equalto that of the baseline.Figure 4 shows how far down in the documentsour system was able to correctly identify highlightsources.
Although the distribution is still heavilyskewed towards extracting sentences from the be-ginning of the document, it is so to a lesser extentthan just using positional information as a prior;see Figure 2.In a third set of experiments we measured then-gram overlap between the sentences we haveidentified as highlight sources and the actual storyhighlights in the ground truth.
To this end we use420System Recall Precision F-Measure ExtractedBaseline-fixed 40.69 44.14 42.35 240AURUM-fixed 41.88 (+2.96%?)
45.40 (+2.85%) 43.57 (+2.88%?)
240Baseline-thresh 42.91 41.82 42.36 269AURUM-thresh 44.49 (+3.73%?)
43.30 (+3.53%) 43.88 (+3.59%?)
269Table 5: Evaluation scores for the four extraction systems.System ROUGE-1 ROUGE-2Baseline-fixed 47.73 15.98AURUM-fixed 49.20 (+3.09%?)
16.53 (+3.63%?
)Baseline-thresh 55.11 19.31AURUM-thresh 56.73 (+2.96%?)
19.66 (+1.87%)Table 6: ROUGE scores for AURUM-fixed, returning 4 sentences, and AURUM-thresh, returningbetween 3 and 6 sentences.Figure 4: Position of correctly extracted sourcesby AURUM-thresh.ROUGE (Lin, 2004), a recall-oriented evaluationpackage for automatic summarization.
ROUGEoperates essentially by comparing n-gram co-occurrences between a candidate summary and anumber of reference summaries, and comparingthat number in turn to the total number of n-gramsin the reference summaries:ROUGE-n =?S?References?ngramn?SMatch(ngramn)?S?References?ngramn?SCount(ngramn)Where n is the length of the n-gram, with lengthsof 1 and 2 words most commonly used in currentevaluations.
ROUGE has become the standard toolfor evaluating automatic summaries, though it isnot the optimal system for this experiment.
This isdue to the fact that it is geared towards a differenttask?as ours is not automatic summarization perse?and that ROUGE works best judging betweena number of candidate and model summaries.
TheROUGE scores are shown in Table 6.Similar to the precision and recall scores, ourapproach consistently outperforms the baseline,with all but one difference being statistically sig-nificant.
Furthermore, in 76.2 percent of the doc-uments, our system?s ROUGE-1 score is higherthan or equal to that of the baseline, and like-wise for 85.2 percent of ROUGE-2 scores.
OurROUGE scores and their improvements over thebaseline are comparable to the results of Svoreet al (2007), who optimized their approach to-wards ROUGE and gained significant improve-ments from using third-party data resources, bothof which our approach does not require.5Table 7 shows the unique sentences extracted byevery system, which are the number of sentencesone system extracted correctly while the other didnot; this is thus an intuitive measure of how muchtwo systems differ.
Essentially, a system couldsimply pick the first two sentences of each arti-cle and might thus achieve higher precision scores,since it is less likely to return ?wrong?
sentences.However, if the scores are similar but there is adifference in the number of unique sentences ex-tracted, this means a system has gone beyond thefirst 4 sentences and extracted others from deeperdown inside the text.To get a better understanding of the impor-tance of the individual features we examined theweights as determined by YASMET.
Table 8 con-tains example output from the development sets,with feature selection determined implicitly bythe weights the MaxEnt model assigns, wherenon-discriminative features receive a low weight.5Since the test data of (Svore et al, 2007) is not publiclyavailable we were unable to carry out a more detailed com-parison.421Clearly, sentence position is of highest impor-tance, while trigram ?trigger?
phrases were quiteimportant as well.
Simple bigrams continued tobe a good indicator of data value, as is oftenthe case.
Proper nouns proved to be a valuablepointer to new information, but mention of thenews agency?s name had less of an impact thanoriginally thought.
Other particularly significantfeatures included temporal adjectives, superlativesand all n-gram measures.System Unique highlight sources BaselineAURUM-fixed 11.8 7.2AURUM-thresh 14.2 7.6Table 7: Unique recall scores for the systems.Feature Weight Feature WeightSentence pos.
10.23 Superlative 4.15Proper noun 5.18 Temporal adj.
1.75Trigger 3-gram 3.70 1-gram score 2.74Spawn 2-gram 3.73 3-gram score 3.75CNN mention 1.30 Trigger 2-gram 3.74Table 8: Typical weights learned from the data.5 ConclusionsA system for extracting essential facts from a newsarticle has been outlined here.
Finding the datanuggets deeper down is a cross between keyphraseextraction and automatic summarization, a taskwhich requires more elaborate features and param-eters.Our approach emphasizes a wide variety of fea-tures, including many linguistic features.
Thesefeatures range from the standard (n-gram fre-quency), through the essential (sentence position),to the semantic (spawned phrases, verb classes andtypes of adverbs).Our experimental results show that a combina-tion of statistical and linguistic features can leadto competitive performance.
Our approach notonly outperformed a notoriously difficult baselinebut also achieved similar performance to the ap-proach of (Svore et al, 2007), without requiringtheir third-party data resources.On top of the statistically significant improve-ments of our approach over the baseline, we seevalue in the fact that it does not settle for sentencesfrom the beginning of the articles.Most single-document automatic summariza-tion systems use other features, ranging fromdiscourse structure to lexical chains.
Consider-ing Marcu?s conclusion (2003) that different ap-proaches should be combined in order to createa good summarization system (aided by machinelearning), there seems to be room yet to use ba-sic linguistic cues.
Seeing as how our linguis-tic features?which are predominantly semantic?aid in this task, it is quite possible that further in-tegration will aid in both automatic summarizationand keyphrase extraction tasks.ReferencesKen Barker and Nadia Cornacchia.
2000.
Using nounphrase heads to extract document keyphrases.
InProceedings of the 13th Conference of the CSCSI,AI 2000, volume 1882 of Lecture Notes in ArtificialIntelligence, pages 40?52.Jaime G. Carbonell and Jade Goldstein.
1998.
Theuse of MMR, diversity-based reranking for reorder-ing documents and producing summaries.
In Pro-ceedings of SIGIR 1998, pages 335?336.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the First Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 132?139.Wesley T. Chuang and Jihoon Yang.
2000.
Extractingsentence segments for text summarization: A ma-chine learning approach.
In Proceedings of the 23rdACM SIGIR, pages 152?159.Bonnie Dorr, David Zajic, and Richard Schwartz.2003.
Hedge Trimmer: A parse-and-trim approachto headline generation.
In Proceedings of the HLT-NAACL 03 Summarization Workshop, pages 1?8.Brad Efron.
1979.
Bootstrap methods: Another lookat the jackknife.
Annals of Statistics, 7(1):1?26.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, andJaime Carbonell.
1999.
Summarizing text docu-ments: Sentence selection and evaluation metrics.In Proceedings of the 22nd annual internationalACM SIGIR on Research and Development in IR,pages 121?128.Anette Hulth.
2004.
Combining Machine Learn-ing and Natural Language Processing for AutomaticKeyword Extraction.
Ph.D. thesis, Department ofComputer and Systems Sciences, Stockholm Uni-versity.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization?step one: Sentence compres-sion.
In Proceedings of AAAI 2000, pages 703?710.422Bruce Krulwich and Chad Burkey.
1996.
Learninguser information interests through the extraction ofsemantically significant phrases.
In M. Hearst andH.
Hirsh, editors, AAAI 1996 Spring Symposium onMachine Learning in Information Access.Daniel S. Leite, Lucia H.M. Rino, Thiago A.S. Pardo,and Maria das Grac?as V. Nunes.
2007.
Extrac-tive automatic summarization: Does more linguis-tic knowledge make a difference?
In TextGraphs-2:Graph-Based Algorithms for Natural Language Pro-cessing, pages 17?24, Rochester, New York, USA.Association for Computational Linguistics.Chin-Yew Lin.
2004.
ROUGE: a package for auto-matic evaluation of summaries.
In Proceedings ofthe Workshop on Text Summarization Branches Out(WAS 2004), Barcelona, Spain.Robert Malouf.
2002.
A comparison of algorithmsfor maximum entropy parameter estimation.
In Pro-ceedings of the Sixth Conference on Natural Lan-guage Learning (CoNLL-2002), pages 49?55.Daniel Marcu.
1995.
Discourse trees are good in-dicators of importance in text.
In Inderjeet Maniand Mark T. Maybury, editors, Advances in Auto-matic Text Summarization, pages 123?136, Cam-bridge, MA.
MIT Press.Daniel Marcu.
2003.
Automatic abstracting.
In Ency-clopedia of Library and Information Science, pages245?256.Kathleen McKeown, Judith Klavans, Vasileios Hatzi-vassiloglou, Regina Barzilay, and Eleazar Eskin.1999.
Towards multidocument summarization byreformulation: Progress and prospects.
In Proceed-ing of the 16th national conference of the AmericanAssociation for Artificial Intelligence (AAAI-1999),pages 453?460.Ani Nenkova.
2005.
Automatic text summarization ofnewswire: Lessons learned from the document un-derstanding conference.
In 20th National Confer-ence on Artificial Intelligence (AAAI 2005).J.
Larocca Neto, A.A. Freitas, and C.A.A Kaestner.2002.
Automatic text summarization using a ma-chine learning approach.
In XVI Brazilian Symp.
onArtificial Intelligence, volume 2057 of Lecture Notesin Artificial Intelligence, pages 205?215.J.
J. Pollock and Antonio Zamora.
1975.
Automaticabstracting research at chemical abstracts service.Journal of Chemical Information and Computer Sci-ences, 15(4).Krysta M. Svore, Lucy Vanderwende, and Christo-pher J.C. Burges.
2007.
Enhancing single-document summarization by combining RankNetand third-party sources.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 448?457.Gian Lorenzo Thione, Martin van den Berg, LiviaPolanyi, and Chris Culy.
2004.
Hybrid text sum-marization: Combining external relevance measureswith structural analysis.
In Proceedings of the ACL-04, pages 51?55.Peter D. Turney.
2000.
Learning algorithmsfor keyphrase extraction.
Information Retrieval,2(4):303?336.Ian H. Witten, Gordon W. Paynter, Eibe Frank, CarlGutwin, and Craig G. Nevill-Manning.
1999.
Kea:Practical automatic keyphrase extraction.
In Pro-ceedings of the ACM Conference on Digital Li-braries (DL-99).423
