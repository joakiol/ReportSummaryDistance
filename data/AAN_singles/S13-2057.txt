Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 351?355, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsFBK-irst : A Multi-Phase Kernel Based Approach for Drug-DrugInteraction Detection and Classification that Exploits Linguistic InformationMd.
Faisal Mahbub Chowdhury ?
?
and Alberto Lavelli ??
Fondazione Bruno Kessler (FBK-irst), Italy?
University of Trento, Italyfmchowdhury@gmail.com, lavelli@fbk.euAbstractThis paper presents the multi-phase relationextraction (RE) approach which was used forthe DDI Extraction task of SemEval 2013.
Asa preliminary step, the proposed approach in-directly (and automatically) exploits the scopeof negation cues and the semantic roles of in-volved entities for reducing the skewness inthe training data as well as discarding possiblenegative instances from the test data.
Then, astate-of-the-art hybrid kernel is used to traina classifier which is later applied on the in-stances of the test data not filtered out by theprevious step.
The official results of the taskshow that our approach yields an F-score of0.80 for DDI detection and an F-score of 0.65for DDI detection and classification.
Our sys-tem obtained significantly higher results thanall the other participating teams in this sharedtask and has been ranked 1st.1 IntroductionDrug-drug interaction (DDI) is a condition when onedrug influences the level or activity of another.
Theextraction of DDIs has significant importance forpublic health safety.
It was reported that about 2.2million people in USA, age 57 to 85, were takingpotentially dangerous combinations of drugs (Lan-dau, 2009).
Another report mentioned that deathsfrom accidental drug interactions rose by 68 percentbetween 1999 and 2004 (Payne, 2007).
The DDIEx-traction 2011 and DDIExtraction 2013 shared tasksunderline the importance of DDI extraction.The DDIExtraction 2013 task concerns the recog-nition of drugs and the extraction of drug-drug in-teractions from biomedical literature.
The dataset ofthe shared task is composed by texts from the Drug-Bank database as well as MedLine abstracts in or-der to deal with different type of texts and languagestyles.
Participants were asked to not only extractDDIs but also classify them into one of four pre-defined classes: advise, effect, mechanism and int.A detailed description of the task settings and datacan be found in Segura-Bedmar et al(2013).The system that we used in this shared taskcombines various techniques proposed in our re-cent research activities for relation extraction (RE)(Chowdhury and Lavelli, 2012a; Chowdhury andLavelli, 2012b; Chowdhury and Lavelli, 2013).12 DDI DetectionOur system performs DDI detection and classifica-tion in two separate steps.
In this section, we explainhow DDI detection (i.e.
whether two drug mentionsparticipate in a DDI) is accomplished.
DDI classifi-cation will be described in Section 3.There are three phases for DDI detection: (i) dis-card less informative sentences, (ii) discard less in-formative instances, and (iii) train the system (a sin-gle model regardless of DDI types) on the remainingtraining instances and identify possible DDIs fromthe remaining test instances.
These phases are de-scribed below.2.1 Exploiting the scope of negations forsentence filteringNegation is a linguistic phenomenon where a nega-tion cue (e.g.
not) can alter the meaning of a partic-1Available in https://github.com/fmchowdhury/HyREX.351ular text segment or of a fact.
This text segment (orfact) is said to be inside the scope of such negation(cue).
In one of our recent papers (Chowdhury andLavelli, 2013), we proposed how to exploit the scopeof negations for RE.
We hypothesize that a classi-fier trained solely on features related to the scope ofnegations can be used to pro-actively filter groupsof instances which are less informative and mostlynegative.To be more precise, we propose to train a classi-fier (which will be applied before using the kernelbased RE classifier mentioned in Section 2.3) thatwould check whether all the target entity mentionsinside a sentence along with possible relation clues(or trigger words), if any, fall (directly or indirectly)under the scope of a negation cue.
If such a sentenceis found, then it would be identified as less informa-tive and discarded (i.e.
the candidate mention pairsinside such sentence would not be considered).
Dur-ing training (and testing), we group the instances bysentences.
Any sentence that contains at least onerelation of interest is considered by the less infor-mative sentence (LIS) classifier as a positive (train-ing/test) instance.
The remaining sentences are con-sidered as negative instances.We use a number of features related to negationscopes to train a binary SVM classifier that filters outless informative sentences.
These features are basi-cally contextual and shallow linguistic features.
Dueto space limitation, we do not report these featureshere.
Interested readers are referred to Chowdhuryand Lavelli (2013).The objective of the classifier is to decide whetherall target entity mentions as well as any possible ev-idence inside the corresponding sentence fall underthe scope of a negation cue in such a way that thesentence is unlikely to contain the relation of in-terest (e.g.
DDI).
If the classifier finds such a sen-tence, then it is assigned the negative class label.
Atpresent, we focus only on the first occurrence of thenegation cues ?no?, ?n?t?
or ?not?.
These cues usu-ally occur more frequently and generally have largernegation scope than other negation cues.The LIS classifier is trained using a linear SVMclassifier.
Its hyper-parameters are tuned duringtraining for obtaining maximum recall.
In this waywe minimize the number of false negatives (i.e.
sen-tences that contain relations but are wrongly filteredout).
Once the classifier is trained using the trainingdata, we apply it on both the training and test data.However, if the recall of the LIS classifier is foundto be below a threshold value (we set it to 70.0) dur-ing cross validation on the training data of a corpus,it is not used for sentence filtering on such corpus.Any (training/test) sentence that is classified asnegative is considered as a less informative sentenceand is filtered out.
In other words, such a sentence isnot considered for RE.
However, it should be notedthat, if such a sentence is a test sentence and it con-tains positive RE instances, then all these filteredpositive RE instances are automatically consideredas false negatives during the calculation of RE per-formance.We rule out sentences (i.e.
we consider them nei-ther positive nor negative instances for training theclassifier that filters less informative sentences) dur-ing both training and testing if any of the followingconditions holds:?
The sentence contains less than two target en-tity mentions (such sentence would not containthe relation of interest anyway).?
It has any of the following phrases ?
?notrecommended?, ?should not be?
or ?must notbe?.2?
There is no ?no?, ?n?t?
or ?not?
in the sentence.?
No target entity mention appears in the sen-tence after ?no?, ?n?t?
or ?not?.2.2 Discarding instances using semantic rolesand contextual evidenceFor identifying less informative negative instances,we exploit static (i.e.
already known, heuristicallymotivated) and dynamic (i.e.
automatically col-lected from the data) knowledge which has beenproposed in Chowdhury and Lavelli (2012b).
Thisknowledge is described by the following criteria:?
C1: If each of the two entity mentions (of acandidate pair) has anti-positive governors (seeSection 2.2.1) with respect to the type of therelation, then they are not likely to be in a givenrelation.2These expressions often provide clues that one of the drugentity mentions negatively influences the level of activity of theother.352?
C2: If two entity mentions in a sentence referto the same entity, then it is unlikely that theywould have a relation between themselves.?
C3: If a mention is the abbreviation of anothermention (i.e.
they refer to the same entity), thenthey are unlikely to be in a relation.Criteria C2 and C3 (static knowledge) are quiteintuitive.
For criterion C1, we construct on the fly alist of anti-positive governors (dynamic knowledge)taken from the training data and use them for de-tecting pairs that are unlikely to be in relation.
Asfor criterion C2, we simply check whether two men-tions have the same name and there is more than onecharacter between them.
For criterion C3, we lookfor any expression of the form ?Entity1 (Entity2)?and consider ?Entity2?
as an abbreviation or alias of?Entity1?.The above criteria are used to filter instances fromboth training and test data.
Any positive test instancefiltered out by these criteria is automatically consid-ered as a false negative during the calculation of REperformance.2.2.1 Anti-positive governorsThe semantic roles of the entity mentions may in-directly contribute either to relate or not to relatethem in a particular relation type (e.g.
PPI) in thecorresponding context.
To put it differently, the se-mantic roles of two mentions in the same contextcould provide an indication whether the relation ofinterest does not hold between them.
Interestingly,the word on which a certain entity mention is (syn-tactically) dependent (along with the dependencytype) could often provide a clue of the semantic roleof such mention in the corresponding sentence.Our goal is to automatically identify the words(if any) that tend to prevent mentions, which are di-rectly dependent on those words, from participatingin a certain relation of interest with any other men-tion in the same sentence.
We call such words anti-positive governors and assume that they could be ex-ploited to identify negative instances (i.e.
negativeentity mention pairs) in advance.
Interested readersare referred to Chowdhury and Lavelli (2012b) forexample and description of how anti-positive gov-ernors are automatically collected from the trainingdata.2.3 Hybrid Kernel based RE ClassifierAs RE classifier we use the following hybrid kernelthat has been proposed in Chowdhury and Lavelli(2013).
It is defined as follows:KHybrid (R1, R2) = KHF (R1, R2) + KSL(R1, R2) + w * KPET (R1, R2)where KHF is a feature based kernel (Chowdhuryand Lavelli, 2013) that uses a heterogeneous setof features, KSL is the Shallow Linguistic (SL)kernel proposed by Giuliano et al(2006), andKPET stands for the Path-enclosed Tree (PET) ker-nel (Moschitti, 2004).
w is a multiplicative constantthat allows the hybrid kernel to assign more (or less)weight to the information obtained using tree struc-tures depending on the corpus.
We exploit the SVM-Light-TK toolkit (Moschitti, 2006; Joachims, 1999)for kernel computation.
The parameters are tunedby doing 5-fold cross validation on the training data.3 DDI Type ClassificationThe next step is to classify the extracted DDIs intodifferent categories.
We train 4 separate models foreach of the DDI types (one Vs all) to predict theclass label of the extracted DDIs.
During this train-ing, all the negative instances from the training dataare removed.
The filtering techniques described inSections 2.1 and 2.2 are not used in this stage.The extracted DDIs are assigned a default DDIclass label.
Once the above models are trained, theyare applied on the extracted DDIs from the test data.The class label of the model which has the highestconfidence score for an extracted DDI instance is as-signed to such instance.4 Data Pre-processing and ExperimentalSettingsThe Charniak-Johnson reranking parser (Charniakand Johnson, 2005), along with a self-trainedbiomedical parsing model (McClosky, 2010), hasbeen used for tokenization, POS-tagging and pars-ing of the sentences.
Then the parse trees are pro-cessed by the Stanford parser (Klein and Manning,2003) to obtain syntactic dependencies.
The Stan-ford parser often skips some syntactic dependenciesin output.
We use the rules proposed in Chowdhury353and Lavelli (2012a) to recover some of such depen-dencies.
We use the same techniques for unknowncharacters (if any) as described in Chowdhury andLavelli (2011).Our system uses the SVM-Light-TK toolkit3(Moschitti, 2006; Joachims, 1999) for computationof the hybrid kernels.
The ratio of negative and posi-tive examples has been used as the value of the cost-ratio-factor parameter.
The SL kernel is computedusing the jSRE tool4.The KHF kernel can exploit non-target entitiesto extract important clues (Chowdhury and Lavelli,2013).
So, we use a publicly available state-of-the-art NER system called BioEnEx (Chowdhury andLavelli, 2010) to automatically annotate both thetraining and the test data with disease mentions.The DDIExtraction 2013 shared task data includetwo types of texts: texts taken from the DrugBankdatabase and texts taken from MedLine abstracts.During training we used both types together.5 Experimental ResultsTable 1 shows the results of 5-fold cross validationfor DDI detection on the training data.
As we cansee, the usage of the LIS and LII filtering techniquesimproves both precision and recall.We submitted three runs for the DDIExtraction2013 shared task.
The only difference between thethree runs concerns the default class label (i.e.
theclass chosen when none of the separate models as-signs a class label to a predicted DDI).
Such defaultclass label is ?int?, ?effect?
and ?mechanism?
forrun 1, 2 and 3 respectively.
According to the offi-cial results provided by the task organisers, our bestresult was obtained by run 2 (shown in Table 2).According to the official results, the performancefor ?advise?
is very low (F1 0.29) in MedLine texts,while the performance for ?int?
is comparativelymuch higher (F1 0.57) with respect to the one of theother DDI types.
In comparison, the performancefor ?int?
is much lower (F1 0.55) in DrugBank textswith respect to the one of the other DDI types.In MedLine test data, the number of ?effect?
(62)and ?mechanism?
(24) DDIs is much higher thanthat of ?advise?
(7) and ?int?
(2).
On the other3http://disi.unitn.it/moschitti/Tree-Kernel.htm4http://hlt.fbk.eu/en/technology/jSREP R F1KHybrid 0.66 0.80 0.72LIS filtering + KHybrid 0.67 0.80 0.73LIS filtering + LII filtering 0.68 0.82 0.74+ KHybridTable 1: Comparison of results for DDI detection on thetraining data using 5-fold cross validation.
Parameter tun-ing is not done during these experiments.P R F1All textDDI detection only 0.79 0.81 0.80Detection and Classification 0.65 0.66 0.65DrugBank textDDI detection only 0.82 0.84 0.83Detection and Classification 0.67 0.69 0.68MedLine textDDI detection only 0.56 0.51 0.53Detection and Classification 0.42 0.38 0.40Table 2: Official results of the best run (run 2) of oursystem in the DDIExtraction 2013 shared task.hand, in DrugBank test data, the different DDIs aremore evenly distributed ?
?effect?
(298), ?mecha-nism?
(278), ?advise?
(214) and ?int?
(94).Initially, it was not clear to us why our system (aswell as other participants) achieves so much higherresults on the DrugBank sentences in comparison toMedLine sentences.
Statistics of the average num-ber of words show that the length of the two typesof training sentences are substantially similar (Drug-Bank : 21.2, MedLine : 22.3).
It is true that the num-ber of the training sentences for the former is almost5.3 times higher than the latter.
But it could not bethe main reason for such high discrepancies.So, we turned our attention to the presence of thecue words.
In the 4,683 sentences of the DrugBanktraining set (which have at least one drug mention),we found that the words ?increase?
and ?decrease?are present in 721 and 319 sentences respectively.While in the 877 sentences of the MedLine train-ing set (which have at least one drug mention), wefound that the same words are present in only 67and 40 sentences respectively.
In other words, thepresence of these two important cue words in the354DrugBank sentences is twice more likely than thatin the MedLine sentences.
We assume similar obser-vations might be also possible for other cue words.Hence, this is probably the main reason why the re-sults are so much better on the DrugBank sentences.6 ConclusionIn this paper, we have described a novel multi-phaseRE approach that outperformed all the other partic-ipating teams in the DDI Detection and Classifica-tion task at SemEval 2013.
The central componentof the proposed approach is a state-of-the-art hybridkernel.
Our approach also indirectly (and automat-ically) exploits the scope of negation cues and thesemantic roles of the involved entities.AcknowledgmentsThis work is supported by the project ?eOnco - Pervasiveknowledge and data management in cancer care?.
Theauthors would like to thank Alessandro Moschitti for hishelp in the use of SVM-Light-TK.ReferencesE Charniak and M Johnson.
2005.
Coarse-to-fine n-bestparsing and MaxEnt discriminative reranking.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL 2005).MFM Chowdhury and A Lavelli.
2010.
Disease mentionrecognition with specific features.
In Proceedings ofthe 2010 Workshop on Biomedical Natural LanguageProcessing, pages 83?90, Uppsala, Sweden, July.MFM Chowdhury and A Lavelli.
2011.
Drug-drug inter-action extraction using composite kernels.
In Proceed-ings of the 1st Challenge task on Drug-Drug Interac-tion Extraction (DDIExtraction 2011), pages 27?33,Huelva, Spain, September.MFM Chowdhury and A Lavelli.
2012a.
Combining treestructures, flat features and patterns for biomedical re-lation extraction.
In Proceedings of the 13th Confer-ence of the European Chapter of the Association forComputational Linguistics (EACL 2012), pages 420?429, Avignon, France, April.MFM Chowdhury and A Lavelli.
2012b.
Impact of LessSkewed Distributions on Efficiency and Effectivenessof Biomedical Relation Extraction.
In Proceedings ofthe 24th International Conference on ComputationalLinguistics (COLING 2012), Mumbai, India, Decem-ber.MFM Chowdhury and A Lavelli.
2013.
Exploiting theScope of Negations and Heterogeneous Features forRelation Extraction: A Case Study for Drug-Drug In-teraction Extraction.
In Proceedings of the 2013 Con-ference of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnology (NAACL 2013), Atlanta, USA, June.C Giuliano, A Lavelli, and L Romano.
2006.
Exploit-ing shallow linguistic information for relation extrac-tion from biomedical literature.
In Proceedings of the11th Conference of the European Chapter of the As-sociation for Computational Linguistics (EACL 2006),pages 401?408.T Joachims.
1999.
Making large-scale support vec-tor machine learning practical.
In Advances in ker-nel methods: support vector learning, pages 169?184.MIT Press, Cambridge, MA, USA.D Klein and C Manning.
2003.
Accurate unlexicalizedparsing.
In Proceedings of the 41st Annual Meetingof the Association for Computational Linguistics (ACL2003), pages 423?430, Sapporo, Japan.E Landau.
2009.
Jackson?s death raises ques-tions about drug interactions [Published in CNN;June 26, 2009].
http://edition.cnn.com/2009/HEALTH/06/26/jackson.drug.interaction.caution/index.html.D McClosky.
2010.
Any Domain Parsing: AutomaticDomain Adaptation for Natural Language Parsing.Ph.D.
thesis, Department of Computer Science, BrownUniversity.A Moschitti.
2004.
A study on convolution kernels forshallow semantic parsing.
In Proceedings of the 42ndAnnual Meeting of the Association for ComputationalLinguistics, ACL ?04, Barcelona, Spain.A Moschitti.
2006.
Making tree kernels practical for nat-ural language learning.
In Proceedings of 11th Confer-ence of the European Chapter of the Association forcomputational Linguistics (EACL 2006), pages 113?120, Trento, Italy.JW Payne.
2007.
A Dangerous Mix [Publishedin The Washington Post; February 27, 2007].http://www.washingtonpost.com/wp-dyn/content/article/2007/02/23/AR2007022301780.html.I Segura-Bedmar, P Mart?
?nez, and M Herrero-Zazo.2013.
SemEval-2013 task 9: Extraction of drug-druginteractions from biomedical texts.
In Proceedings ofthe 7th International Workshop on Semantic Evalua-tion (SemEval 2013), Atlanta, USA, June.355
