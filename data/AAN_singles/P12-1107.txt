Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1015?1024,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsSentence Simplification by Monolingual Machine TranslationSander WubbenTilburg UniversityP.O.
Box 901355000 LE TilburgThe Netherlandss.wubben@uvt.nlAntal van den BoschRadboud University NijmegenP.O.
Box 91036500 HD NijmegenThe Netherlandsa.vandenbosch@let.ru.nlEmiel KrahmerTilburg UniversityP.O.
Box 901355000 LE TilburgThe Netherlandse.j.krahmer@uvt.nlAbstractIn this paper we describe a method for simpli-fying sentences using Phrase Based MachineTranslation, augmented with a re-rankingheuristic based on dissimilarity, and trained ona monolingual parallel corpus.
We compareour system to a word-substitution baseline andtwo state-of-the-art systems, all trained andtested on paired sentences from the Englishpart of Wikipedia and Simple Wikipedia.
Hu-man test subjects judge the output of the dif-ferent systems.
Analysing the judgementsshows that by relatively careful phrase-basedparaphrasing our model achieves similar sim-plification results to state-of-the-art systems,while generating better formed output.
Wealso argue that text readability metrics suchas the Flesch-Kincaid grade level should beused with caution when evaluating the outputof simplification systems.1 IntroductionSentence simplification can be defined as the processof producing a simplified version of a sentence bychanging some of the lexical material and grammat-ical structure of that sentence, while still preservingthe semantic content of the original sentence, in or-der to ease its understanding.
Particularly languagelearners (Siddharthan, 2002), people with readingdisabilities (Inui et al, 2003) such as aphasia (Car-roll et al, 1999), and low-literacy readers (Watanabeet al, 2009) can benefit from this application.
It canserve to generate output in a specific limited format,such as subtitles (Daelemans et al, 2004).
Sentencesimplification can also serve to preprocess the inputof other tasks, such as summarization (Knight andMarcu, 2000), parsing, machine translation (Chan-drasekar et al, 1996), semantic role labeling (Vick-rey and Koller, 2008) or sentence fusion (Filippovaand Strube, 2008).The goal of simplification is to achieve an im-provement in readability, defined as the ease withwhich a text can be understood.
Some of the factorsthat are known to help increase the readability of textare the vocabulary used, the length of the sentences,the syntactic structures present in the text, and theusage of discourse markers.
One effort to create asimple version of English at the vocabulary level hasbeen the creation of Basic English by Charles KayOgden.
Basic English is a controlled language witha basic vocabulary consisting of 850 words.
Accord-ing to Ogden, 90 percent of all dictionary entries canbe paraphrased using these 850 words.
An exam-ple of a resource that is written using mainly BasicEnglish is the English Simple Wikipedia.
Articleson English Simple Wikipedia are similar to articlesfound in the traditional English Wikipedia, but writ-ten using a limited vocabulary (using Basic Englishwhere possible).
Generally the structure of the sen-tences in English Simple Wikipedia is less compli-cated and the sentences are somewhat shorter thanthose found in English Wikipedia; we offer more de-tailed statistics below.1.1 Related workMost earlier work on sentence simplificationadopted rule-based approaches.
A frequently ap-plied type of rule, aimed to reduce overall sentencelength, splits long sentences on the basis of syntactic1015information (Chandrasekar and Srinivas, 1997; Car-roll et al, 1998; Canning et al, 2000; Vickrey andKoller, 2008).
There has also been work on lexi-cal substitution for simplification, where the aim isto substitute difficult words with simpler synonyms,derived from WordNet or dictionaries (Inui et al,2003).Zhu et al (2010) examine the use of paired doc-uments in English Wikipedia and Simple Wikipediafor a data-driven approach to the sentence simplifi-cation task.
They propose a probabilistic, syntax-based machine translation approach to the problemand compare against a baseline of no simplificationand a phrase-based machine translation approach.In a similar vein, Coster and Kauchak (2011) usea parallel corpus of paired documents from Sim-ple Wikipedia and Wikipedia to train a phrase-basedmachine translation model coupled with a deletionmodel.
Another useful resource is the edit his-tory of Simple Wikipedia, from which simplifica-tions can be learned (Yatskar et al, 2010).
Woods-end and Lapata (2011) investigate the use of SimpleWikipedia edit histories and an aligned Wikipedia?Simple Wikipedia corpus to induce a model basedon quasi-synchronous grammar.
They select themost appropriate simplification by using integer lin-ear programming.We follow Zhu et al (2010) and Coster andKauchak (2011) in proposing that sentence simpli-fication can be approached as a monolingual ma-chine translation task, where the source and targetlanguages are the same and where the output shouldbe simpler in form from the input but similar inmeaning.
We differ from the approach of Zhu etal.
(2010) in the sense that we do not take syntac-tic information into account; we rely on PBMT todo its work and implicitly learn simplifying para-phrasings of phrases.
Our approach differs fromCoster and Kauchak (2011) in the sense that insteadof focusing on deletion in the PBMT decoding stage,we focus on dissimilarity, as simplification does notnecessarily imply shortening (Woodsend and Lap-ata, 2011), or as the Simple Wikipedia guidelinesstate, ?simpler does not mean short?1.
Table 1.1shows the average sentence length and the average1http://simple.wikipedia.org/wiki/Main_Page/Introductionword length for Wikipedia and Simple Wikipediasentences in the PWKP dataset used in this study(Zhu et al, 2010).
These numbers suggest that, al-though the selection criteria for sentences to be in-cluded in this dataset are biased (see Section 2.2),Simple Wikipedia sentences are about 17% shorter,while the average word length is virtually equal.Sent.
length Token lengthSimple Wikipedia 20.87 4.89Wikipedia 25.01 5.06Table 1: Sentence and token length statistics for thePWKP dataset (Zhu et al, 2010).Statistical machine translation (SMT) has alreadybeen successfully applied to the related task of para-phrasing (Quirk et al, 2004; Bannard and Callison-Burch, 2005; Madnani et al, 2007; Callison-Burch,2008; Zhao et al, 2009; Wubben et al, 2010).
SMTtypically makes use of large parallel corpora to traina model on.
These corpora need to be aligned atthe sentence level.
Large parallel corpora, such asthe multilingual proceedings of the European Parlia-ment (Europarl), are readily available for many lan-guages.
Phrase-Based Machine Translation (PBMT)is a form of SMT where the translation model aimsto translate longer sequences of words (?phrases?
)in one go, solving part of the word ordering problemalong the way that would be left to the target lan-guage model in a word-based SMT system.
PMBToperates purely on statistics and no linguistic knowl-edge is involved in the process: the phrases that arealigned are motivated statistically, rather than lin-guistically.
This makes PBMT adaptable to any lan-guage pair for which there is a parallel corpus avail-able.
The PBMT model makes use of a translationmodel, derived from the parallel corpus, and a lan-guage model, derived from a monolingual corpus inthe target language.
The language model is typicallyan n-gram model with smoothing.
For any given in-put sentence, a search is carried out producing ann-best list of candidate translations, ranked by thedecoder score, a complex scoring function includ-ing likelihood scores from the translation model,and the target language model.
In principle, all ofthis should be transportable to a data-driven machinetranslation account of sentence simplification, pro-1016vided that a parallel corpus is available that pairs textto simplified versions of that text.1.2 This studyIn this work we aim to investigate the use of phrase-based machine translation modified with a dissim-ilarity component for the task of sentence simplifi-cation.
While Zhu et al (2010) have demonstratedthat their approach outperforms a PBMT approachin terms of Flesch Reading Ease test scores, we arenot aware of any studies that evaluate PBMT for sen-tence simplification with human judgements.
In thisstudy we evaluate the output of Zhu et al (2010)(henceforth referred to as ?Zhu?
), Woodsend and La-pata (2011) (henceforth referred to as ?RevILP?
),our PBMT based system with dissimilarity-basedre-ranking (henceforth referred to as ?PBMT-R?
), aword-substitution baseline, and, as a gold standard,the original Simple Wikipedia sentences.
We willfirst discuss the baseline, followed by the Zhu sys-tem, the RevILP system, and our PBMT-R systemin Section 2.
We then describe the experiment withhuman judges in Section 3, and its results in Sec-tion 4.
We close this paper by critically discussingour results in Section 5.2 Sentence Simplification Models2.1 Word-Substitution BaselineThe word substitution baseline replaces words inthe source sentence with (near-)synonyms that aremore likely according to a language model.
Foreach noun, adjective and verb in the sentence thismodel takes that word and its part-of-speech tagand retrieves from WordNet al synonyms from allsynsets the word occurs in.
The word is then re-placed by all of its synset words, and each replace-ment is scored by a SRILM language model (Stol-cke, 2002) with probabilities that are obtained fromtraining on the Simple Wikipedia data.
The alter-native that has the highest probability according tothe language model is kept.
If no relevant alterna-tive is found, the word is left unchanged.
We usethe Memory-Based Tagger (Daelemans et al, 1996)trained on the Brown corpus to compute the part-of-speech tags.
The WordNet::QueryData2 Perl mod-2http://search.cpan.org/dist/WordNet-QueryData/QueryData.pmule is used to query WordNet (Fellbaum, 1998).2.2 Zhu et alZhu et al (2010) learn a sentence simplificationmodel which is able to perform four rewrite op-erations on the parse trees of the input sentences,namely substitution, reordering, splitting, and dele-tion.
Their model is inspired by syntax-basedSMT (Yamada and Knight, 2001) and consists ofa language model, a translation model and a de-coder.
The four mentioned simplification opera-tions together form the translation model.
Theirmodel is trained on a corpus containing aligned sen-tences from English Wikipedia and English SimpleWikipedia called PWKP.
The PWKP dataset con-sists of 108,016 pairs of aligned lines from 65,133Wikipedia and Simple Wikipedia articles.
These ar-ticles were paired by following the ?interlanguagelink?3.
TF*IDF at the sentence level was used toalign the sentences in the different articles (Nelkenand Shieber, 2006).Zhu et al (2010) evaluate their system usingBLEU and NIST scores, as well as various read-ability scores that only take into account the outputsentence, such as the Flesch Reading Ease test andn-gram language model perplexity.
Although theirsystem outperforms several baselines at the level ofthese readability metrics, they do not achieve betterwhen evaluated with BLEU or NIST.2.3 RevILPWoodsend and Lapata?s (2011) model is basedon quasi-synchronous grammar (Smith and Eisner,2006).
Quasi-synchronous grammar generates aloose alignment between parse trees.
It operates onindividual sentences annotated with syntactic infor-mation in the form of phrase structure trees.
Quasi-synchronous grammar is used to generate all pos-sible rewrite operations, after which integer linearprogramming is employed to select the most ap-propriate simplification.
Their model is trained ontwo different datasets: one containing alignmentsbetween Wikipedia and English Simple Wikipedia(AlignILP), and one containing alignments betweenedits in the revision history of Simple Wikipedia(RevILP).
RevILP performs best according to the3http://en.wikipedia.org/wiki/Help:Interlanguage_links1017human judgements conducted in their study.
Theyshow that it achieves better scores than Zhu et al(2010)?s system and is not scored significantly dif-ferently from English Simple Wikipedia.
In thisstudy we compare against their best performing sys-tem, the RevILP system.0 2 4 6 8 10 12 14 16 1801234n-bestLevenshteinDistance0 2 4 6 8 10 12 14 16 1802468101214n-bestFlesch-KincaidFigure 1: Levenshtein distance and Flesch-Kincaid scoreof output when varying the n of the n-best output ofMoses.2.4 PBMT-RWe use the Moses software to train a PBMTmodel (Koehn et al, 2007).
The data we use is thePWKP dataset created by Zhu et al (2010).
In gen-eral, a statistical machine translation model finds abest translation e?
of a text in language f to a textin language e by combining a translation model thatfinds the most likely translation p(f |e) with a lan-guage model that outputs the most likely sentencep(e):e?
= argmaxe?e?p(f |e)p(e)The GIZA++ statistical alignment package isused to perform the word alignments, which arelater combined into phrase alignments in the Mosespipeline (Och and Ney, 2003) to build the sentencesimplification model.
GIZA++ utilizes IBM Models1 to 5 and an HMM word alignment model to findstatistically motivated alignments between words.We first tokenize and lowercase all data and use allunique sentences from the Simple Wikipedia partof the PWKP training set to train an n-gram lan-guage model with the SRILM toolkit to learn theprobabilities of different n-grams.
Then we invokethe GIZA++ aligner using the training simplifica-tion pairs.
We run GIZA++ with standard settingsand we perform no optimization.
This results in aphrase table containing phrase pairs from Wikipediaand Simple Wikipedia and their conditional proba-bilities as assigned by Moses.
Finally, we use theMoses decoder to generate simplifications for thesentences in the test set.
For each sentence we letthe system generate the ten best distinct solutions(or less, if fewer than ten solutions are generated) asranked by Moses.Arguably, dissimilarity is a key factor in simpli-fication (and in paraphrasing in general).
As outputwe would like to be able to select fluent sentencesthat adequately convey the meaning of the originalinput, yet that contain differences that operational-ize the intended simplification.
When training ourPBMT system on the PWKP data we may assumethat the system learns to simplify automatically, yetthere is no aspect of the decoder function in Mosesthat is sensitive to the fact that it should try to bedifferent from the input ?
Moses may well trans-late input to unchanged output, as much of our train-ing data consists of partially equal input and outputstrings.To expand the functionality of Moses in the in-tended direction we perform post-hoc re-ranking onthe output based on dissimilarity to the input.
Wedo this to select output that is as different as possi-ble from the source sentence, so that it ideally con-1018tains multiple simplifications; at the same time, webase our re-ranking on a top-n of output candidatesaccording to Moses, with a small n, to ensure thatthe quality of the output in terms of fluency and ade-quacy is also controlled for.
Setting n = 10, for eachsource sentence we re-rank the ten best sentencesas scored by the decoder according to the Leven-shtein Distance (or edit distance) measure (Leven-shtein, 1966) at the word level between the inputand output sentence, counting the minimum num-ber of edits needed to transform the source stringinto the target string, where the allowable edit op-erations are insertion, deletion, and substitution of asingle word.
In case of a tie in Levenshtein Distance,we select the sequence with the better decoder score.When Moses is unable to generate ten different sen-tences, we select from the lower number of outputs.Figure 1 displays Levenshtein Distance and Flesch-Kincaid grade level scores for different values of n.We use the Lingua::EN::Fathom module4 to calcu-late Flesch-Kincaid grade level scores.
The read-ability score stays more or less the same, indicatingno relation between n and readability.
The averageedit distance starts out at just above 2 when selectingthe 1-best output string, and increases roughly untiln = 10.2.5 Descriptive statisticsTable 2 displays the average edit distance and thepercentage of cases in which no edits were per-formed for each of the systems and for SimpleWikipedia.
We see that the Levenshtein distance be-tween Wikipedia and Simple Wikipedia is the mostsubstantial with an average of 12.3 edits.
Giventhat the average number of tokens is about 25 forWikipedia and 21 for Simple Wikipedia (cf.
Ta-ble 1.1), these numbers indicate that the changes inSimple Wikipedia go substantially beyond the aver-age four-word length difference.
On average, eightmore words are interchanged for other words.
Abouthalf of the original tokens in the source sentence donot return in the output.
Of the three simplifica-tion systems, the Zhu system (7.95) and the RevILP(7.18) attain similar edit distances, less substantialthan the edits in Simple Wikipedia, but still consid-4http://http://search.cpan.org/?kimryan/Lingua-EN-Fathom-1.15/lib/Lingua/EN/Fathom.pmerable compared to the baseline word-substitutionsystem (4.26) and PBMT-R (3.08).
Our system isclearly conservative in its edits.System LD Perc.
no editsSimple Wikipedia 12.30 3Word Sub 4.26 0Zhu 7.95 2RevILP 7.18 22PBMT-R 3.08 5Table 2: Levenshtein Distance and percentage of unal-tered output sentences.On the other hand, we observe some differencesin the percentage of cases in which the systems de-cide to produce a sentence identical to the input.In 22 percent of the cases the RevILP system doesnot alter the sentence.
The other systems make thisdecision about as often as the gold standard, Sim-ple Wikipedia, where only 3% of sentences remainunchanged.
The word-substitution baseline alwaysmanages to make at least one change.3 Evaluation3.1 ParticipantsParticipants were 46 students of Tilburg University,who participated for partial course credits.
All werenative speakers of Dutch, and all were proficient inEnglish, having taken a course on Academic Englishat University level.3.2 MaterialsWe use the test set used by Zhu et al (2010) andWoodsend and Lapata (2011).
This test set consistsof 100 sentences from articles on English Wikipedia,paired with sentences from corresponding articles inEnglish Simple Wikipedia.
We selected only thosesentences where every system would perform min-imally one edit, because we only want to comparethe different systems when they actually generate al-tered, assumedly simplified output.
From this sub-set we randomly pick 20 source sentences, result-ing in 20 clusters of one source sentence and 5 sim-plified sentences, as generated by humans (SimpleWikipedia) and the four systems.10193.3 ProcedureThe participants were told that they participated inthe evaluation of a system that could simplify sen-tences, and that they would see one source sentenceand five automatically simplified versions of thatsentence.
They were not informed of the fact that weevaluated in fact four different systems and the orig-inal Simple Wikipedia sentence.
Following earlierevaluation studies (Doddington, 2002; Woodsendand Lapata, 2011), we asked participants to evalu-ate Simplicity, Fluency and Adequacy of the targetheadlines on a five point Likert scale.
Fluency wasdefined in the instructions as the extent to which asentence is proper, grammatical English.
Adequacywas defined as the extent to which the sentence hasthe same meaning as the source sentence.
Simplic-ity was defined as the extent to which the sentencewas simpler than the original and thus easier to un-derstand.
The order in which the clusters had to bejudged was randomized and the order of the outputof the various systems was randomized as well.4 Results4.1 Automatic measuresThe results of the automatic measures are displayedin Table 3.
In terms of the Flesch-Kincaid gradelevel score, where lower scores are better, the Zhusystem scores best, with 7.86 even lower than Sim-ple Wikipedia (8.57).
Increasingly worse Flesch-Kincaid scores are produced by RevILP (8.61) andPBMT-R (13.38), while the word substitution base-line scores worst (14.64).
With regard to the BLEUscore, where Simple Wikipedia is the reference, thePBMT-R system scores highest with 0.43, followedby the RevILP system (0.42) and the Zhu system(0.38).
The word substitution baseline scores low-est with a BLEU score of 0.34.System Flesch-Kincaid BLEUSimple Wikipedia 8.57 1Word Sub 14.64 0.34Zhu 7.86 0.38RevILP 8.61 0.42PBMT-R 13.38 0.43Table 3: Flesch-Kincaid grade level and BLEU scores4.2 Human judgementsTo test for significance we ran repeated mea-sures analyses of variance with system (Sim-ple Wikipedia, PBMT-R, Zhu, RevILP, word-substitution baseline) as the independent variable,and the three individual metrics as well as their com-bined mean as the dependent variables.
Mauchlystest for sphericity was used to test for homogeneityof variance, and when this test was significant weapplied a Greenhouse-Geisser correction on the de-grees of freedom (for the purpose of readability wereport the normal degrees of freedom in these cases).Planned pairwise comparisons were made with theBonferroni method.
Table 4 displays these results.First, we consider the 3 metrics in isolation, be-ginning with Fluency.
We find that participantsrated the Fluency of the simplified sentences fromthe four systems and Simple Wikipedia differently,F (4, 180) = 178.436, p < .001, ?2p = .799.
Theword-substitution baseline, Simple Wikipedia andPBMT-R receive the highest scores (3.86, 3.84 and3.83 respectively) and don?t achieve significantlydifferent scores on this dimension.
All other pair-wise comparisons are significant at p < .001.
Rev-ILP attains a score of 3.18, while the Zhu systemachieves the lowest mean judgement score of 2.59.Participants also rated the systems significantlydifferently on the Adequacy scale, F (4, 180) =116.509, p < .001, ?2p = .721.
PBMT-R scoreshighest (3.71), followed by the word-substitutionbaseline (3.58), RevILP (3.28), and then by SimpleWikipedia (2.91) and the Zhu system (2.82).
Sim-ple Wikipedia and the Zhu system do not differ sig-nificantly, and all other pairwise comparisons aresignificant at p < .001.
The low score of SimpleWikipedia indicates indirectly that the human edi-tors of Simple Wikipedia texts often choose to devi-ate quite markedly from the meaning of the originaltext.Key to the task of simplification are the hu-man judgements of Simplicity.
Participants ratedthe Simplicity of the output from the four sys-tems and Simple Wikipedia differently, F (4, 180) =74.959, p < .001, ?2p = .625.
Simple Wikipediascores highest (3.68) and the word substitution base-line scores lowest (2.42).
Between them are theRevILP (2.96), Zhu (2.93) and PBMT-R (2.88) sys-1020System Overall Fluency Adequacy SimplicitySimple Wikipedia 3.46 (0.39) 3.84 (0.46) 2.91 (0.32) 3.68 (0.39)Word Sub 3.39 (0.43) 3.86 (0.49) 3.58 (0.35) 2.42 (0.48)Zhu 2.78 (0.45) 2.59 (0.48) 2.82 (0.37) 2.93 (0.50)RevILP 3.13 (0.36) 3.18 (0.45) 3.28 (0.32) 2.96 (0.39)PBMT-R 3.47 (0.46) 3.83 (0.49) 3.71 (0.44) 2.88 (0.46)Table 4: Mean scores assigned by human subjects, with the standard deviation between bracketsAdequacy Simplicity Flesch-Kincaid BLEUFluency 0.45** 0.24* 0.42** 0.26**Adequacy -0.19 0.40** -0.14Simplicity -0.45** 0.42**Flesch-Kincaid -0.11Table 5: Pearson correlation between the different dimensions as assigned by humans and the automatic metrics.Scores marked * are significant at p < .05 and scores marked ** are significant at p < .01tems, which do not score significantly differentlyfrom each other.
All other pairwise comparisons aresignificant at p < .001.Finally we report on a combined score created byaveraging over the Fluency, Adequacy and Simplic-ity scores.
Inspection of this score, displayed in theleftmost column of Table 4, reveals that the PBMT-R system and Simple Wikipedia score best (3.47and 3.46 respectively), followed by the word substi-tution baseline (3.39), which in turn scores higherthan RevILP (3.13) and the Zhu system (2.78).We find that participants rated the systems signifi-cantly differently overall, F (4, 180) = 98.880, p <.001, ?2p = .687.
All pairwise comparisons were sta-tistically significant (p < .01), except the one be-tween the PBMT-R system and Simple Wikipedia.4.3 CorrelationsTable 5 displays the correlations between the scoresassigned by humans (Fluency, Adequacy and Sim-plicity) and the automatic metrics (Flesch-Kincaidand BLEU).
We see a significant correlation be-tween Fluency and Adequacy (0.45), as well as be-tween Fluency and Simplicity (0.24).
There is a neg-ative significant correlation between Flesch-Kincaidscores and Simplicity (-0.45) while there is a posi-tive significant correlation between Flesch-Kincaidand Adequacy and Fluency.
The significant correla-tions between BLEU and Simplicity (0.42) and Flu-ency (0.26) are both in the positive direction.
Thereis no significant correlation between BLEU and Ad-equacy, indicating BLEU?s relative weakness in as-sessing the semantic overlap between input and out-put.
BLEU and Flesch-Kincaid do not show a sig-nificant correlation.5 DiscussionWe conclude that a phrase-based machine trans-lation system with added dissimilarity-based re-ranking of the best ten output sentences can suc-cessfully be used to perform sentence simplifica-tion.
Even though the system merely performsphrase-based machine translation and is not specif-ically geared towards simplification were it not forthe dissimilarity-based re-ranking of the output, itperforms not significantly differently from state-of-the-art sentence simplification systems in terms ofhuman-judged Simplification.
In terms of Fluencyand Adequacy our system is judged to perform sig-nificantly better.
From the relatively low averagenumbers of edits made by our system we can con-clude that our system performs relatively small num-bers of changes to the input, that still constitute assensible simplifications.
It does not split sentences(which the Zhu and RevILP systems regularly do);it only rephrases phrases.
Yet, it does this betterthan a word-substitution baseline, which can also beconsidered a conservative approach; this is reflectedin the baseline?s high Fluency score (roughly equalto PBMT-R and Simple Wikipedia) and Adequacyscore (only slightly worse than PBMT-R).1021Wikipedia the judge ordered that chapman should receive psychiatric treatment in prison and sentencedhim to twenty years to life , slightly less than the maximum possible of twenty-five years tolife .SimpleWikipediahe was sentenced to twenty-five years to life in prison in 1981 .Word-substitutionbaselinethe judge ordered that chapman should have psychiatric treatment in prison and sentencedhim to twenty years to life , slightly less than the maximum possible of twenty-five years tolife .Zhu the judge ordered that chapman should get psychiatric treatment .
in prison and sentencedhim to twenty years to life , less maximum possible of twenty-five years to life .RevILP the judge ordered that chapman should will get psychiatric treatment in prison .
he sentencedhim to twenty years to life to life .PBMT-R the judge ordered that chapman should get psychiatric treatment in prison and sentenced himto twenty years to life , a little bit less than the highest possible to twenty-five years to life .Table 6: Example outputThe output of all systems, the original and thesimplified version of an example sentence from thePWKP dataset is displayed in Table 6.
The SimpleWikipedia sentences illustrate that significant por-tions of the original sentences may be dropped, andparts of the semantics of the original sentence dis-carded.
We also see the Zhu and RevILP systemsresorting to splitting the original sentence in two,leading to better Flesch-Kincaid scores.
The word-substitution baseline changes ?receive?
in ?have?,while the PBMT-R system changes the same ?re-ceive?
in ?get?, ?slightly?
to ?a little bit?, and ?maxi-mum?
to ?highest?.In terms of automatic measures we see that theZhu system scores particularly well on the Flesch-Kincaid metric, while the RevILP system and ourPBMT-R system achieve the highest BLEU scores.We believe that for the evaluation of sentence sim-plification, BLEU is a more appropriate metric thanFlesch-Kincaid or a similar readability metric, al-though it should be noted that BLEU was found onlyto correlate significantly with Fluency, not with Ad-equacy.
While BLEU and NIST may be used withthis in mind, readability metrics should be avoidedaltogether in our view.
Where machine translationevaluation metrics such as BLEU take into accountgold references, readability metrics only take intoaccount characteristics of the sentence such as wordlength and sentence length, and ignore grammatical-ity or the semantic adequacy of the content of theoutput sentence, which BLEU is aimed to implic-itly approximate by measuring overlap in n-grams.Arguably, readability metrics are best suited to beapplied to texts that can be considered grammati-cal and meaningful, which is not necessarily true forthe output of simplification algorithms.
A disrup-tive example that would illustrate this point wouldbe a system that would randomly split original sen-tences in two or more sequences, achieving consid-erably lower Flesch-Kincaid scores, yet damagingthe grammaticality and semantic coherence of theoriginal text, as is evidenced by the negative cor-relation for Simplicity and positive correlations forFluency and Adequacy in Table 5.In the future we would like to investigate how wecan boost the number of edits the system performs,while still producing grammatical and meaning-preserving output.
Although the comparison againstthe Zhu system, which uses syntax-driven machinetranslation, shows no clear benefit for syntax-basedmachine translation, it may still be the case that ap-proaches such as Hiero (Chiang et al, 2005) andJoshua (Li et al, 2009), enhanced by dissimilarity-based re-ranking, would improve over our currentsystem.
Furthermore, typical simplification oper-ations such as sentence splitting and more radicalsyntax alterations or even document-level operationssuch as manipulations of the co-reference structurewould be interesting to implement and testAcknowledgementsWe are grateful to Zhemin Zhu and Kristian Woods-end for sharing their data.
We would also like tothank the anonymous reviewers for their comments.1022ReferencesColin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In ACL ?05:Proceedings of the 43rd Annual Meeting on Associ-ation for Computational Linguistics, pages 597?604,Morristown, NJ, USA.
Association for ComputationalLinguistics.Chris Callison-Burch.
2008.
Syntactic constraintson paraphrases extracted from parallel corpora.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?08,pages 196?205, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Yvonne Canning, John Tait, Jackie Archibald, and RosCrawley.
2000.
Cohesive regeneration of syntacti-cally simplified newspaper text.
In Proceedings of RO-MAND 2000, Lausanne.John Carroll, Guido Minnen, Yvonne Canning, SiobhanDevlin, and John Tait.
1998.
Practical simplificationof English newspaper text to assist aphasic readers.In AAAI-98 Workshop on Integrating Artificial Intelli-gence and Assistive Technology, Madison, Wisconsin.John Carroll, Guido Minnen, Darren Pearce, YvonneCanning, Siobhan Devlin, and John Tait.
1999.
Sim-plifying text for language-impaired readers.
In Pro-ceedings of EACL?99, Bergen.
ACL.R.
Chandrasekar and B. Srinivas.
1997.
Automaticrules for text simplification.
Knowledge-Based Sys-tems, 10:183?190.Raman Chandrasekar, Christine Doran, and BangaloreSrinivas.
1996.
Motivations and methods for textsimplification.
In Proceedings of the Sixteenth In-ternational Conference on Computational Linguistics(COLING?96), pages 1041?1044.David Chiang, Adam Lopez, Nitin Madnani, ChristofMonz, Philip Resnik, and Michael Subotin.
2005.
Thehiero machine translation system: extensions, evalua-tion, and analysis.
In Proceedings of the conference onHuman Language Technology and Empirical Methodsin Natural Language Processing, HLT ?05, pages 779?786, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Will Coster and David Kauchak.
2011.
Learning tosimplify sentences using wikipedia.
In Proceedingsof the Workshop on Monolingual Text-To-Text Gener-ation, pages 1?9, Portland, Oregon, June.
Associationfor Computational Linguistics.Walter Daelemans, Jakub Zavrel, Peter Berck, and StevenGillis.
1996.
MBT: A Memory-Based Part of SpeechTagger-Generator.
In Proc.
of Fourth Workshop onVery Large Corpora, pages 14?27.
ACL SIGDAT.Walter Daelemans, Anja Hothker, and Erik TjongKim Sang.
2004.
Automatic sentence simplificationfor subtitling in dutch and english.
In Proceedingsof the 4th International Conference on Language Re-sources and Evaluation, pages 1045?1048.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of the second interna-tional conference on Human Language TechnologyResearch, HLT ?02, pages 138?145, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
The MIT Press, May.Katja Filippova and Michael Strube.
2008.
Sentence fu-sion via dependency graph compression.
In Proceed-ings of the 2008 Conference on Empirical Methods inNatural Language Processing, pages 177?185, Hon-olulu, Hawaii, October.
Association for ComputationalLinguistics.Kentaro Inui, Atsushi Fujita, Tetsuro Takahashi, RyuIida, and Tomoya Iwakura.
2003.
Text simplificationfor reading assistance: A project note.
In Proceedingsof the Second International Workshop on Paraphras-ing, pages 9?16, Sapporo, Japan, July.
Association forComputational Linguistics.Kevin Knight and Daniel Marcu.
2000.
Statistics-basedsummarization ?
step one: Sentence compression.
InProceedings of the 17th National Conference on Ar-tificial Intelligence (AAAI), pages 703 ?
710, Austin,Texas, USA, July 30 ?
August 3.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris C.Burch, Marcello Federico, Nicola Bertoldi, BrookeCowan, Wade Shen, Christine Moran, Richard Zens,Chris Dyer, Ondrej Bojar, Alexandra Constantin, andEvan Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In ACL.
The Associa-tion for Computer Linguistics.V.
Levenshtein.
1966.
Binary codes capable of correct-ing deletions, insertions, and reversals.
Soviet PhysicsDoklady, 10(8):707?710.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenN.
G. Thornton, Jonathan Weese, and Omar F. Zaidan.2009.
Joshua: an open source toolkit for parsing-based machine translation.
In Proceedings of theFourth Workshop on Statistical Machine Translation,pages 135?139, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Nitin Madnani, Necip Fazil Ayan, Philip Resnik, andBonnie J. Dorr.
2007.
Using paraphrases for pa-rameter tuning in statistical machine translation.
InProceedings of the Second Workshop on StatisticalMachine Translation, StatMT ?07, pages 120?127,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.1023Rani Nelken and Stuart M. Shieber.
2006.
Towards ro-bust context-sensitive sentence alignment for monolin-gual corpora.
In Proceedings of the 11th Conferenceof the European Chapter of the Association for Com-putational Linguistics (EACL-06), Trento, Italy, 3?7April.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Comput.
Linguist., 29(1):19?51, March.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual machine translation for paraphrase gen-eration.
In Dekang Lin and Dekai Wu, editors, Pro-ceedings of EMNLP 2004, pages 142?149, Barcelona,Spain, July.
Association for Computational Linguis-tics.Advaith Siddharthan.
2002.
An architecture for a textsimplification system.
In Language Engineering Con-ference, page 64.
IEEE Computer Society.David A. Smith and Jason Eisner.
2006.
Quasi-synchronous grammars: Alignment by soft projectionof syntactic dependencies.
In Proceedings of the HLT-NAACL Workshop on Statistical Machine Translation,pages 23?30, New York, June.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In In Proc.
Int.
Conf.
onSpoken Language Processing, pages 901?904, Denver,Colorado.D.
Vickrey and D. Koller.
2008.
Sentence simplificationfor semantic role labeling.
In Proceedings of the 46thMeeting of the Association for Computational Linguis-tics: Human Language Technologies.Willian Massami Watanabe, Arnaldo Candido Junior,Vincius Rodriguez de Uz?da, Renata Pontin de Mat-tos Fortes, Thiago Alexandre Salgueiro Pardo, andSandra M. Alusio.
2009.
Facilita: reading assistancefor low-literacy readers.
In Brad Mehlenbacher, Aris-tidis Protopsaltis, Ashley Williams, and Shaun Slat-tery, editors, SIGDOC, pages 29?36.
ACM.Kristian Woodsend and Mirella Lapata.
2011.
Learningto simplify sentences with quasi-synchronous gram-mar and integer programming.
In Proceedings ofthe 2011 Conference on Empirical Methods in Natu-ral Language Processing, pages 409?420, Edinburgh,Scotland, UK., July.
Association for ComputationalLinguistics.Sander Wubben, Antal van den Bosch, and Emiel Krah-mer.
2010.
Paraphrase generation as monolingualtranslation: data and evaluation.
In Proceedings of the6th International Natural Language Generation Con-ference, INLG ?10, pages 203?207, Stroudsburg, PA,USA.
Association for Computational Linguistics.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedings ofthe 39th Annual Meeting on Association for Computa-tional Linguistics, ACL ?01, pages 523?530, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of simplic-ity: Unsupervised extraction of lexical simplificationsfrom Wikipedia.
In Proceedings of the NAACL, pages365?368.Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li.
2009.Application-driven statistical paraphrase generation.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP: Volume 2 - Volume 2, ACL ?09,pages 834?842, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation model forsentence simplification.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics(Coling 2010), pages 1353?1361, Beijing, China, Au-gust.
Coling 2010 Organizing Committee.1024
