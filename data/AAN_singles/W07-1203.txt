Proceedings of the 5th Workshop on Important Unresolved Matters, pages 17?24,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsFilling Statistics with Linguistics ?Property Design for the Disambiguation of German LFG ParsesMartin ForstInstitute of Natural Language ProcessingUniversity of Stuttgart, Germanyforst@ims.uni-stuttgart.deAbstractWe present a log-linear model for the disam-biguation of the analyses produced by a Ger-man broad-coverage LFG, focussing on theproperties (or features) this model is basedon.
We compare this model to an initialmodel based only on a part of the proper-ties provided to the final model and observethat the performance of a log-linear modelfor parse selection depends heavily on thetypes of properties that it is based on.
Inour case, the error reduction achieved withthe log-linear model based on the extendedset of properties is 51.0% and thus com-pares very favorably to the error reductionof 34.5% achieved with the initial model.1 IntroductionIn the development of stochastic disambiguationmodules for ?deep?
grammars, relatively much workhas gone into the definition of suitable probabilitymodels and the corresponding learning algorithms.Property design, on the contrary, has rather been un-deremphasized, and the properties used in stochas-tic disambiguation modules are most often presentedonly superficially.
This paper?s aim is to draw moreattention to property design by presenting linguisti-cally motivated properties that are used for the dis-ambiguation of the analyses produced by a Germanbroad-coverage LFG and by showing that propertydesign is of crucial importance for the quality ofstochastic models for parse selection.We present, in Section 2, the system that the dis-ambiguation module was developed for as well asthe initially used properties.
In Section 3, we thenpresent a selection of the properties that were ex-pressly designed for the resolution of frequent ambi-guities in German LFG parses.
Section 4 describesexperiments that we carried out with log-linear mod-els based on the initial set of properties and on anextended one.
Section 5 concludes.2 Background2.1 The German ParGram LFGThe grammar for which the log-linear model forparse selection described in this paper was devel-oped is the German ParGram LFG (Dipper, 2003;Rohrer and Forst, 2006).
It has been developed withand for the grammar development and processingplatform XLE (Crouch et al, 2006) and consists ofa symbolic LFG, which can be employed both forparsing and generation, and a two-stage disambigua-tion module, the log-linear model being the compo-nent that carries out the final selection among theparses that have been retained by an Optimality-Theoretically inspired prefilter (Frank et al, 2001;Forst et al, 2005).The grammar has a coverage in terms of fullparses that exceeds 80% on newspaper corpora.
Forsentences out of coverage, it employs the robust-ness techniques (fragment parsing, ?skimming?)
im-plemented in XLE and described in Riezler et al(2002), so that 100% of our corpus sentences receiveat least some sort of analysis.
A dependency-basedevaluation of the analyses produced by the grammaron the TiGer Dependency Bank (Forst et al, 2004)results in an F-score between 80.42% on all gram-17matical relations and morphosyntactic features (or72.59% on grammatical relations only) and 85.50%(or 79.36%).
The lower bound is based on an ar-bitrary selection among the parses built up by thesymbolic grammar; the upper bound is determinedby the best possible selection.2.2 Log-linear models for disambiguationSince Johnson et al (1999), log-linear models ofthe following form have become standard as disam-biguation devices for precision grammars:P?
(x|y) = e?mj=1 ?j ?fj(x,y)?x?
?X(y) e?mj=1 ?j ?fj(x?,y)They are used for parse selection in the English Re-source Grammar (Toutanova et al, 2002), the En-glish ParGram LFG (Riezler et al, 2002), the En-glish Enju HPSG (Miyao and Tsujii, 2002), theHPSG-inspired Alpino parser for Dutch (Maloufand van Noord, 2004; van Noord, 2006) and theEnglish CCG from Edinburgh (Clark and Curran,2004).While relatively much work has gone into thequestion of how to estimate the property weights?1 .
.
.
?m efficiently and accurately on the basisof (annotated) corpus data, the question of howto define suitable and informative property func-tions f1 .
.
.
fm has received relatively little attention.However, we are convinced that property design isthe possibility of improving log-linear models forparse selection now that the machine learning ma-chinery is relatively well established.2.3 Initially used properties for disambiguationThe first set of properties with which we conductedexperiments was built on the model of the propertyset used for the disambiguation of English ParGramLFG parses (Riezler et al, 2002; Riezler and Vasser-man, 2004).
These properties are defined with thehelp of thirteen property templates, which are pa-rameterized for c-structure categories, f-structure at-tributes and/or their possible values.
The templatesare hardwired in XLE, which allows for a very ef-ficient extraction of properties based on them frompacked c-/f-structure representations.
The downsideof the templates being hardwired, however, is that, atleast at first sight, the property developer is confinedto what the developers of the property templates an-ticipated as potentially relevant for disambiguationor, more precisely, for the disambiguation of EnglishLFG analyses.The thirteen property templates can be subdi-vided into c-structure-based property templates andf-structure-based ones.
The c-structure-based prop-erty templates are:?
cs label <XP>: counts the number of XPnodes in the c-structure of an analysis.?
cs num children <XP>: counts the num-ber of children of allXP nodes in a c-structure.?
cs adjacent label <XP> <YP>:counts the number of XP nodes that immedi-ately dominate a Y P node.?
cs sub label <XP> <YP>: counts thenumber ofXP nodes that dominate a Y P node(at arbitrary depth).?
cs embedded <XP> <n>: counts thenumber of XP nodes that dominate n otherdistinct XP nodes (at arbitrary depth).?
cs conj nonpar <n>: counts the numberof coordinated constituents that are not parallelat the nth level of embedding.?
cs right branch: counts the number ofright children in the c-structure of an analysis.The f-structure-based property templates are:?
fs attrs <Attr1 ... Attrn>:counts the number of times that attributesAttr1 .
.
.
Attrn occur in the f-structure of ananalysis.?
fs attr val <Attr> <Val>: countsthe number of times that the atomic attributeAttr has the value V al.?
fs adj attrs <Attr1> <Attr2>:counts the number of times that the com-plex attribute Attr1 immediately embeds theattribute Attr2.?
fs subattr <Attr1> <Attr2> countsthe number of times that the complex attributeAttr1 embeds the attribute Attr2 (at arbitrarydepth).?
lex subcat <Lemma> <SCF1 ...SCFn>: counts the number of times thatthe subcategorizing element Lemma occurswith one of the subcategorization framesSCF1 .
.
.
SCFn.18?
verb arg <Lemma> <GF>: counts thenumber of times that the element Lemma sub-categorizes for the argument GF .Automatically instantiating these templates for allc-structure categories, f-structure attributes and val-ues used in the German ParGram LFG as well as forall lexical elements present in its lexicon results in460,424 properties.3 Property design for the disambiguationof German LFG parsesDespite the very large number of properties that canbe directly constructed on the basis of the thirteenproperty templates provided by XLE, many com-mon ambiguities in German LFG parses cannot becaptured by any of these.3.1 Properties that record the relative linearorder of functionsConsider, e.g., the SUBJ-OBJ ambiguity in (1).
(1) [.
.
.
][.
.
.
]peiltaims[S/O dastheManagement]management[O/S eina?sichtbar?visiblyverbessertes?improved?Ergebnis]resultan.at.?[.
.
. ]
the management aims at a ?visibly im-proved?
result.?
(TIGER Corpus s20834)The c-structure is shared by the two readingsof the sentence, so that c-structure-based proper-ties cannot contribute to the selection of the cor-rect reading; the only f-structure-based proper-ties that differ between the two analyses are ofthe kinds fs adj attrs SUBJ ADJUNCT andfs subattr OBJ ADJUNCT, which are only re-motely, if at all, related to the observed SUBJ-OBJambiguity.
The crucial information from the in-tended reading, namely that the SUBJ precedes theOBJ, is not captured directly by any of the ini-tial properties.
We therefore introduce a new prop-erty template that records the linear order of twogrammatical functions and instantiate it for all rel-evant combinations.
The new properties createdthis way make it possible to capture the defaultorder of nominal arguments, which according toLenerz (1977) and Uszkoreit (1987) (among others),is SUBJ, OBJ-TH, OBJ.Similarly to the SUBJ-OBJ ambiguity just con-sidered, the ADJUNCT-OBL ambiguity in (2) can-not at all be resolved on the basis of c-structure-based properties, and the f-structure-based proper-ties whose values differ among the two readingsseem only remotely related to the observed ambi-guity.
(2) [A/O Dagegen]Against that/In contrastsprachspokesichhimself[.
.
.
][.
.
.
]MichaMichaGuttmannGuttmann[O/A fu?rforgetrennteseparateGedenksta?tten]memorialsaus.out.
?In contrast, [.
.
. ]
Michael Guttmann arguedfor separate memorials.?
(s2090)However, the literature on constituent order in Ger-man, e.g.
Helbig and Buscha (2001), documentsthe tendency of ADJUNCT PPs to precede OBL PPs,which also holds in (2).
We therefore introducedproperties that record the relative linear order of AD-JUNCT PPs and OBL PPs.3.2 Properties that consider the nature of aconstituent wrt.
its functionAlthough linear order plays a major role in the func-tional interpretation of case-ambiguous DPs in Ger-man, it is only one among several ?soft?
constraintsinvolved.
The nature of such a DP may actually alsogive hints to its grammatical function.The tendency of SUBJs to be high on the defi-niteness scale and the animacy scale as well as thetendency of OBJs to be low on these scales hasmainly been observed in studies on differential ob-ject/subject marking (see, e.g., Aissen (2003)).
Nev-ertheless, these tendencies also seem to hold in lan-guages like German, which does not exhibit differ-ential object/subject marking.
In (3), the indefiniteinanimate DP is to be interpreted as the OBJ of thesentence and the definite human DP, as its SUBJ al-though the former precedes the latter.
(3) [O/S NahezuNearlystabilestablePreise]pricesprognostizierenforecast[S/O diethebayerischenBavarianExperten]experts[.
.
.
][.
.
.
].
?The Bavarian experts forecast nearly stableprices [.
.
.
].?
(s7357)19In order to allow these regularities to belearned from corpus data, we defined addi-tional property templates like isDef <GF> andisHuman <GF>,1 which are instantiated for all rel-evant grammatical functions.3.3 Properties for the resolution of attachmentambiguities concerning extraposedconstituentsA further common ambiguity in German con-cerns the functional attachment of extraposed con-stituents, such as relative clauses, dass clauses andinfinitival VPs.
In (4), e.g., there is no hard con-straint that would allow us to determine whether therelative clause modifies Rolle or Autoversicherung.
(4) EineAzentralecentralRollerole[.
.
.
][.
.
.
]kommtcomesdertheAutoversicherungcar insurancezu,to,diewhicheinaFu?nftelfifth[.
.
.
][.
.
.
]vereinnahmt.receives.
?There is a central role for the car insurance,which receives a fifth [.
.
.
].?
(s27539)In order to allow for an improved resolution ofthis kind of attachment ambiguity, we introducedproperties that extract the surface distance of an ex-traposed constituent to its functional head as well asproperties that record how the functional uncertaintypaths involved in these attachments were instanti-ated.
This way, we hope to extract the informationnecessary to model the tendencies observed, e.g., inUszkoreit et al (1998).3.4 Lexicalized properties capturingdependenciesInspired by Malouf and van Noord (2004),we finally also introduced lexicalized proper-ties capturing dependencies.
These are builton the following property templates: DEP12<PoS1> <Dep> <PoS2> <Lemma2>, DEP21<PoS1> <Lemma1> <Dep> <PoS2> and DEP22<PoS1> <Lemma1> <Dep> <PoS2> <Lemma2>.These are intended to capture information on thesubcategorization behavior of lexical elements andon typical collocations.1Humanness information is imported from GermaNet.
"Eine zentrale Rolle kommt der Autoversicherung zu, die ein F?nftel vereinnahmt.
"'zu#kommen<[21:Rolle], [243:Versicherung]>'PRED'Rolle'PRED'zentral<[21:Rolle]>'PRED [21:Rolle]SUBJ107ADJUNCT'vereinnahmen<[434:pro], [528:f?nftel]>'PRED'pro'PRED434SUBJ'f?nftel'PRED'eine'PREDDETSPEC528OBJ [434:pro]PRON-REL [434:pro]TOPIC-REL633ADJ-REL'eine'PREDDETSPEC21SUBJ'Versicherung'PRED'Auto'PRED-12MOD'die'PREDDETSPEC243OBJ-TH[21:Rolle]TOPIC191(a) evaluated as relatively improbable due to negative weight ofDISTANCE-TO-ANTECEDENT %X"Eine zentrale Rolle kommt der Autoversicherung zu, die ein F?nftel vereinnahmt.
"'zu#kommen<[21:Rolle], [243:Versicherung]>'PRED'Rolle'PRED'zentral<[21:Rolle]>'PRED [21:Rolle]SUBJ107ADJUNCT'eine'PREDDETSPEC21SUBJ'Versicherung'PRED'Auto'PRED-12MOD'vereinnahmen<[434:pro], [528:f?nftel]>'PRED'pro'PRED434SUBJ'f?nftel'PRED'eine'PREDDETSPEC528OBJ [434:pro]PRON-REL [434:pro]TOPIC-REL633ADJ-REL'die'PREDDETSPEC243OBJ-TH[21:Rolle]TOPIC191(b) evaluated as more probableFigure 1: Competing f-structures for (4)In the case of (5), the property DEP21 commonAnwalt APP proper, which counts the num-ber of occurrences of the common noun Anwalt(?lawyer?)
that govern a proper name via the depen-dency APP (close apposition), contributes to the cor-rect selection among the analyses illustrated in Fig-ure 2 by capturing the fact that Anwalt is a prototyp-ical head of a close apposition.2(5) [.
.
.
],[.
.
.
]daswhichdentheAnwaltlawyerKlausKlausBolligBolligzumto thevorla?ufigeninterimVerwalteradministratorbestellte.appointed.?[.
.
. ]
which appointed lawyer Klaus Bollig asinterim administrator.?
(s37596)2Since we have a list of title nouns available, we might alsointroduce a more general property that would count the numberof occurrences of title nouns in general that govern a propername via the dependency APP.
Note, however, that the nounsthat be heads of APPs comprise not only title nouns, but alsonouns like Abteilung ?department?, Buch ?book?, etc.20"das den Anwalt Klaus Bollig zum vorl?ufigen Verwalter bestellte"'bestellen<[1:pro], [82:Anwalt], [228:Bollig]>'PRED'pro'PRED1SUBJ'Anwalt'PRED'die'PREDDETSPEC82OBJ'Bollig'PRED'Klaus'PRED188NAME-MOD228OBJ-TH'zu<[246:Verwalter]>'PRED'Verwalter'PRED'vorl?ufig<[246:Verwalter]>'PRED [246:Verwalter]SUBJ334ADJUNCT'die'PREDDETSPEC246OBJ246ADJUNCT[1:pro]PRON-REL [1:pro]TOPIC-REL429(a) evaluated as less probable"das den Anwalt Klaus Bollig zum vorl?ufigen Verwalter bestellte"'bestellen<[1:pro], [82:Anwalt]>'PRED'pro'PRED1SUBJ'Anwalt'PRED'Bollig'PRED'Klaus'PRED188NAME-MOD228APP'die'PREDDETSPEC82OBJ'zu<[246:Verwalter]>'PRED'Verwalter'PRED'vorl?ufig<[246:Verwalter]>'PRED [246:Verwalter]SUBJ334ADJUNCT'die'PREDDETSPEC246OBJ246ADJUNCT[1:pro]PRON-REL [1:pro]TOPIC-REL429(b) evaluated as relatively probable due to highly positive weightof DEP21 common Anwalt APP properFigure 2: Competing f-structures for (5)4 Experiments4.1 DataAll the data we use are from the TIGER Corpus(Brants et al, 2002), a treebank of German news-paper texts comprising about 50,000 sentences.
The1,868 dependency annotations of the TiGer Depen-dency Bank, which have been semi-automaticallyderived from the corresponding treebank graphs, areused for evaluation purposes; we split these into aheld-out set of 371 sentences (and corresponding de-pendency annotations) and a test set of 1,497 sen-tences.
For training, we use packed, i.e.
ambiguous,c/f-structure representations where a proper subsetof the f-structures can be determined as compatiblewith the TIGER graph annotations.
Currently, theseare 8,881 pairs of labelled and unlabelled packedc/f-structure reprentations.From these 8,881 pairs of c/f-structure reprenta-tions, we extract two sets of property forests, onecontaining only the initially used properties, whichare based on the hardwired templates, and one con-taining all properties, i.e.
both the initially used andthe newly introduced ones.4.2 TrainingFor training, we use the cometc software by Ste-fan Riezler, which is part of XLE.
Prior to train-ing, however, we apply a frequency-based cutoff cto the data that ensures that a property is discrimi-native between the intended reading(s) and the un-intended reading(s) in at least c sentences; c is setto 4 on the basis of the evaluation results achievedon our held-out set and following a policy of a ?con-servative?
cutoff whose only purpose is to preventthat weights be learned for sparse properties.
(Fora longer discussion of frequency-based cutoffs, seeForst (2007).)
For the actual estimation of prop-erty weights, we then apply the combined method ofincremental property selection and l1 regularizationproposed in Riezler and Vasserman (2004), adjust-ing the hyperparameters on our held-out set for eachof the two sets of properties.
In order to compara-tively evaluate the importance of property selectionand regularization, we also train models based oneach of the two sets of properties without applyingany kind of these techniques.4.3 EvaluationThe overall results in terms of F-score and error re-duction, defined as F?
=Factual?FlowerFupper?Flower, that thefour resulting systems achieve on our test set of1,497 TiGer DB structures are shown in Table 1.
Inorder to give the reader an idea of the size of the dif-ferent models, we also indicate the number of prop-erties that they are based on.
All of the F-scoreswere calculated by means of the evaluation softwareby Crouch et al (2002).We observe that the models obtained using prop-erty selection and regularization, in addition to be-ing much more compact than their unregularizedcounterparts, perform significantly better than these.More importantly though, we can see that the mostimportant improvement, namely from an error re-duction of 32.5% to one of 42.0% or from 34.8%to 51.0% respectively, is achieved by adding moreinformative properties to the model.Table 2 then shows results broken down accordingto individual dependencies that are achieved with,on the one hand, the best-performing model basedon both the XLE template-based and the newly in-21# prop.
F-sc.
err.
red.XLE template-based properties,unregularized MLE 14,263 82.07 32.5%XLE templ.-based pr.
that survivea freq.-b.
cutoff of 4, n-bestgrafting with l1 regularization 3,400 82.19 34.8%all properties,unregularized MLE 57,934 82.55 42.0%all properties that survive afreq.-b.
cutoff of 4, n-bestgrafting with l1 regularization 4,340 83.01 51.0%Table 1: Overall F-score and corresponding error re-duction achieved by the four different systems on the1,497 TiGer DB structures of our test settroduced properties and, on the other hand, the best-performing model based on XLE template-basedproperties only.
Furthermore, we indicate the re-spective upper and lower bound F-scores, deter-mined by the best possible parse selection and byan arbitrary selection respectively.We observe that the overall F-score is signifi-cantly better with a selection based on the model thatincludes the newly introduced properties than with aselection based on the model that relies on the XLEtemplate-based properties only; overall error reduc-tion increases from 34.5% to 51.0%.
What is partic-ularly interesting is the considerably better error re-duction for the core grammatical functions sb (sub-ject) and oa (accusative object).
But also for rcs(relative clauses) and mos (modifiers or adjuncts),which are notoriously difficult for disambiguationdue to PP and ADVP attachment ambiguities, weobserve an improvement in F-score.Our error reduction of 51.0% also compares fa-vorably to the 36% error reduction on English LFGparses reported in Riezler et al (2002).
However,it is considerably lower than the error reduction of78% reported for the Dutch Alpino parser (Maloufand van Noord, 2004), but this may be due to thefact that our lower bound is calculated on the basisof analyses that have already passed a prefilter andis thus relatively high.5 ConclusionsOur results show that property design is of crucialimportance in the development of a disambiguationmodule for a ?deep?
parser.
They also indicate that itis a good idea to carry out property design in a lin-guistically inspired fashion, i.e.
by referring to thetheoretical literature that deals with soft constraintsthat are active in the language for which the systemis developed.
Property design thus requires a pro-found knowledge of the language under considera-tion (and the theoretical literature that deals with itssyntax), and since the disambiguation module oper-ates on the output of the symbolic grammar, a goodknowledge of the grammar is necessary as well.Weighting against each other the contributions ofdifferent measures taken for improving log-linearmodels for parse selection, we can conclude thatproperty design is at least as important as prop-erty selection and/or regularization, since even acompletely unregularized model based on all prop-erties performs significantly better than the best-adjusted model among the ones that are based onthe template-based properties only.
Moreover, prop-erty design can be carried out in a targeted way,i.e.
properties can be designed in order to improvethe disambiguation of grammatical relations that, sofar, are disambiguated particularly poorly or thatare of special interest for the task that the system?soutput is used for.
By demonstrating that prop-erty design is the key to good log-linear models for?deep?
syntactic disambiguation, our work confirmsthat ?specifying the features of a SUBG [stochasticunification-based grammar] is as much an empiricalmatter as specifying the grammar itself?
(Johnson etal., 1999).AcknowledgementsThe work described in this paper has been carriedout in the DLFG project, which was funded by theGerman Research Foundation (DFG).Furthermore, I thank the audiences at several Par-Gram meetings, at the Research Workshop of theIsrael Science Foundation on Large-scale GrammarDevelopment and Grammar Engineering at the Uni-versity of Haifa and at the SFB 732 Opening Col-loquium in Stuttgart for their important feedback onearlier versions of this work.ReferencesJudith Aissen.
2003.
Differential Object Marking:Iconicity vs. Economy.
Natural Language and Lin-guistic Theory, 21:435?483.22upper stoch.
select.
stoch.
select.
lowerbound all properties templ.-based pr.
boundgramm.
relation/morphosynt.
feature F-sc.
F-sc.
err.
red.
F-sc.
err.
red.
F-sc.all 85.50 83.01 51.0 82.17 34.5 80.42PREDs only 79.36 75.74 46.5 74.69 31.0 72.59app (close apposition) 63 60 63 61 75 55app cl (appositive clause) 53 53 100 52 86 46cc (comparative complement) 28 19 -29 19 -29 21cj (conjunct of coord.)
70 68 50 67 25 66da (dative object) 67 63 67 62 58 55det (determiner) 92 91 50 91 50 90gl (genitive in spec.
pos.)
89 88 75 88 75 85gr (genitive attribute) 88 84 56 84 56 79mo (modifier) 70 63 36 62 27 59mod (non-head in compound) 94 89 29 89 29 87name mod (non-head in compl.
name) 82 80 33 81 67 79number (number as determiner) 83 81 33 81 33 80oa (accusative object) 78 75 77 69 31 65obj (arg.
of prep.
or conj.)
90 88 50 87 25 86oc fin (finite cl.
obj.)
67 64 0 64 0 64oc inf (infinite cl.
obj.)
83 82 0 82 0 82op (prepositional obj.)
57 54 40 54 40 52op dir (directional argument) 30 23 13 23 13 22op loc (local argument) 59 49 29 49 29 45pd (predicative argument) 62 60 50 59 25 58pred restr 92 87 62 84 38 79quant (quantifying determiner) 70 68 33 68 33 67rc (relative clause) 74 62 20 59 0 59sb (subject) 76 73 63 71 38 68sbp (logical subj.
in pass.
constr.)
68 63 62 61 46 55case 87 85 75 83 50 79comp form (complementizer form) 74 72 0 74 100 72coord form (coordinating conj.)
86 86 100 86 100 85degree 89 88 50 87 0 87det type (determiner type) 95 95 ?
95 ?
95fut (future) 86 86 ?
86 ?
86gend (gender) 92 90 60 89 40 87mood 90 90 ?
90 ?
90num (number) 91 89 50 89 50 87pass asp (passive aspect) 80 80 100 79 0 79perf (perfect) 86 85 0 86 100 85pers (person) 85 84 83 82 50 79pron form (pronoun form) 73 73 ?
73 ?
73pron type (pronoun type) 71 70 0 71 100 70tense 92 91 0 91 0 91Table 2: F-scores (in %) in the 1,497 TiGer DB examples of our test set23Sabine Brants, Stefanie Dipper, Silvia Hansen, WolfgangLezius, and George Smith.
2002.
The TIGER tree-bank.
In Proceedings of the Workshop on Treebanksand Linguistic Theories, Sozopol, Bulgaria.Stephen Clark and James R. Curran.
2004.
Parsing theWSJ using CCJ and Log-Linear Models.
In Proceed-ings of the 42nd Annual Meeting of the Associationfor Computational Linguistics (ACL ?04), Barcelona,Spain.Richard Crouch, Ronald M. Kaplan, Tracy H. King, andStefan Riezler.
2002.
A comparison of evaluationmetrics for a broad-coverage parser.
In Proceedingsof the LREC Workshop ?Beyond PARSEVAL?Towardsimproved evaluation mesures for parsing systems?,pages 67?74, Las Palmas, Spain.Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,John Maxwell, and Paula Newman.
2006.
XLE docu-mentation.
Technical report, Palo Alto Research Cen-ter, Palo Alto, CA.Stefanie Dipper.
2003.
Implementing and DocumentingLarge-scale Grammars ?
German LFG.
Ph.D. thesis,IMS, University of Stuttgart.
Arbeitspapiere des Insti-tuts fu?r Maschinelle Sprachverarbeitung (AIMS), Vol-ume 9, Number 1.Martin Forst, Nu?ria Bertomeu, Berthold Crysmann, Fred-erik Fouvry, Silvia Hansen-Schirra, and Valia Kordoni.2004.
Towards a dependency-based gold standardfor German parsers ?
The TiGer Dependency Bank.In Proceedings of the COLING Workshop on Lin-guistically Interpreted Corpora (LINC ?04), Geneva,Switzerland.Martin Forst, Jonas Kuhn, and Christian Rohrer.
2005.Corpus-based learning of OT constraint rankings forlarge-scale LFG grammars.
In Proceedings of the 10thInternational LFG Conference (LFG?05), Bergen,Norway.
CSLI Publications.Martin Forst.
2007.
Disambiguation for a LinguisticallyPrecise German Parser.
Ph.D. thesis, University ofStuttgart.Anette Frank, Tracy Holloway King, Jonas Kuhn, andJohn T. Maxwell.
2001.
Optimality Theory StyleConstraint Ranking in Large-Scale LFGGrammars.
InPeter Sells, editor, Formal and Empirical Issues in Op-timality Theoretic Syntax, pages 367?397.
CSLI Pub-lications, Stanford, CA.Gerhard Helbig and Joachim Buscha.
2001.Deutsche Grammatik ?
Ein Handbuch fu?r denAusla?nderunterricht.
Langenscheidt, Berlin andMunich, Germany.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,and Stefan Riezler.
1999.
Estimators for stochastic?unification-based?
grammars.
In Proceedings of the37th Annual Meeting of the Association for Computa-tional Linguistics 1999, College Park, MD.Ju?rgen Lenerz.
1977.
Zur Abfolge nominaler Satzgliederim Deutschen.
Number 5 in Studien zur deutschenGrammatik.
Narr, Tu?bingen, Germany.Robert Malouf and Gertjan van Noord.
2004.
Wide Cov-erage Parsing with Stochastic Attribute Value Gram-mars.
In Proceedings of the IJCNLP-04 Workshop?Beyond Shallow Analyses - Formalisms and statisti-cal modeling for deep analyses?.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximum en-tropy estimation for feature forests.
In Proceedingsof the Human Language Technology Conference, SanDiego, CA.Stefan Riezler and Alexander Vasserman.
2004.
Gradi-ent feature testing and l1 regularization for maximumentropy parsing.
In Proceedings of the 2004 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP?04), Barcelona, Spain.Stefan Riezler, Tracy Holloway King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell, and Mark John-son.
2002.
Parsing the Wall Street Journal using aLexical-Functional Grammar and Discriminative Esti-mation Techniques.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics 2002, Philadelphia, PA.Christian Rohrer and Martin Forst.
2006.
Improv-ing coverage and parsing quality of a large-scaleLFG for German.
In Proceedings of the LanguageResources and Evaluation Conference (LREC-2006),Genoa, Italy.Kristina Toutanova, Christopher D. Manning, Stuart M.Shieber, Dan Flickinger, and Stephan Oepen.
2002.Parse disambiguation for a rich HPSG grammar.
InFirst Workshop on Treebanks and Linguistic Theories(TLT2002), pages 253?263.Hans Uszkoreit, Thorsten Brants, Brigitte Krenn, LarsKonieczny, Stephan Oepen, and Wojciech Skut.
1998.Relative Clause Extraposition in German ?
Evidencefrom Corpus Studies and Acceptability Ratings.
InProceedings of AMLaP-98, Freiburg, Germany.Hans Uszkoreit.
1987.
Word Order and ConstituentStructure in German.
CSLI Publications, Stanford,CA.Gertjan van Noord.
2006.
At Last Parsing Is NowOperational.
In Piet Mertens, Cedrick Fairon, AnneDister, and Patrick Watrin, editors, TALN06.
VerbumEx Machina.
Actes de la 13e confe?rence sur le traite-ment automatique des langues naturelles, pages 20?42, Leuven, Belgium.24
