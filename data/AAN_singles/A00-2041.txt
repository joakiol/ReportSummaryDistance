A Framework for Robust Semantic InterpretationCaro lyn  P.  Ros@Learn ing  Research  and Deve lopment  CenterUn ivers i ty  of  P i t t sburghP i t t sburgh ,  PA 15260rosecp @pitt.
eduAbst ractThis paper describes AUTOSEM, a robust semanticinterpretation framework that can operate both atparse time and repair time.
The evaluation demon-strates that AUTOSEM achieves a high level of ro-bustness efficiently and without requiring any handcoded knowledge dedicated to repair.1 In t roduct ionIn order for an approach to robust interpretationto be practical it must be efficient, address the ma-jor types of disfluencies that plague spontaneouslyproduced language input, and be domain indepen-dent so thatachieving robustness in a particular do-main does not require an additional knowledge n-gineering effort.
This paper describes AUTOSEM,a semantic interpretation framework that possessesthese three qualities.
While previous approachesto robust interpretation have offered robust parserspaired with separate repair modules~ with separateknowledge sources for each, AUTOSEM is a singleunified framework that can operate both at parsetime and repair time.
AUTOSEM is integrated withthe LCFLEx robust parser (Ros@ and Lavie, to ap-pear; Lavie and Ros@, 2000).
Together AUTOSEMand LCFLEx constitute the robust understandingengine within the CARMEL natural language un-derstanding component developed in the context ofthe Atlas intelligent utoring project (Freedman atal., to appear).
The evaluation reported here demon-strates that AUTOSEM's repair approach operates200 times faster than the most similar competing ap-proach while producing hypotheses of better quality.AUTOSEM provides an interface to allow seman-tic interpretation to operate in parallel with syntac-tic interpretation at parse time in a lexicon drivenfashion.
Domain specific semantic knowledge is en-coded declaratively within a meaning representationspecification.
Semantic constructor functions arecompiled automatically from this specification andthen linked into lexical entries as in the Glue Lan-guage Semantics approach to interpretation (Dal-rymple, 1999).
Based on syntactic head/argumentrelationships assigned at parse time, the construc-tot functions enforce semantic selectiona\] restric-tions and assemble meaning representation struc-tures by composing the meaning representation asso-ciated with the constructor function with the mean-ing representation f each of its arguments.AUTOSEM first attempts to construct analy-ses that satisfy both syntactic and semantic well-formedness conditions.
The LCFLEx parser has theability to efficiently relax syntactic constraints asneeded and as allowed by its parameterized flexi-bility settings.
For sentences remaining beyond theparser's coverage, AUTOSEM's repair algorithm re-lies entirely on semantic knowledge to compose thepartial analyses produced by the parser.
Each se-mantic representation built by AUTOSEM's inter-pretation framework contains a pointer to the con-structor function that built it.
Thus, each partialanalysis can be treated as a constructor functionwith built in knowledge about how the associatedpartial analysis can be combined with other par-tial analyses in a semantically meaningful way.
Ge-netic programming search (Koza, 1992; Koza, 1994)is used to efficiently compose the fragments pro-duced by the parser.
The function definitions com-piled from the meaning representation specificationallow the genetic search to use semantic onstraintsto make effective use of its search space.
Thus, AU-TOSEM operates efficiently, free of any hand codedrepair rules or any knowledge specifically dedicatedto repair unlike other approaches to recovery fromparser failure (Danieli and Gerbino, 1995; Van No-ord, 1997; Kasper et al, 1999).2 The  Mean ing  Representat ionSpec i f i ca t ionAt the heart of AUTOSEM is its interpretationframework composed of semantic onstructor func-tions compiled from a meaning representation spec-ification.
These semantic onstructor functions canbe used at parse time to build up semantic represen-tations.
These same constructor functions can thenbe used in a repair stage to compose the fragmentsreturned by the parser in the cases where the parseris not able to obtain a complete analysis for an ex-311(:type <*state>:isa (<>):instances nil:vars (entity time duration polarity):spec ((who <*entity> entity)(when <*when> time)(how-long <*time-length> duration)(negation \[+/-\] polarity)))(:type <*personal-state>:isa (<*state>):instances nil:vars ():spec ((who <*who> entity)))(:type <busy>:isa (<*personal-state>):instances nil:vars (activity):spec ((frame *busy)(event <*event> activity)))( : type  \[+/-\]: i sa  (<>):instances (+ -):vars nil:spec nil)Figure 1: Sample  mean ing  representat ionspec i f icat ion ent r iestragrammatical input sentence.The meaning representation specification pro-vides a venue for expressing domain specific se-mantic information declaratively.
AUTOSEM pro-duces frame-based meaning representation struc-tures.
Thus, each domain specific meaning repre-sentation specification must define a set of semantictypes that together specify the set of frames andatomic feature values that make up the domain spe-cific frame-based language, which slots are associ-ated with each frame, and what range of frames andatomic feature values may fill each of those slots.AUTOSEM provides a simple formalism for defin-ing meaning representations.
Each entry corre-sponds to a semantic type and contains five fields:: type,  : i sa ,  : ins tances ,  :vars ,  and :spec.
Somesample entries for the appointment scheduling do-main are displayed in Figure 1.
Some details areomitted for simplicity.
The : type field simply con-talus the name of the type.
The : vars  field containsa list of variables, each corresponding to a semanticrole.
The :spec field associates a frame and set ofslots with a type.
For each slot, the : spec field con-rains the name of the slot, the most general type re-striction on the slot, and a specification of where theslot filler comes from.
This third piece of informationcan be either a variable name, indicating that what-ever is bound to that variable is what should fill thatslot, or a function call to another semantic onstruc-tor function, allowing types to specify constraints atmore than one level of embedding.
Similar to the: spec field, the : ins tances  field associates a list ofatomic values with a type.
Inheritance relations aredefined via the : i sa  field.
Types inherit the valuesof each subsuming type's : ins tances ,  :vars ,  and: spec fields.3 Semant ic  In terpretat ion  a t  ParseT ime(:type <cancel>:isa (<*event>): instances nil:vats (agent activity time polarity):spec ((frame *cancel)(engagement <*event> activity)))Figure 2: Mean ing  representat ion  def in i t ion  of<cance l>(:morph cancel:syntax ((cat vlex) (root cancel)(vform bare) (irreg-past +)(irreg-pastpart +)(irreg-prespart +)(subcat (*or* intrans np))(semtag cancel1)):semantics (cancel1 <cancel>((subject agent)(object activity)(tempadjunct time)(negation polarity))))Figure 3: Lex ica l  ent ry  for  the  verb  "cance l "As an extension to LCFLEx's LFG-like pseudo-unification grammar formalism, AUTOSEM pro-vides the inser t - ro le  function as an interface toallow semantic interpretation to operate in parallelwith syntactic interpretation at parse time.
Whenthe insert-role function is used to insert a childconstituent into the slot corresponding to its syntac-tic functional role in a parent constituent, the childconstituent's semantic representation is passed in tothe parent constituent's semantic onstructor func-tion as in the Glue Language Semantics approachto interpretation (Dalrymple, 1999).
AUTOSEM'slexicon formalism allows semantic onstructor func-tions to be linked into lexical entries by means ofthe semtag feature.
Each semtag feature value cor-responds to a semantic constructor function and312mappings between syntactic functional roles suchas sub jec t ,  d i rec t  ob ject ,  and ind i rec t  objectand semantic roles such as agent,  ac t iv i ty ,  ortime.
See Figures 2 and 3 discussed further be-low.
Note that the syntactic features that appearin this example are taken from the COMLEX lex-icon (Grishman et al, 1994).
In order to provideconsistent input to the semantic onstructor func-tious, AUTOSEM assumes a syntactic approach inwhich deep syntactic functional roles are assigned asin CARMEL's syntactic parsing grammar evaluatedin (Freedman et al, to appear).
In this way, for ex-ample, the roles assigned within an active sentenceand its corresponding passive sentence remain thesame.Since the same constructor function is called withdifferent arguments a number of times in order toconstruct an analysis incrementally, an argument isincluded in every constructor function that allowsa "result so far" to be passed in and augmented.Its default value, which is used the first time theconstructor function is executed, is the representa-tion associated with the corresponding type in theabsence of any arguments being instantiated.
Eachtime the constructor function is executed, each of itsarguments that are instantiated are first checked tobe certain that the structures they are instantiatedwith match all of the type restrictions on all of theslots that are bound to that argument.
If they are,the instantiated arguments' tructures are insertedinto the corresponding slots in the "result so far".Otherwise the constructor function fails.Take as an example the sentence "The meeting Ihad scheduled was canceled by you."
as it is pro-cessed by AUTOSEM using the CARMEL grammarand lexicon, which is built on top of the COMLEXlexicon (Grishman et al, 1994).
The grammar as-signs deep syntactic functional roles to constituents.Thus, "you" is the deep subject of "cancel", and"the meeting" is the direct object both of "cancel"and of "schedule".
The detailed subcategorizationclasses associated with verbs, nouns, and adjectivesin COMLEX make it possible to determine whatthese relationships hould be.
The meaning repre-sentation entry for <cancel> as well as the lexicalentry for the verb "cancel" are found in Figures 2and 3 respectively.
Some details are left out for sim-plicity.
When "the meeting I had scheduled" is ana-lyzed as the surface subject of "was canceled", it isassigned the deep syntactic role of object since "wascanceled" is passive.
The verb "cancel" has cance l las its semtag value in the lexicon, cancel  1 is definedthere as being associated with the type <cancel>,and the ob jec t  syntactic role is associated with theac t iv i ty  argument.
Thus, the <cancel> functionis called with its ac t iv i ty  argument instantiatedwith the meaning of "the meeting I had scheduled".Next, when "by you" is attached, "you" is assignedthe deep syntactic role of subject of "cancel".
Thesub jec t  role is associated with the agent argumentin the definition of cance l l .
Thus, <cancel> iscalled a again, this time with "you" instantiatingthe agent argument and the result from the lastcall to <cancel> passed in through the "result sofar" argument.4 Semantic Interpretation for RepairWhile the LCFLEX parser has been demonstratedto robustly parse a variety of disfluencies found inspontaneously generated language (Ros~ and Lavie,to appear), sentences till remain that are beyondits coverage.
Previous research involving the ear-lier GLR* parser (Lavie, 1995) and an earlier repairmodule (Rosd, 1997) has demonstrated that divid-ing the task of robust interpretation i to two stages,namely parsing and repair, provides a better tradeoff between run time and coverage than attempt-ing to place the entire burden of robustness on theparser alone (Ros$, 1997).
Thus, when the flexibilityallowed at parse time is not sufficient o construct ananalysis of an entire sentence for any reason, a frag-mentary analysis is passed into the repair module.For each pair of vertices in the chart the best singleclause level and noun phrase level analysis accord-ing to LCFLEx's statistical disambiguation scores isincluded in the set of fragmentary analyses passedon to the repair stage.
An example 1 is displayed inFigure 4.
Here the sentence "Why don't we make itfrom like eleven to one?"
failed to parse.
In this case,the problem is that the insertion of "like" causes thesentence to be ungrammatical.
When the parser'sflexibility settings are such that it is constrained tobuild analyses only for contiguous portions of text,such an insertion would prevent he parser from con-structing an analysis covering the entire sentence.Nevertheless, it is able to construct analyses for anumber of grammatical subsets of it.Genetic programming search (Koza, 1992; Koza,1994) is used to search for different ways to combinethe fragments.
Genetic programming is an oppor-tunistic search algorithm used for constructing com-puter programs to solve particular problems.
Amongits desirable properties is its ability to search a largespace efficiently by first sampling widely and shal-lowly, and then narrowing in on the regions sur-rounding tile most promising looking points.ZThls example was generated with the grammar used inthe evaluation.
See Section 6.
The AUTOSEM repair algo-rithm can he used with grammars that do not make use ofAUTOSEM's parse-time interface by using a simple conver-sion program that automatically builds a function for eachpartial analysis corresponding to its semantic type.313Sentence: Why don't we make it from like eleven to one?Functions:(<SCHEDULE> NIL T NIL NIL NIL NIL NIL NIL TNIL NIL:STR ((FRAME *SCHEDULE) (ATTITUDE *LET-S)(WHAT ((FRAME *IT)))):COV (6 5 4 3 2 1):SCORE 1.4012985e-45)(<SCHEDULE> NIL T NIL NIL NIL T T NIL NILNIL NIL:STR ((FRAME *SCHEDULE)(WHAT ((FRAME *IT))) (NEGATIVE +)(WHO ((FRAME *WE)))):COV (6 5 4 3 2) :SCORE 3.05483e-43)(<SCHEDULE> NIL NIL NIL NIL NIL NIL NILNIL T NIL NIL:STR ((FRAME *SCHEDULE) (ATTITUDE *LET-S)):COV (5 4 3 2 1):SCORE 1.4012985e-45)(<INTERVAL> NIL T T T:STR ((END ((FRAME *SIMPLE-TIME)(HOUR 1)))(START ((FRAME *SIMPLE-TIME)(HOUR 11)))(INCL-EXCL INCLUSIVE)(FRAME *INTERVAL)):COV (11 10 9):SCORE 1.2102125e-38)(<SCHEDULE> NIL NIL NIL NIL NIL T T NIL NILNIL NIL:STR ((FRAME *SCHEDULE) (NEGATIVE +)(WHO ((FRAME *WE)))):COV (5 4 3 2):SCORE 2.2584231e-37)(<SCHEDULE> NIL T NIL NIL NIL T NIL NIL NILNIL NIL:STR ((FRAME *SCHEDULE) (WHAT ((FRAME *IT)))(WHO ((FRAME *WE)))):COV (6 5 4):SCORE 1.861576e-27)(<PRO> NIL NIL NIL:STR ((FRAME *PRO)):COV (II):SCORE 2.2223547e-18)(<SIMPLE-TIME> NIL NIL NIL NIL NIL NIL T NIL:STR ((FRAME *SIMPLE-TIME) (HOUR 1)):C0V (11):SCORE 2.2223547e-18)(<SIMPLE-TIME> NIL NIL NIL NIL NIL NILT NIL:STR ((FRAME *SIMPLE-TIME) (HOUR II)):COV (9):SCORE 1.0090891e-22)(<IT>:STR ((FRAME *IT)):COV (6):SCORE 2.593466e-13)Ideal Program:(<SCHEDULE> NIL NIL NIL NIL NIL NIL NIL(<INTERVAL> NIL NIL NIL NIL:STR ((END ((FRAME *SIMPLE-TIME) (HOUR I)))(START((FRAME *SIMPLE-TIME) (HOUR Ii)))(INCL-EXCL INCLUSIVE)(FRAME *INTERVAL)):COV (II I0 9):SCORE 1.2102125e-38)NIL NIL NIL:STR ((FRAME *SCHEDULE) (ATTITUDE *LET-S)(WHAT ((FRAME *IT)))):COV (6 5 4 3 2 I):SCORE 1.4012985e-45)Interpretation: Let's schedule it for from eleven o'clock till one o'clockFigure 4: Repair example314.It first takes a list of functions and terminal sym-bols and randomly generates a population of pro-grams.
It then evaluates each program for its "fit-hess" according to some predetermined set of crite-ria.
The most fit programs are then paired up andused to produce the next generation by means of acrossover operation whereby a pair of subprograms,one from each parent program, are swapped.
Thenew generation is evaluated for its fitness, and theprocess continues for a preset number of generations.As mentioned earlier, because ach semantic rep-resentation built by AUTOSEM contains a pointerto the constructor function that built it, each partialanalysis can itself be treated as a constructor func-tion.
Thus, the function set made available to thegenetic programming search for each sentence need-ing repair is derived from the set of partial anal-yses extracted from the parser's chart.
A numberof the functions produced for the example are dis-played in Figure 4.
Some functions have been omit-ted for brevity.
The functions are displayed as func-tion calls, with the name of the function followedby its arguments.
The name of each function corre-sponds to the semantic type from the meaning rep-resentation that corresponds to the associated par-tial analysis.
Following this is a list of place holderscorresponding to each argument position associatedwith the semantic type, as described in Section 2.Each place holder is either n i l  if it is an open placeholder, or 1: if the position has already been filledin the corresponding partial analysis.
The STR fieldcontains the corresponding partial analysis.
This isthe "result so far" parameter discussed in Section3.
The C0V field lists the positions in the sentencecovered by the partial analysis.
Note that in theexample sentence, the word "don't" covers both po-sitions 2 and 3 since the parser expands the con-traction before parsing.
The SCORE field containsthe statistical score assigned by the parser's tatis-tical disambiguation procedure described in (Ros~and Lavie, to appear).The repair process begins as the genetic program-ming algorithm composes the function definitionsinto programs that assemble the fragments producedby the parser.
The genetic programming algorithmhas access to a list of type restrictions that are placedon each argument position by the meaning repre-sentation specification.
Thus, the algorithm ensuresthat the programs that are generated o not vio-late any of the meaning representation's type restric-tions.Once a population of programs is generated ran-domly, each program is evaluated for its fitness.
Asimple function implements a preference for pro-grams that cover more of the sentence with fewersteps while using the analyses the parser assignedthe best statistical scores to.
A score between 0 and1 is first assigned to each program corresponding tothe percentage of the input sentence it covers.
Asecond score between 0 and 1 estimates how com-plicated the program is by dividing the number offunction calls by the length of the sentence and sub-tracting this number from 1.
A third score is as-signed the average of the statistical scores assignedby the parser to the fragments used in the program.Using coefficients based on an intuitive assignment ofrelative importance to the three scores, the final fit-ness value of each program is 1 - \[(.55 *coverageS) +(.25 ?
complexityS) + (.2 ?
statisticalS)\].A typed version of the original crossover algorithmdescribed in (Koza, 1992; Koza, 1994) was used toensure that new programs would not violate anytype restrictions or include more than one partialanalysis covering the same span of text.
This wasaccomplished by first making for each subprogram alist of the subprograms from the alternate program itcould be inserted into without violating any seman-tic constraints.
From these two lists it is possible togenerate a list of all quadruples that specify a sub-program from each parent program to be removedand which subprogram from the alternate parentprogram they could be inserted into.
From this list,all quadruples were removed that would either causea span of text to be covered more than once in aresulting program or would require a subprogramto be inserted into a subprogram that would havebeen removed.
From the remaining list, a quadruplewas selected randomly.
The corresponding crossoveroperation was then executed and the resulting twonew programs were returned.
While this typed vet-sion of crossover is more complex than the originalcrossover operation, it can be executed very rapidlyin practice because the programs are relatively smalland the semantic type restrictions ensure than theinitial lists generated are correspondingly small.5 Re lated  WorkRecent approaches to robust parsing focus on shal-low or partial parsing techniques (Van Noord, 1997;Worm, 1998; Ait-Mokhtar and Chanod, 1997; Ab-ney, 1996).
Rather than attempting to construct aparse covering an entire ungrammatical sentence asin (Lehman, 1989; Hipp, 1992), these approaches at-tempt to construct analyses for maximal contiguousportions of the input.
The weakness of these partialparsing approaches i that part of the original mean-ing of the utterance may be discarded with the por-tion(s) of the utterance that are skipped in order tofind a parsable subset.
Information communicatedby the relationships between these fragments withinthe original text is lost if these fragments are notcombined.
Thus, these less powerful algorithms es-sentially trade effectiveness for efficiency.
Their goalis to introduce enough flexibility to gain an accept-315able level of coverage at an acceptable computationalexpense.Some partial parsing approaches have been cou-pled with a post-parsing repair stage (Danieli andGerbino, 1995; Ros6 and Waibel, 1994; Ros6, 1997;Van Noord, 1997; Kasper et al, 1999) The goalbehind these two stage approaches is to increasethe coverage over partial parsing alone at a rea-sonable computational cost.
Until the introductionof AUTOSEM, the ROSE approach, introduced in(Ros6, 1997), was unique in that it achieved thisgoal without either requiring hand coded knowledgespecifically dedicated to repair or excessive amountsof interaction with the user.
However, although(Ros6, 1997) demonstrates that the two stage ROSEapproach is significantly faster than attempting toachieve the same quality of results in a single stageparsing approach, our evaluation demonstrates thatit remains computationally intractable, requiring onaverage 67 seconds to repair a single parse on a 330MHz Gateway 2000.
In contrast, we demonstratethat AUTOSEM is on average 200 times faster, tak-ing only .33 seconds on average to repair a singleparse while achieving results of superior quality.6 Eva luat ionAn experiment was conducted to evaluate AU-TOSEM's robustness by comparing the effectivenessand efficiency of AUTOSEM's repair approach withthat of the alternative ROSE approach.
The testset used for this evaluation contains 750 sentencesextracted from a corpus of spontaneous schedulingdialogues collected in English.
For both repair ap-proaches we used the meaning representation devel-oped for the appointment scheduling domain thatwas used in previous evaluations of the ROSE ap-proach (Ros6, 1997).
It consists of 260 semantictypes, each expressing domain specific concepts forthe appointment scheduling domain such as busy,cancel ,  and out-of-~;own.
The ROSE meaning rep-resentation specification was easily converted to theformat used in AUTOSEM.
Because a pre-existingsemantic grammar was available that parsed directlyonto this meaning representation, that grammar wasused in the parsing stage to construct analyses.
Thefinal meaning representation structures for the first300 sentences were then passed to a generation com-ponent, and the resulting texts were graded by ahuman judge not involved in developing the researchreported here.
Each result was graded as either Bad,Okay, or Perfect, where Perfect indicates that the re-sult was fluent and communicated the idea from theoriginal sentence.
A grade of Okay indicates thatthe result communicated the correct information,but not fluently.
Those graded Bad either communi-cated incorrect information or were missing part orall of the information communicated in the originalsentence.Each sentence was parsed in two different modes.In LC w/ res tar ts  mode, the parser was allowed toconstruct analyses for contiguous portions of inputstarting at any point in the sentence.
In LCFLExmode, the parser was allowed to start an analysis atany point and skip up to three words within the anal-ysis.
Because the AUTOSEM repair algorithm runssignificantly faster than the ROSE repair algorithm,repair was attempted after every parse rather thanonly when a parse quality heuristic indicated a needas in the ROSE approach (Ros6, 1997).
We com-pared the results of both AUTOSEM and ROSE inconjunction with the LC w/ res tar ts  parsing mode.The results are displayed in Figures 5 and 7.
Be-cause the ROSE approach only runs the full repairalgorithm when its parse quality heuristic indicatesa need and the parser returns more than one par-tial analysis, it only attempted repair for 14% of thesentences in the corpus.
Nevertheless, although theAUTOSEM repair algorithm ran for each sentence,Figure 5 demonstrates that processing time for pars-ing plus repair in the AUTOSEM condition was dra-matically faster on average than with ROSE.
Aver-age processing time for the ROSE algorithm was 200times slower than that for AUTOSEM on sentenceswhere both repair algorithms were used.
In additionto the advantage in terms of speed, the AUTOSEMrepair approach achieved an acceptable grade (Okayor Perfect) on approximately 4% more sentences.Parsing in LC w/ res tar ts  mode plus repair wasalso compared with parsing in LCFLEx mode withskipping up to three words.
Again, LC w/ res tar ts+ AUTOSEM repair achieved a slightly highernumber of acceptable grades, although LCFLExachieved a slightly higher number of Perfect grades.On long sentences (between 15 and 20 words),LCFLEx mode required almost three times as muchtime as LC w/ res tar ts  mode plus AUTOSEM re-pair.
This evaluation confirms our previous resultsthat two stage approaches offer a better processingtime versus robustness trade-off.The primary difference between ROSE and AU-TOSEM is that ROSE uses a single repair function,MY-C0bIB, to combine any two fragments by referringto the meaning representation specification.
Whileit is possible to obtain the same set of repair hy-potheses with ROSE as with AUTOSEM, the ROSEapproach insulates the genetic search from the se-mantic restrictions imposed by the meaning repre-sentation.
These restrictions are visible only locallywithin individual applications of the I~Y-COMB func-tion.
Thus, FIY-COMB must be able to cope with thecase where the arguments passed in cannot be com-bined.
Large portions of the programs generated byROSE as repair hypotheses do not end up contribut-ing to the resulting structure.
The programs gener-3164540353O.~ 25p-2015105i i ,ROSE ~ "CARMEL - - -x - - -)e-.
.
.
.
.
.
.
.
"~ .
.
.
.
.
.
.
.
.
"~ .
.
.
.
.
.
.
.
.
k,- .
.
.
.
.
.
.
.
.
x .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.5 10 15 20Sentence LengthFigure 5: Processing Times for Alternative Strategies1412108Eo.
60;0' ' LCFLEX + repairLCFLEXLC w restarts + repair --.-m---LC w restarts .---.~ ....../ / / "  ..- : .~2 2.
L-.:.
:2 2 2.:.:.:2.
:2  2 2 2 2:.2-" 2;:25 '10 15 20Sentence LengthFigure 6: Processing Times for Alternative Strategiesated by ROSE must therefore be much larger than inAUTOSEM in order to obtain the same results.
Fur-thermore, the fitness of each repair hypothesis canonly be computed by executing the program to ob-tain a result.
The combination of all of these thingsmakes the process of fitness evaluation in ROSE farmore costly than in AUTOSEM.
In contrast, AU-TOSEM's constructor function definitions make itpossible for the genetic search to make use of seman-tic restrictions to speed up the process of convergingon a high quality repair hypothesis.
The tremendousspeed-up offered by the AUTOSEM approach makesit practical to apply repair more often and to use alarger generation size (50 individuals as opposed to32) and a larger number of generations (5 as opposedto 4) for the genetic search.7 Cur rent  D i rec t ionsIn this paper we described AUTOSEM, a new robustsemantic interpretation framework.
Our evaluationdemonstrates that AUTOSEM achieves a greaterlevel of robustness 200 times more efficiently thanthe most similar competing approach.In AUTOSEM, the mapping between syntactic317Bad Okay PerfectLC w/restarts 92 (30.7%) 61 (20.3%) i47 (49.0%)LCFL~x 72 (24.0%) 67 (22.3%) 161 (53.7%)LC w/restarts + ROSE 75 (25.0%) 68 (22.7%) 157 (52.3%)LC w/restarts + AUTOSEM 64 (21.3%) 84 (28.0%) 152 (50.7%)LCFLEx + AUTOSEM 64 (21.3%) 78 (26.0%) i58 (52.7%)Total Acceptable209 (69.3%)228 (76.0%)225 (75.0%)236 (78.7%)236 (78.7%)Figure 7: Interpretat ion quality with and without  repairfunctional roles and semantic arguments i com-pletely determined in the current version.
In somecases, such as with copular constructions and withadjunct prepositional phrases, it would be useful tointroduce some non-determinism o that, for exam-ple, semantic selectional restrictions between the ob-ject.
of the preposition and the semantic structurethat the prepositional phrase is attaching to canmore easily play a role in selecting the appropri-ate semantic relationship.
Exploring approaches forachieving this non-determinism efficiently is one ofour current objectives.8 AcknowledgementsSpecial thanks are due to the JANUS multi-lingual speech-to-speech translation project for mak-ing their interlingua specification and semantic pars-ing and generation grammars available for the eval-uation reported here.
This research was supportedby NSF Grant IRI-94-57637 and Grant 9720359 toCIRCLE, a center for research on intelligent tutor-ing.ReferencesS.
Abney.
1996.
Partial parsing via finite-statecascades.
In Proceedings of the Eighth EuropeanSummer School In Logic, Language and Informa-tion, Prague, Czech Republic.S.
Ait-Mokhtar and J. Chanod.
1997.
Incrementalfinite-state parsing.
In Proceedings of the FifthConference on Applied Natural Language Process-ing.M.
Dalrymple.
1999.
Semantics and Syntax in Lex-ical Functional Grammar.
The MIT Press.M.
Danieli and E. Gerbino.
1995.
Metrics for evalu-ating dialogue strategies in a spoken language sys-tem.
In Working Notes of the AAAI Spring Sym-posium on Empirical Methods in Discourse Inter-pretation and Generation.R.
Freedman, C. P. Ros6, M. A. Ringenberg, andK.
VanLehn.
to appear.
Its tools for naturallanguage dialogue: A domain-independent parserand planner.
In Proceedings of the Intelligent Tu-toring Systems Conference.R.
Grishman, C. Macleod, and A. Meyers.
1994.COMLEX syntax: Building a computational lexi-con.
In Proceedings of the 15th International Con-ference on Computational Linguistics (COLING-94).D.
R. Hipp.
1992.
Design and Development ofSpoken Natural-Language Dialog Parsing Systems.Ph.D.
thesis, Dept.
of Computer Science, DukeUniversity.W.
Kasper, B. Kiefer, H. Krieger, C. Rupp, andK.
Worm.
1999.
Charting the depths of robustspeech parsing.
In Proceedings of the 37th An-nual Meeting of the Association for Computa-tional Linguistics.J.
Koza.
1992.
Genetic Programming: On the Pro-gramming of Computers by Means of Natural Se-lection.
MIT Press.J.
Koza.
1994.
Genetic Programming H. MIT Press.A.
Lavie and C. P. Ros~.
2000.
Optimal ambi-guity packing in unification-augmented context-free grammars.
In Proceedings of the InternationalWorkshop on Parsing Technologies.A.
Lavie.
1995.
A Grammar Based Robust ParserFor Spontaneous Speech.
Ph.D. thesis, School ofComputer Science, Carnegie Mellon University.J.
F. Lehman.
1989.
Adaptive Parsing: Self-Extending Natural Language Interfaces.
Ph.D.thesis, School of Computer Science, Carnegie Mel-lon University.C.
P. Ros~ and A. Lavie.
to appear.
BMancing ro-bustness and efficiency in unification augmentedcontext-free parsers for large practical applica-tions.
In J. C. Junqua and G. Van Noord, editors,Robustness in Language and Speech Technologies.Kluwer Academic Press.C.
P. Ros~ and A. Waibel.
1994.
Recovering fromparser failures: A hybrid statistical/symbolic ap-proach.
In Proceedings of The Balancing Act:Combining Symbolic and Statistical Approaches toLanguage workshop at the 32nd Annual Meeting ofthe A CL.C.
P. Ros& 1997.
Robust Interactive Dialogue Inter-pretation.
Ph.D. thesis, School of Computer Sci-ence, Carnegie Mellon University.G.
Van Noord.
1997.
An efficient implementation fthe head-corner parser.
Computational Linguis-tics, 23(3).K.
Worm.
1998.
A model of robust processing ofspontaneous speech by integrating viable frag-ments.
In Proceedings of COLING-A CL 98.318
