Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 249?256, Vancouver, October 2005. c?2005 Association for Computational LinguisticsBayesian Learning in Text SummarizationTadashi NomotoNational Institute of Japanese Literature1-16-10 Yutaka ShinagawaTokyo 142-8585 Japannomoto@acm.orgAbstractThe paper presents a Bayesian model fortext summarization, which explicitly en-codes and exploits information on how hu-man judgments are distributed over thetext.
Comparison is made against nonBayesian summarizers, using test datafrom Japanese news texts.
It is found thatthe Bayesian approach generally lever-ages performance of a summarizer, attimes giving it a significant lead over non-Bayesian models.1 IntroductionConsider figure 1.
What is shown there is the pro-portion of the times that sentences at particular lo-cations are judged as relevant to summarization, orworthy of inclusion in a summary.
Each panel showsjudgment results on 25 Japanese texts of a particulargenre; columns (G1K3), editorials (G2K3) and newsstories (G3K3).
All the documents are from a sin-gle Japanese news paper, and judgments are elicitedfrom some 100 undergraduate students.
While morewill be given on the details of the data later (Sec-tion 3.2), we can safely ignore them here.Each panel has the horizontal axis representing lo-cation or order of sentence in a document, and thevertical axis the proportion of the times sentences atparticular locations are picked as relevant to summa-rization.
Thus in G1K3, we see that the first sentence(to appear in a document) gets voted for about 12%of the time, while the 26th sentence is voted for lessthan 2% of the time.Curiously enough, each of the panels exhibits adistinct pattern in the way votes are spread acrossa document: G1K3 has the distribution of votes(DOV) with sharp peaks around 1 and 14; in G2K3,the distribution is peaked around 1, with a smallbump around 19; in G3K3, the distribution is sharplyskewed to the left, indicating that the majority ofvotes went to the initial section of a document.
Whatis interesting about the DOV is that we could takeit as indicating a collective preference for what toextract for a summary.
A question is then, can wesomehow exploit the DOV in summarization?
Toour knowledge, no prior work seems to exist thataddresses the question.
The paper discusses howwe could do this under a Bayesian modeling frame-work, where we explicitly represent and make useof the DOV by way of Dirichlet posterior (Congdon,2003).12 Bayesian Model of SummariesSince the business of extractive summarization, suchas one we are concerned with here, is about rankingsentences according to how useful/important theyare as part of summary, we will consider here a par-ticular ranking scheme based on the probability of asentence being part of summary under a given DOV,i.e.,P (y|v), (1)where y denotes a given sentence, and v =(v1, .
.
.
, vn) stands for a DOV, an array of observedvote counts for sentences in the text; v1 refers to thecount of votes for a sentence at the text initial posi-tion, v2 to that for a sentence occurring at the secondplace, etc.Thus given a four sentence long text, if we havethree people in favor of a lead sentence, two in favor1See Yu et al (2004) and Cowans (2004) for its use in IR.2491 4 7 10 14 18 22 26g1k3sentencesvoteswon(%)0.000.050.100.150.200.250.301 4 7 10 14 18 22 26g2k3sentencesvoteswon(%)0.000.050.100.150.200.250.301 4 7 10 14 18 22g3k3sentencesvoteswon(%)0.000.050.100.150.200.250.30Figure 1: Genre-by-genre vote distributionof the second, one for the third, and none for thefourth, then we would have v = (3, 2, 1, 0).Now suppose that each sentence yi (i.e., a sen-tence at the i-th place in the order of appearance) isassociated with what we might call a prior prefer-ence factor ?i, representing how much a sentence ata particular position is favored as part of a summaryin general.
Then the probability that yi finds itself ina summary is given as:?
(yi|?i)P (?i), (2)where ?
denotes some likelihood function, andP (?i) a prior probability of ?i.Since the DOV is something we could actuallyobserve about ?i, we might as well couple ?i withv by making a probability of ?i conditioned on v .Formally, this would be written as:?
(yi|?i)P (?i|v).
(3)The problem, however, is that we know nothingabout what each ?i looks like, except that it shouldsomehow be informed by v .
A typical Bayesian so-lution to this is to ?erase?
?i by marginalizing (sum-ming) over it, which brings us to this:P (yi|v) =??
(yi|?i)P (?i |v) d?i.
(4)Note that equation 4 no longer talks about the proba-bility of yi under a particular ?i; rather it talks aboutthe expected probability for yi with respect to a pref-erence factor dictated by v .
All we need to knowv //???
// yiFigure 2: A graphical viewabout P (?i|v) to compute the expectation is v and aprobability distribution P , and not ?i?s, anymore.We know something about v , and this wouldleave us P .
So what is it?
In principle it couldbe any probability distribution.
However largelyfor the sake of technical convenience, we assumeit is one component of a multinomial distributionknown as the Dirichlet distribution.
In particular,we talk about Dirichlet(?|v), namely a Dirichletposterior of ?, given observations v , where ?
=(?1, .
.
.
, ?i, .
.
.
, ?n), and?ni ?i = 1 (?i > 0).
(Re-markably, if P (?)
is a Dirichlet, so is P (?|v).)
?here represents a vector of preference factors for nsentences ?
which constitute the text.2Accordingly, equation 4 could be rewritten as:P (yi|v) =??(yi|?
)P (?
|v) d?.
(5)An interesting way to look at the model is by wayof a graphical model (GM), which gives some in-tuitive idea of what the model looks like.
In a GMperspective, our model is represented as a simple tri-partite structure (figure 2), in which each node corre-sponds to a variable (parameter), and arcs represent2Since texts generally vary in length, we may set n to a suf-ficiently large number so that none of texts of interest may ex-ceed it in length.
For texts shorter than n, we simply add emptysentences to make them as long as n.250dependencies among them.
x ?
y reads ?y dependson x.?
An arc linkage between v and yi is meant torepresent marginalization over ?.Moreover, we will make use of a scale parame-ter ?
?
1 to have some control over the shapeof the distribution, so we will be working withDirichlet(?|?v) rather than Dirichlet(?|v).
Intu-itively, we might take ?
as representing a degree ofconfidence we have in a set of empirical observa-tions we call v , as increasing the value of ?
has theeffect of reducing variance over each ?i in ?.The expectation and variance of Dirichlet(?|v) aregiven as follows.3E[?i] = viv0 (6)V ar[?i] = vi(v0 ?
vi)v20(v0 + 1), (7)where v0 =?ni vi.
Therefore the variance of ascaled Dirichlet is:V ar[?i|?v] = vi(v0 ?
vi)v20(?v0 + 1).
(8)See how ?
is stuck in the denominator.
Another ob-vious fact about the scaling is that it does not affectthe expectation, which remains the same.To get a feel for the significance of ?, con-sider figure 3; the left panel shows a histogramof 50,000 variates of p1 randomly drawn fromDirichlet(p1, p2|?c1, ?c2), with ?
= 1, and both c1and c2 set to 1.
The graph shows only the p1 partbut things are no different for p2.
(The x-dimensionrepresents a particular value p1 takes (which rangesbetween 0 and 1) and the y-dimension records thenumber of the times p1 takes that value.)
We see thatpoints are spread rather evenly over the probabilityspace.
Now the right panel shows what happens ifyou increase ?
by a factor of 1,000 (which will giveyou P (p1, p2|1000, 1000)); points take a bell shapedform, concentrating in a small region around the ex-pectation of p1.
In the experiments section, we willreturn to the issue of ?
and discuss how it affectsperformance of summarization.Let us turn to the question of how to find a solu-tion to the integral in equation 5.
We will be con-cerned here with two standard approaches to the is-sue: one is based on MAP (maximum a posteriori)3http://www.cis.hut.fi/ahonkela/dippa/dippa.htmland another on numerical integration.
We start offwith a MAP based approach known as Bayesian In-formation Criterion or BIC.For a given model m, BIC seeks an analytical ap-proximation for equation 4, which looks like the fol-lowing:lnP (yi|m) = ln?(yi|??,m)?
k2 lnN, (9)where k denotes the number of free parameters inm, and N that of observations.
??
is a MAP estimateof ?
under m, which is E[?].
It is interesting to notethat BIC makes no reference to prior.
Also worthy ofnote is that a minus of BIC equals MDL (MinimumDescription Length).Alternatively, one might take a more straightfor-ward (and fully Bayesian) approach known as theMonte Carlo integration method (MacKay, 1998)(MC, hereafter) where the integral is approximatedby:P (yi|v) ?
1nn?j=1?
(yi|x(j)), (10)where we draw each sample x(j) randomly from thedistribution P (?|v), and n is the number of x(i)?sso collected.
Note that MC gives an expectation ofP (yi|v) with respect to P (?|v).Furthermore, ?
could be any probabilistic func-tion.
Indeed any discriminative classifier (such asC4.5) will do as long as it generates some kind ofprobability.
Given ?, what remains to do is essen-tially training it on samples bootstrapped (i.e., re-sampled) from the training data based on ?
?
whichwe draw from Dirichlet(?|v).4 To be more spe-cific, suppose that we have a four sentence long textand an array of probabilities ?
= (0.4, 0.3, 0.2, 0.1)drawn from a Dirichlet distribution: which is to say,we have a preference factor of 0.4 for the lead sen-tence, 0.3 for the second sentence, etc.
Then we re-sample with replacement lead sentences from train-ing data with the probability of 0.4, the second withthe probability of 0.3, and so forth.
Obviously, a4It is fairly straightforward to sample from a Dirichlet pos-terior by resorting to a gamma distribution, which is what ishappening here.
In case one is working with a distribution it ishard to sample from, one would usually rely on Markov chainMonte Carlo (MCMC) or variational methods to do the job.251L=1p1Frequency0.0 0.2 0.4 0.6 0.8 1.00100300500L=1000p1Frequency0.46 0.48 0.50 0.52 0.54050010001500Figure 3: Histograms of random draws from Dirichlet(p1, p2|?c1, ?c2) with ?
= 1 (left panel), and ?
=1000 (right panel).high preference factor causes the associated sen-tence to be chosen more often than those with a lowpreference.Thus given a text T = (a, b, c, d) with ?
=(0.4, 0.3, 0.2, 0.1), we could end up with a data setdominated by a few sentence types, such as T ?
=(a, a, a, b), which we proceed to train a classifier onin place of T .
Intuitively, this amounts to induc-ing the classifier to attend to or focus on a partic-ular region or area of a text, and dismiss the rest.Note an interesting parallel to boosting (Freund andSchapire, 1996) and the alternating decision tree(Freund and Mason, 1999).In MC, for each ?
(k) drawn from Dirichlet(?|v),we resample sentences from the training data usingprobabilities specified by ?
(k), use them for train-ing a classifier, and run it on a test document d tofind, for each sentence in d, its probability of beinga ?pick?
(summary-worthy) sentence,i.e., P (yi|?
(k)),which we average across ??s.
In experiments laterdescribed, we apply the procedure for 20,000 runs(meaning we run a classifier on each of 20,000 ?
?swe draw), and average over them to find an estimatefor P (yi|v).As for BIC, we generally operate along the linesof MC, except that we bootstrap sentences usingonly E[?
], and the model complexity term, namely,?k2 lnN is dropped as it has no effect on rankingsentences.
As with MC, we train a classifier on thebootstrapped samples and run it on a test document.Though we work with a set of fixed parameters, abootstrapping based on them still fluctuates, produc-ing a slightly different set of samples each time werun the operation.
To get a reasonable convergencein experiments, we took the procedure to 5,000 iter-ations and averaged over the results.Either with BIC or with MC, building a summa-rizer on it is a fairly straightforward matter.
Givena document d and a compression rate r, what asummarizer would do is simply rank sentences in dbased on P (yi|v) and pick an r portion of highestranking sentences.3 Working with Bayesian Summarist3.1 C4.5In what follows, we will look at whether and how theBayesian approach, when applied for the C4.5 deci-sion tree learner (Quinlan, 1993), leverages its per-formance on real world data.
This means our modelnow operates either byP (yi|v) ?
1nn?j=1?c4.5(yi|x(j)), (11)or bylnP (yi|m) = ln?c4.5(yi|??,m)?
k2 lnN, (12)with the likelihood function ?
filled out by C4.5.Moreover, we compare two versions of the classifier;one with BIC/MC and one without.
We used Wekaimplementations of the algorithm (with default set-tings) in experiments described below (Witten andFrank, 2000).252While C4.5 here is configured to work in a bi-nary (positive/negative) classification scheme, werun it in a ?distributional?
mode, and use a particularclass membership probability it produces, namely,the probability of a sentence being positive, i.e., apick (summary-worthy) sentence, instead of a cate-gory label.Attributes for C4.5 are broadly intended to repre-sent some aspects of a sentence in a document, anobject of interest here.
Thus for each sentence ?, itsencoding involves reference to the following set ofattributes or features.
?LocSen?
gives a normalizedlocation of ?
in the text, i.e., a normalized distancefrom the top of the text; likewise, ?LocPar?
gives anormalized location of the paragraph in which ?
oc-curs, and ?LocWithinPar?
records its normalized lo-cation within a paragraph.
Also included are a fewlength-related features such as the length of text andsentence.
Furthermore we brought in some languagespecific feature which we call ?EndCue.?
It recordsthe morphology of a linguistic element that ends ?,such as inflection, part of speech, etc.In addition, we make use of the weight feature(?Weight?)
for a record on the importance of ?
basedon tf.idf.
Let ?
= w1, .
.
.
, wn, for some word wi.Then the weight W (?)
is given as:W (?)
=?w(1 + log(tf(w))) ?
log(N/df(w)).Here ?tf(w)?
denotes the frequency of word w in agiven document, ?df(w)?
denotes the ?document fre-quency?
of w, or the number of documents whichcontain an occurrence of w. N represents the totalnumber of documents.5Also among the features used here is ?Pos,?
a fea-ture intended to record the position or textual orderof ?, given by how many sentences away it occursfrom the top of text, starting with 0.While we do believe that the attributes discussedabove have a lot to do with the likelihood that a givensentence becomes part of summary, we choose notto consider them parameters of the Bayesian model,just to keep it from getting unduly complex.
Recallthe graphical model in figure 2.5Although one could reasonably argue for normalizingW (?)
by sentence length, it is not entirely clear at the momentwhether it helps in the way of improving performance.3.2 Test DataHere is how we created test data.
We collected threepools of texts from different genres, columns, edito-rials and news stories, from a Japanese financial pa-per (Nihon Keizai Shinbun) published in 1995, eachwith 25 articles.
Then we asked 112 Japanese stu-dents to go over each article and identify 10% worthof sentences they find most important in creatinga summary for that article.
For each sentence, werecorded how many of the subjects are in favor ofits inclusion in summary.
On average, we had aboutseven people working on each text.
In the follow-ing, we say sentences are ?positive?
if there are threeor more people who like to see them in a summary,and ?negative?
otherwise.
For convenience, let uscall the corpus of columns G1K3, that of editorialsG2K3 and that of news stories G3K3.
Additionaldetails are found in table 1.4 Results and DiscussionTables 2 through 4 show how the Bayesian sum-marist performs on G1K3, G2K3, and G3K3.
Thetables list results in precision at compression rates(r) of interest (0 < r < 1).
The figures thereof indi-cate performance averaged over leave-one-out crossvalidation folds.
What this means is that you leaveout one text for testing and use the rest for training,which you repeat for each one of the texts in the data.Since we have 25 texts for each data set, this leadsto a 25-fold cross validation.
Precision is defined bythe ratio of hits (positive sentences) to the numberof sentences retrieved, i.e., r-percent of sentences inthe text.6In each table, figures to the left of the verti-cal line indicate performance of summarizers withBIC/MC and those to the right that of summarizerswithout them.
Parenthetical figures like ?(5K)?
and?(20K)?
indicate the number of iterations we tookthem to: thus BIC(5K) refers to a summarizer basedon C4.5/BIC with scores averaged over 5,000 runs.BSE denotes a reference summarizer based on a reg-ular C4.5, which it involves no resampling of train-ing data.
LEAD refers to a summarizer which works6We do not use recall for a evaluation measure, as the num-ber of positive instances varies from text to text, and may indeedexceed the length of a summary under a particular compressionrate.253Table 1: N represents the number of sentences in G1K3 to G3K3.
Sentences with three or more votes intheir favor are marked positive, that is, for each sentence marked positive, at least three people are in favorof including it in a summary.Genre N Positive (?
3) Negative P/N RatioG1K3 426 67 359 0.187G2K3 558 93 465 0.200G3K3 440 76 364 0.210Table 2: G1K3.
?
= 5.
Dashes indicate no meaningful results.r BIC (5K) MC (20K) BSE LEAD0.05 0.4583 0.4583 ?
0.33330.10 0.4167 0.4167 ?
0.34720.15 0.3333 0.3472 ?
0.26040.20 0.2757 0.2861 ?
0.23060.25 0.2525 0.2772 ?
0.22330.30 0.2368 0.2535 ?
0.2066Table 3: G2K3.
?
= 5.r BIC (5K) MC (20K) BSE LEAD0.05 0.6000 0.5800 0.4200 0.54000.10 0.4200 0.4200 0.3533 0.39330.15 0.3427 0.3560 0.2980 0.31470.20 0.3033 0.3213 0.2780 0.27670.25 0.2993 0.2776 0.2421 0.23970.30 0.2743 0.2750 0.2170 0.2054Table 4: G3K3.
?
= 5.r BIC (5K) MC (20K) BSE LEAD0.05 0.9600 0.9600 0.8400 0.96000.10 0.7600 0.7600 0.6800 0.70000.15 0.6133 0.6000 0.5867 0.51330.20 0.5233 0.5233 0.4967 0.45330.25 0.4367 0.4367 0.3960 0.38400.30 0.4033 0.4033 0.3640 0.36730 (411.0/65.0)Figure 4: A non Bayesian C4.5 trained on G1K3.254LenSenA0 (199.0/23.0)<= 64EndCueA> 64Weight= 0LocWithinPar= 10 (0.0)= 2Weight= 30 (5.0/1.0)= 4LocWithinPar= 50 (7.0)= 6LocWithinPar<= 2.3380 (17.0)> 2.338Weight<= 0LenSenA> 0LocPar<= 2.2551 (7.0)> 2.2551 (4.0)<= 00 (22.0/7.0)> 0LocWithinPar<= 114LocWithinPar> 1140 (38.0/4.0)<= 0.81 (2.0)> 0.81 (10.0/2.0)<= 0.70 (3.0)> 0.70 (11.0/1.0)<= 0.286LocPar> 0.286LenSenA<= 0.6671 (8.0)> 0.6671 (13.0/1.0)<= 720 (8.0)> 720 (15.0)<= 1.707LocSen> 1.707LocWithinPar<= 0.9170 (7.0)> 0.9170 (5.0/1.0)<= 0LenSenA> 01 (17.0)<= 110LocSen> 1100 (3.0)<= 0.3331 (2.0)> 0.3331 (3.0/1.0)<= 0.4290 (6.0)> 0.429Figure 5: A Bayesian (MC) C4.5 trained on G1K3.by selecting sentences from the top of the text.
It isgenerally considered a hard-to-beat approach in thesummarization literature.Table 4 shows results for G3K3 (a news story do-main).
There we find a significantly improvement toperformance of C4.5, whether it operates with BICor MC.
The effect is clearly visible across a wholerange of compression rates, and more so at smallerrates.Table 3 demonstrates that the Bayesian approachis also effective for G2K3 (an editorial domain), out-performing both BSE and LEAD by a large margin.Similarly, we find that our approach comfortablybeats LEAD in G1K3 (a column domain).
Note thedashes for BSE.
What we mean by these, is that weobtained no meaningful results for it, because wewere unable to rank sentences based on predictionsby BSE.
To get an idea of how this happens, let uslook at a decision tree BSE builds for G1K3, whichis shown in figure 4.
What we have there is a deci-sion tree consisting of a single leaf.7 Thus for what-ever sentence we feed to the tree, it throws back thesame membership probability, which is 65/411.
Butthen this would make a BSE based summarizer ut-terly useless, as it reduces to generating a summaryby picking at random, a particular portion of text.87This is not at all surprising as over 80% of sentences in anon resampled text are negative for the most of the time.8Its expected performance (averaged over 106 runs) comesNow Figure 5 shows what happens with theBayesian model (MC), for the same data.
Therewe see a tree of a considerable complexity, with 24leaves and 18 split nodes.Let us now turn to the issues with ?.
As we mightrecall, ?
influences the shape of a Dirichlet distri-bution: a large value of ?
causes the distributionto have less variance and therefore to have a moreacute peak around the expectation.
What this meansis that increasing the value of ?
makes it more likelyto have us drawing samples closer to the expecta-tion.
As a consequence, we would have the MCmodel acting more like the BIC model, which isbased on MAP estimates.
That this is indeed thecase is demonstrated by table 5, which gives resultsfor the MC model on G1K3 to G3K3 at ?
= 1.
Wesee that the MC behaves less like the BIC at ?
= 1than at ?
= 5 (table 2 through 4).Of a particular interest in table 5 is G1K3, wherethe MC suffers a considerable degradation in per-formance, compared to when it works with ?
= 5.G2K3 and G3K3, again, witness some degradationin performance, though not as extensive as in G1K3.It is interesting that at times the MC even works bet-ter with ?
= 1 than ?
= 5 in G2K3 and G3K3.9to: 0.1466 (r = 0.05), 0.1453 (r = 0.1), 0.1508 (r = 0.15),0.1530 (r = 0.2), 0.1534 (r = 0.25), and 0.1544 (r = 0.3).9The results suggest that if one like to have some improve-ment, it is probably a good idea to set ?
to a large value.
But255Table 5: MC (20K).
?
= 1.r G1K3 G2K3 G3K30.05 0.3333 0.5400 0.96000.10 0.3333 0.3867 0.78000.15 0.2917 0.3960 0.58670.20 0.2549 0.3373 0.52000.25 0.2480 0.2910 0.43470.30 0.2594 0.2652 0.4100All in all, the Bayesian model proves more effec-tive in leveraging performance of the summarizer ona DOV exhibiting a complex, multiply peaked formas in G1K3 and G2K3, and less on a DOV whichhas a simple, single-peak structure as in G3K3 (cf.figure 1).105 Concluding RemarksThe paper showed how it is possible to incorporateinformation on human judgments for text summa-rization in a principled manner through Bayesianmodeling, and also demonstrated how the approachleverages performance of a summarizer, using datacollected from human subjects.The present study is motivated by the view thatthat summarization is a particular form of collabo-rative filtering (CF), wherein we view a summaryas a particular set of sentences favored by a par-ticular user or a group of users just like any otherthings people would normally have preference for,such as CDs, books, paintings, emails, news articles,etc.
Importantly, under CF, we would not be asking,what is the ?correct?
or gold standard summary fordocument X?
?
the question that consumed much ofthe past research on summarization.
Rather, what weare asking is, what summary is popularly favored forX?Indeed the fact that there could be as many sum-maries as angles to look at the text from may favorin general how to best set ?
requires some experimenting withdata and the optimal value may vary from domain to domain.An interesting approach would be to empirically optimize ?
us-ing methods suggested in MacKay and Peto (1994).10Incidentally, summarizers, Bayesian or not, perform con-siderably better on G3K3 than on G1K3 or G2K3.
This hap-pens presumably because a large portion of votes concentratein a rather small region of text there, a property any classifiershould pick up easily.the CF view of summary: the idea of what consti-tutes a good summary may vary from person to per-son, and may well be influenced by particular inter-ests and concerns of people we elicit data from.Among some recent work with similar concerns,one notable is the Pyramid scheme (Nenkova andPassonneau, 2004) where one does not declare aparticular human summary a absolute reference tocompare summaries against, but rather makes everyone of multiple human summaries at hand bear onevaluation; Rouge (Lin and Hovy, 2003) representsanother such effort.
The Bayesian summarist rep-resents yet another, whereby one seeks a summarymost typical of those created by humans.ReferencesPeter Congdon.
2003.
Bayesian Statistical Modelling.John Wiley and Sons.Philip J. Cowans.
2004.
Information Retrieval UsingHierarchical Dirichlet Processes.
In Proc.
27th ACMSIGIR.Yoav Freund and Llew Mason.
1999.
The alternatingdecision tree learning algorithm,.
In Proc.
16th ICML.Yoav Freund and Robert E. Schapire.
1996.
Experimentswith a new boosting algorithm.
In Proc.
13th ICML.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic eval-uation of summaries using n-gram co-occurance statis-tics.
In Proc.
HLT-NAACL 2003.David J. C. MacKay and Linda C. Bauman Peto.
1994.
AHierarchical Dirichlet Language Model.
Natural Lan-guage Engineering.D.
J. C. MacKay.
1998.
Introduction to Monte Carlomethods.
In M. I. Jordan, editor, Learning in Graphi-cal Models, Kluwer Academic Press.Ani Nenkova and Rebecca Passonneau.
2004.
Evalua-tion Content Selection in Summarization: The Pyra-mid Method.
In Proc.
HLT-NAACL 2004.J.
Ross Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.Ian H. Witten and Eibe Frank.
2000.
Data Mining: Prac-tical Machine Learning Tools and Techniques withJava Implementations.
Morgan Kaufmann.Kai Yu, Volker Tresp, and Shipeng Yu.
2004.
A Non-parametric Hierarchical Bayesian Framework for In-formation Filtering.
In Proc.
27th ACM SIGIR.256
