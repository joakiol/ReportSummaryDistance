Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 189?194,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsTo Cache or not to Cache?Experiments with Adaptive Models in Statistical Machine TranslationJo?rg TiedemannDepartment of Linguistics and PhilologyUppsala University, Uppsala/Swedenjorg.tiedemann@lingfil.uu.seAbstractWe report results of our submissions tothe WMT 2010 shared translation task inwhich we applied a system that includesadaptive language and translation mod-els.
Adaptation is implemented using ex-ponentially decaying caches storing pre-vious translations as the history for newpredictions.
Evidence from the cache isthen mixed with the global backgroundmodel.
The main problem in this setup iserror propagation and our submissions es-sentially failed to improve over the com-petitive baseline.
There are slight im-provements in lexical choice but the globalperformance decreases in terms of BLEUscores.1 MotivationThe main motivation of our submission was totest the use of adaptive language and translationmodels in a standard phrase-based SMT settingfor the adaptation to wider context beyond sen-tence boundaries.
Adaptive language models havea long tradition in the speech recognition commu-nity and various approaches have been proposedto reduce model perplexity in this way.
The gen-eral task is to adjust statistical models to essen-tial properties of natural language which are usu-ally not captured by standard n-gram models orother local dependency models.
First of all, it isknown that repetition is very common especiallyamong content words (see, for example, wordslike ?honey?, ?milk?, ?land?
and ?flowing?
in fig-ure 1).
In most cases a repeated occurrence of acontent word is much more likely than its first ap-pearance, which is not predicted in this way by astatic language model.
Secondly, the use of ex-pressions is related to the topic in the current dis-course and the chance of using the same topic-related expressions again in running text is higherthan a mixed-topic model would predict.In translation another phenomenon can be ob-served, namely the consistency of translations.Polysemous terms are usually not ambiguous intheir context and, hence, their translations becomeconsistent according to the contextual sense.
Eventhe choice between synonymous translations israther consistent in translated texts as we can seein the example of subtitle translations in figure 1(taken from the OPUS corpus (Tiedemann, 2009)).The 10 commandments Kerd ma luiTo some land flowing with milkand honey!Till ett land fullt av mjo?lk ochhonung.I?ve never tasted honey.Jag har aldrig smakat honung....But will sympathy lead us tothis land flowing with milk andhoney?Men kan sympati leda oss till dettamjo?lkens och honungens land?Mari honey ...Mari, gummanSweetheart,where are yougoing?A?lskling, varska du?...Who was that,honey?Vem var det,gumman?Figure 1: Repetition and translation consistencyAmbiguous terms like ?honey?
are consistentlytranslated into the Swedish counterpart ?honung?
(in the sense of the actual substance) or ?gumman?
(in the metaphoric sense).
Observe that this is trueeven in the latter case where synonymous transla-tions such as ?a?lskling?
would be possible as well.In other words, deciding to stick to consistent lexi-cal translations should be preferred in MT becausethe chance of alternative translations in repeatedcases is low.
Here again, common static transla-tion models do not capture this property at all.In the following we explain our attempt to inte-grate contextual dependencies using cache-basedadaptive models in a standard SMT setup.
Wehave already successfully applied this techniqueto a domain-adaptation task (Tiedemann, 2010).189Now we would like to investigate the robustnessof this model in a more general case where somein-domain training data is available and input datais less repetitive.2 Cache-based Adaptive ModelsThe basic idea behind cache-based models is tomix a large static background model with a smalllocal model that is dynamically estimated from re-cent items from the input stream.
Dynamic cachelanguage models have been introduced by (Kuhnand Mori, 1990) and are often implemented in theform of linear mixtures:P (wn|history) = (1 ?
?
)Pbackground(wn|history) +?Pcache(wn|history)The background model is usually a standard n-gram model taking limited amount of local contextfrom the history into account and the cache modelis often implemented as a simple (unsmoothed)unigram model using the elements stored in afixed-size cache (100-5000 words) to estimateits parameters.
Another improvement can beachieved by making the importance of cached el-ements a function of recency.
This can be doneby introducing a decaying factor in the estima-tion of cache probabilities (Clarkson and Robin-son, 1997):Pcache(wn|wn?k..wn?1) ?1Zn?1?i=n?kI(wn = wi)e??
(n?i)This is basically the model that we applied in ourexperiments as it showed the largest perplexity re-duction in our previous experiments on domainadaptation.Similarly, translation models can be adapted aswell.
This is especially useful to account for trans-lation consistency forcing the decoder to preferidentical translations for repeated terms.
In ourapproach we try to model recency again using adecay factor to compute translation model scoresfrom the cache in the following way (only forsource language phrases fn for which a transla-tion option exist in the cache; we use a score ofzero otherwise):?cache(en|fn) =?Ki=1 I(?en, fn?
= ?ei, fi?)
?
e?
?i?Ki=1 I(fn = fi)The importance of a cached translation option ex-ponentially decays and we normalize the sum ofcached occurrences by the number of translationoptions with the same foreign language item thatwe condition on.Plugging this in into a standard phrase-based SMTengine is rather straightforward.
The use of cache-based language models in SMT have been in-vestigated before (Raab, 2007).
In our case weused Moses as the base decoder (Koehn et al,2007).
The cache-based language model can beintegrated in the decoder by simply adjusting thecall to the language modeling toolkit appropri-ately.
We implemented the exponentially decayingcache model within the standard SRILM toolkit(Stolcke, 2002) and added command line argu-ments to Moses to switch to that model and to setcache parameters such as interpolation, cache sizeand decay.
Adding the translation model cache isa bit more tricky.
For this we added a new featurefunction to the global log-linear model and im-plemented the decaying cache as explained abovewithin the decoder.
Again, simple command-linearguments can be used to switch caching on or offand to adjust cache parameters.One important issue is to decide when and whatto cache.
As we explore a lot of different optionsin decoding it is not feasible to adapt the cachecontinuously.
This would mean a lot of cache op-erations trying to add and remove hypotheses fromthe cache memory.
Therefore, we opted for a con-text model that considers history only from previ-ous sentences.
Once decoding is finished transla-tion options from the best hypothesis found in de-coding are put into language and translation modelcache.
This is arguably a strong approximation ofthe adaptive approach.
However, considering ourspecial concern about wider context across sen-tence boundaries this seems to be a reasonablecompromise between completeness and efficiency.Another issue is related to the selection of itemsto be cached.
As discussed earlier repetition ismost likely to be found among content words.Similarly, translation consistency is less likely tobe true for function words.
In the best case onewould know the likelihood of specific terms tobe repeated.
This could be trained on some de-velopment data possibly in connection with wordclasses instead of fully lexicalized parameters inorder to overcome data sparseness and to improvegenerality.
Even though this idea is very tempt-190ing it would require a substantial extension of ourmodel and would introduce language and domain-specific parameters.
Therefore, we just added asimplistic approach filtering tokens by their lengthin characters instead.
Assuming that longer itemsare more likely to be content words we simply seta threshold to decide whether to add a term to thecache or not.
This threshold can be adjusted usingcommand-line arguments.Finally, we also need to be careful about noisein the cache.
This is essential as the caching ap-proach is prone to error propagation.
However,detecting noise is difficult.
If there would be a no-tion of noise in translation hypotheses, the decoderwould avoid it.
In related work (Nepveu et al,2004) have studied cache-based translation mod-els in connection with interactive machine trans-lation.
In that case, one can assume correct inputafter post-editing the translation suggestions.
Oneway to approach noise reduction in non-interactiveMT is to make use of transition costs in the transla-tion lattice.
Assuming that this cost (which is esti-mated internally within the decoder during the ex-pansion of translation hypotheses) refers to somekind of confidence we can discard translation op-tions above a certain threshold, which is what wedid in the implementation of our translation modelcache.3 ExperimentsWe followed the setup proposed in the sharedtranslation task.
Primarily we concentrated ourefforts on German-English (de-en) and English-German (en-de) using the constrained track, i.e.using the provided training and development datafrom Europarl and the News domain.
Later wealso added experiments for Spanish (es) and En-glish using a similar setup.Our baseline system incorporates the followingcomponents: We trained two separate 5-gram lan-guage models for each language with the standardsmoothing strategies (interpolation and Kneser-Ney discounting), one for Europarl and one for theNews data.
All of them were estimated using theSRILM toolkit except the English News LM forwhich we applied RandLM (Talbot and Osborne,2007) to cope with the large amount of trainingdata.
We also included two separate translationmodels, one for the combined Europarl and Newsdata and one for the News data only.
They wereestimated using the standard tools GIZA++ (Ochand Ney, 2003) and Moses (Koehn et al, 2007)applying default settings and lowercased trainingdata.
Lexicalized reordering was trained on thecombined data set.
All baseline models were thentuned on the News test data from 2008 using mini-mum error rate training (MERT) (Och, 2003).
Theresults in terms of lower-case BLEU scores arelisted in table 1.n-gram scoresBLEU 1 2 3 4de-en baseline 21.3 57.4 27.8 15.1 8.6de-en cache 21.5 58.1 28.1 15.2 8.7en-de baseline 15.6 52.5 21.7 10.6 5.5en-de cache 14.4 52.6 21.0 9.9 4.9es-en baseline 26.7 61.7 32.7 19.9 12.6es-en cache 26.1 62.6 32.7 19.8 12.5en-es baseline 26.9 61.5 33.3 20.5 12.9en-es cache 23.0 60.6 30.4 17.6 10.4Table 1: Results on the WMT10 test set.In the adaptation experiments we applied exactlythe same models using the feature weights fromthe baseline with the addition of the caching com-ponents in both, language models and translationmodels.
Cache parameters are not particularlytuned for the task in our initial experiments whichcould be one reason for the disappointing resultswe obtained.
Some of them can be integrated inthe MERT procedure, for example, the interpola-tion weight of the translation cache.
However, tun-ing these parameters with the standard proceduresappears to be difficult as we will see in later ex-periments presented in section 3.2.
Initially weused settings that appeared to be useful in previ-ous experiments.
In particular, we used a languagemodel cache of 10,000 words with a decay of?
= 0.0005 and an interpolation weight of 0.001.A cache was used in all language models exceptthe English News model for which caching wasnot available (because we did not implement thisfeature for RandLM).
The translation cache sizewas set to 5,000 with a decay factor of 0.001.
Theweight for the translation cache was set to 0.001.Furthermore, we filtered items for the translationcache using a length constraint of 4 characters ormore and a transition cost threshold (log score) of-4.The final results of the adaptive runs are shownin table 1.
In all but one case the cache-based re-sult is below the baseline which is, of course, quitedisappointing.
For German-English a small im-provement can be observed.
However, this maybe rather accidental.
In general, it seems that191the adaptive approach cannot cope with the noiseadded to the cache.3.1 DiscussionThere are two important observations that shouldbe mentioned here.
First of all, the adaptive ap-proach assumes coherent text input.
However, theWMT test-set is composed of many short newsheadlines with various topics involved.
We, there-fore, also ran the adaptive approach on individualnews segments.
The results are illustrated in figure2.Basically, the results do not change comparedto the previous run.
Still, cache-based models per-form worse on average except for the German-English test-set for which we obtained a slightbut insignificant improvement.
Figure 2 plots theBLEU score differences between standard modelsand cached models for the individual news items.We can see a very blurred picture of these indi-vidual scores and the general conclusion is thatcaching failed.
One problem is that the individ-ual news items are very short (around 20 sentenceseach) which is probably too little for caching toshow any positive effect.
Surprising, however, isthe negative influence of caching even on thesesmall documents which is quite similar to the runson the entire sets.
The drop in performance forEnglish-Spanish is especially striking.
We haveno explanation at this point for this exceptional be-havior.A second observation is the variation in individ-ual n-gram precision scores (see table 1).
In all butone case the unigram precision goes up which in-dicates that the cache models often improve lexicalchoice at least in terms of individual words.
Thefirst example in figure 2 could be seen as a slightimprovement due to a consistent lexical choice of?missile?
(instead of ?rocket?
).The main problem, however, in the adaptive ap-proach seems to appear in local contexts whichmight be due to the simplistic language modelingcache.
It would be interesting to study possibilitiesof integrating local dependencies into the cachemodels.
However, there are serious problems withdata sparseness.
Initial experiments with a bigramLM cache did not produce any improvements sofar.Another crucial problem with the cache-basedmodel is of course error propagation.
An exam-ple which is probably due to this issue can be seenbaseline until the end of the journey , are , in turn , tech-nical damage to the rocket .cache until the end of the journey , in turn , technicaldamage to the missile .reference but near the end of the flight there was technicaldamage to the missile .baseline iran has earlier criticism of its human rightsrecord .cache iran rejected previous criticism of its humanrights record .reference iran has dismissed previous criticism of its hu-man rights record .baseline facing conservationists is accused of extortioncache facing conservationists is accused of extortionreference Nature protection officers accused of blackmailbaseline the leitmeritz-polizei accused the chairman ofthe bu?rgervereinigung ?
naturschutzgemein-schaft leitmeritz ?
because of blackmail .cache the leitmeritz-polizei accused the chairman ofthe bu?rgervereinigung ?
naturschutzgemein-schaft leitmeritz ?
because of extortion .reference The Litomerice police have accused the chair-man of the Litomerice Nature Protection Soci-ety civil association of blackmail.Table 2: German to English example translations.in table 2 in the last two translations (propagationof the translation option ?extortion?).
This prob-lem is difficult to get around especially in caseof bad baseline translations.
One possible ideawould be to implement a two-pass procedure torun over the entire input first only to fill the cacheand to identify reliable evidence for certain trans-lation options (possibly focusing on simple trans-lation tasks such as short sentences).
Then, in thesecond pass the adaptive model can be applied toprefer repetition and consistency according to theparameters learned in the first pass.3.2 Parameter OptimizationAnother question is if the cache parameters re-quire careful optimization in order to make thisapproach effective.
An attempt to investigate theinfluence of the cache components by simply vary-ing the interpolation weights gave us the followingresults for English-German (see table 3).fixed cache TM parameters fixed cache LM parameters?LM BLEU ?TM BLEU0.1 14.12 0.1 12.750.01 14.39 0.01 13.040.005 14.40 0.005 13.570.001 14.44 0.001 14.420.0005 14.43 0.0005 14.57Table 3: Results for English to German with vary-ing mixture weights.Looking at these results the tendency of the scores192-10-8-6-4-2024en-de-10-8-6-4-2024de-en-10-8-6-4-2024 en-es-10-8-6-4-2024 es-enFigure 2: BLEU score differences between a standard model and a cached model for individual newssegments from the WMT test-set.seems to suggest that switching off caching is theright thing to do (as one might have expected al-ready from the initial experimental results).
Wedid not perform the same type of investigation forthe other language pairs but we expect a similarbehavior.Even though these results did not encourage usvery much to investigate the possibilities of cacheparameter optimization any further we still tried tolook at the integration of the interpolation weightsinto the MERT procedure.
The weight of the TMcache is especially suited for MERT as this com-ponent is implemented in terms of a separate fea-ture function within the global log-linear modelused in decoding.
The LM mixture model, onthe other hand, is implemented internally withinSRILM and therefore not so straightforward to in-tegrate into standard MERT.
We, therefore, dou-bled the number of LM?s included in the SMTmodel using two standard LM?s and two LM?swith cache (one for Europarl and one for Newsin both cases).
The latter are actually mixtures aswell using a fixed interpolation weight of ?LM =0.5 between the cached component and the back-ground model.
In this way the cached LM?s bene-fit from the smoothing with the static backgroundmodel.
Individual weights for all four LM?s arethen learned in the global MERT procedure.
Un-fortunately, other cache parameters cannot be op-timized in this way as they do not produce any par-ticular values for individual translation hypothesesin decoding.We applied this tuning setup to the English-German translation task and ran MERT on thesame development data as before.
Actually,caching slows down translation quite substantiallywhich makes MERT very slow.
Due to the se-quential caching procedure it is also not possibleto parallelize tuning.
Furthermore, the extra pa-rameters seem to cause problems in convergenceand we had to stop the optimization after 30 iter-ations when BLEU scores seemed to start stabi-lizing around 14.9 (in the standard setup only 12iterations were required to complete tuning).
Un-fortunately, the result is again quite disappointing(see table 4).Actually, the final BLEU score after tuning is evenlower than in our initial runs with fixed cacheparameters taken from previous unrelated exper-iments.
This is very surprising and it looks likethat MERT just failed to find settings close to theglobal optimum because of some strong local sub-optimal points in the search space.
One would ex-pect that it should be possible to obtain at least the193BLEU on dev-set (no caching) 15.2BLEU on dev-set (with caching) 14.9Europarl LM 0.000417News LM 0.057042Europarl LM (with cache) 0.002429News LM (with cache) -0.000604?TM 0.000749BLEU on test-set (no caching) 15.6BLEU on test-set (with caching) 12.7Table 4: Tuning cache parameters.same score on the development set which was notthe case in our experiment.
However, as alreadymentioned, we had to interrupt tuning and thereis still some chance that MERT would have im-proved in later iterations.
At least intuitively, thereseems to be some logic behind the tuned weights(shown in table 4).
The out-of-domain LM (Eu-roparl) obtains a higher weight with caching thanwithout and the in-domain LM (News) is betterwithout it and, therefore, the cached version ob-tains a negative weight.
Furthermore, the TMcache weight is quite similar to the one we used inthe initial experiments.
However, applying thesesettings to the test-set did not work at all.4 ConclusionsIn our WMT10 experiments cache-based adaptivemodels failed to improve translation quality.
Pre-vious experiments have shown that they can beuseful in adapting SMT models to new domains.However, they seem to have their limitations in thegeneral case with mixed topics involved.
A gen-eral problem is error propagation and the corrup-tion of local dependencies due to over-simplifiedcache models.
Parameter optimization seems tobe difficult as well.
These issues should be inves-tigated further in future research.ReferencesP.R.
Clarkson and A. J. Robinson.
1997.
Languagemodel adaptation using mixtures and an exponen-tially decaying cache.
In International Confer-ence on Acoustics, Speech, and Signal Processing(ICASSP), pages 799?802, Munich, Germany.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InACL ?07: Proceedings of the 45th Annual Meetingof the ACL, pages 177?180, Morristown, NJ, USA.Roland Kuhn and Renato De Mori.
1990.
A cache-based natural language model for speech recogni-tion.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 12(6):570?583.Laurent Nepveu, Lapalme, Guy, Langlais, Philippe,and George Foster.
2004.
Adaptive Language andTranslation Models for Interactive Machine Trans-lation.
In Proceedings of the 9th Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 190?197, Barcelona, Spain.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In ACL ?03:Proceedings of the 41st Annual Meeting on Associa-tion for Computational Linguistics, pages 160?167,Morristown, NJ, USA.Martin Raab.
2007.
Language Modeling for MachineTranslation.
VDM Verlag, Saarbru?cken, Germany.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the 7thinternational conference on spoken language pro-cessing (ICSLP 2002), pages 901?904, Denver, CO,USA.David Talbot and Miles Osborne.
2007.
Randomisedlanguage modelling for statistical machine transla-tion.
In ACL ?07: Proceedings of the 45th AnnualMeeting of the ACL, Prague, Czech Republic.
Asso-ciation for Computational Linguistics.Jo?rg Tiedemann.
2009.
News from OPUS - A collec-tion of multilingual parallel corpora with tools andinterfaces.
In Recent Advances in Natural LanguageProcessing, volume V, pages 237?248.
John Ben-jamins, Amsterdam/Philadelphia.Jo?rg Tiedemann.
2010.
Context adaptation in statisti-cal machine translation using models with exponen-tially decaying cache.
In ACL 2010 Workshop onDomain Adaptation for Natural Language Process-ing (DANLP).194
