DARPA FEBRUARY 1992 P ILOT CORPUS CSR"DRY RUN"  BENCHMARK TEST  RESULTSDavid S. PallettNational Institute of Standards and TechnologyBuilding 225, Room A216Gaithersburg, MD 208991 In t roduct ionContinuous speech recognition research activities withinthe DARPA Spoken Language community have, withinthe past several years, been focussed on the ResourceManagement (RM) and Air Travel Information System(ATIS) corpora.
Within the past year, plans have beendeveloped for a large, multi-component "general-purposeEnglish, large vocabulary, natural anguage, high per-plexity corpus" known as the DARPA \[Wall StreetJournal-based\] Continuous peech Recognition (CSR)Corpus \[1\].
Doug Paul, of MIT Lincoln Laboratory(MIT/LL), and Janet Baker, of Dragon Systems, are re-sponsible for many of the details of these plans.
This cor-pus is intended to supplant the RM corpora nd to sup-plement he ATIS corpora as resources for the DARPAspeech recognition research community.Plans to coordinate the design and collection of theCSR Corpus have, since October 1991, been coordinatedby the DARPA CSR Corpus Coordinating Committee(CCCC), chaired by George Doddington, following dis-cussions held by an earlier group \[2\].In a meeting held at MIT Laboratory for Computer Sci-ence (MIT/LCS) in August of 1991, plans were devel-oped for an initial "Pilot Corpus" comprising approxi-mately 40 hours of recorded speech material, which wasto be made available within the DARPA community inadequate time in order to permit reporting preliminaryor "dry run" benchmark tests at the February 1992 meet-ing.Following that meeting, NIST, acting as a DARPA"agent", contracted with Texas Instruments and SRI In-ternational (SRI) \[3\] for collection of the Pilot Corpusand the spoken language group at MIT/LCS also agreedto collect a substantial amount of material for the Pi-lot Corpus \[4\].
NIST prepared the material for produc-tion on recordable CD-ROM media (at MIT/LCS) andscreened the associated transcriptions for conformanceto standards.
The group at SRI was the only groupthat collected "spontaneous dictation" in addition to the"read speech" comprising the bulk of the Pilot CSR Cot-pus.More than 80 hours of material (per microphonechannel) had been collected and distributed to sev-eral DARPA contractors.
This material includedSpeaker-Dependent, Longitudinal Speaker- Dependent,and Speaker-Independent training components as wellas specifically designated Development Test sets.On January 17th (approximately one month before theSpeech and Natural Language Workshop), two CD-ROMs containing a selected portion of the Pilot Cor-pus's Evaluation Test set were were distributed by NISTto four sites: CMU, Dragon Systems, MIT/LL, and SRIInternational.
These sites had indicated interest in par-ticipating in the initial "dry run" benchmark test asso-ciated with the CSR Pilot Corpus.2 Benchmark  Test  Mater ia lThe selected portion of the Evaluation Test Set that wasdistributed for use in the "dry run" benchmark tests, likethe training material, included three major components:(1) Longitudinal-Speaker-Dependent sp ech recognitionsystem test material, for use with the 3 speakers forwhich approximately 2400 CSR WSJ sentence utter-ances, per speaker, were available for speaker-dependentsystem training, (2) Speaker-Dependent system test ma-terial, for use with a set of 12 speakers for which 600sentence utterances were available for speaker-dependentsystem training, and (3) Speaker-Independent systemtest material, with 10 speakers.
The data was furtherbroken down into verbalized-punctuation (VP) and non-verbalized-punctuation (NVP) and 5,000- vs. 20,000-word vocabularies.For the purposes of speaker-independent system devel-opment, a specific set of approximately 7200 utterancesobtained from an independent set of 84 speakers includedin the training portion of the corpus had been designatedwith the concurrence of the CCCC.The tes.t material included material from SRI,MIT/LCS, TI and NIST (for one Speaker Independent382subject).
Approximately 50% was from male speakers,and 50% female.As noted elsewhere \[1-2\], the training and test materialwas selected with reference to predefined 5,000-word and20,000-word lexicons, but with a controlled percentageof out-of-vocabulary (OOV) items.
NIST's analysis ofthe test sets indicate that the actual occurence of OOVitems in the 5,000 word SI test material is approximately1.4% to 2.0%, and for the 20,000 word SI test material,it is 2.0% to 2.5%.
In contrast, for the SI spontaneoustest set, the incidence of OOV items with respect o the5,000 word closed language model is 13.2% to 15.6%, and5.3% to 5.6% with respect o the 20,000 word languagemodel.For this Pilot CSR Corpus, data was collected with both"primary" and "secondary" microphones.
In every case,the primary microphone was a member of the Sennheiserclose-talking, supra-aural headset- mounted, noise can-celling family (e.g., HMD-414, HMD-410).
However, themicrophones used as the secondary microphone were var-ied, and included boundary effect surface-mounted mi-crophones uch as the Crown PCC-160, Crown PZM-6FS, and Shure SM91).3 Benchmark  Test P ro toco lsThe CCCC had agreed that, insofar as very little timehad been allocated for system development and use ofthe Training and Development Test material, the con-tractor's results would not be reported to NIST untilFebruary 17th, less than one week prior to the Speechand Natural Language Workshop.
It was also agreedthat existing scoring software would be used as well aspreviously established procedures for scoring and report-ing speech recogntion benchmark tests.The four sites (CMU \[5\], Dragon Systems \[6\], MIT/LL\[7\] and SRI \[8\]) provided NIST with a total of 22 setsof results for a number of test sets and system config-urations.
The number of test set results provided byindividual contractors ranged from 1 to 10.NIST reported scores back to the contractors on Febru-ary 19th.
Subsequently, small discrepancies (typicallyless than one percent in the individual speakers' scores)were noted between the scores that had been determinedat the individual sites and NIST's scores.
Some of thesediscrepancies were due to a problem in handling the oc-curence of left parenthesis characters, "(", in the hypoth-esis strings in NIST's scoring program, and these differ-ences were resolved after the Workshop.
Consequently,there may be small unresolved ifferences between scoresreported in this paper and others in this Proceedings.4 "Best" Dry  Run  Eva luat ion  TestBenchmark  Test Resu l t sThe DARPA Spoken Language community's efforts tocollect, annotate, process, and distribute the Pilot Cor-pus were challenging and highly stressful.
It was gen-erally agreed that there was insufficient time for systemdevelopment between release of the training data and re-porting "dry run" results, and that the systems for whichresults could be reported at the meeting represented onlypreliminary efforts.Papers presented at the meeting typically include com-ments such as: "The training paradigm outlined.., in thedescription...has only recently been fully implemented..."and "there has not yet been any opportunity for param-eter optimization."
\[6\], or "The tests.., reported here arelittle more than pilot tests for debugging purposes andno strong conclusions can be drawn."
\[7\] and "Our strat-egy was to implement a system as quickly as possible inorder to meet the tight CSR deadline.\[8\]"In view of these comments, and because comparisonsacross sites would be inconclusive, only a selected sub-set of results reported at the meeting are included inthis paper.
Several of the participants suggested that itwould be acceptable to cite the "best" scores, based onlowest word error rate in a given test subset, and to doso without attribution to any specific system.The "dry run" test results included in this paper (Ta-ble 1), are restricted to those selected "best" reportedscores, and are presented without attribution to specificsystems or sites.
References 5 to 8 may contain addi-tional information defining the context of these scores,or contain complementary findings based on experiencewith the development test sets.
Caution should be exer-cised in interpreting these results as a valid indicator ofthe state-of-the-art, because of the short time for systemdevelopment and debugging (as noted above).5 D iscuss ionThe initial "dry run" test results indicate general trends.Many of these trends would seem to be obvious, butare noted because one of the purposes of the CSR PilotCorpus and the "dry run" was to verify the community'sexpectations with respect o challenges inherant in large-vocabulary continuous peech recognition, and to guagethe relative significance of many factors.?
Results for the test sets selected from a smaller vo-cabulary (5,000 vs. 20,000 words) have lower errorrates (e.g., for the longitudinal speaker dependent383speakers, for VP, 6.7% word error for the 5,000 wordtest subset vs. 10.6% for the 20,000 word test sub-set.?
Results for better-trained speaker dependent sys-terns are better than for less-well-trained speaker-dependent systems (e.g., 6.7% word error for thetest subset for the longitudinal speaker dependentspeakers (5,000 word VP) vs. 14.7% for the speakerdependent speakers, for which one-fourth as muchtraining material was available for each speaker).?
Results for speaker-independent systems havehigher error rates than for speaker-dependent sys-tems (e.g., 16.6% word error for the Speaker Inde-pendent subset (5,000-word VP) vs. 12.9% for thecorresponding Speaker Dependent subset and 6.7%for the Longitudinal Speaker Independent subset).?
Results for the VP test subsets in general have lowererror rates than for the NVP test subsets.?
Comparison of spontaneous v .
"read spontaneous"data indicates that the read spontaneous has lowererror rates (as had been noted with earlier ATIS0data).evidence of gain changes between sessions and of theuse of different secondary microphones for different datacollection sessions (i.e., for the adaptation sentences vs.the read Wall Street Journal sessions).Although reservations have been expressed by the par-ticipants in this initial "dry run" test, it should be notedthat the results are highly encouraging in many ways.
Asthe participants noted, "The successful application of...to the WSJ-CSR task demonstrates the utility of..." 'and"We have also demonstrated the utility of... in the con-text of a much larger task".\[5\] and "It is encouragingthat.., given there has not yet been any opportunityfor parameter optimization.
"\[6\], and"The results, how-ever, show promise and will require more rigorous test-ing."
\[7\] and "This is a preliminary report demonstratingthat.., was ported from a 1000-word task (ATIS) to alarge vocabulary task (5000-word) task.., in three manweeks."
\[8\]Based on these observations, and on the experiencegained in designing, collecting, and distributing theDARPA Pilot CSR Corpus, and in rapidly adapting ex-isting technology to the new domain, there is good reasonto look forward to the results of future benchmark tests.The results for the challenging "spontaneous" and "readspontaneous" peech test subsets are based on only onesites's processing of the test data.Only one site \[8\] reported results using both the pri-mary and the secondary microphone(s) for the 5,000word speaker Independent VP subset, reporting 16.6%word error for the primary microphone and 26.0% for thesecondary microphone data.
The incremental degrada-tion in performance was regarded by the developers asless than might have been expected and "noteworthy"\[9\], particularly in view of the fact that the broadbandsignal to noise ratio for the secondary microphone datawas typically 20 to 30 dB less than that for the primarymicrophone data.Substantial variability in the rank-ordering of individualspeakers can be noted across ystems for those data sub-sets for which more than one site's or system's responseswere reported.
Analysis of this data suggests that somesystems had greater variances across the speaker popu-lation than others, perhaps because of inadequate timeto develop robust speaker-independent models.NIST's measurements of the broadband S/N ratios forthe primary microphone data from MIT, SRI, and TIrange from 40 to 48 dB with values for the secondarymicrophone some 20 to 30 dB less than that.
Histogramsshowing the distribution of levels for these files reveal6 AcknowledgementsAt NIST, John Garofolo has been the individual respon-sible for coordinating and screening much of the CSRdata collected at MIT/LCS, SRI and TI.
Brett Tjadenassisted in preparation ofthe master tapes for CD-ROMproduction at MIT/LCS.
Jon Fiscus adapted the NISTspeech recognition scoring software for scoring the testresults and implemented the software in preparing theofficial results.71.ReferencesPaul, D.B.
and Baker, J.M., "The Design for the WallStreet Journal-based CSR Corpus", in Proc.
Speech andNatural Language workshop, February 1992, (M. Mar-cus, ed.)
Morgan Kaufmann Publishers, Inc.2.
Doddington, G.D., "CSR Corpus Development" in Proc.Speech and Natural Language Workshop, February1992, (M. Marcus, ed.)
Morgan Kaufmann Publishers,Inc.3.
Phillips, M. et al, "Collection and Analyses of WSJ-CSR Data at MIT", in Proc.
Speech and Natural Lan-guage Workshop, February 1992, (M. Marcus, ed.)
Mor-gan Kaufmann Publishers, Inc.4.
Bernstein, J. and Danielson, D., "Spontaneous SpeechCollection for the CSR Corpus", in Proc.
Speech andNatural Language Workshop, February 1992, (M. Mar-cus, ed.)
Morgan Kaufmann Publishers, Inc.3845.
Alleva, F. et al, "Applying SPttINX-II to the DARPAWall Street Journal CSR Task", in Proc.
Speech andNatural Language Workshop, February 1992, (M. Mar-cus, ed.)
Morgan Kaufmann Pubhshers, Inc.6.
Baker, J. et al, "Large Vocabulary Recognition ofWall Street Journal sentences at Dragon Systems", inProc.
Speech and Natural Language Workshop, Febru-ary 1992, (M. Marcus, ed.)
Morgan Kaufmann Publish-ers, Inc.7.
Paul, D.B., "The Lincoln Large-Vocabulary HMMCSR', in Proc.
Speech and Natural Language Work-shop, February 1992, (M. Marcus, ed.)
Morgan Kauf-mann Publishers, Inc.8.
Murveit, H., Butzberger, J. and Weintraub, M., "Perfor-mance of SRI's DECIPHER Speech Recognition systemon DARPA's CSR Task", in Proc.
Speech and NaturalLanguage Workshop, February 1992, (M. Marcus, ed.
)Morgan Kaufmann Publishers, Inc.9.
Murveit, H., personal communication to D.S.
Pallett,Mar.6, 1992.385Word Err.
Sent.
Err.
"5,000 Word"l(a) Longitudinal Speaker DependentNVP 10.6 75.6VP 6.7 55.6"20,000 Word" NVP 17.4 82.2VP 14.7 86.7"5,000 Word"l(b) Speaker DependentVP 12.9 73.3"5,000 Word"l(c) Speaker IndependentNVP 17.1 77.5VP 16.6 80.0"20,000 Word" NVP 37.9 94.5VP 32.8 93.4"Spontaneous"l(d) "Spontaneous" Speaker IndependentNVP 55.8 100.0VP 45.5 97.0"Read Spontaneous" NVP 50.2 99.0VP 41.3 98.0Table 1: "Best" Reported Scores, Selected on Word Error Percentage386
