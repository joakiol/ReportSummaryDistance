Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 223?229,Dublin, Ireland, August 23-24, 2014.DCU: Aspect-based Polarity Classification for SemEval Task 4Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab BarmanDasha Bogdanova, Jennifer Foster and Lamia TounsiCNGL Centre for Global Intelligent ContentNational Centre for Language TechnologySchool of ComputingDublin City UniversityDublin, Ireland{jwagner,parora,scortes,ubarman}@computing.dcu.ie{dbogdanova,jfoster,ltounsi}@computing.dcu.ieAbstractWe describe the work carried out by DCUon the Aspect Based Sentiment Analysistask at SemEval 2014.
Our team submit-ted one constrained run for the restaurantdomain and one for the laptop domain forsub-task B (aspect term polarity predic-tion), ranking highest out of 36 systems onthe restaurant test set and joint highest outof 32 systems on the laptop test set.1 IntroductionThis paper describes DCU?s participation in theAspect Term Polarity sub-task of the Aspect BasedSentiment Analysis task at SemEval 2014, whichfocuses on predicting the sentiment polarity of as-pect terms for a restaurant and a laptop dataset.Given, for example, the sentence I have had somany problems with the computer and the aspectterm the computer, the task is to predict whetherthe sentiment expressed towards the aspect term ispositive, negative, neutral or conflict.Our polarity classification system uses super-vised machine learning with support vector ma-chines (SVM) (Boser et al., 1992) to classify anaspect term into one of the four classes.
The fea-tures we employ are word n-grams (with n rang-ing from 1 to 5) in a window around the aspectterm, as well as features derived from scores as-signed by a sentiment lexicon.
Furthermore, toreduce data sparsity, we experiment with replacingsentiment-bearing words in our n-gram feature setwith their polarity scores according to the lexiconand/or their part-of-speech tag.This work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/The paper is organised as follows: in Section 2,we describe the sentiment lexicons used in thiswork and detail the process by which they arecombined, filtered and extended; in Section 3, wedescribe our baseline method, a heuristic approachwhich makes use of the sentiment lexicon, fol-lowed by our machine learning method which in-corporates the rule-based method as features in ad-dition to word n-gram features; in Section 4, wepresent the results of both methods on the trainingand test data, and perform an error analysis on thetest set; in Section 5, we compare our approach toprevious research in sentiment classification; Sec-tion 6 discusses efficiency of our system and on-going work to improve its speed; finally, in Sec-tion 7, we conclude and provide suggestions as tohow this research could be fruitfully extended.2 Sentiment LexiconsThe following four lexicons are employed:1.
MPQA1(Wilson et al., 2005) classifies aword or a stem and its part of speech taginto positive, negative, both or neutral witha strong or weak subjectivity.2.
SentiWordNet2(Baccianella et al., 2010)specifies the positive, negative and objectivescores of a synset and its part of speech tag.3.
General Inquirer3indicates whether a wordexpresses positive or negative sentiment.4.
Bing Liu?s Opinion Lexicon4(Hu and Liu,1http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/2http://sentiwordnet.isti.cnr.it/3http://www.wjh.harvard.edu/?inquirer/inqtabs.txt4http://www.cs.uic.edu/?liub/FBS/sentiment-analysis.html#lexicon2232004) indicates whether a word expressespositive or negative sentiment.2.1 Lexicon CombinationSince the four lexicons differ in their level of detailand in how they present information, it is neces-sary, when combining them, to consolidate the in-formation and present it in a uniform manner.
Ourcombination strategy assigns a sentiment score toa word as follows:?
MPQA: 1 for strong positive subjectivity, -1for strong negative subjectivity, 0.5 for weakpositive subjectivity, -0.5 for weak negativesubjectivity, and 0 otherwise?
SentiWordNet: The positive score if the pos-itive score is greater than the negative and ob-jective scores, the negative score if the nega-tive score is greater than the positive and theobjective scores, and 0 otherwise?
General Inquirer and Bing Liu?s OpinionLexicon: 1 for positive and -1 for negativeThe above four scores are summed to arrive at afinal score between -4 and 4 for a word.52.2 Lexicon FilteringInitial experiments with our sentiment lexicon andthe training data led us to believe that there weremany irrelevant entries that, although capable ofconveying sentiment in some other context, werenot contributing to the sentiment of aspect termsin the two domains of the task.
Therefore, thesewords are manually filtered from the lexicon.
Ex-amples of deleted words are just, clearly, indi-rectly, really and back.2.3 Adding Domain-Specific WordsA manual inspection of the training data revealedwords missing from the merged sentiment lexiconbut which do express sentiment in these domains.Examples are mouthwatering, watery and better-configured.
We add these to the lexicon with ascore of either 1 or -1 (depending on their polarityin the training data).
We also add words (e.g.
zesty,acrid) from an online list of culinary terms.65We also tried to vote over the four lexicon scores but thisdid not improve over summing.6http://world-food-and-wine.com/describing-food2.4 Handling VariationIn order to ensure that all inflected forms of aword are covered, we lemmatise the words in thetraining data using the IMS TreeTagger (Schmid,1994) and we construct new possibilities using asuffix list.
To correct misspelled words, we con-sider the corrected form of a misspelled word to bethe form with the highest frequency in a referencecorpus7among all the forms within an edit dis-tance of 1 and 2 from the misspelled word (Norvig,2012).
Multi-word expressions of the form x-yare added with the polarity of xy or x, as in laid-back/laidback and well-shaped/well.
Expressionsx y, are added with the polarity of x-y, as in soso/so-so.3 MethodologyWe first build a rule-based system which classi-fies the polarity of an aspect term based solely onthe scores assigned by the sentiment lexicon.
Wethen explore different ways of converting the rule-based system into features which can then be com-bined with bag-of-n-gram features in a supervisedmachine learning set-up.3.1 Rule-Based ApproachIn order to predict the polarity of an aspect term,we sum the polarity scores of all the words in thesurrounding sentence according to our sentimentlexicon.
Since not all the sentiment words occur-ring in a sentence influence the polarity of the as-pect term to the same extent, it is important toweight the score of each sentiment word by its dis-tance to the aspect term.
Therefore, for each wordin the sentence which is found in our lexicon wetake the score from the lexicon and divide it by itsdistance to the aspect term.
The distance is calcu-lated using the sum of the following three distancefunctions:?
Token Distance: This function calculates thedifference in the position of the sentimentword and the aspect term by counting the to-kens between them.7The reference corpus consists of about a millionwords retrieved from several public domain books fromProject Gutenberg (http://www.gutenberg.org/),lists of most frequent words from Wiktionary (http://en.wiktionary.org/wiki/Wiktionary:Frequency_lists) and the British National Corpus(http://www.kilgarriff.co.uk/bnc-readme.html) and two thousand laptop reviews crawled from CNET(http://www.cnet.com/).224?
Discourse Chunk Distance: This functioncounts the discourse chunks that must becrossed in order to get from the sentimentword to the aspect term.
If the sentimentword and the aspect term are in the samediscourse chunk, then the distance is zero.We use the discourse segmenter described in(Tofiloski et al., 2009).?
Dependency Path Distance: This functioncalculates the shortest path between the sen-timent word and the aspect term in a syntac-tic dependency graph for the sentence, pro-duced by parsing the sentence with a PCFG-LA parser (Attia et al., 2010) trained on con-sumer review data (Le Roux et al., 2012)8,and converting the resulting phrase-structuretree into a dependency graph using the Stan-ford converter (de Marneffe and Manning,2008) (version 3.3.1).Since our lexicon also contains multi-word ex-pressions such as finger licking, we also look upbigrams and trigrams from the input sentence inour lexicon.
Negation is handled by reversing thepolarity of sentiment words that appear within awindow of three words of the following negators:not, n?t, no and never.For each aspect term, we use the distance-weighted sum of the polarity scores to predict oneof the three classes positive, negative and neutral.9After experimenting with various thresholds wesettled on the following simple strategy: if the po-larity score for an aspect term is greater than zerothen it is classified as positive, if the score is lessthan zero, then it is classified as negative, other-wise it is classified as neutral.3.2 Machine Learning ApproachWe train a four-way SVM classifier for each do-main (laptop and restaurant), using Weka?s SMOimplementation (Platt, 1998; Hall et al., 2009).108To facilitate parsing, the data was normalised using theprocess described in (Le Roux et al., 2012) with minor mod-ifications, e. g. treatment of non-breakable space characters,abbreviations and emoticons.
The normalised version of thedata was used for all experiments.9We also experimented with classifying aspect terms asconflict when the individual scores for positive and negativesentiment were both relatively high.
However, this provedunsuccessful.10We also experimented with logistic regression, randomforests, k-nearest neighbour, naive Bayes and multi-layer per-ceptron in Weka, but did not match performance of an SVMtrained with default parameters.Transf.
n c n-gram Freq.-L?
2 2 cord with 1AL?
2 2 <aspect> with 56ALS?
1 4 <negu080> 595ALSR- 1 4 <negu080> 502AL?
2 4 and skip 1ALSR- 2 4 and <negu080> 25ALSRP 1 4 <negu080>/vb 308Table 1: 7 of the 2,640 bag-of-n-gram featuresextracted for the aspect term cord from the lap-top training sentence I charge it at night and skiptaking the cord with me because of the good bat-tery life.
The last column shows the frequency ofthe feature in the training data.
Transformations:A=aspect, L=lowercase, S=score, R=restricted tocertain POS, P=POS annotationOur system submission uses bag-of-n-gram fea-tures and features derived from the rule-based ap-proach.
Decisions about parameters are made in 5-fold cross-validation on the training data providedfor the task.3.2.1 Bag-of-N-gram FeaturesWe extract features encoding the presence of spe-cific lower-cased n-grams (L) (n = 1, ..., 5) inthe context of the aspect term to be classified (cwords to the left and c words to the right withc = 1, ..., 5, inf) for 10 combinations of trans-formations: replacement of the aspect term with<ASPECT> (A), replacement of sentiment wordswith a discretised score (S), restriction (R) of thesentiment word replacement to certain parts-of-speech, and annotation of the discretised scorewith the POS (P) of the sentiment word.
An ex-ample is shown in Table 1.3.2.2 Adding Rule-Based Score FeaturesWe explore two approaches for incorporating in-formation from the rule-based approach (Sec-tion 3.1) into our SVM classifier.
The first ap-proach is to encode polarity scores directly as thefollowing four features:1. distance-weighted sum of scores of positivewords in the sentence2.
distance-weighted sum of scores of negativewords in the sentence3.
number of positive words in the sentence2254.
number of negative words in the sentenceThe second approach is less direct: for each do-main, we train J48 decision trees with minimumleaf size 60 using the four rule-based features de-scribed above.
We then use the decision rulesand the conjunctions leading from the root nodeto each leaf node to binarise the above four basicscore features, producing 122 features.
Further-more, we add normalised absolute values, rank ofvalues and interval indicators, producing 48 fea-tures.3.2.3 Submitted RunsWe eliminate features that have redundant valuecolumns for the training data, and we apply fre-quency thresholds (13, 18, 25 and 35) to furtherreduce the number of features.
We perform a grid-search to optimise the parameters C and ?
of theSVM RBF kernel.
We choose the system to sub-mit based on average cross-validation accuracy.We experiment with combinations of the three fea-ture sets described above.
We choose the bina-rised features over the raw rule-based scores be-cause cross-validation results are inferior for therule-based scores in initial experiments with fea-ture frequency threshold 35: 70.26 vs. 71.36 forlaptop and 72.06 vs. 72.15 for restaurant.
There-fore, we decide to focus on systems with binarisedscore features for lower feature frequency thresh-olds, which are more CPU-intensive to train.
Forboth domains, the system we end up submittingis a combination of the n-gram features and thebinarised features with parameters C = 3.981,?
= 0.003311 for the laptop data, C = 1.445,?
= 0.003311 for the restaurant data, and a fre-quency threshold of 13.4 Results and AnalysisTable 2 shows the training and test accuracy ofthe task baseline system (Pontiki et al., 2014), amajority baseline classifying everything as posi-tive, our rule-based system and our submitted sys-tem.
The restaurant domain has a higher accuracythan the laptop domain for all systems, the SVMsystem outperforms the rule-based system on bothdomains, and the test accuracy is higher than thetraining accuracy for all systems in the restaurantdomain.We observe that the majority of our systems?
er-rors fall into the following categories:Dataset System Training TestLaptop Baseline ?
51.1%Laptop All positive 41.9% 52.1%Laptop Rule-based 65.4% 67.7%Laptop SVM 72.3% 70.5%Restaurant Baseline ?
64.3%Restaurant All positive 58.6% 64.2%Restaurant Rule-based 69.5% 77.8%Restaurant SVM 72.7% 81.0%Table 2: Accuracy of the task baseline system, asystem classifying everything as positive, our rule-based system and our submitted SVM-based sys-tem on train (5-fold cross-validation) and test sets?
Sentiment not expressed explicitly: Thesentiment cannot be inferred from local lexi-cal and syntactic information, e. g. The sushiis cut in blocks bigger than my cell phone.?
Non-obvious expression of negation: Forexample, The Management was less than ac-comodating [sic].
The rule-based approachdoes not capture such cases and there arenot enough similar training examples for theSVM to learn to correctly classify them.?
Conflict cases: The training data containstoo few examples of conflict sentences for thesystem to learn to detect them.11For the restaurant domain, there are more thanfifty cases where the rule-based approach fails todetect sentiment, but the machine learning ap-proach classifies it correctly.
Most of these casescontain no sentiment lexicon words, thus the rule-based system marks them as being neutral.
How-ever, the machine learning system was able to fig-ure out the correct polarity.
Examples of suchcases include Try the rose roll (not on menu) andThe gnocchi literally melts in your mouth!.
Fur-thermore, in the laptop domain, a number of theerrors made by the rule-based system arise fromthe ambiguous nature of some lexicon words.
Forexample, the sentence Only 2 usb ports ... seemskind of ... limited is misclassified because theword kind is considered to be positive.There are a few cases where the rule-based sys-tem outperforms the machine learning one.
It hap-pens when a sentence contains a rare word withstrong polarity, e. g. the word heavenly in The11We only classify one test instance as conflict.226chocolate raspberry cake is heavenly - not toosweet, but full of flavor.5 Related WorkThe use of supervised machine learning with bag-of-word or bag-of-n-gram feature sets has beena standard approach to the problem of sentimentpolarity classification since the seminal work byPang et al.
(2002) on movie review polarity pre-diction.
Heuristic methods which rely on a lexi-con of sentiment words have also been widespreadand much of the research in this area has beendevoted to the unsupervised induction of goodquality sentiment indicators (see, for example,Hatzivassiloglou and McKeown (1997) and Tur-ney (2002), and Liu (2010) for an overview).
Theintegration of sentiment lexicon scores as fea-tures in supervised machine learning to supple-ment standard bag-of-n-gram features has alsobeen employed before (see, for example, Bak-liwal et al.
(2013)).
The replacement of train-ing/test words with scores/labels from sentimentlexicons has also been used by Baccianella etal.
(2009), who supplement n-grams such as hor-rible location with generalised expressions suchas NEGATIVE location.
Linguistic features whichcapture generalisations at the level of syntax (Mat-sumoto et al., 2005), semantics (Johansson andMoschitti, 2010) and discourse (Lazaridou et al.,2013) have also been widely applied.
In using bi-narised features derived from the nodes of a deci-sion tree, we are following our recent work whichuses the same technique in a different task: qualityestimation for machine translation (Rubino et al.,2012; Rubino et al., 2013).The main novelty in our system lies not in theindividual techniques but rather in they way theyare combined and integrated.
For example, ourcombination of token/chunk/dependency path dis-tance used to weight the relationship between asentiment word and the aspect term has ?
to thebest of our knowledge ?
not been applied before.6 EfficiencyBuilding a system for a shared task, we focussolely on the accuracy of the system in all our deci-sions.
For example, we parse all training and testdata multiple times using different grammars toincrease sentence coverage from 99.87% to 100%.To offer a more practical system, we work onimplementing a simplified, fully automated sys-tem that is more efficient.
So far, we replacedtime-consuming parsing with POS tagging.
Thesystem accepts as input and generates as outputvalid SemEval ABSA XML documents.12Afterextracting the text and the aspect terms from theinput, the text is normalised using the process de-scribed in Footnote 8.
The feature extraction isperformed as described in Section 3 with the fol-lowing modifications:?
The POS information used by the n-gramfeature extractor is obtained using the IMSTreeTagger (Schmid, 1994) instead of usingthe PCFG-LA parser (Attia et al., 2010).?
The distance used by the rule-based approachis the token distance only, instead of a com-bination of three distance functions.The sentiment lexicon and the classification mod-els used are described in Sections 2 and 3 respec-tively.The test sets containing 800 sentences are POStagged in less than half a second each.
Surpris-ingly, accuracy of aspect term polarity predictionincreases to 71.4% (from 70.5% for the submittedsystem) on the laptop test set, using the same SVMparameters as for the submitted system.
However,we see a degradation to 78.8% (from 81.0% for thesubmitted system) for the restaurant test set.
Thisis an encouraging result as the SVM parametersare not yet fully optimised for the slightly differentinformation and as the remaining modifications tobe implemented should not change accuracy anyfurther.The next bottleneck that needs to be addressedbefore the system can be used in applications re-quiring quick responses is the current implementa-tion of the n-gram feature extractor: It enumeratesall n-grams (for all context window sizes and n-gram transformations) only to then intersect thesefeatures with the list of selected features.
For theshared task, this made sense as we initially needall features to make our selection of features, andas we only need to run the feature extractor a fewtimes.
For a practical system that has to processnew test sets frequently, however, it will be moreefficient to check for each selected feature whetherthe respective event occurs in the input.12We validate documents using the XML schema defini-tion provided on the shared task website.2277 ConclusionWe have described our aspect term polarity predic-tion system, which employs supervised machinelearning using a combination of n-grams and sen-timent lexicon features.
Although our submittedsystem performs very well, it is interesting to notethat our rule-based system is not that far behind.This suggests that a state-of-the-art system can bebuild without machine learning and that carefuldesign of the other system components is impor-tant.
However, the very good performance of ourmachine-learning-based system also suggests thatword n-gram features do provide useful informa-tion that is missed by a sentiment lexicon alone,and that it is always worthwhile to perform carefulparameter tuning to eke out as much as possiblefrom such an approach.Future work should investigate how much eachsystem component contributes to the overall per-formance, e. g. lexicon combination, lemmatisa-tion, spelling correction, other normalisations,negation handling, distance function and n-gramfeature transformations.
There is also room forimprovements in most of these components, e. g.our handling of complex negations.
Detection ofconflicts also needs more attention.
Features in-dicating the presence of trigger words for negationand conflicts that are currently used only internallyin the rule-based component could be added to theSVM feature set.
It would also be interesting tosee how the compositional approach described bySocher et al.
(2013) handles these difficult cases.The score features could be easily augmented bybreaking down scores by the four employed lexi-cons.
This way, the SVM can choose to combinethe information from these scores differently thanjust summing them, allowing it to learn more com-plex relations.
Lexicon filtering and addition ofdomain-specific entries could be automated to re-duce the time needed to adjust to a new domain.Finally, machine learning methods that can effi-ciently handle large feature sets such as logisticregression should be tried with the full feature set(not applying frequency thresholds).AcknowledgementsThis research is supported by the Science Foun-dation Ireland (Grant 12/CE/I2267) as part ofCNGL (www.cngl.ie) at Dublin City University.The authors wish to acknowledge the DJEI/DES/SFI/HEA Irish Centre for High-End Computing(ICHEC) for the provision of computational facil-ities and support.
We are grateful to Qun Liu andJosef van Genabith for their helpful comments.ReferencesMohammed Attia, Jennifer Foster, Deirdre Hogan,Joseph Le Roux, Lamia Tounsi, and Josef van Gen-abith.
2010.
Handling unknown words in statis-tical latent-variable parsing models for arabic, en-glish and french.
In Proceedings of the NAACLHLT 2010 First Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 67?75.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2009.
Multi-facet rating of product reviews.In Proceedings of ECIR, pages 461?472.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An Enhanced Lex-ical Resource for Sentiment Analysis and OpinionMining.
In Proceedings of the Seventh Conferenceon International Language Resources and Evalua-tion (LREC?10).Akshat Bakliwal, Jennifer Foster, Jennifer van der Puil,Ron O?Brien, Lamia Tounsi, and Mark Hughes.2013.
Sentiment analysis of political tweets: To-wards an accurate classifier.
In Proceedings of theNAACL Workshop on Language Analysis in SocialMedia, pages 49?58.Bernhard E. Boser, Isabelle M. Guyon, andVladimir N. Vapnik.
1992.
A training algo-rithm for optimal margin classifiers.
In Proceedingsof the Fifth Annual Workshop on ComputationalLearning Theory, pages 144?152.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The stanford typed dependencies rep-resentation.
In COLING 2008 Workshop on Cross-framework and Cross-domain Parser Evaluation.,pages 1?8.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.ACM SIGKDD Explorations Newsletter, 11(1):10?18.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proceedings of the 35th Annual Meetingof the ACL and the 8th Conference of the EuropeanChapter of the ACL, pages 174?181.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the TenthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?04, pages168?177.228Richard Johansson and Alessandro Moschitti.
2010.Syntactic and semantic structure for opinion ex-pression detection.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning, pages 67?76.Angeliki Lazaridou, Ivan Titov, and CarolineSporleder.
2013.
A bayesian model for jointunsupervised induction of sentiment, aspect anddiscourse representations.
In Proceedings ofthe 51th Annual Meeting of the Association forComputational Linguistics, pages 1630?1639.Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-sul Samad Zadeh Kaljahi, and Anton Bryl.
2012.DCU-Paris13 systems for the SANCL 2012 sharedtask.
Notes of the First Workshop on SyntacticAnalysis of Non-Canonical Language (SANCL).Bing Liu.
2010.
Sentiment analysis and subjectivity.In Handbook of Natural Language Processing.Shotaro Matsumoto, Hiroya Takamura, and ManubuOkumura, 2005.
Advances in Knowledge Discoveryand Data Mining, volume 3518 of Lecture Notes inComputer Science, chapter Sentiment ClassificationUsing Word Sub-sequences and Dependency Sub-trees, pages 301?311.Peter Norvig.
2012.
How to write a spelling corrector.http://norvig.com/spell-correct.html.
[Online; accessed 2014-03-19].Po Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification us-ing machine learning techniques.
In Proceedings ofEMNLP, pages 79?86.John C. Platt.
1998.
Fast training of support vec-tor machines using sequential minimal optimization.In B. Schoelkopf, C. Burges, and A. Smola, edi-tors, Advances in Kernel Methods - Support VectorLearning, pages 185?208.Maria Pontiki, Dimitrios Galanis, John Pavlopou-los, Harris Papageorgiou, Ion Androutsopoulos, andSuresh Manandhar.
2014.
Semeval-2014 task 4:Aspect based sentiment analysis.
In Proceedings ofthe International Workshop on Semantic Evaluation(SemEval).Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-hann Roturier, Rasul Samad Zadeh Kaljahi, and FredHollowood.
2012.
Dcu-symantec submission forthe wmt 2012 quality estimation task.
In Proceed-ings of the Seventh Workshop on Statistical MachineTranslation, pages 138?144.Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-hann Roturier, Rasoul Samad Zadeh Kaljahi, andFred Hollowood.
2013.
DCU-Symantec at theWMT 2013 quality estimation shared task.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 392?397.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, pages 44?49.Richard Socher, Alex Perelygin, Jean Y. Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of EMNLP, pages 1631?1642.Milan Tofiloski, Julian Brooke, and Maite Taboada.2009.
A syntactic and lexical-based discourse seg-menter.
In Proceedings of the ACL-IJCNLP 2009Conference Short Papers, ACLShort ?09, pages 77?80.Peter Turney.
2002.
Thumbs up or thumbs down?semantic orientation applied to unsupervised classi-cation of reviews.
In Proceedings of the ACL, pages417?424.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the Con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,HLT ?05, pages 347?354.229
