Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 102?111,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsJoint Annotation of Search QueriesMichael BenderskyDept.
of Computer ScienceUniversity of MassachusettsAmherst, MAbemike@cs.umass.eduW.
Bruce CroftDept.
of Computer ScienceUniversity of MassachusettsAmherst, MAcroft@cs.umass.eduDavid A. SmithDept.
of Computer ScienceUniversity of MassachusettsAmherst, MAdasmith@cs.umass.eduAbstractMarking up search queries with linguistic an-notations such as part-of-speech tags, cap-italization, and segmentation, is an impor-tant part of query processing and understand-ing in information retrieval systems.
Dueto their brevity and idiosyncratic structure,search queries pose a challenge to existingNLP tools.
To address this challenge, wepropose a probabilistic approach for perform-ing joint query annotation.
First, we derivea robust set of unsupervised independent an-notations, using queries and pseudo-relevancefeedback.
Then, we stack additional classi-fiers on the independent annotations, and ex-ploit the dependencies between them to fur-ther improve the accuracy, even with a verylimited amount of available training data.
Weevaluate our method using a range of queriesextracted from a web search log.
Experimen-tal results verify the effectiveness of our ap-proach for both short keyword queries, andverbose natural language queries.1 IntroductionAutomatic mark-up of textual documents with lin-guistic annotations such as part-of-speech tags, sen-tence constituents, named entities, or semantic rolesis a common practice in natural language process-ing (NLP).
It is, however, much less common in in-formation retrieval (IR) applications.
Accordingly,in this paper, we focus on annotating search queriessubmitted by the users to a search engine.There are several key differences between userqueries and the documents used in NLP (e.g., newsarticles or web pages).
As previous research shows,these differences severely limit the applicability ofstandard NLP techniques for annotating queries andrequire development of novel annotation approachesfor query corpora (Bergsma and Wang, 2007; Barr etal., 2008; Lu et al, 2009; Bendersky et al, 2010; Li,2010).The most salient difference between queries anddocuments is their length.
Most search queriesare very short, and even longer queries are usuallyshorter than the average written sentence.
Due totheir brevity, queries often cannot be divided intosub-parts, and do not provide enough context foraccurate annotations to be made using the stan-dard NLP tools such as taggers, parsers or chun-kers, which are trained on more syntactically coher-ent textual units.A recent analysis of web query logs by Benderskyand Croft (2009) shows, however, that despite theirbrevity, queries are grammatically diverse.
Somequeries are keyword concatenations, some are semi-complete verbal phrases and some are wh-questions.It is essential for the search engine to correctly an-notate the query structure, and the quality of thesequery annotations has been shown to be a crucialfirst step towards the development of reliable androbust query processing, representation and under-standing algorithms (Barr et al, 2008; Guo et al,2008; Guo et al, 2009; Manshadi and Li, 2009; Li,2010).However, in current query annotation systems,even sentence-like queries are often hard to parseand annotate, as they are prone to contain mis-spellings and idiosyncratic grammatical structures.102(a) (b) (c)Term CAP TAG SEGwho L X Bwon L V Ithe L X B2004 L X Bkentucky C N Bderby C N ITerm CAP TAG SEGkindred C N Bwhere C X Bwould C X Ii C X Ibe C V ITerm CAP TAG SEGshih C N Btzu C N Ihealth L N Bproblems L N IFigure 1: Examples of a mark-up scheme for annotating capitalization (L ?
lowercase, C ?
otherwise), POS tags (N ?noun, V ?
verb, X ?
otherwise) and segmentation (B/I ?
beginning of/inside the chunk).They also tend to lack prepositions, proper punctu-ation, or capitalization, since users (often correctly)assume that these features are disregarded by the re-trieval system.In this paper, we propose a novel joint query an-notation method to improve the effectiveness of ex-isting query annotations, especially for longer, morecomplex search queries.
Most existing research fo-cuses on using a single type of annotation for infor-mation retrieval such as subject-verb-object depen-dencies (Balasubramanian and Allan, 2009), named-entity recognition (Guo et al, 2009), phrase chunk-ing (Guo et al, 2008), or semantic labeling (Li,2010).In contrast, the main focus of this work is on de-veloping a unified approach for performing reliableannotations of different types.
To this end, we pro-pose a probabilistic method for performing a jointquery annotation.
This method allows us to exploitthe dependency between different unsupervised an-notations to further improve the accuracy of the en-tire set of annotations.
For instance, our methodcan leverage the information about estimated parts-of-speech tags and capitalization of query terms toimprove the accuracy of query segmentation.We empirically evaluate the joint query annota-tion method on a range of query types.
Instead ofjust focusing our attention on keyword queries, asis often done in previous work (Barr et al, 2008;Bergsma and Wang, 2007; Tan and Peng, 2008;Guo et al, 2008), we also explore the performanceof our annotations with more complex natural lan-guage search queries such as verbal phrases and wh-questions, which often pose a challenge for IR appli-cations (Bendersky et al, 2010; Kumaran and Allan,2007; Kumaran and Carvalho, 2009; Lease, 2007).We show that even with a very limited amount oftraining data, our joint annotation method signifi-cantly outperforms annotations that were done in-dependently for these queries.The rest of the paper is organized as follows.
InSection 2 we demonstrate several examples of an-notated search queries.
Then, in Section 3, we in-troduce our joint query annotation method.
In Sec-tion 4 we describe two types of independent queryannotations that are used as input for the joint queryannotation.
Section 5 details the related work andSection 6 presents the experimental results.
We drawthe conclusions from our work in Section 7.2 Query Annotation ExampleTo demonstrate a possible implementation of lin-guistic annotation for search queries, Figure 1presents a simple mark-up scheme, exemplified us-ing three web search queries (as they appear in asearch log): (a) who won the 2004 kentucky derby,(b) kindred where would i be, and (c) shih tzu healthproblems.
In this scheme, each query is marked-up using three annotations: capitalization, POS tags,and segmentation indicators.Note that all the query terms are non-capitalized,and no punctuation is provided by the user, whichcomplicates the query annotation process.
Whilethe simple annotation described in Figure 1 can bedone with a very high accuracy for standard docu-ment corpora, both previous work (Barr et al, 2008;Bergsma and Wang, 2007; Jones and Fain, 2003)and the experimental results in this paper indicatethat it is challenging to perform well on queries.The queries in Figure 1 illustrate this point.
Query(a) in Figure 1 is a wh-question, and it contains103a capitalized concept (?Kentucky Derby?
), a singleverb, and four segments.
Query (b) is a combinationof an artist name and a song title and should be inter-preted as Kindred ?
?Where Would I Be?.
Query (c)is a concatenation of two short noun phrases: ?ShihTzu?
and ?health problems?.3 Joint Query AnnotationGiven a search query Q, which consists of a se-quence of terms (q1, .
.
.
, qn), our goal is to anno-tate it with an appropriate set of linguistic structuresZQ.
In this work, we assume that the setZQ consistsof shallow sequence annotations zQ, each of whichtakes the formzQ = (?1, .
.
.
, ?n).In other words, each symbol ?i ?
zQ annotates asingle query term.Many query annotations that are useful for IRcan be represented using this simple form, includ-ing capitalization, POS tagging, phrase chunking,named entity recognition, and stopword indicators,to name just a few.
For instance, Figure 1 demon-strates an example of a set of annotations ZQ.
Inthis example,ZQ = {CAP,TAG,SEG}.Most previous work on query annotation makesthe independence assumption ?
every annotationzQ ?
ZQ is done separately from the others.
That is,it is assumed that the optimal linguistic annotationz?
(I)Q is the annotation that has the highest probabil-ity given the query Q, regardless of the other anno-tations in the set ZQ.
Formally,z?
(I)Q = argmaxzQp(zQ|Q) (1)The main shortcoming of this approach is in theassumption that the linguistic annotations in the setZQ are independent.
In practice, there are depen-dencies between the different annotations, and theycan be leveraged to derive a better estimate of theentire set of annotations.For instance, imagine that we need to perform twoannotations: capitalization and POS tagging.
Know-ing that a query term is capitalized, we are morelikely to decide that it is a proper noun.
Vice versa,knowing that it is a preposition will reduce its proba-bility of being capitalized.
We would like to capturethis intuition in the annotation process.To address the problem of joint query annotation,we first assume that we have an initial set of annota-tions Z?
(I)Q , which were performed for query Q in-dependently of one another (we will show an exam-ple of how to derive such a set in Section 4).
Giventhe initial set Z?
(I)Q , we are interested in obtainingan annotation set Z?
(J)Q , which jointly optimizes theprobability of all the annotations, i.e.Z?
(J)Q = argmaxZQp(ZQ|Z?
(I)Q ).If the initial set of estimations is reasonably ac-curate, we can make the assumption that the anno-tations in the set Z?
(J)Q are independent given theinitial estimates Z?
(I)Q , allowing us to separately op-timize the probability of each annotation z?
(J)Q ?Z?
(J)Q :z?
(J)Q = argmaxzQp(zQ|Z?
(I)Q ).
(2)From Eq.
2, it is evident that the joint an-notation task becomes that of finding some opti-mal unobserved sequence (annotation z?
(J)Q ), giventhe observed sequences (independent annotation setZ?
(I)Q ).Accordingly, we can directly use a supervised se-quential probabilistic model such as CRF (Laffertyet al, 2001) to find the optimal z?
(J)Q .
In this CRFmodel, the optimal annotation z?
(J)Q is the label weare trying to predict, and the set of independent an-notations Z?
(I)Q is used as the basis for the featuresused for prediction.
Figure 2 outlines the algorithmfor performing the joint query annotation.As input, the algorithm receives a training set ofqueries and their ground truth annotations.
It thenproduces a set of independent annotation estimates,which are jointly used, together with the groundtruth annotations, to learn a CRF model for each an-notation type.
Finally, these CRF models are usedto predict annotations on a held-out set of queries,which are the output of the algorithm.104Input: Qt ?
training set of queries.ZQt ?
ground truth annotations for the training set of queries.Qh ?
held-out set of queries.
(1) Obtain a set of independent annotation estimates Z?
(I)Qt(2) Initialize Z?
(J)Qt ?
?
(3) for each z?
(I)Qt ?
Z?
(I)Qt :(4) Z ?Qt ?
Z?
(I)Qt \ z?
(I)Qt(5) Train a CRF model CRF(zQt) using zQt as a label and Z ?Qt as features.
(6) Predict annotation z?
(J)Qh , using CRF(zQt).
(7) Z?
(J)Qh ?
Z?
(J)Qh ?
z?
(J)Qh .Output: Z?
(J)Qh ?
predicted annotations for the held-out set of queries.Figure 2: Algorithm for performing joint query annotation.Note that this formulation of joint query anno-tation can be viewed as a stacked classification, inwhich a second, more effective, classifier is trainedusing the labels inferred by the first classifier as fea-tures.
Stacked classifiers were recently shown to bean efficient and effective strategy for structured clas-sification in NLP (Nivre and McDonald, 2008; Mar-tins et al, 2008).4 Independent Query AnnotationsWhile the joint annotation method proposed in Sec-tion 3 is general enough to be applied to any set ofindependent query annotations, in this work we fo-cus on two previously proposed independent anno-tation methods based on either the query itself, orthe top sentences retrieved in response to the query(Bendersky et al, 2010).
The main benefits of thesetwo annotation methods are that they can be easilyimplemented using standard software tools, do notrequire any labeled data, and provide reasonable an-notation accuracy.
Next, we briefly describe thesetwo independent annotation methods.4.1 Query-based estimationThe most straightforward way to estimate the con-ditional probabilities in Eq.
1 is using the query it-self.
To make the estimation feasible, Bendersky etal.
(2010) take a bag-of-words approach, and assumeindependence between both the query terms and thecorresponding annotation symbols.
Thus, the inde-pentent annotations in Eq.
1 are given byz?
(QRY )Q = argmax(?1,...,?n)?i?(1,...,n)p(?i|qi).
(3)Following Bendersky et al (2010) we use a largen-gram corpus (Brants and Franz, 2006) to estimatep(?i|qi) for annotating the query with capitalizationand segmentation mark-up, and a standard POS tag-ger1 for part-of-speech tagging of the query.4.2 PRF-based estimationGiven a short, often ungrammatical query, it is hardto accurately estimate the conditional probability inEq.
1 using the query terms alone.
For instance, akeyword query hawaiian falls, which refers to a lo-cation, is inaccurately interpreted by a standard POStagger as a noun-verb pair.
On the other hand, givena sentence from a corpus that is relevant to the querysuch as ?Hawaiian Falls is a family-friendly water-park?, the word ?falls?
is correctly identified by astandard POS tagger as a proper noun.Accordingly, the document corpus can be boot-strapped in order to better estimate the query anno-tation.
To this end, Bendersky et al (2010) employthe pseudo-relevance feedback (PRF) ?
a methodthat has a long record of success in IR for tasks suchas query expansion (Buckley, 1995; Lavrenko andCroft, 2001).In the most general form, given the set of all re-trievable sentences r in the corpus C one can derivep(zQ|Q) =?r?Cp(zQ|r)p(r|Q).Since for most sentences the conditional proba-bility of relevance to the query p(r|Q) is vanish-ingly small, the above can be closely approximated1http://crftagger.sourceforge.net/105by considering only a set of sentences R, retrievedat top-k positions in response to the query Q. Thisyieldsp(zQ|Q) ?
?r?Rp(zQ|r)p(r|Q).Intuitively, the equation above models the query asa mixture of top-k retrieved sentences, where eachsentence is weighted by its relevance to the query.Furthermore, to make the estimation of the condi-tional probability p(zQ|r) feasible, it is assumed thatthe symbols ?i in the annotation sequence are in-dependent, given a sentence r. Note that this as-sumption differs from the independence assumptionin Eq.
3, since here the annotation symbols are notindependent given the query Q.Accordingly, the PRF-based estimate for indepen-dent annotations in Eq.
1 isz?
(PRF )Q = argmax(?1,...,?n)?r?R?i?(1,...,n)p(?i|r)p(r|Q).
(4)Following Bendersky et al (2010), an estimate ofp(?i|r) is a smoothed estimator that combines theinformation from the retrieved sentence r with theinformation about unigrams (for capitalization andPOS tagging) and bigrams (for segmentation) froma large n-gram corpus (Brants and Franz, 2006).5 Related WorkIn recent years, linguistic annotation of searchqueries has been receiving increasing attention as animportant step toward better query processing andunderstanding.
The literature on query annotationincludes query segmentation (Bergsma and Wang,2007; Jones et al, 2006; Guo et al, 2008; Ha-gen et al, 2010; Hagen et al, 2011; Tan and Peng,2008), part-of-speech and semantic tagging (Barr etal., 2008; Manshadi and Li, 2009; Li, 2010), named-entity recognition (Guo et al, 2009; Lu et al, 2009;Shen et al, 2008; Pas?ca, 2007), abbreviation disam-biguation (Wei et al, 2008) and stopword detection(Lo et al, 2005; Jones and Fain, 2003).Most of the previous work on query annotationfocuses on performing a particular annotation task(e.g., segmentation or POS tagging) in isolation.However, these annotations are often related, andthus we take a joint annotation approach, whichcombines several independent annotations to im-prove the overall annotation accuracy.
A similar ap-proach was recently proposed by Guo et al (2008).There are several key differences, however, betweenthe work presented here and their work.First, Guo et al (2008) focus on query refine-ment (spelling corrections, word splitting, etc.)
ofshort keyword queries.
Instead, we are interestedin annotation of queries of different types, includ-ing verbose natural language queries.
While thereis an overlap between query refinement and annota-tion, the focus of the latter is on providing linguisticinformation about existing queries (after initial re-finement has been performed).
Such information isespecially important for more verbose and gramat-ically complex queries.
In addition, while all themethods proposed by Guo et al (2008) require largeamounts of training data (thousands of training ex-amples), our joint annotation method can be effec-tively trained with a minimal human labeling effort(several hundred training examples).An additional research area which is relevant tothis paper is the work on joint structure model-ing (Finkel and Manning, 2009; Toutanova et al,2008) and stacked classification (Nivre and Mc-Donald, 2008; Martins et al, 2008) in natural lan-guage processing.
These approaches have beenshown to be successful for tasks such as parsing andnamed entity recognition in newswire data (Finkeland Manning, 2009) or semantic role labeling in thePenn Treebank and Brown corpus (Toutanova et al,2008).
Similarly to this work in NLP, we demon-strate that a joint approach for modeling the linguis-tic query structure can also be beneficial for IR ap-plications.6 Experiments6.1 Experimental SetupFor evaluating the performance of our query anno-tation methods, we use a random sample of 250queries2 from a search log.
This sample is manuallylabeled with three annotations: capitalization, POStags, and segmentation, according to the descriptionof these annotations in Figure 1.
In this set of 250queries, there are 93 questions, 96 phrases contain-2The annotations are available athttp://ciir.cs.umass.edu/?bemike/data.html106CAPF1 (% impr) MQA (% impr)i-QRY 0.641 (-/-) 0.779 (-/-)i-PRF 0.711?
(+10.9/-) 0.811?
(+4.1/-)j-QRY 0.620?
(-3.3/-12.8) 0.805?
(+3.3/-0.7)j-PRF 0.718?
(+12.0/+0.9) 0.840??(+7.8/+3.6)TAGAcc.
(% impr) MQA (% impr)i-QRY 0.893 (-/-) 0.878 (-/-)i-PRF 0.916?
(+2.6/-) 0.914?
(+4.1/-)j-QRY 0.913?
(+2.2/-0.3) 0.912?
(+3.9/-0.2)j-PRF 0.924?
(+3.5/+0.9) 0.922?
(+5.0/+0.9)SEGF1 (% impr) MQA (% impr)i-QRY 0.694 (-/-) 0.672 (-/-)i-PRF 0.753?
(+8.5/-) 0.710?
(+5.7/-)j-QRY 0.817??
(+17.7/+8.5) 0.803??
(+19.5/+13.1)j-PRF 0.819??
(+18.0/+8.8) 0.803??
(+19.5/+13.1)Table 1: Summary of query annotation performance forcapitalization (CAP), POS tagging (TAG) and segmenta-tion.
Numbers in parentheses indicate % of improvementover the i-QRY and i-PRF baselines, respectively.
Bestresult per measure and annotation is boldfaced.
?
and ?denote statistically significant differences with i-QRY andi-PRF, respectively.ing a verb, and 61 short keyword queries (Figure 1contains a single example of each of these types).In order to test the effectiveness of the joint queryannotation, we compare four methods.
In the firsttwo methods, i-QRY and i-PRF the three annotationsare done independently.
Method i-QRY is based onz?
(QRY )Q estimator (Eq.
3).
Method i-PRF is basedon the z?
(PRF )Q estimator (Eq.
4).The next two methods, j-QRY and j-PRF, are jointannotation methods, which perform a joint optimiza-tion over the entire set of annotations, as describedin the algorithm in Figure 2. j-QRY and j-PRF differin their choice of the initial independent annotationset Z?
(I)Q in line (1) of the algorithm (see Figure 2).j-QRY uses only the annotations performed by i-QRY (3 initial independent annotation estimates),while j-PRF combines the annotations performed byi-QRY with the annotations performed by i-PRF (6initial annotation estimates).
The CRF model train-ing in line (6) of the algorithm is implemented usingCRF++ toolkit3.3http://crfpp.sourceforge.net/The performance of the joint annotation methodsis estimated using a 10-fold cross-validation.
In or-der to test the statistical significance of improve-ments attained by the proposed methods we use atwo-sided Fisher?s randomization test with 20,000permutations.
Results with p-value < 0.05 are con-sidered statistically significant.For reporting the performance of our meth-ods we use two measures.
The first measure isclassification-oriented ?
treating the annotation de-cision for each query term as a classification.
In caseof capitalization and segmentation annotations thesedecisions are binary and we compute the precisionand recall metrics, and report F1 ?
their harmonicmean.
In case of POS tagging, the decisions areternary, and hence we report the classification ac-curacy.We also report an additional, IR-oriented perfor-mance measure.
As is typical in IR, we proposemeasuring the performance of the annotation meth-ods on a per-query basis, to verify that the methodshave uniform impact across queries.
Accordingly,we report the mean of classification accuracies perquery (MQA).
Formally, MQA is computed as?Ni=1 accQiN ,where accQi is the classification accuracy for queryQi, and N is the number of queries.The empirical evaluation is conducted as follows.In Section 6.2, we discuss the general performanceof the four annotation techniques, and compare theeffectiveness of independent and joint annotations.In Section 6.3, we analyze the performance of theindependent and joint annotation methods by querytype.
In Section 6.4, we compare the difficultyof performing query annotations for different querytypes.
Finally, in Section 6.5, we compare the effec-tiveness of the proposed joint annotation for querysegmentation with the existing query segmentationmethods.6.2 General EvaluationTable 1 shows the summary of the performance ofthe two independent and two joint annotation meth-ods for the entire set of 250 queries.
For independentmethods, we see that i-PRF outperforms i-QRY for107CAP Verbal Phrases Questions KeywordsF1 MQA F1 MQA F1 MQAi-PRF 0.750 0.862 0.590 0.839 0.784 0.687j-PRF 0.687?
(-8.4%) 0.839?
(-2.7%) 0.671?
(+13.7%) 0.913?
(+8.8%) 0.814 (+3.8%) 0.732?
(+6.6%)TAG Verbal Phrases Questions KeywordsAcc.
MQA Acc.
MQA Acc.
MQAi-PRF 0.908 0.908 0.932 0.935 0.880 0.890j-PRF 0.904 (-0.4%) 0.906 (-0.2%) 0.951?
(+2.1%) 0.953?
(+1.9%) 0.893 (+1.5%) 0.900 (+1.1%)SEG Verbal Phrases Questions KeywordsF1 MQA F1 MQA F1 MQAi-PRF 0.751 0.700 0.740 0.700 0.816 0.747j-PRF 0.772 (+2.8%) 0.742?
(+6.0%) 0.858?
(+15.9%) 0.838?
(+19.7%) 0.844 (+3.4%) 0.853?
(+14.2%)Table 2: Detailed analysis of the query annotation performance for capitalization (CAP), POS tagging (TAG) andsegmentation by query type.
Numbers in parentheses indicate % of improvement over the i-PRF baseline.
Best resultper measure and annotation is boldfaced.
?
denotes statistically significant differences with i-PRF.all annotation types, using both performance mea-sures.In Table 1, we can also observe that the joint anno-tation methods are, in all cases, better than the cor-responding independent ones.
The highest improve-ments are attained by j-PRF, which always demon-strates the best performance both in terms of F1 andMQA.
These results attest to both the importance ofdoing a joint optimization over the entire set of an-notations and to the robustness of the initial annota-tions done by the i-PRF method.
In all but one case,the j-PRF method, which uses these annotations asfeatures, outperforms the j-QRY method that onlyuses the annotation done by i-QRY .The most significant improvements as a result ofjoint annotation are observed for the segmentationtask.
In this task, joint annotation achieves close to20% improvement in MQA over the i-QRY method,and more than 10% improvement in MQA over the i-PRF method.
These improvements indicate that thesegmentation decisions are strongly guided by cap-italization and POS tagging.
We also note that, incase of segmentation, the differences in performancebetween the two joint annotation methods, j-QRYand j-PRF, are not significant, indicating that thecontext of additional annotations in j-QRY makes upfor the lack of more robust pseudo-relevance feed-back based features.We also note that the lowest performance im-provement as a result of joint annotation is evi-denced for POS tagging.
The improvements of jointannotation method j-PRF over the i-PRF method areless than 1%, and are not statistically significant.This is not surprising, since the standard POS tag-gers often already use bigrams and capitalization attraining time, and do not acquire much additionalinformation from other annotations.6.3 Evaluation by Query TypeTable 2 presents a detailed analysis of the perfor-mance of the best independent (i-PRF) and joint (j-PRF) annotation methods by the three query typesused for evaluation: verbal phrases, questions andkeyword queries.
From the analysis in Table 2, wenote that the contribution of joint annotation variessignificantly across query types.
For instance, us-ing j-PRF always leads to statistically significant im-provements over the i-PRF baseline for questions.On the other hand, it is either statistically indistin-guishable, or even significantly worse (in the case ofcapitalization) than the i-PRF baseline for the verbalphrases.Table 2 also demonstrates that joint annotationhas a different impact on various annotations for thesame query type.
For instance, j-PRF has a signif-icant positive effect on capitalization and segmen-tation for keyword queries, but only marginally im-proves the POS tagging.
Similarly, for the verbalphrases, j-PRF has a significant positive effect onlyfor the segmentation annotation.These variances in the performance of the j-PRFmethod point to the differences in the structure be-108Annotation Performance by Query TypeF1Verbal Phrases Questions Keyword Queries6065707580859095100CAPSEGTAGFigure 3: Comparative performance (in terms of F1 forcapitalization and segmentation and accuracy for POStagging) of the j-PRF method on the three query types.tween the query types.
While dependence betweenthe annotations plays an important role for questionand keyword queries, which often share a commongrammatical structure, this dependence is less use-ful for verbal phrases, which have a more diverselinguistic structure.
Accordingly, a more in-depthinvestigation of the linguistic structure of the verbalphrase queries is an interesting direction for futurework.6.4 Annotation DifficultyRecall that in our experiments, out of the overall 250annotated queries, there are 96 verbal phrases, 93questions and 61 keyword queries.
Figure 3 shows aplot that contrasts the relative performance for thesethree query types of our best-performing joint an-notation method, j-PRF, on capitalization, POS tag-ging and segmentation annotation tasks.
Next, weanalyze the performance profiles for the annotationtasks shown in Figure 3.For the capitalization task, the performance of j-PRF on verbal phrases and questions is similar, withthe difference below 3%.
The performance for key-word queries is much higher ?
with improvementover 20% compared to either of the other two types.We attribute this increase to both a larger numberof positive examples in the short keyword queries(a higher percentage of terms in keyword queries iscapitalized) and their simpler syntactic structure (ad-SEG F1 MQASEG-1 0.768 0.754SEG-2 0.824?
0.787?j-PRF 0.819?
(+6.7%/-0.6%) 0.803?
(+6.5%/+2.1%)Table 3: Comparison of the segmentation performanceof the j-PRF method to two state-of-the-art segmentationmethods.
Numbers in parentheses indicate % of improve-ment over the SEG-1 and SEG-2 baselines respectively.Best result per measure and annotation is boldfaced.
?denotes statistically significant differences with SEG-1.jacent terms in these queries are likely to have thesame case).For the segmentation task, the performance is atits best for the question and keyword queries, and atits worst (with a drop of 11%) for the verbal phrases.We hypothesize that this is due to the fact that ques-tion queries and keyword queries tend to have repet-itive structures, while the grammatical structure forverbose queries is much more diverse.For the tagging task, the performance profile is re-versed, compared to the other two tasks ?
the per-formance is at its worst for keyword queries, sincetheir grammatical structure significantly differs fromthe grammatical structure of sentences in news arti-cles, on which the POS tagger is trained.
For ques-tion queries the performance is the best (6% increaseover the keyword queries), since they resemble sen-tences encountered in traditional corpora.It is important to note that the results reported inFigure 3 are based on training the joint annotationmodel on all available queries with 10-fold cross-validation.
We might get different profiles if a sep-arate annotation model was trained for each querytype.
In our case, however, the number of queriesfrom each type is not sufficient to train a reliablemodel.
We leave the investigation of separate train-ing of joint annotation models by query type to fu-ture work.6.5 Additional ComparisonsIn order to further evaluate the proposed joint an-notation method, j-PRF, in this section we compareits performance to other query annotation methodspreviously reported in the literature.
Unfortunately,there is not much published work on query capi-talization and query POS tagging that goes beyondthe simple query-based methods described in Sec-109tion 4.1.
The published work on the more advancedmethods usually requires access to large amounts ofproprietary user data such as query logs and clicks(Barr et al, 2008; Guo et al, 2008; Guo et al, 2009).Therefore, in this section we focus on recent workon query segmentation (Bergsma and Wang, 2007;Hagen et al, 2010).
We compare the segmentationeffectiveness of our best performing method, j-PRF,to that of these query segmentation methods.The first method, SEG-1, was first proposed byHagen et al (2010).
It is currently the most effectivepublicly disclosed unsupervised query segmentationmethod.
SEG-1 method requires an access to a largeweb n-gram corpus (Brants and Franz, 2006).
Theoptimal segmentation for query Q, S?Q, is then ob-tained usingS?Q = argmaxS?SQ?s?S,|s|>1|s||s|count(s),where SQ is the set of all possible query segmenta-tions, S is a possible segmentation, s is a segmentin S, and count(s) is the frequency of s in the webn-gram corpus.The second method, SEG-2, is based on a success-ful supervised segmentation method, which was firstproposed by Bergsma and Wang (2007).
SEG-2 em-ploys a large set of features, and is pre-trained on thequery collection described by Bergsma and Wang(2007).
The features used by the SEG-2 method aredescribed by Bendersky et al (2009), and include,among others, n-gram frequencies in a sample of aquery log, web corpus and Wikipedia titles.Table 3 demonstrates the comparison between thej-PRF, SEG-1 and SEG-2 methods.
When com-pared to the SEG-1 baseline, j-PRF is significantlymore effective, even though it only employs bigramcounts (see Eq.
4), instead of the high-order n-gramsused by SEG-1, for computing the score of a seg-mentation.
This results underscores the benefit ofjoint annotation, which leverages capitalization andPOS tagging to improve the quality of the segmen-tation.When compared to the SEG-2 baseline, j-PRFand SEG-2 are statistically indistinguishable.
SEG-2posits a slightly better F1, while j-PRF has a betterMQA.
This result demonstrates that the segmenta-tion produced by the j-PRF method is as effective asthe segmentation produced by the current supervisedstate-of-the-art segmentation methods, which em-ploy external data sources and high-order n-grams.The benefit of the j-PRF method compared to theSEG-2 method, is that, simultaneously with the seg-mentation, it produces several additional query an-notations (in this case, capitalization and POS tag-ging), eliminating the need to construct separate se-quence classifiers for each annotation.7 ConclusionsIn this paper, we have investigated a joint approachfor annotating search queries with linguistic struc-tures, including capitalization, POS tags and seg-mentation.
To this end, we proposed a probabilis-tic approach for performing joint query annotationthat takes into account the dependencies that existbetween the different annotation types.Our experimental findings over a range of queriesfrom a web search log unequivocally point to the su-periority of the joint annotation methods over bothquery-based and pseudo-relevance feedback basedindependent annotation methods.
These findings in-dicate that the different annotations are mutually-dependent.We are encouraged by the success of our jointquery annotation technique, and intend to pursue theinvestigation of its utility for IR applications.
In thefuture, we intend to research the use of joint queryannotations for additional IR tasks, e.g., for con-structing better query formulations for ranking al-gorithms.8 AcknowledgmentThis work was supported in part by the Center for In-telligent Information Retrieval and in part by ARRANSF IIS-9014442.
Any opinions, findings and con-clusions or recommendations expressed in this ma-terial are those of the authors and do not necessarilyreflect those of the sponsor.110ReferencesNiranjan Balasubramanian and James Allan.
2009.
Syn-tactic query models for restatement retrieval.
In Proc.of SPIRE, pages 143?155.Cory Barr, Rosie Jones, and Moira Regelson.
2008.
Thelinguistic structure of english web-search queries.
InProc.
of EMNLP, pages 1021?1030.Michael Bendersky and W. Bruce Croft.
2009.
Analysisof long queries in a large scale search log.
In Proc.
ofWorkshop on Web Search Click Data, pages 8?14.Michael Bendersky, David Smith, and W. Bruce Croft.2009.
Two-stage query segmentation for informationretrieval.
In Proc.
of SIGIR, pages 810?811.Michael Bendersky, W. Bruce Croft, and David A. Smith.2010.
Structural annotation of search queries usingpseudo-relevance feedback.
In Proc.
of CIKM, pages1537?1540.Shane Bergsma and Qin I. Wang.
2007.
Learning nounphrase query segmentation.
In Proc.
of EMNLP, pages819?826.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramVersion 1.Chris Buckley.
1995.
Automatic query expansion usingSMART.
In Proc.
of TREC-3, pages 69?80.Jenny R. Finkel and Christopher D. Manning.
2009.Joint parsing and named entity recognition.
In Proc.of NAACL, pages 326?334.Jiafeng Guo, Gu Xu, Hang Li, and Xueqi Cheng.
2008.A unified and discriminative model for query refine-ment.
In Proc.
of SIGIR, pages 379?386.Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li.
2009.Named entity recognition in query.
In Proc.
of SIGIR,pages 267?274.Matthias Hagen, Martin Potthast, Benno Stein, andChristof Braeutigam.
2010.
The power of naive querysegmentation.
In Proc.
of SIGIR, pages 797?798.Matthias Hagen, Martin Potthast, Benno Stein, andChristof Bra?utigam.
2011.
Query segmentation re-visited.
In Proc.
of WWW, pages 97?106.Rosie Jones and Daniel C. Fain.
2003.
Query word dele-tion prediction.
In Proc.
of SIGIR, pages 435?436.Rosie Jones, Benjamin Rey, Omid Madani, and WileyGreiner.
2006.
Generating query substitutions.
InProc.
of WWW, pages 387?396.Giridhar Kumaran and James Allan.
2007.
A case forshorter queries, and helping user create them.
In Proc.of NAACL, pages 220?227.Giridhar Kumaran and Vitor R. Carvalho.
2009.
Re-ducing long queries using query quality predictors.
InProc.
of SIGIR, pages 564?571.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proc.
of ICML, pages 282?289.Victor Lavrenko and W. Bruce Croft.
2001.
Relevancebased language models.
In Proc.
of SIGIR, pages 120?127.Matthew Lease.
2007.
Natural language processing forinformation retrieval: the time is ripe (again).
In Pro-ceedings of PIKM.Xiao Li.
2010.
Understanding the semantic structure ofnoun phrase queries.
In Proc.
of ACL, pages 1337?1345, Morristown, NJ, USA.Rachel T. Lo, Ben He, and Iadh Ounis.
2005.
Auto-matically building a stopword list for an informationretrieval system.
In Proc.
of DIR.Yumao Lu, Fuchun Peng, Gilad Mishne, Xing Wei, andBenoit Dumoulin.
2009.
Improving Web search rel-evance with semantic features.
In Proc.
of EMNLP,pages 648?657.Mehdi Manshadi and Xiao Li.
2009.
Semantic Taggingof Web Search Queries.
In Proc.
of ACL, pages 861?869.Andre?
F. T. Martins, Dipanjan Das, Noah A. Smith, andEric P. Xing.
2008.
Stacking dependency parsers.
InProc.
of EMNLP, pages 157?166.Joakim Nivre and Ryan McDonald.
2008.
Integratinggraph-based and transition-based dependency parsers.In Proc.
of ACL, pages 950?958.Marius Pas?ca.
2007.
Weakly-supervised discovery ofnamed entities using web search queries.
In Proc.
ofCIKM, pages 683?690.Dou Shen, Toby Walkery, Zijian Zhengy, Qiang Yangz,and Ying Li.
2008.
Personal name classification inweb queries.
In Proc.
of WSDM, pages 149?158.Bin Tan and Fuchun Peng.
2008.
Unsupervised querysegmentation using generative language models andWikipedia.
In Proc.
of WWW, pages 347?356.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A global joint model for semanticrole labeling.
Computational Linguistics, 34:161?191,June.Xing Wei, Fuchun Peng, and Benoit Dumoulin.
2008.Analyzing web text association to disambiguate abbre-viation in queries.
In Proc.
of SIGIR, pages 751?752.111
