An Experiment in Hybrid Dictionaryand Statistical Sentence AlignmentNigel Collier, Kenji Ono and Hideki HirakawaCommunication and Information Systems LaboratoriesResearch and Development Center, Toshiba Corporation1 Komukai Toshiba-cho, Kawasaki-shi, Kanagawa 210-85S2, Japan{nigel, ono, hirakawa}@eel, rdc.
toshiba, co. j pAbstractThe task of aligning sentences in parallel corpora oftwo languages has been well studied using pure sta-tistical or linguistic models.
We developed a linguis-tic method based on lexical matching with a bilin-gual dictionary and two statistical methods basedon sentence length ratios and sentence offset prob-abilities.
This paper seeks to further our knowl-edge of the alignment ask by comparing the per-formance of the alignment models when used sepa-rately and together, i.e.
as a hybrid system.
Ourresults show that for our English-Japanese corpus ofnewspaper articles, the hybrid system using lexicalmatching and sentence l ngth ratios outperforms thepure methods.1 IntroductionThere have been many approaches proposed to solvethe problem of aligning corresponding sentences inparallel corpora.
With a few notable exceptionshowever, much of this work has focussed on ei-ther corpora containing European language pairs orclean-parallel corpora where there is little reformat-ting.
In our work we have focussed on developinga method for robust matching of English-Japanesesentences, based primarily on lexical matching.
Themethod combines tatistical information from bytelength ratios.
We show in this paper that this hybridmodel is more effective than its constituent partsused separately.The task of sentence alignment is a critical firststep in many automatic applications involving theanalysis of bilingual texts such as extraction of bilin-gum vocabulary, extraction of translation templates,word sense disambiguation, word and phrase align-ment, and extraction of parameters for statisticaltranslation models.
Many software products whichaid human translators now contain sentence align-ment tools as an aid to speeding up editing and ter-minology searching.Various methods have been developed for sentencealignment which we can categorise as either lexicalsuch as (Chen, 1993), based on a large-scale bilin-gual lexicon; statistical such as (Brown et al, 1991)(Church, 1993)(Gale and Church, 1903)(Kay andRSsheheisen, 1993), based on distributional regular-ities of words or byte-length ratios and possibly in-ducing a bilingual exicon as a by-product, or hybridsuch as (Utsuro et al, 1994) (Wu, 1994), based onsome combination of the other two.
Neither of thepure approaches i entirely satisfactory for the fol-lowing reasons:?
Text volume limits the usefulness of statisticalapproaches.
We would often like to be able toalign small amounts of text, or texts from var-ious domains which do not share the same sta-tistical properties.?
Bilingual dictionary coverage limitations meanthat we will often encounter problems establish-ing a correspondence in non-general domains.?
Dictionary-based approaches are founded on anassumption of lexicul correspondence b tweenlanguage pairs.
We cannot always rely on thisfor non-cognate language pairs, such as Englishand Japanese.?
Texts are often heavily reformatted in trans-lation, so we cannot assume that the corpuswill be clean, i.e.
contain many one-to-one sen-tence mappings.
In this case statistical methodswhich rely on structure correspondence such asbyte-length ratios may not perform well.These factors suggest that some hybrid methodmay give us the best combination of coverage andaccuracy when we have a variety of text domains,text sizes and language pairs.
In this paper we seekto fill a gap in our understanding and to show howthe various components of the hybrid method influ-ence the quality of sentence alignment for Japaneseand English newspaper articles.2 Bilingual Sentence AlignmentThe task of sentence alignment is to match corre-sponding sentences in a text from One language tosentences in a translation of that text in anotherlanguage.
Of particular interest to us is the ap-plication to Asian language pairs.
Previous stud-ies such as (Fung and Wu, 1994) have commented268that methods developed for Indo-European languagepairs using alphabetic haracters have not addressedimportant issues which occur with European-Asianlanguage pairs.
For example, the language pairs areunlikely to be cognates, and they may place sentenceboundaries at different points in the text.
It has alsobeen suggested by (Wu, 1994) that sentence lengthratio correlations may arise partly out of historiccognate-based relationships between Indo-Europeanlanguages.
Methods which perform well for Indo-European language pairs have therefore been foundto be less effective for non-Indo-European languagepairs.In our experiments the languages we use are En-glish (source) and Japanese (translation).
Althoughin our corpus (described below) we observe that, ingeneral, sentences correspond one-to-one we mustalso consider multiple sentence correspondences awell as one-to-zero correspondences.
These cases aresummarised below.1.
1:1 The sentences match one-to-one.2.
l:n One English sentence matches to more thanone Japanese sentence.3.
m:l More than one English sentence matches otone Japanese sentence.4.
m:n More than one English sentence matches tomore than one Japanese sentence.5.
m:0 The English sentence/s have no correspond-ing Japanese sentence.6.
0:n The Japanese sentence/s have no corre-sponding English sentence.In the case of l:n, m:l and m:n correspondences,translation has involved some reformatting and themeaning correspondence is no longer solely at thesentence level.
Ideally we would like smaller units oftext to match because it is easier later on to establishword alignment correspondences.
In the worst caseof multiple correspondence, the translation is spreadacross multiple non-consecutive s ntences.3 CorpusOur primarily motivation is knowledge acquisitionfor machine translation and consequently we are in-terested to acquire vocabulary and other bilingualknowledge which will be useful for users of such sys-tems.
Recently there has been a move towards In-ternet page translation and we consider that one in-teresting domain for users is international news.The bilingual corpus we use in our experiments imade from Reuter news articles which were trans-lated by the Gakken translation agency from En-glish into Japanese 1 .
The translations are quite lit-eral and the contents cover international news forI The corpus was generously made available to us by specialarrangement with Gakkenthe period February 1995 to December 1996.
Wecurrently have over 20,000 articles (approximately47 Mb).
From this corpus we randomly chose 50article pairs and aligned them by hand using a hu-man bilingual checker to form a judgement set.
Thejudgement set consists of 380 English sentences and453 Japanese sentences.
On average ach Englisharticle has 8 lines and each Japanese article 9 lines.The articles themselves form a boundary withinwhich to align constituent sentences.
The corpusis quite well behaved.
We observe many 1:1 corre-spondences, but also a large proportion of 1:2 and1:3 correspondences a  well as reorderings.
Omis-sions seem to be quite rare, so we didn't see manym:0 or 0:n correspondences.An example news article is shown in Figure 1which highlights several interesting points.
Al-though the news article texts are clean and inmachine-tractable format we still found that it wasa significant challenge to reliably identify sentenceboundaries.
A simple illustration of this is shown bythe first Japanese line J1 which usually correspondsto the first two English lines E1 and E2.
This isa result of our general-purpose ntence segmenta-tion algorithm which has difficulty separating theJapanese title from the first sentence.Sentences usually corresponded linearly in ourcorpus, with few reorderings, so the major chal-lenge was to identify multiple correspondences andzero correspondences.
We can see an example of azero correspondence as E5 has no translation in theJapanese text.
A l:n correspondence is shown by E7aligning to both J5 and J6.4 A l ignment  Mode lsIn our investigation we examined the performance ofthree different matching models (lexical matching,byte-length ratios and offset probabilities).
The ba-sic models incorporate dynamic programming to findthe least cost alignment path over the set of Englishand Japanese sentences.
Cost being determined bythe model's cores.
The alignment space includes allpossible combinations of multiple matches upto andincluding 3:3 alignments.
The basic models are nowoutlined below.4.1 Mode l  1: Lexical  vector  matchingThe lexical approach is perhaps the most robust foraligning texts in cognate language pairs, or wherethere is a large amount of reformatting in trans-lation.
It has also been shown to be particularlysuccessful within the vector space model in multilin-gual information retrieval tasks, e.g.
(Collier et al,1998a),(Collier et al, 1998b), for aligning texts innon-cognate languages at the article level.The major limitation with lexical matching isclearly the assumption of lexical correspondence -269El.
Taiwan ruling party sees power struggle in ChinaE2.
TAIPEI , Feb 9 ( Reuter ) - Taiwan's ruling Nationalist Party said a struggle to succeed DengXiaoping as China's most powerful man may have already begun.E3.
"Once Deng Xiaoping dies, a high tier power struggle among the Chinese communists is in-evitable," a Nationalist Party report said.E4.
China and Taiwan have been rivals since the Nationalists lost the Chinese civil war in 1949 andfled to Taiwan.E5.
Both Beijing and Taipei sometimes portray each other in an unfavourable light.E6.
The report said that the position of Deng's chosen successor, President 3iang Zemin, may havebeen subtly undermined of late.E7.
It based its opinion on the fact that two heavyweight political figures have recently used thephrase the "solid central collective leadership and its core" instead of the accepted "collective leader-ship centred on Jiang Zemin" to describe the current leadership structure.E8.
"Such a sensitive statement should not be an unintentional mistake ...E9.
Does this mean the power struggle has gradually surfaced while Deng Xiaoping is still alive ?,"said the report , distributed to journalists.El0.
"At least the information sends a warning signal that the 'core of Jiang' has encountered somesubtle changes," it added .31.
~ '~ l~ l~.~l~:~,  ~P\[~:,~.-,i~-~~'a~t.~l~j~'~.
"~/~:i~.fl~.~:/t'~'H:\]~ \[ '~ 9 13 ~ -I' 9--\] ~'~'~J2.
~ l~: ,  ~~.~6t : i~.~L, /~_@~?~"e,  r l -e ) .
, j ,~~,  ~,~-~.~.~e,  ~,~@~,,J3.
q~l~-~i '~t~,  1~7)" ,  1 9 4 9~l : -q~I~l~,~e)~l : - I~(  , ~ '~ I : -~-9~A~,  t i~~lz~b,5oJs.
~?~I~: ,  ~~t2 .
, , L~,  ~L~?~-e ,  ~ t ?
~ ~ _ ~ ,  "~:~-~J6..: h.~ el:t .  "
i~- :v ,~ t: ?
.5  q~:~l~J"  ~ ~,~' 5 ~z~t~h."?
~ I::oFigure 1: Example English-Japanese news article pairwhich is particularly weak for English and Asianlanguage pairs where structural and semantic dif-ferences mean that transfer often occurs at a levelabove the lexicon.
This is a motivation for incor-porating statistics into the alignment process, butin the initial stage we wanted to treat pure lexicalmatching as our baseline performance.We translated each Japanese sentence into En-glish using dictionary term lookup.
Each Japanesecontent word was assigned a list of possible Englishtranslations and these were used to match againstthe normalised English words in the English sen-tences.
For an English text segment E and the En-glish term list produced from a Japanese text seg-ment J,  which we considered to be a possible unitof correspondence, we calculated similarity usingDice's coefficient score shown in Equation 1.
Thisrather simple measure captures frequency, but notpositional information, q_\]m weights of words aretheir frequencies inside a sentence.2fEj  (1) Dice(E, .1) - fE + f Jwhere lea  is the number of lexical items whichmatch in E and J,  fE is tile number of lexical itemsin E and f j  is the number of lexical items in J.The translation lists for each Japanese word are useddisjunctively, so if one word in the list matches thenwe do not consider the other terms in the list.
Inthis way we maintain term independence.270Our transfer dictionary contained some 79,000 En-glish words in full form together with the list oftranslations in Japanese.
Of these English wordssome 14,000 were proper nouns which were directlyrelevant to the vocabulary typically found in interna-tional news stories.
Additionally we perform lexicalnormalisation before calculating the matching scoreand remove function words with a stop list.4.2 Mode l  2: Byte - length  ra t iosFor Asian language pairs we cannot rely entirelyon dictionary term matching.
Moreover, algorithmswhich rely on matching cognates cannot be appliedeasily to English and some Asian language.
Wewere motivated by statistical alignment models suchas (Gale and Church, 1991) to investigate whetherbyte-length probabilities could improve or replacethe lexical matching based method.
The underlyingassumption is that characters in an English sentenceare responsible for generating some fraction of eachcharacter in the corresponding Japanese sentence.We derived a probability density function by mak-ing the assumption that English .and Japanese sen-tence length ratios are normally distributed.
Theparameters required for the model are the mean, pand variance, ~, which we calculated from a trainingset of 450 hand-aligned sentences.
These are thenentered into Equation 2 to find the probability ofany two sentences (or combinations of sentences formultiple alignments) being in an alignment relationgiven that they have a length ratio of x.The byte length ratios were calculated as thelength of the Japanese text segment divided by thelength of the English text segment.
So in this way wecan incorporate multiple sentence correspondencesinto our model.
Byte lengths for English sentencesare calculated according to the number of non-whitespace characters, with a weighting of 1 for each validcharacter including punctuation.
For the Japanesetext we counted 2 for each non-white space char-acter.
White spaces were treated as having length0.
The ratios for the training set are shown as ahistogram in Figure 2 and seem to support the as-sumption of a normal distribution.The resulting normal curve with ~r = 0.33 and/1 = 0.76 is given in Figure 3, and this can then beused to provide a probability score for any Englishand Japanese sentence being aligned in the Reuters'corpus.Clearly it is not enough simply to assume that oursentence pair lengths follow the normal distribution.We tested this assumption using a standard test, byplotting the ordered ratio scores against the valuescalculated for the normal curve in Figure 3.
If the~,o?
-4  .
s  2 -1I l l , .o ~ 4 S eFigure 2: Sentence l ngth ratios in training set1.41 .a1O.So.e0.40.2o.. 4 + + 3 4 5i*~1 I I  +1Figure 3: Sentence l , gth ratio normal curvedistribution is indeed normal then we would expectthe plot in Figure 4 to yi,?ld a straight line.
We cansee that this is the case l:',r most, although not all,of the observed scores.Although the curve in Figure 4 shows that ourtraining set deviated from the normal distribution ati !io.m 0.,, o.,, o .
,  +,2,,o , .2 , .
.
,+ , .
, ,  , .
?Figure 4: Sentence l ngth ratio normal check curve271I -2~ tOodl i 0 -6  -4Figure 5: Sentence offsets in training setthe extremes we nevertheless proceeded to continuewith our simulations using this model consideringthat the deviations occured at the extreme nds ofthe distribution where relatively few samples werefound.
The weakness of this assumption howeverdoes add extra evidence to doubts which have beenraised, e.g.
(Wu, 1994), about whether the byte-length model by itself can perform well.4.3 Mode l  3: Offset rat iosWe calculated the offsets in the sentence indexes forEnglish and Japanese sentences in an alignment re-lation in the hand-aligned training set.
An offsetdifference was calculated as the Japanese sentenceindex minus the English sentence index within abilingual news article pair.
The values are shownas a histogram in Figure 5.As with the byte-length ratio model, we startedfrom an assumption that sentence correspondenceoffsets were normally distributed.
We then cal-culated the mean and variance for our sample setshown in Figure 5 and used this to form a normalprobability density function (where a = 0.50 and/J - 1.45) shown in Figure 6.The test for normality of the distribution is thesame as for byte-length ratios and is given in Figure7.
We can see that the assumption of normality isparticularly weak for the offset distribution, but weare motivated to see whether such a noisy probabil-ity model can improve alignment results.5 Exper imentsIn this section we present he results of using dif-ferent combinations of the three basic methods.
Wecombined the basic methods to make hybrid modelssimply by taking the product of the scores for themodels given above.
Although this is simplistic wefelt that in the first stage of our investigation it wasbetter to give equal weight to each method.The seven methods we tested are coded as follows:0.11O.
l~t5 .2  0 SD 4mFigure 6: Sentence offsets normal curvef"mOFigure 7: Sentence offscts normal check curveDICE: sentence alignmelit using bilingual dictionaryand Dice's coefficient scores; LEN: sentence align-ment using sentence length ratios; OFFSET: sen-tence alignment using offs,:t probabilities.We performed sentence alignment on our test setof 380 English sentences and 453 Japanese sentences.The results are shown as recall and precision whichwe define in the usual way as follows:recall = #correctly matched sentences retrieved#matched sentences in the test collection (a)precision = #correctly matched sentences retrievedmatched sentences retrieved(4)The results are shown in Table 1.
We see that thebaseline method using lexical matching with a bilin-gual lexicon, DICE, performs better than either ofthe two statistical methods LEN or OFFSET usedseparately.
Offset probabilities in particular per-formed poorly showing tltat we cannot expect thecorrectly matching sentence to appear constantly in272the same highest probability position.-Method Rec.
(%) Pr.
(%)DICE (baseline) 84 85LEN 82 83OFFSET 50 57LEN+OFFSET 70 70DICE+LEN 89 87DICE+OFFSET 80 80DICE+LEN+OFFSET 88 85Table 1: Sentence alignment results as recall andprecision.Considering the hybrid methods, we see signifi-cantly that DICE+LEN provides a clearly better e-sult for both recall and precision to either DICE orLEN used separately.
On inspection we found thatDICE by itself could not distinguish clearly betweenmany candidate sentences.
This occured for two rea-sons.1.
As a result of the limited domain in which newsarticles report, there was a strong lexical over-lap between candidate sentences in a news arti-cle.2.
Secondly, where the lexical overlap was poor be-tween the English sentence and the Japanesetranslation, this leads to low DICE scores.The second reason can be attributed to low cov-erage in the bilingual lexicon with the domain ofthe news articles.
If we had set a minimum thresh-old limit for overlap frequency then we would haveruled out many correct matches which were found.In both cases LEN provides a decisive clue and en-ables us to find the correct result more reliably.
Fur-thermore, we found that LEN was particularly ef-fective at identifying multi-sentence orrespondencescompared to DICE, possibly because some sentencesare very small and provide weak evidence for lexi-cal matching, whereas when they are combined withneighbours they provide significant evidence for theLEN model.Using all methods together however inDICE+LEN+OFFSET seems less promising and webelieve that the offset probabilities are not a reliablemodel.
Possibly this is due to lack of data in thetraining stage when we calculated ~ and p, or thedata set may not in fact be normally distributed asindicated by Figure 7.Finally, we noticed that a consistent factor in theEnglish and Japanese text pairs was that the firsttwo lines of the English were always matched to thefirst line of the Japanese.
This was because the En-glish text separated the title and first line, whereasour sentence segmenter could not do this for theJapanese.
This factor was consistent for all the 50article pairs in our test collection and may have ledto a small deterioration i the results, so the figureswe present are the minimum of what we can expectwhen sentence segmentation is performed correctly.6 ConclusionThe assumption that a partial alignment at the wordlevel from lexical correspondences can clearly in-dicate full sentence alignment is flawed when thetexts contain many sentences with similar vocabu-lary.
This is the case with the news stories used inour experiments and even technical vocabulary andproper nouns are not adequate to clearly discrimi-nate between alternative alignment choices becausethe vocabulary range inside the news article is notlarge.
Moreover, the basic assumption of the lexicalapproach, that the coverage of the bilingual dictio-nary is adequate, cannot be relied on if we requirerobustness.
This has shown the need for some hybridmodel.For our corpus of newspaper articles, the hybridmodel has been shown to clearly improve sentencealignment results compared with the pure modelsused separately.
In the future we would like to makeextensions to the lexical model by incorporatingterm weighting methods from information retrievalsuch as inverse document frequency which may helpto identify more important erms for matching.
Inorder to test the generalisability of our method wealso want to extend our investigation to parallel cor-pora in other domains.AcknowledgementsWe would like to thank Reuters and Gakken for al-lowing us to use the corpus of news stories in ourwork.
We are grateful to Miwako Shimazu for handaligning the judgement sct used in the experimentsand to Akira Kumano and Satoshi Kinoshita foruseful discussions.
Finally we would also like ex-press our appreciation to the anonymous reviewersfor their helpful comments.Re ferencesP.
Brown, J. Lai, and R. Mercer.
1991.
Aligning sen-tences in parallel corpora.
In P9th Annual Meetingof the Association for Computational Linguistics,Berkeley, California, USA.S.
Chen.
1993.
Aligning sentences in bilingual cor-pora using lexical information.
31st Annual Meet-ing of the Association of Computational Linguis-tics, Ohio, USA, 22-26 June.K.
Church.
1993.
Char_align: a program for align-ing parallel texts at the character level.
In 31stAnnual Meeting of the Association for Computa-tional Linguistics, Ohio, USA, pages 1-8, 22-26June.273N.
Collier, H. Hirakawa, and A. Kumano.
1998a.Creating a noisy parallel corpus from newswirearticles using multi-lingual information retrieval.Trans.
of Information Processing Society of Japan(to appear).N.
Collier, H. Hirakawa, and A. Kumano.
1998b.Machine translation vs. dictionary term transla-tion - a comparison for English-Japanese newsarticle alignment.
In Proceedings of COLING-ACL'98, University of Montreal, Canada, 10thAugust.P.
Fung and D. Wu.
1994.
Statistical augmenta-tion of a Chinese machine readable dictionary.
InSecond Annual Workshop on Very Large Corpora,pages 69-85, August.W.
Gale and K. Church.
1991.
A program for align-ing sentences in bilingual corpora.
In Proceedingsof the 29th Annual Conference ofthe Associationfor Computational Linguistics (ACL-91}, Berke-ley, California, pages 177-184.W.
Gale and K. Church.
1993.
A program for align-ing sentences in a bilingual corpora.
Computa-tional Linguistics, 19(1):75-102.M.
Kay and M. Rbshcheisen.
1993.
Text-translationalignment.
Computational Linguistics, 19:121-142.T.
Utsuro, H. Ikeda, M. Yamane, Y. Matsumoto,and N. Nagao.
1994.
Bilingual text match-ing using bilingual dictionary and statistics.
InCOLING-94, 15th International Conference, Ky-oto, Japan, volume 2, August 5-9.D.
Wu.
1994.
Aligning a parallel English-Chinesecorpus statistically with lexical criteria.
In 3endAnnual Meeting of the Association for Computa-tional Linguistics, New Mexico, USA, pages 80-87, June 27-30.274
