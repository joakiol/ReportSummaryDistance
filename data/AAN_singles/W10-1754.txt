Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 354?359,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsTESLA: Translation Evaluation of Sentences withLinear-programming-based AnalysisChang Liu1 and Daniel Dahlmeier2 and Hwee Tou Ng1,21Department of Computer Science, National University of Singapore2NUS Graduate School for Integrative Sciences and Engineering{liuchan1,danielhe,nght}@comp.nus.edu.sgAbstractWe present TESLA-M and TESLA, twonovel automatic machine translation eval-uation metrics with state-of-the-art perfor-mances.
TESLA-M builds on the suc-cess of METEOR and MaxSim, but em-ploys a more expressive linear program-ming framework.
TESLA further exploitsparallel texts to build a shallow seman-tic representation.
We evaluate both onthe WMT 2009 shared evaluation task andshow that they outperform all participatingsystems in most tasks.1 IntroductionIn recent years, many machine translation (MT)evaluation metrics have been proposed, exploitingvarying amounts of linguistic resources.Heavyweight linguistic approaches includingRTE (Pado et al, 2009) and ULC (Gim?nez andM?rquez, 2008) performed the best in the WMT2009 shared evaluation task.
They exploit an ex-tensive array of linguistic features such as parsing,semantic role labeling, textual entailment, and dis-course representation, which may also limit theirpractical applications.Lightweight linguistic approaches such as ME-TEOR (Banerjee and Lavie, 2005), MaxSim(Chan and Ng, 2008), wpF and wpBleu (Popovic?and Ney, 2009) exploit a limited range of linguis-tic information that is relatively cheap to acquireand to compute, including lemmatization, part-of-speech (POS) tagging, and synonym dictionaries.Non-linguistic approaches include BLEU (Pap-ineni et al, 2002) and its variants, TER (Snover etal., 2006), among others.
They operate purely atthe surface word level and no linguistic resourcesare required.
Although still very popular with MTresearchers, they have generally shown inferiorperformances than the linguistic approaches.We believe that the lightweight linguistic ap-proaches are a good compromise given the currentstate of computational linguistics research and re-sources.
In this paper, we devise TESLA-M andTESLA, two lightweight approaches to MT eval-uation.
Specifically: (1) the core features are F-measures derived by matching bags of N-grams;(2) both recall and precision are considered, withmore emphasis on recall; and (3) WordNet syn-onyms feature prominently.The main novelty of TESLA-M compared toMETEOR and MaxSim is that we match the N-grams under a very expressive linear programmingframework, which allows us to assign weights tothe N-grams.
This is in contrast to the greedy ap-proach ofMETEOR, and the more restrictive max-imum bipartite matching formulation of MaxSim.In addition, we present a heavier versionTESLA, which combines the features using a lin-ear model trained on development data, makingit easy to exploit features not on the same scale,and leaving open the possibility of domain adapta-tion.
It also exploits parallel texts of the target lan-guage with other languages as a shallow semanticrepresentation, which allows us to model phrasesynonyms and idioms.
In contrast, METEOR andMaxSim are capable of processing only word syn-onyms from WordNet.The rest of this paper is organized as follows.Section 2 gives a high level overview of the eval-uation task.
Sections 3 and 4 describe TESLA-Mand TESLA, respectively.
Section 5 presents ex-perimental results in the setting of the WMT 2009shared evaluation task.
Finally, Section 6 con-cludes the paper.2 OverviewWe consider the task of evaluating machine trans-lation systems in the direction of translating thesource language to the target language.
Given areference translation and a system translation, the354goal of an automatic machine translation evalua-tion algorithm such as TESLA(-M) is to output ascore predicting the quality of the system transla-tion.
Neither TESLA-M nor TESLA requires thesource text, but as additional linguistic resources,TESLAmakes use of phrase tables generated fromparallel texts of the target language and other lan-guages, which we refer to as pivot languages.
Thesource language may or may not be one of thepivot languages.3 TESLA-MThis section describes TESLA-M, the lighterone among the two metrics.
At the highestlevel, TESLA-M is the arithmetic average of F-measures between bags of N-grams (BNGs).
ABNG is a multiset of weighted N-grams.
Math-ematically, a BNG B consists of tuples (bi, bWi ),where each bi is an N-gram and bWi is a posi-tive real number representing its weight.
In thesimplest case, a BNG contains every N-gram in atranslated sentence, and the weights are just thecounts of the respective N-grams.
However, toemphasize the content words over the functionwords, we discount the weight of an N-gram bya factor of 0.1 for every function word in the N-gram.
We decide whether a word is a functionword based on its POS tag.In TESLA-M, the BNGs are extracted in the tar-get language, so we call them bags of target lan-guage N-grams (BTNGs).3.1 Similarity functionsTo match two BNGs, we first need a similaritymeasure between N-grams.
In this section, wedefine the similarity measures used in our exper-iments.We adopt the similarity measure from MaxSimas sms.
For unigrams x and y,?
If lemma(x) = lemma(y), then sms = 1.?
Otherwise, leta = I(synsets(x) overlap with synsets(y))b = I(POS(x) = POS(y))where I(?)
is the indicator function, thensms = (a + b)/2.The synsets are obtained by querying WordNet(Fellbaum, 1998).
For languages other than En-glish, a synonym dictionary is used instead.We define two other similarity functions be-tween unigrams:slem(x, y) = I(lemma(x) = lemma(y))spos(x, y) = I(POS(x) = POS(y))All the three unigram similarity functions general-ize to N-grams in the same way.
For two N-gramsx = x1,2,...,n and y = y1,2,...,n,s(x, y) ={0 if ?i, s(xi, yi) = 01n?ni=1 s(xi, yi) otherwise3.2 Matching two BNGsNow we describe the procedure of matching twoBNGs.
We take as input the following:1.
Two BNGs, X and Y .
The ith entry in Xis xi and has weight xWi (analogously for yjand yWj ).2.
A similarity measure, s, that gives a similar-ity score between any two entries in the rangeof 0 to 1.Intuitively, we wish to align the entries of the twoBNGs in a way that maximizes the overall simi-larity.
As translations often contain one-to-manyor many-to-many alignments, we allow one entryto split its weight among multiple alignments.
Anexample matching problem is shown in Figure 1a,where the weight of each node is shown, alongwith the similarity for each edge.
Edges with asimilarity of zero are not shown.
The solution tothe matching problem is shown in Figure 1b, andthe overall similarity is 0.5 ?
1.0 + 0.5 ?
0.6 +1.0 ?
0.2 + 1.0 ?
0.1 = 1.1.Mathematically, we formulate this as a (real-valued) linear programming problem1.
The vari-ables are the allocated weights for the edgesw(xi, yj) ?i, jWe maximize?i,js(xi, yj)w(xi, yj)subject tow(xi, yj) ?
0 ?i, j?jw(xi, yj) ?
xWi ?i?iw(xi, yj) ?
yWj ?j1While integer linear programming is NP-complete, real-valued linear programming can be solved efficiently.355w=1.0 w=0.8 w=0.2 w=0.1w=1.0 w=0.8 w=0.1.2s=0.5 s=1.0s=0.5 s=1.0(a) The matching problemw=1.0 w=0.8 w=0.2 w=0.1w=1.0 w=0.8 w=0.1.2w=1.0 w=0.2w=0.s w=0.1(b) The solutionFigure 1: A BNG matching problemThe value of the objective function is the overallsimilarity S. Assuming X is the reference and Yis the system translation, we havePrecision =S?j yWjRecall =S?i xWiThe F-measure is derived from the precision andthe recall:F =Precision ?
Recall??
Precision + (1 ?
?)
?
RecallIn this work, we set ?
= 0.8, following MaxSim.The value gives more importance to the recall thanthe precision.3.3 ScoringThe TESLA-M sentence-level score for a refer-ence and a system translation is the arithmetic av-erage of the BTNG F-measures for unigrams, bi-grams, and trigrams based on similarity functionssms and spos.
We thus have 3?
2 = 6 features forTESLA-M.We can compute a system-level score for a ma-chine translation system by averaging its sentence-level scores over the complete test set.3.4 ReductionWhen every xWi and yWj is 1, the linear program-ming problem proposed above reduces toweightedbipartite matching.
This is a well known result;see for example, Cormen et al (2001) for details.This is the formalism of MaxSim, which precludesthe use of fractional weights.If the similarity function is binary-valuedand transitive, such as slem and spos, thenwe can use a much simpler and faster greedymatching procedure: the best match is simply?g min(?xi=gxWi ,?yi=gyWi ).4 TESLAUnlike the simple arithmetic average used inTESLA-M, TESLA uses a general linear com-bination of three types of features: BTNG F-measures as in TESLA-M, F-measures betweenbags of N-grams in each of the pivot languages,called bags of pivot language N-grams (BPNGs),and normalized language model scores of the sys-tem translation, defined as 1n logP , where n isthe length of the translation, and P the languagemodel probability.
The method of training the lin-ear model depends on the development data.
Inthe case of WMT, the development data is in theform of manual rankings, so we train SVM rank(Joachims, 2006) on these instances to build thelinear model.
In other scenarios, some form of re-gression can be more appropriate.The rest of this section focuses on the genera-tion of the BPNGs.
Their matching is done in thesame way as described for BTNGs in the previoussection.4.1 Phrase level semantic representationGiven a sentence-aligned bitext between the targetlanguage and a pivot language, we can align thetext at the word level using well known tools suchas GIZA++ (Och and Ney, 2003) or the Berkeleyaligner (Liang et al, 2006; Haghighi et al, 2009).We observe that the distribution of alignedphrases in a pivot language can serve as a se-mantic representation of a target language phrase.That is, if two target language phrases are oftenaligned to the same pivot language phrase, thenthey can be inferred to be similar in meaning.Similar observations have been made by previousresearchers (Bannard and Callison-Burch, 2005;Callison-Burch et al, 2006; Snover et al, 2009).We note here two differences from WordNetsynonyms: (1) the relationship is not restricted tothe word level only, and (2) the relationship is notbinary.
The degree of similarity can be measuredby the percentage of overlap between the seman-tic representations.
For example, at the word level,356the phrases good morning and hello are unrelatedeven with a synonym dictionary, but they both veryoften align to the same French phrase bonjour, andwe conclude they are semantically related to a highdegree.4.2 Segmenting a sentence into phrasesTo extend the concept of this semantic represen-tation of phrases to sentences, we segment a sen-tence in the target language into phrases.
Given aphrase table, we can approximate the probabilityof a phrase p by:Pr(p) =N(p)?p?
N(p?
)(1)where N(?)
is the count of a phrase in the phrasetable.
We then define the likelihood of seg-menting a sentence S into a sequence of phrases(p1, p2, .
.
.
, pn) by:Pr(p1, p2, .
.
.
, pn|S) =1Z(S)n?i=1Pr(pi) (2)where Z(S) is a normalizing constant.
The seg-mentation of S that maximizes the probability canbe determined efficiently using a dynamic pro-gramming algorithm.
The formula has a strongpreference for longer phrases, as every Pr(p) isa small fraction.
To deal with out-of-vocabulary(OOV) words, we allow any single word w to beconsidered a phrase, and if N(w) = 0, we setN(w) = 0.5 instead.4.3 BPNGs as sentence level semanticrepresentationSimply merging the phrase-level semantic rep-resentation is insufficient to produce a sensiblesentence-level semantic representation.
As an ex-ample, we consider two target language (English)sentences segmented as follows:1.
||| Hello , ||| Querrien ||| .
|||2.
||| Morning , sir .
|||A naive comparison of the bags of aligned pivotlanguage (French) phrases would likely concludethat the two sentences are completely unrelated,as the bags of aligned phrases are likely to becompletely disjoint.
We tackle this problem byconstructing a confusion network representationof the aligned phrases, as shown in Figures 2 andw=1.=082s25202s2520881252252Figure 2: A confusion network as a semantic rep-resentationw=1.=082s25=108222Figure 3: A degenerate confusion network as a se-mantic representation3.
A confusion network is a compact representa-tion of a potentially exponentially large number ofweighted and likely malformed French sentences.We can collect the N-gram statistics of this ensem-ble of French sentences efficiently from the confu-sion network representation.
For example, the tri-gram Bonjour , Querrien 2 would receive a weightof 0.9 ?
1.0 = 0.9 in Figure 2.
As with BTNGs,we discount the weight of an N-gram by a factorof 0.1 for every function word in the N-gram, soas to place more emphasis on the content words.The collection of all such N-grams and theircorresponding weights forms the BPNG of a sen-tence.
The reference and system BPNGs are thenmatched using the algorithm outlined in Section3.2.4.4 ScoringThe TESLA sentence-level score is a linear com-bination of (1) BTNG F-measures for unigrams,bigrams, and trigrams based on similarity func-tions sms and spos, (2) BPNG F-measures for un-igrams, bigrams, and trigrams based on similar-ity functions slem and spos for each pivot lan-guage, and (3) normalized language model scores.In this work, we use two language models.
Wethus have 3 ?
2 features from the BTNGs, 3 ?2 ?
#pivot languages features from the BPNGs, and2 features from the language models.
Again, wecan compute system-level scores by averaging thesentence-level scores.5 Experiments5.1 SetupWe test our metrics in the setting of the WMT2009 evaluation task (Callison-Burch et al, 2009).The manual judgments from WMT 2008 are used2Note that the N-gram can span more than one segment.357as the development data and the metric is evalu-ated on WMT 2009 manual judgments with re-spect to two criteria: sentence level consistencyand system level correlation.The sentence level consistency is defined as thepercentage of correctly predicted pairs among allthe manually judged pairs.
Pairs judged as tiesby humans are excluded from the evaluation.
Thesystem level correlation is defined as the averageSpearman?s rank correlation coefficient across alltranslation tracks.5.2 Pre-processingWe POS tag and lemmatize the texts using the fol-lowing tools: for English, OpenNLP POS-tagger3and WordNet lemmatizer; for French and German,TreeTagger4; for Spanish, the FreeLing toolkit(Atserias et al, 2006); and for Czech, the Morcemorphological tagger5.For German, we additionally perform nouncompound splitting.
For each noun, we choose thesplit that maximizes the geometric mean of the fre-quency counts of its parts, following the method in(Koehn and Knight, 2003):maxn,p1,p2,...,pn[n?i=1N(pi)] 1nThe resulting compound split sentence is then POStagged and lemmatized.Finally, we remove all non-alphanumeric tokensfrom the text in all languages.
To generate the lan-guage model features, we train SRILM (Stolcke,2002) trigram models with modified Kneser-Neydiscounting on the supplied monolingual Europarland news commentary texts.We build phrase tables from the supplied newscommentary bitexts.
Word alignments are pro-duced by the Berkeley aligner.
The widely usedphrase extraction heuristic in (Koehn et al, 2003)is used to extract phrase pairs and phrases of up to4 words are collected.5.3 Into-English taskFor each of the BNG features, we generate threescores, for unigrams, bigrams, and trigrams re-spectively.
For BPNGs, we generate one suchtriple for each of the four pivot languages supplied,namely Czech, French, German, and Spanish.3opennlp.sourceforge.net4www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger5ufal.mff.cuni.cz/morce/index.phpSystemcorrelationSentenceconsistencyTESLA 0.8993 0.6324TESLA-M 0.8718 0.6097ulc 0.83 0.63maxsim 0.80 0.62meteor-0.6 0.72 0.50Table 1: Into-English task on WMT 2009 dataTable 1 compares the scores of TESLA andTESLA-M against three participants in WMT2009 under identical settings6: ULC (a heavy-weight linguistic approach with the best per-formance in WMT 2009), MaxSim, and ME-TEOR.
The results show that TESLA outperformsall these systems by a substantial margin, andTESLA-M is very competitive too.5.4 Out-of-English taskA synonym dictionary is required for target lan-guages other than English.
We use the freely avail-able Wiktionary dictionary7 for each language.For Spanish, we additionally use the SpanishWordNet, a component of FreeLing.Only one pivot language (English) is used forthe BPNG.
For the English-Czech task, we onlyhave one language model instead of two, as theEuroparl language model is not available.Tables 2 and 3 show the sentence-level consis-tency and system-level correlation respectively ofTESLA and TESLA-M against the best reportedresults in WMT 2009 under identical setting.
Theresults show that both TESLA and TESLA-Mgive very competitive performances.
Interestingly,TESLA and TESLA-M obtain similar scores in theout-of-English task.
This could be because we useonly one pivot language (English), compared tofour in the into-English task.
We plan to inves-tigate this phenomenon in our future work.6 ConclusionThis paper describes TESLA-M and TESLA.
Ourmain contributions are: (1) we generalize thebipartite matching formalism of MaxSim into amore expressive linear programming framework;6The original WMT09 report contained erroneous results.The scores here are the corrected results released after publi-cation.7www.wiktionary.org358en-fr en-de en-es en-cz OverallTESLA 0.6828 0.5734 0.5940 0.5519 0.5796TESLA-M 0.6390 0.5890 0.5927 0.5656 0.5847wcd6p4er 0.67 0.58 0.61 0.59 0.60wpF 0.66 0.60 0.61 n/a 0.61terp 0.62 0.50 0.54 0.31 0.43Table 2: Out-of-English task sentence-level con-sistency on WMT 2009 dataen-fr en-de en-es en-cz OverallTESLA 0.8529 0.7857 0.7272 0.3141 0.6700TESLA-M 0.9294 0.8571 0.7909 0.0857 0.6657wcd6p4er -0.89 0.54 -0.45 -0.1 -0.22wpF 0.90 -0.06 0.58 n/a n/aterp -0.89 0.03 -0.58 -0.40 -0.46Table 3: Out-of-English task system-level correla-tion on WMT 2009 data(2) we exploit parallel texts to create a shallow se-mantic representation of the sentences; and (3) weshow that they outperform all participants in mostWMT 2009 shared evaluation tasks.AcknowledgmentsThis research was done for CSIDM Project No.CSIDM-200804 partially funded by a grant fromthe National Research Foundation (NRF) ad-ministered by the Media Development Authority(MDA) of Singapore.ReferencesJ.
Atserias, B. Casas, E. Comelles, M. Gonz?lez,L.
Padr?, and M. Padr?.
2006.
Freeling 1.3: Syn-tactic and semantic services in an open-source NLPlibrary.
In Proceedings of LREC.S.
Banerjee and A. Lavie.
2005.
METEOR: An auto-matic metric for MT evaluation with improved cor-relation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Eval-uation Measures for Machine Translation and/orSummarization.C.
Bannard and C. Callison-Burch.
2005.
Paraphras-ing with bilingual parallel corpora.
In Proceedingsof ACL.C.
Callison-Burch, P. Koehn, and M. Osborne.
2006.Improved statistical machine translation using para-phrases.
In Proceedings of HLT-NAACL.C.
Callison-Burch, P. Koehn, C. Monz, andJ.
Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of WMT.Y.S.
Chan and H.T.
Ng.
2008.
MAXSIM: A maximumsimilarity metric for machine translation evaluation.In Proceedings of ACL.T.
Cormen, C.E.
Leiserson, R.L.
Rivest, and C. Stein,2001.
Introduction to Algorithms.
MIT Press, Cam-bridge, MA.C.
Fellbaum, editor.
1998.
WordNet: An electroniclexical database.
MIT Press, Cambridge, MA.J.
Gim?nez and L. M?rquez.
2008.
A smorgasbord offeatures for automatic MT evaluation.
In Proceed-ings of the Third WMT.A.
Haghighi, J. Blitzer, J. DeNero, and D. Klein.
2009.Better word alignments with supervised ITG mod-els.
In Proceedings of ACL-IJCNLP.T.
Joachims.
2006.
Training linear svms in linear time.In Proceedings of KDD.P.
Koehn and K. Knight.
2003.
Empirical methods forcompound splitting.
In Proceedings of EACL.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proceedings of HLT-NAACL.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In Proceedings of HLT-NAACL.F.J.
Och and N. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1).S.
Pado, M. Galley, D. Jurafsky, and C.D.
Man-ning.
2009.
Robust machine translation evaluationwith entailment features.
In Proceedings of ACL-IJCNLP.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: A method for automatic evaluation of ma-chine translation.
In Proceedings of ACL.M.
Popovic?
and H. Ney.
2009.
Syntax-oriented eval-uation measures for machine translation output.
InProceedings of WMT.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proceedings ofAMTA.M.
Snover, N. Madnani, B. Dorr, and R. Schwartz.2009.
Fluency, adequacy, or HTER?
Exploring dif-ferent human judgments with a tunable MT metric.In Proceedings of WMT.A.
Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of ICSLP.359
