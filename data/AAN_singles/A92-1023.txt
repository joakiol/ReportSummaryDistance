A Practical Methodologyfor the Evaluation of Spoken Language SystemsSean Boisen and Madeleine Bates*Bol t  Beranek  and Newman,  Inc. (BBN)10 Mou l ton  Street, Cambr idge  MA 02138 USAsbo isen@bbn.com,  bates@bbn.com1 IntroductionA meaningful evaluation methodology can advance thestate-of-the-art by encouraging mature, practical applicationsrather than "toy" implementations.
Evaluation is also cru-cial to assessing competing claims and identifying promis-ing technical approaches.
While work in speech recognition(SR) has a history of evaluation methodologies that per-mit comparison among various systems, until recently nomethodology existed for either developers of natural an-guage (NL) interfaces or researchers in speech understanding(SU) to evaluate and compare the systems they developed.Recently considerable progress has been made by a num-ber of groups involved in the DARPA Spoken LanguageSystems (SLS) program to agree on a methodology forcomparative evaluation of SLS systems, and that method-ology has been put into practice several times in com-parative tests of several SLS systems.
These evaluationsare probably the only NL evaluations other than the seriesof Message Understanding Conferences (Sundheim, 1989;Sundheim, 1991) to have been developed and used by agroup of researchers at different sites, although several ex-cellent workshops have been held to study some of theseproblems (Palmer et al, 1989; Neal et al, 1991).This paper describes a practical "black-box" methodologyfor automatic evaluation of question-answering NL systems.While each new application domain will require some devel-opment of special resources, the heart of the methodology isdomain-independent, and it can be used with either speechor text input.
The particular characteristics of the approachare described in the following section: subsequent sectionspresent its implementation in the DARPA SLS community,and some problems and directions for future development.2 The  Eva luat ion  F ramework2.1 Characteristics of the MethodologyThe goal of this research as been to produce awell-defined,meaningful evaluation methodology which is*The work reported here was supported by the Advanced Re-search Projects Agency and was monitored by the Off'ice of NavalResearch under Contract No.
00014-89-C-0008.
The views andconclusions contained in this document are those of the authorsand should not be interpreted as necessarily representing the offi-cial policies, either expressed or implied, of the Defense AdvancedResearch Projects Agency or the United States Government.?
automatic, to enable evaluation over large quantities ofdatabased on an objective assessment of the understandingcapabilities of a NL system (rather than its user inter-face, portability, speed, etc.)?
capable of application to a wide variety of NL systemsand approaches?
suitable for blind testing?
as non-intrusive as possible on the system being eval-uated (to decrease the costs of evaluation)?
domain independent.The systems are assumed to be front ends to an interactivedatabase query system, implemented in a particular commondomain.The methodology can be described as "black box" in thaithere is no attempt o evaluate the internal representations(syntactic, semantic, etc.)
of a system.
Instead, only thecontent of an answer elrieved from the database is evalu-ated: if the answer is correct, it is assumed that the systemunderstood the query correctly.
Comparing answers has thepractical advantage ofbeing a simple way to give widely var.ied systems acommon basis for comparison.
Although somerecent work has suggested promising approaches (Black e,al., 1991), system-internal representations are hard to com.pare, or even impossible in some cases where System X hmno level of representation corresponding to System Y's.
Iis easy, however, to define a simple common language fo~representing answers (see Appendix A), and easy to ma~system-specific representations i to this common language.This methodology has been successfully applied in thecontext of cross-site blind tests, where the evaluation i:based on input which the system has never seen beforeThis type of evaluation leaves out many other important aspects of a system, such as the user interface, or the utilit:(or speed) of performing a particular task with a system thaincludes a NL component (work by Tennant (1981), Bate:and Rettig (1988), and Neal et al (1991) addresses some othese other factors).Examples below will be taken from the current DARPtSLS application, the Airline Travel Information Systen(ATIS).
This is a database of flights with information othe aircraft, stops and connections, meals, etc.162teNL"meaning"ApplicationInterface I DB CommandsScore (Right, Wrong, No Answer)SLSAnswerv \[!!!iiiiiiiii!#:iiii:#~ii~ii~ii~!~ii~i~iiiii!
!iiiiiiii Data\] AnswerPreparationFigure 1: The evaluation process2.2 Evaluation Architecture and Common ResourcesWe assume an evaluation architecture like that in Figure 1.The shaded components are common resources of the eval-uation, and are not part of the system(s) being evaluated.Specifically, it is assumed there is a common database whichall systems use in producing answers, which defines both thedata tuples (rows in tables) and the data types for elementsof these tuples (string, integer, etc.
).Queries relevant to the database are collected under con-ditions as realistic as possible (see 2.4).
Answers to thecorpus of queries must be provided, expressed in a commonstandard format (Common Answer Specification, or CAS):one such format is exemplified in Appendix A.
Some por-tion of these pairs of queries and answers is then set asideas a test corpus, and the remainder is provided as trainingmaterial.In practice, it has also proved useful to include in thetraining data the database query expression (for example, anSQL expression) which was used to produce the referenceanswer: this often makes it possible for system developersto understand what was expected for a query, even if theanswer is empty or otherwise limited in content.2.2.1 Agreeing on MeaningWhile the pairing of queries with answers provides thetraining and test corpora, these must be augmented by com-mon agreement as to how queries should be answered.
Inpractice, agreeing on the meaning of queries has been oneof the hardest tasks.
The issues are often extremely subtle,and interact with the structure and content of the databasein sometimes unexpected ways.As an example of the problem, consider the followingrequest to an airline information system:List the direct f l ights fromBoston to Dal las that servemeals.It seems straightforward, but should this include flightsthat might stop in Chicago without making a connectionthere?
Should it include flights that serve a snack, since asnack is not considered by some people to be a full meal?Without some common agreement, many systems wouldproduce very different answers for the same questions, allof them equally right according to each system's own defi-nitions of the terms, but not amenable to automatic inter-system comparison.
To implement this methodology forsuch a domain, therefore, it is necessary to stipulate themeaning of potentiMly ambiguous terms such as "mid-day","meals" , "the fare of a flight".
The current list of such"principles of interpretation" for the ATIS domain containsabout 60 specifications, including things like:?
which tables and fields in the database identify the ma-jor entities in the domain (flights, aircraft, fares, etc.)?
how to interpret fare expressions like "one-way fare","the cheapest fare", "excursion fare", etc.?
which cities are to be considered "near" an airport.Some other examples from the current principles of inter-pretation are given in Appendix B.1632.2.2 Reference AnswersIt is not enough to agree on meaning of queries in thechosen domain.
It is also necessary to develop a commonunderstanding of precisely what is to be produced as theanswer, or part of the answer, to a question.For example, if a user asks "What is the departure time ofthe earliest flight from San Francisco to Atlanta?
", one sys-tem might reply with a single time and another might replywith that time plus additional columns containing the carrierand flight number, a third system might also include the ar-rival time and the origin and destination airports.
None ofthese answers could be said to be wrong, although one mightargue about the advantages and disadvantages of tersenessand verbosity.While it is technically possible to mandate xactly whichcolumns from the database should be returned for expres-sions, this is not practical: it requires agreement on a muchlarger set of issues, and conflicts with the principle that eval-uation should be as non-intrusive as possible.
Furthermore,it is not strictly necessary: what matters most is not whethera system provided exactly the same data as some referenceanswer, but whether the correct answer is clearly among thedata provided (as long as no incorrect data was returned).For the sake of automatic evaluation, then, a canonicalreference answer (the minimum "right answer") is devel-oped for each evaluable query in the training set.
The con-tent of this reference answer is determined both by domain-independent linguistic principles (Boisen et al, 1989) anddomain-specific stipulation.
The language used to expressthe answers for the ATIS domain is presented in Appendix A.Evaluation using the minimal answer alone makes it pos-sible to exploit the fact that extra fields in an answer axe notpenalized.
For example, the answer(("AA" 152 0920 1015 "BOS .
.
.
.
CHI""SNACK"  ) )could be produced for any of the following queries:?
"When does American Airlines flight 152 leave?"?
"What's the earliest flight from Boston to Chicago?"?
"Does the 9:20 flight to Chicago serve meals?
"and would be counted correct.For the ATIS evaluations, it was necessary to rectify thisproblem without overly constraining what systems can pro-duce as an answer.
The solution arrived at was to havetwo kinds of reference answers for each query: a minimumanswer, which contains the absolute minimum amount ofdata that must be included in an answer for it to be correct,and a maximum answer (that can be automatically derivedfrom the minimum) containing all the "reasonable" fieldsthat might be included, but no completely irrelevant ones.For example, for a question asking about the arrival time ofa flight, the minimum answer would contain the flight 1Dand the arrival time.
The maximum answer would containthe airline name and flight number, but not the meal ser-vice or any fare information.
In order to be counted correct,the answer produced by a system must contain at least thedata in the minimum answer, and no more than the data inthe maximum answer; if additional fields are produced, theanswer is counted as wrong.
This successfully reduced theincentive for systems to overgenerate answers in hope ofgetting credit for answering queries that they did not reallyunderstand.2.2.3 Comparison SoftwareAnother common resource is software to compare the ref-erence answers to those produced by various systems.
1This task is complicated substantially by the fact that thereference answer is intentionally minimal, but the answersupplied by a system may contain extra information, andcannot be assumed to have the columns or rows in the sameorder as the reference answer.
Some intelligence is there-fore needed to determine when two answers match: simpleidentity tests won't work.In the general case, comparing the atomic values in an an-swer expression just means an identity test.
The only excep-tion is real numbers, for which an epsilon test is performed,to deal with round-off discrepancies arising from differenthardware precision.
2 The number of significant digits thatare required to be the same is a parameter of the comparator.Answer comparison at the level of tables require more so-phistication, since column order is ignored, and the answermay include additional columns that are not in the specifica-tion.
Furthermore, those additional columns can mean thatthe answer will include extra whole tuples not present inthe specification.
For example, in the ATIS domain, if theConcorde and Airbus are both aircraft whose type is "JET",they would together contribute only one tuple (row) to thesimple list of aircraft ypes below.
( ( " JET" )( "TURBOPROP" )( "HEL ICOPTER"  )( "AMPHIB IAN")( "PROPELLER") )On  the other hand, if aircraft names were included in thetable, they would each appear, producing a larger number oftuples overall.
( ( "AEROSPAT IALE  CONCORDE"~. "
JET")( "A IRBUS INDUSTRIE  .... JET")( "LOCKHEED L18 8 ELECTRA ....
TURBOPROP"  \].o.
)With answers in the form of tables, the algorithm exploreseach possible mapping from the required columns found inthe reference answer (henceforth REF) to the actual columnsfound in the answer being evaluated (HYP).
(Naturally, theremust be at least as many columns in HYP as in REF, or theanswer is clearly wrong.)
For each such mapping, it reducesHYP according to the mapping, eliminating any duplicatetuples in the reduced table, and then compares REF againstthat reduced table, testing set-equivalence b tween the two.Special provision is made for single element answers, scthat a scalar REF and a HYP which is a table containinga single element are judged to be equivalent That is, ascalar REF will match either a scalar or a single elemenl1The first implementation of this software was by LanceRamshaw (Boisen et al, 19891.
It has since been re-implementexand modified by NIST for the ATIS evaluations.2For the ATIS evaluations, this identity test has been relaxedsomewhat so that, e.g., strings need not have quotes around theirif they do not contain "white space" characters.
See Appendix tfor further details.164table for HYP, and a REF which is a single element ablespecification will also match either kind of answer.For the ATIS evaluations, two extensions were made tothis approach.
A REF may be ambiguous, containing severalsub expressions each of which is itself a REF: in this case,if HYP matches any of the answers in REF, the comparisonsucceeds.
A special answer token (NO_ANSWER) was alsoagreed to, so that when a system can detect hat it doesn'thave enough information, it can report that fact rather thanguessing.
This is based on the assumption that failing toanswer is less serious than answering incorrectly.2.3 Scoring AnswersExpressing results can be almost as complicated as obtainingthem.
Originally it was thought hat a simple "X percentcorrect" measure would be sufficient, however it becameclear that there was a significant difference between givinga wrong answer and giving no answer at all, so the results arenow presented as: Number right, Number wrong, Numbernot answered, Weighted Error Percentage (weighted so thatwrong answers are twice as bad as no answer at all), andScore (100 - weighted error).Whenever numeric measures of understanding are pre-sented, they should in principle be accompanied by somemeasure of the significance and reliability of the metric.
Al-though precise significance tests for this methodology are notyet known, it is clear that "'black box" testing is not a perfectmeasure.
In particular, it is impossible to tell whether a sys-tem got a correct answer for the "right" reason, rather thanthrough chance: this is especially true when the space ofpossible answers is small (yes-no questions are an extremeanswer).
Since more precise measures are much more costly,however, the present methodology has been considered ad-equate for the current state of the art in NL evaluation.Given that current weighted error rates for the DARPAATIS evaluations range from 55%--18%, we can roughlyestimate the confidence interval to be approximately 8%.
3Another source of variation in the scoring metric is the factthat queries taken from different speakers can vary widelyin terms of how easy it is for systems to understand andanswer them correctly.
For example, in the February 1991ATIS evaluations, the performance of BBN's Delphi SLS ontext input from individual speakers ranged from 75% to 10%correcL The word error from speech recognition was alsothe highest for those speakers with the highest NL error rates,suggesting that individual speaker differences can stronglyimpact he results.3Assuming there is some probability of error in each trial(query), the variance in this error rate can be estimated using theformulawhere e is the error rate expressed as a decimal (so 55% error =0.55), and n is the size of the test set.
Taking e = 0.45 (one of thebetter scores from the February 91 ATIS evaluation), and n -- 145,differences in scores greater than 0.08 (8%) have a 95% likelihoodof being significant.2.4 Evaluation Data2.4.1 Collecting DataThe methodology presented above places no a priori re-strictions on how the data itself should be collected.
Forthe ATIS evaluations, everal different methods of data col-lection, including a method called "Wizard scenarios", wereused to collect raw data, both speech and transcribed text(Hemphill, 1990).
This resulted in the collection of a num-ber of human-machine dialogues.
One advantage of this ap-proach is that it produced both the queries and draft answersat the same time.
It also became clear that the languageobtained is very strongly influenced by the particular task,the domain and database being used, the amount and formof data returned to the user, and the type of data collectionmethodology used.
This is still an area of active research inthe DARPA SLS community.2.4.2 Classifying DataTypically, some of the data which is collected is not suit-able as test data, because:?
the queries fall outside the domain or the database queryapplication?
the queries require capabilities beyond strict NL under-standing (for example, very complex inferencing or theuse of large amounts of knowledge outside the domain)?
the queries are overly vague ("Tell me about .
.
.
")It is also possible that phenomena may arise in test datawhich falls outside the agreement on meanings derived fromthe training data (the "principles of interpretation").
Suchqueries should be excluded from the test corpus, since it isnot possible to make a meaningful comparison on answersunless there is prior agreement on precisely what the answershould be.2.4.3 Discourse ContextThe methodology of comparing paired queries and an-swers assumes the query itself contains all the informationnecessary for producing an answer.
This is, of course, oftennot true in spontaneous goal-directed utterances, ince onequery may create a context for another, and the full con-text is required to answer (e.g., "Show me the flights ... ",'Which of THEM .
.
.")
.
Various means of extending thismethodology for evaluating context-dependent queries havebeen proposed, and some of them have been implementedin the ATIS evaluations (Boisen et al (1989), Hirschman etal.
(1990), Bates and Ayuso (1991), Pallett (1991)).3 The  DARPA SLS  Eva luat ionsThe goal of the DARPA Spoken Language Systems programis to further esearch and demonstrate he potential utility ofspeech understanding.
Currently, at least five major sites(AT&T, BBN, CMU, MIT, and SRI) are developing com-plete SLS systems, and another site (Paramax) is integratingits NL component with other speech systems.
Representa-tives from these and other organizations meet regularly todiscuss program goals and to evaluate progress.165This DARPA SLS community formed a committee onevaluation 4, chaired by David Pallett of the National Insti-tute of Standards and Technology (NIST).
The committeewas to develop a methodology for data collection, trainingdata dissemination, and testing for SLS systems under de-velopment.
The first community-wide evaluation using thefirst version of this methodology took place in June, 1990,with subsequent evaluations in February 1991 and February1992.The emphasis of the committee's work has been on au-tomatic evaluation of queries to an air travel informationsystem (ATIS).
Air travel was chosen as an application thatis easy for everyone to understand.
The methodology pre-sented here was originally developed in the context of theneed for SLS evaluation, and has been extended in importantways by the community based on the practical experienceof doing evaluations.As a result of the ATIS evaluations, a body of resourceshas now been compiled and is available through NIST.
Thisincludes the ATIS relational database, a corpus of pairedqueries and answers, protocols for data collection, soft-ware for automatic omparison of answers, the "Principlesof Interpretation" specifying domain-specific meanings ofqueries, and the CAS format (Appendix A is the currentversion).
Interested parties should contact David Pallet ofNIST for more information.
54 Advantages and Limitations of theMethodo logySeveral benefits come from the use of this methodology:?
It forces advance agreement on the meaning of criticalterms and on some information to be included in theanswer.?
It is objective, to the extent hat a method for selectingtestable queries can be defined, and to the extent hatthe agreements mentioned above can be reached.?
It requires less human effort (primarily in the creating ofcanonical examples and answers) than non-automatic,more subjective valuation.
It is thus better suited tolarge test sets.?
It can be easily extended.Most of the weaknesses of this methodology arise from thefact that the answers produced by a database query systemare only an approximation of its understanding capabilities.As with any black-box approach, it may give undue creditto a system that gets the right answer for the wrong reason(i.e., without really understanding the query), although thisshould be mitigated by using larger and more varied test4The primary members of the original committee are: LynBates (BBN), Debbie Dahl (UNISYS), Bill Fisher (NIST), LynetteHirschman (M1T), Bob Moore (SRI), and Rich Stern (CMU).
Suc-cessor committees have also included Jared Bernstein (SRI), KateHunike-Smith (SRI), Patti Price (SRI), Alex Rudnicky (CMU), andJay Wilpon (AT&T).
Many other people have contributed to thework of these committees and their subcommittees.5David Pallet may be contacted at the National Institute ofStandards and Technology, Technology Building, Room A216,Gaithersburg, MD 20899, (301)975-2944.corpora.
It does not distinguish between merely acceptableanswers and very good answers.Another limitation of this approach is that it does notadequately measure the handling of some phenomena, suchas extended ialogues.5 Other  Eva luat ion  MethodologiesThis approach to evaluation shares many characteristicswith the methods used for the DARPA-sponsored MessageUnderstanding Conferences (Sundheim, 1989; Sundheim,1991).
In particular, both approaches are focused on exter-nal (black-box) evaluation of the understanding capabilitiesof systems using input/output pairs, and there are many sim-ilar problems in precisely specifying how NL systems are tosatisfy the application task.Despite these similarities, this methodology probablycomes closer to evaluating the actual understanding capa-bilities of NL systems.
One reason is that the constraintson both input and output are more rigorous.
For databasequery tasks, virtually every word must be correctly under-stood to produce a correct answer: by contrast, much ofthe MUC-3 texts is irrelevant to the application task.
Sincethis methodology focuses on single queries (.perhaps withadditional context), a smaller amount of language is beingexamined in each individual comparison.Similarly, for database query, the database itself implicitlyconstrains the space of possible answers, and each answeris scored as either correct or incorrect.
This differs fromthe MUC evaluations, where an answer template is a com-posite of many bits of information, and is scored along thedimensions of recall, precision, and overgeneration.Rome Laboratory has also sponsored a recent effort todefine another approach to evaluating NL systems (Neal etal., 1991; Walter, 1992).
This methodology is focussed onhuman evaluation of interactive systems, and is a "glass-box" method which looks at the performance of the linguisticcomponents of the system under review.6 Future IssuesThe hottest opic currently facing the SLS community withrespect o evaluation is what to do about dialogues.
Manyof the natural tasks one might do with a database interfaceinvolve extended problem-solving dialogues, but no method-ology exists for evaluating the capabilities of systems at-tempting to engage in dialogues with users.A Common Answer  Spec i f i ca t ion  (CAS)  forthe ATIS Application(Note: this is the official CAS specification for the DARPAATIS evaluations, as distributed by NIST.
It is domain in-dependent, but not necessarily complete: for example, itassumes that the units of any database value are unambigu-ously determined by the database specification.
This wouldnot be sufficient for applications that allowed unit conver-sion, e.g.
"Show me the weight of .
.
. "
where the weightcould be expressed in tons, metric tons, pounds, etc.
Thissort of extension should not affect he ease of automaticallycomparing answer expressions, however.
)166Basic Syntax in BNFanswer , casl  \[ ( casl  OR answer )casl  , scalar-value \[ relation \] NO.ANSWERI no_answerscalar-value , boolean-value I number-value \[stringboolean-value , YES \[ yes \[ TRUE \[ true \[ NOI no I FALSE I falsenumber-value , integer \] real-numberinteger , \[sign\] digit+sign , + -digit ,0  1 \[ 2 \[ 3 { 4 \[ 5 { 6 I 7 I8 9real-number , sign d ig i t+,  digit* \[ d ig i t+,  digit*string , char_except_whitespace+ I " char* "relation , ( tuple* )tuple ~ ( value+ )value , scalar-value \ [N ILStandard BNF notation has been extended to include twoother common devices : "A+" means "one or more A's"and "m*" means "zero or more A's".The formulation given above does not definechar_except_whitespace and char.
All of the standard ASCIIcharacters count as members of char, and all but "whitespace" are counted as char_except_whitespace.
FollowingANSI "C", blanks, horizontal and vertical tabs, newlines,formfeeds, and comments are, collectively, "white space".The only change in the syntax of CAS itself from theprevious version is that now a string may be represented aseither a sequence of characters not containing white spaceor as a sequence of any characters enclosed in quotationmarks.
Note that only non-exponential real numbers areallowed, and that empty tuples are not allowed (but emptyrelations are).Additional Syntactic ConstraintsThe syntactic lasses boolean-value, string, and number-value define the types "boolean", "string", and "'number",respectively.
All the tuples in a relation must have the samenumber of values, and those values must be of the samerespective types (boolean, string, or number).If a token could represent either a string or a number, itwill be taken to be a number; if it could represent either astring or a boolean, it will be taken to be a boolean.
Inter-pretation as a string may be forced by enclosing a token inquotation marks.In a tuple, N IL  as the representation f missing data isallowed as a special case for any value, so a legal answerindicating the costs of ground transportation in Boston wouldbe({"L" 5.00) ("R" nil)("A" nil) ("R" nil))Elementary Rules for CAS ComparisonsString comparison is case-sensitive, but the distinguishedvalues (YES, NO, TRUE, FALSE, NO~ANSWEP~ and NIL)may be written in either upper or lower case.Each indexical position for a value in a tuple (say, the ith)is assumed to represent the same field or variable in all thetuples in a given relation.Answer relations must be derived from the existing re-lations in the database, either by subsetting and combiningrelations or by operations like averaging, summation, etc.In matching an hypothesized (HYP) CAS form with a ref-erence (REF) one, the order of values in the tuples is notimportant; nor is the order of tuples in a relation, nor theorder of alternatives in a CAS form using OR.
The scoringalgorithm will use the re-ordering that maximizes the indi-cated score.
Extra values in a tuple are not counted as errors,but distinct extra tuples in a relation are.
A tuple is not dis-tinct if its values for the fields specified by the REF CASare the same as another tuple in the relation; these duplicatetuples are ignored.
CAS forms that include alternate CAS'sconnected with OR are intended to allow a single HYP formto match any one of several REF CAS forms.
If the HYPCAS form contains alternates, the score is undefined.In comparing two real number values, a tolerance willbe allowed; the default is -t-.01%.
No tolerance is allowedin the comparison of integers.
In comparing two strings,initial and final sub-strings of white space are ignored.
Incomparing boolean values, TRUE and YES are equivalent,as are FALSE and NO.B Some Examples from the Principles ofInterpretation Document for the ATISApplication(Note: these are excerpted from the official Principles of In-terpretation document dated 11/20/91.
The entire documentis comprised of about 60 different points, and is availablefrom David Pallet at NIST.The term "annotator" below refers to a human prepar-ing training or test data by reviewing reference answers toqueries.
)INTERPETING ATIS QUERIES RE THE DATABASE1 General Principles:1.1 Only reasonable interpretations will be used.An annotator or judge must decide if a linguisticallypossible interpretation is reasonable or not.1.2 The context will be used in deciding if an interpretationis reasonable.. .
.1.3 Each interpretation must be expressible as one SQLstatement.At present (11/18/91) a few specified exceptions to thisprinciple are allowed, such as allowing boolean answersfor yes/no questions.1.4 All interpretations meeting the above rules will be usedby the annotators to generate possible reference an-swers.A query is thus ambiguous iff it has two interpretationsthat are fairly represented by distinct SQL expressions.167The reference SQL expression stands as a semantic rep-resentation or logical form.
If a query has two inter-pretations that result in the same SQL, it will not beconsidered ambiguous.
The fact that the two distinctSQL expressions may yield the same answer given thedatabase is immaterial.The annotators must be aware of the usual sources ofambiguity, such as structural mbiguity, exemplified bycases like "the prices of flights, first class, from X toY", in which the attachment of a modifier that can ap-ply to either prices or flights is unclear.
(This shouldbe (ambiguously) interpreted both ways, as both "thefirst-class prices on flights from X to Y" and "the priceson first-class flights from X to Y".)
More generally, ifstructural ambiguities like this could result in different(SQL) interpretations, they must be treated as ambigu-ous.2 Specific Principles:In this arena, certain English expressions have specialmeanings, particularly in terms of the database distributedby TI in the spring of 1990 and revised in November 1990and May 1991.
Here are the ones we have agreed on: (Inthe following, "A.B" refers to field B of table A.
)2.1 Requests for enumeration.A large class of tables in the database have entries thatcan be taken as defining things that can be asked forin a query.
In the answer, each of these things will beidentified by giving a value of the primary key of itstable.
These tables are:Table Name English Term(s)aircraft aircraft, equipmentairline airlineairport airportflight_stop (intermed.)
stops2.2 Flights.Primary Keyaircraft _codeairline_codeairport_codeflight_id, stop_numberhigh_flight_number2.2.1 A flight "between X and Y" means a flight "fromX toY".?
o .2.2.3 A request for a flight's stops will be interpretedas asking for the intermediate stops only, from theflight_stop table.. o .2.3 Fares.2.3.12.3.22.3.32.3.8A "one-way" fare is a fare for whichround_trip_required = "NO".A "round-trip" fare is a fare with a non-null valuefor fare.round_trip_cost.The "cheapest fare" means the lowest one-direction fare.. .
.Questions about fares will always be treated asfares for flights in the maximal answer.2.4.12.4.22.4.32.4.42.9The normal answer to otherwise unmodified"when" queries will be a time of day, not a dateor a duration.The answer to queries like "On what days doesflight X fly" will be a list of days.day.name fi lds.Queries that refer to a time earlier than 1300 hourswithout specifying "a.m." or "p.m." are ambigu-ous and may be interpreted as either.Periods of the day.The following table gives precise interpretationsfor some vague terms referring to time periods.The time intervals given do not include the endpoints.
Items flagged with "*" are in the current(rdb3.3) database interval table.PERIOD BEGIN TIME END TIMEmorning* 0000 1200afternoon* 1200 1800evening* 1800 2200day* 600 1800night* 1800 600early morning* 0000 800. , .o .
,Meaning requests.2.9.1 With the particular exceptions noted below, re-quests for the "meaning" of something will only beinterpretable if that thing is a code with a canneddecoding definition in the database.
In case thecode field is not the key field of the table, infor-marion should be returned for all tuples that matchon the code field.
Here are the things so defined,with the fields containing their decoding:Table Code Field Decoding Fieldaircraft aircraft_code aircraft_descriptionairline airline_code airline_nameairport airport_code airlx~_namecity city_code city_nameclass_of_service booking_class class_descriptioncode_description code description.
, .2.11 Queries that are literally yes-or-no questions are con-sidereal to be ambiguous between interpretation as ayes-or-no question and interpretation asthe correspond-ing wh-question.
For example, "Are there flights fromBoston to Philly?"
may be answered by either aboolean value ("YES/TRUE/NO/FALSE") or a table offlights from Boston to Philadelphia.2.15 When a query refers to an aircraft ype such as "BOE-ING 767", the manufacturer (if one is given) mustmatch the aircraft.manufacturer fi ld and the type maybe matched against either the aircraft.code field or theaircraft.basic_type field, ambiguously.2.16 Utterances whose answers require arithmetic omputa-tion are not now considered to be interpretable; thisdoes not apply to arithmetic omparisons, includingcomputing the maximum or minimum value of a field,or counting elements of a set of tuples.2.4 Times .
.
.
.168ReferencesB.
Ballard.
A Methodology for Evaluating Near-PrototypeNL Processors.
Technical Report OSU--CISRC-TR-81-4,Ohio State University, 1981.M.
Bates and D. Ayuso.
A proposal for incremental dia-logue evaluation.
In Proceedings of the Speech and NaturalLanguage Workshop, San Mateo, California, February 1991.DARPA, Morgan Kaufmann Publishers, Inc.M.
Bates and M. Rettig.
How to choose NL software.
A/Expert, July 1988.E.
Mack, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman,P.
Harrison, D. Hindle, B. Ingria, F. Jelinek, J. Klavens,M.
Liberman, M. Marcus, S. Roukos, B. Santorini, andT.
Strzalkowski.
A procedure for quantitatively comparingthe syntactic overage of English grammars.
In Proceedingsof the Speech and Natural Language Workshop, San Ma-teo, California, February 1991.
DARPA, Morgan KaufmannPublishers, Inc.S.
Boisen, L. Ramshaw, D. Ayuso, and M. Bates.
A proposalfor SLS evaluation.
In Proceedings of the Speech and Nat-ural Language Workshop, San Marco, California, October1989.
DARPA, Morgan Kaufmann Publishers, Inc.DARPA.
Proceedings of the Speech and Natural LanguageWorkshop, San Mateo, California, June 1990.
Morgan Kauf-mann Publishers, Inc.DARPA.
Proceedings of the Speech and Natural LanguageWorkshop, San Mateo, California, February 1991.
MorganKaufmann Publishers, Inc.DARPA.
Proceedings of the Third Message Understand-ing Conference (MUC-3), San Marco, California, May 1991.Morgan Kaufmann Publishers, Inc.DARPA.
Proceedings of the Speech and Natural LanguageWorkshop, San Mateo, California, February 1992.
MorganKaufmann Publishers, Inc.C.
HemphiU.
TI implementation f corpus collection.
InProceedings of the Speech and Natural Language Workshop,San Marco, California, June 1990.
DARPA, Morgan Kauf-mann Publishers, Inc.L.
Hirschman, D. Dahl, D. McKay, L. Norton, andM.
Linebarger.
A proposal for automatic evaluation of dis-course.
In Proceedings of the Speech and Natural LanguageWorkshop, San Marco, California, June 1990.
DARPA, Mor-gan Kaufmann Publishers, Inc.J.
Neal, T. Finin, R. Grishman, C. Montgomery, and S. Wal-ter.
Workshop on the Evaluation of Natural Language Pro-cessing Systems.
Technical Report (to appear), RADC, June1991.D.
S. Pallett.
DARPA Resource Management and ATISbenchmark test poster session.
In Proceedings of the Speechand Natural Language Workshop, San Mateo, California,February 1991.
DARPA, Morgan Kaufmann Publishers, Inc.M.
Palmer, T. Finin, and S. Walter.
Workshop on the Eval-uation of Natural Language Processing Systems.
TechnicalReport RADC-TR-89-302, RADC, 1989.B.
M. Sundheirn.
Plans for a task-oriented evaluation ofnatural language understanding systems.
In Proceedings ofthe Speech and Natural Language Workshop, ages 197-202,Philadelphia, PA, Februrary 1989.B.
M. Sundheim.
Overview of the Third Message Under-standing Evaluation and Conference.
In Proceedings of theThird Message Understanding Conference (MUC-3), pages3-16, San Marco, California, May 1991.
DARPA, MorganKaufmann Publishers, Inc.H.
Tennant.
Evaluation of Natural Language Processors.PhD thesis, University of Illinois, 1981.S.
Walter.
Neal-Montgomery NLP system evaluationmethodology.
In Proceedings of the Speech and NaturalLanguage Workshop, San Mateo, California, February 1992.DARPA, Morgan Kaufmann Publishers, Inc.R.
M. Weischedel.
Issues and Red Herrings in EvaluatingNatural Language Interfaces.
Pergamnon Press, 1986.169
