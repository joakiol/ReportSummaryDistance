Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 91?99,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsRetrieving Correct Semantic Boundaries in Dependency StructureJinho D. ChoiDepartment of Computer ScienceUniversity of Colorado at Boulderchoijd@colorado.eduMartha PalmerDepartment of LinguisticsUniversity of Colorado at Bouldermartha.palmer@colorado.eduAbstractThis paper describes the retrieval of cor-rect semantic boundaries for predicate-argument structures annotated by depen-dency structure.
Unlike phrase structure,in which arguments are annotated at thephrase level, dependency structure doesnot have phrases so the argument labels areassociated with head words instead: thesubtree of each head word is assumed toinclude the same set of words as the an-notated phrase does in phrase structure.However, at least in English, retrievingsuch subtrees does not always guaranteeretrieval of the correct phrase boundaries.In this paper, we present heuristics thatretrieve correct phrase boundaries for se-mantic arguments, called semantic bound-aries, from dependency trees.
By apply-ing heuristics, we achieved an F1-scoreof 99.54% for correct representation ofsemantic boundaries.
Furthermore, erroranalysis showed that some of the errorscould also be considered correct, depend-ing on the interpretation of the annotation.1 IntroductionDependency structure has recently gained wide in-terest because it is simple yet provides useful in-formation for many NLP tasks such as sentimentanalysis (Kessler and Nicolov, 2009) or machinetranslation (Gildea, 2004).
Although dependencystructure is a kind of syntactic structure, it is quitedifferent from phrase structure: phrase structuregives phrase information by grouping constituentswhereas dependency structure gives dependencyrelations between pairs of words.
Many depen-dency relations (e.g., subject, object) have highcorrelations with semantic roles (e.g., agent, pa-tient), which makes dependency structure suit-able for representing semantic information such aspredicate-argument structure.In 2009, the Conference on Computational Nat-ural Language Learning (CoNLL) opened a sharedtask: the participants were supposed to take de-pendency trees as input and produce semantic rolelabels as output (Hajic?
et al, 2009).
The depen-dency trees were automatically converted from thePenn Treebank (Marcus et al, 1993), which con-sists of phrase structure trees, using some heuris-tics (cf.
Section 3).
The semantic roles were ex-tracted from the Propbank (Palmer et al, 2005).Since Propbank arguments were originally anno-tated at the phrase level using the Penn Treebankand the phrase information got lost during the con-version to the dependency trees, arguments are an-notated on head words instead of phrases in depen-dency trees; the subtree of each head word is as-sumed to include the same set of words as the an-notated phrase does in phrase structure.
Figure 1shows a dependency tree that has been convertedfrom the corresponding phrase structure tree.SNP1DTTheNNSresultsVPVBPappearPP1INinNPNPNNtodayPOS?sNNnewsThe results appear in today 's newsrootNMOD SBJ LOC NMODNMODROOT PMODFigure 1: Phrase vs. dependency structure91In the phrase structure tree, arguments of the verbpredicate appear are annotated on the phrases:NP1 as ARG0 and PP1 as ARGM-LOC.
In the de-pendency tree, the arguments are annotated on thehead words instead: results as the ARG0 and in asthe ARGM-LOC.
In this example, both PP1 and thesubtree of in consist of the same set of words {in,today, ?s, news} (as is the case for NP1 and thesubtree of results); therefore, the phrase bound-aries for the semantic arguments, called semanticboundaries, are retrieved correctly from the depen-dency tree.Retrieving the subtrees of head words usuallygives correct semantic boundaries; however, thereare cases where the strategy does not work.
Forexample, if the verb predicate is a gerund or a past-participle, it is possible that the predicate becomesa syntactic child of the head word annotated as asemantic argument of the predicate.
In Figure 2,the head word plant is annotated as ARG1 of theverb predicate owned, where owned is a child ofplant in the dependency tree.
Thus, retrieving thesubtree of plant would include the predicate it-self, which is not the correct semantic boundaryfor the argument (the correct boundary would beonly {The, plant}).The plant owned by MarkNMOD NMOD LGS PMODFigure 2: Past-participle exampleFor such cases, we need some alternative for re-trieving the correct semantic boundaries.
This isan important issue that has not yet been thoroughlyaddressed.
In this paper, we first show how to con-vert the Penn Treebank style phrase structure todependency structure.
We then describe how toannotate the Propbank arguments, already anno-tated in the phrase structure, on head words in thedependency structure.
Finally, we present heuris-tics that correctly retrieve semantic boundaries inmost cases.
For our experiments, we used the en-tire Penn Treebank (Wall Street Journal).
Our ex-periments show that it is possible to achieve an F1-score of 99.54% for correct representation of thesemantic boundaries.2 Related workEkeklint and Nivre (2007) tried to retrieve seman-tic boundaries by adding extra arcs to dependencytrees, so the structure is no longer a tree but agraph.
They experimented with the same cor-pus, the Penn Treebank, but used a different de-pendency conversion tool, Penn2Malt.1 Our workis distinguished from theirs because we keep thetree structure but use heuristics to find the bound-aries.
Johansson (2008) also tried to find seman-tic boundaries for evaluation of his semantic rolelabeling system using dependency structure.
Heused heuristics that apply to general cases whereaswe add more detailed heuristics for specific cases.3 Converting phrase structure todependency structureWe used the same tool as the one used for theCoNLL?09 shared task to automatically convertthe phrase structure trees in the Penn Treebankto the dependency trees (Johansson and Nugues,2007).
The script gives several options for the con-version; we mostly used the default values exceptfor the following options:2?
splitSlash=false: do not split slashes.
Thisoption is taken so the dependency trees pre-serve the same number of word-tokens as theoriginal phrase structure trees.?
noSecEdges=true: ignore secondary edgesif present.
This option is taken so all sib-lings of verb predicates in phrase structurebecome children of the verbs in dependencystructure regardless of empty categories.
Fig-ure 3 shows the converted dependency tree,which is produced when the secondary edge(*ICH*) is not ignored, and Figure 4 showsthe one produced by ignoring the secondaryedge.
This option is useful because NP?
andPP-2?
are annotated as separate arguments ofthe verb predicate paid in Propbank (NP?
asARG1 and PP-2?
as ARGM-MNR).SNP-1HeVPVBDwasVPVBNpaidNP*-1NP*NP.. salaryPP*ICH*-2PP-2?with ..1http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html2http://nlp.cs.lth.se/software/treebank converter/92paidHe wasrootSBJ NMODROOT NMOD$342Ka salary withVC NMODNMOD$280Ka bonusOBJNMODNMODFigure 3: When the secondary edge is not ignoredpaidHe wasrootSBJ NMODROOT NMOD$342Ka salary withVC NMODNMOD$280Ka bonusOBJNMODADVFigure 4: When the secondary edge is ignoredTotal 49,208 dependency trees were convertedfrom the Penn Treebank.
Although it was pos-sible to apply different values for other options,we found them not helpful in finding correct se-mantic boundaries of Propbank arguments.
Notethat some of non-projective dependencies are re-moved by ignoring the secondary edges.
However,it did not make all dependency trees projective;our methods can be applied for either projectiveor non-projective dependency trees.4 Adding semantic roles to dependencystructure4.1 Finding the head wordsFor each argument in the Propbank annotated ona phrase, we extracted the set of words belongingto the phrase.
Let this set be Sp.
In Figure 1, PP1is the ARGM-LOC of appear so Sp is {in, today,?s, news}.
Next, we found a set of head words,say Sd, whose subtrees cover all words in Sp (e.g.,Sd = {in} in Figure 1).
It would be ideal if thereexisted one head word whose subtree covers allwords in Sp, but this is not always the case.
It ispossible that Sd needs more than one head word tocover all the words in Sp.Figure 5 shows an algorithm that finds a set ofhead words Sd whose subtrees cover all words inSp.
For each word w in Sp, the algorithm checksif w?s subtree gives the maximum coverage (if w?ssubtree contains more words than any other sub-tree); if it does, the algorithm adds w to Sd, re-moves all words in w?s subtree from Sp, then re-peats the search.
The search ends when all wordsin Sp are covered by some subtree of a head wordin Sd.
Notice that the algorithm searches for theminimum number of head words by matching themaximum coverages.Input: Sp = a set of words for each argumentin the PropbankOutput: Sd = a set of head words whosesubtrees cover all words in SpAlgorithm:getHeadWords(Sp)1Sd = {}2while Sp 6= ?
do3max = None4foreach w ?
Sp do5if |subtree(w)| > |subtree(max)|6thenmax = w7end8Sd.add(max)9Sp.removeAll(subtree(max))10end11return Sd12Figure 5: Finding the min-set of head wordsThe algorithm guarantees to find the min-set Sdwhose subtrees cover all words in Sp.
This gives100% recall for Sd compared to Sp; however, theprecision is not guaranteed to be as perfect.
Sec-tion 5 illustrates heuristics that remove the over-generated words so we could improve the preci-sion as well.4.2 Ignoring empty categoriesAs described in Figures 3 and 4, dependency treesdo not include any empty categories (e.g., nullelements, traces, PRO?s): the empty categoriesare dropped during the conversion to the depen-dency trees.
In the Penn Treebank, 11.5% of thePropbank arguments are annotated on empty cat-egories.
Although this is a fair amount, we de-cided to ignore them for now since dependencystructure is not naturally designed to handle emptycategories.
Nonetheless, we are in the process offinding ways of automatically adding empty cate-gories to dependency trees so we can deal with theremaining of 11.5% Propbank arguments.4.3 Handling disjoint argumentsSome Propbank arguments are disjoint in thephrase structure so that they cannot be representedas single head words in dependency trees.
For ex-ample in Figure 6, both NP-1?
and S?
are ARG1 ofthe verb predicate continued but there is no headword for the dependency tree that can representboth phrases.
The algorithm in Figure 5 naturally93handles this kind of disjoint arguments.
Althoughwords in Sp are not entirely consecutive ({Yields,on, mutual, funds, to, slide}), it iteratively findsboth head words correctly: Yields and to.SNP-1?NPYieldsPPINonNPmutual fundsVPVBDcontinuedS?NP*-1VPTOtoVPslideYields on mutual torootNMOD OPRDNMODPMODROOTSBJfunds continued slideIMFigure 6: Disjoint argument example5 Retrieving fine-grained semanticboundariesThere are a total of 292,073 Propbank argumentsin the Penn Treebank, and only 88% of them mapto correct semantic boundaries from the depen-dency trees by taking the subtrees of head words.The errors are typically caused by including morewords than required: the recall is still 100% for theerror cases whereas the precision is not.
Amongseveral error cases, the most critical one is causedby verb predicates whose semantic arguments arethe parents of themselves in the dependency trees(cf.
Figure 2).
In this section, we present heuris-tics to handle such cases so we can achieve preci-sion nearly as good as the recall.5.1 ModalsIn the current dependency structure, modals (e.g.,will, can, do) become the heads of the main verbs.In Figure 7, will is the head of the verb predicateremain in the dependency tree; however, it is alsoan argument (ARGM-MOD) of the verb in Prop-bank.
This can be resolved by retrieving only thehead word, but not the subtree.
Thus, only will isretrieved as the ARGM-MOD of remain.Modals can be followed by conjuncts that arealso modals.
In this case, the entire coordinationis retrieved as ARGM-MOD (e.g., {may, or, may,not} in Figure 8).They will remain on the listSBJrootVC PRD NMODPRDROOTFigure 7: Modal example 1He may or read the bookrootSBJ COORD ADV NMODOBJROOTmay notCONJCOORDFigure 8: Modal example 25.2 NegationsNegations (e.g., not, no longer) are annotated asARGM-NEG in Propbank.
In most cases, nega-tions do not have any child in dependency trees,so retrieving only the negations themselves givesthe correct semantic boundaries for ARGM-NEG,but there are exceptions.
One is where a negationcomes after a conjunction; in which case, the nega-tion becomes the parent of the main verb.
In Fig-ure 9, not is the parent of the verb predicate copyalthough it is the ARGM-NEG of the verb.You may come but notrootSBJ COORDROOTto readPRPcopyVC IM CONJ COORDFigure 9: Negation example 1The other case is where a negation is modified bysome adverb; in which case, the adverb shouldalso be retrieved as well as the negation.
In Fig-ure 10, both no and longer should be retrieved asthe ARGM-NEG of the verb predicate oppose.They no longer the legislationrootSBJNMODOBJopposeAMODTMPROOTFigure 10: Negation example 25.3 Overlapping argumentsPropbank does not allow overlapping arguments.For each predicate, if a word is included in oneargument, it cannot be included in any other argu-ment of the predicate.
In Figure 11, burdens andin the region are annotated as ARG1 and ARGM-LOC of the verb predicate share, respectively.
Thearguments were originally annotated as two sepa-rate phrases in the phrase structure tree; however,94in became the child of burdens during the conver-sion, so the subtree of burdens includes the subtreeof in, which causes overlapping arguments.SNPU.S.VPVBZencouragesSNPJapanVPTOtoVPVBshareNPNPburdensPPin ..U.S. encourages JapaninrootshareLOCOPRDto burdensthe regionNMODPMODOBJIMOBJSBJROOTFigure 11: Overlapping argument example 1When this happens, we reconstruct the depen-dency tree so in becomes the child of share insteadof burdens (Figure 12).
By doing so, taking thesubtrees of burdens and in no longer causes over-lapping arguments.3U.S.
encourages JapaninrootshareOPRDto burdensthe regionNMODPMODOBJIMOBJSBJROOTLOCFigure 12: Overlapping argument example 25.4 Verb predicates whose semanticarguments are their syntactic headsThere are several cases where semantic argumentsof verb predicates become the syntactic heads ofthe verbs.
The modals and negations in the previ-ous sections are special cases where the seman-tic boundaries can be retrieved correctly with-out compromising recall.
The following sec-tions describe other cases, such as relative clauses(Section 5.4.2), gerunds and past-participles (Sec-tion 5.4.3), that may cause a slight decrease in re-call by finding more fine-grained semantic bound-aries.
In these cases, the subtree of the verb predi-cates are excluded from the semantic arguments.3This can be considered as a Treebank/Propbank dis-agreement, which is further discussed in Sectino 6.2.5.4.1 Verb chainsThree kinds of verb chains exist in the currentdependency structure: auxiliary verbs (includingmodals and be-verbs), infinitive markers, and con-junctions.
As discussed in Section 5.1, verb chainsbecome the parents of their main verbs in depen-dency trees.
This indicates that when the subtreeof the main verb is to be excluded from semanticarguments, the verb chain needs to be excluded aswell.
This usually happens when the main verbsare used within relative clauses.
In addition, moreheuristics are needed for retrieving correct seman-tic boundaries for relative clauses, which are fur-ther discussed in Section 5.4.2.The following figures show examples of eachkind of verb chain.
It is possible that multiple verbchains are joined with one main verb.
In this case,we find the top-most verb chain and exclude itsentire subtree from the semantic argument.
In Fig-ure 13, part is annotated as ARG1 of the verb pred-icate gone, chained with the auxiliary verb be, andagain chained with the modal may.
Since may isthe top-most verb chain, we exclude its subtree soonly a part is retrieved as the ARG1 of gone.a part thatbeNMODmay gonePRDVCDEPNMODFigure 13: Auxiliary verb exampleFigure 14 shows the case of infinitive markers.those is annotated as ARG0 of the verb predicateleave, which is first chained with the infinitivemarker to then chained with the verb required.
Byexcluding the subtree of required, only those is re-trieved as the ARG0 of leave.rules aretoughroot thoseROOTonrequiredSBJtoAMODleavePRD PMOD APPO OPRD IMFigure 14: Infinitive marker exampleFigure 15 shows the case of conjunctions.
peopleis annotated as ARG0 of the verb predicate exceed,which is first chained with or then chained withmeet.
By excluding the subtree of meet, only peo-ple is retrieved as the ARG0 of exceed.When a verb predicate is followed by an ob-ject complement (OPRD), the subtree of the objectcomplement is not excluded from the semantic ar-gument.
In Figure 16, distribution is annotated as95peoplewho meetexceedNMODor theDEP NMODOBJexpectationCONJCOORDFigure 15: Conjunction exampleARG1 of the verb predicate expected.
By excludingthe subtree of expected, the object complement tooccur would be excluded as well; however, Prop-bank annotation requires keeping the object com-plement as the part of the argument.
Thus, a dis-tribution to occur is retrieved as the ARG1 of ex-pected.a distribution expected to occurNMOD IMOPRDAPPOFigure 16: Object complement example5.4.2 Relative clausesWhen a verb predicate is within a relative clause,Propbank annotates both the relativizer (if present)and its antecedent as part of the argument.
For ex-ample in Figure 15, people is annotated as ARG0of both meet and exceed.
By excluding the subtreeof meet, the relativizer who is also excluded fromthe semantic argument, which is different from theoriginal Propbank annotation.
In this case, wekeep the relativizer as part of the ARG0; thus, peo-ple who is retrieved as the ARG0 (similarly, a partthat is retrieved as the ARG0 of gone in Figure 13).It is possible that a relativizer is headed by apreposition.
In Figure 17, climate is annotated asARGM-LOC of the verb predicate made and therelativizer which is headed by the preposition in.In this case, both the relativizer and the preposi-tion are included in the semantic argument.
Thus,the climate in which becomes the ARGM-LOC ofmade.theclimate in decisionsthe wasPMODmadewhichNMOD NMODLOCDEPVCFigure 17: Relativizer example5.4.3 Gerunds and past-participlesIn English, when gerunds and past-participles areused without the presence of be-verbs, they oftenfunction as noun modifiers.
Propbank still treatsthem as verb predicates; however, these verbs be-come children of the nouns they modify in the de-pendency structure, so the heuristics discussed inSection 5.4 and 5.4.1 need to be applied to find thecorrect semantic boundaries.
Furthermore, sincethese are special kinds of verbs, they require evenmore rigorous pruning.When a head word, annotated to be a seman-tic argument of a verb predicate, comes after theverb, every word prior to the verb predicate needsto be excluded from the semantic argument.
InFigure 18, group is annotated as ARG0 of theverb predicate publishing, so all words prior to thepredicate (the Dutch) need to be excluded.
Thus,only group is retrieved as the ARG0 of publishing.the Dutch publishing groupNMODNMODNMODFigure 18: Gerund exampleWhen the head word comes before the verb pred-icate, the subtree of the head word, excluding thesubtree of the verb predicate, is retrieved as the se-mantic argument.
In Figure 19, correspondence isannotated as ARG1 of the verb predicate mailed,so the subtree of correspondence, excluding thesubtree of mailed, is retrieved to be the argument.Thus, correspondence about incomplete 8300s be-comes the ARG1 of mailed.correspondence mailed aboutNMODNMODincomplete 8300sNMODPMODFigure 19: Past-participle example 1When the subtree of the verb predicate is imme-diately followed by comma-like punctuation (e.g.,comma, colon, semi-colon, etc.)
and the headword comes before the predicate, every word afterthe punctuation is excluded from the semantic ar-gument.
In Figure 20, fellow is annotated as ARG1of the verb predicate named, so both the subtreeof the verb (named John) and every word after thecomma (, who stayed for years) are excluded fromthe semantic argument.
Thus, only a fellow is re-trieved as the ARG1 of named.5.5 PunctuationFor evaluation, we built a model that excludespunctuation from semantic boundaries for two rea-sons.
First, it is often not clear how punctuation96a named John who stayedfellow , for yearsNMOD APPO OPRDPDEP TMP PMODNMODFigure 20: Past-participle example 2needs to be annotated in either Treebank or Prop-bank; because of that, annotation for punctuationis not entirely consistent, which makes it hard toevaluate.
Second, although punctuation gives use-ful information for obtaining semantic boundaries,it is not crucial for semantic roles.
In fact, someof the state-of-art semantic role labeling systems,such as ASSERT (Pradhan et al, 2004), give anoption for omitting punctuation from the output.For these reasons, our final model ignores punctu-ation for semantic boundaries.6 Evaluations6.1 Model comparisonsThe following list describes six models used forthe experiments.
Model I is the baseline approachthat retrieves all words in the subtrees of headwords as semantic boundaries.
Model II to VI usethe heuristics discussed in the previous sections.Each model inherits all the heuristics from the pre-vious model and adds new heuristics; therefore,each model is expected to perform better than theprevious model.?
I - all words in the subtrees (baseline)?
II - modals + negations (Sections 5.1, 5.2)?
III - overlapping arguments (Section 5.3)?
IV - verb chains + relative clauses (Sec-tions 5.4.1, 5.4.2)?
V - gerunds + past-participles (Section 5.4.3)?
VI - excluding punctuations (Section 5.5)The following list shows measurements used forthe evaluations.
gold(arg) is the gold-standardset of words for the argument arg.
sys(arg) isthe set of words for arg produced by our system.c(arg1, arg2) returns 1 if arg1 is equal to arg2;otherwise, returns 0.
T is the total number of ar-guments in the Propbank.Accuracy =1T??
?argc(gold(arg), sys(arg))Precision =1T??
?arg|gold(arg) ?
sys(arg)||sys(arg)|Recall =1T??
?arg|gold(arg) ?
sys(arg)||gold(arg)|F1 =2 ?
Precision ?RecallPrecision + RecallTable 1 shows the results from the models us-ing the measurements.
As expected, each modelshows improvement over the previous one interms of accuracy and F1-score.
The F1-scoreof Model VI shows improvement that is statisti-cally significant compared to Model I using t-test(t = 149.00, p < 0.0001).
The result from thefinal model is encouraging because it enables usto take full advantage of dependency structure forsemantic role labeling.
Without finding the correctsemantic boundaries, even if a semantic role label-ing system did an excellent job finding the righthead words, we would not be able to find the ac-tual chunks for the arguments.
By using our ap-proach, finding the correct semantic boundaries isno longer an issue for using dependency structurefor automatic semantic role labeling.Model Accuracy Precision Recall F1I 88.00 92.51 100 96.11II 91.84 95.77 100 97.84III 92.17 97.08 100 98.52IV 95.89 98.51 99.95 99.23V 97.00 98.94 99.95 99.44VI 98.20 99.14 99.95 99.54Table 1: Model comparisons (in percentage)6.2 Error analysisAlthough each model consistently shows improve-ment on the precision, the recall is reduced a bit forsome models.
Specifically, the recalls for Mod-els II and III are not 100% but rather 99.9994%and 99.996%, respectively.
We manually checkedall errors for Models II and III and found that theyare caused by inconsistent annotations in the gold-standard.
For Model II, Propbank annotation forARGM-MOD was not done consistently with con-97junctions.
For example in Figure 8, instead of an-notating may or may not as the ARGM-MOD, someannotations include only may and may not but notthe conjunction or.
Since our system consistentlyincluded the conjunctions, they appeared to be dif-ferent from the gold-standard, but are not errors.For Model III, Treebank annotation was notdone consistently for adverbs modifying nega-tions.
For example in Figure 10, longer is some-times (but rarely) annotated as an adjective whereit is supposed to be an adverb.
Furthermore,longer sometimes becomes a child of the verbpredicate oppose (instead of being the child of no).Such annotations made our system exclude longeras a part of ARGM-NEG, but it would have foundthem correctly if the trees were annotated consis-tently.There are a few cases that caused errors in Mod-els IV and V. The most critical one is caused by PP(prepositional phrase) attachment.
In Figure 21,enthusiasm is annotated as ARG1 of the verb pred-icate showed, so our system retrieved the subtreeof enthusiasm, excluding the subtree of showed,as the semantic boundary for the ARG1 (e.g., theenthusiasm).
However, Propbank originally an-notated both the enthusiasm and for stocks as theARG1 in the phrase structure tree (so the preposi-tional phrase got lost in our system).the investors showed forenthusiasm stocksNMODNMODSBJ ADV PMODFigure 21: PP-attachment example 1This happens when there is a disagreement be-tween Treebank and Propbank annotations: theTreebank annotation attached the PP (for stocks)to the verb (showed) whereas the Propbank anno-tation attached the PP to the noun (enthusiasm).This is a potential error in the Treebank.
In thiscase, we can trust the Propbank annotation and re-construct the tree so the Treebank and Propbankannotations agree with each other.
After the re-construction, the dependency tree would look likeone in Figure 22.the investors showed forenthusiasm stocksNMODNMODSBJ PMODADVFigure 22: PP-attachment example 27 Conclusion and future workWe have discussed how to convert phrase struc-ture trees to dependency trees, how to find theminimum-set of head words for Propbank argu-ments in dependency structure, and heuristics forretrieving fine-grained semantic boundaries.
Byusing our approach, we correctly retrieved the se-mantic boundaries of 98.2% of the Propbank ar-guments (F1-score of 99.54%).
Furthermore, theheuristics can be used to fix some of the incon-sistencies in both Treebank and Propbank annota-tions.
Moreover, they suggest ways of reconstruct-ing dependency structure so that it can fit betterwith semantic roles.Retrieving correct semantic boundaries is im-portant for tasks like machine translation wherenot only the head words but also all other wordsmatter to complete the task (Choi et al, 2009).In the future, we are going to apply our approachto other corpora and see how well the heuristicswork.
In addition, we will try to find ways of auto-matically adding empty categories to dependencystructure so we can deal with the full set of Prop-bank arguments.AcknowledgmentsSpecial thanks are due to Professor Joakim Nivreof Uppsala University and Claire Bonial of theUniversity of Colorado at Boulder for very helpfulinsights.
We gratefully acknowledge the supportof the National Science Foundation Grants CISE-CRI-0551615, Towards a Comprehensive Lin-guistic Annotation and CISE-CRI 0709167, Col-laborative: A Multi-Representational and Multi-Layered Treebank for Hindi/Urdu, and a grantfrom the Defense Advanced Research ProjectsAgency (DARPA/IPTO) under the GALE pro-gram, DARPA/CMO Contract No.
HR0011-06-C-0022, subcontract from BBN, Inc. Any opinions,findings, and conclusions or recommendations ex-pressed in this material are those of the authors anddo not necessarily reflect the views of the NationalScience Foundation.ReferencesJinho D. Choi, Martha Palmer, and Nianwen Xue.2009.
Using parallel propbanks to enhance word-alignments.
In Proceedings of ACL-IJCNLP work-shop on Linguistic Annotation (LAW?09), pages121?124.98Susanne Ekeklint and Joakim Nivre.
2007.
Adependency-based conversion of propbank.
InProceedings of NODALIDA workshop on BuildingFrame Semantics Resources for Scandinavian andBaltic Languages (FRAME?07), pages 19?25.Daniel Gildea.
2004.
Dependencies vs. constituentsfor tree-based alignment.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP?04), pages 214?221.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The conll-2009shared task: Syntactic and semantic dependencies inmultiple languages.
In Proceedings of the 13th Con-ference on Computational Natural Language Learn-ing (CoNLL?09), pages 1?18.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion for en-glish.
In Proceedings of the 16th Nordic Conferenceof Computational Linguistics (NODALIDA?07).Richard Johansson.
2008.
Dependency-based Seman-tic Analysis of Natural-language Text.
Ph.D. thesis,Lund University.Jason S. Kessler and Nicolas Nicolov.
2009.
Targetingsentiment expressions through supervised ranking oflinguistic configurations.
In Proceedings of the 3rdInternational AAAI Conference on Weblogs and So-cial Media (ICWSM?09).Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational Linguistics, 19(2):313?330.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,James H. Martin, and Daniel Jurafsky.
2004.
Shal-low semantic parsing using support vector machines.In Proceedings of the Human Language TechnologyConference/North American chapter of the Associ-ation for Computational Linguistics annual meeting(HLT/NAACL?04).99
