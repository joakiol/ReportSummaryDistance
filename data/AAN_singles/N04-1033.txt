Improvements in Phrase-Based Statistical Machine TranslationRichard Zens and Hermann NeyChair of Computer Science VIRWTH Aachen University{zens,ney}@cs.rwth-aachen.deAbstractIn statistical machine translation, the currentlybest performing systems are based in some wayon phrases or word groups.
We describe thebaseline phrase-based translation system andvarious refinements.
We describe a highly ef-ficient monotone search algorithm with a com-plexity linear in the input sentence length.
Wepresent translation results for three tasks: Verb-mobil, Xerox and the Canadian Hansards.
Forthe Xerox task, it takes less than 7 seconds totranslate the whole test set consisting of morethan 10K words.
The translation results forthe Xerox and Canadian Hansards task are verypromising.
The system even outperforms thealignment template system.1 IntroductionIn statistical machine translation, we are given a sourcelanguage (?French?)
sentence fJ1 = f1 .
.
.
fj .
.
.
fJ ,which is to be translated into a target language (?English?
)sentence eI1 = e1 .
.
.
ei .
.
.
eI .
Among all possible targetlanguage sentences, we will choose the sentence with thehighest probability:e?I1 = argmaxeI1{Pr(eI1|fJ1 )} (1)= argmaxeI1{Pr(eI1) ?
Pr(fJ1 |eI1)} (2)The decomposition into two knowledge sources in Equa-tion 2 is known as the source-channel approach to statisti-cal machine translation (Brown et al, 1990).
It allows anindependent modeling of target language model Pr(eI1)and translation model Pr(fJ1 |eI1)1.
The target language1The notational convention will be as follows: we use thesymbol Pr(?)
to denote general probability distributions with(nearly) no specific assumptions.
In contrast, for model-basedprobability distributions, we use the generic symbol p(?
).model describes the well-formedness of the target lan-guage sentence.
The translation model links the sourcelanguage sentence to the target language sentence.
It canbe further decomposed into alignment and lexicon model.The argmax operation denotes the search problem, i.e.the generation of the output sentence in the target lan-guage.
We have to maximize over all possible target lan-guage sentences.An alternative to the classical source-channel ap-proach is the direct modeling of the posterior probabil-ity Pr(eI1|fJ1 ).
Using a log-linear model (Och and Ney,2002), we obtain:Pr(eI1|fJ1 ) = exp( M?m=1?mhm(eI1, fJ1 ))?
Z(fJ1 )Here, Z(fJ1 ) denotes the appropriate normalization con-stant.
As a decision rule, we obtain:e?I1 = argmaxeI1{ M?m=1?mhm(eI1, fJ1 )}This approach is a generalization of the source-channelapproach.
It has the advantage that additional models orfeature functions can be easily integrated into the overallsystem.
The model scaling factors ?M1 are trained accord-ing to the maximum entropy principle, e.g.
using the GISalgorithm.
Alternatively, one can train them with respectto the final translation quality measured by some errorcriterion (Och, 2003).The remaining part of this work is structured as fol-lows: in the next section, we will describe the base-line phrase-based translation model and the extraction ofbilingual phrases.
Then, we will describe refinementsof the baseline model.
In Section 4, we will describe amonotone search algorithm.
Its complexity is linear inthe sentence length.
The next section contains the statis-tics of the corpora that were used.
Then, we will inves-tigate the degree of monotonicity and present the transla-tion results for three tasks: Verbmobil, Xerox and Cana-dian Hansards.2 Phrase-Based Translation2.1 MotivationOne major disadvantage of single-word based approachesis that contextual information is not taken into account.The lexicon probabilities are based only on single words.For many words, the translation depends heavily on thesurrounding words.
In the single-word based translationapproach, this disambiguation is addressed by the lan-guage model only, which is often not capable of doingthis.One way to incorporate the context into the translationmodel is to learn translations for whole phrases insteadof single words.
Here, a phrase is simply a sequence ofwords.
So, the basic idea of phrase-based translation isto segment the given source sentence into phrases, thentranslate each phrase and finally compose the target sen-tence from these phrase translations.2.2 Phrase ExtractionThe system somehow has to learn which phrases aretranslations of each other.
Therefore, we use the follow-ing approach: first, we train statistical alignment modelsusing GIZA++ and compute the Viterbi word alignment ofthe training corpus.
This is done for both translation di-rections.
We take the union of both alignments to obtain asymmetrized word alignment matrix.
This alignment ma-trix is the starting point for the phrase extraction.
The fol-lowing criterion defines the set of bilingual phrases BPof the sentence pair (fJ1 ; eI1) and the alignment matrixA ?
J ?
I that is used in the translation system.BP(fJ1 , eI1, A) ={(f j2j1 , ei2i1):?
(j, i) ?
A : j1 ?
j ?
j2 ?
i1 ?
i ?
i2??
(j, i) ?
A : j1 ?
j ?
j2 ?
i1 ?
i ?
i2}This criterion is identical to the alignment template cri-terion described in (Och et al, 1999).
It means that twophrases are considered to be translations of each other, ifthe words are aligned only within the phrase pair and notto words outside.
The phrases have to be contiguous.2.3 Translation ModelTo use phrases in the translation model, we introduce thehidden variable S. This is a segmentation of the sentencepair (fJ1 ; eI1) into K phrases (f?K1 ; e?K1 ).
We use a one-to-one phrase alignment, i.e.
one source phrase is translatedby exactly one target phrase.
Thus, we obtain:Pr(fJ1 |eI1) =?SPr(fJ1 , S|eI1) (3)=?SPr(S|eI1) ?
Pr(fJ1 |S, eI1) (4)?
maxS{Pr(S|eI1) ?
Pr(f?K1 |e?K1 )}(5)In the preceding step, we used the maximum approxima-tion for the sum over all segmentations.
Next, we allowonly translations that are monotone at the phrase level.So, the phrase f?1 is produced by e?1, the phrase f?2 isproduced by e?2, and so on.
Within the phrases, the re-ordering is learned during training.
Therefore, there is noconstraint on the reordering within the phrases.Pr(f?K1 |e?K1 ) =K?k=1Pr(f?k|f?k?11 , e?K1 ) (6)=K?k=1p(f?k|e?k) (7)Here, we have assumed a zero-order model at the phraselevel.
Finally, we have to estimate the phrase translationprobabilities p(f?
|e?).
This is done via relative frequencies:p(f?
|e?)
= N(f?
, e?)?f?
?
N(f?
?, e?
)(8)Here, N(f?
, e?)
denotes the count of the event that f?
hasbeen seen as a translation of e?.
If one occurrence of e?
hasN > 1 possible translations, each of them contributes toN(f?
, e?)
with 1/N .
These counts are calculated from thetraining corpus.Using a bigram language model and assuming Bayesdecision rule, Equation (2), we obtain the followingsearch criterion:e?I1 = argmaxeI1{Pr(eI1) ?
Pr(fJ1 |eI1)} (9)= argmaxeI1{ I?i=1p(ei|ei?1) (10)?maxSp(S|eI1) ?K?k=1p(f?k|e?k)}?
argmaxeI1,S{ I?i=1p(ei|ei?1)K?k=1p(f?k|e?k)}(11)For the preceding equation, we assumed the segmentationprobability p(S|eI1) to be constant.
The result is a simpletranslation model.
If we interpret this model as a featurefunction in the direct approach, we obtain:hphr(fJ1 , eI1, S,K) = logK?k=1p(f?k|e?k)We use the maximum approximation for the hidden vari-able S. Therefore, the feature functions are dependent onS.
Although the number of phrases K is implicitly givenby the segmentation S, we used both S and K to makethis dependency more obvious.3 RefinementsIn this section, we will describe refinements of thephrase-based translation model.
First, we will describetwo heuristics: word penalty and phrase penalty.
Sec-ond, we will describe a single-word based lexicon model.This will be used to smooth the phrase translation proba-bilities.3.1 Simple HeuristicsIn addition to the baseline model, we use two simpleheuristics, namely word penalty and phrase penalty:hwp(fJ1 , eI1, S,K) = I (12)hpp(fJ1 , eI1, S,K) = K (13)The word penalty feature is simply the target sentencelength.
In combination with the scaling factor this re-sults in a constant cost per produced target languageword.
With this feature, we are able to adjust the sentencelength.
If we set a negative scaling factor, longer sen-tences are more penalized than shorter ones, and the sys-tem will favor shorter translations.
Alternatively, by us-ing a positive scaling factors, the system will favor longertranslations.Similar to the word penalty, the phrase penalty featureresults in a constant cost per produced phrase.
The phrasepenalty is used to adjust the average length of the phrases.A negative weight, meaning real costs per phrase, resultsin a preference for longer phrases.
A positive weight,meaning a bonus per phrase, results in a preference forshorter phrases.3.2 Word-based LexiconWe are using relative frequencies to estimate the phrasetranslation probabilities.
Most of the longer phrases areseen only once in the training corpus.
Therefore, purerelative frequencies overestimate the probability of thosephrases.
To overcome this problem, we use a word-basedlexicon model to smooth the phrase translation probabili-ties.
For a source word f and a target phrase e?
= ei2i1 , weuse the following approximation:p(f |ei2i1) ?
1?i2?i=i1(1?
p(f |ei))This models a disjunctive interaction, also called noisy-OR gate (Pearl, 1988).
The idea is that there are multipleindependent causes ei2i1 that can generate an event f .
Itcan be easily integrated into the search algorithm.
Thecorresponding feature function is:hlex(fJ1 , eI1, S,K) = logK?k=1jk?j=jk?1+1p(fj |e?k)Here, jk and ik denote the final position of phrase numberk in the source and the target sentence, respectively, andwe define j0 := 0 and i0 := 0.To estimate the single-word based translation probabil-ities p(f |e), we use smoothed relative frequencies.
Thesmoothing method we apply is absolute discounting withinterpolation:p(f |e) = max {N(f, e)?
d, 0}N(e) + ?
(e) ?
?
(f)This method is well known from language modeling (Neyet al, 1997).
Here, d is the nonnegative discounting pa-rameter, ?
(e) is a normalization constant and ?
is the nor-malized backing-off distribution.
To compute the counts,we use the same word alignment matrix as for the ex-traction of the bilingual phrases.
The symbol N(e) de-notes the unigram count of a word e and N(f, e) denotesthe count of the event that the target language word e isaligned to the source language word f .
If one occurrenceof e has N > 1 aligned source words, each of them con-tributes with a count of 1/N .
The formula for ?
(e) is:?
(e) = 1N(e)??
?f :N(f,e)>dd+?f :N(f,e)?dN(f, e)?
?= 1N(e)?fmin{d,N(f, e)}This formula is a generalization of the one typically usedin publications on language modeling.
This generaliza-tion is necessary, because the lexicon counts may be frac-tional whereas in language modeling typically integercounts are used.
Additionally, we want to allow discount-ing values d greater than one.
One effect of the discount-ing parameter d is that all lexicon entries with a countless than d are discarded and the freed probability massis redistributed among the other entries.As backing-off distribution ?
(f), we consider two al-ternatives.
The first one is a uniform distribution and thesecond one is the unigram distribution:?1(f) = 1Vf (14)?2(f) = N(f)?f ?
N(f ?
)(15)Here, Vf denotes the vocabulary size of the source lan-guage and N(f) denotes the unigram count of a sourceword f .4 Monotone SearchThe monotone search can be efficiently computed withdynamic programming.
The resulting complexity is lin-ear in the sentence length.
We present the formulae for abigram language model.
This is only for notational con-venience.
The generalization to a higher order languagemodel is straightforward.
For the maximization problemin (11), we define the quantity Q(j, e) as the maximumprobability of a phrase sequence that ends with the lan-guage word e and covers positions 1 to j of the sourcesentence.
Q(J + 1, $) is the probability of the opti-mum translation.
The $ symbol is the sentence boundarymarker.
We obtain the following dynamic programmingrecursion.Q(0, $) = 1Q(j, e) = maxe?,e?,j?M?j?<j{p(f jj?+1|e?)
?
p(e?|e?)
?Q(j?, e?
)}Q(J + 1, $) = maxe?
{Q(J, e?)
?
p($|e?
)}Here, M denotes the maximum phrase length in thesource language.
During the search, we store back-pointers to the maximizing arguments.
After perform-ing the search, we can generate the optimum translation.The resulting algorithm has a worst-case complexity ofO(J ?M ?
Ve ?
E).
Here, Ve denotes the vocabulary sizeof the target language and E denotes the maximum num-ber of phrase translation candidates for a source languagephrase.
Using efficient data structures and taking into ac-count that not all possible target language phrases can oc-cur in translating a specific source language sentence, wecan perform a very efficient search.This monotone algorithm is especially useful for lan-guage pairs that have a similar word order, e.g.
Spanish-English or French-English.5 Corpus StatisticsIn the following sections, we will present results on threetasks: Verbmobil, Xerox and Canadian Hansards.
There-fore, we will show the corpus statistics for each of thesetasks in this section.
The training corpus (Train) of eachtask is used to train a word alignment and then extract thebilingual phrases and the word-based lexicon.
The re-maining free parameters, e.g.
the model scaling factors,are optimized on the development corpus (Dev).
The re-sulting system is then evaluated on the test corpus (Test).Verbmobil Task.
The first task we will present re-sults on is the German?English Verbmobil task (Wahlster,2000).
The domain of this corpus is appointment schedul-ing, travel planning, and hotel reservation.
It consists oftranscriptions of spontaneous speech.
Table 1 shows thecorpus statistics of this task.Xerox task.
Additionally, we carried out experimentson the Spanish?English Xerox task.
The corpus consistsof technical manuals.
This is a rather limited domain task.Table 2 shows the training, development and test corpusstatistics.Canadian Hansards task.
Further experiments werecarried out on the French?English Canadian HansardsTable 1: Statistics of training and test corpus for the Verb-mobil task (PP=perplexity).German EnglishTrain Sentences 58 073Words 519 523 549 921Vocabulary 7 939 4 672Dev Sentences 276Words 3 159 3 438Trigram PP - 28.1Test Sentences 251Words 2 628 2 871Trigram PP - 30.5Table 2: Statistics of training and test corpus for the Xe-rox task (PP=perplexity).Spanish EnglishTrain Sentences 55 761Words 752 606 665 399Vocabulary 11 050 7 956Dev Sentences 1012Words 15 957 14 278Trigram PP ?
28.1Test Sentences 1125Words 10 106 8 370Trigram PP ?
48.3task.
This task contains the proceedings of the Cana-dian parliament.
About 3 million parallel sentences ofthis bilingual data have been made available by the Lin-guistic Data Consortium (LDC).
Here, we use a subsetof the data containing only sentences with a maximumlength of 30 words.
This task covers a large variety oftopics, so this is an open-domain corpus.
This is also re-flected by the large vocabulary size.
Table 3 shows thetraining and test corpus statistics.6 Degree of MonotonicityIn this section, we will investigate the effect of the mono-tonicity constraint.
Therefore, we compute how many ofthe training corpus sentence pairs can be produced withthe monotone phrase-based search.
We compare this tothe number of sentence pairs that can be produced with anonmonotone phrase-based search.
To make these num-bers more realistic, we use leaving-one-out.
Thus phrasesthat are extracted from a specific sentence pair are notused to check its monotonicity.
With leaving-one-out it ispossible that even the nonmonotone search cannot gen-erate a sentence pair.
This happens if a sentence paircontains a word that occurs only once in the training cor-pus.
All phrases that might produce this singleton areexcluded because of the leaving-one-out principle.
NoteTable 3: Statistics of training and test corpus for theCanadian Hansards task (PP=perplexity).French EnglishTrain Sentences 1.5MWords 24M 22MVocabulary 100 269 78 332Dev Sentences 500Words 9 043 8 195Trigram PP ?
57.7Test Sentences 5432Words 97 646 88 773Trigram PP ?
56.7that all these monotonicity consideration are done at thephrase level.
Within the phrases arbitrary reorderings areallowed.
The only restriction is that they occur in thetraining corpus.Table 4 shows the percentage of the training corpusthat can be generated with monotone and nonmonotonephrase-based search.
The number of sentence pairs thatcan be produced with the nonmonotone search gives anestimate of the upper bound for the sentence error rate ofthe phrase-based system that is trained on the given data.The same considerations hold for the monotone search.The maximum source phrase length for the Verbmobiltask and the Xerox task is 12, whereas for the CanadianHansards task we use a maximum of 4, because of thelarge corpus size.
This explains the rather low coverageon the Canadian Hansards task for both the nonmonotoneand the monotone search.For the Xerox task, the nonmonotone search can pro-duce 75.1% of the sentence pairs whereas the mono-tone can produce 65.3%.
The ratio of the two numbersmeasures how much the system deteriorates by using themonotone search and will be called the degree of mono-tonicity.
For the Xerox task, the degree of monotonicityis 87.0%.
This means the monotone search can produce87.0% of the sentence pairs that can be produced withthe nonmonotone search.
We see that for the Spanish-English Xerox task and for the French-English CanadianHansards task, the degree of monotonicity is rather high.For the German-English Verbmobil task it is significantlylower.
This may be caused by the rather free word orderin German and the long range reorderings that are neces-sary to translate the verb group.It should be pointed out that in practice the monotonesearch will perform better than what the preceding esti-mates indicate.
The reason is that we assumed a perfectnonmonotone search, which is difficult to achieve in prac-tice.
This is not only a hard search problem, but also acomplicated modeling problem.
We will see in the nextsection that the monotone search will perform very wellon both the Xerox task and the Canadian Hansards task.Table 4: Degree of monotonicity in the training corporafor all three tasks (numbers in percent).Verbmobil Xerox Hansardsnonmonotone 76.3 75.1 59.7monotone 55.4 65.3 51.5deg.
of mon.
72.6 87.0 86.37 Translation Results7.1 Evaluation CriteriaSo far, in machine translation research a single generallyaccepted criterion for the evaluation of the experimentalresults does not exist.
Therefore, we use a variety of dif-ferent criteria.?
WER (word error rate):The WER is computed as the minimum number ofsubstitution, insertion and deletion operations thathave to be performed to convert the generated sen-tence into the reference sentence.?
PER (position-independent word error rate):A shortcoming of the WER is that it requires a per-fect word order.
The word order of an acceptablesentence can be different from that of the target sen-tence, so that the WER measure alone could be mis-leading.
The PER compares the words in the twosentences ignoring the word order.?
BLEU score:This score measures the precision of unigrams, bi-grams, trigrams and fourgrams with respect to a ref-erence translation with a penalty for too short sen-tences (Papineni et al, 2001).
BLEU measures ac-curacy, i.e.
large BLEU scores are better.?
NIST score:This score is similar to BLEU.
It is a weighted n-gram precision in combination with a penalty fortoo short sentences (Doddington, 2002).
NIST mea-sures accuracy, i.e.
large NIST scores are better.For the Verbmobil task, we have multiple referencesavailable.
Therefore on this task, we compute all the pre-ceding criteria with respect to multiple references.
Toindicate this, we will precede the acronyms with an m(multiple) if multiple references are used.
For the othertwo tasks, only single references are used.7.2 Translation SystemsIn this section, we will describe the systems that wereused.
On the one hand, we have three different variantsof the single-word based model IBM4.
On the other hand,we have two phrase-based systems, namely the alignmenttemplates and the one described in this work.Single-Word Based Systems (SWB).
First, there is amonotone search variant (Mon) that translates each wordof the source sentence from left to right.
The second vari-ant allows reordering according to the so-called IBM con-straints (Berger et al, 1996).
Thus up to three wordsmay be skipped and translated later.
This system willbe denoted by IBM.
The third variant implements spe-cial German-English reordering constraints.
These con-straints are represented by a finite state automaton andoptimized to handle the reorderings of the German verbgroup.
The abbreviation for this variant is GE.
It is onlyused for the German-English Verbmobil task.
This is justan extremely brief description of these systems.
For de-tails, see (Tillmann and Ney, 2003).Phrase-Based System (PB).
For the phrase-based sys-tem, we use the following feature functions: a trigramlanguage model, the phrase translation model and theword-based lexicon model.
The latter two feature func-tions are used for both directions: p(f |e) and p(e|f).Additionally, we use the word and phrase penalty fea-ture functions.
The model scaling factors are optimizedon the development corpus with respect to mWER sim-ilar to (Och, 2003).
We use the Downhill Simplex al-gorithm from (Press et al, 2002).
We do not performthe optimization on N -best lists but we retranslate thewhole development corpus for each iteration of the op-timization algorithm.
This is feasible because this systemis extremely fast.
It takes only a few seconds to translatethe whole development corpus for the Verbmobil task andthe Xerox task; for details see Section 8.
In the experi-ments, the Downhill Simplex algorithm converged afterabout 200 iterations.
This method has the advantage thatit is not limited to the model scaling factors as the methoddescribed in (Och, 2003).
It is also possible to optimizeany other parameter, e.g.
the discounting parameter forthe lexicon smoothing.Alignment Template System (AT).
The alignmenttemplate system (Och et al, 1999) is similar to the sys-tem described in this work.
One difference is that thealignment templates are not defined at the word level butat a word class level.
In addition to the word-based tri-gram model, the alignment template system uses a class-based fivegram language model.
The search algorithm ofthe alignment templates allows arbitrary reorderings ofthe templates.
It penalizes reorderings with costs that arelinear in the jump width.
To make the results as compa-rable as possible, the alignment template system and thephrase-based system start from the same word alignment.The alignment template system uses discriminative train-ing of the model scaling factors as described in (Och andNey, 2002).7.3 Verbmobil TaskWe start with the Verbmobil results.
We studied smooth-ing the lexicon probabilities as described in Section 3.2.The results are summarized in Table 5.
We see that theTable 5: Effect of lexicon smoothing on the translationperformance [%] for the German-English Verbmobil task.system mWER mPER BLEU NISTunsmoothed 37.3 21.1 46.6 7.96uniform 37.0 20.7 47.0 7.99unigram 38.2 22.3 45.5 7.79uniform smoothing method improves translation quality.There is only a minor improvement, but it is consistentamong all evaluation criteria.
It is statistically signifi-cant at the 94% level.
The unigram method hurts perfor-mance.
There is a degradation of the mWER of 0.9%.
Inthe following, all phrase-based systems use the uniformsmoothing method.The translation results of the different systems areshown in Table 6.
Obviously, the monotone phrase-basedsystem outperforms the monotone single-word based sys-tem.
The result of the phrase-based system is comparableto the nonmonotone single-word based search with theIBM constraints.
With respect to the mPER, the PB sys-tem clearly outperforms all single-word based systems.If we compare the monotone phrase-based system withthe nonmonotone alignment template system, we see thatthe mPERs are similar.
Thus the lexical choice of wordsis of the same quality.
Regarding the other evaluationcriteria, which take the word order into account, the non-monotone search of the alignment templates has a clearadvantage.
This was already indicated by the low degreeof monotonicity on this task.
The rather free word orderin German and the long range dependencies of the verbgroup make reorderings necessary.Table 6: Translation performance [%] for the German-English Verbmobil task (251 sentences).system variant mWER mPER BLEU NISTSWB Mon 42.8 29.3 38.0 7.07IBM 37.1 25.0 47.8 7.84GE 35.4 25.3 48.5 7.83PB 37.0 20.7 47.0 7.99AT 30.3 20.6 56.8 8.577.4 Xerox taskThe translation results for the Xerox task are shown inTable 7.
Here, we see that both phrase-based systemsclearly outperform the single-word based systems.
ThePB system performs best on this task.
Compared to theAT system, the BLEU score improves by 4.1% absolute.The improvement of the PB system with respect to theAT system is statistically significant at the 99% level.Table 7: Translation performance [%] for the Spanish-English Xerox task (1125 sentences).System WER PER BLEU NISTSWB IBM 38.8 27.6 55.3 8.00PB 26.5 18.1 67.9 9.07AT 28.9 20.1 63.8 8.767.5 Canadian Hansards taskThe translation results for the Canadian Hansards task areshown in Table 8.
As on the Xerox task, the phrase-basedsystems perform better than the single-word based sys-tems.
The monotone phrase-based system yields evenbetter results than the alignment template system.
Thisimprovement is consistent among all evaluation criteriaand it is statistically significant at the 99% level.Table 8: Translation performance [%] for the French-English Canadian Hansards task (5432 sentences).System Variant WER PER BLEU NISTSWB Mon 65.2 53.0 19.8 5.96IBM 64.5 51.3 20.7 6.21PB 57.8 46.6 27.8 7.15AT 61.1 49.1 26.0 6.718 EfficiencyIn this section, we analyze the translation speed of thephrase-based translation system.
All experiments werecarried out on an AMD Athlon with 2.2GHz.
Note thatthe systems were not optimized for speed.
We used thebest performing systems to measure the translation times.The translation speed of the monotone phrase-basedsystem for all three tasks is shown in Table 9.
For theXerox task, the translation process takes less than 7 sec-onds for the whole 10K words test set.
For the Verbmobiltask, the system is even slightly faster.
It takes about 1.6seconds to translate the whole test set.
For the CanadianHansards task, the translation process is much slower, butthe average time per sentence is still less than 1 second.We think that this slowdown can be attributed to the largetraining corpus.
The system loads only phrase pairs intomemory if the source phrase occurs in the test corpus.Therefore, the large test corpus size for this task also af-fects the translation speed.In Fig.
1, we see the average translation time per sen-tence as a function of the sentence length.
The translationtimes were measured for the translation of the 5432 testsentences of the Canadian Hansards task.
We see a clearlinear dependency.
Even for sentences of thirty words,the translation takes only about 1.5 seconds.Table 9: Translation Speed for all tasks on a AMD Athlon2.2GHz.Verbmobil Xerox Hansardsavg.
sentence length 10.5 13.5 18.0seconds / sentence 0.006 0.007 0.794words / second 1642 1448 22.800.20.40.60.811.21.41.60  5  10  15  20  25  30timesentence lengthFigure 1: Average translation time per sentence as a func-tion of the sentence length for the Canadian Hansards task(5432 test sentences).9 Related WorkRecently, phrase-based translation approaches becamemore and more popular.
Some examples are the align-ment template system in (Och et al, 1999; Och and Ney,2002) that we used for comparison.
In (Zens et al, 2002),a simple phrase-based approach is described that servedas starting point for the system in this work.
(Marcuand Wong, 2002) presents a joint probability model forphrase-based translation.
It does not use the word align-ment for extracting the phrases, but directly generates aphrase alignment.
In (Koehn et al, 2003), various aspectsof phrase-based systems are compared, e.g.
the phraseextraction method, the underlying word alignment model,or the maximum phrase length.
(Tomas and Casacuberta,2003) describes a linear interpolation of a phrase-basedand an alignment template-based approach.10 ConclusionsWe described a phrase-based translation approach.
Thebasic idea of this approach is to remember all bilingualphrases that have been seen in the word-aligned train-ing corpus.
As refinements of the baseline model, wedescribed two simple heuristics: the word penalty fea-ture and the phrase penalty feature.
Additionally, we pre-sented a single-word based lexicon with two smoothingmethods.
The model scaling factors were optimized withrespect to the mWER on the development corpus.We described a highly efficient monotone search al-gorithm.
The worst-case complexity of this algorithm islinear in the sentence length.
This leads to an impressivetranslation speed of more than 1000 words per second forthe Verbmobil task and for the Xerox task.
Even for theCanadian Hansards task the translation of sentences oflength 30 takes only about 1.5 seconds.The described search is monotone at the phrase level.Within the phrases, there are no constraints on the re-orderings.
Therefore, this method is best suited for lan-guage pairs that have a similar order at the level of thephrases learned by the system.
Thus, the translation pro-cess should require only local reorderings.
As the exper-iments have shown, Spanish-English and French-Englishare examples of such language pairs.
For these pairs,the monotone search was found to be sufficient.
Thephrase-based approach clearly outperformed the single-word based systems.
It showed even better performancethan the alignment template system.The experiments on the German-English Verbmobiltask outlined the limitations of the monotone search.As the low degree of monotonicity indicated, reorderingplays an important role on this task.
The rather free wordorder in German as well as the verb group seems to be dif-ficult to translate.
Nevertheless, when ignoring the wordorder and looking at the mPER only, the monotone searchis competitive with the best performing system.For further improvements, we will investigate the use-fulness of additional models, e.g.
modeling the segmen-tation probability.
Also, slightly relaxing the monotonic-ity constraint in a way that still allows an efficient searchis of high interest.
In spirit of the IBM reordering con-straints of the single-word based models, we could allowa phrase to be skipped and to be translated later.AcknowledgmentThis work has been partially funded by the EU projectTransType 2, IST-2001-32091.ReferencesA.
L. Berger, P. F. Brown, S. A. D. Pietra, V. J. D. Pietra,J.
R. Gillett, A. S. Kehler, and R. L. Mercer.
1996.Language translation apparatus and method of usingcontext-based translation models, United States patent,patent number 5510981, April.P.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. DellaPietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, andP.
S. Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2):79?85,June.G.
Doddington.
2002.
Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics.
In Proc.
ARPA Workshop on Human LanguageTechnology.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of the Human Lan-guage Technology Conf.
(HLT-NAACL), pages 127?133, Edmonton, Canada, May/June.D.
Marcu and W. Wong.
2002.
A phrase-based, jointprobability model for statistical machine translation.In Proc.
Conf.
on Empirical Methods for Natural Lan-guage Processing, pages 133?139, Philadelphia, PA,July.H.
Ney, S. Martin, and F. Wessel.
1997.
Statistical lan-guage modeling using leaving-one-out.
In S. Youngand G. Bloothooft, editors, Corpus-Based Methodsin Language and Speech Processing, pages 174?207.Kluwer.F.
J. Och and H. Ney.
2002.
Discriminative training andmaximum entropy models for statistical machine trans-lation.
In Proc.
of the 40th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages295?302, Philadelphia, PA, July.F.
J. Och, C. Tillmann, and H. Ney.
1999.
Improvedalignment models for statistical machine translation.In Proc.
of the Joint SIGDAT Conf.
on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora, pages 20?28, University of Maryland, Col-lege Park, MD, June.F.
J. Och.
2003.
Minimum error rate training in statis-tical machine translation.
In Proc.
of the 41th AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 160?167, Sapporo, Japan, July.K.
A. Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
Technical Report RC22176 (W0109-022),IBM Research Division, Thomas J. Watson ResearchCenter, September.J.
Pearl.
1988.
Probabilistic Reasoning in IntelligentSystems: Networks of Plausible Inference.
MorganKaufmann Publishers, Inc., San Mateo, CA.
Revisedsecond printing.W.
H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P.Flannery.
2002.
Numerical Recipes in C++.
Cam-bridge University Press, Cambridge, UK.C.
Tillmann and H. Ney.
2003.
Word reordering and adynamic programming beam search algorithm for sta-tistical machine translation.
Computational Linguis-tics, 29(1):97?133, March.J.
Tomas and F. Casacuberta.
2003.
Combining phrase-based and template-based aligned models in statisti-cal translation.
In Proc.
of the First Iberian Conf.
onPattern Recognition and Image Analysis, pages 1020?1031, Mallorca, Spain, June.W.
Wahlster, editor.
2000.
Verbmobil: Foundationsof speech-to-speech translations.
Springer Verlag,Berlin, Germany, July.R.
Zens, F. J. Och, and H. Ney.
2002.
Phrase-based sta-tistical machine translation.
In 25th German Confer-ence on Artificial Intelligence (KI2002), pages 18?32,Aachen, Germany, September.
Springer Verlag.
