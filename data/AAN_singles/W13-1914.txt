Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 111?115,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsUsing Latent Dirichlet Allocation for Child Narrative AnalysisKhairun-nisa Hassanali and Yang LiuThe University of Texas at DallasRichardson, TX, USAnisa,yangl@hlt.utdallas.eduThamar SolorioUniversity of Alabama at BirminghamBirmingham, AL, USAsolorio@uab.eduAbstractChild language narratives are used for lan-guage analysis, measurement of languagedevelopment, and the detection of lan-guage impairment.
In this paper, we ex-plore the use of Latent Dirichlet Alloca-tion (LDA) for detecting topics from nar-ratives, and use the topics derived fromLDA in two classification tasks: automaticprediction of coherence and language im-pairment.
Our experiments show LDA isuseful for detecting the topics that corre-spond to the narrative structure.
We alsoobserved improved performance for theautomatic prediction of coherence and lan-guage impairment when we use featuresderived from the topic words provided byLDA.1 IntroductionLanguage sample analysis is a common techniqueused by speech language researchers to measurevarious aspects of language development.
Theseinclude speech fluency, syntax, semantics, and co-herence.
For such analysis, spontaneous narrativeshave been widely used.
Narrating a story or a per-sonal experience requires the narrator to build amental model of the story and use the knowledgeof semantics and syntax to produce a coherent nar-rative.
Children learn from a very early age to nar-rate stories.
The different processes involved ingenerating a narrative have been shown to provideinsights into the language status of children.There has been some prior work on child lan-guage sample analysis using NLP techniques.
Sa-hakian and Snyder (2012) used a set of linguisticfeatures computed on child speech samples to cre-ate language metrics that included age prediction.Gabani et al(2011) combined commonly usedmeasurements in communication disorders withseveral NLP based features for the prediction ofLanguage Impairment (LI) vs.
Typically Develop-ing (TD) children.
The features they used includedmeasures of language productivity, morphosyntac-tic skills, vocabulary knowledge, sentence com-plexity, probabilities from language models, stan-dard scores, and error patterns.
In their work, theyexplored the use of language models and machinelearning methods for the prediction of LI on twotypes of child language data: spontaneous and nar-rative data.Hassanali et al(2012a) analyzed the use ofcoherence in child language and performed auto-matic detection of coherence from child languagetranscripts using features derived from narrativestructure such as the presence of critical narrativecomponents and the use of narrative elements suchas cognitive inferences and social engagement de-vices.
In another study, Hassanali et al(2012b)used several coherence related features to auto-matically detect language impairment.LDA has been used in the field of narrative anal-ysis.
Wallace et al(2012) adapted LDA to the taskof multiple narrative disentanglement, in whichthe aim was to tease apart narratives by assigningpassages from a text to the subnarratives that theybelong to.
They achieved strong empirical results.In this paper, we explore the use of LDA forchild narrative analysis.
We aim to answer twoquestions: Can we apply LDA to children nar-ratives to identify meaningful topics?
Can werepresent these topics automatically and use themfor other tasks, such as coherence detection andlanguage impairment prediction?
Our results arepromising.
We found that using LDA topic model-ing can infer useful topics, and incorporating fea-tures derived from such automatic topics improvesthe performance of coherence classification andlanguage impairment detection over the previouslyreported results.111Coherence Scale TD LI TotalCoherent 81 6 87Incoherent 18 13 31Total 99 19 118Table 1: Number of TD and LI children on a 2-scale coherence level2 DataFor the purpose of the experiments, we used theConti-Ramsden dataset (Wetherell et al 2007a;Wetherell et al 2007b) from the CHILDESdatabase (MacWhinney, 2000).
This dataset con-sists of transcripts belonging to 118 adolescentsaged 14 years.
The adolescents were given thewordless picture story book ?Frog, where areyou??
and asked to narrate the story based on thepictures.
The storybook is about the adventures ofa boy who goes searching for his missing pet frog.Even though our goal is to perform child narrativeanalysis, we used this dataset from adoloscentssince it was publicly available, and was annotatedfor language impairment and coherence.
Of the118 adolescents, 99 adolescents belonged to theTD group and 19 adolescents belonged to the lan-guage impaired group.
Hassanali et al(2012a)annotated this dataset for coherence.
A transcriptwas annotated as coherent, as long as there was nodifficulty in understanding the narrative, and in-coherent otherwise.
Table 1 gives the TD and LIdistribution on a 2-scale coherence level.
Figure1 shows an example of a transcript produced by aTD child.Figure 1: Sample transcript from a TD child3 Narrative Topic Analysis Using LDALatent Dirichlet Allocation (LDA) (Blei et al2003) has been used in NLP to model topics withina collection of documents.
In this study, we useLDA to detect topics in narratives.
Upon exam-ining the transcripts, we observed that each topicwas described in about 3 to 4 utterances.
We there-fore segmented the narratives into chunks of 3 ut-terances, with the assumption that each segmentcorresponds roughly to one topic.We used the software by Blei et al to performLDA.
Prior to performing LDA, we removed thestop words from the transcripts.
We chose ?
tobe 0.8 and K to be 20, where ?
is the parameterof the Dirichlet prior on the per-document topicdistributions and K denotes the number of topicsconsidered in the model.We chose to use the transcripts of TD childrenfor generating the topics, because the transcripts ofTD children have fewer disfluencies, incompleteutterances, and false starts.
As we can observefrom Table 1, a higher percentage of TD childrenproduced coherent narratives when compared tochildren with LI.Table 2 gives the topic words for the top 10topics extracted using LDA.
The topics in Table2 were manually labeled after examination of thetopic words extracted using LDA.
We found thatsome of the topics extracted by LDA correspondedto subtopics.
For example, searching for the frogin the house has subtopics of the boy searchingfor the frog in room and the dog falling out of thewindow, which were part of the topics covered byLDA.
The subtopics are marked in italics in Table2.The following narrative components were iden-tified as important features for the automatic pre-diction of coherence by Hassanali et al(2012a).1.
Instantiation: introduce the main charactersof the story: the boy, the frog, and the dog,and the frog goes missing2.
1st episode: search for the frog in the house3.
2nd episode: search for the frog in the tree4.
3rd episode: search for the frog in the hole inthe ground5.
4th episode: search for the frog near the rock6.
5th episode: search for the frog behind thelog7.
Resolution: boy finds the frog in the river andtakes a frog homeUpon examining the topics extracted by LDA, weobserved that all the components mentioned above1http://www.cs.princeton.edu/ blei/lda-c/index.html112TopicNoTopic Words Used by TD Population Topic Described1 went,frog,sleep,glass,put,caught,jar,yesterday,out,house Introduction2 frog,up,woke,morning,called,gone,escaped,next,kept,realized Frog goes missing3 window,out,fell,dog,falls,broke,quickly,opened,told,breaking Dog falls out of window4 tree,bees,knocked,running,popped,chase,dog,inside,now,flying Dog chases the bees5 deer,rock,top,onto,sort,big,up,behind,rocks,picked Deer behind the rock6 searched,boots,room,bedroom,under,billy,even, floor,tilly,tried Search for frog in room7 dog,chased,owl,tree,bees,boy,came,hole,up,more Boy is chased by owl from atree with beehives8 jar,gone,woke,escaped,night,sleep,asleep,dressed,morning,frog Frog goes missing9 deer,top,onto,running,ways,up,rocks,popped,suddenly,know Boy runs into the deer10 looking,still,dog,quite,cross,obviously,smashes,have,annoyed Displeasure of boy with dogTable 2: Top 10 topic words extracted by LDA on the story telling task.
Subtopics are shown in italics.were present in these topics.
Many of the LDAtopics corresponded to a picture or two in the sto-rybook.4 Using LDA Topics for Coherence andLanguage Impairment ClassificationWe extended the use of LDA for two tasks,namely: the automatic evaluation of coherenceand the automatic evaluation of language impair-ment.
For the experiments below, we used theWEKA toolkit (Hall et al 2009) and built sev-eral models using the naive Bayes, Bayesian netclassifier, Logistic Regression, and Support Vec-tor Machine (SVM) classifier.
Of all these classi-fiers, the naive Bayes classifier performed the best,and we report the results using the naive Bayesclassifier in Tables 3 and 4.
We performed all theexperiments using leave-one-out cross-validation,wherein we excluded the test transcript that be-longed to a TD child from the training set whengenerating topics using LDA.4.1 Automatic Evaluation of CoherenceWe treat the automatic evaluation of coherenceas a classification task.
A transcript could eitherbe classified as coherent or incoherent.
We usethe results of Hassanali et al(2012a) as a base-line.
They used the presence of narrative episodes,and the counts of narrative quality elements suchas cognitive inferences and social engagement de-vices as features in the automatic prediction of co-herence.
We add the features that we automati-cally extracted using LDA.We checked for the presence of at least six ofthe ten topic words or their synonyms per topic ina window of 3 utterances.
If the topic words werepresent, we took this as a presence of a topic; oth-erwise we denoted it as an absence of a topic.
Intotal, there were 20 topics that we extracted usingLDA, which is higher compared to the 8 narrativestructure topics that were annotated for by Has-sanali et al(2012a).Table 3 gives the results for the automatic clas-sification of coherence.
As we observe in Table3, there is an improvement in performance overthe baseline.
We attribute this to the inclusion ofsubtopics that were extracted using LDA.4.2 Automatic Evaluation of LanguageImpairmentWe extended the use of LDA to create a summaryof the narratives.
For the purpose of generating thesummary, we considered only the narratives gen-erated by TD children in the training set.
We gen-erated a summary, by choosing 5 utterances cor-responding to each topic that was generated usingLDA, thereby yielding a summary that consistedof 100 utterances.We observed that different words were used torepresent the same concept.
For example, ?look?and ?search?
were used to represent the conceptof searching for the frog.
Since the narration wasbased on a picture storybook, many of the childrenused different terms to refer to the same animal.For example, ?the deer?
in the story has been inter-preted to be ?deer?, ?reindeer?, ?moose?, ?stag?,?antelope?
by different children.
We created anextended topic vocabulary using Wordnet to in-clude words that were semantically similiar to thetopic keywords.
In addition, for an utterance to be113Feature SetCoherent Incoherent Accuracy(%)Precision Recall F-1 Precision Recall F-1Narrative (Hassanali et al2012a) (baseline)0.869 0.839 0.854 0.588 0.645 0.615 78.814Narrative + automatic topicfeatures0.895 0.885 0.89 0.688 0.71 0.699 83.898Table 3: Automatic classification of coherence on a 2-scale coherence levelin the summary, we put in the additional constraintthat neighbouring utterances within a window of3 utterances also talk about the same topic.
Weused this summary for constructing unigram andbigram word features for the automatic predictionof LI.The features we constructed for the predictionof LI were as follows:1.
Bigrams of the words in the summary2.
Presence or absence of the words in the sum-mary regardless of the position3.
Presence or absence of the topics detected byLDA in the narratives4.
Presence or absence of the topic words thatwere detected using LDAWe used both the topics detected and the pres-ence/absence of topic words as features since thesame topic word could be used across several top-ics.
For example, the words ?frog?, ?dog?, ?boy?,and ?search?
are common across several topics.We refer to the above features as ?new features?.Table 4 gives the results for the automatic pre-diction of LI using different features.
As we canobserve, the performance improves to 0.872 whenwe add the new features to Gabani?s and the nar-rative structure features.
When we use the newfeatures by themselves to predict language impair-ment, the performance is the worst.
We attributethis to the fact that other feature sets are richersince these features take into account aspects suchas syntax and narrative structure.We performed feature analysis on the new fea-tures to see what features contributed the most.The top scoring features were the presence or ab-sence of the topics detected by LDA that corre-sponded to the introduction of the narrative, theresolution of the narrative, the search for the frogin the room, and the search for the frog behindthe log.
The following bigram features generatedfrom the summary contributed the most: ?deerFeature P R F-1Gabani?s (Gabani etal., 2011)0.824 0.737 0.778Narrative (Hassanali etal., 2012a)0.385 0.263 0.313New features 0.308 0.211 0.25Narrative + Gabani?s 0.889 0.842 0.865Narrative + Gabani?s +new features0.85 0.895 0.872Table 4: Automatic classification of language im-pairmentrock?, ?lost frog?, and ?boy hole?.
Using a subsetof these best features did not improve the perfor-mance when we added them to the narrative fea-tures and Gabani?s features.5 ConclusionsIn this paper, we explored the use of LDA in thecontext of child language analysis.
We used LDAto extract topics from child language narrativesand used these topic keywords to create a sum-mary of the narrative and an extended vocabu-lary.
The topics extracted using LDA not onlycovered the main components of the narrative butalso covered subtopics too.
We then used the LDAtopic words and the summary to create featuresfor the automatic prediction of coherence and lan-guage impairment.
Due to higher coverage of theLDA topics as compared to manual annotation, wefound an increase in performance of both auto-matic prediction of coherence and language im-pairment with the addition of the new features.
Weconclude that the use of LDA to model topics andextract summaries is promising for child languageanalysis.AcknowledgementsThis research is supported by NSF awards IIS-1017190 and 1018124.114ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
the Journal of ma-chine Learning research, 3:993?1022.Keyur Gabani, Thamar Solorio, Yang Liu, Khairun-nisa Hassanali, and Christine A. Dollaghan.
2011.Exploring a corpus-based approach for detect-ing language impairment in monolingual English-speaking children.
Artificial Intelligence inMedicine, 53(3):161?170.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.ACM SIGKDD Explorations Newsletter, 11(1):10?18.Khairun-nisa Hassanali, Yang Liu, and ThamarSolorio.
2012a.
Coherence in child language nar-ratives: A case study of annotation and automaticprediction of coherence.
In Proceedings of WOCCI2012 - 3rd Workshop on Child, Computer and Inter-action.Khairun-nisa Hassanali, Yang Liu, and ThamarSolorio.
2012b.
Evaluating NLP features for au-tomatic prediction of language impairment usingchild speech transcripts.
In Proceedings of INTER-SPEECH.Brian MacWhinney.
2000.
The CHILDES project:Tools for analyzing talk, Volume I: Transcription for-mat and programs.
Lawrence Erlbaum Associates.Sam Sahakian and Benjamin Snyder.
2012.
Automat-ically learning measures of child language develop-ment.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics:Short Papers-Volume 2, pages 95?99.
Associationfor Computational Linguistics.Bryon C. Wallace.
2012.
Multiple narrative disentan-glement: Unraveling infinite jest.
In Proceeding ofthe 2012 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 1?10.Danielle Wetherell, Nicola Botting, and Gina Conti-Ramsden.
2007a.
Narrative in adolescent specificlanguage impairment (SLI): a comparison with peersacross two different narrative genres.
InternationalJournal of Language & Communication Disorders,42(5):583?605.Danielle Wetherell, Nicola Botting, and Gina Conti-Ramsden.
2007b.
Narrative skills in adolescentswith a history of SLI in relation to non-verbal IQscores.
Child Language Teaching and Therapy,23(1):95.115
