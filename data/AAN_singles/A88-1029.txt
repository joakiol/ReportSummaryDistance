TIC : PARSING INTERESTING TEXT.David AllportSchool o/Cognitive SciencesUniversity of Sussex,Falmer,Brighton BN1 9QNdavida%ulc ,,c.mssex.cvaxa@cs -clac ad?ABSTRACtThis paper gives an overview of the naturallanguage problems addressed in the TrafficInformation CoHator/Condenscr (TICC) pro-jeer, and describes in some detai l  the"interesting-corner parser" used in the TICC'sNatural  Language Summariser.
The TICC isdesigned to take free text input describinglocal traffic incidents, and automatical ly  out-put local traffic information broadcasts formotorists in appropriate geographical areas.The "interesting-corner parser uses both syn-tactic and semantic information, representedas features in a unification-based grammar, toguide its bi-directional search for significantphrasal groups.1.
INTRODUCTIONThe overal l  goal of the TICC project isto show the potential  benefits of automati -cal ly broadcasting local traffic information.Our target system, dealing with trafficincidents in the Sussex area, is to be com-pleted by September 1989.
The project formspart of the A lvey Mobile Information Sys-tems large-scale Demonstrator.The Natural  Language Summariser com-ponent of this system is being developed atSussex University.
Its function is to accepta series of free text messages describingtraffic incidents, and to extract from thesemessages any information that might berelevant for broadcast o other motorists.The Natural  Language Summariser isdesigned to work in a restricted domain, andonly needs to solve a subset of the problemsof text understanding.
The TICC's outputmessages are short and very simple assem-blies of canned text, posing no significantnatural language generation problems.
Ourmain concern is that the messages hould beuseful to motorists, i.e that they be rel iableindications of the state of the roads at thetime they are broadcast.Programs such as METEO \[Chevalier etal.
1978\] have demonstrated that in a res-tr icted domain with a restricted sub-language,automatic information broadcasts can be use-fuL Programs such as FRUMP \[De Jong1979, De Jong 1982\] have also demonstratedthat expectation-driven analysers can oftensuccessful ly capture the gist of free text.However, the top-down depth-f irstconfirmation of expectations based on sketchyscripts, ignoring most of the input structure,can lead to serious misinterpretat ions \[Ries-beck 82\].
Our concern for accuracy ofinterpretation has led us to a processing stra-tegy in which the Natura l  Language Sum-mariser analyses the input text at a fargreater level of detai l  than is given in theoutput messages, so the system "knows more"about the traffic incidents it is describingthan it says in its broadcasts.
Our parseruses both syntactic and semantic informationto guide its search for phrases in the inputthat might be direct ly or indirect ly relevantto motorists, and explores alternat ive possibleinterpretations bottom-up using an activechart \[Farley 1970, Kay 1973\].This is an ongoing research project, andwe do not claim to have solved al l  theproblems involved in developing a successfulsystem yet.
The current paper considers thepart icular natural  language problems we areaddressing and describes the "interesting-corner parser" that has been implemented inthe prototype system.2.
THE NATURAL LANGUAGESUMMARISER'S TASK2.1 INPUT: Our input data comes fromthe Sussex Police, who have a computer sys-tem for storing the text of incoming trafficmessages from a var iety of sources (eg.patrol  cars, emergency services, motoringorganisations).
An example of the style ofthis text, derived from real input but withnames etc.
changed, is given in fig.1.The series of messages dealing with a211single incident continues over a number ofhours, depending on the severity of theincident, and the TICC can afford to spendup to one minute analysing an averagelength message.
ALl aspects of the policemanagement of the incident are described,and many of the messages are onlyindirectly relevant to motorists.
For example,if one of the vehicles involved in anaccident needs a total l i ft to remove it fromthe road, the l ikely delay time given in thebroadcast message may be longer, althoughthe need for the total l i ft  wilt  not itself bementioned in the broadcast.
Much of theinput is completely uninteresting for theTICC's purposes, such as details of injuriessustained by people involved, or of whichpolice units are dealing with the incident.There is a great variety in prose style,from the "normal" to the highly telegraphic,but there is a strong tendency towards theabbreviated.
It is a non-tr iv ial  task tocorrectly identify the lexical items in thetext.
Parts of the input string which are notrecognised as entries in the Summariser's lex-icon (or regular derivations from entries)may be of four types:i) Names, numbers etc, which may berecognised as such from the context (e.g pchumphr ies  requesta .... ford cortina regABC123).ii) Other English words not in the lexi-con, which cannot rel iably be predicted to beproper names (e.g hovis  /orry b/dwn o/s bull'shead ph).Misspellings of items in the lexicon.iv) Non-standard abbreviations of knownwords or phrases.Abbreviations are not always of "canoni-cal" form, and may be derived from com-plete words in three different ways, as fol-lows:i) Single morpheme roots: These usu-al ly have more than one possible abbreviatedform and never include punctuation eg.
gge,grg or gar for garage.
But some words dohave canonical abbreviations (eg ~-d for roadand st for street (or saint).LD Mu l t i -morpheme roots: These oftentake only the first letter from the first rootmorpheme, and then either part or all of thesecond morpheme.
They occasionally includeslash punctuation eg.
cway, c /way for car-rlageway, recycle, m/c  for trmtorcycle, o/s foroutalde (or offside), and ra for roundabout.Sequences / phrases: Some sequencesof words have canonical abbreviations (e.gbbc and not  britbrdcrp).
Canonical examplesseen in Fig.
1. below include rta for roadtraffc accident and oic for officer in charge.Non-canonical sequences may have avariety of abbreviations for each of the con-stituent words, and may or may not haveslash or period punctuation, eg.
f /b  for ritebrigade, eamb or eaamb for east (sussex) ambu-lance, hazchem for hazardous chemicals.The problem is compounded for theTICC by the fact that the input we receiveis all in upper case, hence the even the con-vention of distinguishing proper name abbre-viations by upper case is not applicable.
Inorder to cope with these different types ofinput string, we need not only a "phrasallexicon* as advocated by Becket \[Becket1975\], but also an *abbreviation lexicon".T/mei 1634 LocatWn: scaynes hill, haywards heath1634 rta serious near top scaynes hill persons trapped rqst esamb f /b  1/2 milesouth Jw freshfleld rd.1638 fm pc 123 acc Inv reqd poss black oic pc 4561639 fire and arab en route1642 req total l i f t  for saloon car rota garage1654 eamb now away from scene1655 freshfleld bodyshop on way1657 fm pc 456 req rd closed n and s of hill st crnr1658 req two traft units to assist re closures1709 can we inform brighton 1234 tell mr fred smith will be late due to th/s rta1715 local authority required loose paving stones1723 fm pc 234 at st george's hosp.
dr in charge having examlued mr jones nowfeels thls is not likely to be a black- driver of lorry has arrived, wil lprobably be released after treatment for cuts.
car 45 will be free from ._hesp in about 20 rainFig.
1.
An  extract from an example (:ctltlc~s) incident log.212Our aim is to have a unified process foridentifying idiomatic or fixed phrases andabbreviated sequences as in iJJ) above, so thatfor example as soon as pass, aaap and a.a.a.p.are all identified as the same "lexicaI item".Work on this is, however, at a prel iminarystage, and we have not yet found any gen-eral solution to the problem.2.2 SUMMARISATION: Deriving a shortbroadcast for motorists from a long series ofmessages uch as that in fig.
1 requires twomain phases.
First, the Natural LanguageSummariser must bui ld up a picture of whatis happening at the scene of the incident.Second, a Tactical Inferencer must decidewhat motorists should be told regarding theincident.The Natural Language Summarising pro-cess also requires two phases.
In the firstphase a Message Analyser extracts interestinginformation from a single message.
In thesecond phase an Event Recogniser putstogether the information from a series ofmessages to bui ld up a description of theincident as a whole, or rather those aspectsof the incident relevant to other motorists(see fig 2. below).The Message Analyser does not bui ld acomplete representation of the syntax andsemantics of messages uch as those at 1709and 1723 in fig.
1 above, since they have nobearing on the progress of the traffic incidentas far as other motorists are concerned.
Itjust searches for phrases describing "interest-ing" events.
These fall into two classes:P r imary  Events: Such as vehiclesblocking the road, substances spilling ontothe road, all or part of the road beingclosed, diversions being put into operation,garage coming to remove vehicles from theroad, services like fire brigade and countycouncil removing hazards, etc.The input messages rarely describe theseevents in full,  so the Event Recogniser mustinfer, for example, that if the local councilhas been called out to remove debris fromthe road, that at some time earlier debrismust have fal len on the road.Secondary  Events:  These includerequests that some of the pr imary eventsshould happen, and people being informedthat primary events have happened are hap-pening or wil l  happen.We wiLt not have any model of thebeliefs of the various agents involved inincident handling.
As far as the TICCNatural Language Summariser is concerned,the meaning of someone being informed thata pr imary event has happened is equivalentto the statement hat it has happened.
Butthe Tactical Inferencer wi l l  use its model ofthe typical progress of traffic incidents topredict the significance of the pr imary eventsfor other motorists.
For example, if a vehicleis stated to need a front suspended tow,then the Tactical Inferencer wi l l  predict thata certain amount of time wi l l  elapse beforethe vehicle is towed away.2.3 OUTPUT: Not every message inputto the system wil l  produce an update to theEvent Recogniser's description of the incident,because the Message Analyser may fail tofind a description of an interesting event.But even when the Event Recogniser passes adescription of a traffic incident to the Tacti-cal Inferencer, this wil l  not necessarily resultin a broadcast.
For example, the EventRecogniser may recognise a series of messagesas describing a traffic light failure incident.The Tactical Inferencer may decide to broad-cast a message about this incident if it hasoccurred on a busy main road in the rushhour, but not if it has occurred late at nightin a smaU village.Free TextMessages ~MessageI Road/Junction Database IAnalyser )----~ <Event Recogniser )------~<Taczical inferencer )I Incident Description Database I I Incident Database IMessages forBroadcasterFlg.
2.
Part of  the TICC system, showing Message Anal~ser,Event Recogniser, and Tactical Inflerencer.213The domain knowledge used in the theTactical Inferencer is non-l inguistlc, and con-cerns inferences about the l ike ly  t ime delaysfor different types of incident, the geographi-cal areas l ike ly  to be affected by  a givenincident, etc.
The Transport  and RoadResearch Laboratory,  part  of the Departmentof Transport,  are assisting us in the develop-ment of rules for this part  of the system.There are other components of the TICCsystem which we do not detaU in this paper,such as the graphical interface, via a map ofthe Sussex area, to a database of of currenttraffic incidents.
A l though the TICC isdesigned to send its messages to a dedicatedbroadcasting system, the actual broadcastingaspect of the project is the responsibi l i ty ofRACAL research, one of our other A lveycoIlaborators.
In our current prototype sys-tem, implemented on a Sun-3 workstat ion,broadcasts to local geographical areas inSussex are simulated, and the TacticalInferencer is extremely simple.3 .
I1V I 'E~ING CORNER PARSINGThe parser that has been implementedfor the Message Analyser  searches bidirection-a l ly  for aU syntactic parses associated withsemanticaUy interesting parts of the input.Before describing the search strategy in moredetail,  we need to c lar i fy  what  a syntacticparse looks l ike in our grammar formalism,and how we specify what  is semantical lyinteresting.3.1 THE GRAMMAR FORMALISM:  Weuse a unification-based grammar formal ism,with rules that look similar to context-freephrase-structure rules.
Both immediate domi-nance and constituent ordering informationare specified by the same rule, rather thanby separate statements as in FUG \[Kay1985\], LFG \[Kaplan & Bresnan 1982\] andGPSG \[Gazdar et at 1985\].
Feature-passingbetween categories in rules is done expl ic i t lywith logical variables, rather than by con-ventions such as the HFC and FFP in GPSG\[Gazdar et al1985\].
Thus the rule format ismost similar to that used in DCG's \[PereLra& Warren 1980\].
Categories in rules arefeature/value trees, and at each level thevalue of a feature may itself be anotherfeature/value tree.
Feature values may begiven logical names, and occurrences offeature values having the same logical namein a rule must unify.The feature trees which constitutecategories in our grammar may specify bothsyntactic and semantic features, so that wecan wr i te "syntactic" rules which also iden-t i fy  the semantic types of their constituents.For example, if  we use the feature sf oncategories to specify a tree of semanticfeatures for that category, then the rule:(1) vp=(s f :VSF) - -> v=(sf=(patient:P):VSF),n I~(sf :P)says that a verb phrase may consist of averb fol lowed by  a noun phrase, and thatthe semantic features on the noun phrase( label led t)) must un i fy  with the semanticfeatures specified as the value of the patientsub-feature of the verb's semantic features,and addi t ional ly  that  the semantic featureson the whole verb phrase ( label led VSF)must un i fy  wi th  the (complete tree of)semantic features on the verb.By adding domain-specif ic semanticfeature information to lexical categories, wegain the power of domain-specif ic semanticgrammars, which have been shown to be suc-cessful for handl ing i l l - formed input in l im-ited domains \[Burton 1976\].
But because weuse unification by extension as the basic cri-terion for node admissabi l i ty  when we testfor rules to licence local trees, we can alsocapture generalisations about syntacticcategories that are not domain-specific.
So forexample if we had a verb-phrase rule suchas (2) and a lexical entry  as in (3):(2) vp - ->  vffi(trftrans), np(3) close vffi(trffi(trans),sf=(event type=road_c losure,agentfservice,patientffiroadlocation))then the verb feature tree specihed in (2)would uni fy  with the verb feature tree in(3).
Hence close can be treated both as adomain specific verb and as an instance ofthe general class of transit ive verbs.Using a feature-based semantic grammartherefore gives us a compact representation ofboth domain independent and domain-specificinformation in a single uni form formal ism.Syntactic generalisations are captured byrules such as (2), and domain-specif ic sub-categorisation information is expressed infeature-trees as in (3), which states that closehas the semantic features of a road-closureevent, expecting an agent wi th  the semanticfeatures of a service (eg police) and a patientwith semantic features indicating (a part  of)a road.
As with al l  sub-languages, our214lexicon also includes domain-specific meaningsfor part icular lexical items, eg.
black mean-ing rata/  (cp messages at 1638 and 1723 infig.
1 above).3.2 A GRAMMAR FOR THE TICCDOMAIN: Writ ing a grammar to give ade-quate coverage of the input that our systemmust handle is a lengthy task, which wi l lcontinue over the next two years.
However,analysis of a corpus of data from police logsof over one hundred incidents in the Sussexarea, and tr ials with experimental grammars,have led us to adopt a style of grammarwhich we expect wi l l  remain constant as thegrammar expands.We do not attempt to map telegraphicforms onto "fuLly grammatical" English formsby some variant of constraint relaxation\[Kwasny & Sondheimer 1981\].
We s implyhave a grammar with fewer constraints.
Thisis because it is not a lways easy to decidewhat is missing from an el l iptical sentence,or which constraints should be relaxed.
Con-sider for example the message at 1655 fromfig.
1, repeated here:(4) freshfield bodyshop on wayIt is not at al l  clear what  the "full" senten-t im form of this message ought to be, sinceit might also have been phrased as one of:(5.1) freshfield bodyshop is on the way(5.2) freshfield bodyshop is on its way(5.3) freshfield bodyshop are on the way(5.4) freshfield bodyshop are on their wayEach of the (5.1)-(5.4) must be al lowedto be grammatical  (and each might occur inour type of input), since noun phrases nam-ing corporate entities can regular ly beregarded as singular or p lural  (cp.
FordMotors has announced massive profits ... vs.Ford Motors have announced massive profits).But in each case the semantic representationthat the Message AnMyser must bui ld onlyneeds to represent the fact that the garagecalled freshfield bodyshop are going some-where (which the Event Recogniser wi l lexpect to be the scene of the incident, inorder to remove the damaged vehicle).
Sincethe distinctions between the syntactic formsin these examples is irrelevant for our pur-poses, it would be a waste of the parser'seffort to introduce search and inference prob-lems in the attempt to map the syntax of(4) uniquely into the syntax of one or otherof the forms in (5).
Indeed it is moreappropriate for our purposes to regard onway as a domain-specific idiomatic phrase,equivalent to ?n route, entre etc (each ofwhich occur in s imilar contexts).In keeping wi th  this approach to i l l -formedness, our grammar contains manycategories (ie feature-trees), that would  notbe recognised as syntactic categories in gram-mars for normal  English, eg.
we have specialrules for phrases containing predicted unk-nowns such as names, car registrationnumbers, etc.
Our parser is looking forphrases describing events rather than sen-tences, and we wi l l  not necessari ly a lwaysassign a structure with a single "S" labelspanning aH the input message.As we noted in 3.1 above, the lexicalentries for words that suggest interestingevents include trees of semantic features thatspecify expected fillers for various roles inthese events.
These feature trees provideselectional restrictions useful for guiding theparse, but do not themselves constitute the"semantics" of the lexical entries.
Thesemantics are represented as f irst-order logicalexpressions in a separate field of the lexicalentry, and representations of the meaning ofphrases are bui l t  using semantic rules associ-ated with each syntactic rule, as phrases arecompleted in the bottom-up parse.3.3 THE SEARCH STRATEGY:Interesting-corner parsing is basical ly anadaptation of bottom-up chart parsing toal low is land-driving through the input string,whi lst  st i l l  parsing each indiv idual  rule uni-directionaUy.
This gives a maximal lyefficient parse for our goal of re l iablyextracting from the input aU and only theinformation that is relevant to other motor-ists.
This form of expectation-driven parsingdiffers from that used in earl ier script-basedsystems such as MARGIE \[Schank 1975\], ELI\[Riesbeck 1978\] and FRUMP in four ways:First, the interesting-corner parser usesan active chart to consider bottom-up al linteresting interpretations that might be givento an input message, rather than proceedingleft to r ight and filtering out later (r ight)candidate interpretations on the basis of ear-l ier ( left) context.Second, if there are no interesting lexicalitems in the input string, or if the onlyinteresting items occur at the (r ight) end ofthe input, there is no attempt to match aUthe leftmost items to a series of candidatescripts or frames using top-down expecta-tions.215Third, the expectations themselves areexpressed declarat ively in feature trees thatform part of the lexical categories, whichcontrol the search via standard unificationwith declarative rules, where previous sys-tems used procedural "requests" in the lexi-con.Fourth, our parser bui lds an explicitsyntactic tree for the input, albeit includingsemantic features, rather than by  bui lding asemantic representation "directly' .The interesting-corner parser checks thesemantic features on every lexical item inthe input to see if they are interesting, butthis is a far faster operation than testingmany times whether a series of lexical itemsmatches the expectations f rom a top-downscript.
This does assume that the parser canidenti fy what  the lexical items are, which isproblematic as we noted in section 2.1 above.But as we shal l  see, the interesting-cornerparser does use predictions about the presenceof lexical items wi th  part icular  features inits search, and hence is in no worse a posi-tion than a str ict ly  top-down parser asregards matching expectations to i l l - formedlexical items.3~.1 UNIDIRECTIONAL ISLAND-DRIVING: Is land-dr iv ing is useful for textwhere one needs to start  f rom clear lyidentifiable (and in our case, semantical lyinteresting) parts of the input and extend theanalysis f rom there to include other parts.But parsing ru les  bi -d irect ional ly  isinherently inefficient.
Consider, for example, achart parse of the input string a b given asingle rule:c=> ab .A standard bottom-up left - to-r ight  activechart parse of this input would  create threenodes (1 a 2 b 3) two active edges (anempty one at node 1 and one f rom nodes 1to 2) and one inactive edge ( f rom node 1 to3).But a bi-directional parse, al lowing therule to be indexed at any point, would bui lda total of 7 active edges (one empty one ateach node, and 2 pairs with one constituentfound, bui l t  in different directions, ie 5 dis-t inct edges).
It would  also bui ld the sameinactive edge in two different directions.
Fora rule with three daughters, a bidirectionalparse produces 14 active edges (9 of whichare dist inct) and again 2 inactive edges.This redundancy in structure-bui ld ingcan be removed by incorporating constituentsinto ru les  unidirect ional ly  whi lst  st i l l  parsingthe text  bidirectionalAy.
We do this byindexing each rule on either le f t -most  orr ight-most daughter,  and parsing in a uniquedirection away from the indexed daughter.In order to preserve completeness in thesearch, the chart must contain l ists of activeand inactive edges for each direction ofexpansion, although the same structure canbe shared in the inactive edge-l ists for bothdirections.
The fundamenta l  rule of edge-combination must be augmented so that whenan inactive edge is added to the chart, itcombines wi th  any appropriate active edgesat both of its ends.
This process might be?
m , called "indexed-corner pars ing ,  m that iteffectively combines left-corner parsing andr ight-corner parsing, and the direction ofparse at any stage s imply  depends upon howthe indiv idual  grammar rules are indexed.The interesting-corner parser implementsan indexed-corner chart  parser, wi th  theaddit ion of an agenda control  mechanism andan indexing principle for grammar rules.3.3.2 AGENDA CONTROL: The insertionof edges into the agenda is constrained bythe value of a "control-feature N, whichspecifies where to look in the feature-treesthat constitute our categories in order to findthe semantical ly ~interesting ~ features.
Inour examples (1) and (2) above, thiscontrol=feature is named sf.
When a normalbottom-up chart parse begins, al l  lexicalitems are tested to see whether they canspawn higher edges.
But in the interesting-corner parse, higher edges are only spawnedfrom lexical items that have a control - featurespecification which unifies wi th  a pre=definedinit ial  value of the control feature.
Thus byassigning (s f -event__ type)  to be the init ialvalue of the control feature, we ensure thatonly  those edges are entered into the agendathat have semantic feature trees that areextensions of this tree (eg the semanticfeature tree for close in (3) above).
Thiseffectively means that parsing must  beginfrom words that suggest some k ind ofinteresting event.
Note that the init ial  activeedges may be proposed from any point inthe input string, and their direction ofexpansion from that point is determined bythe indexing on the rules.For al l  active edges proposed from lexi-cal items that were in i t ia l ly  recognised to beinteresting, the parser checks the l ist of edgessought for "interesting" categories (ie.
thosewith values for the control - feature sf).
I f216there are any, it searches, in the direction ofexpansion for the current active edge, forany lexical items that have a semanticfeature-tree which unifies with the newspecification of what is "interesting'.For example, if the rule given in (1)above is indexed on the left daughter, andan active edge is proposed starting from aninactive edge representing the lexical itemclose defined as in (3) above, then via thelogical name P the features on the noun-phrase being sought become instantiated to(sf -roadlocat ion).
The parser then looksrightwards in the input string for any lexicalitems having semantic feature trees that areextensions of this new tree.
If it finds any,it predicts more active edges from there, andso forth.Fig.
3 below il lustrates the numericalorder in which the interesting-corner parserincorporates nodes into the parse tree for avery simple "sentence" (in our grammar weallow sentences with deleted auxiliaries), butwith the details of the feature trees omittedfor legibility.Extension unification allows one of thestructures to be unified (the target) to be anextension of the other (the pattern), but notvice-versa.
This means that it is more res-tricted than graph unification, and hence canbe implemented mote efficiently.
It is lessrestricted than term unification, and henceless efficient at parse-time, but it does allowthe grammar and lexicon to be far morecompact than they would be with term-unification in the absence of a grammar pre-processor.
However, using extensionunification as the basic operation does alsomean that that the unification of logicalvariables in rules is not order-independent,and hence we need an indexing principle todetermine the direction in which particularrules should be parsed.3.3.3 THE INDEXING PRINCIPLF: Ourgeneral principle for indexing rules is thatwe must parse from categories that specifygeneral information (ie.
that have smallfeature-trees) to those that specify particularmodifications of that general information (ie.that provide extensions to the smaller treesby unification).
This usual ly means that weparse from syntactic heads to complements,eg indexing sentences on the vp (cf.
HPSG\[Proudian & Pollard 1985\]).In our example rule (1), we index onthe verb, because its expectations pecify thegeneral semantic type of the object, and thesemantic feature tree of the noun-phrase wi l lspecify a sub-type of this general type, andtherefore wiLt be an extension of the verb'spatient semantic feature tree.
In the exampleshown in fig 3, the semantic tree of the npbuilt  at node 4 is:( s f - ( road locat ion - (name-hunt ingdon,r t i t le - lane) ) )which unifies by extension with the featuretree (sf - roadlocat ion)) ,  and this as we sawabove became the expected semantic tree forthe noun-phrase when rule (1) unified withthe verb in (3).Final ly, rules for categories that haveexpected unknowns as daughters are alwaysindexed on the known categories, even ifthese ate not the grammatical head (eg weindex on the policeman's title for rules han-dling sgt smith, irtsp brown etc.
and on theknown title of a road for cases like hunting-don lane.
markworthy avenue etc.3.3.4 EXTENSIONS TO THE CURRENTSYSTEM: There are many aspects of theTICC's Natural Language Summarisation otdealt with in this paper, such as the seman-tic rules used in the Message Analyser and10 s7 poltitle unknown 8I Ipc chisholm/v lIclosing5np ~ 4nlunknown 3 nI IhuntingdonFig.
3.
Showing the order tn which the Interesting-corner parserconatructs a parse tree, starting with the mast interesting words.217the Event Recogniser.
There are also manyinadequacies in the current implementation ofthe Message Analyser, eg in its handling ofabbreviations/phrases, and in the handling ofinput that is "ill-formed" even with respectto our relatively unconstrained grammar.However, work is currently in progresson these problems, and we believe that thebasic mechanisms of interesting-corner parsingare sufficiently powerful to enable us toachieve a practical solution, whilst beingsufficiently general to ensure that such asolution will be theoretically interesting.4.
CONCLUSIONThe automatic production of trafficbroadcasts, given the type of free text wehave described in this paper, poses manydifficult problems.
In many ways our overallapproach to these problems follows in a longtradition of semantically driven systems, butthe processing style of our Message Analyseris much closer to that used in contemporarysyntax-driven systems.
We make explicit useof rules in a unification-based grammaticalformalism that express both semantic andsyntactic information declaratively, and ourinteresting-corner parser provides a search ofthe input messages that is both thorough andefficient.We believe that complete understandingof free text messages is well beyond thestate of the art in computational linguistics,but that we can nevertheless develop theTICC's Natural Language Summariser to havesufficient partial understanding to be practi-cally useful.REFERENCF~Becket, J.D.
(1975) *The Phrasal Lexi-con', in R. C. Schank and B. L. Nash-Webber (eds.
), proceedings of the Workshop onTheoretical Issues in Natural Language Pro-cessing.
Cambridge, Mass., Bolt, Beranek andNewman, pp.
70-73.Burton, R. (1976) "Semantic Grammar: anEngineering Technique for ConstructingNatural Language Understanding Systems",Technical Report 3453, Cambridge, Mass.,Bolt, Beranek and Newman.Chevalier, M., Dansereau, J., and Poulin,G.
(1978) "TAUM-METEO: Description DuSysteme.
', Montreed, Groupe TAUM, Univer-site de MontrealDeJong, G.F. (1979) *Skimming Stories inReal Time", Doctoral Thesis, New Haven,Yale University.DeJong, G.F. (1982) *An Overview of theFRUMP System', in Wendy O. Lehnert andMartin H. Ringle (eds.
), Strategies for NaturalLanguage Processing.
Hillsdale, Erlbaum, pp.149-176.Earley, J.
(1970) "An Efficient Context-free Parsing Algorithm", Communications ofthe ACM.
vol.
6, no.
8, pp.
451-455.Gazdar, O., Klein, E., PuHum, G., andSag, I.
(1985) GeneraZlsed Phrase Strt~c~tweGrammar.
Oxford, Blackwell.Kaplan, R., and Bresnan, J.
(1982) "Lexi-cal Functional Grammar: a Formal Systemfor Grammatical Representation', in JoanBresnan (ed.
), The Menta2 Representation ofGrammatical Relations.
Cambridge MA, MITPress, pp.
173-281.Kay, M. (1973) "The MIND System', inRandall Rustin (ed.
), Natural Language Pro-cessing.
New York, Algorithmics Press, pp.155-188.Kay, M. (1985) "Parsing in FunctionalUnification Grammar", in David R. Dowty,Lauri Karttunen and Arnold M.
Zwicky(eds.
), Natural Language Parsing.
Cambridge,Cambridge University Press, pp.
251-278.Kwasny, S.C., and Sondheimer, N.K.
(1981) "Relaxation Theories for Parsing Ill-formed Input', American \]~nnal of Computa-tional Linguistics.
vol.
7, no.
2, pp.
99-108.Pereira, F.C.N., and Warren, D.H.D.
(1980) *Definite Clause Grammars forLanguage Analysis - a Survey of the Formal-ism and a Comparison with AugmentedTransition Networks", Artificial Intelligence.vol.
13, no.
3, pp.
231-278.Proudian, D., and PoLlard, C.J.
(1985)"Parsing Head-driven Phrase Structure Gram-mar',  ACL Proceedings, 23rd Annual Meeting.pp.
167-171.Riesbeck, C.K.
(1978) *An Expectation-driven Production System for NaturalLanguage Understanding', in Donald A.Waterman and Rick Hayes-Roth (eds.
),Pattern-directed Inference Systems.
New York,Academic Press, pp.
399-414.Riesbeck, C.K.
(1982) "Realistic LanguageComprehension", in Wendy G. Lehnert andMartin H. Ringle (eds.
), Strategies for NaturalLanguage Processing.
HHlsdale, Erlbaum, pp.37-54.Schank, R.C.
(1975) Conceptual Informa-tion Processing.
Amsterdam, North-Holland.218
