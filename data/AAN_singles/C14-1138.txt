Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1457?1467, Dublin, Ireland, August 23-29 2014.A Beam-Search Decoder for Disfluency DetectionXuancong Wang1,3Hwee Tou Ng1,2Khe Chai Sim21NUS Graduate School for Integrative Sciences and Engineering2Department of Computer Science, National University of Singapore3Human Language Technology, Institute for Infocomm Research, Singaporexuancong84@gmail.com, {nght, simkc}@comp.nus.edu.sgAbstractIn this paper1, we present a novel beam-search decoder for disfluency detection.
We first pro-pose node-weighted max-margin Markov networks (M3N) to boost the performance on wordsbelonging to specific part-of-speech (POS) classes.
Next, we show the importance of measur-ing the quality of cleaned-up sentences and performing multiple passes of disfluency detection.Finally, we propose using the beam-search decoder to combine multiple discriminative modelssuch as M3N and multiple generative models such as language models (LM) and perform multi-ple passes of disfluency detection.
The decoder iteratively generates new hypotheses from currenthypotheses by making incremental corrections to the current sentence based on certain patternsas well as information provided by existing models.
It then rescores each hypothesis based onfeatures of lexical correctness and fluency.
Our decoder achieves an edit-word F1 score higherthan all previous published scores on the same data set, both with and without using externalsources of information.1 IntroductionDisfluency detection is a useful and important task in Natural Language Processing (NLP) because spon-taneous speech contains a significant proportion of disfluency.
The disfluencies in speech introduce noisein downstream tasks like machine translation and information extraction.
Thus, the task of disfluencydetection not only can help improve the readability of automatically transcribed speech, but also theperformance of downstream NLP tasks.There are mainly two types of disfluencies: filler words and edit words.
Filler words include filledpauses (e.g., ?uh?, ?um?)
and discourse markers (e.g., ?I mean?, ?you know?).
They are insertions inspontaneous speech that indicate pauses or mark boundaries in discourse.
Thus, they do not conveyuseful content information.
Edit words are words that are spoken wrongly and then corrected by thespeaker.
For example, consider the utterance:I want a flightEdit?
??
?to BostonFiller?
??
?uh I meanRepair?
??
?to DenverThe phrase ?to Boston?
forms the edit region to be replaced by ?to Denver?.
The words ?uh I mean?
arefiller words that serve to cue the listener about the error and subsequent correction.
So, the cleaned-upsentence would be ?I want a flight to Denver?, which is what the speaker originally intended to say.
Ingeneral, edit words are more difficult to detect than filler words, and so edit word prediction accuracy ismuch lower.
Thus, in this work, we mainly focus on edit word detection.In Section 2, we briefly introduce previous work.
In Section 3, we describe our improved baselinesystem that will be integrated into our beam-search decoder.
Section 4 presents our beam-search decoderin detail.
In Section 5, we describe our experiments and results.
Section 6 gives the conclusion.1The research reported in this paper was carried out as part of the PhD thesis research of Xuancong Wang at the NUSGraduate School for Integrated Sciences and Engineering.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/14572 Previous WorkResearchers have tried many models for disfluency detection.
Johnson and Charniak (2004) proposed aTAG-based (Tree-Adjoining Grammar) noisy channel model, which showed great improvement over aboosting-based classifier (Charniak and Johnson, 2001).
Maskey et al.
(2006) proposed a phrase-levelmachine translation approach for this task.
Liu et al.
(2006) used conditional random fields (CRFs) (Laf-ferty et al., 2001) for sentence boundary and edit word detection.
They showed that CRFs significantlyoutperformed maximum entropy models and hidden Markov models (HMM).
Zwarts and Johnson (2011)extended this model using minimal expected F-loss oriented n-best reranking.
Georgila (2009) presenteda post-processing method during testing based on integer linear programming (ILP) to incorporate localand global constraints.
In addition to textual information, prosodic features extracted from speech havebeen incorporated to detect edit words in some previous work (Kahn et al., 2005; Liu et al., 2006; Zhanget al., 2006).
Zwarts and Johnson (2011) also trained extra language models on additional corpora, andcompared the effects of adding scores from different language models as features during reranking.
Theyreported that the language models gained approximately 3% in F1-score for edit word detection on theSwitchboard development dataset.
Qian and Liu (2013) proposed multi-step disfluency detection usingweighted max-margin Markov networks (M3N) and achieved the highest F-score of 84.1% without usingany external source of information.
In this paper, we incorporate the M3N model into our beam-searchdecoder framework with some additional features to further improve the result.3 The Improved Baseline SystemWeighted max-margin Markov networks (M3N) (Taskar et al., 2003) have been shown to outperformCRF in (Qian and Liu, 2013), since it can balance precision and recall easily by assigning different losspenalty to different label misclassification pairs.
In this work, we made use of M3N in expanding thesearch space and rescoring the hypotheses.
To facilitate the integration of the M3N system into ourdecoder framework, we made several modifications that slightly improve the M3N baseline system.
Ourimproved baseline system has two stages: the first stage is filler word prediction using M3N to detectwords which can potentially be fillers, and the second stage is joint edit and filler word prediction usingM3N.
The output of the first stage is passed as features into the second stage.
Both stages performfiller word prediction, since we found that joint edit and filler word detection performs better than editword detection alone as edit words tend to co-occur with filler words, and the first-stage output can befed into the second stage to extract additional features.
We also augmented the M3N toolkit to supportadditional feature functions, allow weighting of individual nodes, and control the total number of modelparameters.23.1 Node-Weighted and Label-Weighted Max-Margin Markov Networks (M3N)A max-margin Markov network (M3N) (Taskar et al., 2003) is a sequence labeling model.
It has thesame structure as conditional random fields (CRF) (Lafferty et al., 2001) but with a different objectivefunction.
A CRF is trained to maximize the conditional probability of the true label sequence given theobserved input sequence, while an M3N is trained to maximize the difference between the conditionalprobability of the true label sequence and the incorrectly predicted label sequence (i.e., maximizing themargin).
Thus, we can regard M3N as a support vector machine (SVM) analogue of CRF (Suykens andVandewalle, 1999).The dual form of the training objective function of M3N is formulated as follows:min?12C??x,y?x,y?f(x,y)?22+?x,y?x,yL(x,y)s.t.
?y?x,y= 1,?x and ?x,y?
0, ?x,y(1)2The source code of our augmented M3N toolkit can be downloaded at http://code.google.com/p/m3n-ext/1458where x is the observed input sequence, y ?
Y is the output label sequence, ?x,yare the dual variablesto be optimized, and C ?
0 is the regularization parameter to be tuned.
?f(x,y) = f(x,?y) ?
f(x,?y)is the residual feature vector, where?y is the true label sequence,?y is the predicted label sequence giventhe model, and f(x,y) is the feature vector.
It is implemented as a sum over all nodes:f(x,y) =?tf(x,y, t) (2)where t is the position index of the node in the sequence.
Each component of f(x,y, t) is a featurefunction, f(x,y, t).
For example, f(w0=?so?, y0=?F?, y?1=?O?, t) has a value of 1 only when the word atnode t is ?so?, the label at node t is ?F?
(filler word), and the label at node (t?1) is ?O?
(outside edit/fillerregion, i.e., fluent).
The maximum length of the y history (for this feature function, it is 2 since only y0and y?1are covered) is called the clique order of the feature.
L(x,y) is the loss function.
A standardM3N uses an unweighted hamming loss, which is the number of incorrect nodes:L(x,y) =?t?
(y?t, y?t) where ?
(a, b) ={1, if a = b0, otherwise(3)Qian and Liu (2013) proposed using label-weighted M3N to balance precision and recall by adjustingthe penalty on false positives and false negatives, i.e., v(y?t, y?t) in Eqn.
4.
In this work, we further extendthis technique to individual nodes to train expert models, each specialized in a specific part-of-speech(POS) class.
Our loss function is:L(x,y) =?tuc(t)v(y?t, y?t)?
(y?t, y?t) where uc(t) ={Bc, if POS(t) ?
Sc1, otherwise(4)whereBcis a factor which controls the extent to which the model is biased to minimize errors on specificnodes, POS(t) is the POS tag of the word at node t, and Scis the set of POS tags corresponding to thatexpert class c. We show that by integrating these expert models into our decoder framework, we canachieve further improvement.3.2 FeaturesThe feature templates for filler word and edit word prediction are listed in Table 1 and Table 2 re-spectively.
wirefers to the word at the ithposition relative to the current node; window size is themaximum number of words before and after the current word that the template covers, e.g., w?1w0with a window size of 4 means w?4w?3, w?3w?2, ..., w3w4; pirefers to the POS tag at the ithpo-sition relative to the current node; wi?jrefers to any word from the ithposition to the jthpositionrelative to the current node; wi, 6=Frefers to the ithword (w.r.t.
the current node) not being a fillerword; the multi-pair comparison function I(a, b, c, ...) indicates whether each pair (a and b, b and c,and so on) are identical, for example, if a = b 6= c = d, it will output ?101?
(?1?
for being equal, ?0?for being unequal); and ngram-score features are the natural logarithm of the following probabilities:P (w?3, w?2, w?1, w0), P (w0|w?3, w?2, w?1), P (w?3, w?2, w?1), P (?/s?|w?3, w?2, w?1), P (w?3),P (w?2), P (w?1), P (w0) (??/s??
denotes sentence-end).
We use language models (LM) in two ways:individual n-gram scores as M3N features, and an overall sentence-level score for rescoring in our beam-search decoder.
Our experiments show that this way of using LM gives the best performance.We set the frequency pruning threshold to 5, so that the resulting model has about the same totalnumber of parameters (7.6M) as (Qian and Liu, 2013).
The clique order for each template is determinedby considering the total number of features given that template.
For example, for pause duration, there are10 features (after cumulative binning), so we can set its clique order to 3 since there will be 10?33= 270weights; but for word 3-grams, there are 5M features, so setting its clique order to 3 or 2 will give rise totoo many weights (5M ?
33= 135M for order 3; 5M ?
32= 45M for order 2), thus we will reduce itsclique order to 1.
The same principle applies to other feature templates.1459Feature Template Window Size Clique Orderw04 1w?1w04 2I(wi, wj) 10 2I(wi, wj, wi+1, wj+1) 10 2p04 1p?1p04 2p?2p?1p04 2I(pi, pj) 10 2I(pi, pj, pi+1, pj+1) 10 2transitions 0 3Table 1: Feature templates for filler word predictionFeature Template Window Size Clique Orderw?2w?1w04 2I(wi, wj)(wiif wi=wj) 10 2w0w?6?
?1, w0w1?60 1I(pi, pj) 10 3I(pi, pj)(piif pi=pj) 10 3p?1w02 2w?1p02 2w?2, 6=Fw?1, 6=F0 2w?3, 6=Fw?2, 6=Fw?1, 6=F0 2p?2, 6=Fp?1, 6=F0 2p?3, 6=Fp?2, 6=Fp?1, 6=F0 2ngram-score features 0 3pause duration before w00 3pause duration after w00 3all features for filler word prediction same sameTable 2: Feature templates for edit word prediction4 The Beam-Search Decoder Framework4.1 MotivationThere are several limitations in the current M3N or CRF approach.
Firstly, current models do not measurethe quality of the cleaned-up sentences, i.e., the resulting sentence after removing all predicted filler andedit words.
Secondly, one pass of disfluency detection may not be sufficient to detect all disfluencies.Qian and Liu (2013) showed that we can improve the performance significantly by running a secondpass of edit detection.
Our preliminary experiments also show that additional passes of edit detectionfurther improve the performance.
Lastly, we find that edit word detection accuracy differs significantlyon words of different POS tags (Table 3).
This is because words of different POS tags have differentfeature distributions.
Thus, depending on the POS tag of the current word, the same feature may havedifferent implications for disfluency.
For example, consider the feature I(p0, p2).
When the current wordis a determiner, the feature does not strongly suggest an edit word.
In ?You give me a book, a pen,a pencil, ...?, the determiner ?a?
gets repeated.
However, when the current word is a verb, the featurestrongly suggests it is an edit word.
In ?The hardest thing for us has been is to ...?, both ?has?
and ?is?are third-person singular verbs.
Hence, it might be helpful if we train expert models each specializedin detecting edit words belonging to a specific POS and combine them dynamically according to thePOS.
Motivated by the beam-search decoder for grammatical error correction (Dahlmeier and Ng, 2012)and social media text normalization (Wang and Ng, 2013), we propose a novel beam-search decoder for1460disfluency detection to overcome these limitations.POS Freq.
(%) Edit F1 (%)PRP 25.5 92.33DT 14.2 88.95IN 10.4 84.45VBP 8.3 86.88RB 7.1 81.78CC 4.6 86.76BES 4.2 93.37NN 3.4 52.30VBD 3.1 86.51VB 2.1 70.42VBZ 1.9 79.70... ... ...Table 3: Baseline edit F1 scores for different POS tags4.2 General FrameworkThe goal of the decoder is to find the best hypothesis for a given input sentence w. A hypothesis his a label sequence, one label for every word in the sentence.
To find the best hypothesis, the decoderiteratively generates new hypotheses from current ones using a set of hypothesis producers and rescoreseach hypothesis using a set of hypothesis evaluators.
For each hypothesis produced, the decoder cleansup the sentence by removing all the predicted filler words and edit words so that subsequent operationscan act on the cleaned-up sentence w?
if needed.
Each hypothesis evaluator produces a score f whichmeasures the quality of the current hypothesis based on certain aspects of fluency specific to that hypoth-esis evaluator.
The overall score of a hypothesis is the weighted sum of the scores from all the hypothesisevaluators:score(h,w) =?i?ifi(h,w) (5)The weights ?is are tuned on the development set using minimum error rate training (MERT) (Och,2003).
The decoding algorithm is shown in Algorithm 1.In our description, hidenotes the hypothesized label at the ithposition; widenotes the word at theithposition; |h| denotes the length of the label sequence; fM3N(hi,w) denotes the M3N log-posteriorprobability of the label (at the ithposition of hypothesis h) being the hypothesized label; fM3N(h,w)denotes the normalized joint log probability of hypothesis h given the M3N model (?normalized?
meansdivided by the length of the label sequence); w?
denotes the cleaned-up sentence; fLM(w?)
= fLM(h,w)denotes the language model score of the sentence w?
(cleaned up according to hypothesis h) divided bysentence length; and?h denotes the sub-hypothesis obtained by running M3N on the cleaned-up sentencewith updated features.
Note that a sub-hypothesis will have a shorter label sequence if some wordsare labeled as filler word or edit word in the parent hypothesis.
We can obtain h from?h by insertingall predicted filler and edit words from the parent hypothesis into the sub-hypothesis so that its labelsequence has the same length as the original sentence.4.3 Hypothesis ProducersThe goal of hypothesis producers is to create a search space for rescoring using various hypothesis eval-uators.
Based on the information provided by the existing models and certain patterns where disfluenciesmay occur, we propose the following hypothesis producers for our beam-search decoder:Confusable-phrase-dictionary: The motivation of using this hypothesis producer is to hypothesizelabels for phrases which are commonly misclassified in the development data.
We build a dictionary of1461Algorithm 1The beam-search decoding algorithm for a sentence.
S: hypothesis stack; h: hypothesis; f : hypothe-sis evaluator score vector; ?
: hypothesis evaluator weight vectorINPUT: a sentence w with N wordsOUTPUT: a sequence of N labels, from {E, F, O}1: initialize hypothesis h0, hi=?O?
?i ?
[1, N ]2: SA?
{h0}, SB?
?3: for iter = [1, maxIter] do4: for each h in SAdo5: for each producer in hypothesisProducers do6: for each h?in producer(h) do7: compute f(h?,w) from hypothesisEvaluators8: compute score(h?,w) = ?>?
f(h?,w)9: SB?
SB+ {h?
}10: prune SBaccording to score11: SA?
SB, SB= ?12: return argmaxh{score(h,w)},h ?
SAphrases (up to 5 words) and their corresponding true labels by considering the most frequent incorrectlypredicted phrases in the development set.
During decoding, whenever such a phrase occurs in a sentenceand its label is not the same as that in the dictionary, it is changed to that in the dictionary and a newhypothesis is produced.
For example, if the phrase ?you know?
has occurred 1144 times and has beenmisclassified 31 times, out of which 9 times it should be ?O?, then an entry ?you know O || 1144 31 9?will be added to the dictionary.
If the original sentence contains ?you know?
and it is not labeled as O, anew hypothesis will be generated by labeling it as ?O?.Repetition: Whenever the ithword and the jthword (j > i) are the same, all words from the ithposition (inclusive) to the jthposition (exclusive) are labeled as edit words.
For example, in ?I want tobe able to um I just want it more for multi-tasking?, three hypotheses are produced.
The first hypothesisis produced by labeling every word in ?I want to be able to um?
as edit words since ?I?
is repeated.
Thesecond hypothesis is produced by labeling every word in ?want to be able to um I just?
as edit wordssince ?want?
is repeated.
The third hypothesis is produced by labeling ?to be able?
as edit words since?to?
is repeated.
The window size within which we search for repetitions is set to 12 (i.e., j ?
i ?
12),since the longest edit region (due to repetition) in the development set is of that size.
We introduce thishypothesis producer because the baseline system tends to miss long edit regions, especially when veryfew words in the region are repeated.
However, sometimes a speaker does change what he intends to sayby aborting a sentence so that only the beginning few words are repeated, as in the above sentence.Filler-word-marker: We trained an M3N model for filler word detection.
Multiple passes of fillerword detection on cleaned-up sentences can sometimes detect filler words that are missed in earlierpasses.
This hypothesis producer runs before every iteration starts.
It performs filler word prediction andmodifies the feature table by setting the filler-indicator feature to true so that subsequent operations seethe updated feature.
However, it does not remove filler words during the clean up process because somewords are defined as both filler word and edit word simultaneously.Edit-word-marker: We run our baseline M3N (the second stage) on the cleaned-up sentence andobtain the N -best hypotheses, i.e., the top N hypotheses h with max{fM3N(?h, w?)}.
This produceressentially performs multiple passes of disfluency detection.4.4 Hypothesis EvaluatorsOur decoder uses the following hypothesis evaluators to select the best hypothesis:Fluent language model score: This is the normalized language model score of the cleaned-up sen-tence, i.e., ffluentLM(w?).
A 4-gram language model is trained on the cleaned-up version of the training1462texts (both filler words and edit words are removed).
This score measures the fluency of the resultingcleaned-up sentence w.r.t.
a fluent language model.Disfluent language model score: This is the normalized language model score of the cleaned-up sen-tence, i.e., fdisfluentLM(w?).
A 4-gram language model is trained on the original training texts whichcontain disfluencies.
This score measures the fluency of the resulting cleaned-up sentence w.r.t.
a disflu-ent language model.
These two LM scores provide contrastive measures because if a cleaned up sentencestill contains disfluencies, the disfluent LM will be preferred over the fluent LM.M3N disfluent score: This is the normalized joint log probability score of the current hypothesis h,i.e., fM3N(h,w).
This score measures how much the baseline M3N model favors the disfluency labelassignment of the current hypothesis.M3N fluent score: This is the normalized joint log probability score of labeling the entire cleaned-upsentence as fluent, i.e.,fM3N(?h=O, w?)
=1|?h||?h|?i=1fM3N(?hi=?O?, w?)
(6)This score measures how much the baseline M3N model favors the cleaned-up sentence of the currenthypothesis.
It acts as a discriminative LM in measuring the fluency of the cleaned-up sentence.
If thecleaned-up sentence contains disfluencies, this evaluator function will tend to give a lower score.Expert-POS-class c disfluent score: This is the normalized joint log probability score of the currenthypothesis h under the expert M3N model for POS class c dynamically combined with the baseline M3Nmodel, i.e.,fc(h,w) =1|h||h|?i=1gc(hi,w), gc(hi,w) ={fM3N-c(hi,w) if POS(wi) ?
ScfM3N(hi,w) if POS(wi) /?
Sc(7)Training of the expert M3N models is described in Section 4.6.4.5 Integrating M3N into the Decoder FrameworkIn most previous work such as (Liu et al., 2006) and (Qian and Liu, 2013) that performed filler and editword detection using sequence models, the begin-inside-end-single (BIES) labeling scheme was adopted,i.e., for edit words (E), 4 labels are defined: E B (beginning of an edit region), E I (inside an edit region),E E (end of an edit region), and E S (single-word edit region).
However, since our beam-search decoderneeds to change the labels dynamically among filler words, edit words, and fluent words, it will beproblematic if the label sequence has to conform to the BIES constraint especially when the posteriorsare concerned.
Thus, we use the minimal set of labels: E (Edit word), F (Filler word), O (Outside editand filler region).For the first-stage filler word detection, only ?F?
and ?O?
are used.
To compensate for degradationin performance, we increase the clique order of features to 3.
We found that increasing the cliqueorder has a similar effect as using the BIES labeling scheme.
For example, f(wi=?so?, y0=?E B?, t)means the previous word is not an edit, both the current word and the next word are edit words, i.e.,the previous word, the current word, and the next word can be either O-E-E or F-E-E.
So in our mini-mal labeling scheme, this feature will be decomposed into f(wi=?so?, y?1=?O?, y0=?E?, y+1=?E?, t) andf(wi=?so?, y?1=?F?, y0=?E?, y+1=?E?, t), both having a higher clique order.Our preliminary experiments show that by increasing the clique order of features while reducing thenumber of labels (keeping about the same total number of parameters), we can maintain the same per-formance.
However, training takes a longer time.4.6 POS-Class Specific Expert ModelsWe trained 6 expert M3N models, each focusing on disfluency prediction of words belonging to thecorresponding set of POS tags.
The expert M3N models are trained in the same way as the baselineM3N model, except that we increase the loss weights (Eqn.
4) if the word of that node belongs to1463the corresponding POS class.
That is, M3N-Expert-POS-class-1 is trained to optimize performance onwords belonging to POS-class-1.
Nonetheless, it can still predict disfluency for words in other POSclasses, except that the error rate may be higher because of the way training is biased.Class POS tags Freq.
(%) F1 range1 RBS POS PDT NNPS HVS PRP$ BES PRP 33.5 92.3 ?
1002 MD EX CC DT VBP WP WRB 32.2 86.0 ?
90.83 RB IN 16.7 82.8 ?
83.84 TO VBD WDT RP JJS 5.1 80.0 ?
82.15 VBZ VB VBN JJ 6.1 69.3 ?
78.16 VBG NN CD JJR UH NNS NNP XX RBR 3.2 42.1 ?
64.2Table 4: POS classes for expert M3N models and their baseline F1 scoresWe split all POS tags into 6 classes, by first sorting all POS tags in descending order of their F1scores.
Next, for POS tags with higher F1 scores, we form larger classes (higher total proportion), andfor POS tags with lower F1 scores, we form smaller classes.
The POS classes are shown in Table 4.
Thealgorithm dynamically selects posteriors from different M3N models, depending on the POS tag of thecurrent word.5 Experiments5.1 Experimental SetupWe tested all the systems on the Switchboard Treebank corpus (LDC99T42), using the sametrain/develop/test split as previous work (Johnson and Charniak, 2004; Qian and Liu, 2013).
We removedall partial words and punctuation symbols to simulate the condition when automatic speech recognition(ASR) output is used.
Our training set contains 1.3M words in 174K sentences; our development setcontains 86K words in 10K sentences; and our test set contains 66K words in 8K sentences.
The originalsystem has high precision but low recall, i.e., the system tends to miss out edit words.
The imbalance canbe solved by setting a larger penalty for mis-labeling edits as fluent, i.e., 2 instead of 1 for the weightedhamming loss.
We used the loss matrix, v(y?t, y?t), in Table 5 to balance precision and recall.
We setthe biasing factor Bcto 2, for every class c. We also added two pause duration features (pause durationbefore and after the current word) from the corresponding Switchboard speech corpus (LDC97S62).
Wetrained our acoustic model on the Fisher corpus and used it to perform forced alignment on the Switch-board corpus to obtain the word boundary time information for calculating pause durations.
For thengram-score features, we used the small 4-gram language model trained on the training set with fillerwords and edit words removed.
All continuous features are quantized into 10 discrete bins using cumu-lative binning (Liu et al., 2006).
We set maxIter to 4 in Algorithm 1.
The regularization parameter Cis set to 0.006, obtained by tuning on the development set.Label E F OE 0 1 2F 1 0 2O 1 1 0Table 5: Weighted hamming loss, v(y?t, y?t) for M3N for both stages5.2 ResultsWe use the standard F1 score as our evaluation metric, the same as (Qian and Liu, 2013).
Performancecomparison of the baseline model and expert models on subsets belonging to specific POS classes isshown in Table 6.
It shows that by assigning larger loss weights to nodes belonging to a specific POSclass, we can to various extent boost the performance on words in that POS class.
However, doing so1464will sacrifice the overall performance on the entire data set especially on POS classes with lower baselinescores (see Table 7).
But since we have several expert models, if we combine them, they can complementeach other?s weakness and give an overall slightly better performance.
The result also shows that thegain by training expert models decreases as the baseline performance on that POS class increases.
Forexample, POS class 6 has the poorest baseline performance and the gain is 2.1%.
This gain decreasesgradually as we move up the table rows because the baseline performance gets better.POS class Expert-M3N F1 Baseline-M3N F11 92.5 92.22 87.1 86.93 84.9 84.74 85.3 84.15 71.8 70.46 57.3 55.2Table 6: Edit detection F1 scores (%) of expert models on all words belonging to that POS class in thetest set (expert-M3N column), and baseline model on all words belonging to that POS class in the testset (baseline-M3N column)System F1 (%)Baseline-M3N 84.7Expert-M3N(1) 84.6Expert-M3N(2) 84.4Expert-M3N(3) 84.3Expert-M3N(4) 84.6Expert-M3N(5) 84.0Expert-M3N(6) 83.8Table 7: Degradation of the overall performance by expert models compared to the baseline modelTable 8 shows the performance comparison of our baseline models and our beam-search decoder.Statistical significance tests show that our best decoder model incorporating all hypothesis evaluatorsgives a higher F1 score than the 3-label baseline M3N model (statistically significant at p < 0.001), andthe 3-label baseline M3N model gives a higher F1 score than the M3N system of (Qian and Liu, 2013)(statistically significant at p = 0.02).
Our three baseline models have about the same total number ofparameters (7.6M).
The BIES baseline M3N system uses the same feature templates as shown in Table 2with reduced clique order.
The 2-label baseline M3N system uses the same feature templates with thesame clique order.
Our results also show that joint filler and edit word prediction performs 0.4% betterthan edit word prediction alone.
Direct combination of expert models is done by first running the generalmodel and the expert models on each sentence to obtain all the label sequences (one for each model).Then for every word in the sentence, if its POS belongs to any one of those POS classes, we choose itslabel from the output of the corresponding expert model; otherwise, we choose its label from the outputof the baseline model.For the decoder, M3N-disfluent-score needs to be present in all cases (except when POS experts arepresent); otherwise, the F1 score is much worse because the entire sequence is not covered (i.e., justlooking at the scores from the cleaned-up sentences is not sufficient in deciding how well filler and editwords have been removed).
Adding M3N-fluent-score, Fluent-LM, or Disfluent-LM alone with M3N-disfluent-score gives about the same improvement; but when combined, higher improvement is achieved.Similar to (Qian and Liu, 2013), our system does not make use of any external sources of informationexcept for the last two rows in Table 8 where we added pause duration features.
We found that addingpause duration features gave a small but consistent improvement in all experiments, about 0.3% absolutegain in F1 score.
Our beam-search decoder (multi-threaded implementation) is about 4.5 times slower1465SystemF1(%)M3N system of (Qian and Liu, 2013) 84.1Our baseline M3N (using BIES for E and F) 84.4Our baseline M3N (using 2 labels: E,O) 84.3Our baseline M3N (using 3 labels: E,F,O) 84.7Direct combination of the 6 POS-expert-models according to each word?s POS 85.2Decoder: M3N-disfluent-score + M3N-fluent-score 85.1Decoder: M3N-disfluent-score + Fluent-LM 85.2Decoder: M3N-disfluent-score + Disfluent-LM 85.1Decoder: M3N-disfluent-score + POS-experts 85.2Decoder: M3N-disfluent-score + M3N-fluent-score + Fluent-LM + Disfluent-LM 85.6Decoder: M3N-disfluent-score + M3N-fluent-score + Fluent-LM + Disfluent-LM + POS-experts85.7Decoder: M3N-disfluent-score + M3N-fluent-score + Fluent-LM + Disfluent-LM + PauseDur 85.9Decoder: M3N-disfluent-score + M3N-fluent-score + Fluent-LM + Disfluent-LM + POS-experts + PauseDur86.1Table 8: Performance of the beam-search decoder with different combinations of componentsthan our baseline M3N model (single-threaded).
Overall, it took about 0.4 seconds to detect disfluenciesin one sentence with our proposed beam-search decoder approach.To the best of our knowledge, the best published F1 score on the Switchboard Treebank corpus is84.1% (Qian and Liu, 2013) without the use of external sources of information, and 85.7% (Zwarts andJohnson, 2011) with the use of external sources of information (large language models from additionalcorpora were used in (Zwarts and Johnson, 2011)).
Without the use of external sources of information,our decoder approach achieves an F1 score of 85.7%, significantly higher than the best published F1score of 84.1% of (Qian and Liu, 2013).
Our decoder approach also achieves an F1 score of 86.1% afteradding external sources of information (pause duration features), higher than the F1 score of 85.7% of(Zwarts and Johnson, 2011).5.3 DiscussionWe have manually analyzed the improvement of our decoder over the M3N baseline.
For example,consider the sentence in Table 9.
Both the baseline M3N system and the first pass output of the decoderwill give the cleaned-up sentence ?are these do these programs ...?, which is still disfluent and has arelatively lower fluent LM score but a relatively higher disfluent LM score because of the erroneousn-gram ?are these do these?.
The decoder makes use of the fluent LM and disfluent LM hypothesisevaluators during the beam search and performs additional passes of cleaning and eventually gives thecorrect output.Sentence Um and uh are these like uh do these programs ...Reference F F F E E E F O O O ...M3N baseline F F F O O F F O O O ...Decoder F F F E E E F O O O ...Table 9: An example showing the effect of measuring the quality of the cleaned-up sentence.Overall, our proposed decoder framework outperforms existing approaches.
It also overcomes the lim-itations mentioned in Section 4.1.
For example, hypothesis evaluators like fluent language model scoreand M3N fluent score achieve the purpose of measuring the quality of cleaned-up sentences.
Repeatedlyapplying the edit-word-marker hypothesis producer on a sentence achieves the purpose of cleaning up1466the sentence in multiple passes.
Hypothesis evaluators corresponding to expert models achieve the pur-pose of combining POS class-specific expert models.
All of these components extend the flexibility ofthe decoder framework in performing disfluency detection.6 ConclusionIn conclusion, we have proposed a beam-search decoder approach for disfluency detection.
Our beam-search decoder performs multiple passes of disfluency detection on cleaned-up sentences.
It evaluates thequality of cleaned-up sentences and use it as a feature to rescore hypotheses.
It also combines multipleexpert models to deal with edit words belonging to a specific POS class.
In addition, we also proposed away (using node-weighted M3N in addition to label-weighted M3N) to train expert models each focusingon minimizing errors on words belonging to a specific POS class.
Our experiments show that combiningthe outputs of the expert models directly according to POS tags can give rise to some improvement.Combining the expert model scores with language model scores in a weighted manner using our beam-search decoder achieves further improvement.
To the best of our knowledge, our decoder has achievedthe best published edit-word F1 score on the Switchboard Treebank corpus, both with and without usingexternal sources of information.7 AcknowledgmentsThis research is supported by the Singapore National Research Foundation under its International Re-search Centre @ Singapore Funding Initiative and administered by the IDM Programme Office.ReferencesEugene Charniak and Mark Johnson.
2001.
Edit detection and parsing for transcribed speech.
In Proc.
of NAACL.Daniel Dahlmeier and Hwee Tou Ng.
2012.
A beam-search decoder for grammatical error correction.
In Proc.
ofEMNLP-CoNLL.Kallirroi Georgila.
2009.
Using integer linear programming for detecting speech disfluencies.
In Proc.
of NAACL.Mark Johnson and Eugene Charniak.
2004.
A TAG-based noisy channel model of speech repairs.
In Proc.
ofACL.Jeremy G. Kahn, Matthew Lease, Eugene Charniak, Mark Johnson, and Mari Ostendorf.
2005.
Effective use ofprosody in parsing conversational speech.
In Proc.
of EMNLP.John Lafferty, Andrew McCallum, and Fernando Pereira.
2001 Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proc.
of ICML.Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Dustin Hillard, Mari Ostendorf, and Mary Harper.
2006.
Enrich-ing speech recognition with automatic detection of sentence boundaries and disfluencies.
IEEE Transactions onAudio, Speech, and Language Processing, 14(5).Sameer Maskey, Bowen Zhou, and Yuqing Gao.
2006.
A phrase-level machine translation approach for disfluencydetection using weighted finite state transducers.
In Proc.
of INTERSPEECH.Franz Josef Och.
2003.
Minimum error rate training in statistical machine translation.
In Proc.
of ACL.Xian Qian and Yang Liu.
2013.
Disfluency detection using multi-step stacked learning.
In Proc.
of NAACL.J.A.K.
Suykens and J. Vandewalle.
1999 Least squares support vector machine classifiers.
Neural ProcessingLetters, 9.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003.
Max-margin Markov networks.
In Proc.
of NIPS.Pidong Wang and Hwee Tou Ng.
2013.
A beam-search decoder for normalization of social media text withapplication to machine translation.
In Proc.
of NAACL.Qi Zhang, Fuliang Weng, and Zhe Feng.
2006.
A progressive feature selection algorithm for ultra large featurespaces.
In Proc.
of ACL.Simon Zwarts and Mark Johnson.
2011.
The impact of language models and loss functions on repair disfluencydetection.
In Proc.
of ACL.1467
