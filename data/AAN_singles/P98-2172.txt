Reference Resolution beyond Coreference:a Conceptual Frame and its ApplicationAndrei POPESCU-BELIS, Isabelle ROBBA and G6rard SABAHLanguage and Cognition Group, LIMSI-CNRSB.P.
133Orsay, France, 91403{popescu, robba, gs}@limsi.frAbstractA model for reference use in com-munication is proposed, from a rep-resentationist point of view.
Both thesender and the receiver of a messagehandle representations of their com-mon environment, including mentalrepresentations of objects.
Referenceresolution by a computer is viewed asthe construction of object representa-tions using referring expressions fromthe discourse, whereas often onlycoreference links between such ex-pressions are looked for.
Differencesbetween these two approaches arediscussed.The model has been imple-mented with elementary rules, andtested on complex narrative texts(hundreds to thousands of referringexpressions).
The results support themental representations paradigm.IntroductionMost of the natural anguage understandingmethods have been originally developed ondomain-specific examples, but more re-cently several methods have been applied tolarge corpora, as for instance morpho-syntactic tagging or word-sense disam-biguation.
These methods contribute onlyindirectly to text understanding, being farfrom building a conceptual representationof the processed iscourse.
Anaphora orpronoun resolution have also reached sig-nificant results on unrestricted texts.Coreference resolution is the next step onthe way towards discourse understanding.The Message Understanding Conferences(MUC) propose since 1995 a coreferencetask: coreferring expressions are to belinked using appropriate mark-up.Reference resolution goes further: ithas to find out which object is referred toby an expression, thus gradually building arepresentation f the objects with their fea-tures and evolution.
Coreference resolutionis only part of this task, as coreference isonly a relation between two expressions thatrefer to the same object.A framework for reference use inhuman communication is introduced inSection 1, in order to give a coherent andgeneral view of the phenomenon.
Conse-quences for a resolution mechanism arethen examined: data structures, operations,selectional constraints and activation.
Thisapproach is then compared to others inSection 2.
Section 3 describes briefly theimplementation f the model, the texts andthe scoring methods.
Results are given inSection 4, to corroborate the previous as-sertions and justify the model.1 A general frameworkreference use and resolutionfor1.1 Overview of the modelThe communication situation is deliberatelyconceived here from a representationistpoint of view: the speaker (s) and the hearer(h) share the same world (W) considered asa set of objects with various characteristicsor properties (Figure 1).
Objects can bematerial or conceptual, or even belong tofictitious constructions.
Each individual'sperception of the world is different:ph(W) ~ ps(W).
Perception (p) as well as in-ferences (i) on perceptions using previousknowledge and beliefs provide each indi-vidual with a representation of the world,that is, RWs and RWh, where RWx =ix(px(W)) -- ipx(W).
For computational rea-sons, it is useful to consider that only partof the world W plays a role in the commu-nication act; this is called the topic T, andits representations are RTh and RTs.The speaker produces a discoursemessage (DM) and a gesture message(GM).
Both DM and GM contain referringexpressions (RE), that is, chunks of dis-course or gestures which are mapped toparticular objects of RW.
RWh and RWseach include a list of represented objectswith their properties, called mental repre-sentations (MR).1046SPEAKER (s) ~ HEARER (h)/ Tis(W'T) k ~ ) ih(W'T)RW s --R W s ( h ~  RWhRWs(h(s) ~ RWh(s)RWh(s(h))*** .
, .?
WD ( O, , O2, O3 .... )?
RW s D {MRs(O~),MRs(O2),.
.
.
)?
RW h D (MRh(O~),MRh(O2),,oo~?
RWs(h) D (MRs(MRh(O~)),MRs(MRh(O2)),)))~?
RWh(s) D {MRh(MRs(O,)),MRh(MRs(O2)),Figure 1.
The proposed formal model for reference representationUnderstanding a message cannot be de-fined solely with respect to W, as there is no di-rect access to it.
Instead, each individual buildsa representation of the others' RW, using itsown perceptions and inferences (ip).
Thespeaker has his own RWs and alsoRWs(h) = ips(RWh); the hearer has RWh andRWh(s) = iph(RWs).
This hierarchy, calledspecularity, is potentially infinite, as one mayconceive RWh(s(h)), RWh(s(h(s))), etc.
(it couldbe tentatively asserted that when all the RW ofall individuals become identical for a given as-sertion, the assertion becomes "commonknowledge").A message has been understood if, forthe current topic, RTh(s)- RTs, i.e., if thehearer's representation of the speaker's viewof the world is accurate.
This definition simpli-fies of course reality to make it fit into a com-putational model.
For instance, from a rhetori-cal point of view, a communication succeeds ifRTh changes according to the sender's will.Evolution in time isn't represented yet, so wedo not index the various representations alongthe time axis.In order to understand a message, thehearer has to find out which objects the refer-ring expressions refer to - REs from the dis-course, as well as deictic (pointing) ones.
Thehearer is able to use his own perception of W,namely RWh, and his knowledge, to buildmental representations of objects from the re-ferring expressions.1.2 Human-computer dialog vs. storyunderstanding by a computerWe focus here on the problem of referenceunderstanding by a computer program (c).Such a program has to build and manage, intheory, a RWc and a RWc(s), using informationabout the world, the message itself, and possi-bly a deictic set.For a window manager application ac-cepting natural language commands, the dis-played graphic objects constitute the topic (T),i.e., the part of the world more specificallydealt with.
The program's perception of T istotally accurate (pc(T)= T); pc(T) is the mostimportant and reliable source of information.Mouse pointing provides also direct deictic in-formation.
The difference between RWc andRWc(s) may account for the difference be-tween the complete description of the dis-played objects and their visible features.For a story understanding program, thedirect perception of the shared world W isstrongly reduced, especially for fiction stories.Human readers in this case derive their knowl-edge only from the processed text.
But knowl-edge about basic properties of W and aboutlanguage conventions has still to be shared,otherwise no communication would be possi-ble.
For story processing, both pc(W) and thegesture message are extremely limited, so theprogram has to rely only on discourse infor-mation, thus building fh'st RWc(s) and only af-terwards RWc, using supplementary knowledgeabout W. The gap between RWc(s) and RWc is1047due to the speaker's misuse of referring expres-sions, or to internal contradictions of the story.The system described below follows this sec-ond approach.1 ,3  Data  s t ruc tures  and  operat ionsFor minimal reference resolution, aprogram has to select the referring expressions(RE) of the received message and use them inorder to build a list of mental representationsof objects (MR).
Each MR is a data structurehaving several attributes, depending on theprogram's capacities.
Here is a basic set:?
MR.identificator - -  a number;?
MR. l i s t -o f -REs -  the REs referring to theobject;?
MR.semantic-information.text - -a  con-ceptual structure gathering the properties ofthe object, from the REs and from the sen-tences in which they appear;?
MR.semantic-information.dictionary - -  aconceptual structure gathering the proper-ties of the object from the conceptual dic-tionary (concept lattice) of the system.These properties reflect a priori knowledgeabout the conceptual categories the MRbelongs to;?
MR.relations - -  the relationship of the MRto other MRs, for instance: part-of or com-posed-of (these allow processing of pluralMRs);?
MR.computer -ob jec t -  a pointer on theobject in case it belongs to a computer ap-plication (e.g., a window in a commanddialog);?
MR.perceptual-information ~ an equiva-lent of the previous attribute, in case theprogram handles perceptual representationsof objects.In turn, the computational representation of areferring expression (RE) should have at leastthe following attributes:?
RE.identificator m a number;?
RE .pos i t ion -  uniquely identifies the RE'sposition in the text: number, paragraph,sentence, beginning and ending words;?
RE.syntactic-information - -  a parse tree ofthe RE, the RE's function, or, if available, aparse tree of the whole sentence where theRE appears;?
RE.semantic-information ~ a conceptualstructure for the RE, or, if available, for thewhole sentence.Finally, there are operations on the MR set:?
creation: REi ---> MRnew - -  a new MR is cre-ated when an object is fh'st referred to;?
attachment: REi + MRa ----> MRa ~ when aRE refers to an already represented object,the RE is attached to the MR and the MR'sstructure is updated;?
fusion: MRa + MRb ~ MRnew - -  at a givenpoint, it may appear that two MRs were builtfor the same object, so they have to bemerged.
The symmetrical operation, i.e.,splitting an MR which confusingly repre-sents two objects, is far more difficult to do,as it has to reverse a lot of decisions;?
partition: MRa ~ MRa + MRnew(1) +MRnew(2) + ... ;?
grouping: MRa + MRb ~ MRa + MRb +MRnew(a,b);The last two operations (partition/grouping) aresymmetrical, and prove necessary in order todeal with collections of objects (plurals).
Forinstance, from a collective RE as "the team"(and its MR) the program has to use built-inknowledge to create several MRs correspond-ing to the players, and correctly solve the newRE "the first player".
Conversely, after con-struction of two MRs for "Miss X" and "Mrs.Y", an RE as "the two women" has to be at-tached to the MR which was built by groupingthe previous MRs.
In both cases, theMR.relation attribute has to be correctly filled-in with the type of relation between MRs.If enough data is available, the systemshould build a conceptual structure for the MR(e.g., conceptual graphs), which should incre-mentally gather information from all referringexpressions attached to the same MR. A lower-knowledge technique is to record for each MRa list of "characteristic REs" without any con-ceptual structures, and apply selectional con-straints on it.1 .4  Se lec t ion  heur i s t i csDuring the resolution process, each RE eithertriggers the creation of a new MR or is attachedto an existing MR.
The purpose of the selec-tion heuristics is to answer whether the RE maybe associated to a given MR, after examiningcompatibility between the RE and the otherREs in the MR.list-of-REs.
One of the simplestheuristics is:?
(HI) \[MRa can be the referent of REi\] iff\[RE1 being the first element of MRa.list-of-REs, REi and RE1 can be coreferent\]This presupposes that the first RE referring toan object is typical, which isn't always true.To take advantage of the MR paradigm,it may seem wiser to compare the current RE toall the REs in the MR.list-of-REs.
This list in-cludes also pronominal REs, which are actuallymeaningless for the compatibility test.
DespiteAriel's (1990) claim that there is no clear-cutreferential difference between pronouns and1048nominals, we will exclude pronouns in the im-plementation of our model.
So, a second heu-ristic is:?
(H2) \[MRa can be the referent of REi\] iff\[for all (non-pronominal) REj in MRa.list-of-REs, REi and REj can be coreferent\]This heuristic is in fact quite inefficient: first, itallows for little variation in the naming of areferent.
Second, it neglects an important dis-tinction in RE use, between identification andinformation (as described, for instance, by Ap-pelt and Kronfeld (1987)).
The sender mayuse a particular RE not only to identify theMR, but also to bring supplementary knowl-edge about it; thus, two REs conveying differ-ent pieces of knowledge may well be incom-patible in the system's view.
A more tolerantheuristic is thus:?
(H3) \[MRa can be the referent of REi\] iff\[there exists a (non-pronominal) REj inMRa.list-of-REs o that REi and REj can becoreferent\]A more general heuristic subsumes both H2('all') and H3 ('one'):?
(H4) \[MRa can be the referent of REi\] iff\[REi and REj can be coreferent for morethan X% of the REj in MRa.list-of-REs\]When X varies from 0 to 100, this selectionheuristic varies from H3 to H2 providing in-termediate heuristics that can be tested (?4).H3 seems in fact close to the co-reference paradigm, as it privileges links be-tween individual REs, from which the MRscould even be built a posteriori, using thecoreference chains.
But here MRs are alsocharacterized by an intrinsic activation factor,evolving along the text, which cannot be man-aged in the coreference paradigm.1.5 ActivationThe activation of an MR is computed accord-ing to salience factors (this technique is de-scribed for instance by Lappin and Leass(1994)).
Our salience factors are: de-activationin time, re-activation by various types of RE,re-activation according to the function of theRE.
Among the MRs which pass the selection,activation is used to decide whether the currentRE is added to an MR (the most active) or if anew MR is created.
Activation is thus a dy-namic factor, which changes for each MR ac-cording to the position in  the text and the pre-vious reference resolution decisions.2 Comparison with other worksTheoretical studies of discourse processinghave long been advocating use of various rep-resentations for discourse referents.
However,implementations of running systems haverather focused on anaphora or coreference.Our purpose here is to show how a simplifiedcomputational model of discourse referencecan be implemented and give significant resultsfor reference resolution; we showed previously(Popescu-Belis and Robba 1997) that it wasalso relevant for pronoun resolution.2.1 H igh- leve l  knowledge  mode lsThe idea of tracking discourse referents using"files" for each of them has already beenproposed by Kartunnen (1976).
Evans (1985)and Recanati (1993) are both close to our pro-posals, however they neither give a computa-tional implementation nor an evaluation onreal texts.
Sidner's work (1979) on focus led tosalience factors and activations, but proved toodemanding for an unrestricted use.A more operational system using se-mantic representation of referents is for in-stance LaSIE (Gaizauskas et al 1995), pre-sented at MUC-6, which relies however a lot ontask-dependent knowledge.
The system doesn'tseem to use activation cues.
Another system(Luperfoy 1992) uses "discourse pegs" tomodel referents and was applied successfully toa man-machine dialogue task.From a theoretical point of view, themodel presented by Appelt and Kronfeld(1987) is in its background close to ours.
Be-ing further developed according to the speechacts theory, it relies however on models of in-tentions and beliefs of communicating agentswhich seem uneasy to implement for discourseunderstanding.2 .2  Robust ,  lower - leve l  sys temsSome of the robust approaches derive fromanaphora resolution (e.g., Boguraev and Ken-nedy (1996)) because the antecedent / ana-phoric links are a particular sort of coreferencelinks, which disambiguate pronouns.
Most ofthese systems however emain within the co-reference paradigm, as defined by the MUC-6coreference task.
Numerous low-level tech-niques have been developed, using generallypattern-matching between potentially corefer-ent strings (e.g., McCarthy and Lehnert 1995).An interesting solution has been pro-posed by Lin (1995) using constraint solvingto group REs into MRs.
While this idea fits theMR paradigm, it doesn't work well incremen-tally, which makes use of activation impossible.2.3 Advantages of the MR paradigmGrouping REs into MRs brings decisive ad-1049vantage even without conceptual knowledge.First, it suppresses an artificial ambiguity ofcoreference r solution: if RE1 and RE2 are al-ready known as coreferent, coref(RE1, RE2),there is no conceptual difference betweencoref(RE3, RE1) and coref(RE3, RE2), so thesetwo possibilities houldn't be examined sepa-rately.
Moreover, the system of coreferencelinks makes it very time-consuming to find outwhether REi and REj are coreferent, whereasMRs provide reusable storing of all the alreadyacquired information.Second, coreference links cannot repre-sent multiple dependencies a needed by someobjects which are collections of other objects.Coreference links simply mark identity of thereferent for two REs: collections require typedlinks (part-of /composed-of) between severalobjects, as shown previously.3 Application of the model3.1  Reference  reso lu t ion  mechan ismWe have particularized and implemented thetheoretical model using algorithms in the styleof Lappin and Leass (1994).
We don't wish tooverload this paper with technical details.
TheREs are solved one by one, either by attach-ment to an existent MR, or by creation of anew MR.Selection rules are applied to the exist-ing MRs to find out whether the current REmay or may not refer to the object representedby the MR. As our implementation deals withunrestricted texts, only very basic selectionrules are used; there are two agreement rules(for gender and number) and a semantic rule(synonyms and hyperonyms are compatible).As no semantic network is available for French(e.g., WordNet), only very few synonyms aretaken into account.
Conceptual graphs areneither used, as our conceptual analyzer isn'trobust enough for unrestricted noun phrases.The working memory stores a fixedquota of the most active MRs, the others beingarchived and inaccessible for further resolu-tion.
From a cognitive point of view, this mem-ory mimics the human incapacity to track toomany story characters.
Computationally, it re-duces ambiguity for the attachment of REs,and increases the system's peed.3 .2  The  textsTwo narrative texts have been chosen to testour system: a short story by Stendhal, VittoriaAccoramboni (VA) and the first chapter of anovel by Balzac, Le P~re Goriot (LPG)(Table 1).
VA, available as plain text, under-went manual tagging of paragraphs, sentencesand boundaries of all REs, then conversion to'objects' of our programming environment(Smalltalk).
Using Vapillon's and al.
(1997)LFG parser, an f-structure (parse tree) wasadded to each RE.
Then the correct MRs werecreated using our user-friendly interface.WordsREsMRs (key)RE /MRNominal REsPronoun REsNot parsed REsVA26306383721.7251010226l.PG.eq LPG7405 28576686 3359216 4803.18 7.00390 1864262 139834 97Table 1.
Characteristics of the three texts.LPG was already SGML-encoded withthe REs and MRs, using Bruneseaux and Ro-mary (1997) mark-up conventions.
Only REsreferring to the main characters of the firstchapter were encoded: humans, places and ob-jects.
As a result, the ratio RE / MR is muchgreater than for VA.
The text was converted toSmalltalk objects, f-structures were added tothe REs, and MRs were automatically generatedfrom the SGML tags.
To make comparisonwith VA easier, a fragment of the LPG text wasisolated (LPG.eq); it contains the same amountof REs as VA.It should be noted that in both cases theLFG parser isn't robust enough to deliverproper f-structures for all noun phrases.
Theparser's total silence is ca.
4% and its ambigu-ity ca.
2.7 FS per RE.
Despite such drawbacks(unreliable parser, lack of semantics), we keptworking on complex narrative texts in order tostudy in depth the effects of elementary rulesand parameters in situations where the corefer-ence rate is high.
Reference resolution isprobably easier on technical documentation orarticles, as referents receive more constantnames.3 .3  Eva luat ion  methodsThe MRs produced by the reference resolutionmodule (response) are compared to the correctsolution (key) using an implementation of thealgorithm described by Vilain and al.
(1995),used also in the MUC evaluations.
Althoughthis algorithm was designed for coreferenceevaluation, it builds in fact each coreferencechain, and compares the key and the response1050partition of the RE set in MR subsets - -  it fol-lows thus the MR paradigm.
The algorithmcomputes a recall error (number of corefer-ence links missing in the response vs. the key)and a precision error (number of wrongcoreference links, i.e.
present in the responsebut absent from the key).The MUC scoring method isn't alwaysmeaningful.
We have shown elsewhere(Popescu-Belis and Robba 1998) that it is tooindulgent, and have proposed new algorithmswhich seem to us more relevant, named here'core-MR' and 'exclusive-core-MR'.4 Results  and commentsThe three heuristics H1, H2, H3 havebeen tested on our system, while keeping allother numeric parameters constant.
The resultsTable 2 show that on average the heuristic H3gives here the same results as H1, and is betterthan H2.
As explained above, H2 is clearly toorestrictive.Different ests have been performed toanalyze the system's results.
If MR activationisn't used, the scores decrease dramatically, byca.
50%.
When using the H4 heuristic (variableaverage between H2 and H3) results aren't gen-erally better than those of H3 (except for VA).Compatibility with only one RE of the MRseems thus a good heuristic.H1 (first)R PMUC .66 .60Core .52 .44Ex-C .62 .73MUC .72 .76Core .57 .34Ex-C .40 .54MUC .80 .85Core .38 .40Ex-C .29 .48H2 (all)R P.66 .60.52 .44.63.66.40.38.77.34.28H3(one)R P.70 .60.56 .39.73 .60 .69.70 .72 .76.35 .57 .34.54 .40 54.83 .80 .85.42 .38 .40.48 .29 .48Table 2.
Success cores for selection heuristics(for VA, LPG.eq, LPG)This is confirmed when applying theselection constraints on a limited subset ofMR.list-of-REs.
The worst results are obtainedwhen this set fails to gather the shortest non-pronominal REs of an MR, which shows thatthese shortest strings (one or several) constitutea sort of 'standard name' for the referent, whichsuffices to solve the other references.
The goodscore of H1 tends also to confh-m this view.An optimization algorithm based ongradient descent has been implemented to tunethe activation parameters of the system.
Notsurprisingly, sometimes the local optimum hasno cognitive relevance, as there is no searchingheuristic other than recall+precision decrease.A local optimum obtained on one text stillleads to good (but not optimal) scores on theother texts.
Trained on VA, optimization led toa cumulated 4.3% improvement (precision +recall), and +2.5% on LPG.eq, or in anothertrial to +5.9%.I "-4~- LPG.eq -II- VA -4..- F.measure=68 I8075Avo = 70( Jo-656055.
.
iri -  i r ' "I I !
I50 60 70 80Recall (%)Figure 2.
Influence of memory size on recalland precision (between 2, left, and 60, right)Finally, the limited size buffer storingthe MRs, a cognitively inspired feature, wasstudied.
Variations of the system's perform-ance according to the size of this "workingmemory" show that it has an optimal size,around 20 MRs (Figure 2).
A smaller memoryincreases recall errors, as important MRs aren'tremembered.
A larger memory leads to moreerroneous attachments (precision errors) be-cause the number of MRs available for at-tachment overpasses the selection rules' selec-tiveness.Conc lus ionA theoretical model for reference resolutionhas been presented, as well as an implementa-tion based on the model, which uses only ele-mentary knowledge, available for unrestricted1051texts.
The model shows altogether greater con-ceptual accuracy and higher cognitive rele-vance.
Further technical work will seek a betteruse of the syntactic information; semanticknowledge will be derived in a first approachfrom a synonym dictionary, awaiting the de-velopment of a significant set of canonicalconceptual graphs.Further conceptual work, besides studyof complex plurals, will concern integration oftime to mental representations, as well as pointof view information.AcknowledgmentsThe authors are grateful to F. Bruneseaux andL.
Romary for the LPG text, to A. Reboul fordiscussions on the model, and to one of theanonymous reviewers for very significantcomments.
This work is part of a project sup-ported by the GIS-Sciences de la Cognition.ReferencesAppelt D. and Kronfeld A.
(1987) A ComputationalModel of Referring, IJCAI '87, Milan, volume 2/2,pp.
640-647.Ariel M. (1990) Accessing noun-phrase antecedents,Routledge, London.Bruneseaux F. and Romary L. (1997) Codage desr~fHences et cordfHences clans les dialogues homme-machine, ACH-ALLC '97, Kingston, Ontario, Can-ac~Evans G. (1985) The Varieties of Reference, OxfordUniversity Press, Oxford, UK.Gaizauskas R., Wakao T., Humphreys K., Cunning-ham H. and Wilks Y.
(1995) University of Shef-field: Description of the LaSIE System as used forMUC-6, MUC-6, pp.
207-220.Kennedy C. and Boguraev B.
(1996) Anaphora in aWider Context: Tracking Discourse Referents,ECAI 96, Budapest, Hungary, pp.
582-586.Karttunen L. (1976) Discourse referents.
In "Syntaxand Semantics 7: Notes from the Linguistic Under-ground", J. D. McCawley, ed., Academic Press,New York, pp.
363-385.Lappin S. and Leass H. J.
(1994) An Algorithm forPronominal Anaphora Resolution, ComputationalLinguistics, 20/4, pp.
535-561 .Lin D. (1995) University of Manitoba: Descriptionof the PIE System Used for MUC-6, MUC-6, pp.113-126.Lupeffoy S. (1992) The Representation f Multimo-dal User Interface Dialogues Using Discourse Pegs,30th Annual Meeting of the ACL, University ofDelaware, Newark, Delaware, pp.
22-31.McCarthy J. F. and Lehnert W. G. (1995) Using De-cision Trees for Coreference Resolution, IJCAI '95,Montr6al, Canada, pp.
1050-1055.Popescu-Belis A. and Robba I.
(1997) Cooperationbetween Pronoun and Reference Resolution for Un-restricted Texts, ACL'97 Workshop on OperationalFactors in Practical, Robust Anaphora Resolutionfor Unrestricted Texts, Madrid, Spain, pp.
94-99.Popescu-Belis A. and Robba I.
(1998) Three NewMethods for Evaluating Reference Resolution,LREC'98 Workshop on Linguistic Coreference,Granada, Spain.Recanati F. (1993) Direct Reference: from Languageto Thought, Basil Blackwell, Oxford, UK.Sidner C. L. (1979) Towards a computational theoryof definite anaphora comprehension i  English dis-course, Doctoral Dissertation, Artificial IntelligenceLaboratory, Massachusetts Institute of Technology,Technical Report 537.Vapillon J., Briffault X., Sabah G. and Chibout K.(1997) An Object-Oriented Linguistic EngineeringEnvironment using LFG (Lexical FunctionalGrammar) and CG (Conceptual Graphs), ACL'97Workshop on Computational Environments forGrammar Development and Linguistic Engineering,Madrid, Spain.Vilain M., Burger J., Aberdeen J., Connolly D. andHirshman L. (1995) A Model-Theoretic Corefer-ence Scoring Scheme, 6th Message UnderstandingConference, Columbia, Maryland.1052
