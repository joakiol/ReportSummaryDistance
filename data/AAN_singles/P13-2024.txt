Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 131?136,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Decade of Automatic Content Evaluation of News Summaries:Reassessing the State of the ArtPeter A. RankelUniversity of Marylandrankel@math.umd.eduJohn M. ConroyIDA / Center for Computing Sciencesconroy@super.orgHoa Trang DangNational Institute of Standards and Technologyhoa.dang@nist.govAni NenkovaUniversity of Pennsylvanianenkova@seas.upenn.eduAbstractHow good are automatic content metricsfor news summary evaluation?
Here weprovide a detailed answer to this question,with a particular focus on assessing theability of automatic evaluations to identifystatistically significant differences presentin manual evaluation of content.
Usingfour years of data from the Text AnalysisConference, we analyze the performanceof eight ROUGE variants in terms of ac-curacy, precision and recall in finding sig-nificantly different systems.
Our exper-iments show that some of the neglectedvariants of ROUGE, based on higher or-der n-grams and syntactic dependencies,are most accurate across the years; thecommonly used ROUGE-1 scores findtoo many significant differences betweensystems which manual evaluation woulddeem comparable.
We also test combina-tions of ROUGE variants and find that theyconsiderably improve the accuracy of au-tomatic prediction.1 IntroductionROUGE (Lin, 2004) is a suite of automatic eval-uations for summarization and was introduced adecade ago as a reasonable substitute for costlyand slow human evaluation.
The scores it pro-duces are based on n-gram or syntactic overlap be-tween an automatic summary and a set of humanreference summaries.
However, the field does nothave a good grasp of which of the many evalua-tion scores is most accurate in replicating humanjudgements.
This state of uncertainty has led toproblems in comparing published work, as differ-ent researchers choose to publish different variantsof scores.In this paper we reassess the strengths ofROUGE variants using the data from four yearsof Text Analysis Conference (TAC) evaluations,2008 to 2011.
To assess the performance of the au-tomatic evaluations, we focus on determining sta-tistical significance1 between systems, where thegold-standard comes from comparing the systemsusing manual pyramid and responsiveness evalu-ations.
In this setting, computing correlation co-efficients between manual and automatic scores isnot applicable as it does not take into account thestatistical significance of the differences nor doesit allow the use of more powerful statistical testswhich use pairwise comparisons of performanceon individual document sets.
Instead, we reporton the accuracy of decisions on pairs of systems,as well as the precision and recall of identifyingpairs of systems which exhibit statistically signifi-cant differences in content selection performance.2 BackgroundDuring 2008?2011, automatic summarization sys-tems at TAC were required to create 100-wordsummaries.
Each year there were two multi-document summarization sub-tasks, the initialsummary and the update summary, usually re-ferred to as task A and task B, respectively.
Thetest inputs in each consisted of about 10 docu-ments and the type of summary varied betweenquery-focused and guided.
There are between 44and 48 test inputs on which systems are comparedfor each task.In 2008 and 2009, task A was to produce a1For the purpose of this study, we define a difference assignificant when the test statistic attains a value correspond-ing to a p-value less than 0.05.131query-focused summary in response to a user in-formation need stated both as a brief statementand a paragraph-long description of the informa-tion the user seeks to find.
In 2010 and 2011 taskA was ?guided summarization?, where the test in-puts came from a small set of predefined domains.These domains included accidents and natural dis-asters, attacks, health and safety, endangered re-sources, investigations and trials.
Systems wereprovided with a list of important aspects of infor-mation for each domain and were asked to cover asmany of these aspects as possible.
The writers ofthe reference summaries for evaluation were givensimilar instructions.
In all four years, task B wasto produce an update summary for each of the in-puts given in task A (query-focused or guided).
Ineach case, a new, subsequent set of documents re-lated to the topic of the respective test set for taskA was provided to the system.
The task was togenerate an update summary aimed at a user whohas already read all documents in the inputs fortask A.The two manual evaluation approaches used inTAC 2008?2011 are modified pyramid (Nenkovaet al, 2007) and overall responsiveness.
The pyra-mid method requires several reference summariesfor each input.
These are manually analyzed todiscover content units based on meaning ratherthan specific wording.
Each content unit is as-signed a weight equal to the number of referencesummaries that included that content unit.
Themodified pyramid score is defined as the sum ofweights of the content units in the summary nor-malized by the weight of an ideally informativesummary which expresses n content units, wheren is equal to the average of content units in the ref-erence summaries.
Responsiveness, on the otherhand, is based on direct human judgements, with-out the need for reference summaries.
Assessorsare presented with a statement of the user?s infor-mation need and the summary they need to evalu-ate.
Then they rate how well they think the sum-mary responds to the information need containedin the topic statement.
Responsiveness was ratedon a ten-point scale in 2009, and on a five-pointscale in all other years.For each sub-task during 2008?2011, we ana-lyze the performance of only the top 30 systems,which roughly corresponds to the systems that per-formed better than or around the median accordingto each manual metric.
Table 1 gives the numberof significant differences among the top 30 partici-pating systems.
We keep only the best performingsystems for the analysis because we are interestedin studying how well automatic evaluation metricscan correctly compare very good systems.Year Pyr A Pyr B Resp A Resp B2008 82 109 68 1052009 146 190 106 922010 165 139 150 1282011 39 83 5 11Table 1: Number of pairs of significantly differentsystems among the top 30 across the years.
Thereis a total of 435 pairs in each year.3 Which ROUGE is best?In this section, we study the performance ofseveral ROUGE variants, including ROUGE-n,for n = 1, 2, 3, 4, ROUGE-L, ROUGE-W-1.2,ROUGE-SU4, and ROUGE-BE-HM (Hovy et al,2006).
ROUGE-n measures the n-gram recall ofthe evaluated summary compared to the availablereference summaries.
ROUGE-L is the ratio ofthe number of words in the longest common sub-sequence between the reference and the evaluatedsummary and the number of words in the refer-ence.
ROUGE-W-1.2 is a weighted version ofROUGE-L. ROUGE-SU4 is a combination of skipbigrams and unigrams, where the skip bigrams areformed for all words that appear in the text withno more than four intervening words in between.ROUGE-BE-HM computes recall of dependencysyntactic relations between the summary and thereference.To evaluate how well an automatic evalua-tion metric reproduces human judgments, we useprediction accuracy similar to Owczarzak et al(2012).
For each pair of systems in each subtask,we compare the results of two Wilcoxon signed-rank tests, one using the manual evaluation scoresfor each system and one using the automatic evalu-ation scores for each system (Rankel et al, 2011).2The accuracy then is simply the percent agreementbetween the results of these two tests.2We use the Wilcoxon test as it was demonstrated byRankel et al (2011) to give more statistical power than un-paired tests.
As reported by Yeh (2000), other tests such asrandomized testing, may also be appropriate.
There is con-siderable variation in system performance for different inputs(Nenkova and Louis, 2008) and paired tests remove the effectof the input.132Responsiveness PyramidMetric Acc P R BA Acc P R BAR1 0.58 (0.61) 0.24 0.64 0.57 0.62 (0.66) 0.37 0.67 0.61R2 0.64 (0.63) 0.28 0.60 0.59 0.68 (0.69) 0.43 0.63 0.64R3 0.70 (0.63) 0.31 0.48 0.60 0.73 (0.68) 0.49 0.53 0.66R4 0.73 (0.64) 0.33 0.40 0.60 0.74 (0.65) 0.50 0.45 0.65RL 0.50 (0.59) 0.20 0.56 0.54 0.54 (0.63) 0.29 0.60 0.55R-SU4 0.61(0.62) 0.26 0.61 0.58 0.65 (0.68) 0.40 0.65 0.63R-W-1.2 0.52(0.62) 0.21 0.54 0.55 0.57(0.64) 0.32 0.62 0.57R-BE-HM 0.70 (0.63) 0.30 0.49 0.59 0.74(0.68) 0.49 0.56 0.66Table 2: Accuracy, Precision, Recall, and Balanced Accuracy of each ROUGE variant, averaged acrossall eight tasks in 2008-2011, with and (without) significance.As can be seen in Table 1, the manual evalua-tion metrics often did not show many significantdifferences between systems.3 Thus, it is clearthat the percent agreement will be high for an ap-proach for automatic evaluation that always pre-dicts zero significant differences.
As traditionallydone when dealing which such skewed distribu-tions of classes, we also examine the precisionand recall with respect to finding significant dif-ferences of several ROUGE variants, to better as-sess the quality of their prediction.
To identify ameasure that is strong at both predicting signifi-cant and non-significant differences we computebalanced accuracy, the mean of the accuracy ofpredicting significant differences and the accuracyof predicting no significant difference.4Each of these four measures for judging the per-formance of ROUGE variants has direct intuitiveinterpretation, unlike other opaque measures suchas correlation coefficients and F-measure whichhave formal definitions which do not readily yieldto intuitive understanding.3This is a somewhat surprising finding which may warrantfurther investigation.
One possible explanation is that differ-ent systems generate similar summaries.
Recent work hasshown that this is unlikely to be the case because the collec-tion of summaries from several systems indicates better whatcontent is important than the single best summary (Louis andNenkova, 2013).
The short summary length for which thesummarizers are compared may also contribute to the factthat there are few significant difference.
In early NIST eval-uations manual evaluations could not distinguish automaticand human summaries based on summaries of length 50 and100 words and there were more significant differences be-tween systems for 200-word summaries than for 100-wordsummaries (Nenkova, 2005).4More generally, one could define a utility function whichgives costs associated with errors and benefits to correct pre-diction.
Balanced accuracy weighs all errors as equally badand all correct prediction as equally good (von Neumann andMorgenstern, 1953).Few prior studies have taken statistical signifi-cance into account during the assessment of auto-matic metrics for evaluation.
For this reason wefirst briefly discuss ROUGE accuracy without tak-ing significance into account.
In this special case,agreement simply means that the automatic andmanual evaluations agree on which of two systemsis better, based on each system?s average score forall test inputs for a given task.
It is very rare thatthe average scores of two systems are equal, sothere is always a better system in each pair, andrandom prediction would have 50% accuracy.Many papers do not report the significance ofdifferences in ROUGE scores (for the ROUGEvariant of their choice), but simply claim that theirsystem X with higher average ROUGE score thansystem Y is better than system Y .
Table 2 liststhe average accuracy with significance taken intoaccount and then in parentheses, accuracy withouttaking significance into account.
The data demon-strate that the best accuracy of the eight ROUGEmetrics is a meager 64% for responsiveness whensignificance is not taken into account.
So the con-clusion about the relative merit of systems wouldbe different from that based on manual evaluationin one out of three comparisons.
However, thebest accuracy rises to 73% when significance istaken into account; an incorrect conclusion will bedrawn in one out of four comparisons.
The reduc-tion in error is considerable.Furthermore, ROUGE-3 and ROUGE-4, whichare rarely reported, are among the most accurate.Note also, these results differ considerably fromthose reported by Owczarzak et al (2012), whereROUGE-2 was shown to have accuracy of 81% forresponsiveness and 89% for pyramid.
The widedifferences are due to the fact we are only consid-133ering systems which scored in the top 30.
This il-lustrates that our automatic metrics are not as goodat discriminating systems near the top.
These find-ings give strong support for the idea of requiringauthors to report the significance of the differencebetween their summarization system and the cho-sen baseline; the conclusions about relative meritsof the system would be more similar to those onewould draw from manual evaluation.In addition to accuracy, Table 2 gives precision,recall and balanced accuracy for each of the eightROUGE measures when significance is taken intoaccount.
ROUGE-1 is arguably the most widelyused score in the literature and Table 2 reveals aninteresting property: ROUGE-1 has high recall butlow precision.
This means that it reports many sig-nificant differences, most of which do not exist ac-cording to the manual evaluations.Balanced accuracy helps us identify whichROUGE variants are most accurate in findingstatistical significance and correctly predictingthat two systems are not significantly different.For the pyramid evaluation, the variants withbest balanced accuracy (66%) are ROUGE-3 andROUGE-BE, with ROUGE-4 just a percent lowerat 65%.
For responsiveness the configuration issimilar, with ROUGE-3 and ROUGE-4 tied forbest (60%), and ROUGE-BE just a percent lower.The good performance of higher-order n-gramsis quite surprising because these are practicallynever used for reporting results in the literature.Based on our results however, they are much morelikely to accurately reproduce conclusions thatwould have been drawn from manual evaluationof top-performing systems.4 Multiple hypothesis tests to combineROUGE variantsWe now consider a method to combine multipleevaluation scores in order to obtain a stronger en-semble metric.
The idea of combining ROUGEvariants has been explored in the prior litera-ture.
Conroy and Dang (2008), for example, pro-posed taking linear combinations of ROUGE met-rics.
This approach was extended by Rankel et al(2012) by including measures of linguistic quality.Recently, Amigo?
et al (2012) applied the ?hetero-geneity principle?
and combined ROUGE scoresto improve the precision relative to a human evalu-ation metric.
Their results demonstrate that a con-sensus among ROUGE scores can predict more ac-curately if an improvement in a human evaluationmetric will be achieved.Along the lines of these investigations, we ex-amine the performance of a simple combinationof variants: Call the difference between two sys-tems significant only when all the variants in thecombination indicate significance.
As in the sec-tion above, a paired Wilcoxon signed-rank test isused to determine the level of significance.ROUGE Combination Acc Prec Rec BAR1 R2 R4 RBE 0.76 0.77 0.36 0.76R1 R4 RBE 0.76 0.76 0.36 0.76R2 R4 RBE 0.76 0.74 0.40 0.75R4 RBE 0.76 0.73 0.41 0.75R1 R2 R4 0.76 0.71 0.40 0.74R1 R4 0.75 0.70 0.40 0.73R2 R4 0.75 0.68 0.44 0.73R1 R2 RBE 0.75 0.66 0.48 0.72R2 RBE 0.75 0.64 0.52 0.72R4 0.74 0.62 0.47 0.70R1 RBE 0.74 0.62 0.49 0.70R1 R2 0.73 0.57 0.62 0.70RBE 0.73 0.57 0.58 0.68R2 0.71 0.53 0.69 0.68R1 0.62 0.43 0.69 0.63Table 3: Accuracy, Precision, Recall, and Bal-anced Accuracy of each ROUGE combination onTAC 2008-2010 pyramid.We considered all possible combinations of fourROUGE metrics that exhibited good propertiesin the analyses presented so far: ROUGE-1 (be-cause of its high recall), ROUGE-2 (because ofhigh accuracy when significance is not taken intoaccount) and ROUGE-4 and ROUGE-BE, whichshowed good balanced accuracy.The performance of these combinations for re-producing the decisions in TAC 2008-2010 basedon the pyramid5 evaluation are given in Table 3.The best balanced accuracy (76%) is for the com-bination of all four variants.
As more variants arecombined, precision increases but recalls drops.5 Comparison with automaticevaluations from AESOP 2011In 2009-2011, TAC ran the task of AutomaticallyEvaluating Summaries of Peers (AESOP), to com-5The ordering of the metric combinations relative to re-sponsiveness was almost identical to the ordering relative tothe pyramid evaluation, and precision and recall exhibited thesame trend as more metrics were added to the combination.134Pyramid A Pyramid B Responsiveness A Responsiveness BEvaluation Metric Acc P R BA Acc P R BA Acc P R BA Acc P R BACLASSY1 0.60 0.02 0.60 0.50 0.84 0.03 0.18 0.50 0.61 0.14 0.64 0.54 0.70 0.21 0.22 0.52DemokritosGR1 0.59 0.01 0.20 0.50 0.79 0.07 0.55 0.53 0.66 0.18 0.79 0.58 0.64 0.17 0.24 0.49uOttawa3 0.44 0.01 0.60 0.50 0.48 0.02 0.36 0.50 0.52 0.13 0.77 0.55 0.43 0.13 0.36 0.46DemokritosGR2 0.78 0.01 0.20 0.50 0.76 0.06 0.55 0.52 0.76 0.23 0.69 0.60 0.67 0.22 0.29 0.52C-S-IIITH4 0.69 0.01 0.20 0.50 0.77 0.07 0.64 0.53 0.82 0.29 0.74 0.63 0.60 0.15 0.24 0.47C-S-IIITH1 0.60 0.01 0.40 0.50 0.70 0.06 0.82 0.53 0.69 0.20 0.79 0.59 0.60 0.22 0.42 0.52BEwT-E 0.73 0.01 0.20 0.50 0.80 0.01 0.09 0.49 0.79 0.25 0.72 0.61 0.72 0.31 0.39 0.58R1-R2-R4-RBE 0.89 0.40 0.44 0.67 0.76 0.27 0.17 0.55 0.88 0.00 0.00 0.49 0.91 0.03 0.09 0.50R1-R4-RBE 0.89 0.40 0.44 0.67 0.77 0.35 0.24 0.59 0.88 0.00 0.00 0.49 0.90 0.03 0.09 0.50All ROUGEs 0.89 0.40 0.44 0.67 0.75 0.26 0.16 0.54 0.88 0.00 0.00 0.49 0.91 0.04 0.09 0.51Table 4: Best performing AESOP systems from TAC 2011; Scores within the 95% confidence intervalof the best are in bold face.pare automatic evaluation methods for automaticsummarization.
Here we show how the submit-ted AESOP metrics compare to the best ROUGEvariants that we have established so far.
We reportthe results on 2011 only, because even when thesame team participated in more than one year, themetrics submitted were different and the 2011 re-sults represent the best effort of these teams.
How-ever, as we saw in Table 1, in 2011 there were veryfew significant differences between the top sum-marization systems.
In this sense the tasks thatyear represent a challenging dataset for testing au-tomatic evaluations.The results for the best AESOP systems (ac-cording to one or more measures), and the cor-responding results for the ROUGE combinationsare shown in Table 4.
These AESOP systems are:CLASSY1 (Conroy et al, 2011; Rankel et al,2012), DemokritosGR1 and 2 (Giannakopoulos etal., 2008; Giannakopoulos et al, 2010), uOttawa3(Kennedy et al, 2011), C-S-IITH1 and 4 (Kumaret al, 2011; Kumar et al, 2012), and BEwT-E(Tratz and Hovy, 2008).6 The combination metricsachieve the highest accuracy by generally predict-ing correctly when there are no significant differ-ences between the systems.
In addition, for 2008-2010, where far more differences between systemsoccur, the results of Table 3 show the combina-tion metrics outperformed use of a single metricand are competitive with the best metrics of AE-SOP 2011.
Thus, the combination metrics havethe ability to discriminate under both conditionsgiving good prediction of human evaluation.6To perform the comparison in the table the scores foreach system and document set were needed.
Some systemshave changed after TAC 2011, but the data needed for thesecomparisons were not available.
BEwT-E did not participatein AESOP 2011 and these data were provided by StephenTratz.
Special thanks to Stephen for providing these data.6 ConclusionWe have tested the best-known automatic evalu-ation metrics (ROUGE) on several years of TACdata and compared their performance with re-cently developed AESOP metrics.
We discoveredthat some of the rarely used variants of ROUGEperform surprisingly well, and that by combin-ing different ROUGEs together, one can createan evaluation metric that is extremely competi-tive with metrics submitted to the latest AESOPtask.
Our results were reported in terms of sev-eral different measures, and in each case, com-pared how well the automatic metric predicted sig-nificant differences found in manual evaluation.We believe strongly that developers should includestatistical significance when reporting differencesin ROUGE scores of theirs and other systems,as this improves the accuracy and credibility oftheir results.
Significant improvement in multi-ple ROUGE scores is a significantly stronger in-dicator that the developers have made a notewor-thy improvement in text summarization.
Systemsthat report significant improvement using a com-bination of ROUGE-BE (or its improved versionBEwT-E) in conjunction with ROUGE-1, 2, and4, are more likely to give rise to summaries thathumans would judge as significantly better.AcknowledgmentsThe authors would like to thank Ed Hovy whoraised the question ?How well do automatic met-rics perform when comparing top systems??
Ed?scomments helped motivate this work.
In addition,we would like to thank our anonymous referees fortheir insightful comments, which contributed sig-nificantly to this paper.135ReferencesEnrique Amigo?, Julio Gonzalo, and Felisa Verdejo.2012.
The heterogeneity principle in evaluationmeasures for automatic summarization.
In Pro-ceedings of Workshop on Evaluation Metrics andSystem Comparison for Automatic Summarization,pages 36?43, Montre?al, Canada, June.
Associationfor Computational Linguistics.John M. Conroy and Hoa Trang Dang.
2008.
Mindthe gap: Dangers of divorcing evaluations of sum-mary content from linguistic quality.
In Proceedingsof the 22nd International Conference on Compu-tational Linguistics (Coling 2008), pages 145?152,Manchester, UK, August.
Coling 2008 OrganizingCommittee.John M. Conroy, Judith D. Schlesinger, and Dianne P.O?Leary.
2011.
Nouveau-ROUGE: A Novelty Met-ric for Update Summarization.
Computational Lin-guistics, 37(1):1?8.George Giannakopoulos, Vangelis Karkaletsis,George A. Vouros, and Panagiotis Stamatopoulos.2008.
Summarization system evaluation revisited:N-gram graphs.
TSLP, 5(3).George Giannakopoulos, George A. Vouros, and Van-gelis Karkaletsis.
2010.
Mudos-ng: Multi-document summaries using n-gram graphs (tech re-port).
CoRR, abs/1012.2042.Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Ju-nichi Fukumoto.
2006.
Automated summarizationevaluation with basic elements.
In Proceedings ofthe Fifth International Conference on Language Re-sources and Evaluation (LREC?06), pages 899?902.Alistair Kennedy, Anna Kazantseva Saif Mohammad,Terry Copeck, Diana Inkpen, and Stan Szpakowicz.2011.
Getting emotional about news.
In Fourth TextAnalysis Conference (TAC 2011).Niraj Kumar, Kannan Srinathan, and Vasudeva Varma.2011.
Using unsupervised system with least linguis-tic features for tac-aesop task.
In Fourth Text Analy-sis Conference (TAC 2011).N.
Kumar, K. Srinathan, and V. Varma.
2012.
Us-ing graph based mapping of co-occurring words andcloseness centrality score for summarization evalua-tion.
Computational Linguistics and Intelligent TextProcessing, pages 353?365.Chin-Yew Lin.
2004.
Rouge: A package for auto-matic evaluation of summaries.
In Stan SzpakowiczMarie-Francine Moens, editor, Text SummarizationBranches Out: Proceedings of the ACL-04 Work-shop, pages 74?81, Barcelona, Spain, July.
Associa-tion for Computational Linguistics.Annie Louis and Ani Nenkova.
2013.
Automaticallyassessing machine summary content without a goldstandard.
Computational Linguistics, 39:267?300.Ani Nenkova and Annie Louis.
2008.
Can you sum-marize this?
identifying correlates of input difficultyfor multi-document summarization.
In ACL, pages825?833.Ani Nenkova, Rebecca J. Passonneau, and KathleenMcKeown.
2007.
The pyramid method: Incorpo-rating human content selection variation in summa-rization evaluation.
TSLP, 4(2).Ani Nenkova.
2005.
Discourse factors in multi-document summarization.
In AAAI, pages 1654?1655.Karolina Owczarzak, John M. Conroy, Hoa TrangDang, and Ani Nenkova.
2012.
An assessmentof the accuracy of automatic evaluation in summa-rization.
In Proceedings of Workshop on EvaluationMetrics and System Comparison for Automatic Sum-marization, pages 1?9, Montre?al, Canada, June.
As-sociation for Computational Linguistics.Peter Rankel, John Conroy, Eric Slud, and DianneO?Leary.
2011.
Ranking human and machine sum-marization systems.
In Proceedings of the 2011Conference on Empirical Methods in Natural Lan-guage Processing, pages 467?473, Edinburgh, Scot-land, UK., July.
Association for Computational Lin-guistics.Peter A. Rankel, John M. Conroy, and Judith D.Schlesinger.
2012.
Better metrics to automaticallypredict the quality of a text summary.
Algorithms,5(4):398?420.Stephen Tratz and Eduard Hovy.
2008.
Summarisa-tion evaluation using transformed basic elements.
InProceedings TAC 2008.
NIST.John von Neumann and Oskar Morgenstern.
1953.Theory of games and economic behavior.
PrincetonUniv.
Press, Princeton, NJ, 3. ed.
edition.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Pro-ceedings of the 18th conference on Computationallinguistics - Volume 2, COLING ?00, pages 947?953, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.136
