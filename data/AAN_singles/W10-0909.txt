Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 70?77,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsTowards Learning Rules from Natural TextsJanardhan Rao Doppa, Mohammad NasrEsfahani, Mohammad S. SorowerThomas G. Dietterich, Xiaoli Fern, and Prasad TadepalliSchool of EECS, Oregon State UniversityCorvallis, OR 97330, USAfdoppa,nasresfm,sorower,tgd,xfern,tadepallg@cs.orst.eduAbstractIn this paper, we consider the problem of in-ductively learning rules from specific facts ex-tracted from texts.
This problem is challeng-ing due to two reasons.
First, natural textsare radically incomplete since there are alwaystoo many facts to mention.
Second, naturaltexts are systematically biased towards nov-elty and surprise, which presents an unrep-resentative sample to the learner.
Our solu-tions to these two problems are based on build-ing a generative observation model of what ismentioned and what is extracted given whatis true.
We first present a Multiple-predicateBootstrapping approach that consists of it-eratively learning if-then rules based on animplicit observation model and then imput-ing new facts implied by the learned rules.Second, we present an iterative ensemble co-learning approach, where multiple decision-trees are learned from bootstrap samples ofthe incomplete training data, and facts are im-puted based on weighted majority.1 IntroductionOne of the principal goals of learning by readingis to make the vast amount of natural language text This material is based upon work supported by the De-fense Advanced Research Projects Agency (DARPA) underContract No.
FA8750-09-C-0179.
Any opinions, findings andconclusions or recommendations expressed in this material arethose of the author(s) and do not necessarily reflect the views ofthe DARPA, or the Air Force Research Laboratory (AFRL).
Wethank the reviewers for their insightful comments and helpfulsuggestions.which is on the web accessible to automatic process-ing.
There are at least three different ways in whichthis can be done.
First, factual knowledge on theweb can be extracted as formal relations or tuplesof a data base.
A number of information extractionsystems, starting from the WebKb project (Craven etal., 2000), to Whirl (Cohen, 2000) to the TextRunner(Etzioni et al, 2008) project are of this kind.
Theytypically learn patterns or rules that can be appliedto text to extract instances of relations.
A secondpossibility is to learn general knowledge, rules, orgeneral processes and procedures by reading naturallanguage descriptions of them, for example, extract-ing formal descriptions of the rules of the UnitedStates Senate or a recipe to make a dessert.
A thirdinstance of machine reading is to generalize the factsextracted from the text to learn more general knowl-edge.
For example, one might learn by generalizingfrom reading the obituaries that most people live lessthan 90 years, or people tend to live and die in thecountries they were born in.
In this paper, we con-sider the problem of learning such general rules byreading about specific facts.At first blush, learning rules by reading specificfacts appears to be a composition of information ex-traction followed by rule induction.
In the above ex-ample of learning from obituaries, there is reason tobelieve that this reductionist approach would workwell.
However, there are two principal reasons whythis approach of learning directly from natural textsis problematic.
One is that, unlike databases, the nat-ural texts are radically incomplete.
By this we meanthat many of the facts that are relevant to predictingthe target relation might be missing in the text.
This70is so because in most cases the set of relevant factsis open ended.The second problem, in some ways more worri-some, is that the natural language texts are systemat-ically biased towards newsworthiness, which corre-lates with infrequency or novelty.
This is sometimescalled ?the man bites a dog phenomenon.
?1 Un-fortunately the novelty bias violates the most com-mon assumption of machine learning that the train-ing data is representative of the underlying truth, orequivalently, that any missing information is miss-ing at random.
In particular, since natural langaugetexts are written for people who already possess avast amount of prior knowledge, communication ef-ficiency demands that facts that can be easily in-ferred by most people are left out of the text.To empirically validate our two hypotheses of rad-ical incompleteness and systematic bias of naturaltexts, we have examined a collection of 248 doc-uments related to the topics of people, organiza-tions, and relationships collected by the LinguisticData Consortium (LDC).
We chose the target rela-tionship of the birth place of a person.
It turnedout that the birth place of some person is only men-tioned 23 times in the 248 documents, illustratingthe radical incompleteness of texts mentioned ear-lier.
Moreover, in 14 out of the 23 mentions of thebirth place, the information violates some default in-ferences.
For example, one of the sentences reads:?Ahmed Said Khadr, an Egyptian-born Cana-dian, was killed last October in Pakistan.
?Presumably the phrase ?Egyptian-born?
was con-sidered important by the reporter because it vio-lates our expectation that most Canadians are bornin Canada.
If Khadr was instead born in Canada,the reporter would mostly likely have left out?Canadian-born?
because it is too obvious to men-tion given he is a Canadian.
In all the 9 cases wherethe birth place does not violate the default assump-tions, the story is biographical, e.g., an obituary.In general, only a small part of the whole truth isever mentioned in a given document.
Thus, the re-porter has to make some choices as to what to men-tion and what to leave out.
The key insight of thispaper is that considering how these choices are made1?When a dog bites a man, that is not news, because it hap-pens so often.
But if a man bites a dog, that is news,?
attributedto John Bogart of New York Sun among others.is important in making correct statistical inferences.In the above example, wrong probabilities would bederived if one assumes that the birth place informa-tion is missing at random.In this paper we introduce the notion of a ?men-tion model,?
which models the generative processof what is mentioned in a document.
We also ex-tend this using an ?extraction model,?
which rep-resents the errors in the process of extracting factsfrom the text documents.
The mention model andthe extraction model together represent the probabil-ity that some facts are extracted given the true facts.For learning, we could use an explicit mentionmodel to score hypothesized rules by calculating theprobability that a rule is satisfied by the observedevidence and then pick the rules that are most likelygiven the evidence.
In this paper, we take the sim-pler approach of directly adapting the learning al-gorithms to an implicit mention model, by changingthe way a rule is scored by the available evidence.Since each text document involves multiple pred-icates with relationships between them, we learnrules to predict each predicate from the other pred-icates.
Thus, the goal of the system is to learn asufficiently large set of rules to infer all the miss-ing information as accurately as possible.
To ef-fectively bootstrap the learning process, the learnedrules are used on the incomplete training data to im-pute new facts, which are then used to induce morerules in subsequent iterations.
This approach is mostsimilar to the coupled semi-supervised learning of(Carlson et al, 2010) and general bootstrapping ap-proaches in natural language processing (Yarowsky,1995).
Since this is in the context of multiple-predicate learning in inductive logic programming(ILP) (DeRaedt and Lavra?c, 1996), we call this ap-proach ?Multiple-predicate Bootstrapping.
?One problem with Multiple-predicate Bootstrap-ping is potentially large variance.
To mitigae this,we consider the bagging approach, where multi-ple rule sets are learned from bootstrap samples ofthe training data with an implicit mention model toscore the rules.
We then use these sets of rules as anensemble to impute new facts, and repeat the pro-cess.We evaluate both of these approaches on realworld data processed through synthetic observationmodels.
Our results indicate that when the assump-71tions of the learner suit the observation model, thelearner?s performance is quite good.
Further, weshow that the ensemble approach significantly im-proves the performance of Multiple-predicate Boot-strapping.2 Probabilistic Observation ModelIn this section, we will introduce a notional prob-abilistic observation model that captures what factsare extracted by the programs from the text giventhe true facts about the world and the common senserules of the domain of discourse.The observation model is composed of the men-tion model and the extraction model.
The men-tion model P (MentDBjTrueDB;Rules) mod-els the probability distribution of mentioned facts,MentDB, given the set of true facts TrueDB andthe rules of the domain, Rules.
For example, ifa fact is always true, then the novelty bias dictatesthat it is not mentioned with a high probability.
Thesame is true of any fact entailed by a generally validrule that is common knowledge.
For example, thismodel predicts that since it is common knowledgethat Canadians are born in Canada, the birth placeis not mentioned if a person is a Canadian and wasborn in Canada.The extraction model P (ExtrDBjMentDB)models the probability distribution of extractedfacts, given the set of mentioned facts MentDB.For example, it might model that explicit facts areextracted with high probability and that the extractedfacts are corrupted by coreference errors.
Note thatthe extraction process operates only on the men-tioned part of the database MentDB; it has no in-dependent access to the TrueDB or the Rules.
Inother words, the mentioned database MentDB d-separates the extracted database ExtrDB from thetrue database TrueDB and the Rules, and the con-ditional probability decomposes.We could also model multiple documents gener-ated about the same set of facts TrueDB, and multi-ple databases independently extracted from the samedocument by different extraction systems.
Given anexplicit observation model, the learner can use it toconsider different rule sets and evaluate their like-lihood given some data.
The posterior probabilityof a rule set given an extracted database can be ob-tained by marginalizing over possible true and men-tioned databases.
Thus, in principle, the maximumlikelihood approach to rule learning could work byconsidering each set of rules and evaluating its pos-terior given the extracted database, and picking thebest set.
While conceptually straightforward, thisapproach is highly intractable due to the need tomarginalize over all possible mentioned and truedatabases.
Moreover, it seems unnecessary to forcea choice between sets of rules, since different rulesets do not always conflict.
In the next section, wedescribe a simpler approach of adapting the learningalgorithms directly to score and learn rules using animplicit mention model.3 Multiple-predicate Bootstrapping withan Implicit Mention ModelOur first approach, called ?Multiple-predicate Boot-strapping,?
is inspired by several pieces of workincluding co-training (Blum and Mitchell, 1998),multitask learning (Caruana, 1997), coupled semi-supervised learning (Carlson et al, 2010) and self-training (Yarowsky, 1995).
It is based on learning aset of rules for all the predicates in the domain giventhe others and using them to infer (impute) the miss-ing facts in the training data.
This is repeated forseveral iterations until no more facts can be inferred.The support of a rule is measured by the number ofrecords which satisfy the body of the rule, whereeach record roughly corresponds to a collection ofrelated facts that can be independently generated,e.g., information about a single football game or asingle news item.
The higher the support, the morestatistical evidence we have for judging its predictiveaccuracy.
To use a rule to impute facts, it needs tobe ?promoted,?
which means it should pass a certainthreshold support level.
We measure the precision ofa rule as the ratio of the number of records that non-trivially satisfy the rule to the number that satisfy itsbody, which is a proxy for the conditional probabil-ity of the head given the body.
A rule is non-triviallysatisfied by a record if the rule evaluates to true onthat record for all possible instantiations of its vari-ables, and there is at least one instantiation that sat-isfies its body.
Given multiple promoted rules whichapply to a given instance, we pick the rule with thehighest precision to impute its value.723.1 Implicit Mention ModelsWe adapt the multiple-predicate bootstrapping ap-proach to the case of incomplete data by adjustingthe scoring function of the learning algorithm to re-spect the assumed mention model.
Unlike in themaximum likelihood approach discussed in the pre-vious section, there is no explicit mention modelused by the learner.
Instead the scoring function isoptimized for a presumed implicit mention model.We now discuss three specific mention models andthe corresponding scoring functions.Positive Mention Model: In the ?positive men-tion model,?
it is assumed that any missing factis false.
This justifies counting evidence usingthe negation by failure assumption of Prolog.
Wecall this scoring method ?conservative.?
For exam-ple, the text ?Khadr, a Canadian citizen, was killedin Pakistan?
is counted as not supporting the rulecitizen(X,Y) ) bornIn(X,Y), as we arenot told that bornIn(Khadr,Canada).
Positivemention model is inapplicable for most instances oflearning from natural texts, except for special casessuch as directory web pages.Novelty Mention Model: In the ?novelty mentionmodel,?
it is assumed that facts are missing onlywhen they are entailed by other mentioned facts andrules that are common knowledge.
This suggestsan ?aggressive?
or optimistic scoring of candidaterules, which interprets a missing fact so that it sup-ports the candidate rule.
More precisely, a rule iscounted as non-trivially satisfied by a record if thereis some way of imputing the missing facts in therecord without causing contradiction.
For exam-ple, the text ?Khadr, a Canadian citizen was killedin Pakistan?
is counted as non-trivially supportingthe rule citizen(X,Y) ) bornIn(X,Y) be-cause, adding bornIn(Khadr, Canada) sup-ports the rule without contradicting the available ev-idence.
On the other hand, the above text doesnot support the rule killedIn(X,Y) ) cit-izen(X,Y) because the rule contradicts the evi-dence, assuming that citizen is a functional rela-tionship.Random Mention Model: In the ?random men-tion model,?
it is assumed that facts are missing atrandom.
Since the random facts can be true or false,this mention model suggests counting the evidencefractionally in proportion to its predicted prevalence.Following the previous work on learning from miss-ing data, we call this scoring method ?distributional?
(Saar-Tsechansky and Provost, 2007).
In distribu-tional scoring, we typically learn a distribution overthe values of a literal given its argument and use it toassign a fractional count to the evidence.
This is theapproach taken to account for missing data in Quin-lan?s decision tree algorithm (Quinlan, 1986).
Wewill use this as part of our Ensemble Co-Learningapproach of the next section.3.2 Experimental ResultsWe evaluated Multiple-predicate Bootstrap-ping with implicit mention models on theschema-based NFL database retrieved fromwww.databasefootball.com.
We developedtwo different synthetic observation models.
Theobservation models are based on the Noveltymention model and the Random mention modeland assume perfect extraction in each case.
Thefollowing predicates are manually provided: gameWinner (Game, Team), gameLoser(Game, Team), homeTeam(Game, Team), awayTeam(Game, Team), and teamInGame(Team, Game),with the natural interpretations.
To simplify arith-metic reasoning we replaced the numeric teamscores in the real database with two defined predi-cates teamSmallerScore(Team, Game) andteamGreaterScore(Team, Game) to indi-cate the teams with the smaller and the greaterscores.We generate two sets of synthetic data as follows.In the Random mention model, each predicate ex-cept the teamInGame predicate is omitted inde-pendently with probability p. The Novelty men-tion model, on the other hand, relies on the factthat gameWinner, gameLoser, and teamFi-nalScore are mutually correlated, as are home-Team and awayTeam.
Thus, it picks one predicate7305 01 0 01 5 02 0 0 00 .
20 .
40 .
60 .
800 .
51F r a c t i o no f mi ssin g pr e dicatesS u ppo r tt hr e sho l dA ccuracyM u l t i p l e ?
p r e d i c a t e B o o t s t r a p p i n g R e s u l t s f o r F A R M E R( A g g r e s s i v e ?
N o v e l t y M o d e l )05 01 0 01 5 02 0 0 0 0 .
20 .
4 0 .
60 .
80 .
80 .
8 50 .
90 .
9 51M u l t i p l e ?
p r e d i c a t e B o o t s t r a p p i n g R e s u l t s f o r F O I L( A g g r e s s i v e ?
N o v e l t y M o d e l )F r a c t i o no f mi ssin g p r edic atesS u ppo r tt hr e sho l dA ccuracy(a) (b)0204060801000  0.1  0.2  0.3  0.4  0.5  0.6AccuracyFraction of missing predicatesMultiple-predicate Bootstrapping Results for FARMER(Support threshold = 120)Novelty-AggressiveRandom-AggressiveRandom-ConservativeNovelty-Conservative0204060801000  0.1  0.2  0.3  0.4  0.5  0.6AccuracyFraction of missing predicatesMultiple-predicate Bootstrapping Results for FOIL(Support threshold = 120)Novelty-AggressiveRandom-AggressiveRandom-ConservativeNovelty-Conservative(c) (d)Figure 1: Multiple-predicate bootstrapping Results for (a) FARMER using aggressive-novelty model (b) FOIL usingaggressive-novelty model (c) FARMER with support threshold 120 (d) FOIL with support threshold 120from the first group to mention its values, and omit-seach of the other predicates independently withsome probability q.
Similarly it gives a value to oneof the two predicates in the second group and omitsthe other predicate with probability q.
One conse-quence of this model is that it always has one of thepredicates in the first group and one of the predicatesin the second group, which is sufficient to infer ev-erything if one knew the correct domain rules.
Weevaluate two scoring methods: the aggressive scor-ing and the conservative scoring.We employed two learning systems: Quinlan?sFOIL, which learns relational rules using a greedycovering algorithm (Quinlan, 1990; Cameron-Jones and Quinlan, 1994), and Nijssen and Kok?sFARMER, which is a relational data mining algo-rithm that searches for conjunctions of literals oflarge support using a bottom-up depth first search(Nijssen and Kok, 2003).
Both systems were appliedto learn rules for all target predicates.
One importantdifference to note here is that while FARMER seeksall rules that exceed the necessary support threshold,FOIL only learns rules that are sufficient to classifyall training instances into those that satisfy the tar-get predicate and those that do not.
Secondly, FOILtries to learn maximally deterministic rules, whileFARMER is parameterized by the minimum preci-sion of a rule.
We have not modified the way theyinterpret missing features during learning.
However,after the learning is complete, the rules learned byboth approaches are scored by interpreting the miss-ing data either aggressively or conservatively as de-scribed in the previous section.We ran both systems on synthetic data generatedusing different parameters that control the fractionof missing data and the minimum support threshold74needed for promotion.
In Figures 1(a) and 1(b), theX and Y-axes show the fraction of missing predi-cates and the support threshold for the novelty men-tion model and aggressive scoring of rules for FOILand FARMER.
On the Z-axis is the accuracy of pre-dictions on the missing data, which is the fractionof the total number of initially missing entries thatare correctly imputed.
We can see that aggressivescoring of rules with the novelty mention model per-forms very well even for large numbers of missingvalues for both FARMER and FOIL.
FARMER?sperformance is more robust than FOIL?s becauseFARMER learns all correct rules and uses whicheverrule fires.
For example, in the NFL domain, itcould infer gameWinner from gameLoser orteamSmallerScore or teamGreaterScore.In contrast, FOIL?s covering strategy prevents itfrom learning more than one rule if it finds one per-fect rule.
The results show that FOIL?s performancedegrades at larger fractions of missing data and largesupport thresholds.Figures 1(c) and 1(d) show the accuracy of pre-diction vs. percentage of missing predicates foreach of the mention models and the scoring meth-ods for FARMER and FOIL for a support thresholdof 120.
They show that agressive scoring clearly out-performs conservative scoring for data generated us-ing the novelty mention model.
In FOIL, aggressivescoring also seems to outperform conservative scor-ing on the dataset generated by the random mentionmodel at high levels of missing data.
In FARMER,the two methods perform similarly.
However, theseresults should be interpreted cautiously as they arederived from a single dataset which enjoys determin-istic rules.
We are working towards a more robustevaluation in multiple domains as well as data ex-tracted from natural texts.4 Ensemble Co-learning with an ImplicitMention ModelOne weakness of Multiple-predicate Bootstrappingis its high variance especially when significantamounts of training data are missing.
Aggressiveevaluation of rules in this case would amplify thecontradictory conclusions of different rules.
Thus,picking only one rule among the many possible rulescould lead to dangerously large variance.One way to guard against the variance problemis to use an ensemble approach.
In this section wetest the hypothesis that an ensemble approach wouldbe more robust and exhibit less variance in the con-text of learning from incomplete examples with animplicit mention model.
For the experiments in thissection, we employ a decision tree learner that uses adistributional scoring scheme to handle missing dataas described in (Quinlan, 1986).While classifying an instance, when a missingvalue is encountered, the instance is split into multi-ple pseudo-instances each with a different value forthe missing feature and a weight corresponding tothe estimated probability for the particular missingvalue (based on the frequency of values at this splitin the training data).
Each pseudo-instance is passeddown the tree according to its assigned value.
Af-ter reaching a leaf node, the frequency of the classin the training instances associated with this leaf isreturned as the class-membership probability of thepseudo-instance.
The overall estimated probabilityof class membership is calculated as the weightedaverage of class membership probabilities over allpseudo-instances.
If there is more than one missingvalue, the process recurses with the weights com-bining multiplicatively.
The process is similar atthe training time, except that the information gainat the internal nodes and the class probabilities atthe leaves are calculated based on the weights of therelevant pseudo-instances.We use the confidence level for pruning a decisiontree as a proxy for support of a rule in this case.
Bysetting this parameter to different values, we can ob-tain different degrees of pruning.Experimental Results: We use the Congres-sional Voting Records2 database for our experi-ments.
The (non-text) database includes the partyaffiliation and votes on 16 measures for each mem-ber of the U.S House Representatives.
Although thisdatabase (just like the NFL database) is complete,we generate two different synthetic versions of it tosimulate the extracted facts from typical news sto-ries on this topic.
We use all the instances includingthose with unknown values for training, but do notcount the errors on these unknown values.
We ex-2http://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records7560657075808590951000  0.1  0.2  0.3  0.4  0.5  0.6AccuracyPercentage of missing valuesEnsemble Co-learning with Random modelEnsemble size 1Ensemble size 5Ensemble size 10Ensemble size 15Ensemble size 2060657075808590951000  0.1  0.2  0.3  0.4  0.5  0.6AccuracyPercentage of missing valuesEnsemble Co-learning with Novelty modelEnsemble size 1Ensemble size 5Ensemble size 10Ensemble size 15Ensemble size 20(a) (b)60657075808590951000  0.1  0.2  0.3  0.4  0.5  0.6AccuracyPercentage of missing valuesEnsemble Co-learning vs Multiple-predicate Bootstrapping(Random model)Multiple-predicate BootstrappingEnsemble Colearning with size 2060657075808590951000  0.1  0.2  0.3  0.4  0.5AccuracyPercentage of missing valuesEnsemble Co-learning vs Multiple-predicate Bootstrapping(Novelty model)Multiple-predicate BootstrappingEnsemble Colearning with size 20(c) (d)Figure 2: Results for (a) Ensemble co-learning with Random mention model (b) Ensemble co-learning with Nov-elty mention model (c) Ensemble co-learning vs Multiple-predicate Bootstrapping with Random mention model (d)Ensemble co-learning vs Multiple-predicate Bootstrapping with Novelty mention modelperiment with two different implicit mention mod-els: Random and Novelty.
These are similar to thosewe defined in the previous section.
In the Randommention model, each feature in the dataset is omittedindependently with a probability p. Since we don?tknow the truely predictive rules here unlike in thefootball domain, we learn the novelty model fromthe complete dataset.
Using the complete datasetwhich has n features, we learn a decision tree to pre-dict each feature from all the remaining features.
Weuse these n decision trees to define our novelty men-tion model in the following way.
For each instancein the complete dataset, we randomly pick a featureand see if it can be predicted from all the remain-ing features using the predictive model.
If it canbe predicted, then we will omit it with probabilityp and mention it otherwise.
We use different boot-strap samples to learn the ensemble of trees and im-pute the values using a majority vote.
Note that, thedecision tree cannot always classify an instance suc-cessfully.
Therefore, we will impute the values onlyif the count of majority vote is greater than someminimum threshold (margin).
In our experiments,we use a margin value equal to half of the ensem-ble size and a fixed support of 0.3 (i.e., the confi-dence level for pruning) while learning the decisiontrees.
We employ J48, the WEKA version of Quin-lan?s C4.5 algorithm to learn our decision trees.
Wecompute the accuracy of predictions on the missingdata, which is the fraction of the total number of ini-tially missing entries that are imputed correctly.
Wereport the average results of 20 independent runs.We test the hypothesis that the Ensemble Co-learning is more robust and exhibit less variancein the context of learning from incomplete exam-ples when compared to Multiple-predicate Boot-76strapping.
In Figures 2(a)-(d), the X and Y-axesshow the percentage of missing values and the pre-diction accuracy.
Figures 2(a) and (b) shows the be-havior with different ensemble sizes (1, 5, 10, 15 and20) for both Random and Novelty mention model.We can see that the performance improves as theensemble size grows for both random and noveltymodels.
Figures 2(c) and (d) compares Multiple-predicate Bootstrapping with the best results overthe different ensemble sizes.
We can see that En-semble Co-learning outperforms Multiple-predicateBootstrapping.5 DiscussionLearning general rules by reading natural languagetexts faces the challenges of radical incompletenessand systematic bias.
Statistically, our notion of in-completeness corresponds to the Missing Not AtRandom (MNAR) case, where the probability of anentry being missed may depend on its value or thevalues of other observed variables (Rubin, 1976).One of the key insights of statisticians is tobuild an explicit probabilistic model of missingness,which is captured by our mention model and ex-traction model.
This missingness model might thenbe used in an Expectation Maximization (EM) ap-proach (Schafer and Graham, 2002), where alter-nately, the missing values are imputed by their ex-pected values according to the missingness modeland the model parameters are estimated using themaximum likelihood approach.
Our ?Multiple-predicate Bootstrapping?
is loosely analogous to thisapproach, except that the imputation of missing val-ues is done implicitly while scoring the rules, andthe maximum likelihood parameter learning is re-placed with the learning of relational if-then rules.In the Multiple Imputation (MI) framework of(Rubin, 1987; Schafer and Graham, 2002), the goalis to reduce the variance due to single imputationby combining the results of multiple imputations.This is analogous to Ensemble Co-learning, wherewe learn multiple hypotheses from different boot-strap samples of the training data and impute valuesusing the weighted majority algorithm over the en-semble.
We have shown that the ensemble approachimproves performance.ReferencesAvrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of COLT, pages 92?100.
Morgan Kaufmann Pub-lishers.R.
M. Cameron-Jones and J. R. Quinlan.
1994.
Effi-cient top-down induction of logic programs.
In ACMSIGART Bulletin.Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-tevam R. Hruschka Jr., and Tom M. Mitchell.
2010.Coupled semi-supervised learning for information ex-traction.
In Proceedings of the Third ACM Interna-tional Conference on Web Search and Data Mining(WSDM 2010).R.
Caruana.
1997.
Multitask learning: A knowledge-based source of inductive bias.
Machine Learning,28:41?75.William W. Cohen.
2000.
WHIRL: a word-based infor-mation representation language.
Artif.
Intell., 118(1-2):163?196.Mark Craven, Dan DiPasquo, Dayne Freitag, AndrewMcCallum, Tom M. Mitchell, Kamal Nigam, and Sea?nSlattery.
2000.
Learning to construct knowledge basesfrom the world wide web.
Artif.
Intell., 118(1-2):69?113.Luc DeRaedt and Nada Lavra?c.
1996.
Multiple predi-cate learning in two inductive logic programming set-tings.
Logic Journal of the IGPL, 4(2):227?254.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel S. Weld.
2008.
Open information extractionfrom the web.
Commun.
ACM, 51(12):68?74.Siegfried Nijssen and Joost N. Kok.
2003.
Efficient fre-quent query discovery in FARMER.
In PKDD, pages350?362.Ross Quinlan.
1986.
Induction of decision trees.
Ma-chine Learning, 1(1):81?106.Ross Quinlan.
1990.
Learning logical definitions fromrelations.
Machine Learning, 5:239?266.D.
B. Rubin.
1976.
Inference and missing data.Biometrika, 63(3):581.D.
B. Rubin.
1987.
Multiple Imputation for non-response in surveys.
Wiley New York.Maytal Saar-Tsechansky and Foster Provost.
2007.Handling missing values when applying classifica-tion models.
Journal of Machine Learning Research,8:1625?1657.J.
L. Schafer and J. W. Graham.
2002.
Missing data: Ourview of the state of the art.
Psychological methods,7(2):147?177.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proceed-ings of ACL.77
