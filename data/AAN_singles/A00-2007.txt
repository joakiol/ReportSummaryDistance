Noun Phrase Recognition by System CombinationEr ik  F .
T jong  K im SangCenter for Dutch  Language and SpeechUnivers i ty  of Antwerper ikt@uia,  ua.
ac.
beAbstractThe performance ofmachine learning algorithms canbe improved by combining the output of differentsystems.
In this paper we apply this idea to therecognition of noun phrases.
We generate differentclassifiers by using different representations of thedata.
By combining the results with voting tech-niques described in (Van Halteren et al, 1998) wemanage to improve the best reported performanceson standard ata sets for base noun phrases and ar-bitrary noun phrases.1 Int roduct ion(Van Halteren et al, 1998) and (Brill and Wu, 1998)describe a series of successful experiments for im-proving the performance of part-of-speech taggers.Their results have been obtained by combining theoutput of different aggers with system combinationtechniques uch as majority voting.
This approachcancels errors that are made by the minority of thetaggers.
With the best voting technique, the com-bined results decrease the lowest error rate of thecomponent taggers by as much as 19% (Van Hal-teren et al, 1998).
The fact that combination ofclassifiers leads to improved performance has beenreported in a large body of machine learning work.We would like to know what improvement combi-nation techniques would cause in noun phrase recog-nition.
For this purpose, we apply a single memory-based learning technique to data that has been rep-resented in different ways.
We compare various com-bination techniques on a part of the Penn Treebankand use the best method on standard ata sets forbase noun phrase recognition and arbitrary nounphrase recognition.2 Methods  and exper imentsIn this section we start with a description of our task:recognizing noun phrases.
After this we introducethe different data representations we use and ourmachine learning algorithms.
We conclude with anoutline of techniques for combining classifier esults.2.1 Task descriptionNoun phrase recognition can be divided in two tasks:recognizing base noun phrases and recognizing arbi-trary noun phrases.
Base noun phrases (baseNPs)are noun phrases which do not contain another nounphrase.
For example, the sentenceIn \[ early trading \] in \[ Hong Kong \]\[ Monday \] , \[ gold \] was quoted at\[ $ 366.50 \] \[ an ounce \] .contains six baseNPs (marked as phrases betweensquare brackets).
The phrase $ 366.50 an ounceis a noun phrase as well.
However, it is not abaseNP since it contains two other noun phrases.Two baseNP data sets have been put forward by(Ramshaw and Marcus, 1995).
The main data setconsist of four sections (15-18) of the Wall StreetJournal (WSJ) part of the Penn Treebank (Marcuset al, 1993) as training material and one section(20) as test material 1.
The baseNPs in this data areslightly different from the ones that can be derivedfrom the Treebank, most notably in the attachmentof genitive markers.The recognition task involving arbitrary nounphrases attempts to find both baseNPs and nounphrases that contain other noun phrases.
A stan-dard data set for this task was put forward at theCoNLL-99 workshop.
It consist on the same partsof the Penn Treebank as the main baseNP data set:WSJ sections 15-18 as training data and section 20as test data 2.
The noun phrases in this data setare the same as in the Treebank and therefore thebaseNPs in this data set are slightly different fromthe ones in the (Ramshaw and Marcus, 1995) datasets.In both tasks, performance is measured with threescores.
First, with the percentage of detected nounphrases that are correct (precision).
Second, withthe percentage of noun phrases in the data thatwere found by the classifier (recall).
And third,1This (Ramshaw and Marcus, 1995) baseNP data set isavailable via ftp://ftp.cis.upenn.edu/pub/chunker/2Software for generating the data is available fromhttp://lcg-www.uia.ac.be/conl199/npb/50with the FZ=I rate which is equal to (2*preci-sion*recall)/(precision+recall).
The latter rate hasbeen used as the target for optimization.2.2 Data  representat ionIn our example sentence in section 2.1, noun phrasesare represented by bracket structures.
Both (Mufiozet al, 1999) and (Tjong K im Sang and Veenstra,1999) have shown how classifiers can process bracketstructures.
One  classifier can be trained to recog-nize open brackets (O) while another will processclose brackets (C).
Their results can be converted tobaseNPs by making pairs of open and close bracketswith large probability scores (Mufioz et al, 1999) orby regarding only the shortest phrases between openand close brackets as baseNPs (Tjong K im Sang andVeenstra, 1999).
We have used the bracket repre-sentation (O+C)  in combination with the secondbaseNP construction method.An  alternative representation for baseNPs hasbeen put forward by (Ramshaw and Marcus, 1995).They  have defined baseNP recognition as a taggingtask: words can be inside a baseNP (1) or outside ofbaseNPs (O).
In the case that one baseNP immedi-ately follows another baseNP, the first word in thesecond baseNP receives tag B.
Example:Ino earlyi tradingr ino Hongl KongzMondayB ,o goldz waso quotedo ato $r366.50z anB ounce/ -oThis set of three tags is sufficient for encodingbaseNP structures ince these structures are non-recursive and nonoverlapping.
(Tjong Kim Sang and Veenstra, 1999) have pre-sented three variants of this tagging representation.First, the B tag can be used for the first word ofevery noun phrase (IOB2 representation).
Second,instead of the B tag an E tag can be used to mark thelast word of a baseNP immediately before anotherbaseNP (IOE1).
And third, the E tag can be usedfor every noun phrase final word (IOE2).
They haveused the (Ramshaw and Marcus, 1995) representa-tion as well (IOB1).
We will use these four taggingrepresentations a  well as the O+C representation.2.3 Mach ine  learn ing a lgor i thmsWe have used the memory-based learning algorithmIBI-IG which is part of TiMBL package (Daelemanset al, 1999b).
In memory-based learning the train-ing data is stored and a new item is classified by themost frequent classification among training itemswhich are closest o this new item.
Data items arerepresented as sets of feature-value pairs.
In IBI-IGeach feature receives a weight which is based on theamount of information which it provides for com-puting the classification of the items in the trainingdata.
These feature weights are used for computingthe distance between a pair of data items (Daele-mans et al, 1999b).
ml-IG has been used success-fully on a large variety of natural anguage process-ing tasks.Beside IBI - IG,  we have used IGTREE in the combi-nation experiments.
IGTREE is a decision tree vari-ant of II31-IG (Daelemans et al, 1999b).
It uses thesame feature weight method as IBI-IG.
Data itemsare stored in a tree with the most important featuresclose to the root node.
A new item is classified bytraveling down from the root node until a leaf nodeis reached or no branch is available for the currentfeature value.
The most frequent classification of thecurrent node will be chosen.2.4 Combinat ion  techn iquesOur experiments will result in different classifica-tions of the data and we need to find out how tocombine these.
For this purpose we have evaluateddifferent voting mechanisms, effectively the votingmethods as described in (Van Halteren et al, 1998).All combination methods assign some weight to theresults of the individual classifier.
For each input to-ken, they pick the classification score with the high-est total score.
For example, if five classifiers haveweights 0.9, 0.4, 0.8, 0.6 and 0.6 respectively andthey classify some token as npstart, null, npstart,null and null, then the combination method will picknpstart since it has a higher total score (1.7) thannull (1.6).
The values of the weights are usually es-timated by processing a part of the training data,the tuning data, which has been kept separate astraining data for the combination process.In the first voting method, each of the five classi-tiers receives the same weight (majority).
The sec-ond method regards as the weight of each individualclassification algorithm its accuracy on the tuningdata (TotPrecision).
The third voting method com-putes the precision of each assigned tag per classifierand uses this value as a weight for the classifier inthose cases that it chooses the tag (TagPrecision).The fourth method uses the tag precision weightsas well but it subtracts from them the recall val-ues of the competing classifier esults.
Finally, thefifth method uses not only a weight for the currentclassification but it also computes weights for otherpossible classifications.
The other classifications aredetermined by examining the tuning data and reg-istering the correct values for every pair of classifierresults (pair-wise voting).Apart from these five voting methods we have alsoprocessed the output streams with two classifiers:IBI-IG (memory-based) and IGTREE (decision tree).This approach is called classifier stacking.
Like (VanHalteren et al, 1998), we have used different inputversions: one containing only the classifier outputand another containing both classifier output anda compressed representation f the classifier input.51trainAll correctMajority correctMinority correctAll wrong0 C96.21% 96.66%1.98% 1.64%0.88% 0.75%0.93% 0.95%Table 1: Token classification agreement between thefive classifiers applied to the baseNP training dataafter conversion to the open bracket (O) and theclose bracket representation (C).For the latter purpose we have used the part-of-speech tag of the current word.3 Resu l t sOur first goal was to find out whether system combi-nation could improve performance of baseNP recog-nition and, if this was the fact, to select the bestcombination technique.
For this purpose we per-formed a 10-fold cross validation experiment on thebaseNP training data, sections 15-18 of the WSJpart of the Penn Treebank (211727 tokens).
Likethe data used by (Ramshaw and Marcus, 1995),this data was retagged by the Brill tagger in or-der to obtain realistic part-of-speech (POS) tags 3.The data was segmented into baseNP parts and non-baseNP parts in a similar fashion as the data usedby (Ramshaw and Marcus, 1995).The data was converted to the five data represen-tations (IOB1, IOB2, IOE1, IOE2 and O+C) andIBI-IG was used to classify it by using 10-fold crossvalidation.
This means that the data was dividedin ten consecutive parts of about the same size af-ter which each part was used as test data with theother nine parts as training data.
The standard pa-rameters of IBI-IG have been used except for k, thenumber of examined nearest neighbors, which wasset to three.
Each word in the data was representedby itself and its POS tag and additionally a left andright context of four word-POS tag pairs.
For thefirst four representations, wehave used a second pro-cessing stage as well.
In this stage, a word was repre-sented by itself, its POS tag, a left and right contextof three word-POS tag pairs and a left and rightcontext of two classification results of the first pro-cessing stage (see figure 1).
The second processingstage improved the FZ=I scores with almost 0.7 onaverage.The classifications of the IOB1, IOB2, IOE1 andIOE2 representations were converted to the openbracket (O) and close bracket (C) representations.aNo perfect Penn Treebank POS tags will be available fornovel texts.
If we would have used the Treebank POS tagsfor NP recognition, our performance rates would have beenunrealistically high.trainRepresentationIOB1IOB2IOE1IOE2O+CSimple VotingMajorityTotPrecisionTagPrecisionPrecision-Recall098.01%97.8O%97.97%97.89%97.92%98.19%98.19%98.19%98.19%C98.14%98.08%98.04%98.08%98.13%98.30%98.30%98.30%98.30%Pairwise VotingTagPair 98.19% 98.30%Memory-BasedTags 98.19% 98.34%Tags + POS 98.19% 98.35%Decision TreesTags 98.17% 98.34%Tags + POS 98.17% 98.34%Table 2: Open and close bracket accuracies for thebaseNP training data (211727 tokens).
Each com-bination performs significantly better than any ofthe five individual classifiers listed under Represen-tation.
The performance differences between thecombination methods are not significant.After this conversion step we had five O results andfive C results.
In the bracket representations, to-kens can be classified as either being the first tokenof an NP (or the last in the C representation) or not.The results obtained with these representations havebeen measured with accuracy rates: the percentageof tokens that were classified correctly.
Only aboutone in four tokens are at a baseNP boundary soguessing that a text does not contains baseNPs willalready give us an accuracy of 75%.
Therefore theaccuracy rates obtained with these representationsare high and the room for improvement is small (seetable 1).
However, because of the different treatmentof neighboring chunks, the five classifiers disagree inabout 2.5% of the classifications.
It seems useful touse combination methods for finding the best classi-fication for those ambiguous cases.The five O results and the five C results were pro-cessed by the combination techniques described insection 2.4.
The accuracies per input token for thecombinations can be found in table 2.
For bothdata representations, all combinations perform sig-nificantly better than the best individual classifier(p<0.001 according to a X 2 test) 4.
Unlike in (Van4We have performed significance computat ions  on thebracket accuracy rates because we have been unable to finda satisfactory method for comput ing significance scores for52trading/NN in/IN Hong/NNP Kong/NNP Monday/NNP ,/, gold/NN was/VBD quoted/VBNin/IN Hong/NNP/I Kong/NNP/I Monday/NNP ,/,/O gold/NN/I was/VBDFigure 1: Example of the classifier input features used for classifying Monday in the example sentence.
Thefirst processing stage (top) contains a word and POS context of four left and four right while the secondprocessing stage (bottom) contains a word and POS context of three and a chunk tag context of two.section 20Majority voting(Mufioz et al, 1999)(Tjong Kim Sang and Veenstra~ 1999)(Ramshaw and Marcus, 1995)(Argarnon et al, 1998)accuracy precisionO:98.10% C:98.29% 93.63%O:98.1% C:98.2% 93.1%97.58% 92.50%97.37% 91.80%91.6%recall FZ=I92.89% 93.2692.4% 92.892.25% 92.3792.27% 92.0391.6% 91.6section 00 accuracy precisionMajority voting 0:98.59% C:98.65% 95.04%r (Tjong Kim Sang and Veenstra, 1999) 98.04% 93.71%(Ramshaw and Marcus, 1995) 97.8% 93.1%recall FB=I94.75% 94.9093.90% 93.8193.5% 93.3Table 3: The results of majority voting of different data representations applied to the two standard atasets put forward by (Ramshaw and Marcus, 1995) compared with earlier work.
The accuracy scores indicatehow often a word was classified correctly with the representation used (O, C or IOB1).
The training datafor WSJ section 20 contained 211727 tokens while section 00 was processed with 950028 tokens of trainingdata.
Majority voting outperforms all earlier eported results for the two data sets.Halteren et al, 1998), the best voting technique didnot outperform the best stacked classifier.
Further-more the performance differences between the com-bination methods are not significant (p>0.05).
Toour surprise the five voting techniques performed thesame.
We assume that this has happened becausethe accuracies of the individual classifiers do not dif-fer much and because the classification i volves abinary choice.Since there is no significant difference between thecombination methods, we can use any of them in theremaining experiments.
We have chosen to use ma-jority voting because it does not require tuning data.We have applied it to the two data sets mentionedin (Ramshaw and Marcus, 1995).
The first data setuses WSJ sections 15-18 as training data (211727tokens) and section 20 as test data (47377 tokens).The second one uses sections 02-21 of the same cor-pus as training data (950028 tokens) and section 00as test data (46451 tokens).
All data sets were pro-cessed in the same way as described earlier.
Theresults of these experiments can be found in table 3.With section 20 as test set, we managed to reducethe error of the best result known to us with 6% withthe error rate dropping from 7.2% to 6.74%, and forsection 00 this difference was almost 18% with theFB= 1 rates.error rate dropping from 6.19% to 5.10% (see table3).We have also applied majority voting to the NPdata set put forward on the CoNLL-99 workshop.In this task the goal is to recognize all NPs.
Wehave approached this as repeated baseNP recogni-tion.
A first stage detects the baseNPs.
The recog-nized NPs are replaced by their presumed head wordwith a special POS tag and the result is send to asecond stage which recognizes NPs with one level ofembedding.
The output of this stage is sent to athird stage and this stage finds NPs with two levelsof embedding and so on.In the first processing stage we have used the fivedata representations with majority voting.
This ap-proach did not work as well for other stages.
TheO+C representation utperformed the other fourrepresentations by a large margin for the valida-tion data 5.
This caused the combined output ofall five representations being worse than the O+Cresult.
Therefore we have only used the O+C repre-sentation for recognizing nombaseNPs.
The overallsystem reached an F~=I score of 83.79 and this isslightly better than the best rate reported at the5The validation data  is the test set we have used for esti-mat ing  the best parameters  for the CoNLL experiment: WSJsection 21.53CoNLL-99 workshop (82.98 (CoNLL-99, 1999), anerror reduction of 5%).4 Re la ted  work(Abney, 1991) has proposed to approach parsing bystarting with finding correlated chunks of words.The chunks can be combined to trees by a sec-ond processing stage, the attacher.
(Ramshawand Marcus, 1995) have build a chunker by apply-ing transformation-based learning to sections of thePenn Treebank.
Rather than working with bracketstructures, they have represented the chunking taskas a tagging problem.
POS-like tags were used toaccount for the fact that words were inside or out-side chunks.
They have applied their method to twosegments of the Penn Treebank and these are stillbeing used as benchmark data sets.Several groups have continued working with theRamshaw and Marcus data sets for base nounphrases.
(Argamon et al, 1998) use Memory-BasedSequence Learning for recognizing both NP chunksand VP chunks.
This method records POS tag se-quences which contain chunk boundaries and usesthese sequences to classify the test data.
Its per-formance is somewhat worse than that of Ramshawand Marcus (F~=1=91.6 vs. 92.0) but it is the bestresult obtained without using lexical information 6.
(Cardie and Pierce, 1998) store POS tag sequencesthat make up complete chunks and use these se-quences as rules for classifying unseen data.
Thisapproach performs worse than the method of Arga-mon et al (F~=1=90.9).Three papers mention having used the memory-based learning method IBI-IG.
(Veenstra, 1998) in-troduced cascaded chunking, a two-stage process inwhich the first stage classifications are used to im-prove the performance in a second processing stage.This approach reaches the same performance l velas Argamon et al but it requires lexical informa-tion.
(Daelemans et al, 1999a) report a good per-formance for baseNP recognition but they use a dif-ferent data set and do not mention precision andrecall rates.
(Tjong Kim Sang and Veenstra, 1999)compare different data representations forthis task.Their baseNP results are slightly better than thoseof Ramshaw and Marcus (F~=1=92.37).
(XTAG, 1998) describes a baseNP chunker builtfrom training data by a technique called supertag-ging.
The performance of the chunker was animprovement of the Ramshaw and Marcus results(Fz=I =92.4).
(Mufioz et al, 1999) use SNOW, a net-work of linear units, for recognizing baseNP phrases6We have applied majority voting of five data represen-tations to the Ramshaw and Marcus data set without usinglexical information and the results were: accuracy O: 97.60%,accuracy C: 98.10%, precision: 92.19%, recall: 91.53% andF~=I: 91.86.and SV phrases.
They compare two data representa-tions and report that a representation with bracketstructures outperforms the IOB tagging representa-tion introduced by (Ramshaw and Marcus, 1995).SNoW reaches the best performance on this task(Fz=I =92.8).There has been less work on identifying eneralnoun phrases than on recognizing baseNPs.
(Os-borne, 1999) extended a definite clause grammarwith rules induced by a learner that was based uponthe maximum description length principle.
He pro-cessed other parts of the Penn Treebank than wewith an F~=I rate of about 60.
Our earlier effortto process the CoNLL data set was performed inthe same way as described in this paper but with-out using the combination method for baseNPs.
Weobtained an F~=I rate of 82.98 (CoNLL-99, 1999).5 Conc lud ing  remarksWe have put forward a method for recognizing nounphrases by combining the results of a memory-basedclassifier applied to different representations of thedata.
We have examined ifferent combination tech-niques and each of them performed significantly bet-ter than the best individual classifier.
We have cho-sen to work with majority voting because it doesnot require tuning data and thus enables the indi-vidual classifiers to use all the training data.
Thisapproach was applied to three standard ata setsfor base noun phrase recognition and arbitrary nounphrase recognition.
For all data sets majority votingimproved the best result for that data set known toUS.Varying data representations is not the only wayfor generating different classifiers for combinationpurposes.
We have also tried dividing the trainingdata in partitions (bagging) and working with artifi-cial training data generated by a crossover-like oper-ator borrowed from genetic algorithm theory.
Withour memory-based classifier applied to this data, wehave been unable to generate a combination whichimproved the performance of its best member.
An-other approach would be to use different classifica-tion algorithms and combine the results.
We areworking on this but we are still to overcome the prac-tical problems which prevent us from obtaining ac-ceptable results with the other learning algorithms.AcknowledgementsWe would like to thank the members of the CNTSgroup in Antwerp, Belgium, the members of the ILKgroup in Tilburg, The Netherlands and three anony-mous reviewers for valuable discussions and com-ments.
This research was funded by the EuropeanTMR network Learning Computational Grammars ~.7 http://lcg-www.uia.ac.be/55.ReferencesSteven Abney.
1991.
Parsing by chunks.
In Principle-Based Parsing.
Kluwer Academic Publishers.Shlomo Argamon, Ido Dagan, and Yuval Krymolowski.1998.
A memory-based approach to learning shal-low natural language patterns.
In Proceedings ofCOLING-ACL '98.
Association for ComputationalLinguistics.Eric Brill and Jun Wu.
1998.
Classifier combinationfor improved lexical disambiguation.
In Proceedingsof COLING-ACL '98.
Association for ComputationalLinguistics.Claire Cardie and David Pierce.
1998.
Error-drivenpruning of treebank grammars for base noun phraseidentification.
In Proceedings of COLING-ACL '98.Association for Computational Linguistics.CoNLL-99.
1999.
Conll-99 home page.
http://lcg-www.uia.ac.be/conl199/.Walter Daelemans, Antal van den Bosch, and Jakub Za-vrel.
1999a.
Forgetting exceptions i harmful in lan-guage learning.
Machine Learning, 34.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
1999b.
TiMBL: Tilburg Mem-ory Based Learner, version 2.0, Reference Guide.ILK Technical Report 99-01. http://ilk.kub.nl/-ilk/papers/ilk9901.ps.gz.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19.Marcia Mufioz, Vasin Punyakanok, Dan Roth, and DayZimak.
1999.
A learning approach to shallow parsing.In Proceedings of EMNLP-WVLC'99.
Association forComputational Linguistics.Miles Osborne.
1999.
MDL-based DCG inductionfor NP identification.
In Miles Osborne and ErikTjong Kim Sang, editors, CoNLL-99 ComputationalNatural Language Learning.
Association for Compu-tational Linguistics.Lance A. Ramshaw and Mitchell P. Marcus.
1995.Text chunking using transformation-based l arning.In Proceedings of the Third ACL Workshop on VeryLarge Corpora.
Association for Computational Lin-guistics.Erik F. Tjong Kim Sang and Jorn Veenstra.
1999.
Rep-resenting text chunks.
In Proceedings of EACL '99.
As-sociation for Computational Linguistics.Hans van Halteren, Jakub Zavrel, and Walter Daele-mans.
1998.
Improving data driven wordclass taggingby system combination.
In Proceedings of COLING-A CL'98.
Association for Computational Linguistics.Jorn Veenstra.
1998.
Fast NP chunking using memory-based learning techniques.
In BENELEARN-98: Pro-ceedings of the Eighth Belgian-Dutch Conference onMachine Learning.
ATO-DLO, Wageningen, report352.The XTAG Research Group.
1998.
A LexicalizedTree Adjoining Grammar for English.
IRCS TechReport 98-18, University of Pennsylvania.
(alsocs.CL/9809024).55
