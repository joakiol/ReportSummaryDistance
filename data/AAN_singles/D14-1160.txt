Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1511?1521,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsSensicon: An Automatically Constructed Sensorial LexiconSerra Sinem Tekiro?luUniversity of TrentoFondazione Bruno KesslerTrento, Italytekiroglu@fbk.euG?zde ?zbalTrento RISETrento, Italygozbalde@gmail.comCarlo StrapparavaFondazione Bruno KesslerTrento, Italystrappa@fbk.euAbstractConnecting words with senses, namely,sight, hearing, taste, smell and touch, tocomprehend the sensorial information inlanguage is a straightforward task for hu-mans by using commonsense knowledge.With this in mind, a lexicon associatingwords with senses would be crucial for thecomputational tasks aiming at interpreta-tion of language.
However, to the best ofour knowledge, there is no systematic at-tempt in the literature to build such a re-source.
In this paper, we present a senso-rial lexicon that associates English wordswith senses.
To obtain this resource, weapply a computational method based onbootstrapping and corpus statistics.
Thequality of the resulting lexicon is evaluatedwith a gold standard created via crowd-sourcing.
The results show that a sim-ple classifier relying on the lexicon out-performs two baselines on a sensory clas-sification task, both at word and sentencelevel, and confirm the soundness of theproposed approach for the construction ofthe lexicon and the usefulness of the re-source for computational applications.1 IntroductionSensorial information interpenetrates languageswith various semantic roles in different levels sincethe main interaction instrument of humans with theoutside world is the sensory organs.
The trans-formation of the raw sensations that we receivethrough the sensory organs into our understand-ing of the world has been an important philo-sophical topic for centuries.
According to a clas-sification that dates back to Aristotle (Johansen,1997), senses can be categorized into five modali-ties, namely, sight, hearing, taste, smell and touch.With the help of perception, we can process thedata coming from our sensory receptors and be-come aware of our environment.
While interpret-ing sensory data, we unconsciously use our exist-ing knowledge and experience about the world tocreate a private experience (Bernstein, 2010).Language has a significant role as our maincommunication device to convert our private ex-periences to shared representations of the environ-ment that we perceive (Majid and Levinson, 2011).As a basic example, onomatopoeic words, such asknock or woof, are acquired by direct imitation ofthe sounds allowing us to share the experience ofwhat we hear.
As another example, where an im-itation is not possible, is that giving a name to acolor, such as blue, provides a tool to describe avisual feature of an object.
In addition to the wordsthat describe the direct sensorial features of ob-jects, languages include many other lexical itemsthat are connected to sensory modalities in varioussemantic roles.
For instance, while some wordscan be used to describe a perception activity (e.g.,to sniff, to watch, to feel), others can simply bephysical phenomena that can be perceived by sen-sory receptors (e.g., light, song, salt, smoke).Common usage of language, either written orspoken, can be very dense in terms of sensorialwords.
As an example, the sentence ?I felt the coldbreeze.?
contains three sensorial words: to feel asa perception activity, cold as a perceived sensorialfeature and breeze as a physical phenomenon.
Theconnection to the sense modalities of the wordsmight not be mutually exclusive, that is to say aword can be associated with more than one senses.For instance, the adjective sweet could be associ-ated with both the senses of taste and smell.
Whilewe, as humans, have the ability to connect wordswith senses intuitively by using our commonsenseknowledge, it is not straightforward for machinesto interpret sensorial information.Making use of a lexicon containing sensorialwords could be beneficial for many computa-tional scenarios.
Rodriguez-Esteban and Rzhetsky1511(2008) report that using words related to sensesin a text could clarify the meaning of an abstractconcept by facilitating a more concrete imagina-tion.
To this respect, an existing text could be au-tomatically modified with sensory words for vari-ous purposes such as attracting attention or biasingthe audience towards a specific concept.
Addition-ally, sensory words can be utilized to affect privatepsychology by inducing a positive or negative sen-timent (Majid and Levinson, 2011).
For instance,de Araujo et al.
(2005) show that the pleasantnesslevel of the same odor can be altered by labeling itas body odor or cheddar cheese.
As another moti-vation, the readability and understandability of textcould also be enhanced by using sensory words(Rodriguez-Esteban and Rzhetsky, 2008).
A com-pelling use case of a sensorial lexicon is that auto-matic text modification to change the density of aspecific sense could help people with sensory dis-abilities.
For instance, while teaching a concept toa congenitally blind child, an application that elim-inates color-related descriptions would be benefi-cial.
A sensorial lexicon could also be exploited bysearch engines to personalize the results accordingto user needs.Advertising is another broad area which wouldbenefit from such a resource especially by usingsynaesthesia1, as it strengthens creative thinkingand it is commonly exploited as an imaginationboosting tool in advertisement slogans (Pricken,2008).
As an example, we can consider the slogans?The taste of a paradise?
where the sense of sightis combined with the sense of taste or ?Hear thebig picture?
where sight and hearing are merged.Various studies have been conducted bothin computational linguistics and cognitive sci-ence that build resources associating words withseveral cognitive features such as abstractness-concreteness (Coltheart, 1981; Turney et al.,2011), emotions (Strapparava and Valitutti, 2004;Mohammad and Turney, 2010), colors (?zbal etal., 2011; Mohammad, 2011) and imageability(Coltheart, 1981).
However, to the best of ourknowledge, there is no attempt in the literature tobuild a resource that associates words with senses.In this paper, we propose a computational methodto automatically generate a sensorial lexicon thatassociates words in English with senses.
Ourmethod consists of two main steps.
First, we gen-1American Heritage Dictionary (http://ahdictionary.com/) defines synaesthesia in linguis-tics as the description of one kind of sense impression byusing words that normally describe another.erate a set of seed words for each sense categorywith the help of a bootstrapping approach.
In thesecond step, we exploit a corpus based probabilis-tic technique to create the final lexicon.
We eval-uate this lexicon with the help of a gold standardthat we obtain by using the crowdsourcing serviceof CrowdFlower2.The sensorial lexicon, which we named Sen-sicon, embodies 22,684 English lemmas togetherwith their part-of-speech (POS) information thathave been linked to one or more of the five senses.Each entry in this lexicon consists of a lemma-POSpair and a score for each sensory modality that in-dicates the degree of association.
For instance, theverb stink has the highest score for smell as ex-pected while the scores for the other four sensesare very low.
The noun tree, which is a concreteobject and might be perceived by multiple senses,has high scores for sight, touch and smell.The rest of the paper is organized as follows.We first review previous work relevant to this taskin Section 2.
Then in Section 3, we describe theproposed approach in detail.
In Section 4, we ex-plain the annotation process that we conducted andthe evaluation strategy that we employed.
Finally,in Section 5, we draw our conclusions and outlinepossible future directions.2 Related WorkSince to the best of our knowledge there is no at-tempt in the literature to automatically associatewords with human senses, in this section we willsummarize the most relevant studies that focusedon linking words with various other cognitive fea-tures.There are several studies focusing on word-emotion associations.
WordNet Affect Lexicon(Strapparava and Valitutti, 2004) maps WordNet(Fellbaum, 1998) synsets to various cognitive fea-tures (e.g., emotion, mood, behaviour).
This re-source is created by using a small set of synsetsas seeds and expanding them with the help of se-mantic and lexical relations among these synsets.Yang et al.
(2007) propose a collocation modelwith emoticons instead of seed words while creat-ing an emotion lexicon from a corpus.
Perrie et al.
(2013) build a word-emotion association lexiconby using subsets of a human-annotated lexicon asseed sets.
The authors use frequencies, counts, orunique seed words extracted from an n-gram cor-pus to create lexicons in different sizes.
They pro-2http://www.crowdflower.com/1512pose that larger lexicons with less accurate genera-tion method perform better than the smaller humanannotated lexicons.
While a major drawback ofmanually generated lexicons is that they require agreat deal of human labor, crowdsourcing servicesprovide an easier procedure for manual annota-tions.
Mohammad and Turney (2010) generate anemotion lexicon by using the crowdsourcing ser-vice provided by Amazon Mechanical Turk3 andit covers 14,200 term-emotion associations.Regarding the sentiment orientations and sub-jectivity levels of words, Sentiwordnet (Esuli andSebastiani, 2006) is constructed as an extensionto WordNet and it provides sentiments in synsetlevel.
Positive, negative and neutral values are as-signed to synsets by using ternary classifiers andsynset glosses.
Another study that has been inspi-rational for the design of our approach is Baneaet al.
(2008).
The authors generate a subjectivitylexicon starting with a set of seed words and thenusing a similarity measure among the seeds and thecandidate words.Another cognitive feature relevant to sensorialload of the words is the association between col-ors and words.
Mohammad (2011) builds a color-word association lexicon by organizing a crowd-sourcing task on Amazon Mechanical Turk.
In-stead, ?zbal et al.
(2011) aim to automate thisprocess and propose three computational methodsbased on image analysis, language models and la-tent semantic analysis (LSA) (Landauer and Du-mais, 1997).
The authors compare these meth-ods against a gold standard obtained by the crowd-sourcing service of Amazon Mechanical Turk.The best performance is obtained by using imagefeatures while LSA performs slightly better thanthe baseline.Finally, there have been efforts in the liter-ature about the association of words with theirabstractness-concreteness and imageability levels.MRC Psycholinguistic Database (Coltheart, 1981)includes abstractness-concreteness and imageabil-ity ratings of a small set of words determinedaccording to psycholinguistic experiments.
Tur-ney et al.
(2011) propose to use LSA similaritiesof words with a set of seed words to automati-cally calculate the abstractness and concretenessdegrees of words.3http://www.mturk.com/mturk3 Automatic Association of Senses withWordsWe adopt a two phased computational approach toconstruct a large sensorial lexicon.
First, we em-ploy a bootstrapping strategy to generate a suffi-cient number of sensory seed words from a smallset of manually selected seed words.
In the sec-ond phase, we perform a corpus based probabilisticmethod to estimate the association scores to builda larger lexicon.3.1 Selecting Seed WordsThe first phase of the lexicon construction pro-cess aims to collect sensorial seed words, whichare directly related to senses (e.g., sound, tastyand sightedness).
To achieve that, we utilizeda lexical database called FrameNet (Baker et al.,1998), which is built upon semantic frames of con-cepts in English and lexical units (i.e., words) thatevoke these frames.
The basic idea behind thisresource is that meanings of words can be under-stood on the basis of a semantic frame.
A semanticframe consists of semantic roles called frame ele-ments, which are manually annotated in more than170,000 sentences.
We have considered FrameNetto be especially suitable for the collection of sen-sorial seed words since it includes semantic rolesand syntactic features of sensational and percep-tional concepts.In order to determine the seed lemma-POS pairsin FrameNet, we first manually determined 31frames that we found to be highly connected tosenses such as Hear, Color, Temperature and Per-ception_experience.
Then, we conducted an an-notation task and asked 3 annotators to determinewhich senses the lemma-POS pairs evoking thecollected frames are associated with.
At the end ofthis task, we collected all the pairs (i.e.
277) with100% agreement to constitute our initial seed set.This set contains 277 lemma-POS pairs associatedwith a specific sense such as the verb click withhearing, the noun glitter with sight and aromaticwith smell.3.2 Seed Expansion via BootstrappingIn this step, we aim to extend the seed list that weobtained from FrameNet with the help of a boot-strapping approach.
To achieve that, we adopt asimilar approach to Dias et al.
(2014), who pro-pose a repetitive semantic expansion model to au-tomatically build temporal associations of synsetsin WordNet.
Figure 1 provides an overview ofthe bootstrapping process.
At each iteration, we1513WordNetMapNetSVM Cross-validationSynset SetExpansionSenseSeedSynsetsFrameNetBreak-PointDetectionFigure 1: Bootstrapping procedure to expand theseed list.first expand the seed list by using semantic rela-tions provided by WordNet.
We then evaluate theaccuracy of the new seed list for sense classifica-tion by means of cross-validation against WordNetglosses.
For each sense, we continue iterating un-til the cross-validation accuracy becomes stable orstarts to decrease.
The following sections explainthe whole process in detail.3.2.1 Extending the Seed List with WordNetWhile the initial sensory seed list obtained fromFrameNet contains only 277 lemma-POS pairs,we extend this list by utilizing the semantic re-lations provided by WordNet.
To achieve that,we first map each lemma-POS pair in the seedlist to WordNet synsets with the help of Map-Net (Tonelli and Pighin, 2009), which is a re-source providing direct mapping between Word-Net synsets and FrameNet lexical units.
Then, weadd to the list the synsets that have WordNet re-lations direct antonymy, similarity, derived-from,derivationally-related, pertains-to, attribute andalso-see with the already existing seeds.
For in-stance, we add the synset containing the verb laughfor the synset of the verb cry with the relation di-rect antonymy, or the synset containing the ad-jective chilly for the synset of the adjective coldwith the relation similarity.
We prefer to use theserelations as they might allow us to preserve thesemantic information as much as possible duringthe extension process.
It is worth mentioning thatthese relations were also found to be appropriatefor preserving the affective connotation by Vali-tutti et al.
(2004).
Additionally, we use the rela-tions hyponym and hyponym-instance to enrich theseed set with semantically more specific synsets.For instance, for the noun seed smell, we expandthe list with the hyponyms of its synset such as thenouns bouquet, fragrance, fragrancy, redolenceand sweetness.3.2.2 Cross-validation of Sensorial ModelAfter obtaining new synsets with the help ofWord-Net relations in each bootstrapping cycle, we builda five-class sense classifier over the seed synsetsdefined by their glosses provided in WordNet.Similarly to Dias et al.
(2014), we assume thatthe sense information of sensorial synsets is pre-served in their definitions.
Accordingly, we em-ploy a support vector machine (SVM) (Boser etal., 1992; Vapnik, 1998) model with second de-gree polynomial kernel by representing the glossof each synset as a vector of lemmas weighted bytheir counts.
For each synset, its gloss is lemma-tized by using Stanford Core NLP4 and cleanedfrom the stop words.
After each iteration cycle, weperform a 10-fold cross-validation in the updatedseed list to detect the accuracy of the new sensorialmodel.
For each sense class, we continue iteratingand thereby expanding the seed list until the clas-sifier accuracy steadily drops.Table 1 lists the precision (P), recall (R) andF1 values obtained for each sense after each it-eration until the bootstrapping mechanism stops.While the iteration number is provided in the firstcolumn, the values under the last column grouppresent the micro-average of the resulting multi-class classifier.
The change in the performancevalues of each class in each iteration reveals thatthe number of iterations required to obtain the seedlists varies for each sense.
For instance, the F1value of touch continues to increase until the fourthcycle whereas hearing records a sharp decrease af-ter the first iteration.After the bootstrapping process, we create thefinal lexicon by repeating the expansion for eachclass until the optimal number of iterations isreached.
The last row of Table 1, labeled as Final,demonstrates the accuracy of the classifier trainedand tested on the final lexicon, i.e., using the seedsselected after iteration 2 for Sight, iteration 1 forHearing, iteration 3 for Taste and Smell and it-eration 4 for Touch.
According to F1 measure-ments of each iteration, while hearing and tastehave a lower value for the final model, sight, smelland touch have higher results.
It should also benoted that the micro-average of the F1 values ofthe final model shows an increase when comparedto the third iteration, which has the highest av-erage F1 value among the iterations.
At the end4http://nlp.stanford.edu/software/corenlp.shtml1514of this step we have a seed synset list consistingof 2572 synsets yielding the highest performancewhen used to learn a sensorial model.3.3 Sensorial Lexicon Construction UsingCorpus StatisticsAfter generating the seed lists consisting of synsetsfor each sense category with the help of a set ofWordNet relations and a bootstrapping process, weuse corpus statistics to create our final sensoriallexicon.
More specifically, we exploit a proba-bilistic approach based on the co-occurrence ofthe seeds and the candidate lexical entries.
Sinceworking on the synset level would raise the datasparsity problem in synset tagged corpora such asSemCor (Miller et al., 1993) and we need a cor-pus that provides sufficient statistical information,we migrate from synset level to lexical level.
Ac-cordingly, we treat each POS role of the same lem-mas as a distinct seed and extract 4287 lemma-POSpairs from 2572 synsets.
In this section, we explainthe steps to construct our final sensorial lexicon indetail.3.3.1 Corpus and Candidate WordsAs a corpus, we use a subset of English Giga-Word 5th Edition released by Linguistic Data Con-sortium (LDC)5.
This resource is a collection ofalmost 10 million English newswire documentscollected in recent years, whose content sums upto nearly 5 billion words.
The richly annotatedGigaWord data comprises automatic parses ob-tained with the Stanford parser (Klein and Man-ning, 2003) so that we easily have access to thelemma and POS information of each word in theresource.
For the scope of this study, we workon a randomly chosen subset that contains 79800sentences and we define a co-occurrence event asthe co-existence of a candidate word and a seedword within a window of 9 words(the candidateword, 4 words to its left and 4 words to its right).In this manner, we analyze the co-occurrence ofeach unique lemma-POS pair in the corpuswith thesense seeds.
We eliminate the candidates whichhave less than 5 co-occurrences with the sense cat-egories.3.3.2 Normalized Pointwise MutualInformationFor the co-occurrence analysis of the candidatewords and seeds, we use pointwise mutual in-formation (PMI), which is simply a measure of5http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC2011T07association between the probability of the co-occurrence of two events and their individual prob-abilities when they are assumed to be independent(Church and Hanks, 1990).
PMI can be exploitedas a semantic similarity measure (Han et al., 2013)and it is calculated as:PMI(x, y) = log[p(x, y)p(x)p(y)](1)To calculate the PMI value of a candidate wordand a specific sense, we consider p(x) as the proba-bility of the candidate word to occur in the corpus.Therefore, p(x) is calculated as p(x) = c(x)/N ,where c(x) is the total count of the occurrences ofthe candidate word x in the corpus and N is the to-tal co-occurrence count of all words in the corpus.Similarly, we calculate p(y) as the total occurrencecount of all the seeds for the sense considered (y).p(y) can thus be formulated as c(y)/N .
p(x,y) isthe probability of the co-occurrence of a candidateword x with a sense event y.Amajor shortcoming of PMI is its sensitivity forlow frequency data (Bouma, 2009).
As one pos-sible solution, the author introduces NormalizedPointwiseMutual Information (NPMI), which nor-malizes the PMI values to the range (-1, +1) withthe following formula:NPMI(x, y) =PMI(x, y)?
log p(x, y) (2)We adopt the proposed solution and calculateNPMI values for each candidate word and fivesense events in the corpus.
Sensicon covers 22,684lemma-POS pairs and a score for each sense classthat denotes their association degrees.4 EvaluationTo evaluate the performance of the sensorial clas-sification and the quality of Sensicon, we first cre-ated a gold standard with the help of a crowdsourc-ing task.
Then, we compared the decisions com-ing from Sensicon against the gold standard.
Inthis section, we explain the annotation process thatwe conducted and the evaluation technique that weadopted in detail.
We also provide a brief discus-sion about the obtained results.4.1 Crowdsourcing to Build a Gold StandardThe evaluation phase of Sensicon requires a goldstandard data to be able to conduct a meaningfulassessment.
Since to our best knowledge there isno resource with sensory associations of words or1515Sight Hearing Taste Smell Touch Micro-averageIt# P R F1 P R F1 P R F1 P R F1 P R F1 P R F11 .873 .506 .640 .893 .607 .723 .716 .983 .828 .900 .273 .419 .759 .320 .451 .780 .754 .7292 .666 .890 .762 .829 .414 .552 .869 .929 .898 .746 .473 .579 .714 .439 .543 .791 .787 .7723 .643 .878 .742 .863 .390 .538 .891 .909 .900 .667 .525 .588 .720 .482 .578 .796 .786 .7764 .641 .869 .738 .832 .400 .540 .866 .888 .877 .704 .500 .585 .736 .477 .579 .784 .774 .7655 .640 .869 .737 .832 .400 .540 .866 .888 .877 .704 .500 .585 .738 .474 .578 .784 .774 .764Final .805 .827 .816 .840 .408 .549 .814 .942 .873 .685 .534 .600 .760 .582 .659 .800 .802 .790Table 1: Bootstrapping cycles with validation results.majority class 3 4 5 6 7 8 9 10word 0 0.98 3.84 9.96 11.63 16.66 34.41 12.42sentence 0.58 2.35 7.07 10.91 13.27 15.63 21.23 16.51Table 2: Percentage of words and sentences in each majority class.sentences, we designed our own annotation taskusing the crowdsourcing service of CrowdFlower.For the annotation task, we first compiled a col-lection of sentences to be annotated.
Then, we de-signed two questions that the annotators were ex-pected to answer for a given sentence.
While thefirst question is related to the sense association of awhole sentence, the second asks the annotators tocollect a fine-grained gold standard for word-senseassociations.We collected a dataset of 340 sentences consist-ing of 300 advertisement slogans from 11 adver-tisement categories (e.g., fashion, food, electron-ics) and 40 story sentences from a story corpus.
Wecollected the slogans from various online resourcessuch as http://slogans.wikia.com/wiki andhttp://www.adslogans.co.uk/.
The storycorpus is generated as part of a dissertation re-search (Alm, 2008) and it provides stories as a col-lection of sentences.In both resources, we first determined the can-didate sentences that had at least five tokens andcontained at least one adjective, verb or noun.
Inaddition, we replaced the brand names in the ad-vertisement slogans with X to prevent any bias.For instance, the name of a well-known restaurantin a slogan might cause a bias towards taste.
Fi-nally, the slogans used in the annotation task werechosen randomly among the candidate sentencesby considering a balanced number of slogans fromeach category.
Similarly, 40 story sentences wereselected randomly among the candidate story sen-tences.
To give a more concrete idea, for ourdataset we obtained an advertisement slogan suchas ?X?s Sugar Frosted Flakes They?re Great!?
or astory sentence such as ?The ground is frozen, andbesides the snow has covered everything.
?In the crowdsourcing task we designed, the an-notators were required to answer 2 questions fora given sentence.
In the first question, they wereasked to detect the human senses conveyed or di-rectly described by a given sentence.
To exemplifythese cases, we provided two examples such as ?Isaw the cat?
that directly mentions the action ofseeing and ?The sun was shining on the blue wa-ter.?
that conveys the sense of sight by using vi-sual descriptions or elements like ?blue?
or ?shine?which are notable for their visual properties.
Theannotators were able to select more than one sensefor each sentence and together with the five senseswe provided another option as None which shouldbe selected when an annotator could not associatea sentence with any sense.
The second questionwas devoted do determining word-sense associa-tions.
Here, the annotators were expected to asso-ciate the words in each sentence with at least onesense.
Again, annotators could choose None forevery word that they could not confidently asso-ciate with a sense.The reliability of the annotators was evaluatedon the basis of 20 control sentences which werehighly associated with a specific sense and whichincluded at least one sensorial word.
For instance,for the control sentence ?The skin you love totouch?, we only considered as reliable the anno-tators who associated the sentence with touch andthe word touch with the sense touch6.
Similarly,for the slogan ?The most colourful name in cos-metics.
?, an annotator was expected to associatethe sentence with at least the sense sight and theword colorful to at least the sense sight.
The raterswho scored at least 70% accuracy on average on6If the annotators gave additional answers to the expectedones, we considered their answers as correct.1516the control questions for the two tasks were con-sidered to be reliable.
Each unit was annotated byat least 10 reliable raters.Similarly to Mohammad (2011) and ?zbal et al.
(2011), we calculated the majority class of eachannotated item to measure the agreement amongthe annotators.
Table 2 demonstrates the observedagreement at both word and sentence level.
Since10 annotators participated in the task, the annota-tions with a majority class greater than 5 can beconsidered as reliable (?zbal et al., 2011).
In-deed, for 85.10% of the word annotations the ab-solute majority agreed on the same decision, while77.58% of the annotations in the sentence levelhave majority class greater than 5.
The high agree-ment observed among the annotators in both casesconfirms the quality of the resulting gold standarddata.In Table 3, we present the results of the anno-tation task by providing the association percent-age of each category with each sense, namely sight(Si), hear (He), taste (Ta), smell (Sm) and touch(To).
As demonstrated in the table, while the senseof sight can be observed in almost every advertise-ment category and in story, smell and taste are veryrare.
We observe that the story sentences invoke allsensory modalities except taste, although the per-centage of sentences annotated with smell is rela-tively low.
Similarly, personal care category hasan association with four of the senses while theother categories have either very low or no asso-ciation with some of the sense classes.
Indeed, theperceived sensorial effects in the sentences varyaccording to the category such that the slogans inthe travel category are highly associated with sightwhereas the communication category is highly as-sociated with hearing.
While the connection of thefood and beverages categories with taste is veryhigh as expected, they have no association with thesense of smell.
This kind of analysis could be use-ful for copywriters to decide which sensorymodal-ities to invoke while creating a slogan for a specificproduct category.4.2 Evaluation MeasuresBased on the annotation results of our crowdsourc-ing task, we propose an evaluation technique con-sidering that a lemma-POS or a sentence might beassociated with more than one sensory modalities.Similar to the evaluation framework defined by?zbal et al.
(2011), we adapt the evaluation mea-sures of SemEval-2007 English Lexical Substitu-tion Task (McCarthy and Navigli, 2007), whereCategory Si He Ta Sm Topersonal care 49.36 10.75 0.00 13.29 26.58travel 58.18 0.00 29.09 0.00 12.72fashion 43.47 0.00 0.00 26.08 30.43beauty 84.56 0.00 0.00 0.00 15.43computing 32.25 59.13 0.00 0.00 8.60food 0.00 5.46 94.53 0.00 0.00beverages 22.68 0.00 59.79 0.00 17.52communications 25.00 67.50 0.00 0.00 0.075electronics 45.94 54.05 0.00 0.00 0.00education 28.57 42.85 0.00 0.00 28.57transport 61.81 38.18 0.00 0.00 0.00story 58.37 20.81 0.00 7.23 13.57Table 3: The categories of the annotated data andtheir sense association percentages.a system generates one or more possible substitu-tions for a target word in a sentence preserving itsmeaning.For a given lemma-POS or a sentence, whichwe will name as item in the rest of the section, weallow our system to provide as many sensorial as-sociations as it determines by using a specific lex-icon.
While evaluating a sense-item association ofa method, a best and an oot score are calculated byconsidering the number of the annotators who as-sociate that sense with the given item, the numberof the annotators who associate any sense with thegiven item and the number of the senses the sys-tem gives as an answer for that item.
More specif-ically, best scoring provides a credit for the bestanswer for a given item by dividing it to the num-ber of the answers of the system.
oot scoring, onthe other hand, considers only a certain number ofsystem answers for a given item and does not di-vide the credit to the total number of the answers.Unlike the lexical substitution task, a limited setof labels (i.e., 5 sense labels and none) are allowedfor the sensorial annotation of sentences or lemma-POS pairs.
For this reason, we reformulate out-of-ten (oot) scoring used by McCarthy and Navigli(2007) as out-of-two.In Equation 3, best score for a given item i fromthe set of items I, which consists of the items an-notated with a specific sense by a majority of 5annotators, is formulated where Hiis the multisetof gold standard sense associations for item i andSiis the set of sense associations provided by thesystem.
oot scoring, as formulated in Equation 4,accepts up to 2 sense associations s from the an-swers of system Sifor a given item i and the creditis not divided by the number of the answers of the1517system.best (i) =?s?Sifreq (s ?
Hi)|Hi| ?
|Si|(3)oot (i) =?s?Sifreq (s ?
Hi)|Hi|(4)As formulated in Equation 5, to calculate theprecision of an item-sense association task with aspecific method, the sum of the scores (i.e., bestor oot) for each item is divided by the number ofitems A, for which the method can provide an an-swer.
In recall, the denominator is the number ofthe items in the gold standard for which an answeris given by the annotators.P =?i?Ascorei|A|R =?i?Iscorei|I|(5)4.3 Evaluation MethodFor the evaluation, we compare the accuracy ofa simple classifier based on Sensicon against twobaselines on a sense classification task both atword and sentence level.
To achieve that, we usethe gold standard that we obtain from the crowd-sourcing task and the evaluation measures best andoot.
The lexicon-based classifier simply assignsto each word in a sentence the sense values foundin the lexicon.
The first baseline assigns the mostfrequently annotated sensory modality, which issight, via crowdsourcing task with a float value of1.0 to each lemma-POS pair in the sensorial lexi-con.
The second baseline instead builds the associ-ations by using a Latent Semantic Analysis spacegenerated from the same subset of LDC that we ex-ploit for constructing Sensicon.
More specifically,this baseline calculates the LSA similarities be-tween each candidate lemma-POS pair and senseclass by taking the cosine similarity between thevector of the target lemma-POS pair and the aver-age of the vectors of the related sensory word (i.e.,see, hear, touch, taste, and smell) for each possi-ble POS tag.
For instance, to get the associationscore of a lemma-POS pair with the sense sight,we first average the vectors of see (noun) and see(verb) before calculating its cosine similarity withthe target lemma-POS pair.For the first experiment, i.e., word-sense as-sociation, we automatically associate the lemma-POS pairs obtained from the annotated dataset withsenses by using i) Sensicon, ii) the most-frequent-sense baseline (MFS), iii) the LSA baseline.
Toachieve that, we lemmatize and POS tag each sen-tence in the dataset by using Stanford Core NLP.In the end, for each method and target word, weobtain a list of senses sorted according to theirsensorial association values in decreasing order.It is worth noting that we only consider the non-negative sensorial associations for Sensicon andboth baselines.
For instance, Sensicon associatesthe noun wine with [smell, taste, sight].
In thisexperiment, best scoring considers the associatedsenses as the best answer, smell, taste, sight ac-cording to the previous example, and calculates ascore with respect to the best answer in the goldstandard and the number of the senses in this an-swer.
Instead, oot scoring takes the first two an-swers, smell and taste according to the previousexample, and assigns the score accordingly.To determine the senses associated with a sen-tence for the second experiment, we use a methodsimilar to the one proposed by Turney (2002).
Foreach sense, we simply calculate the average scoreof the lemma-POS pairs in a sentence.
We set athreshold value of 0 to decide whether a sentenceis associated with a given sense.
In this manner,we obtain a sorted list of average sensory scoresfor each sentence according to the three methods.For instance, the classifier based on Sensicon as-sociates the sentence Smash it to pieces, love it tobits.
with [touch, taste].
For the best score, onlytouch would be considered, whereas oot wouldconsider both touch and taste.4.4 Evaluation ResultsIn Table 4, we list the F1 values that we obtainedwith the classifier using Sensicon and the two base-lines (MFS and LSA) according to both best andoot measures.
In addition, we provide the perfor-mance of Sensicon in two preliminary steps, beforebootstrapping (BB) and after bootstrapping (AB)to observe the incremental progress of the lexiconconstruction method.
As can be observed from thetable, the best performance for both experiments isachieved by Sensicon when compared against thebaselines.While in the first experiment the lexicon gen-erated after the bootstrapping step (AB) providesa very similar performance to the final lexiconaccording to the best measure, it can only buildsense associations for 69 lemmas out of 153 ap-pearing in the gold standard.
Instead, the final lex-icon attempts to resolve 129 lemma-sense associa-tions and results in a better recall value.
Addition-ally, AB yields a very high precision as expected,1518since it is created by a controlled semantical ex-pansion from manually annotated sensorial words.BB lexicon includes only 573 lemmas which arecollected from 277 synsets and we can not ob-tain 2 sense association scores for oot in this lexi-con since each lemma is associated with only onesense with a value of 1.
The LSA baseline yieldsa very low performance in the best measure due toits tendency to derive positive values for all sen-sorial associations of a given lemma-POS tuple.Another observed shortcoming of LSA is its fail-ure to correlate the names of the colors with sightwhile this association is explicit for the annotators.On the other hand, LSA baseline significantly im-proves the MFS baseline with a p-value of 0.0009in oot measures.
This result points out that eventhough LSA provides very similar positive asso-ciation values for almost all the sensory modali-ties for a given item, the first two sensorial asso-ciations with the highest values yield a better per-formance on guessing the sensorial characteristicsof a lemma-POS.
Nevertheless, Sensicon signifi-cantly outperforms the LSA baseline in both bestand oot measures with the p-values of 0.0009 and0.0189 respectively.
The statistical significancetests are conducted using one-sided bootstrap re-sampling (Efron and Tibshirani, 1994).Concerning the sentence classification experi-ment, the classifier using Sensicon yields the high-est performance in both measures.
The very highF1 value obtained with the oot scoring indicatesthat the right answer for a sentence is includedin the first two decisions in many cases.
Sensi-con significantly outperforms the LSA baseline onthe best measure (p-value = 0.0069).
On the otherhand, when systems are allowed to provide two an-swers (oot), the performance of LSA comes closeto Sensicon in terms of F1 measure.After the manual analysis of Sensicon and goldstandard data, we observe that the sensorial clas-sification task could be nontrivial.
For instance, astory sentence ?He went to sleep again and snoreduntil the windows shook.?
has been most fre-quently annotated as hearing.
While the sensorial-lexicon classifier associates this sentence withtouch as the best answer, it can provide the cor-rect association hearing as the second best answer.To find out the best sensorial association for a sen-tence, a classification method which exploits var-ious aspects of sensorial elements in a sentence,such as the number of sensorial words or their de-pendencies, could be a better approach than usingonly the average sensorial values.Lemma SentenceModel best oot best ootMost-Frequent-Sense 33.33 33.33 38.90 38.90LSA 18.80 70.38 53.44 76.51Lexicon-BB 45.22 45.22 49.60 51.12Lexicon-AB 55.85 55.85 59.89 63.21Sensicon 55.86 80.13 69.76 80.73Table 4: Evaluation results.Based on our observations of the error cases,we believe that synaesthesia, which is one of themost common metaphoric transfers in language(Williams, 1976), should be further explored forsense classification.
As an example observation,the advertisement slogan ?100% pure squeezedsunshine?
is associated with touch as the best an-swer by Sensicon and taste by LSA baseline whileit is most frequently annotated as sight in thegold standard.
This slogan is an example usageof synaesthesia and metaphors in advertising lan-guage.
To clarify, a product from the category ofbeverages, which might be assumed to have a tasteassociation, is described by a metaphorical substi-tution of a taste-related noun, most probably thename of a fruit, with a sight-related noun; sun-shine.
This metaphorical substitution, then usedas the object of a touch-related verb, to squeeze,produces a synaesthetic expression with touch andsight.5 ConclusionIn this paper we have presented the constructionof Sensicon, a sensorial lexicon, which associateswords with sensory modalities.
This novel aspectof word semantics is captured by employing a two-step strategy.
First, we collected seed words byusing a bootstrapping approach based on a set ofWordNet relations.
Then, we performed a cor-pus based statistical analysis to produce the finallexicon.
Sensicon consists of 22,684 lemma-POSpairs and their association degrees with five sen-sory modalities.
To the best of our knowledge,this is the first systematic attempt to build a sen-sorial lexicon and we believe that our contributionconstitutes a valid starting point for the commu-nity to consider sensorial information conveyed bytext as a feature for various tasks and applications.The results that we obtain by comparing our lexi-con against the gold standard and two baselines arepromising even though not conclusive.
The resultsconfirm the soundness of the proposed approachfor the construction of the lexicon and the useful-1519ness of the resource for text classification and pos-sibly other computational applications.Sensicon is publicly available upon request tothe authors so that the community can benefit fromit for relevant tasks.
From a resource point ofview, we would like to explore the effect of us-ing different kinds of WordNet relations duringthe bootstrapping phase.
It would also be interest-ing to experiment with relations provided by otherresources such as ConceptNet (Liu and Singh,2004), which is a semantic network containingcommon sense, cultural and scientific knowledge.We would also like to use the sensorial lexicon forvarious applicative scenarios such as slanting ex-isting text towards a specific sense with text modi-fication.
We believe that our resource could be ex-tremely useful for automatic content personaliza-tion according to user profiles.
As an example, onecan imagine a system that automatically replaceshearing based expressions with sight based ones inpieces of texts for a hearing-impaired person.
Au-tomating the task of building sensorial associationscould also be beneficial for various tasks that needlinguistic creativity.
For instance, copywriters cantake advantage of a system detecting the sensorialload of a piece of text to generate more appropri-ate advertisement slogans for specific product cat-egories.
Finally, we plan to investigate the impactof using sensory information for metaphor detec-tion and interpretation based on our observationsduring the evaluation.
For instance, the synaes-thetic metaphor bittersweet symphony could be de-tected by determining the sensorial characteriza-tions of its components.AcknowledgementsWe would like to thank Daniele Pighin for his in-sightful comments and valuable suggestions.This work was partially supported by the PerTeproject (Trento RISE).ReferencesEbba Cecilia Ovesdotter Alm.
2008.
Affect in Textand Speech.
Ph.D. thesis, University of Illinois atUrbana-Champaign.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
pages 86?90.Association for Computational Linguistics.Carmen Banea, Rada Mihalcea, and Janyce Wiebe.2008.
A bootstrapping method for building subjec-tivity lexicons for languages with scarce resources.In LREC.Douglas A. Bernstein.
2010.
Essentials of Psychol-ogy.
PSY 113 General Psychology Series.
CengageLearning.Bernhard E. Boser, Isabelle Guyon, and Vladimir Vap-nik.
1992.
A Training Algorithm for Optimal Mar-gin Classifiers.
In Proceedings of the 5th AnnualWorkshop on Computational learning theory.Gerlof Bouma.
2009.
Normalized (pointwise) mutualinformation in collocation extraction.
Proceedingsof GSCL, pages 31?40.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Comput.
Linguist., 16(1):22?29, March.Max Coltheart.
1981.
The mrc psycholinguisticdatabase.
The Quarterly Journal of ExperimentalPsychology, 33(4):497?505.Ivan E. de Araujo, Edmund T. Rolls, Maria In?s Ve-lazco, Christian Margot, and Isabelle Cayeux.
2005.Cognitive modulation of olfactory processing.
Neu-ron, 46(4):671?679.Ga?l Harry Dias, Mohammed Hasanuzzaman,St?phane Ferrari, and Yann Mathet.
2014.Tempowordnet for sentence time tagging.
InProceedings of the Companion Publication of the23rd International Conference on World Wide WebCompanion, WWW Companion ?14, pages 833?838, Republic and Canton of Geneva, Switzerland.InternationalWorldWideWeb Conferences SteeringCommittee.Bradley Efron and Robert J. Tibshirani.
1994.
An in-troduction to the bootstrap, volume 57.
CRC press.Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-wordnet: A publicly available lexical resource foropinion mining.
In Proceedings of LREC, volume 6,pages 417?422.Christiane Fellbaum, editor.
1998.
WordNet An Elec-tronic Lexical Database.
The MIT Press, Cam-bridge, MA ; London.Lushan Han, Tim Finin, Paul McNamee, AnupamJoshi, and Yelena Yesha.
2013.
Improving wordsimilarity by augmenting pmi with estimates of wordpolysemy.
Knowledge and Data Engineering, IEEETransactions on, 25(6):1307?1322.Thomas Kjeller Johansen.
1997.
Aristotle on theSense-organs.
Cambridge University Press.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.1520Thomas K. Landauer and Susan T. Dumais.
1997.A solution to plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological review,104(2):211.Hugo Liu and Push Singh.
2004.
Conceptnet - a practi-cal commonsense reasoning tool-kit.
BT TechnologyJournal, 22(4):211?226, October.Asifa Majid and Stephen C. Levinson.
2011.
Thesenses in language and culture.
The Senses and So-ciety, 6(1):5?18.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
InProceedings of the 4th International Workshop onSemantic Evaluations, pages 48?53.
Association forComputational Linguistics.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunker.
1993.
A semantic concordance.
InProceedings of the workshop on Human LanguageTechnology, HLT ?93, pages 303?308, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Saif M. Mohammad and Peter D. Turney.
2010.
Emo-tions evoked by common words and phrases: Usingmechanical turk to create an emotion lexicon.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Gener-ation of Emotion in Text, pages 26?34.
Associationfor Computational Linguistics.Saif M. Mohammad.
2011.
Colourful language: Mea-suring word-colour associations.
In Proceedings ofthe 2nd Workshop on Cognitive Modeling and Com-putational Linguistics, pages 97?106.
Associationfor Computational Linguistics.G?zde ?zbal, Carlo Strapparava, Rada Mihalcea, andDaniele Pighin.
2011.
A comparison of unsuper-vised methods to associate colors with words.
In Af-fective Computing and Intelligent Interaction, pages42?51.
Springer.Jessica Perrie, Aminul Islam, Evangelos Milios, andVlado Keselj.
2013.
Using google n-grams to ex-pand word-emotion association lexicon.
In Compu-tational Linguistics and Intelligent Text Processing,pages 137?148.
Springer.Mario Pricken.
2008.
Creative Advertising Ideasand Techniques from the World?s Best Campaigns.Thames & Hudson, 2nd edition.Raul Rodriguez-Esteban and Andrey Rzhetsky.
2008.Six senses in the literature.
The bleak sensory land-scape of biomedical texts.
EMBO reports, 9(3):212?215, March.Carlo Strapparava and Alessandro Valitutti.
2004.WordNet-Affect: an affective extension ofWordNet.In Proceedings of LREC, volume 4, pages 1083?1086.Sara Tonelli and Daniele Pighin.
2009.
New featuresfor framenet - wordnet mapping.
In Proceedings ofthe Thirteenth Conference on Computational Natu-ral Language Learning (CoNLL?09), Boulder, CO,USA.Peter D. Turney, Yair Neuman, Dan Assaf, and YohaiCohen.
2011.
Literal and metaphorical sense iden-tification through concrete and abstract context.
InProceedings of the 2011 Conference on the Empiri-cal Methods in Natural Language Processing, pages680?690.Peter D. Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classi-fication of reviews.
In Proceedings of the 40th an-nual meeting on association for computational lin-guistics, pages 417?424.
Association for Computa-tional Linguistics.Alessandro Valitutti, Carlo Strapparava, and OlivieroStock.
2004.
Developing affective lexical resources.PsychNology Journal, 2(1):61?83.Vladimir N. Vapnik.
1998.
Statistical Learning The-ory.
Wiley-Interscience.Joseph M. Williams.
1976.
Synaesthetic adjectives: Apossible law of semantic change.
Language, pages461?478.Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-HsiChen.
2007.
Building emotion lexicon from weblogcorpora.
In Proceedings of the 45th Annual Meetingof the ACL on Interactive Poster and DemonstrationSessions, pages 133?136.
Association for Computa-tional Linguistics.1521
