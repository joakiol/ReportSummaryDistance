Proceedings of NAACL-HLT 2013, pages 221?229,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsUsing Out-of-Domain Data for Lexical Addressee Detection inHuman-Human-Computer DialogHeeyoung Lee1?
Andreas Stolcke2 Elizabeth Shriberg21Dept.
of Electrical Engineering, Stanford University, Stanford, California, USA2Microsoft Research, Mountain View, California, USAheeyoung@stanford.edu, {anstolck,elshribe}@microsoft.comAbstractAddressee detection (AD) is an importantproblem for dialog systems in human-human-computer scenarios (contexts involving mul-tiple people and a system) because system-directed speech must be distinguished fromhuman-directed speech.
Recent work on AD(Shriberg et al 2012) showed good resultsusing prosodic and lexical features trained onin-domain data.
In-domain data, however, isexpensive to collect for each new domain.
Inthis study we focus on lexical models and in-vestigate how well out-of-domain data (eitheroutside the domain, or from single-user sce-narios) can fill in for matched in-domain data.We find that human-addressed speech can bemodeled using out-of-domain conversationalspeech transcripts, and that human-computerutterances can be modeled using single-userdata: the resulting AD system outperformsa system trained only on matched in-domaindata.
Further gains (up to a 4% reduction inequal error rate) are obtained when in-domainand out-of-domain models are interpolated.Finally, we examine which parts of an utter-ance are most useful.
We find that the first1.5 seconds of an utterance contain most ofthe lexical information for AD, and analyzewhich lexical items convey this.
Overall, weconclude that the H-H-C scenario can be ap-proximated by combining data from H-C andH-H scenarios only.
?Work done while first author was an intern with Microsoft.1 IntroductionBefore a spoken dialog system can recognize and in-terpret a user?s speech, it should ideally determineif speech was even meant to be interpreted by thesystem.
We refer to this task as addressee detec-tion (AD).
AD is often overlooked, especially in tra-ditional single-user scenarios, because with the ex-ception of self-talk, side-talk or background speech,the majority of speech is usually system-directed.As dialog systems expand to more natural contextsand multiperson environments, however, AD can be-come a crucial part of the system?s operational re-quirements.
This is particularly true for systems inwhich explicit system addressing (e.g., push-to-talkor required keyword addressing) is undesirable.Past research on addressee detection has focusedon human-human (H-H) settings, such as meetings,sometimes with multimodal cues (op den Akker andTraum, 2009).
Early systems relied primarily on re-jection of H-H utterances either because they couldnot be interpreted (Paek et al 2000), or because theyyielded low speech recognition confidence (Dowd-ing et al 2006).
Some systems combine gazewith lexical and syntactic cues to detect H-H speech(Katzenmaier et al 2004).
Others use relativelysimple prosodic features based on pitch and energyin addition to those derived from automatic speechrecognition (ASR) (Reich et al 2011).With some exceptions (Bohus and Horvitz, 2011;Shriberg et al 2012), relatively little work haslooked at the human-human-computer (H-H-C) sce-nario, i.e.
at contexts involving two or more peoplewho interact both with a system and with each other.221Shriberg et al(2012) found that novel prosodicfeatures were more accurate than lexical or seman-tic features based on speech recognition for the ad-dressee task.
The corpus, also used herein, is com-prised of H-H-C dialog in which roughly half of thecomputer-addressed speech consisted of a small setof fixed commands.
While the word-based featuresmap directly to the commands, they had troubledistinguishing all other (noncommand) computer-directed speech from human-directed speech.
Thisis because addressee detection in the H-H-C sce-nario becomes even more challenging when the sys-tem is designed for natural speech, i.e., utterancesthat are conversational in form and not limited tocommand phrases with restricted syntax.
Further-more, H-H utterances can be about the domain ofthe system (e.g., discussing the dialog task), mak-ing AD based on language content more difficult.The prosodic features were good at both types ofdistinctions?even improving performance signifi-cantly when combined with true-word (cheating)lexical features that have 100% accuracy on thecommands.
Nevertheless, the prior work showedthat lexical n-grams are useful for addressee detec-tion in the H-H-C scenario.A problem with lexical features is that they arehighly task- and domain-dependent.
As with otherlanguage modeling tasks, one usually has to collectmatched training data in significant quantities.
Datacollection is made more cumbersome and expensiveby the multi-user aspect of the scenario.
Thus, forpractical reasons alone, it would be much better ifthe language models for AD could be trained onout-of-domain data, and if whatever in-domain datais needed could be limited to single-user interac-tion.
We show in this paper that precisely this train-ing scenario is feasible and achieves results that arecomparable or better than using completely matchedH-H-C training data.In addition to studying the role of out-of-domaindata for lexical AD models, we also examine whichwords are useful, and how soon in elapsed time theyare available.
Whereas most prior work in AD haslooked at processing of entire utterances, we con-sider an online processing version where AD deci-sions are to be made as soon as possible after anutterance was initiated.
We find that most of theaddressee-relevant lexical information can be foundFigure 1: Conversational Browser dialog system en-vironment with multi-human scenarioin the first 1.5 seconds, and analyze which wordsconvey this information.2 DataWe use in-domain and out-of-domain data from var-ious sources.
The corpora used in this work differ insize, domain, and scenario.2.1 In-domain dataIn-domain data is collected from interactions be-tween two users and a ?Conversational Browser?
(CB) spoken dialog system.
We used the samemethodology as Shriberg et al(2012), but using ad-ditional data.
As depicted in Figure 1, the systemshows a browser on a large TV screen and usersare asked to use natural language for a variety ofinformation-seeking tasks.
For more details aboutthe dialog system and language understanding ap-proach, see Hakkani-Tu?r et al(2011a; 2011b).We split the in-domain data into training, devel-opment, and test sets, preserving sessions.
Each ses-sion is about 5 to 40 minutes long.
Even thoughthe whole conversation is recorded, only the seg-ments captured by the speech recognition systemare used in our experiments.
Each utterance seg-ment belongs to one of four types: computer-command (C-command), comprising navigationalcommands to the system, computer-noncommand(C-noncommand), which are computer-directed ut-terances other than commands, human-directed (H),and mixed (M) utterances, which contain a combina-222Table 1: In-domain corpus(a) Sizes, distribution, and ASR word error rates of in-domain utterance typesData set Train Dev Test WERTranscribed words 6,490 11,298 9,486ASR words 4,649 6,360 5,514 59.3%H (%) 19.1 48.6 37.0 87.6%C-noncomm.
(%) 38.3 27.8 32.2 32.6%C-command (%) 39.9 18.7 27.2 19.7%M (%) 2.7 4.9 3.6 69.6%(b) Example utterances by typeType ExampleH Do you want to watch amovie?C-noncommand How is the weather today?C-command Scroll down, Go back.M Show me sandwich shops.Oh, are you vegetarian?tion of human- and computer-directed speech.
Thesizes and distribution of all utterance types, as wellas sample utterances are shown in Table 1.The ASR system used in the system was based onoff-the-shelf acoustic models and had only the lan-guage model adapted to the domain, using very lim-ited data.
Consequently, as shown in the right-mostcolumn of Table 1(a), the word error rates (WERs)are quite high, especially for human-directed utter-ances.
While these could be improved with tar-geted effort, we consider this a realistic applicationscenario, where in-domain training data is typicallyscarce, at least early in the development process.Therefore, any lexically based AD methods need tobe robust to poor ASR accuracy.2.2 Out-of-domain dataTo replace the hard-to-obtain in-domain H-H-C datafor training, we use the four out-of-domain corpora(two H-C and two H-H) shown in Table 2.Single-user CB data comes from the same Con-versational Browser system as the in-domain data,but with only one user present.
This data can there-fore be used for modeling H-C speech.
Bing anchortext (Huang et al 2010) is a large n-gram corpus ofanchor text associated with links on web pages en-Table 2: Out-of-domain corpora.
?Single-user CB?is a corpus collected in same environment as the H-H-C in-domain data, except that only a single userwas present.Corpus Addressee SizeSingle-user CB H-C 21.9k wordsBing anchor text H-C 1.3B bigramsFisher H-H 21M wordsICSI meetings H-H 0.7M wordsSingle user CB, Bing ?
out-of-domainin-domain  (HC)in-domain (HH)Fisher, ICSI meeting ?
out-of-domainLanguage model for human directed utterances (H)Language model for computer directed utterances (C)?(?|?)1?
???
?(?|?)?(?|?)?(?|?
)Figure 2: Language model-based score computationfor addressee detectioncountered by the Bing search engine.
When userswant to follow a link displayed on screen, they usu-ally speak a variant of the anchor text for the link.We hypothesized that this corpus might aid the mod-eling of computer-noncommand type utterances inwhich such ?verbal clicks?
are frequent.
Fisher tele-phone conversations and ICSI meetings are both cor-pora of human-directed speech.
The Fisher corpus(Cieri et al 2004) comprises two-person telephoneconversations between strangers on prescribed top-ics.
The ICSI meeting corpus (Janin et al 2003)contains multiparty face-to-face technical discus-sions among colleagues.3 Method3.1 Language modeling for addressee detectionWe use a lexical AD system that is based on mod-eling word n-grams in the two addressee-based ut-terance classes, H (for H-H) and C (for H-C utter-ances).
This approach is similar to language model-based approaches to speaker and language recogni-tion, and was shown to be quite effective for thistask (Shriberg et al 2012).
Instead of makinghard decisions, the system outputs a score that is223the length-normalized likelihood ratio of the twoclasses:1|w|logP (w|C)P (w|H), (1)where |w| is the number of words in the recognitionoutput w for an utterance.
P (w|C) and P (w|H) areobtained from class-specific language models.
Fig-ure 2 gives a flow-chart of the score computation.Class likelihoods are obtained from standard tri-gram backoff language models, using Witten-Belldiscounting for smoothing (Witten and Bell, 1991).For combining various training data sources, we uselanguage model adaptation by interpolation (Bel-legarda, 2004).
First, a separate model is trainedfrom each source.
The probability estimates fromin-domain and out-of-domain models are then aver-aged in a weighted fashion:P (wk|hk) = ?Pin(wk|hk) + (1?
?
)Pout(wk|hk)(2)where wk is the k-th word, hk is the (n ?
1)-gramhistory for the wordwk.
?
is the interpolation weightand is obtained by tuning a task-related metric on thedevelopment set.
We investigated optimizing ?
foreither model perplexity or classification accuracy, asdiscussed below.3.2 Part-of-speech-based modelingSo far we have only been modeling the lexical formsof words in utterances.
If we encounter a word neverbefore seen, it would appear as an out-of-vocabularyitem in all class-specific language models, and notcontribute much to the decision.
More generally, ifa word is rare, its n-gram statistics will be unreliableand poorly modeled by the system.
(The sparsenessissue is exacerbated by small amounts of trainingdata as in our scenario.
)One common approach to deal with data sparse-ness in language modeling is to model n-grams overword classes rather than raw words (Brown et al1992).
For example, if we have an utterance Howis the weather in Paris?, the addressee probabilitiesare likely to be similar had we seen London insteadof Paris.
Therefore, replacing words with properlychosen word class labels can give better generaliza-tion from the observed training data.
Among themany methods proposed to class words for languagemodeling purposes we chose part-of-speech (POS)tagging over other, purely data-derived classing al-gorithms (Brown et al 1992), for two reasons.
First,our goal here is not to minimize the perplexity of thedata, but to enhance discrimination among utteranceclasses.
Second, a data-driven class inference algo-rithm would suffer from the same sparseness issueswhen it comes to unseen and rare words (as no ro-bust statistics are available to infer an unseen word?sbest class in the class induction step).
A POS tag-ger, on the other hand, can do quite well on unseenwords, using context and morphological cues.A hidden Markov model tagger using POS-trigram statistics and context-independent classmembership probabilities was used for tagging allLM training data.
The tagger itself had beentrained on the Switchboard (conversational tele-phone speech) transcripts of the Penn Treebank-3 corpus (Marcus et al 1999), and used the 39Treebank POS labels.
To strike a compromise be-tween generalization and discriminative power inthe language model, we retained the topN most fre-quent word types from the in-domain training dataas distinct tokens, and varied N as a metaparam-eter.
Barzilay and Lee (2003) used a similar ideato generalize patterns by substituting words withslots.
This strategy will tend to preserve words thatare either generally frequent function and domain-independent words, capturing stylistic and syntac-tic patterns, or which are frequent domain-specificwords, and can thus help characterize computer-directed utterances.Here is a sample sentence and its transformed ver-sion:Original: Let?s find an Italian restaurantaround this area.POS-tagged: Let?s find an JJ NN around thisarea.The words except Italian and restaurant are un-changed because they are in the list of N most fre-quent words.
We transformed all training and testdata in this fashion and then modeled n-gram statis-tics as before.
The one exception was the Binganchor-text data, which was only available in theform of word n-grams (the sentence context requiredfor accurate POS tagging was missing).224Table 3: Addressee detection performance (EER) with different training setsASR TranscriptBaseline (in-domain only) 31.1 17.3Fisher+ICSI, Single-user CB+Bing (out-of-domain only) 27.8 14.2Baseline + Fisher+ICSI, Single CB + Bing (both-all) 26.9 14.0Baseline + ICSI, Single-user CB (both-small) 26.6 13.03.3 Evaluation metricsTypically, an application-dependent threshold wouldbe applied to the decision score to convert it into abinary decision.
The optimal threshold is a func-tion of prior class probabilities and error costs.
Asin Shriberg et al(2012), we used equal error rate(EER) to compare systems, since we are interestedin the discriminative power of the decision score in-dependent of priors and costs.
EER is the probabilityof false detections and misses at the operating pointat which the two types of errors are equally proba-ble.
A prior-free metric such as EER is more mean-ingful than classification accuracy because the utter-ance type distribution is heavily skewed (Table 1),and because the rate of human- versus computer-directed speech can vary widely depending on theparticular people, domain, and context.
We also useclassification accuracy (based on data priors) in oneanalysis below, because EERs are not comparablefor different test data subdivisions.3.4 Online modelThe actual dialog system used in this work pro-cesses utterances after receiving an entire segmentof speech from the recognition subsystem.
How-ever, we envision that a future version of the sys-tem would perform addressee detection in an onlinemanner, making a decision as soon as enough evi-dence is gathered.
This raises the question how soonthe addressee can be detected once the user startsspeaking.
We simulate this processing mode using awindowed AD model.As shown in Figure 3, we define windows start-ing at the beginning of the utterance and investigatehow AD performance changes as a function of win-dow size.
We use only the words and n-grams fallingcompletely within a given window.
For example, theword find would be excluded from Window 1 in Fig->???
?         find       an     Italian    restaurant   around  this         areaWindow 1Window 2  ?Figure 3: The window modelure 3.The benefit of early detection in this case is thatonce speech is classified as human-directed, it doesnot need to be sent to the speech recognizer and sub-sequent semantic processing.
This saves processingtime, especially if processing happens on a server.Based on the window model performance, we canassess the feasibility of an online AD model, whichcan be approached by shifting the detection windowthrough time and finding addressee changes.4 Results and DiscussionTable 3 compares the performance of our system us-ing various training data sources.
For diagnostic pur-poses we also compare performance based on recog-nized words (the realistic scenario) to that based onhuman transcripts (idealized, best-case word recog-nition).Somewhat surprisingly, the system trained on out-of-domain data alone performs better by 3.3 EERpoints on ASR output and 3.1 points on transcriptscompared to the in-domain baseline.
Combiningin-domain and out-of-domain data (both-all, both-small) gives about 1 point additional EER gain.
Notethat training on in-domain data plus the smaller-sizeout-of-domain corpora (both-small) is better thanusing all available data (both-all).Figure 4 shows the detection error trade-off(DET) between false alarm and miss errors for the2258  7  6543  2  1Figure 4: Detection error trade-off (DET) curves forthe systems in Table 3.
Thin lines at the top rightcorner use ASR output (1-4); thick lines at the bot-tom left corner use reference transcripts (5-8).
Eachline number represents one of the systems in Table 3:1,5 = in-domain only, 2,6 = out-of-domain only, 4,7= both-all, 3,8 = both-small.systems in Table 3.
The DET plot depicts perfor-mance not only at the EER operating point (whichlies on the diagonal), but over the range of possibletrade-offs between false alarm and miss error rates.As can be seen, replacing or combining in-domaindata with out-of-domain data gives clear perfor-mance gains, regardless of operating point (scorethreshold), and for both reference and recognizedwords.Figure 5 shows H-H vs. H-C classification accu-racies on each of the four utterance subtypes listedin Table 1.
It is clear that computer-command ut-terances are the easiest to classify; the accuracy ismore than 90% using transcripts, and more than 85%using ASR output.
This is not surprising, sincecommands are from a fixed small set of phrases.The biggest gain from use of out-of-domain datais found for computer-directed noncommand utter-ances.
This is helpful, since in general it is thenoncommand computer-directed utterances (ratherthan the commands) that are highly confusable withhuman-directed utterances: both use unconstrainednatural language.
We note that H-H utterance arevery poorly recognized in the ASR condition whenonly out-of-domain data is used.
This may be be-20 3040 5060 7080 90100baseline(in-domain only)out-of-domainonlyboth-allboth-smallASR  REFFigure 5: AD accuracies by utterance typeTable 4: Perplexities (computed on dev set ASRwords) by utterance type, for different training cor-pora.
Interpolation refers to the combination of thethree models listed in each case.Test classTraining set H-C H-HIn-domain H-C (ASR) 257 1856Single-user CB 104 1237Bing anchor text 356 789Interpolation 58 370In-domain H-H (ASR) 887 1483Fisher 995 795ICSI meeting 2007 1583Interpolation 355 442cause the human-human corpora used in trainingconsist of transcripts, whereas the ASR output forhuman-directed utterances is very errorful, creatinga severe train-test mismatch.As for the optimization of the mixing weight ?,we found that minimizing perplexity on the devel-opment set of each class is effective.
This is astandard optimization approach for interpolated lan-guage models, and can be carried out efficiently us-ing an expectation maximization algorithm.
We alsotried search-based optimization using the classifica-tion metric (EER) as the criterion.
While this ap-proach could theoretically give better results (sinceperplexity is not a discriminative criterion) we foundno significant improvement in our experiments.226Table 4 shows the perplexities by class of lan-guage models trained on different corpora.
We cantake these as an indication of training/test mismatch(lower perplexity indicating better match).
We alsofind substantial perplexity reductions from interpo-lating models.
In order to make perplexities compa-rable, we trained all models using the union of thevocabularies from the different sources.In spite of perplexity being a good way to opti-mize the weighting of sources, it is not clear that itis a good criterion for selecting data sources.
Forexample, we see that the Fisher model has a muchlower perplexity on H-H utterances than the ICSImeeting model.
However, as reflected in Table 3,the H language model that leaves out the Fisher dataactually performed better.
The most likely expla-nation is that the Fisher corpus is an order of mag-nitude larger than the ICSI corpus, and that sheerdata size, not stylistic similarity, may account for thelower perplexity of the Fisher model.
Further inves-tigation is needed regarding good criteria for corpusselection for classification tasks such as AD.Table 5 shows the EER performance of the POS-based model, for various sizes N of the most-frequent word list.
We observe that the partial re-placement of words with POS tags indeed improvesover the baseline model performance, by 1.5 pointson ASR output and by 1.1 points on transcripts.We also see that the gain over the correspondingword-only model is largest for the in-domain base-line model, and less or non-existent for the out-of-domain model.
This is consistent with the notionthat the in-domain model suffers the most from datasparseness, and therefore has the most to gain frombetter generalization.Interpolating with out-of-domain data still helpshere.
The optimal N differs for ASR output versustranscripts.
The POS-based model with N = 300improves the EER by 0.5 points on ASR output,and N = 1000 improves the EER by 0.8 points ontranscripts.
Here we use relatively large amounts oftraining data, thus the performance gain is smaller,though still meaningful.Figure 6 shows the performance of the systemusing time windows anchored at the beginnings ofutterances.
We incrementally increase the windowwidth from 0.5 seconds to 3 seconds and compareresults to using full utterances.
The leveling off of0.100.150.200.250.300.350.400.5 1 1.5 2 3 fullEqual error rateWindow width (seconds)ASR baselineASR outsideASR both-allASR both-smallREF baselineREF outsideREF both-allREF both-smallFigure 6: Simulated online performance on incre-mental windowsTable 6: The top 15 first words in utterancesASR H-C Transcript H-C ASR H-H Transcript H-Hgo go play Iscroll scroll go ohhstart start is soshow stop it yeahstop show what it?sbing find this yousearch Bing show uhfind search how okayplay pause bing whatpause play select itlook look okay andwhat uh does that?sselect what start ishow how so nothe ohh I wethe error plots indicates that most addressee infor-mation is contained in the first 1 to 1.5 seconds,although some additional information is found inthe later part of utterances (the plots never level offcompletely).
This pattern holds for both in-domainand out-of-domain training, as well as for combinedmodels.To give an intuitive understanding of where thisearly addressee-relevant information comes from,we tabulated the top 15 word unigrams in each ut-terance class, are shown in Table 6.
Note thatthe substantial differences between the third andfourth columns in the table reflect the high ASRerror rate for human-directed utterances, whereas227Table 5: Performance of POS-based model with various top-N word lists (EER)Training data top100 top200 top300 top400 top500 top1000 top2000 OriginalASR baseline 31.6 31.0 29.6 30.1 30.2 31.4 31.5 31.1out-of-domain only 36.5 37.0 37.2 36.9 36.8 36.6 37.3 27.8both-all 28.2 26.6 26.1 26.7 27.4 26.9 27.6 26.9both-small 28.0 26.5 26.2 26.6 26.4 26.3 26.5 26.6REF baseline 17.1 16.2 16.6 17.1 16.7 17.0 17.2 17.3out-of-domain only 17.6 17.6 17.5 17.2 17.1 17.2 18.1 14.2both-all 12.5 12.5 12.5 12.7 12.8 13.2 13.5 14.0both-small 13.0 13.2 12.8 13.2 12.8 12.2 12.7 13.0for computer-directed utterances, the frequent firstwords are mostly recognized correctly.In computer-directed utterances we see mostlycommand verbs, which, due to the imperative syn-tax of these commands occur in utterance-initial po-sition.
Human-directed utterances are characterizedby subject pronouns such as I and it, or answer parti-cles such as yeah and okay, which likewise occur ininitial position.
Based on word frequency and syn-tax alone it is thus clear why the beginnings of utter-ances contain strong lexical cues.5 ConclusionWe explored the use of outside data for traininglexical addressee detection systems for the human-human-computer scenario.
Advantages include sav-ing the time and expense of an in-domain data col-lection, as well as performance gains even whensome in-domain data is available.
We show that H-C training data can be obtained from a single-userH-C collection, and that H-H speech can be mod-eled using general conversational speech.
Using theoutside training data, we obtain results that are evenbetter than results using matched (but smaller) H-H-C training data.
Results can be improved consid-erably by adapting H-C and H-H language modelswith small amounts of matched H-H-C data, via in-terpolation.
The main reason for the improvement isbetter detection of computer-directed noncommandutterances, which tend to be confusable with human-directed utterances.
Another effective way to over-come scarce training data is to replace the less fre-quent words with part-of-speech labels.
In bothbaseline and interpolated model, we found that POS-based models that keep an appropriate number of thetopN most frequent word types can further improvethe system?s performance.In a second study we found that the most salientphrases for lexical addressee detection occur withinthe first 1 to 1.5 seconds of speech in each utter-ance.
It reflects a syntactic tendency of class-specificwords to occur utterance-initially, which shows thefeasibility of the online AD system.AcknowledgmentsWe thank our Microsoft colleagues MadhuChinthakunta, Dilek Hakkani-Tu?r, Larry Heck,Lisa Stiefelman, and Gokhan Tu?r for developingthe dialog system used in this work, as well as formany valuable discussions.
Ashley Fidler was incharge of much of the data collection and annotationrequired for this study.
We also thank Dan Jurafskyfor useful feedback.228ReferencesRegina Barzilay and Lillian Lee.
2003.
Learning toparaphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings HLT-NAACL2003, pages 16?23, Edmonton, Canada.Jerome R. Bellegarda.
2004.
Statistical language modeladaptation: review and perspectives.
Speech Commu-nication, 42:93?108.Dan Bohus and Eric Horvitz.
2011.
Multiparty turn tak-ing in situated dialog: Study, lessons, and directions.In Proceedings ACL SIGDIAL, pages 98?109, Port-land, OR.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,Jenifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479.Christopher Cieri, David Miller, and Kevin Walker.2004.
The Fisher corpus: a resource for the next gen-erations of speech-to-text.
In Proceedings 4th Interna-tional Conference on Language Resources and Evalu-ation, pages 69?71, Lisbon.John Dowding, Richard Alena, William J. Clancey,Maarten Sierhuis, and Jeffrey Graham.
2006.
Areyou talking to me?
dialogue systems supporting mixedteams of humans and robots.
In Proccedings AAAIFall Symposium: Aurally Informed Performance: Inte-grating Machine Listening and Auditory Presentationin Robotic Systems, Washington, DC.Dilek Hakkani-Tu?r, Gokhan Tur, and Larry Heck.
2011a.Research challenges and opportunities in mobile appli-cations [dsp education].
IEEE Signal Processing Mag-azine, 28(4):108 ?110.Dilek Z. Hakkani-Tu?r, Go?khan Tu?r, Larry P. Heck, andElizabeth Shriberg.
2011b.
Bootstrapping domain de-tection using query click logs for new domains.
InProceedings Interspeech, pages 709?712.Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li,Kuansang Wang, and Fritz Behr.
2010.
Exploring webscale language models for search query processing.
InProceedings 19th International Conference on WorldWide Web, pages 451?460, Raleigh, NC.Adam Janin, Don Baron, Jane Edwards, Dan Ellis,David Gelbart, Nelson Morgan, Barbara Peskin, ThiloPfau, Elizabeth Shriberg, Andreas Stolcke, and ChuckWooters.
2003.
The ICSI meeting corpus.
In Pro-ceedings IEEE ICASSP, volume 1, pages 364?367,Hong Kong.Michael Katzenmaier, Rainer Stiefelhagen, and TanjaSchultz.
2004.
Identifying the addressee in human-human-robot interactions based on head pose andspeech.
In Proceedings 6th International Conferenceon Multimodal Interfaces, ICMI, pages 144?151, NewYork, NY, USA.
ACM.Mitchell P. Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank-3.Linguistic Data Consortium, catalog item LDC99T42.Rieks op den Akker and David Traum.
2009.
A com-parison of addressee detection methods for multipartyconversations.
In Proceedings of Diaholmia, pages99?106.Tim Paek, Eric Horvitz, and Eric Ringger.
2000.
Con-tinuous listening for unconstrained spoken dialog.
InProceedings ICSLP, volume 1, pages 138?141, Bei-jing.Daniel Reich, Felix Putze, Dominic Heger, Joris Ijssel-muiden, Rainer Stiefelhagen, and Tanja Schultz.
2011.A real-time speech command detector for a smart con-trol room.
In Proceedings Interspeech, pages 2641?2644, Florence.Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tu?r,and Larry Heck.
2012.
Learning when to listen:Detecting system-addressed speech in human-human-computer dialog.
In Proceedings Interspeech, Port-land, OR.Ian H. Witten and Timothy C. Bell.
1991.
The zero-frequency problem: Estimating the probabilities ofnovel events in adaptive text compression.
IEEETransactions on Information Theory, 37(4):1085?1094.229
