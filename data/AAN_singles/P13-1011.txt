Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 104?113,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsRecognizing Rare Social Phenomena in Conversation:Empowerment Detection in Support Group ChatroomsElijah Mayfield, David Adamson, and Carolyn Penstein Rose?Language Technologies InstituteCarnegie Mellon University5000 Forbes Ave, Pittsburgh, PA 15213{emayfiel, dadamson, cprose}@cs.cmu.eduAbstractAutomated annotation of social behaviorin conversation is necessary for large-scaleanalysis of real-world conversational data.Important behavioral categories, though,are often sparse and often appear onlyin specific subsections of a conversation.This makes supervised machine learningdifficult, through a combination of noisyfeatures and unbalanced class distribu-tions.
We propose within-instance con-tent selection, using cue features to selec-tively suppress sections of text and bias-ing the remaining representation towardsminority classes.
We show the effective-ness of this technique in automated anno-tation of empowerment language in onlinesupport group chatrooms.
Our techniqueis significantly more accurate than multi-ple baselines, especially when prioritizinghigh precision.1 IntroductionQuantitative social science research has experi-enced a recent expansion, out of controlled set-tings and into natural environments.
With thisinflux of interest comes new methodology, andthe inevitable question arises of how to movetowards testable hypotheses, using these uncon-trolled sources of data as scientific lenses into thereal world.The study of conversational transcripts is a keydomain in this new frontier.
There are certainsocial and behavioral phenomena in conversationthat cannot be easily identified through question-naire data, self-reported surveys, or easily ex-tracted user metadata.
Examples of these socialphenomena in conversation include overt displaysof power (Prabhakaran et al, 2012) or indicatorsof rapport and relationship building (Wang et al,2012).
Manually annotating these social phenom-ena cannot scale to large data, so researchers turnto automated annotation of transcripts (Rose?
et al,2008).
While machine learning is highly effec-tive for annotation tasks with relatively balancedlabels, such as sentiment analysis (Pang and Lee,2004), more complex social functions are oftenrarer.
This leads to unbalanced class label distri-butions and a much more difficult machine learn-ing task.
Moreover, features indicative of rare so-cial annotations tend to be drowned out in favor offeatures biased towards the majority class.
The neteffect is that classification algorithms tend to biastowards the majority class, giving low accuracy forrare class detection.Automated annotation of social phenomena alsobrings opportunities for real-world applications.For example, real-time annotation of conversationcan power adaptive intervention in collaborativelearning settings (Rummel et al, 2008; Adamsonand Rose?, 2012).
However, with the considerablepower of automation comes great responsibility.
Itis critical to avoid intervening in the case of er-roneous annotations, as providing unnecessary orinappropriate support in such a setting has beenshown to be harmful to group performance and so-cial cohesion (Dillenbourg, 2002; Stahl, 2012).We propose adaptations to existing machinelearning algorithms which improve recognition ofrare annotations in conversational text data.
Ourprimary contribution comes in the form of within-instance content selection.
We develop a novel al-gorithm based on textual cues, suppressing infor-mation which is likely to be irrelevant to an in-stance?s class label.
This allows features whichpredict minority classes to gain prominence, help-ing to sidestep the frequency of common featurespointing to a majority class label.Additionally, we propose modifications to ex-isting algorithms.
First, we identify a new appli-cation of logistic model trees to text data.
Next,104we define a modification of confidence-based en-semble voting which encourages minority class la-beling.
Using these techniques, we demonstrate asignificant improvement in classifier performancewhen recognizing the language of empowermentin support group chatrooms, a critical applicationarea for researchers studying conversational inter-actions in healthcare (Uden-Kraan et al, 2009).The remainder of this paper is structured as fol-lows.
We introduce the domain of empowermentin support contexts, along with previous studies onthe challenges that these annotations (and similarothers) bring to machine learning.
We introduceour new technique for improving the ability to au-tomate this annotation, along with other optimiza-tions to the machine learning workflow which aretailored to this skewed class balance.
We presentexperimental results showing that our method iseffective, and provide a detailed analysis of the be-havior of our model and the features it uses most.We conclude with a discussion of particularly use-ful applications of this work.2 BackgroundWe ground this paper?s discussion of machinelearning with a real problem, turning to the an-notation of empowerment language in chat1.
Theconcept of empowerment, while a prolific areaof research, lacks a broad definition across pro-fessionals, but broadly relates to ?the power toact efficaciously to bring about desired results?
(Boehm and Staples, 2002) and ?experiencing per-sonal growth as a result of developing skills andabilities along with a more positive self-definition?
(Staples, 1990).
Participants in online supportgroups feel increased empowerment (Uden-Kraanet al, 2009; Barak et al, 2008).
Quantita-tive studies have shown the effect of empower-ment through statistical methods such as structuralequation modeling (Vauth et al, 2007), as havequalitative methods such as deductive transcriptanalysis (Owen et al, 2008) and interview studies(Wahlin et al, 2006).The transition between these styles of researchhas been gradual.
Pioneering work has demon-strated the ability to distinguish empowerment lan-guage in written texts, including prompted writ-ing samples (Pennebaker and Seagal, 1999), nar-1Definitions of empowerment are closely related to thenotion of self-efficacy (Bandura, 1997).
For simplicity, weuse the former term exclusively in this paper.Table 1: Empowerment label distribution in ourcorpus.Annotation Label # %Self-Empowerment NA 1522 79.3POS 202 10.5NEG 196 10.2Other-Empowerment NA 1560 81.3POS 217 11.3NEG 143 7.4ratives in online forums (Hoybye et al, 2005), andsome preliminary analysis of synchronous discus-sion (Ogura et al, 2008; Mayfield et al, 2012b).These transitional works have used limited analy-sis methodology; in the absence of sophisticatednatural language processing, their conclusions of-ten rely on coarse measures, such as word countsand proportions of annotations in a text.Users, of course, do not express empowermentin every thread in which they participate, whichleads to a challenge for machine learning.
Threadsoften focus on a single user?s experiences, inwhich most participants in a chat are merely com-mentators, if they participate at all, matching pre-vious research on shifts in speaker salience overtime (Hassan et al, 2008).
This leads to manyuser threads which are annotated as not applicable(N/A).
We move to our proposed approach withthese skewed distributions in mind.3 DataOur data consists of a set of chatroom conversa-tion transcripts from the Cancer Support Commu-nity2.
Each 90-minute conversation took place inthe context of a weekly meeting in a real-time chat,with up to 6 participants in addition to a profes-sional therapist facilitating the discussion.
In to-tal, 2,206 conversations were collected from 2007-2011.
This data offers potentially rich insight intocoping and social support; however, annotatingsuch a dataset by hand would be prohibitively ex-pensive, even when it is already transcribed.Twenty-one of these conversations have beenannotated, as originally described and analyzedin (Mayfield et al, 2012b)3.
This data was dis-entangled into threads based on common themesor topics, as in prior work (Elsner and Charniak,2www.cancersupportcommunity.org3All annotations were found to be adequately reliable be-tween humans, with thread disentanglement f = 0.75 andempowerment annotation ?
> 0.7.105Figure 1: An example mapping from a single thread?s chat lines (left) to the per-user, per-thread instancesused for classification in this paper (right), with example annotations for self-empowerment indicated.2010; Adams and Martel, 2010).
A novel per-user, per-thread annotation was then employedfor empowerment annotation, following a codingmanual based on definitions like those in Section2.
Each user was assigned a label of positiveor negative empowerment if they exhibited suchemotions, or was left blank if they did not do sowithin the context of that thread.
This annotationwas performed both for their self-empowermentas well as their attitude towards others?
situations(other-empowerment).
An example of this annota-tion for self-empowerment is presented in Figure1 and the distribution of labels is given in Table 1.Most previous annotation tasks attempt to an-notate on a per-utterance basis, such as dialogueact tagging (Popescu-Belis, 2008), or on arbitraryspans of text, such as in the MPQA subjectivitycorpus (Wiebe et al, 2005).
However, for our task,a per-user, per-thread annotation is more appropri-ate, because empowerment is often indicated bestthrough narrative (Hoybye et al, 2005).
Humanannotators are instructed to take this context intoaccount when annotating (Mayfield et al, 2012b).It would therefore be nonsensical to annotate indi-vidual lines as ?embodying?
empowerment.
Simi-lar arguments have been made for sentiment, espe-cially as the field moves towards aspect-orientedsentiment (Breck et al, 2007).
Assigning labelsbased on thread boundaries allows for context tobe meaningfully taken into account, without cross-ing topic boundaries.However, this granularity comes with a price:the distribution of class values in these instancesis highly skewed.
In our data, the vast majority ofusers?
threads are marked as not applicable to em-powerment.
Perhaps more inconveniently, whiletaking context into account is important for reli-able annotation, it leads to extraneous informationin many cases.
Many threads can have multiplelines of contributions that are topically related toan expression of empowerment (and thus belongin the same thread), but which do not indicate anyempowerment themselves.
This exacerbates thelikelihood of instances being classified as N/A.We choose to take advantage of these attributesof threads.
We know from research in discourseanalysis that many sections of conversations areformulaic and rote, like introductions and greet-ings (Schegloff, 1968).
We additionally know thatpolarity often shifts in dialogue through the useof discourse connectives such as conjunctions andtransitional phrases.
These issues have been ad-dressed in work in the language technologies com-munity, most notably through the Penn DiscourseTreebank (Prasad et al, 2008); however, their ap-plications to noisier synchronous conversation hasbeenrare in computational linguistics.With these linguistic insights in mind, we ex-amine how we can make best use of them formachine learning performance.
While techniquesfor predicting rare events (Weiss and Hirsh, 1998)and compensating for class imbalance (Frank and106Bouckaert, 2006), these approaches generally fo-cus on statistical properties of large class sets with-out taking the nature of their datasets into account.In the next section, we propose a new algorithmwhich takes advantage specifically of the linguis-tic phenomena in the conversation-based data thatwe study for empowerment detection.
As such,our algorithm is highly suited to this data and task,with the necessary tradeoff in uncertain generalityto new domains with unrelated data.4 Cue Discovery for Content SelectionOur algorithm performs content selection bylearning a set of cue features.
Each of these fea-tures indicates some linguistic function within thediscourse which should downplay the importanceof features either before or after that discoursemarker.
Our algorithm allows us to evaluate theimpact of rules against a baseline, and to itera-tively judge each rule atop the changes made byprevious rules.This algorithm fits into existing language tech-nologies research which has attempted to partitiondocuments into sections which are more or lessrelevant for classification.
Many researchers haveattempted to make use of cue phrases (Hirschbergand Litman, 1993), especially for segmentationboth in prose (Hearst, 1997) and conversation(Galley et al, 2003).
The approach of content se-lection, meanwhile, has been explored for senti-ment analysis (Pang and Lee, 2004), where indi-vidual sentences may be less subjective and there-fore less relevant to the sentiment classificationtask.
It is also similar conceptually to contentselection algorithms that have been used for textsummarization (Teufel and Moens, 2002) and textgeneration (Sauper and Barzilay, 2009), both ofwhich rely on finding highly-relevant passageswithin source texts.Our work is distinct from these approaches.While we have coarse-grained annotations of em-powerment, there is no direct annotation of whatmakes a good cue for content selection.
Withour cues, we hope to take advantage of shallowdiscourse structure in conversation, such as con-trastive markers, making use of implicit structurein the conversational domain.4.1 NotationBefore describing extensions to the baseline lo-gistic regression model, we define notation.
Ourdata is arranged hierarchically.
We assume thatwe have a collection of d training documents Tr ={D1 .
.
.
Dd}, each of which contains many train-ing instances (in our task, an instance consists ofall lines of chat from one user in one thread).
Ourtotal set of n instances I thus consists of instances{I1, I2, .
.
.
In}.
Each document contains lines ofchat L and each instance Ii is comprised of somesubset of those lines, Li ?
L.Our feature space X = {x1, x2, .
.
.
xm} con-sists of m unigram features representing the ob-served vocabulary used in our corpus.
Each in-stance is associated with a feature vector x?
con-taining values for each x ?
X, and each featurex that is present in the i-th instance maintains a?memory?
of the lines in which it appeared in thatinstance, Lix, where Lix ?
Li.
Our potential out-put labels consist of Y = {NA,NEG,POS},though this generalizes to any nominal classifica-tion task.
Each instance I is associated with ex-actly one y ?
Y for self-empowerment and onefor other-empowerment; these two labels do notinteract and our tasks are treated as independentin this paper4.
We define classifiers as functionsf(x??
y ?
Y); in practice, we use logistic regres-sion via LibLINEAR (Fan et al, 2008).We define a content selection rule as a pairingr = ?c, t?
between a cue feature c ?
X and a se-lection function t ?
T .
We created a list of possi-ble selection functions, given a cue c, maximizingfor generality while being expressive.
These areillustrated in Figure 2 and described below:?
Ignore Local Future (A): Ignore all featuresfrom the two lines after each occurrence of c.?
Ignore All Future (B): Ignore all featuresoccurring after the first occurrence of c.?
Ignore Local History (C): Ignore all featuresin the two lines preceding each occurrence ofc.?
Ignore All History (D): Ignore all featuresoccurring only before the last occurrence ofc.We define an ensemble member E = ?R, fR?
-the ordered list of learned content selection rulesR = [r1, r2, .
.
. ]
and a classifier fC trained on in-stances transformed by those rules.
Our final out-4Future work may examine the interaction of jointly an-notating multiple sparse social phenomena.107Figure 2: Effects of content selection rules, basedon a cue feature (ovals) observed at lines m and n.put of a trained model is a set of ensemble mem-bers {E1, .
.
.
, Ek}.4.2 AlgorithmOur ensemble learning follows the paradigmof cross-validated committees (Parmanto et al,1996), where k ensemble members are trained bysubdividing our training data into k subfolds.
Foreach ensemble classifier, cue rulesR are generatedon k ?
1 subfolds (Trk) and evaluated on the re-maining subfold (Tek).
In practice, with 21 train-ing documents, 7-fold cross-validation, and k = 3ensemble members, each generation set consistsof 12 documents?
instances, while each evaluationset contains instances from 6 documents.Our full algorithm is presented in Algorithm1, and is broken into component parts for clar-ity.
Algorithm 2 begins by measuring the base-line classifier?s ability to recognize minority-classlabels.
After training on Trk, we measure theaverage probability assigned to the correct labelof instances in Tek, but only for instances whosecorrect labels are minority classes (remember, be-cause both Trk and Tek are drawn from the over-all Tr, we have access to true class labels).
Wechoose this subset of only minority instances, aswe are not interested in optimizing to the majorityclass.We next enumerate all rules that we wish tojudge.
To keep this problem tractable, we ignorefeatures which do not occur in at least 5% of train-ing instances.
For the remaining features, we cre-ate a candidate rule for each possible pairing offeatures and selection functions.
For each of thesecandidates, we test its utility by selecting contentas if it were an actual rule, then building a newclassifier (trained on the generation set) using in-stances that have been altered in that way.
In theevaluation set, we measure the difference in prob-ability of minority class labels being assigned cor-rectly between the baseline and this altered space.This measure of an individual rule?s impact is de-scribed in Algorithm 3.Once we have evaluated every possible ruleonce, we select the top-ranked rule and ap-ply it to the feature set.
We then iterativelyprogress through our now-ranked list of candi-dates, each time treating the newly filtered datasetas our new baseline.
We search only top can-didates for efficiency, following the fixed-widthsearch methodology for feature selection in veryhigh-dimensionality feature spaces (Gu?tlein et al,2009).
Each ensemble classifier is finally retrainedon all training data, after applying the correspond-ing content selection rules to that data.5 PredictionOur prediction algorithm begins with a stan-dard implementation of cross-validated commit-tees (Parmanto et al, 1996), whose results areaggregated with a confidence voting method in-tended to favor rare labels (Erp et al, 2002).Cross-validated committees are an ensemble tech-nique used to subsample training data to producemultiple hypotheses for classification.
Each clas-sifier produced by our cue-based transformationis trained on a subset of our training data.
Eachmakes predictions on all test set instances, pro-ducing a distribution of confidence across possi-ble labels.
These values serve as inputs to a votingmethod to produce a final label for each instance.Compared to other ensemble methods, cross-validated committees as described above are agood fit for our task, because of its unique unit ofanalysis.
As thread-level analysis is the set of in-dividual participants?
turns in a conversation, werisk overfitting if we sample from the same con-versations for the training and testing sets.
In con-trast to standard bagging, hard sampling bound-aries never train and test on instances drawn fromthe same conversation.To aggregate the votes from members of this en-semble into a final prediction, we employ a varianton Selfridge?s Pandemonium (Selfridge, 1958).If a minority label is selected as the highest-confidence value in any classifier in our ensem-ble, it is selected.
The majority label, by contrast,is only selected if it is the most likely predictionby all classifiers in our ensemble.
Thus consen-sus is required to elect the majority class, and thestrongest minority candidate is elected otherwise.108In : generation set Trk, evaluation set TekOut: ensemble committee {E1 .
.
.
Ek}for i = 1 to k doRfinal ?
[ ];Xfreq ?
{x ?
X | freq(x) ?
Trk >5%};R?
Xfreq ?
T ;R?
?
R;repeatPbase ?
EvaluateClassifier(Trk,Tek);EvaluateRules(Pbase,Trk,Tek, R?
);Trk,Tek ?
ApplyRule(R?[0]);R?
R?R?[0];??
score(R?
[0]);Rfinal ?
Rfinal +R?[0];R?
?
R[0 .
.
.
50];until ?
< threshold;Trfinal ?
Trk ?
Tek;foreach r ?
Rfinal doTrfinal ?
ApplyRule(Trfinal, r);endTrain f(x??
y) on Trfinal;endAlgorithm 1: LearnSelectionCues()This approach is designed to bias the predictionof our machine learning algorithms in favor of mi-nority classes in a coherent manner.
If there is aplausible model that has been trained which rec-ognizes the possibility of a rare label, it is used;the prediction only reverts to the majority classwhen no plausible minority label could be chosen.As validation of this technique, we compare our?minority pandemonium?
approach against bothtypical pandemonium and standard sum-rule con-fidence voting (Erp et al, 2002).5.1 Logistic Model StumpsOne characteristic of highly skewed data is that,while minority labels may be expressed in a num-ber of different surface forms, there are many ob-vious cases in which they do not apply.
Thesecases can actually be harmful to classification ofborderline cases.
Features that could be given highweight in marginal cases may be undervalued in?low-hanging fruit?
easy cases.
To remove thoseobvious instances, a very simple screening heuris-tic is often enough to eliminate frequent pheno-types of instances where the rare annotation isnot present.
Prior work has sometimes screenedtraining data through obvious heuristic rules, espe-In : generation set Trk, evaluation set TekOut: minority class probability average PbaseTrain f(x??
y) on Trk;Temink ?
{Instance I ?
Tek | yI 6= ?NA?
};Pbase ?
0 ;foreach Instance I ?
Temink doPbase ?
Pbase + P (f(x?I) = yI)endPbase = Pbase/size(Temink )Algorithm 2: EvaluateClassifier()In : Trk, Tek, rules R, base probability PbaseOut: R sorted on each rule?s improvementscoreforeach Rule r ?
R doTr?k,Te?k ?
ApplyRule(Trk,Tek, r);Palter ?
EvaluateClassifier(Tr?k,Te?k);score(r)?
Palter ?
Pbase;endSort R on score(r) from high to low;Algorithm 3: EvaluateRules()cially in speech recognition; for instance, trainingspeech recognition for words followed by a pauseseparately from words followed by another word(Franco et al, 2010), or training separate modelsbased on gender (Jiang et al, 1999).We achieve this instance screening by learn-ing logistic model tree stumps (Landwehr et al,2005), which allow us to quickly partition data ifthere is a particularly easy heuristic that can belearned to eliminate a large number of majority-class labels.
One challenge of this approach isour underlying unigram feature space - tree-basedalgorithms are generally poor classifiers for thehigh-dimensionality, low-information features in alexical feature space (Han et al, 2001).
To com-pensate, we employ a smaller, denser set of binaryfeatures for tree stump screening: instance lengththresholds and LIWC category membership.First, we define a set of features that split basedon the number of lines an instance contains, from1 to 10 (only a tiny fraction of instances are morethan 10 lines long).
For example, a feature split-ting on instances with lines ?
2 would be truefor one- and two-line instances, and false for allothers.
Second, we define a feature for each cate-gory in the Linguistic Inquiry and Word Count dic-tionary (Tausczik and Pennebaker, 2010) - thesebroad classes of words allow for more balanced109Figure 3: Precision/recall curves for algorithms.After 50% recall all models converge and there areno significant differences in performance.splits than would unigrams alone.
Each category?sfeature is true if any word in that category wasused at least once in that instance.We exhaustively sweep this feature space, andreport the most successful stump rules for each an-notation task.
In our other experiments, we reportresults with and without the best rule for this pre-processing step; we also measure its impact alone.6 Experimental ResultsAll experiments were performed using LightSIDE(Mayfield and Rose?, 2013).
We use a binary uni-gram feature space, and we perform 7-fold cross-validation.
Instances from the same chat transcriptnever occur in both train and testing folds.
Fur-thermore, we assume that threads have been dis-entangled already, and our experiments use goldstandard thread structure.
While this is not a triv-ial assumption, prior work has shown thread dis-entanglement to be manageable (Mayfield et al,2012a); we consider it an acceptable simplify-ing assumption for our experiments.
We compareour methods against baselines including a majoritybaseline, a baseline logistic regression classifierwith L2 regularized features, and two common en-semble methods, AdaBoost (Freund and Schapire,1996) and bagging (Breiman, 1996) with logisticregression base classifiers5.Table 2 presents the best-performing resultfrom each classification method.
For self-empowerment recognition, all methods that weintroduce are significant improvements in ?, the5These methods usually use weak, unstable base classi-fiers; however, in our experiments, those performed poorly.Table 2: Performance for baselines, common en-semble algorithms, and proposed methods.
Statis-tically significant improvements over baseline aremarked (p < .01, ?
; p < .05, *; p < 0.1, +).Self OtherMethod % ?
% ?Majority 79.3 .000 81.3 .000LR Baseline 81.0 .367 81.0 .270LR + Boosting 78.1 .325 78.5 .275LR + Bagging 81.2 .352 81.9 .265LR + Committee 81.0 .367 81.0 .270Learned Stumps 81.8* .385?
81.7 .293+Content Selection 80.9 .389?
80.7 .282Stumps+Selection 81.3 .406?
79.4 .254Table 3: Performance of content-selectionwrapped learners, for minority voting and twobaseline voting methods.Self OtherMethod % ?
% ?Pandemonium 80.3 .283 81.4 .239Averaged 80.6 .304 81.6 .251Minority Voting 80.9?
.389?
80.7 .282measurement of agreement over chance, comparedto all baselines.
While accuracy remains stable,this is due to predictions shifting away from themajority class and towards minority classes.
Ourcombined model using both logistic model treestumps and content selection is significantly betterthan either alone (p < .01).
To compare the mi-nority pandemonium voting method against base-lines of simple pandemonium and summed confi-dence voting, Table 3 presents the results of con-tent selection wrappers with each voting method.Minority voting is more effective compared tostandard confidence voting, improving ?
whilemodestly reducing accuracy; this is typical of ashift towards minority class predictions.7 DiscussionThese results show promise for our techniques,which are able to distinguish features of rare la-bels, previously awash in a sea of irrelevance.
Fig-ure 3 shows the impact of our rules as we tuneto different levels of recall, with a large boost inprecision when recall is not important; our modelconverges with the baseline for high-recall, low-precision tuning.
This suggests that our method isparticularly suitable for tasks where confident la-110Table 4: Cue rules commonly selected by the algo-rithm.
Average improvement over the LR baselineis also shown.Self-EmpowermentCue Transformation ?%and,but Ignore Local Future +5.0have Ignore All History +4.3!
Ignore All History +4.2me,my Ignore All History +3.4Other-EmpowermentCue Transformation ?%and,but Ignore Local Future +5.5you Ignore Local History +5.2?s Ignore Local History +4.1that Ignore Local History +3.9beling of a few instances is more important thanlabeling as many instances as possible.
This iscommon when tasks have a high cost or carry highrisk (for instance, providing real-time conversa-tional supports with an agent, where inappropriateintervention could be disruptive).
Other low-recallapplications include exploration large corpora forexemplar instances, where the most confident pre-dictions for a given label should be presented firstfor analyst use.
In the rest of this section, weexamine notable within-instance and per-instancerules selected by our methods.
These rules aresummarized in Tables 4 and 5.For both self- and other-empowerment, we findpronoun rules that match the task (first-person andsecond-person pronouns for self-Empowermentand other-Empowerment respectively).
In bothtasks, we find cue rules that suppress the contextpreceding personal pronouns.
These, as well asthe possessive suffix ?s, echo the per-instance ef-fect of the Self and You splits, anticipating thatwhat follows such a personal reference is likely tobear an evaluation of empowerment.
Exclamationmarks may indicate strong emotion - we find manyinstances where what precedes a line with an ex-clamation is more objective, and what follows in-cludes an assessment.
Conjunctions but and andare selected as cue rules suppressing the two linesthat follow the occurrence - suggesting, as sus-pected, that connective discourse markers play arole in indicating empowerment (Fraser, 1999).The best-performing stump splits for the Self-Empowerment annotation are Line Length ?
1and the LIWC word-categories Article, Swear, andTable 5: Best decision rules for logistic modelstumps.
Significant improvement (p < 0.05) in-dicated with *.Self-EmpowermentSplit Rule ?
??
% ?%Split ?
1 * 0.385 +.018 81.8 +0.8LIWC-Article 0.379 +.012 81.6 +0.6LIWC-Swear * 0.376 +.009 81.4 +0.4LIWC-Self * 0.376 +.009 81.5 +0.5Other-EmpowermentSplit Rule ?
??
% ?%LIWC-You 0.293 +.023 81.7 +0.7LIWC-Eating * 0.283 +.013 81.6 +0.6LIWC-Negate * 0.282 +.012 82.3 +1.3LIWC-Present 0.281 +.011 81.6 +0.6Self.
The split on line length corresponds to theobservation that longer instances provide greateropportunity for personal narrative self-assessmentto occur (95% of single-line instances are labeledNA).
The Article category may serve as a proxy forcontent length - article-less instances in our corpusinclude one-line social greetings and exchangesof contact information.
Swear words may be acue for awareness of self-empowerment - a recentstudy of women coping with illness reported thatswearing in the presence of others, but not alone,was related to potentially harmful outcomes (Rob-bins et al, 2011).
Among other- oriented splitrules, Eating stands out as non-obvious, althoughmedical literature has suggested a link betweendietary behavior and empowerment attitudes in astudy of women with cancer (Pinto et al, 2002).8 ConclusionWe have demonstrated an algorithm for improv-ing automated classification accuracy on highlyskewed tasks for conversational data.
This algo-rithm, particularly its focus on content selection, isrooted in the structural format of our data, whichcan generalize to many tasks involving conversa-tional data.
Our experiments show that this modelsignificantly improves machine learning perfor-mance.
Our algorithm is taking advantage ofstructural facets of discourse markers, lending ba-sic sociolinguistic validity to its behavior.
Thoughwe have treated each of these rarely-occurring la-bels as independent thus far, in practice we knowthat this is not the case.
Joint prediction of labelsthrough structured modeling is an obvious next111step for improving classification accuracy.This is an important step towards large-scaleanalysis of the impact of support groups on pa-tients and caregivers.
Our method can be used toconfidently highlight occurrences of rare labels inlarge data sets.
This has real-world implicationsfor professional intervention in social conversa-tional domains, especially in scenarios where suchan intervention is likely to be associated with ahigh cost or high risk.
With the construction ofmore accurate classifiers, we open the possibilityof automating annotation on large conversationaldatasets, enabling new directions for researcherswith domain expertise.AcknowledgmentsThe research reported here was supported by Na-tional Science Foundation grant IIS-0968485.ReferencesPaige Adams and Craig Martel.
2010.
Conversationalthread extraction and topic detection in text-basedchat.
In Semantic Computing.David Adamson and Carolyn Penstein Rose?.
2012.Coordinating multi-dimensional support in collabo-rative conversational agents.
In Proceedings of In-telligent Tutoring Systems.Albert Bandura.
1997.
Self-Efficacy: The Exercise ofControl.Azy Barak, Meyran Boniel-Nissim, and John Suler.2008.
Fostering empowerment in online supportgroups.
Computers in Human Behavior.A Boehm and L H Staples.
2002.
The functions of thesocial worker in empowering: The voices of con-sumers and professionals.
Social Work.Eric Breck, Yejin Choi, and Claire Cardie.
2007.
Iden-tifying expressions of opinion in context.
In Pro-ceedings of IJCAI.Leo Breiman.
1996.
Bagging predictors.
MachineLearning.Pierre Dillenbourg.
2002.
Over-scripting cscl: Therisks of blending collaborative learning with instruc-tional design.
Three worlds of CSCL.
Can we sup-port CSCL?Micha Elsner and Eugene Charniak.
2010.
Disentan-gling chat.
Computational Linguistics.Merijn Van Erp, Louis Vuurpijl, and LambertSchomaker.
2002.
An overview and comparison ofvoting methods for pattern recognition.
In Frontiersin Handwriting Recognition.
IEEE.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.Horacio Franco, Harry Bratt, Romain Rossier,Venkata Rao Gadde, Elizabeth Shriberg, VictorAbrash, and Kristin Precoda.
2010.
Eduspeak: Aspeech recognition and pronunciation scoring toolkitfor computer-aided language learning applications.Language Testing.Eibe Frank and Remco R Bouckaert.
2006.
Naivebayes for text classification with unbalanced classes.Knowledge Discovery in Databases.Bruce Fraser.
1999.
What are discourse markers?Journal of pragmatics, 31(7):931?952.Yoav Freund and Robert E Schapire.
1996.
Experi-ments with a new boosting algorithm.
In Proceed-ings of ICML.Michel Galley, Kathleen McKeown, Eric Fosler-Lussier, and Hongyan Jing.
2003.
Discourse seg-mentation of multi-party conversation.
In Proceed-ings of ACL.Martin Gu?tlein, Eibe Frank, Mark Hall, and AndreasKarwath.
2009.
Large-scale attribute selection us-ing wrappers.
In Proceedings of IEEE CIDM.Eui-Hong Han, George Karypis, and Vipin Kumar.2001.
Text categorization using weight adjustedk-nearest neighbor classification.
Lecture Notes inComputer Science: Advances in Knowledge Discov-ery and Data Mining.Ahmed Hassan, Anthony Fader, Michael H Crespin,Kevin M Quinn, Burt L Monroe, Michael Colaresi,and Dragomir R Radev.
2008.
Tracking the dy-namic evolution of participant salience in a discus-sion.
In Proceedings of Coling.Marti A Hearst.
1997.
Texttiling: Segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics.Julia Hirschberg and Diane Litman.
1993.
Empiricalstudies on the disambiguation of cue phrases.
Com-putational Linguistics.Mette Terp Hoybye, Christoffer Johansen, and TineTjornhoj-Thomsen.
2005.
Online interaction ef-fects of storytelling in an internet breast cancer sup-port group.
Psycho-oncology.Hui Jiang, Keikichi Hirose, and Qiang Huo.
1999.
Ro-bust speech recognition based on a bayesian predic-tion approach.
In IEEE Transactions on Speech andAudio Processing.Niels Landwehr, Mark Hall, and Eibe Frank.
2005.Logistic model trees.
Machine Learning.Elijah Mayfield and Carolyn Penstein Rose?.
2013.Lightside: Open source machine learning for text.In Handbook of Automated Essay Evaluation: Cur-rent Applications and New Directions.112Elijah Mayfield, David Adamson, and Carolyn Pen-stein Rose?.
2012a.
Hierarchical conversation struc-ture prediction in multi-party chat.
In Proceedingsof SIGDIAL Meeting on Discourse and Dialogue.Elijah Mayfield, Miaomiao Wen, Mitch Golant, andCarolyn Penstein Rose?.
2012b.
Discovering habitsof effective online support group chatrooms.
InACM Conference on Supporting Group Work.Kanayo Ogura, Takashi Kusumi, and Asako Miura.2008.
Analysis of community development usingchat logs: A virtual support group of cancer patients.In Proceedings of the IEEE Symposium on UniversalCommunication.Jason E. Owen, Erin O?Carroll Bantum, and MitchGolant.
2008.
Benefits and challenges experiencedby professional facilitators of online support groupsfor cancer survivors.
In Psycho-Oncology.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe Association for Computational Linguistics.Bambang Parmanto, Paul Munro, and Howard RDoyle.
1996.
Improving committee diagnosis withresampling techniques.
In Proceedings of NIPS.James W Pennebaker and J D Seagal.
1999.
Forminga story: The health benefits of narrative.
Journal ofClinical Psychology.Bernardine M Pinto, Nancy C Maruyama, Matthew MClark, Dean G Cruess, Elyse Park, and MaryRoberts.
2002.
Motivation to modify lifestyle riskbehaviors in women treated for breast cancer.
InMayo Clinic Proceedings.Andrei Popescu-Belis.
2008.
Dimensionality of di-alogue act tagsets: An empirical analysis of largecorpora.
In Language Resources and Evaluation.Vinodkumar Prabhakaran, Owen Rambow, and MonaDiab.
2012.
Predicting overt display of power inwritten dialogs.
In Proceedings of NAACL.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The penn discourse treebank 2.0.
InProceedings of LREC.Megan L Robbins, Elizabeth S Focella, Shelley Kasle,Ana Mar?
?a Lo?pez, Karen L Weihs, and Matthias RMehl.
2011.
Naturalistically observed swear-ing, emotional support, and depressive symptomsin women coping with illness.
Health Psychology,30:789.Carolyn Penstein Rose?, Yi-Chia Wang, Yue Cui, JaimeArguello, Karsten Stegmann, Armin Weinberger,and Frank Fischer.
2008.
Analyzing collabo-rative learning processes automatically: Exploit-ing the advances of computational linguistics incomputer-supported collaborative learning.
In Inter-national Journal of Computer Supported Collabora-tive Learning.Nikol Rummel, Armin Weinberger, Christof Wecker,Frank Fischer, Anne Meier, Eleni Voyiatzaki,George Kahrimanis, Hans Spada, Nikolaos Avouris,and Erin Walker.
2008.
New challenges in cscl:Towards adaptive script support.
In Proceedings ofICLS.Christina Sauper and Regina Barzilay.
2009.
Auto-matically generating wikipedia articles: A structure-aware approach.
In Proceedings of ACL.Emanuel A Schegloff.
1968.
Sequencing in conversa-tional openings.
American Anthropologist.Oliver G Selfridge.
1958.
Pandemonium: aparadigm for learning.
In Proceedings of Sympo-sium on Mechanisation of Thought Processes, Na-tional Physical Laboratory.Gerry Stahl.
2012.
Interaction analysis of a biologychat.
Productive multivocality.Lee H Staples.
1990.
Powerful ideas about empower-ment.
Administration in Social Work.Yla R Tausczik and James W Pennebaker.
2010.
Thepsychological meaning of words: Liwc and comput-erized text analysis methods.
Journal of Languageand Social Psychology.Simone Teufel and Marc Moens.
2002.
Summariz-ing scientic articles: Experiments with relevance andrhetorical status.
Computational Linguistics.C F Van Uden-Kraan, C H C Drossaert, E Taal, E RSeydel, and M A F J Van de Laar.
2009.
Partici-pation in online patient support groups endorses pa-tients empowerment.
Patient Education and Coun-seling.R Vauth, B Kleim, M Wirtz, and P W Corrigan.
2007.Self-efficacy and empowerment as outcomes of self-stigmatizing and coping in schizophrenia.
Psychia-try Research.Ingrid Wahlin, Anna-Christina Ek, and Ewa Idvali.2006.
Patient empowerment in intensive carean in-terview study.
Intensive and Critical Care Nursing.William Yang Wang, Samantha Finkelstein, AmyOgan, Alan Black, and Justine Cassell.
2012.
?loveya, jerkface:?
using sparse log-linear models to buildpositive (and impolite) relationships with teens.
InProceedings of SIGDIAL.Gary M Weiss and Haym Hirsh.
1998.
Learning topredict rare events in event sequences.
In Proceed-ings of KDD.Janyce Wiebe, Theresa Wilson, and Claire Cardie.2005.
Annotating expressions of opinions and emo-tions in language.
Language Resources and Evalu-ation.113
