Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, pages 9?10,Baltimore, Maryland, USA, 22 June 2014.c?2014 Association for Computational LinguisticsStructured Belief Propagation for NLPMatthew R. Gormley Jason EisnerDepartment of Computer ScienceJohns Hopkins University, Baltimore, MD{mrg,jason}@cs.jhu.edu1 Tutorial OverviewStatistical natural language processing relies onprobabilistic models of linguistic structure.
Morecomplex models can help capture our intuitionsabout language, by adding linguistically meaning-ful interactions and latent variables.
However, in-ference and learning in the models we want oftenposes a serious computational challenge.Belief propagation (BP) and its variants pro-vide an attractive approximate solution, especiallyusing recent training methods.
These approachescan handle joint models of interacting compo-nents, are computationally efficient, and have ex-tended the state-of-the-art on a number of com-mon NLP tasks, including dependency parsing,modeling of morphological paradigms, CCG pars-ing, phrase extraction, semantic role labeling, andinformation extraction (Smith and Eisner, 2008;Dreyer and Eisner, 2009; Auli and Lopez, 2011;Burkett and Klein, 2012; Naradowsky et al., 2012;Stoyanov and Eisner, 2012).This tutorial delves into BP with an emphasis onrecent advances that enable state-of-the-art perfor-mance in a variety of tasks.
Our goal is to eluci-date how these approaches can easily be appliedto new problems.
We also cover the theory under-lying them.
Our target audience is researchers inhuman language technologies; we do not assumefamilarity with BP.In the first three sections, we discuss applica-tions of BP to NLP problems, the basics of mod-eling with factor graphs and message passing, andthe theoretical underpinnings of ?what BP is do-ing?
and how it relates to other variational infer-ence techniques.
In the second three sections, wecover key extensions to the standard BP algorithmto enable modeling of linguistic structure, efficientinference, and approximation-aware training.
Wesurvey a variety of software tools and introduce anew software framework that incorporates manyof the modern approaches covered in this tutorial.2 Outline1.
Applications [15 min., Eisner]?
Intro: Modeling with factor graphs?
Morphological paradigms?
Dependency and constituency parsing?
Alignment; Phrase extraction?
Relation extraction; Semantic role labeling?
Targeted sentiment?
Joint models for NLP2.
Belief Propagation Basics [40 min., Eisner]?
Messages and beliefs?
Sum-product, max-product, and determin-istic annealing?
Relation to forward-backward and inside-outside?
Acyclic vs. loopy graphs?
Synchronous vs. asynchronous propaga-tion3.
Theory [25 min., Gormley]?
From arc consistency to BP?
From Gibbs sampling to particle BP to BP?
Other message-passing algorithms?
Bethe free energy?
Connection to PFCGs and FSMs4.
Incorporating Structure into Factors and Vari-ables [30 min., Gormley]?
Embedding dynamic programs (e.g.inside-outside) within factors?
String-valued and tree-valued variables5.
Message approximation and scheduling [20min., Eisner]?
Pruning messages?
Variational approximations?
Residual BP and new variants6.
Approximation-aware Training [30 min., Gorm-ley]?
Empirical risk minimization under approx-imations (ERMA)?
BP as a computational expression graph?
Automatic differentiation (AD)7.
Software [10 min., Gormley]93 InstructorsMatt Gormley is a PhD student at Johns HopkinsUniversity working with Mark Dredze and JasonEisner.
His current research focuses on joint mod-eling of multiple linguistic strata in learning set-tings where supervised resources are scarce.
Hehas authored papers in a variety of areas includingtopic modeling, global optimization, semantic rolelabeling, and grammar induction.Jason Eisner is an Associate Professor in Com-puter Science and Cognitive Science at JohnsHopkins University, where he has received twoschool-wide awards for excellence in teaching.His 80+ papers have presented many models andalgorithms spanning numerous areas of NLP.
Hisgoal is to develop the probabilistic modeling, in-ference, and learning techniques needed for a uni-fied model of all kinds of linguistic structure.
Inparticular, he and his students introduced struc-tured belief propagation, which integrates classi-cal NLP models and their associated dynamic pro-gramming algorithms, as well as loss-calibratedtraining for use with belief propagation.ReferencesMichael Auli and Adam Lopez.
2011.
A compari-son of loopy belief propagation and dual decompo-sition for integrated CCG supertagging and parsing.In Proceedings of ACL.David Burkett and Dan Klein.
2012.
Fast inference inphrase extraction models with belief propagation.
InProceedings of NAACL.Markus Dreyer and Jason Eisner.
2009.
Graphicalmodels over multiple strings.
In Proceedings ofEMNLP.Jason Naradowsky, Sebastian Riedel, and David Smith.2012.
Improving NLP through marginalizationof hidden syntactic structure.
In Proceedings ofEMNLP 2012.David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In Proceedings ofEMNLP.Veselin Stoyanov and Jason Eisner.
2012.
Minimum-risk training of approximate CRF-Based NLP sys-tems.
In Proceedings of NAACL-HLT.10
