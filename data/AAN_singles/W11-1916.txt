Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 102?106,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsMention Detection: Heuristics for the OntoNotes annotationsJonathan K. Kummerfeld, Mohit Bansal, David Burkett and Dan KleinComputer Science DivisionUniversity of California at Berkeley{jkk,mbansal,dburkett,klein}@cs.berkeley.eduAbstractOur submission was a reduced version ofthe system described in Haghighi and Klein(2010), with extensions to improve mentiondetection to suit the OntoNotes annotationscheme.
Including exact matching mentiondetection in this shared task added a new andchallenging dimension to the problem, partic-ularly for our system, which previously useda very permissive detection method.
We im-proved this aspect of the system by addingfilters based on the annotation scheme forOntoNotes and analysis of system behavior onthe development set.
These changes led to im-provements in coreference F-score of 10.06,5.71, 6.78, 6.63 and 3.09 on the MUC, B3,Ceaf-e, Ceaf-m and Blanc, metrics, respec-tively, and a final task score of 47.10.1 IntroductionCoreference resolution is concerned with identifyingmentions of entities in text and determining whichmentions are referring to the same entity.
Previouslythe focus in the field has been on the latter task.Typically, mentions were considered correct if theirspan was within the true span of a gold mention, andcontained the head word.
This task (Pradhan et al,2011) has set a harder challenge by only consideringexact matches to be correct.Our system uses an unsupervised approach basedon a generative model.
Unlike previous work, wedid not use the Bllip or Wikipedia data described inHaghighi and Klein (2010).
This was necessary forthe system to be eligible for the closed task.The system detects mentions by finding the max-imal projection of every noun and pronoun.
For theOntoNotes corpus this approach posed several prob-lems.
First, the annotation scheme explicitly rejectsnoun phrases in certain constructions.
And second,it includes coreference for events as well as things.In preliminary experiments on the development set,we found that spurious mentions were our primarysource of error.
Using an oracle to exclude all spu-rious mentions at evaluation time yielded improve-ments ranging from five to thirty percent across thevarious metrics used in this task.
Thus, we decidedto focus our efforts on methods for detecting and fil-tering spurious mentions.To improve mention detection, we filtered men-tions both before and after coreference resolution.Filters prior to coreference resolution were con-structed based on the annotation scheme and partic-ular cases that should never be mentions (e.g.
singleword spans with the EX tag).
Filters after corefer-ence resolution were constructed based on analysisof common errors on the development set.These changes led to considerable improvementin mention detection precision.
The heuristics usedin post-resolution filtering had a significant negativeimpact on recall, but this cost was out-weighed bythe improvements in precision.
Overall, the use ofthese filters led to a significant improvement in F1across all the coreference resolution evaluation met-rics considered in the task.2 Core SystemWe use a generative approach that is mainly un-supervised, as described in detail in Haghighi and102Klein (2010), and briefly below.2.1 ModelThe system uses all three of the standard abstrac-tions in coreference resolution; mentions, entitiesand types.
A mention is a span in the text, the en-tity is the actual object or event the mention refersto, and each type is a group of entities.
For example,?the Mountain View based search giant?
is a men-tion that refers to the entity Google, which is of typeorganization.At each level we define a set of properties (e.g.proper-head).
For mentions, these properties arelinked directly to words from the span.
For enti-ties, each property corresponds to a list of words,instances of which are seen in specific mentions ofthat entity.
At the type level, we assign a pair ofmultinomials to each property.
The first of thesemultinomials is a distribution over words, reflectingtheir occurrence for this property for entities of thistype.
The second is a distribution over non-negativeintegers, representing the length of word lists for thisproperty in entities of this type.The only form of supervision used in the systemis at the type level.
The set of types is defined andlists of prototype words for each property of eachtype are provided.
We also include a small numberof extra types with no prototype words, for entitiesthat do not fit well in any of the specified types.These abstractions are used to form a generativemodel with three components; a semantic module, adiscourse module and a mention module.
In addi-tion to the properties and corresponding parametersdescribed above, the model is specified by a multi-nomial prior over types (?
), log-linear parametersover discourse choices (pi), and a small number ofhyperparameters (?
).Entities are generated by the semantic module bydrawing a type t according to ?, and then using thattype?s multinomials to populate word lists for eachproperty.The assignment of entities to mentions is handledby the discourse module.
Affinities between men-tions are defined by a log-linear model with param-eters pi for a range of standard features.Finally, the mention module generates the ac-tual words in the span.
Words are drawn for eachproperty from the lists for the relevant entity, witha hyper-parameter for interpolation between a uni-form distribution over the words for the entity andthe underlying distribution for the type.
This allowsthe model to capture the fact that some propertiesuse words that are very specific to the entity (e.g.proper names) while others are not at all specific(e.g.
pronouns).2.2 Learning and InferenceThe learning procedure finds parameters that arelikely under the model?s posterior distribution.
Thisis achieved with a variational approximation thatfactors over the parameters of the model.
Each setof parameters is optimized in turn, while the rest areheld fixed.
The specific update methods vary foreach set of parameters; for details see Section 4 ofHaghighi and Klein (2010).3 Mention detection extensionsThe system described in Haghighi and Klein (2010)includes every NP span as a mention.
When run onthe OntoNotes data this leads to a large number ofspurious mentions, even when ignoring singletons.One challenge when working with the OntoNotesdata is that singleton mentions are not annotated.This makes it difficult to untangle errors in coref-erence resolution and errors in mention detection.
Amention produced by the system might not be in thegold set for one of two reasons; either because it isa spurious mention, or because it is not co-referent.Without manually annotating the singletons in thedata, these two cases cannot be easily separated.3.1 Baseline mention detectionThe standard approach used in the system to detectmentions is to consider each word and its maximalprojection, accepting it only if the span is an NP orthe word is a pronoun.
This approach will intro-duce spurious mentions if the parser makes a mis-take, or if the NP is not considered a mention in theOntoNotes corpus.
In this work, we considered theprovided parses and parses produced by the Berke-ley parser (Petrov et al, 2006) trained on the pro-vided training data.
We added a set of filters basedon the annotation scheme described by Pradhan et al(2007).
Some filters are applied before coreferenceresolution and others afterward, as described below.103Data Set Filters P R FDevNone 37.59 76.93 50.50Pre 39.49 76.83 52.17Post 59.05 68.08 63.24All 58.69 67.98 63.00Test All 56.97 69.77 62.72Table 1: Mention detection performance with varioussubsets of the filters.3.2 Before Coreference ResolutionThe pre-resolution filters were based on three reli-able features of spurious mentions:?
Appositive constructions?
Attributes signaled by copular verbs?
Single word mentions with a POS tag in the set:EX, IN, WRB, WPTo detect appositive constructions we searchedfor the following pattern:NPNP , NP .
.
.And to detect attributes signaled by copular struc-tures we searched for this pattern:VPcop verb NPwhere we used the fairly conservative set of cop-ular verbs: {is, are, was, ?m}.
In bothcases, any mention whose maximal NP projectionappeared as the bold node in a subtree matching thepattern was excluded.In all three cases, errors from the parser (or POStagger) may lead to the deletion of valid mentions.However, we found the impact of this was small andwas outweighed by the number of spurious mentionsremoved.3.3 After Coreference ResolutionTo construct the post-coreference filters we analyzedsystem output on the development set, and tunedFilters MUC B3 Ceaf-e BlancNone 25.24 45.89 50.32 59.12Pre 27.06 47.71 50.15 60.17Post 42.08 62.53 43.88 66.54All 42.03 62.42 43.56 66.60Table 2: Precision for coreference resolution on the devset.Filters MUC B3 Ceaf-e BlancNone 50.54 78.54 26.17 62.77Pre 51.20 77.73 27.23 62.97Post 45.93 64.72 39.84 61.20All 46.21 64.96 39.24 61.28Table 3: Recall for coreference resolution on the dev set.based on MUC and B3 performance.
The final setof filters used were:?
Filter if the head word is in a gazetteer, whichwe constructed based on behavior on the devel-opment set (head words found using the Collins(1999) rules)?
Filter if the POS tag is one of WDT, NNS, RB,JJ, ADJP?
Filter if the mention is a specific case of youor it that is more often generic (you know,you can, it is)?
Filter if the mention is any cardinal other thana yearA few other more specific filters were also in-cluded (e.g.
?s when tagged as PRP) and one typeof exception (if all words are capitalized, the men-tion is kept).4 Other modificationsThe parses in the OntoNotes data include the addi-tion of structure within noun phrases.
Our systemwas not designed to handle the NML tag, so weremoved such nodes, reverting to the standard flat-tened NP structures found in the Penn Treebank.We also trained the Berkeley parser on the pro-vided training data, and used it to label the develop-ment and test sets.1 We found that performance was1In a small number of cases, the Berkeley parser failed, andwe used the provided parse tree instead.104Filters MUC B3 Ceaf-e Ceaf-m BlancNone 33.67 57.93 34.43 42.72 60.60Pre 35.40 59.13 35.29 43.72 61.38Post 43.92 63.61 41.76 49.74 63.26All 44.02 63.66 41.29 49.46 63.34Table 4: F1 scores for coreference resolution on the devset.slightly improved by the use of these parses insteadof the provided parses.5 ResultsSince our focus when extending our system for thistask was on mention detection, we present resultswith variations in the sets of mention filters used.
Inparticular, we have included results for our baselinesystem (None), when only the filters before coref-erence resolution are used (Pre), when only the fil-ters after coreference resolution are used (Post), andwhen all filters are used (All).The main approach behind the pre-coreference fil-ters was to consider the parse to catch cases that arealmost never mentions.
In particular, these filterstarget cases that are explicitly excluded by the an-notation scheme.
As Table 1 shows, this led to a1.90% increase in mention detection precision and0.13% decrease in recall, which is probably a resultof parse errors.For the post-coreference filters, the approach wasquite different.
Each filter was introduced based onanalysis of the errors in the mention sets producedby our system on the development set.
Most of thefilters constructed in this way catch some true men-tions as well as spurious mentions, leading to signif-icant improvements in precision at the cost of recall.Specifically an increase of 21.46% in precision anddecrease of 8.85% in recall, but an overall increaseof 12.74% in F1-score.As Tables 2 and 3 show, these changes in mentiondetection performance generally lead to improve-ments in precision at the expense of recall, with theexception of Ceaf-e where the trends are reversed.However, as shown in Table 4, there is an overallimprovement in F1 in all cases.In general the change from only post-coreferencefilters to all filters is slightly negative.
The final sys-Metric R P F1MUC 46.39 39.56 42.70B3 63.60 57.30 60.29Ceaf-m 45.35 45.35 45.35Ceaf-e 35.05 42.26 38.32Blanc 58.74 61.58 59.91Table 5: Complete results on the test settem used all of the filters because the process used tocreate the post-coreference filters was more suscep-tible to over-fitting, and the pre-coreference filtersprovided such an unambiguously positive contribu-tion to mention detection.6 ConclusionWe modified the coreference system of Haghighiand Klein (2010) to improve mention detection per-formance.
We focused on tuning using the MUC andB3 metrics, but found considerable improvementsacross all metrics.One important difference between the system de-scribed here and previous work was the data avail-able.
Unlike Haghighi and Klein (2010), no extradata from Wikipedia or Bllip was used, a restrictionthat was necessary to be eligible for the closed partof the task.By implementing heuristics based on the annota-tion scheme for the OntoNotes data set and our ownanalysis of system behavior on the development setwe were able to achieve the results shown in Table 5,giving a final task score of 47.10.7 AcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful suggestions.
This research is sup-ported by the Office of Naval Research under MURIGrant No.
N000140911081, and a General Sir JohnMonash Fellowship.ReferencesMichael John Collins.
1999.
Head-driven statisticalmodels for natural language parsing.
Ph.D. thesis,Philadelphia, PA, USA.
AAI9926110.Aria Haghighi and Dan Klein.
2010.
Coreference resolu-tion in a modular, entity-centered model.
In Proceed-105ings of NAACL, pages 385?393, Los Angeles, Califor-nia, June.
Association for Computational Linguistics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of COLING-ACL, pages 433?440, Sydney, Australia, July.
Associ-ation for Computational Linguistics.Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,Jessica MacBride, and Linnea Micciulla.
2007.
Unre-stricted coreference: Identifying entities and events inontonotes.
In Proceedings of the International Confer-ence on Semantic Computing, pages 446?453, Wash-ington, DC, USA.
IEEE Computer Society.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
Conll-2011 shared task: Modeling unrestrictedcoreference in ontonotes.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning (CoNLL 2011), Portland, Oregon,June.106
