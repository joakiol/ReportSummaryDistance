INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 128?130,Utica, May 2012. c?2012 Association for Computational LinguisticsGenerating Natural Language Summaries  for Multimedia  Duo Ding, Florian Metze, Shourabh Rawat, Peter F. Schulam, Susanne Burger School of Computer Science, Carnegie Mellon University Pittsburgh, PA, USA 15213 {dding, fmetze, srawat, pschulam, sburger}@cs.cmu.edu   AbstractIn this paper we introduce an automatic sys-tem that generates textual summaries of Inter-net-style video clips by first identifying suitable high-level descriptive features that have been detected in the video (e.g.
visual concepts, recognized speech, actions, objects, persons, etc.).
Then a natural language genera-tor is constructed using SimpleNLG to com-pile the high-level features into a textual form.
The generated summary contains information from both visual and acoustic sources, intend-ing to give a general review and summary of the video.
To reduce the complexity of the task, we restrict ourselves to work with videos that show a limited number of ?events?.
In this demo paper, we describe the design of the system and present example outputs generated by the video summarization system.1 Introduction The Internet alows us to browse millions of vide-os.
For some of them, the content is well organized with human-generated tags and labels (e.g.
wed-ding ceremony, birthday party, etc.
), but the rate at which content is uploaded daily makes it unrealis-tic to expect that user-provided labels will be suffi-cient for organizing this information in the future.
We believe that automatically generating a brief summary (or ?abstract?)
of videos is both an attrac-tive solution to this problem and an exciting chal-lenge for the natural language generation community.
Converting audio and video output into natural language to create a human readable summary that facilitates effective browsing, sup-ports classification decisions, or helps differentiat-ing videos from one another without having to watch them in their entirety has both academic and practical value.In this paper, we introduce an automatic video summary generation system that uses a natural language realization engine (Gatt and Reiter, 2009) to create sentences based on state-of-the-art video classification features.
These features are comput-ed on a large corpus from the TrecVID evaluation (Bao, et al 2011).
In a recent user study (Ding, et al 2012), we compared automatically generated and manually generated summaries with respect to several tasks.
The study shows, for example, that more specific information (e.g.
?food?
instead of ?some object?)
and temporal information (some-thing happened first and then?)
is helpful in im-proving the quality of machine-generated summaries.
This is a first step to implement an au-tomatic system which is not only able to describe videos using natural language, but accomplishes more sophisticated tasks such as differentiating videos, finding supporting evidence for video clas-sification and other tasks.
2 Related Work Significant work has been done in the field of vid-eo summarization.
A large part of it is based on the idea that the summarization should be a graphical representation such as visually rich storyboards.
These storyboards intended to help users to effi-ciently browse the videos, e.g.
in the Open-Video Archive (Marchionini, Song et al 2009).
Christel, et al (2006) are mainly focusing on the research in user interface designs for video browsing and summarization.
Li, et al (2010) introduced a max-imal marginal relevance algorithm working across video genres to improve the quality of the informa-tive summary for a video, which exploits both au-dio and video information.
Truong et al (2007) worked on techniques targeting video data from various domains that were developed to summarize and organize the information and present surro-gates to the users.
Tan et al (2011) recently have128worked on using recognition techniques to obtain audio-visual concept classifiers to generate textual descriptions of videos.
They manually defined a template for each concept and built a rule-based language generation system to create textual de-scriptions.
But the template approach, which is directly related to specific events, cannot be adapted to new events.
In our work, we use Sim-pleNLG to generate video-specific summaries, which can be applied to any new event.
3 System Description 3.1 ArchitectureFigure 1.
System Architecture.
Figure 1 shows the overall system architecture.
The raw data of the videos is extracted and normal-ized to a format that can be read by the feature da-tabase, which stores all the features from the videos.
The ranker contains a set of algorithms that rank the features from a video and conduct content determination.
For example, when there is a long list of visual conceptual features, the ranker will sort all the features based on their relevance to the specific event?s signature and return a ranked list to the planner.
The planner is the ?commander?
of the system; it receives the ranked features and passes them to the language generator.
For each set of the features, the language generator uses Sim-pleNLG to compile a sentence stating the scenario of the video.
Eventually, the planner combines all the sentences into a summarization passage pre-senting the information detected from the video.
3.2 Feature ExtractionHigh-level features are extracted from the video using the techniques described in (Bao, et al 2011).
Visual conceptual features are detected with SVM classifiers trained on the SIN task in TRECVID 2011 using MOSIFT and CSIFT fea-tures to describe keyframes.
Other features are also extracted, including event labels, event signatures and the event kit, etc.
Event signatures are relevant features describing a certain event, similar to a fin-gerprint, and the event kit is a textual description of important objects and actions that make up the event.
For features that make use of temporal in-formation, we use a GMM based segmenter to cut the audio of each video into small clips (1-3 se-conds) and give a label to each clip.
3.3 Language Generation Taking a series of features, each of the sentence generators composes these features into a human readable sentence using the SimpleNLG generation tool.
We use SimpleNLG at the lexical level (i.e.
orthography, morphology and simple grammar) and at the phrase and sentence level (i.e.
phrase element coordination, clause subordinates).
For each set of features, the system generates a sen-tence specifically mentioning these features.
The VID generator deals with visual concepts, i.e.
the probabilities of the occurrence of 346 visual concepts extracted from the video.
A list of visual features (e.g.
food, people, room) will be processed as follows: SPhraseSpec p = nlgFactory.createClause(); p.setSubject(?the system"); p.setVerb(?observe?
); p.setObject("food, people, room"); p.setFeature(Feature.TENSE, Tense.PAST); p.addComplement(?in the video?)
to generate a sentence like: The system observed food, people and room in the video.
Another sentence generator is the ?temporal in-formation generator?, which takes the temporal information and produces a sentence describing what is happening in the video.
We first segment the audio into small clips lasting three seconds each, and assign an audio semantic label to each clip (e.g.
music, crowd, cheer, speech).
Using tem-poral information, we generate a sentence like:129From the video, the system heard the sound of music at first, then cheer, and then speech.
When the system generates several sentences, we compose them into a summary paragraph of the video.
For example, we combine the subordinate clauses using the conjunction ?because?
: The video summarization system thinks this vid-eo is about Birthday Party because it found 3 Or More People Meeting in Room.
In this sentence, ?Birthday Party?
is the event label for the given video, and ?3 Or More People?, ?Meeting?, ?Room?
are the visual concepts ex-tracted from the video.
4 Demo System Interface We demonstrate the video summarization system in a dynamic web page.
A screen shot of the demo page can be seen in Figure 2.
The top gallery shows several videos for selection.
The user can choose a video by clicking on it, and the selected video will play in the main area of the page.
Once a video is selected and playing, a summary para-graph will be automatically generated and dis-played underneath the video, presenting the video?s information in natural language.Figure 2.
A screen shot of the user interface.
The demo and the interface are currently being tested internally, in order to stabilize and improve all components, and to prepare for task-based and free-form evaluations on platforms such as Ama-zon Mechanical Turk, which will serve to further develop the NLG system.
While the NLG is cur-rently mostly hard-coded, the availability of an evaluation framework will allow us to learn pa-rameters from data, and increase the amount of automation successively.
In future work we will also explore and extend the feature sets by extract-ing additional visual, acoustic, textual features from the video.
We also plan to employ more so-phisticated NLG techniques (e.g.
microplanning and document structuring) to generate more com-plex and authentic natural language sentences.
Acknowledgments This work is partly supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center contract number D11PC20068.
The U.S. Govern-ment is authorized to reproduce and distribute re-prints for Governmental purposes notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as nec-essarily representing the official policies or en-dorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government.
References  Lei Bao, Shoou-I Yu, Zhen-zhong Lan, Arnold Over-wijk, Qin Jin, Brian Langner, Michael Garbus, Su-sanne Burger, Florian Metze, Alexander Hauptmann.
Informedia @TRECVID2011.
TRECVID2011, NIST.
Michael G. Christel.
2006.
Evaluation and User Studies with Respect to Video Summarization and Browsing.
In Proc.
?Multimedia Content Analysis, Manage-ment, and Retrieval?, part of the IS&T/SPIE Sympo-sium on Electronic Imaging.
Duo Ding, Florian Metze, Shourabh Rawat, Peter Franz Schulam, Susanne Burger, Ehsan Younessian, Lei Bao, Michael G. Christel, Alexander Hauptmann.
2012.
Beyond Audio and Video Retrieval: Towards Multimedia Summarization.
In Proc.
2012 Interna-tional Conference on Multimedia Retrieval.
Albert Gatt and Ehud Reiter.
2009.
SimpleNLG: A real-isation engine for practical applications.
In Proc.
12th European Workshop on Natural Language Gnera-toin-2009, pages 90-93.
Yingbo Li, Bernardo Merialdo.
2010.Multi-video Sum-marization Based on AV-MMR.
In Proc.
2010 Int?l Workshop on Content-Based Multimedia Indexing.
Gary Marchionini, Yaxiao Song, and Robert Ferrell.
2009.
Multimedia Surrogates for Video Gisting: To-ward Combining Spoken Words and Imagery.
Infor-mation Processing & Management 45(6), 615-630.
Ba Tu Truong and Svetha Venkatesh.
2007.
Video Ab-straction: A Systematic Review and Classification.
ACM Trans.
(TOMCCAP) 3(1), 1-37.
Chun Chet Tan, Yu-Gang Jiang, Chong-Wah Ngo.
2011.
Towards Textually Describing Complex Video Contents with Audio-Visual Concepts Classifiers.
In Proc.
ACM Multimedia-2011.130
