Proceedings of the 12th Conference of the European Chapter of the ACL, pages 630?638,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsEmpirical evaluations of animacy annotationLilja ?vrelidDepartment of LinguisticsUniversity of PotsdamGermanylilja@ling.uni-potsdam.deAbstractThis article presents empirical evaluationsof aspects of annotation for the linguis-tic property of animacy in Swedish, rang-ing from manual human annotation, auto-matic classification and, finally, an exter-nal evaluation in the task of syntactic pars-ing.
We show that a treatment of animacyas a lexical semantic property of nountypes enables generalization over distri-butional properties of these nouns whichproves beneficial in automatic classifica-tion and furthermore gives significant im-provements in terms of parsing accuracyfor Swedish, compared to a state-of-the-art baseline parser with gold standard ani-macy information.1 IntroductionThe property of animacy influences linguistic phe-nomena in a range of different languages, suchas case marking (Aissen, 2003) and argument re-alization (Bresnan et al, 2005; de Swart et al,2008), and has been shown to constitute an im-portant factor in the production and comprehen-sion of syntactic structure (Branigan et al, 2008;Weckerly and Kutas, 1999).1 In computationallinguistic work, animacy has been shown to pro-vide important information in anaphora resolution(Ora?san and Evans, 2007), argument disambigua-tion (Dell?Orletta et al, 2005) and syntactic pars-ing in general (?vrelid and Nivre, 2007).The dimension of animacy roughly distin-guishes between entities which are alive and en-tities which are not, however, other distinctions1Parts of the research reported in this paper has been sup-ported by the Deutsche Forschungsgemeinschaft (DFG, Son-derforschungsbereich 632, project D4).are also relevant and the animacy dimension is of-ten viewed as a continuum ranging from humansto inanimate objects.
Following Silverstein (1976)several animacy hierarchies have been proposed intypological studies, focusing on the linguistic cat-egory of animacy, i.e., the distinctions which arerelevant for linguistic phenomena.
An example ofan animacy hierarchy, taken from (Aissen, 2003),is provided in (1):(1) Human > Animate > InanimateClearly, non-human animates, like animals, arenot less animate than humans in a biological sense,however, humans and animals show differing lin-guistic behaviour.Empirical studies of animacy require human an-notation efforts, and, in particular, a well-definedannotation task.
However, annotation studies ofanimacy differ distinctly in their treatment of ani-macy as a type or token-level phenomenon, as wellas in terms of granularity of categories.
The useof the annotated data as a computational resourcefurthermore poses requirements on the annotationwhich do not necessarily agree with more theo-retical considerations.
Methods for the inductionof animacy information for use in practical appli-cations require the resolution of issues of level ofrepresentation, as well as granularity.This article addresses these issues through em-pirical and experimental evaluation.
We presentan in-depth study of a manually annotated dataset which indicates that animacy may be treatedas a lexical semantic property at the type level.We then evaluate this proposal through supervisedmachine learning of animacy information and fo-cus on an in-depth error analysis of the resultingclassifier, addressing issues of granularity of theanimacy dimension.
Finally, the automatically an-630notated data set is employed in order to train a syn-tactic parser and we investigate the effect of the an-imacy information and contrast the automaticallyacquired features with gold standard ones.The rest of the article is structured as follows.
Insection 2, we briefly discuss annotation schemesfor animacy, the annotation strategies and cate-gories proposed there.
We go on to describe anno-tation for the binary distinction of ?human refer-ence?
found in a Swedish dependency treebank insection 3 and we perform an evaluation of the con-sistency of the human annotation in terms of lin-guistic level.
In section 4, we present experimentsin lexical acquisition of animacy based on mor-phosyntactic features extracted from a consider-ably larger corpus.
Section 5 presents experimentswith the acquired animacy information applied inthe data-driven dependency parsing of Swedish.Finally, section 6 concludes the article and pro-vides some suggestions for future research.2 Animacy annotationAnnotation for animacy is not a common compo-nent of corpora or treebanks.
However, followingfrom the theoretical interest in the property of an-imacy, there have been some initiatives directed atanimacy annotation of corpus data.Corpus studies of animacy (Yamamoto, 1999;Dahl and Fraurud, 1996) have made use of an-notated data, however they differ in the extent towhich the annotation has been explicitly formu-lated as an annotation scheme.
The annotationstudy presented in Zaenen et.
al.
(2004) makes useof a coding manual designed for a project study-ing genitive modification (Garretson et al, 2004)and presents an explicit annotation scheme for an-imacy, illustrated by figure 1.
The main class dis-tinction for animacy is three-way, distinguishingHuman, Other animate and Inanimate, with sub-classes under two of the main classes.
The ?Otheranimate?
class further distinguishes Organizationsand Animals.
Within the group of inanimates, fur-ther distinctions are made between concrete andnon-concrete inanimate, as well as time and placenominals.2The annotation scheme described in Zaenen et.al.
(2004) annotates the markables according to2The fact that the study focuses on genitival modificationhas clearly influenced the categories distinguished, as theseare all distinctions which have been claimed to influence thechoice of genitive construction.
For instance, temporal nounsare frequent in genitive constructions, unlike the other inani-mate nouns.the animacy of their referent in the particular con-text.
Animacy is thus treated as a token levelproperty, however, has also been proposed as alexical semantic property of nouns (Yamamoto,1999).
The indirect encoding of animacy in lex-ical resources, such as WordNet (Fellbaum, 1998)can also be seen as treating animacy as a type-level property.
We may thus distinguish between apurely type level annotation strategy and a purelytoken level one.
Type level properties hold for lex-emes and are context-independent, i.e., indepen-dent of the particular linguistic context, whereastoken-level properties are determined in contextand hold for referring expressions, rather than lex-emes.3 Human reference in SwedishTalbanken05 is a Swedish treebank which wascreated in the 1970?s and which has recentlybeen converted to dependency format (Nivre etal., 2006b) and made freely available.
The writ-ten sections of the treebank consist of profes-sional prose and student essays and amount to197,123 running tokens, spread over 11,431 sen-tences.
Figure 2 shows the labeled dependencygraph of example (2), taken from Talbanken05.
(2) Sammasameerfarenhetexperiencegjordemadeengelsma?nnenenglishmen-DEF?The same experience, the Englishmen had?___SammaPOKPerfarenhetNN_gjordeVVPTengelsmannenNNDD|HHROOTDT OO SSFigure 2: Dependency representation of example(2) from Talbanken05.In addition to information on part-of-speech, de-pendency head and relation, and various mor-phosyntactic properties such as definiteness, theannotation expresses a distinction for nominal el-ements between reference to human and non-human.
The annotation manual (Teleman, 1974)states that a markable should be tagged as human(HH) if it may be replaced by the interrogative pro-noun vem ?who?
and be referred to by the personalpronouns han ?he?
or hon ?she?.There are clear similarities between the anno-tation for human reference found in Talbanken05and the annotation scheme for animacy discussed631ANIMCONC NCONC TIME PLACEORGHUM InanimateOtheranimateFigure 1: Animacy classification scheme (Zaenen et al, 2004).above.
The human/non-human contrast forms thecentral distinction in the animacy dimension and,in this respect, the annotation schemes do not con-flict.
If we compare the annotation found in Tal-banken05 with the annotation proposed in Zaenenet.
al.
(2004), we find that the schemes differ pri-marily in the granularity of classes distinguished.The main source of variation in class distinctionsconsists in the annotation of collective nouns, in-cluding organizations, as well as animals.3.1 Level of annotationWe distinguished above between type and tokenlevel annotation strategies, where a type level an-notation strategy entails that an element consis-tently be assigned to only one class.
A token levelstrategy, in contrast, does not impose this restric-tion on the annotation and class assignment mayvary depending on the specific context.
Garretsonet.
al (2004) propose a token level annotation strat-egy and state that ?when coding for animacy [.
.
.
]we are not considering the nominal per se (e.g., theword ?church?
), but rather the entity that is the ref-erent of that nominal (e.g.
some particular thing inthe real world)?.
This indicates that for all possiblemarkables, a referent should be determinable.The brief instruction with respect to annotationfor human reference in the annotation manual forTalbanken05 (Teleman, 1974, 223) gives leewayfor interpretation in the annotation and does notclearly state that it should be based on token levelreference in context.
It may thus be interestingto examine the extent to which this manual an-notation is consistent across lexemes or whetherwe observe variation.
We manually examine theintersection of the two classes of noun lemmasin the written sections of Talbanken, i.e., the setof nouns which have been assigned both classesby the annotators.
It contains 82 noun lemmas,which corresponds to only 1.1% of the total num-ber of noun lemmas in the treebank (7554 lem-mas all together).
After a manual inspection ofthe intersective elements along with their linguis-tic contexts, we may group the nouns which wereassigned to both classes, into the following cate-gories:that ?HH?
is the tag forAbstract nouns Nouns with underspecified orvague type level properties with respect to ani-macy, such as quantifying nouns, e.g.
ha?lft ?half?,miljon ?million?, as well as nouns which may beemployed with varying animacy, e.g.
element ?el-ement?, part ?party?, as in (3) and (4):(3) .
.
.
ocksa?.
.
.
alsodentheandraotherpartenHHparty-DEFsta?rstandsutanfo?routside?.
.
.
also the other party is left outside?
(4) Iinettafo?rha?llanderelationshipa?rarealdrigneverba?ggebothparterpartieslikasamestarkastrong?In a relationship, both parties are never equallystrong?We also find that nouns which denote abstract con-cepts regarding humans show variable annotation,e.g.
individ ?individual?, adressat ?addressee?,medlem ?member?, kandidat ?candidate?, repre-sentant ?representative?, auktoritet ?authority?Reference shifting contexts These are nounswhose type level animacy is clear but which areemployed in a specific context which shifts theirreference.
Examples include metonymic usage ofnouns, as in (5) and nouns occurring in derefer-encing constructions, such as predicative construc-tions (6), titles (7) and idioms (8):(5) .
.
.
daghemmensHH.
.
.
kindergarten-DEF.GENotillra?ckligainadequateresurserresources?.
.
.
the kindergarten?s inadequate resources?
(6) .
.
.
fo?r.
.
.
foratttoblibecomeenabragoodsoldatsoldier?.
.
.
in order to become a good soldier?
(7) .
.
.
menar.
.
.
thinksbiskopbishopHellstenHellsten?thinks bishop Hellsten?
(8) tatakestudentenstudent-DEF?graduate from highschool (lit.
take the student)?632It is interesting to note that the main variation inannotation stems precisely from difficulties in de-termining reference, either due to bleak type levelproperties such as for the abstract nouns, or due toproperties of the context, as in the reference shift-ing constructions.
The small amount of variationin the human annotation for animacy clearly sup-ports a type-level approach to animacy, however,underline the influence of the linguistic context onthe conception of animacy, as noted in the litera-ture (Zaenen et al, 2004; Rosenbach, 2008).4 Lexical acquisition of animacyEven though knowledge about the animacy of anoun clearly has some interesting implications, lit-tle work has been done within the field of lexicalacquisition in order to automatically acquire ani-macy information.
Ora?san and Evans (2007) makeuse of hyponym-relations taken from the Word-Net resource in order to classify animate referents.However, such a method is clearly restricted tolanguages for which large scale lexical resources,such as the WordNet, are available.
The task ofanimacy classification bears some resemblance tothe task of named entity recognition (NER) whichusually makes reference to a ?person?
class.
How-ever, whereas most NER systems make extensiveuse of orthographic, morphological or contextualclues (titles, suffixes) and gazetteers, animacy fornouns is not signaled overtly in the same way.Following a strategy in line with work onverb classification (Merlo and Stevenson, 2001;Stevenson and Joanis, 2003), we set out to clas-sify common noun lemmas based on their mor-phosyntactic distribution in a considerably largercorpus.
This is thus equivalent to treatment ofanimacy as a lexical semantic property and theclassification strategy is based on generalizationof morphosyntactic behaviour of common nounsover large quantities of data.
Due to the small sizeof the Talbanken05 treebank and the small amountof variation, this strategy was pursued for the ac-quisition of animacy information.In the animacy classification of common nounswe exploit well-documented correlations betweenmorphosyntactic realization and semantic proper-ties of nouns.
For instance, animate nouns tend tobe realized as agentive subjects, inanimate nounsdo not (Dahl and Fraurud, 1996).
Animate nounsmake good ?possessors?, whereas inanimate nounsare more likely ?possessees?
(Rosenbach, 2008).Table 1 presents an overview of the animacy dataClass Types Tokens coveredAnimate 644 6010Inanimate 6910 34822Total 7554 40832Table 1: The animacy data set from Talbanken05;number of noun lemmas (Types) and tokens ineach class.for common nouns in Talbanken05.
It is clear thatthe data is highly skewed towards the non-humanclass, which accounts for 91.5% of the type in-stances.
For classification we organize the datainto accumulated frequency bins, which includeall nouns with frequencies above a certain thresh-old.
We here approximate the class of ?animate?to ?human?
and the class of ?inanimate?
to ?non-human?.
Intersective elements, see section 3.1, areassigned to their majority class.34.1 Features for animacy classificationWe define a feature space, which makes use ofdistributional data regarding the general syntacticproperties of a noun, as well as various morpho-logical properties.
It is clear that in order for asyntactic environment to be relevant for animacyclassification it must be, at least potentially, nom-inal.
We define the nominal potential of a depen-dency relation as the frequency with which it isrealized by a nominal element (noun or pronoun)and determine empirically a threshold of .10.
Thesyntactic and morphological features in the featurespace are presented below:Syntactic features A feature for each depen-dency relation with nominal potential: (tran-sitive) subject (SUBJ), object (OBJ), preposi-tional complement (PA), root (ROOT)4, ap-position (APP), conjunct (CC), determiner(DET), predicative (PRD), complement ofcomparative subjunction (UK).
We also in-clude a feature for the head of a genitive mod-ifier, the so-called ?possessee?, (GENHD).Morphological features A feature for each mor-phological distinction relevant for a noun3When there is no majority class, i.e.
in the case of ties,the noun is removed from the data set.
12 lemmas were con-sequently removed.4Nominal elements may be assigned the root relation ofthe dependency graph in sentence fragments which do notcontain a finite verb.633in Swedish: gender (NEU/UTR), num-ber (SIN/PLU), definiteness (DEF/IND), case(NOM/GEN).
Also, the part-of-speech tagsdistinguish dates (DAT) and quantifyingnouns (SET), e.g.
del, rad ?part, row?, sothese are also included as features.For extraction of distributional data for the set ofSwedish nouns we make use of the Swedish Pa-role corpus of 21.5M tokens.5 To facilitate featureextraction, we part-of-speech tag the corpus andparse it with MaltParser6, which assigns a depen-dency analysis.74.2 Experimental methodologyFor machine learning, we make use of the TilburgMemory-Based Learner (TiMBL) (Daelemans etal., 2004).8 Memory-based learning is a super-vised machine learning method characterized bya lazy learning algorithm which postpones learn-ing until classification time, using the k-nearestneighbor algorithm for the classification of unseeninstances.
For animacy classification, the TiMBLparameters are optimized on a subset of the fulldata set.9For training and testing of the classifiers, wemake use of leave-one-out cross-validation.
Thebaseline represents assignment of the majorityclass (inanimate) to all nouns in the data set.
Dueto the skewed distribution of classes, as notedabove, the baseline accuracy is very high, usu-ally around 90%.Clearly, however, the class-basedmeasures of precision and recall, as well as thecombined F-score measure are more informativefor these results.
The baseline F-score for the ani-mate class is thus 0, and a main goal is to improveon the rate of true positives for animates, whilelimiting the trade-off in terms of performance for5Parole is freely available at http://spraakbanken.gu.se6http://www.maltparser.org7For part-of-speech tagging, we employ the MaltTagger?
a HMM part-of-speech tagger for Swedish (Hall, 2003).For parsing, we employ MaltParser (Nivre et al, 2006a),a language-independent system for data-driven dependencyparsing , with the pretrained model for Swedish, which hasbeen trained on the tags output by the tagger.8http://ilk.uvt.nl/software.html9For parameter optimization we employ theparamsearch tool, supplied with TiMBL, seehttp://ilk.uvt.nl/software.html.
Paramsearch implementsa hill climbing search for the optimal settings on iterativelylarger parts of the supplied data.
We performed parameteroptimization on 20% of the total data set, where we balancedthe data with respect to frequency.
The resulting settings arek = 11, GainRatio feature weighting and Inverse Linear (IL)class voting weights.Bin Instances Baseline MBL SVM>1000 291 89.3 97.3 95.2>500 597 88.9 97.3 97.1>100 1668 90.5 96.8 96.9>50 2278 90.6 96.1 96.0>10 3786 90.8 95.4 95.1>0 5481 91.3 93.9 93.7Table 2: Accuracy for MBL and SVM classifierson Talbanken05 nouns in accumulated frequencybins by Parole frequency.the majority class of inanimates, which start outwith F-scores approaching 100.
For calculation ofthe statistical significance of differences in the per-formance of classifiers tested on the same data set,McNemar?s test (Dietterich, 1998) is employed.4.3 ResultsColumn four (MBL) in table 2 shows the accu-racy obtained with all features in the general fea-ture space.
We observe a clear improvement onall data sets (p<.0001), compared to the respec-tive baselines.
As we recall, the data sets are suc-cessively larger, hence it seems fair to concludethat the size of the data set partially counteractsthe lower frequency of the test nouns.
It is notsurprising, however, that a method based on dis-tributional features suffers when the absolute fre-quencies approach 1.
We obtain results for ani-macy classification, ranging from 97.3% accuracyto 93.9% depending on the sparsity of the data.With an absolute frequency threshold of 10, weobtain an accuracy of 95.4%, which constitutes a50% reduction of error rate.Table 3 presents the experimental results rela-tive to class.
We find that classification of the inan-imate class is quite stable throughout the experi-ments, whereas the classification of the minorityclass of animate nouns suffers from sparse data.
Itis an important point, however, that it is largely re-call for the animate class which goes down withincreased sparseness, whereas precision remainsquite stable.
All of these properties are clearly ad-vantageous in the application to realistic data sets,where a more conservative classifier is to be pre-ferred.4.4 Error analysisThe human reference annotation of the Tal-banken05 nouns distinguishes only the classes cor-responding to ?human?
and ?inanimate?
along the634Animate InanimatePrecision Recall Fscore Precision Recall Fscore>1000 89.7 83.9 86.7 98.1 98.8 98.5>500 89.1 86.4 87.7 98.3 98.7 98.5>100 87.7 76.6 81.8 97.6 98.9 98.2>50 85.8 70.2 77.2 97.0 98.9 97.9>10 81.9 64.0 71.8 96.4 98.6 97.5>0 75.7 44.9 56.4 94.9 98.6 96.7Table 3: Precision, recall and F-scores for the two classes in MBL-experiments with a general featurespace.>10 nouns(a) (b) ?
classified as222 125 (a) class animate49 3390 (b) class inanimateTable 4: Confusion matrix for the MBL-classifierwith a general feature space on the >10 data seton Talbanken05 nouns.animacy dimension.
An interesting question iswhether the errors show evidence of the gradi-ence in categories discussed earlier and explic-itly expressed in the annotation scheme by Zaenenet.al.
(2004) in figure 1.
If so, we would expecterroneously classified inanimate nouns to containnouns of intermediate animacy, such as animalsand organizations.The error analysis examines the performance ofthe MBL-classifier employing all features on the> 10 data set in order to abstract away from themost serious effects of data sparseness.
Table 4shows a confusion matrix for the classification ofthe nouns.
If we examine the errors for the inan-imate class we indeed find evidence of gradiencewithin this category.
The errors contain a groupof nouns referring to animals and other living be-ings (bacteria, algae), as listed in (9), as well asone noun referring to an ?intelligent machine?, in-cluded in the intermediate animacy category in Za-enen et al (2004).
Collective nouns with humanreference and organizations are also found amongthe errors, listed in (11).
We also find some nounsamong the errors with human denotation, listed in(12).
These are nouns which typically occur indereferencing contexts, such as titles, e.g.
herr?mister?, biskop ?bishop?
and which were anno-tated as non-human referring by the human an-notators.10 Finally, a group of abstract, human-10In fact, both of these showed variable annotation in thetreebank and were assigned their majority class ?
inanimatedenoting nouns are also found among the errors, aslisted in (13).
In summary, we find that nouns withgradient animacy properties account for 53.1% ofthe errors for the inanimate class.
(9) Animals/living beings:alg ?algae?, apa ?monkey?, bakterie ?bacteria?, bjo?rn?bear?, djur ?animal?, fa?gel ?bird?, fladdermo?ss ?bat?,myra ?ant?, ma?s ?seagull?, parasit ?parasite?
(10) Intelligent machines:robot ?robot?
(11) Collective nouns, organizations:myndighet ?authority?, nation ?nation?, fo?retagsledning?corporate-board?, personal ?personell?, stiftelse?foundation?, idrottsklubb ?sport-club?
(12) Human-denoting nouns:biskop ?bishop?, herr ?mister?, nationalist?nationalist?, tolk ?interpreter?
(13) Abstract, human nouns:fo?rlorare ?loser?, huvudpart ?main-party?, konkurrent?competitor?, majoritet ?majority?, va?rd ?host?It is interesting to note that both the hu-man and automatic annotation showed difficul-ties in ascertaining class for a group of ab-stract, human-denoting nouns, like individ ?indi-vidual?, motsta?ndare ?opponent?, kandidat ?candi-date?, representant ?representative?.
These wereall assigned to the animate majority class dur-ing extraction, but were misclassified as inanimateduring classification.4.5 SVM classifiersIn order to evaluate whether the classificationmethod generalizes to a different machine learn-ing algorithm, we design an identical set of experi-ments to the ones presented above, but where clas-sification is performed with Support Vector Ma-chines (SVMs) instead of MBL.
We use the LIB-SVM package (Chang and Lin, 2001) with a RBFkernel (C = 8.0, ?
= 0.5).11?
in the extraction of training data.11As in the MBL-experiment, parameter optimization, i.e.,choice of kernel function, C and ?
values, is performed on20% of the total data set with the easy.py tool, suppliedwith LIBSVM.635As column 5 (SVM) in table 2 shows, the clas-sification results are very similar to the results ob-tained with MBL.12 We furthermore find a verysimilar set of errors, and in particular, we find that51.0 % of the errors for the inanimate class arenouns with the gradient animacy properties pre-sented in (9)-(13) above.5 Parsing with animacy informationAs an external evaluation of our animacy classi-fier, we apply the induced information to the taskof syntactic parsing.
Seeing that we have a tree-bank with gold standard syntactic information andgold standard as well as induced animacy informa-tion, it should be possible to study the direct effectof the added animacy information in the assign-ment of syntactic structure.5.1 Experimental methodologyWe use the freely available MaltParser system,which is a language-independent system for data-driven dependency parsing (Nivre, 2006; Nivre etal., 2006c).
A set of parsers are trained on Tal-banken05, both with and without additional an-imacy information, the origin of which is eitherthe manual annotation described in section 3 orthe automatic animacy classifier described in sec-tion 4.2- 4.4 (MBL).
The common nouns in thetreebank are classified for animacy using leave-one-out training and testing.
This ensures thatthe training and test instances are disjoint at alltimes.
Moreover, the fact that the distributionaldata is taken from a separate data set ensures non-circularity since we are not basing the classifica-tion on gold standard parses.All parsing experiments are performed using10-fold cross-validation for training and testing onthe entire written part of Talbanken05.
Overallparsing accuracy will be reported using the stan-dard metrics of labeled attachment score (LAS)and unlabeled attachment score (UAS).13 Statis-tical significance is checked using Dan Bikel?srandomized parsing evaluation comparator.14 Asour baseline, we use the settings optimized forSwedish in the CoNLL-X shared task (Buchholz12The SVM-classifiers generally show slightly lower re-sults, however, only performance on the >1000 data set issignificantly lower (p<.05).13LAS and UAS report the percentage of tokens that are as-signed the correct head with (labeled) or without (unlabeled)the correct dependency label.14http://www.cis.upenn.edu/?dbikel/software.htmlGold standard AutomaticUAS LAS UAS LASBaseline 89.87 84.92 89.87 84.92Anim 89.81 84.94 89.87 84.99Table 5: Overall results in experiments with au-tomatic features compared to gold standard fea-tures, expressed as unlabeled and labeled attach-ment scores.and Marsi, 2006), where this parser was the bestperforming parser for Swedish.5.2 ResultsThe addition of automatically assigned animacyinformation for common nouns (Anim) causes asmall, but significant improvement in overall re-sults (p<.04) compared to the baseline, as wellas the corresponding gold standard experiment(p<.04).
In the gold standard experiment, the re-sults are not significantly better than the baselineand the main, overall, improvement from the goldstandard animacy information reported in ?vrelidand Nivre (2007) and ?vrelid (2008) stems largelyfrom the animacy annotation of pronouns.15 Thisindicates that the animacy information for com-mon nouns, which has been automatically ac-quired from a considerably larger corpus, capturesdistributional distinctions which are important forthe general effect of animacy and furthermore thatthe differences from the gold standard annotationprove beneficial for the results.We see from Table 5, that the improvement inoverall parse results is mainly in terms of depen-dency labeling, reflected in the LAS score.
Acloser error analysis shows that the performanceof the two parsers employing gold and automaticanimacy information is very similar with respectto dependency relations and we observe an im-proved analysis for subjects, (direct and indirect)objects and subject predicatives with only minorvariations.
This in itself is remarkable, since thecovered set of animate instances is notably smallerin the automatically annotated data set.
We fur-thermore find that the main difference between thegold standard and automatic Anim-experiments15Recall that the Talbanken05 treebank contains animacyinformation for all nominal elements ?
pronouns, proper andcommon nouns.
When the totality of this information isadded the overall parse results are significantly improved(p<.0002) (?vrelid and Nivre, 2007; ?vrelid, 2008).636does not reside in the analysis of syntactic argu-ments, but rather of non-arguments.
One rela-tion for which performance deteriorates with theadded information in the gold Anim-experimentis the nominal postmodifier relation (ET) whichis employed for relative clauses and nominal PP-attachment.
With the automatically assigned fea-ture, in contrast, we observe an improvement inthe performance for the ET relation, compared tothe gold standard experiment, from a F-score inthe latter of 76.14 to 76.40 in the former.
Sincethis is a quite common relation, with a frequencyof 5% in the treebank as a whole, the improvementhas a clear effect on the results.The parser?s analysis of postnominal modifica-tion is influenced by the differences in the addedanimacy annotation for the nominal head, as wellas the internal dependent.
If we examine the cor-rected errors in the automatic experiment, com-pared to the gold standard experiment, we find ele-ments with differing annotation.
Preferences withrespect to the animacy of prepositional comple-ments vary.
In (14), the automatic annotation ofthe noun djur ?animal?
as animate results in cor-rect assignment of the ET relation to the prepo-sition hos ?among?, as well as correct nominal,as opposed to verbal, attachment.
This preposi-tion is one of the few with a preference for an-imate complements in the treebank.
In contrast,the example in (15) illustrates an error where theautomatic classification of barn ?children?
as inan-imate causes a correct analysis of the head prepo-sition om ?about?.16(14) .
.
.
samha?llsbildningar.
.
.
societieshosamongolikadifferentdjuranimals?.
.
.
social organizations among different animals?
(15) Fo?ra?ldrarparentsharhaveva?rdnadencustody-DEFomofsinatheirbarnchildren?Parents have the custody of their children?A more thorough analysis of the different factorsinvolved in PP-attachment is a complex task whichis clearly beyond the scope of the present study.We may note, however, that the distinctions in-duced by the animacy classifier based purely onlinguistic evidence proves useful for the analysisof both arguments and non-arguments.16Recall that the classification is based purely on linguisticevidence and in this respect children largely pattern with theinanimate nouns.
A child is probably more like a physicalobject in the sense that it is something one possesses and oth-erwise reacts to, rather than being an agent that acts upon itssurroundings.6 ConclusionThis article has dealt with an empirical evaluationof animacy annotation in Swedish, where the mainfocus has been on the use of such annotation forcomputational purposes.We have seen that human annotation for ani-macy shows little variation at the type-level fora binary animacy distinction.
Following fromthis observation, we have shown how a type-level induction strategy based on morphosyntac-tic distributional features enables automatic ani-macy classification for noun lemmas which fur-thermore generalizes to different machine learningalgorithms (MBL, SVM).
We obtain results for an-imacy classification, ranging from 97.3% accuracyto 93.9% depending on the sparsity of the data.With an absolute frequency threshold of 10, weobtain an accuracy of 95.4%, which constitutes a50% reduction of error rate.
A detailed error anal-ysis revealed some interesting results and we sawthat more than half of the errors performed by theanimacy classifier for the large class of inanimatenouns actually included elements which have beenassigned an intermediate animacy status in theo-retical work, such as animals and collective nouns.The application of animacy annotation in thetask of syntactic parsing provided a test bed forthe applicability of the annotation, where we couldcontrast the manually assigned classes with theautomatically acquired ones.
The results showedthat the automatically acquired information givesa slight, but significant improvement of overallparse results where the gold standard annotationdoes not, despite a considerably lower coverage.This is a suprising result which highlights impor-tant properties of the annotation.
First of all, theautomatic annotation is completely consistent atthe type level.
Second, the automatic animacyclassifier captures important distributional proper-ties of the nouns, exemplified by the case of nom-inal postmodifiers in PP-attachment.
The auto-matic annotation thus captures a purely linguisticnotion of animacy and abstracts over contextualinfluence in particular instances.Animacy has been shown to be an importantproperty in a range of languages, hence animacyclassification of other languages constitutes an in-teresting line of work for the future, where empir-ical evaluations may point to similarities and dif-ferences in the linguistic expression of animacy.637ReferencesJudith Aissen.
2003.
Differential Object Marking: Iconicityvs.
economy.
Natural Language and Linguistic Theory,21(3):435?483.Holly P. Branigan, Martin J. Pickering, and Mikihiro Tanaka.2008.
Contributions of animacy to grammatical func-tion assignment and word order production.
Lingua,118(2):172?189.Joan Bresnan, Anna Cueni, Tatiana Nikitina, and HaraldBaayen.
2005.
Predicting the dative alternation.
In GosseBouma, Irene Kraemer, and Joost Zwarts, editors, Cog-nitive foundations of interpretation, pages 69?94.
RoyalNetherlands Academy of Science, Amsterdam.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In Proceedingsof the Tenth Conference on Computational Natural Lan-guage Learning (CoNLL-X), pages 149?164.Chih-Chung Chang and Chih-Jen Lin.
2001.
LIBSVM: Alibrary for support vector machines.
Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.Walter Daelemans, Jakub Zavrel, Ko Van der Sloot, and An-tal Van den Bosch.
2004.
TiMBL: Tilburg Memory BasedLearner, version 5.1, Reference Guide.
Technical report,ILK Technical Report Series 04-02.?Osten Dahl and Kari Fraurud.
1996.
Animacy in gram-mar and discourse.
In Thorstein Fretheim and Jeanette K.Gundel, editors, Reference and referent accessibility,pages 47?65.
John Benjamins, Amsterdam.Peter de Swart, Monique Lamers, and Sander Lestrade.
2008.Animacy, argument structure and argument encoding: In-troduction to the special issue on animacy.
Lingua,118(2):131?140.Felice Dell?Orletta, Alessandro Lenci, Simonetta Monte-magni, and Vito Pirrelli.
2005.
Climbing the path togrammar: A maximum entropy model of subject/objectlearning.
In Proceedings of the 2nd Workshop on Psy-chocomputational Models of Human Language Acquisi-tion, pages 72?81.Thomas G. Dietterich.
1998.
Approximate statistical test forcomparing supervised classification learning algorithms.Neural Computation, 10(7):1895?1923.Christiane Fellbaum, editor.
1998.
WordNet: an electroniclexical database.
MIT Press, Cambridge, MA.Gregory Garretson, M. Catherine O?Connor, BarboraSkarabela, and Marjorie Hogan, 2004.
Optimal Typol-ogy of Determiner Phrases Coding Manual.
BostonUniversity, version 3.2 edition.
Downloaded fromhttp://people.bu.edu/depot/coding manual.html on02/15/2006.Johan Hall.
2003.
A probabilistic part-of-speech taggerwith suffix probabilities.
Master?s thesis, Va?xjo?
Univer-sity, Sweden.Paola Merlo and Suzanne Stevenson.
2001.
Automatic verbclassification based on statistical distributions of argumentstructure.
Computational Linguistics, 27(3):373?408.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006a.
Malt-parser: A data-driven parser-generator for dependencyparsing.
In Proceedings of the Fifth International Con-ference on Language Resources and Evaluation (LREC),pages 2216?2219.Joakim Nivre, Jens Nilsson, and Johan Hall.
2006b.
Tal-banken05: A Swedish treebank with phrase structure anddependency annotation.
In Proceedings of the fifth Inter-national Conference on Language Resources and Evalua-tion (LREC), pages 1392?1395.Joakim Nivre, Jens Nilsson, Johan Hall, Gu?ls?en Eryig?it, andSvetoslav Marinov.
2006c.
Labeled pseudo-projectivedependency parsing with Support Vector Machines.
InProceedings of the Conference on Computational NaturalLanguage Learning (CoNLL).Joakim Nivre.
2006.
Inductive Dependency Parsing.Springer, Dordrecht.Constantin Ora?san and Richard Evans.
2007.
NP animacyresolution for anaphora resolution.
Journal of ArtificialIntelligence Research, 29:79?103.Lilja ?vrelid and Joakim Nivre.
2007.
When word order andpart-of-speech tags are not enough ?
Swedish dependencyparsing with rich linguistic features.
In Proceedings of theInternational Conference on Recent Advances in NaturalLanguage Processing (RANLP), pages 447?451.Lilja ?vrelid.
2008.
Linguistic features in data-drivendependency parsing.
In Proceedings of the Conferenceon Computational Natural Language Learning (CoNLL2008).Anette Rosenbach.
2008.
Animacy and grammatical vari-ation - findings from English genitive variation.
Lingua,118(2):151?171.Michael Silverstein.
1976.
Hierarchy of features and erga-tivity.
In Robert M.W.
Dixon, editor, Grammatical cat-egories in Australian Languages, pages 112?171.
Aus-tralian Institute of Aboriginal Studies, Canberra.Suzanne Stevenson and Eric Joanis.
2003.
Semi-supervisedverb class discovery using noisy features.
In Proceedingsof the Conference on Computational Natural LanguageLearning (CoNLL), pages 71?78.Ulf Teleman.
1974.
Manual fo?r grammatisk beskrivning avtalad och skriven svenska.
Studentlitteratur, Lund.J.
Weckerly and M. Kutas.
1999.
An electrophysiologicalanalysis of animacy effects in the processing of object rel-ative sentences.
Psychophysiology, 36:559?570.Mutsumi Yamamoto.
1999.
Animacy and Reference: A cog-nitive approach to corpus linguistics.
John Benjamins,Amsterdam.Annie Zaenen, Jean Carletta, Gregory Garretson, JoanBresnan, Andrew Koontz-Garboden, Tatiana Nikitina,M.
Catherine O?Connor, and Tom Wasow.
2004.
Ani-macy encoding in English: why and how.
In Donna By-ron and Bonnie Webber, editors, Proceedings of the ACLWorkshop on Discourse Annotation.638
