Proceedings of the ACL 2007 Student Research Workshop, pages 37?42,Prague, June 2007. c?2007 Association for Computational LinguisticsKinds of Features for Chinese Opinionated Information RetrievalTaras ZagibalovDepartment of InformaticsUniversity of SussexUnited KingdomT.Zagibalov@sussex.ac.ukAbstractThis paper presents the results of experi-ments in which we tested different kinds offeatures for retrieval of Chinese opinionatedtexts.
We assume that the task of retrieval ofopinionated texts (OIR) can be regarded asa subtask of general IR, but with some dis-tinct features.
The experiments showed thatthe best results were obtained from the com-bination of character-based processing, dic-tionary look up (maximum matching) and anegation check.1 IntroductionThe extraction of opinionated information has re-cently become an important research topic.
Businessand governmental institutions often need to have in-formation about how their products or actions areperceived by people.
Individuals may be interestedin other people?s opinions on various topics rangingfrom political events to consumer products.At the same time globalization has made thewhole world smaller, and a notion of the world asa ?global village?
does not surprise people nowa-days.
In this context we assume information in Chi-nese to be of particular interest as the Chinese world(the mainland China, Taiwan, Hong Kong, Singa-pore and numerous Chinese communities all overthe world) is getting more and more influential overthe world economy and politics.We therefore believe that a system capable of pro-viding access to opinionated information in otherlanguages (especially in Chinese) might be of greatuse for individuals as well as for institutions in-volved in international trade or international rela-tions.The sentiment classification experiments pre-sented in this paper were done in the context ofOpinionated Information Retrieval which is plannedto be a module in a Cross-Language Opinion Extrac-tion system (CLOE).
The main goal of this system isto provide access to opinionated information on anytopic ad-hoc in a language different to the languageof a query.To implement the idea the CLOE system whichis the context for the experiments described in thepaper will consist of four main modules:1.
Query translation2.
Opinionated Information Retrieval3.
Opinionated Information Extraction4.
Results presentationThe OIR module will process complex queriesconsisting of a word sequence indicating a topic andsentiment information.
An example of such a queryis: ?Asus laptop + OPINIONS?, another, more de-tailed query, might be ?Asus laptop + POSITIVEOPINIONS?.Another possible approach to the architecture ofthe CLOE system would be to implement the pro-cessing as a pipeline consisting, first, of using IR toretrieve certain articles relevant to the topic followedby second stage of classifying them according tosentiment polarity.
But such an approach probablywould be too inefficient, as the search will producea lot of irrelevant results (containing no opinionatedinformation).372 Chinese NLP and Feature SelectionProblemOne of the central problems in Chinese NLP is whatthe basic unit1 of processing should be.
The problemis caused by a distinctive feature of the Chinese lan-guage - absence of explicit word boundaries, while itis widely assumed that a word is of extreme impor-tance for any NLP task.
This problem is also crucialfor the present study as the basic unit definition af-fects the kinds of features to be used.In this study we use a mixed approached, basedboth on words (tokens consisting of more than onecharacter) and characters as basic units.
It is alsoimportant to note, that we use notion of words inthe sense of Vocabulary Word as it was stated by Li(2000).
This means that we use only tokens that arelisted in a dictionary, and do not look for all words(including grammar words).3 Related WorkProcessing of subjective texts and opinions has re-ceived a lot of interest recently.
Most of the authorstraditionally use a classification-based approach forsentiment extraction and sentiment polarity detec-tion (for example, Pang et al (2002), Turney (2002),Kim and Hovy (2004) and others), however, the re-search described in this paper uses the informationretrieval (IR) paradigm which has also been used bysome researchers.Several sentiment information retrieval modelswere proposed in the framework of probabilistic lan-guage models by Eguchi and Lavrenko (2006).
Thesetting for the study was a situation when a user?squery specifies not only terms expressing a certaintopic and also specifies a sentiment polarity of in-terest in some manner, which makes this researchvery similar to the present one.
However, we usesentiment scores (not probabilistic language mod-els) for sentiment retrieval (see Section 4.1).
Daveet al (Dave et al, 2003) described a tool for sift-ing through and synthesizing product reviews, au-tomating the sort of work done by aggregation sitesor clipping services.
The authors of this paper usedprobability scores of arbitrary-length substrings thatprovide optimal classification.
Unlike this approach1In the context of this study terms ?feature?
and ?basic unit?are used interchangeably.we use a combination of sentiment weights of char-acters and words (see Section 4).Recently several works on sentiment extractionfrom Chinese texts were published.
In a paper byKu et al (2006a) a dictionary-based approach wasused in the context of sentiment extraction and sum-marization.
The same authors describe a corpus ofopinionated texts in another paper (2006b).
This pa-per also defines the annotations for opinionated ma-terials.
Although we use the same dictionary in ourresearch, we do not use only word-based approachto sentiment detection, but we also use scores forcharacters obtained by processing the dictionary asa training corpus (see Section 4).4 ExperimentsIn this paper we present the results of sentiment clas-sification experiments in which we tested differentkinds of features for retrieval of Chinese opinionatedinformation.As stated earlier (see Section 1), we assume thatthe task of retrieval of opinionated texts (OIR) canbe regarded as a subtask of general IR with a queryconsisting of two parts: (1) words indicating topicand (2) a semantic class indicating sentiment (OPIN-IONS).
The latter part of the query cannot be speci-fied in terms that can be instantly used in the processof retrieval.The sentiment part of the query can be further de-tailed into subcategories such as POSITIVE OPIN-IONS, NEGATIVE OPINIONS, NEUTRAL OPIN-IONS each of which can be split according to sen-timent intensity (HIGHLY POSITIVE OPINIONS,SLIGHTLY NEGATIVE OPINIONS etc.).
Butwhatever level of categorisation we use, the queryis still too abstract and cannot be used in practice.
Ittherefore needs to be put into words and most prob-ably expanded.
The texts should also be indexedwith appropriate sentiment tags which in the contextof sentiment processing implies classification of thetexts according to presence / absence of a sentimentand, if the texts are opinionated, according to theirsentiment polarity.To test the proposed approach we designed twoexperiments.The purpose of the first experiment was to find themost effective kind of features for sentiment polar-38ity discrimination (detection) which can be used forOIR 2.
Nie et al (2000) found that for Chinese IRthe most effective kinds of features were a combina-tion of dictionary look up (longest-match algorithm)together with unigrams (single characters).
The ap-proach was tested in the first experiment.The second experiment was designed to test thefound set of features for text classification (index-ing) for an OIR query of the first level (finds opin-ionated information) and for an OIR query of thesecond level (finds opinionated information withsentiment direction detection), thus the classifiershould 1) detect opinionated texts and 2) classify thefound items either as positive or as negative.As training corpus for the second experiment weuse the NTU sentiment dictionary (NTUSD) (by Kuet al (2006a))3 as well as a list of sentiment scoresof Chinese characters obtained from processing ofthe same dictionary.
Dictionary look up used thelongest-match algorithm.
The dictionary has 2809items in the ?positive?
part and 8273 items in the?negative?.
The same dictionary was also used as acorpus for calculating the sentiment scores of Chi-nese characters.
The use of the dictionary as atraining corpus for obtaining the sentiment scoresof characters is justified by two reasons: 1) it isdomain-independent and 2) it contains only relevant(sentiment-related) information.
The above men-tioned parts of the dictionary used as the corpuscomprised 24308 characters in the ?negative?
partand 7898 characters in the ?positive?
part.4.1 Experiment 1A corpus of E-Bay4 customers?
reviews of productsand services was used as a test corpus.
The totalnumber of reviews is 128, of which 37 are nega-tive (average length 64 characters) and 91 are pos-itive (average length 18 characters), all of the re-views were tagged as ?positive?
or ?negative?
by the2For simplicity we used only binary polarity in both exper-iments: positive or negative.
Thus terms ?sentiment polarity?and ?sentiment direction?
are used interchangeably in this pa-per.3Ku et al (2006a) automatically generated the dictionaryby enlarging an initial manually created seed vocabulary byconsulting two thesauri, including tong2yi4ci2ci2lin2 and theAcademia Sinica Bilingual Ontological Wordnet 3.4http://www.ebay.com.cn/reviewers5.We computed two scores for each item (a review):one for positive sentiment, another for negative sen-timent.
The decision about an item?s sentiment po-larity was made every time by finding the biggestscore of the two.For every phrase (a chunk of characters betweenpunctuation marks) a score was calculated as:Scphrase =?
(Scdictionary) +?
(Sccharacter)where Scdictionary is a dictionary based score calcu-lated using following formula:Scdictionary =LdLs?
100where Ld - length of a dictionary item, Ls - length ofa phrase.
The constant value 100 is used to weightthe score, obtained by a series of preliminary testsas a value that most significantly improved the accu-racy.The sentiment scores for characters were obtainedby the formula:Sci = Fi/F(i+j)where Sci is the sentiment score for a character for agiven class i, Fi - the character?s relative frequencyin a class i, F(i+j) - the character?s relative frequencyin both classes i and j taken as one unit.
The relativefrequency of character c is calculated asFc =?
Nc?
N(1...n)where?Nc is a number of the character?s occur-rences in the corpus, and?
N(1...n) is the number ofall characters in the same corpus.Preliminary tests showed that inverting all thecharacters for which Sci ?
1 improves accuracy.The inverting is calculated as follows:Scinverted = Sci ?
1We compute scores rather than probabilities sincewe are combining information from two distinctsources (characters and words).5The corpus is available athttp://www.informatics.sussex.ac.uk/users/tz21/corpSmall.zip.39In addition to the features specified (charactersand dictionary items) we also used a simple negationcheck.
The system checked two most widely usednegations in Chinese: bu and mei.
Every phrase wascompared with the following pattern: negation+ 0-2characters+ phrase.
The scores of all the unigramsin the phrase that matched the pattern were multi-plied by -1.Finally, the score was calculated for an item as thesum of the phrases?
scores modified by the negationcheck:Scitem =?
(Scphrase ?
NegCheck)For sentiment polarity detection the item scoresfor each of the two polarities were compared to eachother: the polarity with bigger score was assigned tothe item.SentimentPolarity = argmax(Sci|Scj)where Sci is an item score for one polarity and Scjis an item score for the other.The main evaluation measure was accuracy ofsentiment identification, expressed in percent.4.1.1 Results of Experiment 1To find out which kinds of features perform bestfor sentiment polarity detection the system was runseveral times with different settings.Running without character scores (with dictionarylongest-match only) gave the following results: al-most 64% of positive and near 65% for negative re-views were detected correctly, which is 64% accu-racy for the whole corpus (note that a baseline clas-sifier tagging all items as positive achieves an accu-racy of 71.1%).
Characters with sentiment scoresalone performed much better on negative reviews(84% accuracy) rather than on positive (65%), butoverall performance was still better: 70%.
Bothmethods combined gave a significant increase onpositive reviews (73%) and no improvement on neg-ative (84%), giving 77% overall.
The last run waswith the dictionary look up, the characters and thenegation check.
The results were: 77% for positiveand 89% for negative, 80% corpus-wide (see Table1).Judging from the results it is possible to suggestthat both the word-based dictionary look up methodMethod Positive Negative AllDictionary 63.7 64.8 64.0Characters 64.8 83.7 70.3Characters+Dictionary 73.6 83.7 76.5Char?s+Dictionary+negation 76.9 89.1 80.4Table 1: Results of Experiment 1 (accuracy in per-cent).and character-based method contributed to the finalresult.
It also corresponds to the results obtained byNie et al (2000) for Chinese information retrieval,where the same combination of features (charactersand words) also performed best.The negation check increased the performance by3% overall, up to 80%.
Although the performancegain is not very high, the computational cost of thisfeature is very low.As we used a non-balanced corpus (71% of thereviews are positive), it is quite difficult to comparethe results with the results obtained by other authors.But the proposed classifier outperformed some stan-dart classifiers on the same data set: a Naive Bayes(multinomial) classifier gained only 49.6 % of ac-curacy (63 items tagged correctly) while a Supportvector machine classifier got 64.5 % of accuracy (82items).64.2 Experiment 2The second experiment included two parts: deter-mining whether texts are opinionated which is a pre-condition for the processing of the OPINION part ofthe query; and tagging found texts with relevant sen-timent for processing a more detailed form of thisquery POSITIVE/NEGATIVE OPINION.For this experiment we used the features thatshowed the best performance as described in section4.1: the dictionary items and the characters with thesentiment scores.The test corpus for this experiment consisted of282 items, where every item is a paragraph.
We usedparagraphs as basic items in this experiment becauseof two reasons: 1. opinionated texts (reviews) areusually quite short (in our corpus all of them are oneparagraph), while texts of other genres are usuallymuch longer; and 2. for IR tasks it is more usual toretrieve units longer then a sentence.6We used WEKA 3.4.10(http://www.cs.waikato.ac.nz/ ml/weka )40The test corpus has following structure: 128 itemsare opinionated, of which 91 are positive and 37 arenegative (all the items are the reviews used in thefirst experiment, see 4.1).
154 items are not opin-ionated, of which 97 are paragraphs taken from ascientific book on Chinese linguistics and 57 itemsare from articles taken form a Chinese on-line ency-clopedia Baidu Baike7.For the first task we used the following tech-nique: every item was assigned a score (a sum of thecharacters?
scores and dictionary scores described in4.1).
The score was divided by the number of char-acters in the item to obtain the average score:averScitem =ScitemLitemwhere Scitem is the item score, and Litem is thelength of an item (number of characters in it).A positive and a negative average score is com-puted for each item.4.2.1 Results of Experiment 2To determine whether an item is opinionated (forOPINION query), the maximum of the two scoreswas compared to a threshold value.
The best perfor-mance was achieved with the threshold value of 1.6- more than 85% of accuracy8 (see Table 2).Next task (NEGATIVE/POSITIVE OPINIONS)was processed by comparing the negative and pos-itive scores for each found item (see Table 2).Query Recall Precision F-measureOPINION 71.8 85.1 77.9POS/NEG OPINION 64.0 75.9 69.4Table 2: Results of Experiment 2 (in percent).Although the unopinionated texts are very dif-ferent from the opinionated ones in terms of genreand topic, the standard classifiers (Naive Bayes(multinomial) and SVM) failed to identify any non-opinionated texts.
The most probable explanationfor this is that there were no items tagged ?unopin-ionated?
in the training corpus (the sentiment dictio-nary) and there were only words and phrases withpredominant sentiment meaning rather then topic-related.7http://baike.baidu.com/8A random choice could have approximately 55% of accu-racy if tagged all items as negative.It is worth noting that we observed the same rela-tion between subjectivity detection and polarity clas-sification accuracy as described by Pang and Lee(2004) and Eriksson (2006).
The accuracy of thesentiment detection of opinionated texts (excludingerroneously detected unopinionated texts) in Exper-iment 2 has increased by 13% for positive reviewsand by 6% for negative reviews (see Table 3).Query Positive NegativeExperiment 1 76.9 89.1Experiment 2 89.9 95.6Table 3: Accuracy of sentiment polarity detection ofopinionated texts (in percent).5 Conclusion and Future WorkThese preliminary experiments showed that usingsingle characters and dictionary items modified bythe negation check can produce reasonable results:about 78% F-measure for sentiment detection (see4.1.1) and almost 70% F-measure for sentimentpolarity identification (see 4.2.1) in the contextof domain-independent opinionated information re-trieval.
However, since the test corpus is very smallthe results obtained need further validation on biggercorpora.The use of the dictionary as a training corpushelped to avoid domain-dependency, however, usinga dictionary as a training corpus makes it impossibleto obtain grammar information by means of analysisof punctuation marks and grammar word frequen-cies.More intensive use of context information couldimprove the accuracy.
The dictionary-based pro-cessing may benefit from the use of word relationsinformation: some words have sentiment informa-tion only when used with others.
For example,a noun dongxi (?a thing?)
does not seem to haveany sentiment information on its own, although itis tagged as ?negative?
in the dictionary.Some manual filtering of the dictionary may im-prove the output.
It might also be promising to testthe influence on performance of the different classesof words in the dictionary, for example, to use onlyadjectives or adjectives and nouns together (exclud-ing adverbials).Another technique to be tested is computing the41positive and negative scores for the characters usedonly in one class, but absent in another.
In the cur-rent system, characters are assigned only one score(for the class they are present in).
It might improveaccuracy if such characters have an appropriate neg-ative score for the other class.Finally, the average sentiment score may be usedfor sentiment scaling.
For example, if in our exper-iments items with a score less than 1.6 were con-sidered not to be opinionated, then ones with scoremore than 1.6 can be put on a scale where higherscores are interpreted as evidence for higher senti-ment intensity (the highest score was 52).
The ?scal-ing?
approach could help to avoid the problem of as-signing documents to more than one sentiment cate-gory as the approach uses a continuous scale ratherthan a predefined number of rigid classes.
The scale(or the scores directly) may be used as a means ofindexing for a search engine comprising OIR func-tionality.ReferencesKushal Dave, Steve Lawrence, and David M. Pennock.2003.
Mining the peanut gallery: Opinion extractionand semantic classification of product reviews.
In Pro-ceedings of the International World Wide Web Con-ference, pages 519 ?
528, Budapest, Hungary.
ACMPress.Koji Eguchi and Victor Lavrenko.
2006.
Sentiment re-trieval using generative models.
In Proceedings ofthe 2006 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2006), pages 345?354,Sydney, July.Brian Eriksson.
2006.
Sentiment classifica-tion of movie reviews using linguistic parsing.http://www.cs.wisc.edu/?apirak/cs/cs838/eriksson final.pdf.Soo-Min Kim and Eduard H. Hovy.
2004.
Determin-ing the sentiment of opinions.
In Proceedings ofCOLING-04, pages 1367?1373, Geneva, Switzerland,August 23-27.Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006a.Opinion extraction, summarization and tracking innews and blog corpora.
In Proceedings of AAAI-2006Spring Symposium on Computational Approaches toAnalyzing Weblogs, volume AAAI Technical Report,pages 100?107, March.Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006b.Tagging heterogeneous evaluation corpora for opin-ionated tasks.
In Proceedings of the Fifth InternationalConference on Language Resources and Evaluation,pages 667?670, Genoa, Italy, May.Wei Li.
2000.
On Chinese parsing without using a sep-arate word segmenter.
Communication of COLIPS,10:17?67.Jian-Yun Nie, Jiangfeng Gao, Jian Zhang, and MingZhou.
2000.
On the use of words and n-gramsfor Chinese information retrieval.
In Proceedings ofthe 5th International Workshop Information Retrievalwith Asian Languages, pages 141?148.
ACM Press,November.Bo Pang and Lillian Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the 42ndAnnual Meeting of the Association for ComputationalLinguistics, pages 271?278, Barcelona, Spain.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification using ma-chine learning techniques.
In Proceedings of the 2002Conference on Empirical Methods in Natural Lan-guage Processing, pages 79?86, University of Penn-sylvania.Peter D. Turney.
2002.
Thumbs up or thumbs down?Semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics (ACL?02), pages 417?424, Philadelphia, Pennsyl-vania.42
