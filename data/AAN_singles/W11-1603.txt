Workshop on Monolingual Text-To-Text Generation, pages 20?26,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 20?26,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsAn Unsupervised Alignment Algorithm for Text Simplification CorpusConstructionStefan BottTALN Research GroupUniversitat Pompeu FabraC/Tanger 122 - Barcelona - 08018Spainstefan.bott@upf.eduHoracio SaggionTALN Research GroupUniversitat Pompeu FabraC/Tanger 122 - Barcelona - 08018Spainhoracio.saggion@upf.eduAbstractWe present a method for the sentence-levelalignment of short simplified text to the orig-inal text from which they were adapted.
Ourgoal is to align a medium-sized corpus of par-allel text, consisting of short news texts inSpanish with their simplified counterpart.
Notraining data is available for this task, so wehave to rely on unsupervised learning.
In con-trast to bilingual sentence alignment, in thistask we can exploit the fact that the probabilityof sentence correspondence can be estimatedfrom lexical similarity between sentences.
Weshow that the algoithm employed performsbetter than a baseline which approaches theproblem with a TF*IDF sentence similaritymetric.
The alignment algorithm is being usedfor the creation of a corpus for the study oftext simplification in the Spanish language.1 IntroductionText simplification is the process of transforming atext into an equivalent which is more understand-able for a target user.
This simplification is bene-ficial for many groups of readers, such as languagelearners, elderly persons and people with other spe-cial reading and comprehension necessities.
Simpli-fied texts are characterized by a simple and directstyle and a smaller vocabulary which substitutes in-frequent and otherwise difficult words (such as longcomposite nouns, technical terms, neologisms andabstract concepts) by simpler corresponding expres-sions.
Usually unnecessary details are omitted.
An-other characteristic trait of simplified texts is thatusually only one main idea is expressed by a singlesentence.
This also means that in the simplificationprocess complex sentences are often split into sev-eral smaller sentences.The availability of a sentence-aligned corpusof original texts and their simplifications is ofparamount importance for the study of simplifica-tion and for developing an automatic text simplifi-cation system.
The different strategies that humaneditors employ to simplify texts are varied and havethe effect that individual parts of the resulting textmay either become shorter or longer than the orig-inal text.
An editor may, for example, delete de-tailed information, making the text shorter.
Or shemay split complex sentences into various smallersentences.
As a result, simplified texts tend to be-come shorter than the source, but often the numberof sentences increases.
Not all of the informationpresented in the original needs to be preserved but ingeneral all of the information in the simplified textstems from the source text.The need to align parallel texts arises from a largerneed to create a medium size corpus which will al-low the study of the editing process of simplifyingtext, as well as to serve as a gold standard to evalu-ate a text simplification system.Sentence alignment for simplified texts is relatedto, but different from, the alignment of bilingual textand also from the alignment of summaries to an orig-inal text.
Since the alignment of simplified sentencesis a case of monolingual alignment the lexical sim-ilarity between two corresponding sentences can betaken as an indicator of correspondence.This paper is organized as follows: Section 2briefly introduces text simplification which contex-20tualises this piece of research and Section 3 dis-cusses some related work.
In Section 4 we brieflydescribe the texts we are working with and in Sec-tion 5 we present the alignment algorithm.
Section 6presents the details of the experiment and its results.Finally, section 7 gives a concluding discussion andan outlook on future work.2 Text SimplificationThe simplification of written documents by humanshas the objective of making texts more accessible topeople with a linguistic handicap, however manualsimplification of written documents is very expen-sive.
If one considers people who cannot read doc-uments with heavy information load or documentsfrom authorities or governmental sources the percentof need for simplification is estimated at around 25%of the population, it is therefore of great importanceto develop methods and tools to tackle this problem.Automatic text simplification, the task of transform-ing a given text into an ?equivalent?
which is lesscomplex in vocabulary and form, aims at reducingthe efforts and costs associated with human simpli-fication.
In addition to transforming texts into theirsimplification for human consumption, text simpli-fication has other advantages since simpler texts canbe processed more efficiently by different naturallanguage processing processors such as parsers andused in applications such as machine translation, in-formation extraction, question answering, and textsummarization.Early attempts to text simplification were basedon rule-based methods where rules were designedfollowing linguistic intuitions (Chandrasekar et al,1996).
Steps in the process included linguistic textanalysis (including parsing) and pattern matchingand transformation steps.
Other computational mod-els of text simplification included processes of anal-ysis, transformation, and phrase re-generation (Sid-dharthan, 2002) also using rule-based techniques.In the PSET project (Carroll et al, 1998) the pro-posal is for a news simplification system for aphasicreaders and particular attention is paid to linguisticphenomena such as passive constructions and coref-erence which are difficult to deal with by peoplewith disabilities.
The PorSimples project (Alu?
?sio etal., 2008) has looked into simplification of the Por-tuguese language.
The methodology consisted in thecreation of a corpus of simplification at two differentlevels and on the use of the corpus to train a deci-sion procedure for simplification based on linguisticfeatures.
Simplification decisions about whether tosimplify a text or sentence have been studied fol-lowing rule-based paradigms (Chandrasekar et al,1996) or trainable systems (Petersen and Ostendorf,2007) where a corpus of texts and their simplifica-tions becomes necessary.
Some resources are avail-able for the English language such as parallel cor-pora created or studied in various projects (Barzilayand Elhadad, 2003; Feng et al, 2009; Petersen andOstendorf, 2007; Quirk et al, 2004); however thereis no parallel Spanish corpus available for researchinto text simplification.
The algorithms to be pre-sented here will be used to create such resource.3 Related WorkThe problem of sentence alignment was first tack-led in the context of statistical machine translation.Gale and Church (1993) proposed a dynamic pro-gramming algorithm for the sentence-level align-ment of translations that exploited two facts: thelength of translated sentences roughly correspondsto the length of the original sentences and the se-quence of sentences in translated text largely corre-sponds to the original order of sentences.
With thissimple approach they reached a high degree of accu-racy.Within the field of monolingual sentence align-ment a large part of the work has concentrated on thealignment between text summaries and the sourcetexts they summarize.
Jing (2002) present an al-gorithm which aligns strings of words to pieces oftext in an original document using a Hidden MarkovModel.
This approach is very specific to summarytexts, concretely such summaries which have beenproduced by a ?cut and paste?
process.
A workwhich is more closely related to our task is pre-sented in Barzilay and Elhadad (2003).
They carriedout an experiment on two different versions of theEncyclopedia Britannica (the regular version andthe Britannica Elementary) and aligned sentencesin a four-step procedure: They clustered paragraphsinto ?topic?
groups, then they trained a binary clas-sifier (aligned or not aligned) for paragraph pairs21on a handcrafted set of sentence alignments.
Af-ter that they grouped all paragraphs of unseen textpairs into the same topic clusters as in the first stepand aligned the texts on the paragraph level, al-lowing for multiple matches.
Finally they alignedthe sentences within the already aligned paragraphs.Their similarity measure, both for paragraphs andsentences, was based on cosine distance of wordoverlap.
Nelken and Shieber (2006) improve overBarzilay and Elhadad?s work: They use the samedata set, but they base their similarity measure foraligning sentences on a TF*IDF score.
Althoughthis score can be obtained without any training, theyapply logistic regression on these scores and traintwo parameters of this regression model on the train-ing data.
Both of these approaches can be tuned byparameter settings, which results in a trade-off be-tween precision and recall.
Barzilay and Elhadadreport a precision of 76.9% when the recall reaches55.8%.
Nelken and Shieber raise this value to 83.1%with the same recall level and show that TF*IDF isa much better sentence similarity measure.
Zhu etal.
(2010) even report a precision of 91.3% (at thesame fixed recall value of 55.8%) for the alignmentof simple English Wikipedia articles to the EnglishWikipedia counterparts using Nelken and Shieber?sTF*IDF score, but their alignment was part of alarger problem setting and they do not discuss fur-ther details.We consider that our task is not directly compara-ble to this previous work: the texts we are workingwith are direct simplifications of the source texts.
Sowe can assume that all information in the simplifiedtext must stem from the original text.
In addition wecan make the simplifying assumption that there areone-to-many, but no many-to-one relations betweensource sentences and simplified sentences, a simpli-fication which largely holds for our corpus.
Thismeans that all target sentences must find at least onealignment to a source sentence, but not vice versa.Nelken and Shieber make the interesting observa-tion that dynamic programming, as used by Galeand Church (1991) fails to work in the monolingualcase.
Their test data consisted of pairs of encyclo-pedia articles which presented a large intersectionof factual information, but which was not necessar-ily presented in the same order.
The corpus we areworking with, however, largely preserves the orderin which information is presented.4 DatasetWe are working with a corpus of 200 news arti-cles in Spanish covering the following topics: Na-tional News, Society, International News and Cul-ture.
Each of the texts is being adapted by the DILESResearch Group from Universidad Auto?noma deMadrid (Anula, 2007).
Original and adapted ex-amples of texts in Spanish can be seen in Figure 1(the texts are adaptations carried out by DILES forRevista ?La Plaza?).
The texts are being processedusing part-of-speech tagging, named entity recogni-tion, and parsing in order to create an automaticallyannotated corpus.
The bi-texts are first aligned us-ing the tools to be described in this paper and thenpost-edited with the help of a bi-text editor providedin the GATE framework (Cunningham et al, 2002).Figure 2 shows the texts in the alignment editor.This tool is however insufficient for our purposessince it does not provide mechanisms for uploadingthe alignments produced outside the GATE frame-work and for producing stand-alone versions of thebi-texts; we have therefore extended the functionali-ties of the tool for the purpose of corpus creation.5 AlgorithmOur algorithm is based on two intuitions about sim-plified texts (as found in our corpus): As repeatedlyobserved sentences in simplified texts use similarwords to those in the original sentences that theystem from (even if some of the words may haveundergone lexical simplification).
The second ob-servation is very specific to our data: the order inwhich information is presented in simplified textsroughly corresponds to the order of the informationin the source text.
So sentences which are close toeach other in simplified texts correspond to originalsentences which are also close to each other in thesource text.
In many cases, two adjacent simplifiedsentences even correspond to one single sentence inthe source text.
This leads us to apply a simple Hid-den Markov Model which allows for a sequentialclassification.Firstly, we define an alignment as a pair of sen-tences as?source senti, target sentj?,22Original Text Adapted TextUn Plan Global desde tu hogarEl Programa GAP (Global Action Plan) es una iniciativaque se desarrolla en distintos pa?
?ses y que pretende dis-minuir las emisiones de CO2, principales causantes delcambio clima?tico y avanzar hacia ha?bitos ma?s sosteniblesen aspectos como el consumo de agua y energ?
?a, lamovilidad o la gestio?n de los residuos dome?sticos.San Sebastia?n de los Reyes se ha adherido a este Pro-grama.Toda la informacio?n disponible para actuar desde elhogar en la construccio?n de un mundo ma?s sostenible sepuede encontrar en ssreyes.org o programagap.es.Un Plan Global desde tu hogarSan Sebastia?n de los Reyes se ha unido al Plan de Accio?nGlobal (GAP).El Plan es una iniciativa para luchar contra el cambioclima?tico desde tu casa.Los objetivos del Plan son:Disminuir nuestros gastos dome?sticos de agua y energ?
?a.Reducir los efectos dan?inos que producimos en el planetacon nuestros residuos.Mejorar la calidad de vida de nuestra ciudad.Tienes ma?s informacio?n en ssreyes.org y en programa-gap.es.Apu?ntate al programa GAP y desca?rgate los manualescon las propuestas para conservar el planeta.Figure 1: Original Full Document and Easy-to-Read VersionFigure 2: The Alignment Editor with Text and Adaptationwhere a target sentence belongs to the simplifiedtext and the source sentence belongs to the originalsentence.
Applying standard Bayesian decomposi-tion, the probability of an alignment to a given targettext can be calculated as follows:P (alignn1 |target sentm1 ) =P (alignn1 )P (target sentm1 |alignn1 )P (target sentm1 )Since P (target sentm1 ) is constant we can calcu-late the most probable alignment sequence a?lign asfollows:a?lign =arg maxP (alignn1 ) P (target sentm1 |alignn1 ) =arg max?
ni=1P (aligni,j)P (target sentj |aligni,j)This leaves us with two measures: a measureof sentence similarity (the probability of alignmentproper) and a measure of consistency, under the as-sumption that a consistent simplified text presentsthe information in the same order as it is presentedin the source text.
In order to determine a?lign, weapply the Viterbi algorithm (Viterbi, 1967).Sentence similarity can be calculated as follows:P (wordl1|target sentj) =?
lk=1P (target sentj |wordk)P (target sentj)P (wordk)where wordl1 is the sequence of words in thesource sentence i and l is the length of sentence i.This similarity measure is different from bothword overlap cosine distance and TF*IDF.
It is,however, similar to TF*IDF in that it penalizes23words which are frequent in the source text andboosts the score for less frequent words.
In addi-tion we eliminated a short list of stopwords from thecalculation, but this has no significant effect on thegeneral performance.Note that P (wordk) may correspond to a MLEof 0 since simplified texts often use different (andsimpler) words and add connectors, conjunctionsand the like.
For this reason we have to recalcu-late P (wordk) according to a distortion probability?.
Distortion is taken here as the process of wordinsertion or lexical changes.
?
is a small constant,which could be determined empirically, but since notraining data is available we estimated ?
for our ex-periment and set it by hand to a value of 0.0075.Even if we had to estimate ?
we found that the per-formance of the system is robust regarding its value:even for unrealistic values like 0.0001 and 0.1 theperformance only drops by two percent points.P (wordk|distortion) =(1 ?
?
)P (wordk) + ?
(1 ?
P (wordk))For the consistency measure we made theMarkov assumption that each alignment aligni,jonly depends on the proceeding alignmentaligni?1,j?
.
We assume that this is the proba-bility of a distance d between the correspondingsentences of source senti?1 and source senti, i.e.P (source senti|aligni?1,j?k) for each possiblejump distance k. Since long jumps are relativelyrare, we used a normalized even probability dis-tribution for all jump lengths greater than 2 andsmaller than -1.Since we have no training data, we have to ini-tially set these probabilities by hand.
We do thisby assuming that all jump distances k in the rangebetween -1 and 2 are distributed evenly and largerjump distances have an accumulated probabilitymass corresponding to one of the local jump dis-tances.
Although this model for sentence transitionsis apparently oversimplistic and gives a very bad es-timate for each P (source senti|aligni?1,j?k), theprobabilities for P (alignn1 ) give a counterweight tothese bad estimates.
What we can expect is, that af-ter running the aligner once, using very unreliabletransitions probability estimates, the output of thealigner is a set of alignments with an implicit align-ment sequence.
Taking this alignment sequence, wecan calculate newmaximum likelihood estimates foreach jump distance P (source senti|aligni?1,j?k)again, and we can expect that these new estimatesare much better than the original ones.For this reason we apply the Viterbi classifier it-eratively: The first iteration employs the hand setvalues.
Then we run the classifier and determinethe values for P (source senti|aligni?1,j?k) on itsoutput.
Then we run the classifier again, with thenew model and so on.
Interestingly values forP (source senti|aligni?1,j?k) emerge after as littleas two iterations.
After the first iteration, precisionalready lies only 1.2 percent points and recall 1.3points below the stable values.
We will comment onthis finding in Section 7.6 Experiment and ResultsOur goal is to align a larger corpus of Spanish shortnews texts with their simplified counterparts.
At themoment, however, we only have a small sample ofthis corpus available.
The size of this corpus sam-ple is 1840 words of simplified text (145 sentences)which correspond to 2456 (110 sentences) of sourcetext.
We manually created a gold standard which in-cludes all the correct alignments between simplifiedand source sentences.
The results of the classifierwere calculated against this gold standard.As a baseline we used a TF*IDF score basedmethod which chooses for each sentence in the sim-plified text the sentence with the minimal word vec-tor distance.
The procedure is as follows: each sen-tence in the original and simplified document is rep-resented in the vector space model using a term vec-tor (Saggion, 2008).
Each term (e.g.
token) is wei-thed using as TF the frequency of the term in thedocument and IDF = log(N +1/Mt +1) where Mtis the number of sentences 1 containing t and N isthe number of sentences in the corpus (counts areobtained from the set of documents to align).
Assimilarity metric between vectors we use the cosineof the angle between the two vectors given in thefollowing formula:1The relevant unit for the calculation of IDF (the D in IDF)here is the sentence, not the document as in information re-trieval.24cosine(s1, s2) =?ni=1 wi,s1 ?
wi,s2?
?ni=1(wi,s1)2 ??
?ni=1(wi,s2)2Here s1 and s2 are the sentence vectors and wi,skis the weight of term i in sentence sk.
We align allsimplified sentences (i.e.
for the time being no cut-off has been used to identify newmaterial in the sim-plified text).For the calculation of the first baseline we calcu-late IDF over the sentences in whole corpus.
Nelkenand Shieber (2006) argue that that the relevant unitfor this calculation should be each document for thefollowing reason: Some words are much more fre-quent in some texts than they are in others.
For ex-ample the word unicorn is relatively infrequent inEnglish and it it may also be infrequent in a givencolletion of texts.
So this word is highly discrimina-tive and it?s IDF will be relatively high.
In a specifictext about imagenary creatures, however, the sameword unicornmay be much more frequent and henceit?s discrimiative power is much lower.
For this rea-son we calcuated a second baseline, where we cal-culate the IDF only on the sentences of the relevancttexts.Results of aligning all sentences in our samplecorpus using both the baseline and the HMM algo-rithms are given in Table 6.precision recallHMM aligner 82.4% 80.9%alignment only 81.13% 79.63%TF*IDF + transitions 76.1% 73.5%TF*IDF (document) 75.47% 74.07%TF*IDF (full corpus) 62.2% 61.1%If we compare these results to those presented byNelken and Shieber (2006), we can observe that weobtain a comparable precision, but the recall im-proves dramatically from 55.8% (with their specificfeature setting) to 82.4%.
Our TF*IDF baselinesare not directly comparable comparable to Nelkenand Shieber?s results.
The reason why we can-not compare our results directly is that Nelken andShieber use supervised learning in order to optimizethe transformation of TF*IDF scores into probabili-ties and we had no training data available.We included the additional scores for our system,when no transition probabilities are included in thecalculation of the optimal alignment sequence andthe score comes only from the probabilies of ourclalculation of lexical similarity between sentences(alignment only).
These scores show that a large partof the good performance comes from lexical similar-ity and sequencial classification only give an addi-tional final boost, a fact which was already observedby Nelken and Shieber.
We also attribute the factthat the system alrives at stable values after two it-erations to the same efect: lexical similarity seemsto have a much bigger effect on the general perfor-mance.
Still our probability-based similarity meas-sure clearly outperforms the TF*IDF baselines.7 Discussion and OutlookWe have argued above that our task is not directlycomparable to Nelken and Shieber?s alignment oftwo versions of Encyclopedia articles.
First of all,the texts we are working with are simplified texts ina much stricter sense: they are the result of an edit-ing process which turns a source text into a simpli-fied version.
This allows us to use sequential classi-fication which is usually not successful for mono-lingual sentence alignment.
This helps especiallyin the case of simplified sentences which have beenlargely re-written with simpler vocabulary.
Thesecases would normally be hard to align correctly.
Al-though it could be argued that the characteristics ofsuch genuinely simplified text makes the alignmenttask somewhat easier, we would like to stress thatthe alignment method we present makes no use ofany kind of training data, in contrast to Barzilay andElhadad (2003) and, to a minor extent, Nelken andShieber (2006).Although we started out from a very specific needto align a corpus with reliably simplified news arti-cles, we are confident that our approach can be ap-plied in other circumstances.
For future work weare planning to apply this algorithm in combina-tion of a version of Barzilay and Elhadad?s macro-alignment and use sequential classification only forthe alignment of sentences within already alignedparagraphs.
This would make our work directlycomparable.
We are also planning to test our algo-rithm, especially the sentence similarity measure ituses, on data which is similar the data Barzilay andElhadad (and also Nelken and Shieber) used in their25experiment.Finally, the alignment tool will be used tosentence-align a medium-sized parallel Spanish cor-pus of news and their adaptations that will be a muchneeded resource for the study of text simplificationand other natural language processing applications.Since the size of the corpus we have available atthe moment is relatively modest, we are also investi-gating alternative resources which could allow us tocreate a larger parallel corpus.AcknowledgmentsWe thank three anonymous reviewers for their com-ments and suggestions which help improve the fi-nal version of this paper.
The research describedin this paper arises from a Spanish research projectcalled Simplext: An automatic system for text sim-plification (http://www.simplext.es).
Sim-plext is led by Technosite and partially funded bythe Ministry of Industry, Tourism and Trade of theGovernment of Spain, by means of the NationalPlan of Scientific Research, Development and Tech-nological Innovation (I+D+i), within strategic Ac-tion of Telecommunications and Information Soci-ety (Avanza Competitiveness, with file number TSI-020302-2010-84).
We thanks the Department of In-formation and Communication Technologies at UPFfor their support.
We are grateful to ProgramaRamo?n y Cajal from Ministerio de Ciencia e Inno-vacio?n, Spain.ReferencesSandra M.
Alu?
?sio, Lucia Specia, Thiago Alexan-dre Salgueiro Pardo, Erick Galani Maziero, and Re-nata Pontin de Mattos Fortes.
2008.
Towards brazil-ian portuguese automatic text simplification systems.In ACM Symposium on Document Engineering, pages240?248.A.
Anula.
2007.
Tipos de textos, complejidad lingu??
?sticay facilicitacio?n lectora.
In Actas del Sexto Congresode Hispanistas de Asia, pages 45?61.Regina Barzilay and Noemi Elhadad.
2003.
Sentencealignment for monolingual comparable corpora.
InIn Proceedings of the 2003 conference on Empiricalmethods in natural language processing, pages 25?32.John Carroll, Guido Minnen, Yvonne Canning, SiobhanDevlin, and John Tait.
1998.
Practical simplificationof english newspaper text to assist aphasic readers.
InIn Proc.
of AAAI-98 Workshop on Integrating ArtificialIntelligence and Assistive Technology, pages 7?10.Raman Chandrasekar, Christine Doran, and BangaloreSrinivas.
1996.
Motivations and methods for text sim-plification.
In COLING, pages 1041?1044.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: A framework and graphi-cal development environment for robust NLP tools andapplications.
In Proceedings of the 40th AnniversaryMeeting of the Association for Computational Linguis-tics.Lijun Feng, Noemie Elhadad, and Matt Huenerfauth.2009.
Cognitively motivated features for readabilityassessment.
In EACL, pages 229?237.William A. Gale and Kenneth W. Church.
1993.
Aprogram for aligning sentences in bilingual corpora.Computational Linguistics.Hongyan Jing.
2002.
Using hidden markov modeling todecompose human-written summaries.
Comput.
Lin-guist., 28:527?543, December.Rani Nelken and Stuart M. Shieber.
2006.
Towards ro-bust context-sensitive sentence alignment for monolin-gual corpora.
In In 11th Conference of the EuropeanChapter of the Association for Computational Linguis-tics.Sarah E. Petersen and Mari Ostendorf.
2007.
Text sim-plification for language learners: a corpus analysis.
InIn Proc.
of Workshop on Speech and Language Tech-nology for Education.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual machine translation for paraphrase gen-eration.
In In Proceedings of the 2004 Conference onEmpirical Methods in Natural Language Processing,pages 142?149.H.
Saggion.
2008.
SUMMA: A Robust and Adapt-able Summarization Tool.
Traitement Automatiquedes Langues, 49(2):103?125.Advaith Siddharthan.
2002.
An architecture for a textsimplification system.
In In LEC 02: Proceedings ofthe Language Engineering Conference (LEC02, pages64?71.A.
Viterbi.
1967.
Error bounds for convolutional codesand an asymptotically optimum decoding algorithm.IEEE Transactions on Information Theory, 13:260?269.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation model forsentence simplification.
In Proceedings of The 23rdInternational Conference on Computational Linguis-tics, pages 1353?1361, Beijing, China, Aug.26
