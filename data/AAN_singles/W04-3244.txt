Learning Nonstructural Distance Metricby Minimum Cluster DistortionsDaichi Mochihashi, Genichiro KikuiATR Spoken Language Translationresearch laboratoriesHikaridai 2-2-2, Keihanna Science CityKyoto 619-0288, Japandaichi.mochihashi@atr.jpgenichiro.kikui@atr.jpKenji KitaCenter for Advanced InformationTechnology, Tokushima UniversityMinamijosanjima 2-1Tokushima 770-8506, Japankita@is.tokushima-u.ac.jpAbstractMuch natural language processing still depends onthe Euclidean (cosine) distance function betweentwo feature vectors, but this has severe problemswith regard to feature weightings and feature cor-relations.
To answer these problems, we propose anoptimal metric distance that can be used as an alter-native to the cosine distance, thus accommodatingthe two problems at the same time.
This metric isoptimal in the sense of global quadratic minimiza-tion, and can be obtained from the clusters in thetraining data in a supervised fashion.We confirmed the effect of the proposed metricdistance by a synonymous sentence retrieval task,document retrieval task and the K-means clusteringof general vectorial data.
The results showed con-stant improvement over the baseline method of Eu-clid and tf.idf, and were especially prominent forthe sentence retrieval task, showing a 33% increasein the 11-point average precision.1 IntroductionNatural language processing involves many kinds oflinguistic expressions, such as sentences, phrases,documents and the collection of documents.
Com-paring these expressions based on semantic proxim-ity is a fundamental task and has many applications.Generally, two basic approaches exist to comparetwo expressions: (a) structural and (b) nonstruc-tural.
Structural approaches make use of syntacticparsing or dependency analysis to make a rigorouscomparison; nonstructural approaches use vectorrepresentation and provide a rough but fast compar-ison that is required for search/retrieval from a vastamount of corpora.
While structural approacheshave recently become available in a kernel-basedsophisticated treatment (Collins and Duffy, 2001;Suzuki et al, 2003), here we concentrate on non-structural comparison.
This is not only because non-structural comparison constitutes an integral partin structural methods (that is, even in hierarchi-cal methods the leaf comparison is still atomic),but because it is frequently embedded in many ap-plications where structural parsings are not avail-able or computationally too expensive.
For exam-ple, information retrieval has long used the ?bagof words?
approach (Baeza-Yates and Ribeiro-Neto,1999; Schu?tze, 1992) mainly due to a lack of scal-able segmentation algorithms and the huge amountof data involved.
While segmentation algorithms,such as TEXTTILING (Hearst, 1994) and its recentsuccessors using the inter-paragraph similarity ma-trix (Choi, 2000), all themselves use nonstructuralcosine similarity as a measure of semantic proxim-ity between paragraphs.However, the distance function so far has beenlargely defined and used ad hoc, usually by a tf.idfweighting scheme (Salton and Yang, 1973) and asimple cosine similarity, equivalently, an Euclideandot product.
In this paper, we propose an optimaldistance function that is parameterized by a globalmetric matrix.
This metric is optimal in the sense ofglobal quadratic minimization, and can be learnedfrom the given clusters in the training data.
Theseclusters are often attributable with many forms, suchas paragraphs, documents or document collections,as long as the items in the training data are not com-pletely independent.This paper is organized as follows.
In section 2we describe the issue of traditional Euclidean dis-tances, and section 3 places it into general perspec-tive with related works in machine learning.
Section4 introduces the proposed metric, and section 5 vali-dates its effect on the task of sentence retrieval, doc-ument retrieval and the K-means clustering.
Sec-tions 6 and 7 present discussions and the conclusion.2 Issues with Euclidean distancesWhen we address nonstructural matching, linguis-tic expressions are often modeled by a feature vec-tor ~x ?
Rn, with its elements x1 .
.
.
xn correspond-ing to the number of occurrences of i?th feature.
Iffeatures are simply words, this is called a ?bag ofwords?
; but in general, features are not restricted tothis kind, and we will use the general term ?feature?in the rest of the paper.To measure the distance between two vectors~u,~v, a dot product or Euclidean distanced(~u,~v)2 = (~u ?
~v)T (~u ?
~v) (1)= ?ni=1(ui ?
vi)2(where T denotes a transposition) has been em-ployed so far 1, with a heuristic feature weightingsuch as tf.idf in a preprocessing stage.However, there are two main problems with thisdistance:(1) The correlation between features is ignored.
(2) Feature weighting is inevitably arbitrary.Problem (1) is especially important in languages,because linguistic features (e.g., words) generallyhave strong correlations between them, such as col-locations or typical constructions.
But this correla-tion cannot be considered in a simple dot product.While it is possible to address this with a specifickernel function, such as polynomials (Mu?ller et al,2001), this is not available for many problems, suchas information retrieval or question answering, thatdo not fit classifications or cannot be easily ?kernel-ized?.
Problem (2) is a more subtle but inherent one:while tf.idf often works properly in practice, thereare several options, especially in tf such as logs orsquare roots, but we have no principle with whichto choose from.
Further, it has no theoretical basisthat gives any optimality as a distance function.3 Related WorksThe issues above of feature correlations and fea-ture weightings can be summarized as a problem ofdefining an appropriate metric in the feature space,based on the distribution of data.
This problem hasrecently been highlighted in the field of machinelearning research.
(Xing et al, 2002) has an ob-jective that is quite similar to that of this paper, andgives a metric matrix that resembles ours based onsample pairs of ?similar points?
as training data.
(Bach and Jordan, 2004) and (Schultz and Joachims,2004) seek to answer the same problem with an ad-ditional scenario of spectral clustering and relativecomparisons in Support Vector Machines, respec-tively.
In this aspect, our work is a straight succes-sor of (Xing et al, 2002) where its general usagein vector space is preserved.
We offer a discussionon the similarity to our method and our advantages1When we normalize the length of the vectors |~u| = |~v| = 1as commonly adopted, (~u ?
~v)T (~u ?
~v) = |~u|2 + |~v|2 ?
2~u ?~v ?
?~u ?
~v = ?
cos(~u,~v) ; therefore, this includes a cosinesimilarity (Manning and Sch u?tze, 1999).in section 6.
Finally, we note that the Fisher ker-nel of (Jaakkola and Haussler, 1999) has the sameconcept that gives an appropriate similarity of twodata through the Fisher information matrix obtainedfrom the empirical distribution of data.
However, itis often approximated by a unit matrix because ofits heavy computational demand.In the field of information retrieval, (Jiang andBerry, 1998) proposes a Riemannian SVD (R-SVD)from the viewpoint of relevance feedback.
Thiswork is close in spirit to our work, but is not aimedat defining a permanent distance function and doesnot utilize cluster structures existent in the trainingdata.4 Defining an Optimal MetricTo solve the problems in section 2, we note the func-tion that synonymous clusters play.
There are manylevels of (more or less) synonymous clusters in lin-guistic data: phrases, sentences, paragraphs, docu-ments, and, in a web environment, the site that con-tains the document.
These kinds of clusters can of-ten be attributed to linguistic expressions becausethey nest in general so that each expression has aparent cluster.Since these clusters are synonymous, we can ex-pect the vectors in each cluster to concentrate in theideal feature space.
Based on this property, we canintroduce an optimal weighting and correlation in asupervised fashion.
We will describe this methodbelow.4.1 The Basic IdeaAs stated above, vectors in the same cluster musthave a small distance between each other in the idealgeometry.
When we measure an L2-distance be-tween ~u and ~v by a Mahalanobis distance param-eterized by M :dM (~u,~v)2 = (~u ?
~v)T M(~u ?
~v) (2)= ?ni=1?nj=1 mij(ui ?
vi)(uj ?
vj),where symmetric metric matrix M gives both cor-responding feature weights and feature correlations.When we take M = I (unit matrix), we recover theoriginal Euclidean distance (1).Equation (2) can be rewritten as (3) because M issymmetric:dM (~u,~v)2 = (M1/2(~u?~v))T (M1/2(~u?~v)).
(3)Therefore, this distance amounts to a Euclidean dis-tance in M 1/2-mapped space (Xing et al, 2002).Note that this distance is global, and differentfrom the ordinary Mahalanobis distance in patternrecognition (for example, (Duda et al, 2000)) that isdefined for each cluster one by one, using a cluster-specific covariance matrix.
That type of distancecannot be generalized to new kinds of data; there-fore, it has been used for local classifications.
Whatwe want is a global distance metric that is generallyuseful, not a measure for classification to predefinedclusters.
In this respect, (Xing et al, 2002) sharesthe same objective as ours.Therefore, we require an optimization over all theclusters in the training data.
Generally, data in theclusters are distributed as in figure 1(a), comprisingellipsoidal forms that have high (co)variances forsome dimensions and low (co)variances for other di-mensions.
Further, the cluster is not usually alignedto the axes of coordinates.
When we find a globalmetric matrix M that minimizes the cluster distor-tions, namely, one that reduces high variances andexpands low variances for the data to make a spher-ical form as good as possible in the M 1/2-mappedspace (figure 1(b)), we can expect it to capture nec-essary and unnecessary variations and correlationson the features, combining information from manyclusters to produce a more reliable metric that is notlocally optimal.
We will find this optimal M below.xnx1x2HighvarianceHighcovarianceLowvariance(a) Original spacex1x2xn(b) Mapped spaceFigure 1: Geometry of feature space.4.2 Global optimization over clustersSuppose that each data (for example, sentences ordocuments) is a vector ~s ?
Rn, and the whole cor-pus can be divided into N clusters, X1 .
.
.
XN .
Thatis, each vector has a dimension n, and the number ofclusters is N .
For each cluster Xi, cluster centroidci is calculated as ~ci = 1/|Xi|?~s?Xi ~s , where |X|denotes the number of data in X .
When necessary,each element in ~sj or ~ci is referenced as sjk or cik(k = 1 .
.
.
n).The basic idea above is formulated as follows.We seek the metric matrix M that minimizes thedistance between each data ~sj and the cluster cen-troid ~ci, dM (~sj ,~ci) for all clusters X1 .
.
.
XN .Mathematically, this is formulated as a quadraticminimization problemM = arg minMN?i=1?~sj?XidM (~sj,~ci)2= arg minMN?i=1?~sj?Xi(~sj ?
~ci)T M(~sj ?
~ci) (4)under a scale constraint (| ?
| means determinant)|M | = 1.
(5)Scale constraint (5) is necessary for excluding adegenerate solution M = O.
1 is an arbitrary con-stant: when we replace 1 by c, c2M becomes a newsolution.
This minimization problem is an exten-sion to the method of MindReader (Ishikawa et al,1998) to multiple clusters, and has a unique solutionbelow.Theorem The matrix that solves the minimizationproblem (4,5) isM = |A|1/nA?1, (6)where A = [akl] is defined byakl =N?i=1?sj?Xi(sjl ?
cil)(sjk ?
cik) .
(7)Proof: See Appendix A.When A is singular, we can use as A?1 a Moore-Penrose matrix pseudoinverse A+.
Generally, Aconsists of linguistic features and is very sparse, andoften singular.
Therefore, A+ is nearly always nec-essary for the above computation.
For details, seeAppendix B.4.3 GeneralizationWhile we assumed through the above constructionthat each cluster is equally important, this is notthe case in general.
For example, clusters with asmall number of data may be considered weak, andin the hierarchical clustering situation, a ?grand-mother?
cluster may be weaker.
If we have con-fidences ?1 .
.
.
?N for the strength of clustering foreach cluster X1 .
.
.
XN , this information can be in-corporated into (4) by a set of normalized clusterweights ?
?i :M = arg minMN?i=1?
?i?~sj?Xi(~sj ?
~ci)T M(~sj ?
~ci),where ?
?i = ?i/?Nj=1 ?j , and we obtain a respec-tively weighted solution in (7).
Further, we note thatwhen N = 1, this metric recovers the ordinary Ma-halanobis distance in pattern recognition.
However,we used equal weights for the experiments belowbecause the number of data in each cluster was ap-proximately equal.5 ExperimentsWe evaluated our metric distance on the three tasksof synonymous sentence retrieval, document re-trieval, and the K-means clustering of general vec-torial data.
After calculating M on the training dataof clusters, we applied it to the test data to see howwell its clusters could be recovered.
As a measure ofcluster recovery, we use 11-point average precisionand R-precision for the distribution of items of thesame cluster in each retrieval result.
Here, R equalsthe cardinality of the cluster; therefore, R-precisionshows the precision of cluster recovery.5.1 Synonymous sentence retrieval5.1.1 Sentence cluster corpusWe used a paraphrasing corpus of travel conversa-tions (Sugaya et al, 2002) for sentence retrieval.This corpus consists of 33,723,164 Japanese trans-lations, each of which corresponds to one of theoriginal English sentences.
By way of this cor-respondence, Japanese sentences are divided into10,610 clusters.
Therefore, each cluster consistsof Japanese sentences that are possible translationsfrom the same English seed sentence that the clus-ter has.
From this corpus, we constructed 10 setsof data.
Each set contains random selection of 200training clusters and 50 test clusters, and each clus-ter contains a maximum of 100 sentences 2.
Ex-periments were conducted on these 10 datasets foreach level of dimensionality reduction (see below)to produce average statistics.5.1.2 Features and dimensionality reductionAs a feature of a sentence, we adopted unigrams ofall words and bigrams of functional words from thepart-of-speech tags, because the sequence of func-tional words is important in the conversational cor-pus.While the lexicon is limited for travel conversa-tions, the number of features exceeds several thou-sand or more.
This may be prohibitive for the calcu-lation of the metric matrix, therefore, we addition-ally compressed the features with SVD, the samemethod used in Latent Semantic Indexing (Deer-wester et al, 1990).5.1.3 Sentence retrieval resultsQualitative result Figure 5 (last page) shows a sam-ple retrieval result.
A sentence with (*) mark atthe end is the correct answer, that is, a sentencefrom the same original cluster as the query.
We cansee that the results with the metric distance contain2When the number of data in the cluster exceeds this limit,100 sentences are randomly sampled.
All sampling are madewithout replacement.less noise than a standard Euclid baseline with tf.idfweighting, achieving a high-precision retrieval.
Al-though the high rate of dimensionality reduction infigure 6 shows degradation due to the dimensioncontamination, the effect of metric distance is stillapparent despite bad conditions.Quantitative result Figure 2 shows the averagedprecision-recall curves of retrieval and figure 3shows 11-point average precisions, for each rateof dimensionality reduction.
Clearly, our methodachieves higher precision than the standard method,and does not degrade much with feature compres-sions unless we reduce the dimension too much, i.e.,to < 5%.00.20.40.60.810.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1PrecisionRecall1%5%10%20%50%(a) Metric distance +idf00.20.40.60.810.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1PrecisionRecall1%5%10%20%50%(b) Euclidean + idfFigure 2: Precision-recall of sentence retrieval.00.10.20.30.40.50.60.70 5 10 15 20 25 30 35 40 45 50PrecisionDimension Reduction(%)Metric distance +idfEuclidean distance +idfFigure 3: 11-point average precision.5.2 Document retrievalAs a method of tackling clusters of texts, the textclassification task has recently made great advanceswith a Na?
?ve Bayes or SVM classifiers (for exam-ple, (Joachims, 1998)).
However, they all aim atclassifying texts into a few predefined clusters, andcannot deal with a document that fits neither of theclusters.
For example, when we regard a website asa cluster of documents, the possible clusters are nu-merous and constantly increasing, which precludesclassificatory approaches.
For these circumstances,document clustering or retrieval will benefit from aglobal distance metric that exploits the multitude ofcluster structures themselves.5.2.1 Newsgroup text datasetFor this purpose, we used the 20-Newsgroup dataset(Lang, 1995).
This is a standard text classificationdataset that has a relatively large number of classes,20.
Among the 20 newsgroups, we selected 16 clus-ters of training data and 4 clusters of test data, andperformed 5-fold cross validation.
The maximumnumber of documents per cluster is 100, and whenit exceeds this limit, we made a random sampling of100 documents as the sentence retrieval experiment.Because our proposed metric is calculated fromthe distribution of vectors in high-dimensional fea-ture space, it becomes inappropriate if the normof the vectors (largely proportional to documentlength) differs much from document to document.3 Therefore, we used subsampling/oversampling toform a median length (130 words) on training docu-ments.
Further, we preprocessed them with tf.idf asa baseline method.5.2.2 ResultsTable 1 shows R-precision and 11-point averageprecision.
Since the test data contains 4 clusters,the baselines of precision are 0.25.
We can see fromboth results that metric distance produces a betterretrieval over the tf.idf and dot product.
However,refinements in precision are certain (average p =0.0243) but subtle.This can be thought of as the effect of the dimen-sionality reduction performed.
We first decomposedata matrix X by SVD: X = USV ?1 and builda k-dimensional compressed representation Xk =VkX; where Vk denotes a k-largest submatrix of V .From the equation (3), this means a Euclidean dis-tance of M 1/2Xk = M1/2VkX .
Therefore, Vk maysubsume the effect of M in a preprocessing stage.Close inspection of table 1 shows this effect as atradeoff between M and Vk.
To make the most ofmetric distance, we should consider metric induc-tion and dimensionality reduction simultaneously,or reconsider the problem in kernel Hilbert space.Dim.
R-precision 11-pt Avr.
Prec.Red.
Metric Euclid Metric Euclid0.5% 0.421 0.399 0.476 0.4551% 0.388 0.368 0.450 0.4302% 0.359 0.343 0.425 0.4093% 0.344 0.330 0.411 0.3994% 0.335 0.323 0.402 0.3925% 0.329 0.318 0.397 0.38810% 0.316 0.307 0.379 0.37620% 0.343 0.297 0.397 0.365Table 1: Newsgroup text retrieval results.3Normalizing documents to unit length effectively mapsthem to a high-dimensional hypersphere; this proved to pro-duce an unsatisfactory result.
Defining metrics that work ona hypersphere like spherical K-means (Dhillon and Modha,2001) requires further research.5.3 K-means clustering and general vectorialdataMetric distance can also be used for clustering orgeneral vectorial data.
Figure 4 shows the K-meansclustering result of applying our metric distance tosome of the UCI Machine Learning datasets (Blakeand Merz, 1998).
K-means clustering was con-ducted 100 times with a random start, where Kequals the known number of classes in the data 4.Clustering precision was measured as an averageprobability that a randomly picked pair of data willconform to the true clustering (Xing et al, 2002).We also conducted the same clustering for doc-uments of the 20-Newsgroup dataset to get a smallincrease in precision like the document retrieval ex-periment in section 5.2.0.60.70.80.911 2 5 10 13DimensionPrecision(a) ?wine?
dataset0.60.70.80.91 2 5 10 15 20DimensionPrecision(b) ?protein?
dataset0.70.80.911 2 3 4DimensionPrecision(c) ?iris?
dataset0.60.70.80.911 2 5 10 20 3 5DimensionPrecision(d) ?soybean?
datasetFigure 4: K-means clustering of UCI MachineLearning dataset results.
The horizontal axis showscompressed dimensions (rightmost is original).
Theright bar shows clustering precision using Metricdistance, and the left bar shows that using Euclideandistance.6 DiscussionIn this paper, we proposed an optimal distance met-ric based on the idea of minimum cluster distortionin training data.
Although vector distances have fre-quently been used in natural language processing,this is a rather neglected but recently highlightedproblem.
Unlike recently proposed methods withspectral methods or SVMs, our method assumes nosuch additional scenarios and can be considered as4Because of the small size of the dataset, we did not applycross-validation as in other experiments.a straight successor to (Xing et al, 2002)?s work.Their work has the same perspective as ours, andthey calculate a metric matrix A that is similar toours based on a set S of vector pairs (~xi, ~xj) that canbe regarded as similar.
They report that the effec-tiveness of A increases as the number of the trainingpairs S increases; this requires O(n2) sample pointsfrom n training data, and must be optimized by acomputationally expensive Newton-Raphson itera-tion.
On the other hand, our method uses only linearalgebra, and can induce an ideal metric using all thetraining data at the same time.
We believe this met-ric can be useful for many vector-based languageprocessing methods that have used cosine similar-ity.There remains some future directions for re-search.
First, as we stated in section 4.3, the effectof a cluster weighted generalized metric must be in-vestigated and optimal weighting must be induced.Second, as noted in section 5.2.1, the dimensional-ity reduction required for linguistic data may con-strain the performance of the metric distance.
Toalleviate this problem, simultaneous dimensionalityreduction and metric induction may be necessary, orthe same idea in a kernel-based approach is worthconsidering.
The latter obviates the problem of di-mensionality, while it restricts the usage to a situa-tion where the kernel-based approach is available.7 ConclusionWe proposed a global metric distance that is use-ful for clustering or retrieval where Euclidean dis-tance has been used.
This distance is optimal in thesense of quadratic minimization over all the clus-ters in the training data.
Experiments on sentenceretrieval, document retrieval and K-means cluster-ing all showed improvements over Euclidean dis-tance, with a significant refinement with tight train-ing clusters in sentence retrieval.AcknowledgementThe research reported here was supported in part bya contract with the National Institute of Informationand Communications Technology entitled ?A studyof speech dialogue translation technology based ona large corpus?.ReferencesFrancis R. Bach and Michael I. Jordan.
2004.Learning Spectral Clustering.
In Advances inNeural Information Processing Systems 16.
MITPress.Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto.
1999.
Modern Information Retrieval.ACM Press / Addison-Wesley.C.
L. Blake and C. J. Merz.
1998.
UCIRepository of machine learning databases.http://www.ics.uci.edu/?mlearn/MLRepository.html.Freddy Y. Y. Choi.
2000.
Advances in domain inde-pendent linear text segmentation.
In Proceedingsof NAACL-00.Michael Collins and Nigel Duffy.
2001.
Convo-lution Kernels for Natural Language.
In NIPS2001.S.
Deerwester, Susan T. Dumais, and George W.Furnas.
1990.
Indexing by Latent SemanticAnalysis.
Journal of the American Society of In-formation Science, 41(6):391?407.Inderjit S. Dhillon and Dharmendra S. Modha.2001.
Concept Decompositions for Large SparseText Data Using Clustering.
Machine Learning,42(1/2):143?175.Richard O. Duda, Peter E. Hart, and David G. Stork.2000.
Pattern Classification *Second Edition.John Wiley & Sons.Marti Hearst.
1994.
Multi-paragraph segmentationof expository text.
In 32nd.
Annual Meeting ofthe Association for Computational Linguistics,pages 9?16.Yoshiharu Ishikawa, Ravishankar Subramanya, andChristos Faloutsos.
1998.
MindReader: Query-ing Databases Through Multiple Examples.
InProc.
24th Int.
Conf.
Very Large Data Bases,pages 218?227.Tommi S. Jaakkola and David Haussler.
1999.
Ex-ploiting generative models in discriminative clas-sifiers.
In Proc.
of the 1998 Conference on Ad-vances in Neural Information Processing Sys-tems, pages 487?493.Eric P. Jiang and Michael W. Berry.
1998.
Infor-mation Filtering Using the Riemannian SVD (R-SVD).
In Proc.
of IRREGULAR ?98, pages 386?395.Thorsten Joachims.
1998.
Text categorization withsupport vector machines: learning with manyrelevant features.
In Proceedings of ECML-98,number 1398, pages 137?142.Ken Lang.
1995.
Newsweeder: Learning to filternetnews.
In Proceedings of the Twelfth Interna-tional Conference on Machine Learning, pages331?339.Christopher D. Manning and Hinrich Schu?tze.1999.
Foundations of Statistical Natural Lan-guage Processing.
MIT Press.K.
R. Mu?ller, S. Mika, G. Ratsch, and K. Tsuda.2001.
An introduction to kernel-based learningalgorithms.
IEEE Neural Networks, 12(2):181?201.G.
Salton and C. S. Yang.
1973.
On the specifica-tion of term values in automatic indexing.
Jour-nal of Documentation, 29:351?372.Matthew Schultz and Thorsten Joachims.
2004.Learning a Distance Metric from Relative Com-parisons.
In Advances in Neural InformationProcessing Systems 16.
MIT Press.Hinrich Schu?tze.
1992.
Dimensions of Mean-ing.
In Proceedings of Supercomputing?92, pages787?796.F.
Sugaya, T. Takezawa, G. Kikui, and S. Ya-mamoto.
2002.
Proposal for a very-large-corpusacquisition method by cell-formed registration.In Proc.
LREC-2002, volume I, pages 326?328.Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, andEisaku Maeda.
2003.
Hierarchical DirectedAcyclic Graph Kernel: Methods for StructuredNatural Language Data.
In Proc.
of the 41th An-nual Meeting of Association for ComputationalLinguistics (ACL2003), pages 32?39.Eric W. Weisstein.
2004.
Moore-Penrose MatrixInverse.
http://mathworld.wolfram.com/Moore-PenroseMatrixInverse.html.Eric P. Xing, Andrew Y. Ng, Michael I. Jordan,and Stuart Russell.
2002.
Distance metric learn-ing, with application to clustering with side-information.
In NIPS 2002.Appendix A.Derivation of the metric matrixHere we prove theorem 1, namely deriving M thatsatisfies the conditionminMn?i=1?~sj?Xi(~sj ?
~ci)T M(~sj ?
~ci) , (8)under the constraint|M | = 1.
(9)Expanding (8), we get?i?~sj[ n?k=1n?l=1(sjk ?
cik)mkl(sjl ?
cil)], (10)and from (9), for all kn?l=1(?1)k+lmkl|Mkl| = 1 .Thereforen?k=1n?l=1(?1)k+lmkl|Mkl| = n, (11)where Mkl denotes an adjugate matrix of mkl.Therefore, we come to minimize (10) under theconstraint (11).By introducing the Lagrange multiplier ?, we de-fineL =N?i=1?~sj[?k?l(sjk ?
cik)mkl(sjl ?
cil)]??
[?k?l(?1)k+lmkl|Mkl| ?
n].Differentiating by mkl and setting to zero, we obtain?L?mkl=?i?~sj(sjk ?
cik)(sjl ?
cil)?
?
(?1)k+l|Mkl| = 0?
|Mkl| =?i?~sj (sjk ?
cik)(sjl ?
cil)?
(?1)k+l .
(12)Let us define M?1 = [m?1kl ].
Then,m?1kl =(?1)k+l|Mkl||M |= (?1)k+l|Mkl| (... (9))=?i?~sj (sjk ?
cik)(sjl ?
cil)?
(13)(... (12))Therefore, when we defineA = [akl] (14)asakl =N?i=1?~sj?Xi(sjl ?
cil)(sjk ?
cik) , (15)from (13),A = ?M?1... |A| = ?n|M?1| = ?n... ?
= |A|1/n ,where A is defined by (14), (15).Appendix B.Moore-Penrose Matrix PseudoinverseThe Moore-Penrose matrix pseudoinverse A+ of Ais a unique matrix that has a property of normal in-verse in that x = A+y is a shortest length leastsquares solution to Ax = y even if A is singular(Weisstein, 2004).A+ can be calculated simply by a MATLABfunction pinv.
Or alternatively (Ishikawa et al,1998), we can decompose A asA = U?UT ,where U is an orthonormal n ?
n matrix and ?
=diag(?1, .
.
.
, ?R, 0, .
.
.
, 0) (R = rank(A)).
Then,A+ is calculated asA+ = U?+UT ,where ?+ = diag(1/?1, .
.
.
, 1/?R, 0, .
.
.
, 0).Therefore,M = (?1?2 ?
?
?
?R)1/RA+.Query: ?
  ?
(?How much is the total??
)Metric distance:distance synonymous sentence0.2712 ffflfiffi *0.3444 !
"#%$%ffi0.3444 &fl'%("fl)*#$ffi0.369 +%, -"fl)*#$ffi0.4377 */.102$ffi *0.4479 */.102$/ffflfiffi *0.4505 3fl4%$%ffi *0.4558 65%78902$ffi *0.4602 65%78902$/ffflfiffi *0.4682 65%7:2/ff%fi;ffi *0.4729 fl20!$ffi *0.4851 fl20!$/ff%fi;ffi *Euclidean distance:distance synonymous sentence0.1732 3fl4%$%ffi *1.781 %<)*#$ffi *1.902 =%>?@flA/fl$ffi1.966 !
"#%$%ffi1.966 &fl'%("fl)*#$ffi1.974 +%, -"fl)*#$ffi1.983 3fl4%<)*#$ffi *2.283 BC/7!DEFfl$ffi2.505 BC/7!GHFfl$ffi2.65 <JI%K%$ffi2.729 L%MflNO%P9Q);RFS12.749 =%>?@flA/fl$T(* denotes the right answers.
)Figure 5: Sentence retrieval example.Query: ?
UFVWYX6Z[]\_^ 9`a_bc-d ?
(?I?d like some fruit for dessert.?
)Metric distance:distance synonymous sentence0.3531 e/fflgh1$ji;5lk/7%fl/ffflfiffi0.3709 mJnojp!qfl*Qrflshtk01$ffi *0.596 e/fflgh1$ji;5lku01vCffi0.6104 w%xh!$yi;5z{kF01$ffi0.621 w%xh!$yi;5z{kF01$ffflfi;ffi0.6255 <1|}%gh!$ji5tk01$ffi0.6295 w%xh!$yi;5z{kF0!v/C%/ffflfiffi0.6343 <1|}%gh!$ji5tk0!v/C%ffi0.6685 w%xh!$yi;5z{k7fl%fl$ffi0.7966 mJnojp{5%"~rs]h]{k/7%flfl$ffi *Euclidean distance:distance synonymous sentence1.036 e/fflgh1$ji;5lk/7%fl/ffflfiffi1.421 z?
"JCh4?59?uC?*?7%flffflfiffi1.491 ?J?2?F?flo/h!?#K#?
?{k/7%flffflfiffi1.499 ?J?2?F?flo/h!??
)tk7fl%/ff%fi;ffi1.535 ?htk7flflffflfiffi1.622 %?h4?-56?Cfl?*?7flfl/ff#fiffi1.622 %?h4?-56?Cfl?*?7flfl/ff#fiffi: :2.787 mJnojp!qfl*Q?ffi2rshtk/7%flffflfiffi *2.854 ?2?/?h?-??{59?%?
?;RFSF97%flffflfiffiFigure 6: High rate of dimensionality reduction.
