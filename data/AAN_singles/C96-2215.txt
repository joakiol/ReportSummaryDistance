Computat ional  Complexity of Probabil istic Disambiguationby means of Tree-GrammarsKhali l  Sima'an*Research Institute for Language and Speech,Utrecht University, Trans 10, 3512 JK Utrecht, The Netherlands,Email: khalil.simaan(@let.ruu.nl.AbstractThis paper studies the computationalcomplexity of disambiguation underprobabilistic tree-grammars a in (Bod,1992; Schabes and Waters, 1993).
Itpresents a proof that the following prob-lems are NP-hard: computing the MostProbable Parse from a sentence or froma word-graph, and computing the MostProbable Sentence (MPS) from a word-graph.
The NP-hardness of comput-ing the MPS from a word-graph alsoholds for Stochastic Context-Free Gram-mars (SCFGs).1 Mot ivat ionStatistical disambiguation is currently a pop-ular technique in parsing Natural Language.Among the models that implement statisticaldisambiguation one finds the models that em-ploy Tree-Grammars uch as Data OrientedParsing (DOP) (Scha, 1990; nod, 1992) andStochastic (Lexicalized) Tree-Adjoining Grammar(STAG) (Schabes and Waters, 1993).
These mod-els extend the domain of locality for expressingconstraints from simple Context-Free Grammar(CFG) productions to deeper structures calledelementary-trees.
Due to this extension, the oneto one mapping between a derivation and a parse-tree, which holds in CFGs, does not hold anymore; many derivations might generate the sameparse-tree, rl'his seemingly spurious ambiguityturns out crucial for statistical disambiguation asdefined in (Bod, 1992) and in (Schabes and Wa-ters, 1993), where the derivations are considereddifferent stochastic processes and their probabili-ties all contribute to the probability of the gener-ated parse.
Therefore the Most Probable Deriva-tion (MPD)  does not necessarily generate the*Special thanks to Christer Samuelsson whopointed out and helped in solving a problem with aprevious version.
Thanks to Remko Scha, Rens Bodand Eric Aarts for valuable comments, and t~o StevenKrauwer and the STT for the support.Most Probable Parse (MPP) .The problem of computing the MPP in theDOP framework was put forward in (Bod, 1995).The solution which Bod proposes is Monte-Carloestimation (Bod, 11993), which is essentially re-peated random-sampling for minimizing error-rate.
A Viterbi-style optimization for computingthe MPP under I)OP is presented in (Sima'an etal., 1994), but it does not guarantee determin-istic polynomial-time complexity.
In this paperwe present a proof that computing the MPP un-der the above mentioned stochastic tree gram-mars is NP-hard.
Note that for computing theMPD there are deterministic polynomial-time al-gorithms (Schabes and Waters, 1993; Sima'an,1996) 1.
Another problem that turns out also NP-hard is computing the Most Probable Sentence(MPS)  from a given word-graph.
But this prob-lem turns out NP-hard even for SCFGs.Beside the mathematical  interest, this work isdriven by the desire to develop efficient algorithmsfor these problems.
Such algorithms can be useflllfor various applications that demand robust andfaithful disambiguation e.g.
Speech Recognition,information Retrieval.
'\['his proof provides an ex-planation for the source of complexity: and formsa license to redirect the research for solutions to-wards non-standard optimizations.The structure of the paper is as follows.
Sec-tion 2 briefly discusses the preliminaries.
Section 3presents the proofs.
Section 4 discusses this result,points to the source of complexity and suggestssome possible solutions.
The presentation is for-real only where it, seemed necessary.2 Pre l iminar ies2.1 S tochast i c  T ree-Subst l tu t ionGrammar  (STSG)STSGs and SCFGs are closely related.
STSGsand SCFGs are equal in weak generative ca-i The author notes that the actual accuracy figuresof the experiments li ted in (Sima'an, 1995) are muchhigher than the accuracy figures reported in the paper.The lower figures reported in that paper are due to atest-procedure.1175pacity (i.e.
string languages).
This is not thecase for strong generative capacity (i.e.
tree lan-guages); STSGs can generate tree-languages thatare not generatable by SCFGs.
An  STSGis a five-tuple (VN, VT, S, d, PT), where VN andVT denote respectively the finite set of non-terminal and terminal symbols, S denotes thestart non-terminal, C is a finite set of elementary-trees (of arbitrary depth > 1) and PT is a functionwhich assigns a value 0 < PT(t) < 1 (proba-bility) to each elementary-tree t such that for allN EVN: Y\].tee, root(tl=N PT(t) = 1 (where root(t)denotes the root of tree t).
An e lementary - t reein C has only non-terminals as internal nodes butmay have both terminals and non-terminals on itsfrontier.
A non-terminal on the frontier is calledan Open-Tree  (OT).
If the left-most open-treeN of tree t is equal to the root of tree t l  thent o t l  denotes the tree obtained by substituting t lfor N in t. The partial function o is called left-most  subst i tu t ion .
A le f t -most  der ivat ion(1.m.d.)
is a sequence of left-most substitutionslmd = ( .
.
.
( t lOt2)o .
.
.
)o t , , ,  wheret l , .
.
.
, t ,~ E d ,root(tl) = S and the frontier of lmd consists ofonly terminals.
The probability P(Imd) is de-fined as PT(tl) x .
.
.
x  PT(t~).
~'or convenience,derivation in the sequel refers to 1.m.
derivation.A Parse  is a tree generated by a derivation.
Aparse is possibly generatable by many derivations.The probability of a parse is defined as the sum ofthe probabilities of the derivations that generateit.
The probability of a sentence is the sum of theprobabilities of all derivations that generate thatsentence.A word-graph over the alphabet Q is Q1 x?
.
.x  Qm, whereQiC Q, fo ra l l l  < i<_ m. Wedenote this word-graph with Qm if-Qi = Q, fora l l l<  i<  m.2.2 The  3SAT prob lemIt is sufficient o prove that a problem is NP-hardin order to prove that it is intractable.
A problemis NP-hard if it is (at least) as hard as any problemthat has been proved to be NP-complete (i.e.
aproblem that is known to be decidable on a non-deterministic Taring Machine in polynomial-timebut not known to be decidable on a deterministicTuring Machine in polynomial-time).
To provethat problem A is as hard as problem B, one showsa reduction from problem B to problem A. Thereduction must be a deterministic polynomial timetransformation that preserves answers.The NP-complete problem which forms ourstarting-point is the 3SAT (satisfiability) problem.An instance INS of 3SAT can be stated as follows~:Given an arbitrary a Boolean formula in3-conjunctive normal form (3CNF) over:In the sequel, INS, INS's formula and its symbolsrefer to this particular instance of 3SAT.3Without loss of generality we assume that the for-the variables u l , .
.
.
,  un.
Is there an as-signment of values t rue  or false to theBoolean variables such that the givenformula is true ?
Let us denote the givenformula by C1 A C2 A. ?
?
ACm for ra > 1where 6'/ represents (d?1 V dis V dis),for 1 < i < m, 1 < j _< 3, anddij represents a literal uk or ~k for some1< k< n.Optimization problems are known to be(at least) as hard as their decision counter-parts (Garey and Johnson, 1981).
The deci-sion problem related to maximizing a quantity Mwhich is a function of a variable V can be statedas follows: is there a value for V that makes thequantity M greater than or equal to a predeter-mined value m. The decision problems related todisambiguation under DOP can be stated as fol-lows, where G is an STSG, WG is a word-graph,w~isasentence  and0 < p < 1:MPPWG Does the word-graph WG have anyparse, generatable by the STSG G, that hasprobability value greater than or equal to p ?MPS Does the word-graph WG contain any sen-tence, generatable by the STSG G, that hasprobability value greater than or equal to p ?MPP  Does the sentence w~ have a parse gener-atable by the STSG G, that has probabilityvalue greater than or equal to p ?Note that in the sequel MPPWG / MPS / MPPdenotes the decision problem corresponding to theproblem of computing the MPP / MPS / MPPfrom a word-graph / word-graph / sentence re-spectively.3 Complexity of MPPWG,  MPSand MPP3.1 3SAT to MPPWG and MPSThe reduction from the 3SAT instance INS toan MPPWG problem must construct an STSGand a word-graph in deterministic polynomial-time.
Moreover, the answers to the MPPWGinstance must correspond exactly to the an-swers to INS.
The presentation of the reduc-tion shall be accompanied by an example of thefollowing 3SAT instance (Barton et al, 1987):(ul V E2 V ua) A (~1 V l/,2 V U3).
Note that a 3SATinstance is satisfiable iff at least one of the liter-als in each conjunct is assigned the value True.Implicit in this, but crucial, the different occur-rences of the literals of the same variable must beassigned values consistently.Reduct ion :  The reduction constructs an STSGand a word-graph.
The STSG has start-symbolS, two terminals represented by T and F , non-terminals which include (beside S) all Ck, formula does not contain repetition of conjuncts.11762/13 S 2/13S?
, C! '
f?-.Ul 132 tl3 ii1 112 113 n l  112 ll3 )ll 112 U3I , I /T F F T1/3i ' l  ii2 u3W2/13 2/13S St:2 Cl (:2_/7"--._F T T F2/13 2/13 s sI/3C1u I ~ 2 u 3/T113u I ii2 i 13/W/   C2 CI C2 _iT'--_ '3 3T F F T1/21 /21 /21 /2  1 /21 /21 /2  1/2 1/2 \ [ /21/2  I/21 I 112 u3 il3 ~i3 ~3i' i" r i" f i | / /T F F F F T F' F T T T T1/3j@-.._i l I u 2 u 3W1/3Ul u2 u3LW1/3Ul u2 ~3/T1/13Sc IFigure 1: The elementary-trees for the example 3SAT instance1 < k < rn, and both literals of each Boolean vari-able of the formula of INS.
The set of elementary-trees and probabil ity function and the word-graphare constructed as follows:1.
For each Boolean variable ui, 1 < i < n,construct two elementary-trees that corre-spond to assigning the values true and falseto ui consistently through the whole formula.Each of these elementary-trees has root S,with children Ck, 1 5 k < rn, in thesame order as these appear in the formulaof INS; subsequently the children of Ck arethe non-terminals that correspond to its threedisjuncts dkl, dk2 and dk3.
And finally, theassignment of true (false) to ui is modeled bycreating a child terminal T (resp.
F ) to eachnon-terminal ui and P (resp.
T ) to eachul.
The two elementary-trees for u~, of ourexample, are shown in the top left corner offigure 1.2.
The reduction constructs three elementary-trees for each conjunct Ck.
The threeelementary-trees for conjunct Ck have thesame internal structure: root Ck, withthree children that correspond to the dis-juncts dkl, dk2 and dk3 In each of these3.4..elementary-trees xactly one of the disjunctshas as a child the terminal T ; in each ofthem this is a different one.
Each of theseelementary-trees corresponds to the conjunct;where one of the three possible literals is as-signed the value T .
For the elementary-treesof our example see the top right corner of fig-ure l .The reduction constructs for each of tile twoliterals of each variable ni two elementary-trees where the literal is assigned in one caseT and in the other F .
Figure 1 shows theseelementary-trees for variable ul in the bottomleft corner.The reduction constructs one elementary-tree that has root S with children Ck,1 < k < rn, in the same order as theseappear in the formula of INS (see the bottomright corner of figure 1).The probabilities of the elementary-trees thathave the same root non-terminal sum up to 1.The probabil ity of an elementary-tree withroot S that was constructed in step 1 of thisreduction is a value Pi, 1 _< i < n, whereui is the only variable of which the literalsin the elementary-tree at hand are lexical-1177ized (i.e.
have terminal children).
Let ni de-note the number of occurrences of both liter-als of variable ui in the formula of INS.
ThenPi = 0 (?
)ni for some real 0 that has to fulfillsome conditions which will be derived next.The probability of the tree rooted with S andconstructed at step 4 of this reduction mustthen bep0 = \[1 - 2~i= lp l \ ] .
The proba-bility of the elementary-trees of root Ck (step2) is (1), and of root ui or ul (step 3) is (?
).For our example some suitable probabilitiesare shown in figure 1.6.
Let Q denote a threshold probability thatshall be derived hereunder.
The MPPWG(MPS) instance is: does the STSG generatea parse (resp.
sentence) of probability > Q,for the word-graph WG = {T, F} 3m ?Der iv ing  the  probab i l i t i es :  The parses gen-erated by the constructed STSG differ only in thesentences on their frontiers.
Therefore, if a sen-tence is generated by this STSG then it has ex-actly one parse.
This justifies the choice to reduce3SAT to MPPWG and MPS simultaneously.One can recognize two types of derivations inthis STSG.
The f irst  type corresponds to substi-tuting for an open-tree (i.e literal) of any of the2n elementary-trees constructed in step 1 of thereduction.
This type of derivation corresponds toassigning values to all literals of some variable uiin a consistent manner.
For all 1 < i < n the prob-ability of a derivation of this type is13m npi( ) - ' =The second type of derivation corresponds tosubstituting the elementary-trees rooted with Ckin S -+ C1 .
.
.
C,~, and subsequently substitutingin the open-trees that correspond to literals.
Thistype of derivation corresponds to assigning to atleast one literal in each conjunct the value true.The probability of any such derivation is12ml  m " i P0(~) (~) : \[1-20~_~_,(~)n'\](~)~"~(~) TMi= lNow we derive both the threshold Q and theparameter 0.
Any parse (or sentence) that ful-fills both the "consistency of assignment" require-ments and the requirement that each conjunct hasat least one literal with child T , must be gen-erated by n derivations of the first type and atleast one derivation of the second type.
Note thata parse can never be generated by more than nderivations of the first type.
Thus the thresholdq is:nq = + \[ -2oi=IHowever, 0 must fulfill some requirements for ourreduction to be acceptable:1.
For all i: 0<p i  < 1.
This means that for1 < i <_ n: 0 < 0(?)
'~' < 1, and0 < P0 < 1.
However, the last requirement onP0 implies that 0 < 20V"~z--,i=l~2/(!~'~ < 1,which is a stronger requirement thanthe other n requirements.
This re-quirement can also be stated as follows:1 0 < 0 < , .2.
Since we want to be able to know whether aparse is generated by a second type deriva-tion only by looking at the probability of theparse, the probability of a second type deriva-tion must be distinguishable from first typederivations.
Moreover, if a parse is generatedby more than one derivation of the secondtype, we do not want the sum of the prob-abilities of these derivations to be mistakenfor one (or more) first type derivation(s).For any parse, there are at most 3 TM secondtype derivations (e.g.
the sentence T .
.
.T  ).Therefore we require that:i :1Which is equal to 0 > 2~=,(~?__3.
For the resulting STSG to be a probabilis-tic model, the "probabilities" of parses andsentences must be in the interval (0, 1\].
Thisis taken care of by demanding that the sumof the probabilities of elementary-trees thathave the same root non-terminal is 1, andby the definition of the derivation's probabil-ity, the parse's probability, and the sentence'sprobability.There exists a 0 that fulfills all these requirements1 is because the lower bound 2}--2~=1(~)~   + (?
)~always larger than zero and is strictly smaller than1 the upper bound 2~i_~(~)Po lynomla l i ty  of  the  reduct ion :  This reduc-tion is deterministic polynomial-time in n becauseit constructs not more than 2n + 1 + 3m + 4nelementary-trees of maximum number of nodes 47m+ 1.The  reduct ion  preserves  answers :  Theproof concerns the only two possible answers.Yes If INS's answer is Yes then there is an as-signment o the variables that is consistentand where each conjunct has at least one lit-eral assigned true.
Any possible assignmentis represented by one sentence in WG.
Asentence which corresponds to a "successful"assigmnent must be generated by n deriva-tions of the first type and at least one deriva-tion of the second type; this is because the4Note than m is polynomial in n because the for-mula does not contain two identical conjuncts.1178sentence w 3m fulfills n consistency require-ments (one per Boolean variable) and has atleast one W as Wak+l, Wak+2 or  W3k+3 , for all0 < k < m. Both this sentence and itscorresponding parse have probability > Q.Thus MPPWG and MPS also answer Yes.No If INS's answer is No, then all possible assign-mcnts are either not consistent or result in atleast one conjunct with three false disjuncts,or both.
The sentences (parses) that cor-respond to non-consistent assignments eachhave a probability that cannot result in a Yesanswer.
This is the case because such sen-tences have fewer than n derivations of thefirst type, and the derivations of the secondtype can never compensate for that (the re-quirements on 0 take care of this).
For thesentences (parses) that correspond to con-sistent assignments, there is at least some0 < k < m such that wak+l , wak+2 andWak+3 are all F .
These sentences do not havesecond type derivations.
Thus, there is nosentence (parse) that has a probability thatcan result in a Yes answer; the answer of MP-PWG and MPS is NO.We conclude that MPPWG and MPS are bothNP-hard problems.Now we show that MPPWG and MPS are inNP.
A problem is in NP if it is decidable bya non-deterministic Turing machine.
The proofhere is informah we show a non-deterministic al-gorithm that keeps proposing solutions and thenchecking each of them in deterministic polyno-mial time cf.
(Barton et al, 1987).
If one solu-tion is successful then the answer is Yes.
Onepossible non-deterministic algorithm for the MP-PWG and MPS, constructs firstly a parse-forestfor WG in deterministic polynomial time basedon the algorithms in (Schabes and Waters, 1993;Sima'an, 1996), and subsequently traverses thisparse-forest (bottom-up for example) deciding ateach point what path to take.
Upon reachingthe start non-terminal S, it retrieves the sen-tence (parse) and evaluates it in deterministicpolynomial-time (Sima'an et al, 1994), therebyanswering the decision problem.This concludes the proof that MPPWG andMPS are both NP-complete.3.2 NP-eompletetless of MPPThe NP-completeness of MPP can be easily de-duced from the previous ection.
In the reductionthe terminals of the constructed STSG are newsymbols vii, 1 < i < m and 1 < j < 3,instead of T and F that becomc non-terminals.Each of the elementary-trees with root S or Ckis also represented here but  each T and F onthe frontier has a child vkj wherever the T orF appears as the child of the jth child (a lit-eral) of Ck.
For each elementary-tree with rootui or ui, there are 3m elementary-trees in tile newSTSG that correspond each to creating a child~)ij for the T or F on its frontier.
The proba-bility of an elementary-tree rooted by a literal is1 The probabilities of elementary-trees rooted gm"with Ck do not change.
And the probabilities ofthe elementary-trees rooted with S are adaptedfrom the previous reduction by substituting for1 every (?)
the value ~.
The threshold Q and therequirements on 0 are also updated accordingly.The input sentence which the reduction constructsis simply v11.., v3,~.
The decision problem iswhether there is a parse generated by the resultingSTSG for this sentence that has probability largerthan or equal to Q.The rest of the proof is very similar to that insection 3.
Therefore the decision problem MPP isNP-complete.3.3 MPS under  SCFGThe decision problem MPS is NP-complete alsounder SCFG.
The proof is easily deducible fromthe proof concerning MPS for STSGs.
The reduc-tion simply takes the elementary-trees of the MPSfor STSGs and removes their internal structure,thereby obtaining simple CFG productions.
Cru-cially, each elementary-tree r sults in one uniqueCI"G production.
The probabilities are kept thesame.
The word-graph is also the same word-graph as in MPS for STSGs.
The problem is:does the SCFG generate a sentence with probabil-ity _> Q, for the word-graph W G -- {T, F} 3m.The rest of the proof follows directly from sec-tion 3.4 Conclus ion and discussionWe conclude that computing the MI)P / MPS /MPP from a sentence / word-graph / word-graphrespectively is NP-hard under DOP.
Computingthe MPS from a word-graph is NP-hard even un-der SCI,'Gs.
Moreover, these results are applicableto STAG as in (Schabes and Waters, 1993).The proof of the previous section helps in un-derstanding why computing tt, e MPP in DOP issuch a hard problem.
The fact that MPS underSCFG is also NP-hard implies that the complex-ity of the MPPWG, MPS and MPP is due to thedefinitions of the probabilistic model rather thanthe complexity of tile syntactic model.The main source of NP-completeness is the fol-lowing common structure of these problems: theyall search for an entity that maximizes the sumof the probabilities of processes which depend onthat entity.
For the MPS problem of SCFGs forexample, one searches for the sentence which max-imizes the sum of the probabilities of the parsesthat generate that sentence (i.e.
the probabilityof a parse is also a function of whether it gener-ates the sentence at: hand or not).
This is not the1179case, for example, when computing the MPD un-der STSGs (for sentence or even a word-graph),or when computing the MPP under SCFGs (for asentence or a word-graph).The proof in this paper is not a mere theoreticalissue.
An exponential algorithm can be compara-ble to a deterministic polynomial algorithm if thegrammar-size can be neglected and if the expo-nential formula is not much worse than the poly-nomial for realistic sentence lengths.
But as soonas the grammar size becomes an important factor(e.g.
in DOP), polynomiality becomes a very de-sirable quality.
For example tGI e ~ and IGI n a forn < 7 are comparable but for n = 12 the poly-nomial is some 94 times faster.
If the grammarsize is small and the comparison is between 0.001seconds and 0.1 seconds this might be of no prac-tical importance.
But when the grammar size islarge and the comparison is between 60 seconds sand 5640 seconds for a sentence of length 12, thenthings become different.To compute the MPP under DOP, one possi-ble solution involves some heuristic that directsthe search towards the MPP; a form of this strat-egy is the Monte-Carlo technique.
Another so-lution might involve assuming Memory-based be-havior in directing the search towards the most"suitable" parse according to some heuristic eval-uation function that is inferred from the proba-bilistic model.
And a third possible solution is toadjust the probabilities of elementary-trees suchthat it is not necessary to compute the MPP.
Theprobability of an elementary-tree can be redefinedas the sum of the probabilities of all derivationsthat generate it in the given STSG.
This redefi-nition can be applied by off-line computation andnormalization.
Then the probability of a parse isredefined as the probability of the MPD that gen-erates it, thereby collapsing the MPP and MPD.This method assumes full independence beyondthe borders of elementary-trees, which might bean acceptable assumption.Finally, it is worth noting that the solutionsthat we suggested above are merely algorithmic.But the ultimate solution to the complexity ofprobabilistic disambiguation under the currentmodels lies, we believe, only in further incorpo-ration of the crucial elements of the human pro-cessing ability into these models.Re ferencesG.
Edward Barton, Robert Berwick, andEric Sven Ristad.
1987.
Computational Com-plexity and Natural Language.
A BradfordBook, The MIT Press.SThis is a realistic figure from experiments on theATIS.Rens Bod.
1992.
A computational model of lan-guage performance: Data Oriented Parsing.
InProceedings COLING'g2, Nantes.Rens Bod.
1993.
Monte Carlo Parsing.
InProceedings Third International Workshop onParsing Technologies, Tilburg/Durbuy.Rens Bod.
1995.
The Problem of Computing theMost Probable Tree in Data-Oriented Parsingand Stochastic Tree Grammars.
In ProceedingsSeventh Conference of The European Chapterof the A CL, Dublin, March.Michael Garey and David Johnson.
1981.
Com-puters and Intractability.
San Fransisco: W.H.Freeman and Co.John Hopcroft and Jeffrey Ullman.
1979.
Intro-duction to Automata Theory, Lanaguges, andComputation.
Reading, MA: Addison Wesley.Aravind Joshi and Yves Schabes.
1991.
Tree-Adjoining Grammars and Lexicalized Gram-mars.
In M. Nivat and A. Podelski, editors,Tree Automata nd Languages.
Elsevier SciencePublishers.Harry Lewis and Christos Papadimitriou.
1981.Elements of the Theory of Computation.Englewood-Cliffs, N.J., Prentice-Hall.Philip Resnik.
1992.
Probabilistic Tree-AdjoiningGrammar  as a Framework for Statistical Natu-ral Language Processing.
In Proceedings COL-ING'92, Nantes.Arto Salomaa.
1969.
Probabilistic and WeightedGrammars.
Inf.
Control, 15:529-544.Rcmko Scha.
1990.
Language Theory and Lan-guage Technology; Competence and Perfor-mance (in Dutch).
In Q.A.M.
de Kort andG.L.J.
Leerdam, editors, Computertoepassingenin de Neerlandistiek, Almere: LVVN-jaarboek.Yves Schabes and Richard Waters.
1993.
Stochas-tic Lexicalized Context-Free Grammar.
In Pro-ceedings Third IWPT, Tilburg/Durbuy.Khali\] Sima'an, Rens Bod, Steven Krauwer, andRemko Scha.
1994.
Efficient Disambiguation bymeans of Stochastic Tree Substitution Gram-mars.
In Proceedings International Conferenceon New Methods in Language Processing.
CCL,UMIST, Manchester.Khalil Sima'an.
1995.
An optimized algorithm forData Oriented Parsing.
In Proceedings RANLP,Tzigov Chark, Bulgaria.Khalil Sima'an.
1996.
An optimized algorithmfor Data Oriented Parsing.
In R. Mitkov andN.
Nicolov, editors, Recent Advances in Nat-ural Language Processing 1995, volume 136 ofCurrent Issues in Linguistic Theory.
John Ben-jamins, Amsterdam.1180
