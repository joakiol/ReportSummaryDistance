Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 262?267,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsSynthetic Word Parsing Improves Chinese Word SegmentationFei Cheng Kevin Duh Yuji MatsumotoGraduate School of Information ScienceNara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara, 630-0192, Japan{fei-c,kevinduh,matsu}@is.naist.jpAbstractWe present a novel solution to improvethe performance of Chinese word seg-mentation (CWS) using a synthetic wordparser.
The parser analyses the inter-nal structure of words, and attempts toconvert out-of-vocabulary words (OOVs)into in-vocabulary fine-grained sub-words.We propose a pipeline CWS system thatfirst predicts this fine-grained segmenta-tion, then chunks the output to recon-struct the original word segmentation stan-dard.
We achieve competitive results onthe PKU and MSR datasets, with substan-tial improvements in OOV recall.1 IntroductionSince Chinese has no spaces between words to in-dicate word boundaries, Chinese word segmenta-tion is a task to determine word boundaries be-tween characters.
In recent years, research in Chi-nese word segmentation has progressed signifi-cantly, with state-of-the-art performing at around96% in precision and recall (Xue, 2003; Zhangand Clark, 2007; Li and Sun, 2009).However, frequent OOVs are still a crucial issuethat causes low accuracy in word segmentation.Li and Zhou (2012) defined those words that areOOVs but consisting of frequent internal parts aspseudo-OOV words and estimated that over 60%of OOVs are pseudo-OOVs in five common Chi-nese corpora.
For instance, PKU corpus does notcontain the word ???
(exhibition room), eventhough the word??
(exhibit) and?
(room) ap-pear hundreds of times.
Goh et al (2006) alsoclaimed that most OOVs are proper nouns takingthe form of Chinese synthetic words.These previous works suggest that by analysingthe internal structure of the synthetic words, wecan transform pseudo-OOVs into in-vocabularywords (IVs).
By running a synthetic word parseron each of the words in a CWS training set, we cangenerate a fine-grained segmentation standard thatcontains more IVs.
Since the current conditionalrandom field (CRF) word segmenters (Tseng et al,2005; Sun and Xu, 2011) perform well on IVs, thistransforming process can conceivably improve thehandling of pseudo-OOV words, as long as we canrecover the original word segmentation standardfrom the fine-grained sub-word segmentation.In recent years, some related works about im-proving OOV problem in CWS have been ongo-ing.
Sun et al (2012) presented a joint model forChinese word segmentation and OOVs detection.Their models achieved fast training speed, high ac-curacies and increase on OOV recall.
Sun (2011)proposed a similar sub-word structure which isgenerated by merging the segmentations providedby different segmenters (a word-based segmenter,a character-based segmenter and a local characterclassifier).
However, her models does not predictthe sub-words of all the synthetic words, but thosewords with different segmented results of the threesegmenters.
Her work maximizes the agreementof different models to improve CWS performance.Different from her work, we aim to provide an uni-fied way to incorporate morphological informationof the synthetic words into the CWS task.In this paper, we propose a pipeline word seg-mentation system to address the pseudo-OOVproblem.
Our word segmentation system first con-verts the original training data into a fine-grainedstandard by parsing all words with a syntheticword parser (Section 2.1), then trains a CRF-based sub-word segmenter (Section 2.2).
A sec-ond CRF chunker is trained to recover the origi-nal word segmentation given the fine-grained re-sults of the first CRF.
The intuition is that fine-grained sub-word segmentations resolve pseudo-OOVs into IVs, which are easier to predict cor-rectly by the first CRF.
Secondly, by training an-262other CRF that predicts the original word segmen-tation given the fine-grained segmentation as in-put, we can recover the fine-grained output intooriginal word segmentation standard (Section 2.3).The flow chart of our word segmentation system isshown in Figure 1.Synthetic Word Parser CRF-Based Word SegmenterCRF-Based Chunking ModelTesting DataNoTrainingTraining DataTraining Data  (fine-grained)TrainingFine-grained OutputOriginal Standard OutputFigure 1: The Flow Chart of the Chinese WordSegmentation System.2 System Components2.1 Synthetic Word ParserIntuitively, Chinese synthetic words contain inter-nal morphological information that is helpful torecognize OOVs.
Cheng et al (2014) proposeda character-based parser to parse the internal treestructure of words.
For instance, the tree and flatsegmented result of the word ???
(munici-pal government) are shown in Figure 2.
In thiswork, we train a graph-based parser (McDonald,2006) on the data released by Cheng et al (2014)and include the dictionary (NAIST Chinese Dic-tionary1) features and Brown clustering featuresextracted from a large unlabeled corpus (ChineseGigaword Second Edition2) as described in Chenget al (2014).For native Chinese speakers, single characterand two character words are usually treated as the1http://cl.naist.jp/index.php?%B8%F8%B3%AB%A5%EA%A5%BD%A1%BC%A5%B9%2FNCD2https://catalog.ldc.upenn.edu/LDC2005T14smallest units.
In this work, we parse all the wordsin the PKU and MSR training data with characterlength greater than two.
By replacing the wordswith the flat segmented results, we convert thetraining data into a fine-grained word segmenta-tion standard as shown in Figure 3.Figure 2: The Tree Structure of a Sample Wordand the Flat Segmented Result.Original ???
/???
/?
/?
?CWS tags B I E / B I E / S / B EFine-grained ?
/??
/??
/?
/?
/?
?CWS tags S / B E / B E / S / S / B EFigure 3: A Sample Sentence of Labeling Chineseword segmentation tags on the Original and Fine-grained Standard.
In this work, we adopt 4-tag setfor word segmentation.
?B?
denotes the beginningcharacter of a word.
?I?
denotes the middle char-acter of a word.
?E?
denotes the end character ofa word.
?S?
denotes a single character word.2.2 CRF-based Word SegmenterXue et al (2003) proposed a method which treatedChinese word segmentation as a character-basedsequential labeling problem and exploited sev-eral discriminative learning algorithms.
Tsenget al (2005) adopted the CRFs as the learningmethod and obtained the best results in the secondinternational Chinese word segmentation bakeoff-2005.
Moreover, Sun and Xu (2011) attempted toextract information from large unlabeled data toenhance the Chinese word segmentation results.In this work, we train a traditional CRF-basedsupervised model on the fine-grained training data,include the dictionary (NAIST Chinese Dictio-nary) features and access variety features extractedfrom a large unlabeled corpus (Chinese Giga-word Second Edition) as described in Sun andXu (2011).2632.3 CRF-based Chunking ModelIn order to obtain the word segmentation resultwith original word segmentation standard, wetrain a CRF-based chunking model on the originaland fine-grained training data.
We show a sam-ple sentence of labeling chunking tags in Figure 4.Comparing two sentences, we label all commonunits with the tag ?S?.
The words?
and??
aretagged as ?B?
and ?E?, since ?
is the beginningpart of the synthetic word ???
and ??
is theending part.
In the chunking process, the frequentprefix?
is coordinated with neighbouring units tocompose the synthetic word??
?.For each labeling, we include previous, currentand next word as the features for the chunkingmodel.Original ???
/???
/?
/?
?Fine-grained ?
/??
/??
/?
/?
/?
?Chunking tags B / E / B / E / S / SFigure 4: A Sample Sentence of Labeling Chunk-ing Tags.
In this work, we adopt 4-tag set forchunking.
?B?
denotes the beginning part of a syn-thetic word.
?I?
denotes the middle part.
?E?
de-notes the end part.
?S?
denotes a single word.3 Experiments3.1 SettingsCheng et al (2014) released a dictionary of31,849 synthetic words with internal structure an-notated.
Since transliteration words (e.g.
????
Becham) exist in Chinese, our syntheticword parser should perform well not only on syn-thetic words but also on transliteration words.We extracted 6,574 transliteration words from theNAIST Chinese Dictionary and automatically as-signed flat structures for these words.
As a result,we obtained 38,423 words as the training data forour parser.The second international Chinese word seg-mentation bakeoff-2005 provided two annotatedsimplified Chinese corpora: PKU and MSR.
Weconducted all word segmentation experiments onthese two corpora.We used CRF++3(version 0.58) as the imple-mentation of CRFs in our experiments with the de-fault regularization algorithm L2.3The CRF++ package can be found in the following web-site: http://taku910.github.io/crfpp/3.2 Word Segmentation ResultsTable 1 summarizes the word segmentation re-sults on PKU and MSR corpora.
For compari-son, we give a baseline result by training a CRFword segmenter on the original PKU and MSRdata sets with the same features.
Our proposedsystem is expected to improve the word segmen-tation performance on pseudo-OOVs.
Comparedto the baseline, there are significant increases onOOV recall from 0.792 to 0.822 on PKU and 0.682to 0.717 on MSR.
We also evaluated the pseudo-OOV recall and observed 4% increases from thebaseline to the proposed system.
Our proposedsystem achieves higher F-score with 0.961 onPKU and 0.971 on MSR.
Comparing to other sys-tems, our proposed method obtains the state-of-the-art F-score as the results of Zhang et al (2013)who extracted dynamic statistical features fromboth in-domain and out-domain corpus and ourOOV recall significantly outperforms theirs witha 9% lead.
In MSR, we obtain very close OOVrecall and slightly lower F-score than the state-of-the-art system (Sun et al, 2009), which adopted alatent variable CRF model.
However, our systemsignificantly outperforms their system in PKU.
Inboth corpora, our proposed system outperformsthe best ?Bakeoff-2005?
results.We also test the statistical significance of theresults by using the criterion (Sproat and Emer-son, 2003; Emerson, 2005).
The 95% confidenceinterval is given as ?2?p(1?
p)/n, where n isthe number of words in the test data.
They treattwo systems as significantly different (at the 95%confidence level), if at least one of their precision-based confidences ?Cp?
or recall-based ?Cr?
aredifferent.
As the results shown in Table 2, thebaseline and proposed method are significantlydifferent on precision and recall in both PKU andMSR corpus.
In conclusion, our proposed methodsignificantly outperforms the baseline.3.3 Additional ExperimentsWe conducted additional experiments to evaluatethe performance of the synthetic word parser andCRF-based chunking model.First, we are interested in how much parsing ac-curacy is needed for good results.
Figure 5 dis-plays the OOV recall results of our word segmen-tation system when the synthetic word parser istrained with amounts of labeled synthetic wordsdata.
As the data size increases, our word segmen-264SystemPKU MSRP R F RoovRpseudoP R F RoovRpseudoBaseline 0.957 0.960 0.959 0.792 0.797 0.971 0.968 0.970 0.682 0.689Proposed method 0.960 0.962 0.961 0.822 0.838 0.972 0.970 0.971 0.717 0.73Zhang et al (2013) 0.965 0.958 0.961 0.731 - - - - - -Sun et al (2009) 0.956 0.948 0.952 0.778 - 0.973 0.973 0.973 0.722 -Bakeoff-2005 0.953 0.946 0.950 0.636 - 0.962 0.966 0.964 0.717 -Table 1: Comparison of the Proposed Method to the Baseline and Previous works on PKU and MSRCorpora.
Here, ?Rpseudo?
denotes the recall of pseudo-OOV words.
?Bakeoff-2005?
denotes the bestresults of the second international Chinese word segmentation bakeoff-2005 on two corpora.
Sincewe use extra resources and our proposed method replies on the synthetic word parser trained on andictionary with internal structure annotated, the results cannot be directly compared with the state-of-the-art systems.SystemPKU MSRWords P CpR CrWords P CpR CrBaseline 104372 0.957 ?0.00126 0.960 ?0.00121 106873 0.971 ?0.00103 0.968 ?0.00108Proposed 104372 0.960 ?0.00121 0.962 ?0.00118 106873 0.972 ?0.00101 0.970 ?0.00104Table 2: The Statistical Significance Test of the Word Segmentation Results on PKU and MSR Corpora.tation system obtains consistent gains on OOV re-call on both corpora.
On the whole 38K wordstraining data, our system reaches the highest OOVrecall.
An interesting observation is that the OOVrecall on MSR is more sensitive on data sizechanging.
The main reason is the different anno-tation standard of the two corpus.
PKU is a cor-respondingly fine-grained annotated corpus withshorter average word length than MSR.
Our syn-thetic word parser reaches high parsing accuracyon short length words (three-character and four-character words) even with a small training datasize.
With the increase of word length, the parserneeds more training data.
These factors cause thatour system reaches high OOV recall on PKU start-ing from a small training data size and obtainsmore OOV recall gains on MSR when increasingthe training data size.Our pipeline system adopts a chunking modelto recover the original standard from the fine-grained standard.
One question is how difficultis this task.
Unfortunately, we do not have thegold fine-grained input to evaluate the perfor-mance of our chunking model directly; i.e.
it isnot clear whether a segmentation error is due tomis-predictions in the first or second CRF.
There-fore, we use the synthetic word parser to parse allthe words in the gold testing data and generate anartificial gold fine-grained input for the chunkingmodel.
This data keeps the original word bound-5 10 15 20 25 30 35 408080.58181.58282.583Training data size (thousands of words)OOVrecall(percentage)PKU(a) PKU Corpus5 10 15 20 25 30 35 4069.57070.57171.57272.5Training data size (thousands of words)OOVrecall(percentage)MSR(b) MSR Corpus5 10 15 20 25 30 35 40929394959697Training data size (thousands of words)LabeledAccuracy(percentage)Synthetic Word Parser(c) Parsing PerformanceFigure 5: The OOV Recall Evaluation andthe Character Labeled Accuracy (5-fold cross-validation) of the Synthetic Word Parser on Train-ing Data Size.aries and can be used to observe the chunking per-formance.
Table 3 shows that the chunking modelon the artificial data obtains a 0.822 to 0.847 im-provement in OOV recall.
We can interpret thisto mean that 0.025 improvement is possible if thefirst CRF was perfect; on the other hand, the gapbetween 0.847 and 1.0 shows that potentially thesecond CRF is a harder task.
However, the real265gap is less for the lose of the parsing step and theexistence of non-pseudo OOVs.SystemPKU MSRF RoovF RoovProposed 0.961 0.822 0.971 0.717Artifical gold 0.965 0.847 0.973 0.743Table 3: The Word Segmentation evaluation ofthe Chunking Model.
?Artificial gold?
denotesthe word segmentation result when the chunkingmodel runs on the artificial gold input.3.4 AnalysisAs we expected, the proposed method obtains sig-nificant improvement on OOV recall.
In both cor-pora, we observed a number of OOVs are seg-mented correctly.
For instance, ???
(manage-ment law) is an OOV word in PKU corpus.
In thisword, ??
(management) appears frequently and?
(law) is a common suffix in Chinese syntheticwords, such as ???
(administrative law) or ???
(international law).
This type of pseudo-OOVs share a major contribution to upgrade thesystem performance.
We also observed that somepolysemous words bring ambiguities to the chunk-ing step.
The character ?
carries the meanings?will?
as an auxiliary verb or ?meeting?
in a syn-thetic word???
(sports meeting).4 ConclusionIn this paper, we presented a series processes toreduce OOV rate and extract morphological infor-mation inside Chinese synthetic words on a fine-grained word segmentation standard.
As a result,we can improve the Chinese word segmentationperformance (especially on pseudo-OOVs) with-out introducing any new feature types.
Our pro-posed method achieved the state-of-the-art F-scoreand OOV recall on two common corpus PKU andMSR.
However, note that we only exploited theflat segmented results of internal word structurehere.
As future work, we plan to exploit the fulltree structure of synthetic words to improve notonly CWS but also additional downstream taskssuch as sentence parsing.ReferencesFei Cheng, Kevin Duh, and Yuji Matsumoto.
2014.Parsing chinese synthetic words with a character-based dependency model.
In Proceedings of theNinth International Conference on Language Re-sources and Evaluation (LREC?14), Reykjavik, Ice-land, may.
European Language Resources Associa-tion (ELRA).Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedings ofthe fourth SIGHAN workshop on Chinese languageProcessing, volume 133.Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-sumoto.
2006.
Machine learning-based methods tochinese unknown word detection and pos tag guess-ing.
Journal of Chinese Language and Computing,16(4):185?206.Zhongguo Li and Maosong Sun.
2009.
Punctuation asimplicit annotations for chinese word segmentation.Computational Linguistics, 35(4):505?512.Zhongguo Li and Guodong Zhou.
2012.
Unified de-pendency parsing of chinese morphological and syn-tactic structures.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 1445?1454.
Association forComputational Linguistics.Ryan McDonald.
2006.
Discriminative learning andspanning tree algorithms for dependency parsing.Ph.D.
thesis, PhD Thesis.
University of Pennsylva-nia.Richard Sproat and Thomas Emerson.
2003.
Thefirst international chinese word segmentation bake-off.
In Proceedings of the second SIGHAN work-shop on Chinese language processing-Volume 17,pages 133?143.
Association for Computational Lin-guistics.Weiwei Sun and Jia Xu.
2011.
Enhancing chineseword segmentation using unlabeled data.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 970?979.
As-sociation for Computational Linguistics.Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-masa Tsuruoka, and Jun?ichi Tsujii.
2009.
A dis-criminative latent variable chinese segmenter withhybrid word/character information.
In Proceedingsof Human Language Technologies: The 2009 An-nual Conference of the North American Chapterof the Association for Computational Linguistics,pages 56?64.
Association for Computational Lin-guistics.Xu Sun, Houfeng Wang, and Wenjie Li.
2012.
Fast on-line training with frequency-adaptive learning ratesfor chinese word segmentation and new word de-tection.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 253?262.
Asso-ciation for Computational Linguistics.266Weiwei Sun.
2011.
A stacked sub-word modelfor joint chinese word segmentation and part-of-speech tagging.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies-Volume1, pages 1385?1394.
Association for ComputationalLinguistics.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for sighan bake-off 2005.
In Proceedings of the fourth SIGHANworkshop on Chinese language Processing, volume171.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8(1):29?48.Yue Zhang and Stephen Clark.
2007.
Chinese seg-mentation with a word-based perceptron algorithm.In ANNUAL MEETING-ASSOCIATION FOR COM-PUTATIONAL LINGUISTICS, volume 45, page 840.Longkai Zhang, Houfeng Wang, Xu Sun, and MairgupMansur.
2013.
Exploring representations from un-labeled data with co-training for Chinese word seg-mentation.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 311?321, Seattle, Washington, USA,October.
Association for Computational Linguistics.267
