Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1?11,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsOn Dual Decomposition and Linear Programming Relaxationsfor Natural Language ProcessingAlexander M. Rush David Sontag Michael Collins Tommi JaakkolaMIT CSAIL, Cambridge, MA 02139, USA{srush,dsontag,mcollins,tommi}@csail.mit.eduAbstractThis paper introduces dual decomposition as aframework for deriving inference algorithmsfor NLP problems.
The approach relies onstandard dynamic-programming algorithms asoracle solvers for sub-problems, together witha simple method for forcing agreement be-tween the different oracles.
The approachprovably solves a linear programming (LP) re-laxation of the global inference problem.
Itleads to algorithms that are simple, in that theyuse existing decoding algorithms; efficient, inthat they avoid exact algorithms for the fullmodel; and often exact, in that empiricallythey often recover the correct solution in spiteof using an LP relaxation.
We give experimen-tal results on two problems: 1) the combina-tion of two lexicalized parsing models; and2) the combination of a lexicalized parsingmodel and a trigram part-of-speech tagger.1 IntroductionDynamic programming algorithms have been re-markably useful for inference in many NLP prob-lems.
Unfortunately, as models become more com-plex, for example through the addition of new fea-tures or components, dynamic programming algo-rithms can quickly explode in terms of computa-tional or implementational complexity.1 As a re-sult, efficiency of inference is a critical bottleneckfor many problems in statistical NLP.This paper introduces dual decomposition(Dantzig and Wolfe, 1960; Komodakis et al, 2007)as a framework for deriving inference algorithms inNLP.
Dual decomposition leverages the observationthat complex inference problems can often bedecomposed into efficiently solvable sub-problems.The approach leads to inference algorithms with thefollowing properties:1The same is true for NLP inference algorithms based onother exact combinatorial methods, for example methods basedon minimum-weight spanning trees (McDonald et al, 2005), orgraph cuts (Pang and Lee, 2004).?
The resulting algorithms are simple and efficient,building on standard dynamic-programming algo-rithms as oracle solvers for sub-problems,2 to-gether with a method for forcing agreement be-tween the oracles.?
The algorithms provably solve a linear program-ming (LP) relaxation of the original inferenceproblem.?
Empirically, the LP relaxation often leads to anexact solution to the original problem.The approach is very general, and should be appli-cable to a wide range of problems in NLP.
The con-nection to linear programming ensures that the algo-rithms provide a certificate of optimality when theyrecover the exact solution, and also opens up thepossibility of methods that incrementally tighten theLP relaxation until it is exact (Sherali and Adams,1994; Sontag et al, 2008).The structure of this paper is as follows.
Wefirst give two examples as an illustration of the ap-proach: 1) integrated parsing and trigram part-of-speech (POS) tagging; and 2) combined phrase-structure and dependency parsing.
In both settings,it is possible to solve the integrated problem throughan ?intersected?
dynamic program (e.g., for integra-tion of parsing and tagging, the construction fromBar-Hillel et al (1964) can be used).
However,these methods, although polynomial time, are sub-stantially less efficient than our algorithms, and areconsiderably more complex to implement.Next, we describe exact polyhedral formula-tions for the two problems, building on connec-tions between dynamic programming algorithmsand marginal polytopes, as described in Martin et al(1990).
These allow us to precisely characterize therelationship between the exact formulations and the2More generally, other exact inference methods can beused as oracles, for example spanning tree algorithms for non-projective dependency structures.1LP relaxations that we solve.
We then give guaran-tees of convergence for our algorithms by showingthat they are instantiations of Lagrangian relaxation,a general method for solving linear programs of aparticular form.Finally, we describe experiments that demonstratethe effectiveness of our approach.
First, we con-sider the integration of the generative model forphrase-structure parsing of Collins (2003), with thesecond-order discriminative dependency parser ofKoo et al (2008).
This is an interesting problemin its own right: the goal is to inject the high per-formance of discriminative dependency models intophrase-structure parsing.
The method uses off-the-shelf decoders for the two models.
We find threemain results: 1) in spite of solving an LP relax-ation, empirically the method finds an exact solutionon over 99% of the examples; 2) the method con-verges quickly, typically requiring fewer than 10 it-erations of decoding; 3) the method gives gains overa baseline method that forces the phrase-structureparser to produce the same dependencies as the first-best output from the dependency parser (the Collins(2003) model has an F1 score of 88.1%; the base-line method has an F1 score of 89.7%; and the dualdecomposition method has an F1 score of 90.7%).In a second set of experiments, we use dual de-composition to integrate the trigram POS tagger ofToutanova and Manning (2000) with the parser ofCollins (2003).
We again find that the method findsan exact solution in almost all cases, with conver-gence in just a few iterations of decoding.Although the focus of this paper is on dynamicprogramming algorithms?both in the experiments,and also in the formal results concerning marginalpolytopes?it is straightforward to use other com-binatorial algorithms within the approach.
For ex-ample, Koo et al (2010) describe a dual decompo-sition approach for non-projective dependency pars-ing, which makes use of both dynamic programmingand spanning tree inference algorithms.2 Related WorkDual decomposition is a classical method for solv-ing optimization problems that can be decomposedinto efficiently solvable sub-problems.
Our work isinspired by dual decomposition methods for infer-ence in Markov random fields (MRFs) (Wainwrightet al, 2005a; Komodakis et al, 2007; Globerson andJaakkola, 2007).
In this approach, the MRF is de-composed into sub-problems corresponding to tree-structured subgraphs that together cover all edgesof the original graph.
The resulting inference algo-rithms provably solve an LP relaxation of the MRFinference problem, often significantly faster thancommercial LP solvers (Yanover et al, 2006).Our work is also related to methods that incorpo-rate combinatorial solvers within loopy belief prop-agation (LBP), either for MAP inference (Duchi etal., 2007) or for computing marginals (Smith andEisner, 2008).
Our approach similarly makes useof combinatorial algorithms to efficiently solve sub-problems of the global inference problem.
However,unlike LBP, our algorithms have strong theoreticalguarantees, such as guaranteed convergence and thepossibility of a certificate of optimality.
These guar-antees are possible because our algorithms directlysolve an LP relaxation.Other work has considered LP or integer lin-ear programming (ILP) formulations of inference inNLP (Martins et al, 2009; Riedel and Clarke, 2006;Roth and Yih, 2005).
These approaches typicallyuse general-purpose LP or ILP solvers.
Our methodhas the advantage that it leverages underlying struc-ture arising in LP formulations of NLP problems.We will see that dynamic programming algorithmssuch as CKY can be considered to be very effi-cient solvers for particular LPs.
In dual decomposi-tion, these LPs?and their efficient solvers?can beembedded within larger LPs corresponding to morecomplex inference problems.3 Background: Structured Models for NLPWe now describe the type of models used throughoutthe paper.
We take some care to set up notation thatwill allow us to make a clear connection betweeninference problems and linear programming.Our first example is weighted CFG parsing.
Weassume a context-free grammar, in Chomsky normalform, with a set of non-terminals N .
The grammarcontains all rules of the form A ?
B C and A ?w where A,B,C ?
N and w ?
V (it is simpleto relax this assumption to give a more constrainedgrammar).
For rules of the form A ?
w we referto A as the part-of-speech tag for w. We allow anynon-terminal to be at the root of the tree.2Given a sentence with n words, w1, w2, .
.
.
wn, aparse tree is a set of rule productions of the form?A ?
B C, i, k, j?
where A,B,C ?
N , and1 ?
i ?
k < j ?
n. Each rule production rep-resents the use of CFG rule A ?
B C where non-terminal A spans words wi .
.
.
wj , non-terminal Bspans words wi .
.
.
wk, and non-terminal C spanswords wk+1 .
.
.
wj .
There are O(|N |3n3) such ruleproductions.
Each parse tree corresponds to a subsetof these rule productions, of size n?
1, that forms awell-formed parse tree.3We now define the index set for CFG parsing asI = {?A?
B C, i, k, j?
: A,B,C ?
N ,1 ?
i ?
k < j ?
n}Each parse tree is a vector y = {yr : r ?
I},with yr = 1 if rule r is in the parse tree, and yr =0 otherwise.
Hence each parse tree is representedas a vector in {0, 1}m, where m = |I|.
We use Yto denote the set of all valid parse-tree vectors; theset Y is a subset of {0, 1}m (not all binary vectorscorrespond to valid parse trees).In addition, we assume a vector ?
= {?r : r ?I} that specifies a weight for each rule production.4Each ?r can take any value in the reals.
The optimalparse tree is y?
= arg maxy?Y y ?
?
where y ?
?
=?r yr?r is the inner product between y and ?.We use yr and y(r) interchangeably (similarly for?r and ?
(r)) to refer to the r?th component of thevector y.
For example ?
(A ?
B C, i, k, j) is aweight for the rule ?A?
B C, i, k, j?.We will use similar notation for other problems.As a second example, in POS tagging the task is tomap a sentence of n words w1 .
.
.
wn to a tag se-quence t1 .
.
.
tn, where each ti is chosen from a setT of possible tags.
We assume a trigram tagger,where a tag sequence is represented through deci-sions ?
(A,B) ?
C, i?
where A,B,C ?
T , andi ?
{3 .
.
.
n}.
Each production represents a tran-sition where C is the tag of word wi, and (A,B) are3We do not require rules of the form A ?
wi in this repre-sentation, as they are redundant: specifically, a rule production?A ?
B C, i, k, j?
implies a rule B ?
wi iff i = k, andC ?
wj iff j = k + 1.4We do not require parameters for rules of the formA?
w,as they can be folded into rule production parameters.
E.g.,under a PCFG we define ?
(A ?
B C, i, k, j) = logP (A ?B C | A) + ?i,k logP (B ?
wi|B) + ?k+1,j logP (C ?wj |C) where ?x,y = 1 if x = y, 0 otherwise.the previous two tags.
The index set for tagging isItag = {?(A,B)?
C, i?
: A,B,C ?
T , 3 ?
i ?
n}Note that we do not need transitions for i = 1 or i =2, because the transition ?
(A,B) ?
C, 3?
specifiesthe first three tags in the sentence.5Each tag sequence is represented as a vector z ={zr : r ?
Itag}, and we denote the set of valid tagsequences, a subset of {0, 1}|Itag|, as Z .
Given aparameter vector ?
= {?r : r ?
Itag}, the optimaltag sequence is arg maxz?Z z ?
?.As a modification to the above approach, we willfind it convenient to introduce extended index setsfor both the CFG and POS tagging examples.
Forthe CFG case we define the extended index set to beI ?
= I ?
Iuni whereIuni = {(i, t) : i ?
{1 .
.
.
n}, t ?
T}Here each pair (i, t) represents word wi being as-signed the tag t. Thus each parse-tree vector y willhave additional (binary) components y(i, t) spec-ifying whether or not word i is assigned tag t.(Throughout this paper we will assume that the tag-set used by the tagger, T , is a subset of the set of non-terminals considered by the parser, N .)
Note thatthis representation is over-complete, since a parsetree determines a unique tagging for a sentence:more explicitly, for any i ?
{1 .
.
.
n}, Y ?
T , thefollowing linear constraint holds:y(i, Y ) =n?k=i+1?X,Z?Ny(X ?
Y Z, i, i, k) +i?1?k=1?X,Z?Ny(X ?
Z Y, k, i?
1, i)We apply the same extension to the tagging indexset, effectively mapping trigrams down to unigramassignments, again giving an over-complete repre-sentation.
The extended index set for tagging is re-ferred to as I ?tag.From here on we will make exclusive use of ex-tended index sets for CFG parsing and trigram tag-ging.
We use the set Y to refer to the set of validparse structures under the extended representation;5As one example, in an HMM, the parameter ?
((A,B) ?C, 3) would be logP (A|??
)+logP (B|?A)+logP (C|AB)+logP (w1|A) + logP (w2|B) + logP (w3|C), where ?
is thestart symbol.3each y ?
Y is a binary vector of length |I ?|.
Wesimilarly use Z to refer to the set of valid tag struc-tures under the extended representation.
We assumeparameter vectors for the two problems, ?cfg ?
R|I?|and ?tag ?
R|I?tag|.4 Two ExamplesThis section describes the dual decomposition ap-proach for two inference problems in NLP.4.1 Integrated Parsing and Trigram TaggingWe now describe the dual decomposition approachfor integrated parsing and trigram tagging.
First, de-fine the set Q as follows:Q = {(y, z) : y ?
Y, z ?
Z,y(i, t) = z(i, t) for all (i, t) ?
Iuni} (1)Hence Q is the set of all (y, z) pairs that agreeon their part-of-speech assignments.
The integratedparsing and trigram tagging problem is then to solvemax(y,z)?Q(y ?
?cfg + z ?
?tag)(2)This problem is equivalent tomaxy?Y(y ?
?cfg + g(y) ?
?tag)where g : Y ?
Z is a function that maps a parsetree y to its set of trigrams z = g(y).
The benefit ofthe formulation in Eq.
2 is that it makes explicit theidea of maximizing over all pairs (y, z) under a setof agreement constraints y(i, t) = z(i, t)?this con-cept will be central to the algorithms in this paper.With this in mind, we note that we have effi-cient methods for the inference problems of taggingand parsing alone, and that our combined objectivealmost separates into these two independent prob-lems.
In fact, if we drop the y(i, t) = z(i, t) con-straints from the optimization problem, the problemsplits into two parts, each of which can be efficientlysolved using dynamic programming:(y?, z?)
= (arg maxy?Yy ?
?cfg, arg maxz?Zz ?
?tag)Dual decomposition exploits this idea; it results inthe algorithm given in figure 1.
The algorithm opti-mizes the combined objective by repeatedly solvingthe two sub-problems separately?that is, it directlySet u(1)(i, t)?
0 for all (i, t) ?
Iunifor k = 1 to K doy(k) ?
arg maxy?Y(y ?
?cfg ??
(i,t)?Iuniu(k)(i, t)y(i, t))z(k) ?
arg maxz?Z(z ?
?tag +?
(i,t)?Iuniu(k)(i, t)z(i, t))if y(k)(i, t) = z(k)(i, t) for all (i, t) ?
Iuni thenreturn (y(k), z(k))for all (i, t) ?
Iuni,u(k+1)(i, t)?
u(k)(i, t)+?k(y(k)(i, t)?z(k)(i, t))return (y(K), z(K))Figure 1: The algorithm for integrated parsing and tag-ging.
The parameters ?k > 0 for k = 1 .
.
.K specifystep sizes for each iteration, and are discussed further inthe Appendix.
The two arg max problems can be solvedusing dynamic programming.solves the harder optimization problem using an ex-isting CFG parser and trigram tagger.
After eachiteration the algorithm adjusts the weights u(i, t);these updates modify the objective functions for thetwo models, encouraging them to agree on the samePOS sequence.
In section 6.1 we will show that thevariables u(i, t) are Lagrange multipliers enforcingagreement constraints, and that the algorithm corre-sponds to a (sub)gradient method for optimizationof a dual function.
The algorithm is easy to imple-ment: all that is required is a decoding algorithm foreach of the two models, and simple additive updatesto the Lagrange multipliers enforcing agreement be-tween the two models.4.2 Integrating Two Lexicalized ParsersOur second example problem is the integration ofa phrase-structure parser with a higher-order depen-dency parser.
The goal is to add higher-order fea-tures to phrase-structure parsing without greatly in-creasing the complexity of inference.First, we define an index set for second-order un-labeled projective dependency parsing.
The second-order parser considers first-order dependencies, aswell as grandparent and sibling second-order depen-dencies (e.g., see Carreras (2007)).
We assume thatIdep is an index set containing all such dependen-cies (for brevity we omit the details of this indexset).
For convenience we define an extended indexset that makes explicit use of first-order dependen-4cies, I ?dep = Idep ?
Ifirst, whereIfirst = {(i, j) : i ?
{0 .
.
.
n}, j ?
{1 .
.
.
n}, i 6= j}Here (i, j) represents a dependency with head wiand modifier wj (i = 0 corresponds to the root sym-bol in the parse).
We use D ?
{0, 1}|I?dep| to denotethe set of valid projective dependency parses.The second model we use is a lexicalized CFG.Each symbol in the grammar takes the form A(h)where A ?
N is a non-terminal, and h ?
{1 .
.
.
n}is an index specifying that wh is the head of the con-stituent.
Rule productions take the form ?A(a) ?B(b) C(c), i, k, j?
where b ?
{i .
.
.
k}, c ?
{(k +1) .
.
.
j}, and a is equal to b or c, depending onwhether A receives its head-word from its left orright child.
Each such rule implies a dependency(a, b) if a = c, or (a, c) if a = b.
We take Iheadto be the index set of all such rules, and I ?head =Ihead?Ifirst to be the extended index set.
We defineH ?
{0, 1}|I?head| to be the set of valid parse trees.The integrated parsing problem is then to find(y?, d?)
= arg max(y,d)?R(y ?
?head + d ?
?dep)(3)where R = {(y, d) : y ?
H, d ?
D,y(i, j) = d(i, j) for all (i, j) ?
Ifirst}This problem has a very similar structure to theproblem of integrated parsing and tagging, and wecan derive a similar dual decomposition algorithm.The Lagrange multipliers u are a vector in R|Ifirst|enforcing agreement between dependency assign-ments.
The algorithm (omitted for brevity) is identi-cal to the algorithm in figure 1, but with Iuni, Y , Z ,?cfg, and ?tag replaced with Ifirst, H, D, ?head, and?dep respectively.
The algorithm only requires de-coding algorithms for the two models, together withsimple updates to the Lagrange multipliers.5 Marginal Polytopes and LP RelaxationsWe now give formal guarantees for the algorithmsin the previous section, showing that they solve LPrelaxations of the problems in Eqs.
2 and 3.To make the connection to linear programming,we first introduce the idea of marginal polytopes insection 5.1.
In section 5.2, we give a precise state-ment of the LP relaxations that are being solvedby the example algorithms, making direct use ofmarginal polytopes.
In section 6 we will prove thatthe example algorithms solve these LP relaxations.5.1 Marginal PolytopesFor a finite set Y , define the set of all distributionsover elements in Y as ?
= {?
?
R|Y| : ?y ?0,?y?Y ?y = 1}.
Each ?
?
?
gives a vector ofmarginals, ?
=?y?Y ?yy, where ?r can be inter-preted as the probability that yr = 1 for a y selectedat random from the distribution ?.The set of all possible marginal vectors, known asthe marginal polytope, is defined as follows:M = {?
?
Rm : ??
?
?
such that ?
=?y?Y?yy}M is also frequently referred to as the convex hull ofY , written as conv(Y).
We use the notation conv(Y)in the remainder of this paper, instead ofM.For an arbitrary set Y , the marginal polytopeconv(Y) can be complex to describe.6 However,Martin et al (1990) show that for a very generalclass of dynamic programming problems, the cor-responding marginal polytope can be expressed asconv(Y) = {?
?
Rm : A?
= b, ?
?
0} (4)where A is a p?m matrix, b is vector in Rp, and thevalue p is linear in the size of a hypergraph repre-sentation of the dynamic program.
Note that A andb specify a set of p linear constraints.We now give an explicit description of the re-sulting constraints for CFG parsing:7 similar con-straints arise for other dynamic programming algo-rithms for parsing, for example the algorithms ofEisner (2000).
The exact form of the constraints, andthe fact that they are polynomial in number, is notessential for the formal results in this paper.
How-ever, a description of the constraints gives valuableintuition for the structure of the marginal polytope.The constraints are given in figure 2.
To developsome intuition, consider the case where the variables?r are restricted to be binary: hence each binaryvector ?
specifies a parse tree.
The second con-straint in Eq.
5 specifies that exactly one rule mustbe used at the top of the tree.
The set of constraintsin Eq.
6 specify that for each production of the form6For any finite set Y , conv(Y) can be expressed as {?
?Rm : A?
?
b} where A is a matrix of dimension p ?m, andb ?
Rp (see, e.g., Korte and Vygen (2008), pg.
65).
The valuefor p depends on the set Y , and can be exponential in size.7Taskar et al (2004) describe the same set of constraints, butwithout proof of correctness or reference to Martin et al (1990).5?r ?
I?, ?r ?
0 ;XX,Y,Z?Nk=1...(n?1)?
(X ?
Y Z, 1, k, n) = 1 (5)?X ?
N , ?
(i, j) such that 1 ?
i < j ?
n and (i, j) 6= (1, n):XY,Z?Nk=i...(j?1)?
(X ?
Y Z, i, k, j) =XY,Z?Nk=1...(i?1)?
(Y ?
Z X, k, i?
1, j)+XY,Z?Nk=(j+1)...n?
(Y ?
X Z, i, j, k) (6)?Y ?
T, ?i ?
{1 .
.
.
n} : ?
(i, Y ) =XX,Z?Nk=(i+1)...n?
(X ?
Y Z, i, i, k) +XX,Z?Nk=1...(i?1)?
(X ?
Z Y, k, i?
1, i) (7)Figure 2: The linear constraints defining the marginalpolytope for CFG parsing.
?X ?
Y Z, i, k, j?
in a parse tree, there must beexactly one production higher in the tree that gener-ates (X, i, j) as one of its children.
The constraintsin Eq.
7 enforce consistency between the ?
(i, Y )variables and rule variables higher in the tree.
Notethat the constraints in Eqs.
(5?7) can be written in theform A?
= b, ?
?
0, as in Eq.
4.Under these definitions, we have the following:Theorem 5.1 Define Y to be the set of all CFGparses, as defined in section 4.
Thenconv(Y) = {?
?
Rm : ?
satisifies Eqs.
(5?7)}Proof: This theorem is a special case of Martin et al(1990), theorem 2.The marginal polytope for tagging, conv(Z), canalso be expressed using linear constraints as in Eq.
4;see figure 3.
These constraints follow from re-sults for graphical models (Wainwright and Jordan,2008), or from the Martin et al (1990) construction.As a final point, the following theorem gives animportant property of marginal polytopes, which wewill use at several points in this paper:Theorem 5.2 (Korte and Vygen (2008), page 66.
)For any set Y ?
{0, 1}k, and for any vector ?
?
Rk,maxy?Yy ?
?
= max??conv(Y)?
?
?
(8)The theorem states that for a linear objective func-tion, maximization over a discrete set Y can bereplaced by maximization over the convex hull?r ?
I?tag, ?r ?
0 ;XX,Y,Z?T?
((X,Y )?
Z, 3) = 1?X ?
T , ?i ?
{3 .
.
.
n?
1}:XY,Z?T?((Y,Z)?
X, i) =XY,Z?T?((Y,X)?
Z, i+ 1)?X ?
T , ?i ?
{3 .
.
.
n?
2}:XY,Z?T?((Y,Z)?
X, i) =XY,Z?T?
((X,Y )?
Z, i+ 2)?X ?
T,?i ?
{3 .
.
.
n} : ?
(i,X) =XY,Z?T?((Y,Z)?
X, i)?X ?
T : ?
(1, X) =XY,Z?T?
((X,Y )?
Z, 3)?X ?
T : ?
(2, X) =XY,Z?T?((Y,X)?
Z, 3)Figure 3: The linear constraints defining the marginalpolytope for trigram POS tagging.conv(Y).
The problem max?
?conv(Y) ?
??
is a linearprogramming problem.For parsing, this theorem implies that:1.
Weighted CFG parsing can be framed as a linearprogramming problem, of the form max?
?conv(Y) ??
?, where conv(Y) is specified by a polynomial num-ber of linear constraints.2.
Conversely, dynamic programming algorithmssuch as the CKY algorithm can be considered tobe oracles that efficiently solve LPs of the formmax?
?conv(Y) ?
?
?.Similar results apply for the POS tagging case.5.2 Linear Programming RelaxationsWe now describe the LP relaxations that are solvedby the example algorithms in section 4.
We beginwith the algorithm in Figure 1.The original optimization problem was to findmax(y,z)?Q(y ?
?cfg + z ?
?tag)(see Eq.
2).
By the-orem 5.2, this is equivalent to solvingmax(?,?)?conv(Q)(?
?
?cfg + ?
?
?tag)(9)To formulate our approximation, we first define:Q?
= {(?, ?)
: ?
?
conv(Y), ?
?
conv(Z),?
(i, t) = ?
(i, t) for all (i, t) ?
Iuni}6The definition of Q?
is very similar to the definitionof Q (see Eq.
1), the only difference being that Yand Z are replaced by conv(Y) and conv(Z) re-spectively.
Hence any point inQ is also inQ?.
It fol-lows that any point in conv(Q) is also inQ?, becauseQ?
is a convex set defined by linear constraints.The LP relaxation then corresponds to the follow-ing optimization problem:max(?,?)?Q?(?
?
?cfg + ?
?
?tag)(10)Q?
is defined by linear constraints, making this alinear program.
Since Q?
is an outer bound onconv(Q), i.e.
conv(Q) ?
Q?, we obtain the guaran-tee that the value of Eq.
10 always upper bounds thevalue of Eq.
9.In Appendix A we give an example showingthat in general Q?
includes points that are not inconv(Q).
These points exist because the agreementbetween the two parts is now enforced in expecta-tion (?
(i, t) = ?
(i, t) for (i, t) ?
Iuni) rather thanbased on actual assignments.
This agreement con-straint is weaker since different distributions overassignments can still result in the same first orderexpectations.
Thus, the solution to Eq.
10 may bein Q?
but not in conv(Q).
It can be shown thatall such solutions will be fractional, making themeasy to distinguish from Q.
In many applications ofLP relaxations?including the examples discussedin this paper?the relaxation in Eq.
10 turns out tobe tight, in that the solution is often integral (i.e., itis in Q).
In these cases, solving the LP relaxationexactly solves the original problem of interest.In the next section we prove that the algorithmin Figure 1 solves the problem in Eq 10.
A similarresult holds for the algorithm in section 4.2: it solvesa relaxation of Eq.
3, whereR is replaced byR?
= {(?, ?)
: ?
?
conv(H), ?
?
conv(D),?
(i, j) = ?
(i, j) for all (i, j) ?
Ifirst}6 Convergence Guarantees6.1 Lagrangian RelaxationWe now show that the example algorithms solvetheir respective LP relaxations given in the previ-ous section.
We do this by first introducing a gen-eral class of linear programs, together with an op-timization method, Lagrangian relaxation, for solv-ing these LPs.
We then show that the algorithms insection 4 are special cases of the general algorithm.The linear programs we consider take the formmaxx1?X1,x2?X2(?1 ?
x1 + ?2 ?
x2) such that Ex1 = Fx2The matricesE ?
Rq?m andF ?
Rq?l specify q lin-ear ?agreement?
constraints between x1 ?
Rm andx2 ?
Rl.
The setsX1,X2 are also specified by linearconstraints, X1 = {x1 ?
Rm : Ax1 = b, x1 ?
0}and X2 ={x2 ?
Rl : Cx2 = d, x2 ?
0}, hence theproblem is an LP.Note that if we set X1 = conv(Y), X2 =conv(Z), and define E and F to specify the agree-ment constraints ?
(i, t) = ?
(i, t), then we have theLP relaxation in Eq.
10.It is natural to apply Lagrangian relaxation incases where the sub-problems maxx1?X1 ?1 ?x1 andmaxx2?X2 ?2 ?
x2 can be efficiently solved by com-binatorial algorithms for any values of ?1, ?2, butwhere the constraints Ex1 = Fx2 ?complicate?
theproblem.
We introduce Lagrange multipliers u ?
Rqthat enforce the latter set of constraints, giving theLagrangian:L(u, x1, x2) = ?1 ?
x1 + ?2 ?
x2 + u ?
(Ex1 ?
Fx2)The dual objective function isL(u) = maxx1?X1,x2?X2L(u, x1, x2)and the dual problem is to find minu?Rq L(u).Because X1 and X2 are defined by linear con-straints, by strong duality we haveminu?RqL(u) = maxx1?X1,x2?X2:Ex1=Fx2(?1 ?
x1 + ?2 ?
x2)Hence minimizing L(u) will recover the maximumvalue of the original problem.
This leaves open thequestion of how to recover the LP solution (i.e., thepair (x?1, x?2) that achieves this maximum); we dis-cuss this point in section 6.2.The dual L(u) is convex.
However, L(u) isnot differentiable, so we cannot use gradient-basedmethods to optimize it.
Instead, a standard approachis to use a subgradient method.
Subgradients are tan-gent lines that lower bound a function even at pointsof non-differentiability: formally, a subgradient of aconvex function L : Rn ?
R at a point u is a vectorgu such that for all v, L(v) ?
L(u) + gu ?
(v ?
u).7u(1) ?
0for k = 1 to K dox(k)1 ?
arg maxx1?X1(?1 + (u(k))TE) ?
x1x(k)2 ?
arg maxx2?X2(?2 ?
(u(k))TF ) ?
x2if Ex(k)1 = Fx(k)2 return u(k)u(k+1) ?
u(k) ?
?k(Ex(k)1 ?
Fx(k)2 )return u(K)Figure 4: The Lagrangian relaxation algorithm.By standard results, the subgradient for L at a pointu takes a simple form, gu = Ex?1 ?
Fx?2, wherex?1 = arg maxx1?X1(?1 + (u(k))TE) ?
x1x?2 = arg maxx2?X2(?2 ?
(u(k))TF ) ?
x2The beauty of this result is that the values of x?1 andx?2, and by implication the value of the subgradient,can be computed using oracles for the two arg maxsub-problems.Subgradient algorithms perform updates that aresimilar to gradient descent:u(k+1) ?
u(k) ?
?kg(k)where g(k) is the subgradient ofL at u(k) and ?k > 0is the step size of the update.
The complete sub-gradient algorithm is given in figure 4.
The follow-ing convergence theorem is well-known (e.g., seepage 120 of Korte and Vygen (2008)):Theorem 6.1 If limk??
?k = 0 and?
?k=1 ?k =?, then limk??
L(u(k)) = minu L(u).The following proposition is easily verified:Proposition 6.1 The algorithm in figure 1 is an in-stantiation of the algorithm in figure 4,8 with X1 =conv(Y), X2 = conv(Z), and the matrices E andF defined to be binary matrices specifying the con-straints ?
(i, t) = ?
(i, t) for all (i, t) ?
Iuni.Under an appropriate definition of the step sizes ?k,it follows that the algorithm in figure 1 defines asequence of Lagrange multiplers u(k) minimizing adual of the LP relaxation in Eq.
10.
A similar resultholds for the algorithm in section 4.2.8with the caveat that it returns (x(k)1 , x(k)2 ) rather than u(k).6.2 Recovering the LP SolutionThe previous section described how the method infigure 4 can be used to minimize the dualL(u) of theoriginal linear program.
We now turn to the problemof recovering a primal solution (x?1, x?2) of the LP.The method we propose considers two cases:(Case 1) If Ex(k)1 = Fx(k)2 at any stage duringthe algorithm, then simply take (x(k)1 , x(k)2 ) to be theprimal solution.
In this case the pair (x(k)1 , x(k)2 ) ex-actly solves the original LP.9 If this case arises in thealgorithm in figure 1, then the resulting solution isbinary (i.e., it is a member of Q), and the solutionexactly solves the original inference problem.
(Case 2) If case 1 does not arise, then a couple ofstrategies are possible.
(This situation could arisein cases where the LP is not tight?i.e., it has afractional solution?or where K is not large enoughfor convergence.)
The first is to define the pri-mal solution to be the average of the solutions en-countered during the algorithm: x?1 =?k x(k)1 /K,x?2 =?k x(k)2 /K.
Results from Nedic?
and Ozdaglar(2009) show that as K ?
?, these averaged solu-tions converge to the optimal primal solution.10 Asecond strategy (as given in figure 1) is to simplytake (x(K)1 , x(K)2 ) as an approximation to the primalsolution.
This method is a heuristic, but previouswork (e.g., Komodakis et al (2007)) has shown thatit is effective in practice; we use it in this paper.In our experiments we found that in the vast ma-jority of cases, case 1 applies, after a small numberof iterations; see the next section for more details.7 Experiments7.1 Integrated Phrase-Structure andDependency ParsingOur first set of experiments considers the integrationof Model 1 of Collins (2003) (a lexicalized phrase-structure parser, from here on referred to as Model9We have that ?1 ?
x(k)1 + ?2 ?
x(k)2 = L(u(k), x(k)1 , x(k)2 ) =L(u(k)), where the last equality is because x(k)1 and x(k)2 are de-fined by the respective argmax?s.
Thus, (x(k)1 , x(k)2 ) and u(k)are primal and dual optimal.10The resulting fractional solution can be projected back tothe setQ, see (Smith and Eisner, 2008; Martins et al, 2009).8Itn.
1 2 3 4 5-10 11-20 20-50 **Dep 43.5 20.1 10.2 4.9 14.0 5.7 1.4 0.4POS 58.7 15.4 6.3 3.6 10.3 3.8 0.8 1.1Table 1: Convergence results for Section 23 of the WSJTreebank for the dependency parsing and POS experi-ments.
Each column gives the percentage of sentenceswhose exact solutions were found in a given range of sub-gradient iterations.
** is the percentage of sentences thatdid not converge by the iteration limit (K=50).1),11 and the 2nd order discriminative dependencyparser of Koo et al (2008).
The inference problemfor a sentence x is to findy?
= arg maxy?Y(f1(y) + ?f2(y)) (11)where Y is the set of all lexicalized phrase-structuretrees for the sentence x; f1(y) is the score (log prob-ability) under Model 1; f2(y) is the score under Kooet al (2008) for the dependency structure impliedby y; and ?
> 0 is a parameter dictating the relativeweight of the two models.12 This problem is simi-lar to the second example in section 4; a very sim-ilar dual decomposition algorithm to that describedin section 4.2 can be derived.We used the Penn Wall Street Treebank (Marcuset al, 1994) for the experiments, with sections 2-21for training, section 22 for development, and section23 for testing.
The parameter ?
was chosen to opti-mize performance on the development set.We ran the dual decomposition algorithm with alimit of K = 50 iterations.
The dual decomposi-tion algorithm returns an exact solution if case 1 oc-curs as defined in section 6.2; we found that of 2416sentences in section 23, case 1 occurred for 2407(99.6%) sentences.
Table 1 gives statistics showingthe number of iterations required for convergence.Over 80% of the examples converge in 5 iterations orfewer; over 90% converge in 10 iterations or fewer.We compare the accuracy of the dual decomposi-tion approach to two baselines: first, Model 1; andsecond, a naive integration method that enforces thehard constraint that Model 1 must only consider de-11We use a reimplementation that is a slight modification ofCollins Model 1, with very similar performance, and which usesthe TAG formalism of Carreras et al (2008).12Note that the models f1 and f2 were trained separately,using the methods described by Collins (2003) and Koo et al(2008) respectively.Precision Recall F1 DepModel 1 88.4 87.8 88.1 91.4Koo08 Baseline 89.9 89.6 89.7 93.3DD Combination 91.0 90.4 90.7 93.8Table 2: Performance results for Section 23 of the WSJTreebank.
Model 1: a reimplementation of the genera-tive parser of (Collins, 2002).
Koo08 Baseline: Model 1with a hard restriction to dependencies predicted by thediscriminative dependency parser of (Koo et al, 2008).DD Combination: a model that maximizes the joint scoreof the two parsers.
Dep shows the unlabeled dependencyaccuracy of each system.50607080901000  10  20  30  40  50PercentageMaximum Number of Dual Decomposition Iterationsf score% certificates% match K=50Figure 5: Performance on the parsing task assuming afixed number of iterations K. f-score: accuracy of themethod.
% certificates: percentage of examples for whicha certificate of optimality is provided.
% match: percent-age of cases where the output from the method is identicalto the output when using K = 50.pendencies seen in the first-best output from the de-pendency parser.
Table 2 shows all three results.
Thedual decomposition method gives a significant gainin precision and recall over the naive combinationmethod, and boosts the performance of Model 1 toa level that is close to some of the best single-passparsers on the Penn treebank test set.
Dependencyaccuracy is also improved over the Koo et al (2008)model, in spite of the relatively low dependency ac-curacy of Model 1 alone.Figure 5 shows performance of the approach as afunction ofK, the maximum number of iterations ofdual decomposition.
For this experiment, for caseswhere the method has not converged for k ?
K,the output from the algorithm is chosen to be they(k) for k ?
K that maximizes the objective func-tion in Eq.
11.
The graphs show that values of Kless than 50 produce almost identical performance toK = 50, but with fewer cases giving certificates ofoptimality (with K = 10, the f-score of the methodis 90.69%; with K = 5 it is 90.63%).9Precision Recall F1 POS AccFixed Tags 88.1 87.6 87.9 96.7DD Combination 88.7 88.0 88.3 97.1Table 3: Performance results for Section 23 of the WSJ.Model 1 (Fixed Tags): a baseline parser initialized to thebest tag sequence of from the tagger of Toutanova andManning (2000).
DD Combination: a model that maxi-mizes the joint score of parse and tag selection.7.2 Integrated Phrase-Structure Parsing andTrigram POS taggingIn a second experiment, we used dual decomposi-tion to integrate the Model 1 parser with the Stan-ford max-ent trigram POS tagger (Toutanova andManning, 2000), using a very similar algorithm tothat described in section 4.1.
We use the same train-ing/dev/test split as in section 7.1.
The two modelswere again trained separately.We ran the algorithm with a limit of K = 50 it-erations.
Out of 2416 test examples, the algorithmfound an exact solution in 98.9% of the cases.
Ta-ble 1 gives statistics showing the speed of conver-gence for different examples: over 94% of the exam-ples converge to an exact solution in 10 iterations orfewer.
In terms of accuracy, we compare to a base-line approach of using the first-best tag sequenceas input to the parser.
The dual decomposition ap-proach gives 88.3 F1 measure in recovering parse-tree constituents, compared to 87.9 for the baseline.8 ConclusionsWe have introduced dual-decomposition algorithmsfor inference in NLP, given formal properties of thealgorithms in terms of LP relaxations, and demon-strated their effectiveness on problems that wouldtraditionally be solved using intersections of dy-namic programs (Bar-Hillel et al, 1964).
Given thewidespread use of dynamic programming in NLP,there should be many applications for the approach.There are several possible extensions of themethod we have described.
We have focused oncases where two models are being combined; theextension to more than two models is straightfor-ward (e.g., see Komodakis et al (2007)).
This paperhas considered approaches for MAP inference; forclosely related methods that compute approximatemarginals, see Wainwright et al (2005b).A Fractional SolutionsWe now give an example of a point (?, ?)
?
Q?\conv(Q)that demonstrates that the relaxation Q?
is strictly largerthan conv(Q).
Fractional points such as this one can ariseas solutions of the LP relaxation for worst case instances,preventing us from finding an exact solution.Recall that the constraints for Q?
specify that ?
?conv(Y), ?
?
conv(Z), and ?
(i, t) = ?
(i, t) for all(i, t) ?
Iuni.
Since ?
?
conv(Y), ?
must be a con-vex combination of 1 or more members of Y; a similarproperty holds for ?.
The example is as follows.
Thereare two possible parts of speech, A and B, and an addi-tional non-terminal symbol X .
The sentence is of length3, w1 w2 w3.
Let ?
be the convex combination of thefollowing two tag sequences, each with probability 0.5:w1/A w2/A w3/A and w1/A w2/B w3/B.
Let ?
bethe convex combination of the following two parses, eachwith probability 0.5: (X(A w1)(X(A w2)(B w3))) and(X(A w1)(X(B w2)(A w3))).
It can be verified that?
(i, t) = ?
(i, t) for all (i, t), i.e., the marginals for singletags for ?
and ?
agree.
Thus, (?, ?)
?
Q?.To demonstrate that this fractional point is not inconv(Q), we give parameter values such that this frac-tional point is optimal and all integral points (i.e., ac-tual parses) are suboptimal.
For the tagging model, set?(AA?
A, 3) = ?
(AB ?
B, 3) = 0, with all other pa-rameters having a negative value.
For the parsing model,set ?
(X ?
A X, 1, 1, 3) = ?
(X ?
A B, 2, 2, 3) =?
(X ?
B A, 2, 2, 3) = 0, with all other rule parametersbeing negative.
For this objective, the fractional solutionhas value 0, while all integral points (i.e., all points inQ)have a negative value.
By Theorem 5.2, the maximum ofany linear objective over conv(Q) is equal to the maxi-mum over Q.
Thus, (?, ?)
6?
conv(Q).B Step SizeWe used the following step size in our experiments.
First,we initialized ?0 to equal 0.5, a relatively large value.Then we defined ?k = ?0 ?
2?
?k , where ?k is the num-ber of times that L(u(k?))
> L(u(k?
?1)) for k?
?
k. Thislearning rate drops at a rate of 1/2t, where t is the num-ber of times that the dual increases from one iteration tothe next.
See Koo et al (2010) for a similar, but less ag-gressive step size used to solve a different task.Acknowledgments MIT gratefully acknowledges thesupport of Defense Advanced Research Projects Agency(DARPA) Machine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract no.
FA8750-09-C-0181.Any opinions, findings, and conclusion or recommendations ex-pressed in this material are those of the author(s) and do notnecessarily reflect the view of the DARPA, AFRL, or the USgovernment.
Alexander Rush was supported under the GALEprogram of the Defense Advanced Research Projects Agency,Contract No.
HR0011-06-C-0022.
David Sontag was supportedby a Google PhD Fellowship.10ReferencesY.
Bar-Hillel, M. Perles, and E. Shamir.
1964.
On formalproperties of simple phrase structure grammars.
InLanguage and Information: Selected Essays on theirTheory and Application, pages 116?150.X.
Carreras, M. Collins, and T. Koo.
2008.
TAG, dy-namic programming, and the perceptron for efficient,feature-rich parsing.
In Proc CONLL, pages 9?16.X.
Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proc.
CoNLL, pages957?961.M.
Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments withperceptron algorithms.
In Proc.
EMNLP, page 8.M.
Collins.
2003.
Head-driven statistical models for nat-ural language parsing.
In Computational linguistics,volume 29, pages 589?637.G.B.
Dantzig and P. Wolfe.
1960.
Decomposition princi-ple for linear programs.
In Operations research, vol-ume 8, pages 101?111.J.
Duchi, D. Tarlow, G. Elidan, and D. Koller.
2007.Using combinatorial optimization within max-productbelief propagation.
In NIPS, volume 19.J.
Eisner.
2000.
Bilexical grammars and their cubic-timeparsing algorithms.
In Advances in Probabilistic andOther Parsing Technologies, pages 29?62.A.
Globerson and T. Jaakkola.
2007.
Fixing max-product: Convergent message passing algorithms forMAP LP-relaxations.
In NIPS, volume 21.N.
Komodakis, N. Paragios, and G. Tziritas.
2007.MRF optimization via dual decomposition: Message-passing revisited.
In International Conference onComputer Vision.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In Proc.
ACL/HLT.T.
Koo, A.M.
Rush, M. Collins, T. Jaakkola, and D. Son-tag.
2010.
Dual Decomposition for Parsing with Non-Projective Head Automata.
In Proc.
EMNLP, pages63?70.B.H.
Korte and J. Vygen.
2008.
Combinatorial optimiza-tion: theory and algorithms.
Springer Verlag.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1994.
Building a large annotated corpus of English:The Penn Treebank.
In Computational linguistics, vol-ume 19, pages 313?330.R.K.
Martin, R.L.
Rardin, and B.A.
Campbell.
1990.Polyhedral characterization of discrete dynamic pro-gramming.
Operations research, 38(1):127?138.A.F.T.
Martins, N.A.
Smith, and E.P.
Xing.
2009.
Con-cise integer linear programming formulations for de-pendency parsing.
In Proc.
ACL.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005.Non-projective dependency parsing using spanningtree algorithms.
In Proc.
HLT/EMNLP, pages 523?530.Angelia Nedic?
and Asuman Ozdaglar.
2009.
Approxi-mate primal solutions and rate analysis for dual sub-gradient methods.
SIAM Journal on Optimization,19(4):1757?1780.B.
Pang and L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proc.
ACL.S.
Riedel and J. Clarke.
2006.
Incremental integer linearprogramming for non-projective dependency parsing.In Proc.
EMNLP, pages 129?137.D.
Roth and W. Yih.
2005.
Integer linear program-ming inference for conditional random fields.
In Proc.ICML, pages 737?744.Hanif D. Sherali and Warren P. Adams.
1994.
A hi-erarchy of relaxations and convex hull characteriza-tions for mixed-integer zero?one programming prob-lems.
Discrete Applied Mathematics, 52(1):83 ?
106.D.A.
Smith and J. Eisner.
2008.
Dependency parsing bybelief propagation.
In Proc.
EMNLP, pages 145?156.D.
Sontag, T. Meltzer, A. Globerson, T. Jaakkola, andY.
Weiss.
2008.
Tightening LP relaxations for MAPusing message passing.
In Proc.
UAI.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004.
Max-margin parsing.
In Proc.
EMNLP,pages 1?8.K.
Toutanova and C.D.
Manning.
2000.
Enriching theknowledge sources used in a maximum entropy part-of-speech tagger.
In Proc.
EMNLP, pages 63?70.M.
Wainwright and M. I. Jordan.
2008.
Graphical Mod-els, Exponential Families, and Variational Inference.Now Publishers Inc., Hanover, MA, USA.M.
Wainwright, T. Jaakkola, and A. Willsky.
2005a.MAP estimation via agreement on trees: message-passing and linear programming.
In IEEE Transac-tions on Information Theory, volume 51, pages 3697?3717.M.
Wainwright, T. Jaakkola, and A. Willsky.
2005b.
Anew class of upper bounds on the log partition func-tion.
In IEEE Transactions on Information Theory,volume 51, pages 2313?2335.C.
Yanover, T. Meltzer, and Y. Weiss.
2006.
LinearProgramming Relaxations and Belief Propagation?AnEmpirical Study.
In The Journal of Machine LearningResearch, volume 7, page 1907.
MIT Press.11
