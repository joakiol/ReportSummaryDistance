Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1066?1076,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsTranslation with Source Constituency and Dependency TreesFandong Meng??
Jun Xie?
Linfeng Song??
Yajuan Lu??
Qun Liu??
?Key Laboratory of Intelligent Information ProcessingInstitute of Computing Technology, Chinese Academy of Sciences?University of Chinese Academy of Sciences{mengfandong,xiejun,songlinfeng,lvyajuan}@ict.ac.cn?Centre for Next Generation LocalisationFaculty of Engineering and Computing, Dublin City Universityqliu@computing.dcu.ieAbstractWe present a novel translation model, whichsimultaneously exploits the constituency anddependency trees on the source side, to com-bine the advantages of two types of trees.
Wetake head-dependents relations of dependencytrees as backbone and incorporate phrasal n-odes of constituency trees as the source sideof our translation rules, and the target side asstrings.
Our rules hold the property of longdistance reorderings and the compatibilitywith phrases.
Large-scale experimental result-s show that our model achieves significantlyimprovements over the constituency-to-string(+2.45 BLEU on average) and dependency-to-string (+0.91 BLEU on average) model-s, which only employ single type of trees,and significantly outperforms the state-of-the-art hierarchical phrase-based model (+1.12BLEU on average), on three Chinese-EnglishNIST test sets.1 IntroductionIn recent years, syntax-based models have become ahot topic in statistical machine translation.
Accord-ing to the linguistic structures, these models can bebroadly divided into two categories: constituency-based models (Yamada and Knight, 2001; Graehland Knight, 2004; Liu et al 2006; Huang et al2006), and dependency-based models (Lin, 2004;Ding and Palmer, 2005; Quirk et al 2005; Xionget al 2007; Shen et al 2008; Xie et al 2011).These two kinds of models have their own advan-tages, as they capture different linguistic phenome-na.
Constituency trees describe how words and se-quences of words combine to form constituents, andconstituency-based models show better compatibil-ity with phrases.
However, dependency trees de-scribe the grammatical relation between words ofthe sentence, and represent long distance dependen-cies in a concise manner.
Dependency-based mod-els, such as dependency-to-string model (Xie et al2011), exhibit better capability of long distance re-orderings.In this paper, we propose to combine the advan-tages of source side constituency and dependencytrees.
Since the dependency tree is structurally sim-pler and directly represents long distance depen-dencies, we take dependency trees as the backboneand incorporate constituents to them.
Our mod-el employs rules that represent the source side ashead-dependents relations which are incorporatedwith constituency phrasal nodes, and the target sideas strings.
A head-dependents relation (Xie et al2011) is composed of a head and all its dependents independency trees, and it encodes phrase pattern andsentence pattern (typically long distance reorderingrelations).
With the advantages of head-dependentsrelations, the translation rules of our model hold theproperty of long distance reorderings and the com-patibility with phrases.Our new model (Section 2) extracts rules fromword-aligned pairs of source trees (constituencyand dependency) and target strings (Section 3), andtranslate source trees into target strings by employ-ing a bottom-up chart-based algorithm (Section 4).Compared with the constituency-to-string (Liu et al2006) and dependency-to-string (Xie et al 2011)models that only employ a single type of trees, our1066??/VV??
?/NR ?/AD ???/NN?
?/NR ?/M ??/JJ??/ODNP1VP2VP3?????
?
?????
?
???
?NR AD VV NR OD M JJ NNNP1CLPQPNPNPVP2ADVPVP3NPIP(a)(c)Intel         will   launch  Asia     first              super     laptopChinese: ???
?
??
??
??
?
??
??
?English:  Intel will launch the first Ultrabook in AsiaADVP NP(b)Figure 1: Illustration of phrases that can not be captured by a dependency tree (b) while captured by a constituency tree(a), where the bold phrasal nodes NP1,VP2,VP3 indicate the phrases which can not be captured by dependency syn-tactic phrases.
(c) is the corresponding bilingual sentences.
The subscripts of phrasal nodes are used for distinguishingthe nodes with same phrasal categories.approach yields encouraging results by exploiting t-wo types of trees.
Large-scale experiments (Sec-tion 5) on Chinese-English translation show thatour model significantly outperforms the state-of-the-art single constituency-to-string model by av-eraged +2.45 BLEU points, dependency-to-stringmodel by averaged +0.91 BLEU points, and hierar-chical phrase-based model (Chiang, 2005) by aver-aged +1.12 BLEU points, on three Chinese-EnglishNIST test sets.2 GrammarWe take head-dependents relations of dependencytrees as backbone and incorporate phrasal nodes ofconstituency trees as the source side of our transla-tion rules, and the target side as strings.
A head-dependents relation consists of a head and all its de-pendents in dependency trees, and it can representlong distance dependencies.
Incorporating phrasalnodes of constituency trees into head-dependentsrelations further enhances the compatibility withphrases of our rules.
Figure 1 shows an example ofphrases which can not be captured by a dependen-cy tree while captured by a constituency tree, suchas the bold phrasal nodes NP1,VP2 and VP3.
Thephrasal node NP1 in the constituency tree indicatesthat ???
)P?
is a noun phrase and it shouldbe translated as a basic unit, while in the depen-dency tree it is a non-syntactic phrase.
The head-dependents relation in the top level of the dependen-cy tree presents long distance dependencies of thewords ?=A?, ??
?, ???
?, and ?
)P?
in aconcise manner, which is useful for long distance re-ordering.
We adopt this kind of rule representationto hold the property of long distance reorderings andthe compatibility with phrases.Figure 2 shows two examples of our translationrules corresponding to the top level of Figure 1-(b).We can see that r1 captures a head-dependents rela-tion, while r2 extends r1 by incorporating a phrasalnode VP2 to replace the two nodes ???/VV?
and?
)P/NN?.
As shown in Figure 1-(b), VP2 con-sists of two parts, a head node ???/VV?
and asubtree rooted at the dependent node ?
)P/NN?.Therefore, we use VP2 and the POS tags of the t-wo nodes VV and NN to denote the part coveredby VP2 in r2, to indicate that the source sequencecovered by VP2 can be translated by a bilingualphrase.
Since VP2 covers a head node ??
?/VV?,we represent r2 by constructing a new head node10671?????
?
1121 2???
?
1Figure 2: Two examples of our translation rules corre-sponding to the top level of Figure 1-(b).
r1 captures ahead-dependents relation, and r2 extends r1 by incorpo-rating a phrasal node VP2.
?x1:NN?
indicates a substitu-tion site which can be replaced by a subtree whose roothas POS tag ?NN?.
?x1:VP2|||VV NN?
indicates a sub-stitution site which can be replaced by a source phrasecovered by a phrasal node VP (the phrasal node consist-s of two dependency nodes with POS tag VV and NN,respectively).
The underline denotes a leaf node.VP2|||VV NN.
For simplicity, we use a shorten for-m CHDR to represent the head-dependents relationswith/without constituency phrasal nodes.Formally, our grammar G is defined as a 5-tupleG = ?
?, Nc, Nd,?, R?, where ?
is a set of sourcelanguage terminals, Nc is a set of constituencyphrasal categories, Nd is a set of categories (POStags) for the terminals in ?, ?
is a set of target lan-guage terminals, and R is a set of translation rulesthat include bilingual phrases for translating sourcelanguage terminals and CHDR rules for translationand reordering.
A CHDR rule is represented as atriple ?t, s,?
?, where:?
t is CHDR with each node labeled by a ter-minal from ?
or a variable from a set X ={x1, x2, ?
?
? }
constrained by a terminal from ?or a category from Nd or a joint category (con-structed by the categories from Nc and Nd);?
s ?
(X ??)
denotes the target side string;?
?
denotes one-to-one links between nontermi-nals in t and variables in s.We use the lexicon dependency grammar (Hellwig,2006) which adopts a bracket representation to ex-press the head-dependents relation and CHDR.
Forexample, the left-hand sides of r1 and r2 in Figure 2can be respectively represented as follows:(=A) (?)??
(x1:NN)(=A) (?)
x1:VP2|||VV NN??/VV??
?/NR ?/AD ???/NN?
?/NR?/M ??/JJ??/ODNP1VP2VP3???
?
??
??
??
?
??
??
?Parseing      Labelling??
?/NR ?/AD launchIntel will launch ?????
in AsiaIntel will launch in Asia(a)(b)(c)(d)(e)NP1?/M?
?/ODIntel will launch in Asiathe    first(f)UltrabookUltrabook???/NN?
?/NR?/M ??/JJ?
?/ODNP1r3r4 r5r6r7?/M?
?/ODr8(x1:NR) (x2:AD) ??
(x3:???)
x1 x2 launch x3Intel????
will(??
)(x1:M)x2:NP1|||JJ_NN x1 x2 in Aisa?????
Ultrabook??
(?)
the firstTranslation Rulesr3r4r5r6r7r8(g)Figure 3: An example derivation of translation.
(g) listsall the translation rules.
r3, r6 and r8 are CHDR rules,while r4, r5 and r7 are bilingual phrases, which are usedfor translating source terminals.
The dash lines indicatethe reordering when employing a translation rule.The formalized presentation of r2 in Figure 2-(b):t = (=A) (?)
x1:VP2|||VV NNs = Intel will x1?= x1:VP2|||VV NN ?
x1where the underline indicates a leaf node.Figure 3 gives an example of the translationderivation in our model, with the translation rules1068listed in (g).
r3, r6 and r8 are CHDR rules, whiler4, r5 and r7 are bilingual phrases, which are usedfor translating source language terminals.
Given asentence to translate in (a), we first parse it into aconstituency tree and a dependency tree, then labelthe phrasal nodes from the constituency tree to thedependency tree, and yield (b).
Then, we translateit into a target string by the following steps.
At theroot node, we apply rule r3 to translate the top levelhead-dependents relation and results in four unfin-ished substructures and target strings in (c).
From(c) to (d), there are three steps (one rule for one step).We use r4 to translate ?=A?
to ?Intel?, r5 totranslate ???
to ?will?, and r6 to translate the right-most unfinished part.
Then, we apply r7 to translatethe phrase ???
)P?
to ?Ultrabook?, and yield(e).
Finally, we apply r8 to translate the last frag-ment to ?the first?, and get the final result (f).3 Rule ExtractionIn this section, we describe how to extract rules froma set of 4-tuples ?C, T, S,A?, where C is a sourceconstituency tree, T is a source dependency tree, Sis a target side sentence, and A is an word alignmen-t relation between T /C and S. We extract CHDRrules from each 4-tuple ?C, T, S,A?
based on GHK-M algorithm (Galley et al 2004) with three steps:1.
Label the dependency tree with phrasal nodesfrom the constituency tree, and annotate align-ment information to the phrasal nodes labeleddependency tree (Section 3.1).2.
Identify acceptable CHDR fragments from theannotated dependency tree for rule induction(Section 3.2).3.
Induce a set of lexicalized and generalizedCHDR rules from the acceptable fragments(Section 3.3).3.1 AnnotationGiven a 4-tuple ?C, T, S,A?, we first label phrasalnodes from the constituency tree C to the depen-dency tree T , which can be easily accomplished byphrases mapping according to the common coveredsource sequences.
As dependency trees can capturesome phrasal information by dependency syntactic??/VV{3-3}{1-8}???/NR{1-1}{1-1}?/AD{2-2}{2-2}???/NN{6-6}{4-8}??/NR{7-8}{7-8}?/M{null}{4-5}??/JJ{6-6}{6-6}?
?/OD{4-5}{4-5}NP1<6-6>VP2<3-8>VP3<2-8>Figure 4: An annotated dependency tree.
Each node isannotated with two spans, the former is node span andthe latter subtree span.
The fragments covered by phrasalnodes are annotated with phrasal spans.
The nodes de-noted by the solid line box are not nsp consistent.phrases, in order to complement the information thatdependency trees can not capture, we only label thephrasal nodes that cover dependency non-syntacticphrases.Then, we annotate alignment information to thephrasal nodes labeled dependency tree T , as shownin Figure 4.
For description convenience, we makeuse of the notion of spans (Fox, 2002; Lin, 2004).Given a node n in the source phrasal nodes labeledT with word alignment information, the spans of ninduced by the word alignment are consecutive se-quences of words in the target sentence.
As shownin Figure 4, we annotate each node n of phrasal n-odes labeled T with two attributes: node span andsubtree span; besides, we annotate phrasal span tothe parts covered by phrasal nodes in each subtreerooted at n. The three types of spans are defined asfollows:Definition 1 Given a node n, its node span nsp(n)is the consecutive target word sequence aligned withthe node n.Take the node ???/NR?
in Figure 4 for example,nsp(?
?/NR)={7-8}, which corresponds to the tar-get words ?in?
and ?Asia?.Definition 2 Given a subtree T ?
rooted at n, thesubtree span tsp(n) of n is the consecutive targetword sequence from the lower bound of the nsp of1069all nodes in T ?
to the upper bound of the same set ofspans.For instance, tsp()P/NN)={4-8}, which corre-sponds to the target words ?the first Ultrabook in A-sia?, whose indexes are from 4 to 8.Definition 3 Given a fragment f covered by aphrasal node, the phrasal span psp(f) of f isthe consecutive target word sequence aligned withsource string covered by f .For example, psp(VP2)=?3-8?, which correspondsto the target word sequence ?launch the first Ultra-book in Asia?.We say nsp, tsp and psp are consistent accordingto the notion in the phrase-based model (Koehn etal., 2003).
For example, nsp(?
?/NR), tsp()P/NN) and psp(NP1) are consistent while nsp(?
?/JJ) and nsp()P/NN) are not consistent.The annotation can be achieved by a single pos-torder transversal of the phrasal nodes labeled de-pendency tree.
For simplicity, we call the annotat-ed phrasal nodes labeled dependency tree annotateddependency tree.
The extraction of bilingual phrases(including the translation of head node, dependen-cy syntactic phrases and the fragment covered by aphrasal node) can be readily achieved by the algo-rithm described in Koehn et al (2003).
In the fol-lowing, we focus on CHDR rules extraction.3.2 Acceptable Fragments IdentificationBefore present the method of acceptable fragmentsidentification, we give a brief description of CHDRfragments.
A CHDR fragment is an annotated frag-ment that consists of a source head-dependents rela-tion with/without constituency phrasal nodes, a tar-get string and the word alignment information be-tween the source and target side.
We identify the ac-ceptable CHDR fragments that are suitable for ruleinduction from the annotated dependency tree.
Wedivide the acceptable CHDR fragments into two cat-egories depending on whether the fragments con-tain phrasal nodes.
If an acceptable CHDR frag-ment does not contain phrasal nodes, we call itCHDR-normal fragment, otherwise CHDR-phrasalfragment.
Given a CHDR fragment F rooted at n,we say F is acceptable if it satisfies any one of thefollowing properties:CHDR-phrasal Rulesr9: (???)(?
)x1:VP2|||VV_NN Intel will x1r10: (x1:NR)(x2:AD)x3:VP2|||VV_NN x1 x2 x3r11: (???
)x1:VP3|||AD_VV_NN Intel x1r12: (x1:NR)x2:VP3|||AD_VV_NN x1 x2CHDR-normal Rulesr4: (x1:NR) (x2:AD) ??
(x3:NN) x1 x2 launch x3Intel will launch x1r3: (???)
(?)??
(x1:NN)r2: (x1:NR) (x2:AD) ??
(x3:???)
x1 x2 launch x3r1: (???)
(?)??
(x1:???)
Intel will launch x1r5: (???)
(?)
x1:VV (x2:???)
Intel will x1 x2r8: (x1:NR) (x2:AD) x3:VV (x4:NN) x1 x2 x3 x4r6: (x1:NR) (x2:AD) x3:VV (x4:???)
x1 x2 x3 x4Intel will x1 x2r7: (???)
(?)
x1:VV (x2:NN)(d)??/VV??
?/NR ?/AD ??
?/NNIntel1will2launch3the first Ultrabook in Asia4-8(a)Intel1will2launch the first Ultrabook in Asia3-8VP2??/VV??
?/NR ?/AD ??
?/NN(b)(c)Intel1will launch the first Ultrabook in Asia2-8VP3??/VV??
?/NR ?/AD ??
?/NNVP2|||VV_NNVP3|||AD_VV_NNFigure 5: Examples of a CHDR-normal fragment (a), twoCHDR-phrasal fragments (b) and (c) that are identifiedfrom the top level of the annotated dependency tree inFigure 4, and the corresponding CHDR rules (d) inducedfrom (a), (b) and (c).
The underline denotes a leaf node.1.
Without phrasal nodes, the node span of theroot n is consistent and the subtree spans ofn?s all dependents are consistent.
For example,Figure 5-(a) shows a CHDR-normal fragmen-t that identified from the top level of the an-notated dependency tree in Figure 4, since thensp(?
?/VV), tsp(=A/NR), tsp(?/AD)and tsp()P/NN) are consistent.10702.
With phrasal nodes, the phrasal spans ofphrasal nodes are consistent; and for the othernodes, the node span of head (if it is not cov-ered by any phrasal node) is consistent, and thesubtree spans of dependents are consistent.
Forinstance, Figure 5-(b) and (c) show two CHDR-phrasal fragments identified from the top levelof Figure 4.
In Figure 5-(b), psp(VP2), tsp(=A/NR) and tsp(?/AD) are consistent.
InFigure 5-(c), psp(VP3) and tsp(=A/NR)are consistent.The identification of acceptable fragments can beachieved by a single postorder transversal of the an-notated dependency tree.
Typically, each acceptablefragment contains at most three types of nodes: headnode, head of the related CHDR; internal nodes, in-ternal nodes of the related CHDR except head node;leaf nodes, leaf nodes of the related CHDR.3.3 Rule InductionFrom each acceptable CHDR fragment, we inducea set of lexicalized and generalized CHDR rules.We induce CHDR-normal rules and CHDR-phrasalrules from CHDR-normal fragments and CHDR-phrasal fragments, respectively.We first induce a lexicalized form of CHDR rulefrom an acceptable CHDR fragment:1.
For a CHDR-normal fragment, we first markthe internal nodes as substitution sites.
Thisforms the input of a CHDR-normal rule.
Thenwe generate the target string according to thenode span of the head and the subtree spans ofthe dependents, and turn the word sequencescovered by the internal nodes into variables.This forms the output of a lexicalized CHDR-normal rule.2.
For a CHDR-phrasal fragment, we first markthe internal nodes and the phrasal nodes as sub-stitution sites.
This forms the input of a CHDR-phrasal rule.
Then we construct the output ofthe CHDR-phrasal rule in almost the same waywith constructing CHDR-normal rules, exceptthat we replace the target sequences covered bythe internal nodes and the phrasal nodes withvariables.For example, rule r1 in Figure 5-(d) is a lexicalizedCHDR-normal rule induced from the CHDR-normalfragment in Figure 5-(a).
r9 and r11 are CHDR-phrasal rules induced from the CHDR-phrasal frag-ment in Figure 5-(b) and Figure 5-(c) respectively.As we can see, these CHDR-phrasal rules are par-tially unlexicalized.To alleviate the sparseness problem, we gener-alize the lexicalized CHDR-normal rules and par-tially unlexicalized CHDR-phrasal rules with un-lexicalized nodes by the method proposed in Xieet al (2011).
As the modification relations be-tween head and dependents are determined by theedges, we can replace the lexical word of each n-ode with its category (POS tag) and obtain newhead-dependents relations with unlexicalized nodeskeeping the same modification relations.
We gen-eralize the rule by simultaneously turn the nodes ofthe same type (head, internal, leaf) into their cate-gories.
For example, CHDR-normal rules r2 ?
r7are generalized from r1 in Figure 5-(d).
Besides, r10and r12 are the corresponding generalized CHDR-phrasal rules.
Actually, our CHDR rules are the su-perset of head-dependents relation rules in Xie etal., (2011).
CHDR-normal rules are equivalent withthe head-dependents relation rules and the CHDR-phrasal rules are the extension of these rules.
Forconvenience of description, we use the subscript todistinguish the phrasal nodes with the same catego-ry, such as VP2 and VP3.
In actual operation, we useVP instead of VP2 and VP3.We handle the unaligned words of the target sideby extending the node spans of the lexicalized headand leaf nodes, and the subtree spans of the lexical-ized dependents, on both left and right directions.This procedure is similar with the method of Ochand Ney, (2004).
During this process, we might ob-tain m(m ?
1) CHDR rules from an acceptablefragment.
Each of these rules is assigned with a frac-tional count 1/m.
We take the extracted rule set asobserved data and make use of relative frequency es-timator to obtain the translation probabilities P (t|s)and P (s|t).4 Decoding and the ModelFollowing Och and Ney, (2002), we adopt a generalloglinear model.
Let d be a derivation that convert a1071source phrasal nodes labeled dependency tree into atarget string e. The probability of d is defined as:P (d) ?
?i?i(d)?i (1)where ?i are features defined on derivations and ?iare feature weights.
In our experiments of this paper,the features are used as follows:?
CHDR rules translation probabilities P (t|s)and P (s|t), and CHDR rules lexical translationprobabilities Plex(t|s) and Plex(s|t);?
bilingual phrases translation probabilitiesPbp(t|s) and Pbp(s|t), and bilingual phraseslexical translation probabilities Pbplex(t|s) andPbplex(s|t);?
rule penalty exp(?1);?
pseudo translation rule penalty exp(?1);?
target word penalty exp(|e|);?
language model Plm(e).We have twelve features in our model.
The values ofthe first four features are accumulated on the CHDRrules and the next four features are accumulated onthe bilingual phrases.
We also use a pseudo transla-tion rule (constructed according to the word order ofhead-dependents relation) as a feature to guaranteethe complete translation when no matched rules canbe found during decoding.Our decoder is based on bottom-up chart-basedalgorithm.
It finds the best derivation that convertthe input phrasal nodes labeled dependency tree intoa target string among all possible derivations.
Giv-en the source constituency tree and dependency tree,we first generate phrasal nodes labeled dependencytree T as described in Section 3.1, then the decodertransverses each node in T by postorder.
For eachnode n, it enumerates all instances of CHDR rootedat n, and checks the rule set for matched translationrules.
A larger translation is generated by substitut-ing the variables in the target side of a translationrule with the translations of the corresponding de-pendents.
Cube pruning (Chiang, 2007; Huang andChiang, 2007) is used to find the k-best items withintegrated language model for each node.To balance the performance and speed of the de-coder, we limit the search space by reducing thenumber of translation rules used for each node.There are two ways to limit the rule table size: bya fixed limit (rule-limit) of how many rules are re-trieved for each input node, and by a threshold (rule-threshold) to specify that the rule with a score low-er than ?
times of the best score should be discard-ed.
On the other hand, instead of keeping the fulllist of candidates for a given node, we keep a top-scoring subset of the candidates.
This can also bedone by a fixed limit (stack-limit) and a threshold(stack-threshold).5 ExperimentsWe evaluated the performance of our model by com-paring with hierarchical phrase-based model (Chi-ang, 2007), constituency-to-string model (Liu et al2006) and dependency-to-string model (Xie et al2011) on Chinese-English translation.
First, we de-scribe data preparation (Section 5.1) and systems(Section 5.2).
Then, we validate that our model sig-nificantly outperforms all the other baseline models(Section 5.3).
Finally, we give detail analysis (Sec-tion 5.4).5.1 Data PreparationOur training data consists of 1.25M sentence pairsextracted from LDC 1 data.
We choose NIST MTEvaluation test set 2002 as our development set,NIST MT Evaluation test sets 2003 (MT03), 2004(MT04) and 2005 (MT05) as our test sets.
The qual-ity of translations is evaluated by the case insensitiveNIST BLEU-4 metric 2.We parse the source sentences to constituencytrees (without binarization) and projective depen-dency trees with Stanford Parser (Klein and Man-ning, 2002).
The word alignments are obtained byrunning GIZA++ (Och and Ney, 2003) on the corpusin both directions and using the ?grow-diag-final-and?
balance strategy (Koehn et al 2003).
We getbilingual phrases from word-aligned data with algo-rithm described in Koehn et al(2003) by runningMoses Toolkit 3.
We apply SRI Language ModelingToolkit (Stolcke and others, 2002) to train a 4-gram1Including LDC2002E18, LDC2003E07, LDC2003E14,Hansards portion of LDC2004T07, LDC2004T08 and LD-C2005T06.2ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl3http://www.statmt.org/moses/1072System Rule # MT03 MT04 MT05 AverageMoses-chart 116.4M 34.65 36.47 34.39 35.17cons2str 25.4M+32.5M 33.14 35.12 33.27 33.84dep2str 19.6M+32.5M 34.85 36.57 34.72 35.38consdep2str 23.3M+32.5M 35.57* 37.68* 35.62* 36.29Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets of different systems.The ?+?
denotes that the rules are composed of syntactic translation rules and bilingual phrases (32.5M).
The ?
*?denotes that the results are significantly better than all the other systems (p<0.01).language model with modified Kneser-Ney smooth-ing on the Xinhua portion of the English Gigawordcorpus.
We make use of the standard MERT (Och,2003) to tune the feature weights in order to maxi-mize the system?s BLEU score on the developmentset.
The statistical significance test is performed bysign-test (Collins et al 2005).5.2 SystemsWe take the open source hierarchical phrase-basedsystem Moses-chart (with default configuration),our in-house constituency-to-string system cons2strand dependency-to-string system dep2str as ourbaseline systems.For cons2str, we follow Liu et al (Liu et al2006) to strict that the height of a rule tree is nogreater than 3 and phrase length is no greater than7.
To keep consistent with our proposed model,we implement the dependency-to-string model (X-ie et al 2011) with GHKM (Galley et al 2004)rule extraction algorithm and utilize bilingual phras-es to translate source head node and dependencysyntactic phrases.
Our dep2str shows comparableperformance with Xie et al (2011), which can beseen by comparing with the results of hierarchicalphrase-based model in our experiments.
For dep2strand our proposed model consdep2str, we set rule-threshold and stack-threshold to 10?3, rule-limit to100, stack-limit to 300, and phrase length limit to 7.5.3 Experimental ResultsTable 1 illustrates the translation results of our ex-periments.
As we can see, our consdep2str sys-tem has gained the best results on all test sets, with+1.12 BLEU points higher than Moses-chart, +2.45BLEU points higher than cons2str, and +0.91 BLEUpoints higher than dep2str, averagely on MT03,MT04 and MT05.
Our model significantly outper-forms all the other baseline models, with p<0.01on statistical significance test sign-test (Collins etal., 2005).
By exploiting two types of trees onsource side, our model gains significant improve-ments over constituency-to-string and dependency-to-string models, which employ single type of trees.Table 1 also lists the statistical results of rules ex-tracted from training data by different systems.
Ac-cording to our statistics, the number of rules extract-ed by our consdep2str system is about 18.88% largerthan dep2str, without regard to the 32.5M bilingualphrases.
The extra rules are CHDR-phrasal rules,which can bring in BLEU improvements by enhanc-ing the compatibility with phrases.
We will conducta deep analysis in the next sub-section.5.4 AnalysisIn this section, we first illustrate the influence ofCHDR-phrasal rules in our consdep2str model.
Wecalculate the proportion of 1-best translations in testsets that employ CHDR-phrasal rules, and we cal-l this proportion ?CHDR-phrasal Sent.?.
Besides,the proportion of CHDR-phrasal rules in all CHDRrules is calculated in these translations, and we cal-l this proportion ?CHDR-phrasal Rule?.
Table 2lists the using of CHDR-phrasal rules on test sets,showing that CHDR-phrasal Sent.
on all test setsare higher than 50%, and CHDR-phrasal Rule on al-l three test sets are higher than 10%.
These resultsindicate that CHDR-phrasal rules do play a role indecoding.Furthermore, we compare some actual transla-tions of our test sets generated by cons2str, de-p2str and consdep2str systems, as shown in Fig-ure 6.
In the first example, the Chinese input hold-s long distance dependencies ??
?I ??
?...
\u ... L?
'?
?, which correspondto the sentence pattern ?noun+adverb+prepositional1073System MT03 MT04 MT05CHDR-phrasal Sent.
50.71 61.80 56.19CHDR-phrasal Rule 10.53 13.55 10.83Table 2: The proportion (%) of 1-best translations thatemploys CHDR-phrasal rules (CHDR-phrasal Sent.)
andthe proportion (%) of CHDR-phrasal rules in all CHDRrules in these translations (CHDR-phrasal Rule).phrase+verb+noun?.
Cons2str gives a bad resultwith wrong global reordering, while our consdep2strsystem gains an almost correct result since we cap-ture this pattern by CHDR-normal rules.
In the sec-ond example, we can see that the Chinese phrase?2g?y?
is a non-syntactic phrase in the depen-dency tree, and this phrase can not be captured byhead-dependents relation rules in Xie et al (2011),thus can not be translated as one unit.
Since we en-code constituency phrasal nodes to the dependencytree, ?2g?y?
is labeled by a phrasal node ?VP?
(means verb phrase), which can be captured by ourCHDR-phrasal rules and translated into the correctresult ?reemergence?
with bilingual phrases.By combining the merits of constituency anddependency trees, our consdep2str model learnsCHDR-normal rules to acquire the property of longdistance reorderings and CHDR-phrasal rules to ob-tain good compatibility with phrases.6 Related WorkIn recent years, syntax-based models have witnessedpromising improvements.
Some researchers makeefforts on constituency-based models (Graehl andKnight, 2004; Liu et al 2006; Huang et al 2006;Zhang et al 2007; Mi et al 2008; Liu et al 2009;Liu et al 2011; Zhai et al 2012).
Some works payattention to dependency-based models (Lin, 2004;Ding and Palmer, 2005; Quirk et al 2005; Xiong etal., 2007; Shen et al 2008; Xie et al 2011).
Thesemodels are based on single type of trees.There are also some approaches combining mer-its of different structures.
Marton and Resnik (2008)took the source constituency tree into account andadded soft constraints to the hierarchical phrase-based model (Chiang, 2005).
Cherry (2008) u-tilized dependency tree to add syntactic cohesionto the phrased-based model.
Mi and Liu, (2010)proposed a constituency-to-dependency translationmodel, which utilizes constituency forests on thesource side to direct the translation, and depen-dency trees on the target side to ensure grammati-cality.
Feng et al(2012) presented a hierarchicalchunk-to-string translation model, which is a com-promise between the hierarchical phrase-based mod-el and the constituency-to-string model.
Most work-s make effort to introduce linguistic knowledge in-to the phrase-based model and hierarchical phrase-based model with constituency trees.
Only the workproposed by Mi and Liu, (2010) utilized constituen-cy and dependency trees, while their work appliedtwo types of trees on two sides.Instead, our model simultaneously utilizes con-stituency and dependency trees on the source side todirect the translation, which is concerned with com-bining the advantages of two types of trees in trans-lation rules to advance the state-of-the-art machinetranslation.7 ConclusionIn this paper, we present a novel model that si-multaneously utilizes constituency and dependencytrees on the source side to direct the translation.
Tocombine the merits of constituency and dependen-cy trees, our model employs head-dependents rela-tions incorporating with constituency phrasal nodes.Experimental results show that our model exhibitsgood performance and significantly outperforms thestate-of-the-art constituency-to-string, dependency-to-string and hierarchical phrase-based models.
Forthe first time, source side constituency and depen-dency trees are simultaneously utilized to direct thetranslation, and the model surpasses the state-of-the-art translation models.Since constituency tree binarization can leadto more constituency-to-string rules and syntacticphrases in rule extraction and decoding, which im-prove the performance of constituency-to-string sys-tems, for future work, we would like to do researchon encoding binarized constituency trees to depen-dency trees to improve translation performance.AcknowledgmentsThe authors were supported by National Natural Sci-ence Foundation of China (Contracts 61202216),1074MT05 ---- segment 448???
??
?
??
??
???
??
??
?
??
??
??
?cons2srt: united nations with the indonesian government have expressed concern over the time limit for foreign troops .consdep2srt: the united nations has expressed concern over the deadline of the indonesian government on foreign troops .reference: The United Nations has expressed concern over the deadline the Indonesian government imposed on foreign troops.???
??
??
??
???
??
??
?
???
??
??
?dobjpobjprepadvmodnsubjpnuctthe united nations has the deadline of the indonesian government on foreign troopsexpressed concern over .??
??
??
?
??
??
???
???
6$56??
?
?dep2srt: ??
again severe acute respiratory syndrome ( SARS ) case ?
?consdep2srt: ??
reemergence of a severe acute respiratory syndrome ( SARS ) case?
?reference: ??
the reemergence of a severe acute respiratory syndrome (SARS) case ?
?MT04 ---- segment 194dep cons & dep??/VV?
?/AD ?/DEGVPreemergence???/NN?
?/JJ ??/JJ??/VV?
?/AD ?/DEGagain???/NN?
?/JJ ?
?/JJFigure 6: Actual examples translated by the cons2str, dep2str and consdep2str systems.863 State Key Project (No.
2011AA01A207),and National Key Technology R&D Program (No.2012BAH39B03), Key Project of Knowledge Inno-vation Program of Chinese Academy of Sciences(No.
KGZD-EW-501).
Qun Liu.s work waspartially supported by Science Foundation Ireland(Grant No.
07/CE/I1142) as part of the CNGLat Dublin City University.
Sincere thanks to theanonymous reviewers for their thorough reviewingand valuable suggestions.
We appreciate Haitao Mi,Zhaopeng Tu and Anbang Zhao for insightful ad-vices in writing.ReferencesColin Cherry.
2008.
Cohesive phrase-based decoding forstatistical machine translation.
In ACL, pages 72?80.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting on Association for Computa-tional Linguistics, pages 263?270.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 531?540.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependency in-sertion grammars.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguistic-s, pages 541?548.Yang Feng, Dongdong Zhang, Mu Li, Ming Zhou, andQun Liu.
2012.
Hierarchical chunk-to-string transla-tion.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics: LongPapers-Volume 1, pages 950?958.Heidi J Fox.
2002.
Phrasal cohesion and statisticalmachine translation.
In Proceedings of the ACL-02conference on Empirical methods in natural languageprocessing-Volume 10, pages 304?3111.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule.
In Pro-1075ceedings of HLT/NAACL, volume 4, pages 273?280.Boston.Jonathan Graehl and Kevin Knight.
2004.
Training treetransducers.
In Proc.
HLT-NAACL, pages 105?112.Peter Hellwig.
2006.
Parsing with dependency gram-mars.
An International Handbook of ContemporaryResearch, 2:1081?1109.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InAnnual Meeting-Association For Computational Lin-guistics, volume 45, pages 144?151.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.
S-tatistical syntax-directed translation with extended do-main of locality.
In Proceedings of AMTA, pages 66?73.Dan Klein and Christopher D Manning.
2002.
Fast exactinference with a factored model for natural languageparsing.
In Advances in neural information processingsystems, volume 15, pages 3?10.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Conference of the North AmericanChapter of the Association for Computational Linguis-tics on Human Language Technology-Volume 1, pages48?54.Dekang Lin.
2004.
A path-based transfer model for ma-chine translation.
In Proceedings of the 20th interna-tional conference on Computational Linguistics, pages625?630.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics, pages 609?616.Yang Liu, Yajuan Lu?, and Qun Liu.
2009.
Improvingtree-to-tree translation with packed forests.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International Join-t Conference on Natural Language Processing of theAFNLP: Volume 2-Volume 2, pages 558?566.Yang Liu, Qun Liu, and Yajuan Lu?.
2011.
Adjoiningtree-to-string translation.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies-Volume1, pages 1278?1287.Yuval Marton and Philip Resnik.
2008.
Soft syntacticconstraints for hierarchical phrased-based translation.In Proceedings of ACL-08: HLT, pages 1003?1011.Haitao Mi and Qun Liu.
2010.
Constituency to depen-dency translation with forests.
In Proceedings of the48th Annual Meeting of the Association for Computa-tional Linguistics, pages 1433?1442.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL-08: HLT,pages 192?199.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native training and maximum entropy models for s-tatistical machine translation.
In Proceedings of the40th Annual Meeting on Association for Computation-al Linguistics, pages 295?302.Franz Josef Och and Hermann Ney.
2003.
A systemat-ic comparison of various statistical alignment models.Computational linguistics, 29(1):19?51.Franz Josef Och and Hermann Ney.
2004.
The alignmen-t template approach to statistical machine translation.Computational linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting on Association for Computation-al Linguistics-Volume 1, pages 160?167.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal smt.
In Proceedings of the 43rd AnnualMeeting on Association for Computational Linguistic-s, pages 271?279.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL-08: HLT, pages 577?585.Andreas Stolcke et al2002.
Srilm-an extensible lan-guage modeling toolkit.
In Proceedings of the inter-national conference on spoken language processing,volume 2, pages 901?904.Jun Xie, Haitao Mi, and Qun Liu.
2011.
A nov-el dependency-to-string model for statistical machinetranslation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 216?226.Deyi Xiong, Qun Liu, and Shouxun Lin.
2007.
A de-pendency treelet string correspondence model for s-tatistical machine translation.
In Proceedings of theSecond Workshop on Statistical Machine Translation,pages 40?47.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of the39th Annual Meeting on Association for Computation-al Linguistics, pages 523?530.Feifei Zhai, Jiajun Zhang, Yu Zhou, and ChengqingZong.
2012.
Tree-based translation without usingparse trees.
In Proceedings of COLING 2012, pages3037?3054.Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng Li,and Chew Lim Tan.
2007.
A tree-to-tree alignment-based model for statistical machine translation.
MT-Summit-07, pages 535?542.1076
