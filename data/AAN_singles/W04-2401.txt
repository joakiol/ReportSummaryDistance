A Linear Programming Formulation for Global Inference in NaturalLanguage TasksDan Roth Wen-tau YihDepartment of Computer ScienceUniversity of Illinois at Urbana-Champaign{danr, yih}@uiuc.eduAbstractGiven a collection of discrete random variablesrepresenting outcomes of learned local predic-tors in natural language, e.g., named entitiesand relations, we seek an optimal global as-signment to the variables in the presence ofgeneral (non-sequential) constraints.
Examplesof these constraints include the type of argu-ments a relation can take, and the mutual activ-ity of different relations, etc.
We develop a lin-ear programming formulation for this problemand evaluate it in the context of simultaneouslylearning named entities and relations.
Our ap-proach allows us to efficiently incorporate do-main and task specific constraints at decisiontime, resulting in significant improvements inthe accuracy and the ?human-like?
quality ofthe inferences.1 IntroductionNatural language decisions often depend on the out-comes of several different but mutually dependent predic-tions.
These predictions must respect some constraintsthat could arise from the nature of the data or from do-main or task specific conditions.
For example, in part-of-speech tagging, a sentence must have at least one verb,and cannot have three consecutive verbs.
These facts canbe used as constraints.
In named entity recognition, ?noentities can overlap?
is a common constraint used in var-ious works (Tjong Kim Sang and De Meulder, 2003).Efficient solutions to problems of these sort have beengiven when the constraints on the predictors are sequen-tial (Dietterich, 2002).
These solutions can be cate-gorized into the following two frameworks.
Learningglobal models trains a probabilistic model under the con-straints imposed by the domain.
Examples include varia-tions of HMMs, conditional models and sequential varia-tions of Markov random fields (Lafferty et al, 2001).
Theother framework, inference with classifiers (Roth, 2002),views maintaining constraints and learning classifiers asseparate processes.
Various local classifiers are trainedwithout the knowledge of constraints.
The predictionsare taken as input on the inference procedure which thenfinds the best global prediction.
In addition to the concep-tual simplicity of this approach, it also seems to performbetter experimentally (Tjong Kim Sang and De Meulder,2003).Typically, efficient inference procedures in both frame-works rely on dynamic programming (e.g., Viterbi),which works well in sequential data.
However, in manyimportant problems, the structure is more general, result-ing in computationally intractable inference.
Problems ofthese sorts have been studied in computer vision, whereinference is generally performed over low level measure-ments rather than over higher level predictors (Levin etal., 2002; Boykov et al, 2001).This work develops a novel inference with classifiersapproach.
Rather than being restricted on sequential data,we study a fairly general setting.
The problem is definedin terms of a collection of discrete random variables rep-resenting binary relations and their arguments; we seekan optimal assignment to the variables in the presence ofthe constraints on the binary relations between variablesand the relation types.The key insight to this solution comes from re-cent techniques developed for approximation algo-rithms (Chekuri et al, 2001).
Following this work, wemodel inference as an optimization problem, and showhow to cast it as a linear program.
Using existing numer-ical packages, which are able to solve very large linearprogramming problems in a very short time1, inferencecan be done very quickly.Our approach could be contrasted with other ap-1For example, (CPLEX, 2003) is able to solve a linear pro-gramming problem of 13 million variables within 5 minutes.proaches to sequential inference or to general Markovrandom field approaches (Lafferty et al, 2001; Taskar etal., 2002).
The key difference is that in these approaches,the model is learned globally, under the constraints im-posed by the domain.
In our approach, predictors do notneed to be learned in the context of the decision tasks,but rather can be learned in other contexts, or incorpo-rated as background knowledge.
This way, our approachallows the incorporation of constraints into decisions in adynamic fashion and can therefore support task specificinferences.
The significance of this is clearly shown inour experimental results.We develop our models in the context of natural lan-guage inferences and evaluate it here on the problem ofsimultaneously recognizing named entities and relationsbetween them.1.1 Entity and Relation RecognitionThis is the problem of recognizing the kill (KFJ, Os-wald) relation in the sentence ?J.
V. Oswald wasmurdered at JFK after his assassin,R.
U. KFJ...?
This task requires making severallocal decisions, such as identifying named entities in thesentence, in order to support the relation identification.For example, it may be useful to identify that Oswaldand KFJ are people, and JFK is a location.
This, in turn,may help to identify that the kill action is described in thesentence.
At the same time, the relation kill constrains itsarguments to be people (or at least, not to be locations)and helps to enforce that Oswald and KFJ are likely tobe people, while JFK is not.In our model, we first learn a collection of ?local?
pre-dictors, e.g., entity and relation identifiers.
At decisiontime, given a sentence, we produce a global decision thatoptimizes over the suggestions of the classifiers that areactive in the sentence, known constraints among themand, potentially, domain or tasks specific constraints rel-evant to the current decision.Although a brute-force algorithm may seem feasiblefor short sentences, as the number of entity variablegrows, the computation becomes intractable very quickly.Given n entities in a sentence, there are O(n2) possiblerelations between them.
Assume that each variable (en-tity or relation) can take l labels (?none?
is one of theselabels).
Thus, there are ln2 possible assignments, whichis too large even for a small n.When evaluated on simultaneous learning of namedentities and relations, our approach not only providesa significant improvement in the predictors?
accuracy;more importantly, it provides coherent solutions.
Whilemany statistical methods make ?stupid?
mistakes (i.e.,inconsistency among predictions), that no human evermakes, as we show, our approach improves also the qual-ity of the inference significantly.The rest of the paper is organized as follows.
Section 2formally defines our problem and section 3 describes thecomputational approach we propose.
Experimental re-sults are given in section 4, followed by some discussionand conclusion in section 5.2 The Relational Inference ProblemWe consider the relational inference problem within thereasoning with classifiers paradigm, and study a spe-cific but fairly general instantiation of this problem, moti-vated by the problem of recognizing named entities (e.g.,persons, locations, organization names) and relations be-tween them (e.g.
work for, located in, live in).
We con-sider a set V which consists of two types of variables V =E ?
R. The first set of variables E = {E1, E2, ?
?
?
, En}ranges LE .
The value (called ?label?)
assigned to Ei ?
Eis denoted fEi ?
LE .
The second set of variablesR = {Rij}{1?i,j?n;i6=j} is viewed as binary relationsover E .
Specifically, for each pair of entities Ei and Ej ,i 6= j, we use Rij and Rji to denote the (binary) relations(Ei, Ej) and (Ej , Ei) respectively.
The set of labels ofrelations is LR and the label assigned to relation Rij ?
Ris fRij ?
LR.Apparently, there exists some constraints on the labelsof corresponding relation and entity variables.
For in-stance, if the relation is live in, then the first entity shouldbe a person, and the second entity should be a location.The correspondence between the relation and entity vari-ables can be represented by a bipartite graph.
Each rela-tion variable Rij is connected to its first entity Ei , andsecond entity Ej .
We use N 1 and N 2 to denote the entityvariables of a relation Rij .
Specifically, Ei = N 1(Rij)and Ej = N 2(Rij).In addition, we define a set of constraints on the out-comes of the variables in V .
C1 : LE ?
LR ?
{0, 1}constraint values of the first argument of a relation.
C2is defined similarly and constrains the second argumenta relation can take.
For example, (born in, person) isin C1 but not in C2 because the first entity of relationborn in has to be a person and the second entity can onlybe a location instead of a person.
Note that while wedefine the constraints here as Boolean, our formalismsin fact allows for stochastic constraints.
Also note thatwe can define a large number of constraints, such asCR : LR ?
LR ?
{0, 1} which constrain types of re-lations, etc.
In fact, as will be clear in Sec.
3 the languagefor defining constraints is very rich ?
linear (in)equalitiesover V .We exemplify the framework using the problem of si-multaneous recognition of named entities and relations insentences.
Briefly speaking, we assume a learning mech-anism that can recognize entity phrases in sentences,based on local contextual features.
Similarly, we assumea learning mechanism that can recognize the semantic re-lation between two given phrases in a sentence.We seek an inference algorithm that can produce a co-herent labeling of entities and relations in a given sen-tence.
Furthermore, it follows, as best as possible therecommendation of the entity and relation classifiers, butalso satisfies natural constraints that exist on whether spe-cific entities can be the argument of specific relations,whether two relations can occur together at the sametime, or any other information that might be available atthe inference time (e.g., suppose it is known that enti-ties A and B represent the same location; one may like toincorporate an additional constraint that prevents an in-ference of the type: ?C lives in A; C does not live in B?
).We note that a large number of problems can be mod-eled this way.
Examples include problems such as chunk-ing sentences (Punyakanok and Roth, 2001), coreferenceresolution and sequencing problems in computational bi-ology.
In fact, each of the components of our problemhere, the separate task of recognizing named entities insentences and the task of recognizing semantic relationsbetween phrases, can be modeled this way.
However,our goal is specifically to consider interacting problemsat different levels, resulting in more complex constraintsamong them, and exhibit the power of our method.The most direct way to formalize our inference prob-lem is via the formalism of Markov Random Field (MRF)theory (Li, 2001).
Rather than doing that, for compu-tational reasons, we first use a fairly standard transfor-mation of MRF to a discrete optimization problem (see(Kleinberg and Tardos, 1999) for details).
Specifically,under weak assumptions we can view the inference prob-lem as the following optimization problem, which aimsto minimize the objective function that is the sum of thefollowing two cost functions.Assignment cost: the cost of deviating from the assign-ment of the variables V given by the classifiers.
The spe-cific cost function we use is defined as follows: Let l bethe label assigned to variable u ?
V .
If the marginal prob-ability estimation is p = P (fu = l), then the assignmentcost cu(l) is ?
log p.Constraint cost: the cost imposed by breaking con-straints between neighboring nodes.
The specific costfunction we use is defined as follows: Consider two en-tity nodes Ei, Ej and its corresponding relation node Rij ;that is, Ei = N 1(Rij) and Ej = N 2(Rij).
The con-straint cost indicates whether the labels are consistentwith the constraints.
In particular, we use: d1(fEi , fRij )is 0 if (fRij , fEi) ?
C1; otherwise, d1(fEi , fRij ) is ?
2.Similarly, we use d2 to force the consistency of the sec-ond argument of a relation.2In practice, we use a very large number (e.g., 915).Since we are seeking the most probable global assign-ment that satisfies the constraints, therefore, the overallcost function we optimize, for a global labeling f of allvariables is:C(f) =?u?Vcu(fu)+?Rij?R[d1(fRij , fEi) + d2(fRij , fEj )] (1)3 A Computational Approach toRelational InferenceUnfortunately, it is not hard to see that the combinatorialproblem (Eq.
1) is computationally intractable even whenplacing assumptions on the cost function (Kleinberg andTardos, 1999).
The computational approach we adopt isto develop a linear programming (LP) formulation of theproblem, and then solve the corresponding integer lin-ear programming (ILP) problem.
Our LP formulation isbased on the method proposed by (Chekuri et al, 2001).Since the objective function (Eq.
1) is not a linear func-tion in terms of the labels, we introduce new binary vari-ables to represent different possible assignments to eachoriginal variable; we then represent the objective functionas a linear function of these binary variables.Let x{u,i} be a {0, 1}-variable, defined to be 1 if andonly if variable u is labeled i, where u ?
E , i ?
LE oru ?
R, i ?
LR.
For example, x{E1,2} = 1 when thelabel of entity E1 is 2; x{R23,3} = 0 when the label of re-lation R23 is not 3.
Let x{Rij ,r,Ei,e1} be a {0, 1}-variableindicating whether relation Rij is assigned label r andits first argument, Ei, is assigned label e1.
For instance,x{R12,1,E1,2} = 1 means the label of relation R12 is 1and the label of its first argument, E1, is 2.
Similarly,x{Rij ,r,Ej ,e2} = 1 indicates that Rij is assigned label rand its second argument, Ej , is assigned label e2.
Withthese definitions, the optimization problem can be repre-sented as the following ILP problem (Figure 1).Equations (2) and (3) require that each entity or rela-tion variable can only be assigned one label.
Equations(4) and (5) assure that the assignment to each entity orrelation variable is consistent with the assignment to itsneighboring variables.
(6), (7), and (8) are the integralconstraints on these binary variables.There are several advantages of representing the prob-lem in an LP formulation.
First of all, linear (in)equalitiesare fairly general and are able to represent many typesof constraints (e.g., the decision time constraint in theexperiment in Sec.
4).
More importantly, an ILP prob-lem at this scale can be solved very quickly using currentcommercial LP/ILP packages, like (Xpress-MP, 2003) or(CPLEX, 2003).
We introduce the general strategies ofsolving an ILP problem here.min?E?E?e?LEcE(e) ?
x{E,e} +?R?R?r?LRcR(r) ?
x{R,r}+?Ei,Ej?EEi 6=Ej[?r?LR?e1?LEd1(r, e1) ?
x{Rij ,r,Ei,e1} +?r?LR?e2?LEd2(r, e2) ?
x{Rij ,r,Ej ,e2}]subject to:?e?LEx{E,e} = 1 ?E ?
E (2)?r?LRx{R,r} = 1 ?R ?
R (3)x{E,e} =?r?LRx{R,r,E,e} ?E ?
E and ?R ?
{R : E = N 1(R) or R : E = N 2(R)} (4)x{R,r} =?e?LEx{R,r,E,e} ?R ?
R and ?E = N 1(R) or E = N 2(R) (5)x{E,e} ?
{0, 1} ?E ?
E , e ?
LE (6)x{R,r} ?
{0, 1} ?R ?
R, r ?
LR (7)x{R,r,E,e} ?
{0, 1} ?R ?
R, r ?
LR, E ?
E , e ?
LE (8)Figure 1: Integer Linear Programming Formulation3.1 Linear Programming Relaxation (LPR)To solve an ILP problem, a natural idea is to relax theintegral constraints.
That is, replacing (6), (7), and (8)with:x{E,e} ?
0 ?E ?
E , e ?
LE (9)x{R,r} ?
0 ?R ?
R, r ?
LR (10)x{R,r,E,e} ?
0 ?R ?
R, r ?
LR,E ?
E , e ?
LE (11)If LPR returns an integer solution, then it is also theoptimal solution to the ILP problem.
If the solution isnon integer, then at least it gives a lower bound to thevalue of the cost function, which can be used in modi-fying the problem and getting closer to deriving an op-timal integer solution.
A direct way to handle the noninteger solution is called rounding, which finds an inte-ger point that is close to the non integer solution.
Un-der some conditions of cost functions, which do not holdhere, a well designed rounding algorithm can be shownthat the rounded solution is a good approximation to theoptimal solution (Kleinberg and Tardos, 1999; Chekuri etal., 2001).
Nevertheless, in general, the outcomes of therounding procedure may not even be a legal solution tothe problem.3.2 Branch & Bound and Cutting PlaneBranch and bound is the method that divides an ILP prob-lem into several LP subproblems.
It uses LPR as a sub-routine to generate dual (upper and lower) bounds to re-duce the search space, and finds the optimal solution aswell.
When LPR finds a non integer solution, it splits theproblem on the non integer variable.
For example, sup-pose variable xi is fractional in an non integer solution tothe ILP problem min{cx : x ?
S, x ?
{0, 1}n}, where Sis the linear constraints.
The ILP problem can be split intotwo sub LPR problems, min{cx : x ?
S?
{xi = 0}} andmin{cx : x ?
S?
{xi = 1}}.
Since any feasible solutionprovides an upper bound and any LPR solution generatesa lower bound, the search tree can be effectively cut.Another strategy of dealing with non integer points,which is often combined with branch & bound, is calledcutting plane.
When a non integer solution is given byLPR, it adds a new linear constraint that makes the non in-teger point infeasible, while still keeps the optimal integersolution in the feasible region.
As a result, the feasibleregion is closer to the ideal polyhedron, which is the con-vex hull of feasible integer solutions.
The most famouscutting plane algorithm is Gomory?s fractional cuttingplane method (Wolsey, 1998), which can be shown thatonly finite number of additional constraints are needed.Moreover, researchers develop different cutting plane al-gorithms for different types of ILP problems.
One exam-ple is (Wang and Regan, 2000), which only focuses onbinary ILP problems.Although in theory, a search based strategy may needseveral steps to find the optimal solution, LPR alwaysgenerates integer solutions in our experiments.
This phe-nomenon may link to the theory of unimodularity.3.3 UnimodularityWhen the coefficient matrix of a given linear programin its standard form is unimodular, it can be shown thatthe optimal solution to the linear program is in fact inte-gral (Schrijver, 1986).
In other words, LPR is guaranteedto produce an integer solution.Definition 3.1 A matrix A of rank m is called unimodu-lar if all the entries ofA are integers, and the determinantof every square submatrix of A of order m is in 0,+1,-1.Theorem 3.1 (Veinott & Dantzig) Let A be an (m,n)-integral matrix with full row rank m. Then the polyhe-dron {x|x ?
0;Ax = b} is integral for each integralvector b, if and only if A is unimodular.Theorem 3.1 indicates that if a linear programmingproblem is in its standard form, then regardless of thecost function and the integral vector b, the optimal so-lution is an integer if and only if the coefficient matrix Ais unimodular.Although the coefficient matrix in our problem is notunimodular, LPR still produces integer solutions for allthe (thousands of cases) we have experimented with.
Thismay be due to the fact that the coefficient matrix sharesmany properties of a unimodular matrix.
As a result, mostof the vertices of the polyhedron are integer points.
An-other possible reason is that given the cost function wehave, the optimal solution is always integer.
Because ofthe availability of very efficient LP/ILP packages, we de-fer the exploration of this direction for now.4 ExperimentsWe describe below two experiments on the problem ofsimultaneously recognizing entities and relations.
In thefirst, we view the task as a knowledge acquisition task?
we let the system read sentences and identify entitiesand relations among them.
Given that this is a difficulttask which may require quite often information beyondthe sentence, we consider also a ?forced decision?
task,in which we simulate a question answering situation ?we ask the system, say, ?who killed whom?
and evaluateit on identifying correctly the relation and its arguments,given that it is known that somewhere in this sentencethis relation is active.
In addition, this evaluation exhibitsthe ability of our approach to incorporate task specificconstraints at decision time.Our experiments are based on the TREC data set(which consists of articles from WSJ, AP, etc.)
that weannotated for named entities and relations.
In order toeffectively observe the interaction between relations andentities, we picked 1437 sentences that have at least oneactive relation.
Among those sentences, there are 5336entities, and 19048 pairs of entities (binary relations).
En-tity labels include 1685 persons, 1968 locations, 978 or-ganizations and 705 others.
Relation labels include 406located in, 394 work for, 451 orgBased in, 521 live in,268 kill, and 17007 none.
Note that most pairs of entitieshave no active relations at all.
Therefore, relation nonesignificantly outnumbers others.
Examples of each rela-tion label and the constraints between a relation variableand its two entity arguments are shown as follows.Relation Entity1 Entity2 Examplelocated in loc loc (New York, US)work for per org (Bill Gates, Microsoft)orgBased in org loc (HP, Palo Alto)live in per loc (Bush, US)kill per per (Oswald, JFK)In order to focus on the evaluation of our inferenceprocedure, we assume the problem of segmentation (orphrase detection) (Abney, 1991; Punyakanok and Roth,2001) is solved, and the entity boundaries are given to usas input; thus we only concentrate on their classifications.We evaluate our LP based global inference procedureagainst two simpler approaches and a third that is givenmore information at learning time.
Basic, only tests ourentity and relation classifiers, which are trained indepen-dently using only local features.
In particular, the relationclassifier does not know the labels of its entity arguments,and the entity classifier does not know the labels of rela-tions in the sentence either.
Since basic classifiers areused in all approaches, we describe how they are trainedhere.For the entity classifier, one set of features are ex-tracted from words within a size 4 window around thetarget phrase.
They are: (1) words, part-of-speech tags,and conjunctions of them; (2) bigrams and trigrams ofthe mixture of words and tags.
In addition, some otherfeatures are extracted from the target phrase, including:symbol explanationicap the first character of a word is capitalizedacap all characters of a word are capitalizedincap some characters of a word are capitalizedsuffix the suffix of a word is ?ing?, ?ment?, etc.bigram bigram of words in the target phraselen number of words in the target phraseplace3 the phrase is/has a known place?s nameprof3 the phrase is/has a professional title (e.g.
Lt.)name3 the phrase is/has a known person?s nameFor the relation classifier, there are three sets of fea-tures: (1) features similar to those used in the entity clas-sification are extracted from the two argument entities of3We collect names of famous places, people and popular ti-tles from other data sources in advance.Pattern Examplearg1 , arg2 San Jose, CAarg1 , ?
?
?
a ?
?
?
arg2 prof John Smith, a Starbucks manager ?
?
?in/at arg1 in/at/, arg2 Officials in Perugia in Umbria province said ?
?
?arg2 prof arg1 CNN reporter David McKinley ?
?
?arg1 ?
?
?
native of ?
?
?
arg2 Elizabeth Dole is a native of Salisbury, N.C.arg1 ?
?
?
based in/at arg2 Leslie Kota, a spokeswoman for K mart based in Troy, Mich. said ?
?
?Table 1: Some patterns used in relation classificationthe relation; (2) conjunctions of the features from the twoarguments; (3) some patterns extracted from the sentenceor between the two arguments.
Some features in category(3) are ?the number of words between arg1 and arg2 ?,?whether arg1 and arg2 are the same word?, or ?arg1 isthe beginning of the sentence and has words that consistof all capitalized characters?, where arg1 and arg2 rep-resent the first and second argument entities respectively.In addition, Table 1 presents some patterns we use.The learning algorithm used is a variation of the Win-now update rule incorporated in SNoW (Roth, 1998;Roth and Yih, 2002), a multi-class classifier that is specif-ically tailored for large scale learning tasks.
SNoW learnsa sparse network of linear functions, in which the targets(entity classes or relation classes, in this case) are repre-sented as linear functions over a common feature space.While SNoW can be used as a classifier and predicts us-ing a winner-take-all mechanism over the activation valueof the target classes, we can also rely directly on the rawactivation value it outputs, which is the weighted linearsum of the active features, to estimate the posteriors.
Itcan be verified that the resulting values are monotonicwith the confidence in the prediction, therefore provide agood source of probability estimation.
We use softmax(Bishop, 1995) over the raw activation values as condi-tional probabilities.
Specifically, suppose the number ofclasses is n, and the raw activation values of class i isacti.
The posterior estimation for class i is derived by thefollowing equation.pi =eacti?1?j?n eactjPipeline, mimics the typical strategy in solving com-plex natural language problems ?
separating a task intoseveral stages and solving them sequentially.
For exam-ple, a named entity recognizer may be trained using a dif-ferent corpus in advance, and given to a relation classifieras a tool to extract features.
This approach first trains anentity classifier as described in the basic approach, andthen uses the prediction of entities in addition to otherlocal features to learn the relation identifier.
Note thatalthough the true labels of entities are known here whentraining the relation identifier, this may not be the casein general NLP problems.
Since only the predicted en-tity labels are available in testing, learning on the predic-tions of the entity classifier presumably makes the rela-tion classifier more tolerant to the mistakes of the entityclassifier.
In fact, we also observe this phenomenon em-pirically.
When the relation classifier is trained using thetrue entity labels, the performance is much worse thanusing the predicted entity labels.LP, is our global inference procedure.
It takes as in-put the constraints between a relation and its entity argu-ments, and the output (the estimated probability distribu-tion of labels) of the basic classifiers.
Note that LP maychange the predictions for either entity labels or relationlabels, while pipeline fully trusts the labels of entity clas-sifier, and only the relation predictions may be differentfrom the basic relation classifier.
In other words, LP isable to enhance the performance of entity classification,which is impossible for pipeline.The final approach, Omniscience, tests the conceptualupper bound of this entity/relation classification problem.It also trains the two classifiers separately as the basicapproach.
However, it assumes that the entity classifierknows the correct relation labels, and similarly the rela-tion classifier knows the right entity labels as well.
Thisadditional information is then used as features in trainingand testing.
Note that this assumption is totally unrealis-tic.
Nevertheless, it may give us a hint that how much aglobal inference can achieve.4.1 ResultsTables 2 & 3 show the performance of each approach inF?=1 using 5-fold cross-validation.
The results show thatLP performs consistently better than basic and pipeline,both in entities and relations.
Note that LP does not applylearning at all, but still outperforms pipeline, which usesentity predictions as new features in learning.
The resultsof the omniscient classifiers reveal that there is still roomfor improvement.
One option is to apply learning to tunea better cost function in the LP approach.One of the more significant results in our experiments,we believe, is the improvement in the quality of the deci-sions.
As mentioned in Sec.
1, incorporating constraintshelps to avoid inconsistency in classification.
It is in-Approach person organization locationRec.
Prec.
F1 Rec.
Prec.
F1 Rec.
Prec.
F1Basic 89.4 89.2 89.3 86.9 91.4 89.1 68.2 90.9 77.9Pipeline 89.4 89.2 89.3 86.9 91.4 89.1 68.2 90.9 77.9LP 90.4 90.0 90.2 88.5 91.7 90.1 71.5 91.0 80.1Omniscient 94.9 93.5 94.2 92.3 96.5 94.4 88.3 93.4 90.8Table 2: Results of Entity ClassificationApproach located in work for orgBased inRec.
Prec.
F1 Rec.
Prec.
F1 Rec.
Prec.
F1Basic 54.7 43.0 48.2 42.1 51.6 46.4 36.1 84.9 50.6Pipeline 51.2 51.6 51.4 41.4 55.6 47.5 36.9 76.6 49.9LP 53.2 59.5 56.2 40.4 72.9 52.0 36.3 90.1 51.7Omniscient 64.0 54.5 58.9 50.5 69.1 58.4 50.2 76.7 60.7Approach live in killRec.
Prec.
F1 Rec.
Prec.
F1Basic 39.7 61.6 48.3 82.1 73.6 77.6Pipeline 42.6 62.2 50.6 83.2 76.4 79.6LP 41.5 68.1 51.6 81.3 82.2 81.7Omniscient 57.0 60.7 58.8 82.1 74.6 78.2Table 3: Results of Relation Classificationteresting to investigate how often such mistakes happenwithout global inference, and see how effectively theglobal inference enhances this.For this purpose, we define the quality of the decisionas follows.
For an active relation of which the label isclassified correctly, if both its argument entities are alsopredicted correctly, we count it as a coherent prediction.Quality is then the number of coherent predictions di-vided by the sum of coherent and incoherent predictions.Since the basic and pipeline approaches do not have aglobal view of the labels of entities and relations, 5%to 25% of the predictions are incoherent.
Therefore, thequality is not always good.
On the other hand, our globalinference procedure, LP, takes the natural constraints intoaccount, so it never generates incoherent predictions.
Ifthe relation classifier has the correct entity labels as fea-tures, a good learner should learn the constraints as well.As a result, the quality of omniscient is almost as good asLP.Another experiment we did is the forced decision test,which boosts the F1 of ?kill?
relation to 86.2%.
Herewe consider only sentences in which the ?kill?
relationis active.
We force the system to determine which of thepossible relations in a sentence (i.e., which pair of en-tities) has this relation by adding a new linear equality.This is a realistic situation (e.g., in the context of ques-tion answering) in that it adds an external constraint, notpresent at the time of learning the classifiers and it eval-uates the ability of our inference algorithm to cope withit.
The results exhibit that our expectations are correct.In fact, we believe that in natural situations the numberof constraints that can apply is even larger.
Observingthe algorithm performs on other, specific, forced deci-sion tasks verifies that LP is reliable in these situations.As shown in the experiment, it even performs better thanomniscience, which is given more information at learningtime, but cannot adapt to the situation at decision time.5 DiscussionWe presented an linear programming based approachfor global inference where decisions depend on the out-comes of several different but mutually dependent classi-fiers.
Even in the presence of a fairly general constraintstructure, deviating from the sequential nature typicallystudied, this approach can find the optimal solution effi-ciently.Contrary to general search schemes (e.g., beamsearch), which do not guarantee optimality, the linear pro-gramming approach provides an efficient way to findingthe optimal solution.
The key advantage of the linearprogramming formulation is its generality and flexibility;in particular, it supports the ability to incorporate classi-fiers learned in other contexts, ?hints?
supplied and de-cision time constraints, and reason with all these for thebest global prediction.
In sharp contrast with the typi-cally used pipeline framework, our formulation does notblindly trust the results of some classifiers, and thereforeis able to overcome mistakes made by classifiers with thehelp of constraints.Our experiments have demonstrated these advantagesby considering the interaction between entity and rela-tion classifiers.
In fact, more classifiers can be added andused within the same framework.
For example, if coref-erence resolution is available, it is possible to incorporateit in the form of constraints that force the labels of the co-referred entities to be the same (but, of course, allowingthe global solution to reject the suggestion of these clas-sifiers).
Consequently, this may enhance the performanceof entity/relation recognition and, at the same time, cor-rect possible coreference resolution errors.
Another ex-ample is to use chunking information for better relationidentification; suppose, for example, that we have avail-able chunking information that identifies Subj+Verb andVerb+Object phrases.
Given a sentence that has the verb?murder?, we may conclude that the subject and object ofthis verb are in a ?kill?
relation.
Since the chunking in-formation is used in the global inference procedure, thisinformation will contribute to enhancing its performanceand robustness, relying on having more constraints andovercoming possible mistakes by some of the classifiers.Moreover, in an interactive environment where a user cansupply new constraints (e.g., a question answering situa-tion) this framework is able to make use of the new in-formation and enhance the performance at decision time,without retraining the classifiers.As we show, our formulation supports not only im-proved accuracy, but also improves the ?human-like?quality of the decisions.
We believe that it has the poten-tial to be a powerful way for supporting natural languageinferences.Acknowledgements This research has been supportedby NFS grants CAREER IIS-9984168, ITR IIS-0085836,EIA-0224453, an ONR MURI Award, and an equipmentdonation from AMD.
We also thank the anonymous ref-erees for their useful comments.ReferencesS.
Abney.
1991.
Parsing by chunks.
In S. AbneyR.
Berwick and C. Tenny, editors, Principle-basedparsing: Computation and Psycholinguistics, pages257?278.
Kluwer, Dordrecht.C.
Bishop, 1995.
Neural Networks for Pattern Recogni-tion, chapter 6.4: Modelling conditional distributions,page 215.
Oxford University Press.Y.
Boykov, O. Veksler, and R. Zabih.
2001.
Fast ap-proximate energy minimization via graph cuts.
IEEETransactions on Pattern Analysis and Machine Intelli-gence, 23(11):1222?1239, November.C.
Chekuri, S. Khanna, J. Naor, and L. Zosin.
2001.
Ap-proximation algorithms for the metric labeling prob-lem via a new linear programming formulation.
InSymposium on Discrete Algorithms, pages 109?118.CPLEX.
2003.
ILOG, Inc. CPLEX.http://www.ilog.com/products/cplex/.T.
Dietterich.
2002.
Machine learning for sequentialdata: A review.
In Structural, Syntactic, and StatisticalPattern Recognition, pages 15?30.
Springer-Verlag.J.
Kleinberg and E. Tardos.
1999.
Approximation algo-rithms for classification problems with pairwise rela-tionships: Metric labeling and markov random fields.In IEEE Symposium on Foundations of Computer Sci-ence, pages 14?23.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
18thInternational Conf.
on Machine Learning, pages 282?289.
Morgan Kaufmann, San Francisco, CA.A.
Levin, A. Zomet, and Yair Weiss.
2002.
Learningto perceive transparency from the statistics of natu-ral scenes.
In NIPS-15; The 2002 Conference on Ad-vances in Neural Information Processing Systems.S.
Li.
2001.
Markov Random Field Modeling in ImageAnalisys.
Springer-Verlag.V.
Punyakanok and D. Roth.
2001.
The use of classifiersin sequential inference.
In NIPS-13; The 2000 Confer-ence on Advances in Neural Information ProcessingSystems, pages 995?1001.
MIT Press.D.
Roth and W. Yih.
2002.
Probabilistic reasoning forentity & relation recognition.
In COLING 2002, The19th International Conference on Computational Lin-guistics, pages 835?841.D.
Roth.
1998.
Learning to resolve natural language am-biguities: A unified approach.
In Proc.
of AAAI, pages806?813.D.
Roth.
2002.
Reasoning with classifiers.
In Proc.
ofthe European Conference on Machine Learning, pages506?510.A.
Schrijver.
1986.
Theory of Linear and Integer Pro-gramming.
Wiley Interscience series in discrete math-matics.
John Wiley & Sons, December.B.
Taskar, A. Pieter, and D. Koller.
2002.
Discrimina-tive probabilistic models for relational data.
In Proc.
ofUncertainty in Artificial Intelligence, pages 485?492.E.
Tjong Kim Sang and F. De Meulder.
2003.
Intro-duction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proc.
ofCoNLL-2003, pages 142?147.
Edmonton, Canada.X.
Wang and A. Regan.
2000.
A cutting plane methodfor integer programming problems with binary vari-ables.
Technical Report UCI-ITS-WP-00-12, Univer-sity of California, Irvine.L.
Wolsey.
1998.
Integer Programming.
John Wiley &Sons, Inc.Xpress-MP.
2003.
Dash Optimization.
Xpress-MP.http://www.dashoptimization.com/products.html.
