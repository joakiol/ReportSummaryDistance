Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 29?36, New York City, June 2006. c?2006 Association for Computational LinguisticsWhat are the Productive Units of Natural Language Grammar?
A DOPApproach to the Automatic Identification of Constructions.Willem ZuidemaInstitute for Logic, Language and ComputationUniversity of AmsterdamPlantage Muidergracht 24, 1018 TV, Amsterdam, the Netherlands.jzuidema@science.uva.nlAbstractWe explore a novel computational ap-proach to identifying ?constructions?
or?multi-word expressions?
(MWEs) in anannotated corpus.
In this approach,MWEs have no special status, but emergein a general procedure for finding the beststatistical grammar to describe the train-ing corpus.
The statistical grammar for-malism used is that of stochastic tree sub-stitution grammars (STSGs), such as usedin Data-Oriented Parsing.
We present analgorithm for calculating the expected fre-quencies of arbitrary subtrees given theparameters of an STSG, and a methodfor estimating the parameters of an STSGgiven observed frequencies in a tree bank.We report quantitative results on the ATIScorpus of phrase-structure annotated sen-tences, and give examples of the MWEsextracted from this corpus.1 IntroductionMany current theories of language use and acquisi-tion assume that language users store and use muchlarger fragments of language than the single wordsand rules of combination of traditional linguisticmodels.
Such fragments are often called construc-tions, and the theories that assign them a centralrole ?construction grammar?
(Goldberg, 1995; Kayand Fillmore, 1999; Tomasello, 2000; Jackendoff,2002, among others).
For construction grammar-ians, multi-word expressions (MWEs) such as id-ioms, collocations, fixed expressions and compoundverbs and nouns, are not so much exceptions to therule, but rather extreme cases that reveal some fun-damental properties of natural language.In the construction grammar tradition, co-occurrence statistics from corpora have often beenused as evidence for hypothesized constructions.However, such statistics are typically gathered ona case-by-case basis, and no reliable procedure ex-ists to automatically identify constructions.
In con-trast, in computational linguistics, many automaticprocedures are studied for identifying MWEs (Saget al, 2002) ?
with varying success ?
but here theyare treated as exceptions: identifying multi-word ex-pressions is a pre-processing step, where typicallyadjacent words are grouped together after which theusual procedures for syntactic or semantic analysiscan be applied.
In this paper I explore an alter-native formal and computational approach, wheremulti-word constructions have no special status,but emerge in a general procedure to find the beststatistical grammar to describe a training corpus.Crucially, I use a formalism known as ?StochasticTree Substitution Grammars?
(henceforth, STSGs),which can represent single words, contiguous andnoncontiguous MWEs, context-free rules or com-plete parse trees in a unified representation.My approach is closely related to work in statisti-cal parsing known as Data-Oriented Parsing (DOP),an empirically highly successful approach with la-beled recall and precision scores on the Penn TreeBank that are among the best currently obtained(Bod, 2003).
DOP, first proposed in (Scha, 1990),29can be seen as an early formalization and combina-tion of ideas from construction grammar and statis-tical parsing.
Its key innovations were (i) the pro-posal to use fragments of trees from a tree bank asthe symbolic backbone; (ii) the proposal to allow, inprinciple, trees of arbitrary size and shape as the el-ementary units of combination; (iii) the proposal touse the occurrence and co-occurrence frequencies asthe basis for structural disambiguation in parsing.The model I develop in this paper is true to thesegeneral DOP ideals, although it differs in impor-tant respects from the many DOP implementationsthat have been studied since its first inception (Bod,1993; Goodman, 1996; Bod, 1998; Sima?an, 2002;Collins and Duffy, 2002; Bod et al, 2003, and manyothers).
The crucial difference is in the estimationprocedure for choosing the weights of the STSGbased on observed frequencies in a corpus.
ExistingDOP models converge to STSGs that either (i) giveall subtrees of the observed trees nonzero weights(Bod, 1993; Bod, 2003), or (ii) give only the largestpossible fragments nonzero weights (Sima?an andBuratto, 2003; Zollmann and Sima?an, 2005).
Themodel in this paper, in contrast, aims at finding thesmallest set of productive units that explain the oc-currences and co-occurrences in a corpus.
Largesubtrees only receive non-zero weights, if they occurmore frequently than can be expected on the basis ofthe weights of smaller subtrees.2 Formalism, Notation and Definitions2.1 Stochastic Tree Substitution GrammarsSTSGs are a simple generalization of Stochas-tic Context Free Grammars (henceforth, SCFGs),where the productive units are elementary trees ofarbitrary size instead of the rewrite rules in SCFGs(which can be viewed as trees of depth 1).
STSGsform a restricted subclass of Stochastic Tree Adjoin-ing Grammars (henceforth, STAGs) (Resnik, 1992;Schabes, 1992), the difference being that STSGsonly allow for substitution and not for adjunction(Joshi and Sarkar, 2003).
This limits the genera-tive capacity to that of context-free grammars, andmeans STSGs cannot be fully lexicalized.
Theselimitations notwithstanding, the close relationshipwith STAGs is an attractive feature with extensionsto the class of mildly context-sensitive languages(Joshi et al, 1991) in mind.
Most importantly, how-ever, STSGs are already able to model a vast rangeof statistical dependencies between words and con-stituents, which allows them to rightly predict theoccurrences of many constructions (Bod, 1998).For completeness, we include the usual defi-nitions of STSGs, the substitution operation andderivation and parse probabilities (Bod, 1998), us-ing our own notation.
An STSG is a 5-tuple?Vn, Vt, S, T, w?, where Vn is the set of non-terminalsymbols; Vt is the set of terminal symbols; S ?
Vn isthe start symbol; T is a set of elementary trees, suchthat for every t ?
T the unique root node r(t) ?
Vn,the set of internal nodes i(t) ?
Vn and the set of leafnodes l(t) ?
Vn ?
Vt; finally, w : T ?
[0, 1] is aprobability (weight) distribution over the elementarytrees, such that for any t ?
T , ?t?
?R(t) w(t?)
= 1,where R(t) is the set of elementary trees with thesame root label as t. It will prove useful to also de-fine the set of all possible trees ?
over the definedalphabets (with the same conditions on root, internaland leaf nodes as for T ), and the set of all possiblecomplete parse trees ?
(with r(t) = S and all leafnodes l(t) ?
Vt).
Obviously, T ?
?
and ?
?
?.The substitution operation ?
is defined if the left-most nonterminal leaf in t1 is identical to the root oft2.
Performing substitution t1 ?
t2 yields t3, if t3 isidentical to t1 with the leftmost nonterminal leaf re-placed by t2.
A derivation is a sequence of elemen-tary trees, where the first tree t ?
T has root-labelS and every next tree combines through substitutionwith the result of the substitutions before it.
Theprobability of a derivation d is defined as the prod-uct of weights of the elementary trees involved:P (d = t1 ?
.
.
.
?
tn) =n?i=1(w (ti)) .
(1)A parse tree is any tree t ?
?.
Multiple derivationscan yield the same parse tree; the probability of aparse tree p equals the sum of the probabilities ofthe different derivations that yield that same tree:P (p) =?d:d?=p(P (d)) , (2)where d?
is the tree derived by derivation d.In this paper, we are only concerned with gram-mars that define proper probability distributions over30trees, such that the probability of all derivations sumup to 1 and no probability mass gets lost in deriva-tions that never reach a terminal yield.
We require:?p?
?P (p) =?d:d??
?P (d) = 1.
(3)2.2 Usage Frequency and OccurrenceFrequencyIn addition to these conventional definitions, we willmake use in this paper of the concepts ?usage fre-quency?
and ?occurrence frequency?.
When weconsider an arbitrary subtree t, the usage frequencyu(t) describes the relative frequency with which el-ementary tree t is involved in a set of derivations.Given a grammar G ?
STSG, the expected usagefrequency is:u(t) =?d:t?d(P (d) C (t, d)) , (4)where C (t, d) gives the number of occurrences oft in d. The set of derivations, and hence usage fre-quency, is usually considered hidden information.The occurrence frequency f(t) describes the rela-tive frequency with which t occurs as a subtree of aset of parse trees, which is usually assumed to beobservable information.
If grammar G is used togenerate trees, it will create a tree bank where eachparse tree will occur with an expected frequency asin equation (2).
More generally, the expected oc-currence frequency f(t) (relative to the number n ofcomplete trees in the tree bank) of a subtree t is:E[f(t)] =?p:t?p?
(P (p)C (t, p?))
, (5)where p?
is the multiset of all subtrees of p.Hence, w(t), u(t) and f(t) all assign values (thelatter two not necessarily between 0 and 1) to trees.An important question is how these different val-ues can be related.
For STSGs which have onlyelementary trees of depth 1, and are thus equiva-lent to SCFGs, these relations are straightforward:the usage frequency of an elementary tree simplyequals its expected frequency, and can be derivedfrom the weights by multiplying inside and out-side probabilities (Lari and Young, 1990).
Estimat-ing the weights of an (unconstrained and untrans-formed) SCFG from an tree bank is straightforward,as weights, in the limit, simply equal the relativefrequency of each depth-1 subtree (relative to otherdepth-1 subtrees with the same root label).When elementary trees can be of arbitrary depth,however, many different derivations can yield thesame tree, and a given subtree t can emerge with-out the corresponding elementary tree ever havingbeen used.
The expected frequencies are sums ofproducts, and ?
if one wants to avoid exhaustivelyenumerating all possible parse trees ?
surprisinglydifficult to calculate, as will become clear below.2.3 From weights to usage frequencies andbackRelating usage frequencies to weights is relativelysimple.
With a bit of algebra we can work out thefollowing relations:u(t) =??
?w(t) if r(t) = Sw(t)?t?
:r(t)?l(t?)u(t?
)Ct?t otherwise(6)where C t?t gives the number of occurrences of theroot label r(t) of t among the leaves of t?.
The in-verse relation is straightforward:w(t) = u(t)?t?
?R(t) u(t?).
(7)2.4 From usage frequency to expectedfrequencyThe two remaining problems ?
calculating expectedfrequencies from weights and estimating the weightsfrom observed frequencies ?
are surprisingly dif-ficult and heretofore not satisfactorily solved.
In(Zuidema, 2006) we evaluate existing estimationmethods for Data-Oriented Parsing, and show thatthey are ill-suited for learning tasks such as stud-ied in this paper.
In the next section, we present anew algorithm for estimation, which makes use ofa method for calculating expected frequencies thatwe sketch in this section.
This method makes use ofsub- and supertree relations that we explain first.We define two types of subtrees of a given tree t,which, for lack of better terminology, we will call?twigs?
and ?prunes?
of t. Twigs are those subtreesheaded by any of t?s internal nodes and everything31below.
Prunes are those subtrees headed by t?s root-node, pruned at any number (?
0) of internal nodes.Using ?
to indicate left-most substitution, we write:?
t1 is a twig of t2, if either t1 = t2 or ?t3, suchthat t3 ?
t1 = t2;?
t1 is a prune of t2, if either t1 = t2 or ?t3 .
.
.
tn,such that t1 ?
t3 .
.
.
?
tn = t2;?
t?
= prx(t), if x is a set of nodes in t, such thatif t is pruned at each i ?
x it equals t?.Thus defined, the set of all subtrees st(t) of t cor-responds to the set of all prunes of all twigs of t:st(t) = {t??|?t?(t?
?
tw(t) ?
t??
?
pr(t?
)).We further define the sets of supertwigs, super-prunes and supertrees as follows:?
t?w(t) = {t?|t ?
tw(t?)}?
p?rx(t) = {t?|t = prx(t?)}?
s?t(t) = {t?|t ?
st(t?
)}.Using these sets, and the set of derivations D(t) ofthe fragment t, a general expression for the expectedfrequency of t is:E[f(t)] =?d?D(t)???
=???ctw(d1)??
??
dprx(t)(?)u(?
?)?
=?t???d2,...,dn???
??
dprx(t)(t?)w(?
?)
(8)where ?d1, .
.
.
, dn?
is the sequence of elementarytrees in derivation d. A derivation of this equationis provided on the author?s website1.
Note that it1http://staff.science.uva.nl/?jzuidema.
The intuition behindit is as follows.
Observe first that there are many ways in whichan arbitrary fragment t can emerge, many of which do not in-volve the usage of the elementary tree t. It is useful to partitionthe set of all derivations of complete parse trees according to thesubstitution sites inside t that they involve, and hence accordingto the corresponding derivations of t. The first summation in (8)simply sums over all these cases.Each derivation of t involves a first elementary tree d1, andpossibly a sequence of further elementary trees ?d2, .
.
.
, dn?.Roughly speaking, the ?-term in equation (8) describes the fre-quency with which a d1 will be generated.
The ?-term thendescribes the probability that d1 will be expanded as t. Theequation simplifies considerably for those fragments that haveno nonterminal leaves: the set dprx(t) then only contains t, andthe two summations over this set disappear.
The equation fur-ther simplifies if only depth-1 elementary trees have nonzeroweights (i.e.
for SCFGs): ?
and ?
then essentially give outsideand inside probabilities (Lari and Young, 1990).
However, forunconstrained STSGs we need all sums and products in (8).will, in general, be computationally extremely ex-pensive to calculate E[f(t)] .
We will come back tocomputational efficiency issues in the discussion.3 Estimation: push-n-pullThe goal of this paper is an automatic discoveryprocedure for finding ?constructions?
based on oc-currence and co-occurrence frequencies in a corpus.Now that we have introduced the necessary termi-nology, we can reformulate this goal as follows:What are the elementary trees with multiple wordswith the highest usage frequency in the STSG esti-mated from an annotated corpus?
Thus phrased, thecrucial next step is to decide on an estimation proce-dure for learning an STSG from a corpus.Here we develop an estimation procedure we call?push-n-pull?.
The basic idea is as follows.
Givenan initial setting of the parameters, the method cal-culates the expected frequency of all complete andincomplete trees.
If a tree?s expected frequency ishigher than its observed frequency, the method sub-tracts the difference from the tree?s score, and dis-tributes (?pushes?)
it over the trees involved in itsderivations.
If it is lower, it ?pulls?
the differencefrom these same derivations.
The method includes abias for moving probability mass to smaller elemen-tary trees, to avoid overfitting; its effects becomesmaller as more data gets observed.Because the method for calculating estimated fre-quency works with usage-frequencies, the push-n-pull algorithm also uses these as parameters.
Moreprecisely, it manipulates a ?score?, which is theproduct of usage frequency and the total number ofparse trees observed.
Implicit here is the assumptionthat by shifting usage frequencies between differentderivations, the relation with weights remains as inequation (6).
Simulations suggest this is reasonable.In the current implementation, the method startswith all frequency mass in the longest derivations,i.e.
in the depth-1 elementary trees.
Finally, the cur-rent implementation is incremental.
It keeps track ofthe frequencies with which it observes subtrees in acorpus.
For each tree received, it finds all derivationsand all probabilities, updates frequencies and scoresaccording to the rules sketched above.
In pseudo-code, the push-n-pull algorithm is as follows:for each observed parse tree p32for each depth-1 subtree t in pupdate-score(t, 1.0)for each subtree t of p?
=min(sc(t), B + ?
(E[f(t)] ?
f(t)))??
= 0for each of n derivations d of tlet t?
.
.
.
t??
be all elementary trees in d?
=min(sc(t?
), .
.
.
, sc(t??),??/n)???
= ?for each elementary tree t?
in dupdate-score(t?
, ?
)update-score (t,??
)where sc(t) is the score of t, B is the bias to-wards smaller subtrees, ?
is the learning rate param-eter and f(t) is the observed frequency of t.
??
thusgives the actual change in the score of t, based onthe difference between expected and observed fre-quency, bias, learning rate and how much scores canbe pushed or pulled2.
For computational efficiency,only subtrees with a depth no larger than d = 3 ord = 4 and only derivations involving 2 elementarytrees are considered.4 ResultsWe have implemented the algorithms for calculat-ing the expected frequency, and the push-n-pull al-gorithm for estimation.
We have evaluated the algo-rithms on a number of simple example STSGs andfound that the expected frequency algorithm cor-rectly predicts observed frequencies.
We have fur-ther found that ?
unlike existing estimation meth-ods ?
the push-n-pull algorithm converges to STSGsthat closely model the observed frequencies (i.e.
thatmaximize the likelihood of the data) without puttingall probability mass in the largest elementary trees(i.e.
whilst retaining generalizations about the data).Here we report first quantitative results on theATIS3 corpus (Hemphill et al, 1990).
Before pro-cessing, all trees (train and test set) were convertedto a format that our current implementation requires(all non-terminal labels are unique, all internal nodeshave two daughters, all preterminal nodes have asingle lexical daughter; all unary productions andall traces were removed).
The set of trees was ran-domly split in a train set of 462 trees, and a test set2An important topic for future research is to clarify the rela-tion between push-n-pull and Expectation Maximization.of 116 trees.
The push-n-pull algorithm was thenrun in 10 passes over the train set, with d = 3,B = 0 and ?
= 0.1.
By calculating the most proba-ble parse3 for each yield of the trees in test set, andrunning ?evalb?
we arrive at the following quantita-tive results: a string set coverage of 84% (19 failedparses), labeled recall of 95.07, and labeled preci-sion of 95.07.
We obtained almost identical num-bers on the same data with a reimplementation ofthe DOP1 algorithm (Bod, 1998).method # rules Cov.
LR LP EMDOP1 77852 84% 95.07 95.07 83.5p-n-p 58799 84% 95.07 95.07 83.5Table 1: Parseval scores of DOP1 and push-n-pullon the same 462-116 random train-testset split of atreebank derived from the ATIS3 corpus (we empha-size that all trees, also those of the test-set, were con-verted to Chomsky Normal Form, whereby unaryproduction and traces were removed and top-nodesrelabeled ?TOP?.
These results are thus not compa-rable to previous methods evaluated on the ATIS3corpus.)
EM is ?exact match?.method # rules Cov.
LR LP EMsc > 0.3 8593 77% 80.8 80.8 46.3sc > 0.1 98443 77% 81.9 81.9 48.8Table 2: Parseval scores using a p-n-p inducedSTSG on the same treebank as in table 1, using adifferent random 525-53 train-testset split.
Shownare results were only elementary trees with scoreshigher than 0.3 and 0.1 respectively are used.However, more interesting is a qualitative anal-ysis of the STSG induced, which shows that, un-like DOP1, push-n-pull arrives at a grammar thatgives high weights (and scores) to those elementary3We approximated the most probable parse as follows (fol-lowing (Bod, 2003)).
We first converted the induced STSG toan isomorph SCFG, by giving the internal nodes of every ele-mentary tree t unique address-labels, and reading off all CFGproductions (all with weight 1.0, except for the top-production,which receives the weight of t).
An existing SCFG parser(Schmid, 2004) was then used, with a simple unknown wordheuristic, to generate the Viterbi n-best parses with n = 100,and, after removing the address labels, all equal parses and theirprobabilities were summed, and the one with highest probabil-ity chosen.33trees that best explain the overrepresentation of cer-tain constructions in the data.
For instance, in a runwith d = 4, ?
= 1.0, B = 1.0, the 50 elemen-tary trees with the highest scores, as shown in fig-ure 1, are all exemplary of frequent formulas in theATIS corpus such as ?show me X?, ?I?d like to X?,?which of these?, ?what is the X?, ?cheapest fare?and ?flights from X to Y?.
In short, the push-n-pullalgorithm ?
while starting out considering all possi-ble subtrees ?
converges to a grammar which makeslinguistically relevant generalizations.
This allowsfor a more compact grammar (58799 rules in theSCFG reduction, vs. 77852 for DOP1), whilst re-taining DOP?s excellent empirical performance.5 DiscussionCalculating E[f(t)] using equation (8) can be ex-tremely expensive in computational terms.
One willtypically want to calculate this value for all subtrees,the number of which is exponential in the size of thetrees in the training data.
For each subtree t, we willneed to consider the set of all its derivations (expo-nential in the size of t), and for each derivation theset of supertwigs of the first elementary trees and,for incompletely lexicalized subtrees, the set of su-perprunes of all elementary trees in their derivations.The latter two sets, however, need not be constructedfor every time the expected frequency E[f(t)] is cal-culated.
Instead, we can, as we do in the current im-plementation, keep track of the two sums for everychange of the weights.However, there are many further possibilities forimproving the efficiency of the algorithm that arecurrently not implemented.
Equation (8) remainsvalid under various restrictions on the elementarytrees that we are willing to consider as productiveunits.
Some of these will remove the exponential de-pendence on the size of the trees in the training data.For instance, in the case where we restrict the pro-ductive units (with nonzero weights) to depth-1 trees(i.e.
CFG rules), equation (8) collapses to the prod-uct of inside and outside probabilities, which can becalculated using dynamical programming in polyno-mial time (Lari and Young, 1990).
A major topic forfuture research is to define linguistically motivatedrestrictions that allow for efficient computation.Another concern is the size of the grammar theestimation procedure produces, and hence the timeand space efficiency of the resulting parser.
Ta-ble 1 already showed that push-n-pull leads to amore concise grammar.
The reason is that many po-tential elementary trees receive a score (and weight)0.
More generally, push-n-pull generates extremelytilted score distributions, which allows for evenmore compact but highly accurate approximations.In table 2 we show, for the d = 4 grammar of fig-ure 1, that a 10-fold reduction of the grammar sizeby pruning elementary trees with low scores, leadsonly to a small decrease in the LP and LR measures.Another interesting question is if and how thecurrent algorithm can be extended to the full classof Stochastic Tree-Adjoining Grammars (Schabes,1992; Resnik, 1992).
With the added operation ofadjunction, equation (8) is not valid anymore.
Giventhe computational complexities that it already givesrise to, however, it seems that issue of linguisti-cally motivated restrictions (other than lexicaliza-tion) should be considered first.
Finally, given thatthe current approach is dependent on the availabilityof a large annotated corpus, an important questionis if and how it can be extended to work with un-labeled data.
That is, can we transform the push-n-pull algorithm to perform the unsupervised learningof STSGs?
Although most work on unsupervisedgrammar learning concerns SCFGs (including someof our own (Zuidema, 2003)) it is interesting to notethat much of the evidence for construction grammarin fact comes from the language acquisition litera-ture (Tomasello, 2000).6 ConclusionsTheoretical linguistics has long strived to accountfor the unbounded productivity of natural languagesyntax with as few units and rules of combinationas possible.
In contrast, construction grammar andrelated theories of grammar postulate a heteroge-neous and redundant storage of ?constructions?.
Ifthis view is correct, we expect to see statistical sig-natures of these constructions in the distributionalinformation that can be derived from corpora of nat-ural language utterances.
How can we recover thosesignatures?
In this paper we have presented an ap-proach to identifying the relevant statistical correla-tions in a corpus based on the assumption that the34TOPVB?SHOW?VP*PRP?ME?NPNP*DT NNSNP**PP-DIR PP-DIR*(a) The ?show me NP PP?
frame,which occurs very frequently inthe training data and is repre-sented in several elementary treeswith high weight.WHNP-1WDT?WHICH?PPIN?OF?NPDT?THESE?NNS?FLIGHTS?
(b) The complete parse treefor the sentence ?Which ofthese flights?, which occurs16 times in training data.TOPNNS?FLIGHTS?NP*PP-DIRIN?FROM?NP**NNP NNP*PP-DIR*TO?TO?NNP**(c) The frame for ?flights from NP toNP?1.
((TOP (VB ?SHOW?)
(VP* (PRP ?ME?)
(NP (NP* DT NNS) (NP** PP-DIR PP-DIR*)))) 17.79 0.008 30)2.
((TOP (VB ?SHOW?)
(VP* (PRP ?ME?)
(NP (NP* DT NNS) NP**))) 10.34 0.004 463.
(TOP (PRP ?I?)
(VP (MD ?WOULD?)
(VP* (VB ?LIKE?)
(VP** TO VP***)))) 10.02 0.009 204.
(WHNP-1 (WDT ?WHICH?)
(PP (IN ?OF?)
(NP (DT ?THESE?)
(NNS ?FLIGHTS?))))
8.80 0.078 165.
(TOP (WP ?WHAT?)
(SQ (VBZ ?IS?)
(NP-SBJ (DT ?THE?)
(NN ?PRICE?))))
8.76 0.005 206.
(TOP (WHNP (WDT ?WHAT?)
(NNS ?FLIGHTS?))
(SQ (VBP ?ARE?)
(SQ* (EX ?THERE?)
SQ**))) 8.25 0.006 367.
(VP* (PRP ?ME?)
(NP (NP* (DT ?THE?)
(NNS ?FLIGHTS?))
(NP** (PP-DIR IN NNP) (PP-DIR* TO NNP*)))) 7.90 0.023 188.
(TOP (WHNP (WDT ?WHAT?)
(NNS ?FLIGHTS?))
(SQ (VBP ?ARE?)
(SQ* (EX ?THERE?)
(SQ** PP-DIR-3 PP-DIR-4)))) 6.64 0.005 269.
(TOP (PRP ?I?)
(VP MD (VP* (VB ?LIKE?)
(VP** TO VP***)))) 6.48 0.006 2010.
(TOP (PRP ?I?)
(VP (VBP ?NEED?)
(NP (NP* DT NN) (NP** PP-DIR NP***)))) 5.01 0.004 1011.
(TOP (VB ?SHOW?)
(VP* (PRP ?ME?)
(NP (DT ?THE?)
NNS))) 4.94 0.002 1612.
(TOP WP (SQ (VBZ ?IS?)
(NP-SBJ (DT ?THE?)
(NN ?PRICE?))))
4.91 0.0028 2013.
(TOP (WHNP (WDT ?WHAT?)
(NNS ?FLIGHTS?))
(SQ (VBP ?ARE?)
(SQ* EX (SQ** PP-DIR-3 PP-DIR-4)))) 4.16 0.003 2614.
(TOP (VB ?SHOW?)
(VP* (PRP ?ME?)
(NP (NNS ?FLIGHTS?)
NP*))) 4.01 0.001 1615.
(TOP (VB ?SHOW?)
(VP* (PRP ?ME?)
(NP (DT ?THE?)
NP*))) 3.94 0.002 1216.
(TOP (WHNP (WDT ?WHAT?)
(NNS ?FLIGHTS?))
(SQ (VBP ?ARE?)
(SQ* EX SQ**))) 3.92 0.003 3617.
(TOP (PRP ?I?)
(VP (VBP ?NEED?)
(NP (NP* DT NN) NP**))) 3.85 0.003 1418.
(TOP (WP ?WHAT?)
(SQ VBZ (NP-SBJ (DT ?THE?)
(NN ?PRICE?))))
3.79 0.002 2019.
(WHNP-1 (WDT ?WHICH?)
(PP (IN ?OF?)
(NP (DT ?THESE?)
NNS))) 3.65 0.032 1620.
(TOP (VB ?SHOW?)
(VP* (PRP ?ME?)
(NP NP* (SBAR WDT VP**)))) 3.64 0.002 1421.
(TOP (VB ?SHOW?)
(VP* PRP (NP (NP* DT NNS) (NP** PP-DIR PP-DIR*)))) 3.61 0.002 3022.
(TOP (WHNP (WDT ?WHAT?)
NNS) (SQ (VBP ?ARE?)
(SQ* (EX ?THERE?)
(SQ** PP-DIR-3 PP-DIR-4)))) 3.30 0.002 2623.
(VP (MD ?WOULD?)
(VP* (VB ?LIKE?)
(VP** (TO ?TO?)
(VP*** VB* VP****)))) 3.25 0.012 1624.
(TOP (WDT ?WHICH?)
VP) 3.1460636 0.001646589 1225.
(TOP (VB ?SHOW?)
(VP* (PRP ?ME?)
(NP (NP* DT NP**) NP***))) 3.03 0.001 1226.
(TOP (VB ?SHOW?)
(VP* (PRP ?ME?)
(NP NP* (NP*** PP-DIR PP-DIR*)))) 2.97 0.001 1227.
(PP (IN ?OF?)
(NP* (NN* ?FLIGHT?)
(NP** NNP (NP*** NNP* NP****)))) 2.95 0.015 828.
(TOP (VB ?SHOW?)
(VP* (PRP ?ME?)
(NP (DT ?THE?)
(NNS ?FARES?))))
2.85 0.001 829.
(VP (VBP ?NEED?)
(NP (NP* (DT ?A?)
(NN ?FLIGHT?))
(NP** PP-DIR NP***))) 2.77 0.009 1230.
(TOP (VB ?SHOW?)
(VP* (PRP ?ME?)
(NP NP* (NP** PP-DIR PP-DIR*)))) 2.77 0.001 3431.
(TOP (JJS ?CHEAPEST?)
(NN ?FARE?))
2.74 0.001 632.
(TOP (VB ?SHOW?)
(VP* (PRP ?ME?)
(NP (NP* DT NP**) (NP*** PP-DIR PP-DIR*)))) 2.71 0.001 833.
(TOP (NN ?PRICE?)
(PP (IN ?OF?)
(NP* (NN* ?FLIGHT?)
(NP** NNP NP***)))) 2.69 0.001 634.
(TOP (NN ?PRICE?)
(PP (IN ?OF?)
(NP* (NN* ?FLIGHT?)
NP**))) 2.68 0.001 835.
(PP-DIR (IN ?FROM?)
(NP (NNP ?WASHINGTON?)
(NP* (NNP* ?D?)
(NNP** ?C?))))
2.67 0.006 636.
(PP-DIR (IN ?FROM?)
(NP** (NNP ?NEWARK?)
(NP*** (NNP* ?NEW?)
(NNP** ?JERSEY?))))
2.60 0.005 637.
(S* (PRP ?I?)
(VP (MD ?WOULD?)
(VP* (VB ?LIKE?)
(VP** TO VP***)))) 2.59 0.11 838.
(TOP (VBZ ?DOES?)
(SQ* (NP-SBJ DT (NN ?FLIGHT?))
(VP (VB ?SERVE?)
(NN* ?DINNER?))))
2.48 0.002 839.
(TOP (PRP ?I?)
(VP (MD ?WOULD?)
(VP* (VB ?LIKE?)
VP**))) 2.37 0.002 2040.
(TOP (WP ?WHAT?)
(SQ (VBZ ?IS?)
(NP-SBJ DT (NN ?PRICE?))))
2.33 0.001 2041.
(S* (PRP ?I?)
(VP MD (VP* (VB ?LIKE?)
(VP** TO VP***)))) 2.33 0.100 842.
(WHNP**** (PP-TMP (IN* ?ON?)
(NNP** ?FRIDAY?))
(PP-LOC (IN** ?ON?)
(NP (NNP*** ?AMERICAN?)
(NNP**** ?AIRLINES?))))
2.30 0.086 643.
(VP* (PRP ?ME?)
(NP (NP* (DT ?THE?)
NNS) (NP** (PP-DIR IN NNP) (PP-DIR* TO NNP*)))) 2.29 0.007 1844.
(TOP (WHNP* (WDT ?WHAT?)
(NNS ?FLIGHTS?))
(WHNP** (PP-DIR (IN ?FROM?)
NNP) (WHNP*** (PP-DIR* TO NNP*) (PP-TMP IN* NNP**)))) 2.28 0.001 1245.
(SQ (VBP ?ARE?)
(SQ* EX (SQ** (PP-DIR-3 IN NNP) (PP-DIR-4 TO NNP*)))) 2.26 0.015 1446.
(TOP (VB ?SHOW?)
(VP* (PRP ?ME?)
(NP (NP* DT NNS) (SBAR WDT VP**)))) 2.22 0.001 847.
(TOP (NNS ?FLIGHTS?)
(NP* (PP-DIR (IN ?FROM?)
(NP** NNP NNP*)) (PP-DIR* (TO ?TO?)
NNP**))) 2.20 0.001 10)48.
((VP (VBP ?NEED?)
(NP (NP* (DT ?A?)
(NN ?FLIGHT?))
(NP** (PP-DIR IN NNP) NP***))) 2.1346128 0.007185978 10)49.
((NP (NP* (DT ?THE?)
(NNS ?FLIGHTS?))
(NP** (PP-DIR (IN ?FROM?)
(NNP ?BALTIMORE?))
(PP-DIR* (TO ?TO?)
(NNP* ?OAKLAND?))))
2.1335514 0.00381956 10)50.
((TOP (VB ?SHOW?)
(VP* (PRP ?ME?)
(NP (NP* DT NNS) (NP** PP-DIR NP***)))) 2.09 0.001 8)Figure 1: Three examples and a list of the first 50 elementary trees with multiple words of an STSG inducedusing the push-n-pull algorithm on the ATIS3 corpus.
For use in the current implementation, the parsetrees have been converted to Chomsky Normal Form (all occurrences of A ?
B, B ?
?
are replaced byA ?
?
; all occurrences of A ?
BC?
are replaced by A ?
BA?, A?
?
C?
), all non-terminal labels aremade unique for a particular parse tree (address labeling not shown) and all top nodes are replaced by thenon-terminal ?TOP?.
Listed are the elementary trees of the induced STSG with for each tree the score, theweight and the frequency with which it occurs in the training set.35corpus is generated by an STSG, and by inferringthe properties of that underlying STSG.
Given ourbest guess of the STSG that generated the data, wecan start to ask questions like: which subtrees areoverrepresented in the corpus?
Which correlationsare so strong that it is reasonable to think of the cor-related phrases as a single unit?
We presented a newalgorithm for estimating weights of an STSG from acorpus, and reported promising empirical results ona small corpus.AcknowledgmentsThe author is funded by the Netherlands Organi-sation for Scientific Research (Exacte Wetenschap-pen), project number 612.066.405.
Many thanks toYoav Seginer, Rens Bod and Remko Scha and theanonymous reviewers for very useful comments.ReferencesRens Bod, Remko Scha, and Khalil Sima?an, editors.2003.
Data-Oriented Parsing.
CSLI Publications,University of Chicago Press, Chicago, IL.Rens Bod.
1993.
Using an annotated corpus as a stochas-tic grammar.
In Proceedings EACL?93, pages 37?44.Rens Bod.
1998.
Beyond Grammar: An experience-based theory of language.
CSLI, Stanford, CA.Rens Bod.
2003.
An efficient implementation of a newDOP model.
In Proceedings EACL?03.Michael Collins and Nigel Duffy.
2002.
New rankingalgorithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron.
ACL?02.Adele E. Goldberg.
1995.
Constructions: A Construc-tion Grammar Approach to Argument Structure.
TheUniversity of Chicago Press, Chicago, IL.Joshua Goodman.
1996.
Efficient algorithms for parsingthe DOP model.
In Proceedings EMNLP?96, p. 143?152.C.T.
Hemphill, J.J. Godfrey, and G.R.
Doddington.
1990.The ATIS spoken language systems pilot corpus.
InProceedings of the DARPA Speech and Natural Lan-guage Workshop.
Morgan Kaufman, Hidden Valley.Ray Jackendoff.
2002.
Foundations of Language.
Ox-ford University Press, Oxford, UK.Aravind Joshi and Anoop Sarkar.
2003.
Tree adjoininggrammars and their application to statistical parsing.In Bod et al (Bod et al, 2003), pages 253?282.A.
Joshi, K. Vijay-Shanker, and D. Weir.
1991.
Theconvergence of mildly context-sensitive grammar for-malisms.
In Peter Sells, Stuart Shieber, and Tom Wa-sow, editors, Foundational issues in natural languageprocessing, pages 21?82.
MIT Press, Cambridge MA.P.
Kay and C. Fillmore.
1999.
Grammatical construc-tions and linguistic generalizations.
Language, 75:1?33.K.
Lari and S.J.
Young.
1990.
The estimation of stochas-tic context-free grammars using the inside-outside al-gorithm.
Computer Speech and Language, 4:35?56.Philip Resnik.
1992.
Probabilistic tree-adjoining gram-mar as a framework for statistical natural languageprocessing.
In Proceedings COLING?92, p. 418?424.Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann A.Copestake, and Dan Flickinger.
2002.
Multiword ex-pressions: A pain in the neck for NLP.
In ProceedingsCICLing, pages 1?15.Remko Scha.
1990.
Taaltheorie en taaltechnolo-gie; competence en performance.
In R. de Kortand G.L.J.
Leerdam, editors, Computertoepassingenin de Neerlandistiek, pages 7?22.
LVVN, Almere.http://iaaa.nl/rs/LeerdamE.html.Yves Schabes.
1992.
Stochastic lexicalized tree-adjoining grammars.
In Proceedings COLING?92,pages 425?432.Helmut Schmid.
2004.
Efficient parsing of highly am-biguous context-free grammars with bit vectors.
InProceedings COLING?04.Khalil Sima?an and Luciano Buratto.
2003.
Backoff pa-rameter estimation for the DOP model.
In ProceedingsECML?03, pages 373?384.Khalil Sima?an.
2002.
Computational complexity ofprobabilistic disambiguation.
Grammars, 5(2):125?151.Michael Tomasello.
2000.
The item-based nature of chil-dren?s early syntactic development.
Trends in Cogni-tive Science, 4(4):156?163.Andreas Zollmann and Khalil Sima?an.
2005.
A consis-tent and efficient estimator for data-oriented parsing.Journal of Automata, Languages and Combinatorics.Willem Zuidema.
2003.
How the poverty of the stimulussolves the poverty of the stimulus.
In Suzanna Becker,Sebastian Thrun, and Klaus Obermayer, editors, Ad-vances in Neural Information Processing Systems 15,pages 51?58.
MIT Press, Cambridge, MA.Willem Zuidema.
2006.
Theoretical evaluation of esti-mation methods for Data-Oriented Parsing.
In Pro-ceedings EACL?06 (Conference Companion), pages183?186.36
