Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 213?216,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPChinese Term Extraction Using Different Types of RelevanceYuhang Yang1, Tiejun Zhao1, Qin Lu2, Dequan Zheng1 and Hao Yu11School of Computer Science and Technology,Harbin Institute of Technology, Harbin 150001, China{yhyang,tjzhao,dqzheng,yu}@mtlab.hit.edu.cn2Department of Computing,The Hong Kong Polytechnic University, Hong Kong, Chinacsluqin@comp.polyu.edu.hkAbstractThis paper presents a new term extraction ap-proach using relevance between term candi-dates calculated by a link analysis basedmethod.
Different types of relevance are usedseparately or jointly for term verification.
Theproposed approach requires no prior domainknowledge and no adaptation for new domains.Consequently, the method can be used in anydomain corpus and it is especially useful forresource-limited domains.
Evaluations con-ducted on two different domains for Chineseterm extraction show significant improve-ments over existing techniques and also verifythe efficiency and relative domain independentnature of the approach.1 IntroductionTerms are the lexical units to represent the mostfundamental knowledge of a domain.
Term ex-traction is an essential task in domain knowledgeacquisition which can be used for lexicon update,domain ontology construction, etc.
Term extrac-tion involves two steps.
The first step extractscandidates by unithood calculation to qualify astring as a valid term.
The second step verifiesthem through termhood measures (Kageura andUmino, 1996) to validate their domain specificity.Many previous studies are conducted on termcandidate extraction.
Other tasks such as namedentity recognition, meaningful word extractionand unknown word detection, use techniquessimilar to that for term candidate extraction.
But,their focuses are not on domain specificity.
Thisstudy focuses on the verification of candidates bytermhood calculation.Relevance between term candidates and docu-ments is the most popular feature used for termverification such as TF-IDF (Salton and McGill,1983; Frank, 1999) and Inter-Domain Entropy(Chang, 2005), which are all based on the hy-pothesis that ?if a candidate occurs frequently ina few documents of a domain, it is likely a term?.Limited distribution information of term candi-dates in different documents often limits the abil-ity of such algorithms to distinguish terms fromnon-terms.
There are also attempts to use priordomain specific knowledge and annotated cor-pora for term verification.
TV_ConSem (Ji andLu, 2007) calculates the percentage of contextwords in a domain lexicon using both frequencyinformation and semantic information.
However,this technique requires a domain lexicon whosesize and quality have great impact on the per-formance of the algorithm.
Some supervisedlearning approaches have been applied to pro-tein/gene name recognition (Zhou et al, 2005)and Chinese new word identification (Li et al,2004) using SVM classifiers (Vapnik, 1995)which also require large domain corpora and an-notations.
The latest work by Yang (2008) ap-plied the relevance between term candidates andsentences by using the link analysis approachbased on the HITS algorithm to achieve betterperformance.In this work, a new feature on the relevancebetween different term candidates is integratedwith other features to validate their domainspecificity.
The relevance between candidateterms may be useful to identify domain specificterms based on two assumptions.
First, terms aremore likely to occur with other terms in order toexpress domain information.
Second, term can-didates extracted from domain corpora are likely213to be domain specific.
Previous work by (e.g.
Jiand Lu, 2007) uses similar information by com-paring the context to an existing large domainlexicon.
In this study, the relevance betweenterm candidates are iteratively calculated bygraphs using link analysis algorithm to avoid thedependency on prior domain knowledge.The rest of the paper is organized as follows.Section 2 describes the proposed algorithms.Section 3 explains the experiments and the per-formance evaluation.
Section 4 concludes andpresents the future plans.2 MethodologyThis study assumes the availability of term can-didates since the focus is on term verification bytermhood calculation.
Three types of relevanceare first calculated including (1) the term candi-date relevance, CC; (2) the candidate to sentencerelevance, CS; and the candidates to documentrelevance, CD.
Terms are then verified by usingdifferent types of relevance.2.1 Relevance between Term CandidatesBased on the assumptions that term candidatesare likely to be used together in order to repre-sent a particular domain concept, relevance ofterm candidates can be represented by graphs ina domain corpus.
In this study, CC is defined astheir co-occurrence in the same sentence of thedomain corpus.
For each document, a graph ofterm candidates is first constructed.
In the graph,a node is a term candidate.
If two term candi-dates TC1 and TC2 occur in the same sentence,two directional links between TC1 to TC2 aregiven to indicate their mutually related.
Candi-dates with overlapped substrings are not removedwhich means long terms can be linked to theircomponents if the components are also candi-dates.After graph construction, the term candidaterelevance, CC, is then iteratively calculated usingthe PageRank algorithm (Page et al 1998) origi-nally proposed for information retrieval.
PageR-ank assumes that the more a node is connected toother nodes, it is more likely to be a salient node.The algorithm assigns the significance score toeach node according to the number of nodes link-ing to it as well as the significance of the nodes.The PageRank calculation PR of a node A isshown as follows:))()(...)()()()(()1()(2211ttBCBPRBCBPRBCBPRddAPR ++++?=(1)where B1, B2,?, Bt are all nodes linked to node A;C(Bi) is the number of outgoing links from nodeBi; d is the factor to avoid loop trap in thegraphic structure.
d is set to 0.85 as suggested in(Page et al, 1998).
Initially, all PR weights areset to 1.
The weight score of each node are ob-tained by (1), iteratively.
The significance ofeach term candidate in the domain specific cor-pus is then derived based on the significance ofother candidates it co-occurred with.
The CCweight of term candidate TCi is given by its PRvalue after k iterations, a parameter to be deter-mined experimentally.2.2 Relevance between Term Candidatesand SentencesA domain specific term is more likely to be con-tained in domain relevant sentences.
Relevancebetween term candidate and sentences, referredto as CS, is calculated using the TV_HITS (TermVerification ?
HITS) algorithm proposed in(Yang et al, 2008) based on  Hyperlink-InducedTopic Search (HITS) algorithm (Kleinberg,1997).
In TV_HITS, a good hub in the domaincorpus is a sentence that contains many goodauthorities; a good authority is a term candidatethat is contained in many good hubs.In TV_HITS, a node p can either be a sentenceor a term candidate.
If a term candidate TC iscontained in a sentence Sen of the domain corpus,there is a directional link from Sen to TC.TV_HITS then makes use of the relationship be-tween candidates and sentences via an iterativeprocess to update CS weight for each TC.Let VA(w(p1)A, w(p2)A,?, w(pn)A) denote theauthority vector and VH(w(p1)H, w(p2)H,?, w(pn)H)denote the hub vector.
VA and VH are initializedto (1, 1,?, 1).
Given weights VA and VH with adirectional link p?q, w(q)A and w(p)H are up-dated by using the I operation(an in-pointer to anode) and the O operation(an out-pointer to anode) shown as follows.
The CS weight of termcandidate TCi is given by its w(q)A value afteriteration.I operation:          (2) ??
?=EqpHA w(p)w(q)O operation:         (3) ??
?=EqpAH w(q)w(p)2.3 Relevance between Term Candidatesand DocumentsThe relevance between term candidates anddocuments is used in many term extraction algo-214rithms.
The relevance is measured by the TF-IDFvalue according to the following equations:)IDF(TC)TF(TC)TFIDF(TC iii ?=      (4)))(log()(ii TCDFDTCIDF =             (5)where TF(TCi) is the number of times term can-didate TCi occurs in the domain corpus, DF(TCi)is the number of documents in which TCi occursat least once, |D| is the total number of docu-ments in the corpus, IDF(TCi) is the inversedocument frequency which can be calculatedfrom the document frequency.2.4 Combination of RelevanceTo evaluate the effective of the different types ofrelevance, they are combined in different ways inthe evaluation.
Term candidates are then rankedaccording to the corresponding termhood valuesTh(TC) and the top ranked candidates are con-sidered terms.For each document Dj in the domain corpuswhere a term candidate TCi occurs, there is CCijweight and a CSij weight.
When features CC andCS are used separately, termhood ThCC(TCi) andThCS(TCi) are calculated by averaging CCij andCSij, respectively.
Termhood of different combi-nations are given in formula (6) to (9).
R(TCi)denotes the ranking position of TCi.
)(TCR)(TCR)(TCThiCSiCCiCSCC11 +=+    (6))log()()(CjijiCDCC DFDCCTCTh ?=+     (7))log()()(CjijiCDCS DFDCSTCTh ?=+     (8))(TCR)(TCRTCThiCDCSiCDCCiCDCSCC++++ += 11)( (9)3 Performance Evaluation3.1 Data PreparationTo evaluate the performance of the proposedrelevance measures for Chinese in different do-mains, experiments are conducted on two sepa-rate domain corpora CorpusIT and CorpusLegal.,respectively.
CorpusIT includes academic papersof 6.64M in size from Chinese IT journals be-tween 1998 and 2000.
CorpusLegal includes thecomplete set of official Chinese constitutionallaw articles and Economics/Finance law articlesof 1.04M in size (http://www.law-lib.com/).For comparison to previous work, all termcandidates are extracted from the same domaincorpora using the delimiter based algorithmTCE_DI (Term Candidate Extraction ?
DelimiterIdentification) which is efficient according to(Yang et al, 2008).
In TCE_DI, term delimitersare identified first.
Words between delimiters arethen taken as term candidates.The performances are evaluated in terms ofprecision (P), recall (R) and F-value (F).
Sincethe corpora are relatively large, sampling is usedfor evaluation based on fixed interval of 1 ineach 10 ranked results.
The verification of all thesampled data is carried out manually by two ex-perts independently.
To evaluate the recall, a setof correct terms which are manually verifiedfrom the extracted terms by different methods isconstructed as the standard answer.
The answerset is certainly not complete.
But it is useful as aperformance indication for comparison since it isfair to all algorithms.3.2 Evaluation on Term ExtractionFor comparison, three reference algorithms areused in the evaluation.
The first algorithm isTV_LinkA which takes CS and CD into consid-eration and performs well (Yang et al, 2008).The second one is a supervised learning ap-proach based on a SVM classifier, SVMlight(Joachims, 1999).
Internal and external featuresare used by SVMlight.
The third algorithm is thepopular used TF-IDF algorithm.
All the refer-ence algorithms require no training exceptSVMlight.
Two training sets containing thousandsof positive and negative examples from IT do-main and legal domain are constructed for theSVM classifier.
The training and testing sets arenot overlapped.Table 1 and Table 2 show the performance ofthe proposed algorithms using different featuresfor IT domain and legal domain, respectively.The algorithm using CD alone is the same as theTF-IDF algorithm.
The algorithm using CS andCD is the TV_LinkA algorithm.Algorithms Precision(%)Recall(%)F-value(%)SVM 63.6 49.5 55.6CC 47.1 36.5 41.2CS 65.6 51 57.4CD(TF-IDF) 64.8 50.4 56.7CC+CS 80.4 62.5 70.3CC+CD 49 38.1 42.9CS+CD(TV_LinkA)75.4 58.6 66CC+CS+CD 82.8 64.4 72.4Table 1.
Performance on IT Domain215Algorithms Precision(%)Recall(%)F-value(%)SVM 60.1 54.2 57.3CC 45.2 40.3 42.6CS 70.5 40.1 51.1CD(TF-IDF) 59.4 52.9 56CC+CS 64.2 49.9 56.1CC+CD 48.4 43.1 45.6CS+CD(TV_LinkA)67.4 60.1 63.5CC+CS+CD 70.2 62.6 66.2Table 2.
Performance on Legal DomainTable 1 and Table 2 show that the proposedalgorithms achieve similar performance on bothdomains.
The proposed algorithm using all threefeatures (CC+CS+CD) performs the best.
Theresults confirm that the proposed approach arequite stable across domains and the relevancebetween candidates are efficient for improvingperformance of term extraction in different do-mains.
The algorithm using CC only does notachieve good performance.
Neither does CC+CS.The main reason is that the term candidates usedin the experiments are extracted using theTCE_DI algorithm which can extract candidateswith low statistical significance.
TCE_DI pro-vides a better compromise between recall andprecision.
CC alone is vulnerable to noisy candi-dates since it relies on the relevance betweencandidates themselves.
However, as an addi-tional feature to the combined use of CS and CD(TV_LinkA), improvement of over 10% on F-value is obtained for the IT domain, and 5% forthe legal domain.
This is because the noise dataare eliminated by CS and CD, and CC help toidentify additional terms that may not be statisti-cally significant.4 Conclusion and Future WorkIn conclusion, this paper exploits the relevancebetween term candidates as an additional featurefor term extraction approach.
The proposed ap-proach requires no prior domain knowledge andno adaptation for new domains.
Experiments forterm extraction are conducted on IT domain andlegal domain, respectively.
Evaluations indicatethat the proposed algorithm using different typesof relevance achieves the best performance inboth domains without training.In this work, only co-occurrence in a sentenceis used as the relevance between term candidates.Other features such as syntactic relations canalso be exploited.
The performance may be fur-ther improved by using more efficient combina-tion strategies.
It would also be interesting toapply this approach to other languages such asEnglish.Acknowledgement: The project is partially sup-ported by the Hong Kong Polytechnic University(PolyU CRG G-U297)ReferencesChang Jing-Shin.
2005.
Domain Specific Word Ex-traction from Hierarchical Web Documents: AFirst Step toward Building Lexicon Trees fromWeb Corpora.
In Proc of the 4th SIGHAN Work-shop on Chinese Language Learning: 64-71.Eibe Frank, Gordon.
W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.
Do-main-specific Keyphrase Extraction.
In Proc.of16th Int.
Joint Conf.
on AI,  IJCAI-99: 668-673.Joachims T. 2000.
Estimating the Generalization Per-formance of a SVM Efficiently.
In Proc.
of the IntConf.
on Machine Learning, Morgan Kaufman,2000.Kageura K., and B. Umino.
1996.
Methods of auto-matic term recognition: a review.
Term 3(2):259-289.Kleinberg J.
1997.
Authoritative sources in a hyper-linked environment.
In Proc.
of the 9th ACM-SIAMSymposium on Discrete Algorithms: 668-677.
NewOrleans, America, January 1997.Ji Luning, and Qin Lu.
2007.
Chinese Term Extrac-tion Using Window-Based Contextual Information.In Proc.
of CICLing 2007, LNCS 4394: 62 ?
74.Li Hongqiao, Chang-Ning Huang, Jianfeng Gao, andXiaozhong Fan.
The Use of SVM for Chinese NewWord Identification.
In Proc.
of the 1st Int.JointConf.
on NLP (IJCNLP2004): 723-732.
Hainan Is-land, China, March 2004.Salton, G., and McGill, M.J. (1983).
Introduction toModern Information Retrieval.
McGraw-Hill.S.
Brin, L. Page.
The anatomy of a large-scale hyper-textual web search engine.
The 7th Int.
World WideWeb Conf, Brisbane, Australia, April 1998, 107-117.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer, 1995.Yang Yuhang, Qin Lu, Tiejun Zhao.
(2008).
ChineseTerm Extraction Using Minimal Resources.
The22nd Int.
Conf.
on Computational Linguistics (Col-ing 2008).
Manchester, Aug., 2008, 1033-1040.Zhou GD, Shen D, Zhang J, Su J, and Tan SH.
2005.Recognition of Protein/Gene Names from Text us-ing an Ensemble of Classifiers.
BMC Bioinformat-ics 2005, 6(Suppl 1):S7.216
