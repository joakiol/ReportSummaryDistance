Constructing Semantic Space Models from Parsed CorporaSebastian Pad?Department of Computational LinguisticsSaarland UniversityPO Box 15 11 5066041 Saarbr?cken, Germanypado@coli.uni-sb.deMirella LapataDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 Portobello StreetSheffield S1 4DP, UKmlap@dcs.shef.ac.ukAbstractTraditional vector-based models use wordco-occurrence counts from large corporato represent lexical meaning.
In this pa-per we present a novel approach for con-structing semantic spaces that takes syn-tactic relations into account.
We introducea formalisation for this class of modelsand evaluate their adequacy on two mod-elling tasks: semantic priming and auto-matic discrimination of lexical relations.1 IntroductionVector-based models of word co-occurrence haveproved a useful representational framework for avariety of natural language processing (NLP) taskssuch as word sense discrimination (Sch?tze, 1998),text segmentation (Choi et al, 2001), contextualspelling correction (Jones and Martin, 1997), auto-matic thesaurus extraction (Grefenstette, 1994), andnotably information retrieval (Salton et al, 1975).Vector-based representations of lexical meaninghave been also popular in cognitive science andfigure prominently in a variety of modelling stud-ies ranging from similarity judgements (McDonald,2000) to semantic priming (Lund and Burgess, 1996;Lowe and McDonald, 2000) and text comprehension(Landauer and Dumais, 1997).In this approach semantic information is extractedfrom large bodies of text under the assumption thatthe context surrounding a given word provides im-portant information about its meaning.
The semanticproperties of words are represented by vectors thatare constructed from the observed distributional pat-terns of co-occurrence of their neighbouring words.Co-occurrence information is typically collected ina frequency matrix, where each row corresponds toa unique target word and each column represents itslinguistic context.Contexts are defined as a small number of wordssurrounding the target word (Lund and Burgess,1996; Lowe and McDonald, 2000) or as entire para-graphs, even documents (Landauer and Dumais,1997).
Context is typically treated as a set ofunordered words, although in some cases syntac-tic information is taken into account (Lin, 1998;Grefenstette, 1994; Lee, 1999).
A word can bethus viewed as a point in an n-dimensional semanticspace.
The semantic similarity between words canbe then mathematically computed by measuring thedistance between points in the semantic space usinga metric such as cosine or Euclidean distance.In the variants of vector-based models where nolinguistic knowledge is used, differences amongparts of speech for the same word (e.g., to drinkvs.
a drink ) are not taken into account in the con-struction of the semantic space, although in somecases word lexemes are used rather than word sur-face forms (Lowe and McDonald, 2000; McDonald,2000).
Minimal assumptions are made with respectto syntactic dependencies among words.
In fact it isassumed that all context words within a certain dis-tance from the target word are semantically relevant.The lack of syntactic information makes the build-ing of semantic space models relatively straightfor-ward and language independent (all that is needed isa corpus of written or spoken text).
However, thisentails that contextual information contributes indis-criminately to a word?s meaning.Some studies have tried to incorporate syntacticinformation into vector-based models.
In this view,the semantic space is constructed from words thatbear a syntactic relationship to the target word of in-terest.
This makes semantic spaces more flexible,different types of contexts can be selected and wordsdo not have to physically co-occur to be consideredcontextually relevant.
However, existing models ei-ther concentrate on specific relations for construct-ing the semantic space such as objects (e.g., Lee,1999) or collapse all types of syntactic relationsavailable for a given target word (Grefenstette, 1994;Lin, 1998).
Although syntactic information is nowused to select a word?s appropriate contexts, this in-formation is not explicitly captured in the contextsthemselves (which are still represented by words)and is therefore not amenable to further processing.A commonly raised criticism for both types of se-mantic space models (i.e., word-based and syntax-based) concerns the notion of semantic similarity.Proximity between two words in the semantic spacecannot indicate the nature of the lexical relations be-tween them.
Distributionally similar words can beantonyms, synonyms, hyponyms or in some casessemantically unrelated.
This limits the applicationof semantic space models for NLP tasks which re-quire distinguishing between lexical relations.In this paper we generalise semantic space modelsby proposing a flexible conceptualisation of contextwhich is parametrisable in terms of syntactic rela-tions.
We develop a general framework for vector-based models which can be optimised for differenttasks.
Our framework allows the construction of se-mantic space to take place over words or syntacticrelations thus bridging the distance between word-based and syntax-based models.
Furthermore, weshow how our model can incorporate well-defined,informative contexts in a principled way which re-tains information about the syntactic relations avail-able for a given target word.We first evaluate our model on semantic prim-ing, a phenomenon that has received much attentionin computational psycholinguistics and is typicallymodelled using word-based semantic spaces.
Wenext conduct a study that shows that our model issensitive to different types of lexical relations.2 Dependency-based Vector Space ModelsOnce we move away from words as the basic con-text unit, the issue of representation of syntactic in-formation becomes pertinent.
Information about thedependency relations between words abstracts overword order and can be considered as an intermediatelayer between surface syntax and semantics.
MoreDetaNlorryAuxmightVcarryAsweetNapplessubjdetauxobjmodFigure 1: A dependency parse of a short sentenceformally, dependencies are asymmetric binary rela-tionships between a head and a modifier (Tesni?re,1959).
The structure of a sentence can be repre-sented by a set of dependency relationships that forma tree as shown in Figure 1.
Here the head of the sen-tence is the verb carry which is in turn modified byits subject lorry and its object apples.It is the dependencies in Figure 1 that will formthe context over which the semantic space will beconstructed.
The construction mechanism sets outby identifying the local context of a target word,which is a subset of all dependency paths startingfrom it.
The paths consist of the dependency edgesof the tree labelled with dependency relations suchas subj, obj, or aux (see Figure 1).
The paths can beranked by a path value function which gives differ-ent weight to different dependency types (for exam-ple, it can be argued that subjects and objects conveymore semantic information than determiners).
Tar-get words are then represented in terms of syntacticfeatures which form the dimensions of the seman-tic space.
Paths are mapped to features by the pathequivalence relation and the appropriate cells in thematrix are incremented.2.1 Definition of Semantic SpaceWe assume the semantic space formalisation pro-posed by Lowe (2001).
A semantic space is a matrixwhose rows correspond to target words and columnsto dimensions which Lowe calls basis elements:Definition 1.
A Semantic Space Model is a matrixK = B?T , where bi ?
B denotes the basis elementof column i, t j ?
T denotes the target word of row j,and Ki j the cell (i, j).T is the set of words for which the matrix con-tains representations; this can be either word typesor word tokens.
In this paper, we assume that co-occurrence counts are constructed over word types,but the framework can be easily adapted to representword tokens instead.In traditional semantic spaces, the cells Ki j ofthe matrix correspond to word co-occurrence counts.This is no longer the case for dependency-basedmodels.
In the following we explain how co-occurrence counts are constructed.2.2 Building the ContextThe first step in constructing a semantic space froma large collection of dependency relations is to con-struct a word?s local context.Definition 2.
The dependency parse p of a sentences is an undirected graph p(s) = (Vp,Ep).
The set ofnodes corresponds to words of the sentence: Vp ={w1, .
.
.
,wn}.
The set of edges is Ep ?Vp ?Vp.Definition 3.
A class q is a three-tuple consistingof a POS-tag, a relation, and another POS-tag.
Wewrite Q for the set of all classes Cat ?R?Cat.
Foreach parse p, the labelling function Lp : Ep ?
Q as-signs a class to every edge of the parse.In Figure 1, the labelling function labels the left-most edge as Lp((a, lorry)) = ?Det,det,N?.
Note thatDet represents the POS-tag ?determiner?
and det thedependency relation ?determiner?.In traditional models, the target words are sur-rounded by context words.
In a dependency-basedmodel, the target words are surrounded by depen-dency paths.Definition 4.
A path ?
is an ordered tuple of edges?e1, .
.
.
,en?
?
Enp so that?
i : (ei?1 = (v1,v2) ?
ei = (v3,v4)) ?
v2 = v3Definition 5.
A path anchored at a word w is a path?e1, .
.
.
,en?
so that e1 = (v1,v2) and w = v1.
Write?w for the set of all paths over Ep anchored at w.In words, a path is a tuple of connected edges ina parse graph and it is anchored at w if it starts at w.In Figure 1, the set of paths anchored at lorry 1 is:{?(lorry,carry)?,?(lorry,carry),(carry,apples)?,?(lorry,a)?,?
(lorry,carry),(carry,might)?, .
.
.
}The local context of a word is the set or a subset ofits anchored paths.
The class information can alwaysbe recovered by means of the labelling function.Definition 6.
A local context of a word w from asentence s is a subset of the anchored paths at w. Afunction c : W ?
2?w which assigns a local contextto a word is called a context specification function.1For the sake of brevity, we only show paths up to length 2.The context specification function allows to elim-inate paths on the basis of their classes.
For exam-ple, it is possible to eliminate all paths from the setof anchored paths but those which contain immedi-ate subject and direct object relations.
This can beformalised as:c(w) = {?
?
?w |?
= ?e??
(Lp(e) = ?V,obj,N?
?Lp(e) = ?V,subj,N?
)}In Figure 1, the labels of the two edges whichform paths of length 1 and conform to this contextspecification are marked in boldface.
Notice that thelocal context of lorry contains only one anchoredpath (c(lorry) = {?(lorry,carry)?
}).2.3 Quantifying the ContextThe second step in the construction of thedependency-based semantic models is to specify therelative importance of different paths.
Linguistic in-formation can be incorporated into our frameworkthrough the path value function.Definition 7.
The path value function v assigns areal number to a path: v : ?
?
R.For instance, the path value function could pe-nalise longer paths for only expressing indirect re-lationships between words.
An example of a length-based path value function is v(?)
= 1nwhere ?
=?e1, .
.
.
,en?.
This function assigns a value of 1 to theone path from c(lorry) and fractions to longer paths.Once the value of all paths in the local contextis determined, the dimensions of the space must bespecified.
Unlike word-based models, our contextscontain syntactic information and dimensions canbe defined in terms of syntactic features.
The pathequivalence relation combines functionally equiva-lent dependency paths that share a syntactic featureinto equivalence classes.Definition 8.
Let ?
be the path equivalence relationon ?.
The partition induced by this equivalence re-lation is the set of basis elements B.For example, it is possible to combine all pathswhich end at the same word: A path which startsat wi and ends at w j, irrespectively of its length andclass, will be the co-occurrence of wi and w j. Thisword-based equivalence function can be defined inthe following manner:?
(v1,v2), .
.
.
,(vn?1,vn)?
?
?
(v?1,v?2), .
.
.
,(v?m?1,v?m)?iff vn = v?mThis means that in Figure 1 the set of basis elementsis the set of words at which paths end.
Although co-occurrence counts are constructed over words like intraditional semantic space models, it is only wordswhich stand in a syntactic relationship to the targetthat are taken into account.Once the value of all paths in the local contextis determined, the local observed frequency for theco-occurrence of a basis element b with the targetword w is just the sum of values of all paths ?
inthis context which express the basis element b. Theglobal observed frequency is the sum of the localobserved frequencies for all occurrences of a targetword type t and is therefore a measure for the co-occurrence of t and b over the whole corpus.Definition 9.
Global observed frequency:?f (b, t) = ?w?W (t)???C(w)???bv(?
)As Lowe (2001) notes, raw frequency counts arelikely to give misleading results.
Due to the Zip-fian distribution of word types, words occurringwith similar frequencies will be judged more similarthan they actually are.
A lexical association func-tion can be used to explicitly factor out chance co-occurrences.Definition 10.
Write A for the lexical associationfunction which computes the value of a cell of thematrix from a co-occurrence frequency:Ki j = A( ?f (bi, t j))3 Evaluation3.1 Parameter SettingsAll our experiments were conducted on the BritishNational Corpus (BNC), a 100 million word col-lection of samples of written and spoken language(Burnard, 1995).
We used Lin?s (1998) broad cover-age dependency parser MINIPAR to obtain a parsedversion of the corpus.
MINIPAR employs a man-ually constructed grammar and a lexicon derivedfrom WordNet with the addition of proper names(130,000 entries in total).
Lexicon entries con-tain part-of-speech and subcategorization informa-tion.
The grammar is represented as a network of35 nodes (i.e., grammatical categories) and 59 edges(i.e., types of syntactic (dependency) relationships).MINIPAR uses a distributed chart parsing algorithm.Grammar rules are implemented as constraints asso-ciated with the nodes and edges.Cosine distance cos(~x,~y) = ?i xiyi?
?i x2i?
?i y2iSkew divergence s?
(~x,~y) = ?i xi log xi?xi+(1??
)yiFigure 2: Distance measuresThe dependency-based semantic space was con-structed with the word-based path equivalence func-tion from Section 2.3.
As basis elements for our se-mantic space the 1000 most frequent words in theBNC were used.
Each element of the resulting vec-tor was replaced with its log-likelihood value (seeDefinition 10 in Section 2.3) which can be consid-ered as an estimate of how surprising or distinctivea co-occurrence pair is (Dunning, 1993).We experimented with a variety of distance mea-sures such as cosine, Euclidean distance, L1 norm,Jaccard?s coefficient, Kullback-Leibler divergenceand the Skew divergence (see Lee 1999 for anoverview).
We obtained the best results for co-sine (Experiment 1) and Skew divergence (Experi-ment 2).
The two measures are shown in Figure 2.The Skew divergence represents a generalisation ofthe Kullback-Leibler divergence and was proposedby Lee (1999) as a linguistically motivated distancemeasure.
We use a value of ?
= .99.We explored in detail the influence of differenttypes and sizes of context by varying the contextspecification and path value functions.
Contextswere defined over a set of 23 most frequent depen-dency relations which accounted for half of the de-pendency edges found in our corpus.
From these,we constructed four context specification functions:(a) minimum contexts containing paths of length 1(in Figure 1 sweet and carry are the minimum con-text for apples), (b) np context adds dependency in-formation relevant for noun compounds to minimumcontext, (c) wide takes into account paths of lengthlonger than 1 that represent meaningful linguistic re-lations such as argument structure, but also prepo-sitional phrases and embedded clauses (in Figure 1the wide context of apples is sweet, carry, lorry, andmight ), and (d) maximum combined all of the aboveinto a rich context representation.Four path valuation functions were used: (a) plainassigns the same value to every path, (b) lengthassigns a value inversely proportional to a path?slength, (c) oblique ranks paths according to theobliqueness hierarchy of grammatical relations(Keenan and Comrie, 1977), and (d) oblengthcontext specification path value function1 minimum plain2 minimum oblique3 np plain4 np length5 np oblique6 np oblength7 wide plain8 wide length9 wide oblique10 wide oblength11 maximum plain12 maximum length13 maximum oblique14 maximum oblengthTable 1: The fourteen modelscombines length and oblique .
The resulting 14parametrisations are shown in Table 1.
Length-based and length-neutral path value functions arecollapsed for the minimum context specificationsince it only considers paths of length 1.We further compare in Experiments 1 and 2 ourdependency-based model against a state-of-the-artvector-based model where context is defined as a?bag of words?.
Note that considerable latitude isallowed in setting parameters for vector-based mod-els.
In order to allow a fair comparison, we se-lected parameters for the traditional model that havebeen considered optimal in the literature (Patel et al,1998), namely a symmetric 10 word window andthe most frequent 500 content words from the BNCas dimensions.
These parameters were similar tothose used by Lowe and McDonald (2000) (symmet-ric 10 word window and 536 content words).
Againthe log-likelihood score is used to factor out chanceco-occurrences.3.2 Experiment 1: PrimingA large number of modelling studies in psycholin-guistics have focused on simulating semantic prim-ing studies.
The semantic priming paradigm pro-vides a natural test bed for semantic space modelsas it concentrates on the semantic similarity or dis-similarity between a prime and its target, and it isprecisely this type of lexical relations that vector-based models capture.In this experiment we focus on Balota and Lorch?s(1986) mediated priming study.
In semantic primingtransient presentation of a prime word like tiger di-rectly facilitates pronunciation or lexical decision ona target word like lion.
Mediated priming extendsthis paradigm by additionally allowing indirectly re-lated words as primes ?
like stripes, which is onlyrelated to lion by means of the intermediate concepttiger.
Balota and Lorch (1986) obtained small medi-ated priming effects for pronunciation tasks but notfor lexical decision.
For the pronunciation task, re-action times were reduced significantly for both di-rect and mediated primes, however the effect waslarger for direct primes.There are at least two semantic space simulationsthat attempt to shed light on the mediated primingeffect.
Lowe and McDonald (2000) replicated boththe direct and mediated priming effects, whereasLivesay and Burgess (1997) could only replicate di-rect priming.
In their study, mediated primes werefarther from their targets than unrelated words.3.2.1 Materials and DesignMaterials were taken form Balota and Lorch(1986).
They consist of 48 target words, each pairedwith a related and a mediated prime (e.g., lion-tiger-stripes).
Each related-mediated prime tuple waspaired with an unrelated control randomly selectedfrom the complement set of related primes.3.2.2 ProcedureOne stimulus was removed as it had a low cor-pus frequency (less than 100), which meant thatthe resulting vector would be unreliable.
We con-structed vectors from the BNC for all stimuli withthe dependency-based models and the traditionalmodel, using the parametrisations given in Sec-tion 3.1 and cosine as a distance measure.
We calcu-lated the distance in semantic space between targetsand their direct primes (TarDirP), targets and theirmediated primes (TarMedP), targets and their unre-lated controls (TarUnC) for both models.3.2.3 ResultsWe carried out a one-way Analysis of Variance(ANOVA) with the distance as dependent variable(TarDirP, TarMedP, TarUnC).
Recall from Table 1that we experimented with fourteen different con-text definitions.
A reliable effect of distance wasobserved for all models (p < .001).
We used the?2 statistic to calculate the amount of variance ac-counted for by the different models.
Figure 3 plots?2 against the different contexts.
The best resultwas obtained for model 7 which accounts for 23.1%of the variance (F(2,140) = 20.576, p < .001) andcorresponds to the wide context specification andthe plain path value function.
A reliable distanceeffect was also observed for the traditional vector-based model (F(2,138) = 9.384, p < .001).00.050.10.150.20.251  2  3  4  5  6  7  8  9  10  11  12  13  14etasquaredmodelTarDirP -- TarMedP -- TarUnCTarDirP -- TarUnCTarMedP -- TarUnCFigure 3: ?2 scores for mediated priming materialsModel TarDirP ?
TarUnC TarMedP ?
TarUnCModel 7 F = 25.290 (p < .001) F = .001 (p = .790)Traditional F = 12.185 (p = .001) F = .172 (p = .680)L & McD F = 24.105 (p < .001) F = 13.107 (p < .001)Table 2: Size of direct and mediated priming effectsPairwise ANOVAs were further performed to ex-amine the size of the direct and mediated priming ef-fects individually (see Table 2).
There was a reliabledirect priming effect (F(1,94) = 25.290, p < .001)but we failed to find a reliable mediated primingeffect (F(1,93) = .001, p = .790).
A reliable di-rect priming effect (F(1,92) = 12.185, p = .001)but no mediated priming effect was also obtained forthe traditional vector-based model.
We used the ?2statistic to compare the effect sizes obtained for thedependency-based and traditional model.
The bestdependency-based model accounted for 23.1% ofthe variance, whereas the traditional model ac-counted for 12.2% (see also Table 2).Our results indicate that dependency-based mod-els are able to model direct priming across a widerange of parameters.
Our results also show thatlarger contexts (see models 7 and 11 in Figure 3) aremore informative than smaller contexts (see mod-els 1 and 3 in Figure 3), but note that the wide con-text specification performed better than maximum.
Atleast for mediated priming, a uniform path value asassigned by the plain path value function outper-forms all other functions (see Figure 3).Neither our dependency-based model nor the tra-ditional model were able to replicate the mediatedpriming effect reported by Lowe and McDonald(2000) (see L & McD in Table 2).
This may bedue to differences in lemmatisation of the BNC,the parametrisations of the model or the choice ofcontext words (Lowe and McDonald use a spe-cial procedure to identify ?reliable?
context words).Our results also differ from Livesay and Burgess(1997) who found that mediated primes were fur-ther from their targets than unrelated controls, us-ing however a model and corpus different from theones we employed for our comparative studies.
Inthe dependency-based model, mediated primes werevirtually indistinguishable from unrelated words.In sum, our results indicate that a model whichtakes syntactic information into account outper-forms a traditional vector-based model which sim-ply relies on word occurrences.
Our model is ableto reproduce the well-established direct priming ef-fect but not the more controversial mediated prim-ing effect.
Our results point to the need for furthercomparative studies among semantic space modelswhere variables such as corpus choice and size aswell as preprocessing (e.g., lemmatisation, tokeni-sation) are controlled for.3.3 Experiment 2: Encoding of RelationsIn this experiment we examine whether dependency-based models construct a semantic space that encap-sulates different lexical relations.
More specifically,we will assess whether word pairs capturing differ-ent types of semantic relations (e.g., hyponymy, syn-onymy) can be distinguished in terms of their dis-tances in the semantic space.3.3.1 Materials and DesignOur experimental materials were taken fromHodgson (1991) who in an attempt to investigatewhich types of lexical relations induce priming col-lected a set of 142 word pairs exemplifying the fol-lowing semantic relations: (a) synonymy (wordswith the same meaning, value and worth ), (b) su-perordination and subordination (one word is an in-stance of the kind expressed by the other word, painand sensation), (c) category coordination (wordswhich express two instances of a common super-ordinate concept, truck and train), (d) antonymy(words with opposite meaning, friend and enemy),(e) conceptual association (the first word subjectsproduce in free association given the other word,leash and dog), and (f) phrasal association (wordswhich co-occur in phrases private and property).The pairs were selected to be unambiguous exam-ples of the relation type they instantiate and werematched for frequency.
The pairs cover a wide rangeof parts of speech, like adjectives, verbs, and nouns.0.140.150.160.170.180.190.20.211  2  3  4  5  6  7  8  9  10  11  12  13  14etasquaredmodelHodgson skew divergenceFigure 4: ?2 scores for the Hodgson materialsMean PA SUP CO ANT SYNCA 16.25 ?
?
?
?PA 15.13 ?
?SUP 11.04CO 10.45ANT 10.07SYN 8.87Table 3: Mean skew divergences and Tukey test re-sults for model 73.3.2 ProcedureAs in Experiment 1, six words with low fre-quencies (less than 100) were removed from thematerials.
Vectors were computed for the re-maining 278 words for both the traditional andthe dependency-based models, again with theparametrisations detailed in Section 3.1.
We calcu-lated the semantic distance for every word pair, thistime using Skew divergence as distance measure.3.3.3 ResultsWe carried out an ANOVA with the lexical rela-tion as factor and the distance as dependent variable.The lexical relation factor had six levels, namely therelations detailed in Section 3.3.1.
We found no ef-fect of semantic distance for the traditional semanticspace model (F(5,141) = 1.481, p = .200).
The ?2statistic revealed that only 5.2% of the variance wasaccounted for.
On the other hand, a reliable effectof distance was observed for all dependency-basedmodels (p < .001).
Model 7 (wide context specifi-cation and plain path value function) accounted forthe highest amount of variance in our data (20.3%).Our results can be seen in Figure 4.We examined whether there are any significantdifferences among the six relations using Post-hocTukey tests.
The pairwise comparisons for model 7are given in Table 3.
The mean distances for concep-tual associates (CA), phrasal associates (PA), super-ordinates/subordinates (SUP), category coordinates(CO), antonyms (ANT), and synonyms (SYN) arealso shown in Table 3.
There is no significant differ-ence between PA and CA, although SUP, CO, ANT,and SYN, are all significantly different from CA (seeTable 3, where ?
indicates statistical significance,a = .05).
Furthermore, ANT and SYN are signifi-cantly different from PA.Kilgarriff and Yallop (2000) point out that man-ually constructed taxonomies or thesauri are typ-ically organised according to synonymy and hy-ponymy for nouns and verbs and antonymy for ad-jectives.
They further argue that for automaticallyconstructed thesauri similar words are words thateither co-occur with each other or with the samewords.
The relations SYN, SUP, CO, and ANT can bethought of as representing taxonomy-related knowl-edge, whereas CA and PA correspond to the wordclusters found in automatically constructed thesauri.In fact an ANOVA reveals that the distinction be-tween these two classes of relations can be madereliably (F(1,136) = 15.347, p < .001), after col-lapsing SYN, SUP, CO, and ANT into one class andCA and PA into another.Our results suggest that dependency-based vectorspace models can, at least to a certain degree, dis-tinguish among different types of lexical relations,while this seems to be more difficult for traditionalsemantic space models.
The Tukey test revealed thatcategory coordination is reliably distinguished fromall other relations and that phrasal association is re-liably different from antonymy and synonymy.
Tax-onomy related relations (e.g., synonymy, antonymy,hyponymy) can be reliably distinguished from con-ceptual and phrasal association.
However, no reli-able differences were found between closely associ-ated relations such as antonymy and synonymy.Our results further indicate that context encodingplays an important role in discriminating lexical re-lations.
As in Experiment 1 our best results wereobtained with the wide context specification.
Also,weighting schemes such as the obliqueness hierar-chy length again decreased the model?s performance(see conditions 2, 5, 9, and 13 in Figure 4), show-ing that dependency relations contribute equally tothe representation of a word?s meaning.
This pointsto the fact that rich context encodings with a widerange of dependency relations are promising for cap-turing lexical semantic distinctions.
However, theperformance for maximum context specification waslower, which indicates that collapsing all depen-dency relations is not the optimal method, at leastfor the tasks attempted here.4 DiscussionIn this paper we presented a novel semantic spacemodel that enriches traditional vector-based modelswith syntactic information.
The model is highly gen-eral and can be optimised for different tasks.
It ex-tends prior work on syntax-based models (Grefen-stette, 1994; Lin, 1998), by providing a generalframework for defining context so that a large num-ber of syntactic relations can be used in the construc-tion of the semantic space.Our approach differs from Lin (1998) in threeimportant ways: (a) by introducing dependencypaths we can capture non-immediate relationshipsbetween words (i.e., between subjects and objects),whereas Lin considers only local context (depen-dency edges in our terminology); the semanticspace is therefore constructed solely from isolatedhead/modifier pairs and their inter-dependencies arenot taken into account; (b) Lin creates the semanticspace from the set of dependency edges that are rel-evant for a given word; by introducing dependencylabels and the path value function we can selectivelyweight the importance of different labels (e.g., sub-ject, object, modifier) and parametrize the space ac-cordingly for different tasks; (c) considerable flexi-bility is allowed in our formulation for selecting thedimensions of the semantic space; the latter can bewords (see the leaves in Figure 1), parts of speechor dependency edges; in Lin?s approach, it is onlydependency edges (features in his terminology) thatform the dimensions of the semantic space.Experiment 1 revealed that the dependency-basedmodel adequately simulates semantic priming.
Ex-periment 2 showed that a model that relies on richcontext specifications can reliably distinguish be-tween different types of lexical relations.
Our re-sults indicate that a number of NLP tasks couldpotentially benefit from dependency-based models.These are particularly relevant for word sense dis-crimination, automatic thesaurus construction, auto-matic clustering and in general similarity-based ap-proaches to NLP.ReferencesBalota, David A. and Robert Lorch, Jr. 1986.
Depth of au-tomatic spreading activation: Mediated priming effects inpronunciation but not in lexical decision.
Journal of Ex-perimental Psychology: Learning, Memory and Cognition12(3):336?45.Burnard, Lou.
1995.
Users Guide for the British National Cor-pus.
British National Corpus Consortium, Oxford UniversityComputing Service.Choi, Freddy, Peter Wiemer-Hastings, and Johanna Moore.2001.
Latent Semantic Analysis for text segmentation.
InProceedings of EMNLP 2001.
Seattle, WA.Dunning, Ted.
1993.
Accurate methods for the statistics of sur-prise and coincidence.
Computational Linguistics 19:61?74.Grefenstette, Gregory.
1994.
Explorations in Automatic The-saurus Discovery.
Kluwer Academic Publishers.Hodgson, James M. 1991.
Informational constraints on pre-lexical priming.
Language and Cognitive Processes 6:169?205.Jones, Michael P. and James H. Martin.
1997.
Contextualspelling correction using Latent Semantic Analysis.
In Pro-ceedings of the ANLP 97.Keenan, E. and B. Comrie.
1977.
Noun phrase accessibility anduniversal grammar.
Linguistic Inquiry (8):62?100.Kilgarriff, Adam and Colin Yallop.
2000.
What?s in a thesaurus.In Proceedings of LREC 2000. pages 1371?1379.Landauer, T. and S. Dumais.
1997.
A solution to Platos prob-lem: the latent semantic analysis theory of acquisition, in-duction, and representation of knowledge.
Psychological Re-view 104(2):211?240.Lee, Lillian.
1999.
Measures of distributional similarity.
InProceedings of ACL ?99.
pages 25?32.Lin, Dekang.
1998.
Automatic retrieval and clustering of simi-lar words.
In Proceedings of COLING-ACL 1998.
Montr?al,Canada, pages 768?511.Lin, Dekang.
2001.
LaTaT: Language and text analysis tools.In J. Allan, editor, Proceedings of HLT 2001.
Morgan Kauf-mann, San Francisco.Livesay, K. and C. Burgess.
1997.
Mediated priming in high-dimensional meaning space: What is "mediated" in mediatedpriming?
In Proceedings of COGSCI 1997.
Lawrence Erl-baum Associates.Lowe, Will.
2001.
Towards a theory of semantic space.
In Pro-ceedings of COGSCI 2001.
Lawrence Erlbaum Associates,pages 576?81.Lowe, Will and Scott McDonald.
2000.
The direct route: Medi-ated priming in semantic space.
In Proceedings of COGSCI2000.
Lawrence Erlbaum Associates, pages 675?80.Lund, Kevin and Curt Burgess.
1996.
Producing high-dimensional semantic spaces from lexical co-occurrence.Behavior Research Methods, Instruments, and Computers28:203?8.McDonald, Scott.
2000.
Environmental Determinants of LexicalProcessing Effort.
Ph.D. thesis, University of Edinburgh.Patel, Malti, John A. Bullinaria, and Joseph P. Levy.
1998.
Ex-tracting semantic representations from large text corpora.
InProceedings of the 4th Neural Computation and PsychologyWorkshop.
London, pages 199?212.Salton, G, A Wang, and C Yang.
1975.
A vector-space modelfor information retrieval.
Journal of the American Societyfor Information Science 18(613?620).Sch?tze, Hinrich.
1998.
Automatic word sense discrimination.Computational Linguistics 24(1):97?124.Tesni?re, Lucien.
1959.
Elements de syntaxe structurale.Klincksieck, Paris.
