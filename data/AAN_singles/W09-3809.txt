Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 65?68,Paris, October 2009. c?2009 Association for Computational LinguisticsDeductive Parsing with Interaction GrammarsJoseph Le RouxNCLT, School of Computing,Dublin City Universityjleroux@computing.dcu.ieAbstractWe present a parsing algorithm for In-teraction Grammars using the deductiveparsing framework.
This approach bringsnew perspectives to this problem, depart-ing from previous methods which rely onconstraint-solving techniques.1 IntroductionAn Interaction Grammar (IG) (Guillaume and Per-rier, 2008) is a lexicalized grammatical formal-ism that primarily focuses on valency, explicitlyexpressed using polarities decorating syntagms.These polarities and the use of underspecifiedstructures naturally lead parsing to be viewed asa constraint-solving problem ?
for example (Bon-fante et al) reduce the parsing problem to a graph-rewriting problem in (2003) .However, in this article we depart from this ap-proach and present an algorithm close to (Earley,1970) for context-free grammars.
We introducethis algorithm using the standard framework of de-ductive parsing (Shieber et al, 1995).This article is organised as follows: we firstpresent IGs (section 2), then we describe the algo-rithm (section 3).
Finally we discuss some techni-cal points and conclude (sections 4 and 5).2 Interaction GrammarsWe briefly introduce IGs as in (Guillaume and Per-rier, 2008)1.
However, we omit polarized featurestructures, for the sake of exposition.2.1 Polarized Tree DescriptionsThe structures associated with words by the lexi-con are Polarized Tree Descriptions (PTDs).
Theyrepresent fragments of parse trees.
The nodes ofthese structures are labelled with a category and a1This paper also discusses the linguistic motivations be-hind IGs.polarity.
IGs use 4 polarities, P = {?,?,=,?
},namely positive, negative, neutral and virtual.A multiset of polarities is superposable2 if itcontains at most one?
and at most one?.A multiset of polarities is saturated if it containseither (1) one?, one?
and any number of?
and=, or (2) zero?, zero?, any number of?
and atleast one =.The two previous definitions can be extended tonodes: a multiset of nodes is saturated (resp.
su-perposable) if all the elements have the same cat-egory and if the induced multiset of polarities issaturated (resp.
superposable).A PTD is a DAG with four binary relations: theimmediate dominance >, the general dominance>?, the immediate precedence ?
and the generalprecedence ?+.
A valid PTD is a PTD where (1)> and >?
define a tree structure3 , (2) ?
and ?+are restricted to couples of nodes having the sameancestor by >, and (3) one leaf is the anchor.
Inthe rest of this paper, all PTDs will be valid.We now introduce some notations : if n >?m, we say that m is constrained by n and for aset of nodes N , we define Nz = {N |?M ?N ,MzN} wherez is a binary relation.2.2 GrammarsAn IG is a tuple G = {?,C, S,P, phon}, where ?is the terminal alphabet, C the non-terminal alpha-bet, S ?
C the initial symbol, P is a set of PTDswith node labels in C?
P, and phon is a functionfrom anchors in P to ?.The structure obtained from parsing is a syntac-tic tree, a totally ordered tree in which all nodesare labelled with a non-terminal.
We call lab(A)the label of node A.
If a leaf L is labelled with aterminal, this terminal is denoted word(L).2This name comes from the superposition introduced inprevious presentations of IGs.3For readers familiar with D-Tree Grammars (Rambow etal., 1995), > adds an i-edge while >?
adds a d-edge.65We will write M  N if the node M isthe mother of N and N  [N1, .
.
.
, Nk] if theN is the mother of the ordered list of nodes[N1, .
.
.
, Nk].
The order between siblings can alsobe expressed using the relation ??
: M ??
Nmeans that N is the immediate successor of M .?
?+ is the transitive closure of ??
and ?
thereflexive transitive closure of.We define the phonological projection PPof a node as : PP (M) = [t] if M[] and word(M) = t, or PP (M) =[PP (N1) .
.
.
PP (Nk)] if M  [N1, .
.
.
, Nk]A syntactic tree T is a model for a multisetD ofPTDs if there exists a total function I from nodesin D (ND) to nodes in T (NT ).
I must respectthe following conditions, where variables M,Nrange over ND and A,B over NT :1.
I?1(A) is saturated and non-empty.2.
if M > N then I(M) I(N)3. if M >?
N then I(M)?
I(N)4. if M ?
N then I(M) ??
I(N)5. if M ?+ N then I(M) ?
?+ I(N)6. if A B then there exists M ?
I?1(A) andN ?
I?1(B) such that M > N7.
lab(A) = lab(M) for all M ?
I?1(A)8. if phon(M) = w then PP (I(M)) = [w]Given an IG G = {?,C, S,P, phon} and a sen-tence s = w1, .
.
.
, wn in ?
?, a syntactic tree T isa parse tree for s if there exists a multiset of PTDsD from P such that the root node R of T is la-belled with S and PP (R) = [w1, .
.
.
, wn].
Thelanguage generated by G is the set of strings in ?
?for which there is a parse tree.3 Parsing AlgorithmWe use the deductive parsing framework (Shieberet al, 1995).
A state of the parser is encoded as anitem, created by applying deductive rules.
Our al-gorithm resembles the Earley algorithm for CFGsand uses the same rules : prediction, scanning andcompletion.3.1 ItemsItems [A(H,N,F ) ?
?
?
?, i, j, (O,U,D)] con-sist of a dotted rule, 2 position indexes and a 3-tuple of sets of constrained nodes.The dotted rule A(H,N,F ) ?
?
?
?
meansthat there exists a node A in the parse tree withantecedentsH ?N ?F .
Elements of the sequence?
are also nodes of the parse tree.
For the sequence?, the elements have the form Bk(Hk) where Bkis a node of the parse tree and Hk is a subset of itsantecedents, the predicted antecedents.This item asserts that a syntactic tree can be par-tially built from the input grammar and sentence,that contains A  [A1 .
.
.
AkB1 .
.
.
Bl] and thatPP (A1) ?
?
?
?
?
PP (Ak) = [mi+1 .
.
.mj ].The proper use of constrained nodes is managedby O, U and D:?
Nodes in D are available in prediction to findantecedents for new parse tree nodes.?
Nodes in O must be used in a sub-parse.
Touse an item as a completer, O must be empty.?
U contains constrained nodes that have beenused in a prediction, and for which the con-straining nodes have not been completed yet.Moreover, we will use 3 additional symbols: >as the left-hand side of the axiom item which canbe seen as a dummy root, and  or  that markitems for which prediction is not terminated.We will need sequences of antecedents that re-spect the order relations of an IG.
Given a set ofnodes N , we define the set of all these orderings:ord(N ) = {[N1 .
.
.Nk]|(Ni)1?i?k is a partition of N?1 ?
i ?
k,Ni is superposable ?if n1, n2 ?
N and n1 ?
n2 then?1 ?
j < k s.t.
n1 ?
Nj and n2 ?
Nj+1?if n1, n2 ?
N and n1 ?+ n2 then?1 ?
i < j ?
k s.t.
n1 ?
Ni and n2 ?
Nj}3.2 Deductive RulesIn this section, we assume an input sentence s =w1, .
.
.
, wn and a IG G = {?,C, S,P, phon}.Axiom This rule creates the first item.
It pre-pares the prediction of a node of category S start-ing at position 0 without constrained nodes.
[> ?
?S(?
), 0, 0, (?, ?, ?
)]ax66Prediction This rule initializes a sub-parse.
Wedivide it in three in order to introduce the differentconstraints one at a time.
[A(H,N,F )?
?
?
C(HC)?, i, j, (O,U,D)][C(HC , ?, ?)?
, j, j, (?, U,D ?O)] p1In this first step, we initialize a new sub-parseat the current position j where C will be the pre-dicted node that we want to find antecedents for.
Ifsome antecedentsHC have already been predictedwe use them.
The nodes in O, that must be usedin one of the sub-parse of A, become available aspossible antecedents for C.[C(HC , ?, ?)?
, j, j, (?, U1, D1)][C(HC , NC , ?)?
, j, j, (?, U2, D2)]p2??????????
?HC ?NC 6= ?HC ?NC is superposableNC ?
D1 ?
roots(P)D2 = D1 ?NCU2 = U1 ?
(D1 ?NC)In this second step, new antecedents for C areadded from the set NC , chosen among availablenodes in D1 and root nodes from the PTDs ofthe grammar.
The 3 node sets are then updated.Constrained nodes that have been chosen as an-tecedents for C are not available anymore and areadded to the set of used constrained nodes.
[C(HC , NC , ?)?
, j, j, (?, U,D)][C(HC , NC , FC)?
?
?, j, j, (O,U,D)]p3??????????
?HC ?NC ?
FC is saturated?
?
ord((HC ?NC ?
FC)>)FC = ?iQi, Q0 ?
(HC ?NC)>?
, Qi+1 ?
Q>?iO = (HC ?NC ?
FC)>?
?
FCno anchor node in HC ?NC ?
FCIn this last step of prediction, we can choosenew antecedents for C among nodes constrainedby antecedents already chosen in the previoussteps in order to saturate them.
This choice is re-cursive : each added antecedent triggers the pos-sibility of choosing the nodes it constrains.
Thesecond part of this step consists of predicting theshape of the tree.
We need to order and superposethe daughter nodes of the antecedents in such away that ordering relations in PTDs are respected:an element of ord((HC ?NC ?
FC)>) is chosen.Finally, the nodes that must be used in a sub-parse are the ones that are constrained by an-tecedents of C and not antecedents themselves.Scan This is the rule that checks predictionsagainst the input string.
It is similar to the previ-ous rule, but one (and only one) of the antecedentsmust be an anchor.
[C(HC , NC , ?)?
, j, j, (?, U,D)][C(HC , NC , FC)?
?, j, j + 1, (?, U,D)]s??????????????
?HC ?NC ?
FC is saturated(HC ?NC ?
FC)> = ?FC = ?iQi, Q0 ?
(HC ?NC)>?
, Qi+1 ?
Q>?i(HC ?NC ?
FC)>?
?
FC = ?one anchor a in HC ?NC ?
FCphon(a) = wj+1If the expected terminal is read on the inputstring, parsing can proceed.
Note that antecedentsfor C should not constrain nodes that are not an-tecedents of C themselves.Completion This rule extends a parse by com-bining it with a complete sub-parse.
[A(H,N,F )?
?
?
C(Hc)?, i, j, (O1, U1, D1)][C(HC , NC , FC)?
?
?, j, k, (?, U2, D2)][A(H,N,F )?
?C ?
?, i, k, (O3, U3, D3)]c??????????????
?NC ?
D1 ?O1 ?
PD2 ?
(D1 ?O1)?NCU1 ?
U2O3 = O1 ?
U2D3 = D1 ?
U2U3 = U2 ?O1We have to make sure that the second hypothe-sis is a sub-parse for the first : (1) the set of avail-able nodes in the sub-parse must be a subset ofthe available nodes for current parse, (2) the set ofused nodes in the main parse must be a subset ofthe used nodes in the sub-parse and (3) used nodesconstrained by the first hypothesis disappear.Goal Parsing is successful if the following itemis created : [> ?
S?, 0, n, (?, ?, ?
)].4 Discussion4.1 Consistency and completenessAn item [A(H,N,F )?
??
?, i, j, (O,U,D)] as-serts the following invariants :67?
A and the elements ?l of ?
are models forsaturated sets of nodes.
Conditions 1, 7 and 3(reflexive case) of a model are respected.?
Elements ?k of ?
are superposable.
Then wehave ?k ?
(A?1)> (conditions 2 and 6).?
the sequence ??
is compatible with the orderrelations from the PTDs (conditions 4 and 5).?
PP (?1) ?
.
.
.
?
PP (?l) = [wi+1 ?
?
?wj ]?
a node N in U is a constrained node in re-lation >?
with a node such that condition 3holds.These invariants can be checked by inductionon rules.
Hence, such an item asserts there exists afunction J from the nodes of a subset of the PTDsof an IG to a syntactic tree with its root labelledby S and phonological projection w1 .
.
.
wj .
Thisfunction has the same properties as the function Ifor models but conditions 2 to 5 only apply if bothnodes are in the domain of J .
The parsing processextends the domain until (1) all the nodes of eachPTD selected are used and (2) the input string hasbeen read completely.
Then J defines a syntactictree which is a parse tree.4.2 Sources of non-determinismThe parsing problem in IGs is a NP-hard prob-lem (Bonfante et al, 2003).
Our presentation letsus see several sources of non-determinism.In p2, new antecedents are chosen among avail-able nodes and root nodes of PTDs from the in-put grammar.
There is an exponential number ofsuch choices.
However, IGs are lexicalized : onlyPTDs associated with a word in the sentence willbe used and efficient lexical filters have been de-veloped for IGs (Bonfante et al, 2006) that drasti-cally decrease the number of PTDs to consider.In p3 and s, constrained nodes can be chosen asantecedents (nodes in FC).
There is again an ex-ponential number of such choices.
But in existingIGs, nodes have at most one successor by >?
andthere is no chain of nodes in relation by >?.
Con-sequently, |FC | can be bounded by |HC ?NC |.In p3, daughters must be partitioned.
Instead ofbuilding all these partitions in p3 and generatingmany useless items, one can think of a lazy ap-proach like the one proposed by (Nederhof et al,2003) for pomset-CFGS.It can be noticed that the completion rule, whilehaving the most positional indexes, is not a partic-ular source of non-determinism.5 ConclusionWe presented a parsing algorithm for IGs.
Al-though we used a simplified version without polar-ized feature structures, adding a unification mech-anism shouldn?t be an issue.
The novelty ofthis presentation is the use of deductive parsingfor a formalism developed in the model-theoreticframework (Pullum and Scholz, 2001).This change of perspective provides new in-sights on the causes of non-determinism.
It isa first step to a precise complexity study of theproblem.
In the future, it will be interesting tosearch for algorithmical approximations to im-prove efficiency.
Another way to overcome NP-hardness is to restrict superpositions, as in (k-)TT-MCTAGs (Kallmeyer and Parmentier, 2008).ReferencesG.
Bonfante, B. Guillaume, and G. Perrier.
2003.Analyse syntaxique e?lectrostatique.
Traitement Au-tomatique des Langues, 44(3).G.
Bonfante, J.
Le Roux, and G. Perrier.
2006.
Lexi-cal disambiguation with polarities and automata.
InProceedings of CIAA.J.
Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 13(2):94?102.B.
Guillaume and G. Perrier.
2008.
Interaction Gram-mars.
Research Report RR-6621, INRIA.L.
Kallmeyer and Y. Parmentier.
2008.
On the relationbetween TT-MCTAG and RCG.
In Proceedings ofLATA.M.J.
Nederhof, G. Satta, and S. Shieber.
2003.
Par-tially ordered multiset context-free grammars andID/LP parsing.
In Proceedings of IWPT.G.
Pullum and B. Scholz.
2001.
On the distinction be-tween model-theoretic and generative-enumerativesyntactic frameworks.
In Proccedings of LACL.O.
Rambow, K. Vijay-Shanker, and D. Weir.
1995.
D-tree grammars.
In Proceedings of ACL.S.
Shieber, Y. Schabes, and F. Pereira.
1995.
Principlesand implementation of deductive parsing.
Journal ofLogic Programming, 24(1?2):3?36.68
