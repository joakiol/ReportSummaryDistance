Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 683?691,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsRedundancy Detection in ESL WritingsHuichao Xue and Rebecca HwaDepartment of Computer Science,University of Pittsburgh,210 S Bouquet St, Pittsburgh, PA 15260, USA{hux10,hwa}@cs.pitt.eduAbstractThis paper investigates redundancy detec-tion in ESL writings.
We propose a mea-sure that assigns high scores to words andphrases that are likely to be redundantwithin a given sentence.
The measure iscomposed of two components: one cap-tures fluency with a language model; theother captures meaning preservation basedon analyzing alignments between wordsand their translations.
Experiments showthat the proposed measure is five timesmore accurate than the random baseline.1 IntroductionWriting concisely is challenging.
It is especiallythe case when writing in a foreign language thatone is still learning.
As a non-native speaker, itis more difficult to judge whether a word or aphrase is redundant.
This study focuses on auto-matically detecting redundancies in English as aSecond Language learners?
writings.Redundancies occur when the writer includessome extraneous word or phrase that do not addto the meaning of the sentence but possibly makethe sentence more awkward to read.
Upon re-moval of the unnecessary words or phrases, thesentence should improve in its fluency while main-taining the original meaning.
In the NUCLEcorpus (Dahlmeier and Ng, 2011), an annotatedlearner corpus comprised of essays written by pri-marily Singaporean students, 13.71% errors aretagged as ?local redundancy errors?, making re-dundancy error the second most frequent prob-lem.1Although redundancies occur frequently, it hasnot been studied as widely as other ESL errors.
A1The most frequent error type is Wrong collocation/idiompreposition, which comprises 15.69% of the total errors.major challenge is that, unlike mistakes that vio-late the grammaticality of a sentence, redundan-cies do not necessarily ?break?
the sentence.
De-termining which word or phrase is redundant ismore of a stylistic question; it is more subjective,and sometimes difficult even for a native speaker.To the best of our knowledge, this paper reportsa first study on redundancy detection.
In particu-lar, we focus on the task of defining a redundancymeasure that estimates the likelihood that a givenword or phrase within a sentence might be extrane-ous.
We propose a measure that takes into accounteach word?s contribution to fluency and meaning.The fluency component computes the languagemodel score of the sentence after the deletion of aword or a phrase.
The meaning preservation com-ponent makes use of the sentence?s translation intoanother language as pivot, then it applies a statis-tical machine translation (SMT) alignment modelto infer the contribution of each word/phrase to themeaning of the sentence.
As a first experiment, weevaluate our measures on their abilities in pickingthe most redundant phrase of a given length.
Weshow that our measure is five times more accuratethan a random baseline.2 Redundancies in ESL WritingsAccording to The Elements of Style (Strunk,1918): concise writing requires that ?every wordtell.?
In that sense, words that ?do not tell?are redundant.
Determining whether a certainword/phrase is redundant is a stylistic question,which is difficult to quantify.
As a result, most an-notation resources do not explicitly identify redun-dancies.
One exception is the NUCLE corpus.
Be-low are some examples from the NUCLE corpus,where the bold-faced words/phrases are marked asredundant.Ex1: First of all , there should be a careful con-sideration about what are the things that gov-ernments should pay for.683Ex2: GM wishes to reposition itself as an inno-vative company to the public.Ex3: These findings are often unpredictable anduncertain.Ex4: .
.
.
the cost incurred is not only just largesum of money .
.
.These words/phrases are considered redundant be-cause they are unnecessary (e.g.
Ex1, Ex2) orrepetitive (e.g.
Ex3, Ex4).However, in NUCLE?s annotation scheme,some words that were marked redundant are re-ally words that carry undesirable meanings.
Forexample:Ex5: .
.
.
through which they can insert a special.
.
.Ex6: .
.
.
the analysis and therefore selection ofa single solution for adaptation.
.
.Note that unlike redundancies, these undesirablewords/phrases change the sentences?
meanings.Despite the difference in definitions, our exper-imental work uses the NUCLE corpus becauseit provides many real world examples of redun-dancy.While redundancy detection has not yet beenwidely studied, it is related to several areas of ac-tive research, such as grammatical error correction(GEC), sentence simplification and sentence com-pression.Work in GEC attempts to build automatic sys-tems to detect/correct grammatical errors (Lea-cock et al., 2010; Liu et al., 2010; Tetreault et al.,2010; Dahlmeier and Ng, 2011; Rozovskaya andRoth, 2010).
Both redundancy detection and GECaim to improve students?
writings.
However, be-cause redundancies do not necessarily break gram-maticality, they have received little attention inGEC.Sentence compression and sentence simplifica-tion also consider deleting words from input sen-tences.
However, these tasks have different goals.Automated sentence simplification (Coster andKauchak, 2011) systems aim at reducing the gram-matical complexity of an input sentence.
To il-lustrate the difference, consider the phrase ?crit-ical reception.?
A sentence simplification sys-tem might rewrite it into ?reviews?
; but a sys-tem that removes redundancy should leave it un-changed because neither ?critical?
nor ?reception?is extraneous.
Moreover, consider the redundantphrase ?had once before?
in Ex4.
A simplificationsystem does not need to change it because thesewords do not add complexity to the sentence.??and?the?cost?incurred?is?not?only?just?large?sum?of?money?
?Figure 1: Among the three circled words, ?just?is more redundant because deleting it hurts netherfluency nor meaning.Sentence compression systems (Jing, 2000;Knight and Marcu, 2000; McDonald, 2006;Clarke and Lapata, 2007) aim at shortening asentence while retaining the most important in-formation and keeping it grammatically correct.This goal distinguishes these systems from oursin two major aspects.
First, sentence compres-sion systems assume that the original sentence iswell-written; therefore retaining words specific tothe sentence (e.g.
?uncertain?
in Ex3) can be agood strategy (Clarke and Lapata, 2007).
In theESL context, however, even specific words couldstill be redundant.
For example, although ?un-certain?
is specific to Ex3, it is redundant, be-cause its meaning is already implied by ?unpre-dictable?.
Second, sentence compression systemstry to shorten a sentence as much as possible, butan ESL redundancy detector should leave as muchof the input sentences unchanged, if possible.One challenge involved in redundancy detectionis that it often involves open class words (Ex3), aswell as multi-word expressions (Ex1, Ex4).
Cur-rent GEC systems dealing with such error typesare mostly MT based.
MT systems tend to ei-ther require large training corpora (Brockett et al.,2006; Liu et al., 2010), or provide whole sentencerewritings (Madnani et al., 2012).
Hermet andD?esilets (2009) attempted to extract single prepo-sition corrections from whole sentence rewritings.Our work incorporates alignments information tohandle complex changes on both word and phraselevels.In our approximation, we consider MT out-put as an approximation of word/phrase mean-ings.
Using words in other languages to repre-sent meanings has been explored in Carpuat andWu (2007), where the focus is the aligned words?identities.
Our work instead focuses more on howmany words each word is aligned to.3 A Probabilistic Model of RedundancyWe consider a word or a phrase to be redundantif deleting it results in a fluent English sentencethat conveys the same meaning as before.
Forexample, ?not?
and ?the?
are not considered re-684(a) Unaligned Englishwords are consideredredundant.
(b) Multiple English wordsaligned to the same meaningunit.
These words are con-sidered redundant.Figure 2: Configurations our system consider asredundant.
In each figure, the shaded squares arethe words considered to be more redundant thanother words in the same figure.dundant in Figure 1.
This is because discarding?not?
would flip the sentence?s meaning; discard-ing ?the?
would lose a necessary determiner be-fore a noun.
In contrast, discarding ?just?
wouldhurt neither fluency nor meaning.
It is thus con-sidered to be more redundant.Therefore, our computational model needs toconsider words?
contributions to both fluency andmeaning.
Figure 2 illustrates words?
contributionto meaning.
In those two examples, each sub-graph visualizes a sentence: English words corre-spond to squares in the top row, while their mean-ings correspond to circles in the bottom row.
Theknowledge of which word represents what mean-ing helps in evaluating its contribution.
In partic-ular, if a word does not connote any significantmeaning, deleting it would not affect the overallsentence; if several words express the same mean-ing, then deleting some of them might not affectthe overall sentence either.
Also, deleting a moresemantically meaningful word (or phrase) is morelikely to cause a loss of meaning of the overall sen-tence (e.g.
uncertain v.s.
the).Our model computes a single probabilistic valuefor both fluency judgment and meaning preserva-tion ?
the log-likelihood that after deleting a cer-tain word or phrase of a sentence, the new sen-tence is still fluent and conveys the same meaningas before.
This value reflects our definition of re-dundancy ?
the higher this probability, the moreredundant the given word/phrase is.More formally, suppose an English sentence econtains lewords: e = e1e2.
.
.
ele; after somesub-string es,t= es.
.
.
et(1 ?
s ?
t ?
le) isdeleted from e, we obtain a shorter sentence, de-noted as es,t?.
We wish to compute the quantityR(s, t; e), the chance that the sub-string es,tis re-dundant in sentence e. We propose a probabilisticmodel to formalize this notion.LetM be a random variable over some meaningrepresentation; Pr(M |e) is the likelihood that Mcarries the meaning of e. If the sub-string es,tisredundant, then the new sentence es,t?should stillexpress the same meaning; Pr(es,t?|M) computesthe likelihood that the after-deletion sentence canbe generated from meaning M .R(s, t; e)= log?M=mPr(m|e) Pr(es,t?|m)= log?M=mPr(m|e) Pr(es,t?)
Pr(m|es,t?
)Pr(m)= log Pr(es,t?)
+ log?M=mPr(m|es,t?)
Pr(m|e)Pr(m)= LM(es,t?)
+ AGR(M |es,t?, e) (1)The first term LM(es,t?)
is the after-deletion sen-tence?s log-likelihood, which reflects its fluency.We calculate the first term with a trigram languagemodel (LM).The second term AGR(M |es,t?, e) can be inter-preted as the chance that e and es,t?carry the samemeaning, discounted by ?chance agreement?.
Thisterm captures meaning preservation.The two terms above are complementary to eachother.
Intuitively, LM prefers keeping commonwords in es,t?(e.g.
the, to) while AGR preferskeeping words specific to e (e.g.
disease, hyper-tension).To make the calculation of the second termpractical, we make two simplifying assumptions.Assumption 1 A sentence?s meaning can be rep-resented by its translations in another language; itswords?
contributions to the meaning of the sen-tence can be represented by the mapping betweenthe words in the original sentence and its transla-tions (Figure 3).Note that the choice of translation language mayimpact the interpretation of words?
contributions.We will discuss about this issue in our experiments(Section 5).Assumption 2 Instead of considering all possi-ble translations f for e, our computation will makeuse of the most likely translation, f?.685??
??
?
????
??
????????
???
?a new idea is created through results from rigorous process of research .Figure 3: Illustration of Assumption 1 and Approximation 1.
An English sentence?s meaning is presentedas a Chinese translation.
Meanwhile, each (English) word?s contribution to the sentence meaning isrealized as a word alignment.
For Approximation 1, note that sentence alignments normally won?t beaffected before/after deleting words (e.g.
?results from?)
from the source sentence.With the two approximations:AGR(M |es,t?, e) ?
logPr(f?|es,t?)
Pr(f?|e)Pr(f?
)= log Pr(f?|es,t?)
+ C1(e)(We use Ci(e) to denote constant numbers withinsentence e throughout the paper.
)We now rely on a statistical machine translationmodel to approximate the translation probabilitylog Pr(f?|es,t?
).One naive way of calculating this probabil-ity measure is to consult the MT system.
Thismethod, however, is too computationally expen-sive for one single input sentence.
For a sentenceof length n, calculating the redundancy measurefor all chunks in it would require issuing O(n2)translation queries.
We propose an approximationthat instead calculates the difference of translationprobability caused by discarding es,t, based on ananalysis on the alignment structure between e andf?.
We show the measure boils down to sum-ming the expected number of aligned words foreach ei(s ?
i ?
t), and possibly weighting thesenumbers by ei?s unigram probability.
This methodrequires one translation query, and O(n2) queriesinto a language model, which is much more suit-able for practical applications.
Our method alsosheds light on the role of alignment structures inthe redundancy detection context.3.1 Alignments ApproximationOne key insight in our approximation is that thealignment structure a between es,t?and f?wouldbe largely similar with the alignment structure be-tween e and f?.
We illustrate this notion in Fig-ure 3.
Note that after deleting two words ?resultsfrom?
from the source sentence in Figure 3, thealignment structure remains unchanged elsewhere.Also, ???
?, the word once connected with ?re-sults?, can now be seen as connected to blanks.We hence approximate log Pr(f?|es,t?)
byreusing the alignment structure between eand f?.
To make the alignment structurescompatible, we start with redefining es,t?ase1, e2, .
.
.
, es?1,, .
.
.
,, et+1, .
.
.
, ele, wherethe deleted words are left blank.Let Pr(a|f, e) be the posterior distribution ofalignment structure between sentence pair (f, e).Approximation 1 We formalize the similaritybetween the alignment structures by assuming theKL-divergence between their alignment distribu-tions to be small.DKL(a|f?, e; a|f?, es,t?)
?
0This allows using Pr(a|f?, e) to help approximatelog Pr(f?|es,t?
):log Pr(f?|es,t?
)= log?aPr(a|f?, e)Pr(f?, a|es,t?
)Pr(a|f?, e)=?aPr(a|f?, e) logPr(f?, a|es,t?
)Pr(a|f?, e)+?aPr(a|f?, e) log(Pr(f?|es,t?
)/Pr(f?, a|es,t?
)Pr(a|f?, e))?
??
?DKL(a|f?,e;a|f?,es,t?)?0?
?aPr(a|f?, e) log Pr(f?|es,t?, a) + C2(e)We then use an SMT model to calculatelog Pr(f?|es,t?, a), the translation probability undera given alignment structure.3.2 The Translation ModelApproximation 2 We will use IBMModel 1 (Brown et al., 1993) to calculatelog Pr(f?|es,t?, a)IBM Model 1 is one of the earliest statisti-cal translation models.
It helps us to compute686log Pr(f?|es,t?, a) by making explicit how eachword contributes to words it aligns with.
In partic-ular, to compute the probability that f is a transla-tion of e, Pr(f |e), IBM Model 1 defined a gener-ative alignment model where every word fiin f isaligned with exactly one word eaiin e, so that fiand eaiare word level translations of each other.
?aPr(a|f?, e) log Pr(f?|es,t?, a)=?aPr(a|f?, e)?1?i?lf?log Pr(fi?|es,t?ai)=?aPr(a|f?, e)?1?i?lf?logPr(fi?|es,t?ai)Pr(fi?|eai)+ C3(e)Note thatlogPr(fi?|es,t?ai)Pr(fi?|eai)={0 , for ai/?
{s .
.
.
t}logPr(fi?|)Pr(fi?|eai), otherwise?aPr(a|f?, e)?1?i?lf?logPr(fi?|es,t?ai)Pr(fi?|eai)=?aPr(a|f?, e)?1?i?lf?
?s?j?tIai=jlogPr(fi?|)Pr(fi?|ej)=?s?j?t?1?i?lf?Pr(ai= j|f?, e)?
??
?Ai,jlogPr(fi?|)Pr(fi?|ej)?
??
?DIFF(es,t?,e)Here Ai,j= Pr(ai= j | f?, e), which is theprobability of the i-th word in the translation beingaligned to the j-th word in the original sentence.3.3 Per-word ContributionThrough deductions,R(s, t; e) = LM(es,t?)
+ DIFF(es,t?, e)+C1(e) + C2(e) + C3(e)the redundancy measure boils down to how wedefine Pr(fi?|j), which is: when we discard ej,how do we generate the word it aligns fi?with inits translation.
This value reflects ej?s contributionin generating fi?.We approximate Pr(fi?|j) in two ways.1.
Suppose that all words in the translationare of equal importance.
We assumelogPr(fi?|)Pr(fi?|ej)= ?Cc, where Ccis a constantnumber.
A larger Ccvalue indicates a higherimportance of ejduring the translation.DIFF(es,t?, e) = ?Cc?s?j?t?1?i?lf?Ai,j= ?Cc?s?j?tA(j) (2)Here A(j) is the expected number of align-ments to ej.
This metric demonstrates the in-tuition that words aligned to more words inthe translation are less redundant.2.
We note that rare words are often more im-portant, and therefore harder to be generated.We assume Pr(fi?|) = Pr(ej|) Pr(fi?|ej).DIFF(es,t?, e)=?s?j?t?1?i?lf?Ai,jlogPr(ej|) Pr(fi?|ej)Pr(fi?|ej)=?s?j?tA(j) log Pr(ej|) (3)This gives us counts on how likely eachword is aligned with Chinese words ac-cording to Pr(a|f?, e), where each word isweighted by its importance log Pr(ej|).
Weuse ej?s unigram probability to approximatelog Pr(ej|).When estimating the alignment probabilitiesAi,j, we smooth the alignment result from Googletranslation using Dirichlet-smoothing, where weset ?
= 0.1 empirically based on experiments inthe development dataset.4 Experimental SetupA fully automated redundancy detector has to de-cide (1) whether a given sentence contains any re-dundancy errors; (2) how many words constitutethe redundant part; and (3) which exact words areredundant.
In this paper, we focus on the third partwhile assuming the first two are given.
Thus, ourexperimental task is: given a sentence known tocontain a redundant phrase of a particular length,can that redundant phrase be accurately identified?For most sentences in our study, this results inchoosing one from around 20 words/phrases.While the task has a somewhat limited scope, itallows us to see how we could formally measurethe difference between redundant words/phrases687and non-redundant ones.
For each measure, weobserve whether it has assigned the highest scoreto the redundant part of the sentence.
We comparethe proposed redundancy model described in Sec-tion 3 against a set of baselines and other potentialredundancy measures (to be described shortly).To better understand different measures?
perfor-mance on function words vs. content words, wealso calculate the percentage of redundant func-tion/content words that are detected successfully ?accuracy in both categories.
In our experiments,we consider prepositions and determiners as func-tion words; and we consider other words/phrasesas content words/phrases.4.1 Redundancy MeasuresTo gain insight into redundancy error detection?sdifficulty, we first consider a random baseline.random The random baseline assigns a randomscore to each word/phrase.
The resulting systemwill pick one word/phrase of the given length atrandom.We consider relying on large scale languagemodels to decide redundancy.trigram We use a trigram language model tocapture fluency, by calculating the log-likelihoodof the whole sentence after discarding the givenword/phrase.
A higher probability indicates ahigher fluency.round-trip Inspired by Madnani et al.
(2012;Hermet and D?esilets (2009), an MT system mayeliminate grammar errors with the help of largescale language models.
In this method, we analyzewhich parts are considered redundant by an MTsystem by comparing the original sentence withits round-trip translation.
We use Google trans-late to first translate one sentence into a pivot lan-guage, and then back to English.
We measure onephrase?s redundancy by the number of words thatdisappeared after the round-trip.
We determine ifone word disappeared in two ways:extract word match: one word is considereddisappeared if the same word does not occurin the round-trip.aligned word: we use the Berkeleyaligner (DeNero and Klein, 2007) toalign original sentences with their round-triptranslations.
Unaligned words are consideredto have disappeared.We consider measures for words/phrases?
con-tributions to sentence meaning.sig-score This measure accounts for whetherone word wiis capturing the gist of a sen-tence (Clarke and Lapata, 2007)2.
It was shown tohelp decide whether one part should be discardedduring sentence compression.I(wi) = ?lN?
filogFaFifiand Fiare the frequencies of wiin the currentdocument and a large corpus respectively; Faisthe number of all word occurrences in the corpus;l is the number of clause constituents above wi;N is the deepest level of clause embeddings.
Thismeasure assigns low scores to document specificwords occurring at deep syntax levels.align # We use the number of alignments that aword/phrase has in the translation to measure itsredundancy, as deducted in Equation 2.contrib We compute the word/phrase?s contri-bution to meaning, according to Equation 3.We consider the combinations of measures.trigram + Ccalign # We use a linear combina-tion between language model and align # (Equa-tion 2).
We tune Ccon development data.trigram+contrib This measure (as we proposedin Section 3) is the sum of the trigram lan-guage model component and the contrib compo-nent which represents the phrase?s contribution tomeaning.trigram+?
round-trip/sig-score We combinelanguage model with round-trip and sig-scorelinearly (McDonald, 2006; Clarke and Lapata,2007).
To obtain baselines that are as strong aspossible, we tune the weight ?
on evaluation datafor best accuracy.4.2 Pivot LanguagesOur proposed model uses machine translation out-puts from different pivot languages.
To see whichlanguage helps measuring redundancy, we com-pare 52 pivot languages available at Google trans-late3for meaning representation4.2We extend this measure, which was only defined for con-tent words in Clarke and Lapata (2007), to include all Englishwords.3http://translate.google.com4These languages include Albanian (sq), Arabic (ar),Azerbaijani (az), Irish (ga), Estonian (et), Basque (eu),688length count percentage1 356 67.55%2 80 15.18%3 40 7.59%4 18 3.42%other 33 6.26%Table 1: Length distribution of redundant chunks?lengths in the evaluation data.4.3 Data and ToolsWe extract instances from the NUCLE cor-pus (Dahlmeier and Ng, 2011), an error annotatedcorpus mainly written by Singaporean students,to conduct this study.
The corpus is composedof 1,414 student essays on various topics.
An-notations in NUCLE include error locations, er-ror types, and suggested corrections.
Redundancyerrors are marked by annotators as Rloc.
In thisstudy, we only consider the cases where the sug-gested correction is to delete the redundant part(97.09% among all Rloc errors).To construct our evaluation dataset, wepick sentences with exactly one redundantword/phrase.
This is the most common case(81.18%) among sentences containing redundantwords/phrases.
We use 10% of the essays (336sentences) for development purposes, and an-other 200 essays as the evaluation corpus (527sentences).
A distribution of redundant chunks?lengths in evaluation corpus is shown in Table 1.We train a trigram language model using theSRILM toolkit (Stolcke, 2002) on the AgenceFrance-Presse (afp) portion of the English Giga-words corpus.5 ExperimentsThe experiment aims to address the followingquestions: (1) Does a sentence?s translation serveas a reasonable approximation for its meaning?
(2)Byelorussian (be), Bulgarian (bg), Icelandic (is), Polish (pl),Persian (fa), Boolean (language ((Afrikaans) (af), Danish(da), German (de), Russian (ru), French (fr), Tagalog (tl),Finnish (fi), Khmer (km), Georgian (ka), Gujarati (gu),Haitian (Creole (ht), Korean (ko), Dutch (nl), Galician (gl),Catalan (ca), Czech (cs), Kannada (kn), Croatian (hr), Latin(la), Latvian (lv), Lao (lo), Lithuanian (lt), Romanian (ro),Maltese (mt), Malay (ms), Macedonian (mk), Bengali (bn),Norwegian (no), Portuguese (pt), Japanese (ja), Swedish (sv),Serbian (sr), Esperanto (eo), Slovak (sk), Slovenian (sl),Swahili (sw), Telugu (te), Tamil (ta), Thai (th), Turkish (tr),Welsh (cy), Urdu (ur), Ukrainian (uk), Hebrew (iw), Greek(el), Spanish (es), Hungarian (hu), Armenian (hy), Italian (it),Yiddish (yi), Hindi (hi), Indonesian (id), English (en), Viet-namese (vi), Simplified Chinese (zh-CN), Traditional Chi-nese (zh-TW).Metrics overallfunctionwordscontentwordsrandom 4.44% 4.62% 4.36%trigram 8.06% 3.95% 9.73%sig-score 10.71% 22.16% 6.07%round-trip (aligned word) 10.69% 12.72% 9.87%round-trip (exact wordmatch)5.75% 4.27% 6.35%trigram + ?
round-trip(aligned word)14.80% 11.84% 16.00%trigram + ?
round-trip(exact word match)9.49% 4.61% 11.47%trigram + ?
sig-score 11.01% 22.68% 6.28%align # 5.04% 3.36% 5.72%trigram + Cc?align # 9.58% 4.61% 11.60%contrib 8.59% 20.23% 3.87%trigram + contrib 21.63% 38.16% 14.93%Table 2: Redundancy part identification accuraciesfor different redundancy metrics on NUCLE cor-pus, using French as the pivot language.If so, does the choice of the pivot language matter?
(3) How do the potentially conflicting goals of pre-serving fluency versus preserving meaning impactthe definition of a redundancy measure?Our experimental results are presented in Fig-ure 4 and Table 2.
In Figure 4 we compare usingdifferent pivot languages in our proposed model;in Table 2 we compare using different redundancymetrics for the same pivot language ?
French.Figure 4: Using different pivot languages for re-dundancy measurement.First, compared to other measures, our proposedmodel best captures redundancy.
In particular, ourmodel picks the correct redundant chunk 21.63%of the time, which is five times higher than the ran-dom baseline.
This suggests that using translationto approximate sentence meanings is a plausibleoption.
Note that one partial reason for the lowfigures is the limitation of data resources.
Duringerror analysis, we found linkers/connectors (e.g.moreover, however) and modal auxiliaries (e.g.689can, had) are often marked redundant when theyactually carry undesirable meanings (Ex6, Ex5).These cases comprise a 16% portion among ourmodel?s failures.
Despite this limitation, the evalu-ation still suggests that current approaches are notready for a full redundancy detection pipeline.Second, we find that the choice of pivot lan-guage does make a difference.
Experimental resultsuggests that the system tends to achieve higherredundancy detection accuracy when using trans-lations of a language more similar to English.
Inparticular, when using European languages (e.g.German (de), French (fr), Hungarian (hu) etc.)
aspivot, the system performs much better than usingAsian languages (e.g.
Chinese (zh-CN), Japanese(ja), Thai (th) etc.).
One reason for this phe-nomenon is that the default Google translation out-put in Asian languages (as well as the alignmentbetween English and these languages) are orga-nized into characters, while characters are not theminimum meaning component.
For example, inChinese, ????
is the translation of ?explana-tion?, but the two characters ???
and ???
mean?to solve?
and ?to release?
respectively.
In thealignment output, this will cause certain words be-ing associated with more or less alignments thanothers.
In this case, the number of alignments nolonger directly reflect how many meaning unitsa certain word helps to express.
To confirm thisphenomenon, we tried improving the system usingSimplified Chinese as the pivot language by merg-ing characters together.
In particular, we appliedChinese tokenization (Chang et al., 2008), andthen merged alignments accordingly.
This raisedthe system?s accuracy from 17.74% to 20.11%.Third, to better understand the salient featuresof a successful redundancy measure, we experi-mented with using different components in isola-tion.
We find that the language model componentis better at detecting redundant content words,while the alignment analysis component is betterat detecting redundant function words.
The lan-guage model detects the function word redundan-cies with a worse accuracy than the random base-line; the alignment analysis component also has aworse accuracy than the random baseline on con-tent words.
However, the English language modeland the alignment analysis result can build on topof each other when we analyze the redundancies.We also found that alignments help us to betteraccount for each word?s contribution to the ?mean-ing?
of the sentence.
A linear combination of alanguage model score and our proposed measurebased on analysis of alignments best captures re-dundancy.
However, as our experimental resultssuggest, it is necessary both to use alignments intranslation outputs, and to use them in a goodway.
Alignments help isolating fluency from themeaning component ?
making them easy to inte-grate.
As our experiments demonstrated, althoughmethods comparing Google round-trip transla-tion?s output with the original sentence could leadto a 10.69% prediction accuracy, it is harder tocombine it with the English language model.
Thisis partly because of the non-orthogonality of thesetwo measures ?
the English language model hasalready been used in the round-trip translation re-sult.
Also, an information theoretical interpreta-tion of alignments is essential for the model?s suc-cess.
For example, a more naive way of usingalignment results, align #, which counts the num-ber of alignments, leads to a much lower accuracy.6 ConclusionsDespite the prevalence of redundant phrases inESL writings, there has not been much work in theautomatic detection of these problems.
We con-duct a first study on developing a computationalmodel of redundancies.
We propose to account forwords/phrases redundancies by comparing an ESLsentence with outputs from off-the-shelf machinetranslation systems.
We propose a redundancymeasure based on this comparison.
We show thatby interpreting the translation outputs with IBMModels, redundancies can be measured by a lin-ear combination of a language model score andthe words?
contribution to the sentence?s mean-ing.
This measure accounts for both the fluencyand completeness of a sentence after removing onechunk.
The proposed measure outperforms the di-rect round-trip translation and a random baselineby a large margin.AcknowledgementsThis work is supported by U.S. National ScienceFoundation Grant IIS-0745914.
We thank theanonymous reviewers for their suggestions; wealso thank Joel Tetreault, Janyce Wiebe, WencanLuo, Fan Zhang, Lingjia Deng, Jiahe Qian, NitinMadnani and Yafei Wei for helpful discussions.690ReferencesChris Brockett, William B. Dolan, and Michael Ga-mon.
2006.
Correcting ESL errors using phrasalsmt techniques.
In Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and the 44th annual meeting of the Associationfor Computational Linguistics, ACL-44, pages 249?256, Sydney, Australia.
Association for Computa-tional Linguistics.P.F.
Brown, V.J.D.
Pietra, S.A.D.
Pietra, and R.L.
Mer-cer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computationallinguistics, 19(2):263?311.M.
Carpuat and D. Wu.
2007.
Improving statis-tical machine translation using word sense disam-biguation.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 61?72.Pi-Chuan Chang, Michel Galley, and Christopher DManning.
2008.
Optimizing chinese word segmen-tation for machine translation performance.
In Pro-ceedings of the Third Workshop on Statistical Ma-chine Translation, pages 224?232.
Association forComputational Linguistics.James Clarke and Mirella Lapata.
2007.
Modellingcompression with discourse constraints.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 1?11.Will Coster and David Kauchak.
2011.
Learning tosimplify sentences using wikipedia.
In Proceedingsof the Workshop on Monolingual Text-To-Text Gen-eration, pages 1?9, Portland, Oregon, June.
Associ-ation for Computational Linguistics.Daniel Dahlmeier and Hwee Tou Ng.
2011.
Grammat-ical error correction with alternating structure opti-mization.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1,HLT ?11, pages 915?923, Portland, Oregon, USA.Association for Computational Linguistics.John DeNero and Dan Klein.
2007.
Tailoring wordalignments to syntactic machine translation.
In Pro-ceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 17?24,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Matthieu Hermet and Alain D?esilets.
2009.
Using firstand second language models to correct prepositionerrors in second language authoring.
In Proceedingsof the Fourth Workshop on Innovative Use of NLPfor Building Educational Applications, pages 64?72.Association for Computational Linguistics.Hongyan Jing.
2000.
Sentence reduction for auto-matic text summarization.
In Proceedings of thesixth conference on Applied natural language pro-cessing, pages 310?315.
Association for Computa-tional Linguistics.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization - step one: Sentence compres-sion.
In Proceedings of the Seventeenth NationalConference on Artificial Intelligence and TwelfthConference on Innovative Applications of ArtificialIntelligence, pages 703?710.
AAAI Press.C.
Leacock, M. Chodorow, M. Gamon, and J. Tetreault.2010.
Automated grammatical error detection forlanguage learners.
Synthesis lectures on human lan-guage technologies, 3(1):1?134.Xiaohua Liu, Bo Han, Kuan Li, Stephan HyeonjunStiller, and Ming Zhou.
2010.
SRL-based verb se-lection for ESL.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?10, pages 1068?1076, Cam-bridge, Massachusetts.
Association for Computa-tional Linguistics.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012.
Re-examining machine translation metricsfor paraphrase identification.
In Proceedings ofthe 2012 Conference of the North American Chap-ter of the Association for Computational Linguis-tics: Human Language Technologies, pages 182?190, Montr?eal, Canada, June.
Association for Com-putational Linguistics.Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proceed-ings of the 11th Conference of the European Chap-ter of the Association for Computational Linguistics,volume 6, pages 297?304.
Association for Compu-tational Linguistics.Alla Rozovskaya and Dan Roth.
2010.
Generatingconfusion sets for context-sensitive error correction.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 961?970, Cambridge, Massachusetts.
As-sociation for Computational Linguistics.Andreas Stolcke.
2002.
Srilm-an extensible languagemodeling toolkit.
In Proceedings of the interna-tional conference on spoken language processing,volume 2, pages 901?904.William Strunk.
1918.
The elements of style / byWilliam Strunk, Jr.Joel Tetreault, Jennifer Foster, and Martin Chodorow.2010.
Using parse features for preposition selec-tion and error detection.
In Proceedings of theACL 2010 Conference Short Papers, ACLShort ?10,pages 353?358, Uppsala, Sweden.
Association forComputational Linguistics.691
