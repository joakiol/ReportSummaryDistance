First Joint Conference on Lexical and Computational Semantics (*SEM), pages 170?179,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsModelling selectional preferences in a lexical hierarchyDiarmuid O?
Se?aghdhaComputer LaboratoryUniversity of CambridgeCambridge, UKdo242@cam.ac.ukAnna KorhonenComputer LaboratoryUniversity of CambridgeCambridge, UKAnna.Korhonen@cl.cam.ac.ukAbstractThis paper describes Bayesian selectionalpreference models that incorporate knowledgefrom a lexical hierarchy such as WordNet.
In-spired by previous work on modelling withWordNet, these approaches are based either on?cutting?
the hierarchy at an appropriate levelof generalisation or on a ?walking?
model thatselects a path from the root to a leaf.
Inan evaluation comparing against human plau-sibility judgements, we show that the mod-els presented here outperform previously pro-posed comparable WordNet-based models, arecompetitive with state-of-the-art selectionalpreference models and are particularly well-suited to estimating plausibility for items thatwere not seen in training.1 IntroductionThe concept of selectional preference captures theintuitive fact that predicates in language have a bet-ter semantic ?fit?
for certain arguments than oth-ers.
For example, the direct object argument slotof the verb eat is more plausibly filled by a typeof food (I ate a pizza) than by a type of vehicle (Iate a car), while the subject slot of the verb laughis more plausibly filled by a person than by a veg-etable.
Human language users?
knowledge aboutselectional preferences has been implicated in anal-yses of metaphor processing (Wilks, 1978) and inpsycholinguistic studies of comprehension (Rayneret al, 2004).
In Natural Language Processing, au-tomatically acquired preference models have beenshown to aid a number of tasks, including semanticrole labelling (Zapirain et al, 2009), parsing (Zhouet al, 2011) and lexical disambiguation (Thater etal., 2010; O?
Se?aghdha and Korhonen, 2011).It is tempting to assume that with a large enoughcorpus, preference learning reduces to a simple lan-guage modelling task that can be solved by countingpredicate-argument co-occurrences.
Indeed, Kellerand Lapata (2003) show that relatively good perfor-mance at plausibility estimation can be attained bysubmitting queries to a Web search engine.
How-ever, there are many scenarios where this approachis insufficient: for languages and language domainswhere Web-scale data is unavailable, for predicatetypes (e.g., inference rules or semantic roles) thatcannot be retrieved by keyword search and for ap-plications where accurate models of rarer words arerequired.
O?
Se?aghdha (2010) shows that the Web-based approach is reliably outperformed by morecomplex models trained on smaller corpora for lessfrequent predicate-argument combinations.
Modelsthat induce a level of semantic representation, suchas probabilistic latent variable models, have a furtheradvantage in that they can provide rich structured in-formation for downstream tasks such as lexical dis-ambiguation (O?
Se?aghdha and Korhonen, 2011) andsemantic relation mining (Yao et al, 2011).Recent research has investigated the potentialof Bayesian probabilistic models such as LatentDirichlet Allocation (LDA) for modelling selec-tional preferences (O?
Se?aghdha, 2010; Ritter et al,2010; Reisinger and Mooney, 2011).
These mod-els are flexible and robust, yielding superior perfor-mance compared to previous approaches.
In thispaper we present a preliminary study of analogous170models that make use of a lexical hierarchy (in ourcase the WordNet hierarchy).
We describe two broadclasses of probabilistic models over WordNet andhow they can be implemented in a Bayesian frame-work.
The two main potential advantages of in-corporating WordNet information are: (a) improvedpredictions about rare and out-of-vocabulary argu-ments; (b) the ability to perform syntactic wordsense disambiguation with a principled probabilisticmodel and without the need for an additional stepthat heuristically maps latent variables onto Word-Net senses.
Focussing here on (a), we demon-strate that our models attain better performance thanpreviously-proposed WordNet-based methods on aplausibility estimation task and are particularly well-suited to estimating plausibility for arguments thatwere not seen in training and for which LDA cannotmake useful predictions.2 Background and Related WorkThe WordNet lexical hierarchy (Fellbaum, 1998)is one of the most-used resources in NLP, findingmany applications in both the definition of tasks (e.g.the SENSEVAL/SemEval word sense disambigua-tion tasks) and in the construction of systems.
Theidea of using WordNet to define selectional prefer-ences was first implemented by Resnik (1993), whoproposed a measure of associational strength be-tween a semantic class s and a predicate p corre-sponding to a relation type r:A(s, p, r) =1?P (s|p, r) log2P (s|p, r)P (s|r)(1)where ?
is a normalisation term.
This measure cap-tures the degree to which the probability of seeings given the predicate p differs from the prior proba-bility of s. Given that we are often interested in thepreference of p for a word w rather than a class andwords generally map onto multiple classes, Resniksuggests calculating A(s, p, r) for all classes thatcould potentially be expressed by w and predictingthe maximal value.Cut-based models assume that modelling the se-lectional preference of a predicate involves findingthe right ?level of generalisation?
in the WordNethierarchy.
For example, the direct object slot ofthe verb eat can be associated with the subhierarchyrooted at the synset food#n#1, as all hyponyms ofthat synset are assumed to be edible and the imme-diate hypernym of the synset, substance#n#1, is toogeneral given that many substances are rarely eaten.1This leads to the notion of ?cutting?
the hierarchy atone or more positions (Li and Abe, 1998).
The mod-elling task then becomes that of finding the cuts thatare maximally general without overgeneralising.
Liand Abe (1998) propose a model in which the appro-priate cut c is selected according to the MinimumDescription Length principle; this principle explic-itly accounts for the trade-off between generalisa-tion and accuracy by minimising a sum of model de-scription length and data description length.
Theprobability of a predicate p taking as its argumentan synset s is modelled as:Pla(s|p, r) = P (s|cs,p,r)P (c|p) (2)where cs,p,r is the portion of the cut learned for pthat dominates s. The distribution P (s|cs,p,r) is heldto be uniform over all synsets dominated by cs,p,r,while P (c|p) is given by a maximum likelihood es-timate.Clark and Weir (2002) present a model that, whilenot explicitly described as cut-based, likewise seeksto find the right level of generalisation for an obser-vation.
In this case, the hypernym at which to ?cut?is chosen by a chi-squared test: if the aggregate pref-erence of p for classes in the subhierarchy rooted at cdiffers significantly from the individual preferencesof p for the immediate children of c, the hierarchy iscut below c. The probability of p taking a synset sas its argument is given by:Pcw(s|p, r) =P (p|cs,p,r, r)P (s|r)P (p|r)?s?
?S P (p|cs?,p,r, r)P (s?|r)P (p|r)(3)where cs,p,r is the root node of the subhierarchy con-taining s that was selected for p.An alternative approach to modelling with Word-Net uses its hierarchical structure to define a Markovmodel with transitions from senses to senses andfrom senses to words.
The intuition here is that eachobservation is generated by a ?walk?
from the rootof the hierarchy to a leaf node and emitting the word1In this paper we use WordNet version 3.0, except wherestated otherwise.171corresponding to the leaf.
Abney and Light (1999)proposed such a model for selectional preferences,trained via EM, but failed to achieve competitiveperformance on a pseudodisambiguation task.The models described above have subsequentlybeen used in many different studies.
For exam-ple: McCarthy and Carroll (2003) use Li and Abe?smethod in a word sense disambiguation setting;Schulte im Walde et al (2008) use their MDL ap-proach as part of a system for syntactic and seman-tic subcategorisation frame learning; Shutova (2010)deploys Resnik?s method for metaphor interpreta-tion.
Brockmann and Lapata (2003) report a com-parative evaluation in which the methods of Resnikand Clark and Weir outpeform Li and Abe?s methodon a plausibility estimation task.Much recent work on preference learning has fo-cused on purely distributional methods that do notuse a predefined hierarchy but learn to make general-isations about predicates and arguments from corpusobservations alone.
These methods can be vector-based (Erk et al, 2010; Thater et al, 2010), dis-criminative (Bergsma et al, 2008) or probabilistic(O?
Se?aghdha, 2010; Ritter et al, 2010; Reisingerand Mooney, 2011).
In the probabilistic category,Bayesian models based on the ?topic modelling?framework (Blei et al, 2003b) have been shown toachieve state-of-the-art performance in a number ofevaluation settings; the models considered in this pa-per are also related to this framework.In machine learning, researchers have proposeda variety of topic modelling methods where the la-tent variables are arranged in a hierarchical structure(Blei et al, 2003a; Mimno et al, 2007).
In con-trast to the present work, these models use a rel-atively shallow hierarchy (e.g., 3 levels) and anyhierarchy node can in principle emit any vocabu-lary item; they thus provide a poor match for ourgoal of modelling over WordNet.
Boyd-Graber etal.
(2007) describe a topic model that is directly in-fluenced by Abney and Light?s Markov model ap-proach; this model (LDAWN) is described further inSection 3.3 below.
Reisinger and Pas?ca (2009) in-vestigate Bayesian methods for attaching attributesharvested from the Web at an appropriate level inthe WordNet hierarchy; this task is related in spiritto the preference learning task.3 Probabilistic modelling over WordNet3.1 NotationWe assume that we have a lexical hierarchy in theform of a directed acyclic graph G = (S,E) whereeach node (or synset) s ?
S is associated with aset of words Wn belonging to a large vocabulary V .Each edge e ?
E leads from a node n to its children(or hyponyms) Chn.
As G is a DAG, a node mayhave more than one parent but there are no cycles.The ultimate goal is to learn a distribution over theargument vocabulary V for each predicate p in a setP , through observing predicate-argument pairs.
Apredicate is understood to correspond to a pairing ofa lexical item v and a relation type r, for examplep = (eat, direct object).
The list of observationsfor a predicate p is denoted by Observations(p).3.2 Cut-based modelsModel 1 Generative story for WN-CUTfor cut c ?
{1 .
.
.
|C|} do?c ?Multinomial(?c)end forfor predicate p ?
{1 .
.
.
|P |} do?p ?
Dirichlet(?
)for argument instance i ?
Observations(p)doci ?Multinomial(?p)wi ?Multinomial(?ci)end forend forThe first model we consider, WN-CUT, is directlyinspired by Li and Abe?s model (2).
Each predicatep is associated with a distribution over ?cuts?, i.e.,complete subgraphs of G rooted at a single nodeand containing all nodes dominated by the root.
Itfollows that the number of possible cuts is the sameas the number of synsets.
Each cut c is associatedwith a non-uniform distribution over the set of wordsWc that can be generated by the synsets containedin c. As well as the use of a non-uniform emis-sion distribution and the placing of Dirichlet priorson the multinomial over cuts, a significant differ-ence from Li and Abe?s model is that overlappingcuts are permitted (indeed, every cut has non-zeroprobability given a predicate).
For example, the172model may learn that the direct object slot of eatgives high probability to the cut rooted at food#n#1but also that the cut rooted at substance#n#1 hasnon-negligible probability, higher than that assignedto phenomenon#n#1.
It follows that the estimatedprobability of p taking argument w takes into ac-count all possible cuts, weighted by their probabilitygiven p.The generative story for WN-CUT is given in Al-gorithm 1; this describes the assumptions made bythe model about how a corpus of observations is gen-erated.
The probability of predicate p taking argu-ment w is defined as (4); an empirical posterior esti-mate of this quantity can be computed from a Gibbssampling state via (5):P (w|p) =?cP (c|p)P (w|c) (4)?
?cfcp + ?f?p + |C|?fwc + ?f?c + |Wc|?
(5)where fcw is the number of observations contain-ing argument w that have been assigned cut c, fcpis the number of observations containing predicatep that have been assigned cut c and fc?, f?p are themarginal counts for cut c and predicate p, respec-tively.
The two terms that are multiplied in (4) playcomplementary roles analogous to those of the twodescription lengths in Li and Abe?s MDL formula-tion; P (c|p) will prefer to reuse more general cuts,while P (w|c) will prefer more specific cuts with asmaller associated argument vocabulary.As the number of words |Wc| that can be emittedby a cut |c| varies according to the size of the sub-hierarchy under the cut, the proportion of probabilitymass accorded to the likelihood and the prior in (5)is not constant.
An alternative formulation is to keepthe distribution of mass between likelihood and priorconstant but vary the value of the individual ?c pa-rameters according to cut size.
Experiments suggestthat this alternative does not differ in performance.The second cut-based model, WN-CUT-TOPICS,extends WN-CUT by adding two extra layers of la-tent variables.
Firstly, the choice of cut is condi-tional on a ?topic?
variable z rather than directlyconditioned on the predicate; when the topic vocab-ulary Z is much smaller than the cut vocabulary C,this has the effect of clustering the cuts.
Secondly,Model 2 Generative story for WN-CUT-TOPICSfor topic z ?
{1 .
.
.
|Z|} do?z ?
Dirichlet(?
)end forfor cut c ?
{1 .
.
.
|C|} do?c ?
Dirichlet(?c)end forfor synset s ?
{1 .
.
.
|S|} do?s ?
Dirichlet(?s)end forfor predicate p ?
{1 .
.
.
|P |} do?p ?
Dirichlet(?
)for argument instance i ?
Observations(p)dozi ?Multinomial(?p)ci ?Multinomial(?z)si ?Multinomial(?c)wi ?Multinomial(?s)end forend forinstead of immediately drawing a word once a cuthas been chosen, the model first draws a synset sand then draws a word from the vocabularyWs asso-ciated with that synset.
This has two advantages; itdirectly disambiguates each observation to a specificsynset rather than to a region of the hierarchy and itshould also improve plausibility predictions for raresynonyms of common arguments.
The generativestory for WN-CUT-TOPICS is given in Algorithm 2;the distribution over arguments for p is given in (6)and the corresponding posterior estimate in (7):P (w|p) =?zP (z|p)?cP (c|z)?sP (s|c)P (w|s)(6)?
?zfzp + ?zf?p +?z?
?z?
?cfcz + ?f?z + |C|??
?sfsc + ?f?c + |Sc|?fws + ?f?s + |Ws|?
(7)As before, fzp, fcz , fsc and fws are the re-spective co-occurrence counts of topics/predicates,cuts/topics, synsets/cuts and words/synsets in thesampling state and f?p, f?z , f?c and f?s are the cor-responding marginal counts.173Since WN-CUT and WN-CUT-TOPICS are con-structed from multinomials with Dirichlet priors,it is relatively straightforward to train them bycollapsed Gibbs sampling (Griffiths and Steyvers,2004), an iterative method whereby each latent vari-able in the model is stochastically updated accord-ing to the distribution given by conditioning on thecurrent latent variable assignments of all other to-kens.
In the case of WN-CUT, this amounts to up-dating the cut assignment ci for each token in turn.For WN-CUT-TOPICS there are three variables toupdate; ci and si must be updated simultaneously,but zi can be updated independently for the bene-fit of efficiency.
Although WordNet contains 82,115noun synsets, updates for ci and si can be computedvery efficiently, as there are typically few possiblesynsets for a given word type and few possible cutsfor a given synset (the maximum synset depth is 19).The hyperparameters for the various Dirichlet pri-ors are also reestimated in the course of learning; thevalues of these hyperparameters control the degreeof sparsity preferred by the model.
The ?top-level?hyperparameters ?
in WN-CUT and ?
in WN-CUT-TOPICS are estimated using a fixed-point iterationproposed by Wallach (2008); the other hyperparam-eters are learned by slice sampling (Neal, 2003).3.3 Walk-based modelsAbney and Light (1999) proposed an approach toselectional preference learning in which argumentsare generated for predicates by following a path?
= (l1, .
.
.
, l|?|) from the root of the hierarchy to aleaf node and emitting the corresponding word.
Thepath is chosen according to a Markov model withtransition probabilities specific to each predicate.
Inthis model, each leaf node is associated with a sin-gle word; the synsets associated with that word arethe immediate parent nodes of the leaf.
Abney andLight found that their model did not match the per-formance of Resnik?s (1993) simpler method.
Wehave had a similar lack of success with a Bayesianversion of this model, which we do not describe fur-ther here.Boyd-Graber et al (2007) describe a related topicmodel, LDAWN, for word sense disambiguationthat adds an intermediate layer of latent variablesZ on which the Markov model parameters are con-ditioned.
In their application, each document in aModel 3 Generative story for LDAWNfor topic z ?
{1 .
.
.
|Z|} dofor synset s ?
{1 .
.
.
|S|} doDraw transition probabilities ?z,s ?Dirichlet(?
?s)end forend forfor predicate p ?
{1 .
.
.
|P |} do?p ?
Dirichlet(?
)for argument instance i ?
Observations(p)dozi ?Multinomial(?p)Create a path starting at the root synset ?0:while not at a leaf node do?t+1 ?Multinomial(?zi,?t)end whileEmit the word at the leaf as wiend forend forcorpus is associated with a distribution over topicsand each topic is associated with a distribution overpaths.
The clustering effect of the topic layer allowsthe documents to ?share?
information and hence al-leviate problems due to sparsity.
By analogy to Ab-ney and Light, it is a short and intuitive step to ap-ply LDAWN to selectional preference learning.
Thegenerative story for LDAWN is given in Algorithm3; the probability model for P (w|p) is defined by (8)and the posterior estimate is (9):P (w|p) =?zP (z|p)??1[??
w]P (?|z) (8)?
?zfzp + ?zf?p +?z?
?z???1[??
w]?|?|?1?i=1fz,li?li+1 + ??li?li+1fz,li??
+ ?
(9)where 1[?
?
w] = 1 when the path ?
leads to leafnode w and has value 0 otherwise.
Following Boyd-Graber et al the Dirichlet priors on the transitionprobabilities are parameterised by the product of astrength parameter ?
and a distribution ?s, the latterbeing fixed according to relative corpus frequenciesto ?guide?
the model towards more fruitful paths.Gibbs sampling updates for LDAWN are given inBoyd-Graber et al (2007).
As before, we reestimate174SEEN:staff morale 0.4889team morale 0.5945issue morale 0.0595UNSEEN:pupil morale 0.4318minute morale -0.0352snow morale -0.2748Table 1: Extract from the noun-noun section of Keller andLapata?s (2003) dataset, with human plausibility scoresthe hyperparameters during learning; ?
is estimatedby Wallach?s fixed-point iteration and ?
is estimatedby slice sampling.4 Experiments4.1 Experimental procedureWe evaluate our methods by comparing their predic-tions to human judgements of predicate-argumentplausibility.
This is a standard approach to se-lectional preference evaluation (Keller and Lapata,2003; Brockmann and Lapata, 2003; O?
Se?aghdha,2010) and arguably yields a better appraisal of amodel?s intrinsic semantic quality than other eval-uations such as pseudo-disambiguation or held-outlikelihood prediction.2 We use a set of plau-sibility judgements collected by Keller and Lap-ata (2003).
This dataset comprises 180 predicate-argument combinations for each of three syntacticrelations: verb-object, noun-noun modification andadjective-noun modification.
The data for each re-lation is divided into a ?seen?
portion containing90 combinations that were observed in the BritishNational Corpus and an ?unseen?
portion contain-ing 90 combinations that do not appear (thoughthe predicates and arguments do appear separately).Plausibility judgements were elicited from a largegroup of human subjects, then normalised and log-transformed.
Table 1 gives a representative illus-tration of the data.
Following the evaluation in O?Se?aghdha (2010), with which we wish to compare,we use Pearson r and Spearman ?
correlation coef-ficients as performance measures.All models were trained on the 90-million word2For a related argument in the context of topic model evalu-ation, see Chang et al (2009).written component of the British National Cor-pus,3 lemmatised, POS-tagged and parsed with theRASP toolkit (Briscoe et al, 2006).
We removedpredicates occurring with just one argument typeand all tokens containing non-alphabetic characters.The resulting datasets consist of 3,587,172 verb-object observations (7,954 predicate types, 80,107argument types), 3,732,470 noun-noun observations(68,303 predicate types, 105,425 argument types)and 3,843,346 adjective-noun observations (29,975predicate types, 62,595 argument types).All the Bayesian models were trained by Gibbssampling, as outlined above.
For each model we runthree sampling chains for 1,000 iterations and aver-age the plausibility predictions for each to produce afinal prediction P (w|p) for each predicate-argumentitem.
As the evaluation demands an estimate of thejoint probability P (w, p) we multiply the predictedP (w|p) by a predicate probability P (p|r) estimatedfrom relative corpus frequencies.
In training we usea burn-in period of 200 iterations, after which hyper-parameters are reestimated and P (p|r) predictionsare sampled every 50 iterations.
All probability es-timates are log-transformed to match the gold stan-dard judgements.In order to compare against previously proposedselectional preference approaches based on Word-Net we also reimplemented the methods that per-formed best in the evaluation of Brockmann andLapata (2003): Resnik (1993) and Clark and Weir(2002).
For Resnik?s model we used WordNet 2.1rather than WordNet 3.0 as the former has multi-ple roots, a property that turns out to be necessaryfor good performance.
Clark and Weir?s methodrequires that the user specify a significance thresh-old ?
to be used in deciding where to cut; to giveit the best possible chance we tested with a rangeof values (0.05, 0.3, 0.6, 0.9) and report results forthe best-performing setting, which consistently was?
= 0.9.
One can also use different statistical hy-pothesis tests; again we choose the test giving thebest results, which was Pearson?s chi-squared test.As this method produces a probability estimate con-ditioned on the predicate p we multiply by a MLEestimate of P (p|r) and log-transform the result.3http://www.natcorp.ox.ac.uk/175eat food#n#1, aliment#n#1, entity#n#1, solid#n#1, food#n#2drink fluid#n#1, liquid#n#1, entity#n#1, alcohol#n#1, beverage#n#1appoint individual#n#1, entity#n#1, chief#n#1, being#n#2, expert#n#1publish abstract entity#n#1, piece of writing#n#1, communication#n#2, publication#n#1Table 2: Most probable cuts learned by WN-CUT for the object argument of selected verbsVerb-object Noun-noun Adjective-nounSeen Unseen Seen Unseen Seen Unseenr ?
r ?
r ?
r ?
r ?
r ?WN-CUT .593 .582 .514 .571 .550 .584 .564 .590 .561 .618 .453 .439WN-CUT-100 .500 .529 .575 .630 .619 .639 .662 .706 .537 .510 .464 .431WN-CUT-200 .538 .546 .557 .608 .595 .632 .639 .669 .585 .587 .435 .431LDAWN-100 .497 .538 .558 .594 .605 .619 .635 .633 .549 .545 .459 .462LDAWN-200 .546 .562 .508 .548 .610 .654 .526 .568 .578 .583 .453 .450Resnik .384 .473 .469 .470 .242 .187 .152 .037 .309 .388 .311 .280Clark/Weir .489 .546 .312 .365 .441 .521 .543 .576 .440 .476 .271 .242BNC (MLE) .620 .614 .196 .222 .544 .604 .114 .125 .543 .622 .135 .102LDA .504 .541 .558 .603 .615 .641 .636 .666 .594 .558 .468 .459Table 3: Results (Pearson r and Spearman ?
correlations) on Keller and Lapata?s (2003) plausibility data; underliningdenotes the best-performing WordNet-based model, boldface denotes the overall best performance4.2 ResultsTable 2 demonstrates the top cuts learned by theWN-CUT model from the verb-object training datafor a selection of verbs.
Table 3 gives quanti-tative results for the WordNet-based models un-der consideration, as well as results reported by O?Se?aghdha (2010) for a purely distributional LDAmodel with 100 topics and a Maximum LikelihoodEstimate model learned from the BNC.
In general,the Bayesian WordNet-based models outperform themodels of Resnik and Clark and Weir, and are com-petitive with the state-of-the-art LDA results.
Totest the statistical significance of performance differ-ences we use the test proposed by Meng et al (1992)for comparing correlated correlations, i.e., correla-tion scores with a shared gold standard.
The dif-ferences between Bayesian WordNet models are notsignificant (p > 0.05, two-tailed) for any dataset orevaluation measure.
However, all Bayesian mod-els improve significantly over Resnik?s and Clarkand Weir?s models for multiple conditions.
Perhapssurprisingly, the relatively simple WN-CUT modelscores the greatest number of significant improve-ments over both Resnik (7 out of 12 conditions)and Clark and Weir (8 out of 12), though the otherBayesian models do follow close behind.
This maysuggest that the incorporation of WordNet structureinto the model in itself provides much of the cluster-ing benefit provided by an additional layer of ?topic?latent variables.4In order to test the ability of the WordNet-basedmodels to make predictions about arguments thatare absent from the training vocabulary, we createdan artificial out-of-vocabulary dataset by removingeach of the Keller and Lapata argument words fromthe input corpus and retraining.
An LDA selectionalpreference model will completely fail here, but wehope that the WordNet models can still make rela-tively accurate predictions by leveraging the addi-tional lexical knowledge provided by the hierarchy.For example, if one knows that a tomatillo is classedas a vegetable in WordNet, one can predict a rel-atively high probability that it can be eaten, eventhough the word tomatillo does not appear in theBNC.As a baseline we use a BNC-trained model that4An alternative hypothesis is that samplers for the morecomplex models take longer to ?mix?.
We have run some exper-iments with 5,000 iterations but did not observe an improvementin performance.176Verb-object Noun-noun Adjective-nounSeen Unseen Seen Unseen Seen Unseenr ?
r ?
r ?
r ?
r ?
r ?WN-CUT .334 .326 .518 .569 .252 .212 .254 .274 .451 .397 .471 .458WN-CUT-100 .308 .357 .459 .489 .223 .207 .126 .074 .285 .264 .234 .226WN-CUT-200 .273 .321 .452 .482 .192 .174 .115 .053 .266 .212 .220 .214LDAWN-100 .223 .235 .410 .391 .259 .220 .132 .138 .016 .037 .264 .254LDAWN-200 .291 .285 .392 .379 .240 .163 .118 .131 .041 .078 .209 .212Resnik .203 .341 .472 .497 .054 -.054 .184 .089 .353 .393 .333 .365Clark/Weir .222 .287 .201 .235 .225 .162 .279 .304 .313 .202 .190 .148BNC .206 .224 .276 .240 .256 .240 .223 .225 .088 .103 .220 .231Table 4: Forced-OOV results (Pearson r and Spearman ?
correlations) on Keller and Lapata?s (2003) plausibility datapredicts P (w, p) proportional to the MLE predicateprobability P (p); a distributional LDA model willmake essentially the same prediction.
Clark andWeir?s method does not have full coverage; if nosense s of an argument appears in the data thenP (s|p) is zero for all senses and the resulting pre-diction is zero, which cannot be log-transformed.To sidestep this issue, unseen senses are assigned apseudofrequency of 0.1.
Results for this ?forced-OOV?
task are presented in Table 4.
WN-CUTproves the most adept at generalising to unseen ar-guments, attaining the best performance on 7 of 12dataset/evaluation conditions and a statistically sig-nificant improvement over the baseline on 6.
We ob-serve that estimating the plausibility of unseen ar-guments for noun-noun modifiers is particularly dif-ficult.
One obvious explanation is that the trainingdata for this relation has fewer tokens per predi-cate, making it more difficult to learn their prefer-ences.
A second, more hypothetical, explanation isthat the ontological structure of WordNet is a rela-tively poor fit for the preferences of nominal modi-fiers; it is well-known that almost any pair of nounscan combine to produce a minimally plausible noun-noun compound (Downing, 1977) and it may be thatthis behaviour is ill-suited by the assumption thatpreferences are sparse distributions over regions ofWordNet.5 ConclusionIn this paper we have presented a range ofBayesian selectional preference models that incor-porate knowledge about the structure of a lexical hi-erarchy.
One motivation for this work was to testthe hypothesis that such knowledge can be helpfulin constructing robust models that can handle rareand unseen arguments.
To this end we have re-ported a plausibility-based evaluation in which ourmodels outperform previously proposed WordNet-based preference models and make sensible predic-tions for out-of-vocabulary items.
A second motiva-tion, which we intend to explore in future work, isto apply our models in the context of a word sensedisambiguation task.
Previous studies have demon-strated the effectiveness of distributional Bayesianselectional preference models for predicting lexicalsubstitutes (O?
Se?aghdha and Korhonen, 2011) butthese models lack a principled way to map a wordonto its most likely WordNet sense.
The methodspresented in this paper offer a promising solution tothis issue.
Another potential research direction is in-tegration of semantic relation extraction algorithmswith WordNet or other lexical resources, along thelines of Pennacchiotti and Pantel (2006) and VanDurme et al (2009).AcknowledgementsThe work in this paper was funded by the EP-SRC (UK) grant EP/G051070/1, EU grant 7FP-ITC-248064 and the Royal Society, (UK).ReferencesSteven Abney and Marc Light.
1999.
Hiding a semantichierarchy in a Markov model.
In Proceedings of theACL-99 Workshop on Unsupervised Learning in NLP,College Park, MD.177Shane Bergsma, Dekang Lin, and Randy Goebel.
2008.Discriminative learning of selectional preferencesfrom unlabeled text.
In Proceedings of EMNLP-08,Honolulu, HI.David M. Blei, Thomas L. Griffiths, Michael I. Jordan,and Joshua B. Tenenbaum.
2003a.
Hierarchical topicmodels and the nested Chinese Restaurant Process.
InProceedings of NIPS-03, Vancouver, BC.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003b.
Latent Dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Jordan Boyd-Graber, David Blei, and Xiaojin Zhu.
2007.A topic model for word sense disambiguation.
In Pro-ceedings of EMNLP-CoNLL-07, Prague, Czech Re-public.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proceed-ings of the ACL-06 Interactive Presentation Sessions,Sydney, Australia.Carsten Brockmann and Mirella Lapata.
2003.
Evalu-ating and combining approaches to selectional pref-erence acquisition.
In Proceedings of EACL-03, Bu-dapest, Hungary.Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,Chong Wang, and David M. Blei.
2009.
Reading tealeaves: How humans interpret topic models.
In Pro-ceedings of NIPS-09, Vancouver, BC.Stephen Clark and David Weir.
2002.
Class-based prob-ability estimation using a semantic hierarchy.
Compu-tational Linguistics, 28(2), 187?206.Pamela Downing.
1977.
On the creation and use of En-glish compound nouns.
Language, 53(4):810?842.Katrin Erk, Sebastian Pado?, and Ulrike Pado?.
2010.
Aflexible, corpus-driven model of regular and inverseselectional preferences.
Computational Linguistics,36(4):723?763.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge, MA.Thomas L. Griffiths and Mark Steyvers.
2004.
Find-ing scientific topics.
Proceedings of the NationalAcademy of Sciences, 101(suppl.
1):5228?5235.Frank Keller and Mirella Lapata.
2003.
Using the Web toobtain frequencies for unseen bigrams.
ComputationalLinguistics, 29(3):459?484.Hang Li and Naoki Abe.
1998.
Generalizing case framesusing a thesaurus and the MDL principle.
Computa-tional Linguistics, 24(2):217?244.Diana McCarthy and John Carroll.
2003.
Disambiguat-ing nouns, verbs and adjectives using automaticallyacquired selectional preferences.
Computational Lin-guistics, 29(4):639?654.Xiao-Li Meng, Robert Rosenthal, and Donald B. Rubin.1992.
Comparing correlated correlation coefficients.Psychological Bulletin, 111(1):172?175.David Mimno, Wei Li, and Andrew McCallum.
2007.Mixtures of hierarchical topics with Pachinko alloca-tion.
In Proceedings of ICML-07, Corvallis, OR.Radford M. Neal.
2003.
Slice sampling.
Annals ofStatistics, 31(3):705?767.Diarmuid O?
Se?aghdha and Anna Korhonen.
2011.
Prob-abilistic models of similarity in syntactic context.
InProceedings of EMNLP-11, Edinburgh, UK.Diarmuid O?
Se?aghdha.
2010.
Latent variable modelsof selectional preference.
In Proceedings of ACL-10,Uppsala, Sweden.Marco Pennacchiotti and Patrick Pantel.
2006.
Ontolo-gizing semantic relations.
In Proceedings of COLING-ACL-06, Sydney, Australia.Keith Rayner, Tessa Warren, Barbara J. Juhasz, and Si-mon P. Liversedge.
2004.
The effect of plausibil-ity on eye movements in reading.
Journal of Experi-mental Psychology: Learning Memory and Cognition,30(6):1290?1301.Joseph Reisinger and Raymond Mooney.
2011.
Cross-cutting models of lexical semantics.
In Proceedings ofEMNLP-11, Edinburgh, UK.Joseph Reisinger and Marius Pas?ca.
2009.
Latent vari-able models of concept-attribute attachment.
In Pro-ceedings of ACL-IJCNLP-09, Suntec, Singapore.Philip Resnik.
1993.
Selection and Information:A Class-Based Approach to Lexical Relationships.Ph.D.
thesis, University of Pennsylvania.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A la-tent Dirichlet alocation method for selectional prefer-ences.
In Proceedings ACL-10, Uppsala, Sweden.Sabine Schulte im Walde, Christian Hying, ChristianScheible, and Helmut Schmid.
2008.
Combining EMtraining and the MDL principle for an automatic verbclassification incorporating selectional preferences.
InProceedings of ACL-08:HLT, Columbus, OH.Ekaterina Shutova.
2010.
Automatic metaphor inter-pretation as a paraphrasing task.
In Proceedings ofNAACL-HLT-10, Los Angeles, CA.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations usingsyntactically enriched vector models.
In Proceedingsof ACL-10, Uppsala, Sweden.Benjamin Van Durme, Philip Michalak, and Lenhart K.Schubert.
2009.
Deriving generalized knowledgefrom corpora using WordNet abstraction.
In Proceed-ings of EACL-09, Athens, Greece.Hanna Wallach.
2008.
Structured Topic Models for Lan-guage.
Ph.D. thesis, University of Cambridge.Yorick Wilks.
1978.
Making preferences more active.Artificial Intelligence, 11:197?225.Limin Yao, Aria Haghighi, Sebastian Riedel, and AndrewMcCallum.
2011.
Structured relation discovery using178generative models.
In Proceedings of EMNLP-11, Ed-inburgh, UK.Ben?at Zapirain, Eneko Agirre, and Llu?
?s Ma`rquez.
2009.Generalizing over lexical features: Selectional prefer-ences for semantic role classification.
In Proceedingsof ACL-IJCNLP-09, Singapore.Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011.Exploiting web-derived selectional preference to im-prove statistical dependency parsing.
In Proceedingsof ACL-11, Portland, OR.179
