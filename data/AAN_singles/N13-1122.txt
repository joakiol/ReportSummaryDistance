Proceedings of NAACL-HLT 2013, pages 1020?1030,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsTo Link or Not to Link?
A Study on End-to-EndTweet Entity LinkingStephen GuoStanford Universitysdguo@cs.stanford.eduMing-Wei Chang Emre K?c?manMicrosoft Research{minchang, emrek}@microsoft.comAbstractInformation extraction from microblog postsis an important task, as today microblogs cap-ture an unprecedented amount of informationand provide a view into the pulse of the world.As the core component of information extrac-tion, we consider the task of Twitter entitylinking in this paper.In the current entity linking literature, mentiondetection and entity disambiguation are fre-quently cast as equally important but distinctproblems.
However, in our task, we find thatmention detection is often the performancebottleneck.
The reason is that messages onmicro-blogs are short, noisy and informal textswith little context, and often contain phraseswith ambiguous meanings.To rigorously address the Twitter entity link-ing problem, we propose a structural SVMalgorithm for entity linking that jointly op-timizes mention detection and entity disam-biguation as a single end-to-end task.
By com-bining structural learning and a variety of first-order, second-order, and context-sensitive fea-tures, our system is able to outperform exist-ing state-of-the art entity linking systems by15% F1.1 IntroductionMicroblogging services, such as Twitter and Face-book, are today capturing the largest volume everrecorded of fine-grained discussions spanning ahuge breadth of topics, from the mundane to the his-toric.
The micro-blogging service Twitter reportsthat it alone captures over 340M short messages,or tweets, per day.1 From such micro-blogging ser-vices?
data streams, researchers have reported min-ing insights about a variety of domains, from elec-tion results (Tumasjan et al 2010) and democracymovements (Starbird and Palen, 2012) to health is-sues and disease spreading (Paul and Dredze, 2011;Sadilek et al 2012), as well as tracking prod-uct feedback and sentiment (Asur and Huberman,2010).A critical step in mining information from amicro-blogging service, such as Twitter, is the iden-tification of entities in tweets.
In order to minethe relationship between drugs, symptoms and side-effects, or track the popularity of politicians or sen-timent about social issues, we must first be able toidentify the topics and specific entities being dis-cussed.
The challenge is that messages on micro-blogs are short, noisy, and informal texts with littlecontext, and often contain phrases with ambiguousmeanings.
For example, ?one day?
may be either aset phrase or a reference to a movie.
Given suchdifficulties, current mining and analysis of micro-blogs lists limits its application to certain domainswith easy-to-recognize, unambiguous entities in or-der to avoid noise in the extraction results.We begin this paper with a thorough investigationof mention detection and entity disambiguation forsocial media, focused on the Twitter micro-bloggingservice.
Mention detection is the task of extractionsurface form candidates that can link to an entity inthe domain of interest.
Entity disambiguation is thetask of linking an extracted mention to a specific def-inition or instance of an entity in a knowledge base.1http://blog.twitter.com/2012/03/twitter-turns-six.html1020While mention detection and entity disambigua-tion are frequently cast as equally important but dis-tinct and separate problems, we find that mentiondetection is where today?s systems and our base-line techniques incur the most failures.
Detectingthe correct entity mention is a significant challengegiven mis-capitalizations, incorrect grammar, andambiguous phrases.
In (Ritter et al 2011), the au-thors report their system achieves 0.64 to 0.67 F1 onnamed entity segmentation results with 34K tokensof labeled examples.
On the other hand, once thecorrect entity mention is detected, a trivial disam-biguation that maps to the most popular entity2 willachieve 85% accuracy in our set.Our primary contribution in this paper is a re-casting and merging of the tasks of mention detec-tion and entity disambiguation into a single end-to-end entity linking task.
We achieve significantimprovements by applying structural learning tech-niques to jointly optimize the detection and disam-biguation of entities.
Treating detection and disam-biguation as a single task also enables us to apply alarge set of new features, conventionally used onlyfor disambiguation, to the initial detection of men-tions.
These features, derived from external knowl-edge bases, include entity popularity and inter-entityrelations from external knowledge bases, and are notwell utilized in current mention detection systems.For example, consider the following partial tweet:(1) The town is so, so good.
And don?tworry Ben, we already forgave youfor Gigli.
Really.Determining whether or not ?The town?
is a mentionof a location or other specific entity based solely onlexical and syntactic features is challenging.
Know-ing ?The Town?
is the name of a recent movie helps,and we can we be more confident if we know thatBen Affleck is an actor in the movie, and Gigli isanother of his movies.To train and evaluate our system, we created threeseparate annotated data sets of approximately 500tweets each.
These data sets are hand annotatedwith entity links to Wikipedia.
We evaluate our sys-tem by comparing its performance at detecting en-2What we mean here is ?the most linked entity?.
See Sec-tion 3 for details.tities to the performance of two state-of-the-art en-tity linking systems, Cucerzan (Cucerzan, 2007) andTagMe (Ferragina and Scaiella, 2010), and find thatour system outperforms them significantly by 15%in absolute F1.The rest of this paper describes related work, ourstructured learning approach to entity linking, andour experimental results.2 Related WorkBuilding an entity linking system requires solvingtwo interrelated sub-problems: mention detectionand entity disambiguation.
The significant portionof recent work in the literature (Ratinov et al 2011;Davis et al 2012; Sil et al 2012; Demartini et al2012; Wang et al 2012; Han and Sun, 2011; Hanet al 2011) focuses solely upon the entity linkingproblem.
The entity linking systems of these studiesassume that entity mentions are provided by a sepa-rate mention detection system.
In contrast, our studyjointly identifies and disambiguates entity mentionswithin tweets (short text fragments).A subset of existing literature targets end-to-endlinking (Cucerzan, 2007; Milne and Witten, 2008;Kulkarni et al 2009; Ferragina and Scaiella, 2010;Han and Sun, 2011; Meij et al 2012), but thereare quite a few differences between our work andeach of these systems.
Some systems (Milne andWitten, 2008; Kulkarni et al 2009; Han and Sun,2011) heavily depend on Wikipedia text and mightnot work well in short and noisy tweets.
Many sys-tems (Mihalcea and Csomai, 2007; Cucerzan, 2007;Milne and Witten, 2008; Ferragina and Scaiella,2010) treat mention detection and entity disam-biguation as two different problems.
(Meij et al2012) is the most related to our paper.
While theirsystem also considers mention detection and entitydisambiguation together, they do not consider entity-to-entity relationships and do not incorporate con-textual words from tweets.An area of work closely related to the mentiondetection problem is the Named Entity Recogni-tion (NER) problem, the identification of textualphrases which belong to core categories (Person,Location, Organization).
It is well-known that NERsystems trained on well-written documents performvery poorly on short, noisy text, such as tweets (Rit-1021ter et al 2011).
There have been a few recent stud-ies proposing Twitter-specific NER systems (Li etal., 2012; Ritter et al 2011).3 PreliminariesFor performing entity linking on Twitter, we chooseWikipedia as our external knowledge base of enti-ties.Entity We define an entity as a nonambiguous, ter-minal page (e.g., The Town (the film)) in Wikipedia(i.e., a Wikipedia page that is not a category, dis-ambiguation, list, or redirect page).
We define ananchor phrase (surface form) as the textual phrase(e.g., the town) which can potentially link to someentities.
We define an entity mention as an anchorphrase and the context (?the town?
in the exam-ple tweet in Section 1), where its semantic meaningumambiguously represents a specific entity.
Notethat an entity may be represented by multiple sur-face forms.Wikipedia Lexicon Construction Following theassumptions used in most prior entity linking re-search, we assume that surface forms of entities canbe found as anchor phrases in Wikipedia.
In or-der to construct a Wikipedia lexicon, we first collectall anchors phases in Wikipedia.
For each anchorphrase (surface form) s, we construct a lexicon en-try by gathering the set of entities {e1, e2, .
.
.
eK}that can be linked from s. We also collect the num-ber of times anchor a links to the entity ei, d(s, ei).We define P (ei|s) = d(s, ei)/d(s), where d(s) rep-resents the number of times s appears in Wikipedia.We refer e?
as the most linked entity for anchor s ife?
= arg maxe P (ei|s).Candidate Generation Given a tweet t, we ex-tract all k-grams of size ?
k. For each k-gram,we find all entities where this k-gram is an anchorphrase.
If a k-gram is an anchor phrase for at leastone entity, then the k-gram is a candidate entitymention.
In general, we identify many candidatephrase per tweet; let U(t) = {c1, c2, .
.
.}
denotethe set of candidates in tweet t. We refer to s(c)as the surface form (e.g., the anchor phrase) of c.Compared to the anchor phrase, the candidate alsocarries the context and position information.
LetE(ci) = {e1, e2, .
.
.
,NIL} denote the set of entitieswhich candidate i may be linked to, plus the addi-tional special token NIL.
Note that the size of E(ci)is always at least 2.Task Definition First, our system generates candi-date entity mentions, textual phrases which can pos-sibly be entity mentions.
Our system then performsfiltering and optimization to process the list of can-didates.
For each candidate, our system links thecandidate to a special NIL token or links the candi-date to its corresponding entity in Wikipedia.
Moreformally, given a tweet t and its candidate set U(t),the goal of the system is to predict yi ?
E(ci),?ci ?U(t).Comparison to the TAC KBP Competition It isimportant to state that our definition of the entitylinking problem differs significantly from the entitylinking problem as defined by the TAC KBP com-petition (Ji et al 2010; Ji et al 2011).
In the TAC,there is no true mention detection problem; everycandidate in the TAC is an entity mention that rep-resents an entity.
Another difference is that the TACallows for an entity mention to map to an entity notin the external knowledge base (Wikipedia); our sys-tem does not provide special handling of this case.Comparison to Named Entity RecognitionThere are also important differences between ourtask and the canonical NER task.
For example,NER systems identify common names, such as?Robert,?
as entities.
In our task, we only consider aprediction as a success if the system can determinewhich person in Wikipedia ?Robert?
is referring to.In other words, our definition of entities dependson the given knowledge base, rather than humanjudgment.
Hence, it is difficult to make a fair systemcomparison of our system to NER systems.4 Entity Linking as Structural LearningIn our framework, we use structural learning as atool to capture the relationship between entities.
Wedefine yi as the output for ci, where yi ?
E(ci).
LetT = |U(t)| and y = {y1, y2, .
.
.
, yT }.
The fea-ture function for the whole assignment can be writ-ten as ?
(t, U(t),y).
The score for the assignmenty can be obtained as the linear product between theweight vector w and the feature vector.
For an inputexample, the prediction can be found by solving the1022inference problem:y?
= arg maxywT?
(t, U(t),y) (1)We use a Structural SVM (SSVM) (Taskar etal., 2004; Tsochantaridis et al 2005; Chang et al2010) as our learning algorithm.
To train the weightvector w, we minimize the objective function of theSSVMminw?w?22+ Cl?i=1?2i (2)where l is the number of labeled examples andwT?
(ti, c(ti),yi)??
(yi,y) + wT?
(ti, c(ti),y)?
?i, ?i,yWe denote yi as the gold assignment for xi and de-fine ?
(yi,y) as the Hamming distance between twoassignments yi and y.4.1 FeaturesFeature definitions are very important as they definethe shapes of the structures.
Our feature vector isdefined as?
(t, U(t),y) =?i?
(t, ci, yi)+?i<j?
(t, ci, yi, cj , yj)where ci and cj is the i-th and j-th candidates inU(t), respectively.First, we assign ?
(t, ci,NIL) to be a special biasfeature.
The corresponding weight value behaves asa threshold to cut-off mentions.
Recall in our defini-tion that yi = NIL represents that the candidate ci isnot a mention.The first order features for ?
(t, ci, e) are de-scribed as follows.
In general, we can classify ourfeatures into two types: mention-specific featuresand entity-specific features.
For a given candidateci, mention-specific features only consider the sur-face form of ci and the tweet t. Entity-specific fea-tures also consider the knowledge base content ofthe entity e. Prior work in the entity linking liter-ature has primarily focused on entity-specific fea-tures, as most prior work solves entity disambigua-tion with given mentions.Base and Capitalization Rate Our base featuresare from two resources.
Let s(c) denote the sur-face form of candidate c. The link probabilityPl(s(c)) and P (e|s(c)) features are extracted fromWikipedia.
We explained P (e|s(c)) in Section 3.Link probability Pl(s(c)) is the probability that aphrase is used as an anchor in Wikipedia.
We alsoadd a third feature that captures normalized linkcount.
Besides these three features, we also havea feature to indicate if a is a stop word, and a fea-ture indicating the number of tokens in a.
The viewcount and P (e|s) features are entity-specific, whilethe other three features are mention-specific.For each phrase s(c), we also collect statisticsabout the probability that a phrase is capitalized inWikipedia.
We refer to this feature as the capitaliza-tion rate feature, Pc(s(c)).Popularity Feature We have access to 300GBsof Wikipedia page view counts, representing onemonths worth of page view information, we usethis as popularity data.3 As mentioned in Sec-tion 3, we find that the most often linked Wikipediaarticles might not be the most popular ones onTwitter.
Using page view statistics helps our sys-tem correct this bias.
We define another prob-ability based on page view statistics Pv(ei|c) =v(ei)/(?e?E(c)/{NIL} v(e)), where v(e) representsthe view count for the page e.Context Capitalization Our context capitaliza-tion features indicate if the current candidate, theword before, and the word after the candidate arecapitalized.Entity Type and Tf-idf We use the procedure pro-posed in (Ratinov et al 2011) to extract keywordphrases from categories for each Wikipedia page,and then build a rule-based system using keywordphrases to classify if each entity page belongs to oneof the following entity types: Person, Location, Or-ganization, TV Show, Book/Magazine and Movie.4For a given candidate c and an entity e, the associ-ated binary feature becomes active if the entity be-longs to a specific entity type.
There are six entitytype features in our system.3http://dammit.lt/wikistats4The entity type prediction accuracy of our rule-based sys-tem on the development set is around 95%.1023Features DescriptionsBase Pl(si), P (e|s), normalized link counts, stopword, # tokensCap.
Rate Pc(si)Popularity Pv(e|s), normalized page view count,Pv(e|s)P (e|s)Context Cap.
Three features indicating if the current candi-date and the words before and after are capi-talizedEntity Type Six binary features for each entity typeTf-idf Two features for the similarity between theword vectors of the entity and the tweetSecond-Order Jac(ei, ej), P (ei|si)P (ej |sj), Pc(si)Pc(sj),Pl(si)Pl(sj)Table 1: Summary of the features used in our structurallearning systems.We also include tf-idf features in our system.
Foreach Wikipedia page, we collect the top 100 tf-idfwords.
We add one feature that is the dot productbetween the tf-idf word vector of e and the words oftweet t. We include a second feature that representsthe average tf-idf score of all words that appear inboth e and t.Second-order features We include four very sim-ple second-order features ?
(t, ci, ei, cj , ej) to cap-ture more complex relations between entities andcandidates.
The first feature is the Jaccard distancebetween two Wikipedia pages ei and ej .
Let ?
(ei)denote the set of Wikipedia pages that contain a hy-perlink to ei.
We define the Jaccard distance be-tween ei and ej as:Jac(ei, ej) =|?
(ei) ?
?(ej)||?
(ei) ?
?
(ej)|This feature has a similar effect as the normal-ized Google distance (Cilibrasi and Vitanyi, 2007),which has been used for many entity linking sys-tems.
Let us use the following shorthand: si = s(ci)and sj = s(cj).
We have also included three featuresP (ei|si)P (ej |sj), Pc(si)Pc(sj) and Pl(si)Pl(sj) toincrease the expressivity of our model.4.2 Mining Additional Contextual WordsUnlike mention detection systems used in other NLPtasks, there are no lexical features in our system.Lexical features are important as they can capturesemantic meaning precisely.
However, given thatwe do not have many labeled examples, lexical fea-tures can lead to overfitting.
The diverse languagein tweets also make it more difficult to use lexicalfeatures.Our solution for this problem is to use a very sim-ple method to mine context words for different enti-ties from a large, unlabeled tweet corpus.
The algo-rithm works as follows:1.
Train an end-to-end entity linking system andthen apply it to a large, unlabeled tweet corpus2.
Extract contextual words for each entity typebased on the pseudo-labeled data.3.
Train the entity linking system again with newcontextual features.In this paper, we only use the word before and theword after as our contextual word for a candidate.Note that while there are ambiguous phrases on thesurface (e.g., ?friends?
can be a TV show or justa regular phrase), certain phrases are unambiguous(e.g., ?CSI : Miami?).
As contextual words are oftenshared within the same entity type (e.g.
?watching?is likely to appear before a tv show), those words canpotentially improve our final system.Let wi denote the i-th word in the tweet and tidenote the entity type for the i-th word.5 We use avery simple rule to select a set of left context wordsQ(R) for entity type R.Q(R) = {wi | P (ti+1 = R|wi) > r, d(wi) > z}where d(wi) represent the number of times the wordwi appears in the unlabeled set.
The first rule is tosimply find a word which is more likely to be fol-lowed by an entity.
The second rule filter outs noisywords (e.g., Twitter handles) in the unlabeled set.The right context words are also extracted in a simi-lar way.To train the second end-to-end entity linking sys-tem, we add one additional feature for the contextualwords.
For the feature vector ?
(t, ci, e), the contextfeature is active if the candidate ci is capitalized6 andthe context words around ci belongs to Q(R), givenR is the entity type for the entity e.5The tag ti belongs to the entity type R if our system links acandidate c to an entity with type R and c covers the word wi.6The word ?watching?
can be a TV show while most of thetime it is not.
These common makes this contextual featurenoisy.
We found that the context feature can only be reliablyapplied when the candidate is capitalized.10244.3 Cohesiveness ScoreThere are several ways to consider entity-entity co-hesiveness besides using the second-order featuresdirectly.
In our model, we also consider a modi-fied cohesiveness score proposed in (Ferragina andScaiella, 2010).
The idea behind the cohesivenessscore is to estimate the correlations between differ-ent entities by using weighted Jaccard scores.7There are two rounds in the procedure of com-puting the cohesiveness score.
We first estimate ap-proximately the most probable entity for each candi-date given all the other candidates in the same tweet.In the second round, the cohesiveness score is thenproduced with respect to the most probable entitycomputed in the first round.More formally, in the first round, we compute therelevance score for each candidate and entity pair:Rel(e, c|t) =?c?
6=c?e??E(c?)
P (e?|c?
)Jac(e, e?
)|U(t)|.Then, the cohesiveness score is computed byScoh(e, c|t) =?c?
6=c Jac(e, e?(c?
))P (e?(c?)|c?
)|U(t)|,where the e?(c?)
= arg maxe?E(c?
)Rel(e, c?|t).
Wethen put the cohesiveness score as a feature for each(e, c) pair.
In practice, we found that the cohesive-ness score in the model can significantly increase thedisambiguation ability of the model without usingthe second-order information.4.4 InferenceIn order to train and test the SSVM model, one needsto solve both the inference problem Eq.
(3) and theloss-augmented inference problem.
Without second-order features, the inference and loss-augmented in-ference problems can be easily solved, given thateach component can be solved independently byy?i = arg maxy?E(ci)wT?
(t, ci, y) (3)While the inference problem can be solved inde-pendently, the training algorithm still considers thewhole assignment together in the training procedure.7In our experiments, we only apply the cohesiveness scoretechnique on candidates which pass the filtering procedure.
Seesection 5 for more details for our filtering process.Data #Tweets #Cand #Men.
P@1Train 473 8212 218 85.3%Test 1 500 8950 249 87.7%Test 2 488 7781 332 89.6%Table 2: Labeled example statistics.
?#Cand?
representsthe total number of candidates we found in this dataset.?#Men.?
is the total number of mentions that disam-biguate to an entity.
The top-1 rate (P@1) represents theproportion of the mentions that disambiguate to the mostlinked entity in Wikipedia.With the second-order features, the inferenceproblem becomes NP-hard.
While one can resort tousing integer linear programming to find the optimalsolution, we choose not to do so.
We instead use thebeam search algorithm.
Our beam search algorithmfirst arranges the candidates from left to right, andthen solve the inference problems approximately.5 ExperimentsWe collected unlabeled Twitter data from two re-sources and then asked human annotators to labeleach tweet with a set of entities present.
Our anno-tators ignored the following: duplicate entities pertweet, ambiguous entity mentions, and entities notpresent in Wikipedia.
We next describe the two setsof Twitter data used as our training data and test-ing data.
In addition to these two datasets, we alsorandomly sampled another 200 tweets as our devel-opment set.Ritter We sampled 473 and 500 tweets8 from thedata used in (Ritter et al 2011) to be our trainingdata and test data, respectively.
We did not use anylabels generated by (Ritter et al 2011); our annota-tors completely re-annotated each tweets with its setof entities.
We refer to the first set as Train and thesecond set as Test 1.Entertainment To check if our system has theability to generalize across different domains, wesampled another 488 tweets related to entertain-ment entities.
Our main focus was to extracttweets that contained TV shows, Movies, and8We originally labeled 1000 tweets but then found 27 re-peated tweets in the dataset.
Therefore, we remove those 27tweets in the training set.1025Books/Magazines.
Identifying tweets from a spe-cific domain is a research topic on its own, so wefollowed (Dalvi et al 2012), and used a keywordmatching method.9 After sampling this set of tweets,we asked our annotators to label the data in the sameway as before (all entities are labeled, not just en-tertainment entities).
We refer to this tweet set asTest 2.After sampling, all tweets were then normalizedin the following way.
First, we removed all retweetsymbols (RT) and special symbols, as these are to-kens that may easily confuse NER systems.
Wetreated punctuation as separate tokens.
Hashtags (#)play a very important role in tweets as they oftencarry critical information.
We used the followingweb service10 to break the hashtags into tokens (e.g.,the service will break ?#TheCloneWars?
into ?theclone wars?)
(Wang et al 2011).The statistics of our labeled examples are pre-sented in Table 2.
First, note that the average numberof mentions per tweet is well below 1.
In fact, manytweets are personal conversations and do not carryany entities that can be linked to Wikipedia.
Still,many candidates are generated (such as ?really?)
forthose tweets, given that those candidates can still po-tentially link to an entity (?really?
could be a TVchannel).
Therefore, it is very important to includetweets without entities in the training set because wedo not want our system to create unnecessary linksto entities.Another interesting thing to note is the percent-age of entity mentions that disambiguate directly totheir most often linked entities in Wikipedia.
If wesimply disambiguate each entity mention to its mostlinked entity in Wikipedia, we can already achieve85% to 90% accuracy, if mention detection is per-fectly accurate.
However, mention detection is a dif-ficult problem as only about 3% of candidates arevalid entity mentions.It is worthwhile to mention that, as per (Ferraginaand Scaiella, 2010), for computational efficiency,9We use the following word list :?movie?, ?tv?, ?episode?,?film?, ?actor?, ?actors?, ?actress?, ?director?, ?directors?,?movies?, ?episodes?, ?book?, ?novel?, ?reading?, ?read?,?watch?, ?watching?, ?show?, ?books?, ?novels?, ?movies?,?author?
and ?authors?.10http://web-ngram.research.microsoft.com/info/break.htmlwe apply several preprocessing steps before runningour entity linking system.
First, for each anchor inWikipedia, we gather all entities it can disambiguateto and remove from that anchor?s entity set alenti-ties that are linked less than 2% of the time.
Second,we apply a modified filtering procedure similar tothat proposed in (Ferragina and Scaiella, 2010) tofilter the set of candidates per tweet.Evaluation Our annotated datasets contain enti-ties from many Wikipedia categories.
For eval-uation, we primarily focus on entities belongingto a set of six core categories (Person, Location,Organization, TV Show, Book/Magazine, Movie).We believe it is necessary to focus upon core en-tities, rather than considering all possible entitiesin Wikipedia.
Most common words in the Englishlanguage have their own Wikpedia page, but mostwords are not important enough to be considered en-tities.
In general, there is a large degree of subjectiv-ity when comparing different entity linking datasets;different researchers have their own interpretation ofwhat constitutes an entity.
For example, we exam-ined the annotation used in (Meij et al 2012) andfound it to be extremely lenient, when compared toour own beliefs of what is an entity.
Therefore, webelieve evaluating performance on restricted entitytypes is the only fair way to compare different end-to-end entity linking systems.We evaluate the performance of our system ona per-tweet basis, by comparing the set of anno-tated ?gold?
entities with the set of entities predictedby our system, and computing performance metrics(precision, recall, F1).
We choose to evaluate oursystem on a per-tweet basis, as opposed to a per-entity basis, because we wish to avoid the issue ofmatching segmentations.
For example, it is quitecommon to observe multiple overlapping phrases ina tweet that should be linked to the same entity (e.g.,?President Obama?
and ?Obama?).
When evaluat-ing our system, we compute performance metrics forboth all entities and core entities.11Parameters In our implementation, we fixed theregularization parameter C = 10.
When beam-11To decide if an entity is a core entity or not, we use thefollowing procedure.
For the gold entities, the annotators alsoannotate type of the entity.
We decide the entity type of thepredicted entities using the procedure described in Section 4.1.1026ModelTest 1 Test 2P R F1 P R F1Cucerzan 64.8 42.2 51.1 64.9 39.7 49.5TagMe 38.8 69.0 49.7 34.9 70.3 46.7SSVM 78.8 59.9 68.0 75.0 57.7 65.2Table 3: Comparisons between different end-to-end en-tity linking systems.
We evaluate performance on coreentities, as it is the only fair way to compare differentsystems.search is used, the beam size is set to be 50, andwe only consider the top 10 candidates for each can-didate to speed the inference process.
In the contextword mining algorithm, r = 0.5% and z = 1000.5.1 ResultsIn the following, we analyze the contributions ofeach component in our system and compare our finalsystems to other existing end-to-end entity linkingsystems.System Comparison We compare our final sys-tem to other state-of-the-art systems in Table 3.CUCERZAN represents a modified implementationof the system in (Cucerzan, 2007).
TagMe is an end-to-end linking system that focuses on short texts,including tweets.
Our system significantly outper-forms these two systems in both precision and re-call.
Note that CUCERZAN?s system is a state-of-the-art system on well-written documents with pro-vided entity mentions.
The system (Cucerzan, 2007)has been extended by the authors and won the TACKBP competition in 2010 (Ji et al 2010).There are two possible reasons to explain why oursystem outperforms CUCERZAN.
First, their men-tion detection is a carefully designed system targetedtoward documents, not tweets.
Their system has seg-mentation issues when applied to Twitter, as it reliesheavily upon capitalization when identifying candi-date entity mentions.
Second, their system heav-ily depends on the fact that related entities shouldappear together within documents.
However, giventhat tweets are very short, some of their most impor-tant features are not suitable for the Twitter domain.Our system outperforms TagMe because we use amore sophisticated machine learning approach, ascompared to their system.
TagMe links too manyStructural SVMTest 1 Test 2All Core All CoreBase 35.9 42.9 47.7 52.5+Cap.
Rate 38.4 45.6 49.9 53.7+Popularity 41.3 47.9 50.3 55.1+Context Cap 43.7 52.0 50.7 54.8+Entity Type 47.9 57.0 53.5 59.0+Tfidf 53.2 63.1 56.8 61.9Table 4: Feature Study: F1 for entity linking perfor-mance.
?All?
means evaluation on all annotated entities.?Core?
means evaluation only on our six entity types.Each row contains all additional features of the row aboveit.spurious entity mentions for common words.
This isa result of their algorithm?s over-emphasis on entity-entity co-occurrence features.Feature Study We study the contributions of eachfeature group in our system in Table 4.
We summa-rize our discoveries as follows:First, we find collecting statistics from a large cor-pus helps the system significantly.
In addition toP (e|s), we find that capitalization rate features of-fer around 3% to 4% F1 improvement in Test 1.Similarly, popularity features are also important, asit corrects bias existing in Wikipedia link statistics.Compared to lexical features, using statistical fea-tures offers a great advantage of reducing the needfor large amounts of labeled data.We also find entity related features (Popularity,Entity Type, Tf-idf) are crucial.
Given that between85% to 90% of our mentions should directly disam-biguate to the most often linked entities, one mightthink entity-specific features are not important inour task.
Interestingly, entity-specific features areamong the most important features.
The discoveryconfirms our hypothesis: it is critical to considermention detection and entity disambiguation as asingle problem, rather than as separate problems ina two staged approach used by many other entitylinking systems.
Note that capitalization rate andcontext capitalization features are mention-specific.Additionally, we find that mixing mention-specificfeatures and entity-specific features results in a bet-ter model.1027Entity Type Words appearing beforethe mentionWords appearing after thementionPerson wr, dominating, rip, quar-terback, singer, featuring,defender, rb, minister, ac-tress, twitition, secretarytarde, format, noite, suf-fers, dire, admits, sen-ators, urges, performs,joinsTV Show sbs, assistir, assistindo,otm, watching, nw,watchn, viagra, watchin,verskit, performances,premieres, finale, par-ody, marathon, season,episodes, spoilers, sketchTable 5: An example of context words that are automati-cally extracted from 20 million unlabeled tweets.
For thesake of brevity, we only display context words for twocategories.
Note that there are misspelled words (suchas ?watchn?)
and abbreviations (such as nw) that do notappear in well-written documents.Advance ModelsTest 1 Test 2All Core All CoreSSVM (Table 4) 53.2 63.1 56.8 61.9+Context 53.9 64.6 58.6 63.4+Cohesiveness 55.6 66.5 59.7 65.1+2nd order 58.1 68.0 60.6 65.2Table 6: Evaluation results (F1) of the advanced models.
?+ Context?
is the model that uses additional context fea-tures extracted from 20 millions unlabeled tweets.
?+ Co-hesiveness?
is the model with both additional context andcohesiveness features.
?+2nd order?
is our final model(which incorporates context, cohesiveness, and second-order features).Mining Context Words We verify the effective-ness of adding contextual features that are extractedautomatically from large unlabeled data.
We applyour system (with all first-order features) on a set of20 million unlabeled tweets we collected.
Contextwords are then extracted using the simple rules de-scribed in Section 4.
We list the top 10 words weextracted in Table 5.
Due to space limitations, weonly list the words for the Person and TV Show cat-egories.
The results are interesting as we are ableto find common misspelled words and abbreviationsused in Twitter.
For example, we find that ?watchn?means ?watching?
and ?nw?
means ?now watching,?and they are usually words found before TV shows.We also find tweeters frequently use abbreviationsfor people?s jobs.
For example, ?wr?
means ?widereceiver?
and ?rb?
means ?running back.?
Whenmined context is added into our system, the perfor-mance improves significantly (Table 6).
We notethat extending context mining algorithms in a large-scale, principled approach is an important next re-search topic.Capturing Entity-Entity Relationships In thispaper, we use two methods to capture the relation-ship between entities: adding the cohesiveness scoreand using second order information.
Until now, weonly considered features that can be extracted fromonly one entity.
Past research has shown that consid-ering features that involve multiple entities can im-prove entity linking performance, given that relatedentities are more likely to appear together in a doc-ument.
When these type of features are added, weneed to perform beamsearch, as the exact inferenceprocedure can be prohibitively expensive.As displayed in Table 6, we find that either addingthe cohesiveness score or using second order infor-mation can improve prediction.
Using both methodsimproves the model even more.
Comparing compu-tation overhead, computing cohesiveness is signifi-cantly more cost-effective than using second-orderinformation.6 ConclusionIn this paper, we propose a structural SVM methodto address the problem of end-to-end entity linkingon Twitter.
By considering mention detection andentity disambiguation together, we build a end-to-end entity linking system that outperforms currentstate-of-the-art systems.There are plenty of research problems left to beaddressed.
Developing a better algorithm for min-ing contextual words is an important research topic.It would also be interesting to design a method thatjointly learns NER models and entity linking mod-els.ReferencesS.
Asur and B.A.
Huberman.
2010.
Predicting the futurewith social media.
arXiv preprint arXiv:1003.5699.M.
Chang, V. Srikumar, D. Goldwasser, and D. Roth.2010.
Structured output learning with indirect super-vision.
In Proceedings of the International Conferenceon Machine Learning (ICML).R.L.
Cilibrasi and P.M.B.
Vitanyi.
2007.
The googlesimilarity distance.
Knowledge and Data Engineering,IEEE Transactions on, 19(3):370?383.1028S.
Cucerzan.
2007.
Large-scale named entity disam-biguation based on Wikipedia data.
In Proceedings ofthe 2007 Joint Conference of EMNLP-CoNLL, pages708?716.N.
Dalvi, R. Kumar, and B. Pang.
2012.
Object match-ing in tweets with spatial models.
In Proceedings ofthe fifth ACM international conference on Web searchand data mining, WSDM ?12, pages 43?52, New York,NY, USA.
ACM.A.
Davis, A. Veloso, A. S. da Silva, W. Meira, Jr., andA.
H. F. Laender.
2012.
Named entity disambiguationin streaming data.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 815?824, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.G.
Demartini, D. E. Difallah, and P. Cudre?-Mauroux.2012.
Zencrowd: leveraging probabilistic reasoningand crowdsourcing techniques for large-scale entitylinking.
In The International World Wide Web Con-ference, pages 469?478, New York, NY, USA.
ACM.P.
Ferragina and U. Scaiella.
2010.
Tagme: on-the-flyannotation of short text fragments (by wikipedia enti-ties).
In Proceedings of the 19th ACM internationalconference on Information and knowledge manage-ment, CIKM ?10, pages 1625?1628, New York, NY,USA.
ACM.X.
Han and L. Sun.
2011.
A generative entity-mentionmodel for linking entities with knowledge base.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies - Volume 1, HLT ?11, pages 945?954, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.X.
Han, L. Sun, and J. Zhao.
2011.
Collective entitylinking in web text: a graph-based method.
In Pro-ceedings of the 34th international ACM SIGIR con-ference on Research and development in InformationRetrieval, SIGIR ?11, pages 765?774, New York, NY,USA.
ACM.H.
Ji, R. Grishman, H.T.
Dang, K. Griffitt, and J. Ellis.2010.
Overview of the tac 2010 knowledge base pop-ulation track.
In Proceedings of the TAC 2010 Work-shop.H.
Ji, R. Grishman, and Dang.
2011.
Overview of the tac2011 knowledge base population track.
In Proceed-ings of the TAC 2011 Workshop.S.
Kulkarni, A. Singh, G. Ramakrishnan, andS.
Chakrabarti.
2009.
Collective annotation ofwikipedia entities in web text.
In Proceedings ofthe 15th ACM SIGKDD international conference onKnowledge discovery and data mining, Proceedingsof International Conference on Knowledge Discoveryand Data Mining (KDD), pages 457?466, New York,NY, USA.
ACM.C.
Li, J. Weng, Q.
He, Y. Yao, A. Datta, A.
Sun, and B.-S.Lee.
2012.
Twiner: named entity recognition in tar-geted twitter stream.
In Proceedings of the 35th inter-national ACM SIGIR conference on Research and de-velopment in information retrieval, Proceedings of In-ternational Conference on Research and Developmentin Information Retrieval, SIGIR, pages 721?730, NewYork, NY, USA.
ACM.E.
Meij, W. Weerkamp, and M. de Rijke.
2012.
Addingsemantics to microblog posts.
In Proceedings of thefifth ACM international conference on Web search anddata mining, pages 563?572, New York, NY, USA.ACM.R.
Mihalcea and A. Csomai.
2007.
Wikify!
: linking doc-uments to encyclopedic knowledge.
In Proceedingsof ACM Conference on Information and KnowledgeManagement (CIKM), pages 233?242.
ACM.D.
Milne and I. H. Witten.
2008.
Learning to linkwith wikipedia.
In Proceedings of ACM Conferenceon Information and Knowledge Management (CIKM),pages 509?518, New York, NY, USA.
ACM.M.J.
Paul and M. Dredze.
2011.
You are what you tweet:Analyzing twitter for public health.
In Fifth Interna-tional AAAI Conference on Weblogs and Social Media(ICWSM 2011).L.
Ratinov, D. Roth, D. Downey, and M. Anderson.2011.
Local and global algorithms for disambiguationto wikipedia.
In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics (ACL),pages 1375?1384, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.A.
Ritter, S. Clark, Mausam, and O. Etzioni.
2011.Named entity recognition in tweets: an experimentalstudy.
In Proceedings of the Conference on EmpiricalMethods for Natural Language Processing (EMNLP),pages 1524?1534, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.A.
Sadilek, H. Kautz, and V. Silenzio.
2012.
Model-ing spread of disease from social interactions.
In SixthAAAI International Conference on Weblogs and SocialMedia (ICWSM).A.
Sil, E. Cronin, P. Nie, Y. Yang, A.-M. Popescu,and A. Yates.
2012.
Linking named entities toany database.
In Proceedings of the Conference onEmpirical Methods for Natural Language Processing(EMNLP), pages 116?127, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.K.
Starbird and L. Palen.
2012.
(how) will the revolu-tion be retweeted?
: information diffusion and the 2011egyptian uprising.
In Proceedings of the acm 2012conference on computer supported cooperative work,pages 7?16.
ACM.1029B.
Taskar, C. Guestrin, and D. Koller.
2004.
Max-marginmarkov networks.
In The Conference on Advances inNeural Information Processing Systems (NIPS).I.
Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-tun.
2005.
Large margin methods for structured andinterdependent output variables.
Journal of MachineLearning Research, 6:1453?1484, September.A.
Tumasjan, T.O.
Sprenger, P.G.
Sandner, and I.M.Welpe.
2010.
Predicting elections with twitter: What140 characters reveal about political sentiment.
InProceedings of the fourth international aaai confer-ence on weblogs and social media, pages 178?185.K.
Wang, C. Thrasher, and B.J.P.
Hsu.
2011.
Web scalenlp: a case study on url word breaking.
In The Inter-national World Wide Web Conference, pages 357?366.ACM.C.
Wang, K. Chakrabarti, T. Cheng, and S. Chaudhuri.2012.
Targeted disambiguation of ad-hoc, homoge-neous sets of named entities.
In The InternationalWorld Wide Web Conference, pages 719?728, NewYork, NY, USA.
ACM.1030
