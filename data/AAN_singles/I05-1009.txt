Entropy as an Indicator of Context Boundaries?An Experiment Using a Web Search Engine?Kumiko Tanaka-IshiiGraduate School of Information Science and Technology,University of Tokyokumiko@i.u-tokyo.ac.jpAbstract.
Previous works have suggested that the uncertainty of tokenscoming after a sequence helps determine whether a given position isat a context boundary.
This feature of language has been applied tounsupervised text segmentation and term extraction.
In this paper, wefundamentally verify this feature.
An experiment was performed using aweb search engine, in order to clarify the extent to which this assumptionholds.
The verification was applied to Chinese and Japanese.1 IntroductionThe theme of this paper is the following assumption:The uncertainty of tokens coming after a sequence helps determine whethera given position is at a context boundary.
(A)Intuitively, the variety of successive tokens at each character inside a word mono-tonically decreases according to the offset length, because the longer the preced-ing character n-gram, the longer the preceding context and the more it restrictsthe appearance of possible next tokens.
On the other hand, the uncertainty atthe position of a word border becomes greater and the complexity increases, asthe position is out of context.
This suggests that a word border can be detectedby focusing on the differentials of the uncertainty of branching.
This assumptionis illustrated in Figure 1.
In this paper, we measure this uncertainty of successivetokens by utilizing the entropy of branching (which we mathematically define inthe next section).This assumption dates back to the fundamental work done by Harris [6] in1955, where he says that when the number of different tokens coming after everyprefix of a word marks the maximum value, then the location corresponds to themorpheme boundary.
Recently, with the increasing availability of corpora, thischaracteristic of language data has been applied for unsupervised text segmenta-tion into words and morphemes.
Kempe [8] reports an experiment to detect wordborders in German and English texts by monitoring the entropy of successivecharacters for 4-grams.
Many works in unsupervised segmentation utilise thefact that the branching stays low inside words but increases at a word or mor-pheme border.
Some works apply this fact in terms of frequency [10] [2], whileothers utilise more sophisticated statistical measures: Sun et al [12] use mutualinformation; Creutz [4] use MDL to decompose Finnish texts into morphemes.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
93?105, 2005.c?
Springer-Verlag Berlin Heidelberg 200594 K. Tanaka-IshiiThis assumption seems to hold not only at the character level but also atthe word level.
For example, the uncertainty of words coming after the wordsequence, ?The United States of?, is small (because the word America is verylikely to occur), whereas the uncertainty is greater for the sequence ?computa-tional linguistics?, suggesting that there is a context boundary just after thisterm.
This observation at the word level has been applied to term extractionby utilising the number of different words coming after a word sequence as anindicator of collocation boundaries [5] [9].Fig.
1.
Intuitive illustration of a variety of successive tokens and a word boundaryAs can be seen in these previous works, the above assumption (A) seemsto govern language structure both microscopically at the morpheme level andmacroscopically at the phrase level.
Assumption (A) is interesting not only froman engineering viewpoint but also from a language and cognitive science view-point.
For example, some recent studies report that the statistical, innate struc-ture of language plays an important role in children?s language acquisition [11].Therefore, it is important to understand the innate structure of language, inorder to shed light on how people actually acquire it.Consequently, this paper verifies assumption (A) in a fundamental manner.We address the questions of why and to what extent (A) holds.
Unlike recent,previous works based on limited numbers of corpora, we use a web search engineto obtain statistics, in order to avoid the sparseness problem as much as pos-sible.
Our discussion focuses on correlating the entropy of branching and wordboundaries, because the definition of a word boundary is clearer than that of amorpheme or phrase unit.
In terms of detecting word boundaries, our experi-ments were performed in character sequence, so we chose two languages in whichsegmentation is a crucial problem: Chinese which contains only ideograms, andJapanese, which contains both ideograms and phonograms.
Before describingthe experiments, we discuss assumption (A) in more detail.2 The AssumptionGiven a set of elements ?
and a set of n-gram sequences ?n formed of ?, theconditional entropy of an element occurring after an n-gram sequence Xn isdefined asEntropy as an Indicator of Context Boundaries 95Fig.
2.
Decrease in H(X|Xn) for characters when n is increasedH(X |Xn) = ??xn?
?nP (Xn = xn)?x?
?P (X = x|Xn = xn) log P (X = x|Xn = xn)where P (X = x) indicates the probability of occurrence of x.A well-known observation on language data states that H(X |Xn) decreasesas n increases [3].
For example, Figure 2 shows the entropy values as n increasesfrom 1 to 9 for a character sequence.
The two lines correspond to Japanese andEnglish data, from corpora consisting of the Mainichi newspaper (30 MB) andthe WSJ (30 MB), respectively.
This phenomenon indicates that X will becomeeasier to estimate as the context of Xn gets longer.
This can be intuitivelyunderstood: it is easy to guess that ?e?
will follow after ?Hello!
How ar?, but itis difficult to guess what comes after the short string ?He?.The last term ?
log P (X = x|Xn = xn) in formula above indicates theinformation of a token of x coming after xn, and thus the branching after xn.The latter half of the formula, the local entropy value for a given xnH(X |Xn = xn) = ??x?
?P (X = x|Xn = xn) log P (X = x|Xn = xn), (1)indicates the average information of branching for a specific n-gram sequence xn.As our interest in this paper is this local entropy, we denote simply H(X |Xn =xn) as h(xn) in the rest of this paper.The decrease in H(X |Xn) globally indicates that given an n-length sequencexn and another (n + 1)-length sequence yn+1, the following inequality holds onaverage:h(xn) > h(yn+1).
(2)One reason why inequality (2) holds for language data is that there is context inlanguage, and yn+1 carries a longer context as compared with xn.
Therefore, ifwe suppose that xn is the prefix of xn+1, then it is very likely thath(xn) > h(xn+1) (3)holds, because the longer the preceding n-gram, the longer the same context.
Forexample, it is easier to guess what comes after x6=?natura?
than what comesafter x5 = ?natur?.
Therefore, the decrease in H(X |Xn) can be expressed as the96 K. Tanaka-IshiiFig.
3.
Our model for boundary detection based on the entropy of branchingconcept that if the context is longer, the uncertainty of the branching decreaseson average.
Then, taking the logical contraposition, if the uncertainty does notdecrease, the context is not longer, which can be interpreted as the following:If the complexity of successive tokens increases, the location is at thecontext border.
(B)For example, in the case of x7 = ?natural?, the entropy h(?natural?)
shouldbe larger than h(?natura?
), because it is uncertain what character will allow x7to succeed.
In the next section, we utilise assumption (B) to detect the contextboundary.3 Boundary Detection Using the Entropy of BranchingAssumption (B) gives a hint on how to utilise the branching entropy as anindicator of the context boundary.
When two semantic units, both longer than1, are put together, the entropy would appear as in the first figure of Figure 3.The first semantic unit is from offsets 0 to 4, and the second is from 4 to 8,with each unit formed by elements of ?.
In the figure, one possible transition ofbranching degree is shown, where the plot at k on the horizontal axis denotesthe entropy for h(x0,k) and xn,m denotes the substring between offsets n and m.Ideally, the entropy would take a maximum at 4, because it will decrease ask is increased in the ranges of k < 4 and 4 < k < 8, and at k = 4, it will rise.Therefore, the position at k = 4 is detected as the ?local maximum value?
whenmonitoring h(x0,k) over k. The boundary condition after such observation canbe redefined as the following:Bmax Boundaries are locations where the entropy is locally maximised.A similar method is proposed by Harris [6], where morpheme borders can bedetected by using the local maximum of the number of different tokens comingafter a prefix.This only holds, however, for semantic units longer than 1.
Units often havea length of 1: at the character level, in Japanese and Chinese, there are manyone-character words, and at the word level, there are many single words that donot form collocations.
If a unit has length 1, then the situation will look like thesecond graph in Figure 3, where three semantic units, x0,4, x4,5 x5,8, are present,with the middle unit having length 1.
First, at k = 4, the value of h increases.Entropy as an Indicator of Context Boundaries 97At k = 5, the value may increase or decrease, because the longer context resultsin an uncertainty decrease, though an uncertainty decrease does not necessarilymean a longer context.
When h increases at k = 5, the situation would look likethe second graph.
In this case, the condition Bmaxwill not suffice, and we needa second boundary condition:Bincrease Boundaries are locations where the entropy is increased.On the other hand, when h decreases at k = 5, then even Bincreasecannot beapplied to detect k = 5 as a boundary.
We have other chances to detect k = 5,however, by considering h(xi,k) where 0 < i < k. According to inequality (2),then, a similar trend should be present for plots of h(xi,k), assuming h(x0,n) >h(x0,n+1); then, we haveh(xi,n) > h(xi,n+1), for 0 < i < n. (4)The value h(xi,k) would hopefully rise for some i if the boundary at k = 5 isimportant, although h(xi,k) can increase or decrease at k = 5, just as in the casefor h(x0,n).Therefore, when the target language consists of many one element units,Bincreaseis crucial for collecting all boundaries.
Note that boundaries detectedby Bmaxare included in those detected by the condition Bincrease.Fig.
4.
Kempe?s model for boundary detectionKempe?s detection model is based solely on the assumption that the un-certainty of branching takes a local maximum at a context boundary.
Withoutany grounding on this assumption, Kempe [8] simply calculates the entropy ofbranching for a fixed length of 4-grams.
Therefore, the length of n is set to 3,h(xi?3,i) is calculated for all i, and the maximum values are claimed to indicatethe word boundary.
This model is illustrated in Figure 4, where the plot at eachk indicates the value of h(xk?3,k).
Note that at k = 4, the h value will be highest.It is not possible, however, to judge whether h(xi?3,i) is larger than h(xi?2,i+1)in general: Kempe?s experiments show that the h value simply oscillates at a lowvalue in such cases.In contrast, our model is based on the monotonic decrease in H(X |Xn).
Itexplains the increase in h at the context boundary by considering the entropydecrease with a longer context.98 K. Tanaka-IshiiSummarising what we have examined, in order to verify assumption (A),which is replaced by assumption (B), the following questions must be answeredexperimentally:Q1 Does the condition described by inequality (3) hold?Q2 Does the condition described by inequality (4) hold?Q3 To what extent are boundaries extracted by Bmaxor Bincrease?In the rest of this paper, we demonstrate our experimental verification of thesequestions.So far, we have considered only regular order processing: the branching degreeis calculated for successive elements of xn.
We can also consider the reverse order,which involves calculating h for the previous element of xn.
In the case of theprevious element, the question is whether the head of xn forms the beginning ofa context boundary.
We use the subscripts suc and prev to indicate the regularand reverse orders, respectively.
Thus, the regular order is denoted as hsuc(xn),while the reverse order is denoted by hprev(xn).In the next section, we explain how we measure the statistics of xn, beforeproceeding to analyze our results.4 Measuring Statistics by Using the WebIn the experiments described in this paper, the frequency counts were obtainedusing a search engine.
This was done because the web represents the largest pos-sible database, enabling us to avoid the data sparseness problem to the greatestextent possible.Given a sequence xn, h(xn) is measured by the following procedure.1.
xn is sent to a search engine.2.
One thousand snippets, at maximum, are downloaded and xn is searchedfor through these snippets.
If the number of occurrences is smaller than N ,then the system reports that xn is unmeasurable.3.
The elements occurring before and after xn are counted, and hsuc(xn) andhprev(xn) are calculated.N is a parameter in the experiments described in the following section, anda higher N will give higher precision and lower recall.
Another aspect of theexperiment is that the data sparseness problem quickly becomes significant forlonger strings.
To address these issues, we chose N=30.The value of h is influenced by the indexing strategy used by a given searchengine.
Defining f(x) as the frequency count for string x as reported by thesearch engine,f(xn) > f(xn+1) (5)should usually hold if xn is a prefix of xn+1, because all occurrences of xn containoccurrences of xn+1.
In practice, this does not hold for many search engines,namely, those in which xn+1 is indexed separately from xn and an occurrence ofxn+1 is not included in one of xn.
For example, the frequency count of ?mode?does not include that of ?model?, because it is indexed separately.
In particular,Entropy as an Indicator of Context Boundaries 99Fig.
5.
Entropy changes for a Japanese character sequence (left:regular; right:reverse)search engines use this indexing strategy at the string level for languages inwhich words are separated by spaces, and in our case, we need a search enginein which the count of xn includes that of xn+1.
Although we are interested inthe distribution of tokens coming after the string xn and not directly in thefrequency, a larger value of f(xn) can lead to a larger branching entropy.Among the many available search engines, we decided to use AltaVista, be-cause its indexing strategy seems to follow inequality (5) better than do thestrategies of other search engines.
AltaVista used to utilise string-based index-ing, especially for non-segmented languages.
Indexing strategies are currentlytrade secrets, however, so companies rarely make them available to the pub-lic.
We could only guess at AltaVistafs strategy by experimenting with someconcrete examples based on inequality (5).5 Analysis for Small ExamplesWe will first examine the validity of the previous discussion by analysing somesmall examples.
Here, we utilise Japanese examples, because this language con-tains both phonograms and ideograms, and it can thus demonstrate the featuresof our method for both cases.The two graphs in Figure 5 show the actual transition of h for a Japanesesentence formed of 11 characters: x0,11 = (We think ofthe future of (natural) language processing (studies)).
The vertical axis representsthe entropy value, and the horizontal axis indicates the offset of the string.
Inthe left graph, each line starting at an offset of m+1 indicates the entropy valuesof hsuc(xm,m+n) for n > 0, with plotted points appearing at k = m + n. Forexample, the leftmost solid line starting at offset k = 1 plots the h values of x0,nfor n > 0, with m=0 (refer to the labels on some plots):x0,1 =x0,2 =.
.
.x0,5 =with each value of h for the above sequence x0,n appearing at the location of n.??????????????????
?100 K. Tanaka-IshiiConcerning this line, we may observe that the value increases slightly at po-sition k = 2, which is the boundary of the word (language).
This locationwill become a boundary for both conditions, Bmaxand Bincrease.
Then, at posi-tion k = 3, the value drastically decreases, because the character coming after(language proce) is limited (as an analogy in English, ssing is the majorcandidate that comes after language proce).
The value rises again at x0,4, be-cause the sequence leaves the context of (language processing).
Thislocation will also become a boundary whether Bmaxor Bincreaseis chosen.
Theline stops at n = 5, because the statistics of the strings x0,n for n > 5 wereunmeasurable.The second leftmost line starting from k = 2 shows the transition of theentropy values of hsuc(x1,1+n) for n > 0; that is, for the strings starting fromthe second character , and so forth.
We can observe a trend similar tothat of the first line, except that the value also increases at 5, suggesting thatk = 5 is the boundary, given the condition Bincrease.The left graph thus contains 10 lines.
Most of the lines are locally maximizedor become unmeasurable at the offset of k = 5, which is the end of a large portionof the sentence.
Also, some lines increase at k = 2, 4, 7, and 8, indicating theends of words, which is correct.
Some lines increase at low values at 10: thisis due to the verb (think), whose conjugation stem is detected as aborder.Similarly, the right-hand graph shows the results for the reverse order, whereeach line ending at m ?
1 indicates the plots of the value of hprev(xm?n,m) forn > 0, with the plotted points appearing at position k = m ?
n. For example,the rightmost line plots h for strings ending with (from m = 11 and n = 10down to 5):x10,11 =x9,11 =.
.
.x6,11 =x5,11 =where x4,11 became unmeasurable.
The lines should be analysed from back tofront, where the increase or maximum indicates the beginning of a word.
Overall,the lines ending at 4 or 5 were unmeasurable, and the values rise or take amaximum at k = 2, 4 or 7.Note that the results obtained from the processing in each direction differ.The forward pass detects 2,4,5,7,8, whereas the backward pass detects 2,4,7.The forward pass tends to detect the end of a context, while the backward passtypically detects the beginning of a context.
Also, it must be noted that thisanalysis not only shows the segmenting position but also the structure of thesentence.
For example, a rupture of the lines and a large increase in h are seenat k = 5, indicating the large semantic segmentation position of the sentence.
Inthe right-hand graph, too, we can see two large local maxima at 4 and 7.
Thesesegment the sentence into three different semantic parts.???????????????????????????
?Entropy as an Indicator of Context Boundaries 101Fig.
6.
Other segmentation examplesOn these two graphs, questions Q1 through Q3 from ?3 can be addressed asfollows.
First, as for Q1, the condition indicated by inequality (3) holds in mostcases where all lines decrease at k = 3, 6, 9, which correspond to inside words.There is one counter-example, however, caused by conjugation.
In Japanese con-jugation, a verb has a prefix as the stem, and the suffix varies.
Therefore, withour method, the endpoint of the stem will be regarded as the boundary.
As con-jugation is common in languages based on phonograms, we may guess that thisphenomenon will decrease the performance of boundary detection.As for Q2, we can say that the condition indicated by inequality (4) holds,as the upward and downward trends at the same offset k look similar.
Heretoo, there is a counter-example, in the case of a one element word, as indicatedin ?3.
There are two one-word words x4,5= and x7,8= , where thegradients of the lines differ according to the context length.
In the case of oneof these words, h can rise or fall between two successive boundaries indicatinga beginning and end.
Still, we can see that this is complemented by examininglines starting from other offsets.
For example, at k = 5, some lines end with anincrease.As for Q3, if we pick boundary condition Bmax, by regarding any unmeasur-able case as h = ?
?, and any maximum of any line as denoting the boundary,then the entry string will be segmented into the following:This segmentation result is equivalent to that obtained by many other Japanesesegmentation tools.
Taking Bincreaseas the boundary condition, another bound-ary is detected in the middle of the last verb (think, segmented at(language)j (processing)j (of)j (future)j (of)j (think).?
???
??
??
?
?????
?
?102 K. Tanaka-Ishiithe stem of the verb)?.
If we consider detecting the word boundary, then thissegmentation is incorrect; therefore, to increase the precision, it would be betterto apply a threshold to filter out cases like this.
If we consider the morphemelevel, however, then this detection is not irrelevant.These results show that the entropy of branching works as a measure ofcontext boundaries, not only indicating word boundaries, but also showing thesentence structure of multiple layers, at the morpheme, word, and phrase levels.Some other successful segmentation examples in Chinese and Japanese areshown in Figure 6.
These cases were segmented by using Bmax.
Examples 1through 4 are from Chinese, and 5 through 12 are from Japanese, where ?|?
indi-cates the border.
As this method requires only a search engine, it can segmenttexts that are normally difficult to process by using language tools, such as insti-tution names (5, 6), colloquial expressions (7 to 10), and even some expressionstaken from Buddhist scripture (11, 12).6 Performance on a Larger Scale6.1 SettingsIn this section, we show the results of larger-scale segmentation experiments onChinese and Japanese.
The reason for the choice of languages lies in the fact thatthe process utilised here is based on the key assumption regarding the semanticaspects of language data.
As an ideogram already forms a semantic unit as itself,we intended to observe the performance of the procedure with respect to bothideograms and phonograms.
As Chinese contains ideograms only, while Japanesecontains both ideograms and phonograms, we chose these two languages.Because we need correct boundaries with which to compare our results, weutilised manually segmented corpora: the People?s Daily corpus from BeijingUniversity [7] for Chinese, and the Kyoto University Corpus [1] for Japanese.In the previous section, we calculated h for almost all substrings of a givenstring.
This requires O(n2) searches of strings, with n being the length of thegiven string.
Additionally, the process requires a heavy access load to the websearch engine.
As our interest is in verifying assumption (B), we conducted ourexperiment using the following algorithm for a given string x.1.
Set m = 0, n=1.2.
Calculate h for xm,n3.
If the entropy is unmeasurable, set m = m + 1,n = m + 2, and go to step 2.4.
Compare the result with that for xm,n?1.5.
If the value of h fulfils the boundary conditions, then output n as the bound-ary.
Set m = m + 1, n = m + 2, and go to 2.6.
Otherwise, set n = n + 1 and go to 2.The point of the algorithm is to ensure that the string length is not increased oncethe boundary is found, or if the entropy becomes unmeasurable.
This algorithmbecomes O(n2) in the worst case where no boundary is found and all substringsare measurable, although this is very unlikely to be the case.
Note that thisEntropy as an Indicator of Context Boundaries 103Fig.
7.
Precision and recall of word segmentation using the branching entropy in Chi-nese and Japanesealgorithm defines the regular order case, but we also conducted experiments inreverse order, too.As for the boundary condition, we utilized Bincrease, as it includes Bmax.
Athreshold val could be set to the margin of difference:h(xn+1) ?
h(xn) > val.
(6)The larger val is, the higher the precision, and the lower the recall.
We variedval in the experiment in order to obtain the precision and recall curve.As the process is slow and heavy, the experiment could not be run throughmillions of words.
Therefore, we took out portions of the corpora used for eachlanguage, which consisted of around 2000 words (Chinese 2039, Japanese 2254).These corpora were first segmented into phrases at commas, and each phrasewas fed into the procedure described above.
The suggested boundaries werethen compared with the original, correct boundaries.6.2 ResultsThe results are shown in Figure 7.
The horizontal axis and vertical axes representthe precision and recall, respectively.
The figure contains two lines, correspondingto the results for Japanese or Chinese.
Each line is plotted by varying val from0.0 to 3.0 with a margin of 0.5, where the leftmost points of the lines are theresults obtained for val=0.0.The precision was more than 90% for Chinese with val > 2.5.
In the caseof Japanese, the precision deteriorated by about 10%.
Even without a threshold(val = 0.0), however, the method maintained good precision in both languages.The locations indicated incorrectly were inside phonogram sequences consist-ing of long foreign terms, and in inflections in the endings of verbs and adjectives.In fact, among the incorrect points, many could be detected as correct segmenta-tions.
For example, in Chinese, surnames were separated from first names by our104 K. Tanaka-Ishiiprocedure, whereas in the original corpus, complete names are regarded as singlewords.
As another example in Chinese, the character is used to indicate?-ist?
in English, as in (revolutionist) and our process suggested thatthere is a border in between However, in the original corpus,these words are not segmented before but are instead treated as one word.Unlike the precision, the recall ranged significantly according to the thresh-old.
When val was high, the recall became small, and the texts were segmentedinto larger phrasal portions.
Some successful examples in Japanese for val=3.0are shown in the following.The segments show the global structure of the phrases, and thus, this resultdemonstrates the potential validity of assumption (B).
In fact, such sentencesegmentation into phrases would be better performed in a word-based manner,rather than a character-based manner, because our character-based experimentmixes the word-level and character-level aspects at the same time.
Some previousworks on collocation extraction have tried boundary detection using branching[5].
Boundary detection by branching outputs tightly coupled words that can bequite different from traditional grammatical phrases.
Verification of such aspectsremains as part of our future work.Overall, in these experiments, we could obtain a glimpse of language structurebased on assumption (B) where semantic units of different levels (morpheme,word, phrase) overlaid one another, as if to form a fractal of the context.
Theentropy of branching is interesting in that it has the potential to detect allboundaries of different layers within the same framework.7 ConclusionWe conducted a fundamental analysis to verify that the uncertainty of tokenscoming after a sequence can serve to determine whether a position is at a con-text boundary.
By inferring this feature of language from the well-known factthat the entropy of successive tokens decreases when a longer context is taken,we examined how boundaries could be detected by monitoring the entropy ofsuccessive tokens.
Then, we conducted two experiments, a small one in Japanese,and a larger-scale experiment in both Chinese and Japanese, to actually segmentwords by using only the entropy value.
Statistical measures were obtained usinga web search engine in order to overcome data sparseness.Through analysis of Japanese examples, we found that the method workedbetter for sequences of ideograms, rather than for phonograms.
Also, we ob-served that semantic layers of different levels (morpheme, word, phrase) couldpotentially be detected by monitoring the entropy of branching.
In our larger-scale experiment, points of increasing entropy correlated well with word bordersand{ are jbig j problems j suchas powerdecentralizaion.
){ (We think that j it is not the timeforbreakup).??????
????????
???
????????????????
??????
(ThereEntropy as an Indicator of Context Boundaries 105References1.
Kyoto University Text Corpus Version 3.0, 2003. http://www.kc.t.u-tokyo.ac.jp/nl-resource/corpus.html.2.
R.K. Ando and L. Lee.
Mostly-unsupervised statistical segmentation of japanese:Applications to kanji.
In ANLP-NAACL, 2000.3.
T.C.
Bell, J.G.
Cleary, and I. H. Witten.
Text Compression.
Prentice Hall, 1990.4.
M. Creutz and Lagus K. Unsupervised discovery of morphemes.
In Workshop ofthe ACL Special Interest Group in Computational Phonology, pages 21?30, 2002.5.
T.K.
Frantzi and S. Ananiadou.
Extracting nested collocations.
16th COLING,pages 41?46, 1996.6.
S.Z.
Harris.
From phoneme to morpheme.
Language, pages 190?222, 1955.7.
ICL.
People daily corpus, beijing university, 1999.
Institute of ComputationalLinguistics, Beijing University http://162.105.203.93/Introduction/ corpustag-ging.htm.8.
A. Kempe.
Experiments in unsupervised entropy-based corpus segmentation.
InWorkshop of EACL in Computational Natural Language Learning, pages 7?13,1999.9.
H. Nakagawa and T. Mori.
A simple but powerful automatic termextractionmethod.
In Computerm2: 2nd International Workshop on Computational Termi-nology, pages 29?35, 2002.10.
S. Nobesawa, J. Tsutsumi, D.S.
Jang, T. Sano, K. Sato, and M Nakanishi.
Seg-menting sentences into linky strings using d-bigram statistics.
In COLING, pages586?591, 1998.11.
J.R. Saffran.
Words in a sea of sounds: The output of statistical learning.
Cognition,81:149?169, 2001.12.
M. Sun, Dayang S., and B. K. Tsou.
Chinese word segmentation without usinglexicon and hand-crafted training data.
In COLING-ACL, 1998.especially in the case of Chinese.
These results reveal an interesting aspect ofthe statistical structure of language.
