Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 164?173,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsRegularized Structured Perceptron:A Case Study on Chinese Word Segmentation, POS Tagging and ParsingKaixu ZhangXiamen UniversityFujian, P.R.
Chinakareyzhang@gmail.comJinsong SuXiamen UniversityFujian, P.R.
Chinajssu@xmu.edu.cnChangle ZhouXiamen UniversityFujian, P.R.
Chinadozero@xmu.edu.cnAbstractStructured perceptron becomes popularfor various NLP tasks such as tagging andparsing.
Practical studies on NLP did notpay much attention to its regularization.
Inthis paper, we study three simple but effec-tive task-independent regularization meth-ods: (1) one is to average weights of dif-ferent trained models to reduce the biascaused by the specific order of the train-ing examples; (2) one is to add penaltyterm to the loss function; (3) and one isto randomly corrupt the data flow duringtraining which is called dropout in the neu-ral network.
Experiments are conductedon three NLP tasks, namely Chinese wordsegmentation, part-of-speech tagging anddependency parsing.
Applying proper reg-ularization methods or their combinations,the error reductions with respect to the av-eraged perceptron for some of these taskscan be up to 10%.1 IntroductionStructured perceptron is a linear classification al-gorithm.
It is used for word segmentation (Zhangand Clark, 2011), POS (part-of-speech) tagging(Collins, 2002), syntactical parsing (Collins andRoark, 2004), semantical parsing (Zettlemoyerand Collins, 2009) and other NLP tasks.The averaged perceptron or the voted percep-tron (Collins, 2002) is proposed for better gener-alization.
Early update (Collins and Roark, 2004;Huang et al., 2012) is used for inexact decod-ing algorithms such as the beam search.
Dis-tributed training (McDonald et al., 2010) and theminibatch and parallelization method (Zhao andHuang, 2013) are recently proposed.
Some otherrelated work focuses on the task-specified featureengineering.Regularization is to improve the ability ofgeneralization and avoid over-fitting for machinelearning algorithms including online learning al-gorithms (Do et al., 2009; Xiao, 2010).
But prac-tical studies on NLP did not pay much attention tothe regularization of the structured perceptron.
Asa result, for some tasks the model learned usingperceptron algorithm is not as good as the modellearned using regularized condition random field.In this paper, we treat the perceptron algorithmas a special case of the stochastic gradient de-scent (SGD) algorithm and study three kinds ofsimple but effective task-independent regulariza-tion methods that can be applied.
The averagingmethod is to average the weight vectors of differ-ent models.
We propose a ?shuffle-and-average?method to reduce the bias caused by the specificorder of the training examples.
The traditionalpenalty method is to add penalty term to the lossfunction.
The dropout method is to randomly cor-rupt the data flow during training.
We show thatthis dropout method originally used in neural net-work also helps the structured perceptron.In Section 2, we describe the perceptron algo-rithm as a special case of the stochastic gradientdescent algorithm.
Then we discuss three kinds ofregularization methods for structured perceptronin Section 3, 4 and 5, respectively.
Experimentsconducted in Section 6 shows that these regular-ization methods and their combinations improveperformances of NLP tasks such as Chinese wordsegmentation, POS tagging and dependency pars-ing.
Applying proper regularization methods, theerror reductions of these NLP tasks can be up to10%.
We finally conclude this work in Section 7.164w ?
?
(x,y)output layer?
(x,y)xinput layeryhidden layerFigure 1: A structured perceptron can be seen as amulti-layer feed-forward neural network.2 Structured PerceptronWe treat the structured perceptron architecture asa multi-layer feed-forward neural network as inFigure 1 and treat the perceptron algorithm as aspecial case of the stochastic gradient descent al-gorithm in order to describe all the regularizationmethods.The network of the structured perceptron hasthree layers.
The input vector x and output vectory of the structured classification task are concate-nated as the input layer.
The hidden layer is thefeature vector ?(x,y).
The connections betweenthe input layer and the hidden layer are usuallyhand-crafted and fixed during training and predict-ing.
And the output layer of the network is a scalarw ??
(x,y) which is used to evaluate the matchingof the vector x and y.Besides the common process to calculate theoutput layer given the input layer, there is a pro-cess called decoding, which is to find a vector z tomaximum the activation of the output layer:zi= arg maxzw ?
?
(xi, z) (1)By carefully designing the feature vector, the de-coding can be efficiently performed using dynamicprogramming.
Beam search is also commonlyused for the decoding of syntactical parsing tasks.In the predicting precess, the vector z is thestructured output corresponding to x.
In the train-ing precess, what we expect is that for every inputxi, the vector zithat maximums the activation ofthe output layer is exactly the gold standard outputyi.We define the loss function as the sum of themargins of the whole training data:L(w) =?i{w ?
?
(xi, zi)?w ?
?
(xi,yi)}= w?i???i(2)where?
?i= ?
(xi, zi)?
?
(xi,yi) (3)The unconstrained optimization problem of thetraining process isarg minwL(w) (4)The loss function is not convex but calculatingthe derivative is easy.
One of the algorithms tosolve this optimization problem is SGD.
Here weuse the minibatch with size of 1, which means inevery iteration we use only one training exampleto approximate the loss function and the gradientto update the weight vector:w(t+1)?
w(t)?
??L?w????w(t)?
w(t)?
???
(t)(5)where w(t)is the weight vector after t updates.Note that in this case, the learning rate ?
can be setto an arbitrary positive real number.
In the percep-tron algorithm commonly used in NLP (Collins,2002) , ?
is not changed respect to t. We fix ?
tobe 1 in this paper without loss of generality.3 Averaging3.1 Averaged PerceptronAveraging the weight vectors in the learning pro-cess is one of the most popular regularizationtechniques of the structured perceptron (Collins,2002).
And it is also the only used regulariza-tion technique for many practical studies on NLP(Jiang et al., 2009; Huang and Sagae, 2010).Suppose the learning algorithm stopped after Tupdates.
The final weight vector is calculated as:w =1TT?t=1w(t)(6)The intuition might be that the learned weightvector is dependent on the order of the training ex-amples.
The final vector w(T )may be more ap-propriate for the last few training examples thanthe previous ones.
The averaging method is usedto avoid such tendency.
Similar treatment is usedin other sequential algorithm such as the Markovchain Monte Carlo sampling method.Since this regularization technique is widelyused and tested, it is used for all the models inthe experiments of this paper.
Any other regular-ization methods are applied to this basic averagedperceptron.1653.2 Shuffle and AverageAs we has mentioned that the learned weight vec-tor is strongly dependent on the order of the train-ing examples, randomly shuffling the training ex-amples results in different weight vectors.
Basedon such observation, we training different weightvectors using the same training examples with dif-ferent orders, and average them to get the finalweight vector.
We use this method to further min-imize the side effect caused by this online algo-rithm.Suppose we shuffle and train n different weightvectors w[1], .
.
.
,w[n], the j-th component of thefinal vector can be simply calculated aswj=?ni=1w[i]jn(7)Note that generally these models do not sharethe same feature set.
Features may be used in onemodel but not in another one.
When w[i]j= 0, itdoes not imply that this feature has no effect onthis problem.
It only implies that this feature doesnot have chances to be tested.
We propose a modi-fied equation to only average the non-zero compo-nents:wj=?ni=1w[i]j???
{i|w[i]j6= 0, i = 1, ?
?
?
, n}???
(8)This equation makes the low-frequency featuresmore important in the final model.4 PenaltyAdding penalty term to the loss function is a com-mon and traditional regularization method to avoidover-fitting.
It is widely used for the optimizationproblems of logistic regression, support vectormachine, conditional random field and other mod-els.
Penalty terms for probabilistic models can beinterpreted as a prior over the weights (Chen andRosenfeld, 1999).
It is also called ?weight decay?in artificial neural network (Moody et al., 1995).The use of the penalty term is to prevent the com-ponents of the weight vector to become too large.In Section 2 we have modeled the perceptron al-gorithm as an SGD algorithm with an explicit lossfunction, the additional penalty term is thereforeeasy to be employed.4.1 L2-norm penaltyWe can add a square of the L2-norm of the weightvector as the penalty term to the loss function asL = w ??i?
?i+?22?w?22(9)where ?2is a hyper-parameter to determine thestrength of the penalty.In the SGD algorithm, the update method of theweight vector is thusw(t+1)?
(1?
??2)w(t)?
???
(t)(10)The term (1?
?
?2) is used to decay the weight inevery updates.
This forces the weights to be closeto zero.4.2 L1-norm penaltyAnother commonly used penalty term is the L1-norm of the weight vector.
This kinds of termsusually results in sparse weight vector.
Since theaveraged perceptron is used, the final averagedweight vector will not be sparse.The loss function using the L1-nrom penalty isL = w ??i?
?i+ ?1?w?1(11)where ?1is the hyper-parameter to determine thestrength of the penalty.The derivative of the penalty term is discontin-uous.
We update the weights asw(t+1)i?max{0, |w(t)i| ?
??1}|w(t)i|w(t)i?
???
(t)i(12)This ensures that the weight decay will not changethe sign of the weight.An modified version of the L1 penalty for theonline learning is the cumulative L1 penalty (Tsu-ruoka et al., 2009), which is used to make thestochastic gradient of the penalty term more closeto the true gradient.
The update is divided into twosteps.
In the first step, the weight vector is updatedaccording to the loss function without the penaltytermw(t+12)i?
w(t)i?
???
(t)i(13)And the cumulative penalty is calculated sepa-ratelyc(t+12)i?
c(t)i+ ?
?1(14)166In the second step, |wi| and ciare compared andat most one of them is non-zero before the nextupdatem ?
min{|w(t+12)i|, c(t+12)i} (15)w(t+1)i?max{0,|w(t+12)i|?m}|w(t+12)i|w(t+12)i(16)c(t+1)i?
c(t+12)i?m (17)5 DropoutDropout (Hinton et al., 2012) is originally a regu-larization method used for the artificial neural net-work.
It corrupts one or more layers of a feed-forward network during training, by randomlyomitting some of the neurons.
If the input layeris corrupted during the training of an autoencoder,the model is called denoising autoencoder (Vin-cent et al., 2008).The reason why such treatment can regularizethe parameters are explained in different ways.Hinton et al.
(2012) argued that the final modelis an average of a large number of models and thedropout forces the model to learn good featureswhich are less co-adapted.
Vincent et al.
(2008)argued that by using dropout of the input layer, themodel can learn how to deal with examples out-side the low-dimensional manifold that the train-ing data concentrate.Models not so deep such as the structured per-ceptron may also benefit from this idea.
Follow-ing the dropout method used in neural network, wegive the similar method for structured perceptron.5.1 Input LayerWe can perform dropout for structured perceptronby corrupting the input layer in Figure 1.
Sincewe concern that what y exactly is, we only corruptx.
The components of the corrupted vector?x iscalculated asx?i= xini(18)where ni?
Bern(p) obey a Binomial distributionwith the hyper-parameter p.During training, the decoding processing withthe corrupted input isz = arg maxzw ?
?
(?x, z) (19)The x in the loss function is also substituted withthe corrupted version?x.Note that the corruption decreases the numberof non-zero components of the feature vector ?,which makes the decoding algorithm harder to findthe gold standard y.For NLP tasks, the input vector x could be asequence of tokens (words, POS tags, etc.).
Thecorruption substitutes some of the tokens with aspecial token null.
Any features contain such to-ken will be omitted (This is also the case for theout-of-vocabulary words during predicting).
Sothe dropout of x in NLP during training can beexplained as to randomly mask some of the inputtokens.
The decoder algorithm needs to find outthe correct answer even if some parts of the inputare unseen.
This harder situation could force thelearning algorithm to learn better models.5.2 Hidden LayerThe dropout can also be performed at the hiddenlayer.
Likewise, the components of the corruptedfeature vector??
is calculated as?
?i= ?imi(20)where mi?
Bern(q) obey a Binomial distributionwith the hyper-parameter q.The ?
in the decoding processing during train-ing and the loss function is substituted with?
?.6 ExperimentsIn this section, we first introduce three NLP tasksusing structured perceptron namely Chinese wordsegmentation, POS tagging and dependency pars-ing.
Then we investigate the effects of regular-ization methods for structured perceptron mainlyon the development set of character-based Chineseword segmentation.
Finally, we compare the finalperformances on the test sets of these three tasksusing regularization methods with related work.6.1 Tasks6.1.1 Chinese Word SegmentationA Chinese word consists of one or more Chinesecharacters.
But there is no spaces in the sentencesto indicating words.
Chinese word segmentationis the task to segment words in the sentence.We use a character-based Chinese word seg-mentation model as the baseline.
Like part-of-speech tagging which is to assign POS tags towords sequence, character-based Chinese wordsegmentation is to assign tags to character se-quence.
The tag set of four tags is commonly used:167Type TemplatesUnigram ?xi?1, yi?, ?xi, yi?, ?xi+1, yi?Bigram ?xi?2, xi?1, yi?, ?xi?1, xi, yi?
?xi, xi+1, yi?, ?xi+1, xi+2, yi?transition ?yi?1, yi?Table 1: Feature templates for the character-based Chinese word segmentation model and thejoint Chinese word segmentation and POS taggingmodel.tag S indicates that the character forms a single-character words; tag B / E indicates that the char-acter is at the beginning / end of a multi-characterwords; tag M indicates that the character is in themiddle of a multi-character words.For example, if the tag sequence for the inputx = ??????
(21)isy = BMESBE, (22)the corresponding segmentation result is???
?
??.
(23)Table 1 shows the set of the feature templateswhich is a subset of some related work (Ng andLow, 2004; Jiang et al., 2009) .Following Sun (2011), we split the Chinesetreebank 5 into training set, development set andtest set.
F-measure (Emerson, 2005) is used as themeasurement of the performance.6.1.2 Part-of-Speech TaggingThe second task is joint Chinese word segmenta-tion and POS tagging.
This can also be modeledas a character-based sequence labeling task.The tag set is a Cartesian product of the tag setfor Chinese word segmentation and the set of POStags.
For example, the tag B-NN indicates thecharacter is the first character of a multi-characternoun.
The tag sequencey = B-NR M-NR E-NR S-DEG B-NN E-NN,(24)for the input sentence in Equation (21) results in???
NR ?
DEG ??
NN.
(25)The same feature templates shown in Table 1are used for joint Chinese word segmentation andPOS tagging.Also, we use the same training set, developmentset and test set based on CTB5 corpus as the Chi-nese word segmentation task.
F-measure for jointChinese word segmentation and POS tagging isused as the measurement of the performance.6.1.3 Dependency ParsingThe syntactical parsing tasks are different withpreviously introduced tagging tasks.
To investi-gate the effects of regularization methods on theparsing tasks, we fully re-implement the linear-time incremental shift-reduce dependency parserby Huang and Sagae (2010).
The structure per-ceptron is used to train such model.
The modeltotally employs 28 feature templates proposed byHuang and Sagae (2010).Since the search space for parsing tasks is quitelarger than the search space for tagging tasks, Ex-act search algorithms such as dynamic program-ming can not be used.
Besides, beam search withstate merging is used for decoding.
The early up-date strategy (Collins and Roark, 2004) is also em-ployed.In order to compare to the related work, un-like the Chinese word segmentation and the POStagging task, we split the CTB5 corpus follow-ing Zhang et al.(2008).
Two types of accuraciesare used to measure the performances, namelyword and complete match (excluding punctua-tions) (Huang and Sagae, 2010).6.2 AveragingFirst, we investigate the effect of averaging tech-niques for regularization.
Figure 2 shows the in-fluence of the number of the averaged models byusing the ?shuffle-and-average?
method describedin section 3.2.
The performances of the Chineseword segmentation, POS tagging and parsing tasksare all increased by averaging models trained withthe same training data with different orders.
The?shuffle-and-average?
method is effective to re-duce the bias caused by the specific order of thetraining examples.For the Chinese word segmentation task whichis a relatively simple task, averaging about five dif-ferent models can achieve the best effect; whereasfor POS tagging and parsing, averaging moremodels will continually increase the performanceeven when the number of models approaches 10.The dotted lines in Figure 2 indicate the perfor-mances by using Equation (7) for model averag-ing.
The solid lines indicate the performances by168(a) Chinese word segmentation (b) POS tagging (c) Dependency parsingFigure 2: The influence of the number of the averaged models using the ?shuffle-and-average?
methodfor (a) Chinese word segmentation, (b) POS tagging and (c) dependency parsing.
?Shuffle?
means toonly average the non-zero weights (Equation (8)), while ?Shuffle (average all)?
means to average allweights (Equation (7)).using Equation (8) for model averaging.
Accord-ing to these three different tasks, Equation (8) al-ways performs better than Equation (7).
We willuse Equation (8) denoted as ?Shuffle?
for the restof the experiments.6.3 PenaltyHere we investigate the penalty techniques for reg-ularization only using the character-based Chineseword segmentation task.Figure 3 shows the effect of adding L1-normand L2-norm penalty terms to the loss function.With appropriate hyper-parameters, the perfor-mances are increased.
According to the per-formances, adding L2 penalty is slightly betterthan adding L1 penalty or adding cumulative L1penalty.We then combine the ?shuffle-and-average?method with the penalty methods.
The perfor-mances (solid lines in Figure 3) are further im-proved and are better than those of models thatonly use one regularization method.6.4 DropoutWe also investigate the dropout method for regu-larization using the character-based Chinese wordsegmentation task.Figure 4 shows the effect of the dropout method(?dropout?
for the input layer and ?dropout (?
)?for the hidden layer) and the combination of thedropout and ?shuffle-and-average?
method (solidline).
We observed that the dropout for the hid-den layer is not effective for structured perceptron.This may caused by that the connections betweenthe input layer and the hidden layer are fixed dur-ing training.
Neurons in the hidden layer can notFigure 4: Influences of the hyper-parameter p (forthe input layer, denoted as ?dropout?)
or q (forthe hidden layer, denoted as ?dropout (?)?)
for thedropout method.changes the weights to learn different representa-tions for the input layer.
On the other hand, thedropout for the input layer improves the perfor-mance.
Combining the dropout and the ?shuffle-and-average?
method, the performance is furtherimproved.Figure 5 shows the effect of the combination ofthe three regularization methods.
We see that nomatter what other regularization methods are al-ready used, adding ?shuffle-and-average?
methodcan always improve the performance.
The effectsof the penalty method and the dropout methodhave some overlap, since combining these twomethod does not result in a significant improve-ment of the performance.6.5 Final Results6.5.1 Chinese Word SegmentationTable 2 shows the final results of the character-based Chinese word segmentation task on the testset of the CTB5 corpus.Structure perceptron with feature templates in169(a) L2-norm penalty (b) L1-norm penaltyFigure 3: influence of the hyper-parameter ?2in the L2-norm penalty term and ?1in the L1-norm penaltyterm (?l1-c?
indicates the cumulative L1 penalty) for the character-based Chinese word segmentationtask.Figure 5: The combination of these three regular-ization methods.Table 1 is used.
We use the ?shuffle-and-average?
(5 models), the L2 penalty method (?2= 10?4),the dropout method (p = 3%) and their combina-tions to regularize the structured perceptron.To compare with the perceptron algorithm, weuse the conditional random field model (CRF)with the same feature templates in Table 1 to trainthe model parameters.
The toolkit CRF++1withthe L2-norm penalty is used to train the weights.The hyper-parameter C = 20 is tuned using thedevelopment set.Jiang et al.
(2009) proposed a character-basedmodel employing similar feature templates usingaveraged perceptron.
The feature templates arefollowing Ng and Low (2004).
Zhang and Clark(2011) proposed a word-based model employingboth character-based features and more sophis-ticated word-based features using also averagedperceptron.
There are other related results (Jianget al., 2012) of open test including the final resultof Jiang et al.
(2009).
Since their models usedextra resources, they are not comparable with the1http://crfpp.googlecode.com/svn/trunk/doc/index.htmlsf(Jiang et al., 2009) 0.9735(Zhang and Clark, 2011) 0.9750?CRF++ (C = 20) 0.9742Averaged Percetron 0.9734+ Shuffle 0.9755+ L2 0.9736+ L2 + Shuffle 0.9772+ Dropout 0.9741+ Dropout+ Shuffle 0.9765+ L2 + Dropout 0.9749+ L2 + Dropout+ Shuffle 0.9771Table 2: Final results of the character-based Chi-nese word segmentation task on CTB5.
?
This re-sult is read from a figure in that paper.sfWord-based model 0.9758+ Shuffle 0.9787+ L2 + Shuffle 0.9791+ L2 + Dropout+ Shuffle 0.9791Table 3: Final results of the word-based Chineseword segmentation task on CTB5.results in this paper.The results in Table 2 shows that with properregularization methods, the models trained usingperceptron algorithm can outperform CRF modelswith the same feature templates and other modelswith more sophisticated features trained using theaveraged perceptron without other regularizationmethods.We further re-implemented a word-based Chi-nese word segmentation model with the featuretemplates following Zhang et al.
(2012), which170sf jf(Jiang et al., 2008) 0.9785 0.9341(Kruengkrai et al., 2009) 0.9787 0.9367(Zhang and Clark, 2010) 0.9778 0.9367(Sun, 2011) 0.9817 0.9402Character-based model 0.9779 0.9336+ Shuffle 0.9802 0.9375+ Dropout 0.9789 0.9361+ Dropout+ Shuffle 0.9809 0.9407+ word-based re-ranking 0.9813 0.9438Table 4: Final results of the POS tagging task onCTB5.word compl.
(Huang and Sagae, 2010) 85.20 33.72our re-implementation 85.22 34.15+ Shuffle 85.65 34.52+ Dropout 85.32 34.04+ Dropout+ Shuffle 85.71 34.57Table 5: Final results of the dependency parsingtask on CTB5.is similar with the model proposed by Zhang andClark (2011).
Beam search with early-update isused for decoding instead of dynamic program-ming.
The results with different regularizationmethods are shown in Figure 3.
These regulariza-tion methods show similar characteristics for theword-based model.6.5.2 POS TaggingThe results of the POS tagging models on theCTB5 corpus are shown in Table 4.
Structure per-ceptron with feature templates in Table 1 is used.The F-measures for word segmentation (sf) andfor joint word segmentation and POS tagging (jf)are listed.We use the ?shuffle-and-average?
(10 models),the dropout method (p = 5%) and their combina-tion to regularize the structured perceptron.Jiang et al.
(2008) used a character-basedmodel using perceptron for POS tagging and alog-linear model for re-ranking.
Kruengkrai etal.
(2009) proposed a hybrid model includingcharacter-based and word-based features.
Zhangand Clark (2010) proposed a word-based modelusing perceptron.
Sun (2011) proposed a frame-work based on stacked learning consisting of foursub-models.
For the closed test, this model hasthe best performance on the CTB5 corpus to ourknowledge.
Other results (Wang et al., 2011; Sunand Wan, 2012) for the open test are not listedsince they are not comparable with the results inthis paper.If we define the error rate as 1?
jf, the error re-duction by applying regularization methods for thecharacter-based model is more than 10%.
Com-paring to the related work, the character-basedmodel that we used is quite simple.
But usingthe regularization methods discussed in this paper,it provides a comparable performance to the bestmodel in the literature.6.5.3 Dependency ParsingTable 5 shows the final results of the dependencyparsing task on the CTB5 corpus.
We use the?shuffle-and-average?
(10 models), the dropoutmethod (p = 5% only for the words in the input)and their combination to regularize the structuredperceptron based on Huang and Sagae?s (2010).The performance of the parsing model is alsoimproved by using more regularization methods,although the improvement is not as remarkableas those for tagging tasks.
For the parsing tasks,there are many other factors that impact the per-formance.7 ConclusionThe ?shuffle-and-average?
method can effectivelyreduce the bias caused by the specific order of thetraining examples.
It can improve the performanceeven if some other regularization methods are ap-plied.When we treat the perceptron algorithm as aspecial case of the SGD algorithm, the traditionalpenalty methods can be applied.
And our observa-tion is that L2 penalty is better than L1 penalty.The dropout method is derived from the neuralnetwork.
Corrupting the input during training im-proves the ability of generalization.
The effects ofthe penalty method and the dropout method havesome overlap.Experiments showed that these regularizationmethods help different NLP tasks such as Chineseword segmentation, POS tagging and dependencyparsing.
Applying proper regularization methods,the error reductions for some of these NLP taskscan be up to 10%.
We believe that these meth-ods can also help other models which are based onstructured perceptron.171AcknowledgmentsThe authors want to thanks all the reviews formany pertinent comments which have improvedthe quality of this paper.
The authors are supportedby NSFC (No.
61273338 and No.
61303082), theDoctoral Program of Higher Education of China(No.
20120121120046) and China PostdoctoralScience Foundation (No.
2013M541861).ReferencesStanley F Chen and Ronald Rosenfeld.
1999.
A gaus-sian prior for smoothing maximum entropy models.Technical report, DTIC Document.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of the 42nd Meeting of the Association for Com-putational Linguistics (ACL?04), Main Volume, page111118, Barcelona, Spain, July.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
pages 1?8.Chuong B Do, Quoc V Le, and Chuan-Sheng Foo.2009.
Proximal regularization for online and batchlearning.
In Proceedings of the 26th Annual Interna-tional Conference on Machine Learning, pages 257?264.
ACM.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedings ofthe Fourth SIGHANWorkshop on Chinese LanguageProcessing, pages 123?133.
Jeju Island, Korea.Geoffrey E. Hinton, Nitish Srivastava, AlexKrizhevsky, Ilya Sutskever, and Ruslan R. Salakhut-dinov.
2012.
Improving neural networks bypreventing co-adaptation of feature detectors.
arXivpreprint arXiv:1207.0580.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 1077?1086, Uppsala, Sweden, July.
Association for Com-putational Linguistics.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages142?151.
Association for Computational Linguis-tics.Wenbin Jiang, Liang Huang, Qun Liu, and YajuanL?.
2008.
A cascaded linear model for joint chi-nese word segmentation and part-of-speech tagging.In Proceedings of ACL-08: HLT, pages 897?904,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.
Au-tomatic adaptation of annotation standards: Chineseword segmentation and POS tagging - a case study.In Proceedings of the 47th ACL, pages 522?530,Suntec, Singapore, August.
Association for Compu-tational Linguistics.Wenbin Jiang, Fandong Meng, Qun Liu, and YajuanL?.
2012.
Iterative annotation transformation withpredict-self reestimation for chinese word segmen-tation.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 412?420, Jeju Island, Korea, July.Association for Computational Linguistics.Canasai Kruengkrai, Kiyotaka Uchimoto, Jun?ichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hy-brid model for joint chinese word segmentation andPOS tagging.
In Proc.
of ACL-IJCNLP 2009, pages513?521, Suntec, Singapore.
Association for Com-putational Linguistics.Ryan McDonald, Keith Hall, and Gideon Mann.
2010.Distributed training strategies for the structured per-ceptron.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 456?464.
Association for Computa-tional Linguistics.JE Moody, SJ Hanson, Anders Krogh, and John AHertz.
1995.
A simple weight decay can improvegeneralization.
Advances in neural information pro-cessing systems, 4:950?957.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?word-based or character-based?
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 277?284, Barcelona, Spain, July.
Associationfor Computational Linguistics.Weiwei Sun and Xiaojun Wan.
2012.
Reducing ap-proximation and estimation errors for chinese lexi-cal processing with heterogeneous annotations.
InProceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 232?241, Jeju Island, Korea,July.
Association for Computational Linguistics.Weiwei Sun.
2011.
A stacked sub-word model forjoint chinese word segmentation and part-of-speechtagging.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 1385?1394, Portland, Oregon, USA, June.
Association forComputational Linguistics.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-niadou.
2009.
Stochastic gradient descent train-ing for l1-regularized log-linear models with cumu-lative penalty.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the1724th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 1-Volume1, pages 477?485.
Association for ComputationalLinguistics.Pascal Vincent, Hugo Larochelle, Yoshua Bengio, andPierre-Antoine Manzagol.
2008.
Extracting andcomposing robust features with denoising autoen-coders.
In Proceedings of the 25th internationalconference on Machine learning, page 10961103.Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.2011.
Improving chinese word segmentation andPOS tagging with semi-supervised methods usinglarge auto-analyzed data.
In Proceedings of 5th In-ternational Joint Conference on Natural LanguageProcessing, pages 309?317, Chiang Mai, Thailand,November.
Asian Federation of Natural LanguageProcessing.Lin Xiao.
2010.
Dual averaging methods for reg-ularized stochastic learning and online optimiza-tion.
The Journal of Machine Learning Research,9999:2543?2596.Luke S Zettlemoyer and Michael Collins.
2009.Learning context-dependent mappings from sen-tences to logical form.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on Natu-ral Language Processing of the AFNLP: Volume 2-Volume 2, pages 976?984.
Association for Compu-tational Linguistics.Yue Zhang and Stephen Clark.
2008.
A tale of twoparsers: Investigating and combining graph-basedand transition-based dependency parsing.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, page 562571,Honolulu, Hawaii, October.
Association for Compu-tational Linguistics.Yue Zhang and Stephen Clark.
2010.
A fast decoderfor joint word segmentation and POS-Tagging usinga single discriminative model.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 843?852, Cambridge,MA, October.
Association for Computational Lin-guistics.Y.
Zhang and S. Clark.
2011.
Syntactic processingusing the generalized perceptron and beam search.Computational Linguistics, (Early Access):1?47.Kaixu Zhang, Maosong Sun, and Changle Zhou.
2012.Word segmentation on chinese mirco-blog data witha linear-time incremental model.
In Proceedings ofthe Second CIPS-SIGHAN Joint Conference on Chi-nese Language Processing, pages 41?46, Tianjin,China, December.
Association for ComputationalLinguistics.Kai Zhao and Liang Huang.
2013.
Minibatch and par-allelization for online large margin structured learn-ing.
In Proceedings of NAACL-HLT, pages 370?379.173
