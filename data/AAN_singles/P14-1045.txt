Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 479?488,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsPredicting the relevance of distributional semantic similarity withcontextual informationPhilippe MullerIRIT, Toulouse UniversityUniversit?e Paul Sabatier118 Route de Narbonne31062 Toulouse Cedex 04philippe.muller@irit.frC?ecile FabreCLLE, Toulouse UniversityUniversit?e Toulouse-Le Mirail5 alles A. Machado31058 Toulouse Cedexcecile.fabre@univ-tlse2.frCl?ementine AdamCLLE, Toulouse UniversityUniversit?e Toulouse-Le Mirail5 alles A. Machado31058 Toulouse Cedexclementine.adam@univ-tlse2.frAbstractUsing distributional analysis methods tocompute semantic proximity links be-tween words has become commonplacein NLP.
The resulting relations are oftennoisy or difficult to interpret in general.This paper focuses on the issues of eval-uating a distributional resource and filter-ing the relations it contains, but insteadof considering it in abstracto, we focuson pairs of words in context.
In a dis-course, we are interested in knowing if thesemantic link between two items is a by-product of textual coherence or is irrele-vant.
We first set up a human annotationof semantic links with or without contex-tual information to show the importance ofthe textual context in evaluating the rele-vance of semantic similarity, and to assessthe prevalence of actual semantic relationsbetween word tokens.
We then built an ex-periment to automatically predict this rel-evance, evaluated on the reliable referencedata set which was the outcome of the firstannotation.
We show that in-document in-formation greatly improve the predictionmade by the similarity level alone.1 IntroductionThe goal of the work presented in this paper is toimprove distributional thesauri, and to help evalu-ate the content of such resources.
A distributionalthesaurus is a lexical network that lists semanticneighbours, computed from a corpus and a simi-larity measure between lexical items, which gen-erally captures the similarity of contexts in whichthe items occur.
This way of building a seman-tic network has been very popular since (Grefen-stette, 1994; Lin, 1998), even though the nature ofthe information it contains is hard to define, andits evaluation is far from obvious.
A distributionalthesaurus includes a lot of ?noise?
from a seman-tic point of view, but also lists relevant lexical pairsthat escape classical lexical relations such as syn-onymy or hypernymy.There is a classical dichotomy when evaluat-ing NLP components between extrinsic and in-trinsic evaluations (Jones, 1994), and this appliesto distributional thesauri (Curran, 2004; Poibeauand Messiant, 2008).
Extrinsic evaluations mea-sure the capacity of a system in which a resourceor a component to evaluate has been used, for in-stance in this case information retrieval (van derPlas, 2008) or word sense disambiguation (Weedsand Weir, 2005).
Intrinsic evaluations try to mea-sure the resource itself with respect to some hu-man standard or judgment, for instance by com-paring a distributional resource with respect to anexisting synonym dictionary or similarity judg-ment produced by human subjects (Pado and La-pata, 2007; Baroni and Lenci, 2010).
The short-comings of these methods have been underlinedin (Baroni and Lenci, 2011).
Lexical resourcesdesigned for other objectives put the spotlight onspecific areas of the distributional thesaurus.
Theyare not suitable for the evaluation of the wholerange of semantic relatedness that is exhibited bydistributional similarities, which exceeds the lim-its of classical lexical relations, even though re-searchers have tried to collect equivalent resourcesmanually, to be used as a gold standard (Weeds,2003; Bordag, 2008; Anguiano et al, 2011).
Oneadvantage of distributional similarities is to exhibita lot of different semantic relations, not necessar-ily standard lexical relations.
Even with respectto established lexical resources, distributional ap-proaches may improve coverage, complicating theevaluation even more.The method we propose here has been de-signed as an intrinsic evaluation with a view tovalidate semantic proximity links in a broad per-479spective, to cover what (Morris and Hirst, 2004)call ?non classical lexical semantic relations?.For instance, agentive relations (author/publish,author/publication) or associative relations (ac-tor/cinema) should be considered.
At the sametime, we want to filter associations that can beconsidered as accidental in a semantic perspective(e.g.
flag and composer are similar because theyappear a lot with nationality names).
We do thisby judging the relevance of a lexical relation in acontext where both elements of a lexical pair oc-cur.
We show not only that this improves the relia-bility of human judgments, but also that it gives aframework where this relevance can be predictedautomatically.
We hypothetize that evaluating andfiltering semantic relations in texts where lexicalitems occur would help tasks that naturally makeuse of semantic similarity relations, but assessingthis goes beyond the present work.In the rest of this paper, we describe the re-source we used as a case study, and the data wecollected to evaluate its content (section 2).
Wepresent the experiments we set up to automaticallyfilter semantic relations in context, with variousgroups of features that take into account informa-tion from the corpus used to build the thesaurusand contextual information related to occurrencesof semantic neighbours 3).
Finally we discusssome related work on the evaluation and improve-ment of distributional resources (section 4).2 Evaluation of lexical similarity incontext2.1 DataWe use a distributional resource for French, builton a 200M word corpus extracted from the FrenchWikipedia, following principles laid out in (Bouri-gault, 2002) from a structured model (Baroniand Lenci, 2010), i.e.
using syntactic con-texts.
In this approach, contexts are triples (gover-nor,relation,dependent) derived from syntactic de-pendency structures.
Governors and dependentsare verbs, adjectives and nouns.
Multiword unitsare available, but they form a very small subsetof the resulting neighbours.
Base elements in thethesaurus are of two types: arguments (depen-dents?
lemma) and predicates (governor+relation).This is to keep the predicate/argument distinctionsince similarities will be computed between pred-icate pairs or argument pairs, and a lexical itemcan appear in many predicates and as an argument(e.g.
interest as argument, interest for as one pred-icate).
The similarity of distributions was com-puted with Lin?s score (Lin, 1998).We will talk of lexical neighbours or distribu-tional neighbours to label pairs of predicates or ar-guments, and in the rest of the paper we consideronly lexical pairs with a Lin score of at least 0.1,which means about 1.4M pairs.
This somewhatarbitrary level is an a priori threshold to limit theresulting database, and it is conservative enoughnot to exclude potential interesting relations.
Thedistribution of scores is given figure 1; 97% of theselected pairs have a score between 0.1 and 0.29.Figure 1: Histogram of Lin scores for pairs con-sidered.To ease the use of lexical neighbours in our ex-periments, we merged together predicates that in-clude the same lexical unit, a posteriori.
Thusthere is no need for a syntactic analysis of the con-text considered when exploiting the resource, andsparsity is less of an issue1.2.2 AnnotationIn order to evaluate the resource, we set up an an-notation in context: pairs of lexical items are tobe judged in their context of use, in texts wherethey occur together.
To verify that this method-ology is useful, we did a preliminary annotationto contrast judgment on lexical pairs with or with-out this contextual information.
Then we made alarger annotation in context once we were assuredof the reliability of the methodology.For the preliminary test, we asked three annota-tors to judge the similarity of pairs of lexical itemswithout any context (no-context), and to judge the1Whenever two predicates with the same lemma havecommon neighbours, we average the score of the pairs.480[...] Le ventre de l?impala de m?eme que ses l`evres et sa queue sont blancs.
Il faut aussi mentionner leurs lignes noires uniques`a chaque individu au bout des oreilles , sur le dos de la queue et sur le front.
Ces lignes noires sont tr`es utiles aux impalaspuisque ce sont des signes qui leur permettent de se reconnaitre entre eux.
Ils poss`edent aussi des glandes s?ecr?etant des odeurssur les pattes arri`eres et sur le front.
Ces odeurs permettent ?egalement aux individus de se reconnaitre entre eux.
Il a ?egalementdes coussinets noirs situ?es, `a l?arri`ere de ses pattes .
Les impalas m?ales et femelles ont une morphologie diff?erente.
En effet,on peut facilement distinguer un m?ale par ses cornes en forme de S qui mesurent de 40 `a 90 cm de long.Les impalas vivent dans les savanes o`u l?
herbe (courte ou moyenne) abonde.
Bien qu?ils appr?ecient la proximit?e d?une sourced?eau, celle-ci n?est g?en?eralement pas essentielle aux impalas puisqu?ils peuvent se satisfaire de l?eau contenue dans l?
herbequ?ils consomment.
Leur environnement est relativement peu accident?e et n?est compos?e que d?
herbes , de buissons ainsi quede quelques arbres.[...
]Figure 2: Example excerpt during the annotation of lexical pairs: annotators focus on a target item (herecorne, horn, in blue) and must judge yellow words (pending: oreille/queue, ear/tail), either validatingtheir relevance (green words: pattes, legs) or rejecting them (red words: herbe, grass).
The text describesthe morphology of the impala, and its habitat.similarity of pairs presented within a paragraphwhere they both occur (in context).
The three an-notators were linguists, and two of them (1 and3) knew about the resource and how it was built.For each annotation, 100 pairs were randomly se-lected, with the following constraints:?
for the no-context annotation, candidate pairshad a Lin score above 0.2, which placed themin the top 14% of lexical neighbours with re-spect to the similarity level.?
for the in context annotation, the only con-straint was that the pairs occur in the sameparagraph somewhere in the corpus used tobuild the resource.
The example paragraphwas chosen at random.The guidelines given in both cases were thesame: ?Do you think the two words are seman-tically close ?
In other words, is there a seman-tic relation between them, either classical (syn-onymy, hypernymy, co-hyponymy, meronymy, co-meronymy) or not (the relation can be paraphrasedbut does not belong to the previous cases) ?
?For the pre-test, agreement was rather moderatewithout context (the average of pairwise kappaswas .46), and much better with a context (aver-age = .68), with agreement rates above 90%.
Thisseems to validate the feasability of a reliable anno-tation of relatedness in context, so we went on fora larger annotation with two of the previous anno-tators.For the larger annotation, the protocol wasslightly changed: two annotators were given 42full texts from the original corpus where lexicalneighbours occurred.
They were asked to judgethe relation between two items types, regardless ofthe number of occurrences in the text.
This timethere was no filtering of the lexical pairs beyondthe 0.1 threshold of the original resource.
We fol-lowed the well-known postulate (Gale et al, 1992)that all occurrences of a word in the same dis-course tend to have the same sense (?one senseper discourse?
), in order to decrease the annotatorworkload.
We also assumed that the relation be-tween these items remain stable within the docu-ment, an arguably strong hypothesis that needed tobe checked against inter-annotator agreement be-fore beginning the final annotation .
It turns outthat the kappa score (0.80) shows a better inter-annotator agreement than during the preliminarytest, which can be explained by the larger contextgiven to the annotator (the whole text), and thusmore occurrences of each element in the pair tojudge, and also because the annotators were moreexperienced after the preliminary test.
Agreementmeasures are summed-up table 1.
An excerpt of anexample text, as it was presented to the annotators,is shown figure 2.Overall, it took only a few days to annotate9885 pairs of lexical items.
Among the pairs thatwere presented to the annotators, about 11% werejudged as relevant by the annotators.
It is noteasy to decide if the non-relevant pairs are justnoise, or context-dependent associations that werenot present in the actual text considered (for pol-ysemy reasons for instance), or just low-level as-sociations.
An important aspect is thus to guar-antee that there is a correlation between the sim-481Annotators Non-contextual ContextualAgreement rate Kappa Agreement rate KappaN1+N2 77% 0.52 91% 0.66N1+N3 70% 0.36 92% 0.69N2+N3 79% 0.50 92% 0.69Average 75, 3% 0,46 91, 7% 0,68Experts NA NA 90.8% 0.80Table 1: Inter-annotator agreements with Cohen?s Kappa for contextual and non-contextual annotations.N1, N2, N3 were annotators during the pre-test; expert annotation was made on a different dataset fromthe same corpus, only with the full discourse context.ilarity score (Lin?s score here), and the evaluatedrelevance of the neighbour pairs.
Pearson corre-lation factor shows that Lin score is indeed sig-nificantly correlated to the annotated relevance oflexical pairs, albeit not strongly (r = 0.159).The produced annotation2can be used as a ref-erence to explore various aspects of distributionalresources, with the caveat that it is as such a bitdependent on the particular resource used.
Wenonetheless assume that some of the relevant pairswould appear in other thesauri, or would be of in-terest in an evaluation of another resource.The first thing we can analyse from the anno-tated data is the impact of a threshold on Lin?sscore to select relevant lexical pairs.
The resourceitself is built by choosing a cut-off which is sup-posed to keep pairs with a satisfactory similar-ity, but this threshold is rather arbitrary.
Figure3 shows the influence of the threshold value to se-lect relevant pairs, when considering precision andrecall of the pairs that are kept when choosing thethreshold, evaluated against the human annotationof relevance in context.
In case one wants to opti-mize the F-score (the harmonic mean of precisionand recall) when extracting relevant pairs, we cansee that the optimal point is at .24 for a thresholdof .22 on Lin?s score.
This can be considered as abaseline for extraction of relevant lexical pairs, towhich we turn in the following section.3 Experiments: predicting relevance incontextThe outcome of the contextual annotation pre-sented above is a rather sizeable dataset of val-idated semantic links, and we showed these lin-guistic judgments to be reliable.
We used this2Freely available here http://www.irit.fr/?Philippe.Muller/resources.html.Figure 3: Precision and recall on relevant linkswith respect to a threshold on the similarity mea-sure (Lin?s score)dataset to set up a supervised classification exper-iment in order to automatically predict the rele-vance of a semantic link in a given discourse.
Wepresent now the list of features that were used forthe model.
They can be divided in three groups,according to their origin: they are computed fromthe whole corpus, gathered from the distributionalresource, or extracted from the considered textwhich contains the semantic pair to be evaluated.3.1 FeaturesFor each pair neighboura/neighbourb, we com-puted a set of features from Wikipedia (the corpusused to derive the distributional similarity): Wefirst computed the frequencies of each item in thecorpus, freqaand freqb, from which we derive?
freqmin, freqmax: the min and max offreqaand freqb;?
freq?
: the combination of the two, orlog(freqa?
freqb)482We also measured the syntagmatic association ofneighbouraand neighbourb, with a mutual infor-mation measure (Church and Hanks, 1990), com-puted from the cooccurrence of two tokens withinthe same paragraph in Wikipedia.
This is a ratherlarge window, and thus gives a good coveragewith respect to the neighbour database (70% of allpairs).A straightforward parameter to include to pre-dict the relevance of a link is of course the simi-larity measure itself, here Lin?s information mea-sure.
But this can be complemented by additionalinformation on the similarity of the neighbours,namely:?
each neighbour productivity : prodaandprodbare defined as the numbers ofneighbours of respectively neighbouraandneighbourbin the database (thus related to-kens with a similarity above the threshold),from which we derive three features as forfrequencies: the min, the max, and the logof the product.
The idea is that neighbourswhith very high productivity give rise to lessreliable relations.?
the ranks of tokens in other related itemsneighbours: ranka?bis defined as the rank ofneighbouraamong neighbours of neighbourbordered with respect to Lin?s score; rangb?ais defined similarly and again we consideras features the min, max and log-product ofthese ranks.We add two categorial features, of a more linguis-tic nature:?
cats is the pair of part-of-speech for the re-lated items, e.g.
to distinguish the relevanceof NN or VV pairs.?
predarg is related to the predicate/argumentdistinction: are the related items predicates orarguments ?The last set of features derive from the occur-rences of related tokens in the considered dis-courses:First, we take into account the frequencies ofitems within the text, with three features as before:the min of the frequencies of the two related items,the max, and the log-product.
Then we consider atf?idf (Salton et al, 1975) measure, to evaluate thespecificity and arguably the importance of a wordFeature Descriptionfreqminmin(freqa, freqb)freqmaxmax(freqa, freqb)freq?log(freqa?
freqb)im im = logP (a,b)P (a)?P (b)lin Lin?s scorerankminmin(ranka?b, rankb?a)rankmaxmax(ranka?b, rankb?a)rank?log(ranka?b?
rankb?a)prodminmin(proda, prodb)prodmaxmax(proda, prodb)prod?log(proda?
prodb)cats neighbour pos pairpredarg predicate or argumentfreqtxtminmin(freqtxta, freqtxtb)freqtxtmaxmax(freqtxta, freqtxtb)freqtxt?log(freqtxta?
freqstxtb)tf?ipf tf?ipf (neighboura)?tf?ipf (neighbourb)coprphcopresence in a sentencecoprparacopresence in a paragraphsd smallest distance betweenneighbouraand neighbourbgd highest distance between neighbouraand neighbourbad average distance between neighbouraand neighbourbprodtxtminmin(proda, prodb)prodtxtmaxmax(proda, prodb)prodtxt?log(proda?
prodb)cc belong to the same lexical connectedcomponentTable 2: Summary of features used in the super-vised model, with respect to two lexical items aand b.
The first group is corpus related, the secondgroup is related to the distributional database, thethird group is related to the textual context.
Freqis related to the frequencies in the corpus, Freqtextthe frequencies in the considered text.483in a document or within a document.
Several vari-ants of tf?idf have been proposed to adapt the mea-sure to more local areas in a text with respect to thewhole document.
For instance (Dias et al, 2007)propose a tf?isf (term frequency ?
inverse sentencefrequency), for topic segmentation.
We similarlydefined a tf?ipf measure based on the frequency ofa word within a paragraph with respect to its fre-quency within the text.
The resulting feature weused is the product of this measure for neighbouraand neighbourb.A few other contextual features are included inthe model: the distances between pairs of relateditems, instantiated as:?
distance in words between occurrences of re-lated word types:?
minimal distance between two occur-rences (sd)?
maximal distance between two occur-rences (gd)?
average distance (ad) ;?
boolean features indicating whetherneighbouraand neighbourbappear inthe same sentence (coprs) or the sameparagraph (coprpara).Finally, we took into account the network of re-lated lexical items, by considering the largest setsof words present in the text and connected in thedatabase (self-connected components), by addingthe following features:?
the degree of each lemma, seen as a nodein this similarity graph, combined as abovein minimal degree of the pair, maximal de-gree, and product of degrees (prodtxtmin,prodtxtmax, prodtxt?).
This is the numberof pairs (present in the text) where a lemmaappears in.?
a boolean feature cc saying whether a lexi-cal pair belongs to a connected component ofthe text, except the largest.
This reflects thefact that a small component may concern alexical field which is more specific and thusmore relevant to the text.Figure 4 shows examples of self-connectedcomponents in an excerpt of the page on Go-rille (gorilla), e.g.
the set {pelage, dos, four-rure} (coat, back, fur).The last feature is probably not entirely indepen-dent from the productivity of an item, or from thetf.ipf measure.Table 2 sums up the features used in our model.3.2 ModelOur task is to identify relevant similarities betweenlexical items, between all possible related pairs,and we want to train an inductive model, a clas-sifier, to extract the relevant links.
We have seenthat the relevant/not relevant classification is veryimbalanced, biased towards the ?not relevant?
cat-egory (about 11%/89%), so we applied methodsdedicated to counter-balance this, and will focuson the precision and recall of the predicted rele-vant links.Following a classical methodology, we made a10-fold cross-validation to evaluate robustly theperformance of the classifiers.
We tested a fewpopular machine learning methods, and report ontwo of them, a naive bayes model and the bestmethod on our dataset, the Random Forest clas-sifier (Breiman, 2001).
Other popular methods(maximum entropy, SVM) have shown slightly in-ferior combined F-score, even though precisionand recall might yield more important variations.As a baseline, we can also consider a simplethreshold on the lexical similarity score, in ourcase Lin?s measure, which we have shown to yieldthe best F-score of 24% when set at 0.22.To address class imbalance, two broad types ofmethods can be applied to help the model focuson the minority class.
The first one is to resam-ple the training data to balance the two classes,the second one is to penalize differently the twoclasses during training when the model makes amistake (a mistake on the minority class beingmade more costly than on the majority class).
Wetested the two strategies, by applying the classicalSmote method of (Chawla et al, 2002) as a kindof resampling, and the ensemble method Meta-Cost of (Domingos, 1999) as a cost-aware learn-ing method.
Smote synthetizes and adds new in-stances similar to the minority class instances andis more efficient than a mere resampling.
Meta-Cost is an interesting meta-learner that can useany classifier as a base classifier.
We used Weka?simplementations of these methods (Frank et al,2004), and our experiments and comparisons arethus easily replicated on our dataset, provided withthis paper, even though they can be improved by484Le gorille est apr`es le bonobo et le chimpanz?e , du point de vue g?en?etique , l?
animal le plus prochede l?
humain .
Cette parent?e a ?et?e confirm?ee par les similitudes entre les chromosomes et les groupessanguins .
Notre g?enome ne diff`ere que de 2 % de celui du gorille .Redress?es , les gorilles atteignent une taille de 1,75 m`etre , mais ils sont en fait un peu plus grands carils ont les genoux fl?echis .
L?
envergure des bras d?epasse la longueur du corps et peut atteindre 2,75m`etres .Il existe une grande diff?erence de masse entre les sexes : les femelles p`esent de 90 `a 150 kilogrammeset les m?ales jusqu?
`a 275.
En captivit?e , particuli`erement bien nourris , ils atteignent 350 kilogrammes.Le pelage d?epend du sexe et de l?
?age .
Chez les m?ales les plus ?ag?es se d?eveloppe sur le dos unefourrure gris argent?e , d?
o`u leur nom de ?dos argent?es?
.
Le pelage des gorilles de montagne estparticuli`erement long et soyeux .Comme tous les anthropodes , les gorilles sont d?epourvus de queue .
Leur anatomie est puissante , levisage et les oreilles sont glabres et ils pr?esentent des torus supra-orbitaires marqu?es .Figure 4: A few connected lexical components of the similarity graph, projected on a text, each in adifferent color.
The groups are, in order of appearance of the first element: {genetic, close, human},{similarity, kinship}, {chromosome, genome}, {male, female}, {coat, back, fur}, {age/N, aged/A},{ear, tail, face}.
The text describes the gorilla species, more particularly its morphology.
Gray words areother lexical elements in the neighbour database.refinements of these techniques.
We chose thefollowing settings for the different models: naivebayes uses a kernel density estimation for numer-ical features, as this generally improves perfor-mance.
For Random Forests, we chose to have tentrees, and each decision is taken on a randomlychosen set of five features.
For resampling, Smoteadvises to double the number of instances of theminority class, and we observed that a bigger re-sampling degrades performances.
For cost-awarelearning, a sensible choice is to invert the class ra-tio for the cost ratio, i.e.
here the cost of a mistakeon a relevant link (false negative) is exactly 8.5times higher than the cost on a non-relevant link(false positive), as non-relevant instances are 8.5times more present than relevant ones.3.3 ResultsWe are interested in the precision and recall forthe ?relevant?
class.
If we take the best simpleclassifier (random forests), the precision and re-call are 68.1% and 24.2% for an F-score of 35.7%,and this is significantly beaten by the Naive Bayesmethod as precision and recall are more even (F-score of 41.5%).
This is already a big improve-ment on the use of the similarity measure alone(24%).
Also note that predicting every link as rel-evant would result in a 2.6% precision, and thus a5% F-score.
The random forest model is signifi-cantly improved by the balancing techniques: theoverall best F-score of 46.3% is reached with Ran-dom Forests and the cost-aware learning method.Table 3 sums up the scores for the different con-figurations, with precision, recall, F-score and theconfidence interval on the F-score.
We analysedthe learning curve by doing a cross-validation onreduced set of instances (from 10% to 90%); F1-scores range from 37.3% with 10% of instancesand stabilize at 80%, with small increment in ev-ery case.The filtering approach we propose seems toyield good results, by augmenting the similaritybuilt on the whole corpus with signals from the lo-cal contexts and documents where related lexicalitems appear together.To try to analyse the role of each set of fea-tures, we repeated the experiment but changed theset of features used during training, and results areshown table 4 for the best method (RF with cost-aware learning).We can see that similarity-related features (mea-sures, ranks) have the biggest impact, but the otherones also seem to play a significant role.
We candraw the tentative conclusion that the quality ofdistributional relations depends on the contextual-izing of the related lexical items, beyond just thesimilarity score and the ranks of items as neigh-bours of other items.485Method Precision Recall F-score CIBaseline (Lin threshold) 24.0 24.0 24.0RF 68.1 24.2 35.7 ?
3.4NB 34.8 51.3 41.5 ?
2.6RF+resampling 56.6 32.0 40.9 ?
3.3NB+resampling 32.8 54.0 40.7 ?
2.5RF+cost aware learning 40.4 54.3 46.3 ?
2.7NB+cost aware learning 27.3 61.5 37.8 ?
2.2Table 3: Classification scores (%) on the relevant class.
CI is the confidence interval on the F-score (RF= Random Forest, NB= naive bayes).Features Prec.
Recall F-scoreall 40.4 54.3 46.3all ?
corpus feat.
37.4 52.8 43.8all ?
similarity feat.
36.1 49.5 41.8all ?
contextual feat.
36.5 54.8 43.8Table 4: Impact of each group of features on the best scores (%) : the lowest the results, the bigger theimpact of the removed group of features.4 Related workOur work is related to two issues: evaluating dis-tributional resources, and improving them.
Eval-uating distributional resources is the subject of alot of methodological reflection (Sahlgren, 2006),and as we said in the introduction, evaluations canbe divided between extrinsic and intrinsic evalua-tions.
In extrinsic evaluations, models are evalu-ated against benchmarks focusing on a single taskor a single aspect of a resource: either discrimina-tive, TOEFL-like tests (Freitag et al, 2005), anal-ogy production (Turney, 2008), or synonym selec-tion (Weeds, 2003; Anguiano et al, 2011; Fer-ret, 2013; Curran and Moens, 2002).
In intrin-sic evaluations, associations norms are used, suchas the 353 word-similarity dataset (Finkelstein etal., 2002), e.g.
(Pado and Lapata, 2007; Agirre etal., 2009), or specifically designed test cases, asin (Baroni and Lenci, 2011).
We differ from allthese evaluation procedures as we do not focus onan essential view of the relatedness of two lexicalitems, but evaluate the link in a context where therelevance of the link is in question, an ?existential?view of semantic relatedness.As for improving distributional thesauri, out-side of numerous alternate approaches to theconstruction, there is a body of work focusingon improving an existing resource, for instancereweighting context features once an initial the-saurus is built (Zhitomirsky-Geffet and Dagan,2009), or post-processing the resource to filter badneighbours or re-ranking neighbours of a giventarget (Ferret, 2013).
They still use ?essential?evaluation measures (mostly synonym extraction),although the latter comes close to our work sinceit also trains a model to detect (intrinsically) badneighbours by using example sentences with thewords to discriminate.
We are not aware of anywork that would try to evaluate differently seman-tic neighbours according to the context they ap-pear in.5 ConclusionWe proposed a method to reliably evaluate distri-butional semantic similarity in a broad sense byconsidering the validation of lexical pairs in con-texts where they both appear.
This helps cover nonclassical semantic relations which are hard to eval-uate with classical resources.
We also presented asupervised learning model which combines globalfeatures from the corpus used to built a distribu-tional thesaurus and local features from the textwhere similarities are to be judged as relevant ornot to the coherence of a document.
It seemsfrom these experiments that the quality of distri-butional relations depends on the contextualizingof the related lexical items, beyond just the simi-486larity score and the ranks of items as neighbours ofother items.
This can hopefully help filter out lex-ical pairs when word lexical similarity is used asan information source where context is important:lexical disambiguation (Miller et al, 2012), topicsegmentation (Guinaudeau et al, 2012).
This canalso be a preprocessing step when looking for sim-ilarities at higher levels, for instance at the sen-tence level (Mihalcea et al, 2006) or other macro-textual level (Agirre et al, 2013), since these arealways aggregation functions of word similarities.There are limits to what is presented here: we needto evaluate the importance of the level of noise inthe distributional neighbours database, or at leastthe quantity of non-semantic relations present, andthis depends on the way the database is built.
Ourstarting corpus is relatively small compared to cur-rent efforts in this framework.
We are confidentthat the same methodology can be followed, eventhough the quantitative results may vary, since itis independent of the particular distributional the-saurus we used, and the way the similarities arecomputed.ReferencesE.
Agirre, E. Alfonseca, K. Hall, J. Kravalova,M.
Pas?ca, and A. Soroa.
2009.
A study on similar-ity and relatedness using distributional and wordnet-based approaches.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 19?27.
Asso-ciation for Computational Linguistics.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*sem 2013 sharedtask: Semantic textual similarity.
In Second JointConference on Lexical and Computational Seman-tics (*SEM), Volume 1: Proceedings of the MainConference and the Shared Task: Semantic TextualSimilarity, pages 32?43, Atlanta, Georgia, USA,June.
Association for Computational Linguistics.E.H.
Anguiano, P. Denis, et al 2011.
FreDist: Au-tomatic construction of distributional thesauri forFrench.
In Actes de la 18eme conf?erence surle traitement automatique des langues naturelles,pages 119?124.M.
Baroni and A. Lenci.
2010.
Distributional mem-ory: A general framework for corpus-based seman-tics.
Computational Linguistics, 36(4):673?721.M.
Baroni and A. Lenci.
2011.
How we BLESSed dis-tributional semantic evaluation.
GEMS 2011, pages1?10.Stefan Bordag.
2008.
A comparison of co-occurrenceand similarity measures as simulations of context.In Alexander F. Gelbukh, editor, CICLing, volume4919 of Lecture Notes in Computer Science, pages52?63.
Springer.D.
Bourigault.
2002.
UPERY : un outild?analyse distributionnelle tendue pour la construc-tion d?ontologies partir de corpus.
In Actes de la9e confrence sur le Traitement Automatique de laLangue Naturelle, pages 75?84, Nancy.Leo Breiman.
2001.
Random forests.
Machine Learn-ing, 45(1):5?32.Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.Hall, and W. Philip Kegelmeyer.
2002.
Smote: Syn-thetic minority over-sampling technique.
J. Artif.
In-tell.
Res.
(JAIR), 16:321?357.Kenneth Church and Patrick Hanks.
1990.
Word as-sociation norms, mutual information, and lexicogra-phy.
Computational Linguistics, 16(1):pp.
22?29.James R. Curran and Marc Moens.
2002.
Improve-ments in automatic thesaurus extraction.
In Pro-ceedings of the ACL-02 Workshop on UnsupervisedLexical Acquisition, pages 59?66.J.R.
Curran.
2004.
From distributional to semanticsimilarity.
Ph.D. thesis, University of Edinburgh.Ga?el Dias, Elsa Alves, and Jos?e Gabriel Pereira Lopes.2007.
Topic segmentation algorithms for text sum-marization and passage retrieval: an exhaustive eval-uation.
In Proceedings of the 22nd national confer-ence on Artificial intelligence - Volume 2, AAAI?07,pages 1334?1339.
AAAI Press.Pedro Domingos.
1999.
Metacost: A general methodfor making classifiers cost-sensitive.
In Usama M.Fayyad, Surajit Chaudhuri, and David Madigan, ed-itors, KDD, pages 155?164.
ACM.Olivier Ferret.
2013.
Identifying bad semantic neigh-bors for improving distributional thesauri.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 561?571, Sofia, Bulgaria, August.Association for Computational Linguistics.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: theconcept revisited.
ACM Trans.
Inf.
Syst., 20(1):116?131.Eibe Frank, Mark Hall, , and Len Trigg.
2004.Weka 3.3: Data mining software in java.www.cs.waikato.ac.nz/ml/weka/.Dayne Freitag, Matthias Blume, John Byrnes, Ed-mond Chow, Sadik Kapadia, Richard Rohwer, andZhiqiang Wang.
2005.
New experiments in distri-butional representations of synonymy.
In Proceed-ings of CoNLL, pages 25?32, Ann Arbor, Michigan,June.
Association for Computational Linguistics.487W.
Gale, K. Church, and D. Yarowsky.
1992.
Onesense per discourse.
In In Proceedings of the 4thDARPA Speech and Natural Language Workshop,New-York, pages 233?237.G.
Grefenstette.
1994.
Explorations in automatic the-saurus discovery.
Kluwer Academic Pub., Boston.Camille Guinaudeau, Guillaume Gravier, and PascaleS?ebillot.
2012.
Enhancing lexical cohesion measurewith confidence measures, semantic relations andlanguage model interpolation for multimedia spo-ken content topic segmentation.
Computer Speech& Language, 26(2):90?104.Karen Sparck Jones.
1994.
Towards better NLP sys-tem evaluation.
In Proceedings of the Human Lan-guage Technology Conference, pages 102?107.
As-sociation for Computational Linguistics.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the 15th InternationalConference on Machine Learning, pages 296?304,Madison.Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In Proceedingsof the 21st national conference on Artificial intel-ligence, AAAI06, volume 1, pages 775?780.
AAAIPress.Tristan Miller, Chris Biemann, Torsten Zesch, andIryna Gurevych.
2012.
Using distributional similar-ity for lexical expansion in knowledge-based wordsense disambiguation.
In Proceedings of COLING2012, pages 1781?1796, Mumbai, India, December.The COLING 2012 Organizing Committee.J.
Morris and G. Hirst.
2004.
Non-classical lexical se-mantic relations.
In Proceedings of the HLT Work-shop on Computational Lexical Semantics, pages46?51, Boston.Sebastian Pado and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Thierry Poibeau and C?edric Messiant.
2008.
Do westill Need Gold Standards for Evaluation?
In Pro-ceedings of the Language Resource and EvaluationConference.Magnus Sahlgren.
2006.
Towards pertinent evalua-tion methodologies for word-space models.
In InProceedings of the 5th International Conference onLanguage Resources and Evaluation.G.
Salton, C. S. Yang, and C. T. Yu.
1975.
A theoryof term importance in automatic text analysis.
Jour-nal of the American Society for Information Science,26(1):33?44.Peter D. Turney.
2008.
A uniform approach to analo-gies, synonyms, antonyms, and associations.
InProceedings of the 22nd International Conferenceon Computational Linguistics - Volume 1, COLING?08, pages 905?912, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.L.
van der Plas.
2008.
Automatic Lexico-Semantic Ac-quisition for Question Answering.
Ph.D. thesis, Uni-versity of Groningen.J.
Weeds and D. Weir.
2005.
Co-occurrence retrieval:A flexible framework for lexical distributional simi-larity.
Computational Linguistics, 31(4):439?475.Julie Elizabeth Weeds.
2003.
Measures and Appli-cations of Lexical Distributional Similarity.
Ph.D.thesis, University of Sussex.Maayan Zhitomirsky-Geffet and Ido Dagan.
2009.Bootstrapping distributional feature vector quality.Computational Linguistics, 35(3):435?461.488
