Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1327?1335,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsRare Word Translation Extraction from Aligned Comparable DocumentsEmmanuel Prochasson and Pascale FungHuman Language Technology CenterHong Kong University of Science and TechnologyClear Water Bay, Kowloon, Hong Kong{eemmanuel,pascale}@ust.hkAbstractWe present a first known result of high pre-cision rare word bilingual extraction fromcomparable corpora, using aligned compara-ble documents and supervised classification.We incorporate two features, a context-vectorsimilarity and a co-occurrence model betweenwords in aligned documents in a machinelearning approach.
We test our hypothesison different pairs of languages and corpora.We obtain very high F-Measure between 80%and 98% for recognizing and extracting cor-rect translations for rare terms (from 1 to 5 oc-currences).
Moreover, we show that our sys-tem can be trained on a pair of languages andtest on a different pair of languages, obtain-ing a F-Measure of 77% for the classificationof Chinese-English translations using a train-ing corpus of Spanish-French.
Our method istherefore even potentially applicable to low re-sources languages without training data.1 IntroductionRare words have long been a challenge to translateautomatically using statistical methods due to theirlow occurrences.
However, the Zipf?s Law claimsthat, for any corpus of natural language text, the fre-quency of a word wn (n being its rank in the fre-quency table) will be roughly twice as high as thefrequency of word wn+1.
The logical consequenceis that in any corpus, there are very few frequentwords and many rare words.We propose a novel approach to extract rare wordtranslations from comparable corpora, relying ontwo main features.The first feature is the context-vector similar-ity (Fung, 2000; Chiao and Zweigenbaum, 2002;Laroche and Langlais, 2010): each word is charac-terized by its context in both source and target cor-pora, words in translation should have similar con-text in both languages.The second feature follows the assumption thatspecific terms and their translations should appeartogether often in documents on the same topic, andrarely in non-related documents.
This is the gen-eral assumption behind early work on bilingual lex-icon extraction from parallel documents using sen-tence boundary as the context window size for co-occurrence computation, we suggest to extend it toaligned comparable documents using document asthe context window.
This document context is toolarge for co-occurrence computation of functionalwords or high frequency content words, but we showthrough observations and experiments that this win-dow size is appropriate for rare words.Both these features are unreliable when the num-ber of occurrences of words are low.
We sug-gest however that they are complementary and canbe used together in a machine learning approach.Moreover, we suggest that the model trained for onepair of languages can be successfully applied to ex-tract translations from another pair of languages.This paper is organized as follows.
In the nextsection, we discuss the challenge of rare lexiconextraction, explaining the reasons why classic ap-proaches on comparable corpora fail at dealing withrare words.
We then discuss in section 3 the con-cept of aligned comparable documents and how weexploited those documents for bilingual lexicon ex-traction in section 4.
We present our resources andimplementation in section 5 then carry out and com-ment several experiments in section 6.13272 The challenge of rare lexicon extractionThere are few previous works focusing on the ex-traction of rare word translations, especially fromcomparable corpora.
One of the earliest works isfrom (Pekar et al, 2006).
They emphasized thefact that the context-vector based approach, used forprocessing comparable corpora, perform quite un-reliably on all but the most frequent words.
In anutshell1, this approach proceeds by gathering thecontext of words in source and target languages in-side context-vectors, then compares source and tar-get context-vectors using similarity measures.
Ina monolingual context, such an approach is usedto automatically get synonymy relationship betweenwords to build thesaurus (Grefenstette, 1994).
In themultilingual case, it is used to extract translations,that is, pairs of words with the same meaning insource and target corpora.
It relies on the Firthienhypothesis that you shall know a word by the com-pany it keeps (Firth, 1957).To show that the frequency of a word influencesits alignment, (Pekar et al, 2006) used six pairs ofcomparable corpora, ranking translations accordingto their frequencies.
The less frequent words areranked around 100-160 by their algorithm, while themost frequent ones typically appear at rank 20-40.We ran a similar experiment using a French-English comparable corpus containing medical doc-uments, all related to the topic of breast cancer,all manually classified as scientific discourse.
TheFrench part contains about 530,000 words while theEnglish part contains about 7.4 millions words.
Forthis experiment though, we sampled the English partto obtain a 530,000-words large corpus, matchingthe size of the French part.Using an implementation of the context-vectorsimilarity, we show in figure 1 that frequent words(above 400 occurrences in the corpus) reach a 60%precision whereas rare words (below 15 occur-rences) are correctly aligned in only 5% of the time.These results can be explained by the fact that, forthe vector comparison to be efficient, the informa-tion they store has to be relevant and discriminatory.If there are not enough occurrences of a word, it is1Detailed presentations can be found for example in (Fung,2000; Chiao and Zweigenbaum, 2002; Laroche and Langlais,2010).Figure 1: Results for context-vector based translationsextraction with respect to word frequency.
The verticalaxis is the amount of correct translations found for Top1,and the horizontal axis is the word occurrences in the cor-pus.impossible to get a precise description of the typicalcontext of this word, and therefore its descriptionis likely to be very different for source and targetwords in translation.We confirmed this result with another observa-tion on the full English part of the previous cor-pus, randomly split in 14 samples of the same size.The context-vectors for very frequent words, suchas cancer (between 3,000 and 4,000 occurrences ineach sample) are very similar across the subsets.Less frequent words, such as abnormality (between70 and 16 occurrences in each sample) have veryunstable context-vectors, hence a lower similarityacross the subsets.
This observation actually indi-cates that it will be difficult to align abnormalitywith itself.3 Aligned comparable documentsA pair of aligned comparable documents is a par-ticular case of comparable corpus: two compara-ble documents share the same topic and domain;they both relate the same information but are notmutual translations; although they might share par-allel chunks (Munteanu and Marcu, 2005) ?
para-graphs, sentences or phrases ?
in the general casethey were written independently.
These compara-ble documents, when concatenated together in order,form an aligned comparable corpus.1328Examples of such aligned documents can befound, for example in (Munteanu and Marcu, 2005):they aligned comparable documents with close pub-lication dates.
(Tao and Zhai, 2005) used an iter-ative, bootstrapping approach to align comparabledocuments using examples of already aligned cor-pora.
(Smith et al, 2010) aligned documents fromWikipedia following the interlingual links providedon articles.We take advantage of this alignment between doc-uments: by looking at what is common betweentwo aligned documents and what is different inother documents, we obtain more precise informa-tion about terms than when using a larger compa-rable corpus without alignment.
This is especiallyinteresting in the case of rare lexicon as the clas-sic context-vector similarity is not discriminatoryenough and fails at raising interesting translation forrare words.4 Rare word translations from alignedcomparable documents4.1 Co-occurrence modelDifferent approaches have been proposed for bilin-gual lexicon extraction from parallel corpora, rely-ing on the assumption that a word has one sense, onetranslation, no missing translation, and that its trans-lation appears in aligned parallel sentences (Fung,2000).
Therefore, translations can be extracted bycomparing the distribution of words across the sen-tences.
For example, (Gale and Church, 1991) useda derivative of the ?2 statistics to evaluate the as-sociation between words in aligned region of paral-lel documents.
Such association scores evaluate thestrength of the relation between events.
In the caseof parallel sentences and lexicon extraction, theymeasure how often two words appear in aligned sen-tences, and how often one appears without the other.More precisely, they will compare their number ofco-occurrences against the expected number of co-occurrences under the null-hypothesis that words arerandomly distributed.
If they appear together moreoften than expected, they are considered as associ-ated (Evert, 2008).We focus in this work on rare words, more pre-cisely on specialized terminology.
We define themas the set of terms that appear from 1 (hapaxes)to 5 times.
We use a strategy similar to the oneapplied on parallel sentences, but rely on aligneddocuments.
Our hypothesis is very similar: wordsin translation should appear in aligned comparabledocuments.
We used the Jaccard similarity (eq.
1)to evaluate the association between words amongaligned comparable documents.
In the general case,this measure would not give relevant scores due tofrequency issue: it produces the same scores fortwo words that appear always together, and neverone without the other, disregarding the fact that theyappear 500 times or one time only.
Other associ-ation scores generally rely on occurrence and co-occurrence counts to tackle this issue (such as thelog-likelihood, eq.
2).
In our case, the number ofco-occurrences will be limited by the number of oc-currences of the words, from 1 to 5.
Therefore, theJaccard similarity efficiently reflects what we wantto observe.J(wi, wj) =|Ai ?Aj ||Ai ?Aj |;Ai = {d : wi ?
d} (1)A score of 1 indicates a perfect association(words always appear together, never one withoutthe other), the more one word appears without theother, the lower the score.4.2 Context-vector similarityWe implemented the context-vector similarity in away similar to (Morin et al, 2007).
In all experi-ments, we used the same set of parameters, as theyyielded the best results on our corpora.
We built thecontext-vectors using nouns only as seed lexicon,with a window size of 20.
Source context-vectorsare translated in the target language using the re-sources presented in the next section.
We used thelog-likelihood (Dunning, 1993, eq.
2) for context-vector normalization (O is the observed number ofco-occurrence in the corpus, E is the expected num-ber of co-occurrences under the null hypothesis).We used the Cosine similarity (eq.
3) for context-vector comparisons.ll(wi, wj) = 2?ijOijlogOijEij(2)Cosine(A,B) = A ?B?A?2 + ?B?2 ?A ?B (3)13294.3 Binary classification of rare translationsWe suggest to incorporate both the context-vectorsimilarity and the co-occurrence features in a ma-chine learning approach.
This approach consists oftraining a classifier on positive examples of transla-tion pairs, and negative examples of non-translationspairs.
The trained model (in our case, a decisiontree) is then used to tag an unknown pair of words aseither ?Translation?
or ?Non-Translation?.One potential problem for building the trainingset, as pointed out for example by (Zhao and Ng,2007) is this: we have a limited number of pos-itive examples, but a very large amount of non-translation examples as obviously is the case forrare word translations in any training corpus.
In-cluding two many negative examples in the trainingset would lead the classifier to label every pairs as?Non-Translation?.To tackle this problem, (Zhao and Ng, 2007)tuned the imbalance of positive/negative ratio by re-sampling the positive examples in the training set.We chose to reduce the set of negative examples,and found that a ratio of five negative examples toone positive is optimal in our case.
A lower ratioimproves precision but reduces recall for the ?Trans-lation?
class.It is also desirable that the classifier focuses ondiscriminating between confusing pairs of transla-tions.
As most of the negative examples have anull co-occurrence score and a null context-vectorsimilarity, they are excluded from the training set.The negative examples are randomly chosen amongthose that fulfill the following constraints:?
non-null features ;?
ratio of number of occurrences betweensource/target words higher than 0.2 and lowerthan 5.We use the J48 decision tree algorithm, in theWeka environment (Hall et al, 2009).
Features arecomputed using the Jaccard similarity (section 3)for the co-occurrence model, and the implementa-tion of the context-vector similarity presented in sec-tion 4.2.4.4 Extension to another pair of languagesEven though the context vector similarity has beenshown to achieve different accuracy depending onthe pair of languages involved, the co-occurrencemodel is totally language independent.
In the case ofbinary classification of translations, the two modelsare complementary to each other: word pairs withnull co-occurrence are not considered by the contextmodel while the context vector model gives more se-mantic information than the co-occurrence model.For these reasons, we suggest that it is possibleto use a decision tree trained on one pair of lan-guages to extract translations from another pair oflanguages.
A similar approach is proposed in (Al-fonseca et al, 2008): they present a word decom-position model designed for German language thatthey successfully applied to other compounding lan-guages.
Our approach consists in training a decisiontree on a pair of languages and applying this modelto the classification of unknown pairs of words inanother pair of languages.
Such an approach is es-pecially useful for prospecting new translations fromless known languages, using a well known languageas training.We used the same algorithms and same features asin the previous sections, but used the data computedfrom one pair of languages as the training set, andthe data computed from another pair of languages asthe testing set.5 Experimental setup5.1 CorporaWe built several corpora using two different strate-gies.
The first set was built using Wikipedia and theinterlingual links available on articles (that pointsto another version of the same article in anotherlanguage).
We started from the list of all Frencharticles2 and randomly selected articles that pro-vide a link to Spanish and English versions.
Wedownloaded those, and clean them by removing thewikipedia formatting tags to obtain raw UTF8 texts.Articles were not selected based on their sizes, thevocabulary used, nor a particular topic.
We obtainedabout 20,000 aligned documents for each language.A second set was built using an in-house system2Available on http://download.wikimedia.org/.1330[WP] French [WP] English [WP] Es [CLIR] En [CLIR] Zh#documents 20,169 20,169 20,169 15,3247 15,3247#tokens 4,008,284 5,470,661 2,741,789 1,334,071 1,228,330#unique tokens 120,238 128,831 103,398 30,984 60,015Table 1: Statistics for all parts of all corpora.
(unpublished) that seeks for comparable and paral-lel documents from the web.
Starting from a list ofChinese documents (in this case, mostly news arti-cles), we automatically selected English target docu-ments using Cross Language Information Retrieval.About 85% of the paired documents obtained are di-rect translations (header/footer of web pages apart).However, they will be processed just like alignedcomparable documents, that is, we will not take ad-vantage of the structure of the parallel contents toimprove accuracy, but will use the exact same ap-proach that we applied for the Wikipedia documents.We gathered about 15,000 pairs of documents em-ploying this method.All corpora were processed using Tree-Tagger3for segmentation and Part-of-Speech tagging.
Wefocused on nouns only and discarded all other to-kens.
We would record the lemmatized form oftokens when available, otherwise we would recordthe original form.
Table 1 summarizes main statis-tics for each corpus; [WP] refers to the Wikipediacorpora, [CLIR] to the Chinese-English corpora ex-tracted through cross language information retrieval.5.2 DictionariesWe need a bilingual seed lexicon for the context-vector similarity.
We used a French-English lex-icon obtained from the Web.
It contains about67,000 entries.
The Spanish-English and Spanish-French dictionaries were extracted from the linguis-tic resources of the Apertium project4.
We ob-tained approximately 22,500 Spanish-English trans-lations and 12,000 for Spanish-French.
Finally, forChinese-English we used the LDC2002L27 resourcefrom the Linguistic Data Consortium5 with about122,000 entries.3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html4http://www.apertium.org5http://www.ldc.upenn.edu5.3 Evaluation listsTo evaluate our approach, we needed evaluation listsof terms for which translations are already known.We used the Medical Subject Headlines, from theUMLS meta-thesaurus6 which provides a lexicon ofspecialized, medical terminology, notably in Span-ish, English and French.
We used the LDC lexi-con presented in the previous section for Chinese-English.From these resources, we selected all the sourcewords that appears from 1 to 5 times in the corporain order to build the evaluation lists.5.4 Oracle translationsWe looked at the corpora to evaluate how manytranslation pairs from the evaluation lists can befound across the aligned comparable documents.Those translations are hereafter the oracle transla-tions.
For French/English, French/Spanish and En-glish/Spanish, about 60% of the translation pairs canbe found.
For Chinese/English, this ratio reaches45%.
The main reason for this lower result is theinaccuracy of the segmentation tool used to processChinese.
Segmentation tools usually rely on a train-ing corpus and typically fail at handling rare wordswhich, by definition, were unlikely to be found in thetraining examples.
Therefore, some rare Chinese to-kens found in our corpus are the results of faulty seg-mentation, and the translation of those faulty wordscan not be found in related documents.
We encoun-tered the same issue but at a much lower degree forother languages because of spelling mistakes and/orimproper Part-of-Speech tagging.6 ExperimentsWe ran three different experiments.
Experiment Icompares the accuracy of the context-vector sim-ilarity and the co-occurrence model.
ExperimentII uses supervised classification with both features.6http://www.nlm.nih.gov/research/umls/1331Figure 2: Experiment I: comparison of accuracy obtained for the Top10 with the context-vector similarity and theco-occurrence model, for hapaxes (left) and words that appear 2 to 5 times (right).Experiment III extracts translation from a pair oflanguages, using a classifier trained on another pairof languages.6.1 Experiment I: co-occurrence model vs.context-vector similarityWe split the French-English part of the Wikipediacorpus into different samples: the first sample con-tains 500 pairs of documents.
We then aggregatedmore documents to this initial sample to test differ-ent sizes of corpora.
We built the sample in order toensure hapaxes in the whole corpus are hapaxes inall subsets.
That is, we ensured the 431 hapaxes inthe evaluation lists are represented in the 500 docu-ments subset.We extracted translations in two different ways:1. using the co-occurrence model;2. using the context-vector based approach, withthe same evaluation lists.The accuracy is computed on 1,000 pairs of trans-lations from the set of oracle translations, and mea-sures the amount of correct translations found for the10 best ranks (Top10) after ranking the candidatesaccording to their score (context-vector similarity orco-occurrence model).
The results are presented infigure 2.We can draw two conclusions out of these results.First, the size of the corpus influences the qualityof the bilingual lexicon extraction when using theco-occurrence model.
This is especially interestingwith hapaxes, for which frequency does not changewith the increase of the size of the corpora.
The ac-curacy is improved by adding more information tothe corpus, even if this additional information doesnot cover the pairs of translations we are looking for.The added documents will weaken the associationof incorrect translations, without changing the as-sociation for rare terms translations.
For example,the precision for hapaxes using the co-occurrencemodel ranges from less than 1% when using only500 pairs of documents, to about 13% when usingall documents.
The second conclusion is that theco-occurrence model outperforms the context-vectorsimilarity.However, both these approaches still performpoorly.
In the next experiment, we propose to com-bine them using supervised classification.6.2 Experiment II: binary classification oftranslationFor each corpus or combination of corpora ?English-Spanish, English-French, Spanish-Frenchand Chinese-English, we ran three experiments, us-ing the following features for supervised learning oftranslations:?
the context-vector similarity;?
the co-occurrence model;?
both features together.The parameters are discussed in section 4.3.
Weused all the oracle translations to train the positivevalues.
Results are presented in table 2, they arecomputed using a 10-folds cross validation.
ClassT refers to ?Translation?, ?T to ?Non-Translation?.The evaluation of precision/recall/F-Measure for theclass ?Translation?
are given in equation 4 to 6.1332Precision Recall F-Measure Cl.English-Spanishcontext- 0.0% 0.0% 0.0% Tvectors 83.3% 99.9% 90.8% ?Tco-occ.
66.2% 44.2% 53.0% Tmodel 89.5% 95.5% 92.4% ?Tboth 98.6% 88.6% 93.4% T97.8% 99.8% 98.7% ?TFrench-Englishcontext- 76.5% 10.3% 18.1% Tvectors 90.9% 99.6% 95.1% ?Tco-occ.
85.7% 1.2% 2.4% Tmodel 90.1% 100% 94.8% ?Tboth 81.0% 80.2% 80.6% T94.9% 98.7% 96.8% ?TFrench-Spanishcontext- 0.0% 0.0% 0.0% Tvectors 81.0% 100% 89.5% ?Tco-occ.
64.2% 46.5% 53.9% Tmodel 88.2% 93.9% 91.0% ?Tboth 98.7% 94.6% 96.7% T98.8% 99.7% 99.2% ?TChinese-Englishcontext- 69.6% 13.3% 22.3% Tvectors 91.0% 93.1% 92.1% ?Tco-occ.
73.8% 32.5% 45.1% Tmodel 85.2% 97.1% 90.8% ?Tboth 86.7% 74.7% 80.3% T96.3% 98.3% 97.3% ?TTable 2: Experiment II: results of binary classification for?Translation?
and ?Non-Translation?.precisionT =|T ?
oracle||T | (4)recallT =|T ?
oracle||oracle| (5)FMeasure = 2?
precision?
recallprecision+ recall (6)These results show first that one feature is gen-erally not discriminatory enough to discern correcttranslation and non-translation pairs.
For examplewith Spanish-English, by using context-vector sim-ilarity only, we obtained very high recall/precisionfor the classification of ?Non-Translation?, but nullprecision/recall for the classification of ?Transla-tion?.
In some other cases, we obtained high pre-cision but poor recall with one feature only, which isnot a usefully result as well since most of the correcttranslations are still labeled as ?Non-Translation?.However, when using both features, the precisionis strongly improved up to 98% (English-Spanishor French-Spanish) with a high recall of about 90%for class T. We also achieved about 86%/75% pre-cision/recall in the case of Chinese-English, eventhough they are very distant languages.
This last re-sult is also very promising since it has been obtainedfrom a fully automatically built corpus.
Table 3shows some examples of correctly labeled ?Trans-lation?.The decision trees obtained indicate that, in gen-eral, word pairs with very high co-occurrence modelscores are translations, and that the context-vectorsimilarity disambiguate candidates with lower co-occurrence model scores.
Interestingly, the traineddecision trees are very similar between the differentpairs of languages, which inspired the next experi-ment.6.3 Experiment III: extension to another pairof languagesIn the last experiment, we focused on using theknowledge acquired with a given pair of languagesto recognize proper translation pairs using a dif-ferent pair of languages.
For this experiment, weused the data from one corpus to train the classifier,and used the data from another combination of lan-guages as the test set.
Results are displayed in ta-ble 4.These last results are of great interest becausethey show that translation pairs can be correctlyclassified even with a classifier trained on anotherpair of languages.
This is very promising be-cause it allows one to prospect new languages usingknowledge acquired on a known pairs of languages.As an example, we reached a 77% F-Measure forChinese-English alignment using a classifier trainedon Spanish-French features.
This not only confirmsthe precision/recall of our approach in general, butalso shows that the model obtained by training tendsto be very stable and accurate across different pairsof languages and different corpora.1333Tested withTrained with Sp-En Sp-Fr Fr-En Zh-EnSp-En 98.6/88.8/93.5 98.7/94.9/96.8 91.5/48.3/63.2 99.3/63.0/77.1Sp-Fr 89.5/77.9/83.9 90.4/82.9/86.5 75.4/53.5/62.6 98.7/63.3/77.1Fr-En 89.5/77.9/83.9 90.4/82.9/86.5 85.2/80.0/82.6 81.0/87.6/84.2Zh-En 96.6/89.2/92.7 97.7/94.9/96.3 81.1/50.9/62.5 97.4/65.1/78.1Table 4: Experiment III: Precision/Recall/F-Measure for label ?Translation?, obtained for all training/testing set com-binations.English Frenchmyometrium myome`trelysergide lysergidehyoscyamus jusquiamelysichiton lysichitonbrassicaceae brassicace?esyarrow achille?espikemoss se?laginelleleiomyoma fibromyomeryegrass ivraieEnglish Spanishspirometry espirometr?
?alolium loliumomentum epiplo?npilocarpine pilocarpinachickenpox varicelabruxism bruxismopsittaciformes psittaciformescommodification mercantilizacio?ntalus astra?galoEnglish Chinesehooliganism ?
?kindergarten ??
?oyster ?
?fascism ????
?taxonomy ??
?mongolian ??
?subpoena ?
?rupee ?
?archbishop ??
?serfdom ?
?typhoid ?
?Table 3: Experiment II and III: examples of rare wordtranslations found by our algorithm.
Note that eventhough some words such as ?kindergarten?
are not rarein general, they occur with very low frequency in the testcorpus.7 ConclusionWe presented a new approach for extracting transla-tions of rare words among aligned comparable doc-uments.
To the best of our knowledge, this is oneof the first high accuracy extraction of rare lexi-con from non-parallel documents.
We obtained a F-Measure ranging from about 80% (French-English,Chinese-English) to 97% (French-Spanish).
We alsoobtained good results for extracting lexicon for apair of languages, using a decision tree trained withthe data computed on another pair of languages.We yielded a 77% F-Measure for the extraction ofChinese-English lexicon, using Spanish-French fortraining the model.On top of these promising results, our approachpresents several other advantages.
First, we showedthat it works well on automatically built corporawhich require minimal human intervention.
Alignedcomparable documents can easily be collected andare available in large volumes.
Moreover, the pro-posed machine learning method incorporating bothcontext-vector and co-occurrence model has shownto give good results on pairs of languages that arevery different from each other, such as Chinese-English.
It is also applicable across different train-ing and testing language pairs, making it possiblefor us to find rare word translations even for lan-guages without training data.
The co-occurrencemodel is completely language independent and havebeen shown to give good results on various pairs oflanguages, including Chinese-English.AcknowledgmentsThe authors would like to thank Emmanuel Morin(LINA CNRS 6241) for providing us the compa-rable corpus used for the experiment in section 2,Simon Shi for extracting and providing the corpus1334described in section 5.1, and the anonymous re-viewers for their valuable comments.
This researchis partly supported by ITS/189/09 AND BBNX02-20F00310/11PN.ReferencesEnrique Alfonseca, Slaven Bilac, and Stefan Pharies.2008.
Decompounding query keywords from com-pounding languages.
In Proceedings of the 46th An-nual Meeting of the Association for ComputationalLinguistics (ACL?08), pages 253?256.Yun-Chuang Chiao and Pierre Zweigenbaum.
2002.Looking for candidate translational equivalents in spe-cialized, comparable corpora.
In Proceedings of the19th International Conference on Computational Lin-guistics (COLING?02), pages 1208?1212.Ted Dunning.
1993.
Accurate Methods for the Statisticsof Surprise and Coincidence.
Computational Linguis-tics, 19(1):61?74.Stefan Evert.
2008.
Corpora and collocations.
InA.
Ludeling and M. Kyto, editors, Corpus Linguis-tics.
An International Handbook, chapter 58.
Moutonde Gruyter, Berlin.John Firth.
1957.
A synopsis of linguistic theory 1930-1955.
Studies in Linguistic Analysis, Philological.Longman.Pascale Fung.
2000.
A statistical view on bilingual lex-icon extraction?from parallel corpora to non-parallelcorpora.
In Jean Ve?ronis, editor, Parallel Text Pro-cessing, page 428.
Kluwer Academic Publishers.William A. Gale and Kenneth W. Church.
1991.
Iden-tifying word correspondence in parallel texts.
InProceedings of the workshop on Speech and NaturalLanguage, HLT?91, pages 152?157, Morristown, NJ,USA.
Association for Computational Linguistics.Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publisher.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.SIGKDD Explorations, 11.Audrey Laroche and Philippe Langlais.
2010.
Revisitingcontext-based projection methods for term-translationspotting in comparable corpora.
In 23rd Interna-tional Conference on Computational Linguistics (Col-ing 2010), pages 617?625, Beijing, China, Aug.Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, andKyo Kageura.
2007.
Bilingual Terminology Mining ?Using Brain, not brawn comparable corpora.
In Pro-ceedings of the 45th Annual Meeting of the Associationfor Computational Linguistics (ACL?07), pages 664?671, Prague, Czech Republic.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving Machine Translation Performance by Exploit-ing Non-Parallel Corpora.
Computational Linguistics,31(4):477?504.Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and An-drea Mulloni.
2006.
Finding translations for low-frequency words in comparable corpora.
MachineTranslation, 20(4):247?266.Jason R. Smith, Chris Quirk, and Kristina Toutanova.2010.
Extracting parallel sentences from comparablecorpora using document level alignment.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the ACL, pages 403?411.Tao Tao and ChengXiang Zhai.
2005.
Mining compa-rable bilingual text corpora for cross-language infor-mation integration.
In KDD ?05: Proceedings of theeleventh ACM SIGKDD international conference onKnowledge discovery in data mining, pages 691?696,New York, NY, USA.
ACM.Shanheng Zhao and Hwee Tou Ng.
2007.
Identifi-cation and resolution of Chinese zero pronouns: Amachine learning approach.
In Proceedings of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), Prague, CzechRepublic.1335
