Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 850?857,Sydney, July 2006. c?2006 Association for Computational LinguisticsTrimming CFG Parse Trees for Sentence Compression Using MachineLearning ApproachesYuya Unno1 Takashi Ninomiya2 Yusuke Miyao1 Jun?ichi Tsujii1341Department of Computer Science, University of Tokyo2Information Technology Center, University of Tokyo3School of Informatics, University of Manchester4SORST, JSTHongo 7-3-1, Bunkyo-ku, Tokyo, Japan{unno, yusuke, tsujii}@is.s.u-tokyo.ac.jpninomi@r.dl.itc.u-tokyo.ac.jpAbstractSentence compression is a task of creatinga short grammatical sentence by removingextraneous words or phrases from an origi-nal sentence while preserving its meaning.Existing methods learn statistics on trim-ming context-free grammar (CFG) rules.However, these methods sometimes elim-inate the original meaning by incorrectlyremoving important parts of sentences, be-cause trimming probabilities only dependon parents?
and daughters?
non-terminalsin applied CFG rules.
We apply a maxi-mum entropy model to the above method.Our method can easily include variousfeatures, for example, other parts of aparse tree or words the sentences contain.We evaluated the method using manuallycompressed sentences and human judg-ments.
We found that our method pro-duced more grammatical and informativecompressed sentences than other methods.1 IntroductionIn most automatic summarization approaches, textis summarized by extracting sentences from agiven document without modifying the sentencesthemselves.
Although these methods have beensignificantly improved to extract good sentencesas summaries, they are not intended to shorten sen-tences; i.e., the output often has redundant wordsor phrases.
These methods cannot be used tomake a shorter sentence from an input sentence orfor other applications such as generating headlinenews (Dorr et al, 2003) or messages for the smallscreens of mobile devices.
We need to compresssentences to obtain short and useful summaries.This task is called sentence compression.While several methods have been proposed forsentence compression (Witbrock and Mittal, 1999;Jing and McKeown, 1999; Vandeghinste and Pan,2004), this paper focuses on Knight and Marcu?snoisy-channel model (Knight and Marcu, 2000)and presents an extension of their method.
Theydeveloped a probabilistic model for trimming aCFG parse tree of an input sentence.
Theirmethod drops words of input sentences but doesnot change their order or change the words.
Theyuse a parallel corpus that contains pairs of origi-nal and compressed sentences.
The method makesCFG parse trees of both original and compressedsentences and learns trimming probabilities fromthese pairs.
Although their method is concise andwell-defined, its accuracy is still unsatisfactory.Their method has two problems.
One is that prob-abilities are calculated only from the frequenciesof applied CFG rules, and other characteristics likewhether the phrase includes negative words cannotbe introduced.
The other problem is that the parsetrees of original and compressed sentences some-times do not correspond.To solve the former problem, we apply a maxi-mum entropy model to Knight and Marcu?s modelto introduce machine learning features that are de-fined not only for CFG rules but also for othercharacteristics in a parse tree, such as the depthfrom the root node or words it contains.
To solvethe latter problem, we introduce a novel matchingmethod, the bottom-up method, to learn compli-cated relations of two unmatched trees.We evaluated each algorithm using the Ziff-Davis corpus, which has long and short sentencepairs.
We compared our method with Knight andMarcu?s method in terms of F -measures, bigramF -measures, BLEU scores and human judgments.8502 Background2.1 The Noisy-Channel Model for SentenceCompressionKnight and Marcu proposed a sentence compres-sion method using a noisy-channel model (Knightand Marcu, 2000).
This model assumes that a longsentence was originally a short one and that thelonger sentence was generated because some un-necessary words were added.
Given a long sen-tence l, it finds a short sentence s that maximizesP (s|l).
This is equivalent to finding the s thatmaximizes P (s) ?
P (l|s) in Bayes?
Rule.The expression P (s) is the source model, whichgives the probability that s is the original shortstring.
When s is ungrammatical, P (s) becomessmall.
The expression P (l|s) is the channelmodel, which gives the probability that s is ex-panded to l. When s does not include importantwords of l, P (l|s) has a low value.In the Knight and Marcu?s model, a proba-bilistic context-free grammar (PCFG) score and aword-bigram score are incorporated as the sourcemodel.
To estimate the channel model, Knightand Marcu used the Ziff-Davis parallel corpus,which contains long sentences and correspondingshort sentences compressed by humans.
Note thateach compressed sentence is a subsequence of thecorresponding original sentence.
They first parseboth the original and compressed sentences usinga CFG parser to create parse trees.
When twonodes of the original and compressed trees havethe same non-terminals, and the daughter nodes ofthe compressed tree are a subsequence of the orig-inal tree, they count the node pair as a joint event.For example, in Figure 1, the original parse treecontains a rule rl = (B ?
D E F ), and the com-pressed parse tree contains rs = (B ?
D F ).They assume that rs was expanded into rl, andcount the node pairs as joint events.
The expan-sion probability of two rules is given by:Pexpand (rl|rs) =count(joint(rl, rs))count(rs).Finally, new subtrees grow from new daugh-ter nodes in each expanded node.
In Figure 1,(E (G g) (H h)) grows from E. The PCFGscores, Pcfg , of these subtrees are calculated.Then, each probability is assumed to be indepen-dent of the others, and the channel model, P (l|s),is calculated as the product of all expansion prob-abilities of joint events and PCFG scores of newAB CE FDdg hfcAB CFDd fcG HFigure 1: Examples of original and compressedparse trees.subtrees:P (l|s) =?
(rl,rs)?RPexpand (rl|rs) ?
?r?R?Pcfg(r),where R is the set of rule pairs, and R?
is the setof generation rules in new subtrees.To compress an input sentence, they create atree with the highest score of all possible trees.They pack all possible trees in a shared-foreststructure (Langkilde, 2000).
The forest structureis represented by an AND-OR tree, and it con-tains many tree structures.
The forest represen-tation saves memory and makes calculation fasterbecause the trees share sub structures, and this canreduce the total number of calculations.They normalize each log probability using thelength of the compressed sentence; that is, they di-vide the log probability by the length of the com-pressed sentence.Turner and Charniak (Turner and Charniak,2005) added some special rules and applied thismethod to unsupervised learning to overcome thelack of training data.
However their model alsohas the same problem.
McDonald (McDonald,2006) independently proposed a new machinelearning approach.
He does not trim input parsetrees but uses rich features about syntactic treesand improved performance.2.2 Maximum Entropy ModelThe maximum entropy model (Berger et al, 1996)estimates a probability distribution from trainingdata.
The model creates the most ?uniform?
distri-bution within the constraints given by users.
Thedistribution with the maximum entropy is consid-ered the most uniform.Given two finite sets of event variables, X andY , we estimate their joint probability distribution,P (x, y).
An output, y (?
Y), is produced, and851contextual information, x (?
X ), is observed.
Torepresent whether the event (x, y) satisfies a cer-tain feature, we introduce a feature function.
Afeature function fi returns 1 iff the event (x, y) sat-isfies the feature i and returns 0 otherwise.Given training data {(x1, y1), ?
?
?
, (xn, yn)},we assume that the expectation of fi on the dis-tribution of the model conforms to that on the em-pirical probability distribution P?
(x, y).
We selectthe probability distribution that satisfies these con-straints of all feature functions and maximizes itsentropy, H(P ) = ?
?x,y P (x, y) log (P (x, y)).3 Methods3.1 Maximum Entropy Model for SentenceCompressionWe describe a maximum entropy method as anatural extension of Knight and Marcu?s noisy-channel model (Knight and Marcu, 2000).
Knightand Marcu?s method uses only mother and daugh-ter local relations in CFG parse trees.
Therefore,it sometimes eliminates the meanings of the origi-nal sentences.
For example, their method cannotdistinguish ?never?
and ?always?
because thesetwo adverbs are assigned the same non-terminalsin parse trees.
However, if ?never?
is removedfrom a sentence, the meaning of the sentence com-pletely changes.
Turner and Charniak (Turner andCharniak, 2005) revised and improved Knight andMarcu?s algorithm; however, their algorithm alsouses only mother and daughter relations and hasthe same problem.
We use other information asfeature functions of the maximum entropy model,and this model can deal with many features moreappropriately than using simple frequency.Suppose that we trim a node in the original fullparse tree.
For example, suppose we have a mothernode A and daughter nodes (B C D) that are de-rived using a CFG rule.
We must leave at least onenon-terminal in the daughter nodes.
The trim can-didates of this rule are the members of the set ofsubsequences, Y , of (B C D), or the seven non-terminal sequences below:Y = {B,C,D,BC,BD,CD,BCD}.For each y (?
Y), such as (B C), the trimmingprobability, P (y|Y) = Ptrim(A ?
B C|A ?B C D), is calculated by using the maximum en-tropy model.
We assume that these joint events areindependent of each other and calculate the proba-bility that an original sentence, l, is compressed toDescription1 the mother node2 the current node3 the daughter node sequence in the original sentenceand which daughters are removed4 the daughter node sequence in the compressed sen-tence5 the number of daughter nodes6 the depth from the root7 the daughter non-terminals that are removed8 the daughter terminals that are removed9 whether the daughters are ?negative adverbs?, andremoved10 tri-gram of daughter nodes11 only one daughter exists, and its non-terminal is thesame as that of the current node12 only one daughter exists, and its non-terminal is thesame as that of the mother node13 how many daughter nodes are removed14 the number of terminals the current node contains15 whether the head daughter is removed16 the left-most and the right-most daughters17 the left and the right siblingsTable 1: Features for maximum entropy model.s as the product of all trimming probabilities, likein Knight and Marcu?s method.P (s|l) =?
(rs,rl)?RPtrim(rs|rl),where R is the set of compressed and original rulepairs in joint events.
Note that our model does notuse Bayes?
Rule or any language models.For example, in Figure 1, the trimming proba-bility is calculated as below:P (s|l) = Ptrim(A ?
B C|A ?
B C)?Ptrim(B ?
D F |B ?
D E F ).To represent all summary candidates, we cre-ate a compression forest as Knight and Marcu did.We select the tree assigned the highest probabilityfrom the forest.Features in the maximum entropy model are de-fined for a tree node and its surroundings.
Whenwe process one node, or one non-terminal x, wecall it the current node.
We focus on not only xand its daughter nodes, but its mother node, itssibling nodes, terminals of its subtree and so on.The features we used are listed in Table 1.Knight and Marcu divided the log probabilitiesby the length of the summary.
We extend this ideaso that we can change the output length flexibly.We introduce a length parameter, ?, and define ascore S?
as S?
(s) = length(s)?
log P (s|l), wherel is an input sentence to be shortened, and s is a852summary candidate.
Because log P (s|l) is nega-tive, short sentences obtain a high score for large?, and long ones get a low score.
The parameter?
can be negative or positive, and we can use it tocontrol the average length of outputs.3.2 Bottom-Up MethodAs explained in Section 2.1, in Knight andMarcu?s method, both original and compressedsentences are parsed, and correspondences of CFGrules are identified.
However, when the daugh-ter nodes of a compressed rule are not a subse-quence of the daughter nodes in the original one,the method cannot learn this joint event.
A com-plex sentence is a typical example.
A complexsentence is a sentence that includes another sen-tence as a part.
An example of a parse tree of acomplex sentence and its compressed version isshown in Figure 2.
When we extract joint eventsfrom these two trees, we cannot match the tworoot nodes because the sequence of the daughternodes of the root node of the compressed parsetree, (NP ADVP VP .
), is not a subsequenceof the daughter nodes of the original parse tree,(S , NP VP .).
Turner and Charniak (Turner andCharniak, 2005) solve this problem by appendingspecial rules that are applied when a mother nodeand its daughter node have the same label.
How-ever, there are several types of such problems likeFigure 2.
We need to extract these structures froma training corpus.We propose a bottom-up method to solve theproblem explained above.
In our method, onlyoriginal sentences are parsed, and the parse treesof compressed sentences are extracted from theoriginal parse trees.
An example of this methodis shown in Figure 3.
The original sentence is ?dg h f c?, and its compressed sentence is ?d g c?.First, each terminal in the parse tree of the originalsentence is marked if it exists in the compressedsentence.
In the figure, the marked terminals arerepresented by circles.
Second, each non-terminalin the original parse tree is marked if it has at leastone marked terminal in its sub-trees.
These arerepresented as bold boxes in the figure.
If non-terminals contain marked non-terminals in theirsub-trees, these non-terminals are also marked re-cursively.
These marked non-terminals and termi-nals compose a tree structure like that on the right-hand side in the figure.
These non-terminals rep-resent joint events at each node.SS ,,NP VPI said..S..NP VPADVPI never think soNP VPADVPI never think sotop topFigure 2: Example of parse tree pair that cannotbe matched.AB CE FDG HhfAB CEDdgcdgcGFigure 3: Example of bottom-up method.Note that this ?tree?
is not guaranteed to bea grammatical ?parse tree?
by the CFG gram-mar.
For example, from the tree of Figure 2,(S (S ?
?
? )
(, , ) (NP I) (VP said) (.
.
)), a newtree, (S (S ?
?
? )
(.
.
)), is extracted.
However, therule (S ?
S .)
is ungrammatical.4 Experiment4.1 Evaluation MethodWe evaluated each sentence compression methodusing word F -measures, bigram F -measures, andBLEU scores (Papineni et al, 2002).
BLEU scoresare usually used for evaluating machine transla-tion quality.
A BLEU score is defined as theweighted geometric average of n-gram precisionswith length penalties.
We used from unigram to4-gram precisions and uniform weights for theBLEU scores.ROUGE (Lin, 2004) is a set of recall-based cri-teria that is mainly used for evaluating summa-rization tasks.
ROUGE-N uses average N-gram re-call, and ROUGE-1 is word recall.
ROUGE-L usesthe length of the longest common subsequence(LCS) of the original and summarized sentences.In our model, the length of the LCS is equal tothe number of common words, and ROUGE-L isequal to the unigram F -measure because wordsare not rearranged.
ROUGE-L and ROUGE-1 aresupposed to be appropriate for the headline gener-853ation task (Lin, 2004).
This is not our task, but itis the most similar task in his paper.We also evaluated the methods using humanjudgments.
The evaluator is not the author but nota native English speaker.
The judgment used thesame criteria as those in Knight and Marcu?s meth-ods.
We performed two experiments.
In the firstexperiment, evaluators scored from 1 to 5 pointsthe grammaticality of the compressed sentence.
Inthe second one, they scored from 1 to 5 pointshow well the compressed sentence contained theimportant words of the original one.We used the parallel corpus used in Ref.
(Knightand Marcu, 2000).
This corpus consists of sen-tence pairs extracted automatically from the Ziff-Davis corpus, a set of newspaper articles aboutcomputer products.
This corpus has 1087 sentencepairs.
Thirty-two of these sentences were used forthe human judgments in Knight and Marcu?s ex-periment, and the same sentences were used forour human judgments.
The rest of the sentenceswere randomly shuffled, and 527 sentence pairswere used as a training corpus, 263 pairs as a de-velopment corpus, and 264 pairs as a test corpus.To parse these corpora, we used Charniak andJohnson?s parser (Charniak and Johnson, 2005).4.2 Settings of Two ExperimentsWe experimented with/without goal sentencelength for summaries.In the first experiment, the system was givenonly a sentence and no sentence length informa-tion.
The sentence compression problem withoutthe length information is a general task, but evalu-ating it is difficult because the correct length of asummary is not generally defined even by humans.The following example shows this.Original:?A font, on the other hand, is a subcate-gory of a typeface, such as Helvetica Bold or Hel-vetica Medium.
?Human: ?A font is a subcategory of a typeface,such as Helvetica Bold.
?System: ?A font is a subcategory of a typeface.
?The ?such as?
phrase is removed in this sys-tem output, but it is not removed in the humansummary.
Neither result is wrong, but in suchsituations, the evaluation score of the system de-creases.
This is because the compression rate ofeach algorithm is different, and evaluation scoresare affected by the lengths of system outputs.
Forthis reason, results with different lengths cannot be00.10.20.30.40.50.60.70.80  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1F-measureCompression ratioNoisy-channelMEME + bottom-upFigure 4: F -measures and compression ratios.00.10.20.30.40.50.60.70.80  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1BigramF-measureCompression ratioNoisy-channelMEME + bottom-upFigure 5: Bigram F -measures and compressionratios.compared easily.
We therefore examined the rela-tions between the average compression ratios andevaluation scores for all methods by changing thesystem summary length with the different lengthparameter ?
introduced in Section 3.1.In the second experiment, the system was givena sentence and the length for the compressed sen-tence.
We compressed each input sentence to thelength of the sentence in its goal summary.
Thissentence compression problem is easier than thatin which the system can generate sentences of anylength.
We selected the highest-scored sentencefrom the sentences of length l. Note that the re-calls, precisions and F-measures have the samescores in this setting.4.3 Results of ExperimentsThe results of the experiment without the sen-tence length information are shown in Figure 4,5 and 6.
Noisy-channel indicates the results of thenoisy-channel model, ME indicates the results ofthe maximum-entropy method, and ME + bottom-up indicates the results of the maximum-entropy85400.10.20.30.40.50.60.70.80  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1BLEUscoreCompression ratioNoisy-channelMEME + bottom-upFigure 6: BLEU scores and compression ratios.0.40.50.60.70.80.9Noisy-channel ME ME+bottom-upF-measurebigram-F-measureBLEUFigure 7: Results of experiments with length in-formation.method with the bottom-up method.
We used thelength parameter, ?, introduced in Section 3.1, andobtained a set of summaries with different aver-age lengths.
We plotted the compression ratiosand three scores in the figures.
In these figures,a compression ratio is the ratio of the total num-ber of words in compressed sentences to the totalnumber of words in the original sentences.In these figures, our maximum entropy meth-ods obtained higher scores than the noisy-channelmodel at all compression ratios.
The maximumentropy method with the bottom-up method obtainthe highest scores on these three measures.The results of the experiment with the sentencelength information are shown in Figure 7.
In thisexperiment, the scores of the maximum entropymethods were higher than the scores of the noisy-channel model.
The maximum entropy methodwith the bottom-up method achieved the highestscores on each measure.The results of the human judgments are shownin Table 2.
In this experiment, each length of out-put is same as the length of goal sentence.
TheMethod Grammar ImportanceHuman 4.94 4.31Noisy-channel 3.81 3.38ME 3.88 3.38ME + bottom-up 4.22 4.06Table 2: Results of human judgments.maximum entropy with the bottom-up method ob-tained the highest scores of the three methods.
Wedid t-tests (5% significance).
Between the noisy-channel model and the maximum entropy with thebottom-up method, importance is significantly dif-ferent but grammaticality is not.
Between the hu-man and the maximum entropy with the bottom-up method, grammaticality is significantly differ-ent but importance is not.
There are no significantdifferences between the noisy-channel model andthe maximum entropy model.4.3.1 Problem of Negative AdverbsOne problem of the noisy-channel model is thatit cannot distinguish the meanings of removedwords.
That is, it sometimes removes semanticallyimportant words, such as ?not?
and ?never?, be-cause the expansion probability depends only onnon-terminals of parent and daughter nodes.For example, our test corpus includes 15 sen-tences that contain ?not?.
The noisy-channelmodel removed six ?not?s, and the meanings ofthe sentences were reversed.
However, the twomaximum entropy methods removed only one?not?
because they have ?negative adverb?
as afeature in their models.
The first example in Ta-ble 3 shows one of these sentences.
In this exam-ple, only Noisy-channel removed ?not?.4.3.2 Effect of Bottom-Up MethodOur bottom-up method achieved the highestaccuracy, in terms of F -measures, bigram F -measures, BLEU scores and human judgments.The results were fairly good, especially when itsummarized complex sentences, which have sen-tences as parts.
The second example in Table 3 isa typical complex sentence.
In this example, onlyME + bottom-up correctly remove ?he said?.Most of the complex sentences were correctlycompressed by the bottom-up method, but a fewsentences like the third example in Table 3 werenot.
In this example, the original sentence wasparsed as shown in Figure 8 (left).
If this sen-tence is compressed to the human output, its parsetree has to be like that in Figure 8 (middle) using855Original a file or application ??
alias ?
?similar in effect to the ms-dos pathstatement provides a visible icon infolders where an aliased applicationdoes not actually reside .Human a file or application alias providesa visible icon in folders where analiased application does not actuallyreside .Noisy-channela similar in effect to ms-dosstatement provides a visible icon infolders where an aliased applicationdoes reside .ME a or application alias statementprovides a visible icon in folderswhere an aliased application does notactually reside .ME +bottom-upa file or application statementprovides a visible icon in folderswhere an aliased application does notactually reside .Original the user can then abort thetransmission , he said .Human the user can then abort thetransmission .Noisy-channelthe user can abort the transmissionsaid .ME the user can abort the transmissionsaid .ME +bottom-upthe user can then abort thetransmission .Original it is likely that both companies willwork on integrating multimedia withdatabase technologies .Human both companies will work onintegrating multimedia with databasetechnologies .Noisy-channelit is likely that both companies willwork on integrating .ME it is likely that both companies willwork on integrating .ME +bottom-upit is will work on integratingmultimedia with database technologies.Table 3: Examples of compressed sentences.our method.
When a parse tree is too long fromthe root to the leaves like this, some nodes aretrimmed but others are not because we assume thateach trimming probability is independent.
Thecompressed sentence is ungrammatical, as in thethird example in Table 3.We have to constrain such ungrammatical sen-tences or introduce another rule that reconstructsa short tree as in Figure 8 (right).
That is, we in-troduce a new transformation rule that compresses(A1 (B (C (A2 ?
?
? ))))
to (A2 ?
?
?
).4.4 Comparison with Original ResultsWe compared our results with Knight and Marcu?soriginal results.
They implemented two methods:one is the noisy-channel model and the other isa decision-based model.
Each model produced32 compressed sentences, and we calculated F -measures, bigram F -measures, and BLEU scores.We used the length parameter ?
= 0.5 for themaximum-entropy method and ?
= ?0.25 forSVPis ADJP SBARlikely that Sboth companieswill ...SItboth companieswill ...SVPSBARSboth companieswill ...(left) (middle) (right)Figure 8: Parse trees of complicated complex sen-tences.Method Comp.
F-measure bigram F-measureBLEUNoisy-channel70.19% 68.80 55.96 44.54Decision-based57.26% 71.25 61.93 58.21ME 66.51% 73.10 62.86 53.51ME +bottom-up58.14% 78.58 70.30 65.26Human 53.59%Table 4: Comparison with original results.the maximum-entropy method with the bottom-upmethod.
These two values were determined usingexperiments on the development set, which did notcontain the 32 test sentences.The results are shown in Table 4.
Noisy-channelindicates the results of Knight and Marcu?s noisy-channel model, and Decision-based indicates theresults of Knight and Marcu?s decision-basedmodel.
Comp.
indicates the compression ratio ofeach result.
Our two methods achieved higher ac-curacy than the noisy-channel model.
The resultsof the decision-based model and our maximum-entropy method were not significantly different.Our maximum-entropy method with the bottom-up method achieved the highest accuracy.4.5 Corpus Size and Output AccuracyIn general, using more training data improves theaccuracy of outputs and using less data results inlow accuracy.
Our experiment has the problemthat the training corpus was small.
To study the re-lation between training corpus size and accuracy,we experimented using different training corpussizes and compared accuracy of the output.Figure 9 shows the relations between trainingcorpus size and three scores, F -measures, bigramF -measures and BLEU scores, when we used themaximum entropy method with the bottom-upmethod.
This graph suggests that the accuracy in-8560.550.60.650.70.750.80.850  100  200  300  400  500  600  700  800ScoreSize of training corpusBLEU scoreF-measurebigram F-measureFigure 9: Relation between training corpus sizeand evaluation score.creases when the corpus size is increased.
Overabout 600 sentences, the increase becomes slower.The graph shows that the training corpus waslarge enough for this study.
However, if we intro-duced other specific features, such as lexical fea-tures, a larger corpus would be required.5 ConclusionWe presented a maximum entropy model to ex-tend the sentence compression methods describedby Knight and Marcu (Knight and Marcu, 2000).Our proposals are two-fold.
First, our maxi-mum entropy model allows us to incorporate var-ious characteristics, such as a mother node or thedepth from a root node, into a probabilistic modelfor determining which part of an input sentenceis removed.
Second, our bottom-up method ofmatching original and compressed parse trees canmatch tree structures that cannot be matched usingKnight and Marcu?s method.The experimental results show that our maxi-mum entropy method improved the accuracy ofsentence compression as determined by three eval-uation criteria: F -measures, bigram F -measuresand BLEU scores.
Using our bottom-up methodfurther improved accuracy and produced shortsummaries that could not be produced by previ-ous methods.
However, we need to modify thismodel to appropriately process more complicatedsentences because some sentences were not cor-rectly summarized.
Human judgments showedthat the maximum entropy model with the bottom-up method provided more grammatical and moreinformative summaries than other methods.Though our training corpus was small, our ex-periments demonstrated that the data was suffi-cient.
To improve our approaches, we can intro-duce more feature functions, especially more se-mantic or lexical features, and to deal with thesefeatures, we need a larger corpus.AcknowledgementsWe would like to thank Prof. Kevin Knight andProf.
Daniel Marcu for providing their parallelcorpus and the experimental results.ReferencesA.
L. Berger, V. J. Della Pietra, and S. A. Della Pietra.1996.
A Maximum Entropy Approach to NaturalLanguage Processing.
Computational Linguistics,22(1):39?71.E.
Charniak and M. Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking.In Proc.
of ACL?05, pages 173?180.B.
Dorr, D. Zajic, and R. Schwartz.
2003.
Hedge Trim-mer: A Parse-and-Trim Approach to Headline Gen-eration.
In Proc.
of DUC 2003, pages 1?8.H.
Jing and K. R. McKeown.
1999.
The decomposi-tion of human-written summary sentences.
In Proc.of SIGIR?99, pages 129?136.K.
Knight and D. Marcu.
2000.
Statistics-Based Sum-marization - Step One: Sentence Compression.
InProc.
of AAAI/IAAI?00, pages 703?710.I.
Langkilde.
2000.
Forest-Based Statistical SentenceGeneration.
In Proc.
of NAACL?00, pages 170?177.C.
Lin.
2004.
ROUGE: A Package for AutomaticEvaluation of Summaries.
In Text SummarizationBranches Out: Proc.
of ACL?04 Workshop, pages74?81.R.
McDonald.
2006.
Discriminative Sentence Com-pression with Soft Syntactic Evidence.
In Proc.
ofEACL?06.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a Method for Automatic Evaluation of Ma-chine Translation.
In Proc.
of ACL?02, pages 311?318.J.
Turner and E. Charniak.
2005.
Supervised and Un-supervised Learning for Sentence Compression.
InProc.
of ACL?05, pages 290?297.V.
Vandeghinste and Y. Pan.
2004.
Sentence Com-pression for Automated Subtitling: A Hybrid Ap-proach.
In Text Summarization Branches Out: Proc.of ACL?04 Workshop, pages 89?95.M.
J. Witbrock and V. O. Mittal.
1999.
Ultra-Summarization: A Statistical Approach to Generat-ing Highly Condensed Non-Extractive Summaries.In Proc.
of SIGIR?99, pages 315?316.857
