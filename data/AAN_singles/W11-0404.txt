Proceedings of the Fifth Law Workshop (LAW V), pages 30?37,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsHow Good is the Crowd at ?real?
WSD?Jisup HongInternational Computer Science InstituteBerkeley, CAjhong@icsi.berkeley.eduCollin F. BakerInternational Computer Science InstituteBerkeley, CAcollinb@icsi.berkeley.eduAbstractThere has been a great deal of excitement re-cently about using the ?wisdom of the crowd?to collect data of all kinds, quickly and cheaply(Howe, 2008; von Ahn and Dabbish, 2008).Snow et al (Snow et al, 2008) were the firstto give a convincing demonstration that at leastsome kinds of linguistic data can be gatheredfrom workers on the web more cheaply thanand as accurately as from local experts, andthere has been a steady stream of papers andworkshops since then with similar results.
e.g.
(Callison-Burch and Dredze, 2010).Many of the tasks which have been success-fully crowdsourced involve judgments whichare similar to those performed in everyday life,such as recognizing unclear writing (von Ahnet al, 2008), or, for those tasks that require con-siderable judgment, the responses are usuallybinary or from a small set of responses, suchas sentiment analysis (Mellebeek et al, 2010)or ratings (Heilman and Smith, 2010).
Sincethe FrameNet process is known to be relativelyexpensive, we were interested in whether theFrameNet process of fine word sense discrimi-nation and marking of dependents with seman-tic roles could be performed more cheaply andequally accurately using Amazon?s MechanicalTurk (AMT) or similar resources.
We report ona partial success in this respect and how it wasachieved.1 Defining the taskThe usual FrameNet process for annotating exam-ples of a particular lexical unit (LU), is to first ex-tract examples of this sense from a corpus, based oncollocational and syntactic patterns, storing them insubcorpora; this process is called subcorporation.Given an LU, vanguarders begin by composing rulesconsisting of syntactic patterns and instructions asto whether to include or exclude the sentences thatmatch them.
An automated system extracts sentencescontaining uses of the LU?s lemma, applies POS tag-ging and chunk parsing, and then matches the sen-tences against the rules in their specified order to al-low for cascading effects.
Ultimately, the result is aset of subcorpora, each corresponding to a pattern,and containing sentences likely to exhibit a use ofthe LU.
More recently, a system has been developedin collaboration with the Sketch Engine ((Kilgarriffet al, July 2004) http://www.sketchengine.co.uk) to accelerate this process by giving annota-tors a graphical interface in which precomputed col-locational pattern matches can be more directly as-signed to the various LUs corresponding to a givenlemma.
The actual annotation of the frame ele-ments (FEs) is facilitated by having pre-selected setsof sentences which are at least likely to contain theright sense of the word, and which share a syntac-tic pattern.
Therefore, we first focused on the framediscrimination task (which in other contexts wouldbe called word sense discrimination), which we as-sumed to be simpler to collect data for than the FEannotation task, and which is a prerequisite for it.We began by evaluating the resources that AMTprovides for designing and implementingHuman In-telligence Tasks (HITs); we quickly determined thatthe UI provided by AMT would not suffice for thetask we planned.
Specifically, it lacks the ability to:?
randomize the selection options,30?
present questions from a set one at a time,?
randomize the order in which a set of questionsare presented, or?
record response times for each question.We therefore decided to design our HITs usingAmazon?s ?External Question HIT Type?, and toserve the HITs from our own web server.
In this sys-tem, when workers view or execute a HIT, the con-tent of the HIT window is supplied from our server,and responses are stored directly in a database run-ning our own server, rather than Amazon?s.
Workerslog in through AMT and are ultimately paid throughAMT, but the content of the tasks can be completelycontrolled though our web server.The Frame Discrimination Task can be set up in anumber of ways, such as:1.
Present a single sentence with the lemma high-lighted.
Workers must select a frame (or ?noneof the above?)
from a multiple-choice list offrames we provide.2.
Present a list of sentences all containing uses ofthe same lemma.
Workers must check off all thesentences that contain uses of a given frame.3.
Present a list of sentences all containing uses ofthe same lemma.
Provide one example sentencefrom each frame and ask users to categorize thesentences.In order to get started as quickly as possible andget a baseline result, we chose the first of the abovemethods, which is the most straightforward from atheoretical point of view.
For example, the lemmamight be gain.v, which has two LUs, one in theChange position on a scale frame, and another inthe Getting frame.
The HIT displays one sentence ata time, with the lemma highlighted; below the sen-tence, a multiple-choice selection is presented withthe Frame names:You will have to GAIN their support,if change is to be brought about.Change_position_on_a_scaleGettingNone of the aboveWhen users mouse-over the name of a frame,a pop-up displays an example sentence from thatFrame (from a different LU in the same frame).
Userscan also click the name of the frame, which causesthe browser to open another window with the framedefinition.
This process repeats for 12 sentences, atwhich point the HIT is over, and results are enteredinto our database.Sources of material for testingWe had no shortage of sentences for the framediscrimination task; we started with some of themany unannotated sentences already in the FrameNetdatabase.
In the usual process of subcorporation,each of the subcorpora matches one specific pattern;the goal is to extract roughly 20 examples of eachcollocational/syntactic pattern, and to annotate one ortwo of each.
The following are examples from amongthe patterns used for rip.v in the Removing frame:NP T NP [PP f="from"]NP T NP [w "out"]The first pattern would match sentences like, ?Iripped the top from my pack of cigarettes,?
and thesecond, ?She ripped the telephone out of the wall.
?We do not presume, however, that we will al-ways be able to define patterns for all of the possi-ble valences of a predicator, so we also include two?other?
subcorpora.
The first of these (named ?other-matched?)
contains 50 sentences (provided there areenough instances in the corpus) which matched anyone of the preceding patterns but were left over af-ter 20 had been extracted for each pattern.
The sec-ond (?other-unmatched?)
contains sentences in whichthe lemma occurs (with the right POS) which didnot match any of the earlier patterns.
Vanguarderscarefully check these ?other?
subcorpora to see if thelemma is used in a syntactic valence which was notforeseen; if they find any such new valences, theyare annotated.
Typically, this means that there areroughly 100 extra unannotated sentences for eachLU.
For this experiment, we extracted 10 sentencesfrom the ?other-matched?
subcorpus of each of theLUs for the lemma, meaning that they had alreadymatched some pattern which was designed for one ofthose LUs.
In addition to the unannotated sentences,we randomly selected three annotated sentences fromeach LU, two to use as included gold-standard items31Frame name ExampleCause to fragment The revolution has RIPPED thousands of Cuban families apart .
.
.Damaging .
.
.Mo?s dress is RIPPED by a drunken admirer.Removing Sinatra then reportedly RIPPED the phone out of the wall .
.
.Self motion A tornado RIPPED through Salt Lake City .
.
.Judgment communication (no annotated examples?related to rip into.v)Position on a scale Eggs, shellfish and cheese are all HIGH in cholesterol .
.
.Dimension An adult tiger stands at least 3 ft (90 cm) HIGH at the shoulder.Intoxication Exhausted but HIGH on adrenalin, he would roam about the house.
.
.Measurable attributes Finally we came to a HIGH plastic wall.Evidence Our results SHOW that unmodified oligonucleotides can provide .
.
.Reasoning He uses economics to SHOW how this is so.Obviousness .
.
.
sighting black mountain tops SHOWING through the ice-cap.Cotheme When they were SHOWN to their table, .
.
.Finish competition (no annotated examples?
Fair Lady placed in the second race at Aqueduct.
)Cause to perceive A second inner pylon SHOWS Ptolemy XIII paying homage to Isis .
.
.Table 1: LUs (senses) for rip.v, high.a, and show.vfor checking accuracy, and one to use as the exam-ple in the preview of the HIT.
These sentences wererandomized and separated into batches of 12 for eachHIT; all of which were inserted into a database on alocal web server.
A local CGI script (reached fromAMT) calls the database for the examples in eachHIT and stores the workers?
responses in the samedatabase.We ran three trials under this setup, for the lem-mas rip.v, high.a, and show.v.
Based on the successof earlier studies, our concern initially was to makeour tasks be sufficiently challenging so as to be use-ful for evaluating AMT.
Thus, we chose lemmas withfour to five senses rather than just two or three.
Inaddition, for these three lemmas, each of the sensesappears with sufficient frequency in the corpus sothat all senses are realistically available for consid-eration.1 The frames for each of these lemmas areshown in Table 1; some of these distinctions are fairlysubtle; we will discuss some examples below.To combine responses, we took the modal responseas the result for each item; in cases of ties, we choserandomly, and split the response count where neces-sary.
On this basis, for rip.v, the workers had an ac-curacy of 32.16 correct out of 48 items (67%), for1An exception is the show.v in the Finish competitionframe, which we excluded for this reason, as in Mucho MachoMan showed in the 2011 Kentucky Derby.high.a, they got 22 out of 49 correct (46%), and forshow.v, 37 out of 60 items (62%), as shown in Ta-ble 2.
If we consider that FrameNet has four senses(LUs) for rip.v and high.a and five for show.v, thismight not sound too awful, but if we think of this aspre-processing, so that the resulting sentences can beannotated in the correct frame, it leaves a lot to bedesired.
If we raise the agreement criteria, by filter-ing out items on which the margin between the modalresponse and the next highest is 35% or greater (i.e.those with high agreement among workers), we canget higher accuracy (shown in the right two columnsof Table 2), at the expense of failing to classify 3/4of the items, hardly a solution to the problem.Trials with CrowdFlowerWe decided to try our task on CrowdFlower (http://crowdflower.com, formerly Dolores Labs), acompany that provides tools and custom solutions tomake crowdsourcing tasks easier to create and man-age, including techniques to assure a certain level ofquality in the results.
While working with Crowd-Flower, our tasks were running on AMT, althoughCrowdFlower also provides other labor pools, such asSamasource (http://www.samasource.org),depending on the nature of the task.
We tried run-ning the task for rip.v on Crowdflower?s system, us-ing the same HIT design as before, (recreated using32Lemma No.
senses No.
Items Accuracy Filtered Items Accuracy.rip.v 4 48 67% 10 90%high.a 4 48 46% 12 58%show.v 5 60 62% 11 64%Table 2: Results from Trial 1: Rip.v, high.a and show.vtheir self-serve UI design tools), but with differentsentences.
Once again, we selected 12 sentences foreach of the 4 LUs, for a total of 48 sentences.
Wewanted to collect 10 judgments per sentence, for atotal of 480 judgments.
Of the 12 sentences in eachHIT, 2 were already annotated and used as a goldstandard.However, after starting this job, we found that theCrowdFlower system automatically halted the jobsafter a few hours due to poor average performance onthe gold standard items.
After having the job haltedrepeatedly, we were finally able to force it to finishby suspending use of the gold standard to judge ac-curacy.
In other words, the system was telling us thatthe task was too hard for the workers.Revised CrowdFlower TrialsAfter our difficulties with the first trial on Crowd-Flower?s system, we visited their offices for anon-site consultation.
We learned more about howCrowdFlower?s system works, and received sugges-tions on how to improve performance:?
Run a larger set of data; they recommended atleast 200 sentences for a job.?
Embed 20% gold standard items so that there isat least one per page of questions, since, withoutgold standard items, workers will answer ran-domly, or always choose the first option.?
Get rid of the frame names and use somethingeasier to understand.?
Provide more detailed instructions that includeexamples.Based on this consultation, we made the follow-ing changes in our HITs: (1) Replaced frame nameswith hand-crafted synonyms, (2) Renamed the taskand rewrote all instructions to avoid jargon, (3) Re-moved links and roll-overs giving examples or refer-ring people to external documentation, and (4) Ex-tracted 60 sentences per LU, of which 10 are goldstandard.Although we planned to do this for rip.v, high.a,and show.v, we found that it was too difficult to comeup with synonyms for high.a, so we ran trials only forrip.v and show.v.
For rip.v, with four senses, we col-lected 10 judgments each on 240 sentences, for a to-tal of 2400 judgments.
For show.v, with five senses,we collected 10 judgments each on 300 sentences,for a total of 3000 judgments.
In the final trials,the weighted majority response provided by Crowd-Flower was found to be correct 75% for rip.v and80% for show.v.
This was encouraging, but we wereconcerned with the limitations of this method: (1)The calculation used to select the ?weighted major-ity response?
is proprietary to CrowdFlower, so thatwe could not know the details or change it, and (2)the final trials required handcrafted definitions, syn-onyms, and very clear definitions for each LU, whichis at best time-consuming, and sometimes impossible(as is likely case for high.a), meaning the method willnot scale well.
As researchers, the first limitation isespecially problematic as it is necessary to know ex-actly what methods we are using in our research andbe able to share them openly.
For these reasons, wedecided to go back to building our own interfaces onAMT, and to look for approaches that would be moreautomatic.Return to AMTWe redesigned the HIT around a pile-sorting model;instead of seeing one sentence and choosing betweenframes (whether by name or by synonym), workersare shown model sentences for each LU (i.e.
in eachframe), and then asked to categorize a list of sen-tences that are displayed all at once.
Consequently,the worker generates a set of piles each correspond-ing to a frame/LU.
The advantages of this approachare as follows:?
Workers can more easily exploit paradigmatic33contrasts across sentences to decide which cate-gory to put them in.?
Workers can recategorize sentences after ini-tially putting them into a pile.?
Workers have example sentences using the LUsin question, which constitutes more informationthan the frame name (assuming that they werenot going to the FrameNet website to peruse an-notation).?
HITs can be generated automatically, without ushaving to manually create synonyms for eachLU, which turned out to be quite difficult.This approach, however, does have some disadvan-tages:?
We need to pre-annotate at least 1 sentence perLU in order to have example sentences.?
Having lots of sentences presented at once clut-ters up the screen and requires scrolling.?
The HIT interface is much more complex andpotentially more fragile.Because of the complexity of the new interface andthe increased screen space required for each addi-tional sense, we decided to begin trials on the lemmajustify.v which (we believe) has just two senses, butstill requires a fairly difficult distinction, between theDeserving frame, as in The evolutionary analogy isclose enough to JUSTIFY borrowing the term, .
.
.and the Justifying frame, as in This final section al-lows Mr Hicks to JUSTIFY the implementation of abcas.
.
.
.
These two sentences were were annotated inthe FrameNet data, and were randomly selected toserve as the models for the workers, illustrating thedanger of choosing randomly in such cases!For all HITs, the sentences were randomized in or-der, as well as the order of the example sentences.Example sentences retained the same colors, i.e.the frame/color correspondence was kept constant,so as not to confuse workers working on multipleHITs.
Sentences were horizontally aligned so thatthe highlighted target word was centered and verti-cally aligned across the sentences.
Each sentence hada drop-down box to its right where workers could se-lect a category to place it in.
Each sense category wasrepresented by a model sentence with the frame nameas a label for the category.
We collected 10 judgmentseach on 132 sentences, with workers being asked tocategorize 18 sentences in each HIT.
In the first trial,accuracy was 55%.
In trial 2, the model sentenceswere modified to also show frame element annota-tion, in the hope that the fact that the Justifying useshave an Agent as the subject, while the Deservinguses have a State of affairs as the subject would beclearer.
An image of the HIT interface, with FE an-notation displayed on the model sentences, is shownin Figure 1.
Despite the added information, accuracydecreased to 45%.Qualifying the prospectsIn trial 3, we kept the HIT interface the same, includ-ing the model sentences, but added (1) a qualificationtest that was designed to evaluate the worker?s abilityin English, (2) required that the workers have regis-tered a US address with Amazon and (3) required thatworkers have an overall HIT acceptance rate greaterthan 75%.
Although over 100 workers took the qual-ification test, no workers accepted the HIT.
In trial4 we raised the rate of pay to $.25/HIT, but still gotonly 1 worker.On the suspicion that our problem was partiallycaused by not having enough HITs to make it worththe workers?
time to do them, in Trial 5 we posted thesame HITs 3 times, amounting to 24 HITs, worth $6,from a worker?s point of view; this raised the num-ber of workers to 5 for all three HITs.
Through theHITs completed by those workers, we collected 1 to2 judgments on 107 of the 132 sentences posted, with63% accuracy overall, and 86% accuracy on the goldsentences.
Looking at their answers for each frame,workers correctly categorized 93% of cases of Justi-fying but only 52% of cases of Deserving.In trial 6, we then customized the instructions (thistime automatically, rather than manually) to refer tothe lemma specifically rather than via a generic de-scription like ?the highlighted word.?
In addition, weremoved the qualification test so as to make our HITsavailable to a much larger pool of workers, but keptthe other two requirements.
We ran HITs again with18 sentences each, 2 of which were gold.
We decidedto try a different lemma with two sense distinctions,top.a, and to make it more worthwhile for workersto annotate our data by posting HITs simultaneously345/11/11 1:15 AMPut Sentences into GroupsPage 1 of 2http://framenet.icsi.berkeley.edu:22222/mturkdev/fnsortui_fe_oc.php?assignmentId=ASSIGNMENT_ID_NOT_AVAILABLE&hitId=2OKJENLVWJ5O4YNGNPQ81MWB8BN0S2This is only a preview.
Please accept this HIT before working on it.Put Sentences into GroupsInstructions: (click to show)Groups:The evolutionary analogy is close enough to JUSTIFY borrowing the term , and I make no ...DeservingState_of_affairsAction3.... ; certainly their expected sales would nothaveJUSTIFIED their production .... final section allows Mr Hicks to JUSTIFY the implementation of abc as a better ...JustifyingAgentAct2.
uh-huh i could never JUSTIFY owning a personal computer at at homeNone_of_the_aboveSentences to Group: 16 remaining1.
...
US is that there is not enough information yet to JUSTIFY expensive remedial action .4.
... this extent , the fascination of the experiments is JUSTIFIED .5.... were pursued vigorously and with a vengeancemorallyJUSTIFIEDby the offender 's wickedness , then ` our " society...6.
... making the point , it does apply but it has to be JUSTIFIED .7.
How does Ormrod J.
JUSTIFY his decision ?8.that there are some searches the war on drugs cannotJUSTIFY .
``9.... taken care to make just enough extremestatements toJUSTIFYhis ' credentials ' with outright racists and neo-Nazis ...Change groupChange groupChoosegroupChoosegroupChoosegroupChoosegroupChoosegroupChoosegroupChoosegroupDeservingJustifyingNone_of_the_aboveFigure 1: HIT Screen for justify.v (after two sentences have been categorized)for rip.v and high.a.
We posted 8 HITs for top.a, 16HITs for high.a and 16 for rip.v, for a total of 40 HITsacross all three lemmas, paying $.15/HIT and collect-ing 10 assignments/HIT.These results were much more satisfactory, withaccuracy as shown in Table 3.
Filtering out itemsby raising the agreement criteria (as before) to 35%or greater between the modal response and the nexthighest, yielded even better accuracy, above 90% forall three lemmas, at the cost of failing to classify ap-proximately 10% to 30% of the items.In response to the relative success of this trial, weposted HITs for three additional lemmas: thirst.n,range.n, and history.n, with 3, 4, and 5 senses, respec-tively.
We chose these lemmas to ascertain whetherthere would be an effect on performance from thenumber of senses.
Thus all three lemmas were also ofthe name POS.
For Trial 7, although we kept the sameinterface, we experimented with changing the pay,and offering bonuses in an effort to maintain goodstanding among AMT workers concerned with theirHIT acceptance record.
For previous HITs, workershad to correctly categorize both gold sentences in or-der to receive any payment.
We changed this sys-tem so that the HIT is accepted if the worker catego-rizes 1 gold sentence correctly, and awards a bonusif they categorize both correctly.
Our hope was thatthis change would enable us to experiment with post-ing difficult HITs without losing our credibility.
Theresults from this trial, also presented in Table 3, showaccuracy at 92%, 87%, and 73%, respectively forthirst.n, range.n, and history.n.
These results seemedto suggest that increasing the number of senses to dis-criminate increases the difficulty of the HIT.It will be recalled that on every item, the work-ers have a choice ?none of the above?.
One ofthe difficulties is that this choice covers a variety ofcases, including those where the word is the wrongpart of speech (a fairly frequent occurrence, despitethe high accuracy cited for POS tagging) and thosewhere the needed sense has simply not been includedin FrameNet.
The latter was the case for the wordrange.n, which was run once with three senses andthen again with five senses, after the LUs for (firing,artillery) range and the ?stove?
sense were added.With the two additional senses, the accuracy actuallywent up from 87% to 92%.
Although it is possiblethat the improvement could be due to a training ef-fect connected to an increase in the number of items,it suggests that having more sense distinctions doesnot necessarily increase difficulty of discrimination.35Lemma No.
senses No.
Items Accuracy Filtered Items Accuracytop.a 2 144 92% 134 96%rip.v 4 288 85% 228 92%high.a 4 288 80% 198 92%thirst.n 2 144 92% 128 95%range.n 3 216 87% 177 93%history.n 4 288 73% 199 86%range.n 5 360 92% 335 96%Table 3: Results from recent trials, including accuracy after filtering on the basis of agreementRemoving Cause to fragment Self motion Damaging None of the aboveN= 104 51 33 64 36Removing 97 93 1 1 2 0Cause to fragment 45 1 41 0 1 2Self motion 25 1 0 24 0 0Damaging 84 8 9 7 58 2None of the above 37 1 0 1 3 32Table 4: Confusion matrix for rip.v (rows=gold standard)2 What we can learn from the Turkers?difficulties?Consider the confusion matrix shown in Table 4; hereeach row represents the items grouped by the goldstandard sense (?expected?
); each column representsthe items grouped by the most frequent worker judg-ment (?observed?
).The accuracy on this HIT set was 85%, in accordwith the much larger numbers along the diagonal, butthe really interesting cases lie off the diagonal, wherethe plurality of the workers disagreed with the ex-perts.
In some cases, the workers are simply right,and the expert was wrong, as in This new wave ofanonymous buildings .
.
.
has RIPPED the heart out ofHammersmith., which the gold standard has as Dam-aging, but where the workers voted 7 to 3 for Re-moving.
In this case, the expert vanguarder appearsto have classified the metaphorical use of rip.v usingthe target domain, rather than the source domain, asis the FrameNet policy on ?productive?
(rather than?lexicalized?)
metaphor (Ruppenhofer et al, 2006,Sec.
6.4)2.
In practice, this classification would mostlikely have been corrected at the annotation phase, asthe FEs are clearly those of the source domain, in-2Available from the FrameNet website, http://framenet.icsi.berkeley.edu.volving removing something (a Theme) out of some-thing else (a Source).
In other cases, such as I rippedopen the envelopes., the gold standard correctly hasDamaging, while the workers have 4 Removing, 3Cause to fragment, and 3 Damaging.
There is agood possibility that the envelopes fragmented (al-though this is not implied, nor necessary to remove aletter from an envelope), and the purpose is likely toremove something from the envelopes, which mightfalsely suggest Removing.In other cases, the senses are so closely enmeshed,that is seems rather arbitrary to choose one: e.g.
IRIP up an old T-shirt of mine and offer it.
The shirtis certainly damaged and almost certainly fragmentedas a result of the same action.
.
.
.
the Oklahoma wasRIPPED apart when seven torpedoes hit her.
strictlyspeaking, the ship is caused to fragment, but the mil-itary purpose is to damage her beyond repair, if pos-sible.
And there are fairly often examples where thesentence in isolation is ambiguous: Rain RIPPED an-other piece of croissant, The sky RIPPED and hungin tatters , revealing plasterboard and lath behind.Such cases are pushing us toward trying to incorpo-rate blending of senses into our paradigm, along thelines of (Erk and McCarthy, 2009).363 ConclusionWe have shown that it is possible to set up HITs onAmazon Mechanical Turk to discriminate the fairlyfine sense distinctions used in FrameNet, if the rightapproach is taken, and that the results reach a levelof accuracy that can be useful for further processing,as well as serving as a cross-check on the expert dataand an invitation to re-think the task itself.
Althoughthe total amount of data collected may not be largeby some standards, it has been sufficient to give agood sense of which techniques work for the type ofWSD problems we are facing.
We intend to continueinvestigating the general applicability of this systemfor frame disambiguation, including further analysisof our data to better understand the factors that makea disambiguation task more or less difficult for crowdworkers.
All the data collected in the course of thisstudy, and the software used to collect and analyze it,will be made available on the FrameNet website.AcknowledgmentsThis material is based upon work supported bythe National Science Foundation under Grant No.0947841 (CISE EAGER) ?Crowdsourcing for NLP?
;the Sketch Engine GUI was developed under NSFGrant IIS-00535297 ?Rapid Development of aFrame-Semantic Lexicon?.ReferencesChris Callison-Burch and Mark Dredze, editors.
2010.Proceedings of the NAACL HLT 2010 Workshop onCreating Speech and Language Data with Amazon?sMechanical Turk, Los Angeles, June.
Association forComputational Linguistics.Katrin Erk and Diana McCarthy.
2009.
Graded wordsense assignment.
In Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 440?449, Singapore, August.
Associa-tion for Computational Linguistics.Michael Heilman and Noah A. Smith.
2010.
Ratingcomputer-generated questions with Mechanical Turk.In Proceedings of the NAACL HLT 2010 Workshop onCreating Speech and Language Data with Amazon?sMechanical Turk, pages 35?40, Los Angeles, June.
As-sociation for Computational Linguistics.Jeff Howe.
2008.
Crowdsourcing.
Crown Business, NewYork.Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and DavidTugwell.
July 2004.
The Sketch Engine.
In Proceed-ings of EURALEX 2004, Lorient, France.Bart Mellebeek, Francesc Benavent, Jens Grivolla, JoanCodina, Marta R. Costa-Jussa`, and Rafael Banchs.2010.
Opinion mining of spanish customer commentswith non-expert annotations on mechanical turk.
InProceedings of the NAACL HLT 2010 Workshop onCreating Speech and Language Data with Amazon?sMechanical Turk, pages 114?121, Los Angeles, June.Association for Computational Linguistics.Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.Petruck, Christopher R. Johnson, and Jan Scheffczyk.2006.
FrameNet II: Extended Theory and Practice.
In-ternational Computer Science Institute, Berkeley, Cali-fornia.
Distributed with the FrameNet data.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Ng.
2008.
Cheap and fast ?
but is it good?Evaluating non-expert annotations for natural languagetasks.
In Proceedings of the 2008 Conference on Em-pirical Methods in Natural Language Processing, pages254?263, Honolulu, Hawaii, October.
Association forComputational Linguistics.Lu?
?s von Ahn and Laura Dabbish.
2008.
Designing gameswith a purpose.
Communications of the ACM, 51:58?67., August.Lu?
?s von Ahn, Benjamin Maurer, Colin McMillen, DavidAbraham, and Manuel Blum.
2008. reCAPTCHA:Human-based character recognition via web securitymeasures.
Science, 321(5895):1465?1468.37
