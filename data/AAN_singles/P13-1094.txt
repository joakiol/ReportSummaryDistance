Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 954?963,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSentiment RelevanceChristian ScheibleInstitute for Natural Language ProcessingUniversity of Stuttgart, Germanyscheibcn@ims.uni-stuttgart.deHinrich Schu?tzeCenter for Informationand Language ProcessingUniversity of Munich, GermanyAbstractA number of different notions, includingsubjectivity, have been proposed for dis-tinguishing parts of documents that con-vey sentiment from those that do not.
Wepropose a new concept, sentiment rele-vance, to make this distinction and arguethat it better reflects the requirements ofsentiment analysis systems.
We demon-strate experimentally that sentiment rele-vance and subjectivity are related, but dif-ferent.
Since no large amount of labeledtraining data for our new notion of sen-timent relevance is available, we investi-gate two semi-supervised methods for cre-ating sentiment relevance classifiers: a dis-tant supervision approach that leveragesstructured information about the domainof the reviews; and transfer learning onfeature representations based on lexicaltaxonomies that enables knowledge trans-fer.
We show that both methods learn sen-timent relevance classifiers that performwell.1 IntroductionIt is generally recognized in sentiment analy-sis that only a subset of the content of a doc-ument contributes to the sentiment it conveys.For this reason, some authors distinguish thecategories subjective and objective (Wilson andWiebe, 2003).
Subjective statements refer to theinternal state of mind of a person, which cannot beobserved.
In contrast, objective statements can beverified by observing and checking reality.
Somesentiment analysis systems filter out objective lan-guage and predict sentiment based on subjectivelanguage only because objective statements do notdirectly reveal sentiment.Even though the categories subjective/objectiveare well-established in philosophy, we argue thatthey are not optimal for sentiment analysis.
We in-stead introduce the notion of sentiment relevance(S-relevance or SR for short).
A sentence or lin-guistic expression is S-relevant if it contains infor-mation about the sentiment the document conveys;it is S-nonrelevant (SNR) otherwise.Ideally, we would like to have at our disposala large annotated training set for our new con-cept of sentiment relevance.
However, such aresource does not yet exist.
For this reason,we investigate two semi-supervised approaches toS-relevance classification that do not require S-relevance-labeled data.
The first approach is dis-tant supervision (DS).
We create an initial label-ing based on domain-specific metadata that we ex-tract from a public database and show that thisimproves performance by 5.8% F1 compared to abaseline.
The second approach is transfer learning(TL) (Thrun, 1996).
We show that TL improvesF1 by 12.6% for sentiment relevance classificationwhen we use a feature representation based on lex-ical taxonomies that supports knowledge transfer.In our approach, we classify sentences as S-(non)relevant because this is the most fine-grainedlevel at which S-relevance manifests itself; at theword or phrase level, S-relevance classificationis not possible because of scope and context ef-fects.
However, S-relevance is also a discoursephenomenon: authors tend to structure documentsinto S-relevant passages and S-nonrelevant pas-sages.
To impose this discourse constraint, we em-ploy a sequence model.
We represent each docu-ment as a graph of sentences and apply a minimumcut method.The rest of the paper is structured as follows.Section 2 introduces the concept of sentiment rel-evance and relates it to subjectivity.
In Section 3,we review previous work related to sentiment rel-evance.
Next, we describe the methods applied inthis paper (Section 4) and the features we extract(Section 5).
Finally, we turn to the description and954results of our experiments on distant supervision(Section 6) and transfer learning (Section 7).
Weend with a conclusion in Section 8.2 Sentiment RelevanceSentiment Relevance is a concept to distinguishcontent informative for determining the sentimentof a document from uninformative content.
Thisis in contrast to the usual distinction between sub-jective and objective content.
Although there isoverlap between the two notions, they are differ-ent.
Consider the following examples for subjec-tive and objective sentences:(1) Subjective example: Bruce Banner, a genet-ics researcher with a tragic past, suffers a horribleaccident.
(2) Objective example: The movie won aGolden Globe for best foreign film and an Oscar.Sentence (1) is subjective because assessmentslike tragic past and horrible accident are subjec-tive to the reader and writer.
Sentence (2) is objec-tive since we can check the truth of the statement.However, even though sentence (1) has negativesubjective content, it is not S-relevant because itis about the plot of the movie and can appear ina glowingly positive review.
Conversely, sentence(2) contributes to the positive opinion expressedby the author.
Subjectivity and S-relevance aretwo distinct concepts that do not imply each other:Generally neutral and objective sentences can beS-relevant while certain subjective content is S-nonrelevant.
Below, we first describe the annota-tion procedure for the sentiment relevance corpusand then demonstrate empirically that subjectivityand S-relevance differ.2.1 Sentiment Relevance CorpusFor our initial experiments, we focus on senti-ment relevance classification in the movie domain.To create a sentiment-relevance-annotated corpus,the SR corpus, we randomly selected 125 docu-ments from the movie review data set (Pang et al,2002).1 Two annotators annotated the sentencesfor S-relevance, using the labels SR and SNR.
If nodecision can be made because a sentence containsboth S-relevant and S-nonrelevant linguistic ma-terial, it is marked as uncertain.
We excluded360 sentences that were labeled uncertain from the1We used the texts from the raw HTML files since theprocessed version does not have capitalization.evaluation.
In total, the SR corpus contains 2759S-relevant and 728 S-nonrelevant sentences.
Fig-ure 1 shows an excerpt from the corpus.
The fullcorpus is available online.2First, we study agreement between human an-notators.
We had 762 sentences annotated for S-relevance by both annotators with an agreement(Fleiss?
?)
of .69.
In addition, we obtained sub-jectivity annotations for the same data on AmazonMechanical Turk, obtaining each label through avote of three, with an agreement of ?
= .61.
How-ever, the agreement of the subjectivity and rele-vance labelings after voting, assuming that sub-jectivity equals relevance, is only at ?
= .48.This suggests that there is indeed a measurabledifference between subjectivity and relevance.
Anannotator who we asked to examine the 225 ex-amples where the annotations disagree found that83.5% of these cases are true differences.2.2 Contrastive Classification ExperimentWe will now examine the similarities of S-relevance and an existing subjectivity dataset.Pang and Lee (2004) introduced subjectivity data(henceforth P&L corpus) that consists of 5000highly subjective (quote) review snippets from rot-tentomatoes.com and 5000 objective (plot) sen-tences from IMDb plot descriptions.We now show that although the P&L selectioncriteria (quotes, plot) bear resemblance to the def-inition of S-relevance, the two concepts are differ-ent.We use quote as S-relevant and plot as S-nonrelevant data in TL.
We divide both the SRand P&L corpora into training (50%) and test sets(50%) and train a Maximum Entropy (MaxEnt)classifier (Manning and Klein, 2003) with bag-of-word features.
Macro-averaged F1 for the fourpossible training-test combinations is shown in Ta-ble 1.
The results clearly show that the classesdefined by the two labeled sets are different.
Aclassifier trained on P&L performs worse by about8% on SR than a classifier trained on SR (68.5 vs.76.4).
A classifier trained on SR performs worseby more than 20% on P&L than a classifier trainedon P&L (67.4 vs. 89.7).Note that the classes are not balanced in theS-relevance data while they are balanced in thesubjectivity data.
This can cause a misestimation2http://www.ims.uni-stuttgart.de/forschung/ressourcen/korpora/sentimentrelevance/955O SNR Braxton is a gambling addict in deep to Mook (Ellen Burstyn), a local bookie.S SNR Kennesaw is bitter about his marriage to a socialite (Rosanna Arquette), believing his wifeto be unfaithful.S SR The plot is twisty and complex, with lots of lengthy flashbacks, and plenty of surprises.S SR However, there are times when it is needlessly complex, and at least one instance thestorytelling turns so muddled that the answers to important plot points actually get lost.S SR Take a look at L. A.
Confidential, or the film?s more likely inspiration, The Usual Suspectsfor how a complex plot can properly be handled.Figure 1: Example data from the SR corpus with subjectivity (S/O) and S-relevance (SR/SNR) annota-tionstestP&L SRtrain P&L 89.7 68.5SR 67.4 76.4Table 1: TL/in-task F1 for P&L and SR corporavocabulary fpSR fpSNR{actor, director, story} 0 7.5{good, bad, great} 11.5 4.8Table 2: % incorrect sentences containing specificwordsof class probabilities and lead to the experiencedperformance drops.
Indeed, if we either balancethe S-relevance data or unbalance the subjectivitydata, we can significantly increase F1 to 74.8%and 77.9%, respectively, in the noisy label trans-fer setting.
Note however that this step is difficultin practical applications if the actual label distri-bution is unknown.
Also, in a real practical ap-plication the distribution of the data is what it is ?it cannot be adjusted to the training set.
We willshow in Section 7 that using an unsupervised se-quence model is superior to artificial manipulationof class-imbalances.An error analysis for the classifier trained onP&L shows that many sentences misclassified asS-relevant (fpSR) contain polar words; for exam-ple, Then, the situation turns bad.
In contrast, sen-tences misclassified as S-nonrelevant (fpSNR) con-tain named entities or plot and movie business vo-cabulary; for example, Tim Roth delivers the mostimpressive acting job by getting the body languageright.The word count statistics in Table 2 show thisfor three polar words and for three plot/moviebusiness words.
The P&L-trained classifier seemsto have a strong bias to classify sentences with po-lar words as S-relevant even if they are not, per-haps because most training instances for the cat-egory quote are highly subjective, so that thereis insufficient representation of less emphatic S-relevant sentences.
These snippets rarely con-tain plot/movie-business words, so that the P&L-trained classifier assigns almost all sentences withsuch words to the category S-nonrelevant.3 Related WorkMany publications have addressed subjectivity insentiment analysis.
Two important papers that arebased on the original philosophical definition ofthe term (internal state of mind vs. external real-ity) are (Wilson and Wiebe, 2003) and (Riloff andWiebe, 2003).
As we argue above, if the goal is toidentify parts of a document that are useful/non-useful for sentiment analysis, then S-relevance isa better notion to use.Researchers have implicitly deviated from thephilosophical definition because they were primar-ily interested in satisfying the needs of a particulartask.
For example, Pang and Lee (2004) use a min-imum cut graph model for review summarization.Because they do not directly evaluate the resultsof subjectivity classification, it is not clear to whatextent their method is able to identify subjectivitycorrectly.In general, it is not possible to know what theunderlying concepts of a statistical classificationare if no detailed annotation guidelines exist andno direct evaluation of manually labeled data isperformed.Our work is most closely related to (Taboadaet al, 2009) who define a fine-grained classifica-tion that is similar to sentiment relevance on thehighest level.
However, unlike our study, theyfail to experimentally compare their classificationscheme to prior work in their experiments and956to show that this scheme is different.
In addi-tion, they work on the paragraph level.
How-ever, paragraphs often contain a mix of S-relevantand S-nonrelevant sentences.
We use the mini-mum cut method and are therefore able to incorpo-rate discourse-level constraints in a more flexiblefashion, giving preference to ?relevance-uniform?paragraphs without mandating them.Ta?ckstro?m and McDonald (2011) develop afine-grained annotation scheme that includes S-nonrelevance as one of five categories.
However,they do not use the category S-nonrelevance di-rectly in their experiments and do not evaluateclassification accuracy for it.
We do not use theirdata set as it would cause domain mismatch be-tween the product reviews they use and the avail-able movie review subjectivity data (Pang and Lee,2004) in the TL approach.
Changing both the do-main (movies to products) and the task (subjectiv-ity to S-relevance) would give rise to interactionsthat we would like to avoid in our study.The notion of annotator rationales (Zaidan etal., 2007) has some overlap with our notion ofsentiment relevance.
Yessenalina et al (2010)use rationales in a multi-level model to integratesentence-level information into a document classi-fier.
Neither paper presents a direct gold standardevaluation of the accuracy of rationale detection.In summary, no direct evaluation of sentimentrelevance has been performed previously.
Onecontribution in this paper is that we provide asingle-domain gold standard for sentiment rele-vance, created based on clear annotation guide-lines, and use it for direct evaluation.Sentiment relevance is also related to reviewmining (e.g., (Ding et al, 2008)) and sentimentretrieval techniques (e.g., (Eguchi and Lavrenko,2006)) in that they aim to find phrases, sentencesor snippets that are relevant for sentiment, eitherwith respect to certain features or with a focus onhigh-precision retrieval (cf.
(Liu, 2010)).
How-ever, finding a few S-relevant items with high pre-cision is much easier than the task we address: ex-haustive classification of all sentences.Another contribution is that we show that gen-eralization based on semantic classes improves S-relevance classification.
While previous work hasshown the utility of other types of feature gen-eralization for sentiment and subjectivity analysis(e.g., syntax and part-of-speech (Riloff and Wiebe,2003)), semantic classes have so far not been ex-ploited.Named-entity features in movie reviews werefirst used by Zhuang et al (2006), in the formof feature-opinion pairs (e.g., a positive opinionabout the acting).
They show that recognizing plotelements (e.g., script) and classes of people (e.g.,actor) benefits review summarization.
We followtheir approach by using IMDb to define namedentity features.
We extend their work by intro-ducing methods for labeling partial uses of namesand pronominal references.
We address a differentproblem (S-relevance vs. opinions) and use differ-ent methods (graph-based and statistical vs. rule-based).Ta?ckstro?m and McDonald (2011) also solve asimilar sequence problem by applying a distantlysupervised classifier with an unsupervised hiddensequence component.
Their setup differs fromours as our focus lies on pattern-based distant su-pervision instead of distant supervision using doc-uments for sentence classification.Transfer learning has been applied previously insentiment analysis (Tan and Cheng, 2009), target-ing polarity detection.4 MethodsDue to the sequential properties of S-relevance (cf.Taboada et al (2009)), we impose the discourseconstraint that an S-relevant (resp.
S-nonrelevant)sentence tends to follow an S-relevant (resp.
S-nonrelevant) sentence.
Following Pang and Lee(2004), we use minimum cut (MinCut) to formal-ize this discourse constraint.For a document with n sentences, we create agraph with n + 2 nodes: n sentence nodes andsource and sink nodes.
We define source andsink to represent the classes S-relevance and S-nonrelevance, respectively, and refer to them asSR and SNR.The individual weight ind(s, x) between a sen-tence s and the source/sink node x ?
{SR,SNR}is weighted according to some confidence mea-sure for assigning it to the corresponding class.The weight on the edge from the document?sith sentence si to its j th sentence sj is set toassoc(si, sj) = c/(j ?
i)2 where c is a parame-ter (cf.
(Pang and Lee, 2004)).
The minimum cutis a tradeoff between the confidence of the clas-sification decisions and ?discourse coherence?.The discourse constraint often has the effect thathigh-confidence labels are propagated over the se-957quence.
As a result, outliers with low confidenceare eliminated and we get a ?smoother?
label se-quence.To compute minimum cuts, we use the push-relabel maximum flow method (Cherkassky andGoldberg, 1995).3We need to find values for multiple free param-eters related to the sequence model.
Supervisedoptimization is impossible as we do not have anylabeled data.
We therefore resort to a proxy mea-sure, the run count.
A run is a sequence of sen-tences with the same label.
We set each param-eter p to the value that produces a median runcount that is closest to the true median run count(or, in case of a tie, closest to the true mean runcount).
We assume that the optimal median/meanrun count is known.
In practice, it can be estimatedfrom a small number of documents.
We find theoptimal value of p by grid search.5 FeaturesChoosing features is crucial in situations whereno high-quality training data is available.
We areinterested in features that are robust and supportgeneralization.
We propose two linguistic featuretypes for S-relevance classification that meet theserequirements.5.1 Generalization through SemanticFeaturesDistant supervision and transfer learning are set-tings where exact training data is unavailable.
Wetherefore introduce generalization features whichare more likely to support knowledge transfer.
Togeneralize over concepts, we use knowledge fromtaxonomies.
A set of generalizations can be in-duced by making a cut in the taxonomy and defin-ing the concepts there as base classes.
For nouns,the taxonomy is WordNet (Miller, 1995) for whichCoreLex (Buitelaar, 1998) gives a set of basictypes.
For verbs, VerbNet (Kipper et al, 2008)already contains base classes.We add for each verb in VerbNet and for eachnoun in CoreLex its base class or basic type asan additional feature where words tagged by themate tagger (Bohnet, 2010) as NN.
* are treated asnouns and words tagged as VB.
* as verbs.
For ex-ample, the verb suggest occurs in the VerbNet baseclass say, so we add a feature VN:say to the fea-3using the HIPR tool (www.avglab.com/andrew/soft.html)ture representation.
We refer to these feature setsas CoreLex (CX) and VerbNet (VN) features and totheir combination as semantic features (SEM).5.2 Named EntitiesAs standard named entity recognition (NER) sys-tems do not capture categories that are relevant tothe movie domain, we opt for a lexicon-based ap-proach similar to (Zhuang et al, 2006).
We usethe IMDb movie metadata database4 from whichwe extract names for the categories <ACTOR>,<PERSONNEL> (directors, screenwriters, andcomposers), and <CHARACTER> (movie charac-ters).
Many entries are unsuitable for NER, e.g.,dog is frequently listed as a character.
We filterout all words that also appear in lower case in a listof English words extracted from the dict.cc dictio-nary.5A name n can be ambiguous between the cat-egories (e.g., John Williams).
We disambiguateby calculating the maximum likelihood estimateof p(c|n) = f(n,c)Pc?
f(n,c?
)where c is one of thethree categories and f(n, c) is the number of timesn occurs in the database as a member of cat-egory c. We also calculate these probabilitiesfor all tokens that make up a name.
While thiscan cause false positives, it can help in manycases where the name obviously belongs to a cat-egory (e.g., Skywalker in Luke Skywalker is verylikely a character reference).
We always inter-pret a name preceding an actor in parenthesesas a character mention, e.g., Reese Witherspoonin Tracy Flick (Reese Witherspoon) is an over-achiever [.
.
. ]
This way, we can recognize charac-ter mentions for which IMDb provides insufficientinformation.In addition, we use a set of simple rules to prop-agate annotations to related terms.
If a capitalizedword occurs, we check whether it is part of an al-ready recognized named entity.
For example, ifwe encounter Robin and we previously encoun-tered Robin Hood, we assume that the two enti-ties match.
Personal pronouns will match the mostrecently encountered named entity.
This rule hasprecedence over NER, so if a name matches a la-beled entity, we do not attempt to label it throughNER.The aforementioned features are encoded as bi-nary presence indicators for each sentence.
This4www.imdb.com/interfaces/5dict.cc958feature set is referred to as named entities (NE).5.3 Sequential FeaturesFollowing previous sequence classification workwith Maximum Entropy models (e.g., (Ratna-parkhi, 1996)), we use selected features of adja-cent sentences.
If a sentence contains a feature F,we add the feature F+1 to the following sentence.For example, if a <CHARACTER> feature occursin a sentence, <CHARACTER+1> is added to thefollowing sentence.
For S-relevance classification,we perform this operation only for NE features asthey are restricted to a few classes and thus willnot enlarge the feature space notably.
We refer tothis feature set as sequential features (SQ).6 Distant SupervisionSince a large labeled resource for sentiment rele-vance classification is not yet available, we inves-tigate semi-supervised methods for creating sen-timent relevance classifiers.
In this section, weshow how to bootstrap a sentiment relevance clas-sifier by distant supervision (DS) .Even though we do not have sentiment rele-vance annotations, there are sources of metadataabout the movie domain that we can leverage fordistant supervision.
Specifically, movie databaseslike IMDb contain both metadata about the plot,in particular the characters of a movie, and meta-data about the ?creators?
who were involved in theproduction of the movie: actors, writers, direc-tors, and composers.
On the one hand, statementsabout characters usually describe the plot and arenot sentiment relevant and on the other hand, state-ments about the creators tend to be evaluations oftheir contributions ?
positive or negative ?
to themovie.
We formulate a classification rule basedon this observation: Count occurrences of NE fea-tures and label sentences that contain a majorityof creators (and tied cases) as SR and sentencesthat contain a majority of characters as SNR.
Thissimple labeling rule covers 1583 sentences withan F1 score of 67.2% on the SR corpus.
We callthese labels inferred from NE metadata distant su-pervision (DS) labels.
This is a form of distantsupervision in that we use the IMDb database asdescribed in Section 5 to automatically label sen-tences based on which metadata from the databasethey contain.To increase coverage, we train a Maximum En-tropy (MaxEnt) classifier (Manning and Klein,2003) on the labels.
The MaxEnt model achievesan F1 of 61.2% on the SR corpus (Table 3, line 2).As this classifier uses training data that is biasedtowards a specialized case (sentences containingthe named entity types creators and characters),it does not generalize well to other S-relevanceproblems and thus yields lower performance onthe full dataset.
This distant supervision setup suf-fers from two issues.
First, the classifier only seesa subset of examples that contain named entities,making generalization to other types of expres-sions difficult.
Second, there is no way to controlthe quality of the input to the classifier, as we haveno confidence measure for our distant supervisionlabeling rule.
We will address these two issues byintroducing an intermediate step, the unsupervisedsequence model introduced in Section 4.As described in Section 4, each document isrepresented as a graph of sentences and weightsbetween sentences and source/sink nodes repre-senting SR/SNR are set to the confidence valuesobtained from the distantly trained MaxEnt clas-sifier.
We then apply MinCut as described in thefollowing paragraphs and select the most confidentexamples as training material for a new classifier.6.1 MinCut SetupWe follow the general MinCut setup described inSection 4.
As explained above, we assume thatcreators and directors indicate relevance and char-acters indicate nonrelevance.
Accordingly, wedefine nSR to be the number of <ACTOR> and<PERSONNEL> features occurring in a sentence,and nSNR the number of <CHARACTER> features.We then set the individual weight between a sen-tence and the source/sink nodes to ind(s, x) = nxwhere x ?
{SR,SNR}.
The MinCut parameter cis set to 1; we wish to give the association scoreshigh weights as there might be long spans thathave individual weights with zero values.6.2 Confidence-based Data SelectionWe use the output of the base classifier to train su-pervised models.
Since the MinCut model is basedon a weak assumption, it will make many false de-cisions.
To eliminate incorrect decisions, we onlyuse documents as training data that were labeledwith high confidence.
As the confidence measurefor a document, we use the maximum flow value f?
the ?amount of fluid?
flowing through the docu-ment.
The max-flow min-cut theorem (Ford andFulkerson, 1956) implies that if the flow value959Model Features FSR FSNR Fm1 Majority BL ?
88.3 0.0 44.22 MaxEnt (DSlabels) NE 79.8 42.6 61.213 DSlabels+MinCut NE 79.6 48.2 63.9124 DS MaxEnt NE 84.8 46.4 65.6125 DS MaxEnt NE+SEM 85.2 48.0 66.61246 DS CRF NE 83.4 49.5 66.4127 DS MaxEnt NE+SQ 84.8 49.2 67.012348 DS MaxEnt NE+SQ+SEM 84.5 49.1 66.81234Table 3: Classification results: FSR (S-relevant F1), FSNR (S-nonrelevant F1), and Fm (macro-averagedF1).
Superscript numbers indicate a significant improvement over the corresponding line.is low, then the cut was found more quickly andthus can be easier to calculate; this means that thesentence is more likely to have been assigned tothe correct segment.
Following this assumption,we train MaxEnt and Conditional Random Field(CRF, (McCallum, 2002)) classifiers on the k%of documents that have the lowest maximum flowvalues f , where k is a parameter which we op-timize using the run count method introduced inSection 4.6.3 Experiments and ResultsTable 3 shows S-relevant (FSR), S-nonrelevant(FSNR) and macro average (Fm) F1 values for dif-ferent setups with this parameter.
We compare thefollowing setups: (1) The majority baseline (BL)i.e., choosing the most frequent label (SR).
(2) aMaxEnt baseline trained on DS labels without ap-plication of MinCut; (3) the base classifier usingMinCut (DSlabels+MinCut) as described above.Conditions 4-8 train supervised classifiers basedon the labels from DSlabels+MinCut: (4) MaxEntwith named entities (NE); (5) MaxEnt with NEand semantic (SEM) features; (6) CRF with NE;(7) MaxEnt with NE and sequential (SQ) features;(8) MaxEnt with NE, SQ, and SEM.We test statistical significance using the approx-imate randomization test (Noreen, 1989) on doc-uments with 10,000 iterations at p < .05.
Weachieve classification results above baseline usingthe MinCut base classifier (line 3) and a consider-able improvement through distant supervision.
Wefound that all classifiers using DS labels and Min-cut are significantly better than MaxEnt trained onpurely rule-based DS labels (line 2).
Also, theMaxEnt models using SQ features (lines 7,8) aresignificantly better than the MinCut base classi-fier (line 3).
For comparison to a chain-based se-quence model, we train a CRF (line 6); however,the improvement over MaxEnt (line 4) is not sig-nificant.We found that both semantic (lines 5,8) and se-quential (lines 7,8) features help to improve theclassifier.
The best model (line 7) performs bet-ter than MinCut (3) by 3.1% and better than train-ing on purely rule-generated DS labels (line 2) by5.8%.
However, we did not find a cumulative ef-fect (line 8) of the two feature sets.Generally, the quality of NER is crucial in thistask.
While IMDb is in general a thoroughly com-piled database, it is not perfect.
For example, allmain characters in Groundhog Day are listed withtheir first name only even though the full namesare given in the movie.
Also, some entries are in-tentionally incomplete to avoid spoiling the plot.The data also contains ambiguities between char-acters and titles (e.g., Forrest Gump) that are im-possible to resolve with our maximum likelihoodmethod.
In some types of movies, e.g., documen-taries, the distinction between characters and ac-tors makes little sense.
Furthermore, ambiguitieslike occurrences of common names such as Johnare impossible to resolve if there is no earlier fullreferring expression (e.g., John Williams).Feature analysis for the best model using DSlabels (7) shows that NE features are dominant.This correlation is not surprising as the seed la-bels were induced based on NE features.
Interest-ingly, some subjective features, e.g., horrible havehigh weights for S-nonrelevance, as they are asso-ciated with non-relevant content such as plot de-scriptions.To summarize, the results of our experimentsusing distant supervision show that a sentimentrelevance classifier can be trained successfully bylabeling data with a few simple feature rules, with960MinCut-based input significantly outperformingthe baseline.
Named entity recognition, accom-plished with data extracted from a domain-specificdatabase, plays a significant rule in creating an ini-tial labeling.7 Transfer LearningTo address the problem that we do not haveenough labeled SR data we now investigate a sec-ond semi-supervised method for SR classification,transfer learning (TL).
We will use the P&L data(introduced in Section 2.2) for training.
This dataset has labels that are intended to be subjectivitylabels.
However, they were automatically createdusing heuristics and the resulting labels can be ei-ther viewed as noisy SR labels or noisy subjectiv-ity labels.
Compared to distant supervision, thekey advantage of training on P&L is that the train-ing set is much larger, containing around 7 timesas much data.In TL, the key to success is to find a general-ized feature representation that supports knowl-edge transfer.
We use a semantic feature gener-alization method that relies on taxonomies to in-troduce such features.We again use MinCut to impose discourse con-straints.
This time, we first classify the data us-ing a supervised classifier and then use MinCut tosmooth the sequences.
The baseline (BL) uses asimple bag-of-words representation of sentencesfor classification which we then extend with se-mantic features.7.1 MinCut SetupWe again implement the basic MinCut setup fromSection 4.
We set the individual weight ind(s, x)on the edge between sentence s and class x to theestimate p(x|s) returned by the supervised classi-fier.
The parameter c of the MinCut model is tunedusing the run count method described in Section 4.7.2 Experiments and ResultsAs we would expect, the baseline performance ofthe supervised classifier on SR is low: 69.9% (Ta-ble 4, line 1).
MinCut significantly boosts the per-formance by 7.9% to 77.5% (line 1), a result sim-ilar to (Pang and Lee, 2004).
Adding semanticfeatures improves supervised classification signif-icantly by 5.7% (75.6% on line 4).
When MinCutand both types of semantic features are used to-gether, these improvements are partially cumula-0.2 0.4 0.6 0.8 1.0246810crunlength777879808182F 1median run countmean run countF1Figure 2: F1 measure for different values of c.Horizontal line: optimal median run count.
Cir-cle: selected point.tive: an improvement over the baseline by 12.6%to 82.5% (line 4).We also experiment with a training set where anartificial class imbalance is introduced, matchingthe 80:20 imbalance of SR:SNR in the S-relevancecorpus.
After applying MinCut, we find that whilethe results for BL with and without imbalancesdoes not differ significantly.
However, models us-ing CX and VN features and imbalances are ac-tually significantly inferior to the respective bal-anced versions.
This result suggests that MinCutis more effective at coping with class imbalancesthan artificial balancing.MinCut and semantic features are successful forTL because both impose constraints that are moreuseful in a setup where noise is a major problem.MinCut can exploit test set information withoutsupervision as the MinCut graph is built directlyon each test set review.
If high-confidence infor-mation is ?seeded?
within a document and thenspread to neighbors, mistakes with low confidenceare corrected.
This way, MinCut also leads to acompensation of different class imbalances.The results are evidence that semantic featuresare robust to the differences between subjectivityand S-relevance (cf.
Section 2).
In the CX+VNmodel, meaningful feature classes receive highweights, e.g., the human class from CoreLexwhich contains professions that are frequently as-sociated with non-relevant plot descriptions.To illustrate the run-based parameter optimiza-tion criterion, we show F1 and median/mean runlengths for different values of c for the best TL961Model base classifier MinCutFSR FSNR Fm FSR FSNR Fm1 BL 81.1 58.6 69.9 87.2 67.8 77.5B2 CX 82.9 60.1 71.5B 89.0 70.3 79.7BM3 VN 85.6 62.1 73.9B 91.4 73.6 82.5BM4 CX+VN 88.3 62.9 75.6B 92.7 72.2 82.5BMTable 4: Classification results: FSR (S-relevant F1), FSNR (S-nonrelevant F1), and Fm (macro-averagedF1).
B indicates a significant improvement over the BL base classifier (69.9), M over BL MinCut (77.5).setting (line 4) in Figure 2.
Due to differences inthe base classifier, the optimum of c may vary be-tween the experiments.
A weaker base classifiermay yield a higher weight on the sequence model,resulting in a larger c. The circled point shows thedata point selected through optimization.
The op-timization criterion does not always correlate per-fectly with F1.
However, we find no statisticallysignificant difference between the selected resultand the highest F1 value.These experiments demonstrate that S-relevance classification improves considerablythrough TL if semantic feature generalizationand unsupervised sequence classification throughMinCut are applied.8 ConclusionA number of different notions, including subjec-tivity, have been proposed for distinguishing partsof documents that convey sentiment from thosethat do not.
We introduced sentiment relevance tomake this distinction and argued that it better re-flects the requirements of sentiment analysis sys-tems.
Our experiments demonstrated that senti-ment relevance and subjectivity are related, butdifferent.
To enable other researchers to use thisnew notion of S-relevance, we have published theannotated S-relevance corpus used in this paper.Since a large labeled sentiment relevance re-source does not yet exist, we investigated semi-supervised approaches to S-relevance classifica-tion that do not require S-relevance-labeled data.We showed that a combination of different tech-niques gives us the best results: semantic gener-alization features, imposing discourse constraintsimplemented as the minimum cut graph-theoreticmethod, automatic ?distant?
labeling based on adomain-specific metadata database and transferlearning to exploit existing labels for a relatedclassification problem.In future work, we plan to use sentiment rele-vance in a downstream task such as review sum-marization.AcknowledgmentsThis work was funded by the DFG through theSonderforschungsbereich 732.
We thank CharlesJochim, Wiltrud Kessler, and Khalid Al Khatib formany helpful comments and discussions.ReferencesBernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics (Coling 2010), pages 89?97, Bei-jing, China, August.
Coling 2010 Organizing Com-mittee.P.
Buitelaar.
1998.
CoreLex: systematic polysemy andunderspecification.
Ph.D. thesis, Brandeis Univer-sity.B.
Cherkassky and A. Goldberg.
1995.
On imple-menting push-relabel method for the maximum flowproblem.
Integer Programming and CombinatorialOptimization, pages 157?171.X.
Ding, B. Liu, and P. S. Yu.
2008.
A holistic lexicon-based approach to opinion mining.
In WSDM 2008,pages 231?240.K.
Eguchi and V. Lavrenko.
2006.
Sentiment retrievalusing generative models.
In EMNLP 2006, pages345?354.L.R.
Ford and D.R.
Fulkerson.
1956.
Maximal flowthrough a network.
Canadian Journal of Mathemat-ics, 8(3):399?404.K.
Kipper, A. Korhonen, N. Ryant, and M. Palmer.2008.
A large-scale classification of English verbs.Language Resources and Evaluation, 42(1):21?40.B.
Liu.
2010.
Sentiment analysis and subjectivity.Handbook of Natural Language Processing, pages978?1420085921.C.
Manning and D. Klein.
2003.
Optimization, maxentmodels, and conditional estimation without magic.In NAACL-HLT 2003: Tutorials, page 8.962A.K.
McCallum.
2002.
Mallet: A machine learningfor language toolkit.G.A.
Miller.
1995.
Wordnet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.E.W.
Noreen.
1989.
Computer Intensive Methods forHypothesis Testing: An Introduction.
Wiley.B.
Pang and L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In ACL 2004, pages 271?278.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
: sentiment classification using machine learningtechniques.
In ACL-EMNLP 2002, pages 79?86.A.M.
Popescu and O. Etzioni.
2005.
Extracting prod-uct features and opinions from reviews.
In Proceed-ings of the conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing, pages 339?346.
Association for Compu-tational Linguistics.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proceedings of the con-ference on empirical methods in natural languageprocessing, volume 1, pages 133?142.E.
Riloff and J. Wiebe.
2003.
Learning extraction pat-terns for subjective expressions.
In EMNLP 2003,pages 105?112.M.
Taboada, J. Brooke, and M. Stede.
2009.
Genre-based paragraph classification for sentiment analy-sis.
In SIGdial 2009, pages 62?70.O.
Ta?ckstro?m and R. McDonald.
2011.
Discover-ing fine-grained sentiment with latent variable struc-tured prediction models.
In ECIR 2011, pages 368?374.S.
Tan and X. Cheng.
2009.
Improving SCL modelfor sentiment-transfer learning.
In ACL 2009, pages181?184.S.
Thrun.
1996.
Is learning the n-th thing any easierthan learning the first?
In NIPS 1996, pages 640?646.T.
Wilson and J. Wiebe.
2003.
Annotating opinions inthe world press.
In 4th SIGdial Workshop on Dis-course and Dialogue, pages 13?22.A.
Yessenalina, Y. Yue, and C. Cardie.
2010.
Multi-level structured models for document-level senti-ment classification.
In EMNLP 2010, pages 1046?1056.O.
Zaidan, J. Eisner, and C. Piatko.
2007.
Using anno-tator rationales to improve machine learning for textcategorization.
In NAACL-HLT 2007, pages 260?267.L.
Zhuang, F. Jing, and X. Zhu.
2006.
Movie reviewmining and summarization.
In CIKM 2006, pages43?50.963
