Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 23?27,Baltimore, Maryland, USA, June 26, 2014.c?2014 Association for Computational LinguisticsFinding Eyewitness Tweets During CrisesFred Morstatter1, Nichola Lubold1, Heather Pon-Barry1, J ?urgen Pfeffer2, and Huan Liu11Arizona State University, Tempe, Arizona, USA2Carnegie Mellon University, Pittsburgh, Pennsylvania, USA{fred.morstatter, nlubold, ponbarry, huan.liu}@asu.edu, jpfeffer@cs.cmu.eduAbstractDisaster response agencies incorporate so-cial media as a source of fast-breaking in-formation to understand the needs of peo-ple affected by the many crises that oc-cur around the world.
These agencies lookfor tweets from within the region affectedby the crisis to get the latest updates onthe status of the affected region.
Howeveronly 1% of all tweets are ?geotagged?
withexplicit location information.
In this workwe seek to identify non-geotagged tweetsthat originate from within the crisis region.Towards this, we address three questions:(1) is there a difference between the lan-guage of tweets originating within a crisisregion, (2) what linguistic patterns differ-entiate within-region and outside-regiontweets, and (3) can we automatically iden-tify those originating within the crisis re-gion in real-time?1 IntroductionDue to Twitter?s massive popularity, it has becomea tool used by first responders?those who providefirst-hand aid in times of crisis?to understand cri-sis situations and identify the people in the mostdire need of assistance (United Nations, 2012).
Todo this, first responders can survey ?geotagged?tweets: those where the user has supplied a ge-ographic location.
The advantage of geotaggedtweets is that first responders know whether a per-son is tweeting from within the affected region oris tweeting from afar.
Tweets from within thisregion are more likely to contain emerging top-ics (Kumar et al., 2013) and tactical, actionable,information that contribute to situational aware-ness (Verma et al., 2011).A major limitation of surveying geotaggedtweets is that only 1% of all tweets are geo-tagged (Morstatter et al., 2013).
This leaves thefirst responders unable to tap into the vast major-ity of the tweets they collect.
This limitation leadsto the question driving this work: can we discoverwhether a tweet originates from within a crisis re-gion using only the language used of the tweet?We focus on the language of a tweet as thedefining factor of location for three major reasons:(1) the language of Twitter users is dependent ontheir location (Cheng et al., 2010), (2) the text isreadily available in every tweet, and (3) the text al-lows for real-time analysis.
Due to the short timewindow presented by most crises, first respondersneed to be able to locate users quickly.Towards this goal, we examine tweets from tworecent crises: the Boston Marathon bombing andHurricane Sandy.
We show that linguistic differ-ences exist between tweets authored inside andoutside the affected regions.
By analyzing the textof individual tweets we can predict whether thetweet originates from within the crisis region, inreal-time.
To better understand the characteristicsof crisis-time language on Twitter, we concludewith a discussion of the linguistic features that ourmodels find most discriminative.2 Language Differences in CrisesIn order for a language-based approach to be ableto distinguish tweets inside of the crisis region, thelanguage used by those in the region during cri-sis has to be different from those outside.
In thissection, we verify that there are both regional andtemporal differences in the language tweeted.
Tostart, we introduce the data sets we use throughoutthe rest of this paper.
We then measure the differ-ence in language, finding that language changestemporally and regionally at the time of the crisis.2.1 Twitter Crisis DatasetsThe Twitter data used in our experiments comesfrom two crises: the Boston Marathon bombingand Hurricane Sandy.
Both events provoked a sig-nificant Twitter response from within and beyond23Table 1: Properties of the Twitter crisis datasets.Property Boston SandyCrisis Start 15 Apr 14:48 29 Oct 20:00Crisis End 16 Apr 00:00 30 Oct 01:00Epicenter 42.35, ?71.08 40.75, ?73.99Radius 19 km 20 km|IR| 11,601 5,017|OR| 541,581 195,957|PC-IR| 14,052 N/A|PC-OR| 228,766 N/Athe affected regions.The Boston Marathon Bombing occurred atthe finish line of the Boston Marathon on April15th, 2013 at 14:48 Eastern.
We collected geo-tagged tweets from the continental United Statesfrom 2013-04-09 00:00 to 2013-04-22 00:00 uti-lizing Twitter?s Filter API.Hurricane Sandy was a ?superstorm?
thatravaged the Eastern United States in October,2012.
Utilizing Twitter?s Filter API, we collectedtweets based on several keywords pertaining to thestorm.
Filtering by keywords, this dataset containsboth geotagged and non-geotagged data beginningfrom the day the storm made landfall (2012-10-29)to several days after (2012-11-02).2.2 Data PartitioningFor the Boston Bombing and Hurricane Sandydatasets, we partitioned the tweets published dur-ing the crisis time into two distinct parts based onlocation: (1) inside the crisis region (IR), and (2)outside the crisis region (OR).For the Boston Bombing dataset, we are able toextract two additional groups: (1) pre-crisis tweets(posted before the time of the crisis) from insidethe crisis region (PC-IR) and (2) pre-crisis tweetsfrom outside the crisis region (PC-OR).
We take atime-based sample from 10:00?14:48 Eastern onApril 15th, 2013 to obtain PC-IR and PC-OR.Because the bombing was an abrupt event withno warning, we choose a time period immediatelypreceding its onset.
The number of tweets in eachdataset partition is shown in Table 1.2.3 Pre-Crisis vs. During-Crisis LanguageFor the Boston dataset, we compare the wordsused hour by hour between 10:00?19:00 on April15th.
For each pair of hours, we compute theJensen-Shannon (J-S) divergence (Lin, 1991) ofthe probability distributions of the words used(a) Temporal lan-guage differences.
(b) Geographic lan-guage differences:tranquil time.
(c) Geographic lan-guage differences:crisis time.Figure 1: Temporal and geographic differences oflanguage (calculated using Jensen-Shannon diver-gence); darker shades represent greater difference.To illustrate geographic differences, we compareBoston with three other major U.S. cities.within those hours.
Figure 1(a) shows these J-Sdivergence values.
We see an abrupt change inlanguage in the hours before the bombing (10:00?14:00) and those after the bombing (15:00?19:00).We also note that the tranquil hours are relativelystable.
This suggests that language models trainedon tweets from tranquil time are less informativefor modeling crisis-time langauge.2.4 IR vs. OR LanguageWe verify that the tweets authored inside of thecrisis use different words from those outside theregion.
We compare the difference in Boston (B)to three other major U.S. cities: Chicago (C), LosAngeles (L), and Miami (M).
To obtain a base-line, we compare the cities during tranquil timesusing PC-IR and PC-OR datasets.
The results areshown in Figure 1.
The tranquil time comparison,shown in Figure 1(b), displays a low divergencebetween all pairs of cities.
In contrast, Figure 1(c)shows a wider divergence between the same cities,with Boston displaying the greatest divergence.3 Linguistic FeaturesAs Twitter is a conversational, real-time, mi-croblogging site, the structure of tweets offersmany opportunities for extracting different typesof features that represent the different linguisticproperties of informal text.
Our approach is tocompare the utility, in classifying tweets as IR orOR, of several linguistic features.
We preprocessthe tweets by extracting tokens using the CMUTwitter NLP tokenizer (Owoputi et al., 2013).Unigrams and Bigrams We extract the raw fre-quency counts of the word unigrams and bigrams.POS Tags We extract part-of-speech tags foreach word in the tweet using the CMU TwitterNLP POS tagger (Owoputi et al., 2013).
We con-24sider CMU ARK POS tags, developed specificallyfor the dynamic and informal nature of tweets, aswell as Penn Treebank (PTB) style POS tags.
TheARK POS tags are coarser than the PTB tags andcan identify Twitter-specific entities in the datalike hashtags.
By comparing both tag sets, we canmeasure the effectiveness of both the fine-grainedversus coarse-grained tag sets.Shallow Parsing In addition to the POS tags,we extract shallow parsing tags along with theheadword associated with the tag using the toolprovided by Ritter et al.
(2011).
For example,in the noun phrase ?the movie?
we would ex-tract the headword ?movie?
and represent it as[...movie...]NP.
The underlying motivation is thatthis class may give more insight into the syntacticdifferences of IR tweets versus OR tweets.Crisis-Sensitive (CS) Features We create amixed-class of ?crisis sensitive?
features com-posed of word-based, part of speech, and syntac-tic constituent attributes.
These are based on ouranalysis of the Boston Marathon data set.
We laterapply these features to the Hurricane Sandy dataset to validate whether the features are generaliz-able across crises and discuss this in the results.
?We extract ?in?
prepositional phrases of theform [in ... /N]PP.
For example, ?in Boston.?
Themotivation is this use of ?in,?
such as with a lo-cation or a nonspecific time, may be indicative ofcrisis language.?
We extract verbs in relationship to the exis-tential there.
As the existential there is usuallythe grammatical subject and describes an abstrac-tion, it may be indicative of situational awarenessmessages within the disaster region.?
Part-of-Speech tag sequences that are fre-quent in IR tweets (from our development set) aregiven special consideration.
We find sequenceswhich are used more widely during the time of thisdisaster.
Some of the ARK tag sequences include:?N R?, ?L A?, ?N P?, ?P D N?, ?L A !
?, ?A N P?.4 ExperimentsHere, we assess the effectiveness of our linguisticfeatures at the task of identifying tweets originat-ing from within the crisis region.
To do this weuse a Na?
?ve Bayes classifier configured with anindividual set of feature classes.
Each of our fea-tures are represented as raw frequency counts ofthe number of times they occur within the tweet.The output is a prediction of whether the tweetis inside region (IR) or outside region (OR).
WeTable 2: Top Feature Combinations: Unigrams(Uni), Bigrams (Bi) and Crisis-Sensitive (CS)combinations have the best results.Top Feature Combos Prec.
Recall F1Boston BombingUni + Bi 0.853 0.805 0.828Uni + Bi + Shallow Parse 0.892 0.771 0.828Uni + Bi + CS 0.857 0.806 0.831All Features 0.897 0.742 0.812Hurricane SandyUni + Bi 0.942 0.820 0.877Uni + Bi + Shallow Parse + CS 0.956 0.803 0.873Uni + Bi + CS 0.947 0.826 0.882All Features 0.960 0.786 0.864identify the features that can differentiate the twoclasses of users, and we show that this process canindeed be automated.4.1 Experiment ProcedureWe ensure a 50/50 split of IR and OR instancesby sampling the OR dataset.
Using the classifierdescribed above, we perform 3?
5-fold cross val-idation on the data.
Because of the 50/50 split,a ?select-all?
baseline that labels all tweets as IRwill have an accuracy of 50%, a precision of 50%,and a recall of 100%.
All precision and recall val-ues are from the perspective of the IR class.4.2 Feature Class AnalysisWe compare all possible combinations of individ-ual feature classes and we report precision, recall,and F1-scores for the best combinations in Table 2.In both crises all of the top performing fea-ture combinations contain both bigram and uni-gram feature classes.
However, our top perform-ing feature combinations demonstrate that bigramsin combination with unigrams have added util-ity.
We also see that the crisis-sensitive featuresare present in the top performing combinations forboth data sets.
The CS feature class was derivedfrom Boston Bombing data, so its presence in thetop groups from Hurricane Sandy is an indicationthat these features are general, and may be usefulfor finding users in these and future crises.4.3 Most Informative Linguistic FeaturesTo see which individual features within the classesgive the best information, we make a modificationto the experiment setup described in Section 4.1:we replace the Na?
?ve Bayes classifier with a Logis-tic Regression classifier to utilize the coefficientsit learns as a metric for feature importance.
We re-port the top three features of each class label fromeach feature set in Table 3.The individual unigram and bigram featureswith the most weight have a clear semantic rela-25Table 3: Top 3 features indicative of each class within each feature set for both crises.Feature Set (Class) Boston Marathon Bombing Hurricane SandyUnigram (IR) #prayforboston, boston, explosion @kiirkobangz, upset, stayingUnigram (OR) money, weather, gone #tomyfuturechildren, #tomyfutureson, byeBigram (IR) ?in boston?, ?the marathon?, ?i?m safe?
?railroad :?, ?evacuation zone?, ?storm warning?Bigram (OR) ?i?m at?, ?s/o to?, ?, fl?
?you will?, ?
: i?ve?, ?hurricane ,?ARK POS (IR) ?P $ ?
?, ?L !
?, ?!
R P?
?P #?, ??
?
A?, ?
@ @ #?ARK POS (OR) ?O #?, ?!
N O?, ?L P R?
?P V $?, ?A ?
?
?, ?N L A?PTB POS (IR) ?CD NN JJ?, ?CD VBD?, ?JJS NN TO?
?USR DT JJS?, ?VB TO RB?, ?IN RB JJ?PTB POS (OR) ?NNP -RRB-?, ?.
JJ JJ?, ?JJ NN CD?
?NNS IN NNS?, ?PRP JJ PRP?, ?JJ NNP NNP?Shallow Parse (IR) [...explosion...]NP, [...marathon...]NP,[...bombs...]NP[...bomb...]NP, [...waz...]V P,[...evacuation...]NPShallow Parse (OR) [...school...]NP, [...song...]NP,[...breakfast...]NP[...school...]NP, [...head...]NP, [...wit...]PPCS (IR) [in boston/N]PP, [for boston/N]PP,?i?m/L safe/A?
?while/P a/D hurricane/N?, ?of/P my/Dhouse/N?, [in http://t.co/UxkKJLoX/N]PPCS (OR) ?to/P the/D beach/N?, [at la/N]PP,[in love/N]PP[like water/N]PP, ?shutdowns/N on/P?,?prayer/N for/P?tionship to the crisis.
Comparing the two crises,the top features for Hurricane Sandy are more con-cerned with user-user communication.
For ex-ample, the heavily-weighted ARK POS trigram?
@ @ #?
is highly indicative of users spreadinginformation between each other.
One explanationis that the concern with communication could bea result of the warning that came from the storm.The bigram ?hurricane ,?
is the 3rd most in-dicative of a tweet originating from outside the re-gion.
This is likely because the word occurs in thegeneral discussion outside of the crisis region.5 Related WorkGeolocation: Eisenstein et al.
(2010) first lookedat the problem of using latent variables to ex-plain the distribution of text in tweets.
This prob-lem was revisited from the perspective of geodesicgrids in Wing and Baldridge (2011) and furtherimproved by flexible adaptive grids (Roller et al.,2012).
Cheng et al.
(2010) employed an approachthat looks at a user?s tweets and estimates theuser?s location based on words with a local geo-graphical scope.
Han et al.
(2013) combines tweettext with metadata to predict a user?s location.Mass Emergencies: De Longueville et al.
(2009) study Twitter?s use as a sensor for crisisinformation by studying the geographical proper-ties of users?
tweets.
In Castillo et al.
(2011),the authors analyze the text and social networkof tweets to classify their newsworthiness.
Kumaret al.
(2013) use geotagged tweets to find emerg-ing topics in crisis data.
Investigating linguisticfeatures, Verma et al.
(2011) show the efficacyof language features at finding crisis-time tweetsthat contain tactical, actionable information, con-tributing to situational awareness.
Using a largerdataset, we automatically discover linguistic fea-tures that can help with situational awareness.6 Conclusion and Future WorkThis paper addresses the challenge of findingtweets that originate from a crisis region usingonly the language of each tweet.
We find that thetweets authored from within the crisis region dodiffer, from both tweets published during tranquiltime periods and from tweets published from othergeographic regions.
We compare the utility of sev-eral linguistic feature classes that may help to dis-tinguish the two classes and build a classifier basedon these features to automate the process of iden-tifying the IR tweets.
We find that our classifierperforms well and that this approach is suitable forattacking this problem.Future work includes incorporating the wealthof tweets preceding the disaster for better predic-tions.
Preliminary tests have shown positive re-sults; for example we found early, non-geotaggedreports of flooding in the Hoboken train tunnelsduring Hurricane Sandy1.
Future work may alsoconsider additional features, such as sentiment.AcknowledgmentsThis work is sponsored in part by the Officeof Naval Research, grants N000141010091 andN000141110527, and the Ira A. Fulton Schools ofEngineering, through fellowships to F. Morstatterand N. Lubold.
We thank Alan Ritter and the ARKresearch group at CMU for sharing their tools.1An extended version of this paper is available at: http://www.public.asu.edu/?fmorstat/paperpdfs/lang loc.pdf.26ReferencesCarlos Castillo, Marcelo Mendoza, and BarbaraPoblete.
2011.
Information Credibility on Twitter.In Proceedings of the 20th International Conferenceon World Wide Web, pages 675?684.
ACM.Zhiyuan Cheng, James Caverlee, and Kyumin Lee.2010.
You Are Where You Tweet: A Content-BasedApproach to Geo-locating Twitter Users.
In Pro-ceedings of the 19th ACM International Conferenceon Information and Knowledge Management, pages759?768.
ACM.Bertrand De Longueville, Robin S Smith, and Gian-luca Luraschi.
2009.
?OMG, from here, I cansee the flames!?
: a use case of mining LocationBased Social Networks to acquire spatio-temporaldata on forest fires.
In Proceedings of the 2009 In-ternational Workshop on Location Based Social Net-works, pages 73?80.
ACM.Jacob Eisenstein, Brendan O?Connor, Noah A Smith,and Eric P Xing.
2010.
A Latent Variable Modelfor Geographic Lexical Variation.
In Proceedings ofthe 2010 Conference on Empirical Methods in Nat-ural Language Processing, pages 1277?1287.
Asso-ciation for Computational Linguistics.Bo Han, Paul Cook, and Timothy Baldwin.
2013.
AStacking-based Approach to Twitter User Geoloca-tion Prediction.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (ACL 2013): System Demonstrations, pages7?12.Shamanth Kumar, Fred Morstatter, Reza Zafarani, andHuan Liu.
2013.
Whom Should I Follow?
: Identi-fying Relevant Users During Crises.
In Proceedingsof the 24th ACM Conference on Hypertext and So-cial Media, HT ?13, pages 139?147, New York, NY,USA.
ACM.Jianhua Lin.
1991.
Divergence measures based on theshannon entropy.
IEEE Transactions on InformationTheory, 37(1):145?151.Fred Morstatter, J?urgen Pfeffer, Huan Liu, and Kath-leen M Carley.
2013.
Is the Sample Good Enough?Comparing Data from Twitter?s Streaming API withTwitter?s Firehose.
Proceedings of The Interna-tional Conference on Weblogs and Social Media.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved Part-of-Speech Tagging forOnline Conversational Text with Word Clusters.
InProceedings of NAACL-HLT, pages 380?390.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An ex-perimental study.
In EMNLP.Stephen Roller, Michael Speriosu, Sarat Rallapalli,Benjamin Wing, and Jason Baldridge.
2012.
Super-vised Text-Based Geolocation using Language Mod-els on an Adaptive Grid.
In Proceedings of the 2012Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pages 1500?1510.
Associationfor Computational Linguistics.United Nations.
2012.
Humanitarianism in the Net-work Age.
United Nations Office for the Coordina-tion of Humanitarian Affairs.Sudha Verma, Sarah Vieweg, William J Corvey, LeysiaPalen, James H Martin, Martha Palmer, AaronSchram, and Kenneth Mark Anderson.
2011.
Natu-ral Language Processing to the Rescue?
Extracting?Situational Awareness?
Tweets During Mass Emer-gency.
In ICWSM.Benjamin Wing and Jason Baldridge.
2011.
SimpleSupervised Document Geolocation with GeodesicGrids.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics (ACL2011), pages 955?964.27
