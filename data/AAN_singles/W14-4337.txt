Proceedings of the SIGDIAL 2014 Conference, pages 263?272,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsThe Second Dialog State Tracking ChallengeMatthew Henderson1, Blaise Thomson1and Jason Williams21Department of Engineering, University of Cambridge, U.K.2Microsoft Research, Redmond, WA, USAmh521@eng.cam.ac.uk brmt2@eng.cam.ac.uk jason.williams@microsoft.comAbstractA spoken dialog system, while commu-nicating with a user, must keep track ofwhat the user wants from the system ateach step.
This process, termed dialogstate tracking, is essential for a success-ful dialog system as it directly informs thesystem?s actions.
The first Dialog StateTracking Challenge allowed for evalua-tion of different dialog state tracking tech-niques, providing common testbeds andevaluation suites.
This paper presents asecond challenge, which continues thistradition and introduces some additionalfeatures ?
a new domain, changing usergoals and a richer dialog state.
The chal-lenge received 31 entries from 9 researchgroups.
The results suggest that whilelarge improvements on a competitive base-line are possible, trackers are still proneto degradation in mismatched conditions.An investigation into ensemble learningdemonstrates the most accurate trackingcan be achieved by combining multipletrackers.1 IntroductionSpoken language provides a medium of communi-cation that is natural to users as well as hands- andeyes-free.
Voice-based computer systems, calledspoken dialog systems, allow users to interact us-ing speech to achieve a goal.
Efficient operation ofa spoken dialog system requires a component thatcan track what has happened in a dialog, incor-porating system outputs, user speech and contextfrom previous turns.
The building and evaluationof these trackers is an important field of researchsince the performance of dialog state tracking isimportant for the final performance of a completesystem.Until recently, it was difficult to compare ap-proaches to state tracking because of the wide va-riety of metrics and corpora used for evaluation.The first dialog state tracking challenge (DSTC1)attempted to overcome this by defining a challengetask with standard test conditions, freely availablecorpora and open access (Williams et al., 2013).This paper presents the results of a second chal-lenge, which continues in this tradition with theinclusion of additional features relevant to the re-search community.Some key differences to the first challenge in-clude:?
The domain is restaurant search instead ofbus timetable information.
This provides par-ticipants with a different category of interac-tion where there is a database of matching en-tities.?
Users?
goals are permitted to change.
In thefirst challenge, the user was assumed to al-ways want a specific bus journey.
In this chal-lenge the user?s goal can change.
For exam-ple, they may want a ?Chinese?
restaurant atthe start of the dialog but change to wanting?Italian?
food by the end.?
The dialog state uses a richer representa-tion than in DSTC1, including not only theslot/value attributes of the user goal, but alsotheir search method, and what informationthey wanted the system to read out.As well as presenting the results of the differentstate trackers, this paper attempts to obtain someinsights into research progress by analysing theirperformance.
This includes analyses of the predic-tive power of performance on the development set,the effects of tracking the dialog state using jointdistributions, and the correlation between 1-bestaccuracy and overall quality of probability distri-butions output by trackers.
An evaluation of theeffects of ensemble learning is also performed.The paper begins with an overview of the chal-263lenge in section 2.
The labelling scheme and met-rics used for evaluation are discussed in section 3followed by a summary of the results of the chal-lenge in section 4.
An analysis of ensemble learn-ing is presented in section 5.
Section 6 concludesthe paper.2 Challenge overview2.1 Problem statementThis section defines the problem of dialog statetracking as it is presented in the challenge.
Thechallenge evaluates state tracking for dialogswhere users search for restaurants by specifyingconstraints, and may ask for information such asthe phone number.
The dialog state is formu-lated in a manner which is general to informationbrowsing tasks such as this.Included with the data is an ontology1, whichgives details of all possible dialog states.
Theontology includes a list of attributes termed re-questable slots which the user may request, suchas the food type or phone number.
It also providesa list of informable slots which are attributes thatmay be provided as constraints.
Each informableslot has a set of possible values.
Table 1 gives de-tails on the ontology used in DSTC2.The dialog state at each turn consists of threecomponents:?
The goal constraint for each informable slot.This is either an assignment of a value fromthe ontology which the user has specified asa constraint, or is a special value ?
eitherDontcare which means the user has no pref-erence, or None which means the user is yetto specify a valid goal for this slot.?
A set of requested slots, i.e.
those slotswhose values have been requested by theuser, and should be informed by the system.?
An assignment of the current dialog searchmethod.
This is one of?
by constraints, if the user is attemptingto issue a constraint,?
by alternatives, if the user is requestingalternative suitable venues,?
by name, if the user is attempting to askabout a specific venue by its name,?
finished, if the user wants to end the call?
or none otherwise.Note that in DSTC1, the set of dialog states1Note that this ontology includes only the schema for di-alog states and not the database entrieswas dependent on the hypotheses given by a Spo-ken Language Understanding component (SLU)(Williams et al., 2013), whereas here the state islabelled independently of any SLU (see section 3).Appendix B gives an example dialog with the statelabelled at each turn.A tracker must use information up to a giventurn in the dialog, and output a probability distri-bution over dialog states for the turn.
Trackersoutput separately the distributions for goal con-straints, requested slots and the method.
They mayeither report a joint distribution over the goal con-straints, or supply marginal distributions and letthe joint goal constraint distribution be calculatedas a product of the marginals.2.2 Challenge designDSTC2 studies the problem of dialog state track-ing as a corpus-based task, similar to DSTC1.
Thechallenge task is to re-run dialog state trackingover a test corpus of dialogs.A corpus-based challenge means all trackersare evaluated on the same dialogs, allowing di-rect comparison between trackers.
There is alsono need for teams to expend time and money inbuilding an end-to-end system and getting users,meaning a low barrier to entry.When a tracker is deployed, it will inevitably al-ter the performance of the dialog system it is partof, relative to any previously collected dialogs.
Inorder to simulate this, and to penalise overfitting toknown conditions, evaluation dialogs in the chal-lenge are drawn from dialogs with a dialog man-ager which is not found in the training data.2.3 DataA large corpus of dialogs with various telephone-based dialog systems was collected using Ama-zon Mechanical Turk.
The dialogs used in thechallenge come from 6 conditions; all combina-tions of 3 dialog managers and 2 speech recognis-ers.
There are roughly 500 dialogs in each condi-tion, of average length 7.88 turns from 184 uniquecallers.The 3 dialog managers are:?
DM-HC, a simple tracker maintaining a sin-gle top dialog state, and a hand-crafted policy?
DM-POMDPHC, a dynamic Bayesian net-work for tracking a distribution of dialogstates, and a hand-crafted policy?
DM-POMDP, the same tracking method asDM-POMDPHC, with a policy learnt using264Slot RequestableInformablearea yesyes.
5 values; north,south, east, west, centrefood yesyes, 91 possible valuesname yesyes, 113 possible valuespricerange yesyes, 3 possible valuesaddr yesnophone yesnopostcode yesnosignature yesnoTable 1: Ontology used in DSTC2 for restaurant informa-tion.
Counts do not include the special Dontcare value.POMDP reinforcement learningThe 2 speech recognisers are:?
ASR-degraded, speech recogniser with arti-ficially degraded statistical acoustic models?
ASR-good, full speech recogniser optimisedfor the domainThese give two acoustic conditions, the de-graded model producing dialogs at higher errorrates.
The degraded models simulate in-car con-ditions and are described in Young et al.
(2013).The set of all calls with DM-POMDP, with bothspeech recognition configurations, constitutes thetest set.
All calls with the other two dialog man-agers are used for the training and developmentset.
Specifically, the datasets are arranged as so:?
dstc2 train.
Labelled dataset released in Oc-tober 2013, with 1612 calls from DM-HC andDM-POMDPHC, and both ASR conditions.?
dstc2 dev.
Labelled dataset released at thesame time as dstc2 train, with 506 calls underthe same conditions as dstc2 train.
No callerin this set appears in dstc2 train.?
dstc2 test.
Set used for evaluation.
Releasedunlabelled at the beginning of the evaluationweek.
This consists of all 1117 dialogs withDM-POMDP.Paid Amazon Mechanical Turkers were as-signed tasks and asked to call the dialog systems.Callers were asked to find restaurants that matchedparticular constraints on the slots area, pricerangeand food.
To elicit more complex dialogs, includ-ing changing goals (goals in DSTC1 were alwaysconstant), the users were sometimes asked to findmore than one restaurant.
In cases where a match-ing restaurant did not exist they were required toseek an alternative, for example finding an Indianinstead of an Italian restaurant.A breakdown of the frequency of goal con-straint changes is given in table 2, showing around40% of all dialogs involved a change in goal con-straint.
The distribution of the goal constraints in50100150200Figure 1: Histogram of values for the food constraint (ex-cluding dontcare) in all data.
The most frequent values areIndian, Chinese, Italian and European.Datasettrain dev testarea 2.9% 1.4% 3.8%food 37.3% 34.0% 40.9%name 0.0% 0.0% 0.0%pricerange 1.7% 1.6% 3.1%any 40.1% 37.0% 44.5%Table 2: Percentage of dialogs which included a change inthe goal constraint for each informable (and any slot).
Barelyany users asked for restaurants by name.the data was reasonably uniform across the areaand pricerange slots, but was skewed for food asshown in figure 1.
The skew arises from the distri-bution of the restaurants in the system?s database;many food types have very few matching venues.Recently, researchers have started using wordconfusion networks for spoken language under-standing (Henderson et al., 2012; T?ur et al., 2013).Unfortunately, word confusion networks were notlogged at the time of collecting the dialog data.
Inorder to provide word confusion networks, ASRwas run offline in batch mode on each dialog us-ing similar models as the live system.
This givesa second set of ASR results, labelled batch, whichnot only includes ASR N -best lists (as in live re-sults), but also word confusion networks.For each dataset and speech recogniser, table 3gives the Word Error Rate on the top ASR hypoth-esis, and F-score for the top SLU hypothesis (cal-culated as in Henderson et al.
(2012)).
Note thebatch ASR was always less accurate than the live.Live BatchDataset ASR WER F-score WERtraindegraded 30.7% 72.4% 37.7%good 22.4% 78.7% 25.5%all 26.4% 75.7% 31.3%devdegraded 40.4% 67.3% 47.3%good 25.2% 75.2% 30.0%all 31.9% 71.6% 37.6%testdegraded 33.6% 70.0% 41.1%good 23.5% 77.8% 27.1%all 28.7% 73.8% 34.3%Table 3: Word Error Rate on the top hypothesis, and F-scoreon top SLU hypothesis.2653 Labelling and evaluationThe output of each tracker is a distribution overdialog states for each turn, as explained in section2.1.
To allow evaluation of the tracker output, thesingle correct dialog state at each turn is labelled.Labelling of the dialog state is facilitated by firstlabelling each user utterance with its semantic rep-resentation, in the dialog act format described inHenderson et al.
(2013) (some example seman-tic representations are given in appendix B).
Thesemantic labelling was achieved by first crowd-sourcing the transcription of the audio to text.Next a semantic decoder was run over the tran-scriptions, and the authors corrected the decoder?sresults by hand.
Given the sequence of machineactions and user actions, both represented seman-tically, the true dialog state is computed determin-istically using a simple set of rules.Recall the dialog state is composed of multiplecomponents; the goal constraint for each slot, therequested slots, and the method.
Each of theseis evaluated separately, by comparing the trackeroutput to the correct label.
The joint over the goalconstraints is evaluated in the same way, where thetracker may either explicitly enumerate and scoreits joint hypotheses, or let the joint be computed asthe product of the distributions over the slots.A bank of metrics which look at the tracker out-put and the correct labels are calculated in the eval-uation.
These metrics are a slightly expanded setof those calculated in DSTC1.Denote an example probability distributiongiven by a tracker as p and the correct label to bei, so we have that the probability reported to thecorrect hypothesis is pi, and?jpj= 1.Accuracy measures the fraction of turns wherethe top hypothesis is correct, i.e.
where i =argmaxjpj.
AvgP, average probability, mea-sures the mean score of the correct hypothesis, pi.This gives some idea of the quality of the scoregiven to the correct hypothesis, ignoring the restof the distribution.
Neglogp is the mean nega-tive logarithm of the score given to the correct hy-pothesis, ?
logpi.
Sometimes called the negativelog likelihood, this is a standard score in machinelearning tasks.
MRR is the mean reciprocal rankof the top hypothesis, i.e.11+kwhere jk= i andpj0?
pj1?
.
.
..
This metric measures the qual-ity of the ranking, without necessarily treating thescores as probabilities.
L2 measures the squareof the l2norm between the distribution and thecorrect label, indicating quality of the whole re-ported distribution.
It is calculated for one turnas (1 ?
pi)2+?j 6=ip2j.
Two metrics, Updateprecision and Update accuracy measure the ac-curacy and precision of updates to the top scoringhypothesis from one turn to the next.
For moredetails, see Higashinaka et al.
(2004), which findsthese metrics to be highly correlated with dialogsuccess in their data.Finally there is a set of measures relating tothe receiver operating characteristic (ROC) curves,which measure the discrimination of the scores forthe highest-ranked hypotheses.
Two versions ofROC are computed, V1 and V2.
V1 computescorrect-accepts (CA), false accepts (FA) and false-rejects (FR) as fractions of all utterances.
TheV2 metrics consider fractions of correctly classi-fied utterances, meaning the values always reach100% regardless of the accuracy.
V2 metrics mea-sure discrimination independently of the accuracy,and are therefore only comparable between track-ers with similar accuracies.Several metrics are computed from the ROCstatistics.
ROC V1 EER computes the false ac-ceptance rate at the point where false-accepts areequal to false-rejects.
ROC V1 CA05, ROC V1CA10, ROCV1 CA20 and ROCV2 CA05, ROCV2 CA10, ROC V2 CA20, compute the correctacceptance rates for both versions of ROC at false-acceptance rates 0.05, 0.10, and 0.20.Two schedules are used to decide which turns toinclude when computing each metric.
Schedule 1includes every turn.
Schedule 2 only includes aturn if any SLU hypothesis up to and including theturn contains some information about the compo-nent of the dialog state in question, or if the correctlabel is not None.
E.g.
for a goal constraint, this iswhether the slot has appeared with a value in anySLU hypothesis, an affirm/negate act has appearedafter a system confirmation of the slot, or the userhas in fact informed the slot regardless of the SLU.The data is labelled using two schemes.
Thefirst, scheme A, is considered the standard la-belling of the dialog state.
Under this scheme,each component of the state is defined as the mostrecently asserted value given by the user.
TheNone value is used to indicate that a value is yetto be given.
Appendix B demonstrates labellingunder scheme A.A second labelling scheme, scheme B, is in-cluded in the evaluation, where labels are prop-266agated backwards through the dialog.
This la-belling scheme is designed to assess whether atracker is able to predict a user?s intention be-fore it has been stated.
Under scheme B, the la-bel at a current turn for a particular component ofthe dialog state is considered to be the next valuewhich the user settles on, and is reset in the caseof goal constraints if the slot value pair is given ina canthelp act by the system (i.e.
the system hasinformed that this constraint is not satisfiable).3.1 Featured metricsAll combinations of metrics, state components,schedules and labelling schemes give rise to 815total metrics calculated per tracker in evaluation.Although each may have its particular motiva-tion, many of the metrics will be highly corre-lated.
From the results of DSTC1 it was foundthe metrics could be roughly split into 3 indepen-dent groups; one measuring 1-best quality (e.g.Acc), another measuring probability calibration(e.g.
L2), and the last measuring discrimination(e.g.
ROC metrics) (Williams et al., 2013).By selecting a representative from each of thesegroups, the following were chosen as featuredmetrics:?
Accuracy, schedule 2, scheme A?
L2 norm, schedule 2, scheme A?
ROC V2 CA 5, schedule 2, scheme AAccuracy is a particularly important measurefor dialog management techniques which onlyconsider the top dialog state hypothesis at eachturn, while L2 is of more importance when mul-tiple dialog states are considered in action selec-tion.
Note that the ROC metric is only compara-ble among systems operating at similar accuracies,and while L2 should be minimised, Accuracy andthe ROC metric should be maximised.Each of these, calculated for joint goal con-straints, search method and combined re-quested slots, gives 9 metrics altogether whichparticipants were advised to focus on optimizing.3.2 Baseline trackersThree baseline trackers were entered in the chal-lenge, under the ID ?team0?.
Source code forall the baseline systems is available on the DSTCwebsite2.
The first, ?team0.entry0?, follows sim-ple rules commonly used in spoken dialog sys-tems.
It gives a single hypothesis for each slot,2http://camdial.org/?mh521/dstc/whose value is the top scoring suggestion so far inthe dialog.
Note that this tracker does not accountwell for goal constraint changes; the hypothesisedvalue for a slot will only change if a new valueoccurs with a higher confidence.The focus baseline, ?team0.entry1?, includes asimple model of changing goal constraints.
Be-liefs are updated for the goal constraint s = v, atturn t, P (s = v), using the rule:P (s = v)t= qtP (s = v)t?1+ SLU (s = v)twhere 0 ?
SLU(s = v)t?
1 is the evidencefor s = v given by the SLU in turn t, and qt=?v?SLU(s = v?)t?
1.Another baseline tracker, based on the trackerpresented in Wang and Lemon (2013) is includedin the evaluation, labelled ?team0.entry2?.
Thistracker uses a selection of domain independentrules to update the beliefs, similar to the focusbaseline.
One rule uses a learnt parameter calledthe noise adjustment, to adjust the SLU scores.Full details of this and all baseline trackers are pro-vided on the DSTC website.Finally, an oracle tracker is included under thelabel ?team0.entry3?.
This reports the correct la-bel with score 1 for each component of the dialogstate, but only if it has been suggested in the dialogso far by the SLU.
This gives an upper-bound forthe performance of a tracker which uses only theSLU and its suggested hypotheses.4 ResultsAltogether 9 research teams participated in thechallenge.
Each team could submit a maximum of5 trackers, and 31 trackers were submitted in total.Teams are identified by anonymous team numbersteam1-9, and baseline systems are grouped underteam0.
Appendix A gives the results on the fea-tured metrics for each entry submitted to the chal-lenge.
The full results, including tracker output,details of each tracker and scripts to run the evalu-ation are available on the DSTC2 website.The table in appendix A specifies which of theinputs available were used for each tracker- fromlive ASR, live SLU and batch ASR.
This facil-itates comparisons between systems which usedthe same information.A variety of techniques were used in the sub-mitted trackers.
Some participants provided shortsynopses, which are available in the downloadfrom the DSTC2 website.
Full details on the track-ers themselves are published at SIGdial 2014.267For the ?requested slot?
task, some trackers out-performed the oracle tracker.
This was possiblebecause trackers could guess a slot was requestedusing dialog context, even if there was no mentionof it in the SLU output.Participants were asked to report the results oftheir trackers on the dstcs2 dev development set.Figure 2 gives some insight into how well perfor-mance on the development set predicted perfor-mance on the test set.
Metrics are reported as per-centage improvement relative to the focus base-line to normalise for the difficulty of the datasets;in general trackers achieved higher accuracies onthe test set than on development.
Figure 2 showsthat the development set provided reasonable pre-dictions, though in all cases improvement rel-ative to the baseline was overestimated, some-times drastically.
This suggests that approaches totracking have trouble with generalisation, under-performing in the mismatched conditions of thetest set which used an unseen dialog manager.Joint Goal Constraint Accuracy0.3 0.2 0.1 0.1team1entry0team2entry1team3entry0team4entry0team5entry4team6entry2team7entry0team8entry1team9entry0Joint Goal Constraint L2team1entry0team2entry1team3entry0team4entry0team5entry4team6entry2team7entry0team8entry1team9entry00.2 0.2 0.4 0.6Figure 2: Performance relative to the focus baseline (per-centage increase) for dev set (white) and test set (grey).
Topentry for each team chosen based on joint goal constraint ac-curacy.
A lower L2 score is better.Recall from section 2, trackers could outputjoint distributions for goal constraints, or simplyoutput one distribution for each slot and allow thejoint to be calculated as the product.
Two teams,team2 and team8, opted to output a joint distribu-tion for some of their entries.
Figure 3 comparesperformance on the test set for these trackers be-tween the joint distributions they reported, and thejoint calculated as the product.
The entries fromteam2 were able to show an increase in the accu-racy of the top joint goal constraint hypotheses,but seemingly at a cost in terms of the L2 score.Conversely the entries from team8, though oper-ating at lower performance than the focus base-line, were able to show an improvement in L2 at aslight loss in accuracy.
These results suggest that atracking method is yet to be proposed which can,at least on this data, improve both accuracy andthe L2 score of tracker output by reporting jointpredictions of goal constraints.Accuracyteam0entry2team2entry0team2entry1team2entry2team2entry3team2entry4team8entry0team8entry1team8entry2team8entry30.70 0.72 0.74 0.76 0.780.03% 1.34%0.44%-0.11%0.20%-0.30%-0.04%-0.09%-0.04%-0.09%L2team0entry2team2entry0team2entry1team2entry3team2entry4team8entry0team8entry1team8entry2team8entry3team8entry40.4 0.5 0.6 0.7team0entry2team2entry0team2entry1team2entry2team2entry3team2entry4team8entry0team8entry1team8entry2team8entry30.03% 38.21%52.17%0.22% 23.05% -2.00%-1.73%-1.69%-1.79%-1.75%Figure 3: Influence of reporting a full joint distribution.White bar shows test set performance computing the goalconstraints as a product of independent marginals; dark bar isperformance with a full joint distribution.
All entries whichreported a full joint are shown.
A lower L2 score is better.It is of interest to investigate the correlation be-tween accuracy and L2.
Figure 4 plots these met-rics for each tracker on joint goal constraints.
Wesee that in general a lower L2 score correlates witha higher accuracy, but there are examples of highaccuracy trackers which do poorly in terms of L2.This further justifies the reporting of these as twoseparate featured metrics.0.50 0.55 0.60 0.65 0.70 0.75 0.800.30.40.50.60.70.8 team2entry0team2entry1team4entry0team2entry3focus baseline, team0entry2AccuracyL2Figure 4: Scatterplot of joint goal constraint accuracy andjoint goal constraint L2 for each entry.
Plotted line is least-squares linear regression, L2 = 1.53?
1.43Accuracy268Joint goal Method RequestedTracker Acc.
L2 Acc.
L2 Acc.
L2Single best entry 0.784 0.346 0.950 0.082 0.978 0.035Score averaging: top 2 entries 0.787 0.364- 0.945- 0.083 0.976 0.039-Score averaging: top 5 entries 0.777 0.347 0.945 0.089- 0.976 0.038Score averaging: top 10 entries 0.760- 0.364- 0.934- 0.108- 0.967- 0.056-Score averaging: all entries 0.765- 0.362- 0.934- 0.103- 0.971- 0.052-Stacking: top 2 entries 0.789 0.322+ 0.949 0.085- 0.977 0.040-Stacking: top 5 entries 0.795+ 0.315+ 0.949 0.084 0.978 0.037Stacking: top 10 entries 0.796+ 0.312+ 0.949 0.083 0.979 0.035Stacking: all entries 0.798+ 0.308+ 0.950 0.083 0.980 0.034Table 4: Accuracy and L2 for Joint goal constraint, Method, and Requested slots for the single best tracker (by accuracy) inDSTC2, and various ensemble methods.
?Top N entries?
means the N entries with highest accuracies from distinct teams, wherethe baselines are included as a team.
+/- indicates statistically significantly better/worse than the single best entry (p < 0.01),computed with McNemar?s test for accuracy and the paired t-test for L2, both with Bonferroni correction for repeated tests.5 Ensemble learningThe dialog state tracking challenge provides anopportunity to study ensemble learning ?
i.e.
syn-thesizing the output of many trackers to improveperformance beyond any single tracker.
Here weconsider two forms of ensemble learning: scoreaveraging and stacking.In score averaging, the final score of a class iscomputed as the mean of the scores output by alltrackers for that class.
One of score averaging?sstrengths is that it requires no additional trainingdata beyond that used to train the constituent track-ers.
If each tracker?s output is correct more thanhalf the time, and if the errors made by trackers arenot correlated, then score averaging is guaranteedto improve performance (since the majority votewill be correct in the limit).
In (Lee and Eskenazi,2013), score averaging (there called ?system com-bination?)
has been applied to combine the outputof four dialog state trackers.
To help decorrelateerrors, constituent trackers were trained on differ-ent subsets of data, and used different machinelearning methods.
The relative error rate reductionwas 5.1% on the test set.The second approach to ensemble learning isstacking (Wolpert, 1992).
In stacking, the scoresoutput by the constituent classifiers are fed to anew classifier that makes a final prediction.
Inother words, the output of each constituent classi-fier is viewed as a feature, and the new final classi-fier can learn the correlations and error patterns ofeach.
For this reason, stacking often outperformsscore averaging, particularly when errors are cor-related.
However, stacking requires a validationset for training the final classifier.
In DSTC2, weonly have access to trackers?
output on the test set.Therefore, to estimate the performance of stack-ing, we perform cross-validation on the test set:the test set is divided into two folds.
First, fold 1is used for training the final classifier, and fold 2is used for testing.
Then the process is reversed.The two test outputs are then concatenated.
Notethat models are never trained and tested on thesame data.
A maximum entropy model (maxent) isused (details in (Metallinou et al., 2013)), which iscommon practice for stacking classifiers.
In addi-tion, maxent was found to yield best performancein DSTC1 (Lee and Eskenazi, 2013).Table 4 reports accuracy and L2 for goal con-straints, search method, and requested slots.
Foreach ensemble method and each quantity (column)the table gives results for combining the top track-ers from 2 or 5 distinct teams, for combining thetop tracker from each team, and combining alltrackers (including the baselines as a team).
Forexample, the joint goal constraint ensemble withthe top 2 entries was built from team2.entry1 &team4.entry0, and the method ensemble with thetop 2 entries from team2.entry4 & team4.entry0.Table 4 shows two interesting trends.
The firstis that score averaging does not improve perfor-mance, and performance declines as more track-ers are combined, yielding a statistically signifi-cant decrease across all metrics.
This suggests thatthe errors of the different trackers are correlated,which is unsurprising since they were trained onthe same data.
On the other hand, stacking yieldsa statistically significant improvement in accuracyfor goal constraints, and doesn?t degrade accuracyfor the search method and requested slots.
Forstacking, the trend is that adding more trackers in-creases performance ?
for example, combining thebest tracker from every team improves goal con-straint accuracy from 78.4% to 79.8%.For completeness, we note that the additionaldata could alternatively be used to improve the ac-curacy of a constituent classifier; given the con-straints of the challenge, we can?t assess the mag-269nitude of that improvement, so it is an open ques-tion whether stacking is the best use of additionaldata.
Also, the training and test conditions ofthe final stacking classifier are not mis-matched,whereas in practice they would be.
Nonethe-less, this result does suggest that, if additionaldata is available, stacking can be used to success-fully combine multiple trackers and achieve per-formance better than the single best tracker.6 ConclusionsDSTC2 continues the tradition of DSTC1 by pro-viding a common testbed for dialog state track-ing, introducing some additional features relevantto the research community?
specifically a newdomain, changing user goals and a richer dialogstate.
The data, evaluation scripts, and baselinetrackers will remain available and open to the re-search community online.Results from the previous challenge motivatedthe selection of a few metrics as featured met-rics, which facilitate comparisons between track-ers.
Analysis of the performance on the matcheddevelopment set and the mismatched test set sug-gests that there still appears to be limitations ongeneralisation, as found in DSTC1.
The resultsalso suggest there are limitations in exploiting cor-relations between slots, with few teams exploitingjoint distributions and the effects of doing so beingmixed.
Investigating ensemble learning demon-strates the effectiveness of combining tracker out-puts.
Ensemble learning exploits the strengths ofindividual trackers to provide better quality outputthan any constituent tracker in the group.A follow up challenge, DSTC3, will presentthe problem of adapting to a new domain withvery few example dialogs.
Future work shouldalso verify that improvements in dialog state track-ing translate to improvements in end-to-end dia-log system performance.
In this challenge, paidsubjects were used as users with real informationneeds were not available.
However, differencesbetween these two user groups have been shown(Raux et al., 2005), so future studies should alsotest on real users.AcknowledgementsThe authors thank the advisory committee fortheir valuable input: Paul Crook, Maxine Eske-nazi, Milica Ga?si?c, Helen Hastie, Kee-Eung Kim,Sungjin Lee, Oliver Lemon, Olivier Pietquin,Joelle Pineau, Deepak Ramachandran, BrianStrope and Steve Young.
The authors also thankZhuoran Wang for providing a baseline tracker,and DJ Kim, Sungjin Lee & David Traum for com-ments on evaluation metrics.
Finally, thanks toSIGdial for their endorsement, and to the partic-ipants for making the challenge a success.ReferencesMatthew Henderson, Milica Ga?si?c, Blaise Thom-son, Pirros Tsiakoulis, Kai Yu, and Steve Young.2012.
Discriminative Spoken Language Under-standing Using Word Confusion Networks.
In Spo-ken Language Technology Workshop, 2012.
IEEE.Matthew Henderson, Blaise Thomson, and JasonWilliams.
2013.
Dialog State Tracking Challenge2 & 3 Handbook.
camdial.org/?mh521/dstc/.Ryuichiro Higashinaka, Noboru Miyazaki, MikioNakano, and Kiyoaki Aikawa.
2004.
Evaluat-ing discourse understanding in spoken dialogue sys-tems.
ACM Trans.
Speech Lang.
Process., Novem-ber.Sungjin Lee and Maxine Eskenazi.
2013.
Recipe forbuilding robust spoken dialog state trackers: Dialogstate tracking challenge system description.
In Pro-ceedings of the SIGDIAL 2013 Conference.Angeliki Metallinou, Dan Bohus, and Jason D.Williams.
2013.
Discriminative state tracking forspoken dialog systems.
In Proc Association forComputational Linguistics, Sofia.Antoine Raux, Brian Langner, Dan Bohus, Alan WBlack, and Maxine Eskenazi.
2005.
Let?s go public!Taking a spoken dialog system to the real world.G?okhan T?ur, Anoop Deoras, and Dilek Hakkani-T?ur.2013.
Semantic parsing using word confusion net-works with conditional random fields.
In INTER-SPEECH.Zhuoran Wang and Oliver Lemon.
2013.
A simpleand generic belief tracking mechanism for the dia-log state tracking challenge: On the believability ofobserved information.
In Proceedings of the SIG-DIAL 2013 Conference.Jason Williams, Antoine Raux, Deepak Ramachadran,and Alan Black.
2013.
The Dialog State Track-ing Challenge.
In Proceedings of the SIGDIAL 2013Conference, Metz, France, August.David H. Wolpert.
1992.
Stacked generalization.
Neu-ral Networks, 5:241?259.Steve Young, Catherine Breslin, Milica Ga?si?c,Matthew Henderson, Dongho Kim, Martin Szum-mer, Blaise Thomson, Pirros Tsiakoulis, and EliTzirkel Hancock.
2013.
Evaluation of StatisticalPOMDP-based Dialogue Systems in Noisy Environ-ment.
In Proceedings of IWSDS, Napa, USA, Jan-uary.270Appendix A: Featured results of evaluationTracker Inputs Joint Goal Constraints Search Method Requested Slotsteam entryLiveASRLiveSLUBatchASRAcc L2 ROC Acc L2 ROC Acc L2 ROC0* 0 X 0.619 0.738 0.000 0.879 0.209 0.000 0.884 0.196 0.0001 X 0.719 0.464 0.000 0.867 0.210 0.349 0.879 0.206 0.0002 X 0.711 0.466 0.000 0.897 0.158 0.000 0.884 0.201 0.0003 X?0.850 0.300 0.000 0.986 0.028 0.000 0.957 0.086 0.0001 0 X 0.601 0.649 0.064 0.904 0.155 0.187 0.960 0.073 0.0001 X 0.596 0.671 0.036 0.877 0.204 0.397 0.957 0.081 0.0002 0 X X 0.775 0.758 0.063 0.944 0.092 0.306 0.954 0.073 0.3831 X X X 0.784 0.735 0.065 0.947 0.087 0.355 0.957 0.068 0.4462 X 0.668 0.505 0.249 0.944 0.095 0.499 0.972 0.043 0.3003 X X X 0.771 0.354 0.313 0.947 0.093 0.294 0.941 0.090 0.2624 X X X 0.773 0.467 0.140 0.950 0.082 0.351 0.968 0.050 0.4973 0 X 0.729 0.452 0.000 0.878 0.210 0.000 0.889 0.188 0.0004 0 X 0.768 0.346 0.365 0.940 0.095 0.452 0.978 0.035 0.5251 X 0.746 0.381 0.383 0.939 0.097 0.423 0.977 0.038 0.4902 X 0.742 0.387 0.345 0.922 0.124 0.447 0.957 0.069 0.3403 X 0.737 0.406 0.321 0.922 0.125 0.406 0.957 0.073 0.3855 0 X X 0.686 0.628 0.000 0.889 0.221 0.000 0.868 0.264 0.0001 X X 0.609 0.782 0.000 0.927 0.147 0.000 0.974 0.053 0.0002 X X 0.637 0.726 0.000 0.927 0.147 0.000 0.974 0.053 0.0003 X X 0.609 0.782 0.000 0.927 0.147 0.000 0.974 0.053 0.0004 X X 0.695 0.610 0.000 0.927 0.147 0.000 0.974 0.053 0.0006 0 X 0.713 0.461 0.100 0.865 0.228 0.199 0.932 0.118 0.0571 X 0.707 0.447 0.223 0.871 0.211 0.290 0.947 0.093 0.2182 X 0.718 0.437 0.207 0.871 0.210 0.287 0.951 0.085 0.2257 0 X 0.750 0.416 0.081 0.936 0.105 0.237 0.970 0.056 0.0001 X 0.739 0.428 0.159 0.921 0.161 0.554 0.970 0.056 0.0002 X 0.750 0.416 0.081 0.929 0.117 0.379 0.971 0.054 0.0003 X 0.725 0.432 0.105 0.936 0.105 0.237 0.972 0.047 0.0004 X 0.735 0.433 0.086 0.910 0.140 0.280 0.946 0.089 0.1908 0 X 0.692 0.505 0.071 0.899 0.153 0.000 0.935 0.106 0.0001 X 0.699 0.498 0.067 0.899 0.153 0.000 0.939 0.101 0.0002 X 0.698 0.504 0.067 0.899 0.153 0.000 0.939 0.101 0.0003 X 0.697 0.501 0.068 0.899 0.153 0.000 0.939 0.101 0.0004 X 0.697 0.508 0.068 0.899 0.153 0.000 0.939 0.101 0.0009 0 X 0.499 0.760 0.000 0.857 0.229 0.000 0.905 0.149 0.000* The entries under team0 are the baseline systems mentioned in section 3.2.?team0.entry3 is theoracle tracker, which uses the labels on the test set and limits itself to hypotheses suggested by the liveSLU.The top score in each column is indicated by bold-type.
The ROC metric is only comparable for trackersoperating at a similar accuracy, and so the highest values are not indicated.271Appendix B: Sample dialog, labels, and tracker outputS:U:Which part of town?The north uh area0.2 inform(food=north_african) area=northmethod=byconstraintsrequested=()0.1 inform(area=north)0.2 food=north_african0.1 area=northrequest(area)inform(area=north)0.9 byconstraints0.1 none0.0 phone0.0 addressActual input and output SLU hypotheses and scores Labels Example tracker output Correct?S:U:Which part of town?A cheap place inthe northinform(area=north,pricerange=cheap)0.8 inform(area=north),inform(pricerange=cheap)area=northpricerange=cheapmethod=byconstraintsrequested=()0.1 inform(area=north)0.7 area=northpricerange=cheap0.1 area=northfood=north_africanrequest(area)0.9 byconstraints0.1 none0.0 phone0.0 addressS:U:Clown caf?
is a cheaprestaurant in thenorth part of town.Do you have anyothers l ike that,maybe in the southpart of town?reqalts(area=south)0.7 reqalts(area=south) area=southpricerange=cheapmethod=byalternativesrequested=()0.2 reqmore()0.8 area=southpricerange=cheap0.1 area=northpricerange=cheap0.6 byalternatives0.2 byconstraints0.0 phone0.0 addressS:U:Galleria is a cheaprestaurant in thesouth.What is their phonenumber andaddress?request(phone),request(address)0.6 request(phone) area=southpricerange=cheapmethod=byalternativesrequested= (phone,address)0.2 request(phone),request(address)0.9 area=southpricerange=cheap0.1 area=northpricerange=cheap0.5 byconstraints0.4 byalternatives0.8 phone0.3 address0.1 request(address)0.7 ()0.2 ()0.1 ()0.0 ()Example dialog illustrating DSTC2 data, labels, and evaluation procedure.
The left column shows theactual system output and user input.
The second column shows two SLU N-Best hypothesis and theirscores.
In practice, up to 10 SLU N-Best hypotheses are output.
In the right 3 columns, the three shadedregions correspond to the three components of the dialog state output by a tracker at each turn.
The blueregion corresponds to the user?s joint goal constraint; the red region to the user?s search method; andthe yellow region to the slots requested by the user.
For space, only 2 of the 5 methods and 2 of the8 requestable slots are shown.
The third column shows the label (correct output) for each component.The fourth column shows example tracker output for each of these three quantities, and the fifth columnindicates correctness.
A goal constraint is correct if it exactly matches the label.
Therefore, 0 or 1 ofthe output goal constraints is correct, and all the others are incorrect.
Accuracy is determined by thecorrectness of the goal constraint with the highest tracker score.
For search method, exactly one methodis correct at each turn, so correctness is determined by comparing the maximum scoring method to thelabel.
For requested slots, each slot can be requested (or not) in the same turn, so each requestable slotis separately marked as correct or incorrect.
The quantity requested.all averages the correctness of allrequested slots.272
