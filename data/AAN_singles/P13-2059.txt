Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 328?333,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSign Language Lexical Recognition With Propositional DynamicLogicArturo CurielUniversit?
Paul Sabatier118 route de Narbonne, IRIT,31062, Toulouse, Francecuriel@irit.frChristophe ColletUniversit?
Paul Sabatier118 route de Narbonne, IRIT,31062, Toulouse, Francecollet@irit.frAbstractThis paper explores the use of Proposi-tional Dynamic Logic (PDL) as a suit-able formal framework for describingSign Language (SL), the language ofdeaf people, in the context of natu-ral language processing.
SLs are vi-sual, complete, standalone languageswhich are just as expressive as oral lan-guages.
Signs in SL usually correspondto sequences of highly specific bodypostures interleaved with movements,which make reference to real world ob-jects, characters or situations.
Here wepropose a formal representation of SLsigns, that will help us with the analysisof automatically-collected hand track-ing data from French Sign Language(FSL) video corpora.
We further showhow such a representation could help uswith the design of computer aided SLverification tools, which in turn wouldbring us closer to the development of anautomatic recognition system for theselanguages.1 IntroductionSign languages (SL), the vernaculars of deafpeople, are complete, rich, standalone commu-nication systems which have evolved in paral-lel with oral languages (Valli and Lucas, 2000).However, in contrast to the last ones, researchin automatic SL processing has not yet man-aged to build a complete, formal definition ori-ented to their automatic recognition (Cuxacand Dalle, 2007).
In SL, both hands and non-manual features (NMF), e.g.
facial muscles,can convey information with their placements,configurations and movements.
These particu-lar conditions can difficult the construction ofa formal description with common natural lan-guage processing (NLP) methods, since the ex-isting modeling techniques are mostly designedto work with one-channel sound productionsinherent to oral languages, rather than withthe multi-channel partially-synchronized infor-mation induced by SLs.Our research strives to address the formal-ization problem by introducing a logical lan-guage that lets us represent SL from the lowestlevel, so as to render the recognition task moreapproachable.
For this, we use an instance ofa formal logic, specifically Propositional Dy-namic Logic (PDL), as a possible descriptionlanguage for SL signs.For the rest of this section, we will present abrief introduction to current research efforts inthe area.
Section 2 presents a general descrip-tion of our formalism, while section 3 showshow our work can be used when confrontedwith real world data.
Finally, section 4 presentour final observations and future work.Images for the examples where taken from(DictaSign, 2012) corpus.1.1 Current Sign Language ResearchExtensive efforts have been made to achieveefficient automatic capture and representationof the subtle nuances commonly present insign language discourse (Ong and Ranganath,2005).
Research ranges from the developmentof hand and body trackers (Dreuw et al, 2009;Gianni and Dalle, 2009), to the design of highlevel SL representation models (Lejeune, 2004;Lenseigne and Dalle, 2006).
Linguistic re-search in the area has focused on the character-ization of corporal expressions into meaning-ful transcriptions (Dreuw et al, 2010; Stokoe,2005) or common patterns across SL (Aronoffet al, 2005; Meir et al, 2006; Wittmann,1991), so as to gain understanding of the un-328derlying mechanisms of SL communication.Works like (Losson and Vannobel, 1998) dealwith the creation of a lexical description ori-ented to computer-based sign animation.
Re-port (Filhol, 2009) describes a lexical specifi-cation to address the same problem.
Both pro-pose a thoroughly geometrical parametric en-coding of signs, thus leaving behind meaning-ful information necessary for recognition andintroducing data beyond the scope of recog-nition.
This complicates the reutilization oftheir formal descriptions.
Besides, they don?ttake in account the presence of partial informa-tion.
Treating partiality is important for us,since it is often the case with automatic toolsthat incomplete or unrecognizable informationarises.
Finally, little to no work has been di-rected towards the unification of raw collecteddata from SL corpora with higher level descrip-tions (Dalle, 2006).2 Propositional Dynamic Logic forSLPropositional Dynamic Logic (PDL) is a multi-modal logic, first defined by (Fischer and Lad-ner, 1979).
It provides a language for describ-ing programs, their correctness and termina-tion, by allowing them to be modal operators.We work with our own variant of this logic,the Propositional Dynamic Logic for Sign Lan-guage (PDLSL), which is just an instantiationof PDL where we take signers?
movements asprograms.Our sign formalization is based on the ap-proach of (Liddell and Johnson, 1989) and (Fil-hol, 2008).
They describe signs as sequences ofimmutable key postures and movement transi-tions.In general, each key posture will be charac-terized by the concurrent parametric state ofeach body articulator over a time-interval.
Forus, a body articulator is any relevant body partinvolved in signing.
The parameters taken inaccount can vary from articulator to articula-tor, but most of the time they comprise theirconfigurations, orientations and their place-ment within one or more places of articulation.Transitions will correspond to the movementsexecuted between fixed postures.2.1 SyntaxWe need to define some primitive sets that willlimit the domain of our logical language.Definition 2.1 (Sign Language primi-tives).
Let BSL = {D,W,R,L} be the set ofrelevant body articulators for SL, where D, W,R and L represent the dominant, weak, rightand left hands, respectively.
Both D and Wcan be aliases for the right or left hands, butthey change depending on whether the signeris right-handed or left-handed, or even depend-ing on the context.Let ?
be the two-dimensional projection ofa human body skeleton, seen by the front.
Wedefine the set of places of articulation for SL as?SL = {HEAD, CHEST, NEUTRAL, .
.
.
}, such thatfor each ?
?
?SL, ?
is a sub-plane of ?, asshown graphically in figure 1.Let CSL be the set of possible morphologicalconfigurations for a hand.Let ?
= {?,?,?,?, ?,?,?,?}
be the setof relative directions from the signer?s point ofview, where each arrow represents one of eightpossible two-dimensional direction vectors thatshare the same origin.
For vector ?
?
?, wedefine vector ???
as the same as ?
but with theinverted abscissa axis, such that ???
?
?.
Letvector ??
indicate movement with respect to thedominant or weak hand in the following man-ner:??
={?
if D ?
R or W ?
L???
if D ?
L or W ?
RFinally, let ?
?v1 and ?
?v2 be any two vectors withthe same origin.
We denote the rotation anglebetween the two as ?(?
?v1 ,?
?v2).Now we define the set of atomic propositionsthat we will use to characterize fixed states,and a set of atomic actions to describe move-ments.Definition 2.2 (Atomic Propositions forSL Body Articulators ?SL).
The set ofatomic propositions for SL articulators (?SL)is defined as:?SL = {?1?
?2 ,??1?
, T?1?2 ,F?1c ,??
?1}where ?1, ?2 ?
BSL, ?
?
?, ?
?
?SL andc ?
CSL.329Figure 1: Possible places of articulation in BSL.Intuitively, ?1?
?2 indicates that articulator ?1is placed in relative direction ?
with respectto articulator ?2.
Let the current place ofarticulation of ?2 be the origin point of ?2?sCartesian system (C?2).
Let vector??
?1 de-scribe the current place of articulation of ?1in C?2.
Proposition ?1?
?2 holds when ??
?v ?
?,?(??
?1, ?)
?
?(???1,?
?v ).??1?
asserts that articulator ?1 is located in?.T ?1?2 is active whenever articulator ?1 physi-cally touches articulator ?2.F?1c indicates that c is the morphologicalconfiguration of articulator ?1.Finally, ??
?1 means that an articulator ?1 isoriented towards direction ?
?
?.
For hands,??
?1 will hold whenever the vector perpendicu-lar to the plane of the palm has the smallestrotation angle with respect to ?.Definition 2.3 (Atomic Actions for SLBody Articulators ?SL).
The atomic ac-tions for SL articulators ( ?SL) are given bythe following set:?SL = {?
?1 ,!
?1}where ?
?
?
and ?1 ?
BSL.Let ?1?s position before movement be the ori-gin of ?1?s Cartesian system (C?1) and??
?1 bethe position vector of ?1 in C?1 after moving.Action ?
?1 indicates that ?1 moves in relativedirection ?
in C?1 if ??
?v ?
?, ?(??
?1, ?)
??(???1,?
?v ).Action !
?1 occurs when articulator ?1moves rapidly and continuously (thrills) with-out changing it?s current place of articulation.Definition 2.4 (Action Language for SLBody Articulators ASL).
The action lan-guage for body articulators (ASL) is given bythe following rule:?
::= pi | ?
?
?
| ?
?
?
| ?;?
| ?
?where pi ?
?SL.Intuitively, ?
?
?
indicates the concurrentexecution of two actions, while ?
?
?
meansthat at least one of two actions will be non-deterministically executed.
Action ?;?
de-scribes the sequential execution of two actions.Finally, action ??
indicates the reflexive tran-sitive closure of ?.Definition 2.5 (Language PDLSL ).
Theformulae ?
of PDLSL are given by the followingrule:?
::= > | p | ??
| ?
?
?
| [?
]?where p ?
?SL, ?
?
ASL.2.2 SemanticsPDLSL formulas are interpreted over labeledtransition systems (LTS), in the spirit of thepossible worlds model introduced by (Hin-tikka, 1962).
Models correspond to connectedgraphs representing key postures and transi-tions: states are determined by the values oftheir propositions, while edges represent setsof executed movements.
Here we present onlya small extract of the logic semantics.Definition 2.6 (Sign Language UtteranceModel USL).
A sign language utterance model(USL), is a tuple USL = (S,R, J?K?SL , J?K?SL)where:?
S is a non-empty set of states?
R is a transition relation R ?
S?S where,?s ?
S, ?s?
?
S such that (s, s?)
?
R.?
J?K?SL : ?SL ?
R, denotes the functionmapping actions to the set of binary rela-tions.?
J?K?SL : S ?
2?SL , maps each state to aset of atomic propositions.330We also need to define a structure over se-quences of states to model internal dependen-cies between them, nevertheless we decided toomit the rest of our semantics, alongside satis-faction conditions, for the sake of readability.3 Use Case: Semi-Automatic SignRecognitionWe now present an example of how we can useour formalism in a semi-automatic sign recog-nition system.
Figure 2 shows a simple modulediagram exemplifying information flow in thesystem?s architecture.
We proceed to brieflydescribe each of our modules and how theywork together.CorpusTrackingand Seg-mentationModuleKeypostures &transitionsPDLSLModelExtractionModulePDLSLVerificationModulePDLSLGraphSignFormul?UserInputSignProposalsFigure 2: Information flow in a semi-automaticSL lexical recognition system.3.1 Tracking and SegmentationModuleThe process starts by capturing relevant infor-mation from video corpora.
We use an exist-ing head and hand tracker expressly developedfor SL research (Gonzalez and Collet, 2011).This tool analyses individual video instances,and returns the frame-by-frame positions ofthe tracked articulators.
By using this infor-mation, the module can immediately calculatespeeds and directions on the fly for each hand.The module further employs the methodproposed by the authors in (Gonzalez andCollet, 2012) to achieve sub-lexical segmenta-tion from the previously calculated data.
Likethem, we use the relative velocity betweenhands to identify when hands either move atthe same time, independently or don?t move atall.
With these, we can produce a set of possi-ble key postures and transitions that will serveas input to the modeling module.3.2 Model Extraction ModuleThis module calculates a propositional statefor each static posture, where atomic PDLSLformulas codify the information tracked in theprevious part.
Detected movements are inter-preted as PDLSL actions between states....R?L?LTORSE?RR_SIDEOFBODY?FRL_CONFIG?FLFIST_CONFIG?T RL......R?L?LL_SIDEOFBODY?RR_SIDEOFBODYFRKEY_CONFIGFLKEY_CONFIG?T RL...?L!D ?
!G...R?L?LCENTEROFBODY?RR_SIDEOFHEADFRBEAK_CONFIGFLINDEX_CONFIG?T RL...?L ...R?L?LL_SIDEOFBODY?RR_SIDEOFBODYFROPENPALM_CONFIGFLOPENPALM_CONFIG?T RL...?LFigure 3: Example of modeling over four auto-matically identified frames as possible key pos-tures.Figure 3 shows an example of the process.Here, each key posture is codified into propo-sitions acknowledging the hand positions withrespect to each other (R?L ), their place of artic-ulation (e.g.
?left hand floats over the torse?with ?LTORSE), their configuration (e.g.
?righthand is open?
with FROPENPALM_CONFIG) and theirmovements (e.g.
?left hand moves to the up-left direction?
with ?L).This module also checks that the generatedgraph is correct: it will discard simple track-ing errors to ensure that the resulting LTS willremain consistent.3.3 Verification ModuleFirst of all, the verification module has to beloaded with a database of sign descriptions en-coded as PDLSL formulas.
These will charac-terize the specific sequence of key postures thatmorphologically describe a sign.
For exam-ple, let?s take the case for sign ?route?
in FSL,shown in figure 4, with the following PDLSLformulation,Example 3.1 (ROUTEFSL formula).
(?RFACE ?
?LFACE ?
L?R ?
FRCLAMP ?
FLCLAMP ?
T RL )?
[?R ?
?L](L?R ?
FRCLAMP ?
FLCLAMP ?
?T RL )(1)331Figure 4: ROUTEFSL production.Formula (1) describes ROUTEFSL as a signwith two key postures, connected by a two-hand simultaneous movement (representedwith operator ?).
It also indicates the posi-tion of each hand, their orientation, whetherthey touch and their respective configurations(in this example, both hold the same CLAMPconfiguration).The module can then verify whether a signformula in the lexical database holds in anysub-sequence of states of the graph generatedin the previous step.
Algorithm 1 sums up theprocess.Algorithm 1 PDLSL Verification AlgorithmRequire: SL modelMSLRequire: connected graph GSLRequire: lexical database DBSL1: Proposals_For[state_qty]2: for state s ?
GSL do3: for sign ?
?
DBSL where s ?
?
do4: if MSL, s |= ?
then5: Proposals_For[s].append(?
)6: end if7: end for8: end for9: return Proposals_ForFor each state, the algorithm returns a setof possible signs.
Expert users (or higher levelalgorithms) can further refine the process byintroducing additional information previouslymissed by the tracker.4 Conclusions and Future WorkWe have shown how a logical language canbe used to model SL signs for semi-automaticrecognition, albeit with some restrictions.
Thetraits we have chosen to represent were im-posed by the limits of the tracking tools wehad to our disposition, most notably workingwith 2D coordinates.
With these in mind, wetried to design something flexible that couldbe easily adapted by computer scientists andlinguists alike.
Our primitive sets, were inten-tionally defined in a very general fashion dueto the same reason: all of the perceived di-rections, articulators and places of articulationcan easily change their domains, depending onthe SL we are modeling or the technologicalconstraints we have to deal with.
Proposi-tions can also be changed, or even induced, byexisting written sign representation languagessuch as Zebedee (Filhol, 2008) or HamNoSys(Hanke, 2004), mainly for the sake of extend-ability.From the application side, we still need tocreate an extensive sign database codified inPDLSL and try recognition on other corpora,with different tracking information.
For ver-ification and model extraction, further opti-mizations are expected, including the handlingof data inconsistencies and repairing brokenqueries when verifying the graph.Regarding our theoretical issues, futurework will be centered in improving our lan-guage to better comply with SL research.
Thisincludes adding new features, like incorpo-rating probability representation to improverecognition.
We also expect to finish the defini-tion of our formal semantics, as well as provingcorrection and complexity of our algorithms.ReferencesMark Aronoff, Irit Meir, andWendy Sandler.
2005.The paradox of sign language morphology.
Lan-guage, 81(2):301.Christian Cuxac and Patrice Dalle.
2007.
Probl?-matique des chercheurs en traitement automa-tique des langues des signes, volume 48 ofTraitement Automatique des Langues.
Lavoisier,http://www.editions-hermes.fr/, October.Patrice Dalle.
2006.
High level models for sign lan-guage analysis by a vision system.
In Workshopon the Representation and Processing of SignLanguage: Lexicographic Matters and DidacticScenarios (LREC), Italy, ELDA, page 17?20.DictaSign.
2012. http://www.dictasign.eu.Philippe Dreuw, Daniel Stein, and Hermann Ney.2009.
Enhancing a sign language transla-tion system with vision-based features.
InMiguel Sales Dias, Sylvie Gibet, Marcelo M.332Wanderley, and Rafael Bastos, editors, Gesture-Based Human-Computer Interaction and Simu-lation, number 5085 in Lecture Notes in Com-puter Science, pages 108?113.
Springer BerlinHeidelberg, January.Philippe Dreuw, Hermann Ney, Gregorio Martinez,Onno Crasborn, Justus Piater, Jose MiguelMoya, and Mark Wheatley.
2010.
The Sign-Speak project - bridging the gap between sign-ers and speakers.
In Nicoletta Calzolari (Con-ference Chair), Khalid Choukri, and et.
al., ed-itors, Proceedings of the Seventh InternationalConference on Language Resources and Evalu-ation (LREC?10), Valletta, Malta, May.
Euro-pean Language Resources Association (ELRA).Michael Filhol.
2008.
Mod?le descriptif des signespour un traitement automatique des langues dessignes.
Ph.D. thesis, Universit?
Paris-sud (Paris11).Michael Filhol.
2009.
Zebedee: a lexical descrip-tion model for sign language synthesis.
Internal,LIMSI.Michael J. Fischer and Richard E. Ladner.
1979.Propositional dynamic logic of regular pro-grams.
Journal of Computer and System Sci-ences, 18(2):194?211, April.Fr?d?ric Gianni and Patrice Dalle.
2009.
Ro-bust tracking for processing of videos of com-munication?s gestures.
Gesture-Based Human-Computer Interaction and Simulation, page93?101.Matilde Gonzalez and Christophe Collet.
2011.Robust body parts tracking using particle fil-ter and dynamic template.
In 2011 18th IEEEInternational Conference on Image Processing(ICIP), pages 529 ?532, September.Matilde Gonzalez and Christophe Collet.
2012.Sign segmentation using dynamics and handconfiguration for semi-automatic annotation ofsign language corpora.
In Eleni Efthimiou,Georgios Kouroupetroglou, and Stavroula-EvitaFotinea, editors, Gesture and Sign Languagein Human-Computer Interaction and EmbodiedCommunication, number 7206 in Lecture Notesin Computer Science, pages 204?215.
SpringerBerlin Heidelberg, January.Thomas Hanke.
2004.
HamNoSys?Representingsign language data in language resources andlanguage processing contexts.
In Proceedings ofthe Workshop on the Representation and Pro-cessing of Sign Languages ?From SignWriting toImage Processing.
Information, Lisbon, Portu-gal, 30 May.Jaakko Hintikka.
1962.
Knowledge and Belief.Ithaca, N.Y.,Cornell University Press.Fanch Lejeune.
2004.
Analyse s?mantico-cognitived?
?nonc?s en Langue des Signes Fran\ccaisepour une g?n?ration automatique de s?quencesgestuelles.
Ph.D. thesis, PhD thesis, Orsay Uni-versity, France.Boris Lenseigne and Patrice Dalle.
2006.
Us-ing signing space as a representation for signlanguage processing.
In Sylvie Gibet, NicolasCourty, and Jean-Fran?ois Kamp, editors, Ges-ture in Human-Computer Interaction and Sim-ulation, number 3881 in Lecture Notes in Com-puter Science, pages 25?36.
Springer Berlin Hei-delberg, January.S.
K. Liddell and R. E. Johnson.
1989.
Americansign language: The phonological base.
GallaudetUniversity Press, Washington.
DC.Olivier Losson and Jean-Marc Vannobel.
1998.Sign language formal description and synthe-sis.
INT.JOURNAL OF VIRTUAL REALITY,3:27?34.Irit Meir, Carol Padden, Mark Aronoff, and WendySandler.
2006.
Re-thinking sign language verbclasses: the body as subject.
In Sign Languages:Spinning and Unraveling the Past, Present andFuture.
9th Theoretical Issues in Sign LanguageResearch Conference, Florianopolis, Brazil, vol-ume 382.Sylvie C. W. Ong and Surendra Ranganath.
2005.Automatic sign language analysis: a survey andthe future beyond lexical meaning.
IEEE Trans-actions on Pattern Analysis and Machine Intel-ligence, 27(6):873 ?
891, June.William C. Stokoe.
2005.
Sign language structure:An outline of the visual communication systemsof the american deaf.
Journal of Deaf Studiesand Deaf Education, 10(1):3?37, January.Clayton Valli and Ceil Lucas.
2000.
Linguistics ofAmerican Sign Language Text, 3rd Edition: AnIntroduction.
Gallaudet University Press.Henri Wittmann.
1991.
Classification linguis-tique des langues sign?es non vocalement.
Revuequ?b?coise de linguistique th?orique et appliqu?e,10(1):88.333
