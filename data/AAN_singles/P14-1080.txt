Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 850?860,Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational LinguisticsEnhancing Grammatical Cohesion:Generating Transitional Expressions for SMTMei Tu             Yu Zhou           Chengqing ZongNational Laboratory of Pattern Recognition,Institute of Automation,Chinese Academy of Sciences{mtu,yzhou,cqzong}@nlpr.ia.ac.cnAbstractTransitional expressions provide glue thatholds ideas together in a text and enhance thelogical organization, which together help im-prove readability of a text.
However, in mostcurrent statistical machine translation (SMT)systems, the outputs of compound-complexsentences still lack proper transitional expres-sions.
As a result, the translations are oftenhard to read and understand.
To address thisissue, we propose two novel models to en-courage generating such transitional expres-sions by introducing the source compound-complex sentence structure (CSS).
Our modelsinclude a CSS-based translation model, whichgenerates new CSS-based translation rules,and a generative transfer model, which en-courages producing transitional expressionsduring decoding.
The two models are integrat-ed into a hierarchical phrase-based translationsystem to evaluate their effectiveness.
The ex-perimental results show that significant im-provements are achieved on various test datameanwhile the translations are more cohesiveand smooth.1 IntroductionDuring the last decade, great progress has beenmade on statistical machine translation (SMT)models.
However, these translations still sufferfrom poor readability, especially translations ofcompound-complex sentences.
One of the mainreasons may be that most existing models con-centrate more on producing well-translated localsentence fragments, but largely ignore globalcohesion between the fragments.
Generally, co-hesion, including lexical and grammatical cohe-sion, contributes much to the understandabilityand smoothness of a text.Recently, researchers have begun addressingthe lexical cohesion of SMT (Gong et al, 2011;Xiao et al, 2011; Wong and Kit, 2012; Xiong,2013).
These efforts focus mainly on the co-occurrence of lexical items in a similar environ-ment.
Grammatical cohesion1 (Halliday and Has-san, 1976) in SMT has been little mentioned inprevious work.
Translations without grammaticalcohesion is hard to read, mostly due to loss ofcohesive and transitional expressions betweentwo sentence fragments.
Thus, generating transi-tional expressions is necessary for achievinggrammatical cohesion.
However, it is not easy toproduce such transitional expressions in SMT.As an example, consider the Chinese-to-Englishtranslation in Figure 1.Source Chinese sentence:[??
??
??
?
??
??
?
]1   [  ?
?Alth ugh   reduce  pollution  of   calls    continue  ,           public??
??
?
]2   [??
??
??
?
?
?gr wing    angry  ,         pollution  still    become   more   worse?
?
]3  [??
??
??
?
???
?
]4already   ,   more   show  environment  protection  of    urgent .Target English golden translation:Despite frequent calls for cutting pollution, andgrowing public anger, the proble  has only g t worse,which increasingly shows the urgency of environmentalprotection.Figure 1: An example of Chinese-to-English transla-tion.
The English translation sentence has three transi-tional phrases: Despite, and, which.There are 4 sub-sentences separated by com-mas in the Chinese sentence.
We have tried totranslate the Chinese sentence using many well-1Grammatical cohesion can make relations among sentenc-es more explicit.
There are various grammatically cohesivedevices (reference, substitution ellipsis and conjunction)that tie fragments together in a cohesive way.850known online translators, but find that it is verydifficult to generate the target transitional ex-pressions, especially when there is no explicitconnective word in the source sentence, such asgenerating ?and ?
and ?which?
in Figure 1.Fortunately, the functional relationships be-tween two neighboring source sub-sentencesprovide us with a good perspective and the inspi-ration to generate those transitional phrases.
Fig-ure 1 shows that the first and the second Chinesesub-sentences form a parallel relation.
Thus,even though there is no distinct connective wordat the beginning of the second source sub-sentence, a good translator is still able to insert orgenerate an ?and?
as a connection word to makethe target translation more cohesive.Based on the above analysis, this paper focus-es on the target grammatical cohesion in SMT tomake the translation more understandable, espe-cially for languages with great difference in lin-guistic structure like Chinese and English.
To thebest of our knowledge, our work is the first at-tempt to generate target transitional expressionsfor SMT grammatical cohesion by introducingthe functional relationships of source sentences.In this work, we propose two models.
One is anew translation model that is utilized to generatenew translation rules combined with the infor-mation of source functional relationships.
Theother is a generative transfer model that encour-ages producing transitional phrases during de-coding.
Our experimental results on Chinese-to-English translation demonstrate that the transla-tion readability is greatly improved by introduc-ing the cohesive information.The remainder of the paper is organized asfollows.
In Section 2, we describe the functionalrelationships of Chinese compound-complex sen-tences.
In Section 3, we present our models andshow how to integrate the models into an SMTsystem.
Our experimental results are reported inSection 4.
A survey of related work is conductedin Section 5, and we conclude our work and out-line the future work in Section 6.2 Chinese Compound-Complex Sen-tence StructureTo acquire the functional relationships of a Chi-nese compound-complex sentence, Zhou (2004)proposed a well-annotated scheme to build theCompound-complex Sentence Structure (CSS).The structure explicitly shows the minimal se-mantic spans, called elementary units (eus), andalso depicts the hierarchical relations among eus.There are 11 common types of functional rela-tionships 2  annotated in the Tsinghua ChineseTreebank (Zhou, 2004).Under the annotation scheme of the TsinghuaChinese Treebank, the Chinese sentence of ex-ample in Figure 1 is represented as the treeshown in Figure 2.
In this example, each sub-sentence is an eu.
eu1 and eu2 are combined witha parallel relationship, followed by eu3 with anadversative relationship.
eu1, eu2, and eu3 form alarge semantic span3, connected with eu4 by aconsequence relationship.
All of the eus are or-ganized into various functional relationships andfinally form a hierarchical tree.par llel-[(1,1), (2,2)]dversative-[(1,2),(3,3)]consequence-[(1,3),(4,4)]??
????
???
?
???
????
????
?eu1eu2eu3eu4??
????
???
??
???
????
?Figure 2: The compound-complex sentencestructure of the Chinese sentence in Figure 1.Formally, given a compound-complex sen-tence structure (CSS), each node in the CSS canbe represented as a tuple1 1[( , ),...( , ),..., ( , )]?
l l L LR s e s e s e. R represents therelationship, which has L children.
For eachchild of R , a pair ( , )lls e records its start and endeus.
For example, adversative-[(1,2), (3,3)] inFigure 2 means that two children are controlledby the relationship adversative, and the left childconsists of eu1 and eu2, while the right child con-tains only eu3.CSS has much in common with RhetoricalStructure (Mann and Thompson, 1988) in Eng-lish, which also describe the semantic relationbetween discourse units.
But the RhetoricalStructure involves much richer relations on thedocument-level, and little corpus is open forChinese.In the following, we will describe in detailhow to utilize such CSS information for model-ling in SMT.2 They are parallel, consequence, progressive, alternative,causal, purpose, hypothesis, condition, adversative, expla-nation, and flowing relationships.3 A semantic span can include one or more eus.8513 ModellingOur purpose is to enhance the grammatical cohe-sion by exploiting the source CSS information.Therefore, theoretically, the conditional probabil-ity of a target translation es conditioned on thesource CSS-based tree ft is given by ( | )s tP e f ,and the final translation se  is obtained with thefollowing formula:argmax{P( | )} (1)?Ss s tee e fFollowing Och and Ney (2002), our model isframed as a log-linear model:exp ( , )( | ) (2)exp ( , )????
??
?sk k k s ts tk k k s thPhee fe fe' fwhere ( , )s th e f is a feature with weight?
.
Then,the best translation is:argmaxexp ( , ) (3)ss k k k s th??
?ee e fOur models make use of CSS with two strate-gies:1) CSS-based translation model: followingformula (1), we obtain the cohesion informationby modifying the translation rules with theirprobabilities ( | )s tP e f  based on word align-ments between the source CSS-tree and the tar-get string;2) CSS-based transfer model: followingformula (3), we introduce a transfer score to en-courage the decoder to generate transitionalwords and phrases; the score is utilized as an ad-ditional feature ( , )k s th e f  in the log-linear model.3.1 CSS-based Translation ModelFor the existing translation models, the entiretraining process is conducted at the lexical orsyntactic level without grammatically cohesiveinformation.
As a result, it is difficult to utilizesuch cohesive information during decoding.
In-stead, we reserve the cohesive information in thetraining process by converting the original sourcesentence into tagged-flattened CSS and then per-form word alignment and extract the translationrules from the bilingual flattened source CSS andthe target string.As introduced in Section 2, a CSS consists ofnodes, and a node can be represented as a tuple1 1[( , ),...( , ),...,( , )]L Ll lR s e s e s e?
.
In this represen-tation, the relationship R is the most importantfactor because different relationships directlyreflect different cohesive expressions.
In addition,the children?s positions always play a strong rolein choosing cohesive expressions because transi-tional expressions vary for children with differ-ent positions.
For example, when translating thelast child of a parallel relation, we always useword ?and?
as the transitional expression seen inFigure 3, but we will not use it for the first childof a parallel relation.
Therefore, in the trainingprocess we just keep the information of relation-ships and children?s positions when convertingDespite    frequent    calls   for   cutting   pollution  ,   and   growing   public   anger   ,<Parallel  @B>  ??
??
??
?
??
??
?
<Parallel  @E>  ??
??
??
?parallel??
??
??
?
??
??
?
??
??
??
?Despite    frequent    calls   for   cutting   pollution  ,   and   growing   public   anger   ,(a)(b)Original hierarchical rules:[X] ??
|||  and growing [X]Modified hierarchical rules:<parallel  @E >  [X] ??
|||  and growing [X](c)Figure 3: An example of modifying translation rules.
@B means the current structure informationcomes from the first child, and @E means from the last child.852the source CSS to a tagged-flattened string.Considering that the absolute position (indexof the eu, such as 1, 2, 3) is somehow sparse inthe corpus, we employ the relative position in-stead.
B (Beginning) represents the first child ofa relationship, E (End) means the last child of arelationship, and M (Middle) represents all themiddle children.Under this agreement, the original ChineseCSS-based tree will be converted to a newtagged-flattened string.
Note the converting ex-ample from Figure 3(a) to Figure 3(b): node par-allel-[(1,1), (2,2)] (see Figure 2) is converted toa flat string.
Its first child is represented as <par-allel, @B> with the semantic span, while the lastchild is <parallel, @E> with the correspondingsemantic span.We then perform word alignment on the modi-fied bilingual sentences, and extract the newtranslation rules based on the new alignment, asshown in Figure 3(b) to Figure 3(c).
Now thenewly extracted rule ?<parallel, @E > [X] ?
?||| and growing [X] ?
is tagged with cohesive in-formation.
Thus, if the similar relationship paral-lel occurs in the test source sentence, this type ofrule is more likely to be chosen to generate thecohesive word ?and?
during decoding because itis more discriminating than the original rules ([X]??
||| and growing [X]).
The conditional prob-abilities of the new translation rules are calculat-ed following (Chiang, 2005).3.2 CSS-based Transfer modelIn general, according to formula (3), the transla-tion quality based on the log-linear model is re-lated tightly with the features chosen.
Most trans-lation systems adopt the features from a transla-tion model, a language model, and sometimes areordering model.
To give a bonus to generatingcohesive expressions during decoding, we havedesigned a special additional feature.
The addi-tional feature is represented as a probability cal-culated by a transfer model.Given the source CSS information, we wantour transfer model to predict the most possiblecohesive expressions.
For example, given twosemantic spans with a parallel relationship andmany translation candidates, our transfer modelis expected to assign higher scores to those withtransitional expressions such as ?and?
or ?as wellas?.Let0 1, ,... nw w w?w  represent the transitionalexpressions observed in the target string.
Ourtransfer model can be represented as a condition-al probability:( | ) (4)P CSSwBy deriving each node of the CSS, we canobtain a factored formula:,( | ) ( | , ) (5)i j ij i jP CSS P R RP?
?w wwhereijwis the transitional expression producedby the thj child of the thi node of the CSS.
iR isthe relationship type of the thi node.
For the thjchild in the thi  node,jRPis its relative position(B, M or E) introduced in Section 3.1.The process of training this transfer model andsmoothing is similar to the process of training alanguage model.
We obtain the factored transferprobability as follows,110 0( | , )( | , ) ( | , , ) (6)ij i jinkj kki jP R RPP w R RP P w w R RP???
?wwhere0 0 ,... (7)nij nw w w?
?wFollowing (Bilmes and Kirchhoff, 2003), theconditional probabilities 10( | , , )ik jkP w w R RP?
informula (6) are estimated in the same way as afactored language model, which has the ad-vantage of easily incorporating various linguisticinformation.Considering thatijwcommonly appears at thebeginning of the target translation of a sourcesemantic span such as ?which ?
?, namely, theleft-frontier phrases, we focus only on the left-frontier phrases when training this model.
Notethat if there exists a target word before a leftfrontier, and this word is aligned to NULL, wewill expand the left frontier to this word.
Theexpansion process will be repeated until there isno such word.
For example, if we take the CSSand the alignment in Figure 3(a) for training, theleft frontier of the second child will be expandedfrom ?growing?
to ?and?.
In addition, taking thetri-gram left-frontier phrase for example, we canobtain a training sample such asijw= and grow-ing public, R=parallel, RP = E.By learning such probabilities for differenttransitional expressions conditioned on differentrelationships, we are able to capture the innerconnection between the source CSS and the pro-jected target cohesive phrases.
Thus, during de-coding, if we add the probability generated bythe transfer model of ( | )P CSSw as a feature in853formula (3), it will certainly contribute to select-ing more cohesive candidates.3.3 Elementary-Unit Cohesion ConstraintAs mentioned in Section 3.2, in the transfermodel, the transitional phrases are expected tooccur at the left frontier of a projected span ontarget side.
In fact, this depends on the assump-tion that the projected translations of any twodisjoint source semantic spans are also disjoint tokeep their own semantic integrity.
We call thisassumption the integrity assumption.
This as-sumption is intuitive and supported by statistics.After analyzing 1,007 golden aligned Chinese-English sentence-pairs, we find that approxi-mately 90% of the pairs comply with the as-sumption.
However, in real automatically alignednoisy data, the ratio of complying pairs reducesto 71%4.
Two projected translations that violatethe integrity assumption may mutually overlap,which causes our confusion on where to extractthe transitional phrases.
In this case, extractedtransitional phrases are likely to be wrong.To increase the chance of extracting correcttransitional phrases, the alignment results mustbe modified to reduce the impact of incorrectalignment.
We propose a dynamic cleaningmethod to ensure that the most expressive transi-tional phrases fall in the accessible extractionrange before training the transfer model.3.3.1 EUC and non-EUCAs we have defined in Section 2, the minimalsemantic span is called elementary unit (eu).
Ifthe source eu and its projected target span com-ply with the integrity assumption, we say thatsuch an eu and its projected span have Elemen-tary-Unit-Cohesion (EUC).
We define EUCformally as follows.Given two elementary unitsAeu  and Beu ,and their projected target spansAps and Bpsbound by the word alignment, the alignmentcomplies with EUC only if there is no overlapbetweenAps  and Bps .
Otherwise, the alignmentis called non-EUC.
The common EUC and non-EUC cases are illustrated in Figure 4.EUC is the basic case for the integrity as-sumption.
For the best cases, the elementaryunits comply with EUC, and thus the semantic4 The aligning tool is GIZA++ with 5 iterations of Model 1,5 iterations of HMM, and 10 iterations of Model 4.
TheGIZA++ code can be downloaded fromhttps://code.google.com/p/giza-pp/spans combined by elementary units are certainlysubject to the integrity assumption.uA euBpsA psB(a) mono EUC caseeuA euBpsApsB(b) swap EUC case euA euBpsA psB(c) non-EUC caseFigure.4 The schematic diagram of EUC casesand non-EUC case.3.3.2 A Dynamic Cleaning MethodAn intuitive method to clean the alignment re-sults is to drop off the noisy word-to-word linksthat cause non-EUC.
Considering that the drop-ping process is a post-editing method for theoriginal alignment obtained by a state-of-the-artaligner such as GIZA++, we do not expect over-deleting.
Therefore, we tend to take a relativelyconservative strategy to minimize the deletingoperation.Given a sentence-pair (f, e), suppose that0{ ,..., ,..., }i If f f?f  is divided into M elemen-tary units0{ ,..., ,..., }m MU u u u?
, and e has Nwords, that is,0{ ,..., ,..., }n Ne e e?e .
If A  is theword alignment of (f, e), then the goal is to con-struct the maximum subset *A A?
under thecondition that *A  is the word alignment with theconstraint of EU.
The search process can be de-scribed as the pseudo code in Figure 5.In Figure 5, we scan each target word and eachsource eu to assign each word to a unique eu un-der the EUC constraint with the lowest cost.Function cost( , )n m  in line 6 computes thecounts of deleted links that force the thn targetword to align only to words in the range of thethm eu.
For example, if the thn target word isaligned to the thi , ( 1)thi ?
, and ( 2)thi ?
word insource side, while the thi word belongs to1`muand the ( 1)thi ?
and ( 2)thi ?
words belong to2mu, then1cost( , ) 2mn u ?, and2cost( , ) 1mn u ?.In line 6, Score[n][m] saves a list of scores, eachscore computed by adding the current cost(n, m)with the history score of each list of Score[n-1].854Before the next iteration, the bad branches arepruned, as seen in line 5.
We adopt the followingtwo ways to prune:(1) EUC constraint: if the current link violatesEUC alignment, delete it.
(2) Keep the hypothesis with a fixed maximumsize to avoid too large a searching space.Figure 5.
The pseudo code of dynamic cleaningmethod.4 Experiments4.1 Experimental SetupTo obtain the CSSs of Chinese sentences, we usethe Chinese parser proposed in (Tu et al, 2013a).Their parser first segments the compound-complex sentence into a series of elementaryunits, and then builds structure of the hierarchicalrelationships among these elementary units.Their parser was reported to achieve an F-scorefor elementary unit segmentation of approxi-mately 0.89.
The progressive, causal, and condi-tion terms of functional relationships can be rec-ognized with precisions of 0.86, 0.8, and 0.75,respectively, while others, such as purpose, par-allel, and flowing, achieve only 0.5, 0.59 and0.62, respectively.The translation experiments have been con-ducted in the Chinese-to-English direction.
Thebilingual training data for translation model andCSS-based transfer model is FBIS corpus withapproximately 7.1 million Chinese words and 9.2million English words.
We obtain the wordalignment with the grow-diag-final-and strategywith GIZA++.
Before training the CSS-basedtransfer model, the alignment for transfer modelis modified by our dynamic cleaning method.During the cleaning process, the maximum sizeof hypothesis is limited to 5.
A 5-gram languagemodel is trained with SRILM5 on the combina-tion of the Xinhua portion of the English Giga-word corpus combined with the English part ofFBIS.
For tuning and testing, we use NIST03evaluation data as the development set.NIST04/05/06, CWMT08-Development 6  andCWMT08-Evaluation data are used for testingunder the measure metric of BLEU-4 (Papineniet al 2002) with the shortest length penalty.Table 1 shows how the CSS is distributed inall testing sets.
According to the statistics in Ta-ble 1, we see that CSS is really widely distribut-ed in the NIST and CWMT corpora, which im-plies that the translation quality may benefit sub-stantially from the CSS information, if it is wellconsidered in SMT.4.2 Extracted Transitional ExpressionsEleven types of Chinese functional relationshipsand their English left-frontier phrases (tri-gram)learned by our transfer model are given in Table2.The results in Table 2 show that some left-frontier phrases reflect the source functional rela-tionship well, especially for those with betterprecision of relationship recognition, such asprogressive, causal and condition.
Conversely,lower precision of relationship recognition mayweaken the learning ability of the transfer model.For example, noisy left-frontier phrases are easi-ly generated under relationships such as paralleland purpose.5 http://www.speech.sri.com/projects/srilm/6 The China Workshop on Machine Translation//Pseudo code for dynamic cleaning1: Score [N+1][M]={[0]}N M?
/* initializecumulative cost score chart*/2: Path [M]=[[]]                  /*initialize tracking path*/3: forn = 1 N?
:{           /*  scan target words*/4:   for 0 1m M?
?
?
:{        /*scan source U set */5:     PrunePath();/* prune invalid  path and high-cost path*/6:     Score[n][m]=GetScore(Score[n-1], cost(n, m))/*compute current cumulative cost score by previ-ous score and current cost*/7:      SaveCurrentPath(Path[m]);/*add current index to Path*/8:  }//end m9:}//end n10: OptimalPath =[ ]argmax{ [ ][ ]}Path m Score N m;Total CSS Ratio(%)NIST04 1,788 1,307 73.1NIST05 1,082 849 78.5NIST06 1,000 745 74.5CWMT08-Dev.
1,006 818 81.3CWMT08-Eval.
1,006 818 81.3Table 1.
The numbers of sentences and theCSS ratios of all sentences.
CWMT08-Dev.
isshort for CWMT08 Development data andCWMT08-Eval.
is CWMT08 Evaluation da-ta.8554.3 Results on SMT with Different StrategiesFor this work, we use an in-house decoder tobuild the SMT baseline; it combines the hierar-chical phrase-based translation model (Chiang,2005; Chiang, 2007) with the BTG (Wu, 1996)reordering model (Xiong et al, 2006; Zens andNey, 2006; He et al, 2010).To test the effectiveness of the proposed mod-els, we have compared the translation quality ofdifferent integration strategies.
First, we adoptedonly the tagged-flattened rules in the hierarchicaltranslation system.
Next, we added the log prob-ability generated by the transfer model as a fea-ture into the baseline features.
The baseline fea-tures include bi-directional phrase translationprobabilities, bi-directional lexical translationprobabilities, the BTG re-ordering features, andthe language model feature.
The tri-gram left-frontier phrase was adopted in the experiment.Then the probability generated by the transfermodel with EUC constraint is added.
Finally, weincorporated the tagged-flattened rules and theadditional transfer model feature together.Table 3 shows the results of these different in-tegrated strategies.
In Table 3, almost all BLEUscores are improved, no matter what strategy isused.
In particular, the best performance markedin bold is as high as 1.24, 0.94, and 0.82 BLEUpoints, respectively, over the baseline system onNIST04, CWMT08 Development, and CWMT08Evaluation data.
The strategy of ?TFS+ Flat-tened Rule?
is the most stable.
Meanwhile the?Flattened Rule?
achieves better performancethan ?TFS?.
The merits of ?Flattened Rule?
aretwo-fold: 1) In training process, the new wordalignment upon modified sentence pairs canalign transitional expressions to flattened CSStags; 2) In decoding process, the CSS-based rulesare more discriminating than the original rules,which is more flexible than ?TFS?.
From thetable, we cannot conclude that the EUC con-straint will certainly promote translation quality,but the transfer model performs better with theconstraint on most testing sets.4.4 Analysis of Different Effects of DifferentN-gramsAs mentioned in Section 4.3, we have noted theeffectiveness of tri-gram transfer model, whichmeans 2n ?
in formula (7).
In fact, the lengths ofcommon transitional expressions vary from oneword to several words.
To evaluate the effects ofdifferent n-grams for our proposed transfer mod-el, we compared the uni-/bi-/tri-gram transfermodels in SMT, and illustrate the results in Fig-Relation Left-frontier phrases (tri-gram)parallel as well as;   at the same; ?progressive but will also; in addition to;?causaltherefore , the;   for this reason;   as aresult; because it is;   so it is;?condition as long as;   only when the?hypothesis if we do; if it is;  if the us; ?alternative regardless of whether;?purposeit is necessary;further promote the ;?explanation that is ,;  the first is; first is the;?adversative however , the ;  but it is; ?flowing this is a; which is an; ?consequence so that the; to ensure that?Table 2.
Chinese functional relations and theircorresponding English left-frontier phraseslearned by our transfer model.
The noun phrasesstarting with a definite / indefinite word are fil-tered because they are unlikely to be the transi-tional phrases.NIST04 NIST05 NIST06CWMT08?sDev.CWMT08?sEval.Baseline   33.42   31.99   33.88       26.14       23.88+Flattened Rule   34.54**   32.32   34.58**       26.79**       24.70**+TFS (without EUC)   33.93**   32.04   34.40*       26.44       24.58**+TFS   33.84**   32.63*   34.15       27.08**       24.65**+TFS+ Flattened Rule   34.66**   32.54 34.52**       26.87**       24.49**+ Flattened Rule: only use the tagged-flattened translation rules+ TFS:  only use the transfer model score as an additional feature (based on 3-gramtransitional phrase)+ TFS + Flattened Rule: both are used*: value with * means that it is significantly better than the baseline with p<0.05**: value with ** means that it is significantly better than the baseline with p<0.01Table 3.
BLEU scores of the testing sets with different integrating strategies856ure 6.
In this experiment, the CSS-based transla-tion rules and the CSS-based transfer model areboth incorporated.
Considering time and compu-ting resources, in the rest of our paper, our analy-sis is conducted on NIST05 and NIST06.We choose 0,1, 2n ?
in this experiment forthat the common English transitional expressionsare primarily conjunctions, most of which areless than 4 words.
Results in Figure 6 show thatthe uni-gram and tri-gram transitional expres-sions seem more fitting for our transfer model.One possible reason is that uni-gram or tri-gramconjunctions are more utilized in an English text.In a conjunction expression list proposed by(Williams, 1983) which summarizes the differ-ent kinds of conjunctions based on the work ofHalliday and Hassan (1976), we obtain the statis-tical results on uni-/bi-/tri-gram expressions,which are about 52.1%/16.9%/23.9% respective-ly.4.5 Experiments on Big Training DataTo further evaluate the effectiveness of the pro-posed models, we also conducted an experimenton a larger set of bilingual training data from theLDC corpus7 for translation model and transfermodel.
The training corpus contains 2.1M sen-tence pairs with approximately 27.7M Chinesewords and 31.9M English words.
All the othersettings were the same as the SMT experimentsof sub-section 4.3.
The final BLEU scores onNIST05 and NIST06 are given in Table 4.The results in Table 4 further verify the effec-tiveness of our proposed models.
The best per-formance with bold marking scored as high as0.83 and 0.64 BLEU points, respectively over the7 LDC category number: LDC2000T50, DC2002E18,LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27,LDC2005T10 and LDC2005T34.baseline system on NIST05 and NIST06 evalua-tion data.4.6 Translation ExamplesTwo SMT examples of Chinese-to-English aregiven in Table 5.
We observe that compared tothe baseline, our approach has obvious ad-vantages on translating the implicit relations, dueto generating translational expressions on targetside.
Moreover, with the transitional expressions,cohesion of the entire translation improves.
No-tably, the transitional expressions in this worklike ?including, there are, the core of which?
arenot linguistic conjunctions.
We would like to callthem ?generalized?
conjunctions, because theytie semantic fragments together, analogously tolinguistic conjunctions.5 Related WorkImproving cohesion for complex sentences ordiscourse translation has attracted much attentionin recent years.
Such research efforts can beroughly divided into two groups: 1) research onlexical cohesion, which mainly contributes to theselection of generated target words; 2) efforts toimprove the grammatical cohesion, such as dis-ambiguation of references and connectives.In lexical cohesion work, (Gong et al, 2011;Xiao et al, 2011; Wong and Kit, 2012) built dis-course-based models to ensure lexical cohesionor consistency.
In (Xiong et al, 2013a), threedifferent features were designed to capture thelexical cohesion for document-level machinetranslation.
(Xiong et al, 2013b) incorporatedlexical-chain-based models (Morris and Hirst,1991) into machine translation.
They generatedthe target lexical chains based on the sourceFigure 6.
Different translation qualities alongwith different n-grams for transfer model.303132333435NIST05 NIST06BLEUTesting SetUni-gramBi-gramTri-gramNIST05 NIST06Baseline    35.20     35.52+Flattened Rule    36.03** 36.10*+TFS    35.56* 36.04*+TFS +Flattened Rule    36.02**    36.16**+ Flattened Rule: only use the tagged-flattened transla-tion rules+ TFS:  only use the transfer model score as an addi-tional feature (3-gram transitional phrase)+ TFS + Flattened Rule: both are used*: value with * means that it is significantly better thanthe baseline with p<0.05**: value with ** means that it is significantly betterthan the baseline with p<0.01Table 4.
BLEU scores on the large-scale trainingdata.857chains via maximum entropy classifiers, andused the target chains to work on the word selec-tion.Limited work has been conducted on gram-matical cohesion.
(Marcu et al, 2000) designed adiscourse structure transfer module, but it fo-cused on converting the semantic structure ratherthan actual translation.
(Tu et al, 2013b) provid-ed a Rhetorical-Structure-Theory-based tree-to-string translation method for complex sentenceswith explicit relations inspired by (Marcu et al,2000), but their models worked only for explicitfunctional relations, and they were concernedmainly with the translation integrity of semanticspan rather than cohesion.
(Meyer and Popescu-Belis, 2012) used sense-labeled discourse con-nectives for machine translation from English toFrench.
They added the labels assigned to con-nectives as an additional input to an SMT system,but their experimental results show that the im-provements under the evaluation metric of BLEUwere not significant.
(Nagard and Koehn, 2010)addresses the problems of reference or anaphoraresolution inspired by work of Mitkov et al(1995).To the best of our knowledge, our work is thefirst attempt to exploit the source functional rela-tionship to generate the target transitional ex-pressions for grammatical cohesion, and we havesuccessfully incorporated the proposed modelsinto an SMT system with significant improve-ment of BLEU metrics.6 ConclusionIn this paper, we focus on capturing cohesioninformation to enhance the grammatical cohesionof machine translation.
By taking the source CSSinto consideration, we build bridges to connectthe source functional relationships in CSS to tar-get transitional expressions; such a process isvery similar to human translating.Our contributions can be summarized as: 1)the new translation rules are more discriminativeand sensitive to cohesive information by convert-ing the source string into a CSS-based tagged-flattened string; 2) the new additional featuresembedded in the log-linear model can encouragethe decoder to produce transitional expressions.The experimental results show that significantimprovements have been achieved on varioustest data, meanwhile the translations are morecohesive and smooth, which together demon-strate the effectiveness of our proposed models.In the future, we will extend our methods toother translation models, such as the syntax-based model, to study how to further improve theperformance of SMT systems.
Besides, morelanguage pairs with various linguistic structureswill be taken into consideration.AcknowledgementWe would like to thank Jiajun Zhang for provid-ing the BTG-based hierarchical decoder.
Theresearch work has been partially funded by theNatural Science Foundation of China underGrant No.
61333018, the Hi-Tech Research andDevelopment Program (?863?
Program) of Chinaunder Grant No.
2012AA011101, and alsothe Key Project of Knowledge Innovation Pro-gram of Chinese Academy of Sciences underGrant No.
KGZD-EW-501 as well.Source ??????????????????
?????????????????
?
?ReferenceIn the past three years, the sequencing of three chromosomes has been completed, includingchromosomes 20 , 21 , and 22 .BaselineIn the past three years , now has three terms of the completion of the chromosomes , 20 , 21and 22 .ImprovedIn the past three years , there are three chromosomes to accomplish , including 20 , 21 and22 .Source ??????????????????????????????????
?ReferenceThe above-mentioned propositions constitute the basic connotation of this one-china principlewith safeguarding china ' s sovereignty and territorial integrity as its core .BaselineThe above-mentioned propositions constitute the basic meaning of the one-china principle isthe core of safeguard china ' s sovereignty and territorial integrity .ImprovedThe above-mentioned propositions constitute the basic meaning of the one-china principle ,the core of which is to safeguard china ' s sovereignty and territorial integrity .Table 5.
Examples of baseline and the improved system outputs.858ReferencesJeff A. Bilmes and Katrin Kirchhoff.
Factored lan-guage models and generalized parallel backoff.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Compu-tational Linguistics on Human Language Technol-ogy: companion volume of the Proceedings ofHLT-NAACL 2003--short papers-Volume 2: 4-6.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 263?270.David Chiang.
2007.
Hierarchical phrase-basedtranslation.
Computational Linguistics, pages33(2):201?228.Zhengxian Gong, Min Zhang, and Guodong Zhou.Cache-based document-level statistical machinetranslation, 2011, Edinburgh, Scotland, UK.
InProceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages909?919.Liane Guillou.
2013.
Analysing lexical consistency intranslation.
In Proceedings of the Workshop onDiscourse in Machine Translation, pages 10?18,SofiaMichael A.K.
Halliday, Hasan R. Cohesion in English.1976.
London: Longman.Zhongjun He, Yao Meng, and Hao Yu.
2010b.
Maxi-mum Entropy Based Phrase Reordering for Hier-archical Phrase-based Translation.
In Proc.
of theConf.
on Empirical Methods for Natural LanguageProcessing (EMNLP), pages 555?563.Annie Louis and Ani Nenkova.
2012.
A coherencemodel based on syntactic patterns.
In Proceedingsof the 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning, pages 1157?1168, Jeju Island, Korea, July.William C Mann and Sandra A Thompson.
1988.Rhetorical structure theory: Toward a functionaltheory of text organization.
Text, 8(3):243?281.Ruslan Mitkov, Sung-Kwon Choi, and Randall Sharp.1995.
Anaphora resolution in Machine Transla-tion.
In Proceedings of the Sixth InternationalConference on Theoretical and Methodological Is-sues in Machine Translation.Thomas Meyer and Andrei Popescu-Belis.
Usingsense-labeled discourse connectives for statisticalmachine translation, 2012, In Proceedings of theJoint Workshop on Exploiting Synergies betweenInformation Retrieval and Machine Translation(ESIRMT) and Hybrid Approaches to MachineTranslation (HyTra), pages:129-138.Jane Morris and Graeme Hirst.
1991.
Lexical cohe-sion computed by thesaural relations as an indica-tor of the structure of text.
Comput.
Linguist.,17(1):21?48, March.Ronan L Nagard and Philipp Koehn.
2010, Aidingpronoun translation with co-reference resolution,In proceedings of the Joint Fifth Workshop on Sta-tistical Machine Translation and MetricsMATR,pages 252-261.Franz J Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statisti-cal machine translation.
In Proc.
of ACL, pages295?302.Kishore Papineni, Salim Roukos, Todd Ward, et al2002, BLEU: a method for automatic evaluationof machine translation.
In proceedings of the 40thannual meeting on association for computationallinguistics.
pages: 311-318.Rashmi Prasad, Nikhil Dinesh, Alan Lee, EleniMiltsakaki, Livio Robaldo, Aravind Joshi, andBonnie Webber.
2008.
The Penn Discourse Tree-bank 2.0.
In Proceedings of the 6th InternationalConference on Language Resources and Evalua-tion (LREC 2008).Williams Ray.
Teaching the Recognition of CohesiveTies in Reading a Foreign, 1983.
Reading in aforeign language, 1(1), pages: 35-52.Radu Soricut and Daniel Marcu.
2003.
Sentence leveldiscourse parsing using syntactic and lexical in-formation.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human Lan-guage Technology-Volume 1, pages 149?156.Mei Tu, Yu Zhou, and Chengqing Zong.
2013a, ANovel Translation Framework Based on Rhetori-cal Structure Theory.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics, short paper, Sofia, Bulgaria,pages 370?374.Mei Tu, Yu Zhou, Chengqing Zong.
2013b, Automat-ically Parsing Chinese Discourse Based on Maxi-mum Entropy.
In The 2nd Conference on NaturalLanguage Processing & Chinese Computing.Ashish Vaswani, Liang Huang and David Chiang,Huang L, Chiang D. 2012, Smaller alignmentmodels for better translations: unsupervised wordalignment with the l 0-norm.
In Proceedings of the50th Annual Meeting of the Association for Com-putational Linguistics: Long Papers-Volume1,pages 311-319.Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.Document-level consistency verification in ma-chine translation.
September 2011, Xiamen, China.In Proceedings of the 2011 MT summit XIII, pag-es 131?138.859Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model forstatistical machine translation.
In Proceedings ofthe 44th Annual Meeting of the Association forComputational Linguistics, pages 521?528.Deyi Xiong, Guosheng Ben, Min Zhang, Yajuan Lv,and Qun Liu.
2013 (a).
Modeling lexical cohesionfor document-level machine translation.
In Pro-ceedings of the Twenty-Third International JointConference on Artificial Intelligence (IJCAI-13),Beijing, China, August.Deyi Xiong, Ding Yang, Min Zhang and Chew LimTan, 2013 (b).
Lexical Chain Based CohesionModels for Document-Level Statistical MachineTranslation.
In Proceedings of the 2013 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages: 1563-1573.Richard Zens and Hermann Ney.
2006.
Discrimina-tive reordering models for statistical machinetranslation.
In Proceedings of theWorkshop onStatistical Machine Translation, pages 55?63.Qiang Zhou, 2004, Annotation Scheme for ChineseTreebank, Journal of Chinese Information Pro-cessing, 18(4): 1-8.860
