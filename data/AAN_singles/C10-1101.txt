Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 895?903,Beijing, August 2010Citation Summarization Through Keyphrase ExtractionVahed QazvinianDepartment of EECSUniversity of Michiganvahed@umich.eduDragomir R. RadevSchool of Information andDepartment of EECSUniversity of Michiganradev@umich.eduArzucan ?Ozgu?rDepartment of EECSUniversity of Michiganozgur@umich.eduAbstractThis paper presents an approach to sum-marize single scientific papers, by extract-ing its contributions from the set of cita-tion sentences written in other papers.
Ourmethodology is based on extracting sig-nificant keyphrases from the set of cita-tion sentences and using these keyphrasesto build the summary.
Comparisons showhow this methodology excels at the taskof single paper summarization, and how itout-performs other multi-document sum-marization methods.1 IntroductionIn recent years statistical physicists and computerscientists have shown great interest in analyzingcomplex adaptive systems.
The study of such sys-tems can provide valuable insight on the behav-ioral aspects of the involved agents with potentialapplications in economics and science.
One suchaspect is to understand what motivates people toprovide the n+1st review of an artifact given thatthey are unlikely to add something significant thathas not already been said or emphasized.
Cita-tions are part of such complex systems where ar-ticles use citations as a way to mention differentcontributions of other papers, resulting in a col-lective system.The focus of this work is on the corpora cre-ated based on citation sentences.
A citation sen-tence is a sentence in an article containing a ci-tation and can contain zero or more nuggets (i.e.,non-overlapping contributions) about the cited ar-ticle.
For example the following sentences are afew citation sentences that appeared in the NLPliterature in past that talk about Resnik?s work.The STRAND system (Resnik, 1999), for example, usesstructural markup information from the pages, withoutlooking at their content, to attempt to align them.Resnik (1999) addressed the issue oflanguage identification for finding Web pages inthe languages of interest.Mining the Web for bilingual text (Resnik, 1999) is notlikely to provide sufficient quantities of high qualitydata..The set of citations is important to analyze be-cause human summarizers have put their effortcollectively but independently to read the targetarticle and cite its important contributions.
Thishas been shown in other work too (Elkiss et al,2008; Nanba et al, 2004; Qazvinian and Radev,2008; Mei and Zhai, 2008; Mohammad et al,2009).
In this work, we introduce a techniqueto summarize the set of citation sentences andcover the major contributions of the target paper.Our methodology first finds the set of keyphrasesthat represent important information units (i.e.,nuggets), and then finds the best set of k sentencesto cover more, and more important nuggets.Our results confirm the effectiveness of themethod and show that it outperforms other stateof the art summarization techniques.
Moreover,as shown in the paper, this method does not needto calculate the full cosine similarity matrix for adocument cluster, which is the most time consum-ing part of the mentioned baseline methods.1.1 Related WorkPrevious work has used citations to produce sum-maries of scientific work (Qazvinian and Radev,8952008; Mei and Zhai, 2008; Elkiss et al, 2008).Other work (Bradshaw, 2003; Bradshaw, 2002)benefits from citations to determine the content ofarticles and introduce ?Reference Directed Index-ing?
to improve the results of a search engine.In other work, (Nanba and Okumura, 1999) an-alyze citation sentences and automatically cate-gorize citations into three groups using 160 pre-defined phrase-based rules to support a system forwriting a survey.
Previous research has shownthe importance of the citation summaries in un-derstanding what a paper contributes.
In partic-ular, (Elkiss et al, 2008) performed a large-scalestudy on citation summaries and their importance.Results from this experiment confirmed that the?Self Cohesion?
(Elkiss et al, 2008) of a citationsummary of an article is consistently higher thanthe that of its abstract and that citations containadditional information that does not appear in ab-stracts.Kan et al (2002) use annotated bibliographiesto cover certain aspects of summarization and sug-gest using metadata and critical document featuresas well as the prominent content-based features tosummarize documents.
Kupiec et al (1995) usea statistical method and show how extracts canbe used to create summaries but use no annotatedmetadata in summarization.Siddharthan and Teufel describe a new task todecide the scientific attribution of an article (Sid-dharthan and Teufel, 2007) and show high hu-man agreement as well as an improvement in theperformance of Argumentative Zoning (Teufel,2005).
Argumentative Zoning is a rhetorical clas-sification task, in which sentences are labeled asone of Own, Other, Background, Textual, Aim,Basis, Contrast according to their role in the au-thor?s argument.
These all show the importanceof citation summaries and the vast area for newwork to analyze them to produce a summary for agiven topic.The Maximal Marginal Relevance (MMR)summarization method, which is based on agreedy algorithm, is described in (Carbonell andGoldstein, 1998).
MMR uses the full similaritymatrix to choose the sentences that are the leastsimilar to the sentences already selected for thesummary.
We selected this method as one of ourFact Occurrencesf1: ?
Supervised Learning?
5f2: ?
instance/concept relations?
3f3: ?Part-of-Speech tagging?
3f4: ?filtering QA results?
2f5: ?lexico-semantic information?
2f6: ?hyponym relations?
2Table 2: Nuggets of P03-1001 extracted by anno-tators.baseline methods, which we have explained inmore details in Section 4.2 DataIn order to evaluate our method, we use the ACLAnthology Network (AAN), which is a collec-tion of papers from the Computational Linguisticsjournal and proceedings from ACL conferencesand workshops and includes more than 13, 000 pa-pers (Radev et al, 2009).
We use 25 manually an-notated papers from (Qazvinian and Radev, 2008),which are highly cited articles in AAN.
Table 1shows the ACL ID, title, and the number of cita-tion sentences for these papers.The annotation guidelines asked a number ofannotators to read the citation summary of eachpaper and extract a list of the main contribu-tions of that paper.
Each item on the list is anon-overlapping contribution (nugget) perceivedby reading the citation summary.
The annota-tion strictly instructed the annotators to focus onthe citing sentences to do the task and not theirown background on the topic.
Then, extractednuggets are reviewed and those nuggets that haveonly been mentioned by 1 annotator are removed.Finally, the union of the rest is used as a set ofnuggets representing each paper.Table 2 lists the nuggets extracted by annotatorsfor P03-1001.3 MethodologyOur methodology assumes that each citation sen-tence covers 0 or more nuggets about the citedpapers, and tries to pick sentences that maximizenugget coverage with respect to summary length.These nuggets are essentially represented usingkeyphrases.
Therefore, we try to extract signifi-cant keyphrases in order to represent nuggets eachsentence contains.
Here, the keyphrases are ex-896ACL-ID Title # citationsN03-1017 Statistical Phrase-Based Translation 180P02-1006 Learning Surface Text Patterns For A Question Answering System 74P05-1012 On-line Large-Margin Training Of Dependency Parsers 71C96-1058 Three New Probabilistic Models For Dependency Parsing: An Exploration 66P05-1033 A Hierarchical Phrase-Based Model For Statistical Machine Translation 65P97-1003 Three Generative, Lexicalized Models For Statistical Parsing 55P99-1065 A Statistical Parser For Czech 54J04-4002 The Alignment Template Approach To Statistical Machine Translation 50D03-1017 Towards Answering Opinion Questions: Separating Facts From Opinions ... 42P05-1013 Pseudo-Projective Dependency Parsing 40W00-0403 Centroid-Based Summarization Of Multiple Documents: Sentence Extraction, ... 31P03-1001 Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked 27N04-1033 Improvements In Phrase-Based Statistical Machine Translation 24A00-2024 Cut And Paste Based Text Summarization 20W00-0603 A Rule-Based Question Answering System For Reading Comprehension Tests 19A00-1043 Sentence Reduction For Automatic Text Summarization 19C00-1072 The Automated Acquisition Of Topic Signatures For Text Summarization 19W05-1203 Measuring The Semantic Similarity Of Texts 17W03-0510 The Potential And Limitations Of Automatic Sentence Extraction For Summarization 15W03-0301 An Evaluation Exercise For Word Alignment 14A00-1023 A Question Answering System Supported By Information Extraction 13D04-9907 Scaling Web-Based Acquisition Of Entailment Relations 12P05-1014 The Distributional Inclusion Hypotheses And Lexical Entailment 10H05-1047 A Semantic Approach To Recognizing Textual Entailment 8H05-1079 Recognising Textual Entailment With Logical Inference 9Table 1: List of papers chosen from AAN for evaluation together with the number of sentences citingeach.unique all max frequnigrams 229,631 7,746,792 437,308bigrams 2,256,385 7,746,791 73,9573-grams 5,125,249 7,746,790 3,6004-grams 6,713,568 7,746,789 2,408Table 3: Statistics on the abstract corpus in AANused as the background datapressed using N -grams, and thus these buildingunits are the key elements to our summarization.For each citation sentence di, our method first ex-tracts a set of important keyphrases, Di, and thentries to find sentences that have a larger number ofimportant and non-redundant keyphrases.
In orderto take the first step, we extract statistically sig-nificantly frequent N -grams (up to N = 4) fromeach citing sentence and use them as the set ofrepresentative keyphrases for that citing sentence.3.1 Automatic Keyphrase ExtractionA list of keyphrases for each citation sentence canbe generated by extracting N -grams that occursignificantly frequently in that sentence comparedto a large corpus of such N -grams.
Our methodfor such an extraction is inspired by the previ-ous work by Tomokiyo and Hurst (Tomokiyo andHurst, 2003).A language model, M, is a statistical modelthat assigns probabilities to a sequence of N -grams.
Every language model is a probability dis-tribution over all N -grams and thus the probabili-ties of all N -grams of the same length sum up to1.
In order to extract keyphrases from a text us-ing statistical significance we need two languagemodels.
The first model is referred to as the Back-ground Model (BM) and is built using a largetext corpus.
Here we build the BM using the textof all the paper abstracts provided in AAN1.
Thesecond language model is called the ForegroundModel (FM) and is the model built on the textfrom which keyphrases are being extracted.
Inthis work, the set of all citation sentences that citea particular target paper are used to build a fore-ground language model.Let gi be an N -gram of size i and CM(gi) de-note the count of gi in the modelM.
First, we ex-tract the counts of each N -grams in both the back-ground (BM) and the foreground corpora (FM).1http://chernobog.si.umich.edu/clair/anthology/index.cgi897MBM =Xgi?
{BM?FM}1NBM =Xgi?
{BM?FM}CBM(gi)NFM =Xgi?FMCFM(gi)p?FM(gi) = CFM(gi)/NFMp?BM(gi) = (CBM(gi) + 1)/(MBM +NBM)The last equation is also known as Laplacesmoothing (Manning and Schutze, 2002) and han-dles the N -grams in the foreground corpus thathave a 0 occurrence frequency in the backgroundcorpus.
Next, we extract N -grams from the fore-ground corpus that have significant frequenciescompared to the frequency of the same N -gramsin the background model and its individual termsin the foreground model.To measure how randomly a set of consecu-tive terms are forming an N -gram, Tomokiyo andHurst (Tomokiyo and Hurst, 2003) use pointwisedivergence.
In particular, for an N -gram of size i,gi = (w1w2 ?
?
?wi),?gi(FMi?FM1) = p?FM(gi) log(p?FM(gi)Qij=1 p?FM(wj))This equation shows the extent to which theterms forming gi have occurred together ran-domly.
In other words, it indicates the extent of in-formation that we lose by assuming independenceof each word by applying the unigram model, in-stead of the N -gram model.In addition, to measure how randomly a se-quence of words appear in the foreground modelwith respect to the background model, we usepointwise divergence as well.
Here, pointwise di-vergence defines how much information we loseby assuming that gi is drawn from the backgroundmodel instead of the foreground model:?gi(FMi?BMi) = p?FM(gi) log(p?FM(gi)p?BM(gi))(Corley and Mihalcea, 2005) applied or uti-lized lexical based word overlap measures.
{overlap measures, word overlap, lexicalbased, utilized lexical}Table 4: Example: citation sentence for W05-1203 written by D06-1621, and its extracted bi-grams.We set the criteria of choosing a sequence ofwords as significant to be whether it has posi-tive pointwise divergence with respect to both thebackground model, and individual terms of theforeground model.
In other words we extract all gifrom FM for which the both properties are posi-tive:?gi(FMi?BMi) > 0?gi(FMi?FM1) ?
0The equality condition in the second equationis specifically set to handle unigrams, in whichp?FM(gi) =?ij=1 p?FM(wj).In order to handle the text corpora and build-ing the language models, we have used the CMU-Cambridge Language Model toolkit (Clarksonand Rosenfeld, 1997).
We use the set of cita-tion sentences for each paper to build foregroundlanguage models.
Furthermore, we employ thistool and make the background model using nearly11,000 abstracts from AAN.
Table 3 summarizessome of the statistics about the background data.Once keyphrases (significant N -grams) of eachsentence are extracted, we remove all N -grams inwhich more than half of the terms are stopwords.For instance, we remove all stopword unigrams,if any, and all bigrams with at least one stop-word in them.
For 3-grams and 4-grams we usea threshold of 2 and 3 stopwords respectively.
Af-ter that, the set of remaining N -grams is used torepresent each sentence and to build summaries.Table 4 shows an example of a citation sentencefrom D06-1621 citing W05-1203 (Corley and Mi-halcea, 2005), and its extracted bigrams.3.2 Sentence SelectionAfter extracting the set of keyphrases for each sen-tence, di, the sentence is represented using its set898of N -grams, denoted by Di.
Then, the goal isto pick sentences (sets) for each paper that covermore important and non-redundant keyphrases.Essentially, keyphrases that have been repeated inmore sentences are more important and could rep-resent more important nuggets.
Therefore, sen-tences that contain more frequent keyphrases aremore important.
Based on this intuition we definethe reward of building a summary comprising aset of keyphrases S asf(S) = |S ?A|where A is the set of all keyphrases from sen-tences not in the summary.The set function f has three main properties.First, it is non-negative.
Second, it is mono-tone (i.e., For every set v we have f(S + v) ?f(S)).
Third, f is sub-modular.
The submodular-ity means that for a set v and two sets S ?
T wehavef(S + v)?
f(S) ?
f(T + v)?
f(T )Intuitively, this property implies that adding a setv to S will increase the reward at least as muchas it would to a larger set T .
In the summariza-tion setting, this means that adding a sentence toa smaller summary will increase the reward of thesummary at least as much as adding it to a largersummary that subsumes it.
The following theoremformalizes this and is followed by a proof.Theorem 1 The reward function f is submodular.ProofWe start by defining a gain function G of addingsentence (set) Di to Sk?1 where Sk?1 is the setof keyphrases in a summary built using k?
1 sen-tences, and Di is a candidate sentence to be added:G(Di,Sk?1) = f(Sk?1 ?Di)?
f(Sk?1)Simple investigation through a Venn diagramproof shows that G can be re-written asG(Di,Sk?1) = |Di ?
(?j 6=iDj)?
Sk?1|Let?s denote Di?
(?j 6=iDj) by ?i.
The follow-ing equations prove the theorem.Sk?1 ?
SkS ?k?1 ?
S ?k?i ?
S ?k?1 ?
?i ?
S ?k?i ?
Sk?1 ?
?i ?
Sk| ?i ?Sk?1| ?
| ?i ?Sk|G(Di,Sk?1) ?
G(Di,Sk)f(Sk?1 ?Di)?
f(Sk?1) ?
f(Sk ?Di)?
f(Sk)Here, S ?k is the set of all N -grams in the vo-cabulary that are not present in Sk.
The gain ofadding a sentence, Di, to an empty summary is anon-negative value.G(Di,S0) = C ?
0By induction, we will getG(Di,S0) ?
G(Di,S1) ?
?
?
?
?
G(Di,Sk) ?
02Theorem 1 implies the general case of submodu-larity:?m,n, 0 ?
m ?
n ?
|D| ?
G(Di,Sm) ?
G(Di,Sn)Maximizing this submodular function is an NP-hard problem (Khuller et al, 1999).
A commonway to solve this maximization problem is to startwith an empty set, and in each iteration pick a setthat maximizes the gain.
It has been shown be-fore in (Kulik et al, 2009) that if f is a submod-ular, nondecreasing set function and f(?)
= 0,then such a greedy algorithm finds a set S , whosegain is at least as high as (1 ?
1/e) of the bestpossible solution.
Therefore, we can optimize thekeyphrase coverage as described in Algorithm 1.4 Experimental SetupWe use the annotated data described in Section 2.In summary, the annotation consisted of two parts:nugget extraction and nugget distribution analy-sis.
Five annotators were employed to annotatethe sentences in each of the 25 citation summariesand write down the nuggets (non-overlapping con-tributions) of the target paper.
Then using these899Summary generated using bigram-based keyphrasesID SentenceP06-1048:1 Ziff-Davis Corpus Most previous work (Jing 2000; Knight and Marcu 2002; Riezler et al2003; Nguyen et al2004a; Turner and Charniak 2005;McDonald 2006) has relied on automatically constructed parallel corpora for training and evaluation purposes.J05-4004:18 Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore1996; Mahesh 1997; Carroll et al1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daume III and Marcu2002; Daume III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases, and sentences are selected in an extraction process.A00-2024:9 The evaluation of sentence reduction (see (Jing, 2000) for details) used a corpus of 500 sentences and their reduced forms in human-written abstracts.N03-1026:17 To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000)and Jing (2000).P06-2019:5 Jing (2000) was perhaps the first to tackle the sentence compression problem.Table 5: Bigram-based summary generated for A00-1043.Algorithm 1 The greedy algorithm for summarygenerationk ?
the number of sentences in the summaryDi ?
keyphrases in diS ?
?for l = 1 to k dosl ?
argmaxDi?D |Di ?
(?j 6=iDj)|S ?
S ?
slfor j = 1 to |D| doDj ?
Dj ?
slend forend forreturn Snugget sets, each sentence was annotated with thenuggets it contains.
This results in a sentence-fact matrix that helps with the evaluation of thesummary.
The summarization goal and the intu-ition behind the summarizing system is to select afew (5 in our experiments) sentences and cover asmany nuggets as possible.
Each sentence in a cita-tion summary may contain 0 or more nuggets andnot all nuggets are mentioned an equal number oftimes.
Covering some nuggets (contributions) istherefore more important than others and shouldbe weighted highly.To capture this property, the pyramid scoreseems the best evaluation metric to use.
We usethe pyramid evaluation method (Nenkova and Pas-sonneau, 2004) at the sentence level to evaluatethe summary created for each set.
We benefitfrom the list of annotated nuggets provided by theannotators as the ground truth of the summariza-tion evaluation.
These annotations give the list ofnuggets covered by each sentence in each citationsummary, which are equivalent to the summariza-tion content unit (SCU) as described in (Nenkovaand Passonneau, 2004).The pyramid score for a summary is calculatedas follows.
Assume a pyramid that has n tiers, Ti,where tier Ti > Tj if i > j (i.e., Ti is not belowTj , and that if a nugget appears in more sentences,it falls in a higher tier.).
Tier Ti contains nuggetsthat appeared in i sentences, and thus has weighti.
Suppose |Ti| shows the number of nuggets intier Ti, and Qi is the size of a subset of Ti whosemembers appear in the summary.
Further supposeQ shows the sum of the weights of the facts thatare covered by the summary.
Q =?ni=1 i?Qi.In addition, the optimal pyramid score for a sum-mary with X facts, isMax =nXi=j+1i?
|Ti|+ j ?
(X ?nXi=j+1|Ti|)where j = maxi(?nt=i |Tt| ?
X).
The pyra-mid score for a summary is then calculated as fol-lows.P = QMaxThis score ranges from 0 to 1, and a highscore shows the summary contains more heavilyweighted facts.4.1 Baselines and Gold StandardsTo evaluate the quality of the summaries gen-erated by the greedy algorithm, we compare itspyramid score in each of the 25 citation sum-maries with those of a gold standard, a randomsummary, and four other methods.
The gold stan-dards are summaries created manually using 5sentences.
The 5 sentences are manually selectedin a way to cover as many nuggets as possible withhigher priority for the nuggets with higher fre-quencies.
We also created random summaries us-ing Mead (Radev et al, 2004).
These summaries900are basically a random selection of 5 sentencesfrom the pool of sentences in the citation sum-mary.
Generally we expect the summaries cre-ated by the greedy method to be significantly bet-ter than random ones.In addition to the gold and random summaries,we also used 4 baseline state of the art sum-marizers: LexRank, the clustering C-RR andC-LexRank, and Maximal Marginal Relevance(MMR).
LexRank (Erkan and Radev, 2004) worksbased on a random walk on the cosine similar-ity of sentences and prints out the most frequentlyvisited sentences.
Said differently, LexRank firstbuilds a network in which nodes are sentences andedges are cosine similarity values.
It then uses theeigenvalue centralities to find the most central sen-tences.
For each set, the top 5 sentences on the listare chosen for the summary.The clustering methods, C-RR and C-LexRank,work by clustering the cosine similarity networkof sentences.
In such a network, nodes are sen-tences and edges are cosine similarity of nodepairs.
Clustering would intuitively put nodes withsimilar nuggets in the same clusters as they aremore similar to each other.
The C-RR method asdescribed in (Qazvinian and Radev, 2008) uses around-robin fashion to pick sentences from eachcluster, assuming that the clustering will put thesentences with similar facts into the same clus-ters.
Unlike C-RR, C-LexRank uses LexRank tofind the most salient sentences in each cluster, andprints out the most central nodes of each cluster assummary sentences.Finally, MMR uses the full cosine similaritymatrix and greedily chooses sentences that are theleast similar to those already selected for the sum-mary (Carbonell and Goldstein, 1998).
In partic-ular,MMR = arg mindi?D?A[maxdj?ASim(di, dj)]where A is the set of sentences in the summary,initially set to A = ?.
This method is differentfrom ours in that it chooses the least similar sen-tence to the summary in each iteration.4.2 Results and DiscussionAs mentioned before, we use the text of the ab-stracts of all the papers in AAN as the back-ground, and each citation set as a separate fore-ground corpus.
For each citation set, we use themethod described in Section 3.1 to extract signif-icant N -grams of each sentence.
We then use thekeyphrase set representation of each sentence tobuild the summaries using Algorithm 1.
For eachof the 25 citation summaries, we build 4 differ-ent summaries using unigrams, bigrams, 3-grams,and 4-grams respectively.
Table 5 shows a 5-sentence summary created using algorithm 1 forthe paper A00-1043 (Jing, 2000).The pyramid scores for different methods arereported in Figure 1 together with the scoresof gold standards, manually created to cover asmany nuggets as possible in 5 sentences, aswell as summary evaluations of the 4 baselinemethods described above.
This Figure showshow the keyphrase based summarization methodwhen employing N -grams of size 3 or smaller,outperforms other baseline systems significantly.More importantly, Figure 1 also indicates that thismethod shows more stable results and low varia-tion in summary quality when keyphrases of size 3or smaller are employed.
In contrast, MMR showshigh variation in summary qualities making sum-maries that obtain pyramid scores as low as 0.15.Another important advantage of this method isthat we do not need to calculate the cosine simi-larity of the pairs of sentences, which would add arunning time of O(|D|2|V |) in the number of doc-uments, |D|, and the size of the vocabulary |V | tothe algorithm.5 Conclusion and Future WorkThis paper presents a summarization methodol-ogy that employs keyphrase extraction to find im-portant contributions of scientific articles.
Thesummarization is based on citation sentences andpicks sentences to cover nuggets (represented bykeyphrases) or contributions of the target papers.In this setting the best summary would have as fewsentences and at the same time as many nuggetsas possible.
In this work, we use pointwise KL-divergence to extract statistically significant N -grams and use them to represent nuggets.
Wethen apply a new set function for the task of sum-marizing scientific articles.
We have proved thatthis function is submodular and concluded that a90100.10.20.30.40.50.60.70.80.91Gold Mead LexRank C?RR C?LexRank MMR 1?gram 2?gram 3?gram 4?gramPyramidScoreFigure 1: Evaluation Results (summaries with 5 sentences): The median pyramid score over 25 datasetsusing different methods.greedy algorithm will result in a near-optimum setof covered nuggets using only 5 sentences.
Ourexperiments in this paper confirm that the sum-maries created based on the presented algorithmare better than randomly generated summary, andalso outperform other state of the art summariza-tion methods in most cases.
Moreover, we showhow this method generates more stable summarieswith lower variation in summary quality when N -grams of size 3 or smaller are employed.A future direction for this work is to performpost-processing on the summaries and re-generatesentences that cover the extracted nuggets.
How-ever, the ultimate goal is to eventually developsystems that can produce summaries of entireresearch areas, summaries that will enable re-searchers to easily and quickly switch betweenfields of research.One future study that will help us generatebetter summaries is to understand how nuggetsare generated by authors.
In fact, modeling thenugget coverage behavior of paper authors willhelp us identify more important nuggets and dis-cover some aspects of the paper that would oth-erwise be too difficult by just reading the paperitself.6 AcknowledgementsThis work is in part supported by the NationalScience Foundation grant ?iOPENER: A Flexi-ble Framework to Support Rapid Learning in Un-familiar Research Domains?, jointly awarded toUniversity of Michigan and University of Mary-land as IIS 0705832, and in part by the NIH GrantU54 DA021519 to the National Center for Inte-grative Biomedical Informatics.Any opinions, findings, and conclusions or rec-ommendations expressed in this paper are thoseof the authors and do not necessarily reflect theviews of the supporters.ReferencesBradshaw, Shannon.
2002.
Reference Directed Index-ing: Indexing Scientific Literature in the Context ofIts Use.
Ph.D. thesis, Northwestern University.Bradshaw, Shannon.
2003.
Reference directed index-ing: Redeeming relevance for subject search in ci-tation indexes.
In Proceedings of the 7th European902Conference on Research and Advanced Technologyfor Digital Libraries.Carbonell, Jaime G. and Jade Goldstein.
1998.
Theuse of MMR, diversity-based reranking for reorder-ing documents and producing summaries.
In SI-GIR?98, pages 335?336.Clarkson, PR and R Rosenfeld.
1997.
Statistical lan-guage modeling using the cmu-cambridge toolkit.Proceedings ESCA Eurospeech, 47:45?148.Elkiss, Aaron, Siwei Shen, Anthony Fader, Gu?nes?Erkan, David States, and Dragomir R. Radev.
2008.Blind men and elephants: What do citation sum-maries tell us about a research article?
Journal ofthe American Society for Information Science andTechnology, 59(1):51?62.Erkan, Gu?nes?
and Dragomir R. Radev.
2004.
Lexrank:Graph-based centrality as salience in text summa-rization.
Journal of Artificial Intelligence Research.Jing, Hongyan.
2000.
Sentence reduction for auto-matic text summarization.
In Proceedings of thesixth conference on Applied natural language pro-cessing, pages 310?315, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Kan, Min-Yen, Judith L. Klavans, and Kathleen R.McKeown.
2002.
Using the Annotated Bibliogra-phy as a Resource for Indicative Summarization.
InProceedings of LREC 2002, Las Palmas, Spain.Khuller, Samir, Anna Moss, and Joseph (Seffi) Naor.1999.
The budgeted maximum coverage problem.Inf.
Process.
Lett., 70(1):39?45.Kulik, Ariel, Hadas Shachnai, and Tami Tamir.
2009.Maximizing submodular set functions subject tomultiple linear constraints.
In SODA ?09, pages545?554.Kupiec, Julian, Jan Pedersen, and Francine Chen.1995.
A trainable document summarizer.
In SIGIR?95, pages 68?73, New York, NY, USA.
ACM.Manning, Christopher D. and Hirich Schutze.
2002.Foundations of Statistical Natural Language Pro-cessing.
The MIT Press, Cambridge, Mas-sachusetts, London, England.Mei, Qiaozhu and ChengXiang Zhai.
2008.
Generat-ing impact-based summaries for scientific literature.In Proceedings of ACL ?08, pages 816?824.Mohammad, Saif, Bonnie Dorr, Melissa Egan, AhmedHassan, Pradeep Muthukrishan, Vahed Qazvinian,Dragomir Radev, and David Zajic.
2009.
Usingcitations to generate surveys of scientific paradigms.In NAACL 2009, pages 584?592, June.Nanba, Hidetsugu and Manabu Okumura.
1999.
To-wards multi-paper summarization using referenceinformation.
In IJCAI1999, pages 926?931.Nanba, Hidetsugu, Noriko Kando, and Manabu Oku-mura.
2004.
Classification of research papers us-ing citation links and citation types: Towards au-tomatic review article generation.
In Proceedingsof the 11th SIG Classification Research Workshop,pages 117?134, Chicago, USA.Nenkova, Ani and Rebecca Passonneau.
2004.
Evalu-ating content selection in summarization: The pyra-mid method.
Proceedings of the HLT-NAACL con-ference.Qazvinian, Vahed and Dragomir R. Radev.
2008.
Sci-entific paper summarization using citation summarynetworks.
In COLING 2008, Manchester, UK.Radev, Dragomir, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda C?elebi, StankoDimitrov, Elliott Drabek, Ali Hakim, Wai Lam,Danyu Liu, Jahna Otterbacher, Hong Qi, HoracioSaggion, Simone Teufel, Michael Topper, AdamWinkel, and Zhu Zhang.
2004.
MEAD - a platformfor multidocument multilingual text summarization.In LREC 2004, Lisbon, Portugal, May.Radev, Dragomir R., Pradeep Muthukrishnan, and Va-hed Qazvinian.
2009.
The ACL anthology networkcorpus.
In ACL workshop on Natural LanguageProcessing and Information Retrieval for Digital Li-braries.Siddharthan, Advaith and Simone Teufel.
2007.Whose idea was this, and why does it matter?
at-tributing scientific work to citations.
In Proceedingsof NAACL/HLT-07.Teufel, Simone.
2005.
Argumentative Zoning for Im-proved Citation Indexing.
Computing Attitude andAffect in Text: Theory and Applications, pages 159?170.Tomokiyo, Takashi and Matthew Hurst.
2003.
A lan-guage model approach to keyphrase extraction.
InProceedings of the ACL 2003 workshop on Multi-word expressions, pages 33?40.903
