Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1308?1316,Beijing, August 2010Maximum Metric Score Training for Coreference ResolutionShanheng Zhao and Hwee Tou NgDepartment of Computer ScienceNational University of Singapore{zhaosh,nght}@comp.nus.edu.sgAbstractA large body of prior research on coref-erence resolution recasts the problem asa two-class classification problem.
How-ever, standard supervised machine learn-ing algorithms that minimize classifica-tion errors on the training instances do notalways lead to maximizing the F-measureof the chosen evaluation metric for coref-erence resolution.
In this paper, we pro-pose a novel approach comprising the useof instance weighting and beam search tomaximize the evaluation metric score onthe training corpus during training.
Ex-perimental results show that this approachachieves significant improvement over thestate-of-the-art.
We report results on stan-dard benchmark corpora (two MUC cor-pora and three ACE corpora), when evalu-ated using the link-based MUC metric andthe mention-based B-CUBED metric.1 IntroductionCoreference resolution refers to the process ofdetermining whether two or more noun phrases(NPs) in a text refer to the same entity.
Suc-cessful coreference resolution benefits many nat-ural language processing tasks.
In the literature,most prior work on coreference resolution recaststhe problem as a two-class classification problem.Machine learning-based classifiers are applied todetermine whether a candidate anaphor and a po-tential antecedent are coreferential (Soon et al,2001; Ng and Cardie, 2002b).A large body of prior research on corefer-ence resolution follows the same process: dur-ing training, they apply standard supervised ma-chine learning algorithms to minimize the numberof misclassified training instances; during testing,they maximize either the local or the global proba-bility of the coreferential relation assignments ac-cording to the specific chosen resolution method.However, minimizing the number of misclas-sified training instances during training does notguarantee maximizing the F-measure of the cho-sen evaluation metric for coreference resolution.First of all, coreference is a rare relation.
Thereare far fewer positive training instances than neg-ative ones.
Simply minimizing the number of mis-classified training instances is suboptimal and fa-vors negative training instances.
Secondly, evalu-ation metrics for coreference resolution are basedon global assignments.
Not all errors have thesame impact on the metric score.
Furthermore, theextracted training instances are not equally easy tobe classified.In this paper, we propose a novel approachcomprising the use of instance weighting andbeam search to address the above issues.
Our pro-posed maximum metric score training (MMST)approach performs maximization of the chosenevaluation metric score on the training corpus dur-ing training.
It iteratively assigns higher weightsto the hard-to-classify training instances.
The out-put of training is a standard classifier.
Hence,during testing, MMST is faster than approacheswhich optimize the assignment of coreferential re-lations during testing.
Experimental results showthat MMST achieves significant improvementsover the baselines.
Unlike most of the previouswork, we report improved results over the state-of-the-art on all five standard benchmark corpora1308(two MUC corpora and three ACE corpora), withboth the link-based MUC metric and the mention-based B-CUBED metric.The rest of this paper is organized as follows.We first review the related work and the evaluationmetrics for coreference resolution in Section 2 and3, respectively.
Section 4 describes the proposedMMST algorithm.
Experimental results and re-lated discussions are given in Section 5.
Finally,we conclude in Section 6.2 Related WorkSoon et al (2001) proposed a training and test-ing framework for coreference resolution.
Dur-ing training, a positive training instance is formedby a pair of markables, i.e., the anaphor (a nounphrase) and its closest antecedent (another nounphrase).
Each markable (noun phrase) betweenthe two, together with the anaphor, form a neg-ative training instance.
A classifier is trained onall training instances, using a standard supervisedlearning algorithm.
During testing, all precedingmarkables of a candidate anaphor are consideredas potential antecedents, and are tested in a back-to-front manner.
The process stops if either an an-tecedent is found or the beginning of the text isreached.
This framework has been widely used inthe community of coreference resolution.Recent work boosted the performance of coref-erence resolution by exploiting fine-tuned featuresets under the above framework, or adopting al-ternative resolution methods during testing (Ngand Cardie, 2002b; Yang et al, 2003; Denis andBaldridge, 2007; Versley et al, 2008).Ng (2005) proposed a ranking model to maxi-mize F-measure during testing.
In the approach, ndifferent coreference outputs for each test text aregenerated, by varying four components in a coref-erence resolution system, i.e., the learning algo-rithm, the instance creation method, the featureset, and the clustering algorithm.
An SVM-basedranker then picks the output that is likely to havethe highest F-measure.
However, this approachis time-consuming during testing, as F-measuremaximization is performed during testing.
Thislimits its usage on a very large corpus.In the community of machine learning, re-searchers have proposed approaches for learninga model to optimize a chosen evaluation met-ric other than classification accuracy on all train-ing instances.
Joachims (2005) suggested the useof support vector machines to optimize nonlinearevaluation metrics.
However, the approach doesnot differentiate between the errors in the samecategory in the contingency table.
Furthermore, itdoes not take into account inter-instance relation(e.g., transitivity), which the evaluation metric forcoreference resolution cares about.Daume III (2006) proposed a structured learn-ing framework for coreference resolution to ap-proximately optimize the ACE metric.
Our pro-posed approach differs in two aspects.
First, wedirectly optimize the evaluation metric itself, andnot by approximation.
Second, unlike the incre-mental local loss in Daume III (2006), we evaluatethe metric score globally.In contrast to Ng (2005), Ng and Cardie(2002a) proposed a rule-induction system withrule pruning.
However, their approach is specificto rule induction, and is not applicable to othersupervised learning classifiers.
Ng (2004) varieddifferent components of coreference resolution,choosing the combination of components that re-sults in a classifier with the highest F-measure ona held-out development set during training.
Incontrast, our proposed approach employs instanceweighting and beam search to maximize the F-measure of the evaluation metric during training.Our approach is general and applicable to any su-pervised learning classifiers.Recently, Wick and McCallum (2009) pro-posed a partition-wise model for coreference reso-lution to maximize a chosen evaluation metric us-ing the Metropolis-Hastings algorithm (Metropo-lis et al, 1953; Hastings, 1970).
However, theyfound that training on classification accuracy, inmost cases, outperformed training on the corefer-ence evaluation metrics.
Furthermore, similar toNg (2005), their approach requires the generationof multiple coreference assignments during test-ing.Vemulapalli et al (2009) proposed a document-level boosting technique for coreference resolu-tion by re-weighting the documents that havethe lowest F-measures.
By combining multipleclassifiers generated in multiple iterations, they1309achieved a CEAF score slightly better than thebaseline.
Different from them, our approachworks at the instance level, and we output a sin-gle classifier.3 Coreference Evaluation MetricsIn this section, we review two commonly usedevaluation metrics for coreference resolution.First, we introduce the terminology.
The goldstandard annotation and the output by a coref-erence resolution system are called key and re-sponse, respectively.
In both the key and the re-sponse, a coreference chain is formed by a set ofcoreferential mentions.
A mention (or markable)is a noun phrase which satisfies the markable def-inition in an individual corpus.
A link refers to apair of coreferential mentions.
If a mention has nolinks to other mentions, it is called a singleton.3.1 The MUC Evaluation MetricVilain et al (1995) introduced the link-basedMUC evaluation metric for the MUC-6 and MUC-7 coreference tasks.
Let Si be an equivalenceclass generated by the key (i.e., Si is a corefer-ence chain), and p(Si) be a partition of Si relativeto the response.
Recall is the number of correctlyidentified links over the number of links in the key.Recall =?
(|Si| ?
|p(Si)|)?
(|Si| ?
1)Precision, on the other hand, is defined in the op-posite way by switching the role of key and re-sponse.
F-measure is a trade-off between recalland precision.F = 2 ?Recall ?
PrecisionRecall + Precision3.2 The B-CUBED Evaluation MetricBagga and Baldwin (1998) introduced themention-based B-CUBED metric.
The B-CUBED metric measures the accuracy of coref-erence resolution based on individual mentions.Hence, it also gives credit to the identification ofsingletons, which the MUC metric does not.
Re-call is computed asRecall = 1N?d?D?m?d|Om||Sm|where D, d, and m are the set of documents, adocument, and a mention, respectively.
Sm is theequivalence class generated by the key that con-tains m, while Om is the overlap of Sm and theequivalence class generated by the response thatcontains m. N is the total number of mentions inD.
The precision, again, is computed by switch-ing the role of key and response.
F-measure iscomputed in the same way as the MUC metric.4 Maximum Metric Score TrainingBefore explaining the algorithm, we describe ourcoreference clustering method used during test-ing.
It is the same as most prior work in the lit-erature, including Soon et al (2001) and Ng andCardie (2002b).
The individual classification de-cisions made by the coreference classifier do notguarantee that transitivity of coreferential NPs isobeyed.
So it can happen that the pair A and B,and the pair B and C are both classified as coref-erential, but the pair A and C is not classifiedas coreferential by the classifier.
After all coref-erential markable pairs are found (no matter byclosest-first, best-first, or resolving-all strategiesas in different prior work), all coreferential pairsare clustered together to form the coreference out-put.
By doing so, transitivity is kept: a markable isin a coreference chain if and only if it is classifiedto be coreferential to at least one other markablein the chain.4.1 Instance WeightingSuppose there are mk and mr coreferential linksin the key and the response, respectively, and acoreference resolution system successfully pre-dicts n correct links.
The recall and the preci-sion are then nmk andnmr , respectively.
The learntclassifier predicts false positive and false negativeinstances during testing.
For a false positive in-stance, if we could successfully predict it as neg-ative, the recall is unchanged, but the precisionwill be nmr?1 , which is higher than the originalprecision nmr .
For a false negative instance, itis more subtle.
If the two markables in the in-stance are determined to be in the same corefer-ence chain by the clustering algorithm, it does notmatter whether we predict this instance as posi-tive or negative, i.e., this false negative does not1310change the F-measure of the evaluation metric atall.
If the two markables are not in the same coref-erence chain under the clustering, in case that wecan predict it as positive, the recall will be n+1mk ,which is higher than the original recall nmk , andthe precision will be n+1mr+1 , which is higher thanthe original precision nmr , as n < mr.
In bothcases, the F-measure improves.
If we can instructthe learning algorithm to pay more attention tothese false positive and false negative instancesand to predict them correctly by assigning themmore weight, we should be able to improve theF-measure.In the literature, besides the training instanceextraction methods proposed by Soon et al(2001) and Ng and Cardie (2002b) as discussedin Section 2, McCarthy and Lehnert (1995) usedall possible pairs of training instances.
We alsouse all pairs of training instances in our approachto keep as much information as possible.
Initiallyall the pairs are equally weighted.
We then itera-tively assign more weights to the hard-to-classifypairs.
The iterative process is conducted by abeam search algorithm.4.2 Beam SearchOur proposed MMST algorithm searches for a setof weights to assign to training instances suchthat the classifier trained on the weighted traininginstances gives the maximum coreference metricscore when evaluated on the training instances.Beam search is used to limit the search.
Eachsearch state corresponds to a set of weighted train-ing instances, a classifier trained on the weightedtraining instances minimizing misclassifications,and the F-measure of the classifier when evalu-ated on the weighted training instances using thechosen coreference evaluation metric.
The rootof the search tree is the initial search state whereall the training instances have identical weights ofone.
Each search state s can expand into two dif-ferent children search states sl and sr. sl (sr) cor-responds to assigning higher weights to the falsepositive (negative) training instances in s. Thesearch space thus forms a binary search tree.Figure 1 shows an example of a binary searchtree.
Initially, the tree has only one node: the root(node 1 in the figure).
In each iteration, the algo-12 34 5 6 78 9 10 11Figure 1: An example of a binary search treerithm expands all the leaf nodes in the beam.
Forexample, in the first iteration, node 1 is expandedto generate node 2 and 3, which corresponds toadding weights to false positive and false nega-tive training instances, respectively.
An expandednode always has two children in the binary searchtree.
All the nodes are then sorted in descendingorder of F-measure.
Only the top M nodes arekept, and the remaining nodes are discarded.
Thediscarded nodes can either be leaf nodes or non-leaf nodes.
For example, if node 5 is discardedbecause of low F-measure, it will not be expandedto generate children in the binary search tree.
Theiterative algorithm stops when all the nodes in thebeam are non-leaf nodes, i.e., all the nodes in thebeam have been expanded.Figure 2 gives the formal description of theproposed maximum metric score training algo-rithm.
In the algorithm, assume that we haveN texts T1, T2, .
.
., TN in the training data set.mki and mkj are the ith and jth markable inthe text Tk, respectively.
Hence, for all i <j, (mki,mkj , wkij) is a training instance for themarkable pair (mki,mkj), in which wkij is theweight of the instance.
Let Lkij and L?kij be thetrue and predicted label of the pair (mki,mkj),respectively.
Let W , C, F , and E be the set ofweights {wkij |1 ?
k ?
N, i < j}, the classifier,the F-measure, and a boolean indicator of whetherthe search state has been expanded, respectively.Finally, M is the beam size, and ?
controls howmuch we update the weights in each iteration.Since we train the model on all possible pairs,during testing we also test if a potential anaphor iscoreferential to each preceding antecedent.1311INPUT: T1, T2, .
.
.
, TNOUTPUT: classifier Cwkij ?
1, for all 1 ?
k ?
N and i < jC ?
train({(mki,mkj , wkij)|1 ?
k ?
N, i < j})F ?
resolve and evaluate T1, .
.
.
, TN with CE ?
falseBEAM?
{(W,C, F,E)}repeatBEAM?
?
{}for all (W,C, F,E) in BEAM doBEAM?
?
BEAM??
{(W,C, F, true)}if E=false thenpredict all L?kij with C (1 ?
k ?
N, i < j)cluster into coreference chains based on L?kijW ?
?
Wfor all 1 ?
k ?
N, i < j doif Lkij = false and L?kij = true thenw?kij ?
w?kij + ?end ifend forC?
?
train({(mki,mkj , w?kij)|1 ?
k ?
N, i < j})F ?
?
resolve and evaluate T1, .
.
.
, TN with C?BEAM?
?
BEAM??
{(W ?, C?, F ?, false)}W ??
?
Wfor all 1 ?
k ?
N, i < j doif Lkij = true and L?kij = false andChain(mki) 6= Chain(mkj) thenw?
?kij ?
w?
?kij + ?end ifend forC??
?
train({(mki,mkj , w?
?kij)|1 ?
k ?
N, i < j})F ??
?
resolve and evaluate T1, .
.
.
, TN with C??BEAM?
?
BEAM??
{(W ?
?, C?
?, F ?
?, false)}end ifend forBEAM?
BEAM?sort BEAM in descending order of F , keep top M elementsuntil for all E of all elements in BEAM, E = truereturn C, from the top element (W,C, F,E) of BEAMFigure 2: The maximum metric score training(MMST) algorithm5 Experiments5.1 Experimental SetupIn the experiments, we used all the five commonlyused evaluation corpora for coreference resolu-tion, namely the two MUC corpora (MUC6 andMUC7) and the three ACE corpora (BNEWS,NPAPER, and NWIRE).
The MUC6 and theMUC7 corpora were defined in the DARPA Mes-sage Understanding Conference (MUC-6, 1995;MUC-7, 1998).
The dry-run texts were used as thetraining data sets.
In both corpora, each trainingdata set contains 30 texts.
The test data sets forMUC6 and MUC7 consist of the 30 and 20 for-mal evaluation texts, respectively.
The ACE cor-pora were defined in NIST Automatic Content Ex-traction phase 2 (ACE-2) (NIST, 2002).
The threedata sets are from different news sources: broad-cast news (BNEWS), newspaper (NPAPER), andnewswire (NWIRE).
Each of the three data setscontains two portions: training and developmenttest.
They were used as our training set and testset, respectively.
The BNEWS, NPAPER, andNWIRE data sets contain 216, 76, and 130 train-ing texts, and 51, 17, and 29 test texts, respec-tively.Unlike some previous work on coreference res-olution that assumes that the gold standard mark-ables are known, we work directly on raw text in-put.
Versley et al (2008) presented the BARTpackage1, an open source coreference resolutiontoolkit, that accepts raw text input and reportedstate-of-the-art MUC F-measures on the threeACE corpora.
BART uses an extended feature setand tree kernel support vector machines (SVM)under the Soon et al (2001) training and testingframework.
We used the BART package in our ex-periments, and implemented the proposed MMSTalgorithm on top of it.
In our experiments reportedin this paper, the features we used are identical tothe features output by the preprocessing code ofBART reported in Versley et al (2008), exceptthat we did not use their tree-valued and string-valued features (see the next subsection for de-tails).Since we use automatically extracted mark-ables, it is possible that some extracted markablesand the gold standard markables are unmatched,or twinless as defined in Stoyanov et al (2009).How to use the B-CUBED metric for evaluatingtwinless markables has been explored recently.
Inthis paper, we adopt the B3all variation proposedby Stoyanov et al (2009), which retains all twin-less markables.
We also experimented with theirB30 variation, which gave similar results.
Notethat no matter which variant of the B-CUBEDmetric is used, it is a fair comparison as long asthe baseline and our proposed MMST algorithmare compared against each other using the sameB-CUBED variant.5.2 The Baseline SystemsWe include state-of-the-art coreference resolutionsystems in the literature for comparison.
Sincewe use the BART package in our experiments,1http://www.sfs.uni-tuebingen.de/?versley/BART/1312we include the results of the original BART sys-tem (with its extended feature set and SVM-light-TK (Moschitti, 2006), as reported in Versley et al(2008)) as the first system for comparison.
Vers-ley et al (2008) reported only the results on thethree ACE data sets with the MUC evaluation met-ric.
Since we used all the five data sets in ourexperiments, for fair comparison, we also includethe MUC results reported in Ng (2004).
To thebest of our knowledge, Ng (2004) was the onlyprior work which reported MUC metric scores onall the five data sets.
The MUC metric scores ofVersley et al (2008) and Ng (2004) are listed inthe row ?Versley et al 08?
and ?Ng 04?, respec-tively, in Table 1.
For the B-CUBED metric, weinclude Ng (2005) for comparison, although it isunclear how Ng (2005) interpreted the B-CUBEDmetric.
The scores are listed in the row ?Ng 05?in Table 2.Tree kernel SVM learning is time-consuming.To reduce the training time needed, instead of us-ing SVM-light-TK, we used a much faster learn-ing algorithm, J48, which is the WEKA imple-mentation of the C4.5 decision tree learning algo-rithm.
(Quinlan, 1993; Witten and Frank, 2005).As tree-valued features and string-valued featurescannot be used with J48, in our experiments weexcluded them from the extended feature set thatBART used to produce state-of-the-art MUC F-measures on the three ACE corpora.
All our re-sults in this paper were obtained using this re-duced feature set and J48 decision tree learn-ing.
However, given sufficient computational re-sources, our proposed approach is able to apply toany supervised machine learning algorithms.Our baselines that follow the Soon et al (2001)framework, using the reduced feature set and J48decision tree learning, are shown in the row ?SNL-Style Baseline?
in Table 1 and 2.
The resultssuggest that our baseline system is comparableto the state of the art.
Although in Table 1, theperformance of the SNL-style baseline is slightlylower than Versley et al (2008) on the three ACEcorpora, the computational time needed has beengreatly reduced.Our MMST algorithm trains and tests on allpairs of markables.
To show the effectiveness ofweight updating of MMST, we built another base-line which trains and tests on all pairs.
The per-formance of this system is shown in the row ?All-Style Baseline?
in Table 1 and 2.5.3 Results Using Maximum Metric ScoreTrainingNext, we show the results of using the proposedmaximum metric score training algorithm.
Fromthe description of the algorithm, it can be seen thatthere are two parameters in the algorithm.
Oneparameter is M , the size of the beam.
The otherparameter is ?, which controls how much we in-crease the weight of a training instance in eachiteration.Since the best M and ?
for the MUC evaluationmetric were not known, we used held-out develop-ment sets to tune the parameters.
Specifically, wetrained classifiers with different combinations ofM and ?
on a development training set, and eval-uated their performances on a development testset.
In our experiments, the development trainingset contained 2/3 of the texts in the training setof each individual corpus, while the developmenttest set contained the remaining 1/3 of the texts.After having picked the best M and ?
values, wetrained a classifier on the entire training set withthe chosen parameters.
The learnt classifier wasthen applied to the test set.2 4 6 8 10 12 14 16 18 20525456586062646668MF?measureMMSTSNL?Style BaselineAll?Style BaselineFigure 3: Tuning M on the held-out developmentsetTo limit the search space, we tuned the twoparameters sequentially.
First, we fixed ?
=1, which is equivalent to duplicating each train-ing instance once in J48, and evaluated M =2, 4, 6, .
.
.
, 20.
After having chosen the bestM that corresponded to the maximum F-measure,we fixed the value of M , and evaluated ?
=0.1, 0.2, 0.3, .
.
.
, 2.0.
Take MUC6 as an exam-1313MUC6 MUC7 BNEWS NPAPER NWIREModel R P F R P F R P F R P F R P FVersley et al 08 ?
?
60.7 65.4 63.0 64.1 67.7 65.8 60.4 65.2 62.7Ng 04 75.8 61.4 67.9 64.2 60.2 62.1 63.1 67.8 65.4 73.5 63.3 68.0 53.1 60.6 56.6SNL-Style Baseline 67.0 49.2 56.7 63.0 54.2 58.3 57.4 64.3 60.7 61.6 67.3 64.3 58.6 66.1 62.1All-Style Baseline 56.9 69.2 62.5 51.5 73.4 60.6 53.0 76.7 62.7 56.3 75.4 64.4 53.0 74.5 61.9MMST 73.3 59.9 65.9????
66.8 59.8 63.1???
70.5 61.9 65.9???
69.9 64.0 66.8?
64.7 64.7 64.7??
?M = 6, ?
= 1.0 M = 6, ?
= 0.7 M = 6, ?
= 1.8 M = 6, ?
= 0.9 M = 14, ?
= 0.7Table 1: Results for the two MUC and three ACE corpora with MUC evaluation metricMUC6 MUC7 BNEWS NPAPER NWIREModel R P F R P F R P F R P F R P FNg 05 ?
?
57.0 77.1 65.6 62.8 71.2 66.7 59.3 75.4 66.4SNL-Style Baseline 57.8 74.4 65.1 57.6 76.5 65.7 62.0 74.7 67.8 61.8 70.4 65.8 65.8 75.9 70.5All-Style Baseline 51.6 86.3 64.6 49.1 90.1 63.6 61.6 83.7 71.0 63.9 74.0 68.6 64.8 80.1 71.7MMST 62.7 81.5 70.9????
61.8 73.6 67.2??
61.6 83.7 71.0??
63.1 76.2 69.1??
64.3 81.0 71.7M = 6, ?
= 1.0 M = 8, ?
= 0.8 M = 6, ?
= 0.9 M = 14, ?
= 0.5 M = 6, ?
= 0.1Table 2: Results for the two MUC and three ACE corpora with B3 evaluation metric0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2525456586062646668deltaF?measureMMSTSNL?Style BaselineAll?Style BaselineFigure 4: Tuning ?
on the held-out developmentsetple.
The results of tuning M on MUC6 are shownin Figure 3.
The maximum F-measure is obtainedwhen M = 4 and M = 6.
On all the different Mvalues we have tried, MMST outperforms both theSNL-style baseline and the All-style baseline onthe development test set.
We then fixed M = 6,and evaluated different ?
values.
The results areshown in Figure 4.
The best F-measure was ob-tained when ?
= 1.0.
Again, on all the different?
values we have tried, MMST outperforms bothbaselines on the development test set.The rows ?MMST?
in Table 1 and 2 show theperformance of MMST on the test sets, with thetuned parameters indicated.
In our experiments,the statistical significance test was conducted asin Chinchor (1995).
?
and ??
stand for p < 0.05and p < 0.01 over the SNL-style baseline, respec-tively.
?
and ??
stand for p < 0.05 and p < 0.01over the All-style baseline, respectively.For the MUC metric, when compared to theAll-style baseline, MMST gains 3.4, 2.5, 3.2, 2.4,and 2.8 improvement in F-measure on MUC6,MUC7, BNEWS, NPAPER, and NWIRE, respec-tively.
The experimental results clearly show thatMMST gains not only consistent, but also sta-tistically significant improvement over both theSNL-style baseline and the All-style baseline in allcombinations (five data sets and two baselines) onthe MUC metric, except that it is not significant(p = 0.06) over the SNL-style baseline in NPA-PER.
As for the B-CUBED metric, MMST gainssignificant improvement in F-measure on MUC6and MUC7 data sets, while its performance onthe three ACE data sets are comparable to the All-style baseline.5.4 DiscussionTo see how MMST actually updates the weight,we use the MUC metric as an example.
Under theexperimental settings, it takes 6 ?
9 iterations forMMST to stop on the five data sets.
The numberof explored states in the binary search tree, includ-ing the root, is 33, 39, 25, 29, and 75 on MUC6,MUC7, BNEWS, NPAPER, and NWIRE, respec-tively.
It is instructive to find out the final weightof each instance.
Take MUC6 as an example, thenumber of positive instances with weight 1, 2, 3,and 4 are 5,204, 1,568, 1,379, and 1,844, respec-tively, while the number of negative instances withweight 1 and 2 are 503,141 and 1,755, respec-1314tively.
Counting the weighted number of instances(e.g., an instance with weight 2 is equivalent to 2instances), we have 19,853 positive and 506,651negative training instances.
This changes the ratioof the positive instances from 1.9% to 3.8%.
As aby-product, MMST reduces data skewness, whileusing all possible NP pairs for training to keep asmuch information as possible.The change of weights of the training instancesis equivalent to the change of distribution of thetraining instances.
This effectively changes theclassification hypothesis to the one that tends toyield higher evaluation metric score.
Take the fol-lowing sentence in the MUC6 data set as an ex-ample:In a news release, the company said the newname more accurately reflects its focus on high-technology communications, including businessand entertainment software, interactive mediaand wireless data and voice transmission.In the above example, the pronoun its is coref-erential to the antecedent NP the company.
Thebaseline classifier gives a probability of 0.02 thatthe two NPs are coreferential.
The pair is clas-sified wrongly and none of the other pairs in thearticle can link the two NPs together through clus-tering.
However, with MMST, this probability in-creases to 0.54, which leads to the correct classi-fication.
This is because the baseline classifier isnot good at predicting in the case when the sec-ond markable is a pronoun.
In the above exam-ple, its can have another candidate antecedent thenew name.
There are far more negative traininginstances than positive ones for this case.
In fact,in the induced decision tree by the baseline, theleaf node corresponding to the pair the company?
its has 7,782 training instances, out of whichonly 175 are positive.
With MMST, however,these numbers decrease to 83 and 45, respectively.MMST also promotes the Anaphor Is Pronounfeature to a higher level in the decision tree.
Al-though we use decision tree to illustrate the work-ing of the algorithm, MMST is not limited to treelearning, and can make use of any learning algo-rithms that are able to take advantage of instanceweighting.It can also be seen that with the B-CUBEDmetric, MMST gains improvement on MUC6 andMUC7, but not on the three ACE corpora.
How-ever, the results of MMST on the three ACE cor-pora with the B-CUBED evaluation metric are atleast comparable with the All-style baseline.
Thisis because we always pick the classifier which cor-responds to the maximum evaluation metric scoreon the training set and the classifier correspond-ing to the All-style baseline is one of the candi-dates.
In addition, our MMST approach improvesupon state-of-the-art results (Ng, 2004; Ng, 2005;Versley et al, 2008) on most of the five standardbenchmark corpora (two MUC corpora and threeACE corpora), with both the link-based MUCmetric and the mention-based B-CUBED metric.Finally, our approach performs all the F-measure maximization during training, and is veryfast during testing, since the output of the MMSTalgorithm is a standard classifier.
For example,on the MUC6 data set with the MUC evaluationmetric, it took 1.6 hours and 31 seconds for train-ing and testing, respectively, on an Intel Xeon2.33GHz machine.6 ConclusionIn this paper, we present a novel maximum met-ric score training approach comprising the use ofinstance weighting and beam search to maximizethe chosen coreference metric score on the train-ing corpus during training.
Experimental resultsshow that the approach achieves significant im-provement over the baseline systems.
The pro-posed approach improves upon state-of-the-art re-sults on most of the five standard benchmark cor-pora (two MUC corpora and three ACE corpora),with both the link-based MUC metric and themention-based B-CUBED metric.AcknowledgmentsWe thank Yannick Versley for providing usthe BART package and the preprocessed data.This research was done for CSIDM Project No.CSIDM-200804 partially funded by a grant fromthe National Research Foundation (NRF) ad-ministered by the Media Development Authority(MDA) of Singapore.1315ReferencesBagga, Amit and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In Proceedings ofthe LREC1998, pages 563?566.Chinchor, Nancy.
1995.
Statistical significance ofMUC-6 results.
In Proceedings of the MUC-6,pages 39?43.Daume III, Hal.
2006.
Practical Structured Learn-ing for Natural Language Processing.
Ph.D. thesis,University of Southern California.Denis, Pascal and Jason Baldridge.
2007.
Joint deter-mination of anaphoricity and coreference resolutionusing integer programming.
In Proceedings of theNAACL-HLT2007, pages 236?243.Hastings, W. K. 1970.
Monte Carlo sampling meth-ods using Markov chains and their applications.Biometrika, 57(1):97?109.Joachims, Thorsten.
2005.
A support vector methodfor multivariate performance measures.
In Proceed-ings of the ICML2005, pages 377?384.McCarthy, Joseph F. and Wendy G. Lehnert.
1995.Using decision trees for coreference resolution.
InProceedings of the IJCAI1995, pages 1050?1055.Metropolis, Nicholas, Arianna W. Rosenbluth, Mar-shall N. Rosenbluth, Augusta H. Teller, and EdwardTeller.
1953.
Equation of state calculations by fastcomputing machines.
Journal of Chemical Physics,21(6):1087?1092.Moschitti, Alessandro.
2006.
Making tree kernelspractical for natural language learning.
In Proceed-ings of the EACL2006, pages 113?120.MUC-6.
1995.
Coreference task definition (v2.3, 8Sep 95).
In Proceedings of the MUC-6, pages 335?344.MUC-7.
1998.
Coreference task definition (v3.0, 13Jul 97).
In Proceedings of the MUC-7.Ng, Vincent and Claire Cardie.
2002a.
Combiningsample selection and error-driven pruning for ma-chine learning of coreference rules.
In Proceedingsof the EMNLP2002, pages 55?62.Ng, Vincent and Claire Cardie.
2002b.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the ACL2002, pages 104?111.Ng, Vincent.
2004.
Improving Machine Learning Ap-proaches to Noun Phrase Coreference Resolution.Ph.D.
thesis, Cornell University.Ng, Vincent.
2005.
Machine learning for coreferenceresolution: From local classification to global rank-ing.
In Proceedings of the ACL2005, pages 157?164.NIST.
2002.
The ACE 2002 evaluation plan.ftp://jaguar.ncsl.nist.gov/ace/doc/ACE-EvalPlan-2002-v06.pdf.Quinlan, J. Ross.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.Soon, Wee Meng, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational Linguistics, 27(4):521?544.Stoyanov, Veselin, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrasecoreference resolution: Making sense of the state-of-the-art.
In Proceedings of the ACL-IJCNLP2009,pages 656?664.Vemulapalli, Smita, Xiaoqiang Luo, John F. Pitrelli,and Imed Zitouni.
2009.
Classifier combinationtechniques applied to coreference resolution.
InProceedings of the NAACL-HLT2009 Student Re-search Workshop and Doctoral Consortium, pages1?6.Versley, Yannick, Simone Paolo Ponzetto, MassimoPoesio, Vladimir Eidelman, Alan Jern, Jason Smith,Xiaofeng Yang, and Alessandro Moschitti.
2008.BART: A modular toolkit for coreference resolu-tion.
In Proceedings of the ACL2008:HLT DemoSession, pages 9?12.Vilain, Marc, John Burger, John Aberdeen, DennisConnolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of the MUC-6, pages 45?52.Wick, Michael and Andrew McCallum.
2009.
Ad-vances in learning and inference for partition-wise models of coreference resolution.
Techni-cal Report UM-CS-2009-028, University of Mas-sachusets, Amherst, USA.Witten, Ian H. and Eibe Frank.
2005.
Data Mining:Practical Machine Learning Tools and Techniques.The Morgan Kaufmann Series in Data ManagementSystems.
Morgan Kaufmann Publishers, second edi-tion.Yang, Xiaofeng, Guodong Zhou, Jian Su, andChew Lim Tan.
2003.
Coreference resolution us-ing competition learning approach.
In Proceedingsof the ACL2003, pages 176?183.1316
