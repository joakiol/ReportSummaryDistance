Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 97?106,Baltimore, Maryland USA, June 27, 2014.c?2014 Association for Computational LinguisticsMining Themes and Interests in the Asperger?s and Autism CommunityYangfeng Ji, Hwajung Hong, Rosa Arriaga, Agata Rozga, Gregory Abowd, Jacob EisensteinSchool of Interactive ComputingGeorgia Institute of Technology{jiyfeng,hwajung,arriaga,agata,abowd,jacobe}@gatech.eduAbstractDiscussion forums offer a new source ofinsight for the experiences and challengesfaced by individuals affected by mentaldisorders.
Language technology can helpdomain experts gather insight from theseforums, by aggregating themes and userbehaviors across thousands of conversa-tions.
We present a novel model for webforums, which captures both thematic con-tent as well as user-specific interests.
Ap-plying this model to the Aspies Central fo-rum (which covers issues related to As-perger?s syndrome and autism spectrumdisorder), we identify several topics ofconcern to individuals who report being onthe autism spectrum.
We perform the eval-uation on the data collected from AspiesCentral forum, including 1,939 threads,29,947 posts and 972 users.
Quantita-tive evaluations demonstrate that the top-ics extracted by this model are substan-tially more than those obtained by LatentDirichlet Allocation and the Author-TopicModel.
Qualitative analysis by subject-matter experts suggests intriguing direc-tions for future investigation.1 IntroductionOnline forums can offer new insights on men-tal disorders, by leveraging the experiences of af-fected individuals ?
in their own words.
Suchinsights can potentially help mental health profes-sionals and caregivers.
Below is an example dia-logue from the Aspies Central forum,1where indi-viduals who report being on the autism spectrum(and their families and friends) exchange adviceand discuss their experiences:1http://www.aspiescentral.com?
User A: Do you feel paranoid at work?.
.
.
What are some situations in which youthink you have been unfairly treated??
User B: Actually I am going through some-thing like that now, and it is very difficult tokeep it under control.
.
.?
User A: Yes, yes that is it.
Exactly .
.
.
I thinkit might be an Aspie trait to do that, I meanover think everything and take it too literally??
User B: It probably is an Aspie trait.
I?vebeen told too that I am too hard on myself.Aspies Central, like other related forums, hasthousands of such exchanges.
However, aggregat-ing insight from this wealth of information posesobvious challenges.
Manual analysis is extremelytime-consuming and labor-intensive, thus limitingthe scope of data that can be considered.
In addi-tion, manual coding systems raise validity ques-tions, because they can tacitly impose the pre-existing views of the experimenter on all sub-sequent analysis.
There is therefore a need forcomputational tools that support large-scale ex-ploratory textual analysis of such forums.In this paper, we present a tool for automati-cally mining web forums to explore textual themesand user interests.
Our system is based on LatentDirichlet Allocation (LDA; Blei et al, 2003), but iscustomized for this setting in two key ways:?
By modeling sparsely-varying topics, we caneasily recover key terms of interest, whileretaining robustness to large vocabulary andsmall counts (Eisenstein et al., 2011).?
By modeling author preference by topic, wecan quickly identify topics of interest for eachuser, and simultaneously recover topics thatbetter distinguish the perspectives of each au-thor.The key technical challenge in this work lies inbringing together several disparate modalities into97a single modeling framework: text, authorship,and thread structure.
We present a joint Bayesiangraphical model that unifies these facets, discov-ering both an underlying set of topical themes,and the relationship of these themes to authors.We derive a variational inference algorithm forthis model, and apply the resulting software on adataset gathered from Aspies Central.The topics and insights produced by our systemare evaluated both quantitatively and qualitatively.In a blind comparison with LDA and the author-topic model (Steyvers et al., 2004), both subject-matter experts and lay users find the topics gener-ated by our system to be substantially more coher-ent and relevant.
A subsequent qualitative analysisaligns these topics with existing theory about theautism spectrum, and suggests new potential in-sights and avenues for future investigation.2 Aspies Central ForumAspies Central (AC) is an online forum for indi-viduals on the autism spectrum, and has publiclyaccessible discussion boards.
Members of the sitedo not necessarily have to have an official diag-nosis of autism or a related condition.
Neurotyp-ical individuals (people not on the autism spec-trum) are also allowed to participate in the fo-rum.
The forum includes more than 19 discussionboards with subjects ranging from general discus-sions about the autism spectrum to private discus-sions about personal concerns.
As of March 2014,AC hosts 5,393 threads, 89,211 individual posts,and 3,278 members.AC consists of fifteen public discussion boardsand four private discussion boards that requiremembership.
We collected data only frompublicly-accessible discussion boards.
In addition,we excluded discussion boards that were website-specific (announcement-and-introduce-yourself),those mainly used by family and friends of in-dividuals on the spectrum (friends-and-family) orresearchers (autism-news-and-research), and onefor amusement (forum-games).
Thus, we focusedon ten discussion boards (aspergers-syndrome-Autism-and-HFA, PDD-NOS-social-anxiety-and-others, obsessions-and-interests, friendships-and-social-skills, education-and-employment, love-relationships-and-dating, autism-spectrum-help-and-support, off-topic-discussion, entertainment-discussion, computers-technology-discussion), inwhich AC users discuss their everyday expe-?
?dzdpnwdpnm?kad?biyik ?N PD KAKFigure 1: Plate diagram.
Shaded notes represent observedvariables, clear nodes represent latent variables, arrows in-dicate probabilistic dependencies, and plates indicate repeti-tion.riences, concerns, and challenges.
Using thepython library Beautiful Soup, we collected 1,939threads (29,947 individual posts) from the discus-sion board archives over a time period from June1, 2010 to July 27, 2013.
For a given post, weextracted associated metadata such as the authoridentifier and posting timestamps.3 Model SpecificationOur goal is to develop a model that captures thepreeminent themes and user behaviors from tracesof user behaviors in online forums.
The modelshould unite textual content with authorship andthread structure, by connecting these observedvariables through a set of latent variables rep-resenting conceptual topics and user preferences.In this section, we present the statistical specifi-cation of just such a model, using the machineryof Bayesian graphical models.
Specifically, themodel descibes a stochastic process by which theobserved variables are emitted from prior proba-bility distributions shaped by the latent variables.By performing Bayesian statistical inference inthis model, we can recover a probability distribu-tion around the latent variables of interest.We now describe the components of the modelthat generate each set of observed variables.
Themodel is shown as a plate diagram in Figure 1, andthe notation is summarized in Table 1.3.1 Generating the textThe part of the model which produces the text it-self is similar to standard latent Dirichlet alloca-tion (LDA) (Blei et al., 2003).
We assume a setof K latent topics, which are distributions overeach word in a finite vocabulary.
These topics are98Symbol DescriptionD number of threadsPdnumber of posts in thread dNpnumber of word tokens in post p?
parameter of topic distribution of threads?dthe multinomial distribution of topics specific to the thread dzdpnthe topic associated with the nth token in post p of thread dwdpnthe nth token in post p of thread dadauthorship distribution for question post and answer posts inthread d respectivelyyikthe topic-preference indicator of author i on topic kbithe Gaussian distribution of author i?s selection bias?ktopic k in log linear spacem background topic?
topic weights matrix?2?variance of feature weights?2bvariance of selection bias?
prior probability of authors?
preference on any topicTable 1: Mathematical notationsshared among all D threads in the collection, buteach thread has its own distribution over the top-ics.We make use of the SAGE parametrization forgenerative models of text (Eisenstein et al., 2011).SAGE uses adaptive sparsity to induce topics thatdeviate from a background word distribution inonly a few key words, without requiring a regular-ization parameter.
The background distribution iswritten m, and the deviation for topic k is written?k, so that Pr(w = v|?k,m) ?
exp (mv+ ?kv).Each word tokenwdpn(the nthword in post p ofthread d) is generated from the probability distri-bution associated with a single topic, indexed bythe latent variable zdpn?
{1 .
.
.K}.
This latentvariable is drawn from a prior ?d, which is theprobability distribution over topics associated withall posts in thread d.3.2 Generating the authorWe have metadata indicating the author of eachpost, and we assume that users are more likelyto participate in threads that relate to their topic-specific preference.
In addition, some people maybe more or less likely to participate overall.
Weextend the LDA generative model to incorporateeach of these intuitions.For each author i, we define a latent preferencevector yi, where yik?
{0, 1} indicates whetherthe author i prefers to answer questions abouttopic k. We place a Bernoulli prior on each yik, sothat yik?
Bern(?
), where Bern(y; ?)
= ?y(1 ??)(1?y).
Induction of y is one of the key infer-ence tasks for the model, since this captures topic-specific preference.It is also a fact that some individuals will partic-ipate in a conversation regardless of whether theyhave anything useful to add.
To model this gen-eral tendency, we add an ?bias?
variable bi?
R.When biis negative, this means that author i willbe reluctant to participate even when she does haverelevant interests.Finally, various topics may require different lev-els of preference; some may capture only generalknowledge that many individuals are able to pro-vide, while others may be more obscure.
We in-troduce a diagonal topic-weight matrix ?, where?kk= ?k?
0 is the importance of preference fortopic k. We can easily generalize the model by in-cluding non-zero off-diagonal elements, but leavethis for future work.The generative distribution for the observed au-thor variable is a log-linear function of y and b:Pr(adi= 1|?d,y,?, b) =exp(?Td?yi+ bi)?Aj=1exp(?Td?yj+ bj)(1)This distribution is multinomial over authors; eachauthor?s probability of responding to a thread de-pends on the topics in the thread (?d), the author?spreference on those topics (yi), the importance ofpreference for each topic (?
), and the bias parame-ter bi.
We exponentiate and then normalize, yield-ing a multinomial distribution.The authorship distribution in Equation (1)refers to a probability of user i authoring a singleresponse post in thread d (we will handle questionposts next).
Let us construct a binary vector a(r)d,where it is 1 if author i has authored any responseposts in thread d, and zero otherwise.
The proba-bility distribution for this vector can be writtenP (a(r)d|?d,y,?, b) ?A?i=1(exp(?Td?yi+ bi)?Aj=1exp(?Td?yj+ bj))a(r)di(2)One of the goals of this model is to distinguishfrequent responders (i.e., potential experts) fromindividuals who post questions in a given topic.Therefore, we make the probability of author i ini-tiating thread d depend on the value 1 ?
ykiforeach topic k. We write the binary vector a(q)d,where a(q)di= 1 if author i has written the ques-tion post, and zero otherwise.
Note that there canonly be one question post, so a(q)dis an indicatorvector.
Its probability is written asp(a(q)d|?d,y,?, b) ?A?i=1(exp(?Td?(1?
yi) + bi)?Aj=1exp(?Td?(1?
yj) + bj))a(q)di(3)99We can put these pieces together for a completedistribution over authorship for thread d:P (ad, |?d,y,?, b) ?A?i=1(exp(?Td?yi+ bi)?Aj=1exp(?Td?yj+ bj))a(r)di?A?i=1(exp(?Td?(1?
yi) + bi)?Aj=1exp(?Td?(1?
yj) + bj))a(q)di(4)where ad= {a(q)d,a(r)d}.
The probabilityp(ad|?d,y,?, b) combines the authorship distri-bution of authors from question post and answerposts in thread d. The identity of the original ques-tion poster does not appear in the answer vector,since further posts are taken to be refinements ofthe original question.This model is similar in spirit to super-vised latent Dirichlet allocation (sLDA) (Blei andMcAuliffe, 2007).
However, there are two key dif-ferences.
First, sLDA uses point estimation to ob-tain a weight for each topic.
In contrast, we per-form Bayesian inference on the author-topic pref-erence y.
Second, sLDA generates the metadatafrom the dot-product of the weights and?z, whilewe use ?
directly.
The sLDA paper argues thatthere is a risk of overfitting, where some of the top-ics serve only to explain the metadata and nevergenerate any of the text.
This problem does notarise in our experiments.3.3 Formal generative storyWe are now ready to formally define the generativeprocess of our model:1.
For each topic k(a) Set the word probabilities ?k=exp(m+?k)?iexp(mi+?ki)2.
For each author i(a) Draw the selection bias bi?
N (0, ?2b)(b) For each topic ki.
Draw the author-topic preferencelevel yik?
Bern(?)3.
For each thread d(a) Draw topic proportions ?d?
Dir(?
)(b) Draw the author vector adfrom Equa-tion (4)(c) For each post pi.
For each word in this postA.
Draw topic assignment zdpn?Mult(?d)B.
Draw wordwdpn?
Mult(?zdpn)4 Inference and estimationThe purpose of inference and estimation is to re-cover probability distributions and point estimatesfor the quantities of interest: the content of thetopics, the assignment of topics to threads, au-thor preferences for each topic, etc.
While recentprogress in probabilistic programming has im-proved capabilities for automating inference andestimation directly from the model specification,2here we develop a custom algorithm, based onvariational mean field (Wainwright and Jordan,2008).
Specifically, we approximate the distribu-tion over topic proportions, topic indicators, andauthor-topic preference P (?, z,y|w,a,x) with amean field approximationq(?,z,y|?, ?, ?)
=A?i=1K?k=1q(yik|?ik)D?d=1Pd?p=1Np,d?n=1q(zdpn|?dpn)D?d=1q(?d|?d)(5)where Pdis the number of posts in thread d, Kis the number of topics, and Npis the number ofword tokens in post Pd.
The variational parame-ters of q(?)
are ?, ?, ?.
We will write ???
to indicatean expectation under the distribution q(?, z,y).We employ point estimates for the variablesb (author selection bias), ?
(topic-time featureweights), ?
(topic-word log-probability devia-tions), and diagonal elements of ?
(topic weights).The estimation of ?
follows the procedure definedin SAGE (Eisenstein et al., 2011); we explain theestimation of the remaining parameters below.Given the variational distribution in Equation(5), the inference on our topic model can be for-mulated as constrained optimization of this bound.min L(?, ?, ?
; b,?,?)s.t.?dk?
0 ?d, k?dpn?
0,?k?dpnk= 1 ?d, p, n0 ?
?ik?
1 ?i, k?k?
0 ?k(6)The constraints are due to the parametric formof the variational approximation: q(?d|?d) isDirichlet, and requires non-negative parameters;2see http://probabilistic-programming.org/100q(zdpn|?dpn) is multinomial, and requires that?dpnlie on the K ?
1 simplex; q(yik|?ik) isBernoulli and requires that ?ikbe between 0 and1.
In addition, as a topic weight, ?kshould also benon-negative.Algorithm 1 One pass of the variational inferencealgorithm for our model.for d = 1, .
.
.
, D dowhile not converged dofor p = 1, .
.
.
, Pddofor n = 1, .
.
.
, Np,ddoUpdate ?dpnkusing Equation (7) for each k =1, .
.
.
,Kend forend forUpdate ?dkby optimizing Equation (6) with Equa-tion (10) for each k = 1, .
.
.
,Kend whileend forfor i = 1, .
.
.
, A doUpdate ?ikby optimizing Equation (6) with Equa-tion (13) for each k = 1, .
.
.
,KUpdate?biby optimizing Equation (6) with Equa-tion (14)end forfor k = 1, .
.
.
,K doUpdate ?kwith Equation (15)end for4.1 Word-topic indicatorsWith the variational distribution in Equation (5),the inference on ?dpnfor a given token n in post pof thread d is same as in LDA.
For the nth tokenin post p of thread d,?dpnk?
?kwdpnexp(?log ?dk?)
(7)where ?
is defined in the generative story and?log ?dk?
is the expectation of log ?dkunder thedistribution q(?dk|?d),?log ?dk?
= ?(?dk)??
(K?k=1?dk) (8)where ?(?)
is the Digamma function, the firstderivative of the log-gamma function.For the other variational parameters ?
and ?, wecan not obtain a closed form solution.
As the con-straints on these parameters are all convex with re-spect to each component, we employed a projectedquasi-Newton algorithm proposed in (Schmidt etal., 2009) to optimize L in Equation (6).
One passof the variational inference procedure is summa-rized in Algorithm 1.Since every step in this algo-rithm will not decrease the variational bound, theoverall algorithm is guaranteed to converge.4.2 Document-topic distributionThe inference for document-topic proportions isdifferent from LDA, due to the generation of theauthor vector ad, which depends on ?d.
For agiven thread d, the part of the bound associatedwith the variational parameter ?disL?d= ?log p(?d|?d)?+ ?log p(ad|?d,y,?, b)?+Pd?p=1Np,d?n=1?log p(zdpn|?d)?
?
?q(?d|?d)?
(9)and the derivative of L?dwith respect to ?dkisdL?dd?dk= ??(?dk)(?dk+Pd?p=1Np,d?n=1?dpnk?
?dk)???(K?k=1?dk)K?k=1(?dk+Pd?p=1Np,d?n=1?dpnk?
?dk)+dd?dk?log p(ad|?d,y,?, b)?
,(10)where ??(?)
is the trigramma function.
The firsttwo lines of Equation (10) are identical to LDA?svariational inference, which obtains a closed-formsolution by setting ?dk= ?dk+?p,n?dpnk.
Theadditional term for generating the authorship vec-tor adeliminates this closed-form solution andforces us to turn to gradient-based optimization.The expectation on the log probability of theauthorship involves the expectation on the logpartition function, which we approximate usingJensen?s inequality.
We then derive the gradient,??
?dk?log p(ad|?d,y,?, b)??
?k(A?i=1a(r)di?ik?A(r)dA?i=1?ik?a(r)di|?d,y?)?
?k(A?i=1a(q)di?ik?A?i=1?ik?a(q)di|?d,y?
)(11)The convenience variable A(r)dcounts the numberof distinct response authors in thread d; recall thatthere can be only one question author.
The nota-tion?a(r)di|?d,y?=exp(??T??
?yi?+ bi)?jexp(??T??
?yj?+ bj),represents the generative probability of a(r)di= 1under the current variational distributions q(?d)and q(yi).
The notation?a(q)di|?d,y?is analo-gous, but represents the question post indicatora(q)di.1014.3 Author-topic preferenceThe variational distribution over author-topicpreference is q(yik|?ik); as this distribution isBernoulli, ?yik?
= ?ik, the parameter itself prox-ies for the topic-specific author preference ?
howmuch author i prefers to answer posts on topic k.The part of the variational bound the relates tothe author preferences isL?=D?d=1?log p(ad|?d,y,?, b)?+A?i=1K?k=1?p(yik|?)?
?A?i=1K?k=1?q(yik|?ik)?
(12)For author i on topic k, the derivative of?log p(ad|?d,y,?, b)?
for document d with re-spect to ?ikisdd?ik?logP (ad|?d,y,?, b)??
??dk??k(a(r)di??a(r)di|?d,y??
a(q)di+?a(q)di|?d,y?
),(13)where ??dk?
=?dk?k??dk?.
Thus, participating as arespondent increases ?ikto the extent that topic kis involved in the thread; participating as the ques-tioner decreases ?ikby a corresponding amount.4.4 Point estimatesWe make point estimates of the following param-eters: author selection bias biand topic-specificpreference weights ?k.
All updates are basedon maximum a posteriori estimation or maximumlikelihood estimation.Selection bias For the selection bias biof au-thor i given a thread d, the objective function inEquation (6) with the prior of bi?
N (0, ?2b) isminimized by a quasi-Newton algorithm with thefollowing derivative?
?bi?logP (ad|?d,y,?, b)?
?
a(r)d,i?
?a(r)di|?d,y?+ a(q)d,i??a(q)di|?d,y?
(14)The zero-mean Gaussian prior shrinks bitowardszero by subtracting bi/?2bfrom this gradient.
Notethat the gradient in Equation (14) is non-negativewhenever author i participates in thread d. Thismeans any post from this author, whether questionposts or answer posts, will have a positive contri-bution of the author?s selection bias.
This meansthat any activity in the forum will elevate the se-lection bias bi, but will not necessarily increase theimputed preference level.Topic weights The topic-specific preferenceweight ?kis updated by considering the derivativeof variational bound with respect to ?k?L??k=D?d=1??
?k?p(ad|?d,y,?, b)?
(15)where for a given document d,??
?k?log p(ad|?d,y,?, b)?
?
??dk??k?A?i=1?ik(a(r)i?
a(q)i+?a(q)di|?d,y??A(r)d?a(r)di|?d,y?
)Thus, ?kwill converge at a value where the ob-served posting counts matches the expectationsunder ?log p(ad|?d,y,?, b)?.5 Quantitative EvaluationTo validate the topics identified by the model,we performed a manual evaluation, combining theopinions of both novices as well as subject matterexperts in Autism and Asberger?s Syndrome.
Thepurpose of the evaluation is to determine whetherthe topics induced by the proposed model are morecoherent than topics from generic alternatives suchas LDA and the author-topic model, which are notspecifically designed for forums.5.1 Experiment SetupPreprocessing Preprocessing was minimal.
Wetokenized texts using white space and removedpunctuations at the beginning/end of each token.We removed words that appear less than fivetimes, resulting in a vocabulary of the 4903 mostfrequently-used words.Baseline Models We considered two baselinemodels in the evaulation.
The first baseline modelis latent Dirichlet allocation (LDA), which consid-ers only the text and ignores the metadata (Bleiet al., 2003).
The second baseline is the Author-Topic (AT) model, which extends LDA by associ-ating authors with topics (Rosen-Zvi et al., 2004;Steyvers et al., 2004).
Both baselines are im-plemented in the Matlab Topic Modeling Tool-box (Steyvers and Griffiths, 2005).Parameter Settings For all three models, we setK = 50.
Our model includes the three tunableparameters ?, the Bernoulli prior on topic-specificexpertise; ?2b, the variance prior on use selection102bias; and ?, the prior on document-topic distri-bution.
In the following experiments, we chose?
= 0.2, ?2b= 1.0, ?
= 1.0.
LDA and AT sharetwo parameters, ?, the symmetric Dirichlet priorfor document-topic distribution; ?, the symmetricDirichlet prior for the topic-word distribution.
Inboth models, we set ?
= 3.0 and ?
= 0.01.
Allparameters were selected in advance of the experi-ments; further tuning of these paramters is left forfuture work.5.2 Topic Coherence EvaluationTo be useful, a topic model should produce topicsthat human readers judge to be coherent.
Whilesome automated metrics have been shown to co-here with human coherence judgments (Newmanet al., 2010), it is possible that naive raters mighthave different judgments from subject matter ex-perts.
For this reason, we focused on human eval-uation, including both expert and novice opinions.One rater, R1, is an author of the paper (HH) anda Ph.D. student focusing on designing technologyto understand and support individuals with autismspectrum disorder.
The remaining three raters arenot authors of the paper and are not domain ex-perts.In the evaluation protocol, raters were presentedwith batteries of fifteen topics, from which theywere asked to select the three most coherent.
Ineach of the ten batteries, there were five topicsfrom each model, permuted at random.
Thus, af-ter completing the task, all 150 topics ?
50 topicsfrom each model ?
were rated.
The user interfaceof topic coherence evaluation is given in Figure 2,including the specific prompt.We note that this evaluation differs from the?intrusion task?
proposed by Chang et al.
(2009),in which raters are asked to guess which wordwas randomly inserted into a topic.
While the in-trusion task protocol avoids relying on subjectivejudgments of the meaning of ?coherence,?
it pre-vents expert raters from expressing a preferencefor topics that might be especially useful for anal-ysis of autism spectrum disorder.
Prior work hasalso shown that the variance of these tasks is high,making it difficult to distinguish between models.Table 2 shows, for each rater, the percentage oftopics were chosen from each model as the mostcoherent within each battery.
On average, 80% ofthe topics were chosen from our proposed model.If all three models are equally good at discover-Figure 2: The user interface of topic coherenceevaluation.RaterModel R1 R2 R3 R4 AverageOur model 70% 93% 80% 77% 80%AT 17% 7% 13% 10% 12%LDA 13% 0% 7% 13% 8%Table 2: Percentage of the most coherent topics that areselected from three different topic models: our model, theAuthor-Topic Model (AT), and latent Dirichlet allocation(LDA).ing coherent topics, the average percentage acrossthree models should be roughly equal.
Note thatthe opinion of the expert rater R1 is generally sim-ilar to the other three raters.6 Analysis of Aspies Central TopicsIn this section, we further use our model to ex-plore more information about the Aspies Centralforum.
We want to examine whether the autism-related topics identified the model can support re-searchers to gain qualitative understanding of theneeds and concerns of autism forum users.
We arealso interested in understanding the users?
behav-ioral patterns on autism-related topics.
The anal-ysis task has three components: first we will de-scribe the interesting topics from the autism do-main perpective.
Then we will find out the pro-portion of each topic, including autism related top-ics.
Finally, in order to understand the user activ-ity patterns on these autism related topics we willderive the topic-specific preference ranking of theusers from our model.103Index Proportion Top keywords Index Proportion Top keywords1 1.7% dont im organization couldnt construction 2 2.6% yah supervisor behavior taboo phone3 2.2% game watched games fallout played 4 3.5% volunteering esteem community art self5 1.1% nobody smell boss fool smelling 6 3.2% firefox razor blades pc console7 3.4% doesn?t it?s mandarin i?ve that?s 8 2.1% diagnosed facessenses visualize visual9 1.7% obsessions bookscollecting library authors 10 2.6% ptsd central cure neurotypical we11 1.2% stims mom nails lip shoes 12 1.8% classroom campus tag numbers exams13 1.6% battery hawke charlie ive swing 14 1.9% divorce william women marryrates15 0.1% chocolate pdd milk romance nose 16 5.8% kinda holland neccesarily employment bucks17 0.6% eat burgers jokes memory foods 18 2.4% dryer martial dream wake schedule19 3.7% depression beleive christianity buddhism becouse 20 1.4% grudges pairs glasses museum frames21 0.4% alma star gods alien sun 22 2.6% facebook profiles befriend friendships friends23 0.4% trilogy sci-fi cartoon iphone grandma 24 2.7% flapping stuffed toes curse animal25 1.5% empathy smells compassion emotions emotional 26 1.7% males evolution females originally constructive27 0.5% list dedicate lists humor song 28 4.6% nts aspies autie qc intuitive29 2.7% captain i?m film anime that?s 30 3.6% homeless pic wild math laugh31 3.3% shave exhausting during terrified products 32 5.6% you?re you your yourself hiring33 4.6% dictionary asks there?re offend fog 34 1.5% grade ed school 7th diploma35 1.0% cave blonde hair bald disney 36 1.9% diagnosis autism syndrome symptoms aspergers37 1.3% song joanna newsom rap favorites 38 1.8% poetry asleep children ghosts lots39 2.1% heat iron adhd chaos pills 40 3.6% bike zone rides zoning worrying41 1.2% uk maths team teams op 42 0.8% book books read reading kindle43 1.0% husband narcissist husband?s he hyper 44 1.1% songs guitar drums music synth45 1.3% autism disorder spectrum disorders pervasive 46 0.7% dog noise dogs barking noisy47 0.6% relationship women relationships sexual sexually 48 0.9% weed marijuana pot smoking fishing49 0.9% him he his bernard je 50 2.0% her she she?s kyoko she?llTable 3: 50 topics identified by our model.
The ?proportion?
columns show the topic proportions in thedataset.
Furthermore, 14 topics are highlighted as interesting topics for autism research.Table 3 shows all 50 topics from our model.
Foreach topic, we show the top five words related tothis topic.
We further identified fourteen topics(highlighted with blue color), which are particu-larly relevant to understand autism.Among the identified topics, there are threepopular topics discussed in the Aspies Central fo-rum: topic 4, topic 19 and topic 31.
From the topword list, we identified that topic 4 is composedof keywords related to psychological (e.g., self-esteem, art) and social (e.g., volunteering, com-munity) well-being of the Aspies Central users.Topic 19 includes discussion on mental healthissues (e.g., depression) and religious activities(e.g., believe, christianity, buddhism) as copingstrategies.
Topic 31 addresses a specific personalhygiene issue ?
helping people with autism learnto shave.
This might be difficult for individualswith sensory issues: for example, they may beterrified by the sound and vibration generated bythe shaver.
For example, topic 22 is about mak-ing friends and maintaining friendship; topic 12 isabout educational issues ranging from seeking ed-ucational resources to improving academic skillsand adjusting to college life.In addition to identifying meaningful topics, an-other capability of our model is to discover users?topic preferences and expertise.
Recall that, foruser i and topic k, our model estimates a author-topic preference variable ?ik.
Each ?ikrangesfrom 0 to 1, indicating the probability of user i toTopic User index5 USER 1, USER 2, USER 3, USER 4, USER 58 USER 1, USER 2, USER 6, USER 5, USER 712 USER 1, USER 2, USER 4, USER 8, USER 319 USER 1, USER 2, USER 3, USER 4, USER 722 USER 1, USER 2, USER 3, USER 9, USER 731 USER 1, USER 3, USER 2, USER 6, USER 1036 USER 1, USER 2, USER 4, USER 3, USER 1145 USER 1, USER 3, USER 4, USER 12, USER 1347 USER 2, USER 14, USER 15, USER 16 , USER 648 USER 5, USER 4, USER 6, USER 9, USER 2Table 4: The ranking of user preference on some interest-ing topics (we replace user IDs with user indices to avoidany privacy-related issue).
USER 1 is the moderator of thisforum.
In total, our model identifies 16 user with high topic-specific preference from 10 interesting topics.
For the other4 interesting topics, there is no user with significantly highpreference.answer a question on topic k. As we set the priorprobability of author-topic preference to be 0.2,we show topic-author pairs for which ?ik> 0.2in Table 4.The dominance of USER 1 in these topics is ex-plained by the fact that this user is the moderatorof the forum.
Besides, we also find some otherusers participating in most of the interesting top-ics, such as USER 2 and USER 3.
On the otherhand, users like USER 14 and USER 15 only showup in few topics.
This observation is supported bytheir activities on discussion boards.
Searching onthe Aspies Certral forum, we found most answerposts of user USER 15 are from the board ?love-104relationships-and-dating?.7 Related WorkSocial media has become an important source ofhealth information (Choudhury et al., 2014).
Forexample, Twitter has been used both for miningboth public health information (Paul and Dredze,2011) and for estimating individual health sta-tus (Sokolova et al., 2013; Teodoro and Naaman,2013).
Domain-specific online communities, suchAspies Central, have their own advantages, tar-geting specific issues and featuring more close-knit and long-term relationships among mem-bers (Newton et al., 2009).Previous studies on mining health informationshow that technical models and tools from com-putational linguistics are helpful for both under-standing contents and providing informative fea-tures.
Sokolova and Bobicev (2011) use sentimentanalysis to analyze opinions expressed in health-related Web messages; Hong et al.
(2012) focuson lexical differences to automatically distinguishschizophrenic patients from healthy individuals.Topic models have previously been used tomine health information: Resnik et al.
(2013) useLDA to improve the prediction for neuroticismand depression on college students, while Paul andDredze (2013) customize their factorial LDA tomodel the joint effect of drug, aspect, and routeof administration.
Most relevantly for the currentpaper, Nguyen et al.
(2013) use LDA to discoverautism-related topics, using a dataset of 10,000posts from ten different autism commnities.
How-ever, their focus was on automated classification ofcommunities as autism-related or not, rather thanon analysis and on providing support for qualita-tive autism researchers.
The applicability of themodel developed in our paper towards classifica-tion tasks is a potential direction for future re-search.In general, topic models capture latent themesin document collections, characterizing each doc-ument in the collection as a mixture of topics (Bleiet al., 2003).
A natural extension of topic mod-els is to infer the relationships between topics andmetadata such as authorship or time.
A relativelysimple approach is to represent authors as an ag-gregation of the topics in all documents they havewritten (Wagner et al., 2012).
More sophisticatedtopic models, such as Author-Topic (AT) model(Rosen-Zvi et al., 2004; Steyvers et al., 2004) as-sume that each document is generated by a mix-ture of its authors?
topic distributions.
Our modelcan be viewed as one further extension of topicmodels by incorporating more metadata informa-tion (authorship, thread structure) in online fo-rums.8 ConclusionThis paper describes how topic models can offerinsights on the issues and challenges faced by in-dividuals on the autism spectrum.
In particular,we demonstrate that by unifying textual contentwith authorship and thread structure metadata, wecan obtain more coherent topics and better under-stand user activity patterns.
This coherence is val-idated by manual annotations from both expertsand non-experts.
Thus, we believe that our modelprovides a promising mechanism to capture be-havioral and psychological attributes relating tothe special populations affected by their cognitivedisabilities, some of which may signal needs andconcerns about their mental health and social well-being.We hope that this paper encourages future ap-plications of topic modeling to help psychologistsunderstand the autism spectrum and other psycho-logical disorders ?
and we hope to obtain furthervalidation of our model through its utility in suchqualitative research.
Other directions for futurework include replication of our results across mul-tiple forums, and applications to other conditionssuch as depression and attention deficit hyperac-tivity disorder (ADHD).AcknowledgmentsThis research was supported by a Google FacultyAward to the last author.
We thank the three re-viewers for their detailed and helpful suggestionsto improve the paper.ReferencesDavid M. Blei and Jon D. McAuliffe.
2007.
Super-vised Topic Models.
In NIPS.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
the Journal of ma-chine Learning research, 3:993?1022.Jonathan Chang, Jordan L. Boyd-Graber, Sean Gerrish,Chong Wang, and David M. Blei.
2009.
ReadingTea Leaves: How Humans Interpret Topic Models.In Yoshua Bengio, Dale Schuurmans, John D. Laf-ferty, Christopher K. I. Williams, and Aron Culotta,105editors, NIPS, pages 288?296.
Curran Associates,Inc.Munmun De Choudhury, Meredith Ringel Morris, andRyen W. White.
2014.
Seeking and Sharing HealthInformation Online: Comparing Search Engines andSocial Media.
In Procedings of CHI.Jacob Eisenstein, Amr Ahmed, and Eric P. Xing.
2011.Sparse Additive Generative Models of Text.
InICML.Kai Hong, Christian G. Kohler, Mary E. March, Am-ber A. Parker, and Ani Nenkova.
2012.
Lexi-cal Differences in Autobiographical Narratives fromSchizophrenic Patients and Healthy Controls.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages37?47.
Association for Computational Linguistics,July.David Newman, Jey Han Lau, Karl Grieser, and Tim-othy Baldwin.
2010.
Automatic evaluation oftopic coherence.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 100?108.
Association for Computa-tional Linguistics.A.
Taylor Newton, Adam D.I.
Kramer, and Daniel N.McIntosh.
2009.
Autism online: a comparisonof word usage in bloggers with and without autismspectrum disorders.
In Proceedings of the SIGCHIConference on Human Factors in Computing Sys-tems, pages 463?466.
ACM.Thin Nguyen, Dinh Phung, and Svetha Venkatesh.2013.
Analysis of psycholinguistic processes andtopics in online autism communities.
In Multimediaand Expo (ICME), 2013 IEEE International Confer-ence on, pages 1?6.
IEEE.Michael J. Paul and Mark Dredze.
2011.
You AreWhat You Tweet: Analyzing Twitter for PublicHealth.
In ICWSM.Michael J. Paul and Mark Dredze.
2013.
Drug Ex-traction from the Web: Summarizing Drug Expe-riences with Multi-Dimensional Topic Models.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 168?178, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Philip Resnik, Anderson Garron, and Rebecca Resnik.2013.
Using Topic Modeling to Improve Predictionof Neuroticism and Depression in College Students.In Proceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing.Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers,and Padhraic Smyth.
2004.
The Author-TopicModel for Authors and Documents.
In UAI.Mark Schmidt, Ewout van den Berg, Michael P. Fried-lander, and Kevin Muphy.
2009.
Optimizing CostlyFunctions with Simple Constraints: A Limited-Memory Projected Quasi-Netton Algorithm.
In AIS-TATS.Marina Sokolova and Victoria Bobicev.
2011.
Sen-timents and Opinions in Health-related Web mes-sages.
In Proceedings of the International Confer-ence Recent Advances in Natural Language Pro-cessing 2011, pages 132?139, Hissar, Bulgaria,September.
RANLP 2011 Organising Committee.Marina Sokolova, Stan Matwin, Yasser Jafer, andDavid Schramm.
2013.
How Joe and Jane Tweetabout Their Health: Mining for Personal Health In-formation on Twitter.
In Proceedings of the In-ternational Conference Recent Advances in Natu-ral Language Processing RANLP 2013, pages 626?632, Hissar, Bulgaria, September.
INCOMA Ltd.Shoumen, BULGARIA.Mark Steyvers and Thomas Griffiths.
2005.
MatlabTopic Modeling Toolbox 1.4.Mark Steyvers, Padhraic Smyth, and Thomas Griffiths.2004.
Probabilistic Author-Topic Models for Infor-mation Discovery.
In KDD.Rannie Teodoro and Mor Naaman.
2013.
Fitter withTwitter: Understanding Personal Health and FitnessActivity in Social Media.
In Proceedings of the7th International Conference on Weblogs and SocialMedia.Claudia Wagner, Vera Liao, Peter Pirolli, Les Nel-son, and Markus Strohmaier.
2012.
It?s not intheir tweets: Modeling topical expertise of Twitterusers.
In ASE/IEEE International Conference on So-cial Computing.Martin J. Wainwright and Michael I. Jordan.
2008.Graphical models, exponential families, and varia-tional inference.
Foundations and Trends in Ma-chine Learning, 1(1-2):1?305.106
