A Neural Network System for Large-Vocabulary ContinuousSpeech Recognition in Variable Acoustic EnvironmentsJ.
Flanagan +, Q. Lin +, J. Pearson*, and B. de Vries*+CAIP  Center ,  Rutgers  Un ivers i ty* Dav id  Sarnof f  Research  CenterPerformance of speech recognizers is typically degraded bydeleterious properties of the acoustic environment, such asmultipath distortion (reverberation) and ambient noise.
Thedegradation becomes more prominent as the microphone ispositioned more distant from the speaker, for instance, ina teleconferencing application.
Mismatched training andtesting conditions, such as frequency response, microphone,signal-to-noise ratio (SNR), and room reverberation, also de-grade recognition performance.
Among available approachesto handling mismatches between training and testing condi-tions, a popular one is to retrain the speech recognizer un-der new environments.
Hidden Markov models (HMM) haveto date been accepted as an effective classification methodfor large vocabulary continuous peech recognition, e.g., theARPA-sponsored SPHINX and DECIPHER.
Retraining ofHMM-based recognizers is a complex and tedious task.
Itrequires recollection of speech data under corresponding con-ditions and reestimation of HMM's parameters.
Particularlygreat time and effort are needed to retrain a recognizer whichoperates in a speaker-independent mode, which is the modeof greatest general interest.ARPA Contract (No.
DABT63-93-C-0037), entitled "A neu-ral network system for large-vocabulary continuous speechrecognition in variable acoustic environments," aims to de-velop a system of microphone arrays (MA) and neural net-works (NN) for robust speech recognition.
The system will ex-pand the power and advantages of existing ARPA speech rec-ognizers to practical acoustic environments where users neednot be encumbered by hand-held, body-worn, or tethered mi-crophone equipment, and must have freedom of movement.
(Examples include Combat Information Centers, large groupconferences, and mobile hands-busy eyes-busy maintenancetasks.)
Use of MA provides autodirective sound pickup thatis higher in quality than conventional microphones used atdistances.
NN processors learn and compensate for environ-mental interference, and to adapt the testing condition to thetraining condition.
Recognition performance in hostile acous-tic environments can thereby be elevated without the need toretrain the recognizer.The Contract covers joint research between the CAIP Cen-ter and Sarnoff Research Center.
It commenced July 19,1993 and will run for 3 years.
To date, a new speech cor-pus has been created to evaluate the recognition performanceof synergistically-integrated systems of MA, NN, and ARPAspeech recognizers.
(The corpus is available to ARPA con-tractors upon request.)
The corpus consists of 50 male and 30female speakers.
Each speaker speaks 20 isolated command-words, 10 digits, and 10 continuous entences elected fromthe Resource Management task.
Two recording sessions aremade for each speaker.
One session is for simultaneous record-ing from a head-mounted close-talking microphone (HMD224) and from a 1-D beamforming line array microphone.The other is for simultaneous recording of a desk-mountedmicrophone (PZM 160) and the line array microphone.
Therecording environment is a hard-walled laboratory room of6x6x2.7 meters, having a reverberation time of approximately0.5 second.
Both the desk-mounted microphone and the linearray microphone are placed 3 meters from the subjects.
Therecording is done with an Ariel ProPort with a sampling fre-quency of 16 kHz and 16-bit linear quantization.A preliminary NN design is also completed.
It consists of13 multi-layer perceptrons (MLP), one for each of the 13 cep-strum coefficients used in the SPHINX recognizer.
Each MLPhas 3 layers.
The input layer has 9 input nodes, covering thecurrent speech frame and four preceding and four followingframes.
There are 4 sigmoid nodes in the hidden layer and1 linear node in the output layer.
The NN is trained usingbackpropagation methods when microphone-array speech andclose-talking speech are both available.
The trained MLP'sare then utilized to transform cepstrum coefficients of ar-ray speech to those appropriate to high-quality, close-talkingspeech.
The transformed cepstrum features are used as inputsto the SPHINX speech recognizer.Promising results have been achieved for recognition of iso-lated utterances.
Recognition accuracy under reverberantconditions is increased from from 13% for desk-mounted mi-crophone to 85% for the MA/NN system.
It is also found thatthe system of MA and NN in unmatched training/testing con-ditions yields higher word accuracies than those obtained witha retrained SPHINX when using array input for both trainingand testing, i.e., a matchedtralning/testing condition.
Similarresults have been obtained for speaker identification.Various NN architectures are being explored to identify an op-timal one.
Dynamic-time-warping (DTW)  classification tech-niques are also being included for experimental comparison.Because DTW recognizers have a simple back end, the capa-bility of the MA/NN system in improving recognition perfor-mance can be assessed more reliably.
Evaluation experimentsfor continuous peech are under way.
Work is planned to usethe DECIPHER recognizer for comparisons.
Also planned isa study of feature measures most appropriate for NN adapta-tion.
The CAIP Center has ongoing NSF projects on develop-ing 2-D and 3-D microphone arrays.
These new array micro-phones will later be used in the present work.
By the end ofthe Contract, we will implement and demonstrate a reliable,real-time, hands-free prototype speech recognition system forlaboratory evaluation.
The Contract work will directly im-pact the successful applications of ARPA automatic speechrecognition systems in adverse practical environments.470
