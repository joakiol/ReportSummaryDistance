Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 412?420, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsIterative Annotation Transformation with Predict-Self Reestimationfor Chinese Word SegmentationWenbin Jiang and Fandong Meng and Qun Liu and Yajuan Lu?Key Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of Sciences{jiangwenbin, mengfandong, liuqun, lvyajuan}@ict.ac.cnAbstractIn this paper we first describe the technol-ogy of automatic annotation transformation,which is based on the annotation adaptationalgorithm (Jiang et al2009).
It can auto-matically transform a human-annotated cor-pus from one annotation guideline to another.We then propose two optimization strategies,iterative training and predict-self reestimation,to further improve the accuracy of annota-tion guideline transformation.
Experiments onChinese word segmentation show that, the it-erative training strategy together with predict-self reestimation brings significant improve-ment over the simple annotation transforma-tion baseline, and leads to classifiers with sig-nificantly higher accuracy and several timesfaster processing than annotation adaptationdoes.
On the Penn Chinese Treebank 5.0,it achieves an F-measure of 98.43%, signif-icantly outperforms previous works althoughusing a single classifier with only local fea-tures.1 IntroductionAnnotation guideline adaptation depicts a generalpipeline to integrate the knowledge of corpora withdifferent underling annotation guidelines (Jiang etal., 2009).
In annotation adaptation two classifiersare cascaded together, where the classification re-sults of the lower classifier are used as guiding fea-tures of the upper classifier, in order to achieve moreaccurate classification.
This method can automat-ically adapt the divergence between different an-notation guidelines and bring improvement to Chi-nese word segmentation.
However, the need of cas-caded classification decisions makes it less practicalfor tasks of high computational complexity such asparsing, and less efficient to incorporate more thantwo annotated corpora.In this paper, we first describe the algorithm ofautomatic annotation transformation.
It is based onthe annotation adaptation algorithm, and it focuseson the automatic transformation (rather than adapta-tion) of a human-annotated corpus from one annota-tion guideline to another.
First, a classifier is trainedon the corpus with an annotation guideline not de-sired, it is used to classify the corpus with the an-notation guideline we want, so as to obtain a corpuswith parallel annotation guidelines.
Then a secondclassifier is trained on the parallelly annotated cor-pus to learn the statistical regularity of annotationtransformation, and it is used to process the previouscorpus to transform its annotation guideline to thatof the target corpus.
Instead of the online knowl-edge integration methodology of annotation adapta-tion, annotation transformation can lead to improvedclassification accuracy in an offline manner by usingthe transformed corpora as additional training datafor the classifier.
This method leads to an enhancedclassifier with much faster processing than the cas-caded classifiers in annotation adaptation.We then propose two optimization strategies, iter-ative training and predict-self reestimation, to fur-ther improve the accuracy of annotation transfor-mation.
Although the transformation classifierscan only be trained on corpora with autogenerated(rather than gold) parallel annotations, an iterativetraining procedure can gradually improve the trans-412formation accuracy by iteratively optimizing the par-allelly annotated corpora.
Both source-to-target andtarget-to-source annotation transformations are per-formed in each training iteration, and the trans-formed corpora are used to provide better annota-tions for the parallelly annotated corpora of the nextiteration; then the better parallelly annotated corporawill result in more accurate transformation classi-fiers, which will generate better transformed corporain the new iteration.
The predict-self reestimationis based on the following hypothesis, a better trans-formation result should be easier to be transformedback to the original form.
The predict-self heuristicis also validated by Daume?
III (2009) in unsuper-vised dependency parsing.Experiments in Chinese word segmentation showthat, the iterative training strategy together withpredict-self reestimation brings significant improve-ment over the simple annotation transformationbaseline.
We perform optimized annotation trans-formation from the People?s Daily (Yu et al2001)to the Penn Chinese Treebank 5.0 (CTB) (Xue etal., 2005), in order to improve the word segmenterwith CTB annotation guideline.
Compared to anno-tation adaptation, the optimized annotation transfor-mation strategy leads to classifiers with significantlyhigher accuracy and several times faster processingon the same data sets.
On CTB 5.0, it achieves an F-measure of 98.43%, significantly outperforms pre-vious works although using a single classifier withonly local features.The rest of the paper is organized as follows.Section 2 describes the classification-based Chineseword segmentation method.
Section 3 details thesimple annotation transformation algorithm and thetwo optimization methods.
After the introduction ofrelated works in section 4, we give the experimentalresults on Chinese word segmentation in section 5.2 Classification-Based Chinese WordSegmentationChinese word segmentation can be formalized asthe problem of sequence labeling (Xue and Shen,2003), where each character in the sentence is givena boundary tag denoting its position in a word.
Fol-lowing Ng and Low (2004), joint word segmenta-tion and part-of-speech (POS) tagging can also beAlgorithm 1 Perceptron training algorithm.1: Input: Training examples (xi, yi)2: ~??
03: for t?
1 .. T do4: for i?
1 .. N do5: zi ?
argmaxz?GEN(xi) ?
(xi, z) ?
~?6: if zi 6= yi then7: ~??
~?
+ ?
(xi, yi)??
(xi, zi)8: Output: Parameters ~?solved in a character classification approach by ex-tending the boundary tags to include POS informa-tion.
For word segmentation we adopt the 4 bound-ary tags of Ng and Low (2004), b, m, e and s, whereb, m and e mean the beginning, the middle and theend of a word, and s indicates a single-characterword.
The word segmentation result can be gen-erated by splitting the labeled character sequenceinto subsequences of pattern s or bm?e, indicatingsingle-character words or multi-character words, re-spectively.We choose the perceptron algorithm (Collins,2002) to train the character classifier.
It is an onlinetraining algorithm and has been successfully used inmany NLP tasks, including POS tagging (Collins,2002), parsing (Collins and Roark, 2004) and wordsegmentation (Zhang and Clark, 2007; Jiang et al2008; Zhang and Clark, 2010).The training procedure learns a discriminativemodel mapping from the inputs x ?
X to the outputsy ?
Y , where X is the set of sentences in the train-ing corpus and Y is the set of corresponding labeledresults.
We use the function GEN(x) to enumeratethe candidate results of an input x, and the function?
to map a training example (x, y) ?
X ?
Y to afeature vector ?
(x, y) ?
Rd.
Given the charactersequence x, the decoder finds the output F (x) thatmaximizes the score function:F (x) = argmaxy?GEN(x)S(y|~?,?, x)= argmaxy?GEN(x)?
(x, y) ?
~?
(1)Where ~?
?
Rd is the parameter vector (that is, thediscriminative model) and ?
(x, y) ?
~?
is the innerproduct of ?
(x, y) and ~?.Algorithm 1 shows the perceptron algorithm fortuning the parameter ~?.
The ?averaged parameters?413Type Feature TemplatesUnigram C?2 C?1 C0C1 C2Bigram C?2C?1 C?1C0 C0C1C1C2 C?1C1Property Pu(C0)T (C?2)T (C?1)T (C0)T (C1)T (C2)Table 1: Feature templates for classification-based Chi-nese segmentation model.technology (Collins, 2002) is used for better per-formance.
The feature templates for the classifieris shown in Table 1.
C0 denotes the current char-acter, while C?i/Ci denote the ith character to theleft/right of C0.
The function Pu(?)
returns truefor a punctuation character and false for others, thefunction T (?)
classifies a character into four types:number, date, English letter and others.3 Iterative and Predict-Self AnnotationTransformationThis section first describes the technology of au-tomatic annotation transformation, then introducesthe two optimization strategies, iterative training andpredict-self reestimation.
Iterative training takesa global view, it conducts several rounds of bidi-rectional annotation transformations, and improvethe transformation performance round by round.Predict-self reestimation takes a local view instead,it considers each training sentence, and improves thetransformation performance by taking into accountthe predication result of the reverse transformation.The two strategies can be adopted jointly to obtainbetter transformation performance.3.1 Automatic Annotation TransformationAnnotation adaptation can integrate the knowledgefrom two corpora with different underling annota-tion guidelines.
First, a classifier (source classi-fier) is trained on the corpus (source corpus) withan annotation standard (source annotation) not de-sired, it is then used to classify the corpus (targetcorpus) with the annotation standard (target annota-tion) we want.
Then a second classifier (transforma-tion classifier 1) is trained on the target corpus with1It is called target classifier in (Jiang et al2009).
Wethink that transformation classifier better reflects its role, theType Feature TemplatesBaseline C?2 C?1 C0C1 C2C?2C?1 C?1C0 C0C1C1C2 C?1C1Pu(C0)T (C?2)T (C?1)T (C0)T (C1)T (C2)Guiding ?C?2 ?
?
C?1 ?
?
C0 ?
?C1 ?
?
C2 ?
?C?2C?1 ?
?
C?1C0 ?
?
C0C1 ?
?C1C2 ?
?
C?1C1 ?
?Pu(C0) ?
?T (C?2)T (C?1)T (C0)T (C1)T (C2) ?
?Table 2: Feature templates for annotation transformation,where ?
is short for ?
(C0), representing the source an-notation of C0.the source classifier?s classification result as guid-ing features.
In decoding, a raw sentence is first de-coded by the source classifier, and then inputted intothe transformation classifier together with the anno-tations given by the source classifier, so as to obtainan improved classification result.However, annotation adaptation has a drawback,it has to cascade two classifiers in decoding to inte-grate the knowledge in two corpora, thus seriouslydegrades the processing speed.
This paper describesa variant of annotation adaptation, name annotationtransformation, aiming at automatic transformation(rather than adaptation) between annotation stan-dards of human-annotated corpora.
In annotationtransformation, a source classifier and a transforma-tion classifier are trained in the same way as in an-notation adaptation.
The transformation classifier isused to process the source corpus, with the classi-fication label derived from the segmented sentencesas the guiding features, so as to relabel the sourcecorpus with the target annotation guideline.
By inte-grating the target corpus and the transformed sourcecorpus for the training of the character classifier, im-proved classification accuracy can be achieved.Both the source classifier and the transforma-tion classifier are trained with the perceptron algo-rithm.
The feature templates used for the sourceclassifier are the same with those for the baselinerenaming also avoids name confusion in the optimized annota-tion transformation.414Algorithm 2 Baseline annotation transformation.1: function ANNOTRANS(Cs, Ct)2: Ms ?
TRAIN(Cs)3: Cst ?
ANNOTATE(Ms, Ct)4: Ms?t ?
TRANSTRAIN(Cst , Ct)5: Cts ?
TRANSANNOTATE(Ms?t, Cs)6: Ct?
?
Cts ?
Ct7: return Ct?8: function DECODE(M, ?, x)9: return argmaxy?GEN(x) S(y|M,?, x)character classifier.
The feature templates for thetransformation classifier are the same with those inannotation adaptation, as listed in Table 2.
Al-gorithm 2 shows the overall training algorithmfor annotation transformation.
Cs and Ct denotethe source corpus and the target corpus; Ms andMs?t denote the source classifier and the trans-formation classifier; Cqp denotes the p corpus re-labeled in q annotation guideline, for example Ctsis the source corpus transformed to target annota-tion guideline; Functions TRAIN and TRANSTRAINboth invoke the perceptron algorithm, yet withdifferent feature sets; Functions ANNOTATE andTRANSANNOTATE call the function DECODE withdifferent models (source/transformation classifiers),feature functions (without/with guiding features),and inputs (raw/source-annotated sentences).The best training iterations for the functionsTRAIN and TRANSTRAIN are determined on the de-veloping sets of the source corpus and the targetcorpus, respectively.
In the algorithm the param-eters corresponding to developing sets are omittedfor simplicity.
Compared to the online knowledgeintegration methodology of annotation adaptation,annotation transformation leads to improved perfor-mance in an offline manner by integrating corporabefore the training procedure.
This manner couldachieve processing several times as fast as the cas-caded classifiers in annotation adaptation.
In the fol-lowing we will describe the two optimization strate-gies in details.3.2 Iterative Training for AnnotationTransformationThe training of annotation transformation is basedon an auto-generated (rather than gold) parallelly an-notated corpus, where the source annotation is pro-Algorithm 3 Iterative annotation transformation.1: function ITERANNOTRANS(Cs, Ct)2: Ms ?
TRAIN(Cs)3: Cst ?
ANNOTATE(Ms, Ct)4: Mt ?
TRAIN(Ct)5: Cts ?
ANNOTATE(Mt, Cs)6: repeat7: Ms?t ?
TRANSTRAIN(Cst , Ct)8: Mt?s ?
TRANSTRAIN(Cts, Cs)9: Cts ?
TRANSANNOTATE(Ms?t, Cs)10: Cst ?
TRANSANNOTATE(Mt?s, Ct)11: Ct?
?
Cts ?
Ct12: M?
?
TRAIN(Ct?
)13: until EVAL(M?)
converges14: return Ct?15: function DECODE(M, ?, x)16: return argmaxy?GEN(x) S(y|M,?, x)vided by the source classifier.
Therefore, the perfor-mance of transformation training is correspondinglydetermined by the accuracy of the source classifier.We propose an iterative training procedure togradually improve the transformation accuracy byiteratively optimizing the parallelly annotated cor-pora.
In each training iteration, both source-to-targetand target-to-source annotation transformations areperformed, and the transformed corpora are used toprovide better annotations for the parallelly anno-tated corpora of the next iteration.
Then in the newiteration, the better parallelly annotated corpora willresult in more accurate transformation classifiers, soas to generate better transformed corpora.Algorithm 3 shows the overall procedure of theiterative training method.
The loop of lines 6-13iteratively performs source-to-target and target-to-source annotation transformations.
The source an-notations of the parallelly annotated corpora, Cst andCts, are initialized by applying the source and tar-get classifiers respectively on the target and sourcecorpora (lines 2-5).
In each training iteration, thetransformation classifiers are trained on the currentparallelly annotated corpora (lines 7-8), they areused to produce the transformed corpora (lines 9-10)which provide better annotations for the parallellyannotated corpora of the next iteration.
The itera-tive training terminates when the performance of theclassifier trained on the merged corpus Cts ?
Ct con-verges.415The discriminative training of TRANSTRAIN pre-dicts the target annotations with the guidance ofsource annotations.
In the first iteration, the trans-formed corpora generated by the transformationclassifiers are better than the initialized ones gener-ated by the source and target classifiers, due to theassistance of the guiding features.
In the follow-ing iterations, the transformed corpora provide bet-ter annotations for the parallelly annotated corporaof the subsequent iteration, the transformation ac-curacy will improve gradually along with optimiza-tion of the parallelly annotated corpora until conver-gence.3.3 Predict-Self Reestimation for AnnotationTransformationThe predict-self hypothesis is implicit in many unsu-pervised learning approaches, such as Markov ran-dom field.
This methodology has also been success-fully used by Daume?
III (2009) in unsupervised de-pendency parsing.
The basic idea of predict-self isthat, if a prediction is a better candidate for an input,it can be easier converted back to the original inputby a reverse procedure.
If applied to the task of an-notation transformation, predict-self indicates that abetter transformation candidate following the targetannotation guideline can be easier transformed backto the original form following the source annotationguideline.The most intuitionistic strategy to introduce thepredict-self methodology into annotation transfor-mation is using a reversed annotation transforma-tion procedure to filter out unreliable predictions ofthe previous transformation.
In detail, a source-to-target annotation transformation is performed on thesource annotated sentence to obtain a prediction thatfollows the target annotation guideline, then a sec-ond, target-to-source transformation is performedon this prediction result to check whether it canbe transformed back to the previous source annota-tion.
Transformation results failing in this reversalverification are discarded, so this strategy is namedpredict-self filtration.A more precious strategy can be called predict-self reestimation.
Instead of using the reversedtransformation procedure for filtration, the rees-timation strategy integrates the scores given bythe source-to-target and target-to-source annotationtransformation models when evaluating the transfor-mation candidates.
By properly tuning the relativeweights of the two transformation directions, bet-ter transformation performance would be achieved.The scores of the two transformation models areweighted integrated in a log-linear manner:S+(y|Ms?t,Mt?s,?, x)= (1?
?)?
S(y|Ms?t,?, x)+ ??
S(x|Mt?s,?, y)(2)The weight parameter ?
is tuned on the develop-ing set.
To integrating the predict-self reestima-tion into the iterative transformation training, a re-versed transformation model is introduced and theenhanced scoring function above is used when thefunction TRANSANNOTATE invokes the functionDECODE.4 Related WorksResearches focused on the automatic adaptationbetween different corpora can be roughly clas-sified into two kinds, adaptation between differ-ent domains (with different statistical distribution)(Blitzer et al2006; Daume?
III, 2007), and adapta-tion between different annotation guidelines (Jianget al2009; Zhu et al2011).
There are alsosome efforts that totally or partially resort to man-ual transformation rules, to conduct treebank con-version (Cahill and Mccarthy, 2002; Hockenmaierand Steedman, 2007; Clark and Curran, 2009), andword segmentation guideline transformation (Gaoet al2004; Mi et al2008).
This work focuseson the automatic transformation between annotationguidelines, and proposes better annotation transfor-mation technologies to improve the transformationaccuracy and the utilization rate of human-annotatedknowledge.The iterative training procedure proposed in thiswork shares some similarity with the co-training al-gorithm in parsing (Sarkar, 2001), where the train-ing procedure lets two different models learn fromeach other during parsing the raw text.
The keyidea of co-training is utilize the complementarity ofdifferent parsing models to mine additional trainingdata from raw text, while iterative training for an-notation transformation emphasizes the iterative op-timization of the parellelly annotated corpora used416Partition Sections # of wordCTBTraining 1?
270 0.47M400?
9311001?
1151Developing 301?
325 6.66KTest 271?
300 7.82KPDTraining 02?
06 5.86MTest 01 1.07MTable 3: Data partitioning for CTB and PD.to train the transformation models.
The predict-self methodology is implicit in many unsupervisedlearning approaches, it has been successfully usedby (Daume?
III, 2009) in unsupervised dependencyparsing.
We adapt this idea to the scenario of anno-tation transformation to improve transformation ac-curacy.In recent years many works have been devoted tothe word segmentation task.
For example, the in-troduction of global training or complicated features(Zhang and Clark, 2007; Zhang and Clark, 2010);the investigation of word structures (Li, 2011);the strategies of hybrid, joint or stacked modeling(Nakagawa and Uchimoto, 2007; Kruengkrai et al2009; Wang et al2010; Sun, 2011), and the semi-supervised and unsupervised technologies utilizingraw text (Zhao and Kit, 2008; Johnson and Gold-water, 2009; Mochihashi et al2009; Hewlett andCohen, 2011).
We estimate that the annotation trans-formation technologies can be adopted jointly withcomplicated features, system combination and semi-supervised/unsupervised technologies to further im-prove segmentation performance.5 Experiments and AnalysisWe perform annotation transformation from Peo-ple?s Daily (PD) (Yu et al2001) to Penn Chi-nese Treebank 5.0 (CTB) (Xue et al2005), follow-ing the same experimental setting as the annotationadaptation work (Jiang et al2009) for convenienceof comparison.
The two corpora are segmented fol-lowing different segmentation guidelines and differlargely in quantity of data.
CTB is smaller in sizewith about 0.5M words, while PD is much larger,containing nearly 6M words.Test on (F1%)Train on CTB SPDCTB 97.35 86.65(?
10.70)SPD 91.23(?
3.02) 94.25Table 4: Performance of the perceptron classifiers forChinese word segmentation.Model Time (s) Accuracy (F1%)Merging 1.33 93.79Anno.
Adapt.
4.39 97.67Anno.
Trans.
1.33 97.69Baseline 1.21 97.35Table 5: Comparison of the baseline annotation transfor-mation, annotation adaptation and a simple corpus merg-ing strategy.To approximate more general scenarios of anno-tation adaptation problems, we extract from PD asubset which is comparable to CTB in size.
We ran-domly select 20, 000 sentences (0.45M words) fromthe PD training data as the new training set, and1000/1000 sentences from the PD test data as thenew test/developing set.
2 We name the smaller ver-sion of PD as SPD.
The balanced source corpus andtarget corpus also facilitate the investigation of an-notation transformation.5.1 Baseline Classifiers for Word SegmentationWe train the baseline perceptron classifiers de-scribed in section 2 on the training sets of SPDand CTB, using the developing sets to determine thebest training iterations.
The performance measure-ment indicators for word segmentation is balancedF-measure, F = 2PR/(P + R), a function of Pre-cision P and Recall R. where P is the percentageof words in segmentation result that are segmentedcorrectly, and R is the percentage of correctly seg-mented words in the gold standard words.Accuracies of the baseline classifiers are listed inTable 4.
We also report the performance of the clas-sifiers on the test sets of the opposite corpora.
Ex-perimental results are in line with our expectations.A classifier performs better in its corresponding testset, and performs significantly worse on a test setfollowing a different annotation guideline.2There are many extremely long sentences in original PDcorpus, we split them into normal sentences according to periodpunctuations.41795.495.695.89696.296.40  1  2  3  4  5  6  7  8  9  10Accuracy(F%)Training iterationsIterative trainingBaseline annotation transformationFigure 1: Learning curve of iterative training for annota-tion transformation.5.2 Annotation Transformation vs. AnnotationAdaptationExperiments of annotation transformation are con-ducted on the direction of SPD-to-CTB.
The trans-formed corpus can be merged into the regular cor-pus, so as to train an enhanced classifier.
As com-parison, the cascaded model of annotation adapta-tion (Jiang et al2009) is faithfully implemented(yet using our feature representation) and tested onthe same adaptation direction.Table 5 shows the performances of the classi-fiers resulted by the baseline annotation transforma-tion and annotation adaptation, as well as the clas-sifier trained on the directly merged corpus.
Thetime costs for decoding are also listed to facilitatethe comparison of practicality.
We find that the sim-ple corpus merging strategy leads to dramatic de-crease in accuracy, due to the different and incom-patible annotation guidelines.
The baseline annota-tion transformation method leads to a classifier withaccuracy increment comparable to that of the anno-tation adaptation strategy, while consuming only onethird of the decoding time.5.3 Iterative Training with Predict-SelfReestimationWe adopt the iterative training strategy to the base-line annotation transformation model.
The CTB de-veloping set is used to determine the best trainingiteration for annotation transformation from SPD toCTB.
After each iteration, we test the performanceof the classifier trained on the merged corpus.
Fig-ure 1 shows the performance curve, with iterations95.495.695.89696.296.40  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Accuracy(F%)Predict-self ratioPredict-self reestimationPredict-self filtrationBaseline annotation transformationFigure 2: Performance of predict-self filtration andpredict-self reestimation.95.495.695.89696.296.40  1  2  3  4  5  6  7  8  9  10Accuracy(F%)Training iterationsIterative training with predict-self reestimationIterative trainingFigure 3: Learning curve of iterative training withpredict-self reestimation for annotation transformation.ranging from 1 to 10.
The performance of the base-line annotation transformation model is naturally in-cluded in the curve (located at iteration 1).
Thecurve shows that the performance of the classifiertrained on the merged corpus consistently improvesfrom iteration 2 to iteration 5.Experimental results of predict-self filtration andpredict-self reestimation are shown in Figure 2.The curve shows the performance of the predict-selfreestimation according to a series of weight param-eters, ranging from 0 to 1 with step 0.05.
The pointat ?
= 0 shows the performance of the baselineannotation transformation strategy.
The upper hor-izontal line shows the performance of predict-selffiltration.
We find that predict-self filtration bringsslight improvement over the baseline, and predict-self reestimation outperforms the filtration strategywhen ?
falls in a proper range.
An initial analysison the experimental results of predict-self filtration418Model Time (s) Accuracy (F1%)SPD?
CTBAnno.
Adapt.
4.39 97.67Opt.
Trans.
1.33 97.97PD?
CTBAnno.
Adapt.
4.76 98.15Opt.
Trans.
1.37 98.43Previous Works(Jiang et al2008) 97.85(Kruengkrai et al2009) 97.87(Zhang and Clark, 2010) 97.79(Sun, 2011) 98.17Table 6: The performance of the iterative annotationtransformation with predict-self reestimation comparedwith annotation adaptation.shows that, the filtration discards 5% of the train-ing sentences and these discarded sentences containnearly 10% of training words.
It can be confirmedthat the sentences discarded by predict-self filtra-tion are much longer and more complicated.
With aproperly tuned weight, predict-self reestimation canmake better use of the training data.
The best F-measure improvement achieved over the annotationtransformation baseline is 0.3 points, a little worsethan that brought by iterative training.Figure 3 shows the performance curve of iterativeannotation transformation with predict-self reesti-mation.
We find that the predict-self reestimationbrings improvement to the iterative training at eachiteration.
The maximum performance is achievedat iteration 4.
The corresponding model is evalu-ated on the test set of CTB, table 6 shows the ex-perimental results.
Compared to annotation adapta-tion, the optimized annotation transformation strat-egy leads to a classifier with significantly higher ac-curacy and several times faster processing.
Whenusing the whole PD as the source corpus, the finalclassifier 3 achieves an F-measure of 98.43%, sig-nificantly outperforms previous works although us-ing a single classifier with only local features.
Ofcourse, the comparison between our system and pre-vious works without using additional training datais unfair.
This work aim to find another way to im-prove Chinese word segmentation, which focuses onthe collection of more training data instead of mak-3The predict-self reestimation ratio ?
is fixed after the firsttraining iteration for efficiency.ing full use of a certain corpus.
We believe that theperformance can be further improved by adoptingthe advanced technologies of previous works, suchas complicated features and model combination.Considering the fact that today some corpora forword segmentation are really large (usually tensof thousands of sentences), it is necessary to ob-tain the latest CTB and investigate whether andhow much does annotation transformation bring im-provement to a much higher baseline.
On the otherhand, it is valuable to conduct experiments withmore source-annotated training data, such as theSIGHAN dataset, to investigate the trend of im-provement along with the increment of the addi-tional annotated sentences.
It is also valuable toevaluate the improved word segmenter on the out-of-domain datasets.
However, currently most cor-pora for Chinese word segmentation do not explic-itly distinguish the domains of their data sections, itmakes such evaluations difficult to conduct.6 Conclusion and Future WorksIn this paper, we first describe an annotation trans-formation algorithm to automatically transform ahuman-annotated corpus from one annotation guide-line to another.
Then we propose two optimizationstrategies, iterative training and predict-self reesti-mation, to further improve the accuracy of anno-tation guideline transformation.
On Chinese wordsegmentation, the optimized annotation transforma-tion strategy leads to classifiers with obviously bet-ter performance and several times faster processingon the same datasets, compared to annotation adap-tation.
When adopting the whole PD as the sourcecorpus, the final classifier significantly outperformsprevious works on CTB 5.0, although using a singleclassifier with only local features.As future works, we will investigate the accel-eration of the iterative training and the weight pa-rameter tuning, and extend the optimized annotationtransformation strategy to joint Chinese word seg-mentation and POS tagging, parsing and other NLPtasks.AcknowledgmentsThe authors were supported by National NaturalScience Foundation of China, Contracts 90920004419and 61100082, and 863 State Key Project No.2011AA01A207.
We are grateful to the anonymousreviewers for their thorough reviewing and valuablesuggestions.ReferencesJohn Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of EMNLP.Aoife Cahill and Mairead Mccarthy.
2002.
Automaticannotation of the penn treebank with lfg f-structure in-formation.
In in Proceedings of the LREC Workshop.Stephen Clark and James R. Curran.
2009.
Comparingthe accuracy of ccg and penn treebank parsers.
In Pro-ceedings of ACL-IJCNLP.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceedingsof ACL 2004.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP, pages 1?8, Philadelphia, USA.Hal Daume?
III.
2007.
Frustratingly easy domain adapta-tion.
In Proceedings of ACL.Hal Daume?
III.
2009.
Unsupervised search-based struc-tured prediction.
In Proceedings of ICML.Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,Hongqiao Li, Xinsong Xia, and Haowei Qin.
2004.Adaptive chinese word segmentation.
In Proceedingsof ACL.Daniel Hewlett and Paul Cohen.
2011.
Fully unsuper-vised word segmentation with bve and mdl.
In Pro-ceedings of ACL.Julia Hockenmaier and Mark Steedman.
2007.
Ccgbank:a corpus of ccg derivations and dependency structuresextracted from the penn treebank.
In ComputationalLinguistics, volume 33(3), pages 355?396.Wenbin Jiang, Liang Huang, Yajuan Lv, and Qun Liu.2008.
A cascaded linear model for joint chinese wordsegmentation and part-of-speech tagging.
In Proceed-ings of ACL.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.
Au-tomatic adaptation of annotation standards: Chineseword segmentation and pos tagging?a case study.
InProceedings of the 47th ACL.Mark Johnson and Sharon Goldwater.
2009.
Improvingnonparameteric bayesian inference: experiments onunsupervised word segmentation with adaptor gram-mars.
In Proceedings of NAACL.Canasai Kruengkrai, Kiyotaka Uchimoto, JunichiKazama, Yiou Wang, Kentaro Torisawa, and HitoshiIsahara.
2009.
An error-driven word-character hybridmodel for joint chinese word segmentation and postagging.
In Proceedings of ACL-IJCNLP.Zhongguo Li.
2011.
Parsing the internal structure ofwords: A new paradigm for chineseword segmenta-tion.
In Proceedings of ACL.Haitao Mi, Deyi Xiong, and Qun Liu.
2008.
Researchon strategy of integrating chinese lexical analysis andparser.
In Journal of Chinese Information Processing.Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.2009.
Bayesian unsupervised word segmentation withnested pitman-yor language modeling.
In Proceedingsof ACL-IJCNLP.Tetsuji Nakagawa and Kiyotaka Uchimoto.
2007.
A hy-brid approach to word segmentation and pos tagging.In Proceedings of ACL.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese part-of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based?
In Proceedings of EMNLP.Anoop Sarkar.
2001.
Applying co-training methods tostatistical parsing.
In Proceedings of NAACL.Weiwei Sun.
2011.
A stacked sub-word model forjoint chinese word segmentation and part-of-speechtagging.
In Proceedings of ACL.Kun Wang, Chengqing Zong, and Keh-Yih Su.
2010.
Acharacter-based joint model for chinese word segmen-tation.
In Proceedings of COLING.Nianwen Xue and Libin Shen.
2003.
Chinese word seg-mentation as lmr tagging.
In Proceedings of SIGHANWorkshop.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
In Natural Lan-guage Engineering.Shiwen Yu, Jianming Lu, Xuefeng Zhu, Huiming Duan,Shiyong Kang, Honglin Sun, Hui Wang, Qiang Zhao,and Weidong Zhan.
2001.
Processing norms of mod-ern chinese corpus.
Technical report.Yue Zhang and Stephen Clark.
2007.
Chinese segmenta-tion with a word-based perceptron algorithm.
In Pro-ceedings of ACL 2007.Yue Zhang and Stephen Clark.
2010.
A fast decoder forjoint word segmentation and pos-tagging using a sin-gle discriminative model.
In Proceedings of EMNLP.Hai Zhao and Chunyu Kit.
2008.
Unsupervised segmen-tation helps supervised learning of character taggingfor word segmentation and named entity recognition.In Proceedings of SIGHAN Workshop.Muhua Zhu, Jingbo Zhu, and Minghan Hu.
2011.
Betterautomatic treebank conversion using a feature-basedapproach.
In Proceedings of ACL.420
