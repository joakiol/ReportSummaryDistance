Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 46?53,Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational LinguisticsUsing Grammar Rule Clusters for Semantic Relation ClassificationEmily JamisonIndependent ScholarLos Alamos, NM 87544, USAjamison@ling.ohio-state.eduAbstractAutomatically-derived grammars, such as thesplit-and-merge model, have proven helpfulin parsing (Petrov et al, 2006).
As suchgrammars are refined, latent information isrecovered which may be usable for linguis-tic tasks besides parsing.
In this paper, wepresent and examine a new method of seman-tic relation classification: using automatically-derived grammar rule clusters as a robustknowledge source for semantic relation clas-sification.
We examine performance of thisfeature group on the SemEval 2010 RelationClassification corpus, and find that it improvesperformance over both more coarse-grainedand more fine-grained syntactic and colloca-tional features in semantic relation classifica-tion.1 IntroductionIn the process of discovering a refined grammarstarting from rules in the original treebank gram-mar, latent-variable grammars recover latent infor-mation.
Intuitively, the new split grammar statesshould reflect linguistic information that has beengeneralized from the lexical level but is not so gen-eral as the original syntactic level.
While the in-tended use of this information is to improve syntac-tic parsing, the lexically-derived nature of the splitgrammar states suggests it may contain semantic in-formation as well.Petrov et al (2006) note that while some of thesesplit grammar states reflect true linguistic informa-tion, such as the clustering of verbs with similar de-pendencies, other grammar states may reflect use-less information, such as a split between rules thateach terminate in a comma.
However, it is the auto-matic nature of grammar splitting which shows po-tential for deriving semantic knowledge; such splitgrammar states may reflect statistical and linguisticobservations not noticed by humans.In this paper, we use this recovered latent infor-mation for the classification of semantic relations.Our goal is to determine whether recovered latentgrammatical information is capable of contributingto the real-world linguistic task of relation classifica-tion.
We will compare the feature performance of re-covered latent information with that of other syntac-tic and collocational features to determine whetheror not the recovered latent information is helpful insemantic relation classification.2 Task DescriptionWe performed the task of classifying semantic rela-tions from SemEval 2010 Task 8: Multi-way Clas-sification of Semantic Relations between Pairs ofNominals.
Each instance consists of a sentence,marked with two nominals, e1 and e2.
One of 19possible direction-sensitive relations is annotated foreach pair of nominals.
Two examples are shown be-low.?
The <e1>author</e1> of a keygen uses a<e2>disassembler</e2> to look at the rawassembly code.Relation: Instrument-Agency(e2,e1)?
Their <e1>knowledge</e1> of the powerand rank symbols of the Continental em-46pires was gained from the numerous Germanic<e2>recruits</e2> in the Roman army, andfrom the Roman practice of enfeoffing variousGermanic warrior groups with land in the im-perial provinces.Relation: Entity-Origin(e1,e2)We classified the semantic relations using aMaximum Entropy classifier.
In our system,classification was 19-way, direction-sensitive1between the classifications: Entity-Origin, Entity-Destination, Cause-Effect, Product-Producer,Content-Container, Instrument-Agency, Member-Collection, Component-Whole, Message-Topic,and Other (non-directional).
The model was trainedon the 8000-instance training section of the Se-mEval 2010 Task 8 Semantic Relations Corpus.Distribution of the training data is shown in Table 1(Hendrickx et al, 2010).Class count % of DataOther 1410 17.63%Cause-Effect 1003 12.54%Component-Whole 941 11.76%Entity-Destination 845 10.56%Product-Producer 717 8.96%Entity-Origin 716 8.95%Member-Collection 690 8.63%Message-Topic 634 7.92%Content-Container 540 6.75%Instrument-Agency 504 6.30%Table 1: Class distribution in the training section of Se-mEval 2010 Task 8 Semantic Relations Corpus.We tested the model on the 2717-instance testingsection of the same corpus.
For each instance, theuser was provided with a sentence containing twomarked entities, e1 and e2.
We structured the tasksuch that, for each instance, we chose the best se-mantic relation out of the 19 available.In this paper, we use grammatical cluster infor-mation (i.e., recovered latent information) from theBerkeley Parser (Petrov 2006) as semantic featuresof syntactic origin to classify semantic relations inthe SemEval 2010 Semantic Relations corpus, in a1i.e., with a Content-Container relation, the nominal that isthe container and the nominal that is the content cannot be re-versed.Maximum Entropy model.
We conduct two sets ofexperiments.
In the first experiment, we examine theeffect of using Berkeley Parser latent cluster featuresto enhance specificity over more general features(POS tags and others), where the cluster features areinherently more closely tuned with the data than theother features, and more likely to lead to an over-fitted model.
In the second experiment, we examinethe effect of using cluster features to enhance gener-alizability over more specific features (the words ofthe cluster features?
terminal nodes), in which casethe cluster features generalize over othe more spe-cific features, but are more likely to miss detailedpatterns.2.1 Previous WorkThe classification of semantic relations has beenproposed to help NLP tasks ranging from word sensedisambiguation, language modelling, paraphrasing,and recognising textual entailment (Hendrickx et al,2010).Semantic world knowledge is crucial for accuratesemantic classification of many types, and sourcesrange from the hand-crafted-yet sparse (such asWordNet) to the robust-yet-noisy (such as the In-ternet).
For this community task, teams proposeda variety of knowledge sources and other fea-tures for their relation classification, from knowl-edge databases (Tymoshenko and Giuliano, 2010),WordNet (Rink and Harabagiu, 2010), Wikipedia(Szarvas and Gurevych, 2010), to formal linguisticLevin classes (Rink and Harabagiu, 2010), to col-locational metrics (Rink and Harabagiu, 2010) andstems (Chen et al, 2010).Syntactic features present special benefits to anysemantic classification task: they can generalizeover the local context in ways that collocational met-rics cannot, and unlike knowledge database sourceswhich assign the most common word sense to aword, syntactic features are sensitive to the word?ssense, as determined by the local context of theword.
Several teams in SemEval 2010 Task 8 usedsyntactic features for semantic relation classifica-tion.
Chen et al (2010) use a feature set of thesyntactic parent node held in common by the twonominals.
Rink and Harabagiu (2010) use a featureset of dependency paths of length 1 or 2 from thedependency tree around the two nominals.472.2 Grammatical cluster informationFor our investigations, we used the Berkeley Parser(Petrov et al2006, Petrov and Klein 2007) as asource of grammar rule clusters.
We used theeng sm6.gr off-the-shelf model.The Berkeley Parser starts with an initial gram-mar extracted from Wall Street Journal corpus sec-tions 2-22.
The parser then tries to learn a set of ruleprobabilities over latent annotations to maximize thelikelihood of the training trees using Expectation-Maximization (EM).Consider a sentence w and its unannotated tree T ,a non-terminal A spanning (r, t), and its children Band C spanning (r, s) and (s, t).
Ax is a subsymbolofA, Bx ofB, and Cx of C. We calculate the poste-rior probability of all annotated rules and positionsfor each training set tree T in the Expectation step(Petrov et al, 2006):(1)P ((r, s, t, Ax ?
ByCz) | w, T ) ?
POUT(r, t, Ax)??
(Ax ?
ByCz)PIN(r, s, By)PIN(s, t, Cz)The probabilities from the Expectation step act asweighted observations to update the rule probabili-ties in the Maximization step:?
(Ax ?
ByCz) :=#{Ax ?
ByCz}?y?,z?#{Ax ?
By?Cz?
}(2)In each cycle of EM, the grammar is split ran-domly in halves, and some halves are merged backtogether.
The grammar is retrained, and the resultsare used to initialize the next round of EM.In the splitting step, all grammatical nodes aresplit in two.
Although the grammar grows morefinely fitted to the training data with each splittingstep, its size quickly becomes unmanageable, itsrules become overfitted, and because the splits arenot a result of likelihood calculation, many unhelp-ful rules are produced.
The merging step functionsto remove unhelpful rules.
In the merging step, eachsplit is examined for the loss of likelihood removingit would cause; splits whose likelihood contributionis below a cutoff are re-combined.The experiments we perform in this paper are agamble on the possibility that the saved splits arepicking up semantic information from the rule struc-ture they reflect in the increased likelihood.
We usethe final split cluster ID?s (PP-5, PP-8, etc.)
as fea-tures in our experiments.22.3 FeaturesWe used several sets of features in our experi-ments.
All POS-tags, syntactic structure, and Clus-ter ID features come from the Berkeley Parser.
Thelemmatization comes from Morpha (Minnen et al,2001).
All features occurring less than two times inthe training data were discarded, for ease of process-ing.
A sample sentence and the resulting featuresare shown in Table 2.
Note that all features, col-locational and syntactic, were used for discoveringsemantic knowledge.The Crayola <e1>box</e1> con-tained two <e2>pencils</e2>.SW the-dt, crayola-jj, contain-vbd,two-cd, pencil-nns, box-nnIBW contained, two, contained?twoOCW crayola-jj, box-nn, contain-vbd,pencil-nnsPOS-tags vbd, cd, vbd?cdID?s vbd6, cd1, vbd6?cd1Table 2: A sample sentence and its accompanying fea-tures.Collocational Features:?
Surrounding Words (SW): From Ye andBaldwin?s (2007) preposition sense disam-biguation system, this set of features consistsof lemmas of all of the words within a windowof seven words before and after each of e1 ande2.
Features are not, however, marked with rel-ative location, as we found that this reduced ac-curacy.?
In-Between Words (IBW): This bag of fea-tures consists of the string of words occuring inthe sentence in between e1 and e2, exclusive, aswell as all the substrings of those consecutivewords.
We tried marking each feature with itsrelative location, but we found that results im-proved without location marking, and so we donot use location marking in these experiments.2Note that cluster ID?s are only meaningful when comparedto other cluster ID?s split from the same parent node.48Syntactic Features:?
Open Class Words (OCW): from Ye andBaldwin?s (2007) preposition sense disam-biguation system, this set of features consistsof the lemmas of all of the open-class words inthe sentence (i.e., NP, VP, ADJP, ADVP).?
POS-tags: The POS tags of the words (i.e., ter-minal nodes) and all consecutive strings of POStags in between e1 and e2, exclusive.
Tags arefrom the Berkeley Parser.?
Cluster ID?s: The Berkeley Parser syntacticrule cluster ID?s and POS-tags of the termi-nal nodes in between e1 and e2.
ID numbersare only relevant when comparing ID?s with thesame POS tag.3 Experiment: Cluster ID?s as more spcificfeaturesIn our first experiment, we compared two sys-tems of Surrounding Words, Open-Class Words, andIn-Between Terminal Tags, with and without In-Between Terminal Cluster ID?s.
The results areshown in Table 3.3.1 Results and AnalysisTable 3 shows the results of adding more spe-cific Cluster ID features to the more general POS-tag, Open-Class, and Surrounding-Words features.While this could have led to an over-fitted model, ap-parently it did not.
Overall precision increased from66.60% to 68.62%, an increase of 2.02%, yet recallalso increased, from 64.26% to 65.33%, an increaseof 1.07%.
The more precise, more closely-fittedfeatures did not harm performance, but actually en-hanced it.
The Maximum Entropy learner itself pre-ferred the Cluster ID features: Table 4 shows per-class POS-tag and Cluster-ID features with a lambdavalue over 0.25, comparing when both POS-tag fea-tures and Cluster ID tags are available, versus justPOS-tags (all among other features used in Experi-ment 1).
When given the opportunity, the MaxEntlearner considered the Cluster ID features more im-portant than the POS-tag features.As shown in Table 3, we can see that addingthe Cluster ID?s did mildly increase F-measure(by 1.41 %, from 65.01% to 66.42%3.
How-ever, when viewed on a class-by-class basis, someclasses show great improvement with the additionof Cluster ID?s while others remain unchanged.
Theclasses Cause-Effect, Component-Whole, Content-Container, Instrument-Agency, and Message-Topicall gained significantly with the addition of clusterID features.
We investigated important features ofthese classes more carefully.Classes that significantly improved with ClusterID?s:?
Cause-Effect: Cluster ID features that cor-related highly with Cause-Effect, besideskeyword-type single word clusters (from, that),were a cluster of certain occurances of theprepositions by, from, of, in; and a cluster ofcause-type verbs (shown in Table 5) plus thephrase by.?
Content-Container: Features positively corre-lated with Content-Container, besides somekeywords and phrases such as full of, was, in,and the/a, included a Cluster ID feature witha number of verbs commonly used to refer tocontainers and the processes of filling and emp-tying them, such as leaked, contained, poured,stuffed, took, injected, inserted, and found.
Theverbs from this feature are listed in Table 6.?
Instrument-Agency: Several Cluster ID fea-tures of verbs correlate with this class.
Al-though it is not as obvious as the verb list withCause-Effect, Table 7 compares several verbclusters that did have a noticeable positive cor-relation with Instrument-Agency with severalverb clusters of the same POS-tags that did notcorrelate.?
Component-Whole: Notable keyword and keyphrase features include of the/a/an, has a, andhas.
One Cluster ID feature is a cluster of third-person, possessive, and reflexive pronouns.
Al-3An F-measure of 66.42% would have put our system in themiddle of the pack on Task performance if it had participatedin the actual SemEval 2010 Task 8.
Task results for the entiredataset ranged from 82.18% F-m with a carefully-design knowl-edge database, to 52.16% with parse features, NE?s, and seman-tic seed lists, and 26.67% using punctuation, prepositional pat-terns, and context words.
Our goal, however, is to determinewhether recovered latent grammatical information is capable ofcontributing to relation classification at all.49Precision Recall F-measureClass no ID w/ ID diff no ID w/ ID diff no ID w/ ID diffCause-Effect 79.43 82.62 3.19 76.52 76.83 0.31 77.95 79.62 1.67Component-Whole 56.58 61.86 5.28 55.13 57.69 2.56 55.84 59.70 3.86Content-Container 74.37 77.04 2.67 77.08 78.65 1.57 75.70 77.84 2.14Entity-Destination 73.30 72.60 -0.70 88.36 88.01 -0.35 80.12 79.57 -0.55Entity-Origin 67.42 66.67 -0.75 69.77 69.77 0.00 68.57 68.18 -0.35Instrument-Agency 55.73 56.30 0.57 46.79 48.72 1.93 50.87 52.23 1.36Member-Collection 68.40 67.57 -0.83 73.39 75.11 1.72 70.81 71.14 0.33Message-Topic 65.95 70.47 4.52 46.74 52.11 5.37 54.71 59.91 5.20Product-Producer 58.19 62.50 4.31 44.59 41.13 -3.46 50.49 49.61 -0.88Other 30.97 28.65 -2.32 36.56 35.46 -1.10 33.54 31.69 -1.85Total, Macro-Avg 66.60 68.62 2.02 64.26 65.33 1.07 65.01 66.42 1.41Table 3: Comparison of Open-Class Words, Surrounding Words, and POS-tags, with and without Cluster ID features.Per SemEval2010 task standards, total does not include ?Other?.
Directionality is evaluated, but results are combinedfor viewability.
Bold-font differences are most notable.POS only POS &Cluster IDClass POS POS ID?sCause-Effect 5 0 6Component-Whole 4 1 6Content-Container 4 0 7Entity-Destination 4 0 4Entity-Origin 2 1 6Instrument-Agency 7 0 6Member-Collection 3 0 2Message-Topic 3 0 6Product-Producer 5 1 5Other 1 1 0Table 4: Number of POS-tag and Cluster ID featureswith a lambda value over 0.25, with and without Clus-ter ID features being available.
High lambda values areassigned when a classifier finds the features has a highpositive correlation with correct examples in the trainingdata.though it is a somewhat rare feature, when itoccurs it is positively-correlated.
An exampleof a Component-Whole pronoun is below:He stopped rowing when theboat was opposite to the paddlewheel of the steamer, and the<e1>steamer</e1> stopped her<e2>engine</e2> at the sametime.accompanied, affected, built, caused, completed,composed, contained, cooked, covered, created,derived, developed, discovered, distilled, driven,enclosed, fabricated, followed, founded, generated,given, known, led, made, manufactured, obtained,offered, produced, published, raised, represented,run, shared, supported, transmitted, triggered, used,wrapped, writtenTable 5: Contents of the VBN-12 Cluster that occurred 3or more times in the Relational Semantics corpus trainingdata.
Many of the verbs denote a cause-effect relation-ship.?
Message-Topic: Some helpful keyword fea-tures were in, to, and that.
A helpful Cluster IDfeature was a cluster of the prepositions about,over, upon, around, and between.
The modelalso ranked highly a Cluster ID feature contain-ing a number of ?discussion?
and ?document?verbs, shown in Table 8.Classes that did not significantly improve withCluster ID?s:?
Entity-Origin: This class is suspected to havebeen plagued by faulty annotation.
7 out ofthe first 24 training examples are incorrectlymarked as Entity-Origin, according to the cor-pus?s definitions.
This noise in the data likelyprevents effective comparison of features.
De-spite the noise, some clusters with a high cor-relation to the class include: a cluster of verbs50adjusted, applied, became, brought, built,caused, contained, created, described, did,established, examined, featured, followed,formed, found, gave, included, injected,inserted, introduced, involved, joined,leaked, made, marked, posted, poured,produced, released, reported, saw, sent,spotted, stuffed, took, used, was, were, won,wrote, wrapped, writtenTable 6: Contents of the VBD-5 Cluster that occurred 3or more times in the Relational Semantics corpus trainingdata.
A number of the verbs can refer to actions involvingcontainers and their contents.consisting of mostly made, kept, and left; anda cluster of verbs consisting mostly of made,left, kept, departed, arrived, travelled, and con-sisted.?
Entity-Destination: The Cluster ID featuresthat correlated highly with this class mark in-dividual key words and phrases: for the, on the,to the, and to.
Clusters of words were not help-ful for this class.?
Product-Producer: The Cluster ID featuresthat strongly correlated with this class con-sisted mostly of the words who/whom and by;individual key word features would have beenjust as good.
Several clusters of verbs werehighly correlated as well, but apparently therewas too much noise in the clusters for them tobe effective.?
Member-Collection: This class used the ?jj-24?
POS cluster, which contains the word otheramong other adjectives.
This is probably fromthe classic Member-Collection phrase ?Y andother X?s?.
However, this features, along with afeature mostly consisting of of, was not enoughto make much difference (0.33%) over POSfeatures.?
Other: The class Other decreased in F-measure with the addition of cluster ID fea-tures.
Combined with the overall F-measureincrease for all the regular classes, we inter-pret this decrease in F-measure as an increasein entropy, as more examples with identifiableuseful features are removed from the Other cat-egory, and the MaxEnt learner has fewer accu-rate patterns with which to cluster this diversegroup of examples.
In order words, we actuallydesire to see a decrease in Other F-measure, asthe examples in Other have almost nothing incommon with eachother and should be hard toidentify.Overall, some of the semantic relation classeswere correlated with features of syntactic clusters,and the clusters boosted scores, while other classesweren?t, and their scores remained roughly the same.The results of this experiment show that syntacticclusters did not lead to overtraining of data, and werehelpful with semantic relation classification.4 Experiment: Cluster ID?s as moregeneral featuresIn our second experiment, using the same experi-mental set-up but different features, we comparedIn-Between Words (IBW), IBW Plus POS-tags, andIBW Plus POS-tags Plus Cluster ID features.
Theresults are shown in Table 9.
The goal of this exper-iment is to compare Cluster ID features to an evenmore fine-grained feature, the words themselves.The words, POS-tags, and Cluster ID tags all con-cern the same nodes in the sentence.4.1 Results and AnalysisTable 9 shows the results of adding coarser-grainedCluster ID features to the more specific In-Between-Words features, as well as to the POS-tag features.The addition of Cluster ID features improved clas-sification over IBW plus POS-tags, as well as IBWalone.
While the previous experiment showed thatCluster ID features were not too specific to be help-ful, this experiment shows that they are also not toogeneral as to blur lexical patterns.
While overall F-measure increased 2.13% from IBW with the addi-tion of POS-tag features, from 63.35% to 65.48%,F-measure also increased further by the addition ofCluster ID features to IBW plus POS-tags, with a to-tal increase of 2.72% over IBW features alone, from63.35% to 66.07%.Table 10 breaks down results into Precision andRecall for the different groups of features.
Since thisexperiment was starting with a more precise base-51Cluster WordsInstrument-Agency Positively-correlated Clusters:VBD-7 approached, arrived, attached, bought, built, carried, caught, changed, chose, clicked, contained, covered,deposited, described, directed, donated, dragged, dropped, entered, erected, established, explained, fetched,fired, fled, gave, grabbed, hit, inserted, joined, kept, killed, knew, left, lived, lost, made, moved, noticed,observed, opened, organized, packed, passed, performed, placed, poured, prepared, presented, pressed,pulled, pushed, put, removed, rescheduled, saw, scaled, searched, sent, sold, spent, stirred, struck, stuffed,threw, took, tore, turned, used, was, wroteVBZ-9 applies, assists, brings, builds, changes, comprises, considers, contains, converts, covers, creates, cuts,describes, emits, encloses, enters, gets, hits, holds, joins, keeps, leaves, makes, needs, offers, plays, portrays,prepares, provides, removes, s, spreads, stirs, studies, teaches, uses, writesNon-positively-correlated Clusters:VBD-4 became, bought, carried, caused, completed, contained, created, developed, dug, filled, formed, got, had,held, issued, killed, made, presented, produced, reached, received, required, saw, showed, stopped, took,triggeredVBD-9 began, kept, started, stoppedVBD-8 continued, decided, had, happened, managed, needed, seemed, tried, used, wantedVBD-2 found, learned, noted, noticed, read, revealed, sawVBZ-8 arrives, brings, comes, comprises, consists, contains, contributes, copes, departs, extends, falls, feels, flows,focuses, goes, grows, hangs, leads, looks, moves, originates, passes, pulls, refers, relates, rests, results,returns, runs, s, sits, speaks, starts, stops, talks, travels, uprisesTable 7: Some positively-correlating and non-correlating verb clusters for Instrument-Agency.
Verbs occurred atleast 3 times in the Relational Semantics corpus training data.
Many verbs from positively-correlating Cluster IDfeatures may occur with mention of a tool or object to be used to carry out the action.attaches, builds, carries, causes, combines,comprises, contains, creates, describes, discusses,encloses, gives, holds, includes, keeps, makes,manipulates, means, needs, offers, performs,presents, processes, provides, represents,requires, s, shows, takes, wears, writesTable 8: Contents of the VBZ-11 Cluster that occurred 3or more times in the Relational Semantics corpus trainingdata.
Many of the verbs are associated with documents orspeaking.line, IBW features, and adding coarser grained fea-tures, POS-tags and Cluster IDs, we might expectto see a simultaneous decrease in precision and in-crease in recall from the baseline IBW to the en-hanced, POS-tag and Cluster ID versions.
As can beseen in Table 10, this is exactly what happens.
How-ever, Cluster ID features are found to be helpful tothe overall goal of semantic relation classification,because they increase recall by much more (4.44%)than they decrease precision (-0.44%).Classes for which the Cluster ID plusPOS-tag plus IBW combo was highest in-clude Content-Container, Entity-Destination,Member-Collection, Message-Topic, and Other.Component-Whole, Instrument-Agency, andProduct-Producer all showed gains over just IBW,but had lower scores than IBW plus just POS-tags.Only Cause-Effect and Entity-Origin failed toshow any improvement with POS-tags or ClusterID?s over the baseline IBW features.A comparison of features between Experiments1 and 2 showed that nearly all of the significantlyhelpful positive-corellated Cluster ID features (withlambda greater than 0.25) in Experiment 2 were alsoimportant in Experiment 1.
Some cluster ID featuresin Experiment 1 that isolated out a single word werereplaced in Experiment 2 by a more-accurate indi-vidual IBW word feature.5 ConclusionIn this paper, we presented a new method of se-mantic relation classification: using automatically-derived grammar rule clusters as a semantic knowl-edge source for relation classification.
We testedperformance of the feature on the SemEval 2010Relation Classification corpus, and found that it im-proved performance over both more coarse-grained52F-measureClass IBW +POS IBW+POS diff +ID IBW+ID diffCause-Effect 83.39 80.19 -3.20 81.55 -1.84Component-Whole 52.50 56.07 3.57 55.06 2.56Content-Container 73.79 73.27 -0.52 75.19 1.40Entity-Destination 77.98 80.06 2.08 81.49 3.51Entity-Origin 68.56 67.21 -1.35 67.33 -1.23Instrument-Agency 54.29 56.43 2.14 55.63 1.34Member-Collection 73.22 75.30 2.08 75.50 2.28Message-Topic 39.59 47.06 7.47 49.45 9.86Product-Producer 46.88 53.77 6.89 53.46 6.58Other 27.69 30.08 2.39 30.63 2.94Total, Macro-Avg 63.35 65.48 2.13 66.07 2.72Table 9: F-measure comparison of In-Between Words, IBW plus POS-tags, and IBW plus POS-tags plus ClusterID features.
Per SemEval2010 task standards, total does not include Other.
Bold-font differences are the highestimprovements (or baseline, whichever is higher).Analysis iBW +POS iBW+POSdiff+ID iBW+IDdiffPrecision 67.03 66.28 -0.75 66.59 -0.44Recall 62.10 66.12 4.02 66.54 4.44Table 10: Comparison of IBW, IBW plus POS-tags,and IBW plus POS-tags plus Cluster ID features.
PerSemEval2010 Task 8 standards, total does not includeOther.
Bold-font differences are the highest improve-ments (or baseline).and more fine-grained syntactic and collocationalfeatures in semantic relation classification.AcknowledgmentsThe author wishes to thank William Schuler andYannick Versley for their advice and support on thisproject.ReferencesYuan Chen , Man Lan , Jian Su , Zhi Min Zhou , Yu Xu2010.
ECNU: Effective Semantic Relations Classifi-cation without Complicated Features or Multiple Ex-ternal Corpora.
Proceedings of the 5th InternationalWorkshop on Semantic Evaluation.Iris Hendrickx , Su Nam Kim , Zornitsa Kozareva ,Preslav Nakov , Diarmuid O Seaghdha , SebastianPado, Marco Pennacchiotti , Lorenza Romano, StanSzpakowicz.
2010.
SemEval-2010 Task 8: Multi-WayClassification of Semantic Relations Between Pairs ofNominals.
Proceedings of the 5th International Work-shop on Semantic Evaluation.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Natu-ral Language Engineering, 7(3):207223.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, and Inter-pretable Tree Annotation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th Annual Meeting of the Association ofComputational Linguistics.Slav Petrov and Dan Klein.
2007.
Improved Inferencefor Unlexicalized Parsing.
Proceedings of NAACL2007.Bryan Rink and Sanda Harabagiu 2010.
UTD: Classi-fying Semantic Relations by Combining Lexical andSemantic Resources.
Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation.Gyorgy Szarvas and Iryna Gurevych 2010.
TUD: seman-tic relatedness for relation classification Proceedingsof the 5th International Workshop on Semantic Evalu-ation.Kateryna Tymoshenko and Claudio Giuliano.
2010.FBK-IRST: Semantic Relation Extraction using Cyc.Proceedings of the 5th International Workshop on Se-mantic Evaluation.Patrick Ye and Timothy Baldwin.
2007.
MELB-YB:Preposition Sense Disambiguation Using Rich Seman-tic Features.
Proceedings of SemEval 2007.53
