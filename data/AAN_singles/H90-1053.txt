Statistical Parsing of MessagesMahesh V. Chitrao and Ralph GrishmanDepartment of Computer ScienceCourant Institute of Mathematical ScienceNew York UniversityMessage ProcessingThe recent rend in natural anguage processing researchhas been to develop systems that deal with text concern-ing small, well defined domains.
One practical applica-tion for such systems is to process messages pertaining tosome very specific task or activity \[5\].
The advantage ofdealing with such domains is twofold - firstly, due to thenarrowness of the domain, it is possible to encode mostof the knowledge related to the domain and to make thisknowledge accessible to the natural anguage processingsystem, which in turn can use this knowledge to dis-ambiguate the meanings of the messages.
Secondly, insuch a domain, there is not a great diversity of languageconstructs and therefore it becomes easier to constructa grammar which will capture all the constructs whichexist in this sub-language.However, some practical aspects of such domains tendto make the problem somewhat difficult.
Often, the mes-sages tend not to be absolutely grammatically correct.As a result, the grammar designed for such a systemneeds to be far more forgiving than one designed forthe task of parsing edited English.
This can result ina proliferation of parses, which in turn makes the dis-ambiguation task more difficult.
This problem is furthercompounded by the telegraphic nature of the discourse,since telegraphic discourse is more prone to be syntacti-cally ambiguous.Statistical ParsingThe major objective of the research described in this pa-per is to use statistical data to evaluate the likelihoodof a parse in order to help the parser prune out un-likely parses.
Our conjecture - supported by our resultsand some prior, similar experiments - is that a moreprobable parse has a greater chance of being the correctone.
The related work by the research team at UCREL(Unit for Computer Research on the English Language)at Lancaster University \[4\] and at IBM Research Lab.
atYorktown Heights \[3\] addressed the same problem in thecontext of unconstrained text.
The success rate of about50% achieved by the probabilistic parser at UCREL iscertainly impressive \[4\], considering the fact that theircorpus included a range of domains and text types.
How-263ever, our objectives differ from that of UCREL in thatour aim is to build a system for a specific applicationtask (viz.
information extraction).
For this reason, wesacrifice breadth of coverage for higher reliability.We use a model of probabilistic sentence generationbased on a context-free grammar with independent prob-abilities for the expansion of any non-terminal by its al-ternate productions.
These probabilities are adjustedto generate sentences in which the frequency of particu-lar constructs corresponds to that observed in a samplecorpus.
The resulting grammar gives us not only theprobabilities of sentences, but also the probabilities ofalternate parses for a single sentence.
It is the latterprobabilities which are of primary interest o us in ana-lyzing new sentences.Initially, probabilities of all individual productionswere determined using a corpus of text.
This body oftext was parsed using the NYU PROTEUS grammarwhich is a subset of Sager's grammar\[10\] based on Lin-guistic String Theory \[7\].
The parsing was performedusing a chart parser \[8\].
The probabilities of the produc-tions were computed by an iterative procedure describedbelow.The parser was then modified to use these probabili-ties.
The probability of a parse tree is computed as theproduct of probabilities of all the productions used inthat particular parse tree.
The parsing algorithm gener-ates parses in the best-first order \[9\] (i.e.
most probableparse first).The prioritizing of productions not only helps in thedisambiguation of parses but also forces the parser totry more likely productions first.
As a result, there is atwofold improvement attributable to this modified pars-ing algorithm.
Firstly, the accuracy of the parsing im-proves since the parser provides a mechanism to iden-tify the most likely parse.
Secondly, the tendency ofthe parser to try more likely productions first results ina quicker conclusion of the search process and a fasterparser 1.To further enhance the performance of the system,probabilities of individual productions were computedseparately for each of the contexts in which they could1 Speed of the parser can be approximately assessed from thenumber of edges generated.
An edge is an intermediate hypothesismade by the parser \[8\]be invoked and these probabilities were used to computeprobabilities of parse trees.
This modification improvedthe speed and accuracy of the system still further.This paper briefly describes the methodology underly-ing statistical parsing and corroborates through experi-mental results the claim that parsing guided with statis-tical information is more robust and faster.Est imation of Probabil it ies ofProduct ionsIn this experiment, a corpus of 105 Navy OPREP mes-sages (prepared for Message Understanding Conference -2 \[11\]) was parsed using the PROTEUS grammar.
Parsetrees of each sentence were accumulated separately.
2The algorithm for estimation of probabilites of produc-tions resembles the Inside Outside Algorithm \[1\].
Theinitial estimate of the probability of 7~,j, the jth parsetree of the i th sentence is1Pro(7~j) = ~isake of enabling "best first parsing", the agenda was con-verted into a priority queue (or a heap), where the prior-ity of each incomplete parse was computed by adding thepriorities of all constituent edges.
This computation wasperformed in a recursive manner, since edges themselvesare incomplete parses.
Moreover, whenever an edge useda certain production, the corresponding priority (givenby (3)) is also added to the priority of that edge.The objective of the modified parsing algorithm is togenerate parses in the most-likely-first order.
The prior-ity of a parse tree (finished or unfinished) is asserted tobe the product of the probabilities of all the productionsused in it.
For a grammar with a decision tree of averagebranching factor 5, a parse tree that uses 20 different pro-ductions will have a priority of the order (0.2) 2?
which isa small floating point number.
In order to avoid dealingwith such small numbers, probabilities are converted tolog scale.
This has the added advantage of being able toadd probabilites instead of having to multiply them.
WeassignPriority(P,~,n) = 10.0 x log(Pr(Pm,,)) (3)where Ni is the number of parses obtained for the i thsentence.
We further define the current partial count ofthe n th production of the mth nonterminal attributableto T/,j to beC0(P,~,,,7~,/) = Pro(Ti,j) x C(Pm,,,Ti,j) (1)where C(Pm,,, 7~,j) is the actual count of the numberof times this production is used in Ti,j.
From this, anestimate of the probability of production Pro,, can befound asThis way, whenever a new edge is added to the parsetree, its priority is added to the priority of the parsetree.Even though this modification in the parsing algo-rithm resulted in significant improvement in the accu-racy and the speed of parsing, it was felt that the per-formance of the parsing could be further enhanced by ex-tracting more specific information from the corpus.
Thefollowing section describes one such scheme.Pro(Pm,, Ira) = ~TC?
(Pm'n'T) (2)E. ET Pm,., T)Using these estimates, it is possible to come up with anew estima~ of Pr(Ti, j)  for each tree by multiplying theprobabilities of all the productions used in that deriva-tion.
This enables us to reestimate C(Pm,n,T/ j) ,  whichin turn allow us to reestimate Pr(Pm,, I m).
This mech-anism leads to an iterative process.
After convergence,this iterative process gives us estimates of the probabil-ities of each production in the grammar.Mod i f ied  Pars ing  A lgor i thmThe parser which PROTEUS uses is a chart parser.
Thisparser maintains a list of incomplete parses, which iscMled agenda.
In the earlier parsing algorithm, partiMparses were chosen in LIFO order for expansion 3.
For the2The PROTEUS grammar used for this task included a numberof ad-hoc scoring constraints to prefer full sentence analyses tofragments and run-ons.
In accumulating parse trees, we only tookthe top-scoring trees for each sentence.
For subsequent iterations,we used only these parse trees with new weights assigned; we didnot reparse the corpus to obtain additional trees.3 This simply means that search space was searched in a depth-first order; there is no pecticular eason for this choiceFine Grained StatisticsIn the above scheme, the amount of priority due to aproduction depends only on the production and not onthe place where the production is used.
For example,use of the productionSA "--, NULLwill involve a fixed amount of priority no matter wherethis SA appears.
This strategy disregards the fact thatSA appearing at certain locations may have a greaterprobability to use this production and SA appearing atsome other place may have a lesser probability to usethis production.
Therefore, it was expected that if theprobabilities for a certain non-terminal using a certainproduction were computed separately for each instanceof each nonterminal ppearing on the right hand side of aproduction, it will lead to a more informed parser.
Suchprobabilities were obtained and the parsing algorithmwas modified accordingly.
This change resulted in aneven better perfomance, both with respect o speed andaccuracy.
The following section briefly describes the fur-ther changes necessary in the parsing algorithm in orderto be able to use fine-grained statistical information.264Parsing Method Correct Misplaced Prepositional IncorrectPhraseNo statistical parsingContext free statistical ParsingContext free statistical parsingwith heuristic penaltiesContext sensitive statistical parsingContext sensitive statistical parsingwith heuristic penalites34474253454546555158Edges61 57726547 55302243 4834323637501357419772Table h Comparison of correctness of parsing with various parsing methodsModified Parsing Algorithm for FineGrained StatisticsThe parsing algorithm for parsing with fine-grained pri-orities becomes somewhat more complex.
In this scheme,the priority attributable to a production is influenced bythe context in which it is used.
Since the priority of anedge is the sum of the priority of the production usedby the edge and priorities of its constituents, the overallpriority of an edge varies according to the context.
Thiseffect can be best captured by breaking up the priority ofan edge into two components - the absolute componentand the relative component.
While the absolute com-ponent for an edge is constant, the relative componentdepends on the context in which the priority of this edgeis being computed, since priorities of the same produc-tion are different in different contexts.
These changesare discussed in greater detail in \[2\]Heuristic PenaltiesEven with the improvements detailed in the two priorsections, the parser suffered from a serious problem.
Themodified search algorithm, which is best first, ignoredthe length of hypotheses being compared.
As a result, asthe parsing progressed towards the right of the sentence,it would often thrash after the lengthier (and possiblycorrec 0 partial parses gathered more penalty than short,unpromising hypotheses near the left of the sentence.
Inorder to have a strictly best-first search strategy, thisphenomenon is unavoidable 4.
However, we studied thetrade-off that existed by penalizing shorter hypothesesby a fixed amount per word 5.
This way, the ~hrashmgwas avoided, the parser became faster and the parsercould parse several sentences which could not be parsedearlier under the same edge limit 6.
However, the parserlost the characteristic ofbeing best first and this resultedin a slight degradation of accuracy.4 The best first search expects an  admissible heurist ic,  which bydefinition, always underest imates  the residual penalty.
We exper-imented with some admissible heuristics, but  they were too weakto  improve performance significantly.5This idea is due to Sallm Roukos (personal communicat ion)6Usually, a l imit is imposed on the number  of edges the parsercan generate before abandoning the search for the parse.
Thisoccasionally results in a s i tuat ion where the parser does not findan  exist ing correct parseResultsThe statistical data was obtained from a corpus of about300 parsed sentences.
To assess the performance of var-ious parsing algorithms, each version of the modifiedparser and grammar was run on a sample of 140 sen-tences, which were selected from the training corpus.For verification purposes, the correct regularized syntac-tic structures were manually garnered for each of thesesentences.
These were compared with the top-ranked(highest probability) parse produced by each of the var-ious parsing schemes.
The results are summarized intable 1.Because the single most frequent source of error ina parse was a misplaced prepositional phrase, we sepa-rated out this parsing error from other errors.
The tableshows the number of sentences parsed correctly, the num-ber that had one or two misplaced prepositional phrases,and the number with other errors, for each parsing strat-egy.
It also shows the total number of edges built by theparser for all 140 sentences.
Since the number of edgesroughly represents the computational complexity of theparsing algorithm, it can be treated as an approximatemeasure of time spent.The following conclusions can be drawn about the per-formance of the parser that uses the statistical informa-tion:1.
Parsing guided by statistics does better in speed aswell as accuracy in comparison with the earlier al-gorithm.2.
The fine-grained statistics does better than ordinarystatistics both in speed and accuracy.3.
The use of heuristic penalties improves the perfor-mance with respect o speed and the number of sen-tences parsed but accuracy suffers.AcknowledgementThis research was supported in part by the DefenseAdvanced Research Projects Agency under contractN00014-K-85-0163 and grant N00014-90-J-1851 from theOffice of Naval Research.265References[1] J. K. Baker, 1979, Trainable Grammars for SpeechRecognition, Speech Communication Papers for the97th Meeting of the Acoustic Society of America,D.H.
Klatt and J.J. Wolf (eds).
[2] Mahesh Chitrao, Forthcoming, Statistical.
Parsingof Messages, Ph.D. dissertation, New York Univer-sity, New York.
[3] Tetsunosuke Fujisaki, 1984, A Stochastic Approachto Sentence Parsing, in Proceedings of the 10th In-ternational Conference on Computational Linguis-tics and 22nd Annual Meeting of ACL.
[4] Roger Garside, Geoffrey Leech and Geoffry Samp-son, 1987, The Computational Analysis of English,Longman, London, UK.
[5] Ralph Grishman and John Sterling, 1989, Prefer-ence Semantics for Message Understanding, Pro-ceedings of DARPA Speech and Natural LanguageWorkshop, I-Iarwich Port, MA.
[6] Ralph Grishman, 1986, Computational Linguistics,An Introduction, Cambridge University Press, Cam-bridge, UK.
[7] Z. Harris, 1962, String Analysis of Sentence Struc-ture, Mouton & Co., The Hague.
[8] M. Kay, 1967, Experiments with a Powerful Parser,Proceedings of the Second International Conferenceon Computational Linguistics, Grenoble.
[9] Nils Nilsson, 1981, Principles of Artificial Intelli-gence, Tioga Publishing Company, Palo Alto, CA.
[10] Naomi Sager, 1981, Natural Language InformationProcessing, Addison-Wesley, Reading, MA.
[11] Beth Sundheim, 1989, Navy Tactical Incident Re-porting in a Highly Constrained Sublanguage: Ex-amples and Analysis.
Naval Ocean Systems CenterTechnical Document 1477.266
