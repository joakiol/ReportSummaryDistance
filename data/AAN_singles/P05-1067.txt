Proceedings of the 43rd Annual Meeting of the ACL, pages 541?548,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsMachine Translation Using ProbabilisticSynchronous Dependency Insertion GrammarsYuan Ding Martha PalmerDepartment of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104, USA{yding, mpalmer}@linc.cis.upenn.eduAbstractSyntax-based statistical machine transla-tion (MT) aims at applying statisticalmodels to structured data.
In this paper,we present a syntax-based statistical ma-chine translation system based on a prob-abilistic synchronous dependencyinsertion grammar.
Synchronous depend-ency insertion grammars are a version ofsynchronous grammars defined on de-pendency trees.
We first introduce ourapproach to inducing such a grammarfrom parallel corpora.
Second, we de-scribe the graphical model for the ma-chine translation task, which can also beviewed as a stochastic tree-to-tree trans-ducer.
We introduce a polynomial timedecoding algorithm for the model.
Weevaluate the outputs of our MT system us-ing the NIST and Bleu automatic MTevaluation software.
The result shows thatour system outperforms the baseline sys-tem based on the IBM models in bothtranslation speed and quality.1 IntroductionStatistical approaches to machine translation, pio-neered by (Brown et al, 1993), achieved impres-sive performance by leveraging large amounts ofparallel corpora.
Such approaches, which are es-sentially stochastic string-to-string transducers, donot explicitly model natural language syntax orsemantics.
In reality, pure statistical systems some-times suffer from ungrammatical outputs, whichare understandable at the phrasal level but some-times hard to comprehend as a coherent sentence.In recent years, syntax-based statistical machinetranslation, which aims at applying statistical mod-els to structural data, has begun to emerge.
Withthe research advances in natural language parsing,especially the broad-coverage parsers trained fromtreebanks, for example (Collins, 1999), the utiliza-tion of structural analysis of different languageshas been made possible.
Ideally, by combining thenatural language syntax and machine learningmethods, a broad-coverage and linguistically well-motivated statistical MT system can be constructed.However, structural divergences between lan-guages (Dorr, 1994)?which are due to either sys-tematic differences between languages or loosetranslations in real corpora?pose a major chal-lenge to syntax-based statistical MT.
As a result,the syntax based MT systems have to transducebetween non-isomorphic tree structures.
(Wu, 1997) introduced a polynomial-time solu-tion for the alignment problem based on synchro-nous binary trees.
(Alshawi et al, 2000) representseach production in parallel dependency trees as afinite-state transducer.
Both approaches learn thetree representations directly from parallel sen-tences, and do not make allowances for non-isomorphic structures.
(Yamada and Knight, 2001,2002) modeled translation as a sequence of treeoperations transforming a syntactic tree into astring of the target language.When researchers try to use syntax trees in bothlanguages, the problem of non-isomorphism mustbe addressed.
In theory, stochastic tree transducersand some versions of synchronous grammars pro-vide solutions for the non-isomorphic tree basedtransduction problem and hence possible solutionsfor MT.
Synchronous Tree Adjoining Grammars,proposed by (Shieber and Schabes, 1990), wereintroduced primarily for semantics but were lateralso proposed for translation.
Eisner (2003) pro-posed viewing the MT problem as a probabilisticsynchronous tree substitution grammar parsing541problem.
Melamed (2003, 2004) formalized theMT problem as synchronous parsing based onmultitext grammars.
Graehl and Knight (2004) de-fined training and decoding algorithms for bothgeneralized tree-to-tree and tree-to-string transduc-ers.
All these approaches, though different in for-malism, model the two languages using tree-basedtransduction rules or a synchronous grammar, pos-sibly probabilistic, and using multi-lemma elemen-tary structures as atomic units.
The machinetranslation is done either as a stochastic tree-to-treetransduction or a synchronous parsing process.However, few of the above mentioned formal-isms have large scale implementations.
And to thebest of our knowledge, the advantages of syntaxbased statistical MT systems over pure statisticalMT systems have yet to be empirically verified.We believe difficulties in inducing a synchro-nous grammar or a set of tree transduction rulesfrom large scale parallel corpora are caused by:1.
The abilities of synchronous grammars andtree transducers to handle non-isomorphismare limited.
At some level, a synchronousderivation process must exist between thesource and target language sentences.2.
The training and/or induction of a synchro-nous grammar or a set of transduction rulesare usually computationally expensive if allthe possible operations and elementary struc-tures are allowed.
The exhaustive search forall the possible sub-sentential structures in asyntax tree of a sentence is NP-complete.3.
The problem is aggravated by the non-perfecttraining corpora.
Loose translations are less ofa problem for string based approaches than forapproaches that require syntactic analysis.Hajic et al (2002) limited non-isomorphism byn-to-m matching of nodes in the two trees.
How-ever, even after extending this model by allowingcloning operations on subtrees, Gildea (2003)found that parallel trees over-constrained thealignment problem, and achieved better resultswith a tree-to-string model than with a tree-to-treemodel using two trees.
In a different approach,Hwa et al (2002) aligned the parallel sentencesusing phrase based statistical MT models and thenprojected the alignments back to the parse trees.This motivated us to look for a more efficientand effective way to induce a synchronous gram-mar from parallel corpora and to build an MT sys-tem that performs competitively with the purestatistical MT systems.
We chose to build the syn-chronous grammar on the parallel dependencystructures of the sentences.
The synchronousgrammar is induced by hierarchical tree partition-ing operations.
The rest of this paper describes thesystem details as follows: Sections 2 and 3 de-scribe the motivation behind the usage of depend-ency structures and how a version of synchronousdependency grammar is learned.
This grammar isused as the primary translation knowledge sourcefor our system.
Section 4 defines the tree-to-treetransducer and the graphical model for the stochas-tic tree-to-tree transduction process and introducesa polynomial time decoding algorithm for thetransducer.
We evaluate our system in section 5with the NIST/Bleu automatic MT evaluationsoftware and the results are discussed in Section 6.2 The Synchronous Grammar2.1 Why Dependency Structures?According to Fox (2002), dependency representa-tions have the best inter-lingual phrasal cohesionproperties.
The percentage for head crossings is12.62% and that of modifier crossings is 9.22%.Furthermore, a grammar based on dependencystructures has the advantage of being simple informalism yet having CFG equivalent formal gen-erative capacity (Ding and Palmer, 2004b).Dependency structures are inherently lexical-ized as each node is one word.
In comparison,phrasal structures (treebank style trees) have twonode types: terminals store the lexical items andnon-terminals store word order and phrasal scopes.2.2 Synchronous Dependency Insertion GrammarsDing and Palmer (2004b) described one version ofsynchronous grammar: Synchronous DependencyInsertion Grammars.
A Dependency InsertionGrammars (DIG) is a generative grammar formal-ism that captures word order phenomena within thedependency representation.
In the scenario of twolanguages, the two sentences in the source and tar-get languages can be modeled as being generatedfrom a synchronous derivation process.A synchronous derivation process for the twosyntactic structures of both languages suggests thelevel of cross-lingual isomorphism between thetwo trees (e.g.
Synchronous Tree AdjoiningGrammars (Shieber and Schabes, 1990)).542Apart from other details, a DIG can be viewedas a tree substitution grammar defined on depend-ency trees (as opposed to phrasal structure trees).The basic units of the grammar are elementarytrees (ET), which are sub-sentential dependencystructures containing one or more lexical items.The synchronous version, SDIG, assumes that theisomorphism of the two syntactic structures is atthe ET level, rather than at the word level, henceallowing non-isomorphic tree to tree mapping.We illustrate how the SDIG works using thefollowing pseudo-translation example:y [Source] The girl kissed her kitty cat.y [Target] The girl gave a kiss to her cat.Figure 1.An exampleFigure 2.Tree-to-treetransductionAlmost any tree-transduction operations de-fined on a single node will fail to generate the tar-get sentence from the source sentence withoutusing insertion/deletion operations.
However, if weview each dependency tree as an assembly of indi-visible sub-sentential elementary trees (ETs), wecan find a proper way to transduce the input tree tothe output tree.
An ET is a single ?symbol?
in atransducer?s language.
As shown in Figure 2, eachcircle stands for an ET and thick arrows denote thetransduction of each ET as a single symbol.3 Inducing a Synchronous DependencyInsertion GrammarAs the start to our syntax-based SMT system, theSDIG must be learned from the parallel corpora.3.1 Cross-lingual Dependency InconsistenciesOne straightforward way to induce a generativegrammar is using EM style estimation on the gen-erative process.
Different versions of such trainingalgorithms can be found in (Hajic et al, 2002; Eis-ner 2003; Gildea 2003; Graehl and Knight 2004).However, a synchronous derivation processcannot handle two types of cross-language map-pings: crossing-dependencies (parent-descendentswitch) and broken dependencies (descendent ap-pears elsewhere), which are illustrated below:Figure 3.
Cross-lingual dependency consistenciesIn the above graph, the two sides are Englishand the foreign dependency trees.
Each node in atree stands for a lemma in a dependency tree.
Thearrows denote aligned nodes and those resultinginconsistent dependencies are marked with a ?
*?.Fox (2002) collected the statistics mainly onFrench and English data: in dependency represen-tations, the percentage of head crossings perchance (case [b] in the graph) is 12.62%.Using the statistics on cross-lingual dependencyconsistencies from a small word to word alignedChinese-English parallel corpus1, we found that thepercentage of crossing-dependencies (case [b])between Chinese and English is 4.7% while that ofbroken dependencies (case [c]) is 59.3%.The large number of broken dependencies pre-sents a major challenge for grammar inductionbased on a top-down style EM learning process.Such broken and crossing dependencies can bemodeled by SDIG if they appear inside a pair ofelementary trees.
However, if they appear betweenthe elementary trees, they are not compatible withthe isomorphism assumption on which SDIG isbased.
Nevertheless, the hope is that the fact thatthe training corpus contains a significant percent-age of dependency inconsistencies does not meanthat during decoding the target language sentencecannot be written in a dependency consistent way.3.2 Grammar Induction by SynchronousHierarchical Tree Partitioning(Ding and Palmer, 2004a) gave a polynomial timesolution for learning parallel sub-sentential de-1  Total 826 sentence pairs, 9957 Chinese words, 12660 Eng-lish words.
Data made available by the courtesy of MicrosoftResearch, Asia and IBM T.J. Watson Research.543pendency structures from non-isomorphic depend-ency trees.
Our approach, while similar to (Dingand Palmer, 2004a) in that we also iteratively parti-tion the parallel dependency trees based on a heu-ristic function, departs (Ding and Palmer, 2004a)in three ways: (1) we base the hierarchical tree par-titioning operations on the categories of the de-pendency trees; (2) the statistics of the resultanttree pairs from the partitioning operation are col-lected at each iteration rather than at the end of thealgorithm; (3) we do not re-train the word to wordprobabilities at each iteration.
Our grammar induc-tion algorithm is sketched below:Step 0.
View each tree as a ?bag of words?
and train astatistical translation model on all the tree pairs toacquire word-to-word translation probabilities.
Inour implementation, the IBM Model 1 (Brown etal., 1993) is used.Step 1.
Let i  denote the current iteration and let[ ]C CategorySequence i=  be the current syntac-tic category set.For each tree pair in the corpus, do {a) For the tentative synchronous partitioning opera-tion, use a heuristic function to select the BEST wordpair * *( , )i je f , where both * *,i je f  are NOT ?chosen?,*( )iCategory e C?
and *( )jCategory f C?
.b) If * *( , )i je f  is found in (a), mark * *,i je f  as ?cho-sen?
and go back to (a), else go to (c).c) Execute the synchronous tree partitioning opera-tion on all the ?chosen?
word pairs on the tree pair.Hence, several new tree pairs are created.
Replace theold tree pair with the new tree pairs together with therest of the old tree pair.d) Collect the statistics for all the new tree pairs aselementary tree pairs.
}Step 2.
1i i= + .
Go to Step 1 for the next iteration.At each iteration, one specific set of categoriesof nodes is handled.
The category sequence weused in the grammar induction is:1.
Top-NP: the noun phrases that do not haveanother noun phrase as parent or ancestor.2.
NP: all the noun phrases3.
VP, IP, S, SBAR:  verb phrases equivalents.4.
PP, ADJP, ADVP, JJ, RB: all the modifiers5.
CD: all the numbers.We first process top NP chunks because they arethe most stable between languages.
Interestingly,NPs are also used as anchor points to learn mono-lingual paraphrases (Ibrahim et al, 2003).
Thephrasal structure categories can be extracted fromautomatic parsers using methods in (Xia, 2001).An illustration is given below (Chinese in pin-yin form).
The placement of the dependency arcsreflects the relative word order between a parentnode and all its immediate children.
The collectedETs are put into square boxes and the partitioningoperations taken are marked with dotted arrows.y [English]   I have been in Canada since 1947.y [Chinese]  Wo 1947 nian yilai  yizhi   zhu  zai  jianada.y [Glossary]  I   1947 year since always live in  Canada[ ITERATION 1 & 2 ] Partition at word pair(?I?
and ?wo?)
(?Canada?
and ?janada?
)[ ITERATION 3 ] (?been?
and ?zhu?)
are chosen but nopartition operation is taken because they are roots.
[ ITERATION 4 ] Partition at word pair(?since?
and ?yilai?)
(?in?
and ?zai?
)[ ITERATION 5 ] Partition at ?1947?
and ?1947?
[ FINALLY ] Total of 6 resultant ET pairs (figure omitted)Figure 4.
An Example3.3 HeuristicsSimilar to (Ding and Palmer, 2004a), we also use aheuristic function in Step 1(a) of the algorithm torank all the word pairs for the tentative tree parti-544tioning operation.
The heuristic function is basedon a set of heuristics, most of which are similar tothose in (Ding and Palmer, 2004a).For a word pair ( , )i je f for the tentative parti-tioning operation, we briefly describe the heuristics:y Inside-outside probabilities: We borrow theidea from PCFG parsing.
This is the probabil-ity of an English subtree (inside) generating aforeign subtree and the probability of the Eng-lish residual tree (outside) generating a for-eign residual tree.
Here both probabilities arebased on a ?bag of words?
model.y Inside-outside penalties: here the probabilitiesof the inside English subtree generating theoutside foreign residual tree and outside Eng-lish residual tree generating the inside Englishsubtree are used as penalty terms.y Entropy: the entropy of the word to wordtranslation probability of the English word ie .y Part-of-Speech mapping template: whether thePOS tags of the two words are in the ?highlylikely to match?
POS tag pairs.y Word translation probability: P( | )j if e .y Rank: the rank of the word to word probabil-ity of jf  in as a translation of ie  among allthe foreign words in the current tree.The above heuristics are a set of real valuednumbers.
We use a Maximum Entropy model tointerpolate the heuristics in a log-linear fashion,which is different from the error minimizationtraining in (Ding and Palmer, 2004a).
( )0 1P | ( , ), ( , )... ( , )1 exp ( , )i j i j n i jk k i j sky h e f h e f h e fh e fZ?
??
?= +?
??
??
(1)where (0,1)y =  as labeled in the training datawhether the two words are mapped with each other.The MaxEnt model is trained using the sameword level aligned parallel corpus as the one inSection 3.1.
Although the training corpus isn?tlarge, the fact that we only have a handful of pa-rameters to fit eased the problem.3.4 A Scaled-down SDIGIt is worth noting that the set of derived paralleldependency Elementary Trees is not a full-fledgedSDIG yet.
Many features in the SDIG formalismsuch as arguments, head percolation, etc.
are notyet filled.
We nevertheless use this derived gram-mar as a Mini-SDIG, assuming the unfilled fea-tures as empty by default.
A full-fledged SDIGremains a goal for future research.4 The Machine Translation System4.1 System ArchitectureAs discussed before (see Figure 1 and 2), the archi-tecture of our syntax based statistical MT system isillustrated in Figure 5.
Note that this is a non-deterministic process.
The input sentence is firstparsed using an automatic parser and a dependencytree is derived.
The rest of the pipeline can beviewed as a stochastic tree transducer.
The MTdecoding starts first by decomposing the input de-pendency tree in to elementary trees.
Several dif-ferent results of the decomposition are possible.Each decomposition is indeed a derivation processon the foreign side of SDIG.
Then the elementarytrees go through a transfer phase and target ETs arecombined together into the output.Figure 5.
System architecture4.2 The Graphical ModelThe stochastic tree-to-tree transducer we proposemodels MT as a probabilistic optimization process.Let f  be the input sentence (foreign language),and e  be the output sentence (English).
We haveP( | ) P( )P( | )P( )f e ee ff= , and the best translation is:* arg max P( | )P( )ee f e e=    (2)P( | )f e  and P( )e  are also known as the ?trans-lation model?
(TM) and the ?language model?(LM).
Assuming the decomposition of the foreigntree is given, our approach, which is based on ETs,uses the graphical model shown in Figure 6.In the model, the left side is the input depend-ency tree (foreign language) and the right side isthe output dependency tree (English).
Each circlestands for an ET.
The solid lines denote the syntac-tical dependencies while the dashed arrows denotethe statistical dependencies.545Figure 6The graphicalmodelLet T( )x be the dependency tree constructedfrom sentence x .
A tree-decomposition functionD( )t  is defined on a dependency tree t , and out-puts a certain ET derivation tree of  t , which isgenerated by decomposing t  into ETs.
Given t ,there could be multiple decompositions.
Condi-tioned on decomposition D , we can rewrite (2) as:* arg max P( , | )P( )arg max P( | , )P( | )P( )e De De f e D Df e D e D D==??
(3)By definition, the ET derivation trees of the in-put and output trees should be isomorphic:D(T( )) D(T( ))f e?
.
Let Tran( )u  be a set of possi-ble translations for the ET u .
We have:D(T( )), D(T( )), Tran( )P( | , ) P(T( ) | P(T( ), )P( | )u f v e v uf e D f e Du v?
?
?== ?
(4)For any ET v  in a given ET derivation tree d ,let Root( )d  be the root ET of d , and letParent( )v  denote the parent ET of  v .
We have:( )( )D(T( )), Root(D(T( ))P( | ) P(T( ) | )P Root D(T( )P( | Parent( ))v e v ee D e Dev v?
?== ??
???
??
??
(5)where, letting root( )v  denote the root word of v ,( ) ( )( )P | Parent( ) P root( ) | root Parent( )v v v v=  (6)The prior probability of tree decomposition isdefined as: ( )D(T( ))P D(T( )) P( )u ff u?= ?
(7)Figure 7Comparing tothe HMMAn analogy between our model and a HiddenMarkov Model (Figure 7) may be helpful.
In Eq.
(4), P( | )u v  is analogous to the emission probablyP( | )i io s  in an HMM.
In Eq.
(5), P( | Parent( ))v v  isanalogous to the transition probability 1P( | )i is s ?
inan HMM.
While HMM is defined on a sequenceour model is defined on the derivation tree of ETs.4.3 Other Factorsy Augmenting parallel ET pairsIn reality, the learned parallel ETs are unlikely tocover all the structures that we may encounter indecoding.
As a unified approach, we augment theSDIG by adding all the possible word pairs ( , )j if eas a parallel ET pair and using the IBM Model 1(Brown et al, 1993) word to word translationprobability as the ET translation probability.y Smoothing the ET translation probabilities.The LM probabilities P( | Parent( ))v v  are simplyestimated using the relative frequencies.
In order tohandle possible noise from the ET pair learningprocess, the ET translation probabilities P ( | )emp u vestimated by relative frequencies are smoothedusing a word level model.
For each ET pair ( , )u v ,we interpolate the empirical probability with the?bag of words?
probability and then re-normalize:size( )1 1P( | ) P ( , ) P( | )size( )ijemp j ive vf uu v u v f eZ u ?
?= ?
??
(8)4.4 Polynomial Time DecodingFor efficiency reasons, we use maximum approxi-mation for (3).
Instead of summing over all thepossible decompositions, we only search for thebest decomposition as follows:,*, * arg max P( | , )P( | )P( )e De D f e D e D D=  (9)So bringing equations (4) to (9) together, thebest translation would maximize:( )P( | ) P Root( ) P( | Parent( )) P( )u v e v v u?
??
?
??
??
??
?
?
(10)Observing the similarity between our modeland a HMM, our dynamic programming decodingalgorithm is in spirit similar to the Viterbi algo-rithm except that instead of being sequential thedecoding is done on trees in a top down fashion.As to the relative orders of the ETs, we cur-rently choose not to reorder the children ETs giventhe parent ET because: (1) the permutation of theETs is computationally expensive (2) it is possiblethat we can resort to simple linguistic treatmentson the output dependency tree to order the ETs.Currently, all the ETs are attached to each other546at their root nodes.In our implementation, the different decomposi-tions of the input dependency tree are stored in ashared forest structure, utilizing the dynamic pro-gramming property of the tree structures explicitly.Suppose the input sentence has n  words andthe shared forest representation has m  nodes.Suppose for each word, there are maximally kdifferent ETs containing it, we have knm ?
.
Letb  be the max breadth factor in the packed forest, itcan be shown that the decoder visits at most mbnodes during execution.
Hence, we have:)()( kbnOdecodingT ?
(11)which is linear to the input size.
Combined with apolynomial time parsing algorithm, the wholedecoding process is polynomial time.5 EvaluationWe implemented the above approach for a Chi-nese-English machine translation system.
We usedan automatic syntactic parser (Bikel, 2002) to pro-duce the parallel parse trees.
The parser wastrained using the Penn English/Chinese Treebanks.We then used the algorithm in (Xia 2001) to con-vert the phrasal structure trees to dependency treesto acquire the parallel dependency trees.
The statis-tics of the datasets we used are shown as follows:Dataset Xinhua FBIS NISTSentence# 56263 45212 206Chinese word# 1456495 1185297 27.4 averageEnglish word# 1490498 1611932 37.7 averageUsage training training testingFigure 8.
Evaluation data detailsThe training set consists of Xinhua newswiredata from LDC and the FBIS data (mostly news),both filtered to ensure parallel sentence pair quality.We used the development test data from the 2001NIST MT evaluation workshop as our test data forthe MT system performance.
In the testing data,each input Chinese sentence has 4 English transla-tions as references.
Our MT system was evaluatedusing the n-gram based Bleu (Papineni et al, 2002)and NIST machine translation evaluation software.We used the NIST software package ?mteval?
ver-sion 11a, configured as case-insensitive.In comparison, we deployed the GIZA++ MTmodeling tool kit, which is an implementation ofthe IBM Models 1 to 4 (Brown et al, 1993; Al-Onaizan et al, 1999; Och and Ney, 2003).
TheIBM models were trained on the same training dataas our system.
We used the ISI Rewrite decoder(Germann et al 2001) to decode the IBM models.The results are shown in Figure 9.
The scoretypes ?I?
and ?C?
stand for individual and cumula-tive n-gram scores.
The final NIST and Bleu scoresare marked with bold fonts.Systems Score Type 1-gram 2-gram 3-gram 4-gramNIST 2.562 0.412 0.051 0.008I Bleu 0.714 0.267 0.099 0.040NIST 2.562 2.974 3.025 3.034IBMModel 4 C Bleu 0.470 0.287 0.175 0.109NIST 5.130 0.763 0.082 0.013I Bleu 0.688 0.224 0.075 0.029NIST 5.130 5.892 5.978 5.987SDIGC Bleu 0.674 0.384 0.221 0.132Figure 9.
Evaluation Results.The evaluation results show that the NIST scoreachieved a 97.3% increase, while the Bleu scoreincreased by 21.1%.In terms of decoding speed, the Rewrite de-coder took 8102 seconds to decode the test sen-tences on a Xeon 1.2GHz 2GB memory machine.On the same machine, the SDIG decoder took 3seconds to decode, excluding the parsing time.
Therecent advances in parsing have achieved parserswith 3( )O n  time complexity without the grammarconstant (McDonald et al, 2005).
It can be ex-pected that the total decoding time for SDIG canbe as short as 0.1 second per sentence.Neither of the two systems has any specifictranslation components, which are usually presentin real world systems (E.g.
components that trans-late numbers, dates, names, etc.)
It is reasonable toexpect that the performance of SDIG can be furtherimproved with such specific optimizations.6 DiscussionsWe noticed that the SDIG system outputs tend tobe longer than those of the IBM Model 4 system,and are closer to human translations in length.Translation Type Human SDIG IBM-4Avg.
Sent.
Len.
37.7 33.6 24.2Figure 10.
Average Sentence Word CountThis partly explains why the IBM Model 4 systemhas slightly higher individual n-gram precisionscores (while the SDIG system outputs are stillbetter in terms of absolute matches).547The relative orders between the parent and childETs in the output tree is currently kept the same asthe orders in the input tree.
Admittedly, we bene-fited from the fact that both Chinese and Englishare SVO languages, and that many of orderingsbetween the arguments and adjuncts can be keptthe same.
However, we did notice that this simple?ostrich?
treatment caused outputs such as ?foreignfinancial institutions the president of?.While statistical modeling of children reorder-ing is one possible remedy for this problem, webelieve simple linguistic treatment is another, asthe output of the SDIG system is an Englishdependency tree rather than a string of words.7 Conclusions and Future WorkIn this paper we presented a syntax-based statisti-cal MT system based on a Synchronous Depend-ency Insertion Grammar and a non-isomorphicstochastic tree-to-tree transducer.
A graphicalmodel for the transducer is defined and a polyno-mial time decoding algorithm is introduced.
Theresults of our current implementation were evalu-ated using the NIST and Bleu automatic MTevaluation software.
The evaluation shows that theSDIG system outperforms an IBM Model 4 basedsystem in both speed and quality.Future work includes a full-fledged version ofSDIG and a more sophisticated MT pipeline withpossibly a tri-gram language model for decoding.ReferencesY.
Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty,I.
D. Melamed, F. Och, D. Purdy, N. A. Smith, and D.Yarowsky.
1999.
Statistical machine translation.Technical report, CLSP, Johns Hopkins University.H.
Alshawi, S. Bangalore, S. Douglas.
2000.
Learningdependency translation models as collections of finitestate head transducers.
Comp.
Linguistics, 26(1):45-60.Daniel M. Bikel.
2002.
Design of a multi-lingual, paral-lel-processing statistical parsing engine.
In HLT 2002.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert Mercer.
1993.
The mathe-matics of statistical machine translation: parameter es-timation.
Computational Linguistics, 19(2): 263-311.Michael John Collins.
1999.
Head-driven StatisticalModels for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia.Ding and Palmer.
2004a.
Automatic Learning of Paral-lel Dependency Treelet Pairs.
In First InternationalJoint Conference on NLP (IJCNLP-04).Ding and Palmer.
2004b.
Synchronous DependencyInsertion Grammars: A Grammar Formalism for Syn-tax Based Statistical MT.
Workshop on Recent Ad-vances in Dependency Grammars, COLING-04.Bonnie J. Dorr.
1994.
Machine translation divergences:A formal description and proposed solution.
Compu-tational Linguistics, 20(4): 597-633.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In ACL-03.
(compan-ion volume), Sapporo, July.Heidi J.
Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In Proceedings of EMNLP-02.Ulrich Germann, Michael Jahr, Kevin Knight, DanielMarcu, and Kenji Yamada.
2001.
Fast Decoding andOptimal Decoding for Machine Translation.
ACL-01.Daniel Gildea.
2003.
Loosely tree based alignment formachine translation.
ACL-03, Japan.Jonathan Graehl and Kevin Knight.
2004.
Training TreeTransducers.
In NAACL/HLT-2004Jan Hajic, et al 2002.
Natural language generation inthe context of machine translation.
Summer workshopfinal report, Center for Language and Speech Process-ing, Johns Hopkins University, Baltimore.Rebecca Hwa, Philip S. Resnik, Amy Weinberg, andOkan Kolak.
2002.
Evaluating translational corre-spondence using annotation projection.
ACL-02Ali Ibrahim, Boris Katz, and Jimmy Lin.
2003.
Extract-ing Structural Paraphrases from Aligned Monolin-gual Corpora.
In Proceedings of the SecondInternational Workshop on Paraphrasing (IWP 2003)Dan Melamed.
2004.
Statistical Machine Translation byParsing.
In ACL-04, Barcelona, Spain.Dan Melamed.
2003.
Multitext Grammars and Synchro-nous Parsers, In NAACL/HLT-2003.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
ACL-02, Philadelphia, USA.Ryan McDonald, Koby Crammer and Fernando Pereira.2005.
Online Large-Margin Training of DependencyParsers.
ACL-05.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51.S.
M. Shieber and Y. Schabes.
1990.
Synchronous Tree-Adjoining Grammars, Proceedings of the 13thCOLING, pp.
253-258, August 1990.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):3-403.Fei Xia.
2001.
Automatic grammar generation from twodifferent perspectives.
PhD thesis, U. of Pennsylvania.Kenji Yamada and Kevin Knight.
2001.
A syntax basedstatistical translation model.
ACL-01, France.Kenji Yamada and Kevin Knight.
2002.
A decoder forsyntax-based statistical MT.
ACL-02, Philadelphia.548
