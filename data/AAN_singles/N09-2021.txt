Proceedings of NAACL HLT 2009: Short Papers, pages 81?84,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsDetecting Pitch Accents at the Word, Syllable and Vowel LevelAndrew RosenbergColumbia UniversityDepartment of Computer Scienceamaxwell@cs.columbia.eduJulia HirschbergColumbia UniversityDepartment of Computer Sciencejulia@cs.columbia.eduAbstractThe automatic identification of prosodicevents such as pitch accent in English has longbeen a topic of interest to speech researchers,with applications to a variety of spoken lan-guage processing tasks.
However, much re-mains to be understood about the best meth-ods for obtaining high accuracy detection.
Wedescribe experiments examining the optimaldomain for accent analysis.
Specifically, wecompare pitch accent identification at the syl-lable, vowel or word level as domains for anal-ysis of acoustic indicators of accent.
Our re-sults indicate that a word-based approach issuperior to syllable- or vowel-based detection,achieving an accuracy of 84.2%.1 IntroductionProsody in a language like Standard American En-glish can be used by speakers to convey semantic,pragmatic and paralinguistic information.
Words aremade intonationall prominent, or accented to conveyinformation such as contrast, focus, topic, and in-formation status.
The communicative implicationsof accenting influence the interpretation of a wordor phrase.
However, the acoustic excursions associ-ated with accent are typically aligned with the lex-ically stressed syllable of the accented word.
Thisdisparity between the domains of acoustic proper-ties and communicative impact has led to differentapproaches to pitch accent detection, and to the useof different domains of analysis.In this paper, we compare automatic pitch accentdetection at the vowel, syllable, and word level todetermine which approach is optimal.
While lex-ical and syntactic information has been shown tocontribute to the detection of pitch accent, we onlyexplore acoustic features.
This decision allows usto closely examine the indicators of accent that arepresent in the speech signal in isolation from lin-guistic effects that may indicate that a word or syl-lable may be accented.
The choice of domain forautomatic pitch accent prediction it also related tohow that prediction is to be used and impacts howit can be evaluated in comparison with other re-search efforts.
While some downstream spoken lan-guage processing tasks benefit by knowing whichsyllable in a word is accented, such as clarifica-tion of communication misunderstandings, such as?I said unlock the door ?
not lock it!
?, most appli-cations care only about which word is intonation-ally prominent.
For the identification of contrast,given/new status, or focus, only word-level informa-tion is required.
While the performance of nucleus-or syllable-based predictions can be translated toword predictions, such a translation is rarely per-formed, making it difficult to compare performanceand thus determine which approach is best.In this paper, we describe experiments in pitch ac-cent detection comparing the use of vowel nuclei,syllables and words as units of analysis.
In Section2, we discuss related work.
We describe the ma-terials in Section 3, the experiments themselves inSection 4 and conclude in Section 5.2 Related WorkAcoustic-based approaches to pitch accent detectionhave explored prediction at the word, syllable, and81vowel level, but have rarely compared predictionaccuracies across these different domains.
An ex-ception is the work of Ross and Ostendorf (1996),who detect accent on the Boston University RadioNews Corpus (BURNC) at both the syllable andword level.
Using CART predictions as input to anHMM, they detect pitch accents on syllables spokenby a single speaker from BURNC with 87.7% accu-racy, corresponding to 82.5% word-based accuracy,using both lexical and acoustic features.
In compar-ing the discriminative usefulness of syllables vs. syl-lable nuclei for accent detection, Tamburini (2003)finds syllable nuclei (vowel) duration to be as usefulto full syllables.
Rosenberg and Hirschberg (2007)used an energy-based ensemble technique to detectpitch accents with 84.1% accuracy on the read por-tion of the Boston Directions Corpus, without us-ing lexical information.
Sridhar et al (2008) ob-tain 86.0% word-based accuracy using maximumentropy models from acoustic and syntactic infor-mation on the BURNC.
Syllable-based detectionby Ananthakrishnan and Narayanan (2008) com-bines acoustic, lexical and syntactic FSM modelsto achieve a detection rate of 86.75%.
Similarsuprasegmental features have also been explored inwork at SRI/ICSI which employs a hidden eventmodel to model intonational information for a va-riety of tasks including punctuation and disfluencydetection (Baron et al, 2002).
However, whileprogress has been made in accent detection perfor-mance in the past 15 years, with both word and syl-lable accuracy at about 86%, these accuracies havebeen achieved with different methods and some haveincluded lexico-syntactic as well as acoustic fea-tures.
It is still not clear which domain of acousticanalysis provides the most accurate cues for accentprediction.
To address this issue, our work comparesaccent detection at the syllable nucleus, full syllable,and word levels, using a common modeling tech-nique and a common corpus, to focus on the ques-tion of which domain of acoustic analysis is mostuseful for pitch accent prediction.3 Boston University Radio News CorpusOur experiments use 157.9 minutes (29,578 words)from six speakers in the BURNC (Ostendorf et al,1995) recordings of professionally read radio news.This corpus has been prosodically annotated withfull ToBI labeling (Silverman et al, 1992), includ-ing the presence and type of accents; these are an-notated at the syllable level and 54.7% (16,178) ofwords are accented.
Time-aligned phone boundariesgenerated by forced alignment are used to identifyvowel regions for analysis.
There are 48,359 vow-els in the corpus and 34.8 of these are accented.
Togenerate time-aligned syllable boundaries, we alignthe forced-aligned phones with a syllabified lexiconincluded with the corpus.The use of BURNC for comparative accent pre-diction in our three domains is not straightforward,due to anomalies in the corpus.
First, the lexiconand forced-alignment output in BURNC use distinctphonetic inventories; to align these, we have em-ployed a minimum edit distance procedure wherealigning any two vowels incurs zero cost.
This guar-antees that, at a minimum the vowels will be alignedcorrectly.
Also, the number of syllables per wordin the lexicon does not always match the numberof vowels in the forced alignment.
This leads to114 syllables containing two forced-aligned vowels,and 8 containing none.
Instead of performing posthoc correction of the syllabification results, we in-clude all of the automatically identified syllables inthe data set.
This syllabification approach generates48,253 syllables, 16,781 (34.8%) bearing accent.4 Pitch Accent Detection ExperimentsWe train logistic regression models to detect thepresence of pitch accent using acoustic featuresdrawn from each word, syllable and vowel, usingWeka (Witten et al, 1999).
The features we use in-cluded pitch (f0), energy and duration, which havebeen shown to correlate with pitch accent in En-glish.
To model these, we calculate pitch and en-ergy contours for each token using Praat (Boersma,2001).
Duration information is derived using thevowel, syllable or word segmentation described inSection 3.
The feature vectors we construct includefeatures derived from both raw and speaker z-scorenormalized1 pitch and energy contours.
The featurevector used in all three analysis scenarios is com-prised of minimum, maximum, mean, standard de-1Z-score normalization:xnorm = x???
, where x is a valueto normalize, ?
and ?
are mean and standard deviation.
Theseare estimated from all pitch or intensity values for a speaker.82viation and the z-score of the maximum of these rawand normalized acoustic contours.
The duration ofthe region in seconds is also included.The results of ten-fold cross validation classifica-tion experiments are shown in Table 1.
Note that,when running ten-fold cross validation on syllablesand vowels, we divide the folds by words, so thateach syllable within a word is a member of thesame fold.
To allow for direct comparison of thethree approaches, we generate word-based resultsfrom vowel- and syllable-based experiments.
If anysyllable or vowel in a word is hypothesized as ac-cented, the containing word is predicted to be ac-cented.
Vowel/syllable accuracies should be higherRegion Accuracy (%) F-MeasureVowel 68.5?
0.319 0.651?
0.00329Syllable 75.6?
0.125 0.756?
0.00188Word 82.9?
0.168 0.845?
0.00162Table 1: Word-level accuracy and F-Measurethan word-based accuracies since the baseline is sig-nificantly higher.
However, we find that the F-measure for detecting accent is consistently higherfor word-based results.
A prediction of accentedon any component syllable is sufficient to generatea correct word prediction.Our results suggest, first of all, that there is dis-criminative information beyond the syllable nucleus.Syllable-based classification is significantly betterthan vowel-based classification, whether we com-pare accuracy or F-measure.
It is possible that thenarrow region of analysis offered by syllable andvowel-based analysis makes the aggregated featuresmore susceptible to the effects of noise.
Moreover,errors in the forced-alignment phone boundariesand syllabification may negatively impact the per-formance of vowel- and syllable-based approaches.Until automatic phone alignment improves, word-based prediction appears to be more reliable.
Anautomatic, acoustic syllable-nucleus detection ap-proach may be able generate more discriminative re-gions of analysis for pitch accent detection than theforced-alignment and lexicon alignment techniqueused here.
This remains an area for future study.However, if we accept that the feature represen-tations accurately model the acoustic informationcontained in the regions of analysis and that theBURNC annotation is accurate, the most likely ex-planation for the superiority of word-based predic-tion over syllable- or vowel-based strategiesis is thatthe acoustic excursions correlated with accent occuroutside a word?s lexically stressed syllable.
In par-ticular, complex pitch accents in English are gener-ally realized on multiple syllables.
To examine thispossibility, we looked at the distribution of missesfrom the three classification scenarios.
The distribu-tion of pitch accent types of missed detections usingevaluation of the three scenarios is shown in Table2.
In the ToBI framework, the complex pitch ac-cents include L+H*, L*+H, H+!H* and their down-stepped variants.
As we suspected, larger units ofanalysis lead to improved performance on complextones; ?2 analysis of the difference between the er-ror distributions yields a ?2 of 42.108, p< 0.0001.Since accenting is the perception of a word asmore prominent than surrounding words, featuresthat incorporate local contextual acoustic informa-tion should improve detection accuracy at all lev-els.
To represent surrounding acoustic context infeature vectors, we calculate the z-score of the max-imum and mean pitch and energy over six regions.Three of these are ?short range?
regions: one pre-vious region, one following region, and both theprevious and following region.
The other three are?long range?
regions.
For words, these regionsare defined as two previous words, two followingwords, and both two previous and two followingwords.
To give syllable- and vowel-based classifi-cation scenarios access to a comparable amount ofacoustic context, the ?long range?
regions coveredranges of three syllables or vowels.
There are ap-proximately 1.63 syllables/vowels per word in theBURNC corpus; thus, on balance, a window of twowords is equivalent to one of three syllables.
Du-ration is also normalized relative to the duration ofregions within the contextual regions.
Accuracy andf-measure results from ten-fold cross validation ex-periments are shown in Table 3.
We find dramaticAnalysis Region Accuracy (%) F-MeasureVowel 77.4?
0.264 0.774?
0.00370Syllable 81.9?
0.197 0.829?
0.00195Word 84.2?
0.247 0.858?
0.00276Table 3: Word-level accuracy and F-Measure with Con-textual Featuresincreases in the performance of vowel- and syllable-83Region H* L* Complex Total MissesVowel .6825 (3732) .0686 (375) .2489 (1361) 1.0 (5468)Syllable .7033 (2422) .0851 (293) .2117 (729) 1.0 (3444)Word .7422 (2002) .0610 (165) .1986 (537) 1.0 (2704)Table 2: Distribution of missed detections organized by H*, L* and complex pitch accents.based performance when we include contextual fea-tures.
Vowel-based classification shows nearly 10%absolute increase accuracy when translated to theword level.
The improvements in word-based clas-sification, however, are less dramatic.
It may bethat word-based analysis already incorporates muchthe contextual information that is helpful for detect-ing pitch accents.
The feature representations ineach of these three experiments include a compara-ble amount of acoustic context.
This suggests thatthe superiority of word-based detection is not sim-ply due to the access to more contextual informa-tion, but rather that there is discriminative informa-tion outside the accent-bearing syllable.5 Conclusion and Future WorkIn this paper, we describe experiments comparingthe detection of pitch accents on three acoustic do-mains ?
words, syllables and vowels ?
using acous-tic features alone.
To permit direct comparison be-tween accent prediction in these three domains ofanalysis, we generate word-, syllable-, and vowel-based results directly, and then transfer syllable- andnucleus-based predictions to word predictions.Our experiments show that word-based accentdetection significantly outperforms syllable- andvowel-based approaches.
Extracting features thatincorporate acoustic information from surroundingcontext improves performance in all three domains.We find that there is, in fact, acoustic informationdiscriminative to pitch accent that is found withinaccented words, outside the accent-bearing sylla-ble.
We achieve 84.2% word-based accuracy ?significantly below the 86.0% reported by Sridharet al (2008) using syntactic and acoustic compo-nents.
However, our experiments use only acousticfeatures, since we are concerned with comparing do-mains of acoustic analysis within the larger task ofaccent identification.
Our 84.2% accuracy is signifi-cantly higher than the 80.09% accuracy obtained bythe 10ms frame-based acoustic modeling describedin (Sridhar et al, 2008).
Our aggregations of pitchand energy contours over a region of analysis appearto be more helpful than short frame modeling.In future work, we will explore a number of tech-niques to transfer word based predictions to sylla-bles.
This will allow us to compare word-based de-tection to published syllable-based results.
Prelimi-nary results suggest that word-based detection is su-perior regardless of the domain of evaluation.ReferencesS.
Ananthakrishnan and S. Narayanan.
2008.
Auto-matic prosodic event detection using acoustic, lexicaland syntactic evidence.
IEEE Transactions on Audio,Speech & Language Processing, 16(1):216?228.D.
Baron, E. Shriberg, and A. Stolcke.
2002.
Auto-matic punctuation and disfluency detection in multi-party meetings using prosodic and lexical cues.
In IC-SLP.P.
Boersma.
2001.
Praat, a system for doing phoneticsby computer.
Glot International, 5(9-10):341?345.M.
Ostendorf, P. Price, and S. Shattuck-Hufnagel.
1995.The boston university radio news corpus.
TechnicalReport ECS-95-001, Boston University, March.A.
Rosenberg and J. Hirschberg.
2007.
Detecting pitchaccent using pitch-corrected energy-based predictors.In Interspeech.K.
Ross and M. Ostendorf.
1996.
Prediction of ab-stract prosodic labels for speech synthesis.
ComputerSpeech & Language, 10(3):155?185.K.
Silverman, M. Beckman, J. Pitrelli, M. Osten-dorf, C. Wightman, P. Price, J. Pierrehumbert, andJ.
Hirschberg.
1992.
Tobi: A standard for labeling en-glish prosody.
In Proc.
of the 1992 International Con-ference on Spoken Language Processing, volume 2,pages 12?16.V.
R. Sridhar, S. Bangalore, and S. Narayanan.
2008.Exploiting acoustic and syntactic features for prosodylabeling in a maximum entropy framework.
IEEETransactions on Audio, Speech & Language Process-ing, 16(4):797?811.F.
Tamburini.
2003.
Prosodic prominence detection inspeech.
In ISSPA2003, pages 385?388.I.
Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, andS.
Cunningham.
1999.
Weka: Practical machinelearning tools and techniques with java implementa-tion.
In ICONIP/ANZIIS/ANNES International Work-shop, pages 192?196.84
