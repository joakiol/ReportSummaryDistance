Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 125?131,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLexicon Stratification for Translating Out-of-Vocabulary WordsYulia TsvetkovLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USAytsvetko@cs.cmu.eduChris DyerLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USAcdyer@cs.cmu.eduAbstractA language lexicon can be divided into fourmain strata, depending on origin of words:core vocabulary words, fully- and partially-assimilated foreign words, and unassim-ilated foreign words (or transliterations).This paper focuses on translation of fully-and partially-assimilated foreign words,called ?borrowed words?.
Borrowed words(or loanwords) are content words found innearly all languages, occupying up to 70%of the vocabulary.
We use models of lexi-cal borrowing in machine translation as apivoting mechanism to obtain translationsof out-of-vocabulary loanwords in a low-resource language.
Our framework obtainssubstantial improvements (up to 1.6 BLEU)over standard baselines.1 IntroductionOut-of-vocabulary (OOV) words are a ubiquitousand difficult problem in statistical machine transla-tion (SMT).
When a translation system encountersan OOV?a word that was not observed in the train-ing data, and the trained system thus lacks its trans-lation variants?it usually outputs the word just asit is in the source language, producing erroneousand disfluent translations.All SMT systems, even when trained on billion-sentence-size parallel corpora, are prone to OOVs.These are often named entities and neologisms.However, OOV problem is much more serious inlow-resource scenarios: there, OOVs are primarilynot lexicon-peripheral items such as names and spe-cialized/technical terms, but regular content words.Procuring translations for OOVs has been a sub-ject of active research for decades.
Translation ofnamed entities is usually generated using translit-eration techniques (Al-Onaizan and Knight, 2002;Hermjakob et al, 2008; Habash, 2008).
Extractinga translation lexicon for recovering OOV contentwords and phrases is done by mining bi-lingualand monolingual resources (Rapp, 1995; Callison-Burch et al, 2006; Haghighi et al, 2008; Mar-ton et al, 2009; Razmara et al, 2013; Salujaet al, 2014; Zhao et al, 2015).
In addition,OOV content words can be recovered by exploitingcognates, by transliterating and then pivoting viaa closely-related resource-richer language, whensuch a language exists (Haji?c et al, 2000; Mannand Yarowsky, 2001; Kondrak et al, 2003; De Gis-pert and Marino, 2006; Durrani et al, 2010; Wanget al, 2012; Nakov and Ng, 2012; Dholakia andSarkar, 2014).
Our work is similar in spirit to thelatter line of research, but we show how to curatetranslations for OOV content words by pivoting viaan unrelated, often typologically distant resource-rich languages.
To achieve this goal, we replacetransliteration by a new technique that capturesmore complex morpho-phonological transforma-tions of historically-related words.CorePeripheralPartially assimilatedFully assimilatedLEXICONFigure 1: A language lexicon can be divided into four mainstrata, depending on origin of words.
This work focuses onfully- and partially-assimilated foreign words, called borrowedwords.
Borrowed words (or loanwords) are content wordsfound in all languages, occupying up to 70% of the vocabulary.Our method is inspired by prior research inconstraint-based phonology, advocating ?lexiconstratification,?
i.e., splitting the language lexiconinto separate strata, depending on origin of wordsand degree of their assimilation in the language (It?and Mester, 1995).
As shown in figure 1, there arefour main strata: core vocabulary, foreign wordsthat are fully assimilated, partially-assimilated for-125eign words, and named entities which belong tothe peripheral stratum.
Our work focuses on thefully- and partially-assimilated foreign words, i.e.,words that historically were borrowed from anotherlanguage.
Borrowing is the pervasive linguisticphenomenon of transferring and adapting linguisticconstructions (lexical, phonological, morphologi-cal, and syntactic) from a ?donor?
language intoa ?recipient?
language (Thomason and Kaufman,2001).
In this work, we advocate a pivoting mech-anism exploiting lexical borrowing to bridge be-tween resource-rich and resource-poor languages.Our method (?2) employs a model of lexicalborrowing to obtain cross-lingual links from loan-words in a low-resource language to their donorsin a resource-rich language (?2.1).
The donorlanguage is used as pivot to obtain translationsvia triangulation of OOV loanwords (?2.2).
Weconduct experiments with two resource-poor se-tups: Swahili?English, pivoting via Arabic, andRomanian?English,pivoting via French (?3).
Weprovide a systematic quantitative analysis of con-tribution of integrated OOV translations, relativeto baselines and upper bounds, and on corpora ofvarying sizes (?4).
The proposed approach yieldssubstantial improvement (up to +1.6 BLEU) inSwahili?Arabic?English translation, and a smallbut statistically significant improvement (+0.2BLEU) in Romanian?French?English.2 MethodologyOur high-level solution is depicted in figure 2.Given an OOV word in resource-poor SMT, weplug it into a borrowing system (?2.1) that identi-fies the list of plausible donor words in the donorlanguage.
Then, using the resource-rich SMT, wetranslate the donor words to the same target lan-guage as in the resource-poor SMT (here, English).Finally, we integrate translation candidates in theresource-poor system (?2.2).2.1 Models of Lexical BorrowingBorrowed words (also called loanwords) are foundin nearly all languages, and routinely account for10?70% of the vocabulary (Haspelmath and Tad-mor, 2009).
Borrowing occurs across geneticallyand typologically unrelated languages, for exam-ple, about 40% of Swahili?s vocabulary is borrowedfrom Arabic (Johnson, 1939).
Importantly, sinceresource-rich languages are (historically) geopoliti-cally important languages, borrowed words often6:$+ILI?EN*LI6+VaIari___ 229NiWXrXNi___ 22975$16/$7,21C$1','$7(6$5$%IC?EN*LI6+????
? ysAfr  ___ WraYel???
? trky  ___ WXrNiVh$5$%ICWo6:$+ILI%2552:,1*Figure 2: To improve a resource-poor Swahili?English SMTsystem, we extract translation candidates for OOV Swahiliwords borrowed from Arabic using the Swahili-to-Arabic bor-rowing system and Arabic?English resource-rich SMT.bridge between resource-rich and resource-limitedlanguages; we use this observation in our work.Transliteration and cognate discovery modelsperform poorly in the task of loanword genera-tion/identification (Tsvetkov et al, 2015).
Themain reason is that the recipient language, in whichborrowed words are fully or partially assimilated,may have very different morpho-phonological prop-erties from the donor language (e.g., ?orange?
and?sugar?
are not perceived as foreign by native speak-ers, but these are English words borrowed fromArabic l.'PAK (nArnj)1and Q???
@ (Alskr), respec-tively).
Therefore, morpho-phonological loanwordadaptation is more complex than is typically cap-tured by transliteration or cognate models.We employ a discriminative cross-lingual modelof lexical borrowing to identify plausible donorsgiven a loanword (Tsvetkov et al, 2015).
Themodel is implemented in a cascade of finite-statetransducers that first maps orthographic word formsin two languages into a common space of their pho-netic representation (using IPA?the InternationalPhonetic Alphabet), and then performs morpholog-ical and phonological updates to the input wordin one language to identify its (donor/loan) coun-terpart in another language.
Transduction oper-ations include stripping donor language prefixesand suffixes, appending recipient affixes, insertion,deletion, and substitution of consonants and vow-els.
The output of the model, given an input loan-word, is a n-best list of donor candidates, rankedby linguistic constraints of the donor and recipientlanguages.21We use Buckwalter notation to write Arabic glosses.2In this work, we give as input into the borrowing systemall OOV words, although, clearly, not all OOVs are loanwords,and not all loanword OOVs are borrowed from the donorlanguage.
However, an important property of the borrowingmodel is that its operations are not general, but specific to1262.2 Pivoting via BorrowingWe now discuss integrating translation candidatesacquired via borrowing plus resource-rich transla-tion.
For each OOV, the borrowing system pro-duces the n-best list of plausible donors; for eachdonor we then extract the k-best list of its transla-tions.3Then, we pair the OOV with the resultingn?
k translation candidates.
The translation can-didates are noisy: some of the generated donorsmay be erroneous, the errors are then propagatedin translation.
To allow the low-resource systemto leverage good translations that are missing inthe default phrase inventory, while being stable tonoisy translation hypotheses, we integrate the ac-quired translation candidates as synthetic phrases(Tsvetkov et al, 2013; Chahuneau et al, 2013).Synthetic phrases is a strategy of integrating trans-lated phrases directly in the MT translation model,rather than via pre- or post-processing MT inputsand outputs.
Synthetic phrases are phrasal trans-lations that are not directly extractable from thetraining data, generated by auxiliary translationand postediting processes (for example, extractedfrom a borrowing model).
An important advantageof synthetic phrases is that they are recall-oriented,allowing the system to leverage good translationsthat are missing in the default phrase inventory,while being stable to noisy translation hypotheses.To let the translation model learn whether to trustthese phrases, the translation options obtained fromthe borrowing model are augmented with a booleantranslation feature indicating that the phrase wasgenerated externally.
Additional features annotat-ing the integrated OOV translations correspond toproperties of the donor?loan words?
relation; theirgoal is to provide an indication of plausibility ofthe pair (to mark possible errors in the outputs ofthe borrowing system).We employ two types of features: phonetic andsemantic.
Since borrowing is primarily a phonolog-ical phenomenon, phonetic features will providean indication of how typical (or atypical) pronun-ciation of the word in a language; loanwords areexpected to be less typical than core vocabularythe language-pair and reduced only to a small set of plausiblechanges that the donor word can undergo in the process ofassimilation in the recipient language.
Thus, the borrowingsystem only minimally overgenerates the set of output candi-dates given an input.
If the borrowing system encounters aninput word that was not borrowed from the target donor lan-guage, it usually (but not always) produces an empty output.3We set n and k to 5, we did not experiment with othervalues.words.
The goal of semantic features is to mea-sure semantic similarity between donor and loanwords: erroneous candidates and borrowed wordsthat changed meaning over time are expected tohave different meaning from the OOV.Phonetic features.
To compute phonetic fea-tures we first train a (5-gram) language model (LM)of IPA pronunciations of the donor/recipient lan-guage vocabulary (phoneLM).
Then, we re-scorepronunciations of the donor and loanword can-didates using the LMs.4We hypothesize that indonor?loanword pairs the donor phoneLM scoreis higher but the loanword score is lower (i.e., theloanword phonology is atypical in the recipient lan-guage).
We capture this intuition in three features:f1=PphoneLM(donor), f2=PphoneLM(loanword),and the harmonic mean between the two scoresf3=2f1f2f1+f2.Semantic features.
We compute a semantic sim-ilarity feature between the candidate donor andthe OOV loanword as follows.
We first train, us-ing large monolingual corpora, 100-dimensionalword vector representations for donor and recip-ient language vocabularies.5Then, we employcanonical correlation analysis (CCA) with smalldonor?loanword dictionaries (training sets in theborrowing models) to project the word embeddingsinto 50-dimensional vectors with maximized cor-relation between their dimensions.
The semanticfeature annotating the synthetic translation candi-dates is cosine distance between the resulting donorand loanword vectors.
We use the word2vec tool(Mikolov et al, 2013) to train monolingual vec-tors,6and the CCA-based tool (Faruqui and Dyer,2014) for projecting word vectors.73 Experimental SetupDatasets and software.
The Swahili?Englishparallel corpus was crawled from the Global Voicesproject website8.
To simulate resource-poor sce-nario for the Romanian?English language pair, wesample a parallel corpus of same size from the tran-scribed TED talks (Cettolo et al, 2012).
To evalu-4For Arabic and French we use the GlobalPhone pro-nunciation dictionaries (Schultz et al, 2013) (we manuallyconvert them to IPA).
For Swahili and Romanian we automati-cally construct pronunciation dictionaries using the Omniglotgrapheme-to-IPA conversion rules at www.omniglot.com.5We assume that while parallel data is limited in the recip-ient language, monolingual data is available.6code.google.com/p/word2vec7github.com/mfaruqui/eacl14-cca8sw.globalvoicesonline.org127ate translation improvement on corpora of differentsizes we conduct experiments with sub-sampled4K, 8K, and 14K parallel sentences from the train-ing corpora (the smaller the training corpus, themore OOVs it has).
Corpora sizes along with statis-tics of source-side OOV tokens and types are givenin tables 1 and 2.
Statistics of the held-out devand test sets used in all translation experiments aregiven in table 3.SW?EN RO?ENdev test dev testSentences 1,552 1,732 2,687 2,265Tokens 33,446 35,057 24,754 19,659Types 7,008 7,180 5,141 4,328Table 3: Dev and test corpora sizes.In all the MT experiments, we use the cdec9toolkit (Dyer et al, 2010), and optimize parameterswith MERT (Och, 2003).
English 4-gram languagemodels with Kneser-Ney smoothing (Kneser andNey, 1995) are trained using KenLM (Heafield,2011) on the target side of the parallel training cor-pora and on the Gigaword corpus (Parker et al,2009).
Results are reported using case-insensitiveBLEU with a single reference (Papineni et al,2002).
We train three systems for each MT setup;reported BLEU scores are averaged over systems.Upper bounds.
The goal of our experiments isnot only to evaluate the contribution of the OOVdictionaries that we extract when pivoting via bor-rowing, but also to understand the potential con-tribution of the lexicon stratification.
What is theoverall improvement that can be achieved if we cor-rectly translate all OOVs that were borrowed fromanother language?
What is the overall improve-ment that can be achieved if we correctly translateall OOVs?
We answer this question by defining?upper bound?
experiments.
In the upper boundexperiment we word-align all available parallel cor-pora, including dev and test sets, and extract fromthe alignments oracle translations of OOV words.Then, we append the extracted OOV dictionariesto the training corpora and re-train SMT setupswithout OOVs.
Translation scores of the resultingsystem provide an upper bound of an improvementfrom correctly translating all OOVs.
When weappend oracle translations of the subset of OOVdictionaries, in particular translations of all OOVsfor which the output of the borrowing system is9www.cdec-decoder.orgnot empty, we obtain an upper bound that can beachieved using our method (if the borrowing sys-tem provided perfect outputs).
Understanding theupper bounds is relevant not only for our experi-ments, but for any experiments that involve aug-menting translation dictionaries; however, we arenot aware of prior work providing similar analy-sis of upper bounds, and we recommend this asa calibrating procedure for future work on OOVmitigation strategies.Borrowing-augmented setups.
As described in?2.2, we integrate translations of OOV loanwordsin the translation model.
Due to data sparsity,we conjecture that non-OOVs that occur only fewtimes in the training corpus can also lack appro-priate translation candidates, i.e., these are target-language OOVs.
We therefore run the borrowingsystem on OOVs and non-OOV words that occurless than 3 times in the training corpus.
We list intable 4 sizes of translated lexicons that we integratein translation tables.4K 8K 14KLoan OOVs in SW?EN 5,050 4,219 3,577Loan OOVs in RO?EN 347 271 216Table 4: Sizes of translated lexicons extracted using pivotingvia borrowing and integrated in translation models.Transliteration-augmented setups.
In ad-dition to the standard baselines, we evaluatetransliteration-augmented setups, where wereplace the borrowing model by a transliterationmodel (Ammar et al, 2012).
The model is alinear-chain CRF where we label each sourcecharacter with a sequence of target characters.
Thefeatures are label unigrams and bigrams, separatelyor conjoined with a moving window of sourcecharacters.
We employ the Swahili?Arabic andRomanian?French transliteration systems thatwere used as baselines in (Tsvetkov et al, 2015).As in the borrowing system, transliteration outputsare filtered to contain only target language lexicons.We list in table 5 sizes of obtained translatedlexicons.4K 8K 14KTranslit.
OOVs in SW?EN 49 32 22Translit.
OOVs in RO?EN 906 714 578Table 5: Sizes of translated lexicons extracted using pivotingvia transliteration and integrated in translation models.1284K 8K 14KTokens 84,764 170,493 300,648Types 14,554 23,134 33,288OOV tokens 4,465 (12.7%) 3,509 (10.0%) 2,965 (8.4%)OOV types 3,610 (50.3%) 2,950 (41.1%) 2,523 (35.1%)Table 1: Statistics of the Swahili?English corpora and source-side OOV for 4K, 8K, 14K parallel training sentences.4K 8K 14KTokens 35,978 71,584 121,718Types 7,210 11,144 15,112OOV tokens 3,268 (16.6%) 2,585 (13.1%) 2,177 (11.1%)OOV types 2,382 (55.0%) 1,922 (44.4%) 1,649 (38.1%)Table 2: Statistics of the Romanian?English corpora and source-side OOV for 4K, 8K, 14K parallel training sentences.4 ResultsTranslation results are shown in tables 6 and 7.We evaluate separately the contribution of the in-tegrated OOV translations, and the same transla-tions annotated with phonetic and semantic fea-tures.
We also provide upper bound scores forintegrated loanword dictionaries as well as for re-covering all OOVs.4K 8K 14KBaseline 13.2 15.1 17.1+ Translit.
OOVs 13.4 15.3 17.2+ Loan OOVs 14.3 15.7 18.2+ Features 14.8 16.4 18.4Upper bound loan 18.9 19.1 20.7Upper bound all OOVs 19.2 20.4 21.1Table 6: Swahili?English MT experiments.4K 8K 14KBaseline 15.8 18.5 20.7+ Translit.
OOVs 15.8 18.7 20.8+ Loan OOVs 16.0 18.7 20.7+ Features 16.0 18.6 20.6Upper bound loan 16.6 19.4 20.9Upper bound all OOVs 28.0 28.8 30.4Table 7: Romanian?English MT experiments.Swahili?English MT performance is improvedby up to +1.6 BLEU when we augment itwith translated OOV loanwords leveraged fromthe Arabic?Swahili borrowing and then Arabic?English MT.
The contribution of the borrowingdictionaries is +0.6?1.1 BLEU, and phonetic andsemantic features contribute additional half BLEU.More importantly, upper bound results show thatthe system can be improved more substantially withbetter dictionaries of OOV loanwords.
This resultconfirms that OOV borrowed words is an importanttype of OOVs, and with proper modeling it has thepotential to improve translation by a large margin.Romanian?English systems obtain only small (butsignificant for 4K and 8K, p < .01) improvement.However, this is expected as the rate of borrow-ing from French into Romanian is smaller, and, asthe result, the integrated loanword dictionaries aresmall.
Transliteration baseline, conversely, is moreeffective in Romanian?French language pair, astwo languages are related typologically, and havecommon cognates in addition to loanwords.
Still,even with these dictionaries the translations withpivoting via borrowing/transliteration improve, andeven almost approach the upper bounds results.5 ConclusionThis paper focuses on fully- and partially-assimilated foreign words in the source lexicon?borrowed words?and a method for obtaining theirtranslations.
Our results substantially improvetranslation and confirm that OOV loanwords areimportant and merit further investigation.
In addi-tion, we propose a simple technique to calculate anupper bound of improvements that can be obtainedfrom integrating OOV translations in SMT.AcknowledgmentsThis work was supported by the U.S. Army Re-search Laboratory and the U.S. Army ResearchOffice under contract/grant number W911NF-10-1-0533.
Computational resources were provided byGoogle Cloud Computing grant.
We are gratefulto Waleed Ammar for his help with transliteration,and to the anonymous reviewers.129ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
Machinetransliteration of names in Arabic text.
In Proc.the ACL workshop on Computational Approaches toSemitic Languages, pages 1?13.Waleed Ammar, Chris Dyer, and Noah A. Smith.
2012.Transliteration by sequence labeling with lattice en-codings and reranking.
In Proc.
NEWS workshop atACL.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine transla-tion using paraphrases.
In Proc.
NAACL, pages 17?24.Mauro Cettolo, Christian Girardi, and Marcello Fed-erico.
2012.
WIT3: Web inventory of transcribedand translated talks.
In Proc.
EAMT, pages 261?268.Victor Chahuneau, Eva Schlinger, Noah A Smith, andChris Dyer.
2013.
Translating into morphologi-cally rich languages with synthetic phrases.
In Proc.EMNLP, pages 1677?1687.Adri?
De Gispert and Jose B Marino.
2006.
Catalan-English statistical machine translation without par-allel corpus: bridging through Spanish.
In Proc.LREC, pages 65?68.Rohit Dholakia and Anoop Sarkar.
2014.
Pivot-basedtriangulation for low-resource languages.
In Proc.AMTA, pages 315?328.Nadir Durrani, Hassan Sajjad, Alexander Fraser, andHelmut Schmid.
2010.
Hindi-to-Urdu machinetranslation through transliteration.
In Proc.
ACL,pages 465?474.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proc.
ACL System Demonstrations, pages 7?12.Manaal Faruqui and Chris Dyer.
2014.
Improvingvector space word representations using multilingualcorrelation.
In Proc.
EACL, pages 462?471.Nizar Habash.
2008.
Four techniques for online han-dling of out-of-vocabulary words in Arabic-Englishstatistical machine translation.
In Proc.
ACL, pages57?60.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proc.
ACL, pages771?779.Jan Haji?c, Jan Hric, and Vladislav Kubo?n.
2000.
Ma-chine translation of very close languages.
In Proc.ANLP, pages 7?12.Martin Haspelmath and Uri Tadmor, editors.
2009.Loanwords in the World?s Languages: A Compara-tive Handbook.
Max Planck Institute for Evolution-ary Anthropology, Leipzig.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proc.
WMT, pages 187?197.Ulf Hermjakob, Kevin Knight, and Hal Daum?
III.2008.
Name translation in statistical machinetranslation-learning when to transliterate.
In Proc.ACL, pages 389?397.Junko It?
and Armin Mester.
1995.
The core-peripherystructure of the lexicon and constraints on reranking.Papers in Optimality Theory, 18:181?209.Frederick Johnson.
1939.
Standard Swahili-Englishdictionary.
Oxford University Press.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Proc.ICASSP, volume 1, pages 181?184.Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.2003.
Cognates can improve statistical translationmodels.
In Proc.
HLT-NAACL, pages 46?48.Gideon S Mann and David Yarowsky.
2001.
Multipathtranslation lexicon induction via bridge languages.In Proc.
HLT-NAACL, pages 1?8.Yuval Marton, Chris Callison-Burch, and Philip Resnik.2009.
Improved statistical machine translation us-ing monolingually-derived paraphrases.
In Proc.EMNLP, pages 381?390.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Proc.
NIPS, pages 3111?3119.Preslav Nakov and Hwee Tou Ng.
2012.
Improv-ing statistical machine translation for a resource-poor language using related resource-rich languages.Journal of Artificial Intelligence Research, pages179?222.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
ACL, pages160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proc.
ACL, pages311?318.Robert Parker, David Graff, Junbo Kong, Ke Chen, andKazuaki Maeda.
2009.
English Gigaword fourthedition.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proc.
ACL, pages 320?322.Majid Razmara, Maryam Siahbani, Reza Haffari, andAnoop Sarkar.
2013.
Graph propagation for para-phrasing out-of-vocabulary words in statistical ma-chine translation.
In Proc.
ACL, pages 1105?1115.Avneesh Saluja, Hany Hassan, Kristina Toutanova, andChris Quirk.
2014.
Graph-based semi-supervisedlearning of translation models from monolingualdata.
In Proc.
ACL, pages 676?686.Tanja Schultz, Ngoc Thang Vu, and Tim Schlippe.2013.
GlobalPhone: A multilingual text & speechdatabase in 20 languages.
In Proc.
ICASSP, pages8126?8130.Sarah Grey Thomason and Terrence Kaufman.
2001.Language contact.
Edinburgh University Press Ed-inburgh.130Yulia Tsvetkov, Chris Dyer, Lori Levin, and ArchnaBhatia.
2013.
Generating English determiners inphrase-based translation with synthetic translationoptions.
In Proc.
WMT, pages 271?280.Yulia Tsvetkov, Waleed Ammar, and Chris Dyer.
2015.Constraint-based models of lexical borrowing.
InProc.
NAACL, pages 598?608.Pidong Wang, Preslav Nakov, and Hwee Tou Ng.
2012.Source language adaptation for resource-poor ma-chine translation.
In Proc.
EMNLP, pages 286?296.Kai Zhao, Hany Hassan, and Michael Auli.
2015.Learning translation models from monolingual con-tinuous representations.
In Proc.
NAACL.131
