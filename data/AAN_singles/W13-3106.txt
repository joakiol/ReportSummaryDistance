Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 45?49,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsMultilingual Multi-Document Summarization with POLY2Marina LitvakDepartment of Software EngineeringShamoon College of EngineeringBeer Sheva, Israelmarinal@sce.ac.ilNatalia VanetikDepartment of Software EngineeringShamoon College of EngineeringBeer Sheva, Israelnatalyav@sce.ac.ilAbstractIn this paper we present a linear model forthe problem of text summarization, wherea summary preserves the information cov-erage as much as possible in comparisonto the original document set.
We reducethe problem of finding the best summaryto the problem of finding the point on aconvex polytope closest to the given hy-perplane, and solve it efficiently with thehelp of fractional linear programming.
Wesupply here an overview of our system, ti-tled POLY2, that participated in the Multi-Ling contest at ACL 2013.1 IntroductionAutomated text summarization is an active fieldof research in various communities like Informa-tion Retrieval (IR), Natural Language Processing(NLP), and Text Mining (TM).Some authors reduce summarization to themaximum coverage problem (Takamura and Oku-mura, 2009; Gillick and Favre, 2009) that, de-spite a great performance, is known as NP-hard (Khuller et al 1999).
Linear Program-ming helps to find an accurate approximated so-lution to this problem and became very popularin summarization field in the last years (Gillickand Favre, 2009; Woodsend and Lapata, 2010; Hi-toshi Nishikawa and Kikui, 2010; Makino et al2011).
However, most mentioned works use expo-nential number of constraints.Trying to solve a trade-off between summaryquality and time complexity, we propose a novelsummarization model solving the approximatedmaximum coverage problem by linear program-ming in polynomial time.
We measure informa-tion coverage by terms1 and strive to obtain a sum-mary that preserves the optimal value of the cho-1normalized meaningful wordssen objective function as much as possible in com-parison to the original document.
Various objec-tive functions combining different parameters liketerm?s position and its frequency are introducedand evaluated.Our method ranks and extracts significant sen-tences into a summary and it can be generalizedfor both single-document and multi-documentsummarization.
Also, it can be easily adapted tocross-lingual/multilingual summarization.Formally speaking, in this paper we introduce(1) a novel text representation model expanding aclassic Vector Space Model (Salton et al 1975)to Hyperplane and Half-spaces, (2) re-formulatedextractive summarization problem as an optimiza-tion task and (3) its solution using linear program-ming.
The main challenge of this paper is a newtext representation model making possible to rep-resent an exponential number of extracts withoutcomputing them explicitly, and finding the opti-mal one by simple minimizing a distance functionin polynomial time.2 Our Method2.1 DefinitionsWe are given a set of sentences S1, ..., Sn derivedfrom a document or a cluster of related documents.Meaningful words in these sentences are entirelydescribed by terms T1, ..., Tm.
Our goal is to finda subset Si1 , ..., Sik consisting of sentences suchthat (1) there are at most N terms in these sen-tences, (2) term frequency is preserved as muchas possible w.r.t.
the original sentence set, (3) re-dundant information among k selected sentencesis minimized.We use the standard sentence-term matrix, A =(aij) of size m?
n, for initial data representation,where aij = k if term Ti appears in the sentenceSj precisely k times.Our goal is to find subset i1, ..., ik of A?s45columns so that the chosen submatrix representsthe best possible summary under some constraints.Since it is hard to determine what is the bestsummary mathematically (this task is usually leftto human experts), we wish to express summaryquality as a linear function of the underlying ma-trix.
We strive to find a summary that gives an op-timal value once the function in question has beendetermined.Basic text preprocessing includes sentencesplitting and tokenization.
Also, additional stepslike stopwords removal, stemming, synonym res-olution, etc.
may be performed for resource-richlanguages.2.2 Polytope as a document representationWe represent every sentence by a hyperplane andthe lower half-space of that hyperplane.
In a way,the hyperplane bounding each half-space is thesentence itself, and a half-space below it is anapproximation of that sentence.
An intersectionof lower half-spaces in Euclidean space forms aconvex polyhedron, and in our case the faces ofthis polyhedron are intersections of hyperplanesbounding lower half-spaces that stand for docu-ment sentences.
We add trivial constraints so thatthe polyhedron representing the entire documentis bounded, i.e.
is a polytope.
All possible extractsfrom the document can be represented by hyper-plane intersections.
Thus the boundary of the re-sulting polytope is a good approximation for ex-tracts that can be generated from the given docu-ment.We view every column of the sentence-term ma-trix as a linear constraint representing a lowerhalf-space in Rmn.
An occurrence of term ti insentence Sj is represented by variable xij .
Themaximality constraint on the number of terms inthe summary can be easily expressed as a con-straint on the sum of these variables.
Note thateach sentence constraint uses its n unique vari-ables, thus making sure that the intersection of ev-ery subset of sentence hyperplane is not empty andis well-defined.Every sentence Sj in our document is a lowerhalf-space of a hyperlane in Rmn, defined withcolumns of A and variables x1j , .
.
.
, xmj repre-senting the terms in this sentence:A[][j] = [a1j , .
.
.
, amj ]xj = [x1j , .
.
.
, xmj ] for all 1 ?
j ?
nWe define a system of linear inequalitiesA[][j] ?
xTj =?mi=1 aijxij ??
A[][j] ?
1T =?mi=1 aij(1)Every inequality of this form defines a lower half-space of a hyperplaneHi := (A[][j] ?
xTj = A[][j] ?
1T )To say that every term is either present or absentfrom the chosen extract, we add constraints 0 ?xij ?
1.
Intuitively, the entire hyperplane Hi andtherefore every point p ?
Hi represents sentenceSi.
Then a subset of r sentences is represented byintersection of r hyperplanes.2.3 Summary constraintsWe express summarization constraints in the formof linear inequalities.
Maximality constraint onthe number of terms in the summary can be eas-ily expressed as a constraint on the sum of all termvariables xij , and the same goes for minimalityconstraint.2.4 The polytope modelHaving defined linear inequalities that describeeach sentence in a document separately and thetotal number of terms in sentence subset, we cannow look at them together as a system:???????????????
?mi=1 ai1xi1 ?
?mi=1 ai1.
.
.
?mi=1 ainxin ?
?mi=1 ainTmin ?
?mi=1?nj=1 xij ?
TmaxWmin ?
?mi=1?nj=1 aijxij ?Wmax0 ?
xij ?
1(2)First n inequalities describe sentences S1, .
.
.
, Sn,the next two inequalities describes constraints onthe total number of terms and words in a summary,and the final constraint determines upper andlower boundaries for all sentence-term variables.Intersections of S1, .
.
.
, Sn are well-defined, sinceevery pair of sentences is described by a linearconstraints on different n-tuple of variables xij outof total mn variables.
Since every inequality in thesystem (2) is linear, the entire system describes aconvex polyhedron, which we denote by P.2.5 Objectives and summary extractionWe assume here that the surface of the polyhedronP is a suitable representation of all the possible46Function Formula DescriptionMaximal Weighted max?mi=1 witi, Maximizes the information coverage as a weighted term sum.Term Sum (OBJ1) ti =?nj=1 xji We used the following types of term weights wi.
(1) POS EQ, where wi = 1 for all i;(2) POS F, where wi = 1app(i) and app(i) is the index of a sentencein the document where the term Ti first appeared;(3) POS B, where wi = max{ 1app(i) ,1n?app(i)+1};(4) TF, where wi = tf(i) and tf(i) is the term frequency of term Ti;(5) TFISF, where wi = tf(i) ?
isf(i)and isf(i) is the inverse sentence frequency of TiDistance Function min?mi=1(t?i ?
pi)2, Minimizes the Eucledian distance between terms t = (t1, .
.
.
, tm)(OBJ2) (1) t?i = ti =?nj=1 xji (a point on the polytope P representing a generated summary)and ?i pi = 1, or and the vector p = (p1, .
.
.
, pm) (expressing document properties(2) t?i =ti?mj=1 tjwe wish to preserve and representing the ?ideal?
summary).and pi = tf(i) We used the following options for t and p representation.
(1) MTC, where t is a summary term count vectorand p contains all the terms precisely once, thusminimizing repetition but increasing terms coverage.
(2) MTF, where t contains term frequency of terms in a summaryand p contains term frequency for terms in documents.Sentence Overlap min?nj=1?nk=j+1 ovljk, Minimizes the Jaccard similarity between sentences in a summary(OBJ3) ovljk =|Sj?Sk||Sj?Sk|= (denoted by ovljk for Sj and Sk).=?mi=1 w(aij ,aik)(xij+xik)?mi=1(aij+aik)w(aij , aik) is 1 if the term Ti is present in both sentences Sj and Skand is 0 otherwise.Maximal Bigram max?i,j biij , Maximizes the information coverage as a bigram sum.Sum (OBJ4) where ?i, j 0 ?
biij ?
1 Variable biij is defined for every bigram (Ti, Tj) in the text.Table 1: Objective functions for summarization using polytope model.sentence subsets.
Fortunately, we do not need toscan the whole set of P?s surfaces but rather to findthe point on P that optimizes the chosen objectivefunction.
Table 1 contains four different objectivefunctions that we used for summarization, alongwith descriptions of the changes in the model thatwere required for each function.Since fractional LP method not only findsthe optimal value of objective function but alsopresents an evidence to that optimality in the formof a point x = (xij), we use the point?s data to findwhat sentences belong to the chosen summary.The point x may satisfy several of the sentenceinequalities as equalities, while other inequalitiesmay not turn into equalities.
If sentence inequalityis turned into equality by x, the sentence necessar-ily belongs to the chosen summary.
Otherwise, thepoint x that optimizes the chosen objective func-tion represents a summary that does not containsufficient number of words or terms.
In this casewe add additional sentences to the summary andwe choose these sentences on the basis of theirdistance from the point x.
Sentence hyperplanesthat are the nearest to the point x are chosen in agreedy manner and added to the summary.
Thistest is straightforward and takes O(mn) time.3 POLY2: system descriptionWe title our system POLY2 as a double POLYoccured in ?POLYnomial Summarization usingPOLYtope model?.
POLY2 is implemented inJava and it uses lp-solve software (Berkelaar,1999) in order to perform Linear Programming.The input for our system is initial collection oftexts to be summarized.
Four main parts of POLY2are: preprocessing, linear program generation, lin-ear program application and testing.
Data flow ofthe system is depicted in Figure 1.The preprocessing step consists of followingparts.
During the very first step, initial documentsundergo stop word removal, stemming and syn-onym resolution (if available for the chosen lan-guage).
Then, a sentence-term matrix is gener-ated in the form of a text file, where every linedescribes a term and every column ?
a sentence.Also, the index list for multi-document summa-rization is generated.
In this list serial number ofeach sentence is paired up with its serial numberin its document.
This information is used laterin order to decide how close each sentence is toits document?s boundaries.
Finally, we generatelist of bigrams (a consecutive appearance of twoterms) for every sentence.
All of the files gener-47Figure 1: Data flow of the POLY2 system.Figure 2: Linear program generated by POLY2system.ated during preprocessing are text files.The main part of POLY2 is linear programgeneration.
The system allows to select documentmatrix files and auxiliary files and objective func-tion and generates a system of linear inequalitiestogether with an objective function in format ac-ceptable by lp-solve software.
Figure 2 shows asample LP file contents.
Note that file generationfor every one of our objective function takes onlyseconds for a single documents.The next step is to run linear program and ex-tract its results.
We use lp-solve Java API to per-form this task and extract coordinates of point xthat optimizes the chosen objective function.
Inorder to construct the summary, we measure thenormalized distance from point x to every one ofthe sentence hyperplanes.
Since this informationis readily available through lp-solve API, the taskrequires sorting of n real numbers, where n is thenumber of sentences in all of the documents to-gether.
Hyperplanes whose distance to x is mini-mal represent the sentences participating in the fi-nal summary.
Running time of this step for a sin-gle file does not exceed three seconds.The final (optional) step is to verify gener-ated summaries, that can be performed with thehelp of any evaluation system.
In our case, theROUGE (Lin, 2004) system has been used.4 Conclusions and Future WorkIn this paper we present an extractive summariza-tion system POLY2 based on a linear program-ming model.
We represent the document as a setof intersecting hyperplanes.
Every possible sum-mary of a document is represented as the inter-section of two or more hyperlanes.
We considerthe summary to be the best if the optimal value ofobjective function is preserved during summariza-tion, and translate the summarization problem intoa problem of finding a point on a convex polytopewhich is the closest to the hyperplane describingthe ?ideal?
summary.
We introduce multiple ob-jective functions describing the distance betweena summary (a point on a convex polytope) andthe best summary (the hyperplane).
Since the in-troduced objectives behaves differently on differ-ent languages, only two of them were indicatedas primary systems and evaluated by MultiLing2013 organizers?OBJPOS F1 and OBJ3?denotedby ID5 and ID51.Below we summarize the results of automatedevaluations in MultiLing 2013 (ROUGE-1,2,3,4and three N-gram graph methods) for POLY2 inthree languages.English: 7th place in all ROUGE metrics(stemmed) and AutoSummENG, and 8th place inMeMoG and NPowER (out of 10 systems);Hebrew: 3rd place in ROUGE-1, 5th place inROUGE-2 and MeMoG, 4th rank in ROUGE-3and ROUGE-4, and only 6th rank in terms of Au-toSummENG and NPowER (out of 9 participants);Arabic: 7th rank in ROUGE-1,2 and all NGGmetrics, 6th rank in terms of ROUGE-3, and 5thplace in ROUGE-4 (out of 10 summarizers).As it can be seen, the best performance forPOLY2 has been achieved on the dataset of He-brew documents.Since fractional linear programming problemcan be solved in polynomial time (Karmarkar,1984), the time complexity of our approach ispolynomial.48AcknowledgmentsAuthors thank Igor Vinokur for the software main-tenance and running of experiments.ReferencesMichel Berkelaar.
1999. lp-solve free soft-ware.
http://lpsolve.sourceforge.net/5.5/.Dan Gillick and Benoit Favre.
2009.
A ScalableGlobal Model for Summarization.
In Proceedingsof the NAACL HLT Workshop on Integer Linear Pro-gramming for Natural Language Processing, pages10?18.Yoshihiro Matsuo Hitoshi Nishikawa,Takaaki Hasegawa and Genichiro Kikui.
2010.Opinion Summarization with Integer Linear Pro-gramming Formulation for Sentence Extraction andOrdering.
In Coling 2010: Poster Volume, pages910?918.N.
Karmarkar.
1984.
New polynomial-time algorithmfor linear programming.
Combinatorica, 4:373?395.L.
G. Khachiyan and M. J. Todd.
1993.
On the com-plexity of approximating the maximal inscribed el-lipsoid for a polytope.
Mathematical Programming,61:137?159.L.
G. Khachiyan.
1996.
Rounding of polytopes in thereal number model of computation.
Mathematics ofOperations Research, 21:307?320.Samir Khuller, Anna Moss, and Joseph (Seffi) Naor.1999.
The budgeted maximum coverage problem.Information Precessing Letters, 70(1):39?45.Chin-Yew Lin.
2004.
Rouge: a package for auto-matic evaluation of summaries.
In Workshop onText Summarization Branches Out, Post-ConferenceWorkshop of ACL 2004, pages 25?26.Takuya Makino, Hiroya Takamura, and Manabu Oku-mura.
2011.
Balanced coverage of aspects for textsummarization.
In TAC ?11: Proceedings of TextAnalysis Conference.G.
Salton, C. Yang, and A. Wong.
1975.
A vector-space model for information retrieval.
Communica-tions of the ACM, 18.Hiroya Takamura and Manabu Okumura.
2009.
Textsummarization model based on maximum coverageproblem and its variant.
In EACL ?09: Proceed-ings of the 12th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 781?789.Kristian Woodsend and Mirella Lapata.
2010.
Auto-matic Generation of Story Highlights.
In ACL ?10:Proceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, pages 565?574.49
