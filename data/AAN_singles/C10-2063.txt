Coling 2010: Poster Volume, pages 552?560,Beijing, August 2010Local Space-Time Smoothing for Version Controlled DocumentsSeungyeon KimGeorgia Institute of TechnologyGuy LebanonGeorgia Institute of TechnologyAbstractUnlike static documents, version con-trolled documents are continuously editedby one or more authors.
Such collabo-rative revision process makes traditionalmodeling and visualization techniques in-appropriate.
In this paper we propose anew representation based on local space-time smoothing that captures importantrevision patterns.
We demonstrate the ap-plicability of our framework using experi-ments on synthetic and real-world data.1 IntroductionMost computational linguistics studies concen-trate on modeling or analyzing documents as se-quences of words.
In this paper we considermodeling and visualizing version controlled doc-uments which is the authoring process leading tothe final word sequence.
In particular, we focus ondocuments whose authoring process naturally seg-ments into consecutive versions.
The revisions, asthe differences between consecutive versions areoften called, may be authored by a single authoror by multiple authors working collaboratively.One popular way to keep track of version con-trolled documents is using a version control sys-tem such as CVS or Subversion (SVN).
This isoften the case with books or with large com-puter code projects.
In other cases, more special-ized computational infrastructure may be avail-able, as is the case with the authoring API ofWikipedia.org, Slashdot.com, and Google Wave.Accessing such API provides information aboutwhat each revision contains, when was it sub-mitted, and who edited it.
In any case, we for-mally consider a version controlled document asa sequence of documents d1, .
.
.
, dl indexed bytheir revision number where di typically containssome locally concentrated additions or deletions,as compared to di?1.In this paper we develop a continuous represen-tation of version controlled documents that gener-alizes the locally weighted bag of words represen-tation (Lebanon et al, 2007).
The representationsmooths the sequence of version controlled doc-uments across two axes-time t and space s. Thetime axis t represents the revision and the spaceaxis s represents document position.
The smooth-ing results in a continuous map from a space-timedomain to the simplex of term frequency vectors?
: ?
?
PV where ?
?
R2, and (1)PV =??
?w ?
R|V | : wi ?
0,|V |?i=1wi = 1???
.The mapping above (V is the vocabulary) cap-tures the variation in the local distribution of wordcontent across time and space.
Thus [?
(s, t)]w isthe (smoothed) probability of observing word win space s (document position) and time t (ver-sion).
Geometrically, ?
realizes a divergence-freevector field (since ?w[?
(s, t)]w = 1, ?
has zerodivergence) over the space-time domain ?.We consider the following four version con-trolled document analysis tasks.
The first task isvisualizing word-content changes with respect tospace (how quickly the document changes its con-tent), time (how much does the current versiondiffers from the previous one), or mixed space-time.
The second task is detecting sharp transi-tions or edges in word content.
The third taskis concerned with segmenting the space-time do-main into a finite partition reflecting word content.The fourth task is predicting future revisions.
Ourmain tool in addressing tasks 1-4 above is to an-alyze the values of the vector field ?
and its first552order derivatives fields??
= (?
?s, ?
?t) .
(2)2 Space-Time Smoothing for VersionControlled DocumentsWith no loss of generality we identify the vocabu-lary V with positive integers {1, .
.
.
, V } and rep-resent a word w ?
V by a unit vector1 (all zeroexcept for 1 at the w-component)e(w) = (0, .
.
.
, 0, 1, 0, .
.
.
, 0)> w ?
V. (3)We extend this definition to word sequencesthus representing documents ?w1, .
.
.
, wN ?
(wi ?V ) as sequences of V -dimensional vectors?e(w1), .
.
.
, e(wN )?.
Similarly, a version con-trolled document is sequence of documentsd(1), .
.
.
, d(l) of potentially different lengthsd(j) = ?w(j)1 , .
.
.
, w(j)N(j)?.
Using (3) we representa version controlled document as the arraye(w(1)1 ), .
.
.
, e(w(1)N(1)).........e(w(l)1 ), .
.
.
, e(w(l)N(l))(4)where columns and rows correspond to space(document position) and time (versions).The array (4) of high dimensional vectors repre-sents the version controlled document without anyloss of information.
Nevertheless the high dimen-sionality of V suggests we smooth the vectors in(4) with neighboring vectors in order to better cap-ture the local word content.
Specifically we con-volve each component of (4) with a 2-D smooth-ing kernel Kh to obtain a smooth vector field ?over space-time (Wand and Jones, 1995) e.g.,?
(s, t) =?s??t?Kh(s?
s?, t?
t?)e(w(t?)s?
)Kh(x, y) ?
exp(?
(x2 + y2)/(2h2)).
(5)Thus as (s, t) vary over a continuous domain ?
?R2, ?
(s, t), which is a weighted combination ofneighboring unit vectors, traces a continuous sur-face in PV ?
RV .
Assuming that the kernelKh is a normalized density it can be shown that1Note the slight abuse of notation as V represents both aset of words and an integer V = {1, .
.
.
, V } with V = |V |.?
(s, t) is a non-negative normalized vector i.e.,?
(s, t) ?
PV (see (1) for a definition of PV ) mea-suring the local distribution of words around thespace-time location (s, t).
It thus extends the con-cept of lowbow (locally weighted bag of words)introduced in (Lebanon et al, 2007) from singledocuments to version controlled documents.One difficulty with the above scheme is thatthe document versions d1, .
.
.
, dl may be of dif-ferent lengths.
We consider two ways to resolvethis issue.
The first pads shorter document ver-sions with zero vectors as needed.
We refer to theresulting representation ?
as the non-normalizedrepresentation.
The second approach normalizesall document versions to a common length, say?lj=1 N(j).
That is each word in the first doc-ument is expanded into?j 6=1 N(j) words, eachword in the second document is expanded into?j 6=2 N(j) words etc.
We refer to the resultingrepresentation ?
as the normalized representation.The non-normalized representation has the ad-vantage of conveying absolute lengths.
For ex-ample, it makes it possible to track how differ-ent portions of the document grow or shrink (interms of number of words) with the version num-ber.
The normalized representation has the advan-tage of conveying lengths relative to the documentlength.
For example, it makes it possible to trackhow different portions of the document grow orshrink with the version number relative to the to-tal document length.
In either case, the space-timedomain ?
on which ?
is defined (5) is a two di-mensional rectangular domain ?
= [0, I]?
[0, J ].Before proceeding to examine how ?
may beused in the four tasks described in Section 1 wedemonstrate our framework with a simple low di-mensional example.
Assuming a vocabulary oftwo words V = {1, 2} we can visualize ?
bydisplaying its first component as a grayscale im-age (since [?
(s, t)]2 = 1 ?
[?
(s, t)]1 the sec-ond component is redundant).
Specifically, wecreated a version controlled document with threecontiguous segments whose {1, 2} words weresampled from Bernoulli distributions with param-eters 0.3 (first segment), 0.7 (second segment),and 0.5 (third segment).
That is, the probabilityof getting 1 is highest for the second segment,equal for the third and lowest for the first seg-ment.
The initial lengths of the segments were553Figure 1: Four space-time representations of a simple synthetic version controlled document over V = {1, 2} (see textfor more details).
The left panel displays the first component of (4) (non-smoothed array of unit vectors corresponding towords).
The second and third panels display [?
(s, t)]1 for the non-normalized and normalized representations respectively.The fourth panel displays the gradient vector field (?
?s(s, t), ?
?t(s, t)) (contour levels represent the gradient magnitude).
Theblack portions of the first two panels correspond to zero padding due to unequal lengths of the different versions.30, 40 and 120 words with the first segment in-creasing and the third segment decreasing at halfthe rate of the first segment with each revision.The length of the second segment was constantacross the different versions.
Figure 1 displaysthe nonsmoothed ragged array (4) (left), the non-normalized [?
(s, t)]1 (middle left) and the normal-ized [?
(s, t)]1 (middle right).While the left panel doesn?t distinguish muchbetween the second and third segment the twosmoothed representations display a nice seg-mentation of the space-time domain into threesegments, each with roughly uniform values.The non-normalized representation (middle left)makes it easy to see that the total length of theversion controlled document is increasing but itis not easy to judge what happens to the relativesizes of the three segments.
The normalized rep-resentation (middle right) makes it easy to see thatthe first segment increases in size, the second isconstant, and the third decreases in size.
It is alsopossible to notice that the growth rate of the firstsegment is higher than the decay rate of the third.3 Visualizing Change in Space-TimeWe apply the space-time representation to fourtasks.
The first task, visualizing change, is de-scribed in this section.
The remaining three tasksare described in the next three section.The space-time domain ?
represents the unionof all document versions and all document posi-tions.
Some parts of ?
are more homogeneousand some are less in terms of their local word dis-tribution.
Locations in ?
where the local worddistribution substantially diverges from its neigh-bors correspond to sharp content transitions.
Onthe other hand, locations whose word distributionis more or less constant correspond to slow con-tent variation.We distinguish between three different types ofchanges.
The first occurs when the word contentchanges substantially between neighboring doc-ument positions within a certain document ver-sion.
As an example consider a document loca-tion whose content shifts from high level introduc-tory motivation to a detailed technical description.Such change is represented by??
?s(s, t)?2 =V?w=1(?[?
(s, t)]w?s)2.
(6)A second type of change occurs when a certaindocument position undergoes substantial changein local word distribution across neighboring ver-sions.
An example is erroneous content in oneversion being heavily revised in the next version.Such change along the time axis corresponds tothe magnitude of??
?t(s, t)?2 =V?w=1(?[?
(s, t)]w?t)2.
(7)Expression (6) may be used to measure the in-stantaneous rate of change in the local word dis-tribution.
Alternatively, integrating (6) provides aglobal measure of changeh(s) =???
?s(s, t)?2 dt, g(t) =???
?t(s, t)?2 dswith h(s) describing the total amount of spatialchange across all revisions and g(t) describing554Figure 2: Gradient and edges for a portion of the version controlled Wikipedia Religion article.
The left panel displays??
?s(s, t)?2 (amount of change across document locations for different versions).
The second panel displays ??
?t(s, t)?2(amount of change across versions for different document positions).
The third panel displays the local maxima of??
?s(s, t)?2 + ??
?t(s, t)?2 which correspond to potential edges, either vertical lines (section and subsection boundaries) orhorizontal lines (between substantial revisions).
The fourth panel displays boundaries of sections and subsections as blackand gray lines respectively.the total amount of version change across differ-ent document positions.
h(s) may be used to de-tect document regions undergoing repeated sub-stantial content revisions and g(t) may be used todetect revisions in which substantial content hasbeen modified across the entire document.We conclude with the integrated directionalderivative?
10???s(r)??s(?
(r)) + ??t(r)??t(?
(r))?2 dr (8)where ?
: [0, 1] ?
?
is a parameterized curve inthe space-time and ??
its tangent vector.
Expres-sion (8) may be used to measure change along adynamically moving document anchor such as theboundary between two book chapters.
The spacecoordinate of such anchor shifts with the versionnumber (due to the addition and removal of con-tent across versions) and so integrating the gra-dient across one of the two axis as in (7) is notappropriate.
Defining ?
(r) to be a parameterizedcurve in space-time realizing the anchor positions(s, t) ?
?
across multiple revisions, (8) measuresthe amount of change at the anchor point.3.1 ExperimentsThe right panel of Figure 1 shows the gradientvector field corresponding to the synthetic ver-sion controlled document described in the previ-ous section.
As expected, it tends to be orthog-onal to the segment boundaries.
Its magnitude isdisplayed by the contour lines which show highestmagnitudes around segment boundaries.Figure 2 shows the norm ??
?s(s, t)?2 (left),??
?t(s, t)?2 (middle left) and the local maximaof ??
?s(s, t)?2 + ??
?t(s, t)?2 (middle right) for aportion of the version controlled Wikipedia Re-ligion article.
The first panel shows the amountof change in local word distribution within doc-uments.
High values correspond to boundariesbetween sections, topics or other document seg-ments.
The second panel shows the amount ofchange as one version is replaced with another.It shows which revisions change the word distri-butions substantially and which result in a rela-tively minor change.
The third panel shows onlythe local maxima which correspond to edges be-tween topics or segments (vertical lines) or revi-sions (horizontal lines).4 Edge DetectionIn many cases documents may be divided tosemantically coherent segments.
Examples oftext segments include individual news stories instreaming broadcast news transcription, sectionsin article or books, and individual messages in adiscussion board or an email trail.
For non-versioncontrolled documents finding the text segments isequivalent to finding the boundaries or edges be-tween consecutive segments.
See (Hearst, 1997;Beeferman et al, 1999; McCallum et al, 2000)for several recent studies in this area.Things get a bit more complicated in the caseof version controlled documents.
Segments, andtheir boundaries exist in each version.
As incase of image processing, we may view segmentboundaries as edges in the space-time domain?.
These boundaries separate the segments fromeach other, much like borders separate countries555Figure 3: Gradient and edges of a portion of the version controlled Atlanta Wikipedia article (top row) and the GoogleWave Amazon Kindle FAQ (bottom row).
The left column displays the magnitude of the gradient in both space and time??
?s(s, t)?2 + ??
?t(s, t)?.
The middle column displays the local maxima of the gradient magnitude (left column).
Theright column displays the actual segment boundaries as vertical lines (section headings for Wikipedia and author change inGoogle Wave).
The gradient maxima corresponding to vertical lines in the middle column matches nicely the Wikipediasection boundaries.
The gradient maxima corresponding to horizontal lines in the middle column correspond nicely to majorrevisions indicated by a discontinuities in the location of the section boundaries.in a two dimensional geographical map.Assuming all edges are correctly identified, wecan easily identify the segments as the interiorpoints of the closed boundaries.
In general, how-ever, attempts to identify segment boundaries oredges will only be partially successful.
As a resultpredicted edges in practice are not closed and donot lead to interior segments.
We consider now thetask of predicting segment boundaries or edges in?
and postpone the task of predicting a segmenta-tion to the next section.Edges, or transitions between segments, corre-spond to abrupt changes in the local word dis-tribution.
We thus characterize them as pointsin ?
having high gradient value.
In particu-lar, we distinguish between vertical edges (transi-tions across document positions), horizontal edges(transitions across versions), and diagonal edges(transitions across both document position andversion).
These three types of edges may be di-agnosed based on the magnitudes of ?
?s, ?
?t, and?
?1?s + ?
?2?t respectively.4.1 ExperimentsBesides the synthetic data results in Figure 2,we conducted edge detection experiments on sixdifferent real world datasets.
Five datasets areWikipedia.com articles: Atlanta, Religion, Lan-guage, European Union, and Beijing.
Religionand European Union are version controlled docu-ments with relatively frequent updates, while At-lanta, language, and Beijing have less frequentchanges.
The sixth dataset is the Google WaveAmazon Kindle FAQ which is a less structuredversion controlled document.Preprocessing included removing html tags andpictures, word stemming, stop-word removal, andremoving any non alphabetic characters (numbersand punctuations).
The section heading informa-tion of Wikipedia and the information of authorof each posting in Google Wave is used as groundtruth for segment boundaries.
This informationwas separated from the dataset and was used fortraining and evaluation (on testing set).Figure 3 displays a gradient information, localmaxima, and ground truth segment boundaries for556Article Rev.
Voc.
p(y) Error Rate F1 MeasureSize a b c a b cAtlanta 2000 3078 0.401 0.401 0.424 0.339 0.000 0.467 0.504Religion 2000 2880 0.403 0.404 0.432 0.357 0.000 0.470 0.552Language 2000 3727 0.292 0.292 0.450 0.298 0.000 0.379 0.091European Union 2000 2382 0.534 0.467 0.544 0.435 0.696 0.397 0.663Beijing 2000 3857 0.543 0.456 0.474 0.391 0.704 0.512 0.682Amazon Kindle FAQ 100 573 0.339 0.338 0.522 0.313 0.000 0.436 0.558Figure 4: Test set error rate and F1 measure for edge prediction (section boundaries in Wikipedia articles and authorchange in Google Wave).
The space-time domain ?
was divided to a grid with each cell labeled edge (y = 1) or no edge(y = 0) depending on whether it contained any edges.
Method a corresponds to a predictor that always selects the majorityclass.
Method b corresponds to the TextTiling test segmentation algorithm (Hearst, 1997) without paragraph boundariesinformation.
Method c corresponds to a logistic regression classifier whose feature set is composed of statistical summaries(mean, median, max, min) of ?
?s(s, t) within the grid cell in question as well as neighboring cells.the version controlled Wikipedia articles Religionand Atlanta.
The local gradient maxima nicelymatch the segment boundaries which lead us toconsider training a logistic regression classifier ona feature set composed of gradient value statis-tics (min, max, mean, median of ??
?s(s, t)?
in theappropriate location as well as its neighbors (thespace-time domain?was divided into a finite gridwhere each cell either contained an edge (y = 1)or did not (y = 0)).
The table in Figure 4 displaysthe test set accuracy and F1 measure of three pre-dictors: our logistic regression (method c) as wellas two baselines: predicting edge/no-edge basedon the marginal p(y) distribution (method a) andTextTiling (method b) (Hearst, 1997) which is apopular text segmentation algorithm.
Since we donot assume paragraph information in our experi-ment we ignored this component and consideredthe document as a sequence with w = 20 and29 minimum depth gaps parameters (see (Hearst,1997)).
We conclude from the figure that the gra-dient information leads to better prediction thanTextTiling (on both accuracy and F1 measure).5 SegmentationAs mentioned in the previous section, predictingedges may not result in closed boundaries.
It ispossible to analyze the location and direction ofthe predicted edges and aggregate them into a se-quence of closed boundaries surrounding the seg-ments.
We take a different approach and partitionpoints in ?
to k distinct values or segments basedon local word content and space-time proximity.For two points (s1, t2), (s2, t2) ?
?
to be in thesame segment we expect ?
(s1, t1) to be similar to?
(s2, t2) and for (s1, t1) to be close to (s2, t2).The first condition asserts that the two locationsdiscuss the same topic.
The second condition as-serts that the two locations are not too far fromeach other in the space time domain.
More specif-ically, we propose to segment ?
by clustering itspoints based on the following geometryd((s1, t1), (s2, t2)) = dH(?
(s1, t1), ?
(s2, t2))+?c1(s1 ?
s2)2 + c2(t1 ?
t2)2 (9)where dH : PV ?
PV ?
R is Hellinger distanced2H(u, v) =V?i=1(?ui ??vi)2.
(10)The weights c1, c2 are used to balance the contri-butions of word content similarity with the simi-larity in time and space.5.1 ExperimentsFigure 5 displays the ground truth segment bound-aries and the segmentation results obtained by ap-plying k-means clustering (k = 11) to the metric(9).
The figure shows that the predicted segmentslargely match actual edges in the documents eventhough no edge or gradient information was usedin the segmentation process.6 Predicting Future OperationsThe fourth and final task is predicting a futurerevision dl+1 based on the smoothed representa-tion of the present and past versions d1, .
.
.
, dl.
In557Figure 5: Predicted segmentation (top) and ground truth segment boundaries (bottom) of portions of the version controlledWikipedia articles Religion (left), Atlanta (middle) and the Google Wave Amazon Kindle FAQ(right).
The predicted segmentsmatch the ground truth segment boundaries.
Note that the first 100 revisions are used in Google Wave result.
The proportionof the segments that appeared in the beginning is keep decreasing while the revisions increases and new segments appears.terms of ?, this means predicting features associ-ated with ?
(s, t), t ?
t?
based on ?
(s, t), t < t?.6.1 ExperimentsWe concentrate on predicting whether Wikipediaedits are reversed in the next revision.
This ac-tion, marked by a label UNDO or REVERT in theWikipedia API, is important for preventing con-tent abuse or removing immature content (by pre-dicting ahead of time suspicious revisions).We predict whether a version will undergoUNDO in the next version using a support vec-tor machine based on statistical summaries (mean,median, min, max) of the following featureset ??
?s(s, t)?, ??
?s(s, t)?, ??
?t(s, t)?
), ??
?t(s, t)?,g(h), and h(s).
Figure 6 shows the test set er-ror and F1 measure for the logistic regressionbased on the smoothed space-time representation(method c), as well as two baselines.
The firstbaseline (method a) predicts the majority classand the second baseline (method b) is a logisticregression based on the term frequency content ofthe current test version.
Using the derivatives of?, we obtain a prediction that is better than choos-ing majority class or logistic regression based onword content.
We thus conclude that the deriva-tives above provide more useful information (re-sulting in lower error and higher F1) for predictingfuture operations than word content features.7 Related WorkWhile document analysis is a very active researcharea, there has been relatively little work on ex-amining version controlled documents.
Our ap-proach is the first to consider version controlleddocuments as continuous mappings from a space-time domain to the space of local word distribu-tions.
It extends the ideas in (Lebanon et al, 2007)of using kernel smoothing to create a continuousrepresentation of documents.
In fact, our frame-work generalizes (Lebanon et al, 2007) as it re-verts to it in the case of a single revision.Other approaches to sequential analysis of doc-uments concentrate on discrete spaces and dis-crete models, with the possible extension of(Wang et al, 2009).
Related papers on segmenta-tion and sequential document analysis are (Hearst,558Article Rev.
Voc.
p(y) Error Rate F1 MeasureSize a b c a b cAtlanta 2000 3078 0.218 0.219 0.313 0.212 0.000 0.320 0.477Religion 2000 2880 0.123 0.122 0.223 0.125 0.000 0.294 0.281Language 2000 3727 0.189 0.189 0.259 0.187 0.000 0.334 0.455European Union 2000 2382 0.213 0.208 0.331 0.209 0.000 0.275 0.410Beijing 2000 3857 0.137 0.137 0.219 0.136 0.000 0.247 0.284Figure 6: Error rate and F1 measure over held out test set of predicting future UNDO operation in Wikipedia articles.Method a corresponds to a predictor that always selects the majority class.
Method b corresponds to a logistic regressionbased on the term frequency vector of the current version.
Method c corresponds a logistic regression that uses summaries(mean, median, max, min) of ??
?s(s, t)?, ??
?s(s, t)?, g(t), and h(s).1997; Beeferman et al, 1999; McCallum et al,2000) with (Hearst, 1997) being the closest inspirit to our approach.
An influential model fortopic modeling within and across documents is la-tent Dirichlet alocation (Blei et al, 2003; Bleiand Lafferty, 2006).
Our approach differs in be-ing fully non-parametric and in that it does notrequire iterative parametric estimation or integra-tion.
The interpretation of local word smoothingas a non-parametric statistical estimator (Lebanonet al, 2007) may be extended to our paper in astraightforward manner.Several attempts have been made to visualizethemes and topics in documents, either by keep-ing track of the word distribution or by dimen-sionality reduction techniques e.g., (Fortuna et al,2005; Havre et al, 2002; Spoerri, 1993; Thomasand Cook, 2005).
Such studies tend to visualize acorpus of unrelated documents as opposed to or-dered collections of revisions which we explore.8 Summary and DiscussionThe task of analyzing and visualizing version con-trolled document is an important one.
It allowsexternal control and monitoring of collaborativelyauthored resources such as Wikipedia, GoogleWave, and CVS or SVN documents.
Our frame-work is the first to develop analysis and visualiza-tion tools in this setting.
It presents a new rep-resentation for version controlled documents thatuses local smoothing to map a space-time domain?
to the simplex of tf vectors PV .
We demon-strate the applicability of the representation forfour tasks: visualizing change, predicting edges,segmentation, and predicting future revision oper-ations.Visualizing changes may highlight significantstructural changes for the benefit of users and helpthe collaborative authoring process.
Improvededge prediction and text segmentation may assistin discovering structural or semantic changes andtheir evolution with the authoring process.
Pre-dicting future operation may assist authors as wellas prevent abuse in coauthoring projects such asWikipedia.The experiments described in this paper wereconducted on synthetic, Wikipedia and GoogleWave articles.
They show that the proposed for-malism achieves good performance both qualita-tively and quantitatively as compared to standardbaseline algorithms.It is intriguing to consider the similarity be-tween our representation and image processing.Predicting segment boundaries are similar to edgedetection in images.
Segmenting version con-trolled documents may be reduced to image seg-mentation.
Predicting future operations is similarto completing image parts based on the remain-ing pixels and a statistical model.
Due to its longand successful history, image processing is a goodcandidate for providing useful tools for versioncontrolled document analysis.
Our framework fa-cilitates this analogy and we believe is likely to re-sult in novel models and analysis tools inspired bycurrent image processing paradigms.
A few po-tential examples are wavelet filtering, image com-pression, and statistical models such as Markovrandom fields.AcknowledgementsThe research described in this paper was fundedin part by NSF grant IIS-0746853.559ReferencesBeeferman, D., A. Berger, and J. D. Lafferty.
1999.Statistical models for text segmentation.
MachineLearning, 34(1-3):177?210.Blei, D. and J. Lafferty.
2006.
Dynamic topic models.In Proc.
of the International Conference onMachineLearning.Blei, D., A. Ng, , and M. Jordan.
2003.
Latent dirich-let alocation.
Journal of Machine Learning Re-search, 3:993?1022.Fortuna, B., M. Grobelnik, and D. Mladenic.
2005.Visualization of text document corpus.
Informatica,29:497?502.Havre, S., E. Hetzler, P. Whitney, and L. Nowell.
2002.Themeriver: Visualizing thematic changes in largedocument collections.
IEEE Transactions on Visu-alization and Computer Graphics, 8(1).Hearst, M. A.
1997.
Texttiling: Segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1):33?64.Lebanon, G., Y. Mao, and J. Dillon.
2007.
The lo-cally weighted bag of words framework for doc-uments.
Journal of Machine Learning Research,8:2405?2441, October.McCallum, A., D. Freitag, and F. Pereira.
2000.
Max-imum entropy Markov models for information ex-traction and segmentation.
In Proc.
of the Interna-tional Conference on Machine Learning.Spoerri, A.
1993.
InfoCrystal: A visual tool for infor-mation retrieval.
In Proc.
of IEEE Visualization.Thomas, J. J. and K. A. Cook, editors.
2005.
Illu-minating the Path: The Research and DevelopmentAgenda for Visual Analytics.
IEEE Computer Soci-ety.Wand, M. P. and M. C. Jones.
1995.
Kernel Smooth-ing.
Chapman and Hall/CRC.Wang, C., D. Blei, and D. Heckerman.
2009.
Continu-ous time dynamic topic models.
In Proc.
of Uncer-tainty in Artificial Intelligence.560
