Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 205?213,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsLeveraging Crowdsourcing for Paraphrase RecognitionMartin TschirsichDepartment of Computer Science,TU Darmstadtm.tschirsich@gmx.deGerold HintzDepartment of Computer Science,TU Darmstadtgerold.hintz@googlemail.comAbstractCrowdsourcing, while ideally reducingboth costs and the need for domain ex-perts, is no all-purpose tool.
We reviewhow paraphrase recognition has benefitedfrom crowdsourcing in the past and iden-tify two problems in paraphrase acqui-sition and semantic similarity evaluationthat can be solved by employing a smartcrowdsourcing strategy.
First, we employthe CrowdFlower platform to conduct anexperiment on sub-sentential paraphraseacquisition with early exclusion of low-accuracy crowdworkers.
Second, we com-pare two human intelligence task designsfor evaluating phrase pairs on a semanticsimilarity scale.
While the first experimentconfirms our strategy successful at tack-ling the problem of missing gold in para-phrase generation, the results of the sec-ond experiment suggest that, for both se-mantic similarity evaluation on a contin-uous and a binary scale, querying crowd-workers for a semantic similarity value ona multi-grade scale yields better resultsthan directly asking for a binary classifi-cation.1 IntroductionParaphrase recognition1 means to analysewhether two texts are paraphrastic, i.e.
?a pairof units of text deemed to be interchangeable?
(Dras, 1999).
It has numerous applications ininformation retrieval, information extraction,machine translation and plagiarism detection.For instance, an internet search provider couldrecognize "murder of the 35th U.S. president"and "assassination of John F. Kennedy" to be1the terms paraphrase detection and paraphrase identifi-cation might be used insteadparaphrases of each other and thus yield the sameresult.
Paraphrase recognition is an open researchproblem and, even though having progressedimmensely in recent years (Socher et al 2011),state of the art performance is still below thehuman reference.In this research, we analyse how crowdsourcingcan contribute to paraphrase recognition.
Crowd-sourcing is the process of outsourcing a vastnumber of small, simple tasks, so called HITs2, toa distributed group of unskilled workers, so calledcrowdworkers3.
Reviewing current literature onthe topic, we identify two problems in paraphraseacquisition and semantic similarity evaluation thatcan be solved by employing a smart crowdsourc-ing strategy.
First, we propose how to reduceparaphrase generation costs by early exclusion oflow-accuracy crowdworkers.
Second, we comparetwo HIT designs for evaluating phrase pairs on acontinuous semantic similarity scale.
In order toevaluate our crowdsourcing strategies, we conductour own experiments via the CROWDFLOWER4platform.The rest of the paper is structured as follows.Section 2 first gives an overview of related workand lines out current approaches.
We then pro-ceed to our own experiments on crowdsourcingparaphrase acquisition (3.3) and semantic similar-ity evaluation (3.4).
Section 4 and 5 conclude thestudy and propose future work in the area of para-phrase recognition and crowdsourcing.2 Literature ReviewMany research fields rely on paraphrase recogni-tion and contribute to it, as there are many relatedconcepts.
These include inference rule discoveryfor question-answering and information retrieval(Lin and Pantel, 2001), idiom or multiword ex-2Human Intelligence Tasks3often referred to as turkers4http://crowdflower.com205pression acquisition (Fellbaum et al 2006) andidentification (Boukobza and Rappoport, 2009),machine translation evaluation (Snover et al2009), textual entailment recognition, and manymore.2.1 Paraphrase DefinitionThe notion of a paraphrase is closely related to theconcepts of semantic similarity and word ontol-ogy and an exact definition is not trivial.
Often,complex annotation guidelines and aggregated ex-pert agreements decide whether phrases are to beconsidered paraphrastic or not (Dolan and Brock-ett, 2005).
Formal definitions based e.g.
on a do-main theory and derivable facts (Burrows et al2013) have little practical relevance in paraphraserecognition.
In terms of the semantic similarityrelations ?equals?, ?restates?, ?generalizes?, ?spec-ifies?
and ?intersects?
(Marsi and Krahmer, 2010),?paraphrase?
is equated with ?restates?.It is important to note that in the context ofcrowdsourcing, we, as well as most authors, relyon the crowdworker?s intuition of what a para-phrase is.
Usually, only a limited list of examplesof desired valid paraphrases is given to the crowd-worker as a reference.2.2 Paraphrase RecognitionAccording to Socher et al(2011), paraphraserecognition ?determines whether two phrases ofarbitrary length and form capture the same mean-ing?.
Paraphrase recognition is mostly under-stood as a binary classification process, althoughrecently, some authors proposed a continuous se-mantic similarity measure (Madnani et al 2012).Competing paraphrase recognition approachesare often compared by their performance on theMicrosoft Research Paraphrase Corpus (MSRPC).Until 2011, simple features such as n-gram over-lap, dependency tree overlap as well as depen-dency tree edit distance produced the best resultsin terms of accuracy and F-measure values.
How-ever, algorithms based solely on such features cannot identify semantic equivalence of synonymouswords or phrases.
Therefore, some authors sub-sequently integrated Wordnet synonyms as wellas other corpus-based semantic similarity mea-sures.
The work of Madnani et al(2012) basedon the TERP machine translation evaluation met-ric (Snover et al 2009) using synonyms and sub-sentential paraphrases presents the current state ofthe art for paraphrase detection on the MSRPCFigure 1: Highest ranking accuracy and F-measure over time for paraphrase recognition onthe MSRPC with an inter-rater agreement amongsthuman annotators of 84%with an accuracy of 77.4% and F-measure of84.1%.
The inter-rater agreement amongst humanannotators of 84% on the MSRPC can be consid-ered as an upper bound for the accuracy that couldbe obtained using automatic methods (Fernandoand Stevenson, 2008).As has become apparent, modern paraphraserecognition algorithms are evaulated on and incor-porate semantic similarity measures trained on ac-quired paraphrases.
Therefore, we subsequentlygive an overview over established paraphrase ac-quisition approaches.2.3 Paraphrase AcquisitionParaphrase acquisition5 is the process of collectingor generating phrase-paraphrase pairs, often for agiven set of phrases.
All strategies require a sub-sequent verification of the acquired paraphrases,either done by experts or trusted crowdworkers.2.3.1 Sentential ParaphrasesMost literature on paraphrase acquisition dealswith sentential or sentence-level paraphrases.Bouamor et al(2012) identify five strategiessuch as the translation based methods (Zhou etal., 2006) using parallel corpora or alignment oftopic-clustered news articles (Dolan and Brockett,2005).Via Crowdsourcing In an outstanding ap-proach, Chen and Dolan (2011) collected para-phrases by asking crowdworkers to describe short5also referred to as paraphrase generation206videos.
A more cost-effective multi-stage crowd-sourcing framework was presented by Negri et al(2012) with the goal to increase lexical divergenceof the collected paraphrases.2.3.2 Sub-Sentential ParaphrasesIncorporating sub-sentential paraphrases in ma-chine translation metrics also used for paraphrasedetection has proven effective (Madnani et al2012).
A large corpus consisting of more than15 million sub-sentential paraphrases was assem-bled by Bannard and Callison-Burch (2005) usinga pivot-based paraphrase acquisition method.Via Crowdsourcing Buzek et al(2010) ac-quired paraphrases of sentence parts problematicfor translation systems using AMAZON MECHAN-ICAL TURK.
Bouamor et al(2012) collected sub-sentential paraphrases in the context of a web-based game.2.3.3 Passage-level paraphrasesPassage-level paraphrase acquisition has beentreated within the context of the evaluation labon uncovering plagiarism, authorship, and socialsoftware misuse (PAN) (Potthast et al 2010):Burrows et al(2013) acquired passage-levelparaphrases for the WEBIS-CPC-11 corpus viacrowdsourcing.2.4 Semantic Similarity EvaluationParaphrase verification can be said to be a man-ual semantic similarity evaluation done by expertsor trusted crowdworkers, most often on a binaryscale.
However, Madnani et al(2012) believethat ?binary indicators of semantic equivalenceare not ideal and a continuous value [.
.
. ]
in-dicating the degree to which two pairs are para-phrastic is more suitable for most approaches?.They propose averaging a large number of bi-nary crowdworker judgements or, alternatively, asmaller number of judgements on an ordinal scaleas in the SEMEVAL-2012 Semantic Textual Simi-larity (STS) task (Agirre et al 2012).
A continu-ous semantic similarity score is also used to weighthe influence of sub-sentential paraphrases used bythe TERP metric.3 Our Experiments3.1 The CrowdFlower PlatformCROWDFLOWER is a web service for HITproviders, abstracting from the actual platform onwhich these tasks are run.
A web interface, incor-porating a graphical editor as well as the CROWD-FLOWER MARKUP LANGUAGE6 (CML), can beused to model these tasks.
CROWDFLOWER pro-vides fine-grained controls over how these tasksare executed, for instance, by restricting crowd-workers to live in specific countries or by limitingthe number of HITs a single worker is allowed tocomplete.Furthermore, CROWDFLOWER provides a so-phisticated system to verify the correctness of thecollected data, aiming at early detection and ex-clusion of spammers and low-accuracy workersfrom the job: gold items.
Gold items consist of aHIT, e.g.
a pair of paraphrases together with one ormore possible valid answers.
Once gold items arepresent in the dataset, workers are prompted to an-swer these correctly before being eligible to workon the actual data.
Additionally, during the run ofa job, CROWDFLOWER uses hidden gold items torevise the trustworthiness of a human worker.3.2 Human Intelligence Task DesignApart from gold items, the actual HIT design hasthe biggest impact on the quality of the collecteddata.
Correct instructions as well as good exam-ples have a great influence on data quality.
By us-ing CML validation features, bad user input can beprevented from being collected in the first place.Care must also be taken not to introduce an artifi-cial bias by offering answer choices of different(time-)complexity.
Within our experiments, wefollowed common human interface design princi-ples such as colour coding answer options.3.3 Crowdsourcing Sub-SententialParaphrase AcquisitionThe biggest challenge in paraphrase acquisitionvia crowdsourcing is the low and varying accu-racy of the crowdworkers: ?The challenge [.
.
. ]
isautomatic quality assurance; without such meansthe crowdsourcing paradigm is not effective, andwithout crowdsourcing the creation of test cor-pora is unacceptably expensive for realistic orderof magnitudes?
(Burrows et al 2013).We propose a new crowdsourcing strategy thatallows for early detection of low-accuracy work-ers during the generation stage.
This preventsthese unwanted crowdworkers from completing6CML documentation: http://crowdflower.com/docs/cml207HITs that would almost certainly not be validatedlater on.
We focus on the acquisition of sub-sentential paraphrases for a given set of phrases,where pivot-based paraphrase acquisition methodsmight not be applicable.
Transferring our observa-tions to other types of paraphrases should be un-problematic.3.3.1 Phrase-Paraphrase GenerationFor this simple baseline strategy, we asked thecrowdworker to generate a short phrase along withits paraphrase (p1, p2) while providing a small setof examples.3.3.2 Two-Staged Paraphrase GenerationThis is the traditional crowdsourcing strategy.
Ina first generation stage, we presented the crowd-worker with a phrase p1 and asked for its para-phrase p2.
In a second validation stage, two orthree workers were asked to verify each gener-ated phrase-paraphrase pair until an unambigu-ous agreement was reached.
As the answers inthe validation stage are binary, gold-items wereadded to improve the accuracy of the collected val-idation judgements.
Negri et al(2012) showedthat after such a validation stage, expert ratersagreed in 92% of the cases with the aggregatedcrowdworker judgements.
However, the genera-tion stage is without gold and we cannot excludelow accuracy workers early enough not to costmoney.
We used the regular expression verifierprovided by CROWDFLOWER to ensure that thegenerated paraphrases contain at least one wordand are not equal to the given phrases.
Other thanthis however, the worker could enter any text.Input Phrases As input data, we required mean-ingful chunks.
For this, any constituent of a sen-tence can be used.
A small number of examplessuggested that verb phrases have a high potentialof yielding interesting paraphrases, as they oftenhave to be replaced as an isolated unit (?get a flu??
?catch a cold?).
Therefore, we extracted verbphrases of two to five words from a source cor-pus.
For this, we used the POS tagger of NLTK7(A Maxent Treebank POS tagger trained on PennTreebank) and a simple chunking grammar parser.Offering a Choice of Input Phrase A crowd-worker might not always be able to come up with aparaphrase for a given phrase.
If a worker receives7NATURAL LANGUAGE TOOLKIT (NLTK): http://nltk.org/phrasesverifyPara-phrases phrasesphrasesphrasesgenerate generatePara-phrases phrasesgenerate?verifyverifygenerate generate generateverifyFigure 2: Illustration of the multi-stage paraphrasegeneration processone chunk at a time, he has to deal with it no mat-ter how unfeasible it is for paraphrasing.
One so-lution to this problem would be to offer a back-outoption, in which a worker could declare a unit asunsolvable and possibly explain why.
This how-ever could easily be exploited by human workers,resulting in many unsolved items.
An alternativesolution is to offer workers a choice of the inputphrase they want to paraphrase.
We designed aHIT with a set of three different input phrases ofwhich they have to pick one to paraphrase.
If oneof these options is repeatedly declined by multipleworkers, we can declare it as bad, without havinga worker pass on a unit.
However, it turned out thatless than 1% proved unsolvable and we thereforedeemed such measures unnecessary.3.3.3 Multi-Staged Paraphrase GenerationWe improved the traditional two-stage approachby combining the generation and verificationsteps.
The task to decide whether a given pair is aparaphrase is combined with the task of paraphras-ing a chunk.
The matching of verification andgeneration items is arbitrary.
Figure 2 illustratesthis approach.
After an initial generate stage, sub-sequent stages are combined verify/generate jobs.The benefit of this approach is that verification of208phrase pairs allows the usage of gold-items.
Wecan now assess the trustworthiness of a crowd-worker through gold, and we indirectly infer theirability to paraphrase from their ability to decide iftwo items are paraphrases.
The aim of this processis to reduce the number of incorrect paraphrasesbeing generated in the first place, and thus improvethe efficiency of the CROWDFLOWER task.In contrast to Negri et al(2012), we did not re-strict access to the later stages of this job to high-accuracy workers of previous stages since our in-termingled gold-items are expected to filter outlow-accuracy workers in each succeeding stage.Therefore, we expect to attract contributors froma bigger pool of possibly cheaper workers.3.3.4 EvaluationWhile only 28% of the collected pairs were val-idated after the traditional two-staged paraphrasegeneration, this percentage increased to 80% inthe second validation stage belonging to the multi-stage approach.
Although the experiment wasconducted on a small number of phrases, this re-sult is a good indicator that our hypothesis is cor-rect and that a combined generation and verifi-cation stage with gold items can reduce costs byearly exclusion of low-accuracy workers.Lexical divergence measures (TERP) decline,but this is expected after filtering out pos-sibly highly divergent non-paraphrastic pairs.While our generation costs per non-validated sub-sentential paraphrase were around the same asthose reported by Buzek et al(2010) (0.024$), thecosts for validated sub-sentential paraphrases werenot much higher (0.06$).
Negri et al(2012) reportcosts of 0.27$ per sentential paraphrase, howeverthese costs are difficult to compare, also becausewe did not optimize for lexical divergence.3.4 Crowdsourcing Semantic SimilarityEvaluationWe conducted an experiment in order to determinehow to optimally query continuous semantic sim-ilarity scores from crowdworkers.
The two dif-ferent examined methods originally proposed byMadnani et al(2012) are binary and senary8 se-mantic similarity evaluation.
Paraphrases weretaken from the MSRPC.
Optimality was definedby two different criteria: First, we analysed howwell the (binary) paraphrase classification by do-main experts on the MSRPC can be reproduced8senary: {0, 1, 2, 3, 4, 5} as opposed to binary {0, 1}.from our collected judgements.
Second, we anal-ysed how consistent our collected judgements are.Since we could not find any reference corpusfor semantic similarity evaluation apart from theSEMEVAL-2012 STS gold that was also acquiredvia crowdsourcing, we resorted to training a ma-chine learning classifier and comparing relativeperformance on the collected training data.3.4.1 Binary Semantic SimilarityCrowdworkers were asked to give a binary clas-sification of two phrases as either paraphrastic ornon-paraphrastic.
Binary decisions were enforcedsince no third option was given.
Three examplesof valid paraphrases were given.A minimum of 20 judgements each for 207phrase pairs were collected for 0.01$ per judge-ment.
In order to deter spammers and the most in-accurate workers, we converted 14% of the phrasepairs - those with high expected inter-rater agree-ment - to gold items.
Low inter-rater agreementon a phrase pair hinted at medium, high inter-rateragreement hinted at low or high semantic similar-ity.
Trusted crowdworkers had an average gold ac-curacy of 93% on these gold items.3.4.2 Senary Semantic SimilarityCrowdworkers were asked to give a senary clas-sification of two phrases.
The six classes wereequivalent to those defined by the SemEval STStask.
A short annotation guide consisting of oneexample per category was provided.A minimum of 8 judgements each for 667phrase pairs were collected for 0.02$ per judge-ment.
In order to deter spammers and the most in-accurate workers, we converted 13% of the phrasepairs to gold items.
Gold items were accepted aslong as the judgement lay within an acceptablerange of an expected similarity value.3.4.3 Input Aggregation and NormalizationThe following two phrase pairs demonstrate therelationship between binary inter-rater agreementand aggregated senary semantic similarity:1.
?It appears that many employers accused ofworkplace discrimination will be consideredguilty until they can prove themselves inno-cent," he said.Employers accused of workplace dis-crimination now are considered guilty untilthey can prove themselves innocent.209Name Stage # Phrase Pairs TERPPhrase-Paraphrase Generation Generation 100 0.89Two-Staged Generation1.
Generation 378 0.852.
Validation 109 (28%) 0.68Multi-Staged Generation3.
Generation + Gold 165 0.724.
Validation 134 (80%) 0.64Table 1: Two-staged (1.
- 2.)
and multi-staged (1.
- 4.)
paraphrase generation results.
Percentage valuesdenote the amount of validated pairs relative to the preceding generation stage.2.
Sixteen days later, as superheated air fromthe shuttle?s reentry rushed into the damagedwing, "there was no possibility for crewsurvival," the board said.Sixteen days later, as superheated airfrom the shuttle?s re-entry rushed into thedamaged wing, there was no possibility forcrew survival, the board said.
?The binary inter-rater agreement for the firstphrase pair is low (10%), so crowdworkers seem-ingly could not decide between paraphrastic andnon-paraphrastic.
Accordingly, the averagedsenary semantic similarity takes an intermediatevalue (3.4).The binary inter-rater agreement for the sec-ond phrase pair however is very high (100%), sowe expect the sentences to be either clearly non-paraphrastic or clearly paraphrastic.
A maximalaveraged senary semantic similarity value of 5.0confirms this intuition.In order to make aggregated binary and senaryinput comparable, we scaled the binary judge-ments so that the sampled average and variancematched that of the senary judgements.
Thesesemantic similarities are strongly correlated (3a)with Pearson coefficient of 0.81 and seem to re-spect the MSRPC expert annotator rating withpositive correlation between aggregated semanticsimilarity and binary MSRPC classification.With reference to Denkowski and Lavie (2010),we used the following aggregation and normaliza-tion techniques:Straight Average The aggregated semantic sim-ilarity is the average of all collected judge-ments.
This is our baseline approach.Judge Normalization To compensate for differ-ent evaluation standards, each judge?s judge-ments are scaled so that its sample averageand variance matches that of the average (3b).Judge Outlier Removal Removing judgeswhose inter-rater agreement with the averageis less than 0.5; motivated by Agirre etal.
(2012): ?Given the high quality of theannotations among the turkers, we couldalternatively use the correlation betweenthe turkers itself to detect poor qualityannotators?.Weighted Voting Each judge?s judgements areweighted by its inter-rater agreement with theaverage.We also wanted to know whether limiting theamount of possible HITs or judgements percrowdworker could increase the quality of thecollected judgements.
However, while high-throughput crowdworkers showed lower variancein their agreement compared to crowdworkerswith a small number of completed HITs, correla-tion between the number of completed HITs andagreement was very weak (3c) with Pearson coef-ficient of 0.01.3.4.4 Machine Learning EvaluationWe trained the UKP machine learning classi-fier originally developed for the Semantic TextualSimilarity (STS) task at SemEval-2012 (B?r et al2012) on the averaged binary and senary judge-ments for 207 identical phrase pairs.
Since wewere not interested in the performance of the ma-chine learning classifier but in the quality of thecollected data, we measured the relative perfor-mance of the learned model on the training data.The number of training examples remained con-stant.
This was repeated multiple times whilevarying the number of judgements used in the ag-gregation of the semantic similarity values.
Weobserved that with increasing number of judge-ments, the correlation coefficient converges seem-ingly against an upper bound (binary: 0.68 for 20judgements, senary: 0.741 for 8 judgements).
The2101 2 3 4 512345Senary Semantic SimilarityBinarySemanticSimilarity(a) Correlation between aggregatedsenary and binary semantic similar-ity (black = paraphrases according toMSRPC)0 5 10 15012345Phrase PairJudgementJudgeAverageNormalized(b) Judge normalization0 200 400 600?0.200.20.40.60.811.2Number of HITsAgreement(c) The activity of a crowdworker doesnot correlate with agreement to the aver-ageFigure 3: Input aggregation and normalizationmachine learning classifier performs best whentrained on semantic similarity data collected on asenary scale (4).
Even if we only take the firstthree senary judgements per phrase pair into ac-count, it is still superior to 20 binary judgementsalthough the total amount of information queriedfrom the crowdworkers is much smaller.In a second step, we compared the perfor-mance while employing different input normaliza-tion techniques on the whole set of 667 phrasepairs with senary judgements.
While all tech-niques increased the trained classifier?s perfor-mance, weighted voting performed best (2).0 5 10 15 200.550.60.650.70.75# Judgements0 2 4 6 8AgreementBinarySenaryFigure 4: Machine learning results (agreement =correlation with training data)3.4.5 MSRPC EvaluationIn addition to the machine learning evaluation, wecompared our results to the binary semantic simi-larity classification given by the MSRPC expertannotators.
In order to do so, we had to findan optimal threshold in [0, 5] splitting our seman-tic similarity range in two, dividing paraphras-Technique CorrelationStraight Average 0.716Judge Outlier Removal 0.719Judge Normalization 0.721Weighted Voting 0.722Table 2: Input normalization resultstic from non-paraphrastic phrase pairs.
Again,this was repeated multiple times while varyingthe number of judgements used in the aggrega-tion of the semantic similarity values.
However,this time we did not simply take the first n judge-ments each, but averaged over different possiblesampling combinations.
We measured percentageagreement with MSRPC and the optimal thresh-old for non-weighted and weighted judgements,since weighted voting performed best in the ma-chine learning evaluation (5c).Surprisingly, even for binary paraphrastic-non-paraphrastic classification, querying a senary se-mantic similarity value from crowdworkers yieldsbetter results than directly asking for a binary clas-sification.
However, the results also indicate thatin both cases, input normalization plays an im-portant role and agreement could be improved bymore sophisticated or combined input normaliza-tion techniques as well as by collecting additionaljudgements.A semantic similarity of 3.1 (senary) (5a) re-spectively 3.5 (binary) (5b) corresponds opti-mally to the paraphrastic-non-paraphrastic thresh-old chosen by the MSRPC expert annotators.Costs per evaluated phrase pair were at 0.16$2110 1 2 3 4 5020406080(a) Optimal threshold for senarysemantic similarity is 3.10 1 2 3 4 5050100150(b) Optimal threshold for binarysemantic similarity is 3.50 4 8 12 16 200.780.80.820.840.860.88# JudgementsAgreementBinarySenaryWeightedWeighted0 2 4 6 8 10(c) Average agreement with MSRPCwith optimal threshold per number ofjudgementsFigure 5: MSRPC evaluation (agreement = percentual agreement with aggregated judgements)(senary, 8 judgements) compared to 0.20$ for theSEMEVAL-2012 STS task (senary, 5 judgements).However, we did not examine how this and possi-ble further cost reduction impacts agreement withMSRPC.4 ConclusionWe presented a multi-stage crowdsourcing ap-proach tackling the problem of missing gold inparaphrase generation.
This approach has shownto work very well for sub-sentential paraphrasegeneration and we strongly believe that it willwork equally well for sentential paraphrase gen-eration, resulting in significantly reduced costs ofparaphrase corpus creation.We also compared different crowdsourcing ap-proaches towards semantic similarity evaluation,showing that for both semantic similarity evalua-tion on a continuous and a binary scale, queryingan ordinal senary semantic similarity value fromcrowdworkers yields better results than directlyasking for a binary classification.5 Future WorkOur goal to sub-sentential paraphrase generationwas cost minimization by early removal of low-accuracy workers.
Apart from being grammaticaland paraphrastic, we did not enforce other qual-ity constraints on the collected data.
A combina-tion of our multi-stage approach with that of Ne-gri et al(2012) could prove successful if bothcost and quality, i.e.
lexical divergence betweenphrase-paraphrase pairs, are to be optimized.There is also room for reducing the cost ofthe verification stage e.g.
by automatically filter-ing out paraphrases before presenting them to acrowdworker using e.g.
lexical divergence, lengthof the sentence or other measures as it was doneby Burrows et al(2013).Another interesting question we could not an-swer due to budget constraints is: Can the crowdreplace the expert and if yes, how many crowd-workers are needed to do so reliably?
One pos-sible way to answer this question for paraphraseevaluation would be to collect semantic similarityjudgements for the whole MSRPC and to see howmany judgements per phrase are needed to reliablyreproduce the MSRPC classification results withan inter-rater agreement of 84% for the whole cor-pus.AcknowledgementsThe authors would like to thank Chris Biemannof TU Darmstadt, Germany, for pointing us to theproblem of paraphrase evaluation via crowdsourc-ing leading to this research as well as his supervi-sion and helpful suggestions.
We also thank ourreviewers for their feedback.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In *SEM 2012:The First Joint Conference on Lexical and Compu-tational Semantics ?
Volume 1: Proceedings of themain conference and the shared task, and Volume 2:Proceedings of the Sixth International Workshop onSemantic Evaluation (SemEval 2012), pages 385?393, Montr?al, Canada, 7-8 June.
Association forComputational Linguistics.212Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, ACL ?05, pages 597?604, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Daniel B?r, Chris Biemann, Iryna Gurevych, andTorsten Zesch.
2012.
Ukp: Computing seman-tic textual similarity by combining multiple contentsimilarity measures.
In Proceedings of the 6th In-ternational Workshop on Semantic Evaluation, heldin conjunction with the 1st Joint Conference on Lex-ical and Computational Semantics, pages 435?440,Montreal, Canada, Jun.Houda Bouamor, Aur?lien Max, Gabriel Illouz, andAnne Vilnat.
2012.
A contrastive review ofparaphrase acquisition techniques.
In Proceed-ings of the Eight International Conference on Lan-guage Resources and Evaluation (LREC?12), Istan-bul, Turkey, may.Ram Boukobza and Ari Rappoport.
2009.
Multi-wordexpression identification using sentence surface fea-tures.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing, pages 468?477, Singapore, August.
Associationfor Computational Linguistics.Steven Burrows, Martin Potthast, and Benno Stein.2013.
Paraphrase Acquisition via Crowdsourcingand Machine Learning.
Transactions on IntelligentSystems and Technology (ACM TIST) (to appear).Olivia Buzek, Philip Resnik, and Benjamin B. Beder-son.
2010.
Error driven paraphrase annotation usingmechanical turk.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk, CSLDAMT?10, pages 217?221, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.David L. Chen and William B. Dolan.
2011.
Collect-ing highly parallel data for paraphrase evaluation.
InProceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics, pages 190?200, Portland, Oregon, USA, June.Michael Denkowski and Alon Lavie.
2010.
Ex-ploring normalization techniques for human judg-ments of machine translation adequacy collectedusing amazon mechanical turk.
In Proceedingsof the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechan-ical Turk, CSLDAMT ?10, pages 57?61, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.William B. Dolan and Chris Brockett.
2005.
Auto-matically constructing a corpus of sentential para-phrases.
In Third International Workshop on Para-phrasing (IWP2005).
Asia Federation of NaturalLanguage Processing.Mark Dras.
1999.
Tree Adjoining Grammar and theReluctant Paraphrasing of Text.
Ph.D. thesis, Mac-quarie University.Christiane Fellbaum, Alexander Geyken, Axel Herold,Fabian Koerner, and Gerald Neumann.
2006.Corpus-based Studies of German Idioms and LightVerbs.
International Journal of Lexicography,19(4):349?360, December.Samuel Fernando and Mark Stevenson.
2008.
A se-mantic similarity approach to paraphrase detection.In Proceedings of the 11th Annual Research Collo-quium of the UK Special Interest Group for Compu-tational Linguistics.Dekang Lin and Patrick Pantel.
2001.
Discovery ofinference rules for question-answering.
Nat.
Lang.Eng., 7(4):343?360, December.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012.
Re-examining machine translation metricsfor paraphrase identification.
In Proceedings of the2012 Conference of the North American Chapterof the Association for Computational Linguistics:Human Language Technologies, NAACL HLT ?12,pages 182?190, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Erwin Marsi and Emiel Krahmer.
2010.
Automaticanalysis of semantic similarity in comparable textthrough syntactic tree matching.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics (Coling 2010), pages 752?760,Beijing, China, August.
Coling 2010 OrganizingCommittee.Matteo Negri, Yashar Mehdad, Alessandro Marchetti,Danilo Giampiccolo, and Luisa Bentivogli.
2012.Chinese whispers: Cooperative paraphrase acqui-sition.
In Proceedings of the Eight InternationalConference on Language Resources and Evaluation(LREC?12), Istanbul, Turkey, may.Martin Potthast, Alberto Barr?n-Cede?o, AndreasEiselt, Benno Stein, and Paolo Rosso.
2010.Overview of the 2nd international competition onplagiarism detection.
Notebook Papers of CLEF, 10.Matthew G Snover, Nitin Madnani, Bonnie Dorr, andRichard Schwartz.
2009.
Ter-plus: Paraphrase, se-mantic, and alignment enhancements to translationedit rate.
Machine Translation, 23(2):117?127.Richard Socher, Eric H. Huang, Jeffrey Pennington,Andrew Y. Ng, and Christopher D. Manning.
2011.Dynamic Pooling and Unfolding Recursive Autoen-coders for Paraphrase Detection.
In Advances inNeural Information Processing Systems 24.Liang Zhou, Chin-Yew Lin, and Eduard Hovy.
2006.Re-evaluating machine translation results with para-phrase support.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?06, pages 77?84, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.213
