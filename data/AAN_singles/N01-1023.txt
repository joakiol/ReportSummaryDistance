Applying Co-Training methods to Statistical ParsingAnoop SarkarDept.
of Computer and Information ScienceUniversity of Pennsylvania200 South 33rd Street,Philadelphia, PA 19104-6389 USAanoop@linc.cis.upenn.eduAbstractWe propose a novel Co-Training method for statisticalparsing.
The algorithm takes as input a small corpus(9695 sentences) annotated with parse trees, a dictionaryof possible lexicalized structures for each word in thetraining set and a large pool of unlabeled text.
The algo-rithm iteratively labels the entire data set with parse trees.Using empirical results based on parsing the Wall StreetJournal corpus we show that training a statistical parseron the combined labeled and unlabeled data strongly out-performs training only on the labeled data.1 IntroductionThe current crop of statistical parsers share a similartraining methodology.
They train from the Penn Tree-bank (Marcus et al, 1993); a collection of 40,000 sen-tences that are labeled with corrected parse trees (ap-proximately a million word tokens).
In this paper, weexplore methods for statistical parsing that can be usedto combine small amounts of labeled data with unlimitedamounts of unlabeled data.
In the experiment reportedhere, we use 9695 sentences of bracketed data (234467word tokens).
Such methods are attractive for the follow-ing reasons: Bracketing sentences is an expensive process.
Aparser that can be trained on a small amount of la-beled data will reduce this annotation cost. Creating statistical parsers for novel domains andnew languages will become easier. Combining labeled data with unlabeled data allowsexploration of unsupervised methods which cannow be tested using evaluations compatible with su-pervised statistical parsing.In this paper we introduce a new approach that com-bines unlabeled data with a small amount of labeled(bracketed) data to train a statistical parser.
We use a Co-Training method (Yarowsky, 1995; Blum and Mitchell, I would like to thank Aravind Joshi, Mitch Marcus, Mark Liberman,B.
Srinivas, David Chiang and the anonymous reviewers for helpfulcomments on this work.
This work was partially supported by NSFGrant SBR8920230, ARO Grant DAAH0404-94-G-0426, and DARPAGrant N66001-00-1-8915.1998; Goldman and Zhou, 2000) that has been used pre-viously to train classifiers in applications like word-sensedisambiguation (Yarowsky, 1995), document classifica-tion (Blum and Mitchell, 1998) and named-entity recog-nition (Collins and Singer, 1999) and apply this methodto the more complex domain of statistical parsing.2 Unsupervised techniques in languageprocessingWhile machine learning techniques that exploit anno-tated data have been very successful in attacking prob-lems in NLP, there are still some aspects which are con-sidered to be open issues: Adapting to new domains: training on one domain,testing (using) on another. Higher performance when using limited amounts ofannotated data. Separating structural (robust) aspects of the prob-lem from lexical (sparse) ones to improve perfor-mance on unseen data.In the particular domain of statistical parsing there hasbeen limited success in moving towards unsupervisedmachine learning techniques (see Section 7 for more dis-cussion).
A more promising approach is that of combin-ing small amounts of seed labeled data with unlimitedamounts of unlabeled data to bootstrap statistical parsers.In this paper, we use one such machine learning tech-nique: Co-Training, which has been used successfully inseveral classification tasks like web page classification,word sense disambiguation and named-entity recogni-tion.Early work in combining labeled and unlabeled datafor NLP tasks was done in the area of unsupervised partof speech (POS) tagging.
(Cutting et al, 1992) reportedvery high results (96% on the Brown corpus) for un-supervised POS tagging using Hidden Markov Models(HMMs) by exploiting hand-built tag dictionaries andequivalence classes.
Tag dictionaries are predefined as-signments of all possible POS tags to words in the testdata.
This impressive result triggered several follow-upstudies in which the effect of hand tuning the tag dictio-nary was quantified as a combination of labeled and unla-Pierre/NNP Vinken/NNPNPwill/MDjoin/VBthe/DT board/NNNPas/INa/DT non?executive/JJ director/NNNPPPVPVPSFigure 1: An example of the kind of output expected from a statistical parser.beled data.
The experiments in (Merialdo, 1994; Elwor-thy, 1994) showed that only in very specific cases HMMswere effective in combining labeled and unlabeled data.However, (Brill, 1997) showed that aggressively us-ing tag dictionaries extracted from labeled data could beused to bootstrap an unsupervised POS tagger with highaccuracy (approx 95% on WSJ data).
We exploit this ap-proach of using tag dictionaries in our method as well(see Section 3.2 for more details).
It is important to pointout that, before attacking the problem of parsing usingsimilar machine learning techniques, we face a represen-tational problem which makes it difficult to define thenotion of tag dictionary for a statistical parser.The problem we face in parsing is more complex thanassigning a small fixed set of labels to examples.
If theparser is to be generally applicable, it has to producea fairly complex ?label?
given an input sentence.
Forexample, given the sentence Pierre Vinken will join theboard as a non-executive director, the parser is expectedto produce an output as shown in Figure 1.Since the entire parse cannot be reasonably consideredas a monolithic label, the usual method in parsing is todecompose the structure assigned in the following way:S(join) !
NP(Vinken) VP(join)NP(Vinken) !
Pierre VinkenVP(join) !
will VP(join)VP(join) !
join NP(board) PP(as): : :However, such a recursive decomposition of structuredoes not allow a simple notion of a tag dictionary.
Wesolve this problem by decomposing the structure in anapproach that is different from that shown above whichuses context-free rules.The approach uses the notion of tree rewriting asdefined in the Lexicalized Tree Adjoining Grammar(LTAG) formalism (Joshi and Schabes, 1992)1 which re-1This is a lexicalized version of Tree Adjoining Grammar (Joshi etal., 1975; Joshi, 1985).tains the notion of lexicalization that is crucial in the suc-cess of a statistical parser while permitting a simple def-inition of tag dictionary.
For example, the parse in Fig-ure 1 can be generated by assigning the structured labelsshown in Figure 2 to each word in the sentence (for sim-plicity, we assume that the noun phrases are generatedhere as a single word).
We use a tool described in (Xiaet al, 2000) to convert the Penn Treebank into this rep-resentation.Pierre VinkenNPwill VPVPNPjoin NPVPSthe boardNPVPas NPPPVPa non?executive directorNPFigure 2: Parsing as tree classification and attachment.Combining the trees together by rewriting nodes astrees (explained in Section 2.1) gives us the parse treein Figure 1.
A history of the bi-lexical dependencies thatdefine the probability model used to construct the parseis shown in Figure 3.
This history is called the derivationtree.In addition, as a byproduct of this kind of represen-tation we obtain more than the phrase structure of eachsentence.
We also produce a more embellished parse inwhich phenomena such as predicate-argument structure,subcategorization and movement are given a probabilis-tic treatment.Pierre_Vinken will the_boarda_nonexecutive_directorasjoinFigure 3: A derivation indicating all the attachments be-tween trees that have occurred during the parse of thesentence.2.1 The Generative ModelA stochastic LTAG derivation proceeds as follows (Sch-abes, 1992; Resnik, 1992).
An initial tree is selected withprobability Pinit and other trees selected by words in thesentence are combined using the operations of substitu-tion and adjoining.
These operations are explained belowwith examples.
Each of these operations is performedwith probability Pattach.For each  that can be valid start of a derivation:XPinit() = 1Substitution is defined as rewriting a node in the fron-tier of a tree with probability Pattach which is said to beproper if:X0Pattach(;  !0) = 1where ;  !
0 indicates that tree  0 is substitutinginto node  in tree  .
An example of the operation ofsubstitution is shown in Figure 4.Adjoining is defined as rewriting any internal node of atree by another tree.
This is a recursive rule and each ad-joining operation is performed with probability Pattachwhich is proper if:Pattach(;  !
NA) +X0Pattach(;  !0) = 1Pattach here is the probability that0 rewrites an in-ternal node  in tree  or that no adjoining (NA) occursat node  in  .
The additional factor that accounts for noadjoining at a node is required for the probability to bewell-formed.
An example of the operation of adjoiningis shown in Figure 5.Each LTAG derivationD which was built starting fromtree  with n subsequent attachments has the probability:Pr(D) = Pinit()Y1inPattach(;  !0i)Pierre VinkenNPjoin NPVPSNPjoin NPVPSPierre VinkenNPFigure 4: Example substitution of the tree forPierre Vinken into the tree for join: (join);NP !0(Pierre Vinken).will VPVPNPjoin NPVPSNPwilljoin NPVPVPSFigure 5: Example adjoining of the tree for will into thetree for join: (join);VP !
0(will).Note that assuming each tree is lexicalized by oneword the derivation D corresponds to a sentence of n+1words.In the next section we show how to exploit this notionof tag dictionary to the problem of statistical parsing.3 Co-Training methods for parsingMany supervised methods of learning from a Treebankhave been studied.
The question we want to pursue inthis paper is whether unlabeled data can be used to im-prove the performance of a statistical parser and at thesame time reduce the amount of labeled training datanecessary for good performance.
We will assume thedata that is input to our method will have the followingcharacteristics:1.
A small set of sentences labeled with correctedparse trees and large set of unlabeled data.2.
A pair of probabilistic models that form parts of astatistical parser.
This pair of models must be ableto mutually constrain each other.3.
A tag dictionary (used within a backoff smoothingstrategy) for labels are not covered in the labeledset.The pair of probabilistic models can be exploited tobootstrap new information from unlabeled data.
Sinceboth of these steps ultimately have to agree with eachother, we can utilize an iterative method called Co-Training that attempts to increase agreement between apair of statistical models by exploiting mutual constraintsbetween their output.Co-Training has been used before in applications likeword-sense disambiguation (Yarowsky, 1995), web-pageclassification (Blum and Mitchell, 1998) and named-entity identification (Collins and Singer, 1999).
In allof these cases, using unlabeled data has resulted in per-formance that rivals training solely from labeled data.However, these previous approaches were on tasks thatinvolved identifying the right label from a small set oflabels (typically 2?3), and in a relatively small parame-ter space.
Compared to these earlier models, a statisticalparser has a very large parameter space and the labelsthat are expected as output are parse trees which haveto be built up recursively.
We discuss previous work incombining labeled and unlabeled data in more detail inSection 7.Co-training (Blum and Mitchell, 1998; Yarowsky,1995) can be informally described in the following man-ner: Pick two (or more) ?views?
of a classification prob-lem. Build separate models for each of these ?views?
andtrain each model on a small set of labeled data. Sample an unlabeled data set and to find examplesthat each model independently labels with high con-fidence.
(Nigam and Ghani, 2000) Confidently labeled examples can be picked in var-ious ways.
(Collins and Singer, 1999; Goldman andZhou, 2000) Take these examples as being valuable as trainingexamples and iterate this procedure until the unla-beled data is exhausted.Effectively, by picking confidently labeled data fromeach model to add to the training data, one model is la-beling data for the other model.3.1 Lexicalized Grammars and Mutual ConstraintsIn the representation we use, parsing using a lexicalizedgrammar is done in two steps:1.
Assigning a set of lexicalized structures to eachword in the input sentence (as shown in Figure 2).2.
Finding the correct attachments between thesestructures to get the best parse (as shown in Fig-ure 1).Each of these two steps involves ambiguity which canbe resolved using a statistical model.
By explicitly rep-resenting these two steps independently, we can pursueindependent statistical models for each step:1.
Each word in the sentence can take many differentlexicalized structures.
We can introduce a statisticalmodel that disambiguates the lexicalized structureassigned to a word depending on the local context.2.
After each word is assigned a certain set of lexical-ized structures, finding the right parse tree involvescomputing the correct attachments between theselexicalized structures.
Disambiguating attachmentscorrectly using an appropriate statistical model isessential to finding the right parse tree.These two models have to agree with each other onthe trees assigned to each word in the sentence.
Not onlydo the right trees have to be assigned as predicted by thefirst model, but they also have to fit together to cover theentire sentence as predicted by the second model2.
Thisrepresents the mutual constraint that each model placeson the other.3.2 Tag DictionariesFor the words that appear in the (unlabeled) training data,we collect a list of part-of-speech labels and trees thateach word is known to select in the training data.
Thisinformation is stored in a POS tag dictionary and a treedictionary.
It is important to note that no frequency orany other distributional information is stored.
The onlyinformation stored in the dictionary is which tags or treescan be selected by each word in the training data.We use a count cutoff for trees in the labeled data andcombine observed counts into an unobserved tree count.This is similar to the usual technique of assigning thetoken unknown to infrequent word tokens.
In this way,trees unseen in the labeled data but in the tag dictionaryare assigned a probability in the parser.The problem of lexical coverage is a severe one forunsupervised approaches.
The use of tag dictionaries isa way around this problem.
Such an approach has al-ready been used for unsupervised part-of-speech taggingin (Brill, 1997) where seed data of which POS tags canbe selected by each word is given as input to the unsu-pervised tagger.2See x7 for a discussion of the relation of this approach to that ofSuperTagging (Srinivas, 1997)In future work, it would be interesting to extend mod-els for unknown-word handling or other machine learn-ing techniques in clustering or the learning of subcatego-rization frames to the creation of such tag dictionaries.4 ModelsAs described before, we treat parsing as a two-step pro-cess.
The two models that we use are:1.
H1: selects trees based on previous context (taggingprobability model)2.
H2: computes attachments between trees and re-turns best parse (parsing probability model)4.1 H1: Tagging probability modelWe select the most likely trees for each word by examin-ing the local context.
The statistical model we use to de-cide this is the trigram model that was used by B. Srinivasin his SuperTagging model (Srinivas, 1997).
The modelassigns an n-best lattice of tree assignments associatedwith the input sentence with each path corresponding toan assignment of an elementary tree for each word in thesentence.
(for further details, see (Srinivas, 1997)).P (TjW)= P (T0: : : TnjW0: : :Wn) (1)=P (T0: : : Tn) P (W0: : :WnjT0: : : Tn)P (W0: : :Wn)(2) P (TijTi 2Ti 1) P (WijTi) (3)where T0: : : Tnis a sequence of elementary trees as-signed to the sentence W0: : :Wn.We get (2) by using Bayes theorem and we obtain (3)from (2) by ignore the denominator and by applying theusual Markov assumptions.The output of this model is a probabilistic ranking oftrees for the input sentence which is sensitive to a smalllocal context window.4.2 H2: Parsing probability modelOnce the words in a sentence have selected a set of el-ementary trees, parsing is the process of attaching thesetrees together to give us a consistent bracketing of thesentences.
Notation: Let  stand for an elementary treewhich is lexicalized by a word: w and a part of speechtag: p.Let Pinit (introduced earlier in 2.1) stand for the prob-ability of being root of a derivation tree defined as fol-lows:XPinit() = 1including lexical information, this is written as:Pr(; w; pjtop = 1) =Pr( jtop = 1) (4)Pr(pj; top = 1) (5)Pr(wj; p; top = 1); (6)where the variable top indicates that  is the tree thatbegins the current derivation.
There is a useful approxi-mation for Pinit:Pr(; w; pjtop = 1)  Pr(labeljtop = 1)where label is the label of the root node of  .^Pr(labeljtop = 1) =Count(top = 1; label) +Count(top = 1) + N(7)where N is the number of bracketing labels and  is aconstant used to smooth zero counts.Let Pattach (introduced earlier in 2.1) stand for theprobability of attachment of  0 into another  :Pattach(;  !
NA) +X0Pattach(;  !0) = 1including lexical information, this is written as:Pr(0; p0; w0jNode; ; w; p) (8)Pr(NAjNode; ; w; p) (9)We decompose (8) into the following components:Pr(0; p0; w0jNode; ; w; p) =Pr(0jNode; ; w; p) (10)Pr(p0j0; Node; ; w; p) (11)Pr(w0jp0;0; Node; ; w; p); (12)We do a similar decomposition for (9).For each of the equations above, we use a backoffmodel which is used to handle sparse data problems.
Wecompute a backoff model as follows:Let e1stand for the original lexicalized model and e2be the backoff level which only uses part of speech infor-mation:e1: Node; ; w; pe2: Node; ; pFor both Pinit and Pattach, let c = Count(e1).
Thenthe backoff model is computed as follows:(c)e1+ (1  (c))e2where (c) = c(c+D)and D is the diversity of e1(i.e.the number of distinct counts for e1).For Pattach we further smooth probabilities (10), (11)and (12).
We use (10) as an example, the other two arehandled in the same way.^Pr(0jNode; ; w; p) =(Count(Node; ; w; p;0) + )(Count(Node; ; w; p) + k)(13)Count(Node; ; w; p) =Xy2T0Count(Node; ; w; p; y) (14)where k is the diversity of adjunction, that is: the num-ber of different trees that can attach at that node.
T 0 isthe set of all trees  0 that can possibly attach at Node intree  .For our experiments, the value of  is set to 1100;000.5 Co-Training algorithmWe are now in the position to describe the Co-Trainingalgorithm, which combines the models described in Sec-tion 4.1 and in Section 4.2 in order to iteratively label alarge pool of unlabeled data.We use the following datasets in the algorithm:labeled a set of sentences bracketed with the correctparse trees.cache a small pool of sentences which is the focus ofeach iteration of the Co-Training algorithm.unlabeled a large set of unlabeled sentences.
The onlyinformation we collect from this set of sentences isa tree-dictionary: tree-dict and part-of-speech dic-tionary: pos-dict.
Construction of these dictionariesis covered in Section 3.2.In addition to the above datasets, we also use the usualdevelopment test set (termed dev in this paper), and a testset (called test) which is used to evaluate the bracketingaccuracy of the parser.The Co-Training algorithm consists of the followingsteps which are repeated iteratively until all the sentencesin the set unlabeled are exhausted.1.
Input: labeled and unlabeled2.
Update cache Randomly select sentences from unlabeled andrefill cache If cache is empty; exit3.
Train models H1 and H2 using labeled4.
Apply H1 and H2 to cache.5.
Pick most probablen from H1 (run through H2) andadd to labeled.6.
Pick most probable n from H2 and add to labeled7.
n = n + k; Go to Step 2For the experiment reported here, n = 10, and k wasset to be n in each iteration.
We ran the algorithm for 12iterations (covering 20480 of the sentences in unlabeled)and then added the best parses for all the remaining sen-tences.6 Experiment6.1 SetupThe experiments we report were done on the Penn Tree-bank WSJ Corpus (Marcus et al, 1993).
The varioussettings for the Co-Training algorithm (from Section 5)are as follows: labeled was set to Sections 02-06 of the Penn Tree-bank WSJ (9625 sentences) unlabeled was 30137 sentences (Section 07-21 ofthe Treebank stripped of all annotations). A tag dictionary of all lexicalized trees from labeledand unlabeled. Novel trees were treated as unknown tree tokens. The cache size was 3000 sentences.While it might seem expensive to run the parser overthe cache multiple times, we use the pruning capabilitiesof the parser to good use here.
During the iterations weset the beam size to a value which is likely to prune outall derivations for a large portion of the cache except themost likely ones.
This allows the parser to run faster,hence avoiding the usual problem with running an iter-ative algorithm over thousands of sentences.
In the ini-tial runs we also limit the length of the sentences enteredinto the cache because shorter sentences are more likelyto beat out the longer sentences in any case.
The beamsize is reset when running the parser on the test data toallow the parser a better chance at finding the most likelyparse.6.2 ResultsWe scored the output of the parser on Section 23 of theWall Street Journal Penn Treebank.
The following aresome aspects of the scoring that might be useful for com-parision with other results: No punctuations are scored,including sentence final punctuation.
Empty elementsare not scored.
We used EVALB (written by SatoshiSekine and Michael Collins) which scores based on PAR-SEVAL (Black et al, 1991); with the standard parame-ter file (as per standard practice, part of speech bracketswere not part of the evaluation).
Also, we used AdwaitRatnaparkhi?s part-of-speech tagger (Ratnaparkhi, 1996)to tag unknown words in the test data.We obtained 80.02% and 79.64% labeled bracketingprecision and recall respectively (as defined in (Blacket al, 1991)).
The baseline model which was onlytrained on the 9695 sentences of labeled data performedat 72.23% and 69.12% precision and recall.
These re-sults show that training a statistical parser using our Co-training method to combine labeled and unlabeled datastrongly outperforms training only on the labeled data.It is important to note that unlike previous studies, ourmethod of moving towards unsupervised parsing are di-rectly compared to the output of supervised parsers.Certain differences in the applicability of the usualmethods of smoothing to our parser cause the lower ac-curacy as compared to other state of the art statisticalparsers.
However, we have consistently seen increase inperformance when using the Co-Training method overthe baseline across several trials.
It should be empha-sised that this is a result based on less than 20% of datathat is usually used by other parsers.
We are experiment-ing with the use of an even smaller set of labeled data toinvestigate the learning curve.7 Previous Work: Combining Labeled andUnlabeled DataThe two-step procedure used in our Co-Training methodfor statistical parsing was incipient in the SuperTag-ger (Srinivas, 1997) which is a statistical model for tag-ging sentences with elementary lexicalized structures.This was particularly so in the Lightweight DependencyAnalyzer (LDA), which used shortest attachment heuris-tics after an initial SuperTagging stage to find syntacticdependencies between words in a sentence.
However,there was no statistical model for attachments and thenotion of mutual constraints between these two steps wasnot exploited in this work.Previous studies in unsupervised methods for parsinghave concentrated on the use of inside-outside algorithm(Lari and Young, 1990; Carroll and Rooth, 1998).
How-ever, there are several limitations of the inside-outside al-gorithm for unsupervised parsing, see (Marcken, 1995)for some experiments that draw out the mismatch be-tween minimizing error rate and iteratively increasing thelikelihood of the corpus.
Other approaches have tried tomove away from phrase structural representations intodependency style parsing (Lafferty et al, 1992; Fong andWu, 1996).
However, there are still inherent computa-tional limitations due to the vast search space (see (Pietraet al, 1994) for discussion).
None of these approachescan even be realistically compared to supervised parsersthat are trained and tested on the kind of representationsand the complexity of sentences that are found in thePenn Treebank.
(Chelba and Jelinek, 1998) combine unlabeled andlabeled data for parsing with a view towards languagemodeling applications.
The goal in their work is not toget the right bracketing or dependencies but to reduce theword error rate in a speech recognizer.Our approach is closely related to previous Co-Training methods (Yarowsky, 1995; Blum and Mitchell,1998; Goldman and Zhou, 2000; Collins and Singer,1999).
(Yarowsky, 1995) first introduced an iterativemethod for increasing a small set of seed data used todisambiguate dual word senses by exploiting the con-straint that in a segment of discourse only one sense ofa word is used.
This use of unlabeled data improvedperformance of the disambiguator above that of purelysupervised methods.
(Blum and Mitchell, 1998) furtherembellish this approach and gave it the name of Co-Training.
Their definition of Co-Training includes thenotion (exploited in this paper) that different models canconstrain each other by exploiting different ?views?
of thedata.
They also prove some PAC results on learnability.They also discuss an application of classifying web pagesby using their method of mutually constrained models.
(Collins and Singer, 1999) further extend the use of clas-sifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co-Boosting).
(Goldman and Zhou, 2000) provide a variantof Co-Training which is suited to the learning of deci-sion trees where the data is split up into different equiv-alence classes for each of the models and they use hy-pothesis testing to determine the agreement between themodels.
In future work we would like to experimentwhether some of these ideas could be incorporated intoour model.In future work we would like to explore use of the en-tire 1M words of the WSJ Penn Treebank as our labeleddata and to use a larger set of unbracketed WSJ data asinput to the Co-Training algorithm.
In addition, we planto explore the following points that bear on understand-ing the nature of the Co-Training learning algorithm: The contribution of the dictionary of trees extractedfrom the unlabeled set is an issue that we would liketo explore in future experiments.
Ideally, we wish todesign a co-training method where no such informa-tion is used from the unlabeled set. The relationship between co-training and EM bearsinvestigation.
(Nigam and Ghani, 2000) is a studywhich tries to separate two factors: (1) The gradi-ent descent aspect of EM vs. the iterative natureof co-training and (2) The generative model used inEM vs. the conditional independence between thefeatures used by the two models that is exploited inco-training.
Also, EM has been used successfullyin text classification in combination of labeled andunlabeled data (see (Nigam et al, 1999)). In our experiments, unlike (Blum and Mitchell,1998) we do not balance the label priors when pick-ing new labeled examples for addition to the train-ing data.
One way to incorporate this into our algo-rithm would be to incorporate some form of sampleselection (or active learning) into the selection ofexamples that are considered as labeled with highconfidence (Hwa, 2000).8 ConclusionIn this paper, we proposed a new approach for traininga statistical parser that combines labeled with unlabeleddata.
It uses a Co-Training method where a pair of mod-els attempt to increase their agreement on labeling thedata.
The algorithm takes as input a small corpus of9695 sentences (234467 word tokens) of bracketed data,a large pool of unlabeled text and a tag dictionary of lexi-calized structures for each word in this training set (basedon the LTAG formalism).
The algorithm presented itera-tively labels the unlabeled data set with parse trees.
Wethen train a statistical parser on the combined set of la-beled and unlabeled data.We obtained 80.02% and 79.64% labeled bracketingprecision and recall respectively.
The baseline modelwhich was only trained on the 9695 sentences of labeleddata performed at 72.23% and 69.12% precision and re-call.
These results show that training a statistical parserusing our Co-training method to combine labeled and un-labeled data strongly outperforms training only on the la-beled data.It is important to note that unlike previous studies, ourmethod of moving towards unsupervised parsing can bedirectly compared to the output of supervised parsers.Unlike previous approaches to unsupervised parsing ourmethod can be trained and tested on the kind of represen-tations and the complexity of sentences that are found inthe Penn Treebank.In addition, as a byproduct of our representation weobtain more than the phrase structure of each sentence.We also produce a more embellished parse in which phe-nomena such as predicate-argument structure, subcate-gorization and movement are given a probabilistic treat-ment.ReferencesE.
Black, S. Abney, D. Flickinger, C. Gdaniec, R. Grishman, P. Har-rison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman,M.
Marcus, S. Roukos, B. Santorini, and T. Strzalkowski.
1991.A procedure for quantitatively comparing the syntactic coverage ofenglish grammars.
In Proc.
DARPA Speech and Natural LanguageWorkshop, pages 306?311.
Morgan Kaufmann.A.
Blum and T. Mitchell.
1998.
Combining Labeled and UnlabeledData with Co-Training.
In Proc.
of 11th Annual Conf.
on Comp.Learning Theory (COLT), pages 92?100.E.
Brill.
1997.
Unsupervised learning of disambiguation rules for partof speech tagging.
In Natural Language Processing Using VeryLarge Corpora.
Kluwer Academic Press.G.
Carroll and M. Rooth.
1998.
Valence In-duction with a Head-Lexicalized PCFG.http://xxx.lanl.gov/abs/cmp-lg/9805001, May.C.
Chelba and F. Jelinek.
1998.
Exploiting syntactic structure for lan-guage modeling.
In Proc.
of COLING-ACL ?98, pages 225?231,Montreal.M.
Collins and Y.
Singer.
1999.
Unsupervised Models for Named En-tity Classification.
In Proc.
of WVLC/EMNLP-99, pages 100?110.D.
Cutting, J. Kupiec, J. Pedersen, and P. Sibun.
1992.
A practicalpart-of-speech tagger.
In Proc.
of 3rd ANLP Conf., Trento, Italy.ACL.D.
Elworthy.
1994.
Does baum-welch re-estimation help taggers?
InProc.
of 4th ANLP Conf., pages 53?58, Stuttgart, October 13-15.E.
W. Fong and D. Wu.
1996.
Learning restricted probabilistic linkgrammars.
In S. Wermter, E. Riloff, and G. Scheler, editors, Con-nectionist, Statistical and Symbolic Approaches to Learning for Nat-ural Language Processing, pages 173?187.
Springer-Verlag.S.
Goldman and Y. Zhou.
2000.
Enhancing supervised learning withunlabeled data.
In Proc.
of ICML?2000, Stanford University, June29?July 2.Rebecca Hwa.
2000.
Sample selection for statistical grammar induc-tion.
In Proceedings of EMNLP/VLC-2000, pages 45?52.A.
K. Joshi and Y. Schabes.
1992.
Tree-adjoining grammar and lex-icalized grammars.
In M. Nivat and A. Podelski, editors, Tree au-tomata and languages, pages 409?431.
Elsevier Science.A.
K. Joshi, L. Levy, and M. Takahashi.
1975.
Tree Adjunct Gram-mars.
Journal of Computer and System Sciences.A.
K. Joshi.
1985.
Tree Adjoining Grammars: How much context Sen-sitivity is required to provide a reasonable structural description.
InD.
Dowty, I. Karttunen, and A. Zwicky, editors, Natural LanguageParsing, pages 206?250.
Cambridge University Press, Cambridge,U.K.J.
Lafferty, D. Sleator, and D. Temperley.
1992.
Grammatical trigrams:A probabilistic model of link grammar.
In Proc.
of the AAAI Conf.on Probabilistic Approaches to Natural Language.K.
Lari and S. J.
Young.
1990.
The estimation of stochastic context-free grammars using the Inside-Outside algorithm.
ComputerSpeech and Language, 4:35?56.C.
de Marcken.
1995.
Lexical heads, phrase structure and the induc-tion of grammar.
In D. Yarowsky and K. Church, editors, Proc.
of3rd WVLC, pages 14?26, MIT, Cambridge, MA.M.
Marcus, B. Santorini, and M. Marcinkiewiecz.
1993.
Buildinga large annotated corpus of english.
Computational Linguistics,19(2):313?330.B.
Merialdo.
1994.
Tagging english text with a probabilistic model.Computational Linguistics, 20(2):155?172.Kamal Nigam and Rayid Ghani.
2000.
Analyzing the effectiveness andapplicability of co-training.
In Proc.
of Ninth International Confer-ence on Information and Knowledge (CIKM-2000).Kamal Nigam, Andrew McCallum, Sebastian Thrun, and TomMitchell.
1999.
Text Classification from Labeled and UnlabeledDocuments using EM.
Machine Learning, 1(34).S.
Della Pietra, V. Della Pietra, J. Gillett, J. Lafferty, H. Printz, andL.
Ures?.
1994.
Inference and estimation of a long-range trigrammodel.
In R. Carrasco and J. Oncina, editors, Proc.
of ICGI-94.Springer-Verlag.A.
Ratnaparkhi.
1996.
A Maximum Entropy Part-Of-Speech Tagger.In Proc.
of EMNLP-96, University of Pennsylvania.P.
Resnik.
1992.
Probabilistic tree-adjoining grammars as a frameworkfor statistical natural language processing.
In Proc.
of COLING ?92,volume 2, pages 418?424, Nantes, France.Y.
Schabes.
1992.
Stochastic lexicalized tree-adjoining grammars.
InProc.
of COLING ?92, volume 2, pages 426?432, Nantes, France.B.
Srinivas.
1997.
Complexity of Lexical Descriptions and its Rele-vance to Partial Parsing.
Ph.D. thesis, Department of Computerand Information Sciences, University of Pennsylvania.F.
Xia, M. Palmer, and A. Joshi.
2000.
A Uniform Method of GrammarExtraction and its Applications.
In Proc.
of EMNLP/VLC-2000.D.
Yarowsky.
1995.
Unsupervised Word Sense Disambiguation Rival-ing Supervised Methods.
In Proc.
33rd Meeting of the ACL, pages189?196, Cambridge, MA.
