Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 138?147,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsEvaluating Word Order Recursively over Permutation-ForestsMilo?s Stanojevi?c and Khalil Sima?anInstitute for Logic, Language and ComputationUniversity of AmsterdamScience Park 107, 1098 XG Amsterdam, The Netherlands{m.stanojevic,k.simaan}@uva.nlAbstractAutomatically evaluating word order ofMT system output at the sentence-level ischallenging.
At the sentence-level, ngramcounts are rather sparse which makes itdifficult to measure word order quality ef-fectively using lexicalized units.
Recentapproaches abstract away from lexicaliza-tion by assigning a score to the permuta-tion representing how word positions insystem output move around relative to areference translation.
Metrics over per-mutations exist (e.g., Kendal tau or Spear-man Rho) and have been shown to beuseful in earlier work.
However, noneof the existing metrics over permutationsgroups word positions recursively intolarger phrase-like blocks, which makes itdifficult to account for long-distance re-ordering phenomena.
In this paper we ex-plore novel metrics computed over Per-mutation Forests (PEFs), packed chartsof Permutation Trees (PETs), which aretree decompositions of a permutation intoprimitive ordering units.
We empiricallycompare PEFs metric against five knownreordering metrics on WMT13 data for tenlanguage pairs.
The PEFs metric showsbetter correlation with human ranking thanthe other metrics almost on all languagepairs.
None of the other metrics exhibitsas stable behavior across language pairs.1 IntroductionEvaluating word order (also reordering) in MT isone of the main ingredients in automatic MT eval-uation, e.g., (Papineni et al., 2002; Denkowskiand Lavie, 2011).
To monitor progress on eval-uating reordering, recent work explores dedicatedreordering evaluation metrics, cf.
(Birch and Os-borne, 2011; Isozaki et al., 2010; Talbot et al.,2011).
Existing work computes the correlation be-tween the ranking of the outputs of different sys-tems by an evaluation metric to human ranking, one.g., the WMT evaluation data.For evaluating reordering, it is necessary toword align system output with the correspond-ing reference translation.
For convenience, a 1:1alignment (a permutation) is induced between thewords on both sides (Birch and Osborne, 2011),possibly leaving words unaligned on either side.Existing work then concentrates on defining mea-sures of reordering over permutations, cf.
(Lap-ata, 2006; Birch and Osborne, 2011; Isozaki et al.,2010; Talbot et al., 2011).
Popular metrics overpermutations are: Kendall?s tau, Spearman, Ham-ming distance, Ulam and Fuzzy score.
These met-rics treat a permutation as a flat sequence of inte-gers or blocks, disregarding the possibility of hier-archical grouping into phrase-like units, making itdifficult to measure long-range order divergence.Next we will show by example that permutationsalso contain latent atomic units that govern the re-cursive reordering of phrase-like units.
Account-ing for these latent reorderings could actually befar simpler than the flat view of a permutation.Isozaki et al.
(2010) argue that the conventionalmetrics cannot measure well the long distancereordering between an English reference sentence?A because B?
and a Japanese-English hypothesistranslation ?B because A?, where A and B areblocks of any length with internal monotonicalignments.
In this paper we explore the idea offactorizing permutations into permutation-trees(PETs) (Gildea et al., 2006) and defining new138?2,4,1,3?2 ?1,2?4 ?1,2?5 61 3Figure 1: A permutation tree for ?2, 4, 5, 6, 1, 3?tree-based reordering metrics which aims atdealing with this type of long range reorderings.For the Isozaki et al.
(2010) Japanese-Englishexample, there are two PETs (when leaving A andB as encapsulated blocks):?2,1?A ?2,1?because B?2,1?
?2,1?A becauseBOur PET-based metrics interpolate the scores overthe two inversion operators ?2, 1?
with the internalscores for A and B, incorporating a weightfor subtree height.
If both A and B are largeblocks, internally monotonically (also known asstraight) aligned, then our measure will not countevery single reordering of a word in A or B,but will consider this case as block reordering.From a PET perspective, the distance of thereordering is far smaller than when looking at aflat permutation.
But does this hierarchical viewof reordering cohere better with human judgementthan string-based metrics?The example above also shows that a permuta-tion may factorize into different PETs, each corre-sponding to a different segmentation of a sentencepair into phrase-pairs.
In this paper we introducepermutation forests (PEFs); a PEF is a hypergraphthat compactly packs the set of PETs that factorizea permutation.There is yet a more profoud reasoning behindPETs than only accounting for long-range reorder-ings.
The example in Figure 1 gives the flavor ofPETs.
Observe how every internal node in thisPET dominates a subtree whose fringe1is itself apermutation over an integer sub-range of the orig-inal permutation.
Every node is decorated with apermutation over the child positions (called oper-ator).
For example ?4, 5, 6?
constitutes a contigu-ous range of integers (corresponding to a phrasepair), and hence will be grouped into a subtree;1Ordered sequence of leaf nodes.which in turn can be internally re-grouped into abinary branching subtree.
Every node in a PET isminimum branching, i.e., the permutation factor-izes into a minimum number of adjacent permuta-tions over integer sub-ranges (Albert and Atkin-son, 2005).
The node operators in a PET areknown to be the atomic building blocks of all per-mutations (called primal permutations).
Becausethese are building atomic units of reordering, itmakes sense to want to measure reordering as afunction of the individual cost of these operators.In this work we propose to compute new reorder-ing measures that aggregate over the individualnode-permutations in these PETs.While PETs where exploited rather recently forextracting features used in the BEER metric sys-tem description (Stanojevi?c and Sima?an, 2014) inthe official WMT 2014 competition, this work isthe first to propose integral recursive metrics overPETs and PEFs solely for measuring reordering(as opposed to individual non-recursive features ina full metric that measures at the same time bothfluency and adequacy).
We empirically show thata PEF-based evaluation measure correlates betterwith human rankings than the string-based mea-sures on eight of the ten language pairs in WMT13data.
For the 9thlanguage pair it is close to best,and for the 10th(English-Czech) we find a likelyexplanation in the Findings of the 2013 WMT (Bo-jar et al., 2013).
Crucially, the PEF-based mea-sure shows more stable ranking across languagepairs than any of the other measures.
The metricis available online as free software2.2 Measures on permutations: BaselinesIn (Birch and Osborne, 2010; Birch and Osborne,2011) Kendall?s tau and Hamming distance arecombined with unigram BLEU (BLEU-1) leadingto LRscore showing better correlation with humanjudgment than BLEU-4.
Birch et al.
(2010) ad-ditionally tests Ulam distance (longest commonsubsequence ?
LCS ?
normalized by the permu-tation length) and the square root of Kendall?s tau.Isozaki et al.
(2010) presents a similar approachto (Birch and Osborne, 2011) additionally test-ing Spearman rho as a distance measure.
Talbotet al.
(2011) extracts a reordering measure fromMETEOR (Denkowski and Lavie, 2011) dubbedFuzzy Reordering Score and evaluates it on MTreordering quality.2https://github.com/stanojevic/beer139For an evaluation metric we need a functionwhich would have the standard behaviour of evalu-ation metrics - the higher the score the better.
Bel-low we define the baseline metrics that were usedin our experiments.Baselines A permutation over [1..n] (subrangeof the positive integers where n > 1) is a bijectivefunction from [1..n] to itself.
To represent permu-tations we will use angle brackets as in ?2, 4, 3, 1?.Given a permutation pi over [1..n], the notation pii(1 ?
i ?
n) stands for the integer in the ithposi-tion in pi; pi(i) stands for the index of the positionin pi where integer i appears; and pijistands for the(contiguous) sub-sequence of integers pii, .
.
.
pij.The definitions of five commonly used met-rics over permutations are shown in Figure 2.In these definitions, we use LCS to stand forLongest Common Subsequence, and Kronecker?
[a] which is 1 if (a == true) else zero, andAn1= ?1, ?
?
?
, n?
which is the identity permuta-tion over [1..n].
We note that all existing metricskendall(pi) =?n?1i=1?nj=i+1?
[pi(i) < pi(j)](n2?
n)/2hamming(pi) =?ni=1?
[pii== i]nspearman(pi) = 1?3?ni=1(pii?
i)2n(n2?
1)ulam(pi) =LCS(pi,An1)?
1n?
1fuzzy(pi) = 1?c?
1n?
1where c is # of monotone sub-permutationsFigure 2: Five commonly used metrics over per-mutationsare defined directly over flat string-level permuta-tions.
In the next section we present an alternativeview of permutations are compositional, recursivetree structures.3 Measures on Permutation ForestsExisting work, e.g., (Gildea et al., 2006), showshow to factorize any permutation pi over [1..n]into a canonical permutation tree (PET).
Here wewill summarize the relevant aspects and extendPETs to permutation forests (PEFs).A non-empty sub-sequence pijiof a permutationpi is isomorphic with a permutation over [1..(j ?i + 1)] iff the set {pii, .
.
.
, pij} is a contiguousrange of positive integers.
We will use the terma sub-permutation of pi to refer to a subsequenceof pi that is isomorphic with a permutation.
Notethat not every subsequence of a permutation pi isnecessarily isomorphic with a permutation, e.g.,the subsequence ?3, 5?
of ?1, 2, 3, 5, 4?
is not asub-permutation.
One sub-permutation pi1of pi issmaller than another sub-permutation pi2of pi iffevery integer in pi1is smaller than all integers inpi2.
In this sense we can put a full order on non-overlapping sub-permutations of pi and rank themfrom the smallest to the largest.For every permutation pi there is a minimumnumber of adjacent sub-permutations it can be fac-torized into (see e.g., (Gildea et al., 2006)).
Wewill call this minimum number the arity of pi anddenote it with a(pi) (or simply a when pi is un-derstood from the context).
For example, the arityof pi = ?5, 7, 4, 6, 3, 1, 2?
is a = 2 because it canbe split into a minimum of two sub-permutations(Figure 3), e.g.
?5, 7, 4, 6, 3?
and ?1, 2?
(but alter-natively also ?5, 7, 4, 6?
and ?3, 1, 2?).
In contrast,pi = ?2, 4, 1, 3?
(also known as the Wu (1997) per-mutation) cannot be split into less than four sub-permutations, i.e., a = 4.
Factorization can beapplied recursively to the sub-permutations of pi,resulting in a tree structure (see Figure 3) called apermutation tree (PET) (Gildea et al., 2006; Zhangand Gildea, 2007; Maillette de Buy Wenniger andSima?an, 2011).Some permutations factorize into multiple alter-native PETs.
For pi = ?4, 3, 2, 1?
there are fivePETs shown in Figure 3.
The alternative PETscan be packed into an O(n2) permutation forest(PEF).
For many computational purposes, a sin-gle canonical PET is sufficient, cf.
(Gildea et al.,2006).
However, while different PETs of pi exhibitthe same reordering pattern, their different binarybranching structures might indicate important dif-ferences as we show in our experiments.A permutation forest (akin to a parse forest)F for pi (over [1..n]) is a data structure consistingof a subset of {[[i, j, Iji, Oji]] | 0 ?
i ?
j ?
n},where Ijiis a (possibly empty) set of inferences(sets of split points) for piji+1and Ojiis an oper-ator shared by all inferences of piji+1.
If piji+1isa sub-permutation and it has arity a ?
(j ?
(i +140?2,1??2,1?
?2,4,1,3?5 7 4 63?1,2?1 2?2,1?4 ?2,1?3 ?2,1?2 1?2,1?4 ?2,1?
?2,1?3 21?2,1?
?2,1?4 3?2,1?2 1?2,1??2,1?
?2,1?4 321?2,1?
?2,1?4 ?2,1?3 21Figure 3: A PET for pi = ?5, 7, 4, 6, 3, 1, 2?.
And five different PETs for pi = ?4, 3, 2, 1?.1)), then each inference consists of a a ?
1-tuple[l1, .
.
.
, la?1], where for each 1 ?
x ?
(a?
1), lxis a ?split point?
which is given by the index of thelast integer in the xthsub-permutation in pi.
Thepermutation of the a sub-permutations (?children?of piji+1) is stored in Ojiand it is the same for allinferences of that span (Zhang et al., 2008).
?2,1?43 2 1?2,1?4 3 2 1?2,1?4 3 21Figure 4: The factorizations of pi = ?4, 3, 2, 1?.Let us exemplify the inferences on pi =?4, 3, 2, 1?
(see Figure 4) which factorizes intopairs of sub-permutations (a = 2): a split pointcan be at positions with index l1?
{1, 2, 3}.Each of these split points (factorizations) of pi willbe represented as an inference for the same rootnode which covers the whole of pi (placed in entry[0, 4]); the operator of the inference here consistsof the permutation ?2, 1?
(swapping the two rangescovered by the children sub-permutations) and in-ference consists of a?
1 indexes l1, .
.
.
, la?1sig-nifying the split points of pi into sub-permutations:since a = 2 for pi, then a single index l1?
{1, 2, 3} is stored with every inference.
For thefactorization ((4, 3), (2, 1)) the index l1= 2 sig-nifying that the second position is a split point into?4, 3?
(stored in entry [0, 2]) and ?2, 1?
(stored inentry [2, 4]).
For the other factorizations of pi sim-ilar inferences are stored in the permutation forest.Figure 5 shows a simple top-down factorizationalgorithm which starts out by computing the ar-ity a using function a(pi).
If a = 1, a single leafnode is stored with an empty set of inferences.
Ifa > 1 then the algorithm computes all possiblefactorizations of pi into a sub-permutations (a se-quence of a?
1 split points) and stores their infer-ences together as Ijiand their operator Ojiasso-ciated with a node in entry [[i, j, Iji, Oji]].
Subse-quently, the algorithm applies recursively to eachsub-permutation.
Efficiency is a topic beyondthe scope of this paper, but this naive algorithmhas worst case time complexity O(n3), and whencomputing only a single canonical PET this can beO(n) (see e.g., (Zhang and Gildea, 2007)).Function PEF (i, j, pi,F);# Args: sub-perm.
pi over [i..j] and forest FOutput: Parse-Forest F(pi) for pi;beginif ([[i, j, ?]]
?
F) then return F ; #memoizationa := a(pi);if a = 1 return F := F ?
{[[i, j, ?
]]};For each set of split points {l1, .
.
.
, la?1} doOji:= RankListOf(pil1(l0+1), pil2(l1+1), .
.
.
, pila(la?1+1));Iji:= Iji?
[l1, .
.
.
, la?1];For each piv?
{pil1l0+1, pil2(l1+1), .
.
.
, pila(la?1+1)} doF := F ?
PermForest(piv);F := F ?
{[[i, j, Iji, Oji]]};Return F ;end;Figure 5: Pseudo-code of permutation-forest fac-torization algorithm.
Function a(pi) returns the ar-ity of pi.
Function RankListOf(r1, .
.
.
, rm) re-turns the list of rank positions (i.e., a permutation)of sub-permutations r1, .
.
.
, rmafter sorting themsmallest first.
The top-level call to this algorithmuses pi, i = 0, j = n and F = ?.Our measure (PEFscore) uses a functionopScore(p) which assigns a score to a given oper-ator, which can be instantiated to any of the exist-ing scoring measures listed in Section 2, but in thiscase we opted for a very simple function whichgives score 1 to monotone permutation and score0 to any other permutation.Given an inference l ?
Ijiwhere l =[l1, .
.
.
, la?1], we will use the notation lxto referto split point lxin l where 1 ?
x ?
(a ?
1), withthe convenient boundary assumption that l0= iand la= j.141PEFscore(pi) = ?node(0, n, PEF (pi))?node(i, j,F) =??????????????
?if (Iji== ?)
then 1else if (a(piji+1) = j ?
i) then opScore(Oji)else ?
?
opScore(Oji) + (1?
?)?
?l?Iji?inf(l,F ,a(piji+1))|Iji|?
??
?Avg.
inference score over Iji?inf(l,F , a) =?ax=1?[lx?lx?1>1]??node(l(x?1),lx,F)?ax=1?[lx?l(x?1)>1]?
??
?Avg.
score for non-terminal childrenopScore(p) ={if (p == ?1, 2?)
then 1else 0Figure 6: The PEF ScoreThe PEF-score, PEFscore(pi) in Figure 6,computes a score for the single root node[[0, n, In0, On0]]) in the permutation forest.
Thisscore is the average inference score ?infover allinferences of this node.
The score of an inference?infinterpolates (?)
between the opScore of theoperator in the current span and (1?
?)
the scoresof each child node.
The interpolation parameter ?can be tuned on a development set.The PET-score (single PET) is a simplificationof the PEF-score where the summation over all in-ferences of a node?l?Ijiin ?nodeis replaced by?Select a canonical l ?
Iji?.4 Experimental settingData The data that was used for experiments arehuman rankings of translations from WMT13 (Bo-jar et al., 2013).
The data covers 10 language pairswith a diverse set of systems used for translation.Each human evaluator was presented with 5 differ-ent translations, source sentence and a referencetranslation and asked to rank system translationsby their quality (ties were allowed).3Meta-evaluation The standard way for doingmeta-evaluation on the sentence level is withKendall?s tau correlation coefficient (Callison-Burch et al., 2012) computed on the number oftimes an evaluation metric and a human evaluatoragree (and disagree) on the rankings of pairs of3We would like to extend our work also to English-Japanese but we do not have access to such data at the mo-ment.
In any case, the WMT13 data is the largest publiclyavailable data of this kind.translations.
We extract pairs of translations fromhuman evaluated data and compute their scoreswith all metrics.
If the ranking assigned by a met-ric is the same as the ranking assigned by a hu-man evaluator then that pair is considered concor-dant, otherwise it is a discordant pair.
All pairswhich have the same score by the metric or arejudged as ties by human evaluators are not usedin meta-evaluation.
The formula that was used forcomputing Kendall?s tau correlation coefficient isshown in Equation 1.
Note that the formula forKendall tau rank correlation coefficient that is usedin meta-evaluation is different from the Kendalltau similarity function used for evaluating permu-tations.
The values that it returns are in the range[?1, 1], where ?1 means that order is always op-posite from the human judgment while the value 1means that metric ranks the system translations inthe same way as humans do.?
=#concordant pairs?#discordant pairs#concordant pairs+#discordant pairs(1)Evaluating reordering Since system transla-tions do not differ only in the word order but alsoin lexical choice, we follow Birch and Osborne(2010) and interpolate the score given by each re-ordering metric with the same lexical score.
Forlexical scoring we use unigram BLEU.
The param-eter that balances the weights for these two metrics?
is chosen to be 0.5 so it would not underesti-mate the lexical differences between translations(?
0.5) but also would not turn the whole met-ric into unigram BLEU (?
0.5).
The equation142for this interpolation is shown in Equation 2.4FullMetric(ref, sys) = ?
lexical(ref, sys) +(1?
?)?
bp(|ref |, |pi|)?
ordering(pi) (2)Where pi(ref, sys) is the permutation represent-ing the word alignment from sys to ref .
The ef-fect of ?
on the German-English evaluation is vis-ible on Figure 7.
The PET and PEF measures havean extra parameter ?
that gives importance to thelong distance errors that also needs to be tuned.
OnFigure 8 we can see the effect of ?
on German-English for ?
= 0.5.
For all language pairs for?
= 0.6 both PETs and PEFs get good results sowe picked that as value for ?
in our experiments.Figure 7: Effect of ?
on German-English evalua-tion for ?
= 0.6Choice of word alignments The issue we didnot discuss so far is how to find a permutationfrom system and reference translations.
One wayis to first get alignments between the source sen-tence and the system translation (from a decoderor by automatically aligning sentences), and alsoalignments between the source sentence and thereference translation (manually or automaticallyaligned).
Subsequently we must make those align-ments 1-to-1 and merge them into a permutation.That is the approach that was followed in previ-ous work (Birch and Osborne, 2011; Talbot et al.,4Note that for reordering evaluation it does not makesense to tune ?
because that would blur the individual contri-butions of reordering and adequacy during meta evaluation,which is confirmed by Figure 7 showing that ?
0.5 leadsto similar performance for all metrics.Figure 8: Effect of ?
on German-English evalua-tion for ?
= 0.52011).
Alternatively, we may align system and ref-erence translations directly.
One of the simplestways to do that is by finding exact matches be-tween words and bigrams between system and ref-erence translation as done in (Isozaki et al., 2010).The way we align system and reference transla-tions is by using the aligner supplied with ME-TEOR (Denkowski and Lavie, 2011) for finding1-to-1 alignments which are later converted to apermutation.
The advantage of this method is thatit can do non-exact matching by stemming or us-ing additional sources for semantic similarity suchas WordNets and paraphrase tables.
Since we willnot have a perfect permutation as input, becausemany words in the reference or system transla-tions might not be aligned, we introduce a brevitypenalty (bp(?, ?)
in Equation 2) for the orderingcomponent as in (Isozaki et al., 2010).
The brevitypenalty is the same as in BLEU with the smalldifference that instead of taking the length of sys-tem and reference translation as its parameters, ittakes the length of the system permutation and thelength of the reference.5 Empirical resultsThe results are shown in Table 1 and Table 2.These scores could be much higher if we usedsome more sophisticated measure than unigramBLEU for the lexical part (for example recall isvery useful in evaluation of the system translations(Lavie et al., 2004)).
However, this is not the issuehere since our goal is merely to compare differentways to evaluate word order.
All metrics that wetested have the same lexical component, get thesame permutation as their input and have the samevalue for ?.143English-CzechEnglish-SpanishEnglish-GermanEnglish-RussianEnglish-FrenchKendall 0.16 0.170 0.183 0.193 0.218Spearman 0.157 0.170 0.181 0.192 0.215Hamming 0.150 0.163 0.168 0.187 0.196FuzzyScore 0.155 0.166 0.178 0.189 0.215Ulam 0.159 0.170 0.181 0.189 0.221PEFs 0.156 0.173 0.185 0.196 0.219PETs 0.157 0.165 0.182 0.195 0.216Table 1: Sentence level Kendall tau scores fortranslation out of English with ?
= 0.5 and ?
=0.6Czech-EnglishSpanish-EnglishGerman-EnglishRussian-EnglishFrench-EnglishKendall 0.196 0.265 0.235 0.173 0.223Spearman 0.199 0.265 0.236 0.173 0.222Hamming 0.172 0.239 0.215 0.157 0.206FuzzyScore 0.184 0.263 0.228 0.169 0.216Ulam 0.188 0.264 0.232 0.171 0.221PEFs 0.201 0.265 0.237 0.181 0.228PETs 0.200 0.264 0.234 0.174 0.221Table 2: Sentence level Kendall tau scores fortranslation into English with ?
= 0.5 and ?
= 0.65.1 Does hierarchical structure improveevaluation?The results in Tables 1, 2 and 3 suggest that thePEFscore which uses hierarchy over permutationsoutperforms the string based permutation metricsin the majority of the language pairs.
The mainexception is the English-Czech language pair inwhich both PETs and PEFs based metric do notgive good results compared to some other met-rics.
For discussion about English-Czech look atthe section 6.1.5.2 Do PEFs help over one canonical PET?From Figures 9 and 10 it is clear that using allpermutation trees instead of only canonical onesmakes the metric more stable in all language pairs.Not only that it makes results more stable but itmetric avg rank avg KendallPEFs 1.6 0.2041Kendall 2.65 0.2016Spearman 3.4 0.201PETs 3.55 0.2008Ulam 4 0.1996FuzzyScore 5.8 0.1963Hamming 7 0.1853Table 3: Average ranks and average Kendallscores for each tested metrics over all languagepairsFigure 9: Plot of scaled Kendall tau correlation fortranslation from Englishalso improves them in all cases except in English-Czech where both PETs and PEFs perform badly.The main reason why PEFs outperform PETs isthat they encode all possible phrase segmentationsof monotone and inverted sub-permutations.
Bygiving the score that considers all segmentations,PEFs also include the right segmentation (the oneperceived by human evaluators as the right seg-mentation), while PETs get the right segmentationonly if the right segmentation is the canonical one.5.3 Is improvement consistent over languagepairs?Table 3 shows average rank (metric?s position af-ter sorting all metrics by their correlation for eachlanguage pair) and average Kendall tau correlationcoefficient over the ten language pairs.
The tableshows clearly that the PEFs metric outperforms allother metrics.
To make it more visible how met-rics perform on the different language pairs, Fig-ures 9 and 10 show Kendall tau correlation co-efficient scaled between the best scoring metricfor the given language (in most cases PEFs) and144Figure 10: Plot of scaled Kendall tau correlationfor translation into Englishthe worst scoring metric (in all cases Hammingscore).
We can see that, except in English-Czech,PEFs are consistently the best or second best (onlyin English-French) metric in all language pairs.PETs are not stable and do not give equally goodresults in all language pairs.
Hamming distanceis without exception the worst metric for evalua-tion since it is very strict about positioning of thewords (it does not take relative ordering betweenwords into account).
Kendall tau is the only stringbased metric that gives relatively good scores inall language pairs and in one (English-Czech) it isthe best scoring one.6 Further experiments and analysisSo far we have shown that PEFs outperform theexisting metrics over the majority of languagepairs.
There are two pending issues to discuss.Why is English-Czech seemingly so difficult?And does preferring inversion over non-binarybranching correlate better with human judgement.6.1 The results on English-CzechThe English-Czech language pair turned out tobe the hardest one to evaluate for all metrics.All metrics that were used in the meta-evaluationthat we conducted give much lower Kendall taucorrelation coefficient compared to the other lan-guage pairs.
The experiments conducted by otherresearchers on the same dataset (Mach?a?cek andBojar, 2013), using full evaluation metrics, alsoget far lower Kendall tau correlation coefficientfor English-Czech than for other language pairs.In the description of WMT13 data that we used(Bojar et al., 2013), it is shown that annotator-agreement for English-Czech is a few times lowerthan for other languages.
English-Russian, whichis linguistically similar to English-Czech, doesnot show low numbers in these categories, and isone of the language pairs where our metrics per-form the best.
The alignment ratio is equally highbetween English-Czech and English-Russian (butthat does not rule out the possibility that the align-ments are of different quality).
One seeminglyunlikely explanation is that English-Czech mightbe a harder task in general, and might require amore sophisticated measure.
However, the moreplausible explanation is that the WMT13 data forEnglish-Czech is not of the same quality as otherlanguage pairs.
It could be that data filtering, forexample by taking only judgments for which manyevaluators agree, could give more trustworthy re-sults.6.2 Is inversion preferred over non-binarybranching?Since our original version of the scoring functionfor PETs and PEFs on the operator level does notdiscriminate between kinds of non-monotone op-erators (all non-monotone get zero as a score) wealso tested whether discriminating between inver-sion (binary) and non-binary operators make anydifference.English-CzechEnglish-SpanishEnglish-GermanEnglish-RussianEnglish-FrenchPEFs ?
= 0.0 0.156 0.173 0.185 0.196 0.219PEFs ?
= 0.5 0.157 0.175 0.183 0.195 0.219PETs ?
= 0.0 0.157 0.165 0.182 0.195 0.216PETs ?
= 0.5 0.158 0.165 0.183 0.195 0.217Table 4: Sentence level Kendall tau score fortranslation out of English different ?
with ?
= 0.5and ?
= 0.6Intuitively, we might expect that inverted binaryoperators are preferred by human evaluators overnon-binary ones.
So instead of assigning zero as ascore to inverted nodes we give them 0.5, while fornon-binary nodes we remain with zero.
The ex-periments with the inverted operator scored with0.5 (i.e., ?
= 0.5) are shown in Tables 4 and 5.The results show that there is no clear improve-ment by distinguishing between the two kinds of145Czech-EnglishSpanish-EnglishGerman-EnglishRussian-EnglishFrench-EnglishPEFs ?
= 0.0 0.201 0.265 0.237 0.181 0.228PEFs ?
= 0.5 0.201 0.264 0.235 0.179 0.227PETs ?
= 0.0 0.200 0.264 0.234 0.174 0.221PETs ?
= 0.5 0.202 0.263 0.235 0.176 0.224Table 5: Sentence level Kendall tau score fortranslation into English for different ?
with ?
=0.5 and ?
= 0.6non-monotone operators on the nodes.7 ConclusionsRepresenting order differences as compact permu-tation forests provides a good basis for develop-ing evaluation measures of word order differences.These hierarchical representations of permutationsbring together two crucial elements (1) groupingwords into blocks, and (2) factorizing reorder-ing phenomena recursively over these groupings.Earlier work on MT evaluation metrics has of-ten stressed the importance of the first ingredient(grouping into blocks) but employed it merely in aflat (non-recursive) fashion.
In this work we pre-sented novel metrics based on permutation treesand forests (the PETscore and PEFscore) wherethe second ingredient (factorizing reordering phe-nomena recursively) plays a major role.
Permuta-tion forests compactly represent all possible blockgroupings for a given permutation, whereas per-mutation trees select a single canonical grouping.Our experiments with WMT13 data show that ourPEFscore metric outperforms the existing string-based metrics on the large majority of languagepairs, and in the minority of cases where it is notranked first, it ranks high.
Crucially, the PEFs-core is by far the most stable reordering score overten language pairs, and works well also for lan-guage pairs with long range reordering phenom-ena (English-German, German-English, English-Russian and Russian-English).AcknowledgmentsThis work is supported by STW grant nr.
12271and NWO VICI grant nr.
277-89-002.
We thankTAUS and the other DatAptor project User Boardmembers.
We also thank Ivan Titov for helpfulcomments on the ideas presented in this paper.ReferencesMichael H. Albert and Mike D. Atkinson.
2005.
Sim-ple permutations and pattern restricted permutations.Discrete Mathematics, 300(1-3):1?15.Alexandra Birch and Miles Osborne.
2010.
LRscorefor Evaluating Lexical and Reordering Quality inMT.
In Proceedings of the Joint Fifth Workshop onStatistical Machine Translation and MetricsMATR,pages 327?332, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Alexandra Birch and Miles Osborne.
2011.
Reorder-ing Metrics for MT.
In Proceedings of the Associ-ation for Computational Linguistics, Portland, Ore-gon, USA.
Association for Computational Linguis-tics.Alexandra Birch, Miles Osborne, and Phil Blunsom.2010.
Metrics for MT evaluation: evaluating re-ordering.
Machine Translation, pages 1?12.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 1?44, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montr?eal, Canada, June.
Association forComputational Linguistics.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimizationand Evaluation of Machine Translation Systems.
InProceedings of the EMNLP 2011 Workshop on Sta-tistical Machine Translation.Daniel Gildea, Giorgio Satta, and Hao Zhang.
2006.Factoring Synchronous Grammars by Sorting.
InACL.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010.
Automaticevaluation of translation quality for distant languagepairs.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?10, pages 944?952, Stroudsburg, PA,USA.
Association for Computational Linguistics.Mirella Lapata.
2006.
Automatic Evaluation of In-formation Ordering: Kendall?s Tau.
ComputationalLinguistics, 32(4):471?484.146Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-man.
2004.
The significance of recall in auto-matic metrics for MT evaluation.
In Proceedings ofthe Sixth Conference of the Association for MachineTranslation in the Americas.Matou?s Mach?a?cek and Ond?rej Bojar.
2013.
Resultsof the WMT13 Metrics Shared Task.
In Proceed-ings of the Eighth Workshop on Statistical MachineTranslation, pages 45?51, Sofia, Bulgaria, August.Association for Computational Linguistics.Gideon Maillette de Buy Wenniger and Khalil Sima?an.2011.
Hierarchical Translation Equivalence overWord Alignments.
In ILLC Prepublication Series,PP-2011-38.
University of Amsterdam.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof ACL?02, pages 311?318, Philadelphia, PA, USA.Milo?s Stanojevi?c and Khalil Sima?an.
2014.
BEER:BEtter Evaluation as Ranking.
In Proceedings of theNinth Workshop on Statistical Machine Translation,pages 414?419, Baltimore, Maryland, USA, June.Association for Computational Linguistics.David Talbot, Hideto Kazawa, Hiroshi Ichikawa, JasonKatz-Brown, Masakazu Seno, and Franz Och.
2011.A Lightweight Evaluation Framework for MachineTranslation Reordering.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages12?21, Edinburgh, Scotland, July.
Association forComputational Linguistics.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 3(23):377?403.Hao Zhang and Daniel Gildea.
2007.
Factorizationof Synchronous Context-Free Grammars in LinearTime.
In NAACL Workshop on Syntax and Structurein Statistical Translation (SSST), pages 25?32.Hao Zhang, Daniel Gildea, and David Chiang.
2008.Extracting synchronous grammar rules from word-level alignments in linear time.
In Proceedingsof the 22nd International Conference on Computa-tional Linguistics-Volume 1, pages 1081?1088.
As-sociation for Computational Linguistics.147
