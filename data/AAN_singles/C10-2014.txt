Coling 2010: Poster Volume, pages 117?125,Beijing, August 2010Tree Topological Features for Unlexicalized ParsingSamuel W. K. Chan?
Lawrence Y. L. Cheung# Mickey W. C.
Chong??Dept.
of Decision SciencesChinese University of Hong Kong#Dept.
of Linguistics & Modern LanguagesChinese University of Hong Kong{swkchan, yllcheung, mickey_chong}@cuhk.edu.hkAbstractAs unlexicalized parsing lacks word to-ken information, it is important to inves-tigate novel parsing features to improvethe accuracy.
This paper studies a set oftree topological (TT) features.
Theyquantitatively describe the tree shapedominated by each non-terminal node.The features are useful in capturing lin-guistic notions such as grammaticalweight and syntactic branching, whichare factors important to syntactic proc-essing but overlooked in the parsing lit-erature.
By using an ensemble classifier-based model, TT features can signifi-cantly improve the parsing accuracy ofour unlexicalized parser.
Further, theease of estimating TT feature valuesmakes them easy to be incorporated intovirtually any mainstream parsers.1 IntroductionMany state-of-the-art parsers work with lexical-ized parsing models that utilize the informationand statistics of word tokens (Magerman, 1995;Collins, 1999, 2003; Charniak, 2000).
The per-formance of lexicalized models is susceptible tovocabulary variation as lexical statistics is oftencorpus-specific (Ratnaparkhi, 1999; Gildea,2001).
As parsers are typically evaluated usingthe Penn Treebank (Marcus et al, 1993), whichis based on financial news, the problems oflexicalized parsing could easily be overlooked.Unlexicalized models, on the other hand, areless sensitive to lexical variation and are moreportable across domains.
Though the perform-ance of unlexicalized models was believed notto exceed that of lexicalized models (Klein &Manning, 2003), Petrov & Klein (2007) showthat unlexicalized parsers can match lexicalizedparsers in performance using the grammar rulesplitting technique.
Given the practical advan-tages and the latest development, unlexicalizedparsing deserves further scrutiny.A profitable direction of research on unlexi-calized parsing is to investigate novel parsingfeatures.
This paper examines a set of what wecall tree topological (TT) features, includingphrase span, phrase height, tree skewness, etc.This study is motivated by the fact that conven-tional parsers rarely consider the shape ofsubtrees dominated by these nodes and relyprimarily on matching tags.
As a result, an NPwith a complicated structure is treated the sameas an NP that dominates only one word.
How-ever, our study shows that TT features are use-ful predictors of phrase boundaries, a criticalambiguity resolution issue.
TT features havetwo more advantages.
First, TT features capturelinguistic properties, such as branching andgrammatical ?heaviness?, across different syn-tactic structures.
Second, they are easily com-putable without the need for extra language re-sources.The organization of the paper is as follows.Section 2 reviews the features commonly usedin parsing.
Section 3 provides the details of TTfeatures in the unlexicalized parser.
The parseris evaluated in Section 4.
In Section 5, wediscuss the effectiveness and advantages of TTfeatures in parsing and possible enhancement.This is followed by a conclusion in Section 6.2 Related Work2.1 Parsing FeaturesThis section reviews major types of informationin parsing.117Tags: The dominant types of information thatdrive parsing and chunking algorithms arePOS/syntactic tags, context-free grammar (CFG)rules, and their statistical properties.
Matchingtags against CFG rules to form phrases is centralto all basic parsing algorithms such as Cocke-Kasami-Younger (CKY) algorithm, and the Ear-ley algorithm, and the chart parsing.Word Token-based: Machine learning and sta-tistical modelling emerged in the 90s as an idealcomputational approach to feature-rich parsing.Classifiers can typically capitalize on a large setof features in decision making.
Magerman(1995), Ratnaparkh (1999) and Charniak (2000)used classifiers to model dependencies betweenword pairs.
They popularized the use word to-kens as attributes in lexicalized parsing.
Collins(1999, 2003) also integrated information likehead word and distance from head into the sta-tistical model to enhance probabilistic chartparsing.
Since then, word tokens, head wordsand their statistical derivatives have becomestandard features in many parsers.
Word tokeninformation is also fundamental to dependencyparsing (K?bler et al, 2009) because depend-ency grammar is rooted in the idea that the headand the dependent word are related by differentdependency relations.Semantic-based: Some efforts have also beenmade to consider semantic features, such assense tags, in parsing.
Words are first taggedwith semantic classes, often using WordNet-based resources.
The lexical semantic class canbe instructive to the selection of the correctparse from a set of candidate structures.
It hasbeen reported that the lexical semantics ofwords is effective in resolving structural ambi-guity, especially PP-attachment (Black et al,1992; Stetina & Nagao, 1997; Agirre et al,2008).
Nevertheless, the use of semantic fea-tures has still been relatively rare.
They incuroverheads in acquiring semantic language re-sources, such as sense-tagged corpora andWordNet databases.
Semantic-based parsingalso requires accurate sense-tagging.Since substantial gain from tag features isunlikely in the near future and deriving seman-tic features is often a tremendous task, there is apressing need to seek for new features, particu-larly in unlexicalized parsing.2.2 Linguistic-motivated FeaturesIn this section, a review of the linguistic motiva-tion behind the TT features is provided.Grammatical Weight: Apart from syntacticcategories, linguists have long observed that thenumber of words (often referred to as ?weight?or ?heaviness?)
in a phrase can affect syntacticprocessing of sentences (Quirk et al, 1985; Wa-sow, 1997; Rosenbach, 2005).
It correspondsroughly to the span feature described in Section3.2.
The effect of grammatical weight oftenmanifests in word order variation.
Heavy NPshift, dative alternation, particle movement andextraposition in English are canonical exampleswhere ?heavy?
chunks get dislocated to the endof a sentence.
In his corpus analysis, Wasow(1997) found that weight is a very crucial factorin determining dative alternation.
Hawkins(1994) also argued that due to processing con-straints, the human syntactic processor tends togroup an incoming stream of words as rapidlyas possible, preferring smaller chunks on the left.Tree Topology: CFG-based parsing approachhides the structural properties of the dominatedsubtree from the associated syntactic tag.
Struc-tural topology, or tree shape, however, can beuseful in guiding the parser to group tags intophrases.
Structures significantly deviating fromleft/right branching, e.g.
center embedding, aremuch more difficult to process and rare in pro-duction (Gibson, 1998).
Another example is theresolution of scope ambiguity in coordinatestructures (CSs).
CSs are common but notori-ously difficult to parse due to scope ambiguitywhen the conjuncts are complex (Collins, 1999;K?bler et al, 2009).
One good cue to the prob-lem is that humans prefer CSs with parallel in-ternal syntactic structures (Frazier et al, 2000).In a corpus-based study, Dubey et al (2008)show that structural repetition across conjunctsis significantly more frequent.
The implicationto parsing is that preference should be given tobracketing in which conjuncts are structurallysimilar.
TT information can inform the parser ofthe structural properties of phrases.3 An Ensemble-based ParserTo accommodate a large set of features, we optfor classifier-based parsing because classifiers118can easily handle many features, as pointed outin Ratnaparkhi (1999).
This is different fromchart parsing models popular in many parsers(e.g.
Collins, 2003) which require special statis-tical modelling.
Our parser starts from a stringof POS tags without any hints from words.
Asin other similar approaches (Abney 1991; Ram-shaw & Marcus, 1995; Sang, 2001; Sagae &Lavie, 2005), the first and the foremost problemthat has to be resolved is to identify the bound-ary points of phrases, without any explicitgrammar rules.
Here we adopt the ensemblelearning technique to unveil boundary points, orchunking points hereafter.
Two heterogeneousand mutually independent attribute feature setsare introduced in Section 3.2 and 3.3.3.1 Basic Architecture of the ParserOur parser has two modules, namely, a chunkerand a phrase recognizer.
The chunker locatesthe boundaries of chunks while the phrase rec-ognizer predicts the non-terminal syntactic tagof the identified chunks, e.g.
NP, VP, etc.
In thechunker, we explore a new approach that aimsat identifying chunk boundaries.
Assume thatthe input of the chunker is a tag sequence <x0 ?xn ?
xm> where 0 ?
n ?
m. Let yn be the point offocus between two consecutive tags xn and xn+1.The chunker classifies all focus points as eithera chunking point or a merging point at the rele-vant level.
A focus point yn is a merging point ifxn and xn+1 are siblings of the same parent nodein the target parse tree.
Otherwise, yn is a chunk-ing point.
Consider the tag sequence and theexpected classification of points in the examplebelow.
Chunking points are marked with ?%?and merging points with ?+?.PRP % VBZ % DT % RB   +  JJ  % NNHe    is    a    very    nice  guyThe point between RB and JJ is a mergingpoint because they are siblings of the parentnode ADJP in the target parse tree.
The pointbetween DT and RB is a chunking point sinceDT and RB are not siblings and do not share thesame parent node.
Chunks are defined as theconsecutive tag sequences not split up by %.When a focus point yn is classified as a chunk-ing point, it effectively means that no fragmentpreceding yn can combine with any fragmentfollowing yn to form a phrase, i.e.
a distituent.Both the chunker and the recognizer aretrained using the Penn Treebank (Marcus et al,1993).
In addition, we adopt the ensemble tech-nique to combine two sets of heterogeneous fea-tures.
The method yields a much more accuratepredictive power (Dietterich, 2000).
One neces-sary and sufficient condition for an ensemble ofclassifiers to be more accurate than any of itsindividual members is that the classifiers mustbe diverse.
Table 1 summaries the basic ration-ale of the parser.
The two feature sets will befurther explained in Section 3.2 and 3.3. Prepare training data from the Treebank basedon topological & information-theoretic features Train the chunker and phrase recognizer usingthe ensemble technique For any input tag sequence l,WHILE l contains more than one element DOIDENTIFY the status, + or %, of each focuspoint in lRECOGNIZE the syntactic tag (ST) of eachidentified chunkUPDATE l with the new ST sequenceENDWHILE Display the parse treeTable 1.
Basic rationale of the parserThe learning module acquires the knowledgeencoded in the Penn Treebank to support vari-ous classification tasks.
The input tag sequenceis first fed into the chunker.
The phrase recog-nizer then analyzes the chunker?s output andassigns non-terminal syntactic tags (e.g.
NP, VP,etc.)
to identified chunks.
The updated tag se-quence is fed back to the chunker for processingat the next level.
The iteration continues until acomplete parse is formed.3.2 Tree Topological Feature SetTree topological (TT) features describe theshape of subtrees quantitatively.
Our approachto addressing this problem involves examining aset of topological features, without any assump-tion of the word tokens.
They all have been im-plemented for chunking.Node Coordinates (NCs): NCs include the levelof focus (LF) and the relative position (RP) ofthe target subtree.
The level of focus is definedas the total number of levels under the targetnode, with the terminal level inclusive while theRP indicates the linear position of the targetnode in that level.
As in Figure 1, the LF for119subtree A and B are the same; however, the RPfor subtree A is smaller than that for subtree B.Span Ratio (SR): The SR is defined as the totalnumber of terminal nodes spanned under thetarget node and is divided by the length of thesentence.
In Figure 1, the span ratio for the tar-get node VP at subtree B is 5/12.
This ratio il-lustrates not only how many terminal nodes arecovered by the target node, but also how far thetarget node is from the root S.Aspect Ratio (AR): The AR of a target node in asubtree is defined as the ratio of the total num-ber of non-terminal nodes involved to the totalnumber of terminal nodes spanned.
The AR isalso indicative of the average branching factorof the subtree.Skewness Measure (SM): The SM estimates thedegree to which the subtree leans towards eitherleft or right.
In this research, the SM of a subtreeis evaluated by the distribution of the length ofthe paths connecting the target node and eachterminal node it dominates.
The length of a pathfrom a target node V to a terminal node T is thenumber of edges between V and T. For a treewith n terminal nodes, there are n paths.
A pivotis defined as the [n/2]th terminal node when n isodd and between [n/2]th and [(n+1)/2]th termi-nal nodes if n is even, where [ ] is a ceilingfunction.
The SM is defined as( )?????????????=??=>31301???
?niiiixxSMiEqn (1)where xi is the length of the i-th path pointing tothe i-th terminal node, x and ?
are the averageand standard deviation of the length of all pathsat that level of focus (LF).
?i is the distancemeasured from the i-th terminal node to thepivot.
The distance is positive if the terminalnode is to the left of the pivot, zero if it is rightat the pivot, and negative if the terminal node isto the right of the pivot.
Obviously, if thelengths of all paths are the same in the tree, thenumerator of Eqn (1) will be crossed out and theSM returns to zero.
The pivot also provides anaxis of vertical flipping where the SM still holds.The farther the terminal node from the pivot, thelonger the distance.
The distances ?
provide themoment factors to quantify the skewness oftrees.
For illustration, let us consider subtree Bwith the target node VP at level of focus (LF) =4 in Figure 1.
Since there are five terminalnodes, the pivot is at the third node VB.
Thelengths of the paths xi from left to right in thesubtree are 1, 2, 3, 4, 4 and the moment factors?i for the paths are 2, 1, 0, -1, -2.
Assuming thatx and ?
for all the trees in the Treebank atlevel 4 are, say, 2.9 and 1.2 respectively, thenSM = -3.55.
It implies that subtree B under thetarget node VP has a strong right branching ten-dency, even though it has a very uniformbranching factor which is usually defined as thenumber of children at each node.          !Figure 1.
Two different subtrees in the sentence SIn our parser, to determine whether the twotarget nodes at level 4, i.e., NP and VP, shouldbe merged to form a S at level 5 or not, an at-tribute vector with TT features for both NP andVP are devised as a training case.
The corre-sponding target attribute is a binary value, i.e.,chunking vs. merging.
In addition, a set of if-merged attributes are introduced.
For example,the attribute SM-if-merged indicates the changesof the SM if both target nodes are merged.
Thisis particularly helpful since they are predictiveunder our bottom-up derivation strategy.3.3 Information-Theoretic Feature SetContext features are usually helpful in manyapplications of supervised language learning.
Inmodelling context, one of the most centralmethodological concepts is co-occurrence.While collocation is the probabilistic co-occurrence of pure word tokens, colligation isdefined as the co-occurrence of word tokenswith grammatical patterning such as POS cate-120gories (Hunston, 2001).
In this research, to cap-ture the colligation without word tokens, a slid-ing window of 6 POS tags at the neighborhoodof the focus point yn is defined as our first set ofcontext attributes.
In addition, we define a set ofinformation-theoretic (IT) attributes which re-flect the likelihood of the fragment collocation.Various adjacent POS fragments around thefocus point yn are constructed, as in Table 2.xn-2 xn-1 xn xn+1 xn+2 xn+3 Colligation meas.xn-1 xn   d1:?
(xn-1, xn)xn xn+1 d2:?
(xn, xn+1)xn+1 xn+2 d3:?
(xn+1, xn+2)xn-2 xn-1 xn   d4:?
(xn-2xn-1, xn)xn-1 xn xn+1 d5:?
(xn-1xn, xn+1)xn xn+1 xn+2 d6:?
(xn, xn+1xn+2)xn+1 xn+2 xn+3 d7:?
(xn+1, xn+2xn+3)Table 2.
Colligation as context measure in various adjacentPOS fragments where the focus point yn is between xn andxn+1An n-gram is treated as a 2-gram of an n1-gram and an n2-gram, where n1 + n2 = n(Magerman & Marcus, 1990).
The information-theoretic function ?, namely, mutual informa-tion (MI), quantifies the co-occurrence of frag-ments.
MI compares the probability of observ-ing n1-gram and n2-gram together to the prob-ability of observing them by chance (Church &Hanks, 1989).
Here is an example illustratingthe set of attributes.
Take the point yn betweenRB and JJ in Section 3.1 as an example.
d5represents the MI between (DT RB) and JJ, i.e.MI(DT/RB, JJ).3.4 Multiple Classifications using Ensem-ble TechniqueThe basic idea of ensemble techniques involvesconsidering several classification methods ormultiple outputs to reach a decision.
An ensem-ble of classifiers is a set of classifiers whoseindividual decisions are combined in someway,?typically by weighted or un-weighted vot-ing to classify new examples.
Empiricallyspeaking, ensembles methods deliver highlyaccurate classifiers by combining less accurateones.
They tend to yield better results than asingle classifier in those situations when differ-ent classifiers have different error characteris-tics and their errors can compensate each other.Two questions need to be addressed whenbuilding and using an ensemble that integratesthe predictions of several classifiers.
First, whatdata are used to train the classifiers so that theerrors made by one classifier could be remediedby the other?
Second, how are the individualclassifiers fused or integrated to produce a finalensemble prediction?
As shown in the last twosections, we address the first question by intro-ducing two heterogeneous and mutually inde-pendent attribute feature sets, namely the treetopological (TT) features and information-theoretic (IT) features.
Instead of training all thefeatures to form a single giant classifier, weproduce two distinct, sometimes diversified,training sets of data to form two separate mod-erate classifiers, in the hope that they will pro-duce a highly accurate prediction.
The secondquestion is addressed by employing the boostingalgorithm.
Boosting is an effective method thatproduces a very accurate prediction rule bycombining rough and moderately inaccuraterules of thumb (Schapire & Singer, 2000).
Itgenerates the classifiers in an iterative way.
Atthe early beginning, an initial base classifierusing a set of training data with equal weight isfirst constructed.
When the prediction of thebase classifier differs from the expected out-come, the weight of the poorly predicted data isincreased to an extent based on their misclassi-fication rate on the preceding classifiers.
As aresult, the learning of the subsequent classifierwill focus on learning the training data that aremisclassified, or poorly predicted.
This processcontinues until a specified number of iterationsis reached or a predefined termination conditionis met.
The ensemble prediction is also aweighted voting process, where the weight of aclassifier is based on its errors over the trainingdata used to generate it.
The first practicalboosting algorithm, AdaBoost, was introducedby Freund & Schapire (1997), and solved manypractical difficulties of the earlier boosting algo-rithms.
Table 3 illustrates the main idea of thealgorithm.
Interested readers can refer to theliterature for detailed discussion (Freund &Schapire, 1997; Hastie et al, 2001).121Given: (x1, y1),..,(xm, ym) where xi ?
X, yi ?
Y = {-1, +1}Initialize D1(i) = 1/mFor t = 1, ?, T Train a weak learner using distribution Dt Get a weak hypothesis ht : X ?
{-1, +1} with error?t = Pri~Dt[ht(xi) ?
yi] Choose  ????????
?=ttt ??
?1ln21 Update:Dt+1(i) =????=??iitiitttyxheyxheZiDtt)(if)(if)(?
?=titittZxhyiD ))(exp()( ?
?where Zt is a normalization factor Output:H(x) = ??????
?=Tttt xh1)(sign ?Table 3.
Adaboost algorithm4 Experimental ResultsTable 4 presents some sampled statistics of theskewness measure (SM) of some major phrasetypes, which include VP, NP, S, and PP, basedon Sections 2?21 of the Penn Treebank (Mar-cus et al, 1993).VP L2-VP L3-VP L4-VP L5-VPN 18,406 22,052 18,035 15,911Mean -1.022 -4.454 -4.004 -3.738S.D.
1.018 1.406 1.438 1.405tscore  284.085* -31.483* -17.216*NP L2-NP L3-NP L4-NP L5-NPN 23,270 28,172 10,827 8,375Mean 1.013 -1.313 -1.432 -2.171S.D.
1.284 2.013 1.821 1.628tscore  158.748* 5.609* 29.614*S L2-S L3-S L4-S L5-SN 2,233 5,020 7,049 7,572Mean 0.688 -1.825 -1.459 -1.517S.D.
1.229 2.732 2.451 2.128tscore  54.031* -7.568* 1.523PP L2-PP L3-PP L4-PP L5-PPN 53,589 11,329 11,537 5,057Mean -1.337 -3.322 -3.951 -3.301S.D.
0.935 1.148 1.112 1.183tscore  172.352* 42.073* -33.173*Table 4.
SM values for various phrases (* = the mean inthe column is statistically significantly different from themean in the immediately following column, with degree offreedom in all cases greater than 120)For illustration purpose, the count of Level 2 VPsubtrees, their SM mean and standard deviationare -1.022 and 1.018 respectively.
Weperformed t-tests for difference in means be-tween various levels, even under the samephrase type.
For example, the t score for thedifference in mean between L2-VP and L3-VPis 284.085, which indicates a strong differencein SM values between the two levels.The means of all phrases beyond level 2 arenegative, consistent with the fact that English isgenerally a right branching language.
When wecompare the SM values across phrase types, it iseasy to notice that VPs and PPs have largernegative values, meaning that the skewness tothe right is more prominent.
Even within thesame phrase type, the SM values may differ sig-nificantly as one moves from its current level toparent level.
The SM offers an indicator thatdifferentiates different phrase types with differ-ent syntactic levels.
Chunkers can use this addi-tional parameter to do chunking better.Our parsing models were trained and testedusing the Penn Treebank (Marcus et al, 1993).Following the convention of previous studies,we pre-processed the trees by removing NULLelements and functional tags and collapsingADVP and PRT into ADVP.
Sections 2?21 areused for training and Section 23 for testing.
Toevaluate the contribution of the features, fivedifferent experiments were set up, as in Table 5.Experiment Features involvedE1 POS tags only (=baseline)E2 POS+ITE3 POS+IT+TT (node coordinates only)E4 POS+TT (with all features)E5 All features in E3 & E4Table 5.
Parsing features in five experimentsE1 is the baseline experiment with tag fea-tures only.
E2 and E4 include additional IT andTT features respectively.
E3 and E5 are partialand full mixture of the two feature types.
In theevaluation below, the chunker, phrase recog-nizer and parser are the same throughout thefive sets of experiments.
They only differ interms of features used (i.e.
E1?E5).
We firststudy the impact of the feature sets on chunking.Five chunkers CH1?CH5 are evaluated.Table 6 shows the training and test errors infive different chunkers in the respective ex-periments.
All chunkers were trained using theensemble-based learning.
If one compares CH2and CH4, it is clear that both IT and TT features122enhance sentence chunking but the gain fromTT features (i.e.
CH4) is much more substantial.The best chunkers (CH4 and CH5) reduce thetest error rate from the baseline 4.36% to 3.25%.Chunkers Training error % Test error %CH1 1.66 4.36CH2 1.53 4.32CH3 0.69 3.79CH4 0.33 3.25CH5 0.45 3.25Table 6.
Performance of the five chunkersSimilarly, the phrase recognizer uses ensem-ble learning to capture the rule patterns.
Insteadof reading off the rules straight from a lookuptable, the learning can predict the syntactic tagseven when it encounters rules not covered in thetreebank.
Certainly, the learning allows the rec-ognizer to take into account features more thanjust the tags.
The error rates in training and test-ing are 0.09% and 0.68% respectively.
Thechunker and the phrase recognizer were assem-bled to form a parser.
The features described inTable 5 were used to construct five parsers.
Weuse the PARSEVAL measures to compare theperformance as shown in Table 7.R P F CBs 0 CBs 2 CBsP1 78.9 77.6 78.3 1.6 48.7 76.4P2 81.9 79.7 80.8 1.5 50.6 78.7P3 85.1 82.8 83.4 1.4 53.3 80.2P4 84.1 82.2 83.1 1.5 52.7 78.1P5 84.7 83.4 84.0 1.3 54.6 80.5Table 7.
Performance of five parsers corresponding to fivedifferent experiments E1?E5Our baseline parser (P1) actually performsquite well.
With only tag features, it achieves anF-score of 78.3%.
Both IT and TT features canseparately enhance the parsing performance (P2and P4).
However, the gain from TT features(78.383.1%) is much more than that from ITfeatures (78.380.8%).
When the two featuresets are combined, they consistently producebetter results.
The best (P5) has an F-score of84.0%.
Even though the test errors in CH4 andCH5 are the same as shown in Table 6, P5 dem-onstrates that the cooperative effect of utilizingTT and IT features and leads to better parsingresults.5 Discussion5.1 Tree Topology and StructuresOur study has provided a way to quantitativelycapture linguists?
various insights that tree to-pology is helpful in syntactic structure building(e.g.
grammatical weight, subtree shape, etc.
).The SM seems to capture the basic right branch-ing property.
It is noteworthy that Collins (2003)found that the parsing model that can learn thebranching property of structures delivers a muchbetter parsing performance over the one thatcannot.
In our case, chunkers refer to TT fea-tures to distinguish different phrase types andlevels, and assign chunking points in such a waythat the resulting phrases can be maximallysimilar to the trees in the treebank topologically.Apart from the overall accuracy, one may ask inwhat way TT features improve parsing.
Here weprovide our preliminary analysis on one syntac-tic construction that can be benefitted from aTT-feature-aware parser.
The structure is coor-dinate structures (CSs).
A practical cue is thatconjuncts tend to be similar syntactically (andsemantically).
TT-feature-aware parsers canproduce more symmetrical conjuncts.
All rulesof the form ?XP ?
XP ?and?
XP?
were ex-tracted from the training data.NP L3 (-CS) L3-(+CS) L4 (-CS) L4-(+CS)N 27,950 222 10,222 605Mean -1.321 -0.397 -1.448 -1.162S.D.
2.010 2.190 1.806 2.047tscore  -6.266* -3.360*VP L3 (-CS) L3-(+CS) L4 (-CS) L4-(+CS)N 21,855 197 17,711 324Mean -4.488 -0.628 -4.063 -0.793S.D.
1.350 2.136 1.364 1.676tscore  -25.319* -34.908*Table 8.
TT feature values of coordinate structures (+CS =node that immediately dominates a CS; -CS otherwise; * =the mean in the column is statistically significantly differ-ent from the mean in the immediately following column).We compared the SM of CS and non-CS phrasesusing t-tests for mean difference.
The t-score iscalculated based on unequal sample sizes andunequal variances.
As shown in Table 8, wehave to reject the null hypothesis that theirmeans of the SM, between phrases with andwithout a CS, are equal at ?
= 0.0005 signifi-cance level.
In other words, phrases with andwithout a CS are statistically different.
+CSphrases are much more balanced with a smallerSM value from -0.4 to -1.2.
-CS columns gener-ally have a much larger SM value, ranging from123-1.321 to -4.488.
The SM offers information forthe chunkers to avoid over- or under-chunkingconjuncts in phrases with a coordination marker(e.g.
?and?
).5.2 Implications to ParsingThe findings in Section 4 indicate that the pre-sented initial version of the unlexicalized parserperforms on a par with the first generation lexi-calized parsers (e.g.
Magerman, 1995).
Thepromising results have two implications.
First,the integration of IT and TT features producessubstantial gain over the baseline model.
TTfeatures consistently outperform IT features bya noticeable margin.
To the best of our knowl-edge, TT features have not been systematicallyinvestigated in parsing before.
The effectivenessof these new features suggests that in addition toimproving algorithms, practitioners should notoverlook the development of new features.Second, the implementation of TT and IT fea-tures is simple and relatively computationallyinexpensive.
No extra resources or complicatedalgorithms are needed to compute TT features.Most importantly, they are suitable to the strin-gent requirements of unlexicalized parsing inwhich no word token information is allowed.The features can be added to other parsers rela-tively easily without substantial changes.5.3 Further WorkThe reported parsing results pertain to the initialversion of the parser.
There is still room for fur-ther improvement.
First, it would be interestingto integrate TT features in combination withother design features (e.g.
rule splitting) into theunlexicalized parser to enhance the results.Moreover, TT features is likely to enhance lexi-calized parsers too.
Second, more detailedanalysis of TT features can be conducted in dif-ferent syntactic constructions.
It is quite possi-ble that TT features are more useful to somesyntactic structures than others.
TT featuresseem to be good cues for identifying CSs.
It ispossible to compare the outputs from parserswith and without TT features (e.g.
P1 vs. P4).The contribution of TT features towards specificconstructions can be estimated empirically.Third, an insight from Collins (2003) is thathead words and their POS tags in lexicalizedparsing can improve parsing.
In unlexicalizedmodels, one can use the head POS tag alone toapproximate similar mechanism.6 ConclusionThis paper has demonstrated that TT featuresgive rise to substantial gain in our classifier-based unlexicalized parser.
The IT features havebeen explored as well, though the performancegain is more moderate.
TT features can be inex-pensively computed and flexibly incorporatedinto different types of parsers.
Our parsingmodel matches early lexicalized parsing modelsin performance, and has good potential to doeven better with adjustment and optimization.The statistical analysis of the treebank showsthat TT features are effective in capturing basiclinguistic properties, such as grammaticalweight and branching direction, which are over-looked in previous studies of parsing.
We havealso hinted how TT features may have reducedchunking errors of CSs by producing balancedconjuncts.
Though the present study focuses onunlexicalized parsing, it is likely that TT fea-tures can contribute to accuracy enhancement inother parsing models as well.AcknowledgmentsThe work described in this paper was partiallysupported by the grants from the ResearchGrants Council of the Hong Kong Special Ad-ministrative Region, China (Project Nos.CUHK440607 and CUHK440609).
We alsothank Henry Lee, our computer officer, for hisnetwork support during the experiments.ReferencesAbney, Steven.
1991.
Parsing by Chunks.
In Ber-wick, R., Abney, S., Tenny, C.
(eds.
), Principle-Based Parsing.
Kluwer Academic.Agirre, Eneko, Timothy Baldwin, and David Marti-nez.
2008.
Improving Parsing and PP AttachmentPerformance with Sense Information.
In Proceed-ings of the 46th Annual Meeting of the HumanLanguage Technology Conference (HLT?08).Black, Ezra, Frederick Jelinek, John Lafferty, DavidMagerman, Robert Mercer, and Salim Roukos.1992.
Towards History-based Grammars: UsingRicher Models for Probabilistic Parsing.
In Pro-ceedings of the 5th DARPA Speech and NaturalLanguage Workshop.124Charniak, Eugene.
2000.
A Maximum-Entropy-Inspired Parser.
In Proceedings of the 1st Meetingof the North American Chapter of the Associationof Computational Linguistics.Church, Kenneth.
and Patrick Hanks.
1989.
WordAssociation Norms, Mutual Information and Lexi-cography.
In Proceedings of the Association forComputational Linguistics 27.Collins, Michael.
1999.
Head-driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia.Collins, Michael.
2003.
Head-Driven StatisticalModels for Natural Language Parsing.
Computa-tional Linguistics 29 (4): 589?637.Dieterich, Thomas G. 2000.
Ensemble Methods inMachine Learning.
Lecture Notes in ComputerScience, v.1857.Dubey, Amit, Frank Keller, and Patrick Sturt.
2008A Probabilistic Corpus-based Model of SyntacticParallelism.
Cognition 109 (3): 326-344.Frazier, Lyn, Alan Munn and Charles Clifton 2000.Processing Coordinate Structures.
Journal of Psy-cholinguistic Research 29 (4): 343?370.Freund, Yoav and Robert E. Schapire.
1997.
A Deci-sion-Theoretic Generalization of On-line Learningand an Application to Boosting.
Journal of Com-puter and System Sciences 55 (1): 119?139.Gibson, Edward.
1998.
Linguistic Complexity: Lo-cality of Syntactic Dependencies.
Cognition 68(1): 1?76.Gildea, Daniel.
2001.
Corpus Variation and ParserPerformance.
In Proceedings of 2001 Conferenceon Empirical Methods in Natural Language Proc-essing (EMNLP).Hastie, Trevor, Robert Tibshirani  and Jerome Fried-man.
2001.
The Elements of Statistical Learning.Springer.Hawkins, John.
1994.
A Performance Theory of Or-der and Constituency.
Cambridge Univ.
Press.Hunston, Susan.
2001.
Colligation, Lexis, Pattern,and Text.
In M. Scott and G. Thompson.
(ed.
),Patterns of Text: In Honour of Michael Hoey.Amsterdam, Philadelphia: John Benjamins.Klein, Dan, and Christopher Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of the41st Meeting of the Association for Computa-tional Linguistics.K?bler, Sandra, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Morgan & ClaypoolPublishers.Magerman, David.
1995.
Statistical Decision-treeModels for Parsing.
In Proceedings of the 33rdAnnual Meeting on Association for ComputationalLinguistics.Magerman, David, and Mitchell Marcus.
1990.
Pars-ing a Natural Language Using Mutual InformationStatistics.
In Proceedings of 8th National Confer-ence on Artificial Intelligence (AAAI-90).Marcus, Mitchell, Beatrice.
Santorini, and MaryMarcinkiewicz 1993.
Building a Large AnnotatedCorpus of English: the Penn Treebank.
Computa-tional Linguistics 19 (2): 313?330.Petrov, Slav, and Dan Klein.
2007.
Learning andInference for Hierarchically Split PCFGs.
In Pro-ceedings of the 22nd Conference on Artificial In-telligence.Quirk, Randolph, Sidney Greenbaum, GeoffreyLeech, and Jan Svartvik.
1985.
A Grammar ofContemporary English.
London: Longman.Ramshaw, Lance A., and Mitchell P. Marcus.
1995.Text Chunking Using Transformation-basedLearning.
In Proceedings of the 3rd Workshop onVery Large Corpora.Ratnaparkhi, Adwait.
1999.
Learning to Parse Natu-ral Language with Maximum Entropy Models.Machine Learning 34 (1-3): 151?175.Rosenbach, Anette.
2005.
Animacy versus Weight asDeterminants of Grammatical Variation in Eng-lish.
Language 81 (3): 613-644.Sagae, Kenji, and Alon Lavie.
2005.
A Classifier-Based Parser with Linear Run-Time Complexity.In Proceedings of the Ninth International Work-shop on Parsing Technologies (IWPT).Sang, Erik.
2001.
Transforming a Chunker to aParser.
In J. Veenstra, W. Daelemans, K. Sima?an,J.
Zavrel (eds.
), Computational Linguistics in theNetherlands 2000.Schapire, Robert E., & Yoram Singer.
2000.
BoosT-exter: A Boosting-based System for Text Catego-rization.
Machine Learning 39 (2-3): 135?168.Stetina, Jiri, and Nagao, Makoto.
1997.
Corpus-based PP Attachment Ambiguity Resolution witha Semantic Dictionary.
In Proceedings of the 5thWorkshop on Very Large Corpora.Wasow, Thomas.
1997.
Remarks on GrammaticalWeight.
Language Variation and Change 9: 81?105.125
