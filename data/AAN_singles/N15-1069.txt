Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 672?682,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsUnsupervised Multi-Domain Adaptation with Feature EmbeddingsYi Yang and Jacob EisensteinSchool of Interactive ComputingGeorgia Institute of TechnologyAtlanta, GA 30308{yiyang+jacobe}@gatech.eduAbstractRepresentation learning is the dominant tech-nique for unsupervised domain adaptation, butexisting approaches have two major weak-nesses.
First, they often require the spec-ification of ?pivot features?
that generalizeacross domains, which are selected by task-specific heuristics.
We show that a novel butsimple feature embedding approach providesbetter performance, by exploiting the featuretemplate structure common in NLP problems.Second, unsupervised domain adaptation istypically treated as a task of moving from asingle source to a single target domain.
Inreality, test data may be diverse, relating tothe training data in some ways but not oth-ers.
We propose an alternative formulation,in which each instance has a vector of do-main attributes, can be used to learn distill thedomain-invariant properties of each feature.11 IntroductionDomain adaptation is crucial if natural languageprocessing is to be successfully employed in high-impact application areas such as social media, pa-tient medical records, and historical texts.
Unsuper-vised domain adaptation is particularly appealing,since it requires no labeled data in the target domain.Some of the most successful approaches to unsu-pervised domain adaptation are based on representa-tion learning: transforming sparse high-dimensionalsurface features into dense vector representations,1Source code and a demo are available at https://github.com/yiyang-gt/feat2vec1800 1750 1700 1650 1600 1550 1500NarrativesLettersDissertationtTheatreFigure 1: Domain graph for the Tycho Brahe cor-pus (Galves and Faria, 2010).
Suppose we want to adaptfrom 19th Century narratives to 16th Century disser-tations: can unlabeled data from other domains help?which are often more robust to domain shift (Blitzeret al, 2006; Glorot et al, 2011).
However, thesemethods are computationally expensive to train, andoften require special task-specific heuristics to selectgood ?pivot features.
?A second, more subtle challenge for unsuperviseddomain adaptation is that it is normally framed asadapting from a single source domain to a single tar-get domain.
For example, we may be given part-of-speech labeled text from 19th Century narratives,and we hope to adapt the tagger to work on academicdissertations from the 16th Century.
This ignorestext from the intervening centuries, as well as textthat is related by genre, such as 16th Century narra-tives and 19th Century dissertations (see Figure 1).We address a new challenge of unsupervised multi-domain adaptation, where the goal is to leverage thisadditional unlabeled data to improve performance inthe target domain.22Multiple domains have been considered in supervised do-main adaptation (e.g., Mansour et al, 2009), but these ap-proaches are not directly applicable when there is no labeleddata outside the source domain.672Figure 2: Representation learning techniques in structured feature spacesWe present FEMA (Feature EMbeddings for do-main Adaptation), a novel representation learningapproach for domain adaptation in structured featurespaces.
Like prior work in representation learning,FEMA learns dense features that are more robust todomain shift.
However, rather than performing rep-resentation learning by reconstructing pivot features,FEMA uses techniques from neural language mod-els to obtain low-dimensional embeddings directly.FEMA outperforms prior work on adapting POS tag-ging from the Penn Treebank to web text, and it eas-ily generalizes to unsupervised multi-domain adap-tation, further improving performance by learninggeneralizable models across multiple domains.2 Learning feature embeddingsFeature co-occurrence statistics are the primarysource of information driving many unsupervisedmethods for domain adaptation; they enable theinduction of representations that are more similaracross the source and target domain, reducing theerror introduced by domain shift (Ben-David et al,2010).
For example, both Structural Correspon-dence Learning (SCL; Blitzer et al, 2006) and De-noising Autoencoders (Chen et al, 2012) learn toreconstruct a subset of ?pivot features?, as shown inFigure 2(a).
The reconstruction function ?
whichis learned from unlabeled data in both domains ?
isthen employed to project each instance into a denserepresentation, which will hopefully be better suitedto cross-domain generalization.
The pivot featuresare chosen to be both predictive of the label and gen-eral across domains.
Meeting these two criteria re-quires task-specific heuristics; for example, differ-ent pivot selection techniques are employed in SCLfor syntactic tagging (Blitzer et al, 2006) and senti-ment analysis (Blitzer et al, 2007).
Furthermore, thepivot features correspond to a small subspace of thefeature co-occurrence matrix.
In Denoising Autoen-coders, each pivot feature corresponds to a densefeature in the transformed representation, but largedense feature vectors impose substantial computa-tional costs at learning time.
In SCL, each pivot fea-ture introduces a new classification problem, whichmakes computation of the cross-domain representa-tion expensive.
In either case, we face a tradeoffbetween the amount of feature co-occurrence infor-mation that we can use, and the computational com-plexity for representation learning and downstreamtraining.This tradeoff can be avoided by inducing lowdimensional feature embeddings directly.
We ex-ploit the tendency of many NLP tasks to divide fea-tures into templates, with exactly one active fea-ture per template (Smith, 2011); this is shown inthe center of Figure 2.
Rather than treating eachinstance as an undifferentiated bag-of-features, weuse this template structure to induce feature embed-dings, which are dense representations of individualfeatures.
Each embedding is selected to help pre-dict the features that fill out the other templates: forexample, an embedding for the current word featureis selected to help predict the previous word featureand successor word feature, and vice versa; see Fig-ure 2(b).
The embeddings for each active feature arethen concatenated together across templates, givinga dense representation for the entire instance.Our approach is motivated by word embeddings,673in which dense representations are learned for indi-vidual words based on their neighbors (Turian et al,2010; Xiao and Guo, 2013), but rather than learninga single embedding for each word, we learn embed-dings for each feature.
This means that the embed-ding of, say, ?toughness?
will differ depending onwhether it appears in the current-word template orthe previous-word template (see Table 6).
This pro-vides additional flexibility for the downstream learn-ing algorithm, and the increase in the dimensional-ity of the overall dense representation can be off-set by learning shorter embeddings for each feature.In Section 4, we show that feature embeddings con-vincingly outperform word embeddings on two part-of-speech tagging tasks.Our feature embeddings are based on theskip-gram model, trained with negative sam-pling (Mikolov et al, 2013a), which is a simpleyet efficient method for learning word embeddings.Rather than predicting adjacent words, the trainingobjective in our case is to find feature embeddingsthat are useful for predicting other active features inthe instance.
For the instance n ?
{1 .
.
.
N} andfeature template t ?
{1 .
.
.
T}, we denote fn(t) asthe index of the active feature; for example, in the in-stance shown in Figure 2, fn(t) = ?new?
when t in-dicates the previous-word template.
The skip-gramapproach induces distinct ?input?
and ?output?
em-beddings for each feature, written ufn(t)and vfn(t),respectively.
The role of these embeddings can beseen in the negative sampling objective,`n=1TT?t=1T?t?6=t[log ?(u>fn(t)vfn(t?
))+kEi?P(n)t?log ?
(?u>fn(t)vi)], (1)where t and t?are feature templates, k is the num-ber of negative samples, P(n)t?is a noise distributionfor template t?, and ?
is the sigmoid function.
Thisobjective is derived from noise-contrastive estima-tion (Gutmann and Hyv?arinen, 2012), and is cho-sen to maximize the unnormalized log-likelihood ofthe observed feature co-occurrence pairs, while min-imizing the unnormalized log-likelihood of ?nega-tive?
samples, drawn from the noise distribution.Feature embeddings can be applied to domainadaptation by learning embeddings of all featureson the union of the source and target data sets; weconsider the extension to multiple domains in thenext section.
The dense feature vector for each in-stance is obtained by concatenating the feature em-beddings for each template.
Finally, since it has beenshown that nonlinearity is important for generatingrobust representations (Bengio et al, 2013), we fol-low Chen et al (2012) and apply the hyperbolic tan-gent function to the embeddings.
The augmentedrepresentation x(aug)nof instance n is the concatena-tion of the original feature vector and the feature em-beddings,x(aug)n= xn?
tanh[ufn(1)?
?
?
?
?
ufn(T )],where ?
is vector concatenation.3 Feature embeddings across domainsWe now describe how to extend the feature em-bedding idea beyond a single source and target do-main, to unsupervised multi-attribute domain adap-tation (Joshi et al, 2013).
In this setting, each in-stance is associated with M metadata domain at-tributes, which could encode temporal epoch, genre,or other aspects of the domain.
The challenge ofdomain adaptation is that the meaning of featurescan shift across each metadata dimension: for ex-ample, the meaning of ?plant?
may depend on genre(agriculture versus industry), while the meaning of?like?
may depend on epoch.
To account for this, thefeature embeddings should smoothly shift over do-main graphs, such as the one shown in Figure 1; thiswould allow us to isolate the domain general aspectsof each feature.
Related settings have been consid-ered only for supervised domain adaptation, wheresome labeled data is available in each domain (Joshiet al, 2013), but not in the unsupervised case.More formally, we assume each instance n isaugmented with a vector of M binary domain at-tributes, zn?
{0, 1}M. These attributes may over-lap, so that we could have an attribute for the epoch1800-1849, and another for the epoch 1800-1899.We define zn,0= 1 as a shared attribute, which isactive for all instances.
We capture domain shiftby estimating embeddings h(m)ifor each feature icrossed with each domain attribute m. We thencompute the embedding for each instance by sum-ming across the relevant domain attributes, as shown674Figure 3: Aggregating multiple embeddings.in Figure 3.
The local ?input?
feature embeddingufn(t)is then defined as the summation, ufn(t)=?Mm=0zn,mh(m)fn(t).The role of the global embedding h(0)iis to cap-ture domain-neutral information about the featurei, while the other embeddings capture attribute-specific information.
The global feature embed-dings should therefore be more robust to domainshift, which is ?explained away?
by the attribute-specific embeddings.
We therefore use only theseembeddings when constructing the augmented rep-resentation, x(aug)n. To ensure that the global embed-dings capture all of the domain-general informationabout each feature, we place an L2 regularizer onthe attribute-specific embeddings.
Note that we donot learn attribute-specific ?output?
embeddings v;these are shared across all instances, regardless ofdomain.The attribute-based embeddings yield a new train-ing objective for instance n,`n=1TT?t=1T?t?6=t[log ?([M?m=0zn,mh(m)fn(t)]>vfn(t?
))+kEi?P(n)t?log ?(?[M?m=0zn,mh(m)fn(t)]>vi)].
(2)For brevity, we omit the regularizer from Equa-tion 2.
For feature fn(t), the (unregularized) gra-dients of h(m)fn(t)and vfn(t?
)w.r.t `n,tare?`n,th(m)fn(t)=1TT?t?6=tzn,m[(1?
?(u>fn(t)vfn(t?)))vfn(t?)?kEi?P(n)t??(u>fn(t)vi)vi](3)?`n,tvfn(t?)=1TT?t?6=t(1?
?(u>fn(t)vfn(t?)))ufn(t).
(4)For each feature i drawn from the noise distribu-tion P(n)t?, the gradient of viw.r.t `n,tis?`n,tvi= ?1T?(u>fn(t)vi)ufn(t).
(5)4 ExperimentsWe evaluate FEMA on part-of-speech (POS) tagging,in two settings: (1) adaptation of English POS tag-ging from news text to web text, as in the SANCLshared task (Petrov and McDonald, 2012); (2) adap-tation of Portuguese POS tagging across a graphof related domains over several centuries and gen-res, from the Tycho Brahe corpus (Galves and Faria,2010).
These evaluations are complementary: En-glish POS tagging gives us the opportunity to eval-uate feature embeddings in a well-studied and high-impact application; Portuguese POS tagging enablesevaluation of multi-attribute domain adaptation, anddemonstrates the capability of our approach in amorphologically-rich language, with a correspond-ingly large number of part-of-speech tags (383).
Asmore historical labeled data becomes available forEnglish and other languages, we will be able toevaluate feature embeddings and related techniquesthere.4.1 Implementation detailsWhile POS tagging is classically treated as a struc-tured prediction problem, we follow Schnabel andSch?utze (2014) by taking a classification-based ap-proach.
Feature embeddings can easily be used infeature-rich sequence labeling algorithms such asconditional random fields or structured perceptron,but our pilot experiments suggest that with suffi-ciently rich features, classification-based methodscan be extremely competitive on these datasets, ata fraction of the computational cost.
Specifically,we apply a support vector machine (SVM) classifier,675Component Feature templateLexical (5) wi?2= X,wi?1= Y, .
.
.Affixes (8)X is prefix of wi, |X| ?
4X is suffix of wi, |X| ?
4Orthography (3) wicontains number, uppercase char-acter, or hyphenTable 1: Basic feature templates for token wi.adding dense features from FEMA (and the alterna-tive representation learning techniques) to a set ofbasic features.4.1.1 Basic featuresWe apply sixteen feature templates, motivated byby Ratnaparkhi (1996).
Table 1 provides a summaryof the templates; there are four templates each forthe prefix and suffix features.
Feature embeddingsare learned for all lexical and affix features, yield-ing a total of thirteen embeddings per instance.
Wedo not learn embeddings for the binary orthographicfeatures.
Santos and Zadrozny (2014) demonstratethe utility of embeddings for affix features.4.1.2 Competitive systemsWe consider three competitive unsupervised do-main adaptation methods.
Structural Correspon-dence Learning (Blitzer et al, 2006, SCL) createsa binary classification problem for each pivot fea-ture, and uses the weights of the resulting classifiersto project the instances into a dense representation.Marginalized Denoising Autoencoders (Chen et al,2012, mDA) learn robust representation across do-mains by reconstructing pivot features from artifi-cially corrupted input instances.
We use structureddropout noise, which has achieved state-of-art re-sults on domain adaptation for part-of-speech tag-ging (Yang and Eisenstein, 2014).
We also directlycompare with WORD2VEC3word embeddings, andwith a ?no-adaptation?
baseline in which only sur-face features are used.4.1.3 Parameter tuningAll the hyperparameters are tuned on develop-ment data.
Following Blitzer et al (2006), we con-sider pivot features that appear more than 50 times in3https://code.google.com/p/word2vec/all the domains for SCL and mDA.
In SCL, the pa-rameter K selects the number of singular vectors ofthe projection matrix to consider; we try values be-tween 10 and 100, and also employ feature normal-ization and rescaling.
For embedding-based meth-ods, we choose embedding sizes and numbers ofnegative samples from {25, 50, 100, 150, 200} and{5, 10, 15, 20} respectively.
The noise distributionP(n)tis simply the unigram probability of each fea-ture in the template t. Mikolov et al (2013b) arguefor exponentiating the unigram distribution, but wefind it makes little difference here.
The window sizeof word embeddings is set as 5.
As noted above,the attribute-specific embeddings are regularized, toencourage use of the shared embedding h(0).
Theregularization penalty is selected by grid search over{0.001, 0.01, 0.1, 1.0, 10.0}.
In general, we find thatthe hyperparameters that yield good word embed-dings tend to yield good feature embeddings too.4.2 Evaluation 1: Web textRecent work in domain adaptation for natural lan-guage processing has focused on the data from theshared task on Syntactic Analysis of Non-CanonicalLanguage (SANCL; Petrov and McDonald, 2012),which contains several web-related corpora (news-groups, reviews, weblogs, answers, emails) as wellas the WSJ portion of OntoNotes corpus (Hovy etal., 2006).
Following Schnabel and Sch?utze (2014),we use sections 02-21 of WSJ for training and sec-tion 22 for development, and use 100,000 unlabeledWSJ sentences from 1988 for learning representa-tions.
On the web text side, each of the five targetdomains has an unlabeled training set of 100,000sentences (except the ANSWERS domain, which has27,274 unlabeled sentences), along with develop-ment and test sets of about 1000 labeled sentenceseach.
In the spirit of truly unsupervised domainadaptation, we do not use any target domain data forparameter tuning.Settings For FEMA, we consider only the single-embedding setting, learning a single feature embed-ding jointly across all domains.
We select 6918 pivotfeatures for SCL, according to the method describedabove; the final dense representation is produced byperforming a truncated singular value decomposi-tion on the projection matrix that arises from the676Target baseline MEMM SCL mDA word2vec FLORS FEMANEWSGROUPS 88.56 89.11 89.33 89.87 89.70 90.86 91.26REVIEWS 91.02 91.43 91.53 91.96 91.70 92.95 92.82WEBLOGS 93.67 94.15 94.28 94.18 94.17 94.71 94.95ANSWERS 89.05 88.92 89.56 90.06 89.83 90.30 90.69EMAILS 88.12 88.68 88.42 88.71 88.51 89.44 89.72AVERAGE 90.08 90.46 90.63 90.95 90.78 91.65 91.89Table 2: Accuracy results for adaptation from WSJ to Web Text on SANCL dev set.Target baseline MEMM SCL mDA word2vec FLORS FEMANEWSGROUPS 91.02 91.25 91.51 91.83 91.35 92.41 92.60REVIEWS 89.79 90.30 90.29 90.95 90.87 92.25 92.15WEBLOGS 91.85 92.32 92.32 92.39 92.42 93.14 93.43ANSWERS 89.52 89.74 90.04 90.61 90.48 91.17 91.35EMAILS 87.45 87.77 88.04 88.11 88.28 88.67 89.02AVERAGE 89.93 90.28 90.44 90.78 90.68 91.53 91.71Table 3: Accuracy results for adaptation from WSJ to Web Text on SANCL test set.weights of the pivot feature predictors.
The mDAmethod does not include any such matrix factor-ization step, and therefore generates a number ofdense features equal to the number of pivot features.Memory constraints force us to choose fewer pivots,which we achieve by raising the threshold to 200,yielding 2754 pivot features.Additional systems Aside from SCL andmDA, we compare against published results ofFLORS (Schnabel and Sch?utze, 2014), which usesdistributional features for domain adaptation.
Wealso republish the baseline results of Schnabel andSch?utze (2014) using the Stanford POS Tagger, amaximum entropy Markov model (MEMM) tagger.Results As shown in Table 2 and 3, FEMA outper-forms competitive systems on all target domains ex-cept REVIEW, where FLORS performs slightly bet-ter.
FLORS uses more basic features than FEMA;these features could in principle be combined withfeature embeddings for better performance.
Com-pared with the other representation learning ap-proaches, FEMA is roughly 1% better on average,corresponding to an error reduction of 10%.
Itstraining time is approximately 70 minutes on a 24-core machine, using an implementation based onFigure 4: Accuracy results with different latent dimen-sions on SANCL dev sets.gensim.4This is slightly faster than SCL, althoughslower than mDA with structured dropout noise.Figure 4 shows the average accuracy on theSANCL development set, versus the latent dimen-sions of different methods.
The latent dimension ofSCL is modulated by the number of singular vec-tors; we consider sizes 10, 25, 50, 75, and 100.
InmDA, we consider pivot feature frequency thresh-olds 500, 400, 300, 250, and 200.
For FEMA, weconsider embedding sizes 25, 50, 100, 150, and200.
The resulting latent dimensionality multipliesthese sizes by the number of non-binary templates4http://radimrehurek.com/gensim/677Task baseline SCL mDA word2vecFEMAsingleembeddingattributeembeddingsfrom 1800-1849?
1750 88.74 89.31 90.11 89.24 90.25 90.59?
1700 89.97 90.41 91.39 90.51 91.61 92.03?
1650 85.94 86.76 87.69 86.22 87.64 88.12?
1600 86.21 87.65 88.63 87.41 89.39 89.77?
1550 88.92 89.92 90.79 89.85 91.47 91.78?
1500 85.32 86.82 87.64 86.60 89.29 89.89AVERAGE 87.52 88.48 89.37 88.30 89.94 90.36from 1750-1849?
1700 94.37 94.60 94.86 94.60 95.14 95.22?
1650 91.49 91.78 92.52 91.85 92.56 93.26?
1600 91.92 92.51 93.14 92.83 93.80 93.89?
1550 92.75 93.21 93.53 93.21 94.23 94.20?
1500 89.87 90.53 91.31 91.48 92.05 92.95AVERAGE 92.08 92.53 93.07 92.80 93.56 93.90Table 4: Accuracy results for adaptation in the Tycho Brahe corpus of historical Portuguese.13.
FEMA dominates the other approaches acrossthe complete range of latent dimensionalities.
Thebest parameters for SCL are dimensionality K = 50and rescale factor ?
= 5.
For both FEMA andWORD2VEC, the best embedding size is 100 and thebest number of negative samples is 5.4.3 Evaluation 2: Historical PortugueseNext, we consider the problem of multi-attribute do-main adaptation, using the Tycho Brahe corpus ofhistorical Portuguese text (Galves and Faria, 2010),which contains syntactic annotations of Portuguesetexts in four genres over several centuries (Figure 1).We focus on temporal adaptation: training on themost modern data in the corpus, and testing on in-creasingly distant historical text.Settings For FEMA, we consider domain attributesfor 50-year temporal epochs and genres; we also cre-ate an additional attribute merging all instances thatare in neither the source nor target domain.
In SCLand mDA, 1823 pivot features pass the threshold.Optimizing on a source-domain development set, wefind that the best parameters for SCL are dimension-ality K = 25 and rescale factor ?
= 5.
The bestembedding size and negative sample number are 50and 15 for both FEMA and WORD2VEC.Results As shown in Table 4, FEMA outperformscompetitive systems on all tasks.
The column ?sin-gle embedding?
reports results with a single featureembedding per feature, ignoring domain attributes;the column ?attribute embeddings?
shows that learn-ing feature embeddings for domain attributes furtherimproves performance, by 0.3-0.4% on average.5 Similarity in the embedding spaceThe utility of word and feature embeddings for POStagging task can be evaluated through word simi-larity in the embedding space, and its relationshipto type-level part-of-speech labels.
To measure thelabel consistency between each word and its top Qclosest words in the vocabulary we compute,Consistency =?|V |i=1?Qj=1?
(wi, cij)|V | ?Q(6)where |V | is the number of words in the vocabulary,wiis the i-th word in the vocabulary, cijis the j-th closest word to wiin the embedding space (usingcosine similarity), ?
(wi, cij) is an indicator functionthat is equal to 1 if wiand cijhave the same mostcommon part-of-speech in labeled data.We compare feature embeddings of different tem-plates against WORD2VEC embeddings.
All em-beddings are trained on the SANCL data, which is678Embedding Q = 5 Q = 10 Q = 50 Q = 100WORD2VEC 47.64 46.17 41.96 40.09FEMA-current 68.54 66.93 62.36 59.94FEMA-prev 55.34 54.18 50.41 48.39FEMA-next 57.13 55.78 52.04 49.97FEMA-all 70.63 69.60 65.95 63.91Table 5: Label consistency of the Q-most similar wordsin each embedding.
FEMA-all is the concatenation of thecurrent, previous, and next-word FEMA embeddings.also used to obtain the most common tag for eachword.
Table 5 shows that the FEMA embeddingsare more consistent with the type-level POS tagsthan WORD2VEC embeddings.
This is not surpris-ing, since they are based on feature templates thatare specifically designed for capturing syntactic reg-ularities.
In simultaneously published work, Linget al (2015) present ?position-specific?
word em-beddings, which are an alternative method to inducemore syntactically-oriented word embeddings.Table 6 shows the most similar words for threequery keywords, in each of four different embed-dings.
The next-word and previous-word embed-dings are most related to syntax, because they helpto predict each other and the current-word feature;the current-word embedding brings in aspects of or-thography, because it must help to predict the affixfeatures.
In morphologically rich languages such asPortuguese, this can help to compute good embed-dings for rare inflected words.
This advantage holdseven in English: the word ?toughness?
appears onlyonce in the SANCL data, but the FEMA-current em-bedding is able to capture its morphological simi-larity to words such as ?tightness?
and ?thickness?.In WORD2VEC, the lists of most similar words tendto combine syntax and topic information, and fail tocapture syntactic regularities such as the relationshipbetween ?and?
and ?or?.6 Related WorkRepresentation learning Representational differ-ences between source and target domains can be amajor source of errors in the target domain (Ben-David et al, 2010).
To solve this problem, cross-domain representations were first induced via auxil-iary prediction problems (Ando and Zhang, 2005),such as the prediction of pivot features (Blitzer et?new?FEMA-current nephew, news, newlywed, newer,newspaperFEMA-prev current, local, existing, interna-tional, entireFEMA-next real, big, basic, local, personalWORD2VEC current, special, existing, newly,own?toughness?FEMA-current tightness, trespass, topless, thick-ness, tendernessFEMA-prev underside, firepower, buzzwords,confiscation, explorersFEMA-next aspirations, anguish, pointers, or-ganisation, responsibilitiesWORD2VEC parenting, empathy, ailment, rote,nerves?and?FEMA-current amd, announced, afnd, anesthetized,anguishedFEMA-prev or, but, as, when, althoughFEMA-next or, but, without, since, whenWORD2VEC but, while, which, because, practi-callyTable 6: Most similar words for three queries, in eachembedding space.al., 2006).
In these approaches, as well as in laterwork on denoising autoencoders (Chen et al, 2012),the key mechanism is to learn a function to predict asubset of features for each instance, based on otherfeatures of the instance.
Since no labeled data is re-quired to learn the representation, target-domain in-stances can be incorporated, revealing connectionsbetween features that appear only in the target do-main and features that appear in the source domaintraining data.
The design of auxiliary predictionproblems and the selection of pivot features both in-volve heuristic decisions, which may vary depend-ing on the task.
FEMA avoids the selection of pivotfeatures by directly learning a low-dimensional rep-resentation, through which features in each templatepredict the other templates.An alternative is to link unsupervised learning inthe source and target domains with the label dis-tribution in the source domain, through the frame-work of posterior regularization (Ganchev et al,2010).
This idea is applied to domain adaptationby Huang and Yates (2012), and to cross-lingual679learning by Ganchev and Das (2013).
This approachrequires a forward-backward computation for repre-sentation learning, while FEMA representations canbe learned without dynamic programming, throughnegative sampling.Word embeddings Word embeddings can beviewed as special case of representation learning,where the goal is to learn representations for eachword, and then to supply these representations inplace of lexical features.
Early work focused ondiscrete clusters (Brown et al, 1990), while morerecent approaches induce dense vector representa-tions; Turian et al (2010) compare Brown clus-ters with neural word embeddings from Collobertand Weston (2008) and Mnih and Hinton (2009).Word embeddings can also be computed via neu-ral language models (Mikolov et al, 2013b), orfrom canonical correlation analysis (Dhillon et al,2011).
Xiao and Guo (2013) induce word em-beddings across multiple domains, and concate-nate these representations into a single feature vec-tor for labeled instances in each domain, followingEasyAdapt (Daum?e III, 2007).
However, they donot apply this idea to unsupervised domain adapta-tion, and do not work in the structured feature settingthat we consider here.
Bamman et al (2014) learngeographically-specific word embeddings, in an ap-proach that is similar to our multi-domain featureembeddings, but they do not consider the applica-tion to domain adaptation.
We can also view the dis-tributed representations in FLORS as a sort of wordembedding, computed directly from rescaled bigramcounts (Schnabel and Sch?utze, 2014).Feature embeddings are based on a different phi-losophy than word embeddings.
While many NLPfeatures are lexical in nature, the role of a wordtowards linguistic structure prediction may differacross feature templates.
Applying a single wordrepresentation across all templates is therefore sub-optimal.
Another difference is that feature embed-dings can apply to units other than words, such ascharacter strings and shape features.
The tradeoffis that feature embeddings must be recomputed foreach set of feature templates, unlike word embed-dings, which can simply be downloaded and pluggedinto any NLP problem.
However, computing fea-ture embeddings is easy in practice, since it requiresonly a light modification to existing well-optimizedimplementations for computing word embeddings.Multi-domain adaptation The question of adap-tation across multiple domains has mainly been ad-dressed in the context of supervised multi-domainlearning, with labeled data available in all do-mains (Daum?e III, 2007).
Finkel and Manning(2009) propagate classification parameters across atree of domains, so that classifiers for sibling do-mains are more similar; Daum?e III (2009) showshow to induce such trees using a nonparametricBayesian model.
Dredze et al (2010) combine clas-sifier weights using confidence-weighted learning,which represents the covariance of the weight vec-tors.
Joshi et al (2013) formulate the problem ofmulti-attribute multi-domain learning, where all at-tributes are potential distinctions between domains;Wang et al (2013) present an approach for automat-ically partitioning instances into domains accordingto such metadata features.
Our formulation is relatedto multi-domain learning, particularly in the multi-attribute setting.
However, rather than partitioningall instances into domains, the domain attribute for-mulation allows information to be shared across in-stances which share metadata attributes.
We areunaware of prior research on unsupervised multi-domain adaptation.7 ConclusionFeature embeddings can be used for domain adap-tation in any problem involving feature templates.They offer strong performance, avoid practicaldrawbacks of alternative representation learning ap-proaches, and are easy to learn using existing wordembedding methods.
By combining feature em-beddings with metadata domain attributes, we canperform domain adaptation across a network of in-terrelated domains, distilling the domain-invariantessence of each feature to obtain more robust rep-resentations.Acknowledgments This research was supportedby National Science Foundation award 1349837.We thank the reviewers for their feedback.
Thanksalso to Hal Daum?e III, Chris Dyer, Slav Petrov, andDjam?e Seddah.680ReferencesRie Kubota Ando and Tong Zhang.
2005.
A frameworkfor learning predictive structures from multiple tasksand unlabeled data.
The Journal of Machine LearningResearch, 6:1817?1853.David Bamman, Chris Dyer, and Noah A. Smith.
2014.Distributed representations of geographically situatedlanguage.
In Proceedings of the Association for Com-putational Linguistics (ACL), pages 828?834, Balti-more, MD.Shai Ben-David, John Blitzer, Koby Crammer, AlexKulesza, Fernando Pereira, and Jennifer WortmanVaughan.
2010.
A theory of learning from differentdomains.
Machine learning, 79(1-2):151?175.Yoshua Bengio, Aaron Courville, and Pascal Vincent.2013.
Representation learning: A review and new per-spectives.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 35(8):1798?1828.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of Empirical Meth-ods for Natural Language Processing (EMNLP), pages120?128.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InProceedings of the Association for Computational Lin-guistics (ACL), pages 440?447, Prague.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,Jenifer C. Lai, and Robert L. Mercer.
1990.
Class-Based N-Gram models of natural language.
Computa-tional Linguistics, 18:18?4.Minmin Chen, Z. Xu, Killian Weinberger, and Fei Sha.2012.
Marginalized denoising autoencoders for do-main adaptation.
In Proceedings of the InternationalConference on Machine Learning (ICML).Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof the International Conference on Machine Learning(ICML), pages 160?167.Hal Daum?e III.
2007.
Frustratingly easy domain adapta-tion.
In Proceedings of the Association for Computa-tional Linguistics (ACL), Prague.Hal Daum?e III.
2009.
Bayesian multitask learning withlatent hierarchies.
In Proceedings of the Twenty-FifthConference on Uncertainty in Artificial Intelligence,pages 135?142.
AUAI Press.Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.2011.
Multi-view learning of word embeddings viacca.
In Advances in Neural Information ProcessingSystems, pages 199?207.Mark Dredze, Alex Kulesza, and Koby Crammer.
2010.Multi-domain learning by confidence-weighted pa-rameter combination.
Machine Learning, 79(1-2):123?149.E.
Eaton, M. Desjardins, and T. Lane.
2008.
Model-ing transfer relationships between learning tasks forimproved inductive transfer.
Machine Learning andKnowledge Discovery in Databases, pages 317?332.Jenny R. Finkel and Christopher Manning.
2009.
Hier-archical bayesian domain adaptation.
In Proceedingsof the North American Chapter of the Association forComputational Linguistics (NAACL), pages 602?610,Boulder, CO.Charlotte Galves and Pablo Faria.
2010.
TychoBrahe Parsed Corpus of Historical Portuguese.http://www.tycho.iel.unicamp.br/?tycho/corpus/en/index.html.Kuzman Ganchev and Dipanjan Das.
2013.
Cross-lingual discriminative learning of sequence modelswith posterior regularization.
In Proceedings of Em-pirical Methods for Natural Language Processing(EMNLP), pages 1996?2006.Kuzman Ganchev, Joao Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
The Journal of MachineLearning Research, 11:2001?2049.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Pro-ceedings of the International Conference on MachineLearning (ICML), Seattle, WA.Michael U Gutmann and Aapo Hyv?arinen.
2012.Noise-contrastive estimation of unnormalized statisti-cal models, with applications to natural image statis-tics.
The Journal of Machine Learning Research,13(1):307?361.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In Proceedings of the North Ameri-can Chapter of the Association for Computational Lin-guistics (NAACL), pages 57?60, New York, NY.Fei Huang and Alexander Yates.
2012.
Biased represen-tation learning for domain adaptation.
In Proceedingsof Empirical Methods for Natural Language Process-ing (EMNLP), pages 1313?1323.Mahesh Joshi, Mark Dredze, William W. Cohen, and Car-olyn P. Ros?e.
2013.
What?s in a domain?
multi-domain learning for multi-attribute data.
In Proceed-ings of the North American Chapter of the Associationfor Computational Linguistics (NAACL), pages 685?690, Atlanta, GA.Wang Ling, Chris Dyer, Alan Black, and Isabel Trancoso.2015.
Two/too simple adaptations of word2vec for681syntax problems.
In Proceedings of the North Ameri-can Chapter of the Association for Computational Lin-guistics (NAACL), Denver, CO.Yishay Mansour, Mehryar Mohri, and Afshin Ros-tamizadeh.
2009.
Domain adaptation with multiplesources.
In Neural Information Processing Systems(NIPS), pages 1041?1048.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word representa-tions in vector space.
In Proceedings of InternationalConference on Learning Representations.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed represen-tations of words and phrases and their composition-ality.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Andriy Mnih and Geoffrey E Hinton.
2009.
A scalablehierarchical distributed language model.
In NeuralInformation Processing Systems (NIPS), pages 1081?1088.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 shared task on parsing the web.
In Notesof the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), volume 59.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proceedings ofEmpirical Methods for Natural Language Processing(EMNLP), pages 133?142.Cicero D. Santos and Bianca Zadrozny.
2014.
Learningcharacter-level representations for part-of-speech tag-ging.
In Proceedings of the International Conferenceon Machine Learning (ICML), pages 1818?1826.Tobias Schnabel and Hinrich Sch?utze.
2014.
Flors: Fastand simple domain adaptation for part-of-speech tag-ging.
Transactions of the Association of Computa-tional Linguistics, 2:51?62.Noah A Smith.
2011.
Linguistic structure prediction.Synthesis Lectures on Human Language Technologies,4(2):1?274.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word Representation: A Simple and General Methodfor Semi-Supervised Learning.
In Proceedings ofthe Association for Computational Linguistics (ACL),pages 384?394, Uppsala, Sweden.Di Wang, Chenyan Xiong, and William Yang Wang.2013.
Automatic domain partitioning for multi-domain learning.
In Proceedings of Empirical Meth-ods for Natural Language Processing (EMNLP), pages869?873.Min Xiao and Yuhong Guo.
2013.
Domain adaptationfor sequence labeling tasks with a probabilistic lan-guage adaptation model.
In Proceedings of the In-ternational Conference on Machine Learning (ICML),pages 293?301.Yi Yang and Jacob Eisenstein.
2014.
Fast easy unsuper-vised domain adaptation with marginalized structureddropout.
In Proceedings of the Association for Com-putational Linguistics (ACL), Baltimore, MD.682
