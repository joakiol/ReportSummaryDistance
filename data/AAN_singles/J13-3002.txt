Relational Features in Fine-GrainedOpinion AnalysisRichard Johansson?University of GothenburgAlessandro Moschitti?
?University of TrentoFine-grained opinion analysis methods often make use of linguistic features but typically donot take the interaction between opinions into account.
This article describes a set of experimentsthat demonstrate that relational features, mainly derived from dependency-syntactic and se-mantic role structures, can significantly improve the performance of automatic systems for anumber of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinionholders, and determining the polarities of opinion expressions.
These features make it possibleto model the way opinions expressed in natural-language discourse interact in a sentence overarbitrary distances.
The use of relations requires us to consider multiple opinions simultaneously,which makes the search for the optimal analysis intractable.
However, a reranker can be used asa sufficiently accurate and efficient approximation.A number of feature sets and machine learning approaches for the rerankers are evaluated.For the task of opinion expression extraction, the best model shows a 10-point absolute improve-ment in soft recall on the MPQA corpus over a conventional sequence labeler based on localcontextual features, while precision decreases only slightly.
Significant improvements are alsoseen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall,respectively.
In addition, the systems outperform previously published results for unlabeled (6F-measure points) and polarity-labeled (10?15 points) opinion expression extraction.
Finally,as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practicalopinion mining tasks.
In all scenarios considered, the machine learning features derived from theopinion expressions lead to statistically significant improvements.1.
IntroductionAutomatic methods for the analysis of opinions (textual expressions of emotions, be-liefs, and evaluations) have attracted considerable attention in the natural language?
Spra?kbanken, Department of Swedish, University of Gothenburg, Box 100, SE-40530 Gothenburg,Sweden.
E-mail: richard.johansson@gu.se.
The work described here was mainly carried out at theUniversity of Trento.??
DISI ?
Department of Information Engineering and Computer Science, University of Trento,Via Sommarive 14, 38123 Trento (TN), Italy.
E-mail: moschitti@disi.unitn.it.Submission received: 11 January 2012; revised version received: 11 May 2012; accepted for publication:11 June 2012.doi:10.1162/COLI a 00141?
2013 Association for Computational LinguisticsComputational Linguistics Volume 39, Number 3processing community in recent years (Pang and Lee 2008).
Apart from their interestfrom a linguistic and psychological point of view, the technologies emerging from thisresearch have obvious practical uses, either as stand-alone applications or supportingother tools such as information retrieval or question answering systems.The research community initially focused on high-level tasks such as retrieving doc-uments or passages expressing opinion, or classifying the polarity of a given text, andthese coarse-grained problem formulations naturally led to the application of methodsderived from standard retrieval or text categorization techniques.
The models under-lying these approaches have used very simple feature representations such as purelylexical (Pang, Lee, and Vaithyanathan 2002; Yu and Hatzivassiloglou 2003) or low-levelgrammatical features such as part-of-speech tags and functional words (Wiebe, Bruce,and O?Hara 1999).
This is in line with the general consensus in the information retrievalcommunity that very little can be gained by complex linguistic processing for tasks suchas text categorization and search (Moschitti and Basili 2004).
There are a few exceptions,such as Karlgren et al(2010), who showed that construction features added to a bag-of-words representation resulted in improved performance on a number of coarse-grainedopinion analysis tasks.
Similarly, Greene and Resnik (2009) argued that a speaker?sattitude can be predicted from syntactic features such as the selection of a transitiveor intransitive verb frame.In contrast to the early work, recent years have seen a shift towards more detailedproblem formulations where the task is not only to find a piece of opinionated text,but also to extract a structured representation of the opinion.
For instance, we maydetermine the person holding the opinion (the holder) and towards which entity or factit is directed (the topic), whether it is positive or negative (the polarity), and the strengthof the opinion (the intensity).
The increasing complexity of representation leads usfrom retrieval and categorization deep into natural language processing territory; themethods used here have been inspired by information extraction and semantic rolelabeling, combinatorial optimization, and structured machine learning.
For such tasks,deeper representations of linguistic structure have seen more use than in the coarse-grained case.
Syntactic and shallow-semantic relations have repeatedly proven usefulfor subtasks of opinion analysis that are relational in nature, above all for determiningthe holder or topic of a given opinion, in which case there is considerable similarity totasks such as semantic role labeling (Kim and Hovy 2006; Ruppenhofer, Somasundaran,and Wiebe 2008).There has been no systematic research, however, on the role played by linguisticstructure in the relations between opinions expressed in text, despite the fact that theopinion expressions in a sentence are not independent but organized rhetorically toachieve a communicative effect intended by the speaker.
We therefore expect that theinterplay between opinion expressions can be exploited to derive information useful forthe analysis of opinions expressed in text.
In this article, we start from this intuition andpropose several novel features derived from the interdependencies between opinionexpressions on the syntactic and shallow-semantic levels.Based on these features, we devised structured prediction models for (1) extractionof opinion expressions, (2) joint expression extraction and holder extraction, and (3) jointexpression extraction and polarity labeling.
The models were trained using a numberof discriminative machine learning methods.
Because the interdependency featuresrequired us to consider more than one opinion expression at a time, the inference stepscarried out at training and prediction time could not rely on commonly used opinionexpression mark-up methods based on Viterbi search, but we show that an approximatesearch method using reranking suffices for this purpose: In a first step a base system474Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysisusing local features and efficient search generates a small set of hypotheses, and in asecond step a classifier using the complex features selects the final output from thehypothesis set.
This approach allows us to make use of arbitrary features extracted fromthe complete set of opinion expressions in a sentence, without having to impose anyrestriction on the expressivity of the features.
An additional advantage is that it is fairlyeasy to implement as long as the underlying system is able to generate k-best output.The interaction-based reranking systems were evaluated on a test set extractedfrom the MPQA corpus, and compared to strong baselines consisting of stand-alonesystems for opinion expression mark-up, opinion holder extraction, and polarity classi-fication.
Our evaluations showed that (1) the best opinion expression mark-up systemwe evaluated achieved a 10-point absolute improvement in soft recall, and a 5-point im-provement in F-measure, over the baseline sequence labeler.
Our system outper-formed previously described opinion expression mark-up tools by six points in overlapF-measure.
(2) The recall was boosted by almost 10 points for the holder extraction task(over three points in F-measure) by modeling the interaction of opinion expressionswith respect to holders.
(3) We saw an improvement for the extraction of polarity-labeled expression of four F-measure points.
Our result for opinion extraction and po-larity labeling is especially striking when compared with the best previously publishedend-to-end system for this task: 10?15 points in F-measure improvement.
In addition tothe performance evaluations, we studied the impact of features on the subtasks, and theeffect of the choice of the machine learning method for training the reranker.As a final extrinsic evaluation of the system, we evaluated the usefulness of itsoutput in a number of applications.
Although there have been several publicationsdetailing the extraction of MPQA-style opinion expressions, as far as we are aware therehas been no attempt to use them in an application.
In contrast, we show that the opinionexpressions as defined by the MPQA corpus may be used to derive machine learningfeatures that are useful in two practical opinion mining tasks; the addition of thesefeatures leads to statistically significant improvements in all scenarios we evaluated.First, we develop a system for the extraction of evaluations of product attributesfrom product reviews (Hu and Liu 2004a, 2004b; Popescu and Etzioni 2005; Titov andMcDonald 2008), and we show that the features derived from opinion expressions leadto significant improvement.
Secondly, we show that fine-grained opinion structuralinformation can even be used to build features that improve a coarse-grained sentimenttask: document polarity classification of reviews (Pang, Lee, and Vaithyanathan 2002;Pang and Lee 2004).After the present introduction, Section 2 gives a linguistic motivation and anoverview of the related work; Section 3 describes the MPQA opinion corpus and itsunderlying representation; Section 4 illustrates the baseline systems: a sequence labelerfor the extraction of opinion expressions and classifiers for opinion holder extractionand polarity labeling; Section 5 reports on the main contribution: the description of theinteraction models and their features; finally, Section 7 presents the experimental resultsand Section 8 derives the conclusions.2.
Motivation and Related WorkIntuitively, interdependency features could be useful in the process of locating anddisambiguating expressions of opinion.
These expressions tend to occur in patterns, andthe presence of one opinionated piece of text may influence our interpretation of anotheras opinionated or not.
Consider, for instance, the word said in sentences (a) and (b) inExample (1), where the presence or non-presence of emotionally loaded words in the475Computational Linguistics Volume 39, Number 3complement clause provides evidence for the interpretation as a subjective opinion oran objective speech event.
(In the example, opinionated expressions are marked S forsubjective and the non-opinionated speech event O for objective.
)Example (1)(a) ?We will identify the [culprits]S of these clashes and [punish]S them,?
he [said]S.(b) On Monday, 80 Libyan soldiers disembarked from an Antonov transport planecarrying military equipment, an African diplomat [said]O.Moreover, opinions expressed in a sentence are interdependent when it comes to theresolution of their holders?the person or entity having the attitude expressed in theopinion expression.
Clearly, the structure of the sentence is also influential for this taskbecause certain linguistic constructions provide evidence for opinion holder correlation.In the most obvious case, shown in the two sentences in Example (2), pejorative wordsshare the opinion holder with the communication and categorization verbs dominatingthem.
(Here, opinions are marked S and holders H.)Example (2)(a) [Domestic observers]H [tended to side with]S the MDC, [denouncing]S the electionas [fraud-tainted]S and [unfair]S.(b) [Bush]H [labeled]S North Korea, Iran and Iraq an ?
[axis of evil]S.?In addition, interaction is important when determining opinion polarity.
Here, relationsthat influence polarity interpretation include coordination, verb?complement, as well asother types of discourse relations.
In particular, the presence of a COMPARISON discourserelation, such as contrast or concession (Prasad et al2008), may allow us to infer thatopinion expressions have different polarities.
In Example (3), we see how contrastivediscourse connectives (underlined) are used when there are contrasting polarities in thesurrounding opinion expressions.
(Positive opinions are tagged ?+?, negative ?-?.
)Example (3)(a) ?
[This is no blind violence but rather targeted violence]-,?
Annemie Neyts [said]-.
?However, the movement [is more than that]+.?
(b) ?
[Trade alone will not save the world]-,?
Neyts [said]-, but it constitutes an[important]+ factor for economic development.The problems we focus on in this article?extracting opinion expressions with holdersand polarity labeling?have naturally been studied previously, especially since therelease of the MPQA corpus (Wiebe, Wilson, and Cardie 2005).
For the first subtask,because the MPQA corpus uses span-based annotation to represent opinions, it isnatural to apply straightforward sequence labeling techniques to extract them.
This ideahas resulted in a number of publications (Choi, Breck, and Cardie 2006; Breck, Choi, andCardie 2007).
Such systems do not use any features describing the interaction betweenopinions, and it would not be possible to add interaction features because a Viterbi-based sequence labeler by construction is restricted to using local features in a smallcontextual window.Works using syntactic features to extract topics and holders of opinions are numer-ous (Bethard et al2005; Kobayashi, Inui, and Matsumoto 2007; Joshi and Penstein-Rose?476Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis2009; Wu et al2009).
Semantic role analysis has also proven useful: Kim and Hovy(2006) used a FrameNet-based semantic role labeler to determine holder and topic ofopinions.
Similarly, Choi, Breck, and Cardie (2006) successfully used a PropBank-basedrole labeler for opinion holder extraction, and Wiegand and Klakow (2010) recently ap-plied tree kernel learning methods on a combination of syntactic and semantic role treesfor extracting holders, but did not consider their relations to the opinion expressions.Ruppenhofer, Somasundaran, and Wiebe (2008) argued that semantic role techniquesare useful but not completely sufficient for holder and topic identification, and thatother linguistic phenomena must be studied as well.
Choi, Breck, and Cardie (2006)built a joint model of opinion expression extraction and holder extraction and appliedinteger linear programming to carry out the optimization step.While the tasks of opinion expression detection and polarity classification of opin-ion expressions (Wilson, Wiebe, and Hoffmann 2009) have mostly been studied inisolation, Choi and Cardie (2010) developed a sequence labeler that simultaneouslyextracted opinion expressions and assigned them polarity values and this is so far theonly published result on joint opinion segmentation and polarity classification.
Theirexperiment, however, lacked the obvious baseline: a standard pipeline consisting of anexpression tagger followed by a polarity classifier.
In addition, although their modelis the first end-to-end system for opinion expression extraction and polarity classifica-tion, it is still based on sequence labeling and thus by construction limited in featureexpressivity.On a conceptual level, discourse-oriented approaches (Asher, Benamara, andMathieu 2009; Somasundaran et al2009; Zirn et al2011) applying interaction featuresfor polarity classification are arguably the most related because they are driven bya vision similar to ours: Individual opinion expressions interplay in discourse andthus provide information about each other.
On a practical level there are obviousdifferences, since our features are extracted from syntactic and shallow-semanticlinguistic representations, which we argue are reflections of discourse structure, whilethey extract features directly from a discourse representation.
It is doubtful whetherautomatic discourse representation extraction in text is currently mature enough toprovide informative features, whereas syntactic parsing and shallow-semantic analysisare today fairly reliable.
Another related line of work is represented by Choi and Cardie(2008), where interaction features based on compositional semantics were used in a jointmodel for the assignment of polarity values to pre-segmented opinion expressions in asentence.3.
The MPQA Corpus and its Annotation of Opinion ExpressionsThe most detailed linguistic resource useful for developing automatic systems for opin-ion analysis is theMPQA corpus (Wiebe,Wilson, and Cardie 2005).
In this article, we usethe word opinion in its broadest sense, equivalent to the word private state used by theMPQA annotators (page 2): ?opinions, emotions, sentiments, speculations, evaluations,and other private states (Quirk et al1985), i.e., internal states that cannot be directlyobserved by others.
?The central building block in the MPQA annotation is the opinion expression(or subjective expression): A text piece that expresses a private state, allowing us todraw the conclusion that someone has a particular emotion or belief about some topic.Identifying these units allows us to carry out further analysis, such as the determinationof opinion holder and the polarity of the opinion.
The annotation scheme defines twotypes of opinion expressions: direct subjective expressions (DSEs), which are explicit477Computational Linguistics Volume 39, Number 3mentions of attitude or evaluation, and expressive subjective elements (ESEs), whichsignal the attitude of the speaker by the choice of words.
The prototypical example ofa DSE would be a verb of statement, feeling, or categorization such as praise or disgust.ESEs, on the other hand, are less easy to categorize syntactically; prototypical exampleswould include simple value-expressing adjectives such as beautiful and strongly chargedwords like appeasement or big government.
In addition to DSEs and ESEs, the corpus alsocontains annotation for non-subjective statements, which are referred to as objectivespeech events (OSEs).
Some words such as say may appear as DSEs or OSEs dependingon the context, so for an automatic system there is a need for disambiguation.Example (4) shows a number of sentences from the MPQA corpus where DSEs andESEs have been manually annotated.Example (4)(a) He [made such charges]DSE [despite the fact]ESE that women?s political, social, andcultural participation is [not less than that]ESE of men.
(b) [However]ESE, it is becoming [rather fashionable]ESE to [exchange harsh words]DSEwith each other [like kids]ESE.
(c) For instance, he [denounced]DSE as a [human rights violation]ESE the banning andseizure of satellite dishes in Iran.
(d) This [is viewed]DSE as the [main impediment]ESE to the establishment of politicalorder in the country.Every expression in the corpus is connected to an opinion holder,1 an entity thatexperiences the sentiment or utters the evaluation that appears textually in the opinionexpression.
For DSEs, it is often fairly straightforward to find the opinion holdersbecause they tend to be realized as direct semantic arguments filling semantic rolessuch as SPEAKER or EXPERIENCER?and the DSEs tend to be verbs or nominalizations.For ESEs, the connection between the expression and the opinion holder is typicallyless clear-cut than for DSEs; the holder is more frequently implicit or located outsidethe sentence for ESEs than for DSEs.The MPQA corpus does not annotate links directly from opinion expressions toparticular mentions of a holder entity.
Instead, the opinions are connected to holdercoreference chains that may span the whole text.
Some opinion expressions are linkedto entities that are not explicitly mentioned in the text.
If this entity is the author of thetext, it is called the writer, otherwise implicit.
Example (5) shows sentences annotatedwith expressions and holders.Example (5)(a) For instance, [he]H1 [denounced]DSE/H1 as a [human rights violation]ESE/H1 thebanning and seizure of satellite dishes in Iran.
(b) [(writer)]H1: [He]H2 [made such charges]DSE/H2 [despite the fact]ESE/H1 thatwomen?s political, social, and cultural participation is [not less than that]ESE/H1 of men.
(c) [(implicit)]H1: This [is viewed]DSE/H1 as the [main impediment]ESE/H1 to the estab-lishment of political order in the country.1 The MPQA uses the term source but we prefer the term holder because it seems to be more common.478Johansson and Moschitti Relational Features in Fine-Grained Opinion AnalysisFinally, MPQA associates opinion expressions (DSEs and ESEs, but not OSEs) witha polarity feature taking the values POSITIVE, NEGATIVE, NEUTRAL, and BOTH, andwith an intensity feature taking the values LOW, MEDIUM, HIGH, and EXTREME.
Thetwo sentences in Example (6) from the MPQA corpus show opinion expressions withpolarities.
Positive polarity is represented with a ?+?
and negative with a ?-?.Example (6)(a) We foresaw electoral [fraud]- but not [daylight robbery]-.
(b) Join in this [wonderful]+ event and help Jameson Camp continue to provide theyear-round support that gives kids a [chance to create dreams]+.The corpus does not currently contain annotation of topics (evaluees) of opinions,although there have been efforts to add this separately (Stoyanov and Cardie 2008).4.
Baseline Systems for Fine-Grained Opinion AnalysisThe assessment of our reranking-based systems requires us to compare against strongbaselines.
We developed (1) a sequence labeler for opinion expression extraction similarto that by Breck, Choi, and Cardie (2007), (2) a set of classifiers to determine theopinion holder, and (3) a multiclass classifier that assigns polarity to a given opinionexpression similar to that described by Wilson, Wiebe, and Hoffmann (2009).
Thesetools were also used to generate the hypothesis sets for the rerankers described inSection 5.4.1 Sequence Labeler for Opinion Expression Mark-upTo extract opinion expressions, we implemented a standard sequence labeler for sub-jective expression mark-up similar to the approach by Breck, Choi, and Cardie (2007).The sequence labeler extracted basic grammatical and lexical features (word, lemma,and POS tag), as well as prior polarity and intensity features derived from the lexiconcreated by Wilson, Wiebe, and Hoffmann (2005), which we refer to as subjectivityclues.
It is important to note that prior subjectivity does not always imply subjectivityin a particular context; this is why contextual features are essential for this task.
Thegrammatical features and subjectivity clues were extracted in a window of size 3 aroundthe word in focus.
We encoded the opinionated expression brackets by means of theIOB2 scheme (Tjong Kim Sang and Veenstra 1999).
When using this representation, weare unable to handle overlapping opinion expressions, but they are fortunately rare.To exemplify, Figure 1 shows an example of a sentence and how it is processedby the sequence labeler.
The ESE defenseless situation is encoded in IOB2 as two tagsB-ESE and I-ESE.
There are four input columns (words, lemmas, POS tags, subjectivityclues) and one output column (opinion expression tags in IOB2 encoding).
The figurealso shows the sliding window from which the feature extraction function can extractfeatures when predicting an output tag (at the arrow).We trained the model using the method by Collins (2002), with a Viterbi decoderand the online Passive?Aggressive algorithm (Crammer et al2006) to estimate themodel weights.
The learning algorithm parameters were tuned on a development set.When searching for the best value of the C parameter, we varied it along a log scale from479Computational Linguistics Volume 39, Number 3HRWhasdenouncedthesituationoftheseprisonersHRWhavedenouncethedefenselesssituationofthisprisonerdefenselessNNPVBZVBNDTJJNNINDTNNS?????
?str/neg?weak/negOOB?ESEI?ESEB?DSEOFigure 1Sequence labeling example.0.001 to 100, and the best value was 0.1.
We used the max-loss version of the algorithmand ten iterations through the training set.4.2 Classifiers for Opinion Holder ExtractionThe problem of extracting opinion holders for a given opinion expression is in manyways similar to argument detection in semantic role labeling (Choi, Breck, and Cardie2006; Ruppenhofer, Somasundaran, and Wiebe 2008).
For instance, in the simplestcase when the opinion expression is a verb of evaluation or categorization, findingthe holder would entail finding a semantic argument such as an EXPERIENCER orCOMMUNICATOR.
We therefore approached this problem using methods inspired bysemantic role labeling: Given an opinion expression in a sentence, we define binaryclassifiers that decide whether each noun phrase of the sentence is its holder or not.Separate classifiers were trained to extract holders for DSEs, ESEs, and OSEs.Hereafter, we describe the feature set used by the classifiers.
Our walkthroughexample is given by the sentence in Figure 1.
Some features are derived from thesyntactic and shallow semantic analysis of the sentence, shown in Figure 2 (Section 6.1gives more details on this).SYNTACTIC PATH.
Similarly to the path feature widely used in semantic role labeling(SRL), we extract a feature representing the path in the dependency tree betweenthe expression and the holder (Johansson and Nugues 2008).
For instance, the pathfrom denounced to HRW in the example is VC?SBJ?.SHALLOW-SEMANTIC RELATION.
If there is a direct shallow-semantic relation betweenthe expression and the holder, we use a feature representing its semantic role, suchas A0 between denounced and HRW.EXPRESSION HEAD WORD, POS, AND LEMMA.
denounced, VBD, denounce for de-nounced; situation, NN, situation for defenseless situation.HEAD WORD AND POS OF HOLDER CANDIDATE.
HRW, NNP for HRW.DOMINATING EXPRESSION TYPE.
When locating the holder for the ESE defenselesssituation, we extract a feature representing the fact that it is syntactically dominatedby a DSE.
At test time, the expression and its type were extracted automatically.480Johansson and Moschitti Relational Features in Fine-Grained Opinion AnalysisCONTEXT WORDS AND POS FOR HOLDER CANDIDATE.
Words adjacent to the leftand right; for HRW we extract Right:has, Right:VBZ.EXPRESSION VERB VOICE.
Similar to the common voice feature used in SRL.
Takesthe values Active, Passive, and None (for non-verbal opinion expressions).
In theexample, we get Active for denounced and None for defenseless situation.EXPRESSION DEPENDENCY RELATION TO PARENT.
VC and OBJ, respectively.There are also differences compared with typical argument extraction in SRL, however.First, as outlined in Section 3, it is important to note that the MPQA corpus does notannotate direct links from opinions to holders, but from opinions to holder coreferencechains.
To handle this issue, we used the following approach when training the classi-fier: We created a positive training instance for each member of the coreference chainoccurring in the same sentence as the opinion, and negative training instances for allother noun phrases in the sentence.
We do not use coreference information at test time,in order for the system not to rely on automatic coreference resolution.A second complication is that in addition to the explicit noun phrases in the samesentence, an opinion may be linked to an implicit holder; a special case of implicit holderis the writer of the text.
To detect implicit and writer links, we trained two separateclassifiers that did not use the features requiring a holder phrase.
We did not try tolink opinion expressions to explicitly expressed holders outside the sentence; this isan interesting open problem with some connections to inter-sentential semantic rolelabeling, a problem whose study is in its infancy (Gerber and Chai 2010).We implemented the classifiers as linear support vector machines (SVMs; Boser,Guyon, and Vapnik 1992) using the LIBLINEAR software (Fan et al2008).
To handlethe restriction that every expression can have at most one holder, the classifier selectsonly the highest-scoring opinion holder candidate at test time.
We tuned the learningparameters on a development set, and the best results were obtained with an L2-regularized L2-loss SVM and a C value of 1.4.3 Polarity ClassifierGiven an expression, we use a classifier to assign a polarity value: POSITIVE, NEUTRAL,or NEGATIVE.
Following Choi and Cardie (2010), the polarity value BOTH was mappedto NEUTRAL?the expressions having this value were in any case very few.
In the caseswhere the polarity value was empty or missing, we set the polarity to NEUTRAL.
Inaddition, the annotators of the MPQA corpus may use special uncertainty labels in thecase where the annotator was unsure of which polarity to assign, such as UNCERTAIN-POSITIVE.
In these cases, we just removed the uncertainty label.We again trained SVMs to carry out this classification.
The problem of polarity clas-sification has been studied in detail by Wilson, Wiebe, and Hoffmann (2009), who useda set of carefully devised linguistic features.
Our classifier is simpler and is based onfairly shallow features.
Like the sequence labeler for opinion expressions, this classifiermade use of the lexicon of subjectivity clues.The feature set used by the polarity classifier consisted of the following features.The examples come from the opinion expression defenseless situation in Figure 1.WORDS IN EXPRESSION: defenseless, situation.POS TAGS IN EXPRESSION: JJ, NN481Computational Linguistics Volume 39, Number 3SUBJECTIVITY CLUES OF WORDS IN EXPRESSION: None.WORD BIGRAMS IN EXPRESSION: defenseless situation.WORDS BEFORE AND AFTER EXPRESSION: B:the, A:of.POS TAGS BEFORE AND AFTER EXPRESSION: B:DT, A:IN.To train the support vector classifiers, we again used LIBLINEAR with the same param-eters.
The three-class classification problem was binarized using the one-versus-allmethod.5.
Fine-Grained Opinion Analysis with Interaction FeaturesBecause there is a combinatorial number of ways to segment a sentence into opin-ion expressions, and label the opinion expressions with type labels (DSE, ESE, OSE)as well as polarities and opinion holders, the tractability of the opinion expressionsegmentation task will obviously depend on whether we impose restrictions on theproblem in a way that allows for efficient inference.
Most previous work (Choi, Breck,and Cardie 2006; Breck, Choi, and Cardie 2007; Choi and Cardie 2010) used Markovfactorizations and could thus apply standard sequence labeling techniques where theargmax step was carried out using the Viterbi algorithm (as described in Section 4.1).As we argued in the introduction, however, it makes linguistic sense that opinionexpression segmentation and other tasks could be improved if relations between possibleexpressions were considered; these relations can be syntactic or semantic in nature,for instance.We will show that adding relational features causes the Markov assumption tobreak down and the problem of finding the best analysis to become computationallyintractable.
We thus have to turn to approximate inference methods based on reranking,which can be trained efficiently.5.1 Formalization of the ModelWe formulate the problem of extracting the opinion structure (the set of opinion expres-sions, and possibly also their holders or polarities) from a given sentence as a structuredprediction problemy?
= argmaxyw ?
?
(x, y) (1)where w is a weight vector and ?
(x, y) a feature extraction function representing a sen-tence x and an opinion structure y as a high-dimensional feature vector.
We now furtherdecompose the feature representation ?
into a local part ?L and a nonlocal part ?NL:?
= ?L +?NL (2)where ?L is a standard first-order Markov factorization, and ?NL represents the non-local interactions between pairs of opinion expressions:?NL(x, y) =?ei,ej?y,ei 6=ej?p(ei, ej, x) (3)482Johansson and Moschitti Relational Features in Fine-Grained Opinion AnalysisThe feature function ?p represents a pair of opinion expressions ei and ej and theirinteraction in the sentence x, such as the syntactic and semantic relations connectingthem.5.2 Approximate Inference with Interaction FeaturesIt is easy to see that the inference step argmaxy w ?
?
(x, y) is NP-hard for unrestrictedpairwise interaction feature representations ?
: This class of models includes simplerones such as loopy Markov random fields, where inference is known to be NP-hard andrequire the use of approximate approaches such as belief propagation.
Although it ispossible that search algorithms for approximate inference in ourmodel could be devisedalong similar lines, we sidestepped this issue by using a reranking decomposition of theproblem:r Apply a simple model based on local context features ?L but no structuralinteraction features.
Generate a small hypothesis set of size k.r Apply a complex model using interaction features ?NL to pick the tophypothesis from the hypothesis set.The advantages of a reranking approach compared with more complex approaches re-quiring advanced search techniques are mainly simplicity and efficiency: This approachis conceptually simple and fairly easy to implement provided that k-best output canbe generated efficiently, which is certainly true for the Viterbi-based sequence labelerdescribed in Section 4.1.
The features can then be arbitrarily complex because we donot have to think about how the problem structure affects the algorithmic complexity ofthe inference step.
Reranking has been used in a wide range of applications, starting inspeech recognition (Schwartz and Austin 1991) and very commonly in syntactic parsingof natural language (Collins 2000).The hypothesis generation procedure becomes slightly more complex when polar-ity values and opinion holders of the opinion expressions enter the picture.
In that case,we not only need hypotheses generated by a sequence labeler, but also the outputsof a secondary classifier: the holder extractor (Section 4.2) or the polarity classifier(Section 4.3).
The hypothesis set generation thus proceeds as follows:r For a given sentence, let the base sequence labeler generate up to k1sequences of unlabeled opinion expressions;r for every sequence, apply the secondary classifier to generate up to k2outputs.The hypothesis set size is thus at most k1 ?
k2.To illustrate this process we give a hypothetical example, assuming k1 = k2 = 2and the sentence The appeasement emboldened the terrorists.
We first apply the opinionexpression extractor to generate a set of k1 possible segmentations of the sentence:The [appeasement] emboldened the [terrorists]The [appeasement] [emboldened] the [terrorists]483Computational Linguistics Volume 39, Number 3In the second step, we add polarity values, up to k2 labelings for every segmentationcandidate:The [appeasement]?
emboldened the [terrorists]?The [appeasement]?
[emboldened]+ the [terrorists]?The [appeasement]0 emboldened the [terrorists]?The [appeasement]?
[emboldened]0 the [terrorists]?5.3 Training the RerankersIn addition to the approximate inference method to carry out the maximization of Equa-tion (1), we still need a machine learning method to assign weights to the vector w byestimating on a training set.
We investigated a number of machine learning approachesto train the rerankers.5.3.1 Structured SVM Learning.
We first applied the method of large-margin estimationfor structured output spaces, also known as structured support vector machines.
Inthis method, we use quadratic optimization to find the smallest weight vector w thatsatisfies the constraint that the difference in output score between the correct output yand an incorrect output y?
should be at least ?
(y, y?
), where ?
is a loss function basedon the degree of error in the output y?
with respect to the gold standard y.
This is ageneralization of the well-known support vector machine from binary classification toprediction of structured objects.Formally, for a given training set T = {?xi, yi?}
where the output space for theinput xi is Yi, we state the learning problem as a constrained quadratic optimizationprogram:minimizew ?w?2subject to w ?
(?
(xi, yi)?
?
(xi, yij)) ?
?
(yi, yij),?
?xi, yi?
?
T , yij ?
Yi(4)Because real-world data tend to be noisy, this optimization problem is usually alsoregularized to reduce overfitting, which leads to the introduction of a parameter C as inregular support vector machines (see Taskar, Guestrin, and Koller [2004] inter alia fordetails).The optimization problem (4) is usually not solved directly because the number ofconstraints makes a direct solution of the optimization program intractable for mostrealistic types of problems.
Instead, an approximation has to be used in practice, andwe used the SVMstructsoftware package (Tsochantaridis et al2005; Joachims, Finley,and Yu 2009), which finds a solution to the quadratic program by successively findingits most violated constraints and adding them to a working set.
We used the defaultvalues for the learning parameters, except for the parameter C, which was determinedby optimizing on a development set.
This resulted in a C value of 500.We defined the loss function ?
as 1 minus the intersection F-measure defined inSection 7.1.
When training rerankers for the complex extraction tasks (expressions +holders or expressions + polarities), we used a weighted combination of F-measuresfor the primary task (expressions) and the secondary task (holders or polarities, seeSections 7.1.1 and 7.1.2, respectively).484Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis5.3.2 On-line Learning.
In addition to the structured SVM learning method, we trainedmodels using two variants of on-line learning.
Such learning methods are a feasiblealternative for performance reasons.
We investigated two on-line learning algorithms:the popular structured perceptron (Collins 2002) and the Passive?Aggressive (PA)algorithm (Crammer et al2006).
To increase robustness, we used an averaged imple-mentation (Freund and Schapire 1999) of both algorithms.The difference between the two algorithms is the way the weight vector is incre-mented in each step.
In the perceptron, for a given input x, we compute an update tothe current weight vector by computing the difference between the correct output y andthe predicted output y?.
Pseudocode is given in Algorithm 1.Algorithm 1 The structured perceptron algorithm.function PERCEPTRON(T ,N)input Training set T = {(xi, yi)}Ti=1Number of iterations Nw0 ?
(0, .
.
.
, 0)repeat N timesfor (x, y) in Ty??
argmaxh w ?
?
(x, h)wi+1 ?
wi +?
(x, y)?
?
(x, y?)i?
i+ 1return 1NT?NTi=1 wiThe PA algorithm, with pseudocode in Algorithm 2, is based on the theory of large-margin learning similar to the structured SVM.
Here we instead base the update stepon the y?
that violates the margin constraints maximally, also taking the loss function ?into account.
The update step length ?
is computed based on the margin; this updateis bounded by a regularization constant C, which we set to 0.005 after tuning on adevelopment set.
The number N of iterations through the training set was 8 for bothon-line algorithms.Algorithm 2 The on-line passive?aggressive algorithm.function PASSIVE?AGGRESSIVE(T ,N,C)input Training set T = {(xi, yi)}Ti=1Number of iterations NRegularization parameter Cw0 ?
(0, .
.
.
, 0)repeat N timesfor (x, y) in Ty??
argmaxh w ?
(?
(x, h)?
?
(x, y))+??
(y, h)??
min(C, w?(?(x,y?)??(x,y))+??(y,y?)??(x,y?)??
(x,y)?2)wi+1 ?
wi + ?(?
(x, y)?
?
(x, y?))i?
i+ 1return 1NT?NTi=1 wi485Computational Linguistics Volume 39, Number 36.
Overview of the Interaction FeaturesThe feature extraction function ?NL extracts three groups of interaction features: (1)features considering the opinion expressions only; (2) features considering opinionholders; and (3) features considering polarity values.In addition to the interaction features ?NL, the rerankers used features representingthe scores output by the base models (opinion expression sequence labeler and sec-ondary classifiers); they did not directly use the local features ?L.
We normalized thescores over the k candidates so that their exponentials summed to 1.6.1 Syntactic and Shallow Semantic AnalysisThe features used by the rerankers, as well as the opinion holder extractor in Section 4.2,are to a large extent derived from syntactic and semantic role structures.
To extractthem, we used the syntactic?semantic parser by Johansson and Nugues (2008), whichannotates the sentences with dependency syntax (Mel?c?uk 1988) and shallow semanticsin the PropBank (Palmer, Gildea, and Kingsbury 2005) and NomBank (Meyers et al2004) frameworks, using the format of the CoNLL-2008 Shared Task (Surdeanu et al2008).
The system includes a sense disambiguator that assigns PropBank or NomBanksenses to the predicate words.Figure 2 shows an example of the structure of the annotation: The sentence HRWdenounced the defenseless situation of these prisoners, where denounced is a DSE and de-fenseless situation is an ESE, has been annotated with dependency syntax (above thetext) and semantic role structure (below the text).
The predicate denounced, which is aninstance of the PropBank frame denounce.01, has two semantic arguments: the Speaker(A0, or Agent in VerbNet terminology) and the Subject (A1, or Theme), which arerealized on the surface-syntactic level as a subject and a direct object, respectively.
Sim-ilarly, situation has the NomBank frame situation.01 and an EXPERIENCER semanticargument (A0).6.2 Opinion Expression Interaction FeaturesThe rerankers use two types of structural features: syntactic features extracted fromthe dependency tree, and semantic features extracted from the predicate?argument(semantic role) graph.The syntactic features are based on paths through the dependency tree.
This leads toa minor complication for multiword opinion expressions; we select the shortest possiblepath in such cases.
For instance, in the sentence in Figure 2, the path will be computedbetween denounced and situation.HRW thehas [denounced] defenseless[ situation]ESEdenounce.01A0NMODNMODOBJSBJ VCDSE of these prisonersNMOD NMODPMODsituation.01A0A1Figure 2Example sentence and its syntactic and shallow-semantic analysis.486Johansson and Moschitti Relational Features in Fine-Grained Opinion AnalysisWe used the following syntactic interaction features.
All examples refer to Figure 2.SYNTACTIC PATH.
Given a pair of opinion expressions, we use a feature representingthe labels of the two expressions and the path between them through the syntactictree, following standard practice from dependency-based semantic role labeling(Johansson and Nugues 2008).
For instance, for the DSE denounced and the ESEdefenseless situation in Figure 2, we represent the syntactic configuration using thefeature DSE:OBJ?
:ESE, meaning that the syntactic relation between the DSE and theESE consists of a single link representing a grammatical object.LEXICALIZED PATH.
Same as above, but with lexical information attached: DSE/denounced:OBJ?:ESE/situation.DOMINANCE.
In addition to the features based on syntactic paths, we created amore generic feature template describing dominance relations between expres-sions.
For instance, from the graph in Figure 2, we extract the feature DSE/denounced?ESE/situation, meaning that a DSE with the word denounced domi-nates an ESE with the word situation.The features based on semantic roles were the following:PREDICATE SENSE LABEL.
For every predicate found inside an opinion expression, weadd a feature consisting of the expression label and the predicate sense identifier.For instance, the verb denounced, which is also a DSE, is representedwith the featureDSE/denounce.01.PREDICATE AND ARGUMENT LABEL.
For every argument of a predicate inside anopinion expression, we also create a feature representing the predicate?argumentpair: DSE/denounced.01:A0.CONNECTING ARGUMENT LABEL.
When a predicate inside some opinion expressionis connected to some argument inside another opinion expression, we use a featureconsisting of the two expression labels and the argument label.
For instance, theESE defenseless situation is connected to the DSE denounced via an A1 label, and werepresent this using a feature DSE:A1:ESE.6.3 Opinion Holder Interaction FeaturesIn addition, we modeled the interaction between different opinions with respect to theirholders.
We used the following two features to represent this interaction:SHARED HOLDERS.
A feature representing whether or not two opinion expressionshave the same holder.
For instance, if a DSE dominates an ESE and they have thesame holder as in Figure 2, where the holder is HRW, we represent this by thefeature DSE:ESE:true.HOLDER TYPES + PATH.
A feature representing the types of the holders, combined withthe syntactic path between the expressions.
The types take the following possiblevalues: explicit, implicit, writer.
In Figure 2, we would thus extract the featureDSE/Expl:OBJ?
:ESE/Expl.487Computational Linguistics Volume 39, Number 36.4 Polarity Interaction FeaturesThe model used the following features that take the polarities of the expressions intoaccount.
These features are extracted from DSEs and ESEs only, because the OSEs haveno polarity values.
The examples of extracted features are given with respect to the twoopinion expressions (denounced and defenseless situation) in Figure 2, both of which havea negative polarity value.POLARITY PAIR.
For every pair of opinion expressions in the sentence, we create afeature consisting of the pair of polarity values, such as NEGATIVE:NEGATIVE.POLARITY PAIR AND SYNTACTIC PATH.
NEGATIVE:OBJ?
:NEGATIVE.POLARITY PAIR AND SYNTACTIC DOMINANCE.
NEGATIVE?NEGATIVE.POLARITY PAIR AND WORD PAIR.
NEGATIVE-denounced:NEGATIVE-situation.POLARITY PAIR AND EXPRESSION TYPES.
Adds the expression types (ESE or DSE) tothe polarity pair: DSE-NEGATIVE:ESE-NEGATIVE.POLARITY PAIR AND TYPES AND SYNTACTIC PATH.
Adds syntactic information to thetype and polarity combination: DSE-NEGATIVE:OBJ?
:ESE-NEGATIVE.POLARITY PAIR AND SHALLOW-SEMANTIC RELATION.
When two opinion expres-sions are directly connected through a link in the shallow-semantic structure, wecreate a feature based on the semantic role label of the connecting link: NEGA-TIVE:A1:NEGATIVE.POLARITY PAIR AND WORDS ALONG SYNTACTIC PATH.
We follow the syntactic pathbetween the two expressions and create a feature for every word we pass onthe way.
In the example, no such feature is extracted because the expressions aredirectly connected.7.
ExperimentsWe trained and evaluated the rerankers on version 2.0 of the MPQA corpus,2 whichcontains 692 documents.
We discarded one document whose annotation was garbledand we split the remaining 691 into a training set (541 documents) and a test set (150documents).
We also set aside a development set of 90 documents from the training setthat we used when developing features and tuning learning algorithm parameters; allexperiments described in this article, however, usedmodels that were trained on the fulltraining set.
Table 1 shows some statistics about the training and test sets: the numberof documents and sentences; the number of DSEs, ESEs, and OSEs; and the number ofexpressions marked with the various polarity labels.We considered three experimental settings: (1) opinion expression extraction;(2) joint opinion expression and holder extraction; and (3) joint opinion expression andpolarity classification.
Finally, the polarity-based opinion extraction system was used inan extrinsic evaluation: document polarity classification of movie reviews.To generate the training data for the rerankers, we carried out a 5-fold hold-outprocedure: We split the training set into five pieces, trained a sequence labeler andsecondary classifiers on pieces 1?4, applied them to piece 5, and so on.2 http://www.cs.pitt.edu/mpqa/databaserelease/.488Johansson and Moschitti Relational Features in Fine-Grained Opinion AnalysisTable 1Statistics for the training and test splits of the MPQA collection.Training TestDocuments 541 150Sentences 12,010 3,743DSE 8,389 2,442ESE 10,279 3,370OSE 3,048 704POSITIVE 3,192 1,049NEGATIVE 6,093 1,675NEUTRAL 9,105 3,007BOTH 278 817.1 Evaluation MetricsBecause expression boundaries are hard to define rigorously (Wiebe,Wilson, and Cardie2005), our evaluations mainly used intersection-based precision and recallmeasures toscore the quality of the system output.
The idea is to assign values between 0 and 1, asopposed to traditional precision and recall where a span is counted as either correctlyor incorrectly detected.
We thus define the span coverage c of a span s (a set of tokenindices) with respect to another span s?, which measures how well s?
is covered by s:c(s, s?)
= |s ?
s?||s?|In this formula, |s|means the length of the span s, and the intersection ?
gives the set oftoken indices that two spans have in common.
Because our evaluation takes span labels(DSE, ESE, OSE) into account, we set c(s, s?)
to zero if the labels associated with s and s?are different.Using the span coverage, we define the span set coverage C of a set of spans S withrespect to a set S?
:C(S,S? )
=?sj?S?s?k?S?c(sj, s?k)We now define the intersection-based precision P and recall R of a proposed set of spansS?
with respect to a gold standard set S as follows:P(S, S?)
= C(S, S?
)|S?| R(S, S?)
=C(S?,S)|S|Note that in this formula, |S|means the number of spans in a set S.Conventionally, when measuring the quality of a system for an information extrac-tion task, a predicted entity is counted as correct if it exactly matches the boundaries ofa corresponding entity in the gold standard; there is thus no reward for close matches.Because the boundaries of the spans annotated in the MPQA corpus are not strictly489Computational Linguistics Volume 39, Number 3defined in the annotation guidelines (Wiebe, Wilson, and Cardie 2005), however, mea-suring precision and recall using exact boundary scoring will result in figures that aretoo low to be indicative of the usefulness of the system.
Therefore, most work usingthis corpus instead use overlap-based precision and recall measures, where a spanis counted as correctly detected if it overlaps with a span in the gold standard (Choi,Breck, and Cardie 2006; Breck, Choi, and Cardie 2007).
As pointed out by Breck, Choi,and Cardie (2007), this is problematic because it will tend to reward long spans?forinstance, a span covering the whole sentence will always be counted as correct if thegold standard contains any span for that sentence.
Conversely, the overlap metric doesnot give higher credit to a span that is perfectly detected than to one that has a very lowoverlap with the gold standard.The precision and recall measures proposed here correct the problem with overlap-based measures: If the system proposes a span covering the whole sentence, the spancoverage will be low and result in a low soft precision, and a low soft recall will beassigned if only a small part of a gold standard span is covered.
Note that our measuresare bounded below by the exact measures and above by the overlap-based measures.7.1.1 Opinion Holders.
To score the extraction of opinion holders, we started from thesame basic idea: Assign a score based on intersection.
The evaluation of this taskis more complex, however, because (1) we only want to give credit for holders forcorrectly extracted opinion expressions; (2) the gold standard links opinion expressionsto coreference chains rather than individual mentions of holders; and (3) the holderentity may be the writer or implicit (see Section 4.2).We therefore used the following method: If the system has proposed an opinionexpression e and its holder h, we first located the expression e?
in the gold standard thatmost closely corresponds to e, that is e?
= argmaxx c(x, e), regardless of the span labelsof e and e?.
To assign a score to the proposed holder entity, we then selected the mostclosely corresponding gold standard holder entity h?
in the coreference chain H?
linkedto e?
: h?
= argmaxx?H?
c(x, h).
Finally, we computed the precision and recall scores usingc(h?, h) and c(h, h?).
We stress again that the gold standard coreference chains were usedfor evaluation purposes only, and that our system did not make use of them at test time.If the system guesses that the holder of some opinion is the writer entity, we scoreit as perfectly detected (coverage 1) if the coreference chain H annotated in the goldstandard contains the writer, and a full error (coverage 0) otherwise, and similar if h isimplicit.7.1.2 Polarity.
In our experiments involving opinion expressions with polarities, wereport precision and recall values for polarity-labeled opinion expression segmentation:In order to be assigned an intersection score above zero, a segment must be labeled withthe correct polarity.
In the gold standard, all polarity labels were changed as describedin Section 4.3.
In these evaluations, OSEs were ignored and DSEs and ESEs were notdistinguished.7.2 Experiments in Opinion Expression ExtractionThe first task we considered was the extraction of opinion expression (labeled withexpression types).
We first studied the impact of the machine learning method andhypothesis set size on the reranker performance.
Then, we carried out an analysis of theeffectiveness of the features used by the reranker.
We finally compared the performanceof the expression extraction system with previous work (Breck, Choi, and Cardie 2007).490Johansson and Moschitti Relational Features in Fine-Grained Opinion AnalysisTable 2Evaluation of reranking learning methods.Learning method P R FBaseline 63.4 ?
1.5 46.8 ?
1.2 53.8 ?
1.1Structured SVM 61.8 ?
1.5 52.5 ?
1.3 56.8 ?
1.1Perceptron 62.8 ?
1.5 48.1 ?
1.3 54.5 ?
1.2Passive?Aggressive 63.5 ?
1.5 51.8 ?
1.3 57.0 ?
1.1Table 3Oracle and reranker performance as a function of candidate set size.Reranked Oraclek P R F P R F1 63.36 46.77 53.82 63.36 46.77 53.822 63.70 48.17 54.86 72.66 55.18 62.724 63.57 49.78 55.84 79.12 62.24 69.688 63.50 51.79 57.05 83.72 68.14 75.1316 63.00 52.94 57.54 86.92 72.79 79.2332 62.15 54.50 58.07 89.18 76.76 82.5164 61.03 55.67 58.23 91.09 80.19 85.29128 60.22 56.45 58.27 92.63 83.00 87.55256 59.87 57.22 58.51 94.01 85.27 89.437.2.1 Evaluation of Machine Learning Methods.
We compared the machine learning meth-ods described in Section 5.
In these experiments, we used a hypothesis set size k of 8.
Allfeatures from Section 6.2 were used.
Table 2 shows the results of the evaluations usingthe precision and recall measures described earlier.3 The baseline is the result of takingthe top-scoring labeling from the base sequence labeler.We note that the margin-based methods?structured SVM and the on-line PAalgorithm?outperform the perceptron soundly, which shows the benefit of learningmethods that make use of the cost function ?.
Comparing the two best-performinglearning methods, we note that the reranker using the structured SVM is more recall-oriented whereas the PA-based reranker more precision-oriented; the difference inF-measure is not statistically significant.
In the remainder of this article, all rerankers aretrained using the PA learning algorithm (with the same parameters) because its trainingprocess is much faster than that of the structured SVM.7.2.2 Candidate Set Size.
In any method based on reranking, it is important to study theinfluence of the hypothesis set size on the quality of the reranked output.
In addition,an interesting question is what the upper bound on reranker performance is?theoracle performance.
Table 3 shows the result of an experiment that investigates thesequestions.3 All confidence intervals in this article are at the 95% level and were estimated using a resampling method(Hjorth 1993).
The significance tests for differences were carried out using permutation tests.491Computational Linguistics Volume 39, Number 3Table 4Investigation of the contribution of syntactic features.Feature set P R FBaseline 63.4 ?
1.5 46.8 ?
1.2 53.8 ?
1.1All syntactic features 62.5 ?
1.4 53.2 ?
1.2 57.5 ?
1.1Removed SYNTACTIC PATH 64.4 ?
1.5 48.7 ?
1.2 55.5 ?
1.1Removed LEXICAL PATH 62.6 ?
1.4 53.2 ?
1.2 57.5 ?
1.1Removed DOMINANCE 62.3 ?
1.5 52.9 ?
1.2 57.2 ?
1.1As is common in reranking tasks, the reranker can exploit only a fraction of thepotential improvement?the reduction of the F-measure error ranges between 10% and15% of the oracle error reduction for all hypothesis set sizes.The most visible effect of the reranker is that the recall is greatly improved.
Thisdoes not seem to have an adverse effect on the precision, however, until the candidateset size goes above eight?in fact, the precision actually improves over the baseline forsmall candidate set sizes.
After the size goes above eight, the recall (and the F-measure)still rises, but at the cost of decreased precision.
In the remainder of this article, we useda k value of 64, which we thought gave a good balance between processing time andperformance.7.2.3 Feature Analysis.
We studied the impact of syntactic and semantic structural fea-tures on the performance of the reranker.
Table 4 shows the result of an investigationof the contribution of the syntactic features.
Using all the syntactic features (and nosemantic features) gives an F-measure roughly four points above the baseline.
We thencarried out an ablation test and measured the F-measure obtained when each one ofthe three syntactic features has been removed.
It is clear that the unlexicalized syntacticpath is the most important syntactic feature; this feature causes a two-point drop inF-measure when removed, which is clearly statistically significant (p < 0.0001).
Theeffect of the two lexicalized features is smaller, with only DOMINANCE causing a sig-nificant (p < 0.05) drop when removed.A similar result was obtained when studying the semantic features (Table 5).
Re-moving the connecting label feature, which is unlexicalized, has a greater effect thanremoving the other two semantic features, which are lexicalized.
Only the connectinglabel causes a statistically significant drop when removed (p < 0.0001).Because our most effective structural features combine a pair of opinion expressionlabels with a tree fragment, it is interesting to study whether the expression labels alonewould be enough.
If this were the case, we could conclude that the improvement isTable 5Investigation of the contribution of semantic features.Feature set P R FBaseline 63.4 ?
1.5 46.8 ?
1.2 53.8 ?
1.1All semantic features 61.3 ?
1.4 53.8 ?
1.3 57.3 ?
1.1Removed PREDICATE SENSE LABEL 61.3 ?
1.4 53.8 ?
1.3 57.3 ?
1.1Removed PREDICATE+ARGUMENT LABEL 61.0 ?
1.4 53.6 ?
1.3 57.0 ?
1.1Removed CONNECTING ARGUMENT LABEL 60.7 ?
1.4 50.5 ?
1.2 55.1 ?
1.1492Johansson and Moschitti Relational Features in Fine-Grained Opinion AnalysisTable 6Structural features compared to label pairs.Feature set P R FBaseline 63.4 ?
1.5 46.8 ?
1.2 53.8 ?
1.1Label pairs 62.0 ?
1.5 52.7 ?
1.2 57.0 ?
1.1All syntactic features 62.5 ?
1.4 53.2 ?
1.2 57.5 ?
1.1All semantic features 61.3 ?
1.4 53.8 ?
1.3 57.3 ?
1.1Syntactic + semantic 61.0 ?
1.4 55.7 ?
1.2 58.2 ?
1.1Syntactic + semantic + label pairs 61.6 ?
1.4 54.8 ?
1.3 58.0 ?
1.1caused not by the structural features, but just by learning which combinations of labelsare common in the training set, such as that DSE+ESE would be more common thanOSE+ESE.
We thus carried out an experiment comparing a reranker using label pairfeatures against rerankers based on syntactic features only, semantic features only, andthe full feature set.
Table 6 shows the results.
We see that the reranker using label pairsindeed achieves a performance well above the baseline.
Its performance is below that ofany reranker using structural features, however.
In addition, we see no improvementwhen adding label pair features to the structural feature set; this is to be expectedbecause the label pair information is subsumed by the structural features.7.2.4 Analysis of the Performance Depending on Expression Type.
In order to better under-stand the performance details of the expression extraction, we analyzed how well itextracted the three different classes of expressions.
Table 7 shows the results of thisevaluation.
The DSE row in the table thus shows the results of the performance on DSEs,without taking ESEs or OSEs into account.Apart from evaluations of the three different types of expressions, we evaluatedthe performance for a number of combined classes that we think may be interestingfor applications: DSE & ESE, finding all opinionated expressions and ignoring objectivespeech events; DSE & OSE, finding opinionated and non-opinionated speech and cat-egorization events and ignoring expressive elements; and unlabeled evaluation of alltypes of MPQA expressions.
The same extraction system was used in all experiments,and it was not retrained to maximize the different measures of performance.Again, the strongest overall tendency is that the reranker boosts the recall.
Goinginto the details, we see that the reranker gives very large improvements for DSEs andOSEs, but a smaller improvement for the combined DSE & OSE class.
This shows thatTable 7Performance depending on the type of expression.Baseline RerankedP R F P R FDSE 68.5 ?
2.1 57.8 ?
2.0 62.7 ?
1.7 67.0 ?
2.0 64.9 ?
1.9 66.0 ?
1.6ESE 63.0 ?
2.1 36.9 ?
1.5 46.5 ?
1.4 58.2 ?
1.9 46.2 ?
1.6 51.5 ?
1.3OSE 53.5 ?
3.5 62.3 ?
3.5 57.5 ?
3.0 57.0 ?
3.3 73.9 ?
3.2 64.3 ?
2.7DSE & ESE 71.1 ?
1.6 48.1 ?
1.2 57.4 ?
1.1 68.0 ?
1.5 57.6 ?
1.3 62.4 ?
1.0DSE & OSE 76.6 ?
1.8 70.3 ?
1.6 73.3 ?
1.3 72.6 ?
1.8 75.8 ?
1.5 74.2 ?
1.2Unlabeled 75.6 ?
1.5 55.3 ?
1.2 63.9 ?
1.0 71.0 ?
1.4 63.7 ?
1.2 67.2 ?
1.0493Computational Linguistics Volume 39, Number 3one of the most clear benefits of the complex features is to help disambiguate theseexpressions.
This also affects the performance for general opinionated expressions (DSE& ESE).7.2.5 Comparison with Breck, Choi, and Cardie (2007).
Comparison of systems in opinionexpression detection is often nontrivial because evaluation settings have differedwidely.
Since our problem setting?marking up and labeling opinion expressions in theMPQA corpus?is most similar to that described by Breck, Choi, and Cardie (2007), wecarried out an evaluation using the setting from their experiment.For compatibility with their experimental set-up, this experiment differed from theones described in the previous sections in the following ways:r The results were measured using the overlap-based precision and recall,although this is problematic as pointed out in Section 7.1.r The system did not need to distinguish DSEs and ESEs and did not have todetect the OSEs.r Instead of the training/test split used in the previous evaluations, thesystems were evaluated using a 10-fold cross-validation over the same setof 400 documents and the same cross-validation split as used in Breck,Choi, and Cardie?s experiment.
Each of the 10 rerankers was evaluated onone fold and trained on data generated in a cross-validation over theremaining nine folds.Again, our reranker uses the PA learning method with the full feature set (Section 6.2)and a hypothesis set size k of 64.
Table 8 shows the performance of our baseline (Section4.1) and reranked system, along with the best results reported by Breck, Choi, andCardie (2007).We see that the performance of our system is clearly higher?in both precision andrecall?than all results reported by Breck, Choi, and Cardie (2007).
Note that our systemwas optimized for the intersection metric rather than the overlap metric and that wedid not retrain it for this evaluation.7.3 Opinion Holder ExtractionTable 9 shows the performance of our holder extraction systems, evaluated usingthe scoring method described in Section 7.1.1.
We compared the performance of thereranker using opinion holder interaction features (Section 6.3) to two baselines: Thefirst of them consisted of the opinion expression sequence labeler (ES, Section 4.1) andthe holder extraction classifier (HC, Section 4.2), without modeling any interactionsbetween opinions.
The second and more challenging baseline was implemented byTable 8Results using the evaluation setting from Breck, Choi, and Cardie (2007).System P R FBreck, Choi, and Cardie (2007) 71.64 74.70 73.05Baseline 86.1 ?
1.0 66.7 ?
0.8 75.1 ?
0.7Reranked 83.4 ?
1.0 75.0 ?
0.8 79.0 ?
0.6494Johansson and Moschitti Relational Features in Fine-Grained Opinion AnalysisTable 9Opinion holder extraction results.System P R FES+HC 57.7 ?
1.7 45.3 ?
1.3 50.8 ?
1.3ES+ER+HC 53.3 ?
1.5 52.0 ?
1.4 52.6 ?
1.3ES+HC+EHR 53.2 ?
1.6 55.1 ?
1.5 54.2 ?
1.4adding the opinion expression reranker (ER) without holder interaction features tothe pipeline.
This results in a large performance boost simply as a consequence ofimproved expression detection, because a correct expression is required to get creditfor a holder.
However, both baselines are outperformed by the reranker using holderinteraction features, which we refer to as the expression/holder reranker (EHR); thedifferences to the strong baseline in recall and F-measure are both statistically significant(p < 0.0001).We carried out an ablation test to gauge the impact of the two holder interactionfeatures; we see in Table 10 that both of them contribute to improving the recall, and theeffect on the precision is negligible.
The statistical significance for the recall improve-ment is highest for SHARED HOLDERS (p < 0.0001) and lower for HOLDER TYPES +PATH (p < 0.02).We omit a comparison with previous work in holder extraction because our formu-lation of the opinion holder extraction problem is different from those used in previouspublications.
Choi, Breck, and Cardie (2006) used the holders of a simplified set ofopinion expressions, whereas Wiegand and Klakow (2010) extracted every entity taggedas ?source?
inMPQA regardless of whether it was connected to any opinion expression.Neither of them extracted implicit or writer holders.Table 11 shows a detailed breakdown of the holder extraction results based onopinion expression type (DSE, OSE, and ESE), and whether the holder is internally orexternally located; that is, whether or not the holder is textually realized in the samesentence as the opinion expression.
In addition, Table 12 shows the performance for thetwo types of externally located holders.As we noted in previous evaluations, the most obvious change between the baselinesystem and the reranker is that the recall and F-measure are improved; this is the casein every single evaluation.
As previously, a large share of the improvement is explainedsimply by improved expression detection, which can be seen by comparing the rerankedsystem to the strong baseline (ES+ER+HC).
For the most important situations, however,we see improvement when using the reranker with holder interaction features.
Inthose cases it outperforms the strong baseline significantly: DSE internal: p < 0.001,ESE internal p < 0.001, ESE external p < 0.05 (Table 11), writer p < 0.05 (Table 12).Table 10Opinion holder reranker feature ablation test.Feature set P R FBoth features 53.2 ?
1.6 55.1 ?
1.5 54.2 ?
1.4Removed HOLDER TYPES + PATH 53.1 ?
1.6 54.6 ?
1.5 53.8 ?
1.3Removed SHARED HOLDERS 53.1 ?
1.5 53.6 ?
1.5 53.3 ?
1.3495Computational Linguistics Volume 39, Number 3Table 11Detailed opinion holder extraction results.DSE Internal ExternalP R F P R FES+HC 57.4 ?
2.4 48.9 ?
2.2 52.8 ?
1.9 32.3 ?
6.8 25.8 ?
5.8 28.7 ?
5.8ES+ER+HC 56.7 ?
2.2 54.2 ?
2.2 55.5 ?
1.9 33.3 ?
5.9 34.2 ?
6.1 33.7 ?
5.5ES+HC+EHR 55.6 ?
2.2 58.8 ?
2.3 57.2 ?
1.9 35.2 ?
6.2 32.1 ?
6.0 33.6 ?
5.6OSE Internal ExternalP R F P R FES+HC 46.2 ?
3.6 57.2 ?
3.9 51.1 ?
3.3 39.7 ?
12.0 35.2 ?
11.2 37.3 ?
10.5ES+ER+HC 48.6 ?
3.4 66.8 ?
3.7 56.2 ?
3.1 36.8 ?
11.0 39.4 ?
11.4 38.1 ?
10.2ES+HC+EHR 50.4 ?
3.6 65.9 ?
3.9 57.1 ?
3.2 35.9 ?
10.9 39.4 ?
11.4 37.6 ?
10.1ESE Internal ExternalP R F P R FES+HC 50.5 ?
4.7 19.2 ?
2.1 27.8 ?
2.7 45.1 ?
3.0 41.2 ?
2.5 43.0 ?
2.4ES+ER+HC 48.3 ?
3.9 29.3 ?
2.8 36.4 ?
2.9 40.7 ?
2.6 48.4 ?
2.7 44.2 ?
2.3ES+HC+EHR 40.4 ?
3.4 36.5 ?
3.2 39.8 ?
3.0 43.2 ?
2.8 47.7 ?
2.9 45.3 ?
2.4The only common case where the improvement is not statistically significant is OSEinternal.The improvements are most notable for internally located holders, and especiallyfor the ESEs.
Extracting the opinion holder for ESEs is often complex because the ex-pression and the holder are typically not directly connected on the syntactic or shallow-semantic level, as opposed to the typical situation for DSEs.
When we use the reranker,however, the interaction features may help us make use of the holders of other opinionexpressions in the same sentence; for instance, the interaction features make it easierto distinguish cases like ?the film was [awful]ESE?
with an external (writer) holder fromcases such as ?I [thought]DSE the film was [awful]ESE?
with an internal holder directlyconnected to a DSE.Table 12Opinion holder extraction results for external holders.Writer P R FES+HC 44.8?3.0 42.8?2.6 43.8?2.4ES+ER+HC 40.6?2.6 50.3?2.7 44.9?2.3ES+HC+EHR 42.7?2.8 49.7?2.9 45.9?2.4Implicit P R FES+HC 41.2?6.4 28.3?4.8 33.6?4.9ES+ER+HC 38.7?5.4 34.4?5.1 36.4?4.7ES+HC+EHR 43.1?5.9 32.9?5.0 37.4?4.8496Johansson and Moschitti Relational Features in Fine-Grained Opinion AnalysisTable 13Overall evaluation of polarity-labeled opinion expression extraction.System P R FES+PC 56.5 ?
1.7 38.4 ?
1.2 45.7 ?
1.2ES+ER+PC 53.8 ?
1.6 44.5 ?
1.3 48.8 ?
1.2ES+PC+EPR 54.7 ?
1.6 45.6 ?
1.3 49.7 ?
1.27.4 Polarity ClassificationTo evaluate the effect of the polarity-based reranker, we carried out experiments tocompare it with two baseline systems similarly to the evaluations of holder extractionperformance.
Table 13 shows the precision, recall, and F-measures.
The evaluation usedthe polarity-based intersection metric (Section 7.1.2).
The first baseline consisted of anexpression segmenter and a polarity classifier (ES+PC), and the second also included anexpression reranker (ER).
The reranker using polarity interaction features is referred toas the expression/polarity reranker (EPR).The result shows that the polarity-based reranker gives a significant boost in recall,which is in line with our previous results that also mainly improved the recall.
Theprecision shows a slight decrease from the ES+PC baseline but much lower than therecall improvement.
The differences between the polarity reranker and the strongestbaseline are all statistically significant (precision p < 0.02, recall and F-measurep < 0.005).In addition, we evaluated the performance for individual polarity values.
Thefigures are shown in Table 14.
We see that the differences in performance when addingthe polarity reranker are concentrated to the more frequent polarity values (NEUTRALand NEGATIVE).Table 14Intersection-based evaluation for individual polarity values.POSITIVE P R FES+PC 53.5 ?
3.7 37.3 ?
3.0 43.9 ?
2.8ES+ER+PC 50.5 ?
3.4 41.8 ?
3.0 45.8 ?
2.6ES+PC+EPR 51.0 ?
3.5 41.6 ?
3.1 45.8 ?
2.7NEUTRAL P R FES+PC 56.4 ?
2.3 37.8 ?
1.7 45.3 ?
1.7ES+ER+PC 54.0 ?
2.1 45.2 ?
1.8 49.2 ?
1.6ES+PC+EPR 55.8 ?
2.1 46.1 ?
1.8 50.5 ?
1.6NEGATIVE P R FES+PC 58.4 ?
2.8 40.1 ?
2.4 47.6 ?
2.2ES+ER+PC 55.5 ?
2.7 45.0 ?
2.3 49.7 ?
2.0ES+PC+EPR 54.9 ?
2.7 47.0 ?
2.4 50.6 ?
2.0497Computational Linguistics Volume 39, Number 3Table 15Overlap-based evaluation for individual polarity values, and comparison with the resultsreported by Choi and Cardie (2010).POSITIVE P R FES+PC 59.4 ?
2.6 46.1 ?
2.1 51.9 ?
2.0ES+ER+PC 53.1 ?
2.3 50.9 ?
2.2 52.0 ?
1.9ES+PC+EPR 58.2 ?
2.5 49.3 ?
2.2 53.4 ?
2.0ES+PC+EPRp 63.6 ?
2.8 44.9 ?
2.2 52.7 ?
2.1Choi and Cardie (2010) 67.1 31.8 43.1NEUTRAL P R FES+PC 60.9 ?
1.4 49.2 ?
1.2 54.5 ?
1.0ES+ER+PC 55.1 ?
1.2 57.7 ?
1.2 56.4 ?
1.0ES+PC+EPR 60.3 ?
1.3 55.8 ?
1.2 58.0 ?
1.1ES+PC+EPRp 68.3 ?
1.5 48.2 ?
1.2 56.5 ?
1.2Choi and Cardie (2010) 66.6 31.9 43.1NEGATIVE P R FES+PC 72.1 ?
1.8 52.0 ?
1.5 60.4 ?
1.4ES+ER+PC 65.4 ?
1.7 58.2 ?
1.4 61.6 ?
1.3ES+PC+EPR 67.6 ?
1.7 59.9 ?
1.5 63.5 ?
1.3ES+PC+EPRp 75.4 ?
2.0 55.0 ?
1.5 63.6 ?
1.4Choi and Cardie (2010) 76.2 40.4 52.8Finally, we carried out an evaluation in the setting4 of Choi and Cardie (2010) andthe figures are shown in Table 15.
The table shows our baseline and integrated systemsalong with the figures5 from Choi and Cardie.
Instead of a single value for all polarities,we show the performance for every individual polarity value (POSITIVE, NEUTRAL,NEGATIVE).
This evaluation uses the overlap metric instead of the intersection-basedone.
As we have pointed out, we use the overlap metric for compatibility although it isproblematic.As can be seen from the table, the system by Choi and Cardie (2010) shows a largeprecision bias despite being optimizedwith respect to the recall-promoting overlapmet-ric.
In recall and F-measure, their system is significantly outperformed for all polarityvalues by our baseline consisting of a pipeline of opinion expression extraction andpolarity classifier.
In addition, our joint model clearly outperforms the pipeline.
Theprecision is slightly lower overall, but this is offset by large boosts in recall in all cases.In order to rule out the hypothesis that our F-measure improvement compared withthe Choi and Cardie system could be caused just by rebalancing precision and recall, weadditionally trained a precision-biased reranker EPRp by changing the loss function ?
(see Section 5.3) from 1?
Fi to 1?
13Fi ?
23Po, where Fi is the intersection F-measure andPo the overlap precision.
When we use this reranker, we achieve almost the same levelsof precision as reported by Choi and Cardie, even outperforming their precision for the4 In addition to polarity, their system also assigned opinion intensity, which we do not consider here.5 Confidence intervals for Choi and Cardie (2010) are omitted because we had no access to their output.498Johansson and Moschitti Relational Features in Fine-Grained Opinion AnalysisNEUTRAL polarity value, while the recall values are still massively higher.
The precisionbias causes slight drops in F-measure for the POSITIVE and NEUTRAL polarities.7.5 First Extrinsic Evaluation: Extraction of Evaluations of Product AttributesAs an extrinsic evaluation of the opinion expression extraction system, we evaluatedthe impact of the expressions on a practical application: extraction of evaluations ofattributes from product reviews.
We first describe the collection we used and then theimplementation of the extractor.We used the annotated data set by Hu and Liu (2004a, 2004b)6 for the experimentsin extraction of attribute evaluations from product reviews.
The collection containsreviews of five products: one DVD player, two cameras, one MP3 player, and onecellular phone.
In this data set, every sentence is associated with a set of attribute eval-uations.
An evaluation consists of an attribute name and an evaluation value between?3 and +3, where ?3 means a strongly negative evaluation and +3 strongly positive.For instance, the sentence this player boasts a decent size and weight, a relatively-intuitivenavigational system that categorizes based on id3 tags, and excellent sound is tagged with theattribute evaluations size +2, weight +2, navigational system +2, sound +2.
In thiswork, we do not make use of the exact value of the evaluation but only its sign.
Weremoved the product attribute mentions in the form of anaphoric pronouns referringto entities mentioned in previous sentences; these cases are directly marked in thedata set.7.5.1 Implementation.We considered two problems: (1) extraction of attribute evaluationswithout taking the polarity into account, and (2) extraction with polarity (positive ornegative).
The former is modeled as a binary classifier that tags each word in the review(except the punctuation) as an evaluation or not, and the latter requires the definition ofa three-class polarity classifier.
For both tasks, we compared three feature sets: a baselineusing simple features, a stronger baseline using a lexicon, and finally a system usingfeatures derived from opinion expressions.Similarly to the opinion expression polarity classifier, we implemented the clas-sifiers as SVMs that we trained using LIBLINEAR.
For the extraction task withoutpolarities, the best results were obtained using an L2-regularized L2-loss SVM and aC value of 0.1.
For the polarity task, we used a multiclass SVM (Crammer and Singer2001) with the same parameters.
To handle the precision/recall tradeoff, we varied theclass weighting for the null class.The baseline classifier used features based on lexical information (word, POS tag,and lemma) in a window of size 3 around the word under consideration (the focusword).
In addition, it had two features representing the overall sentence polarities.
Tocompute the polarities, we trained bag-of-words classifiers following the implemen-tation by Pang, Lee, and Vaithyanathan (2002).
Two separate classifiers were used:one for positive and one for negative polarity.
Note that these classifiers detect thepresence of positive or negative polarity, which may thus occur in the same sentence.
Theclassifiers were trained on the MPQA corpus, where we counted a sentence as positiveif it contained a positive opinion expression with an intensity of at least MEDIUM, andconversely for the negative polarity.6 http://www.cs.uic.edu/?liub/FBS/CustomerReviewData.zip.499Computational Linguistics Volume 39, Number 37.5.2 Features Using a Sentiment Lexicon.
Many previous implementations for severalopinion-related tasks make use of sentiment lexicons, so the stronger baseline systemused features based on the subjectivity lexicon by Wilson, Wiebe, and Hoffmann (2005),which we previously used for opinion expression segmentation in Section 4.1 and forpolarity classification in Section 4.3.
We created a classifier using a number of featuresbased on this lexicon.These features make use of the syntactic and semantic structure of the sentence.In the following examples, we use the sentence The software itself was not so easy to use,presented in Figure 3.
In this sentence, consider the focus word software.
One word islisted in the lexicon as associated with positive sentiment: easy.
The system then extractsthe following features:SENTIMENT LEXICON POLARITIES.
For every word in the sentence that is listed inthe lexicon, we add a feature.
Given the example sentence, we will thus add afeature lex pol:positive because of the word easy, which is listed as positive inthe sentiment lexicon.CLOSEST PREVIOUS AND FOLLOWING SENTIMENT WORD.
If there are sentimentwords before or after the focus word, we add the closest of them to the featurevector.
In this case, there is no previous sentiment word, so we only extract fol-lowing word:easy.SYNTACTIC PATHS TO SENTIMENT WORDS.
For every sentiment word in the sentence,we extract a syntactic path similar to our previous feature sets.
This represents asyntactic pattern describing the relation between the sentiment word and the focuswords.
For instance, in the example we extract the path SBJ?PRD?, representing acopula construction: The word software is connected to the sentiment word easy firstup through a subject link and then down through a predicative complement link.SEMANTIC LINKS TO SENTIMENT WORDS.
When there is a direct semantic role linkbetween a sentiment word and the focus word, we add a feature for the semanticrole label.
No such features are extracted in the example sentence.
The focus wordis an argument but no sentiment word is also a predicate.7.5.3 Extended Feature Set Based on MPQA Opinion Expressions.
We finally created anextended feature set incorporating the following features derived from MPQA-styleopinion expressions, which we extracted automatically.
The features are similar inconstruction to those extracted by means of the sentiment lexicon.
The following listdescribes the new features exemplified with the same sentence above, which contains anegative opinion expression not so easy.softwareThe itself was not so easyA1NMOD APPOSBJ PRDADV AMOD[ ]ESEusetoIMuse.01AMODFigure 3Example sentence for product feature evaluation extraction.500Johansson and Moschitti Relational Features in Fine-Grained Opinion AnalysisTable 16Product attribute evaluation extraction performance.Feature representation Unlabeled Polarity-labeledBaseline 49.8 ?
2.0 39.6 ?
2.0Lexicon 53.8 ?
2.0 46.2 ?
2.0Opinion expressions 54.8 ?
2.0 49.0 ?
2.0OPINION EXPRESSION POLARITIES.
For every opinion expression extracted by theautomatic system, we add a feature representing the polarity of the expression.In the example, we get op expr:negative.CLOSEST PREVIOUS AND FOLLOWING OPINION EXPRESSION WORD.
We extract fea-tures for the closest words before and after the focus word that are contained insome opinion expression.
In the example, there is an expression not so easy after thefocus word software, so we get a single feature following expr:not.SYNTACTIC PATHS TO OPINION EXPRESSIONS.
For every opinion expression in thesentence, we extract a path from the expression to the focus word.
Because opinionexpressions frequently consist of more than one word, we use the shortest path.
Inthis case, we will thus again get SBJ?PRD?.SEMANTIC LINKS TO OPINION EXPRESSIONS.
Finally, we extracted features in casethere were semantic role links.
Again, we get no features based on the semanticrole structure in the example since the opinion expression contains no predicate orargument.7.5.4 Results.
We evaluated the performance of the product attribute evaluation extrac-tion using a 10-fold cross-validation procedure on the whole data set.
We evaluatedthree classifiers: a baseline that did not use the lexicon or the opinion expressions, aclassifier that adds the lexicon-based features, and finally the classifier that adds theMPQA opinion expressions.
The F-measures are shown in Table 16 for the extractiontask, and Figure 4 shows the precision/recall plots.
There are clear improvements whenadding the lexicon features, but the highest performing system is the one that alsoused the opinion expression features.
The difference between the two top-performing0.40.450.50.550.60.650.70.4 0.45 0.5 0.55 0.6 0.65 0.7RecallPrecisionUnlabeledBaselineLexiconOpinion Expressions0.250.30.350.40.450.50.550.60.650.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 0.65RecallPrecisionPolarity-labeledBaselineLexiconOpinion ExpressionsFigure 4Precision / recall curves for extraction of product attribute evaluations.501Computational Linguistics Volume 39, Number 3classifiers is statistically significant (p < 0.001).
For the extraction task where we alsoconsider the polarities, the difference is even greater: almost three F-measure points.7.6 Second Extrinsic Evaluation: Document Polarity Classification ExperimentIn a second extrinsic evaluation of the opinion expression extractor, we investigatedhow expression-based features affect the performance of a document-level polarityclassifier of reviews as positive or negative.
We followed the same evaluation protocolas in the first extrinsic evaluation, where we compare three classifiers of increasingcomplexity: (1) a baseline using a pure word-based representation, (2) a stronger base-line adding features derived from a sentiment lexicon, and (3) a classifier with featuresextracted from opinion expressions.The task of categorizing a full document as positive or negative can be viewedas a document categorization task, and this has led to the application of standardtext categorization techniques (Pang, Lee, and Vaithyanathan 2002).
We followed thisapproach and implemented the document polarity classifier as a binary linear SVM;this learning method has a long tradition of successful application in text categorization(Joachims 2002).For these experiments, we used six collections.
The first one consisted of moviereviews written in English extracted from the Web by Pang and Lee (2004).7 This dataset is an extension of a smaller set (Pang, Lee, and Vaithyanathan 2002) that has beenused in a large number of experiments.
The remaining five sets consisted of productreviews gathered by Blitzer, Dredze, and Pereira (2007).8 We used five of the largestsubsets: reviews of DVDs, software, books, music, and cameras.
In all six collections,1,000 documents were labeled by humans as positive and 1,000 as negative.Following Pang and Lee (2004), the documents were represented as bag-of-wordfeature vectors based on presence features for individual words.
No weighting such asIDF was used.
The vectors were normalized to unit length.
Again, we trained the SVMsusing LIBLINEAR, and the best results were obtained using an L2-regularized L2-lossversion of the SVM with a C value of 1.7.6.1 Features Based on the Subjectivity Lexicon.
We used features based on the subjectivitylexicon by Wilson, Wiebe, and Hoffmann (2005) that we used for opinion expressionsegmentation in Section 4.1 and for polarity classification in Section 4.3.
For every wordwhose lemma is listed in the lexicon, we added a feature consisting of the word and itsprior polarity and intensity to the bag-of-words feature vector.The feature examples are taken from the sentence HRW has denounced the defenselesssituation of these prisoners, where denounce is listed in the lexicon as strong/negativeand prisoner as weak/negative.LEXICON POLARITY.
negative.LEXICON POLARITY AND INTENSITY.
strong/negative, weak/negative.LEXICON POLARITY AND WORD.
denounced/negative, prisoners/negative.7 http://www.cs.cornell.edu/people/pabo/movie-review-data/.8 http://www.cs.jhu.edu/?mdredze/datasets/sentiment/unprocessed.tar.gz.502Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysis7.6.2 Features Extracted from Opinion Expressions.
Finally, we created a feature set basedon the opinion expressions with polarities.
We give examples from the same sentence;here, denounced is a negative DSE and defenseless situation is a negative ESE.EXPRESSION POLARITY.
negative.EXPRESSION POLARITY AND WORD.
negative/denounced, negative/defenseless,negative/situation.EXPRESSION TYPE AND WORD.
DSE/denounced, ESE/defenseless, ESE/situation.7.6.3 Evaluation Results.
To evaluate the performance of the document polarity classifiers,we carried out a 10-fold cross-validation procedure for every review collection.
Weevaluated three classifiers: one using only bag-of-words features (?Baseline?
); one usingfeatures extracted from the subjectivity lexicon (?Lexicon?
); and finally one also usingthe expression-based features (?Expressions?
).In order to abstract away from the tuning threshold, the performances were mea-sured using AUC, the area under ROC curve.
The AUC values are given in Table 17.These evaluations show that the classifier adding features extracted from the opin-ion expressions significantly outperforms the classifier using only a bag-of-words fea-ture representation and also that using the lexicon-based features.
This demonstratesthat the extraction and disambiguation of opinion expressions in their context is usefulfor a coarse-grained task such as document polarity classification.
The differences inAUC values between the two best configurations are statistically significant (p < 0.005for all six collections).
In addition, we show the precision/recall plots in Figure 5; wesee that for all six collections, the expression-based set-up outperforms the other twonear the precision/recall breakeven point.The collection where we can see the most significant difference is the movie reviewset.
The main difference of this collection compared with the other collections is that itsdocuments are larger: The average size of a document here is about four times largerthan in the other collections.
In addition, its reviews often contain large sections that arepurely factual in nature, mainly plot descriptions.
The opinion expression identificationmay be seen as a way to process the document to highlight the interesting parts onwhich the classifier should focus.8.
ConclusionWe have shown that features derived from grammatical and semantic role structurecan be used to improve three fundamental tasks in fine-grained opinion analysis: thedetection of opinionated expressions, the extraction of opinion holders, and finally theTable 17Document polarity classification evaluation (AUC values).Feature set Movie DVD Software Books Music CamerasBaseline 93.1 ?
1.0 85.1 ?
1.7 91.0 ?
1.3 85.7 ?
1.6 84.7 ?
1.7 91.9 ?
1.2Lexicon 93.8 ?
1.0 86.6 ?
1.6 92.3 ?
1.2 87.4 ?
1.5 86.6 ?
1.5 92.9 ?
1.1Expressions 94.7 ?
0.9 87.2 ?
1.5 92.9 ?
1.1 88.1 ?
1.5 87.5 ?
1.5 93.6 ?
1.1503Computational Linguistics Volume 39, Number 30.60.650.70.750.80.850.90.9510.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1RecallPrecisionMoviesBaselineLexiconOpinion Expressions0.60.650.70.750.80.850.90.9510.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1RecallPrecisionDVDsBaselineLexiconOpinion Expressions0.60.650.70.750.80.850.90.9510.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1RecallPrecisionSoftwareBaselineLexiconOpinion Expressions0.60.650.70.750.80.850.90.9510.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1RecallPrecisionBooksBaselineLexiconOpinion Expressions0.60.650.70.750.80.850.90.9510.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1RecallPrecisionMusicBaselineLexiconOpinion Expressions0.60.650.70.750.80.850.90.9510.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1RecallPrecisionCamerasBaselineLexiconOpinion ExpressionsFigure 5Precision / recall curves for detection of positive reviews.assignment of polarity labels to opinion expressions.
The main idea is to use relationalfeatures describing the interaction of opinion expressions through linguistic structuressuch as syntax and semantics.
This is not only interesting from a practical point ofview (improving performance) but also confirms our linguistic intuitions that surface-linguistic structure phenomena such as syntax and shallow semantics are used in theencoding of the rhetorical organization of the sentence, and that we can thus extractuseful information from those structures.504Johansson and Moschitti Relational Features in Fine-Grained Opinion AnalysisBecause our feature sets are based on interaction between opinion expressionsthat can appear anywhere in a sentence, exact inference in this model becomes in-tractable.
To overcome this issue, we used an approximate search strategy based onreranking: In the first step, we used the baseline systems, which use only simple localfeatures, to generate a relatively small hypothesis set; we then applied a classifier usinginteraction features to pick the final result.
A common objection to reranking is thatthe candidate set may not be diverse enough to allow for much improvement unlessit is very large; the candidates may be trivial variations that are all very similar tothe top-scoring candidate.
Investigating inference methods that take a less brute-forceapproach than plain reranking is thus another possible future direction.
Interestingexamples of such inference methods include forest reranking (Huang 2008) and loopybelief propagation (Smith and Eisner 2008).
Nevertheless, although the development ofsuch algorithms is a fascinating research problem, it will not necessarily result in a moreusable system: Rerankers impose very few restrictions on feature expressivity andmakeit easy to trade accuracy for efficiency.We investigated the effect of machine learning features, as well as other designparameters such as the choice of machine learning method and the size of thehypothesis set.
For the features, we analyzed the impact of using syntax and semanticsand saw that the best models are those making use of both.
The most effective featureswe have found are purely structural, based on tree fragments in a syntactic or semantictree.
Features involving words generally did not seem to have the same impact.
Sparsitymay certainly be an issue for features defined in terms of tree fragments.
Possible futureextensions in this area could include bootstrapping methods to mine for meaningfulfragments unseen in the training set, or methods to group such features into clusters toreduce the sparsity.In addition to the core results on fine-grained opinion analysis, we have describedexperiments demonstrating that features extracted from opinion expressions can beused to improve practical applications: extraction of evaluations of product attributes,and document polarity classification.
Although for the first task it may be fairly obviousthat it is useful to carry out a fine-grained analysis of the sentence opinion structure,the second result is more unexpected because the document polarity classification taskis a high-level and coarse-grained task.
For both tasks, we saw statistically significantincreases in performance compared not only to simple baselines, but also comparedto strong baselines using a lexicon of sentiment words.
Although the lexicon leads toclear improvements, the best classifiers also used the features extracted from the opinionexpressions.It is remarkable that the opinion expressions as defined by the MPQA corpus areuseful for practical applications on reviews from several domains, because this corpusmainly consists of news documents related to political topics; this shows that the expres-sion identifier has been able to generalize from the specific domains.
It would still berelevant, however, to apply domain adaptation techniques (Blitzer, Dredze, and Pereira2007).
It could also be interesting to see how domain-specific opinion word lexiconscould improve over the generic lexicon we used here; especially if such a lexicon wereautomatically constructed (Jijkoun, de Rijke, and Weerkamp 2010).There are multiple additional opportunities for future work in this area.
An impor-tant issue that we have left open is the coreference problem for holder extraction, whichhas been studied by Stoyanov and Cardie (2006).
Similarly, recent work has tried toincorporate complex, high-level linguistic structure such as discourse representations(Asher, Benamara, and Mathieu 2009; Somasundaran et al2009; Zirn et al2011); it isclear that these structures are very relevant for explaining the way humans organize505Computational Linguistics Volume 39, Number 3their expressions of opinions rhetorically.
Theoretical depth does not necessarily guar-antee practical applicability, however, and the challenge is as usual to find a middleground that balances our goals: explanatory power in theory, significant performancegains in practice, computational tractability, and robustness in difficult circumstances.AcknowledgmentsWe would like to thank Eric Breck andYejin Choi for clarifying their results andexperimental set-up, and for sharing theircross-validation split.
In addition, we aregrateful to the anonymous reviewers,whose feedback has helped to improvethe clarity and readability of this article.The research described here has receivedfunding from the European Community?sSeventh Framework Programme(FP7/2007-2013) under grant 231126:LivingKnowledge?Facts, Opinions and Biasin Time, and under grant 247758:Trustworthy Eternal Systems via EvolvingSoftware, Data and Knowledge (EternalS).ReferencesAsher, Nicholas, Farah Benamara, andYannick Mathieu.
2009.
Appraisal ofopinion expressions in discourse.Lingvisticae Investigations, 31(2):279?292.Bethard, Steven, Hong Yu, Ashley Thornton,Vasileios Hatzivassiloglou, andDan Jurafsky.
2005.
Extracting opinionpropositions and opinion holdersusing syntactic and lexical cues.In James G. Shanahan, Yan Qu, andJanyce Wiebe, editors, ComputingAttitude and Affect in Text: Theory andApplications.
Springer, New York,chapter 11, pages 125?140.Blitzer, John, Mark Dredze, and FernandoPereira.
2007.
Biographies, Bollywood,boom-boxes and blenders: Domainadaptation for sentiment classification.
InProceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics(ACL-07), pages 440?447, Prague.Boser, Bernhard, Isabelle Guyon, andVladimir Vapnik.
1992.
A trainingalgorithm for optimal margin classifiers.In Proceedings of the Fifth Annual Workshopon Computational Learning Theory,pages 144?152, Pittsburgh, PA.Breck, Eric, Yejin Choi, and Claire Cardie.2007.
Identifying expressions of opinionin context.
In IJCAI 2007, Proceedings ofthe 20th International Joint Conference onArtificial Intelligence, pages 2,683?2,688,Hyderabad.Choi, Yejin, Eric Breck, and Claire Cardie.2006.
Joint extraction of entities andrelations for opinion recognition.
InProceedings of the 2006 Conference onEmpirical Methods in Natural LanguageProcessing, pages 431?439, Sydney.Choi, Yejin and Claire Cardie.
2008.Learning with compositional semanticsas structural inference for subsententialsentiment analysis.
In Proceedings of the2008 Conference on Empirical Methodsin Natural Language Processing,pages 793?801, Honolulu, HI.Choi, Yejin and Claire Cardie.
2010.Hierarchical sequential learning forextracting opinions and their attributes.In Proceedings of the 48th Annual Meetingof the Association for ComputationalLinguistics, pages 269?274, Uppsala.Collins, Michael.
2000.
Discriminativereranking for natural languageparsing.
In Proceedings of theSeventeenth International Conference onMachine Learning, pages 175?182,San Francisco, CA.Collins, Michael.
2002.
Discriminativetraining methods for hidden Markovmodels: Theory and experiments withperceptron algorithms.
In Proceedings of the2002 Conference on Empirical Methods inNatural Language Processing (EMNLP 2002),pages 1?8, Philadelphia, PA.Crammer, Koby, Ofer Dekel, Joseph Keshet,Shai Shalev-Schwartz, and Yoram Singer.2006.
Online passive-aggressivealgorithms.
Journal of Machine LearningResearch, 2006(7):551?585.Crammer, Koby and Yoram Singer.
2001.On the algorithmic implementation ofmulticlass kernel-based vector machines.Journal of Machine Learning Research,2001(2):265?585.Fan, Rong-En, Kai-Wei Chang, Cho-JuiHsieh, Xiang-Rui Wang, and Chih-Jen Lin.2008.
LIBLINEAR: A library for large linearclassification.
Journal of Machine LearningResearch, 9:1871?1874.Freund, Yoav and Robert E. Schapire.
1999.Large margin classification using theperceptron algorithm.
Machine Learning,37(3):277?296.Gerber, Matthew and Joyce Chai.
2010.Beyond NomBank: A study of implicit506Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysisarguments for nominal predicates.In Proceedings of the 48th Annual Meetingof the Association for ComputationalLinguistics, pages 1,583?1,592, Uppsala.Greene, Stephan and Philip Resnik.2009.
More than words: Syntacticpackaging and implicit sentiment.In Proceedings of Human LanguageTechnologies: The 2009 Annual Conferenceof the North American Chapter of theAssociation for Computational Linguistics,pages 503?511, Boulder, CO.Hjorth, J. S. Urban.
1993.
Computer IntensiveStatistical Methods.
Chapman and Hall,London.Hu, Minqing and Bing Liu.
2004a.
Miningand summarizing customer reviews.In Proceedings of the ACM SIGKDDInternational Conference on KnowledgeDiscovery and Data Mining (KDD-04),pages 168?177, Seattle, WA.Hu, Minqing and Bing Liu.
2004b.
Miningopinion features in customer reviews.In Proceedings of the Nineteeth NationalConference on Artificial Intellgience(AAAI-2004), pages 755?760,San Jose, CA.Huang, Liang.
2008.
Forest reranking:Discriminative parsing with non-localfeatures.
In Proceedings of ACL-08: HLT,pages 586?594, Columbus, OH.Jijkoun, Valentin, Maarten de Rijke, andWouter Weerkamp.
2010.
Generatingfocused topic-specific sentiment lexicons.In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics,pages 585?594, Uppsala.Joachims, Thorsten.
2002.
Learning to ClassifyText using Support Vector Machines.Kluwer/Springer, Boston.Joachims, Thorsten, Thomas Finley, andChun-Nam Yu.
2009.
Cutting-planetraining of structural SVMs.
MachineLearning, 77(1):27?59.Johansson, Richard and Pierre Nugues.
2008.Dependency-based syntactic?semanticanalysis with PropBank and NomBank.In CoNLL 2008: Proceedings of the TwelfthConference on Natural Language Learning,pages 183?187, Manchester.Joshi, Mahesh and Carolyn Penstein-Rose?.2009.
Generalizing dependency featuresfor opinion mining.
In Proceedings of theACL-IJCNLP 2009 Conference Short Papers,pages 313?316, Singapore.Karlgren, Jussi, Gunnar Eriksson, MagnusSahlgren, and Oscar Ta?ckstro?m.
2010.Between bags and trees?Constructionalpatterns in text used for attitudeidentification.
In Proceedings of ECIR 2010,32nd European Conference on InformationRetrieval, pages 38?49, Milton Keynes.Kim, Soo-Min and Eduard Hovy.
2006.Extracting opinions, opinion holders, andtopics expressed in online news mediatext.
In Proceedings of the Workshop onSentiment and Subjectivity in Text,pages 1?8, Sydney.Kobayashi, Nozomi, Kentaro Inui, andYuji Matsumoto.
2007.
Extractingaspect-evaluation and aspect-of relationsin opinion mining.
In Proceedings of the2007 Joint Conference on Empirical Methodsin Natural Language Processing andComputational Natural Language Learning(EMNLP-CoNLL), pages 1,065?1,074,Prague.Mel?c?uk, Igor A.
1988.
Dependency Syntax:Theory and Practice.
State University Pressof New York, Albany.Meyers, Adam, Ruth Reeves, CatherineMacleod, Rachel Szekely, VeronikaZielinska, Brian Young, and RalphGrishman.
2004.
The NomBank project:An interim report.
In HLT-NAACL 2004Workshop: Frontiers in Corpus Annotation,pages 24?31, Boston, MA.Moschitti, Alessandro and Roberto Basili.2004.
Complex linguistic features fortext classification: A comprehensivestudy.
In Proceedings of the 26th EuropeanConference on Information RetrievalResearch (ECIR 2004), pages 181?196,Sunderland.Palmer, Martha, Dan Gildea, and PaulKingsbury.
2005.
The proposition bank:An annotated corpus of semantic roles.Computational Linguistics, 31(1):71?105.Pang, Bo and Lillian Lee.
2004.
A sentimentaleducation: Sentiment analysis usingsubjectivity summarization based onminimum cuts.
In Proceedings of the 42ndMeeting of the Association for ComputationalLinguistics (ACL?04), Main Volume,pages 271?278, Barcelona.Pang, Bo and Lillian Lee.
2008.
Opinionmining and sentiment analysis.Foundations and Trends in InformationRetrieval, 2(1?2):1?135.Pang, Bo, Lillian Lee, and ShivakumarVaithyanathan.
2002.
Thumbs up?Sentiment classification using machinelearning techniques.
In Proceedings of the2002 Conference on Empirical Methods inNatural Language Processing, pages 79?86,Philadelphia, PA.Popescu, Ana-Maria and Oren Etzioni.
2005.Extracting product features and opinions507Computational Linguistics Volume 39, Number 3from reviews.
In Proceedings of HumanLanguage Technology Conference andConference on Empirical Methods in NaturalLanguage Processing, pages 339?346,Vancouver.Prasad, Rashmi, Nikhil Dinesh, Alan Lee,Eleni Miltsakaki, Livio Robaldo,Aravind Joshi, and Bonnie Webber.2008.
The Penn Discourse Treebank 2.0.In Proceedings of the 6th InternationalConference on Languages Resourcesand Evaluations (LREC 2008),pages 2,961?2,968, Marrakech.Quirk, Randolph, Sidney Greenbaum,Geoffrey Leech, and Jan Svartvik.
1985.A Comprehensive Grammar of the EnglishLanguage.
Longman, New York.Ruppenhofer, Josef, Swapna Somasundaran,and Janyce Wiebe.
2008.
Finding thesources and targets of subjectiveexpressions.
In Proceedings of the SixthInternational Language Resources andEvaluation (LREC?08), pages 2,781?2,788,Marrakech.Schwartz, Richard and Steve Austin.
1991.A comparison of several approximatealgorithms for finding multiple (n-best)sentence hypotheses.
In Proceedings of theIEEE International Conference on Acoustics,Speech, and Signal Processing (ICASSP),pages 701?704, Toronto.Smith, David and Jason Eisner.
2008.Dependency parsing by beliefpropagation.
In Proceedings of theConference on Empirical Methods inNatural Language Processing (EMNLP),pages 145?156, Honolulu, HI.Somasundaran, Swapna, Galileo Namata,Janyce Wiebe, and Lise Getoor.
2009.Supervised and unsupervised methodsin employing discourse relations forimproving opinion polarity classification.In Proceedings of the 2009 Conference onEmpirical Methods in Natural LanguageProcessing, pages 170?179, Singapore.Stoyanov, Veselin and Claire Cardie.2006.
Partially supervised coreferenceresolution for opinion summarizationthrough structured rule learning.In Proceedings of the 2006 Conferenceon Empirical Methods in NaturalLanguage Processing, pages 336?344,Sydney.Stoyanov, Veselin and Claire Cardie.2008.
Annotating topics of opinions.In Proceedings of the Sixth InternationalLanguage Resources and Evaluation(LREC?08), pages 3,213?3,217,Marrakech.Surdeanu, Mihai, Richard Johansson,Adam Meyers, Llu?
?s Ma`rquez, andJoakim Nivre.
2008.
The CoNLL-2008shared task on joint parsing ofsyntactic and semantic dependencies.In Proceedings of CoNLL 2008,pages 159?177, Manchester.Taskar, Ben, Carlos Guestrin, and DaphneKoller.
2004.
Max-margin Markovnetworks.
In Advances in Neural InformationProcessing Systems 16, pages 25?32,Cambridge, MA, MIT Press.Titov, Ivan and Ryan McDonald.
2008.A joint model of text and aspectratings for sentiment summarization.In Proceedings of ACL-08: HLT,pages 308?316, Columbus, OH.Tjong Kim Sang, Erik F., and Jorn Veenstra.1999.
Representing text chunks.In Proceedings of the Ninth Conferenceof the European Chapter of the Associationfor Computational Linguistics,pages 173?179, Bergen.Tsochantaridis, Ioannis, Thorsten Joachims,Thomas Hofmann, and Yasemin Altun.2005.
Large margin methods forstructured and interdependentoutput variables.
Journal of MachineLearning Research, 6(Sep):1453?1484.Wiebe, Janyce, Rebecca Bruce, and ThomasO?Hara.
1999.
Development and use of agold standard data set for subjectivityclassifications.
In Proceedings of the 37thAnnual Meeting of the Association forComputational Linguistics, pages 246?253,College Park, MD.Wiebe, Janyce, Theresa Wilson, and ClaireCardie.
2005.
Annotating expressions ofopinions and emotions in language.Language Resources and Evaluation,39(2-3):165?210.Wiegand, Michael and Dietrich Klakow.2010.
Convolution kernels for opinionholder extraction.
In Human LanguageTechnologies: The 2010 Annual Conferenceof the North American Chapter of theAssociation for Computational Linguistics,pages 795?803, Los Angeles, CA.Wilson, Theresa, Janyce Wiebe, andPaul Hoffmann.
2005.
Recognizingcontextual polarity in phrase-levelsentiment analysis.
In Proceedings ofHuman Language Technology Conferenceand Conference on Empirical Methodsin Natural Language Processing,pages 347?354, Vancouver.Wilson, Theresa, Janyce Wiebe, and PaulHoffmann.
2009.
Recognizing contextualpolarity: An exploration of features508Johansson and Moschitti Relational Features in Fine-Grained Opinion Analysisfor phrase-level sentiment analysis.Computational Linguistics, 35(3):399?433.Wu, Yuanbin, Qi Zhang, XuangjingHuang, and Lide Wu.
2009.
Phrasedependency parsing for opinionmining.
In Proceedings of the 2009Conference on Empirical Methods inNatural Language Processing,pages 1,533?1,541, Singapore.Yu, Hong and Vasileios Hatzivassiloglou.2003.
Towards answering opinionquestions: Separating facts from opinionsand identifying the polarity of opinionsentences.
In Proceedings of the Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP-2003), pages 129?136,Sapporo.Zirn, Ca?cilia, Mathias Niepert, HeinerStuckenschmidt, and Michael Strube.
2011.Fine-grained sentiment analysis withstructural features.
In Proceedings of 5thInternational Joint Conference on NaturalLanguage Processing, pages 336?344,Chiang Mai.509
