Proceedings of the 7th Workshop on Statistical Machine Translation, pages 1?9,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsPutting Human Assessments of Machine Translation Systems in OrderAdam LopezHuman Language Technology Center of ExcellenceJohns Hopkins UniversityAbstractHuman assessment is often considered thegold standard in evaluation of translation sys-tems.
But in order for the evaluation tobe meaningful, the rankings obtained fromhuman assessment must be consistent andrepeatable.
Recent analysis by Bojar etal.
(2011) raised several concerns about therankings derived from human assessments ofEnglish-Czech translation systems in the 2010Workshop on Machine Translation.
We extendtheir analysis to all of the ranking tasks from2010 and 2011, and show through an exten-sion of their reasoning that the ranking is nat-urally cast as an instance of finding the mini-mum feedback arc set in a tournament, a well-known NP-complete problem.
All instancesof this problem in the workshop data are ef-ficiently solvable, but in some cases the rank-ings it produces are surprisingly different fromthe ones previously published.
This leads tostrong caveats and recommendations for bothproducers and consumers of these rankings.1 IntroductionThe value of machine translation depends on its util-ity to human users, either directly through their useof it, or indirectly through downstream tasks suchas cross-lingual information extraction or retrieval.It is therefore essential to assess machine transla-tion systems according to this utility, but there is awidespread perception that direct human assessmentis costly, unreproducible, and difficult to interpret.Automatic metrics that predict human utility havetherefore attracted substantial attention since theyare at least cheap and reproducible given identicaldata conditions, though they are frequently and cor-rectly criticized for low interpretability and correla-tion with true utility.
Their use (and abuse) remainscontentious.The organizers of the annual Workshop on Ma-chine Translation (WMT) have taken a strong stancein this debate, asserting the primacy of human eval-uation.
Every annual report of their findings since2007 has included a variant of the following state-ment:It is our contention that automatic mea-sures are an imperfect substitute for hu-man assessment of translation quality.Therefore, we define the manual evalua-tion to be primary, and use the humanjudgments to validate automatic metrics.
(Callison-Burch et al, 2011)The workshop?s human evaluation component hasbeen gradually refined over several years, and as aconsequence it has produced a fantastic collection ofpublicly available data consisting primarily of pair-wise judgements of translation systems made by hu-man assessors across a wide variety of languagesand tasks.
Despite superb effort in the collection ofthese assessments, less attention has been focusedon the final product derived from them: a totally-ordered ranking of translation systems participatingin each task.
Many of the official workshop resultsdepend crucially on this ranking, including the eval-uation of both machine translation systems and auto-matic metrics.
Considering the enormous costs andconsequences of the ranking, it is important to ask:is the method of constructing it accurate?
The num-ber of possible rankings is combinatorially large?with at least ten systems (accounting for more than1half the cases we analyzed) there are over three mil-lion possible rankings, and with at least twenty (oc-curring a few times), there are over 1018 possiblerankings.
Exceptional care is therefore required inproducing the rankings.Bojar et al (2011) observed a number of discrep-ancies in the ranking of English-Czech systems fromthe 2010 workshop, making these questions evermore pressing.
We extend their analysis in severalways.1.
We show, through a logical extension of theirreasoning about flaws in the evaluation, thatthe final ranking can be naturally cast as an in-stance of the minimal feedback arc set problem,a well-known NP-Hard problem.2.
We analyze 25 tasks that were evaluated usingpairwise assessments from human annotators in2010 and 2011.3.
We produce new rankings for each of the tasks,which are in some cases surprisingly differentfrom the published rankings.4.
We identify a new set of concerns about sourcesof error and uncertainty in the data.2 Human Assessment as Pairwise RankingThe workshop has conducted a variety of differentmanual evaluation tasks over the last several years,but its mainstay has been the relative ranking task.Assessors are presented with a source sentence fol-lowed by up to five translations, and are asked torank the translations from best to worst, with tiesallowed.
Since it is usually infeasible to collect in-dividual judgements for all sentences for all pairs ofsystems on each task, consecutive sequences of threesentences were randomly sampled from the test data,with each sentence in each sequence presented to thesame annotator.
Some samples were presented mul-tiple times to the same assessor or to multiple asses-sors in order to measure intra- and inter-annotatoragreement rates.
Since there are often more thanfive systems participating in the campaign, the can-didate translations are likewise sampled from a poolconsisting of the machine translations and a humanreference translation, which is included for qualityJHU 1 JHU?BBN-COMBOBBN-COMBO 2 JHU?RWTHRWTH 3 JHU?RWTH-COMBORWTH-COMBO 3 JHU?CMUCMU 4 BBN-COMBO?RWTHBBN-COMBO?RWTH-COMBOBBN-COMBO?CMURWTH?RWTH-COMBORWTH?CMURWTH-COMBO?CMUFigure 1: Example human relative ranking of five sys-tems (left) and the inferred pairwise rankings (right) ona single sentence from the WMT 2010 German-Englishcampaign.control purposes.
It is important to note that the al-gorithm used to compute the published final rank-ings included all of this data, including comparisonsagainst the reference and the redundant assessmentsused to compute inter-annotator agreement.The raw data obtained from this process is a largeset of assessments.
Each assessment consists of alist of up to five systems (including the reference),and a partial or total ordering of the list.
The relativeranking of each pair of systems contained in the listis then taken to be their pairwise ranking.
Hence asingle assessment of five systems yields ten implicitpairwise rankings, as illustrated in Figure 1.3 From Pairwise to Total RankingGiven these pairwise rankings, the question now be-comes: how do we decide on a total ordering ofthe systems?
In the WMT evaluation, this total or-dering has two critical functions: it is published asthe official ranking of the participating systems; andit is used as the ground truth against which auto-matic evaluation metrics are graded, using Spear-man?s rank correlation coefficient (without ties) asthe measure of accuracy.
Choosing a total order isnon-trivial: there are N !
possible orderings of Nsystems.
Even with relatively small N of the work-shop, this number can grow extremely large (over1025 in the worst case of 25 systems).The method used to generate the published rank-ings is simple.
For each system A among the setS of ranked systems (which includes the reference),2compute the number of times that A is ranked betterthan or equivalent to any system B ?
S, and thendivide by the total number of comparisons involv-ing A, yielding the following statistic for system A,which we call WMT-OFFICAL.score(A) =?B?S count(A  B)?B?S,3?
{?,?,}, count(A3B)(1)The systems are ranked according to this statistic,with higher scores resulting in a better rank.Bojar et al (2011) raise many concerns about thismethod for ranking the systems.
While we refer thereader to their paper for a detailed analysis, we focuson two issues here:?
Since ties are rewarded, systems may be un-duly rewarded for merely being similar to oth-ers, rather than clearly better.
This is of particu-lar concern since there is often a cohort of verysimilar systems in the pool, such as those basedon very similar techniques.?
Since the reference is overwhelmingly favoredby the assessors, those systems that are morefrequently compared against the reference inthe random sample will be unfairly penalized.These observations suggest that the statisticshould be changed to reward only outright wins inpairwise comparisons, and to lessen the number ofcomparisons to the reference.
While they do notrecommend a specific sampling rate for comparisonsagainst the reference, the logical conclusion of theirreasoning is that it should not be sampled at all.
Thisyields the following statistic similar to one reportedin the appendices of the WMT proceedings, whichwe call HEURISTIC 2.score(A) =?B?S?ref count(A ?
B)?B?S?ref,3?
{?,?,}, count(A3B)(2)However, the analysis by Bojar et al (2011) goesfurther and suggests disregarding the effect of tiesaltogether by removing them from the denominator.This yields their final recommended statistic, whichwe call BOJAR.score(A) =?B?S?ref count(A ?
B)?B?S?ref,3?
{?,}, count(A3B)(3)Superficially, this appears to be an improve-ment.
However, we observe in the rankings thattwo anonymized commercial systems, denoted ON-LINEA and ONLINEB, consistently appear at or nearthe top of the rankings in all tasks.
It is natural towonder: even if we leave out the reference fromcomparisons, couldn?t a system still be penalizedsimply by being compared against ONLINEA andONLINEB more frequently than its competitors?
Onthe other hand, couldn?t a system be rewarded sim-ply by being compared against a bad system morefrequently than its competitors?There are many possible decisions that we couldmake, each leading to a different ranking.
However,there is a more fundamental problem: each of theseheuristic scores is based on statistics aggregated overcompletely incomparable sets of data.
Any totalordering of the systems must make a decision be-tween every pair of systems.
When that ranking iscomputed using scores computed with any of Equa-tions 1 through 3, we aggregate over completely dif-ferent sets of sentences, rates of comparison withother systems, and even annotators!
Deriving sta-tistical conclusions from such comparisons is at bestsuspect.
If we want to rank A and B relative to eachother, it would be more reliable to aggregate overthe same set of sentences, same rates of comparison,and the same annotators.
Fortunately, we have thisdata in abundance: it is the collection of pairwisejudgements that we started with.4 Pairwise Ranking as a TournamentThe human assessments are a classic example of atournament.
A tournament is a graph of N verticeswith exactly(N2)directed edges?one between eachpair of vertices.
The edge connecting each pair ofvertices A and B points to whichever vertex whichis worse in an observed pairwise comparison be-tween them.
Tournaments are a natural represen-tation of many ranking problems, including searchresults, transferable voting systems, and ranking ofsports teams.1Consider the simple weighted tournament de-picted in Figure 2.
This tournament is acyclic, whichmeans that we can obtain a total ordering of the ver-1The original motivating application was modeling the peck-ing order of chickens (Landau, 1951).3ABCD321112Consistent ranking: A ?
B ?
C ?
DRanking according to Eq.
1: A ?
C ?
B ?
DFigure 2: A weighted tournament and two different rank-ings of its vertices.tices that is consistent with all of the pairwise rank-ings simply by sorting the vertices topologically.
Westart by choosing the vertex with no incoming edges(i.e.
the one that wins in all pairwise comparisons),place it at the top of the ranking, and remove it alongwith all of its outgoing edges from the graph.
Wethen repeat the procedure with the remaining ver-tices in the graph, placing the next vertex behindthe first one, and so on.
The result is a ranking thatpreserves all of the pairwise rankings in the originalgraph.This example also highlights a problem in Equa-tion 1.
Imagine an idealized case in which the con-sistent ranking of the vertices in Figure 2 is their trueranking, and furthermore that this ranking is unam-biguous: that is, no matter how many times we sam-ple the comparison A with B, the result is alwaysthat A ?
B, and likewise for all vertices.
If theweights in this example represented the number ofrandom samples for each system, then Equation 1will give the inaccurate ranking shown, since it pro-duces a score of 25 for B and24 for C.Tournaments can contain cycles, and as we willshow this is often the case in the WMT data.
Whenthis happens, a reasonable solution is to minimizethe discrepancy between the ranking and the ob-served data.
We can do this by reversing a set ofedges in the graph such that (1) the resulting graphis acyclic, and (2) the summed weights of the re-versed edges is minimized.
A set of edges satisfyingthese constraints is called the minimum feedback arcset (Figure 3).The feedback arc set problem on general graphsEFGH321212Figure 3: A tournament with a cycle on vertices E, F ,and G. The dotted edge is the only element of a minimumfeedback arc set: reversing it produces an acyclic graph.Algorithm 1 Minimum feedback arc set solverInput: Graph G = (V,E), weights w : E ?
R+Initialize all costs to?Let cost(?)?
0Add ?
to agenda ArepeatLet R??
argminR?A cost(R)Remove R?
from A .
R?
is a partial rankingLet U ?
V \R?
.
set of unranked verticesfor each vertex v ?
U doAdd R?
?
v to agendaLet c??v?
?U :?v?,v?
?E w(?v?, v?
)Let d?
cost(R?)
+ cLet cost(R??{v})?
min(cost(R??
{v}), d)until argminR?A cost(h) = Vis one of the 21 classic problems shown to beNP-complete by Karp (1972).2 Finding the mini-mum feedback arc set in a tournament was shownto be NP-hard by Alon (2006) and Charbit et al(2007).
However, the specific instances exhibitedin the workshop data tend to have only a few cy-cles, so a relatively straightforward algorithm (for-malized above for completeness) solves them ex-actly without much difficulty.
The basic idea is toconstruct a dynamic program over the possible rank-ings.
Each item in the dynamic program representsa ranking of some subset of the vertices.
An itemis extended by choosing one of the unranked ver-tices and appending it to the hypothesis, adding toits cost the weights of all edges from the other un-ranked vertices to the newly appended vertex (the2Karp proved NP-completeness of the decision problem thatasks whether there is a feedback arc set of size k; NP-hardnessof the minimization problem follows.4Task name #sys #pairs Task name #sys #pairs2010 Czech-English 12 5375 2011 English-French individual 17 90862010 English-Czech 17 13538 2011 English-German syscomb 4 43742010 English-French 19 7962 2011 English-German individual 22 129962010 English-German 18 13694 2011 English-Spanish syscomb 4 59302010 English-Spanish 16 5174 2011 English-Spanish individual 15 111302010 French-English 24 8294 2011 French-English syscomb 6 30002010 German-English 25 10424 2011 French-English individual 18 69862010 Spanish-English 14 11307 2011 German-English syscomb 8 38442011 Czech-English syscomb 4 2602 2011 German-English individual 20 90792011 Czech-English individual 8 4922 2011 Spanish-English syscomb 6 41562011 English-Czech syscomb 2 2686 2011 Spanish-English individual 15 56522011 English-Czech individual 10 17875 2011 Urdu-English tunable metrics 8 62572011 English-French syscomb 2 880Table 1: The set of tasks we analyzed, including the number of participating systems (excluding the reference, #sys),and the number of implicit pairwise judgements collected (including the reference, #pairs).edges to be reversed).
This hypothesis space shouldbe familiar to most machine translation researcherssince it closely resembles the search space definedby a phrase-based translation model (Koehn, 2004).We use Dijkstra?s algorithm (1959) to explore it ef-ficiently; the complete algorithm is simply a gener-alization of the simple algorithm for acyclic tourna-ments described above.5 Experiments and AnalysisWe experimented with 25 relative ranking tasks pro-duced by WMT 2010 (Callison-Burch et al, 2010)and WMT 2011 (Callison-Burch et al, 2011); thefull set is shown in Table 1.
For each task we con-sidered four possible methods of ranking the data:sorting by any of Equation 1 through 3, and sort-ing consistent with reversal of a minimum feedbackarc set (MFAS).
To weight the edges for the latterapproach, we simply used the difference in num-ber of assessments preferring one system over theother; that is, an edge from A to B is weightedcount(A ?
B)?
count(A  B).
If this quantity isnegative, there is instead an edge from B to A. Thepurpose of this simple weighting is to ensure a so-lution that minimizes the number of disagreementswith all available evidence, counting each pairwisecomparison as equal.33This is not necessarily the best choice of weighting.
Forinstance, (Bojar et al, 2011) observe that human assessments ofWMT-OFFICIAL MFAS BOJAR(Eq 1) (Eq 3)ONLINE-B CU-MARECEK ONLINE-BCU-BOJAR ONLINE-B CU-BOJARCU-MARECEK CU-BOJAR CU-MARECEKCU-TAMCHYNA CU-TAMCHYNA CU-TAMCHYNAUEDIN CU-POPEL CU-POPELCU-POPEL UEDIN UEDINCOMMERCIAL2 COMMERCIAL1 COMMERCIAL2COMMERCIAL1 COMMERCIAL2 COMMERCIAL1JHU JHU JHUCU-ZEMAN CU-ZEMAN CU-ZEMAN38 0 69Table 2: Different rankings of the 2011 Czech-Englishtask.
Only the MFAS ranking is acyclic with respect topairwise judgements.
The final row indicates the weightof the voilated edges.An MFAS solution written in Python took only afew minutes to produce rankings for all 25 tasks on a2.13 GHz Intel Core 2 Duo processor, demonstratingthat it is completely feasible despite being theoreti-cally intractible.
One value of computing this solu-tion is that it enables us to answer several questions,shorter sentences tend to be more consistent with each other, soperhaps they should be weighted more highly.
Unfortunately,it is not clear how to evaluate alternative weighting schemes,since there is no ground truth for such meta-evaluations.5ONLINEB LIUM ?
ONLINEB 1 RWTH-COMBORWTH-COMBO UPV-COMBO ?
CAMBRIDGE 6 CMU-HYPOSEL-COMBOCMU-HYPOSEL-COMBO JHU ?
CAMBRIDGE 1 DCU-COMBOCAMBRIDGE LIMSI ?
UEDIN 1 ONLINEBLIUM LIMSI ?
CMU-HYPOSEL-COMBO 1 LIUMDCU-COMBO LIUM-COMBO ?
CAMBRIDGE 1 CMU-HEAFIELD-COMBOCMU-HEAFIELD-COMBO LIUM-COMBO ?
NRC 3 UPV-COMBOUPV-COMBO RALI ?
UEDIN 1 NRCNRC RALI ?
UPV-COMBO 4 CAMBRIDGEUEDIN RALI ?
JHU 1 UEDINJHU RALI ?
LIUM 3 JHU-COMBOLIMSI LIG ?
UEDIN 6 LIMSIJHU-COMBO BBN-COMBO ?
NRC 3 RALILIUM-COMBO BBN-COMBO ?
UEDIN 5 LIUM-COMBORALI BBN-COMBO ?
UPV-COMBO 5 BBN-COMBOLIG BBN-COMBO ?
JHU 4 JHUBBN-COMBO RWTH ?
UPV-COMBO 3 RWTHRWTH CMU-STATXFER ?
JHU 1 LIGCMU-STATXFER CMU-STATXFER ?
LIG 1 ONLINEAONLINEA ONLINEA ?
RWTH 1 CMU-STATXFERHUICONG ONLINEA ?
JHU 2 HUICONGDFKI HUICONG ?
LIG 3 DFKICU-ZEMAN DFKI ?
RWTH 3 GENEVAGENEVA DFKI ?
CMU-STATXFER 1 CU-ZEMANTable 3: 2010 French-English reranking with MFAS solver.
The left column shows the optimal ranking, while thecenter shows the pairwise rankings that are violated by this ranking, along with their edge weights.
The right columnshows the ranking under WMT-OFFICIAL (Eq.
1), originally published as two separate tables.both about the pairwise data itself, and the proposedheuristic ranking of Bojar et al (2011).5.1 Cycles in the Pairwise RankingsOur first experiment checks for cycles in the tourna-ments.
Only nine were acyclic, including all eightof the system combination tasks, each of which con-tained only a handful of systems.
The most inter-esting, however, is the 2011 English-Czech individ-ual task.
This task is notable because the heuristicrankings do not produce a ranking that is consistentwith all of the pairwise judgements, even though oneexists.
The three rankings are illustrated side-by-side in Table 2.
One obvious problem is that neitherheuristic score correctly identifies CU-MARECEK asthe best system, even though it wins pairwise com-parisons against all other systems (the WMT 2011proceedings do identify it as a winner, despite notplacing it in the highest rank).On the other hand, the most difficult task to dis-entangle is the 2010 French-English task (Table 3),which included 25 systems (individual and systemcombinations were evaluated as a group for this task,despite being reported in separate tables in officialresults).
Its optimal ranking with MFAS still vio-lates 61 pairwise ranking samples ?
there is sim-ply no sensible way to put these systems into a to-tal order.
On the other hand, the heuristic rankingsbased on Equations 1 through 3 violate even morecomparisons: 107, 108, and 118, respectively.
Onceagain we see a curious result in the top of the heuris-tic rankings, with system ONLINEB falling severalspots below the top position in the heurstic ranking,despite losing out only to LIUM by one vote.Our major concern, however, is that over half ofthe tasks included cycles of one form or another inthe tournaments.
This represents a strong inconsis-6tency in the data.5.2 Evaluation of Heuristic ScoresTaking the analysis above further, we find that thetotal number of violations of pairwise preferencesacross all tasks stands at 396 for the MFAS solution,and at 1140, 1215, 979 for Equations 1 through 3.This empirically validates the suggestion by Bojaret al (2011) to remove ties from both the numera-tor and denominator of the heuristic measure.
Onthe other hand, despite the intuitive arguments in itsfavor, the empirical evidence does not strongly fa-vor any of the heuristic measures, all of which aresubstantially worse than the MFAS solution.In fact, HEURISTIC 2 (Eq.
2) fails quite spec-tacularly in one case: on the ranking of the sys-tems produced by the tunable metrics task of WMT2011 (Figure 4).
Apart from producing a rankingvery inconsistent with the pairwise judgements, itachieves a Spearman?s rank correlation coefficentof 0.43 with the MFAS solution.
By comparison,WMT-OFFICIAL (Eq.
1) produces the best ranking,with a correlation of 0.93 with the MFAS solution.The two heuristic measures obtain an even lowercorrelation of 0.19 with each other.
This differencein the two rankings was noted in the WMT 2011report; however comparison with the MFAS rankersuggests that the published rankings according to theofficial metric are about as accurate as those basedon other heuristic metrics.6 DiscussionUnfortunately, reliably ranking translation systemsbased on human assessments appears to be a difficulttask, and it is unclear that WMT has succeeded yet.Some results presented here, such as the completeinability to obtain a sensible ordering on the 2010French-English task?or to produce an acyclic tour-nament on more than half the tasks?indicate thatfurther work is needed, and we feel that the pub-lished results of the human assessment should be re-garded with a healthy skepticism.
There are manypotential sources of uncertainty in the data:?
It is quite rare that one system is uniformly bet-ter than another.
Rather, one system will tendto perform better in aggregate across many sen-tences.
The number of sentences on which thisMFAS Ranking HEURISTIC 2 RankingCMU-BLEU CU-SEMPOS-BLEUCMU-BLEU-SINGLE NUS-TESLA-FCU-SEMPOS-BLEU CMU-BLEURWTH-CDER CMU-BLEU-SINGLECMU-METEOR STANFORD-DCPSTANFORD-DCP CMU-METEORNUS-TESLA-F RWTH-CDERSHEFFIELD-ROSE SHEFFIELD-ROSETable 4: Rankings of the WMT 2011 tunable metricstask.
MFAS finds a near-optimal solution, violating onlysix judgements with reversals of CMU-METEOR ?
CMU-BLEU and STANFORD-DCP ?
CMU-BLEU-SINGLE.
Incontrast, the HEURISTIC2 (Eq.
2) solution violates 103pairwise judgements.improvement can be reliably observed will varygreatly.
In many cases, it may be less than thenumber of samples.?
Individual assessors may be biased or mali-cious.?
The reliability of pairwise judgements varieswith sentence length, as noted by Bojar et al(2011).?
The pairwise judgements are not made directly,but inferred from a larger relative ranking.?
The pairwise judgements are not independent,since each sample consists of consecutive sen-tences from the same document.
It is likelythat some systems are systematically better orworse on particular documents.?
The pairwise judgements are not independent,since many of the assessments are intention-ally repeated to assess intra- and inter-annotatoragreement.?
Many of the systems will covary, since they areoften based on the same underlying techniquesand software.How much does any one or all of these factorsaffect the final ranking?
The technique describedabove does not even attempt to address this ques-tion.
Indeed, modeling this kind of data still ap-pears to be unsolved: a recent paper by Wauthier7and Jordan (2011) on modeling latent annotator biaspresents one of the first attempts at solving just oneof the above problems, let alne all of them.Simple hypothesis testing of the type reported inthe workshop results is simply inadequate to teaseapart the many interacting effects in this type ofdata and may lead to many unjustified conclusions.The tables in the Appendix of Callison-Burch et al(2011) report p-values of up to 1%, computed forevery pairwise comparison in the dataset.
However,there are over two thousand comparisons in this ap-pendix, so even at an error rate of 1% we would ex-pect more than twenty to be wrong.
Making mattersworse, many of the p-values are in fact much thanhigher than 1%.
It is quite reasonable to assumethat hundreds of the pairwise rankings inferred fromthese tables are incorrect, or at least meaningless.Methods for multiple hypothesis testing (Benjaminiand Hochberg, 1995) should be explored.In short, there is much work to be done.
This pa-per has raised more questions than it answered, butwe offer several recommendations.?
We recommend against using the metric pro-posed by Bojar et al (2011).
While their anal-ysis is very insightful, their proposed heuristicmetric is not substantially better than the met-ric used in the official rankings.
If anything, anMFAS-based ranking should be preferred sinceit can minimize discrepancies with the pairwiserankings, but as we have discussed, we believethis is far from a complete solution.?
Reconsider the use of total ordering, especiallyfor the evaluation of automatic metrics.
Asdemonstrated in this paper, there are many pos-sible ways to generate a total ordering, and thechoice of one may be arbitrary.
In some casesthere may not be enough evidence to support atotal ordering, or the evidence is contradictory,and committing to one may be a source of sub-stantial noise in the gold standard for evaluatingautomatic metrics.?
Consider a pilot study to clearly identify whichsources of uncertainty in the data affect therankings and devise methods to account for it,which may involve redesigning the data collec-tion protocol.
The current approach is designedto collect data for a variety of different goals,including intra- and inter-annotator agreement,pairwise coverage, and maximum throughput.However, some of goals are at cross-purposesin that they make it more difficult to make reli-able statistical inferences about any one aspectof the data.
Additional care should be takento minimize dependencies between the samplesused to produce the final ranking.?
Encourage further detailed analysis of the ex-isting datasets, perhaps through a shared task.The data that has been amassed so far throughWMT is the best available resource for mak-ing progress on solving the difficult problem ofproducing reliable and repeatable human rank-ings of machine translation systems.
However,this problem is not solved yet, and it will re-quire sustained effort to make that progress.AcknowledgementsThanks to Ondre?j Bojar, Philipp Koehn, and Mar-tin Popel for very helpful discussion related to thiswork, the anonymous reviewers for detailed andhelpful comments, and Chris Callison-Burch for en-couraging this investigation and for many explana-tions and additional data from the workshop.ReferencesN.
Alon.
2006.
Ranking tournaments.
SIAM Journal onDiscrete Mathematics, 20(1):137?142.Y.
Benjamini and Y. Hochberg.
1995.
Controlling thefalse discovery rate: a practical and powerful approachto multiple testing.
Journal of the Royal StatisticalSociety, 57:289?300.O.
Bojar, M.
Ercegovc?evic?, M. Popel, and O. F. Zaidan.2011.
A grain of salt for the WMT manual evaluation.In Proc.
of WMT.C.
Callison-Burch, P. Koehn, C. Monz, K. Peterson,M.
Przybocki, and O. Zaidan.
2010.
Findings of the2010 joint workshop on statistical machine translationand metrics for machine translation.
In Proc.
of WMT.C.
Callison-Burch, P. Koehn, C. Monz, and O. F. Zaidan.2011.
Findings of the 2011 workshop on statisticalmachine translation.
In Proc.
of WMT.P.
Charbit, S. Thomass, and A. Yeo.
2007.
The minimumfeedback arc set problem is NP-hard for tournaments.Combinatorics, Probability and Computing, 16.8E.
W. Dijkstra.
1959.
A note on two problems in connex-ion with graphs.
Numerische Mathematik, 1:269?271.R.
M. Karp.
1972.
Reducibility among combinatorialproblems.
In Symposium on the Complexity of Com-puter Computations.P.
Koehn.
2004.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.In Proc.
of AMTA.H.
G. Landau.
1951.
On dominance relations andthe structure of animal societies: I effect of inher-ent characteristics.
Bulletin of Mathematical Biology,13(1):1?19.F.
L. Wauthier and M. I. Jordan.
2011.
Bayesian biasmitigation for crowdsourcing.
In Proc.
of NIPS.9
