Proceedings of ACL-08: HLT, pages 452?460,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSolving Relational Similarity Problems Using the Web as a CorpusPreslav Nakov?EECS, CS divisionUniversity of California at BerkeleyBerkeley, CA 94720, USAnakov@cs.berkeley.eduMarti A. HearstSchool of InformationUniversity of California at BerkeleyBerkeley, CA 94720, USAhearst@ischool.berkeley.eduAbstractWe present a simple linguistically-motivatedmethod for characterizing the semantic rela-tions that hold between two nouns.
The ap-proach leverages the vast size of the Webin order to build lexically-specific features.The main idea is to look for verbs, preposi-tions, and coordinating conjunctions that canhelp make explicit the hidden relations be-tween the target nouns.
Using these fea-tures in instance-based classifiers, we demon-strate state-of-the-art results on various rela-tional similarity problems, including mappingnoun-modifier pairs to abstract relations likeTIME, LOCATION and CONTAINER, charac-terizing noun-noun compounds in terms of ab-stract linguistic predicates like CAUSE, USE,and FROM, classifying the relations betweennominals in context, and solving SAT verbalanalogy problems.
In essence, the approachputs together some existing ideas, showingthat they apply generally to various semantictasks, finding that verbs are especially usefulfeatures.1 IntroductionDespite the tremendous amount of work on wordsimilarity (see (Budanitsky and Hirst, 2006) for anoverview), there is surprisingly little research on theimportant related problem of relational similarity ?semantic similarity between pairs of words.
Stu-dents who took the SAT test before 2005 or who?After January 2008 at the Linguistic Modeling Depart-ment, Institute for Parallel Processing, Bulgarian Academy ofSciences, nakov@lml.bas.bgare taking the GRE test nowadays are familiar withan instance of this problem ?
verbal analogy ques-tions, which ask whether, e.g., the relationship be-tween ostrich and bird is more similar to that be-tween lion and cat, or rather between primate andmonkey.
These analogies are difficult, and the aver-age test taker gives a correct answer 57% of the time(Turney and Littman, 2005).Many NLP applications could benefit from solv-ing relational similarity problems, including butnot limited to question answering, information re-trieval, machine translation, word sense disambigua-tion, and information extraction.
For example, arelational search engine like TextRunner, whichserves queries like ?find all X such that X causeswrinkles?, asking for all entities that are in a par-ticular relation with a given entity (Cafarella et al,2006), needs to recognize that laugh wrinkles isan instance of CAUSE-EFFECT.
While there arenot many success stories so far, measuring seman-tic similarity has proven its advantages for textualentailment (Tatu and Moldovan, 2005).In this paper, we introduce a novel linguistically-motivated Web-based approach to relational simi-larity, which, despite its simplicity, achieves state-of-the-art performance on a number of problems.Following Turney (2006b), we test our approachon SAT verbal analogy questions and on mappingnoun-modifier pairs to abstract relations like TIME,LOCATION and CONTAINER.
We further apply itto (1) characterizing noun-noun compounds usingabstract linguistic predicates like CAUSE, USE, andFROM, and (2) classifying the relation between pairsof nominals in context.4522 Related Work2.1 Characterizing Semantic RelationsTurney and Littman (2005) characterize the relation-ship between two words as a vector with coordinatescorresponding to the Web frequencies of 128 fixedphrases like ?X for Y ?
and ?Y for X?
instantiatedfrom a fixed set of 64 joining terms like for, suchas, not the, is *, etc.
These vectors are used in anearest-neighbor classifier to solve SAT verbal anal-ogy problems, yielding 47% accuracy.
The same ap-proach is applied to classifying noun-modifier pairs:using the Diverse dataset of Nastase and Szpakow-icz (2003), Turney&Littman achieve F-measures of26.5% with 30 fine-grained relations, and 43.2%with 5 course-grained relations.Turney (2005) extends the above approach by in-troducing the latent relational analysis (LRA), whichuses automatically generated synonyms, learns suit-able patterns, and performs singular value decom-position in order to smooth the frequencies.
The fullalgorithm consists of 12 steps described in detail in(Turney, 2006b).
When applied to SAT questions,it achieves the state-of-the-art accuracy of 56%.
Onthe Diverse dataset, it yields an F-measure of 39.8%with 30 classes, and 58% with 5 classes.Turney (2006a) presents an unsupervised algo-rithm for mining the Web for patterns expressingimplicit semantic relations.
For example, CAUSE(e.g., cold virus) is best characterized by ?Y * causesX?, and ?Y in * early X?
is the best pattern forTEMPORAL (e.g., morning frost).
With 5 classes,he achieves F-measure=50.2%.2.2 Noun-Noun Compound SemanticsLauer (1995) reduces the problem of noun com-pound interpretation to choosing the best paraphras-ing preposition from the following set: of, for, in,at, on, from, with or about.
He achieved 40% accu-racy using corpus frequencies.
This result was im-proved to 55.7% by Lapata and Keller (2005) whoused Web-derived n-gram frequencies.Barker and Szpakowicz (1998) use syntactic cluesand the identity of the nouns in a nearest-neighborclassifier, achieving 60-70% accuracy.Rosario and Hearst (2001) used a discriminativeclassifier to assign 18 relations for noun compoundsfrom biomedical text, achieving 60% accuracy.Rosario et al (2002) reported 90% accuracy witha ?descent of hierarchy?
approach which character-izes the relationship between the nouns in a bio-science noun-noun compound based on the MeSHcategories the nouns belong to.Girju et al (2005) apply both classic (SVM anddecision trees) and novel supervised models (seman-tic scattering and iterative semantic specialization),using WordNet, word sense disambiguation, and aset of linguistic features.
They test their systemagainst both Lauer?s 8 prepositional paraphrases andanother set of 21 semantic relations, achieving up to54% accuracy on the latter.In a previous work (Nakov and Hearst, 2006), wehave shown that the relationship between the nounsin a noun-noun compound can be characterized us-ing verbs extracted from the Web, but we providedno formal evaluation.Kim and Baldwin (2006) characterized the se-mantic relationship in a noun-noun compound us-ing the verbs connecting the two nouns by compar-ing them to predefined seed verbs.
Their approachis highly resource intensive (uses WordNet, CoreLexand Moby?s thesaurus), and is quite sensitive to theseed set of verbs: on a collection of 453 examplesand 19 relations, they achieved 52.6% accuracy with84 seed verbs, but only 46.7% with 57 seed verbs.2.3 Paraphrase AcquisitionOur method of extraction of paraphrasing verbs andprepositions is similar to previous paraphrase ac-quisition approaches.
Lin and Pantel (2001) ex-tract paraphrases from dependency tree paths whoseends contain semantically similar sets of words bygeneralizing over these ends.
For example, given?X solves Y?, they extract paraphrases like ?X findsa solution to Y?, ?X tries to solve Y?, ?X resolvesY?, ?Y is resolved by X?, etc.
The approach is ex-tended by Shinyama et al (2002), who use namedentity recognizers and look for anchors belong-ing to matching semantic classes, e.g., LOCATION,ORGANIZATION.
The idea is further extended byNakov et al (2004), who apply it in the biomedicaldomain, imposing the additional restriction that thesentences from which the paraphrases are extractedcite the same target paper.4532.4 Word SimilarityAnother important group of related work is on us-ing syntactic dependency features in a vector-spacemodel for measuring word similarity, e.g., (Alshawiand Carter, 1994), (Grishman and Sterling, 1994),(Ruge, 1992), and (Lin, 1998).
For example, given anoun, Lin (1998) extracts verbs that have that nounas a subject or object, and adjectives that modify it.3 MethodGiven a pair of nouns, we try to characterize thesemantic relation between them by leveraging thevast size of the Web to build linguistically-motivatedlexically-specific features.
We mine the Web forsentences containing the target nouns, and we ex-tract the connecting verbs, prepositions, and coordi-nating conjunctions, which we use in a vector-spacemodel to measure relational similarity.The process of extraction starts with exact phrasequeries issued against a Web search engine (Google)using the following patterns:?infl1 THAT * infl2?
?infl2 THAT * infl1?
?infl1 * infl2?
?infl2 * infl1?where: infl1 and infl2 are inflected variants ofnoun1 and noun2 generated using the Java Word-Net Library1; THAT is a complementizer and can bethat, which, or who; and * stands for 0 or more (upto 8) instances of Google?s star operator.The first two patterns are subsumed by the lasttwo and are used to obtain more sentences from thesearch engine since including e.g.
that in the querychanges the set of returned results and their ranking.For each query, we collect the text snippets fromthe result set (up to 1,000 per query).
We split theminto sentences, and we filter out all incomplete onesand those that do not contain the target nouns.
Wefurther make sure that the word sequence follow-ing the second mentioned target noun is nonemptyand contains at least one nonnoun, thus ensuringthe snippet includes the entire noun phrase: snippetsrepresenting incomplete sentences often end with aperiod anyway.
We then perform POS tagging us-ing the Stanford POS tagger (Toutanova et al, 2003)1JWNL: http://jwordnet.sourceforge.netFreq.
Feature POS Direction2205 of P 2 ?
11923 be V 1 ?
2771 include V 1 ?
2382 serve on V 2 ?
1189 chair V 2 ?
1189 have V 1 ?
2169 consist of V 1 ?
2148 comprise V 1 ?
2106 sit on V 2 ?
181 be chaired by V 1 ?
278 appoint V 1 ?
277 on P 2 ?
166 and C 1 ?
266 be elected V 1 ?
258 replace V 1 ?
248 lead V 2 ?
147 be intended for V 1 ?
245 join V 2 ?
1. .
.
.
.
.
.
.
.
.
.
.4 be signed up for V 2 ?
1Table 1: The most frequent Web-derived features forcommittee member.
Here V stands for verb (possibly+preposition and/or +particle), P for preposition and Cfor coordinating conjunction; 1 ?
2 means committeeprecedes the feature and member follows it; 2 ?
1meansmember precedes the feature and committee follows it.and shallow parsing with the OpenNLP tools2, andwe extract the following types of features:Verb: We extract a verb if the subject NP of thatverb is headed by one of the target nouns (or an in-flected form), and its direct object NP is headed bythe other target noun (or an inflected form).
For ex-ample, the verb include will be extracted from ?Thecommittee includes many members.?
We also ex-tract verbs from relative clauses, e.g., ?This is a com-mittee which includes many members.?
Verb parti-cles are also recognized, e.g., ?The committee mustrotate off 1/3 of its members.?
We ignore modalsand auxiliaries, but retain the passive be.
Finally, welemmatize the main verb using WordNet?s morpho-logical analyzer Morphy (Fellbaum, 1998).Verb+Preposition: If the subject NP of a verb isheaded by one of the target nouns (or an inflectedform), and its indirect object is a PP containing anNP which is headed by the other target noun (or aninflected form), we extract the verb and the preposi-2OpenNLP: http://opennlp.sourceforge.net454tion heading that PP, e.g., ?The thesis advisory com-mittee consists of three qualified members.?
As inthe verb case, we extract verb+preposition from rel-ative clauses, we include particles, we ignore modalsand auxiliaries, and we lemmatize the verbs.Preposition: If one of the target nouns is the headof an NP containing a PP with an internal NP headedby the other target noun (or an inflected form), weextract the preposition heading that PP, e.g., ?Themembers of the committee held a meeting.
?Coordinating conjunction: If the two targetnouns are the heads of coordinated NPs, we extractthe coordinating conjunction.In addition to the lexical part, for each extractedfeature, we keep a direction.
Therefore the preposi-tion of represents two different features in the fol-lowing examples ?member of the committee?
and?committee of members?.
See Table 1 for examples.We use the above-described features to calculaterelational similarity, i.e., similarity between pairs ofnouns.
In order to downweight very common fea-tures like of, we use TF.IDF-weighting:w(x) = TF (x)?
log(NDF (x))(1)In the above formula, TF (x) is the number oftimes the feature x has been extracted for the tar-get noun pair, DF (x) is the total number of trainingnoun pairs that have that feature, and N is the totalnumber of training noun pairs.Given two nouns and their TF.IDF-weighted fre-quency vectors A and B, we calculate the similaritybetween them using the following generalized vari-ant of the Dice coefficient:Dice(A,B) =2?
?ni=1 min(ai, bi)?ni=1 ai +?ni=1 bi(2)Other variants are also possible, e.g., Lin (1998).4 Relational Similarity Experiments4.1 SAT Verbal AnalogyFollowing Turney (2006b), we use SAT verbal anal-ogy as a benchmark problem for relational similar-ity.
We experiment with the 374 SAT questionscollected by Turney and Littman (2005).
Table 2shows two sample questions: the top word pairsostrich:bird palatable:toothsome(a) lion:cat (a) rancid:fragrant(b) goose:flock (b) chewy:textured(c) ewe:sheep (c) coarse:rough(d) cub:bear (d) solitude:company(e) primate:monkey (e) no choiceTable 2: SAT verbal analogy: sample questions.
Thestem is in bold, the correct answer is in italic, and thedistractors are in plain text.are called stems, the ones in italic are the solu-tions, and the remaining ones are distractors.
Tur-ney (2006b) achieves 56% accuracy on this dataset,which matches the average human performance of57%, and represents a significant improvement overthe 20% random-guessing baseline.Note that the righthand side example in Table2 is missing one distractor; so do 21 questions.The dataset alo mixes different parts of speech:while solitude and company are nouns, all remainingwords are adjectives.
Other examples contain verbsand adverbs, and even relate pairs of different POS.This is problematic for our approach, which requiresthat both words be nouns3.
After having filtered allexamples containing nonnouns, we ended up with184 questions, which we used in the evaluation.Given a verbal analogy example, we build six fea-ture vectors ?
one for each of the six word pairs.
Wethen calculate the relational similarity between thestem of the analogy and each of the five candidates,and we choose the pair with the highest score; wemake no prediction in case of a tie.The evaluation results for a leave-one-out cross-validation are shown in Table 3.
We also show 95%-confidence intervals for the accuracy.
The last linein the table shows the performance of Turney?s LRAwhen limited to the 184 noun-only examples.
Ourbest model v + p + c performs a bit better, 71.3%vs.
67.4%, but the difference is not statistically sig-nificant.
However, this ?inferred?
accuracy could bemisleading, and the LRA could have performed bet-ter if it was restricted to solve noun-only analogies,which seem easier than the general ones, as demon-strated by the significant increase in accuracy forLRA when limited to nouns: 67.4% vs. 56%.3It can be extended to handle adjective-noun pairs as well,as demonstrated in section 4.2 below.455Model X ?
?
Accuracy Cover.v + p + c 129 52 3 71.3?7.0 98.4v 122 56 6 68.5?7.2 96.7v + p 119 61 4 66.1?7.2 97.8v + c 117 62 5 65.4?7.2 97.3p + c 90 90 4 50.0?7.2 97.8p 84 94 6 47.2?7.2 96.7baseline 37 147 0 20.0?5.2 100.0LRA 122 59 3 67.4?7.1 98.4Table 3: SAT verbal analogy: 184 noun-only examples.v stands for verb, p for preposition, and c for coordinatingconjunction.
For each model, the number of correct (X),wrong (?
), and nonclassified examples (?)
is shown, fol-lowed by accuracy and coverage (in %s).Model X ?
?
Accuracy Cover.v + p 240 352 8 40.5?3.9 98.7v + p + c 238 354 8 40.2?3.9 98.7v 234 350 16 40.1?3.9 97.3v + c 230 362 8 38.9?3.8 98.7p + c 114 471 15 19.5?3.0 97.5p 110 475 15 19.1?3.0 97.5baseline 49 551 0 8.2?1.9 100.0LRA 239 361 0 39.8?3.8 100.0Table 4: Head-modifier relations, 30 classes: evaluationon the Diverse dataset, micro-averaged (in %s).4.2 Head-Modifier RelationsNext, we experiment with the Diverse dataset ofBarker and Szpakowicz (1998), which consists of600 head-modifier pairs: noun-noun, adjective-nounand adverb-noun.
Each example is annotated withone of 30 fine-grained relations, which are fur-ther grouped into the following 5 coarse-grainedclasses (the fine-grained relations are shown inparentheses): CAUSALITY (cause, effect, purpose,detraction), TEMPORALITY (frequency, time at,time through), SPATIAL (direction, location, lo-cation at, location from), PARTICIPANT (agent,beneficiary, instrument, object, object property,part, possessor, property, product, source, stative,whole) and QUALITY (container, content, equa-tive, material, measure, topic, type).
For example,exam anxiety is classified as effect and therefore asCAUSALITY, and blue book is property and there-fore also PARTICIPANT.Some examples in the dataset are problematic forour method.
First, in three cases, there are two mod-ifiers, e.g., infectious disease agent, and we had toignore the first one.
Second, seven examples havean adverb modifier, e.g., daily exercise, and 262 ex-amples have an adjective modifier, e.g., tiny cloud.We treat them as if the modifier was a noun, whichworks in many cases, since many adjectives and ad-verbs can be used predicatively, e.g., ?This exerciseis performed daily.?
or ?This cloud looks very tiny.
?For the evaluation, we created a feature vector foreach head-modifier pair, and we performed a leave-one-out cross-validation: we left one example fortesting and we trained on the remaining 599 ones,repeating this procedure 600 times so that each ex-ample be used for testing.
Following Turney andLittman (2005) we used a 1-nearest-neighbor classi-fier.
We calculated the similarity between the featurevector of the testing example and each of the train-ing examples?
vectors.
If there was a unique mostsimilar training example, we predicted its class, andif there were ties, we chose the class predicted by themajority of tied examples, if there was a majority.The results for the 30-class Diverse dataset areshown in Table 4.
Our best model achieves 40.5%accuracy, which is slightly better than LRA?s 39.8%,but the difference is not statistically significant.Table 4 shows that the verbs are the most impor-tant features, yielding about 40% accuracy regard-less of whether used alone or in combination withprepositions and/or coordinating conjunctions; notusing them results in 50% drop in accuracy.The reason coordinating conjunctions do not helpis that head-modifier relations are typically ex-pressed with verbal or prepositional paraphrases.Therefore, coordinating conjunctions only help withsome infrequent relations like equative, e.g., findingplayer and coach on the Web suggests an equativerelation for player coach (and for coach player).As Table 3 shows, this is different for SAT ver-bal analogy, where verbs are still the most importantfeature type and the only whose presence/absencemakes a statistical difference.
However, this timecoordinating conjunctions (with prepositions) dohelp a bit (the difference is not statistically signifi-cant) since SAT verbal analogy questions ask for abroader range of relations, e.g., antonymy, for whichcoordinating conjunctions like but are helpful.456Model Accuracyv + p + c + sent + query (type C) 68.1?4.0v 67.9?4.0v + p + c 67.8?4.0v + p + c + sent (type A) 67.3?4.0v + p 66.9?4.0sent (sentence words only) 59.3?4.2p 58.4?4.2Baseline (majority class) 57.0?4.2v + p + c + sent + query (C), 8 stars 67.0?4.0v + p + c + sent (A), 8 stars 65.4?4.1Best type C on SemEval 67.0?4.0Best type A on SemEval 66.0?4.1Table 5: Relations between nominals: evaluation on theSemEval dataset.
Accuracy is macro-averaged (in %s),up to 10 Google stars are used unless otherwise stated.4.3 Relations Between NominalsWe further experimented with the SemEval?07 task4 dataset (Girju et al, 2007), where each exampleconsists of a sentence, a target semantic relation, twonominals to be judged on whether they are in that re-lation, manually annotated WordNet senses, and theWeb query used to obtain the sentence:"Among the contents of the<e1>vessel</e1> were a set ofcarpenter?s <e2>tools</e2>, severallarge storage jars, ceramic utensils,ropes and remnants of food, as wellas a heavy load of ballast stones.
"WordNet(e1) = "vessel%1:06:00::",WordNet(e2) = "tool%1:06:00::",Content-Container(e2, e1) = "true",Query = "contents of the * were a"The following nonexhaustive and possibly over-lapping relations are possible: Cause-Effect(e.g., hormone-growth), Instrument-Agency(e.g., laser-printer), Theme-Tool (e.g., work-force), Origin-Entity (e.g., grain-alcohol),Content-Container (e.g., bananas-basket),Product-Producer (e.g., honey-bee), andPart-Whole (e.g., leg-table).
Each relation isconsidered in isolation; there are 140 training and atleast 70 test examples per relation.Given an example, we reduced the target entitiese1 and e2 to single nouns by retaining their headsonly.
We then mined the Web for sentences con-taining these nouns, and we extracted the above-described feature types: verbs, prepositions and co-ordinating conjunctions.
We further used the follow-ing problem-specific contextual feature types:Sentence words: after stop words removal andstemming with the Porter (1980) stemmer;Entity words: lemmata of the words in e1 and e2;Query words: words part of the query string.Each feature type has a specific prefix which pre-vents it from mixing with other feature types; thelast feature type is used for type C only (see below).The SemEval competition defines four types ofsystems, depending on whether the manually anno-tatedWordNet senses and theGoogle query are used:A (WordNet=no, Query=no), B (WordNet=yes,Query=no), C (WordNet=no, Query=yes), and D(WordNet=yes, Query=yes).
We experimented withtypes A and C only since we believe that having themanually annotated WordNet sense keys is an unre-alistic assumption for a real-world application.As before, we used a 1-nearest-neighbor classifierwith TF.IDF-weighting, breaking ties by predictingthe majority class on the training data.
The evalu-ation results are shown in Table 5.
We studied theeffect of different subsets of features and of moreGoogle star operators.
As the table shows, usingup to ten Google stars instead of up to eight (seesection 3) yields a slight improvement in accuracyfor systems of both type A (65.4% vs. 67.3%) andtype C (67.0% vs. 68.1%).
Both results representa statistically significant improvement over the ma-jority class baseline and over using sentence wordsonly, and a slight improvement over the best type Aand type C systems on SemEval?07, which achieved66% and 67% accuracy, respectively.44.4 Noun-Noun Compound RelationsThe last dataset we experimented with is a subsetof the 387 examples listed in the appendix of (Levi,1978).
Levi?s theory is one of the most impor-tant linguistic theories of the syntax and semanticsof complex nominals ?
a general concept grouping4The best type B system on SemEval achieved 76.3% ac-curacy using the manually-annotated WordNet senses in contextfor each example, which constitutes an additional data source,as opposed to an additional resource.
The systems that usedWordNet as a resource only, i.e., ignoring the manually anno-tated senses, were classified as type A or C. (Girju et al, 2007)457USING THAT NOT USING THATModel Accuracy Cover.
ANF ASF Accuracy Cover.
ANF ASFHuman: all v 78.4?6.0 99.5 34.3 70.9 ?
?
?Human: first v from each worker 72.3?6.4 99.5 11.6 25.5 ?
?
?
?v + p + c 50.0?6.7 99.1 216.6 1716.0 49.1?6.7 99.1 206.6 1647.6v + p 50.0?6.7 99.1 208.9 1427.9 47.6?6.6 99.1 198.9 1359.5v + c 46.7?6.6 99.1 187.8 1107.2 43.9?6.5 99.1 177.8 1038.8v 45.8?6.6 99.1 180.0 819.1 42.9?6.5 99.1 170.0 750.7p 33.0?6.0 99.1 28.9 608.8 33.0?6.0 99.1 28.9 608.8p + c 32.1?5.9 99.1 36.6 896.9 32.1?5.9 99.1 36.6 896.9Baseline 19.6?4.8 100.0 ?
?
?
?
?
?Table 6: Noun-noun compound relations, 12 classes: evaluation on Levi-214 dataset.
Shown are micro-averagedaccuracy and coverage in %s, followed by average number of features (ANF) and average sum of feature frequencies(ASF) per example.
The righthand side reports the results when the query patterns involving THAT were not used.
Forcomparison purposes, the top rows show the performance with the human-proposed verbs used as features.together the partially overlapping classes of nom-inal compounds (e.g., peanut butter), nominaliza-tions (e.g., dream analysis), and nonpredicate nounphrases (e.g., electric shock).In Levi?s theory, complex nominals can be derivedfrom relative clauses by removing one of the fol-lowing 12 abstract predicates: CAUSE1 (e.g., teargas), CAUSE2 (e.g., drug deaths), HAVE1 (e.g., ap-ple cake), HAVE2 (e.g., lemon peel), MAKE1 (e.g.,silkworm), MAKE2 (e.g., snowball), USE (e.g., steamiron), BE (e.g., soldier ant), IN (e.g., field mouse),FOR (e.g., horse doctor), FROM (e.g., olive oil), andABOUT (e.g., price war).
In the resulting nominals,the modifier is typically the object of the predicate;when it is the subject, the predicate is marked withthe index 2.
The second derivational mechanism inthe theory is nominalization; it produces nominalswhose head is a nominalized verb.Since we are interested in noun compounds only,we manually cleansed the set of 387 examples.
Wefirst excluded all concatenations (e.g., silkworm) andexamples with adjectival modifiers (e.g., electricshock), thus obtaining 250 noun-noun compounds(Levi-250 dataset).
We further filtered out all nom-inalizations for which the dataset provides no ab-stract predicate (e.g., city planner), thus ending upwith 214 examples (Levi-214 dataset).As in the previous experiments, for each of the214 noun-noun compounds, we mined the Webfor sentences containing both target nouns, fromwhich we extracted paraphrasing verbs, prepositionsand coordinating conjunctions.
We then performedleave-one-out cross-validation experiments with a1-nearest-neighbor classifier, trying to predict thecorrect predicate for the testing example.
The re-sults are shown in Table 6.
As we can see, us-ing prepositions alone yields about 33% accuracy,which is a statistically significant improvement overthe majority-class baseline.
Overall, the most impor-tant features are the verbs: they yield 45.8% accu-racy when used alone, and 50% together with prepo-sitions.
Adding coordinating conjunctions helps abit with verbs, but not with prepositions.
Note how-ever that none of the differences between the differ-ent feature combinations involving verbs are statis-tically significant.The righthand side of the table reports the resultswhen the query patterns involving THAT (see section3) were not used.
We can observe a small 1-3% dropin accuracy for all models involving verbs, but it isnot statistically significant.We also show the average number of distinct fea-tures and sum of feature counts per example: as wecan see, there is a strong positive correlation be-tween number of features and accuracy.5 Comparison to Human JudgmentsSince in all above tasks the most important fea-tures were the verbs, we decided to compare ourWeb-derived verbs to human-proposed ones for allnoun-noun compounds in the Levi-250 dataset.
Weasked human subjects to produce verbs, possibly458followed by prepositions, that could be used in aparaphrase involving that.
For example, olive oilcan be paraphrased as ?oil that comes from olives?,?oil that is obtained from olives?
or ?oil that is fromolives?.
Note that this implicitly allows for prepo-sitional paraphrases ?
when the verb is to be and isfollowed by a preposition, as in the last paraphrase.We used the Amazon Mechanical Turk Web ser-vice5 to recruit human subjects, and we instructedthem to propose at least three paraphrasing verbsper noun-noun compound, if possible.
We randomlydistributed the noun-noun compounds into groups of5 and we requested 25 different human subjects pergroup.
Each human subject was allowed to workon any number of groups, but not on the same onetwice.
A total of 174 different human subjects pro-duced 19,018 verbs.
After filtering the bad submis-sions and normalizing the verbs, we ended up with17,821 verbs.
See (Nakov, 2007) for further de-tails on the process of extraction and cleansing.
Thedataset itself is freely available (Nakov, 2008).We compared the human-proposed and the Web-derived verbs for Levi-214, aggregated by relation.Given a relation, we collected all verbs belong-ing to noun-noun compounds from that relation to-gether with their frequencies.
From a vector-spacemodel point of view, we summed their correspond-ing frequency vectors.
We did this separately forthe human- and the program-generated verbs, andwe compared the resulting vectors using Dice co-efficient with TF.IDF, calculated as before.
Figure1 shows the cosine correlations using all human-proposed verbs and the first verb from each judge.We can see a very-high correlation (mid-70% tomid-90%) for relations like CAUSE1, MAKE1, BE,but low correlations of 11-30% for reverse relationslike HAVE2 and MAKE2.
Interestingly, using the firstverb only improves the results for highly-correlatedrelations, but negatively affects low-correlated ones.Finally, we repeated the cross-validation exper-iment with the Levi-214 dataset, this time usingthe human-proposed verbs6 as features.
As Table6 shows, we achieved 78.4% accuracy using allverbs (and and 72.3% with the first verb from eachworker), which is a statistically significant improve-5http://www.mturk.com6Note that the human subjects proposed their verbs withoutany context and independently of our Web-derived sentences.Figure 1: Cosine correlation (in %s) between thehuman- and the program- generated verbs by rela-tion: using all human-proposed verbs vs. the first verb.ment over the 50% of our best Web-based model.This result is strong for a 12-way classification prob-lem, and confirms our observation that verbs andprepositions are among the most important featuresfor relational similarity problems.
It further suggeststhat the human-proposed verbs might be an upperbound on the accuracy that could be achieved withautomatically extracted features.6 Conclusions and Future WorkWe have presented a simple approach for character-izing the relation between a pair of nouns in termsof linguistically-motivated features which could beuseful for many NLP tasks.
We found that verbswere especially useful features for this task.
An im-portant advantage of the approach is that it does notrequire knowledge about the semantics of the indi-vidual nouns.
A potential drawback is that it mightnot work well for low-frequency words.The evaluation on several relational similarityproblems, including SAT verbal analogy, head-modifier relations, and relations between complexnominals has shown state-of-the-art performance.The presented approach can be further extended toother combinations of parts of speech: not just noun-noun and adjective-noun.
Using a parser with aricher set of syntactic dependency features, e.g., asproposed by Pado?
and Lapata (2007), is anotherpromising direction for future work.AcknowledgmentsThis research was supported in part by NSF DBI-0317510.459ReferencesHiyan Alshawi and David Carter.
1994.
Trainingand scaling preference functions for disambiguation.Computational Linguistics, 20(4):635?648.Ken Barker and Stan Szpakowicz.
1998.
Semi-automaticrecognition of noun modifier relationships.
In Proc.
ofComputational linguistics, pages 96?102.Alexander Budanitsky and Graeme Hirst.
2006.
Evalu-ating wordnet-based measures of lexical semantic re-latedness.
Computational Linguistics, 32(1):13?47.Michael Cafarella, Michele Banko, and Oren Etzioni.2006.
Relational Web search.
Technical Report 2006-04-02, University of Washington, Department of Com-puter Science and Engineering.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Roxana Girju, Dan Moldovan, Marta Tatu, and DanielAntohe.
2005.
On the semantics of noun compounds.Journal of Computer Speech and Language - SpecialIssue on Multiword Expressions, 4(19):479?496.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.Semeval-2007 task 04: Classification of semantic rela-tions between nominals.
In Proceedings of SemEval,pages 13?18, Prague, Czech Republic.Ralph Grishman and John Sterling.
1994.
Generalizingautomatically generated selectional patterns.
In Pro-ceedings of the 15th conference on Computational lin-guistics, pages 742?747.Su Nam Kim and Timothy Baldwin.
2006.
Interpret-ing semantic relations in noun compounds via verb se-mantics.
In Proceedings of the COLING/ACL on Mainconference poster sessions, pages 491?498.Mirella Lapata and Frank Keller.
2005.
Web-basedmodels for natural language processing.
ACM Trans.Speech Lang.
Process., 2(1):3.Mark Lauer.
1995.
Designing Statistical LanguageLearners: Experiments on Noun Compounds.
Ph.D.thesis, Dept.
of Computing, Macquarie University,Australia.Judith Levi.
1978.
The Syntax and Semantics of ComplexNominals.
Academic Press, New York.Dekang Lin and Patrick Pantel.
2001.
Discovery of infer-ence rules for question-answering.
Natural LanguageEngineering, 7(4):343?360.Dekang Lin.
1998.
An information-theoretic definitionof similarity.
In Proceedings of ICML, pages 296?304.Preslav Nakov and Marti Hearst.
2006.
Using verbs tocharacterize noun-noun relations.
In AIMSA, volume4183 of LNCS, pages 233?244.
Springer.Preslav Nakov, Ariel Schwartz, and Marti Hearst.
2004.Citances: Citation sentences for semantic analysis ofbioscience text.
In Proceedings of SIGIR?04Workshopon Search and Discovery in Bioinformatics, pages 81?88, Sheffield, UK.Preslav Nakov.
2007.
Using the Web as an ImplicitTraining Set: Application to Noun Compound Syntaxand Semantics.
Ph.D. thesis, EECS Department, Uni-versity of California, Berkeley, UCB/EECS-2007-173.Preslav Nakov.
2008.
Paraphrasing verbs for noun com-pound interpretation.
In Proceedings of the LREC?08Workshop: Towards a Shared Task for Multiword Ex-pressions (MWE?08), Marrakech, Morocco.Vivi Nastase and Stan Szpakowicz.
2003.
Exploringnoun-modifier semantic relations.
In Fifth Interna-tional Workshop on Computational Semantics (IWCS-5), pages 285?301, Tilburg, The Netherlands.Sebastian Pado?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Martin Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Barbara Rosario and Marti Hearst.
2001.
Classifying thesemantic relations in noun compounds via a domain-specific lexical hierarchy.
In Proceedings of EMNLP,pages 82?90.Barbara Rosario, Marti Hearst, and Charles Fillmore.2002.
The descent of hierarchy, and selection in rela-tional semantics.
In Proceedings of ACL, pages 247?254.Gerda Ruge.
1992.
Experiment on linguistically-basedterm associations.
Inf.
Process.
Manage., 28(3):317?332.Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.2002.
Automatic paraphrase acquisition from news ar-ticles.
In Proceedings of HLT, pages 313?318.Marta Tatu and Dan Moldovan.
2005.
A semantic ap-proach to recognizing textual entailment.
In Proceed-ings of HLT, pages 371?378.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of HLT-NAACL, pages 252?259.Peter Turney and Michael Littman.
2005.
Corpus-basedlearning of analogies and semantic relations.
MachineLearning Journal, 60(1-3):251?278.Peter Turney.
2005.
Measuring semantic similarity bylatent relational analysis.
In Proceedings of IJCAI,pages 1136?1141.Peter Turney.
2006a.
Expressing implicit semantic re-lations without supervision.
In Proceedings of ACL,pages 313?320.Peter Turney.
2006b.
Similarity of semantic relations.Computational Linguistics, 32(3):379?416.460
