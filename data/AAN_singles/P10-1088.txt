Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 854?864,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsBucking the Trend: Large-Scale Cost-Focused Active Learning forStatistical Machine TranslationMichael BloodgoodHuman Language TechnologyCenter of ExcellenceJohns Hopkins UniversityBaltimore, MD 21211bloodgood@jhu.eduChris Callison-BurchCenter for Language andSpeech ProcessingJohns Hopkins UniversityBaltimore, MD 21211ccb@cs.jhu.eduAbstractWe explore how to improve machine trans-lation systems by adding more translationdata in situations where we already havesubstantial resources.
The main challengeis how to buck the trend of diminishing re-turns that is commonly encountered.
Wepresent an active learning-style data solic-itation algorithm to meet this challenge.We test it, gathering annotations via Ama-zon Mechanical Turk, and find that we getan order of magnitude increase in perfor-mance rates of improvement.1 IntroductionFigure 1 shows the learning curves for two state ofthe art statistical machine translation (SMT) sys-tems for Urdu-English translation.
Observe howthe learning curves rise rapidly at first but then atrend of diminishing returns occurs: put simply,the curves flatten.This paper investigates whether we can buck thetrend of diminishing returns, and if so, how we cando it effectively.
Active learning (AL) has been ap-plied to SMT recently (Haffari et al, 2009; Haffariand Sarkar, 2009) but they were interested in start-ing with a tiny seed set of data, and they stoppedtheir investigations after only adding a relativelytiny amount of data as depicted in Figure 1.In contrast, we are interested in applying ALwhen a large amount of data already exists as isthe case for many important lanuage pairs.
We de-velop an AL algorithm that focuses on keeping an-notation costs (measured by time in seconds) low.It succeeds in doing this by only soliciting trans-lations for parts of sentences.
We show that thisgets a savings in human annotation time above andbeyond what the reduction in # words annotatedwould have indicated by a factor of about threeand speculate as to why.0 2 4 6 8 10x 104051015202530Number of Sentences in Training DataBLEUScoreJSyntax and JHier Learning Curves on the LDC Urdu?English Language Pack (BLEU vs Sentences)jHierjSyntaxas far as previous AL for SMT research studies were conductedwhere we begin our main investigations into bucking the trend of diminishing returnsFigure 1: Syntax-based and Hierarchical Phrase-Based MT systems?
learning curves on the LDCUrdu-English language pack.
The x-axis measuresthe number of sentence pairs in the training data.The y-axis measures BLEU score.
Note the di-minishing returns as more data is added.
Alsonote how relatively early on in the process pre-vious studies were terminated.
In contrast, thefocus of our main experiments doesn?t even be-gin until much higher performance has alreadybeen achieved with a period of diminishing returnsfirmly established.We conduct experiments for Urdu-Englishtranslation, gathering annotations via AmazonMechanical Turk (MTurk) and show that we canindeed buck the trend of diminishing returns,achieving an order of magnitude increase in therate of improvement in performance.Section 2 discusses related work; Section 3discusses preliminary experiments that show theguiding principles behind the algorithm we use;Section 4 explains our method for soliciting newtranslation data; Section 5 presents our main re-sults; and Section 6 concludes.8542 Related WorkActive learning has been shown to be effectivefor improving NLP systems and reducing anno-tation burdens for a number of NLP tasks (see,e.g., (Hwa, 2000; Sassano, 2002; Bloodgoodand Vijay-Shanker, 2008; Bloodgood and Vijay-Shanker, 2009b; Mairesse et al, 2010; Vickrey etal., 2010)).
The current paper is most highly re-lated to previous work falling into three main ar-eas: use of AL when large corpora already exist;cost-focused AL; and AL for SMT.In a sense, the work of Banko and Brill (2001)is closely related to ours.
Though their focus ismainly on investigating the performance of learn-ing methods on giant corpora many orders of mag-nitude larger than previously used, they do lay outhow AL might be useful to apply to acquire datato augment a large set cheaply because they rec-ognize the problem of diminishing returns that wediscussed in Section 1.The second area of work that is related to ours isprevious work on AL that is cost-conscious.
Thevast majority of AL research has not focused onaccurate cost accounting and a typical assumptionis that each annotatable has equal annotation cost.An early exception in the AL for NLP field wasthe work of Hwa (2000), which makes a point ofusing # of brackets to measure cost for a syntac-tic analysis task instead of using # of sentences.Another relatively early work in our field alongthese lines was the work of Ngai and Yarowsky(2000), which measured actual times of annota-tion to compare the efficacy of rule writing ver-sus annotation with AL for the task of BaseNPchunking.
Osborne and Baldridge (2004) arguedfor the use of discriminant cost over unit cost forthe task of Head Phrase Structure Grammar parseselection.
King et al (2004) design a robot thattests gene functions.
The robot chooses whichexperiments to conduct by using AL and takesmonetary costs (in pounds sterling) into accountduring AL selection and evaluation.
Unlike oursituation for SMT, their costs are all known be-forehand because they are simply the cost of ma-terials to conduct the experiments, which are al-ready known to the robot.
Hachey et al (2005)showed that selectively sampled examples for anNER task took longer to annotate and had lowerinter-annotator agreement.
This work is related toours because it shows that how examples are se-lected can impact the cost of annotation, an ideawe turn around to use for our advantage when de-veloping our data selection algorithm.
Haertel etal.
(2008) emphasize measuring costs carefully forAL for POS tagging.
They develop a model basedon a user study that can estimate the time requiredfor POS annotating.
Kapoor et al (2007) assigncosts for AL based on message length for a voice-mail classification task.
In contrast, we show forSMT that annotation times do not scale accordingto length in words and we show our method canachieve a speedup in annotation time above andbeyond what the reduction in words would indi-cate.
Tomanek and Hahn (2009) measure cost by #of tokens for an NER task.
Their AL method onlysolicits labels for parts of sentences in the interestof reducing annotation effort.
Along these lines,our method is similar in the respect that we alsowill only solicit annotation for parts of sentences,though we prefer to measure cost with time andwe show that time doesn?t track with token lengthfor SMT.Haffari et al (2009), Haffari and Sarkar (2009),and Ambati et al (2010) investigate AL for SMT.There are two major differences between our workand this previous work.
One is that our intendeduse cases are very different.
They deal with themore traditional AL setting of starting from an ex-tremely small set of seed data.
Also, by SMT stan-dards, they only add a very tiny amount of dataduring AL.
All their simulations top out at 10,000sentences of labeled data and the models learnedhave relatively low translation quality compared tothe state of the art.On the other hand, in the current paper, wedemonstrate how to apply AL in situations wherewe already have large corpora.
Our goal is to buckthe trend of diminishing returns and use AL toadd data to build some of the highest-performingMT systems in the world while keeping annota-tion costs low.
See Figure 1 from Section 1, whichcontrasts where (Haffari et al, 2009; Haffari andSarkar, 2009) stop their investigations with wherewe begin our studies.The other major difference is that (Haffari et al,2009; Haffari and Sarkar, 2009) measure annota-tion cost by # of sentences.
In contrast, we bringto light some potential drawbacks of this practice,showing it can lead to different conclusions thanif other annotation cost metrics are used, such astime and money, which are the metrics that we use.8553 Simulation ExperimentsHere we report on results of simulation experi-ments that help to illustrate and motivate the de-sign decisions of the algorithm we present in Sec-tion 4.
We use the Urdu-English language pack1from the Linguistic Data Consortium (LDC),which contains ?
88000 Urdu-English sentencetranslation pairs, amounting to?
1.7 million Urduwords translated into English.
All experiments inthis paper evaluate on a genre-balanced split of theNIST2008 Urdu-English test set.
In addition, thelanguage pack contains an Urdu-English dictio-nary consisting of ?
114000 entries.
In all the ex-periments, we use the dictionary at every iterationof training.
This will make it harder for us to showour methods providing substantial gains since thedictionary will provide a higher base performanceto begin with.
However, it would be artificial toignore dictionary resources when they exist.We experiment with two translation models: hi-erarchical phrase-based translation (Chiang, 2007)and syntax augmented translation (Zollmann andVenugopal, 2006), both of which are implementedin the Joshua decoder (Li et al, 2009).
We here-after refer to these systems as jHier and jSyntax,respectively.We will now present results of experiments withdifferent methods for growing MT training data.The results are organized into three areas of inves-tigations:1. annotation costs;2. managing uncertainty; and3.
how to automatically detect when to stop so-liciting annotations from a pool of data.3.1 Annotation CostsWe begin our cost investigations with four sim-ple methods for growing MT training data: ran-dom, shortest, longest, and VocabGrowth sen-tence selection.
The first three methods are self-explanatory.
VocabGrowth (hereafter VG) selec-tion is modeled after the best methods from previ-ous work (Haffari et al, 2009; Haffari and Sarkar,2009), which are based on preferring sentencesthat contain phrases that occur frequently in un-labeled data and infrequently in the so-far labeleddata.
Our VG method selects sentences for transla-tion that contain n-grams (for n in {1,2,3,4}) that1LDC Catalog No.
: LDC2006E110.Init:Go through all available trainingdata (labeled and unlabeled)and obtain frequency counts forevery n-gram (n in {1, 2, 3, 4})that occurs.sortedNGrams?
Sort n-grams byfrequency in descending order.Loopuntil stopping criterion (see Section 3.3) is met1.
trigger ?
Go down sortedNGrams listand find the first n-gram that isn?t covered inthe so far labeled training data.2.
selectedSentence?
Find a sentencethat contains trigger.3.
Remove selectedSentence from unlabeleddata and add it to labeled training data.End LoopFigure 2: The VG sentence selection algorithmdo not occur at all in our so-far labeled data.
Wecall an n-gram ?covered?
if it occurs at least oncein our so-far labeled data.
VG has a preferencefor covering frequent n-grams before covering in-frequent n-grams.
The VG method is depicted inFigure 2.Figure 3 shows the learning curves for bothjHier and jSyntax for VG selection and randomselection.
The y-axis measures BLEU score (Pap-ineni et al, 2002),which is a fast automatic way ofmeasuring translation quality that has been shownto correlate with human judgments and is perhapsthe most widely used metric in the MT commu-nity.
The x-axis measures the number of sen-tence translation pairs in the training data.
The VGcurves are cut off at the point at which the stoppingcriterion in Section 3.3 is met.
From Figure 3 itmight appear that VG selection is better than ran-dom selection, achieving higher-performing sys-tems with fewer translations in the labeled data.However, it is important to take care when mea-suring annotation costs (especially for relativelycomplicated tasks such as translation).
Figure 4shows the learning curves for the same systemsand selection methods as in Figure 3 but now thex-axis measures the number of foreign words inthe training data.
The difference between VG andrandom selection now appears smaller.For an extreme case, to illustrate the ramifica-8560 10,000 20,000 30,000 40,000 50,000 60,000 70,000 80,000 90,000051015202530jHier and jSyntax: VG vs Random selection (BLEU vs Sents)Number of Sentence Pairs in the Training DataBLEUScorejHier: random selectionjHier: VG selectionjSyntax: random selectionjSyntax: VG selectionwhere we will start our main experimentswhere previous AL for SMT research stopped their experimentsFigure 3: Random vs VG selection.
The x-axismeasures the number of sentence pairs in the train-ing data.
The y-axis measures BLEU score.tions of measuring translation annotation cost by #of sentences versus # of words, consider Figures 5and 6.
They both show the same three selectionmethods but Figure 5 measures the x-axis by # ofsentences and Figure 6 measures by # of words.
InFigure 5, one would conclude that shortest is a farinferior selection method to longest but in Figure 6one would conclude the opposite.Measuring annotation time and cost in dol-lars are probably the most important measuresof annotation cost.
We can?t measure these forthe simulated experiments but we will use time(in seconds) and money (in US dollars) as costmeasures in Section 5, which discusses our non-simulated AL experiments.
If # sentences or #words track these other more relevant costs in pre-dictable known relationships, then it would sufficeto measure # sentences or # words instead.
But it?sclear that different sentences can have very differ-ent annotation time requirements according to howlong and complicated they are so we will not use# sentences as an annotation cost any more.
It isnot as clear how # words tracks with annotationtime.
In Section 5 we will present evidence show-ing that time per word can vary considerably andalso show a method for soliciting annotations thatreduces time per word by nearly a factor of three.As it is prudent to evaluate using accurate costaccounting, so it is also prudent to develop newAL algorithms that take costs carefully into ac-count.
Hence, reducing annotation time burdens0 0.5 1 1.5 2x 106051015202530jHier and jSyntax: VG vs Random selection (BLEU vs FWords)Number of Foreign Words in Training DataBLEUScorejHier: random selectionjHier: VG selectionjSyntax: random selectionjSyntax: VG selectionFigure 4: Random vs VG selection.
The x-axismeasures the number of foreign words in the train-ing data.
The y-axis measures BLEU score.instead of the # of sentences translated (whichmight be quite a different thing) will be a corner-stone of the algorithm we describe in Section 4.3.2 Managing UncertaintyOne of the most successful of all AL methods de-veloped to date is uncertainty sampling and it hasbeen applied successfully many times (e.g.,(Lewisand Gale, 1994; Tong and Koller, 2002)).
Theintuition is clear: much can be learned (poten-tially) if there is great uncertainty.
However, withMT being a relatively complicated task (comparedwith binary classification, for example), it mightbe the case that the uncertainty approach has tobe re-considered.
If words have never occurredin the training data, then uncertainty can be ex-pected to be high.
But we are concerned that if asentence is translated for which (almost) no wordshave been seen in training yet, though uncertaintywill be high (which is usually considered good forAL), the word alignments may be incorrect andthen subsequent learning from that translation pairwill be severely hampered.We tested this hypothesis and Figure 7 showsempirical evidence that it is true.
Along with VG,two other selection methods?
learning curves arecharted in Figure 7: mostNew, which prefers toselect those sentences which have the largest # ofunseen words in them; and moderateNew, whichaims to prefer sentences that have a moderate #of unseen words, preferring sentences with ?
ten8570 2 4 6 8 10x 1040510152025 jHiero: Random, Shortest, and Longest selectionBLEUScoreNumber of Sentences in Training DatarandomshortestlongestFigure 5: Random vs Shortest vs Longest selec-tion.
The x-axis measures the number of sentencepairs in the training data.
The y-axis measuresBLEU score.unknown words in them.
One can see that most-New underperforms VG.
This could have been dueto VG?s frequency component, which mostNewdoesn?t have.
But moderateNew also doesn?t havea frequency preference so it is likely that mostNewwinds up overwhelming the MT training system,word alignments are incorrect, and less is learnedas a result.
In light of this, the algorithm we de-velop in Section 4 will be designed to avoid thisword alignment danger.3.3 Automatic StoppingThe problem of automatically detecting when tostop AL is a substantial one, discussed at lengthin the literature (e.g., (Bloodgood and Vijay-Shanker, 2009a; Schohn and Cohn, 2000; Vla-chos, 2008)).
In our simulation, we stop VG onceall n-grams (n in {1,2,3,4}) have been covered.Though simple, this stopping criterion seems towork well as can be seen by where the curve forVG is cut off in Figures 3 and 4.
It stops af-ter 1,293,093 words have been translated, withjHier?s BLEU=21.92 and jSyntax?s BLEU=26.10at the stopping point.
The ending BLEU scores(with the full corpus annotated) are 21.87 and26.01 for jHier and jSyntax, respectively.
Soour stopping criterion saves 22.3% of the anno-tation (in terms of words) and actually achievesslightly higher BLEU scores than if all the datawere used.
Note: this ?less is more?
phenomenon0 0.5 1 1.5 2x 1060510152025Number of Foreign Words in Training DataBLEUScorejHiero: Longest, Shortest, and Random SelectionrandomshortestlongestFigure 6: Random vs Shortest vs Longest selec-tion.
The x-axis measures the number of foreignwords in the training data.
The y-axis measuresBLEU score.has been commonly observed in AL settings (e.g.,(Bloodgood and Vijay-Shanker, 2009a; Schohnand Cohn, 2000)).4 Highlighted N-Gram MethodIn this section we describe a method for solicit-ing human translations that we have applied suc-cessfully to improving translation quality in real(not simulated) conditions.
We call the method theHighlighted N-Gram method, or HNG, for short.HNG solicits translations only for trigger n-gramsand not for entire sentences.
We provide senten-tial context, highlight the trigger n-gram that wewant translated, and ask for a translation of just thehighlighted trigger n-gram.
HNG asks for transla-tions for triggers in the same order that the triggersare encountered by the algorithm in Figure 2.
Ascreenshot of our interface is depicted in Figure 8.The same stopping criterion is used as was used inthe last section.
When the stopping criterion be-comes true, it is time to tap a new unlabeled poolof foreign text, if available.Our motivations for soliciting translations foronly parts of sentences are twofold, correspondingto two possible cases.
Case one is that a translationmodel learned from the so-far labeled data will beable to translate most of the non-trigger words inthe sentence correctly.
Thus, by asking a humanto translate only the trigger words, we avoid wast-ing human translation effort.
(We will show in8580 0.5 1 1.5 2x 1060510152025Number of Foreign Words in Training DataBLEUScorejHiero: VG vs mostNew vs moderateNewVGmostNewmoderateNewFigure 7: VG vs MostNew vs ModerateNew se-lection.
The x-axis measures the number of sen-tence pairs in the training data.
The y-axis mea-sures BLEU score.!
"#$% "& '() '* +,-./0)1 234 5678 9:-!
!
"#$ %$&'$ &( )*+ ;<= '$ >/?
@3 /A>.
+B!C D)C EF GH?I '3") D)+0) +&  .
"J& "J& "$K$!1 2L)M8 ':?3N !O#)P& GQ6- '& R7@* /& ST& ST& !9,8 UV)WX'8 ,"*)- !.
( /0."
234 !.C 234 !D#8 EY).<3 '8 MH3 G:Z !
"-[$% '8 R3\5#)T= 5#)] '3E& >'#)P8 ><&  .^ : S_ <* '(* C+& +:Z '* /`$>a UH$GX "& 5,-.b '8 "$c 9* S_ /& <* dH#$!<)+& ?
e(@)f e3<g : #1.2 #1.2 "(:Z  .<* e@* ':) K) C+) E#) '* +$ /H0) +& <* G:I ' 3.45 ' '& ')C',$% "& 5#:68 +$  .
!1 ') '(,) 67$ !8 ' )9 GQ)PI '& UI ') .hX +& !.C !1 '$ "i3 !
"-!f "(:Z ':) K)/H0)  ' .Figure 8: Screenshot of the interface we used forsoliciting translations for triggers.the next section that we even get a much largerspeedup above and beyond what the reduction innumber of translated words would give us.)
Casetwo is that a translation model learned from the so-far labeled data will (in addition to not being ableto translate the trigger words correctly) also not beable to translate most of the non-trigger words cor-rectly.
One might think then that this would be agreat sentence to have translated because the ma-chine can potentially learn a lot from the transla-tion.
Indeed, one of the overarching themes of ALresearch is to query examples where uncertainty isgreatest.
But, as we showed evidence for in thelast section, for the case of SMT, too much un-certainty could in a sense overwhelm the machineand it might be better to provide new training datain a more gradual manner.
A sentence with large#s of unseen words is likely to get word-alignedincorrectly and then learning from that translationcould be hampered.
By asking for a translationof only the trigger words, we expect to be able tocircumvent this problem in large part.The next section presents the results of experi-ments that show that the HNG algorithm is indeedpractically effective.
Also, the next section ana-lyzes results regarding various aspects of HNG?sbehavior in more depth.5 Experiments and Discussion5.1 General SetupWe set out to see whether we could use the HNGmethod to achieve translation quality improve-ments by gathering additional translations to addto the training data of the entire LDC languagepack, including its dictionary.
In particular, wewanted to see if we could achieve translation im-provements on top of already state-of-the-art per-forming systems trained already on the entire LDCcorpus.
Note that at the outset this is an ambitiousendeavor (recall the flattening of the curves in Fig-ure 1 from Section 1).Snow et al (2008) explored the use of the Ama-zon Mechanical Turk (MTurk) web service forgathering annotations for a variety of natural lan-guage processing tasks and recently MTurk hasbeen shown to be a quick, cost-effective way togather Urdu-English translations (Bloodgood andCallison-Burch, 2010).
We used the MTurk webservice to gather our annotations.
Specifically, wefirst crawled a large set of BBC articles on the in-ternet in Urdu and used this as our unlabeled poolfrom which to gather annotations.
We applied theHNG method from Section 4 to determine what topost on MTurk for workers to translate.2 We gath-ered 20,580 n-gram translations for which we paid$0.01 USD per translation, giving us a total costof $205.80 USD.
We also gathered 1632 randomlychosen Urdu sentence translations as a control set,for which we paid $0.10 USD per sentence trans-lation.32For practical reasons we restricted ourselves to not con-sidering sentences that were longer than 60 Urdu words, how-ever.3The prices we paid were not market-driven.
We justchose prices we thought were reasonable.
In hindsight, givenhow much quicker the phrase translations are for people wecould have had a greater disparity in price.8595.2 Accounting for Translation TimeMTurk returns with each assignment the ?Work-TimeInSeconds.?
This is the amount of time be-tween when a worker accepts an assignment andwhen the worker submits the completed assign-ment.
We use this value to estimate annotationtimes.4Figure 9 shows HNG collection versus randomcollection from MTurk.
The x-axis measures thenumber of seconds of annotation time.
Note thatHNG is more effective.
A result that may be par-ticularly interesting is that HNG results in a timespeedup by more than just the reduction in trans-lated words would indicate.
The average time totranslate a word of Urdu with the sentence post-ings to MTurk was 32.92 seconds.
The averagetime to translate a word with the HNG postings toMTurk was 11.98 seconds.
This is nearly threetimes faster.
Figure 10 shows the distribution ofspeeds (in seconds per word) for HNG postingsversus complete sentence postings.
Note that theHNG postings consistently result in faster transla-tion speeds than the sentence postings5.We hypothesize that this speedup comes aboutbecause when translating a full sentence, there?sthe time required to examine each word and trans-late them in some sense (even if not one-to-one)and then there is an extra significant overhead timeto put it all together and synthesize into a largersentence translation.
The factor of three speedupis evidence that this overhead is significant effortcompared to just quickly translating short n-gramsfrom a sentence.
This speedup is an additionalbenefit of the HNG approach.5.3 Bucking the TrendWe gathered translations for?
54,500 Urdu wordsvia the use of HNG on MTurk.
This is a rela-tively small amount, ?
3% of the LDC corpus.Figure 11 shows the performance when we addthis training data to the LDC corpus.
The rect-4It?s imperfect because of network delays and if a personis multitasking or pausing between their accept and submittimes.
Nonetheless, the times ought to be better estimates asthey are taken over larger samples.5The average speed for the HNG postings seems to beslower than the histogram indicates.
This is because therewere a few extremely slow outlier speeds for a handful ofHNG postings.
These are almost certainly not cases when theturker is working continuously on the task and so the averagespeed we computed for the HNG postings might be slowerthan the actual speed and hence the true speedup may evenbe faster than indicated by the difference between the aver-age speeds we reported.0 1 2 3 4 5 6x 10521.621.82222.222.422.622.8Number of Seconds of Annotation TimeBLEUScorejHier: HNG Collection vs Random Collection of Annotations from MTurkrandomHNGFigure 9: HNG vs Random collection of new datavia MTurk.
y-axis measures BLEU.
x-axis mea-sures annotation time in seconds.angle around the last 700,000 words of the LDCdata is wide and short (it has a height of 0.9 BLEUpoints and a width of 700,000 words) but the rect-angle around the newly added translations is nar-row and tall (a height of 1 BLEU point and awidth of 54,500 words).
Visually, it appears weare succeeding in bucking the trend of diminish-ing returns.
We further confirmed this by runninga least-squares linear regression on the points ofthe last 700,000 words annotated in the LDC dataand also for the points in the new data that we ac-quired via MTurk for $205.80 USD.
We find thatthe slope fit to our new data is 6.6245E-06 BLEUpoints per Urdu word, or 6.6245 BLEU points fora million Urdu words.
The slope fit to the LDCdata is only 7.4957E-07 BLEU points per word,or only 0.74957 BLEU points for a million words.This is already an order of magnitude differencethat would make the difference between it beingworth adding more data and not being worth it;and this is leaving aside the added time speedupthat our method enjoys.Still, we wondered why we could not haveraised BLEU scores even faster.
The main hur-dle seems to be one of coverage.
Of the 20,580 n-grams we collected, only 571 (i.e., 2.77%) of themever even occur in the test set.5.4 Beyond BLEU ScoresBLEU is an imperfect metric (Callison-Burch etal., 2006).
One reason is that it rates all ngram8600 20 40 60 80 100 12000.050.10.150.20.25Time (in seconds) per foreign word translatedRelative FrequencyHistogram showing the distribution of translation speeds (in seconds per foreign word) when translations are collected via n?grams versus via complete sentencesn?gramssentencesaverage time perword for sentencesaverage time perword for n?gramsFigure 10: Distribution of translation speeds (inseconds per word) for HNG postings versus com-plete sentence postings.
The y-axis measures rel-ative frequency.
The x-axis measures translationspeed in seconds per word (so farther to the left isfaster).mismatches equally although some are much moreimportant than others.
Another reason is it?s notintuitive what a gain of x BLEU points means inpractice.
Here we show some concrete exampletranslations to show the types of improvementswe?re achieving and also some examples whichsuggest improvements we can make to our AL se-lection algorithm in the future.
Figure 12 shows aprototypical example of our system working.Figure 13 shows an example where the strategyis working partially but not as well as it might.
TheUrdu phrase was translated by turkers as ?gownedveil?.
However, since the word aligner just alignsthe word to ?gowned?, we only see ?gowned?
inour output.
This prompts a number of discussionpoints.
First, the ?after system?
has better transla-tions but they?re not rewarded by BLEU scores be-cause the references use the words ?burqah?
or just?veil?
without ?gowned?.
Second, we hypothesizethat we may be able to see improvements by over-riding the automatic alignment software when-ever we obtain a many-to-one or one-to-many (interms of words) translation for one of our triggerphrases.
In such cases, we?d like to make sure thatevery word on the ?many?
side is aligned to the1 1.2 1.4 1.6 1.8x 1062121.52222.52323.5 Bucking the Trend: JHiero Translation Quality versus Number of Foreign Words AnnotatedBLEUScoreNumber of Foreign Words Annotatedthe approx.
54,500 foreign wordswe selectively sampled for annotation cost = $205.80last approx.
700,000 foreign words annotated in LDC dataFigure 11: Bucking the trend: performance ofHNG-selected additional data from BBC webcrawl data annotated via Amazon MechanicalTurk.
y-axis measures BLEU.
x-axis measuresnumber of words annotated.Figure 12: Example of strategy working.single word on the ?one?
side.
For example, wewould force both ?gowned?
and ?veil?
to be alignedto the single Urdu word instead of allowing the au-tomatic aligner to only align ?gowned?.Figure 14 shows an example where our ?before?system already got the translation correct withoutthe need for the additional phrase translation.
Thisis because though the ?before?
system had neverseen the Urdu expression for ?12May?, it had seenthe Urdu words for ?12?
and ?May?
in isolationand was able to successfully compose them.
Anarea of future work is to use the ?before?
system todetermine such cases automatically and avoid ask-ing humans to provide translations in such cases.861Figure 13: Example showing where we can im-prove our selection strategy.Figure 14: Example showing where we can im-prove our selection strategy.6 Conclusions and Future WorkWe succeeded in bucking the trend of diminishingreturns and improving translation quality whilekeeping annotation costs low.
In future work wewould like to apply these ideas to domain adap-tation (say, general-purpose MT system to workfor scientific domain such as chemistry).
Also, wewould like to test with more languages, increasethe amount of data we can gather, and investigatestopping criteria further.
Also, we would like toinvestigate increasing the efficiency of the selec-tion algorithm by addressing issues such as the oneraised by the 12 May example presented earlier.AcknowledgementsThis work was supported by the Johns HopkinsUniversity Human Language Technology Centerof Excellence.
Any opinions, findings, conclu-sions, or recommendations expressed in this mate-rial are those of the authors and do not necessarilyreflect the views of the sponsor.ReferencesVamshi Ambati, Stephan Vogel, and Jaime Carbonell.2010.
Active learning and crowd-sourcing for ma-chine translation.
In Proceedings of the Seventh con-ference on International Language Resources andEvaluation (LREC?10), Valletta, Malta, may.
Euro-pean Language Resources Association (ELRA).Michele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disambigua-tion.
In Proceedings of 39th Annual Meeting of theAssociation for Computational Linguistics, pages26?33, Toulouse, France, July.
Association for Com-putational Linguistics.Michael Bloodgood and Chris Callison-Burch.
2010.Using mechanical turk to build machine translationevaluation sets.
In Proceedings of the Workshop onCreating Speech and Language Data With Amazon?sMechanical Turk, Los Angeles, California, June.Association for Computational Linguistics.Michael Bloodgood and K Vijay-Shanker.
2008.
Anapproach to reducing annotation costs for bionlp.In Proceedings of the Workshop on Current Trendsin Biomedical Natural Language Processing, pages104?105, Columbus, Ohio, June.
Association forComputational Linguistics.Michael Bloodgood and K Vijay-Shanker.
2009a.
Amethod for stopping active learning based on stabi-lizing predictions and the need for user-adjustablestopping.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning(CoNLL-2009), pages 39?47, Boulder, Colorado,June.
Association for Computational Linguistics.Michael Bloodgood and K Vijay-Shanker.
2009b.
Tak-ing into account the differences between activelyand passively acquired data: The case of activelearning with support vector machines for imbal-anced datasets.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics (NAACL), pages 137?140, Boulder, Colorado, June.
Association for Com-putational Linguistics.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of Bleu in ma-chine translation research.
In 11th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics (EACL-2006), Trento, Italy.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Ben Hachey, Beatrice Alex, and Markus Becker.
2005.Investigating the effects of selective sampling on theannotation task.
In Proceedings of the Ninth Confer-ence on Computational Natural Language Learning(CoNLL-2005), pages 144?151, Ann Arbor, Michi-gan, June.
Association for Computational Linguis-tics.Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-roll, and Peter McClanahan.
2008.
Assessing the862costs of sampling methods in active learning for an-notation.
In Proceedings of ACL-08: HLT, Short Pa-pers, pages 65?68, Columbus, Ohio, June.
Associa-tion for Computational Linguistics.Gholamreza Haffari and Anoop Sarkar.
2009.
Activelearning for multilingual statistical machine trans-lation.
In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th In-ternational Joint Conference on Natural LanguageProcessing of the AFNLP, pages 181?189, Suntec,Singapore, August.
Association for ComputationalLinguistics.Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.2009.
Active learning for statistical phrase-basedmachine translation.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 415?423,Boulder, Colorado, June.
Association for Computa-tional Linguistics.Rebecca Hwa.
2000.
Sample selection for statisticalgrammar induction.
In Hinrich Schu?tze and Keh-Yih Su, editors, Proceedings of the 2000 Joint SIG-DAT Conference on Empirical Methods in NaturalLanguage Processing, pages 45?53.
Association forComputational Linguistics, Somerset, New Jersey.Ashish Kapoor, Eric Horvitz, and Sumit Basu.
2007.Selective supervision: Guiding supervised learn-ing with decision-theoretic active learning.
InManuela M. Veloso, editor, IJCAI 2007, Proceed-ings of the 20th International Joint Conference onArtificial Intelligence, Hyderabad, India, January 6-12, 2007, pages 877?882.Ross D. King, Kenneth E. Whelan, Ffion M.Jones, Philip G. K. Reiser, Christopher H. Bryant,Stephen H. Muggleton, Douglas B. Kell, andStephen G. Oliver.
2004.
Functional genomic hy-pothesis generation and experimentation by a robotscientist.
Nature, 427:247?252, 15 January.David D. Lewis and William A. Gale.
1994.
A se-quential algorithm for training text classifiers.
In SI-GIR ?94: Proceedings of the 17th annual interna-tional ACM SIGIR conference on Research and de-velopment in information retrieval, pages 3?12, NewYork, NY, USA.
Springer-Verlag New York, Inc.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Sanjeev Khudanpur, Lane Schwartz, WrenThornton, Jonathan Weese, and Omar Zaidan.
2009.Joshua: An open source toolkit for parsing-basedmachine translation.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, pages135?139, Athens, Greece, March.
Association forComputational Linguistics.Francois Mairesse, Milica Gasic, Filip Jurcicek, SimonKeizer, Jorge Prombonas, Blaise Thomson, Kai Yu,and Steve Young.
2010.
Phrase-based statisticallanguage generation using graphical models and ac-tive learning.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics (ACL), Uppsala, Sweden, July.
Associationfor Computational Linguistics.Grace Ngai and David Yarowsky.
2000.
Rule writ-ing or annotation: cost-efficient resource usage forbase noun phrase chunking.
In Proceedings of the38th Annual Meeting of the Association for Compu-tational Linguistics.
Association for ComputationalLinguistics.Miles Osborne and Jason Baldridge.
2004.
Ensemble-based active learning for parse selection.
InDaniel Marcu Susan Dumais and Salim Roukos, ed-itors, HLT-NAACL 2004: Main Proceedings, pages89?96, Boston, Massachusetts, USA, May 2 - May7.
Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.
Association for Computa-tional Linguistics.Manabu Sassano.
2002.
An empirical study of activelearning with support vector machines for japaneseword segmentation.
In ACL ?02: Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, pages 505?512, Morristown, NJ,USA.
Association for Computational Linguistics.Greg Schohn and David Cohn.
2000.
Less is more:Active learning with support vector machines.
InProc.
17th International Conf.
on Machine Learn-ing, pages 839?846.
Morgan Kaufmann, San Fran-cisco, CA.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Ng.
2008.
Cheap and fast ?
but is itgood?
evaluating non-expert annotations for natu-ral language tasks.
In Proceedings of the 2008 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 254?263, Honolulu, Hawaii, Oc-tober.
Association for Computational Linguistics.Katrin Tomanek and Udo Hahn.
2009.
Semi-supervised active learning for sequence labeling.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP, pages 1039?1047, Suntec, Singapore,August.
Association for Computational Linguistics.Simon Tong and Daphne Koller.
2002.
Support vec-tor machine active learning with applications to textclassification.
Journal of Machine Learning Re-search (JMLR), 2:45?66.David Vickrey, Oscar Kipersztok, and Daphne Koller.2010.
An active learning approach to finding relatedterms.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics (ACL), Uppsala, Sweden, July.
Association forComputational Linguistics.863Andreas Vlachos.
2008.
A stopping criterion foractive learning.
Computer Speech and Language,22(3):295?312.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart pars-ing.
In Proceedings of the NAACL-2006 Workshopon Statistical Machine Translation (WMT06), NewYork, New York.864
