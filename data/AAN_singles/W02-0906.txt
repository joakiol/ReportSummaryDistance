Learning Argument/Adjunct Distinction for BasqueAbstractThis paper presents experiments performed onlexical knowledge acquisition in the form ofverbal argumental information.
The systemobtains the data from raw corpora after theapplication of a partial parser and statisticalfilters.
We used two different statistical filtersto acquire the argumental information: MutualInformation, and Fisher?s Exact test.Due to the characteristics of agglutinativelanguages like Basque, the usual classificationof arguments in terms of their syntacticcategory (such as NP or PP) is not suitable.For that reason, the arguments will beclassified in 48 different kinds of casemarkers, which makes the system fine grainedif compared to equivalent systems that havebeen developed for other languages.This work addresses the problem ofdistinguishing arguments from adjuncts, thisbeing one of the most significant sources ofnoise in subcategorization frame acquisition.IntroductionIn recent years a considerable effort has been doneon the acquisition of lexical information.
Asseveral authors point out, this information is usefulfor a wide range of applications.
For example, J.Carroll et al (1998) show how addingsubcategorization information improves theperformance of a parser.With this in mind our aim is to obtain a systemthat automatically discriminates betweensubcategorized elements of verbs (arguments) andnon-subcategorized ones (adjuncts).We have evaluated our system in two ways:comparing the results to a gold standard andestimating the coverage over sentences in thecorpus.
The purpose was to find out which was theimpact of each approach on this particular task.The two methods of evaluation yield significantlydifferent results.Basque is the subject of this study.
A languagethat, in contrast to languages like English, haslimited resources in the form of digital corpora,computational lexicons, grammars or annotatedtreebanks.
Therefore, any effort like the onepresented here, oriented to create lexical resources,has to be driven to do as much automatic work aspossible, minimizing development costs.The paper is divided into 4 sections.
The firstsection is devoted to explain the theoreticalmotivations underlying the process.
The secondsection is a description of the different stages ofthe system.
The third section presents the resultsobtained.
The fourth section is a review ofprevious work on automatic subcategorizationacquisition.
Finally, we present the mainconclusions.1 The argument/adjunct distinctionTalking about Subcategorization Frames (SCF),means talking about arguments.
Many existingsystems acquire directly a set of possible SCFswithout any previous filtering of adjuncts.However, adjuncts are a substantial source of noiseand therefore, in order to avoid this problem, ourapproach addresses the problem of theargument/adjunct distinction.The argument/adjunct distinction is probablyone of the most unclear issues in linguistics.
Thedistinction has being presented, for example, in thegenerativist tradition, in the following way:arguments are those elements participating in theevent and adjuncts are those elementscontextualizing or locating the event.This definition seems to be quite clear, butwhen we deal with concrete examples it is not theIzaskun Aldezabal, Maxux Aranzabe, KoldoGojenola , Kepa SarasolaDept.
of Computer Languages and Systems,University of the Basque Country, 649 P. K.,E-20080 Donostia,Basque CountryAitziber AtutxaUniversity of MarylandCollege ParkMaryland, 20740jibatsaa@si.ehu.esJuly 2002, pp.
42-50.
Association for Computational Linguistics.ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,Unsupervised Lexical Acquisition: Proceedings of the Workshop of thecase.
For example, if we take two verbs, talk andplay.a.
Yesterday I talked with Mary.b.
Yesterday I played soccer with Mary.Here Mary is a participant of the event in bothcases, therefore under the given definition bothwould be arguments.
But this is contradictory towhat traditional views consider in practice.
ThePP, with Mary, is considered an argument of talkbut not an argument of play.
It is true that there aredifferences between both of them because playingdoes not require two participants (though it canhave them), while talking (under the sense ofcommunicating) seems to require two participants.Finer argument/adjunct distinction have alsobeen proposed differentiating between basicarguments, pseudo-arguments and adjuncts.
Basicarguments are those required by the verb.
Pseudo-arguments are those that even if they are notrequired by the verb, when appearing they extendthe verbal semantics, for example, adding newparticipants.
And finally adjuncts, which would becontextualizers of the event.
The most radical viewis to consider the argument/adjunct distinction as acontinuum where the elements belonging to theextremes of this continuum can be easily classifiedas arguments or adjuncts.
On the contrary, theelements belonging to the central part of thecontinuum can be easily misclassified.
For furtherreference see C. Schutze (1995), J.M.
Gawron(1986), C. Verspoor (1997), J. Grimshaw (1990),and N. Chomsky (1995).From the different diagnostics proposed in theliterature some are quite consistent among variousauthors (R. Grishman et al 1994, C. Pollard and I.Sag 1987, C. Verspoor 1997).1) The Obligatoriness condition.
When a verbdemands obligatorily the appearance of anelement, this element will be an argument.a.
John put the book on the tableb.
*John put the book2) Frequency.
Arguments of a verb occur morefrequently with that verb than with the otherverbs.a.
I came from home (argument).b.
I heard it from you (adjunct).3) Iterability: Several instances of the sameadjunct can appear together with a verb, whileseveral instances of an argument cannot appearwith a verb.a.
I saw you in Washington, in theKenedy Center.b.
*I saw Alice John (being John andAlice two persons)4) Relative order: Arguments tend to appear closerto the verb than adjuncts.a.
I put the book on the table at threeb.
*I put at three the book on thetable5) Implicational test:  Arguments are semanticallyimplied, even when they are optional.a.
I came to your house (from x)b. I heard that (from x)The third and fourth tests were not very usefulto us.
Iterability test is quite weak because it seemsto rely more on some other semantic notions suchas part/whole relation than in the argument/adjunctdistinction.
For example, sentence 3.a would begrammatical due to semantic plausibility.
TheKennedy Center is a part of Washington, thereforeto see somebody in the Kennedy Center and seehim in Washington are not semanticallyincompatible, so it is plausible to say it.
In the caseof 3.b John is not a part of  Alice and therefore it isnot plausible to see Alice John.
But for example itis plausible to say I saw you the hand.
The relativeorder test is difficult to apply on a language likeBasque which is a free word order language.The first and fifth tests are robust enough to beuseful in practice.
But only the two firstdiagnostics can be captured statistically by theapplication of association measures like MutualInformation.
We did not come out with anystraightforward way to apply the fifth testcomputationally.Before talking about the different measuresapplied, we will present step by step the wholeprocess we pursued for achieving theargument/adjunct distinction.2 The acquisition processOur starting point was a raw newspaper corpusfrom of 1,337,445 words, where there wereinstances of 1,412 verbs.
From them, we selected640 verbs as statistically relevant because theyappear in more than 10 sentences.As we said earlier, our goal was to distinguisharguments from adjuncts.
When starting from rawcorpus, like in this case, it is necessary to getinstances of verbs together with their dependents(arguments and adjuncts).
We obtained thisinformation applying a partial parser (section 2.1)to the corpus.
Once we had the dependents,statistical measures helped us deciding which werearguments and which were adjuncts (section 2.2).2.1 The parsing phaseAiming to obtain the data against which statisticalfilters will be applied, we analyzed the corpususing several available linguistic resources:?
First, we performed morphological analysis ofthe corpus, based on two-level morphology (K.Koskenniemi 1983; I. Alegria et al 1996) anddisambiguation using the Constraint Grammarformalism (Karlsson et al 1995, Aduriz et al1997).?
Second, a shallow parser was applied (I.Aldezabal et al 2000), which recognizes basicsyntactic units including noun phrases,prepositional phrases and several types ofsubordinate sentences.?
The third step consisted in linking each verband its dependents.
Basque lacks a robustparser as in (T. Briscoe & J. Carroll 1997, D.Kawahara et al 2001) and, therefore, we used afinite state grammar to link the dependents(both arguments and adjuncts) with the verb (I.I.
Aldezabal et al 2001).
This grammar wasdeveloped using the Xerox Finite State Tool (L.Karttunen et al 1997).
Figure 1 shows theresult of the parsing phase.
In this case, bothcommitative and inessive cases (PPs) areadjuncts, while the ergative NP is an argument.The linking of dependents to a verb is nottrivial considering that Basque is a languagewith free order of constituents, and any elementappearing between two verbs could be, inprinciple, dependent on any of them.
Manyproblems must be taken into account, such asambiguity and determination of clauseboundaries, among others.
We evaluated theaccuracy up to this point, obtaining a precisionover dependents of 87% and a recall of 66%.So the input data to the next phase wasrelatively noisy.2.2 The argument selection phaseIn the data resulting from the shallow parsingphase we counted up to 65 different cases (types ofarguments, including postpositions and differenttypes of suffixes).
These are divided in two maingroups:?
43 correspond to postpositions.
Some of themcan be directly mapped to English prepositions,but in many cases several Basque postpositionscorrespond to just one English preposition (seeTable 1a.).
This set alo contains postpositions1)?
(a) [ EEBBetako lehendakariak] (b) [UEko 15 herrialdeetako merkataritza ministroekin](c) [bazkaldu behar zuen] (d) [negoziazioen bilgunean] ?2) ?
the president of the USA had to eat with the ministers of Commerce of 15 countries of the UE inthe negotiation center ?
(a)  [EEBB-etako lehendakari-a-k]       (b)  [UE-ko    15 herrialde-etako    merkataritza ministro-ekin][USA-of         president-the-erg.]
[UE-of    15 countries-of          Commerce ministers-with]NP-ergative(president, singular)                PP(with)-commitative(minister, plural)The president of the USA  with the ministers of Commerce of 15 countries of the UE(c) [bazkaldu behar zuen]                  (d)   [negoziazio-en     bilgune-an][to eat        had]                                  [negotiation-of     center-in]verb(eat)                                        PP(in)-inessive(center, singular)had to eat in the negotiation centerFigure 1.
Example of the output of the shallow parsing phase: 1) Input (in Basque), 2) English translation,.Below (c) Verb phrase and (a,b,d) verbal dependents (phrases), and also under the case+headthat map to categories other than Englishprepositions, such as adverbs (Table 1b).Table 1.
Correspondence between Englishprepositions and Basque postpositions.English Basquea.
to dative (suffix)alative (suffix)final ablative (suffix)b. like -en gisa (suffix)gisabezalalegez?
22 types of sentential complements (Forinstance, English that complementizercorresponds to several subordination suffixes:-la, -n, -na, -nik).This shows to which extent the range ofarguments is fine grained, in contrast to otherworks where the range is at the categorial level,such as NP or PP (M. Brent 1993, C. Manning1993, P. Merlo & M. Leybold 2001).Due to the complexity carried by having such ahigh number of cases, we decided to gatherpostpositions that are semantically equivalent oralmost equivalent (for example, English betweenand among).
Even if there are some semanticdifferences between them they do not seem to berelevant at the syntactic level.
Some linguists werein charge of completing this grouping task.
Evenconsidering the risk of making mistakes whengrouping the cases, we concluded that the loss ofaccuracy due to having too sparse data(consequence of having many cases) would beworse than the noise introduced by any mistake inthe grouping.
The resulting set contained 48 cases.The complexity is reduced but it is stillconsiderable.Most of the work on automatic acquisition ofsubcategorization information (J. Carroll & T.Briscoe 1997, A. Sarkar & D. Zeman 2000, A.Korhonen 2001) apply statistical methods(hypothesis testing).
Basically the idea is thefollowing: they get "possible subcategorizationframes" from automatically parsed data (eithercompletely or partially parsed) or from asyntactically annotated corpus.
Afterwards astatistical filter is employed to decide whetherthose "possible frames" are or not realsubcategorization frames.
These statisticalmethods can be problematic mostly because theyperform badly on sparse data.
In order to avoid asmuch as possible data sparseness, we decided todesign a system that learns which are thearguments of a given verb instead of learningwhole frames.
Frames are combinations ofarguments, and considering that our system dealswith 48 cases, the number of combinations washigh, resulting in sparse data.
So we decided towork at the level of the argument/adjunctdistinction.
Working on this distinction is also veryuseful to avoid noise in the subcategorizationframe, because in this task adjuncts are synonymsof noise.
A system that tries to getsubcategorization frames without previouslymaking the argument/adjunct distinction suffers ofhaving sparse and noisy data.To accomplish the argument/adjunct distinctionwe applied two measures: Mutual Information(MI), and Fisher's Exact Test (for moreinformation on these measures, see C. Manning &H. Sch?tze 1999).
MI is a measure coming fromInformation Theory, defined as the logarithm ofthe ratio between the probability of the co-occurrence of the verb and the case, and theprobability of the verb and the case appearingtogether calculated from their independentprobability.
So higher Mutual Information valuescorrespond to higher associated verb and cases(see table 2).Table 2.
Examples from MI values for verb-casepairsverb case MIatera(to take/go out) ablative(from) 1.830atera(to take/go out) instrumental(with) -0.955erabili(to use) gisa(as) 2.255erabili(to use) instrumental(with) -0.783Mutual Information shows higher values foratera-ablative(to go/take out), erabili-gisa (to use-as).
These pairs were manually tagged asarguments, therefore Mutual information makesthe right prediction.
On the contrary, atera-instrumental (to go/take out-with), erabili-instrumental (to use-with) were manually tagged asadjuncts.
Mutual information values in table 2 goalong with the manual tagging for these last pairsas well, because the Mutual information values arelow as should correspond to adjuncts.Fisher?s Exact Test is a hypothesis testingstatistical measure1.
We used the left-side versionof the test (see T. Pederssen, 1996).
Under thisversion the test tells us how likely it would be toperform the same experiment again and be lessaccurate.
That is to say, if you were repeating theexperiment and there were no relation between theverb and the case, you would have a bigprobability of finding a lower co-occurrencefrequency than the one you observed in yourexperiment.
So higher left-side Fisher values tellus that there is a correlation between the verb andthe case (see table 3.
)Table 3.
Examples of Fisher?s Exact Test  values forverb-case pairsverb Case Fisheratera(to take/go out) Ablative(from) 1.0000atera(to take/go out) instrumental(with) 0.0003erabili(to use) gisa(as) 1.0000erabili(to use) instrumental(with) 0.0002Fisher?s Exact values show higher values foratera-ablative(to go/take out), erabili-gisa (to use-as).
These values predict correctly the associationbetween the verbs and cases for these examples.The low values for the atera-instrumental (togo/take out-with), and erabili-instrumental (to use-with) pairs, should be interpreted as the non-association between the verbs and the cases inthese examples, that is to say, they are adjuncts.And again, the prediction would be right accordingto the taggers.These tests are broadly used to discoverassociations between words, but they showdifferent behaviour depending on the nature of thedata.
We did not want to make any a prioridecision on the measure employed.
On thecontrary, we aimed to check which test behavedbetter on our data.3 EvaluationWe found in the literature two main approaches toevaluate a system like the one proposed in thispaper (T. Briscoe & J. Carroll 1997, A. Sarkar &D. Zeman 2000, A. Korhonen 2001):1 There are two ways of interpreting Fisher?s test, as oneor two sided test.
In the one sided fashion there is stillanother interpretation, as a right or left sided test.?
Comparing the obtained information with agold standard.?
Calculating the coverage of the obtainedinformation on a corpus.
This can give  anestimate of how well the information obtainedcould help a parser on that corpus.Under the former approach a further distinctionemerges: using a dictionary as a gold standard, orperforming manual evaluation, where somelinguists extract the subcategorization framesappearing in a corpus and comparing them with theset of subcategorization frames obtainedautomatically.We decided to evaluate the system both ways,that is to say, using a gold standard and calculatingthe coverage over a corpus.
The intention was todetermine, all things being equal, the impact ofdoing it one way or the other.3.1 Evaluation 1: comparison of the results with agold standardFrom the 640 analyzed verbs, we selected 10 forevaluation.
For each of these verbs we extractedfrom the corpus the list of all their dependents.
Thelist was a set of bare verb-case pairs, that is, nocontext was involved and, therefore, as the senseof the given verb could not be derived, differentsenses of the verb were taken into account.
Weprovided 4 human annotators/taggers with this listand they marked each dependent as eitherargument or adjunct.
The taggers accomplished thetask three times.
Once, with the simple guidelineof the implicational test and obligatoriness test, butwith no further consensus.
The inter-taggeragreement was low (57%).
The taggers gatheredand realized that the problem came mostly fromsemantics.
While some taggers tagged the verb-case pairs assuming a concrete semantic domainthe others took into account a wider rage of senses(moreover, in some cases the senses did not evenmatch).
So the tagging was repeated when all ofthem considered the same semantics to thedifferent verbs.
The inter-tagger agreement raisedup to a 80%.
The taggers gathered again to discuss,deciding over the non clear pairs.The list obtained from merging2 the 4 lists inone is taken to be our gold standard.
Notice that2 Merging was possible once the annotators agreed onthe marking of each element.when the annotators decided whether a possibleargument was really an argument or not, nocontext was involved.
In other words, they weredeciding over bare pairs of verbs and cases.Therefore different senses of the verb wereconsidered because there was no way todisambiguate the specific meaning of the verb.
Sothe evaluation is an approximation of how wellwould the system perform over any corpus.
Table4 shows the results in terms of Precision andRecall.Table 4.
Results of Evaluation 1 (contextindependent)Precision Recall F-scoreMI 62% 50% 55%Fisher 64% 44% 52%3.2 Evaluation 2: Calculation of the coverage on acorpusThe initial corpus was divided in two parts, one fortraining the system and another one for evaluatingit.
From the fraction reserved for evaluation weextracted 200 sentences corresponding to the same10 verbs used in the "gold standard" basedevaluation.
In this case, the task carried out by theannotators consisted in extracting, for each of the200 sentences, the elements (arguments/adjuncts)linked to the corresponding verb.
Each elementwas marked as argument or adjunct.
Note that inthis case the annotation takes place inside thecontext of the sentence.
In other words, the verbshows precise semantics.We performed a simple evaluation on thesentences (see table 5), calculating precision andrecall over each argument marked by theannotators3.
For example, if a verb appeared in asentence with two arguments and the statisticalfilters were recognizing them as arguments, bothprecision and recall would be 100%.
If, on thecontrary, only one was found, then precisionwould be 100%, and recall 50%.Table 5.
Results of Evaluation 2 (inside context)Precision Recall F-scoreMI 93% 97% 95%Fisher 93% 93% 93%3 The inter-tagger agreement in this case was of  97%.3.3 DiscussionIt is obvious that the results attained in the firstevaluation are different than those in the secondone.
The origin of this difference comes mostly, onone hand, from semantics and, on the other hand,from the nature of statistics:?
Semantic source.
The former evaluation wasnot contextualized, while the latter used thesentence context.
Our experience showed usthat broader semantics (non-contextualizedevaluation) leads to a situation where thenumber of arguments increases with respect tonarrower (contextualized evaluation)semantics.
This happens because in manycases different senses of the same verb requiredifferent arguments.
So when the meaning ofthe verb is not specified, different meaningshave to be taken into account and, therefore,the task becomes more difficult.?
Statistical reason.
The disagreement in theresults comes from the nature of the statisticsthemselves.
Any statistical measure performsbetter on the most frequent cases than on theless frequent ones.
In the first experiment allpossible arguments are evaluated, includingthe less frequent ones, whereas in the secondexperiment only the possible arguments foundin the piece of corpus used were evaluated.
Inmost of the cases, the possible argumentsfound were the most frequent ones.At this point it is important to note that thesystem deals with non-structural cases.
In Basquethere are three structural cases (ergative, absolutiveand dative) which are special because, when theyappear, they are always arguments.
Theycorrespond to the subject, direct object and indirectobject functions.
These cases are not veryconflictive about argumenthood, mainly because inBasque the auxiliary bears information about theirappearance in the sentence.
So they are easilyrecognized and linked to the corresponding verb.That is the reason for not including them in thiswork.
Precision and recall would improveconsiderably if they were included because theyare the most frequent cases (as statistics performwell over frequent data), and also because theshallow parser links them correctly using theinformation carried by the auxiliary.
Notice thatwe did not incorporate them because in the futurewe would like to use the subcategorizationinformation obtained for helping our parser, andthe non-structural cases are the most problematicones.4 Related workConcerning the acquisition of verbsubcategorization information, there are proposalsranging from manual examination of corpora (R.Grishman et al 1994) to fully automaticapproaches.Table 3, partially borrowed from A. Korhonen(2001), summarizes several systems onsubcategorization frame acquisition.C.
Manning (1993) presents the acquisition ofsubcategorization frames from unlabelled textcorpora.
He uses a stochastic tagger and a finitestate parser to obtain instances of verbs with theiradjacent elements (either arguments or adjuncts),and then a statistical filtering phase producessubcategorization frames (from a set of previouslydefined 19 frames) for each verb.T.
Briscoe and J. Carroll (1997) describe agrammar based experiment for the extraction ofsubcategorization frames with their associatedrelative frequencies, obtaining 76.6% precisionand 43.4% recall.
Regarding evaluation, they usethe ANLT and COMLEX Syntax dictionaries asgold standard.
They also performed evaluation ofcoverage over a corpus.
For our work, we couldnot make use of any previous information onsubcategorization, because there is nothing like asubcategorization dictionary for Basque.A.
Sarkar and D. Zeman (2000) report resultson the automatic acquisition of subcategorizationframes for verbs in Czech, a free word orderlanguage.
The input to the system is a set ofmanually annotated sentences from a treebank,where each verb is linked with its dependents(without distinguishing arguments and adjuncts).The task consists in iteratively eliminatingelements from the possible frames with the aim ofremoving adjuncts.
For evaluation, they give anestimate of how many of the obtained framesappear in a set of 500 sentences where dependentswere annotated manually, showing animprovement from a baseline of 57% (all elementsare adjuncts) to 88%.Comparing this approach to our work, we mustpoint out that Sarkar and Zeman's data does notcome from raw corpus, and thus they do not dealwith the problem of noise coming from the parsingphase.
Their main limitation comes by relying on atreebank, which is an expensive resource.D.
Kawahara et al (2001) use a full syntacticparser to obtain a case frame dictionary forJapanese, where arguments are distinguished bytheir syntactic case, including their headword(selectional restrictions).
The resulting case framecomponents are selected by a frequency threshold.Table 3.
Summary of several systems on subcategorization information.Method Numberof framesNumberof verbsLinguisticresourcesF-Score(evaluationbased on agold standard)Coverage on acorpusC.
Manning (1993) 19 200 POS tagger + simplefinite state parser58T.
Briscoe & J.Carroll (1997)161 14 Full parser 55A.
Sarkar & D.Zeman (2000)137 914 Annotated treebank - 88D.
Kawahara et al(2001)- 23,497 Full parser  82 accuracyM.
Maragoudakis etal.
(2001)- 47 Simple phrasechunker77This paper - 640 Morph.
Analyzer +Phrase Chunker +Finite State Parser55 95M.
Maragoudakis et al (2001) apply amorphological analyzer and phrase chunkingmodule to acquire subcategorization frames forModern Greek.
In contrast to this work, they usedifferent machine learning techniques.
They claimthat Bayesian Belief Networks are the bestlearning technique.P.
Merlo and M. Leybold (2001) presentlearning experiments for automatic distinction ofarguments and adjuncts, applied to the case ofprepositional phrases attached to a verb.
She usesdecision trees tested on a set of 400 verb instanceswith a single PP, reaching an accuracy of 86.5%over a baseline of 74%.Note that both Manning and Merlo andLeybold's systems learn from contexts with justone PP (maximum) per verb (finite state filter).Our system learns from contexts with up to 5 PPs.Furthermore, we distinguish 48 different kinds ofcases, hence the number of combinations isconsiderably bigger.Regarding the parsing phase, the systemspresented so far are heterogeneous.
WhileManning, Merlo and Leybold and Maragoudakis etal.
use very simple parsing techniques, Briscoe andCarroll and Kawahara et al use sophisticatedparsers.
Our system can be placed between thesetwo approaches.
The result of the shallow parsingis not simple in that it relies on a robustmorphological analysis and disambiguation.Remember that Basque is an agglutinativelanguage with strong morphology and, therefore,this stage is particularly relevant.
Moreover, thefinite state filter we used for parsing is verysophisticated (L. Karttunen et al 1997, I.Aldezabal et al 2001), compared to Manning's.ConclusionThis work describes an initial effort to obtainsubcategorization information for Basque.
Tosuccessfully perform this task we had to go deeperthan mere syntactic categories (NP, PP, ?
)enriching the set of possible arguments to 48different classes.
This leads to quite sparse data.Together with sparseness, another problemcommon to every subcategorization acquisitionsystem is that of noise, coming from adjuncts andincorrectly parsed elements.
For that reason, wedefined subcategorization acquisition in terms ofdistinguishing between arguments and adjuncts.The system presented was applied to anewspaper corpus.
Subcategorization acquisition ishighly associated to semantics in that differentsenses of a verb will most of the times showdifferent subcategorization information.
Thus, thetask of learning subcategorization information isinfluenced by the corpus.
As for the evaluation ofthis work, we carried out two different kinds ofevaluation.
This way, we verified the relevance ofsemantics in this kind of task.For the future, we plan to incorporate theinformation resulting from this work in our parsingsystem.
We hope that this will lead to better resultsin parsing.
Consequently, we would get bettersubcategorization information, in a bootstrappingcycle.
We also plan to improve the results by usingsemantic information as proposed in A. Korhonen(2001).AcknowledgementsThis work has been supported by the Departmentof Economy of the Government of Gipuzkoa, TheUniversity of the Basque Country, the Departmentof Education of the Basque Government and theCommission of Science and Technology of theSpanish Government.ReferencesI.
Aduriz, J. M. Arriola, X. Artola, A.
D?az deIlarraza, K. Gojenola and M. Maritxalar (1997)Morphosyntactic disambiguation for Basque based onthe Constraint Grammar Formalism.
Conference onRecent Advances in Natural Language Processing(RANLP).I.
Alegria, X. Artola, K. Sarasola and M. Urkia (1996)Automatic morphological analysis of Basque.
Literaryand Linguistic Computing.
11 (4), Oxford University.I.
Aldezabal, K. Gojenola and K. Sarasola (2000) ABootstrapping Approach to Parser Development.International Workshop on Parsing Technologies(IWPT), Trento.I.
Aldezabal, M. Aranzabe, A. Atutxa, K. Gojenola,M.
Oronoz M. and Sarasola K. (2001) Application offinite-state transducers to the acquisition of verbsubcategorization information.
Finite State Methodsin Natural Language Processing, ESSLLI Workshop,Helsinki.M.
R. Brent (1993) From Grammar to Lexicon:Unsupervised Learning of Lexical Syntax.Computational Linguistics, 19:243-262.T.
Briscoe and J. Carroll  (1997) Automatic Extractionof Subcategorization from Corpora.
ANLP-97:356-363.J.
Carroll, G. Minnen and T. Briscoe (1998) CanSubcategorization Probabilities Help a StatisticalParser?
Proceedings of the 6th ACL/SIGDATWorkshop on Very Large Corpora, Montreal.N.
Chomsky (1995) The Minimalist Program.Cambridge MA, MIT Press.T.
Dunning  (1993) Accurate Methods for theStatistics of Surprise and Coincidence.
ComputationalLinguistics 19, 1J.M.
Gawron (1986) Situations and prepositions.Linguistics and Philosophy 9(3), 327-382.J.
Grimshaw (1990) Argument Structure.
Cambridge,MA, MIT Press.R.
Grishman, C. Macleod, A. Meyers (1994) ComlexSyntax: Building a Computational Lexicon.
COLING-94.F.
Karlsson, A. Voutilainen, J. Heikkila, A. Anttila(1995) Constraint Grammar: A Language-independent System for Parsing Unrestricted Text.Mouton de Gruyter.L.
Karttunen, J.P. Chanod, G. Grefenstette, A. Schiller(1997) Regular Expressions For LanguageEngineering.
Natural Language Engineering.D.
Kawahara, N. Kaji and S. Kurohashi (2000)Japanese Case Structure Analysis by UnsupervisedConstruction of a Case Frame Dictionary.
COLING-2000, Saarbrucken.A.
Korhonen (2001) Subcategorization acquisition.Unpublished  PhD Thesis, University of Cambridge.K.
Koskenniemi (1983) Two-level Morphology: Ageneral Computational Model for Word-FormRecognition and Production.
PhD thesis, Universityof Helsinki.J.
Kuhn, J. Eckle-Kohlerm and C. Rohrer (1998)Lexicon Acquisition with and for Symbolic NLP-Systems -- a Bootstrapping Approach.
FirstInternational Conference on Language Resources andEvaluation (LREC98), Granada.C.D.
Manning (1993) Automatic Acquisition of aLarge Subcategorization Dictionary from Corpora.Proceedings of the 31th ACL.C.D.
Manning and H. Sch?tze (1999) Foundations ofStatistical Natural Language Processing.
The MITPress, Cambridge, Massachusetts.M.
Maragoudakis, K. Kermanidis, N. Fakotakis andG.
Kokkinakis (2001) Learning Automatic Acquisitionof Subcategorization Frames using BayesianInference and Support Vector Machines.
The 2001IEEE International Conference on Data Mining,IMDC'01, San Jos?.P.
Merlo and M. Leybold (2001) AutomaticDistinction of Arguments and Modifiers: the Case ofPrepositional Phrases.
EACL-2001, Toulousse.T.
Pederssen (1996) Fishing for Exactness In theProceeding of the South-Central SAS User GroupConference (SCSUG-96).C.
Pollard and I.
Sag (1987) An information basedSyntax and Semantics, volume 13.
CSLI lecture.Notes, Standford University.A.
Sarkar and D. Zeman (2000) Automatic Extractionof Subcategorization Frames for Czech.
COLING-2000, Saarbrucken.C.
Schutze (1995) PP Attachment and Argumenthood.MIT Working Papers in Linguistics.C.
Verspoor (1997) Contextually-Dependent LexicalSemantics.
PhD thesis, Brandeis University, MA.
