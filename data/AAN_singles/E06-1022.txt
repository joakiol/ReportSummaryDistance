Addressee Identification in Face-to-Face MeetingsNatasa Jovanovic, Rieks op den Akker and Anton NijholtUniversity of TwentePO Box 217 EnschedeThe Netherlands{natasa,infrieks,A.Nijholt}@ewi.utwente.nlAbstractWe present results on addressee identifica-tion in four-participants face-to-face meet-ings using Bayesian Network and NaiveBayes classifiers.
First, we investigatehow well the addressee of a dialogueact can be predicted based on gaze, ut-terance and conversational context fea-tures.
Then, we explore whether informa-tion about meeting context can aid classi-fiers?
performances.
Both classifiers per-form the best when conversational contextand utterance features are combined withspeaker?s gaze information.
The classifiersshow little gain from information aboutmeeting context.1 IntroductionAddressing is an aspect of every form of commu-nication.
It represents a form of orientation anddirectionality of the act the current actor performstoward the particular other(s) who are involved inan interaction.
In conversational communicationinvolving two participants, the hearer is always theaddressee of the speech act that the speaker per-forms.
Addressing, however, becomes a real issuein multi-party conversation.The concept of addressee as well as a vari-ety of mechanisms that people use in addressingtheir speech have been extensively investigatedby conversational analysts and social psycholo-gists (Goffman, 1981a; Goodwin, 1981; Clark andCarlson, 1982).Recently, addressing has received consider-able attention in modeling multi-party interac-tion in various domains.
Research on au-tomatic addressee identification has been con-ducted in the context of mixed human-humanand human-computer interaction (Bakx et al,2003; van Turnhout et al, 2005), human-human-robot interaction (Katzenmaier et al, 2004), andmixed human-agents and multi-agents interaction(Traum, 2004).
In the context of automatic anal-ysis of multi-party face-to-face conversation, Ot-suka et al (2005) proposed a framework forautomating inference of conversational structurethat is defined in terms of conversational roles:speaker, addressee and unaddressed participants.In this paper, we focus on addressee identifica-tion in a special type of communication, namely,face-to-face meetings.
Moreover, we restrict ouranalysis to small group meetings with four partic-ipants.
Automatic analysis of recorded meetingshas become an emerging domain for a range ofresearch focusing on different aspects of interac-tions among meeting participants.
The outcomesof this research should be combined in a targetedapplication that would provide users with usefulinformation about meetings.
For answering ques-tions such as ?Who was asked to prepare a presen-tation for the next meeting??
or ?Were there anyarguments between participants A and B?
?, somesort of understanding of dialogue structure is re-quired.
In addition to identification of dialogueacts that participants perform in multi-party dia-logues, identification of addressees of those acts isalso important for inferring dialogue structure.There are many applications related to meetingresearch that could benefit from studying address-ing in human-human interactions.
The resultscan be used by those who develop communicativeagents in interactive intelligent environments andremote meeting assistants.
These agents need torecognize when they are being addressed and howthey should address people in the environment.This paper presents results on addressee identi-169fication in four-participants face-to-face meetingsusing Bayesian Network and Naive Bayes classi-fiers.
The goals in the current paper are (1) tofind relevant features for addressee classificationin meeting conversations using information ob-tained from multi-modal resources - gaze, speechand conversational context, (2) to explore to whatextent the performances of classifiers can be im-proved by combining different types of featuresobtained from these resources, (3) to investigatewhether the information about meeting contextcan aid the performances of classifiers, and (4) tocompare performances of the Bayesian Networkand Naive Bayes classifiers for the task of ad-dressee prediction over various feature sets.2 Addressing in face-to-face meetingsWhen a speaker contributes to the conversation, allthose participants who happen to be in perceptualrange of this event will have ?some sort of partic-ipation status relative to it?.
The conversationalroles that the participants take in a given conversa-tional situation make up the ?participation frame-work?
(Goffman, 1981b).Goffman (1976) distinguished three basic kindsof hearers: those who overhear, whether or nottheir unratified participation is unintentional or en-couraged; those who are ratified but are not specif-ically addressed by the speaker (also called unad-dressed recipients (Goffman, 1981a)); and thoseratified participants who are addressed.
Ratifiedparticipants are those participants who are allowedto take part in conversation.
Regarding hearers?roles in meetings, we are focused only on ratifiedparticipants.
Therefore, the problem of addresseeidentification amounts to the problem of distin-guishing addressed from unaddressed participantsfor each dialogue act that speakers perform.Goffman (1981a) defined addressees as those?ratified participants () oriented to by the speakerin a manner to suggest that his words are particu-larly for them, and that some answer is thereforeanticipated from them, more so than from the otherratified participants?.
According to this, it is thespeaker who selects his addressee; the addressee isthe one who is expected by the speaker to react onwhat the speaker says and to whom, therefore, thespeaker is giving primary attention in the presentact.In meeting conversations, a speaker may ad-dress his utterance to the whole group of partici-pants present in the meeting, or to a particular sub-group of them, or to a single participant in partic-ular.
A speaker can also just think aloud or mum-ble to himself without really addressing anybody(e.g.
?What else do I want to say??
(while try-ing to evoke more details about the issue that he ispresenting)).
We excluded self-addressed speechfrom our study.Addressing behavior is behavior that speakersshow to express to whom they are addressing theirspeech.
It depends on the course of the conver-sation, the status of attention of participants, theircurrent involvement in the discussion as well ason what the participants know about each others?roles and knowledge, whether explicit addressingbehavior is called for.
Using a vocative is the ex-plicit verbal way to address someone.
In somecases the speaker identifies the addressee of hisspeech by looking at the addressee, sometimes ac-companying this by deictic hand gestures.
Ad-dressees can also be designated by the manner ofspeaking.
For example, by whispering, a speakercan select a single individual or a group of peopleas addressees.
Addressees are often designated bythe content of what is being said.
For example,when making the suggestion ?We all have to de-cide together about the design?, the speaker is ad-dressing the whole group.In meetings, people may perform various groupactions (termed as meeting actions) such as pre-sentations, discussions or monologues (McCowanet al, 2003).
A type of group action that meetingparticipants perform may influence the speaker?saddressing behavior.
For example, speakers mayshow different behavior during a presentation thanduring a discussion when addressing an individ-ual: regardless of the fact that a speaker hasturned his back to a participant in the audienceduring a presentation, he most probably addresseshis speech to the group including that participant,whereas the same behavior during a discussion, inmany situations, indicates that that participant isunaddressed.In this paper, we focus on speech and gaze as-pects of addressing behavior as well as on con-textual aspects such as conversational history andmeeting actions.3 Cues for addressee identificationIn this section, we present our motivation for fea-ture selection, referring also to some existing work170on the examination of cues that are relevant for ad-dressee identification.Adjacency pairs and addressing - Adjacencypairs (AP) are minimal dialogic units that con-sist of pairs of utterances called ?first pair-part?
(or a-part) and the ?second pair-part?
(or b-part)that are produced by different speakers.
Examplesinclude question-answers or statement-agreement.In the exploration of the conversational organiza-tion, special attention has been given to the a-partsthat are used as one of the basic techniques for se-lecting a next speaker (Sacks et al, 1974).
For ad-dressee identification, the main focus is on b-partsand their addressees.
It is to be expected that thea-part provides a useful cue for identification ofaddressee of the b-part (Galley et al, 2004).
How-ever, it does not imply that the speaker of the a-partis always the addressee of the b-part.
For example,A can address a question to B, whereas B?s replyto A?s question is addressed to the whole group.
Inthis case, the addressee of the b-part includes thespeaker of the a-part.Dialogue acts and addressing When designingan utterance, a speaker intends not only to per-form a certain communicative act that contributesto a coherent dialogue (in the literature referredto as dialogue act), but also to perform that act to-ward the particular others.
Within a turn, a speakermay perform several dialogue acts, each of thosehaving its own addressee ( e.g.
I agree with you[agreement; addressed to a previous speaker] butis this what we want [information request; ad-dressed to the group]).
Dialogue act types canprovide useful information about addressing typessince some types of dialogue acts -such as agree-ments or disagreements- tend to be addressed toan individual rather than to a group.
More infor-mation about the addressee of a dialogue can beinduced by combining the dialogue act informa-tion with some lexical markers that are used as ad-dressee ?indicators?
(e.g.
you, we, everybody, allof you) (Jovanovic and op den Akker, 2004).Gaze behavior and addressing Analyzingdyadic conversations, researchers into socialinteraction observed that gaze in social inter-action is used for several purposes: to controlcommunication, to provide a visual feedback, tocommunicate emotions and to communicate thenature of relationships (Kendon, 1967; Argyle,1969).Recent studies into multi-party interaction em-phasized the relevance of gaze as a means of ad-dressing.
Vertegaal (1998) investigated to what ex-tent the focus of visual attention might function asan indicator for the focus of ?dialogic attention?
infour-participants face-to-face conversations.
?Di-alogic attention?
refers to attention while listeningto a person as well as attention while talking toone or more persons.
Empirical findings show thatwhen a speaker is addressing an individual, thereis 77% chance that the gazed person is addressed.When addressing a triad, speaker gaze seems to beevenly distributed over the listeners in the situa-tion where participants are seated around the ta-ble.
It is also shown that on average a speakerspends significantly more time gazing at an indi-vidual when addressing the whole group, than atothers when addressing a single individual.
Whenaddressing an individual, people gaze 1.6 timesmore while listening (62%) than while speaking(40%).
When addressing a triad the amount ofspeaker gaze increases significantly to 59%.
Ac-cording to all these estimates, we can expect thatgaze directional cues are good indicators for ad-dressee prediction.However, these findings cannot be generalizedin the situations where some objects of interest arepresent in the conversational environment, sinceit is expected that the amount of time spent look-ing at the persons will decrease significantly.
Asshown in (Bakx et al, 2003), in a situation wherea user interacts with a multimodal information sys-tem and in the meantime talks to another person,the user looks most of the time at the system, bothwhen talking to the system (94%) and when talk-ing to the user (57%).
Also, another person looksat the system in 60% of cases when talking to theuser.
Bakx et al (2003) also showed that some im-provement in addressee detection can be achievedby combining utterance duration with gaze.In meeting conversations, the contribution ofthe gaze direction to addressee prediction is alsoaffected by the current meeting activity and seat-ing arrangement (Jovanovic and op den Akker,2004).
For example, when giving a presentation,a speaker most probably addresses his speech tothe whole audience, although he may only look ata single participant in the audience.
A seating ar-rangement determines a visible area for each meet-ing participant.
During a turn, a speaker mostlylooks at the participants who are in his visible area.171Moreover, the speaker frequently looks at a sin-gle participant in his visual area when addressinga group.
However, when he wants to address a sin-gle participant outside his visual area, he will oftenturn his body and head toward that participant.In this paper, we explored not only the effec-tiveness of the speaker?s gaze direction, but alsothe effectiveness of the listeners?
gaze directionsas cues for addressee prediction.Meeting context and addressing As Goff-man (1981a) has noted, ?the notion of a conver-sational encounter does not suffice in dealing withthe context in which words are spoken; a socialoccasion involving a podium event or no speechevent at all may be involved, and in any case, thewhole social situation, the whole surround, mustalways be considered?.
A set of various meet-ing actions that participants perform in meetings isone aspect of the social situation that differentiatesmeetings from other contexts of talk such as ordi-nary conversations, interviews or trials.
As notedabove, it influences addressing behavior as wellas the contribution of gaze to addressee identifi-cation.
Furthermore, distributions of addressingtypes vary for different meeting actions.
Clearly,the percentage of the utterances addressed to thewhole group during a presentation is expected tobe much higher than during a discussion.4 Data collectionTo train and test our classifiers, we used a smallmultimodal corpus developed for studying ad-dressing behavior in meetings (Jovanovic et al,2005).
The corpus contains 12 meetings recordedat the IDIAP smart meeting room in the researchprogram of the M41 and AMI projects2.
Theroom has been equipped with fully synchronizedmulti-channel audio and video recording devices,a whiteboard and a projector screen.
The seatingarrangement includes two participants at each oftwo opposite sides of the rectangular table.
Thetotal amount of the recorded data is approximately75 minutes.
For experiments presented in this pa-per, we have selected meetings from the M4 datacollection.
These meetings are scripted in terms oftype and schedule of group actions, but content isnatural and unconstrained.The meetings are manually annotated with dia-logue acts, addressees, adjacency pairs and gaze1http://www.m4project.org2http://www.amiproject.orgdirection.
Each type of annotation is describedin detail in (Jovanovic et al, 2005).
Additionally,the available annotations of meeting actions for theM4 meetings3 were converted into the corpus for-mat and included in the collection.The dialogue act tag set employed for the cor-pus creation is based on the MRDA (MeetingRecorder Dialogue Act) tag set (Dhillon et al,2004).
The MRDA tag set represents a modifi-cation of the SWDB-DAMSL tag set (Jurafsky etal., 1997) for an application to multi-party meet-ing dialogues.
The tag set used for the corpus cre-ation is made by grouping the MRDA tags into 17categories that are divided into seven groups: ac-knowledgments/backchannels, statements, ques-tions, responses, action motivators, checks and po-liteness mechanisms.
A mapping between this tagset and the MRDA tag set is given in (Jovanovicet al, 2005).
Unlike MRDA where each utteranceis marked with a label made up of one or moretags from the set, each utterance in the corpus ismarked as Unlabeled or with exactly one tagfrom the set.
Adjacency pairs are labeled by mark-ing dialogue acts that occur as their a-part and b-part.Since all meetings in the corpus consist of fourparticipants, the addressee of a dialogue act is la-beled as Unknown or with one of the followingaddressee tags: individual Px, a subgroup of par-ticipants Px,Py or the whole audience Px,Py,Pz.Labeling gaze direction denotes labeling gazedtargets for each meeting participants.
As the onlytargets of interest for addressee identification aremeeting participants, the meetings were annotatedwith the tag set that contains tags that are linked toeach participant Px and the NoTarget tag that isused when the speaker does not look at any of theparticipants.Meetings are annotated with a set of six meet-ing actions described in (McCowan et al, 2003):monologue, presentation, white-board, discussion,consensus, disagreement and note-taking.Reliability of the annotation schema As re-ported in (Jovanovic et al, 2005), gaze annota-tion has been reproduced reliably (segmentation80.40% (N=939); classification ?
= 0.95).
Table1 shows reliability of dialogue act segmentationas well as Kappa values for dialogue act and ad-dressee classification for two different annotation3http://mmm.idiap.ch/172groups that annotated two different sets of meetingdata.Group Seg(%) N DA(?)
ADD(?
)B&E 91.73 377 0.77 0.81M&R 86.14 367 0.70 0.70Table 1: Inter-annotator agreement on DA and ad-dressee annotation: N- number of agreed segments5 Addressee classificationIn this section we present the results on addresseeclassification in four-persons face-to-face meet-ings using Bayesian Network and Naive Bayesclassifiers.5.1 Classification taskIn a dialogue situation, which is an event whichlasts as long as the dialogue act performed by thespeaker in that situation, the class variable is theaddressee of the dialogue act (ADD).
Since thereare only a few instances of subgroup addressing inthe data, we removed them from the data set andexcluded all possible subgroups of meeting par-ticipants from the set of class values.
Therefore,we define addressee classifiers to identify one ofthe following class values: individual Px wherex ?
{0,1,2,3} and ALLPwhich denotes the wholegroup.5.2 Feature setTo identify the addressee of a dialogue act weinitially used three sorts of features: conversa-tional context features (later referred to as contex-tual features), utterance features and gaze features.Additionally, we conducted experiments with anextended feature set including a feature that con-veys information about meeting context.Contextual features provide information aboutthe preceding utterances.
We experimented withusing information about the speaker, the addresseeand the dialogue act of the immediately precedingutterance on the same or a different channel (SP-1, ADD-1, DA-1) as well as information aboutthe related utterance (SP-R, ADD-R, DA-R).
A re-lated utterance is the utterance that is the a-part ofan adjacency pair with the current utterance as theb-part.
Information about the speaker of the cur-rent utterance (SP) has also been included in thecontextual feature set.As utterance features, we used a subset of lex-ical features presented in (Jovanovic and op denAkker, 2004) as useful cues for determiningwhether the utterance is single or group addressed.The subset includes the following features:?
does the utterance contain personal pronouns ?we?
or?you?, both of them, or neither of them??
does the utterance contain possessive pronouns or pos-sessive adjectives (?your/yours?
or ?our/ours?
), theircombination or neither of them??
does the utterance contain indefinite pronouns such as?somebody?, ?someone?, ?anybody?, ?anyone?, ?ev-erybody?
or ?everyone???
does the utterance contain the name of participant Px?Utterance features also include information aboutthe utterance?s conversational function (DA tag)and information about utterance duration i.e.whether the utterance is short or long.
In our ex-periments, an utterance is considered as a short ut-terance, if its duration is less than or equal to 1sec.We experimented with a variety of gaze fea-tures.
In the first experiment, for each participantPx we defined a set of features in the form Px-looks-Py and Px-looks-NT where x,y ?
{0,1,2,3}and x 6= y; Px-looks-NT represents that partici-pant Px does not look at any of the participants.The value set represents the number of times thatspeaker Px looks at Py or looks away during thetime span of the utterance: zero for 0, one for 1,two for 2 and more for 3 or more times.
In thesecond experiment, we defined a feature set thatincorporates only information about gaze directionof the current speaker (SP-looks-Px and SP-looks-NT) with the same value set as in the first experi-ment.As to meeting context, we experimented withdifferent values of the feature that represents themeeting actions (MA-TYPE).
First, we used a fullset of speech based meeting actions that was ap-plied for the manual annotation of the meetings inthe corpus: monologue, discussion, presentation,white-board, consensus and disagreement.
As theresults on modeling group actions in meetings pre-sented in (McCowan et al, 2003) indicate thatconsensus and disagreements were mostly mis-classified as discussion, we have also conductedexperiments with a set of four values for MA-TYPE, where consensus, disagreement and dis-cussion meeting actions were grouped in one cat-egory marked as discussion.1735.3 Results and DiscussionsTo train and test the addressee classifiers, we usedthe hand-annotated M4 data from the corpus.
Af-ter we had discarded the instances labeled withUnknown or subgroup addressee tags, there were781 instances left available for the experiments.The distribution of the class values in the selecteddata is presented in Table 2.ALLP P0 P1 P2 P340.20% 13.83% 17.03% 15.88% 13.06%Table 2: Distribution of addressee valuesFor learning the Bayesian Network structure,we applied the K2 algorithm (Cooper and Her-skovits, 1992).
The algorithm requires an orderingon the observable features; different ordering leadsto different network structures.
We conducted ex-periments with several orderings regarding featuretypes as well as with different orderings regardingfeatures of the same type.
The obtained classifi-cation results for different orderings were nearlyidentical.
For learning conditional probability dis-tributions, we used the algorithm implemented intheWEKA toolbox4 that produces direct estimatesof the conditional probabilities.5.3.1 Initial experiments without meetingcontextThe performances of the classifiers are mea-sured using different feature sets.
First, we mea-sured the performances of classifiers using utter-ance features, gaze features and contextual fea-tures separately.
Then, we conducted experimentswith all possible combinations of different types offeatures.
For each classifier, we performed 10-foldcross-validation.
Table 3 summarizes the accura-cies of the classifiers (with 95% confidence inter-val) for different feature sets (1) using gaze infor-mation of all meeting participants and (2) usingonly information about speaker gaze direction.The results show that the Bayesian Networkclassifier outperforms the Naive Bayes classifierfor all feature sets, although the difference is sig-nificant only for the feature sets that include con-textual features.For the feature set that contains only informa-tion about gaze behavior combined with infor-mation about the speaker (Gaze+SP), both clas-sifiers perform significantly better when exploit-ing gaze information of all meeting participants.4http://www.cs.waikato.ac.nz/ ml/weka/In other words, when using solely focus of visualattention to identify the addressee of a dialogueact, listeners?
focus of attention provides valuableinformation for addressee prediction.
The sameconclusion can be drawn when adding informa-tion about utterance duration to the gaze featureset (Gaze+SP+Short), although for the BayesianNetwork classifier the difference is not significant.For all other feature sets, the classifiers do not per-form significantly different when including or ex-cluding the listeners gaze information.
Even more,both classifiers perform better using only speakergaze information in all cases except when com-bined utterance and gaze features are exploited(Utterance+Gaze+SP).The Bayesian network and Naive Bayes clas-sifiers show the same changes in the perfor-mances over different feature sets.
The re-sults indicate that the selected utterance fea-tures are less informative for addressee predic-tion (BN:52.62%, NB:52.50%) compared to con-textual features (BN:73.11%; NB:68.12%) or fea-tures of gaze behavior (BN:66.45%, NB:64.53%).The results also show that adding the informa-tion about the utterance duration to the gaze fea-tures, slightly increases the accuracies of the clas-sifiers (BN:67.73%, NB:65.94%), which confirmsfindings presented in (Bakx et al, 2003).
Com-bining the information from the gaze and speechchannels significantly improves the performancesof the classifiers (BN:70.68%; NB:69.78%) incomparison to performances obtained from eachchannel separately.
Furthermore, higher accura-cies are gained when adding contextual features tothe utterance features (BN:76.82%; NB:72.21%)and even more to the features of gaze behavior(BN:80.03%, NB:77.59%).
As it is expected, thebest performances are achieved by combining allthree types of features (BN:82.59%, NB:78.49%),although not significantly better compared to com-bined contextual and gaze features.We also explored how well the addressee can bepredicted excluding information about the relatedutterance (i.e.
AP information).
The best perfor-mances are achieved combining speaker gaze in-formation with contextual and utterance features(BN:79.39%; NB:76.06%).
A small decrease inthe classification accuracies when excluding APinformation (about 3%) indicates that remainingcontextual, utterance and gaze features capturemost of the useful information provided by AP.174Baysian Networks Naive BayesFeature sets Gaze All Gaze SP Gaze All Gaze SPAll Features 81.05% (?2.75) 82.59% (?2.66) 78.10% (?2.90) 78.49% (?2.88)Context 73.11% (?3.11) 68.12% (?3.27)Utterance+SP 52.62% (?3.50) 52.50% (?3.50)Gaze+SP 66.45% (?3.31) 62.36% (?3.40) 64.53% (?3.36) 59.02% (?3.45)Gaze+SP+Short 67.73% (?3.28) 66.45% (?3.31) 65.94% (?3.32) 61.46% (?3.41)Context+Utterance 76.82% (?2.96) 72.21% (?3.14)Context+Gaze 79.00% (?2.86) 80.03% (?2.80) 74.90% (?3.04) 77.59% (?2.92)Utterance+Gaze+SP 70.68% (?3.19) 70.04% (?3.21) 69.78% (?3.22) 68.63% (?3.25)Table 3: Classification results for Bayesian Network and Naive Bayes classifiers using gaze informationof all meeting participants (Gaze All) and using speaker gaze information (Gaze SP)Error analysis Further analysis of confusionmatrixes for the best performed BN and NB clas-sifiers, show that most misclassifications were be-tween addressing types (individual vs. group):each Px was more confused with ALLP than withPy.
A similar type of confusion is observed be-tween human annotators regarding addressee an-notation (Jovanovic et al, 2005).
Out of all mis-classified cases for each classifier, individual typesof addressing (Px) were, in average, misclassifiedwith addressing the group (ALLP) in 73% casesfor NB, and 68% cases for BN.5.3.2 Experiments with meeting contextWe examined whether meeting context informa-tion can aid the classifiers?
performances.
First,we conducted experiments using the six valuesset for the MA-TYPE feature.
Then, we exper-imented with employing the reduced set of fourtypes of meeting actions (see Section 5.2).
Theaccuracies obtained by combining the MA-TYPEfeature with contextual, utterance and gaze fea-tures are presented in Table 4.Bayesian Networks Naive BayesFeatures Gaze All Gaze SP Gaze All Gaze SPMA-6+All 81.82% 82.84% 78.74% 79.90%MA-4+All 81.69% 83.74% 78.23% 79.13%Table 4: Classification results combining MA-TYPE with the initial feature setThe results indicate that adding meeting con-text information to the initial feature set improvesslightly, but not significantly, the classifiers?
per-formances.
The highest accuracy (83.74%) isachieved using the Bayesian Network classifier bycombining the four-values MA-TYPE feature withcontextual, utterance and the speaker?s gaze fea-tures.6 Conclusion and Future workWe presented results on addressee classificationin four-participants face-to-face meetings usingBayesian Network and Naive Bayes classifiers.The experiments presented should be seen as pre-liminary explorations of appropriate features andmodels for addressee identification in meetings.We investigated how well the addressee of a di-alogue act can be predicted (1) using utterance,gaze and conversational context features alone aswell as (2) using various combinations of thesefeatures.
Regarding gaze features, classifiers?
per-formances are measured using gaze directionalcues of the speaker only as well as of all meetingparticipants.
We found that contextual informa-tion aids classifiers?
performances over gaze in-formation as well as over utterance information.Furthermore, the results indicate that selected ut-terance features are the most unreliable cues foraddressee prediction.
The listeners?
gaze direc-tion provides useful information only in the situa-tion where gaze features are used alone.
Combina-tions of features from various resources increasesclassifiers?
performances in comparison to perfor-mances obtained from each resource separately.However, the highest accuracies for both classi-fiers are reached by combining contextual and ut-terance features with speaker?s gaze (BN:82.59%,NB:78.49%).
We have also explored the ef-fect of meeting context on the classification task.Surprisingly, addressee classifiers showed littlegain from the information about meeting actions(BN:83.74%, NB:79.90%).
For all feature sets,the Bayesian Network classifier outperforms theNaive Bayes classifier.In contrast to Vertegaal (1998) and Otsuka etal.
(2005) findings, where it is shown that gazecan be a good predictor for addressee in four-participants face-to-face conversations, our results175show that in four-participants face-to-face meet-ings, gaze is less effective as an addressee indi-cator.
This can be due to several reasons.
First,they used different seating arrangements which isimplicated in the organization of gaze.
Second,our meeting environment contains attentional ?dis-tracters?
such as whiteboard, projector screen andnotes.
Finally, during a meeting, in contrast to anordinary conversation, participants perform vari-ous meeting actions which may influence gaze asan aspect of addressing behavior.We will continue our work on addressee identi-fication on the large AMI data collection that iscurrently in production.
The AMI corpus con-tains more natural, scenario-based, meetings thatinvolve groups focused on the design of a TV re-mote control.
Some initial experiments on theAMI pilot data show that additional challenges foraddressee identification on the AMI data are: rolesthat participants play in the meetings (e.g.
projectmanager or marketing expert) and additional at-tentional ?distracters?
present in the meeting roomsuch as, the task object at first place and laptops.This means that a richer feature set should be ex-plored to improve classifiers?
performances on theAMI data including, for example, the backgroundknowledge about participants?
roles.
We will alsofocus on the development of new models that bet-ter handle conditional and contextual dependen-cies among different types of features.AcknowledgmentsThis work was partly supported by the EuropeanUnion 6th FWP IST Integrated Project AMI (Aug-mented Multi-party Interaction, FP6-506811, pub-lication AMI-153).ReferencesM.
Argyle.
1969.
Social Interaction.
London: Tavis-tock Press.I.
Bakx, K. van Turnhout, and J. Terken.
2003.
Facialorientation during multi-party interaction with infor-mation kiosks.
In Proc.
of INTERACT.H.
H. Clark and B. T. Carlson.
1982.
Hearers andspeech acts.
Language, 58:332?373.G.
Cooper and E. Herskovits.
1992.
Bayesian methodfor the induction of probabilistic networks fromdata.
Machine Learning, 9:309?347.R.
Dhillon, S. Bhagat, H. Carvey, and E. Shriberg.2004.
Meeting recorder project: Dialogue act label-ing guide.
Technical report, ICSI, Berkeley, USA.M.
Galley, K. McKeown, J. Hirschberg, andE.
Shriberg.
2004.
Identifying agreement anddisagreement in conversational speech: Use ofbayesian networks to model pragmatic dependen-cies.
In Proc.
of 42nd Meeting of the ACL.E.
Goffman.
1976.
Replies and responses.
Languagein Society, 5:257?313.E.
Goffman.
1981a.
Footing.
In Forms of Talk, pages124?159.
University of Pennsylvania Press.E.
Goffman.
1981b.
Forms of Talk.
University ofPennsylvania Press, Philadelphia.C.
Goodwin.
1981.
Conversational Organiza-tion: Interaction Between Speakers and Hearers.NY:Academic Press.N.
Jovanovic and R. op den Akker.
2004.
Towardsautomatic addressee identification in multi-party di-alogues.
In Proc of the 5th SIGDial.N.
Jovanovic, R. op den Akker, and A. Nijholt.
2005.A corpus for studying addressing behavior in face-to-face meetings.
In Proc.
of the 6th SIGDial.D.
Jurafsky, L. Shriberg, and D. Biasca.
1997.
Switch-board swbd-damsl shallow-discourse-function an-notation coders manual, draft 13.
Technical report,University of Colorado, Institute of Cognitive Sci-ence.M.
Katzenmaier, R. Stiefelhagen, and T. Schultz.
2004.Identifying the addressee in human-human-robot in-teractions based on head pose and speech.
In Proc.of ICMI.A.
Kendon.
1967.
Some functions of gaze direction insocial interaction.
Acta Psychologica, 32:1?25.I.
McCowan, S. Bengio, D. Gatica-Perez, G. Lathoud,F.
Monay, D. Moore, P. Wellner, and H. Bourlard.2003.
Modeling human interactions in meetings.
InProc.
IEEE ICASSP.K.
Otsuka, Y. Takemae, J. Yamato, and H. Murase.2005.
A probabilistic inference of multiparty-conversation structure based on markov-switchingmodels of gaze patterns, head directions, and utter-ances.
In Proc.
of ICMI.H.
Sacks, E. A. Schegloff, and G. Jefferson.
1974.A simplest systematics for the organization of turn-taking for conversation.
Language, 50:696?735.D.
Traum.
2004.
Issues in multi-party dialogues.
InF.
Dignum, editor, Advances in Agent Communica-tion, pages 201?211.
Springer-Verlag.K.
van Turnhout, J. Terken, I. Bakx, and B. Eggen.2005.
Identifying the intended addressee inmixed human-human and human-computer interac-tion from non-verbal features.
In Proc.
of ICMI.R.
Vertegaal.
1998.
Look who is talking to whom.Ph.D.
thesis, University of Twente, September.176
