Proceedings of the SIGDIAL 2014 Conference, pages 113?122,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsCombining Task and Dialogue Streams inUnsupervised Dialogue Act ModelsAysu Ezen-Can and Kristy Elizabeth BoyerDepartment of Computer ScienceNorth Carolina State Universityaezen,keboyer@ncsu.eduAbstractUnsupervised machine learning ap-proaches hold great promise for recog-nizing dialogue acts, but the performanceof these models tends to be much lowerthan the accuracies reached by supervisedmodels.
However, some dialogues, suchas task-oriented dialogues with paralleltask streams, hold rich information thathas not yet been leveraged within unsu-pervised dialogue act models.
This paperinvestigates incorporating task featuresinto an unsupervised dialogue act modeltrained on a corpus of human tutoring inintroductory computer science.
Exper-imental results show that incorporatingtask features and dialogue history fea-tures significantly improve unsuperviseddialogue act classification, particularlywithin a hierarchical framework that givesprominence to dialogue history.
Thiswork constitutes a step toward buildinghigh-performing unsupervised dialogueact models that will be used in the nextgeneration of task-oriented dialoguesystems.1 IntroductionDialogue acts represent the underlying intent of ut-terances (Austin, 1975; Searle, 1969), and consti-tute a crucial level of representation for dialoguesystems (Sridhar et al., 2009).
The task of auto-matic dialogue act classification has been exten-sively studied for decades within several domainsincluding train fares and timetables (Allen et al.,1995; Core and Allen, 1997; Crook et al., 2009;Traum, 1999), virtual personal assistants (Chenand Di Eugenio, 2013), conversational telephonespeech (Stolcke et al., 2000), Wikipedia talk pages(Ferschke et al., 2012) and as in the case of thispaper, tutorial dialogue (Serafin and Di Eugenio,2004; Forbes-Riley and Litman, 2005; Boyer etal., 2011; Dzikovska et al., 2013).Most of the prior work on dialogue act classi-fication has depended on manually applying dia-logue act tags and then leveraging supervised ma-chine learning (Di Eugenio et al., 2010; Keizeret al., 2002; Reithinger and Klesen, 1997; Ser-afin and Di Eugenio, 2004).
This process involvesengineering a dialogue act taxonomy (or using anexisting one, though domain-specific phenomenacan be difficult to capture within multi-purpose di-alogue act taxonomies) and manually annotatingeach utterance in the corpus.
Then, the taggedutterances are provided to a supervised machinelearner.
This supervised approach can achievestrong performance, in excess of 75% accuracyon manual tags, approaching the agreement levelthat is sometimes observed between human anno-tators (Sridhar et al., 2009; Serafin and Di Euge-nio, 2004; Chen and Di Eugenio, 2013).However, the supervised approach has severalmajor drawbacks, including the fact that hand-crafting dialogue act tagsets and applying themmanually tend to be bottlenecks within the re-search and design process.
To overcome thesedrawbacks, the field has recently seen growingmomentum surrounding unsupervised approaches,which do not require any manual labels duringmodel training (Crook et al., 2009; Joty et al.,2011; Lee et al., 2013).
A variety of unsupervisedmachine learning techniques have been investi-gated for dialogue act classification, and each lineof investigation has explored which features bestsupport this goal.
However, to date the best per-forming unsupervised models achieve in the rangeof 40% (Rus et al., 2012) to 60% (Joty et al., 2011)training set accuracy on manual tags, substantiallylower than the mid-70% accuracy (Sridhar et al.,2009) often achieved on testing sets with super-vised models.113In order to close this performance gap betweenunsupervised and supervised techniques, we sug-gest that it is crucial to enrich the features availableto unsupervised models.
In particular, when a di-alogue is task-oriented and includes a rich sourceof information within a parallel task stream, thesefeatures may substantially boost the ability of anunsupervised model to distinguish dialogue acts.For example, in situated dialogue, features rep-resenting the state of the physical world maybe highly influential for dialogue act modeling(Grosz and Sidner, 1986).Human tutorial dialogue, which is the domainbeing considered in the current work, often ex-hibits this structure: the task artifact is external tothe dialogue utterances themselves (in the case ofour work, this artifact is a computer program thatthe student is constructing).
Task features havealready been shown beneficial for supervised di-alogue act classification in our domain (Ha et al.,2012).
We hypothesize that including these taskfeatures within an unsupervised model will signif-icantly improve its performance.
In addition, wehypothesize that including dialogue history as aprominent feature within an unsupervised modelwill provide significant improvement.This paper represents the first investigation intocombining task and dialogue features within anunsupervised dialogue act classification model.First, we discuss representation of these task fea-tures and dialogue structure features, and comparethese representations within both flat and hierar-chical clustering approaches.
Second, we reporton experiments that demonstrate that the inclusionof task features significantly improves dialogueact classification, and that a hierarchical clusterstructure which explicitly captures dialogue his-tory performs best.
Finally, we break down themodel?s performance by dialogue act and investi-gate which features are most beneficial for distin-guishing particular acts.
These contributions con-stitute a step toward building high-performing un-supervised dialogue act models that can be used inthe next generation of task-oriented dialogue sys-tems.2 Related WorkThere is a rich body of work on dialogue act clas-sification.
Supervised approaches for dialogue actclassification aimed at improving performance byusing several features such as dialogue structureincluding position of the turn (Ferschke et al.,2012), speaker of an utterance (Tavafi et al., 2013),previous dialogue acts (Kim et al., 2010), lexicalfeatures such as words (Stolcke et al., 2000), syn-tactic features including part-of-speech tags (Ban-galore et al., 2008; Marineau et al., 2000), task-subtask structure (Boyer et al., 2010) acoustic andprosodic cues (Sridhar et al., 2009; Jurafsky et al.,1998), and body posture (Ha et al., 2012).For the growing body of work in unsuperviseddialogue act classification a subset of these fea-tures have been utilized.
The words (Crook etal., 2009), topic words (Ritter et al., 2010), func-tion words (Ezen-Can and Boyer, 2013b), begin-ning portions of utterances (Rus et al., 2012), part-of-speech tags and dependency trees (Joty et al.,2011), and state transition probabilities in Markovmodels (Lee et al., 2013) are among the list offeatures investigated for unsupervised modeling ofdialogue acts.
However, the accuracies achievedby the best of these models are well below the ac-curacies achieved by supervised techniques.
Toimprove performance of unsupervised models fortask-oriented dialogue, utilizing a combination oftask and dialogue features is a promising direction.3 CorpusThe task-oriented dialogue corpus used in thiswork was collected in a computer-mediated hu-man tutorial dialogue study.
Students (n =42) and tutors interacted through textual dialoguewithin an online learning environment for intro-ductory Java programming (Ha et al., 2012).
Thestudents were novices, never having programmedin Java previously.
The tutorial dialogue inter-face consisted of four windows, one describing thelearning task, another where students wrote pro-gramming code, beneath that the output of eithercompiling or executing the program, and finallythe textual dialogue window (Figure 1).As students and tutors interacted through thisinterface, all dialogue messages and keystroke-level task events were logged to a database.
Onlystudents could compose, compile, and execute thecode, so task actions represent student actionswhile dialogue messages were composed by bothparticipants.
The corpus contains six lessons foreach student-tutor pair, of which only the first les-son was annotated with dialogue act tags (?=0.80).This annotated set contains 5,705 utterances(4,065 tutor and 1,640 student).
The average num-114Figure 1: The tutorial dialogue interface with fourwindows.ber of utterances (both tutor and student) per tutor-ing session was 116 (min = 70, max = 211).
Theaverage number of tutor utterances per session is96 (min=44, max=156) whereas for students it is39 (min=18, max=69) for the annotated set.
Theaverage number of words per utterance for stu-dents is 4.4 and for tutors it is 5.4.
This annotatedset is used in the current analysis for both trainingand testing where cross-validation is applied.
Asdescribed later, a separate set containing 462 un-annotated utterances is used as a development setfor determining the number of clusters.The dialogue stream of this corpus was manu-ally annotated as part of previous work on super-vised dialogue act modeling which achieved 69%accuracy with Conditional Random Fields (Ha etal., 2012).
A brief description of the student di-alogue act tags, which are the focus of the mod-els reported in this paper, is shown in Table 1.The most frequent dialogue act (A) constitutes thebaseline chance (39.85%).
In the current work, themanually applied dialogue act labels are not uti-lized during model training, but are only used forevaluation purposes as our models?
accuracies arereported for manual tags on a held-out test set.An excerpt from the corpus is shown in Table 2.Note that the current work focuses on classifyingstudent dialogue act tags, since in an automated di-alogue system the tutor moves would be generatedby the system and their dialogue acts tags wouldtherefore be known.4 FeaturesA key issue for dialogue act classification in task-oriented dialogue involves how to represent dia-Student Dialogue Act DistributionAnswer (A) 39.85Acknowledgement (ACK) 21.31Statement (S) 21.20Question (Q) 15.15Request for Feedback (RF) 0.98Clarification (C) 0.79Other (O) 0.61Table 1: Student dialogue act tags and their fre-quencies.Tutor: ready?
[Q]Student: yep [A]Tutor moves on to next taskStudent: cool [S]Student compiles and runs the code.Program output: ?Hello World?Tutor: excellent [PF]Tutor: add a space to make the output lookprettier [DIR]Student: why doesnt it stop on the next linein this case?
[Q]Program haltsTutor: it did [A]Student runs the program successfully.Tutor: good.
[PF]Table 2: Excerpt of dialogue from the corpus andthe task action that follows utterances.logue and task events.
This section describes howfeatures were extracted from the corpus of humantutorial dialogue.We use three sets of features: lexical features,dialogue context features, and task features.
Thelexical and dialogue context features are extractedfrom the textual dialogue utterances within thecorpus.
The task features are extracted from theinteraction traces within the computer-mediatedlearning environment and represent a keystroke-level log of events as students worked toward solv-ing the computer programming problems.4.1 Lexical FeaturesBecause one of the main goals of our work in thelonger term is to perform automatic dialogue actclassification in real time, we took as a primaryconsideration the ability to quickly extract lexicalfeatures.
The features utilized in the current in-vestigation consist only of word unigrams.
In ad-115dition to their ease of extraction, our prior workhas shown that addition of part-of-speech tags andand syntax features did not significantly improvethe accuracy of supervised dialogue act classifiersin this domain (Boyer et al., 2010), and these fea-tures can be time-consuming to extract in real time(Ha et al., 2012).The choice to use word unigrams rather thanhigher order n-grams is further facilitated by thefact that our clustering technique leverages thelongest common sub-sequence (LCS) metric tomeasure distances between utterances.
This met-ric counts shared sub-sequences of not-necessarilycontiguous words (Hirschberg, 1975).
In this way,the LCS metric provides a flexible way for n-grams and skip-n-grams to be treated as impor-tant units within the clustering, while the raw fea-tures themselves consist only of word unigrams.
(We report on a comparison between LCS and bi-grams later in the discussion section.)
UtilizingLCS, there exists a distance (1-similarity) valuefrom each utterance to every other utterance.4.2 Dialogue Context FeaturesBased on previous work on a similar human tuto-rial dialogue corpus (Ha et al., 2012), we utilizefour features that provide information about the di-alogue structure.
These features are depicted inTable 3.
Note that our goal within this work is toclassify student dialogue moves, not tutor moves,because in a dialogue system the tutor?s moves aresystem-generated with associated known dialogueacts.Feature DescriptionUtterancepositionThe relative position of anutterance from the beginning ofthe dialogue.UtterancelengthThe number of tokens in theutterance, including words andpunctuation.PreviousauthorAuthor of the previous dialoguemessage (tutor or student) at thetime message sent.Previoustutordialogue actDialogue act of the previoustutor utterance.Table 3: Dialogue context features and their de-scriptions.4.3 Task FeaturesAs described previously, the corpus contains twochannels of information: the dialogue utterances,from which the lexical and dialogue context fea-tures were extracted, and in addition, the taskstream consisting of student problem-solving ac-tivities such as authoring code, compiling, and ex-ecuting the program.
The programming activitiesof students were logged to a database along withall of the dialogue events during tutoring.A set of task features was found to be impor-tant for dialogue act classification in this domainin prior work, including most recent programmingaction, status of the most recent task activity andtask activity flag representing whether the utter-ance was preceded by a student?s task activity (Haet al., 2012).
We expand this set of features asshown in Table 4.5 ExperimentsThe goal of this work is to investigate the im-pact of including task and dialogue context fea-tures on unsupervised dialogue act models.
Wehypothesize that incorporating task features willsignificantly improve the performance of an un-supervised model, and we also hypothesize thatproperly incorporating dialogue context features,which are at a different granularity than the lex-ical features extracted from utterances, will sub-stantially improve model accuracy.5.1 Dialogue Act Modeling With k-medoidsClusteringThe unsupervised models investigated here use k-medoids clustering, which is a well-known clus-tering technique that takes actual data points asthe center of each cluster (Ng and Han, 1994),in contrast to k-means which may have syntheticpoints as centroids.
In k-medoids, the centroidsare initially selected and then the algorithm iter-ates, reassigning data points in each iteration, un-til the clusters converge.
In standard k-medoidsclustering the initial seeds are selected randomlyand then a correct distribution of data points isidentified through the iteration and convergenceprocess.
For dialogue act classification, the in-fluence of the initial seeds is substantial becausethe frequencies across dialogue tags are typicallyunbalanced.
To overcome this challenge, we usea greedy seed selection approach similar to theone used in k-means++ (Arthur and Vassilvitskii,116Feature Descriptionprev actionMost recent action of thestudent (composing a dialogueutterance, constructing code,compiling or executing code).task beginWhether the student utterance isthe first utterance since thebeginning of the subtask.task stuWhether the student utterancewas preceded by a task event.task prev tutTask activity flag indicatingwhether the closest tutorutterance in this subtask waspreceded by a task activity.task statusThe status of the most recentcoding action (begin, stop,success, error and input sent).time elapsedTime elapsed between theprevious tutor message and thecurrent student utterance.errorsNumber of errors in thestudent?s latest code.delta errorsDifference in the number oferrors in the task between twoutterances in the same dialogue.stu # taskNumber of student dialoguemessages sent within the currenttask.stu # dialNumber of student dialoguemessages sent within the currentdialogue.tut # taskNumber of tutor dialoguemessages sent within the currentsubtask.tut # dialNumber of tutor dialoguemessages sent within the currentdialogue.Table 4: Task features extracted from student com-puter programming activities.2007) which selects the first seed randomly andthen greedily chooses seeds that are farthest fromthe chosen seeds.
The goal of using this approachin our application is to choose seeds from differentdialogue acts so that the final model achieves goodcoverage.
Our preliminary experiments demon-strated that this greedy seed selection combinedwith k-medoids outperforms other clustering ap-proaches including those utilized in our prior work(Ezen-Can and Boyer, 2013a).In order to select the number of clusters k,a subset of the corpus, constituting 25% of thefull corpus (that were not tagged) composed of462 utterances, was separated as a developmentset.
First, we examined the coherence of clus-ters at different values of k using intra-cluster dis-tances.
This technique involves identifying an ?el-bow?
where the decrease in intra-cluster distancebecomes less rapid (since adding more clusters cancontinue to decrease intra-cluster distance to thepoint of overfitting) (Figure 2).
The graph sug-gests an elbow at k=5.
Because there may be mul-tiple elbows in the intra-cluster distance, a sec-ond method utilizing Bayesian Information Crite-rion (BIC) was used which penalizes models asthe number of parameters increases.
The lower theBIC value, the better the model is, achieved at k=5as well.Figure 2: Intra-cluster distances with varyingnumber of clusters.Unlike many other investigations into unsuper-vised dialogue act classification, the current ap-proach reports accuracy on held-out test data, noton the data on which the model was trained.
Eventhough the model training process does not utilizeavailable manual tags, requiring the learned unsu-pervised model to perform well on held-out testdata more closely mimics the broader goal of ourwork which is to utilize these unsupervised mod-els within deployed dialogue systems, where mostutterances to be classified have never been encoun-tered by the model before.The procedure for model training and test-ing uses leave-one-student-out cross-validation.Rather than other forms of leave-one-out or strat-ified cross-validation, leave-one-student-out en-sures that each student?s set of dialogue utterancesare treated as the testing set while the model istrained on all other students?
utterances.
Thisprocess is repeated until each student?s utterances117have served as a held-out test set (in our case, thisresults in n=42 folds).
Within each fold, the clus-ters are learned during training and then for eachutterance in the test set, its closest cluster is com-puted by taking the average distance of the test ut-terance to the elements in the cluster.
The majoritylabel of the closest cluster is assigned as the dia-logue act tag for the test utterance.
If the assigneddialogue act tag matches the manual label of thetest utterance, the utterance is counted as correctclassification.
The average accuracy is computedas the number of correct classifications divided bythe total number of classifications.5.2 Experimental ResultsWe conducted experiments with seven differentfeature combinations: L, lexical features only,T , task features only, D, dialogue context fea-tures only, and then the combinations of these fea-tures, T + D, T + L, D + L, and T + D + L.We hypothesized that the addition of task featureswould significantly improve the models?
accuracy.As shown in Table 5, adding task features to di-alogue context features significantly outperformsdialogue context features alone (T + D > D).Similarly, adding task features to lexical featuresprovides significant improvement (T + L > L).However, adding task features to the dialogue con-text plus lexical features model does not providebenefit, and in fact slightly (not significantly) de-grades performance (T + D + L 6> D + L).
Asreflected by the Kappa scores, the test set perfor-mance attained by these models is hardly betterthan would be expected by chance.FeaturesAccuracy(%)KappaFlatClusteringL 33 0.02T 37.7 0.07D 37.6 0.07T+D 39.1* 0.07T+L 38* 0.06D+L 38.3 0.07T+D+L 37.3 0.05Table 5: Test set accuracies and Kappa for the flatclustering model (L: Lexical features, D: Dialoguecontext features, T: Task features) *indicates sta-tistically significant compared to the similar modelwithout task features (p < 0.05).5.3 Utilizing Dialogue HistoryThe importance of dialogue history, particularlythe influence of the most recent turn on an upcom-ing turn, is widely recognized within dialogue re-search, notably by work on adjacency pairs (Sche-gloff and Sacks, 1973; Forbes-Riley et al., 2007;Midgley et al., 2009).
Based on these findings, wehypothesized that dialogue history would be sub-stantially beneficial for unsupervised dialogue actmodels as it has been observed to be in numer-ous studies on supervised classification.
However,as seen in the previous section, adding these di-alogue context features with equal weight to themodel using Cosine distance only improved itsperformance slightly though statistically signifi-cantly (for example, T+D > T ), while the overallperformance is still barely above random chance.In an attempt to substantially boost the perfor-mance of the unsupervised dialogue act classi-fier, we experimented with a hierarchical cluster-ing structure in which the model first branches onthe previous tutor move, and then the clusteringmodels are learned as described previously at theleaves of the tree (Figure 3).This branching approach results in somebranches with too few utterances to train a multi-cluster model.
To deal with this situation we set athreshold of n=10 utterances.
For those subgroupswith fewer than 10 utterances, we take a simplemajority vote to classify test cases, and for thosesubgroups with 10 or larger utterances we train acluster model and use it to classify test cases.
Forthe entire corpus, the number of utterances in eachbranch is presented in Table 6.Tutor?s Previous Dialogue ActQ S PF Adoclustering...doclusteringdoclusteringdoclusteringFigure 3: Branching student utterances accordingto previous tutor dialogue act.As the results in Table 7 show, the performanceof the model with hierarchical structure is signif-icantly better than the flat clustering model.
Notethat each feature in this table leverages previous118Tutor DialogueAct# of studentutterancesQ 818S 464H 125PF 91A 61ACK 11C 8O 8RACK 6Table 6: The number of student utterances afterbranching on the previous tutor dialogue act.tutor dialogue act while branching.
Branchingon previous tutor move boosted the model?s accu-racy for student move dialogue act classificationby approximately 30% accuracy across all featuresets, a difference that is statistically significant inevery case.
With the hierarchical model struc-ture, the best performance is achieved by includ-ing all three types of features: lexical, dialoguecontext and task.
However, our hypothesis thattask features would significantly improve the ac-curacy does not hold within the hierarchical clus-tering model (T +D 6> D and T + L 6> L).FeaturesAccuracy(%)KappaHierarchicalT 64.2?0.45D 63.2?0.46L 60.7?0.41T+D 62.1?0.44T+L 63.3*?0.45D+L 63.6?0.46T+D+L 65*?0.48Table 7: Test set accuracies and Kappa for branch-ing on previous tutor dialogue act (L: Lexical fea-tures, D: Dialogue context features, T: Task fea-tures) *indicates statistically significant comparedto the similar model without task features and ?
in-dicates hierarchical clustering performing signifi-cantly better than flat with same features.
(p <0.05).6 DiscussionThe experimental results provide compelling ev-idence that an inclusive approach to features forunsupervised dialogue act modeling holds greatpromise.
However, we observed a stark differencein model performance when the tutor?s previousmove was simply included as one of many featureswithin a flat clustering model compared to whenthe previous tutor move was treated as a branch-ing feature.
In this section we take a closer lookand discuss the features that help distinguish par-ticular dialogue acts from each other.Using the hierarchical T +D+L model whichperformed best within the experiments, we exam-ine the confusion matrix (Figure 4).
Statementsand acknowledgments prove challenging for themodel, 51.3% and 61.5% accuracy overall.
More-over, these two tags are easily confused with eachother: 29.7% of statements were misclassifiedas acknowledgments, while 21.2% of acknowl-edgments were misclassified as statements.
Theworst overall classification accuracy was for ques-tions (6%) and the best was achieved for answers(95.3%).Figure 4: Confusion matrix for hierarchical modelutilizing all features: T+D+L.When we analyze the performance of differentsets of features with respect to individual dialogueacts, some interesting results emerge.
The anal-ysis shows that task features are especially goodfor classifying statements.
Using only task fea-tures, the model correctly classified 61.8% state-ments, compared to the lower 51.3% accuracy thatthe overall best model (T + D + L) achieved onstatements.
When we consider the nature of thestatement dialogue act within this corpus, we notethat it is a large category that encompasses a vari-ety of utterances, some of which have lexical fea-tures in common with acknowledgments.
In thiscase, task features are particularly helpful.For acknowledgments, a combination of taskand lexical features performed best (63.6% ac-119curacy) compared to the overall best performingmodel which achieved a slightly lower 61.5% ac-curacy on acknowledgments.
Acknowledgmentsare another example of an act that may take am-biguous surface form; for example, in our cor-pus an utterance ?yes?
appears as both an answerand an acknowledgment depending on its context.Therefore, higher level features such as the onesprovided by task may be more helpful.For questions, the highest performing featureset is L. However, as shown in Table 8, the modelperformed poorly on questions.
Inspection of themodels reveals that questions are varied in termsof structure throughout the corpus and it is hard todistinguish them from other dialogue acts.
For in-stance there are two consequent utterances ?i needa write statement?
and ?don?t i?, both of which aremanually labeled as questions.
However, in termsof the structure, the first utterance looks very sim-ilar to a statement and therefore the model has dif-ficulty grouping it with questions.
Due to the largevariety of question forms in the corpus, it is pos-sible that the clustering performed poorly on thisdialogue act.
In future work it will be promising toinvestigate the dialogue structures which producequestions and to weight them more in the featureset in order to increase performance of clusteringfor questions.We performed one additional experiment tocompare the performance of the LCS metric withbigrams.
For bigrams, the average leave-one-student-out test accuracy was 25% with flat clus-tering compared to the lexical-only case usingLCS (L) which reached 33%.Features S A Q ACKL 21.5 41.3 14.2 20.4T 61.76 95.27 7.30 40.90D 48.16 95.27 3.00 60.30T+D 52.69 94.68 3.43 51.64T+L 42.78 95.13 6.01 63.58D+L 43.63 94.98 8.58 62.09T+D+L 51.27 95.27 6.01 61.49Table 8: Accuracies for individual dialogue acts.Acts with fewer than 10 utterances after branchingare omitted from the table.7 Conclusion and Future WorkDialogue act classification is crucial for dialoguemanagement, and unsupervised modeling ap-proaches hold great promise for automatically ex-tracting classification models from corpora.
Thispaper has focused on unsupervised dialogue actclassification for task-oriented dialogue, investi-gating the impact of task features and dialoguecontext features on model accuracy within bothflat and hierarchical clusterings.
Experimentalresults confirm that utilizing a combination oftask and dialogue features improves accuracy andthat incorporating one previous tutor move as ahigh-level branching feature a provides particu-larly marked benefit.
Moreover, it was found thattask features are particularly important for iden-tifying particular dialogue moves such as state-ments, for which the model with task features onlyoutperformed the model with all features.In addition to the task stream, future workshould consider other sources of nonverbal cuessuch as posture, gesture and facial expressions toinvestigate the extent to which these can be suc-cessfully incorporated in unsupervised dialogueact models.
Second, models that are built in spe-cialized ways to different user groups (e.g., bygender or by incoming skill level) should be inves-tigated.
Finally, the performance of unsuperviseddialogue act classification models must ultimatelymove toward evaluation within implemented dia-logue systems (Ezen-Can and Boyer, 2013a).
Theoverarching goal of these investigations is to cre-ate unsupervised dialogue act models that performwell enough to be used within deployed dialoguesystems and enable the system to respond success-fully.
It is hoped that in the future, dialogue actclassification models for many domains can be ex-tracted automatically from corpora of human dia-logue in those domains without the need for anymanual annotation.AcknowledgmentsThanks to the members of the LearnDialoguegroup at North Carolina State University for theirhelpful input.
This work is supported in part by theNational Science Foundation through Grant DRL-1007962 and the STARS Alliance, CNS-1042468.Any opinions, findings, conclusions, or recom-mendations expressed in this report are those ofthe participants, and do not necessarily representthe official views, opinions, or policy of the Na-tional Science Foundation.120ReferencesJames F. Allen, Lenhart K. Schubert, George Ferguson,Peter Heeman, Chung Hee Hwang, Tsuneaki Kato,Marc Light, Nathaniel Martin, Bradford Miller,Massimo Poesio, et al.
1995.
The TRAINS project:A case study in building a conversational planningagent.
Journal of Experimental & Theoretical Arti-ficial Intelligence, 7(1):7?48.David Arthur and Sergei Vassilvitskii.
2007. k-means++: The advantages of careful seeding.
InProceedings of the 18th ACM-SIAM Symposium onDiscrete Algorithms, pages 1027?1035.
Society forIndustrial and Applied Mathematics.John Langshaw Austin.
1975.
How To Do Things withWords, volume 1955.
Oxford University Press.Srinivas Bangalore, Giuseppe Di Fabbrizio, andAmanda Stent.
2008.
Learning the structure oftask-driven human?human dialogs.
IEEE Transac-tions on Audio, Speech, and Language Processing,16(7):1249?1259.Kristy Elizabeth Boyer, Eun Young Ha, RobertPhillips, Michael D. Wallis, Mladen A. Vouk, andJames C. Lester.
2010.
Dialogue act modeling ina complex task-oriented domain.
In Proceedings ofSIGDIAL, pages 297?305.
Association for Compu-tational Linguistics.Kristy Elizabeth Boyer, Eun Young Ha, RobertPhillips, and James Lester.
2011.
The impact oftask-oriented feature sets on HMMs for dialoguemodeling.
In Proceedings of SIGDIAL, pages 49?58.
Association for Computational Linguistics.Lin Chen and Barbara Di Eugenio.
2013.
Multimodal-ity and dialogue act classification in the RoboHelperproject.
In Proceedings of SIGDIAL, pages 183?192.Mark G. Core and James Allen.
1997.
Coding dialogswith the DAMSL annotation scheme.
In Proceed-ings of the AAAI Fall Symposium on CommunicativeAction in Humans and Machines, pages 28?35.Nigel Crook, Ramon Granell, and Stephen Pulman.2009.
Unsupervised classification of dialogue actsusing a Dirichlet process mixture model.
In Pro-ceedings of SIGDIAL, pages 341?348.
Associationfor Computational Linguistics.Barbara Di Eugenio, Zhuli Xie, and Riccardo Serafin.2010.
Dialogue act classification, higher order di-alogue structure, and instance-based learning.
Dia-logue & Discourse, 1(2):1?24.Myroslava O. Dzikovska, Elaine Farrow, and Jo-hanna D. Moore.
2013.
Combining semantic inter-pretation and statistical classification for improvedexplanation processing in a tutorial dialogue system.In Artificial Intelligence in Education, pages 279?288.Aysu Ezen-Can and Kristy Elizabeth Boyer.
2013a.In-context evaluation of unsupervised dialogue actmodels for tutorial dialogue.
In Proceedings of SIG-DIAL, pages 324?328.Aysu Ezen-Can and Kristy Elizabeth Boyer.
2013b.Unsupervised classification of student dialogue actswith query-likelihood clustering.
In InternationalConference on Educational Data Mining, pages 20?27.Oliver Ferschke, Iryna Gurevych, and Yevgen Chebo-tar.
2012.
Behind the article: Recognizing dialogacts in Wikipedia talk pages.
In Proceedings of the13th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 777?786.Kate Forbes-Riley and Diane J. Litman.
2005.
Us-ing bigrams to identify relationships between stu-dent certainness states and tutor responses in a spo-ken dialogue corpus.
In Proceedings of the SIG-DIAL Workshop, pages 87?96.Kate Forbes-Riley, Mihai Rotaru, Diane J. Litman, andJoel Tetreault.
2007.
Exploring affect-context de-pendencies for adaptive system development.
InHuman Language Technologies 2007: The Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 41?44.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-putational Linguistics, 12(3):175?204.Eun Young Ha, Joseph F. Grafsgaard, Christopher M.Mitchell, Kristy Elizabeth Boyer, and James C.Lester.
2012.
Combining verbal and nonverbalfeatures to overcome the ?information gap?
in task-oriented dialogue.
In Proceedings of SIGDIAL,pages 247?256.Daniel S. Hirschberg.
1975.
A linear space al-gorithm for computing maximal common subse-quences.
Communications of the ACM, 18(6):341?343.Shafiq Joty, Giuseppe Carenini, and Chin-Yew Lin.2011.
Unsupervised modeling of dialog acts inasynchronous conversations.
In Proceedings of the22nd International Joint Conference on Artificial In-telligence, pages 1807?1813.Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox, andTraci Curl.
1998.
Lexical, prosodic, and syn-tactic cues for dialog acts.
In Proceedings of theACL/COLING-98 Workshop on Discourse Relationsand Discourse Markers, pages 114?120.Simon Keizer, Rieks op den Akker, and Anton Nijholt.2002.
Dialogue act recognition with Bayesian net-works for Dutch dialogues.
In Proceedings of theSIGDIAL Workshop, pages 88?94.121Su Nam Kim, Lawrence Cavedon, and Timothy Bald-win.
2010.
Classifying dialogue acts in one-on-one live chats.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 862?871.Donghyeon Lee, Minwoo Jeong, Kyungduk Kim, andSeonghan Ryu.
2013.
Unsupervised spoken lan-guage understanding for a multi-domain dialog sys-tem.
IEEE Transactions On Audio, Speech, andLanguage Processing, 21(11):2451?2464.Johanna Marineau, Peter Wiemer-Hastings, Derek Har-ter, Brent Olde, Patrick Chipman, Ashish Karnavat,Victoria Pomeroy, Sonya Rajan, Art Graesser, Tutor-ing Research Group, et al.
2000.
Classification ofspeech acts in tutorial dialog.
In Proceedings of theWorkshop on Modeling Human Teaching Tactics andStrategies at the Intelligent Tutoring Systems Con-ference, pages 65?71.T.
Daniel Midgley, Shelly Harrison, and Cara Mac-Nish.
2009.
Empirical verification of adjacencypairs using dialogue segmentation.
In Proceedingsof SIGDIAL, pages 104?108.Raymond Ng and Jiawei Han.
1994.
Efficient and ef-fective clustering methods for spatial data mining.In Proceedings of the 20th International Conferenceon Very Large Data Bases, pages 144?155.Norbert Reithinger and Martin Klesen.
1997.
Dia-logue act classification using language models.
InProceedings of EuroSpeech, pages 2235?2238.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Unsu-pervised modeling of twitter conversations.
In Pro-ceedings of the Association for Computational Lin-guistics, pages 172?180.Vasile Rus, Cristian Moldovan, Nobal Niraula, andArthur C. Graesser.
2012.
Automated discovery ofspeech act categories in educational games.
In Inter-national Conference on Educational Data Mining,pages 25?32.Emanuel A. Schegloff and Harvey Sacks.
1973.
Open-ing up closings.
Semiotica, 8(4):289?327.John R. Searle.
1969.
Speech Acts: An Essay inthe Philosophy of Language.
Cambridge UniversityPress.Riccardo Serafin and Barbara Di Eugenio.
2004.FLSA: Extending latent semantic analysis with fea-tures for dialogue act classification.
In Proceedingsof the 42nd Annual Meeting on Association for Com-putational Linguistics, pages 692?699.
Associationfor Computational Linguistics.Rangarajan Sridhar, Vivek Kumar, Srinivas Bangalore,and Shrikanth Narayanan.
2009.
Combining lexi-cal, syntactic and prosodic cues for improved onlinedialog act tagging.
Computer Speech & Language,23(4):407?422.Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Rachel Martin, Carol Van Ess-Dykema, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26(3):339?373.Maryam Tavafi, Yashar Mehdad, Shafiq Joty, GiuseppeCarenini, and Raymond Ng.
2013.
Dialogue actrecognition in synchronous and asynchronous con-versations.
In Proceedings of SIGDIAL, pages 117?121.David R. Traum.
1999.
Speech acts for dialogueagents.
In Foundations of Rational Agency, pages169?201.
Springer.122
