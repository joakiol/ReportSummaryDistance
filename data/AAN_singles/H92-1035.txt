Improving State-of-the-Art Continuous Speech Recognition SystemsUsing the N-Best Paradigm with Neural NetworksS.
Austin, G. Zavaliagkos t, J. Makhoul, and R. SchwartzBBN Systems and  Techno log ies ,  Cambr idge ,  MA 02138tNor theastern  Un ivers i ty ,  Boston ,  MA 02115ABSTRACTIn an effort to advance the state of the art in continuous peechrecognition employing hidden Markov models (HMM), SegmentalNeural Nets (SNN) were introduced recently to ameliorate the well-known limitations of HMMs, namely, the conditional-independencelimitation and the relative difficulty with which HMMs can handlesegmental features.
We describe a hybrid SNN/I-IMM system thatcombines the speed and performance of our HMM system with thesegmental modeling capabilities of SNNs.
The integration of thetwo acoustic modeling techniques i achieved successfully via theN-best rescoring paradigm.
The N-best lists are used not only forrecognition, but also during training.
This discriminative trainingusing N-best is demonstrated to improve performance.
When testedon the DARPA Resource Management speaker-independent corpus,the hybrid SNN/HMM system decreases the error by about 20%compared to the state-of-the-art HMM system.INTRODUCTIONIn February 1991, we introduced at the DARPA Speechand Natural Language Workshop the concept of a Segmen-tal Neural Net (SNN) for phonetic modeling in continuousspeech recognition \[1\].
The SNN was introduced to over-come some of the well-known limitations of hidden Markovmodels (HMM) which now represent the state of the art incontinuous peech recognition (CSR).
Two such limitationsare (i) the conditional-independence assumption, which pre-vents a HMM from taking full advantage of the correlationthat exists among the frames of a phonetic segment, and(ii) the awkwardness with which segmental features (suchas duration) can be incorporated into HMM systems.
Wedeveloped the concept of SNN specifically to overcome thetwo HMM limitations just mentioned for phonetic modelingin speech.
However, neural nets are known to require a largeamount of computation, especially for training.
Also, thereis no known efficient search technique for finding the bestscoring segmentation with neural nets in continuous peech.Therefore, we have developed a hybrid SNN/HMM systemthat is designed to take full advantage of the good prop-erties of both methods: the phonetic modeling propertiesof SNNs and the good computational properties of HMMs.The two methods are integrated through the use of the N-best paradigm, which was developed in conjunction with theBYBLOS system at BBN \[7,6\].A year ago, we presented very preliminary results usingour hybrid system on the speaker-dependent portion of theDARPA Resource Management Corpus \[1\].
Also, the train-ing of the neural net was performed only on the correct ran-scription of the utterances.
In this paper, we describe the per-formance of the hybrid system on the speaker-independentportion of the Resource Management corpus, using discrim-inative training on the whole N-best list.
Below, we give adescription of the SNN, the integration of the SNN with theHMM models using the N-best paradigm, the training of thehybrid SNN/I-IMM system using the whole N-best list, andthe results on a development set.SEGMENTAL NEURAL NET STRUCTUREThe SNN differs from other approaches to the use of neuralnetworks in speech recognition in that it attempts to recog-nize each phoneme by using all the frames in a phoneticsegment simultaneously to perform the recognition.
TheSNN is a neural network that takes the frames of a pho-netic segment as input and produces as output an estimateof the probability of a phoneme given the input segment.But the SNN requires the availability of some form of pho-netic segmentation of the speech.
To consider all possiblesegmentations of the input speech would be computation-ally prohibitive.
We describe in Section 3 how we use theHMM to obtain likely candidate segmentations.
Here, weshall assume that a phonetic segmentation has been madeavailable.The structure of a typical SNN is shown in Figurel.
Theinput to the net is a fixed number of frames of speech fea-tures (5 frames in our system).
The features in each 10-msframe consist of 16 scalar values: 14 reel-warped cepstralcoefficients, power, and power difference.
Thus, the inputto the SNN consists of a total of 80 features.
But the ac-tual number of actual frames in a phonetic segment is vari-able.
Therefore, we convert he variable number of frames ineach segment to a fixed number of frames (in this case, fiveframes).
In this way, the SNN is able to deal effectively withvariable-length segments in continuous peech.
The requi-site time warping is performed by a quasi-linear sampling ofthe feature vectors comprising the segment.
For example, ina 17-frame phonetic segment, we would use frames 1, 5, 9,13, and 17 as input to the SNN.
In a 3-frame segment, thefive frames used are 1, 1, 2, 3, 3, with a repetition of the180system.warpingN-best HMMrecognitionN-best listphonetic segmentFigure 1: The Segmental Neural Network model samples theframes in a segment and produces a single segment score.first and third frames.
In this sampling, we are using a re-sult from Stochastic Segment Models (SSM) \[5\] in which itwas found that sampling of naturally-occurring frames givesbetter esults than strict linear interpolation.Since there are 53 phonemes defined in our system, weused SNNs with 53 outputs, each representing one of thephonemes in the system.THE N-BEST RESCORING PARADIGMwithout an algorithm that can efficiently search all word-sequence and segmentation posibilities in a large-vocabularyCSR system, the amount of computation required to incor-porate the SNN into such a system would be prohibitive.However, it is possible to use the N-best paradigm to makesuch an incorporation feasible.The N-best paradigm \[7,6\] was originally developed at BBNas a simple way to ameliorate the effects of errors in speechrecognition when integrating speech with natural anguageprocessing.
Instead of producing a single sequence words,the recognition component produces a list of N best-scoringsequences.
%The list of N sentences i ordered by over-all score in matching the input utterance.
For integrationwith natural anguage, we send the list of N sentences tothe natural anguage component, which processes the sen-tences in the order given and chooses the highest scoringsentence that can be understood by the system.
However,we found that the N-best paradigm can also be very usefulfor improving speech recognition performance when moreexpensive sources of knowledge (such as cross-word effectsand higher-order statistical grammars) cannot be computedefficiently during the recognition.
All one does is rescorethe N-best list with the new sources of knowledge and re-order the list.
The SNN is a good example of an expensiveknowledge source, whose use would benefit greatly from us-ing N-best rescoring, thus comprising a hybrid SNN/HMMHMM rescorlng ~ SNN segmentation rescori ngandI labels IHMM SC~ ~ scoresr I t?
"C ?'?
"t JJ- and reorder listFigure 2: Schematic diagram of the hybrid SNN/HMM sys-tem using the Nzbest rescofing paradigm.Figure 2 shows a block diagram of the hybrid SNN/HMMsystem.
A spoken utterance is processed by the HMM rec-ognizer to produce a list of the N best-scoring sentence hy-potheses.
The length of this list is chosen to be long enoughto almost always include the correct answer (from experi-ence, N=20 is usually sufficien0.
Thereafter, the recognitiontask is reduced to selecting the best hypothesis from the N-best list.
Because these N-best lists are quite short (e.g.,N=20), each hypothesis can be examined and scored usingalgorithms which would have been computafionally impossi-ble with a search through the entire vocabulary.
In addition,it is possible to generate several types of scoring for eachhypothesis.
This not only provides a very effective meansof comparing the effectiveness of different speech models(e.g., SNN versus HMM), but it also provides an easy wayto combine several radically different models.One most obvious way in which the SNN could use theN-best list would be to use the HMM system to generatea segmentation for each N-best hypothesis (by finding themost likely HMM state sequence according to that hypothe-sis) and to use the SNN to generate a score for the hypothesisusing this segmentation.
This SNN score for a hypothesisis the logarithm of the product of the appropriate SNN out-puts for all the segments in a segmentation according to thathypothesis.
The chosen answer would be the hypothesiswith the best SNN score.
However, it is also possible to181generate several scores for each hypothesis, such as SNNscore, HMM score (which is the logarithm of the HMMlikelihood), grammar score, and the hypothesized numberof words and phonemes.
We can then generate a compos-ite score by, for example, taking a linear combination ofthe individual scores.
After we have rescored the N-Bestlist, we can reorder it according to the new scores.
If theCSR system is required to output just a single hypothesis,the highest scoring hypothesis chosen.
We call this wholeprocess the N-best rescoring paradigm.The linear combination that comprises the composite scoreis determined by selecting the weights that give the bestperformance over a development test set.
These weightscan be chosen automatically \[4\].
The number of wordsand phonemes are included in the composite score becausethey serve the same pu~ose as word and phoneme insertionpenalties in a HMM CSR system.SEGMENTAL NEURAL NET TRAIN ING1-Best TrainingIn our original training algorithm, we first segmented allof the training utterances into phonetic segments using theHMM models and the utterance transcriptions.
Each seg-ment then serves as a positive example of the SNN outputcorresponding to the phonetic label of the segment and as anegative xample for all the other 52 phonetic SNN outputs.We call this training method 1-best raining.The SNN was originally trained using a mean-square error(MSE) criterion - i.e., the SNN was trained to minimizeN 1 E = ~ E(yc (n)  - de(n)) 2n--|where yc(n) is the network output for phoneme class c forthe n m training vector and de(n) is the desired output for thatvector (l if the segment belongs to class c and 0 otherwise).This measure can lead to gross errors at low values of yc(n)when segment scores are multiplied together.
Accordingly,we adopted the log-error training criterion \[3\], which is ofthe formN 1 E = -~ ~ log (u~(n)- \[1 - de(n)\]) 2Iz=lThis can be shown to have several advantages over the MSEcriterion.
When the neural net non-linearity is the usualsigmoid function, this error measure has only one minimumfor single layer nets.
In addition, the gradient is simple andavoids the problem of "weight locking" (where large errorsdo not change because of small gradients in the sigmoid).Durat ionBecause of the time-waiping function (which transformsphonetic segments of any length into a fixed-length repre-sentation), the SNN score for a segment is independent of182the duration of the segment.
In order to provide informationabout he duration to the SNN, we constructed a simple du-rational model.
For each phoneme, a histogram was made ofsegment durations in the training data.
This histogram wasthen smoothed by convolving with a triangular window, andprobabilities falling below a floor level were reset to thatlevel.
The duration score was multiplied by the neural netscore to give an overall segment score.N-best TrainingIn our latest version of the training algorithm, we take theN-best paradigm a step further and perform what we callN-best raining, which is a form of discriminative training.First, we take the HMM-based segmentations of the trainingutterances according to the correct word sequence.
Thesesegments are used only as positive xamples (i.e., trained tooutput 1) for the appropriate SNN outputs.We then produce the N-best lists for all of the training sen-tences.
For each of the incorrect hypotheses in the N-bestlist, we obtain the H/VIM-based segmentation and isolatethose segments that differ from the segmentation accordingto the correct ranscription and use them as negative train-ing for the SNN outputs (i.e., trained to output 0).
Thus wetrain negatively on the "misrecognized" parts of the incorrecthypothesis.lime?
- -  ' " 1 " 1 ' 1 " '  hypotheele carrlere wore In Atlanticwrong I I I  I I I I I  Ihypotheal8 cmTlere one Atlanticbadeegment8Figure 3: N-Best training trains the SNN to specificallyreject those segments from an incorrect hypothesis that theHMM consioders likelyThis new method has the advantage that the SNN is specif-ically trained to discriminate among the choices that theHMM system considers diflicult.
This is better than the1-best raining algorithm, which only uses the segmentationof the correct utterance transcription, because N-best train-ing directly optimizes the performance of the SNN in theN-best rescoring paradigm.If, for example, the transcription of part of an utterance"...  carriers were in Atlantic..." and a likely N-best hy-pothesis was "...  carriers one Atlantic..." the segments cor-responding to the word "one" (as generated by a constrainedHMM alignment) would be presented to the SNN as nega-Table 1: SNN development on February '89 test setOriginal SSN (MSE)+ Duration+ Log-Error Criterion+ N-Best trainingWordError (%)13.712.711.69.0Table 2: Hybrid SNN/HMM system: test results.System* N Feb '89 Oct '89HMM 1 3.5 3.8SNN 20 9.0 - -SNN+HMM 2 3.3 - -SNN+HMM 4 2.9 - -SNN+HMM 20 2.8 3.0* All systems include word and segment scores.tive training.
To determine ff a segment should be presentedto the SNN as negative input, the label and position of eachsegment in a hypothesis i compared to the segments seen inthe correct segmentation f the utterance.
If  either the labelor the position (subject o a tolerance threshold) of the seg-ment does not match a segment in the correct segmentation,it is presented as negative training.EXPERIMENTAL CONDITIONS ANDRESULTSExperiments to test the performance of the hybridSNN/HMM system were performed on the Speaker Indepen-dent (SI) portion of the DARPA 1000-word Resource Man-agement speech corpus, using the standard word-pair gram-mar (perplexity 60).
The training set consisted of utterancesfrom 109 speakers, 2830 utterances from male speakers and1160 utterances from female speakers, and the February '89test set was used for development of the system.
The Octo-ber '89 test set was used for the final independent test.In our initial experiments, we used the February '89 devel-opment set.
Table 1 shows the word error rates when werescored the N=20 N-best lists at the various stages of de-velopment of the SNN.
It should be noted that the figures donot reflect the unaided performance of the SNN in recogni-tion, since the N-best list was generated by a HMM system,but instead illustrate the effectiveness of the respective im-provements.The original l-layer SNN was trained using the 1-best rain-ing algorithm and the MSE criterion; it gave an error rateof 13.7%.
The incorporation of the duration term and theadoption of the log-error training criterion both resulted insome improvement, bringing the error rate to 11.6%.When we used the N-best training (which used the SNNproduced by the 1-best training as an initial estimate), theerror rate dropped to 9.0%, confirming our belief that theN-best training is more effective than the 1-best raining inthe N-best rescoring paradigm.
This final condition was thenused to generate the SNN score to examine the behavior ofthe hybrid SNN/HMM system.Table 2 shows the results of combining the HMM and SNNscores in the re-ordering of the N-Best list.
Taking the topanswer of the N-best list (as produced by the HMM system)gave an error rate of 3.5% on the February '89 develop-ment test set.
Upon re-ordering the N=20 list on the basisof the SNN score alone, the error rate was 9.0%.
How-ever, upon combining the HMM and SNN scores, the errorrate decreased over that of the HMM alone.
The error ratedecreased as the value of N used in the N-best list was in-creased.
For N=2, the error decreased to 3.3%, then to 2.9%for N=4, and finally to 2.8% for N=20.Based upon the results of the February'89 development set,we rescored the 20-best lists generated from the October'89 with the hybrid system.
This independent test yieldedan even larger improvement, reducing the error rate from3.8% in the HMM-based system to 3.0% in the SNN/HMMsystem.
This represents a 20% reduction in error rate.Given that the HMM system used in our experiments rep-resented the state of the art in CSR, the hybrid SNN/HMMsystem has now established a new state of the art.CONCLUSIONSWe have presented the Segmental Neural Net as a methodfor phonetic modeling in large vocabulary CSR systems andhave demonstrated that, when combined with a conventionalHMM, the SNN gives an improvement over the performanceof a state-of-the-art HMM CSR system.We have used the N-best rescoring paradigm to achieve thisimprovement in two ways.
Firstly, the N-best rescoringparadigm has allowed us to design and test the SNN withlittle regard to the usual problem of searching when dealingwith a large vocabulary speech recognition system.
See-ondiy, the paradigm provides a simple way of combiningthe best aspects of two systems, leading to a combined sys-tem which exceeds the performance of either one alone.Future work will concentrate on modifying the N-best train-ing algorithm to model context in the SNN.
We will also in-vestigate possible improvements to the structure of the SNN,including different network architectures and additional seg-ment features.AcknowledgmentsThe authors would like to thank Amro E1-Jaroudi of theUniversity of Pittsburgh for his help in several aspects ofthis work.
This work was sponsored by DARPA.183REFERENCES1.
Austin, S., Makhoul, J., Schwartz, R., and Zavaliagkos, G.,"Continuous Speech Recognition Using Segmental NeuralNets," Proc.
DARPA Speech and Natural Language Workshop,Pacific Grove, CA, pp.
249-252, Morgan Kaufmann Publish-ers, February 1991.2.
Austin, S., Peterson, P., Placeway, P., Schwartz, R., Van-degrift, J., "Towards a Real-Time Spoken Language SystemUsing Commercial Hardware," Proc.
DARPA Speech and Nat-ural Language Workshop, Hidden Valley, PA, Morgan Kauf-mann Publishers, June 1990.3.
E1-Jaroudi, A. and Makhoul, J., "A New Error Criterion forPosterior Probability Estimation with Neural Nets," Interna-tional Joint Conference on Neural Networks, San Diego, CA,June 1990, Vol IlL pp.
185-192.4.
Ostendorf, M., Kannan, A., Austin, S., Kimball, O., Schwartz,R., Rohlicek, J.R., "Integration of Diverse RecognitionMethodologies Through Reevaluation of N-Best SentenceHypotheses," Proceedings of the DARPA Speech and NaturalLanguage Workshop, Pacific Grove, CA, Morgan KaufmannPublishers, February 1991.5.
Ostendorf, M. and Roukos S., "A Stochastic Segment Modelfor Phoneme-based Continuous Speech Recognition," IEEETrans.
Acoustic Speech and Signal Processing, Vol.
ASSP-37(12), December 1989, pp.
1857-1869.6.
Schwartz, R. and Austin, S., "A comparison of Several Ap-proximate Algorithms for Finding Multiple (N-Best) SentenceHypotheses," ICAASP-91, Toronto, Canada, May 1991, pp.?
701-704.7.
Schwartz, R. and Chow, Y.L., "The N-Best Algorithm: AnEfficient and Exact Procedure for Finding the N Most LikelySentence Hypotheses," ICASSP-90, Albuquerque, NM, April1990, pp.
81-84.184
