Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 188?193,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsTransition-based Dependency Parsing with Rich Non-local FeaturesYue ZhangUniversity of CambridgeComputer Laboratoryyue.zhang@cl.cam.ac.ukJoakim NivreUppsala UniversityDepartment of Linguistics and Philologyjoakim.nivre@lingfil.uu.seAbstractTransition-based dependency parsers gener-ally use heuristic decoding algorithms but canaccommodate arbitrarily rich feature represen-tations.
In this paper, we show that we can im-prove the accuracy of such parsers by consid-ering even richer feature sets than those em-ployed in previous systems.
In the standardPenn Treebank setup, our novel features im-prove attachment score form 91.4% to 92.9%,giving the best results so far for transition-based parsing and rivaling the best resultsoverall.
For the Chinese Treebank, they give asignficant improvement of the state of the art.An open source release of our parser is freelyavailable.1 IntroductionTransition-based dependency parsing (Yamada andMatsumoto, 2003; Nivre et al, 2006b; Zhang andClark, 2008; Huang and Sagae, 2010) utilize a deter-ministic shift-reduce process for making structuralpredictions.
Compared to graph-based dependencyparsing, it typically offers linear time complexityand the comparative freedom to define non-local fea-tures, as exemplified by the comparison betweenMaltParser and MSTParser (Nivre et al, 2006b; Mc-Donald et al, 2005; McDonald and Nivre, 2007).Recent research has addressed two potential dis-advantages of systems like MaltParser.
In theaspect of decoding, beam-search (Johansson andNugues, 2007; Zhang and Clark, 2008; Huang etal., 2009) and partial dynamic-programming (Huangand Sagae, 2010) have been applied to improve upongreedy one-best search, and positive results were re-ported.
In the aspect of training, global structurallearning has been used to replace local learning oneach decision (Zhang and Clark, 2008; Huang et al,2009), although the effect of global learning has notbeen separated out and studied alone.In this short paper, we study a third aspect in astatistical system: feature definition.
Representingthe type of information a statistical system uses tomake predictions, feature templates can be one ofthe most important factors determining parsing ac-curacy.
Various recent attempts have been madeto include non-local features into graph-based de-pendency parsing (Smith and Eisner, 2008; Martinset al, 2009; Koo and Collins, 2010).
Transition-based parsing, by contrast, can easily accommodatearbitrarily complex representations involving non-local features.
Complex non-local features, such asbracket matching and rhythmic patterns, are usedin transition-based constituency parsing (Zhang andClark, 2009; Wang et al, 2006), and most transition-based dependency parsers incorporate some non-local features, but current practice is nevertheless touse a rather restricted set of features, as exemplifiedby the default feature models in MaltParser (Nivre etal., 2006a).
We explore considerably richer featurerepresentations and show that they improve parsingaccuracy significantly.In standard experiments using the Penn Treebank,our parser gets an unlabeled attachment score of92.9%, which is the best result achieved with atransition-based parser and comparable to the stateof the art.
For the Chinese Treebank, our parser getsa score of 86.0%, the best reported result so far.1882 The Transition-based Parsing AlgorithmIn a typical transition-based parsing process, the in-put words are put into a queue and partially builtstructures are organized by a stack.
A set of shift-reduce actions are defined, which consume wordsfrom the queue and build the output parse.
Recentresearch have focused on action sets that build pro-jective dependency trees in an arc-eager (Nivre etal., 2006b; Zhang and Clark, 2008) or arc-standard(Yamada and Matsumoto, 2003; Huang and Sagae,2010) process.
We adopt the arc-eager system1, forwhich the actions are:?
Shift, which removes the front of the queueand pushes it onto the top of the stack;?
Reduce, which pops the top item off the stack;?
LeftArc, which pops the top item off thestack, and adds it as a modifier to the front ofthe queue;?
RightArc, which removes the front of thequeue, pushes it onto the stack and adds it asa modifier to the top of the stack.Further, we follow Zhang and Clark (2008) andHuang et al (2009) and use the generalized percep-tron (Collins, 2002) for global learning and beam-search for decoding.
Unlike both earlier global-learning parsers, which only perform unlabeledparsing, we perform labeled parsing by augmentingthe LeftArc and RightArc actions with the setof dependency labels.
Hence our work is in line withTitov and Henderson (2007) in using labeled transi-tions with global learning.
Moreover, we will seethat label information can actually improve link ac-curacy.3 Feature TemplatesAt each step during a parsing process, theparser configuration can be represented by a tuple?S,N,A?, where S is the stack, N is the queue ofincoming words, and A is the set of dependencyarcs that have been built.
Denoting the top of stack1It is very likely that the type of features explored in thispaper would be beneficial also for the arc-standard system, al-though the exact same feature templates would not be applicablebecause of differences in the parsing order.from single wordsS0wp; S0w; S0p; N0wp; N0w; N0p;N1wp; N1w; N1p; N2wp; N2w; N2p;from word pairsS0wpN0wp; S0wpN0w; S0wN0wp; S0wpN0p;S0pN0wp; S0wN0w; S0pN0pN0pN1pfrom three wordsN0pN1pN2p; S0pN0pN1p; S0hpS0pN0p;S0pS0lpN0p; S0pS0rpN0p; S0pN0pN0lpTable 1: Baseline feature templates.w ?
word; p ?
POS-tag.distanceS0wd; S0pd; N0wd; N0pd;S0wN0wd; S0pN0pd;valencyS0wvr; S0pvr; S0wvl; S0pvl; N0wvl; N0pvl;unigramsS0hw; S0hp; S0l; S0lw; S0lp; S0ll;S0rw; S0rp; S0rl;N0lw; N0lp; N0ll;third-orderS0h2w; S0h2p; S0hl; S0l2w; S0l2p; S0l2l;S0r2w; S0r2p; S0r2l; N0l2w; N0l2p; N0l2l;S0pS0lpS0l2p; S0pS0rpS0r2p;S0pS0hpS0h2p; N0pN0lpN0l2p;label setS0wsr; S0psr; S0wsl; S0psl; N0wsl; N0psl;Table 2: New feature templates.w ?
word; p ?
POS-tag; vl, vr ?
valency; l ?dependency label, sl, sr ?
labelset.with S0, the front items from the queue with N0,N1, and N2, the head of S0 (if any) with S0h, theleftmost and rightmost modifiers of S0 (if any) withS0l and S0r, respectively, and the leftmost modifierof N0 (if any) with N0l, the baseline features areshown in Table 1.
These features are mostly takenfrom Zhang and Clark (2008) and Huang and Sagae(2010), and our parser reproduces the same accura-cies as reported by both papers.
In this table, w andp represents the word and POS-tag, respectively.
Forexample, S0pN0wp represents the feature templatethat takes the word and POS-tag of N0, and com-bines it with the word of S0.189In this short paper, we extend the baseline featuretemplates with the following:Distance between S0 and N0Direction and distance between a pair of head andmodifier have been used in the standard featuretemplates for maximum spanning tree parsing (Mc-Donald et al, 2005).
Distance information hasalso been used in the easy-first parser of (Goldbergand Elhadad, 2010).
For a transition-based parser,direction information is indirectly included in theLeftArc and RightArc actions.
We add the dis-tance between S0 and N0 to the feature set by com-bining it with the word and POS-tag of S0 and N0,as shown in Table 2.It is worth noticing that the use of distance in-formation in our transition-based model is differentfrom that in a typical graph-based parser such asMSTParser.
The distance between S0 and N0 willcorrespond to the distance between a pair of headand modifier when an LeftArc action is taken, forexample, but not when a Shift action is taken.Valency of S0 and N0The number of modifiers to a given head is usedby the graph-based submodel of Zhang and Clark(2008) and the models of Martins et al (2009) andSagae and Tsujii (2007).
We include similar infor-mation in our model.
In particular, we calculate thenumber of left and right modifiers separately, call-ing them left valency and right valency, respectively.Left and right valencies are represented by vl and vrin Table 2, respectively.
They are combined with theword and POS-tag of S0 and N0 to form new featuretemplates.Again, the use of valency information in ourtransition-based parser is different from the afore-mentioned graph-based models.
In our case,valency information is put into the context of theshift-reduce process, and used together with eachaction to give a score to the local decision.Unigram information for S0h, S0l, S0r and N0lThe head, left/rightmost modifiers of S0 and theleftmost modifier of N0 have been used by mostarc-eager transition-based parsers we are aware ofthrough the combination of their POS-tag with infor-mation from S0 and N0.
Such use is exemplified bythe feature templates ?from three words?
in Table 1.We further use their word and POS-tag informationas ?unigram?
features in Table 2.
Moreover, weinclude the dependency label information in theunigram features, represented by l in the table.
Uni-gram label information has been used in MaltParser(Nivre et al, 2006a; Nivre, 2006).Third-order features of S0 and N0Higher-order context features have been used bygraph-based dependency parsers to improve accura-cies (Carreras, 2007; Koo and Collins, 2010).
Weinclude information of third order dependency arcsin our new feature templates, when available.
InTable 2, S0h2, S0l2, S0r2 and N0l2 refer to the headof S0h, the second leftmost modifier and the secondrightmost modifier of S0, and the second leftmostmodifier of N0, respectively.
The new templatesinclude unigram word, POS-tag and dependencylabels of S0h2, S0l2, S0r2 and N0l2, as well asPOS-tag combinations with S0 and N0.Set of dependency labels with S0 and N0As a more global feature, we include the set ofunique dependency labels from the modifiers of S0and N0.
This information is combined with the wordand POS-tag of S0 and N0 to make feature templates.In Table 2, sl and sr stands for the set of labels onthe left and right of the head, respectively.4 ExperimentsOur experiments were performed using the PennTreebank (PTB) and Chinese Treebank (CTB) data.We follow the standard approach to split PTB3, usingsections 2 ?
21 for training, section 22 for develop-ment and 23 for final testing.
Bracketed sentencesfrom PTB were transformed into dependency for-mats using the Penn2Malt tool.2 Following Huangand Sagae (2010), we assign POS-tags to the trainingdata using ten-way jackknifing.
We used our imple-mentation of the Collins (2002) tagger (with 97.3%accuracy on a standard Penn Treebank test) to per-form POS-tagging.
For all experiments, we set thebeam size to 64 for the parser, and report unlabeledand labeled attachment scores (UAS, LAS) and un-labeled exact match (UEM) for evaluation.2http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html190feature UAS UEMbaseline 92.18% 45.76%+distance 92.25% 46.24%+valency 92.49% 47.65%+unigrams 92.89% 48.47%+third-order 93.07% 49.59%+label set 93.14% 50.12%Table 3: The effect of new features on the developmentset for English.
UAS = unlabeled attachment score; UEM= unlabeled exact match.UAS UEM LASZ&C08 transition 91.4% 41.8% ?H&S10 91.4% ?
?this paper baseline 91.4% 42.5% 90.1%this paper extended 92.9% 48.0% 91.8%MSTParser 91.5% 42.5% ?K08 standard 92.0% ?
?K&C10 model 1 93.0% ?
?K&C10 model 2 92.9% ?
?Table 4: Final test accuracies for English.
UAS = unla-beled attachment score; UEM = unlabeled exact match;LAS = labeled attachment score.4.1 Development ExperimentsTable 3 shows the effect of new features on the de-velopment test data for English.
We start with thebaseline features in Table 1, and incrementally addthe distance, valency, unigram, third-order and labelset feature templates in Table 2.
Each group of newfeature templates improved the accuracies over theprevious system, and the final accuracy with all newfeatures was 93.14% in unlabeled attachment score.4.2 Final Test ResultsTable 4 shows the final test results of ourparser for English.
We include in the tableresults from the pure transition-based parser ofZhang and Clark (2008) (row ?Z&C08 transition?
),the dynamic-programming arc-standard parser ofHuang and Sagae (2010) (row ?H&S10?
), and graph-based models including MSTParser (McDonald andPereira, 2006), the baseline feature parser of Koo etal.
(2008) (row ?K08 baeline?
), and the two modelsof Koo and Collins (2010).
Our extended parser sig-nificantly outperformed the baseline parser, achiev-UAS UEM LASZ&C08 transition 84.3% 32.8% ?H&S10 85.2% 33.7% ?this paper extended 86.0% 36.9% 84.4%Table 5: Final test accuracies for Chinese.
UAS = unla-beled attachment score; UEM = unlabeled exact match;LAS = labeled attachment score.ing the highest attachment score reported for atransition-based parser, comparable to those of thebest graph-based parsers.Our experiments were performed on a Linux plat-form with a 2GHz CPU.
The speed of our baselineparser was 50 sentences per second.
With all newfeatures added, the speed dropped to 29 sentencesper second.As an alternative to Penn2Malt, bracketed sen-tences can also be transformed into Stanford depen-dencies (De Marneffe et al, 2006).
Our parser gave93.5% UAS, 91.9% LAS and 52.1% UEM whentrained and evaluated on Stanford basic dependen-cies, which are projective dependency trees.
Cer etal.
(2010) report results on Stanford collapsed de-pendencies, which allow a word to have multipleheads and therefore cannot be produced by a reg-ular dependency parser.
Their results are relevantalthough not directly comparable with ours.4.3 Chinese Test ResultsTable 5 shows the results of our final parser, the puretransition-based parser of Zhang and Clark (2008),and the parser of Huang and Sagae (2010) on Chi-nese.
We take the standard split of CTB and use goldsegmentation and POS-tags for the input.
Our scoresfor this test set are the best reported so far and sig-nificantly better than the previous systems.5 ConclusionWe have shown that enriching the feature repre-sentation significantly improves the accuracy of ourtransition-based dependency parser.
The effect ofthe new features appears to outweigh the effect ofcombining transition-based and graph-based mod-els, reported by Zhang and Clark (2008), as wellas the effect of using dynamic programming, as in-Huang and Sagae (2010).
This shows that featuredefinition is a crucial aspect of transition-based pars-191ing.
In fact, some of the new feature templates in thispaper, such as distance and valency, are among thosewhich are in the graph-based submodel of Zhangand Clark (2008), but not the transition-based sub-model.
Therefore our new features to some extentachieved the same effect as their model combina-tion.
The new features are also hard to use in dy-namic programming because they add considerablecomplexity to the parse items.Enriched feature representations have been stud-ied as an important factor for improving the accu-racies of graph-based dependency parsing also.
Re-cent research including the use of loopy belief net-work (Smith and Eisner, 2008), integer linear pro-gramming (Martins et al, 2009) and an improveddynamic programming algorithm (Koo and Collins,2010) can be seen as methods to incorporate non-local features into a graph-based model.An open source release of our parser, togetherwith trained models for English and Chinese, arefreely available.3AcknowledgementsWe thank the anonymous reviewers for their usefulcomments.
Yue Zhang is supported by the Euro-pean Union Seventh Framework Programme (FP7-ICT-2009-4) under grant agreement no.
247762.ReferencesXavier Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proceedings of theCoNLL Shared Task Session of EMNLP/CoNLL, pages957?961, Prague, Czech Republic.Daniel Cer, Marie-Catherine de Marneffe, Dan Juraf-sky, and Chris Manning.
2010.
Parsing to stan-ford dependencies: Trade-offs between speed and ac-curacy.
In Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC?10).Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP, pages 1?8, Philadelphia, USA.Marie-catherine De Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC.3http://www.sourceforge.net/projects/zpar.
version 0.5.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Porceedings of HLT/NAACL, pages742?750, Los Angeles, California, June.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
In Pro-ceedings of ACL, pages 1077?1086, Uppsala, Sweden,July.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proceedings of EMNLP, pages 1222?1231,Singapore.Richard Johansson and Pierre Nugues.
2007.
Incre-mental dependency parsing using online learning.
InProceedings of CoNLL/EMNLP, pages 1134?1138,Prague, Czech Republic.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of ACL,pages 1?11, Uppsala, Sweden, July.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Pro-ceedings of ACL/HLT, pages 595?603, Columbus,Ohio, June.Andre Martins, Noah Smith, and Eric Xing.
2009.
Con-cise integer linear programming formulations for de-pendency parsing.
In Proceedings of ACL/IJCNLP,pages 342?350, Suntec, Singapore, August.Ryan McDonald and Joakim Nivre.
2007.
Characteriz-ing the errors of data-driven dependency parsing mod-els.
In Proceedings of EMNLP/CoNLL, pages 122?131, Prague, Czech Republic.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing algo-rithms.
In Proceedings of EACL, pages 81?88, Trento,Italy, April.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of ACL, pages 91?98, AnnArbor, Michigan, June.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006a.Maltparser: A data-driven parser-generator for depen-dency parsing.
pages 2216?2219.Joakim Nivre, Johan Hall, Jens Nilsson, Gu?ls?en Eryig?it,and Svetoslav Marinov.
2006b.
Labeled pseudo-projective dependency parsing with support vector ma-chines.
In Proceedings of CoNLL, pages 221?225,New York, USA.Joakim Nivre.
2006.
Inductive Dependency Parsing.Springer.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependency pars-ing and domain adaptation with LR models and parserensembles.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007, pages 1044?1050,192Prague, Czech Republic, June.
Association for Com-putational Linguistics.David Smith and Jason Eisner.
2008.
Dependency pars-ing by belief propagation.
In Proceedings of EMNLP,pages 145?156, Honolulu, Hawaii, October.Ivan Titov and James Henderson.
2007.
A latent variablemodel for generative dependency parsing.
In Proceed-ings of IWPT, pages 144?155, Prague, Czech Repub-lic, June.Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, andXihong Wu.
2006.
Chinese word segmentation withmaximum entropy and n-gram language model.
InProceedings of SIGHAN Workshop, pages 138?141,Sydney, Australia, July.H Yamada and Y Matsumoto.
2003.
Statistical depen-dency analysis using support vector machines.
In Pro-ceedings of IWPT, Nancy, France.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: investigating and combining graph-basedand transition-based dependency parsing using beam-search.
In Proceedings of EMNLP, Hawaii, USA.Yue Zhang and Stephen Clark.
2009.
Transition-basedparsing of the Chinese Treebank using a global dis-criminative model.
In Proceedings of IWPT, Paris,France, October.193
