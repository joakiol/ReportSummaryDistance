Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 50?58,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsGeneral estimation and evaluationof compositional distributional semantic modelsGeorgiana Dinu and Nghia The Pham and Marco BaroniCenter for Mind/Brain Sciences (University of Trento, Italy)(georgiana.dinu|thenghia.pham|marco.baroni)@unitn.itAbstractIn recent years, there has been widespreadinterest in compositional distributionalsemantic models (cDSMs), that derivemeaning representations for phrases fromtheir parts.
We present an evaluation of al-ternative cDSMs under truly comparableconditions.
In particular, we extend theidea of Baroni and Zamparelli (2010) andGuevara (2010) to use corpus-extractedexamples of the target phrases for param-eter estimation to the other models pro-posed in the literature, so that all modelscan be tested under the same training con-ditions.
The linguistically motivated func-tional model of Baroni and Zamparelli(2010) and Coecke et al(2010) emergesas the winner in all our tests.1 IntroductionThe need to assess similarity in meaning is cen-tral to many language technology applications,and distributional methods are the most robust ap-proach to the task.
These methods measure wordsimilarity based on patterns of occurrence in largecorpora, following the intuition that similar wordsoccur in similar contexts.
More precisely, vectorspace models, the most widely used distributionalmodels, represent words as high-dimensional vec-tors, where the dimensions represent (functionsof) context features, such as co-occurring contextwords.
The relatedness of two words is assessedby comparing their vector representations.The question of assessing meaning similarityabove the word level within the distributionalparadigm has received a lot of attention in re-cent years.
A number of compositional frame-works have been proposed in the literature, eachof these defining operations to combine word vec-tors into representations for phrases or even en-tire sentences.
These range from simple but ro-bust methods such as vector addition to more ad-vanced methods, such as learning function wordsas tensors and composing constituents through in-ner product operations.
Empirical evaluations inwhich alternative methods are tested in compara-ble settings are thus called for.
This is compli-cated by the fact that the proposed compositionalframeworks package together a number of choicesthat are conceptually distinct, but difficult to disen-tangle.
Broadly, these concern (i) the input repre-sentations fed to composition; (ii) the compositionoperation proper; (iii) the method to estimate theparameters of the composition operation.For example, Mitchell and Lapata in their clas-sic 2010 study propose a set of composition op-erations (multiplicative, additive, etc.
), but theyalso experiment with two different kinds of inputrepresentations (vectors recording co-occurrencewith words vs. distributions over latent topics) anduse supervised training via a grid search over pa-rameter settings to estimate their models.
Gue-vara (2010), to give just one further example, isnot only proposing a different composition methodwith respect to Mitchell and Lapata, but he isalso adopting different input vectors (word co-occurrences compressed via SVD) and an unsu-pervised estimation method based on minimizingthe distance of composed vectors to their equiva-lents directly extracted from the source corpus.Blacoe and Lapata (2012) have recently high-lighted the importance of teasing apart the differ-ent aspects of a composition framework, present-ing an evaluation in which different input vectorrepresentations are crossed with different compo-sition methods.
However, two out of three com-position methods they evaluate are parameter-free,so that they can side-step the issue of fixing the pa-rameter estimation method.In this work, we evaluate all composition meth-ods we know of, excluding a few that lag be-50hind the state of the art or are special cases ofthose we consider, while keeping the estimationmethod constant.
This evaluation is made pos-sible by our extension to all target compositionmodels of the corpus-extracted phrase approxima-tion method originally proposed in ad-hoc settingsby Baroni and Zamparelli (2010) and Guevara(2010).
For the models for which it is feasible,we compare the phrase approximation approachto supervised estimation with crossvalidation, andshow that phrase approximation is competitive,thus confirming that we are not comparing mod-els under poor training conditions.
Our tests areconducted over three tasks that involve differentsyntactic constructions and evaluation setups.
Fi-nally, we consider a range of parameter settings forthe input vector representations, to insure that ourresults are not too brittle or parameter-dependent.12 Composition frameworksDistributional semantic models (DSMs) approxi-mate word meanings with vectors recording theirpatterns of co-occurrence with corpus contexts(e.g., other words).
There is an extensive literatureon how to develop such models and on their eval-uation (see, e.g., Clark (2012), Erk (2012), Tur-ney and Pantel (2010)).
We focus here on compo-sitional DSMs (cDSMs).
After discussing someoptions pertaining to the input vectors, we reviewall the composition operations we are aware of(excluding only the tensor-product-based modelsshown by Mitchell and Lapata (2010) to be muchworse than simpler models),2 and then methods toestimate their parameters.Input vectors Different studies have assumeddifferent distributional inputs to composition.These include bag-of-words co-occurrence vec-tors, possibly mapped to lower dimensionalitywith SVD or other techniques (Mitchell and La-pata (2010) and many others), vectors whose di-1We made the software we used to construct seman-tic models and estimate and test composition methodsavailable online at http://clic.cimec.unitn.it/composes/toolkit/2Erk and Pado?
(2008) and Thater et al(2010) use in-put vectors that have been adapted to their phrasal contexts,but then apply straightforward composition operations suchas addition and multiplication to these contextualized vec-tors.
Their approaches are thus not alternative cDSMs, butspecial ways to construct the input vectors.
Grefenstette andSadrzadeh (2011a; 2011b) and Kartsaklis et al(2012) pro-pose estimation techniques for the tensors in the functionalmodel of Coecke et al(2010).
Turney (2012) does not com-pose representations but similarity scores.Model Composition function ParametersAdd w1~u + w2~v w1, w2Mult ~uw1  ~vw2 w1, w2Dil ||~u||22~v + (?
?
1)?~u,~v?~u ?Fulladd W1~u + W2~v W1,W2 ?
Rm?mLexfunc Au~v Au ?
Rm?mFulllex tanh([W1,W2]hAu~vAv~ui) W1,W2,Au, Av ?
Rm?mTable 1: Composition functions of inputs (u, v).mensions record the syntactic link between targetsand collocates (Erk and Pado?, 2008; Thater et al2010), and most recently vectors based on neurallanguage models (Socher et al 2011; Socher etal., 2012).
Blacoe and Lapata (2012) comparedthe three representations on phrase similarity andparaphrase detection, concluding that ?simple isbest?, that is, the bag-of-words approach performsat least as good or better than either syntax-basedor neural representations across the board.
Here,we take their message home and we focus on bag-of-words representations, exploring the impact ofvarious parameters within this approach.Most frameworks assume that word vectorsconstitute rigid inputs fixed before composition,often using a separate word-similarity task inde-pendent of composition.
The only exception isSocher et al(2012), where the values in the in-put vectors are re-estimated during compositionparameter optimization.
Our re-implementation oftheir method assumes rigid input vectors instead.Composition operations Mitchell and Lapata(2008; 2010) present a set of simple but effec-tive models in which each component of the outputvector is a function of the corresponding compo-nents of the inputs.
Given input vectors ~u and ~v,the weighted additive model (Add) returns theirweighted sum: ~p = w1~u + w2~v.
In the dilationmodel (Dil), the output vector is obtained by de-composing one of the input vectors, say ~v, intoa vector parallel to ~u and an orthogonal vector,and then dilating only the parallel vector by a fac-tor ?
before re-combining (formula in Table 1).Mitchell and Lapata also propose a simple mul-tiplicative model in which the output componentsare obtained by component-wise multiplication ofthe corresponding input components.
We intro-duce here its natural weighted extension (Mult),that takes w1 and w2 powers of the componentsbefore multiplying, such that each phrase compo-nent pi is given by: pi = uw1i vw2i .51Guevara (2010) and Zanzotto et al(2010) ex-plore a full form of the additive model (Fulladd),where the two vectors entering a composition pro-cess are pre-multiplied by weight matrices beforebeing added, so that each output component isa weighted sum of all input components: ~p =W1~u + W2~v.Baroni and Zamparelli (2010) and Coecke etal.
(2010), taking inspiration from formal seman-tics, characterize composition as function applica-tion.
For example, Baroni and Zamparelli modeladjective-noun phrases by treating the adjectiveas a function from nouns onto (modified) nouns.Given that linear functions can be expressed bymatrices and their application by matrix-by-vectormultiplication, a functor (such as the adjective) isrepresented by a matrix Au to be composed withthe argument vector ~v (e.g., the noun) by multi-plication, returning the lexical function (Lexfunc)representation of the phrase: ~p = Au~v.The method proposed by Socher et al(2012)(see Socher et al(2011) for an earlier proposalfrom the same team) can be seen as a combinationand non-linear extension of Fulladd and Lexfunc(that we thus call Fulllex) in which both phraseelements act as functors (matrices) and arguments(vectors).
Given input terms u and v representedby (~u,Au) and (~v,Av), respectively, their com-position vector is obtained by applying first a lin-ear transformation and then the hyperbolic tangentfunction to the concatenation of the products Au~vand Av~u (see Table 1 for the equation).
Socherand colleagues also present a way to construct ma-trix representations for specific phrases, neededto scale this composition method to larger con-stituents.
We ignore it here since we focus on thetwo-word case.Estimating composition parameters If wehave manually labeled example data for a targettask, we can use supervised machine learning tooptimize parameters.
Mitchell and Lapata (2008;2010), since their models have just a few param-eters to optimize, use a direct grid search for theparameter setting that performs best on the train-ing data.
Socher et al(2012) train their modelsusing multinomial softmax classifiers.If our goal is to develop a cDSM optimized fora specific task, supervised methods are undoubt-edly the most promising approach.
However, ev-ery time we face a new task, parameters must bere-estimated from scratch, which goes against theidea of distributional semantics as a general sim-ilarity resource (Baroni and Lenci, 2010).
More-over, supervised methods are highly composition-model-dependent, and for models such as Fulladdand Lexfunc we are not aware of proposals abouthow to estimate them in a supervised manner.Socher et al(2011) propose an autoencodingstrategy.
Given a decomposition function that re-constructs the constituent vectors from a phrasevector (e.g., it re-generates green and jacket vec-tors from the composed green jacket vector), thecomposition parameters minimize the distance be-tween the original and reconstructed input vectors.This method does not require hand-labeled train-ing data, but it is restricted to cDSMs for whichan appropriate decomposition function can be de-fined, and even in this case the learning problemmight lack a closed-form solution.Guevara (2010) and Baroni and Zamparelli(2010) optimize parameters using examples ofhow the output vectors should look like that aredirectly extracted from the corpus.
To learn, say, aLexfunc matrix representing the adjective green,we extract from the corpus example vectors of?N, green N?
pairs that occur with sufficient fre-quency (?car, green car?, ?jacket, green jacket?,?politician, green politician?, .
.
.
).
We then useleast-squares methods to find weights for the greenmatrix that minimize the distance between thegreen N vectors generated by the model given theinput N and the corresponding corpus-observedphrase vectors.
This is a very general approach, itdoes not require hand-labeled data, and it has thenice property that corpus-harvested phrase vec-tors provide direct evidence of the polysemous be-haviour of functors (the green jacket vs. politiciancontexts, for example, will be very different).
Inthe next section, we extend the corpus-extractedphrase approximation method to all cDSMs de-scribed above, with closed-form solutions for allbut the Fulllex model, for which we propose arapidly converging iterative estimation method.3 Least-squares model estimation usingcorpus-extracted phrase vectors3Notation Given two matricesX,Y ?
Rm?n wedenote their inner product by ?X,Y ?, (?X,Y ?
=?mi=1?nj=1 xijyij).
Similarly we denote by?u, v?
the dot product of two vectors u, v ?
Rm?1and by ||u|| the Euclidean norm of a vector:3Proofs omitted due to space constraints.52||u|| = ?u, u?1/2.
We use the following Frobe-nius norm notation: ||X||F = ?X,X?1/2.
Vectorsare assumed to be column vectors and we use xito stand for the i-th (m ?
1)-dimensional columnof matrix X .
We use [X,Y ] ?
Rm?2n to denotethe horizontal concatenation of two matrices while[XY]?
R2m?n is their vertical concatenation.General problem statement We assume vocab-ularies of constituents U , V and that of resultingphrases P .
The training data consist of a set oftuples (u, v, p) where p stands for the phrase asso-ciated to the constituents u and v:T = {(ui, vi, pi)|(ui, vi, pi) ?
U?V?P, 1 ?
i ?
k}We build the matrices U, V, P ?
Rm?k by con-catenating the vectors associated to the trainingdata elements as columns.4Given the training data matrices, the generalproblem can be stated as:??
= arg min?||P ?
fcomp?
(U, V )||Fwhere fcomp?
is a composition function and ?stands for a list of parameters that this compositionfunction is associated to.
The composition func-tions are defined: fcomp?
: Rm?1 ?
Rm?1 ?Rm?1 and fcomp?
(U, V ) stands for their naturalextension when applied on the individual columnsof the U and V matrices.Add The weighted additive model returns thesum of the composing vectors which have beenre-weighted by some scalars w1 and w2: ~p =w1~u + w2~v.
The problem becomes:w?1, w?2 = arg minw1,w2?R||P ?
w1U ?
w2V ||FThe optimal w1 and w2 are given by:w?1 =||V ||2F ?U,P ?
?
?U, V ?
?V, P ?||U ||2F ||V ||2F ?
?U, V ?2(1)w?2 =||U ||2F ?V, P ?
?
?U, V ?
?U,P ?||U ||2F ||V ||2F ?
?U, V ?2(2)4In reality, not all composition models require u, v and pto have the same dimensionality.Dil Given two vectors ~u and ~v, the dilationmodel computes the phrase vector ~p = ||~u||2~v +(?
?
1)?~u,~v?~u where the parameter ?
is a scalar.The problem becomes:??
= arg min?
?R||P ?V D||ui||2 ?UD(?
?1)?ui,vi?||Fwhere by D||ui||2 and D(??1)?ui,vi?
we denotediagonal matrices with diagonal elements (i, i)given by ||ui||2 and (?
?
1)?ui, vi?
respectively.The solution is:??
= 1?
?ki=1?ui, (||ui||2vi ?
pi)?
?ui, vi?
?ki=1?ui, vi?2||ui||2Mult Given two vectors ~u and ~v, the weightedmultiplicative model computes the phrase vector~p = ~uw1  ~vw2 where  stands for component-wise multiplication.
We assume for this model thatU, V, P ?
Rm?n++ , i.e.
that the entries are strictlylarger than 0: in practice we add a small smooth-ing constant to all elements to achieve this (Multperforms badly on negative entries, such as thoseproduced by SVD).
We use the w1 and w2 weightsobtained when solving the much simpler relatedproblem:5w?1, w?2 = arg minw1,w2?R||log(P )?log(U.?w1V.?w2)||Fwhere .?
stands for the component-wise power op-eration.
The solution is the same as that for Add,given in equations (1) and (2), with U ?
log(U),V ?
log(V ) and P ?
log(P ).Fulladd The full additive model assumes thecomposition of two vectors to be ~p = W1~u+W2~vwhere W1,W2 ?
Rm?m.
The problem is:[W1,W2]?
= arg min[W1,W2]?Rm?2m||P?
[W1W2][UV]||This is a multivariate linear regression prob-lem (Hastie et al 2009) for which the leastsquares estimate is given by: [W1,W2] =((XTX)?1XTY )T where we use X = [UT , V T ]and Y = P T .Lexfunc The lexical function compositionmethod learns a matrix representation for eachfunctor (given by U here) and defines compositionas matrix-vector multiplication.
More precisely:5In practice training Mult this way achieves similar orlower errors in comparison to Add.53~p = Au~v where Au is a matrix associated to eachfunctor u ?
U .
We denote by Tu the trainingdata subset associated to an element u, whichcontains only tuples which have u as first element.Learning the matrix representations amounts tosolving the set of problems:Au = arg minAu?Rm?m||Pu ?AuVu||for each u ?
U where Pu, Vu ?
Rm?|Tu|are the matrices corresponding to the Tu train-ing subset.
The solutions are given by: Au =((VuV Tu )?1VuP Tu )T .
This composition functiondoes not use the functor vectors.Fulllex This model can be seen as a generaliza-tion of Lexfunc which makes no assumption onwhich of the constituents is a functor, so that bothwords get a matrix and a vector representation.The composition function is:~p = tanh([W1,W2][Au~vAv~u])where Au and Av are the matrices associated toconstituents u and v and [W1,W2] ?
Rm?2m.The estimation problem is given in Figure 1.This is the only composition model which doesnot have a closed-form solution.
We use a blockcoordinate descent method, in which we fix eachof the matrix variables but one and solve the corre-sponding least-squares linear regression problem,for which we can use the closed-form solution.Fixing everything but [W1,W2]:[W ?1 ,W?2 ] = ((XTX)?1XTY )TX =[[Au1 ~v1, ..., Auk ~vk][Av1 ~u1, ..., Avk ~uk]]TY = atanh(P T )Fixing everything but Au for some element u,the objective function becomes:||atanh(Pu)?W1AuVu?W2[Av1~u, ..., Avk?~u]||Fwhere v1...vk?
?
V are the elements occurringwith u in the training data and Vu the matrix result-ing from their concatenation.
The update formulafor the Au matrices becomes:A?u = W?11 ((XTX)?1XTY )TX = V TuY = (atanh(Pu)?W2[Av1~u, ..., Avk?~u])TIn all our experiments, Fulllex estimation con-verges after very few passes though the matrices.Despite the very large number of parameters ofthis model, when evaluating on the test data we ob-serve that using a higher dimensional space (suchas 200 dimensions) still performs better than alower dimensional one (e.g., 50 dimensions).4 Evaluation setup and implementation4.1 DatasetsWe evaluate the composition methods on threephrase-based benchmarks that test the models ona variety of composition processes and similarity-based tasks.Intransitive sentences The first dataset, intro-duced by Mitchell and Lapata (2008), focuses onsimple sentences consisting of intransitive verbsand their noun subjects.
It contains a total of120 sentence pairs together with human similar-ity judgments on a 7-point scale.
For exam-ple, conflict erupts/conflict bursts is scored 7, skinglows/skin burns is scored 1.
On average, eachpair is rated by 30 participants.
Rather than eval-uating against mean scores, we use each rating asa separate data point, as done by Mitchell and La-pata.
We report Spearman correlations betweenhuman-assigned scores and model cosine scores.Adjective-noun phrases Turney (2012) intro-duced a dataset including both noun-noun com-pounds and adjective-noun phrases (ANs).
Wefocus on the latter, and we frame the task dif-ferently from Turney?s original definition due todata sparsity issues.6 In our version, the datasetcontains 620 ANs, each paired with a single-noun paraphrase.
Examples include: archaeolog-ical site/dig, spousal relationship/marriage anddangerous undertaking/adventure.
We evaluate amodel by computing the cosine of all 20K nouns inour semantic space with the target AN, and look-ing at the rank of the correct paraphrase in this list.The lower the rank, the better the model.
We re-port median rank across the test items.Determiner phrases The last dataset, intro-duced in Bernardi et al(2013), focuses on aclass of grammatical terms (rather than content6Turney used a corpus of about 50 billion words, almost20 times larger than ours, and we have very poor or no cov-erage of many original items, making the ?multiple-choice?evaluation proposed by Turney meaningless in our case.54W ?1 ,W?2 , A?u1 , ..., A?v1 , ... =arg minRm?m||atanh(P T )?
[W1,W2][[Au1 ~v1, ..., Auk ~vk][Av1 ~u1, ..., Avk ~uk]]||F=arg minRm?m||atanh(P T )?W1[Au1 ~v1, ..., Auk ~vk]?W2[Av1 ~u1, ..., Avk ~uk]||FFigure 1: Fulllex estimation problem.words), namely determiners.
It is a multiple-choice test where target nouns (e.g., amnesia)must be matched with the most closely relateddeterminer(-noun) phrases (DPs) (e.g., no mem-ory).
The task differs from the previous one alsobecause here the targets are single words, and therelated items are composite.
There are 173 tar-get nouns in total, each paired with one correctDP response, as well as 5 foils, namely the de-terminer (no) and noun (memory) from the correctresponse and three more DPs, two of which con-tain the same noun as the correct phrase (less mem-ory, all memory), the third the same determiner(no repertoire).
Other examples of targets/related-phrases are polysemy/several senses and tril-ogy/three books.
The models compute cosines be-tween target noun and responses and are scoredbased on their accuracy at ranking the correctphrase first.4.2 Input vectorsWe extracted distributional semantic vectors us-ing as source corpus the concatenation of ukWaC,Wikipedia (2009 dump) and BNC, 2.8 billion to-kens in total.7 We use a bag-of-words approachand we count co-occurrences within sentences andwith a limit of maximally 50 words surroundingthe target word.
By tuning on the MEN lexicalrelatedness dataset,8 we decided to use the top10K most frequent content lemmas as context fea-tures (vs. top 10K inflected forms), and we experi-mented with positive Pointwise and Local MutualInformation (Evert, 2005) as association measures(vs. raw counts, log transform and a probabilityratio measure) and dimensionality reduction byNon-negative Matrix Factorization (NMF, Lee andSeung (2000)) and Singular Value Decomposition(SVD, Golub and Van Loan (1996)) (both outper-forming full dimensionality vectors on MEN).
For7http://wacky.sslmit.unibo.it;http://www.natcorp.ox.ac.uk8http://clic.cimec.unitn.it/?elia.bruni/MENboth reduction techniques, we varied the numberof dimensions to be preserved from 50 to 300 in50-unit intervals.
As Local Mutual Informationperformed very poorly across composition exper-iments and other parameter choices, we droppedit.
We will thus report, for each experiment andcomposition method, the distribution of the rele-vant performance measure across 12 input settings(NMF vs. SVD times 6 dimensionalities).
How-ever, since the Mult model, as expected, workedvery poorly when the input vectors contained neg-ative values, as is the case with SVD, for thismodel we report result distributions across the 6NMF variations only.4.3 Composition model estimationTraining by approximating the corpus-extractedphrase vectors requires corpus-based examples ofinput (constituent word) and output (phrase) vec-tors for the composition processes to be learned.In all cases, training examples are simply selectedbased on corpus frequency.
For the first experi-ment, we have 42 distinct target verbs and a totalof ?20K training instances, that is, ?
?noun, verb?,noun-verb?
tuples (505 per verb on average).
Forthe second experiment, we have 479 adjectives and?1 million ?
?adjective, noun?, adjective-noun?training tuples (2K per adjective on average).
Inthe third, 50 determiners and 50K ?
?determiner,noun?, determiner-noun?
tuples (1K per deter-miner).
For all models except Lexfunc and Ful-llex, training examples are pooled across target el-ements to learn a single set of parameters.
TheLexfunc model takes only argument word vectorsas inputs (the functors in the three datasets areverbs, adjectives and determiners, respectively).
Aseparate weight matrix is learned for each func-tor, using the corresponding training data.9 TheFulllex method jointly learns distinct matrix rep-resentations for both left- and right-hand side con-9For the Lexfunc model we have experimented with leastsqueares regression with and without regularization, obtain-ing similar results.55stituents.
For this reason, we must train this modelon balanced datasets.
More precisely, for the in-transitive verb experiments, we use training datacontaining noun-verb phrases in which the verbsand the nouns are present in the lists of 1,500most frequent verbs/nouns respectively, adding tothese the verbs and nouns present in our dataset.We obtain 400K training tuples.
We create thetraining data similarity for the other datasets ob-taining 440K adjective-noun and 50K determinerphrase training tuples, respectively (we also exper-imented with Fulllex trained on the same tuplesused for the other models, obtaining considerablyworse results than those reported).
Finally, for Dilwe treat direction of stretching as a further param-eter to be optimized, and find that for intransitivesit is better to stretch verbs, in the other datasetsnouns.For the simple composition models for whichparameters consist of one or two scalars, namelyAdd, Mult and Dil, we also tune the parame-ters through 5-fold crossvalidation on the datasets,directly optimizing the parameters on the targettasks.
For Add and Mult, we search w1, w2through the crossproduct of the interval [0 : 5] in0.2-sized steps.
For Dil we use ?
?
[0 : 20], againin 0.2-sized steps.5 Evaluation resultsWe begin with some remarks pertaining to theoverall quality of and motivation for corpus-phrase-based estimation.
In seven out of ninecomparisons of this unsupervised technique withfully supervised crossvalidation (3 ?simple?
mod-els ?Add, Dil and Mult?
times 3 test sets), therewas no significant difference between the two esti-mation methods.10 Supervised estimation outper-formed the corpus-phrase-based method only forDil on the intransitive sentence and AN bench-marks, but crossvalidated Dil was outperformedby at least one phrase-estimated simple model onboth benchmarks.The rightmost boxes in the panels of Fig-ure 2 depict the performance distribution for us-ing phrase vectors directly extracted from thecorpus to tackle the various tasks.
This non-compositional approach outperforms all composi-tional methods in two tasks over three, and it isone of the best approaches in the third, although10Significance assessed through Tukey Honestly Signifi-cant Difference tests (Abdi and Williams, 2010), ?
= 0.05.in all cases even its top scores are far from thetheoretical ceiling.
Still, performance is impres-sive, especially in light of the fact that the non-compositional approach suffers of serious data-sparseness problems.
Performance on the intran-sitive task is above state-of-the-art despite the factthat for almost half of the cases one test phraseis not in the corpus, resulting in 0 vectors andconsequently 0 similarity pairs.
The other bench-marks have better corpus-phrase coverage (nearlyperfect AN coverage; for DPs, about 90% correctphrase responses are in the corpus), but many tar-get phrases occur only rarely, leading to unreliabledistributional vectors.
We interpret these results asa goodmotivation for corpus-phrase-based estima-tion.
On the one hand they show how good thesevectors are, and thus that they are sensible targetsof learning.
On the other hand, they do not suffice,since natural language is infinitely productive andthus no corpus can provide full phrase coverage,justifying the whole compositional enterprise.The other boxes in Figure 2 report the perfor-mance of the composition methods trained by cor-pus phrase approximation.
Nearly all models aresignificantly above chance in all tasks, except forFulladd on intransitive sentences.
To put AN me-dian ranks into perspective, consider that a medianrank as high as 8,300 has near-0 probability to oc-cur by chance.
For DP accuracy, random guessinggets 0.17% accuracy.Lexfunc emerges consistently as the best model.On intransitive constructions, it significantly out-performs all other models except Mult, but the dif-ference approaches significance even with respectto the latter (p = 0.071).
On this task, Lexfunc?smedian correlation (0.26) is nearly equivalent tothe best correlation across a wide range of parame-ters reported by Erk and Pado?
(2008) (0.27).
In theAN task, Lexfunc significantly outperforms Ful-llex and Dil and, visually, its distribution is slightlymore skewed towards lower (better) ranks than anyother model.
In the DP task, Lexfunc significantlyoutperforms Add and Mult and, visually, most ofits distribution lies above that of the other mod-els.
Most importantly, Lexfunc is the only modelthat is consistent across the three tasks, with allother models displaying instead a brittle perfor-mance pattern.11Still, the top-performance range of all models11No systematic trend emerged pertaining to the input vec-tor parameters (SVD vs. NMF and retained dimension num-ber).56Add Dil Mult FulladdLexfunc FulllexCorpus0.000.050.100.150.200.250.30 Intransitive sentenceslllllAdd Dil Mult FulladdLexfunc FulllexCorpus1000800600400200ANsllAdd Dil Mult FulladdLexfunc FulllexCorpus0.150.200.250.300.35DPsFigure 2: Boxplots displaying composition model performance distribution on three benchmarks, acrossinput vector settings (6 datapoints for Mult, 12 for all other models).
For intransitive sentences, figure ofmerit is Spearman correlation, for ANs median rank of correct paraphrase, and for DPs correct responseaccuracy.
The boxplots display the distribution median as a thick horizontal line within a box extendingfrom first to third quartile.
Whiskers cover 1.5 of interquartile range in each direction from the box, andextreme outliers outside this extended range are plotted as circles.on the three tasks is underwhelming, and none ofthem succeeds in exploiting compositionality todo significantly better than using whatever phrasevectors can be extracted from the corpus directly.Clearly, much work is still needed to develop trulysuccessful cDSMs.The AN results might look particularly worry-ing, considering that even the top (lowest) medianranks are above 100.
A qualitative analysis, how-ever, suggests that the actual performance is notas bad as the numerical scores suggest, since of-ten the nearest neighbours of the ANs to be para-phrased are nouns that are as strongly related tothe ANs as the gold standard response (althoughnot necessarily proper paraphrases).
For example,the gold response to colorimetric analysis is col-orimetry, whereas the Lexfunc (NMF, 300 dimen-sions) nearest neighbour is chromatography; thegold response to heavy particle is baryon, whereasLexfunc proposes muon; for melodic phrase thegold is tune and Lexfunc has appoggiatura; for in-door garden, the gold is hothouse but Lexfunc pro-poses glasshouse (followed by the more sophisti-cated orangery!
), and so on and so forth.6 ConclusionWe extended the unsupervised corpus-extractedphrase approximation method of Guevara (2010)and Baroni and Zamparelli (2010) to estimateall known state-of-the-art cDSMs, using closed-form solutions or simple iterative procedures inall cases.
Equipped with a general estimation ap-proach, we thoroughly evaluated the cDSMs ina comparable setting.
The linguistically moti-vated Lexfunc model of Baroni and Zamparelli(2010) and Coecke et al(2010) was the win-ner across three composition tasks, also outper-forming the more complex Fulllex model, our re-implementation of Socher et als (2012) compo-sition method (of course, the composition methodis only one aspect of Socher et als architecture).All other composition methods behaved inconsis-tently.In the near future, we want to focus on improv-ing estimation itself.
In particular, we want toexplore ways to automatically select good phraseexamples for training, beyond simple frequencythresholds.
We tested composition methods ontwo-word phrase benchmarks.
Another naturalnext step is to apply the composition rules recur-sively, to obtain representations of larger chunks,up to full sentences, coming, in this way, nearer tothe ultimate goal of compositional distributionalsemantics.AcknowledgmentsWe acknowledge ERC 2011 Starting IndependentResearch Grant n. 283554 (COMPOSES).57ReferencesHerve?
Abdi and Lynne Williams.
2010.
Newman-Keuls and Tukey test.
In Neil Salkind, Bruce Frey,and Dondald Dougherty, editors, Encyclopedia ofResearch Design, pages 897?904.
Sage, ThousandOaks, CA.Marco Baroni and Alessandro Lenci.
2010.
Dis-tributional Memory: A general framework forcorpus-based semantics.
Computational Linguis-tics, 36(4):673?721.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of EMNLP, pages 1183?1193, Boston,MA.Raffaella Bernardi, Georgiana Dinu, Marco Marelli,and Marco Baroni.
2013.
A relatedness benchmarkto test the role of determiners in compositional dis-tributional semantics.
In Proceedings of ACL (ShortPapers), Sofia, Bulgaria.
In press.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for seman-tic composition.
In Proceedings of EMNLP, pages546?556, Jeju Island, Korea.Stephen Clark.
2012.
Vector space models of lexicalmeaning.
In Shalom Lappin and Chris Fox, editors,Handbook of Contemporary Semantics, 2nd edition.Blackwell, Malden, MA.
In press.Bob Coecke, Mehrnoosh Sadrzadeh, and StephenClark.
2010.
Mathematical foundations for a com-positional distributional model of meaning.
Linguis-tic Analysis, 36:345?384.Katrin Erk and Sebastian Pado?.
2008.
A structuredvector space model for word meaning in context.
InProceedings of EMNLP, pages 897?906, Honolulu,HI.Katrin Erk.
2012.
Vector space models of word mean-ing and phrase meaning: A survey.
Language andLinguistics Compass, 6(10):635?653.Stefan Evert.
2005.
The Statistics of Word Cooccur-rences.
Dissertation, Stuttgart University.Gene Golub and Charles Van Loan.
1996.
MatrixComputations (3rd ed.).
JHU Press, Baltimore, MD.Edward Grefenstette and Mehrnoosh Sadrzadeh.2011a.
Experimental support for a categorical com-positional distributional model of meaning.
In Pro-ceedings of EMNLP, pages 1394?1404, Edinburgh,UK.Edward Grefenstette and Mehrnoosh Sadrzadeh.2011b.
Experimenting with transitive verbs in a Dis-CoCat.
In Proceedings of GEMS, pages 62?66, Ed-inburgh, UK.Emiliano Guevara.
2010.
A regression model ofadjective-noun compositionality in distributional se-mantics.
In Proceedings of GEMS, pages 33?37,Uppsala, Sweden.Trevor Hastie, Robert Tibshirani, and Jerome Fried-man.
2009.
The Elements of Statistical Learning,2nd ed.
Springer, New York.Dimitri Kartsaklis, Mehrnoosh Sadrzadeh, and StephenPulman.
2012.
A unified sentence space forcategorical distributional-compositional semantics:Theory and experiments.
In Proceedings of COL-ING: Posters, pages 549?558, Mumbai, India.Daniel Lee and Sebastian Seung.
2000.
Algorithms forNon-negative Matrix Factorization.
In Proceedingsof NIPS, pages 556?562.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL, pages 236?244, Columbus, OH.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Richard Socher, Eric Huang, Jeffrey Pennin, AndrewNg, and Christopher Manning.
2011.
Dynamicpooling and unfolding recursive autoencoders forparaphrase detection.
In Proceedings of NIPS, pages801?809, Granada, Spain.Richard Socher, Brody Huval, Christopher Manning,and Andrew Ng.
2012.
Semantic compositionalitythrough recursive matrix-vector spaces.
In Proceed-ings of EMNLP, pages 1201?1211, Jeju Island, Ko-rea.Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.2010.
Contextualizing semantic representations us-ing syntactically enriched vector models.
In Pro-ceedings of ACL, pages 948?957, Uppsala, Sweden.Peter Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.Peter Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.Journal of Artificial Intelligence Research, 44:533?585.Fabio Zanzotto, Ioannis Korkontzelos, FrancescaFalucchi, and Suresh Manandhar.
2010.
Estimat-ing linear models for compositional distributionalsemantics.
In Proceedings of COLING, pages 1263?1271, Beijing, China.58
