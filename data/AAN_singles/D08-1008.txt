Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 69?78,Honolulu, October 2008. c?2008 Association for Computational LinguisticsDependency-based Semantic Role Labeling of PropBankRichard Johansson and Pierre NuguesLund University, Sweden{richard, pierre}@cs.lth.seAbstractWe present a PropBank semantic role label-ing system for English that is integrated witha dependency parser.
To tackle the problemof joint syntactic?semantic analysis, the sys-tem relies on a syntactic and a semantic sub-component.
The syntactic model is a projec-tive parser using pseudo-projective transfor-mations, and the semantic model uses globalinference mechanisms on top of a pipeline ofclassifiers.
The complete syntactic?semanticoutput is selected from a candidate pool gen-erated by the subsystems.We evaluate the system on the CoNLL-2005 test sets using segment-based anddependency-based metrics.
Using thesegment-based CoNLL-2005 metric, oursystem achieves a near state-of-the-art F1figure of 77.97 on the WSJ+Brown test set,or 78.84 if punctuation is treated consistently.Using a dependency-based metric, the F1figure of our system is 84.29 on the testset from CoNLL-2008.
Our system is thefirst dependency-based semantic role labelerfor PropBank that rivals constituent-basedsystems in terms of performance.1 IntroductionAutomatic semantic role labeling (SRL), the taskof determining who does what to whom, is a use-ful intermediate step in NLP applications perform-ing semantic analysis.
It has obvious applicationsfor template-filling tasks such as information extrac-tion and question answering (Surdeanu et al, 2003;Moschitti et al, 2003).
It has also been used inprototypes of NLP systems that carry out complexreasoning, such as entailment recognition systems(Haghighi et al, 2005; Hickl et al, 2006).
In addi-tion, role-semantic features have recently been usedto extend vector-space representations in automaticdocument categorization (Persson et al, 2008).The NLP community has recently devoted muchattention to developing accurate and robust methodsfor performing role-semantic analysis automatically,and a number of multi-system evaluations have beencarried out (Litkowski, 2004; Carreras andM?rquez,2005; Baker et al, 2007; Surdeanu et al, 2008).Following the seminal work of Gildea and Juraf-sky (2002), there have been many extensions in ma-chine learning models, feature engineering (Xue andPalmer, 2004), and inference procedures (Toutanovaet al, 2005; Surdeanu et al, 2007; Punyakanok etal., 2008).With very few exceptions (e.g.
Collobert andWeston, 2007), published SRL methods have usedsome sort of syntactic structure as input (Gildea andPalmer, 2002; Punyakanok et al, 2008).
Most sys-tems for automatic role-semantic analysis have usedconstituent syntax as in the Penn Treebank (Marcuset al, 1993), although there has also been much re-search on the use of shallow syntax (Carreras andM?rquez, 2004) in SRL.In comparison, dependency syntax has receivedrelatively little attention for the SRL task, despitethe fact that dependency structures offer a moretransparent encoding of predicate?argument rela-tions.
Furthermore, the few systems based on de-pendencies that have been presented have generallyperformed much worse than their constituent-based69counterparts.
For instance, Pradhan et al (2005) re-ported that a system using a rule-based dependencyparser achieved much inferior results compared to asystem using a state-of-the-art statistical constituentparser: The F-measure on WSJ section 23 droppedfrom 78.8 to 47.2, or from 83.7 to 61.7 when usinga head-based evaluation.
In a similar vein, Swansonand Gordon (2006) reported that parse tree path fea-tures extracted from a rule-based dependency parserare much less reliable than those from a modern con-stituent parser.In contrast, we recently carried out a de-tailed comparison (Johansson and Nugues, 2008b)between constituent-based and dependency-basedSRL systems for FrameNet, in which the results ofthe two types of systems where almost equivalentwhen using modern statistical dependency parsers.We suggested that the previous lack of progress independency-based SRL was due to low parsing ac-curacy.
The experiments showed that the grammat-ical function information available in dependencyrepresentations results in a steeper learning curvewhen training semantic role classifiers, and it alsoseemed that the dependency-based role classifierswere more resilient to lexical problems caused bychange of domain.The recent CoNLL-2008 Shared Task (Surdeanuet al, 2008) was an attempt to show that SRL can beaccurately carried out using only dependency syn-tax.
However, these results are not easy to compareto previously published results since the task defini-tions and evaluation metrics were different.This paper compares the best-performing sys-tem in the CoNLL-2008 Shared Task (Johans-son and Nugues, 2008a) with previously publishedconstituent-based SRL systems.
The system carriesout joint dependency-syntactic and semantic anal-ysis.
We first describe its implementation in Sec-tion 2, and then compare the system with the bestsystem in the CoNLL-2005 Shared Task in Section3.
Since the outputs of the two systems are differ-ent, we carry out two types of evaluations: first byusing the traditional segment-based metric used inthe CoNLL-2005 Shared Task, and then by usingthe dependency-based metric from the CoNLL-2008Shared Task.
Both evaluations require a transforma-tion of the output of one system: For the segment-based metric, we have to convert the dependency-based output to segments; and for the dependency-based metric, a head-finding procedure is needed toselect heads in segments.
For the first time for a sys-tem using only dependency syntax, we report resultsfor PropBank-based semantic role labeling of En-glish that are close to the state of the art, and forsome measures even superior.2 Syntactic?Semantic DependencyAnalysisThe training corpus that we used is the dependency-annotated Penn Treebank from the 2008 CoNLLShared Task on joint syntactic?semantic analysis(Surdeanu et al, 2008).
Figure 1 shows a sentenceannotated in this framework.
The CoNLL task in-volved semantic analysis of predicates from Prop-Bank (for verbs, such as plan) and NomBank (fornouns, such as investment); in this paper, we reportthe performance on PropBank predicates only sincewe compare our system with previously publishedPropBank-based SRL systems.Chrysler plans new investment in Latin Americaplan.01LOC PMODNMODNMODOBJA0investment.01A1A0 A2SBJROOTFigure 1: An example sentence annotated with syntacticand semantic dependency structures.We model the problem of constructing a syntac-tic and a semantic graph as a task to be solvedjointly.
Intuitively, syntax and semantics are highlyinterdependent and semantic interpretation shouldhelp syntactic disambiguation, and joint syntactic?semantic analysis has a long tradition in deep-linguistic formalisms.
Using a discriminative model,we thus formulate the problem of finding a syntactictree y?syn and a semantic graph y?sem for a sentencex as maximizing a function Fjoint that scores thecomplete syntactic?semantic structure:?y?syn, y?sem?
= arg maxysyn,ysemFjoint(x, ysyn, ysem)The dependencies in the feature representation usedto compute Fjoint determine the tractability of the70rerankingPred?argLinguisticconstraintsSensedisambig.
Argumentidentification ArgumentlabelingdependencySyntacticparsingGlobal semantic modelSyntactic?semanticrerankingSemantic pipelineFigure 2: The architecture of the syntactic?semantic analyzer.search procedure needed to perform the maximiza-tion.
To be able to use complex syntactic featuressuch as paths when predicting semantic structures,exact search is clearly intractable.
This is true evenwith simpler feature representations ?
the problemis a special case of multi-headed dependency analy-sis, which is NP-hard even if the number of heads isbounded (Chickering et al, 1994).This means that we must resort to a simplifica-tion such as an incremental method or a rerank-ing approach.
We chose the latter option and thuscreated syntactic and semantic submodels.
Thejoint syntactic?semantic prediction is selected froma small list of candidates generated by the respectivesubsystems.
Figure 2 shows the architecture.2.1 Syntactic SubmodelWe model the process of syntactic parsing of asentence x as finding the parse tree y?syn =argmaxysyn Fsyn(x, ysyn) that maximizes a scoringfunction Fsyn.
The learning problem consists of fit-ting this function so that the cost of the predictions isas low as possible according to a cost function ?syn.In this work, we consider linear scoring functions ofthe following form:Fsyn(x, ysyn) = ?syn(x, ysyn) ?wwhere ?syn(x, ysyn) is a numeric feature represen-tation of the pair (x, ysyn) andw a vector of featureweights.
We defined the syntactic cost ?syn as thesum of link costs, where the link cost was 0 for acorrect dependency link with a correct label, 0.5 fora correct link with an incorrect label, and 1 for anincorrect link.A widely used discriminative framework for fit-ting the weight vector is the max-margin model(Taskar et al, 2003), which is a generalization ofthe well-known support vector machines to gen-eral cost-based prediction problems.
Since the largenumber of training examples and features in ourcase make an exact solution of the max-margin op-timization problem impractical, we used the on-line passive?aggressive algorithm (Crammer et al,2006), which approximates the optimization processin two ways:?
The weight vector w is updated incrementally,one example at a time.?
For each example, only the most violated con-straint is considered.The algorithm is a margin-based variant of the per-ceptron (preliminary experiments show that it out-performs the ordinary perceptron on this task).
Al-gorithm 1 shows pseudocode for the algorithm.Algorithm 1 The Online PA Algorithminput Training set T = {(xt, yt)}Tt=1Number of iterations NRegularization parameter CInitialize w to zerosrepeat N timesfor (xt, yt) in Tlet y?t = argmaxy F (xt, y) + ?
(yt, y)let ?t = min(C, F (xt,y?t)?F (xt,yt)+?(yt,y?t)??(x,yt)??
(x,y?t)?2)w ?
w + ?t(?
(x, yt)??
(x, y?t))return waverageWe used a C value of 0.01, and the number ofiterations was 6.2.1.1 Features and SearchThe feature function?syn is a factored represen-tation, meaning that we compute the score of thecomplete parse tree by summing the scores of itsparts, referred to as factors:?
(x, y) ?w =?f?y?
(x, f) ?w71We used a second-order factorization (McDonaldand Pereira, 2006; Carreras, 2007), meaning thatthe factors are subtrees consisting of four links: thegovernor?dependent link, its sibling link, and theleftmost and rightmost dependent links of the depen-dent.This factorization allows us to express useful fea-tures, but also forces us to adopt the expensivesearch procedure by Carreras (2007), which ex-tends Eisner?s span-based dynamic programming al-gorithm (1996) to allow second-order feature depen-dencies.
This algorithm has a time complexity ofO(n4), where n is the number of words in the sen-tence.
The search was constrained to disallow mul-tiple root links.To evaluate the argmax in Algorithm 1 duringtraining, we need to handle the cost function ?syn inaddition to the factor scores.
Since the cost function?syn is based on the cost of single links, this caneasily be integrated into the factor-based search.2.1.2 Handling Nonprojective LinksAlthough only 0.4% of the links in the trainingset are nonprojective, 7.6% of the sentences con-tain at least one nonprojective link.
Many of theselinks represent long-range dependencies ?
such aswh-movement ?
that are valuable for semantic pro-cessing.
Nonprojectivity cannot be handled byspan-based dynamic programming algorithms.
Forparsers that consider features of single links only, theChu-Liu/Edmonds algorithm can be used instead.However, this algorithm cannot be generalized to thesecond-order setting ?McDonald and Pereira (2006)proved that this problem is NP-hard, and describedan approximate greedy search algorithm.To simplify implementation, we instead opted forthe pseudo-projective approach (Nivre and Nilsson,2005), in which nonprojective links are lifted up-wards in the tree to achieve projectivity, and spe-cial trace labels are used to enable recovery of thenonprojective links at parse time.
The use of tracelabels in the pseudo-projective transformation leadsto a proliferation of edge label types: from 69 to 234in the training set, many of which occur only once.Since the running time of our parser depends on thenumber of labels, we used only the 20 most frequenttrace labels.2.2 Semantic SubmodelOur semantic model consists of three parts:?
A SRL classifier pipeline that generates a list ofcandidate predicate?argument structures.?
A constraint system that filters the candidatelist to enforce linguistic restrictions on theglobal configuration of arguments.?
A global reranker that assigns scores topredicate?argument structures in the filteredcandidate list.Rather than training the models on gold-standardsyntactic input, we created an automatically parsedtraining set by 5-fold cross-validation.
Trainingon automatic syntax makes the semantic classifiersmore resilient to parsing errors, in particular adjunctlabeling errors.2.2.1 SRL PipelineThe SRL pipeline consists of classifiers for pred-icate disambiguation, argument identification, andargument labeling.
For the predicate disambigua-tion classifiers, we trained one subclassifier for eachlemma.
All classifiers in the pipeline were L2-regularized linear logistic regression classifiers, im-plemented using the efficient LIBLINEAR package(Lin et al, 2008).
For multiclass problems, we usedthe one-vs-all binarization method, which makes iteasy to prevent outputs not allowed by the PropBankframe.Since our classifiers were logistic, their outputvalues could be meaningfully interpreted as prob-abilities.
This allowed us to combine the scoresfrom subclassifiers into a score for the completepredicate?argument structure.
To generate the can-didate lists used by the global SRL models, we ap-plied beam search based on these scores using abeam width of 4.The argument identification classifier was pre-ceded by a pruning step similar to the constituent-based pruning by Xue and Palmer (2004).The features used by the classifiers are listed inTable 1, and are described in Appendix A.
We se-lected the feature sets by greedy forward subset se-lection.72Feature PredDis ArgId ArgLabPREDWORD ?PREDLEMMA ?PREDPARENTWORD/POS ?CHILDDEPSET ?
?
?CHILDWORDSET ?CHILDWORDDEPSET ?CHILDPOSSET ?CHILDPOSDEPSET ?DEPSUBCAT ?PREDRELTOPARENT ?PREDPARENTWORD/POS ?PREDLEMMASENSE ?
?VOICE ?
?POSITION ?
?ARGWORD/POS ?
?LEFTWORD/POS ?RIGHTWORD/POS ?
?LEFTSIBLINGWORD/POS ?PREDPOS ?
?RELPATH ?
?VERBCHAINHASSUBJ ?
?CONTROLLERHASOBJ ?PREDRELTOPARENT ?
?FUNCTION ?Table 1: Classifier features in predicate disambiguation(PredDis), argument identification (ArgId), and argumentlabeling (ArgLab).2.2.2 Linguistically Motivated GlobalConstraintsThe following three global constraints were usedto filter the candidates generated by the pipeline.CORE ARGUMENT CONSISTENCY.
Core argu-ment labels must not appear more than once.DISCONTINUITY CONSISTENCY.
If there is a la-bel C-X, it must be preceded by a label X.REFERENCE CONSISTENCY.
If there is a label R-X and the label is inside an attributive relativeclause, it must be preceded by a label X.2.2.3 Predicate?Argument RerankerToutanova et al (2005) have showed that a globalmodel that scores the complete predicate?argumentstructure can lead to substantial performance gains.We therefore created a global SRL classifier usingthe following global features in addition to the fea-tures from the pipeline:CORE ARGUMENT LABEL SEQUENCE.
The com-plete sequence of core argument labels.
Thesequence also includes the predicate and voice,for instance A0+break.01/Active+A1.MISSING CORE ARGUMENT LABELS.
The set ofcore argument labels declared in the PropBankframe that are not present in the predicate?argument structure.Similarly to the syntactic submodel, we trainedthe global SRL model using the online passive?aggressive algorithm.
The cost function ?
wasdefined as the number of incorrect links in thepredicate?argument structure.
The number of iter-ations was 20 and the regularization parameter Cwas 0.01.
Interestingly, we noted that the globalSRL model outperformed the pipeline even whenno global features were added.
This shows that theglobal learning model can correct label bias prob-lems introduced by the pipeline architecture.2.3 Syntactic?Semantic RerankingAs described previously, we carried out rerankingon the candidate set of complete syntactic?semanticstructures.
To do this, we used the top 16 trees fromthe syntactic module and applied a linear model:Fjoint(x, ysyn, ysem) = ?joint(x, ysyn, ysem) ?wOur baseline joint feature representation?joint con-tained only three features: the log probability of thesyntactic tree and the log probability of the seman-tic structure according to the pipeline and the globalmodel, respectively.
This model was trained on thecomplete training set using cross-validation.
Theprobabilities were obtained using the multinomiallogistic function (?softmax?
).We carried out an initial experiment with a morecomplex joint feature representation, but failed toimprove over the baseline.
Time prevented us fromexploring this direction conclusively.3 Comparisons with Previous ResultsTo compare our results with previously publishedresults in SRL, we carried out an experiment com-paring our system to the top system (Punyakanok etal., 2008) in the CoNLL-2005 Shared Task.
How-ever, comparison is nontrivial since the output ofthe CoNLL-2005 systems was a set of labeled seg-ments, while the CoNLL-2008 systems (includingours) produced labeled semantic dependency links.To have a fair comparison of our link-based sys-tem against previous segment-based systems, we73carried out a two-way evaluation: In the first eval-uation, the dependency-based output was convertedto segments and evaluated using the segment scorerfrom CoNLL-2005, and in the second evaluation, weapplied a head-finding procedure to the output of asegment-based system and scored the result usingthe link-based CoNLL-2008 scorer.It can be discussed which of the two metrics ismost correlated with application performance.
Thetraditional metric used in the CoNLL-2005 tasktreats SRL as a bracketing problem, meaning thatthe entities scored by the evaluation procedure arelabeled snippets of text; however, it is questionablewhether this is the proper way to evaluate a taskwhose purpose is to find semantic relations betweenlogical entities.
We believe that the same criticismsthat have been leveled at the PARSEVAL metricfor constituent structures are equally valid for thebracket-based evaluation of SRL systems.
The in-appropriateness of the traditional metric has led toa number of alternative metrics (Litkowski, 2004;Baker et al, 2007; Surdeanu et al, 2008).3.1 Segment-based EvaluationTo be able to score the output of a dependency-basedSRL system using the segment scorer, a conversionstep is needed.
Algorithm 2 shows how a set of seg-ments is constructed from an argument dependencynode.
For each argument node, the algorithm com-putes the yield Y of the argument node, i.e.
the set ofdependency nodes to include in the bracketing.
Thisset is then partitioned into contiguous parts, fromwhich the segments are computed.
In most cases,the yield is just the subtree dominated by the argu-ment node.
However, if the argument dominates thepredicate, then the branch containing the predicateis removed.Table 2 shows the performance figures of oursystem on the WSJ and Brown corpora: preci-sion, recall, F1-measure, and complete propositionaccuracy (PP).
These figures are compared to thebest-performing system in the CoNLL-2005 SharedTask (Punyakanok et al, 2008), referred to as Pun-yakanok in the table, and the best result currentlypublished (Surdeanu et al, 2007), referred to as Sur-deanu.
To validate the sanity of the segment cre-ation algorithm, the table also shows the result of ap-plying segment creation to gold-standard syntactic?Algorithm 2 Segment creation from an argumentdependency node.input Predicate node p, argument node aif a does not dominate pY ?
{n : a dominates n}elsec?
the child of a that dominates pY ?
{n : a dominates n} \ {n : c dominates n}end ifS ?
partition of Y into contiguous subsetsreturn {(min-index s,max-index s) : s ?
S}WSJ P R F1 PPOur system 82.22 77.72 79.90 57.24Punyakanok 82.28 76.78 79.44 53.79Surdeanu 87.47 74.67 80.56 51.66Gold standard 97.38 96.77 97.08 93.20Brown P R F1 PPOur system 68.79 61.87 65.15 32.34Punyakanok 73.38 62.93 67.75 32.34Surdeanu 81.75 61.32 70.08 34.33Gold standard 97.22 96.55 96.89 92.79WSJ+Brown P R F1 PPOur system 80.50 75.59 77.97 53.94Punyakanok 81.18 74.92 77.92 50.95Surdeanu 86.78 72.88 79.22 49.36Gold standard 97.36 96.75 97.05 93.15Table 2: Evaluation with unnormalized segments.semantic trees.
We see that the two conversion pro-cedures involved (constituent-to-dependency con-version by the CoNLL-2008 Shared Task organizers,and our dependency-to-segment conversion) worksatisfactorily although the process is not completelylossless.During inspection of the output, we noted thatmany errors arise from inconsistent punctuation at-tachment in PropBank/Treebank.
We therefore nor-malized the segments to exclude punctuation at thebeginning or end of a segment.
The results of thisevaluation is shown in Table 3.
This table does notinclude the Surdeanu system since we did not have74access to its output.WSJ P R F1 PPOur system 82.95 78.40 80.61 58.65Punyakanok 82.67 77.14 79.81 54.55Gold standard 97.85 97.24 97.54 94.34Brown P R F1 PPOur system 70.84 63.71 67.09 36.94Punyakanok 74.29 63.71 68.60 34.08Gold standard 97.46 96.78 97.12 93.41WSJ+Brown P R F1 PPOur system 81.39 76.44 78.84 55.77Punyakanok 81.63 75.34 78.36 51.84Gold standard 97.80 97.18 97.48 94.22Table 3: Evaluation with normalized segments.The results on the WSJ test set clearly showthat dependency-based SRL systems can rivalconstituent-based systems in terms of performance?
it clearly outperforms the Punyakanok system, andhas a higher recall and complete proposition accu-racy than the Surdeanu system.
We interpret the highrecall as a result of the dependency syntactic repre-sentation, which makes the parse tree paths simplerand thus the arguments easier to find.For the Brown test set, on the other hand, thedependency-based system suffers from a low pre-cision compared to the constituent-based systems.Our error analysis indicates that the domain changecaused problems with prepositional attachment forthe dependency parser ?
it is well-known that prepo-sitional attachment is a highly lexicalized problem,and thus sensitive to domain changes.
We believethat the reason why the constituent-based systemsare more robust in this respect is that they utilize acombination strategy, using inputs from two differ-ent full constituent parsers, a clause bracketer, anda chunker.
However, caution is needed when draw-ing conclusions from results on the Brown test set,which is only 7,585 words, compared to the 59,100words in the WSJ test set.3.2 Dependency-based EvaluationIt has previously been noted (Pradhan et al, 2005)that a segment-based evaluation may be unfavorableto a dependency-based system, and that an evalua-tion that scores argument heads may be more indica-tive of its true performance.
We thus carried out anevaluation using the evaluation script of the CoNLL-2008 Shared Task.
In this evaluation method, an ar-gument is counted as correctly identified if its headand label are correct.
Note that this is not equivalentto the segment-based metric: In a perfectly identi-fied segment, we may still pick out the wrong head,and if the head is correct, we may infer an incorrectsegment.
The evaluation script also scores predicatedisambiguation performance; we did not include thisscore since the 2005 systems did not output predi-cate sense identifiers.Since CoNLL-2005-style segments have no in-ternal tree structure, it is nontrivial to extract ahead.
It is conceivable that the output of the parsersused by the Punyakanok system could be used toextract heads, but this is not recommendable be-cause the Punyakanok system is an ensemble sys-tem and a segment does not always exactly matcha constituent in a parse tree.
Furthermore, theCoNLL-2008 constituent-to-dependency conversionmethod uses a richer structure than just the raw con-stituents: empty categories, grammatical functions,and named entities.
To recreate this additional infor-mation, we would have to apply automatic systemsand end up with unreliable results.Instead, we thus chose to find an upper boundon the performance of the segment-based system.We applied a simple head-finding procedure (Algo-rithm 3) to find a set of head nodes for each seg-ment.
Since the CoNLL-2005 output does not in-clude dependency information, the algorithm usesgold-standard dependencies and intersects segmentswith the gold-standard segments.
This will give usan upper bound, since if the segment contains thecorrect head, it will always be counted as correct.The algorithm looks for dependencies leaving thesegment, and if multiple outgoing edges are found,a couple of simple heuristics are applied.
We foundthat the best performance is achieved when selectingonly one outgoing edge.
?Small clauses,?
which aresplit into an object and a predicative complement inthe dependency framework, are the only cases wherewe select two heads.Table 4 shows the results of the dependency-based evaluation.
In the table, the output of the75Algorithm 3 Finding head nodes in a segment.input Argument segment aif a overlaps with a segment in the gold standarda?
intersection of a and gold standardF ?
{n : governor of n outside a}if |F | = 1return Fremove punctuation nodes from Fif |F | = 1return Fif F = {n1, n2, .
.
.}
where n1 is an object and n2 isthe predicative part of a small clausereturn {n1, n2}if F contains a node n that is a subject or an objectreturn {n}elsereturn {n}, where n is the leftmost node in Fdependency-based system is compared to the seman-tic dependency links automatically extracted fromthe segments of the Punyakanok system.WSJ P R F1 PPOur system 88.46 83.55 85.93 61.97Punyakanok 87.25 81.59 84.32 58.17Brown P R F1 PPOur system 77.67 69.63 73.43 41.32Punyakanok 80.29 68.59 73.98 37.28WSJ+Brown P R F1 PPOur system 87.07 81.68 84.29 59.22Punyakanok 86.94 80.21 83.45 55.39Table 4: Dependency-based evaluation.In this evaluation, the dependency-based systemhas a higher F1-measure than the Punyakanok sys-tem on both test sets.
This suggests that the main ad-vantage of using a dependency-based semantic rolelabeler is that it is better at finding the heads ofsemantic arguments, rather than finding segments.The results are also interesting in comparison tothe multi-view system described by Pradhan et al(2005), which has a reported head F1 measure of85.2 on the WSJ test set.
The figure is not exactlycompatible with ours, however, since that systemused a different head extraction mechanism.4 ConclusionWe have described a dependency-based system1 forsemantic role labeling of English in the PropBankframework.
Our evaluations show that the perfor-mance of our system is close to the state of theart.
This holds regardless of whether a segment-based or a dependency-based metric is used.
In-terestingly, our system has a complete propositionaccuracy that surpasses other systems by nearly 3percentage points.
Our system is the first semanticrole labeler based only on syntactic dependency thatachieves a competitive performance.Evaluation and comparison is a difficult issuesince the natural output of a dependency-based sys-tem is a set of semantic links rather than segments,as is normally the case for traditional systems.
Tohandle this situation fairly to both types of systems,we carried out a two-way evaluation: conversion ofdependencies to segments for the dependency-basedsystem, and head-finding heuristics for segment-based systems.
However, the latter is difficult sinceno structure is available inside segments, and wehad to resort to computing upper-bound results usinggold-standard input; despite this, the dependency-based system clearly outperformed the upper boundof the performance of the segment-based system.The comparison can also be slightly misleadingsince the dependency-based system was optimizedfor the dependency metric and previous systems forthe segment metric.Our evaluations suggest that the dependency-based SRL system is biased to finding argumentheads, rather than argument text snippets, and thisis of course perfectly logical.
Whether this is an ad-vantage or a drawback will depend on the applica-tion ?
for instance, a template-filling system mightneed complete segments, while an SRL-based vectorspace representation for text categorization, or a rea-soning application, might prefer using heads only.In the future, we would like to further investigatewhether syntactic and semantic analysis could be in-tegrated more tightly.
In this work, we used a sim-1Our system is freely available for download athttp://nlp.cs.lth.se/lth_srl.76plistic loose coupling by means of reranking a smallset of complete structures.
The same criticisms thatare often leveled at reranking-based models clearlyapply here too: The set of tentative analyses from thesubmodules is too small, and the correct analysis isoften pruned too early.
An example of a method tomitigate this shortcoming is the forest reranking byHuang (2008), in which complex features are evalu-ated as early as possible.A Classifier FeaturesFeatures Used in Predicate DisambiguationPREDWORD, PREDLEMMA.
The lexical form andlemma of the predicate.PREDPARENTWORD and PREDPARENTPOS.Form and part-of-speech tag of the parent nodeof the predicate.CHILDDEPSET, CHILDWORDSET, CHILD-WORDDEPSET, CHILDPOSSET, CHILD-POSDEPSET.
These features represent the setof dependents of the predicate using combina-tions of dependency labels, words, and parts ofspeech.DEPSUBCAT.
Subcategorization frame: the con-catenation of the dependency labels of the pred-icate dependents.PREDRELTOPARENT.
Dependency relation be-tween the predicate and its parent.Features Used in Argument Identification andLabelingPREDLEMMASENSE.
The lemma and sense num-ber of the predicate, e.g.
give.01.VOICE.
For verbs, this feature is Active or Passive.For nouns, it is not defined.POSITION.
Position of the argument with respectto the predicate: Before, After, or On.ARGWORD and ARGPOS.
Lexical form and part-of-speech tag of the argument node.LEFTWORD, LEFTPOS, RIGHTWORD, RIGHT-POS.
Form/part-of-speech tag of the left-most/rightmost dependent of the argument.LEFTSIBLINGWORD, LEFTSIBLINGPOS.Form/part-of-speech tag of the left sibling ofthe argument.PREDPOS.
Part-of-speech tag of the predicate.RELPATH.
A representation of the complex gram-matical relation between the predicate and theargument.
It consists of the sequence of de-pendency relation labels and link directions inthe path between predicate and argument, e.g.IM?OPRD?OBJ?.VERBCHAINHASSUBJ.
Binary feature that is setto true if the predicate verb chain has a subject.The purpose of this feature is to resolve verbcoordination ambiguity as in Figure 3.CONTROLLERHASOBJ.
Binary feature that is trueif the link between the predicate verb chain andits parent is OPRD, and the parent has an ob-ject.
This feature is meant to resolve controlambiguity as in Figure 4.FUNCTION.
The grammatical function of the argu-ment node.
For direct dependents of the predi-cate, this is identical to the RELPATH.ISBJeat drinkyouandCOORD SBJCONJROOTSBJ COORDROOTdrinkandeatICONJFigure 3: Coordination ambiguity: The subject I is in anambiguous position with respect to drink.I toIMSBJwant sleephimOBJOPRDROOTIMsleepISBJwantROOTtoOPRDFigure 4: Subject/object control ambiguity: I is in an am-biguous position with respect to sleep.77ReferencesCollin Baker, Michael Ellsworth, and Katrin Erk.
2007.SemEval task 19: Frame semantic structure extraction.In Proceedings of SemEval-2007.Xavier Carreras and Llu?s M?rquez.
2004.
Introductionto the CoNLL-2004 shared task: Semantic role label-ing.
In Proceedings of CoNLL-2004.Xavier Carreras and Llu?s M?rquez.
2005.
Introductionto the CoNLL-2005 shared task: Semantic role label-ing.
In Proceedings of CoNLL-2005.Xavier Carreras.
2007.
Experiments with a higher-order projective dependency parser.
In Proceedings ofCoNLL-2007.David M. Chickering, Dan Geiger, and David Hecker-man.
1994.
Learning Bayesian networks: The com-bination of knowledge and statistical data.
TechnicalReport MSR-TR-94-09, Microsoft Research.Ronan Collobert and Jason Weston.
2007.
Fast semanticextraction using a novel neural network architecture.In Proceedings of ACL-2007.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Schwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
JMLR, 2006(7):551?585.Jason M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proceed-ings of ICCL.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Daniel Gildea and Martha Palmer.
2002.
The necessityof syntactic parsing for predicate argument recogni-tion.
In Proceedings of the ACL-2002.Aria Haghighi, Andrew Y. Ng, and Christopher D. Man-ning.
2005.
Robust textual inference via graph match-ing.
In Proceedings of EMNLP-2005.Andrew Hickl, Jeremy Bensley, John Williams, KirkRoberts, Bryan Rink, and Ying Shi.
2006.
Recogniz-ing textual entailment with LCC?s GROUNDHOG sys-tems.
In Proceedings of the Second PASCAL Recog-nizing Textual Entailment Challenge.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL-2008.Richard Johansson and Pierre Nugues.
2008a.Dependency-based syntactic?semantic analysis withPropBank and NomBank.
In Proceedings of theShared Task Session of CoNLL-2008.Richard Johansson and Pierre Nugues.
2008b.
The effectof syntactic representation on semantic role labeling.In Proceedings of COLING-2008.Chih-Jen Lin, Ruby C. Weng, and S. Sathiya Keerthi.2008.
Trust region Newton method for large-scale lo-gistic regression.
JMLR, 2008(9):627?650.Ken Litkowski.
2004.
Senseval-3 task: Automatic label-ing of semantic roles.
In Proceedings of Senseval-3.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In Proceedings of EACL-2006.Alessandro Moschitti, Paul Mora?rescu, and SandaHarabagiu.
2003.
Open domain information extrac-tion via automatic semantic labeling.
In Proceedingsof FLAIRS.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projectivedependency parsing.
In Proceedings of ACL-2005.Jacob Persson, Richard Johansson, and Pierre Nugues.2008.
Text categorization using predicate?argumentstructures.
Submitted.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Daniel Jurafsky.
2005.
Semantic role la-beling using different syntactic views.
In Proceedingsof ACL-2005.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics,34(2):257?287.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using predicate-argument struc-tures for information extraction.
In Proceedings ofACL-2003.Mihai Surdeanu, Llu?s M?rquez, Xavier Carreras, andPere R. Comas.
2007.
Combination strategies for se-mantic role labeling.
Journal of Artificial IntelligenceResearch, 29:105?151.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?s M?rquez, and Joakim Nivre.
2008.
TheCoNLL?2008 shared task on joint parsing of syntac-tic and semantic dependencies.
In Proceedings ofCoNLL?2008.Reid Swanson and Andrew S. Gordon.
2006.
A compari-son of alternative parse tree paths for labeling semanticroles.
In Proceedings of COLING/ACL-2006.Ben Taskar, Carlos Guestrin, and Daphne Koller.
2003.Max-margin Markov networks.
In Proceedings ofNIPS-2003.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2005.
Joint learning improves semantic rolelabeling.
In Proceedings of ACL-2005.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Proceedings ofEMNLP-2004.78
