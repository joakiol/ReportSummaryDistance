Group ing  Words  Us ing  Stat is t ica l  ContextChristopher C. Huckle *Department ofPsychology,7 George Square,Edinburgh EH8 9JZ,Scotland,U.K.cch@cast le ,  ed.
ac.
ukAbstractThis paper describes the use of statisticalanalyses of untagged corpora to detectsimilarities and differences in the mean-ing of words in text.
This work is mo-tivated by psychological as well as bycomputational issues.
The limitationsof the method of cluster analysis in as-sessing the success of such analyses arediscussed, and ongoing research using analternative unsupervised neural networkapproach is described.IntroductionThere has been considerable recent interest in theuse of statistical methods for grouping words inlarge on-line corpora into categories which capturesome of our intuitions about the reference of thewords we use and the relationships between them(e.g.
Brown et al, 1992; Schiitze, 1993).Although they have received most attentionfrom within computational linguistics, such ap-proaches are also of interest from the point of viewof psychology.
The huge task of developing con-cepts of word meanings is one that human beingsreadily achieve; we are all generally aware of thesimilarities and differences between the meaningsof words, despite the fact that in many cases thesemeanings are not amenable to rigourous defini-tion.
Whilst supervision may enable children tolearn the meanings of a limited number of com-mon words, it seems extremely unlikely that thegreater part of our understanding of word mean-ings is achieved in this way.
Experimental evi-dence shows (Harris, 1992) that the occurrence ofwords in young children's language is strongly in-fluenced by the appearance of those words in thespeech they hear around them, and it may be thatthis process continues indefinitely.
Such a processwould seem to be particularly important when ac-counting for our understanding of abstract words,such as 'similar' and 'justice', which lack concrete*The author is supported by the Carnegie Trust forthe Universities of Scotlandreferents.
Despite our difficulty in being able toprovide clear definitions for such words, we havestrong intuitions about their usage and can read-ily categorize them on the basis of similarity inmeaning.
This process of developing concepts forabstract words is one which psychological researchhas tended to ignore.This situation suggests that the learning of themeanings of many words, and their relation to themeanings of other words, may be achieved in anunsupervised fashion, and that our ability to de-velop a categorization for words may be driven, atleast in part, by structure latent in the languagebeing learned.
Recent work in computational lin-guistics which makes use of statisticM methods tocluster words into groups which reflect heir mean-ing is attractive in this context as it potentiallyprovides a means for developing conceptual struc-ture without supervision, without giving any priorinformation about the language to the system, andwithout making a priori distinctions between con-crete and abstract words.Supervision and knowledge of syntax (muchuseful information about which, as Finch andChater (1992) have argued, is also contained insimple distributional statistics) arc two additionalfactors which are likely to assist in the process ofdeveloping concepts of word meanings.
However,by focusing on the single; intralinguistic, source ofinformation provided by the language data alone,we may be able to obtain useful insights regardingits influence on our conceptual structure.Approaches to Semantic ClusteringA number of analyses were carried out on textcorpora to examine the sorts of semantic group-ings that can be achieved using simple statisticalmethods.
Using an approach similar to that ofBrown et al (1992), each 'target word '1 wi inthe corpus was represented as a vector in whicheach component j is the probability that any one1 For convenience, target words were taken as the nmost frequent words in the corpus, with n often equalto 1000278word position in a 'context window' will be occu-pied by a 'context word' wj, given that the win-dow is centred on word wi.
The length of thewindow used can be varied.
The basic outline ofthe moving window used is shown in figure 1.
Asfigure 1 indicates, the portion of the moving win-dow in which the context words are contained mayexclude a small number of word positions imme-diately adjacent o the target word.
This is toweaken the effects of syntax, although the analy-ses described here do not make use of this facility.Following the creation of these vectors, heirarchi-Figure 1: Design of the Moving Windowword or immediately following the target word.Whilst it seems reasonable to suppose that chil-dren acquiring word meanings would be able tomake use of more than this limited amount of con-text information, the analyses were carried out toinvestigate performance of the system under suchcrude conditions.It was found on examination of the dendro-grams resulting from the cluster analyses thateven using this extremely impoverished source ofinformation about the target words did permit alimited number of semantically coherent group-ings of words to be created.
The members of someof these groups were selected following inspectionof the relevant dendrograms and are listed in table1.
Despite the existence of the groupings hownC.~lte~ Wolds ~ ~ WC~SD/revtion of Moving Window Through Textcal cluster analysis was carried out over them, us-ing Euclidean distance between vectors as a sim-ilarity metric.
Analyses were also carried out inwhich, as with Finch and Chater (1992), the dis-tance metric used was the Spearman Rank Cor-relation coefficient.
The approach described herediffers from that of Brown et al (1992) in thatcontext words both preceding and following thetarget word are considered (although informationabout the ordering of the context was not used),and in that Euclidean distance, rather than aver-age mutual information, is used for clustering.Each of the methods described here representseach target word in the same manner, regardless ofthe syntactic or semantic designation which mightconventionally be assigned to it.
Thus any differ-ences or similarities between words must be de-tected purely from the statistics of the usage ofthe words, which are in turn determined by thecharacteristics of the contexts in which they oc-cur.ResultsThe methods outlined above were used to clus-ter words appearing in the Lund corpus (470,000words), a corpus created from issues of the WallStreet Journal (1.1 million words), and a corpuscreated from the works of Anthony Trollope (1.7million words).Initial analyses were carried out on the Lundand Trollope corpora using a short window lengthof only one word position either side of the targetword.
That is, target words were represented byvectors whose components reflected the (bigram)statistics of occurrence of context words at theword position immediately preceding the targetTable 1: Semantic GroupingsPossible Designatiouc~ GroupMental StatesO~ur~ Corpus)Days c/the Week(Lurid Cori~s)Measures(Lurid CoraLs)PeopleNumbersmono W c~q~)Unite of Time "(Trollope Cor/~s)Paris c~ the body(Tronope Corpus)Humen'Fan'~LlyMembers(Trollope Corpus)i Group Memberswant, wanted, tried, went, derided, think, thousht,hope, believe, knew, feel, felt, expect, wish, forget.Eriday, thursday, saturday, sunday, monday,wednesday, tuesday.ninety, pounds, years, days, minutes, hours, double,~es.boy, girl, man, woman.six, twelve, twice, twenty, two, three, fottr, ten, five, seven.mcxtths, Tears, days, hours, o'ckx:k, times.arm, mouth, pocket, arms, chair, sister, thoughts, feet,eye, heart, father, face, head, eye~ hand, eats, hands,bosom.aunt, mind, uncle, husband, cousin, motheT, daughter,brother, niec~in ~able 1 and a small number of others like them,they represent only a small proportion of the 1000target words subjected to the analysis.
Besidesthose shown above, a number of other types ofgroupings were evident which appeared to reflectsyntactic rather than more specific semantic har-acteristics.
This is perhaps not surprising if oneregards the problem of grouping words on the ba-sis of similarity as one of prediction; given statis-tical information only about those words immedi-ately adjacent o a particular target word, it maybe possible to say with reasonable confidence thatthe target word is a noun, a verb, or an adjective,but information about wider context is likely tobe needed in order to provide more specific predic-tions about the particular noun, verb, or adjectivein question.
Since this information is not present,the dendrograms resulting from the analysis howgroupings of prepositions, adjectives, verbs, andso on.
Also present are groups of words whosemembers all commonly precede or follow a partic-ular particle.Further analyses were carried out in which thelength of the context window was extended to 5279words either side of the target word.
The den-drograms resulting from these analyses did notshow any marked improvement over those ob-tained from the earlier analyses, and even whenthe window length was increased to 25 words eachside of the target word, clear differences were noteasy to detect from the dendrograms, although thesorts of groupings noted earlier were still identifi-able.Future  D i rec t ionsThe use of cluster analysis and related techniqueshas been popular for presenting the results of re-cent statistical anguage work within computa-tional linguistics.
However, such methods clearlyhave a number of limitations.
Firstly, it is diffi-cult to compare dendrograms rigourously, whichmeans that it can be difficult to determine whichof a number of alternative approaches or sets ofparameters i  turning out to be the most success-ful.
Secondly, the lack of an objective measureof the clusters obtained means that assessmentsof the success of a particular technique for cat-egorizing language may well be unreliable; it isquite possible to focus on the .attractive lookinggroupings revealed in a dendrogram whilst ignor-ing what may be a very large number of less at-tractive ones.These criticisms arise largely because clusteranalysis is a purely descriptive statistical method,and strongly suggest that alternative methodsmust be found which can provide a more objec-tive measure of the success of the technique beingused.
Of these, word sense disambiguation is at-tractive.
Since we can obtain from native speakersan assessment ofthe correct senses of target wordsin different contexts, we do have a means for de-termining how often a particular technique is ableto give the correct sense for a particular targetword.
In other words, the evaluation of a nativespeaker can potentially be used to assess perfor-mance each time the system encounters a targetword in context and assigns that word to a par-ticular sense class.
Whilst such assessments mightalso be applicable to the analysis of dendrograms,word sense disambiguation is of interest since itconstitutes the task that continually meets humanlanguage users when reading text or listening tospeech.For these reasons, current work is focusing onthe problem of disambiguating words given sta-tistical context.
To achieve this, an unsupervisedcompetitive neural network is being used.
Thishas several features which appear to be desirable.Firstly, as in the human case, learning proceedson-line, without any need for a separate stage ofstatistical analysis.
Such a system has the poten-tial to begin developing clusters from the very firstexposure to the linguistic input, and the clustersinto which the input words are placed evolve con-tinuously during the learning process.
Thus onecan usefully examine the state of the clusters atany point during learning.
Secondly, it is straight-forward to allow any given word to be clusteredinto as many separate clusters as the system dic-tates (subject o the maximum number of outputunits available).
Thus, the neural network ap-proach, unlike that described above, has the po-tential to allow separate senses of a word to bedistinguished on the basis of their context.
Thisis not to say that non-neural network approachescould not permit a word to belong to more thanone cluster (e.g.
Pereira et al, 1993), but ratherthat this is a very natural and attractive conse-quence of trsing the unsupervised neural networkapproach.At present, work is being undertaken to exam-ine how well a simple competitive neural networkcan perform on such a task.
Preliminary workhas been undertaken using a simple competitiveneural network similar to that described by Finchand Chater (1992).
Unlike them, though, provi-sion was made for presenting words along withcontext during the test phase as well as the train-ing phase.
This potentially allows disambigua-tion performance to be examined at any time.Initial work using the very simple artificial cor-pus devised by Elman (1988) has been encourag-ing, with the network demonstrating near-perfectperformance in distinguishing between ouns andverbs in the corpus.Re ferencesPeter F. Brown, Vincent J. Della Pietra, Peter V.deSouza, Jenifer C. Lai, and Robert L. Mercer.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467-479.Jeffrey L. Elman.
1988.
Finding Structure inTime.
CRL Technical Report 8801.
Center forResearch in Language, University of California,San Diego.Steven P. Finch and Nicholas 3.
Chater.
1992.Bootstrapping syntactic categories.
In Proceed-ings of the l~th Annual Conference of the Cogni-tive Science Society Of America, pages 820-825.Bloomington, Indiana.Margaret Harris.
1992.
Language Experienceand Early Language Development.
Lawrence Erl-baum, Hove, U.K.Fernando Pereira, Naftali Tishby, and Lilian Lee.1993.
Distributional Clustering of English Words.In Proceedings of the Association for Compu-tational Linguistics, volume 31, pages 183-190.ACL.Hinrich Schfitze.
1993.
Part-of-speech-inductionfrom scratch.
In Proceedings of the Associationfor Computational Linguistics, volume 31, pages251-258.
ACL.280
