Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 425?429, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSwatCS: Combining simple classifiers with estimated accuracySam Clark and Richard WicentowskiDepartment of Computer ScienceSwarthmore CollegeSwarthmore, PA 19081 USAsclark2@sccs.swarthmore.edu and richardw@cs.swarthmore.eduAbstractThis paper is an overview of the SwatCSsystem submitted to SemEval-2013 Task 2A:Contextual Polarity Disambiguation.
The sen-timent of individual phrases within a tweetare labeled using a combination of classifierstrained on a range of lexical features.
Theclassifiers are combined by estimating the ac-curacy of the classifiers on each tweet.
Perfor-mance is measured when using only the pro-vided training data, and separately when in-cluding external data.1 IntroductionSpurred on by the wide-spread use of the social net-works to communicate with friends, fans and cus-tomers around the globe, Twitter has been adoptedby celebrities, athletes, politicians, and major com-panies as a platform that mitigates the interaction be-tween individuals.Analysis of this Twitter data can provide insightsinto how users express themselves.
For example,many new forms of expression and language fea-tures have emerged on Twitter, including expres-sions containing mentions, hashtags, emoticons, andabbreviations.
This research leverages the lexicalfeatures in tweets to predict whether a phrase withina tweet conveys a positive or negative sentiment.2 Related WorkA common goal of past research has been to discoverand extract features from tweets that accurately in-dicate sentiment (Liu, 2010).
The importance offeature selection and machine learning in sentimentanalysis has been explored prior to the rise of so-cial networks.
For example, Pang and Lee (2004)apply machine learning techniques to extracted fea-tures from movie reviews.More recent feature-based systems include alexicon-based approach (Taboada et al 2011), anda more focused study on the importance of both ad-verbs and adjectives in determining sentiment (Be-namara et al 2007).
Other examples include us-ing looser descriptions of sentiment rather than rigidpositive/negative labelings (Whitelaw et al 2005)and investigating how connections between userscan be used to predict sentiment (Tan et al 2011).This task differs from past work in sentiment anal-ysis of tweets because we aim to build a model capa-ble of predicting the sentiment of sub-phrases withinthe tweet rather than considering the entire tweet.Specifically, ?given a message containing a markedinstance of a word or a phrase, determine whetherthat instance is positive, negative or neutral in thatcontext?
(Wilson et al 2013).
Research on context-oriented polarity predates the emergence of socialnetworks: (Nasukawa and Yi, 2003) predict senti-ment of subsections in a larger document.N-gram features, part of speech features and?micro-blogging features?
have been used as accu-rate indicators of polarity (Kouloumpis et al 2011).The ?micro-blogging features?
are of particular in-terest as they provide insight into how users haveadapted Twitter tokens to natural language to por-tray sentiment.
These features include hashtags andemoticons (Kouloumpis et al 2011).4253 DataThe task organizers provided a manually-labeled setof tweets.
For parts of this study, their data was sup-plemented with external data (Go et al 2009).As part of pre-processing, all tweets werepart-of-speech tagged using the ARK TweetNLPtools (Owoputi et al 2013).
All punctuation wasstripped, except for #hashtags, @mentions,emoticons :), and exclamation marks.
All hyper-links were replaced with a common string, ?URL?.3.1 Common DataThe provided training data was a collection of ap-proximately 15K tweets, manually labeled for senti-ment (positive, negative, neutral, or objective) (Wil-son et al 2013).
These sentiment labels appliedto a specific phrase within the tweet and did notnecessarily match the sentiment of the entire tweet.Each tweet had at least one labeled phrase, thoughsome tweets had multiple phrases labeled individu-ally.
Overall, 37% of tweets had one labeled phrase,with an average of 2.58 labeled phrases per tweet.Each of our classifiers were binary classifiers, la-beling phrases as either positive or negative.
Assuch, approximately 10.5K phrases labeled as objec-tive or neutral were pruned from the training data,resulting in a final training set containing 5362 la-beled phrases, 3445 positive and 1917 negative.The test data consisted of tweets and SMS mes-sages, although the training data contained onlytweets.
The test set for the phrase-level task (Task A)contained 4435 tweets and 2334 SMS messages.3.2 Outside DataTask organizers allowed two submissions, a con-strained submission using only the provided trainingdata, and an unconstrained submission allowing theuse of external data.
For the unconstrained submis-sion, we used a data set built by Go et al(2009).
Thedata set was automatically labeled using emoticonsto predict sentiment.
We used a 50K tweet subsetcontaining 25K positive and 25K negative tweets.3.3 Phrase IsolationFor tweets containing a single labeled phrase, we usethe entire tweet as the context for the phrase.
Fortweets containing two labeled phrases, we use theunigram label bigram labelhappy pos not going neggood pos looking forward posgreat pos happy birthday poslove pos last episode negbest pos i?m mad negTable 1: The 5 most influential unigram and bigramsranked by information gain.context from the start of the tweet to the end of thefirst phrase as the context for the first phrase, and thecontext from the start of the second phrase to the endof the tweet for the second phrase.
If more than twophrases are present, the context for any phrase in themiddle of the tweet is limited to only the words inthe labeled phrase.4 ClassifiersThe system uses a combination of naive Bayes clas-sifiers to label the input.
Each classifier is trained ona single feature extracted from the tweet.
The classi-fiers are combined using a confidence-weighted vot-ing scheme.
The system applies a simple negationscheme to all of the language features used by theclassifiers.
Any word following a negation term inthe phrase has the substring ?NOT?
prefixed to it.This negation scheme was applied to n-gram fea-tures and lexicon features.4.1 N-gram FeaturesRather than use all of the n-grams as features, weranked each n-gram (w/POS tags) by calculating itschi-square-based information gain.
The top 2000n-grams (1000 positive, 1000 negative) are used asfeatures in the n-gram classifier.
Both a unigram andbigram classifier use these ranked (word/POS) fea-tures.
Table 1 shows the highest ranked unigramsand bigrams using this method.4.2 Sentiment Lexicon FeaturesA second classifier uses the MPQA subjectivity lex-icon (Wiebe et al 2005).
We extract both the po-larity and the polarity strength for each word/POSin the lexicon matching a word/POS in the phrase?scontext.
We refer to this classifier as the lexiconclassifier.4260.78 0.8 0.82 0.84 0.86 0.88 0.9 0.92 0.94 0.96 0.980  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9Confidence/classifier accuracy alpha = abs(P(pos) - P(neg))Alpha vs. Classifier accuracyFigure 1: Classifier accuracy increases as the differencebetween the probabilities of the labelings increases.4.3 Part of Speech and Special Token FeaturesThree additional classifiers were built using featuresextracted from the tweets.
Our third classifier usesonly the raw counts of specific part of speech tags:adjectives, adverbs, interjections, and emoticons.The fourth classifier uses the emoticons as a fea-ture.
To reduce the noise in the emoticon feature set,many (over 25) different emoticons are mapped tothe basic ?:)?
and ?:(?
expressions.
Some emoticonssuch as ?xD?
did not map to these basic expressions.A fifth classifier gives added weight to words withextraneous repeated letters.
Words containing twoor more repeated letters (that are not in a dictionary,e.g.
?heyyyyy?, ?sweeeet?)
are mapped to their pre-sumed correct spelling (e.g.
?hey?, ?sweet?
).5 Confidence-Based ClassificationTo combine all of the classifiers, the system esti-mates the confidence of each classifier and only ac-cepts the classification output if the confidence ishigher than a specified baseline.
To establish a clas-sifier?s confidence, we take the absolute value ofthe difference between a classifier?s positive outputprobability and negative output probability, whichwe call alpha.
Alpha values close to 1 indicate highconfidence in the predicted label; values close to 0indicate low confidence in the predicted label.5.1 Classifier VotingThe predicted accuracy of each classifier is deter-mined after the trained classifiers are evaluated us-ing a development set with known labels.
Using thedev set, we calculate the accuracy of each classi-rank classifier data polarity acc1 unigrams (C) positive 0.892 unigrams (U) positive 0.883 lexicon (C) negative 0.834 lexicon (U) negative 0.815 tagcount (C) positive 0.786 bigrams (C) positive 0.757 tagcount (U) novote <0.658 bigrams (U) novote <0.65Table 2: An example of the polarity and correspondingaccuracy output for each classifier for a single tweet.
Thelabels (C) and (U) indicate whether the classifier wastrained on constrained training data or on unconstraineddata (Go et al 2009).fier at alpha values between 0 and 1.
The result isa trained classifier with an approximation of overallclassification accuracy at a given alpha value.
Fig-ure 1 shows the relationship between alpha valueand overall classifier accuracy.
As expected, classi-fication accuracy increases as confidence increases.Table 2 shows the breakdown of classifier accu-racy for a single tweet using both provided and ex-ternal data.
The accuracy listed is the classifier-specific accuracy determined by the alpha value forthat phrase in the tweet.
Using a dev set, we ex-perimentally established the most effective baselineto be 0.65.
In the voting system described below,only classifiers with confidence above the baseline(per marked phrase) are used.
Therefore, the spe-cific combination of classifiers used for each phrasemay be different.An unlabeled phrase is assigned a polarity andconfidence value from each classifier.
These proba-bilities are combined using a voting system to deter-mine a single output.
This voting system calculatesthe final labeling by computing the average proba-bility for each label only for those classifiers withestimated accuracies above the baseline.
The labelwith the highest overall probability is selected.6 ResultsThe constrained submission only allowed for train-ing on the provided data and placed 17 out of 23entries.
The unconstrained submission was trainedon both the provided data and the external data andplaced 6 out of 8 entries.
Both submissions were427unigram label bigram label lexicon labelaint neg school tomorrow neg bad negexcited pos not going neg excited possucks neg didn?t get neg tired negsick neg might not neg dead negpoor neg gonna miss neg poor negsmh pos still haven?t neg happy postough pos breakout kings neg black neggreatest pos work tomorrow neg good posf*ck neg ray lewis pos hate negnets neg can?t wait pos sorry negTable 3: The most influential features from the unigram,bigram, and lexicon classifiers.evaluated using the Twitter and SMS data describedin Section 3.1.
As mentioned, our system used a bi-nary classifier, predicting only positive and negativelabels, making no neutral classifications.The constrained system evaluated on the Twittertest set had an F-measure of .672, with a high dis-parity between the F-measure for tweets labeled aspositive versus those labeled as negative (.79 vs .53).The unconstrained system on the Twitter test set un-derperformed our constrained system, with an F-measure of only .639.The constrained system on the SMS test setyielded an F-measure of .660; the unconstrained sys-tem on the same data yielded an F-measure of .679.6.1 Features ExtractedThe most important features extracted by the un-igram, bigram and lexicon classifiers are shownin Table 3.
Features such as ?ray lewis?, ?smh?,?school tomorrow?, ?work tomorrow?, ?breakoutkings?
and ?nets?
demonstrate that the classifiersformed a relationship between sentiment and collo-quial language.
An example of this understanding isassigning a strong negative sentiment to ?sucks?
(asthe verb ?to suck?
does not carry sentiment).
The bi-grams ?breakout kings?, ?ray lewis?
and ?nets?
areinteresting features because their sentiment is highlycultural: ?breakout kings?
is a popular TV show thatwas canceled, ?ray lewis?
a high profile player foran NFL team, and ?nets?
a reference to the strug-gling NBA basketball team.
Expressions such as?smh?
(a widely-used abbreviation for ?shaking myhead?)
show how detecting tweet- and SMS-specificlanguage is important to understanding sentiment inthis domain.7 DiscussionThis supervised system combines many featuresto classify positive and negative sentiment at thephrase-level.
Phrase-based isolation (Section 3.3)limits irrelevant context in the model.
By estimat-ing classifier confidence on a per-phrase basis, thesystem can prioritize confident classifiers and ignoreless-confident ones before combination.Similar results on the Twitter and SMS data setsindicates the similarity between the domains.
Theexternal data improved the system on the SMS dataand reduced system accuracy on the Twitter data.This difference in performance may be an indicationthat the supplemental data set was noisier than weexpected, or that it was more applicable to the SMSdomain (SMS) than we anticipated.There was a noticeable difference between pos-itive and negative classification accuracy for all ofthe submissions.
This difference is likely due to ei-ther a positive bias in training set used (the providedtraining data is 64% positive, 36% negative) or a se-lection of features that favored positive sentiment.7.1 Improvements and Future WorkUnfortunately, the time constraints of the evalua-tion exercise led to a programming bug that wasn?tcaught until after the submission deadline.
In pre-processing, we accidentally stripped most of theemoticon features out of the text.
While it is un-clear how much this would have effected our finalperformance, such features have been demonstratedas valuable in similar tasks.
After fixing this bugthe system performs better in both constrained andunconstrained situations (as evaluated on the devel-opment set).We would like to increase the size of external dataset to include all of the approximately 380K tweets(rather than the 50K subset we used).
This expandedtraining set would likely improve the robustness ofthe system.
Specifically, we would expect classifierswith limited coverage, such as the repeat-letter clas-sifier, to yield increased performance.428ReferencesFarah Benamara, Carmine Cesarano, Antonio Picariello,Diego Reforgiato, and V Subrahmanian.
2007.
Senti-ment analysis: Adjectives and adverbs are better thanadjectives alone.
In Proceedings of the InternationalConference on Weblogs and Social Media (ICWSM).A.
Go, R. Bhayani, and Huang.
L. 2009.
Twitter senti-ment classification using distant supervision.
Techni-cal report, Stanford University.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter sentiment analysis: The goodthe bad and the omg.
In Proceedings of the Fifth In-ternational AAAI Conference on Weblogs and SocialMedia, pages 538?541.Bing Liu.
2010.
Sentiment analysis and subjectivity.Handbook of natural language processing, 2:568.Tetsuya Nasukawa and Jeonghee Yi.
2003.
Sentimentanalysis: capturing favorability using natural languageprocessing.
In Proceedings of the 2nd internationalconference on Knowledge capture, K-CAP ?03, pages70?77.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, KevinGimpel, Nathan Schneider, and Noah A. Smith.
2013.Improved part-of-speech tagging for online conver-sational text with word clusters.
In Proceedings ofNAACL 2013.Bo Pang and Lillian Lee.
2004.
A sentimental edu-cation: sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, ACL ?04, Stroudsburg, PA, USA.Association for Computational Linguistics.Maite Taboada, Julian Brooke, Milan Tofiloski, KimberlyVoll, and Manfred Stede.
2011.
Lexicon-based meth-ods for sentiment analysis.
Computational linguistics,37(2):267?307.Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, MingZhou, and Ping Li.
2011.
User-level sentiment anal-ysis incorporating social networks.
In Proceedings ofthe 17th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 1397?1405.Casey Whitelaw, Navendu Garg, and Shlomo Argamon.2005.
Using appraisal groups for sentiment analysis.In Proceedings of the 14th ACM international con-ference on Information and knowledge management,CIKM ?05, pages 625?631.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage.
Language Resources and Evaluation, 39(2-3):165?210.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, SaraRosenthal, Veselin Stoyanov, and Alan Ritter.
2013.SemEval-2013 task 2: Sentiment analysis in twitter.In Proceedings of the International Workshop on Se-mantic Evaluation, SemEval?13.429
