Controlled Realization of Complex Objectsby Reversing the Output of a ParserDavid D. McDonaldGensym Corporation, 125 CambridgePark DriveCambridge, MA 02139 dmcdonald@gensym.comAbst rac tThis paper Is astudy in the tactics of content selection and realization at the micro-planninglevel.
It presents a .technique for controlling the content and phrasing of complex sentencesthrough?
the use of data derived from a parser that has read through a corpus and taken note ofwhich variations do and do not occur in the realization of the concepts in the genre the corpus istaken from.
These findings are entered as annotations on a new representational?
device, a'saturation lattice', that provides a systematic way to define partial information and is thejumping off point for the micro-planner.
The generator and parser are both based ?on adeclarative, bi-directional representation f realization relationship between concepts and text.Keywords: ?
generation, bi-directional, realization, model-driven1.
MotivationsIf research on natural anguage generation ('nlg'), as it is understood by the audience of thisworkshop, ?
is to garner support from.
Commercial interests and move beyond sponsored research andacademics, it will have to provide a set of commercially valuable, sophisticated tools that can achieveresults well beyond those of a good database system's report generator ('dbg')----our practicalcompetition.
In the early days of work in nlg it was ?difficult enough to say "The Knox is enroute toSarasibo", but any dbg can say that today.
NLG techniques now ten years old can apply techniques forsubsequent reference and aggregation?
and produce reasonably fluent paragraphs from sets of simple?
propositions, e,g., "The Knox, which is C4, is?
?enroute to Sarasibo.
It will arrive ..."; see Hovy (1990).
Atool set that can provide that sort of smoothing over the outPut of a dbgis a good start, but we need to dobetter.When we look at human report generators, uch as journalists or the authors of press releases, wesee two skills that are utterly beyond the abilities of a dbg: compact, syntactically rich sentence structure,.38!
!iI,Iili liIiiiiand the ability to realize partially saturated relations.
The first ability is familiar to the nlg communityand is typified by state of the art systems uch as Robin's Streak (Robin & McKeown ?I996).
The secondis best shown through an example?
Consider the following sentence, l which was produced by a computerprogram applying the techniques describe in this paper~(1) "'Net income in the third quarter ended Nov. 30, 1995 totaled $55.9 million, a 34 percentincrease over net income of $41.8 million in the comparable quarter of the precedingfiscal year.
"This sentence is missing one of the terms required to make it a complete (saturated) relation, ?namely thecompany whose earnings these are.
The person who originally planned this sentence recognized that theidentity of the company would be 'obvious from context' and that it should beomitted here in order to becohesive.In a corpus of such earnings reports we see the primary, 14 term "financial report' relation slicedand diced ?
in myriad ways, with variations in what terms are omitted, how they are grouped intomaximally projecting phrases, how they are distributed into sentences, and which terms are used as thehead.
Almost without exception each instance is the realization of only part of the relation, and the Skillof the authors is in appreciating what combinations (partial saturations) they can legitimately expressgiven the style that the intended audience is accustomed to.
Capturing this skill in a natural anguagegeneration system will take us in the right direction visa vie our commercial competition.
The question,of course, is how to do it.The problem naturally divides into two aspects.
Following Levelt (1989), these are macro-planning:the selection of what sets of terms are to be included, how they are to be distributed within the text as awhole, and most of the lexical selection; and micro-planning: determining the Structure of the text andthe apportioning of the terms into phrases, clauses and sentences that properly realize the macro-planner's pecifications of salience while maintaining cohesion, avoiding unintendedredundancy, andthe like.?
?
2 In  this paper I propose how to solve, even finesse, the micro-planning problems posed by acomplex yet mundane domain such as earnings reports ('era'), namely by reversing the output of aparser.
In broad outline this is done as follows.This is an exampleof the sort of sentence one tends to see at the beginning of the second paragraph of a pressrelease reporting acompany's quarterly earnings: the one that contains the primary information to be coiiveyedI frankly do not understand the basis of the macro-planning evidenced by the authors of ern articles.
Perhaps 10%of the variance can be explained by the occasional goal of hiding bad news, but otherwise the pattern of thedistribution and inclusion of information isanything but obvious; I suspect that much of the decision making is arote form of copying what other authors write.
A careful longitudinal study of the structure articles from known39The parser 3 is augmented to record the manner in which the phrases that it parses have beenrealized.
This record is couched in terms of the same set of linguistic resources as used by the generator,in this instance a TAG and a set of  reversible mapping rules linking concepts and resources.
Afterparsing a corpus of texts, the result is an annotation of all the particular ways in which the phrases(thosethat were understood) have been realized (and implicitly the ways in which they have not been realized).This lattice of realization types is deployed in the micro-planner by starting with the content selected bythe macro-planner(some structure over partially saturated relations), finding the point in the lattice thatcorresponds to (each of) the partial relations, and then selecting from among the strands of alternativerealization types within the lattice according to simple notions like theme vs. background.
The selectionis read out to create the text structure, and the rest o f  the generation process (surface realization)proceeds normally.2.
Arch i tec tureThe overall design of the generation architecture used here is as described in Meteer 1992,following a set of principles laid out in McDonald, Pustejovsky, and Meteer 1988.
It is a message-drivensystem that begins with Structures taken directly from the underlying system for which it is speaking,realizing them monotonically via a succession of progressively more linguistic represen-tational .levelsthrough the use of an ?extensive set of abstract linguistic resources ultimately grounded in a TAGgrammar of English."
The source structures are represented in a Kl-one derived system called Krisp (McDonald 1994s),and the parser that produces the corpus-based lattice of realization types is Sparser (McDonald 1992,1994a)mtwo complete, mature systems.
We will introduce only as much information about them asnecessary to support he rest of the discussion.
Much of this paper will be devoted to an extension toKrisp, a "saturation lattice', that is the basis of this technique of micro-planning by reversing the parser'soutput.At present he new generator, to be christened ,Magellan", implements only the micro-planning andlater stages of generation.
There is no speaker in a situation with genuine motivations, without which aauthors might shed some light on the problem.
If pressed, I would make a macro-planner from reverse ngineeredschemas with a randQm element.3 I will use the term 'parse' and 'parser' as a convenient ?
short-hand for designating the full natural languageunderstanding system that is actually being used.
This does not Stretch the usual senses of the term too much sinceSparser does do its semantic interpretation at literally the same time as its parsing into syntactic phrases.
The keydifference from the usual parser is that the end result is a set of objects in a domain model rather than just a parsetree.?
40|II.IIIi,l:iligeneration system (or certainly a fully-articulated theory of generation) is incom-plete.
In its stead, as away to exercise the micro-planner, is a windup toy - -a  largely graphical interface that permits theexperimenter to stipulate the inpu t and decision criteria that a macro-planner would have produced andsee what text results.The  generation proces  starts with the Krisp units representing a full relation.
We select by hand thefragment of it to be expressed and some simple information-structure parameters.
Then the micro-planning mechanism described here is deployed to populate a Text Structure representation, which hasbeen excerpted irectly from Meteer's Spokesman system (1992).
Spokesman's mechanisms then readout the Text Structure to create the TAG derivation tree that is the input to Mumble (Meteer et al !987),which in turn produces a TAG-based surface structure and from that the eventual text.
~2.1 Categories in the domain modelThe micro-planner's task is to realize a single, highly-structured, compositional ?relation as a TextStructure.
To illustrate theresources it uses to do this, consider these two (made up) sentences:.
(2) "GTE owns BBN.
"(3) "BBN is a Subsidiary of GTE.
"These express the same information.
They should be represented by the same object in the domainmodel.
Which of the two alternative realizations of this object a speaker will choose is a question ofwhich of the two companies they decide to make the theme.The expression below defines the type, or 'semantic ategory', that these texts instantiate.
This is aLisp expression that is evaluated at the time the domain model is loaded.
It makes reference to severalthings that will already have been defined, notably thecategory's parent super-category in the taxonomiclattice, named 'owner-owns-owned', 4 and two tree families in the grammar.
Once the expression hasbeen executed, theresult is (1) a category object representing the type in the domain model; (2) a setofphrase structure rules in the semantic grammar used bythe parser (based on the 'realization' field); and(3) a "saturation lattice' (based on the 'binds' field) which is described below.
(The realization field isonly sketched here since it's values would make little sense without he background that will be supplied4 There is an unfortunate ndency for names like these to dominate how a person thinks aboutconcepts.
Simplybecause names are necessarily (if they are to be useful) comprised of regular, suggestive natural language words,they can too often cloud the mind to the possibility that here are many different ways to realize the sameconceptual content (see, e.g., Elhadad et al 1996) This is something always to be guarded against.The choice of names for all the objects in this domain model and grammar isarbitrary and strictly for theconvenience of the human designer.
Because they are implemented in terms of objects and pointers, the namesare not even used at runtime, and serve only to provide a way to uniquely designate he objects in the writtenexpressions that are needed to initially define them when a direct manipulation i terface is not being used41in a later section; here simply observe that there are two alternative classes of realizations available to?
objects of this category, each with its own family of syntactic trees and own primary, content-beatinglexeme.
)(de f ine -category  co -owns-co:b inds  ( (parent  .
company)( subs id ia ry  .
company): spec ia l i zes  (owner -owns-owned (owner  .
parent )(owned subs id ia ry )  ): rea l i  zat  ion( ( : t ree - fami ly  t rans i t i ve /pass ive:mapp ing  ( ... "own"  _ ) )( : t ree - fami ly  nomina l -b inary - re la t ion:mapp ing  ( ... " subs id ia ry"  ... ) ) ) )This category corresponds roughly to a KLOne concept with two slots named 'parent' and 'subsidiary',whose values are restricted to objects of type (category) company.
The "specializes' field indicates howthis category is knit into the taxonomic lattice and the inheritance path of its slots, which are termed'variables' in Krisp.2.2 Saturat ion  latticesThe reference model for Krisp is the Lambda Calculus, where there is a well articulated notion ofexpressions that have only bound a few of their variables to specific values and left the others open.
Such'partially saturated relations' have a first class representation in Krisp.
This representation is supportedby a lattice 'below' each of the categories of the normal taxonomic (is-a) lattice.
This lattice definestypes for all of the category's sets of possible partial instantiations and provides a representational anchorfor the realization annotations that the parser lays down and the micro-planner uses.As shown below, a Saturation latticeconsists of a linked set of nodes that represent all the possiblecombinations of bound and open variables of the category, including a pseudo-variable "self that allowsus to include the category itself.
(This variable is usually realized as the verb of a clause or the head nounof a np.)
Notice the use of single-letter abbreviations for the variables when they appear i n multi-variablenodes.self ( 's ' )  p reat (t ip) 's+ p+bs a id iaby( '  ' )b42In the present example, the lattice is relatively simple with just three levels, s At the top we have theinformation states where one of the three variables is bound and the other two open.
Next these nodes('lattice points') converge to form a level where each possible combination of two bound and "one openvariable is represented.
These then join to form the lattice point that represents the state where therelation is fully saturated, i.e.
all of its variables are bound.
This bottom node in the lattice is annotatedby the various contexts in this category has appeared as a contiguous phrase: as a whole sentence, as asubordinated clause, as a reduced phrase in a conjunct, etc.
The abstract resources for these contextscorrespond to attachment points in Mumble and usually adjunctions in a TAG.The saturation lattice is used in the parsing direction to provide a set of indexes that anchor andorganize partial results so that phrases with the same denotation are directed, through the paths of thelattice, to the same model-level object.
6 In the generation direction it is used to inform the micro-plannerof the realization potential of each of the partial relation types.
The basis of this information is a set ofannotations on the lattice points that record what realizations the parser has seen for that combination ofbound and open variables and in what context s they have occurred.These annotations are recorded or elaborated every time the parser reads a text that has instances(partial or full) of that category.
For example, if we imagine that the parser has seen just examples 2 and3, then, roughly speaking, it will have recorded that the combination of self and subsidiary ('s+b') can berealized as a VP ("owns BBN") and that s+p can be realized as a possessive NP ("subsidiary of GTE"),but it will have no reason to believe that there is a direct (self-contained) realization of p+b since.it hasnever seen them together as the only content elements in one phrase.
7 Should it later read a text thatincludes the phrasing "'...BBN, a subsidiary of GT ..." (or for that matter "'...lsoQuest, a subsidiary ofSRA..."), it will extend the annotation on the s+b lattice point to include that relative clause pattern.7If a category defines N variables then its saturation lattice has N+I factorial nodes over N+I" levels.
For the initialfinancial example, which is the realization of a 14-tuple, this means its lattice could in principle contain severalbillion nodes distributed across 15 levels.
!t obviously does not, and the reason is simply that the lattice is 0nlyinstantiated as the parser finds particular combinations of variables.
Because the compartmentalization of theelements of the 14 tuple is high and their actual patterns of combination relatively few, the lattice has not quite ahundred nodes as this is written.This is the way that Krisp implements the 'uniqueness principle' articulated by Maida and Shapiro 1982 wherebyevery individual has a single representation i  thedomain model regardless of how often or in what context itOccurs.Given our knowledge of English grammar, we can imagine the gapping construction where this would occur:"GTE owns BBN and IBM Lotus", but i t has not occurred in this corpus, therefore it is not included in therealization patterns recorded in the saturation lattice.432.3 Strands of annotat ionsThe annotations on the lattice points are not independent.
They are linked together in  strandsrunning down through the saturation lattice that reflect he parser's derivation tree as it accdmulatedsuccessively arger portions of the text to the head-line of its maximal phrases, binding one variable afteranother in a particular order and thereby following a particular path down through the lattice.
Eachderivation tree that has been seen for a given combination of variable bindings is a separate strand.
It isthe micro-planner's job to choose one 0f these strands based on the properties of the individual nodes iti s  Comprised of (one for each binding).
Having selected astrand, it then Creates (or extends) the TextStructure by taking the concrete relation that it has been given by the macro planner and using the strandas a recipe for introducing the objects in the relation into the Text Structure.
They are added one by oneas the micro-planner reads out the strand from top to bottom, at each step adding the object hat is boundto the variable that was added at that lattice point.This use of Strands lets us capture some delicate co-occurrence Constrains for free because :therealizations of the terms in a relation (and their constituent terms) are not independent but must followthe pattern defined by the selected strand.
In the ern domain consider the common alternation in theplacement of the "fractional time period' term ("quarter", "'nine months", etc.)
with respect o the'financial item' term ("earnings", "'turnover", etc.)
in a phrase that anchors the reporting period to aparticular point in the calendar.
We typically see phrasings like #4 or #5 but never the combination i #6.
(4) "'..,quarterly earnings for the period ending March 31..." "(5) ...earnings for the quarter ending March 31..."(6) * "...quarterly earnings for the quarter ending March 31..."The question is how is #6 to be avoided.
The source for the anchor adjunct includes the fact that the'period' is one fiscal quarter; what is the constraint mechanism that suppresses the expression of theactual time period when it has been stipulated, thematically, tOappear with the head noun?The answer is simply that the pattern of realiz~itions in #6 has never been seen by the parser andconsequently there is 90 strand for it in the lattice.
The parser has done all the work and the micro-planne r reaps the benefitS without he need for any sort of active constraint propagation mechanism.
Itjust reads out the template that the parser has provided.3.
Bi-directional ResourcesThis technique ispredicated on the parser and generator sharing the same model of the language sothat the observations of the parser can be capitalized on by the generator.
Such reversibility is a common,44if seldom deployed, idea in computational linguistics (see papers in Strzalkowski 1994).
Here the turn-around point is in the domain model that represents what the parser understood rather than at thetypically chosen level of logical form (see, e.g., Shieber et al 1990).
This has choice has considerable?advantage in leverage because the model can be very abstract, and in practical engineering since thedomain model is invariably developed by reverse-engineering actual texts.
We earlier saw an example ofa category in the model.
Now we turn to the resources that define the (bulk of) the linguistic knowledge.The grammar is a TAG, ?given in its usual form on the generation side in Mumble, but a verydifferent one on the parsing side.
s For the parser, the TAG is ?reorganized (by hand) by sectioning thetrees horizontally into patterns of immediate constituents in the manner of SchabeS and Waters (1992) asshown in the example below, 9 which is followed by the full detail of the part of  the realization field ofco-owns-co that goes with this tree family; syntactic categories on the left side of the mapping arereplaced with the semantic categories on the right.
(define-exploded-tree-family transitive/passive:binding-parameters ( agent patient )labels ( s vp vg np/subject np/object  )cases ((:theme (s (np/subject vp): head r ight-edge:binds (agent left-edge)))( : theme (s (np/subject vg):head r ight-edge:binds (agent left-edge)))(:final (vp (vg np/object):head left-edge:binds (patient right-edge)))(:theme (s (np/object vg/+ed):head r ight-edge:binds (patient left-edge)))(:subordinated (vp (vg/+ed by/pp/np/subject):head left-edge:binds (agent right-edge) ) ) ) )( : tree-family transit ive/passive:mapping ((agent .
parent)(patient .
subsidiary)( s  .
se l f )  ;, i .e.
the category co-owns-co(vp .
self)(vg .
"own")(np/subject .
company)(np/object .
company) ) )The annotations left by the parser on the nodes of the saturation lattice are essentially just pointer s backto the rule in the 'exploded' tree family that it applied when it added the constituent hat bound that term.In the sense of Appeit (1988) this is a 'compilation'-based treatment Of bi-directional processing.Note that it does not include rewrite expressions for any of the 'oblique' forms that clauses are subject o(relatives, reductions under conjunction, clefts); these are standard to clauses of all sorts and the parser handlesthem through a common set of rules of a different kind.45.?
Thus the annotation includes the label, such as theme or final, that characterizes the comparative statusof the np term in the rule, making it available to the micro-planner to aid in its choice of strands.?
\4.
Conc lus ionsBy us ing the saturation lattices to guide its micro,planning process and control its choice ?
ofmappings from conceptual objects to.
linguistic resources, this system can readily produce the long,syntactically elaborate texts that a common in commercial news sources (after all, the parser has virtuallylaid down a template for the generator to follow), and by being trained on the appropriate corpus it cando so using thestyle that is natural to the genre.
The macro-planner can freely rearrange or factor ?
theinformation that the parser ead when it goes to generated new texts, but it will be unable to violate thenorms of how that information is expressed simply ?because it wil l  be uSing no source other than thesaturation lattices to make the final realization decisions.These sub-categ0ry saturation lattices are(to my knowledge) a new representational device, one thatpermits us to make use of the knowledge of how particular types of information are expressed that anyparser implicitly deploys.
As implemented , the process o f  annotating the nodes is a completely automaticside-effect of incrementally indexing the information contained in partial phrases during the course of aparse.
Because the process is ubiquitous and covers all the particular facts I?
that are acquired from thereading (and these are all the facts the system knows), the planning that is done during generation isfreed from needing to worry about fine-grained etails, is assured that its text plans will be expressible,and can concentrate on the substantive issues of what particulars to include and where to place the?emphasis.Finally, this approach may lead us to a psycholinguistic model of how it is that people so readilyadapt heir style of writing or speaking to the patterns what they have recently heard or read.ReferencesAppelt, Doug.
1989.
"Bidirectional Grammars and the Design of Natural Language Generation Systems".
in Wilks(ed.)
Theoretical Issues in Natural Language Processing, Lawrence Edbaum, Hillsdale, New Jersey, pp.
199-205.Elhadad M., K. McKeown, J. Robin.
1996.
Floating Constraints inLexical .Choice.
Computational Linguistics.H0vy, Eduard (1990) "Unresolved Issues in Paragraph Planning", in Dfile.
Meliish& Zock (eds.)
Current Researchin Natural Language Generation, Academic Press, New York.Levelt P. 1989.
Language Production.
MIT Press.10 As opposed to the generic knowledge mbodied in the semantic categories of the domain model.46 "Maida A.
& S. Shapiro.
1982.
"Intensional Concepts in Propositional Semantic Networks".
Cognitive Science 6, pp.291-330.McDonald D. 1992.
"An Efficient Chart-based Algorithm for Partial-Parsing of Unrestricted Texts".
proceedings ofthe 3d Conference on Applied Natural Language Processing (ACL), Trento, Italy, April 1992, pp.
193-.200McDonald D. 1994a.
Reversible NLP by linking the grammar to the knowledge base.
in Strazalkowski 1994, 257-291.McDonald D. 1994b.
'Krisp' a representation for the semantic interpretation oftexts.
Mind and Machines 4, 59-73,McDonald D., J. Pustejovsky, M. Meteer.
1988.
Factors contributing to efficiency in natural language generation.
InKempen G.
(Ed.)
1987 Natural Language Generation, Martinus Nijhoff, Dordrecht, 159-181.Marie Meteer (1992) Expressibility and the Problem of Efficient Text Planning, Pinter Publishers, London.Meteer, Marie W., David McDonald, Scott Anderson, David Forster, Linda Gay, Alison Huettner & Penelope Sibun,(1987) Mumble-86: Design and Implementation, TR #87-87 Dept.
ComPuter & Information Science, UMass.,174 pgs.Robin, J and K. McKeown.
1996.
Empirically designing and evaluating anew revision-based model for summary "generation.
Artificial Intelligence 85; August.Shieber, Stuart, Gertjan van Noord, Fernando Pereira & Robert Moore (1990) "Semantic-Head-Driven Generation,Computational Linguistics (16) 1, Marc h 1990, pp.
30-42.Strzalkowski T. 1994.
Reversible Grammar in Natural Language Processing.
Kluwer Academic.I47
