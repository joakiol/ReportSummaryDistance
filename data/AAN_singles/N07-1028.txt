Proceedings of NAACL HLT 2007, pages 220?227,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsA Case for Shorter Queries, and Helping Users Create ThemGiridhar Kumaran and James AllanCenter for Intelligent Information RetrievalDepartment of Computer ScienceUniversity of Massachusetts AmherstAmherst, MA 01003, USA{giridhar,allan}@cs.umass.eduAbstractInformation retrieval systems are fre-quently required to handle long queries.Simply using all terms in the query or re-lying on the underlying retrieval modelto appropriately weight terms often leadsto ineffective retrieval.
We show that re-writing the query to a version that com-prises a small subset of appropriate termsfrom the original query greatly improveseffectiveness.
Targeting a demonstratedpotential improvement of almost 50% onsome difficult TREC queries and their as-sociated collections, we develop a suite ofautomatic techniques to re-write queriesand study their characteristics.
We showthat the shortcomings of automatic meth-ods can be ameliorated by some simpleuser interaction, and report results that areon average 25% better than the baseline.1 IntroductionQuery expansion has long been a focus of infor-mation retrieval research.
Given an arbitrary shortquery, the goal was to find and include additionalrelated and suitably-weighted terms to the originalquery to produce a more effective version.
In this pa-per we focus on a complementary problem ?
queryre-writing.
Given a long query we explore whetherthere is utility in modifying it to a more concise ver-sion such that the original information need is stillexpressed.The Y!Q beta1 search engine allows users to se-lect large portions of text from documents and issuethem as queries.
The search engine is designed toencourage users to submit long queries such as thisexample from the web site ?I need to know the gasmileage for my Audi A8 2004 model?.
The moti-vation for encouraging this type of querying is thatlonger queries would provide more information inthe form of context (Kraft et al, 2006), and this ad-ditional information could be leveraged to providea better search experience.
However, handling suchlong queries is a challenge.
The use of all the termsfrom the user?s input can rapidly narrow down theset of matching documents, especially if a booleanretrieval model is adopted.
While one would ex-pect the underlying retrieval model to appropriatelyassign weights to different terms in the query andreturn only relevant content, it is widely acknowl-edged that models fail due to a variety of reasons(Harman and Buckley, 2004), and are not suited totackle every possible query.Recently, there has been great interest in personal-ized search (Teevan et al, 2005), where the query ismodified based on a user?s profile.
The profile usu-ally consists of documents previously viewed, websites recently visited, e-mail correspondence and soon.
Common procedures for using this large amountof information usually involve creating huge queryvectors with some sort of term-weighting mecha-nism to favor different portions of the profile.The queries used in the TREC ad-hoc tracks con-sist of title, description and narrative sections, ofprogressively increasing length.
The title, of length1http://yq.search.yahoo.com/220ranging from a single term to four terms is consid-ered a concise query, while the description is consid-ered a longer version of the title expressing the sameinformation need.
Almost all research on the TRECad-hoc retrieval track reports results using only thetitle portion as the query, and a combination of thetitle and description as a separate query.
Most re-ported results show that the latter is more effectivethan the former, though in the case of some hard col-lections the opposite is true.
However, as we shallshow later, there is tremendous scope for improve-ment.
Formulating a shorter query from the descrip-tion can lead to significant improvements in perfor-mance.In the light of the above, we believe there is greatutility in creating query-rewriting mechanisms forhandling long queries.
This paper is organized inthe following way.
We start with some examplesand explore ways by which we can create concisehigh-quality reformulations of long queries in Sec-tion 2.
We describe our baseline system in Section 3and motivate our investigations with experiments inSection 4.
Since automatic methods have shortfalls,we present a procedure in Section 5 to involve usersin selecting a good shorter query from a small selec-tion of alternatives.
We report and discuss the resultsof this approach in Section 6.
Related work is pre-sented in Section 7.
We wrap up with conclusionsand future directions in Section 8.2 Selecting sub-queriesConsider the following query:Define Argentine and British international rela-tions.When this query was issued to a search engine,the average precision (AP, Section 3) of the resultswas 0.424.
When we selected subsets of terms (sub-queries) from the query, and ran them as distinctqueries, the performance was as shown in Table 1.
Itcan be observed that there are seven different waysof re-writing the original query to attain better per-formance.
The best query, also among the shortest,did not have a natural-language flavor to it.
It how-ever had an effectiveness almost 50% more than theoriginal query.
This immense potential for improve-ment by query re-writing is the motivation for thispaper.Query AP.... ....international relate 0.000define international relate 0.000.... ....define argentina 0.123international relate argentina 0.130define relate argentina 0.141relate argentina 0.173define britain international relate argentina 0.424define britain international argentina 0.469britain international relate argentina 0.490define britain relate argentina 0.494britain international argentina 0.528define britain argentina 0.546britain relate argentina 0.563britain argentina 0.626Table 1: The results of using all possible subsets (ex-cluding singletons) of the original query as queries.The query terms were stemmed and stopped.Analysis of the terms in the sub-queries and therelationship of the sub-queries with the originalquery revealed a few interesting insights that had po-tential to be leveraged to aid sub-query selection.1.
Terms in the original query that a human wouldconsider vital in conveying the type of infor-mation desired were missing from the best sub-queries.
For example, the best sub-query forthe example was britain argentina, omittingany reference to international relations.
Thisalso reveals a mismatch between the user?squery and the way terms occurred in the corpus,and suggests that an approximate query couldat times be a better starting point for search.2.
The sub-query would often contain only termsthat a human would consider vital to the querywhile the original query would also (naturally)contain them, albeit weighted lower with re-spect to other terms.
This is a common prob-lem (Harman and Buckley, 2004), and the fo-cus of efforts to isolate the key concept termsin queries (Buckley et al, 2000; Allan et al,1996).3.
Good sub-queries were missing many of thenoise terms found in the original query.
Ideallythe retrieval model would weight them lower,but dropping them completely from the queryappeared to be more effective.2214.
Sub-queries a human would consider as an in-complete expression of information need some-times performed better than the original query.Our example illustrates this point.Given the above empirical observations, we ex-plored a variety of procedures to refine a long queryinto a shorter one that retained the key terms.
We ex-pected the set of terms of a good sub-query to havethe following properties.A.
Minimal Cardinality: Any set that containsmore than the minimum number of terms to retrieverelevant documents could suffer from concept drift.B.
Coherency: The terms that constitute the sub-query should be coherent, i.e.
they should buttresseach other in representing the information need.
Ifneed be, terms that the user considered important butled to retrieval of non-relevant documents should bedropped.Some of the sub-query selection methods we ex-plored with these properties in mind are reported be-low.2.1 Mutual InformationLet X and Y be two random variables, with jointdistribution P (x, y) and marginal distributions P (x)and P (y) respectively.
The mutual information isthen defined as:I(X;Y ) =?x?yp(x, y)log p(x, y)p(x)p(y)(1)Intuitively, mutual information measures the infor-mation about X that is shared by Y .
If X and Y areindependent, then X contains no information aboutY and vice versa and hence their mutual informationis zero.
Mutual Information is attractive because it isnot only easy to compute, but also takes into consid-eration corpus statistics and semantics.
The mutualinformation between two terms (Church and Hanks,1989) can be calculated using Equation 2.I(x, y) = logn(x,y)Nn(x)Nn(y)N(2)n(x, y) is the number of times terms x and y oc-curred within a term window of 100 terms across thecorpus, while n(x) and n(y) are the frequencies ofx and y in the collection of size N terms.To tackle the situation where we have an arbi-trary number of variables (terms) we extend the two-variable case to the multivariate case.
The extension,called multivariate mutual information (MVMI) canbe generalized from Equation 1 to:I(X1;X2;X3; ...;XN ) =N?i=1(?1)i?1?X?
(X1,X2,X3,...,XN),|X|=kH(X) (3)The calculation of multivariate information usingEquation 3 was very cumbersome, and we insteadworked with the approximation (Kern et al, 2003)given below.I(X1;X2;X3; ...;XN ) = (4)?i,j={1,2,3,...,N ;i6=j}I(Xi;Xj) (5)For the case involving multiple terms, we calcu-lated MVMI as the sum of the pair-wise mutual in-formation for all terms in the candidate sub-query.This can be also viewed as the creation of a com-pletely connected graph G = (V,E), where the ver-tices V are the terms and the edges E are weightedusing the mutual information between the verticesthey connect.To select a score representative of the quality ofa sub-query we considered several options includ-ing the sum, average, median and minimum of theedge weights.
We performed experiments on a setof candidate queries to determine how well each ofthese measures tracked AP, and found that the aver-age worked best.
We refer to the sub-query selectionprocedure using the average score as Average.2.2 Maximum Spanning TreeIt is well-known that an average is easily skewedby outliers.
In other words, the existence of one ormore terms that have low mutual information withevery other term could potentially distort results.This problem could be further compounded by thefact that mutual information measured using Equa-tion 2 could have a negative value.
We attempted222to tackle this problem by considering another mea-sure that involved creating a maximum spanning tree(MaxST) over the fully connected graph G, and us-ing the weight of the identified tree as a measure rep-resentative of the candidate query?s quality (Rijsber-gen, 1979).
We used Kruskal?s minimum spanningtree (Cormen et al, 2001) algorithm after negatingthe edge weights to obtain a MaxST.
We refer to thesub-query selection procedure using the weight ofthe maximum spanning tree as MaxST.2.3 Named EntitiesNamed entities (names of persons, places, organiza-tions, dates, etc.)
are known to play an importantanchor role in many information retrieval applica-tions.
In our example from Section 2, sub-querieswithout Britain or Argentina will not be effectiveeven though the mutual information score of theother two terms international and relations mightindicate otherwise.
We experimented with anotherversion of sub-query selection that considered onlysub-queries that retained at least one of the namedentities from the original query.
We refer to the vari-ants that retained named entities as NE Average andNE MasT.3 Experimental SetupWe used version 2.3.2 of the Indri search engine, de-veloped as part of the Lemur2 project.
While theinference network-based retrieval framework of In-dri permits the use of structured queries, the useof language modeling techniques provides better es-timates of probabilities for query evaluation.
Thepseudo-relevance feedback mechanism we used isbased on relevance models (Lavrenko and Croft,2001).To extract named entities from the queries, weused BBN Identifinder (Bikel et al, 1999).
Thenamed entities identified were of type Person, Lo-cation, Organization, Date, and Time.We used the TREC Robust 2004 and Robust 2005(Voorhees, 2006) document collections for our ex-periments.
The 2004 Robust collection containsaround half a million documents from the Finan-cial Times, the Federal Register, the LA Times, andFBIS.
The Robust 2005 collection is the one-million2http://www.lemurproject.orgdocument AQUAINT collection.
All the documentswere from English newswire.
We chose these col-lections because they and their associated queriesare known to be hard, and hence present a chal-lenging environment.
We stemmed the collectionsusing the Krovetz stemmer provided as part of In-dri, and used a manually-created stoplist of twentyterms (a, an, and, are, at, as, be, for, in, is, it, of, on,or, that, the, to, was, with and what).
To determinethe best query selection procedure, we analyzed 163queries from the Robust 2004 track, and used 30 and50 queries from the 2004 and 2005 Robust tracks re-spectively for evaluation and user studies.For all systems, we report mean average preci-sion (MAP) and geometric mean average precision(GMAP).
MAP is the most widely used measure inInformation Retrieval.
While precision is the frac-tion of the retrieved documents that are relevant, av-erage precision (AP) is a single value obtained byaveraging the precision values at each new relevantdocument observed.
MAP is the arithmetic mean ofthe APs of a set of queries.
Similarly, GMAP is thegeometric mean of the APs of a set of queries.
TheGMAP measure is more indicative of performanceacross an entire set of queries.
MAP can be skewedby the presence of a few well-performing queries,and hence is not as good a measure as GMAP fromthe perspective of measure comprehensive perfor-mance.4 ExperimentsWe first ran two baseline experiments to record thequality of the available long query and the shorterversion.
As mentioned in Section 1, we used thedescription and title sections of each TREC queryas surrogates for the long and short versions re-spectively of a query.
The results are presented inthe first two rows, Baseline and Pseudo-relevanceFeedback (PRF), of Table 2.
Measured in terms ofMAP and GMAP (Section3), using just the title re-sults in better performance than using the descrip-tion.
This clearly indicates the existence of terms inthe description that while elaborating an informationneed hurt retrieval performance.
The result of usingpseudo-relevance feedback (PRF) on both the titleand description show moderate gains - a known factabout this particular collection and associated train-223MAP GMAPLong Query Baseline 0.243 0.136(Description) PRF 0.270 0.124Short Query Baseline 0.249 0.154(Title) PRF 0.269 0.148Best sub-query Baseline 0.342 0.270(Combination) PRF 0.343 0.241Table 2: Results across 163 training queries on theRobust 2004 collection.
Using the best sub-queryresults in almost 50% improvement over the baselineing queries.To show the potential and utility of query re-writing, we first present results that show the upperbound on performance that can obtained by doingso.
We ran retrieval experiments with every combi-nation of query terms.
For a query of length n, thereare 2n combinations.
We limited our experimentsto queries of length n ?
12.
Selecting the perfor-mance obtained by the best sub-query of each queryrevealed an upper bound in performance almost 50%better than the baseline (Table 2).To evaluate the automatic sub-query selectionprocedures developed in Section 2, we performedretrieval experiments using the sub-queries selectedusing them.
The results, which are presented in Ta-ble 3, show that the automatic sub-query selectionprocess was a failure.
The results of automatic se-lection were worse than even the baseline, and therewas no significant difference between using any ofthe different sub-query selection procedures.The failure of the automatic techniques could beattributed to the fact that we were working with theassumption that term co-occurrence could be usedto model a user?s information need.
To see if therewas any general utility in using the procedures toselect sub-queries, we selected the best-performingsub-query from the top 10 ranked by each selectionprocedure (Table 4).
While the effectiveness in eachcase as measured by MAP is not close to the bestpossible MAP, 0.342, they are all significantly betterthan the baseline of 0.243.5 Interacting with the userThe final results we presented in the last sectionhinted at a potential for user interaction.
We envi-MAP GMAPBaseline 0.243 0.136Average 0.172 0.025MaxST 0.172 0.025NE Average 0.170 0.023NE MaxST 0.182 0.029Table 3: Score of the highest rank sub-query by var-ious measures.MAP GMAPBaseline 0.243 0.136AverageTop10 0.296 0.167MaxSTTop10 0.293 0.150NE AverageTop10 0.278 0.156NE MaxSTTop10 0.286 0.159Table 4: Score of the best sub-query in the top 10ranked by various measuressioned providing the user with a list of the top 10sub-query candidates using a good ranking proce-dure, and asking her to select the sub-query she feltwas most appropriate.
This additional round of hu-man intervention could potentially compensate forthe inability of the ranking measures to select thebest sub-query automatically.5.1 User interface designWe displayed the description (the long query) andnarrative portion of each TREC query in the inter-face.
The narrative was provided to help the partic-ipant understand what information the user who is-sued the query was interested in.
The title was kepthidden to avoid influencing the participant?s choiceof the best sub-query.
A list of candidate sub-querieswas displayed along with links that could be clickedon to display a short section of text in a designatedarea.
The intention was to provide an example ofwhat would potentially be retrieved with a high rankif the candidate sub-query were used.
The partici-pant used this information to make two decisions -the perceived quality of each sub-query, and the bestsub-query from the list.
A facility to indicate thatnone of the candidates were good was also included.224Percentage of candidatesbetter than baselineAverage 28.5%MaxST 35.5%NE Average 31.1%NE MaxST 36.6%Table 5: Number of candidates from top 10 that ex-ceeded the baseline5.2 User interface content issuesThe two key issues we faced while determining thecontent of the user interface were:A.
Deciding which sub-query selection procedureto use to get the top 10 candidate sub-queries: Todetermine this in the absence of any significant dif-ference in performance due to the top-ranked can-didate selected by each procedure, we looked at thenumber of candidates each procedure brought intothe top 10 that were better than the baseline query,as measured by MAP.
This was guided by the beliefthat greater the number of better candidates in thetop 10, the higher the probability that the user wouldselect a better sub-query.
Table 5 shows how each ofthe selection procedures compared.
The NE MaxSTranking procedure had the most number of bettersub-queries in the top 10, and hence was chosen.B.
Displaying context: Simply displaying a listof 10 candidates without any supportive informationwould make the task of the user difficult.
This was incontrast to query expansion techniques (Anick andTipirneni, 1999) where displaying a list of terms suf-ficed as the task of the user was to disambiguateor expand a short query.
An experiment was per-formed in which a single user worked with a set of30 queries from Robust 2004, and an accompanyingset of 10 candidate sub-queries each, twice - oncewith passages providing context and one with snip-pets providing context.
The top-ranked passage wasgenerated by modifying the candidate query intoone that retrieved passages of fixed length insteadof documents.
Snippets, like those seen along withlinks to top-ranked documents in the results fromalmost all popular search engines, were generatedafter a document-level query was used to query thecollection.
The order in which the two contexts werepresented to the user was randomized to prevent theMAP GMAPSnippet as Context 0.348 0.170Passage as Context 0.296 0.151Table 6: Results showing the MAP over 19 of 30queries that the user provided selections for usingeach context type.user from assuming a quality order.
We see that pre-senting the snippet led to better MAP that presentingthe passage (Table 6).
The reason for this could bethat the top-ranking passage we displayed was froma document ranked lower by the document-focussedversion of the query.
Since we finally measure MAPonly with respect to document ranking, and the snip-pet was generated from the top-ranked document,we hypothesize that this led to the snippet being abetter context to display.6 User EvaluationWe conducted an exploratory study with five par-ticipants - four of them were graduate students incomputer science while the fifth had a backgroundin the social sciences and was reasonably proficientin the use of computers and internet search engines.The participants worked with 30 queries from Ro-bust 2004, and 50 from Robust 20053.
The baselinevalues reported are automatic runs with the descrip-tion as the query.Table 7 shows that all five participants4 wereable to choose sub-queries that led to an improve-ment in performance over the baseline (TREC titlequery only).
This improvement is not only on MAPbut also on GMAP, indicating that user interactionhelped improve a wide spectrum of queries.
Mostnotable were the improvements in P@5 and P@10.This attested to the fact that the interaction tech-nique we explored was precision-enhancing.
An-other interesting result, from # sub-queries selectedwas that participants were able to decide in a largenumber of cases that re-writing was either not usefulfor a query, or that none of the options presented tothem were better.
Showing context appears to havehelped.3Participant 4 looked that only 34 of the 50 queries presented4The p value for testing statistical significance of MAP im-provement for Participant 5 was 0.053 - the result very narrowlymissed being statistically significant.225# Queries # sub-queries % sub-queries MAP GMAP P@5 p@10selected betterBaseline 0.203 0.159 0.
476 0.5071 50 26 80.7% With Interaction 0.249 0.199 0.615 0.580Upper Bound 0.336 0.282 0.784 0.719Baseline 0.224 0.156 0.484 0.5262 50 19 78.9% With Interaction 0.277 0.209 0.652 0.621Upper Bound 0.359 0.293 0.810 0.742Baseline 0.217 0.126 0.452 0.4323 80 53 73.5% With Interaction 0.276 0.166 0.573 0.501Upper Bound 0.354 0.263 0.762 0.654Baseline 0.192 0.142 0.462 0.5254 50(34) 19 68.7% With Interaction 0.255 0.175 0.612 0.600Upper Bound 0.344 0.310 0.862 0.800Baseline 0.206 0.111 0.433 0.4105 80 65 61.5% With Interaction 0.231 0.115 0.486 0.429Upper Bound 0.341 0.245 0.738 0.640Table 7: # Queries refers to the number of queries that were presented to the participant while # sub-queriesselected refers to the number of queries for which the participant chose a sub-query.
All scores includingupper bounds were calculated only considering the queries for which the participant selected a sub-query.An entry in bold means that the improvement in MAP is statistically significant.
Statistical significance wasmeasured using a paired t-test, with ?
set to 0.05.7 Related WorkOur interest in finding a concise sub-query that ef-fectively captures the information need is reminis-cent of previous work in (Buckley et al, 2000).However, the focus was more on balancing the ef-fect of query expansion techniques such that differ-ent concepts in the query were equally benefited.Mutual information has been used previously in(Church and Hanks, 1989) to identify collocations ofterms for identifying semantic relationships in text.Experiments were confined to bigrams.
The use ofMaST over a graph of mutual information valuesto incorporate the most significant dependencies be-tween terms was first noted in (Rijsbergen, 1979).Extensions can be found in a different field - imageprocessing (Kern et al, 2003) - where multivariatemutual information is frequently used.Work done by (White et al, 2005) provided a ba-sis for our decision to show context for sub-query se-lection.
The useful result that top-ranked sentencescould be used to guide users towards relevant mate-rial helped us design an user interface that the par-ticipants found very convenient to use.A related problem addressed by (Cronen-Townsend et al, 2002) was determining query qual-ity.
This is known to be a very hard problem, andvarious efforts (Carmel et al, 2006; Vinay et al,2006) have been made towards formalizing and un-derstanding it.Previous work (Shapiro and Taksa, 2003) in theweb environment attempted to convert a user?s natu-ral language query into one suited for use with websearch engines.
However, the focus was on merg-ing the results from using different sub-queries, andnot selection of a single sub-query.
Our approachof re-writing queries could be compared to query re-formulation, wherein a user follows up a query withsuccessive reformulations of the original.
In the webenvironment, studies have shown that most usersstill enter only one or two queries, and conduct lim-ited query reformulation (Spink et al, 2002).
We hy-pothesize that the techniques we have developed willbe well-suited for search engines like Ask Jeeveswhere 50% of the queries are in question format226(Spink and Ozmultu, 2002).
More experimentationin the Web domain is required to substantiate this.8 ConclusionsOur results clearly show that shorter reformulationsof long queries can greatly impact performance.
Webelieve that our technique has great potential to beused in an adaptive information retrieval environ-ment, where the user starts off with a more generalinformation need and a looser notion of relevance.The initial query can then be made longer to expressa most focused information need.As part of future work, we plan to conduct a moreelaborate study with more interaction strategies in-cluded.
Better techniques to select effective sub-queries are also in the pipeline.
Since we used mu-tual information as the basis for most of our sub-query selection procedures, we could not considersub-queries that comprised of a single term.
We planto address this issue too in future work.9 AcknowledgmentsThis work was supported in part by the Centerfor Intelligent Information Retrieval and in part bythe Defense Advanced Research Projects Agency(DARPA) under contract number HR0011-06-C-0023.
Any opinions, findings and conclusions orrecommendations expressed in this material are theauthors and do not necessarily reflect those of thesponsor.
We also thank the anonymous reviewersfor their valuable comments.ReferencesJames Allan, James P. Callan, W. Bruce Croft, Lisa Ballesteros,John Broglio, Jinxi Xu, and Hongming Shu.
1996.
Inqueryat TREC-5.
In TREC.Peter G. Anick and Suresh Tipirneni.
1999.
The paraphrasesearch assistant: terminological feedback for iterative infor-mation seeking.
In 22nd ACM SIGIR Proceedings, pages153?159.Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel.1999.
An algorithm that learns what?s in a name.
MachineLearning, 34(1-3):211?231.Chris Buckley, Mandar Mitra, Janet Walz, and Claire Cardie.2000.
Using clustering and superconcepts within smart:TREC 6.
Information Processing and Management,36(1):109?131.David Carmel, Elad Yom-Tov, Adam Darlow, and Dan Pelleg.2006.
What makes a query difficult?
In 29th ACM SIGIRProceedings, pages 390?397.Kenneth Ward Church and Patrick Hanks.
1989.
Word associ-ation norms, mutual information, and lexicography.
In 27thACL Proceedings, pages 76?83.Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest,and Clifford Stein.
2001.
Introduction to Algorithms, Sec-ond Edition.
The MIT Electrical Engineering and ComputerScience Series.
The MIT Press.Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft.
2002.Predicting query performance.
In 25th ACM SIGIR Proceed-ings, pages 299?306.Donna Harman and Chris Buckley.
2004.
The NRRC reliableinformation access (RIA) workshop.
In 27th ACM SIGIRProceedings, pages 528?529.Jeffrey P. Kern, Marios Pattichis, and Samuel D. Stearns.
2003.Registration of image cubes using multivariate mutual infor-mation.
In Thirty-Seventh Asilomar Conference, volume 2,pages 1645?1649.Reiner Kraft, Chi Chao Chang, Farzin Maghoul, and Ravi Ku-mar.
2006.
Searching with context.
In 15th InternationalCIKM Conference Proceedings, pages 477?486.Victor Lavrenko and W. Bruce Croft.
2001.
Relevance basedlanguage models.
In 24th ACM SIGIR Conference Proceed-ings, pages 120?127.C.
J.
Van Rijsbergen.
1979.
Information Retrieval.Butterworth-Heinemann, Newton, MA, USA, 2 edition.Jacob Shapiro and Isak Taksa.
2003.
Constructing web searchqueries from the user?s information need expressed in a nat-ural language.
In Proceedings of the 2003 ACM Symposiumon Applied Computing, pages 1157?1162.Amanda Spink and H. Cenk Ozmultu.
2002.
Characteristics ofquestion format web queries: An exploratory study.
Infor-mation Processing and Management, 38(4):453?471.Amanda Spink, Bernard J. Jansen, Dietmar Wolfram, and TefkoSaracevic.
2002.
From e-sex to e-commerce: Web searchchanges.
Computer, 35(3):107?109.Jaime Teevan, Susan T. Dumais, and Eric Horvitz.
2005.
Per-sonalizing search via automated analysis of interests and ac-tivities.
In 28th ACM SIGIR Proceedings, pages 449?456.Vishwa Vinay, Ingemar J. Cox, Natasa Milic-Frayling, and KenWood.
2006.
On ranking the effectiveness of searches.
In29th ACM SIGIR Proceedings, pages 398?404.Ellen M. Voorhees.
2006.
The TREC 2005 robust track.
SIGIRForum, 40(1):41?48.Ryen W. White, Joemon M. Jose, and Ian Ruthven.
2005.
Us-ing top-ranking sentences to facilitate effective informationaccess: Book reviews.
JAIST, 56(10):1113?1125.227
