Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 891?896,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsDiscriminative Improvements to Distributional Sentence SimilarityYangfeng JiSchool of Interactive ComputingGeorgia Institute of Technologyjiyfeng@gatech.eduJacob EisensteinSchool of Interactive ComputingGeorgia Institute of Technologyjacobe@gatech.eduAbstractMatrix and tensor factorization have been ap-plied to a number of semantic relatednesstasks, including paraphrase identification.
Thekey idea is that similarity in the latent spaceimplies semantic relatedness.
We describethree ways in which labeled data can im-prove the accuracy of these approaches onparaphrase classification.
First, we designa new discriminative term-weighting metriccalled TF-KLD, which outperforms TF-IDF.Next, we show that using the latent repre-sentation from matrix factorization as featuresin a classification algorithm substantially im-proves accuracy.
Finally, we combine latentfeatures with fine-grained n-gram overlap fea-tures, yielding performance that is 3% moreaccurate than the prior state-of-the-art.1 IntroductionMeasuring the semantic similarity of short unitsof text is fundamental to many natural languageprocessing tasks, from evaluating machine transla-tion (Kauchak and Barzilay, 2006) to grouping re-dundant event mentions in social media (Petrovic?et al 2010).
The task is challenging because ofthe infinitely diverse set of possible linguistic real-izations for any idea (Bhagat and Hovy, 2013), andbecause of the short length of individual sentences,which means that standard bag-of-words representa-tions will be hopelessly sparse.Distributional methods address this problem bytransforming the high-dimensional bag-of-wordsrepresentation into a lower-dimensional latent space.This can be accomplished by factoring a matrixor tensor of term-context counts (Turney and Pan-tel, 2010); proximity in the induced latent spacehas been shown to correlate with semantic similar-ity (Mihalcea et al 2006).
However, factoring theterm-context matrix means throwing away a consid-erable amount of information, as the original ma-trix of size M ?N (number of instances by numberof features) is factored into two smaller matrices ofsize M ?K and N ?K, with K  M,N .
If thefactorization does not take into account labeled dataabout semantic similarity, important information canbe lost.In this paper, we show how labeled data can con-siderably improve distributional methods for mea-suring semantic similarity.
First, we develop anew discriminative term-weighting metric calledTF-KLD, which is applied to the term-context ma-trix before factorization.
On a standard paraphraseidentification task (Dolan et al 2004), this methodimproves on both traditional TF-IDF and WeightedTextual Matrix Factorization (WTMF; Guo andDiab, 2012).
Next, we convert the latent repre-sentations of each sentence pair into a feature vec-tor, which is used as input to a linear SVM clas-sifier.
This yields further improvements and sub-stantially outperforms the current state-of-the-arton paraphrase classification.
We then add ?fine-grained?
features about the lexical similarity of thesentence pair.
The combination of latent and fine-grained features yields further improvements in ac-curacy, demonstrating that these feature sets providecomplementary information on semantic similarity.8912 Related WorkWithout attempting to do justice to the entire lit-erature on paraphrase identification, we note threehigh-level approaches: (1) string similarity metricssuch as n-gram overlap and BLEU score (Wan etal., 2006; Madnani et al 2012), as well as stringkernels (Bu et al 2012); (2) syntactic operationson the parse structure (Wu, 2005; Das and Smith,2009); and (3) distributional methods, such as la-tent semantic analysis (LSA; Landauer et al 1998),which are most relevant to our work.
One appli-cation of distributional techniques is to replace in-dividual words with distributionally similar alterna-tives (Kauchak and Barzilay, 2006).
Alternatively,Blacoe and Lapata (2012) show that latent word rep-resentations can be combined with simple element-wise operations to identify the semantic similarityof larger units of text.
Socher et al(2011) pro-pose a syntactically-informed approach to combineword representations, using a recursive auto-encoderto propagate meaning through the parse tree.We take a different approach: rather than repre-senting the meanings of individual words, we di-rectly obtain a distributional representation for theentire sentence.
This is inspired by Mihalcea et al(2006) and Guo and Diab (2012), who treat sen-tences as pseudo-documents in an LSA framework,and identify paraphrases using similarity in the la-tent space.
We show that the performance of suchtechniques can be improved dramatically by usingsupervised information to (1) reweight the individ-ual distributional features and (2) learn the impor-tance of each latent dimension.3 Discriminative feature weightingDistributional representations (Turney and Pantel,2010) can be induced from a co-occurrence ma-trix W ?
RM?N , where M is the number of in-stances and N is the number of distributional fea-tures.
For paraphrase identification, each instanceis a sentence; features may be unigrams, or mayinclude higher-order n-grams or dependency pairs.By decomposing the matrix W, we hope to obtaina latent representation in which semantically-relatedsentences are similar.
Singular value decomposition(SVD) is traditionally used to perform this factoriza-tion.
However, recent work has demonstrated the ro-bustness of nonnegative matrix factorization (NMF;Lee and Seung, 2001) for text mining tasks (Xu etal., 2003; Arora et al 2012); the difference fromSVD is the addition of a non-negativity constraintin the latent representation based on non-orthogonalbasis.WhileW may simply contain counts of distribu-tional features, prior work has demonstrated the util-ity of reweighting these counts (Turney and Pantel,2010).
TF-IDF is a standard approach, as the inversedocument frequency (IDF) term increases the impor-tance of rare words, which may be more discrimi-native.
Guo and Diab (2012) show that applying aspecial weight to unseen words can further improve-ment performance on paraphrase identification.We present a new weighting scheme, TF-KLD,based on supervised information.
The key idea isto increase the weights of distributional features thatare discriminative, and to decrease the weights offeatures that are not.
Conceptually, this is similarto Linear Discriminant Analysis, a supervised fea-ture weighting scheme for continuous data (Murphy,2012).More formally, we assume labeled sentence pairsof the form ?~w(1)i , ~w(2)i , ri?, where ~w(1)i is the bi-narized vector of distributional features for the firstsentence, ~w(2)i is the binarized vector of distribu-tional features for the second sentence, and ri ?
{0, 1} indicates whether they are labeled as a para-phrase pair.
Assuming the order of the sentenceswithin the pair is irrelevant, then for k-th distribu-tional feature, we define two Bernoulli distributions:?
pk = P (w(1)ik |w(2)ik = 1, ri = 1).
This is theprobability that sentence w(1)i contains featurek, given that k appears in w(2)i and the two sen-tences are labeled as paraphrases, ri = 1.?
qk = P (w(1)ik |w(2)ik = 1, ri = 0).
This is theprobability that sentence w(1)i contains featurek, given that k appears in w(2)i and the two sen-tences are labeled as not paraphrases, ri = 0.The Kullback-Leibler divergence KL(pk||qk) =?x pk(x) logpk(x)qk(x)is then a measure of the discrim-inability of feature k, and is guaranteed to be non-8920.0 0.2 0.4 0.6 0.8 1.0pk0.00.20.40.60.81.01?qkneithernornotfearsamebutoffsharesstudythen0.2000.2000.4000.4000.6000.6000.800 0.8001.0001.000Figure 1: Conditional probabilities for a few hand-selected unigram features, with lines showing contourswith identical KL-divergence.
The probabilities are es-timated based on the MSRPC training set (Dolan et al2004).negative.1 We use this divergence to reweight thefeatures in W before performing the matrix factor-ization.
This has the effect of increasing the weightsof features whose likelihood of appearing in a pairof sentences is strongly influenced by the paraphraserelationship between the two sentences.
On the otherhand, if pk = qk, then the KL-divergence will bezero, and the feature will be ignored in the ma-trix factorization.
We name this weighting schemeTF-KLD, since it includes the term frequency andthe KL-divergence.Taking the unigram feature not as an example, wehave pk = [0.66, 0.34] and qk = [0.31, 0.69], for aKL-divergence of 0.25: the likelihood of this wordbeing shared between two sentence is strongly de-pendent on whether the sentences are paraphrases.In contrast, the feature then has pk = [0.33, 0.67]and qk = [0.32, 0.68], for a KL-divergence of 3.9?10?4.
Figure 1 shows the distributions of these andother unigram features with respect to pk and 1?qk.The diagonal line running through the middle of theplot indicates zero KL-divergence, so features onthis line will be ignored.1We obtain very similar results with the opposite divergenceKL(qk||pk).
However, the symmetric Jensen-Shannon diver-gence performs poorly.1 unigram recall2 unigram precision3 bigram recall4 bigram precision5 dependency relation recall6 dependency relation precision7 BLEU recall8 BLEU precision9 Difference of sentence length10 Tree-editing distanceTable 1: Fine-grained features for paraphrase classifica-tion, selected from prior work (Wan et al 2006).4 Supervised classificationWhile previous work has performed paraphrase clas-sification using distance or similarity in the latentspace (Guo and Diab, 2012; Socher et al 2011),more direct supervision can be applied.
Specifically,we convert the latent representations of a pair of sen-tences ~v1 and ~v2 into a sample vector,~s(~v1, ~v2) =[~v1 + ~v2, |~v1 ?
~v2|], (1)concatenating the element-wise sum ~v1 +~v2 and ab-solute difference |~v1 ?
~v2|.
Note that ~s(?, ?)
is sym-metric, since ~s(~v1, ~v2) = ~s(~v2, ~v1).
Given this rep-resentation, we can use any supervised classificationalgorithm.A further advantage of treating paraphrase as asupervised classification problem is that we can ap-ply additional features besides the latent represen-tation.
We consider a subset of features identifiedby Wan et al(2006), listed in Table 1.
These fea-tures mainly capture fine-grained similarity betweensentences, for example by counting specific unigramand bigram overlap.5 ExperimentsOur experiments test the utility of the TF-KLD weighting towards paraphrase classification,using the Microsoft Research Paraphrase Corpus(Dolan et al 2004).
The training set contains 2753true paraphrase pairs and 1323 false paraphrasepairs; the test set contains 1147 and 578 pairs, re-spectively.The TF-KLD weights are constructed from onlythe training set, while matrix factorizations are per-893formed on the entire corpus.
Matrix factorization onboth training and (unlabeled) test data can be viewedas a form of transductive learning (Gammerman etal., 1998), where we assume access to unlabeled testset instances.2 We also consider an inductive setting,where we construct the basis of the latent space fromonly the training set, and then project the test setonto this basis to find the corresponding latent rep-resentation.
The performance differences betweenthe transductive and inductive settings were gener-ally between 0.5% and 1%, as noted in detail be-low.
We reiterate that the TF-KLD weights are nevercomputed from test set data.Prior work on this dataset is described in sec-tion 2.
To our knowledge, the current state-of-the-art is a supervised system that combines several ma-chine translation metrics (Madnani et al 2012), butwe also compare with state-of-the-art unsupervisedmatrix factorization work (Guo and Diab, 2012).5.1 Similarity-based classificationIn the first experiment, we predict whether a pairof sentences is a paraphrase by measuring their co-sine similarity in latent space, using a threshold forthe classification boundary.
As in prior work (Guoand Diab, 2012), the threshold is tuned on held-outtraining data.
We consider two distributional featuresets: FEAT1, which includes unigrams; and FEAT2,which also includes bigrams and unlabeled depen-dency pairs obtained from MaltParser (Nivre et al2007).
To compare with Guo and Diab (2012), weset the latent dimensionality to K = 100, which wasthe same in their paper.
Both SVD and NMF factor-ization are evaluated; in both cases, we minimize theFrobenius norm of the reconstruction error.Table 2 compares the accuracy of a num-ber of different configurations.
The transductiveTF-KLD weighting yields the best overall accu-racy, achieving 72.75% when combined with non-negative matrix factorization.
While NMF performsslightly better than SVD in both comparisons, themajor difference is the performance of discrimina-tive TF-KLD weighting, which outperforms TF-IDFregardless of the factorization technique.
When we2Another example of transductive learning in NLP iswhen Turian et al(2010) induced word representations from acorpus that included both training and test data for their down-stream named entity recognition task.50 100 150 200 250 300 350 400K6065707580Accuracy(%)Feat1_TF-IDF_SVMFeat2_TF-IDF_SVMFeat1_TF-KLD_SVMFeat2_TF-KLD_SVMFigure 2: Accuracy of feature and weighting combina-tions in the classification framework.perform the matrix factorization on only the trainingdata, the accuracy on the test set is 73.58%, with F1score 80.55%.5.2 Supervised classificationNext, we apply supervised classification, construct-ing sample vectors from the latent representation asshown in Equation 1.
For classification, we choosea Support Vector Machine with a linear kernel (Fanet al 2008), leaving a thorough comparison of clas-sifiers for future work.
The classifier parameter C istuned on a development set comprising 20% of theoriginal training set.Figure 2 presents results for a range of latent di-mensionalities.
Supervised learning identifies theimportant dimensions in the latent space, yieldingsignificantly better performance that the similarity-based classification from the previous experiment.In Table 3, we compare against prior publishedwork, using the held-out development set to selectthe best value of K (again, K = 400).
The bestresult is from TF-KLD, with distributional featuresFEAT2, achieving 79.76% accuracy and 85.87% F1.This is well beyond all known prior results on thistask.
When we induce the latent basis from onlythe training data, we get 78.55% on accuracy and84.59% F1, also better than the previous state-of-art.Finally, we augment the distributional represen-tation, concatenating the ten ?fine-grained?
fea-tures in Table 1 to the sample vectors describedin Equation 1.
As shown in Table 3, the accu-894Factorization Feature set Weighting K Measure Accuracy (%) F1SVD unigrams TF-IDF 100 cosine sim.
68.92 80.33NMF unigrams TF-IDF 100 cosine sim.
68.96 80.14WTMF unigrams TF-IDF 100 cosine sim.
71.51 not reportedSVD unigrams TF-KLD 100 cosine sim.
72.23 81.19NMF unigrams TF-KLD 100 cosine sim.
72.75 81.48Table 2: Similarity-based paraphrase identification accuracy.
Results for WTMF are reprinted from the paper by Guoand Diab (2012).Acc.
F1Most common class 66.5 79.9(Wan et al 2006) 75.6 83.0(Das and Smith, 2009) 73.9 82.3(Das and Smith, 2009) with 18 features 76.1 82.7(Bu et al 2012) 76.3 not reported(Socher et al 2011) 76.8 83.6(Madnani et al 2012) 77.4 84.1FEAT2, TF-KLD, SVM 79.76 85.87FEAT2, TF-KLD, SVM, Fine-grained features 80.41 85.96Table 3: Supervised classification.
Results from prior work are reprinted.racy now improves to 80.41%, with an F1 score of85.96%.
When the latent representation is inducedfrom only the training data, the corresponding re-sults are 79.94% on accuracy and 85.36% F1, againbetter than the previous state-of-the-art.
These re-sults show that the information captured by the dis-tributional representation can still be augmented bymore fine-grained traditional features.6 ConclusionWe have presented three ways in which labeleddata can improve distributional measures of seman-tic similarity at the sentence level.
The main innova-tion is TF-KLD, which discriminatively reweightsthe distributional features before factorization, sothat discriminability impacts the induction of the la-tent representation.
We then transform the latentrepresentation into a sample vector for supervisedlearning, obtaining results that strongly outperformthe prior state-of-the-art; adding fine-grained lexi-cal features further increases performance.
Theseideas may have applicability in other semantic sim-ilarity tasks, and we are also eager to apply them tonew, large-scale automatically-induced paraphrasecorpora (Ganitkevitch et al 2013).AcknowledgmentsWe thank the reviewers for their helpful feedback,and Weiwei Guo for quickly answering questionsabout his implementation.
This research was sup-ported by a Google Faculty Research Award to thesecond author.ReferencesSanjeev Arora, Rong Ge, and Ankur Moitra.
2012.Learning Topic Models - Going beyond SVD.
InFOCS, pages 1?10.Rahul Bhagat and Eduard Hovy.
2013.
What Is a Para-phrase?
Computational Linguistics.William Blacoe and Mirella Lapata.
2012.
A Com-parison of Vector-based Representations for Seman-tic Composition.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 546?556, Stroudsburg, PA,USA.
Association for Computational Linguistics.Fan Bu, Hang Li, and Xiaoyan Zhu.
2012.
String Re-writing kernel.
In Proceedings of ACL, pages 449?458.
Association for Computational Linguistics.Dipanjan Das and Noah A. Smith.
2009.
Paraphraseidentification as probabilistic quasi-synchronousrecognition.
In Proceedings of the Joint Conference895of the Annual Meeting of the Association for Com-putational Linguistics and the International JointConference on Natural Language Processing, pages468?476, Stroudsburg, PA, USA.
Association forComputational Linguistics.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised Construction of Large Paraphrase Corpora:Exploiting Massively Parallel News Sources.
In COL-ING.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A Li-brary for Large Linear Classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Alexander Gammerman, Volodya Vovk, and VladimirVapnik.
1998.
Learning by transduction.
In Proceed-ings of the Fourteenth conference on Uncertainty inartificial intelligence, pages 148?155.
Morgan Kauf-mann Publishers Inc.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The ParaphraseDatabase.
In Proceedings of NAACL, pages 758?764.Association for Computational Linguistics.Weiwei Guo and Mona Diab.
2012.
Modeling Sentencesin the Latent Space.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Linguis-tics, pages 864?872, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.David Kauchak and Regina Barzilay.
2006.
Para-phrasing for automatic evaluation.
In Proceedingsof NAACL, pages 455?462.
Association for Computa-tional Linguistics.Thomas Landauer, Peter W. Foltz, and Darrel Laham.1998.
Introduction to Latent Semantic Analysis.
Dis-cource Processes, 25:259?284.Daniel D. Lee and H. Sebastian Seung.
2001.
Al-gorithms for Non-Negative Matrix Factorization.
InAdvances in Neural Information Processing Systems(NIPS).Nitin Madnani, Joel R. Tetreault, and Martin Chodorow.2012.
Re-examining Machine Translation Metrics forParaphrase Identification.
In HLT-NAACL, pages 182?190.
The Association for Computational Linguistics.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In AAAI.Kevin P. Murphy.
2012.
Machine Learning: A Proba-bilistic Perspective.
The MIT Press.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007.
MaltParser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.2010.
Streaming first story detection with applicationto twitter.
In Proceedings of HLT-NAACL, pages 181?189.
Association for Computational Linguistics.Richard Socher, Eric H. Huang, Jeffrey Pennington, An-drew Y. Ng, and Christopher D. Manning.
2011.Dynamic Pooling And Unfolding Recursive Autoen-coders For Paraphrase Detection.
In Advances in Neu-ral Information Processing Systems (NIPS).Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word Representation: A Simple and General Methodfor Semi-Supervised Learning.
In ACL, pages 384?394.Peter D. Turney and Patrick Pantel.
2010.
From Fre-quency to Meaning: Vector Space Models of Seman-tics.
JAIR, 37:141?188.Ssephen Wan, Mark Dras, Robert Dale, and Cecile Paris.2006.
Using Dependency-based Features to Take the?Para-farce?
out of Paraphrase.
In Proceedings of theAustralasian Language Technology Workshop.Dekai Wu.
2005.
Recognizing paraphrases and textualentailment using inversion transduction grammars.
InProceedings of the ACL Workshop on Empirical Mod-eling of Semantic Equivalence and Entailment, pages25?30.
Association for Computational Linguistics.Wei Xu, Xin Liu, and Yihong Gong.
2003.
DocumentClustering based on Non-Negative Matrix Factoriza-tion.
In SIGIR, pages 267?273.896
