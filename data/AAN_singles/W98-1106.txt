An Empirical Approach to Conceptual Case Frame AcquisitionEllen Riloff and Mark SchmelzenbachDepartment of Computer ScienceUniversity of UtahSalt Lake City, UT 84112riloff@cs.utah.edu, schmelzeQcs.utah.eduAbstractConceptual natural language processing systemsusually rely on case frame instantiation to recog-nize events and role objects in text.
But generat-ing a good set of case frames for a domain is time-consuming, tedious, and prone to errors of omission.We have developed a corpus-based algorithm foracquiring conceptual case frames empirically fromunannotated text.
Our algorithm builds on previ-ous research on corpus-based methods for acquiringextraction patterns and semantic lexicons.
Givenextraction patterns and a semantic lexicon for a do-main, our algorithm learns semantic preferences foreach extraction pattern and merges the syntacti-cally compatible patterns to produce multi-slot caseframes with selectional restrictions.
The case framesgenerate more cohesive output and produce fewerfalse hits than the original extraction patterns.
Oursystem requires only preclassified training texts anda few hours of manual review to filter the dictionar-ies, demonstrating that conceptual case frames canbe acquired from unannotated text without specialtraining resources.1 MotivationConceptual natural anguage processing typically in-volves case frame instantiation to recognize ventsand role objects in text.
For example, an NLP sys-tem designed for a business domain might use caseframes to recognize business activities uch as merg-ers, acquisitions, or joint ventures.
The case frameswould contain slots for thematic roles that are asso-ciated with each event.
For example, case framesfor business activities might contain slots for theagents (e.g., companies or people who merge or ac-quire others) and the objects (e.g., companies thatare acquired or products that are being developed).Unfortunately, acquiring a good set of case framesfor a domain can be a major undertaking.
Caseframes are often lexically indexed so that each caseframe is tailored for a specific set of linguistic expres-sions and their expectations.
For example, one caseframe might be activated by the phrase '~oint ven-ture" and contain slots to recognize the partner corn -49panies and objects of the joint venture (e.g., childcompany or product).
A different case frame mightbe activated by the word "acquisition ~'and containslots to recognize the agent (e.g., the acquiring com-pany or person) and the object of the acquisition.Devising the right set of role assignments for a caseframe can be surprisingly difficult.
Determining thenecessary thematic roles for an event is relativelystraightforward, but anticipating how they will bemanifested syntactically can be tricky.
For example,consider some of the manually defined case framesthat were used to recognize terrorist events in theUMass MUC-4 system (Lehnert et al, 1992a).ATTACK (passive-verb "attacked")Victim = subjectTarget = subjectPerpetrator = pp(by)Ins t rument= pp(by)ACCUSATION (active-verb "blamed")Accuser = subjectPerpetrator = direct objectPerpetrator = pp(on)SABOTAGE (noun "sabotage")Perpetrator = pp(by)Instrument = pp(with)Location = pp(on)Victim = pp(against), pp(of), pp(on)Target = pp(against), pp(of), pp(on)The ATTACK case frame shows a very commonsituation where multiple conceptual roles map to thesame syntactic role.
When "attacked" is used as apassive verb, the subject may be either a victim ora physical target, and the object of the preposition"by" may be the agent or instrument.
It is easy for aperson to miss one of these possibilities when defin-ing the case frame manually.
The ACCUSATIONcase frame shows that the same conceptual role canbe filled by multiple syntactic roles.
For example,the person accused of a crime may be the directobject of "blamed" (e.g., "The government blamedJohn Smith for the crime") or may be the object ofthe preposition "on" (e.g., "The government blamedthe crime on John Smith").
The SABOTAGE caseframe illustrates that a multitude of prepositionalarguments may be necessary for some case frames.Prepositional arguments are especially difficult for aperson to anticipate when defining case frames byhand.It is virtually impossible for a person to correctlyand completely anticipate all of the arguments thatare necessary for a large set of case frames for adomain.
Omitting an important argument will re-sult in the failure to recognize role objects in cer-tain syntactic onstructions.
In practice, people of-ten turn to the corpus to look for argument struc-tures that they might have missed.
For example,the UMass/MUC-4 terrorism case frames were de-veloped by applying an initial set of case framesto hundreds of sample texts and looking for placeswhere the case frames failed to recognize desiredinformation.
But this approach is extremely time-consuming unless the answers are known in advance(i.e., the information that should have been ex-tracted), which is unrealistic for most applications.It should be possible, however, to learn case framestructures automaticallyfrom a text corpus.
Towardthis end, we have been developing a corpus-b~edapproach to conceptual case frame acquisition.
Ourapproach builds upon earlier work on corpus-basedmethods for generating extraction patterns (Riloff,1996b) and semantic lexicons (Riloff and Shepherd,1997).
Our new system constructs conceptual caseframes by learning semantic preferences for extrac-tion patterns and merging syntactically compatiblepatterns into more complex structures.
The result-ing case frames can have slots for multiple role ob-jects and each slot has a set of learned selectionalrestrictions for its role object.The first section of this paper begins with back-ground about AutoSlog-TS, a corpus-based systemfor generating extraction patterns automatically,and the extraction patterns that it generates.
Thefollowing section presents a new corpus-based algo-rithm that uses the extraction patterns as a build-ing block for constructing conceptual case framestructures.
We then show several examples of caseframes that were generated automatically using thismethod.
Finally, we present experimental resultsthat compare the performance of the case frameswith the extraction patterns.
Our results show thatthe conceptual case frames produce substantiallyfewer false hits than the extraction patterns.2 AutoS log-TS :  generat ing  s impleext ract ion  pat ternsIn the past few years, several systems have been de-veloped to generate structures for information ex-traction automatically.
However, these systems usu-ally need special training resources that are expen-sive to obtain.
One of the first such systems was Au-toSlog (Riloff, 1993; Riloff, 1996a), which generatesextraction patterns from annotated text.
The pat-terns produced by AutoSlog achieved 98% of the per-formance of hand-crafted extraction patterns, butAutoSlog requires a training corpus that is manuallytagged with domain-specific annotations.
Anotherearly system, PALKA (Kim and Moldovan, 1993),requires domain-specific frames with keyword lists,CRYSTAL (Soderland et al, 1995) requires an anno-tated training corpus, RAPIER (Califf and Mooney,1997) requires filled templates, and LIEP (Huffman,1996) requires keywords and annotated training ex-amples.
PALKA and CRYSTAL also require seman-tic lexicons, while LIEP uses domain-specific con-cept recognizers.AutoSlog-TS (Riloff, 1996b) is a derivative of Au-toSlog that was designed to obviate the need for spe-cial training data.
AutoSlog-TS generates extrac-tion patterns using only a "preclassified" trainingcorpus: one set of texts that are relevant o the do-main, and one set of texts that are irrelevant.
Thetexts do not need to be annotated in any way.AutoSlog-TS generates the same simple extractionpatterns that AutoSlog generates.
Each pattern isactivated by a keyword in a specific linguistic con-text.
For example, one extraction pattern may betriggered by the word "murdered" in passive verbconstructions, while a different extraction patternmay be triggered by "murdered" in active verb con-structions.
Each pattern extracts information froma syntactic constituent in the current clause: thesubject, the direct object, or a prepositional phrase.
"AutoSlog-TS generates extraction patterns bymaking two passes over the corpus.
In the firstpass, AutoSlog-TS uses AutoSlog's heuristics in anexhaustive fashion to generate a set of patterns thatcollectively extract every noun phrase in the cor-pus.
In the second pass, AutoSlog-TS computesstatistics to determine which extraction patterns aremost strongly correlated with the relevant rainingtexts.
The patterns are ranked so that those moststrongly associated with the domain appear at thetop.
Figure 1 shows the top 20 extraction patternsproduced by AutoSlog-TS for the MUC-4 terrorismdomain (MUC-4 Proceedings, 1992).
The rankedlist is then presented to a human to decide whichpatterns hould be kept.
For example, the pattern"<subject:> exploded" should be retained because itis likely to extract relevant information about bomb-ings.
However, the pattern "<subject> said" shouldbe discarded because it is not likely to extract infor-mation about terrorism and will probably extracta lot of irrelevant information.
The human reviewerassigns a conceptual role to each accepted pattern tocharacterize its extractions.
For example, the pat-tern "<subject> was murdered" would be assigned50the role victim for its extractions.1.2.3.4.5.6.7.8.9.i0.i i.12.13.14.15.16.17.18.19.20.<subject> exploded<subject> reported<subject> was killed<subject> located<subject> took_place<subject> was kidnapped<subject> was injured<subject> carried_outcaused <direct-obj><subject> was wounded<subject> caused<subject> occurredclaimed <direct-obj ><subject> was murderedmurder of <noun-phrase><subject> claimed responsibility<subject> was reported<subject> saidexploded in <noun-phrase><subject> kidnappedFigure 1: Top 20 extraction patterns for a terrorismdomainThe extraction patterns learned by AutoSlog-TS(and AutoSlog) have two serious limitations.
First,each pattern extracts only one item, which causesthe output to be artificially fragmented.
For exam-ple, the sentence "Guerrillas kidnapped the mayor inBogota" produces three extractions (Guerrillas, themayor, and Bogota), each in a separate structure.This fragmented representation causes unnecessarywork for subsequent components that need to piecethe information back together.
Second, the patternsdo not include semantic onstraints o they producemany spurious extractionsJTheoretically, conceptual case frames hould over-come both of these limitations.
Multi-slot caseframes will allow several role objects associated withthe same event to be instantiated as part of the samestructure.
This produces a more coherent represen-tation, which is more natural for subsequent event ordiscourse processing.
Furthermore, if each slot hasselectional restrictions associated with its legal roleobjects, then the case frames should produce fewerfalse hits (i.e., spurious extractions).In the next section, we describe a corpus-based al-gorithm that constructs conceptual case frames em-pirically by learning semantic preferences for eachextraction pattern and using these preferences to as-sign conceptual roles automatically.
(Consequently,the human reviewer no longer needs to assign roles tothe extraction patterns manually.)
Extraction pat-terns with compatible syntactic onstraints are then1Semantic onstraints could be associated with the con-ceptual roles assigned by the human reviewer, but our goal isto assign both the conceptual roles and selectional restrictionsautomatically.51merged to produce multi-slot case frames with se-lectional restrictions.
The conceptual case framesshould be more reliable at identifying relevant infor-mation (our experimental results support this hy-pothesis), and the case frames can instantiate mul-tiple role objects in a single structure to simplifysubsequent discourse processing.3 Generat ing  conceptua l  case  f ramesf rom ext rac t ion  pat ternsThe algorithm for building conceptual case framesbegins with extraction patterns and a semantic lex-icon for the domain.
The semantic lexicon is a dic-tionary of words that belong to relevant semanticcategories.
We used AutoSlog-TS to generate theextraction patterns and a corpus-based algorithm togenerate the semantic lexicon.
~The corpus-based algorithm that we used to buildthe semantic lexicon (Riloff and Shepherd, 1997) re-quires five "seed words" as input for each semanticcategory, and produces a ranked list of words thatare statistically associated with each category.
First,the algorithm looks for all sentences in Khich a seedword is used as the head noun of a noun phrase.For each such occurrence of a seed word, the algo-rithm collects a small context window around theseed word.
The context window consists of the clos-est noun to the left of the seed word, and the clos-est noun to its right.
The context windows for allseed words that belong to the same category arethen combined, and each word is assigned a cate-gory score.
The category score is (essentially) theconditional probability that the word appears in acategory context.
The words are ranked by this scoreand the top five are dynamically added to the seedword list.
This bootstrapping process dynamicallygrows the seed word list so that each iteration pro-duces a larger category context.
After several itera-tions, the final list of ranked words usually containsmany words that belong to the category, especiallynear the top.
The ranked list is presented to a user,who scans down the list and removes any words thatdo not belong to the category.
For more details ofthis algorithm, see (Riloff and Shepherd, 1997).A flowchart for the case frame generation processappears in Figure 2.
AutoSlog-TS produces a rankedlist of extraction patterns and our semantic lexicongenerator produces a ranked list of words for eachcategory.
Generating these lists is fully automatic,but a human must review them to decide which ex-traction patterns and category words to keep.
Thisis the only part of the process that involves humaninteraction.~Other methods could be used to generate these items,including the use of existing knowledge bases such as Word-Net (Miller, 1990) or Cyc (Lenat et al, 1986) if they haveadequate coverage for the domain.seedwordsranked extractionN~ / ranked  categorypatterns ~ ~ wordsextract ion~semanticpatterns lexiconSemantic Preference \]Generator \]extraction patternswith semantic profileslist ofdomain "-'-~ \[ Role Assignment \]roles "expanded extractionpatterns\[ Pattern Merging \]Conceptual Case FramesFigure 2: Generating case framesNext, the extraction patterns are applied to thetexts to generate a semantic profile for each pattern.The semantic profile shows the semantic ategoriesthat were extracted by each pattern, based on thehead noun of each extraction.
Figure 3 shows thesemantic profile for the pattern "attack on <noun-phrase>".
PFreq  is the number of times that theextraction pattern fired, SFreq is the number oftimes that the pattern extracted the given seman-tic category, and Prob  is the estimated probabilityof the pattern extracting the given semantic ate-gory (SFreq/PFreq) .
Note that many extractionswill not be labeled with any semantic category ifthe head noun is unknown (i.e., not in the semanticlexicon).Figure 3 shows that attacks are often carried outon buildings, civilians, dates, government officials,locations, military people, and vehicles.
It seemsobvious that attacks will occur on people and onphysical targets, but a person might not realize thatattacks will also occur on dates (e.g., Monday) andon locations (e.g., a neighborhood).
This exampleshows how the corpus-based approach can identifysemantic preferences that a person might not antic-ipate.
Also, note that the semantic profile shows noinstances of attacks on terrorists or weapons, whichmakes sense in this domain.52Sem.
Category PFreq SFreq ProbBUILDING 149 15 0.I0CIVILIAN 149 5 0.03DATE 149 7 0.05GOVOFFICIAL 149 4 0.03LOCATION 149 4 0.03MILITARYPEOPLE 149 13 0.09TERRORIST 149 0 0.00VEHICLE 149 4 0.03WEAPON 149 0 0.00Figure 3: Semantic profile for "attack on <noun-phrase>"The semantic profile is used to select semanticpreferences that are strong enough to become se-lectional restrictions.
We use the following formulato identify strong semantic preferences:(SFreq > FI) or ((SFreq ~ F2) and (Prob > P))The first test selects semantic categories that areextracted with high frequency, under the assumptionthat this reflects a real association with the cate-gory.
The second case selects semantic categoriesthat represent a relatively high percentage of theextractions even though the frequency might be low(e.g., 2 out of 4 extractions).
In our experiments, wechose F1=3, F2=2, and P=0.1.
We used fairly le-nient criteria because (a) patterns can often extractseveral types of objects that belong to different se-mantic categories, and (b) many extractions containunknown words.
Also, remember that the seman-tic lexicon is reliable because it was reviewed by aperson, so it is usually meaningful when a patternextracts a semantic category even once.
The thresh-oIds are needed only to eliminate noise, which can becaused by misparsed sentences or polysemous words.The semantic preferences are used to assign con-ceptual roles to each extraction pattern.
At thispoint, one additional piece of input is needed: alist of conceptual roles and associated semantic cate-gories for the domain.
The conceptual roles identifythe types of information that need to be recognized.Figure 4 shows the conceptual roles used for the ter-rorism domain.Domain Role Semantic CategoriesPerpetrator TERRORISTTarget BUILDING, VEHICLEVictim CIVILIAN, GOVOFFICIALLocation LOCATIONInstrument WEAPONDate TIMEFigure 4: Conceptual roles for terrorismEach extraction pattern is expanded to include aset of conceptual roles based on its semantic prefer-ences.
These conceptual roles are assigned automat-ically based on a pattern's emantic profile.
Thisprocess eliminates the need for a human to assignroles to the extraction patterns by hand, as had beennecessary when using AutoSlog or AutoSlog-TS bythemselves.For example, the pattern "machinegunned<direct-obj>" had strong semantic preferences forBUILDING, CIVILIAN, LOCATION, and VEHICLE, so itwas expanded to have three conceptual roles withfour selectional restrictions.
The expanded extrac-tion pattern for "machinegunned <direct-obj>" is:"machinegunned <direct-obj>" .-+Victim CIVILIANTarget BUILDING VEHICLELocation LOCATIONOnly semantic categories that were associatedwith a pattern are included as selectional restric-tions.
For example, the GOVOFFICIAL category alsorepresents possible terrorism victims, but it was notstrongly associated with the pattern.
Our rationaleis that an individual pattern may have a strong pref-erence for only a subset of the categories that canbe associated with a role.
For example, the pattern"<subject> was ambushed" showed a preference .forVEHICLE extractions but not BUILDING extractions,which makes ense because it is hard to imagine am-bushing a building.
Including only VEHICLE as itsselectional restriction for targets might help elimi-nate incorrect building extractions.
One could ar-gue that this pattern is not likely to find buildingextractions anyway so the selectional restriction willnot matter, but the selectional restriction might helpfilter out incorrect extractions due to misparses ormetaphor (e.g., "The White House was ambushed byreporters.").
Ultimately, it is an empirical questionwhether it is better to include all of the semanticcategories associated with a conceptual role or not.Finally, we merge the expanded extraction pat-terns into multi-slot case frames.
All extraction pat-terns that share the same trigger word and compat-ible syntactic constraints are merged into a singlestructure.
For example, we would merge all patternstriggered by a specific verb in its passive voice.
Forexample, the patterns "<subject> was kidnapped","was kidnapped by <noun-phrase>", and "was kid-napped in <noun-phrase>" would be merged into asingle case frame.
Similarly, we would merge all pat-terns triggered by a specific verb in its active voice.For example, we would merge patterns for the ac-tive form of "destroyed" that extract the subject of"destroyed", its direct object, and any prepositionalphrases that are associated with it.
We also mergesyntactically compatible patterns that are triggeredby the same noun (e.g., "assassination") or by thesame infinitive verb structure (e.g., "to kill").
Whenwe merge extraction patterns into a case frame, allof the slots are simply unioned together.4 ExamplesIn this section, we show several examples of caseframes that were generated automatically by our sys-tem.
Figure 5 shows a simple case frame triggered byactive forms of the verb "ambushed".
The subjectis extracted as a perpetrator and has a selectionalrestriction of TERRORIST.
The direct object is ex-tracted as a target and has a selectional restriction ofVEHICLE.
Note that the case frame does not containa victim slot, even though it is theoretically possibleto ambush people.
During training, the "ambushed<direct-obj>" pattern extracted 13 people, 11 ofwhom were recognized as MILITARYPEOPLE.
Sinceour domain roles only list civilians and governmentofficials as legitimate terrorism victims 3, a victimslot was not created.
This example shows how thecase frames are tailored for the domain empirically.Caseframe: (active_verb ambushed)perpetrator subject TER.RORISTtarget direct-obj VEHICLEFigure 5: Case frame for active forms of "ambushed"Figure 6 shows a case frame triggered by activeforms of "blew_up" .4 This case frame extracts infor-mation from an entire sentence into a single struc-ture.
The subject (perpetrator), direct object (tar-get), and a prepositional phrase (in location) will allbe extracted together.Caseframe: (active_verb blew_up)perpetrator subject TERRORISTtarget d.irect-obj BUILDING VEHICLElocation pp(in) LOCATIONFigure 6: Case frame for active forms of "blew_up"The case frame in Figure 7 illustrates how a se-mantic category can show up in multiple places.This case frame will handle phrases like "the guer-rillas detonated a bomb", as well as "the bomb det-onated".
Both constructions are very common inthe training corpus so the system added slots forboth possibilities.
It would be easy for a human tooverlook some of these variations when creating caseframes by hand.The case frame in Figure 8 is activated by thenoun "attack" and includes slots for a variety ofprepositional phrases.
The same preposition can rec-ognize different ypes of information (e.g., "on" canrecognize targets, victims, locations, and dates).
Andthe same role can be filled by different prepositions3Events involving military victims were classified as mil-itary incidents, not terrorism, according to the MUCG-4guidelines.4Underscored words represent lexicalized expressions inour phrasal lexicon.53Caseframe: (active_verb detonated)perpetrator subject TERRORISTinstrument subject WEAPONinstrument direct-obj WEAPONFigure 7: Case frame for active forms of "detonated"(e.g., targets can be extracted from "on", "against",or "at").
This example again shows the power ofcorpus-based methods to identify common construc-tions empirically.
Anticipating all of these prepo-sitional arguments would be difficult for a person.Caseframe: (noun attack)target pp(on) BUILDING VEHICLEvictim pp(on) CIVILIAN GOVOFFICIALlocation pp(on) LOCATIONdate pp(on) TIMEtarget pp(against) BUILDING VEHICLEvictim pp(agalnst) CIVILIANtarget pp(at) BUILDINGlocation pp(at) LOCATIONFigure 8: Case frame for noun forms of "attack"A disadvantage of this automated method is thatinappropriate slots sometimes end up in the caseframes.
For example, Figure 9 shows a case framethat is activated by passive forms of the verb"killed".
Some of the slots are correct: the sub-ject is assigned to the victim slot and objects of thepreposition "by" are assigned to the perpetrator andinstrument slots.
However, the remaining slots donot make sense.
The location slot is the result ofpol-ysemy; many person names are also location names,such as "Flores'.
The date slot was produced by in-correct parses of date expressions.
The perpetrator(subject) and victim (pp (by)) slots were caused byincorrect role assignments.
The list of domain rolesassumes that terrorists are always perpetrators andcivilians are always victims, but of course this is nottrue.
Terrorists can be killed and civilians can bekillers.Caseframe: (passive_verb killed)victim subject CIVILIAN GOVOFFICIALperpetrator subject TERRORISTlocation subject  LOCATIONdate subject TIMEperpetrator pp(by) TERIq.OBISTvictim pp(by) CIVILIANinstrument pp(by) WEAPONFigure 9: Case frame for passive forms of "killed"The previous example illustrates ome of the prob-lems that can occur when generating case frames au-tomatically.
Currently, we are assuming that eachsemantic ategory will be uniquely associated with54a conceptual role, which may be an unrealistic as-sumption for some domains.
One avenue for futurework is to develop more sophisticated methods formapping semantic preferences to conceptual roles.One could also have a human review the case framesand manually remove inappropriate slots.
For now,we chose to avoid additional human interaction andused the case frames exactly as they were generated.5 EvaluationThe purpose of the selectional restrictions is to con-strain the types of information that can be instan-tiated by each slot.
Consequently, we hoped thatthe case frames would be more reliably instanti-ated than the extraction patterns, thereby produc-ing fewer false hits.
To evaluate the case frames,we used the same corpus and evaluation metrics asprevious experiments with AutoSlog and AutoSlog-TS (Riloff, 1996b) so that we can draw comparisonsbetween them.
For training, we used the 1500 MUC-4 development texts to generate the extraction pat-terns and the semantic lexicon.
AutoSlog-TS gener-ated 44,013 extraction patterns in its first pass.
Af-ter discarding the patterns that occurred only once,the remaining 11,517 patterns were applied to thecorpus for the second pass and ranked for manualreview.
We reviewed the top 2168 patterns 5 andkept 306 extraction patterns for the final dictionary.We built a semantic lexicon for nine categories as-sociated with terrorism: BUILDING, CIVILIAN, GOV-OFFICIAL, MILITARYPEOPLE, LOCATION, TERROR-IST~ DATEs VEHICLE, WEAPON.
We reviewed thetop 500 words for each category.
It takes about 30m~nutes to review a category assuming that the re-viewer is familiar with the domain.
Our final seman-tic dictionary contained 494 words.
In total, the re-view process required approximately 6 person-hours:1.5 hours to review the extraction patterns plus 4.5hours to review the words for 9 semantic ategories.From the extraction patterns and semantic lexicon,our system generated 137 conceptual case frames.One important question is how to deal with un-known words during extraction.
This is especiallyimportant in the terrorism domain because many ofthe extracted items are proper names, which can-not be expected to be in the semantic lexicon.
Weallowed unknown words to fill all eligible slots andthen used a precedence scheme so that each item wasinstantiated by only one slot.
Precedence was basedon the order of the roles shown in Figure 4.
This isnot a very satisfying solution and one of the weak-nesses of our current approach.
Handling unknownwords more intelligently is an important directionfor future research.We compared AutoSlog-TS' extraction patternsSWe decided to review the top 2000 but continued clownthe list until there were no more ties.S lo t  co t  mis  mlb dup  spu  R P, Perp  25 31 10 18 84 .45 .31V ic t im 44 23 16 24 62 .66 .47, ' , Target  31 22 17 23 66 .58 .39I Instr 16 15 l 7 17 23 .52 .52I Total  116 91 ~ 50 82 235 .56 .41Table 1: AutoSlog-TS resultswith the case frames using 100 blind texts from theMUC-4 test set.
The MUC-4 answer keys were usedto score the output.
Each extracted item was scoredas either correct, mislabeled, duplicate, or spurious.An item was correct if it matched against he answerkeys.
An item was mislabeled if it matched againstthe answer keys but was extracted as the wrong typeof object (e.g., if a victim was extracted as a perpe-trator).
An item was a duplicate if it was coreferentwith an item in the answer keys.
Correct items ex-tracted more than once were scored as duplicates, aswell as correct but underspecified extractions such as"Kennedy" instead of "John F. Kennedy" r An itemwas spurious if it did not appear in the answer keys.All items extracted from irrelevant exts were spuri-ous.
Finally, items in the answer keys that were notextracted were counted as missing.
Correct + miss-ing equals the total number of items in the answerkeys.STable 1 shows the results 9 for AutoSlog-TS' ex-traction patterns, and Table 2 shows the results forthe case frames.
We computed Recal l  (R)  as cor-rect / (correct + missing), and Prec is ion (P)  as(correct + duplicate) / (correct + duplicate + misla-beled + spurious).
The extraction patterns and caseframes achieved similar recall results, although thecase frames missed seven correct extractions.
How-ever the case frames produced substantially fewerfalse hits, producing 82 fewer spurious extractions.Note that perpetrators exhibited by far the low-est precision.
The reason is that the perpetratorslot received highest precedence among competingslots for unknown words.
Changing the precedences25 relevant texts and 25 i r re levant  exts  from each of theTST3 and TST4 test sets.7The ra t iona le  for scoring coreferent phrases  as duplicatesinstead of spurious is that the extraction pattern or case framewas instantiated with  a reference to the correct answer.
Inother words, the pattern (or case f rame)  did the r ight  th ing .Resolving coreferent phrases to produce the best answer is aproblem for subsequent discourse analys is ,  which is not ad-dressed by the work presented here.SA caveat is that the MUC-4 answer keys contain some"opt iona l"  answers .
We scored these as correct if they wereextracted but they were never scored as missing,  which ishow the "optional" items were scored in MUC-4.
Note thatthe number of possible extractions can vary  depending on theoutput of the system.9We reimplemented AutoSlog-TS to  use a different sen-tence analyzer, so these results are  s l ight ly  di f ferent f romthose reported in (Riloff, 1996b).55S lo t  cor  mis  mlb  dup  spu  lq.
!
PiPerp  26 30 4 17 71 .46 .36V ic t im 38 28 24 12 26 .58 .50Target  28 25 3 29 48 .53 .53Ins t r  : 17 14 2 19 8 .55 .78Tota l  109 97 33 77 153 .53 .50Table 2: Case frame resultsscheme produces a bubble effect where many incor-rect extractions hift to the primary default cate-gory.
The case frames therefore have the potentialfor even higher precision if the unknown words arehandled better.
Expanding the semantic lexicon isone option, and additional work may suggest waysto choose slots for unknown words more intelligently.6 ConclusionsWe have shown that conceptual case frames canbe generated automatically using unannotated textas input, coupled with a few hours of manual re-view.
Our results for the terrorism domain showthat the case frames achieve similar recall levels asthe extraction patterns, but with substantially fewerfalse hits.
Our results are not directly comparableto the MUC-4 results because the MUC-4 systemscontained additional components, uch as domain-specific discourse analyzers that resolved coreferentnoun phrases, merged event descriptions, and ill-tered out irrelevant information.
The work pre-sented here only addresses the initial stage of in-formation extraction.
However, in previous workwe showed that AutoSlog-TS achieved performancecomparable to AutoSlog (Riloff, 1996b), which per-formed very well in the MUC-4 evaluation (Lehn-ert et al, 1992b).
Since the conceptual case framesachieved comparable r call and higher precision thanAutoSlog-TS' extraction patterns, our results sug-gest that the case frames performed well relative toprevious work on this domain.Several other systems learn extraction patternsthat can also be viewed as conceptual case frameswith selectional restrictions (e.g., PALKA (Kim andMoldovan, 1993) and CRYSTAL (Soderland et al,1995)).
The case frames learned by our system arenot necessarily more powerful then those generatedby other systems.
The advantage of our approachis that it requires no special training resources.
Ourtechnique requires only preclassified training textsand a few hours of manual filtering to build the in-termediate dictionaries.
Given preclassified texts, itis possible to build a dictionary of conceptual caseframes for a new domain in one day.Another advantage of our approach is its highlyempirical nature; a corpus often reveals importantpatterns in a domain that are not necessarily in-tuitive to people.
By using corpus-based methodsto generate all of the intermediate dictionaries andthe final case frame structures, the most importantwords, role assignments, and semantic prefe:rencesare less likely to be missed.
Our empirical approachaims to exploit the text corpus to automatically ac-quire the syntactic and semantic role assignmentsthat are necessary to achieve good performalace inthe domain.ReferencesM.
E. Califf and R. J. Mooney.
1997.
Relational Learn-ing of Pattern-Match Rules for Information Extrac-tion.
In Proceedings of the A CL Workshop on NaturalLanguage Learning, pages 9-.-15.S.
Hu.ffman.
1996.
Learning information extraction pat-terns from examples.
In Stefan Wermter, Ellen Riloff,and Gabfiele Scheler, editors, Connectionist, Statisti-cal, and Symbolic Approaches to Learning for NaturalLanguage Processing, pages 246-.-260.
Springer-Verlag,Berlin.J.
Kim and D. Moldovan.
1993.
Acquisition of SemanticPatterns for Information Extraction from Corpora.
InProceedings of the Ninth IEEE Conference on Artifi-cial Intelligence for Applications, pages 171-176, LosAlamitos, CA.
IEEE Computer Society Press.W.
Lehnert, C. Cardie, D. Fisher, J. McCarthy,E.
Rilott, and S. Soderland.
1992a.
University of Mas-sachusetts: Description of the CIRCUS System asUsed for MUC-4.
In Proceedings of the Fourth Mes-sage Understanding Conference (MUG-4), pages 282-288, San Mateo, CA.
Morgan Kaufmann.W.
Lehnert, C. Cardie, D. Fisher, J. McCarthy,E.
Riloff, and S. Soderland.
1992b.
University of Mas-sachusetts: MUC-4 Test Results and Analysis.
In Pro-ceedings of the Fourth Message Understanding Confer-ence (MUG-4), pages 151-158, San Mateo, CA.
Mor-gan Kaufmann.D.
B. Lenat, M. Prakash, and M. Shepherd.
1986.
CYC:Using Common Sense Knowledge to Overcome Brit-tleness and Knowledge-Acquisition Bottlenecks.
AIMagazine, 6:65---85.G.
Miller.
1990.
Wordnet: An On-fine Lex.ical Database.International Journal of Lexicography, 3(4).MUC-4 Proceedings.
1992.
Proceedings of the FourthMessage Understanding Conference (MUC-$).
Mor-gan Kaufmarm, San Mateo, CA.E.
Riloff and J. Shepherd.
1997.
A Corpus-Ba.sed Ap-proach for Building Semantic Lexicons.
In Proceed-ings of the Second Conference on Empirical Methodsin Natural Language Processing, pages 117-124.E.
Riloff.
1993.
Automatically Constructing a Dictio-nary for Information Extraction Tasks.
In Proceed-ings of the Eleventh National Conference on ArtificialIntelligence, pages 811-816.
AAAI Press/The MITPre.~.E.
Riloff.
1996a.
An Empirical Study of AutomatedDictionary Construction for Information Extraction inThree Domains.
Artificial Intelligence, 85:101-134.E.
Riloff.
1996b.
Automatically Generating ExtractionPatterns from Untagged Text.
In Proceedings of theThirteenth National Conference on Artificial Intelli-gence, pages 1044-1049.
The AAAI Press/MIT Press.56S.
Soderland, D. Fisher, J. Aseltine, and W. Lehnert.1995.
CRYSTAL: Inducing a conceptual dictionary.In Proceedings of the Fourteenth International JointConference on Artificial Intelligence, pages 1314-1319.
