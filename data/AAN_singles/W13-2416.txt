Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 110?118,Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational LinguisticsSemi-automatic Acquisition of Lexical Resources and Grammars forEvent Extraction in Bulgarian and CzechHristo TanevJoint Research CentreEuropean Commissionvia Fermi 2749, IspraItalyhristo.tanev@jrc.ec.europa.euJosef SteinbergerUniversity of West BohemiaFaculty of Applied SciencesDepartment of Computer Science and EngineeringNTIS Centre Univerzini 8, 30614 PlzenCzech Republicjstein@kiv.zcu.czAbstractIn this paper we present a semi-automaticapproach for acqusition of lexico-syntacticknowledge for event extraction in twoSlavic languages, namely Bulgarian andCzech.
The method uses several weakly-supervised and unsupervised algorithms,based on distributional semantics.
More-over, an intervention from a language ex-pert is envisaged on different steps in thelearning procedure, which increases its ac-curacy, with respect to unsupervised meth-ods for lexical and grammar learning.1 IntroductionAutomatic detection and extraction of events fromonline news provide means for tracking the devel-opments in the World politics, economy and otherimportant areas of life.Event extraction is a branch of information ex-traction, whose goal is the automatic retrieval ofstructured information about events described innatural language texts.
Events include interac-tions among different entities, to each of whichan event-specific semantic role can be assigned.This role reflects the way in which the entity par-ticipates in the event and interacts with the otherentities.
For example, in the fragment ?Three peo-ple were injured in a building collapse?, the phrase?three people?
may be assigned a semantic roleinjured ?
victim.
The list of semantic roles de-pends on the adopted event model.The event extraction technology may decreasethe information overload, it allows automatic con-version of unstructured text data into structuredone, it can be used to pinpoint interesting news ar-ticles, also extracted entities and their correspond-ing semantic roles can provide brief summaries ofthe articles.Using lexico-syntactic knowledge is one ofthe promising directions in modeling the event-specific semantic roles (Hogenboom et al 2011).While for English linear patterns seem to workquite well (Tanev et al 2008), for other lan-guages,where word ordering is more free, cas-caded grammars proved to improve the results(Zavarella et al 2008).
In particular, Slavic lan-guages are more free-order than English; conse-quently, using cascaded grammars may be consid-ered a relevant approach.In this paper we present an ongoing effortto build event extraction cascaded grammars forBulgarian and Czech in the domain of violentnews.
To achieve this goal we put forward asemi-automatic approach for building of event ex-traction grammars, which uses several weakly-supervised algorithms for acquisition of lexicalknowledge, based on distributional semantics andclustering.
Moreover, the lexical knowledge islearned in the form of semantic classes, which thencan be used as a basis for building of a domain-specific ontology.To the best of our knowledge, there are noprevious attempts to perform event extraction forSlavic languages, apart from the work presented in(Turchi et al 2011).The importance of Czech and Bulgarian lan-guages comes from the geopolitical positions ofthe countries where they are spoken: Czech Re-public is in a central geographical position be-tween Eastern and Western Europe; Bulgaria is onthe borders of the European Union, on a crossroadbetween Europe and Asia, surrounded by differentcultures, languages and religions.
These geopo-litical factors contribute to the importance of thenews from Czech Republic and Bulgaria and con-sequently make automatic event extraction fromthese news an useful technology for political an-alysts.The paper has the following structure: In sec-tion 2 we make a short overview of the related ap-110proaches; in section 3 we describe our method forlexical and grammar learning; section 4 presentsour experiments and evaluation for Bulgarian andCzech languages and section 5 discusses the out-come of the experiments and some future direc-tions.2 Related WorkThere are different approaches for event extrac-tion.
Most of the work up to now has aimedat English (see among the others (Naughton etal., 2006) and (Yangarber et al 2000)), however(Turchi et al 2011) presented automatic learningof event extraction patterns for Russian, Englishand Italian.Our work is based on weakly supervised algo-rithms for learning of semantic classes and pat-terns, presented in (Tanev et al 2009) and (Tanevand Zavarella, 2013); these approaches are basedon distributional semantics.
There are differentother methods which use this paradigm: A con-cept and pattern learning Web agent, called NELL(Never Ending Language Learning) is presented in(Carlson et al 2010).
Parallel learning of seman-tic classes and patterns was presented in (Riloffand Jones, 1999).
However these approaches donot try to derive grammars from the acquired re-sources, but stop at purely lexical level.Relevant to our approach are the grammar learn-ing approaches.
A survey of supervised and unsu-pervised approaches is presented in (D?Ulizia etal., 2011).
The supervised ones require annotationof big amounts of data which makes the develop-ment process long and laborious.
On the otherhand, unsupervised methods try to generalize allthe training data by using different heuristics likethe minimal description length.
Since for eventextraction only specific parts of the text are ana-lyzed, in order to use unsupervised grammar ac-quisition methods for learning of event extractiongrammars, one should collect the exact phraseswhich describe the events.
In practice, this wouldtransform the unsupervised methods into super-vised ones.
With respect to the state-of-the artgrammar inference approaches, our method allowsfor more interaction between the grammar expertand the learning system.
Moreover, our learningstarts from lexical items and not from annotatedtexts, which decreases the development efforts.3 Semi-automatic Learning of Lexicaand GrammarsThe event extraction grammar, exploited in our ap-proach is a cascaded grammar which on the firstlevels detects references to entities, like people,groups of people, vehicles, etc.
On the upper lev-els our cascaded grammar detects certain eventsin which these entities participate: In the domainof violent news, people may get killed, wounded,kidnapped, arrested, etc.
If we consider as an ex-ample the following Bulgarian text: ?????
???-?????????
????
??????????
?????
??
???????
????????????
?
???????
??
????????
?(?A group of protesters were arrested yesterdayduring demonstrations in the centre of the capi-tal?
), our grammar will detect first that ????????????????
? (?A group of protesters?)
refersto a group of people and then, it will find that?????
????????????
????
??????????'?
(?Agroup of protesters were arrested?)
refers to an ar-rest event where the aforementioned group of peo-ple is assigned the semantic role arrested.In order to build such a grammar, we acquiresemi-automatically the following resources:1. a dictionary of words which refer to peo-ple and other entities in the required domain-specific context, e.g.
?????
? , ?voja?k?
(?soldier?
in Bulgarian and Czech), ???
? ,zena ( ?woman?
in Bulgarian and Czech),etc.2.
a list of modifiers and other words whichappear in phrases referring to those entities,e.g.
??????
? , ?civiln???
(?civil?
in Bulgar-ian and Czech), ???
? (?NATO?
), etc.3.
grammar rules for parsing entity-referringphrases.
For example, a simple rule can be:PERSON PHRASE ?
PERconnector ORGwhere PER and ORG are words and multi-words, referring to people and organizations,connector ?
?
? for Bulgarian orconnector ?
??
(empty string) for Czech.This rule can parse phrases like ??????
?????
? or ?voja?k NATO?
(?NATO soldier?)4.
a list of words which participate in eventpatterns like ????????
? , ?zadrz?en?
(?ar-rested?
in Bulgarian and Czech) or ???
? ,?zabit?
( ?killed?
in Bulgarian and Czech).1115. a set of grammar rules which parse event-description phrases.
For example, a simplerule can be:KILLING ?
PER connectorKILLED PARTICIPLEwhere connector ?
???
? for Bulgarianor connector ?
byl for Czech.This rule will recognize phrases like ???-???
??
????
????
???
? or ?Voja?kNATO byl zabit?
(?A NATO soldier waskilled?
in Bulgarian and Czech?
)In order to acquire this type of domain lexicaand a grammar, we make use of a semi-automaticmethod which acquires in parallel grammar rulesand dictionaries.
Our method exploits severalstate-of-the-art algorithms for expanding of se-mantic classes, distributional clustering, learningof patterns and learning of modifiers, described in(Tanev and Zavarella, 2013).
The semantic classexpansion algorithm was presented also in (Tanevet al 2009).
These algorithms are multilingial andall of them are based on distributional semantics.They use a non-annotated text corpus for training.We integrated these algorithms in a semi-automatic schema for grammar learning, which isstill in phase of development.
Here is the basicschema of the approach:1.
The user provides a small list of seed words,which designate people or other domain-specific entities, e.g.?
soldiers?,?civilians?,?fighters?
(We will use only English-language examples for short, however themethod is multilingual and consequently ap-plicable for Czech and Bulgarian).2.
Using the multilingual semantic class ex-pansion algorithm (Tanev et al 2009)other words are learned (e.g.
?policemen?,?women?, etc.
), which are likely to belongto the same semantic class.
First, the algo-rithm finds typical contextual patterns for theseed words from not annotated text.
For ex-ample, all the words, referring to people tendto appear in linear patterns like [PEOPLE]were killed, thousands of [PEOPLE] , [PEO-PLE] are responsible, etc.
Then, other wordswhich tend to participatre in the same con-textual patterns are extracted from the unan-notated text corpus.
In such a way the al-gorithm learns additional words like ?police-men?, ?killers?, ?terrorists?, ?women?, ?chil-dren?, etc.3.
Since automatic approaches for learning ofsemantic classes always return some noisein the output, a manual cleaning by a do-main expert takes place as a next step of ourmethod.4.
Learning modifiers: At this step, for each se-mantic class learned at the previous step (e.g.PEOPLE, we run the modifier learning algo-rithm, put forward by (Tanev and Zavarella,2013) , which learns domain-specific syn-tactic modifiers.
Regarding the class PEO-PLE), the modifiers will be words like ?Russian?, ?American?, ?armed?, ?unarmed?,?masked?, etc.
The modifier learning algo-rithm exploits the principle that the contextdistribution of words from a semantic classis most likely similar to the context distribu-tion of these words with syntactic modifiersattached.
The algorithm uses this heuristicand does not use any morphological infor-mation to ensure applications in multilingualsettings.5.
Manual cleaning of the modifier list6.
Adding the following grammar rule at thefirst level of the cascaded grammar, whichuses the semantic classes and modifiers,learned at the previous steps:Entity(class : C) ?
(LModif(class :C))?
Word(class : C) (RModif(class :C))?This rule parses phrases, like ?masked gun-men from IRA?, referring to an entity froma semantic class C, e.g.
PERSON.
It shouldconsist of a sequence of 0 or more left mod-ifiers for this class, e.g.
?masked?, a wordfrom this class (?gunmen?
in this example)and a sequence of 0 or more right modifiers(?from IRA?
in the example?).7.
Modifiers learned by the modifier learningalgorithm do not cover all the variations inthe structure of the entity-referring phrases,since sometimes the structure is more com-plex and cannot be encoded through a list oflexical patterns.
Consider, for example, thefollowing phrase ?soldiers from the specialforces of the Russian Navy?.
There is a little112chance that our modifier learning algorithmacquires the string ?from the special forcesof the Russian Navy?, on the other handthe following two grammar rules can do theparsing:RIGHT PEOPLE MODIFIER ??from?
?MILITARY FORMATIONMILITARY FORMATION ?LeftModMF ?
MFW RightModMF?where MILITARY FORMATION is aphrase which refers to some organization (inthe example, shown above, the phrase is ?thespecial forces of the Russian Navy?
), MFWis a term which refers to a military formation(?the special forces?)
and LeftModMF andRightModMF are left and right modifiersof the military formation entity (for example,a right modifier is?of the Russian Navy?
).In order to learn such more complex struc-ture, we propose the following procedure:(a) The linguistic expert chooses seman-tic classes, for which more elaboratedgrammar rules should be developed.Let?s take for example the class PEO-PLE.
(b) Using the context learning sub-algorithm of the semantic class expan-sion, used in step 2, we find contextualpatterns which tend to co-occur withthis class.
Apart from the patternsshown in step 2, we also learn patternslike [PEOPLE] from the special forces,[PEOPLE] from the Marines, [PEO-PLE] from the Russian Federation,[PEOPLE] from the Czech Republic,[PEOPLE] with guns, [PEOPLE] withknives, [PEOPLE] with masks, etc.
(c) We generalize contextual patterns, in or-der to create grammar rules.
In the firststep we create automatically syntacticclusters separately for left and rightcontextual patterns.
Syntactic clusteringputs in one cluster patterns where theslot and the content-bearing words areconnected by the same sequence of stopwords.
In the example, shown above,we will have two syntactic clusters ofpatterns: The first consists of patternswhich begin with [PEOPLE] from theand the second contains the patterns,which start with [PEOPLE] with.
Theseclusters can be represented via grammarrules in the following way:RIGHT PEOPLE MODIFIER ??fromthe?
XX?
(special forces | Marines | RussianFederation | Czech Republic)RIGHT PEOPLE MODIFIER ??with?
YY?
(knives | guns | masks)(d) Now, several operations can be donewith the clusters of words inside thegrammar rules:?
Words inside a cluster can be clus-tered further on the basis of theirsemantics.
In our system we usebottom up agglomerative cluster-ing, where each word is representedas a vector of its context features.Manual cleaning and merging ofthe clusters may be necessary af-ter this automatic process.
If wordsare not many, only manual clus-tering can also be an option.
Inthe example above ?special forces?and ?Marines?
may form one clus-ter, since both words designate theclass MILITARY FORMATION andthe other two words designate coun-tries and also form a separate seman-tic class.?
In the grammar introduce new non-terminal symbols, corresponding tothe newly learnt semantic classes.Then, in the grammar rules substi-tute lists of words with referencesto these symbols.
(Still we domodification of the grammar rulesmanually, however we envisage toautomate this process in the future).For example, the ruleX ?
(special forces | Marines| Russian Federation | Czech Re-public)will be transformed intoX ?
(MILITARY FORMATION |COUNTRY)MILITARY FORMATION ?
(spe-cial forces | Marines)COUNTRY ?
(Russian Federation113PEOPLE?
(NUMBER ??
(from) )?
PEOPLEaExample: ?????
??
???????????
??????
? (?two of the Bulgarian soldiers?)PEOPLEa?
PEOPLEb ((??
(from) | ??
(of) | ?
(in)) (ORG | PLACE ))*Example: ?????????
??
??
? (?staff from the MVR (Ministry of the Internal Affairs)?)PEOPLEb?
LeftPM* PEOPLE W RightPM*Example: ??????????
??????????
?
??????
? (?unknown attackers with hoods?
)Table 1: Rules for entity recognition for the Bulgarian language| Czech Republic)?
Clusters can be expanded by usingthe semantic class expansion algo-rithm, introduced before, followedby manual cleaning.
In our example,this will add other words for MIL-ITARY FORMATION and COUN-TRY.
Consequently, the range of thephrases, parsable by the grammarrules will be augmented.
(e) The linguistic expert may choose a sub-set of the semantic classes, obtainedon the previous step, (e.g.
the the se-mantic class MILITARY FORMATION)to be modeled further via extending thegrammar with rules about their left andright modifiers.
Then, the semantic classis recursively passed to the input of thisgrammar learning procedure.8.
Learning event patterns: In this step we learnpatterns like [PEOPLE] ????
?????????
?or [PEOPLE] ?byl zadrz?en?
([PEOPLE]were/was arrested in Bulgarian and Czech).The pattern learning algorithm collects con-text patterns for one of the considered en-tity categories (e.g.
[PEOPLE].
This is donethrough the context learning sub-algorithmdescribed in step 2.
Then, it searches forsuch context patterns, which contain words,having distributional similarity to words, de-scribing the target event (e.g.
?????????
? ,?zadrz?en?
(?arrested?
)).For example, if we want to learn patterns forarrest events in Bulgarian, the algorithm firstlearns contexts of [PEOPLE].
These con-texts are [PEOPLE] ????
?????
([PEO-PLE] were killed), ??????
[PEOPLE](thousands of [PEOPLE]), [PEOPLE] ????????????
([PEOPLE] were captured), etc.Then, we pass to the semantic expansion al-gorithm (see step 2) seed words which ex-press the event arrest, namely ?????????,?????????
? (?apprehended?, ?arrested?),etc.
Then, it will discover other similar wordslike ???????
? (?captured?).
Finally, thealgorithm searches such contextual patterns,which contain any of the seed and learntwords.
For example, the pattern [PEOPLE]????
????????
([PEOPLE] were captured)is one of the newly learnt patterns for arrestevents.9.
Generalizing the patterns: In this step we ap-ply a generalization algorithm, described instep 7 to learn grammar rules which parseevents.
For example, two of the learned rulesfor parsing of arrest events in Bulgarian are:ARREST ?
PEOPLE ???
? (?were?
)ARREST PARTICIPLEARREST PARTICIPLE ?
( ?????????
?(arrested) | ???????
?(captured) |????????
? (handcuffed) )The outcome of this learning schema is a gram-mar and dictionaries which recognize descriptionsof different types of domain-specific entities andevents, which happened with these entities.
More-over, the dictionaries describe semantic classesfrom the target domain and can be used further forcreation of a domain ontology.4 Experiments and EvaluationIn our experiments, we applied the procedureshown above to learn grammars and dictionariesfor parsing of phrases, referring to people, groupsof people and violent events in Bulgarian andCzech news.
We used for training 1 million newstitles for Bulgarian and Czech, downloaded from114KILLING?
KILL VERB (a (and) | i (and) | jeden (one) | jeden z (one of) )?
[PEOPLE]KILL VERB?
(zabit (killed) | zabila | zahynul (died) | zabiti | ubodal (stabbed) | ubodala | ...)KILLING?
KILL ADJ [PEOPLE]KILL ADJ?
(mrtvou (dead) | mrtve?ho (dead) | ...)KILLING?
[PEOPLE] KILL VERBaKILL VERBa?
(zahynul (died) | zamr?el (died) | ...)KILLING?
[PEOPLE] byl (was) KILL VERBbKILL VERBb?
(zabit (killed) | ...)Table 2: Rules for parsing of killing events and their victims in Czechthe Web and a small number of seed terms, refer-ring to people and actions.
We had more availabletime to work for the Bulgarian language, that iswhy we learned more complex grammar for Bul-garian.
Both for Czech and Bulgarian, we learnedgrammar rules parsing event description phraseswith one participating entity, which is a person ora group of people.
This is simplification, since of-ten an event contains more than one participant,in such cases our grammar can detect the separatephrases with their corresponding participants, butcurrently it is out of the scope of the grammar toconnect these entities.
The event detection rulesin our grammar are divided into semantic classes,where each class of rules detects specific type ofevents like arrest, killing, wounding, etc.
and alsoassigns an event specific semantic role to the par-ticipating entity, e.g.
victim, perpetrator, arrested,kidnapped.In order to implement our grammars, we usedthe EXPRESS grammar engine (Piskorski, 2007).It is a tool for building of cascaded grammarswhere specific parts of the parsed phrase are as-signed semantic roles.
We used this last feature ofEXPRESS to assign semantic roles of the partici-pating person entities.For Czech we learned a grammar which de-tects killings and their victims.
For Bulgarian, welearned a grammar, which parses phrases referringto killings, woundings and their victims, arrestsand who is arrested, kidnappings and other violentevents with their perpetrators and targeted people.4.1 Learning people-recognition rulesFor Czech our entity extraction grammar was rel-atively simple, since we learned just a dictionaryof left modifiers.
Therefore, we skipped step 7 inthe learning schema, via which more elaboratedentity recognition grammars are learned.
Thus,the Czech grammar for recognizing phrases,referring to people contains the following rules:PEOPLE?
LeftMod* PEOPLE TERMLeftMod ?
(?mladou?
(?young?)
|?nezna?me?mu?(?unknown?)
| ?stars????
(?old?)
|...)PEOPLE TERM ?
(?voja?ci?
(?soldiers?)
|?civiliste??(?civilians?)
| ?z?enu?
(?woman?)
|...)This grammar recognizes phrases like ?mladouz?enu?
(?young woman?
in Czech).
Two dictionar-ies were acquired in the learning process: A dic-tionary of nouns, referring to people and left mod-ifiers of people.
The dictionary of people-referringnouns contains 268 entries, obtained as a resultof the semantic class expansion algorithm.
Weused as a seed set 17 words like ?muz?i?
(?men?),?voia?ci?
(?soldiers?
), etc.
The algorithm learned1009 new words and bigrams, 251 of which werecorrect (25%), that is refer to people.
One problemhere was that not all morphological forms werelearned by our class expansion algorithm.
In alanguage with rich noun morphology, as Czech is,this influenced on the coverage of our dictionaries.After manual cleaning of the output from themodifier learning algorithm, we obtained 603terms; the learning accuracy of the algorithm wasfound to be 55% .For Bulgarian we learned a more elaboratedpeople recognition grammar, which is able toparse more complex phrases like ????
??
?????-??????
?????????
? (?one of the masked attack-ers?)
and ?????
??
??????????
??????????
????
? (?soldiers from the Bulgarian contingentin Iraq?).
The most important rules which welearned are shown in Table 1.
In these rules PEO-PLE W encodes a noun or a bigram which refersto people, ORG is an organization; we learnedmostly organizations, related to the domain of se-curity, such as different types of military and otherarmed formations like ??????
??
???
? (?secu-115rity forces?
), also governmental organizations, etc.PLACE stands for names of places and commonnouns, referring to places such as ????????
?(?the capital?).
We also learned modifiers for thesecategories and added them to the grammar.
(Forsimplicity, we do not show the grammar rules forparsing ORG abd PLACE; we will just mentionthat both types of phrases are allowed to have a se-quence of left modifiers, one or more nouns fromthe corresponding class and a sequence of 0 ormore right modifiers.)
Both categories PLACEand ORG were obtained in step 7 of the learn-ing schema, when exploring the clusters of wordswhich appear as modifiers after the nouns, refer-ring to people, like in the following example ???-??
??
??????????
?????????
? (?soldiers fromthe Bulgarian contingent?
); then, we applied man-ual unification of the clusters and their subsequentexpansion, using the semantic class expansion al-gorithm.Regarding the semantic class expansion, with20 seed terms we acquired around 2100 terms,from which we manually filtered the wrong onesand we left 1200 correct terms, referring to peo-ple; the accuracy of the algorithm was found to be57% in this case.We learned 1723 nouns for organizations and523 place names and common nouns.
We did nottrack the accuracy of the learning for these twoclasses.
We also learned 319 relevant modifiersfor people-referring phrases; the accuracy of themodifier learning algorithm was found to be 67%for this task.4.2 Learning of event detection rulesThis learning takes place in step 8 and 9 ofour learning schema.
As it was explained, firstlinear patterns like [PEOPLE] ?byl zadrz?en?
([PEOPLE] was arrested ) are learned, thenthrough a semi-automatic generalization processthese patterns are transformed into rules like:ARREST?
PEOPLE ?byl?
ARREST VERBIn our experiments for Czech we learned gram-mar rules and a dictionary which recognize dif-ferent syntactic constructions, expressing killingevents and the victims.
These rules encode 156event patterns.
The most important of these rulesare shown in Table 2.
Part of the event rule learn-ing process is expansion of a seed set of verbs, andother words, referring to the considered event (inthis case killing).For this task the semantic classexpansion algorithm showed significantly loweraccuracy with respect to expanding sets of nouns -only 5%.
Nevertheless, the algorithm learned 54Czech words, expressing killing and death.For Bulgarian we learned rules for detection ofkilling and its victims, but also rules for parsing ofwounding events, arrests, targeting of people in vi-olent events, kidnapping, and perpetrators of vio-lent events.
These rules encode 605 event patterns.Some of the rules are shown in Table 3.4.3 Evaluation of event extractionIn order to evaluate the performance of our gram-mars, we created two types of corpora: For theprecision evaluation we created bigger corpus ofrandomly picked excerpts of news from Bulgar-ian and Czech online news sources.
More pre-cisely, we used 7?550 news titles for Czech and12?850 news titles in Bulgarian.
We also car-ried out a preliminary recall evaluation on a verysmall text collection: We manually chose sen-tences which report about violent events of thetypes which our grammars are able to capture.
Weselected 17 sentences for Czech and 28 for Bul-garian.
We parsed the corpora with our EXPRESSgrammars and evaluated the correctness of the ex-tracted events.
Since each event rule has assignedan event type and a semantic role for the partic-ipating people reference, we considered a correctmatch only when both a correct event type and acorrect semantic role are assigned to the matchedtext fragment.
Table 4 shows the results from ourevaluation.
The low recall in Czech was mostlydue to the insufficient lexicon for people and thetoo simplistic grammar.Language Precision RecallBulgarian 93% 39%Czech 88% 6%Table 4: Event extraction accuracy5 DiscussionIn this paper we presented a semi-automatic ap-proach for learning of grammar and lexical knowl-edge from unannotated text corpora.
The methodis multilingual and relies on distributional ap-proaches for semantic clustering and class expan-sion.116KILLING?
KILL VERB (????
(were) | ??
(are)) [PEOPLE]KILL VERB?
(????????
(killed) | ?????
(killed) | ????????????
(shot to death) | ...)KILLING?
KILL PHRASE ??
(of) [PEOPLE]KILL PHRASE?
(????
??????
(took the life) | ???????
???????
(caused the death) | ...)WOUNDING?
WOUND VERB (????
(were) | ??
(are)) [PEOPLE]WOUND VERB?
(??????
(wounded) | ????????????
(injured) | ...)ARREST?
[PEOPLE] ARREST VERBARREST VERB?
(??????????
(arrested) | ?????????
(detained) | ...)Table 3: Some event parsing rules for BulgarianWe are currently developing event extractiongrammars for Czech and Bulgarian.
Preliminaryevaluation shows promising results for the preci-sion, while the recall is still quite low.
One ofthe factors which influences the law recall wasthe insufficient number of different morphologicalword variations in the learned dictionaries.
Themorphological richness of Slavic languages can beconsidered by adding morphological dictionariesto the system or creating an automatic procedurewhich detects the most common endings of thenouns and other words and expands the dictionar-ies with morphological forms.Another problem in the processing of theSlavic languages is their relatively free order.To cope with that, often the grammar engineershould introduce additional variants of alreadylearned grammar rules.
This can be done semi-automatically, where the system may suggest ad-ditional rules to the grammar developer.
This canbe done through development of grammar meta-rules.With respect to other approaches, grammarsprovide transparent, easy to expand model of thedomain.
The automatically learned grammars canbe corrected and extended manually with hand-crafted rules and linguistic resources, such as mor-phological dictionaries.
Moreover, one can tryto introduce grammar rules from already existinggrammars.
This, of course, is not trivial because ofthe different formalisms exploited by each gram-mar.
It is noteworthy that the extracted semanticclasses can be used to create an ontology of thedomain.
In this clue, parallel learning of a domain-specific grammars and ontologies could be an in-teresting direction for future research.The manual efforts in the development of thegrammars and the lexical resources were mainlycleaning of already generated lists of words andmanual selection and unification of word clus-ters.
Although we did not evaluate precisely theinvested manual efforts, one can estimate themby the size of the automatically acquired wordlists and their accuracy, given in section Semi-automatic Learning of Lexica and Grammars.We plan to expand the Czech grammar withrules for more event types.
Also, we think to ex-tend both the Bulgarian and the Czech event ex-traction grammars and the lexical resources, sothat it will be possible to detect also disasters, hu-manitarian crises and their consequences.
Thiswill increase the applicability and usefulness ofour event extraction grammars.AcknowledgmentsThis work was partially supported by project?NTIS - New Technologies for InformationSociety?, European Center of Excellence,CZ.1.05/1.1.00/02.0090.ReferencesA.
Carlson, J. Betteridge, B. Kisiel, B.
Settles, R. Este-vam, J. Hruschka, and T. Mitchell.
2010.
Toward anarchitecture for never-ending language learning.
InProceedings of the Twenty-Fourth AAAI Conferenceon Artificial Intelligence (AAAI-10).A.
D?Ulizia, F. Ferri, and P. Grifoni.
2011.
A survey ofgrammatical inference methods for natural languagelearning.
Artificial Intelligence Review vol.
36 issue1.F.
Hogenboom, F. Frasincar, U. Kaymak, and F. Jong.2011.
An overview of event extraction from text.In Workshop on Detection, Representation, and Ex-ploitation of Events in the Semantic Web (DeRiVE2011) at ISWC 2011.M.
Naughton, N. Kushmerick, and J. Carthy.2006.
Event Extraction from Heterogeneous NewsSources.
In Proceedings of the AAAI 2006 workshopon Event Extraction and Synthesis, Menlo Park, Cal-ifornia, USA.117J.
Piskorski.
2007.
ExPRESS ?
Extraction PatternRecognition Engine and Specification Suite.
In Pro-ceedings of FSMNLP 2007.E.
Riloff and R. Jones.
1999.
Learning dictionaries forinformation extraction by multi-level bootstrapping.In Proceedings of the Sixteenth National Conferenceon Artificial Intelligence (AAAI 99).H.
Tanev and V. Zavarella.
2013.
Multilingual learn-ing and population of event ontologies.
a case studyfor social media.
In P. Buitelaar and P. Cimiano, ed-itors, Towards Multilingual Semantic Web (in press).Springer, Berlin & New York.H.
Tanev, J. Piskorski, and M. Atkinson.
2008.
Real-Time News Event Extraction for Global Crisis Mon-itoring.
In Proceedings of NLDB 2008., pages 207?218.H.
Tanev, V. Zavarella, J. Linge, M. Kabadjov, J. Pisko-rski, M. Atkinson, and R. Steinberger.
2009.
Ex-ploiting Machine Learning Techniques to Build anEvent Extraction System for Portuguese and Span-ish.
Linguama?tica: Revista para o ProcessamentoAutoma?tico das L?
?nguas Ibe?ricas, 2:550?566.M.
Turchi, V. Zavarella, and H. Tanev.
2011.
Pat-tern learning for event extraction using monolingualstatistical machine translation.
In Proceedings ofRecent Advances in Natural Language Processing(RANLP 2011), Hissar, Bulgaria.R.
Yangarber, R. Grishman, P. Tapanainen, and S. Hut-tunen.
2000.
Unsupervised Discovery of Scenario-Level Patterns for Information Extraction.
InProceedings of ANLP-NAACL 2000, Seattle, USA,2000.V.
Zavarella, H. Tanev, and J. Piskorski.
2008.
EventExtraction for Italian using a Cascade of Finite-StateGrammars.
In Proceedings of FSMNLP 2008.118
