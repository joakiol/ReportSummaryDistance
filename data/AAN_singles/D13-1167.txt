Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1602?1612,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsMulti-Relational Latent Semantic AnalysisKai-Wei Chang?University of IllinoisUrbana, IL 61801, USAkchang10@illinois.eduWen-tau Yih Christopher MeekMicrosoft ResearchRedmond, WA 98052, USA{scottyih,meek}@microsoft.comAbstractWe present Multi-Relational Latent Seman-tic Analysis (MRLSA) which generalizes La-tent Semantic Analysis (LSA).
MRLSA pro-vides an elegant approach to combining mul-tiple relations between words by construct-ing a 3-way tensor.
Similar to LSA, a low-rank approximation of the tensor is derivedusing a tensor decomposition.
Each word inthe vocabulary is thus represented by a vec-tor in the latent semantic space and each re-lation is captured by a latent square matrix.The degree of two words having a specificrelation can then be measured through sim-ple linear algebraic operations.
We demon-strate that by integrating multiple relationsfrom both homogeneous and heterogeneousinformation sources, MRLSA achieves state-of-the-art performance on existing benchmarkdatasets for two relations, antonymy and is-a.1 IntroductionContinuous semantic space representations haveproven successful in a wide variety of NLP and IRapplications, such as document clustering (Xu et al2003) and cross-lingual document retrieval (Dumaiset al 1997; Platt et al 2010) at the document leveland sentential semantics (Guo and Diab, 2012; Guoand Diab, 2013) and syntactic parsing (Socher etal., 2013) at the sentence level.
Such representa-tions also play an important role in applications forlexical semantics, such as word sense disambigua-tion (Boyd-Graber et al 2007), measuring word?Work conducted while interning at Microsoft Research.similarity (Deerwester et al 1990) and relationalsimilarity (Turney, 2006; Zhila et al 2013; Mikolovet al 2013).
In many of these applications, La-tent Semantic Analysis (LSA) (Deerwester et al1990) has been widely used, serving as a fundamen-tal component or as a strong baseline.LSA operates by mapping text objects, typicallydocuments and words, to a latent semantic space.The proximity of the vectors in this space impliesthat the original text objects are semantically re-lated.
However, one well-known limitation of LSAis that it is unable to differentiate fine-grained re-lations.
For instance, when applied to lexical se-mantics, synonyms and antonyms may both be as-signed high similarity scores (Landauer and Laham,1998; Landauer, 2002).
Asymmetric relations likehyponyms and hypernyms also cannot be differenti-ated.
Although there exists some recent work, suchas PILSA which tries to overcome this weaknessof LSA by introducing the notion of polarity (Yihet al 2012).
This extension, however, can onlyhandle two opposing relations (e.g., synonyms andantonyms), leaving open the challenge of encodingmultiple relations.In this paper, we propose Multi-Relational LatentSemantic Analysis (MRLSA), which strictly gener-alizes LSA to incorporate information of multiplerelations concurrently.
Similar to LSA or PILSAwhen applied to lexical semantics, each word is stillmapped to a vector in the latent space.
However,when measuring whether two words have a specificrelation (e.g., antonymy or is-a), the word vectorswill be mapped to a new space according to the rela-tion where the degree of having this relation will be1602judged by cosine similarity.
The raw data construc-tion in MRLSA is straightforward and similar to thedocument-term matrix in LSA.
However, instead ofusing one matrix to capture all relations, we extendthe representation to a 3-way tensor.
Each slice cor-responds to the document-term matrix in the originalLSA design but for a specific relation.
Analogous toLSA, the whole linear transformation mapping is de-rived through tensor decomposition, which providesa low-rank approximation of the original tensor.
Asa result, previously unseen relations between twowords can be discovered, and the information en-coded in other relations can influence the construc-tion of the latent representations, and thus poten-tially improves the overall quality.
In addition, theinformation in different slices can come from het-erogeneous sources (conceptually similar to (Riedelet al 2013)), which not only improves the model,but also extends the word coverage in a reliable way.We provide empirical evidence that MRLSA is ef-fective using two different word relations: antonymyand is-a.
We use the benchmark GRE test of closest-opposites (Mohammad et al 2008) to show thatMRLSA performs comparably to PILSA, which wasthe pervious state-of-the-art approach on this prob-lem, when given the same amount of information.
Inaddition, when other words and relations are avail-able, potentially from additional resources, MRLSAis able to outperform previous methods significantly.We use the is-a relation to demonstrate that MRLSAis capable of handling asymmetric relations.
Wetake the list of word pairs from the Class-Inclusion(i.e., is-a) relations in SemEval-2012 Task 2 (Jur-gens et al 2012), and use our model to measure thedegree of two words have this relation.
The mea-sures derived from our model correlate with humanjudgement better than the best system that partici-pated in the task.The rest of this paper is organized as follows.
Wefirst survey some related work in Section 2, followedby a more detailed description of LSA and PILSAin Section 3.
Our proposed model, MRLSA, is pre-sented in Section 4.
Section 5 presents our experi-mental results.
Finally, Section 6 concludes the pa-per.2 Related WorkMRLSA can be viewed as a model that derives gen-eral continuous space representations for capturinglexical semantics, with the help of tensor decompo-sition techniques.
We highlight some recent workrelated to our approach.The most commonly used continuous space rep-resentation of text is arguably the vector spacemodel (VSM) (Turney and Pantel, 2010).
In thisrepresentation, each text object can be representedby a high-dimensional sparse vector, such as aterm-vector or a document-vector that denotes thestatistics of term occurrences (Salton et al 1975)in a large corpus.
The text can also be repre-sented by a low-dimensional dense vector derivedby linear projection models like latent semanticanalysis (LSA) (Deerwester et al 1990), by dis-criminative learning methods like Siamese neuralnetworks (Yih et al 2011), recurrent neural net-works (Mikolov et al 2013) and recursive neu-ral networks (Socher et al 2011), or by graphicalmodels such as probabilistic latent semantic anal-ysis (PLSA) (Hofmann, 1999) and latent Dirichletallocation (LDA) (Blei et al 2003).
As a general-ization of LSA, MRLSA is also a linear projectionmodel.
However, while the words are representedby vectors as well, multiple relations between wordsare captured separately by matrices.In the context of lexical semantics, VSMs providea natural way of measuring semantic word related-ness by computing the distance between the cor-responding vectors, which has been a standard ap-proach (Agirre et al 2009; Reisinger and Mooney,2010; Yih and Qazvinian, 2012).
These approachesdo not apply directly to the problem of modelingother types of relations.
Existing methods that dohandle multiple relations often use a model com-bination scheme to integrate signals from varioustypes of information sources.
For instance, mor-phological variations discovered from the Googlen-gram corpus have been combined with informa-tion from thesauri and vector-based word related-ness models for detecting antonyms (Mohammad etal., 2008).
An alternative approach proposed by Tur-ney (2008) that handles synonyms, antonyms andassociations is to use a uniform approach by firstreducing the problem to determining whether two1603pairs of words can be analogous, and then predictingit using a supervised model with features based onthe frequencies of patterns in the corpus.
Similarly,to measure whether two word pairs have the samerelation, Zhila et al(2013) proposed to combine het-erogeneous models, which achieved state-of-the-artperformance.
In comparison, MRLSA models mul-tiple lexical relations holistically.
The degree thattwo words having a particular relation is estimatedusing the same linear function of the correspondingvectors and matrix.Tensor decomposition generalizes matrix factor-ization and has been applied to several NLP applica-tions recently.
For example, Cohen et al(2013) pro-posed an approximation algorithm for PCFG pars-ing that relies on Kruskal decomposition.
Van deCruys et al(2013) modeled the composition ofsubject-verb-object triples using Tucker decompo-sition, which results in a better similarity measurefor transitive phrases.
Similar to this constructionbut used in the community-based question answer-ing (CQA) scenario, Qiu et al(2013) representedtriples of question title, question content and answeras a tensor and applied 3-mode SVD to derive latentsemantic representations for question matching.
Theconstruction of MRLSA bears some resemblance tothe work that use tensors to capture triples.
How-ever, our goal of modeling different relations for lex-ical semantics is very different from the intended us-age of tensor decomposition in the existing work.3 Latent Semantic AnalysisLatent Semantic Analysis (LSA) (Deerwester et al1990) is a widely used continuous vector spacemodel that maps words and documents into a lowdimensional space.
LSA consists of two main steps.First, taking a collection of d documents that con-tains words from a vocabulary list of size n, it firstconstructs a d ?
n document-term matrix W to en-code the occurrence information of a word in a docu-ment.
For instance, in its simplest form, the elementWi,j can be the term frequency of the j-th word inthe i-th document.
In practice, a weighting schemethat better captures the importance of a word in thedocument, such as TF?IDF (Salton et al 1975),is often used instead.
Notice that ?document?
heresimply means a group of words and has been appliedW V X = U TFigure 1: SVD applied to a d?n document-term ma-trix W. The rank-k approximation, X, is the mul-tiplication of U, ?
and VT , where U and V ared ?
k and n ?
k orthonormal matrices and ?
is ak ?
k diagonal matrix.
The column vectors of VTmultiplied by the singular values ?
represent wordsin the latent semantic space.to various texts including news articles, sentencesand bags of words.
Once the matrix is constructed,the second step is to apply singular value decom-position (SVD) to W in order to derive a low-rankapproximation.
To have a rank-k approximation, Xis the reconstruction matrix of W, defined asW ?
X = U?VT (1)where the dimensions of U and V are d?
k andn?
k, respectively, and ?
is a k ?
k diagonal ma-trix.
In addition, the columns in U and V are or-thonormal and the elements in ?
are the singularvalues and are conventionally reverse-ordered.
Fig-ure 1 illustrates this decomposition.LSA can be used to compute the similarity be-tween two documents or two words in the latentspace.
For instance, to compare the u-th and v-thwords in the vocabulary, one can compute the co-sine similarity of the u-th and v-th column vectorsof X, the reconstruction matrix of W. In contrast toa direct lexical matching via the columns of W, thesimilarity measure computed as a result of the SVDmay have a nonzero similarity score even if thesetwo words do not co-occur in any documents.
Thisis due to the fact that those words can share somelatent components.An alternative view of using LSA is to treat thecolumn vectors of ?VT as a representation of thewords in a new k-dimensional latent space.
Thiscomes from the observation that the inner productof every two column vectors in X is the inner prod-uct of the corresponding column vectors of ?VT ,1604joyfulnessgladdensad1anger1-101100-10100-110000100000000Figure 2: The matrix construction of PILSA.
Thevocabulary is {joy, gladden, sorrow, sadden, anger,emotion, feeling} and target words are {joyfulness,gladden, sad, anger}.
For ease of presentation,we show the numbers with 0-1 values instead ofTF?IDF scores.
The polarity (i.e., sign) indicateswhether the term in the vocabulary is a synonym orantonym of the target word.which can be derived from the equations below.XTX = (U?VT )T (U?VT )= V?UTU?VT (?
is diagonal)= V?2VT (Columns of U are orthonormal)= (?VT )T (?VT ) (2)Thus, the semantic relatedness between the i-th andj-th words can be computed by cosine similarity1:cos(X:,i,X:,j) (3)When used to compare words, one well-knownlimitation of LSA is that the score captures the gen-eral notion of semantic similarity, and is unableto distinguish fine-grained word relations, such asantonyms (Landauer and Laham, 1998; Landauer,2002).
This is due to the fact that the raw matrix rep-resentation only records the occurrences of words indocuments without knowing the specific relation be-tween the word and document.
To address this issue,Yih et al(2012) proposed a polarity inducing latentsemantic analysis model recently, which we intro-duce next.1Cosine similarity is equivalent to the inner product of thenormalized vectors.3.1 Polarity Inducing Latent SemanticAnalysisIn order to distinguish antonyms from synonyms,the polarity inducing LSA (PILSA) model (Yih etal., 2012) takes a thesaurus as input.
Synonyms andantonyms of the same target word are grouped to-gether as a ?document?
and a document-term matrixis constructed accordingly as done in LSA.
Becauseeach word in a group belongs to either one of the twoopposite relations, synonymy and antonymy, the po-larity information is induced by flipping the signs ofantonyms.
While the absolute value of each elementin the matrix is still the same TF?IDF score, theelements that correspond to the antonyms becomenegative.This design has an intriguing effect.
When com-paring two words using the cosine similarity (or sim-ply inner product) of their corresponding columnvectors in the matrix, the score of a synonym pairremains positive, but the score of an antonym pairbecomes negative.
Figure 2 illustrates this designusing a simplified matrix as example.Once the matrix is constructed, PILSA appliesSVD as done in LSA, which generalizes the modelto go beyond lexical matching.
The sign of the co-sine score of the column vectors of any two wordsindicates whether they are close to synonyms or toantonyms and the absolute value reflects the degreeof the relation.
When all the column vectors are nor-malized to unit vectors, it can also be viewed as syn-onyms are clustered together and antonyms lie onthe opposite sides of a unit sphere.
Although PILSAsuccessfully extends LSA to handle not just one sin-gle occurrence relation, the extension is limited toencoding two opposing relations4 Multi-Relational Latent SemanticAnalysisThe fundamental reason why it is difficult to handlemultiple relations is due to the 2-dimensional ma-trix representation.
In order to overcome this, weencode the raw data in a 3-way tensor.
Each slicecaptures a particular relation and is in the format ofthe document-term matrix in LSA.
Just as in LSA,where the low-rank approximation by SVD helpsgeneralize the representation and discover unseenrelations, we apply a tensor decomposition method,1605joyfulnessgladdensad1anger100110000100010000100000000(a) Synonym layerjoyfulnessgladdensad0anger010000010000100000000000000(b) Antonym layerjoyfulnessgladdensad0anger000000000000000000010111011(c) Hypernym layerFigure 3: The three slices of MRLSA raw tensorW for an example with vocabulary {joy, gladden, sorrow,sadden, anger, emotion, feeling} and target words {joyfulness, gladden, sad, anger}.
Figures 3(a), 3(b), 3(c)show the matrices W:,:,syn, W:,:,ant, W:,:,hyper, respectively.
Rows represent documents (see definition intext), and columns represent words.
For ease of presentation, we show numbers with 0-1 values instead ofTF?IDF scores.the Tucker decomposition, to the tensor.4.1 Representing Multi-Relational Data inTensorsA tensor is simply a multi-dimensional array.
In thiswork, we use a 3-way tensor W to encode multi-ple word relations.
An element of W is denotedby Wi,j,k using its indices, and W:,:,k representsthe k-th slice of W (a slice of a 3-way tensor isa matrix, obtained by fixing the third index).
Fol-lowing (Kolda and Bader, 2009), a fiber of a ten-sor W:,j,k is a vector, which is a high order analogof a matrix row or column.When constructing the raw tensorW in MRLSA,each slice is analogous to the document-term ma-trix in LSA, but created based on the data of a par-ticular relation, such as synonyms.
With a slightabuse of notation, we sometimes use the value ratherthan index when there is no confusion.
For in-stance, W:,?word?,k represents the fiber correspond-ing to the ?word?
in slice k, and W:,:,syn refers tothe slice that encodes the synonymy relation.
Belowwe use an example to compare this construction tothe raw matrix in PILSA, and discuss how it extendsLSA.Suppose we are interested in representing two re-lations, synonymy and antonymy.
The raw tensor inMRLSA would then consist of two slices, W:,:,synand W:,:,ant, to encode synonyms and antonyms oftarget words from a knowledge source (e.g., a the-saurus).
Each row in W:,:,syn represents the syn-onyms of a target word, and the correspondingrow in W:,:,ant encodes its antonyms.
Figures 3(a)and 3(b) illustrate an example, where ?joy?, ?glad-den?
are synonyms of the target word ?joyfulness?and ?sorrow?
is its antonym.
Therefore, the valuesof the corresponding entries are 1.
Notice that thematrix W?
= W:,:,syn ?
W:,:,ant is identical to thePILSA raw matrix.
We can extend the constructionabove to enable MRLSA to utilize other semanticrelations (e.g., hypernymy) by adding a slice cor-responding to each relation of interest.
Fig.
3(c)demonstrates how to add another slice W:,:,hyper tothe tensor for encoding hypernyms.4.2 Tensor DecompositionThe MRLSA raw tensor encodes relations in one ormore data resources, such as thesauri.
However, theknowledge from a thesaurus is usually noisy and in-complete.
In this section, we derive a low-rank ap-proximation of the tensor to generalize the knowl-edge.
This step is analogous to the rank-k approxi-mation in LSA.Various tensor decomposition methods have beenproposed in literature.
Among them, Tucker decom-position (Tucker, 1966) is recognized as a multi-dimensional extension of SVD and has been widelyused in many applications.
An illustration of thismethod is in Fig.
4(a).
In Tucker decomposition,a d?
n?m tensor W is decomposed into fourcomponents G,U,V,T.
A low-rank approximation1606X U VGTT=W(a) Tucker Tensor DecompositionX U VS: , : , 1 T=S(b) Our ReformulationFigure 4: Fig.
4(a) illustrates the Tucker tensor decomposition method which factors a 3-way tensorW tothree orthogonal matrices, U,V,T, and a core tensor G. We further apply a n-mode matrix product on thecore tensor G with T. Consequently, each slice of the resulted core tensor S (a square matrix) captures asemantic relation type, and each column of VT is a vector representing a word.X ofW is defined byWi,j,k ?
Xi,j,k=R1?r1=1R2?r2=1R3?r3=1Gr1,r2,r3Ui,r1Vj,r2Tk,r3 ,where G is a core tensor with dimensionsR1?R2?R3 and U,V,T are orthogonal matrices with di-mensions d ?
R1, n ?
R2,m ?
R3, respectively.The rank parameters R1 ?
d,R2 ?
n,R3 ?
m aregiven as input to the algorithm.
In MRLSA, m (thenumber of relations) is usually small, while d and nare typically large (often in the scale of hundredsof thousands).
Therefore, we choose R1 = R2 = ?
,?
d, n andR3 = m, where ?
is typically less than1000.To make the analogy to SVD clear, we rewrite theresults of Tucker decomposition by performing a n-mode matrix product over the core tensor G with thematrix T. This produces a tensor S where each sliceis a linear combination of the slices of G with coeffi-cients given by T (see (Kolda and Bader, 2009) fordetail).
That is, we haveS:,:,k =m?t=1Tt,kG:,:,t, ?k.An illustration is shown in Fig.
4(b), Then, astraightforward calculation shows that k-th slice oftensorW is approximated byW:,:,k ?
X:,:,k = US:,:,kVT .
(4)Comparing Eq.
(4) to Eq.
(1), one can observethat matrices U and V play similar roles here, andeach slice of the core tensor S is analogous to ?.However, the square matrix G:,:,k is not necessaryto be diagonal.
As in SVD, the column vectorsof G:,:,kVT (capture both word and relation infor-mation) behave similarly to the column vectors ofthe original tensor sliceW:,:,k.4.3 Measuring the Degrees of Word RelationsIn principle, the raw information in the input ten-sor W can be used for computing lexical similarityusing the cosine score between the column vectorsfor two words from the same slice of the tensor.
Tomeasure the degree of other relations, however, ourapproach requires one to specify a pivot slice.
Thekey role of the pivot slice is to expand the lexicalcoverage of the relation of interest to additional lexi-cal entries and, for this reason, the pivot slice shouldbe chosen to capture the equivalence of the lexicalentries.
In this paper, we use the synonymy relationas our pivot slice.
First we consider measuring thedegree of a relation rel holding between the i-th andj-th words using the raw tensor W , which can becomputed ascos(W:,i,syn,W:,j,rel).
(5)This measurement can be motivated from the logicalrule: syn(wordi, target) ?
rel(target,wordj) ?rel(wordi,wordj), where the pivot relation syn ex-pands the coverage of the relation of interest rel.Turning to the use of the tensor decomposition,we use a similar derivation to Eq.
(3), and measurethe degree of relation rel between two words bycos(S:,:,synVTi,:,S:,:,relVTj,:).
(6)1607For instance, the degree of antonymy between?joy?
and ?sorrow?
is measured by the co-sine similarity between the respective fiberscos(X:,?joy?,syn,X:,?sorrow?,ant).
We can encode bothsymmetric relations (e.g., antonymy and synonymy)and asymmetric relations (e.g., hypernymy andhyponymy) in the same tensor representation.
For asymmetric relation, we use both cos(X:,i,syn,X:,j,rel)and cos(X:,j,syn,X:,i,rel) and measure the degree ofa symmetric relation by the average of these twocosine similarity scores.
However, for asymmetricrelations, we use only cos(X:,i,syn,X:,j,rel).5 ExperimentsWe evaluate MRLSA on two tasks: answering theclosest-opposite GRE questions and measuring de-grees of various class-inclusion (i.e., is-a) relations.In both tasks, we design the experiments to empir-ically validate the following claims.
When encod-ing two opposite relations from the same source,MRLSA performs comparably to PILSA.
However,MRLSA generalizes LSA to model multiple rela-tions, which could be obtained from both homoge-neous and heterogeneous data sources.
As a result,the performance of a target task can be further im-proved.5.1 Experimental SetupWe construct the raw tensors to encode a particularrelation in each slice based on two data sources.Encarta The Encarta thesaurus is developed byBloomsbury Publishing Plc2.
For each target word,it provides a list of synonyms and antonyms.
Weuse the same version of the thesaurus as in (Yih etal., 2012), which contains about 47k words and avocabulary list of approximately 50k words.WordNet We use four types of relations fromWordNet: synonymy, antonymy, hypernymy andhyponymy.
The number of target words and thesize of the vocabulary in our version are 117,791and 149,400, respectively.
WordNet has better vo-cabulary coverage, but fewer antonym pairs.
Forinstance, the WordNet antonym slice contains only46,945 nonzero entries, while the Encarta antonymslice has 129,733.2http://www.bloomsbury.comWe apply a memory-efficient Tucker decomposi-tion algorithm (Kolda and Sun, 2008) implementedin tensor toolbox v2.5 (Bader et al 2012)3 to factorthe tensor.
The largest tensor considered in this pa-per can be decomposed in about 3 hours using lessthan 4GB of memory with a commodity PC.5.2 Answering GRE Antonym QuestionsThe first task is to answer the closest-opposite ques-tions from the GRE test provided by Mohammad etal.
(2008)4.
Each question in this test consists ofa target word and five candidate words, where thegoal is to pick the candidate word that has the mostopposite meaning to the target word.
In order tohave a fair comparison, we use the same data splitas in (Mohammad et al 2008), with 162 questionsused for the development set and 950 for test.
Fol-lowing (Mohammad et al 2008; Yih et al 2012),we report the results in precision (accuracy of thequestions that the system attempts to answer), re-call (percentage of the questions answered correctlyover all questions) and F1 (the harmonic mean ofprecision and recall).We tune two sets of parameters using the devel-opment set: (1) the rank parameter ?
in the tensordecomposition and (2) the scaling factors of differ-ent slices of the tensor.
The rank parameter spec-ifies the number of dimensions of the latent space.In the experiments, We pick the best value of ?
from{100, 200, 300, 500, 750, 1000}.
The scaling factorsadjust the values of each slice of the tensor.
The el-ements of each slice are multiplied by the scalingfactor before factorization.
This is important be-cause Tucker decomposition minimizes the recon-struction error (the Frobenius norm of the residualtensor).
As a result, the slice with a larger range ofvalues becomes more influential to U and V. In thiswork, we fixW:,:,ant, and search for the scaling fac-tor of W:,:,syn in {0.25, 0.5, 1, 2, 4} and the factorsofW:,:,hyper andW:,:,hypo in {0.0625, 0.125, 0.25}.Table 1 summarizes the results of training3http://www.sandia.gov/?tgkolda/TensorToolbox.
The Tucker decomposition involvesperforming SVD on a large matrix.
We modify the MATLABcode of tensor toolbox to use the built-in svd function insteadof svds.
This modification reduces both the running time andmemory usage.4http://www.saifmohammad.com1608Dev.
Set Test SetPrec.
Rec.
F1 Prec.
Rec.
F1WordNet Lookup 0.40 0.40 0.40 0.42 0.41 0.42WordNet RawTensor 0.42 0.41 0.42 0.42 0.41 0.42WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60WordNet MRLSA:Syn+Ant 0.63 0.62 0.62 0.59 0.58 0.59WordNet MRLSA:4-layers 0.66 0.65 0.65 0.61 0.59 0.60Encarta Lookup 0.65 0.61 0.63 0.61 0.56 0.59Encarta RawTensor 0.67 0.64 0.65 0.62 0.57 0.59Encarta PILSA 0.86 0.81 0.84 0.81 0.74 0.77Encarta MRLSA:Syn+Ant 0.87 0.82 0.84 0.82 0.74 0.78MRLSA:WordNet+Encarta 0.88 0.85 0.87 0.81 0.77 0.79Table 1: GRE antonym test results of models based on Encarta and WordNet data in precision, recall and F1.RawTensor evaluates the performance of the tensor with 2 slices encoding synonyms and antonyms be-fore decomposition (see Eq.
(5)), which is comparable to checking the original data directly (Lookup).MRLSA:Syn+Ant applies Tucker decomposition to the raw tensor and measures the degree of antonymyusing Eq.
(6).
The result is similar to that of PILSA (see Sec.
3.1).
MRLSA:4-layers adds hypernyms andhyponyms from WordNet; MRLSA:WordNet+Encarta consists of synonyms/antonyms from Encarta and hy-pernyms/hyponyms from WordNet, where the target words are aligned using the synonymy relations.
Bothmodels demonstrate the advantage of encoding more relations, from either the same or different resources.MRLSA using two different corpora, Encarta andWordNet.
The performance of the MRLSA rawtensor is close to that of looking up the thesaurus.This indicates the tensor representation is able tocapture the word relations explicitly described inthe thesaurus.
After conducting tensor decomposi-tion, MRLSA:Syn+Ant achieves similar results toPILSA.
This confirms our claim that when giv-ing the same among of information, MRLSA per-forms at least comparably to PILSA.
However, thetrue power of MRLSA is its ability to incorpo-rate other semantic relations to boost the perfor-mance of the target task.
For example, whenwe add the hypernymy and hyponymy relations tothe tensor, these class-inclusion relations provide aweak signal to help resolve antonymy.
We sus-pect that this is due to the fact that antonyms typ-ically share the same properties but only have theopposite meaning on one particular semantic di-mension.
For instance, the antonyms ?sadness?and ?happiness?
are different forms of emotion.When two words are hyponyms of a target word,the likelihood that they are antonyms should thusbe increased.
We show that the target relationsand these auxiliary semantic relations can be col-lected from the same data source (e.g., WordNetMRLSA:4-layers) or from multiple, heterogeneoussources (e.g., MRLSA:WordNET+Encarta).
In bothcases, the performance of the model improves asmore relations are incorporated.
Moreover, our ex-periments show that adding the hypernym and hy-ponym layers from WordNet improves modelingantonym relations based on the Encarta thesaurus.This suggests that the weak signal from a resourcewith a large vocabulary (e.g., WordNet) can helppredict relations between out-of-vocabulary wordsand thus improve the recall.To better understand the model, we examine thetop antonyms for three question words from theGRE test.
The lists below show antonyms and theirMRLSA scores for each of the GRE question wordsas determined by the MRLSA:WordNET+Encartamodel.
Antonyms that can be found directly in theEncarta thesaurus are in italics.inanimate alive (0.91), living (0.90), bodily (0.90), in-the-flesh (0.89), incarnate (0.89)alleviate exacerbate (0.68), make-worse (0.67), in-flame (0.66), amplify (0.65), stir-up (0.64)relish detest (0.33), abhor (0.33), abominate (0.33), de-spise (0.33), loathe (0.31)We can see that from these examples, MRLSA not1609Dev.
Test1a (Taxonomic) 1b (Functional) 1c (Singular) 1d (Plural) Avg.WordNet Lookup 52.9 34.5 41.4 34.3 36.7WordNet RawTensor 51.0 38.3 50.0 42.1 43.5WordNet MRLSA:Syn+Hypony 55.8 41.7 (43.2) 51.0 (51.4) 37.5 (44.4) 43.4 (46.3)WordNet MRLSA:4-layers 52.9 51.5 (53.9) 51.9 (60.0) 43.5 (50.5) 49.0 (54.8)MRLSA:WordNet+Encarta 62.1 55.3 (58.7) 57.1 (65.7) 48.6 (53.7) 55.8 (60.1)UTDNB (Rink and Harabagiu, 2012) - 38.3 36.7 28.2 34.4Table 2: Results of measuring the class-inclusion (is-a) relations in MaxDiff accuracy (see text for de-tail).
RawTensor has synonym and hyponym slices and measures the degree of is-a relation using Eq.
(5).MRLSA:Syn+Hypo factors the raw tensor and judges the relation by Eq.
(6).
The constructions ofMRLSA:4-layers and MRLSA:WordNet+Encarta are the same as in Sec.
5.2 (see the caption of Table 1for detail).
For MRLSA models, numbers shown in the parentheses are the results when parameters aretuned using the test sets.
UTDNB is the results of the best performing system in SemEval-2012 Task 2.only preserves the antonyms in the thesaurus, butalso discovers additional ones, such as exacerbateand inflame for ?alleviate?.
Another interesting find-ing is that while the scores are useful in rankingthe candidate words, they might not be comparableacross different question words.
This could be anissue for some applications, which need to make abinary decision on whether two words are antonyms.5.3 Measuring degrees of Is-A relationsWe evaluate MRLSA using the class-inclusion por-tion of SemEval-2012 Task 2 data (Jurgens et al2012).
Here the goal is to measure the degreeof two words having the is-a relation.
Five an-notated datasets are provided for different subcate-gories of this relation: 1a-taxonomic, 1b-functional,1c-singular, 1d-plural, 1e-class individual.
We omit1e because it focuses on real world entities (e.g.,queen:Elizabeth, river:Nile), which are not includedin WordNet.Each dataset contains about 100 questions basedon approximately 40 word pairs.
The question con-sists of 4 randomly chosen word pairs and asks thebest and worst pairs that exemplify the specific is-arelation.
The performance is measured by the av-erage prediction accuracy, also called the MaxDiffaccuracy (Louviere and Woodworth, 1991).Because the questions are generated from thesame set of word pairs, these questions are not mutu-ally independent.
Therefore, it is not proper to splitthe data of each subcategory into the developmentand test sets.
Alternatively, we follow the settingof SemEval-2012 Task 2 and use the first subcat-egory (1a-taxonomy) to tune the model and eval-uate its performance based on the results on otherdatasets.
Since the models are tuned and tested ondifferent types of subcategories, they might not bethe optimal ones when evaluated on the test sets.Therefore, we show results using the best parame-ters tuned on the development set and those tuned onthe test set, where the latter suggests a performanceupper-bound.
Besides the rank parameter, we tunethe scaling factors of the synonym, hypernym andhyponym slices from {4, 16, 64}.
The scaling factorof the antonym slice is fixed to 1.Table 2 shows the performance in MaxDiff accu-racy.
Results show that even the raw tensor repre-sentation (RawTensor) performs better than Word-Net lookup.
We suspect that this is because thetensor representation can capture the fact that thehyponyms of a word are usually synonymous toeach other.
By performing Tucker decompositionon the raw Tensor, MRLSA achieves better per-formance.
MRLSA:4-layers further leverages theinformation from antonyms and hypernyms andthus improves the model.
As we notice in theGRE antonym test, models based on the Encartathesaurus perform better in predicting antonyms.Therefore, it is interesting to check if combiningsynonyms and antonyms from Encarta helps.
Asa result, MRLSA:WordNet+Encarta improves overMRLSA:4-layers significantly.
This demonstratesagain that MRLSA can leverage knowledge stored inheterogeneous resources.
Notably, MRLSA outper-1610forms the best system participated in the SemEval-2012 task with a large margin, with a difference of21.4 in MaxDiff accuracy.Next we examine the top words that have the is-a relation relative to three question words from thetask.
The lists below show the hyponyms and theirrespective MRLSA scores for each of the questionwords as determined by MRLSA:4-layers.bird ostrich (0.75), gamecock (0.75), nighthawk (0.75),amazon (0.74), parrot (0.74)automobile minivan (0.48), wagon (0.48), taxi (0.46),minicab (0.45), gypsy cab (0.45)vegetable buttercrunch (0.61), yellow turnip (0.61), ro-maine (0.61), chipotle (0.61), chilli (0.61)Although the model in general does a good jobfinding hyponyms, we observe that some suggestedwords, such as buttercrunch (a mild lettuce) vs.?vegetable?, do not seem intuitive (e.g., compared tocarrot).
Having one additional slice to capture thegeneral term co-occurrence relation may help im-prove the model in this respect.6 ConclusionsIn this paper, we propose Multi-Relational LatentSemantic Analysis (MRLSA) which generalizes La-tent Semantic Analysis (LSA) for lexical seman-tics.
MRLSA models multiple word relations byleveraging a 3-way tensor, where each slice cap-tures one particular relation.
A low-rank approx-imation of the tensor is then derived using a ten-sor decomposition.
Consequently, words in the vo-cabulary are represented by vectors in the latent se-mantic space, and each relation is captured by alatent square matrix.
Given two words, MRLSAnot only can measure their degree of having a spe-cific relation, but also can discover unknown rela-tions between them.
These advantages have beendemonstrated in our experiments.
By encoding re-lations from both homogeneous or heterogeneousdata sources, MRLSA achieves state-of-the-art per-formance on existing benchmark datasets for two re-lations, antonymy and is-a.For future work, we plan to explore directions thataim for improving both the quality and word cover-age of the model.
For instance, the knowledge en-coded by MRLSA can be enriched by adding morerelations from a variety of linguistic resources, in-cluding the co-occurrence relations from large cor-pora.
On model refinement, we notice that MRLSAcan be viewed as a 3-layer neural network withoutapplying the sigmoid function.
Following the strat-egy of using Siamese neural networks to enhancePILSA (Yih et al 2012), training MRLSA with amulti-task discriminative learning setting can be apromising approach as well.AcknowledgmentsWe thank Geoff Zweig for valuable discussions andthe anonymous reviewers for their comments.ReferencesE.
Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?caand A. Soroa.
2009.
A study on similarity and re-latedness using distributional and WordNet-based ap-proaches.
In NAACL ?09, pages 19?27.Brett W. Bader, Tamara G. Kolda, et al2012.
Matlabtensor toolbox version 2.5.
Available online, January.David M. Blei, Andrew Y. Ng, Michael I. Jordan, andJohn Lafferty.
2003.
Latent dirichlet alcation.
Jour-nal of Machine Learning Research, 3:993?1022.Jordan L Boyd-Graber, David M Blei, and Xiaojin Zhu.2007.
A topic model for word sense disambiguation.In EMNLP-CoNLL, pages 1024?1033.Shay B. Cohen, Giorgio Satta, and Michael Collins.2013.
Approximate PCFG parsing using tensor de-composition.
In NAACL-HLT 2013, pages 487?496.S.
Deerwester, S. Dumais, G. Furnas, T. Landauer, andR.
Harshman.
1990.
Indexing by latent semantic anal-ysis.
Journal of the American Society for InformationScience, 41(96).S.
Dumais, T. Letsche, M. Littman, and T. Landauer.1997.
Automatic cross-language retrieval using latentsemantic indexing.
In AAAI-97 Spring Symposium Se-ries: Cross-Language Text and Speech Retrieval.Weiwei Guo and Mona Diab.
2012.
Modeling sentencesin the latent space.
In ACL 2012, pages 864?872.Weiwei Guo and Mona Diab.
2013.
Improving lexicalsemantics for sentential semantics: Modeling selec-tional preference and similar words in a latent variablemodel.
In NAACL-HLT 2013, pages 739?745.Thomas Hofmann.
1999.
Probabilistic latent semanticanalysis.
In Proceedings of Uncertainty in ArtificialIntelligence, pages 289?296.D.
Jurgens, S. Mohammad, P. Turney, and K. Holyoak.2012.
SemEval-2012 Task 2: Measuring degrees ofrelational similarity.
In Proceedings of the Sixth Inter-national Workshop on Semantic Evaluation (SemEval2012), pages 356?364.1611Tamara G. Kolda and Brett W. Bader.
2009.
Ten-sor decompositions and applications.
SIAM Review,51(3):455?500, September.Tamara G. Kolda and Jimeng Sun.
2008.
Scalable ten-sor decompositions for multi-aspect data mining.
InICDM 2008, pages 363?372.T.
Landauer and D. Laham.
1998.
Learning human-like knowledge by singular value decomposition: Aprogress report.
In NIPS 1998.T.
Landauer.
2002.
On the computational basis of learn-ing and cognition: Arguments from lsa.
Psychology ofLearning and Motivation, 41:43?84.Jordan J. Louviere and G. G. Woodworth.
1991.
Best-worst scaling: A model for the largest difference judg-ments.
Technical report, University of Alberta.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous space wordrepresentations.
In NAACL-HLT 2013.Saif Mohammad, Bonnie Dorr, and Graeme Hirst.
2008.Computing word pair antonymy.
In Empirical Meth-ods in Natural Language Processing (EMNLP).John Platt, Kristina Toutanova, and Wen-tau Yih.
2010.Translingual document representations from discrimi-native projections.
In Proceedings of EMNLP, pages251?261.Xipeng Qiu, Le Tian, and Xuanjing Huang.
2013.
Latentsemantic tensor indexing for community-based ques-tion answering.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Linguis-tics (Volume 2: Short Papers), pages 434?439, Sofia,Bulgaria, August.
Association for Computational Lin-guistics.Joseph Reisinger and Raymond J. Mooney.
2010.
Multi-prototype vector-space models of word meaning.
InProceedings of HLT-NAACL, pages 109?117.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M. Marlin.
2013.
Relation extractionwith matrix factorization and universal schemas.
InNAACL-HLT 2013, pages 74?84.Bryan Rink and Sanda Harabagiu.
2012.
UTD: Deter-mining relational similarity using lexical patterns.
InProceedings of the Sixth International Workshop onSemantic Evaluation (SemEval 2012), pages 413?418,Montre?al, Canada, 7-8 June.
Association for Compu-tational Linguistics.G.
Salton, A. Wong, and C. S. Yang.
1975.
A VectorSpace Model for Automatic Indexing.
Communica-tions of the ACM, 18(11).Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,and Christopher D. Manning.
2011.
Parsing naturalscenes and natural language with recursive neural net-works.
In ICML ?11.Richard Socher, John Bauer, Christopher D. Manning,and Andrew Y. Ng.
2013.
Parsing with compositionalvector grammars.
In Annual Meeting of the Associa-tion for Computational Linguistics (ACL).Ledyard R Tucker.
1966.
Some mathematicalnotes on three-mode factor analysis.
Psychometrika,31(3):279?311.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: Vector space models of semantics.Journal of Artificial Intelligence Research, 37(1):141?188.P.
D. Turney.
2006.
Similarity of semantic relations.Computational Linguistics, 32(3):379?416.Peter Turney.
2008.
A uniform approach to analo-gies, synonyms, antonyms, and associations.
In In-ternational Conference on Computational Linguistics(COLING).Tim Van de Cruys, Thierry Poibeau, and Anna Korho-nen.
2013.
A tensor-based factorization model of se-mantic compositionality.
In Proceedings of the 2013Conference of the North American Chapter of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 1142?1151, Atlanta, Geor-gia, June.
Association for Computational Linguistics.Wei Xu, Xin Liu, and Yihong Gong.
2003.
Documentclustering based on non-negative matrix factorization.In Proceedings of the 26th annual international ACMSIGIR conference on Research and development in in-formaion retrieval, pages 267?273, New York, NY,USA.
ACM.Wen-tau Yih and Vahed Qazvinian.
2012.
Measur-ing word relatedness using heterogeneous vector spacemodels.
In Proceedings of NAACL-HLT, pages 616?620, Montre?al, Canada, June.Wen-tau Yih, Kristina Toutanova, John C. Platt, andChristopher Meek.
2011.
Learning discriminativeprojections for text similarity measures.
In Proceed-ings of the Fifteenth Conference on ComputationalNatural Language Learning, pages 247?256, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.Wen-tau Yih, Geoffrey Zweig, and John Platt.
2012.
Po-larity inducing latent semantic analysis.
In Proceed-ings of NAACL-HLT, pages 1212?1222, Jeju Island,Korea, July.Alisa Zhila, Wen-tau Yih, Christopher Meek, GeoffreyZweig, and Tomas Mikolov.
2013.
Combining het-erogeneous models for measuring relational similar-ity.
In Proceedings of the 2013 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies, pages 1000?1009, Atlanta, Georgia, June.
Asso-ciation for Computational Linguistics.1612
