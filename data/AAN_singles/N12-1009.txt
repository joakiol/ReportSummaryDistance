2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 80?90,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsReference Scope Identification in Citing SentencesAmjad Abu-JbaraEECS DepartmentUniversity of MichiganAnn Arbor, MI, USAamjbara@umich.eduDragomir RadevEECS DepartmentUniversity of MichiganAnn Arbor, MI, USAradev@umich.eduAbstractA citing sentence is one that appears in a sci-entific article and cites previous work.
Cit-ing sentences have been studied and used inmany applications.
For example, they havebeen used in scientific paper summarization,automatic survey generation, paraphrase iden-tification, and citation function classification.Citing sentences that cite multiple papers arecommon in scientific writing.
This observa-tion should be taken into consideration whenusing citing sentences in applications.
For in-stance, when a citing sentence is used in asummary of a scientific paper, only the frag-ments of the sentence that are relevant to thesummarized paper should be included in thesummary.
In this paper, we present and com-pare three different approaches for identifyingthe fragments of a citing sentence that are re-lated to a given target reference.
Our methodsare: word classification, sequence labeling,and segment classification.
Our experimentsshow that segment classification achieves thebest results.1 IntroductionCitation plays an important role in science.
It makesthe accumulation of knowledge possible.
When areference appears in a scientific article, it is usuallyaccompanied by a span of text that highlights theimportant contributions of the cited article.
Wecall a sentence that contains an explicit referenceto previous work a citation sentence.
For example,sentence (1) below is a citing sentence that cites apaper by Philip Resnik and describes the problemResnik addressed in his paper.
(1) Resnik (1999) addressed the issue of language identificationfor finding Web pages in the languages of interest.Previous work has studied and used citation sen-tences in various applications such as: scientific pa-per summarization (Elkiss et al, 2008; Qazvinianand Radev, 2008; Mei and Zhai, 2008; Qazvinianet al, 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011), automatic survey genera-tion (Nanba et al, 2000; Mohammad et al, 2009),citation function classification (Nanba et al, 2000;Teufel et al, 2006; Siddharthan and Teufel, 2007;Teufel, 2007), and paraphrase recognition (Nakov etal., 2004; Schwartz et al, 2007).Sentence (1) above contains one reference, andthe whole sentence is talking about that reference.This is not always the case in scientific writing.Sentences that contain references to multiple papersare very common.
For example, sentence (2) belowcontains three references.
(2) Grefenstette and Nioche (2000) and Jones and Ghani (2000)use the web to generate corpora for languages where electronicresources are scarce, while Resnik (1999) describes a methodfor mining the web for bilingual texts.80The first fragment describes the contribution ofGrefenstette and Nioche (2000) and Jones and Ghani(2000).
The second fragment describes the contribu-tion of Resnik (1999).This observation should be taken into considera-tion when using citing sentences in the aforemen-tioned applications.
For example, in citation-basedsummarization of scientific papers, a subset of cit-ing sentences that cite a given target paper is se-lected and used to form a summary of that paper.It is very likely that one or more of the selected sen-tences cite multiple papers besides the target.
Thismeans that some of the text included in the sum-mary might be irrelevant to the summarized paper.Including irrelevant text in the summary introducesseveral problems.
First, the summarization task aimsat summarizing the contributions of the target paperusing minimal text.
Extraneous text takes space inthe summary while being irrelevant and less impor-tant.
Second, including irrelevant text in the sum-mary breaks the context and confuses the reader.Therefore, if sentence (2) above is to be added toa citation-based summary of Resniks?
(1999) paper,only the underlined fragment should be added to thesummary and the rest of the sentence should be ex-cluded.For another example, consider the task of citationfunction classification.
The goal of this task is todetermine the reason for citing paper B by paper Abased on linguistic and structural features extractedfrom citing sentences that appear in A and cite B. Ifa citing sentence in A cites multiple papers besidesB, classification features should be extracted onlyfrom the fragments of the sentence that are relevantto B.
Sentence (3) below shows an examples of thiscase.
(3) Cohn and Lapata (2008) used the GHKM extraction method (Galleyet al, 2004), which is limited to constituent phrases and thus producesa reasonably small set of syntactic rules.If the target reference is Cohn and Lapata (2008),only the underlined segment should be used for fea-ture extraction.
The limitation stated in the sec-ond segment of sentence is referring to Galley et al,(2004).In this paper, we address the problem of identi-fying the fragments of a citing sentence that are re-lated to a given target reference.
Henceforth, we usethe term Reference Scope to refer to those fragments.We present and compare three different approachesto this problem.In the first approach, we define the problem as aword classification task.
We classify each word inthe sentence as inside or outside the scope of the tar-get reference.In the second approach, we define the problem asa sequence labeling problem.
This is different fromthe first approach in that the label assigned to eachword is dependent on the labels of nearby words.
Inthe third approach, instead of classifying individualwords, we split the sentence into segments and clas-sify each segment as inside or outside the scope ofthe target reference.Applying any of the three approaches is pre-ceded by a preprocessing stage.
In this stage, cit-ing sentences are analyzed to tag references, iden-tify groups of references, and distinguish betweensyntactic and non-syntactic references.The rest of this paper is organized as follows.
Sec-tion 2 examines the related work.
We define theproblem in Section3.
Section 4 presents our ap-proaches.
Experiments, results and analysis are pre-sented in Section 5.
We conclude and provide direc-tions to future work in Section 62 Related WorkOur work is related to a large body of research oncitations (Hodges, 1972; Garfield et al, 1984).
Theinterest in studying citations stems from the fact thatbibliometric measures are commonly used to esti-mate the impact of a researcher?s work (Borgmanand Furner, 2002; Luukkonen, 1992).
White (2004)provides a good recent survey of the different re-search lines that use citations.
In this section we re-view the research lines that are relevant to our work81and show how our work is different.One line of research that is related to our workhas to do with identifying what Nanba and Oku-mura (1999) call the citing area They define the cit-ing area as the succession of sentences that appeararound the location of a given reference in a sci-entific paper and have connection to it.
Their al-gorithm starts by adding the sentence that containsthe target reference as the first member sentence inthe citing area.
Then, they use a set of cue wordsand hand-crafted rules to determine whether the sur-rounding sentences should be added to the citingarea or not.
In (Nanba et al, 2000) they use their cit-ing area identification algorithm to improve citationtype classification and automatic survey generation.Qazvinian and Radev (2010) addressed a simi-lar problem.
They proposed a method based onprobabilistic inference to extract non-explicit cit-ing sentences; i.e., sentences that appear aroundthe sentence that contains the target reference andare related to it.
They showed experimentally thatcitation-based survey generation produces better re-sults when using both explicit and non-explicit cit-ing sentences rather than using the explicit onesalone.Although this work shares the same general goalwith ours (i.e identifying the pieces of text that arerelevant to a given target reference), our work is dif-ferent in two ways.
First, previous work mostly ig-nored the fact that the citing sentence itself mightbe citing multiple references.
Second, it defined theciting area (Nanba and Okumura, 1999) or the ci-tation context (Qazvinian and Radev, 2010) as a setof whole contiguous sentences.
In our work, we ad-dress the case where one citing sentence cites mul-tiple papers, and define what we call the referencescope to be the fragments (not necessarily contigu-ous) of the citing sentence that are related to the tar-get reference.In a recent work on citation-based summarizationby Abu-Jbara and Radev (2011), the authors noticedthe issue of having multiple references in one sen-tence.
They raised this issue when they discussedthe factors that impede the coherence and the read-ability of citation-based summaries.
They suggestedthat removing the fragments of a citing sentence thatare not relevant to the summarized paper will sig-nificantly improve the quality of the produced sum-maries.
In their work, they defined the scope of agiven reference as the shortest fragment of the citingsentence that contains the reference and could forma grammatical sentence if the rest of the sentencewas removed.
They identify the scope by generatingthe syntactic parse tree of the sentence and then find-ing the text that corresponds to the smallest subtreerooted at an S node and contains the target referencenode as one of its leaf nodes.
They admitted thattheir method was very basic and works only whenthe scope forms one grammatical fragment, whichis not true in many cases.Athar (2011) noticed the same issue with cit-ing sentences that cite multiple references, but thistime in the context of sentiment analysis in ci-tations.
He showed experimentally that identify-ing what he termed the scope of citation influ-ence improves sentiment classification accuracy.
Headapted the same basic method proposed by Abu-Jbara and Radev (2011).
We use this method as abaseline in our evaluation below.In addition to this related work, there is a largebody of research that used citing sentences in differ-ent applications.
For example, citing sentences havebeen used to summarize the contributions of a scien-tific paper (Qazvinian and Radev, 2008; Qazvinianet al, 2010; Qazvinian and Radev, 2010; Abu-Jbaraand Radev, 2011).
They have been also used togenerate surveys of scientific paradigms (Nanba andOkumura, 1999; Mohammad et al, 2009).
Severalother papers analyzed citing sentences to recognizethe citation function; i.e., the author?s reason for cit-ing a given paper (Nanba et al, 2000; Teufel et al,2006; Teufel, 2007).
Schwartz et al (2007) pro-posed a method for aligning the words within citingsentences that cite the same paper.
The goal of hiswork was to aid named entity recognition and para-phrase identification in scientific papers.82We believe that all the these applications will ben-efit from the output of our work.3 Problem DefinitionThe problem that we are trying to solve is to iden-tify which fragments of a given citing sentence thatcites multiple references are semantically relatedto a given target reference.
As stated above, wecall these fragments the reference scope.
Formally,given a citing sentence S = {w1, w2, ..., wn} wherew1, w2, ..., wn are the tokens of the sentence andgiven that S contains a set of two or more referencesR, we want to assign the label 1 to the word wi if itfalls in the scope of a given target reference r ?
R,and 0 otherwise.For example, sentences (4) and (5) below arelabeled for the target references Tetreault andChodorow (2008), and Cutting et al(1992) respec-tively.
The underlined words are labeled 1 (i.e.,inside the target reference scope), while all othersare labeled 0.
(4) For example, Tetreault and Chodorow (2008) use a maximumentropy classifier to build a model of correct preposition usage, with 7million instances in their training set, and Lee and Knutsson (2008)use memory-based learning, with 10 million sentences in their trainingset.
(5) There are many POS taggers developed using different techniquesfor many major languages such as transformation-based error-drivenlearning (Brill, 1995), decision trees (Black et al, 1992), Markovmodel (Cutting et al, 1992), maximum entropy methods (Ratnaparkhi,1996) etc for English.4 ApproachIn this section, we present our approach for address-ing the problem defined in the previous section.
Ourapproach involves two stages: 1) preprocessing and2) reference scope identification.
We present threealternative methods for the second stage.
The fol-lowing two subsections describe the two stages.4.1 Stage 1: PreprocessingThe goal of the preprocessing stage is to clean andprepare the citing sentence for the next processingsteps.
The second stage involves higher level textprocessing such as part-of-speech tagging, syntac-tic parsing, and dependency parsing.
The availabletools for these tasks are not trained on citing sen-tences which contain references written in a specialformat.
For example, it is very common in scien-tific writing to have references (usually written be-tween parentheses) that are not a syntactic part of thesentence.
It is also common to cite a group of ref-erences who share the same contribution by listingthem between parentheses separated by a comma ora semi-colon.
We address these issues to improvethe accuracy of the processing done in the secondstage.
The preprocessing stage involves three tasks:4.1.1 Reference TaggingThe first preprocessing task is to find and tag allthe references that appear in the citing sentence.Authors of scientific articles use standard patternsto include references in text.
We apply a regularexpression to find all the references that appearin a sentence.
We replace each reference with aplaceholder.
The target reference is replaced byTREF.
Each other reference is replaced by REF.We keep track of the original text of each replacedreference.
Sentence (6) below shows an example ofa citing sentence with the references replaced.
(6) These constraints can be lexicalized (REF.1; REF.2), un-lexicalized (REF.3; TREF.4) or automatically learned (REF.5;REF.6).4.1.2 Reference GroupingIt is common in scientific writing to attribute onecontribution to a group of references.
Sentence (6)above contains three groups of references.
Eachgroup constitutes one entity.
Therefore, we replaceeach group with a placeholder.
We use GTREFto replace a group of references that contains thetarget reference, and GREF to replace a group ofreferences that does not contain the target reference.83Sentence (7) below is the same as sentence (6) butwith the three groups of references replaced.
(7) These constraints can be lexicalized (GREF.1), unlexicalized(GTREF.2) or automatically learned (GREF.3).4.1.3 Non-syntactic Reference RemovalA reference (REF or TREF) or a group of refer-ences (GREF or GTREF) could either be a syntacticconstituent and has a semantic role in the sentence(e.g.
GTREF.1 in sentence (8) below) or not (e.g.REF.2 in sentence (8)).
(8) (GTREF.1) apply fuzzy techniques for integrating sourcesyntax into hierarchical phrase-based systems (REF.2).The task in this step is to determine whether a ref-erence is a syntactic component in the sentence ornot.
If yes, we keep it as is.
If not, we remove itfrom the sentence and keep track of its position.
Ac-cordingly, after this step, REF.2 in sentence (8) willbe removed.
We use a rule-based algorithm to deter-mine whether a reference should be removed fromthe sentence or kept.
Our algorithm (Algorithm 1)uses stylistic and linguistic features such as the styleof the reference, the position of the reference, andthe surrounding words to make the decision.When a reference is removed, we pick a wordfrom the sentence to represent it.
This is needed forfeature extraction in the next stage.
We use as a rep-resentative the head of the closest noun phrase (NP)that comes before the position of the removed refer-ence.
For example, in sentence (8) above, the closestNP before REF.2 is hierarchical phrase-based sys-tems and the head is the noun systems.4.2 Stage 2: Reference Scope IdentificationIn this section we present three different methodsfor identifying the scope of a given reference withina citing sentence.
We compare the performance ofthese methods in Section 5.
The following three sub-sections describe the methods.Algorithm 1 Remove Non-syntactic ReferencesRequire: A citing sentence S1: for all Reference R (REF, TREF, GREF, or GTREF)in S do2: if R style matches ?Authors (year)?
then3: Keep R // syntactic4: else if R is the first word in the sentence or in aclause then5: Keep R // syntactic6: else if R is preceded by a preposition (in, of, by,etc.)
then7: Keep R // syntactic8: else9: Remove R // non-syntactic10: end if11: end for4.2.1 Word ClassificationIn this method we define reference scope identifi-cation as a classification task of the individual wordsof the citing sentence.
Each word is classified asinside or outside the scope of a given target refer-ence.
We use a number of linguistic and structuralfeatures to train a classification model on a set oflabeled sentences.
The trained model is then usedto label new sentences.
The features that we use totrain the model are listed in Table 1.
We use theStanford parser (Klein and Manning, 2003) for syn-tactic and dependency parsing.
We experiment withtwo classification algorithms: Support Vector Ma-chines (SVM) and logistic regression.4.2.2 Sequence LabelingIn the method described in Section 4.2.1 above,we classify each word independently from the la-bels of the nearby words.
The nature of our task,however, suggests that the accuracy of word classifi-cation can be improved by considering the labels ofthe words surrounding the word being classified.
Itis very likely that the word takes the same label asthe word before and after it if they all belong to thesame clause in the sentence.
In this method we de-fine the problem as a sequence labeling task.
Now,instead of looking for the best label for each wordindividually, we look for the globally best sequence84Feature DescriptionDistance The distance (in words) between the word and the target reference.Position This feature takes the value 1 if the word comes before the target reference, and 0 otherwise.Segment After splitting the sentence into segments by punctuation and coordination conjunctions, this feature takesthe value 1 if the word occurs in the same segment with the target reference, and 0 otherwise.Part of speech tag The part of speech tag of the word, the word before, and the word after.Dependency Distance Length of the shortest dependency path (in the dependency parse tree) that connects the word to the tar-get reference or its representative.
It has been shown in previous work on relation extraction that theshortest path between any two entities captures the information required to assert a relationship betweenthem (Bunescu and Mooney, 2005)Dependency Relations This item includes a set of features.
Each features corresponds to a dependency relation type.
If the relationappears in the dependency path that connects the word to the target reference or its representative, itscorresponding feature takes the value 1, and 0 otherwise.Common Ancestor Node The type of the node in the syntactic parse tree that is the least common ancestor of the word and the targetreference.Syntactic Distance The number of edges in the shortest path that connects the word and the target reference in the syntacticparse tree.Table 1: The features used for word classification and sequence labelingof labels for all the words in the sentence at once.We use Conditional Random Fields (CRF) as oursequence labeling algorithm.
In particular, we usefirst-order chain-structured CRF.
The chain consistsof two sets of nodes: a set of hidden nodes Y whichrepresent the scope labels (0 or 1) in our case, anda set of observed nodes X which represent the ob-served features.
The task is to estimate the probabil-ity of a sequence of labels Y given the sequence ofobserved features X: P (Y|X)Lafferty et al (2001) define this probability to bea normalized product of potential functions ?
:P (y|x) =?t?k(yt, yt?1, x) (1)Where ?k(yt, yt?1, x) is defined as?k(yt, yt?1, x) = exp(?k?kf(yt, yt?1, x)) (2)where f(yt, yt?1, x) is a transition feature func-tion of the label at positions i ?
1 and i and theobservation sequence x; and ?j is parameter to beestimated from training data.
We use, as the obser-vations at each position, the same features that weused in Section 4.2.1 above (Table 1).4.2.3 Segment ClassificationWe noticed that the scope of a given referenceoften consists of units of higher granularity thanwords.
Therefore, in this method, we split thesentence into segments of contiguous words and,instead of labeling individual words, we labelthe whole segment as inside or outside the scopeof the target reference.
We experimented withtwo different segmentation methods.
In the firstmethod (method-1), we segment the sentence atpunctuation marks, coordination conjunctions, anda set of special expressions such as ?for example?,?for instance?, ?including?, ?includes?, ?such as?,?like?, etc.
Sentence (8) below shows an example ofthis segmentation method (Segments are enclosedin square brackets).
(8) [Rerankers have been successfully applied to numerous NLPtasks such as] [parse selection (GTREF)], [parse reranking (GREF)],[question-answering (REF)].In the second segmentation method (method-2),we split the sentence into segments of finer gran-ularity.
We use a chunking tool to identify noungroups, verb groups, preposition groups, adjective85groups, and adverb groups.
Each such group (orchunk) forms a segment.
If a word does not belongto any chunk, it forms a singleton segment byitself.
Sentence (9) below shows an example of thissegmentation method (Segments are enclosed insquare brackets).
(9) [To] [score] [the output] [of] [the coreference models],[we] [employ] [the commonly-used MUC scoring program (REF)][and] [the recently-developed CEAF scoring program (TREF)].We assign a label to each segment in two steps.
Inthe first step, we use the sequence labeling methoddescribed in Section 4.2.2 to assign labels to all theindividual words in the sentence.
In the second step,we aggregate the labels of all the words contained ina segment to assign a label to the whole segment.
Weexperimented with three different label aggregationrules: 1) rule-1: assign to the segment the majoritylabel of the words it contains, and 2) rule-2: assignto the segment the label 1 (i.e., inside) if at least oneof the words contained in the segment is labeled 1,and assign the label 0 to the segment otherwise, and3) rule-3: assign the label 0 to the segment if at leastof the words it contains is labeled 0, and assign 1otherwise.5 Evaluation5.1 DataWe use the ACL Anthology Network corpus(AAN) (Radev et al, 2009) in our evaluation.
AANis a publicly available collection of more than 19,000NLP papers.
AAN provides a manually curated cita-tion network of its papers and the citing sentence(s)associated with each edge.
The current release ofAAN contains about 76,000 unique citing sentences56% of which contain 2 or more references and 44%contain 1 reference only.
From this set, we ran-domly selected 3500 citing sentences, each contain-ing at least two references (3.75 references on aver-age with a standard deviation of 2.5).
The total num-ber of references in this set of sentences is 19,591.We split the data set into two random subsets:a development set (200 sentences) and a train-ing/testing set (3300 sentences).
We used the devel-opment set to study the data and develop our strate-gies of addressing the problem.
The second set wasused to train and test the system in a cross-validationmode.5.2 AnnotationWe asked graduate students with good backgroundin NLP (the area of the annotated sentences) to pro-vide three annotations for each sentence in the dataset described above.
First, we asked them to de-termine whether each of the references in the sen-tence was correctly tagged or not.
Second, we askedthem to determine for each reference whether it is asyntactic constituent in the sentence or not.
Third,we asked them to determine and label the scope ofone reference in each sentence which was markedas a target reference (TREF).
We designed a user-friendly tool to collect the annotations from the stu-dents.To estimate the inter-annotator agreement, wepicked 500 random sentences from our data set andassigned them to two different annotators.
The inter-annotator agreement was perfect on both the refer-ence tagging annotation and the reference syntacti-cality annotation.
This is expected since both are ob-jective, clear, and easy tasks.
To measure the inter-annotator agreement on the scope annotation task,we deal with it as a word classification task.
Thisallows us to use the popular classification agreementmeasure, the Kappa coefficient (Cohen, 1968).
TheKappa coefficient is defined as follows:K =P (A)?
P (E)1?
P (E)(3)where P(A) is the relative observed agreementamong raters and P(E) is the hypothetical probabil-ity of chance agreement.
The agreement betweenthe two annotators on the scope identification taskwas K = 0.61.
On Landis and Kochs (Landis andKoch, 1977) scale, this value indicates substantialagreement.865.3 Experimental SetupWe use the Edinburgh Language Technology TextTokenization Toolkit (LT-TTT) (Grover et al, 2000)for text tokenization, part-of-speech tagging, chunk-ing, and noun phrase head identification.
We usethe Stanford parser (Klein and Manning, 2003) forsyntactic and dependency parsing.
We use Lib-SVM (Chang and Lin, 2011) for Support Vector Ma-chines (SVM) classification.
Our SVM model uses alinear kernel.
We use Weka (Hall et al, 2009) for lo-gistic regression classification.
We use the MachineLearning for Language Toolkit (MALLET) (McCal-lum, 2002) for CRF-based sequence labeling.
Inall the scope identification experiments and resultsbelow, we use 10-fold cross validation for train-ing/testing.5.4 Preprocessing Component EvaluationWe ran our three rule-based preprocessing moduleson the testing data set and compared the output tothe human annotations.
The test set was not usedin the tuning of the system but was done using thedevelopment data set as described above.
We reportthe results for each of the preprocessing modules.Our reference tagging module achieved 98.3% pre-cision and 93.1% recall.
Most of the errors weredue to issues with text extraction from PDF or dueto bad references practices by some authors (i.e., notfollowing scientific referencing standards).
Our ref-erence grouping module achieved perfect accuracyfor all the correctly tagged references.
This wasexpected since this is a straightforward task.
Thenon-syntactic reference removal module achieved90.08% precision and 90.1% recall.
Again, most ofthe errors were the result of bad referencing prac-tices by the authors.5.5 Reference Scope IdentificationExperimentsWe conducted several experiments to compare themethods proposed in Section 4 and their variants.We ran all the experiments on the training/testingset (the 3300 sentences) described in Section 5.1.Method Accuracy Precision Recall F-measureAR-2011 54.0% 63.3% 33.1% 41.5%WC-SVM 74.9% 74.5% 93.4% 82.9%WC-LR 74.3% 76.8% 88.0% 82.0%SL-CRF 78.2% 80.1% 94.2% 86.6%SC-S1-R1 73.7% 72.1% 97.8% 83.0%SC-S1-R2 69.3% 68.4% 98.9% 80.8%SC-S1-R3 60.0% 61.8% 73.3% 60.9%SC-S2-R1 81.8% 81.2% 93.8% 87.0%SC-S2-R2 78.2% 77.3% 94.9% 85.2%SC-S2-R3 66.1% 67.1% 71.2% 69.1%Table 3: Results of scope identification using the differentalgorithms described in the paperThe experiments that we ran are as follows: 1) wordclassification using a SVM classifier (WC-SVM);2) word classification using a logistic regressionclassifier(WC-LR); 3) CRF-based sequence labeling(SL-CRF); 4) segment classification using segmen-tation method-1 and label aggregation rule-1 (SC-S1-R1); 5,6,7,8,9) same as (4) but using differentcombinations of segmentation methods 1 and 2, andlabel aggregation rules 1,2 and 3: SC-S1-R2, SC-S1-R3, SC-S2-R1, SC-S2-R2, SC-S2-R3 (where Sxrefers to segmentation method x and Ry refers tolabel aggregation rule y all as explained in Sec-tion 4.2.3).
Finally, 10) we compare our meth-ods to the baseline method proposed by Abu-Jbaraand Radev (2011) which was described in Section 4(AR-2011).To better understand which of the features listedin Table 1 are more important for the task, we useGuyon et al?s (2002) method for feature selectionusing SVM to rank the features based on their im-portance.
The results of the experiments and thefeature analysis are presented and discussed in thefollowing subsection.5.6 Results and Discussion5.6.1 Experimental ResultsWe ran the experiments described in the previ-ous subsection on the testing data described in Sec-87Method OutputExample1 Word Classification(WC-SVM)A wide range of contextual information, such as surrounding words (GREF ), dependency or case structure(GTREF ), and dependency path (GREF ), has been utilized for similarity calculation, and achieved consid-erable success.Sequence Labeling (SL-CRF)A wide range of contextual information, such as surrounding words (GREF), dependency or case structure(GTREF), and dependency path (GREF ), has been utilized for similarity calculation, and achieved consid-erable success.Segment Classification(SC-S2-R1)A wide range of contextual information, such as surrounding words (GREF ), dependency or case structure(GTREF ), and dependency path (GREF ), has been utilized for similarity calculation, and achievedconsiderable success.Example2 Word Classification(WC-SVM)Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering(REF).Sequence Labeling (SL-CRF)Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering(REF).Segment Classification(SC-S2-R1)Some approaches have used WordNet for the generalization step (GTREF), others EM-based clustering(REF).Table 2: Two example outputs produced by the three methodstion 5.1.
Table 3 compares the precision, recall, F1,and accuracy for the three methods described in Sec-tion 4 and their variations.
All the metrics were com-puted at the word level.
The results show that all ourmethods outperform the baseline method AR-2011that was proposed by Abu-Jbara and Radev (2011).In the word classification method, we notice no sig-nificant difference between the performance of theSVM vs Logistic Regression classifier.
We also no-tice that the CRF-based sequence labeling methodperforms significantly better than the word classi-fication method.
This result corroborates our intu-ition that the labels of neighboring words are de-pendent.
The results also show that segment la-beling generally performs better than word label-ing.
More specifically, the results indicate that seg-mentation based on chunking and the label aggre-gation based on plurality when used together (i.e.,SC-S2-R1) achieve higher precision, accuracy, andF-measure than the punctuation-based segmentationand the other label aggregation rules.Table 2 shows the output of the three methods ontwo example sentences.
The underlined words arelabeled by the system as scope words.5.6.2 Feature AnalysisWe performed an analysis of our classificationfeatures using Guyon et al (2002) method.
Theanalysis revealed that both structural and syntacticfeatures are important.
Among the syntactic fea-tures, the dependency path is the most important.Among the structural features, the segment feature(as described in Table 1) is the most important.6 ConclusionsWe presented and compared three different meth-ods for reference scope identification: word classi-fication, sequence labeling, and segment classifica-tion.
Our results indicate that segment classificationachieves the best performance.
The next direction inthis research is to extract the scope of a given refer-ence as a standalone grammatical sentence.
In manycases, the scope identified by our method can forma grammatical sentence with no or minimal postpro-cessing.
In other cases, more advanced text regener-ation techniques are needed for scope extraction.ReferencesAmjad Abu-Jbara and Dragomir Radev.
2011.
Coherentcitation-based summarization of scientific papers.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-88guage Technologies, pages 500?509, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Awais Athar.
2011.
Sentiment analysis of citations us-ing sentence structure-based features.
In Proceedingsof the ACL 2011 Student Session, pages 81?87, Port-land, OR, USA, June.
Association for ComputationalLinguistics.Christine L. Borgman and Jonathan Furner.
2002.
Schol-arly communication and bibliometrics.
ANNUAL RE-VIEW OF INFORMATION SCIENCE AND TECH-NOLOGY, 36(1):2?72.Razvan Bunescu and Raymond Mooney.
2005.
A short-est path dependency kernel for relation extraction.
InProceedings of Human Language Technology Confer-ence and Conference on Empirical Methods in Nat-ural Language Processing, pages 724?731, Vancou-ver, British Columbia, Canada, October.
Associationfor Computational Linguistics.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM:A library for support vector machines.
ACM Transac-tions on Intelligent Systems and Technology, 2:27:1?27:27.J.
Cohen.
1968.
Weighted kappa: Nominal scale agree-ment with provision for scaled disagreement or partialcredit.
Psychological Bulletin, 70:213?220.Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes?
Erkan,David States, and Dragomir Radev.
2008.
Blind menand elephants: What do citation summaries tell usabout a research article?
J.
Am.
Soc.
Inf.
Sci.
Tech-nol., 59(1):51?62.E.
Garfield, Irving H. Sher, and R. J. Torpie.
1984.
TheUse of Citation Data in Writing the History of Science.Institute for Scientific Information Inc., Philadelphia,Pennsylvania, USA.Claire Grover, Colin Matheson, Andrei Mikheev, andMarc Moens.
2000.
Lt ttt - a flexible tokenisationtool.
In In Proceedings of Second International Con-ference on Language Resources and Evaluation, pages1147?1154.Isabelle Guyon, Jason Weston, Stephen Barnhill, andVladimir Vapnik.
2002.
Gene selection for cancerclassification using support vector machines.
Mach.Learn., 46:389?422, March.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18.T.
L. Hodges.
1972.
Citation indexing-its theoryand application in science, technology, and humani-ties.
Ph.D. thesis, University of California at Berke-ley.Ph.D.
thesis, University of California at Berkeley.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In IN PROCEEDINGS OFTHE 41ST ANNUAL MEETING OF THE ASSOCIA-TION FOR COMPUTATIONAL LINGUISTICS, pages423?430.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the Eighteenth InternationalConference on Machine Learning, ICML ?01, pages282?289, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.J.
Richard Landis and Gary G. Koch.
1977.
The Mea-surement of Observer Agreement for Categorical Data.Biometrics, 33(1):159?174, March.Terttu Luukkonen.
1992.
Is scientists?
publishing be-haviour rewardseeking?
Scientometrics, 24:297?319.10.1007/BF02017913.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Qiaozhu Mei and ChengXiang Zhai.
2008.
Generatingimpact-based summaries for scientific literature.
InProceedings of ACL-08: HLT, pages 816?824, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Saif Mohammad, Bonnie Dorr, Melissa Egan, AhmedHassan, Pradeep Muthukrishan, Vahed Qazvinian,Dragomir Radev, and David Zajic.
2009.
Using ci-tations to generate surveys of scientific paradigms.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 584?592, Boulder, Colorado, June.
Associationfor Computational Linguistics.Preslav I. Nakov, Ariel S. Schwartz, and Marti A. Hearst.2004.
Citances: Citation sentences for semantic anal-ysis of bioscience text.
In In Proceedings of the SI-GIR04 workshop on Search and Discovery in Bioin-formatics.Hidetsugu Nanba and Manabu Okumura.
1999.
To-wards multi-paper summarization using reference in-formation.
In IJCAI ?99: Proceedings of the Six-teenth International Joint Conference on Artificial In-telligence, pages 926?931, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Hidetsugu Nanba, Noriko Kando, Manabu Okumura, andOf Information Science.
2000.
Classification of re-search papers using citation links and citation types:Towards automatic review article generation.Vahed Qazvinian and Dragomir R. Radev.
2008.
Scien-tific paper summarization using citation summary net-works.
In Proceedings of the 22nd International Con-ference on Computational Linguistics (Coling 2008),pages 689?696, Manchester, UK, August.
Coling 2008Organizing Committee.89Vahed Qazvinian and Dragomir R. Radev.
2010.
Identi-fying non-explicit citing sentences for citation-basedsummarization.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 555?564, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Vahed Qazvinian, Dragomir R. Radev, and ArzucanOzgur.
2010.
Citation summarization throughkeyphrase extraction.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics(Coling 2010), pages 895?903, Beijing, China, Au-gust.
Coling 2010 Organizing Committee.Dragomir R. Radev, Pradeep Muthukrishnan, and VahedQazvinian.
2009.
The acl anthology network corpus.In NLPIR4DL ?09: Proceedings of the 2009 Workshopon Text and Citation Analysis for Scholarly Digital Li-braries, pages 54?61, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Ariel Schwartz, Anna Divoli, and Marti Hearst.
2007.Multiple alignment of citation sentences with con-ditional random fields and posterior decoding.
InProceedings of the 2007 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL), pages 847?857.Advaith Siddharthan and Simone Teufel.
2007.
Whoseidea was this, and why does it matter?
attributingscientific work to citations.
In In Proceedings ofNAACL/HLT-07.Simone Teufel, Advaith Siddharthan, and Dan Tidhar.2006.
Automatic classification of citation function.
InIn Proc.
of EMNLP-06.Simone Teufel.
2007.
Argumentative zoning for im-proved citation indexing.
computing attitude and affectin text.
In Theory and Applications, pages 159170.Howard D. White.
2004.
Citation analysis and discourseanalysis revisited.
Applied Linguistics, 25(1):89?116.90
