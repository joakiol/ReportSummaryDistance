Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 300?305,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsDetecting Missing Hyphens in Learner TextAoife Cahill?, Martin Chodorow?, Susanne Wolff?
and Nitin Madnani??
Educational Testing Service, 660 Rosedale Road, Princeton, NJ 08541, USA{acahill, swolff, nmadnani}@ets.org?
Hunter College and the Graduate Center, City University of New York, NY 10065, USAmartin.chodorow@hunter.cuny.eduAbstractWe present a method for automatically de-tecting missing hyphens in English text.
Ourmethod goes beyond a purely dictionary-basedapproach and also takes context into account.We evaluate our model on artificially gener-ated data as well as naturally occurring learnertext.
Our best-performing model achieveshigh precision and reasonable recall, makingit suitable for inclusion in a system that givesfeedback to language learners.1 IntroductionWhile errors of punctuation are not as frequent, noroften as serious, as some of the other typical mis-takes that learners make, they are nevertheless animportant consideration for students aiming to im-prove the overall quality of their writing.
In this pa-per we focus on the error of missing hyphens.
Thefollowing example is a typical mistake made by astudent writer:(1) Schools may have more after school sports.In this case the tokens after and school should be hy-phenated as they modify the noun sports.
However,in Example (2) a hyphen between after and schoolwould be incorrect, since in this instance after func-tions as as the head of a prepositional phrase modi-fying went.
(2) I went to the dentist after school today.These examples illustrate that purely dictionary-based approaches to detecting missing hyphens arenot likely to be sophisticated enough to differentiatethe contexts in which a hyphen is required.
In addi-tion, learner text frequently contains other grammat-ical and spelling errors, further complicating auto-matic error detection.
Example (3) contains an errorfather like instead of father likes to.
This causes dif-ficulty for automated hyphenation systems becauselike is a frequent suffix of hyphenated words andplay can function as a noun.
(3) My father like play basketball with me.In this paper, we propose a classifier-based approachto automatically detecting missing hyphen errors.The goal of our system is to detect missing hyphenerrors and provide feedback to language learners.Therefore, we place more importance on the preci-sion of the system than recall.
We train our model onfeatures that take the context of a pair of words intoaccount, as well as other discriminative features.
Wepresent a number of evaluations on both artificiallygenerated errors and naturally occurring learner er-rors and show that our classifiers achieve high preci-sion and reasonable recall.2 Related WorkThe task of detecting missing hyphens is related toprevious work on detecting punctuation errors.
Oneof the classes of errors in the Helping Our Own(HOO) 2011 shared task (Dale and Kilgarriff, 2011)was punctuation.
Comma errors are the most fre-quent kind of punctuation error made by learners.
Is-rael et al(2012) present a model for detecting thesekinds of errors in learner texts.
They train CRF mod-els on sentences from unedited essays written byhigh-level college students and show that they per-forms well on detecting errors in learner text.
As300far as we are aware, the HOO 2011 system descrip-tion of Rozovskaya et al(2011) is the only work tospecifically reference hyphen errors.
They use rulesderived from frequencies in the training corpus todetermine whether a hyphen was required betweentwo words separated by white space.The task of detecting missing hyphens is relatedto the task of inserting punctuation into the output ofunpunctuated text (for example, the output of speechrecognition, automatic generation, machine transla-tion, etc.).
Systems that are built on the output ofspeech recognition can obviously take features likeprosody into account.
In our case, we are deal-ing only with written text.
Gravano et al(2009)present an n-gram-based model for automaticallyadding punctuation and capitalization to the outputof an ASR system, without taking any of the speechsignal information into account.
They conclude thatmore training data, rather than wider n-gram con-texts leads to a greater improvement in accuracy.3 BaselinesWe implement three baseline systems which we willlater compare to our classification approach.
Thefirst baseline is a na?
?ve heuristic that predicts a miss-ing hyphen between bigrams that appear hyphenatedin the Collins Dictionary.1 As a somewhat less-na?
?ve baseline, we implement a heuristic that pre-dicts a missing hyphen between bigrams that occurhyphenated more than 1,000 times in Wikipedia.
Athird baseline is a heuristic that predicts a missinghyphen between bigrams where the probability ofthe hyphenated form as estimated from Wikipediais greater than 0.66, meaning that the hyphenatedbigram is twice as likely as the non-hyphenated bi-gram.
This baseline is similar to the approach takenby Rozovskaya et al(2011), except that the proba-bilities are estimated from a much larger corpus.4 System DescriptionUsing the features in Table 1, we build a logis-tic regression model which assigns a probability tothe likelihood of a hyphen occurring between twowords, wi and wi+1.
As we are primarily interestedin using this system for giving feedback to languagelearners, we require very high precision.
Therefore,1LDC catalog number LDC93T1Tokens wi?1, wi, wi+1, wi+2Stems si?1, si, si+1, si+2Tags ti?1, ti, ti+1, ti+2Bigrams wi?wi+1, si?si+1, ti?ti+1Dict Does the hyphenated form appear inthe Collins dictionary?Prob What is the probability of the wordbigram appearing hyphenated inWikipedia?Distance Distance to following and preced-ing verb, nounVerb/Noun Is there a verb/noun preced-ing/following this bigramTable 1: Features used in all models.
Positive in-stances are those where there was a hyphen betweenwi and wi+1 in the data.
Stems are generated usingNLTK?s implementation of the Lancaster Stemmer,and tags are obtained from the Stanford Parser.we only predict a missing hyphen error when theprobability of the prediction is >0.99.We experiment with two different sources oftraining data, in addition to their combination.
Wefirst train on well-edited text, using almost 1.8 mil-lion sentences from the San Jose Mercury News cor-pus.2 For training, hyphenated words are automati-cally split (i.e.
well-known becomes well known).The positive examples for the classifier are all bi-grams where a hyphen was removed.
Negative ex-amples consist of bigrams where there was no hy-phen in the training data.
Since this is over 99% ofthe data, we randomly sample 3% of the negativeexamples for training.
We also restrict the negativeexamples to only the most likely contexts, where acontext is defined as a part-of-speech bigram.
A listof possible contexts in which hyphens occur is ex-tracted from the entire training set.
Only contextsthat occur more than 20 times are selected duringtraining.
All contexts are evaluated during testing.Table 2 lists some of the most frequent contexts withexamples of when they should be hyphenated andwhen they should remain unhyphenated.The second data source for training the modelcomes from pairs of revisions from Wikipedia ar-ticles.
Following Cahill et al(2013), we automati-cally extract a corpus of error annotations for miss-2LDC catalog number LDC93T3A.301Context Hyphenated UnhyphenatedNN NN terrific truck-stopwaitressa quake insurancesurchargeCD CD Twenty-two thou-sandthe 126 millionAmericansJJ NN an early-morningblazean entire practicesessionCD NN a two-year contract about 600 tank carsNN VBN a court-orderedprograma letter delivered to-dayTable 2: Some frequent likely POS contexts for hy-phenation, with examples from the Brown corpus.ing hyphens.
This is done by extracting the plaintext from every revision to every article and com-paring adjacent pairs of revisions.
For each article,chains of errors are detected, using the surroundingtext to identify them.
When a chain begins and endswith the same form, it is ignored.
Only the first andlast points in an error chain are retained for train-ing.
An example chain is the following: It has beenan ancient {focal point ?
location ?
focal point?
focal-point} of trade and migration., where wewould extract the correction focal point ?
focal-point.
In total, we extract a corpus of 390,298 sen-tences containing missing hyphen error annotations.Finally, we combine both data sources.5 Evaluating on Artificial DataSince there are large corpora of well-edited textreadily available, it is easy to evaluate on artifi-cial data.
For testing, we take 24,243 sentencesfrom the Brown corpus and automatically removehyphens from the 2,072 hyphenated words (but notfree-standing dashes).
Each system makes a predic-tion for all bigrams about whether a hyphen shouldappear between the pair of words.
We measure theperformance of each system in terms of precision, P,(how many of the missing hyphen errors predictedby the system were true errors), recall, R, (how manyof the artificially removed hyphens the system de-tected as errors) and f-score, F, (the harmonic meanof precision and recall).
The results are given inTable 3, and also include the raw number of truepositives, TP, detected by each system.
The resultsshow that the baseline using Wikipedia probabilitiesobtains the highest precision, however with low re-call.
The classifiers trained on newswire text and theTP P R FBaselineCollins dict 397 40.5 19.2 26.0Wiki Counts-1000 359 39.1 17.3 24.0Wiki Probs-0.66 811 85.5 39.1 53.7ClassifierSJM-trained 1097 82.0 52.9 64.3Wiki-revision-trained 1061 72.8 51.2 60.1Combined 1106 80.9 53.4 64.3Table 3: Results of evaluating on the Brown Corpuswith hyphens removedcombined news and Wikipedia revision text achievethe highest overall f-score.
Figure (1a) shows thePrecision Recall curves for the Wikipedia baselinesand the three classifiers.
The curves mirror the re-sults in the table, showing that the classifier trainedon the newswire text, and the classifier trained on thecombined data perform best.
The Wikipedia countsbaseline performs worst.6 Evaluating on Learner TextWe carry out two evaluations of our system onlearner text.
We first evaluate on the missing hyphenerrors contained in the CLC-FCE (Yannakoudakis etal., 2011).
This corpus contains 1,244 exam scriptswritten by learners of English as part of the Cam-bridge ESOL First Certificate in English.
In total,there are 173 instances of missing hyphen errors.The results are given in Table 4, and the precisionrecall curves are displayed in Figure (1b).The results show that the classifiers consistentlyachieve high precision on this data set.
This is asexpected, given the high threshold set.
Looking atthe curves, it seems that a slightly lower threshold inthis case may lead to better results.
The curves showthat the combined classifier is performing slightlybetter than the other two classifiers.
The baselinesare clearly not performing as well on this dataset.While the overall size of the CLC-FCE data setis quite large, the low frequency of this kind of er-ror means that the evaluation was carried out on arelatively small number of examples.
For this rea-son, the reliability of the results may be called intoquestion.
There is, for instance, a striking differencebetween the f-scores for the Collins Dictionary base-3020.0 0.2 0.4 0.6 0.80.00.20.40.60.81.0RecallPrecisioncombinedsjmwikibaseline?wiki?countsbaseline?wiki?probs(a) Brown Corpus0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0RecallPrecisioncombinedsjmwikibaseline?wiki?countsbaseline?wiki?probs(b) CLC-FCE CorpusFigure 1: Precision Recall curves for the Wikipedia baselines and the three classifiers.TP P R FBaselineCollins dict 131 64.5 75.7 69.7Wiki Counts-1000 141 73.1 81.5 77.0Wiki Probs-0.66 36 92.3 20.8 34.0ClassifierSJM-trained 60 84.5 34.7 49.2Wiki-revision-trained 71 98.6 41.0 58.0Combined 66 98.5 38.2 55.0Table 4: Results of evaluating on the CLC-FCEdatasetline on the Brown corpus (26.0) and on the learnerdata (69.7).
Inspection of the 131 true positives forthe learner data reveal that 87 of these are cases of asingle type, the word ?make-up?, which students of-ten wrote without a hyphen in response to a promptabout a fashion and leisure show.
Since the hyphen-ated form was in the Collins Dictionary, the base-line system was credited with detection of this error.However, when the 87 occurrences of ?make up?
areremoved from the data set, the values of precision,recall and f-score for the Collins Dictionary baselinefall to 37.9, 51.2, and 42.9, respectively.
This pointsto a problem for system evaluation that is more gen-303eral than the low frequency of an error type, suchas missing hyphens.
The more general problem isthat of non-independence among errors, which oc-curs when an individual writer contributes multipletimes to an error count or when a particular promptgives rise to many occurrences of the same error, asin the current case of ?make-up?.Despite the problem of non-independent errors, amore accurate picture of system performance maynonetheless emerge with more evidence.
Therefore,we evaluate system precision on a data set of 1,000student GRE and TOEFL essays written by both na-tive and nonnative speakers, across a wide range ofproficiency levels and prompts.
The essays, drawnfrom 295 prompts, ranged in length from 1 to 50sentences, with an average of 378 words per essay.We manually inspect a random sample of 100 in-stances where each system detected a missing hy-phen.
Two native-English speakers judged the cor-rectness of the predictions using the Chicago Man-ual of Style as a guide.3 Inter-annotator agreementon the binary classification task for 600 items was0.79?, showing high agreement.
The results aregiven in Table 5.Total Judge-1 Judge 2Predictions Precision PrecisionBaselineCollins dict 416 11 8Wiki Counts 2185 20 21Wiki Probs 224 54 52ClassifierSJM-trained 421 62 69Wiki-revision 577 43 41Combined 450 60 62Table 5: Precision results on 1000 student responses,estimated by randomly sampling 100 hyphen predic-tions of each system and manually evaluating them.The results show that the first two baseline sys-tems do not perform well on this essay data.
Thisis mainly because they do not take context into ac-count.
Many of the errors made by these systems in-volved verb + preposition bigrams, as in Examples(4) and (5).
Restricting the detection by probabilityclearly improves precision, but at the cost of recall3http://www.chicagomanualofstyle.org(only 224 total instances of missing hyphen errorsdetected, the lowest of all 6 systems).
In the man-ual evaluation, the system trained on the SJM corpusachieves the highest precision, though all precisionfigures are lower than the previous evaluations.
Ex-ample (6) is a typical example of the kinds of falsepositives made by the classifier models.
(4) If these men were required to step-down after alimited number of years, the damage would becontained.
(5) These families may even choose to eat at-homethan outside.
(6) The wellness program will save money in thelong-term.Future work will explore additional features thatmay help improve performance.
A more thoroughstudy will also be carried out to fully understand thedifferences in performance of the classifiers acrosscorpora.
Another direction to explore in future workis the related task of identifying extraneous hyphensin learner text.
These are even less frequent thanmissing hyphens (87 annotated cases in the CLC-FCE corpus), but we believe a similar classificationapproach could be successful.7 ConclusionIn this paper we presented a model for automaticallydetecting missing hyphen errors in learner text.
Weexperimented with two kinds of training data, onewell-edited text, and the other an automatically ex-tracted corpus of error annotations.
When evaluat-ing on artificially generated errors in otherwise well-edited text, the classifiers generally performed bet-ter than the baseline systems.
When evaluating onthe small number of missing hyphen errors in theCLC-FCE corpus, the word-based models did well,though the classifiers also achieved consistently highprecision.
A precision-only evaluation on a sampleof learner essays resulted in overall lower scores, butthe classifier trained on well-edited text performedbest.
In general, the classifiers outperform the base-line, especially in terms of precision, showing thattaking context into account when detecting thesekinds of errors is important.304ReferencesAoife Cahill, Nitin Madnani, Joel Tetreault, and DianeNapolitano.
2013.
Robust Systems for PrepositionError Correction Using Wikipedia Revisions.
In Pro-ceedings of the Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, Atlanta, GA.Robert Dale and Adam Kilgarriff.
2011.
Helping OurOwn: The HOO 2011 Pilot Shared Task.
In Pro-ceedings of the Generation Challenges Session at the13th EuropeanWorkshop on Natural Language Gener-ation, pages 242?249, Nancy, France, September.
As-sociation for Computational Linguistics.Agustin Gravano, Martin Jansche, and Michiel Bacchi-ani.
2009.
Restoring punctuation and capitalization intranscribed speech.
In Acoustics, Speech and SignalProcessing, 2009.
ICASSP 2009.
IEEE InternationalConference on, pages 4741?4744.
IEEE.Ross Israel, Joel Tetreault, and Martin Chodorow.
2012.Correcting Comma Errors in Learner Essays, andRestoring Commas in Newswire Text.
In Proceed-ings of the 2012 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 284?294, Montre?al, Canada, June.
Association for Compu-tational Linguistics.Alla Rozovskaya, Mark Sammons, Joshua Gioja, andDan Roth.
2011.
University of Illinois System inHOO Text Correction Shared Task.
In Proceedingsof the Generation Challenges Session at the 13th Eu-ropean Workshop on Natural Language Generation,pages 263?266, Nancy, France, September.
Associa-tion for Computational Linguistics.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A New Dataset and Method for Automati-cally Grading ESOL Texts.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 180?189, Portland, Oregon, USA, June.
Associ-ation for Computational Linguistics.305
