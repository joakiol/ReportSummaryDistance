Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 486?496,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Word Reordering Model for Improved Machine TranslationKarthik VisweswariahIBM Research IndiaBangalore, Indiav-karthik@in.ibm.comRajakrishnan RajkumarDept.
of LinguisticsOhio State Universityraja@ling.osu.eduAnkur GandheIBM Research IndiaBangalore, Indiaankugand@in.ibm.comAnanthakrishnan RamanathanIBM Research IndiaBangalore, Indiaaramana2@in.ibm.comJiri NavratilIBM T.J. Watson Research CenterYorktown Heights, New Yorkjiri@us.ibm.comAbstractPreordering of source side sentences hasproved to be useful in improving statisticalmachine translation.
Most work has used aparser in the source language along with rulesto map the source language word order intothe target language word order.
The require-ment to have a source language parser is a ma-jor drawback, which we seek to overcome inthis paper.
Instead of using a parser and thenusing rules to order the source side sentencewe learn a model that can directly reordersource side sentences to match target word or-der using a small parallel corpus with high-quality word alignments.
Our model learnspairwise costs of a word immediately preced-ing another word.
We use the Lin-Kernighanheuristic to find the best source reordering ef-ficiently during training and testing and showthat it suffices to provide good quality reorder-ing.We show gains in translation performancebased on our reordering model for translatingfrom Hindi to English, Urdu to English (witha public dataset), and English to Hindi.
ForEnglish to Hindi we show that our techniqueachieves better performance than a methodthat uses rules applied to the source side En-glish parse.1 IntroductionLanguages differ in the way they order words to pro-duce sentences representing the same meaning.
Ma-chine translation systems need to reorder words inthe source sentence to produce fluent output in thetarget language that preserves the meaning of thesource sentence.Current phrase based machine translation systemscan capture short range reorderings via the phrasetable.
Even the capturing of these local reorderingphenomena is constrained by the amount of trainingdata available.
For example, if adjectives precedenouns in the source language and follow nouns in thetarget language we still need to see a particular ad-jective noun pair in the parallel corpus to handle thereordering via the phrase table.
Phrase based sys-tems also rely on the target side language model toproduce the right target side order.
This is knownto be inadequate (Al-Onaizan and Papineni, 2006),and this inadequacy has spurred various attempts toovercome the problem of handling differing wordorder in languages.One approach is through distortion models, thattry to model which reorderings are more likelythan others.
The simplest models just penalizelong jumps in the source sentence when producingthe target sentence.
These models have also beengeneralized (Al-Onaizan and Papineni, 2006; Till-man, 2004) to allow for lexical dependencies on thesource.
While these models are simple, and canbe integrated with the decoder they are insufficientto capture long-range reordering phenomena espe-cially for language pairs that differ significantly.The weakness of these simple distortion modelshas been overcome using syntax of either the sourceor target sentence (Yamada and Knight, 2002; Gal-ley et al, 2006; Liu et al, 2006; Zollmann and Venu-gopal, 2006).
While these methods have shown tobe useful in improving machine translation perfor-486mance they generally involve joint parsing of thesource and target language which is significantlymore computationally expensive when compared tophrase based translation systems.
Another approachthat overcomes this weakness, is to to reorder thesource sentence based on rules applied on the sourceparse (either hand written or learned from data) bothwhen training and testing (Collins et al, 2005; Gen-zel, 2010; Visweswariah et al, 2010).In this paper we propose a novel method for deal-ing with the word order problem that is efficient anddoes not rely on a source or target side parse beingavailable.
We cast the word ordering problem as aTraveling Salesman Problem (TSP) based on previ-ous work on word-based and phrased-based statis-tical machine translation (Tillmann and Ney, 2003;Zaslavskiy et al, 2009).
Words are the cities in theTSP and the objective is to learn the distance be-tween words so that the shortest tour corresponds tothe ordering of the words in the source sentence inthe target language.
We show that the TSP distancesfor reordering can be learned from a small amountof high-quality word alignment data by means ofpairwise word comparisons and an informative fea-ture set involving words and part-of-speech (POS)tags adapted and extended from prior work on de-pendency parsing (McDonald et al, 2005b).
Ob-taining high-quality word alignments that we requirefor training is fairly easy compared with obtaining atreebank required to obtain parses for use in syntaxbased methods.We show experimentally that our reorderingmodel, even when used to reorder sentences fortraining and testing (rather than being used as anadditional score in the decoder) improves machinetranslation performance for: Hindi ?
English, En-glish?Hindi, and Urdu?
English.
Although Urduis similar to Hindi from the point of reordering phe-nomena we include it in our experiments since thereare publicly available datasets for Urdu-English.
ForEnglish ?
Hindi we obtained better machine trans-lation performance with our reordering model ascompared to a method that uses reordering rules ap-plied to the source side parse.The rest of the paper is organized as follows.
Sec-tion 2 reviews related work and places our work incontext.
Section 3 outlines reordering issues dueto syntactic differences between Hindi and English.Section 4 presents our reordering model, Section 5presents experimental results and Section 6 presentsour conclusions and possible future work.2 Related workThere have been several studies demonstrating im-proved machine translation performance by reorder-ing source side sentences based on rules applied tothe source side parse during training and decoding.Much of this work has used hand written rules andseveral language pairs have been studied e.g Germanto English (Collins et al, 2005), Chinese to English(Wang et al, 2007), English to Hindi (Ramanathanet al, 2009), English to Arabic (Badr et al, 2009)and Japanese to English (Lee et al, 2010).
Therehave also been some studies where the rules arelearned from the data (Genzel, 2010; Visweswariahet al, 2010; Xia and McCord, 2004).
In additionthere has been work (Yamada and Knight, 2002;Zollmann and Venugopal, 2006; Galley et al, 2006;Liu et al, 2006) which uses source and/or targetside syntax in a Context Free Grammar frameworkwhich results in machine translation decoding beingconsidered as a parsing problem.
In this paper wepropose a model that does not require either sourceor target side syntax while also preserving the effi-ciency of reordering techniques based on rules ap-plied to the source side parse.In work that is closely related to ours, (Trombleand Eisner, 2009) formulated word reordering as aLinear Ordering Problem (LOP), an NP-hard permu-tation problem.
They learned LOP model weightscapable of assigning a score to every possible per-mutation of the source language sentence from analigned corpus by using a averaged perceptron learn-ing model.
The key difference between our modeland the model in (Tromble and Eisner, 2009) is thatwhile they learn costs of a word wi appearing any-where before wj , we learn costs of wi immediatelypreceding wj .
This results in more compact modelsand (as we show in Section 5) better models.Our model results in us having to solve a TSPinstance.
The relation between the TSP and ma-chine translation decoding has been explored before.
(Knight, 1999) showed that TSP is a sub-class ofMTdecoding and thus established that the latter is NP-hard.
(Zaslavskiy et al, 2009) casts phrase-based487decoding as a TSP and they show favorable speedperformance trade-offs compared with Moses, anexisting state-of-the-art decoder.
In (Tillmann andNey, 2003), a beam-search algorithm used for TSPis adapted to work with an IBM-4 word-based modeland phrase-based model respectively.
As opposedto calculating TSP distances from existing machinetranslation components ( viz.
the translation, dis-tortion and language model probabilities) we learnmodel weights to reorder source sentences to matchtarget word order using an informative feature setadapted from graph-based dependency parsing (Mc-Donald et al, 2005a).3 Hindi-English reordering issuesThis section provides a brief survey of constructionsthat the two languages in question differ as well ashave in common.
(Ramanathan et al, 2009) notesthe following divergences:?
English follows SVO order while Hindi followsSOV order?
English uses prepositions while Hindi usespost-positions?
Hindi allows greater word order freedom?
Hindi has a relatively richer case-marking sys-temIn addition to these differences, (Visweswariah etal., 2010) mention the similarity in word order inthe case of adjective noun sequences (some booksvs.
kuch kitab).4 Reordering modelConsider a source sentence w consisting of a se-quence of n words w1, w2, ... wn that we wouldlike to reorder into the target language order.
Givena permutation pi of the indices 1..n, let the candi-date reordering be wpi1 , wpi2 , ..., wpin .
Thus, pii de-notes the index of the word in the source sentencethat maps to position i in the candidate reordering.Clearly there are n!
such permutations.
Our reorder-ing model assigns costs to candidate permutationsas:C(pi|w) =?ic(pii?1, pii).The cost c(m,n) can be thought of as the cost of theword at index m immediately preceding the wordwith index n in the candidate reordering.
In this pa-per, we parametrize the costs as:c(m,n) = ?T?
(w,m, n),where ?
is a learned vector of weights and ?
is avector of feature functions.Given a source sentence w we reorder it accord-ing to the permutation pi that minimizes the costC(pi|w).
Thus, we would like our cost functionC(pi|w) to be such that the correct reordering pi?
hasthe lowest cost of all possible reorderings pi.
In Sec-tion 4.1 we describe the features ?
that we use, andin Section 4.2 we describe how we train the weights?
to obtain a good reordering model.Given our model structure, the minimizationproblem that we need to solve is identical to solvinga Asymmetric Traveling Salesman Problem (ATSP)with each word corresponding to a city, and the costsc(m,n) representing the pairwise distances betweenthe cities.
Consider the following example:English input: John eats applesHindi: John seba(apples) khaataa hai(eats)Desired reordered English: John apples eatsThe ATSP that we need to solve is representedpictorially in Figure 1 with sample costs.
Note thatwe have one extra node numbered 0.
We start andend the tour at node 0, and this determines the firstword in the reordered sentence.
In this example theminimum cost tour is:Start ?
John ?
apple ?
eatsrecovering the right reordering for translation intoHindi.Solving the ATSP (which is a well known NP hardproblem) efficiently is crucial for the efficiency ofour reordering model.
To solve the ATSP, we firstconvert the ATSP to a symmetric TSP and then usethe Lin-Kernighan heuristic as implemented in Con-corde, a state-of-the-art TSP solver (Applegate et al,2005).
We also experimented with using the exactTSP solver in Concorde but since it was slower anddid not improve performance we preferred using theLin-Kernighan heuristic.
To convert the ATSP toa symmetric TSP we double the size of the orig-inal problem creating a node N ?
for every nodeN in the original graph.
Following (Hornik and4883apples2eats1John0Startc(2,3)=3c(3,2)=1c(0,3)=5c(3,0)=2c(0,1)=-1c(1,0)= 5 c(1,3)=0c(0,2)=5c(1,2)=3c(2,1)=5c(3,1)=5c(2,0)=-2Figure 1: Example of an ATSP for reordering the sen-tence: John eats apples.Hahsler, 2009), we then set new costs as follows:c?
(A,B) = ?, c?(A,B?)
= c?(B?
, A) = c(A,B)and C(A,A?)
= ??.
Even with this doubling ofthe number of nodes, we observed that solving theTSPs with the Lin-Kernighan heuristic is very fast,taking roughly 10 milliseconds per sentence on av-erage.
Overall, this means that our reordering modelis as fast as parsing and hence our model is compara-ble in performance to techniques based on applyingrules to the parse tree.4.1 FeaturesSince we would like to model reordering phenomenawhich are largely related to analyzing the syntax ofthe source sentence, we chose to use features basedon those that have in the past been used for parsing(McDonald et al, 2005a).
A subset of the featureswe use was also used for reordering in (Tromble andEisner, 2009).To be able to generalize from relatively smallamounts of data, we use features that in addition todepending on the words in the input sentence w de-pend on the part-of-speech (POS) tags of the wordsin the input sentence.
All features ?
(w, i, j) we useare binary features, that fire based on the identitiesof the words and POS tags at or surrounding posi-tions i and j in the source sentence.
The first set offeature templates we use are given in Table 1.
Thesefeatures depend only on the identities of the wordand POS tag of the two positions i and j and we callwi pi wj pj?
?
?
??????
??
??
??
??
??
??
?
?Table 1: Bigram feature templates used to calculate thecost that word at position i immediately precedes word atposition j in the target word order.
wi (pi) denotes theword (POS tag) at position i in the source sentence.
Eachof the templates is also conjoined with i-j the signed dis-tance between the two words in the source sentence.these Bigram features.The second set of feature templates we use aregiven in Table 2.
These features, in addition to ex-amining positions i and j examine the surround-ing positions.
We instantiate these feature templatesseparately for the POS tag sequence and for theword sequence.
We call these two feature sets Con-textPOS and ContextWord respectively.
When in-stantiated with POS tags, the first row of Table 2looks at all POS tags between positions i and j.
(Tromble and Eisner, 2009) use Bigram and Con-textPOS features, while we extend their feature setwith the use of ContextWord features.
Since Hindiis verb final, in Hindi sentences with multiple verbgroups it is rare for words with a verb in betweento be placed together in the reordering to match En-glish.
Looking at the POS tags of words betweenpositions i and j allows us to penalize such reorder-ings.Each of the templates described in Table 1 andTable 2 is also conjoined with i-j the signed dis-tance between the two words in the source sentence.The values of i-j between 5 and 10, and greater than10 are quantized (negative values are similarly quan-tized).In Section 5.2 we report on experiments showingthe relative performance of these different feature489oi?1 oi oi+1 ob oj?1 oj oj+1?
?
??
?
?
??
?
??
?
??
?
??
?
??
?
?
??
?
??
?
??
?
??
?
??
?
?
??
?
??
?
??
?
?
??
?
??
?
?Table 2: Context feature templates used to calculate thecost that word at position i immediately precedes wordat position j in the target word order.
oi denotes the ob-servation at position i in the source sentence and ob de-notes an observation at a position between i and j (i.ei + 1 ?
b ?
j ?
1).
Each of the templates is instan-tiated with the observation sequence o taken to be theword sequence w and the POS tag sequence p. Each ofthe templates is also conjoined with i-j the signed dis-tance between the two positions in the source sentence.types for the task of reordering Hindi sentences tobe in English word order.4.2 TrainingTo train the weights ?
in our model, we need acollection of sentences, where we have the desiredreference reordering pi?
(x) for each input sentencex.
To obtain these reference reorderings we useword aligned source-target sentence pairs.
The qual-ity and consistency of these reference reorderingswill depend on the quality of the word alignmentsthat we use.
Given word aligned source and tar-get sentences, we drop the source words that are notaligned.
Let mi be the mean of the target word po-sitions that the source word at index i is aligned to.We then sort the source indices in increasing orderof mi.
If mi = mj (for example, because wi and wjare aligned to the same set of words) we keep themin the same order that they occurred in the sourcesentence.
Obtaining the target ordering in this man-ner, is certainly not the only possible way and wewould like to explore better treatment of this in fu-ture work.We used the single best Margin Infused Re-laxed Algorithm (MIRA) ((McDonald et al, 2005b),(Crammer and Singer, 2003)) with the online up-dates to our parameters being given by?i+1 = argmin?||?
?
?i||s.t.
C(pi?|w) < C(p?i|w)?
L(pi?, p?i).In the equation above,p?i = argminpiC(pi|x)is the best reordering based on the current parametervalue and L is a loss function.
We take the loss ofa reordering to be the number of words for whichthe preceding word is wrong relative to the referencetarget order.We also experimented with the averaged percep-tron algorithm (Collins, 2002), but found single bestMIRA to work slightly better and hence used MIRAfor all our experiments.5 ExperimentsIn this section we report on experiments to evalu-ate our reordering model.
The first method we usefor evaluation (monolingual BLEU) is by generat-ing the desired reordering of the source sentence (asdescribed in Section 4.2) and compare the reorderedoutput to this desired reordered sentence using theBLEU metric.
In addition, to these monolingualBLEU results, we also evaluate (in Section 5.5) thereordering by its effect on eventual machine transla-tion performance.We note that our reordering techniques uses POSinformation for the input sentence.
The POS taggersused in this paper are Maximum Entropy Markovmodels trained using manually annotated POS cor-pora.
For Hindi, we used roughly fifty thousandwords with twenty six tags from the corpus de-scribed in (Dalal et al, 2007).
For Urdu we usedroughly fifty thousand words and forty six tags fromthe CRULP corpus (Hussain, 2008) and for Englishwe used the Wall Street Journal section of the PennTreebank.4905.1 Reordering model training data andalignment qualityTo train our reordering models we need training datawhere we have the input source language sentenceand the desired reordering in the target language.As described in Section 4.2 we derive the refer-ence reordered sentence using word alignments.
Ta-ble 3 presents our monolingual BLEU results forHindi to English reordering as the source of theword alignments is varied.
All results in Table 3are with Bigram and ContextPOS features.
We haveword alignments from three sources: A small setof hand aligned sentences, HMM alignments (Vo-gel et al, 1996) and alignments obtained using a su-pervised Maximum Entropy aligner (Ittycheriah andRoukos, 2005) trained on the hand alignments.
TheF-measure for the HMM alignments were 65% and78% for the Maximum Entropy model alignments.We see that the quality of the alignments is an im-portant determiner of reordering performance.
Row1 shows the BLEU for unreordered (baseline) Hindicompared with the Hindi sentences reordered in En-glish Order.
Using just HMM alignments to trainour model we do worse than unreordered Hindi.
Al-though using the Maximum Entropy alignments isbetter than using HMM alignments, we do not im-prove upon a small number of hand alignments byusing all the Maximum Entropy alignments.To improve upon the model trained with onlyhand alignments we selected a small number of snip-pets of sentences from our Maximum Entropy align-ments.
The goal was to pick parts of sentenceswhere the alignment is reliable enough to use fortraining.
The heuristic we used in the selection ofsnippets was to pick maximal snippets of at least7 consecutive Hindi words with all Hindi wordsaligned to a consecutive span of English words,with no unaligned English words in the span and noEnglish words aligned to Hindi words outside thespan.
Adding snippets selected with this heuristicimproves the reordering performance of our modelas seen in the last row of Table 3.5.2 Feature set comparisonIn this section we report on experiments to deter-mine the performance of the different classes of fea-tures (Bigram, ContextPos and ContextWord) dis-HMM MaxEnt Hand BLEU- - - 35.9220K - - 35.4- 220K - 47.0- 220K 6K 48.4- - 6K 49.0- Good 17K 6K 51.3Table 3: Monolingual BLEU scores for Hindi to Englishreordering using models trained on different alignmenttypes and tested on a development set of 280 Hindi sen-tences (5590 tokens).Feature templateBigram ContextPOS ContextWord BLEU- - - 35.9?
- - 43.8?
?
- 49.0?
?
?
51.3Table 4: Monolingual BLEU scores for Hindi to En-glish reordering using models trained with different fea-ture sets and tested on a development set of 280 Hindisentences (5590 tokens).cussed in Section 4.1.
Table 4 shows monolingualBLEU results for training with different features setsfor Hindi to English reordering.
In all cases, weuse a set of 6000 sentence pairs which were handaligned to generate the training data.
It is clear thatall three sets of features contribute to performance ofthe reordering model, however the number of Con-textWord features is larger than the number of Bi-gram and ContextPOS features put together, and itmay be desirable to select from this set of featuresespecially when training on large amounts of data.5.3 Monolingual reordering comparisonsTable 5 compares our reordering model with a reim-plementation of the reordering model proposed in(Tromble and Eisner, 2009).
Both the models useexactly the same features (bigram features and Con-textPOS features) and are trained on the same data.To generate our training data, for Hindi to Englishand English to Hindi we use a set of 6000 handaligned sentences, for Urdu to English we use a setof 8500 hand aligned sentences and for English toFrench we use a set of 10000 hand aligned sentences(a subset of Europarl and Hansards corpus).
Our491Language pair Monolingual BLEUSource Target Unreordered LOP TSPHindi English 35.9 36.6 49.0English Hindi 34.4 48.4 56.7Urdu English 35.6 39.5 49.9English French 64.4 78.2 81.2Table 5: Monolingual BLEU scores comparing the orig-inal source order with desired target reorder without re-ordering, and reordering using our model (TSP) and themodel proposed in (Tromble and Eisner, 2009) (LOP).test data consisted of 280 sentences for Hindi to En-glish and 400 sentences for all other language pairsgenerated from hand aligned sentences.
We includeEnglish-French here to compare on a fairly similarlanguage pair with local reordering phenomena (themain difference being that in French adjectives gen-erally follow nouns).
We note that our model outper-forms the model proposed in (Tromble and Eisner,2009) in all cases.5.4 Analysis of reordering performanceTo get a feel for the qualitative performance of ourreordering algorithm and the kind of phenomena itis able to capture, we analyze the reordering per-formance in terms of (i) whether the clause restruc-turing is done correctly ?
these can be thought ofas medium-to-long range reorderings, (ii) whetherclause boundaries are respected, and (iii) whether lo-cal (short range) reordering is performed correctly.The following analysis is for Hindi to English re-ordering with the best model (this is also the modelused for Machine Translation experiments reportedon in Section 5.5).?
Clause structure: As discussed in Section 3,the canonical clause order in Hindi is SOV,while in English it is SVO.
However, variationson this structure are possible and quite frequent(e.g., clauses with two objects).
To evaluateclause restructuring, we compared sequencesof subjects, objects and verbs in the output andreference reorderings.We had a set of 70 sentences annotated withsubject, direct object, indirect object and verbinformation ?
these annotations were made onthe head word of each phrase, and the compar-isons were on sequences of these words aloneand not the entire constituent phrase.
52 sen-tences were reordered by the model to matchthe order of the corresponding reference.
Eightsentences were ordered correctly but differentlyfrom the reference, because the reference wasexpressed in non-canonical fashion (e.g., in thepassive) ?
note that these cases negatively im-pact the monolingual BLEU score.
The follow-ing example shows a sentence being reorderedcorrectly, where, however, the reference is ex-pressed differently (note the position of thesubject ?policy?
(niiti) in the reference and thereordered output) 1:Input: aba1 (now) taka2 (till) aisii3 (this) niiti4(policy) kabhii5 (ever) nahii6 (not) rahii7 (has)hai8 (been)Reordered: taka2 (till) aba1 (now) aisii3 (this)niiti4 (policy) hai8 (been) kabhii5 (ever) nahii6(not) rahii7 (has)Reference: taka2 (till) aba1 (now) aisii3 (this)kabhii5 (ever) nahii6 (not) rahii7 (has) hai8(been) niiti4 (policy)English: Till now this never has been the policyThe remaining ten sentences were reordered in-correctly.
These errors are largely in clauseswhich deviate from the SVO order in someway ?
clauses with multiple subjects or objects,clauses with no object, etc.. For example, thefollowing sentence with two subjects and ob-jects corresponding to the verb wearing has notbeen reordered correctly.Input: sabhii1 (all) purusha2 (men) safeda3(white) evama4 (and) mahilaaen5 (women)kesariyaa6 (saffron) vastra7 (clothes) dhaarana8(wear) kiye9 hue10 (-ing) thiin11 (were)Reordered: sabhii1 (all) purusha2 (men)safeda3 (white) evama4 (and) mahilaaen5(women) kesariyaa6 (saffron) vastra7 (clothes)dhaarana8 (wear) thiin11 (were) kiye9 hue10 (-ing)Reference: sabhii1 (all) purusha2 (men)thiin11 (were) dhaarana8 (wear) kiye9 hue10 (-1The numeric subscripts in the examples indicate word po-sitions in the input.492ing) safeda3 (white) evama4 (and) mahilaaen5(women) kesariyaa6 (saffron)English: All men were wearing white and thewomen saffronThe model possibly needs more data with pat-terns that deviate from the standard SOV orderto learn to reorder them correctly.
We couldalso add to the model, features pertaining tosubject, object, etc.?
Clause boundaries: Measured on a set of844 sentences which were marked with clauseboundaries, 37 sentences (4.4 %) had reorder-ings that violated these boundaries.
An exam-ple of such a clause-boundary violation is be-low:Input: main1 (I) sarakaara2 (government) kaa3(of) dhyaana4 (attention) maananiiya5 (hon-ourable) pradhaana6 (prime) mantri7 (min-ister) dvaaraa8 (by) isa9 (this) sabhaa10(house) me11 (in) kiye12 gaye13 (made) isa14(this) vaade15 (promise) ki16 ora17 (towards)dilaanaa18 (to bring) chaahuungaa19 (wouldlike)Reordered: main1 (I) chahuungaa19 (wouldlike) dilaanaa18 (to bring) kii16 ora17 (to-wards) isa9 (this) vaade15 (promise) kiye12gaye13 (made) dvaaraa8 (by) maananiiya5(honourable) mantri7 (minister) pradhaana6(prime) dhyaana4 (attention) kaa3 (of)sarakaara2 (government) men11 (in) isa14 (this)sabhaa10 (house)Reference: main1 (I) chahuungaa19 (wouldlike) dilaanaa18 (to bring) dhyaana4 (attention)kaa3 (of) sarakaara2 (government) kii16 ora17(towards) isa9 (this) vaade15 (promise) kiye12gaye13 (made) dvaaraa8 (by) maananiiya5(honourable) mantri7 (minister) pradhaana6(prime) men11 (in) isa9 (this) sabhaa10 (house)English I would like to bring the attention ofthe government towards this promise made bythe honourable prime minister in this house.Note how the italicized clause, which is kepttogether in the reference, is split up incorrectlyin the reordered output.
The proportion of suchboundary violations is, however, quite low, be-cause Hindi being a verb-final language, mostclauses end with a verb and it is probably quitestraightforward for the model to keep clausesseparate.
A clause boundary detection programshould make it possible to eliminate the re-maining errors.?
Local reordering: To estimate the short rangereordering performance, we consider how of-ten different POS bigrams in the input are re-ordered correctly.
Here, we expect the modelto reorder prepositions correctly, and to avoidany reordering that moves apart nouns and theiradjectival pre-modifiers or components of com-pound nouns (see Section 3).
Table 6 sum-marizes the reordering performance for thesecategories for a set of 280 sentences (same asthe test set used in Section 5.1).
Each rowin Table 6 indicates the total number of cor-rect instances for the pair, i.e., the number ofinstances of the pair in the reference (columntitled Total), the number of instances that al-ready appear in the correct order in the input(column Input), and the number that are or-dered correctly by the reordering model (col-umn Reordered).
The first two rows show thatadjective-noun and noun-noun (compounds)are in most cases correctly retained in the orig-inal order by the model.
The final row showsthat while many prepositions have been movedinto their correct positions, there are still quite afew mismatches with the reference.
An impor-tant reason why this happens is that nouns mod-ified by prepositional phrases can often also beexpressed as noun compounds.
For example,vidyuta (electricity) kii (of) aavashyakataaen(requirements) in Hindi can be expressed eitheras ?requirements of electricity?
or ?electricityrequirements?.
The latter expression results ina match with the input (explaining many of the104 correct orders in the input) and a mismatchwith the model?s reordering.
The same problemin the training data would also adversely impactthe learning of the preposition reordering rule.493POS pair Total Input Reorderedadj-noun 234 192 196noun-noun 46 44 42prep-noun 436 104 250Table 6: An analyis of reordering for a few POS bigrams5.5 Machine translation resultsWe now present experiments in incorporating the re-ordering model in machine translation systems.
Forall results presented here, we reorder the training andtest data using the single best reordering based onour reordering model for each sentence.
For each ofthe language pairs we evaluated, we trained DirectTranslation Model 2 (DTM) systems (Ittycheriahand Roukos, 2007) with and without reordering andcompared performance on test data.
We note that theDTM system includes features that allow it to modellexicalized reordering phenomena.
The reorderingwindow size was set to +/-8 words for both the base-line and our reordered input.
In our experiments, weleft the word alignments fixed, i.e we reordered theexisting word alignments rather than realigning thesentences after reordering.
Redoing the word align-ments with the reordered data could potentially givefurther small improvements.
We note that we ob-tained better baseline performance using DTM sys-tems than the standard Moses/Giza++ pipeline (e.gwe obtained a BLEU of 14.9 for English to Hindiwith a standard Moses/Giza++ pipeline).
For all ofour systems we used a combination of HMM (Vo-gel et al, 1996) and MaxEnt alignments (Ittycheriahand Roukos, 2005).For our Hindi-English experiments we use a train-ing set of roughly 250k sentences (5.5Mwords) con-sisting of the Darpa-TIDES dataset (Bojar et al,2010) and an internal dataset from several domainsbut dominated by news.
Our test set was roughly1.2K sentences from the news domain with a sin-gle reference.
To train our reordering model, weused roughly 6K alignments plus 17K snippets se-lected from MaxEnt alignments as described in Sec-tion 5.1 with bigram, ContextPOS and ContextWordfeatures.
The monolingual reordering BLEU (on thesame data reported on in Section 5.3) was 54.0 forHindi to English and 60.8 for English to Hindi.For our Urdu-English experiments we used 70kLanguage pair BLEUSource Target Unreordered ReorderedHindi English 14.7 16.7Urdu English 23.3 24.8English Hindi 20.7 22.5Table 7: Translation performance without reordering(baseline) compared with performance after preorderingwith our reordering model.sentences from the NIST MT-08 training corpusand used the MT-08 eval set for testing.
We notethat the MT-08 eval set has four references as com-pared to one reference for our Hindi-English testset.
This largely explains the improved baseline per-formance for Urdu-English as compared to Hindi-English.
We present averaged results for the Weband News part of the test sets.
To train the reorder-ing model we used 9K hand alignments and 11Ksnippets extracted from MaxEnt alignments as de-scribed in Section 5.1 with bigram, ContextPOS andContextWord context feature.
The monolingual re-ordering BLEU for the reordering model thus ob-tained (on the same data reported on in Section 5.3)was 52.7.Table 7 shows that for Hindi to English, Englishto Hindi and for Urdu to English we see a gainof 1.5 - 2 BLEU points.
For English ?
Hindiwe also experimented with a system that uses rules(learned from the data using the methods describedin (Visweswariah et al, 2010)) applied to a parse toreorder source side English sentences.
This systemhad a BLEU score of 21.2, which is an improvementover the baseline, but our reordering model is betterby 1.3 BLEU points.An added benefit of our reordering model is thatthe decoder can be run with a smaller search spaceexploring only a small amount of reordering with-out losing accuracy but running substantially faster.Table 8 shows the variation in machine Hindi to En-glish translation performance with varying skip size(this parameter sets the maximum number of wordsskipped during decoding, lower values are associ-ated with a restricted decoder search space and in-creased speed).494skip Unreordered Reordered2 12.2 16.74 13.4 16.78 14.7 16.4Table 8: Translation performance with/without reorder-ing with varying decoder search space.6 Conclusion and future workIn this paper we presented a reordering model toreorder source language data to make it resemblethe target language word order without using eithera source or target parser.
We showed consistentgains of up to 2 BLEU points in machine transla-tion performance using this model to preorder train-ing and test data.
We show better performance com-pared to syntax based reordering rules for Englishto Hindi translation.
Our model used only a part ofspeech tagger (sometimes trained with fairly smallamounts of data) and a small corpus of word align-ments.
Considering the fact that treebanks requiredto build high quality parsers are costly to obtain, wethink that our reordering model is a viable alterna-tive to using syntax for reordering.
We also note,that with the preordering based on our reorderingmodel we can achieve the best BLEU scores witha much tighter search space in the decoder.
Even ac-counting for the cost of finding the best reorderingaccording to our model, this usually results in fasterprocessing than if we did not have the reordering inplace.In future work we plan to explore using more datafrom automatic alignments, perhaps by consideringa joint model for aligning and reordering.
We wouldalso like to explore doing away with the requirementof having a POS tagger, using completely unsuper-vised methods to class words.
We currently onlylook at word pairs in calculating the loss functionused in MIRA updates.
We would like to investigatethe use of other loss functions and their effect on re-ordering performance.
We also would like to explorewhether the use of scores from our reordering modeldirectly in machine translation systems can improveperformance relative to using just the single best re-ordering.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Dis-tortion models for statistical machine translation.
InProceedings of ACL, ACL-44, pages 529?536, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.David L. Applegate, Robert E. Bixby, Vasek Chvatal, andWilliam J. Cook.
2005.
Concorde tsp solver.
Inhttp://www.tsp.gatech.edu/.Ibrahim Badr, Rabih Zbib, and James Glass.
2009.
Syn-tactic phrase reordering for English-to-Arabic statisti-cal machine translation.
In Proceedings of EACL.Ondrej Bojar, Pavel Stranak, and Daniel Zeman.
2010.Data issues in English-to-Hindi machine translation.In LREC.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL, pages 531?540,Morristown, NJ, USA.
Association for ComputationalLinguistics.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP.K.
Crammer and Y.
Singer.
2003.
Ultraconservative on-line algorithms for multiclass problems.
Journal ofMachine Learning Research.Aniket Dalal, Kumar Nagaraj, Uma Sawant, SandeepShelke, and Pushpak Bhattacharyya.
2007.
Buildingfeature rich pos tagger for morphologically rich lan-guages: Experiences in Hindi.
In Proceedings of In-ternational Conference on Natural Language Process-ing.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,W.
Wang, and I. Thayer.
2006.
Scalable inference andtraining of context-rich syntactic translation models.In Proceedings of ACL.D.
Genzel.
2010.
Automatically learning source-side re-ordering rules for large scale machine translation.
InProceedings of the 23rd International Conference onComputational Linguistics.Kurt Hornik and Michael Hahsler.
2009.
TSP?infrastructure for the traveling salesperson problem.Journal of Statistical Software, 23(i02).Sarmad Hussain.
2008.
Resources for Urdu languageprocessing.
In Proceedings of the 6th Workshop onAsian Language Resources, IJCNLP?08.Abraham Ittycheriah and Salim Roukos.
2005.
A max-imum entropy word aligner for Arabic-English ma-chine translation.
In Proceedings of HLT/EMNLP,HLT ?05, pages 89?96, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.495Abraham Ittycheriah and Salim Roukos.
2007.
Directtranslation model 2.
In Proceedings of HLT-NAACL,pages 57?64.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
Comput.
Linguist.,25:607?615, December.Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.
2010.Constituent reordering and syntax models for English-to-Japanese statistical machine translation.
In COL-ING.Y.
Liu, Q. Liu, and S. Lin.
2006.
Tree-to-String align-ment template for statistical machine translation.
InProceedings of ACL.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
On-line large-margin training of dependency parsers.
InProceedings of ACL.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedings ofHLT.Ananthakrishnan Ramanathan, Hansraj Choudhary,Avishek Ghosh, and Pushpak Bhattacharyya.
2009.Case markers and morphology: addressing the cruxof the fluency problem in English-Hindi smt.
InProceedings of ACL-IJCNLP.Christoph Tillman.
2004.
A unigram orientation modelfor statistical machine translation.
In Proceedings ofHLT-NAACL.Christoph Tillmann and Hermann Ney.
2003.
Word re-ordering and a dynamic programming beam search al-gorithm for statistical machine translation.
Computa-tional Linguistics, 29(1):97?133.Roy Tromble and Jason Eisner.
2009.
Learning linear or-dering problems for better translation.
In Proceedingsof EMNLP.Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,Vijil Chenthamarakshan, and Nandakishore Kamb-hatla.
2010.
Syntax based reordering with automat-ically derived rules for improved statistical machinetranslation.
In Proceedings of the 23rd InternationalConference on Computational Linguistics.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of the 16th conference on Com-putational Linguistics.Chao Wang, Michael Collins, and Philipp Koehn.
2007.Chinese syntactic reordering for statistical machinetranslation.
In Proceedings of EMNLP-CoNLL.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical MT system with automatically learned rewritepatterns.
In COLING.Kenji Yamada and Kevin Knight.
2002.
A decoder forsyntax-based statistical mt.
In Proceedings of ACL.Mikhail Zaslavskiy, Marc Dymetman, and Nicola Can-cedda.
2009.
Phrase-based statistical machine transla-tion as a traveling salesman problem.
In Proceedingsof ACL-IJCNLP.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings on the Workshop on Statistical MachineTranslation.496
