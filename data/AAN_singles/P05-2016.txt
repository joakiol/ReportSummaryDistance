Proceedings of the ACL Student Research Workshop, pages 91?96,Ann Arbor, Michigan, June 2005. c?2005 Association for Computational LinguisticsDependency-Based Statistical Machine TranslationHeidi J. FoxBrown Laboratory for Linguistic Information ProcessingBrown University, Box 1910, Providence, RI 02912hjf@cs.brown.eduAbstractWe present a Czech-English statisticalmachine translation system which per-forms tree-to-tree translation of depen-dency structures.
The only bilingual re-source required is a sentence-aligned par-allel corpus.
All other resources aremonolingual.
We also refer to an evalua-tion method and plan to compare our sys-tem?s output with a benchmark system.1 IntroductionThe goal of statistical machine translation (SMT) isto develop mathematical models of the translationprocess whose parameters can be automatically esti-mated from a parallel corpus.
Given a string of for-eign words F, we seek to find the English string Ewhich is a ?correct?
translation of the foreign string.The first work on SMT done at IBM (Brown et al,1990; Brown et al, 1992; Brown et al, 1993; Bergeret al, 1994), used a noisy-channel model, resultingin what Brown et al (1993) call ?the FundamentalEquation of Machine Translation?:E?
=argmaxE P (E)P (F | E) (1)In this equation we see that the translation prob-lem is factored into two subproblems.
P (E) is thelanguage model and P (F | E) is the translationmodel.
The work described here focuses on devel-oping improvements to the translation model.While the IBM work was groundbreaking, it wasalso deficient in several ways.
Their model trans-lates words in isolation, and the component whichaccounts for word order differences between lan-guages is based on linear position in the sentence.Conspicuously absent is all but the most elementaryuse of syntactic information.
Several researchershave subsequently formulated models which incor-porate the intuition that syntactically close con-stituents tend to stay close across languages.
Beloware descriptions of some of these different methodsof integrating syntax.?
Stochastic Inversion Transduction Grammars(Wu and Wong, 1998): This formalism uses agrammar for English and from it derives a pos-sible grammar for the foreign language.
Thisderivation includes adding productions wherethe order of the RHS is reversed from the or-dering of the English.?
Syntax-based Statistical Translation (Yamadaand Knight, 2001): This model extends theabove by allowing all possible permutations ofthe RHS of the English rules.?
Statistical Phrase-based Translation (Koehnet al, 2003): Here ?phrase-based?
means?subsequence-based?, as there is no guaranteethat the phrases learned by the model will haveany relation to what we would think of as syn-tactic phrases.?
Dependency-based Translation ( ?Cmejrek et al,2003): This model assumes a dependencyparser for the foreign language.
The syntacticstructure and labels are preserved during trans-lation.
Transfer is purely lexical.
A generatorbuilds an English sentence out of the structure,labels, and translated words.912 System OverviewThe basic framework of our system is quite similarto that of ?Cmejrek et al (2003) (we reuse many oftheir ancillary modules).
The difference is in howwe use the dependency structures.
?Cmejrek et alonly translate the lexical items.
The dependencystructure and any features on the nodes are preservedand all other processing is left to the generator.
Inaddition to lexical translation, our system modelsstructural changes and changes to feature values, foralthough dependency structures are fairly well pre-served across languages (Fox, 2002), there are cer-tainly many instances where the structure must bemodified.While the entire translation system is too large todiscuss in detail here, I will provide brief descrip-tions of ancillary components.
References are pro-vided, where available, for those who want more in-formation.2.1 Corpus PreparationOur parallel Czech-English corpus is comprised ofWall Street Journal articles from 1989.
The Englishdata is from the University of Pennsylvania Tree-bank (Marcus et al, 1993; Marcus et al, 1994).The Czech translations of these articles are providedas part of the Prague Dependency Treebank (PDT)(Bo?hmova?
et al, 2001).
In order to learn the pa-rameters for our model, we must first create aligneddependency structures for the sentence pairs in ourcorpus.
This process begins with the building of de-pendency structures.Since Czech is a highly inflected language, mor-phological tagging is extremely helpful for down-stream processing.
We generate the tags usingthe system described in (Hajic?
and Hladka?, 1998).The tagged sentences are parsed by the Charniakparser, this time trained on Czech data from the PDT.The resulting phrase structures are converted to tec-togrammatical dependency structures via the proce-dure documented in (Bo?hmova?, 2001).
Under thisformalism, function words are deleted and any in-formation contained in them is preserved in featuresattached to the remaining nodes.
Finally, functors(such as agent or patient) are automatically assignedto nodes in the tree ( ?Zabokrtsky?
et al, 2002).On the English side, the process is simpler.
Wejapan automobile dealers association... ...NNP NNP NNPS NNjapan automobile dealers association... ...NNP NNP NNPS NNSPLITN N A NCZ3CZ2CZ1...
obchodn?
?k japonsky?
...automobilasociaceEN2EN1EN2EN1EN3Figure 1: Left SPLIT Exampleparse with the Charniak parser (Charniak, 2000)and convert the resulting phrase-structure trees to afunction-argument formalism, which, like the tec-togrammatic formalism, removes function words.This conversion is accomplished via deterministicapplication of approximately 20 rules.2.2 Aligning the Dependency StructuresWe now generate the alignments between the pairsof dependency structures we have created.
We be-gin by producing word alignments with a model verysimilar to that of IBM Model 4 (Brown et al, 1993).We keep fifty possible alignments and require thateach word has at least two possible alignments.
Wethen align phrases based on the alignments of thewords in each phrase span.
If there is no satisfac-tory alignment, then we allow for structural muta-tions.
The probabilities for these mutations are re-fined via another round of alignment.
The structuralmutations allowed are described below.
Examplesare shown in phrase-structure format rather than de-pendency format for ease of explanation.92BUDCZ2CZ1bear stearnsN NNspolec?nostEN1EN2... stearns ...NNP NNPbear...
...Figure 2: BUD Example?
KEEP: No change.
This is the default.?
SPLIT: One English phrase aligns with twoCzech phrases and splitting the English phraseresults in a better alignment.
There are threetypes of split (left, right, middle) whose proba-bilities are also estimated.
In the original struc-ture of Figure 1, English node EN1 would alignwith Czech nodes CZ1 and CZ2.
Splitting theEnglish by adding child node EN3 results in abetter alignment.?
BUD: This adds a unary level in the Englishtree in the case when one English node alignsto two Czech nodes, but one of the Czech nodesis the parent of the other.
In Figure 2, the Czechhas one extra word ?spolec?nost?
(?company?
)compared with the English.
English node EN1would normally align to both CZ1 and CZ2.Adding a unary node EN2 to the English resultsin a better alignment.?
ERASE: There is no corresponding Czech nodefor the English one.
In Figure 3, the English hastwo nodes, EN1 and EN2, which have no corre-sponding Czech nodes.
Erasing them brings theCzech and English structures into alignment.?
PHRASE-TO-WORD: An entire Englishphrase aligns with one Czech word.
Thisoperates similarly to ERASE.NNJJ WDT VBD NNPNNJJ WDT VBD NNPERASE ERASEA N P V NCZ2CZ1ktery?...
rok srpen ...fiska?ln??
zar??
?EN4EN3EN2EN1year began august ...which... fiscalEN4EN3year began august ...which... fiscalFigure 3: ERASE Example3 Translation ModelGiven E , the parse of the English string, our trans-lation model can be formalized as P (F | E).
LetE1 .
.
.
En be the nodes in the English parse, F bea parse of the Czech string, and F1 .
.
.Fm be thenodes in the Czech parse.
Then,P (F | E) =?FforFP (F1 .
.
.Fm | E1 .
.
.
En) (2)We initially make several strong independence as-sumptions which we hope to eventually weaken.The first is that each Czech parse node is generatedindependently of every other one.
Further, we spec-ify that each English parse node generates exactlyone (possibly NULL) Czech parse node.P (F | E) =?Fi?FP (Fi | E1 .
.
.
En) =n?i=1P (Fi | Ei)(3)An English parse node Ei contains the followinginformation:?
An English word: ei?
A part of speech: tei?
A vector of n features (e.g.
negation or tense):< ?ei [1], .
.
.
, ?ei [n] >93?
A list of dependent nodesIn order to produce a Czech parse node Fi, wemust generate the following:Lemma fi: We generate the Czech lemma fi de-pendent only on the English word ei.Part of Speech tfi : We generate Czech part ofspeech tfi dependent on the part of speech ofthe Czech parent tfpar(i) and the correspondingEnglish part of speech tei .Features < ?fi [1], .
.
.
, ?fi [n] >: There are severalfeatures (see Table 1) associated with eachparse node.
Of these, all except IND are typi-cal morphological and analytical features.
IND(indicator) is a loosely-specified feature com-prised of functors, where assigned, and otherwords or small phrases (often prepositions)which are attached to the node and indicatesomething about the node?s function in the sen-tence.
(e.g.
an IND of ?at?
could indicate alocative function).
We generate each Czechfeature ?fi [j] dependent only on its correspond-ing English feature ?ei [j].Head Position hi: When an English word isaligned to the head of a Czech phrase, theEnglish word is typically also the head of itsrespective phrase.
But, this is not always thecase, so we model the probability that the En-glish head will be aligned to either the Czechhead or to one of its children.
To simplify,we set the probability for each particular childbeing the head to be uniform in the numberof children.
The head position is generatedindependent of the rest of the sentence.Structural Mutation mi: Dependency structuresare fairly well preserved across languages, butthere are cases when the structures need to bemodified.
Section 2.2 contains descriptionsof the different structural changes whichwe model.
The mutation type is generatedindependent of the rest of the sentence.Feature DescriptionNEG NegationSTY Style (e.g.
statement, question)QUO Is node part of a quoted expression?MD Modal verb associated with nodeTEN Tense (past, present, future)MOOD Mood (infinitive, perfect, progressive)CONJ Is node part of a conjoined expression?IND IndicatorTable 1: Features3.1 Model with Independence AssumptionsWith all of the independence assumptions describedabove, the translation model becomes:P (Fi | Ei) = P (fi | ei)P (tfi | tei , tfpar(i))?P (hi)P (mi)n?j=1P (?fi [j] | ?ei [j]) (4)4 TrainingThe Czech and English data are preprocessed (seeSection 2.1) and the resulting dependency structuresare aligned (see Section 2.2).
We estimate the modelparameters from this aligned data by maximum like-lihood estimation.
In addition, we gather the inverseprobabilities P (E | F ) for use in the figure of meritwhich guides the decoder?s search.5 DecodingGiven a Czech sentence to translate, we first pro-cess it as described in Section 2.1.
The resulting de-pendency structure is the input to the decoder.
Thedecoder itself is a best-first decoder whose priorityqueue holds partially-constructed English nodes.For our figure of merit to guide the search, we usethe probability P (E | F ).
We normalize this us-ing the perplexity (2H ) to compensate for the differ-ent number of possible values for the features ?
[j].Given two different features whose values have thesame probability, the figure of merit for the featurewith the greater uncertainty will be boosted.
Thisprevents features with few possible values from mo-nopolizing the search at the expense of the other fea-tures.
Thus, for feature ?ei [j], the figure of merit isFOM = P (?ei [j] | ?fi [j]) ?
2H(?ei [j]|?fi [j]) (5)94Since our goal is to build a forest of partial trans-lations, we translate each Czech dependency nodeindependently of the others.
(As more conditioningfactors are added in the future, we will instead trans-late small subtrees rather than single nodes.)
Eachtranslated node Ei is constructed incrementally in thefollowing order:1.
Choose the head position hi2.
Generate the part of speech tei3.
For j = 1..n, generate ?ei [j]4.
Choose a structural mutation miEnglish nodes continue to be generated until ei-ther the queue or some other stopping conditionis reached (e.g.
having a certain number of possi-ble translations for each Czech node).
After stop-ping, we are left with a forest of English dependencynodes or subtrees.6 Language ModelWe use a syntax-based language model which wasoriginally developed for use in speech recognition(Charniak, 2001) and later adapted to work with asyntax-based machine translation system (Charniaket al, 2001).
This language model requires a for-est of partial phrase structures as input.
Therefore,the format of the output of the decoder must bechanged.
This is the inverse transformation of theone performed during corpus preparation.
We ac-complish this with a statistical tree transformationmodel whose parameters are estimated during thecorpus preparation phase.7 EvaluationWe propose to evaluate system performance withversion 0.9 of the NIST automated scorer (NIST,2002), which is a modification of the BLEU sys-tem (Papineni et al, 2001).
BLEU calculates a scorebased on a weighted sum of the counts of matchingn-grams, along with a penalty for a significant dif-ference in length between the system output and thereference translation closest in length.
Experimentshave shown a high degree of correlation betweenBLEU score and the translation quality judgmentsof humans.
The most interesting difference in theNIST scorer is that it weights n-grams based on anotion of informativeness.
Details of the scorer canbe found in their paper.For our experiments, we propose to use the datafrom the PDT, which has already been segmentedinto training, held out (or development test), andevaluation sets.
As a baseline, we will run theGIZA++ implementation of IBM?s Model 4 trans-lation algorithm under the same training conditionsas our own system (Al-Onaizan et al, 1999; Och andNey, 2000; Germann et al, 2001).8 Future WorkOur first priority is to complete the final pieces sothat we have an end-to-end system to experimentwith.
Once we are able to evaluate our system out-put, our first priority will be to analyze the systemerrors and adjust the model accordingly.
We recog-nize that our independence assumptions are gener-ally too strong, and improving them is a hight pri-ority.
Adding more conditioning factors should im-prove the quality of the decoder output as well as re-ducing the amount of probability mass lost on struc-tures which are not well formed.
With this will comesparse data issues, so it will also be important for usto incorporate smoothing into the model.There are many interesting subproblems whichdeserve attention and we hope to examine at least acouple of these in the near future.
Among these arediscontinuous constituents, head switching, phrasaltranslation, English word stemming, and improvedmodeling of structural changes.AcknowledgmentsThis work was supported in part by NSF grantIGERT-9870676.
We would like to thank Jan Hajic?,Martin ?Cmejrek, Jan Cur??
?n for all of their assistance.ReferencesYaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, andDavid Yarowsky.
1999.
Statistical machinetranslation: Final report, JHU workshop 1999.www.clsp.jhu.edu/ws99/projects/mt/final report/mt-final-report.ps.95Adam L. Berger, Peter F. Brown, Stephen A. Della Pietra,Vincent J. Della Pietra, John R. Gillett, John D. Laf-ferty, Robert L. Mercer, Harry Printz, and Lubos?
Ures?.1994.
The Candide system for machine translation.
InProceedings of the ARPA Human Language Technol-ogy Workshop.Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and BarboraHladka?.
2001.
The Prague Dependency Treebank:Three-level annotation scenario.
In Anne Abeille?, ed-itor, Treebanks: Building and Using Syntactically An-notated Corpora.
Kluwer Academic Publishers.Alena Bo?hmova?.
2001.
Automatic procedures in tec-togrammatical tagging.
The Prague Bulletin of Math-ematical Linguistics, 76.Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-ferty, Robert L. Mercer, and Paul S. Roossin.
1990.
Astatistical approach to machine translation.
Computa-tional Linguistics, 16(2):79?85.Peter F. Brown, Stephen A. Della Petra, Vincent J.Della Pietra, John D. Lafferty, and Robert L. Mer-cer.
1992.
Analysis, statistical transfer, and synthesisin machine translation.
In Proceedings of the FourthInternational Conference on Theoretical and Method-ological Issues in Machine Translation, pages 83?100.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
The math-ematics of machine translation: Parameter estimation.Computational Linguistics, 19(2):263?311, June.Eugene Charniak, Kevin Knight, and Kenji Yamada.2001.
Syntax-based language models for statisticalmachine translation.
In Proceedings of the 39th An-nual Meeting of the Association for ComputationalLinguistics, Toulouse, France, July.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st Meeting of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.Eugene Charniak.
2001.
Immediate-head parsing forlanguage models.
In Proceedings of the 39th AnnualMeeting of the Association for Computational Linguis-tics, pages 116?123, Toulouse, France, July.Martin ?Cmejrek, Jan Cur??
?n, and Jir???
Havelka.
2003.Czech-English Dependency-based Machine Transla-tion.
In EACL 2003 Proceedings of the Conference,pages 83?90, April 12?17, 2003.Heidi Fox.
2002.
Phrasal cohesion and statistical ma-chine translation.
In Proceedings of the 2002 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP 2002), July.Ulrich Germann, Michael Jahr, Kevin Knight, DanielMarcu, and Kenji Yamada.
2001.
Fast decoding andoptimal decoding for machine translation.
In Proceed-ings of the 39th Annual Meeting of the Association forComputational Linguistics.Jan Hajic?
and Barbora Hladka?.
1998.
Tagging Inflec-tive Languages: Prediction of Morphological Cate-gories for a Rich, Structured Tagset.
In Proceedings ofCOLING-ACL Conference, pages 483?490, Montreal,Canada.Philip Koehn, Franz Josef Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofthe Human Language Technology and North Ameri-can Association for Computational Linguistics Con-ference, Edmonton, Canada, May.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 13(2):313?330, June.Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,Robert MacIntyre, Ann Bies, Mark Ferguson, KarenKatz, and Britta Schasberger.
1994.
The Penn Tree-bank: Annotating predicate argument structure.
InProceedings of the ARPA Human Language Technol-ogy Workshop, pages 114?119.NIST.
2002.
Automatic evaluation of machine trans-lation quality using n-gram co-occurrence statistics.www.nist.gov/speech/tests/mt/doc/ngram-study.pdf.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics, pages 440?447.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: A method for automatic evalu-ation of machine translation.
Technical report, IBM.Dekai Wu and Hongsing Wong.
1998.
Machine trans-lation with a stochastic grammatical channel.
In Pro-ceedings of the 36th Annual Meeting of the Associationfor Computational Linguistics, pages 1408?1414.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of the39th Annual Meeting of the Association for Compu-tational Linguistics.Zdene?k ?Zabokrtsky?, Petr Sgall, and Sas?o Dz?eroski.
2002.Machine learning approach to automatic functor as-signment in the Prague Dependency Treebank.
In Pro-ceedings of LREC 2002 (Third International Confer-ence on Language Resources and Evaluation), vol-ume V, pages 1513?1520, Las Palmas de Gran Ca-naria, Spain.96
