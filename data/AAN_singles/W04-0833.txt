Simple Features for Statistical Word Sense DisambiguationAbolfazl K. Lamjiri, Osama El Demerdash, Leila KosseimCLaC LaboratoryDepartment of Computer ScienceConcordia University, Montreal, Canada{a keigho,osama el,kosseim}@cs.concordia.caAbstractIn this paper, we describe our experiments onstatistical word sense disambiguation (WSD)using two systems based on different ap-proaches: Na?
?ve Bayes on word tokens and Max-imum Entropy on local syntactic and seman-tic features.
In the first approach, we considera context window and a sub-window within itaround the word to disambiguate.
Within theoutside window, only content words are con-sidered, but within the sub-window, all wordsare taken into account.
Both window sizes aretuned by the system for each word to disam-biguate and accuracies of 75% and 67% were re-spectively obtained for coarse and fine grainedevaluations.
In the second system, sense res-olution is done using an approximate syntac-tic structure as well as semantics of neighbor-ing nouns as features to a Maximum Entropylearner.
Accuracies of 70% and 63% were ob-tained for coarse and fine grained evaluations.1 IntroductionIn this paper, we present the two systems webuilt for our first participation in the Englishlexical sample task at Senseval-3.
In the firstsystem, a Na?
?ve Bayes learner based on contextwords as features is implemented.
In the secondsystem, an approximate syntactic structure, inaddition to semantics of the nouns around theambiguous word are selected as features to learnwith a Maximum Entropy learner.In Section 2, a brief overview of related workon WSD is presented.
Sections 3 and 4 providespecifications of our two systems.
Section 5discusses the results obtained and remarks onthem, and finally in Section 6, conclusion andour future work direction are given.2 Related WorkIn 1950, Kaplan carried out one of the earliestWSD experiments and showed that the accu-racy of sense resolution does not improve whenmore than four words around the target areconsidered (Ide and Ve?ronis, 1998).
While re-searchers such as Masterman (1961), Gougen-heim and Michea (1961), agree with this ob-servation (Ide and Ve?ronis, 1998), our resultsdemonstrate that this does not generally ap-ply to all words.
A large context window pro-vides domain information which increases theaccuracy for some target words such as bank.n,but not others like different.a or use.v (see Sec-tion 3).
This confirms Mihalcea?s observations(Mihalcea, 2002).
In our system we allow alarger context window size and for most of thewords such context window is selected by thesystem.Another trend consists in defining and us-ing semantic preferences for the target word.For example, the verb drink prefers an ani-mate subject in its imbibe sense.
Boguraevshows that this does not work for polysemousverbs because of metaphoric expressions (Ideand Ve?ronis, 1998).Furthermore, the grammatical structures thetarget word takes part in can be used as a distin-guishing tool: ?the word ?keep?, can be disam-biguated by determining whether its object isgerund (He kept eating), adjectival phrase (Hekept calm), or noun phrase (He kept a record)?
(Reifler, 1955).
In our second system we ap-proximate the syntactic structures of a word, inits different senses.Mooney (Mooney, 1996) has discussed theeffect of bias on inductive learning methods.In this work we also show sensitivity of Na?
?veBayes to the distribution of samples.3 Na?
?ve Bayes for Learning ContextWordsIn our approach, a large window and a smallersub-window are centered around the targetword.
We account for all words within the sub-window but use a POS filter as well as a shortstop-word list to filter out non-content wordsAssociation for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of SystemsFigure 1: The effect of choosing different win-dow and sub-window sizes for the word bank.n.The best accuracy is achieved with a windowand sub-window size of around 450 and 50 char-acters respectively, while for example 50 and 25provide very low accuracy.from the context.
The filter retains only openclass words, i.e.
nouns, adjectives, adverbs, andverbs, and rejects words tagged otherwise.3.1 Changing the context window sizeFigure 1 shows the effect of selecting differ-ent window and sub-window sizes for the wordbank.n.
It is clear that precision is very sensitiveto the selected window size.
Other words alsohave such variations in their precision results.The system decides on the best window sizesfor every word by examining possible windowsize values ranging from 25 to 750 characters1.Table 1 shows the optimal window sizes se-lected for a number of words from different wordclasses.
The baseline is considered individuallyfor every word as the ratio of the most com-mon sense in the training samples.
We used theSenseval-3 training set for the English lexicalsample task for training.
It includes a total of7860 tagged samples for 57 ambiguous words.15% of this data was used for validation, whilethe rest was used for training.3.2 Approximate SmoothingDuring the testing phase, given the context ofthe target word, the score of every sense is com-puted using the Na?
?ve Bayes formula:Scoresensei = log p(sensei) +?klog p(wordk)where, wordk is every word inside the contextwindow (recall that these are all the words in1For technical reasons, character is used instead ofword as the unit, making sure no word is cut at theextremities.the sub-window, and filtered words in the largewindow).Various smoothing algorithms could be usedto reduce the probability of seen words and dis-tributing them among unseen words.
However,tuning various smoothing parameters is delicateas it involves keeping an appropriate amountof held-out data.
Instead, we implemented anapproximate smoothing method, which seemsto perform better compared to Ng?s (Ng, 1997)approximate smoothing.
In our simple approxi-mate smoothing the probability of seen wordsis not discounted to compensate for those ofunseen words2.
Finding a proper value toassign to unseen words was done experimen-tally; for a relatively large training data set,p(an unseen word) = 10?10 and for a smallset, 10?9 resulted in the highest accuracy withour 15% validation set3.
The intuition is that,with a small training set, more unseen wordsare likely to be seen during the testing phase,and in order to prevent the accumulating scorepenalty value from becoming relatively high, alower probability value is selected.
Additionally,the selection should not result in large differ-ences in the computed scores of different sensesof the target word.A simple function assigns 10?10 in any of thefollowing conditions: the total number of wordsseen is larger than 4300, the number of train-ing instances is greater than 230, or the contextwindow size is larger than 400 characters.
Thefunction returns 10?9 otherwise.4 Maximum Entropy learning ofsyntax and semanticsSyntactic structures as well as semantics of thewords around the ambiguous word are strongclues for sense resolution in many words.
How-ever, deriving and using exact syntactic infor-mation introduces its own difficulties.
So, wetried to use approximate syntactic structures bylearning the following features in a context win-dow bounded by the last punctuation before andthe first punctuation after the ambiguous word:1.
Article Bef: If there is any article before,the string token is considered as the valueof this feature.2.
POS, POS Bef, POS Aft: The part ofspeech of the target, the part of speech ofthe word before (after) if any.2This results in a total probability mass larger than1; but still allows us to rank the probability of the senses.3The logarithm was taken in base 10.Word WS SW Diff Base Bys Entadd.v 100 25 4.1 46 69 82argument.n 175 75 3.1 47 45 54ask.v 725 150 5.2 36 37 65decide.v 725 375 5.2 77 65 75different.a 175 0 4.0 47 34 48eat.v 550 150 3.1 81 76 86miss.v 425 125 5.1 28 40 53simple.a 400 25 9.0 40 11 33sort.n 175 75 4.0 66 60 71use.v 50 25 5.6 58 57 79wash.v 50 25 5.5 56 62 71Table 1: Optimal window configuration andperformance of both systems for the wordson which Max Entropy has performed bet-ter than Na?
?ve Bayes.
(WS=Optimal win-dow size; SW=Optimal sub-window size;Diff=Average absolute difference between thedistribution of training and test samples;Accuracy (Base=Baseline; Bys=Na?
?ve Bayes;Ent=Max Entropy)).3.
Prep Bef, Prep Aft: The last prepositionbefore, and the first preposition after thetarget, if any.4.
Sem Bef, Sem Aft: The general semanticcategory of the noun before (after) the tar-get.
The category, which can be ?animate?,?inanimate?, or ?abstract?, is computed bytraversing hypernym synsets of WordNetfor all the senses of that noun.
The firstsemantic category observed is returned, or?inanimate?
is returned as the default value.The first three items are taken from Mihal-cea?s work (Mihalcea, 2002) which are usefulfeatures for most of the words.
The range of allthese features are closed sets; so Maximum En-tropy is not biased by the distribution of train-ing samples among senses, which is a side-effectof Na?
?ve Bayes learners (see Section 5.2)4.The following is an example of the fea-tures extracted for sample miss.v.bnc.00045286:?
.
.
.
?
I?ll miss the kids.
But .
.
.
?
:Article Bef=null,POS Bef="MD", POS="VB", POS Aft="DT",Prep Bef=null, Prep Aft=null,Sem Bef=null, Sem After="animate"4The Maximum Entropy program we used to learnthese features was obtained from the OpenNLP site:http://maxent.sourceforge.net/index.htmlWord Na?
?ve Bayes Max EntropyCategory coarse fine coarse finenouns 76% 70% 70% 61%verbs 76% 67% 74% 66%adjectives 59% 45% 59% 47%Total 75% 67% 70% 63%Table 2: Results of both approaches in fine andcoarse grain evaluation.5 Results and DiscussionThe Word Sense Disambiguator program hasbeen written as a Processing Resource in theGate Architecture5.
It uses the ANNIE Tok-enizer and POS Tagger which are provided ascomponents of Gate.Table 2 shows the results of both systems foreach category of words.
It can be seen that ap-proximate syntactic information has performedrelatively better with adjectives which are gen-erally harder to disambiguate.5.1 Window size and the commonesteffectThe optimal window size seems to be relatedto the distribution of the senses in the train-ing samples and the number of training sam-ples available for a word.
Indeed, a large win-dow size is selected when the number of samplesis large, and the samples are not evenly dis-tributed among senses.
Basically because thewords in Senseval are not mostly topical words,Na?
?ve Bayes is working strongly with the com-monest effect.
On the other hand, when a smallwindow size is selected, the commonest effectmostly vanishes and instead, collocations are re-lied upon.5.2 Distribution of samplesA Na?
?ve Bayes method is quite sensitive to theproportion of training and test samples: if thecommonest class presented as test is differentfrom the commonest class in training for ex-ample, this method performs poorly.
This isa serious problem of Na?
?ve Bayes towards realworld WSD.
For testing this claim, we made thefollowing hypothesis: When the mean of abso-lute difference of the test samples and trainingsamples among classes of senses is more than4%, Na?
?ve Bayes method performs at most 20%above baseline6.
Table 3 shows that this hypoth-esis is confirmed in 82% of the cases (41 words5http://www.gate.ac.uk/6The following exceptional cases are not considered:1) When baseline is above 70%, getting 20% above base-Acc ?
20 Acc > 20Dist ?
4.0 5 26Dist > 4.0 15 4Table 3: Sensitivity of Na?
?ve Bayes to the dis-tribution of samples (Acc=Accuracy amounthigher than baseline; Dist=Mean of distributionchange.
)out of 50 ambiguous words that satisfy the con-ditions).
Furthermore, such words are not nec-essarily difficult words.
Our Maximum Entropymethod performed on average 25% above thebaseline on 7 of them (ask.v, decide.v, differ-ent.a, difficulty.n, sort.n, use.v, wash.v some ofwhich are shown in Table 1).5.3 Rare samplesNa?
?ve Bayes mostly ignores the senses with afew samples in the training and gets its scoreon the senses with large number of training in-stances, while Maximum Entropy exploits fea-tures from senses which have had a few trainingsamples.5.4 Using lemmas and synsetsWe tried working with word lemmas insteadof derivated forms; however, for some wordsit causes loss in accuracy.
For example, forthe adjective different.a, with window and sub-window size of 175 and 0, it reduces the accu-racy from 60% to 46% with the validation set.However, for the noun sort.n, precision increasesfrom 62% to 72% with a window size of 650 andsub-window size of 50.
We believe that somesenses come with a specific form of their neigh-boring tokens and lemmatization removes thisdistinguishing feature.We also tried storing synsets of words as fea-tures for the Na?
?ve Bayes learner, but obtainedno significant change in the results.6 Conclusion and Future WorkThere is no fixed context window size applica-ble to all ambiguous words in the Na?
?ve Bayesapproach: keeping a large context window pro-vides domain information which increases theresolution accuracy for some target words butnot others.
For non-topical words, large win-dow size is selected only in order to exploit thedistribution of samples.line is really difficult, 2) When the difference is mostlyon the commonest sense being seen more than expected,so the score is favored (7 words out of 57 satisfy theseconditions.
)Rough syntactic information performed wellin our second system using Maximum Entropymodeling.
This suggests that some senses canbe strongly identified by syntax, leaving resolu-tion of other senses to other methods.
A simple,rough heuristic for recognizing when to rely onsyntactic information in our system is when theselected window size by Na?
?ve Bayes is relativelysmall.We tried two simple methods for combiningthe two methods: considering context words asfeatures in Max Entropy learner, and, establish-ing a separate Na?
?ve Bayes learner for each syn-tactic/semantic feature and adding their scoresto the basic contextual Na?
?ve Bayes.
These pre-liminary experiments did not result in any no-ticeable improvement.Finally, using more semantic features fromWordNet, such as verb sub-categorizationframes (which are not consistently available)may help in distinguishing the senses.AcknowledgmentsMany thanks to Glenda B. Anaya and MichelleKhalife?
for their invaluable help.ReferencesN.
Ide and J.
Ve?ronis.
1998.
Introduction to thespecial issue on word sense disambiguation:the state of the art.
Computational Linguis-tics, 24(1):1?40.R.
Mihalcea.
2002.
Instance based learningwith automatic feature selection applied toword sense disambiguation.
In Proceedings ofCOLING?02, Taiwan.R.
J. Mooney.
1996.
Comparative experimentson disambiguating word senses: An illustra-tion of the role of bias in machine learning.
InProceedings of EMNLP-96, pages 82?91, PA.H.
T. Ng.
1997.
Exemplar-based word sensedisambiguation: Some recent improvements.In Proceedings of EMNLP-97, pages 208?213.NJ.E.
Reifler.
1955.
The mechanical determinationof meaning.
In Machine translation of lan-guages, pages 136?164, New York.
John Wi-ley and Sons.
