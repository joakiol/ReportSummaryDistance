Efficient, High-Performance Algorithms forN-Best SearchRichard Schwartz, Steve AustinBBN Systems and Technologies Inc.10 Moulton St.Cambridge, MA, 02138AbstractWe present two efficient search algorithms for real-time spo-ken language systems.
The first called the Word-DependentN-Best algorithm is an improved algorithm for finding thetop N sentence hypotheses.
The new algorithm is shownto perform as well as the Exact Sentence-Dependent al-gorithm presented previously but with an order of mag-nitude less computation.
The second algorithm is a fastmatch scheme for continuous peech recognition called theForward-Backward Search.
This algorithm, which is directlymotivated by the Baum-Welch Forward-Backward trainingalgorithm, has been shown to reduce the computation of atime-synchronous beam search by a factor of 40 with noadditional search errors.1.
IntroductionIn a Spoken Language System (SLS) we must use all avail-able knowledge sources (KSs) to decide on the spoken sen-tence.
While there are many knowledge sources, they areoften grouped together into speech models, statistical lan-guage model, and natural anguage understanding models.To optimize accuracy we must choose the sentence that hasthe highest score (probability) given all of the KSs.
Thispotentially requires a very large search space.
The N-Bestparadigm for integrating several diverse KSs has been de-scribed previously \[2, 10\].
First, we use a subset of the KSsto choose a small number of likely sentences.
Then thesesentences are scored using the remainder of the KSs.In Chow et.
al., we also presented an efficient speechrecognition search algorithm that was capable of comput-ing the N most likely sentence hypotheses for an utterance,given the speech models and statistical language models.However, this algorithm greatly increases the needed com-putation over that needed for finding the best single sen-tence.
In this paper we introduce two techniques that dra-matically decrease the computation eeded for the N-Bestsearch.
These algorithms are being used in a real-time SLS\[1\].
In the remainder of the introduction we review the exactN-Best search briefly and describe its problems.
In Section2 we describe two approximations to the exact algorithmand compare their accuracy with that of the exact algorithm.The resulting algorithm is still not fast enough for real-time implementation.
In Section 3 we present a newsentence-level fast match scheme for continuous peechrecognition.
The algorithm is motivated by the mathematicsof the Baum-Welch Forward-Backward training algorithm.The N-Best ParadigmThe basic notion of the n-best paradigm is that, while wemust ultimately use all the available KSs to improve recog-nition accuracy, the sources vary greatly in terms of per-plexity reduction and required complexity.
For example,a first-order statistical language model can reduce perplex-ity by at least a factor of 10 with little computation, whileapplying complete natural language (NL) models of syn-tax and semantics to all partial hypotheses typically requiresmore computation for less perplexity reduction.
(Murveit\[6\] has shown that the use of an efficiently implementedsyntax within a recognition search actually slowed down thesearch unless it was used very sparingly.)
Therefore it isadvantageous to use a strategy in which we use the mostpowerful, efficient KSs first to produce a scored list of allthe likely sentences.
This list is then filtered and reorderedusing the remaining KSs to arrive at the best single sentence.Figure 1 contains a block diagram that illustrates this basicidea.
In addition to reducing total computation the result-ing systems would be more modular ff we could separateradically different KSs.The  Exact Sentence-Dependent Algor i thmWe have previously presented an efficient ime-synchronousalgorithm for finding the N most likely sentence hypotheses.This algorithm was unique in that it computed the correctforward probability score for each hypothesis found.
Theway this is accomplished is that, at each state, we keep anindependent score for each different preceding sequence ofwords.
That is, the scores for two theories are added onlyif the preceding word sequences are identical.
We preserveup to N different heories at each state, as long as they areabove the pruning beamwidth.
This algorithm guaranteesfinding the N best hypotheses within a threshold of the besthypothesis.
The algorithm was optimized to avoid expen-sive sorting operations o that it required computation thatwas less than linear with the number of sentence hypothesesfound.
It is easy to show that the inaccuracy in the scorescomputed is bounded by the product of the sentence l ngthi I .
.B .
t  R?
?rd'r List | _ Top ChoiceKS 1 KS2Statistical GrammarSyntaxStatistical Grammar + Syntax1st order statisticalFull NLPSemantics, etc.Semantics, etc.Higher-order statisticalFigure 1: The N-best Search Paradigm.
The most efficientknowledge sources, KS1, are used to find the N Best sen-tences.
Then the remaining knowledge sources, KS2 areused to reorder the sentences and pick the most likely one.and the pruning beamwidth.
For example, if a sentenceis 1000 frarms long and a relative pruning beamwidth of10-15 is maintained throughout the sentence, then all scoresare guaranteed to be accurate to within 10 -12 of the maxi-mum score.
The proof is not given here, since it is not thesubject of this paper.
In the remainder of the paper we willrefer to this particular algorithm as the Exact algorithm orthe Sentence-Dependent algorithm.There is a problem associated with the use of this exactalgorithm.
If we assume that the probability of a single wordbeing misrecognized is roughly independent of the positionwithin a sentence, then we would expect hat alonger sen-tence will have more errors.
Consequently the typical rankof the correct answer will be lower (further from the top) onlonger sentences.
Therefore if we wanted the algorithm tofind the correct answer within the list of hypotheses somefixed percentage of the time, the value of N will have toincrease significantly for longer sentences.When we examine the different answers found we no-tice that, many of the different answers are simple one-wordvariations of each other.
This is likely to result in muchduplicated computation.
One might imagine that if the dif-ference between two hypothesized word sequences were sev-eral words in the past then any difference in score due tothat past word would remain constant.
In the next section wepresent two algorithms that attempt to avoid these problems.2.
Two Approximate N-Best AlgorithmsWhile the exact N-Best algorithm is theoretically interesting,we can generate lists of sentences with much less computa-tion if we are willing to allow for some approximations.
Aslong as the correct sentence can be guaranteed to be withinthe list, the list can always be reordered by rescoring eachhypothesis individually at the end.
We present wo suchapproximate algorithms with reduced computation.Lattice N-BestThe first algorithm will derive an approximate list of theN Best sentences with no more computation than the usual1-Best search.
Figure 2 illustrates the algorithm.
Withinwords we use the time-synchronous forward-pass search al-gorithm \[8\], with only one theory at each state.
We add theprobabilities of all paths that come to each state.
At eachgrammar node (for each frame) we simply store all of thetheories that arrive at that node along with their respectivescores in a traceback list.
This requires no extra compu-tation above the 1-Best algorithm.
The score for the besthypothesis at the grammar node is sent on as in the nor-rnal time-synchronous forward-pass search.
A pointer to thesaved list is also sent on.
At the end of the sentence wesimply search (recursively) through the saved Iraceback listsfor all of the complete sentence hypotheses that are abovesome threshold below the best theory.
This recursive Irace-back can be performed very quickly.
(We typically extractthe 100 best answers, which causes no noticeable delay.
)We call this algorithm the Lattice N-Best algorithm sincewe essentially have a dense word lattice represented by thetraceback information.
Another advantage of this algorithmis that it naturally produces more answers for longer sen-tences .mam~_.~~mmWord  1 ~'~ Word 4Word 2Word 3 Word KFigure 2: The Lattice N-best Algorithm.
We save all theo-ries at grammar nodes.
Then we recursively Irace back allsequences.This algorithm is similar to the one suggested by Stein-biss \[9\], with a few differences.
First, he uses the stan-dard Viterbi algorithm rather than the time-synchronous al-gorithm within words.
That is he takes the maximum of thepath probabilities at a state rather than the sum.
We haveobserved a 20% higher error raate when using the maximumrather than the sum.
The second ifference is that when sev-eral word hypotheses come together at a common grammarnode at the same lime, he traces back each of the choicesand keeps the N (typically 10) best sentence hypotheses upto that lime and node.
This step unnecessarily imits theo,mher of sentence hypotheses that are produced to N. Asabove the score of the best hypothesis sent on to all wordsfollowing the grammar node.
At the end of the sentence hethen has an approximation to the 3r best sentences.
He re-ports that one third of the errors made by the 1-Best searchare corrected in this way.
However, as with a word lattice,many of the words are constrained to end at the same time- which leads to the main problem with this algorithm.The Lattice N-Best algorithm, while very fast, underesti-mates or misses high scoring hypotheses.
Figure 3 showsan example in which two different words (words 1 and 2)can each be followed by the same word (word 3).
Sincethere is only one theory at each state within a word, there isonly one best beginning time.
This best beginning time isdetermined by the best boundary between the best previousword (word 2 in the example) and the current word.
But, asshown in Figure 3, the second-best theory involving a differ-ent previous word (word 1 in the example), would naturallyend at a slightly different lime.
Thus the best score for thesecond-best theory would be severely underestimated or lostaltogether.word":~.t imeFigure 3: Alternate paths in the Lattice algorithm.
The bestpath for words 2-3 overrides the best path for words 1-3.Word-Dependent N-BestAs a compromise between the exact sentence-dependentalgorithm and the lattice algorithm we devised a Word-Dependent N-Best algorithm_ We reason that while the beststarting lime for a word does depend on the preceding word,it probably does not depend on any word before that.
There-fore instead of separating theories based on the whole pre-ceding sequence, we separate them only ff previous wordis different.
At each state within the word we preserve thetotal probability for each of n (<< N) different precedingwords.
At the end of each word we record the score foreach hypothesis along with the name of the previous word.Then we proceed on with a single theory with the name ofthe word that just ended.
At the end of the sentence we per-form a recursive traceback to derive a large list of the mostlikely sentences.
The resulting theory paths are illustratedschematically in Figure 4.
Like the lattice algorithm thewordTheor ies  arecombined, so wecan have morethan one startt ime tor th isI mode l .'
i1t imeFigure 4: Alternate paths in the Word-Dependent algorithm.Best path for words 1-3 is preserved along with path forwords 2-3.word-dependent algorithm naturally produces more answersfor longer sentences.
However, since we keep multiple the-ories within the word, we correctly identify the second bestpath.
While the computation needed is greater than for thelattice algorithm it is less than for the sentence-dependent al-gorithm, since the number of theories only needs to accountfor number of possible previous words - not all possible pre-ceding sequences.
Therefore the number n, of theories keptlocally only needs to be 3 to 6 instead of 20 to 100.Comparison of N-Best AlgorithmsWe performed experiments to compare the behavior of thethree N-Best algorithms.
In all three cases we used the ClassGrammar \[3\], a first-order statistical grammar based on 100word classes.
All words within a class are assumed equallylikely.
The test set perplexity is approximately 100.
The testset used was the June '88 speaker-dependent test set of 300sentences.
To enable direct comparison with previous resultswe did not use models of triphones across word boundaries,and the models were not smoothed.
We expect all threealgorithms to improve significantly when the latest modelingmethods are used.8~oE- -100.0.... 96,0Word-Dependent N-Best.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Sentence-Dependent N-BestLattice N-Best~o go go 106rank ot correct answerFigure 5: Comparison of the Rank of the Correct Sentencefor the Sentence-Dependent, Word-Dependent, and LatliceN-Best Algorithms.Figure 5 shows the cumulative distribution of the rankof the correct answer for the three algorithms.
As can beseen, all three algorithms get the sentence correct on the firstchoice about 62% of the time.
All three cumulative distri-butions increase substantially with more choices.
However,we observe that the Word-Dependent algorithm yields accu-racies quite close to that of the Exact Sentence-Dependentalgorithm, while the Lattice N-Best is substantially worse.In particular, the sentence error rate at rank 100 (8%) is dou-ble that of the Word-Dependent algorithm (4%).
Therefore,ff we can afford the computation of the Word-Dependentalgorithm it is clearly preferred.We also observe in Figure 5 that the Word-Dependentalgorithm is actually better than the Sentence-Dependent al-gorithm for very high ranks.
This is because the score of thecorrect word sequence fell outside the pruning beamwidth.However, in the Word-Dependent algorithm each hypothesisgets the benefit of the best heory two words back.
Thereforethe correct answer was preserved in the traceback.
This isanother advantage that both of the approximate algorithmshave over the Sentence-Dependent algorithm.In the next section we describe a technique that can beused to speed up all of these time-synchronous search algo-rithms by a large factor.3.
Forward-Backward SearchThe time-synchronous beam search follows a large numberof theories on the off chance that they will get better duringthe remainder of the sentence.
Typically, we must keepover 1000 theories to guarantee finding the highest answer.In some sense the computation for all but one answer willhave been wasted.We need a way to speed up the beam search without caus-ing search errors.
We could prune out most of the choicesif we only knew the correct answer ahead of time or if wecould look ahead at the remainder of the sentence.
Severalpapers have described fast match schemes that look ahead(incurring adelay) to determine which words are likely (e.g.\[4\]).
The basic idea is to perform some approximate matchthat can be used to eliminate most of the possible followingwords.
However, since we cannot ell when words end incontinuous speech, the predictions of the score for each wordis quite approximate.
In addition, even if a word matcheswell we cannot ell whether the remainder of the sentencewill be consistent with that word without looking furtherahead and incurring a longer delay.Let us consider the time-synchronous forward pass.
Thescore at any given state and time at(s) is the probability ofthe input up to time t, summed over all of the paths thatget to state s at t. When these scores are normalized theygive the relative probability of paths ending at this state asopposed to paths ending at any other state.
These forwardpass probabilities are the ideal measure to predict whichtheories in a backward search are expected to score well.Figure 6 illustrates everal paths from the beginning of anutterance to different states at time t, and several theoriesfrom the end of the utterance T backward to time t. Fromthe Baum-Welch Forward-Backward Iraining algorithm wehave7t(s)GTwhere 7t(s) is the probability of the data given all pathsthrough state s, divided by the probability of the data forall paths, which is the probability that slate s is appropriateat time t. aT is derived from the forward pass.
Of courseif we have already gone through the whole utterance in theforward direction we already know the most likely sentence.Now let us consider apractical Forward-Backward Searchalgorithm.
First we perform a forward pass over the wholeutterance using a simplified acoustics or language model.
Ineach fran~ we save the highest forward probability and theprobabilities of all words that have ending scores above thepruning beamwidth.
Typically this includes about 20 wordsin each frame.
Then we perform a search in the backwarddirection.
This search uses the normal beam search withinwords.
However, whenever a score is about to be trans-fered backwards through the language model into the endof a word we first check whether that word had an endingscore for that frame in the forward pass.
That is we ask,"Was there a reasonable path from the beginning of the ut-terance to this time ending with this word?"
Again, referringto Figure 6, the backward theory that is looking for wordscoreforwardsib.
~ backwardsFigure 6: Forward-Backward Search.
Forward and back-ward scores for the same state and time are added to predictfinal score for each theory extension.d cannot find any corresponding forward score, and so isaborted.
When there is a score, as in the cases for wordsa,b,c, then we multiply the present backward score of thetheory,/3t(s) by the forward pass score for this word; at(s),divided by the whole sentence score, aT.
Only if this ra-tio is greater than the pruning beamwidth do we extend thetheory backwards by this word.
For example, although thebackward theory looking for word c has a good score, thecorresponding forward score c' is not good, and the productmay be pruned out.The Forward-Backward search is only useful ff the for-ward pass is faster than the backward would have been.
Thiscan be true if we use a different grammar, or a less expensiveacoustic model.
If the forward acoustic models or languagemodel is different han in the backward pass, then we mustreestimate txa, before using it in the algorithm above.
Forsimplicity we estimate txT at each time t asat(t) = max at(s) maxB (s)the product of the maximum state scores in each direction.
(Note that since the two maxima are not necessarily on thesame state it would be more accurate to useat ( t )  = max a~(s)#t(s)forcing the two states to be the same.
However, since mostof the active states are internal to words, this would requirea large computation and also require that we had stored allof the state scores in the forward direction for every time.
)We observe that the average number of active phonemearcs in the backward irection is reduced by a factor of 40(e.g.
from.
800 to 20) - with a corresonding reduction incomputation and with no increase in search errors.Uses of Forward-Backward SearchAs stated above, this algorithm is only useful when the for-ward pass can be computed ifferently (much more quicldy)than the backward (real) search.
For example, we could usea null grammar in the forward direction and a more com-plex grammar in the backward search.
We have used thisextensively in our past work with very large RTN grammarsor high-order statistical grammars \[7\].
When no grammaris used in the forward pass we can compact the entire dic-tionary into a phonetic tree, thereby greatly reducing thecomputation for large dictionaries.A variation on the above use is to use a simpler acous-tic model in the forward direction.
For example restrictingthe model to triphones within words, using simpler HMMtopologies, etc.A second use is for real-time computation of the N Bestsentences \[1\].
First we perform a normal 1-Best search for-ward.
The best answer can be processed by NL immediately(on another processor) while we perform the N-Best searchbackwards.
We find that the backward N-Best search is spedup by a factor of 40 when using the forward pass scores forpruning.
Thus the delay until we have the remainder of theanswers is usually quite short.
If the delay is less than thetime required to process the first answer through NL, thenwe have lost no time.Finally, we can use the Forward-Backward Search togreatly reduce the time needed for experiments.
Experi-ments involving expensive decoding conditions can be re-duced from days to hours.
For example all of the exper-irnents with the Word-Dependent and Lattice N-Best algo-rithms were performed using the Forward-Backward Search.\4.
Conc lus ionWe have considered several approximations to the exactSentence-Dependent N-Best algorithm, and evaluated themthoroughly.
We show that an approximation that only sepa-rates theories when the previous words are different allowsa significant reduction in computation, makes the algorithmscalable to long sentences and less susceptable to pruningerrors, and does not increase the search errors measurably.In contrast, he Lattice N-Best algorithm, which is still lessexpensive, appears to miss twice as many sentences withinthe N-Best choices.We have introduced a new two-pass earch strategy calledthe Forward-Backward Search, which is generally applicableto a wide range of problems.
This strategy increases thespeed of the recognition search by a factor of 40 with noadditional pruning errors observed.10AcknowledgementThis work was supported by the Defense Advanced ResearchProjects Agency and monitored by the Office of Naval Re-search under Contract No.
N00014-89-C-0008.References\[I\] Austin, S., Peterson, P., Placeway, P., Schwartz, R,and Vandergrift, J., "Toward a Real-Time CommercialSystem Using Commercial Hardware".
Proceedings ofthe DARPA Speech and Natural Language WorkshopHidden Valley, June 1990 (1990).\[2\] Chow, Y-L. and Schwartz, R.M., "The N-Best Algo-rithm: An Efficient Procedure for Finding Top N Sen-tence Hypotheses".
Proceedings of the DARPA Speechand Natural Language Workshop Cape Cod, October1989 (1989).\[3\] Derr, A., and Schwartz, R.M., "A Simple Statisti-cal Class Grammar for Measuring Speech RecognitionPerformance".
Proceedings of the DARPA Speech andNatural Language Workshop Cape Cod, October 1989(1989).\[4\] Bahl, L.R., de Souza, P., Gopalakrishnan, P.S.,Kanevsky, D., and Nahamoo, D. "Constructing Groupsof Acoustically Confusable Words".
Proceedings of theICASSP 90, April, 1990.\[5\] Fissore, L., Micca, G., and Pieraccini, R., "Very LargeVocabulary Isolated Utterance Recognition: A Com-parison Between One Pass and Two Pass Strategies".Proceedings of the ICASSP 88, pp.
267-270, April,1988.\[6\] Murveit, H., "Integrating Natural Language Constraintsinto HMM-based Speech Recognition".
Proceedings ofthe ICASSP 90, April, 1990.\[7\] Rohlicek, J.A., Chow, Y-L., and Roucos, S., "Statis-tical Language Modeling Using a Small Corpus froman Application Domain".
Proceedings of the DARPASpeech and Natural Language Workshop Cambridge,October 1987 (1987).
Also in Proceedings of theICASSP 88, pp.
267-270, April, 1988.\[8\] Schwartz, R.M., Chow, Y., Kimball, O., Roucos, S.,Krasner, M., and Makhoul, J.
"Context-DependentModeling for Acoustic-Phonetic Recognition of Con-tinuous Speech".
Proceedings of the ICASSP 85, pp.1205-1208, March, 1985.\[9\] V. Steinbiss (1989) "Sentence-Hypotheses Generationin a Continuous-Speech Recognition System," Proc.of the European Conf.
on Speech Communciation a dTechnology, Paris, Sept. 1989, Vol.
2, pp.
51-54\[10\] Young, S. (1984) "Generating Multiple Solutions fromConnected Word DP Recognition Algorithms".
Proc.of the Institute of Acoustics, 1984, Vol.
6 Part 4, pp.351-35411
