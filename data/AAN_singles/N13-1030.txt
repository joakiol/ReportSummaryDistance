Proceedings of NAACL-HLT 2013, pages 298?305,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsKeyphrase Extraction for N-best Reranking in Multi-Sentence CompressionFlorian Boudin and Emmanuel MorinLINA - UMR CNRS 6241, Universite?
de Nantes, France{florian.boudin,emmanuel.morin}@univ-nantes.frAbstractMulti-Sentence Compression (MSC) is thetask of generating a short single sentence sum-mary from a cluster of related sentences.
Thispaper presents an N-best reranking methodbased on keyphrase extraction.
Compressioncandidates generated by a word graph-basedMSC approach are reranked according to thenumber and relevance of keyphrases they con-tain.
Both manual and automatic evaluationswere performed using a dataset made of clus-ters of newswire sentences.
Results show thatthe proposed method significantly improvesthe informativity of the generated compres-sions.1 IntroductionMulti-Sentence Compression (MSC) can be broadlydescribed as the task of generating a short single sen-tence summary from a cluster of related sentences.It has recently attracted much attention, mostly be-cause of its relevance to single or multi-documentextractive summarization.
A standard way to gen-erate summaries consists in ranking sentences byimportance, cluster them by similarity and select asentence from the top ranked clusters (Wang et al2008).
One difficulty is then to generate concise,non-redundant summaries.
Selected sentences al-most always contain additional information specificto the documents from which they came, leading toreadability issues in the summary.Sentence Compression (SC), i.e.
the task ofsummarizing a sentence while retaining most ofthe informational content and remaining grammat-ical (Jing, 2000), is a straightforward solution to thisproblem.
Another solution would be to create, foreach cluster of related sentences, a concise and flu-ent fusion of information, reflecting facts commonto all sentences.
Originally defined as sentence fu-sion (Barzilay and McKeown, 2005), MSC is a text-to-text generation process in which a novel sentenceis produced as a result of summarizing common in-formation across a set of similar sentences.Most of the previous MSC approaches rely onsyntactic parsers for producing grammatical com-pressions, e.g.
(Filippova and Strube, 2008; El-sner and Santhanam, 2011).
Recently, (Filippova,2010) proposed a word graph-based approach whichonly requires a Part-Of-Speech (POS) tagger and alist of stopwords.
The key assumption behind herapproach is that redundancy within the set of relatedsentences provides a reliable way of generating in-formative and grammatical sentences.
Although thisapproach seemingly works well, 48% to 60% of thegenerated sentences are missing important informa-tion about the set of related sentences.
In this study,we aim at producing more informative sentences bymaximizing the range of topics they cover.Keyphrases are words that capture the main top-ics of a document.
Extracting keyphrases can benefitvarious Natural Language Processing tasks such assummarization, information retrieval and question-answering (Kim et al 2010).
In summarization,keyphrases provide semantic metadata that representthe content of a document.
Sentences containing themost relevant keyphrases are used to generate thesummary (D?Avanzo and Magnini, 2005).
In thesame way, we hypothesize that keyphrases can beused to better generate sentences that convey the gist298of the set of related sentences.In this paper, we present a reranking methodof N-best multi-sentence compressions based onkeyphrase extraction and describe a series of experi-ments conducted on a manually constructed evalua-tion corpus.
More precisely, the main contributionsof our work are as follows:?
We extend Filippova (2010)?s word graph-based MSC approach to produce well-punctuated and more informative compres-sions.?
We investigate the use of automatic MachineTranslation (MT) and summarization evalua-tion metrics to evaluate MSC performance.?
We introduce a French evaluation dataset madeof 40 sets of related sentences along with refer-ence compressions composed by humans.The rest of this paper is organized as follows.
Wefirst briefly review the previous work, followed bya description of the method we propose.
Next, wegive the details of the evaluation dataset we haveconstructed and present our experiments and results.Lastly, we conclude with a discussion and directionsfor further work.2 Related work2.1 Multi-sentence compressionMSC have received much attention recently andmany different approaches have been proposed.
Thepioneering work of (Barzilay and McKeown, 2005)introduced the framework used by many subsequentworks: input sentences are represented by depen-dency trees, some words are aligned to merge thetrees into a lattice, and the lattice is linearized usingtree traversal to produce fusion sentences.
(Filip-pova and Strube, 2008) cast MSC as an integer linearprogram, and show promising results for German.Later, (Elsner and Santhanam, 2011) proposed a su-pervised approach trained on examples of manuallyfused sentences.Previously described approaches require the useof a syntactic parser to control the grammatical-ity of the output.
As an alternative, several wordgraph-based approaches that only require a POStagger were proposed.
The key assumption isthat redundancy provides a reliable way of gen-erating grammatical sentences.
First, a directedword graph is constructed from the set of input sen-tences in which nodes represent unique words, de-fined as word and POS tuples, and edges expressthe original structure of sentences (i.e.
word order-ing).
Sentence compressions are obtained by find-ing commonly used paths in the graph.
Word graph-based MSC approaches were used in different tasks,such as guided microblog summarization (Sharifiet al 2010), opinion summarization (Ganesan etal., 2010) and newswire summarization (Filippova,2010).2.2 Keyphrase extractionKeyphrases are words that are representative of themain content of documents.
Extracting keyphrasescan benefit various Natural Language Processingtasks such as summarization, information retrievaland question-answering (Kim et al 2010).
Previ-ous works fall into two categories: supervised andunsupervised methods.
The idea behind supervisedmethods is to recast keyphrase extraction as a binaryclassification task.
A model is trained using anno-tated data to determine whether a given phrase is akeyphrase or not (Frank et al 1999; Turney, 2000).Unsupervised approaches proposed so far have in-volved a number of techniques, including languagemodeling (Tomokiyo and Hurst, 2003), graph-basedranking (Mihalcea and Tarau, 2004; Wan and Xiao,2008) and clustering (Liu et al 2009).
While super-vised approaches have generally proven more suc-cessful, the need for training data and the bias to-wards the domain on which they are trained remaintwo critical issues.3 MethodIn this section, we first describe Filippova (2010)?sword graph-based MSC approach.
Then, we presentthe keyphrase extraction approach we use and ourmethod for reranking generated compressions.3.1 Description of Filippova?s approachLet G = (V, E) be a directed graph with the setof vertices (nodes) V and a set of directed edges E,where E is a subset of V ?
V .
Given a set of re-lated sentences S = {s1, s2, ..., sn}, a word graphis constructed by iteratively adding sentences to it.299world'spintatortoiseislandthelast kindhastobeknownasgiantaway-end-hislonesomegeorgeadiedthebelievedofpassed-start-the......(3)(2)Figure 1: Word graph constructed from the set of related sentences, a possible compression path is also given.Figure 1 is an illustration of the word graph con-structed from the following sentences.
For clarity,edge weights are omitted and italicized fragmentsfrom the sentences are replaced with dots.1.
Lonesome George, the world?s last Pinta Islandgiant tortoise, has passed away.2.
The giant tortoise known as Lonesome Georgedied Sunday at the Galapagos National Park inEcuador.3.
He was only about a hundred years old, butthe last known giant Pinta tortoise, LonesomeGeorge, has passed away.4.
Lonesome George, a giant tortoise believed tobe the last of his kind, has died.At the first step, the graph simply represents onesentence plus the start and end symbols (?start?
and?end?
in Figure 1).
A node is added to G for eachword in the sentence, and words adjacent in the sen-tence are connected with directed edges.
A wordfrom the following sentences is mapped onto an ex-isting node in the graph if they have the same lower-cased word form and POS and that no word from thissentence has already been mapped onto this node.
Anew node is created if there is no suitable candidatein the graph.Words are added to the graph in the following or-der:i. non-stopwords for which no candidate exists inthe graph or for which an unambiguous map-ping is possible;ii.
non-stopwords for which there are either sev-eral possible candidates in the graph or whichoccur more than once in the sentence;iii.
stopwords.For the last two groups of words where mappingis ambiguous (i.e.
there are two or more nodes inthe graph that refer to the same word/POS tuple),the immediate context (the preceding and followingwords in the sentence and the neighboring nodes inthe graph) or the frequency (i.e.
the node which haswords mapped onto it) are used to select the candi-date node.
We use the stopword list included in nltk1extended with temporal nouns (e.g.
monday, yester-day).In Filippova?s approach, punctuation marks areexcluded.
To generate well-punctuated compres-sions, we simply added a fourth step for addingpunctuation marks in the graph.
When mapping isambiguous, we select the candidate which has thesame immediate context.Once the words from a sentence are added to thegraph, words adjacent in the sentence are connectedwith directed edges.
Edge weights are calculated us-ing the weighting function defined in Equation 1.1http://nltk.org/300w(i, j) =cohesion(i, j)freq(i)?
freq(j)(1)cohesion(i, j) =freq(i) + freq(j)?s?S d(s, i, j)?1(2)where freq(i) is the number of words mapped to thenode i.
The function d(s, i, j) refers to the distancebetween the offset positions of words i and j in sen-tence s.The purpose of this function is two fold: i. togenerate a grammatical compression, links betweenwords which appear often in this order are favored(see Equation 2); ii.
to generate an informative com-pression, the weight of edges connecting salientnodes is decreased.A K-shortest paths algorithm is then used to findthe 50 shortest paths from start to end nodes in thegraph.
Paths shorter than eight words or that do notcontain a verb are filtered.
The remaining paths arereranked by normalizing the total path weight overits length.
The path which has the lightest averageedge weight is then considered as the best compres-sion.3.2 Reranking paths using keyphrasesThe main difficulty of MSC is to generate sentencesthat are both informative and grammatically correct.Here, redundancy within the set of input sentencesis used to identify important words and salient linksbetween words.
Although this approach seeminglyworks well, important information is missing in 48%to 60% of the generated sentences (Filippova, 2010).One of the reasons for this is that node salienceis estimated only with the frequency measure.
Totackle this issue, we propose to rerank the N-best listof compressions using keyphrases extracted fromthe set of related sentences.
Intuitively, an infor-mative sentence should contain the most relevantkeyphrases.
We propose to rerank generated com-pressions according to the number and relevance ofkeyphrases they contain.An unsupervised method based on (Wan andXiao, 2008) is used to extract keyphrases from eachset of related sentences.
This method is based onthe assumption that a word recommends other co-occurring words, and the strength of the recommen-dation is recursively computed based on the im-portance of the words making the recommendation.Keyphrase extraction can be divided into two steps.First, a weighted graph is constructed from the setof related sentences, in which nodes represent wordsdefined as word and POS tuples.
Two nodes (words)are connected if their corresponding lexical units co-occur within a sentence.
Edge weights are the num-ber of times two words co-occur.
TextRank (Mihal-cea and Tarau, 2004), a graph-based ranking algo-rithm that takes into account edge weights, is ap-plied for computing a salience score for each node.The score for node Vi is initialized with a defaultvalue and is computed in an iterative manner untilconvergence using this equation:S(Vi) = (1?d)+d?
?Vj?adj(Vi)wji?Vk?adj(Vi)wjkS(Vi)where adj(Vi) denotes the neighbors of Vi and d isthe damping factor set to 0.85.The second step consists in generating and scor-ing keyphrase candidates.
Sequences of adja-cent words satisfying a specific syntactic patternare collapsed into multi-word phrases.
We use(ADJ)*(NPP|NC)+(ADJ)* for French, in whichADJ are adjectives, NPP are proper nouns and NCare common nouns.The score of a candidate keyphrase k is computedby summing the salience scores of the words it con-tains normalized by its length + 1 to favor longern-grams (see equation 3).score(k) =?w?k TextRank(w)length(k) + 1(3)The small vocabulary size as well as the highredundancy within the set of related sentences aretwo factors that make keyphrase extraction easierto achieve.
On the other hand, a large numberof the generated keyphrases are redundant.
Somekeyphrases may be contained within larger ones,e.g.
giant tortoise and Pinta Island giant tortoise.
Tosolve this problem, generated keyphrases are clus-tered using word overlap.
For each cluster, we thenselect the keyphrase with the highest score.
This fil-tering process enables the generation of a smallersubset of keyphrases while having a better coverageof the cluster content.301Reranking techniques can suffer from the limitedscope of the N-best list, which may rule out manypotentially good candidates.
For this reason, we usea larger number of paths than the one in (Filippova,2010).
Accordingly, the K-shortest paths algorithmis used to find the 200 shortest paths.
We rerank thepaths by normalizing the total path weight over itslength multiplied by the sum of keyphrase scores itcontains.
The score of a sentence compression c isgiven by:score(c) =?i,j?path(c)w(i,j)length(c)?
?k?c score(k)(4)4 Experimental settings4.1 Construction of the evaluation datasetTo our knowledge, there is no dataset available toevaluate MSC in an automatic way.
The perfor-mance of the previously described approaches wasassessed by human judges.
In this work, we intro-duce a new evaluation dataset made of 40 sets of re-lated sentences along with reference compressionscomposed by human assessors.
The purpose of thisdataset is to investigate the use of existing automaticevaluation metrics for the MSC task.Similar to (Filippova, 2010), we collected newsarticles presented in clusters on the French edition ofGoogle News2 over a period of three months.
Clus-ters composed of at least 20 news articles and con-taining one single prevailing event were manuallyselected.
To obtain the sets of related sentences, weextracted the first sentences from each article in thecluster, removing duplicates.
Leading sentences innews articles are known to provide a good summaryof the article content and are used as a baseline insummarization (Dang, 2005).The resulting dataset contains 618 sentences (33tokens on average) spread over 40 clusters.
Thenumber of sentences within each cluster is on av-erage 15, with a minimum of 7 and a maximum of36.
The word redundancy rate within the dataset,computed as the number of unique words over thenumber of words for each cluster, is 38.8%.Three reference compressions were manuallycomposed for each set of sentences.
Human an-notators, all native French speakers, were asked to2http://news.google.frcarefully read the set of sentences, extract the mostsalient facts and generate a sentence (compression)that summarize the set of sentences.
Annotatorswere also told to introduce as little new vocabu-lary as possible in their compressions.
The purposeof this guideline is to reduce the number of possi-ble mismatches, as existing evaluation metrics arebased on n-gram comparison.
Reference compres-sions have a compression rate of 60%.4.2 Automatic evaluationThe use of automatic methods for evaluatingmachine-generated text has gradually become themainstream in Computational Linguistics.
Wellknown examples are the ROUGE (Lin, 2004) andBLEU (Papineni et al 2002) evaluation metrics usedin the summarization and MT communities.
Thesemetrics assess the quality of a system output by com-puting its similarity to one or more human-generatedreferences.Prior work in sentence compression use the F1measure over grammatical relations to evaluate can-didate compressions (Riezler et al 2003).
It wasshown to correlate significantly with human judg-ments (Clarke and Lapata, 2006) and behave sim-ilarly to BLEU (Unno et al 2006).
However,this metric is not entirely reliable as it depends onparser accuracy and the type of dependency relationsused (Napoles et al 2011).
In this work, the fol-lowing evaluation measures are considered relevant:BLEU3, ROUGE-1 (unigrams), ROUGE-2 (bigrams)and ROUGE-SU4 (bigrams with skip distance up to4 words)4.
ROUGE measures are computed usingstopword removal and French stemming 5.4.3 Manual evaluationThe quality of the generated compressions was as-sessed in an experiment with human raters.
Two as-pects were considered: grammaticality and informa-tivity.
Following previous work (Barzilay and McK-eown, 2005), we asked raters to assess grammati-cality on a 3-points scale: perfect (2 pts), if the com-pression is a complete grammatical sentence; almost3ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl4We use the version 1.5.5 of the ROUGE package availablefrom http://www.berouge.com5http://snowball.tartarus.org/302(1 pt), if it requires minor editing, e.g.
one mistakein articles, agreement or punctuation; ungrammati-cal (0 pts), if it is none of the above.
Raters were ex-plicitly asked to ignore lack of capitalization whileevaluating grammaticality.Informativity is evaluated according to the 3-points scale defined in (Filippova, 2010): perfect (2pts), if the compression conveys the gist of the mainevent and is more or less like the summary the per-son would produce himself; related (1 pt), if it isrelated to the the main theme but misses somethingimportant; unrelated (0 pts), if the compression isnot related to the main theme.Three raters, all native French speakers, werehired to assess the generated compressions.5 ResultsTo evaluate the effectiveness of our method, wecompare the compressions generated with Filip-pova?s approach (denoted as baseline) against theones obtained by reranking paths using keyphrases(denoted as KeyRank).
We evaluated the agreementbetween the three raters using Fleiss?s kappa (Art-stein and Poesio, 2008).
The ?
value is 0.56 whichdenotes a moderate agreement.Table 1 presents the average grammaticality andinformativity scores.
Results achieved by the base-line are consistent with the ones presented in (Fil-ippova, 2010).
We observe a significant improve-ment in informativity for KeyRank.
Grammaticalityscores are, however, slightly decreased.
One reasonfor that is the reranking we added to the shortest pathmethod that outputs longer compressions.
The aver-age length for our method is nevertheless drasticallyshorter than the average length of the input sentences(19 vs. 33 tokens).
This corresponds to a compres-sion rate (58%) that is close to the one observed onreference compressions (60%).Table 2 shows the distributions over the threescores for both grammaticality and informativity.We observe that 97.5% of the compressions gener-ated with KeyRank are related to the main themeof the cluster, and 62.5% convey the very gist ofit without missing any important information.
Thisrepresents an absolute increase of 19.2% over thebaseline.
Although our reranking method has lowergrammaticality scores, 65% of the generated sen-Method Gram.
Info.LengthCompRAvg.
Std.Dev.Baseline 1.63 1.33 16.3 4.8 50%KeyRank 1.53 1.60?
19 6.1 58%Table 1: Average ratings over all clusters and raters alongwith average compression length (in tokens), standard de-viation and corresponding compression rate (?
indicatessignificance at the 0.01 level using Student?s t-test).tences are perfectly grammatical.MethodGram.
Info.0 1 2 0 1 2Baseline 9.2% 18.3% 72.5% 10.0% 46.7% 43.3%KeyRank 11.7% 23.3% 65.0% 2.5% 35.0% 62.5%Table 2: Distribution over possible manual ratings forgrammaticality and informativity.
Ratings are expressedon a scale of 0 to 2.Table 3 shows the performance of the baselineand our reranking method in terms of ROUGE andBLEU scores.
KeyRank significantly outperformsthe baseline according to the different ROUGE met-rics.
This indicates an improvement in informativityfor the compressions generated using our method.We observe a large but not significant increase inBLEU scores.
The slightly decreased grammatical-ity scores could be a reason for this.
BLEU is essen-tially a precision metric, and it measures how well acompression candidate overlaps with multiple refer-ences.
Longer n-grams used by BLEU6 tend to scorefor grammaticality rather than content.Metric Baseline KeyRankROUGE-1 0.57441 0.65677?ROUGE-2 0.39212 0.44140?ROUGE-SU4 0.37004 0.43443?BLEU 0.61560 0.65770Table 3: Automatic evaluation scores (?
and ?
indicatesignificance at the 0.01 and 0.001 levels respectively us-ing Student?s t-test)To assess the effectiveness of automatic evalua-6BLEU measures are computed using 4-grams.303tion metrics, we compute the Pearson?s correlationcoefficient between ROUGE and BLEU scores andaveraged manual ratings.
According to Table 4, re-sults show medium to strong correlation betweenROUGE scores and informativity ratings.
On theother hand, BLEU scores better correlate with gram-maticality ratings.
Overall, automatic evaluationmetrics are not highly correlated with manual rat-ings.
One reason for that may be that the manualscore assignments are arbitrary (i.e.
0, 1, 2), and thata score of one is in fact closer to two than to zero.Results suggest that automatic metrics do give an in-dication of the compression quality, but can not re-place manual evaluation.Metric Gram.
Info.ROUGE-1 0.402 0.591ROUGE-2 0.432 0.494ROUGE-SU4 0.386 0.542BLEU 0.444 0.401Table 4: Pearson correlation coefficients for automaticmetrics vs. average human ratings.6 ConclusionThis paper presented a multi-sentence compres-sion approach that uses keyphrases to generatemore informative compressions.
We extended Fil-ippova (2010)?s word graph-based MSC approachby adding a re-reranking step that favors compres-sions that contain the most relevant keyphrases ofthe input sentence set.
An implementation of theproposed multi-sentence compression approach isavailable for download7.
We constructed an eval-uation dataset made of 40 sets of related sentencesalong with reference compressions composed by hu-mans.
This dataset is freely available for download8.We performed both manual and automatic evalua-tions and showed that our method significantly im-proves the informativity of the generated compres-sions.
We also investigated the correlation betweenmanual and automatic evaluation metrics and foundthat ROUGE and BLEU have a medium correlationwith manual ratings.7https://github.com/boudinfl/takahe8https://github.com/boudinfl/lina-mscIn future work, we intend to examine how gram-maticality of the generated compressions can be en-hanced.
Similar to the work of Hasan et al(2006) inthe Machine Translation field, we plan to experimentwith high order POS language models reranking.AcknowledgmentsThe authors would like to thank Sebastia?n Pen?a Sal-darriaga and Ophe?lie Lacroix for helpful commentson this work.
We thank the anonymous reviewers fortheir useful comments.
This work was supported bythe French Agence Nationale de la Recherche undergrant ANR-12-CORD-0027 and by the French Re-gion Pays de Loire in the context of the DEPARTproject (http://www.projetdepart.org/).ReferencesR.
Artstein and M. Poesio.
2008.
Inter-coder agreementfor computational linguistics.
Computational Linguis-tics, 34(4):555?596.Regina Barzilay and Kathleen R. McKeown.
2005.
Sen-tence fusion for multidocument news summarization.Computational Linguistics, 31(3):297?328.James Clarke and Mirella Lapata.
2006.
Models forsentence compression: A comparison across domains,training requirements and evaluation measures.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 377?384, Sydney, Australia, July.
Associationfor Computational Linguistics.Hoa Trang Dang.
2005.
Overview of duc 2005.
In Pro-ceedings of the Document Understanding Conference.Ernesto D?Avanzo and Bernardo Magnini.
2005.
Akeyphrase-based approach to summarization: the lakesystem at duc-2005.
In Proceedings of the DocumentUnderstanding Conference.Micha Elsner and Deepak Santhanam.
2011.
Learning tofuse disparate sentences.
In Proceedings of the Work-shop on Monolingual Text-To-Text Generation, pages54?63, Portland, Oregon, June.
Association for Com-putational Linguistics.Katja Filippova and Michael Strube.
2008.
Sentence fu-sion via dependency graph compression.
In Proceed-ings of the 2008 Conference on Empirical Methods inNatural Language Processing, pages 177?185, Hon-olulu, Hawaii, October.
Association for ComputationalLinguistics.Katja Filippova.
2010.
Multi-Sentence Compression:Finding Shortest Paths in Word Graphs.
In Proceed-304ings of the 23rd International Conference on Com-putational Linguistics (Coling 2010), pages 322?330,Beijing, China, August.
Coling 2010 Organizing Com-mittee.Eibe Frank, Gordon W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.Domain-specific keyphrase extraction.Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.2010.
Opinosis: A Graph Based Approach to Abstrac-tive Summarization of Highly Redundant Opinions.
InProceedings of the 23rd International Conference onComputational Linguistics (Coling 2010), pages 340?348, Beijing, China, August.
Coling 2010 OrganizingCommittee.S.
Hasan, O. Bender, and H. Ney.
2006.
Reranking trans-lation hypotheses using structural properties.
In Pro-ceedings of the EACL Workshop on Learning Struc-tured Information in Natural Language Applications,pages 41?48.Hongyan Jing.
2000.
Sentence reduction for auto-matic text summarization.
In Proceedings of the SixthConference on Applied Natural Language Processing,pages 310?315, Seattle, Washington, USA, April.
As-sociation for Computational Linguistics.Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-thy Baldwin.
2010.
Semeval-2010 task 5 : Automatickeyphrase extraction from scientific articles.
In Pro-ceedings of the 5th International Workshop on Seman-tic Evaluation, pages 21?26, Uppsala, Sweden, July.Association for Computational Linguistics.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization BranchesOut: Proceedings of the ACL-04 Workshop, pages 74?81, Barcelona, Spain, July.
Association for Computa-tional Linguistics.Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun.2009.
Clustering to find exemplar terms for keyphraseextraction.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Process-ing, pages 257?266, Singapore, August.
Associationfor Computational Linguistics.Rada Mihalcea and Paul Tarau.
2004.
Textrank: Bring-ing order into texts.
In Dekang Lin and Dekai Wu,editors, Proceedings of EMNLP 2004, pages 404?411,Barcelona, Spain, July.
Association for ComputationalLinguistics.Courtney Napoles, Benjamin Van Durme, and ChrisCallison-Burch.
2011.
Evaluating sentence compres-sion: Pitfalls and suggested remedies.
In Proceedingsof the Workshop on Monolingual Text-To-Text Gener-ation, pages 91?97, Portland, Oregon, June.
Associa-tion for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsylva-nia, USA, July.
Association for Computational Lin-guistics.Stefan Riezler, Tracy H. King, Richard Crouch, and An-nie Zaenen.
2003.
Statistical sentence condensationusing ambiguity packing and stochastic disambigua-tion methods for lexical-functional grammar.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology-Volume 1,pages 118?125.
Association for Computational Lin-guistics.Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.2010.
Summarizing Microblogs Automatically.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 685?688, Los Angeles, California, June.
Association forComputational Linguistics.Takashi Tomokiyo and Matthew Hurst.
2003.
A lan-guage model approach to keyphrase extraction.
InProceedings of the ACL 2003 Workshop on Multi-word Expressions: Analysis, Acquisition and Treat-ment, pages 33?40, Sapporo, Japan, July.
Associationfor Computational Linguistics.Peter D. Turney.
2000.
Learning algorithmsfor keyphrase extraction.
Information Retrieval,2(4):303?336.Yuya Unno, Takashi Ninomiya, Yusuke Miyao, andJun?ichi Tsujii.
2006.
Trimming cfg parse treesfor sentence compression using machine learning ap-proaches.
In Proceedings of the COLING/ACL 2006Main Conference Poster Sessions, pages 850?857,Sydney, Australia, July.
Association for Computa-tional Linguistics.Xiaojun Wan and Jianguo Xiao.
2008.
Collabrank:Towards a collaborative approach to single-documentkeyphrase extraction.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics(Coling 2008), pages 969?976, Manchester, UK, Au-gust.
Coling 2008 Organizing Committee.Dingding Wang, Tao Li, Shenghuo Zhu, and Chris Ding.2008.
Multi-document Summarization via Sentence-Level Semantic Analysis and Symmetric Matrix Fac-torization.
In Proceedings of the 31st annual inter-national ACM SIGIR conference on Research and de-velopment in information retrieval, SIGIR ?08, pages307?314, New York, NY, USA.
ACM.305
