Proceedings of ACL-08: HLT, pages 496?504,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCombining EM Training and the MDL Principle for anAutomatic Verb Classification incorporating Selectional PreferencesSabine Schulte im Walde, Christian Hying, Christian Scheible, Helmut SchmidInstitute for Natural Language ProcessingUniversity of Stuttgart, Germany{schulte,hyingcn,scheibcn,schmid}@ims.uni-stuttgart.deAbstractThis paper presents an innovative, complexapproach to semantic verb classification thatrelies on selectional preferences as verb prop-erties.
The probabilistic verb class model un-derlying the semantic classes is trained bya combination of the EM algorithm and theMDL principle, providing soft clusters withtwo dimensions (verb senses and subcategori-sation frames with selectional preferences) asa result.
A language-model-based evaluationshows that after 10 training iterations the verbclass model results are above the baseline re-sults.1 IntroductionIn recent years, the computational linguistics com-munity has developed an impressive number of se-mantic verb classifications, i.e., classifications thatgeneralise over verbs according to their semanticproperties.
Intuitive examples of such classifica-tions are the MOTION WITH A VEHICLE class, in-cluding verbs such as drive, fly, row, etc., or theBREAK A SOLID SURFACE WITH AN INSTRUMENTclass, including verbs such as break, crush, frac-ture, smash, etc.
Semantic verb classifications areof great interest to computational linguistics, specifi-cally regarding the pervasive problem of data sparse-ness in the processing of natural language.
Up tonow, such classifications have been used in applica-tions such as word sense disambiguation (Dorr andJones, 1996; Kohomban and Lee, 2005), machinetranslation (Prescher et al, 2000; Koehn and Hoang,2007), document classification (Klavans and Kan,1998), and in statistical lexical acquisition in gen-eral (Rooth et al, 1999; Merlo and Stevenson, 2001;Korhonen, 2002; Schulte im Walde, 2006).Given that the creation of semantic verb classi-fications is not an end task in itself, but dependson the application scenario of the classification, wefind various approaches to an automatic induction ofsemantic verb classifications.
For example, Siegeland McKeown (2000) used several machine learn-ing algorithms to perform an automatic aspectualclassification of English verbs into event and sta-tive verbs.
Merlo and Stevenson (2001) presentedan automatic classification of three types of Englishintransitive verbs, based on argument structure andheuristics to thematic relations.
Pereira et al (1993)and Rooth et al (1999) relied on the Expectation-Maximisation algorithm to induce soft clusters ofverbs, based on the verbs?
direct object nouns.
Sim-ilarly, Korhonen et al (2003) relied on the Informa-tion Bottleneck (Tishby et al, 1999) and subcate-gorisation frame types to induce soft verb clusters.This paper presents an innovative, complex ap-proach to semantic verb classes that relies on se-lectional preferences as verb properties.
The un-derlying linguistic assumption for this verb classmodel is that verbs which agree on their selec-tional preferences belong to a common seman-tic class.
The model is implemented as a soft-clustering approach, in order to capture the poly-semy of the verbs.
The training procedure uses theExpectation-Maximisation (EM) algorithm (Baum,1972) to iteratively improve the probabilistic param-eters of the model, and applies the Minimum De-scription Length (MDL) principle (Rissanen, 1978)to induce WordNet-based selectional preferences forarguments within subcategorisation frames.
Ourmodel is potentially useful for lexical induction(e.g., verb senses, subcategorisation and selectionalpreferences, collocations, and verb alternations),496and for NLP applications in sparse data situations.In this paper, we provide an evaluation based on alanguage model.The remainder of the paper is organised as fol-lows.
Section 2 introduces our probabilistic verbclass model, the EM training, and how we incor-porate the MDL principle.
Section 3 describes theclustering experiments, including the experimentalsetup, the evaluation, and the results.
Section 4 re-ports on related work, before we close with a sum-mary and outlook in Section 5.2 Verb Class Model2.1 Probabilistic ModelThis paper suggests a probabilistic model of verbclasses that groups verbs into clusters with simi-lar subcategorisation frames and selectional prefer-ences.
Verbs may be assigned to several clusters(soft clustering) which allows the model to describethe subcategorisation properties of several verb read-ings separately.
The number of clusters is definedin advance, but the assignment of the verbs to theclusters is learnt during training.
It is assumed thatall verb readings belonging to one cluster have simi-lar subcategorisation and selectional properties.
Theselectional preferences are expressed in terms of se-mantic concepts from WordNet, rather than a set ofindividual words.
Finally, the model assumes thatthe different arguments are mutually independent forall subcategorisation frames of a cluster.
From thelast assumption, it follows that any statistical depen-dency between the arguments of a verb has to be ex-plained by multiple readings.The statistical model is characterised by the fol-lowing equation which defines the probability of averb v with a subcategorisation frame f and argu-ments a1, ..., anf :p(v, f, a1, ..., anf ) =?cp(c) p(v|c) p(f |c) ?nf?i=1?r?Rp(r|c, f, i) p(ai|r)The model describes a stochastic process which gen-erates a verb-argument tuple like ?speak, subj-pp.to,professor, audience?
by1.
selecting some cluster c, e.g.
c3 (which mightcorrespond to a set of communication verbs),with probability p(c3),2. selecting a verb v, here the verb speak, fromcluster c3 with probability p(speak|c3),3. selecting a subcategorisation frame f , heresubj-pp.to, with probability p(subj-pp.to|c3);note that the frame probability only depends onthe cluster, and not on the verb,4.
selecting a WordNet concept r for each argu-ment slot, e.g.
person for the first slot withprobability p(person|c3, subj-pp.to, 1) and so-cial group for the second slot with probabilityp(social group|c3, subj-pp.to, 2),5. selecting a word ai to instantiate each con-cept as argument i; in our example, wemight choose professor for person withprobability p(professor|person) and au-dience for social group with probabilityp(audience|social group).The model contains two hidden variables, namelythe clusters c and the selectional preferences r. In or-der to obtain the overall probability of a given verb-argument tuple, we have to sum over all possible val-ues of these hidden variables.The assumption that the arguments are indepen-dent of the verb given the cluster is essential for ob-taining a clustering algorithm because it forces theEM algorithm to make the verbs within a cluster assimilar as possible.1 The assumption that the differ-ent arguments of a verb are mutually independent isimportant to reduce the parameter set to a tractablesizeThe fact that verbs select for concepts rather thanindividual words also reduces the number of param-eters and helps to avoid sparse data problems.
Theapplication of the MDL principle guarantees that noimportant information is lost.The probabilities p(r|c, f, i) and p(a|r) men-tioned above are not represented as atomic enti-ties.
Instead, we follow an approach by Abney1The EM algorithm adjusts the model parameters in such away that the probability assigned to the training tuples is max-imised.
Given the model constraints, the data probability canonly be maximised by making the verbs within a cluster as sim-ilar to each other as possible, regarding the required arguments.497and Light (1999) and turn WordNet into a HiddenMarkov model (HMM).
We create a new pseudo-concept for each WordNet noun and add it as a hy-ponym to each synset containing this word.
In ad-dition, we assign a probability to each hypernymy?hyponymy transition, such that the probabilities ofthe hyponymy links of a synset sum up to 1.
Thepseudo-concept nodes emit the respective word witha probability of 1, whereas the regular concept nodesare non-emitting nodes.
The probability of a pathin this (a priori) WordNet HMM is the product ofthe probabilities of the transitions within the path.The probability p(a|r) is then defined as the sumof the probabilities of all paths from the concept rto the word a.
Similarly, we create a partial Word-Net HMM for each argument slot ?c, f, i?
which en-codes the selectional preferences.
It contains onlythe WordNet concepts that the slot selects for, ac-cording to the MDL principle (cf.
Section 2.3), andthe dominating concepts.
The probability p(r|c, f, i)is the total probability of all paths from the top-mostWordNet concept entity to the terminal node r.2.2 EM TrainingThe model is trained on verb-argument tuples ofthe form described above, i.e., consisting of a verband a subcategorisation frame, plus the nominal2heads of the arguments.
The tuples may be ex-tracted from parsed data, or from a treebank.
Be-cause of the hidden variables, the model is trainediteratively with the Expectation-Maximisation algo-rithm (Baum, 1972).
The parameters are randomlyinitialised and then re-estimated with the Inside-Outside algorithm (Lari and Young, 1990) which isan instance of the EM algorithm for training Proba-bilistic Context-Free Grammars (PCFGs).The PCFG training algorithm is applicable herebecause we can define a PCFG for each of our mod-els which generates the same verb-argument tupleswith the same probability.
The PCFG is defined asfollows:(1) The start symbol is TOP.
(2) For each cluster c, we add a rule TOP ?
Vc Acwhose probability is p(c).2Arguments with lexical heads other than nouns (e.g., sub-categorised clauses) are not included in the selectional prefer-ence induction.
(3) For each verb v in cluster c, we add a ruleVc ?
v with probability p(v|c).
(4) For each subcategorisation frame f of cluster cwith length n, we add a rule Ac ?
f Rc,f,1,entity... Rc,f,n,entity with probability p(f |c).
(5) For each transition from a node r to a node r?in the selectional preference model for slot i ofthe subcategorisation frame f of cluster c, weadd a rule Rc,f,i,r ?
Rc,f,i,r?
whose probabilityis the transition probability from r to r?
in therespective WordNet-HMM.
(6) For each terminal node r in the selectional pref-erence model, we add a rule Rc,f,i,r ?
Rr whoseprobability is 1.
With this rule, we ?jump?
fromthe selectional restriction model to the corre-sponding node in the a priori model.
(7) For each transition from a node r to a node r?in the a priori model, we add a rule Rr ?
Rr?whose probability is the transition probabilityfrom r to r?
in the a priori WordNet-HMM.
(8) For each word node a in the a priori model, weadd a rule Ra ?
a whose probability is 1.Based on the above definitions, a partial ?parse?
for?speak subj-pp.to professor audience?, referring tocluster 3 and one possible WordNet path, is shown inFigure 1.
The connections within R3 (R3,...,entity?R3,...,person/group) and within R (Rperson/group?Rprofessor/audience) refer to sequential applicationsof rule types (5) and (7), respectively.TOPV3speakA3subj-pp.to R3,subj?pp.to,1,entityR3,subj?pp.to,1,personRpersonRprofessorprofessorR3,subj?pp.to,2,entityR3,subj?pp.to,2,groupRgroupRaudienceaudienceFigure 1: Example parse tree.The EM training algorithm maximises the likelihoodof the training data.4982.3 MDL PrincipleA model with a large number of fine-grained con-cepts as selectional preferences assigns a higherlikelihood to the data than a model with a small num-ber of general concepts, because in general a largernumber of parameters is better in describing train-ing data.
Consequently, the EM algorithm a pri-ori prefers fine-grained concepts but ?
due to sparsedata problems ?
tends to overfit the training data.
Inorder to find selectional preferences with an appro-priate granularity, we apply the Minimum Descrip-tion Length principle, an approach from InformationTheory.
According to the MDL principle, the modelwith minimal description length should be chosen.The description length itself is the sum of the modellength and the data length, with the model lengthdefined as the number of bits needed to encode themodel and its parameters, and the data length de-fined as the number of bits required to encode thetraining data with the given model.
According tocoding theory, an optimal encoding uses ?log2pbits, on average, to encode data whose probabilityis p. Usually, the model length increases and thedata length decreases as more parameters are addedto a model.
The MDL principle finds a compromisebetween the size of the model and the accuracy ofthe data description.Our selectional preference model relies on Li andAbe (1998), applying the MDL principle to deter-mine selectional preferences of verbs and their argu-ments, by means of a concept hierarchy ordered byhypernym/hyponym relations.
Given a set of nounswithin a specific argument slot as a sample, the ap-proach finds the cut3 in a concept hierarchy whichminimises the sum of encoding both the model andthe data.
The model length (ML) is defined asML = k2 ?
log2 |S|,with k the number of concepts in the partial hierar-chy between the top concept and the concepts in thecut, and |S| the sample size, i.e., the total frequencyof the data set.
The data length (DL) is defined asDL = ?
?n?Slog2 p(n).3A cut is defined as a set of concepts in the concept hier-archy that defines a partition of the ?leaf?
concepts (the lowestconcepts in the hierarchy), viewing each concept in the cut asrepresenting the set of all leaf concepts it dominates.The probability of a noun p(n) is determined by di-viding the total probability of the concept class thenoun belongs to, p(concept), by the size of thatclass, |concept|, i.e., the number of nouns that aredominated by that concept:p(n) = p(concept)|concept| .The higher the concept within the hierarchy, themore nouns receive an equal probability, and thegreater is the data length.The probability of the concept class in turn is de-termined by dividing the frequency of the conceptclass f(concept) by the sample size:p(concept) = f(concept)|S| ,where f(concept) is calculated by upward propaga-tion of the frequencies of the nominal lexemes fromthe data sample through the hierarchy.
For exam-ple, if the nouns coffee, tea, milk appeared with fre-quencies 25, 50, 3, respectively, within a specific ar-gument slot, then their hypernym concept beveragewould be assigned a frequency of 78, and these 78would be propagated further upwards to the next hy-pernyms, etc.
As a result, each concept class is as-signed a fraction of the frequency of the whole dataset (and the top concept receives the total frequencyof the data set).
For calculating p(concept) (and theoverall data length), though, only the concept classeswithin the cut through the hierarchy are relevant.Our model uses WordNet 3.0 as the concept hier-archy, and comprises one (complete) a priori Word-Net model for the lexical head probabilities p(a|r)and one (partial) model for each selectional proba-bility distribution p(r|c, f, i), cf.
Section 2.1.2.4 Combining EM and MDLThe training procedure that combines the EM train-ing with the MDL principle can be summarised asfollows.1.
The probabilities of a verb class model with cclasses and a pre-defined set of verbs and framesare initialised randomly.
The selectional preferencemodels start out with the most general WordNet con-cept only, i.e., the partial WordNet hierarchies un-derlying the probabilities p(r|c, f, i) initially onlycontain the concept r for entity.4992.
The model is trained for a pre-defined num-ber of iterations.
In each iteration, not only themodel probabilities are re-estimated and maximised(as done by EM), but also the cuts through the con-cept hierarchies that represent the various selectionalpreference models are re-assessed.
In each iteration,the following steps are performed.
(a) The partial WordNet hierarchies that representthe selectional preference models are expanded toinclude the hyponyms of the respective leaf con-cepts of the partial hierarchies.
I.e., in the first itera-tion, all models are expanded towards the hyponymsof entity, and in subsequent iterations each selec-tional preference model is expanded to include thehyponyms of the leaf nodes in the partial hierarchiesresulting from the previous iteration.
This expansionstep allows the selection models to become more andmore detailed, as the training proceeds and the verbclusters (and their selectional restrictions) becomeincreasingly specific.
(b) The training tuples are processed: For each tu-ple, a PCFG parse forest as indicated by Figure 1is done, and the Inside-Outside algorithm is appliedto estimate the frequencies of the ?parse tree rules?,given the current model probabilities.
(c) The MDL principle is applied to each selectionalpreference model: Starting from the respective leafconcepts in the partial hierarchies, MDL is calcu-lated to compare each set of hyponym concepts thatshare a hypernym with the respective hypernym con-cept.
If the MDL is lower for the set of hyponymsthan the hypernym, the hyponyms are left in the par-tial hierarchy.
Otherwise the expansion of the hyper-nym towards the hyponyms is undone and we con-tinue recursively upwards the hierarchy, calculatingMDL to compare the former hypernym and its co-hyponyms with the next upper hypernym, etc.
Therecursion allows the training algorithm to removenodes which were added in earlier iterations and areno longer relevant.
It stops if the MDL is lower forthe hyponyms than for the hypernym.This step results in selectional preference modelsthat minimally contain the top concept entity, andmaximally contain the partial WordNet hierarchybetween entity and the concept classes that havebeen expanded within this iteration.
(d) The probabilities of the verb class model aremaximised based on the frequency estimates ob-tained in step (b).3 ExperimentsThe model is generally applicable to all languagesfor which WordNet exists, and for which the Word-Net functions provided by Princeton University areavailable.
For the purposes of this paper, we chooseEnglish as a case study.3.1 Experimental SetupThe input data for training the verb class mod-els were derived from Viterbi parses of the wholeBritish National Corpus, using the lexicalised PCFGfor English by Carroll and Rooth (1998).
We tookonly active clauses into account, and disregardedauxiliary and modal verbs as well as particle verbs,leaving a total of 4,852,371 Viterbi parses.
Those in-put tuples were then divided into 90% training dataand 10% test data, providing 4,367,130 training tu-ples (over 2,769,804 types), and 485,241 test tuples(over 368,103 types).As we wanted to train and assess our verb classmodel under various conditions, we used differentfractions of the training data in different trainingregimes.
Because of time and memory constraints,we only used training tuples that appeared at leasttwice.
(For the sake of comparison, we also trainedone model on all tuples.)
Furthermore, we dis-regarded tuples with personal pronoun arguments;they are not represented in WordNet, and even ifthey are added (e.g.
to general concepts such asperson, entity) they have a rather destructive ef-fect.
We considered two subsets of the subcate-gorisation frames with 10 and 20 elements, whichwere chosen according to their overall frequency inthe training data; for example, the 10 most frequentframe types were subj:obj, subj, subj:ap, subj:to,subj:obj:obj2, subj:obj:pp-in, subj:adv, subj:pp-in,subj:vbase, subj:that.4 When relying on theses10/20 subcategorisation frames, plus including theabove restrictions, we were left with 39,773/158,134and 42,826/166,303 training tuple types/tokens, re-spectively.
The overall number of training tuples4A frame lists its arguments, separated by ?:?.
Most argu-ments within the frame types should be self-explanatory.
ap isan adjectival phrase.500was therefore much smaller than the generally avail-able data.
The corresponding numbers including tu-ples with a frequency of one were 478,717/597,078and 577,755/701,232.The number of clusters in the experiments was ei-ther 20 or 50, and we used up to 50 iterations overthe training tuples.
The model probabilities wereoutput after each 5th iteration.
The output comprisesall model probabilities introduced in Section 2.1.The following sections describe the evaluation of theexperiments, and the results.3.2 EvaluationOne of the goals in the development of the presentedverb class model was to obtain an accurate statisticalmodel of verb-argument tuples, i.e.
a model whichprecisely predicts the tuple probabilities.
In orderto evaluate the performance of the model in this re-spect, we conducted an evaluation experiment, inwhich we computed the probability which the verbclass model assigns to our test tuples and comparedit to the corresponding probability assigned by abaseline model.
The model with the higher proba-bility is judged the better model.We expected that the verb class model wouldperform better than the baseline model on tupleswhere one or more of the arguments were not ob-served with the respective verb, because either theargument itself or a semantically similar argument(according to the selectional preferences) was ob-served with verbs belonging to the same cluster.
Wealso expected that the verb class model assigns alower probability than the baseline model to test tu-ples which frequently occurred in the training data,since the verb class model fails to describe preciselythe idiosyncratic properties of verbs which are notshared by the other verbs of its cluster.The Baseline Model The baseline model decom-poses the probability of a verb-argument tuple into aproduct of conditional probabilities:5p(v, f, anf1 ) = p(v) p(f |v)nf?i=1p(ai|ai?11 , ?v, f?, fi)5fi is the label of the ith slot.
The verb and the subcategori-sation frame are enclosed in angle brackets because they aretreated as a unit during smoothing.The probability of our example tuple ?speak,subj-pp.to, professor, audience?
in the base-line model is then p(speak) p(subj-pp.to|speak)p(professor|?speak, subj-pp.to?, subj) p(audience|professor, ?speak, subj-pp.to?, pp.to).The model contains no hidden variables.
Thus theparameters can be directly estimated from the train-ing data with relative frequencies.
The parameterestimates are smoothed with modified Kneser-Neysmoothing (Chen and Goodman, 1998), such thatthe probability of each tuple is positive.Smoothing of the Verb Class Model Althoughthe verb class model has a built-in smoothing capac-ity, it needs additional smoothing for two reasons:Firstly, some of the nouns in the test data did notoccur in the training data.
The verb class modelassigns a zero probability to such nouns.
Hencewe smoothed the concept instantiation probabilitiesp(noun|concept) with Witten-Bell smoothing (Chenand Goodman, 1998).
Secondly, we smoothed theprobabilities of the concepts in the selectional pref-erence models where zero probabilities may occur.The smoothing ensures that the verb class modelassigns a positive probability to each verb-argumenttuple with a known verb, a known subcategorisationframe, and arguments which are in WordNet.
Othertuples were excluded from the evaluation becausethe verb class model cannot deal with them.3.3 ResultsThe evaluation results of our classification experi-ments are presented in Table 1, for 20 and 50 clus-ters, with 10 and 20 subcategorisation frame types.The table cells provide the loge of the probabilitiesper tuple token.
The probabilities increase with thenumber of iterations, flattening out after approx.
25iterations, as illustrated by Figure 2.
Both for 10and 20 frames, the results are better for 50 than for20 clusters, with small differences between 10 and20 frames.
The results vary between -11.850 and-10.620 (for 5-50 iterations), in comparison to base-line values of -11.546 and -11.770 for 10 and 20frames, respectively.
The results thus show that ourverb class model results are above the baseline re-sults after 10 iterations; this means that our statis-tical model then assigns higher probabilities to thetest tuples than the baseline model.501No.
of IterationClusters 5 10 15 20 25 30 35 40 45 5010 frames20 -11.770 -11.408 -10.978 -10.900 -10.853 -10.841 -10.831 -10.823 -10.817 -10.81250 -11.850 -11.452 -11.061 -10.904 -10.730 -10.690 -10.668 -10.628 -10.625 -10.62020 frames20 -11.769 -11.430 -11.186 -10.971 -10.921 -10.899 -10.886 -10.875 -10.873 -10.86950 -11.841 -11.472 -11.018 -10.850 -10.737 -10.728 -10.706 -10.680 -10.662 -10.648Table 1: Clustering results ?
BNC tuples.Figure 2: Illustration of clustering results.Including input tuples with a frequency of one inthe training data with 10 subcategorisation frames(as mentioned in Section 3.1) decreases the loge pertuple to between -13.151 and -12.498 (for 5-50 it-erations), with similar training behaviour as in Fig-ure 2, and in comparsion to a baseline of -17.988.The differences in the result indicate that the mod-els including the hapax legomena are worse than themodels that excluded the sparse events; at the sametime, the differences between baseline and cluster-ing model are larger.In order to get an intuition about the qualitativeresults of the clusterings, we select two exampleclusters that illustrate that the idea of the verb classmodel has been realised within the clusters.
Ac-cording to our own intuition, the clusters are over-all semantically impressive, beyond the examples.Future work will assess by semantics-based eval-uations of the clusters (such as pseudo-word dis-ambiguation, or a comparison against existing verbclassifications), whether this intuition is justified,whether it transfers to the majority of verbs withinthe cluster analyses, and whether the clusters cap-ture polysemic verbs appropriately.The two examples are taken from the 10 frame/50cluster verb class model, with probabilities of 0.05and 0.04.
The ten most probable verbs in the firstcluster are show, suggest, indicate, reveal, find, im-ply, conclude, demonstrate, state, mean, with thetwo most probable frame types subj and subj:that,i.e., the intransitive frame, and a frame that subcat-egorises a that clause.
As selectional preferenceswithin the intransitive frame (and quite similarlyin the subj:that frame), the most probable conceptclasses6 are study, report, survey, name, research,result, evidence.
The underlined nouns representspecific concept classes, because they are leaf nodesin the selectional preference hierarchy, thus refer-ring to very specific selectional preferences, whichare potentially useful for collocation induction.
Theten most probable verbs in the second cluster arearise, remain, exist, continue, need, occur, change,improve, begin, become, with the intransitive framebeing most probable.
The most probable conceptclasses are problem, condition, question, naturalphenomenon, situation.
The two examples illustratethat the verbs within a cluster are semantically re-lated, and that they share obvious subcategorisationframes with intuitively plausible selectional prefer-ences.4 Related WorkOur model is an extension of and thus most closelyrelated to the latent semantic clustering (LSC) model(Rooth et al, 1999) for verb-argument pairs ?v, a?which defines their probability as follows:p(v, a) =?cp(c) p(v|c) p(a|c)In comparison to our model, the LSC model onlyconsiders a single argument (such as direct objects),6For readability, we only list one noun per WordNet concept.502or a fixed number of arguments from one particu-lar subcategorisation frame, whereas our model de-fines a probability distribution over all subcategori-sation frames.
Furthermore, our model specifies se-lectional preferences in terms of general WordNetconcepts rather than sets of individual words.In a similar vein, our model is both similar anddistinct in comparison to the soft clustering ap-proaches by Pereira et al (1993) and Korhonen etal.
(2003).
Pereira et al (1993) suggested determin-istic annealing to cluster verb-argument pairs intoclasses of verbs and nouns.
On the one hand, theirmodel is asymmetric, thus not giving the same in-terpretation power to verbs and arguments; on theother hand, the model provides a more fine-grainedclustering for nouns, in the form of an additional hi-erarchical structure of the noun clusters.
Korhonenet al (2003) used verb-frame pairs (instead of verb-argument pairs) to cluster verbs relying on the Infor-mation Bottleneck (Tishby et al, 1999).
They hada focus on the interpretation of verbal polysemy asrepresented by the soft clusters.
The main differenceof our model in comparison to the above two modelsis, again, that we incorporate selectional preferences(rather than individual words, or subcategorisationframes).In addition to the above soft-clustering models,various approaches towards semantic verb classifi-cation have relied on hard-clustering models, thussimplifying the notion of verbal polysemy.
Twolarge-scale approaches of this kind are Schulte imWalde (2006), who used k-Means on verb subcat-egorisation frames and verbal arguments to clusterverbs semantically, and Joanis et al (2008), who ap-plied Support Vector Machines to a variety of verbfeatures, including subcategorisation slots, tense,voice, and an approximation to animacy.
To thebest of our knowledge, Schulte im Walde (2006) isthe only hard-clustering approach that previously in-corporated selectional preferences as verb features.However, her model was not soft-clustering, andshe only used a simple approach to represent selec-tional preferences by WordNet?s top-level concepts,instead of making use of the whole hierarchy andmore sophisticated methods, as in the current paper.Last but not least, there are other models of se-lectional preferences than the MDL model we usedin our paper.
Most such models also rely on theWordNet hierarchy (Resnik, 1997; Abney and Light,1999; Ciaramita and Johnson, 2000; Clark and Weir,2002).
Brockmann and Lapata (2003) comparedsome of the models against human judgements onthe acceptability of sentences, and demonstrated thatthe models were significantly correlated with humanratings, and that no model performed best; rather,the different methods are suited for different argu-ment relations.5 Summary and OutlookThis paper presented an innovative, complex ap-proach to semantic verb classes that relies on se-lectional preferences as verb properties.
The prob-abilistic verb class model underlying the semanticclasses was trained by a combination of the EM al-gorithm and the MDL principle, providing soft clus-ters with two dimensions (verb senses and subcate-gorisation frames with selectional preferences) as aresult.
A language model-based evaluation showedthat after 10 training iterations the verb class modelresults are above the baseline results.We plan to improve the verb class model with re-spect to (i) a concept-wise (instead of a cut-wise)implementation of the MDL principle, to operate onconcepts instead of combinations of concepts; and(ii) variations of the concept hierarchy, using e.g.
thesense-clustered WordNets from the Stanford Word-Net Project (Snow et al, 2007), or a WordNet ver-sion improved by concepts from DOLCE (Gangemiet al, 2003), to check on the influence of concep-tual details on the clustering results.
Furthermore,we aim to use the verb class model in NLP tasks, (i)as resource for lexical induction of verb senses, verbalternations, and collocations, and (ii) as a lexicalresource for the statistical disambiguation of parsetrees.ReferencesSteven Abney and Marc Light.
1999.
Hiding a Seman-tic Class Hierarchy in a Markow Model.
In Proceed-ings of the ACL Workshop on Unsupervised Learningin Natural Language Processing, pages 1?8, CollegePark, MD.Leonard E. Baum.
1972.
An Inequality and AssociatedMaximization Technique in Statistical Estimation forProbabilistic Functions of Markov Processes.
Inequal-ities, III:1?8.503Carsten Brockmann and Mirella Lapata.
2003.
Evaluat-ing and Combining Approaches to Selectional Prefer-ence Acquisition.
In Proceedings of the 10th Confer-ence of the European Chapter of the Association forComputational Linguistics, pages 27?34, Budapest,Hungary.Glenn Carroll and Mats Rooth.
1998.
Valence Inductionwith a Head-Lexicalized PCFG.
In Proceedings of the3rd Conference on Empirical Methods in Natural Lan-guage Processing, Granada, Spain.Stanley Chen and Joshua Goodman.
1998.
An EmpiricalStudy of Smoothing Techniques for Language Model-ing.
Technical Report TR-10-98, Center for Researchin Computing Technology, Harvard University.Massimiliano Ciaramita and Mark Johnson.
2000.
Ex-plaining away Ambiguity: Learning Verb SelectionalPreference with Bayesian Networks.
In Proceedingsof the 18th International Conference on Computa-tional Linguistics, pages 187?193, Saarbru?cken, Ger-many.Stephen Clark and David Weir.
2002.
Class-Based Prob-ability Estimation using a Semantic Hierarchy.
Com-putational Linguistics, 28(2):187?206.Bonnie J. Dorr and Doug Jones.
1996.
Role of WordSense Disambiguation in Lexical Acquisition: Predict-ing Semantics from Syntactic Cues.
In Proceedings ofthe 16th International Conference on ComputationalLinguistics, pages 322?327, Copenhagen, Denmark.Aldo Gangemi, Nicola Guarino, Claudio Masolo, andAlessandro Oltramari.
2003.
Sweetening WordNetwith DOLCE.
AI Magazine, 24(3):13?24.Eric Joanis, Suzanne Stevenson, and David James.
2008?A General Feature Space for Automatic Verb Classifi-cation.
Natural Language Engineering.
To appear.Judith L. Klavans and Min-Yen Kan. 1998.
The Roleof Verbs in Document Analysis.
In Proceedings ofthe 17th International Conference on ComputationalLinguistics and the 36th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 680?686,Montreal, Canada.Philipp Koehn and Hieu Hoang.
2007.
Factored Trans-lation Models.
In Proceedings of the Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 868?876, Prague, Czech Republic.Upali S. Kohomban and Wee Sun Lee.
2005.
LearningSemantic Classes for Word Sense Disambiguation.
InProceedings of the 43rd Annual Meeting on Associa-tion for Computational Linguistics, pages 34?41, AnnArbor, MI.Anna Korhonen, Yuval Krymolowski, and Zvika Marx.2003.
Clustering Polysemic Subcategorization FrameDistributions Semantically.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 64?71, Sapporo, Japan.Anna Korhonen.
2002.
Subcategorization Acquisition.Ph.D.
thesis, University of Cambridge, Computer Lab-oratory.
Technical Report UCAM-CL-TR-530.Karim Lari and Steve J.
Young.
1990.
The Estimation ofStochastic Context-Free Grammars using the Inside-Outside Algorithm.
Computer Speech and Language,4:35?56.Hang Li and Naoki Abe.
1998.
Generalizing CaseFrames Using a Thesaurus and the MDL Principle.Computational Linguistics, 24(2):217?244.Paola Merlo and Suzanne Stevenson.
2001.
AutomaticVerb Classification Based on Statistical Distributionsof Argument Structure.
Computational Linguistics,27(3):373?408.Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.Distributional Clustering of English Words.
In Pro-ceedings of the 31st Annual Meeting of the Associ-ation for Computational Linguistics, pages 183?190,Columbus, OH.Detlef Prescher, Stefan Riezler, and Mats Rooth.
2000.Using a Probabilistic Class-Based Lexicon for LexicalAmbiguity Resolution.
In Proceedings of the 18th In-ternational Conference on Computational Linguistics.Philip Resnik.
1997.
Selectional Preference and SenseDisambiguation.
In Proceedings of the ACL SIGLEXWorkshop on Tagging Text with Lexical Semantics:Why, What, and How?, Washington, DC.Jorma Rissanen.
1978.
Modeling by Shortest Data De-scription.
Automatica, 14:465?471.Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-roll, and Franz Beil.
1999.
Inducing a SemanticallyAnnotated Lexicon via EM-Based Clustering.
In Pro-ceedings of the 37th Annual Meeting of the Associationfor Computational Linguistics, Maryland, MD.Sabine Schulte im Walde.
2006.
Experiments on the Au-tomatic Induction of German Semantic Verb Classes.Computational Linguistics, 32(2):159?194.Eric V. Siegel and Kathleen R. McKeown.
2000.Learning Methods to Combine Linguistic Indica-tors: Improving Aspectual Classification and Reveal-ing Linguistic Insights.
Computational Linguistics,26(4):595?628.Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-drew Y. Ng.
2007.
Learning to Merge Word Senses.In Proceedings of the joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, Prague, CzechRepublic.Naftali Tishby, Fernando Pereira, and William Bialek.1999.
The Information Bottleneck Method.
In Pro-ceedings of the 37th Annual Conference on Communi-cation, Control, and Computing, Monticello, IL.504
