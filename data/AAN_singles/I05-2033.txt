POS tagger combinations on Hungarian textAndra?s Kuba, La?szlo?
Felfo?ldi, Andra?s KocsorResearch Group on Artificial Intelligence,University of Szeged,Aradi vrt.
1, H-6720, Hungary{akuba, lfelfold, kocsor}@inf.u-szeged.huAbstractIn this paper we will briefly surveythe key results achieved so far inHungarian POS tagging and showhow classifier combination tech-niques can aid the POS taggers.Methods are evaluated on a manu-ally annotated corpus containing 1.2million words.
POS tagger testswere performed on single-domain,multiple domain and cross-domaintest settings, and, to improve the ac-curacy of the taggers, various com-bination rules were implemented.The results indicate that combina-tion schemas (like the Boosting al-gorithm) are promising tools whichcan significantly degrade the classi-fication errors, and produce a moreeffective tagger application.1 Introduction an related worksPart-of-speech (POS) tagging is perhaps oneof the most basic tasks in natural languageprocessing.
In this paper we will reviewthe current state-of-the-art in Hungarian POStagging, and investigate the possibilities ofimproving the results of the taggers by ap-plying classifier combination techniques.We used a Transformation Based Learner(TBL) as the base tagger for combinationexperiments, and the learner algorithm de-termines the set of applicable combinationschemes.
From this set we chose two algo-rithms called Bagging and Adaboost.M1.In the next subsection, the most impor-tant published results of the last few yearsin Hungarian POS tagging are summarized.The TBL tagger is described in detail in Sec-tion 2, then the corpora and the data setswe used for our investigations are presentedin Section ??.
The classifier combination anddetails about the implementation of this tech-nique are described in Section 3.
After theresults of the boosted tagger are presented inSection 4.
Lastly, some conclusions about theeffectiveness and efficiency of our boosting ap-proach are made in the final section.1.1 POS Tagging of Hungarian TextsStandard POS tagging methods were appliedto Hungarian as soon as the first annotatedcorpora appeared that were big enough toserve as a training database for various meth-ods.
The TELRI corpus (Dimitrova et al,1998) was the first corpus that was usedfor testing different POS tagging methods.This corpus contains approximately 80, 000words.
Later, as the Hungarian National Cor-pus (Va?radi, 2002) and the Manually Anno-tated Hungarian Corpus (the Szeged Corpus)(Alexin et al, 2003) became available, an op-portunity was provided to test the results onbigger corpora (153M and 1.2M words, re-spectively).In recent years several authors have pub-lished many useful POS tagging results inHungarian.
It is generally believed that, ow-191ing to the fairly free word order and the ag-glutinative property of the Hungarian lan-guage, there are more special problems as-sociated with Hungarian than those of theIndo-European languages.
However, the lat-est results are comparable to results achievedin English and other well-studied languages.Fruitful approaches for Hungarian POS tag-ging are Hidden Markov Models, Transforma-tion Based Learning and rule-based learningmethods.One of the most common POS tagging ap-proaches is to build a tagger based on Hid-den Markov Models (HMM).
Tufis (Tufis etal., 2000) reported good results with the Tri-grams and Tags (TnT) tagger (Brants, 2000).A slightly better version of TnT was employedby Oravecz (Oravecz and Dienes, 2002), andit achieved excellent results.
In their pa-per, Oravecz and Dienes (Oravecz and Dienes,2002) argue that regardless of the rich mor-phology and relatively free word order, thePOS tagging of Hungarian with HMM meth-ods is possible and effective once one is ableto handle the data sparsity problem.
Theyused a modified version of TnT that was sup-ported by an external morphological analyzer.In this way the trigram tagger was able tomake better guesses about the unseen wordsand therefore to get better results.
An ex-ample of the results achieved by this trigramtagger is presented in the first row of Table 1.Another approach besides the statisticalmethods is the rule-based learning one.
Avaluable feature of the rule-based methods isthat the rules these methods work with areusually more intelligible to humans than theparameters of statistical methods.
For Hun-garian, a few such approaches are available inthe literature.In a comprehensive investigation,Horva?th et al (Horva?th et al, 1999) appliedfive different machine learning methods toHungarian POS tagging.
They tested C4.5,PHM, RIBL, Progol and AGLEARN (Alexinet al, 1999) methods on the TELRI corpus.The results of C4.5 and the best tagger foundin this investigation (RIBL) are presentedin the second and third rows of Table 1.Tagger AccuracyTnT + Morph.
Ana.
98.11%C4.5 97.60%Best method (RIBL) 98.03%RGLearn 97.32%TBL 91.94%TBL + Morph.
Ana.
96.52%Best combination 96.95%Table 1: Results achieved by various Hungar-ian POS taggers.Ho?cza (Ho?cza et al, 2003) used a differentrule generalization method called RGLearn.Row 4 shows the test results of that taggerin Table 1.
Transformation Based Learningis a rule-based method that we will discussin depth in Section 2.
Megyesi (Megyesi,1999) and Kuba et al (Kuba et al, 2004)produced results with TBL taggers thatare given in Table 1, in rows 5 and 6,respectively.
Kuba et al (Kuba et al, 2004)performed experiments with combinations ofvarious tagger methods.
The combinationsoutperformed their component taggers inalmost every case.
However, in the differenttest sets, different combinations proved thebest, so no conclusion could be drawn aboutthe best combination.
The combined taggerthat performed best on the largest test set isshown in row 7 of Table 1.2 The TBL taggerTransformation Based Learning (TBL) wasintroduced by Brill (Brill, 1995) for the taskof POS tagging.
Brill?s implementation con-sists of two processing steps.
In the first step,a lexical tagger calculates the POS tags basedon lexical information only (word forms).
Theresult of the lexical tagger is used as a firstguess in the second run where both the wordforms and the actual POS tags are applied bythe contextual tagger.
Both lexical and con-textual taggers make use of the TBL concept.During training, TBL performs a greedysearch in a rule space in order to find the rulesthat best improve the correctness of the cur-192rent tagging.
The rule space contains rulesthat change the POS tag of some words ac-cording to their environments.
From theserules, an ordered list is created.
In the tag-ging phase, the rules on the rule list are ap-plied one after another in the same order asthe rule list.
After the last rule is applied, thecurrent tag sequence is returned as a result.For the Hungarian language, Megyesi ap-plied this technique initially with moderatesuccess.
(Megyesi, 1998) The weak part ofher first implementation was the lexical mod-ule of the tagger, as described in (Megyesi,1999).
With the use of extended lexical tem-plates, TBL produced a much better perfor-mance but still lagged behind the statisticaltaggers.We chose a different approach that is simi-lar to (Kuba et al, 2004).
The first guess ofthe TBL tagger is the result of the baselinetagger.
For the second run, the contextualtagger implementation we used is based onthe fnTBL learner module.
(Ngai and Flo-rian, 2001) We used the standard parametersettings included in the fnTBL package.2.1 Baseline TaggerThe baseline tagger relies on an external mor-phological analyzer1 to get the list of possiblePOS tags.
If the word occurs in the trainingdata, the word gets its most frequent POStag in the training.
If the word does not ap-pear in the training, but representatives of itsambiguity class (words with the same possiblePOS tags) are present, then the most frequenttag of all these words will be selected.
Other-wise, the word gets the first tag from the listof possible POS tags.Some results produced by the baseline tag-ger and the improvements achieved by theTBL tagger are given in Table 2.3 Classifier CombinationsThe goal of designing pattern recognition sys-tems is to achieve the best possible classifica-tion performance for the specified task.
Thisobjective traditionally led to the development1We used the Humor (Pro?sze?ky, 1995) analyzer de-veloped by MorphoLogic Ltd.Test Domain Baseline TBLfull corpus 94.94% 96.52%business news 97.56% 98.26%cross-domain 79.51% 95.79%Table 2: Accuracy of the baseline tagger andthe TBL tagger.of different classification schemes for recogni-tion problems the user would like solved.
Ex-periments shows that although one of the de-signs should yield the best performance, thesets of patterns misclassified by the differentclassifiers do not necessarily overlap.
Theseobservations motivated the relatively recentinterest in combining classifiers.
The mainidea behind it is not to rely on the decisionof a single classifier.
Rather, all of the induc-ers or their subsets are employed for decision-making by combining their individual opin-ions to produce a final decision.3.1 BaggingThe Bagging (Bootstrap aggregating) algo-rithm (Breiman, 1996) applies majority vot-ing (Sum Rule) to aggregate the classifiersgenerated by different bootstrap samples.
Abootstrap sample is generated by uniformlysampling m instances from the training setwith replacement.
T bootstrap samplesB1, B2, ..., BT are generated and a classifierCi is built from each bootstrap sample Bi.Bagging algorithmRequire: Training Set S = {x1, .
.
.
,xm}Ensure: Combined classifier C?for i = 1 .
.
.
T doS?
= bootstrap sample from STrain classifer Ci on S?end forC?
(x) = argmaxj?iI[Ci(x) = ?j ]For a given bootstrap sample, an instancein the training set will have a probability1 ?
(1 ?
1/m)m of being selected at leastonce from the m instances that are randomlypicked from the training set.
For large m,193this is about 1-1/e = 63.2%.
This perturba-tion causes different classifiers to be built ifthe inducer is unstable (e.g.
ANNs, decisiontrees) and the performance may improve if theinduced classifiers are uncorrelated.
However,Bagging can slightly degrade the performanceof stable algorithms (e.g.
kNN) since effec-tively smaller training sets are used for train-ing.3.2 BoostingBoosting (Freund and Schapire, 1996) wasintroduced by Shapire as a method for im-proving the performance of a weak learningalgorithm.
AdaBoost changes the weights ofthe training instances provided as input foreach inducer based on classifiers that werepreviously built.
The final decision is madeusing a weighted majority voting schema foreach classifier, whose weights depend on theperformance of the training set used to buildit.Adaboost algorithmRequire: Training Set S = {x1, .
.
.
,xm}Ensure: Combined classifier C?d(1)j = 1/N for all j = 1 .
.
.mfor t = 1 .
.
.
T doTrain classifier Ct with respect to the dis-tribution d(t).Calculate the weighted training error tof Ct: t =m?j=1d(t)j I[Ct(xj) 6= ?j ]if t > 1/2 then ExitSet ?t: ?t =12 log1 ?
ttd(t+1)j = d(t+1)j1Zte?
?tI[Ct(x)6=?j ],where Zt is the normalization constant,such that:m?j=1d(t+1)j = 1end forC?
(x) = argmaxj?t?tI[Ct(x) = ?j ]The boosting algorithm requires a weaklearning algorithm whose error is bounded bya constant strictly less than 1/2.
In the case ofmulti-class classifications this condition mightbe difficult to guarantee, and various tech-niques may need to be applied to get roundthis restriction.There is an important issue that relatesto the construction of weak learners.
Atstep t the weak learner is constructed basedon the weighting dt.
Basically, there aretwo approaches for taking this weighting intoaccount.
In the first approach we assumethat the learning algorithm can operate withreweighted examples.
For instance, when thelearner minimizes a cost function, one canconstruct a revised cost function which as-signs weights to each of the examples.
How-ever, not all learners can be easily adapted tosuch an inclusion of the weights.
The otherapproach is based on resampling the data withreplacement.
This approach is more generalas it is applicable to all kinds of learners.4 Boosted results for TBLTBL belongs to the group of learners that gen-erates abstract information, i.e.
only the classlabel of the source instance.
Although it ispossible to transcribe the output format toconfidence type, this limitation degrades therange of the applicable combination schemes.Min, Max and Prod Rules cannot producea competitive classifier ensemble, while SumRule and Borda Count are equivalent to ma-jority voting.
From the set of available boost-ing algorithms we may only apply those meth-ods that do not require the modification of thelearner.For the experiments we chose Bagging anda modified Adaboost.M1 algorithm as boost-ing.
Since the learner is incapable of han-dling instance weights, individual trainingdatasets were generated by bootstrapping (i.e.resamping with replacement).
The origi-nal Adaboost.M1 algorithm requires that theweighted error should be below 50%.
In thiscase the modified algorithm reinitializes theinstance weights and goes on with the pro-cessing.The Boosting algorithm is based on weight-ing the independent instances based on theclassification error of the previously trained1940 20 40 60 80 10011.21.41.61.82number of classifiersclassificationerror [%]Recognition error on training dataset0 20 40 60 80 10011.21.41.61.82number of classifiersclassificationerror [%]Recognition error on test datasetFigure 1: Classification error of Bagging andBoosting algorithm on the training and train-ing datatsets ( dashed: Bagging, solid: Boost-ing).learners.
TBL operates on words, but wordsare not treated as independent instances.Their context, the position in the sentence,affects the building of the classifier.
Thusinstead of the straightforward selection ofwords, the boosting method handles the sen-tences as instance samples.
The classificationerror of the instances are calculated as thearithmetic mean of the classification errorsof the words in the corresponding sentence.Despite this, the combined final error is ex-pressed as the relative number of the misclas-sified words.The training and testing datasets were cho-sen from the business news domain of theSzeged Corpus.
The train database containedabout 128,000 annotated words in 5700 sen-tences.
The remaining part of the domain,the test database, has 13,300 words in 600sentences.The results of training and testing errorrates are shown in Fig.1.
The classificationerror of the stand-alone TBL algorithm on thetest dataset was 1.74%.
Boosting is capable ofdecreasing it to below 1.31%, which means a24.7% relative error reduction.
As the graphsshow, boosting achieves this during the first20 iterations, so further processing steps can-not make much difference to the classificationaccuracy.
It can also be seen that the train-ing error does not converge to a zero-errorlevel.
This behavior is due to the fact thatthe learner cannot maintain the 50% weightederror limit condition.
Bagging achieved onlya moderate gain in accuracy, its relative errorreduction rate being 18%.5 ConclusionsIn this paper we investigated the possibilityof improving the classification performance ofPOS taggers by applying classifier combina-tion schemas.
For the experiments we choseTBL as a tagger and Bagging and Boostingas combination schemas.
The results indi-cates that Bagging and Boosting can reducethe classification error of the TBL tagger by18% and 24.7%, respectively.It is clear that further improvementscould be made by tailoring the tagger algo-rithm to the requirements of more sophis-ticated boosting methods like Adaboost.M2and other derivatives optimized for multi-class recognition.
Another promising ideafor more effective combinations is that ofapplying confidence-type generative (ANN,kNN, SVM) or discriminative (HMM) learn-ers (Duda et al, 2001; Bishop, 1995; Vap-nik, 1998).
These kinds of classifiers pro-vide more information about the recognitionresults and can improve the cooperation be-tween the combiner and the classifiers.
In thefuture we plan to investigate these alterna-tives for constructing better tagger combina-tions.195ReferencesZolta?n Alexin, Szilvia Zvada, and TiborGyimo?thy.
1999.
Application of AGLEARNon Hungarian Part-of-Speech tagging.
InD.
Parigot and M. Mernik, editors, Sec-ond Workshop on Attribute Grammars andtheir Applications, WAGA?99, pages 133?152, Amsterdam, The Netherlands.
INRIArocquencourt.Zolta?n Alexin, Ja?nos Csirik, Tibor Gyimo?thy,Ka?roly Bibok, Csaba Hatvani, Ga?bor Pro?sze?ki,and La?szlo?
Tihanyi.
2003.
Manually anno-tated Hungarian corpus.
In Proceedings of theResearch Note Sessions of the 10th Confer-ence of the European Chapter of the Associa-tion for Computational Linguistics, EACL?03,pages 53?56, Budapest, Hungary.C.
M. Bishop.
1995.
Neural Networks for PatternRecognition.
Oxford University Press.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.
In Proceedings of the SixthApplied Natural Language Processing, ANLP-2000, Seattle, WA.L.
Breiman.
1996.
Bagging predictors.
MachineLearning, 24, no.
2:123?140.Eric Brill.
1995.
Transformation-based error-driven learning and natural language process-ing: A case study in part-of-speech tagging.Computational Linguistics, 21(4):543?565.Ludmila Dimitrova, Tomaz?
Erjavec, Nancy Ide,Heiki Jaan Kaalep, Vladimir Petkevic, and DanTufis.
1998.
Multext-east: Parallel and com-parable corpora and lexicons for six Centraland Eastern European languages.
In ChristianBoitet and Pete Whitelock, editors, Proceedingsof the Thirty-Sixth Annual Meeting of the Asso-ciation for Computational Linguistics and Sev-enteenth International Conference on Compu-tational Linguistics, pages 315?319, San Fran-cisco, California.
Morgan Kaufmann Publish-ers.R.
O. Duda, P. E. Hart, and D. G. Stork.
2001.Pattern Classification.
John Wiley and Son,New York.Yoav Freund and Robert E. Schapire.
1996.
Ex-periments with a new boosting algorithm.
InMorgan Kaufmann, editor, Proc InternationalConference on Machine Learning, pages 148?156, San Francisco.Andra?s Ho?cza, Zolta?n Alexin, Do?ra Csendes,Ja?nos Csirik, and Tibor Gyimo?thy.
2003.
Ap-plication of ILP methods in different naturallanguage processing phases for information ex-traction from Hungarian texts.
In Proceedingsof the Kalma?r Workshop on Logic and Com-puter Science, pages 107?116, Szeged, Hungary.Tama?s Horva?th, Zolta?n Alexin, Tibor Gyimo?thy,and Stefan Wrobel.
1999.
Application of dif-ferent learning methods to Hungarian Part-of-Speech tagging.
In S. Dz?eroski and P. Flach,editors, Proceedings of ILP99, volume 1634 ofLNAI, pages 128?139.
Springer Verlag.Andra?s Kuba, Andra?s Ho?cza, and Ja?nos Csirik.2004.
POS tagging of Hungarian with com-bined statistical and rule-based methods.
InPetr Sojka, Ivan Kopecek, and Karel Pala, ed-itors, Text, Speech and Dialogue, Proceedingsof the 7th International Conference, TSD 2004,pages 113?121, Brno, Czech Republic.Bea?ta Megyesi.
1998.
Brill?s rule-based POS tag-ger for Hungarian.
Master?s thesis, Departmentof Linguistics, Stockholm University, Sweden.Bea?ta Megyesi.
1999.
Improving Brill?s POStagger for an agglutinative language.
In Pro-ceedings of the Joint Sigdat Conference on Em-pirical Methods in Natural Language Process-ing and Very Large Corpora, EMNLP/VLC ?99,pages 275?284.Grace Ngai and Radu Florian.
2001.Transformation-based learning in the fastlane.
In Proceedings of North American ACL2001, pages 40?47, June.Csaba Oravecz and Pe?ter Dienes.
2002.
Efficientstochastic Part-of-Speech tagging for Hungar-ian.
In Proceedings of the Third InternationalConference on Language Resources and Evalu-ation, LREC2002, pages 710?717, Las Palmas.Ga?bor Pro?sze?ky.
1995.
Humor: a morphologicalsystem for corpus analysis.
In H. Retting, edi-tor, Language Resources for Language Technol-ogy, Proceedings of the First European TELRISeminar, pages 149?158, Tihany, Hungary.Dan Tufis, Pe?ter Dienes, Csaba Oravecz, andTama?s Va?radi.
2000.
Principled hidden tagsetdesign for tiered tagging of Hungarian.V.
N. Vapnik.
1998.
Statistical Learning Theory.John Wiley and Son.Tama?s Va?radi.
2002.
The Hungarian NationalCorpus.
In Proceedings of the Third Interna-tional Conference on Language Resources andEvaluation, LREC2002, pages 385?396, LasPalmas de Gran Canaria.196
