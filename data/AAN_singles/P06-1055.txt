Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 433?440,Sydney, July 2006. c?2006 Association for Computational LinguisticsLearning Accurate, Compact, and Interpretable Tree AnnotationSlav Petrov Leon Barrett Romain Thibaux Dan KleinComputer Science Division, EECS DepartmentUniversity of California at BerkeleyBerkeley, CA 94720{petrov, lbarrett, thibaux, klein}@eecs.berkeley.eduAbstractWe present an automatic approach to tree annota-tion in which basic nonterminal symbols are alter-nately split and merged to maximize the likelihoodof a training treebank.
Starting with a simple X-bar grammar, we learn a new grammar whose non-terminals are subsymbols of the original nontermi-nals.
In contrast with previous work, we are ableto split various terminals to different degrees, as ap-propriate to the actual complexity in the data.
Ourgrammars automatically learn the kinds of linguisticdistinctions exhibited in previous work on manualtree annotation.
On the other hand, our grammarsare much more compact and substantially more ac-curate than previous work on automatic annotation.Despite its simplicity, our best grammar achievesan F1 of 90.2% on the Penn Treebank, higher thanfully lexicalized systems.1 IntroductionProbabilistic context-free grammars (PCFGs) underliemost high-performance parsers in one way or another(Collins, 1999; Charniak, 2000; Charniak and Johnson,2005).
However, as demonstrated in Charniak (1996)and Klein and Manning (2003), a PCFG which sim-ply takes the empirical rules and probabilities off of atreebank does not perform well.
This naive grammaris a poor one because its context-freedom assumptionsare too strong in some places (e.g.
it assumes that sub-ject and object NPs share the same distribution) and tooweak in others (e.g.
it assumes that long rewrites arenot decomposable into smaller steps).
Therefore, a va-riety of techniques have been developed to both enrichand generalize the naive grammar, ranging from simpletree annotation and symbol splitting (Johnson, 1998;Klein and Manning, 2003) to full lexicalization and in-tricate smoothing (Collins, 1999; Charniak, 2000).In this paper, we investigate the learning of a gram-mar consistent with a treebank at the level of evalua-tion symbols (such as NP, VP, etc.)
but split based onthe likelihood of the training trees.
Klein and Manning(2003) addressed this question from a linguistic per-spective, starting with a Markov grammar and manu-ally splitting symbols in response to observed linguistictrends in the data.
For example, the symbol NP mightbe split into the subsymbol NP?S in subject positionand the subsymbol NP?VP in object position.
Recently,Matsuzaki et al (2005) and also Prescher (2005) ex-hibited an automatic approach in which each symbol issplit into a fixed number of subsymbols.
For example,NP would be split into NP-1 through NP-8.
Their ex-citing result was that, while grammars quickly grew toolarge to be managed, a 16-subsymbol induced grammarreached the parsing performance of Klein and Manning(2003)?s manual grammar.
Other work has also investi-gated aspects of automatic grammar refinement; for ex-ample, Chiang and Bikel (2002) learn annotations suchas head rules in a constrained declarative language fortree-adjoining grammars.We present a method that combines the strengths ofboth manual and automatic approaches while address-ing some of their common shortcomings.
Like Mat-suzaki et al (2005) and Prescher (2005), we inducesplits in a fully automatic fashion.
However, we use amore sophisticated split-and-merge approach that allo-cates subsymbols adaptively where they are most effec-tive, like a linguist would.
The grammars recover pat-terns like those discussed in Klein and Manning (2003),heavily articulating complex and frequent categorieslike NP and VP while barely splitting rare or simpleones (see Section 3 for an empirical analysis).Empirically, hierarchical splitting increases the ac-curacy and lowers the variance of the learned gram-mars.
Another contribution is that, unlike previouswork, we investigate smoothed models, allowing us tosplit grammars more heavily before running into theoversplitting effect discussed in Klein and Manning(2003), where data fragmentation outweighs increasedexpressivity.Our method is capable of learning grammars of sub-stantially smaller size and higher accuracy than previ-ous grammar refinement work, starting from a simplerinitial grammar.
For example, even beginning with anX-bar grammar (see Section 1.1) with 98 symbols, ourbest grammar, using 1043 symbols, achieves a test setF1 of 90.2%.
This is a 27% reduction in error and a sig-nificant reduction in size1 over the most accurate gram-1This is a 97.5% reduction in number of symbols.
Mat-suzaki et al (2005) do not report a number of rules, but oursmall number of symbols and our hierarchical training (which433(a) FRAGRBNotNPDTthisNNyear..(b) ROOTFRAGFRAGRBNotNPDTthisNNyear..Figure 1: (a) The original tree.
(b) The X-bar tree.mar in Matsuzaki et al (2005).
Our grammar?s accu-racy was higher than fully lexicalized systems, includ-ing the maximum-entropy inspired parser of Charniakand Johnson (2005).1.1 Experimental SetupWe ran our experiments on the Wall Street Journal(WSJ) portion of the Penn Treebank using the stan-dard setup: we trained on sections 2 to 21, and weused section 1 as a validation set for tuning model hy-perparameters.
Section 22 was used as developmentset for intermediate results.
All of section 23 was re-served for the final test.
We used the EVALB parsevalreference implementation, available from Sekine andCollins (1997), for scoring.
All reported developmentset results are averages over four runs.
For the final testwe selected the grammar that performed best on the de-velopment set.Our experiments are based on a completely unanno-tated X-bar style grammar, obtained directly from thePenn Treebank by the binarization procedure shown inFigure 1.
For each local tree rooted at an evaluationnonterminal X , we introduce a cascade of new nodeslabeled X so that each has two children.
Rather thanexperiment with head-outward binarization as in Kleinand Manning (2003), we simply used a left branchingbinarization; Matsuzaki et al (2005) contains a com-parison showing that the differences between binariza-tions are small.2 LearningTo obtain a grammar from the training trees, we wantto learn a set of rule probabilities ?
on latent annota-tions that maximize the likelihood of the training trees,despite the fact that the original trees lack the latentannotations.
The Expectation-Maximization (EM) al-gorithm allows us to do exactly that.2 Given a sen-tence w and its unannotated tree T , consider a non-terminal A spanning (r, t) and its children B and Cspanning (r, s) and (s, t).
Let Ax be a subsymbolof A, By of B, and Cz of C. Then the inside andoutside probabilities PIN(r, t, Ax) def= P (wr:t|Ax) andPOUT(r, t, Ax) def= P (w1:rAxwt:n) can be computed re-encourages sparsity) suggest a large reduction.2Other techniques are also possible; Henderson (2004)uses neural networks to induce latent left-corner parser states.cursively:PIN(r, t, Ax) =?y,z?
(Ax ?
ByCz)?PIN(r, s, By)PIN(s, t, Cz)POUT(r, s, By) =?x,z?
(Ax ?
ByCz)?POUT(r, t, Ax)PIN(s, t, Cz)POUT(s, t, Cz) =?x,y?
(Ax ?
ByCz)?POUT(r, t, Ax)PIN(r, s, By)Although we show only the binary component here, ofcourse there are both binary and unary productions thatare included.
In the Expectation step, one computesthe posterior probability of each annotated rule and po-sition in each training set tree T :P ((r, s, t, Ax ?
ByCz)|w, T ) ?
POUT(r, t, Ax)??
(Ax ?
ByCz)PIN(r, s, By)PIN(s, t, Cz) (1)In the Maximization step, one uses the above probabil-ities as weighted observations to update the rule proba-bilities:?
(Ax ?
ByCz) :=#{Ax ?
ByCz}?y?,z?
#{Ax ?
By?Cz?
}Note that, because there is no uncertainty about the lo-cation of the brackets, this formulation of the inside-outside algorithm is linear in the length of the sentencerather than cubic (Pereira and Schabes, 1992).For our lexicon, we used a simple yet robust methodfor dealing with unknown and rare words by extract-ing a small number of features from the word and thencomputing appproximate tagging probabilities.32.1 InitializationEM is only guaranteed to find a local maximum of thelikelihood, and, indeed, in practice it often gets stuck ina suboptimal configuration.
If the search space is verylarge, even restarting may not be sufficient to alleviatethis problem.
One workaround is to manually specifysome of the annotations.
For instance, Matsuzaki et al(2005) start by annotating their grammar with the iden-tity of the parent and sibling, which are observed (i.e.not latent), before adding latent annotations.4 If thesemanual annotations are good, they reduce the searchspace for EM by constraining it to a smaller region.
Onthe other hand, this pre-splitting defeats some of thepurpose of automatically learning latent annotations,3A word is classified into one of 50 unknown word cate-gories based on the presence of features such as capital let-ters, digits, and certain suffixes and its tagging probability isgiven by: P?
(word|tag) = k P?
(class|tag) where k is a con-stant representing P (word|class) and can simply be dropped.Rare words are modeled using a combination of their knownand unknown distributions.4In other words, in the terminology of Klein and Man-ning (2003), they begin with a (vertical order=2, horizontalorder=1) baseline grammar.434DTthe (0.50) a (0.24) The (0.08)that (0.15) this (0.14) some (0.11)this (0.39)that (0.28)That (0.11)this (0.52)that (0.36)another (0.04)That (0.38)This (0.34)each (0.07)some (0.20)all (0.19)those (0.12)some (0.37)all (0.29)those (0.14)these (0.27)both (0.21)Some (0.15)the (0.54) a (0.25) The (0.09)the (0.80)The (0.15)a (0.01)the (0.96)a (0.01)The (0.01)The (0.93)A(0.02)No(0.01)a (0.61)the (0.19)an (0.10)a (0.75)an (0.12)the (0.03)Figure 2: Evolution of the DT tag during hierarchical splitting and merging.
Shown are the top three words foreach subcategory and their respective probability.leaving to the user the task of guessing what a goodstarting annotation might be.We take a different, fully automated approach.
Westart with a completely unannotated X-bar style gram-mar as described in Section 1.1.
Since we will evaluateour grammar on its ability to recover the Penn Treebanknonterminals, we must include them in our grammar.Therefore, this initialization is the absolute minimumstarting grammar that includes the evaluation nontermi-nals (and maintains separate grammar symbols for eachof them).5 It is a very compact grammar: 98 symbols,6236 unary rules, and 3840 binary rules.
However, italso has a very low parsing performance: 65.8/59.8LP/LR on the development set.2.2 SplittingBeginning with this baseline grammar, we repeatedlysplit and re-train the grammar.
In each iteration weinitialize EM with the results of the smaller gram-mar, splitting every previous annotation symbol in twoand adding a small amount of randomness (1%) tobreak the symmetry.
The results are shown in Fig-ure 3.
Hierarchical splitting leads to better parame-ter estimates over directly estimating a grammar with2k subsymbols per symbol.
While the two proceduresare identical for only two subsymbols (F1: 76.1%),the hierarchical training performs better for four sub-symbols (83.7% vs. 83.2%).
This advantage growsas the number of subsymbols increases (88.4% vs.87.3% for 16 subsymbols).
This trend is to be ex-pected, as the possible interactions between the sub-symbols grows as their number grows.
As an exam-ple of how staged training proceeds, Figure 2 showsthe evolution of the subsymbols of the determiner (DT)tag, which first splits demonstratives from determiners,then splits quantificational elements from demonstra-tives along one branch and definites from indefinitesalong the other.5If our purpose was only to model language, as measuredfor instance by perplexity on new text, it could make senseto erase even the labels of the Penn Treebank to let EM findbetter labels by itself, giving an experiment similar to that ofPereira and Schabes (1992).645 part of speech tags, 27 phrasal categories and the 26intermediate symbols which were added during binarizationBecause EM is a local search method, it is likely toconverge to different local maxima for different runs.In our case, the variance is higher for models with fewsubcategories; because not all dependencies can be ex-pressed with the limited number of subcategories, theresults vary depending on which one EM selects first.As the grammar size increases, the important depen-dencies can be modeled, so the variance decreases.2.3 MergingIt is clear from all previous work that creating more la-tent annotations can increase accuracy.
On the otherhand, oversplitting the grammar can be a serious prob-lem, as detailed in Klein and Manning (2003).
Addingsubsymbols divides grammar statistics into many bins,resulting in a tighter fit to the training data.
At the sametime, each bin gives a less robust estimate of the gram-mar probabilities, leading to overfitting.
Therefore, itwould be to our advantage to split the latent annota-tions only where needed, rather than splitting them allas in Matsuzaki et al (2005).
In addition, if all sym-bols are split equally often, one quickly (4 split cycles)reaches the limits of what is computationally feasiblein terms of training time and memory usage.Consider the comma POS tag.
We would like to seeonly one sort of this tag because, despite its frequency,it always produces the terminal comma (barring a fewannotation errors in the treebank).
On the other hand,we would expect to find an advantage in distinguishingbetween various verbal categories and NP types.
Addi-tionally, splitting symbols like the comma is not onlyunnecessary, but potentially harmful, since it need-lessly fragments observations of other symbols?
behav-ior.It should be noted that simple frequency statistics arenot sufficient for determining how often to split eachsymbol.
Consider the closed part-of-speech classes(e.g.
DT, CC, IN) or the nonterminal ADJP.
Thesesymbols are very common, and certainly do containsubcategories, but there is little to be gained fromexhaustively splitting them before even beginning tomodel the rarer symbols that describe the complex in-ner correlations inside verb phrases.
Our solution isto use a split-and-merge approach broadly reminiscentof ISODATA, a classic clustering procedure (Ball and435Hall, 1967).To prevent oversplitting, we could measure the util-ity of splitting each latent annotation individually andthen split the best ones first.
However, not only is thisimpractical, requiring an entire training phase for eachnew split, but it assumes the contributions of multiplesplits are independent.
In fact, extra subsymbols mayneed to be added to several nonterminals before theycan cooperate to pass information along the parse tree.Therefore, we go in the opposite direction; that is, wesplit every symbol in two, train, and then measure foreach annotation the loss in likelihood incurred whenremoving it.
If this loss is small, the new annotationdoes not carry enough useful information and can beremoved.
What is more, contrary to the gain in like-lihood for splitting, the loss in likelihood for mergingcan be efficiently approximated.7Let T be a training tree generating a sentence w.Consider a node n of T spanning (r, t) with the labelA; that is, the subtree rooted at n generates wr:t andhas the label A.
In the latent model, its label A is splitup into several latent labels, Ax.
The likelihood of thedata can be recovered from the inside and outside prob-abilities at n:P(w, T ) =?xPIN(r, t, Ax)POUT(r, t, Ax) (2)Consider merging, at n only, two annotations A1 andA2.
Since A now combines the statistics of A1 and A2,its production probabilities are the sum of those of A1and A2, weighted by their relative frequency p1 and p2in the training data.
Therefore the inside score of A is:PIN(r, t, A) = p1PIN(r, t, A1) + p2PIN(r, t, A2)Since A can be produced as A1 or A2 by its parents, itsoutside score is:POUT(r, t, A) = POUT(r, t, A1) + POUT(r, t, A2)Replacing these quantities in (2) gives us the likelihoodPn(w, T ) where these two annotations and their corre-sponding rules have been merged, around only node n.We approximate the overall loss in data likelihooddue to merging A1 and A2 everywhere in all sentenceswi by the product of this loss for each local change:?ANNOTATION (A1, A2) =?i?n?TiPn(wi, Ti)P(wi, Ti)This expression is an approximation because it neglectsinteractions between instances of a symbol at multipleplaces in the same tree.
These instances, however, are7The idea of merging complex hypotheses to encouragegeneralization is also examined in Stolcke and Omohundro(1994), who used a chunking approach to propose new pro-ductions in fully unsupervised grammar induction.
They alsofound it necessary to make local choices to guide their likeli-hood search.often far apart and are likely to interact only weakly,and this simplification avoids the prohibitive cost ofrunning an inference algorithm for each tree and an-notation.
We refer to the operation of splitting anno-tations and re-merging some them based on likelihoodloss as a split-merge (SM) cycle.
SM cycles allow us toprogressively increase the complexity of our grammar,giving priority to the most useful extensions.In our experiments, merging was quite valuable.
De-pending on how many splits were reversed, we couldreduce the grammar size at the cost of little or no lossof performance, or even a gain.
We found that merging50% of the newly split symbols dramatically reducedthe grammar size after each splitting round, so that af-ter 6 SM cycles, the grammar was only 17% of the sizeit would otherwise have been (1043 vs. 6273 subcat-egories), while at the same time there was no loss inaccuracy (Figure 3).
Actually, the accuracy even in-creases, by 1.1% at 5 SM cycles.
The numbers of splitslearned turned out to not be a direct function of symbolfrequency; the numbers of symbols for both lexical andnonlexical tags after 4 SM cycles are given in Table 2.Furthermore, merging makes large amounts of splittingpossible.
It allows us to go from 4 splits, equivalent tothe 24 = 16 substates of Matsuzaki et al (2005), to 6SM iterations, which take a few days to run on the PennTreebank.2.4 SmoothingSplitting nonterminals leads to a better fit to the data byallowing each annotation to specialize in representingonly a fraction of the data.
The smaller this fraction,the higher the risk of overfitting.
Merging, by allow-ing only the most beneficial annotations, helps mitigatethis risk, but it is not the only way.
We can furtherminimize overfitting by forcing the production proba-bilities from annotations of the same nonterminal to besimilar.
For example, a noun phrase in subject positioncertainly has a distinct distribution, but it may benefitfrom being smoothed with counts from all other nounphrases.
Smoothing the productions of each subsym-bol by shrinking them towards their common base sym-bol gives us a more reliable estimate, allowing them toshare statistical strength.We perform smoothing in a linear way.
The es-timated probability of a production px = P(Ax ?By Cz) is interpolated with the average over all sub-symbols of A.p?x = (1 ?
?
)px + ?p?
where p?
=1n?xpxHere, ?
is a small constant: we found 0.01 to be a goodvalue, but the actual quantity was surprisingly unimpor-tant.
Because smoothing is most necessary when pro-duction statistics are least reliable, we expect smooth-ing to help more with larger numbers of subsymbols.This is exactly what we observe in Figure 3, wheresmoothing initially hurts (subsymbols are quite distinct436and do not need their estimates pooled) but eventuallyhelps (as symbols have finer distinctions in behaviorand smaller data support).2.5 ParsingWhen parsing new sentences with an annotated gram-mar, returning the most likely (unannotated) tree is in-tractable: to obtain the probability of an unannotatedtree, one must sum over combinatorially many annota-tion trees (derivations) for each tree (Sima?an, 1992).Matsuzaki et al (2005) discuss two approximations.The first is settling for the most probable derivationrather than most probable parse, i.e.
returning the singlemost likely (Viterbi) annotated tree (derivation).
Thisapproximation is justified if the sum is dominated byone particular annotated tree.
The second approxima-tion that Matsuzaki et al (2005) present is the Viterbiparse under a new sentence-specific PCFG, whose ruleprobabilities are given as the solution of a variationalapproximation of the original grammar.
However, theirrule probabilities turn out to be the posterior probabil-ity, given the sentence, of each rule being used at eachposition in the tree.
Their algorithm is therefore the la-belled recall algorithm of Goodman (1996) but appliedto rules.
That is, it returns the tree whose expectednumber of correct rules is maximal.
Thus, assumingone is interested in a per-position score like F1 (whichis its own debate), this method of parsing is actuallymore appropriate than finding the most likely parse,not simply a cheap approximation of it, and it need notbe derived by a variational argument.
We refer to thismethod of parsing as the max-rule parser.
Since thismethod is not a contribution of this paper, we refer thereader to the fuller presentations in Goodman (1996)and Matsuzaki et al (2005).
Note that contrary to theoriginal labelled recall algorithm, which maximizes thenumber of correct symbols, this tree only contains rulesallowed by the grammar.
As a result, the percentage ofcomplete matches with the max-rule parser is typicallyhigher than with the Viterbi parser.
(37.5% vs. 35.8%for our best grammar).These posterior rule probabilities are still given by(1), but, since the structure of the tree is no longerknown, we must sum over it when computing the in-side and outside probabilities:PIN(r, t, Ax)=?B,C,s?y,z?
(Ax ?
ByCz)?PIN(r, s, By)PIN(s, t, Cz)POUT(r, s, By)=?A,C,t?x,z?
(Ax ?
ByCz)?POUT(r, t, Ax)PIN(s, t, Cz)POUT(s, t, Cz)=?A,B,r?x,y?
(Ax ?
ByCz)?POUT(r, t, Ax)PIN(r, s, By)For efficiency reasons, we use a coarse-to-fine prun-ing scheme like that of Caraballo and Charniak (1998).For a given sentence, we first run the inside-outsidealgorithm using the baseline (unannotated) grammar,747678808284868890200  400  600  800  1000F1Total number of grammar symbols50% Merging and Smoothing50% MergingSplitting but no MergingFlat TrainingFigure 3: Hierarchical training leads to better parame-ter estimates.
Merging reduces the grammar size sig-nificantly, while preserving the accuracy and enablingus to do more SM cycles.
Parameter smoothing leadsto even better accuracy for grammars with high com-plexity.producing a packed forest representation of the poste-rior symbol probabilities for each span.
For example,one span might have a posterior probability of 0.8 ofthe symbol NP, but e?10 for PP.
Then, we parse with thelarger annotated grammar, but, at each span, we pruneaway any symbols whose posterior probability underthe baseline grammar falls below a certain threshold(e?8 in our experiments).
Even though our baselinegrammar has a very low accuracy, we found that thispruning barely impacts the performance of our bettergrammars, while significantly reducing the computa-tional cost.
For a grammar with 479 subcategories (4SM cycles), lowering the threshold to e?15 led to an F1improvement of 0.13% (89.03 vs. 89.16) on the devel-opment set but increased the parsing time by a factor of16.3 AnalysisSo far, we have presented a split-merge method forlearning to iteratively subcategorize basic symbolslike NP and VP into automatically induced subsym-bols (subcategories in the original sense of Chomsky(1965)).
This approach gives parsing accuracies of upto 90.7% on the development set, substantially higherthan previous symbol-splitting approaches, while start-ing from an extremely simple base grammar.
However,in general, any automatic induction system is in dan-ger of being entirely uninterpretable.
In this section,we examine the learned grammars, discussing what islearned.
We focus particularly on connections with thelinguistically motivated annotations of Klein and Man-ning (2003), which we do generally recover.Inspecting a large grammar by hand is difficult, butfortunately, our baseline grammar has less than 100nonterminal symbols, and even our most complicatedgrammar has only 1043 total (sub)symbols.
It is there-437VBZVBZ-0 gives sells takesVBZ-1 comes goes worksVBZ-2 includes owns isVBZ-3 puts provides takesVBZ-4 says adds SaysVBZ-5 believes means thinksVBZ-6 expects makes callsVBZ-7 plans expects wantsVBZ-8 is ?s getsVBZ-9 ?s is remainsVBZ-10 has ?s isVBZ-11 does Is DoesNNPNNP-0 Jr. Goldman INC.NNP-1 Bush Noriega PetersNNP-2 J. E. L.NNP-3 York Francisco StreetNNP-4 Inc Exchange CoNNP-5 Inc. Corp. Co.NNP-6 Stock Exchange YorkNNP-7 Corp. Inc. GroupNNP-8 Congress Japan IBMNNP-9 Friday September AugustNNP-10 Shearson D. FordNNP-11 U.S. Treasury SenateNNP-12 John Robert JamesNNP-13 Mr. Ms. PresidentNNP-14 Oct. Nov. Sept.NNP-15 New San WallJJSJJS-0 largest latest biggestJJS-1 least best worstJJS-2 most Most leastDTDT-0 the The aDT-1 A An AnotherDT-2 The No ThisDT-3 The Some TheseDT-4 all those someDT-5 some these bothDT-6 That This eachDT-7 this that eachDT-8 the The aDT-9 no any someDT-10 an a theDT-11 a this theCDCD-0 1 50 100CD-1 8.50 15 1.2CD-2 8 10 20CD-3 1 30 31CD-4 1989 1990 1988CD-5 1988 1987 1990CD-6 two three fiveCD-7 one One ThreeCD-8 12 34 14CD-9 78 58 34CD-10 one two threeCD-11 million billion trillionPRPPRP-0 It He IPRP-1 it he theyPRP-2 it them himRBRRBR-0 further lower higherRBR-1 more less MoreRBR-2 earlier Earlier laterININ-0 In With AfterIN-1 In For AtIN-2 in for onIN-3 of for onIN-4 from on withIN-5 at for byIN-6 by in withIN-7 for with onIN-8 If While AsIN-9 because if whileIN-10 whether if ThatIN-11 that like whetherIN-12 about over betweenIN-13 as de UpIN-14 than ago untilIN-15 out up downRBRB-0 recently previously stillRB-1 here back nowRB-2 very highly relativelyRB-3 so too asRB-4 also now stillRB-5 however Now HoweverRB-6 much far enoughRB-7 even well thenRB-8 as about nearlyRB-9 only just almostRB-10 ago earlier laterRB-11 rather instead becauseRB-12 back close aheadRB-13 up down offRB-14 not Not maybeRB-15 n?t not alsoTable 1: The most frequent three words in the subcategories of several part-of-speech tags.fore relatively straightforward to review the broad be-havior of a grammar.
In this section, we review arandomly-selected grammar after 4 SM cycles that pro-duced an F1 score on the development set of 89.11.
Wefeel it is reasonable to present only a single grammarbecause all the grammars are very similar.
For exam-ple, after 4 SM cycles, the F1 scores of the 4 trainedgrammars have a variance of only 0.024, which is tinycompared to the deviation of 0.43 obtained by Mat-suzaki et al (2005)).
Furthermore, these grammarsallocate splits to nonterminals with a variance of only0.32, so they agree to within a single latent state.3.1 Lexical SplitsOne of the original motivations for lexicalization ofparsers is the fact that part-of-speech (POS) tags areusually far too general to encapsulate a word?s syntac-tic behavior.
In the limit, each word may well haveits own unique syntactic behavior, especially when, asin modern parsers, semantic selectional preferences arelumped in with traditional syntactic trends.
However,in practice, and given limited data, the relationship be-tween specific words and their syntactic contexts maybe best modeled at a level more fine than POS tag butless fine than lexical identity.In our model, POS tags are split just like any othergrammar symbol: the subsymbols for several tags areshown in Table 1, along with their most frequent mem-bers.
In most cases, the categories are recognizable aseither classic subcategories or an interpretable divisionof some other kind.Nominal categories are the most heavily split (seeTable 2), and have the splits which are most semanticin nature (though not without syntactic correlations).For example, plural common nouns (NNS) divide intothe maximum number of categories (16).
One cate-gory consists primarily of dates, whose typical parentis an NP subsymbol whose typical parent is a root S,essentially modeling the temporal noun annotation dis-cussed in Klein and Manning (2003).
Another cate-gory specializes in capitalized words, preferring as aparent an NP with an S parent (i.e.
subject position).A third category specializes in monetary units, andso on.
These kinds of syntactico-semantic categoriesare typical, and, given distributional clustering resultslike those of Schuetze (1998), unsurprising.
The sin-gular nouns are broadly similar, if slightly more ho-mogenous, being dominated by categories for stocksand trading.
The proper noun category (NNP, shown)also splits into the maximum 16 categories, includingmonths, countries, variants of Co. and Inc., first names,last names, initials, and so on.Verbal categories are also heavily split.
Verbal sub-categories sometimes reflect syntactic selectional pref-erences, sometimes reflect semantic selectional prefer-ences, and sometimes reflect other aspects of verbalsyntax.
For example, the present tense third personverb subsymbols (VBZ) are shown.
The auxiliaries getthree clear categories: do, have, and be (this patternrepeats in other tenses), as well a fourth category forthe ambiguous ?s.
Verbs of communication (says) and438NNP 62 CC 7 WP$ 2 NP 37 CONJP 2JJ 58 JJR 5 WDT 2 VP 32 FRAG 2NNS 57 JJS 5 -RRB- 2 PP 28 NAC 2NN 56 : 5 ?
1 ADVP 22 UCP 2VBN 49 PRP 4 FW 1 S 21 WHADVP 2RB 47 PRP$ 4 RBS 1 ADJP 19 INTJ 1VBG 40 MD 3 TO 1 SBAR 15 SBARQ 1VB 37 RBR 3 $ 1 QP 9 RRC 1VBD 36 WP 2 UH 1 WHNP 5 WHADJP 1CD 32 POS 2 , 1 PRN 4 X 1IN 27 PDT 2 ?
1 NX 4 ROOT 1VBZ 25 WRB 2 SYM 1 SINV 3 LST 1VBP 19 -LRB- 2 RP 1 PRT 2DT 17 .
2 LS 1 WHPP 2NNPS 11 EX 2 # 1 SQ 2Table 2: Number of latent annotations determined byour split-merge procedure after 6 SM cyclespropositional attitudes (beleives) that tend to take in-flected sentential complements dominate two classes,while control verbs (wants) fill out another.As an example of a less-split category, the superla-tive adjectives (JJS) are split into three categories,corresponding principally to most, least, and largest,with most frequent parents NP, QP, and ADVP, respec-tively.
The relative adjectives (JJR) are split in the sameway.
Relative adverbs (RBR) are split into a differentthree categories, corresponding to (usually metaphor-ical) distance (further), degree (more), and time (ear-lier).
Personal pronouns (PRP) are well-divided intothree categories, roughly: nominative case, accusativecase, and sentence-initial nominative case, which eachcorrelate very strongly with syntactic position.
As an-other example of a specific trend which was mentionedby Klein and Manning (2003), adverbs (RB) do containsplits for adverbs under ADVPs (also), NPs (only), andVPs (not).Functional categories generally show fewer splits,but those splits that they do exhibit are known to bestrongly correlated with syntactic behavior.
For exam-ple, determiners (DT) divide along several axes: defi-nite (the), indefinite (a), demonstrative (this), quantifi-cational (some), negative polarity (no, any), and var-ious upper- and lower-case distinctions inside thesetypes.
Here, it is interesting to note that these distinc-tions emerge in a predictable order (see Figure 2 for DTsplits), beginning with the distinction between demon-stratives and non-demonstratives, with the other dis-tinctions emerging subsequently; this echoes the resultof Klein and Manning (2003), where the authors choseto distinguish the demonstrative constrast, but not theadditional ones learned here.Another very important distinction, as shown inKlein and Manning (2003), is the various subdivi-sions in the preposition class (IN).
Learned first isthe split between subordinating conjunctions like thatand proper prepositions.
Then, subdivisions of eachemerge: wh-subordinators like if, noun-modifyingprepositions like of, predominantly verb-modifyingones like from, and so on.Many other interesting patterns emerge, includingADVPADVP-0 RB-13 NP-2 RB-13 PP-3 IN-15 NP-2ADVP-1 NP-3 RB-10 NP-3 RBR-2 NP-3 IN-14ADVP-2 IN-5 JJS-1 RB-8 RB-6 RB-6 RBR-1ADVP-3 RBR-0 RB-12 PP-0 RP-0ADVP-4 RB-3 RB-6 ADVP-2 SBAR-8 ADVP-2 PP-5ADVP-5 RB-5 NP-3 RB-10 RB-0ADVP-6 RB-4 RB-0 RB-3 RB-6ADVP-7 RB-7 IN-5 JJS-1 RB-6ADVP-8 RB-0 RBS-0 RBR-1 IN-14ADVP-9 RB-1 IN-15 RBR-0SINVSINV-0 VP-14 NP-7 VP-14 VP-15 NP-7 NP-9VP-14 NP-7 .-0SINV-1 S-6 ,-0 VP-14 NP-7 .-0S-11 VP-14 NP-7 .-0Table 3: The most frequent three productions of somelatent annotations.many classical distinctions not specifically mentionedor modeled in previous work.
For example, the wh-determiners (WDT) split into one class for that and an-other for which, while the wh-adverbs align by refer-ence type: event-based how and why vs. entity-basedwhen and where.
The possesive particle (POS) has oneclass for the standard ?s, but another for the plural-onlyapostrophe.
As a final example, the cardinal numbernonterminal (CD) induces various categories for dates,fractions, spelled-out numbers, large (usually financial)digit sequences, and others.3.2 Phrasal SplitsAnalyzing the splits of phrasal nonterminals is moredifficult than for lexical categories, and we can merelygive illustrations.
We show some of the top productionsof two categories in Table 3.A nonterminal split can be used to model an other-wise uncaptured correlation between that symbol?s ex-ternal context (e.g.
its parent symbol) and its internalcontext (e.g.
its child symbols).
A particularly clean ex-ample of a split correlating external with internal con-texts is the inverted sentence category (SINV), whichhas only two subsymbols, one which usually has theROOT symbol as its parent (and which has sentence fi-nal puncutation as its last child), and a second subsym-bol which occurs in embedded contexts (and does notend in punctuation).
Such patterns are common, but of-ten less easy to predict.
For example, possesive NPs gettwo subsymbols, depending on whether their possessoris a person / country or an organization.
The externalcorrelation turns out to be that people and countries aremore likely to possess a subject NP, while organizationsare more likely to possess an object NP.Nonterminal splits can also be used to relay infor-mation between distant tree nodes, though untanglingthis kind of propagation and distilling it into clean ex-amples is not trivial.
As one example, the subsym-bol S-12 (matrix clauses) occurs only under the ROOTsymbol.
S-12?s children usually include NP-8, whichin turn usually includes PRP-0, the capitalized nomi-native pronouns, DT-{1,2,6} (the capitalized determin-439ers), and so on.
This same propagation occurs evenmore frequently in the intermediate symbols, with, forexample, one subsymbol of NP symbol specializing inpropagating proper noun sequences.Verb phrases, unsurprisingly, also receive a full setof subsymbols, including categories for infinitive VPs,passive VPs, several for intransitive VPs, several fortransitive VPs with NP and PP objects, and one forsentential complements.
As an example of how lexi-cal splits can interact with phrasal splits, the two mostfrequent rewrites involving intransitive past tense verbs(VBD) involve two different VPs and VBDs: VP-14 ?VBD-13 and VP-15 ?
VBD-12.
The difference is thatVP-14s are main clause VPs, while VP-15s are sub-ordinate clause VPs.
Correspondingly, VBD-13s areverbs of communication (said, reported), while VBD-12s are an assortment of verbs which often appear insubordinate contexts (did, began).Other interesting phenomena also emerge.
For ex-ample, intermediate symbols, which in previous workwere very heavily, manually split using a Markov pro-cess, end up encoding processes which are largelyMarkov, but more complex.
For example, some classesof adverb phrases (those with RB-4 as their head) are?forgotten?
by the VP intermediate grammar.
The rele-vant rule is the very probable VP-2 ?
VP-2 ADVP-6;adding this ADVP to a growing VP does not change theVP subsymbol.
In essense, at least a partial distinctionbetween verbal arguments and verbal adjucts has beenlearned (as exploited in Collins (1999), for example).4 ConclusionsBy using a split-and-merge strategy and beginning withthe barest possible initial structure, our method reli-ably learns a PCFG that is remarkably good at pars-ing.
Hierarchical split/merge training enables us tolearn compact but accurate grammars, ranging from ex-tremely compact (an F1 of 78% with only 147 sym-bols) to extremely accurate (an F1 of 90.2% for ourlargest grammar with only 1043 symbols).
Splittingprovides a tight fit to the training data, while mergingimproves generalization and controls grammar size.
Inorder to overcome data fragmentation and overfitting,we smooth our parameters.
Smoothing allows us toadd a larger number of annotations, each specializingin only a fraction of the data, without overfitting ourtraining set.
As one can see in Table 4, the resultingparser ranks among the best lexicalized parsers, beat-ing those of Collins (1999) and Charniak and Johnson(2005).8 Its F1 performance is a 27% reduction in er-ror over Matsuzaki et al (2005) and Klein and Man-ning (2003).
Not only is our parser more accurate, butthe learned grammar is also significantly smaller thanthat of previous work.
While this all is accomplishedwith only automatic learning, the resulting grammar is8Even with the Viterbi parser our best grammar achieves88.7/88.9 LP/LR.?
40 words LP LR CB 0CBKlein and Manning (2003) 86.9 85.7 1.10 60.3Matsuzaki et al (2005) 86.6 86.7 1.19 61.1Collins (1999) 88.7 88.5 0.92 66.7Charniak and Johnson (2005) 90.1 90.1 0.74 70.1This Paper 90.3 90.0 0.78 68.5all sentences LP LR CB 0CBKlein and Manning (2003) 86.3 85.1 1.31 57.2Matsuzaki et al (2005) 86.1 86.0 1.39 58.3Collins (1999) 88.3 88.1 1.06 64.0Charniak and Johnson (2005) 89.5 89.6 0.88 67.6This Paper 89.8 89.6 0.92 66.3Table 4: Comparison of our results with those of others.human-interpretable.
It shows most of the manually in-troduced annotations discussed by Klein and Manning(2003), but also learns other linguistic phenomena.ReferencesG.
Ball and D. Hall.
1967.
A clustering technique for sum-marizing multivariate data.
Behavioral Science.S.
Caraballo and E. Charniak.
1998.
New figures of meritfor best?first probabilistic chart parsing.
In ComputationalLingusitics, p. 275?298.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-bestparsing and maxent discriminative reranking.
In ACL?05,p.
173?180.E.
Charniak.
1996.
Tree-bank grammars.
In AAAI ?96, p.1031?1036.E.
Charniak.
2000.
A maximum?entropy?inspired parser.
InNAACL ?00, p. 132?139.D.
Chiang and D. Bikel.
2002.
Recovering latent informationin treebanks.
In Computational Linguistics.N.
Chomsky.
1965.
Aspects of the Theory of Syntax.
MITPress.M.
Collins.
1999.
Head-Driven Statistical Models for Natu-ral Language Parsing.
Ph.D. thesis, U. of Pennsylvania.J.
Goodman.
1996.
Parsing algorithms and metrics.
In ACL?96, p. 177?183.J.
Henderson.
2004.
Discriminative training of a neural net-work statistical parser.
In ACL ?04.M.
Johnson.
1998.
PCFG models of linguistic tree represen-tations.
Computational Linguistics, 24:613?632.D.
Klein and C. Manning.
2003.
Accurate unlexicalizedparsing.
ACL ?03, p. 423?430.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
ProbabilisticCFG with latent annotations.
In ACL ?05, p. 75?82.F.
Pereira and Y. Schabes.
1992.
Inside-outside reestimationfrom partially bracketed corpora.
In ACL ?92, p. 128?135.D.
Prescher.
2005.
Inducing head-driven PCFGs with la-tent heads: Refining a tree-bank grammar for parsing.
InECML?05.H.
Schuetze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?124.S.
Sekine and M. J. Collins.
1997.
EVALB bracket scoringprogram.
http://nlp.cs.nyu.edu/evalb/.K.
Sima?an.
1992.
Computatoinal complexity of probabilis-tic disambiguation.
Grammars, 5:125?151.A.
Stolcke and S. Omohundro.
1994.
Inducing probabilisticgrammars by bayesian model merging.
In GrammaticalInference and Applications, p. 106?118.440
