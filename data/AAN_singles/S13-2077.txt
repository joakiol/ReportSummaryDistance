Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 466?470, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsFBK: Sentiment Analysis in Twitter with TweetstedMd.
Faisal Mahbub ChowdhuryFBK and University of Trento, Italyfmchowdhury@gmail.comMarco GueriniTrento RISE, Italymarco.guerini@trentorise.euSara TonelliFBK, Trento, Italysatonelli@fbk.euAlberto LavelliFBK, Trento, Italylavelli@fbk.euAbstractThis paper presents the Tweetsted system im-plemented for the SemEval 2013 task on Sen-timent Analysis in Twitter.
In particular, weparticipated in Task B on Message Polar-ity Classification in the Constrained setting.The approach is based on the exploitation ofvarious resources such as SentiWordNet andLIWC.
Official results show that our approachyields a F-score of 0.5976 for Twitter mes-sages (11th out of 35) and a F-score of 0.5487for SMS messages (8th out of 28 participants).1 IntroductionMicroblogging is currently a very popular commu-nication tool where millions of users share opinionson different aspects of life.
For this reason it is avaluable source of data for opinion mining and sen-timent analysis.Working with such type of texts presents chal-lenges for NLP beyond those typically encounteredwhen dealing with more traditional texts, such asnewswire data.
Tweets are short, the language usedis very informal, with creative spelling and punctua-tion, misspellings, slang, new words, URLs, genre-specific terminology and abbreviations, and #hash-tags.
These characteristics need to be handled withspecific approaches.This paper presents the approach adopted for theSemEval 2013 task on Sentiment Analysis in Twit-ter, in particular Task B on Message Polarity Clas-sification in the Constrained setting (i.e., using theprovided training data only).The goal of Task B on Message Polarity Classi-fication is the following: given a message, decidewhether it expresses a positive, negative, or neutralsentiment.
For messages conveying both a positiveand a negative sentiment, whichever is the strongersentiment should be chosen.Two modalities are possible: (1) Constrained (us-ing the provided training data only; other resources,such as lexica, are allowed; however, it is not al-lowed to use additional tweets/SMS messages or ad-ditional sentences with sentiment annotations); and(2) Unconstrained (using additional data for train-ing, e.g., additional tweets/SMS messages or addi-tional sentences annotated for sentiment).
We par-ticipated in the Constrained modality.We adopted a supervised machine learning (ML)approach based on various contextual and seman-tic features.
In particular, we exploited resourcessuch as SentiWordNet (Esuli and Sebastiani, 2006),LIWC (Pennebaker and Francis, 2001), and the lex-icons described in Mohammad et al(2009).Critical features include: whether the mes-sage contains intensifiers, adjectives, interjections,presence of positive or negative emoticons, pos-sible message polarity based on SentiWordNetscores (Esuli and Sebastiani, 2006; Gatti andGuerini, 2012), scores based on LIWC cate-gories (Pennebaker and Francis, 2001), negatedwords, etc.2 System DescriptionOur supervised ML-based approach relies on Sup-port Vector Machines (SVMs).
The SVM imple-mentation used in the system is LIBSVM (Chang466and Lin, 2001) for training SVM models and test-ing.
Moreover, in the preprocessing phase we usedTweetNLP (Owoputi et al 2013), a POS tagger ex-plicitly tailored for working on tweets.We adopted a 2 stage approach: (1) during stage1, we performed a binary classification of messagesaccording to the classes neutral vs subjective; (2)in stage 2, we performed a binary classification ofsubjective messages according to the classes positivevs negative.
We performed various experiments onthe training and development sets exploring the useof different features (see Section 2.1) to find the bestconfigurations for the official submission.2.1 Feature listWe implement several features divided into threegroups: contextual features, semantic features fromcontext and semantic features from external re-sources.
The complete list is reported in Table 1.Contextual features are features computed byconsidering only the tokens in the tweets/SMS andthe associated part of speech.Semantic Features from Context are featuresbased on words polarity.
Emoticons were recog-nized through a list of emoticons extracted fromWikipedia1 and then manually labeled as positive ornegative.
Negated words (feature n. 18) are any to-ken occurring between n?t, not, no and a comma, ex-cluding those tagged as function words.
Feature n.19 captures tokens (or sequences of tokens) labeledwith a positive or negative polarity in the resourcedescribed in Mohammad et al(2009).
The intensi-fiers considered for Feature n. 20 have been identi-fied by implementing a simple algorithm that detectstokens containing anomalously repeated characters(e.g.
happyyyyy).
Feature n. 21 was computed bytraining the system on the training data and predict-ing labels for the test data, and then using these la-bels as new features to train the system again.Semantic Features from external resources in-clude word classes from the Linguistic Inquiryand Word Count (LIWC), a tool that calculatesthe degree to which people use different cate-gories of words related to psycholinguistic pro-cesses (Pennebaker and Francis, 2001).
LIWC in-1http://en.wikipedia.org/wiki/List_of_emoticonscludes about 2,200 words and stems grouped into 70broad categories relevant to psychological processes(e.g., EMOTION, COGNITION).
Sample words areshown in Table 2.For each non-zero valued LIWC category of a cor-responding tweet/SMS, we added a feature for thatcategory and used the category score as the valueof that feature.
We call this LWIC string feature.Alternatively, we also added a separate feature foreach non-zero valued LIWC category and set 1 asthe value of that feature.
This feature is called LWICboolean.We also used words prior polarity - i.e.
if a wordout of context evokes something positive or nega-tive.
For this, we relied on SentiWordNet, a broad-coverage resource that provides polarities for (al-most) every word.
Since words can have multi-ple senses, we compute the prior polarity of a wordstarting from the polarity of each sense and returningits polarity strength as an index between -1 and 1.We tested 14 formulae that combine posterior polar-ities in different ways to obtain a word prior polarity,as reported in (Gatti and Guerini, 2012).For the SWNscoresMaximum feature, we selectthe prior polarity of the word in a tweet/SMS hav-ing the maximum absolute score among all words(of that tweet/SMS).
For SWNscoresPolarityCount,we select the polarity (positive, negative or neutral)that is assigned to the majority of the words.
Asfor SWNscoresSum, it corresponds to the sum ofthe prior polarities associated with all words in thetweet/SMS.3 Experimental SetupIn order to select the best performing feature set,we carried out several 5-fold cross validation ex-periments on the training data.
We report in Table3 the best performing feature set.
In particular, weadopted a 2 stage approach:1. during the first stage we performed a binaryclassification of messages according to theclasses neutral vs subjective;2. in the second stage, we performed a binaryclassification of subjective messages accordingto the classes positive vs negative.We opted for a two stage binary classification ap-proach, since we observed that it produces slightly467Contextual Features1.
noOfAdjectives num2.
adjective list string3.
interjection list string4.
firstInterj string5.
lastInterj string6.
bigramList string7.
beginsWithRT boolean8.
hasRTinMiddle boolean9.
endsWithLink boolean10.
endsWithHashtag boolean11.
hasQuestion booleanSemantic Features from Context12.
noOfPositiveEmoticons num13.
noOfNegativeEmoticons num14.
beginsWithPosEmoticon boolean15.
beginsWithNegEmoticon boolean16.
endsWithPosEmoticon boolean17.
endsWithNegEmoticon boolean18.
negatedWords string19.
indexOfChunksWithPolarity string20.
containsIntensifier boolean21.
labelPredictedBySystem pos./neg./neut.Semantic Features from External Resources22.
LIWC string string23.
LIWC boolean string24.
SWNscoresMaximum pos./neg./neut.25.
SWNscoresPolarityCount pos./neg./neut.26.
SWNscoresSum pos./neg./neut.Table 1: Complete feature list.LABEL Sample wordsCERTAIN all, very, fact*, exact*, certain*, completelyDISCREP but, if, expect*, shouldTENTAT or, some, may, possib*, probab*SENSES observ*, discuss*, shows, appearsSELF we, our, I, usSOCIAL discuss*, interact*, suggest*, argu*OPTIM best, easy*, enthus*, hope, prideANGER hate, kill, annoyedINHIB block, constrain, stopTable 2: Word categories along with sample wordsbetter results than a single stage multi-class ap-proach (i.e.
neutral vs positive vs negative).2 Dif-ferent combinations of classifiers were explored ob-taining comparable results.
Here we will report only2The average F-scores (pos and neg) for two stage and singlestage approaches obtained using the official scorer, by trainingon the training data and testing on the development data, are0.5682 and 0.5611 respectively.the best results.STAGE 1.
The best result for stage (1), neutral vssubjective, obtained with 5-fold cross validation ontraining set only, accounts for an accuracy of 69.6%.Instead, the best result for stage (1), obtained withtraining on training data and testing on developmentdata, accounts for an accuracy of 72.67%.The list of best features is reported in Table 3.Feature selection was performed by starting from asmall set of basic features, and then by adding theremaining features incrementally.Contextual Features2.
adjective list string3.
interjection list string5.
lastInterj stringSemantic Features from Context12.
noOfPositiveEmoticons num13.
noOfNegativeEmoticons num18.
negatedWords string19.
indexOfChunksWithPolarity string20.
containsIntensifier booleanSemantic Features from external resources23.
LIWC boolean string24.
SWNscoresMaximum posi./neg./neut.Table 3: Best performing feature set.STAGE 2.
In stage (2), positive vs negative, westarted from the best feature set obtained from stage(1) and added the remaining features one by one in-crementally.
In this case, we kept SWNscoresMaxi-mum without testing again other formulae; in partic-ular, to compute words prior polarity, we also keptthe first sense approach, that assigns to every wordthe SWN score of its most frequent sense and provedto be the most discriminative in the first stage neutralvs.
subjective.
We found that none of the feature setsproduced better results than that obtained using thebest feature set selected from stage (1).
So, the bestfeature set for stage (2) is unchanged.
We trainedthe system on the training data and tested it on thedevelopment data, achieving an accuracy of 80.67%.4 EvaluationThe SemEval task organizers (Wilson et al 2013)provided two test sets on which the systems wereto be evaluated: one included Twitter messages, i.e.the same type of texts included in the training set,468while the other comprised SMS messages, i.e.
textshaving more or less the same length as the Twitterdata but (supposedly) a different style.
We appliedthe same model, trained both on the training and thedevelopment set, on the two types of data, withoutany specific adaptation.The Twitter test set was composed of 3,813tweets.
Official results show that our approachyields an F-score of 0.5976 for Twitter messages(11th out of 35), while the best performing systemobtained an F-score of 0.6902.
The confusion ma-trix is reported in Table 4, while the score detailsin Table 5.
The latter table shows that our systemachieves the lowest results on negative tweets, bothin terms of precision and of recall.gs/pred positive negative neutralpositive 946 101 525negative 90 274 237neutral 210 70 1360Table 4: Confusion matrix for Twitter taskclass prec recall F-scorepositive 0.7592 0.6018 0.6714negative 0.6157 0.4559 0.5239neutral 0.6409 0.8293 0.7230average(pos and neg) 0.5976Table 5: Detailed results for Twitter taskThe SMS test set for the competition was com-posed of 2,094 SMS.
Official results provided by thetask organizers show that our approach yields an F-score of 0.5487 for SMS messages (8th out of 28participants), while the best performing system ob-tained an F-score of 0.6846.
The confusion matrixis reported in Table 6, while the score details in Ta-ble 7.
Also in this case the recognition of negativemessages achieves by far the poorest performance.A comparison of the results on the two test setsshows that, as expected, our system performs bet-ter on tweets than on SMS.
However, precisionachieved by the system on neutral SMS is 0.12points better on text messages than on tweets.Interestingly, it appears from the results in Ta-bles 5 and 7 (and from the distribution of the classesin the data sets) that there may be a correlation be-tween the number of tweets/SMS for a particularclass and the performance obtained for such class.We plan to further investigate this issue.gs/pred positive negative neutralpositive 320 44 128negative 66 171 157neutral 208 64 936Table 6: Confusion matrix for SMS taskclass prec recall F-scorepositive 0.5387 0.6504 0.5893negative 0.6129 0.4340 0.5082neutral 0.7666 0.7748 0.7707average(pos and neg) 0.5487Table 7: Detailed results for SMS task5 ConclusionsIn this paper, we presented Tweetsted, the system de-veloped by FBK for the SemEval 2013 task on Sen-timent Analysis.
We trained a classifier performinga two-step binary classification, i.e.
first neutral vs.subjective data, and then positive vs. negative ones.We implemented a set of features including contex-tual and semantic ones.
We also integrated in ourfeature representation external knowledge from Sen-tiWordNet, LIWC and the resource by Mohammadet al(2009).
On both test sets (i.e., Twitter mes-sages and SMS) of the constrained modality of thechallenge, we achieved a good performance, beingamong the top 30% of the competing systems.
Inthe near future, we plan to perform an error analysisof the wrongly classified data to investigate possibleclassification issues, in particular the lower perfor-mance on negative tweets and SMS.AcknowledgmentsThis work is supported by ?eOnco - Pervasive knowledgeand data management in cancer care?
and ?Trento RISEPerTe?
projects.ReferencesChih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.469A.
Esuli and F. Sebastiani.
2006.
SentiWordNet: Apublicly available lexical resource for opinion min-ing.
In Proceedings of the 5th International Confer-ence on Language Resources and Evaluation (LREC2006), Genoa, Italy.Lorenzo Gatti and Marco Guerini.
2012.
Assessing sen-timent strength in words prior polarities.
In Proceed-ings of COLING 2012: Posters, pages 361?370, Mum-bai, India, December.
The COLING 2012 OrganizingCommittee.Saif Mohammad, Bonnie Dorr, and Cody Dunne.
2009.Generating High-Coverage Semantic Orientation Lex-icons From Overtly Marked Words and a Thesaurus.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, KevinGimpel, Nathan Schneider, and Noah A. Smith.
2013.Improved part-of-speech tagging for online conver-sational text with word clusters.
In Proceedings ofNAACL 2013, Atlanta, Georgia, June.J.
Pennebaker and M. Francis.
2001.
Linguistic inquiryand word count: LIWC.
Erlbaum Publishers.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, SaraRosenthal, Veselin Stoyanov, and Alan Ritter.
2013.SemEval-2013 task 2: Sentiment analysis in twitter.In Proceedings of the International Workshop on Se-mantic Evaluation, SemEval ?13, June.470
