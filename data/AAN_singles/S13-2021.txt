Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 118?123, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsECNUCS: Recognizing Cross-lingual Textual Entailment Using MultipleText Similarity and Text Difference MeasuresJiang ZHAODepartment of ComputerScience and TechnologyEast China Normal UniversityShanghai, P.R.China51121201042@ecnu.cnMan LAN?Department of ComputerScience and TechnologyEast China Normal UniversityShanghai, P.R.Chinamlan@cs.ecnu.edu.cnZheng-Yu NIUBaidu Inc.Beijing, P.R.Chinaniuzhengyu@baidu.comAbstractThis paper presents our approach used forcross-lingual textual entailment task (task 8)organized within SemEval 2013.
Cross-lingual textual entailment (CLTE) tries to de-tect the entailment relationship between twotext fragments in different languages.
Wesolved this problem in three steps.
Firstly,we use a off-the-shelf machine translation(MT) tool to convert the two input texts intothe same language.
Then after performing atext preprocessing, we extract multiple featuretypes with respect to surface text and gram-mar.
We also propose novel feature typesregarding to sentence difference and seman-tic similarity based on our observations in thepreliminary experiments.
Finally, we adopt amulticlass SVM algorithm for classification.The results on the cross-lingual data collec-tions provided by SemEval 2013 show that (1)we can build portable and effective systemsacross languages using MT and multiple ef-fective features; (2) our systems achieve thebest results among the participants on two testdatasets, i.e., FRA-ENG and DEU-ENG.1 IntroductionThe Cross-lingual Textual Entailment (CLTE) taskin SemEval 2013 consists in detecting the entail-ment relationship between two topic-related textfragments (usually called T(ext) and H(ypothesis))in different languages, which is a cross-lingual ex-tension of TE task in (Dagan and Glickman, 2004).We say T entails H if the meaning of H can be in-ferred from the meaning of T. Mehdad et al(2010b)firstly proposed this problem within a new challeng-ing application scenario, i.e., content synchroniza-tion.
In consideration of the directionality, the taskneeds to assign one of the following entailment judg-ments to a pair of sentences (1) forward: unidirec-tional entailment from T to H; (2) backward: unidi-rectional entailment from H to T; (3) bidirectional:the two fragments entail each other (i.e., semanticequivalence); (4) non-entailment: there is no entail-ment between T and H.During the last decades, many researchers andcommunities have paid a lot of attention to resolvethe TE detection (e.g., seven times of the Rec-ognizing Textual Entailment Challenge, i.e., fromRTE1 to RET7, have been held) since identifyingthe relationship between two sentences is at the coreof many NLP applications, such as text summa-rization (Lloret et al 2008) or question answer-ing (Harabagiu and Hickl, 2006).
For example,in text summarization, a redundant sentence shouldbe omitted from the summary if this sentence canbe entailed from other expressions in the summary.CLTE extends those tasks with lingual dimension-ality, where more than one language is involved.Although it is a relatively new task, a basic solu-tion has been provided in (Mehdad et al 2010b),which brings the problem back to monolingual sce-nario using MT to translate H into the language ofT.
The promising performance indicates the poten-tialities of such a simple approach which integratesMT and monolingual TE algorithms (Castillo, 2011;Jimenez et al 2012; Mehdad et al 2010a).In this work, we regard CLTE as a multiclass clas-sification problem, in which multiple feature typesare used in conjunction with a multiclass SVM clas-sifier.
Specifically, our approach can be dividedinto three steps.
Firstly, following (Espla`-Gomiset al 2012; Meng et al 2012), we use MT to118bridge the gap of language differences between Tand H. Secondly, we perform a preprocessing pro-cedure to maximize the similarity of the two textfragments so as to make a more accurate calcula-tion of surface text similarity measures.
Besides sev-eral features described in previous work (Malakasi-otis, 2009; Espla`-Gomis et al 2012), we also pro-pose several novel features regarding to sentence dif-ference and semantic similarity.
Finally, all thesefeatures are combined together and serves as inputof a multiclass SVM classifier.
After analyzing ofthe results obtained in preliminary experiments, wealso cast this problem as a hierarchical classificationproblem.The remainder of the paper is organized as fol-lows.
Section 2 describes different features used inour systems.
Section 3 presents the system settingsincluding the datasets and preprocessing.
Section 4shows the results of different systems on differentlanguage pairs.
Finally, we conclude this paper withfuture work in Section 5.2 FeaturesIn this section, we will describe a variety of featuretypes used in our experiments.2.1 Basic featuresThe BC feature set consists of length measures onvariety sets including |A|, |B|, |A?B|, |B?A|, |A?B|, |A ?
B|, |A|/|B| and |B|/|A|, where A and Brepresent two texts, and the length of set is the num-ber of non-repeated elements in this set.
Once weview the text as a set of words, A?B means the setof words found in A but not in B, A ?
B means theset of words found in either A or B and A?B meansthe set of shared words found in both A and B.Given a pair of texts, i.e., <T,H>, which are indifferent languages, we use MT to translate one ofthem to make them in the same language.
Thus,we can get two pairs of texts, i.e., <Tt,H> and<T,H t>.
We apply the above eight length measuresto the two pairs, resulting in a total of 16 features.2.2 Surface Text Similarity featuresFollowing (Malakasiotis and Androutsopoulos,2007), the surface text similarity (STS) feature setcontains nine similarity measures:Jaccard coefficient: It is defined as |A?B||A?B| , where|A ?B| and |A ?B| are as in the BC.Dice coefficient: Defined as 2?|A?B||A|+|B| .Overlap coefficient: This is the following quantity,Overlap(A,B) = |A?B||A| .Weighted overlap coefficient: We assign the tf*idfvalue to each word in the sentence to distinguishthe importance of different words.
The weightedoverlap coefficient is defined as follows:WOverlap(A,B) =?wi?A?B Wwi?wi?AWwi,where Wwiis the weight of word wi.Cosine similarity: cos(?
?x ,?
?y ) =?
?x ???y??
?x ????
?y ?
, where?
?x and ?
?y are vectorial representations of texts (i.e.A and B) in tf ?
idf schema.Manhattan distance: Defined as M(?
?x ,?
?y ) =n?i=1|xi ?
yi|.Euclidean distance: Defined as E(?
?x ,?
?y ) =?n?i=1(xi ?
yi)2.Edit distance: This is the minimum number of op-erations needed to transform A to B.
We define anoperation as an insertion, deletion or substitution ofa word.Jaro-Winker distance: Following (Winkler andothers, 1999), the Jaro-Winkler distance is a mea-sure of similarity between two strings at the wordlevel.In total, we can get 11 features in this feature set.2.3 Sematic Similarity featuresAlmost every previous work used the surface textsor exploited the meanings of words in the dictio-nary to calculate the similarity of two sentencesrather than the actual meaning in the sentence.
Inthis feature set (SS), we introduce a latent modelto model the semantic representations of sentencessince latent models are capable of capturing thecontextual meaning of words in sentences.
Weused weighted textual matrix factorization (WTMF)(Guo and Diab, 2012) to model the semantics ofthe sentences.
The model factorizes the originalterm-sentence matrix X into two matrices such thatXi,j ?
P T?,iQ?,j , where P?,i is a latent semantics119vector profile for word wi and Q?,j is the vector pro-file that represents the sentence sj .
The weight ma-trix W is introduced in the optimization process inorder to model the missing words at the right levelof emphasis.
We propose three similarity measuresaccording to different strategies:wtw: word-to-word based similarity defined assim(A,B) = lg?wi?AWwi?maxwj?B(P?,i,P?,j)?wi?AWwi.wts: word-to-sentence based similarity defined assim(A,B) = lg?wi?AWwi?P?,i?Q?,k?wi?AWwi.sts: sentence-to-sentence based similarity defined assim(A,B) = lg (Q?,i ?Q?,j).Also we calculate the cosine similarity, Euclideanand Manhattan distance, weighted overlap coeffi-cient using those semantics vectors, resulting in 10features.2.4 Sentence Difference featuresMost of those above measures are symmetric andonly a few are asymmetric, which means they maynot be very suitable for the task that requires dealingwith directional problems.
We solve this problem byintroducing sentence difference measures.We observed that many entailment relationshipsbetween two sentences are determined by only tinyparts of the sentences.
As a result, the similarity ofsuch two sentences by using above measures will beclose to 1, which may mislead the classifier.
Fur-thermore, almost all similarity measures in STS aresymmetric, which means the same similarity has nohelp to distinguish the different directions.
Based onthe above considerations, we propose a novel sen-tence difference (SD) feature set to discover the dif-ferences between two sentences and tell the classi-fier the possibility the entailment should not hold.The sentence difference features are extracted asfollows.
Firstly, a word in one sentence is consid-ered as matched if we can find the same word in theother sentence.
Then we find all matched words andcount the number of unmatched words in each sen-tence, resulting in 2 features.
If one sentence hasno unmatched words, we say that this sentence canbe entailed by the other sentence.
That is, we caninfer the entailment class through the number of un-matched words.
We regard this label as our thirdfeature type.
Secondly, different POS types of un-matched words may have different impacts on theclassification, therefore we count the number of un-matched words in each sentence that belong to asmall set of POS tags (here consider only NN, JJ,RB, VB and CD tags), which produces 10 features,resulting in a total of 13 sentence difference features.2.5 Grammatical Relationship featuresThe grammatical relationship feature type (GR) isdesigned to capture the grammatical relationship be-tween two sentences.
We first replace the words in asentence with their part-of-speech (POS) tags, thenapply the STS measures on this new ?sentence?.In addition, we use the Stanford Parser to get thedependency information represented in a form of re-lation units (e.g.
nsubj(example, this)).
We calculatethe BC measures on those units and the overlap co-efficients together with the harmonic mean of them.Finally, we get 22 features.2.6 Bias featuresThe bias features (BS) are to check the differencesbetween two sentences in certain special aspects,such as polarity and named entity.
We use a methodbased on subjectivity of lexicons (Loughran and Mc-Donald, 2011) to get the polarity of a sentence bysimply comparing the numbers of positive and neg-ative words.
If the numbers are the same, then weset the feature to 1, otherwise -1.
Also, we checkwhether one sentence entails the other using onlythe named entity information.
We consider four cat-egories of named entities, i.e., person, organization,location, number, which are recognized by using theStanford NER toolkit.
We set the feature to 1 if thenamed entities in one sentence are found in the othersentence, otherwise -1.
As a result, this feature setcontains 9 features.3 Experimental SettingWe evaluated our approach using the data setsprovided in the task 8 of SemEval 2013 (Ne-gri et al 2013).
The data sets consist of acollection of 1500 text fragment pairs (1000 fortraining consisting of training and test set in Se-mEval 2012 and 500 for test) in each languagepair.
Four different language pairs are provided:German-English, French-English, Italian-Englishand Spanish-English.
See (Negri et al 2013) formore detailed description.1203.1 PreprocessWe performed the following text preprocessing.Firstly, we employed the state-of-the-art StatisticalMachine Translator, i.e., Google translator, to trans-late each pair of texts <T,H> into <Tt,H> and<T,H t>, thus they were in the same language.
Thenwe extracted all above described feature sets fromthe pair <T t,H> (note that <T,Ht> are also usedin BC), so the below steps were mainly operated onthis pair.
After that, all sentences were tokenizedand lemmatized using the Stanford Lemmatizer andall stop words were removed, followed by the equiv-alent replacement procedure.
The replacement pro-cedure consists of the following 3 steps:Abbreviative replacement.
Many phrases or orga-nizations can be abbreviated to a set of capitalizedletters, e.g.
?New Jersey?
is usually wrote as ?NJ?for short.
In this step, we checked every word whoselength is 2 or 3 and if it is the same as the ?word?consisting of the first letters of the successive wordsin another sentence, then we replaced it by them.Semantic replacement.
We observed that althoughsome lemmas in H and T were in the different forms,they actually shared the same meaning, e.g.
?hap-pen?
and ?occur?.
Here, we focused on replacing alemma in one sentence with another lemma in theother sentence if they were: 1) in the same syn-onymy set; or 2) gloss-related.
Two lemmas weregloss-related if a lemma appeared in the gloss of theother.
For example, the gloss of ?trip?
is ?a jour-ney for some purpose?
(WordNet 2.1 was used forlooking up the synonymy and gloss of a lemma), sothe lemma ?journey?
is gloss-related with ?trip?.
Noword sense disambiguation was performed and allsynsets for a particular lemma were considered.Context replacement.
The context of a lemmais defined as the non-stopword lemmas around it.Given two text fragments, i.e., T. ...be erroneouslylabel as a ?register sex offender.?
and H. ...be mis-takenly inscribe as a ?register sex offender?., af-ter the semantic replacement, we can recognize thelemma ?erroneously?
was replaceable by ?mistak-enly?.
However, WordNet 2.1 cannot recognize thelemmas ?label?
and ?inscribe?
which can also bereplaceable.
To address this problem, we simply as-sumed that two lemmas surrounded by the same con-text can be replaceable as well.
In the experiments,we set the window size of context replacement as 3.This step is the foundation of the extraction ofthe sentence different features and can also allevi-ate the imprecise similarity measure problem exist-ing in STS caused by the possibility of the lemmasin totally different forms sharing the same sense.3.2 System ConfigurationWe selected 500 samples from the training data asdevelopment set (i.e.
test set in SemEval 2012) andperformed a series of preliminary experiments toevaluate the effectiveness of different feature typesin isolation and also in different combinations.
Ac-cording to the results on the development set, weconfigured five different systems on each languagepair as our final submissions with different featuretypes and classification strategies.
Table1 shows thefive configurations of those systems.System Feature Set Description1 all flat, SVM2 best feature sets flat, SVM3 best feature sets flat, Majority Voting4 best feature sets flat, only 500 instancesfor train, SVM5 best feature sets hierarchical, SVMTable 1: System configurations using different strategiesbased on the results of preliminary experiments.Among them, System 1 serves as a baseline thatused all features and was trained using a flat SVMwhile System 2 used only the best feature combi-nations.
In our preliminary experiments, differentlanguage pairs had different best feature combina-tions (showed in Table 2).
In System 3 we per-formed a majority voting strategy to combine theresults of different algorithm (i.e.
MaxEnt, SVM,liblinear) to further improve performance.
System4 is a backup system that used only the training setin SemEval 2012 to explore the influence of the dif-ferent size of train set.
Based on the analysis of thepreliminary results on development set, we also findthat the misclassification mainly occur between theclass of backward and others.
So in System 5, weadopted hierarchical classification technique to filterout backward class in the first level using a binaryclassifier and then conducted multi-class classifica-tion among the remaining three classes.121We used a linear SVM with the trade-off parame-ter C=1000 (also in liblinear).
The parameters in SSare set as below: the dimension of sematic space is100, the weight of missing words is 100 and the reg-ularization factor is 0.01.
In the hierarchical classifi-cation, we use the liblinear (Fan et al 2008) to traina binary classifier and SVM for a multi-class classi-fier with the same parameters in other Systems.4 Results and discussionTable 2 lists the final results of our five systems onthe test samples in terms of four language pairs.
Thebest feature set combinations for different languagepairs are also shown.
The last two rows list the re-sults of the best and runner-up team among six par-ticipants, which is released by the organizers.From this table, we have some interesting find-ings.Firstly, the feature types BC and SD appear in allbest feature combinations.
This indicates that thelength and sentence difference information are goodand effective label indicators.Secondly, based on the comparison between Sys-tem 1 and System 2, we find that the behavior of thebest feature sets of different language pairs on testand development datasets is quite different.
Specif-ically, the best feature set performs better on FRA-ENG and DEU-ENG data sets than the full featureset.
However, the full feature set performs the beston SPA-ENG and ITA-ENG data sets.
The reasonmay be the different distribution properties of testand development data sets.Thirdly, although the only difference betweenSystem 2 and System 4 is the size of training sam-ples, System 4 trained on a small number of traininginstances even makes a 1.6% improvement in accu-racy over System 2 on DEU-ENG data set.
Thisis beyond our expectation and it indicates that theCLTE may not be sensitive to the size of data set.Fourthly, by adopting a majority voting scheme,System 3 achieves the best results on two data setsamong five systems and obtains 45.8% accuracy onFRA-ENG which is the best result among all partic-ipants.
This indicates the majority voting strategy isa effective way to boost the performance.Fifthly, System 5 which adopts hierarchical clas-sification technique fails to make further improve-ment.
But it still outperforms the runner-up systemin this task on FRA-ENG and DEU-ENG.
We spec-ulate that the failure of System 5 may be caused bythe errors sensitive to hierarchical structure in hier-archical classification.In general, our approaches obtained very goodresults on all the language pairs.
On FRA-ENGand DEU-ENG, we achieved the best results amongthe 16 systems with the accuracy 45.8% and 45.3%respectively and largely outperformed the runner-up.
The results on SPA-ENG and ITA-ENG werealso promising, achieving the second and third placeamong the 16 systems.5 ConclusionWe have proposed several effectively features con-sisting of sentence semantic similarity and sentencedifference, which work together with other featurespresented by the previous work to solve the cross-lingual textual entailment problem.
With the aidof machine translation, we can handle the cross-linguality.
We submitted five systems on each lan-guage pair and obtained the best result on two datasets, i.e., FRA-ENG and DEU-ENG, and ranked the2nd and the 3rd on other two language pairs respec-tively.
Interestingly, we find some simple featuretypes like BC and SD are good class indicators andcan be easily acquired.
In future work, we will in-vestigate the discriminating power of different fea-ture types in the CLTE task on different languages.AcknowledgementsThe authors would like to thank the organizers andreviewers for this interesting task and their helpfulsuggestions and comments, which improves the fi-nal version of this paper.
This research is supportedby grants from National Natural Science Foundationof China (No.60903093), Shanghai Pujiang TalentProgram (No.09PJ1404500), Doctoral Fund of Min-istry of Education of China (No.
20090076120029)and Shanghai Knowledge Service Platform Project(No.
ZF1213).ReferencesJulio Javier Castillo.
2011.
A wordnet-based seman-tic approach to textual entailment and cross-lingual122System SPA-ENG ITA-ENG FRA-ENG DEU-ENG1 0.428 0.426 0.438 0.4222 0.404 0.420 0.450 0.4363 0.408 0.426 0.458 0.4324 0.422 0.416 0.436 0.4525 0.392 0.402 0.442 0.426Bestfeature setBC+STS+SS+GR+SDBC+SD+SS+GR+BS SD+BC+STSBC+STS+SS+BS+SDBest 0.434 0.454 0.458 0.452runner-up 0.428 0.432 0.426 0.414Table 2: The accuracy results of our systems on different language pairs released by the organizer.textual entailment.
International Journal of MachineLearning and Cybernetics, 2(3):177?189.Ido Dagan and Oren Glickman.
2004.
Probabilistic tex-tual entailment: Generic applied modeling of languagevariability.
In Proceedings of the PASCAL Workshopon LearningMethods for Text Understanding andMin-ing.Miquel Espla`-Gomis, Felipe Sa?nchez-Mart?
?nez, andMikel L. Forcada.
2012.
Ualacant: Using online ma-chine translation for cross-lingual textual entailment.In Proceedings of the Sixth International Workshop onSemantic Evaluation (SemEval 2012), pages 472?476,Montre?al, Canada, 7-8 June.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
Liblinear: A libraryfor large linear classification.
The Journal of MachineLearning Research, 9:1871?1874.Weiwei Guo and Mona Diab.
2012.
Modeling sentencesin the latent space.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Linguis-tics.Sanda Harabagiu and Andrew Hickl.
2006.
Methods forusing textual entailment in open-domain question an-swering.
InProceedings of the 21st InternationalCon-ference on Computational Linguistics and 44th AnnualMeeting of the Association for Computational Linguis-tics, pages 905?912, Sydney, Australia, July.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2012.
Soft cardinality+ ml: Learning adaptivesimilarity functions for cross-lingual textual entail-ment.
In Proceedings of the 6th International Work-shop on Semantic Evaluation (SemEval 2012).Elena Lloret, Oscar Ferra?ndez, Rafael Munoz, andManuel Palomar.
2008.
A text summarization ap-proach under the influence of textual entailment.
InProceedings of the 5th International Workshop onNatural Language Processing and Cognitive Science(NLPCS 2008), pages 22?31.Tim Loughran and Bill McDonald.
2011.
When is aliability not a liability?
textual analysis, dictionaries,and 10-ks.
The Journal of Finance, 66(1):35?65.Prodromos Malakasiotis and Ion Androutsopoulos.2007.
Learning textual entailment using svms andstring similarity measures.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Para-phrasing, pages 42?47.Prodromos Malakasiotis.
2009.
Paraphrase recognitionusing machine learning to combine similarity mea-sures.
In Proceedings of the ACL-IJCNLP 2009 Stu-dent Research Workshop, pages 27?35.Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-simo Zanzotto.
2010a.
Syntactic/semantic structuresfor textual entailment recognition.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 1020?1028.Yashar Mehdad, Matteo Negri, and Marcello Federico.2010b.
Towards cross-lingual textual entailment.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 321?324, Los Angeles, California, June.Fandong Meng, Hao Xiong, and Qun Liu.
2012.
Ict:A translation based method for cross-lingual textualentailment.
In Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation (SemEval 2012),pages 715?720, Montre?al, Canada, 7-8 June.M.
Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, andD.
Giampiccolo.
2013.
Semeval-2013 Task 8: Cross-lingual Textual Entailment for Content Synchroniza-tion.
In Proceedings of the 7th InternationalWorkshopon Semantic Evaluation (SemEval 2013).William E Winkler et al1999.
The state of record link-age and current research problems.123
