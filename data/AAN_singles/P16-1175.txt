Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1859?1869,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsUser Modeling in Language Learning with Macaronic TextsAdithya Renduchintala and Rebecca Knowles and Philipp Koehn and Jason EisnerDepartment of Computer ScienceJohns Hopkins University{adi.r,rknowles,phi,eisner}@jhu.eduAbstractForeign language learners can acquire newvocabulary by using cognate and con-text clues when reading.
To measuresuch incidental comprehension, we devisean experimental framework that involvesreading mixed-language ?macaronic?
sen-tences.
Using data collected via Ama-zon Mechanical Turk, we train a graphi-cal model to simulate a human subject?scomprehension of foreign words, based oncognate clues (edit distance to an Englishword), context clues (pointwise mutual in-formation), and prior exposure.
Our modeldoes a reasonable job at predicting whichwords a user will be able to understand,which should facilitate the automatic con-struction of comprehensible text for per-sonalized foreign language education.1 IntroductionSecond language (L2) learning requires the ac-quisition of vocabulary as well as knowledge ofthe language?s constructions.
One of the ways inwhich learners become familiar with novel vocab-ulary and constructions is through reading.
Ac-cording to Krashen?s Input Hypothesis (Krashen,1989), learners acquire language through inciden-tal learning, which occurs when learners are ex-posed to comprehensible input.
What constitutes?comprehensible input?
for a learner varies astheir knowledge of the L2 increases.
For example,a student in their first month of German lessonswould be hard-pressed to read German novels oreven front-page news, but they might understandbrief descriptions of daily routines.
Comprehen-sible input need not be completely familiar to thelearner; it could include novel vocabulary items orstructures (whose meanings they can glean fromcontext).
Such input falls in the ?zone of proxi-mal development?
(Vygotski?
?, 2012), just outsideof the learner?s comfort zone.
The related con-cept of ?scaffolding?
(Wood et al, 1976) consistsof providing assistance to the learner at a level thatis just sufficient for them to complete their task,which in our case is understanding a sentence.Automatic selection or construction ofcomprehensible input?perhaps online andpersonalized?would be a useful educationaltechnology.
However, this requires modeling thestudent: what can an L2 learner understand in agiven context?
In this paper, we develop a modeland train its parameters on data that we collect.For the remainder of the paper we focus onnative English speakers learning German.
Ourmethodology is a novel solution to the problemof controlling for the learner?s German skill level.We use subjects with zero previous knowledge ofGerman, but we translate portions of the sentenceinto English.
Thus, we can presume that they doalready know the English words and do not al-ready know the German words (except from see-ing them in earlier trials within our experiment).We are interested in whether they can jointly inferthe meanings of the remaining German words inthe sentence, so we ask them to guess.The resulting stimuli are sentences like ?DerPolizist arrested the Bankr?auber.?
Even a readerwith no knowledge of German is likely to be ableto understand this sentence reasonably well by us-ing cognate and context clues.
We refer to thisas a macaronic sentence; so-called macaronic lan-guage is a pastiche of two or more languages (of-ten intended for humorous effect).Our experimental subjects are required to guesswhat ?Polizist?
and ?Bankr?auber?
mean in thissentence.
We train a featurized model to pre-dict these guesses jointly within each sentence andthereby predict incidental comprehension on anymacaronic sentence.
Indeed, we hope our modeldesign will generalize from predicting incidentalcomprehension on macaronic sentences (for ourbeginner subjects, who need some context wordsto be in English) to predicting incidental compre-hension on full German sentences (for more ad-1859vanced students, who understand some of the con-text words as if they were in English).
In addition,we are developing a user interface that uses maca-ronic sentences directly as a medium of languageinstruction: our companion paper (Renduchintalaet al, 2016) gives an overview of that project.We briefly review previous work, then describeour data collection setup and the data obtained.
Fi-nally, we discuss our model of learner comprehen-sion and validate our model?s predictions.2 Previous WorkNatural language processing (NLP) has long beenapplied to education, but the majority of this workfocuses on evaluation and assessment.
Promi-nent recent examples include Heilman and Mad-nani (2012), Burstein et al (2013) and Madnaniet al (2012).
Other works fall more along thelines of intelligent and adaptive tutoring systemsdesigned to improve learning outcomes.
Most ofthose are outside of the area of NLP (typically fo-cusing on math or science).
An overview of NLP-based work in the education sphere can be foundin Litman (2016).
There has also been work spe-cific to second language acquisition, such as?Ozbalet al (2014), where the focus has been to builda system to help learners retain new vocabulary.However, much of the existing work on incidentallearning is found in the education and cognitivescience literature rather than NLP.Our work is related to Labutov and Lipson(2014), which also tries to leverage incidentallearning using mixed L1 and L2 language.
Wheretheir work uses surprisal to choose contexts inwhich to insert L2 vocabulary, we consider bothcontext features and other factors such as cognatefeatures (described in detail in 4.1).
We collectdata that gives direct evidence of the user?s un-derstanding of words (by asking them to provideEnglish guesses) rather than indirectly (via ques-tions about sentence validity, which runs the riskof overestimating their knowledge of a word, if,for instance, they?ve only learned whether it is an-imate or inanimate rather than the exact meaning).Furthermore, we are not only interested in whethera mixed L1 and L2 sentence is comprehensible;we are also interested in determining a distributionover the learner?s belief state for each word in thesentence.
We do this in an engaging, game-likesetting, which provides the user with hints whenthe task is too difficult for them to complete.3 Data Collection SetupOur method of scaffolding is to replace certain for-eign words and phrases with their English trans-lations, yielding a macaronic sentence.1Simplypresenting these to a learner would not give usfeedback on the learner?s belief state for each for-eign word.
Even assessing the learner?s readingcomprehension would give only weak indirect in-formation about what was understood.
Thus, wecollect data where a learner explicitly guesses aforeign word?s translation when seen in the mac-aronic context.
These guesses are then treated assupervised labels to train our user model.We used Amazon Mechanical Turk (MTurk) tocollect data.
Users qualified for tasks by complet-ing a short quiz and survey about their languageknowledge.
Only users whose results indicatedno knowledge of German and self-identified asnative speakers of English were allowed to com-plete tasks.
With German as the foreign language,we generated content by crawling a simplified-German news website, nachrichtenleicht.de.
We chose simplified German in order tominimize translation errors and to make the taskmore suitable for novice learners.
We translatedeach German sentence using the Moses Statis-tical Machine Translation (SMT) toolkit (Koehnet al, 2007).
The SMT system was trained on theGerman-English Commoncrawl parallel text usedin WMT 2015 (Bojar et al, 2015).We used 200 German sentences, presentingeach to 10 different users.
In MTurk jargon, thisyielded 2000 Human Intelligence Tasks (HITs).Each HIT required its user to participate in severalrounds of guessing as the English translation wasincrementally revealed.
A user was paid US$0.12per HIT, with a bonus of US$6 to any user whoaccumulated more than 2000 total points.Our HIT user interface is shown in the video athttps://youtu.be/9PczEcnr4F8.3.1 HITs and SubmissionsFor each HIT, the user first sees a German sen-tence2(Figure 1).
A text box is presented beloweach German word in the sentence, for the user1Although the language distinction is indicated by italicsand color, users were left to figure this out on their own.2Except that we first ?translate?
any German words thathave identical spelling in English (case-insensitive).
This in-cludes most proper names, numerals, and punctuation marks.Such translated words are displayed in English style (blueitalics), and the user is not asked to guess their meanings.1860Figure 1: After a user submits a set of guesses (top), the in-terface marks the correct guesses in green and also reveals aset of translation clues (bottom).
The user now has the oppor-tunity to guess again for the remaining German words.to type in their ?best guess?
of what each Ger-man word means.
The user must fill in at leasthalf of the text boxes before submitting this set ofguesses.
The resulting submission?i.e., the maca-ronic sentence together with the set of guesses?islogged in a database as a single training example,and the system displays feedback to the user aboutwhich guesses were correct.After each submission, new clues are revealed(providing increased scaffolding) and the user isasked to guess again.
The process continues,yielding multiple submissions, until all Germanwords in the sentence have been translated.
At thispoint, the entire HIT is considered completed andthe user moves to a new HIT (i.e., a new sentence).From our 2000 HITs, we obtained 9392 submis-sions (4.7 per HIT) from 79 distinct MTurk users.3.2 CluesEach update provides new clues to help the usermake further guesses.
There are 2 kinds of clues:Translation Clue (Figure 1): A set of words thatwere originally in German are replaced with theirEnglish translations.
The text boxes below thesewords disappear, since it is no longer necessary toguess them.Reordering Clue (Figure 2): A German sub-string is moved into a more English-like position.The reordering positions are calculated using theword and phrase alignments obtained from Moses.Each time the user submits a set of guesses, wereveal a sequence of n = max(1, round(N/3))clues, whereN is the number of German words re-maining in the sentence.
For each clue, we samplea token that is currently in German.
If the token isFigure 2: In this case, after the user submits a set of guesses(top), two clues are revealed (bottom): ausgestellt ismoved into English order and then translated.part of a movable phrase, we move that phrase;otherwise we translate the minimal phrase con-taining that token.
These moves correspond ex-actly to clues that a user could request by clickingon the token in the macaronic reading interface ofRenduchintala et al (2016)?see that paper for de-tails of how moves are constructed and animated.In our present experiments, the system is in controlinstead, and grants clues by ?randomly clicking?on n tokens.The system?s probability of sampling a given to-ken is proportional to its unigram type probabilityin the WMT corpus.
Thus, rarer words tend to re-main in German for longer, allowing the Turker toattempt more guesses for these difficult words.3.3 FeedbackWhen a user submits a set of guesses, the sys-tem responds with feedback.
Each guess is vis-ibly ?marked?
in left-to-right order, momentarilyshaded with green (for correct), yellow (for close)or red (for incorrect).
Depending on whethera guess is correct, close, or wrong, users areawarded points as discussed below.
Yellow andred shading then fades, to signal to the user thatthey may try entering a new guess.
Correctguesses remain on the screen for the entire task.3.4 PointsAdding points to the process (Figures 1?2) addsa game-like quality and lets us incentivize usersby paying them for good performance (see sec-tion 3).
We award 10 points for each exactly cor-rect guess (case-insensitive).
We give additional?effort points?
for a guess that is close to the cor-1861rect translation, as measured by cosine similarityin vector space.
(We used pre-trained GLoVe wordvectors (Pennington et al, 2014); when the guessor correct translation has multiple words, we takethe average of the word vectors.)
We deduct effortpoints for guesses that are careless or very poor.Our rubric for effort points is as follows:ep=?????????
?1, if e?
is repeated or nonsense (red)?1, if sim(e?, e?)
< 0 (red)0, if 0 ?
sim(e?, e?)
< 0.4 (red)0, if e?
is blank10?
sim(e?, e?)
otherwise (yellow)Here sim(e?, e?)
is cosine similarity between thevector embeddings of the user?s guess e?
and ourreference translation e?.
A ?nonsense?
guess con-tains a word that does not appear in the sentencebitext nor in the 20,000 most frequent word typesin the GLoVe training corpus.
A ?repeated?
guessis an incorrect guess that appears more than oncein the set of guesses being submitted.In some cases, e?
or e?may itself consist of mul-tiple words.
In this case, our points and feedbackare based on the best match between any word ofe?
and any word of e?.
In alignments where mul-tiple German words translate as a single phrase,3we take the phrasal translation to be the correctanswer e?for each of the German words.3.5 NormalizationAfter collecting the data, we normalized the userguesses for further analysis.
All guesses werelowercased.
Multi-word guesses were crudely re-placed by the longest word in the guess (breakingties in favor of the earliest word).The guesses included many spelling errors aswell as some nonsense strings and direct copiesof the input.
We defined the dictionary to be the100,000 most frequent word types (lowercased)from the WMT English data.
If a user?s guess e?does not match e?and is not in the dictionary, wereplace it with?
the special symbol <COPY>, if e?
appearsto be a copy of the German source word f(meaning that its Levenshtein distance fromf is < 0.2 ?max(|e?|, |f |));?
else, the closest word in the dictionary4asmeasured by Levenshtein distance (breaking3Our German-English alignments are constructed as inRenduchintala et al (2016).4Considering only words returned by the Pyenchant ?sug-gest?
function (http://pythonhosted.org/pyenchant/).ties alphabetically), provided the dictionaryhas a word at distance ?
2;?
else <BLANK>, as if the user had not guessed.4 User ModelIn each submission, the user jointly guesses sev-eral English words, given spelling and contextclues.
One way that a machine could performthis task is via probabilistic inference in a factorgraph?and we take this as our model of how thehuman user solves the problem.The user observes a German sentence f =[f1, f2, .
.
.
, fi, .
.
.
fn].
The translation of eachword token fiis Ei, which is from the user?spoint of view a random variable.
Let Obs denotethe set of indices i for which the user also ob-serves that Ei= e?i, the aligned reference trans-lation, because e?ihas already been guessed cor-rectly (green feedback) or shown as a clue.
Thus,the user?s posterior distribution over E is P?
(E =e | EObs= e?Obs, f , history), where ?history?
de-notes the user?s history of past interactions.We assume that a user?s submission?e is derivedfrom this posterior distribution simply as a ran-dom sample.
We try to fit the parameter vector?
to maximize the log-probability of the submis-sion.
Note that our model is trained on the userguesses e?, not the reference translations e?.
Thatis, we seek parameters ?
that would explain whyall users made their guesses.Although we fit a single ?, this does not meanthat we treat users as interchangeable (since ?can include user-specific parameters) or unvary-ing (since our model conditions users?
behavior ontheir history, which can capture some learning).4.1 Factor GraphWe model the posterior distribution as a condi-tional random field (Figure 3) in which the valueof Eidepends on the form of fias well as onthe meanings ej(which may be either observed orjointly guessed) of the context words at j 6= i:P?
(E = e | EObs = e?Obs, f , history) (1)?
?i/?Obs(?ef(ei, fi) ?
?j 6=i?ee(ei, ej, i?
j))We will define the factors ?
(the potential func-tions) in such a way that they do not ?know Ger-man?
but only have access to information that isavailable to an naive English speaker.
In brief, the1862f1.
.
.fi.
.
.fnE1.
.
.Ei.
.
.En?ee(e1, ei) ?ee(ei, en)?ee(e1, en)?ef(e1, f1) ?ef(ei, fi) ?ef(en, fn)Figure 3: Model for user understanding of L2 words in sen-tential context.
This figure shows an inference problem inwhich all the observed words in the sentence are in German(that is, Obs = ?).
As the user observes translations via cluesor correctly-marked guesses, some of the Eibecome shaded.factor ?ef(ei, fi) considers whether the hypothe-sized English word ei?looks like?
the observedGerman word fi, and whether the user has previ-ously observed during data collection that eiis acorrect or incorrect translation of fi.
Meanwhile,the factor ?ee(ei, ej) considers whether eiis com-monly seen in the context of ejin English text.
Forexample, the user will elevate the probability thatEi= cake if they are fairly certain that Ejis arelated word like eat or chocolate.The potential functions ?
are parameterized by?, a vector of feature weights.
For convenience,we define the features in such a way that we ex-pect their weights to be positive.
We rely onjust 6 features at present (see section 6 for futurework), although each is complex and real-valued.Thus, the weights ?
control the relative influenceof these 6 different types of information on a user?sguess.
Our features broadly fall under the follow-ing categories: Cognate, History, and Context.
Weprecomputed cognate and context features, whilehistory features are computed on-the-fly for eachtraining instance.
All features are case-insensitive.4.1.1 Cognate and History FeaturesFor each German token fi, the ?effactor can scoreeach possible guess eiof its translation:?ef(ei, fi) = exp(?ef?
?ef(ei, fi)) (2)The feature function ?efreturns a vector of 4 realnumbers:?
Orthographic Similarity: The normalizedLevenshtein distance between the 2 strings.
?eforth(ei, fi) = 1?lev(ei, fi)max(|ei|, |fi|)(3)The weight on this feature encodes how muchusers pay attention to spelling.?
Pronunciation Similarity: This feature is sim-ilar to the previous one, except that it cal-culates the normalized distance between thepronunciations of the two words:?efpron(ei, fi) = ?eforth(prn(ei), prn(fi)) (4)where the function prn(x) maps a string xto its pronunciation.
We obtained pronuncia-tions for all words in the English and Germanvocabularies using the CMU pronunciationdictionary tool (Weide, 1998).
Note that weuse English pronunciation rules even for Ger-man words.
This is because we are modelinga naive learner who may, in the absence ofintuition about German pronunciation rules,apply English pronunciation rules to German.?
Positive History Feature: If a user has beenrewarded in a previous HIT for guessing eias a translation of fi, then they should bemore likely to guess it again.
We define?efhist+(ei, fi) to be 1 in this case and 0 oth-erwise.
The weight on this feature encodeswhether users learn from positive feedback.?
Negative History Feature: If a user has al-ready incorrectly guessed eias a translationof fiin a previous submission during thisHIT, then they should be less likely to guess itagain.
We define ?efhist-(ei, fi) to be?1 in thiscase and 0 otherwise.
The weight on this fea-ture encodes whether users remember nega-tive feedback.54.1.2 Context FeaturesIn the same way, the ?effactor can score the com-patibility of a guess eiwith a context word ej,which may itself be a guess, or may be observed:?eeij(ei, ej) = exp(?ee?
?ee(ei, ej, i?
j)) (5)?eereturns a vector of 2 real numbers:?eepmi(ei, ej) ={PMI(ei, ej) if |i?
j| > 10 otherwise(6)?eepmi1(ei, ej) ={PMI1(ei, ej) if |i?
j| = 10 otherwise(7)5At least in short-term memory?this feature currentlyomits to consider any negative feedback from previous HITs.1863where the pointwise mutual informationPMI(x, y) measures the degree to which theEnglish words x, y tend to occur in the sameEnglish sentence, and PMI1(x, y) measures howoften they tend to occur in adjacent positions.These measurements are estimated from theEnglish side of the WMT corpus, with smoothingperformed as in Knowles et al (2016).For example, if fi= Suppe, the user?s guess ofEishould be influenced by fj= Brot appearingin the same sentence, if the user suspects or ob-serves that its translation is Ej= bread.
ThePMI feature knows that soup and bread tendto appear in the same English sentences, whereasPMI1knows that they tend not to appear in the bi-gram soup bread or bread soup.4.1.3 User-Specific FeaturesApart from the basic 6-feature model, we alsotrained a version that includes user-specific copiesof each feature (similar to the domain adaptationtechnique of Daum?e III (2007)).
For example,?eforth,32(ei, fi) is defined to equal ?eforth(ei, fi) forsubmissions by user 32, and defined to be 0 forsubmissions by other users.Thus, with 79 users in our dataset, we learned6 ?
80 feature weights: a local weight vectorfor each user and a global vector of ?backoff?weights.
The global weight ?eforthis large if usersin general reward orthographic similarity, while?eforth,32(which may be positive or negative) cap-tures the degree to which user 32 rewards it moreor less than is typical.
The user-specific featuresare intended to capture individual differences inincidental comprehension.4.2 InferenceAccording to our model, the probability that theuser guesses Ei= e?iis given by a marginal prob-ability from the CRF.
Computing these marginalsis a combinatorial optimization problem that in-volves reasoning jointly about the possible valuesof each Ei(i /?
Obs), which range over the En-glish vocabulary Ve.We employ loopy belief propagation (Murphyet al, 1999) to obtain approximate marginals overthe variables E. A tree-based schedule for mes-sage passing was used (Dreyer and Eisner, 2009,footnote 22).
We run 3 iterations with a new ran-dom root for each iteration.We define the vocabulary Veto consist ofall reference translations e?iand normalized userguesses e?ifrom our entire dataset (see section 3.5),about 5K types altogether including <BLANK>and <COPY>.
We define the cognate features totreat <BLANK> as the empty string and to treat<COPY> as fi.
We define the PMI of these spe-cial symbols with any e to be the mean PMI with eof all dictionary words, so that they are essentiallyuninformative.4.3 Parameter EstimationWe learn our parameter vector ?
to approximatelymaximize the regularized log-likelihood of theusers?
guesses:(?logP?
(E = ?e | EObs = e?Obs, f , history))?
?||?||2(8)where the summation is over all submissions inour dataset.
The gradient of each summand re-duces to a difference between observed and ex-pected values of the feature vector ?
= (?ef,?ee),summed over all factors in (1).
The observed fea-tures are computed directly by setting E =?e.
Theexpected features (which arise from the log of thenormalization constant of (1)) are computed ap-proximately by loopy belief propagation.We trained ?
using stochastic gradient descent(SGD),6with a learning rate of 0.1 and regulariza-tion parameter of 0.2.
The regularization parame-ter was tuned on our development set.5 Experimental ResultsWe divided our data randomly into 5550 traininginstances, 1903 development instances, and 1939test instances.
Each instance was a single submis-sion from one user, consisting of a batch of ?si-multaneous?
guesses on a macaronic sentence.We noted qualitatively that when a large num-ber of English words have been revealed, particu-larly content words, the users tend to make betterguesses.
Conversely, when most context is Ger-man, we unsuprisingly see the user leave manyguesses blank and make other guesses based onstring similarity triggers.
Such submissions aredifficult to predict as different users will come upwith a wide variety of guesses; our model there-fore resorts to predicting similar-sounding words.For detailed examples of this see Appendix A.6To speed up training, SGD was parallelized using Rechtet al?s (2011) Hogwild!
algorithm.
We trained for 8 epochs.1864ModelRecall at k(dev)Recall at k(test)1 25 50 1 25 50Basic 15.24 34.26 38.08 16.14 35.56 40.30User-Adapted 15.33 34.40 38.67 16.45 35.71 40.57Table 1: Percentage of foreign words for which the user?s ac-tual guess appears in our top-k list of predictions, for modelswith and without user-specific features (k ?
{1, 25, 50}).For each foreign word fiin a submission withi /?
Obs, our inference method (section 4.2) pre-dicts a marginal probability distribution over auser?s guesses e?i.
Table 1 shows our ability to pre-dict user guesses.7Recall that this task is essen-tially a structured prediction task that does joint4919-way classification of each German word.Roughly 1/3 of the time, our model?s top 25 wordsinclude the user?s exact guess.However, the recall reported in Table 1 is toostringent for our educational application.
Wecould give the model partial credit for predicting asynonym of the learner?s guess e?.
More precisely,we would like to give the model partial credit forpredicting when the learner will make a poor guessof the truth e?
?even if the model does not predictthe user?s specific incorrect guess e?.To get at this question, we use English word em-beddings (as in section 3.4) as a proxy for the se-mantics and morphology of the words.
We mea-sure the actual quality of the learner?s guess e?as its cosine similarity to the truth, sim(e?, e?
).While quality of 1 is an exact match, and qual-ity scores > 0.75 are consistently good matches,we found quality of ?
0.6 also reasonable.
Pairssuch as (mosque, islamic) and (politics,government) are examples from the collecteddata with quality ?
0.6.
As quality becomes< 0.4, however, the relationship becomes tenuous,e.g., (refugee, soil).Similarly, we measure the predicted quality assim(e, e?
), where e is the model?s 1-best predic-tion of the user?s guess.
Figure 4 plots predictedvs.
actual quality (each point represents one ofthe learner?s guesses on development data), ob-taining a correlation of 0.38, which we call the?quality correlation?
or QC.
A clear diagonal bandcan be seen, corresponding to the instances where7Throughout this section, we ignore the 5.2% of tokens onwhich the user did not guess (i.e., the guess was <BLANK>after the normalization of section 3.5).
Our present modelsimply treats <BLANK> as an ordinary and very bland word(section 4.2), rather than truly attempting to predict when theuser will not guess.
Indeed, the model?s posterior probabilityof <BLANK> in these cases is a paltry 0.0000267 on average(versus 0.0000106 when the user does guess).
See section 6.Figure 4: Actual quality sim(e?, e?)
of the learner?s guess e?
ondevelopment data, versus predicted quality sim(e, e?)
wheree is the basic model?s 1-best prediction.Figure 5: Actual quality sim(e?, e?)
of the learner?s guess e?on development data, versus the expectation of the predictedquality sim(e, e?)
where e is distributed according to the ba-sic model?s posterior.the model exactly predicts the user?s guess.
Thecloud around the diagonal is formed by instanceswhere the model?s prediction was not identical tothe user?s guess but had similar quality.We also consider the expected predicted qual-ity, averaging over the model?s predictions e ofe?
(for all e ?
Ve) in proportion to the probabili-ties that it assigns them.
This allows the model tomore smoothly assess whether the learner is likelyto make a high-quality guess.
Figure 5 shows thisversion, where the points tend to shift upward andthe quality correlation (QC) rises to 0.53.All QC values are given in Table 2.
We used ex-pected QC on the development set as the criterionfor selecting the regularization coefficient ?
and asthe early stopping criterion during training.1865ModelDev TestExp 1-Best Exp 1-BestBasic 0.525 0.379 0.543 0.411User-Adapted 0.527 0.427 0.544 0.439Table 2: Quality correlations: basic and user-adapted models.Feature RemovedQCExpected 1-BestNone 0.522 0.425Cognate 0.516 0.366?Context 0.510 0.366?History 0.499?0.259?Table 3: Impact on quality correlation (QC) of removingfeatures from the model.
Ablated QC values marked withasterisk?differ significantly from the full-model QC valuesin the first row (p < 0.05, using the test of Preacher (2002)).5.1 Feature AblationTo test the usefulness of different features, wetrained our model with various feature categoriesdisabled.
To speed up experimentation, we sam-pled 1000 instances from the training set andtrained our model on those.
The resulting QC val-ues on dev data are shown in Table 3.
We see thatremoving history-based features has the most sig-nificant impact on model performance: both QCmeasures drop relative to the full model.
For cog-nate and context features, we see no significant im-pact on the expected QC, but a significant drop inthe 1-best QC, especially for context features.5.2 Analysis of User AdaptationTable 2 shows that the user-specific features sig-nificantly improve the 1-best QC of our model, al-though the much smaller improvement in expectedQC is insignificant.User adaptation allows us to discern differ-ent styles of incidental comprehension.
A user-adapted model makes fine-grained predictions thatcould help to construct better macaronic sentencesfor a given user.
Each user who completed atleast 10 HITs has their user-specific weight vec-tor shown as a row in Figure 6.
Recall that theuser-specific weights are not used in isolation, butare added to backoff weights shared by all users.These user-specific weight vectors cluster intofour groups.
Furthermore, the average points perHIT differ by cluster (significantly between eachcluster pair), reflecting the success of differentstrategies.8Users in group (a) employ a generalist8Recall that in our data collection process, we awardpoints for each HIT (section 3.4).
While the points were de-signed more as a reward than as an evaluation of learner suc-cess, a higher score does reflect more guesses that were cor-Figure 6: The user-specific weight vectors, clustered intogroups.
Average points per HIT for the HITs completed byeach group: (a) 45, (b) 48, (c) 50 and (d) 42.strategy for incidental comprehension.
They paytypical or greater-than-typical attention to all fea-tures of the current HIT, but many of them havediminished memory for vocabulary learned dur-ing past HITs (the hist+ feature).
Users in group(b) seem to use the opposite strategy, derivingtheir success from retaining common vocabularyacross HITs (hist+) and falling back on orthogra-phy for new words.
Group (c) users, who earnedthe most points per HIT, appear to make heavyuse of context and pronunciation features togetherwith hist+.
We also see that pronunciation sim-ilarity seems to be a stronger feature for group(c) users, in contrast to the more superficial ortho-graphic similarity.
Group (d), which earned thefewest points per HIT, appears to be an ?extreme?version of group (b): these users pay unusually lit-tle attention to any model features other than or-thographic similarity and hist+.
(More precisely,the model finds group (d)?s guesses harder to pre-dict on the basis of the available features, and sogives a more uniform distribution over Ve.
)6 Future Improvements to the ModelOur model?s feature set (section 4.1) could clearlybe refined and extended.
Indeed, in a separate pa-per (Knowles et al, 2016), we use a more tightlycontrolled experimental design to explore somesimple feature variants.
A cheap way to vet fea-tures would be to test whether they help on thetask of modeling reference translations, which arerect or close, while a lower score indicates that some wordswere never guessed before the system revealed them as clues.1866more plentiful and less noisy than the user guesses.For Cognate features, there exist many othergood string similarity metrics (including trainableones).
We could also include ?effeatures that con-sider whether ei?s part of speech, frequency, andlength are plausible given fi?s burstiness, observedfrequency, and length.
(E.g., only short commonwords are plausibly translated as determiners.
)For Context features, we could design versionsthat are more sensitive to the position and status ofthe context word j.
We speculate that the actual in-fluence of ejon a user?s guess eiis stronger whenejis observed rather than itself guessed; whenthere are fewer intervening tokens (and particu-larly fewer observed ones); and when j < i. Or-thogonally, ?ef(ei, ej) could go beyond PMI andwindowed PMI to also consider cosine similarity,as well as variants of these metrics that are thresh-olded or nonlinearly transformed.
Finally, we donot have to treat the context positions j as indepen-dent multiplicative influences as in equation (1)(cf.
Naive Bayes): we could instead use a topicmodel or some form of language model to deter-mine a conditional probability distribution overEigiven all other words in the context.An obvious gap in our current feature set is thatwe have no ?efeatures to capture that some wordsei?
Veare more likely guesses a priori.
By defin-ing several versions of this feature, based on fre-quencies in corpora of different reading levels, wecould learn user-specific weights modeling whichusers are unlikely to think of an obscure word.We should also include features that fire specifi-cally on the reference translation e?iand the specialsymbols <BLANK> and <COPY>, as each is muchmore likely than the other features would suggest.For History features, we could consider nega-tive feedback from other HITs (not just the currentHIT), as well as positive information provided byrevealed clues (not just confirmed guesses).
Wecould also devise non-binary versions in whichmore recent or more frequent feedback on a wordhas a stronger effect.
More ambitiously, wecould model generalization: after being shownthat Kind means child, a learner might increasethe probability that the similar word Kindermeans child or something related (children,childish, .
.
.
), whether because of superficialorthographic similarity or a deeper understandingof the morphology.
Similarly, a learner mightgradually acquire a model of typical spellingchanges in English-German cognate pairs.A more significant extension would be to modela user?s learning process.
Instead of represent-ing each user by a small vector of user-specificweights, we could recognize that the user?s guess-ing strategy and knowledge can change over time.A serious deficiency in our current model (notto mention our evaluation metrics!)
is that wetreat <BLANK> like any other word.
A more at-tractive approach would be to learn a stochasticlink from the posterior distribution to the user?sguess or non-guess, instead of assuming that theuser simply samples the guess from the poste-rior.
As a simple example, we might say the userguesses e ?
Vewith probability p(e)?
?wherep(e) is the posterior probability and ?
> 1 is alearned parameter?with the remaining probabil-ity assigned to <BLANK>.
This says that the usertends to avoid guessing except when there are rel-atively high-probability words to guess.7 ConclusionWe have presented a methodology for collectingdata and training a model to estimate a foreign lan-guage learner?s understanding of L2 vocabulary inpartially understood contexts.
Both are novel con-tributions to the study of L2 acquisition.Our current model is arguably crude, with only6 features, yet it can already often do a reason-able job of predicting what a user might guess andwhether the user?s guess will be roughly correct.This opens the door to a number of future direc-tions with applications to language acquisition us-ing personalized content and learners?
knowledge.We plan a deeper investigation into how learn-ers detect and combine cues for incidental com-prehension.
We also leave as future work the in-tegration of this model into an adaptive systemthat tracks learner understanding and creates scaf-folded content that falls in their zone of prox-imal development, keeping them engaged whilestretching their understanding.AcknowledgmentsThis work was supported by a seed grant fromthe Science of Learning Institute at Johns HopkinsUniversity, and also by a National Science Foun-dation Graduate Research Fellowship (Grant No.DGE-1232825) to the second author.
We thankChadia Abras, Adam Teichert, and Sanjeev Khu-danpur for helpful discussions and suggestions.1867ReferencesOnd?rej Bojar, Rajen Chatterjee, Christian Feder-mann, Barry Haddow, Matthias Huck, ChrisHokamp, Philipp Koehn, Varvara Logacheva,Christof Monz, Matteo Negri, Matt Post, Car-olina Scarton, Lucia Specia, and Marco Turchi.Findings of the 2015 Workshop on StatisticalMachine Translation.
In Proceedings of theTenth Workshop on Statistical Machine Trans-lation, pages 1?46, 2015.Jill Burstein, Joel Tetreault, and Nitin Madnani.The e-rater automated essay scoring system.
InMark D. Shermis, editor, Handbook of Auto-mated Essay Evaluation: Current Applicationsand New Directions, pages 55?67.
Routledge,2013.Hal Daum?e III.
Frustratingly easy domain adap-tation.
In Proceedings of ACL, pages 256?263,June 2007.Markus Dreyer and Jason Eisner.
Graphical mod-els over multiple strings.
In Proceedings ofEMNLP, pages 101?110, Singapore, August2009.Michael Heilman and Nitin Madnani.
ETS: Dis-criminative edit models for paraphrase scoring.In Joint Proceedings of *SEM and SemEval,pages 529?535, June 2012.Rebecca Knowles, Adithya Renduchintala,Philipp Koehn, and Jason Eisner.
Analyzinglearner understanding of novel L2 vocabulary.2016.
To appear.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen,Christine Moran, Richard Zens, Chris Dyer,Ondrej Bojar, Alexandra Constantin, andEvan Herbst.
Moses: Open source toolkit forstatistical machine translation.
In Proceedingsof ACL: Short Papers, pages 177?180, 2007.Stephen Krashen.
We acquire vocabulary andspelling by reading: Additional evidence for theinput hypothesis.
The Modern Language Jour-nal, 73(4):440?464, 1989.Igor Labutov and Hod Lipson.
Generating code-switched text for lexical learning.
In Proceed-ings of ACL, pages 562?571, 2014.Diane Litman.
Natural language processing forenhancing teaching and learning.
In Proceed-ings of AAAI, 2016.Nitin Madnani, Michael Heilman, Joel Tetreault,and Martin Chodorow.
Identifying high-levelorganizational elements in argumentative dis-course.
In Proceedings of NAACL-HLT, pages20?28, 2012.Kevin P. Murphy, Yair Weiss, and Michael I. Jor-dan.
Loopy belief propagation for approximateinference: An empirical study.
In Proceedingsof UAI, pages 467?475, 1999.G?ozde?Ozbal, Daniele Pighin, and Carlo Strappa-rava.
Automation and evaluation of the key-word method for second language learning.
InProceedings of ACL (Volume 2: Short Papers),pages 352?357, 2014.Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
GLoVe: Global vectorsfor word representation.
In Proceedings ofEMNLP, volume 14, pages 1532?1543, 2014.K.
J.
Preacher.
Calculation for the test of the differ-ence between two independent correlation coef-ficients [computer software], May 2002.Benjamin Recht, Christopher Re, Stephen Wright,and Feng Niu.
Hogwild!
: A lock-free ap-proach to parallelizing stochastic gradient de-scent.
In Advances in Neural Information Pro-cessing Systems, pages 693?701, 2011.Adithya Renduchintala, Rebecca Knowles,Philipp Koehn, and Jason Eisner.
Creatinginteractive macaronic interfaces for languagelearning.
In Proceedings of ACL (SystemDemonstrations), 2016.Lev Vygotski??.
Thought and Language (Revisedand Expanded Edition).
MIT Press, 2012.R.
Weide.
The CMU pronunciation dictionary, re-lease 0.6, 1998.David Wood, Jerome S. Bruner, and Gail Ross.The role of tutoring in problem solving.
Jour-nal of Child Psychology and Psychiatry, 17(2):89?100, 1976.1868AppendicesA Example of Learner Guesses vs. Model PredictionsTo give a sense of the problem difficulty, we have hand-picked and presented two training examples(submissions) along with the predictions of our basic model and their log-probabilities.
In Figure 7a alarge portion of the sentence has been revealed to the user in English (blue text) only 2 words are inGerman.
The text in bold font is the user?s guess.
Our model expected both words to be guessed; thepredictions are listed below the German words Verschiedene and Regierungen.
The referencetranslation for the 2 words are Various and governments.
In Figure 7b we see a much hardercontext where only one word is shown in English and this word is not particularly helpful as a contextualanchor.
(a)(b)Figure 7: Two examples of the system?s predictions of what the user will guess on a single submission, contrasted with theuser?s actual guess.
(The user?s previous submissions on the same task instance are not shown.)
In 7a, the model correctlyexpects that the substantial context will inform the user?s guess.
In 7b, the model predicts that the user will fall back onstring similarity?although we can see that the user?s actual guess of and day was likely informed by their guess of night,an influence that our CRF did consider.
The numbers shown are log-probabilities.
Both examples show the sentences in amacaronic state (after some reordering or translation has occurred).
For example, the original text of the German sentence in 7breads Deshalb durften die Paare nur noch ein Kind bekommen .
The macaronic version has undergonesome reordering, and has also erroneously dropped the verb due to an incorrect alignment.1869
