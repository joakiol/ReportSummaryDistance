Proceedings of ACL-08: HLT, pages 798?806,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSemantic Role Labeling Systems for Arabic using Kernel MethodsMona DiabCCLS, Columbia UniversityNew York, NY 10115, USAmdiab@ccls.columbia.eduAlessandro MoschittiDISI, University of TrentoTrento, I-38100, Italymoschitti@disi.unitn.itDaniele PighinFBK-irst; DISI, University of TrentoTrento, I-38100, Italypighin@fbk.euAbstractThere is a widely held belief in the natural lan-guage and computational linguistics commu-nities that Semantic Role Labeling (SRL) isa significant step toward improving importantapplications, e.g.
question answering and in-formation extraction.
In this paper, we presentan SRL system for Modern Standard Arabicthat exploits many aspects of the rich mor-phological features of the language.
The ex-periments on the pilot Arabic Propbank datashow that our system based on Support VectorMachines and Kernel Methods yields a globalSRL F1 score of 82.17%, which improves thecurrent state-of-the-art in Arabic SRL.1 IntroductionShallow approaches to semantic processing are mak-ing large strides in the direction of efficiently andeffectively deriving tacit semantic information fromtext.
Semantic Role Labeling (SRL) is one such ap-proach.
With the advent of faster and more power-ful computers, more effective machine learning al-gorithms, and importantly, large data resources an-notated with relevant levels of semantic information,such as the FrameNet (Baker et al, 1998) and Prob-Bank (Kingsbury and Palmer, 2003), we are seeinga surge in efficient approaches to SRL (Carreras andMa`rquez, 2005).SRL is the process by which predicates and theirarguments are identified and their roles are definedin a sentence.
For example, in the English sen-tence, ?John likes apples.
?, the predicate is ?likes?whereas ?John?
and ?apples?, bear the semantic rolelabels agent (ARG0) and theme (ARG1).
The cru-cial fact about semantic roles is that regardless ofthe overt syntactic structure variation, the underly-ing predicates remain the same.
Hence, for the sen-tence ?John opened the door?
and ?the door opened?,though ?the door?
is the object of the first sentenceand the subject of the second, it is the ?theme?
inboth sentences.
Same idea applies to passive con-structions, for example.There is a widely held belief in the NLP and com-putational linguistics communities that identifyingand defining roles of predicate arguments in a sen-tence has a lot of potential for and is a significantstep toward improving important applications suchas document retrieval, machine translation, questionanswering and information extraction (Moschitti etal., 2007).To date, most of the reported SRL systems are forEnglish, and most of the data resources exist for En-glish.
We do see some headway for other languagessuch as German and Chinese (Erk and Pado, 2006;Sun and Jurafsky, 2004).
The systems for the otherlanguages follow the successful models devised forEnglish, e.g.
(Gildea and Jurafsky, 2002; Gildea andPalmer, 2002; Chen and Rambow, 2003; Thompsonet al, 2003; Pradhan et al, 2003; Moschitti, 2004;Xue and Palmer, 2004; Haghighi et al, 2005).
In thesame spirit and facilitated by the release of the Se-mEval 2007 Task 18 data1, based on the Pilot ArabicPropbank, a preliminary SRL system exists for Ara-bic2 (Diab and Moschitti, 2007; Diab et al, 2007a).However, it did not exploit some special character-istics of the Arabic language on the SRL task.In this paper, we present an SRL system for MSAthat exploits many aspects of the rich morphologicalfeatures of the language.
It is based on a supervisedmodel that uses support vector machines (SVM)technology (Vapnik, 1998) for argument boundarydetection and argument classification.
It is trainedand tested using the pilot Arabic Propbank data re-leased as part of the SemEval 2007 data.
Given thelack of a reliable Arabic deep syntactic parser, we1http://nlp.cs.swarthmore.edu/semeval/2We use Arabic to refer to Modern Standard Arabic (MSA).798use gold standard trees from the Arabic Tree Bank(ATB) (Maamouri et al, 2004).This paper is laid out as follows: Section 2presents facts about the Arabic language especiallyin relevant contrast to English; Section 3 presentsthe approach and system adopted for this work; Sec-tion 4 presents the experimental setup, results anddiscussion.
Finally, Section 5 draws our conclu-sions.2 Arabic Language and Impact on SRLArabic is a very different language from English inseveral respects relevant to the SRL task.
Arabic is asemitic language.
It is known for its templatic mor-phology where words are made up of roots and af-fixes.
Clitics agglutinate to words.
Clitics includeprepositions, conjunctions, and pronouns.In contrast to English, Arabic exhibits rich mor-phology.
Similar to English, Arabic verbs explic-itly encode tense, voice, Number, and Person fea-tures.
Additionally, Arabic encodes verbs with Gen-der, Mood (subjunctive, indicative and jussive) in-formation.
For nominals (nouns, adjectives, propernames), Arabic encodes syntactic Case (accusative,genitive and nominative), Number, Gender and Def-initeness features.
In general, many of the morpho-logical features of the language are expressed viashort vowels also known as diacritics3 .Unlike English, syntactically Arabic is a pro-droplanguage, where the subject of a verb may be im-plicitly encoded in the verb morphology.
Hence, weobserve sentences such as ?A?KQ.
?
@ ??
@ Akl AlbrtqAl?ate-[he] the-oranges?, where the verb Akl encodesthe third Person Masculine Singular subject in theverbal morphology.
It is worth noting that in theATB 35% of all sentences are pro-dropped for sub-ject (Maamouri et al, 2006).
Unless the syntacticparse is very accurate in identifying the pro-droppedcase, identifying the syntactic subject and the under-lying semantic arguments are a challenge for suchpro-drop cases.Arabic syntax exhibits relative free word order.Arabic allows for both subject-verb-object (SVO)and verb-subject-object (VSO) argument orders.4 In3Diacritics encode the vocalic structure, namely the shortvowels, as well as the gemmination marker for consonantal dou-bling, among other markers.4MSA less often allows for OSV, or OVS.the VSO constructions, the verb agrees with the syn-tactic subject in Gender only, while in the SVO con-structions, the verb agrees with the subject in bothNumber and Gender.
Even though, in the ATB, anequal distribution of both VSO and SVO is observed(each appearing 30% of the time), it is known thatin general Arabic is predominantly in VSO order.Moreover, the pro-drop cases could effectively beperceived as VSO orders for the purposes of SRL.Syntactic Case is very important in the cases of VSOand pro-drop constructions as they indicate the syn-tactic roles of the object arguments with accusativeCase.
Unless the morphology of syntactic Case isexplicitly present, such free word order could runthe SRL system into significant confusion for manyof the predicates where both arguments are semanti-cally of the same type.Arabic exhibits more complex noun phrases thanEnglish mainly to express possession.
These con-structions are known as idafa constructions.
Mod-ern standard Arabic does not have a special parti-cle expressing possession.
In these complex struc-tures a surface indefinite noun (missing an explicitdefinite article) may be followed by a definite nounmarked with genitive Case, rendering the first nounsyntactically definite.
For example, IJ.
?
@ ?g.
P rjlAlbyt ?man the-house?
meaning ?man of the house?,?g.
P becomes definite.
An adjective modifying thenoun ?g.
P will have to agree with it in Number,Gender, Definiteness, and Case.
However, with-out explicit morphological encoding of these agree-ments, the scope of the arguments would be con-fusing to an SRL system.
In a sentence such as?K???
@ IJ.
?
@ ?g.
P rjlu Albyti AlTwylu meaning ?thetall man of the house?
: ?man?
is definite, masculine,singular, nominative, corresponding to Definiteness,Gender, Number and Case, respectively; ?the-house?is definite, masculine, singular, genitive; ?the-tall?
isdefinite, masculine, singular, nominative.
We notethat ?man?
and ?tall?
agree in Number, Gender, Caseand Definiteness.
Syntactic Case is marked usingshort vowels u, and i at the end of the word.
Hence,rjlu and AlTwylu agree in their Case ending5 With-out the explicit marking of the Case information,5The presence of the Albyti is crucial as it renders rjlu defi-nite therefore allowing the agreement with AlTwylu to be com-plete.799SVPVBDpredicate@YK.startedNPARG0NPNN?KPpresidentNPNNZ @P 	P??@ministersJJ??J??@ChineseNPNNP?
PZhuNNP?m. 	'?PRongjiNPARG1NPNN?PAKPvisitJJ?J??
?PofficialPPIN?toNPNNPY	J??
@IndiaNPARGM?TMPNPNNYgB@SundayJJ???A??
@pastFigure 1: Annotated Arabic Tree corresponding to ?Chinese Prime minister Zhu Rongjy started an official visit to India last Sunday.
?namely in the word endings, it could be equally validthat ?the-tall?
modifies ?the-house?
since they agreein Number, Gender and Definiteness as explicitlymarked by the Definiteness article Al.
Hence, theseidafa constructions could be tricky for SRL in theabsence of explicit morphological features.
This iscompounded by the general absence of short vowels,expressed by diacritics (i.e.
the u and i in rjlu and Al-byti,) in naturally occurring text.
Idafa constructionsin the ATB exhibit recursive structure, embeddingother NPs, compared to English where possession isannotated with flat NPs and is designated by a pos-sessive marker.Arabic texts are underspecified for diacritics todifferent degrees depending on the genre of thetext (Diab et al, 2007b).
Such an underspecifica-tion of diacritics masks some of the very relevantmorpho-syntactic interactions between the differentcategories such as agreement between nominals andtheir modifiers as exemplified before, or verbs andtheir subjects.Having highlighted the differences, we hypothe-size that the interaction between the rich morphol-ogy (if explicitly marked and present) and syntaxcould help with the SRL task.
The presence of ex-plicit Number and Gender agreement as well as Caseinformation aids with identification of the syntacticsubject and object even if the word order is relativelyfree.
Gender, Number, Definiteness and Case agree-ment between nouns and their modifiers and othernominals, should give clues to the scope of argu-ments as well as their classes.
The presence of suchmorpho-syntactic information should lead to betterargument boundary detection and better classifica-tion.3 An SRL system for ArabicThe previous section suggests that an optimal modelshould take into account specific characteristics ofFeature Name DescriptionPredicate Lemmatization of the predicate wordPath Syntactic path linking the predicate andan argument, e.g.
NN?NP?VP?VBXPartial path Path feature limited to the branching ofthe argumentNo-direction path Like Path without traversal directionsPhrase type Syntactic type of the argument nodePosition Relative position of the argument withrespect to the predicateVerb subcategorization Production rule expanding the predicateparent nodeSyntactic Frame Position of the NPs surrounding thepredicateFirst and last word/POS First and last words and POS tags ofcandidate argument phrasesTable 1: Standard linguistic features employed by most SRL systems.Arabic.
In this research, we go beyond the previ-ously proposed basic SRL system for Arabic (Diabet al, 2007a; Diab and Moschitti, 2007).
We exploitthe full morphological potential of the language toverify our hypothesis that taking advantage of theinteraction between morphology and syntax can im-prove on a basic SRL system for morphologicallyrich languages.Similar to the previous Arabic SRL systems, ouradopted SRL models use Support Vector Machinesto implement a two step classification approach,i.e.
boundary detection and argument classifica-tion.
Such models have already been investigatedin (Pradhan et al, 2005; Moschitti et al, 2005).
Thetwo step classification description is as follows.3.1 Predicate Argument ExtractionThe extraction of predicative structures is based onthe sentence level.
Given a sentence, its predicates,as indicated by verbs, have to be identified alongwith their arguments.
This problem is usually di-vided in two subtasks: (a) the detection of the targetargument boundaries, i.e.
the span of the argumentwords in the sentence, and (b) the classification ofthe argument type, e.g.
Arg0 or ArgM for Propbank800SNPNNPMaryVPVBDboughtNPDaNcat?VPVBDboughtNPDaNcatVPVBD NPDaNcatVPVBDboughtNPD NcatVPVBDboughtNPD NVPVBDboughtNPNPDaNcatNPNNPMaryNNPMaryVBDboughtDaNcat .
.
.Figure 2: Fragment space generated by a tree kernel function for the sentence Mary bought a cat.or Agent and Goal for the FrameNet.The standard approach to learn both the detectionand the classification of predicate arguments is sum-marized by the following steps:(a) Given a sentence from the training-set, generatea full syntactic parse-tree;(b) let P and A be the set of predicates and the setof parse-tree nodes (i.e.
the potential arguments), re-spectively;(c) for each pair ?p, a?
?
P ?A: extract the featurerepresentation set, Fp,a and put it in T+ (positive ex-amples) if the subtree rooted in a covers exactly thewords of one argument of p, otherwise put it in T?
(negative examples).For instance, in Figure 1, for each combination ofthe predicate started with the nodes NP, S, VP, VPD,NNP, NN, PP, JJ or IN the instances Fstarted,a aregenerated.
In case the node a exactly covers ?presi-dent ministers Chinese Zhu Rongji?
or ?visit officialto India?, Fp,a will be a positive instance otherwiseit will be a negative one, e.g.
Fstarted,IN .The T+ and T?
sets are used to train the bound-ary classifier.
To train the multi-class classifier, T+can be reorganized as positive T+argi and negativeT?argi examples for each argument i.
This way, an in-dividual ONE-vs-ALL classifier for each argument ican be trained.
We adopt this solution, accordingto (Pradhan et al, 2005), since it is simple and ef-fective.
In the classification phase, given an unseensentence, all its Fp,a are generated and classified byeach individual classifier Ci.
The argument associ-ated with the maximum among the scores providedby the individual classifiers is eventually selected.The above approach assigns labels independently,without considering the whole predicate argumentstructure.
As a consequence, the classifier outputmay generate overlapping arguments.
Thus, to makethe annotations globally consistent, we apply a dis-ambiguating heuristic adopted from (Diab and Mos-chitti, 2007) that selects only one argument amongmultiple overlapping arguments.3.2 FeaturesThe discovery of relevant features is, as usual, acomplex task.
The choice of features is further com-pounded for a language such as Arabic given its richmorphology and morpho-syntactic interactions.To date, there is a common consensus on the set ofbasic standard features for SRL, which we will referto as standard.
The set of standard features, refers tounstructured information derived from parse trees.e.g.
Phrase Type, Predicate Word or Head Word.Typically the standard features are language inde-pendent.
In our experiments we employ the featureslisted in Table 1, defined in (Gildea and Jurafsky,2002; Pradhan et al, 2005; Xue and Palmer, 2004).For example, the Phrase Type indicates the syntac-tic type of the phrase labeled as a predicate argu-ment, e.g.
NP for ARG1 in Figure 1.
The Parse TreePath contains the path in the parse tree between thepredicate and the argument phrase, expressed as asequence of nonterminal labels linked by direction(up or down) symbols, e.g.
VBD ?
VP ?
NP forARG1 in Figure 1.
The Predicate Word is the surfaceform of the verbal predicate, e.g.
started for all argu-ments.
The standard features, as successful as theyare, are designed primarily for English.
They are notexploiting the different characteristics of the Arabiclanguage as expressed through morphology.
Hence,we explicitly encode new SRL features that capturethe richness of Arabic morphology and its role inmorpho-syntactic behavior.
The set of morphologi-cal attributes include: inflectional morphology suchas Number, Gender, Definiteness, Mood, Case, Per-son; derivational morphology such as the Lemmaform of the words with all the diacritics explicitlymarked; vowelized and fully diacritized form of thesurface form; the English gloss6.
It is worth notingthat there exists highly accurate morphological tag-gers for Arabic such as the MADA system (Habashand Rambow, 2005; Roth et al, 2008).
MADA tags6The gloss is not sense disambiguated, hence they includehomonyms.801Feature Name DescriptionDefiniteness Applies to nominals, values are definite, indefinite or inapplicableNumber Applies to nominals and verbs, values are singular, plural or dual or inapplicableGender Applies to nominals, values are feminine, masculine or inapplicableCase Applies to nominals, values are accusative, genitive, nominative or inapplicableMood Applies to verbs, values are subjunctive, indicative, jussive or inapplicablePerson Applies to verbs and pronouns, values are 1st, 2nd, 3rd person or inapplicableLemma The citation form of the word fully diacritized with the short vowels and gemmination markers if applicableGloss this is the corresponding English meaning as rendered by the underlying lexicon.Vocalized word The surface form of the word with all the relevant diacritics.
Unlike Lemma, it includes all the inflections.Unvowelized word The naturally occurring form of the word in the sentence with no diacritics.Table 2: Rich morphological features encoded in the Extended Argument Structure Tree (EAST).modern standard Arabic with all the relevant mor-phological features as well as it produces highly ac-curate lemma and gloss information by tapping intoan underlying morphological lexicon.
A list of theextended features is described in Table 2.The set of possible features and their combina-tions are very large leading to an intractable fea-ture selection problem.
Therefore, we exploit wellknown kernel methods, namely tree kernels, to ro-bustly experiment with all the features simultane-ously.
Such kernel engineering, as shown in (Mos-chitti, 2004), allows us to experiment with manysyntactic/semantic features seamlessly.3.3 Engineering Arabic Features with KernelMethodsFeature engineering via kernel methods is a usefultechnique that allows us to save a lot of time in thedesign and implementation of features.
The basicidea is (a) to design a set of basic value-attributefeatures and apply polynomial kernels and generateall possible combinations; or (b) to design basic treestructures expressing properties related to the targetlinguistic objects and use tree kernels to generateall possible tree subparts, which will constitute thefeature representation vectors for the learning algo-rithm.Tree kernels evaluate the similarity between twotrees in terms of their overlap, generally measuredas the number of common substructures (Collinsand Duffy, 2002).
For example, Figure 2, showsa small parse tree and some of its fragments.
Todesign a function which computes the number ofcommon substructures between two trees t1 and t2,let us define the set of fragments F={f1, f2, ..} andthe indicator function Ii(n), equal to 1 if the tar-get fi is rooted at node n and 0 otherwise.
A treekernel function KT (?)
over two trees is defined as:VPVBD@YK.NPNPNN?KPNPNNZ @P 	P??@JJ??J??@NPNNP?
PNNP?m. 	'?PFigure 3: Example of the positive AST structured feature encodingthe argument ARG0 in the sentence depicted in Figure 1.KT (t1, t2) =?n1?Nt1?n2?Nt2 ?
(n1, n2), whereNt1 and Nt2 are the sets of nodes of t1 and t2, re-spectively.
The function ?(?)
evaluates the num-ber of common fragments rooted in n1 and n2, i.e.?
(n1, n2) =?|F|i=1 Ii(n1)Ii(n2).
?
can be ef-ficiently computed with the algorithm proposed in(Collins and Duffy, 2002).3.4 Structural Features for ArabicIn order to incorporate the characteristically richArabic morphology features structurally in the treerepresentations, we convert the features into value-attribute pairs at the leaf node level of the tree.
Fig1 illustrates the morphologically underspecified treewith some of the morphological features encoded inthe POS tag such as VBD indicating past tense.
Thiscontrasts with Fig.
4 which shows an excerpt of thesame tree encoding the chosen relevant morpholog-ical features.For the sake of classification, we will be dealingwith two kinds of structures: the Argument StructureTree (AST) (Pighin and Basili, 2006) and the Ex-tended Argument Structure Tree (EAST).
The ASTis defined as the minimal subtree encompassing alland only the leaf nodes encoding words belongingto the predicate or one of its arguments.
An ASTexample is shown in Figure 3.
The EAST is thecorresponding structure in which all the leaf nodeshave been extended with the ten morphological fea-802VPVBDFEATGenderMASCFEATNumberSFEATPerson3FEATLemmabada>-aFEATGlossstart/begin+he/itFEATVocalbada>aFEATUnVocalbd>NPNPNNFEATDefiniteDEFFEATGenderMASCFEATNumberSFEATCaseGENFEATLemmara}iysFEATGlosspresident/head/chairmanFEATVocalra}iysiNP.
.
.NP.
.
.Figure 4: An excerpt of the EAST corresponding to the AST shown in Figure 3, with attribute-value extended morphological features representedas leaf nodes.tures described in Table 2, forming a vector of 10preterminal-terminal node pairs that replace the sur-face of the leaf.
The resulting EAST structure isshown in Figure 4.Not all the features are instantiated for all the leafnode words.
Due to space limitations, in the fig-ure we did not include the Features that have NULLvalues.
For instance, Definiteness is always asso-ciated with nominals, hence the verb@YK.
bd?
?start?is assigned a NULL value for the Definite feature.Verbs exhibit Gender information depending on in-flections.
For our example,@YK.
?started?
is inflectedfor masculine Gender, singular Number, third per-son.
On the other hand, the noun Z @P 	P??
@ is definiteand is assigned genitive Case since it is in a posses-sive, idafa, construction.The features encoded by the EAST can providevery useful hints for boundary and role classifica-tion.
Considering Figure 1, argument boundaries isnot as straight forward to identify as there are sev-eral NPs.
Assuming that the inner most NP ?minis-ters the-Chinese?
is a valid Argument could poten-tially be accepted.
There is ample evidence that anyNN followed by a JJ would make a perfectly validArgument.
However, an AST structure would maskthe fact that the JJ ?the-Chinese?
does not modify theNN ?ministers?
since they do not agree in Number7,and in syntactic Case, where the latter is genitive andthe former is nominative.
?the-Chinese?
in fact mod-ifies ?president?
as they agree on all the underlyingmorphological features.
Conversely, the EAST inFigure 4 explicitly encodes this agreement includ-ing an agreement on Definiteness.
It is worth notingthat just observing the Arabic word ?KP ?president?in Fig 1, the system would assume that it is an indef-inite word since it does not include the definite arti-7The POS tag on this node is NN as broken plural, however,the underlying morphological feature Number is plural.cle ?@.
Therefore, the system could be lead astray toconclude that ?the-Chinese?
does not modify ?pres-ident?
but rather ?the-ministers?.
Without knowingthe Case information and the agreement features be-tween the verb@YK.
?started?
and the two nouns head-ing the two main NPs in our tree, the syntactic sub-ject can be either ?PAKP ?visit?
or ?KP ?president?
inFigure 1.
The EAST is more effective in identifyingthe first noun as the syntactic subject and the secondas the object since the morphological information in-dicates that they are in nominative and accusativeCase, respectively.
Also the agreement in Genderand Number between the verb and the syntactic sub-ject is identified in the enriched tree.
We see that @YK.?started?
and ?KP ?president?
agree in being singu-lar and masculine.
If ?PAKP ?visit?
were the syntacticsubject, we would have seen the verb inflected asH@YK.
?started-FEM?
with a feminine inflection to re-flect the verb-subject agreement on Gender.
Hencethese agreement features should help with the clas-sification task.4 ExperimentsIn these experiments we investigate (a) if the tech-nology proposed in previous work for automaticSRL of English texts is suitable for Arabic SRLsystems, and (b) the impact of tree kernels usingnew tree structures on Arabic SRL.
For this purpose,we test our models on the two individual phasesof the traditional 2-stage SRL model (i.e.
bound-ary detection and argument classification) and onthe complete SRL task.
We use three different fea-ture spaces: a set of standard attribute-value featuresand the AST and the EAST structures defined in3.4.
Standard feature vectors can be combined witha polynomial kernel (Poly), which, when the de-gree is larger than 1, automatically generates featureconjunctions.
This, as suggested in (Pradhan et al,2005; Moschitti, 2004), can help stressing the differ-803ences between different argument types.
Tree struc-tures can be used in the learning algorithm thanks tothe tree kernels described in Section 3.3.
Moreover,to verify if the above feature sets are equivalent orcomplementary, we can join them by means of addi-tive operation which always produces a valid kernel(Shawe-Taylor and Cristianini, 2004).4.1 Experimental setupWe use the dataset released in the SemEval 2007Task 18 on Arabic Semantic Labeling (Diab et al,2007a).
The data covers the 95 most frequentverbs in the Arabic Treebank III ver.
2 (ATB).The ATB consists of MSA newswire data from theAnnhar newspaper, spanning the months from Julyto November, 2002.
All our experiments are carriedout with gold standard trees.An important characteristic of the dataset isthe use of unvowelized Arabic in the Buckwaltertransliteration scheme for deriving the basic featuresfor the AST experimental condition.
The data com-prises a development set, a test set and a trainingset of 886, 902 and 8,402 sentences, respectively,where each set contain 1725, 1661 and 21,194 argu-ment instances.
These instances are distributed over26 different role types.
The training instances ofthe boundary detection task also include parse-treenodes that do not correspond to correct boundaries(we only considered 350K examples).
For the exper-iments, we use SVM-Light-TK toolkit8 (Moschitti,2004; Moschitti, 2006) and its SVM-Light defaultparameters.
The system performance, i.e.
F1 on sin-gle boundary and role classifier, accuracy of the rolemulti-classifier and the F1 of the complete SRL sys-tems, are computed by means of the CoNLL evalua-tor9.4.2 ResultsFigure 5 reports the F1 of the SVM boundary classi-fier using Polynomial Kernels with a degree from 1to 6 (i.e.
Polyi), the AST and the EAST kernels andtheir combinations.
We note that as we introduceconjunctions, i.e.
a degree larger than 2, the F1 in-creases by more than 3 percentage points.
Thus, notonly are the English features meaningful for Ara-bic but also their combinations are important, reveal-8http://disi.unitn.it/?moschitti9http://www.lsi.upc.es/?srlconll/soft.htmlFigure 5: Impact of polynomial kernel, tree kernels and their combi-nations on boundary detection.Figure 6: Impact of the polynomial kernel, tree kernels and theircombinations on the accuracy in role classification (gold boundaries)and on the F1 of complete SRL task (boundary + role classification).ing that both languages share an underlying syntax-semantics interface.
Moreover, we note that the F1of EAST is higher than the F1 of AST which in turnis higher than the linear kernel (Poly1).
However,when conjunctive features (Poly2-4) are used thesystem accuracy exceeds those of tree kernel mod-els alone.
Further increasing the polynomial degree(Poly5-6) generates very complex hypotheses whichresult in very low accuracy values.Therefore, to improve the polynomial kernel, wesum it to the contribution of AST and/or EAST,obtaining AST+Poly3 (polynomial kernel of degree3), EAST+Poly3 and AST+EAST+Poly3, whose F1scores are also shown in Figure 5.
Such com-bined models improve on the best polynomial ker-nel.
However, not much difference is shown be-tween AST and EAST on boundary detection.
Thisis expected since we are using gold standard trees.We hypothesize that the rich morphological fea-tures will help more with the role classificationtask.
Therefore, we evaluate role classification withgold boundaries.
The curve labeled ?classification?in Figure 6 illustrates the accuracy of the SVMrole multi-classifier according to different kernels.804P3 AST EAST AST+P3EAST+P3AST+EAST+P3P 81.73 80.33 81.7 81.73 82.46 83.08R 78.93 75.98 77.42 80.01 80.67 81.28F1 80.31 78.09 79.51 80.86 81.56 82.17Table 3: F1 of different models on the Arabic SRL task.Again, we note that a degree larger than 1 yieldsa significant improvement of more than 3 percentpoints, suggesting that the design of Arabic SRLsystem based on SVMs requires polynomial kernels.In contrast to the boundary results, EAST highly im-proves over AST (by about 3 percentage points) andproduces an F1 comparable to the best Polynomialkernel.
Moreover, AST+Poly3, EAST+Poly3 andAST+EAST+Poly3 all yield different degrees of im-provement, where the latter model is both the richestin terms of features and the most accurate.These results strongly suggest that: (a) tree ker-nels generate new syntactic features that are usefulfor the classification of Arabic semantic roles; (b)the richer morphology of Arabic language shouldbe exploited effectively to obtain accurate SRL sys-tems; (c) tree kernels appears to be a viable approachto effectively achieve this goal.To illustrate the practical feasibility of our system,we investigate the complete SRL task where boththe boundary detection and argument role classifica-tion are performed automatically.
The curve labeled?boundary + role classification?
in Figure 6 reportsthe F1 of SRL systems based on the previous ker-nels.
The trend of the plot is similar to the gold-standard boundaries case.
The difference amongthe F1 scores of the AST+Poly3, EAST+Poly3 andAST+EAST+Poly3 is slightly reduced.
This maybe attributed to the fact that they produce similarboundary detection results, which in turn, for theglobal SRL outcome, are summed to those of theclassification phase.
Table 3 details the differencesamong the models and shows that the best modelimproves the SRL system based on the polynomialkernel, i.e.
the SRL state-of-the-art for Arabic, byabout 2 percentage points.
This is a very large im-provement for SRL systems (Carreras and Ma`rquez,2005).
These results confirm that the new enrichedstructures along with tree kernels are a promising ap-proach for Arabic SRL systems.Finally, Table 4 reports the F1 of the best model,AST+EAST+Poly3, for individual arguments in theRole Precision Recall F?=1ARG0 96.14% 97.27% 96.70ARG0-STR 100.00% 20.00% 33.33ARG1 88.52% 92.70% 90.57ARG1-STR 33.33% 15.38% 21.05ARG2 69.35% 76.67% 72.82ARG3 66.67% 16.67% 26.67ARGM-ADV 66.98% 61.74% 64.25ARGM-CAU 100.00% 9.09% 16.67ARGM-CND 25.00% 33.33% 28.57ARGM-LOC 67.44% 95.08% 78.91ARGM-MNR 54.00% 49.09% 51.43ARGM-NEG 80.85% 97.44% 88.37ARGM-PRD 20.00% 8.33% 11.76ARGM-PRP 85.71% 66.67% 75.00ARGM-TMP 91.35% 88.79% 90.05Table 4: SRL F1 of the single arguments using theAST+EAST+Poly3 kernel.SRL task.
We note that, as for English SRL, ARG0shows high values (96.70%).
Conversely, ARG1seems more difficult to be classified in Arabic.
TheF1 for ARG1 is only 90.57% compared with 96.70%for ARG0.This may be attributed to the different possi-ble syntactic orders of Arabic consructions confus-ing the syntactic subject with the object especiallywhere there is no clear morphological features onthe arguments to decide either way.5 ConclusionsWe have presented a model for Arabic SRL thatyields a global SRL F1 score of 82.17% by combin-ing rich structured features and traditional attribute-value features derived from English SRL systems.The resulting system significantly improves previ-ously reported results on the same task and dataset.This outcome is very promising given that the avail-able data is small compared to the English data sets.For future work, we would like to explore furtherexplicit morphological features such as aspect tenseand voice as well as richer POS tag sets such as thoseproposed in (Diab, 2007).
Finally, we would like toexperiment with automatic parses and different syn-tactic formalisms such as dependencies and shallowparses.AcknowledgementsMona Diab is partly funded by DARPA Contract No.
HR0011-06-C-0023.
Alessandro Moschitti has been partially funded byCCLS of the Columbia University and by the FP6 IST LUNAproject contract no 33549.805ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In COLING-ACL ?98: University of Montre?al.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introductionto the CoNLL-2005 Shared Task: Semantic Role La-beling.
In Proceedings of CoNLL-2005, Ann Arbor,Michigan.John Chen and Owen Rambow.
2003.
Use of Deep Lin-guistic Features for the Recognition and Labeling ofSemantic Arguments.
In Proceedings of EMNLP, Sap-poro, Japan.Michael Collins and Nigel Duffy.
2002.
New RankingAlgorithms for Parsing and Tagging: Kernels over Dis-crete structures, and the voted perceptron.
In ACL02.Mona Diab and Alessandro Moschitti.
2007.
SemanticParsing for Modern Standard Arabic.
In Proceedingsof RANLP, Borovets, Bulgaria.Mona Diab, Musa Alkhalifa, Sabry ElKateb, ChristianeFellbaum, Aous Mansouri, and Martha Palmer.
2007a.Semeval-2007 task 18: Arabic Semantic Labeling.
InProceedings of SemEval-2007, Prague, Czech Repub-lic.Mona Diab, Mahmoud Ghoneim, and Nizar Habash.2007b.
Arabic Diacritization in the Context of Sta-tistical Machine Translation.
In Proceedings of MT-Summit, Copenhagen, Denmark.Mona Diab.
2007.
Towards an Optimal Pos Tag Set forModern Standard Arabic Processing.
In Proceedingsof RANLP, Borovets, Bulgaria.Katrin Erk and Sebastian Pado.
2006.
Shalmaneser ?
AToolchain for Shallow Semantic Parsing.
Proceedingsof LREC.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic La-beling of Semantic Roles.
Computational Linguistics.Daniel Gildea and Martha Palmer.
2002.
The Neces-sity of Parsing for Predicate Argument Recognition.In Proceedings of ACL-02, Philadelphia, PA, USA.Nizar Habash and Owen Rambow.
2005.
Arabic Tok-enization, Part-of-Speech Tagging and MorphologicalDisambiguation in One Fell Swoop.
In Proceedings ofACL?05, Ann Arbor, Michigan.Aria Haghighi, Kristina Toutanova, and ChristopherManning.
2005.
A Joint Model for Semantic RoleLabeling.
In Proceedings ofCoNLL-2005, Ann Arbor,Michigan.Paul Kingsbury and Martha Palmer.
2003.
Propbank: theNext Level of Treebank.
In Proceedings of Treebanksand Lexical Theories.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The Penn Arabic Treebank :Building a Large-Scale Annotated Arabic Corpus.Mohamed Maamouri, Ann Bies, Tim Buckwalter, MonaDiab, Nizar Habash, Owen Rambow, and DalilaTabessi.
2006.
Developing and Using a Pilot DialectalArabic Treebank.Alessandro Moschitti, Ana-Maria Giuglea, BonaventuraCoppola, and Roberto Basili.
2005.
HierarchicalSemantic Role Labeling.
In Proceedings of CoNLL-2005, Ann Arbor, Michigan.Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,and Suresh Manandhar.
2007.
Exploiting Syntacticand Shallow Semantic Kernels for Question AnswerClassification.
In Proceedings of ACL?07, Prague,Czech Republic.Alessandro Moschitti.
2004.
A Study on ConvolutionKernels for Shallow Semantic Parsing.
In proceedingsof ACL?04, Barcelona, Spain.Alessandro Moschitti.
2006.
Making Tree Kernels Prac-tical for Natural Language Learning.
In Proceedingsof EACL?06.Alessandro Moschitti, Daniele Pighin and Roberto Basili.2006.
Semantic Role Labeling via Tree Kernel JointInference.
In Proceedings of CoNLL-X.Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.Martin, and Daniel Jurafsky.
2003.
Semantic RoleParsing: Adding Semantic Structure to UnstructuredText.
In Proceedings ICDM?03, Melbourne, USA.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James H. Martin, and Daniel Jurafsky.2005.
Support Vector Learning for Semantic Argu-ment Classification.
Machine Learning.Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,and Cynthia Rudin.
2008.
Arabic Morphological Tag-ging, Diacritization, and Lemmatization Using Lex-eme Models and Feature Ranking.
In ACL?08, ShortPapers, Columbus, Ohio, June.John Shawe-Taylor and Nello Cristianini.
2004.
KernelMethods for Pattern Analysis.
Cambridge UniversityPress.Honglin Sun and Daniel Jurafsky.
2004.
Shallow Seman-tic Parsing of Chinese.
In Proceedings of NAACL-HLT.Cynthia A. Thompson, Roger Levy, and ChristopherManning.
2003.
A Generative Model for SemanticRole Labeling.
In ECML?03.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.John Wiley and Sons.Nianwen Xue and Martha Palmer.
2004.
CalibratingFeatures for Semantic Role Labeling.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP 2004,Barcelona, Spain.806
