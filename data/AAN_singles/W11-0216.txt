Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 124?133,Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational LinguisticsA Study on Dependency Tree Kernelsfor Automatic Extraction of Protein-Protein InteractionMd.
Faisal Mahbub Chowdhury ?
?
and Alberto Lavelli ?
and Alessandro Moschitti ??
Department of Information Engineering and Computer Science, University of Trento, Italy?
Human Language Technology Research Unit, Fondazione Bruno Kessler, Trento, Italy{chowdhury,lavelli}@fbk.eu, moschitti@disi.unitn.itAbstractKernel methods are considered the most ef-fective techniques for various relation extrac-tion (RE) tasks as they provide higher accu-racy than other approaches.
In this paper,we introduce new dependency tree (DT) ker-nels for RE by improving on previously pro-posed dependency tree structures.
These arefurther enhanced to design more effective ap-proaches that we call mildly extended depen-dency tree (MEDT) kernels.
The empirical re-sults on the protein-protein interaction (PPI)extraction task on the AIMed corpus show thattree kernels based on our proposed DT struc-tures achieve higher accuracy than previouslyproposed DT and phrase structure tree (PST)kernels.1 IntroductionRelation extraction (RE) aims at identifying in-stances of pre-defined relation types in text as forexample the extraction of protein-protein interaction(PPI) from the following sentence:?Native C8 also formed a heterodimerwith C5, and low concentrations ofpolyionic ligands such as protamine andsuramin inhibited the interaction.
?After identification of the relevant named entities(NE, in this case proteins) C8 and C5, the RE taskdetermines whether there is a PPI relationship be-tween the entities above (which is true in the exam-ple).Kernel based approaches for RE have drawn a lotof interest in recent years since they can exploit ahuge amount of features without an explicit repre-sentation.
Some of these approaches are structurekernels (e.g.
tree kernels), which carry out struc-tural similarities between instances of relations, rep-resented as phrase structures or dependency trees,in terms of common substructures.
Other kernelssimply use techniques such as bag-of-words, subse-quences, etc.
to map the syntactic and contextualinformation to flat features, and later compute simi-larity.One variation of tree kernels is the dependencytree (DT) kernel (Culotta and Sorensen, 2004;Nguyen et al, 2009).
A DT kernel (DTK) is atree kernel that is computed on a dependency tree(or subtree).
A dependency tree encodes grammati-cal relations between words in a sentence where thewords are nodes, and dependency types (i.e.
gram-matical functions of children nodes with respect totheir parents) are edges.
The main advantage of aDT in comparison with phrase structure tree (PST)is that the former allows for relating two words di-rectly (and in more compact substructures than PST)even if they are far apart in the corresponding sen-tence according to their lexical word order.Several kernel approaches exploit syntactic de-pendencies among words for PPI extraction frombiomedical text in the form of dependency graphs ordependency paths (e.g.
Kim et al (2010) or Airolaet al (2008)).
However, to the best of our knowl-edge, there are only few works on the use of DTkernels for this task.
Therefore, exploring the po-tential of DTKs applied to different structures is aworthwhile research direction.
A DTK, pioneeredby Culotta and Sorensen (2004), is typically appliedto the minimal or smallest common subtree that in-cludes a target pair of entities.
Such subtree reduces124Figure 1: Part of the DT for the sentence ?The bindingepitopes of BMP-2 for BMPR-IA was characterized usingBMP-2 mutant proteins?.
The dotted area indicates theminimal subtree.unnecessary information by placing word(s) closerto its dependent(s) inside the tree and emphasizeslocal features of relations.
Nevertheless, there arecases where a minimal subtree might not contain im-portant cue words or predicates.
For example, con-sider the following sentence where a PPI relationholds between BMP-2 and BMPR-IA, but the mini-mal subtree does not contain the cue word ?binding?as shown in Figure 1:The binding epitopes of BMP-2 forBMPR-IA was characterized using BMP-2 mutant proteins.In this paper we investigate two assumptions.
Thefirst is that a DTK based on a mild extension ofminimal subtrees would produce better results thanthe DTK on minimal subtrees.
The second is thatpreviously proposed DT structures can be furtherimproved by introducing simplified representationof the entities as well as augmenting nodes in theDT tree structure with relevant features.
This paperpresents an evaluation of the above assumptions.More specifically, the contributions of this paperare the following:?
We propose the use of new DT structures,which are improvement on the structures de-fined in Nguyen et al (2009) with the most gen-eral (in terms of substructures) DTK, i.e.
Par-tial Tree Kernel (PTK) (Moschitti, 2006).?
We firstly propose the use of the UnlexicalizedPTK (Severyn and Moschitti, 2010) with ourdependency structures, which significantly im-proves PTK.?
We compare the performance of the proposedDTKs on PPI with the one of PST kernels andshow that, on biomedical text, DT kernels per-form better.?
Finally, we introduce a novel approach (calledmildly extended dependency tree (MEDT) ker-nel1, which achieves the best performanceamong various (both DT and PST) tree kernels.The remainder of the paper is organized as fol-lows.
In Section 2, we introduce tree kernels and re-lation extraction and we also review previous work.Section 3 describes the unlexicalized PTK (uPTK).Then, in Section 4, we define our proposed DT struc-tures including MEDT.
Section 5 describes the ex-perimental results on the AIMed corpus (Bunescu etal., 2005) and discusses their outcomes.
Finally, weconclude with a summary of our study as well asplans for future work.2 Background and Related WorkThe main stream work for Relation Extraction useskernel methods.
In particular, as the syntactic struc-ture is very important to derive the relationships be-tween entities in text, several tree kernels have beendesigned and experimented.
In this section, we in-troduce such kernels, the problem of relation extrac-tion and we also focus on the biomedical domain.2.1 Tree Kernel typesThe objective behind the use of tree kernels isto compute the similarity between two instancesthrough counting similarities of their sub-structures.Among the different proposed methods, two of themost effective approaches are Subset Tree (SST)kernel (Collins and Duffy, 2001) and Partial TreeKernel (PTK) (Moschitti, 2006).The SST kernel generalizes the subtree ker-nel (Vishwanathan and Smola, 2002), which consid-ers all common subtrees in the tree representation oftwo compared sentences.
In other words, two sub-trees are identical if the node labels and order of chil-dren are identical for all nodes.
The SST kernel re-laxes the constraint that requires leaves to be alwaysincluded in the sub-structures.
In SST, for a givennode, either none or all of its children have to be in-cluded in the resulting subset tree.
An extension of1We defined new structures, which as it is well known itcorresponds to define a new kernel.125the SST kernel is the SST+bow (bag-of-words) ker-nel (Zhang and Lee, 2003; Moschitti, 2006a), whichconsiders individual leaves as sub-structures as well.The PT kernel (Moschitti, 2006) is more flexi-ble than SST by virtually allowing any tree sub-structure; the only constraint is that the order of childnodes must be identical.
Both SST and PT kernelsare convolution tree kernels2.The PT kernel is the most complete in terms ofstructures.
However, the massive presence of childnode subsequences and single child nodes, which ina DT often correspond to words, may cause overfit-ting.
Thus we propose the use of the unlexicalized(i.e.
PT kernel without leaves) tree kernel (uPTK)(Severyn and Moschitti, 2010), in which structurescomposed by only one lexical element, i.e.
singlenodes, are removed from the feature space (see Sec-tion 3).2.2 Relation Extraction using Tree KernelsA first version of dependency tree kernels (DTKs)was proposed by Culotta and Sorensen (2004).
Intheir approach, they find the smallest common sub-tree in the DT that includes a given pair of enti-ties.
Then, each node of the subtree is representedas a feature vector.
Finally, these vectors are usedto compute similarity.
However, the tree kernel theydefined is not a convolution kernel, and hence it gen-erates a much lower number of sub-structures result-ing in lower performance.For any two entities e1 and e2 in a DT, Nguyenet al (2009) defined the following three dependencystructures to be exploited by convolution tree ker-nels:?
Dependency Words (DW) tree: a DW tree isthe minimal subtree of a DT, which includes e1and e2.
An extra node is inserted as parent ofthe corresponding NE, labeled with the NE cat-egory.
Only words are considered in this tree.?
Grammatical Relation (GR) tree: a GR treeis similar to a DW tree except that words arereplaced by their grammatical functions, e.g.prep, nsubj, etc.2Convolution kernels aim to capture structural informationin term of sub-structures, providing a viable alternative to flatfeatures (Moschitti, 2004).?
Grammatical Relation and Words (GRW) tree:a GRW tree is the minimal subtree that usesboth words and grammatical functions, wherethe latter are inserted as parent nodes of the for-mer.Using PTK for the above dependency tree struc-tures, the authors achieved an F-measure of 56.3 (forDW), 60.2 (for GR) and 58.5 (for GRW) on the ACE2004 corpus3.Moschitti (2004) proposed the so called path-enclosed tree (PET)4 of a PST for Semantic RoleLabeling.
This was later adapted by Zhang et al(2005) for relation extraction.
A PET is the smallestcommon subtree of a PST, which includes the twoentities involved in a relation.Zhou et al (2007) proposed the so called context-sensitive tree kernel approach based on PST, whichexpands PET to include necessary contextual in-formation.
The expansion is carried out by someheuristics tuned on the target RE task.Nguyen et al (2009) improved the PET represen-tation by inserting extra nodes for denoting the NEcategory of the entities inside the subtree.
They alsoused sequence kernels from tree paths, which pro-vided higher accuracy.2.3 Relation Extraction in the biomedicaldomainThere are several benchmarks for the PPI task,which adopt different PPI annotations.
Conse-quently the experimental results obtained by dif-ferent approaches are often difficult to compare.Pyysalo et al (2008) put together these corpora (in-cluding the AIMed corpus used in this paper) in acommon format for comparative evaluation.
Eachof these corpora is known as converted corpus of thecorresponding original corpus.Several kernel-based RE approaches have beenreported to date for the PPI task.
These are based onvarious methods such as subsequence kernel (Lodhiet al, 2002; Bunescu and Mooney, 2006), depen-dency graph kernel (Bunescu and Mooney, 2005),etc.
Different work exploited dependency analy-ses with different kernel approaches such as bag-of-3http://projects.ldc.upenn.edu/ace/4Also known as shortest path-enclosed tree or SPT (Zhou etal., 2007).126words kernel (e.g.
Miwa et al (2009)), graph basedkernel (e.g.
Kim et al (2010)), etc.
However, thereare only few researches that attempted the exploita-tion of tree kernels on dependency tree structures.S?tre et al (2007) used DT kernels on AIMedcorpus and achieved an F-score of 37.1.
The re-sults were far better when they combined the out-put of the dependency parser with that of a Head-driven Phrase Structure Grammar (HPSG) parser,and applied tree kernel on it.
Miwa et al (2009) alsoproposed a hybrid kernel 5, which is a compositionof all-dependency-paths kernel (Airola et al, 2008),bag-of-words kernel and SST kernel.
They usedmultiple parser inputs.
Their system is the currentstate-of-the-art for PPI extraction on several bench-marks.
Interestingly, they applied SST kernel on theshortest dependency paths between pairs of proteinsand achieved a relatively high F-score of 55.1.
How-ever, the trees they constructed from the shortest de-pendency paths are actually not dependency trees.
Ina dependency tree, there is only one node for eachindividual word whereas in their constructed trees(please refer to Fig.
6 of Miwa et al (2009)), a word(that belongs to the shortest path) has as many noderepresentations as the number of dependency rela-tions with other words (those belonging to the short-est path).
Perhaps, this redundancy of informationmight be the reason their approach achieved higherresult.
In addition to work on PPI pair extraction,there has been some approaches that exploited de-pendency parse analyses along with kernel methodsfor identifying sentences that might contain PPI pairs(e.g.
Erkan et al (2007)).In this paper, we focus on finding the best repre-sentation based on a single structure.
We speculatethat this can be helpful to improve the state-of-the-art using several combinations of structures and fea-tures.
As a first step, we decided to use uPTK, whichis more robust to overfitting as the description in thenext section unveil.5The term ?hybrid kernel?
is identical to ?combined kernel?.It refers to those kernels that combine multiple types of kernels(e.g., tree kernels, graph kernels, etc)3 Unlexicalized Partial Tree Kernel(uPTK)The uPTK was firstly proposed in (Severyn andMoschitti, 2010) and experimented with semanticrole labeling (SRL).
The results showed no improve-ment for such task but it is well known that in SRLlexical information is essential (so in that case itcould have been inappropriate).
The uPTK defini-tion follows the general setting of tree kernels.A tree kernel function over two trees, T1 and T2,is defined asTK(T1, T2) =?n1?NT1?n2?NT2?
(n1, n2),where NT1 and NT2 are the sets of nodes in T1 andT2, respectively, and?
(n1, n2) =|F|?i=1?i(n1)?i(n2).The ?
function is equal to the number of commonfragments rooted in nodes n1 and n2 and thus de-pends on the fragment type.The algorithm for the uPTK computation straight-forwardly follows from the definition of the ?
func-tion of PTK provided in (Moschitti, 2006).
Giventwo nodes n1 and n2 in the corresponding two treesT1 and T2, ?
is evaluated as follows:1. if the node labels of n1 and n2 are different then?
(n1, n2) = 0;2. else ?
(n1, n2) = ?
(?2 +?~I1,~I2,l(~I1)=l(~I2)?d(~I1)+d(~I2)l(~I1)?j=1?
(cn1(~I1j), cn2(~I2j))),where:1.
~I1 = ?h1, h2, h3, ..?
and ~I2 = ?k1, k2, k3, ..?are index sequences associated with the orderedchild sequences cn1 of n1 and cn2 of n2, respec-tively;2.
~I1j and ~I2j point to the j-th child in the corre-sponding sequence;3.
l(?)
returns the sequence length, i.e.
the numberof children;1274. d(~I1) = ~I1l(~I1)?~I11 + 1 and d(~I2) = ~I2l(~I2)?~I21 + 1; and5.
?
and ?
are two decay factors for the size ofthe tree and for the length of the child subse-quences with respect to the original sequence,i.e.
we account for gaps.The uPTK can be obtained by removing ?2 fromthe equation in step 2.
An efficient algorithm for thecomputation of PTK is given in (Moschitti, 2006).This evaluates ?
by summing the contribution oftree structures coming from different types of se-quences, e.g.
those composed by p children suchas:?
(n1, n2) = ?
(?2 +?lmp=1 ?p(cn1 , cn2)), (1)where ?p evaluates the number of common subtreesrooted in subsequences of exactly p children (of n1and n2) and lm = min{l(cn1), l(cn2)}.
It is easy toverify that we can use the recursive computation of?p by simply removing ?2 from Eq.
1.4 Proposed dependency structures andMEDT kernelOur objective is twofold: (a) the definition of im-proved DT structures and (b) the design of new DTkernels to include important words residing outsideof the shortest dependency tree, which are neglectedin current approaches.
For achieving point (a), wemodify the DW, GR and GRW structures, previouslyproposed by Nguyen et al (2009).
The new pro-posed structures are the following:?
Grammatical Relation and lemma (GRL) tree:A GRL tree is similar to a GRW tree exceptthat words are replaced by their correspondinglemmas.?
Grammatical Relation, PoS and lemma(GRPL) tree: A GRPL tree is an extension of aGRL tree, where the part-of-speech (PoS) tagof each of the corresponding words is insertedas a new node between its grammatical func-tion and its lemma, i.e.
the new node becomesthe parent node of the node containing thelemma.Figure 2: Part of the DT for the sentence ?Interactionwas identified between BMP-2 and BMPR-IA?.
The dot-ted area indicates the minimal subtree.Figure 3: Part of the DT for the sentence ?Phe93 formsextensive contacts with a peptide ligand in the crystalstructure of the EBP bound to an EMP1?.
The dottedarea indicates the minimal subtree.?
Ordered GRL (OGRL) or ordered GRW(OGRW) tree: in a GRW (or GRL) tree, thenode containing the grammatical function ofa word is inserted as the parent node of suchword.
So, if the word has a parent node con-taining its NE category, the newly inserted nodewith grammatical function becomes the childnode of the node containing NE category, i.e.the order of the nodes is the following ?
?NEcategory ?
grammatical relation ?
word (orlemma)?.
However, in OGRW (or OGRL), thisordering is modified as follows ?
?grammaticalrelation?
NE category?
word (or lemma)?.?
Ordered GRPL (OGRPL) tree: this is similarto the OGRL tree except for the order of thenodes, which is the following ?
?grammaticalrelation?
NE category?
PoS?
lemma?.?
Simplified (S) tree: any tree structure wouldbecome an S tree if it contains simplified repre-sentations of the entity types, where all its partsexcept the head word of a multi-word entity arenot considered in the minimal subtree.The second objective is to extend DTKs to includeimportant cue words or predicates that are missing128in the minimal subtree.
We do so by mildly expand-ing the minimal subtree, i.e.
we define the mildlyextended DT (MEDT) kernel.
We propose three dif-ferent expansion rules for three versions of MEDTas follows:?
Expansion rule for MEDT-1 kernel: If the rootof the minimal subtree is not a modifier (e.g.adjective) or a verb, then look for such node inits children or in its parent (in the original DTtree) to extend the subtree.The following example shows a sentence wherethis rule would be applicable:The binding epitopes of BMP-2for BMPR-IA was characterized us-ing BMP-2 mutant proteins.Here, the cue word is ?binding?, the root of theminimal subtree is ?epitopes?
and the target en-tities are BMP-2 and BMPR-IA.
However, asshown in Figure 1, the minimal subtree doesnot contain the cue word.?
Expansion rule for MEDT-2 kernel: If the rootof the minimal subtree is a verb and its subject(or passive subject) in the original DT tree isnot included in the subtree, then include it.Consider the following sentence:Interaction was identified be-tween BMP-2 and BMPR-IA.Here, the cue word is ?Interaction?, the rootis ?identified?
and the entities are BMP-2 andBMPR-IA.
The passive subject ?Interaction?does not belong to the minimal subtree (seeFigure 2).?
Expansion rule for MEDT-3 kernel: If the rootof the minimal subtree is the head word of oneof the interacting entities, then add the parentnode (in the original DT tree) of the root nodeas the new root of the subtree.This is an example sentence where this rule isapplicable (see Figure 3):Phe93 forms extensive contactswith a peptide ligand in the crystalstructure of the EBP bound to anEMP1.5 Experiments and resultsWe carried out several experiments with differentdependency structures and tree kernels.
Most im-portantly, we tested tree kernels on PST and our im-proved representations for DT.5.1 Data and experimental setupWe used the AIMed corpus (Bunescu et al, 2005)converted using the software provided by Pyysalo etal.
(2008).
AIMed is the largest benchmark corpus(in terms of number of sentences) for the PPI task.It contains 1,955 sentences, in which are annotated1,000 positive PPI and 4,834 negative pairs.We use the Stanford parser6 for parsing the data.7The SPECIALIST lexicon tool8 is used to normalizewords to avoid spelling variations and also to pro-vide lemmas.
For training and evaluating tree ker-nels, we use the SVM-LIGHT-TK toolkit9 (Mos-chitti, 2006; Joachims, 1999).
We tuned the param-eters ?, ?
and c following the approach described byHsu et al (2003), and used biased hyperplane.10 Allthe other parameters are left as their default values.Our experiments are evaluated with 10-fold crossvalidation using the same split of the AIMed corpusused by Bunescu et al (2005).5.2 Results and DiscussionThe results of different tree kernels applied to dif-ferent structures are shown in Tables 1 and 2.
Allthe tree structures are tested with four different treekernel types: SST, SST+bow, PTK and uPTK.According to the empirical outcome, our new DTstructures perform better than the existing tree struc-tures.
The highest result (F: 46.26) is obtained byapplying uPTK to MEDT-3 (SOGRL).
This is 6.68higher than the best F-measure obtained by previousDT structures proposed in Nguyen et al (2009), and0.36 higher than the best F-measure obtained usingPST (PET).6http://nlp.stanford.edu/software/lex-parser.shtml7For some of the positive PPI pairs, the connecting depen-dency tree could not be constructed due to parsing errors forthe corresponding sentences.
Such pairs are considered as falsenegative (FN) during precision and recall measurements.8http://lexsrv3.nlm.nih.gov/SPECIALIST/index.html9http://disi.unitn.it/moschitti/Tree-Kernel.htm10Please refer to http://svmlight.joachims.org/ andhttp://disi.unitn.it/moschitti/Tree-Kernel.htm for detailsabout parameters of the respective tools129DT DT DT DT DT DT DT DT DT(GR) (SGR) (DW) (SDW) (GRW) (SGRW) (SGRL) (SGRPL) (OGRPL)SST P: 55.29 P: 54.22 P: 31.87 P: 30.74 P: 52.76 P: 52.47 P: 56.09 P: 56.03 P: 57.85R: 23.5 R: 24.4 R: 27.5 R: 27.3 R: 33.4 R: 30.8 R: 33.6 R: 33.0 R: 31.7F: 32.98 F: 33.66 F: 29.52 F: 28.92 F: 40.9 F: 38.82 F: 42.03 F: 41.54 F: 40.96SST P: 57.87 P: 54.91 P: 30.71 P: 29.98 P: 52.98 P: 51.06 P: 51.99 P: 56.8 P: 61.73+ R: 21.7 R: 23.5 R: 26.9 R: 25.9 R: 32.0 R: 31.3 R: 31.4 R: 28.8 R: 29.2bow F: 31.56 F: 32.91 F: 28.68 F: 27.79 F: 39.9 F: 38.81 F: 39.15 F: 38.22 F: 39.65PT P: 60.0 P: 57.84 P: 40.44 P: 42.2 P: 53.35 P: 53.41 P: 51.29 P: 52.88 P: 53.55R: 15.9 R: 16.6 R: 23.9 R: 26.5 R: 34.2 R: 36.0 R: 37.9 R: 33.0 R: 33.2F: 25.14 F: 25.8 F: 30.04 F: 32.56 F: 41.68 F: 43.01 F: 43.59 F: 40.64 F: 40.99uPT P: 58.77 P: 59.5 P: 29.21 P: 29.52 P: 51.86 P: 52.17 P: 52.1 P: 54.64 P: 56.43R: 23.8 R: 26.0 R: 30.2 R: 31.5 R: 32.0 R: 33.7 R: 36.0 R: 31.2 R: 30.7F: 33.88 F: 36.19 F: 29.7 F: 30.48 F: 39.58 F: 40.95 F: 42.58 F: 39.72 F: 39.77Table 1: Performance of DT (GR), DT (DW) and DT (GRW) (proposed by (Nguyen et al, 2009)) and their modifiedand improved versions on the converted AIMed corpus.RE experiments carried out on newspaper textcorpora (such as ACE 2004) have indicated that ker-nels based on PST obtain better results than kernelsbased on DT.
Interestingly, our experiments on abiomedical text corpus indicate an opposite trend.Intuitively, this might be due to the different na-ture of the PPI task.
PPI can be often identified byspotting cue words such as interaction, binding, etc,since the interacting entities (i.e.
proteins) usuallyhave direct syntactic dependency relation on suchcue words.
This might have allowed kernels basedon DT to be more accurate.Although tree kernels applied on DT and PSTstructures have produced high performance on cor-pora of news text (Zhou et al, 2007; Nguyen et al,2009), in case of biomedical text the results that weobtained are relatively low.
This may be due to thefact that biomedical texts are different from newspa-per texts: more variation in vocabulary, more com-plex naming of (bio) entities, more diversity of thevalency of verbs and so on.One important finding of our experiments is theeffectiveness of the mild extension of DT struc-tures.
MEDT-3 achieves the best result for all ker-nels (SST, SST+bow, PTK and uPTK).
However, theother two versions of MEDT appear to be less effec-tive.In general, the empirical outcome suggests thatuPTK can better exploit our proposed DT structuresas well as PST.
The superiority of uPTK on PTKdemonstrates that single lexical features (i.e.
fea-tures with flat structure) tend to overfit.Finally, we have performed statistical tests to as-sess the significance of our results.
For each kernel(i.e.
SST, SST+bow, PTK, uPTK), the PPI predic-tions using the best structure (i.e.
MEDT-3 appliedto SOGRL) are compared against the predictions ofthe other structures.
The tests were performed usingthe approximate randomization procedure (Noreen,1989).
We set the number of iterations to 1,000 andthe confidence level to 0.01.
According to the tests,for each kernel, our best structure produces signifi-cantly better results.5.3 Comparison with previous workTo the best of our knowledge, the only work on treekernel applied on dependency trees that we can di-rectly compare to ours is reported by S?tre et al(2007).
Their DT kernel achieved an F-score of37.1 on AIMed corpus which is lower than our bestresults.
As discussed earlier, Miwa et al (2009))also used tree kernel on dependency analyses andachieved a much higher result.
However, the treestructure they used contains multiple nodes for a sin-gle word and this does not comply with the con-straints usually applied to dependency tree structures(refer to Section 2.3).
It would be interesting to ex-amine why such type of tree representation leads to130DT DT DT DT MEDT-1 MEDT-2 MEDT-3 PST(SOGRPL) (OGRL) (SOGRW) (SOGRL) (SOGRL) (SOGRL) (SOGRPL) (PET)SST P: 57.59 P: 54.38 P: 51.49 P: 54.08 P: 58.15 P: 54.46 P: 59.55 P: 52.72R: 33.0 R: 33.5 R: 31.2 R: 33.8 R: 34.6 R: 33.6 R: 37.1 R: 35.9F: 41.96 F: 41.46 F: 38.86 F: 41.6 F: 43.39 F: 41.56 F: 45.72 F: 42.71SST P: 60.31 P: 53.22 P: 50.08 P: 53.26 P: 58.84 P: 52.87 P: 59.35 P: 52.88+ R: 30.7 R: 33.1 R: 30.9 R: 32.7 R: 32.6 R: 32.2 R: 34.9 R: 37.7bow F: 40.69 F: 40.82 F: 38.22 F: 40.52 F: 41.96 F: 40.02 F: 43.95 F: 44.02PT P: 55.45 P: 49.78 P: 51.05 P: 51.61 P: 52.94 P: 50.89 P: 54.1 P: 58.39R: 34.6 R: 34.6 R: 34.1 R: 36.9 R: 36.0 R: 37.0 R: 38.9 R: 36.9F: 42.61 F: 40.82 F: 40.89 F: 43.03 F: 42.86 F: 42.85 F: 45.26 F: 45.22uPT P: 56.2 P: 50.87 P: 50.0 P: 52.74 P: 55.0 P: 52.17 P: 56.85 P: 56.6R: 32.2 R: 35.0 R: 33.0 R: 35.6 R: 34.1 R: 34.8 R: 39.0 R: 38.6F: 40.94 F: 41.47 F: 39.76 F: 42.51 F: 42.1 F: 41.75 F: 46.26 F: 45.9Table 2: Performance of the other improved versions of DT kernel structures (including MEDT kernels) as well asPST (PET) kernel (Moschitti, 2004; Nguyen et al, 2009) on the converted AIMed corpus.a better result.In this work, we compare the performance of treekernels applied of DT with that of PST.
Previously,Tikk et al (2010) applied similar kernels on PST forexactly the same task and data set.
They reportedthat SST and PTK (on PST) achieved F-scores of26.2 and 34.6, respectively on the converted AIMedcorpus (refer to Table 2 in their paper).
Such resultsdo not match our figures obtained with the samekernels on PST.
We obtain much higher results forthose kernels.
It is difficult to understand the rea-son for such differences between our and their re-sults.
A possible explanation could be related to pa-rameter settings.
Another source of uncertainty isgiven by the tool for tree kernel computation, whichin their case is not mentioned.
Moreover, their de-scription of PT and SST (in Figure 1 of their paper)appears to be imprecise: for example, in (partial orcomplete) phrase structure trees, words can only ap-pear as leaves but in their figure they appear as non-terminal nodes.The comparison with other kernel approaches (i.e.not necessarily tree kernels on DT or PST) showsthat there are model achieving higher results (e.g.Giuliano et al (2006), Kim et al (2010), Airola etal.
(2008), etc).
State-of-the-art results on most ofthe PPI data sets are obtained by the hybrid kernelpresented in Miwa et al (2009).
As noted earlier,our work focuses on the design of an effective DTKfor PPI that can be combined with others and thatcan hopefully be used to design state-of-the-art hy-brid kernels.6 ConclusionIn this paper, we have proposed a study of PPI ex-traction from specific biomedical data based on treekernels.
We have modeled and experimented withnew kernels and DT structures, which can be ex-ploited for RE tasks in other domains too.More specifically, we applied four different treekernels on existing and newly proposed DT and PSTstructures.
We have introduced some extensions ofDT kernel structures which are linguistically moti-vated.
We call these as mildly extended DT kernels.We have also shown that in PPI extraction lexicalinformation can lead to overfitting as uPTK outper-forms PTK.
In general, the empirical results showthat our DT structures perform better than the previ-ously proposed PST and DT structures.The ultimate objective of our work is to improvetree kernels applied to DT and then combine themwith other types of kernels and data to produce moreaccurate models.AcknowledgmentsThis work was carried out in the context of the project?eOnco - Pervasive knowledge and data management incancer care?.
The authors would like to thank the anony-mous reviewers for providing excellent feedback.131ReferencesA Airola, S Pyysalo, J Bj?orne, T Pahikkala, F Gin-ter, and T Salakoski.
2008.
A graph kernel forprotein-protein interaction extraction.
In Proceedingsof BioNLP 2008, pages 1?9, Columbus, USA.R Bunescu and R Mooney.
2005.
A shortest path depen-dency kernel for relation extraction.
In Proceedings ofHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 724?731, Vancouver, British Columbia,Canada, October.
Association for Computational Lin-guistics.R Bunescu and RJ Mooney.
2006.
Subsequence ker-nels for relation extraction.
In Proceedings of the19th Conference on Neural Information ProcessingSystems, pages 171?178.R Bunescu, R Ge, RJ Kate, EM Marcotte, RJ Mooney,AK Ramani, and YW Wong.
2005.
Compara-tive experiments on learning information extractorsfor proteins and their interactions.
Artificial Intelli-gence in Medicine (Special Issue on Summarizationand Information Extraction from Medical Documents),33(2):139?155.M Collins and N Duffy.
2001.
Convolution kernels fornatural language.
In Proceedings of Neural Informa-tion Processing Systems (NIPS?2001).A Culotta and J Sorensen.
2004.
Dependency tree ker-nels for relation extraction.
In Proceedings of the 42ndAnnual Meeting of the Association for ComputationalLinguistics, Barcelona, Spain.G Erkan, A Ozgur, and DR Radev.
2007.
Semi-Supervised Classification for Extracting Protein Inter-action Sentences using Dependency Parsing.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Compu-tational Natural Language Learning (EMNLP-CoNLL2007), pages 228?237.C Giuliano, A Lavelli, and L Romano.
2006.
Exploit-ing shallow linguistic information for relation extrac-tion from biomedical literature.
In Proceedings of the11th Conference of the European Chapter of the As-sociation for Computational Linguistics (EACL?2006),pages 401?408, Trento, Italy.CW Hsu, CC Chang, and CJ Lin, 2003.
A practical guideto support vector classification.
Department of Com-puter Science and Information Engineering, NationalTaiwan University, Taipei, Taiwan.T Joachims.
1999.
Making large-scale support vec-tor machine learning practical.
In Advances in ker-nel methods: support vector learning, pages 169?184.MIT Press, Cambridge, MA, USA.S Kim, J Yoon, J Yang, and S Park.
2010.
Walk-weightedsubsequence kernels for protein-protein interaction ex-traction.
BMC Bioinformatics, 11(1).H Lodhi, C Saunders, J Shawe-Taylor, N Cristianini, andC Watkins.
2002.
Text classification using string ker-nels.
Journal of Machine Learning Research, 2:419?444, March.M Miwa, R S?tre, Y Miyao, T Ohta, and J Tsujii.
2009.Protein-protein interaction extraction by leveragingmultiple kernels and parsers.
International Journal ofMedical Informatics, 78.A Moschitti.
2004.
A study on convolution kernels forshallow semantic parsing.
In Proceedings of the 42ndAnnual Meeting of the Association for ComputationalLinguistics, ACL ?04, Barcelona, Spain.A Moschitti.
2006.
Efficient convolution kernels for de-pendency and constituent syntactic trees.
In JohannesFu?rnkranz, Tobias Scheffer, and Myra Spiliopoulou,editors, Machine Learning: ECML 2006, volume 4212of Lecture Notes in Computer Science, pages 318?329.Springer Berlin / Heidelberg.A Moschitti.
2006a.
Making Tree Kernels Practical forNatural Language Learning.
In Proceedings of the11th Conference of the European Chapter of the As-sociation for Computational Linguistics, Trento, Italy.TT Nguyen, A Moschitti, and G Riccardi.
2009.
Con-volution kernels on constituent, dependency and se-quential structures for relation extraction.
In Proceed-ings of the 2009 Conference on Empirical Methods inNatural Language Processing (EMNLP?2009), pages1378?1387, Singapore, August.EW Noreen.
1989.
Computer-Intensive Methodsfor Testing Hypotheses: An Introduction.
Wiley-Interscience.S Pyysalo, A Airola, J Heimonen, J Bjo?rne, F Ginter,and T Salakoski.
2008.
Comparative analysis of fiveprotein-protein interaction corpora.
BMC Bioinfor-matics, 9(Suppl 3):S6.R S?tre, K Sagae, and J Tsujii.
2007.
Syntactic featuresfor protein-protein interaction extraction.
In Proceed-ings of the Second International Symposium on Lan-guages in Biology and Medicine (LBM 2007), pages6.1?6.14, Singapore.A Severyn and A Moschitti.
2010.
Fast cutting planetraining for structural kernels.
In Proceedings ofECML-PKDD.D Tikk, P Thomas, P Palaga, J Hakenberg, and U Leser.2010.
A Comprehensive Benchmark of Kernel Meth-ods to Extract Protein-Protein Interactions from Liter-ature.
PLoS Computational Biology, 6(7), July.SVN Vishwanathan and AJ Smola.
2002.
Fast kernels onstrings and trees.
In Proceedings of Neural Informa-tion Processing Systems (NIPS?2002), pages 569?576,Vancouver, British Columbia, Canada.D Zhang and WS Lee.
2003.
Question classification us-ing support vector machines.
In Proceedings of the13226th annual international ACM SIGIR conference onResearch and development in information retrieval,SIGIR ?03, pages 26?32, Toronto, Canada.M Zhang, J Su, D Wang, G Zhou, and CL Tan.
2005.Discovering relations between named entities from alarge raw corpus using tree similarity-based clustering.In Robert Dale, Kam-Fai Wong, Jian Su, and Oi YeeKwong, editors, Natural Language Processing IJC-NLP 2005, volume 3651 of Lecture Notes in ComputerScience, pages 378?389.
Springer Berlin / Heidelberg.GD Zhou, M Zhang, DH Ji, and QM Zhu.
2007.
Treekernel-based relation extraction with context-sensitivestructured parse tree information.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages728?736, June.133
