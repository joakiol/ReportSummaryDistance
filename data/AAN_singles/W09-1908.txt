Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 58?61,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsProactive Learning for Building Machine Translation Systems for MinorityLanguagesVamshi Ambativamshi@cs.cmu.eduLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAJaime Carbonelljgc@cs.cmu.eduLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAAbstractBuilding machine translation (MT) for manyminority languages in the world is a seriouschallenge.
For many minor languages there islittle machine readable text, few knowledge-able linguists, and little money available forMT development.
For these reasons, it be-comes very important for an MT system tomake best use of its resources, both labeledand unlabeled, in building a quality system.In this paper we argue that traditional activelearning setup may not be the right fit for seek-ing annotations required for building a Syn-tax Based MT system for minority languages.We posit that a relatively new variant of activelearning, Proactive Learning, is more suitablefor this task.1 IntroductionSpeakers of minority languages could benefit fromfluent machine translation (MT) between their nativetongue and the dominant language of their region.But scarcity in capital and know-how has largelyrestricted machine translation to the dominant lan-guages of first world nations.
To lower the barrierssurrounding MT system creation, we must reducethe time and resources needed to develop MT fornew language pairs.
Syntax based MT has provento be a good choice for minority language scenario(Lavie et al, 2003).
While the amount of paral-lel data required to build such systems is orders ofmagnitude smaller than corresponding phrase basedstatistical systems (Koehn et al, 2003), the varietyof linguistic annotation required is greater.
Syntaxbased MT systems require lexicons that provide cov-erage for the target translations, synchronous gram-mar rules that define the divergences in word-orderacross the language-pair.
In case of minority lan-guages one can only expect to find meagre amountof such data, if any.
Building such resources effec-tively, within a constrained budget, and deploying anMT system is the need of the day.We first consider ?Active Learning?
(AL) as aframework for building annotated data for the taskof MT.
However, AL relies on unrealistic assump-tions related to the annotation tasks.
For instance,AL assumes there is a unique omniscient oracle.
InMT, it is possible and more general to have multiplesources of information with differing reliabilities orareas of expertise.
A literate bilingual speaker withno extra training can produce translations for word,phrase or sentences and even align them.
But it re-quires a trained linguist to produce syntactic parsetrees.
AL also assumes that the single oracle is per-fect, always providing a correct answer when re-quested.
In reality, an oracle (human or machine)may be incorrect (fallible) with a probability thatshould be a function of the difficulty of the question.There is also no notion of cost associated with theannotation task, that varies across the input space.But in MT, it is easy to see that length of a sentenceand cost of translation are superlinear.
Also not allannotation tasks for MT have the same level of dif-ficulty or cost.
For example, it is relatively cheapto ask a bilingual speaker whether a word, phraseor sentence was correctly translated by the system,but a bit more expensive to ask for a correction.
As-sumptions like these render active learning unsuit-58able for our task at hand which is building an MTsystem for languages with limited resources.
Wemake the case for ?Proactive Learning?
(Donmezand Carbonell, 2008) as a solution for this scenario.In the rest of the paper, we discuss syntax basedMT approach in Section 2.
In Section 3 we firstdiscuss active learning approaches for MT and de-tail the characteristics of MT for minority languagesproblem that render traditional active learning un-suitable for practical purposes.
In Section 4 we dis-cuss proactive learning as a potential solution for thecurrent problem.
We conclude with some challengesthat still remain in applying proactive learning forMT.2 Syntax Based Machine TranslationIn recent years, corpus based approaches to ma-chine translation have become predominant, withPhrase Based Statistical Machine Translation (PB-SMT) (Koehn et al, 2003) being the most ac-tively progressing area.
Recent research in syn-tax based machine translation (Yamada and Knight,2001; Chiang, 2005) incorporates syntactic informa-tion to ameliorate the reordering problem faced byPB-SMT approaches.
While traditional approachesto syntax based MT were dependent on availabil-ity of manual grammar, more recent approaches op-erate within the resources of PB-SMT and inducehierarchical or linguistic grammars from existingphrasal units, to provide better generality and struc-ture for reordering (Yamada and Knight, 2001; Chi-ang, 2005; Wu, 1997).2.1 Resources for Syntax MTSyntax based approaches to MT seek to leverage thestructure of natural language to automatically induceMT systems.
Depending upon the MT system andthe paradigm, the resource requirements may varyand could also include modules such as morpholog-ical analyzers, sense disambiguation modules, gen-erators etc.
A detailed discussion of the comprehen-sive pipeline, may be out of the scope of this pa-per, more so because such resources can not be ex-pected in a low-resource language scenario.
We onlyfocus on the quintessential set of modules for MTpipeline - data acquisition, word-alignment, syntac-tic analysis etc.
The resources can broadly be cat-egorized as ?monolingual?
vs ?bilingual?
dependingupon whether it requires knowledge in one languageor both languages for annotation.
A sample of thedifferent kinds of data and annotation that is ex-pected by an MT system is shown below.
Each ofthe additional information can be seen as extra an-notations for the ?Source?
sentence.
The languageof target in the example is ?Hindi?.?
Source: John ate an apple?
Target: John ne ek seb khaya?
Alignment: (1,1),(2,5),(3,3),(4,4)?
SourceParse: (S (NP (NNP John)) (VP (VBDate) (NP (DT an) (NN apple))))?
Lexicon: (seb?
apple),(ate?
khaya)?
Grammar: VP: V NP?
NP V3 Active Learning for MTModern syntax based MT rides on the success ofboth Statistical Machine Translation and StatisticalParsing.
Active learning has been applied to Statis-tical Parsing (Hwa, 2004; Baldridge and Osborne,2003) to improve sample selection for manual anno-tation.
In case of MT, active learning has remainedlargely unexplored.
Some attempts include trainingmultiple statistical MT systems on varying amountsof data, and exploring a committee based selectionfor re-ranking the data to be translated and includedfor re-training.
But this does not apply to training ina low-resource scenario where data is scarce.In the rest of the section we discuss the differentscenarios that arise in gathering of annotation forMT under a traditional ?active learning?
setup anddiscuss the characteristics of the task that render itdifficult.3.1 Multiple OraclesFor each of the sub-tasks of annotation, in realitywe have multiple sources of information or multi-ple oracles.
We can elicit translations for buildinga parallel corpus from bilingual speakers who speakboth the languages with certain accuracy or from alinguist who is well educated in the formal senseof the languages.
With the success of collabora-tive sites like Amazon?s ?Mechanical Turk?
1, one1http://www.mturk.com/59can provide the task of annotation to multiple ora-cles on the internet (Snow et al, 2008).
The taskof word alignment can be posed in a similar fash-ion too.
More interestingly, there are statistical toolslike GIZA 2 that take as input un-annotated paral-lel data and propose automatic correspondences be-tween words in the language-pair, giving scope to?machine oracles?.3.2 Varying Quality and ReliabilityOracles also vary on the correctness of the answersthey provide (quality) as well as their availability(robustness) to answer.
One typical distinction is?human oracles?
vs ?machine oracles?.
Human or-acle produce higher quality annotations when com-pared to a machine oracle.
We would prefer a treebank of parse trees that were manually created overautomatically generated tree banks.
Similar is thecase with word-alignment and other tasks of trans-lation.
Some oracles are ?reluctant?
to produce anoutput, for example parsers tend to break on reallylong sentences, but when they produce an outputwe can associate some confidence with it about thequality.
One can expect a human oracle to produceparse trees for long sentences, but the quality couldbe questionable.3.3 Non-uniform costsEach of the annotation tasks has a non-uniform costassociated with it, the distribution of which is de-pendent upon the difficulty over the input space.Clearly, length of the sentence is a good indicator ofthe cost.
It takes much longer to translate a sentenceof 100 words than to translate one with 10 words.
Ittakes at least twice as long to create word-alignmentcorrespondences for a sentence-pair with 40 tokensthan a pair with 20 tokens.
Similarly, a human takesmuch longer to manually create parse tree for a longsentence than a short sentence.It is also the case that not all oracles have thesame non-uniform cost distribution over the inputspace.
Some oracles are more expensive than theothers.
For example a practicing linguist?s time isperhaps costlier than that of an undergraduate whois a bilingual speaker.
As noticed above, this mayreflect upon the quality of annotation for the task,2http://www.fjoch.com/GIZA++.htmlbut sometimes a tradeoff to make is cost vs qual-ity.
We can not afford to introduce a grammar ruleof low-quality into the system, but can possibly doaway with an incorrect word-correspondence link.4 Proactive LearningProactive learning (Donmez and Carbonell, 2008) isa generalization of active learning designed to re-lax unrealistic assumptions and thereby reach prac-tical applications.
Active learning seeks to select themost informative unlabeled instances and ask an om-niscient oracle for their labels, so as to retrain thelearning algorithm maximizing accuracy.
However,the oracle is assumed to be infallible (never wrong),indefatigable (always answers), individual (only oneoracle), and insensitive to costs (always free or al-ways charges the same).
Proactive learning relaxesall these four assumptions, relying on a decision-theoretic approach to jointly select the optimal or-acle and instance, by casting the problem as a utilityoptimization problem subject to a budget constraint.maximize E[V (S)] subject to BmaxS?ULE[V (S)]?
?
(?ktk ?
Ck)s.t?ktk ?
Ck = BThe above equation can be interpreted as maximiz-ing the expected value of labeling the input set Sunder the budget constraint B.
The subscript k de-notes the oracle from which the answer was elicitedunder a cost function C. A greedy approximation ofthe above results in the equation 1, where Ek[V (x)]is the expected value of information of the examplex corresponding to oracle k. One can design inter-esting functions that calculate V (x) in case of MT.For example, selecting short sentences with an unre-solved linguistic issue could maximize the utility ofthe data at a low cost.
(x?, k?)
= argmaxx?UEk[V (x)] subject to B (1)We now turn to how proactive learning frameworkhelps solve the issues raised for active learning inMT in section 3.
We can address the issue of multi-ple oracles where one oracle is fallible or reluctant toanswer, by factoring into Equation 2 its probability60function for returning an answer.
The score returnedby such a factoring can be called the utility associ-ated with that input for a particular oracle.
We callthis U(x, k).
A similar factorization can be done inorder to address the issue of oracles that are fallible.U(x, k) = P (ans|x, k) ?
V (x)?
Ck(x?, k?)
= argmaxx?U U(x, k)Since we do not have the P (ans/x, k) distribu-tion information for each oracle, proactive learningproposes to discover this in a discovery phase undersome allocated budget Bd.
Once we have an esti-mate from the discovery phase, the rest of the label-ing proceeds according to the optimization function.For more details of the algorithms refer (Donmezand Carbonell, 2008).
Finally, we can also relax theassumption of uniform cost per annotation, but re-placing the Ck term in the above equations with aCnon?unifk function denoting the non-uniform costfunction associated with the oracle.5 Future ChallengesWhile proactive learning is a good framework forbuilding MT systems for minority languages, thereare however a few issues that still remain that needcareful attention.Joint Utility: In a complex system like MT wheredifferent models combine forces to produce thetranslation we have a situation where we need to op-timize not only for an input and the oracle, but alsothe kind of annotation we would like to elicit.
Forexample given a particular translation model, we donot know if the most optimal thing at a given point isto seek more word-alignment annotation from a par-ticular ?alignment oracle?
or seek parse annotationfrom a ?parsing oracle?.Machine oracles vs Human oracles: The assump-tion with an oracle is that the knowledge and exper-tise of the oracle does not change over the course ofannotation.
We do not assume that the oracle learnsover time and hence the speed of annotation or per-haps the accuracy of annotation increases.
This ishowever very common with ?machine oracles?.
Forexample, an oracle that suggests automatic align-ment of data using statistical concordances may ini-tially be unreliable due to the less amount of data it istrained on, but as it receives more data, the estimatesget better and so the system gets more reliable.Evaluation: Performance of underlying system istypically done by well understood metrics like pre-cision/recall.
However, evaluation of MT outputis quite subjective and automatic evaluation met-rics may be too coarse to distinguish the nuancesof translation.
This becomes quite important inan online active learning setup, where we add an-notated data incrementally, and the immediatelytrained translation models are not sufficient to makea difference in the scores of the evaluation metric.ReferencesJason Baldridge and Miles Osborne.
2003.
Active learn-ing for hpsg parse selection.
In Proc.
of the HLT-NAACL 2003, pages 17?24, Morristown, NJ, USA.Association for Computational Linguistics.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
43rd ACL,pages 263?270, Morristown, NJ, USA.
Association forComputational Linguistics.Pinar Donmez and Jaime G. Carbonell.
2008.
Proactivelearning: cost-sensitive active learning with multipleimperfect oracles.
In CIKM ?08, pages 619?628, NewYork, NY, USA.
ACM.Rebecca Hwa.
2004.
Sample selection for statisticalparsing.
Comput.
Linguist., 30(3):253?276.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of the HLT/NAACL, Edomonton, Canada.Alon Lavie, Stephan Vogel, Lori Levin, Erik Peter-son, Katharina Probst, Ariadna Font Llitjo?s, RachelReynolds, Jaime Carbonell, and Richard Cohen.
2003.Experiments with a hindi-to-english transfer-based mtsystem under a miserly data scenario.
ACM Trans-actions on Asian Language Information Processing(TALIP), 2(2):143?163.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Ng.
2008.
Cheap and fast ?
but is it good?evaluating non-expert annotations for natural languagetasks.
In Proceedings of the EMNLP 2008, pages 254?263, Honolulu, Hawaii, October.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proc.
of ACL ?01,pages 523?530, Morristown, NJ, USA.
Association forComputational Linguistics.61
