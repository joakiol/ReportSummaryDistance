SPOKEN-LANGUAGE RESEARCH AT CARNEGIE MELLONRaj Reddy, Principal InvestigatorSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, Pennsylvania 15213PROJECT GOALSThe goal of speech research at Carnegie Mellon continuesto be the development of spoken language systems thateffectively integrate speech processing into the human-computer interface in a way that facilitates the use of com-puters in the performance of practical tasks.
Research inspoken language is currently focussed in the followingareas:?
Improved speech recognition technologies: Extend-ing the useful vocabulary of of SPHINX-II by use of bet-ter phonetic models and better search techniques,providing for rapid configuration for new tasks.?
Fluent human/machine interfaces: Developing anunderstanding of how people interact by voice withcomputer systems, in the context of Office Managementand other domains.?
Understanding spontaneous spoken language:Developing flexible parsing strategies to cope withphenomena peculiar to the lexical and grammaticalstructure of spoken language.
Development ofautomatictraining procedures for these grammars.?
Dialog modeling: Applying constraints based ondialog, semantic, and pragmatic knowledge to identifyand correct inaccurate portions of recognized utterances.?
Acoustical and environmental robustness: Develop-ing procedures to enable good recognition in office en-vironments with desktop microphones and a useful evelof recognition i  more severe nvironments.RECENT RESULTS?
The SPHINX-II system incorporated sex-dependentsemi-continuous hidden Markov models, a speaker-normalized front end using a codeword-dependentneural network, and shared-distribution phoneticmodels.?
Vocabulary-independent r cognition was improved byintroducing vocabulary-adapted decision trees andvocabulary-bias training, and by incorporating theCDCN and ISDCN acoustical pre-processing al-gorithms.?
SPHINX-II has been extended to the Wall Street JournalCSR task by incorporating a practical form of between-469word co-articulation modeling in the context of a moreefficient beam search.?
The Carnegie Mellon Spoken Language Shell wasreimplemented and additional applications for the OfficeManagement domain were developed, including atelephone dialer and voice editor.?
Grammatical coverage in the ATIS domain was ex-tended.
An initial set of tools was developed to createthe grammar in a semi-automatic fashion from a labelledcorpus.?
The MINDS-II system was developed which identifiesand reprocesses mis-recognized portions of a spoken ut-terance using semantics, pragmatics, inferred speaker in-tentions, and dialog structure in the context of a newly-developed finite-state recognizer.?
Acoustical pre-processing algorithms for environmentalrobustness were extended, made more efficient, anddemonstrated in the ATIS domain.
Pre-processing wascombined microphone arrays and with auditory modelsin pilot experiments.PLANS FOR THE COMING YEAR?
We will extend shared-distribution models to producesenonic baseforms, addressing the problem of new wordlearning and pronunciation optimization, and the thedecision-tree-based none will be made more general.The CDNN-based approach will be extended for bothspeaker and environment normalization.
The use oflong-distance semantic orrelations in language modelsto improve the prediction capability will be explored.?
We will incorporate confidence measures, audio feed-back, and the latest recognition technologies into theOffice Manager system.
We will investigate the be-havior of multi-modal systems that incorporate speechrecognition.?
We will develop architectures and automatic learningalgorithms for SLS systems with greater integration ofrecognition, parsing, and dialog and pragmatics.
Workwill be initiated on the identification of misunderstoodportions of a complete utterance, and the use of partialunderstanding and clarification dialogs.?
We will continue to develop parallel strategies forrobust speech recognition, and we will demonstratethese methods in more adverse acoustical environments.
