Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1660?1669, Dublin, Ireland, August 23-29 2014.Query-Focused Opinion Summarization for User-Generated ContentLu Wang1Hema Raghavan2Claire Cardie1Vittorio Castelli31Department of Computer Science, Cornell University, Ithaca, NY 14853, USA{luwang, cardie}@cs.cornell.edu2LinkedIn, CA, USAhraghavan@linkedin.com3IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USAvittorio@us.ibm.comAbstractWe present a submodular function-based framework for query-focused opinion summarization.
Within ourframework, relevance ordering produced by a statistical ranker, and information coverage with respect totopic distribution and diverse viewpoints are both encoded as submodular functions.
Dispersion functionsare utilized to minimize the redundancy.
We are the first to evaluate different metrics of text similarity forsubmodularity-based summarization methods.
By experimenting on community QA and blog summariza-tion, we show that our system outperforms state-of-the-art approaches in both automatic evaluation andhuman evaluation.
A human evaluation task is conducted on Amazon Mechanical Turk with scale, andshows that our systems are able to generate summaries of high overall quality and information diversity.1 IntroductionSocial media forums, such as social networks, blogs, newsgroups, and community question answering(QA), offer avenues for people to express their opinions as well collect other people?s thoughts on topicsas diverse as health, politics and software (Liu et al., 2008).
However, digesting the large amount ofinformation in long threads on newsgroups, or even knowing which threads to pay attention to, can beoverwhelming.
A text-based summary that highlights the diversity of opinions on a given topic canlighten this information overload.
In this work, we design a submodular function-based framework foropinion summarization on community question answering and blog data.Question: What is the long term effect of piracy on the music and film industry?Best Answer: Rising costs for movies and music.
...
If they sell less, they need to raise the price to make up for what they lost.
Theother thing will be music and movies with less quality.
...Other Answers:Ans1: Its bad... really bad.
(Just watch this movie and you will find out ... Piracy causes rappers to appear on your computer).Ans2: By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies.If they can?t protect their copyrights, they can?t continue to do business.
...Ans4: It is forcing them to rework their business model, which is a good thing.
In short, I don?t think the music industry in particularwill ever enjoy the huge profits of the 90?s.
...Ans6: Please-People in those businesses make millions of dollars as it is!!
I don?t think piracy hurts them at all!!
!Figure 1: Example discussion on Yahoo!
Answers.
Besides the best answer, other answers also containrelevant information (in italics).
For example, the sentence in blue has a contrasting viewpoint comparedto the other answers.Opinion summarization has previously been applied to restricted domains, such as product reviews (Huand Liu, 2004; Lerman et al., 2009) and news (Stoyanov and Cardie, 2006), where the output summaryis either presented in a structured way with respect to each aspect of the product or organized alongcontrastive viewpoints.
Unlike those works, we address user generated online data: community QA andblogs.
These forums use a substantially less formal language than news articles, and at the same timeaddress a much broader spectrum of topics than product reviews.
As a result, they present new challengesfor automatic summarization.
For example, Figure 1 illustrates a sample question from Yahoo!
Answers1along with the answers from different users.
The question receives more than one answer, and one ofthem is selected as the ?best answer?
by the asker or other participants.
In general, answers from otherusers also provide relevant information.
While community QA successfully pools rich knowledge fromthe wisdom of the crowd, users might need to seine through numerous posts to extract the informationThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1http://answers.yahoo.com/1660they need.
Hence, it would be beneficial to summarize answers automatically and present the summariesto users who ask similar questions in the future.
In this work, we aim to return a summary that encapsu-lates different perspectives for a given opinion question and a set of relevant answers or documents.In our work we assume that there is a central topic (or query) on which a user is seeking diverse opin-ions.
We predict query-relevance through automatically learned statistical rankers.
Our ranking functionnot only aims to find sentences that are on the topic of the query but also ones that are ?opinionated?through the use of several features that indicate subjectivity and sentiment.
The relevance score is en-coded in a submodular function.
Diversity is accounted for by a dispersion function that maximizes thepairwise distance between the pairs of sentences selected.Our chief contributions are:(1) We develop a submodular function-based framework for query-focused opinion summarization.
Tothe best of our knowledge, this is the first time that submodular functions have been used to supportopinion summarization.
We test our framework on two tasks: summarizing opinionated sentences incommunity QA (Yahoo!
Answers) and blogs (TAC-2008 corpus).
Human evaluation using Amazon Me-chanical Turk shows that our system generates the best summary 57.1% of the time.
On the other hand,the best answer picked by Yahoo!
users is chosen only 31.9% of the time.
We also obtain significanthigher Pyramid F1 score on the blog task as compared to the system of Lin and Bilmes (2011).
(2) Within our summarization framework, the statistically learned sentence relevance is included as partof our objective function, whereas previous work on submodular summarization (Lin and Bilmes, 2011)only uses ngram overlap for query relevance.
Additionally, we use Latent Dirichlet Allocation (Blei etal., 2003) to model the topic structure of the sentences, and induce clusterings according to the learnedtopics.
Therefore, our system is capable of generating summaries with broader topic coverage.
(3) Furthermore, we are the first to study how different metrics for computing text similarity or dis-similarity affect the quality of submodularity-based summarization methods.
We show empirically thatlexical representation-based similarity, such as TFIDF scores, uniformly outperforms semantic similar-ity computed with WordNet.
Moreover, when measuring the summary diversity, topical representationis marginally better than lexical representation, and both of them beats semantic representation.2 Related WorkOur work falls in the realm of query-focused summarization, where a user asks a question and the sys-tem generates a summary of the answers containing pertinent and diverse information.
A wide rangeof methods have been investigated, where relevance is often estimated through TF-IDF similarity (Car-bonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian modelover queries and documents (Daum?e and Marcu, 2006).
Most work only implicitly penalizes summaryredundancy, e.g.
by downweighting the importance of words that are already selected.Encouraging diversity of a summary has recently been addressed through submodular functions, whichhave been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al.,2012), and comments summarization (Dasgupta et al., 2013).
However, these works either ignore thequery information (when available) or else use simple ngram matching between the query and sentences.In contrast, we propose to optimize an objective function that addresses both relevance and diversity.Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004;Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editori-als (Paul et al., 2010).
Mostly, there is no query information, and summaries are formulated in a struc-tured way based on product features or contrastive standpoints.
Our work is more related to opinionsummarization on user-generated content, such as community QA.
Liu et al.
(2008) manually constructtaxonomies for questions in community QA.
Summaries are generated by clustering sentences accordingto their polarity based on a small dictionary.
Tomasoni and Huang (2010) introduce coverage and qualityconstraints on the sentences, and utilize an integer linear programming framework to select sentences.3 Submodular Opinion SummarizationIn this section, we describe how query-focused opinion summarization can be addressed by submodularfunctions combined with dispersion functions.
We first define our problem.
Then we introduce the1661Basic Features Sentiment Features- answer position in all answers/sentence position in blog - number/portion of sentiment words from a lexicon (Section 3.2)- length of the answer/sentence - if contains sentiment words with the same polarity as- length is less than 5 words sentiment words in queryQuery-Sentence Overlap Features Query-Independent Features- unigram/bigram TF/TFIDF similarity with query - unigram/bigram TFIDF similarity with cluster centroid- number of key phrases in the query that appear in the - sumBasic score (Nenkova and Vanderwende, 2005)sentence.
A model similar to that described in - number of topic signature words (Lin and Hovy, 2000)(Luo et al., 2013) was applied to detect key phrases.
- JS divergence with clusterTable 1: Features used for candidate ranking.
We use them for ranking answers in both community QAand blogs.components of our objective function (Sections 3.1?3.3).
The full objective function is presented inSection 3.4.
Lastly, we describe a greedy algorithm with constant factor approximation to the optimalsolution for generating summaries (Section 3.5).A set of documents or answers to be summarized are first split into a set of individual sentencesV = {s1, ?
?
?
, sn}.
Our problem is to select a subset S ?
V that maximizes a given objective functionf : 2V?
R within a length constraint: S?= argmaxS?Vf(S), subject to | S |?
c. | S | is the length ofthe summary S, and c is the length limit.Definition 1 A function f : 2V?
R is submodular iff for all s ?
V and every S ?
S??
V , it satisfiesf(S ?
{s})?
f(S) ?
f(S??
{s})?
f(S?
).Previous submodularity-based summarization work assumes this diminishing return property makessubmodular functions a natural fit for summarization and achieves state-of-the-art results on variousdatasets.
In this paper, we follow the same assumption and work with non-decreasing submodular func-tions.
Nevertheless, they have limitations, one of which is that functions well suited to modeling diversityare not submodular.
Recently, Dasgupta et al.
(2013) proved that diversity can nonetheless be encodedin well-designed dispersion functions which still maintain a constant factor approximation when solvedby a greedy algorithm.Based on these considerations, we propose an objective function f(S) mainly considering three as-pects: relevance (Section 3.1), coverage (Section 3.2), and non-redundancy (Section 3.3).
Relevanceand coverage are encoded in a non-decreasing submodular function, and non-redundancy is enforced bymaximizing the dispersion function.3.1 Relevance FunctionWe first utilize statistical rankers to produce a preference ordering of the candidate answers or sentences.We choose ListNet (Cao et al., 2007), which has been shown to be effective in many information retrievaltasks, as our ranker.
We use the implementation from Ranklib (Dang, 2011).Features used in the ranking algorithm are summarized in Table 1.
All features are normalized bystandardization.
Due to the length limit, we cannot provide the full results on feature evaluation.
Never-theless, we find that ranking candidates by TFIDF similarity or key phrases overlapping with the querycan produce comparable results with using the full feature set (see Section 5).We take the ranks output by the ranker, and define the relevance of the current summary S as: r(S) =?|S|i?rank?1i, where rankiis the rank of sentence siin V .
For QA answer ranking, sentences from thesame answer have the same ranking.
The function r(S) is our first submodular function.3.2 Coverage FunctionsTopic Coverage.
This function is designed to capture the idea that a comprehensive opinion sum-mary should provide thoughts on distinct aspects.
Topic models such as Latent Dirichlet Allocation(LDA) (Blei et al., 2003) and its variants are able to discover hidden topics or aspects of document col-lections, and thus afford a natural way to cluster texts according to their topics.
Recent work (Xie andXing, 2013) shows the effectiveness of utilizing topic models for newsgroup document clustering.
Wefirst learn an LDA model from the data, and treat each topic as a cluster.
We estimate a sentence-topicdistribution~?
for each sentence, and assign the sentence to the cluster k corresponding to the mode of thedistribution (i.e., k = argmaxi?i).
This naive approach produces comparable clustering performance tothe state-of-the-art according to (Xie and Xing, 2013).
T is defined as the clustering induced by our algo-rithm on the set V .
The topic coverage of the current summary S is defined as t(S) =?T?T?|S ?
T |.1662From the concavity of the square root it follows that sets S with uniform coverages of topics are preferredto sets with skewed coverage.Authorship Coverage.
This term encourages the summarization algorithm to select sentences fromdifferent authors.
Let A be the clustering induced by the sentence to author relation.
In communityQA, sentences from the answers given by the same user belong to the same cluster.
Similarly, sentencesfrom blogs with the same author are in the same cluster.
The authorship score is defined as a(S) =?A?A?|S ?A|.Polarity Coverage.
The polarity score encourages the selection of summaries that cover both positiveand negative opinions.
We categorize each sentence simply by counting the number of polarized wordsgiven by our lexicon.
A sentence belongs to a positive cluster if it has more positive words than negativeones, and vice versa.
If any negator co-occurs with a sentiment word (e.g.
within a window of size 5),the sentiment is reversed.2The polarity clustering P thus have two clusters corresponding to positiveand negative opinions.
The score is defined as p(S) =?P?P?| S ?
P |.
Our lexicon consists ofMPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli andSebastiani, 2006).
Words with conflicting sentiments from different lexicons are removed.Content Coverage.
Similarly to Lin and Bilmes (2011) and Dasgupta et al.
(2013), we use the followingfunction to measure content coverage of the current summary S: c(S) =?v?Vmin(cov(v, S), ?
?cov(v, V )), where cov(v, S) =?u?Ssim(v, u).
We experiment with two types of similarity functions.One is a Cosine TFIDF similarity score.
The other is a WordNet-based semantic similarity score betweenpairwise dependency relations from two sentences (Dasgupta et al., 2013).
Specifically, simSem(v, u) =?reli?v,relj?uWN(ai, aj) ?WN(bi, bj), where reli= (ai, bi), relj= (aj, bj), WN(wi, wj) is theshortest path length.
All scores are scaled onto [0, 1].3.3 Dispersion FunctionSummaries should contain as little redundant information as possible.
We achieve this by adding anadditional term to the objective function, encoded by a dispersion function.
Given a set of sentencesS, a complete graph is constructed with each sentence in S as a node.
The weight of each edge (u, v)is their dissimilarity d?
(u, v).
Then the distance between any pair of u and v, d(u, v), is defined as thetotal weight of the shortest path connecting u and v.3We experiment with two forms of dispersionfunction (Dasgupta et al., 2013): (1) hsum=?u,v?V,u6=vd(u, v), and (2) hmin= minu,v?V,u6=vd(u, v).Then we need to define the dissimilarity function d?
(?, ?).
There are different ways to measure thedissimilarity between sentences (Mihalcea et al., 2006; Agirre et al., 2012).
In this work, we experimentwith three types of dissimilarity functions.Lexical Dissimilarity.
This function is based on the well-known Cosine similarity score using TFIDFweights.
Let simtfidf(u, v) be the Cosine similarity between u and v, then we have d?Lex(u, v) =1?
simtfidf(u, v).Semantic Dissimilarity.
This function is based on the semantic meaning embedded in the dependencyrelations.
d?Sem(u, v) = 1 ?
simSem(v, u), where simSem(v, u) is the semantic similarity used incontent coverage measurement in Section 3.2.Topical Dissimilarity.
We propose a novel dissimilarity measure based on topic models.
Celikyilmazet al.
(2010) show that estimating the similarity between query and passages by using topic structurescan help improve the retrieval performance.
As discussed in the topic coverage in Section 3.2, eachsentence is represented by its sentence-topic distributions estimated by LDA.
For candidate sentence uand v, let their topic distributions be Puand Pv.
Then the dissimilarity between u and v can be definedas: d?Topic(u, v) = JSD(Pu||Pv) =12(?iPu(i) log2Pu(i)Pa(i)+?iPv(i) log2Pv(i)Pa(i)) where Pa(i) =12(Pu(i) + Pv(i)).3.4 Full Objective FunctionThe objective function takes the interpolation of the submodular functions and dispersion function:F(S) = r(S) + ?t(S) + ?a(S) + ?p(S) + ?c(S) + ?h(S).
(1)2There exists a large amount of work on determining the polarity of a sentence (Pang and Lee, 2008) which can be employedfor polarity clustering in this work.
We decide to focus on summarization, and estimate sentence polarity through sentimentword summation (Yu and Hatzivassiloglou, 2003), though we do not distinguish different sentiment words.3This definition of distance is used to produce theoretical guarantees for the greedy algorithm described in Section 3.5.1663The coefficients ?, ?, ?, ?, ?
are non-negative real numbers and can be tuned on a development set.4Notice that each summand except h(S) is a non-decreasing, non-negative, and submodular function,and summation preserves monotonicity, non-negativity, and submodularity.
Dispersion function h(s) iseither hsumor hminas introduced previously.3.5 Summary Generation via Greedy AlgorithmGenerating the summary that maximizes our objective function in Equation 1 is NP-hard (Chandra andHalld?orsson, 1996).
We choose to use a greedy algorithm that guarantees to obtain a constant factor ap-proximation to the optimal solution (Nemhauser et al., 1978; Dasgupta et al., 2013).
Concretely, startingwith an empty set, for each iteration, we add a new sentence so that the current summary achieves themaximum value of the objective function.
In addition to the theoretical guarantee, existing work (Mc-Donald, 2007) has empirically shown that classical greedy algorithms usually works near-optimally.4 Experimental Setup4.1 Opinion Question IdentificationWe first build a classifier to automatically detect opinion oriented questions in Community QA; questionsin the blog dataset are all opinionated.
Our opinion question classifier is trained on two opinion questiondatasets: (1) the first, from Li et al.
(2008a), contains 646 opinionated and 332 objective questions; (2)the second dataset, from Amiri et al.
(2013), consists of 317 implicit opinion questions, such as ?Whatcan you do to help environment?
?, and 317 objective questions.
We train a RBF kernel based SVMclassifier to identify opinion questions, which achieves F1 scores of 0.79 and 0.80 on the two datasetswhen evaluated using 10-fold cross-validation (the best F1 scores reported are 0.75 and 0.79).4.2 DatasetsCommunity QA Summarization: Yahoo!
Answers.
We use the Yahoo!
Answers dataset from Yahoo!WebscopeTMprogram,5which contains 3,895,407 questions.
We first run the opinion question classifierto identify the opinion questions.
For summarization purpose, we require each question having at least 5answers, with the average length of answers larger than 20 words.
This results in 130,609 questions.To make a compelling task, we reserve questions with an average length of answers larger than 50words as our test set for both ranking and summarization; all the other questions are used for training.
Asa result, we have 92,109 questions in the training set for learning the statistical ranker, and 38,500 in thetest set.
The category distribution of training and test questions (Yahoo!
Answers organizes the questionsinto predefined categories) are similar.
10,000 questions from the training set are further reserved as thedevelopment set.
Each question in the Yahoo!
Answers dataset has a user-voted best answer.
These bestanswers are used to train the statistical ranker that predicts relevance.
Separate topic models are learnedfor each category, where the category tag is provided by Yahoo!
Answer.Blog Summarization: TAC 2008.
We use the TAC 2008 corpus (Dang, 2008), which consists of 25topics.
23 of them are provided with human labeled nuggets, which TAC used in human evaluation.
TACalso provides snippets (i.e., sentences) that are frequently retrieved by participant systems or identifiedas relevant by human annotators.
We do not assume those snippets are known to any of our systems.4.3 ComparisonsFor both opinion summarization tasks, we compare with (1) the approach by Dasgupta et al.
(2013), and(2) the systems from Lin and Bilmes (2011) with and without query information.
The sentence clusteringprocess in Lin and Bilmes (2011) is done by using CLUTO (Karypis, 2003).
For the implementation ofsystems in Lin and Bilmes (2011) and Dasgupta et al.
(2013), we always use the parameters reported tohave the best performance in their work.For cQA summarization, we use the best answer voted by the user as a baseline.
Note that this is astrong baseline since all the other systems are unaware of which answer is the best.
For blog summa-rization, we have three additional baselines ?
the best systems in TAC 2008 (Kim et al., 2008; Li et al.,2008b), top sentences returned by our ranker, a baseline produced by TFIDF similarity and a lexicon4The values for the coefficients are 5.0, 1.0, 10.0, 5.0, 10.0 for ?, ?, ?, ?, ?, respectively, as tuned on the development set.5http://sandbox.yahoo.com/1664(henceforth called TFIDF+Lexicon).
In TFIDF+Lexicon, sentences are ranked by the TFIDF similar-ity with the query, and then sentences with sentiment words are selected in sequence.
This baseline aimsto show the performance when we only have access to lexicons without using a learning algorithm.5 Results5.1 Evaluating the RankerWe evaluate our ranker (described in Section 3.1) on the task of best answer prediction.
Table 2 comparesthe average precision and mean reciprocal rank (MRR) of our method to those of three baselines, (1)where answers are ranked randomly (Baseline (Random)), (2) by length (Baseline (Length)), and (3)by Jensen Shannon Divergence (JSD) with all answers.
We expect that the best answer is the one thatcovers the most information, which is likely to have a smaller JSD.
Therefore, we use JSD to rankanswers in the ascending order.
Table 2 manifests that our ranker outperforms all the other methods.Baseline (Random) Baseline (Length) JSD Ranker (ListNet)Avg Precision 0.1305 0.2834 0.4000 0.5336MRR 0.3403 0.4889 0.5909 0.6496Table 2: Performance for best answer prediction.
Our ranker outperforms the three baselines.5.2 Community QA SummarizationAutomatic Evaluation.
Since human written abstracts are not available for the Yahoo!
Answers dataset,we adopt the Jensen-Shannon divergence (JSD) to measure the summary quality.
Intuitively, a smallerJSD implies that the summary covers more of the content in the answer set.
Louis and Nenkova (2013)report that JSD has a strong negative correlation (Spearman correlation = ?0.737) with the overallsummary quality for multi-document summarization (MDS) on news articles and blogs.
Our task issimilar to MDS.
Meanwhile, the average JSD of the best answers in our test set is smaller than that ofthe other answers (0.39 vs. 0.49), with an average length of 103 words compared with 67 words for theother answers.
Also, on the blog task (Section 5.3), the top two systems by JSD also have the top twoROUGE scores (a common metric for summarization evaluation when human-constructed summariesare available).
Thus, we conjecture that JSD is a good metric for community QA summaries.Table 3 (left) shows that our system using a content coverage function based on Cosine using TFIDFweights, and a dispersion function (hsum) based on lexicon dissimilarity and 100 topics, outperforms allof the compared approaches (paired-t test, p < 0.05).
The topic number is tuned on the development set,and we find that varying the number of topics does not impact performance too much.
Meanwhile, bothour system and Dasgupta et al.
(2013) produce better JSD scores than the two variants of the Lin andBilmes (2011) system, which implies the effectiveness of the dispersion function.
We further examine theeffectiveness of each component that contributes to the objective function (Section 3.4), and the resultsare shown in Table 3 (right).Length100 200Best answer 0.3858 -Lin and Bilmes (2011) 0.3398 0.2008Lin and Bilmes (2011) + q 0.3379 0.1988Dasgupta et al.
(2013) 0.3316 0.1939Our system 0.3017 0.1758JSD100JSD200Rel(evance) 0.3424 0.2053Rel + Aut(hor) 0.3375 0.2040Rel + Aut + TM (Topic Models) 0.3366 0.2033Rel + Aut + TM + Pol(arity) 0.3309 0.1983Rel + Aut + TM + Pol + Cont(ent Coverage) 0.3102 0.1851Rel + Aut + TM + Pol + Cont + Disp(ersion) 0.3017 0.1758Table 3: [Left] Summaries evaluated by Jensen-Shannon divergence (JSD) on Yahoo Answer for sum-maries of 100 words and 200 words.
The average length of the best answer is 102.70.
[Right] Valueaddition of each component in the objective function.
The JSD on each line is statistically significantlylower than the JSD on the previous (?
= 0.05).Human Evaluation.
Human evaluation for Yahoo!
Answers is carried out on Amazon Mechanical Turk6with carefully designed tasks (or ?HITs?).
Turkers are presented summaries from different systems in arandom order, and asked to provide two rankings, one for overall quality and the other for informationdiversity.
We indicate that informativeness and non-redundancy are desirable for quality; however, Turk-ers are allowed to consider other desiderata, such as coherence or responsiveness, and write down thosewhen they submit the answers.
Here we believe that ranking the summaries is easier than evaluating eachsummary in isolation (Lerman et al., 2009).6https://www.mturk.com/mturk/1665We randomly select 100 questions from our test set, each of which is evaluated by 4 distinct Turkerslocated in United States.
40 HITs are thus created, each containing 10 different questions.
Four systemsummaries (best answer, Dasgupta et al.
(2013), and our system with 100 and 200 words respectively) aredisplayed along with one noisy summary (i.e.
irrelevant to the question) per question in random order.7We reject Turkers?
HITs if they rank the noisy summary higher than any other.
Two duplicate questionsare added to test intra-annotator agreement.
We reject HITs if Turkers produced inconsistent rankingsfor both duplicate questions.
A total of 137 submissions of which 40 HITs pass the above quality filters.Turkers of all accepted submissions report themselves as native English speakers.
An inter-rater agree-ment of Fleiss?
?
of 0.28 (fair agreement (Landis and Koch, 1977)) is computed for quality ranking and?
is 0.43 (moderate agreement) for diversity ranking.
Table 4 shows the percentage of times a particularmethod is picked as the best summary, and the macro-/micro-average rank of a method, for both overallquality and information diversity.
Macro-average is computed by first averaging the ranks per questionand then averaging across all questions.For overall quality, our system with a 200 word limit is selected as the best in 44.6% of the evaluations.It outperforms the best answer (31.9%) significantly, which suggests that our system summary covers rel-evant information that is not contained in the best answer.
Our system with a length constraint of 100words is chosen as the best for quality 12.5% times while that of Dasgupta et al.
(2013) is chosen 11.0%of the time.
Our system is also voted as the best summary for diversity in 78.7% of the evaluations.
Moreinterestingly, both of our systems, with 100 words and 200 words, outperform the best answer and Das-gupta et al.
(2013) for average ranking (both overall quality and information diversity) significantly byusing Wilcoxon signed-rank test (p < 0.05).
When we check the reasons given by Turkers, we found thatpeople usually prefer our summaries due to ?helpful suggestions that covered many options?
or being?balanced with different opinions?.
When Turks prefer the best answers, they mostly stress on coherenceand responsiveness.
Sample summaries from all the systems are displayed in Figure 2.Length of Summary Overall Quality Information Diversity% Average Rank % Average RankBest Macro Micro Best Macro MicroBest answer 102.70 31.9% 2.68 2.69 9.6% 3.27 3.29Dasgupta et al.
(2013)10011.0% 2.84 2.83 5.0% 2.95 2.94Our system 12.5% 2.50?2.50?6.7% 2.43?2.43?Our system 200 44.6% 1.98?1.98?78.7% 1.35?1.34?Table 4: Human evaluation on Yahoo!
Answer Data.
Boldface implies statistically significance com-pared to other results in the same columns using paired-t test.
Both of our systems are ranked higher(i.e.
numbers in bold with?)
than the best answers voted by Yahoo!
users and system summaries fromDasgupta et al.
(2013).Question: What is the long term effect of piracy on the music and film industry?Dasgupta et al.
(2013) (Qty Rank=2.75 Div.
Rank=2.5):?In short, I don?t think the music industry in particular will ever enjoy the huge profits of the 90?s.
?Please-People in those businesses make millions of dollars as it is !!
I don?t think piracy hurts them at all !!!
?The other thing will be music and movies with less quality.
?Its a big gray area, I dont see anything wrong with burning a mix cd or a cd for a friend so long as youre not selling them for profit.
?By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies.Our system (100 words) (Qty Rank=2.25 Div.
Rank=2.25):?Rising costs for movies and music.
The other thing will be music and movies with less quality.
?Now, with piracy, there isn?t the willingness to take chances.
?But it?s also like the person put the effort into it and they aren?t getting paid.
It?s a big gray area, I don?t see anything wrong with burning a mix cdor a cd for a friend so long as you?re not selling them for profit.
?It is forcing them to rework their business model, which is a good thing.Our system (200 words) (Qty.
Rank=2.25, Div Rank=1.25):?Rising costs for movies and music.
The other thing will be music and movies with less quality.
?Now, with piracy, there isn?t the willingness to take chances.
American Idol is the result of this.
....
The real problem here is that the mainstreammusic will become even tighter.
Record labels will not won?t to go far from what is currently like by the majority.
?I hate when people who have billions of dollars whine about not having more money.
But it?s also like the person put the effort into it and theyaren?t getting paid ...
I don?t see anything wrong with burning a mix cd or a cd for a friend ....?It is forcing them to rework their business model, which is a good thing.
?By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies.Figure 2: Sample summaries from Dasgupta et al.
(2013), and our systems (100 words and 200 words).Sentences from separate bullets (?)
are partial answers from different users.7Note that we aim to compare results with the gold-standard best answers of about 100 words.
The evaluation of the200-word summaries is provided only as an additional data-point.16665.3 Blog SummarizationAutomatic Evaluation.
We use the ROUGE (Lin and Hovy, 2003) software with standard options toautomatically evaluate summaries with reference to the human labeled nuggets as those are availablefor this task.
ROUGE-2 measures bigram overlap and ROUGE-SU4 measures the overlap of unigramand skip-bigram separated by up to four words.
We use the ranker trained on Yahoo!
data to producerelevance ordering, and adopt the system parameters from Section 5.2.
Table 5 (left) shows that oursystem outperforms the best system in TAC?08 with highest ROUGE-2 score (Kim et al., 2008), the twobaselines (TFIDF+Lexicon, and our ranker), Lin and Bilmes (2011), and Dasgupta et al.
(2013).ROUGE-2 ROUGE-SU4 JSDBest system in TAC?08 0.2923 0.3766 0.3286TFIDF + Lexicon 0.3069 0.3876 0.2429Ranker (ListNet) 0.3200 0.3960 0.2293Lin and Bilmes (2011) 0.2732 0.3582 0.2330Lin and Bilmes (2011) + q 0.2852 0.3700 0.2349Dasgupta et al.
(2013) 0.2618 0.3500 0.2370Our system 0.3234 0.3978 0.2258Pyramid F-scoreBest system in TAC?08 0.2225Lin and Bilmes (2011) 0.2790Our system 0.3620Table 5: Results on TAC?08 dataset.
[Left] Our system has significant better ROUGE scores than allthe other systems except our ranker (paired-t test, p < 0.05).
We also achieve the best JS divergence.
[Right] Human evaluation with Pyramid F-score.
Our system significantly outperforms the others.Human Evaluation.
For human evaluation, we use the standard Pyramid F-score used in the TAC?08opinion summarization track with ?
= 3 (Dang, 2008).
In the TAC task, systems are allowed to return upto 7,000 non-white characters for each question.
Since the TAC metric favors recall we do not producesummaries shorter than 7,000 characters.
We ask two human judges to evaluate our system along withthe one that got the highest Pyramid F-score in the TAC?08 and Lin and Bilmes (2011).
Cohen?s ?
forinter-annotator agreement is 0.68 (substantial).
While we did not explicitly evaluate non-redundancy,both of our judges report that our system summaries contain less redundant information.5.4 Further DiscussionYahoo!
AnswerDISPERSIONsumDISPERSIONminDISSIMI ConttfidfContsemConttfidfContsemSemantic 0.3143 0.324 3 0.3129 0.3232Topical 0.3101 0.3202 0.3106 0.3209Lexical 0.3017 0.3147 0.3071 0.3172TAC 2008DISPERSIONsumDISPERSIONminDISSIMI ConttfidfContsemConttfidfContsemSemantic 0.2216 0.2169 0.2772 0.2579Topical 0.2128 0.2090 0.3234 0.3056Lexical 0.2167 0.2129 0.3117 0.3160Table 6: Effect of different dispersion functions, content coverage, and dissimilarity metrics on oursystem.
[Left] JSD values for different combinations on Yahoo!
data, using LDA with 100 topics.All systems are significantly different from each other at significance level ?
= 0.05.
Systems usingsummation of distances for dispersion function (hsum) uniformly outperform the ones using minimumdistance (hmin).
[Right] ROUGE scores of different choices for TAC 2008 data.
All systems use LDAwith 40 topics.
The parameters of our systems are adopted from the ones tuned on Yahoo!
Answers.Given that the text similarity metrics and dispersion functions play important roles in the framework,we further study the effectiveness of different content coverage functions (Cosine using TFIDF vs. Se-mantic), dispersion functions (hsumvs.
hmin), and dissimilarity metrics used in dispersion functions(Semantic vs. Topical vs. Lexical).
Results on Yahoo!
Answer (Table 6 (left)) show that systems usingsummation of distances for dispersion functions (hsum) uniformly outperform the ones using minimumdistance (hmin).
Meanwhile, Cosine using TFIDF is better at measuring content coverage than WordNet-based semantic measurement, and this may due to the limited coverage of WordNet on verbs.
This is alsotrue for dissimilarity metrics.
Results on blog data (Table 6 (right)), however, show that using minimumdistance for dispersion produces better results.
This indicates that optimal dispersion function varies bygenre.
Topical-based dissimilarity also marginally outperforms the other two metrics in blog data.6 ConclusionWe propose a submodular function-based opinion summarization framework.
Tested on community QAand blog summarization, our approach outperforms state-of-the-art methods that are also based on sub-modularity in both automatic evaluation and human evaluation.
Our framework is capable of includingstatistically learned sentence relevance and encouraging the summary to cover diverse topics.
We alsostudy different metrics on text similarity estimation and their effect on summarization.1667ReferencesEneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre.
2012.
Semeval-2012 task 6: A pilot on seman-tic textual similarity.
In Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation(SemEval 2012), pages 385?393, Montr?eal, Canada, 7-8 June.
Association for Computational Linguistics.Hadi Amiri, Zheng-Jun Zha, and Tat-Seng Chua.
2013.
A pattern matching based model for implicit opinionquestion identification.
In AAAI.
AAAI Press.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003.
Latent dirichlet allocation.
J. Mach.
Learn.
Res.,3:993?1022, March.Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li.
2007.
Learning to rank: From pairwise approachto listwise approach.
In Proceedings of the 24th International Conference on Machine Learning, ICML ?07,pages 129?136, New York, NY, USA.
ACM.Jaime Carbonell and Jade Goldstein.
1998.
The use of mmr, diversity-based reranking for reordering documentsand producing summaries.
In Proceedings of the 21st Annual International ACM SIGIR Conference on Researchand Development in Information Retrieval, SIGIR ?98, pages 335?336, New York, NY, USA.
ACM.Asli Celikyilmaz, Dilek Hakkani-Tur, and Gokhan Tur.
2010.
Lda based similarity modeling for question answer-ing.
In Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, SS ?10, pages 1?9, Stroudsburg,PA, USA.
Association for Computational Linguistics.Barun Chandra and Magn?us M. Halld?orsson.
1996.
Facility dispersion and remote subgraphs.
In Proceedingsof the 5th Scandinavian Workshop on Algorithm Theory, SWAT ?96, pages 53?65, London, UK, UK.
Springer-Verlag.Hoa Tran Dang.
2008.
Overview of the tac 2008 opinion question answering and summarization tasks.
In Proc.TAC 2008.Van Dang.
2011.
RankLib.
http://www.cs.umass.edu/?vdang/ranklib.html.Anirban Dasgupta, Ravi Kumar, and Sujith Ravi.
2013.
Summarization through submodularity and dispersion.In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers), pages 1014?1022, Sofia, Bulgaria, August.
Association for Computational Linguistics.Hal Daum?e, III and Daniel Marcu.
2006.
Bayesian query-focused summarization.
In Proceedings of the 21stInternational Conference on Computational Linguistics and the 44th Annual Meeting of the Association forComputational Linguistics, ACL-44, pages 305?312, Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sentiwordnet: A publicly available lexical resource for opinionmining.
In In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC06, pages417?422.Minqing Hu and Bing Liu.
2004.
Mining and summarizing customer reviews.
In Proceedings of the Tenth ACMSIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ?04, pages 168?177, NewYork, NY, USA.
ACM.George Karypis.
2003.
CLUTO - a clustering toolkit.
Technical Report #02-017, November.Hyun Duk Kim, Dae Hoon Park, V.G.Vinod Vydiswaran, and ChengXiang Zhai.
2008.
Opinion summarizationusing entity features and probabilistic sentence coherence optimization: Uiuc at tac 2008 opinion summarizationpilot.
In Proc.
TAC 2008.J R Landis and G G Koch.
1977.
The measurement of observer agreement for categorical data.
Biometrics,33(1):159?174.Kevin Lerman, Sasha Blair-Goldensohn, and Ryan McDonald.
2009.
Sentiment summarization: Evaluating andlearning user preferences.
In Proceedings of the 12th Conference of the European Chapter of the Association forComputational Linguistics, EACL ?09, pages 514?522, Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Baoli Li, Yandong Liu, and Eugene Agichtein.
2008a.
Cocqa: Co-training over questions and answers with anapplication to predicting question subjectivity orientation.
In EMNLP, pages 937?946.Wenjie Li, You Ouyang, Yi Hu, and Furu Wei.
2008b.
Polyu at tac 2008.
In Proc.
TAC 2008.1668Hui Lin and Jeff Bilmes.
2011.
A class of submodular functions for document summarization.
In Proceedingsof the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -Volume 1, HLT ?11, pages 510?520, Stroudsburg, PA, USA.
Association for Computational Linguistics.Chin-Yew Lin and Eduard Hovy.
2000.
The automated acquisition of topic signatures for text summarization.COLING ?00, pages 495?501, Stroudsburg, PA, USA.
Association for Computational Linguistics.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evaluation of summaries using n-gram co-occurrence statistics.In Proceedings of the 2003 Conference of the North American Chapter of the Association for ComputationalLinguistics on Human Language Technology - Volume 1, pages 71?78.Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin, Dingyi Han, and Yong Yu.
2008.
Understanding and sum-marizing answers in community-based question answering services.
In Proceedings of the 22Nd InternationalConference on Computational Linguistics - Volume 1, COLING ?08, pages 497?504, Stroudsburg, PA, USA.Association for Computational Linguistics.Annie Louis and Ani Nenkova.
2013.
Automatically assessing machine summary content without a gold standard.Comput.
Linguist., 39(2):267?300, June.Xiaoqiang Luo, Hema Raghavan, Vittorio Castelli, Sameer Maskey, and Radu Florian.
2013.
Finding what mattersin questions.
In HLT-NAACL, pages 878?887.Ryan McDonald.
2007.
A study of global inference algorithms in multi-document summarization.
ECIR?07,pages 557?564, Berlin, Heidelberg.
Springer-Verlag.Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006.
Corpus-based and knowledge-based measures oftext semantic similarity.
In Proceedings of the 21st National Conference on Artificial Intelligence - Volume 1,AAAI?06, pages 775?780.
AAAI Press.G.
L. Nemhauser, L. A. Wolsey, and M. L. Fisher.
1978.
An analysis of approximations for maximizing submod-ular set functionsI.
Mathematical Programming, 14(1):265?294, December.Ani Nenkova and Lucy Vanderwende.
2005.
The impact of frequency on summarization.
Microsoft Research,Redmond, Washington, Tech.
Rep. MSR-TR-2005-101.Bo Pang and Lillian Lee.
2008.
Opinion mining and sentiment analysis.
Found.
Trends Inf.
Retr., 2(1-2):1?135,January.Michael J. Paul, ChengXiang Zhai, and Roxana Girju.
2010.
Summarizing contrastive viewpoints in opinionatedtext.
In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP?10, pages 66?76, Stroudsburg, PA, USA.
Association for Computational Linguistics.Ruben Sipos, Pannaga Shivaswamy, and Thorsten Joachims.
2012.
Large-margin learning of submodular summa-rization models.
EACL ?12, pages 224?233, Stroudsburg, PA, USA.
Association for Computational Linguistics.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie.
1966.
The General Inquirer: AComputer Approach to Content Analysis.
MIT Press, Cambridge, MA.Veselin Stoyanov and Claire Cardie.
2006.
Partially supervised coreference resolution for opinion summarizationthrough structured rule learning.
In Proceedings of the 2006 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?06, pages 336?344, Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Mattia Tomasoni and Minlie Huang.
2010.
Metadata-aware measures for answer summarization in communityquestion answering.
In Proceedings of the 48th Annual Meeting of the Association for Computational Linguis-tics, ACL ?10, pages 760?769, Stroudsburg, PA, USA.
Association for Computational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005.
Recognizing contextual polarity in phrase-level senti-ment analysis.
In Proceedings of the Conference on Human Language Technology and Empirical Methods inNatural Language Processing, HLT ?05, pages 347?354, Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Pengtao Xie and Eric Xing.
2013.
Integrating document clustering and topic modeling.
In Proceedings of theTwenty-Ninth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-13), pages 694?703, Corvallis, Oregon.
AUAI Press.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towards answering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sentences.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP).1669
