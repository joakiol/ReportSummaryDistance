ESTIMATING THE TRUE PERFORMANCE OFCLASSIFICATION-BASED NLP TECHNOLOGYJames R. NolanSiena CollegeLoudonville, NY 12211jnolan@siena.eduAbstractMany of the tasks associated with naturallanguage processing (NLP) can be viewed asclassification problems.
Examples are thecomputer grading of student writing samplesand speech recognition systems.
If we acceptthis view, then the objective of learningclassifications from sample text is to classify andpredict successfully on new text.
While successin the marketplace an be said to be the ultimatetest of validation for NLP systems, this successis not likely to be achieved unless appropriatetechniques are used to validate the prototype.This paper discusses useful validationtechniques for classification-based NLP systemsand how these techniques may be used toestimate the true performance ofthe system.INTRODUCTIONThe objective of learning classifications fromsample text is to classify and predict successfidlyon new text.
For example, in developing asystem for grading student writing samples, theobjective is to learn how to classify studentwriting samples into grade categories so that wemay use the system to predict successfully thegrade categories for new samples of studentwriting (Nolan, 1997a).The most commonly used measure ofsuccess or failure is a classifier's error rate(Weiss & Kulikowski, 1991).
Each .time theclassifier is presented with a case, it makes adecision about he appropriate class for the case.Sometimes it is right; sometimes it is wrong.The true error rate is statistically defined as theerror rate of the classifier on a large number ofnew cases that converge in the limit to the actualpopulation distribution.If we were given an unlimited numberof cases, the true error rate could be readilycomputed as the number of samples approachedinfinity.
In the real world, the number ofsamples is always finite, and typically relativelysmall.
The major question is then whether it ispossible to extrapolate from empirical error ratescalculated from small sample results to the trueerror rate.
It turns out that there are a number ofways of presenting sample cases to a classifier toget better estimates of the true error rate.
Someof these techniques are better than others.
Instatistical terms, some estimators of the trueerror rate are considered biased.
They tend toestimate too low, i.e., on the optimistic side, ortoo high, i.e., on the pessimistic side.In the next section, we will define justwhat an error is when using classificationsystems for natural language processing.
Theapparent error rate will be contrasted with thetrue error rate.
The effect of classifiercomplexity and feature dimensionality onclassification results will be followed byconclusions.WHAT IS AN ERROR?An error is simply a misclassification: theclassifier is presented a case, and it classifies thecase incorrectly.
If all errors are of equalimportance, a single error rate, calculated asfollows,number of errorserror rate --number of casessummarizes the overall performance of aclassifier.
However, for many applications,distinctions among different types of errors turnout to be important.
For example, the errorcommitted in tentatively diagnosing someone ashealthy when one has a life-threatening illness(known as a false negative decision) is usuallyconsidered far more serious than the oppositetype of error - of diagnosing someone as illwhen one is in fact healthy (known as a falsepositive).
Further tests and the passage of timewill frequently correct he misdiagnosis of the23healthy person without any permanent damage(except possibly to one's pocket book), whereasan illness left untreated will probably get worseand lead to more serious problems, even death.Although not usually a life and death decision,classifying a student's writing sample can resultin the same type of false negative and falsepositive rrors.Let us suppose the writing sampleevaluation is being made to help determinewhether the student will be placed into aprogram designed to help poor writers toimprove their writing skills.
In this case, as inthe previous one, there are two errors that can bemade.
The evaluation of the writing samplecould indicate that the student should not needto be placed in the special writing programwhen in fact they are deficient in writing skills(a false negative).
Or the evaluation couldindicate that the student shouM be placed in thespecial writing program when the student'swriting skills are really at a level indicating heor she does not need extra help (false positive).The question is whether the two typesof errors committed in the writing sampleevaluation scenario - false negative and falsepositive errors, respectively - are of the sameconsequence.
If they are not, then we mustextend our definition of error.Costs and RisksA natural alternative to an error rate aspreviously defined is a misclassification cost(lVlachina, 1987).
Here, instead of designing aclassifier to minimize rror rates, the goal wouldbe to minimize misclassification costs.
Amisclassification cost is simply a number that isassigned as a penalty for making a mistake.
Forexample, in the two-class ituation, a cost of onemight be assigned to a false positive rror and acost of two to a false negative rror.
An averagecost of misclassitication can be obtained byweighing each of the costs by the respectiveerror rate.
Computation,ally, this means that theerrors are converted into costs by multiplying anerror by its misclassification cost.
The effect ofhaving false negatives cost twice what falsepositives cost will be to tolerate many more falsepositive errors than false negative ones for afixed classifier design.
If an optimal decision-making strategy is followed, cost choices have adirect effect on decision thresholds and resultingerror rates.If we assign a cost to each type of erroror misclassification, the total cost ofmisclassification is most directly computed asthe sum of the costs for each error.
If allmisclassifications are assigned a cost of 1, thetotal cost is given by the number of errors, andthe average cost per decision is the error rate.
Byraising or lowering the cost of misclassification,we are biasing decisions in different directions,as if there were more or fewer cases in a givenclass.
Formally, ff i is the predicted class and j isthe true class, then for n classes, the total cost ofmisclassification isn nCost = Z E Eij Ciji= l j= lwhere Eq is the number of errors and Cq is thecost for that type misclassification.
Of course,the cost of a correct classification (Cq, for i=j) is0.For example, using the data in Figure1, ff the cost of misclassifying a class 1 case is 1,and the cost of miselassifying a class 2 case is 2,then the total cost of the classifier is (14 * 1) +(6 * 2) = 26 and the average cost per decision is26/106 = .25.
This is quite different from theresult if costs had been equal and set to 1, whichwould have yielded a total cost of merely 20, andan average cost per decision of .19 (Weiss &Kulikowski, 1991).True ClassPredicted Class 1 21 71 62 14 15Figure 1: Sample Classification ResultsWe have so far considered the costs ofmisclassifications, but not the potential forexpected gains arising from correctclassification.
In risk armlysis or decisionanalysis, both costs (or losses) and benefits(gains) are used to evaluate the performance ofaclassifier.
A rational objective of the classifier isto maximize gains.
The expected gain or loss isthe difference between the gains for correctclassifications and losses for incorrectclassifications.Instead of costs, we can call thenumbers risks.
If misclassification costs are24assigned as negative numbers, and gains fromcorrect classification as positive numbers, thenwe can express the total risk asn nRisk =g g E~jR~i=l j=lwhere Eq is once again the number of errors andR e is the risk of classifying a case that trulybelongs in class j into class i.Costs and risks can all be employed inconjunction with error rate analysis.
In someways, they can be viewed as modified errorrates.
If conventionally agreed upon units, suchas monetary costs, are available to measure thevalue of a quantity, then a good case can bemade for the usefulness of basing a decisionsystem on these alternatives as opposed to onebased irectly on error rates.
The implication forclassification-based NLP is that attention mustbe paid to the context of the particularapplication as regards the costs and risksassociated with the possible errors inclassification.APPARENT VS.
TRUE ERROR RATEAs stated earlier, the true error rate of aclassifier is defined as the error rate of theclassifier ff it was tested on the true distributionof cases in the population - which can beempirically approximated by a very largenumber of new cases gathered independentlyfrom the cases used to design the classifier.The apparent error rate of a classifier isthe error rate of the classifier on the samplecases that were used to design or build thesystem.
In general, the apparent error rates tendto be biased optimistically.
The true error rate isalmost invariably higher than the apparent errorrate.
This happens when the classifier has beenoverfitted (or overspecialized) to the particularcharacteristics of the sample data (Ripley,1996).It is useless to design a classifier thatdoes well on the design sample, but does poorlyon new cases.
And unfortunately, as justmentioned, using solely the apparent error toestimate future performance can often lead todisastrous results on new data.
To illustrate this,we can look at an example from speechrecognition.
Any novice could design a classifierwith a zero apparent error rate simply by using adirect table lookup approach as illustrated inFigure 2.
A sample of one individual's peechand pronunciation patterns become theclassifier.
When trying to interpret a spokenword from this individual, we would just lookupthe answer (classification) in the tablecontaining their speech patterns.If we test on the original speech data,and no pattern is repeated for different classes,we never make a mistake.
Unfortunately, whenwe bring in new speech data (another person'sspeech), the odds of finding the individual casein the aforementioned table are extremelyremote because of the enormous number ofpossible combinations of speech features.~ Decision by \[Table Lookup\[ ~of Original \]~q-"lCases ISamples IFigure 2: Classification by Table LookupThe nature of this problem, which isillustrated most easily with the table lookupapproach, is caused by overfitting the speechclassifier to the data.
Basing our estimate ofperformance of this classifier on the apparenterror rate leads to similar problems.
While thetable lookup is an extreme xample, the extentto which classification methods arc susceptibleto overfitting varies.
Many a learning systemdesigner has been lulled into a false sense ofsecurity by the mirage of low apparent errorrates.This problem is of particular concernwhen analyzing student writing samples wherethe odds of finding a writing sample identical toone in the test sample are extremely remotebecause of the enormous number of possiblecombinations ofwriting features.Fortunately, there are very effectivetechniques for guaranteeing good properties inthe estimates ofa true error rate even for a smallsample.
While these techniques can measure theperformance of a classifier, they do notguarantee that the apparent error rate is close tothe true error rate for a given application.The requirement for any model of trueerror estimation is that the sample data are arandom sample.
This means that the sample(s)should not be preselected in any way.
The25concept of randomness i very important inobtaining a good estimate of the true error rate.A computer classifieation-based NLP system isalways at the mercy of the design samplessupplied to it.
Without a random sample, theerror rate estimates can be compromised, oralternatively, they will apply to a differentpopulation than intended.Train and Test Error Rate EstimationMany researchers have employed the train-and-test paradigm for estimating the true error rate(Nolan, 1997b).
This involves splitting thesample into two groups.
One group is called thetraining set and the other the testing set.
Thetraining set is used to design the classifier, andthe testing set is used strictly for testing.
If we"hide" or "hold out" the test cases and only lookat them after the classifier design is complete,then we have a direct procedural correspondenceto the task of determining the error rate on newcases.
The error rate of the classifier on the testcases is called the test sample rror rate.As usual, the two sets of cases shouldbe random samples from some population.
Inaddition, the case.s in the two sample sets shouldbe independent.
By independent, we mean thatthere is no relationship among them other thanthat they are samples from the same population.To ensure that they are independent, they mightbe gathered at different imes or by differentresearchers.A question that arises with the train-and-test error rate estimation technique can bestated as: "How many test cases are needed forthe test sample error rate to be essentially thetrue error rate?"
The answer is: a surprisinglysmall number.
Moreover, based on the testsample size, we know how far off the testsample estimate can be.
These estimations canbe derived from basic probability theory.Specifically, the accuracy of error rate estimatesfor a specific classifier on independent andrandomly drawn test samples is governed by thebinomial distribution.
While a demonstration fthe use of the binomial distribution is not shownhere, it should be emphasized that the quality ofthe test sample stimate isdirectly dependent onthe number of test cases.
When the test samplesize reaches 1000, the estimates are extremelyaccurate.
At sample size 5000, the test sampleestimate is virtually identical to the true errorrate.Random ResamplingA single random partition of the data set can bemisleading for small or moderately sizedsamples.
In such cases, multiple train-and-testexperiments can do better.
When multiple train-and-test experiments are performed, a newclassifier is learned from each training sample.The estimatod error rate is the average of theerror rates for classifiers derived for theindependently and randomly generated testspartitions.
Random subsampling can producebetter error estimates than a single train-and-testpartition.A special case of resampling is knownas leaving-one-out (Lachenbruch & Mickey,1968).
For a given method and sample size, n, aclassifier is generated using (n-l) cases andtested on the remaining case.
This is repeated ntimes, each time designing a classifier byleaving-one-out.
Thus each ease in the sample isused as a test case, and each time nearly alleases are used to design a classifier.
The errorrate is the number of errors on the single testcases divided by n.Leaving-one-out is an elegant andstraightforward technique for estimatingclassifier error rates.
The leaving-one-outestimator is an almost unbiased estimator of thetrue error rate of a classifier.
This means thatover many different sample sets of size n, theleaving-one-out estimate will average out to thetrue error rate.
Suppose you are given 100sample sets of 50 eases each.
The average of theleaving-one-out estimates for each of the 100sample sets will be very close to the true errorrate.
Because the leaving-one-out estimator isunbiased, for even modest sample sizes of over100, the estimate should be accurate.The great advantage oftlus technique isthat all the cases in the available sample areused for testing, and almost all the cases are alsoused for training the classifier.
In addition,much smaller sample sizes than those requiredin the train-test method can lead to very accurateestimation.
There is an increased computationalcost, however.BootstrappingAlthough the leaving-one-out error rateestimator is an almost unbiased estimator of thetrue error rate of a classifier, there are26difficulties with this technique.
Both the biasand variance of an error estimator contribute tothe inaccuracy and imprecision of the error rateestimate.
While leafing-one.out is nearlyunbiased, its variance is high for small samples.A more recently discovered resamplingmethod, called bootstrapping, has shown muchpromise as an error rate estimator (Efron, 1983).There are numerous bootstrap estimators.
Wewill discuss one, called the e0 bootstrapestimator.
For this, a training group consists ofn cases ampled with replacement from a size nsample.
Sampled with replacement means thatthe training samples are drawn from the data setand placed back after they are used, so theirrepeated use is allowed.
Cases not found in thetraining group form the test group.
Theestimated error rate is the average of the errorrates over a number of iterations.
About 200iterations for bootstrap estimates are considerednecessary to obtain a good estimate.
Thus, this iscomputationally considerably more expensivethan leaving-one-out.CLASSIFIER COMPLEXITY ANDFEATURE DIMENSIONALITYIntuitively, one expects that the moreinformation that is available, the better oneshould do.
The more knowledge we have, thebetter we can make decisions.
Similarly, onemight expect hat a theoretically more powerfulclassification method should work better inpractice.
Surprisingly, in practice, both of theseexpectations are wrong (Wallace & Freeman,1987).Most classification methods involvecompromises.
They make some assumptionsabout the population distribution and about hedecision process fitting a specific type ofrepresentation.
The samples, however, are oftentreated as a somewhat mysterious collection.The features thought o differentiate the objectclasses have been preselected (hopefully by anexperienced person), but initiaily it is not knownwhether they are high quality features orwhether they arc highly noisy or redundant.
Ifthe features all have good predictive capabilities,any one of many classification methods houlddo well.
Otherwise, the situation is much lesspredictable.Suppose one is trying to make anevaluation about the level of readingcomprehension understanding exhibited in asample piece of student writing based on fivefeatures.
Later two new features are added andsamples collected.
Although no data has beendeleted, and new information has been added,some methods may actually yield worse resultson the new, more complete set of data than onthe original, smaller set.
These results can bereflected in poorer apparent error rates, but moreoften in worse (estimated) true error rates.
Whatcauses this phenomenon of performancedegradation with additional information?
Somemethods perform particularly well with good,highly predictive features, but fail apart withnoisy data.
Other methods may overweightredundant features that measure the same thingby, in effect, counting them more than once.In practice, many features used in NLPapplications are often poor, noisy, andredundant.
Adding new information in the formof weak features can actually degradeperformance of the system.
This is particularlytrue of methods that are applied directly to thedata without any estimate of complexity fit tothe data.
For these methods, the primaryapproach to minimize the effects of feature noiseand redundancy is feature selection.
Given someinitial set of features, a feature selectionprocedure will throw out some of the featuresthat are deemed to be noncontributory toclassification.Our goal is to fit a classification modelto the data without overspecializing the learningsystem to the data.
Thus, we must determine justhow complex a classifier the data supports.
Ingeneral, we do not know the answer to thisquestion until we estimate the true error rate fordifferent classifiers and classifier fits.
Inpractice, though, simpler classifiers often dobetter than more complex or theoreticallyadvantageous classifiers.
For some classifiers,the underlying assumptions ofthe more complexclassifier may be violated.
For most classifiers,the data are not strong enough to generalizebeyond an indicated level of complexity fit.
As arule of thumb, one is looking for the simplestsolution that yields good results.CONCLUSIONS ANDRECOMMENDATIONSThe success of a specific classification-basedNLP application depends on several factors,including the power of the training method and27the size of the training sample.
Irrespective ofthe classification method, the performance of aclassification-based NLP system should beevaluated by estimating the accuracy of futurepredictions, technically known as estimating thetrue error rate on future cases.
This is offundamental importance for comparingclassifiers on the same samples and also forselecting key characteristics of many of thenewer classifiers, e.g., neural networks.It has been shown that, with limitedsamples, the best techniques for measuring theperformance of classification-based NLPsystems are resampling methods that simulatethe presentation of new cases by repeatedlyhiding some test cases.
Additionally, attentionmust be paid to the context of the particularNLP application as regards the costs and risksassociated with the possible errors inclassification.Although statistically valid estimates ofthe true error rate will not guarantee success inthe marketplace for NLP systems, they will giveone a measure of confidence in the trueperformance ofthe system.ReferencesEffron, B.
(1983).
Estimating the Error Rate of aPrediction Rule.
Journal of the AmericanStatistical Association, 78:316-333.Lachenbruch, P. and Mickey, M. (1968).Estimation of Error Rates in DiscriminantAnalysis.
Technometrics, 10:1-11.Nolan, James R. (1997a).
The Architecture of aHybrid Knowledge-Based System forEvaluating Writing Samples.
In A Niku-Lari(Ed.
), Expert Systems Applications andArtificial Intelligence Technology Transj~rSeries, EXPERSYS-97.
Gournay s/M,France: IITr International, in press.Nolan, James R. (1997b).
DISXPERT: A Rule-Based Vocational Rehabilitation RiskAssessment System.
Expert Systems WithApplications, in press.Machina, M. (1987).
Decision-Making in thePresence of Risk.
Science, 236: 537-543.Riplcy, Brian D. (1996).
Pattern Recognitionand Neural Networks.
Cambridge:Cambridge University Press.Wallace, C. & Freeman, P. (1987).
Estimationand Inference by Compact Encoding.
Journalof the Royal Statistical Society B.,49B(3):240-265.Weiss, Sholom M. and Kulikowski, Casimar A.(1991).
Computer Systems That Learn SanMeteo, CA: Morgan Kaufmann.28
