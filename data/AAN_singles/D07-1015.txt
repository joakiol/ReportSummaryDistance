Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
141?150, Prague, June 2007. c?2007 Association for Computational LinguisticsStructured Prediction Models via the Matrix-Tree TheoremTerry Koo, Amir Globerson, Xavier Carreras and Michael CollinsMIT CSAIL, Cambridge, MA 02139, USA{maestro,gamir,carreras,mcollins}@csail.mit.eduAbstractThis paper provides an algorithmic frame-work for learning statistical models involv-ing directed spanning trees, or equivalentlynon-projective dependency structures.
Weshow how partition functions and marginalsfor directed spanning trees can be computedby an adaptation of Kirchhoff?s Matrix-TreeTheorem.
To demonstrate an application ofthe method, we perform experiments whichuse the algorithm in training both log-linearand max-margin dependency parsers.
Thenew training methods give improvements inaccuracy over perceptron-trained models.1 IntroductionLearning with structured data typically involvessearching or summing over a set with an exponen-tial number of structured elements, for example theset of all parse trees for a given sentence.
Meth-ods for summing over such structures include theinside-outside algorithm for probabilistic context-free grammars (Baker, 1979), the forward-backwardalgorithm for hidden Markov models (Baum etal., 1970), and the belief-propagation algorithm forgraphical models (Pearl, 1988).
These algorithmscompute marginal probabilities and partition func-tions, quantities which are central to many meth-ods for the statistical modeling of complex struc-tures (e.g., the EM algorithm (Baker, 1979; Baumet al, 1970), contrastive estimation (Smith and Eis-ner, 2005), training algorithms for CRFs (Lafferty etal., 2001), and training algorithms for max-marginmodels (Bartlett et al, 2004; Taskar et al, 2004a)).This paper describes inside-outside-style algo-rithms for the case of directed spanning trees.
Thesestructures are equivalent to non-projective depen-dency parses (McDonald et al, 2005b), and moregenerally could be relevant to any task that involveslearning a mapping from a graph to an underlyingspanning tree.
Unlike the case for projective depen-dency structures, partition functions and marginalsfor non-projective trees cannot be computed usingdynamic-programming methods such as the inside-outside algorithm.
In this paper we describe howthese quantities can be computed by adapting a well-known result in graph theory: Kirchhoff?s Matrix-Tree Theorem (Tutte, 1984).
A na?
?ve application ofthe theorem yields O(n4) and O(n6) algorithms forcomputation of the partition function and marginals,respectively.
However, our adaptation finds the par-tition function and marginals in O(n3) time usingsimple matrix determinant and inversion operations.We demonstrate an application of the new infer-ence algorithm to non-projective dependency pars-ing.
Specifically, we show how to implementtwo popular supervised learning approaches for thistask: globally-normalized log-linear models andmax-margin models.
Log-linear estimation criti-cally depends on the calculation of partition func-tions and marginals, which can be computed byour algorithms.
For max-margin models, Bartlettet al (2004) have provided a simple training al-gorithm, based on exponentiated-gradient (EG) up-dates, that requires computation of marginals andcan thus be implemented within our framework.Both of these methods explicitly minimize the lossincurred when parsing the entire training set.
Thiscontrasts with the online learning algorithms used inprevious work with spanning-tree models (McDon-ald et al, 2005b).We applied the above two marginal-based train-ing algorithms to six languages with varying de-grees of non-projectivity, using datasets obtainedfrom the CoNLL-X shared task (Buchholz andMarsi, 2006).
Our experimental framework com-pared three training approaches: log-linear models,max-margin models, and the averaged perceptron.Each of these was applied to both projective andnon-projective parsing.
Our results demonstrate thatmarginal-based training yields models which out-141perform those trained using the averaged perceptron.In summary, the contributions of this paper are:1.
We introduce algorithms for inside-outside-style calculations for directed spanning trees, orequivalently non-projective dependency struc-tures.
These algorithms should have wideapplicability in learning problems involvingspanning-tree structures.2.
We illustrate the utility of these algorithms inlog-linear training of dependency parsing mod-els, and show improvements in accuracy whencompared to averaged-perceptron training.3.
We also train max-margin models for depen-dency parsing via an EG algorithm (Bartlettet al, 2004).
The experiments presented hereconstitute the first application of this algorithmto a large-scale problem.
We again show im-proved performance over the perceptron.The goal of our experiments is to give a rigorouscomparative study of the marginal-based training al-gorithms and a highly-competitive baseline, the av-eraged perceptron, using the same feature sets forall approaches.
We stress, however, that the purposeof this work is not to give competitive performanceon the CoNLL data sets; this would require furtherengineering of the approach.Similar adaptations of the Matrix-Tree Theoremhave been developed independently and simultane-ously by Smith and Smith (2007) andMcDonald andSatta (2007); see Section 5 for more discussion.2 Background2.1 Discriminative Dependency ParsingDependency parsing is the task of mapping a sen-tence x to a dependency structure y.
Given a sen-tence x with n words, a dependency for that sen-tence is a tuple (h,m) where h ?
[0 .
.
.
n] is theindex of the head word in the sentence, and m ?
[1 .
.
.
n] is the index of a modifier word.
The valueh = 0 is a special root-symbol that may only ap-pear as the head of a dependency.
We use D(x) torefer to all possible dependencies for a sentence x:D(x) = {(h,m) : h ?
[0 .
.
.
n],m ?
[1 .
.
.
n]}.A dependency parse is a set of dependenciesthat forms a directed tree, with the sentence?s root-symbol as its root.
We will consider both projectiveProjective Non-projectiveSingleRoot 1 30 2Heroot saw her 1 30 2Heroot saw herMultiRoot 1 30 2Heroot saw her 1 30 2Heroot saw herFigure 1: Examples of the four types of dependency struc-tures.
We draw dependency arcs from head to modifier.trees, where dependencies are not allowed to cross,and non-projective trees, where crossing dependen-cies are allowed.
Dependency annotations for somelanguages, for example Czech, can exhibit a signifi-cant number of crossing dependencies.
In addition,we consider both single-root and multi-root trees.
Ina single-root tree y, the root-symbol has exactly onechild, while in a multi-root tree, the root-symbol hasone or more children.
This distinction is relevantas our training sets include both single-root corpora(in which all trees are single-root structures) andmulti-root corpora (in which some trees are multi-root structures).The two distinctions described above are orthog-onal, yielding four classes of dependency structures;see Figure 1 for examples of each kind of structure.We use T sp (x) to denote the set of all possible pro-jective single-root dependency structures for a sen-tence x, and T snp(x) to denote the set of single-rootnon-projective structures for x.
The sets T mp (x) andT mnp (x) are defined analogously for multi-root struc-tures.
In contexts where any class of dependencystructures may be used, we use the notation T (x) asa placeholder that may be defined as T sp (x), Tsnp(x),T mp (x) or Tmnp (x).Following McDonald et al (2005a), we use a dis-criminative model for dependency parsing.
Fea-tures in the model are defined through a functionf(x, h,m) which maps a sentence x together witha dependency (h,m) to a feature vector in Rd.
Afeature vector can be sensitive to any properties ofthe triple (x, h,m).
Given a parameter vector w,the optimal dependency structure for a sentence x isy?
(x;w) = argmaxy?T (x)?
(h,m)?yw ?
f(x, h,m) (1)where the set T (x) can be defined as T sp (x), Tsnp(x),T mp (x) or Tmnp (x), depending on the type of parsing.142The parameters w will be learned from a train-ing set {(xi, yi)}Ni=1 where each xi is a sentence andeach yi is a dependency structure.
Much of the pre-vious work on learningw has focused on training lo-cal models (see Section 5).
McDonald et al (2005a;2005b) trained global models using online algo-rithms such as the perceptron algorithm or MIRA.In this paper we consider training algorithms basedon work in conditional random fields (CRFs) (Laf-ferty et al, 2001) and max-margin methods (Taskaret al, 2004a).2.2 Three Inference ProblemsThis section highlights three inference problemswhich arise in training and decoding discriminativedependency parsers, and which are central to the ap-proaches described in this paper.Assume that we have a vector ?
with values?h,m ?
R for all (h,m) ?
D(x); these values cor-respond to weights on the different dependencies inD(x).
Define a conditional distribution over all de-pendency structures y ?
T (x) as follows:P (y |x;?)
=exp{?
(h,m)?y ?h,m}Z(x;?)(2)Z(x;?)
=?y?T (x)exp????(h,m)?y?h,m???
(3)The function Z(x;?)
is commonly referred to as thepartition function.Given the distribution P (y |x;?
), we can definethe marginal probability of a dependency (h,m) as?h,m(x;?)
=?y?T (x) : (h,m)?yP (y |x;?
)The inference problems are then as follows:Problem 1: Decoding:Find argmaxy?T (x)?
(h,m)?y ?h,mProblem 2: Computation of the Partition Func-tion: Calculate Z(x;?
).Problem 3: Computation of the Marginals:For all (h,m) ?
D(x), calculate ?h,m(x;?
).Note that all three problems require a maximiza-tion or summation over the set T (x), which is ex-ponential in size.
There is a clear motivation forbeing able to solve Problem 1: by setting ?h,m =w ?
f(x, h,m), the optimal dependency structurey?
(x;w) (see Eq.
1) can be computed.
In this paperthe motivation for solving Problems 2 and 3 arisesfrom training algorithms for discriminative models.As we will describe in Section 4, both log-linear andmax-margin models can be trained via methods thatmake direct use of algorithms for Problems 2 and 3.In the case of projective dependency structures(i.e., T (x) defined as T sp (x) or Tmp (x)), there arewell-known algorithms for all three inference prob-lems.
Decoding can be carried out using Viterbi-style dynamic-programming algorithms, for exam-ple the O(n3) algorithm of Eisner (1996).
Com-putation of the marginals and partition function canalso be achieved in O(n3) time, using a variant ofthe inside-outside algorithm (Baker, 1979) appliedto the Eisner (1996) data structures (Paskin, 2001).In the non-projective case (i.e., T (x) defined asT snp(x) or Tmnp (x)), McDonald et al (2005b) de-scribe how the CLE algorithm (Chu and Liu, 1965;Edmonds, 1967) can be used for decoding.
How-ever, it is not possible to compute the marginalsand partition function using the inside-outside algo-rithm.
We next describe a method for computingthese quantities in O(n3) time using matrix inverseand determinant operations.3 Spanning-tree inference using theMatrix-Tree TheoremIn this section we present algorithms for computingthe partition function and marginals, as defined inSection 2.2, for non-projective parsing.
We first re-iterate the observation of McDonald et al (2005a)that non-projective parses correspond to directedspanning trees on a complete directed graph of nnodes, where n is the length of the sentence.
Theabove inference problems thus involve summationover the set of all directed spanning trees.
Note thatthis set is exponentially large, and there is no obvi-ous method for decomposing the sum into dynamic-programming-like subproblems.
This section de-scribes how a variant of Kirchhoff?s Matrix-TreeTheorem (Tutte, 1984) can be used to evaluate thepartition function and marginals efficiently.In what follows, we consider the single-root set-ting (i.e., T (x) = T snp(x)), leaving the multi-root143case (i.e., T (x) = T mnp (x)) to Section 3.3.
For asentence x with n words, define a complete directedgraph G on n nodes, where each node correspondsto a word in x, and each edge corresponds to a de-pendency between two words in x.
Note thatG doesnot include the root-symbol h = 0, nor does it ac-count for any dependencies (0,m) headed by theroot-symbol.
We assign non-negative weights to theedges of this graph, yielding the following weightedadjacency matrix A(?)
?
Rn?n, for h,m = 1 .
.
.
n:Ah,m(?)
={0, if h = mexp {?h,m} , otherwiseTo account for the dependencies (0,m) headed bythe root-symbol, we define a vector of root-selectionscores r(?)
?
Rn, form = 1 .
.
.
n:rm(?)
= exp {?0,m}Let the weight of a dependency structure y ?
T snp(x)be defined as:?(y;?)
= rroot(y)(?)?
(h,m)?y : h 6=0Ah,m(?
)Here, root(y) = m : (0,m) ?
y is the child of theroot-symbol; there is exactly one such child, sincey ?
T snp(x).
Eq.
2 and 3 can be rephrased as:P (y |x;?)
=?(y;?)Z(x;?)(4)Z(x;?)
=?y?T snp(x)?(y;?)
(5)In the remainder of this section, we drop the nota-tional dependence on x for brevity.The original Matrix-Tree Theorem addressed theproblem of counting the number of undirected span-ning trees in an undirected graph.
For the modelswe study here, we require a sum of weighted anddirected spanning trees.
Tutte (1984) extended theMatrix-Tree Theorem to this case.
We briefly sum-marize his method below.First, define the Laplacian matrix L(?)
?
Rn?nof G, for h,m = 1 .
.
.
n:Lh,m(?)
={ ?nh?=1Ah?,m(?)
if h = m?Ah,m(?)
otherwiseSecond, for a matrix X , let X(h,m) be the minor ofX with respect to row h and column m; i.e., thedeterminant of the matrix formed by deleting row hand columnm fromX .
Finally, define the weight ofany directed spanning tree of G to be the product ofthe weights Ah,m(?)
for the edges in that tree.Theorem 1 (Tutte, 1984, p. 140).
Let L(?)
be theLaplacian matrix of G. Then L(m,m)(?)
is equal tothe sum of the weights of all directed spanning treesof G which are rooted at m. Furthermore, the mi-nors vary only in sign when traversing the columnsof the Laplacian (Tutte, 1984, p. 150):?h,m : (?1)h+mL(h,m)(?)
= L(m,m)(?)
(6)3.1 Partition functions via matrix determinantsFrom Theorem 1, it directly follows thatL(m,m)(?)
=?y?U(m)?
(h,m)?y : h 6=0Ah,m(?
)where U(m) = {y ?
T snp : root(y) = m}.
Ana?
?ve method for computing the partition function istherefore to evaluateZ(?)
=n?m=1rm(?)L(m,m)(?
)The above would require calculating n determinants,resulting in O(n4) complexity.
However, as weshow below Z(?)
may be obtained in O(n3) timeusing a single determinant evaluation.Define a newmatrix L?(?)
to beL(?)
with the firstrow replaced by the root-selection scores:L?h,m(?)
={rm(?)
h = 1Lh,m(?)
h > 1This matrix allows direct computation of the parti-tion function, as the following proposition shows.Proposition 1 The partition function in Eq.
5 isgiven by Z(?)
= |L?(?
)|.Proof: Consider the row expansion of |L?(?
)| withrespect to row 1:|L?(?
)| =n?m=1(?1)1+mL?1,m(?)L?(1,m)(?)=n?m=1(?1)1+mrm(?)L(1,m)(?)=n?m=1rm(?)L(m,m)(?)
= Z(?
)The second line follows from the construction ofL?(?
), and the third line follows from Eq.
6.1443.2 Marginals via matrix inversionThe marginals we require are given by?h,m(?)
=1Z(?
)?y?T snp : (h,m)?y?(y;?
)To calculate these marginals efficiently for all valuesof (h,m) we use a well-known identity relating thelog partition-function to marginals?h,m(?)
=?
logZ(?)?
?h,mSince the partition function in this case has a closed-form expression (i.e., the determinant of a matrixconstructed from ?
), the marginals can also obtainedin closed form.
Using the chain rule, the derivativeof the log partition-function in Proposition 1 is?h,m(?)
=?
log |L?(?)|??h,m=n?h?=1n?m?=1?
log |L?(?)|?L?h?,m?(?)?L?h?,m?(?)?
?h,mTo perform the derivative, we use the identity?
log |X|?X=(X?1)Tand the fact that ?L?h?,m?(?)/?
?h,m is nonzero foronly a few h?,m?.
Specifically, when h = 0, themarginals are given by?0,m(?)
= rm(?)[L??1(?
)]m,1and for h > 0, the marginals are given by?h,m(?)
= (1 ?
?1,m)Ah,m(?)[L??1(?)]m,m?
(1 ?
?h,1)Ah,m(?)[L??1(?
)]m,hwhere ?h,m is the Kronecker delta.
Thus, the com-plexity of evaluating all the relevant marginals isdominated by the matrix inversion, and the totalcomplexity is therefore O(n3).3.3 Multiple RootsIn the case of multiple roots, we can still computethe partition function and marginals efficiently.
Infact, the derivation of this case is simpler than forsingle-root structures.
Create an extended graph G?which augmentsG with a dummy root node that hasedges pointing to all of the existing nodes, weightedby the appropriate root-selection scores.
Note thatthere is a bijection between directed spanning treesofG?
rooted at the dummy root and multi-root struc-tures y ?
T mnp (x).
Thus, Theorem 1 can be used tocompute the partition function directly: construct aLaplacian matrix L(?)
for G?
and compute the mi-nor L(0,0)(?).
Since this minor is also a determi-nant, the marginals can be obtained analogously tothe single-root case.
More concretely, this techniquecorresponds to defining the matrix L?(?)
asL?(?)
= L(?)
+ diag(r(?
))where diag(v) is the diagonal matrix with the vectorv on its diagonal.3.4 Labeled TreesThe techniques above extend easily to the casewhere dependencies are labeled.
For a model withL different labels, it suffices to define the edgeand root scores as Ah,m(?)
=?L`=1 exp {?h,m,`}and rm(?)
=?L`=1 exp {?0,m,`}.
The partitionfunction over labeled trees is obtained by operat-ing on these values as described previously, andthe marginals are given by an application of thechain rule.
Both inference problems are solvable inO(n3 + Ln2) time.4 Training AlgorithmsThis section describes two methods for parameterestimation that rely explicitly on the computation ofthe partition function and marginals.4.1 Log-Linear EstimationIn conditional log-linear models (Johnson et al,1999; Lafferty et al, 2001), a distribution over parsetrees for a sentence x is defined as follows:P (y |x;w) =exp{?
(h,m)?y w ?
f(x, h,m)}Z(x;w)(7)where Z(x;w) is the partition function, a sum overT sp (x), Tsnp(x), Tmp (x) or Tmnp (x).We train the model using the approach describedby Sha and Pereira (2003).
Assume that we have atraining set {(xi, yi)}Ni=1.
The optimal parameters145are taken to be w?
= argminw L(w) whereL(w) = ?CN?i=1logP (yi |xi;w) +12||w||2The parameterC > 0 is a constant dictating the levelof regularization in the model.Since L(w) is a convex function, gradient de-scent methods can be used to search for the globalminimum.
Such methods typically involve repeatedcomputation of the loss L(w) and gradient ?L(w)?w ,requiring efficient implementations of both func-tions.
Note that the log-probability of a parse islogP (y |x;w) =?
(h,m)?yw ?
f(x, h,m)?
logZ(x;w)so that the main issue in calculating the loss func-tion L(w) is the evaluation of the partition functionsZ(xi;w).
The gradient of the loss is given by?L(w)?w= w ?
CN?i=1?
(h,m)?yif(xi, h,m)+ CN?i=1?
(h,m)?D(xi)?h,m(xi;w)f(xi, h,m)where?h,m(x;w) =?y?T (x) : (h,m)?yP (y |x;w)is the marginal probability of a dependency (h,m).Thus, the main issue in the evaluation of the gradientis the computation of the marginals ?h,m(xi;w).Note that Eq.
7 forms a special case of the log-linear distribution defined in Eq.
2 in Section 2.2.If we set ?h,m = w ?
f(x, h,m) then we haveP (y |x;w) = P (y |x;?
), Z(x;w) = Z(x;?
), and?h,m(x;w) = ?h,m(x;?).
Thus in the projectivecase the inside-outside algorithm can be used to cal-culate the partition function and marginals, therebyenabling training of a log-linear model; in the non-projective case the algorithms in Section 3 can beused for this purpose.4.2 Max-Margin EstimationThe second learning algorithm we consider isthe large-margin approach for structured prediction(Taskar et al, 2004a; Taskar et al, 2004b).
Learningin this framework again involves minimization of aconvex function L(w).
Let the margin for parse treey on the i?th training example be defined asmi,y(w) =?
(h,m)?yiw?f(xi, h,m) ??
(h,m)?yw?f(xi, h,m)The loss function is then defined asL(w) = CN?i=1maxy?T (xi)(Ei,y ?mi,y(w)) +12||w||2where Ei,y is a measure of the loss?or number oferrors?for parse y on the i?th training sentence.
Inthis paper we take Ei,y to be the number of incorrectdependencies in the parse tree y when compared tothe gold-standard parse tree yi.The definition of L(w) makes use of the expres-sion maxy?T (xi) (Ei,y ?mi,y(w)) for the i?th train-ing example, which is commonly referred to as thehinge loss.
Note that Ei,yi = 0, and also thatmi,yi(w) = 0, so that the hinge loss is always non-negative.
In addition, the hinge loss is 0 if and onlyifmi,y(w) ?
Ei,y for all y ?
T (xi).
Thus the hingeloss directly penalizes margins mi,y(w) which areless than their corresponding losses Ei,y.Figure 2 shows an algorithm for minimizingL(w) that is based on the exponentiated-gradient al-gorithm for large-margin optimization described byBartlett et al (2004).
The algorithm maintains a setof weights ?i,h,m for i = 1 .
.
.
N, (h,m) ?
D(xi),which are updated example-by-example.
The algo-rithm relies on the repeated computation of marginalvalues ?i,h,m, which are defined as follows:1?i,h,m =?y?T (xi) : (h,m)?yP (y |xi) (8)P (y |xi) =exp{?
(h,m)?y ?i,h,m}?y?
?T (xi) exp{?(h,m)?y?
?i,h,m}A similar definition is used to derive marginal val-ues ?
?i,h,m from the values ??i,h,m.
Computation ofthe ?
and ??
values is again inference of the formdescribed in Problem 3 in Section 2.2, and can be1Bartlett et al (2004) write P (y |xi) as ?i,y .
The ?i,y vari-ables are dual variables that appear in the dual objective func-tion, i.e., the convex dual of L(w).
Analysis of the algorithmshows that as the ?i,h,m variables are updated, the dual vari-ables converge to the optimal point of the dual objective, andthe parameters w converge to the minimum of L(w).146Inputs: Training examples {(xi, yi)}Ni=1.Parameters: Regularization constant C, starting point ?,number of passes over training set T .Data Structures: Real values ?i,h,m and li,h,m for i =1 .
.
.
N, (h,m) ?
D(xi).
Learning rate ?.Initialization: Set learning rate ?
= 1C .
Set ?i,h,m = ?
for(h,m) ?
yi, and ?i,h,m = 0 for (h,m) /?
yi.
Set li,h,m = 0for (h,m) ?
yi, and li,h,m = 1 for (h,m) /?
yi.
Calculateinitial parameters asw = C?i?
(h,m)?D(xi)?i,h,mf(xi, h,m)where ?i,h,m = (1?
li,h,m ?
?i,h,m) and the ?i,h,m valuesare calculated from the ?i,h,m values as described in Eq.
8.Algorithm: Repeat T passes over the training set, whereeach pass is as follows:Set obj = 0For i = 1 .
.
.
N?
For all (h,m) ?
D(xi):?
?i,h,m = ?i,h,m + ?C (li,h,m +w ?
f(xi, h,m))?
For example i, calculate marginals ?i,h,mfrom ?i,h,m values, and marginals ?
?i,h,mfrom ?
?i,h,m values (see Eq.
8).?
Update the parameters:w = w + C?
(h,m)?D(xi)?i,h,mf(xi, h,m)where ?i,h,m = ?i,h,m ?
??i,h,m,?
For all (h,m) ?
D(xi), set ?i,h,m = ??i,h,m?
Set obj = obj + C?(h,m)?D(xi)li,h,m?
?i,h,mSet obj = obj ?
||w||22 .
If obj has decreasedcompared to last iteration, set ?
= ?2 .Output: Parameter values w.Figure 2: The EG Algorithm for Max-Margin Estimation.The learning rate ?
is halved each time the dual objective func-tion (see (Bartlett et al, 2004)) fails to increase.
In our experi-ments we chose ?
= 9, which was found to work well duringdevelopment of the algorithm.achieved using the inside-outside algorithm for pro-jective structures, and the algorithms described inSection 3 for non-projective structures.5 Related WorkGlobal log-linear training has been used in the con-text of PCFG parsing (Johnson, 2001).
Riezler et al(2004) explore a similar application of log-linearmodels to LFG parsing.
Max-margin learninghas been applied to PCFG parsing by Taskar et al(2004b).
They show that this problem has a QPdual of polynomial size, where the dual variablescorrespond to marginal probabilities of CFG rules.A similar QP dual may be obtained for max-marginprojective dependency parsing.
However, for non-projective parsing, the dual QP would require an ex-ponential number of constraints on the dependencymarginals (Chopra, 1989).
Nevertheless, alternativeoptimization methods like that of Tsochantaridis etal.
(2004), or the EGmethod presented here, can stillbe applied.The majority of previous work on dependencyparsing has focused on local (i.e., classification ofindividual edges) discriminative training methods(Yamada and Matsumoto, 2003; Nivre et al, 2004;Y. Cheng, 2005).
Non-local (i.e., classification ofentire trees) training methods were used by McDon-ald et al (2005a), who employed online learning.Dependency parsing accuracy can be improvedby allowing second-order features, which considermore than one dependency simultaneously.
McDon-ald and Pereira (2006) define a second-order depen-dency parsing model in which interactions betweenadjacent siblings are allowed, and Carreras (2007)defines a second-order model that allows grandpar-ent and sibling interactions.
Both authors give poly-time algorithms for exact projective parsing.
Byadapting the inside-outside algorithm to these mod-els, partition functions and marginals can be com-puted for second-order projective structures, allow-ing log-linear and max-margin training to be ap-plied via the framework developed in this paper.For higher-order non-projective parsing, however,computational complexity results (McDonald andPereira, 2006; McDonald and Satta, 2007) indicatethat exact solutions to the three inference problemsof Section 2.2 will be intractable.
Exploration of ap-proximate second-order non-projective inference isa natural avenue for future research.Two other groups of authors have independentlyand simultaneously proposed adaptations of theMatrix-Tree Theorem for structured inference on di-rected spanning trees (McDonald and Satta, 2007;Smith and Smith, 2007).
There are some algorithmicdifferences between these papers and ours.
First, wedefine both multi-root and single-root algorithms,whereas the other papers only consider multi-root147parsing.
This distinction can be important as oneoften expects a dependency structure to have ex-actly one child attached to the root-symbol, as is thecase in a single-root structure.
Second, McDonaldand Satta (2007) propose an O(n5) algorithm forcomputing the marginals, as opposed to the O(n3)matrix-inversion approach used by Smith and Smith(2007) and ourselves.In addition to the algorithmic differences, bothgroups of authors consider applications of theMatrix-Tree Theorem which we have not discussed.For example, both papers propose minimum-riskdecoding, and McDonald and Satta (2007) dis-cuss unsupervised learning and language model-ing, while Smith and Smith (2007) define hidden-variable models based on spanning trees.In this paper we used EG training methods onlyfor max-margin models (Bartlett et al, 2004).
How-ever, Globerson et al (2007) have recently shownhow EG updates can be applied to efficient trainingof log-linear models.6 Experiments on Dependency ParsingIn this section, we present experimental resultsapplying our inference algorithms for dependencyparsing models.
Our primary purpose is to estab-lish comparisons along two relevant dimensions:projective training vs. non-projective training, andmarginal-based training algorithms vs. the averagedperceptron.
The feature representation and other rel-evant dimensions are kept fixed in the experiments.6.1 Data Sets and FeaturesWe used data from the CoNLL-X shared taskon multilingual dependency parsing (Buchholz andMarsi, 2006).
In our experiments, we used a subsetconsisting of six languages; Table 1 gives details ofthe data sets used.2 For each language we createda validation set that was a subset of the CoNLL-X2Our subset includes the two languages with the lowest ac-curacy in the CoNLL-X evaluations (Turkish and Arabic), thelanguage with the highest accuracy (Japanese), the most non-projective language (Dutch), a moderately non-projective lan-guage (Slovene), and a highly projective language (Spanish).All languages but Spanish have multi-root parses in their data.We are grateful to the providers of the treebanks that constitutedthe data of our experiments (Hajic?
et al, 2004; van der Beek etal., 2002; Kawata and Bartels, 2000; Dz?eroski et al, 2006; Civitand Mart?
?, 2002; Oflazer et al, 2003).language %cd train val.
testArabic 0.34 49,064 5,315 5,373Dutch 4.93 178,861 16,208 5,585Japanese 0.70 141,966 9,495 5,711Slovene 1.59 22,949 5,801 6,390Spanish 0.06 78,310 11,024 5,694Turkish 1.26 51,827 5,683 7,547Table 1: Information for the languages in our experiments.The 2nd column (%cd) is the percentage of crossing dependen-cies in the training and validation sets.
The last three columnsreport the size in tokens of the training, validation and test sets.training set for that language.
The remainder of eachtraining set was used to train the models for the dif-ferent languages.
The validation sets were used totune the meta-parameters (e.g., the value of the reg-ularization constantC) of the different training algo-rithms.
We used the official test sets and evaluationscript from the CoNLL-X task.
All of the results thatwe report are for unlabeled dependency parsing.3The non-projective models were trained on theCoNLL-X data in its original form.
Since the pro-jective models assume that the dependencies in thedata are non-crossing, we created a second train-ing set for each language where non-projective de-pendency structures were automatically transformedinto projective structures.
All projective modelswere trained on these new training sets.4 Our featurespace is based on that of McDonald et al (2005a).56.2 ResultsWe performed experiments using three training al-gorithms: the averaged perceptron (Collins, 2002),log-linear training (via conjugate gradient descent),and max-margin training (via the EG algorithm).Each of these algorithms was trained using pro-jective and non-projective methods, yielding sixtraining settings per language.
The differenttraining algorithms have various meta-parameters,which we optimized on the validation set foreach language/training-setting combination.
The3Our algorithms also support labeled parsing (see Section3.4).
Initial experiments with labeled models showed the sametrend that we report here for unlabeled parsing, so for simplicitywe conducted extensive experiments only for unlabeled parsing.4The transformations were performed by running the pro-jective parser with score +1 on correct dependencies and -1 oth-erwise: the resulting trees are guaranteed to be projective and tohave a minimum loss with respect to the correct tree.
Note thatonly the training sets were transformed.5It should be noted that McDonald et al (2006) use a richerfeature set that is incomparable to our features.148Perceptron Max-Margin Log-Linearp np p np p npAra 71.74 71.84 71.74 72.99 73.11 73.67Dut 77.17 78.83 76.53 79.69 76.23 79.55Jap 91.90 91.78 92.10 92.18 91.68 91.49Slo 78.02 78.66 79.78 80.10 78.24 79.66Spa 81.19 80.02 81.71 81.93 81.75 81.57Tur 71.22 71.70 72.83 72.02 72.26 72.62Table 2: Test data results.
The p and np columns show resultswith projective and non-projective training respectively.Ara Dut Jap Slo Spa Tur AVP 71.74 78.83 91.78 78.66 81.19 71.70 79.05E 72.99 79.69 92.18 80.10 81.93 72.02 79.82L 73.67 79.55 91.49 79.66 81.57 72.26 79.71Table 3: Results for the three training algorithms on the differ-ent languages (P = perceptron, E = EG, L = log-linear models).AV is an average across the results for the different languages.averaged perceptron has a single meta-parameter,namely the number of iterations over the training set.The log-linear models have two meta-parameters:the regularization constant C and the number ofgradient steps T taken by the conjugate-gradientoptimizer.
The EG approach also has two meta-parameters: the regularization constant C and thenumber of iterations, T .6 For models trained usingnon-projective algorithms, both projective and non-projective parsing was tested on the validation set,and the highest scoring of these two approaches wasthen used to decode test data sentences.Table 2 reports test results for the six training sce-narios.
These results show that for Dutch, which isthe language in our data that has the highest num-ber of crossing dependencies, non-projective train-ing gives significant gains over projective trainingfor all three training methods.
For the other lan-guages, non-projective training gives similar or evenimproved performance over projective training.Table 3 gives an additional set of results, whichwere calculated as follows.
For each of the threetraining methods, we used the validation set resultsto choose between projective and non-projectivetraining.
This allows us to make a direct com-parison of the three training algorithms.
Table 36We trained the perceptron for 100 iterations, and chose theiteration which led to the best score on the validation set.
Notethat in all of our experiments, the best perceptron results wereactually obtained with 30 or fewer iterations.
For the log-linearand EG algorithms we tested a number of values for C, and foreach value of C ran 100 gradient steps or EG iterations, finallychoosing the best combination of C and T found in validation.shows the results of this comparison.7 The resultsshow that log-linear and max-margin models bothgive a higher average accuracy than the perceptron.For some languages (e.g., Japanese), the differencesfrom the perceptron are small; however for otherlanguages (e.g., Arabic, Dutch or Slovene) the im-provements seen are quite substantial.7 ConclusionsThis paper describes inference algorithms forspanning-tree distributions, focusing on the funda-mental problems of computing partition functionsand marginals.
Although we concentrate on log-linear and max-margin estimation, the inference al-gorithms we present can serve as black-boxes inmany other statistical modeling techniques.Our experiments suggest that marginal-basedtraining produces more accurate models than per-ceptron learning.
Notably, this is the first large-scaleapplication of the EG algorithm, and shows that it isa promising approach for structured learning.In line with McDonald et al (2005b), we confirmthat spanning-tree models are well-suited to depen-dency parsing, especially for highly non-projectivelanguages such as Dutch.
Moreover, spanning-treemodels should be useful for a variety of other prob-lems involving structured data.AcknowledgmentsThe authors would like to thank the anonymous re-viewers for their constructive comments.
In addi-tion, the authors gratefully acknowledge the follow-ing sources of support.
Terry Koo was funded bya grant from the NSF (DMS-0434222) and a grantfrom NTT, Agmt.
Dtd.
6/21/1998.
Amir Glober-son was supported by a fellowship from the Roth-schild Foundation - Yad Hanadiv.
Xavier Carreraswas supported by the Catalan Ministry of Innova-tion, Universities and Enterprise, and a grant fromNTT, Agmt.
Dtd.
6/21/1998.
Michael Collins wasfunded by NSF grants 0347631 and DMS-0434222.7We ran the sign test at the sentence level to measure thestatistical significance of the results aggregated across the sixlanguages.
Out of 2,472 sentences total, log-linear models gaveimproved parses over the perceptron on 448 sentences, andworse parses on 343 sentences.
The max-margin method gaveimproved/worse parses for 500/383 sentences.
Both results aresignificant with p ?
0.001.149ReferencesJ.
Baker.
1979.
Trainable grammars for speech recognition.
In97th meeting of the Acoustical Society of America.P.
Bartlett, M. Collins, B. Taskar, and D. McAllester.
2004.
Ex-ponentiated gradient algorithms for large?margin structuredclassification.
In NIPS.L.E.
Baum, T. Petrie, G. Soules, and N. Weiss.
1970.
A max-imization technique occurring in the statistical analysis ofprobabilistic functions of markov chains.
Annals of Mathe-matical Statistics, 41:164?171.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared task onmultilingual dependency parsing.
In Proc.
CoNLL-X.X.
Carreras.
2007.
Experiments with a higher-order projectivedependency parser.
In Proc.
EMNLP-CoNLL.S.
Chopra.
1989.
On the spanning tree polyhedron.
Oper.
Res.Lett., pages 25?29.Y.J.
Chu and T.H.
Liu.
1965.
On the shortest arborescence of adirected graph.
Science Sinica, 14:1396?1400.M.
Civit and Ma A.
Mart??.
2002.
Design principles for a Span-ish treebank.
In Proc.
of the First Workshop on Treebanksand Linguistic Theories (TLT).M.
Collins.
2002.
Discriminative training methods for hiddenmarkov models: Theory and experiments with perceptron al-gorithms.
In Proc.
EMNLP.S.
Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z.
Z?abokrtsky, andA.
Z?ele.
2006.
Towards a Slovene dependency treebank.
InProc.
of the Fifth Intern.
Conf.
on Language Resources andEvaluation (LREC).J.
Edmonds.
1967.
Optimum branchings.
Journal of Researchof the National Bureau of Standards, 71B:233?240.J.
Eisner.
1996.
Three new probabilistic models for depen-dency parsing: An exploration.
In Proc.
COLING.A.
Globerson, T. Koo, X. Carreras, and M. Collins.
2007.
Ex-ponentiated gradient algorithms for log-linear structured pre-diction.
In Proc.
ICML.J.
Hajic?, O.
Smrz?, P. Zema?nek, J.
S?naidauf, and E. Bes?ka.
2004.Prague Arabic dependency treebank: Development in dataand tools.
In Proc.
of the NEMLAR Intern.
Conf.
on ArabicLanguage Resources and Tools, pages 110?117.M.
Johnson, S. Geman, S. Canon, Z. Chi, and S. Riezler.
1999.Estimators for stochastic unification-based grammars.
InProc.
ACL.M.
Johnson.
2001.
Joint and conditional estimation of taggingand parsing models.
In Proc.
ACL.Y.
Kawata and J. Bartels.
2000.
Stylebook for the Japanesetreebank in VERBMOBIL.
Verbmobil-Report 240, Seminarfu?r Sprachwissenschaft, Universita?t Tu?bingen.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditonal ran-dom fields: Probabilistic models for segmenting and labelingsequence data.
In Proc.
ICML.R.
McDonald and F. Pereira.
2006.
Online learning of approx-imate dependency parsing algorithms.
In Proc.
EACL.R.
McDonald and G. Satta.
2007.
On the complexity of non-projective data-driven dependency parsing.
In Proc.
IWPT.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
Onlinelarge-margin training of dependency parsers.
In Proc.
ACL.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005b.Non-projective dependency parsing using spanning tree al-gorithms.
In Proc.
HLT-EMNLP.R.
McDonald, K. Lerman, and F. Pereira.
2006.
Multilingualdependency parsing with a two-stage discriminative parser.In Proc.
CoNLL-X.J.
Nivre, J.
Hall, and J. Nilsson.
2004.
Memory-based depen-dency parsing.
In Proc.
CoNLL.K.
Oflazer, B.
Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003.Building a Turkish treebank.
In A.
Abeille?, editor, Tree-banks: Building and Using Parsed Corpora, chapter 15.Kluwer Academic Publishers.M.A.
Paskin.
2001.
Cubic-time parsing and learning algo-rithms for grammatical bigram models.
Technical ReportUCB/CSD-01-1148, University of California, Berkeley.J.
Pearl.
1988.
Probabilistic Reasoning in Intelligent Sys-tems: Networks of Plausible Inference (2nd edition).
Mor-gan Kaufmann Publishers.S.
Riezler, R. Kaplan, T. King, J. Maxwell, A. Vasserman, andR.
Crouch.
2004.
Speed and accuracy in shallow and deepstochastic parsing.
In Proc.
HLT-NAACL.F.
Sha and F. Pereira.
2003.
Shallow parsing with conditionalrandom fields.
In Proc.
HLT-NAACL.N.A.
Smith and J. Eisner.
2005.
Contrastive estimation: Train-ing log-linear models on unlabeled data.
In Proc.
ACL.D.A.
Smith and N.A.
Smith.
2007.
Probabilistic models ofnonprojective dependency trees.
In Proc.
EMNLP-CoNLL.B.
Taskar, C. Guestrin, and D. Koller.
2004a.
Max-marginmarkov networks.
In NIPS.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.2004b.
Max-margin parsing.
In Proc.
EMNLP.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.2004.
Support vector machine learning for interdependentand structured output spaces.
In Proc.
ICML.W.
Tutte.
1984.
Graph Theory.
Addison-Wesley.L.
van der Beek, G. Bouma, R. Malouf, and G. van Noord.2002.
The Alpino dependency treebank.
In ComputationalLinguistics in the Netherlands (CLIN).Y.
Matsumoto Y. Cheng, M. Asahara.
2005.
Machine learning-based dependency analyzer for chinese.
In Proc.
ICCC.H.
Yamada and Y. Matsumoto.
2003.
Statistical dependencyanalysis with support vector machines.
In Proc.
IWPT.150
