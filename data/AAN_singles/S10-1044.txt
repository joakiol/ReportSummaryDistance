Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 202?205,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsFBK NK: a WordNet-based Systemfor Multi-Way Classification of Semantic RelationsMatteo Negri and Milen KouylekovFBK-IrstTrento, Italy{negri,kouylekov}@fbk.euAbstractWe describe a WordNet-based system forthe extraction of semantic relations be-tween pairs of nominals appearing inEnglish texts.
The system adopts alightweight approach, based on traininga Bayesian Network classifier using largesets of binary features.
Our features con-sider: i) the context surrounding the an-notated nominals, and ii) different typesof knowledge extracted from WordNet, in-cluding direct and explicit relations be-tween the annotated nominals, and moregeneral and implicit evidence (e.g.
seman-tic boundary collocations).
The systemachieved a Macro-averaged F1 of 68.02%on the ?Multi-Way Classification of Se-mantic Relations Between Pairs of Nom-inals?
task (Task #8) at SemEval-2010.1 IntroductionThe ?Multi-Way Classification of Semantic Re-lations Between Pairs of Nominals?
task atSemEval-2010 (Hendrickx et al, 2010) consistsin: i) selecting from an inventory of nine possi-ble relations the one that most likely holds be-tween two annotated nominals appearing in the in-put sentence, and ii) specifying the order of thenominals as the arguments of the relation.
In con-trast with the semantic relations classification task(Task #4) at SemEval-2007 (Girju et al, 2007),which treated each semantic relation separately asa single two-class (positive vs. negative) classifi-cation task, this year?s edition of the challenge pre-sented participating systems with a more difficultand realistic multi-way setup, where the relationOther can also be assigned if none of the nine re-lations is suitable for a given sentence.
Examplesof the possible markable relations are reported inTable 11.The objective of our experiments with the pro-posed task is to develop a Relation Extraction sys-tem based on shallow linguistic processing, takingthe most from available thesauri and ontologies.As a first step in this direction, our submitted runshave been obtained by processing the input sen-tences only to lemmatize their terms, and by usingWordNet as the sole source of knowledge.Similar to other approaches (Moldovan andBadulescu, 2009; Beamer et al, 2009), our sys-tem makes use of semantic boundaries extractedfrom the WordNet IS-A backbone.
Such bound-aries (i.e.
divisions in the WordNet hierarchythat best generalize over the training examples)are used to define pairs of high-level synsets withhigh correlation with specific relations.
For in-stance, <microorganism#1, happening#1> and<writing#1, consequence#1> are extracted fromthe training data as valid high-level collocationsrespectively for the relations Cause-Effect andMessage-Topic.
Besides exploiting the Word-Net IS-A hierarchy, the system also uses theholo-/meronymy relations, and information de-rived from the WordNet glosses to capture specificrelations such as Member-Collection and Product-Producer.
In addition, the context surroundingthe annotated nominals is represented as a bag-of-words/synonyms to enhance the relation extractionprocess.
Several experiments have been carriedout encoding all the information as large sets ofbinary features (up to ?6200) to train a BayesianNetwork classifier available in the Weka2toolkit.To capture both the relations and the order of1In the first example the order of the nominals is(<e2>,<e1>), while in the others is (<e1>,<e2>)2http://www.cs.waikato.ac.nz/ml/weka/2021 Cause-Effect(e2,e1) A person infected with a particular <e1>flu</e1> <e2>virus</e2> strain develops anantibody against that virus.2 Instrument-Agency(e1,e2) The <e1>river</e1> once powered a <e2>grist mill</e2>.3 Product-Producer(e1,e2) The <e1>honey</e1><e2>bee</e2> is the third insect genome published by scientists,after a lab workhorse, the fruit fly, and a health menace, the mosquito.4 Content-Container(e1,e2) I emptied the <e1>wine</e1> <e2>bottle</e2> into my glass and toasted my friends.5 Entity-Origin(e1,e2) <e1>This book</e1>is from the 17th <e2>century</e2>.6 Entity-Destination(e1,e2) <e1>Suspects</e1> were handed over to the <e2>police station</e2>.7 Component-Whole(e1,e2) <e1>Headlights</e1> are considered as the eyes of the <e2>vehicle</e2>.8 Member-Collection(e1,e2)Mary looked back and whispered: ?I know every <e1>tree</e1> in this<e2>forest</e2>, every scent?.9 Message-Topic(e1,e2) Here we offer a selection of our favourite <e1>books</e1> on military<e2>history</e2>.Table 1: SemEval-2010 Task #8 semantic relations.their arguments, training sentences having oppo-site argument directions for the same relation havebeen handled separately, and assigned to differentclasses (thus obtaining 18 classes for the nine tar-get relations, plus one for the Other relation).The following sections overview our experi-ments, describing the features used by the sys-tem (Section 2), and the submitted runs with theachieved results (Section 3).
A concluding discus-sion on the results is provided in Section 4.2 Features usedThe system uses two types of boolean features:WordNet features, and context features.2.1 WordNet featuresWordNet features consider different types ofknowledge extracted from WordNet 3.0.Semantic boundary collocations.
Collocationsof high-level synsets featuring a high correlationwith specific relations are acquired from the train-ing set using a bottom-up approach.
Starting fromthe nominals annotated in the training sentences(<e1> and<e2>), the WordNet IS-A backbone isclimbed to collect all their ancestors.
Then, all theancestors?
collocations occurring at least n timesfor at most m relations are retained, and treatedas boolean features (set to 1 for a given sentenceif its annotated nominals appear among their hy-ponyms).
The n and m parameters are optimizedon the training set.Holo-/meronymy relations.
These boolean fea-tures are set to 1 every time a pair of annotatednominals in a sentence is directly connected byholo-/meronyny relations.
They are particularlyappropriate to capture the Component-Whole andMember-Collection relations, as in the 8th exam-ple in Table 1 (where tree#1 is an holonym offorest#1).
Due to time constraints, we did notexplore the possibility to generalize these fea-tures considering transitive closures of the nomi-nals?
hypo-/hypernyms.
This possibility could al-low to handle sentences like ?A <e1>herd</e1>is a large group of <e2>animals</e2>.?
Here,though herd#1 and animal#1 are not directly con-nected by the meronymy relation, all the herd#1meronyms have animal#1 as a common ancestor.Glosses.
Given a pair of annotated nominals<e1>,<e2>, these features are set to 1 every timeeither <e1> appears in the gloss of <e2>, orvice-versa.
They are intended to support the dis-covery of relations in the case of consecutive nom-inals (e.g.
honey#1 and bee#1 in the 3rd examplein Table 1), where contextual information does notprovide sufficient clues to make a choice.
In ourexperiments we extracted features from both tok-enized and lemmatized words (both nominals, andgloss words).
Also in this case, due to time con-straints we did not explore the possibility to gener-alize the feature considering the nominals?
hypo-/hypernyms.
This possibility could allow to handlesentences like examples 1 and 4 in Table 1.
Forinstance in example 4, the gloss of ?bottle?
con-tains two hypernyms of wine#1, namely drink#3and liquid#1, that could successfully trigger theContent-Container relation.Synonyms.
While the previous features operatewith the annotated nominals, WordNet synonymsare used to generalize the other terms in the sen-tence, allowing to extract different types of con-textual features (see the next Section).2.2 Context featuresBesides the annotated nominals, also specificwords (and word combinations) appearing in thesurrounding context often contribute to trigger the203target relations.
Distributional evidence is cap-tured by considering word contexts before, be-tween, and after the annotated nominals.
To thisaim, we experimented with windows of differentsize, containing words that occur in the trainingset a variable number of times.
Both the parame-ters (i.e.
the size of the windows, and the numberof occurrences) are optimized on training data.
Inour experiments we extracted contextual featuresfrom lemmatized sentences.3 Submitted runs and resultsOur participation to the SemEval-2010 Task#8 consisted in four runs, with the best one(FBK NK-RES1) achieving a Macro-averaged F1of 68.02% on the test data.
For this submis-sion, the overall training and test running times areabout 12?30?
and 1?30?
respectively, on an IntelCore2 Quad 2.66GHz with 4GB RAM.FBK NK-RES1.
This run has been obtainedadopting a conservative approach, trying to min-imize the risk of overfitting the training data.
Thefeatures used can be summarized as follows:?
Semantic boundary collocations: all the col-locations of <e1> and <e2> ancestors oc-curring at least 10 times in the training set (mparam.
), for at most 3 relations (n param.);?
Holo-/meronymy relations between the anno-tated nominals;?
Glosses: handled at the level of tokens;?
Context features: left, between, and rightcontext windows of size 3-ALL-3 words re-spectively.
Number of occurrences: 25 (left),10 (between), 25 (right).On the training set, the Bayesian Network classi-fier (trained with 2239 features, and evaluated with10-fold cross-validation) achieves an Accuracy of65.62% (5249 correctly classified instances out of8000), and a Macro F1 of 78.15%.FBK NK-RES2.
Similar to the first run, but:?
Semantic boundary collocations: m=9, n=3;?
Glosses: handled at the level of lemmas;?
Context features: left, between, and rightcontext windows of size 4-ALL-1 words re-spectively (occurrences: 25-10-25).Run 1000 2000 4000 8000FBK NK-RES1 55.71 64.06 67.80 68.02FBK NK-RES2 54.27 63.68 67.08 67.48FBK NK-RES3 54.25 62.73 66.11 66.90FBK NK-RES4 44.11 58.85 63.06 65.84Table 2: Test results (Macro-averaged F1) usingdifferent amounts of training sentences.Based on the observation of system?s behaviour onthe training data, the objectives of this run were to:i) add more collocations as features, ii) increasethe importance of terms appearing in the left con-text, iii) reduce the importance of terms appearingin the right context, and iv) increase the possibil-ity of matching the nominals with gloss terms byconsidering their respective lemmas.
On the train-ing set, the classifier (trained with 2998 features)achieves 66.92% Accuracy (5353 correctly classi-fied instances), and a Macro F1 of 79.56%.FBK NK-RES3.
Similar to the second run, butconsidering the synonyms of the most frequentsense of the words between <e1> and <e2>.The goal of this run was to generalize the con-text between nominals, by considering word lem-mas.
On the training set, the classifier (trainedwith 2998 features) achieves an Accuracy of64.94% (5195 correctly classified instances), anda Macro F1 of 77.38%.FBK NK-RES4.
Similar to the second run, butconsidering semantic boundary collocations oc-curring at least 7 times in the training set (mparam.
), for at most 3 relations (n param.
).The goal of this run was to further increase thenumber of collocations used as features.
On thetraining set, the classifier (trained with 6233 fea-tures) achieves achieves 68.12% Accuracy (5449correct classifications), and 82.24% Macro F1.As regards the results on the test set, Table 2 re-ports the scores achieved by each run using differ-ent portions of the training set (1000, 2000, 4000,8000 examples), while Figure 1 shows the learn-ing curves for each relation of our best run.4 Discussion and conclusionAs can be seen from Table 2, the results contra-dict our expectations about the effectiveness of ourless conservative configurations and, in particular,about the utility of using larger amounts of se-mantic boundary collocations.
The performance204Figure 1: Learning curves on the test set(FBK NK-RES1).decrease from Run2 to Run43clearly indicates anoverfitting problem.
Though suitable to model thetraining data, the additional collocations were notencountered in the test set.
This caused a bias to-wards the Other relation, which reduced the over-all performance of the system.Regarding our best run, Figure 1 shows dif-ferent system?s behaviours with the different tar-get relations.
For some of them (e.g.
Entity-Destination, Cause-Effect) better results are mo-tivated by the fact that they are often triggeredby frequent unambiguous word patterns (e.g.
?<e1>has been moved to a <e2>?, ?<e1>causes <e2>?).
Such relations are effectivelyhandled by the context features which, in contrast,are inadequate for those expressed with high lex-ical variability.
This is particularly evident withthe Other relation, for which the acquired contextfeatures poorly discriminate positive from nega-tive examples even on the training set.For some relations additional evidence is suc-cessfully brought by the WordNet features.
Forinstance, the good results for Member-Collectiondemonstrate the usefulness of the holo-/meronymyfeatures.As regards semantic boundary collocations, tocheck their effectiveness we performed a post-hocanalysis of those used in our best run.
Such anal-ysis was done in two ways: i) by counting thenumber of collocations acquired on the trainingset for each relation Ri, and ii) by calculating theambiguity of each Ri?s collocation on the train-3The only difference between Run2 and Run4 is the addi-tion of around 4000 semantic boundary collocations, whichlead to an overall 2.4% F1 performance decrease.
The de-crease mainly comes in terms of Recall (from 65.91% inRun2 to 63.35% in Run4).ing set (i.e.
the average number of other relationsactivated by the collocation).
The analysis re-vealed that the top performing relations (Member-Collection, Entity-Destination, Cause-Effect, andContent-Container) are those for which we ac-quired lots of unambiguous collocations.
Thesefindings also explain the poor performance on theInstrument-Agency and the Other relation.
ForInstrument-Agency we extracted the lowest num-ber of collocations, which were also the most am-biguous ones.
For the Other relation the high am-biguity of the collocations extracted is not com-pensated by their huge number (around 50% of thetotal collocations acquired).In conclusion, considering i) the level of pro-cessing required (only lemmatization), ii) the factthat WordNet is used as the sole source of knowl-edge, and iii) the many possible solutions leftunexplored due to time constraints, our resultsdemonstrate the validity of our approach, de-spite its simplicity.
Future research will focuson a better use of semantic boundary colloca-tions, on more refined ways to extract knowledgefrom WordNet, and on integrating other knowl-edge sources (e.g.
SUMO, YAGO, Cyc).AcknowledgmentsThe research leading to these results has receivedfunding from the European Community?s Sev-enth Framework Programme (FP7/2007-2013) un-der Grant Agreement n. 248531 (CoSyne project).ReferencesB.
Beamer, A. Rozovskaya, and R. Girju 2008.
Au-tomatic Semantic Relation Extraction with MultipleBoundary Generation.
Proceedings of The NationalConference on Artificial Intelligence (AAAI).R.
Girju, P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-ney, and D. Yuret 2007.
SemEval-2007 task 04:Classification of semantic relations between nomi-nals.
Proceedings of the 4th Semantic EvaluationWorkshop (SemEval-2007).I.
Hendrickx et al 2010.
SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations BetweenPairs of Nominals Proceedings of the 5th SIGLEXWorkshop on Semantic Evaluation.D.
Moldovan, A. Badulescu 2005.
A Semantic Scatter-ing Model for the Automatic Interpretation of Gen-itives.
Proceedings of The Human Language Tech-nology Conference (HLT).205
