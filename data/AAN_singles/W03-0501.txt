Hedge Trimmer: A Parse-and-Trim Approach to Headline GenerationBonnie Dorr, David ZajicUniversity of Marylandbonnie, dmzajic@umiacs.umd.eduRichard SchwartzBBNschwartz@bbn.comAbstractThis paper presents Hedge Trimmer, a HEaDlineGEneration system that creates a headline for a newspa-per story using linguistically-motivated heuristics toguide the choice of a potential headline.
We presentfeasibility tests used to establish the validity of an ap-proach that constructs a headline by selecting words inorder from a story.
In addition, we describe experimen-tal results that demonstrate the effectiveness of our lin-guistically-motivated approach over a HMM-basedmodel, using both human evaluation and automatic met-rics for comparing the two approaches.1 IntroductionIn this paper we present Hedge Trimmer, a HEaD-line GEneration system that creates a headline for anewspaper story by removing constituents from a parsetree of the first sentence until a length threshold hasbeen reached.
Linguistically-motivated heuristics guidethe choice of which constituents of a story should bepreserved, and which ones should be deleted.
Our focusis on headline generation for English newspaper texts,with an eye toward the production of document surro-gates?for cross-language information retrieval?andthe eventual generation of readable headlines fromspeech broadcasts.In contrast to original newspaper headlines, whichare often intended only to catch the eye, our approachproduces informative abstracts describing the maintheme or event of the newspaper article.
We claim thatthe construction of informative abstracts requires accessto deeper linguistic knowledge, in order to make sub-stantial improvements over purely statistical ap-proaches.In this paper, we present our technique for produc-ing headlines using a parse-and-trim approach based onthe BBN Parser.
As described in Miller et al (1998), theBBN parser builds augmented parse trees according to aprocess similar to that   described in Collins (1997).The BBN parser has been used successfully for the taskof information extraction in the SIFT system (Miller etal., 2000).The next section presents previous work in the areaof automatic generation of abstracts.
Following this, wepresent feasibility tests used to establish the validity ofan approach that constructs headlines from words in astory, taken in order and focusing on the earlier part ofthe story.
Next, we describe the application of theparse-and-trim approach to the problem of headlinegeneration.
We discuss the linguistically-motivatedheuristics we use to produce results that are headline-like.
Finally, we evaluate Hedge Trimmer by compar-ing it to our earlier work on headline generation, a prob-abilistic model for automatic headline generation (Zajicet al 2002).
In this paper we will refer to this statisticalsystem as HMM Hedge  We demonstrate the effective-ness of our linguistically-motivated approach, HedgeTrimmer, over the probabilistic model, HMM Hedge,using both human evaluation and automatic metrics.2 Previous WorkOther researchers have investigated the topic ofautomatic generation of abstracts, but the focus has beendifferent, e.g., sentence extraction (Edmundson, 1969;Johnson et al 1993; Kupiec et al, 1995; Mann et al,1992; Teufel and Moens, 1997; Zechner, 1995), proc-essing of structured templates (Paice and Jones, 1993),sentence compression (Hori et al, 2002; Knight andMarcu, 2001; Grefenstette, 1998, Luhn, 1958), and gen-eration of abstracts from multiple sources (Radev andMcKeown, 1998).
We focus instead on the constructionof headline-style abstracts from a single story.Headline generation can be viewed as analogous tostatistical machine translation, where a concise docu-ment is generated from a verbose one using a NoisyChannel Model and the Viterbi search to select the mostlikely summarization.
This approach has been exploredin (Zajic et al, 2002) and (Banko et al, 2000).The approach we use in Hedge is most similar tothat of (Knight and Marcu, 2001), where a single sen-tence is shortened using statistical compression.
As inthis work, we select headline words from story words inthe order that they appear in the story?in particular, thefirst sentence of the story.
However, we use linguisti-cally motivated heuristics for shortening the sentence;there is no statistical model, which means we do notrequire any prior training on a large corpus ofstory/headline pairs.Linguistically motivated heuristics have been usedby (McKeown et al 2002) to distinguish constituents ofparse trees which can be removed without affectinggrammaticality or correctness.
GLEANS (Daum?
et al2002) uses parsing and named entity tagging to fill val-ues in headline templates.Consider the following excerpt from a news story:(1) Story Words: Kurdish guerilla forces movingwith lightning speed poured into Kirkuk todayimmediately after Iraqi troops, fleeing relent-less U.S. airstrikes, abandoned the hub of Iraq?srich northern oil fields.Generated Headline: Kurdish guerilla forcespoured into Kirkuk after Iraqi troops abandonedoil fields.In this case, the words in bold form a fluent andaccurate headline for the story.
Italicized words aredeleted based on information provided in a parse-treerepresentation of the sentence.3 Feasibility TestingOur approach is based on the selection of wordsfrom the original story, in the order that they appear inthe story, and allowing for morphological variation.
Todetermine the feasibility of our headline-generation ap-proach, we first attempted to apply our ?select-words-in-order?
technique by hand.
We asked two subjects towrite headline headlines for 73 AP stories from theTIPSTER corpus for January 1, 1989, by selectingwords in order from the story.
Of the 146 headlines, 2did not meet the ?select-words-in-order?
criteria be-cause of accidental word reordering.
We found that atleast one fluent and accurate headline meeting the crite-ria was created for each of the stories.
The averagelength of the headlines was 10.76 words.Later we examined the distribution of the headlinewords among the sentences of the stories, i.e.
how manycame from the first sentence of a story, how many fromthe second sentence, etc.
The results of this study areshown in Figure 1.
We observe that 86.8% of the head-line words were chosen from the first sentence of theirstories.
We performed a subsequent study in which twosubjects created 100 headlines for 100 AP stories fromAugust 6, 1990.
51.4% of the headline words in thesecond set were chosen from the first sentence.
Thedistribution of headline words for the second set shownin Figure 2.Although humans do not always select headlinewords from the first sentence, we observe that a largepercentage of headline words are often found in the firstsentence.0.00%10.00%20.00%30.00%40.00%50.00%60.00%70.00%80.00%90.00%100.00%N=1N=2N=3N=4N=5N=6N=7N=8N=9N>=10Figure 1: Percentage of words from human-generatedheadlines drawn from Nth sentence of story (Set 1)0.00%10.00%20.00%30.00%40.00%50.00%60.00%N=1N=2N=3N=4N=5N=6N=7N=8N=9N>=10Figure 2:  Percentage of words from human-generated head-lines drawn from Nth sentence of story (Set 2)4 ApproachThe input to Hedge is a story, whose first sentence isimmediately passed through the BBN parser.
Theparse-tree result serves as input to a linguistically-motivated module that selects story words to form head-lines based on key insights gained from our observa-tions of human-constructed headlines.
That is, weconducted a human inspection of the 73 TIPSTER sto-ries mentioned in Section 3 for the purpose of develop-ing the Hedge Trimmer algorithm.Based on our observations of human-producedheadlines, we developed the following algorithm forparse-tree trimming:1.
Choose lowest leftmost S with NP,VP2.
Remove low content unitso some determinerso time expressions3.
Iterative shortening:o XP Reductiono Remove preposed adjunctso Remove trailing PPso Remove trailing SBARsMore recently, we conducted an automatic analysisof the human-generated headlines that supports severalof the insights gleaned from this initial study.
We parsed218 human-produced headlines using the BBN parserand analyzed the results.
For this analysis, we used 72headlines produced by a third participant.1 The parsingresults included 957 noun phrases (NP) and 315 clauses(S).We calculated percentages based on headline-level,NP-level, and Sentence-level structures in the parsingresults.
That is, we counted:?
The percentage of the 957 NPs containing de-terminers and relative clauses?
The percentage of the 218 headlines containingpreposed adjuncts and conjoined S or VPs?
The percentage of the 315 S nodes containingtrailing time expressions, SBARs, and PPsFigure 3 summarizes the results of this automatic analy-sis.
In our initial human inspection, we considered eachof these categories to be reasonable candidates for dele-tion in our parse tree and this automatic analysis indi-cates that we have made reasonable choices for deletion,with the possible exception of trailing PPs, which showup in over half of the human-generated headlines.
Thissuggests that we should proceed with caution with re-spect to the deletion of trailing PPs; thus we considerthis to be an option only if no other is available.HEADLINE-LEVEL PERCENTAGESpreposed adjuncts = 0/218 (0%)conjoined S = 1/218 ( .5%)conjoined VP = 7/218 (3%)NP-LEVEL PERCENTAGESrelative clauses = 3/957 (.3%)determiners = 31/957 (3%); of these, only16 were ?a?
or ?the?
(1.6% overall)S-LEVEL PERCENTAGES2time expressions = 5/315 (1.5%)trailing PPs = 165/315 (52%)trailing SBARs = 24/315 (8%)Figure 3: Percentages found in human-generated headlines1No response was given for one of the 73 stories.2Trailing constituents (SBARs and PPs) are computed bycounting the number of SBARs (or PPs) not designated as anargument of (contained in) a verb phrase.For a comparison, we conducted a second analysisin which we used the same parser on just the first sen-tence of each of the 73 stories.
In this second analysis,the parsing results included 817 noun phrases (NP) and316 clauses (S).
A summary of these results is shown inFigure 4.
Note that, across the board, the percentagesare higher in this analysis than in the results shown inFigure 3 (ranging from 12% higher?in the case of trail-ing PPs?to 1500% higher in the case of time expres-sions), indicating that our choices of deletion in theHedge Trimmer algorithm are well-grounded.HEADLINE-LEVEL PERCENTAGESpreposed adjuncts = 2/73 (2.7%)conjoined S = 3/73 (4%)conjoined VP = 20/73 (27%)NP-LEVEL PERCENTAGESrelative clauses = 29/817 (3.5%)determiners = 205/817 (25%); of these,only 171 were ?a?
or ?the?
(21% overall)S-LEVEL PERCENTAGEStime expressions = 77/316 (24%)trailing PPs = 184/316 (58%)trailing SBARs =  49/316 (16%)Figure 4: Percentages found in first sentence ofeach story.4.1 Choose the Correct S NodeThe first step relies on what is referred to as theProjection Principle in linguistic theory (Chomsky,1981): Predicates project a subject (both dominated byS) in the surface structure.
Our human-generated head-lines always conformed to this rule; thus, we adopted itas a constraint in our algorithm.An example of the application of step 1 above isthe following, where boldfaced material from the parsetree representation is retained and italicized material iseliminated:(2) Input: Rebels agree to talks with government of-ficials said Tuesday.Parse: [S [S [NP Rebels] [VP agree to talkswith government]] officials said Tuesday.
]Output of step 1: Rebels agree to talks with gov-ernment.When the parser produces a correct tree, this step pro-vides a grammatical headline.
However, the parser of-ten produces an incorrect output.
Human inspection ofour 624-sentence DUC-2003 evaluation set revealedthat there were two such scenarios, illustrated by thefollowing cases:(3) [S [SBAR What started as a local contro-versy] [VP has evolved into an internationalscandal.
]](4) [NP [NP Bangladesh] [CC and] [NP [NP In-dia] [VP signed a water sharing accord.
]]]In the first case, an S exists, but it does not conformto the requirements of step 1.
This occurred in 2.6% ofthe sentences in the DUC-2003 evaluation data.
Weresolve this by selecting the lowest leftmost S, i.e., theentire string ?What started as a local controversy hasevolved into an international scandal?
in the exampleabove.In the second case, there is no S available.
This oc-curred in 3.4% of the sentences in the evaluation data.We resolve this by selecting the root of the parse tree;this would be the entire string ?Bangladesh and Indiasigned a water sharing accord?
above.
No other parsererrors were encountered in the DUC-2003 evaluationdata.4.2 Removal of Low Content NodesStep 2 of our algorithm eliminates low-contentunits.
We start with the simplest low-content units: thedeterminers a and the.
Other determiners were not con-sidered for deletion because our analysis of the human-constructed headlines revealed that most of the otherdeterminers provide important information, e.g., nega-tion (not), quantifiers (each, many, several), and deictics(this, that).Beyond these, we found that the human-generatedheadlines contained very few time expressions which,although certainly not content-free, do not contributetoward conveying the overall ?who/what content?
of thestory.
Since our goal is to provide an informative head-line (i.e., the action and its participants), the identifica-tion and elimination of time expressions provided asignificant boost in the performance of our automaticheadline generator.We identified time expressions in the stories usingBBN?s IdentiFinder?
(Bikel et al 1999).
We imple-mented the elimination of time expressions as a two-step process:?
Use IdentiFinder to mark time expressions?
Remove [PP ?
[NP [X] ?]
?]
and [NP [X]]where X is tagged as part of a time expressionThe following examples illustrate the application ofthis step:(5) Input: The State Department on Friday lifted theban it had imposed on foreign fliers.Parse:  [Det The] State Department [PP [INon] [NP [NNP Friday]]] lifted [Det the] ban ithad imposed on foreign fliers.Output of step 2: State Department lifted ban ithas imposed on foreign fliers.
(6) Input: An international relief agency announcedWednesday that it is withdrawing from NorthKorea.Parse:  [Det An] international relief agency an-nounced [NP [NNP Wednesday]] that it is with-drawing from North Korea.Output of step 2: International relief agency an-nounced that it is withdrawing from North Korea.We found that 53.2% of the stories we examinedcontained at least one time expression which could bedeleted.
Human inspection of the 50 deleted time ex-pressions showed that 38 were desirable deletions, 10were locally undesirable because they introduced anungrammatical fragment,3 and 2 were undesirable be-cause they removed a potentially relevant constituent.However, even an undesirable deletion often pans outfor two reasons: (1) the ungrammatical fragment is fre-quently deleted later by some other rule; and (2) everytime a constituent is removed it makes room under thethreshold for some other, possibly more relevant con-stituent.
Consider the following examples.
(7) At least two people were killed Sunday.
(8) At least two people were killed when single-engine airplane crashed.Example (7) was produced by a system which did notremove time expressions.
Example (8) shows that if thetime expression Sunday were removed, it would makeroom below the 10-word threshold for another impor-tant piece of information.4.3 Iterative ShorteningThe final step, iterative shortening, removes lin-guistically peripheral material?through successive de-letions?until the sentence is shorter than a giventhreshold.
We took the threshold to be 10 for the DUCtask, but it is a configurable parameter.
Also, given thatthe human-generated headlines tended to retain earliermaterial more often than later material, much of our3Two examples of genuinely undesirable time expression deletionare:?
The attack came on the heels of [New Year?s Day].?
[New Year?s Day] brought a foot of snow to the region.iterative shortening is focused on deleting the rightmostphrasal categories until the length is below threshold.There are four types of iterative shortening rules.The first type is a rule we call ?XP-over-XP,?
which isimplemented as follows:In constructions of the form [XP [XP ?]
?]
re-move the other children of the higher XP, whereXP is NP, VP or S.This is a linguistic generalization that allowed us applya single rule to capture three different phenomena (rela-tive clauses, verb-phrase conjunction, and sententialconjunction).
The rule is applied iteratively, from thedeepest rightmost applicable node backwards, until thelength threshold is reached.The impact of XP-over-XP can be seen in these ex-amples of NP-over-NP (relative clauses), VP-over-VP(verb-phrase conjunction), and S-over-S (sentential con-junction), respectively:(9) Input: A fire killed a firefighter who was fatallyinjured as he searched the house.Parse:  [S [Det A] fire killed [Det a]  [NP [NPfirefighter] [SBAR who was fatally injured ashe searched the house] ]]Output of NP-over-NP: fire killed firefighter(10) Input: Illegal fireworks injured hundreds of peo-ple and started six fires.Parse:  [S Illegal fireworks [VP [VP injuredhundreds of people] [CC and] [VP started sixfires] ]]Output of VP-over-VP: Illegal fireworks injuredhundreds of people(11) Input: A company offering blood cholesteroltests in grocery stores says medical technologyhas outpaced state laws, but the state says thecompany doesn?t have the proper licenses.Parse:  [S [Det A] company offering blood cho-lesterol tests in grocery stores says [S [S medi-cal technology has outpaced state laws], [CCbut] [S [Det the] state stays [Det the] companydoesn?t have [Det the] proper licenses.]]
]Output of S-over-S: Company offering bloodcholesterol tests in grocery store says medicaltechnology has outpaced state lawsThe second type of iterative shortening is the re-moval of preposed adjuncts.
The motivation for thistype of shortening is that all of the human-generatedheadlines ignored what we refer to as the preamble ofthe story.
Assuming the Projection principle has beensatisfied, the preamble is viewed as the phrasal materialoccurring before the subject of the sentence.
Thus, ad-juncts are identified linguistically as any XP unit pre-ceding the first NP (the subject) under the S chosen bystep 1.
This type of phrasal modifier is invisible to theXP-over-XP rule, which deletes material under a nodeonly if it dominates another node of the same phrasalcategory.The impact of this type of shortening can be seen inthe following example:(12) Input: According to a now finalized blueprint de-scribed by U.S. officials and other sources, theBush administration plans to take complete, unilat-eral control of a post-Saddam Hussein IraqParse:  [S [PP According to a now-finalized blue-print described by U.S. officials and other sources][Det the] Bush administration plans to takecomplete, unilateral control of [Det a] post-Saddam Hussein Iraq ]Output of Preposed Adjunct Removal: Bush ad-ministration plans to take complete unilateral con-trol of post-Saddam Hussein IraqThe third and fourth types of iterative shorteningare the removal of trailing PPs and SBARs, respec-tively:?
Remove PPs from deepest rightmost node back-ward until length is below threshold.?
Remove SBARs from deepest rightmost nodebackward until length is below threshold.These are the riskiest of the iterative shortening rules,as indicated in our analysis of the human-generatedheadlines.
Thus, we apply these conservatively, onlywhen there are no other categories of rules to apply.Moreover, these rules are applied with a backoff optionto avoid over-trimming the parse tree.
First the PPshortening rule is applied.
If the threshold has beenreached, no more shortening is done.
However, if thethreshold has not been reached, the system reverts to theparse tree as it was before any PPs were removed, andapplies the SBAR shortening rule.
If the threshold stillhas not been reached, the PP rule is applied to the resultof the SBAR rule.Other sequences of shortening rules are possible.The one above was observed to produce the best resultson a 73-sentence development set of stories from theTIPSTER corpus.
The intuition is that, when removingconstituents from a parse tree, it?s best to removesmaller portions during each iteration, to avoid produc-ing trees with undesirably few words.
PPs tend to rep-resent small parts of the tree while SBARs representlarge parts of the tree.
Thus we try to reach the thresh-old by removing small constituents, but if we can?treach the threshold that way, we restore the small con-stituents, remove a large constituent and resume thedeletion of small constituents.The impact of these two types of shortening can beseen in the following examples:(13) Input: More oil-covered sea birds were foundover the weekend.Parse:  [S More oil-covered sea birds werefound [PP over the weekend]]Output of PP Removal: More oil-covered seabirds were found.
(14) Input: Visiting China Interpol chief expressedconfidence in Hong Kong?s smooth transitionwhile assuring closer cooperation after HongKong returns.Parse:  [S Visiting China Interpol chief ex-pressed confidence in Hong Kong?s smoothtransition [SBAR while assuring closer coopera-tion after Hong Kong returns]]Output of SBAR Removal: Visiting China Inter-pol chief expressed confidence in Hong Kong?ssmooth transition5 EvaluationWe conducted two evaluations.
One was an informalhuman assessment and one was a formal automaticevaluation.5.1 HMM HedgeWe compared our current system to a statisticalheadline generation system we presented at the 2001DUC Summarization Workshop (Zajic et al, 2002),which we will refer to as HMM Hedge.
HMM Hedgetreats the summarization problem as analogous to statis-tical machine translation.
The verbose language, arti-cles, is treated as the result of a concise language,headlines, being transmitted through a noisy channel.The result of the transmission is that extra words areadded and some morphological variations occur.
TheViterbi algorithm is used to calculate the most likelyunseen headline to have generated the seen article.
TheViterbi algorithm is biased to favor headline-like char-acteristics gleaned from observation of human perform-ance of the headline-construction task.
Since the 2002Workshop, HMM Hedge has been enhanced by incorpo-rating part of speech of information into the decodingprocess, rejecting headlines that do not contain a wordthat was used as a verb in the story, and allowing mor-phological variation only on words that were used asverbs in the story.
HMM Hedge was trained on 700,000news articles and headlines from the TIPSTER corpus.5.2 Bleu: Automatic EvaluationBLEU (Papineni et al 2002) is a system for auto-matic evaluation of machine translation.
BLEU uses amodified n-gram precision measure to compare machinetranslations to reference human translations.
We treatsummarization as a type of translation from a verboselanguage to a concise one, and compare automaticallygenerated headlines to human generated headlines.For this evaluation we used 100 headlines createdfor 100 AP stories from the TIPSTER collection forAugust 6, 1990 as reference summarizations for thosestories.
These 100 stories had never been run througheither system or evaluated by the authors prior to thisevaluation.
We also used the 2496 manual abstracts forthe DUC2003 10-word summarization task as referencetranslations for the 624 test documents of that task.
Weused two variants of HMM Hedge, one which selectsheadline words from the first 60 words of the story, andone which selects words from the first sentence of thestory.
Table 1 shows the BLEU score using trigrams,and the 95% confidence interval for the score.AP900806 DUC2003HMM60 0.0997 ?
0.0322avg len: 8.620.1050 ?
0.0154avg len: 8.54HMM1Sent 0.0998 ?
0.0354avg len: 8.780.1115 ?
0.0173avg len: 8.95HedgeTr 0.1067 ?
0.0301avg len: 8.270.1341 ?
0.0181avg len: 8.50Table 1These results show that although Hedge Trimmerscores slightly higher than HMM Hedge on both datasets, the results are not statistically significant.
How-ever, we believe that the difference in the quality of thesystems is not adequately reflected by this automaticevaluation.5.3 Human EvaluationHuman evaluation indicates significantly higherscores than might be guessed from the automaticevaluation.
For the 100 AP stories from the TIPSTERcorpus for August 6, 1990, the output of Hedge Trim-mer and HMM Hedge was evaluated by one human.Each headline was given a subjective score from 1 to 5,with 1 being the worst and 5 being the best.
The aver-age score of HMM Hedge was 3.01 with standard devia-tion of 1.11.
The average score of Hedge Trimmer was3.72 with standard deviation of 1.26.
Using a t-score,the difference is significant with greater than 99.9%confidence.The types of problems exhibited by the two systems arequalitatively different.
The probabilistic system is morelikely to produce an ungrammatical result or omit a nec-essary argument, as in the examples below.
(15) HMM60: Nearly drowns in satisfactory condi-tion satisfactory condition.
(16) HMM60: A county jail inmate who noticed.In contrast, the parser-based system is more likelyto fail by producing a grammatical but semanticallyuseless headline.
(17) HedgeTr:  It may not be everyone?s idea espe-cially coming on heels.Finally, even when both systems produce accept-able output, Hedge Trimmer usually produces headlineswhich are more fluent or include more useful informa-tion.
(18)   a. HMM60:  New Year?s eve capsizingb.
HedgeTr:  Sightseeing cruise boat capsizedand sank.
(19)   a. HMM60:  hundreds of Tibetan studentsdemonstrate in Lhasa.b.
HedgeTr:  Hundreds demonstrated in Lhasademanding that Chinese authorities respect cul-ture.6 Conclusions and Future WorkWe have shown the effectiveness of constructingheadlines by selecting words in order from a newspaperstory.
The practice of selecting words from the earlypart of the document has been justified by analyzing thebehavior of humans doing the task, and by automaticevaluation of a system operating on a similar principle.We have compared two systems that use this basictechnique, one taking a statistical approach and theother a linguistic approach.
The results of the linguisti-cally motivated approach show that we can build aworking system with minimal linguistic knowledge andcircumvent the need for large amounts of training data.We should be able to quickly produce a comparablesystem for other languages, especially in light of currentmulti-lingual initiatives that include automatic parserinduction for new languages, e.g.
the TIDES initiative.We plan to enhance Hedge Trimmer by using alanguage model of Headlinese, the language of newspa-per headlines (M?rdh 1980) to guide the system inwhich constituents to remove.
We Also we plan to al-low for morphological variation in verbs to produce thepresent tense headlines typical of Headlinese.Hedge Trimmer will be installed in a translingualdetection system for enhanced display of document sur-rogates for cross-language question answering.
Thissystem will be evaluated in upcoming iCLEF confer-ences.7 AcknowledgementsThe University of Maryland authors are supported,in part, by BBNT Contract 020124-7157, DARPA/ITOContract N66001-97-C-8540, and NSF CISE ResearchInfrastructure Award EIA0130422.
We would like tothank Naomi Chang and Jon Teske for generating refer-ence headlines.ReferencesBanko, M., Mittal, V., Witbrock, M. (2000).
HeadlineGeneration Based on Statistical Translation.
In Pro-ceedings of 38th Meeting of Association for Computa-tion Linguistics, Hong Kong, pp.
218-325.Bikel, D., Schwartz, R., and Weischedel, R. (1999).
Analgorithm that learns what?s in a name.
Machine Learn-ing, 34(1/3), FebruaryChomsky, Noam A.
(1981).
Lectures on Governmentand Binding, Foris Publications, Dordrecht, Holland.Collins, M. (1997).
Three generative lexicalised modelsfor statistical parsing.
In Proceedings of the 35th ACL,1997.Daum?, H., Echihabi, A., Marcu, D., Munteanu, D.,Soricut, R. (2002).
GLEANS: A Generator of LogicalExtracts and Abstracts for Nice Summaries, In Work-shop on Automatic Summarization, Philadelphia, PA,pp.
9-14.Edmundson, H. (1969).
?New methods in automaticextracting.?
Journal of the ACM, 16(2).Grefenstett, G. (1998).
Producing intelligent tele-graphic text reduction to provide an audio scanning ser-vice for the blind.
In Working Notes of the AIII SpringSymposium on Intelligent Text Summarization, StanfordUniversity, CA, pp.
111-118.Hori, C., Furui, S., Malkin, R., Yu, H., Waibel, A.(2002).
Automatic Speech Summarization Applied toEnglish Broadcast News Speech.
In Proceedings of2002 International Conference on Acoustics, Speechand Signal Processing, Istanbul, pp.
9-12.Johnson, F. C., Paice, C. D., Black, W. J., and Neal, A.P.
(1993).
?The application of linguistic processing toautomatic abstract generation.?
Journal of Documentand Text Management, 1(3):215-42.Knight, K. and Marcu, D. (2001).
?Statistics-BasedSummarization Step One: Sentence Compression,?
InProceedings of AAAI-2001.Kupiec, J., Pedersen, J., and Chen, F. (1995).
?A train-able document summarizer.?
In Proceedings of the 18thACM-SIGIR Conference.Luhn, H. P. (1958).
"The automatic creation of litera-ture abstracts."
IBM Journal of Research and Develop-ment, 2(2).Mann, W.C., Matthiesen, C.M.I.M., and Thomspson,S.A.
(1992).
Rhetorical structure theory and text analy-sis.
In Mann, W.C. and Thompson, S.A., editors, Dis-course Description.
J. Benjamins Pub.
Co., Amsterdam.M?rdh, I.
(1980).
Headlinese:  On the Grammar ofEnglish Front Page Headlines, Malmo.McKeown, K.,  Barzilay, R.,  Blair-Goldensohn, S.,Evans, D.,  Hatzivassiloglou, V., Klavans, J., Nenkova,A., Schiffman, B.,  and Sigelman, S. (2002).
?The Co-lumbia Multi-Document Summarizer for DUC 2002,?In Workshop on Automatic Summarization, Philadel-phia, PA, pp.
1-8.Miller, S., Crystal, M., Fox, H., Ramshaw, L., Schwartz,R., Stone, R., Weischedel, R. and Annotation Group, the(1998).
Algorithms that Learn to Extract Information;BBN: Description of the SIFT System as Used forMUC-7.
In Proceedings of the MUC-7.Miller, S., Ramshaw, L., Fox, H., and Weischedel, R.(2000).
?A Novel Use of Statistical Parsing to ExtractInformation from Text,?
In Proceedings of 1st Meetingof the North American Chapter of the ACL, Seattle,WA, pp.226-233.Paice, C. D. and Jones, A. P. (1993).
?The identifica-tion of important concepts in highly structured technicalpapers.?
In Proceedings of the Sixteenth Annual Inter-national ACM SIGIR conference on research and de-velopment in IR.Papineni, K., Roukos, S., Ward, T., and Zhu, W.
(2002).
?BLEU: a Method for Automatic Evaluation of Ma-chine Translation,?
In Proceedings of 40th AnnualMeeting of the Association for Computational Linguis-tics, Philadelphia, PA, pp.
331-318Radev, Dragomir R. and Kathleen R. McKeown (1998).
?Generating Natural Language Summaries from Multi-ple On-Line Sources.?
Computational Linguistics,24(3):469--500, September 1998.Teufel, Simone and Marc Moens (1997).
?Sentenceextraction as a classification task,?
In Proceedings ofthe Workshop on Intelligent and scalable Text summari-zation, ACL/EACL-1997, Madrid, Spain.Zajic, D., Dorr, B., Schwartz, R. (2002) ?AutomaticHeadline Generation for Newspaper Stories,?
In Work-shop on Automatic Summarization, Philadelphia, PA,pp.
78-85.Zechner, K. (1995).
?Automatic text abstracting by se-lecting relevant passages.?
Master's thesis, Centre forCognitive Science, University of Edinburgh.
