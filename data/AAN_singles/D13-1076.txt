Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 808?813,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsUsing Paraphrases and Lexical Semantics to Improve the Accuracy and theRobustness of Supervised Models in Situated Dialogue SystemsClaire GardentCNRS/LORIA, Nancyclaire.gardent@loria.frLina M. Rojas BarahonaUniversit?
de Lorraine/LORIA, Nancylina.rojas@loria.frAbstractThis paper explores to what extent lemmati-sation, lexical resources, distributional seman-tics and paraphrases can increase the accuracyof supervised models for dialogue manage-ment.
The results suggest that each of thesefactors can help improve performance but thatthe impact will vary depending on their com-bination and on the evaluation mode.1 IntroductionOne strand of work in dialog research targets therapid prototyping of virtual humans capable of con-ducting a conversation with humans in the contextof a virtual world.
In particular, question answering(QA) characters can respond to a restricted set oftopics after training on a set of dialogs whose utter-ances are annotated with dialogue acts (Leuski andTraum, 2008).As argued in (Sagae et al 2009), the size of thetraining corpus is a major factor in allowing QAcharacters that are both robust and accurate.
In ad-dition, the training corpus should arguably be ofgood quality in that (i) it should contain the variousways of expressing the same content (paraphrases)and (ii) the data should not be skewed.
In sum, theideal training data should be large (more data isbetter data) ; balanced (similar amount of data foreach class targeted by the classifier) and varied (itshould encompass the largest possible number ofparaphrases and synonyms for the utterances of eachclass).In this paper, we explore different ways of im-proving and complementing the training data of asupervised QA character.
We expand the size andthe quality (less skewed data) of the training corpususing paraphrase generation techniques.
We com-pare the performance obtained on lemmatised vs.non lemmatised data.
And we investigate how vari-ous resources (synonym dictionaries, WordNet, dis-tributional neighbours) can be used to handle unseenwords at run time.2 Related workPrevious work on improving robustness of super-vised dialog systems includes detecting and han-dling out of domain utterances for generating feed-back (Lane et al 2004) ; using domain-restrictedlexical semantics (Hardy et al 2004) ; and work onmanual data expansion (DeVault et al 2011).
Ourwork follows up on this research but provides a sys-tematic investigation of how data expansion, lemma-tisation and synonym handling impacts the perfor-mance of a supervised QA engine.3 Experimental SetupWe run our experiments on a dialog engine de-veloped for a serious game called Mission Plastech-nologie.
In this game, the player must interact withdifferent virtual humans through a sequence of 12subdialogs, each of them occurring in a different partof the virtual world.Training Data.
The training corpus consists ofaround 1250 Human-Human dialogues which weremanually annotated with dialog moves.
As the fol-lowing dialog excerpt illustrates, the dialogs are con-ducted in French and each dialog turn is manu-ally annotated using a set of 28 dialog acts.
For808a more detailed presentation of the training corpusand of the annotation scheme, the reader is referredto (Rojas-Barahona et al 2012a)dialog : 01_dialogDirecteur-Tue Jun 14 11 :04 :23 2011>M.Jasper : Bonjour, je suis M.Jasper le directeur.
|| greet(Hello, I am the director, Mr.
Jasper.
)>M.Jasper : Qu?est-ce que je peux faire pour vous ?
|| ask(task(X))(What can I do for you ?
)>Lucas : je dois sauver mon oncle || first_step(I must rescue my uncle)>M.Jasper : Pour faire votre manette, il vous fautdes plans.
Allez voir dans le bureau d?
?tudes,ils devraient y ?tre.
|| inform(do(first_step))(To build the joystick you will need the plans.You will find them in the Designing Office.
)>M.Jasper : Bonne Chance !
|| quit(Good Luck !
)Dialog Systems For our experiments, we use a hy-brid dialog system similar to that described in (Ro-jas Barahona et al 2012b; Rojas Barahona andGardent, 2012).
This system combines a classifierfor interpreting the players utterances with an infor-mation state dialog manager which selects an appro-priate system response based on the dialog move as-signed by the classifier to the user turn.
The clas-sifier is a logistic regression classifier 1 which wastrained for each subdialog in the game.
The featuresused for training are the set of content words whichare associated with a given dialog move and whichremain after TF*IDF 2 filtering.
Note that in this ex-periment, we do not use contextual features such asthe dialog acts labeling the previous turns.
There aretwo reasons for this.
First, we want to focus on theimpact of synonym handling, paraphrasing and lem-matisation on dialog management.
Removing con-textual features allows us to focus on how contentfeatures (content words) can be improved by thesemechanisms.
Second, when evaluating on the H-Ccorpus (see below), contextual features are often in-correct (because the system might incorrectly inter-pret and thus label a user turn).
Excluding contextualfeatures from training allows for a fair comparisonbetween the H-H and the H-C evaluation.Test Data and Evaluation Metrics We use accu-1.
We used MALLET (McCallum, 2002) for the LR classi-fier with L1 Regularisation.2.
TF*IDF = Term Frequency*Inverse Document Fre-quencyracy (the number of correct classifications dividedby the number of instances in the testset) to mea-sure performance and we carry out two types ofevaluation.
On the one hand, we use 10-fold cross-validation on the EmoSpeech corpus (H-H data).
Onthe other hand, we report accuracy on a corpus of550 Human-Computer (H-C) dialogues obtained byhaving 22 subjects play the game against the QAcharacter trained on the H-H corpus.
As we shall seebelow, performance decreases in this second evalua-tion suggesting that subjects produce different turnswhen playing with a computer than with a humanthereby inducing a weak out-of-domain effect andnegatively impacting classification.
Evaluation onthe H-H corpus therefore gives a measure of howwell the techniques explored help improving the di-alog engine when used in a real life setting.Correspondingly, we use two different tests formeasuring statistical significance.
In the H-H eval-uation, significance is computed using the Wilcoxonsigned rank test because data are dependent and arenot assumed to be normally distributed.
When build-ing the testset we took care of not including para-phrases of utterances in the training partition (foreach paraphrase generated automatically we keeptrack of the original utterance), however utterancesin both datasets might be generated by the same sub-ject, since a subject completed 12 distinct dialoguesduring the game.
Conversely, in the H-C evaluation,training (H-H data) and test (H-C data) sets werecollected under different conditions with differentsubjects therefore significance was computed usingthe McNemar sign-test (Dietterich, 1998).4 Paraphrases, Synonyms andLemmatisationWe explore three main ways of modifying thecontent features used for classification : lemmatisingthe training and the test data ; augmenting the train-ing data with automatically acquired paraphrases ;and substituting unknown words with synonyms atrun time.Lemmatisation We use the French version ofTreetagger 3 to lemmatise both the training and thetest data.
Lemmas without any filtering were used3.
http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/809to train classifiers.
We then compare performancewith and without lemmatisation.
As we shall see,the lemma and the POS tag provided by TreeTag-ger are also used to lookup synonym dictionaries andEuroWordNet when using synonym handling at runtime.Paraphrases : (DeVault et al 2011) showed thatenriching the training corpus with manually addedparaphrases increases accuracy.
Here we exploit au-tomatically acquired paraphrases and use these notonly to increase the size of the training corpus butalso to better balance it 4.
We proceed as follows.First, we generated paraphrases using a pivot ma-chine translation approach where each user utter-ance in the training corpus (around 3610 utterances)was translated into some target language and backinto French.
Using six different languages (English,Spanish, Italian, German, Chinese and Arabian),we generated around 38000 paraphrases.
We usedGoogle Translate API for translating.Category Train Instances Balanced Instancesgreet 24 86help 20 82yes 92 123no 55 117ack 73 135other 27 89quit 38 100find_plans 115 146job 26 88staff 15 77studies 20 82security_policies 24 86?
44.08 100.92?
?32.68 ?23.32TABLE 1: Skewed and Balanced Data on a sample sub-dialog.
The category with lowest number of paraphrasesis greet, with 62 paraphrases, hence lp = 62.
All cat-egories were increased by 62 except find_plans andyes that were increased by half : 31.Second, we eliminate from these paraphrases,words that are likely to be incorrect lexical transla-tions by removing words with low normalized term4.
The Emospeech data is highly skewed with some classesbeing populated with many utterances and others with few.Algorithm extendingDataWithParaphrases(trainingset ts)1.
Let c be the set of categories in ts.2.
?
be the mean of train instances per category3.
?
be the standard deviation of train instances per category4.
Let Npc be the number of paraphrases per category5.
Let lp ?
min Npcj6.
Repeat7.
set i ?
08.
Ninstci be the number of instances per category ci9.
di ?
Ninstci ?
?10.
if di < ?
then11.
Ninstci ?
lp12.
else13.
Ninstci ?lp214.
end if15.
set i?i+116.
if i>?c?
then17.
terminate18.
endFIGURE 1: Algorithm for augmenting the training datawith paraphrases.frequency (< 0.001) across translations i.e., lexicaltranslations given by few translations and/or transla-tion systems.
We then preprocessed the paraphrasesin the same way the utterances of the initial train-ing corpus were preprocessed i.e., utterances wereunaccented, converted to lower-case and stop wordswere removed, the remaining words were filteredwith TF*IDF.
After preprocessing, duplicates wereremoved.Third, we added the paraphrases to the trainingdata seeking to improve the balance between dialogmoves per dialog, as shown in Figure 1.
To this end,we look for the category c with the lowest numberof paraphrases lp (line 5).
We then compute the de-viation di for each dialog move ci from the mean?
in the original training set (line 9).
If the devia-tion di is lower than the standard deviation then weadd lp number of paraphrases instances (line 11).Conversely, if di is higher than the standard devia-tion, we reduce the number of instances to be addedby half lp2 (line 13).
Table 1 shows the original andthe extended training data for the third sub-dialogin the Emospeech game.
In this dialogue the playeris supposed to ask information about the joystickplans (find_plans, which is the mandatory goal).The categories cover mandatory and optional goalsand general dialogue acts, such as greetings, askingfor help, confirm and disconfirm, acknowledgmentand out of topic questions (i.e.
other).Substituting Synonyms for Unknown Words Aword is unknown, if it is a well-formed French810word 5 and if it does not appear in the training cor-pus.
Conversely, a word is known if it is not un-known.When an unknown word w is detected in a playerutterance at runtime, we search for a word w?
whichoccurs in the training data and is either a synonym ofw or a distributional neighbour.
After disambigua-tion, we substitute the unknown word for the syn-onym.To identify synonyms, we make use of two lexicalresources namely, the French version of EuroWord-Net (EWN) (Vossen, 1998), which includes 92833synonyms, hyperonyms and hyponyms pairs, and asynonym lexicon for French (DIC) 6 which contains38505 lemmas and 254149 synonym pairs.
Whilewords are categorised into Noun, Verbs and Adjec-tives in EWN, DIC contains no POS tag information.To identify distributional neighbours, we con-structed semantic word spaces for each subdialogin the EmoSpeech corpus 7 using random indexing(RI) 8 on the training corpus expanded with para-phrases.
Using the cosine measure as similarity met-rics, we then retrieve for any unknown word w, theword w?
which is most similar to w and which ap-pear in the training corpus.For lexical disambiguation, two methods are com-pared.
We use the POS tag provided by TreeTagger.In this case, disambiguation is syntactic only.
Or wepick the synonym with highest probability based ona trigram language model trained on the H-H cor-pus 9.5 Results and DiscussionTable 2 summarises the results obtained in fourmain configurations : (i) with and without para-phrases ; (ii) with and without synonym handling ;(iii) with and without lemmatisation ; and (iv) when5.
A word is determined to be a well-formed French word ifit occurs in the LEFFF dictionary, a large-scale morphologicaland syntactic lexicon for French (Sagot, 2010)6.
DICOSYN (http ://elsap1.unicaen.fr/dicosyn.html).7.
We also used distributional semantics from the Gigawordcorpus but the results were poor probably because of the verydifferent text genre and domains between the the Gigaword andthe MP game.8.
Topics are Dialog acts while documents are utterances ;we used the S-Space Package http://code.google.com/p/airhead-research/wiki/RandomIndexing9.
We used SRILM (http://www.speech.sri.com/projects/srilm)combining lemmatisation with synonym handling.We also compare the results obtained when evalu-ating using 10-fold cross validation on the trainingdata (H-H dialogs) vs. evaluating the performanceof the system on H-C interactions.Overall Impact The largest performance gain isobtained by a combination of the three techniquesexplored in this paper namely, data expansion, syn-onym handling and lemmatisation (+8.9 points forthe cross-validation experiment and +2.3 for the H-C evaluation).Impact of Lexical Substitution at Run Time Be-cause of space restrictions, we do not report herethe results obtained using lexical resources withoutlemmatisation.
However, we found that lexical re-sources are only useful when combined with lemma-tisation.
This is unsurprising since synonym dictio-naries and EuroWordNet only contain lemmas.
In-deed when distributional neighbours are used, lem-matisation has little impact (e.g., 65.11% using dis-tributional neighbours without lemmatisation on theH-H corpus without paraphrases vs. 66.41% whenusing lemmatisation).Another important issue when searching for aword synonym concerns lexical disambiguation : thesynonym used to replace an unknown word shouldcapture the meaning of that word in its given con-text.
We tried using a language model trained on thetraining corpus to choose between synonym candi-dates (i.e., selecting the synonym yielding the high-est sentence probability when substituting that syn-onym for the unknown word) but did not obtain asignificant improvement.
In contrast, it is noticeablethat synonym handling has a higher impact when us-ing EuroWordNet as a lexical resource.
Since Eu-roWordNet contain categorial information while thesynonym dictionaries we used do not, this suggeststhat the categorial disambiguation provided by Tree-Tagger helps identifying an appropriate synonym inEuroWordNet.Finally, it is clear that the lexical resources usedfor this experiment are limited in coverage and qual-ity.
We observed in particular that some words whichare very frequent in the training data (and thus whichcould be used to replace unknown words) do not oc-cur in the synonym dictionaries.
For instance whenusing paraphrases and dictionaries (fourth row and811H LemmatisationH-H Orig.
Lemmas +EWN +DIC +RIOrig.
65.70%?
5.62 66.04%?
6.49 68.17%?
6.98 67.92%?
4.51 66.83%?
5.92Parap.
70.89%?
6.45 74.31%?
4.78* 74.60%?
5.99* 73.07%?
7.71* 72.63%?
5.82*H-C Orig.
Lemmas +EWN +DIC +RIOrig.
59.71%?
16.42 59.88%?
7.19 61.14%?
16.65 61.41%?
16.59 60.75%?
17.39Parap.
59.82%?
15.53 59.48%?
14.02 61.70%?
14.09* 62.01%?
14.37* 61.16%?
14.41*TABLE 2: Accuracy on the H-H and on the H-C corpus.
The star denotes statistical significance with the Wilcoxon test(p < 0.005) used for the HH corpus and the McNemar test (p < 0.005) for the HC corpus.fourth column in Table 2) 50% of the unknownwords were solved, 17% were illformed and 33% re-mained unsolved.
To compensate this deficiency, wetried combining the three lexical resources in vari-ous ways (taking the union or combining them in apipeline using the first resource that would yield asynonym).
However the results did not improve andeven in some cases worsened due probably to the in-sufficient lexical disambiguation.
Interestingly, theresults show that paraphrases always improves syn-onym handling presumably because it increases thesize of the known vocabulary thereby increasing thepossibility of finding a known synonym.In sum, synonym handling helps most when (i)words are lemmatised and (ii) unknown words canbe at least partially (i.e., using POS tag information)disambiguated.
Moreover since data expansion in-creases the set of known words available as potentialsynonyms for unknown words, combining synonymhandling with data expansion further improves ac-curacy.Impact of Lemmatisation When evaluating usingcross validation on the training corpus, lemmatisa-tion increases accuracy by up to 3.42 points indi-cating that unseen word forms negatively impact ac-curacy.
Noticeably however, lemmatisation has nosignificant impact when evaluating on the H-C cor-pus.
This in turn suggests that the lower accuracyobtained on the H-C corpus results not from unseenword forms but from unseen lemmas.Impact of Paraphrases On the H-H corpus, dataexpansion has no significant impact when usedalone.
However it yields an increase of up to 8.27points and in fact, has a statistically significant im-pact, for all configurations involving lemmatisation.Thus, data expansion is best used in combinationwith lemmatisation and their combination permitscreating better, more balanced and more generaltraining data.
On the H-C corpus however, the im-pact is negative or insignificant suggesting that thedecrease in performance on the H-C corpus is due tocontent words that are new with respect to the train-ing data i.e., content words for which neither a syn-onym nor a lemma can be found in the expandedtraining data.ConclusionWhile classifiers are routinely trained on dialogdata to model the dialog management process, theimpact of such basic factors as lemmatisation, au-tomatic data expansion and synonym handling hasremained largely unexplored.
The empirical eval-uation described here suggests that each of thesefactors can help improve performance but that theimpact will vary depending on their combinationand on the evaluation mode.
Combining all threetechniques yields the best results.
We conjecturethat there are two main reasons for this.
First, syn-onym handling is best used in combination withPOS tagging and lemmatisation because these sup-ports partial lexical semantic disambiguation.
Sec-ond, data expansion permits expanding the set ofknown words thereby increasing the possibility offinding a known synonym to replace an unknownword with.AcknowledgmentsThis work was partially supported by the EUfunded Eurostar EmoSpeech project.
We thankGoogle for giving us access to the University Re-search Program of Google Translate.812ReferencesDavid DeVault, Anton Leuski, and Kenji Sagae.
2011.Toward learning and evaluation of dialogue policieswith text examples.
In 12th SIGdial Workshop on Dis-course and Dialogue, Portland, OR, June.Thomas G. Dietterich.
1998.
Approximate statisticaltests for comparing supervised classification learningalgorithms.
Neural Computation, 10 :1895?1923.Hilda Hardy, Tomek Strzalkowski, Min Wu, CristianUrsu, Nick Webb, Alan W. Biermann, R. Bryce In-ouye, and Ashley McKenzie.
2004.
Data-drivenstrategies for an automated dialogue system.
In ACL,pages 71?78.Ian Richard Lane, Tatsuya Kawahara, and Shinichi Ueno.2004.
Example-based training of dialogue planningincorporating user and situation models.
In INTER-SPEECH.Anton Leuski and David Traum.
2008.
A statistical ap-proach for text processing in virtual humans.
In Pro-ceedings of the 26th Army Science Conference.Andrew Kachites McCallum.
2002.
Mallet : A ma-chine learning for language toolkit.
http ://mal-let.cs.umass.edu.Lina Maria Rojas Barahona and Claire Gardent.
2012.What should I do now ?
Supporting conversations ina serious game.
In SeineDial 2012 - 16th Workshopon the Semantics and Pragmatics of Dialogue, Paris,France.
Jonathan Ginzburg (chair), Anne Abeill?, Mar-got Colinet, Gregoire Winterstein.Lina M. Rojas-Barahona, Alejandra Lorenzo, and ClaireGardent.
2012a.
Building and exploiting a corpusof dialog interactions between french speaking virtualand human agents.
In Proceedings of the 8th Interna-tional Conference on Language Resources and Evalu-ation.Lina M. Rojas Barahona, Alejandra Lorenzo, and ClaireGardent.
2012b.
An end-to-end evaluation of twosituated dialog systems.
In Proceedings of the 13thAnnual Meeting of the Special Interest Group on Dis-course and Dialogue, pages 10?19, Seoul, South Ko-rea, July.
Association for Computational Linguistics.K.
Sagae, G. Christian, D. DeVault, , and D.R.
Traum.2009.
Towards natural language understanding of par-tial speech recognition results in dialogue systems.
InProceedings of Human Language Technologies : The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics(NAACL), Companion Volume : Short Papers, pages53?56.Beno?t Sagot.
2010.
The Lefff, a freely available andlarge-coverage morphological and syntactic lexiconfor French.
In 7th international conference on Lan-guage Resources and Evaluation (LREC 2010), Val-letta, Malta.Piek Vossen, editor.
1998.
EuroWordNet : a multilin-gual database with lexical semantic networks.
KluwerAcademic Publishers, Norwell, MA, USA.813
