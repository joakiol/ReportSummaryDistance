Proceedings of NAACL-HLT 2013, pages 772?776,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsGraph-Based Seed Set Expansion for Relation Extraction UsingRandom Walk Hitting TimesJoel LangUniversity of Geneva7 Route de Drize1227 Carouge, Switzerlandjoel.lang@unige.chJames HendersonXerox Research Centre Europe6 Chemin de Maupertuis38240 Meylan, Francejames.henderson@xrce.xerox.comAbstractIterative bootstrapping methods arewidely employed for relation extraction,especially because they require only asmall amount of human supervision.Unfortunately, a phenomenon knownas semantic drift can affect the accuracyof iterative bootstrapping and lead topoor extractions.
This paper proposesan alternative bootstrapping method,which ranks relation tuples by measuringtheir distance to the seed tuples in abipartite tuple-pattern graph.
In contrastto previous bootstrapping methods, ourmethod is not susceptible to semanticdrift, and it empirically results in betterextractions than iterative methods.1 IntroductionThe goal of relation extraction is to extract tu-ples of a particular relation from a corpus ofnatural language text.
A widely employed ap-proach to relation extraction is based on iter-ative bootstrapping (Brin, 1998; Agichtein andGravano, 2000; Pasca et al 2006; Pantel andPennacchiotti, 2006), which can be applied withonly small amounts of supervision and whichscales well to very large datasets.A well-known problem with iterative boot-strapping is a phenomenon known as seman-tic drift (Curran et al 2007): as bootstrap-ping proceeds it is likely that unreliable pat-terns will lead to false extractions.
These extrac-tion errors are amplified in the following itera-tions and the extracted relation will drift awayfrom the intended target.
Semantic drift oftenresults in low precision extractions and there-fore poses a major limitation of iterative boot-strapping algorithms.
Previous work on itera-tive bootstrapping has addressed the issue of re-ducing semantic drift for example by baggingthe results of various runs employing differingseed tuples, constructing filters which identifyfalse tuples or patterns and adding further con-straints to the bootstrapping process (T. McIn-tosh, 2010; McIntosh and Curran, 2009; Curranet al 2007).However, the analysis of Komachi et al(2008) has shown that semantic drift is an in-herent property of iterative bootstrapping algo-rithms and therefore poses a fundamental prob-lem.
They have shown that iterative bootstrap-ping without pruning corresponds to an eigen-vector computation and thus as the number ofiterations increases the resulting ranking will al-ways converge towards the same static rankingof tuples, regardless of the particular choice ofseed instances.In this paper, we describe an alternativemethod, that is not susceptible to semantic drift.We represent our data as a bipartite graph,whose vertices correspond to patterns and tu-ples respectively and whose edges capture cooc-currences and then measure the distance of atuple to the seed set in terms of random walkhitting times.
Experimental results confirm thatsemantic drift is avoided by our method andshow that substantial improvements over iter-ative forms of bootstrapping are possible.7722 Scoring with Hitting TimesFrom a given corpus, we extract a dataset con-sisting of tuples and patterns.
Tuples are pairsof co-occurring strings in the corpus, such as(Bill Gates, Microsoft), which potentially belongto a particular relation of interest.
In our case,patterns are simply the sequence of tokens oc-curring between tuple elements, e.g.
?is thefounder of?.
We represent all the tuple types1X and all the extraction pattern types Y con-tained in a given corpus through an undirected,weighted, bipartite graph G = (V,E) with ver-tices V = X ?
Y and edges E ?
X ?
Y , wherean edge (x, y) ?
E indicates that tuple x oc-currs with pattern y somewhere in the corpus.Edge weights are defined through a weight ma-trix W which holds the weight Wi,j = w(vi, vj)for edges (vi, vj) ?
E. Specifically, we use thecount of how many times a tuple occurs witha pattern in the corpus and weights for uncon-nected vertices are zero.Our goal is to compute a score vector ?
hold-ing a score ?i = ?
(xi) for each tuple xi ?
X,which quantifies how well the tuple matches theseed tuples.
Higher scores indicate that the tu-ple is more likely to belong to the relation de-fined through the seeds and thus the score vec-tor effectively provides a ranking of the tuples.We define scores of tuples based on their dis-tance2 to the seed tuples in the graph.
The dis-tance of some tuple x to the seed set S canbe naturally formalized in terms of the aver-age time it takes until a random walk startingin S reaches x, the hitting time.
The randomwalk is defined through the probability distri-bution over start vertices and through a ma-trix of transition probabilities.
Edge weightsare constrained to be non-negative, which al-lows us to define the transition matrix P withPi,j = p(vj |vi) = 1dviw(vi, vj), where dv =?vk?Vw(v, vk) is the degree of a vertex v ?
V .The distance of two vertices is measured interms of the average time of a random walk be-1Note that we are using tuple and pattern types ratherthan particular mentions in the corpus.2The term is used informally.
In particular, hitting timesare not a distance metric, since they can be asymmetric.tween the two.
Specifically, we adopt the notionof T-truncated hitting time (Sarkar and Moore,2007) defined as the expected number of stepsit takes until a random walk of at most T stepsstarting at vi reaches vj for the first time:hT (vj |vi) ={0 iff.
vj = vi or T=01 +?vk?Vp(vk|vi)hT?1(vj |vk)The truncated hitting time hT (vj |vi) can beapproximately computed by sampling M inde-pendent random walks starting at vi of length Tand computingh?T (vj |vi) =1Mm?k=1tk + (1?mM)T (1)where {t1 .
.
.
tm} are the sampled first-hit timesof random walks which reach vj within T steps(Sarkar et al 2008).The score ?HT (v) of a vertex v /?
S to the seedset S is then defined as the inverse of the aver-age T -truncated hitting time of random walksstarting at a randomly chosen vertex s ?
S:1?HT (v)= hT (v|S) =1|S|?s?ShT (v|s) (2)3 ExperimentsWe extracted tuples and patterns from the fifthedition of the Gigaword corpus (Parker et al2011), by running a named entity tagger andextracting all pairs of named entities and ex-tracting occurring within the same sentencewhich do not have another named entity stand-ing between them.
Gold standard seed and testtuples for a set of relations were obtained fromYAGO (Suchanek et al 2007).
Specifically, wetook all relations for which there are at least300 tuples, each of which occurs at least oncein the corpus.
This resulted in the set of rela-tions shown in Table 1, plus the developmentrelation hasWonPrize.For evaluation, we use the percentile rank ofthe median test set element (PRM, see Francoiset al2007), which reflects the quality of the773full produced ranking, not just the top N ele-ments and is furthermore computable with onlya small set of labeled test tuples 3.We compare our proposed method based onhitting times (HT) with two variants of iterativebootstrapping.
The first one (IB1) does not em-ploy pruning and corresponds to the algorithmdescribed in Komachi et al(2008).
The sec-ond one (IB2) corresponds to a standard boot-strapping algorithm which employs pruning af-ter each step in order to reduce semantic drift.Specifically, scores are pruned after projectingfrom X onto Y and from Y onto X, retainingonly the top N (t) = N0t scores at iteration t andsetting all other scores to zero.3.1 ParametrizationsThe experiments in this section were conductedon the held out development relation hasWon-Prize.
The ranking produced by both forms ofiterative bootstrapping IB1 and IB2 depend onthe number of iterations, as shown in Figure 1.IB1 achieves an optimal ranking after just oneiteration and thereafter scores get worse due tosemantic drift.
In contrast, pruning helps avoidsemantic drift for IB2, which attains an optimalscore after 2 iterations and achieves relativelyconstant scores for several iterations.
However,during iteration 9 an incorrect pattern is keptand this at once leads to a drastic loss in ac-curacy, showing that semantic drift is only de-ferred and not completely eliminated.Our method HT has parameter T , correspond-ing to the truncation time, i.e., maximal numberof steps of a random walk.
Figure 2 shows thePRM of our method for different values of T .Performance gets better as T increases and isoptimal for T = 12, whereas for larger values,the performance gets slightly worse again.
Thefigure shows that, if T is large enough (> 5), thePRM is relatively constant and there is no phe-nomenon comparable to semantic drift, whichcauses instability in the produced rankings.3other common metrics do not satisfy these conditions.Figure 1: PRM for iterative bootstrapping with-out pruning (IB1) and with pruning (IB2).
Alower PRM is better.Figure 2: PRM for our method based on hittingtimes, for different values of the truncation timeparameter T.3.2 Method ComparisonTo evaluate the methods, firstly the parametersfor each method were set to the optimal valuesas determined in the previous section.
For theexperiments here, we again use 200 randomlychosen tuples as the seeds for each relation.
Allthe remaining gold standard tuples are used fortesting.Table 1 shows the PRM for the three methods.For a majority of the relations (12/16) HT at-tains the best, i.e.
lowest, PRM, which confirmsthat hitting times constitute an accurate way ofmeasuring the distance of tuples to the seed set.IB1 and IB2 each perform best on 2/16 of therelations.
A sign test on these results yields that774Relation IB1 IB2 HTcreated 1.82 1.71 0.803dealsWith 0.0262 0.107 0.0481diedIn 30.5 18.4 20.4directed 0.171 0.238 0.166hasChild 7.66 32.2 4.26influences 5.93 5.48 6.60isAffiliatedTo 1.54 2.01 1.30isCitizenOf 1.74 1.87 1.68isLeaderOf 1.37 1.91 0.401isMarriedTo 4.69 4.14 1.27isPoliticianOf 0.0117 0.110 0.0409livesIn 3.17 2.48 1.70owns 11.0 2.10 2.07produced 1.55 0.967 0.240wasBornIn 11.3 9.37 8.42worksAt 1.52 2.21 0.193Table 1: PRM in percent for all relations, for allthree models.
A lower PRM corresponds to abetter model, with the best score indicated inbold.Figure 3: PRM for the three methods, as a func-tion of the size of the seed set for the relationcreated.HT is better than both IB1 and IB2 at signifi-cance level ?
< 0.01.Moreover, the ranking produced by HT is sta-ble and not affected by semantic drift, given thateven where results are worse than for IB1 orIB2, they are still close to the best performingmethod.
In contrast, when semantic drift oc-curs, the performance of IB1 and IB2 can dete-riorate drastically, e.g.
for the worksAt relation,where both IB1 and IB2 produce rankings thatare a lot worse than the one produced by HT.3.3 Sensitivity to Seed Set SizeFigure 3 shows the PRM for each of the threemethods as a function of the size of the seed setfor the relation created.
For small seed sets, theperformance of the iterative methods can be in-creased by adding more seeds.
However, froma seed set size of 50 onwards, performance re-mains relatively constant.
In other words, iter-ative bootstrapping is not benefitting from theinformation provided by the additional labeleddata, and thus has a poor learning performance.In contrast, for our method based on hittingtimes, the performance continually improves asthe seed set size is increased.
Thus, also in termsof learning performance, our method is moresound than iterative bootstrapping.4 ConclusionsThe paper has presented a graph-based methodfor seed set expansion which is not susceptibleto semantic drift and on most relations outper-forms iterative bootstrapping.
The method mea-sures distance between vertices through randomwalk hitting times.
One property which makeshitting times an appropriate distance measureis their ability to reflect the overall connectivitystructure of the graph, in contrast to measuressuch as the shortest path between two vertices.The hitting time will decrease when the num-ber of paths from the start vertex to the tar-get vertex increases, when the length of pathsdecreases or when the likelihood (weights) ofpaths increases.
These properties are particu-larly important when the observed graph edgesmust be assumed to be merely a sample of allplausible edges, possibly perturbated by noise.This has also been asserted by previous work,which has shown that hitting times successfullycapture the notion of similarity for other naturallanguage processing problems such as learningparaphrases (Kok and Brockett, 2010) and re-lated problems such as query suggestion (Meiet al 2008).
Future work will be aimed to-wards employing our hitting time based methodin combination with a richer feature set.775ReferencesAgichtein, E. and Gravano, L. (2000).
Snow-ball: Extracting Relations from Large Plain-text Collections.
In Proceedings of the FifthACM Conference on Digital Libraries.Brin, S. (1998).
Extracting Patterns and Rela-tions from the World-Wide Web.
In Proceed-ings of the 1998 International Workshop on theWeb and Databases.Curran, J., Murphy, T., and Scholz, B.
(2007).Minimising Semantic Drift with Mutual Exclu-sion Bootstrapping.
In Proceedings of the 10thConference of the Pacific Association for Com-putational Linguistics.Francois, F., Pirotte, A., Renders, J., andSaerens, M. (2007).
Random-Walk Computa-tion of Similarities between Nodes of a Graphwith Application to Collaborative Recommen-dation.
IEEE Transactions on Knowledge andData Engineering, 19(3):355 ?369.Kok, S. and Brockett, C. (2010).
Hitting theRight Paraphrases in Good Time.
In Proceed-ings of the Annual Conference of the NorthAmerican Chapter of the Association for Com-putational Linguistics.Komachi, M., Kudo, T., Shimbo, M., and Mat-sumoto, Y.
(2008).
Graph-based Analysisof Semantic Drift in Espresso-like Bootstrap-ping Algorithms.
In Proceedings of the Con-ference on Empirical Methods in Natural Lan-guage Processing.McIntosh, T. and Curran, J.
(2009).
Reduc-ing Semantic Drift with Bagging and Distri-butional Similarity.
In Proceedings of the JointConference of the 47th Annual Meeting of theACL.Mei, Q., Zhou, D., and Church, K. (2008).
QuerySuggestion Using Hitting Time.
In Proceed-ings of the 17th ACM Conference on Informa-tion and Knowledge Management.Pantel, P. and Pennacchiotti, M. (2006).Espresso: Leveraging Generic Patterns for Au-tomatically Harvesting Semantic Relations.
InProceedings of the 21st International Confer-ence on Computational Linguistics and the 44thAnnual Meeting of the Association for Compu-tational Linguistics.Parker, R., Graff, D., Kong, J., Chen, K., andMaeda, K. (2011).
English Gigaword FifthEdition.
Technical report, Linguistic DataConsortium.Pasca, M., Lin, D., Bigham, J., Lifchits, A., andJain, A.
(2006).
Organizing and Searchingthe World Wide Web of Facts ?
Step One: theOne-million Fact Extraction Challenge.
In Pro-ceedings of the 21st National Conference on Ar-tificial Intelligence (AAAI).Sarkar, P. and Moore, A.
(2007).
A TractableApproach to Finding Closest Truncated-commute-time Neighbors in Large Graphs.
InProceedings of the 23rd Conference on Uncer-tainty in Artificial Intelligence.Sarkar, P., Moore, A., and Prakash, A.
(2008).Fast Incremental Proximity Search in LargeGraphs.
In Proceedings of the 25th Interna-tional Conference on Machine Learning.Suchanek, F., Kasneci, G., and Weikum, G.(2007).
Yago: A Core of Semantic Knowl-edge.
In Proceedings of the International WorldWide Web Conference (WWW).T.
McIntosh (2010).
Unsupervised Discoveryof Negative Categories in Lexicon Bootstrap-ping.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Pro-cessing.776
