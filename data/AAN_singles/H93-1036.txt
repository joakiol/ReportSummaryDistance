BUILDING A LARGE ONTOLOGYFOR MACHINE TRANSLAT IONKevin KnightUSC/Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292ABSTRACTThis paper describes efforts underway to construct a large-scale ontology to support semantic processing in the PAN-GLOSS knowledge-base machine translation system.
Be-cause we axe aiming at broad sem~tntic coverage, we are focus-ing on automatic and semi-automatic methods of knowledgeacquisition.
Here we report on algorithms for merging com-plementary online resources, in particular the LDOCE andWordNet dictionaries.
We discuss empirical results, and howthese results have been incorporated into the PANGLOSSontology.1.
IntroductionThe PANGLOSS project is a three-site collaborativeeffort to build a large-scale knowledge-based machinetranslation system.
Key components of PANGLOSSinclude New Mexico State University's ULTRA parser\[Farwell and Wilks, 1991\], Carnegie Mellon's interlinguarepresentation format \[Nirenburg and Defrise, 1991\], andUSC/ISI's PENMAN English generation system \[Pen-man, 1989\].
Another key component currently underconstruction at ISI is the PANGLOSS ontology, a large-scale conceptual network intended to support seman-tic processing in other PANGLOSS modules.
This net-work will contain 50,000 nodes representing commonlyencountered objects, entities, qualities, and relations.The upper (more abstract) region of the ontology iscalled the Ontology Base (OB) and contains approxi-mately 400 items that represent generalizations essentialfor the various PANGLOSS modules' linguistic process-ing during translation.
The middle region of the ontol-ogy, approximately 50,000 items, provides a frameworkfor a generic world model, containing items representingmany English word senses.
The lower (more specific)regions O f the ontology provide anchor points for differ-ent application domains.
Both the middle and domainmodel regions of the ontology house the open-class termsof the MT interlingua.
They also contain specific infor-mation used to screen unlikely semantic and anaphoricinterpretations.The Ontology Base is a synthesis of USC/ISI's PEN-MAN Upper Model \[Bateman, 1990\] and CMU's ON-TOS concept hierarchy \[Carlson and Nirenburg, 1990\].Both of these high-level ontologies were built by hand,and they were merged manually.
Theoretical motiva-tions behind the OB and its current status are describedin \[Hovy and Knight, 1993\].The problem we focus on in this paper is the construc-tion of the large middle region of the ontology.
Becauselarge-scale knowledge resources are difficult to build byhand, we are pursuing primarily automatic methods ap-plied in several stages.
During the first stage we createdseveral tens of thousands of nodes, organized them intosub/superclass taxonomies, and subordinated those tax-onomies to the 400-node Ontology Base.
This work wedescribe below.
Later stages will address the insertion ofadditional semantic information such as restrictions onactors in events, domain/range constraints on relations,and so forth.For the major node creation and taxonomization stage,we have primarily used two on line sources of infor-mation: (1) the Longman Dictionary of Contempo-rary English (LDOCE)\[Group, 1978\], and (2) the lexicaldatabase WordNet \[Miller, 1990\].2.
Merging LDOCE and WordNetLDOCE is a learner's dictionary of English with 27,758words and 74,113 word senses.
Each word sense comeswith:A short definition.
One of the unique features ofLDOCE is that its definitions only use words from a"control vocabulary" list of 2000 words.
This makesit attractive from the point of view of extractingsemantic information by parsing dictionary entries.Examples of usage.One or more of 81 syntactic odes.For nouns, one of 33 semantic odes.For nouns, one of 124 pragmatic odes.185WordNet is a semantic word database based on psy-cholinguistic principles.
Its size is comparable toLDOCE, but its information is organized in a completelydifferent manner.
WordNet groups synonymous wordsenses into single units ("synsets").
Noun senses areorganized into a deep hierarchy, and the database ~socontains part-of links, antonym links, and others.
Ap-proximately 55% of WordNet synsets have brief informaldefinitions.Each of these resources has something to offer a large-scale natural language system, but each is missing im-portant features present in the other.
What we need isa combination of the features of both.Our most significant project to date has been to mergeLDOCE and WordNet.
This involves producing a list ofmatching pairs of word senses, e.g.
:LDOCE WORDNET(abdomen_O_O ABDOMEN-l)(crane_l_2 CRANE-l)(crane_l_1 CRANE-2)(abbess_O_O ABBESS-I)(abbott_O_O ABBOTT-I).
.
.
?
, ?Section 4 describes how we produced this list semi-automatically.
Solving this problem yields several bene-fits:?
It allows us to taxonomize tens of thousands ofLDOCE word senses and subordinate them quicklyto the Ontology Base.
Section 5 describes how wedid this.?
It provides a syntactic and pragmatic lexicon forWordNet, as well as careful definitions.?
It groups LDOCE senses into synonyms ets andtaxonomies.
* It allows us to identify and correct errors in theoriginal resources.3.
Re la ted  WorkOur ontology is a symbolic model for fueling semanticprocessing in a knowledge-based MT system.
We areaiming at broader coverage (dictionary-scale) than haspreviously been available to symbolic MT systems.
Also,we are committed to automatic and semi-automaticmethods of knowledge acquisition from the start.
This,and the fact that we are concentrating on a partic-ular language-processing application, distinguishes thePANGLOSS work from the CYC knowledge base \[Lenatand Guha, 1990\].
We also believe that dictionaries andcorpora are imperfect sources of knowledge, so we stillemploy human effort to check the results of our semi-automatic algorithms.
This is in contrast o purely sta-tistical systems (e.g., \[Brown et al, 1992\]), which aredifficult to inspect and modify.There has been considerable use in the NLP communityof both WordNet (e.g., \[Lehman et al, 1992; Resnik,1992\]) and LDOCE (e.g..., \[Liddy et aL, 1992; Wilks etal., 1990\]), but no one has merged the two in order tocombine their strengths.
The next section describes ourapproach in detail.4.
Algorithms and ResultsWe have developed two algorithms for merging LDOCEand WordNet.
Both algorithms generate lists of sensepairs, where each pair consists of one sense from LDOCEand the proposed matching sense from WordNet, if any.4.1.
Definition MatchThe Definition Match algorithm is based on the ideathat two word senses should he matched if their twodefinitions hare words.
For example, there are two noundefinitions of "batter" in LDOCE:(batter_2_0) "mixture of flour, eggs, and milk,beaten together and used in cooking"(batter_3_0) "a person who bats, esp in baseball - -compare BATSMAN"and two definitions in WordNet:?
(BATTER-l)  "ballplayer who bats"?
(BATTER-2) "a flour mixture thin enough to pouror drop from a spoon"The Definition Match Algorithm will match (batter_2_0)with (BATTER-2) because their definitions hare wordslike "flour" and "mixture."
Similarly (batter_3_0) and(BATTER-I)  both contain the word "bats," so they arealso matched together.Not all senses in WordNet have definitions, but mosthave synonyms and superordinates.
For this reason, thealgorithm looks not only at WordNet definitions, butalso at locally related words and senses.
For example, if186synonyms of WordNet sense x appear in the definitionof LDOCE sense y, then this is evidence that x and yshould be matched.Here is the algorithm:Def in i t ion -MatchFor each English word w found in both LDOCE andWordNet:1.
Let n be the number of senses of w in LDOCE.2.
Let m be the number of senses of w in WordNet.. Identify and stem all open-class, content words inthe definitions (and example sentences) of all sensesof w in both resources...Let ULD be the union of all stemmed content wordsappearing in LDOCE definitions.Let UWN be the same for WordNet, plus all syn-onyms of the senses, their direct superordinates, sib-lings, super-superordinates, as well as stemmed con-tent words from the definitions of direct superordi-nates.6.
Let CW=(ULD N UWN) - w. These are definitionwords common to LDOCE and WordNet..
Create matrix L of the n LDOCE senses and thewords f romCW.
Fora l l0<i<nand0<z< \[CW l:L\[i,z\]= { 0.011"00if the definition of sense iin LDOCE contains word xotherwise8.
Create matrix W of the m WordNet senses and thewords f romCW.
For a l l0<j< mand0<x < \]CW l:1.000.80w\[x , j \ ]  =0.600.01if x is a synonym orsuperordinate of sense jin WordNetif x is contained in thedefinition of sense j orthe definition of itssuperordinateif x is a sibling orsuper-superordinate of sensej in WordNetotherwise9.
Create similarity matrix SIM of LDOCE and Word-Net senses.
For all 0 _< i < n and 0 < j < m:FlCWl-  \]SIMti, j I = .\[ ~ (L \ [ i , x \ ] -W\ [x , j l )  / I CWl10.
Repeat until SIM is a zero matrix:(a) Let SIM\[y, z\] be the largest value in the SIMmatrix.
(b) Generate matched pair of LDOCE sense y andWordNet sense z.
(c) For all 0 _< i < n, set SIM\[i, z\] = 0.0.
(d) For all 0 < j < m, set sIm\[y, j\] = 0.0.In constructing the SIM matrix the algorithm comes upwith a similarity measure between each of the n.m possi-ble pairs of LDOCE and WordNet senses.
This measure,SIM\[i, j\], is a number from 0 to 1, with 1 being as good amatch as possible.
Thus, every matching pair proposedby the algorithm comes with a confidence factor.Empirical results are as follows.
We ran the algorithmover all nouns in both LDOCE and WordNet.
We judgedthe correctness of its proposed matches, keeping recordsof the confidence levels and the degree of ambiguitypresent.For low-ambiguity words (words with exactly two sensesin LDOCE and two in WordNet), the results are:confidence pct.
pct.level correct coverage> 0.0 75% 100%>_ 0.4 85% 53%_> 0.8 90% 27%At confidence levels > 0.0, 75% of the proposed matchesare correct.
If we restrict ourselves to only matches pro-posed at confidence ~ 0.8, accuracy increases to 90%,but we only get 27% of the possible matches.For high-ambiguity words (more than five senses inLDOCE and WordNet), the results are:confidence pet.
pct.level correct coverage> 0.0 47% 100%>_ O.
1 76% 44%>_ 0.2 81% 20%187Accuracy here is worse, but increases harply when weonly consider high confidence matches.The algorithm's performance is quite reasonable, giventhat 45% of WordNet senses have no definitions andthat many existing definitions are brief and containmisspellings.
Still, there are several improvements obe made e.g., modify the "greedy" strategy in whichmatches are extracted from SIM matrix, weigh rarewords in definitions more highly than common ones,and/or score senses with long definitions lower than oneswith short definitions.
These improvements yield onlyslightly better results, however, because most failuresare simply due to the fact that matching sense defini-tions have no words in common.
For example, "seal"has 5 noun senses in LDOCE, one of which is:(seal_l_l) "any of several types of large fish-eating animals living mostly on cool seacoastsand floating ice, with broad flat limbs (FLIP-PERs) suitable for swimming"WordNet has 7 definitions of "seal," one of which is:For example, (bat_l_l) is defined as "any of the severaltypes of specially shaped wooden stick used for .
.
. "
Thegenus term for (bat_l_l) is (stick_l_l).
As another exam-ple, the genus sense of (aisle_0_l) is (passage_0_7).
Thegenus sense and the semantic ode hierarchies were ex-tracted automatically from LDOCE.
The semantic odehierarchy is fairly robust, but since the genus sense hier-archy was generated heuristically, it is only 80% correct.The idea of the Hierarchy Match algorithm is that oncetwo senses are matched, it is a good idea to look attheir respective ancestors and descendants for furthermatches.
For example, once (animal_l_2) and ANIMAL-1 are matched, we can look into the respective animal-subhierarchies.
We find that the word "seal" is locallyunambiguous---only one sense of "seal" refers to an an-imal (in both LDOCE and WordNet).
So we feel con-fident to match those seal-animal senses.
As anotherexample, suppose we know that (swan_dive-0_0) is thesame concept as (SWAN-DIVE-l).
We can then matchtheir superordinates (dive_2_l) and (DIVE-3) with highconfidence; we need not consider other senses of "dive.
"Here is the algorithm:(SEAL-7) "any of numerous marine mammalsthat come on shore to breed; chiefly of coldregions"The Definition Match algorithm cannot see any simi-larity between (seal_l_1) and (SEAL-7), so it does notmatch them.
However, we have developed another matchalgorithm that can handle cases like these.4.2.
Hierarchy MatchThe Hierarchy Match algorithm dispenses with sense def-initions altogether.
Instead, it uses the various sensehierarchies inside LDOCE and WordNet.WordNet noun senses are arranged in a deep is-a hierar-chy.
For example, SEAL-7 is a PINNIPED-1, which ison AQUATIC-MAMMAL-l, which is a EUTHERIAN-1, which is a MAMMAL-l, which is ultimately anANIMAL-I, and so forth.LDOCE has two fairly flat hierarchies.
The semanticcode hierarchy is induced by a set of 33 semantic odesdrawn up by Longman lexicographers.
Each sense ismarked with one of these codes, e.g., "H" for human"P" for plant, "J" for movable object.
The other hier-archy is the genus sense hierarchy.
Researchers at NewMexico State University have built an automatic algo-rithm \[Bruce and Guthrie, 1992\] for locating and disam-biguating enus terms (head nouns) in sense definitions.H ie rarchy-Match1.
Initialize the set of matches:(a) Retrieve all words that are unambiguous inboth LDOCE and WordNet.
Match their cor-responding senses, and place all the matcheson a list called M1.
(b) Retrieve a prepared list of hand-craftedmatches.
Place these matches on a list calledM2.
We created 15 of these, mostly high-level matches like (person_0_l, PERSON-2)and (plant_2_l, PLANT-3).
This step is notstrictly necessary, but provides guidance to thealgorithm.2.
Repeat until M1 and M2 are empty:(a) For each match on M2, look for words thatare unambiguous within the hierarchies rootedat the two matched senses.
Match the sensesof locally unambiguous words and place thematches on M1.
(b) Move all matches from M2 to a list called M3.
(c) For each match on M1, look upward in the twohierarchies from the matched senses.
When-ever a word appears in both hierarchies, matchthe corresponding senses, and place the matchon M2.188(d) Move all matches from M1 to M2.The algorithm operate in phases, shifting matches fromM1 to M2 to M3, placing newly-generated matches onM1 and M2.
Once M1 and M2 are exhausted, M3 con-tains the final list of matches proposed by the algorithm.Again, we can measure the success of the algorithm alongtwo dimensions, coverage and correctness:pct.
matchesphase correct proposedStep 1 99% 756394% 876 Step 2(a)Step 2@Step 2(a)Step 2@Step 2(a)Step 2(c)85% 53093% 201883% 4092% 99100% 2In the end, the algorithm produced 11,128 matches at96% accuracy.
We expected 100% accuracy, but the algo-rithm was foiled at several places by errors in one or an-other of the hierarchies.
For example, (savings_bank_0_0)is mistakenly a subclass of river-bank (bank_l_1) in theLDOCE genus hierarchy, rather than (bank_l_4), themoney-bank.
"Savings bank" senses are matched in stepl(a), so step 2(c) erroneously goes on to match the river-bank of LDOCE with the money-bank of WordNet.Fortunately, the Definition and Hierarchy Match algo-rithms complement one another, and there are severalways to combine them.
Our practical experience hasbeen to run the Hierarchy Match algorithm to comple-tion, remove the matched senses from the databases,then run the Definition Match algorithm.
The DefinitionMatch algorithm's performance improves lightly afterhierarchy matching removes ome word senses.
Once thehigh confidence definition matches have.been verified, weuse them as fuel for another un of the Hierarchy Matchalgorithm.We have built an interface that allows a person to verifymatches produced by both algorithms, and to reject orcorrect faulty matches.
So far, we have 15,000 correctmatches, with 10,000 to follow shortly.
The next sectiondescribes what we do with them in our ontology.5.
The Current OntologyThe ontology currently contains 15,000 noun senses fromLDOCE and 20,000 more from WordNet.
Its purpose isto support semantic processing in the PANGLOSS anal-ysis and generation modules.
Because we have not yettaxonomized adjective and verb senses (see Section 6)semantic support is still very limited.On the generation side, the PENMAN system requiresthat all concepts be subordinated to the PENMAN Up-per Model, which is part of the Ontology Base (OB).
Itis difficult to subordinate tens of thousands of LDOCEword senses to the OB individually, but if we insteadsubordinate various WordNet hierarchies to the OB,the LDOCE senses will follow automatically via theWordNet-LDOCE merge.Subordinating the WordNet noun hierarchy to the OBrequired about 100 manual operations.
Each operationeither merged a WordNet concept with an OB equiva-lent, inserted one or more WordNet concepts betweentwo OB concepts, or attached a WordNet concept belowan OB concept.
The noun senses from WordNet (andtheir matches from LDOCE) fall under all three of theOB's primary top-level categories of OBJECT, PROCESS,and QUALITY.
The PENMAN generator now has accessto the semantic knowledge it needs to generate a broadrange of English.To support parsing, we have manually added about 20mutual-disjoint assertions into the ontology.
One ofthese assertions tates that no individual can be bothan INANIMATE-OBJECT and an ANIMATE-OBJECT, anotherstates that PERSON and 1011-HtrtlAN-ANItlAL are mutuallydisjoint, and so forth.
A parser can use such informationto disambiguate sentences like "this crane is my pet,"where "crane" and "pet" have several senses in LDOCE(crane_l_l, a machine; crane_l_2, a bird; pet_l_1, a do-mestic animal; pet_l_2, a favorite person; etc.).
The onlypair of senses that are not mutually disjoint in our on-tology is (crane_l.2)/(pet_l_l), so this is the preferredinterpretation.
So far, all mutual-disjoint links are be-tween OB concepts.
We plan a study of our lexicon todetermine which nouns have senses that are not distin-guishable on the basis of mutual-disjointness, and thiswill drive further knowledge acquisition of these asser-tions.We are now integrating the ontology with ULTRA,the Prolog-based parsing component of the PANGLOSStranslator.
Although ULTRA parses Spanish input forPANGLOSS, the lexical items have already been seman-tically tagged with LDOCE sense keys, so no large-scaleknowledge acquisition is necessary.
Our first step hasbeen to produce a Prolog version of the ontology, with in-ference rules for inheritance and propagation of mutual-disjoint links.Another use of the ontology has been to help us refineLDOCE and WordNet themselves.
For example, any189sample of the automatically-generated LDOCE genus-sense hierarchy has approximately 20% errors.
Using ourmerged LDOCE-WordNet-OB ontology as a standard,we have been able to locate and fix a large number ofthese errors automatically.6.
Future WorkThere are several items on our immediate agenda:?
Ontologize adjective, verb, and adverb sensesfrom LDOCE.
Most adjective senses either per-tain to objects (e.g., atomic_l_l) or representslot-value pairs in the ontology (e.g.,green_l_lrefers to COLOR/GREEI-C0LOR as pertaining toPItYSICAL-OBJECTs).
Most verb senses refer toPROCESSes, whose participants have class restric-tions, and so forth.
Much of this information can bemined from WordNet and LDOCE, as well as fromonline corpora.?
Extract a large Spanish lexicon for the ontology.
Weplan to use a bilingual Spanish-English dictionary(and merging techniques similar in spirit to the ones,described in this paper) in order to roughly annotatethe ontology with Spanish words and phrases.?
Incrementally flesh out the ontology to improve thequality of PANGLOSS translations.
We will focuson acquiring relations like SIZE, PURPOSE, PART-0F,POSTC011DITI01I, etc., through primarily automaticmethods, including parsing of LDOCE definitionsand processing corpora.7.
AcknowledgmentsI would like to thank Richard Whitney for significant as-sistance in programming and verification.
The OntologyBase was built by Eduard Hovy, Licheng Zeng, AkitoshiOkumura, Richard Whitney, and the author.
I wish toexpress gratitude to Longman Group, Ltd., for makingthe machine readable version of LDOCE, 2nd edition,available to us.
Louise Guthrie assisted in LDOCE ex-traction and kindly provided us with the LDOCE genussense hierarchy.
This work was carried out under ARPAOrder No.
8073, contract MDA904-91-C-5224.ReferencesBateman, J.
1990.
Upper modeling: Organizing knowl-edge for natural language processing.
In Proc.
FifthInternational Workshop on Natural Language Gener-ation, Pittsburgh, PA.Brown, P., V. Della Pietra, P. deSouza, J. Lai, andR.
Mercer.
1992.
Class-based n-gram models of natu-ral language.
Computational Linguistics 18(4).Bruce, Rebecca nd Louise Guthrie.
1992.
Genus dis-ambiguation: A study in weighted preference.
In Pro-ceedings of the 15th International Conference on Com-putational Linguistics (COLING-92).Carlson, L. and S. Nirenburg.
1990.
World Modelingfor NLP.
Tech.
Rep. CMU-CMT-90-121, Center forMachine Translation, Carnegie Mellon University.Farwell, D. and Y. Wilks.
1991.
Ultra: A multilin-gual machine translator.
In Proceedings of the 3rdMT Summit.Longman Group.
1978.
Longman Dictionary of Con-temporary English.
Essex, UK: Longman.Hovy, E. and K. Knight.
1993.
Motivating shared knowl-edge resources: An example from the pangloss collab-oration.
(Submitted to: Theoretical and Method-ological Issues in Machine Translation).Lehman, J., A. Newell, T. Polk, and R. Lewis.
1992.
Therule of language in cognition.
In Conceptions of theHuman Mind, ed.
G. Harman.
Hillsdale, N J: LawrenceErlbaum.
(Forthcoming).Lenat, D. and R.V.
Guha.
1990.
Building LargeKnowledge-Based Systems.
Reading, MA: Addison-Wesley.Liddy, E., W. Paik, and J. Woelfel.
1992.
Use of sub-ject field codes from a machine-readable dictionary forautomatic lassification of documents.
In Advancesin Classification Research: Proc.
3rd ASIS SIG/CRClassification Research Workshop.Miller, George.
1990.
Wordnet: An on-line lexicaldatabase.
International Journal of Lexicography 3(4).
(Special Issue).Nirenburg, S. and C. Defrise.
1991.
Aspects of textmeaning.
In Semantics and the Lexicon, ed.
J. Puste-jovsky.
Dordrecht, Holland: Kluwer.Penman.
1989.
The Penman Documentation.
Teeh.
rep.,USC/Information Sciences Institute.Resnik, P. 1992.
Wordnet and distributional analy-sis: A class-based approach to lexical discovery.
InProc.
AAAI Workshop on Statistically-Based NLPtechniques.Wilks, Y., D. Fuss, C. Guo, J. McDonald, T. Plate, andB.
Slator.
1990.
Providing machine tractable dictio-nary tools.
Machine Translation 5.190
