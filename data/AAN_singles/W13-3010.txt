Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 93?101,Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational LinguisticsInvestigating Connectivity and Consistency Criteria for Phrase PairExtraction in Statistical Machine TranslationSpyros Martzoukos, Christophe Costa Flore?ncio and Christof MonzIntelligent Systems Lab Amsterdam, University of AmsterdamScience Park 904, 1098 XH Amsterdam, The Netherlands{S.Martzoukos, C.Monz}@uva.nl, chriscostafl@gmail.comAbstractThe consistency method has been estab-lished as the standard strategy for extract-ing high quality translation rules in statis-tical machine translation (SMT).
However,no attention has been drawn to why thismethod is successful, other than empiri-cal evidence.
Using concepts from graphtheory, we identify the relation betweenconsistency and components of graphs thatrepresent word-aligned sentence pairs.
Itcan be shown that phrase pairs of interestto SMT form a sigma-algebra generatedby components of such graphs.
This con-struction is generalized by allowing seg-mented sentence pairs, which in turn givesrise to a phrase-based generative model.
Aby-product of this model is a derivation ofprobability mass functions for random par-titions.
These are realized as cases of con-strained, biased sampling without replace-ment and we provide an exact formula forthe probability of a segmentation of a sen-tence.1 IntroductionA parallel corpus, i.e., a collection of sentences ina source and a target language, which are trans-lations of each other, is a core ingredient of ev-ery SMT system.
It serves the purpose of trainingdata, i.e., data from which translation rules are ex-tracted.
In its most basic form, SMT does not re-quire the parallel corpus to be annotated with lin-guistic information, and human supervision is thusrestricted to the construction of the parallel corpus.The extraction of translation rules is done by ap-propriately collecting statistics from the trainingdata.
The pioneering work of (Brown et al 1993)identified the minimum assumptions that shouldbe made in order to extract translation rules anddeveloped the relevant models that made such ex-tractions possible.These models, known as IBMmodels, are basedon standard machine learning techniques.
Theiroutput is a matrix of word alignments for each sen-tence pair in the training data.
These word align-ments provide the input for later approaches thatconstruct phrase-level translation rules which may(Wu, 1997; Yamada and Knight, 2001) or may not(Och et al 1999; Marcu and Wong, 2002) rely onlinguistic information.The method developed in (Och et al 1999),known as the consistency method, is a simple yeteffective method that has become the standard wayof extracting (source, target)-pairs of phrases astranslation rules.
The development of consistencyhas been done entirely on empirical evidence andit has thus been termed a heuristic.In this work we show that the method of (Ochet al 1999) actually encodes a particular type ofstructural information induced by the word align-ment matrices.
Moreover, we show that the way inwhich statistics are extracted from the associatedphrase pairs is insufficient to describe the underly-ing structure.Based on these findings we suggest a phrase-level model in the spirit of the IBM models.
A keyaspect of the model is that it identifies the mostlikely partitions, rather than alignment maps, asso-ciated with appropriately chosen segments of thetraining data.
For that reason, we provide a gen-eral construction of probability mass functions forpartitions and, in particular, an exact formula forthe probability of a segmentation of a sentence.932 Definition of ConsistencyIn this section we provide the definition of consis-tency, which was introduced in (Och et al 1999),refined in (Koehn et al 2003), and we follow(Koehn, 2009) in our description.
We start withsome preliminary definitions.Let S = s1...s|S| be a source sentence, i.e., astring that consists of consecutive source words;each word si is drawn from a source language vo-cabulary and i indicates the position of the wordin S. The operation of string extraction from thewords of S is defined as the construction of thestring s = si1 ...sin from the words of S, with1 ?
i1 < ... < in ?
|S|.
If i1, ..., in are consecu-tive, which implies that s is a substring of S, thens is called a source phrase and we write s ?
S.As a shorthand we also write sini1 for the phrasesi1 ...sin .
Similar definitions apply to the targetside and we denote by T, tj and t a target sen-tence, word and phrase respectively.Let (S = s1s2...s|S|, T = t1t2...t|T |) be a sen-tence pair and letA denote the |S|?|T |matrix thatencodes the existence/absence of word alignmentsin (S, T ) asA(i, j) ={1, if si and tj are aligned0, otherwise,(1)for all i = 1, ..., |S| and j = 1, ..., |T |.
Un-aligned words are allowed.
A pair of strings (s =si1 ...si|s| , t = tj1 ...tj|t|) that is extracted from(S, T ) is termed consistent with A, if the follow-ing conditions are satisfied:1. s ?
S and t ?
T .2.
?k ?
{1, ..., |s|} such that A(ik, j) = 1, thenj ?
{j1, ..., j|t|}.3.
?l ?
{1, ..., |t|} such that A(i, jl) = 1, theni ?
{i1, ..., i|s|}.4.
?k ?
{1, ..., |s|} and ?l ?
{1, ..., |t|} suchthat A(ik, jl) = 1.Condition 1 guarantees that (s, t) is a phrasepair and not just a pair of strings.
Condition 2 saysthat if a word in s is aligned to one or more wordsin T , then all such target words must appear in t.Condition 3 is the equivalent of Condition 2 for thetarget words.
Condition 4 guarantees the existenceof at least one word alignment in (s, t).For a sentence pair (S, T ), the set of all consis-tent pairs with an alignment matrix A is denotedby P (S, T ).
Figure 1(a) shows an example of asentence pair with an alignment matrix togetherwith all its consistent pairs.In SMT the extraction of each consistent pair(s, t) from (S, T ) is followed by a statisticf(s, t;S, T ).
Typically f(s, t;S, T ) counts the oc-currences of (s, t) in (S, T ).
By considering allsentence pairs in the training data, the translationprobability is constructed asp(t|s) =?
(S,T ) f(s, t;S, T )?
(S,T )?t?
f(s, t?
;S, T ), (2)and similarly for p(s|t).
Finally, the entries of thephrase table consist of all extracted phrase pairs,their corresponding translation probabilities andother models which we do not discuss here.3 Consistency and ComponentsFor a given sentence pair (S, T ) and a fixed wordalignment matrixA, our aim is to show the equiva-lence between consistency and connectivity prop-erties of the graph formed by (S, T ) and A. More-over, we explain that the way in which measure-ments are performed is not compatible , in princi-ple, with the underlying structure.
We start withsome basic definitions from graph theory (see forexample (Harary, 1969)).Let G = (V,E) be a graph with vertex set Vand edge set E. Throughout this work, verticesrepresent words and edges represent word align-ments, but the latter will be further generalized inSection 4.
A subgraph H = (V ?, E?)
of G is agraph with V ?
?
V , E?
?
E and the propertythat for each edge in E?, both its endpoints are inV ?.
A path in G is a sequence of edges which con-nect a sequence of distinct vertices.
Two verticesu, v ?
V are called connected if G contains a pathfrom u to v. G is said to be connected if every pairof vertices in G is connected.A connected component, or simply component,of G is a maximal connected subgraph of G. Gis called bipartite if V can be partitioned in setsVS and VT , such that every edge in E connects avertex in VS to one in VT .
The disjoint union ofgraphs, or simply union, is an operation on graphsdefined as follows.
For n graphs with disjoint ver-tex sets V1, ..., Vn (and hence disjoint edge sets),their union is the graph (?ni=1Vi,?ni=1Ei).Consider the graph G whose vertices are thewords of the source and target sentences, andwhose edges are induced by the non-zero entries94t1 t2 t3 t4 t5 t6 t7s1s2s3s4s5 s1 s2t1 t2 t3s3 s4t4 t5 t6s5t7s1 s3t1 t4 t5 s2 s4t2 t3 t6 s5t7,{ }C1= Gs1 s3t1 t4 t5 s2 s4t2 t3 t6 s5t7{ }C2= s1 s3t1 t4 t5 t6 s5t7 s2 s4t2 t3 t6 s2 s4t2 t3 s5t7s1 s3t1 t4 t5s1 s3t1 t4 t5 s2 s4t2 t3 s5t7{ }C3= s1 s3t1 t4 t5 t6 s5t7s2 s4t2 t3 t6s2 s4t2 t3 s5t7s1 s3t1 t4 t5t6s1 s3t1 t4 t5 s2 s4t2 t3{ }C 4= t6 s5t7(s5 , t7) ,(s14 , t15) ,(s5 , t67) ,(s14 , t16) ,(S ,T )(a)(b)P (S ,T )= { }, ,, , , , ,, , ,Figure 1: (a) Left: Sentence pair with an alignment matrix.
Dots indicate existence of word alignments.Right: All consistent pairs.
(b) The graph representation of the matrix in (a), and the sets generated bycomponents of the graph.
Dark shading indicates consistency.of the matrix A.
There are no edges betweenany two source-type vertices nor between any twotarget-type vertices.
Moreover, the source and tar-get language vocabularies are assumed to be dis-joint and thus G is bipartite.
The set of all com-ponents of G is defined as C1 and let k denote itscardinality, i.e., |C1| = k. From the members ofC1 we further construct sets C2, ..., Ck as follows:For each i, 2 ?
i ?
k, any member ofCi is formedby the union of any i distinct members of C1.
Inother words, any member of Ci is a graph with icomponents and each such component is a mem-ber of C1.
The cardinality of Ci is clearly(ki), forevery i, 1 ?
i ?
k.Note that Ck = {G}, since G is the union ofall members of C1.
Moreover, observe that C?
=?ki=1Ci is the set of graphs that can be generatedby all possible unions of G?s components.
In thatsenseC = {?}
?
C?
(3)is the power set of G. Indeed we have |C| = 1 +?ki=1(ki)= 2k as required.1Figure 1(b) shows the graph G and the associ-ated sets Ci of (S, T ) and A in Figure 1(a).
Notethe bijective correspondence between consistent1Here we used the fact that for any set X with |X| =n, the set of all subsets of X , i.e., the power set of X , hascardinalityPni=0`ni?= 2n.pairs and the phrase pairs that can be extractedfrom the vertices of the members of the sets Ci.This is a consequence of consistency Conditions 2and 3, since they provide the sufficient conditionsfor component formation.In general, if a pair of strings (s, t) satisfies theconsistency Conditions 2 and 3, then it can be ex-tracted from the vertices of a graph inCi, for somei.
Moreover, if Conditions 1 and 4 are also satis-fied, i.e., if (s, t) is consistent, then we can writeP (S, T ) =k?i=1{(SH , TH) : H ?
Ci,SH ?
S, TH ?
T},(4)where SH denotes the extracted string from thesource-type vertices of H , and similarly for TH .Having established this relationship, when refer-ring to members of C, we henceforth mean eitherconsistent pairs or inconsistent pairs.
The latterare pairs (SH , TH) for some H ?
C such that atleast either SH 6?
S or TH 6?
T .The construction above shows that phrase pairsof interest to SMT are part of a carefully con-structed subclass of all possible string pairs thatcan be extracted from (S, T ).
The power set Cof G gives rise to a small, possibly minimal, set95in which consistent and inconsistent pairs can bemeasured.1 In other words, since C is (by con-struction) a sigma-algebra, the pair (C1, C) is ameasurable space.
Furthermore, one can constructa measure space (C1, C, f), with an appropriatelychosen measure f : C ?
[0,?
).Is the occurrence-counting measure f of Sec-tion 2 a good choice?
Fix an ordering for Ci, andlet Ci,j denote the jth member of Ci, for all i,1 ?
i ?
k. Furthermore, let ?
(x, y) = 1, if x = yand 0, otherwise.
We argue by contradiction thatthe occurrence-counting measuref(H) =?{H?
: H?
?C, H?
is consistent}?
(H,H ?
), (5)fails to form a measure space.
Suppose that morethan one component of G is consistent, i.e., sup-pose that1 <k?j=1f(C1,j) ?
k. (6)By construction of C, it is guaranteed that1 = f(G) = f(Ck,1) = f(?kj=1 C1,j).
(7)The members of C1 are pairwise disjoint, becauseeach of them is a component ofG.
Thus, since f isassumed to be a measure, sigma-additivity shouldbe satisfied, i.e., we must havef(?kj=1 C1,j) =k?j=1f(C1,j) > 1, (8)which is a contradiction.In practice, the deficiency of using eq.
5 asa statistic could possibly be explained by thefact that the so-called lexical weights are used assmoothing.4 Consistency, Components andSegmentationsIn Section 3 the only relation that was assumedamong source (target) words/vertices was the or-der of appearance in the source (target) sentence.As a result, the graph representation G of (S, T )and A was bipartite.
There are several, linguisti-cally motivated, ways in which a general graph canbe obtained from the bipartite graph G. We ex-plain that the minimal linguistic structure, namely1See Appendix for definitions.sentence segmentations, can provide a generaliza-tion of the construction introduced in Section 3.Let X be a finite set of consecutive integers.
Aconsecutive partition of X is a partition of X suchthat each part consists of integers consecutive inX .
A segmentation ?
of a source sentence S is aconsecutive partition of {1, ..., |S|}.
A part of ?,i.e., a segment, is intuitively interpreted as a phrasein S. In the graph representation G of (S, T ) andA, a segmentation ?
of S is realised by the ex-istence of edges between consecutive source-typevertices whose labels, i.e., word positions in S, ap-pear in the same segment of ?.
The same argumentholds for a target sentence and its words; a targetsegmentation is denoted by ?
.Clearly, there are 2|S|?1 possible ways to seg-ment S and, given a fixed alignment matrix A,the number of all possible graphs that can be con-structed is thus 2|S|+|T |?2.
The bipartite graphof Section 3 is just one possible configuration,namely the one in which each segment of ?
con-sists of exactly one word, and similarly for ?
.
Wedenote this segmentation pair by (?0, ?0).We now turn to extracting consistent pairs inthis general setting from all possible segmenta-tions (?, ?)
for a sentence pair (S, T ) and a fixedalignment matrix A.
As in Section 3, we con-struct graphs G?,?
, associated sets C?,?i , for all i,1 ?
i ?
k?,?
, and C?,?
, for all (?, ?).
Consistentpairs are extracted in lieu of eq.
4, i.e.,P ?,?
(S, T ) =k?,?
?i=1{(SH , TH) : H ?
C?,?i ,SH ?
S, TH ?
T}, (9)and it is trivial to see that{(S, T )} ?
P ?,?
(S, T ) ?
P (S, T ), (10)for all (?, ?).
Note that P (S, T ) = P ?0,?0(S, T )and, depending on the details of A, it is possiblefor other pairs (?, ?)
to attain equality.
Moreover,each consistent pair in P (S, T ) can be be extractedfrom a member of at least one C?,?
.We focus on the sets C?,?1 , i.e., the componentsof G?,?
, for all (?, ?).
In particular, we are inter-ested in the relation between P (S, T ) and C?,?1 ,for all (?, ?).
Each consistent H ?
C?0,?0 canbe converted into a single component by appropri-ately forming edges between consecutive source-type vertices and/or between consecutive target-type vertices.
The resulting component will evi-dently be a member of C?,?1 , for some (?, ?).
It96is important to note that the conversion of a con-sistent H ?
C?0,?0 into a single component neednot be unique; see Figure 2 for a counterexam-ple.
Since (a) such conversions are possible forall consistent H ?
C?0,?0 and (b) P (S, T ) =P ?0,?0(S, T ), it can be deduced that all possibleconsistent pairs can be traced in the sets C?,?1 , forall (?, ?).
In other words, we have:P (S, T ) =??,?
{(SH , TH) : H ?
C?,?1 ,SH ?
S, TH ?
T}.
(11)The above equation says that by taking sen-tence segmentations into account, we can recoverall possible consistent pairs, by inspecting only thecomponents of the underlying graphs.It would be interesting to investigate the re-lation between measure spaces (C?,?1 , C?,?
, f?,?
)and different configurations for A.
We leave thatfor future work and focus on the advantages pro-vided by eq.
11.t 1t 2 t 3 t 4s2 s1 s3t 1t 2 t 3 t 4s2 s1 s3 t 1t 2 t 3 t 4s2 s1 s3t 1t 2 t 3 t 4s2 s1 s3 t 1t 2 t 3 t 4s2 s1 s3Figure 2: A graph with three components (top),and four possible conversions into a single compo-nent by forming edges between contiguous words.5 Towards a phrase-level model thatrespects consistencyThe aim of this section is to exploit the relationestablished in eq.
11 between consistent pairs andcomponents of segmented sentence pairs.
It wasalso shown in Section 2 that the computation of thetranslation models is inappropriate to describe theunderlying structure.
We thus suggest a phrase-based generative model in the spirit of the IBMword-based models, which is compatible with theconstruction of the previous sections.5.1 Hidden variablesAll definitions from the previous sections are car-ried over, and we introduce a new quantity that isassociated with components.
Let G?,?
and C?,?1 ,for some (?, ?)
be as in Section 4, then the setK is defined as follows: Each member of K isa pair of (source, target) sets of segments that cor-responds to the pair of (source, target) vertices ofa consistent member of C?,?1 .
In other words, K isa bisegmentation of a pair of segmented sentencesthat respects consistency.Figure 3 shows three possible ways to con-struct consistent graphs from (S, T ) = (s41, t61),?
= {{1, 2}, {3}, {4}} ?
{x1, x2, x3} and ?
={{1}, {2, 3, 4}, {5}, {6}} ?
{y1, y2, y3, y4}.
Ineach case the exact alignment information is un-known and we have:(a) K ={ ({x1}, {y1}),({x2}, {y2}),({x3}, {y3, y4}) }.
(b) K ={ ({x1, x2}, {y1, y2, y3}),({x3}, {y4})}.
(c) K ={ ({x1}, {y3, y4}),({x2, x3}, {y1, y2})}.t1s1 s2 s3t 2 t3 t4 s5t 5 t6t1 s1 s2 s3t 2 t3 t4t 5 t6s5t1s1 s2 s3t 2 t3t4 t 5t6 s5(a)(b)(c)Figure 3: Three possible ways to construct con-sistent graphs for (s41, t61) and a given segmenta-tion pair.
Exact word alignment information is un-known.In the proposed phrase-level generative modelthe random variables whose instances are ?, ?
and97K are hidden variables.
As with the IBM mod-els, they are associated with the positions of wordsin a sentence, rather than the words themselves.Alignment information is implicitly identified viathe consistent bisegmentation K.Suppose we have a corpus that consists of pairsof parallel sentences (S, T ), and let fS,T denotethe occurrence count of (S, T ) in the corpus.
Also,let lS = |S| and lT = |T |.
The aim is to maximizethe corpus log-likelihood function` =?S,TfS,T log p?
(T |S)=?S,TfS,T log??,?,Kp?
(T, ?, ?,K|S), (12)where ?, ?
and K are hidden variables parameter-ized by a vector ?
of unknown weights, whose val-ues are to be determined.
The expectation max-imization algorithm (Dempster et al 1977) sug-gests that an iterative application of?n+1 = argmax??S,TfS,T?
?,?,Kp?n(?, ?,K|S, T )?log p?
(T, ?, ?,K|S),(13)provides a good approximation for the maximumvalue of `.
As with the IBM models we seek prob-ability mass functions (PMFs) of the formp?
(T, ?, ?,K|S) = p?
(lT |S)p?
(?, ?,K|lT , S)?p?
(T |?, ?,K, lT , S),(14)and decompose further asp?
(?, ?,K|lT , S) = p?
(?, ?
|lT , S)p?
(K|?, ?, lT , S)(15)A further simplification of p?
(?, ?
|lT , S) =p?(?|S)p?(?
|lT ) may not be desirable, but willhelp us understand the relation between ?
and thePMFs.
In particular, we give a formal descriptionof p?
(?|S) and then explain that p?
(K|?, ?, lT , S)and p?
(T |?, ?,K, lT , S) can be computed in asimilar way.5.2 Constrained, biased sampling withoutreplacementThe probability of a segmentation given a sentencecan be realised in two different ways.
We first pro-vide a descriptive approach which is more intu-itive, and we use the sentence S = s41 as an ex-ample whenever necessary.
The set of all possi-ble segments of S is denoted by seg(S) and triv-ially |seg(S)| = |S|(|S| + 1)/2.
Each segmentx ?
seg(S) has a nonnegative weight ?
(x|lS) suchthat ?x?seg(S)?
(x|lS) = 1.
(16)Suppose we have an urn that consists of|seg(S)| weighted balls; each ball corresponds toa segment of S. We sample without replacementwith the aim of collecting enough balls to form asegmentation of S. When drawing a ball x we si-multaneously remove from the urn all other ballsx?
such that x ?
x?
6= ?.
We stop when the urnis empty.
In our example, let the urn contain 10balls and suppose that the first draw is {1, 2}.
Inthe next draw, we have to choose from {3}, {4}and {3, 4} only, since all other balls contain a ?1?and/or a ?2?
and are thus removed.
The sequenceof draws that leads to a segmentation is thus a pathin a decision tree.
Since ?
is a set, there are |?|!different paths that lead to its formation.
The setof all possible segmentations, in all possible waysthat each segmentation can be formed, is encodedby the collection of all such decision trees.The second realisation, which is based on thenotions of cliques and neighborhoods, is moreconstructive and will give rise to the desired PMF.A clique in a graph is a subset U of the vertex setsuch that for every two vertices u, v ?
U , there ex-ists an edge connecting u and v. For any vertex uin a graph, the neighborhood of u is defined as theset N(u) = {v : {u, v} is an edge}.
A maximalclique is a clique U that is not a subset of a largerclique: For each u ?
U and for each v ?
N(u) theset U ?
{v} is not a clique.Let G be the graph whose vertices are all seg-ments of S and whose edges satisfy the conditionthat any two vertices x and x?
form an edge iffx ?
x?
= ?
; see Figure 4 for an example.
G es-sentially provides a compact representation of thedecision trees discussed above.It is not difficult to see that a maximal cliquealso forms a segmentation.
Moreover, the set of allmaximal cliques in G is exactly the set of all pos-sible segmentations for S. Thus, p?
(?|S) shouldsatisfyp?
(?|S) = 0, if ?
is not a clique in G, (17)and ??p?
(?|S) = 1, (18)98{1}{2}{3}{4}{253}{154}{351}{35154} {25351}{2535154}Figure 4: The graph whose vertices are the seg-ments of s41 and whose edges are formed by non-overlapping vertices.where the sum is over all maximal cliques in G.In our example p?
({ {1}, {1, 2} }|S)= 0, be-cause there is no edge connecting segments {1}and {1, 2} so they are not part of any clique.In order to derive an explicit formula forp?
(?|S) we focus on a particular type of pathsin G. A path is called clique-preserving, if ev-ery vertex in the path belongs to the same clique.Our construction should be such that each clique-preserving path has positive probability of occur-ring, and all other paths should have probability0.
We proceed with calculating probabilities ofclique-preserving paths based on the structure ofG and the constraint of eq.
16.The probability p?
(?|S) can be viewed asthe probability of generating all clique-preservingpaths on the maximal clique ?
in G. Since?
is a clique, there are |?|!
possible paths thatspan its vertices.
Let ?
= {x1, ..., x|?|},and let pi denote a permutation of {1, ..., |?|}.We are interested in computing the probabil-ity q?
(xpi(1), ..., xpi(|?|)) of generating a clique-preserving path xpi(1), ..., xpi(|?|) in G.
Thus,p?
(?|S) = p?
({x1, ..., x|?|}|S)=?piq?
(xpi(1), ..., xpi(|?|))=?piq?
(xpi(1)) q?(xpi(2)|xpi(1))?
......?
q?
(xpi(|?|)|xpi(1), ..., xpi(|?|?1)).
(19)The probabilities q?(?)
can be explicitly calcu-lated by taking into account the following ob-servation.
A clique-preserving path on a clique?
can be realised as a sequence of verticesxpi(1), ..., xpi(i), ..., xpi(|?|) with the following con-straint: If at step i ?
1 of the path we are at ver-tex xpi(i?1), then the next vertex xpi(i) should be aneighbor of all of xpi(1), ..., xpi(i?1).
In other wordswe must havexpi(i) ?
Npi,i ?i?1?l=1N(xpi(l)).
(20)Thus, the probability of choosing xpi(i) as the nextvertex of the path is given byq?
(xpi(i)|xpi(1), ..., xpi(i?1)) =?(xpi(i)|lS)?x?Npi,i?
(x|lS),(21)if xpi(i) ?
Npi,i and 0, otherwise.
When choosingthe first vertex of the path (the root in the deci-sion tree) we have Npi,1 = seg(S), which givesq?
(xpi(1)) = ?
(xpi(1)|lS), as required.
Thereforeeq.
19 can be written compactly asp?
(?|S) =??|?|?i=1?(xi|lS)???pi1Q?
(?, pi;S),(22)whereQ?
(?, pi;S) =|?|?i=1?x?Npi,i?
(x|lS) .
(23)The construction above can be generalized inorder to derive a PMF for any random variablewhose values are partitions of a set.
Indeed, by al-lowing the vertices of G to be a subset of a powerset, and keeping the condition of edge formationthe same, probabilities of clique-preserving pathscan be calculated in the same way.
Figure 5 showsthe graph G that represents all possible instances ofK with (S, T ) = (s41, t51), ?
={{1, 2}, {3}, {4}}and ?
={{1}, {2, 3, 4}, {5}}.
Again each maxi-mal clique is a possible consistent bisegmentation.In order for this model to be complete, oneshould solve the maximization step of eq.
13 andcalculate the posterior p?n(?, ?,K|S, T ).
We arenot bereft of hope, as relevant techniques havebeen developed (see Section 6).6 Related WorkTo our knowledge, this is the first attempt to inves-tigate formal motivations behind the consistencymethod.99t 12 s, 23t 4 s,5 t 3 s, 5t 12 s, 1t 4 s,1 t 3 s, 1t 12 s, 5t 3 s, 23 t 4 s, 23t 14 s,13t 14 s, 25t 43 s, 25t 13 s, 15t 14 s,1 t 3 s, 25 t 14 s,5 t 3 s, 13t 43 s, 13Figure 5: Similar to Figure 4 but for consistentbisegmentations with (S, T ) = (s41, t51) and agiven segmentation pair (see text).
For clarity, weshow the phrases that are formed from joining con-tiguous segments in each pair, rather than the seg-ments themselves.Several phrase-level generative models havebeen proposed, almost all relying on multinomialdistributions for the phrase alignments (Marcu andWong, 2002; Zhang et al 2003; Deng and Byrne2005; DeNero et al 2006; Birch et al 2006).This is a consequence of treating alignments asfunctions rather than partitions.Word alignment and phrase extraction via In-version Transduction Grammars (Wu, 1997), is alinguistically motivated method that relies on si-multaneous parsing of source and target sentences(DeNero and Klein, 2010; Cherry and Lin 2007;Neubig et al 2012).The partition probabilities we introduced inSection 5.2 share the same tree structure discussedin (Dennis III, 1991), which has found applica-tions in Information Retrieval (Haffari and Teh,2009).7 ConclusionsWe have identified the relation between consis-tency and components of graphs that representword-aligned sentence pairs.
We showed thatphrase pairs of interest to SMT form a sigma-algebra generated by components of such graphs,but the existing occurrence-counting statistics areinadequate to describe this structure.
A general-ization of our construction via sentence segmenta-tions lead to a realisation of random partitions ascases of constrained, biased sampling without re-placement.
As a consequence, we derived an exactformula for the probability of a segmentation of asentence.Appendix: Measure SpaceThe following standard definitions can be foundin, e.g., (Feller, 1971).
LetX be a set.
A collectionB of subsets of X is called a sigma-algebra if thefollowing conditions hold:1. ?
?
B.2.
If E is in B, then so is its complement X \E.3.
If {Ei} is a countable collection of sets in B,then so is their union ?iEi.Condition 1 guarantees that B is non-empty andConditions 2 and 3 say thatB is closed under com-plementation and countable unions respectively.The pair (X,B) is called a measurable space.A function f : B ?
[0,?)
is called a measureif the following conditions hold:1.
f(?)
= 0.2.
If {Ei} is a countable collection of pairwisedisjoint sets in B, thenf(?iEi) =?if(Ei).Condition 2 is known as sigma-additivity.
Thetriple (X,B, f) is called a measure space.AcknowledgmentsThis research was supported by the EuropeanUnion?s ICT Policy Support Programme as partof the Competitiveness and Innovation FrameworkProgramme, CIP ICT-PSP under grant agreementnr 250430 (GALATEAS) and by the EC fundedproject CoSyne (FP7-ICT-4-24853).ReferencesAlexandra Birch, Chris Callison-Burch, Miles Os-borne and Philipp Koehn.
2006.
Constraining thePhrase-Based, Joint Probability Statistical Transla-tion Model.
In Proc.
of the Workshop on StatisticalMachine Translation, pages 154?157.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.The Mathematics of Statistical Machine Translation.Computational Linguistics, vol.19(2), pages 263?312.100Colin Cherry and Dekang Lin.
2007.
Inversion Trans-duction Grammar for Joint Phrasal TranslationMod-eling.
In Proc.
of SSST, NAACL-HLT / AMTA Work-shop on Syntax and Structure in Statistical Transla-tion, pages 17?24.A.P.
Dempster, N.M. Laird and D.B.
Rubin.
1977.Maximum Likelihood from Incomplete Data via theEM Algorithm.
Journal of the Royal Statistical So-ciety, Series B (Methodological) 39(1), pages 1?38.John DeNero, Dan Gillick, James Zhang and DanKlein.
2006.
Why Generative Phrase Models Un-derperform Surface Heuristics.
In Proc.
of the Work-shop on Statistical Machine Translation, pages 31?38.John DeNero and Dan Klein.
2010.
DiscriminativeModeling of Extraction Sets for Machine Transla-tion.
In Proc.
of the Association for ComputationalLinguistics (ACL), pages 1453?1463.Yonggang Deng and William Byrne.
2005.
HMMWord and Phrase Alignment for Statistical MachineTranslation.
In Proc.
of the Conference on Empir-ical Methods in Natural Language Processing andHuman Language Technology (HLT-EMNLP), pages169?176.Samuel Y. Dennis III.
1991.
On the Hyper-DirichletType 1 and Hyper-Liouville Distributions.
Commu-nications in Statistics - Theory and Methods, 20(12),pages 4069?4081.William Feller.
1971.
An Introduction to ProbabilityTheory and its Applications, Volume II.
John Wiley,New York.Gholamreza Haffari and Yee Whye Teh.
2009.
Hi-erarchical Dirichlet Trees for Information Retrieval.In Proc.
of the Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics on Human Language Technology (HLT-NAACL), pages 173?181.Frank Harary.
1969.
Graph Theory.
Addison?Wesley,Reading, MA.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Proc.of the Conference of the North American Chapterof the Association for Computational Linguistics onHuman Language Technology (HLT-NAACL), pages48?54.Philipp Koehn.
2009.
Statistical Machine Translation.Cambridge University Press, Cambridge, UK.Daniel Marcu and William Wong.
2002.
A Phrase-Based, Joint Probability Model for Statistical Ma-chine Translation.
In Proc.
of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 133?139.Graham Neubig, Taro Watanabe, Eiichiro Sumita,Shinsuke Mori and Tatsuya Kawahara.
2012.
JointPhrase Alignment and Extraction for Statistical Ma-chine Translation.
Journal of Information Process-ing, vol.
20(2), pages 512?523.Franz J. Och, Christoph Tillmann, and Hermann Ney.1999.
Improved Alignment Models for StatisticalMachine Translation.
In Proc.
of the Joint Con-ference of Empirical Methods in Natural LanguageProcessing and Very Large Corpora (EMNLP-VLC),pages 20?28.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Cor-pora.
Computational Linguistics, 23, pages 377?404.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proc.
of theAssociation for Computational Linguistics (ACL),pages 523?530.Ying Zhang, Stephan Vogel and Alex Waibel.
2003.Integrated Phrase Segmentation and Alignment Al-gorithm for Statistical Machine Translation.
InProc.
of the International Conference on NaturalLanguage Processing and Knowledge Engineering(NLP-KE).101
