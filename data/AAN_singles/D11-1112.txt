Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1213?1221,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsComputation of Infix Probabilitiesfor Probabilistic Context-Free GrammarsMark-Jan NederhofSchool of Computer ScienceUniversity of St AndrewsUnited Kingdommarkjan.nederhof@gmail.comGiorgio SattaDept.
of Information EngineeringUniversity of PaduaItalysatta@dei.unipd.itAbstractThe notion of infix probability has been intro-duced in the literature as a generalization ofthe notion of prefix (or initial substring) prob-ability, motivated by applications in speechrecognition and word error correction.
For thecase where a probabilistic context-free gram-mar is used as language model, methods forthe computation of infix probabilities havebeen presented in the literature, based on vari-ous simplifying assumptions.
Here we presenta solution that applies to the problem in its fullgenerality.1 IntroductionProbabilistic context-free grammars (PCFGs forshort) are a statistical model widely used in naturallanguage processing.
Several computational prob-lems related to PCFGs have been investigated inthe literature, motivated by applications in model-ing of natural language syntax.
One such problem isthe computation of prefix probabilities for PCFGs,where we are given as input a PCFG G and a stringw, and we are asked to compute the probability thata sentence generated by G starts with w, that is, hasw as a prefix.
This quantity is defined as the possi-bly infinite sum of the probabilities of all strings ofthe form wx, for any string x over the alphabet of G.The problem of computation of prefix probabili-ties for PCFGs was first formulated by Persoon andFu (1975).
Efficient algorithms for its solution havebeen proposed by Jelinek and Lafferty (1991) andStolcke (1995).
Prefix probabilities can be used tocompute probability distributions for the next wordor part-of-speech, when a prefix of the input has al-ready been processed, as discussed by Jelinek andLafferty (1991).
Such distributions are useful forspeech recognition, where the result of the acous-tic processor is represented as a lattice, and localchoices must be made for a next transition.
In ad-dition, distributions for the next word are also usefulfor applications of word error correction, when oneis processing ?noisy?
text and the parser recognizesan error that must be recovered by operations of in-sertion, replacement or deletion.Motivated by the above applications, the problemof the computation of infix probabilities for PCFGshas been introduced in the literature as a generaliza-tion of the prefix probability problem.
We are nowgiven a PCFG G and a string w, and we are askedto compute the probability that a sentence generatedby G has w as an infix.
This probability is definedas the possibly infinite sum of the probabilities ofall strings of the form xwy, for any pair of strings xand y over the alphabet of G. Besides applicationsin computation of the probability distribution for thenext word token and in word error correction, in-fix probabilities can also be exploited in speech un-derstanding systems to score partial hypotheses inalgorithms based on beam search, as discussed byCorazza et al (1991).Corazza et al (1991) have pointed out that thecomputation of infix probabilities is more difficultthan the computation of prefix probabilities, due tothe added ambiguity that several occurrences of thegiven infix can be found in a single string generatedby the PCFG.
The authors developed solutions forthe case where some distribution can be defined on1213the distance of the infix from the sentence bound-aries, which is a simplifying assumption.
The prob-lem is also considered by Fred (2000), which pro-vides algorithms for the case where the languagemodel is a probabilistic regular grammar.
However,the algorithm in (Fred, 2000) does not apply to caseswith multiple occurrences of the given infix withina string in the language, which is what was pointedout to be the problematic case.In this paper we adopt a novel approach to theproblem of computation of infix probabilities, by re-moving the ambiguity that would be caused by mul-tiple occurrences of the given infix.
Although ourresult is obtained by a combination of well-knowntechniques from the literature on PCFG parsing andpattern matching, as far as we know this is the firstalgorithm for the computation of infix probabilitiesthat works for general PCFG models without any re-strictive assumption.The remainder of this paper is structured as fol-lows.
In Section 2 we explain how the sum of theprobabilities of all trees generated by a PCFG canbe computed as the least fixed-point solution of anon-linear system of equations.
In Section 3 we re-call the construction of a new PCFG out of a givenPCFG and a given finite automaton, such that thelanguage generated by the new grammar is the in-tersection of the languages generated by the givenPCFG and the automaton, and the probabilities ofthe generated strings are preserved.
In Section 4we show how one can efficiently construct an un-ambiguous finite automaton that accepts all stringswith a given infix.
The material from these threesections is combined into a new algorithm in Sec-tion 5, which allows computation of the infix prob-ability for PCFGs.
This is the main result of thispaper.
Several extensions of the basic technique arediscussed in Section 6.
Section 7 discusses imple-mentation and some experiments.2 Sum of probabilities of all derivationsAssume a probabilistic context-free grammar G, rep-resented by a 5-tuple (?, N, S, R, p), where ?
andN are two finite disjoint sets of terminals and non-terminals, respectively, S ?
N is the start symbol,R is a finite set of rules, each of the form A ?
?,whereA ?
N and ?
?
(?
?N)?, and p is a functionfrom rules in R to real numbers in the interval [0, 1].The concept of left-most derivation in one step isrepresented by the notation ?
pi?G ?, which meansthat the left-most occurrence of any nonterminal in?
?
(?
?
N)?
is rewritten by means of some rulepi ?
R. If the rewritten nonterminal is A, then pimust be of the form (A ?
?)
and ?
is the resultof replacing the occurrence of A in ?
by ?.
A left-most derivation with any number of steps, using asequence d of rules, is denoted as ?
d?G ?.
We omitthe subscript G when the PCFG is understood.
Wealso write ?
??
?
when the involved sequence ofrules is of no relevance.
Henceforth, all derivationswe discuss are implicitly left-most.A complete derivation is either the empty se-quence of rules, or a sequence d = pi1 ?
?
?pim, m ?1, of rules such that A d?
w for some A ?
N andw ?
??.
In the latter case, we say the completederivation starts with A, and in the former case, withd an empty sequence of rules, we assume the com-plete derivation starts and ends with a single termi-nal, which is left unspecified.
It is well-known thatthere exists a bijective correspondence between left-most complete derivations starting with nonterminalA and parse trees derived by the grammar with rootA and a yield composed of terminal symbols only.The depth of a complete derivation d is the lengthof the longest path from the root to a leaf in the parsetree associated with d. The length of a path is de-fined as the number of nodes it visits.
Thus if d = pifor some rule pi = (A ?
w) with w ?
?
?, then thedepth of d is 2.The probability p(d) of a complete derivation d =pi1 ?
?
?pim, m ?
1, is:p(d) =m?i=1p(pii).We also assume that p(d) = 1 when d is an emptysequence of rules.
The probability p(w) of a stringw is the sum of all complete derivations deriving thatstring from the start symbol:p(w) =?d: S d?wp(d).With this notation, consistency of a PCFG is de-1214fined as the condition:?d,w: S d?wp(d) = 1.In other words, a PCFG is consistent if the sumof probabilities of all complete derivations startingwith S is 1.
An equivalent definition of consistencyconsiders the sum of probabilities of all strings:?wp(w) = 1.See (Booth and Thompson, 1973) for further discus-sion.In practice, PCFGs are often required to satisfythe additional condition:?pi=(A??
)p(pi) = 1,for each A ?
N .
This condition is called proper-ness.
PCFGs that naturally arise by parameter es-timation from corpora are generally consistent; see(Sa?nchez and Bened?
?, 1997; Chi and Geman, 1998).However, in what follows, neither properness norconsistency is guaranteed.We define the partition function of G as the func-tion Z that assigns to each A ?
N the valueZ(A) =?d,wp(A d?
w).
(1)Note that Z(S) = 1 means that G is consistent.More generally, in later sections we will need tocompute the partition function for non-consistentPCFGs.We can characterize the partition function of aPCFG as a solution of a specific system of equa-tions.
Following the approach in (Harris, 1963; Chi,1999), we introduce generating functions associatedwith the nonterminals of the grammar.
For A ?
Nand ?
?
(N ?
?
)?, we write f(A,?)
to denote thenumber of occurrences of symbol A within string ?.Let N = {A1, A2, .
.
.
, A|N |}.
For each Ak ?
N , letmk be the number of rules in R with left-hand sideAk, and assume some fixed order for these rules.
Foreach i with 1 ?
i ?
mk, let Ak ?
?k,i be the i-thrule with left-hand side Ak.For each k with 1 ?
k ?
|N |, the generatingfunction associated with Ak is defined asgAk(z1, z2, .
.
.
, z|N |) =mk?i=1(p(Ak ?
?k,i) ?|N |?j=1zf(Aj ,?k,i)j).
(2)Furthermore, for each i ?
1 we recursively definefunctions g(i)Ak(z1, z2, .
.
.
, z|N |) byg(1)Ak (z1, z2, .
.
.
, z|N |) = gAk(z1, z2, .
.
.
, z|N |), (3)and, for i ?
2, byg(i)Ak(z1, z2, .
.
.
, z|N |) = (4)gAk( g(i?1)A1 (z1, z2, .
.
.
, z|N |),g(i?1)A2 (z1, z2, .
.
.
, z|N |), .
.
.
,g(i?1)A|N| (z1, z2, .
.
.
, z|N |) ).Using induction it is not difficult to show that, foreach k and i as above, g(i)Ak(0, 0, .
.
.
, 0) is the sum ofthe probabilities of all complete derivations fromAkhaving depth not exceeding i.
This implies that, fori = 0, 1, 2, .
.
., the sequence of the g(i)Ak(0, 0, .
.
.
, 0)monotonically converges to Z(Ak).For each k with 1 ?
k ?
|N | we can now writeZ(Ak) == limi?
?g(i)Ak(0, .
.
.
, 0)= limi?
?gAk( g(i?1)A1 (0, 0, .
.
.
, 0), .
.
.
,g(i?1)A|N| (0, 0, .
.
.
, 0) )= gAk( limi??
g(i?1)A1 (0, 0, .
.
.
, 0), .
.
.
,limi??
g(i?1)A|N| (0, 0, .
.
.
, 0) )= gAk(Z(A1), .
.
.
, Z(A|N |)).The above shows that the values of the partitionfunction provide a solution to the system of the fol-lowing equations, for 1 ?
k ?
|N |:zk = gAk(z1, z2, .
.
.
, z|N |).
(5)In the case of a general PCFG, the above equa-tions are non-linear polynomials with positive (real)coefficients.
We can represent the resulting systemin vector form and write X = g(X).
These systems1215are called monotone systems of polynomial equa-tions and have been investigated by Etessami andYannakakis (2009) and Kiefer et al (2007).
Thesought solution, that is, the partition function, is theleast fixed point solution of X = g(X).For practical reasons, the set of nonterminals ofa grammar is usually divided into maximal subsetsof mutually recursive nonterminals, that is, for eachA and B in such a subset, we have A ??
uB?and B ??
vA?, for some u, v, ?, ?.
This corre-sponds to a strongly connected component if wesee the connection between the left-hand side of arule and a nonterminal member in its right-hand sideas an edge in a directed graph.
For each stronglyconnected component, there is a separate system ofequations of the formX = g(X).
Such systems canbe solved one by one, in a bottom-up order.
Thatis, if one strongly connected component containsnonterminalA, and another contains nonterminalB,where A ??
uB?
for some u, ?, then the system forthe latter component must be solved first.The solution for a system of equations such asthose described above can be irrational and non-expressible by radicals, even if we assume that allthe probabilities of the rules in the input PCFG arerational numbers, as observed by Etessami and Yan-nakakis (2009).
Nonetheless, the partition functioncan still be approximated to any degree of preci-sion by iterative computation of the relation in (4),as done for instance by Stolcke (1995) and by Ab-ney et al (1999).
This corresponds to the so-calledfixed-point iteration method, which is well-knownin the numerical calculus literature and is frequentlyapplied to systems of non-linear equations becauseit can be easily implemented.When a number of standard conditions are met,each iteration of (4) adds a fixed number of bitsto the precision of the solution; see Kelley (1995,Chapter 4).
Since each iteration can easily be im-plemented to run in polynomial time, this meansthat we can approximate the partition function of aPCFG in polynomial time in the size of the PCFGitself and in the number of bits of the desired preci-sion.In practical applications where large PCFGs areempirically estimated from data sets, the standardconditions mentioned above for the polynomial timeapproximation of the partition function are usuallymet.
However, there are some degenerate cases forwhich these standard conditions do not hold, result-ing in exponential time behaviour of the fixed-pointiteration method.
This has been firstly observedin (Etessami and Yannakakis, 2005).An alternative iterative algorithm for the approx-imation of the partition function has been proposedby Etessami and Yannakakis (2009), based on New-ton?s method for the solution of non-linear systemsof equations.
From a theoretical perspective, Kieferet al (2007) have shown that, after a certain numberof initial iterations, Newton?s method adds a fixednumber of bits to the precision of the approximatedsolution, even in the above mentioned cases in whichthe fixed-point iteration method shows exponentialtime behaviour.
However, these authors also showthat, in some degenerate cases, the number of itera-tions needed to compute the first bit of the solutioncan be at least exponential in the size of the system.Experiments with Newton?s method for the ap-proximation of the partition functions of PCFGshave been carried out in several application-orientedsettings, by Wojtczak and Etessami (2007) and byNederhof and Satta (2008), showing considerableimprovements over the fixed-point iteration method.3 Intersection of PCFG and FAIt was shown by Bar-Hillel et al (1964) that context-free languages are closed under intersection withregular languages.
Their proof relied on the con-struction of a new CFG out of an input CFG andan input finite automaton.
Here we extend that con-struction by letting the input grammar be a proba-bilistic CFG.
We refer the reader to (Nederhof andSatta, 2003) for more details.To avoid a number of technical complications, weassume the finite automaton has no epsilon transi-tions, and has only one final state.
In the contextof our use of this construction in the following sec-tions, these restrictions are without loss of general-ity.
Thus, a finite automaton (FA) M is representedby a 5-tuple (?, Q, q0, qf , ?
), where ?
and Q aretwo finite sets of terminals and states, respectively,q0 is the initial state, qf is the final state, and ?
isa finite set of transitions, each of the form s a7?
t,where s, t ?
Q and a ?
?.A complete computation of M accepting string1216w = a1 ?
?
?
an is a sequence c = ?1 ?
?
?
?n of tran-sitions such that ?i = (si?1 ai7?
si) for each i (1 ?i ?
n), for some s0, s1, .
.
.
, sn with s0 = q0 andsn = qf .
The language of all strings accepted byMis denoted by L(M).
A FA is unambiguous if atmost one complete computation exists for each ac-cepted string.
A FA is deterministic if there is atmost one transition s a7?
t for each s and a.For a FAM as above and a PCFG G = (?, N, S,R, p) with the same set of terminals, we constructa new PCFG G?
= (?, N ?, S?, R?, p?
), where N ?
=Q?
(?
?N)?Q, S?
= (q0, S, qf ), and R?
is the setof rules that is obtained as follows.?
For each A ?
X1 ?
?
?Xm in R and each se-quence s0, .
.
.
, sm with si ?
Q, 0 ?
i ?
m,and m ?
0, let (s0, A, sm) ?
(s0, X1, s1) ?
?
?
(sm?1, Xm, sm) be in R?
; if m = 0, the newrule is of the form (s0, A, s0) ?
.
Function p?assigns the same probability to the new rule asp assigned to the original rule.?
For each s a7?
t in ?, let (s, a, t) ?
a be in R?.Function p?
assigns probability 1 to this rule.Intuitively, a rule of G?
is either constructed out ofa rule of G or out of a transition of M. On the basisof this correspondence between rules and transitionsof G?, G and M, it is not difficult to see that eachderivation d?
in G?
deriving string w corresponds to aunique derivation d in G deriving the same string anda unique computation c in M accepting the samestring.
Conversely, if there is a derivation d in Gderiving string w, and some computation c in Maccepting the same string, then the pair of d and ccorresponds to a unique derivation d?
in G?
derivingthe same string w. Furthermore, the probabilities ofd and d?
are equal, by definition of p?.Let us now assume that each string w is acceptedby at most one computation, i.e.
M is unambigu-ous.
If a string w is accepted by M, then there areas many derivations deriving w in G?
as there are inG.
If w is not accepted by M, then there are zeroderivations deriving w in G?.
Consequently:?d?,w:S?
d??G?wp?(d?)
=?d,w:S d?Gw?w?L(M)p(d),or more succinctly:?wp?
(w) =?w?L(M)p(w).Note that the above construction of G?
is exponen-tial in the largest value of m in any rule from G. Forthis reason, G is usually brought in binary form be-fore the intersection, i.e.
the input grammar is trans-formed to let each right-hand side have at most twomembers.
Such a transformation can be realized inlinear time in the size of the grammar.
We will returnto this issue in Section 7.4 Obtaining unambiguous FAsIn the previous section, we explained that unambigu-ous finite automata have special properties with re-spect to the grammar G?
that we may construct outof a FA M and a PCFG G. In this section we dis-cuss how unambiguity can be obtained for the spe-cial case of finite automata accepting the languageof all strings with given infix w ?
??
:Linfix (w) = {xwy | x, y ?
??
}.Any deterministic automaton is also unambigu-ous.
Furthermore, there seem to be no practical al-gorithms that turn FAs into equivalent unambiguousFAs other than the algorithms that also determinizethem.
Therefore, we will henceforth concentrate ondeterministic rather than unambiguous automata.Given a string w = a1 ?
?
?
an, a finite automatonaccepting Linfix (w) can be straightforwardly con-structed.
This automaton has states s0, .
.
.
, sn, tran-sitions s0 a7?
s0 and sn a7?
sn for each a ?
?, andtransition si?1 ai7?
si for each i (1 ?
i ?
n).
Theinitial state is s0 and the final state is sn.
Clearly,there is nondeterminism in state s0.One way to make this automaton deterministic isto apply the general algorithm of determinization offinite automata; see e.g.
(Aho and Ullman, 1972).This algorithm is exponential for general FAs.
Analternative approach is to construct a deterministicfinite automaton directly from w, in line with theKnuth-Morris-Pratt algorithm (Knuth et al, 1977;Gusfield, 1997).
Both approaches result in the samedeterministic FA, which we denote by Iw.
However,the latter approach is easier to implement in such a1217way that the time complexity of constructing the au-tomaton is linear in |w|.The automaton Iw is described as follows.
Thereare n + 1 states t0, .
.
.
, tn, where as before n isthe length of w. The initial state is t0 and the finalstate is tn.
The intuition is that Iw reads a stringx = b1 ?
?
?
bm from left to right, and when it hasread the prefix b1 ?
?
?
bj (0 ?
j ?
m), it is in stateti (0 ?
i < n) if and only if a1 ?
?
?
ai is the longestprefix of w that is also a suffix of b1 ?
?
?
bj .
If theautomaton is in state tn, then this means that w is aninfix of b1 ?
?
?
bj .In more detail, for each i (1 ?
i ?
n) and eacha ?
?, there is a transition ti?1 a7?
tj , where j isthe length of the longest string that is both a prefixof w and a suffix of a1 ?
?
?
ai?1a.
If a = ai, thenclearly j = i, and otherwise j < i.
To ensure that weremain in the final state once an occurrence of infixw has been found, we also add transitions tn a7?
tnfor each a ?
?.
This construction is illustrated inFigure 1.5 Infix probabilityWith the material developed in the previous sections,the problem of computing the infix probabilities canbe effectively solved.
Our goal is to compute forgiven infix w ?
??
and PCFG G = (?, N, S, R,p):?infix (w,G) =?z?Linfix (w)p(z).In Section 4 we have shown the construction of finiteautomaton Iw accepting Linfix (w), by which we ob-tain:?infix (w,G) =?z?L(Iw)p(z).As Iw is deterministic and therefore unambiguous,the results from Section 3 apply and if G?
= (?, N ?,S?, R?, p?)
is the PCFG constructed out of G and Iwthen:?infix (w,G) =?zp?
(z).Finally, we can compute the above sum by applyingthe iterative method discussed in Section 2.6 ExtensionsThe approach discussed above allows for a numberof generalizations.
First, we can replace the infix wby a sequence of infixes w1, .
.
.
, wm, which have tooccur in the given order, one strictly after the other,with arbitrary infixes in between:?island (w1, .
.
.
, wm,G) =?x0,...,xm??
?p(x0w1x1 ?
?
?wmxm).This problem was discussed before by (Corazza etal., 1991), who mentioned applications in speechrecognition.
Further applications are found in com-putational biology, but their discussion is beyond thescope of this paper; see for instance (Apostolico etal., 2005) and references therein.
In order to solvethe problem, we only need a small addition to theprocedures we discussed before.
First we constructseparate automata Iwj (1 ?
j ?
m) as explained inSection 4.
These automata are then composed intoa single automaton I(w1,...,wm).
In this composition,the outgoing transitions of the final state of Iwj , foreach j (1 ?
j < m), are removed and that final stateis merged with the initial state of the next automatonIwj+1 .
The initial state of the composed automatonis the initial state of Iw1 , and the final state is thefinal state of Iwm .
The time costs of constructingI(w1,...,wm) are linear in the sum of the lengths of thestrings wj .Another way to generalize the problem is to re-place w by a finite set L = {w1, .
.
.
, wm}.
The ob-jective is to compute:?infix (L,G) =?w?L,x,y??
?p(xwy)Again, this can be solved by first constructing a de-terministic FA, which is then intersected with G.This FA can be obtained by determinizing a straight-forward nondeterministic FA accepting L, or by di-rectly constructing a deterministic FA along the linesof the Aho-Corasick algorithm (Aho and Corasick,1975).
Construction of the automaton with the latterapproach takes linear time.Further straightforward generalizations involveformalisms such as probabilistic tree adjoininggrammars (Schabes, 1992; Resnik, 1992).
The tech-nique from Section 3 is also applicable in this case,1218t0 t1 t2 t3 t4a b a cb, c a a, b, ccb, c abFigure 1: Deterministic automaton that accepts all strings over alphabet {a, b, c} with infix abac.as the construction from Bar-Hillel et al (1964) car-ries over from context-free grammars to tree ad-joining grammars, and more generally to the linearcontext-free rewriting systems of Vijay-Shanker etal.
(1987).7 ImplementationWe have conducted experiments with the computa-tion of infix probabilities.
The objective was to iden-tify parts of the computation that have a high timeor space demand, and that might be improved.
Theexperiments were run on a desktop with a 3.0 GHzPentium 4 processor.
The implementation languageis C++.The set-up of the experiments is similar to that in(Nederhof and Satta, 2008).
A probabilistic context-free grammar was extracted from sections 2-21 ofthe Penn Treebank version II.
Subtrees that gener-ated the empty string were systematically removed.The result was a CFG with 10,035 rules, 28 nonter-minals and 36 parts-of-speech.
The rule probabili-ties were determined by maximum likelihood esti-mation.
The grammar was subsequently binarized,to avoid exponential behaviour, as explained in Sec-tion 3.We have considered 10 strings of length 7, ran-domly generated, assuming each of the parts-of-speech has the same probability.
For all prefixes ofthose strings from length 2 to length 7, we then com-puted the infix probability.
The duration of the fullcomputation, averaged over the 10 strings of length7, is given in the first row of Table 1.In order to solve the non-linear systems of equa-tions, we used Broyden?s method.
It can be seenas an approximation of Newton?s method.
It re-quires more iterations, but seems to be faster over-all, and more scalable to large problem sizes, due tothe avoidance of matrix inversion, which sometimesmakes Newton?s method prohibitively expensive.
Inour experiments, Broyden?s method was generallyfaster than Newton?s method and much faster thanthe simple iteration method by the relation in (4).For further details on Broyden?s method, we referthe reader to (Kelley, 1995).The main obstacle to computation for infixes sub-stantially longer than 7 symbols is the memory con-sumption rather than the running time.
This is dueto the required square matrices, the dimension ofwhich is the number of nonterminals.
The numberof nonterminals (of the intersection grammar) natu-rally grows as the infix becomes longer.As explained in Section 2, the problem is dividedinto smaller problems by isolating disjoint sets ofmutually recursive nonterminals, or strongly con-nected components.
We found that for the applica-tion to the automata discussed in Section 4, therewere exactly three strongly connected componentsthat contained more than one element, throughoutthe experiments.
For an infix of length n, these com-ponents are:?
C1, which consists of nonterminals of the form(ti, A, tj), where i < n and j < n,?
C2, which consists of nonterminals of the form(ti, A, tj), where i = j = n, and?
C3, which consists of nonterminals of the form(ti, A, tj), where i < j = n.This can be easily explained by looking at the struc-ture of our automata.
See for example Figure 1, withcycles running through states t0, .
.
.
, tn?1, and cy-cles through state tn.
Furthermore, the grammar ex-tracted from the Penn Treebank is heavily recursive,1219infix length 2 3 4 5 6 7total running time 1.07 1.95 5.84 11.38 23.93 45.91Broyden?s method for C1 0.46 0.90 3.42 6.63 12.91 24.38Broyden?s method for C2 0.08 0.04 0.07 0.04 0.03 0.09Broyden?s method for C3 0.20 0.36 0.81 1.74 5.30 9.02Table 1: Running time for infixes from length 2 to length 7.
The infixes are prefixes of 10 random strings of length 7,and reported CPU times (in seconds) are averaged over the 10 strings.so that almost every nonterminal can directly or in-directly call any other.The strongly connected component C2 is alwaysthe same, consisting of 2402 nonterminals, for eachinfix of any length.
(Note that binarization of thegrammar introduced artificial nonterminals.)
Thelast three rows of Table 1 present the time costs ofBroyden?s method, for the three strongly connectedcomponents.The strongly connected componentC3 happens tocorrespond to a linear system of equations.
This isbecause a rule in the intersection grammar with aleft-hand side (ti, A, tj), where i < j = n, musthave a right-hand side of the form (ti, A?, tj), or ofthe form (ti, A1, tk) (tk, A2, tj), with k ?
n. If k <n, then only the second member can be in C3.
Ifk = n, only first member can be in C3.
Hence,such a rule corresponds to a linear equation withinthe system of equations for the entire grammar.A linear system of equations can be solved an-alytically, for example by Gaussian elimination,rather than approximated through Newton?s methodor Broyden?s method.
This means that the runningtimes in the last row of Table 1 can be reduced bytreating C3 differently from the other strongly con-nected components.
However, the running time forC1 dominates the total time consumption.The above investigations were motivated by twoquestions, namely whether any part of the computa-tion can be precomputed, and second, whether infixprobabilities can be computed incrementally, for in-fixes that are extended to the left or to the right.
Thefirst question can be answered affirmatively for C2,as it is always the same.
However, as we can see inTable 1, the computation of C2 amounts to a smallportion of the total time consumption.The second question can be rephrased more pre-cisely as follows.
Suppose we have computed theinfix probability of a string w and have kept inter-mediate results in memory.
Can the computation ofthe infix probability of a string of the form aw orwa,a ?
?, be computed by relying on the existing re-sults, so that the computation is substantially fasterthan if the computation were done from scratch?Our investigations so far have not found a posi-tive answer to this second question.
In particular,the systems of equations for C1 and C3 change fun-damentally if the infix is extended by one more sym-bol, which seems to at least make incremental com-putation very difficult, if not impossible.
Note thatthe algorithms for the computation of prefix prob-abilities by Jelinek and Lafferty (1991) and Stolcke(1995) do allow incrementality, which contributes totheir practical usefulness for speech recognition.8 ConclusionsWe have shown that the problem of infix probabili-ties for PCFGs can be solved by a construction thatintersects a context-free language with a regular lan-guage.
An important constraint is that the finiteautomaton that is input to this construction be un-ambiguous.
We have shown that such an automa-ton can be efficiently constructed.
Once the inputprobabilistic PCFG and the FA have been combinedinto a new probabilistic CFG, the infix probabilitycan be straightforwardly solved by iterative algo-rithms.
Such algorithms include Newton?s method,and Broyden?s method, which was used in our exper-iments.
Our discussion ended with an open questionabout the possibility of incremental computation ofinfix probabilities.ReferencesS.
Abney, D. McAllester, and F. Pereira.
1999.
Relatingprobabilistic grammars and automata.
In 37th Annual1220Meeting of the Association for Computational Linguis-tics, Proceedings of the Conference, pages 542?549,Maryland, USA, June.A.V.
Aho and M.J. Corasick.
1975.
Efficient stringmatching: an aid to bibliographic search.
Communi-cations of the ACM, 18(6):333?340, June.A.V.
Aho and J.D.
Ullman.
1972.
Parsing, volume 1of The Theory of Parsing, Translation and Compiling.Prentice-Hall, Englewood Cliffs, N.J.A.
Apostolico, M. Comin, and L. Parida.
2005.
Con-servative extraction of overrepresented extensible mo-tifs.
In Proceedings of Intelligent Systems for Molecu-lar Biology (ISMB05).Y.
Bar-Hillel, M. Perles, and E. Shamir.
1964.
On formalproperties of simple phrase structure grammars.
InY.
Bar-Hillel, editor, Language and Information: Se-lected Essays on their Theory and Application, chap-ter 9, pages 116?150.
Addison-Wesley, Reading, Mas-sachusetts.T.L.
Booth and R.A. Thompson.
1973.
Applying prob-abilistic measures to abstract languages.
IEEE Trans-actions on Computers, C-22:442?450.Z.
Chi and S. Geman.
1998.
Estimation of probabilis-tic context-free grammars.
Computational Linguistics,24(2):299?305.Z.
Chi.
1999.
Statistical properties of probabilisticcontext-free grammars.
Computational Linguistics,25(1):131?160.A.
Corazza, R. De Mori, R. Gretter, and G. Satta.1991.
Computation of probabilities for an island-driven parser.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 13(9):936?950.K.
Etessami and M. Yannakakis.
2005.
RecursiveMarkov chains, stochastic grammars, and monotonesystems of nonlinear equations.
In 22nd InternationalSymposium on Theoretical Aspects of Computer Sci-ence, volume 3404 of Lecture Notes in Computer Sci-ence, pages 340?352, Stuttgart, Germany.
Springer-Verlag.K.
Etessami and M. Yannakakis.
2009.
RecursiveMarkov chains, stochastic grammars, and monotonesystems of nonlinear equations.
Journal of the ACM,56(1):1?66.A.L.N.
Fred.
2000.
Computation of substring proba-bilities in stochastic grammars.
In A. Oliveira, edi-tor, Grammatical Inference: Algorithms and Applica-tions, volume 1891 of Lecture Notes in Artificial Intel-ligence, pages 103?114.
Springer-Verlag.D.
Gusfield.
1997.
Algorithms on Strings, Trees, andSequences.
Cambridge University Press, Cambridge.T.E.
Harris.
1963.
The Theory of Branching Processes.Springer-Verlag, Berlin, Germany.F.
Jelinek and J.D.
Lafferty.
1991.
Computation of theprobability of initial substring generation by stochas-tic context-free grammars.
Computational Linguistics,17(3):315?323.C.T.
Kelley.
1995.
Iterative Methods for Linear andNonlinear Equations.
Society for Industrial and Ap-plied Mathematics, Philadelphia, PA.S.
Kiefer, M. Luttenberger, and J. Esparza.
2007.
On theconvergence of Newton?s method for monotone sys-tems of polynomial equations.
In Proceedings of the39th ACM Symposium on Theory of Computing, pages217?266.D.E.
Knuth, J.H.
Morris, Jr., and V.R.
Pratt.
1977.
Fastpattern matching in strings.
SIAM Journal on Comput-ing, 6:323?350.M.-J.
Nederhof and G. Satta.
2003.
Probabilistic pars-ing as intersection.
In 8th International Workshop onParsing Technologies, pages 137?148, LORIA, Nancy,France, April.M.-J.
Nederhof and G. Satta.
2008.
Computing parti-tion functions of PCFGs.
Research on Language andComputation, 6(2):139?162.E.
Persoon and K.S.
Fu.
1975.
Sequential classificationof strings generated by SCFG?s.
International Journalof Computer and Information Sciences, 4(3):205?217.P.
Resnik.
1992.
Probabilistic tree-adjoining grammar asa framework for statistical natural language process-ing.
In Proc.
of the fifteenth International Conferenceon Computational Linguistics, Nantes, August, pages418?424.J.-A.
Sa?nchez and J.-M.
Bened??.
1997.
Consistencyof stochastic context-free grammars from probabilis-tic estimation based on growth transformations.
IEEETransactions on Pattern Analysis and Machine Intelli-gence, 19(9):1052?1055, September.Y.
Schabes.
1992.
Stochastic lexicalized tree-adjoininggrammars.
In Proc.
of the fifteenth International Con-ference on Computational Linguistics, Nantes, Au-gust, pages 426?432.A.
Stolcke.
1995.
An efficient probabilistic context-freeparsing algorithm that computes prefix probabilities.Computational Linguistics, 21(2):167?201.K.
Vijay-Shanker, D.J.
Weir, and A.K.
Joshi.
1987.Characterizing structural descriptions produced byvarious grammatical formalisms.
In 25th AnnualMeeting of the Association for Computational Linguis-tics, Proceedings of the Conference, pages 104?111,Stanford, California, USA, July.D.
Wojtczak and K. Etessami.
2007.
PReMo: an an-alyzer for Probabilistic Recursive Models.
In Toolsand Algorithms for the Construction and Analysis ofSystems, 13th International Conference, volume 4424of Lecture Notes in Computer Science, pages 66?71,Braga, Portugal.
Springer-Verlag.1221
