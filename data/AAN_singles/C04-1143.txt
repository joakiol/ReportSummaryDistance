FASIL Email Summarisation SystemAngelo Dalli, Yunqing Xia, Yorick WilksNLP Research GroupDepartment of Computer ScienceUniversity of Sheffield{a.dalli, y.xia, y.wilks}@dcs.shef.ac.ukAbstractEmail summarisation presents a unique set ofrequirements that are different from generaltext summarisation.
This work describes theimplementation of an email summarisationsystem for use in a voice-based Virtual Per-sonal Assistant developed for the EU FASiLProject.
Evaluation results from the first inte-grated version of the project are presented.1 IntroductionEmail is one of the most ubiquitous applications used ona daily basis by millions of people world-wide, tradi-tionally accessed over a fixed terminal or laptop com-puter.
In the past years there has been an increasingdemand for email access over mobile phones.
Our workhas focused on creating an email summarisation servicethat provides quality summaries adaptively and quicklyenough to cater for the tight constrains imposed by areal time text-to-speech system.This work has been done as part of the EuropeanUnion FASiL project, which aims to aims to construct aconversationally intelligent Virtual Personal Assistant(VPA) designed to manage the user?s personal andbusiness information through a voice-based interfaceaccessible over mobile phones.As the quality of life and productivity is to improvedin an increasingly information dominated society, peo-ple need access to information anywhere, anytime.
TheAdaptive Information Management (AIM) service in theFASiL VPA seeks to automatically prioritise and pre-sent information that is most pertinent to the mobileusers and adapt to different user preferences.
The AIMservice is comprised of three main parts: an email sum-mariser, email categoriser, calendar scheduling/PIMinteraction and an adaptive prioritisation service thatoptimizes the sequence in which information is pre-sented, keeping the overall duration of the voice-baseddialogue to a minimum.2 Email CharacteristicsEmail Summarisation techniques share many character-istics with general text summarisation techniques whilecatering for the unique characteristics of email:1. short messages usually between 2 to 800words in length (after thread-filtering)2. frequently do not obey grammatical or con-ventional stylistic conventions3.
are a cross between informal mobile text orchat styles and traditional writing formats4.
display unique thread characteristics with 87%containing three previous emails or less(Fisher and Moody, 2001)All these four main characteristics combined to-gether mean that most document summarisation tech-niques simply do not work well for email.
The voice-based system also required that summaries be producedon demand, with only a short pause allowed for thesummariser to output a result ?
typically a maximum ofaround 1 second per email.Another main constraint imposed in the FASiL VPAwas the presence of two integer parameters ?
the pre-ferred and maximum length of the summary.
Themaximum length constraint had to be obeyed strictly,while striving to fit in the summary into the preferredlength.
These performance and size constraints, coupledwith the four characteristics of email largely determinedthe design of the FASiL Email Summariser.2.1 Short MessagesEmail is a form of short, largely informal, written com-munication that excludes methods that need largeamounts of words and phrases to work well.The main disadvantage is that sometimes the usefulcontent of a whole email message is simply a one wordin case of a yes/no answer to a question or request.
Thesummariser exploits this characteristic by filtering outthreads and other commonly repeated text at the bottomof the email text such as standard email text signatures.If the resulting text is very short and falls within thepreferred length of the summary, the message can beoutput in its entirety to users.
The short messages alsomake it easier to achieve relevancy in the summaries.Inadvertently context is sometimes lost in the sum-mary due to replies occurring in threaded emails.
Also,emails containing lots of question-answer pairs can getsummarised poorly due to the fixed amount of spaceavailable for the summary.2.2 Stylistic Conventions and GrammarEmail messages often do not follow formal stylisticconventions and are may have a substantial level ofspelling mistakes, abbreviations and other features thatmake text analysis difficult.A simple spellchecker using approximate stringmatching and word frequency/occurrence statistics wasused to match misspelled names automatically.Another problem that was encountered was the iden-tification of sentence boundaries, since more than 10%of the emails seen by the summariser frequently hadmissing punctuation and spurious line breaks insertedby various different email programs.
A set of hand-coded heuristics managed to produce acceptable results,identifying sentence boundaries correctly more than90% of the time.2.3 Informal and Formal StylesEmail can often be classified into three categories: in-formal short messages ?
often sent to people whom aredirectly known or with whom there has been a pro-longed discussion or interaction about a subject, mixedformal/informal emails sent to strangers or when re-questing information or replying to questions, and for-mal emails that are generally electronic versions offormal letter writing.The class of emails that cause most problems forsummarisation purposes are the first two classes of e-mails.
One of the main determining factors for the styleadopted by people in replying to emails is the amount oftime that lapses between replies.
Generally email getsmore formal as the time span between replies increases.Informal email can also be recognised by excessiveuse of anaphora that need to be resolved properly beforesummarisation can take place.
The summariser thus hasan anaphora resolver that is capable of resolving ana-phoric references robustly.Linguistic theory indicates that as the formality of atext increases, the number of words in the deictic cate-gory will decrease as the number of words in the non-deictic category increase (and vice-versa).
Deictic (oranaphoric) word classes include words that have vari-able meaning whose meaning needs to be resolvedthrough the surrounding (usually preceding) context.Non-deictic word classes are those words whose mean-ing is largely context-independent, analogous to predi-cates in formal logic.2.4 Threaded EmailsMany emails are composed by replying to an originalemail, often including part or whole of the originalemail together with new content, thus creating a threador chain of emails.
The first email in the thread willpotentially be repeated many times over, which mightmislead the summarisation process.
A thread-detectionfiltering tool is used to eliminate unoriginal content inthe email by comparing the contents of the current emailwith the content of previous emails.
A study of over 57user?s incoming and outgoing emails found that around30% of all emails are threaded.
Around 56% of thethreaded emails contained only one previous email ?
i.e.a request and reply, and 87% of all emails containedonly three previous emails apart from the reply (Fisherand Moody, 2001).Some reply styles also pose a problem when com-bined with threads.
Emails containing a list of questionsor requests for comments are often edited by the reply-ing party and answers inserted directly inside the text ofthe original request, as illustrated in Figure 1.> ?
now coming back to the issue> of whether to include support for> location names in the recogniser> I think that we should include> this ?
your opinions appreciated.I agree with this.Figure 1 Sample Embedded AnswerFigure 1 illustrates the main two difficulties faced bythe summariser in this situation.
While the threadedcontent from the previous reply should be filtered out toidentify the reply, the reply on its own is meaninglesswithout any form of context.
The summariser tries toovercome this by identifying this style of embeddedresponses when the original content is split into chunksor is only partially included in the reply.
The text fallingbefore the answer is then treated as part of the reply.Although this strategy gives acceptable results in somecases, more research is needed into finding the optimalstrategy to extract the right amount of context from thethread without either destroying the context or copyingtoo much from the original request back into the sum-mary.3 Summarisation TechniquesVarious summarisation techniques were considered inthe design of the FASiL email summariser.
Few opera-tional email-specific summarisation systems exist, sothe emphasis was on extracting the best-of-breed tech-niques from document summarisation systems that areapplicable to email summarisation.3.1 Previous WorkMany single-document summarisation systems can besplit according to whether they are extractive or non-extractive systems.
Extractive systems generate summa-ries by extracting selected segments from the originaldocument that are deemed to be most relevant.
Non-extractive systems try to build an abstract representationmodel and re-generate the summary using this modeland words found in the original document.Previous related work on extractive systems in-cluded the use of semantic tagging and co-reference/lexical  chains (Saggion et al, 2003; Barzilayand Elhadad, 1997; Azzam et al, 1998), lexical occur-rence/structural statistics (Mathis et al, 1973), discoursestructure (Marcu, 1998), cue phrases (Luhn, 1958;Paice, 1990; Rau et al, 1994), positional indicators(Edmunson, 1964) and other extraction methods (Kui-pec et al, 1995).Non-extractive systems are less common ?
previousrelated work included reformulation of extracted models(McKeown et al, 1999), gist extraction (Berger andMittal, 2000), machine translation-like approaches(Witbrock and Mittal, 1999) and generative models (DeJong, 1982; Radev and McKeown, 1998; Fum et al,1986; Reihmer and Hahn, 1988; Rau et al,  1989).A sentence-extraction system was decided for theFASiL summariser, with the capability to have phrase-level extraction in the future.
Non-extractive systemswere not likely to work as robustly and give the highquality results needed by the VPA to work as required.Another advantage that extractive systems still pose isthat in general they are more applicable to a wider rangeof arbitrary domains and are more reliable than non-extractive systems (Teufel, 2003).The FASiL summariser uses named entities as anindication of the importance of every sentence, and per-forms anaphora resolution automatically.
Sentences areselected according to named entity density and also ac-cording to their positional ranking.3.2 Summariser ArchitectureThe FASiL Summariser works in conjunction with anumber of different components to present real-timevoice-based summaries to users.
Figure 2 shows theoverall architecture of the summariser and its place inthe FASiL VPA.Figure 2 Summariser and VPA ArchitectureAn XML-based protocol is used to communicatewith the Dialogue Manager enabling the system to beloosely coupled but to have high cohesion (Sommer-ville, 1992).3.3 Named Entity RecognitionOne of the most important components in the FASiLSummariser is the Named Entity Recogniser (NER)system.The NER uses a very efficient trie-like structure tomatch sub-parts of every name (Gusfield, 1997;Stephen, 1994).
An efficient implementation enables theNER to confirm or reject a word as being a named en-tity or not in O(n) time.
Named entities are automati-cally classified according to the following list of 11classes:?
Male proper names (M)?
Female proper names (F)?
Places (towns, cities, etc.)
(P)?
Locations (upstairs, boardroom, etc.)
(L)?
Male titles (Mr., Esq., etc.)
(Mt)?
Female titles (Ms., Mrs., etc.)
(Ft)?
Generic titles (t)?
Date and time references (TIME)?
Male anaphors (Ma)?
Female anaphors (Fa)?
Indeterminate anaphors (a)The gazetteer list for Locations, Titles, and Ana-phors were compiled manually.
Date and time refer-ences were compiled from data supplied in the IBMInternational Components for Unicode (ICU) project(Davis, 2003).
Place names were extracted from dataavailable online from the U.S. Geological Survey Geo-graphic Names Information System and the GEOnetNames Server (GNS) of the U.S. National Imagery andMapping Agency (USGS, 2003; NIMA, 2003).An innovative approach to gathering names for themale and female names was adopted using a small cus-tom-built information extraction system that crawledInternet pages to identify likely proper names in thetexts.
Additional hints were provided by the presence ofanaphora in the same sentence or the following sentenceas the suspected proper name.
The gender of every titleand anaphora was manually noted and this informationwas used to keep a count of the number of male or fe-male titles and anaphors associated with a particularname.
This information enabled the list of names to beorganised by gender, enabling a rough probability to beassigned to suspect words (Azzam et al, 1998; Mitkov,2002).An Internet-based method that verified the list andfiltered out likely spelling mistakes and non-existentnames was then applied to this list, filtering out incor-rectly spelt names and other features such as online chatnicknames (Dalli, 2004).A list of over 592,000 proper names was thus ob-tained by this method with around 284,000 names beingidentified as male and 308,000 names identified as fe-male.
The large size of this list contributed significantlyto the NER?s resulting accuracy and compares favoura-bly with previously compiled lists (Stevenson and Gai-zauskas, 2000).3.4 Anaphora ResolutionExtracting systems suffer from the problem of danglinganaphora in summaries.
Anaphora resolution is an effec-tive way of reducing the incoherence in resulting sum-maries by replacing anaphors with references to theappropriate named entities (Mitkov, 2002).
This substi-tution has the direct effect of making the text less con-text sensitive and implicitly increases the formality ofthe text.Cohesion problems due to semantic discontinuitieswhere concepts and agents are not introduced are alsopartially solved by placing emphasis on named entitiesand performing anaphora resolution.
The major cohe-sion problem that still has not been fully addressed isthe coherence of various events mentioned in the text.The anaphora resolver is aided by the gender-categorised named entity classes, enabling it to performbetter resolution over a wide variety of names.
A simplelinear model is adopted, where the system focusesmainly on nominal and clausal antecedents (Cristea etal., 2000).
The search scope for candidate antecedents isset to the current sentence together with the three pre-ceding sentences as suggested in (Mitkov, 1998) as em-pirical studies show that more than 85% of all cases arehandled correctly with this window size (Mitkov, 2002).Candidate antecedents being discarded after ten sen-tences have been processed without the presence ofanaphora as suggested in (Kameyama, 1997).3.5 Sentence RankingAfter named entity recognition and anaphora resolution,the summariser ranks the various sentences/phrases thatit identifies and selects the best sentences to extract andput in the summary.
The summariser takes two parame-ters apart from the email text itself: a preferred lengthand a maximum length.
Typical lengths are 160 charac-ters preferred with 640 characters maximum, whichcompares to the size a mobile text message.Ranking takes into account three parameters: namedentity density and importance of every class, sentenceposition and the preferred and maximum length parame-ters.0123456781 3 5 7 9 11 13 15 17 19Number of SentencesWeightSeries1 Series2 Series3Series4 Series5Figure 3 Positional sentence weight for varyingsummarisation parametersPositional importance was found to be significant inemail text since relevant information was often found tobe in the first few sentences of the email.Figure 3 shows how the quadratic positional weightfunction ?
changes with position, giving less importanceto sentences as they occur further from the start (al-though the weight is always bigger than zero).
Differentkinds of emails were used to calibrate the weight func-tion.
Series 1 (bottom) represents a typical mobile textmessage length summary with a very long message.Series 4 and 5 (middle) represent the weight functionbehaviour when the summary maximum length is long(approximately more than 1,000 characters), irrelevantof the email message length itself.
Series 2 and 3 (top)represent email messages that fall within the maximumlength constraints.The following ranking function rank(j), where j isthe sentence number, is used to rank and select excerpts:( ) ( )( ) ( ) ( )( )++?= ?=?
???
?0,1ic iijjrank( )?
?
( )????????
??+???????
?+ ?????
jlengthjj1maxwhere ?
and ?
are empirically determined constants,?
is the preferred summary length, and jmax is the num-ber of sentences in the email.
The NER function ?crepresents the number of words of type i in sentence jand ?
(i) gives the weight associated with that type.
Inour case ?
equals 10 since there are 11 named entityclasses.
The NER weights ?
(i) for every class havebeen empirically determined and optimized.
A thirdparameter ?
is used to change the values of ?
and ?
ac-cording to the maximum and preferred lengths togetherwith the email length as shown in Figure 3.The first term handles named entity density, the sec-ond the sentence position and the third biases the rank-ing towards the preferred length.
The sentences are thensorted in rank order and the preferred and maximumlengths used to determine which sentences to return inthe summary.4 Experimental ResultsThe summariser results quality was evaluated againstmanually produced summaries using precision and re-call, together with a more useful utility-based evaluationthat uses a fractional model to cater for varying degreesof importance for different sentences.4.1 Named Entity Recognition PerformanceThe performance of the summariser depends signifi-cantly on the performance of the NER.
Speed tests showthat the NER consistently processes more than 1 millionwps on a 1.6 GHz machine while keeping resource us-age to a manageable 300-400 Mb of memory.Precision and recall curves were calculated for 100emails chosen at random, separated into 10 randomsample groups from representative subsets of the threemain types of emails ?
short, normal and long emails asexplained previously.
The samples were manuallymarked according to the 11 different named entityclasses recognised by the NER to act as a comparativestandard for relevant results.
Figures 4 and 5 respec-tively show the NER precision and recall results.00.20.40.60.811 2 3 4 5 6 7 8 9 10Sample GroupPrecisionM F P LMt Ft t TIMEMa Fa aFigure 4 Precision by Named Entity ClassIt is interesting to note that the NER performedworst at anaphora identification with an average preci-sion of 77.5% for anaphora but 96.7% for the rest of thenamed entity classes.00.20.40.60.811 2 3 4 5 6 7 8 9 10Sample GroupRecallM F P LMt Ft t TIMEMa Fa aFigure 5 Recall by Named Entity ClassFigure 6 shows the average precision and recall av-eraged across all the eleven types of named entityclasses, for the 10 sample email groups.
An averageprecision of 93% was achieved throughout, with 97%recall.00.250.50.7511 2 3 4 5 6 7 8 9 10Sample GroupValueRecall PrecisionFigure 6 Average Precision and RecallIt is interesting to note that the precision and recallcurves do not exhibit the commonly observed inversetrade-off relationship between precision and recall(Buckland and Gey, 1994; Alvarez, 2002).
This result isexplained by the fact that the NER, in this case, canactually identify most named entities in the text withhigh precision while neither over-selecting irrelevantresults nor under-selecting relevant results.4.2 Summariser Results QualityQuality evaluation was performed by selecting 150emails at random and splitting the emails up into 15groups of 10 emails at random to facilitate multiple per-son evaluation.
Each sentence in every email was thenmanually ranked using a scale of 1 to 10.
For recall andprecision calculation, any sentence ranked ?
5 was de-fined as relevant.
Figure 7 shows the precision and re-call values with 74% average precision and 71% aver-age recall.00.511.51 2 3 4 5 6 7 8 9 10 11 12 13 14 15Sample GroupValueRecall PrecisionFigure 7 Summaries Recall and PrecisionA utility-based evaluation was also used to obtainmore intuitive results than those given by precision andrecall using the methods reported in (Jing et al, 1998;Goldstein et al, 1999; Radev et al, 2000).
The averagescore of each summary was compared to the averagescore over infinity expected to be obtained by extractinga combination of the first [1..N] sentences at random.The summary average score was also compared to thescore obtained by an averaged pool of 3 human judges.Figure 8 shows a comparison between the summariserperformance and human performance, with the summar-iser averaging at 86.5% of the human performance,ranging from 60% agreement to 100% agreement withthe gold standard.00.511.521 2 3 4 5 6 7 8 9 10 11 12 13 14 15Sample GroupValueSummariser Utility Gold Standard UtilityFigure 8 Utility Score ComparisonIn Figure 8 a random extraction system is expectedto get a score of 1 averaged across an infinite amount ofruns.
The average sentence compression factor for thesummariser was 42%, exactly the same as the humanjudges?
results.
The selected emails had an averagelength of 14 sentences, varying from 7 to 27 sentences.5 Conclusion and Future WorkThe FASiL Email Summarisation system represents acompact summarisation system optimised for emailsummarisation in a voice-based system context.The excellent performance in both speed and accu-racy of the NER component makes it ideal for re-use inprojects that need high quality real-time identificationand classification of named entities.A future improvement will incorporate a fast POSanalyser to enable phrase-level extraction to take placewhile improving syntactic coherence.
An additionalimprovement will be the incorporation of co-referencechain methods to verify email subject lines and in somecases suggest more appropriate subject lines.The FASiL summariser validates the suitability ofthe combined sentence position and NER-driven ap-proach towards email summarisation with encouragingresults obtained.AcknowledgmentsThis research is funded under the EU FASiL Project, anEU grant in Human Language Technology (IST-2001-38685) (Website: www.fasil.co.uk).ReferencesAlvarez, S. 2002.
?An exact analytical relation amongrecall, precision, and classification accuracy in in-formation retrieval.?
Boston College, Boston, Tech-nical Report BCCS-02-01.Azzam, S., Humphreys, K. and Gaizauskas, R.
1998.?Coreference resolution in a multilingual informationextraction?, Proc.
Workshop on Linguistic Corefer-ence.
Granada, Spain.Barzilay, R. Elhadad, M. 1997.
?Using Lexical Chainsfor Text Summarization.
?, Proc.
ACL Workshop onIntelligent Scaleable Text Summarization, Madrid,Spain.
10-17.Berger, L. Mittal, V. 2000.
?OCELOT: A system forsummarizing web pages?.
Carnegie Mellon Univer-sity.
Just Research.
Pittsburgh, Pennsylvania.Buckland, M. Gey, F. 1994.
?The relationship betweenrecall and precision.?
J. American Society for Infor-mation Science, 45(1):12-19.Cristea, D., Ide, N., Marcu, D., Tablan, V. 2000.
?Anempirical investigation of the relation between dis-course structure and coreference.
?, Proc.
19th Int.Conf.
on Comp.
Linguistics (COLING-2000), Saar-br?cken, Germany.
208-214.Dalli, A.
2004.
?An Internet-based method for Verifica-tion of Extracted Proper Names?.
CICLING-2004.David, C. 2003.
Information Society Statistics: PCs,Internet and mobile phone usage in the EU.
Euro-pean Community, Report KS-NP-03-015-EN-N.Davis, M. 2003.
?An ICU overview?.
Proc.
24th UnicodeConference, Atlanta.
IBM Corporation, California.De Jong, G. 1982.
?An overview of the FRUMP sys-tem.
?, in: Lehnert and Ringle eds., Strategies forNatural Language Processing, Lawrence ErlbaumAssociates, Hillsdale, New Jersey.
149-176.Edmunson, H.P.
1964.
?Problems in automatic extract-ing.
?, Comm.
ACM, 7, 259-263.Fisher, D., Moody, P. 2001.
Studies of Automated Col-lection of Email Records.
University of California,Irvine, Technical Report UCI-ISR-02-4.Fum, D. Guida, G. Tasso, C. 1986.
?Tailoring impor-tance evaluation to reader?s goals: a contribution todescriptive text summarization.?
Proc.
COLING-86,256-259.Goldstein, J. Kantrowitz, M. Mittal, V. Carbonell,Jaime.
1999.
?Summarizing Text Documents: Sen-tence Selection and Evaluation Metrics?, Proc.
ACM-SIGIR 1999, Berkeley, California.Gusfield, D.  1997.
Algorithms on Strings, Trees andSequences.
Cambridge University Press, Cambridge,UK.Halliday, M.A.K.
1985.
Spoken and written language.Oxford University Press, Oxford.Jing, H. Barzilay, R. McKeown, K. Elhadad, M.
1998.?Summarization Evaluation Methods: Experimentsand Analysis?, AAAI Spring Symposium on IntelligentText Summarisation, Stanford, California.Kameyama, M. 1997.
?Recognising referential links: aninformation extraction perspective.
?, Proc.
EACL-97Workshop on Operational Factors in Practical, Ro-bust, Anaphora Resolution, Madrid, Spain.
46-53.Kuipec, J. Pedersen, J. Chen, F. 1995.
?A TrainableDocument Summarizer.
?, Proc.
18th ACM SIGIRConference, Seattle, Washington.
68-73.Luhn, P.H.
1958.
?Automatic creation of literature ab-stracts?.
IBM J.
159-165.Marcu, D. 1998.
?To Build Text Summaries of HighQuality, Nuclearity is not Sufficient.?
Proc.
AAAISymposium on Intelligent Text Summarisation, Stan-ford University, Stanford, California.
1-8.Mathis, B.A.
Rush, J.E.
Young, C.E.
1973.
?Improve-ment of automatic abstracts by the use of structuralanalysis.
?, J. American Society for Information Sci-ence, 24, 101-109.McKeown, K. Klavens, J. Hatzivassiloglou, V. Barzi-lay, R. Eskin, E. 1999.
?Towards MultidocumentSummarization by Reformulation: Progress andProspects.
?, AAAI Symposium on Intelligent TextSummarisation.Mitkov, R. 1998.
?Robust pronoun resolution with lim-ited knowledge.
?, Proc.
17th International Confer-ence on Comp.
Linguistics (COLING-1998),Montreal, Canada.
869-875.Mitkov, R. 2002.
Anaphora Resolution.
London, Long-man.National Imagery and Mapping Agency (NIMA).
2003.GEOnet Names Server (GNS).Paice, C. 1990.
?Constructing literature abstracts bycomputer: techniques and prospects.
?, InformationProcessing and Management, 26:171-186.Radev, D. McKeown, K. 1998.
?Generating NaturalLanguage Summaries from Multiple On-LineSources.
?, Computational Linguistics, 24(3):469-500.Radev, D. Jing, H. Budzikowska, M. 2000.
?Centroid-based summarization of multiple documents: sen-tence extraction, utility-based evaluation, user stud-ies.?
in Automatic Summarisation: ANLP/NAACL2000 Workshop, New Brunswick, New Jersey.Rau, L. Jacobs, P. Zernick, U.
1989.
?Information ex-traction and text summarization using linguisticknowledge acquisition.
?, Information Processing andManagement, 25(4):419-428.Rau, L. Brandow, R. Mitze, K. 1994.
?Domain-Independent Summarization of News.
?, in: Summa-rizing Text for Intelligent Communication, Dagstuhl,Germany.
71-75.Reimer, U. Hahn, U.
1988.
?Text condensation asknowledge base abstraction.?
Proc.
4th Conference onArtificial Intelligence Applications.
338-344.Saggion, H. Bontcheva, K. Cunningham, H. 2003.
?Ro-bust Generic and Query-based Summarisation?.
Proc.EACL-2003, Budapest.Sommerville, I.
1992.
Software Engineering.
4th ed.Addison-Wesley.Stephen, Graham A.
1994.
String Searching Algorithms.World Scientific Publishing, Bangor, Gwynedd, UK.Stevenson, M. Gaizauskas, R. 2000.
?Using Corpus-derived Name Lists for Named Entity Recognition,Proc.
ANLP-2000, Seattle.Teufel, S. 2003.
?Information Retrieval: AutomaticSummarisation?, University of Cambridge.
24-25.Witbrock, M. Mittal, V. 1999.
?Ultra Summarization: AStatistical Approach to Generating Non-ExtractiveSummaries.
?, Just Research, Pittsburgh.United States Geological Survey (USGS).
2003.
Geo-graphic Names Information System (GNIS).http://geonames.usgs.gov/
