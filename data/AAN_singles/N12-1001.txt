2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1?10,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsMultiple Narrative Disentanglement: Unraveling Infinite JestByron C. WallaceTufts University and Tufts Medical CenterBoston, MAbyron.wallace@gmail.comAbstractMany works (of both fiction and non-fiction)span multiple, intersecting narratives, each ofwhich constitutes a story in its own right.
Inthis work I introduce the task of multiple nar-rative disentanglement (MND), in which theaim is to tease these narratives apart by assign-ing passages from a text to the sub-narrativesto which they belong.
The motivating exam-ple I use is David Foster Wallace?s fictionaltext Infinite Jest.
I selected this book becauseit contains multiple, interweaving narrativeswithin its sprawling 1,000-plus pages.
I pro-pose and evaluate a novel unsupervised ap-proach to MND that is motivated by the theoryof narratology.
This method achieves strongempirical results, successfully disentanglingthe threads in Infinite Jest and significantlyoutperforming baseline strategies in doing so.1 IntroductionBoth fictional and non-fictional texts often com-prise multiple, intersecting and inter-related narra-tive arcs.
This work considers the task of identifyingthe (sub-)narratives latent within a narrative text andthe set of passages that comprise them.
As a mo-tivating example, I consider David Foster Wallace?sopus Infinite Jest (Wallace, 1996),1 which containsseveral disparate sub-narratives interleaved through-out its voluminous (meta-)story.
By sub-narrativeI mean, loosely, that these threads constitute theirown independent stories, coherent on their own (i.e.,1No relation.without the broader context of the overarching narra-tive).
I refer to the task of identifying these indepen-dent threads and untangling them from one anotheras multiple narrative disentanglement (MND).The task is of theoretical interest because disen-tanglement is a necessary pre-requisite to makingsense of narrative texts, an interesting direction inNLP that has received an increasing amount of atten-tion (Elson et al, 2010; Elson and McKeown, 2010;Celikyilmaz et al, 2010; Chambers and Jurafsky,2008; Chambers and Jurafsky, 2009).
Recogniz-ing the (main) narrative threads comprising a workprovides a context for interpreting the text.
Disen-tanglement may thus be viewed as the first step ina literary processing ?pipeline?.
Identifying threadsand assigning them to passages may help in auto-matic plot summarization, social network construc-tion and other literary analysis tasks.
Computationalapproaches to literature look to make narrative senseof unstructured text, i.e., construct models that relatecharacters and events chronologically: disentangle-ment is at the heart of this re-construction.But MND is also potentially of more pragmaticimport: disentanglement may be useful for identify-ing and extracting disparate threads in, e.g., a news-magazine article that covers multiple (related) sto-ries.2 Consider an article covering a political race.It would likely contain multiple sub-narratives (thestory of one candidate?s rise and fall, a scandal in apolitical party, etc.)
that may be of interest indepen-dently of the particular race at hand.
Narrative dis-2While narrative colloquially tends to refer to fictional texts,the narrative voice is also frequently used in non-fictional con-texts (Bal, 1997).1entanglement thus has applications outside of com-putational methods for fiction.In this work, I treat MND as an unsupervisedlearning task.
Given a block of narrative text, theaim is to identify the top k sub-narratives therein,and then to extract the passages comprising them.The proposed task is similar in spirit to the prob-lem of chat disentanglement (Elsner and Charniak,2010), in which the aim is to assign each utterance ina chat transcription to an associated conversationalthread.
Indeed, the main objective is the same: dis-entangle fragments of a monolithic text into chrono-logically ordered, independently coherent ?threads?.Despite their similarities, however, narrative disen-tanglement is a qualitatively different task than chatdisentanglement, as I highlight in Section 3.I take inspiration from the literary community,which has studied the theoretical underpinnings ofthe narrative form at length (Prince, 1982; Prince,2003; Abbott, 2008).
I rely especially on the seminalwork of Bal (1997), Narratology, which providesa comprehensive theoretical framework for treatingnarratives.
This narratological theory motivates mystrategy of narrative modeling, in which I first ex-tract the entities in each passage of a text.
I thenuncover the latent narrative compositions of thesepassages by performing latent Dirichlet alocation(LDA) (Blei et al, 2003) over the extracted entities.The main contributions of this work are as fol-lows.
First, I introduce the task of multiple narrativedisentanglement (MND).
Second, motivated by thetheory of narratology (Section 2) I propose a novel,unsupervised method for this task (Section 5) anddemonstrate its superiority over baseline strategiesempirically (Section 6).
Finally, I make available acorpus for this task: the text of Infinite Jest manuallyannotated with narrative tags (Section 4).2 NarratologyI now introduce some useful definitions and con-cepts (Table 1) central to the theory of narratology(Bal, 1997).
These constructs motivate my approachto the task of disentanglement.These definitions imply that the observed narra-tive text has been generated with respect to somenumber of latent fabulas.
A story is a particulartelling of an underlying fabula, i.e., a sequence ofActor an agent that performs actions.
Ac-tors are not necessarily persons.Fabula a series of logically and chronolog-ically related events that are causedor experienced by actors.Story an instantiation of a fabula, told ina particular style (a story tells a fab-ula).
Stories are not necessarily toldin chronological order.Focalizer a special actor from whose point ofview the story is told.Table 1: A small glossary of narratology.events involving actors.
Figure 1 schematizes therelationships between the above constructs.
Thedotted line between author and fabula implies thatauthors sometimes generate the fabula, sometimesnot.
In particular, an author may re-tell a widelyknown fabula (e.g., Hamlet); perhaps from a dif-ferent perspective.
Consider, for example, the playRosencrantz and Guildenstern are Dead (Stoppard,1967), a narrative that re-tells the fabula of Hamletfrom the perspective of the titular characters (bothof whom play a minor part in Hamlet itself).
Froma narratological view, this story is an instantiation ofthe Hamlet fabula imbued with novel aspects (e.g.,the focalizers in this telling are Rosencrantz andGuildenstern, rather than Hamlet).
In non-fictionalworks the fabula corresponds to the actual event se-quence as it happened, and thus is not invented bythe author (save for cases of outright fabrication).Fabulas are essentially actor-driven.
Further, ac-tors tend to occupy particular places, and indeed Bal(1997) highlights locations as one of the defining el-ements of fabulas.
Given these observations, it thusseems fruitful to attempt to identify the agents andlocations (or entities) in each passage of a text as afirst step toward disentanglement.
I will return tothis intuition when I present the narrative modelingmethod in Section 5.
First, I place the present workin context by relating it to existing work on miningliterature and chat disentanglement.3 Relationship to Existing WorkMost similar to MND is the task of chat disentan-glement (Shen et al, 2006; Elsner and Charniak,2010; Elsner and Charniak, 2011), wherein utter-ances (perhaps overheard at a cocktail party) are to2FabulaStorySymbols(e.g., text)AuthorFigure 1: A schematic of the narratology theory.
Thedotted line between author and fabula implies that whengenerating a narrative text, an author may invent a fabula,or may draw upon an existing one.
Together, the authorand fabula jointly give rise to the story, which is commu-nicated via the text.be assigned to conversational threads.
There are,however, important differences between these twotasks.
Notably, utterances in a chat belong to a singlediscussion thread, motivating ?hard?
assignments ofutterances to threads, e.g., using graph-partitioning(Elsner and Charniak, 2010) or k-means like ap-proaches (Shen et al, 2006).
Narratives, however,often intersect: a single passage may belong to mul-tiple narrative threads.
This motivates soft, proba-bilistic assignments of passages to threads.
More-over, narratives are inherently hierarchical.
The lat-ter two observations suggest that probabilistic gen-erative models are appropriate for MND.There has also been recent interesting relatedwork in the unsupervised induction of narrativeschemas (Chambers and Jurafsky, 2008; Chambersand Jurafsky, 2009).
In this work, the authors pro-posed the task of (automatically) discovering theevents comprising a narrative chain.
Here narrativeevent chains were defined by Chambers and Juraf-sky (2008) as partially ordered sets of events involv-ing the same protagonist.
While similar in that theseworks attempt to make sense of narrative texts, thetask at hand is quite different.In particular, narrative schema induction pre-supposes a single narrative thread.
Indeed, the au-thors explicitly make the assumption that a singleprotagonist participates in all of the events forminga narrative chain.
Thus the discovered chains de-scribe actions experienced by the protagonist local-ized within a particular narrative structure.
By con-trast, in this work I treat narrative texts as instan-tiations of fabulas, in line with Bal (1997).
Fab-ulas can be viewed as distributions over charac-ters, events and other entities; this conceptualiza-tion of what constitutes a narrative is broader thanChambers and Jurafsky (2008).
inducing narrativeschemas (Chambers and Jurafsky, 2009) may beviewed as a possible next step in a narrative induc-tion pipeline, subsequent to disentangling the textcomprising individual narrative threads.
Indeed, thelatter task might be viewed as attempting to auto-matically re-construct the fabula latent in a specificnarrative thread.Elsewhere, Elson et al (2010) proposed a methodfor extracting social networks from literary texts.Their method relies on dialogue detection.
This isused to construct a graph representing social inter-actions, in which an edge connecting two charac-ters implies that they have interacted at least once;the weight of the edge encodes the frequency oftheir interactions.
Their method is a pipelined pro-cess comprising three steps: character identification,speech attribution and, finally, graph construction.Their results from the application of this method toa large collection of novels called into question along-held literary hypothesis: namely that there isan inverse correlation between the number of char-acters in a novel and the amount of dialogue it con-tains (Moretti, 2005) (it seems there is not).
By an-swering a literary question empirically, their workdemonstrates the power of computational methodsfor literature analysis.4 Corpus (Infinite Jest)I introduce a new corpus for the task of multiple nar-rative disentanglement (MND): David Foster Wal-lace?s novel Infinite Jest (Wallace, 1996) that I havemanually annotated with narrative tags.3 InfiniteJest is an instructive example for experimenting withMND, as the story moves frequently between a fewmostly independent ?
though ultimately connectedand occasionally intersecting ?
narrative threads.3Available at http://github.com/bwallace/computationaljest.I also note that the text comprises ?100 pages of footnotes, butI did not annotate these.3Annotation, i.e., manually assigning text to oneor more narratives, is tricky due primarily to hav-ing to make decisions about new thread designationand label granularity.4 Start with the first.
Thereis an inherent subjectivity in deciding what consti-tutes a narrative thread.
In this work, I was lib-eral in making this designation, in total assigning 49unique narrative labels.
Most of these tell the storyof particular (minor) characters, who are themselvesactors in a ?higher-level?
narrative ?
as previouslymentioned, narrative structures are inherently hier-archical.
This motivates my liberal introduction ofnarratives: lesser threads are subsumed by their par-ent narratives, and can thus simply be ignored duringanalysis if one is uninterested in them.
Indeed, thiswork focuses only on the three main narratives in thetext (see below).Granularity poses another challenge.
At whatlevel ought the text be annotated?
Should each sen-tence be tagged with associated threads?
Each para-graph?
I let context guide this decision: in somecases tags span a single sentence; more often theyspan paragraphs.
As an example, consider the fol-lowing example of annotated text, wherein the AFRbriefly narrative intersects the story of the ETA (seeTable 2).<AFR>Marathe was charged with this opera-tion?s details ... <ETA>A direct assault upon theAcademy of Tennis itself was impossible.
A.F.R.sfear nothing in this hemisphere except tall and steephillsides.
... </ETA></AFR>Here the ellipses spans several paragraphs.
Precisionprobably matters less than context in MND: identi-fying only sentences that involve a particular sub-narrative, sans context, would probably not be use-ful.
Because the appropriate level of granularity de-pends on the corpus at hand, the task of segmentingthe text into useful chunks is a sub-task of MND.I refer to the segmented pieces of text as passagesand say that a passage belongs to all of the narrativethreads that appear anywhere within it.
Hence in theabove example, the passage containing this excerptwould be designated as belonging to both the ETAand AFR threads.4These complexities seem to be inherent to disentanglementtasks in general: Elsner and Charniak (2010) describe analoguesissues in the case of chat.AFR This is the tale of the wheelchair assassins, aQue`be`cois terrorist group, and their attempts toseize an original copy of a dangerous film.
Fo-calizer: Marathe.EHDRH The Ennet House Drug Recovery House (sic).This narrative concerns the going-ons at a drugrecovery house.
Focalizer: Don Gately.ETA This narrative follows the students and facultyat the Enfield Tennis Academy.
Focalizer: Hal.Table 2: Brief summaries of the main narratives compris-ing Infinite Jest.narrative # of passages prevalenceAFR 30 16%EHDRH 42 23%ETA 69 38%Table 3: Summary statistics for the three main narratives.Infinite Jest is naturally segmented by breaks,i.e., blank lines in the text which typically indicatesome sort of context-shift (functionally, these arelike mini-chapters).
There are 182 such breaks inthe book, demarcating 183 passages.
Each of thesecomprises about 16,000 words and contains an av-erage of 4.6 (out of 49) narratives, according to myannotations.There are three main narrative threads in InfiniteJest, summarized briefly in Table 2.5 I am not alonein designating these as the central plot-lines in thebook.6 Nearly all of the other threads in the text aresubsumed by these (together the three cover 72%of the passages in the book).
These three mainthreads are ideal for evaluating an MND system, fora few reasons.
First, they are largely independent ofone another, i.e., overlap only occasionally (thoughthey do overlap).
Second, they are relatively unam-biguous: it is mostly clear when a passage tells apiece of one of these story-lines, and when it doesnot.
These narratives are thus well-defined, provid-ing a minimal-noise dataset for the task of MND.That I am the single annotator of the corpus (andhence inter-annotator agreement cannot be assessed)is unfortunate; the difficulty of finding someone bothqualified and willing to annotate the 1000+ pagebook precluded this possibility.
I hope to address5I include these only for interested readers: the descriptionsare not technically important for the work here, and one mayequivalently substitute ?narrative 1?, ?narrative 2?, etc.6e.g., http://www.sampottsinc.com/ij/file/IJ Diagram.pdf.4Figure 2: The three main narratives in Infinite Jest.
A colored box implies that the corresponding narrative is presentin the passage at that location in the text; these are scaled relative to the passage length.this shortcoming in future work.Figure 2 depicts the location and duration of thesesub-narratives within the text.
Passages run alongthe bottom axis.
A colored box indicates that thecorresponding narrative is present in the passagefound at that location in the book.
Passages are nor-malized by their length: a wide box implies a longpassage.
The aim of MND, then, is to automaticallyinfer this structure from the narrative text.5 Narrative Modeling for MultipleNarrative DisentanglementThe proposed method is motivated by the theoryof narratology (Bal, 1997), reviewed in Section 2.Specifically I assume that passages are mixtures ofdifferent narratives with associated underlying fabu-las.
Fabulas, in turn, are viewed as distributions overentities.
Entities are typically actors, but may alsobe locations, etc.
; they are what fabulas are about.The idea is to infer from the observed passages theprobable latent fabulas.This is a generative view of narrative texts, whichlends itself naturally to a topic-modeling approach(Steyvers and Griffiths, 2007).
Further, this genera-tive vantage allows one to exploit the machinery oflatent Dirichelet alocation (LDA) (Blei et al, 2003).LDA is a generative model for texts (and discretedata, in general) in which it is assumed that eachdocument in a corpus reflects a mixture of (latent)topics.
The words in the text are thus assumed to begenerated by these topics: topics are multinomialsover words.
Graphically, this model is depicted byFigure 3.
All of the parameters in this model mustbe estimated; only the words in documents are ob-served.
To uncover the topic mixtures latent in doc-LATENT DIRICHLET ALLOCATION!
z w"#MNFigure 1: Graphical model representation of LDA.
The boxes are ?plates?
representing replicates.The outer plate represents documents, while the inner plate represents the repeated choiceof topics and words within a document.where p(zn |") is simply "i for the unique i such that zin = 1.
Integrating over " and summing overz, we obtain the marginal distribution of a document:p(w |!,#) = ?
p(" |!
)?N$n=1%zn p(zn |")p(wn |zn,#)?d".
(3)Finally, taking the product of the marginal probabilities of single documents, we obtain the proba-bility of a corpus:p(D |!,#) = M$d=1?p("d |!)?
Nd$n=1%zdn p(zdn |"d)p(wdn |zdn,#)?d"d .The LDA model is represented as a probabilistic graphical model in Figure 1.
As the figuremakes clear, there are three levels to the LDA representation.
The parameters !
and # are corpus-level parameters, assumed to be sampled once in the process of generating a corpus.
The variables"d are document-level variables, sampled once per document.
Finally, the variables zdn and wdn areword-level variables and are sampled once for each word in each document.It is important to distinguish LDA from a simple Dirichlet-multinomial clustering model.
Aclassical clustering model would involve a two-level model in which a Dirichlet is sampled oncefor a corpus, a multinomial clustering variable is selected once for each document in the corpus,and a set of words are selected for the document conditional on the cluster variable.
As with manyclustering models, such a model restricts a document to being associated with a single topic.
LDA,on the other hand, involves three levels, and notably the topic node is sampled repeatedly within thedocument.
Under this model, documents can be associated with multiple topics.Structures similar to that shown in Figure 1 are often studied in Bayesian statistical modeling,where they are referred to as hierarchical models (Gelman et al, 1995), or more precisely as con-ditionally independent hierarchical models (Kass and Steffey, 1989).
Such models are also oftenreferred to as parametric empirical Bayes models, a term that refers not only to a particular modelstructure, but also to the methods used for estimating parameters in the model (Morris, 1983).
In-deed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameterssuch as !
and # in simple implementations of LDA, but we also consider fuller Bayesian approachesas well.997Figure 3: The graphical model of latent Dirichlet alo-cation (LDA; Figure fro Blei et al (2003)).
?
param-eterizes the multinomial governing topics, i.e., zs.
Theobserved words w are then assumed to be drawn from amultinomial conditioned on z.
Here the plates denote thatthere are N (observed) words and M topics.um nts, standard i ference pr cedures can be usedfor parameter estimation (Jordan et al, 1999).I propose the following approach for MND, whichI will refer to as narrative modeling.
(This pipelineis also described by Figure 4).1.
Segment the raw text into passages.
It is at thelevel of this unit that narratives will be assigned: ifa given narrative tag is anywhere in a passage, thatpassage is deemed as eing a part of said narrative.7In many cases (including the present one) this stepwill be relativ ly trivial; e.g., segm nting th textinto chapters or paragraphs.2.
(Automatically) extract from each of these seg-ments named entities.
The idea is that these includethe primary players in the respective narratives, i.e.,important actors and locations.3.
Perform latent Dirichelet analysis (LDA) overthe entities extracted in (2).
When this topic mod-7This is analogous to a multi-label scenario.5eling is performed over the entities, rather than thetext, I shall refer to it as narrative modeling.As mentioned above, Step (1) will be task-specific: what constitutes a passage is inherentlysubjective.
In many cases, however, the text willlend itself to a ?natural?
segmenting, e.g., at thechapter-level.
Standard statistical techniques fornamed entity recognition (NER) can be used forStep (2) (McCallum and Li, 2003).Algorithm 1 The story of LDA over extracted enti-ties for multiple narrative disentanglement.Draw a mixture of narrative threads ?
?
Dir(?
)for each entity in the passage ei doDraw a narrative thread ti ?Multinomial(?
)Draw ei from p(ei|ti)end forsegmenternarrative textpassagesNERextractorextracted entitiesfor passagesnarrativemodelingFigure 4: The MNDpipeline.For the narrative model-ing Step (3), I use LDA(Blei et al, 2003); thegenerative story for nar-rative modeling is toldby Algorithm 1.8 Thissquares with the narra-tological view: entitiesare observed in the textwith probability propor-tional to their likelihoodof being drawn from thecorresponding latent fabu-las (which we are attempt-ing to recover).
Focus-ing on these entities, ratherthan the raw text, is cru-cial if one is to be compat-ible with the narratologicalview.
The text is merely a particular telling of theunderlying fabula, made noisy by story specific as-pects; extracting entities from the passages effec-tively removes this noise, allowing the model to op-erate over a space more closely tied to the fabulas.In the following section, I demonstrate that this shiftto the entity-space substantially boosts MND perfor-mance.8Liu and Liu (2008) have also proposed topic models overNEs, though in a very different context.The aim is to uncover the top k most salient nar-rative threads in a text, where k is a user-providedparameter.
Indeed one must specify the number ofthreads he or she is interested in identifying (and dis-entangling), because because, due to the hierarchicalnature of narratives, there is no single ?right number?of them.
Consider that the input block of text con-stitutes a perfectly legitimate (meta-)narrative on itsown, for example.
A related issue that must be ad-dressed is that of deciding when to assign a passageto multiple threads.
That is, given the (estimated)narrative mixtures for each passage as an input, towhich (if any) narrative threads ought this passagebe assigned?My approach to this is two-fold.
First, I set athreshold probability ?
such that a passage pi can-not be assigned to a narrative thread t if the esti-mated mixture component is?
?.
I use ?
= 1/k, asthis value implies that the passage is dominated byother threads (in the case that all k threads contributeequally to a passage, the corresponding mixture el-ements would all be 1/k).
Second, I enforce a con-straint that in order to be assigned to the narrative t,a passage must contain at least one of the top l enti-ties involved in t (according to the narrative model).This constraint encodes the intuition that the mainactors (and locations) that constitute a given fabulaare (extremely) likely to be present in any given pas-sage in which it is latent.
I set l = 100, reflectingintuition.
These were the first values I used for bothof these parameters; I did not tune them to the cor-pus at hand.
I did, however, experiment with othervalues after the primary analysis to assess sensitiv-ity.
The proposed algorithm is not terribly sensitiveto either parameter, though both exert influence inthe expected directions: increasing ?
decreases re-call, as passages are less likely to be assigned to nar-ratives.
Decreasing l has a similar effect, but doesnot substantially impact performance unless extremevalues are used.95.1 Focalizer DetectionRecall that the focalizer of a narrative is the agentresponsible for perception: it is from their point ofview that the story is told (Bal, 1997).
One can eas-ily exploit the narrative modeling method above to9Fewer than 10 or more than 500, for example.6automatically identify the (main) focalizer of the un-covered narratives.10 To this end, I simply identifythe highest ranking entity from each narrative thathas also been labeled as a ?person?
(as opposed, e.g.,to an ?organization?
).6 Empirical ResultsI now present experimental results over the InfiniteJest corpus, described in Section 4.
The task here isto uncover the three main narratives in the text, de-picted in Figure 2.
To implement the proposed nar-rative modeling method (Section 5), I first chunkedthe text into passages, delineated in Jest by breaksin the text.
I performed entity extraction over thesepassages using the NLTK toolkit (Bird et al, 2009).I then performed LDA via Mallet (McCallum, 2002)to estimate the narrative mixture components of eachpassage.recall = TP/(TP + FN) (1)precision = TP/(TP + FP ) (2)F = 2 ?
precision ?
recallprecision+ recall (3)I compare the narrative modeling approach pre-sented in the preceding section to three baselines.The simplest of these, round-robin and all-sameare similar to the baselines used for chat disentan-glement (Elsner and Charniak, 2010).
Respectively,these strategies designate each passage as: belong-ing to the next narrative in a given sequence (?narra-tive 1?, ?narrative 2?, ?narrative 3?
), and, belongingto the majority narrative.
In both cases I show thebest result attainable using the method: thus in thecase of the former, I report the best scoring resultsfrom all 3!
possible thread sequences (with respectto macro-averaged F-score) and in the latter case Iuse the true majority narrative.I also evaluate a simple topic-modeling baseline,which is the same as narrative modeling, except that:1) LDA is performed over the full-text (rather thanthe extracted entities) and, 2) there is no constraintenforcing that passages reference an entity associ-ated with the assigned narrative.
I evaluate resultswith respect to per-narrative recall, precision andF-score (Equations 1-3) (where TP=true positive,10Technically, there may be multiple focalizers in a narrative,but more often there is only one.FN=false negative, etc.).
I also consider micro- andmacro-averages of these.To calculate the micro-average, one considerseach passage at a time by counting up the TPs, FPs,TNs and FNs therein for each narrative under con-sideration (w.r.t.
the model being evaluated).
Themicro-average is then calculated using these talliedcounts.
Note that in this case certain narratives maycontribute more to the overall result than others, e.g.those that are common.
By contrast, to calculate themacro-average, one considers each narrative in turnand calculates the average of the metrics of interest(recall, precision) w.r.t.
this narrative over all pas-sages.
An average is then taken over these mean per-formances.
This captures the average performanceof a model over all of the narratives, irrespectiveof their prevalence; in this case, each thread con-tributes equally to the overall result.
Finally, notethat none of the methods explicitly labels the narra-tives they uncover: this assignment can be made bysimply matching the returned narratives to the threadlabels (e.g., ETA) that maximize performance.
Thislabeling is strictly aesthetic; the aim is to recover thelatent narrative threads in text, not to label them.Table 4 presents the main empirical results.
Nei-ther of the simple baseline methods (round-robinand all-same) performed very well.
Both cases, forexample, completely failed to identify the EHDRHthread (though this is hardly surprisingly in the all-same case, which identifies only one thread by def-inition).
The macro-averaged precisions and F-measures are thus undefined in these cases (thesegive rise to a denominator of 0).
With respect tomicro-averaged performance, all-same achieves asubstantially higher F-score than round-robin here,though in general this will be contingent on howdominated the text is by the majority thread.Next consider the two more sophisticated strate-gies, including the proposed narrative modelingmethod.
Start with the performance of full-textTM, i.e., performing standard topic-modeling overthe full-text.
This method improves considerably onthe baselines, achieving a macro-averaged F-scoreof .545.11 But the narrative modeling method (Sec-tion 5) performs substantially better, boosting the11In the full-text case, I evaluated the performance of everypossible assignment of topics to threads, and report the bestscoring result.7Figure 5: The unsupervised re-construction of the three main narratives using the narrative modeling approach.Hatched boxes denote false-positives (designating a passage as belonging to a narrative when it does not); emptyboxes false negatives (failing to assign a passage to narrative to which it belongs).Figure 6: Results using full-text topic modeling (see above caption).macro-averaged F-score by over 15 points (a percentgain of nearly 30%).Figures 5 and 6 depict the unsupervised re-construction of the narrative threads using narrativemodeling and the full-text topic modeling approach,respectively.
Recall that the aim is to re-constructthe narratives depicted in Figure 2.
In these plots, anempty box represents a false negative (i.e., impliesthat this passage contained the corresponding narra-tive but this was not inferred by the model), and ahatched box denotes a false positive (the model as-signed the passage to the corresponding narrative,but the passage did not belong to it).
One can seethat the narrative modeling method (Figure 5) re-constructs the hidden threads much better than doesthe full-text topic modeling approach (Figure 6).Once can see that the latter method has particulartrouble with the EHDRH thread.I also experimented with the focalizer detectionmethod proposed in Section 5.1.
This simple strat-egy achieved 100% accuracy on the three main nar-ratives, correctly identifying by name each of thecorresponding focalizers (see Table 2).6.1 A More Entangled ThreadThe preceding results are positive, insofar as the pro-posed method substantially improves on baselinesand is able to disentangle threads with relativelyhigh fidelity.
These results considered the three mainnarratives that comprise the novel (Figure 2).
Thisis the sort of structure I believe will be most com-mon in narrative disentanglement, as it is likely thatone will mostly be interested in extracting coherentthreads that are largely independent of one another.That said, I will next consider a more entangledthread to see if the method handles these well.
Morespecifically, I introduce the narrative INC, which re-lates the story of the Incandenza family.
This familyis (arguably) the focus of the novel.
The story ofthe Incandenza?s overlaps extremely frequently withthe three main, mostly independent narratives con-sidered thus far (see Figure 6).
This thread is thusdifficult from an MND perspective.I apply the same methods as above to this task, re-questing four (rather than three) sub-narratives, i.e.,k = 4.
Results are summarized in Table 5.12 We ob-12I omit the two baseline strategies due to space constraints;8round-robin all-same full-text TM narrative modelingnarrative recall prec.
F recall prec.
F recall prec.
F recall prec.
FAFR 0.433 0.210 0.283 0.000 undef.
undef.
0.900 0.300 0.450 0.933 0.359 0.519EHDRH 0.000 undef.
undef.
0.000 undef.
undef.
0.786 0.402 0.532 0.929 0.736 0.821ETA 0.369 0.348 0.393 1.000 0.375 0.545 0.667 0.639 0.653 0.855 0.694 0.766macro-avg.
0.260 undef.
undef.
0.333 undef.
undef.
0.752 0.447 0.545 0.906 0.596 0.702micro-avg.
0.262 0.300 0.280 0.489 0.375 0.425 0.752 0.434 0.551 0.894 0.583 0.706Table 4: Empirical results using different strategies for MND.
The top three rows correspond to performance forindividual narratives; the bottom two provide micro- and macro-averages, which are taken over the individual passagesand the narrative-level results, respectively.Figure 7: The INC narrative thread (green, top).
This narrative is substantially more entangled than the others, i.e.,more frequently intersects with the other narratives.full-text TM narrative modelingnarrative recall prec.
F recall prec.
FAFR 0.60 0.30 0.40 0.83 0.50 0.63EHDRH 0.83 0.57 0.67 0.79 0.75 0.77ETA 0.67 0.69 0.68 0.67 0.89 0.76INC 0.57 0.46 0.51 0.43 0.75 0.54macro-avg.
0.67 0.50 0.56 0.68 0.72 0.67micro-avg.
0.65 0.50 0.57 0.62 0.72 0.67Table 5: Results when the fourth narrative, more entan-gled narrative (INC) is added.serve that the narrative modeling strategy again beststhe baseline strategies, achieving a macro-averagedF-score of about 10 points greater than that achievedusing the full-text TM method (a ?20% gain).Focalizer identification is tricky in this case be-cause there are multiple focalizers.
However I notethat using the proposed strategy, four members ofthe Incandenza clan rank in the top five entities as-sociated with this narrative, an encouraging result.13both performed worse than the displayed methods.13The fifth top-ranking entity is Joelle, a girl who plays animportant part in the family saga.7 ConclusionsI have introduced the task of multiple narrative dis-entanglement (MND), and provided a new annotatedcorpus for this task.
I proposed a novel method(narrative modeling) for MND that is motivated bythe theory of narratology.
I demonstrated that thismethod is able to disentangle the narrative threadscomprising Infinite Jest and that it substantially out-performs baselines in terms of doing so.
I also ex-tended the method to automatically identify narra-tive focalizers, and showed that it is possible to doso with near-perfect accuracy.Interesting future directions include exploringsupervised narrative disentanglement, combiningMND with narrative induction (Chambers and Juraf-sky, 2009) and applying MND to non-fictional texts.AcknowledgmentsThanks to Kevin Small and Carla Brodley for sug-gesting improvements to this work, and to all of themembers of the Inman Square Existentialist BookClub for insightful discussions about Jest.9ReferencesH.P.
Abbott.
2008.
The Cambridge introduction to nar-rative.
Cambridge Univ Pr.M Bal.
1997.
Narratology: Introduction to the theory ofnarrative, 3rd ed.
University of Toronto Press.S.
Bird, E. Klein, and E. Loper.
2009.
Natural languageprocessing with Python.
O?Reilly Media.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
Latentdirichlet alocation.
The Journal of Machine LearningResearch, 3:993?1022.A.
Celikyilmaz, D. Hakkani-Tur, H. He, G. Kondrak, andD.
Barbosa.
2010.
The actortopic model for extractingsocial networks in literary narrative.
In NIPS Work-shop: Machine Learning for Social Computing.N.
Chambers and D. Jurafsky.
2008.
Unsupervisedlearning of narrative event chains.
Proceedings ofACL-08: HLT, pages 789?797.N.
Chambers and D. Jurafsky.
2009.
Unsupervisedlearning of narrative schemas and their participants.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 602?610.
Association for Compu-tational Linguistics.M.
Elsner and E. Charniak.
2010.
Disentangling chat.Computational Linguistics, 36(3):389?409.M.
Elsner and E. Charniak.
2011.
Disentangling chatwith local coherence models.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,volume 1, pages 1179?1189.
Association for Compu-tational Linguistics.D.K.
Elson and K.R.
McKeown.
2010.
Automatic attri-bution of quoted speech in literary narrative.
In Pro-ceedings of AAAI.D.K.
Elson, N. Dames, and K.R.
McKeown.
2010.
Ex-tracting social networks from literary fiction.
In Pro-ceedings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 138?147.
Asso-ciation for Computational Linguistics.M.I.
Jordan, Z. Ghahramani, T.S.
Jaakkola, and L.K.Saul.
1999.
An introduction to variational methodsfor graphical models.
Machine learning, 37(2):183?233.Y.
Liu and F. Liu.
2008.
Unsupervised language modeladaptation via topic modeling based on named entityhypotheses.
In Acoustics, Speech and Signal Process-ing, 2008.
ICASSP 2008.
IEEE International Confer-ence on, pages 4921?4924.
IEEE.A.
McCallum and W. Li.
2003.
Early results for namedentity recognition with conditional random fields, fea-ture induction and web-enhanced lexicons.
In Pro-ceedings of the seventh conference on Natural lan-guage learning at HLT-NAACL 2003-Volume 4, pages188?191.
Association for Computational Linguistics.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.F.
Moretti.
2005.
Graphs, Maps, Trees: Abstract modelsfor a literary history.
Verso Books.G.
Prince.
1982.
Narratology: The form and functioningof narrative.
Mouton Berlin.G.
Prince.
2003.
A dictionary of narratology.
Universityof Nebraska Press.D.
Shen, Q. Yang, J.T.
Sun, and Z. Chen.
2006.
Threaddetection in dynamic text message streams.
In Pro-ceedings of the 29th annual international ACM SIGIRconference on Research and development in informa-tion retrieval, pages 35?42.
ACM.M.
Steyvers and T. Griffiths.
2007.
Probabilistictopic models.
Handbook of latent semantic analysis,427(7):424?440.T.
Stoppard.
1967.
Rosencrantz & Guildenstern aredead: a play in three acts.
Samuel French Trade.D.F.
Wallace.
1996.
Infinite Jest.
Little Brown & Co.10
