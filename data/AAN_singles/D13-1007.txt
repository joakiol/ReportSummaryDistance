Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 61?72,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsA Log-Linear Model for Unsupervised Text NormalizationYi YangSchool of Interactive ComputingGeorgia Institute of Technologyyiyang@gatech.eduJacob EisensteinSchool of Interactive ComputingGeorgia Institute of Technologyjacobe@gatech.eduAbstractWe present a unified unsupervised statisticalmodel for text normalization.
The relation-ship between standard and non-standard to-kens is characterized by a log-linear model,permitting arbitrary features.
The weightsof these features are trained in a maximum-likelihood framework, employing a novel se-quential Monte Carlo training algorithm toovercome the large label space, which wouldbe impractical for traditional dynamic pro-gramming solutions.
This model is im-plemented in a normalization system calledUNLOL, which achieves the best known re-sults on two normalization datasets, outper-forming more complex systems.
We use theoutput of UNLOL to automatically normalizea large corpus of social media text, revealing aset of coherent orthographic styles that under-lie online language variation.1 IntroductionSocial media language can differ substantially fromother written text.
Many of the attempts to character-ize and overcome this variation have focused on nor-malization: transforming social media language intotext that better matches standard datasets (Sproat etal., 2001; Liu et al 2011).
Because there is lit-tle available training data, and because social me-dia language changes rapidly (Eisenstein, 2013b),fully supervised training is generally not consideredappropriate for this task.
However, due to the ex-tremely high-dimensional output space ?
arbitrarysequences of words across the vocabulary ?
it isa very challenging problem for unsupervised learn-ing.
Perhaps it is for these reasons that the most suc-cessful systems are pipeline architectures that cob-ble together a diverse array of techniques and re-sources, including statistical language models, de-pendency parsers, string edit distances, off-the-shelfspellcheckers, and curated slang dictionaries (Liu etal., 2011; Han and Baldwin, 2011; Han et al 2013).We propose a different approach, performing nor-malization in a maximum-likelihood framework.There are two main sources of information to beexploited: local context, and surface similarity be-tween the observed strings and normalization can-didates.
We treat the local context using standardlanguage modeling techniques; we treat string simi-larity with a log-linear model that includes featuresfor both surface similarity and word-word pairs.Because labeled examples of normalized textare not available, this model cannot be trainedin the standard supervised fashion.
Nor can weapply dynamic programming techniques for unsu-pervised training of locally-normalized conditionalmodels (Berg-Kirkpatrick et al 2010), as their com-plexity is quadratic in the size of label space; innormalization, the label space is the vocabulary it-self, with at least 104 elements.
Instead, we presenta new training approach using Monte Carlo tech-niques to compute an approximate gradient on thefeature weights.
This training method may be appli-cable in other unsupervised learning problems witha large label space.This model is implemented in a normalizationsystem called UNLOL (unsupervised normalizationin a LOg-Linear model).
It is a lightweight proba-61bilistic approach, relying only on a language modelfor the target domain; it can be adapted to newcorpora text or new domains easily and quickly.Our evaluations show that UNLOL outperforms thestate-of-the-art on standard normalization datasets.In addition, we demonstrate the linguistic insightsthat can be obtained from normalization, usingUNLOL to identify classes of orthographic transfor-mations that form coherent linguistic styles.2 BackgroundThe text normalization task was introducedby Sproat et al(2001), and attained popularity inthe context of SMS messages (Choudhury et al2007b).
It has become still more salient in the era ofwidespread social media, particularly Twitter.
Hanand Baldwin (2011) formally define a normalizationtask for Twitter, focusing on normalizations betweensingle tokens, and excluding multi-word tokens likelol (laugh out loud).
The normalization task hasbeen criticized by Eisenstein (2013b), who arguesthat it strips away important social meanings.
Inrecent work, normalization has been shown to yieldimprovements for part-of-speech tagging (Hanet al 2013), parsing (Zhang et al 2013), andmachine translation (Hassan and Menezes, 2013).As we will show in Section 7, accurate automatednormalization can also improve our understandingof the nature of social media language.Supervised methods Early work on normaliza-tion focused on labeled SMS datasets, using ap-proaches such as noisy-channel modeling (Choud-hury et al 2007a) and machine translation (Awet al 2006), as well as hybrid combinations ofspelling correction and speech recognition (Kobuset al 2008; Beaufort et al 2010).
This worksought to balance language models (favoring wordsthat fit in context) with transformation models (fa-voring words that are similar to the observed text).Our approach can also be seen as a noisy channelmodel, but unlike this prior work, no labeled data isrequired.Unsupervised methods Cook and Stevenson(2009) manually identify several word formationtypes within a noisy channel framework.
Theyparametrize each formation type with a small num-ber of scalar values, so that all legal transformationsof a given type are equally likely.
The scalar pa-rameters are then estimated using expectation max-imization.
This work stands apart from most of theother unsupervised models, which are pipelines.Contractor et al(2010) use string edit distanceto identify closely-related candidate orthographicforms and then decode the message using a languagemodel.
Gouws et al(2011) refine this approachby mining an ?exception dictionary?
of strongly-associated word pairs such as you/u.
Like Con-tractor et al(2010), we apply string edit distance,and like Gouws et al(2011), we capture stronglyrelated word pairs.
However, rather than applyingthese properties as filtering steps in a pipeline, weadd them as features in a unified log-linear model.Recent approaches have sought to improve accu-racy by bringing more external resources and com-plex architectures to bear.
Han and Baldwin (2011)begin with a set of string similarity metrics, and thenapply dependency parsing to identify contextually-similar words.
Liu et al(2011) extract noisy train-ing pairs from the search snippets that result fromcarefully designed queries to Google, and then traina conditional random field (Lafferty et al 2001) toestimate a character-based translation model.
Theylater extend this work by adding a model of vi-sual priming, an off-the-shelf spell-checker, and lo-cal context (Liu et al 2012a).
Hassan and Menezes(2013) use a random walk framework to capturecontextual similarity, which they then interpolatewith an edit distance metric.
Rather than seek-ing additional external resources or designing morecomplex metrics of context and similarity, we pro-pose a unified statistical model, which learns featureweights in a maximum-likelihood framework.3 ApproachOur approach is motivated by the following criteria:?
Unsupervised.
We want to be able to traina model without labeled data.
At present, la-beled data for Twitter normalization is avail-able only in small quantities.
Moreover, associal media language is undergoing rapidchange (Eisenstein, 2013b), labeled datasetsmay become stale and increasingly ill-suited tonew spellings and words.62?
Low-resource.
Other unsupervised ap-proaches take advantage of resources such asslang dictionaries and spell checkers (Han andBaldwin, 2011; Liu et al 2011).
Resourcesthat characterize the current state of internetlanguage risk becoming outdated; in this paperwe investigate whether high-quality normaliza-tion is possible without any such resources.?
Featurized.
The relationship between any pairof words can be characterized in a number ofdifferent ways, ranging from simple character-level rules (e.g., going/goin) to larger substi-tutions (e.g., someone/sum1), and even to pat-terns that are lexically restricted (e.g., you/u,to/2).
For these reasons, we seek a model thatpermits many overlapping features to describecandidate word pairs.
These features may in-clude simple string edit distance metrics, aswell as lexical features that memorize specificpairs of standard and nonstandard words.?
Context-driven.
Learning potentially arbitraryword-to-word transformations without supervi-sion would be impossible without the strongadditional cue of local context.
For example,in the phrasegive me suttin to believe in,even a reader who has never before seen theword suttin may recognize it as a phonetictranscription of something.
The relatively highstring edit distance is overcome by the strongcontextual preference for the word somethingover orthographically closer alternatives suchas button or suiting.
We can apply an arbi-trary target language model, leveraging largeamounts of unlabeled data and catering to thedesired linguistic characteristics of the normal-ized content.?
Holistic.
While several prior approaches ?such as normalization dictionaries ?
operate atthe token level, our approach reasons over thescope of the entire message.
The necessity forsuch holistic, joint inference and learning canbe seen by changing the example above to:gimme suttin 2 beleive innnn.None of these tokens are standard (except 2,which appears in a nonstandard sense here), sowithout joint inference, it would not be possi-ble to use context to help normalize suttin.Only by jointly reasoning over the entire mes-sage can we obtain the correct normalization.These desiderata point towards a featurized se-quence model, which must be trained without la-beled examples.
While there is prior work on train-ing sequence models without supervision (Smithand Eisner, 2005; Berg-Kirkpatrick et al 2010),there is an additional complication not faced bymodels for tasks such as part-of-speech taggingand named entity recognition: the potential labelspace of standard words is large, on the order ofat least 104.
Naive application of Viterbi decod-ing ?
which is a component of training for bothContrastive Estimation (Smith and Eisner, 2005)and the locally-normalized sequence labeling modelof Berg-Kirkpatrick et al(2010) ?
will be stymiedby Viterbi?s quadratic complexity in the dimensionof the label space.
While various pruning heuris-tics may be applied, we instead look to Sequen-tial Monte Carlo (SMC), a randomized algorithmwhich approximates the necessary feature expecta-tions through weighted samples.4 ModelGiven a set of source-language sentences S ={s1, s2, .
.
.}
(e.g., Tweets), our goal is to trans-duce them into target-language sentences T ={t1, t2, .
.
.}
(standard English).
We are given a tar-get language model P (t), which can be estimatedfrom some large set of unlabeled target-languagesentences.
We denote the vocabularies of source lan-guage and target language as ?S and ?T respectively.We define a log-linear model that scores sourceand target strings, with the formP (s|t; ?)
?
exp(?Tf(s, t)).
(1)The desired conditional probability P (t|s) can beobtained by combining this model with the targetlanguage model, P (t|s) ?
P (s|t; ?
)P (t).
Since nolabeled data is available, the parameters ?
must beestimated by maximizing the log-likelihood of thesource-language data.
We define the log-likelihood63`?
(s) for a source-language sentence s as follows:`?
(s) = logP (s) = log?tP (s|t; ?
)P (t)We would like to maximize this objective by mak-ing gradient-based updates.?`?(s)?
?=1P (s)?tP (t)??
?P (s|t; ?
)=?tP (t|s)(f(s, t)?
?s?P (s?|t)f(s?, t))= Et|s[f(s, t)?
Es?|t[f(s?, t)]](2)We are left with a difference in expected featurecounts, as is typical in log-linear models.
However,unlike the supervised case, here both terms are ex-pectations: the outer expectation is over all target se-quences (given the observed source sequence), andthe nested expectation is over all source sequences,given the target sequence.
As the space of possibletarget sequences t grows exponentially in the lengthof the source sequence, it will not be practical tocompute this expectation directly.Dynamic programming is the typical solution forcomputing feature expectations, and can be appliedto sequence models when the feature function de-composes locally.
There are two reasons this willnot work in our case.
First, while the forward-backward algorithm would enable us to computeEt|s, it would not give us the nested expectationEt|s[Es?|t]; this is the classic challenge in trainingglobally-normalized log-linear models without la-beled data (Smith and Eisner, 2005).
Second, bothforward-backward and the Viterbi algorithm havetime complexity that is quadratic in the dimension ofthe label space, at least 104 or 105.
As we will show,Sequential Monte Carlo (SMC) algorithms have anumber of advantages in this setting: they permitthe efficient computation of both the outer and innerexpectations, they are trivially parallelizable, andthe number of samples provides an intuitive tuningtradeoff between accuracy and speed.4.1 Sequential Monte Carlo approximationSequential Monte Carlo algorithms are a class ofsampling-based algorithms in which latent vari-ables are sampled sequentially (Cappe et al 2007).They are particularly well-suited to sequence mod-els, though they can be applied more broadly.
SMCalgorithms maintain a set of weighted hypotheses;the weights correspond to probabilities, and in ourcase, the hypotheses correspond to target languageword sequences.
Specifically, we approximate theconditional probability,P (t1:n|s1:n) ?K?k=1?kn?tk1:n(t1:n),where ?kn is the normalized weight of sample k atword n (?
?kn is the unnormalized weight), and ?tk1:n isa delta function centered at tk1:n.At each step, and for each hypothesis k, a newtarget word is sampled from a proposal distribution,and the weight of the hypothesis is then updated.
Wemaintain feature counts for each hypothesis, and ap-proximate the expectation by taking a weighted av-erage using the hypothesis weights.
The proposaldistribution will be described in detail later.We make a Markov assumption, so that the emis-sion probability P (s|t) decomposes across the ele-ments of the sentence P (s|t) =?Nn P (sn|tn).
Thismeans that the feature functions f(s, t) must decom-pose on each ?sn, tn?
pair.
We can then rewrite (1)asP (s|t; ?)
=N?nexp(?Tf(sn, tn))Z(tn)(3)Z(tn) =?sexp(?Tf(s, tn)).
(4)In addition, we assume that the target languagemodel P (t) can be written as an N-gram languagemodel, P (t) =?n P (tn|tn?1, .
.
.
tn?k+1).
Withthese assumptions, we can view normalization asa finite state-space model in which the target lan-guage model defines the prior distribution of the pro-cess and Equation 3 defines the likelihood function.We are able to compute the the posterior probabil-ity P (t|s) using sequential importance sampling, amember of the SMC family.The crucial idea in sequential importance sam-pling is to update the hypotheses tk1:n and theirweights ?kn so that they approximate the posteriordistribution at the next time step, P (t1:n+1|s1:n+1).64Assuming the proposal distribution has the formQ(tk1:n|s1:n), the importance weights are given by?kn ?P (tk1:n|s1:n)Q(tk1:n|s1:n)(5)In order to update the hypotheses recursively, werewrite P (t1:n|s1:n) as:P (t1:n|s1:n) =P (sn|t1:n, s1:n?1)P (t1:n|s1:n?1)P (sn|s1:n?1)=P (sn|tn)P (tn|t1:n?1, s1:n?1)P (t1:n?1|s1:n?1)P (sn|s1:n?1)?P (sn|tn)P (tn|tn?1)P (t1:n?1|s1:n?1),assuming a bigram language model.
We further as-sume the proposal distribution Q can be factored as:Q(t1:n|s1:n) =Q(tn|t1:n?1, s1:n)Q(t1:n?1|s1:n?1)=Q(tn|tn?1, sn)Q(t1:n?1|s1:n?1).
(6)Then the unnormalized importance weights sim-plify to a recurrence:?
?kn =P (sn|tkn)P (tkn|tkn?1)P (tk1:n?1|s1:n?1)Q(tn|tn?1, sn)Q(tk1:n?1|s1:n?1)(7)=?kn?1P (sn|tkn)P (tkn|tkn?1)Q(tn|tn?1, sn)(8)Therefore, we can approximate the posterior dis-tribution P (tn|s1:n) ?
?Kk=1 ?kn?tkn(tn), and com-pute the outer expectation as follows:Et|s[f(s, t)] =K?k=1?kNN?n=1f(sn, tkn) (9)We compute the nested expectation using a non-sequential Monte Carlo approximation, assumingwe can draw s`,k ?
P (s|tkn).Es|tk [f(s, tk)] =1LN?n=1L?`=1f(s`,kn , tkn)This gives the overall gradient computation:Et|s[f(s, t)?
Es?|t[f(s?, t)]] =1?Kk=1 ??kNK?k=1?
?kN?N?n=1(f(sn, tkn)?1LL?`=1f(s`,kn , tkn))(10)where we sample tkn and update ?kn while mov-ing from left-to-right, and sample s`,kn at each n.Note that although the sequential importance sam-pler moves left-to-right like a filter, we use only thefinal weights ?N to compute the expectation.
Thus,the resulting expectation is based on the distribu-tion P (s1:N |t1:N ), so that no backwards ?smooth-ing?
pass (Godsill et al 2004) is needed to elim-inate bias.
Other applications of sequential MonteCarlo make use of resampling (Cappe et al 2007) toavoid degeneration of the hypothesis weights, but wefound this to be unnecessary due to the short lengthof Twitter messages.4.2 Proposal distributionThe major computational challenge for dynamicprogramming approaches to normalization is thelarge label space, equal to the size of the target vo-cabulary.
It may appear that all we have gainedby applying sequential Monte Carlo is to converta computational problem into a statistical one: anaive sampling approach will have little hope offinding the small high-probability region of the high-dimensional label space.
However, sequential im-portance sampling allows us to address this issuethrough the proposal distribution, from which wesample the candidate words tn.
Careful design of theproposal distribution can guide sampling towardsthe high-probability space.
In the asymptotic limit ofan infinite number of samples, any non-pathologicalproposal distribution will ultimately arrive at the de-sired estimate, but a good proposal distribution cangreatly reduce the number of samples needed.Doucet et al(2001) note that the optimal pro-posal ?
which minimizes the variance of the im-portance weights conditional on t1:n?1 and s1:n ?has the following form:Q(tkn|sn, tkn?1) =P (sn|tkn)P (tkn|tkn?1)?t?
P (sn|t?
)P (t?|tkn?1)(11)65Sampling from this proposal requires computingthe normalized distribution P (sn|tkn); similarly, theupdate of the hypothesis weights (Equation 8) re-quires the calculation ofQ in its normalized form.
Ineach case, the total cost is the product of the vocabu-lary sizes, O(#|?T |#|?S |), which is not tractable asthe vocabularies become large.In low-dimensional settings, a convenient so-lution is to set the proposal distribution equalto the transition distribution, Q(tkn|sn, tkn?1) =P (tkn|tkn?1, .
.
.
, tkn?k+1).
This choice is called the?bootstrap filter,?
and it has the advantage that theweights ?
(k) are exactly identical to the productof emission likelihoods?n P (sn|tkn).
The com-plexity of computing the hypothesis weights is thusO(#|?S |).
However, because this proposal ignoresthe emission likelihood, the bootstrap filter has verylittle hope of finding a high-probability sample inhigh-entropy contexts.We strike a middle ground between efficiency andaccuracy, using a proposal distribution that is closelyrelated to the overall likelihood, yet is tractable tosample and compute:Q(tkn|sn, tkn?1)def=P (sn|tkn)Z(tkn)P (tkn|tkn?1)?t?
P (sn|t?)Z(t?
)P (t?|tkn?1)=exp(?Tf(sn, tn))P (tkn|tkn?1)?t?
exp(?Tf(sn, t?
))P (t?|tkn?1)(12)Here, we simply replace the likelihood distribu-tion in (11) by its unnormalized version.To update the unnormalized hypothesis weights?
?kn, we have?
?kn =?kn?1?t?
exp(?Tf(sn, t?
))P (t?|tkn?1)Z(tkn)(13)The numerator requires summing over all ele-ments in ?T and the denominator Z(tkn) requiressumming over all elements in ?S , for a total cost ofO(#|?T |+ #|?S |).4.3 DecodingGiven an input source sentence s, the decoding prob-lem is to find a target sentence t that maximizesP (t|s) ?
P (s|t)P (t) =?Nn P (sn|tn)P (tn|tn?1).Feature name Descriptionword-word pair A set of binary features for eachsource/target word pair ?s, t?string similarity A set of binary features in-dicating whether s is one ofthe top N string similar non-standard words of t, for N ?
{5, 10, 25, 50, 100, 250, 500, 1000}Table 1: The feature set for our log-linear modelAs with learning, we cannot apply the usual dy-namic programming algorithm (Viterbi), becauseof its quadratic cost in the size of the target lan-guage vocabulary.
This must be multiplied bythe cost of computing the normalized probabilityP (sn|tn), resulting in a prohibitive time complexityof O(#|?S |#|?T |2N).We consider two approximate decoding algo-rithms.
The first is to simply apply the proposal dis-tribution, with linear complexity in the size of thetwo vocabularies.
However, this decoder is not iden-tical to P (t|s), because of the extra factor of Z(t)in the numerator.
Alternatively, we can apply theproposal distribution for selecting target word can-didates, then apply the Viterbi algorithm only withinthese candidates.
The total cost is O(#|?S |T 2N),where T is the number of target word candidates weconsider; this will asymptotically approach P (t|s)as T ?
#|?T |.
Our evaluations use the more expen-sive proposal+Viterbi decoding, but accuracy withthe more efficient proposal-based decoding is verysimilar.4.4 FeaturesOur system uses the feature types described in Ta-ble 1.
The word pair features are designed to cap-ture lexical conventions, e.g.
you/u.
We only con-sider word pair features that fired during training.The string similarity features rely on the similarityfunction proposed by Contractor et al(2010), whichhas proven effective for normalization in prior work.We bin this similarity to create binary features indi-cating whether a string s is in the top-N most similarstrings to t; this binning yields substantial speed im-provements without negatively impacting accuracy.665 Implementation and dataThe model and inference described in the pre-vious section are implemented in a softwaresystem for normalizing text on twitter, calledUNLOL: unsupervised normalization in a LOg-Linear model.
The final system can process roughly10,000 Tweets per hour.
We now describe some im-plementation details.5.1 Normalization candidatesMost tokens in tweets do not require normalization.The question of how to identify which words areto be normalized is still an open problem.
Follow-ing Han and Baldwin (2011), we build a dictionaryof words which are permissible in the target domain,and make no attempt to normalize source stringsthat match these words.
As with other comparableapproaches, we are therefore unable to normalizestrings like ill into I?ll.
Our set of ?in-vocabulary?
(IV) words is based on the GNU aspell dictionary(v0.60.6), containing 97,070 words.
From this dic-tionary, we follow Liu et al(2012a) and remove allthe words with a count of less than 20 in the Edin-burgh Twitter corpus (Petrovic?
et al 2010) ?
re-sulting in a total of 52,449 target words.
All sin-gle characters except a and i are excluded, and rtis treated as in-vocabulary.
For all in-vocabularywords, we define P (sn|tn) = ?
(sn, tn), taking thevalue of zero when sn 6= tn.
This effectively pre-vents our model from attempting to normalize thesewords.In addition to words that are in the target vocabu-lary, there are many other strings that should not benormalized, such as names and multiword shorten-ings (e.g.
going to/gonna).1 We follow prior workand assume that the set of normalization candidatesis known in advance during test set decoding (Han etal., 2013).
However, the unlabeled training data hasno such information.
Thus, during training we at-tempt to normalize all tokens that (1) are not in ourlexicon of IV words, and (2) are composed of letters,numbers and the apostrophe.
This set includes con-tractions like "gonna" and "gotta", which would notappear in the test set, but are nonetheless normalized1Whether multiword shortenings should be normalized is ar-guable, but they are outside the scope of current normalizationdatasets (Han and Baldwin, 2011).during training.
For each OOV token, we conduct apre-normalization step by reducing any repetitionsof more than two letters in the nonstandard words toexactly two letters (e.g., cooool?
cool).5.2 Language modelingThe Kneser-Ney smoothed trigram target languagemodel is estimated with the SRILM toolkit Stolcke(2002), using Tweets from the Edinburgh Twittercorpus that contain no OOV words besides hash-tags and username mentions (following (Han et al2013)).
We use this language model for both trainingand decoding.
We occasionally find training con-texts in which the trigram ?tn, tn?1, tn?2?
is unob-served in the language model data; features resultingfrom such trigrams are not considered when comput-ing the weight gradients.5.3 ParametersThe Monte Carlo approximations require two pa-rameters: the number of samples for sequentialMonte Carlo (K), and the number of samples for thenon-sequential sampler of the nested expectation (L,from Equation 10).
The theory of Monte Carlo ap-proximation states that the quality of the approxima-tion should only improve as the number of samplesincreases; we obtained good results with K = 10and L = 1, and found relatively little improvementby increasing these values.
The number of hypothe-ses considered by the decoder is set to T = 10;again, the performance should only improve with T ,as we more closely approximate full Viterbi decod-ing.6 ExperimentsDatasets We use two existing labeled Twitterdatasets to evaluate our approach.
The first dataset?
which we call LWWL11, based on the names ofits authors Liu et al(2011) ?
contains 3,802 indi-vidual ?nonstandard?
words (i.e., words that are notin the target vocabulary) and their normalized forms.The rest of the message in which the words is appearis not available.
As this corpus does not provide lin-guistic context, its decoding must use a unigram tar-get language model.
The second dataset ?
whichis called LexNorm1.1 by its authors Han and Bald-win (2011) ?
contains 549 complete tweets with1,184 nonstandard tokens (558 unique word types).67Method Dataset Precision Recall F-measure(Liu et al2011)LMML1168.88 68.88 68.88(Liu et al2012) 69.81 69.81 69.81UNLOL 73.04 73.04 73.04(Han and Baldwin, 2011)LexNorm 1.175.30 75.30 75.30(Liu et al2012) 84.13 78.38 81.15(Hassan et al2013) 85.37 56.4 69.93UNLOL 82.09 82.09 82.09UNLOL LexNorm 1.2 82.06 82.06 82.06Table 2: Empirical resultsIn this corpus, we can decode with a trigram lan-guage model.Close analysis of LexNorm1.1 revealed some in-consistencies in annotation (for example, y?alland 2 are sometimes normalized to you and to,but are left unnormalized in other cases).
In ad-dition, several annotations disagree with existingresources on internet language and dialectal En-glish.
For example, smh is normalized to some-how in LexNorm1.1, but internetslang.comand urbandictionary.com assert that it standsfor shake my head, and this is evident from examplessuch as smh at this girl.
Similarly, finnais normalized to finally in LexNorm1.1, but fromthe literature on African American English (Green,2002), it corresponds to fixing to (e.g., i?m finnago home).
To address these issues, we have pro-duced a new version of this dataset, which we callLexNorm1.2 (after consulting with the creators ofLexNorm1.1).
LexNorm1.2 differs from version 1.1in the annotations for 172 of the 2140 OOV words.We evaluate on LexNorm1.1 to compare with priorwork, but we also present results on LexNorm1.2in the hope that it will become standard in futurework on normalization in English.
The datasetis available at http://www.cc.gatech.edu/~jeisenst/lexnorm.v1.2.tgz.To obtain unlabeled training data, we randomlysample 50 tweets from the Edinburgh Twitter cor-pus Petrovic?
et al(2010) for each OOV word.
SomeOOV words appear less than 50 times in the cor-pus, so we obtained more training tweets for themthrough the Twitter search API.Metrics Prior work on these datasets has assumedperfect detection of words requiring normalization,and has focused on finding the correct normalizationfor these words (Han and Baldwin, 2011; Han et al2013).
Recall has been defined as the proportion ofwords requiring normalization which are normalizedcorrectly; precision is defined as the proportion ofnormalizations which are correct.Results We run our training algorithm for two it-erations (pass the training data twice).
The resultsare presented in Table 2.
Our system, UNLOL,achieves the highest published F-measure on bothdatasets.
Performance on LexNorm1.2 is very simi-lar to LexNorm1.1, despite the fact that roughly 8%of the examples were relabeled.In the normalization task that we consider, the to-kens to be normalized are specified in advance.
Thisis the same task specification as in the prior workagainst which we compare.
At test time, our systemattempts normalizes all such tokens; every error isthus both a false positive and false negative, so pre-cision equals to recall for this task; this is also truefor Han and Baldwin (2011) and Liu et al(2011).It is possible to trade recall for precision by re-fusing to normalize words when the system?s confi-dence falls below a threshold.
A good setting of thisthreshold can improve the F-measure, but we did notreport these results because we have no developmentset for parameter tuning.Regularization One potential concern is that thenumber of non-zero feature weights will continuallyincrease until the memory cost becomes overwhelm-ing.
Although we did not run up against mem-680 100000 200000 300000 400000number of features7980818283F-measure?=1e?04?=5e?05?=1e?05?=5e?06 ?=1e?06 ?=0e+00 ?
Dataset F-measure # of features10?4LexNorm 1.179.05 9,2815?
10?5 80.32 11,79410?5 81.00 42,4665?
10?6 82.52 74,74410?6 82.35 241,8200 82.26 369,3665?
10?6 LexNorm 1.2 82.23 74,607Figure 1: Effect of L1 regularization on the F-measure and the number of features with non-zero weightsory limitations in the experiments producing the re-sults in Table 2, this issue can be addressed throughthe application of L1 regularization, which producessparse weight vectors by adding a penalty of ?||?||1to the log-likelihood.
We perform online optimiza-tion of the L1-regularized log-likelihood by apply-ing the truncated gradient method (Langford et al2009).
We use an exponential decreasing learningrate ?k = ?0?k/N , where k is the iteration counterandN is the size of training data.
We set ?0 = 1 and?
= 0.5.
Experiments were run until 300,000 train-ing instances were observed, with a final learningrate of less than 1/32.
As shown in Figure 1, a smallamount of regularization can dramatically decreasethe number of active features without harming per-formance.7 AnalysisWe apply our normalization system to investi-gate the orthographic processes underlying languagevariation in social media.
Using a dataset of 400,000English language tweets, sampled from the monthof August in each year from 2009 to 2012, we ap-ply UNLOL to automatically normalize each token.We then treat these normalizations as labeled train-ing data, and examine the Levenshtein alignment be-tween the source and target tokens.
This alignmentgives approximate character-level transduction rulesto explain each OOV token.
We then examine whichrules are used by each author, constructing a matrixof authors and rules.2Factorization of the author-rule matrix reveals setsof rules that tend to be used together; we mightcall these rulesets ?orthographic styles.?
We applynon-negative matrix factorization (Lee and Seung,2001), which characterizes each author by a vectorof k style loadings, and simultaneously constructsk style dictionaries, which each put weight on dif-ferent orthographic rules.
Because the loadings areconstrained to be non-negative, the factorization canbe seen as sparsely assigning varying amounts ofeach style to each author.
We choose the factoriza-tion that minimizes the Frobenius norm of the recon-struction error, using the NIMFA software package(http://nimfa.biolab.si/).The resulting styles are shown in Table 3, fork = 10; other values of k give similar overall re-sults with more or less detail.
The styles incor-porate a number of linguistic phenomena, includ-ing: expressive lengthening (styles 7-9; see Brodyand Diakopoulos, 2011); g- and t-dropping (style 5,see Eisenstein 2013a) ; th-stopping (style 6); andthe dropping of several word-final vowels (styles1-3).
Some of these styles, such as t-droppingand th-stopping, have direct analogues in spokenlanguage varieties (Tagliamonte and Temple, 2005;Green, 2002), while others, like expressive length-ening, seem more unique to social media.
The re-lationships between these orthographic styles andsocial variables such as geography and demograph-2We tried adding these rules as features and retraining thenormalization system, but this hurt performance.69style rules examples1.
you; o-dropping y/_ ou/_u *y/*_ o/_ u, yu, 2day, knw, gud, yur, wud, yuh, u?ve, toda,everthing, everwhere, ourself2.
e-dropping, u/o be/b_ e/_ o/u e*/_* b, r, luv, cum, hav, mayb, bn, remembr, btween,gunna, gud3.
a-dropping a/_ *a/*_ re/r_ ar/_r r, tht, wht, yrs, bck, strt, gurantee,elementry, wr, rlly, wher, rdy, preciate,neway4.
g-dropping g*/_* ng/n_ g/_ goin, talkin, watchin, feelin, makin5.
t-dropping t*/_* st/s_ t/_ jus, bc, shh, wha, gota, wea, mus, firts, jes,subsistutes6.
th-stopping h/_ *t/*d th/d_ t/d dat, de, skool, fone, dese, dha, shid, dhat,dat?s7.
(kd)-lengthening i_/id _/k _/d _*/k* idk, fuckk, okk, backk, workk, badd, andd,goodd, bedd, elidgible, pidgeon8.
o-lengthening o_/oo _*/o* _/o soo, noo, doo, oohh, loove, thoo, helloo9.
e-lengthening _/i e_/ee _/e _*/e* mee, ive, retweet, bestie, lovee, nicee, heey,likee, iphone, homie, ii, damnit10.
a-adding _/a __/ma _/m _*/a* ima, outta, needa, shoulda, woulda, mm,comming, tomm, boutt, ppreciateTable 3: Orthographic styles induced from automatically normalized Twitter textics must be left to future research, but they offer apromising generalization of prior work that has fo-cused almost exclusively on exclusively on lexicalvariation (Argamon et al 2007; Eisenstein et al2010; Eisenstein et al 2011), with a few exceptionsfor character-level features (Brody and Diakopoulos,2011; Burger et al 2011).Note that style 10 is largely the result of mis-taken normalizations.
The tokens ima, outta, andneeda all refer to multi-word expressions in stan-dard English, and are thus outside the scope of thenormalization task as defined by Han et al(2013).UNLOL has produced incorrect single-token nor-malizations for these terms: i/ima, out/outta, andneed/needa.
But while these normalizations arewrong, the resulting style nonetheless captures a co-herent orthographic phenomenon.8 ConclusionWe have presented a unified, unsupervised statisticalmodel for normalizing social media text, attainingthe best reported performance on the two standardnormalization datasets.
The power of our approachcomes from flexible modeling of word-to-word re-lationships through features, while exploiting con-textual regularity to train the corresponding featureweights without labeled data.
The primary techni-cal challenge was overcoming the large label spaceof the normalization task; we accomplish this us-ing sequential Monte Carlo.
Future work may con-sider whether sequential Monte Carlo can offer sim-ilar advantages in other unsupervised NLP tasks.
Anadditional benefit of our joint statistical approach isthat it may be combined with other downstream lan-guage processing tasks, such as part-of-speech tag-ging (Gimpel et al 2011) and named entity resolu-tion (Liu et al 2012b).AcknowledgmentsWe thank the reviewers for thoughtful commentson our submission.
This work also benefittedfrom discussions with Timothy Baldwin, Paul Cook,Frank Dellaert, Arnoud Doucet, Micha Elsner, andSharon Goldwater.
It was supported by NSF SOCS-1111142.ReferencesS.
Argamon, M. Koppel, J. Pennebaker, and J. Schler.2007.
Mining the blogosphere: age, gender, and thevarieties of self-expression.
First Monday, 12(9).AiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
Aphrase-based statistical model for SMS text normaliza-tion.
In Proceedings of ACL, pages 33?40.70Richard Beaufort, Sophie Roekhaut, Louise-Am?lieCougnon, and C?drick Fairon.
2010.
A hybridrule/model-based finite-state framework for normaliz-ing sms messages.
In Proceedings of ACL, pages 770?779.Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?t?,John DeNero, and Dan Klein.
2010.
Painless un-supervised learning with features.
In Proceedings ofNAACL, pages 582?590.Samuel Brody and Nicholas Diakopoulos.
2011.Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!
: usingword lengthening to detect sentiment in microblogs.In Proceedings of EMNLP.John D. Burger, John C. Henderson, George Kim, andGuido Zarrella.
2011.
Discriminating gender on twit-ter.
In Proceedings of EMNLP.Olivier Cappe, Simon J. Godsill, and Eric Moulines.2007.
An overview of existing methods and recent ad-vances in sequential monte carlo.
Proceedings of theIEEE, 95(5):899?924, May.M.
Choudhury, R. Saraf, V. Jain, A. Mukherjee, S. Sarkar,and A. Basu.
2007a.
Investigation and model-ing of the structure of texting language.
Interna-tional Journal on Document Analysis and Recognition,10(3):157?174.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar, and Anupam Basu.2007b.
Investigation and modeling of the structure oftexting language.
International Journal of DocumentAnalysis and Recognition (IJDAR), 10(3-4):157?174.Danish Contractor, Tanveer A. Faruquie, and L. VenkataSubramaniam.
2010.
Unsupervised cleansing of noisytext.
In Proceedings of COLING, pages 189?196.Paul Cook and Suzanne Stevenson.
2009.
An unsu-pervised model for text message normalization.
InProceedings of the Workshop on Computational Ap-proaches to Linguistic Creativity, CALC ?09, pages71?78, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.A.
Doucet, N.J. Gordon, and V. Krishnamurthy.
2001.Particle filters for state estimation of jump markov lin-ear systems.
Trans.
Sig.
Proc., 49(3):613?624, March.Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,and Eric P. Xing.
2010.
A latent variable model for ge-ographic lexical variation.
In Proceedings of EMNLP.Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.2011.
Discovering sociolinguistic associations withstructured sparsity.
In Proceedings of ACL.Jacob Eisenstein.
2013a.
Phonological factors in socialmedia writing.
In Proceedings of the NAACL Work-shop on Language Analysis in Social Media.Jacob Eisenstein.
2013b.
What to do about bad languageon the internet.
In Proceedings of NAACL, pages 359?369.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging fortwitter: annotation, features, and experiments.
In Pro-ceedings of ACL.Simon J. Godsill, Arnaud Doucet, and Mike West.
2004.Monte carlo smoothing for non-linear time series.
InJournal of the American Statistical Association, pages156?168.Stephan Gouws, Dirk Hovy, and Donald Metzler.
2011.Unsupervised mining of lexical variants from noisytext.
In Proceedings of the First Workshop on Unsu-pervised Learning in NLP, EMNLP ?11.Lisa J.
Green.
2002.
African American English: ALinguistic Introduction.
Cambridge University Press,September.Bo Han and Timothy Baldwin.
2011.
Lexical normalisa-tion of short text messages: makn sens a #twitter.
InProceedings of ACL, pages 368?378.Bo Han, Paul Cook, and Timothy Baldwin.
2013.
Lex-ical normalization for social media text.
ACM Trans-actions on Intelligent Systems and Technology, 4(1):5.Hany Hassan and Arul Menezes.
2013.
Social text nor-malization using contextual graph random walks.
InProceedings of ACL.Catherine Kobus, Fran?ois Yvon, and G?raldineDamnati.
2008.
Normalizing sms: are two metaphorsbetter than one?
In Proceedings of COLING, pages441?448.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proceedings of ICML, pages 282?289.John Langford, Lihong Li, and Tong Zhang.
2009.Sparse online learning via truncated gradient.
TheJournal of Machine Learning Research, 10:777?801.D.
D. Lee and H. S. Seung.
2001.
Algorithms for Non-Negative Matrix Factorization.
In Advances in NeuralInformation Processing Systems (NIPS), volume 13,pages 556?562.Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.2011.
Insertion, deletion, or substitution?
: normaliz-ing text messages without pre-categorization nor su-pervision.
In Proceedings of ACL, pages 71?76.Fei Liu, Fuliang Weng, and Xiao Jiang.
2012a.
A broad-coverage normalization system for social media lan-guage.
In Proceedings of ACL, pages 1035?1044.Xiaohua Liu, Ming Zhou, Xiangyang Zhou, ZhongyangFu, and Furu Wei.
2012b.
Joint inference of namedentity recognition and normalization for tweets.
InProceedings of ACL.71Sa?a Petrovic?, Miles Osborne, and Victor Lavrenko.2010.
The edinburgh twitter corpus.
In Proceedingsof the NAACL HLT Workshop on Computational Lin-guistics in a World of Social Media, pages 25?26.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: training log-linear models on unlabeleddata.
In Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, ACL ?05,pages 354?362, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.R.
Sproat, A.W.
Black, S. Chen, S. Kumar, M. Os-tendorf, and C. Richards.
2001.
Normalization ofnon-standard words.
Computer Speech & Language,15(3):287?333.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of ICSLP, pages901?904.Sali Tagliamonte and Rosalind Temple.
2005.
Newperspectives on an ol?
variable: (t,d) in british en-glish.
Language Variation and Change, 17:281?302,September.Congle Zhang, Tyler Baldwin, Howard Ho, BennyKimelfeld, and Yunyao Li.
2013.
Adaptive parser-centric text normalization.
In Proceedings of ACL,pages 1159?1168.72
