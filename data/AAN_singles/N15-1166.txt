Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1440?1445,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsIdentification and Characterization of Newsworthy Verbs in World NewsBenjamin NyeUniversity of Pennsylvaniabepnye@gmail.comAni NenkovaUniversity of Pennsylvanianenkova@seas.upenn.eduAbstractWe present a data-driven technique for acquir-ing domain-level importance of verbs from theanalysis of abstract/article pairs of world newsarticles.
We show that existing lexical re-sources capture some the semantic character-istics for important words in the domain.
Wedevelop a novel characterization of the associ-ation between verbs and personal story narra-tives, which is descriptive of verbs avoided insummaries for this domain.1 IntroductionSummarization, either by people or machine, callsfor the ability to identify important content.
Com-putational approaches to identifying important con-tent fall into the two extremes of a possible spec-trum.
On one end, the types of important infor-mation for a given domain and topic are prede-fined as information extraction templates definedby experts, as in the earliest approaches to multi-document summarization (Radev and McKeown,1998) and the recently introduced guided summa-rization (Owczarzak and Dang, 2011).
On the otherextreme, traditional systems work only with indica-tors of importance coming solely from the input tobe summarized, or possibly also from the context ofthe input, i.e .
analyzing the anchor text of links to awebpage, or comments on a blog post or citations toa scientific article (Nenkova and McKeown, 2012).Here we explore the feasibility of data-drivenidentification of important information in the worldnews domain.
We specifically focus on the analy-sis of verbs, which is the first step of identifyingevent types of special interest.
The goal is to col-lect evidence of verb importance globally, withoutregard to a particular input or its context.
Suchideas have been explored in the past as subcompo-nents of extractive summarizers (Schiffman et al,2002; Hong and Nenkova, 2014) or as features de-rived from small datasets for sentence compression(Woodsend and Lapata, 2012).
In contrast, in ourwork we rely on large corpora and exclusively focuson the task of acquiring input independent indica-tors of importance.
We also constrain our analysisto a single domain, which allows us to examine thesemantic aspects of the verbs that may contribute totheir perceived importance.We leverage a dataset of human-written sum-maries of news articles to objectively ground the def-inition of word importance.
Summaries are intendedto convey important information while omitting theless important pieces, so words that are important ina newsworthy sense will occur more frequently insummaries.
The same data and intuition was usedrecently to develop a large corpus for determiningentity salience (Dunietz and Gillick, 2014).We derive a list of over one thousand verbs thathave statistically significant bias to appear in thesummaries (important verbs) and verbs with higherrate of occurrence in the original articles (unimpor-tant).
This resource of verbs and their domain-levelimportance may be fruitfully exploited in models ofsummarization that do not use pre-defined templatesbut are richer than approaches that rely solely onanalysis of the article text.We furthermore seek to characterize the proper-ties of words that are biased to occur more often1440in either summaries or in articles.
We noticed thatverbs that tended to be dis-preferred in the sum-maries related to personal narratives, in which peo-ple are described as private entities rather than pub-lic personas.
We applied the same measures that weused to analyze domain-level importance in worldnews to a collection of labeled personal and nonper-sonal blog entries.
Characterizing verbs on the per-sonal vs. nonpersonal dimension indeed turned outto beneficial for explaining domain-level importanceof verbs in world news: personal narratives are notconsidered important in this domain and verbs thattended to get excluded from summaries also tendedto appear more frequently in personal blog entries.This characterization offered broad coverage of thearticle vocabulary and additional explanatory powercompared to a characterization derived from GeneralInquirer categories.1The derived lexical resources may serve as shal-low semantics for a range of language processingtasks such as summarization, news filtering andsearch.22 Determining domain-level importanceTo determine domain-level importance, we use sum-maries and articles from the New York Times Anno-tated Corpus3, a collection of NYT articles that in-cludes genre tags and summaries written by libraryscientists.
We use articles published in the worldnews section between 1996 and 2005 for a total of36,69 article-summary pairs.All of the documents were parsed with the Stan-ford Parser to obtain lemmatized forms of the wordsand part of speech tags.
There are 2,634,850 tokensin the summaries and 32,587,740 tokens in the re-spective articles.The overall verb frequency is very similar in thesummaries (14.5%) and the articles (14.6%).
In ouranalysis we reduce the corpus of summaries andoriginal articles to only the verbs that occur in them.1The two lists characterizing the domain-level impor-tance and the personal?public dimension are availablefor download at http://www.cis.upenn.edu/?nlp/software/importance.html.2Personal perspective verbs may not be important inreporting world news but may be excellent indicators ofcelebrity/gossip search for example.3https://catalog.ldc.upenn.edu/LDC2008T19Then we compare the rate of occurrence of eachverb in the two types of writing.
Verbs used pro-portionally more in summaries are likely to corre-spond to events that are important, while verbs thatoccur more frequently in the articles are less likelyto be related to the key topics of an article.
Togenerate two classes of verbs representing importantand non-important verbs, we consider two measure-ments: the difference of a verb?s usage frequencybetween summaries and articles, and the statisticalsignificance of this bias.Figure 1: Word frequency in summaries vs. articles.
Theblack line is where the frequencies are equal, and wordsplotted in grey have statistically significant differences infrequencies for the two classes.Figure 1 shows the plot of each verb?s probabilityin the summaries, Ps(wi) vs. in the articles, Pa(wi).Points above the line are verbs that occur more fre-quently in articles, while points below the line aremore frequent in summaries.
Dividing the pointsalong this line produces two classes of verbs.
Wecan further quantify how strongly a word is associ-ated with its class using a variety of metrics (Monroeet al, 2008).For this application, we chose to use the log oddsratio.
To measure how much more likely a word wis to occur in a class c, we compute the odds of aword occurring in corpora type c, i.e summary (s) ororiginal article (a):Odds(w, c) =P (w | class = c)1?
P (w | class = c)The ratio of the odds with respect to the two differentcorpora is a measure of how much more frequently1441a word is used in each case.
To make the measureinterpretable, we take the log of the odds ratio pro-ducing the final weight for a word:log(Odds(w, s)Odds(w, a))' log(P (w | class = s)P (w | class = a))This metric gives an intuitive measure of the usagerate of words.
For example, if a word occurs 3 timesmore often in the summaries it will be given a weightof log(3), and if it occurs three times more often inthe articles it will be given a weight of ?
log(3).Figure 2: Frequency vs. log odds ratio for each word.Positive odds ratios correspond to summary-biased wordsand negative correspond to article-biased.
The lines indi-cate integer usage ratios (1:3, 1:2, 1:1, 2:1, 3:1, etc).However, this metric is unreliable for verbs withlow counts in either class of texts.
If a verb occursfive times in summaries and only once in articles, itis difficult to say if there is true signal for impor-tance.
As the number of counts of a verb in the twoclasses increases, so does our certainty about the sig-nificance of any observed differences in usage rates.To obtain a measure of the statistical significance ofthe domain-level importance weight of a verb, wetreat the set of observed tokens as a Bernoulli trialwhere each token occurs either in a summary (suc-cess) or in an article (failure).
We apply a binomialtest to compute the probability of the observed dis-tribution of tokens in the two types of text under thenull hypothesis that the word has equal frequencyin summaries and articles.
The p-value from the testgives a measure of the certainty that a word is impor-tant and not.
We can filter out words with p-valuesSummary-biasedspur hail allege avert slay exile claim intensify ex-tradite oust overturn underscore cite devastate weighdefuse injure curb defy resign suspect warn quell kid-nap stir plot widen charge thwart reviveArticle-biasedchant talk sleep hate graduate realize dress understandquote sound add drink sing refer read think imagineremember shout sit happen cry wave like thank lovesmile accord reply misstateTable 1: Words with highest weights, drawn from verbswith frequency greater than the median verb frequency.above a certain threshold.
Moving the thresholdcloser to 0 enforces more and more certainty aboutthe classification, reducing vocabulary size but alsodecreasing noise.
Discarding verbs with a p-valueof less than 0.05 reduces the vocabulary size from3,924 words to 1,210.In Figure 2, the log odds ratio is plotted againstthe overall frequency for each verb after discardingunreliable verbs.
The most extreme weights occurmostly for the infrequent words, even after filter-ing out low p-value words.
Although these wordshave extremely high bias weights, they tend to beuncommon and not particularly informative.
Exam-ples include verbs such as ?hostage-take?, ?muck-rake?, and ?blaspheme.?
To get a clearer picture oftrends in the verbs in each category, we show in Ta-ble 1 the verbs with the 30 highest and 30 lowestweights among the verbs in the 50th percentile oftotal counts across all documents.In the following two sections, we turn to analyz-ing why certain verbs may be more important in thedomain than others.
First we examine the relation-ship between the summary- or article-bias of a wordand categories in the General Inquirer lexicon.
Thenwe develop a new characterization of verbs showingtheir association with person-centered perspective ofthe narrative.3 General InquirerThe General Inquirer lexicon provides a list of wordsmanually annotated with a variety of tags (Stoneet al, 1966).
We considered eight of these tags thatwere relevant to our task and could explain why1442Tag Summ.
Article Examplesnone 0.58 0.77Negative 0.21 0.05 counterfeit avert, weep wailActive 0.26 0.15 intensify overhaul, grasp hopStrong 0.16 0.03 oust devestate, roar promoteHostile 0.13 0.01 kidnap ravage, shrug crushPower 0.08 0.01 curb reclaim, persuade overcomePositive 0.07 0.03 reinstate mend, reassure hugPassive 0.05 0.05 deplore mourn, gaze huddleWeak 0.04 0.03 flounder sag, abandon hesitateTable 2: Percentage of words in each class covered bydifferent GI tags.
The first two example words comefrom the summary-biased class and last two come fromthe article-biased class.a verb has domain-level importance: NEGATIVE,POSITIVE, ACTIVE, PASSIVE, STRONG, WEAK,HOSTILE, and POWER.Table 2 shows some randomly selected wordsfrom each of the eight GI tags.
The first two wordscome from the summary-biased class and last twocome from the article-biased class.
Table 2 alsoshows the fraction of verbs in each class that oc-cur in the GI with a given tag, as well as the frac-tion of verbs that do not have any of the eight tags.It becomes immediately clear that the GI categoriesdo have explanatory power but that it has a majorproblem with coverage, with the majority of verbsin the summary and article corpora not appearingin the GI at all as shown on the first line.
Notably,the coverage is considerably better for the summary-biased verbs.
Verbs from several GI categories ap-peared notably more often in summaries than in ar-ticles.
For example, verbs with the NEGATIVE tagaccount for 21% of verbs in summaries, but only5% of verbs in articles.
Other such categories in-clude verbs that imply an active physical engage-ment (ACTIVE), imply that the actor is in a posi-tion of power (STRONG), imply that hostility existsbetween the entities involved (HOSTILE) or that im-ply that the actor has the influence to affect the poli-cies of others (POWER).
POSITIVE, PASSIVE, andWEAK verbs had more similar appearance rates inboth classes, but the absolute number of words cov-ered by these tags was low.Increasing the strictness of the p-value cutoff forpruning the vocabulary as described in the previoussection reduces the size of the vocabulary but in-creases the purity of the classes by only includingp-value 0.01 0.001 0.0001 0.00001Summary 0.54 0.53 0.54 0.54Article 0.80 0.86 0.88 0.91Table 3: Percentage of words with zero GI tags for in-creasing p-value cutoff strictnessverbs that have sufficiently different usage ratios.
Asshown in Table 3, as we restrict the vocabulary toincreasingly certain verbs, the proportion of verbsin the summary class that are tagged by the GI re-mains almost constant while the proportion of un-tagged verbs in the article class steadily increases.This indicates that the summary-biased verbs havea consistent distribution of GI tags across all usageratios, while verbs tended to be tagged less often asthe bias towards the articles increased.
Although theGI gives good indicators for which words are likelyto be important summary words but no indicators forwhich words are likely to be of no interest in sum-marizing world news.3.1 Personal StoriesTo get a sense for what aspects of the verb semanticscauses a word to be excluded from the summary, weexamined the contexts for the verbs with the high-est bias weights in each class.
To define the contextfor each verb, we used the dependency relations pro-duced by the Stanford Parser.
Any verb, noun, oradverb placed in a dependency relation with a givenverb is considered to co-occur with it.
For each ofthe ten most highly weighted verbs in each class, Ta-ble 4 shows the lemmas that co-occurred most fre-quently with it.The verbs that are biased towards the articles(not important) seem to capture human element ofthe news reports, corresponding to passages narrat-ing personal stories of ordinary people involved inthe larger political situation discussed in the news.The summary-biased verbs are clearly evocative ofthe NEGATIVE, ACTIVE, STRONG, HOSTILE, andPOWER tags given by the GI and the common us-ages suggested by their contexts tend to be official,non-personal or that of people in public roles.No existing resources provide descriptions of thispersonal vs. non-personal dimension of lexicalmeaning and we decided to derive such a character-ization from data unrelated to the NYT.1443Article-biasedadd country,year,get,States,time,people,do,makedrink drink,do,take,glass,much,make,eatsing song,woman,man,chorus,dance,sing,feelrefer use,official,attack,part,term,program,day,peopleread time,report,people,write,statement,book,manthink time,part,get,do,year,take,issueimagine take,people,get,come,time,make,askremember day,year,time,see,decade,manyshout man,people,hear,soldier,get,come,crowdsit day,man,road,talk,wall,people,watch,tableSummary-biasedspur do,action,help,tell,States,effort,concern,manhail leader,man,call,effort,Clinton,step,election,visitallege part,case,fraud,arrest,help,people,responsibleavert attack,Iraq,action,month,confrontation,crisis,officialslay week,month,member,attack,many,soldier,day,Americansexile country,accuse,many,kill,Hussein,family,friend,Arafatclaim member,bombing,describe,part,group,life,leaderintensify country,States,war,week,demand,day,year,effortextradite Britain,States,try,citizen,Pinochet,trial,member,receiveoust year,Party,Minister,force,coalition,invasion,month,leaderTable 4: Most frequent co-occurring words for the mostextremely weighted verbs.Personal-biasedthreaten wake rain wander kneel yell grin convulsesmile chat hug climb gorge nod crouch laugh sleepperch head parkNonpersonal-biasedacquit deploy misstate founder besiege decriminalizecensure peacekeep headquarter streamline dissociateexcommunicate unveil deadlock modify extradite rat-ify imperil choseTable 5: Top weighted words derived from personal andnon-personal blog entriesFor this purpose, we used a subset of the ICWSM2009 Spinn3r Blog Dataset that has been annotatedwith a semi-supervised classifier trained to identifypersonal stories (Gordon and Swanson, 2009).
Wetook 56,048 blog entries that had been tagged as be-ing a personal story and 2,196,162 blog entries thatwere not identified as personal.We then applied the same procedure that we usedfor the NYT articles to produce two classes ofwords: those biased towards blogs describing per-sonal stories and those biased towards non-personalblogs.
After restricting the vocabulary to only verbswith a binomial test p-value of at most 0.05, we ob-tained log odds ratio weights for 3,143 verbs.
Ofthe 1,210 verbs in the NYT classes, 937 were alsopresent in the restricted blog vocabulary.
The 20most and least personal verbs are shown in Table 5.p-value GI GI+blog0.05 0.134 0.0980.01 0.130 0.087Table 6: 10-fold cross-validation mean squared error of alinear regression for increasingly biased vocabularies.The Pearson correlation between the NYT logodds ratio and the blog log odds ratio is negativeand rather high, -0.54, indicating a strong relation-ship between personal and article-biased words.
Re-stricting the significance to p-value cutoff of 0.01reduces the vocabulary from 937 to 675 verbs, butstrengthens the correlation to -0.61.
Of the top 100summary-biased words, only 18 were personal.
Ofthe top 100 article-biased words, 90 were personal.Not only do the personal/non-personal classesmap on to the summary/article classes well, but theysupply explanatory information about words that theGI did not cover.
In order to measure this effect, wetrained a linear regression to predict the NYT log-odds ratio of a word using a binary feature for eachGI tag, as well as a binary feature indicating no tags.We were interested in the reduction of error whenthe personal-biased information was added.
Addingthe blog log-odds ratio for each word as a featureimproved our results in 10-fold cross-validation, re-ducing the prediction error by almost 30%.
The de-tailed results are shown in Table 6, for experimentsperformed for two different p-value cut-offs.4 ConclusionWe presented a method for data-driven acquisitionof domain-level importance of verbs in reports ofworld news events.
Analysis of the acquired verbsreveals that summary-biased words tend to be morenegative, active, and hostile, while the article-biasedwords mostly describe personal actions.
This lex-icon provides a useful notion of global importancein a domain and can serve as resource for seman-tic characterization of words in a variety of tasks,including sentence selection in summarization, flag-ging articles as newsworthy or filtering uninterest-ing documents.
Additionally, we provide a lexiconfor personal and non-personal verbs that also cap-tures some of the newsworthiness of the article andsummary classes.1444ReferencesJesse Dunietz and Dan Gillick.
A new entity saliencetask with millions of training examples.
EACL2014, page 205, 2014.Andrew Gordon and Reid Swanson.
Identifying per-sonal stories in millions of weblog entries.
InThird International Conference on Weblogs andSocial Media, Data Challenge Workshop, SanJose, CA, 2009.Kai Hong and Ani Nenkova.
Improving the estima-tion of word importance for news multi-documentsummarization.
Proceedings of EACL, 2014.Burt L Monroe, Michael P Colaresi, and Kevin MQuinn.
Fightin?words: Lexical feature selectionand evaluation for identifying the content of po-litical conflict.
Political Analysis, 16(4):372?403,2008.Ani Nenkova and Kathleen McKeown.
A surveyof text summarization techniques.
In Mining TextData, pages 43?76.
Springer US, 2012.Karolina Owczarzak and Hoa Trang Dang.Overview of the tac 2011 summarization track:Guided task and aesop task.
In Proceedingsof the Text Analysis Conference (TAC 2011),Gaithersburg, Maryland, USA, November, 2011.Dragomir R Radev and Kathleen R McKeown.
Gen-erating natural language summaries from multipleon-line sources.
Computational Linguistics, 24(3):470?500, 1998.Barry Schiffman, Ani Nenkova, and Kathleen McK-eown.
Experiments in multidocument summa-rization.
In Proceedings of the second interna-tional conference on Human Language Technol-ogy Research, pages 52?58, 2002.Philip J Stone, Dexter C Dunphy, and Marshall SSmith.
The general inquirer: A computer ap-proach to content analysis.
1966.Kristian Woodsend and Mirella Lapata.
Multiple as-pect summarization using integer linear program-ming.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, pages 233?243, 2012.1445
